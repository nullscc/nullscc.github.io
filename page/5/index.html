
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/5/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-eess.IV_2023_08_21" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/21/eess.IV_2023_08_21/" class="article-date">
  <time datetime="2023-08-20T16:00:00.000Z" itemprop="datePublished">2023-08-21</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/21/eess.IV_2023_08_21/">eess.IV - 2023-08-21 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Extraction-of-Text-from-Optic-Nerve-Optical-Coherence-Tomography-Reports"><a href="#Extraction-of-Text-from-Optic-Nerve-Optical-Coherence-Tomography-Reports" class="headerlink" title="Extraction of Text from Optic Nerve Optical Coherence Tomography Reports"></a>Extraction of Text from Optic Nerve Optical Coherence Tomography Reports</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10790">http://arxiv.org/abs/2308.10790</a></li>
<li>repo_url: None</li>
<li>paper_authors: Iyad Majid, Youchen Victor Zhang, Robert Chang, Sophia Y. Wang</li>
<li>for: This study aimed to develop and evaluate rule-based algorithms for extracting text data, including RNFL values and GCC data, from Zeiss Cirrus OCT scan reports.</li>
<li>methods: The study used DICOM files with encapsulated PDF reports, converted them into image files, and used the PaddleOCR Python package for optical character recognition. Rule-based algorithms were designed and optimized for improved performance in extracting RNFL and GCC data.</li>
<li>results: The developed algorithms demonstrated high precision in extracting data from both RNFL and GCC scans, with slightly better precision for the right eye in RNFL extraction and for the left eye in GCC extraction. However, some values presented more challenges in extraction, such as clock hours 5 and 6 for RNFL thickness and signal strength for GCC.Here’s the information in Simplified Chinese text:</li>
<li>for: 这项研究的目的是开发和评估基于规则的算法，以提高从Zeiss Cirrus光子共振成像扫描报告中提取文本数据，包括肾脉层（RNFL）值和其他膝肾细胞计数（GCC）数据。</li>
<li>methods: 该研究使用DICOM文件中嵌入的PDF报告，并将其转换为图像文件，使用Python中的PaddleOCR包进行光学字符识别。研究人员设计了和优化了基于规则的算法，以提高对RNFL和GCC扫描报告中的数据提取精度。</li>
<li>results: 开发的算法在提取RNFL和GCC扫描报告中的数据时显示了高精度。对于右眼，RNFL提取精度较高（OD: 0.9803 vs. OS: 0.9046），对于左眼，GCC提取精度较高（OD: 0.9567 vs. OS: 0.9677）。然而，一些值在提取时存在更大的挑战，如RNFL厚度的时钟小时5和6，以及GCC的信号强度。<details>
<summary>Abstract</summary>
Purpose: The purpose of this study was to develop and evaluate rule-based algorithms to enhance the extraction of text data, including retinal nerve fiber layer (RNFL) values and other ganglion cell count (GCC) data, from Zeiss Cirrus optical coherence tomography (OCT) scan reports. Methods: DICOM files that contained encapsulated PDF reports with RNFL or Ganglion Cell in their document titles were identified from a clinical imaging repository at a single academic ophthalmic center. PDF reports were then converted into image files and processed using the PaddleOCR Python package for optical character recognition. Rule-based algorithms were designed and iteratively optimized for improved performance in extracting RNFL and GCC data. Evaluation of the algorithms was conducted through manual review of a set of RNFL and GCC reports. Results: The developed algorithms demonstrated high precision in extracting data from both RNFL and GCC scans. Precision was slightly better for the right eye in RNFL extraction (OD: 0.9803 vs. OS: 0.9046), and for the left eye in GCC extraction (OD: 0.9567 vs. OS: 0.9677). Some values presented more challenges in extraction, particularly clock hours 5 and 6 for RNFL thickness, and signal strength for GCC. Conclusions: A customized optical character recognition algorithm can identify numeric results from optical coherence scan reports with high precision. Automated processing of PDF reports can greatly reduce the time to extract OCT results on a large scale.
</details>
<details>
<summary>摘要</summary>
目的：本研究的目的是开发和评估基于规则的算法，以提高从Zeiss Cirrus光合成 Tomatoes(OCT)扫描报告中提取文本数据的精度，包括胁肤神经层(RNFL)值和神经细胞计数(GCC)数据。方法：从一所学术眼科中心的临床扫描存储系统中标记为包含DICOM文档的PDF报告，并将PDF报告转换为图像文件，然后使用Python包PaddleOCR进行光学字符识别。基于规则的算法被设计并优化，以提高提取RNFL和GCC数据的精度。评估算法的效果通过手动复审一组RNFL和GCC报告进行评估。结果：开发的算法在RNFL和GCC扫描报告中提取数据的精度很高，OD和OS的精度分别为0.9803和0.9046，以及0.9567和0.9677。但是，某些值在提取中存在更大的挑战，例如RNFL厚度的时钟小时5和6，以及GCC的信号强度。结论：可以使用自定义的光学字符识别算法来从OCT扫描报告中提取数据，并且自动处理PDF报告可以大幅减少大规模提取OCT结果的时间。
</details></li>
</ul>
<hr>
<h2 id="Dense-Error-Map-Estimation-for-MRI-Ultrasound-Registration-in-Brain-Tumor-Surgery-Using-Swin-UNETR"><a href="#Dense-Error-Map-Estimation-for-MRI-Ultrasound-Registration-in-Brain-Tumor-Surgery-Using-Swin-UNETR" class="headerlink" title="Dense Error Map Estimation for MRI-Ultrasound Registration in Brain Tumor Surgery Using Swin UNETR"></a>Dense Error Map Estimation for MRI-Ultrasound Registration in Brain Tumor Surgery Using Swin UNETR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10784">http://arxiv.org/abs/2308.10784</a></li>
<li>repo_url: None</li>
<li>paper_authors: Soorena Salari, Amirhossein Rasoulian, Hassan Rivaz, Yiming Xiao</li>
<li>for: 降低脑肿瘤手术死亡率的早期手术治疗</li>
<li>methods: 使用投射照像（iUS）跟踪脑组织变形，并使用高精度的MRI-iUS匹配技术更新先前的手术计划，以提高手术安全性和效果</li>
<li>results: 提出了一种基于深度学习（DL）的框架，可以自动评估MRI-iUS匹配结果的质量，并在实际临床数据上显示其性能。<details>
<summary>Abstract</summary>
Early surgical treatment of brain tumors is crucial in reducing patient mortality rates. However, brain tissue deformation (called brain shift) occurs during the surgery, rendering pre-operative images invalid. As a cost-effective and portable tool, intra-operative ultrasound (iUS) can track brain shift, and accurate MRI-iUS registration techniques can update pre-surgical plans and facilitate the interpretation of iUS. This can boost surgical safety and outcomes by maximizing tumor removal while avoiding eloquent regions. However, manual assessment of MRI-iUS registration results in real-time is difficult and prone to errors due to the 3D nature of the data. Automatic algorithms that can quantify the quality of inter-modal medical image registration outcomes can be highly beneficial. Therefore, we propose a novel deep-learning (DL) based framework with the Swin UNETR to automatically assess 3D-patch-wise dense error maps for MRI-iUS registration in iUS-guided brain tumor resection and show its performance with real clinical data for the first time.
</details>
<details>
<summary>摘要</summary>
早期手术治疗脑肿的时间点对病人死亡率有重要影响。然而，手术过程中脑组织变形（称为脑Shift）会使先前的图像无效。作为一种cost-effective和可搬式工具，在手术过程中的ultrasound（iUS）可以跟踪脑Shift，并且精准的MRI-iUS注册技术可以更新先前的 планы和促进iUS的解释。这可以提高手术安全性和效果，最大化肿瘤除除而避免感知区域。然而，手动评估MRI-iUS注册结果的实时性具有困难和错误的可能性，因为数据的3D性。自动的算法可以评估多Modal医疗图像注册结果的质量。因此，我们提出了一个基于深度学习（DL）的框架，使用Swin UNITER来自动评估3D-patch-wise稠密错误地图进行MRI-iUS注册的性能，并在实际临床数据上展示其性能。
</details></li>
</ul>
<hr>
<h2 id="Automated-Identification-of-Failure-Cases-in-Organ-at-Risk-Segmentation-Using-Distance-Metrics-A-Study-on-CT-Data"><a href="#Automated-Identification-of-Failure-Cases-in-Organ-at-Risk-Segmentation-Using-Distance-Metrics-A-Study-on-CT-Data" class="headerlink" title="Automated Identification of Failure Cases in Organ at Risk Segmentation Using Distance Metrics: A Study on CT Data"></a>Automated Identification of Failure Cases in Organ at Risk Segmentation Using Distance Metrics: A Study on CT Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10636">http://arxiv.org/abs/2308.10636</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amin Honarmandi Shandiz, Attila Rádics, Rajesh Tamada, Makk Árpád, Karolina Glowacka, Lehel Ferenczi, Sandeep Dutta, Michael Fanariotis</li>
<li>for: 提高自动生成的肿瘤分割精度，以便更好地规划辐射治疗</li>
<li>methods: 使用维度距离和 Hausdorff 距离的组合来自动标识失败案例，从而更快地修复失败案例</li>
<li>results: 通过设置维度距离和 Hausdorff 距离的阈值，能够快速地自动标识失败案例，并且可以对12个不同的失败案例进行视觉评估<details>
<summary>Abstract</summary>
Automated organ at risk (OAR) segmentation is crucial for radiation therapy planning in CT scans, but the generated contours by automated models can be inaccurate, potentially leading to treatment planning issues. The reasons for these inaccuracies could be varied, such as unclear organ boundaries or inaccurate ground truth due to annotation errors. To improve the model's performance, it is necessary to identify these failure cases during the training process and to correct them with some potential post-processing techniques. However, this process can be time-consuming, as traditionally it requires manual inspection of the predicted output. This paper proposes a method to automatically identify failure cases by setting a threshold for the combination of Dice and Hausdorff distances. This approach reduces the time-consuming task of visually inspecting predicted outputs, allowing for faster identification of failure case candidates. The method was evaluated on 20 cases of six different organs in CT images from clinical expert curated datasets. By setting the thresholds for the Dice and Hausdorff distances, the study was able to differentiate between various states of failure cases and evaluate over 12 cases visually. This thresholding approach could be extended to other organs, leading to faster identification of failure cases and thereby improving the quality of radiation therapy planning.
</details>
<details>
<summary>摘要</summary>
自动化器官风险（OAR）分割是辐射疗法规划CT扫描图中的关键，但自动生成的边界可能存在误差，可能导致治疗规划问题。这些误差的原因可能是不清晰的器官边界或者实际数据错误，导致标注错误。为了提高模型的性能，需要在训练过程中识别这些失败案例，并使用一些可能的后处理技术来更正。然而，这个过程可能占用很多时间，因为传统上需要手动检查预测输出。这篇论文提出了一种方法，通过设置Dice和 Hausdorff距离的组合阈值，自动地识别失败案例。这种方法可以减少手动检查预测输出的时间占用，并允许更快地识别失败案例候选者。该方法在20个不同器官的CT图像中进行了20个案例的评估，通过设置阈值，能够分辨出不同类型的失败案例，并评估12个案例。这种阈值设置方法可以扩展到其他器官，从而更快地识别失败案例，提高辐射疗法规划的质量。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Medical-Image-Segmentation-Optimizing-Cross-Entropy-Weights-and-Post-Processing-with-Autoencoders"><a href="#Enhancing-Medical-Image-Segmentation-Optimizing-Cross-Entropy-Weights-and-Post-Processing-with-Autoencoders" class="headerlink" title="Enhancing Medical Image Segmentation: Optimizing Cross-Entropy Weights and Post-Processing with Autoencoders"></a>Enhancing Medical Image Segmentation: Optimizing Cross-Entropy Weights and Post-Processing with Autoencoders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10488">http://arxiv.org/abs/2308.10488</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pranav Singh, Luoyao Chen, Mei Chen, Jinqian Pan, Raviteja Chukkapalli, Shravan Chaudhari, Jacopo Cirrone</li>
<li>for: 本研究旨在提高医学图像分割的精度和效率，特别是在抗体性疾病如皮肤粘液病中分割细胞和炎症的过程中。</li>
<li>methods: 本研究采用深度学习技术，开发了一种适应性能高的医学图像分割方法，并对捷克网络（U-Net）和U-Net++进行了比较。</li>
<li>results: 实验结果表明，本研究的方法在皮肤粘液病图像分割任务上比现有技术高效率12.26%和12.04%。此外，我们还对loss函数权重的优化和三个医学图像分割任务进行了比较。<details>
<summary>Abstract</summary>
The task of medical image segmentation presents unique challenges, necessitating both localized and holistic semantic understanding to accurately delineate areas of interest, such as critical tissues or aberrant features. This complexity is heightened in medical image segmentation due to the high degree of inter-class similarities, intra-class variations, and possible image obfuscation. The segmentation task further diversifies when considering the study of histopathology slides for autoimmune diseases like dermatomyositis. The analysis of cell inflammation and interaction in these cases has been less studied due to constraints in data acquisition pipelines. Despite the progressive strides in medical science, we lack a comprehensive collection of autoimmune diseases. As autoimmune diseases globally escalate in prevalence and exhibit associations with COVID-19, their study becomes increasingly essential. While there is existing research that integrates artificial intelligence in the analysis of various autoimmune diseases, the exploration of dermatomyositis remains relatively underrepresented. In this paper, we present a deep-learning approach tailored for Medical image segmentation. Our proposed method outperforms the current state-of-the-art techniques by an average of 12.26% for U-Net and 12.04% for U-Net++ across the ResNet family of encoders on the dermatomyositis dataset. Furthermore, we probe the importance of optimizing loss function weights and benchmark our methodology on three challenging medical image segmentation tasks
</details>
<details>
<summary>摘要</summary>
医疗图像分割任务具有独特的挑战，需要同时具备本地化和整体 semantics 的理解，以准确地分割关键区域，如病理性组织或异常特征。这种复杂性在医疗图像分割中受到高度的类间相似性、类内变化和可能的图像掩蔽的影响。医疗图像分割任务进一步复杂化，当考虑到研究 histopathology 板块 для自遗护疾病如dermatomyositis时。对于这些病例，分割和检测细胞Inflammation和互动的分析尚未得到了充分的研究，这主要是因为数据获取管道的限制。尽管医学科技在进步的同时，我们缺乏一个完整的自遗护疾病集合。自遗护疾病在全球范围内的发病率不断增长，并与 COVID-19 相关，因此其研究变得越来越重要。虽然现有的研究已经将人工智能 integrate 到了不同的自遗护疾病的分析中，但是dermatomyositis 的研究仍然相对落后。在这篇文章中，我们提出了一种适用于医疗图像分割的深度学习方法。我们的提议方法在 ResNet 家族Encoder 上的 U-Net 和 U-Net++ 中超过了平均提高12.26%和12.04%。此外，我们还评估了优化损失函数的重要性，并在三个困难的医疗图像分割任务上进行了比较。
</details></li>
</ul>
<hr>
<h2 id="Prediction-of-Pneumonia-and-COVID-19-Using-Deep-Neural-Networks"><a href="#Prediction-of-Pneumonia-and-COVID-19-Using-Deep-Neural-Networks" class="headerlink" title="Prediction of Pneumonia and COVID-19 Using Deep Neural Networks"></a>Prediction of Pneumonia and COVID-19 Using Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10368">http://arxiv.org/abs/2308.10368</a></li>
<li>repo_url: None</li>
<li>paper_authors: M. S. Haque, M. S. Taluckder, S. B. Shawkat, M. A. Shahriyar, M. A. Sayed, C. Modak</li>
<li>for: 这个研究旨在探讨医疗图像分析是否可以帮助早期识别感染病毒和细菌所致的肺炎，以便减少其传播。</li>
<li>methods: 这个研究使用机器学习技术来预测肺炎，并评估不同的机器学习模型在肺炎患者的颈部X射线图像上的表现。</li>
<li>results: 研究发现，使用DenseNet121模型可以实现肺炎的准确预测，其准确率为99.58%。这项研究显示了机器学习技术在精确肺炎诊断中的重要性，并提供了这方面的技术传承。<details>
<summary>Abstract</summary>
Pneumonia, caused by bacteria and viruses, is a rapidly spreading viral infection with global implications. Prompt identification of infected individuals is crucial for containing its transmission. This study explores the potential of medical image analysis to address this challenge. We propose machine-learning techniques for predicting Pneumonia from chest X-ray images. Chest X-ray imaging is vital for Pneumonia diagnosis due to its accessibility and cost-effectiveness. However, interpreting X-rays for Pneumonia detection can be complex, as radiographic features can overlap with other respiratory conditions. We evaluate the performance of different machine learning models, including DenseNet121, Inception Resnet-v2, Inception Resnet-v3, Resnet50, and Xception, using chest X-ray images of pneumonia patients. Performance measures and confusion matrices are employed to assess and compare the models. The findings reveal that DenseNet121 outperforms other models, achieving an accuracy rate of 99.58%. This study underscores the significance of machine learning in the accurate detection of Pneumonia, leveraging chest X-ray images. Our study offers insights into the potential of technology to mitigate the spread of pneumonia through precise diagnostics.
</details>
<details>
<summary>摘要</summary>
《肺炎，由病毒和 бактерий引起的，是一种迅速传播的感染病种，具有全球化的意义。》Prompt identification of infected individuals is crucial for containing the transmission of pneumonia. This study explores the potential of medical image analysis to address this challenge. We propose machine-learning techniques for predicting pneumonia from chest X-ray images. Chest X-ray imaging is vital for pneumonia diagnosis due to its accessibility and cost-effectiveness. However, interpreting X-rays for pneumonia detection can be complex, as radiographic features can overlap with other respiratory conditions. We evaluate the performance of different machine learning models, including DenseNet121, Inception Resnet-v2, Inception Resnet-v3, Resnet50, and Xception, using chest X-ray images of pneumonia patients. Performance measures and confusion matrices are employed to assess and compare the models. The findings reveal that DenseNet121 outperforms other models, achieving an accuracy rate of 99.58%. This study underscores the significance of machine learning in the accurate detection of pneumonia, leveraging chest X-ray images. Our study offers insights into the potential of technology to mitigate the spread of pneumonia through precise diagnostics.Here's the text with some notes on the translation:* "肺炎" (pneumonia) is a noun, and it is translated as "肺炎" (pneumonia) in Simplified Chinese.* "由病毒和 бактерий引起" (caused by bacteria and viruses) is a prepositional phrase, and it is translated as "由病毒和 бактерий引起" (caused by bacteria and viruses) in Simplified Chinese.* "是一种迅速传播的感染病种" (a rapidly spreading viral infection) is a sentence, and it is translated as "是一种迅速传播的感染病种" (a rapidly spreading viral infection) in Simplified Chinese.* "Prompt identification of infected individuals is crucial for containing the transmission of pneumonia" is a sentence, and it is translated as "Prompt identification of infected individuals is crucial for containing the transmission of pneumonia" in Simplified Chinese.* "This study explores the potential of medical image analysis to address this challenge" is a sentence, and it is translated as "这种研究探讨了医疗图像分析如何解决这一挑战" (this study explores the potential of medical image analysis to address this challenge) in Simplified Chinese.* "We propose machine-learning techniques for predicting pneumonia from chest X-ray images" is a sentence, and it is translated as "我们提议使用机器学习技术预测肺炎从胸部X射图像" (we propose machine-learning techniques for predicting pneumonia from chest X-ray images) in Simplified Chinese.* "Chest X-ray imaging is vital for pneumonia diagnosis due to its accessibility and cost-effectiveness" is a sentence, and it is translated as "胸部X射图像诊断肺炎具有可行性和成本效益" (chest X-ray imaging is vital for pneumonia diagnosis due to its accessibility and cost-effectiveness) in Simplified Chinese.* "However, interpreting X-rays for pneumonia detection can be complex, as radiographic features can overlap with other respiratory conditions" is a sentence, and it is translated as "然而，从X射图像中诊断肺炎可能会具有复杂性，因为肺炎的Radiographic特征可能与其他呼吸道疾病重叠" (however, interpreting X-rays for pneumonia detection can be complex, as radiographic features can overlap with other respiratory conditions) in Simplified Chinese.* "We evaluate the performance of different machine learning models, including DenseNet121, Inception Resnet-v2, Inception Resnet-v3, Resnet50, and Xception, using chest X-ray images of pneumonia patients" is a sentence, and it is translated as "我们使用不同的机器学习模型，包括DenseNet121、Inception Resnet-v2、Inception Resnet-v3、Resnet50和Xception，使用肺炎患者的胸部X射图像进行评估" (we evaluate the performance of different machine learning models, including DenseNet121, Inception Resnet-v2, Inception Resnet-v3, Resnet50, and Xception, using chest X-ray images of pneumonia patients) in Simplified Chinese.* "Performance measures and confusion matrices are employed to assess and compare the models" is a sentence, and it is translated as "我们使用性能指标和混淆矩阵来评估和比较不同模型的表现" (performance measures and confusion matrices are employed to assess and compare the models) in Simplified Chinese.* "The findings reveal that DenseNet121 outperforms other models, achieving an accuracy rate of 99.58%" is a sentence, and it is translated as "发现结果表明，DenseNet121模型在识别肺炎方面的准确率为99.58%" (the findings reveal that DenseNet121 outperforms other models, achieving an accuracy rate of 99.58%) in Simplified Chinese.* "This study underscores the significance of machine learning in the accurate detection of pneumonia, leveraging chest X-ray images" is a sentence, and it is translated as "这种研究强调了机器学习在肺炎检测中的重要性，利用胸部X射图像" (this study underscores the significance of machine learning in the accurate detection of pneumonia, leveraging chest X-ray images) in Simplified Chinese.* "Our study offers insights into the potential of technology to mitigate the spread of pneumonia through precise diagnostics" is a sentence, and it is translated as "我们的研究提供了有关技术在防止肺炎传播的精准诊断方面的洞察" (our study offers insights into the potential of technology to mitigate the spread of pneumonia through precise diagnostics) in Simplified Chinese.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/21/eess.IV_2023_08_21/" data-id="clly3dw2m00ey09889ecd2eea" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_20" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/20/cs.LG_2023_08_20/" class="article-date">
  <time datetime="2023-08-19T16:00:00.000Z" itemprop="datePublished">2023-08-20</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/20/cs.LG_2023_08_20/">cs.LG - 2023-08-20 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Preserving-Specificity-in-Federated-Graph-Learning-for-fMRI-based-Neurological-Disorder-Identification"><a href="#Preserving-Specificity-in-Federated-Graph-Learning-for-fMRI-based-Neurological-Disorder-Identification" class="headerlink" title="Preserving Specificity in Federated Graph Learning for fMRI-based Neurological Disorder Identification"></a>Preserving Specificity in Federated Graph Learning for fMRI-based Neurological Disorder Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10302">http://arxiv.org/abs/2308.10302</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junhao Zhang, Qianqian Wang, Xiaochuan Wang, Lishan Qiao, Mingxia Liu</li>
<li>for: 这个研究旨在应用Resting-state functional magnetic resonance imaging (rs-fMRI)和 Federated Learning (FL)技术来探索脑疾病的不均衡连接性，并且考虑到体域特点如年龄、性别和教育水准。</li>
<li>methods: 本研究使用了Graph Neural Network (GNN)和 Federated Learning (FL)技术，并且提出了特定性敏感 Federated Graph Learning (SFGL)框架，以探索不同体域特点下的脑疾病特征。</li>
<li>results: 实验结果显示，SFGL方法在两个rs-fMRI数据集上的调整后，较前方的方法提高了10%至20%的准确率。<details>
<summary>Abstract</summary>
Resting-state functional magnetic resonance imaging (rs-fMRI) offers a non-invasive approach to examining abnormal brain connectivity associated with brain disorders. Graph neural network (GNN) gains popularity in fMRI representation learning and brain disorder analysis with powerful graph representation capabilities. Training a general GNN often necessitates a large-scale dataset from multiple imaging centers/sites, but centralizing multi-site data generally faces inherent challenges related to data privacy, security, and storage burden. Federated Learning (FL) enables collaborative model training without centralized multi-site fMRI data. Unfortunately, previous FL approaches for fMRI analysis often ignore site-specificity, including demographic factors such as age, gender, and education level. To this end, we propose a specificity-aware federated graph learning (SFGL) framework for rs-fMRI analysis and automated brain disorder identification, with a server and multiple clients/sites for federated model aggregation and prediction. At each client, our model consists of a shared and a personalized branch, where parameters of the shared branch are sent to the server while those of the personalized branch remain local. This can facilitate knowledge sharing among sites and also helps preserve site specificity. In the shared branch, we employ a spatio-temporal attention graph isomorphism network to learn dynamic fMRI representations. In the personalized branch, we integrate vectorized demographic information (i.e., age, gender, and education years) and functional connectivity networks to preserve site-specific characteristics. Representations generated by the two branches are then fused for classification. Experimental results on two fMRI datasets with a total of 1,218 subjects suggest that SFGL outperforms several state-of-the-art approaches.
</details>
<details>
<summary>摘要</summary>
RESTING-STATE ФУНКЦИОНАЛНО МАГНЕТНО РЕЗОНАНСНА ИМАЖИНГ (RS-fMRI) ПРОВОДИ ГНИЛОСТИВЫЙ ПОДХОД К ИЗУЧЕНИЮ АБНОРМАЛЬНОЙ БРАИНОВОЙ СОЕДИНЕНИЯ ПРОВОДЗЕМ СО БРАИНОВЫМИ РАЗЛИЧИЯМИ. ГРАФОВАЯ НЕРВНАЯ СИСТЕМА (GNN) ЗНАЧАЕТСЯ В ФМРИ ЗАПИСИВАНИЕ И БРАИНОВОЙ РАЗЛИЧИЕВАНИИ СО ВСЕМОГОВОРТНОЙ ГРАФОЙОЙ РЕПРЕСЕНТАЦИЕЙ. ОБУЧЕНИЕ ГЕНЕРАЛЬНОЙ GNN НОВОЕ РЕЗУЛЬТАТО ВОЗМОЖНО СОВЕРШИТЬ БЕЗ ЦЕНТРАЛИЗОВАННОГО МНОГОСИТЕЙНОГО ДАННЫХ, НО ОБЫЧЕСКИЕ ПРОТОКОЛЫ ФЛЕARNING (FL) ДЛЯ ФМРИАНАЛИЗА СНИЖАЮТ ОБЪЕМ ДАННЫХ ПО КОРАБОРАТИВНОМУ ОБЪЕМУ. НО В ПРЕДЫДСТВИИ ТОГО, ПРОПОЛОЖЕННЫЕ ФЛЕARNING (SFGL) ПРОВОДИТ К КОМБИНИРОВАНИЮ МЕСТООБРАЗНЫХ КАРАКТЕРИСТИК ИЗУЧЕНИЯ БРАИНОВОГО РАЗЛИЧИЕВАНИА. В НАШЕМ ФРАМВОРКЕ, МАТЕРИАЛЫ СЕРВЕРА И МУЛЬТИПЛОВЫЕ КЛИЕНТЫ/СИТИ ИСКАЗЫВАТЬСЯ В ФЕДЕРАТИВНОМ ОБЪЕМЕ, ГДЕ ВСЕ КЛИЕНТЫ ПРОВОДЗЕМ ИЗ СЕРВЕРА СОБСТВЕННЫЕ И ПЕРСОНАЛИЗОВАННЫЕ БРАНЧИ. В СОБСТВЕННОМ БРАНЧЕ МЫ ЗЕМЛЯЕМ СПАЦИО-ВРЕМЕННЫЙ ВАТЕРНЫЙ ГРАФОВЫЙ ИЗОМОРФИЗМ КАК РЕПРЕСЕНТАЦИЮ ФМРИ. В ПЕРСОНАЛИЗОВАННЫМ БРАНЧЕ МЫ СОБИРАЕММ ВЕКТОРИЗИРОВАННЫЕ ДЕМОГРАФИЧЕСКИЕ ИЗМЕРЕНИЯ (НАПРИМЕР, ВОСРОД СТОРОК, И УЧЕТ ОБУЧЕНИЯ) И ФУНКЦИОНАЛЬНУЮ СОЕДИНЕННУЮ СИТЬ, ЧТОБЫ ПРЕЗЕРВИТЬ МЕСТООБРАЗНЫЕ КАРАКТЕРИСТИКИ. ЗАПИСИВАННЫЕ В ДВУХ БРАНЧАХ ЗАТОМ СОБИРАЕТСЯ ДЛЯ КЛАССИРОВКИ. ЭКСПЕРИМЕНТАЛЬНЫЕ РЕЗУЛЬТАТЫ ПРОВОДИТЫ КТОРЫМУ, ЧТО SFGL ПРОВОДИТ К БЫСТРОМ И ГОРОДОМ ПОКОВКЕ, ПРОТИВОЗАПИСАНИЮ ИЗУЧЕНИЯ БРАИНОВОГО РАЗЛИЧИЕВАНИА.
</details></li>
</ul>
<hr>
<h2 id="An-interpretable-deep-learning-method-for-bearing-fault-diagnosis"><a href="#An-interpretable-deep-learning-method-for-bearing-fault-diagnosis" class="headerlink" title="An interpretable deep learning method for bearing fault diagnosis"></a>An interpretable deep learning method for bearing fault diagnosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10292">http://arxiv.org/abs/2308.10292</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Lu, Austin M. Bray, Chao Hu, Andrew T. Zimmerman, Hongyi Xu</li>
<li>for: 这个研究旨在解决深度学习模型中的黑盒问题，以提高人工维护人员对模型的信任度。</li>
<li>methods: 本研究使用了卷积神经网组合Gradient-weighted Class Activation Mapping（Grad-CAM）来实现深度学习模型的解释和可读性。</li>
<li>results: 研究结果显示，使用Grad-CAM可以从training sample中找到重要的特征对照，并将其annotate为健康库（health library）中的一部分。在评估过程中，提出的方法可以从健康库中选择相似的预测基础样本，以提高模型的可信度。<details>
<summary>Abstract</summary>
Deep learning (DL) has gained popularity in recent years as an effective tool for classifying the current health and predicting the future of industrial equipment. However, most DL models have black-box components with an underlying structure that is too complex to be interpreted and explained to human users. This presents significant challenges when deploying these models for safety-critical maintenance tasks, where non-technical personnel often need to have complete trust in the recommendations these models give. To address these challenges, we utilize a convolutional neural network (CNN) with Gradient-weighted Class Activation Mapping (Grad-CAM) activation map visualizations to form an interpretable DL method for classifying bearing faults. After the model training process, we apply Grad-CAM to identify a training sample's feature importance and to form a library of diagnosis knowledge (or health library) containing training samples with annotated feature maps. During the model evaluation process, the proposed approach retrieves prediction basis samples from the health library according to the similarity of the feature importance. The proposed method can be easily applied to any CNN model without modifying the model architecture, and our experimental results show that this method can select prediction basis samples that are intuitively and physically meaningful, improving the model's trustworthiness for human users.
</details>
<details>
<summary>摘要</summary>
深度学习（DL）在最近几年内得到了广泛应用，用于现代工业设备的类型和未来预测。然而，大多数DL模型具有黑盒子组件，其下面结构太复杂，无法被人类用户理解和解释。这些问题在安全维护任务中具有重要性，因为非技术人员经常需要对这些模型的建议产生完全的信任。为解决这些问题，我们使用卷积神经网络（CNN）和梯度权重分类图像（Grad-CAM）活动图像可视化来形成可解释的DL方法，用于分类承载问题。在模型训练过程中，我们使用Grad-CAM来标识训练样本的特征重要性，并将其作为健康图书馆（health library）中的训练样本，并将其标注为特征图像。在评估模型过程中，我们的方法可以从健康图书馆中检索预测基础样本，根据特征重要性的相似性。我们的方法可以轻松地应用于任何CNN模型，无需修改模型结构，我们的实验结果表明，这种方法可以选择Physically meaningful和直观的预测基础样本，提高模型的人类用户的信任度。
</details></li>
</ul>
<hr>
<h2 id="Towards-Few-shot-Coordination-Revisiting-Ad-hoc-Teamplay-Challenge-In-the-Game-of-Hanabi"><a href="#Towards-Few-shot-Coordination-Revisiting-Ad-hoc-Teamplay-Challenge-In-the-Game-of-Hanabi" class="headerlink" title="Towards Few-shot Coordination: Revisiting Ad-hoc Teamplay Challenge In the Game of Hanabi"></a>Towards Few-shot Coordination: Revisiting Ad-hoc Teamplay Challenge In the Game of Hanabi</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10284">http://arxiv.org/abs/2308.10284</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hadi Nekoei, Xutong Zhao, Janarthanan Rajendran, Miao Liu, Sarath Chandar</li>
<li>for: 这个论文的目的是研究 Cooperative Multi-agent Reinforcement Learning（MARL）算法中的零批合作（ZSC）能力，以及如何提高这种能力以适应复杂的任务和变化的环境。</li>
<li>methods: 该论文使用了一种基于 Hanabi 游戏的实验框架，并定义了一个新的 metric called adaptation regret，用于衡量 MARL 算法的适应能力。</li>
<li>results: 实验结果显示，当将 ZSC 算法与不同的学习方法训练的 Agent 结合时，state-of-the-art ZSC 算法的性能很差，并且需要 millions of interaction samples 来适应这些新的合作伙伴。此外，研究发现，通过调整不同的 hyper-parameter 和设计选择，可以提高 Hanabi 算法的适应能力。<details>
<summary>Abstract</summary>
Cooperative Multi-agent Reinforcement Learning (MARL) algorithms with Zero-Shot Coordination (ZSC) have gained significant attention in recent years. ZSC refers to the ability of agents to coordinate zero-shot (without additional interaction experience) with independently trained agents. While ZSC is crucial for cooperative MARL agents, it might not be possible for complex tasks and changing environments. Agents also need to adapt and improve their performance with minimal interaction with other agents. In this work, we show empirically that state-of-the-art ZSC algorithms have poor performance when paired with agents trained with different learning methods, and they require millions of interaction samples to adapt to these new partners. To investigate this issue, we formally defined a framework based on a popular cooperative multi-agent game called Hanabi to evaluate the adaptability of MARL methods. In particular, we created a diverse set of pre-trained agents and defined a new metric called adaptation regret that measures the agent's ability to efficiently adapt and improve its coordination performance when paired with some held-out pool of partners on top of its ZSC performance. After evaluating several SOTA algorithms using our framework, our experiments reveal that naive Independent Q-Learning (IQL) agents in most cases adapt as quickly as the SOTA ZSC algorithm Off-Belief Learning (OBL). This finding raises an interesting research question: How to design MARL algorithms with high ZSC performance and capability of fast adaptation to unseen partners. As a first step, we studied the role of different hyper-parameters and design choices on the adaptability of current MARL algorithms. Our experiments show that two categories of hyper-parameters controlling the training data diversity and optimization process have a significant impact on the adaptability of Hanabi agents.
</details>
<details>
<summary>摘要</summary>
合作多智能 reinforcement learning（MARL）算法 WITH Zero-Shot Coordination（ZSC）在 recent 年份中受到了关注。 ZSC 指的是无需额外交互经验的 AGENTS 之间的协调。 虽然 ZSC 对协作 MARL 代理人来说非常重要，但在复杂任务和变化环境下可能无法实现。 AGENTS 也需要通过最小化交互amples 来改进其性能。 在这项工作中，我们通过实验表明，当将最新的 ZSC 算法与另外的学习方法训练的 AGENTS 结合时，其性能很差。 为了解释这个问题，我们形式地定义了一个基于流行的合作多智能游戏 Hanabi 的框架，用于评估 MARL 方法的适应性。 具体来说，我们创建了一组多样化的预训练 AGENTS，并定义了一个新的指标called adaptation regret，用于衡量 AGENTS 在与其他另外的合作伙伴交互时的快速适应和改进协调性能。 经过我们的实验，我们发现，在 Hanabi 游戏中，简单的独立 Q-学习（IQL） AGENTS 在大多数情况下可以与 SOTA ZSC 算法 Off-Belief Learning（OBL）相当快地适应。 这一发现提出了一个有趣的研究问题：如何设计 MARL 算法，具有高度的 ZSC 性能和适应不visible 合作伙伴的能力。 作为一个第一步，我们研究了现有 MARL 算法中的不同超参数和设计选择对 Hanabi 代理人的适应性有多大的影响。 我们的实验表明，控制训练数据多样性和优化过程的两类超参数具有重要的影响。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Uncertainty-Guided-Model-Selection-for-Data-Driven-PDE-Discovery"><a href="#Adaptive-Uncertainty-Guided-Model-Selection-for-Data-Driven-PDE-Discovery" class="headerlink" title="Adaptive Uncertainty-Guided Model Selection for Data-Driven PDE Discovery"></a>Adaptive Uncertainty-Guided Model Selection for Data-Driven PDE Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10283">http://arxiv.org/abs/2308.10283</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pongpisit-thanasutives/ubic">https://github.com/pongpisit-thanasutives/ubic</a></li>
<li>paper_authors: Pongpisit Thanasutives, Takashi Morita, Masayuki Numao, Ken-ichi Fukui</li>
<li>For: 本研究提出了一种新的参数 adaptive uncertainty-penalized Bayesian information criterion (UBIC), 用于优先选择含有噪声的空间-时间观察数据的简洁参数化 differential equation (PDE)。* Methods: 本研究使用了一种physics-informed neural network learning的数据驱动方法来验证选择的 PDE 是否具有足够的准确性。* Results: 实验结果表明，UBIC 可以成功地选择真实的管理 PDE，并且发现了对噪声数据的适当处理可以提高模型选择的trade-off。Here’s the same information in English:* For: This study proposes a new parameter-adaptive uncertainty-penalized Bayesian information criterion (UBIC) to prioritize the parsimonious partial differential equation (PDE) that sufficiently governs noisy spatial-temporal observed data with few reliable terms.* Methods: The study uses a physics-informed neural network learning approach to validate the selected PDE flexibly against the other discovered PDE.* Results: Experimental results show that UBIC can successfully select the true governing PDE, and reveal an interesting effect of denoising the observed data on improving the trade-off between the BIC score and model complexity.<details>
<summary>Abstract</summary>
We propose a new parameter-adaptive uncertainty-penalized Bayesian information criterion (UBIC) to prioritize the parsimonious partial differential equation (PDE) that sufficiently governs noisy spatial-temporal observed data with few reliable terms. Since the naive use of the BIC for model selection has been known to yield an undesirable overfitted PDE, the UBIC penalizes the found PDE not only by its complexity but also the quantified uncertainty, derived from the model supports' coefficient of variation in a probabilistic view. We also introduce physics-informed neural network learning as a simulation-based approach to further validate the selected PDE flexibly against the other discovered PDE. Numerical results affirm the successful application of the UBIC in identifying the true governing PDE. Additionally, we reveal an interesting effect of denoising the observed data on improving the trade-off between the BIC score and model complexity. Code is available at https://github.com/Pongpisit-Thanasutives/UBIC.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的参数适应不确定性加权 bayesian信息条件（UBIC），用于优先选择含有噪声的空间-时间观测数据中的简洁partial differential equation（PDE）。由于直接使用BIC来进行模型选择可能会导致不想要的过拟合PDE，因此UBIC不仅对发现的PDE进行复杂度惩罚，还对其进行量化的不确定性惩罚，从概率视角来看。我们还引入了物理学习神经网络，作为一种基于实验的方法，以验证选择的PDE的可行性。numerical results表明，UBIC成功地实现了选择真实的导导PDE。此外，我们发现了对观测数据进行降噪有助于改善模型复杂度和BIC分数之间的交互效应。代码可以在https://github.com/Pongpisit-Thanasutives/UBIC上获取。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Spatiotemporal-Traffic-Prediction-through-Urban-Human-Activity-Analysis"><a href="#Enhancing-Spatiotemporal-Traffic-Prediction-through-Urban-Human-Activity-Analysis" class="headerlink" title="Enhancing Spatiotemporal Traffic Prediction through Urban Human Activity Analysis"></a>Enhancing Spatiotemporal Traffic Prediction through Urban Human Activity Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10282">http://arxiv.org/abs/2308.10282</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/suminhan/traffic-uagcrntf">https://github.com/suminhan/traffic-uagcrntf</a></li>
<li>paper_authors: Sumin Han, Youngjun Park, Minji Lee, Jisun An, Dongman Lee</li>
<li>for: 提高城市交通预测精度，帮助确保公民的安全和便利。</li>
<li>methods: 基于图 convolution deep learning 算法，利用人员活动频率数据从国家家庭旅行调查来增强推理 causal 关系 между活动和交通模式。</li>
<li>results: 对比传统的深度学习模型，该方法实现了状态之 искусственный智能模型，无需增加计算负担。<details>
<summary>Abstract</summary>
Traffic prediction is one of the key elements to ensure the safety and convenience of citizens. Existing traffic prediction models primarily focus on deep learning architectures to capture spatial and temporal correlation. They often overlook the underlying nature of traffic. Specifically, the sensor networks in most traffic datasets do not accurately represent the actual road network exploited by vehicles, failing to provide insights into the traffic patterns in urban activities. To overcome these limitations, we propose an improved traffic prediction method based on graph convolution deep learning algorithms. We leverage human activity frequency data from National Household Travel Survey to enhance the inference capability of a causal relationship between activity and traffic patterns. Despite making minimal modifications to the conventional graph convolutional recurrent networks and graph convolutional transformer architectures, our approach achieves state-of-the-art performance without introducing excessive computational overhead.
</details>
<details>
<summary>摘要</summary>
traffic prediction 是一个关键的元素，以确保公民的安全和便利。现有的交通预测模型主要采用深度学习架构，以捕捉空间和时间相关性。它们经常忽略交通的本质。具体来说，交通数据集中的感知网络不准确地表示实际行驶的道路网络，失去了对交通模式的探索。为了解决这些限制，我们提出了基于图конvolution深度学习算法的改进交通预测方法。我们利用国家家庭旅行调查中的人类活动频率数据，以提高 causal 关系 между活动和交通模式的推理能力。虽然我们对传统的图 convolutional recurrent networks 和图 convolutional transformer 架构进行了最小的修改，但我们的方法可以在计算开销不增加的情况下达到状态的最佳性能。
</details></li>
</ul>
<hr>
<h2 id="The-DKU-DUKEECE-System-for-the-Manipulation-Region-Location-Task-of-ADD-2023"><a href="#The-DKU-DUKEECE-System-for-the-Manipulation-Region-Location-Task-of-ADD-2023" class="headerlink" title="The DKU-DUKEECE System for the Manipulation Region Location Task of ADD 2023"></a>The DKU-DUKEECE System for the Manipulation Region Location Task of ADD 2023</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10281">http://arxiv.org/abs/2308.10281</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zexin Cai, Weiqing Wang, Yikang Wang, Ming Li</li>
<li>for: 本文描述了我们在 Audio Deepfake Detection Challenge (ADD 2023) 中的 Track 2 系统，它的目的是识别受修改的音频段。</li>
<li>methods: 我们的方法包括使用多个检测系统来确定受修改的区域和确定其真实性。我们训练并结合了两个帧级系统：一个用于边界检测，另一个用于深伪检测。此外，我们还使用了专门使用真实数据训练的 VAE 模型来确定音频剪辑的真实性。</li>
<li>results: 通过将这三个系统融合，我们的topperforming解决方案在 ADD 挑战中取得了82.23% 的句子准确率和60.66%的 F1 分数，最终的 ADD 分数为0.6713，在 Track 2 中获得了第一名。<details>
<summary>Abstract</summary>
This paper introduces our system designed for Track 2, which focuses on locating manipulated regions, in the second Audio Deepfake Detection Challenge (ADD 2023). Our approach involves the utilization of multiple detection systems to identify splicing regions and determine their authenticity. Specifically, we train and integrate two frame-level systems: one for boundary detection and the other for deepfake detection. Additionally, we employ a third VAE model trained exclusively on genuine data to determine the authenticity of a given audio clip. Through the fusion of these three systems, our top-performing solution for the ADD challenge achieves an impressive 82.23% sentence accuracy and an F1 score of 60.66%. This results in a final ADD score of 0.6713, securing the first rank in Track 2 of ADD 2023.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了我们为Track 2设计的系统，专注于寻找操作区域（splicing regions），在第二届音频深圳检测比赛（ADD 2023）中获得第一名。我们的方法包括多个检测系统的结合，以确定声音片断的真实性。具体来说，我们训练并集成了两个帧级别系统：一个用于边界检测，另一个用于深圳检测。此外，我们还使用专门用于真实数据训练的VAE模型，以确定声音片断的真实性。通过这三种系统的融合，我们在ADD挑战中获得了82.23%的句子准确率和60.66%的F1分数，最终得分为0.6713，在Track 2中排名第一。
</details></li>
</ul>
<hr>
<h2 id="GPFL-Simultaneously-Learning-Global-and-Personalized-Feature-Information-for-Personalized-Federated-Learning"><a href="#GPFL-Simultaneously-Learning-Global-and-Personalized-Feature-Information-for-Personalized-Federated-Learning" class="headerlink" title="GPFL: Simultaneously Learning Global and Personalized Feature Information for Personalized Federated Learning"></a>GPFL: Simultaneously Learning Global and Personalized Feature Information for Personalized Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10279">http://arxiv.org/abs/2308.10279</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianqing Zhang, Yang Hua, Hao Wang, Tao Song, Zhengui Xue, Ruhui Ma, Jian Cao, Haibing Guan</li>
<li>for: 这个论文主要是为了解决 Federated Learning (FL) 中的个人化特征提取问题，提出了一种新的个人化 Federated Learning (pFL) 方法，以实现在多客户端上同时学习全局和个人特征信息。</li>
<li>methods: 该方法使用了一种新的特征提取技术，可以同时学习全局和个人特征信息，并在多客户端上进行协同学习。</li>
<li>results: 在六个数据集上进行了三种统计上不同的实验，证明了 GPFL 在效果、可扩展性、公平性、稳定性和隐私方面比十种现有方法更优。此外，GPFL 还可以避免过拟合并超过基eline的提升。<details>
<summary>Abstract</summary>
Federated Learning (FL) is popular for its privacy-preserving and collaborative learning capabilities. Recently, personalized FL (pFL) has received attention for its ability to address statistical heterogeneity and achieve personalization in FL. However, from the perspective of feature extraction, most existing pFL methods only focus on extracting global or personalized feature information during local training, which fails to meet the collaborative learning and personalization goals of pFL. To address this, we propose a new pFL method, named GPFL, to simultaneously learn global and personalized feature information on each client. We conduct extensive experiments on six datasets in three statistically heterogeneous settings and show the superiority of GPFL over ten state-of-the-art methods regarding effectiveness, scalability, fairness, stability, and privacy. Besides, GPFL mitigates overfitting and outperforms the baselines by up to 8.99% in accuracy.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 是由于其隐私保护和合作学习能力而受欢迎的。最近，个性化 federated learning (pFL) 已经受到关注，因为它可以 Address statistical heterogeneity 和实现个性化。然而，从特征提取的角度来看，大多数现有的 pFL 方法只是在本地训练中提取全局或个性化特征信息，这不符合 pFL 的协作学习和个性化目标。为解决这个问题，我们提出了一种新的 pFL 方法，名为 GPFL，它可以在每个客户端同时学习全局和个性化特征信息。我们在六个数据集上进行了三种 statistically heterogeneous 的实验，并证明 GPFL 在效果、可扩展性、公平性、稳定性和隐私方面超过了十个现有方法。此外，GPFL 可以 Mitigate overfitting 并超过基eline 的性能。
</details></li>
</ul>
<hr>
<h2 id="Minimalist-Traffic-Prediction-Linear-Layer-Is-All-You-Need"><a href="#Minimalist-Traffic-Prediction-Linear-Layer-Is-All-You-Need" class="headerlink" title="Minimalist Traffic Prediction: Linear Layer Is All You Need"></a>Minimalist Traffic Prediction: Linear Layer Is All You Need</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10276">http://arxiv.org/abs/2308.10276</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wenyingduan/STLinear">https://github.com/wenyingduan/STLinear</a></li>
<li>paper_authors: Wenying Duan, Hong Rao, Wei Huang, Xiaoxi He</li>
<li>for: 这篇论文是为了解决智能交通系统（ITS）和智能城市的发展中的交通预测问题而写的。</li>
<li>methods: 本论文提出了三个解决方案：节点嵌入方法、时间序列分解和周期学习。它还介绍了一种名为 STLinear 的简单化模型架构，它在计算复杂性和计算负担方面具有明显的优势。</li>
<li>results: 实验表明，STLinear 能够与其他领先的 STGNN 模型匹配或超越其精度，但具有明显的计算复杂性和计算负担减少（相比于2023年的状态艺术 STGNN 基eline，MACs每个epoch减少了超过95%）。<details>
<summary>Abstract</summary>
Traffic prediction is essential for the progression of Intelligent Transportation Systems (ITS) and the vision of smart cities. While Spatial-Temporal Graph Neural Networks (STGNNs) have shown promise in this domain by leveraging Graph Neural Networks (GNNs) integrated with either RNNs or Transformers, they present challenges such as computational complexity, gradient issues, and resource-intensiveness. This paper addresses these challenges, advocating for three main solutions: a node-embedding approach, time series decomposition, and periodicity learning. We introduce STLinear, a minimalist model architecture designed for optimized efficiency and performance. Unlike traditional STGNNs, STlinear operates fully locally, avoiding inter-node data exchanges, and relies exclusively on linear layers, drastically cutting computational demands. Our empirical studies on real-world datasets confirm STLinear's prowess, matching or exceeding the accuracy of leading STGNNs, but with significantly reduced complexity and computation overhead (more than 95% reduction in MACs per epoch compared to state-of-the-art STGNN baseline published in 2023). In summary, STLinear emerges as a potent, efficient alternative to conventional STGNNs, with profound implications for the future of ITS and smart city initiatives.
</details>
<details>
<summary>摘要</summary>
traffic 预测是智能交通系统（ITS）的核心和智能城市的视野。而 spatial-temporal graph neural networks（STGNNs）在这个领域表现了承诺，通过结合图神经网络（GNNs）和 either RNNs 或 Transformers 来预测交通流。然而，STGNNs 还存在一些挑战，如计算复杂性、梯度问题和资源占用性。这篇文章解决了这些挑战，提出三个主要解决方案：节点嵌入方法、时间序列分解和周期学习。我们介绍了 STLinear，一种最佳化的模型建立，与传统的 STGNNs 不同，STLinear 完全地地方处理，不需要 между节点数据交换，并且仅仅使用线性层，减少了计算需求。我们对实际数据集进行了实验，确认 STLinear 的强大性，与状态之前的 STGNNs 准确性相当或超过，但计算负担减少了超过 95%。总之，STLinear  emerges 为智能交通系统和智能城市initiatives的强大、高效的代替方案。
</details></li>
</ul>
<hr>
<h2 id="SBSM-Pro-Support-Bio-sequence-Machine-for-Proteins"><a href="#SBSM-Pro-Support-Bio-sequence-Machine-for-Proteins" class="headerlink" title="SBSM-Pro: Support Bio-sequence Machine for Proteins"></a>SBSM-Pro: Support Bio-sequence Machine for Proteins</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10275">http://arxiv.org/abs/2308.10275</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wyzbio/support-bio-sequence-machine">https://github.com/wyzbio/support-bio-sequence-machine</a></li>
<li>paper_authors: Yizheng Wang, Yixiao Zhai, Yijie Ding, Quan Zou</li>
<li>for: 本研究旨在提出一种特制 для生物序列分类的支持机器学习模型（SBSM-Pro），帮助指导生物实验和应用。</li>
<li>methods: 该模型从原始序列开始，根据蛋白质物理化学性质进行氨基酸分组，并使用序列对 alignment 测量蛋白质之间的相似性。它采用了一种新的 MKL 方法，将不同类型的信息集成，使用支持向量机器学习进行分类预测。</li>
<li>results: 研究结果表明，SBSM-Pro 在 10 个数据集中表现出色，在蛋白质功能预测和后转录修饰方面进行了正确的识别。这项研究不仅代表了生物序列分类领域的国际前沿，还开创了新的方向，为生物序列分类平台的开发做出了重要贡献。<details>
<summary>Abstract</summary>
Proteins play a pivotal role in biological systems. The use of machine learning algorithms for protein classification can assist and even guide biological experiments, offering crucial insights for biotechnological applications. We propose a support bio-sequence machine for proteins, a model specifically designed for biological sequence classification. This model starts with raw sequences and groups amino acids based on their physicochemical properties. It incorporates sequence alignment to measure the similarities between proteins and uses a novel MKL approach to integrate various types of information, utilizing support vector machines for classification prediction. The results indicate that our model demonstrates commendable performance across 10 datasets in terms of the identification of protein function and posttranslational modification. This research not only showcases state-of-the-art work in protein classification but also paves the way for new directions in this domain, representing a beneficial endeavour in the development of platforms tailored for biological sequence classification. SBSM-Pro is available for access at http://lab.malab.cn/soft/SBSM-Pro/.
</details>
<details>
<summary>摘要</summary>
生物系统中，蛋白质扮演着关键角色。使用机器学习算法进行蛋白质分类可以帮助和指导生物实验，提供生物技术应用中关键的发现。我们提出了一种专门为蛋白质分类设计的生物序列机器学习模型（SBSM-Pro）。这个模型从原始序列开始，根据蛋白质物理化学性质分组氨基酸。它利用序列对alignment测量蛋白质之间的相似性，并采用一种新的MKL方法集成不同类型的信息，使用支持向量机进行分类预测。结果表明，我们的模型在10个数据集中表现出色地预测蛋白质功能和后转化 modify。这项研究不仅代表了蛋白质分类领域的 estado-of-the-art，还开拓了新的发展方向，代表了一项有利的生物序列分类平台开发的努力。SBSM-Pro可以在http://lab.malab.cn/soft/SBSM-Pro/上下载。
</details></li>
</ul>
<hr>
<h2 id="An-alternative-to-SVM-Method-for-Data-Classification"><a href="#An-alternative-to-SVM-Method-for-Data-Classification" class="headerlink" title="An alternative to SVM Method for Data Classification"></a>An alternative to SVM Method for Data Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11579">http://arxiv.org/abs/2308.11579</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/himanshub1007/Alzhimers-Disease-Prediction-Using-Deep-learning">https://github.com/himanshub1007/Alzhimers-Disease-Prediction-Using-Deep-learning</a></li>
<li>paper_authors: Lakhdar Remaki</li>
<li>for: 这篇论文是为了提出一种新的分类方法，以解决支持向量机（SVM）的一些缺点。</li>
<li>methods: 该方法使用最小距离到优化的子空间，以确定分类结果。</li>
<li>results: 研究发现，新方法与支持向量机（SVM）的性能相似，但具有轻量级的优势，如减少计算时间、避免优化过程失败、扩展到多类分类、处理不均衡类型和动态分类等问题。<details>
<summary>Abstract</summary>
Support vector machine (SVM), is a popular kernel method for data classification that demonstrated its efficiency for a large range of practical applications. The method suffers, however, from some weaknesses including; time processing, risk of failure of the optimization process for high dimension cases, generalization to multi-classes, unbalanced classes, and dynamic classification. In this paper an alternative method is proposed having a similar performance, with a sensitive improvement of the aforementioned shortcomings. The new method is based on a minimum distance to optimal subspaces containing the mapped original classes.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Turning-Waste-into-Wealth-Leveraging-Low-Quality-Samples-for-Enhancing-Continuous-Conditional-Generative-Adversarial-Networks"><a href="#Turning-Waste-into-Wealth-Leveraging-Low-Quality-Samples-for-Enhancing-Continuous-Conditional-Generative-Adversarial-Networks" class="headerlink" title="Turning Waste into Wealth: Leveraging Low-Quality Samples for Enhancing Continuous Conditional Generative Adversarial Networks"></a>Turning Waste into Wealth: Leveraging Low-Quality Samples for Enhancing Continuous Conditional Generative Adversarial Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10273">http://arxiv.org/abs/2308.10273</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xin Ding, Yongwei Wang, Zuheng Xu<br>for: 这个论文旨在提高Continuous Conditional Generative Adversarial Networks（CcGANs）中的生成模型，使其能够基于连续数值变量（称为回归标签）进行生成。methods: 这个论文提出了一种新的Negative Data Augmentation（NDA）方法，称为Dual-NDA，以解决CcGANs中的问题，即生成低质量的假图像。Dual-NDA使用了两种类型的负样本：来自预训练CcGAN的视觉不真实图像和 manipulate 真实图像的标签。results: 实验表明，Dual-NDA可以提高CcGANs中生成图像的视觉准确性和标签一致性，而且可以超越当前的状态艺术Conditional GANs和扩散模型，达到新的高水平性能。<details>
<summary>Abstract</summary>
Continuous Conditional Generative Adversarial Networks (CcGANs) enable generative modeling conditional on continuous scalar variables (termed regression labels). However, they can produce subpar fake images due to limited training data. Although Negative Data Augmentation (NDA) effectively enhances unconditional and class-conditional GANs by introducing anomalies into real training images, guiding the GANs away from low-quality outputs, its impact on CcGANs is limited, as it fails to replicate negative samples that may occur during the CcGAN sampling. We present a novel NDA approach called Dual-NDA specifically tailored for CcGANs to address this problem. Dual-NDA employs two types of negative samples: visually unrealistic images generated from a pre-trained CcGAN and label-inconsistent images created by manipulating real images' labels. Leveraging these negative samples, we introduce a novel discriminator objective alongside a modified CcGAN training algorithm. Empirical analysis on UTKFace and Steering Angle reveals that Dual-NDA consistently enhances the visual fidelity and label consistency of fake images generated by CcGANs, exhibiting a substantial performance gain over the vanilla NDA. Moreover, by applying Dual-NDA, CcGANs demonstrate a remarkable advancement beyond the capabilities of state-of-the-art conditional GANs and diffusion models, establishing a new pinnacle of performance.
</details>
<details>
<summary>摘要</summary>
Dual-NDA 使用了两种负样本：由预训练 CcGAN 生成的视觉不可能的图像，以及 manipulate 真实图像的标签以创造的 label-inconsistent 图像。我们利用这些负样本，引入了一种新的识别器目标 alongside 修改后 CcGAN 训练算法。我们的实验表明，使用 Dual-NDA，CcGANs 能够在 UTKFace 和 Steering Angle 上提高假图像的视觉准确性和标签一致性，并且表现出了明显的性能提升。此外，通过应用 Dual-NDA，CcGANs 能够超越现有的 conditional GANs 和扩散模型，达到新的高点性能。
</details></li>
</ul>
<hr>
<h2 id="Large-Transformers-are-Better-EEG-Learners"><a href="#Large-Transformers-are-Better-EEG-Learners" class="headerlink" title="Large Transformers are Better EEG Learners"></a>Large Transformers are Better EEG Learners</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11654">http://arxiv.org/abs/2308.11654</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bingxin Wang, Xiaowen Fu, Yuan Lan, Luchan Zhang, Yang Xiang</li>
<li>for: 这个论文主要针对的是如何将类型变数的电enzephalogram（EEG）资料转换为图像或文本格式，以便使用预训Transformer模型进行预测。</li>
<li>methods: 作者提出了一个名为AdaCE的专案，用于将EEG资料转换为图像或文本格式，并将这些格式与预训Transformer模型进行混合，以便进行预测。</li>
<li>results: 作者的实验结果显示，使用AdaCE模组可以将预训Transformer模型直接 fine-tune 为EEG预测任务，并 achieve state-of-the-art 性能在多种EEG预测任务上。例如，AdaCE在预训Swin-Transformer上 achieve 99.6%，即相对提高9.2%的精度。此外，作者还证明了，将更大的预训模型通过AdaCE进行 fine-tune 可以在EEG预测任务上 achieve better performance。<details>
<summary>Abstract</summary>
Pre-trained large transformer models have achieved remarkable performance in the fields of natural language processing and computer vision. Since the magnitude of available labeled electroencephalogram (EEG) data is much lower than that of text and image data, it is difficult for transformer models pre-trained from EEG to be developed as large as GPT-4 100T to fully unleash the potential of this architecture. In this paper, we show that transformers pre-trained from images as well as text can be directly fine-tuned for EEG-based prediction tasks. We design AdaCE, plug-and-play Adapters for Converting EEG data into image as well as text forms, to fine-tune pre-trained vision and language transformers. The proposed AdaCE module is highly effective for fine-tuning pre-trained transformers while achieving state-of-the-art performance on diverse EEG-based prediction tasks. For example, AdaCE on the pre-trained Swin-Transformer achieves 99.6%, an absolute improvement of 9.2%, on the EEG-decoding task of human activity recognition (UCI HAR). Furthermore, we empirically show that applying the proposed AdaCE to fine-tune larger pre-trained models can achieve better performance on EEG-based predicting tasks, indicating the potential of our adapters for even larger transformers. The plug-and-play AdaCE module can be applied to fine-tuning most of the popular pre-trained transformers on many other time-series data with multiple channels, not limited to EEG data and the models we use. Our code will be available at https://github.com/wangbxj1234/AdaCE.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>大型预训练变换器模型在自然语言处理和计算机视觉领域已经实现了很好的表现。由于电生成ogram（EEG）数据的数量相对较少，预训练后的变换器模型难以达到GPT-4 100T的规模，以全面发挥变换器模型的潜力。在这篇论文中，我们表明了从图像和文本预训练的变换器模型可以直接进行EEG数据适应。我们设计了AdaCE模块，它是一种用于将EEG数据转换为图像和文本形式的插件，以便适应预训练的视觉和语言变换器。我们的AdaCE模块非常有效地进行适应预训练后的变换器模型，并在多种EEG预测任务中达到了状态元的表现。例如，AdaCE在预训练Swin-Transformer上的EEG解码任务上达到了99.6%，相对于基线方法的9.2%提升。此外，我们还证明了在应用我们的AdaCE插件后，可以进行更大的预训练模型的精度调整，这表明了我们的插件在更大的变换器模型上的潜力。插件可以应用于大多数流行的预训练变换器模型上，并不限于EEG数据和我们所用的模型。我们的代码将在https://github.com/wangbxj1234/AdaCE上提供。
</details></li>
</ul>
<hr>
<h2 id="Towards-Synthesizing-Datasets-for-IEEE-802-1-Time-sensitive-Networking"><a href="#Towards-Synthesizing-Datasets-for-IEEE-802-1-Time-sensitive-Networking" class="headerlink" title="Towards Synthesizing Datasets for IEEE 802.1 Time-sensitive Networking"></a>Towards Synthesizing Datasets for IEEE 802.1 Time-sensitive Networking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10255">http://arxiv.org/abs/2308.10255</a></li>
<li>repo_url: None</li>
<li>paper_authors: Doğanalp Ergenç, Nurefşan Sertbaş Bülbül, Lisa Maile, Anna Arestova, Mathias Fischer</li>
<li>for: 本文旨在探讨IEEE 802.1时间敏感网络协议在不同的传统系统中的应用，以及使用人工智能和机器学习模型来开发高级配置和维护方法。</li>
<li>methods: 本文提出了一种使用人工智能和机器学习模型来开发高级配置和维护方法的方法，并分析了该方法的主要需求和可行设计。</li>
<li>results: 本文指出，为了推进IEEE 802.1时间敏感网络协议的研究，需要开发一些可靠的TSN数据集，以便训练人工智能和机器学习模型。<details>
<summary>Abstract</summary>
IEEE 802.1 Time-sensitive Networking (TSN) protocols have recently been proposed to replace legacy networking technologies across different mission-critical systems (MCSs). Design, configuration, and maintenance of TSN within MCSs require advanced methods to tackle the highly complex and interconnected nature of those systems. Accordingly, artificial intelligence (AI) and machine learning (ML) models are the most prominent enablers to develop such methods. However, they usually require a significant amount of data for model training, which is not easily accessible. This short paper aims to recapitulate the need for TSN datasets to flourish research on AI/ML-based techniques for TSN systems. Moreover, it analyzes the main requirements and alternative designs to build a TSN platform to synthesize realistic datasets.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="StableLLaVA-Enhanced-Visual-Instruction-Tuning-with-Synthesized-Image-Dialogue-Data"><a href="#StableLLaVA-Enhanced-Visual-Instruction-Tuning-with-Synthesized-Image-Dialogue-Data" class="headerlink" title="StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data"></a>StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10253">http://arxiv.org/abs/2308.10253</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/icoz69/stablellava">https://github.com/icoz69/stablellava</a></li>
<li>paper_authors: Yanda Li, Chi Zhang, Gang Yu, Zhibin Wang, Bin Fu, Guosheng Lin, Chunhua Shen, Ling Chen, Yunchao Wei</li>
<li>for: 这些论文的主要研究目标是开发一种能够有效地对应文本和视觉模式的大型自然语言模型（LLM），以便理解人类的指令。</li>
<li>methods: 我们提议一种新的数据收集方法，即同步生成图像和对话，以便对视觉指令进行调整。这种方法利用了生成模型的能力，将文本生成模型和对话生成模型结合起来，以生成多样化和可控的数据集。</li>
<li>results: 我们的研究包括对多个数据集进行了广泛的实验，使用开源的 LLAVA 模型作为我们提议的管道进行测试。我们的结果表明，我们的方法可以明显提高 LLM 的多种常见能力，包括图像生成、对话生成、问题回答、文本生成等。<details>
<summary>Abstract</summary>
The remarkable multimodal capabilities demonstrated by OpenAI's GPT-4 have sparked significant interest in the development of multimodal Large Language Models (LLMs). A primary research objective of such models is to align visual and textual modalities effectively while comprehending human instructions. Current methodologies often rely on annotations derived from benchmark datasets to construct image-dialogue datasets for training purposes, akin to instruction tuning in LLMs. However, these datasets often exhibit domain bias, potentially constraining the generative capabilities of the models. In an effort to mitigate these limitations, we propose a novel data collection methodology that synchronously synthesizes images and dialogues for visual instruction tuning. This approach harnesses the power of generative models, marrying the abilities of ChatGPT and text-to-image generative models to yield a diverse and controllable dataset with varied image content. This not only provides greater flexibility compared to existing methodologies but also significantly enhances several model capabilities. Our research includes comprehensive experiments conducted on various datasets using the open-source LLAVA model as a testbed for our proposed pipeline. Our results underscore marked enhancements across more than ten commonly assessed capabilities,
</details>
<details>
<summary>摘要</summary>
“OpenAI的GPT-4的多模式能力引起了很大的关注，推动了多模式大语言模型（LLM）的研发。这些模型的主要研究目标是在理解人类 instrucion 时，有效地对文本和视觉模式进行对应。现有的方法ologies  oft en rely on来自标准 benchmark dataset 的注释来构建图像对话集 для训练purpose，类似于 instruction tuning 在 LLM 中。然而，这些数据集经常受到领域偏见的影响，可能限制模型的生成能力。为了缓解这些限制，我们提出了一种新的数据采集方法，同时生成图像和对话，以便对视觉 instrucion 进行调整。这种方法利用了生成模型的能力，将文本生成模型和图像生成模型结合起来，生成了多样化和可控的数据集。这不仅提供了现有方法所不具备的灵活性，还显著提高了多种模型能力。我们的研究包括对多个数据集进行了全面的实验，使用开源的 LLAVA 模型作为我们提议的管道测试环境。我们的结果表明，我们的方法在多达十个常评价指标上具有明显的提升。”
</details></li>
</ul>
<hr>
<h2 id="Activation-Addition-Steering-Language-Models-Without-Optimization"><a href="#Activation-Addition-Steering-Language-Models-Without-Optimization" class="headerlink" title="Activation Addition: Steering Language Models Without Optimization"></a>Activation Addition: Steering Language Models Without Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10248">http://arxiv.org/abs/2308.10248</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alex Turner, Lisa Thiergart, David Udell, Gavin Leech, Ulisse Mini, Monte MacDiarmid</li>
<li>for: 这个论文旨在提出一种可靠地控制大型自然语言模型（LLM）的行为的方法。</li>
<li>methods: 论文使用激活工程（Activation Addition，ActAdd）方法，在推理时添加一个“导航向量”，通过自然语言来隐式地定义。</li>
<li>results: 论文在GPT-2上进行了OpenWebText和ConceptNet的测试，表明ActAdd方法可以在推理时控制输出的高级性质，并且不会影响模型的目标性能。此外，ActAdd方法比超visionfinetuning和人类反馈学习（RLHF）更加快速，需要更少的计算资源和实现努力，同时允许用户提供自然语言指令。<details>
<summary>Abstract</summary>
Reliably controlling the behavior of large language models (LLMs) is a pressing open problem. Existing methods include supervised finetuning, reinforcement learning from human feedback (RLHF), prompt engineering and guided decoding. We instead investigate activation engineering: modifying activations at inference time to predictably alter model behavior. In particular, we bias the forward pass with an added 'steering vector' implicitly specified through natural language.   Unlike past work which learned these steering vectors (Subramani, Suresh, and Peters 2022; Hernandez, Li, and Andreas 2023), our Activation Addition (ActAdd) method computes them by taking the activation differences that result from pairs of prompts. We demonstrate ActAdd on GPT-2 on OpenWebText and ConceptNet. Our inference-time approach yields control over high-level properties of output and preserves off-target model performance. It involves far less compute and implementation effort compared to finetuning or RLHF, allows users to provide natural language specifications, and its overhead scales naturally with model size.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）的可靠控制问题仍是一个开放的问题。现有的方法包括监督微调、人工反馈学习（RLHF）、提示工程和导航解码。我们则是活动工程：在推理时修改激活来预测性地改变模型行为。具体来说，我们在前进通过添加“导航向量”来隐式地指定自然语言中的批处理。与过去的工作不同（Subramani et al. 2022；Hernandez et al. 2023），我们的激活添加（ActAdd）方法不是学习这些导航向量，而是通过对提示对的激活差异来计算它们。我们在GPT-2上使用OpenWebText和ConceptNet进行了实验，并证明了ActAdd可以在推理时控制输出的高级性质，并且保持目标模型性能。这种在推理时进行的方法比微调或RLHF需要更少的计算和实现努力，允许用户提供自然语言规范，并且其开销随模型大小呈指数增长。
</details></li>
</ul>
<hr>
<h2 id="From-Global-to-Local-Multi-scale-Out-of-distribution-Detection"><a href="#From-Global-to-Local-Multi-scale-Out-of-distribution-Detection" class="headerlink" title="From Global to Local: Multi-scale Out-of-distribution Detection"></a>From Global to Local: Multi-scale Out-of-distribution Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10239">http://arxiv.org/abs/2308.10239</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jimzai/mode-ood">https://github.com/jimzai/mode-ood</a></li>
<li>paper_authors: Ji Zhang, Lianli Gao, Bingguang Hao, Hao Huang, Jingkuan Song, Hengtao Shen</li>
<li>for: 这个研究的目的是提高OD detection的精度，尤其是在遇到未知数据时。</li>
<li>methods: 这个方法使用了多个缩寸的方法，包括global visual information和local region details，以提高OD detection的精度。</li>
<li>results: 这个方法在多个benchmark上的表现比前一代方法好，具体的表现提高了19.24%在False Positive Rate和2.77%在AUC上。<details>
<summary>Abstract</summary>
Out-of-distribution (OOD) detection aims to detect "unknown" data whose labels have not been seen during the in-distribution (ID) training process. Recent progress in representation learning gives rise to distance-based OOD detection that recognizes inputs as ID/OOD according to their relative distances to the training data of ID classes. Previous approaches calculate pairwise distances relying only on global image representations, which can be sub-optimal as the inevitable background clutter and intra-class variation may drive image-level representations from the same ID class far apart in a given representation space. In this work, we overcome this challenge by proposing Multi-scale OOD DEtection (MODE), a first framework leveraging both global visual information and local region details of images to maximally benefit OOD detection. Specifically, we first find that existing models pretrained by off-the-shelf cross-entropy or contrastive losses are incompetent to capture valuable local representations for MODE, due to the scale-discrepancy between the ID training and OOD detection processes. To mitigate this issue and encourage locally discriminative representations in ID training, we propose Attention-based Local PropAgation (ALPA), a trainable objective that exploits a cross-attention mechanism to align and highlight the local regions of the target objects for pairwise examples. During test-time OOD detection, a Cross-Scale Decision (CSD) function is further devised on the most discriminative multi-scale representations to distinguish ID/OOD data more faithfully. We demonstrate the effectiveness and flexibility of MODE on several benchmarks -- on average, MODE outperforms the previous state-of-the-art by up to 19.24% in FPR, 2.77% in AUROC. Code is available at https://github.com/JimZAI/MODE-OOD.
</details>
<details>
<summary>摘要</summary>
OUT-OF-DISTRIBUTION (OOD) 检测的目标是检测“未知”数据，其标签在ID 训练过程中没有出现过。随着表征学学习的进步，距离基于 OOD 检测已经得到了广泛应用。然而，过去的方法通常基于全局图像表征，忽略了图像内部的细节信息，这可能会导致同一个ID类型的图像在给定的表征空间中被分化。在这种情况下，我们提出了一个新的框架，即多scale OOD 检测（MODE），它利用全局视觉信息和图像内部的局部区域特征来最大化 OOD 检测的效果。Specifically, we find that existing models pretrained by off-the-shelf cross-entropy or contrastive losses are incompetent to capture valuable local representations for MODE, due to the scale-discrepancy between the ID training and OOD detection processes. To mitigate this issue and encourage locally discriminative representations in ID training, we propose Attention-based Local PropAgation (ALPA), a trainable objective that exploits a cross-attention mechanism to align and highlight the local regions of the target objects for pairwise examples. During test-time OOD detection, a Cross-Scale Decision (CSD) function is further devised on the most discriminative multi-scale representations to distinguish ID/OOD data more faithfully. We demonstrate the effectiveness and flexibility of MODE on several benchmarks -- on average, MODE outperforms the previous state-of-the-art by up to 19.24% in FPR, 2.77% in AUROC. Code is available at https://github.com/JimZAI/MODE-OOD.
</details></li>
</ul>
<hr>
<h2 id="Thompson-Sampling-for-Real-Valued-Combinatorial-Pure-Exploration-of-Multi-Armed-Bandit"><a href="#Thompson-Sampling-for-Real-Valued-Combinatorial-Pure-Exploration-of-Multi-Armed-Bandit" class="headerlink" title="Thompson Sampling for Real-Valued Combinatorial Pure Exploration of Multi-Armed Bandit"></a>Thompson Sampling for Real-Valued Combinatorial Pure Exploration of Multi-Armed Bandit</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10238">http://arxiv.org/abs/2308.10238</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shintaro Nakamura, Masashi Sugiyama</li>
<li>for:  solve the real-valued combinatorial pure exploration of the multi-armed bandit (R-CPE-MAB) problem, which is to find the optimal action from a finite-sized real-valued action set with as few arm pulls as possible.</li>
<li>methods:  the Generalized Thompson Sampling Explore (GenTS-Explore) algorithm, which is the first algorithm that can work even when the size of the action set is exponentially large in d.</li>
<li>results:  a novel problem-dependent sample complexity lower bound of the R-CPE-MAB problem, and show that the GenTS-Explore algorithm achieves the optimal sample complexity up to a problem-dependent constant factor.Here are the three information in a more concise format, using bullet points:</li>
<li>for:  solve the R-CPE-MAB problem with a large action set.</li>
<li>methods:  GenTS-Explore algorithm.</li>
<li>results:  a novel problem-dependent sample complexity lower bound, and the GenTS-Explore algorithm achieves the optimal sample complexity up to a constant factor.<details>
<summary>Abstract</summary>
We study the real-valued combinatorial pure exploration of the multi-armed bandit (R-CPE-MAB) problem. In R-CPE-MAB, a player is given $d$ stochastic arms, and the reward of each arm $s\in\{1, \ldots, d\}$ follows an unknown distribution with mean $\mu_s$. In each time step, a player pulls a single arm and observes its reward. The player's goal is to identify the optimal \emph{action} $\boldsymbol{\pi}^{*} = \argmax_{\boldsymbol{\pi} \in \mathcal{A}} \boldsymbol{\mu}^{\top}\boldsymbol{\pi}$ from a finite-sized real-valued \emph{action set} $\mathcal{A}\subset \mathbb{R}^{d}$ with as few arm pulls as possible. Previous methods in the R-CPE-MAB assume that the size of the action set $\mathcal{A}$ is polynomial in $d$. We introduce an algorithm named the Generalized Thompson Sampling Explore (GenTS-Explore) algorithm, which is the first algorithm that can work even when the size of the action set is exponentially large in $d$. We also introduce a novel problem-dependent sample complexity lower bound of the R-CPE-MAB problem, and show that the GenTS-Explore algorithm achieves the optimal sample complexity up to a problem-dependent constant factor.
</details>
<details>
<summary>摘要</summary>
我们研究了实数值的可整合探索多臂枪手问题（R-CPE-MAB）。在R-CPE-MAB中，一个玩家被分配了$d$个随机臂，每个臂$s\in\{1, \ldots, d\}$的奖励follows一个未知分布的mean $\mu_s$。在每个时间步骤中，一个玩家抓一个臂并观察其奖励。玩家的目标是从一个封闭的实数值动作集$\mathcal{A}\subset \mathbb{R}^{d}$中选择最佳的动作 $\boldsymbol{\pi}^{*} = \argmax_{\boldsymbol{\pi} \in \mathcal{A}} \boldsymbol{\mu}^{\top}\boldsymbol{\pi}$，以最少的臂抓次数为目标。先前的R-CPE-MAB方法假设动作集$\mathcal{A}$的大小是对数函数的$d$。我们介绍了一个名为Generalized Thompson Sampling Explore（GenTS-Explore）算法，这是第一个可以在动作集$\mathcal{A}$的大小是指数增长的$d$时工作的算法。我们还介绍了一个问题内部依赖的样本缩减下限，并证明GenTS-Explore算法实现了问题内部依赖的样本缩减下限。
</details></li>
</ul>
<hr>
<h2 id="FedSIS-Federated-Split-Learning-with-Intermediate-Representation-Sampling-for-Privacy-preserving-Generalized-Face-Presentation-Attack-Detection"><a href="#FedSIS-Federated-Split-Learning-with-Intermediate-Representation-Sampling-for-Privacy-preserving-Generalized-Face-Presentation-Attack-Detection" class="headerlink" title="FedSIS: Federated Split Learning with Intermediate Representation Sampling for Privacy-preserving Generalized Face Presentation Attack Detection"></a>FedSIS: Federated Split Learning with Intermediate Representation Sampling for Privacy-preserving Generalized Face Presentation Attack Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10236">http://arxiv.org/abs/2308.10236</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/naiftt/fedsis">https://github.com/naiftt/fedsis</a></li>
<li>paper_authors: Naif Alkhunaizi, Koushik Srivatsan, Faris Almalik, Ibrahim Almakky, Karthik Nandakumar</li>
<li>for: 本研究旨在提高面部攻击检测算法（FacePAD）的通用性，解决现有算法的 Achilles heel 问题。</li>
<li>methods: 本研究提出了一种新的框架called Federated Split learning with Intermediate representation Sampling（FedSIS），combines federated learning（FL）和split learning，以保持隐私而实现预测通用性。</li>
<li>results: 研究表明，FedSIS 可以在不需要数据共享的情况下，达到面部攻击检测算法的状态之最游标性表现，并在未见过的频道上实现良好的泛化性。<details>
<summary>Abstract</summary>
Lack of generalization to unseen domains/attacks is the Achilles heel of most face presentation attack detection (FacePAD) algorithms. Existing attempts to enhance the generalizability of FacePAD solutions assume that data from multiple source domains are available with a single entity to enable centralized training. In practice, data from different source domains may be collected by diverse entities, who are often unable to share their data due to legal and privacy constraints. While collaborative learning paradigms such as federated learning (FL) can overcome this problem, standard FL methods are ill-suited for domain generalization because they struggle to surmount the twin challenges of handling non-iid client data distributions during training and generalizing to unseen domains during inference. In this work, a novel framework called Federated Split learning with Intermediate representation Sampling (FedSIS) is introduced for privacy-preserving domain generalization. In FedSIS, a hybrid Vision Transformer (ViT) architecture is learned using a combination of FL and split learning to achieve robustness against statistical heterogeneity in the client data distributions without any sharing of raw data (thereby preserving privacy). To further improve generalization to unseen domains, a novel feature augmentation strategy called intermediate representation sampling is employed, and discriminative information from intermediate blocks of a ViT is distilled using a shared adapter network. The FedSIS approach has been evaluated on two well-known benchmarks for cross-domain FacePAD to demonstrate that it is possible to achieve state-of-the-art generalization performance without data sharing. Code: https://github.com/Naiftt/FedSIS
</details>
<details>
<summary>摘要</summary>
缺乏泛化到未经见的领域/攻击是现有的面孔展示攻击检测（FacePAD）算法的 Achilles heel。现有的增强FacePAD解决方案假设有多个源领域的数据可以在单一实体上进行中央式训练。然而，在实践中，来自不同的源领域的数据可能是由不同的实体收集的，这些实体经常因为法律和隐私限制无法共享自己的数据。而合作学习 paradigm such as federated learning（FL）可以解决这个问题，但标准的FL方法在适应新的领域时存在两大挑战：处理非标一Client数据分布在训练中和在推断中适应未经见的领域。在这种情况下，一种名为 Federated Split learning with Intermediate representation Sampling（FedSIS）的新框架被引入，用于保护隐私的领域泛化。在 FedSIS 中，使用一种混合的 Vision Transformer（ViT）架构，通过 combining FL 和 split learning 来实现对Client数据分布的统计异质性的Robustness，而不需要 Client 数据的共享。为了进一步提高适应未经见的领域，一种名为 intermediate representation sampling 的新的特征增强策略被使用，并通过一个共享的 adapter 网络来浓缩出权威信息。FedSIS 方法在两个常用的 cross-domain FacePAD  benchmark 上进行了评估，并证明了可以在没有数据共享情况下实现状态码的泛化性能。代码：https://github.com/Naiftt/FedSIS
</details></li>
</ul>
<hr>
<h2 id="Karma-Adaptive-Video-Streaming-via-Causal-Sequence-Modeling"><a href="#Karma-Adaptive-Video-Streaming-via-Causal-Sequence-Modeling" class="headerlink" title="Karma: Adaptive Video Streaming via Causal Sequence Modeling"></a>Karma: Adaptive Video Streaming via Causal Sequence Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10230">http://arxiv.org/abs/2308.10230</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fcbw2012/Karma">https://github.com/fcbw2012/Karma</a></li>
<li>paper_authors: Bowei Xu, Hao Chen, Zhan Ma</li>
<li>For: This paper aims to improve the adaptive bitrate (ABR) decision-making process by utilizing causal sequence modeling to comprehend the interrelated causality among past observations, returns, and actions, and timely refining actions when deviations occur.* Methods: The proposed Karma algorithm uses a decision transformer to determine the next action based on a multi-dimensional time series of observations, returns, and actions, with the maximum cumulative future quality of experience (QoE) as an extended return signal.* Results: The paper demonstrates superior performance compared to existing state-of-the-art ABR algorithms, with an average QoE improvement ranging from 10.8% to 18.7% across diverse network conditions, and strong generalization capabilities under unseen networks in both simulations and real-world tests.Here is the text in Simplified Chinese:* For: 这篇论文目标是通过利用 causal sequence modeling 改善 adaptive bitrate (ABR) 决策过程，以便更好地理解过去观察、返回和行为之间的相互关系，并在偏差发生时及时更新行动。* Methods: 提议的 Karma 算法使用决策转换器来确定下一步行动，基于多维时间序列观察、返回和行动中的观察、返回和行动。* Results: 论文表明，相比现有的状态艺术 ABR 算法，Karma 算法在多种网络条件下表现出较高的平均Quality of Experience（QoE）提升，从10.8% 到18.7%。此外，Karma 算法在未见网络上也显示出了强大的泛化能力。<details>
<summary>Abstract</summary>
Optimal adaptive bitrate (ABR) decision depends on a comprehensive characterization of state transitions that involve interrelated modalities over time including environmental observations, returns, and actions. However, state-of-the-art learning-based ABR algorithms solely rely on past observations to decide the next action. This paradigm tends to cause a chain of deviations from optimal action when encountering unfamiliar observations, which consequently undermines the model generalization. This paper presents Karma, an ABR algorithm that utilizes causal sequence modeling to improve generalization by comprehending the interrelated causality among past observations, returns, and actions and timely refining action when deviation occurs. Unlike direct observation-to-action mapping, Karma recurrently maintains a multi-dimensional time series of observations, returns, and actions as input and employs causal sequence modeling via a decision transformer to determine the next action. In the input sequence, Karma uses the maximum cumulative future quality of experience (QoE) (a.k.a, QoE-to-go) as an extended return signal, which is periodically estimated based on current network conditions and playback status. We evaluate Karma through trace-driven simulations and real-world field tests, demonstrating superior performance compared to existing state-of-the-art ABR algorithms, with an average QoE improvement ranging from 10.8% to 18.7% across diverse network conditions. Furthermore, Karma exhibits strong generalization capabilities, showing leading performance under unseen networks in both simulations and real-world tests.
</details>
<details>
<summary>摘要</summary>
优化的适应比率（ABR）决策需要对状态转移进行全面的特征化，包括时间上的相关Modalities。然而，现有的学习基于ABR算法只是基于过去的观察来决定下一个动作。这种做法会导致对于不熟悉的观察而引起链式偏差，从而下降模型泛化。这篇论文提出了Karma算法，它利用 causal sequence modeling来提高泛化性，通过理解过去观察、返回和动作之间的相关 causality，并在偏差发生时进行时间 opportune 的修正。与直接观察到动作映射不同，Karma 使用循环维护一个多维时间序列，并使用决策变换器来确定下一个动作。在输入序列中，Karma 使用最大累积未来体验质量（QoE）作为延长返回信号，这些信号 periodically 根据当前网络conditions和播放状态来 estimating。我们通过跟踪驱动的 simulations 和实际场景测试评估了Karma，并证明它在不同的网络条件下表现出优于现有状态艺术ABR算法，QoE 提升平均值在10.8%到18.7%之间。此外，Karma 表现出了强大的泛化能力，在未看到的网络上仍然保持领先的表现。
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-powered-Combinatorial-Clock-Auction"><a href="#Machine-Learning-powered-Combinatorial-Clock-Auction" class="headerlink" title="Machine Learning-powered Combinatorial Clock Auction"></a>Machine Learning-powered Combinatorial Clock Auction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10226">http://arxiv.org/abs/2308.10226</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/marketdesignresearch/ml-cca">https://github.com/marketdesignresearch/ml-cca</a></li>
<li>paper_authors: Ermis Soumalias, Jakob Weissteiner, Jakob Heiss, Sven Seuken</li>
<li>for: 这篇论文关注了迭代 combinatorial 拍卖（ICA）的设计。ICA 中的主要挑战在于bundle空间随着物品数量的增加而 exponentiates。</li>
<li>methods: 这篇论文提出了一种基于机器学习（ML）的偏好拟合算法，以便从投标者那里获取最重要的信息。</li>
<li>results: 这篇论文的实验结果表明，相比 combinatorial clock auction（CCA），我们的 ML-based demand query mechanism在各个频谱拍卖领域中表现出了显著的高效性。它在一个较小的数量的拍卖轮次中达到了更高的效率，并且使用线性价格时可以达到了巨大的清算潜力。因此，这篇论文 bridge了研究和实践之间的差距，并提出了首个实用的 ML-powered ICA。<details>
<summary>Abstract</summary>
We study the design of iterative combinatorial auctions (ICAs). The main challenge in this domain is that the bundle space grows exponentially in the number of items. To address this, several papers have recently proposed machine learning (ML)-based preference elicitation algorithms that aim to elicit only the most important information from bidders. However, from a practical point of view, the main shortcoming of this prior work is that those designs elicit bidders' preferences via value queries (i.e., ``What is your value for the bundle $\{A,B\}$?''). In most real-world ICA domains, value queries are considered impractical, since they impose an unrealistically high cognitive burden on bidders, which is why they are not used in practice. In this paper, we address this shortcoming by designing an ML-powered combinatorial clock auction that elicits information from the bidders only via demand queries (i.e., ``At prices $p$, what is your most preferred bundle of items?''). We make two key technical contributions: First, we present a novel method for training an ML model on demand queries. Second, based on those trained ML models, we introduce an efficient method for determining the demand query with the highest clearing potential, for which we also provide a theoretical foundation. We experimentally evaluate our ML-based demand query mechanism in several spectrum auction domains and compare it against the most established real-world ICA: the combinatorial clock auction (CCA). Our mechanism significantly outperforms the CCA in terms of efficiency in all domains, it achieves higher efficiency in a significantly reduced number of rounds, and, using linear prices, it exhibits vastly higher clearing potential. Thus, with this paper we bridge the gap between research and practice and propose the first practical ML-powered ICA.
</details>
<details>
<summary>摘要</summary>
我们研究Iterative Combinatorial Auctions（ICA）的设计。ICA的主要挑战在于bundle空间随着物品数量的增加而增加 exponentially。为了解决这个问题，一些最近的论文已经提出了基于机器学习（ML）的偏好探索算法，以探索供应商对不同套件的偏好。然而，从实践的角度来看，这些设计都是通过值询问（i.e.,“What is your value for the bundle $\{A,B\}$?”）来探索供应商的偏好，这种方法在实际应用中被视为不实际。在这篇论文中，我们解决这个问题，通过设计一个基于ML的套件时钟拍卖，通过需求询问（i.e.,“At prices $p$, what is your most preferred bundle of items?”）来探索供应商的偏好。我们的研究做出了两个关键技术贡献：首先，我们提出了一种新的需求询问训练ML模型的方法；其次，基于这些训练的ML模型，我们引入了一种高效的需求询问找到最高清算潜力的方法，并提供了理论基础。我们在几个频谱拍卖领域进行了实验评估，与现有最具実际应用的ICA：套件时钟拍卖（CCA）进行比较。我们的机制在所有领域中都有着明显的高效性，在许多领域中，它在许多更少的轮数中达到了更高的高效性，并且使用线性价格，它的清算潜力是极大的。因此，这篇论文通过实践和理论的研究，提出了第一个实际应用的ML-Powered ICA。
</details></li>
</ul>
<hr>
<h2 id="Soft-Decomposed-Policy-Critic-Bridging-the-Gap-for-Effective-Continuous-Control-with-Discrete-RL"><a href="#Soft-Decomposed-Policy-Critic-Bridging-the-Gap-for-Effective-Continuous-Control-with-Discrete-RL" class="headerlink" title="Soft Decomposed Policy-Critic: Bridging the Gap for Effective Continuous Control with Discrete RL"></a>Soft Decomposed Policy-Critic: Bridging the Gap for Effective Continuous Control with Discrete RL</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10203">http://arxiv.org/abs/2308.10203</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yechen Zhang, Jian Sun, Gang Wang, Zhuo Li, Wei Chen</li>
<li>for: 解决连续控制问题中的维度爆炸问题</li>
<li>methods: combines soft RL和actor-critic技术，独立地对每个动作维度进行柔化，使用共享批处理网络来最大化柔化$Q$-函数</li>
<li>results: 在多种连续控制任务中，比如Mujoco的人工智能和Box2d的两脚行走器，实验结果表明我们提出的方法可以超越现有的连续RL算法表现。<details>
<summary>Abstract</summary>
Discrete reinforcement learning (RL) algorithms have demonstrated exceptional performance in solving sequential decision tasks with discrete action spaces, such as Atari games. However, their effectiveness is hindered when applied to continuous control problems due to the challenge of dimensional explosion. In this paper, we present the Soft Decomposed Policy-Critic (SDPC) architecture, which combines soft RL and actor-critic techniques with discrete RL methods to overcome this limitation. SDPC discretizes each action dimension independently and employs a shared critic network to maximize the soft $Q$-function. This novel approach enables SDPC to support two types of policies: decomposed actors that lead to the Soft Decomposed Actor-Critic (SDAC) algorithm, and decomposed $Q$-networks that generate Boltzmann soft exploration policies, resulting in the Soft Decomposed-Critic Q (SDCQ) algorithm. Through extensive experiments, we demonstrate that our proposed approach outperforms state-of-the-art continuous RL algorithms in a variety of continuous control tasks, including Mujoco's Humanoid and Box2d's BipedalWalker. These empirical results validate the effectiveness of the SDPC architecture in addressing the challenges associated with continuous control.
</details>
<details>
<summary>摘要</summary>
离散强化学习（RL）算法在解决顺序决策任务中的离散动作空间方面表现出色，如Atari游戏。然而，当应用于连续控制问题时，它们的效iveness受到维度爆炸的挑战。在这篇论文中，我们提出了软分解策略-批评（SDPC）架构，它将离散RL和演员-批评技术与离散RL方法相结合，以解决这一问题。SDPC独立地对每个动作维度进行分解，并使用共享批评网络来最大化软Q函数。这种新的方法使得SDPC支持两种策略：分解演员，导致Soft Decomposed Actor-Critic（SDAC）算法，以及分解Q网络，生成Boltzmann软探索策略，导致Soft Decomposed-Critic Q（SDCQ）算法。我们通过广泛的实验表明，我们提出的方法在多种连续控制任务中比州前的连续RL算法表现出色，包括Mujoco的人iform和Box2d的BipedalWalker。这些实验结果证明了SDPC架构在连续控制问题中的有效性。
</details></li>
</ul>
<hr>
<h2 id="Hiding-Backdoors-within-Event-Sequence-Data-via-Poisoning-Attacks"><a href="#Hiding-Backdoors-within-Event-Sequence-Data-via-Poisoning-Attacks" class="headerlink" title="Hiding Backdoors within Event Sequence Data via Poisoning Attacks"></a>Hiding Backdoors within Event Sequence Data via Poisoning Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10201">http://arxiv.org/abs/2308.10201</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elizaveta Kovtun, Alina Ermilova, Dmitry Berestnev, Alexey Zaytsev</li>
<li>for: 这个论文旨在描述如何在金融业中使用深度学习模型，同时解决这些模型受到恶意攻击的问题。</li>
<li>methods: 这个论文使用了一种名为“潜藏后门”的方法，通过在训练过程中插入一个隐藏的攻击点来引入攻击性质的模型。</li>
<li>results:  experiments 表明，这种方法可以在不同的 dataset、架构和模型组件上实现攻击，并且可以让模型在检测攻击时保持正常的功能。<details>
<summary>Abstract</summary>
The financial industry relies on deep learning models for making important decisions. This adoption brings new danger, as deep black-box models are known to be vulnerable to adversarial attacks. In computer vision, one can shape the output during inference by performing an adversarial attack called poisoning via introducing a backdoor into the model during training. For sequences of financial transactions of a customer, insertion of a backdoor is harder to perform, as models operate over a more complex discrete space of sequences, and systematic checks for insecurities occur. We provide a method to introduce concealed backdoors, creating vulnerabilities without altering their functionality for uncontaminated data. To achieve this, we replace a clean model with a poisoned one that is aware of the availability of a backdoor and utilize this knowledge. Our most difficult for uncovering attacks include either additional supervised detection step of poisoned data activated during the test or well-hidden model weight modifications. The experimental study provides insights into how these effects vary across different datasets, architectures, and model components. Alternative methods and baselines, such as distillation-type regularization, are also explored but found to be less efficient. Conducted on three open transaction datasets and architectures, including LSTM, CNN, and Transformer, our findings not only illuminate the vulnerabilities in contemporary models but also can drive the construction of more robust systems.
</details>
<details>
<summary>摘要</summary>
Financial industry 使用深度学习模型作重要决策，这种采用带来新的危险，深度黑盒模型容易受到抗击攻击。在计算机视觉中，可以在推理过程中Shape the outputby performing an adversarial attack called poisoning via introducing a backdoor into the model during training. However, for sequences of financial transactions of a customer, insertion of a backdoor is harder to perform, as models operate over a more complex discrete space of sequences, and systematic checks for insecurities occur. We provide a method to introduce concealed backdoors, creating vulnerabilities without altering their functionality for uncontaminated data. To achieve this, we replace a clean model with a poisoned one that is aware of the availability of a backdoor and utilize this knowledge. Our most difficult for uncovering attacks include either additional supervised detection step of poisoned data activated during the test or well-hidden model weight modifications. The experimental study provides insights into how these effects vary across different datasets, architectures, and model components. Alternative methods and baselines, such as distillation-type regularization, are also explored but found to be less efficient. Conducted on three open transaction datasets and architectures, including LSTM, CNN, and Transformer, our findings not only illuminate the vulnerabilities in contemporary models but also can drive the construction of more robust systems.Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. Traditional Chinese is used in Taiwan, Hong Kong, and other regions.
</details></li>
</ul>
<hr>
<h2 id="Deep-Reinforcement-Learning-for-Artificial-Upwelling-Energy-Management"><a href="#Deep-Reinforcement-Learning-for-Artificial-Upwelling-Energy-Management" class="headerlink" title="Deep Reinforcement Learning for Artificial Upwelling Energy Management"></a>Deep Reinforcement Learning for Artificial Upwelling Energy Management</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10199">http://arxiv.org/abs/2308.10199</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiyuan Zhang, Wei Fan</li>
<li>for: 这篇论文旨在探讨人工上升（Artificial Upwelling，AU）技术是否能够提高海洋抑 carbon 储存，以及AU 系统如何 efficiently 运行。</li>
<li>methods: 该论文提出了一种使用深度强化学习（Deep Reinforcement Learning，DRL）算法来开发高效的 AU 系统操作策略。</li>
<li>results: 通过大量的 simulations，该论文表明了 DRL 算法比传统的规则型approaches和其他 DRL 算法更有效率地减少能源浪费，同时确保 AU 系统的稳定和高效操作。<details>
<summary>Abstract</summary>
The potential of artificial upwelling (AU) as a means of lifting nutrient-rich bottom water to the surface, stimulating seaweed growth, and consequently enhancing ocean carbon sequestration, has been gaining increasing attention in recent years. This has led to the development of the first solar-powered and air-lifted AU system (AUS) in China. However, efficient scheduling of air injection systems remains a crucial challenge in operating AUS, as it holds the potential to significantly improve system efficiency. Conventional approaches based on rules or models are often impractical due to the complex and heterogeneous nature of the marine environment and its associated disturbances. To address this challenge, we propose a novel energy management approach that utilizes deep reinforcement learning (DRL) algorithm to develop efficient strategies for operating AUS. Through extensive simulations, we evaluate the performance of our algorithm and demonstrate its superior effectiveness over traditional rule-based approaches and other DRL algorithms in reducing energy wastage while ensuring the stable and efficient operation of AUS. Our findings suggest that a DRL-based approach offers a promising way for improving the efficiency of AUS and enhancing the sustainability of seaweed cultivation and carbon sequestration in the ocean.
</details>
<details>
<summary>摘要</summary>
人工升浮（AU）的潜在作用是将有营养物质的底水升到表层，促进海藻生长，从而提高海洋碳储存量。在最近几年中，AU在中国已经开始研发首个太阳能驱动、空气升降系统（AUS）。然而，AU系统的有效调度仍然是一个主要挑战，因为它们的 marine 环境复杂且多样化，以及其关联的干扰。为 Addressing this challenge, we propose a novel energy management approach that utilizes deep reinforcement learning (DRL) algorithm to develop efficient strategies for operating AUS. Through extensive simulations, we evaluate the performance of our algorithm and demonstrate its superior effectiveness over traditional rule-based approaches and other DRL algorithms in reducing energy wastage while ensuring the stable and efficient operation of AUS. Our findings suggest that a DRL-based approach offers a promising way for improving the efficiency of AUS and enhancing the sustainability of seaweed cultivation and carbon sequestration in the ocean.
</details></li>
</ul>
<hr>
<h2 id="ProSpire-Proactive-Spatial-Prediction-of-Radio-Environment-Using-Deep-Learning"><a href="#ProSpire-Proactive-Spatial-Prediction-of-Radio-Environment-Using-Deep-Learning" class="headerlink" title="ProSpire: Proactive Spatial Prediction of Radio Environment Using Deep Learning"></a>ProSpire: Proactive Spatial Prediction of Radio Environment Using Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10193">http://arxiv.org/abs/2308.10193</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shamik Sarkar, Dongning Guo, Danijela Cabric</li>
<li>for: 这个研究旨在帮助无线网络中的输送器预测电磁环境，以提高无线网络的多样性和可靠性。</li>
<li>methods: 这个研究使用了一个新的、受到监督学习的框架，即ProSpire，以实现对输送器的探测。ProSpire 利用了一个叫做 RSSu-net 的深度学习方法，实现了对输送器的预测。</li>
<li>results: 这个研究获得了reasonable的实验结果，其中预测误差为5 dB的平均绝对误差，与其他相关方法相当。此外，ProSpire 可以实现对输送器的探测，使其可以在97%的机会下不会导致干扰。相比之下，RSSu-net 的性能比其他相似方法更好，增加了19%的可能性。<details>
<summary>Abstract</summary>
Spatial prediction of the radio propagation environment of a transmitter can assist and improve various aspects of wireless networks. The majority of research in this domain can be categorized as 'reactive' spatial prediction, where the predictions are made based on a small set of measurements from an active transmitter whose radio environment is to be predicted. Emerging spectrum-sharing paradigms would benefit from 'proactive' spatial prediction of the radio environment, where the spatial predictions must be done for a transmitter for which no measurement has been collected.   This paper proposes a novel, supervised deep learning-based framework, ProSpire, that enables spectrum sharing by leveraging the idea of proactive spatial prediction. We carefully address several challenges in ProSpire, such as designing a framework that conveniently collects training data for learning, performing the predictions in a fast manner, enabling operations without an area map, and ensuring that the predictions do not lead to undesired interference. ProSpire relies on the crowdsourcing of transmitters and receivers during their normal operations to address some of the aforementioned challenges. The core component of ProSpire is a deep learning-based image-to-image translation method, which we call RSSu-net. We generate several diverse datasets using ray tracing software and numerically evaluate ProSpire. Our evaluations show that RSSu-net performs reasonably well in terms of signal strength prediction, 5 dB mean absolute error, which is comparable to the average error of other relevant methods. Importantly, due to the merits of RSSu-net, ProSpire creates proactive boundaries around transmitters such that they can be activated with 97% probability of not causing interference. In this regard, the performance of RSSu-net is 19% better than that of other comparable methods.
</details>
<details>
<summary>摘要</summary>
通过预测广播环境，可以提高无线网络的多种方面。大多数研究在这个领域是“反应式”的预测，基于活动发送器的 радио环境进行预测。然而，新兴的spectrum-sharing paradigms需要“积极”的预测，预测发送器没有收集过数据。这篇论文提出了一种新的、深度学习基于的框架，名为ProSpire，它利用了积极预测的想法。我们仔细解决了一些挑战，例如如何便捷地收集训练数据，在快速方式进行预测，不需要地图，以及预测不导致不必要的干扰。ProSpire通过在正常运行时 solicit 发送器和接收器来解决一些上述挑战。核心组件是一种基于图像至图像翻译的深度学习方法，我们称之为RSSu-net。我们使用 ray tracing 软件生成了多种多样的数据集，并 numerically 评估了ProSpire。我们的评估结果表明，RSSu-net在信号强度预测方面表现reasonably well，相比其他相关方法的平均误差为5 dB。这意味着ProSpire可以创建积极的边界，使 transmitter 有97%的概率不会导致干扰。这种性能比其他相似方法高出19%。
</details></li>
</ul>
<hr>
<h2 id="Mimicking-To-Dominate-Imitation-Learning-Strategies-for-Success-in-Multiagent-Competitive-Games"><a href="#Mimicking-To-Dominate-Imitation-Learning-Strategies-for-Success-in-Multiagent-Competitive-Games" class="headerlink" title="Mimicking To Dominate: Imitation Learning Strategies for Success in Multiagent Competitive Games"></a>Mimicking To Dominate: Imitation Learning Strategies for Success in Multiagent Competitive Games</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10188">http://arxiv.org/abs/2308.10188</a></li>
<li>repo_url: None</li>
<li>paper_authors: The Viet Bui, Tien Mai, Thanh Hong Nguyen</li>
<li>for: 本研究旨在Addressing the challenges of training agents in multi-agent competitive games, particularly in mitigating uncertainties in game dynamics.</li>
<li>methods: 我们提出了一种新的多体学习模型，可以预测对手的下一步行动，基于隐藏的对手行动和本地观察。此外，我们还提出了一种新的多体强化学习算法，可以结合我们的模型和策略训练进行一起训练。</li>
<li>results: 我们在三个复杂的游戏环境中进行了广泛的实验，包括SMACv2。实验结果表明，我们的方法可以比现有的多体RL算法 achieve superior performance.<details>
<summary>Abstract</summary>
Training agents in multi-agent competitive games presents significant challenges due to their intricate nature. These challenges are exacerbated by dynamics influenced not only by the environment but also by opponents' strategies. Existing methods often struggle with slow convergence and instability. To address this, we harness the potential of imitation learning to comprehend and anticipate opponents' behavior, aiming to mitigate uncertainties with respect to the game dynamics. Our key contributions include: (i) a new multi-agent imitation learning model for predicting next moves of the opponents -- our model works with hidden opponents' actions and local observations; (ii) a new multi-agent reinforcement learning algorithm that combines our imitation learning model and policy training into one single training process; and (iii) extensive experiments in three challenging game environments, including an advanced version of the Star-Craft multi-agent challenge (i.e., SMACv2). Experimental results show that our approach achieves superior performance compared to existing state-of-the-art multi-agent RL algorithms.
</details>
<details>
<summary>摘要</summary>
training 多个代理人在多代理人竞争游戏中存在巨大的挑战，这些挑战受到环境以及对手策略的影响。现有方法经常受到慢性和不稳定性的影响。为了解决这些问题，我们利用仿制学来理解和预测对手的行为，以降低与游戏动力学有关的不确定性。我们的关键贡献包括：(i) 一种新的多个代理人仿制学模型，用于预测对手的下一步行动，该模型可以处理隐藏的对手行动和地方观察。(ii) 一种新的多个代理人 reinforcement learning 算法，将我们的仿制学模型和策略训练集成在一起，以实现单一的训练过程。(iii) 在三个复杂的游戏环境中进行了广泛的实验，包括 Star-Craft 多代理人挑战（SMACv2）的高级版本。实验结果表明，我们的方法可以与现有的多代理人 RL 算法相比，实现更高的性能。
</details></li>
</ul>
<hr>
<h2 id="Quantization-based-Optimization-with-Perspective-of-Quantum-Mechanics"><a href="#Quantization-based-Optimization-with-Perspective-of-Quantum-Mechanics" class="headerlink" title="Quantization-based Optimization with Perspective of Quantum Mechanics"></a>Quantization-based Optimization with Perspective of Quantum Mechanics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11594">http://arxiv.org/abs/2308.11594</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinwuk Seok, Changsik Cho</li>
<li>for: 本研究旨在探讨量子逻辑如何应用于全球优化问题中，提供一种新的研究框架。</li>
<li>methods: 本文使用量子谱分法对全球优化问题进行分析，揭示了量子力学中允许全球优化的特性。</li>
<li>results: 实验结果表明，量子谱分法中的 Tunneling 效应可以使得找到局部最优点的问题逃脱局部最优点，并且这种效应与量子力学基础的全球优化问题相同。<details>
<summary>Abstract</summary>
Statistical and stochastic analysis based on thermodynamics has been the main analysis framework for stochastic global optimization. Recently, appearing quantum annealing or quantum tunneling algorithm for global optimization, we require a new researching framework for global optimization algorithms. In this paper, we provide the analysis for quantization-based optimization based on the Schr\"odinger equation to reveal what property in quantum mechanics enables global optimization. We present that the tunneling effect derived by the Schr\"odinger equation in quantization-based optimization enables to escape of a local minimum. Additionally, we confirm that this tunneling effect is the same property included in quantum mechanics-based global optimization. Experiments with standard multi-modal benchmark functions represent that the proposed analysis is valid.
</details>
<details>
<summary>摘要</summary>
基于热力学的统计学和随机分析已经是全球优化的主要分析框架。在最近，量子气体或量子隧道算法在全球优化中出现，我们需要一个新的研究框架来探讨全球优化算法。在这篇论文中，我们提供了量子化基于Schrödinger方程的优化分析，以探索量子力学中允许全球优化的属性。我们发现，通过Schrödinger方程中的隧道效应，可以在量子化基于优化中突破本地最小值。此外，我们证明这种隧道效应与量子力学基于全球优化中的同一性。通过对标准多模式函数的实验，我们证明了我们的分析的有效性。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Client-Drift-in-Federated-Learning-A-Logit-Perspective"><a href="#Rethinking-Client-Drift-in-Federated-Learning-A-Logit-Perspective" class="headerlink" title="Rethinking Client Drift in Federated Learning: A Logit Perspective"></a>Rethinking Client Drift in Federated Learning: A Logit Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10162">http://arxiv.org/abs/2308.10162</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunlu Yan, Chun-Mei Feng, Mang Ye, Wangmeng Zuo, Ping Li, Rick Siow Mong Goh, Lei Zhu, C. L. Philip Chen</li>
<li>for: 这个研究是为了解决 Federated Learning (FL) 中 Client Drift 问题，并提高 FL 的性能。</li>
<li>methods: 本研究使用了一个新的 Class prototype Similarity Distillation (FedCSD) 算法，将本地和全球模型进行对预。 FedCSD 不仅将全球知识转移到本地客户端，因为一个未熟悉的全球模型无法提供可靠的知识，即类别相似性信息。而是将本地征推与全球原型之间的相似性用于对预。</li>
<li>results: 实验结果显示，FedCSD 在不同的多 клиєн端数据设定下表现更好，并且可以避免 Catastrophic Forgetting 问题。<details>
<summary>Abstract</summary>
Federated Learning (FL) enables multiple clients to collaboratively learn in a distributed way, allowing for privacy protection. However, the real-world non-IID data will lead to client drift which degrades the performance of FL. Interestingly, we find that the difference in logits between the local and global models increases as the model is continuously updated, thus seriously deteriorating FL performance. This is mainly due to catastrophic forgetting caused by data heterogeneity between clients. To alleviate this problem, we propose a new algorithm, named FedCSD, a Class prototype Similarity Distillation in a federated framework to align the local and global models. FedCSD does not simply transfer global knowledge to local clients, as an undertrained global model cannot provide reliable knowledge, i.e., class similarity information, and its wrong soft labels will mislead the optimization of local models. Concretely, FedCSD introduces a class prototype similarity distillation to align the local logits with the refined global logits that are weighted by the similarity between local logits and the global prototype. To enhance the quality of global logits, FedCSD adopts an adaptive mask to filter out the terrible soft labels of the global models, thereby preventing them to mislead local optimization. Extensive experiments demonstrate the superiority of our method over the state-of-the-art federated learning approaches in various heterogeneous settings. The source code will be released.
</details>
<details>
<summary>摘要</summary>
受欢迎的 Federated Learning (FL) 技术允许多个客户端共同学习，以保护隐私。然而，在实际世界中，客户端数据不够一致，导致客户端漂移，从而下降 FL 性能。我们发现，在不断更新模型时，本地和全球模型之间的差异在逐渐增加，从而严重降低 FL 性能。这主要是由于客户端数据不同性导致的慢速忘记。为解决这问题，我们提出了一种新的算法，即 FedCSD，它是一种基于联合类prototype similarity distillation的联邦框架，用于对本地和全球模型进行对齐。FedCSD不仅将全球知识传递给本地客户端，因为一个受训练不充分的全球模型无法提供可靠的类 similarity 信息，而且其错误的软标签会mislead本地优化。具体来说，FedCSD 引入一种类 prototype similarity distillation，用于对本地征标与全球 проtotypes 之间的类 similarity进行对齐。为提高全球征标的质量，FedCSD 采用了一种适应性掩模，以过滤全球模型的差异化软标签，从而避免它们对本地优化产生负面影响。我们的实验表明，FedCSD 在不同的异质设置下表现出优于当前 state-of-the-art 联邦学习方法。我们将源代码发布。
</details></li>
</ul>
<hr>
<h2 id="Resource-Adaptive-Newton’s-Method-for-Distributed-Learning"><a href="#Resource-Adaptive-Newton’s-Method-for-Distributed-Learning" class="headerlink" title="Resource-Adaptive Newton’s Method for Distributed Learning"></a>Resource-Adaptive Newton’s Method for Distributed Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10154">http://arxiv.org/abs/2308.10154</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuzhen Chen, Yuan Yuan, Youming Tao, Zhipeng Cai, Dongxiao Yu</li>
<li>for: 这篇论文是用于探讨分布式随机优化方法，特别是以新顿方法为基础，以提高性能。</li>
<li>methods: 这篇论文使用了一种名为RANL的新型和有效的算法，它利用了简单的希腊矩来初始化，并通过自适应训练区域的分配来解决新顿方法的问题。</li>
<li>results: 这篇论文的结果显示了RANL算法在数据不均匀和训练资料组件复杂的情况下仍然能够实现线性的快速减退。此外，RANL算法还能够自动适应可用资源，以确保高效率。<details>
<summary>Abstract</summary>
Distributed stochastic optimization methods based on Newton's method offer significant advantages over first-order methods by leveraging curvature information for improved performance. However, the practical applicability of Newton's method is hindered in large-scale and heterogeneous learning environments due to challenges such as high computation and communication costs associated with the Hessian matrix, sub-model diversity, staleness in training, and data heterogeneity. To address these challenges, this paper introduces a novel and efficient algorithm called RANL, which overcomes the limitations of Newton's method by employing a simple Hessian initialization and adaptive assignments of training regions. The algorithm demonstrates impressive convergence properties, which are rigorously analyzed under standard assumptions in stochastic optimization. The theoretical analysis establishes that RANL achieves a linear convergence rate while effectively adapting to available resources and maintaining high efficiency. Unlike traditional first-order methods, RANL exhibits remarkable independence from the condition number of the problem and eliminates the need for complex parameter tuning. These advantages make RANL a promising approach for distributed stochastic optimization in practical scenarios.
</details>
<details>
<summary>摘要</summary>
新类的分布式数据估计方法，基于牛顿法，可以实现更好的性能，但是它实际应用在大规模和多标的学习环境中受到一些挑战，例如牛顿矩阵的计算和通信成本高昂，模型多标的问题，训练过程中的偏预设问题，数据多标性。为了解决这些挑战，本文提出了一个新的和高效的算法，叫做RANL，它可以超越牛顿法的限制，通过简单的牛顿矩阵初始化和自适应的训练区域分配。这个算法在标准假设下进行了严谨的分析，证明了RANL可以在线性几何中实现快速的渐近稳定。不同于传统的首项方法，RANL不受问题的条件数值影响，并且不需要复杂的参数调整。这些优点使RANL成为实际应用中的分布式数据估计方法。
</details></li>
</ul>
<hr>
<h2 id="Global-Warming-In-Ghana’s-Major-Cities-Based-on-Statistical-Analysis-of-NASA’s-POWER-Over-3-Decades"><a href="#Global-Warming-In-Ghana’s-Major-Cities-Based-on-Statistical-Analysis-of-NASA’s-POWER-Over-3-Decades" class="headerlink" title="Global Warming In Ghana’s Major Cities Based on Statistical Analysis of NASA’s POWER Over 3-Decades"></a>Global Warming In Ghana’s Major Cities Based on Statistical Analysis of NASA’s POWER Over 3-Decades</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10909">http://arxiv.org/abs/2308.10909</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joshua Attih</li>
<li>for: 这项研究旨在 investigate Ghana 四大城市的长期温度趋势，以了解地方气候变化的影响和政策制定的依据。</li>
<li>methods: 该研究使用 NASA 的 Prediction of Worldwide Energy Resource (POWER) 数据，并使用统计分析和 XGBoost 机器学习算法来预测温度变化。 地表温度 profiling 图表从 RSLab 平台生成，以提高准确性。</li>
<li>results: 研究发现各城市都有本地气候变化趋势，特别是工业化的 Accra 显示明显的升高趋势。 涉及人口因素不显著。 XGBoost 模型的低 Root Mean Square Error (RMSE) 分数表明其能够准确地捕捉温度模式。 Wa  unexpectedly 的平均温度最高。 预计2023 中期 Accra 的平均温度为 27.86℃，Kumasi 为 27.15℃， Kete-Krachi 为 29.39℃， Wa 为 30.76℃。这些结果可以帮助气候变化策略的制定和实施。<details>
<summary>Abstract</summary>
Global warming's impact on high temperatures in various parts of the world has raised concerns. This study investigates long-term temperature trends in four major Ghanaian cities representing distinct climatic zones. Using NASA's Prediction of Worldwide Energy Resource (POWER) data, statistical analyses assess local climate warming and its implications. Linear regression trend analysis and eXtreme Gradient Boosting (XGBoost) machine learning predict temperature variations. Land Surface Temperature (LST) profile maps generated from the RSLab platform enhance accuracy. Results reveal local warming trends, particularly in industrialized Accra. Demographic factors aren't significant. XGBoost model's low Root Mean Square Error (RMSE) scores demonstrate effectiveness in capturing temperature patterns. Wa unexpectedly has the highest mean temperature. Estimated mean temperatures for mid-2023 are: Accra 27.86{\deg}C, Kumasi 27.15{\deg}C, Kete-Krachi 29.39{\deg}C, and Wa 30.76{\deg}C. These findings improve understanding of local climate warming for policymakers and communities, aiding climate change strategies.
</details>
<details>
<summary>摘要</summary>
全球气候变化对各地高温的影响已引发了关注。这项研究对四个加纳城市进行了长期气温趋势分析，这些城市代表了不同的气候区。使用NASA的Prediction of Worldwide Energy Resource（POWER）数据，统计分析评估了地方气候变暖的影响。线性回归方法和极限梯度提升（XGBoost）机器学习方法预测温度变化。RSLab平台生成的土地表面温度（LST）profile图表提高了准确性。结果显示了地方气候变暖趋势，特别是工业化的阿克拉。人口因素没有显著影响。XGBoost模型的低根据平方误差（RMSE）得分表明其能够准确捕捉温度模式。意外地，华有最高的平均温度。预计2023年中的温度为：阿克拉27.86℃，库马西27.15℃，别克拉29.39℃，和华30.76℃。这些发现可以帮助气候变化策略的制定和社区的决策。
</details></li>
</ul>
<hr>
<h2 id="OCHID-Fi-Occlusion-Robust-Hand-Pose-Estimation-in-3D-via-RF-Vision"><a href="#OCHID-Fi-Occlusion-Robust-Hand-Pose-Estimation-in-3D-via-RF-Vision" class="headerlink" title="OCHID-Fi: Occlusion-Robust Hand Pose Estimation in 3D via RF-Vision"></a>OCHID-Fi: Occlusion-Robust Hand Pose Estimation in 3D via RF-Vision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10146">http://arxiv.org/abs/2308.10146</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shujie Zhang, Tianyue Zheng, Zhe Chen, Jingzhi Hu, Abdelwahed Khamis, Jiajun Liu, Jun Luo<br>for: occluded hand pose estimationmethods: RF-vision and adversarial learningresults: comparable accuracy to CM-HPE under normal conditions, maintains accuracy in occluded scenarios, generalizable to new domains.Here’s the simplified Chinese text:for:  occluded手势识别methods: RF-vision和对抗学习results: 与CM-HPE在正常情况下具有相同的准确率，在遮盖情况下保持准确率，并能在新领域中普适应用.<details>
<summary>Abstract</summary>
Hand Pose Estimation (HPE) is crucial to many applications, but conventional cameras-based CM-HPE methods are completely subject to Line-of-Sight (LoS), as cameras cannot capture occluded objects. In this paper, we propose to exploit Radio-Frequency-Vision (RF-vision) capable of bypassing obstacles for achieving occluded HPE, and we introduce OCHID-Fi as the first RF-HPE method with 3D pose estimation capability. OCHID-Fi employs wideband RF sensors widely available on smart devices (e.g., iPhones) to probe 3D human hand pose and extract their skeletons behind obstacles. To overcome the challenge in labeling RF imaging given its human incomprehensible nature, OCHID-Fi employs a cross-modality and cross-domain training process. It uses a pre-trained CM-HPE network and a synchronized CM/RF dataset, to guide the training of its complex-valued RF-HPE network under LoS conditions. It further transfers knowledge learned from labeled LoS domain to unlabeled occluded domain via adversarial learning, enabling OCHID-Fi to generalize to unseen occluded scenarios. Experimental results demonstrate the superiority of OCHID-Fi: it achieves comparable accuracy to CM-HPE under normal conditions while maintaining such accuracy even in occluded scenarios, with empirical evidence for its generalizability to new domains.
</details>
<details>
<summary>摘要</summary>
手势识别（HPE）是许多应用程序的关键，但传统的相机基于CM-HPE方法是完全依赖于直线视野（LoS），因为相机无法捕捉遮盖物体。在这篇论文中，我们提议利用Radio-Frequency-Vision（RF-vision）来绕过障碍物实现遮盖物体HPE，并介绍了OCHID-Fi作为首个RF-HPE方法，具有3D手势 pose estimation能力。OCHID-Fi利用了广泛可用的智能设备（如iPhone）上的宽频RF传感器来探测3D人手势 pose和其骨架，并在障碍物下实现了高精度的手势识别。为了解决RF图像标注的挑战，OCHID-Fi采用了交叉模式和交叉领域训练过程。它使用了预训练的CM-HPE网络和同步CM/RF数据集，以导引其复杂的RF-HPE网络在LoS条件下进行训练。它还通过对LoS频谱频谱中的标注进行反向传播学习，使OCHID-Fi能够在未看到障碍物的情况下泛化到新领域。实验结果表明，OCHID-Fi具有较高的精度和泛化能力，可以在正常情况下与CM-HPE具有相同的精度，而在障碍物下仍然保持高精度，并且在新领域中进行泛化。
</details></li>
</ul>
<hr>
<h2 id="Wasserstein-Geodesic-Generator-for-Conditional-Distributions"><a href="#Wasserstein-Geodesic-Generator-for-Conditional-Distributions" class="headerlink" title="Wasserstein Geodesic Generator for Conditional Distributions"></a>Wasserstein Geodesic Generator for Conditional Distributions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10145">http://arxiv.org/abs/2308.10145</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kyg0910/wasserstein-geodesic-generator-for-conditional-distributions">https://github.com/kyg0910/wasserstein-geodesic-generator-for-conditional-distributions</a></li>
<li>paper_authors: Young-geun Kim, Kyungbok Lee, Youngwon Choi, Joong-Ho Won, Myunghee Cho Paik</li>
<li>for: 这篇论文是用于研究如何获得高品质的条件生成。</li>
<li>methods: 论文使用估计条件分布的方法，包括 derive 一个可诠释的上界 bound 来定义条件分布，并使用 optimal transport 理论来设计一个名为 Wasserstein geodesic generator 的新型条件生成器。</li>
<li>results: 实验结果显示，提案的方法可以高效地学习条件分布，并可以生成高品质的条件生成。<details>
<summary>Abstract</summary>
Generating samples given a specific label requires estimating conditional distributions. We derive a tractable upper bound of the Wasserstein distance between conditional distributions to lay the theoretical groundwork to learn conditional distributions. Based on this result, we propose a novel conditional generation algorithm where conditional distributions are fully characterized by a metric space defined by a statistical distance. We employ optimal transport theory to propose the Wasserstein geodesic generator, a new conditional generator that learns the Wasserstein geodesic. The proposed method learns both conditional distributions for observed domains and optimal transport maps between them. The conditional distributions given unobserved intermediate domains are on the Wasserstein geodesic between conditional distributions given two observed domain labels. Experiments on face images with light conditions as domain labels demonstrate the efficacy of the proposed method.
</details>
<details>
<summary>摘要</summary>
<<SYS>计算样本 Given Specific标签需要估算conditional Distributions。我们得出了可观察 Wasserstein distance的Upper bound，以准备 theoretically learn conditional Distributions。 Based on this result, we propose a novel conditional generation algorithm, where conditional distributions are fully characterized by a metric space defined by a statistical distance. We employ optimal transport theory to propose the Wasserstein geodesic generator, a new conditional generator that learns the Wasserstein geodesic. The proposed method learns both conditional distributions for observed domains and optimal transport maps between them. The conditional distributions given unobserved intermediate domains are on the Wasserstein geodesic between conditional distributions given two observed domain labels. Experiments on face images with light conditions as domain labels demonstrate the efficacy of the proposed method. tranlation notes:* "conditional distributions" translated as "条件分布"* "Wasserstein distance" translated as "沃斯坦距离"* "metric space" translated as "度量空间"* "optimal transport theory" translated as "最优运输理论"* "Wasserstein geodesic" translated as "沃斯坦曲线"* "conditional generator" translated as "条件生成器"* "observed domains" translated as "观察Domain"* "unobserved intermediate domains" translated as "未观察中间Domain"* "light conditions" translated as "照明条件"Please note that the translation is in Simplified Chinese, and the word order may be different from the original text.
</details></li>
</ul>
<hr>
<h2 id="ExpeL-LLM-Agents-Are-Experiential-Learners"><a href="#ExpeL-LLM-Agents-Are-Experiential-Learners" class="headerlink" title="ExpeL: LLM Agents Are Experiential Learners"></a>ExpeL: LLM Agents Are Experiential Learners</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10144">http://arxiv.org/abs/2308.10144</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Andrewzh112/ExpeL">https://github.com/Andrewzh112/ExpeL</a></li>
<li>paper_authors: Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, Gao Huang</li>
<li>for: 这篇论文的目的是提出一种新的语言模型学习方法，以便在做决策任务时不需要进行参数更新。</li>
<li>methods: 该方法使用自然语言从训练任务中收集经验，并通过自动提取知识来做出 Informed 决策。</li>
<li>results: 该方法在实验中表现出了Robust 的学习效果，表明随着经验的积累，模型的性能会不断提高。<details>
<summary>Abstract</summary>
The recent surge in research interest in applying large language models (LLMs) to decision-making tasks has flourished by leveraging the extensive world knowledge embedded in LLMs. While there is a growing demand to tailor LLMs for custom decision-making tasks, finetuning them for specific tasks is resource-intensive and may diminish the model's generalization capabilities. Moreover, state-of-the-art language models like GPT-4 and Claude are primarily accessible through API calls, with their parametric weights remaining proprietary and unavailable to the public. This scenario emphasizes the growing need for new methodologies that allow learning from agent experiences without requiring parametric updates. To address these problems, we introduce the Experiential Learning (ExpeL) agent. Our agent autonomously gathers experiences and extracts knowledge using natural language from a collection of training tasks. At inference, the agent recalls its extracted insights and past experiences to make informed decisions. Our empirical results highlight the robust learning efficacy of the ExpeL agent, indicating a consistent enhancement in its performance as it accumulates experiences. We further explore the emerging capabilities and transfer learning potential of the ExpeL agent through qualitative observations and additional experiments.
</details>
<details>
<summary>摘要</summary>
To address these issues, we propose the Experiential Learning (ExpeL) agent. Our agent autonomously gathers experiences and extracts knowledge using natural language from a collection of training tasks. At inference, the agent recalls its extracted insights and past experiences to make informed decisions. Our empirical results show that the ExpeL agent exhibits robust learning efficacy, with its performance consistently improving as it accumulates experiences. We also explore the emerging capabilities and transfer learning potential of the ExpeL agent through qualitative observations and additional experiments.
</details></li>
</ul>
<hr>
<h2 id="A-Review-on-Objective-Driven-Artificial-Intelligence"><a href="#A-Review-on-Objective-Driven-Artificial-Intelligence" class="headerlink" title="A Review on Objective-Driven Artificial Intelligence"></a>A Review on Objective-Driven Artificial Intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10135">http://arxiv.org/abs/2308.10135</a></li>
<li>repo_url: None</li>
<li>paper_authors: Apoorv Singh</li>
<li>for: 本文旨在探讨人工智能（AI）技术目前的缺陷，以及如何通过层次规划和能量基、隐变量方法等方法来减少人机智能差距。</li>
<li>methods: 本文主要使用层次规划、能量基、隐变量方法等方法来探讨人工智能技术的发展和应用。</li>
<li>results: 本文指出，现有的AI技术尚未能够准确地理解人类的语言和情感表达，而层次规划和能量基、隐变量方法等方法可以帮助人工智能技术更好地理解人类的语言和情感表达。<details>
<summary>Abstract</summary>
While advancing rapidly, Artificial Intelligence still falls short of human intelligence in several key aspects due to inherent limitations in current AI technologies and our understanding of cognition. Humans have an innate ability to understand context, nuances, and subtle cues in communication, which allows us to comprehend jokes, sarcasm, and metaphors. Machines struggle to interpret such contextual information accurately. Humans possess a vast repository of common-sense knowledge that helps us make logical inferences and predictions about the world. Machines lack this innate understanding and often struggle with making sense of situations that humans find trivial. In this article, we review the prospective Machine Intelligence candidates, a review from Prof. Yann LeCun, and other work that can help close this gap between human and machine intelligence. Specifically, we talk about what's lacking with the current AI techniques such as supervised learning, reinforcement learning, self-supervised learning, etc. Then we show how Hierarchical planning-based approaches can help us close that gap and deep-dive into energy-based, latent-variable methods and Joint embedding predictive architecture methods.
</details>
<details>
<summary>摘要</summary>
artifical intelligence 在快速发展中，但仍然缺乏人类智能的一些关键方面，这主要归结于当前的 AI 技术和我们认知神经科学的限制。人类有内置的理解上下文、涵义和微妙提示的能力，使得我们能够理解讽刺、讽刺和 метаFOR。机器则很难准确地理解这些上下文信息。人类拥有庞大的通用智能知识，帮助我们做出逻辑推理和世界上的预测。机器缺乏这种内置的理解，经常陷入人类轻视的情况。在这篇文章中，我们评论了当前的机器智能候选人，包括Prof. Yann LeCun的评论以及其他工作，以帮助关闭人类和机器智能之间的差距。我们讨论了当前 AI 技术的缺陷，如监督学习、奖励学习、自监学习等。然后，我们介绍了层次规划基础的方法，可以帮助我们关闭这个差距。我们还深入探讨了能量基础、隐变量方法和联合嵌入预测架构。
</details></li>
</ul>
<hr>
<h2 id="AutoReP-Automatic-ReLU-Replacement-for-Fast-Private-Network-Inference"><a href="#AutoReP-Automatic-ReLU-Replacement-for-Fast-Private-Network-Inference" class="headerlink" title="AutoReP: Automatic ReLU Replacement for Fast Private Network Inference"></a>AutoReP: Automatic ReLU Replacement for Fast Private Network Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10134">http://arxiv.org/abs/2308.10134</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/harveyp123/autorep">https://github.com/harveyp123/autorep</a></li>
<li>paper_authors: Hongwu Peng, Shaoyi Huang, Tong Zhou, Yukui Luo, Chenghong Wang, Zigeng Wang, Jiahui Zhao, Xi Xie, Ang Li, Tony Geng, Kaleel Mahmood, Wujie Wen, Xiaolin Xu, Caiwen Ding<br>for: This paper aims to address the data privacy and security issues in Machine-Learning-As-A-Service (MLaaS) market by proposing a gradient-based approach called AutoReP, which reduces the number of non-linear operators and maintains model expressivity.methods: The proposed AutoReP method uses gradient-based approach to select appropriate ReLU and polynomial functions for private inference, and introduces distribution-aware polynomial approximation (DaPa) to accurately approximate ReLUs.results: The experimental results on CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets show significant accuracy improvements over current state-of-the-art methods, e.g., SNL. Specifically, the accuracy improvements are 6.12%, 8.39%, and 9.45%, respectively. Additionally, AutoReP is applied to EfficientNet-B2 on ImageNet dataset, achieving 75.55% accuracy with 176.1 times ReLU budget reduction.<details>
<summary>Abstract</summary>
The growth of the Machine-Learning-As-A-Service (MLaaS) market has highlighted clients' data privacy and security issues. Private inference (PI) techniques using cryptographic primitives offer a solution but often have high computation and communication costs, particularly with non-linear operators like ReLU. Many attempts to reduce ReLU operations exist, but they may need heuristic threshold selection or cause substantial accuracy loss. This work introduces AutoReP, a gradient-based approach to lessen non-linear operators and alleviate these issues. It automates the selection of ReLU and polynomial functions to speed up PI applications and introduces distribution-aware polynomial approximation (DaPa) to maintain model expressivity while accurately approximating ReLUs. Our experimental results demonstrate significant accuracy improvements of 6.12% (94.31%, 12.9K ReLU budget, CIFAR-10), 8.39% (74.92%, 12.9K ReLU budget, CIFAR-100), and 9.45% (63.69%, 55K ReLU budget, Tiny-ImageNet) over current state-of-the-art methods, e.g., SNL. Morever, AutoReP is applied to EfficientNet-B2 on ImageNet dataset, and achieved 75.55% accuracy with 176.1 times ReLU budget reduction.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Intelligent-Communication-Planning-for-Constrained-Environmental-IoT-Sensing-with-Reinforcement-Learning"><a href="#Intelligent-Communication-Planning-for-Constrained-Environmental-IoT-Sensing-with-Reinforcement-Learning" class="headerlink" title="Intelligent Communication Planning for Constrained Environmental IoT Sensing with Reinforcement Learning"></a>Intelligent Communication Planning for Constrained Environmental IoT Sensing with Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10124">http://arxiv.org/abs/2308.10124</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yi Hu, Jinhang Zuo, Bob Iannucci, Carlee Joe-Wong</li>
<li>for: 这篇论文旨在提高环境监测和风险警告，通过部署一个网络的物联网设备（IoT）。</li>
<li>methods: 该论文使用多代理学习（MARL）方法来优化物联网设备的通信策略，以最大化环境数据跟踪准确性，同时满足功率和带宽限制。</li>
<li>results: 实验表明，使用MARL方法可以学习和利用环境数据的空间时间相关性，以减少物联网设备的重复报告。<details>
<summary>Abstract</summary>
Internet of Things (IoT) technologies have enabled numerous data-driven mobile applications and have the potential to significantly improve environmental monitoring and hazard warnings through the deployment of a network of IoT sensors. However, these IoT devices are often power-constrained and utilize wireless communication schemes with limited bandwidth. Such power constraints limit the amount of information each device can share across the network, while bandwidth limitations hinder sensors' coordination of their transmissions. In this work, we formulate the communication planning problem of IoT sensors that track the state of the environment. We seek to optimize sensors' decisions in collecting environmental data under stringent resource constraints. We propose a multi-agent reinforcement learning (MARL) method to find the optimal communication policies for each sensor that maximize the tracking accuracy subject to the power and bandwidth limitations. MARL learns and exploits the spatial-temporal correlation of the environmental data at each sensor's location to reduce the redundant reports from the sensors. Experiments on wildfire spread with LoRA wireless network simulators show that our MARL method can learn to balance the need to collect enough data to predict wildfire spread with unknown bandwidth limitations.
</details>
<details>
<summary>摘要</summary>
互联网物件技术（IoT）已经启用了许多数据驱动的移动应用程序，并有潜力增强环境监控和险情警告通过网络设置 IoT 感知器。然而，这些 IoT 设备通常受限于能源和无线通信协议的限制，这限制每个设备可以在网络上分享的资讯量，而且对于感知器的传输协议的协调也受到限制。在这个工作中，我们将环境监控 IoT 感知器的通信规划问题形式化为一个最佳化问题，以最大化感知器对环境状态的追踪精度，同时遵循能源和传输协议的限制。我们提出了一种基于多代理问题学习（MARL）方法，以便每个感知器可以在网络上传输环境数据，并且可以适当地调整传输策略，以最大化追踪精度。实验结果显示，我们的 MARL 方法可以在不知道传输协议的情况下，对野火传播进行精确的预测。
</details></li>
</ul>
<hr>
<h2 id="Deep-Generative-Modeling-based-Data-Augmentation-with-Demonstration-using-the-BFBT-Benchmark-Void-Fraction-Datasets"><a href="#Deep-Generative-Modeling-based-Data-Augmentation-with-Demonstration-using-the-BFBT-Benchmark-Void-Fraction-Datasets" class="headerlink" title="Deep Generative Modeling-based Data Augmentation with Demonstration using the BFBT Benchmark Void Fraction Datasets"></a>Deep Generative Modeling-based Data Augmentation with Demonstration using the BFBT Benchmark Void Fraction Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10120">http://arxiv.org/abs/2308.10120</a></li>
<li>repo_url: None</li>
<li>paper_authors: Farah Alsafadi, Xu Wu</li>
<li>for: 这个论文的目的是用深度学习（DL）技术来扩展科学数据，以便更好地训练深度学习模型。</li>
<li>methods: 这个论文使用了深度生成模型（DGM），包括生成敌对网络（GAN）、标准化流（NF）、变换自动编码器（VAE）和条件VAE（CVAE）等，来学习训练数据集的下面分布。</li>
<li>results: 研究发现，使用DGM生成的 sintetic数据可以覆盖现有训练数据的限制，并且可以减少训练数据的缺失值。CVAEs的生成性能最佳，其生成的数据具有最小的错误。这些结果表明，DGM可以有效地扩展科学数据，并且可以帮助深度学习模型更加准确地训练。<details>
<summary>Abstract</summary>
Deep learning (DL) has achieved remarkable successes in many disciplines such as computer vision and natural language processing due to the availability of ``big data''. However, such success cannot be easily replicated in many nuclear engineering problems because of the limited amount of training data, especially when the data comes from high-cost experiments. To overcome such a data scarcity issue, this paper explores the applications of deep generative models (DGMs) that have been widely used for image data generation to scientific data augmentation. DGMs, such as generative adversarial networks (GANs), normalizing flows (NFs), variational autoencoders (VAEs), and conditional VAEs (CVAEs), can be trained to learn the underlying probabilistic distribution of the training dataset. Once trained, they can be used to generate synthetic data that are similar to the training data and significantly expand the dataset size. By employing DGMs to augment TRACE simulated data of the steady-state void fractions based on the NUPEC Boiling Water Reactor Full-size Fine-mesh Bundle Test (BFBT) benchmark, this study demonstrates that VAEs, CVAEs, and GANs have comparable generative performance with similar errors in the synthetic data, with CVAEs achieving the smallest errors. The findings shows that DGMs have a great potential to augment scientific data in nuclear engineering, which proves effective for expanding the training dataset and enabling other DL models to be trained more accurately.
</details>
<details>
<summary>摘要</summary>
深度学习（DL）在许多领域取得了很大成功，如计算机视觉和自然语言处理，这主要归功于大量数据的可用性。然而，在核工程问题中，由于数据的有限性，特别是高成本实验室中的数据，因此复制这种成功是不容易的。为解决这个数据缺乏问题，本文探讨了在科学数据增强方面使用深度生成模型（DGM）的应用。DGM包括生成对抗网络（GAN）、Normalizing Flows（NF）、Variational Autoencoders（VAE）和 Conditional VAEs（CVAE）等，可以在训练数据集的下采用学习下面的概率分布。一旦训练完成，它们可以生成与训练数据相似的 sintetic 数据，并大大增加数据集的大小。本研究通过使用 DGM 增强 TRACE 仿真数据，基于 NUPEC Boiling Water Reactor Full-size Fine-mesh Bundle Test（BFBT）标准底本，展示了 VAE、CVAE 和 GAN 在生成数据时的相似性，CVAE 的错误最小。这些发现表明 DGM 在核工程领域有很大的潜力，可以增强数据增强模型的准确性，从而提高核工程领域的研究效果。
</details></li>
</ul>
<hr>
<h2 id="Modeling-Random-Networks-with-Heterogeneous-Reciprocity"><a href="#Modeling-Random-Networks-with-Heterogeneous-Reciprocity" class="headerlink" title="Modeling Random Networks with Heterogeneous Reciprocity"></a>Modeling Random Networks with Heterogeneous Reciprocity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10113">http://arxiv.org/abs/2308.10113</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Cirkovic, Tiandong Wang</li>
<li>For: 本研究旨在模型社交网络中不同水平的反馈行为。* Methods: 本文提出了一种偏好附加模型，用于模拟用户吸引受欢迎用户的行为，以及不同水平的反馈行为。文章还比较了 bayesian 和 frequentist 模型适应技术，以及计算效率高的变量方案。* Results: 对 Facebook 墙上的墙文网络进行分析，发现用户的反馈行为呈差异性分布，并使用模型预测用户之间的连接关系。模型能够捕捉 Facebook 数据中的重概率分布，并分别确定了多个用户群体的反馈行为特征。<details>
<summary>Abstract</summary>
Reciprocity, or the tendency of individuals to mirror behavior, is a key measure that describes information exchange in a social network. Users in social networks tend to engage in different levels of reciprocal behavior. Differences in such behavior may indicate the existence of communities that reciprocate links at varying rates. In this paper, we develop methodology to model the diverse reciprocal behavior in growing social networks. In particular, we present a preferential attachment model with heterogeneous reciprocity that imitates the attraction users have for popular users, plus the heterogeneous nature by which they reciprocate links. We compare Bayesian and frequentist model fitting techniques for large networks, as well as computationally efficient variational alternatives. Cases where the number of communities are known and unknown are both considered. We apply the presented methods to the analysis of a Facebook wallpost network where users have non-uniform reciprocal behavior patterns. The fitted model captures the heavy-tailed nature of the empirical degree distributions in the Facebook data and identifies multiple groups of users that differ in their tendency to reply to and receive responses to wallposts.
</details>
<details>
<summary>摘要</summary>
“循环性”或“模仿行为”是社交网络中信息交换的关键指标。社交网络中的用户通常在不同的水平上进行反馈行为。不同的反馈行为可能表示社交网络中存在不同的社群，这些社群在链接reciprocate的速率上有所不同。在这篇论文中，我们开发了模型社交网络中多样化反馈行为的方法ологи。特别是，我们提出了具有异质反馈的偏好附着模型，该模型模拟用户循环行为的吸引力和不同的反馈方式。我们比较了 bayesian 和频率主义模型适应技术，以及计算效率高的变量替代方法。我们还考虑了知道和不知道社群数量的两种情况。我们应用这些方法分析Facebook墙上的墙posts网络，发现用户的反馈行为具有不均匀的特点，并确定了不同的用户群体。Note: "循环性" (reciprocity) in the text refers to the tendency of individuals to mirror behavior in a social network.
</details></li>
</ul>
<hr>
<h2 id="Robust-Mixture-of-Expert-Training-for-Convolutional-Neural-Networks"><a href="#Robust-Mixture-of-Expert-Training-for-Convolutional-Neural-Networks" class="headerlink" title="Robust Mixture-of-Expert Training for Convolutional Neural Networks"></a>Robust Mixture-of-Expert Training for Convolutional Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10110">http://arxiv.org/abs/2308.10110</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/optml-group/robust-moe-cnn">https://github.com/optml-group/robust-moe-cnn</a></li>
<li>paper_authors: Yihua Zhang, Ruisi Cai, Tianlong Chen, Guanhua Zhang, Huan Zhang, Pin-Yu Chen, Shiyu Chang, Zhangyang Wang, Sijia Liu</li>
<li>for: 本研究旨在探讨如何使用 sparse-gated Mixture of Expert (MoE) 模型提高 convolutional neural networks (CNNs) 的鲁棒性。</li>
<li>methods: 本研究提出了一种新的 router-expert alternating Adversarial training 框架，以提高 MoE-CNN 模型的鲁棒性。</li>
<li>results: 实验结果表明，相比 dense CNN，AdvMoE 可以提高 adversarial robustness 1% ~ 4%，同时具有较高的计算效率，减少了 более半个 inference cost。<details>
<summary>Abstract</summary>
Sparsely-gated Mixture of Expert (MoE), an emerging deep model architecture, has demonstrated a great promise to enable high-accuracy and ultra-efficient model inference. Despite the growing popularity of MoE, little work investigated its potential to advance convolutional neural networks (CNNs), especially in the plane of adversarial robustness. Since the lack of robustness has become one of the main hurdles for CNNs, in this paper we ask: How to adversarially robustify a CNN-based MoE model? Can we robustly train it like an ordinary CNN model? Our pilot study shows that the conventional adversarial training (AT) mechanism (developed for vanilla CNNs) no longer remains effective to robustify an MoE-CNN. To better understand this phenomenon, we dissect the robustness of an MoE-CNN into two dimensions: Robustness of routers (i.e., gating functions to select data-specific experts) and robustness of experts (i.e., the router-guided pathways defined by the subnetworks of the backbone CNN). Our analyses show that routers and experts are hard to adapt to each other in the vanilla AT. Thus, we propose a new router-expert alternating Adversarial training framework for MoE, termed AdvMoE. The effectiveness of our proposal is justified across 4 commonly-used CNN model architectures over 4 benchmark datasets. We find that AdvMoE achieves 1% ~ 4% adversarial robustness improvement over the original dense CNN, and enjoys the efficiency merit of sparsity-gated MoE, leading to more than 50% inference cost reduction. Codes are available at https://github.com/OPTML-Group/Robust-MoE-CNN.
</details>
<details>
<summary>摘要</summary>
这是一篇研究档案，探讨了一种新的深度学习模型架构，即零价对抗性混合专家（MoE）。这种模型架构已经展示了高精度和高效率的推断能力。 despite its growing popularity, little work has been done to investigate its potential to improve convolutional neural networks (CNNs)， especially in the area of adversarial robustness. therefore, we ask: how to adversarially robustify a CNN-based MoE model? can we train it like an ordinary CNN model? our preliminary study shows that the conventional adversarial training (AT) mechanism (developed for vanilla CNNs) is no longer effective to robustify an MoE-CNN. to better understand this phenomenon, we dissect the robustness of an MoE-CNN into two dimensions: the robustness of routers (i.e., gating functions to select data-specific experts) and the robustness of experts (i.e., the router-guided pathways defined by the subnetworks of the backbone CNN). our analyses show that routers and experts are difficult to adapt to each other in the vanilla AT. therefore, we propose a new router-expert alternating adversarial training framework for MoE, termed AdvMoE. the effectiveness of our proposal is justified across 4 commonly-used CNN model architectures over 4 benchmark datasets. we find that AdvMoE achieves 1% to 4% adversarial robustness improvement over the original dense CNN, and enjoys the efficiency merit of sparsity-gated MoE, leading to more than 50% inference cost reduction. codes are available at https://github.com/OPTML-Group/Robust-MoE-CNN.
</details></li>
</ul>
<hr>
<h2 id="An-Online-Multiple-Kernel-Parallelizable-Learning-Scheme"><a href="#An-Online-Multiple-Kernel-Parallelizable-Learning-Scheme" class="headerlink" title="An Online Multiple Kernel Parallelizable Learning Scheme"></a>An Online Multiple Kernel Parallelizable Learning Scheme</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10101">http://arxiv.org/abs/2308.10101</a></li>
<li>repo_url: None</li>
<li>paper_authors: Emilio Ruiz-Moreno, Baltasar Beferull-Lozano</li>
<li>for: 提高单kernel方法的选择灵活性和计算效率，使其能够更好地适应数据丰富任务中的解决空间。</li>
<li>methods: 基于多kernel学习形式，可以将单kernel解决空间扩展到更广泛的解决空间，从而提高解决空间内的性能。同时，该学习形式可以并行化，以便分配计算负担到不同的计算单元。</li>
<li>results: 在实验中，提出的学习方案比单个单kernel方法相比，在累积较少的最小二乘loss metric上表现出更高的性能。<details>
<summary>Abstract</summary>
The performance of reproducing kernel Hilbert space-based methods is known to be sensitive to the choice of the reproducing kernel. Choosing an adequate reproducing kernel can be challenging and computationally demanding, especially in data-rich tasks without prior information about the solution domain. In this paper, we propose a learning scheme that scalably combines several single kernel-based online methods to reduce the kernel-selection bias. The proposed learning scheme applies to any task formulated as a regularized empirical risk minimization convex problem. More specifically, our learning scheme is based on a multi-kernel learning formulation that can be applied to widen any single-kernel solution space, thus increasing the possibility of finding higher-performance solutions. In addition, it is parallelizable, allowing for the distribution of the computational load across different computing units. We show experimentally that the proposed learning scheme outperforms the combined single-kernel online methods separately in terms of the cumulative regularized least squares cost metric.
</details>
<details>
<summary>摘要</summary>
“kernel Hilbert space-based方法的性能受选择kernel的影响很大。选择合适的kernel可以是一个问题，特别是在没有对解题域的内部信息的情况下。本文提出了一个学习方案，可以可扩展性地结合多个单kernel-based在线方法，以减少kernel-选择偏见。这个学习方案适用于任何形式化为单倍阶调整项目的问题。更 Specifically, our learning scheme is based on a multi-kernel learning formulation that can be applied to widen any single-kernel solution space, thus increasing the possibility of finding higher-performance solutions. In addition, it is parallelizable, allowing for the distribution of the computational load across different computing units. We show experimentally that the proposed learning scheme outperforms the combined single-kernel online methods separately in terms of the cumulative regularized least squares cost metric.”Note: The translation is in Simplified Chinese, which is one of the two standard versions of Chinese. The other version is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Geometric-instability-of-graph-neural-networks-on-large-graphs"><a href="#Geometric-instability-of-graph-neural-networks-on-large-graphs" class="headerlink" title="Geometric instability of graph neural networks on large graphs"></a>Geometric instability of graph neural networks on large graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10099">http://arxiv.org/abs/2308.10099</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/brs96/geometric-instability-gnn-large-graphs">https://github.com/brs96/geometric-instability-gnn-large-graphs</a></li>
<li>paper_authors: Emily Morris, Haotian Shen, Weiling Du, Muhammad Hamza Sajjad, Borun Shi</li>
<li>for: 研究图 Néural Networks（GNNs）中的几何不稳定性。</li>
<li>methods: 提出了一种简单、高效的图native Graph Gram Index（GGI）来测量这种不稳定性，该指标具有卷积、旋转、平移和评估顺序不敏感的特点。</li>
<li>results: 通过研究大图上GNN embedding的不稳定性，发现 embeddings的不稳定性随图的大小而变化，并且在node classification和链接预测 tasks上有不同的不稳定性行为。<details>
<summary>Abstract</summary>
We analyse the geometric instability of embeddings produced by graph neural networks (GNNs). Existing methods are only applicable for small graphs and lack context in the graph domain. We propose a simple, efficient and graph-native Graph Gram Index (GGI) to measure such instability which is invariant to permutation, orthogonal transformation, translation and order of evaluation. This allows us to study the varying instability behaviour of GNN embeddings on large graphs for both node classification and link prediction.
</details>
<details>
<summary>摘要</summary>
我们分析图 neural network (GNN) 生成的嵌入的几何不稳定性。现有方法只适用于小图，缺乏图域上的上下文。我们提议一种简单、高效、图原生的图agram Gram Index (GGI) 来衡量这种不稳定性，该指标是对Permutation、旋转、平移和评估顺序进行 invariant。这使得我们可以研究 GNN 嵌入在大图上的不同不稳定行为，包括节点分类和链接预测。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Bilevel-Learning-with-Inexact-Line-Search"><a href="#Dynamic-Bilevel-Learning-with-Inexact-Line-Search" class="headerlink" title="Dynamic Bilevel Learning with Inexact Line Search"></a>Dynamic Bilevel Learning with Inexact Line Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10098">http://arxiv.org/abs/2308.10098</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Sadegh Salehi, Subhadip Mukherjee, Lindon Roberts, Matthias J. Ehrhardt</li>
<li>for: 这篇论文的目的是解决在各种图像和数据科学领域中，特别是使用变分regularization方法时，手动配置正则化参数的问题。</li>
<li>methods: 这篇论文使用了积分学习来学习适当的正则化参数。</li>
<li>results: 这篇论文的实验结果表明，该方法可以有效地解决手动配置正则化参数的问题，并且可以根据实际需求动态地确定需要的精度。<details>
<summary>Abstract</summary>
In various domains within imaging and data science, particularly when addressing tasks modeled utilizing the variational regularization approach, manually configuring regularization parameters presents a formidable challenge. The difficulty intensifies when employing regularizers involving a large number of hyperparameters. To overcome this challenge, bilevel learning is employed to learn suitable hyperparameters. However, due to the use of numerical solvers, the exact gradient with respect to the hyperparameters is unattainable, necessitating the use of methods relying on approximate gradients. State-of-the-art inexact methods a priori select a decreasing summable sequence of the required accuracy and only assure convergence given a sufficiently small fixed step size. Despite this, challenges persist in determining the Lipschitz constant of the hypergradient and identifying an appropriate fixed step size. Conversely, computing exact function values is not feasible, impeding the use of line search. In this work, we introduce a provably convergent inexact backtracking line search involving inexact function evaluations and hypergradients. We show convergence to a stationary point of the loss with respect to hyperparameters. Additionally, we propose an algorithm to determine the required accuracy dynamically. Our numerical experiments demonstrate the efficiency and feasibility of our approach for hyperparameter estimation in variational regularization problems, alongside its robustness in terms of the initial accuracy and step size choices.
</details>
<details>
<summary>摘要</summary>
在各种图像和数据科学领域中，特别是使用变分regularization方法解决问题时，手动配置正则化参数是一项具有挑战性的任务。这种挑战会加剧，当使用含有大量hyperparameter的正则izers时。为了解决这个问题，我们使用笛卡尔学习来学习适当的hyperparameter。然而，由于使用数值解析器，不能获取正则izers的精确梯度，因此需要使用 Approximate Gradients 的方法。现有的 state-of-the-art 不精确方法会选择一个递减和可加性的精度要求，并且只有在 sufficiently small fixed step size 下才能确保收敛。然而，在确定 Lipschitz 常数和选择合适的fixed step size 方面，仍然存在挑战。另外，计算精确函数值是不可能的，从而阻碍了使用线搜索。在这种情况下，我们提出了一种可证明收敛的不精确返回搜索，该搜索利用不精确函数评估和正则izers的梯度。我们证明了该方法可以收敛到正则izers中的一个站点点。此外，我们还提出了一种动态确定所需的精度的算法。我们的数值实验表明，我们的方法可以有效地进行正则izers的优化，并且具有较好的Robustness 性。
</details></li>
</ul>
<hr>
<h2 id="MLOps-A-Review"><a href="#MLOps-A-Review" class="headerlink" title="MLOps: A Review"></a>MLOps: A Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10908">http://arxiv.org/abs/2308.10908</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jenningst/ecommerce-ops">https://github.com/jenningst/ecommerce-ops</a></li>
<li>paper_authors: Samar Wazir, Gautam Siddharth Kashyap, Parag Saxena</li>
<li>for: The paper aims to explore the significance of Machine Learning Operations (MLOps) methods and assess their features and operability to help create software that is simple to use.</li>
<li>methods: The authors evaluate 22 papers that attempted to apply the MLOps idea and assess the features and operability of various MLOps methods.</li>
<li>results: The authors conclude that there is a scarcity of fully effective MLOps methods that can self-regulate by limiting human engagement.Here’s the same information in Simplified Chinese text:</li>
<li>for: 该论文旨在探讨机器学习运维（MLOps）方法的重要性和评估各种MLOps方法的特性和操作性，以帮助创建简单易用的软件。</li>
<li>methods: 作者评估了22篇应用了MLOps想法的论文，并评估各种MLOps方法的特性和操作性。</li>
<li>results: 作者认为，目前 még 缺乏完全有效的MLOps方法，可以通过限制人类参与来自动化进程。<details>
<summary>Abstract</summary>
Recently, Machine Learning (ML) has become a widely accepted method for significant progress that is rapidly evolving. Since it employs computational methods to teach machines and produce acceptable answers. The significance of the Machine Learning Operations (MLOps) methods, which can provide acceptable answers for such problems, is examined in this study. To assist in the creation of software that is simple to use, the authors research MLOps methods. To choose the best tool structure for certain projects, the authors also assess the features and operability of various MLOps methods. A total of 22 papers were assessed that attempted to apply the MLOps idea. Finally, the authors admit the scarcity of fully effective MLOps methods based on which advancements can self-regulate by limiting human engagement.
</details>
<details>
<summary>摘要</summary>
最近，机器学习（ML）已成为广泛接受的方法，迅速发展。由于它利用计算方法教育机器并生成可接受的答案。本研究研究机器学习操作（MLOps）方法，以便为解决这些问题提供可靠的答案。为便于创建简单易用的软件，作者研究了不同的MLOps方法。为选择特定项目适用的最佳工具结构，作者还评估了各种MLOps方法的特性和操作性。本研究总共评估了22篇尝试应用MLOps想法的论文。最后，作者承认MLOps方法的完全可效性尚未得到保证，尤其是限制人类参与的情况下。
</details></li>
</ul>
<hr>
<h2 id="Securing-Pathways-with-Orthogonal-Robots"><a href="#Securing-Pathways-with-Orthogonal-Robots" class="headerlink" title="Securing Pathways with Orthogonal Robots"></a>Securing Pathways with Orthogonal Robots</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10093">http://arxiv.org/abs/2308.10093</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hamid Hoorfar, Faraneh Fathi, Sara Moshtaghi Largani, Alireza Bagheri</li>
<li>for: 本研究旨在提供一种新型的路径保护方法，使用正交机器人来保护路径。</li>
<li>methods: 本研究使用了正交机器人来 efficiently guard orthogonal areas with the minimum number of orthogonal robots.</li>
<li>results: 研究表明可以在线时确定最小数量的正交机器人，以保护路径。<details>
<summary>Abstract</summary>
The protection of pathways holds immense significance across various domains, including urban planning, transportation, surveillance, and security. This article introduces a groundbreaking approach to safeguarding pathways by employing orthogonal robots. The study specifically addresses the challenge of efficiently guarding orthogonal areas with the minimum number of orthogonal robots. The primary focus is on orthogonal pathways, characterized by a path-like dual graph of vertical decomposition. It is demonstrated that determining the minimum number of orthogonal robots for pathways can be achieved in linear time. However, it is essential to note that the general problem of finding the minimum number of robots for simple polygons with general visibility, even in the orthogonal case, is known to be NP-hard. Emphasis is placed on the flexibility of placing robots anywhere within the polygon, whether on the boundary or in the interior.
</details>
<details>
<summary>摘要</summary>
保护路径在多个领域，如城市规划、交通、监视和安全方面，具有极高的重要性。本文介绍了一种创新的路径保护方法，利用正交机器人。研究特点在于有效地使用最少的正交机器人来保护正交区域。研究主要关注正交路径，即笛卡尔分解中的路径 dual graph。实验表明，可以在线性时间内确定最少正交机器人数量。然而，需要注意的是，对于简单多边形的通用可见情况，即使是正交情况，找到最少机器人数量的问题是NP困难的。研究强调了机器人的位置可以在多边形边界上或者在内部。
</details></li>
</ul>
<hr>
<h2 id="Minimizing-Turns-in-Watchman-Robot-Navigation-Strategies-and-Solutions"><a href="#Minimizing-Turns-in-Watchman-Robot-Navigation-Strategies-and-Solutions" class="headerlink" title="Minimizing Turns in Watchman Robot Navigation: Strategies and Solutions"></a>Minimizing Turns in Watchman Robot Navigation: Strategies and Solutions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10090">http://arxiv.org/abs/2308.10090</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hamid Hoorfar, Sara Moshtaghi Largani, Reza Rahimi, Alireza Bagheri</li>
<li>For: 这篇论文的目的是解决监视人员路径问题（OWRP），即在多边形环境中找到最短路径，使机器人可以在一次连续扫描整个环境。* Methods: 本研究使用了有效的线性时间算法，解决OWRP问题，假设环境是单调的。* Results: 研究发现，这种算法可以减少机器人转弯的数量，从而提高机器人在观察和监视方面的效率，并且可以应用于各种实际应用中。<details>
<summary>Abstract</summary>
The Orthogonal Watchman Route Problem (OWRP) entails the search for the shortest path, known as the watchman route, that a robot must follow within a polygonal environment. The primary objective is to ensure that every point in the environment remains visible from at least one point on the route, allowing the robot to survey the entire area in a single, continuous sweep. This research places particular emphasis on reducing the number of turns in the route, as it is crucial for optimizing navigation in watchman routes within the field of robotics. The cost associated with changing direction is of significant importance, especially for specific types of robots. This paper introduces an efficient linear-time algorithm for solving the OWRP under the assumption that the environment is monotone. The findings of this study contribute to the progress of robotic systems by enabling the design of more streamlined patrol robots. These robots are capable of efficiently navigating complex environments while minimizing the number of turns. This advancement enhances their coverage and surveillance capabilities, making them highly effective in various real-world applications.
</details>
<details>
<summary>摘要</summary>
orthogonal 看守路径问题 (OWRP) 是关于找到最短路径，也称为看守路径，Robot在多边形环境中移动的问题。主要目标是确保环境中每个点都可以在一个点上看到，使Robot在一次连续扫描中涵盖整个区域。这种研究强调减少路径转弯数量，因为这对于某些类型的Robot来说非常重要。这篇论文介绍了一种高效的直线时间算法，用于解决 OWRP，假设环境是均匀的。这些发现对于 robotic 系统的进步做出了贡献，可以设计更加流畅的执勤Robot，这些Robot可以在复杂环境中高效巡捕，最小化转弯数量，提高覆盖和监测能力，使其在实际应用中表现出色。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Learning-for-Non-Local-Graphs-with-Multi-Resolution-Structural-Views"><a href="#Contrastive-Learning-for-Non-Local-Graphs-with-Multi-Resolution-Structural-Views" class="headerlink" title="Contrastive Learning for Non-Local Graphs with Multi-Resolution Structural Views"></a>Contrastive Learning for Non-Local Graphs with Multi-Resolution Structural Views</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10077">http://arxiv.org/abs/2308.10077</a></li>
<li>repo_url: None</li>
<li>paper_authors: Asif Khan, Amos Storkey</li>
<li>for: 学习不同类型图的节点水平表示，用于识别骗子和蛋白质功能预测等应用。</li>
<li>methods: 提出了一种基于多视图对比学习的新方法，该方法通过在图上执行扩散缓动来捕捉高级别图 структуры，从而提高节点表示的准确性。</li>
<li>results: 在synthetic和实际结构数据上比较baseline，该方法的表现较佳，胜过best baseline by 16.06% on Cornell, 3.27% on Texas, and 8.04% on Wisconsin。此外，它在邻近任务上也表现出优异，这表明它有效地捕捉了结构信息，并且可以提高下游应用的性能。<details>
<summary>Abstract</summary>
Learning node-level representations of heterophilic graphs is crucial for various applications, including fraudster detection and protein function prediction. In such graphs, nodes share structural similarity identified by the equivalence of their connectivity which is implicitly encoded in the form of higher-order hierarchical information in the graphs. The contrastive methods are popular choices for learning the representation of nodes in a graph. However, existing contrastive methods struggle to capture higher-order graph structures. To address this limitation, we propose a novel multiview contrastive learning approach that integrates diffusion filters on graphs. By incorporating multiple graph views as augmentations, our method captures the structural equivalence in heterophilic graphs, enabling the discovery of hidden relationships and similarities not apparent in traditional node representations. Our approach outperforms baselines on synthetic and real structural datasets, surpassing the best baseline by $16.06\%$ on Cornell, $3.27\%$ on Texas, and $8.04\%$ on Wisconsin. Additionally, it consistently achieves superior performance on proximal tasks, demonstrating its effectiveness in uncovering structural information and improving downstream applications.
</details>
<details>
<summary>摘要</summary>
学习不同类型图的节点级别表示是关键的，包括诈器检测和蛋白质功能预测。在这些图中，节点具有同类结构，可以通过节点之间的连接相似性来隐式地编码在图中。对比方法是选择学习节点在图中的表示方法，但现有的对比方法很难捕捉更高级别的图结构。为解决这些限制，我们提出了一种新的多视图对比学习方法，该方法在图中integration多个视图作为增强。通过这种方法，我们可以捕捉到不同类型图中节点之间的结构相似性，从而找到隐藏的关系和相似性，不同于传统节点表示。我们的方法在 sintetic和实际结构数据上比基eline表现出色，胜过最佳基eline的$16.06\%$在 Cornell，$3.27\%$在 Texas，和$8.04\%$在 Wisconsin。此外，我们的方法在邻近任务上持续表现出色，证明它的有效性在揭示结构信息和提高下游应用中。
</details></li>
</ul>
<hr>
<h2 id="ILCAS-Imitation-Learning-Based-Configuration-Adaptive-Streaming-for-Live-Video-Analytics-with-Cross-Camera-Collaboration"><a href="#ILCAS-Imitation-Learning-Based-Configuration-Adaptive-Streaming-for-Live-Video-Analytics-with-Cross-Camera-Collaboration" class="headerlink" title="ILCAS: Imitation Learning-Based Configuration-Adaptive Streaming for Live Video Analytics with Cross-Camera Collaboration"></a>ILCAS: Imitation Learning-Based Configuration-Adaptive Streaming for Live Video Analytics with Cross-Camera Collaboration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10068">http://arxiv.org/abs/2308.10068</a></li>
<li>repo_url: None</li>
<li>paper_authors: Duo Wu, Dayou Zhang, Miao Zhang, Ruoyu Zhang, Fangxin Wang, Shuguang Cui</li>
<li>for: 这个论文目的是为了提出一个基于模仿学习的配置适应式视频分析系统（ILCAS），以减少对网络带宽的需求和处理时间，并且能够适应不同的视频内容变化。</li>
<li>methods: ILCAS使用了视频动态特征地图和镜头间协力机制，以捕捉视频内容变化并选择适当的配置。它还使用了专家示范的对策来训练代理人，通过动态计划来解决配置适应问题。</li>
<li>results: 实验结果显示，ILCAS比之前的方案有2-20.9%的增加精度和19.9-85.3%的减少块上传延迟。<details>
<summary>Abstract</summary>
The high-accuracy and resource-intensive deep neural networks (DNNs) have been widely adopted by live video analytics (VA), where camera videos are streamed over the network to resource-rich edge/cloud servers for DNN inference. Common video encoding configurations (e.g., resolution and frame rate) have been identified with significant impacts on striking the balance between bandwidth consumption and inference accuracy and therefore their adaption scheme has been a focus of optimization. However, previous profiling-based solutions suffer from high profiling cost, while existing deep reinforcement learning (DRL) based solutions may achieve poor performance due to the usage of fixed reward function for training the agent, which fails to craft the application goals in various scenarios. In this paper, we propose ILCAS, the first imitation learning (IL) based configuration-adaptive VA streaming system. Unlike DRL-based solutions, ILCAS trains the agent with demonstrations collected from the expert which is designed as an offline optimal policy that solves the configuration adaption problem through dynamic programming. To tackle the challenge of video content dynamics, ILCAS derives motion feature maps based on motion vectors which allow ILCAS to visually ``perceive'' video content changes. Moreover, ILCAS incorporates a cross-camera collaboration scheme to exploit the spatio-temporal correlations of cameras for more proper configuration selection. Extensive experiments confirm the superiority of ILCAS compared with state-of-the-art solutions, with 2-20.9% improvement of mean accuracy and 19.9-85.3% reduction of chunk upload lag.
</details>
<details>
<summary>摘要</summary>
高精度和资源占用深度神经网络（DNN）在实时视频分析（VA）中广泛应用，其中摄像头视频流经网络传输至 Edge/云服务器进行DNN推理。常见的视频编码配置（例如分辨率和帧率）有显著影响于占用带宽和推理精度的平衡，因此其适应方案成为优化的焦点。然而，先前的 Profiling-based 解决方案具有高 Profiling 成本，而现有的深度强化学习（DRL）基于解决方案可能因为使用固定的奖励函数进行训练代理人，导致在不同enario中拟合应用目标的问题。在本文中，我们提出了 ILCAS，第一个仿学学习（IL）基于配置适应VA流动系统。与 DRL-based 解决方案不同，ILCAS 通过收集专家设计的示例来训练代理人，通过动态计划解决配置适应问题。为了解决视频内容变化的挑战，ILCAS  derivates 基于运动 вектор的动作特征图， Allow ILCAS 在视觉上“感受”视频内容变化。此外，ILCAS 还实现了相机间协作机制，以便更好地选择适配。经验证明，ILCAS 相比现有的解决方案具有2-20.9%的提升精度和19.9-85.3%的减少块上传延迟。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/20/cs.LG_2023_08_20/" data-id="clly3dvzq006q0988eb5n67bt" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_08_20" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/20/cs.SD_2023_08_20/" class="article-date">
  <time datetime="2023-08-19T16:00:00.000Z" itemprop="datePublished">2023-08-20</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/20/cs.SD_2023_08_20/">cs.SD - 2023-08-20 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Indonesian-Automatic-Speech-Recognition-with-XLSR-53"><a href="#Indonesian-Automatic-Speech-Recognition-with-XLSR-53" class="headerlink" title="Indonesian Automatic Speech Recognition with XLSR-53"></a>Indonesian Automatic Speech Recognition with XLSR-53</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11589">http://arxiv.org/abs/2308.11589</a></li>
<li>repo_url: None</li>
<li>paper_authors: Panji Arisaputra, Amalia Zahra</li>
<li>for: 这个研究旨在开发一个使用 XLSR-53 预训练模型的印度尼西亚自动语音识别（ASR）系统，以减少不英语语言需要的训练数据量以 достичь竞争性 Word Error Rate（WER）。</li>
<li>methods: 这个研究使用了 14 小时、31 分、1 秒的 TITML-IDN 数据集、3 小时、33 分的 Magic Data 数据集和 6 小时、14 分、1 秒的 Common Voice 数据集，并使用了一个语言模型来降低 WER 约 8%，从 20% 降低到 12%。</li>
<li>results: 根据这个研究，可以成功地完善之前的研究，创造一个更好的印度尼西亚 ASR 系统，只需要一小部分的数据。WER 的值为 20%，比 similiar 模型使用 Common Voice 数据集的分test更低。<details>
<summary>Abstract</summary>
This study focuses on the development of Indonesian Automatic Speech Recognition (ASR) using the XLSR-53 pre-trained model, the XLSR stands for cross-lingual speech representations. The use of this XLSR-53 pre-trained model is to significantly reduce the amount of training data in non-English languages required to achieve a competitive Word Error Rate (WER). The total amount of data used in this study is 24 hours, 18 minutes, and 1 second: (1) TITML-IDN 14 hours and 31 minutes; (2) Magic Data 3 hours and 33 minutes; and (3) Common Voice 6 hours, 14 minutes, and 1 second. With a WER of 20%, the model built in this study can compete with similar models using the Common Voice dataset split test. WER can be decreased by around 8% using a language model, resulted in WER from 20% to 12%. Thus, the results of this study have succeeded in perfecting previous research in contributing to the creation of a better Indonesian ASR with a smaller amount of data.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>TITML-IDN: 14 hours and 31 minutes2. Magic Data: 3 hours and 33 minutes3. Common Voice: 6 hours, 14 minutes, and 1 secondWith a WER of 20%, the model built in this study can compete with similar models using the Common Voice dataset split test. Additionally, using a language model can decrease WER by around 8%, resulting in a WER of 12%. Therefore, the results of this study have successfully built upon previous research and contributed to the creation of a better Indonesian ASR with a smaller amount of data.</details></li>
</ol>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/20/cs.SD_2023_08_20/" data-id="clly3dw10009t09881sux4j2p" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_08_20" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/20/eess.IV_2023_08_20/" class="article-date">
  <time datetime="2023-08-19T16:00:00.000Z" itemprop="datePublished">2023-08-20</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/20/eess.IV_2023_08_20/">eess.IV - 2023-08-20 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Boosting-Adversarial-Transferability-by-Block-Shuffle-and-Rotation"><a href="#Boosting-Adversarial-Transferability-by-Block-Shuffle-and-Rotation" class="headerlink" title="Boosting Adversarial Transferability by Block Shuffle and Rotation"></a>Boosting Adversarial Transferability by Block Shuffle and Rotation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10299">http://arxiv.org/abs/2308.10299</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kunyu Wang, Xuanran He, Wenxuan Wang, Xiaosen Wang</li>
<li>for: 防御深度学习模型受到攻击，即使使用黑盒 Setting 进行攻击。</li>
<li>methods: 使用输入变换基于的攻击方法，包括块洗混淆和旋转（BSR）。</li>
<li>results: BSR 可以在单模型和ensemble模型 Setting 下达到更高的传输性能，并且可以与现有的输入变换方法相结合，以获得更好的传输性能。<details>
<summary>Abstract</summary>
Adversarial examples mislead deep neural networks with imperceptible perturbations and have brought significant threats to deep learning. An important aspect is their transferability, which refers to their ability to deceive other models, thus enabling attacks in the black-box setting. Though various methods have been proposed to boost transferability, the performance still falls short compared with white-box attacks. In this work, we observe that existing input transformation based attacks, one of the mainstream transfer-based attacks, result in different attention heatmaps on various models, which might limit the transferability. We also find that breaking the intrinsic relation of the image can disrupt the attention heatmap of the original image. Based on this finding, we propose a novel input transformation based attack called block shuffle and rotation (BSR). Specifically, BSR splits the input image into several blocks, then randomly shuffles and rotates these blocks to construct a set of new images for gradient calculation. Empirical evaluations on the ImageNet dataset demonstrate that BSR could achieve significantly better transferability than the existing input transformation based methods under single-model and ensemble-model settings. Combining BSR with the current input transformation method can further improve the transferability, which significantly outperforms the state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
深度学习中的对抗性示例会使深度神经网络受到不可见的扰动，对深度学习带来了重要的威胁。其中一个重要方面是它们的传输性，即它们能够在黑盒Setting中欺骗其他模型，因此可以实现黑盒攻击。虽然多种方法已经被提出来提高传输性，但其性能仍然落后于白盒攻击。在这项工作中，我们发现现有的输入变换基于攻击，一种主流的传输基于攻击，会导致不同的注意度热图在不同的模型上。我们还发现，打破图像的内在关系可以阻断原始图像的注意度热图。基于这一发现，我们提出了一种新的输入变换基于攻击方法，即块拼接和旋转（BSR）。具体来说，BSR将输入图像分成多个块，然后随机拼接和旋转这些块来构建一组新的图像，用于计算梯度。我们的实验表明，BSR可以在单模型和集成模型设置下 achieves significantly better transferability than现有的输入变换基于攻击方法。同时，BSR与现有的输入变换方法相结合可以进一步提高传输性，并且与当前的状态态-of-the-art方法相比，显著超越。
</details></li>
</ul>
<hr>
<h2 id="Domain-Reduction-Strategy-for-Non-Line-of-Sight-Imaging"><a href="#Domain-Reduction-Strategy-for-Non-Line-of-Sight-Imaging" class="headerlink" title="Domain Reduction Strategy for Non Line of Sight Imaging"></a>Domain Reduction Strategy for Non Line of Sight Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10269">http://arxiv.org/abs/2308.10269</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hyunbo Shim, In Cho, Daekyu Kwon, Seon Joo Kim</li>
<li>for: 非直线视野（NLOS）图像重建</li>
<li>methods: 利用独立计算各点隐藏物变数的方法，并将处理隐藏表面的函数组合成一个更加简洁的问题</li>
<li>results: 在多种NLOS情况下，包括非平面镜壁、稀疏扫描图、对焦和非对焦图像重建等，该方法能够提供高效且稳定的重建结果<details>
<summary>Abstract</summary>
This paper presents a novel optimization-based method for non-line-of-sight (NLOS) imaging that aims to reconstruct hidden scenes under various setups. Our method is built upon the observation that photons returning from each point in hidden volumes can be independently computed if the interactions between hidden surfaces are trivially ignored. We model the generalized light propagation function to accurately represent the transients as a linear combination of these functions. Moreover, our proposed method includes a domain reduction procedure to exclude empty areas of the hidden volumes from the set of propagation functions, thereby improving computational efficiency of the optimization. We demonstrate the effectiveness of the method in various NLOS scenarios, including non-planar relay wall, sparse scanning patterns, confocal and non-confocal, and surface geometry reconstruction. Experiments conducted on both synthetic and real-world data clearly support the superiority and the efficiency of the proposed method in general NLOS scenarios.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Crucial-Feature-Capture-and-Discrimination-for-Limited-Training-Data-SAR-ATR"><a href="#Crucial-Feature-Capture-and-Discrimination-for-Limited-Training-Data-SAR-ATR" class="headerlink" title="Crucial Feature Capture and Discrimination for Limited Training Data SAR ATR"></a>Crucial Feature Capture and Discrimination for Limited Training Data SAR ATR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10911">http://arxiv.org/abs/2308.10911</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cwwangsaratr/saratr_feacapture_discrimination">https://github.com/cwwangsaratr/saratr_feacapture_discrimination</a></li>
<li>paper_authors: Chenwei Wang, Siyi Luo, Jifang Pei, Yulin Huang, Yin Zhang, Jianyu Yang</li>
<li>for: 本研究旨在提高SAR ATR的表现，对于受限 Training 数据的情况下。</li>
<li>methods: 本研究使用了两个分支和两个模组，包括全球帮助分支和本地增强分支、特征捕捉模组和特征分别模组。在每次训练中，全球帮助分支首先完成了初始识别，基于整个图像。然后，特征捕捉模组自动搜寻并锁定了重要的图像区域，我们称之为“图像金钥”。最后，本地增强分支对于这些捕捉的图像区域进行了本地特征抽取，并将其与全局特征进行了联合识别。</li>
<li>results: 本研究的模型验证实验和比较显示，在受限 Training 数据的情况下，我们的方法实现了更好的识别表现，包括改善特征分布和识别概率。在MSTAR和OPENSAR上进行的实验和比较显示，我们的方法已经 дости得了superior的识别表现。<details>
<summary>Abstract</summary>
Although deep learning-based methods have achieved excellent performance on SAR ATR, the fact that it is difficult to acquire and label a lot of SAR images makes these methods, which originally performed well, perform weakly. This may be because most of them consider the whole target images as input, but the researches find that, under limited training data, the deep learning model can't capture discriminative image regions in the whole images, rather focus on more useless even harmful image regions for recognition. Therefore, the results are not satisfactory. In this paper, we design a SAR ATR framework under limited training samples, which mainly consists of two branches and two modules, global assisted branch and local enhanced branch, feature capture module and feature discrimination module. In every training process, the global assisted branch first finishes the initial recognition based on the whole image. Based on the initial recognition results, the feature capture module automatically searches and locks the crucial image regions for correct recognition, which we named as the golden key of image. Then the local extract the local features from the captured crucial image regions. Finally, the overall features and local features are input into the classifier and dynamically weighted using the learnable voting parameters to collaboratively complete the final recognition under limited training samples. The model soundness experiments demonstrate the effectiveness of our method through the improvement of feature distribution and recognition probability. The experimental results and comparisons on MSTAR and OPENSAR show that our method has achieved superior recognition performance.
</details>
<details>
<summary>摘要</summary>
尽管深度学习基本方法在 Synthetic Aperture Radar（SAR）特征识别（ATR）中表现出色，但由于获取和标注大量SAR图像困难，这些方法在限制性训练样本的情况下表现弱。这可能是因为大多数方法将整个目标图像作为输入，但研究人员发现，在有限训练样本下，深度学习模型无法在整个图像中捕捉有利特征区域，而是偏向更无用或甚至有害的图像区域进行识别。因此，结果不满足要求。在这篇论文中，我们设计了一个基于有限训练样本的SAR ATR框架，它包括两个支线和两个模块：全球协助支线和本地增强支线，特征捕捉模块和特征分类模块。在每次训练过程中，全球协助支线首先根据整个图像完成初步识别。基于初步识别结果，特征捕捉模块自动搜索和锁定图像中重要的关键区域，我们称之为图像的“金钥匙”。然后，本地EXTRACT本地特征从捕捉到的关键图像区域。最后，总特征和本地特征被输入到分类器并使用学习投票参数进行协同完成最终识别。实验证明我们的方法的有效性通过特征分布和识别概率的改进。实验结果和相对比较表明，我们的方法在MSTAR和OPENSAR上达到了最高的识别性能。
</details></li>
</ul>
<hr>
<h2 id="An-Entropy-Awareness-Meta-Learning-Method-for-SAR-Open-Set-ATR"><a href="#An-Entropy-Awareness-Meta-Learning-Method-for-SAR-Open-Set-ATR" class="headerlink" title="An Entropy-Awareness Meta-Learning Method for SAR Open-Set ATR"></a>An Entropy-Awareness Meta-Learning Method for SAR Open-Set ATR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10251">http://arxiv.org/abs/2308.10251</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenwei Wang, Siyi Luo, Jifang Pei, Xiaoyu Liu, Yulin Huang, Yin Zhang, Jianyu Yang</li>
<li>for: 提高Synthetic Aperture Radar自动目标识别（SAR ATR）方法的泛化能力，特别是处理未知类目标的开放集成识别（OSR）问题。</li>
<li>methods: 提出一种基于自适应学习和熵觉知的元学习方法，通过元学习任务学习构建动态分配的知识类别特征空间，并通过熵觉知损失进一步增强特征空间的效果和Robustness。</li>
<li>results: 在MSTAR数据集上进行实验，显示了方法的效果性和可靠性。<details>
<summary>Abstract</summary>
Existing synthetic aperture radar automatic target recognition (SAR ATR) methods have been effective for the classification of seen target classes. However, it is more meaningful and challenging to distinguish the unseen target classes, i.e., open set recognition (OSR) problem, which is an urgent problem for the practical SAR ATR. The key solution of OSR is to effectively establish the exclusiveness of feature distribution of known classes. In this letter, we propose an entropy-awareness meta-learning method that improves the exclusiveness of feature distribution of known classes which means our method is effective for not only classifying the seen classes but also encountering the unseen other classes. Through meta-learning tasks, the proposed method learns to construct a feature space of the dynamic-assigned known classes. This feature space is required by the tasks to reject all other classes not belonging to the known classes. At the same time, the proposed entropy-awareness loss helps the model to enhance the feature space with effective and robust discrimination between the known and unknown classes. Therefore, our method can construct a dynamic feature space with discrimination between the known and unknown classes to simultaneously classify the dynamic-assigned known classes and reject the unknown classes. Experiments conducted on the moving and stationary target acquisition and recognition (MSTAR) dataset have shown the effectiveness of our method for SAR OSR.
</details>
<details>
<summary>摘要</summary>
现有的Synthetic Aperture Radar自动目标识别（SAR ATR）方法已经有效地对seen标签类进行分类。然而，更重要和挑战性的是分inguished the unseen target classes，即open set recognition（OSR）问题，这是实际SAR ATR中的紧迫问题。OSR的关键解决方案是有效地建立known classes的特征分布的 exclusiveness。在这封信中，我们提出了一种基于entropy的meta-学习方法，该方法可以提高known classes的特征分布的 exclusiveness，这意味着我们的方法不仅能够分类seen classes，还能够遇到未seen的其他类。通过meta-学习任务，我们的方法学习了construct a feature space of the dynamic-assigned known classes。这个feature space是需要由任务拒绝所有不属于known classes的其他类。同时，我们的entropy-awareness loss帮助模型增强feature space的有效和Robust的 distinguish between known和unknown classes。因此，我们的方法可以construct a dynamic feature space with discrimination between known和unknown classes，同时分类dynamic-assigned known classes和拒绝unknown classes。在MSTAR dataset上进行的实验表明了我们的方法对SAR OSR的效iveness。
</details></li>
</ul>
<hr>
<h2 id="SAR-Ship-Target-Recognition-via-Selective-Feature-Discrimination-and-Multifeature-Center-Classifier"><a href="#SAR-Ship-Target-Recognition-via-Selective-Feature-Discrimination-and-Multifeature-Center-Classifier" class="headerlink" title="SAR Ship Target Recognition via Selective Feature Discrimination and Multifeature Center Classifier"></a>SAR Ship Target Recognition via Selective Feature Discrimination and Multifeature Center Classifier</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10250">http://arxiv.org/abs/2308.10250</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenwei Wang, Siyi Luo, Jifang Pei, Yulin Huang, Yin Zhang, Jianyu Yang</li>
<li>for: 这 paper 是为了提高 SAR 图像识别精度，特别是在减少训练样本数量的情况下。</li>
<li>methods: 本 paper 提出了一种基于选择性特征强制和多特征中心分类器的 SAR 船target 识别方法。该方法包括自动选择与最相似 Inter-class 图像对的相似特征，以及与最不相似 Inner-class 图像对的不相似特征，然后对这些特征进行增强。此外，本方法还使用分类器分配多个学习可能的特征中心，以 conquering 大 inner-class variance。</li>
<li>results: 实验结果表明，本方法在 OpenSARShip 和 FUSAR-Ship 数据集上 achieved 较高的识别精度，而且可以在减少训练 SAR 船样本数量的情况下保持高精度。<details>
<summary>Abstract</summary>
Maritime surveillance is not only necessary for every country, such as in maritime safeguarding and fishing controls, but also plays an essential role in international fields, such as in rescue support and illegal immigration control. Most of the existing automatic target recognition (ATR) methods directly send the extracted whole features of SAR ships into one classifier. The classifiers of most methods only assign one feature center to each class. However, the characteristics of SAR ship images, large inner-class variance, and small interclass difference lead to the whole features containing useless partial features and a single feature center for each class in the classifier failing with large inner-class variance. We proposes a SAR ship target recognition method via selective feature discrimination and multifeature center classifier. The selective feature discrimination automatically finds the similar partial features from the most similar interclass image pairs and the dissimilar partial features from the most dissimilar inner-class image pairs. It then provides a loss to enhance these partial features with more interclass separability. Motivated by divide and conquer, the multifeature center classifier assigns multiple learnable feature centers for each ship class. In this way, the multifeature centers divide the large inner-class variance into several smaller variances and conquered by combining all feature centers of one ship class. Finally, the probability distribution over all feature centers is considered comprehensively to achieve an accurate recognition of SAR ship images. The ablation experiments and experimental results on OpenSARShip and FUSAR-Ship datasets show that our method has achieved superior recognition performance under decreasing training SAR ship samples.
</details>
<details>
<summary>摘要</summary>
海上监控不仅是每个国家的必需，如海上安全和渔业控制，而且在国际领域也扮演着重要角色，如搜救支持和非法移民控制。现有的自动目标识别（ATR）方法大多直接将抽取的整个特征集送入一个分类器。但是SAR船图像的特征是巨大的内类差异和小的间类差异，导致整个特征集含有无用的部分特征和分类器中的每个特征中心都是一个。我们提出了一种基于选择性特征分化和多特征中心分类器的SAR船目标识别方法。选择性特征分化自动从最相似的Interclass图像对中找到相似的部分特征和最不相似的内类图像对中找到不相似的部分特征，然后为这些部分特征提供损失来提高它们的Interclass分离度。受分和胜利的思想 inspirited by divide and conquer，多特征中心分类器将每个船类分配多个学习的特征中心。这样，多特征中心将大内类差异分解成多个小差异，并且通过所有特征中心的组合来实现一个船类的准确识别。最后，对所有特征中心的概率分布进行全面考虑以实现高精度的SAR船图像识别。ablation experiment和OpenSARShip和FUSAR-Ship数据集的实验结果表明，我们的方法在减少的训练SAR船样本下实现了优秀的识别性能。
</details></li>
</ul>
<hr>
<h2 id="SAR-Ship-Target-Recognition-Via-Multi-Scale-Feature-Attention-and-Adaptive-Weighed-Classifier"><a href="#SAR-Ship-Target-Recognition-Via-Multi-Scale-Feature-Attention-and-Adaptive-Weighed-Classifier" class="headerlink" title="SAR Ship Target Recognition Via Multi-Scale Feature Attention and Adaptive-Weighed Classifier"></a>SAR Ship Target Recognition Via Multi-Scale Feature Attention and Adaptive-Weighed Classifier</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10247">http://arxiv.org/abs/2308.10247</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenwei Wang, Jifang Pei, Siyi Luo, Weibo Huo, Yulin Huang, Yin Zhang, Jianyu Yang</li>
<li>for: 本研究旨在提高Synthetic Aperture Radar（SAR）船target recognition的精度，提供了一种基于多scale feature attention和自适应权重分类器的方法。</li>
<li>methods: 该方法首先在网络中构建了一个内网络特征 piramid，以EXTRACT多scale特征从SAR船图像中。然后，使用多scale feature attention可以提取和增强多scale特征中的主要成分，具有更高的内类紧凑性和 между类分离性。最后，使用自适应权重分类器选择有效的特征层次，以实现最终的精度 recognition。</li>
<li>results: 通过对OpenSARship数据集进行实验和比较，该方法被证明可以达到SAR船target recognition的 estado-of-the-art表现。<details>
<summary>Abstract</summary>
Maritime surveillance is indispensable for civilian fields, including national maritime safeguarding, channel monitoring, and so on, in which synthetic aperture radar (SAR) ship target recognition is a crucial research field. The core problem to realizing accurate SAR ship target recognition is the large inner-class variance and inter-class overlap of SAR ship features, which limits the recognition performance. Most existing methods plainly extract multi-scale features of the network and utilize equally each feature scale in the classification stage. However, the shallow multi-scale features are not discriminative enough, and each scale feature is not equally effective for recognition. These factors lead to the limitation of recognition performance. Therefore, we proposed a SAR ship recognition method via multi-scale feature attention and adaptive-weighted classifier to enhance features in each scale, and adaptively choose the effective feature scale for accurate recognition. We first construct an in-network feature pyramid to extract multi-scale features from SAR ship images. Then, the multi-scale feature attention can extract and enhance the principal components from the multi-scale features with more inner-class compactness and inter-class separability. Finally, the adaptive weighted classifier chooses the effective feature scales in the feature pyramid to achieve the final precise recognition. Through experiments and comparisons under OpenSARship data set, the proposed method is validated to achieve state-of-the-art performance for SAR ship recognition.
</details>
<details>
<summary>摘要</summary>
海上监测是民用领域不可或缺的，包括国家海上安全、水道监测等， Synthetic Aperture Radar（SAR）船TargetRecognition是一个重要的研究领域。 however， Due to the large inner-class variance and inter-class overlap of SAR ship features, accurate recognition is limited. Most existing methods simply extract multi-scale features from the network and use each feature scale equally in the classification stage. However, the shallow multi-scale features are not discriminative enough, and each scale feature is not equally effective for recognition. These factors limit the recognition performance. Therefore, we proposed a SAR ship recognition method based on multi-scale feature attention and adaptive-weighted classifiers to enhance features in each scale and adaptively choose the effective feature scale for accurate recognition.First, we construct an in-network feature pyramid to extract multi-scale features from SAR ship images. Then, the multi-scale feature attention can extract and enhance the principal components from the multi-scale features with more inner-class compactness and inter-class separability. Finally, the adaptive weighted classifier chooses the effective feature scales in the feature pyramid to achieve the final precise recognition. Through experiments and comparisons under OpenSARship dataset, the proposed method is validated to achieve state-of-the-art performance for SAR ship recognition.
</details></li>
</ul>
<hr>
<h2 id="SAR-ATR-Method-with-Limited-Training-Data-via-an-Embedded-Feature-Augmenter-and-Dynamic-Hierarchical-Feature-Refiner"><a href="#SAR-ATR-Method-with-Limited-Training-Data-via-an-Embedded-Feature-Augmenter-and-Dynamic-Hierarchical-Feature-Refiner" class="headerlink" title="SAR ATR Method with Limited Training Data via an Embedded Feature Augmenter and Dynamic Hierarchical-Feature Refiner"></a>SAR ATR Method with Limited Training Data via an Embedded Feature Augmenter and Dynamic Hierarchical-Feature Refiner</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10243">http://arxiv.org/abs/2308.10243</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenwei Wang, Siyi Luo, Yulin Huang, Jifang Pei, Yin Zhang, Jianyu Yang</li>
<li>for: 增进 Synthetic Aperture Radar（SAR）自动目标识别（ATR）性能，特别是在有限的训练数据情况下。</li>
<li>methods: 提出了一种新的方法，包括嵌入特征增强器和动态层次特征细化器。嵌入特征增强器通过吸引远离分类中心的虚拟特征，提高了可用于超vised学习的信息量。动态层次特征细化器通过生成动态核函数，捕捉到不同维度的特征，进一步提高了内类紧凑度和对类分离度。</li>
<li>results: 实验结果表明，提出的方法在有限SAR训练数据情况下实现了优秀的ATR性能，并在MSTAR、OpenSARShip和FUSAR-Ship数据集上达到了robust性和稳定性。<details>
<summary>Abstract</summary>
Without sufficient data, the quantity of information available for supervised training is constrained, as obtaining sufficient synthetic aperture radar (SAR) training data in practice is frequently challenging. Therefore, current SAR automatic target recognition (ATR) algorithms perform poorly with limited training data availability, resulting in a critical need to increase SAR ATR performance. In this study, a new method to improve SAR ATR when training data are limited is proposed. First, an embedded feature augmenter is designed to enhance the extracted virtual features located far away from the class center. Based on the relative distribution of the features, the algorithm pulls the corresponding virtual features with different strengths toward the corresponding class center. The designed augmenter increases the amount of information available for supervised training and improves the separability of the extracted features. Second, a dynamic hierarchical-feature refiner is proposed to capture the discriminative local features of the samples. Through dynamically generated kernels, the proposed refiner integrates the discriminative local features of different dimensions into the global features, further enhancing the inner-class compactness and inter-class separability of the extracted features. The proposed method not only increases the amount of information available for supervised training but also extracts the discriminative features from the samples, resulting in superior ATR performance in problems with limited SAR training data. Experimental results on the moving and stationary target acquisition and recognition (MSTAR), OpenSARShip, and FUSAR-Ship benchmark datasets demonstrate the robustness and outstanding ATR performance of the proposed method in response to limited SAR training data.
</details>
<details>
<summary>摘要</summary>
无 suficiente datos de entrenamiento，量度信息可用于超vised学习是受限的，因为在实践中获得 suficiente Synthetic Aperture Radar（SAR）training data是困难的。因此，当前SAR自动目标识别（ATR）算法在有限的training data availability时表现糟糕。在这种情况下，一种新的方法是提出，以提高SAR ATR的性能。首先，一种嵌入式特征增强器是设计的，以增强提取的虚拟特征，特别是那些远离类中心的特征。根据特征的相对分布，算法将相应的虚拟特征与不同强度拖向相应的类中心。这种增强器提高了用于超vised学习的信息量，并提高了提取特征的分类能力。其次，一种动态层次特征级化器是提出的，以捕捉不同维度的特征。通过动态生成的核函数，提出的级化器将不同维度的特征级化到全维度特征中，进一步提高了类内紧密性和类间分离性。这种方法不仅提高了用于超vised学习的信息量，还提取了样本中的特征，从而实现了在有限SAR training data情况下的出色ATR性能。实验结果表明，在MSTAR、OpenSARShip和FUSAR-Ship benchmark dataset上，提出的方法具有强大的Robustness和出色的ATR性能，在有限SAR training data情况下表现优秀。
</details></li>
</ul>
<hr>
<h2 id="Blind-Face-Restoration-for-Under-Display-Camera-via-Dictionary-Guided-Transformer"><a href="#Blind-Face-Restoration-for-Under-Display-Camera-via-Dictionary-Guided-Transformer" class="headerlink" title="Blind Face Restoration for Under-Display Camera via Dictionary Guided Transformer"></a>Blind Face Restoration for Under-Display Camera via Dictionary Guided Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10196">http://arxiv.org/abs/2308.10196</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingfan Tan, Xiaoxu Chen, Tao Wang, Kaihao Zhang, Wenhan Luo, Xiaocun Cao</li>
<li>for: 提供全屏体验，但是由于显示器的特点，UDC图像会受到质量下降的影响。</li>
<li>methods: 提出了一种两stage网络UDC退化模型网络（UDC-DMNet），通过模拟UDC拍摄过程来synthesize UDC图像。还使用高质量的face图像从FFHQ和CelebA-Test创建了UDC face测试集FFHQ-P&#x2F;T和CelebA-Test-P&#x2F;T。</li>
<li>results: 提出了一种基于字典指导 transformer网络（DGFormer），通过引入面部组合字典和UDC图像特点来实现盲目face restauration。实验表明，我们的DGFormer和UDC-DMNet均达到了当前最佳性能。<details>
<summary>Abstract</summary>
By hiding the front-facing camera below the display panel, Under-Display Camera (UDC) provides users with a full-screen experience. However, due to the characteristics of the display, images taken by UDC suffer from significant quality degradation. Methods have been proposed to tackle UDC image restoration and advances have been achieved. There are still no specialized methods and datasets for restoring UDC face images, which may be the most common problem in the UDC scene. To this end, considering color filtering, brightness attenuation, and diffraction in the imaging process of UDC, we propose a two-stage network UDC Degradation Model Network named UDC-DMNet to synthesize UDC images by modeling the processes of UDC imaging. Then we use UDC-DMNet and high-quality face images from FFHQ and CelebA-Test to create UDC face training datasets FFHQ-P/T and testing datasets CelebA-Test-P/T for UDC face restoration. We propose a novel dictionary-guided transformer network named DGFormer. Introducing the facial component dictionary and the characteristics of the UDC image in the restoration makes DGFormer capable of addressing blind face restoration in UDC scenarios. Experiments show that our DGFormer and UDC-DMNet achieve state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
<<SYS>>通过将前置摄像头嵌入到显示板上，Under-Display Camera（UDC）为用户提供了全屏体验。然而，由于显示器的特点，UDC拍摄的图像受到了显著的质量下降。已经提出了修复UDC图像的方法，并取得了进步。然而，还没有专门的方法和数据集用于修复UDC脸图像，这可能是UDC场景中最常见的问题。为此，我们考虑了颜色滤波、亮度减弱和干涉的影像捕捉过程，并提出了一个两stage网络名为UDC-DMNet，用于模拟UDC拍摄过程。然后，我们使用UDC-DMNet和高质量的脸图像从FFHQ和CelebA-Test创建了UDC脸培训集FFHQ-P/T和测试集CelebA-Test-P/T。我们还提出了一种新的字典引导变换网络名为DGFormer，通过引入人脸组件字典和UDC图像的特点，DGFormer可以实现盲目脸修复在UDC场景中。实验表明，我们的DGFormer和UDC-DMNet实现了状态盘的性能。Note: Simplified Chinese is used here, which is a more common writing system in China. If you prefer Traditional Chinese, I can also provide the translation.
</details></li>
</ul>
<hr>
<h2 id="WMFormer-Nested-Transformer-for-Visible-Watermark-Removal-via-Implict-Joint-Learning"><a href="#WMFormer-Nested-Transformer-for-Visible-Watermark-Removal-via-Implict-Joint-Learning" class="headerlink" title="WMFormer++: Nested Transformer for Visible Watermark Removal via Implict Joint Learning"></a>WMFormer++: Nested Transformer for Visible Watermark Removal via Implict Joint Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10195">http://arxiv.org/abs/2308.10195</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongjian Huo, Zehong Zhang, Hanjing Su, Guanbin Li, Chaowei Fang, Qingyao Wu</li>
<li>for: 本研究旨在提高水印去除技术的精度和效率，以提高水印保护的可靠性。</li>
<li>methods: 该研究提出了一种新的 JOINT 学习方法，通过自动控制信息流动来Integrate 多个分支知识，从而提高水印localization和背景 restore 的精度。</li>
<li>results: 实验结果表明，该方法可以具有remarkable superiority，比existingsota-of-the-art方法大幅提高精度和效率。<details>
<summary>Abstract</summary>
Watermarking serves as a widely adopted approach to safeguard media copyright. In parallel, the research focus has extended to watermark removal techniques, offering an adversarial means to enhance watermark robustness and foster advancements in the watermarking field. Existing watermark removal methods mainly rely on UNet with task-specific decoder branches--one for watermark localization and the other for background image restoration. However, watermark localization and background restoration are not isolated tasks; precise watermark localization inherently implies regions necessitating restoration, and the background restoration process contributes to more accurate watermark localization. To holistically integrate information from both branches, we introduce an implicit joint learning paradigm. This empowers the network to autonomously navigate the flow of information between implicit branches through a gate mechanism. Furthermore, we employ cross-channel attention to facilitate local detail restoration and holistic structural comprehension, while harnessing nested structures to integrate multi-scale information. Extensive experiments are conducted on various challenging benchmarks to validate the effectiveness of our proposed method. The results demonstrate our approach's remarkable superiority, surpassing existing state-of-the-art methods by a large margin.
</details>
<details>
<summary>摘要</summary>
水印技术是现代版权保护的广泛采用方法之一。同时，研究焦点已经扩展到水印去除技术，提供了一种对抗性的方法，以提高水印的鲁棒性和推动水印技术的发展。现有的水印去除方法主要基于UNet架构，其中一个任务特定的解码分支用于水印localization，另一个分支用于背景图像修复。然而，水印localization和背景修复不是独立的任务，准确的水印localization直接 imply了需要修复的区域，而背景修复过程也会提高水印localization的准确性。为了整合这两个分支的信息，我们提出了隐式联合学习 paradigm。这种方法使得网络可以自动地在两个分支之间流动信息，通过门控制机制来自动调节信息的流动。此外，我们还使用了跨通道注意力来促进地方细节修复和整体结构认知，同时利用嵌入结构来整合多尺度信息。我们在多种复杂的 benchmark 上进行了广泛的实验，以验证我们的提出方法的有效性。结果表明，我们的方法在与现有状态顶的方法进行比较时表现出了惊人的优势，凌驱了现有的方法。
</details></li>
</ul>
<hr>
<h2 id="EDDense-Net-Fully-Dense-Encoder-Decoder-Network-for-Joint-Segmentation-of-Optic-Cup-and-Disc"><a href="#EDDense-Net-Fully-Dense-Encoder-Decoder-Network-for-Joint-Segmentation-of-Optic-Cup-and-Disc" class="headerlink" title="EDDense-Net: Fully Dense Encoder Decoder Network for Joint Segmentation of Optic Cup and Disc"></a>EDDense-Net: Fully Dense Encoder Decoder Network for Joint Segmentation of Optic Cup and Disc</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10192">http://arxiv.org/abs/2308.10192</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mehwish Mehmood, Khuram Naveed, Haroon Ahmed Khan, Syed S. Naqvi<br>for: 这篇论文是为了检测和诊断眼内疾病 glaucoma 而写的。methods: 这篇论文提出了一种基于 dense block 的 EDDense-Net 分割网络，用于同时 segmentation of OC 和 OD。该网络使用 grouped convolutional layer 以获取和传递图像中的空间信息，并同时减少网络的复杂性。在 semantic segmentation 阶段，使用 dice pixel classification 来缓解类别不均衡问题。results: 该方法在两个公共可用的数据集上进行评估，并在精度和效率方面超越了现有的状态图像方法。这种方法可以用于医学眼科专业人员的助手，帮助他们诊断和分析眼内疾病。<details>
<summary>Abstract</summary>
Glaucoma is an eye disease that causes damage to the optic nerve, which can lead to visual loss and permanent blindness. Early glaucoma detection is therefore critical in order to avoid permanent blindness. The estimation of the cup-to-disc ratio (CDR) during an examination of the optical disc (OD) is used for the diagnosis of glaucoma. In this paper, we present the EDDense-Net segmentation network for the joint segmentation of OC and OD. The encoder and decoder in this network are made up of dense blocks with a grouped convolutional layer in each block, allowing the network to acquire and convey spatial information from the image while simultaneously reducing the network's complexity. To reduce spatial information loss, the optimal number of filters in all convolution layers were utilised. In semantic segmentation, dice pixel classification is employed in the decoder to alleviate the problem of class imbalance. The proposed network was evaluated on two publicly available datasets where it outperformed existing state-of-the-art methods in terms of accuracy and efficiency. For the diagnosis and analysis of glaucoma, this method can be used as a second opinion system to assist medical ophthalmologists.
</details>
<details>
<summary>摘要</summary>
Glaucoma 是一种眼病，可以导致视网膜损害，从而导致视力下降和永久失明。 Early detection of glaucoma 是非常重要，以避免永久失明。在诊断 glaucoma 时，可以使用 optical disc 的 cup-to-disc ratio（CDR）的估算。在这篇论文中，我们提出了 EDDense-Net 分割网络，用于对 optical disc 和 optic cup 的同时分割。这个网络包括 dense block 和 grouped convolutional layer，可以同时保持图像的空间信息并减少网络的复杂性。为了避免空间信息损失，我们在所有卷积层中使用了最佳的筛选器数量。在 semantic segmentation 中，我们使用 dice pixel classification 来缓解类别偏见问题。我们提出的网络在两个公共可用的数据集上进行了评估，并与现有的状态之arte方法相比，在准确性和效率方面表现出色。这种方法可以用于帮助医学眼科医生进行诊断和分析。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Diffusion-Model-with-Auxiliary-Guidance-for-Coarse-to-Fine-PET-Reconstruction"><a href="#Contrastive-Diffusion-Model-with-Auxiliary-Guidance-for-Coarse-to-Fine-PET-Reconstruction" class="headerlink" title="Contrastive Diffusion Model with Auxiliary Guidance for Coarse-to-Fine PET Reconstruction"></a>Contrastive Diffusion Model with Auxiliary Guidance for Coarse-to-Fine PET Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10157">http://arxiv.org/abs/2308.10157</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/show-han/pet-reconstruction">https://github.com/show-han/pet-reconstruction</a></li>
<li>paper_authors: Zeyu Han, Yuhan Wang, Luping Zhou, Peng Wang, Binyu Yan, Jiliu Zhou, Yan Wang, Dinggang Shen</li>
<li>for: 提高标准剂量 позитроном发射tomography（PET）成像质量，降低人体对PET成像的辐射暴露。</li>
<li>methods: 使用生成对抗网络（GANs）和扩散概率模型（DPMs）重建标准剂量PET（SPET）图像从低剂量PET（LPET）图像中。</li>
<li>results: 提出一种块级预测模块（CPM）和一种迭代纠正模块（IRM）组成的粗细预测架构，可以大幅提高样本速度，同时保持LPET图像和重建PET（RPET）图像之间的协同关系，提高临床可靠性。<details>
<summary>Abstract</summary>
To obtain high-quality positron emission tomography (PET) scans while reducing radiation exposure to the human body, various approaches have been proposed to reconstruct standard-dose PET (SPET) images from low-dose PET (LPET) images. One widely adopted technique is the generative adversarial networks (GANs), yet recently, diffusion probabilistic models (DPMs) have emerged as a compelling alternative due to their improved sample quality and higher log-likelihood scores compared to GANs. Despite this, DPMs suffer from two major drawbacks in real clinical settings, i.e., the computationally expensive sampling process and the insufficient preservation of correspondence between the conditioning LPET image and the reconstructed PET (RPET) image. To address the above limitations, this paper presents a coarse-to-fine PET reconstruction framework that consists of a coarse prediction module (CPM) and an iterative refinement module (IRM). The CPM generates a coarse PET image via a deterministic process, and the IRM samples the residual iteratively. By delegating most of the computational overhead to the CPM, the overall sampling speed of our method can be significantly improved. Furthermore, two additional strategies, i.e., an auxiliary guidance strategy and a contrastive diffusion strategy, are proposed and integrated into the reconstruction process, which can enhance the correspondence between the LPET image and the RPET image, further improving clinical reliability. Extensive experiments on two human brain PET datasets demonstrate that our method outperforms the state-of-the-art PET reconstruction methods. The source code is available at \url{https://github.com/Show-han/PET-Reconstruction}.
</details>
<details>
<summary>摘要</summary>
为了获得高质量的 позиトрон发射Tomography（PET）扫描图像，而同时降低人体对X射线暴露，Various approaches have been proposed to reconstruct standard-dose PET（SPET）images from low-dose PET（LPET）images. One widely adopted technique is the generative adversarial networks（GANs）, yet recently, diffusion probabilistic models（DPMs）have emerged as a compelling alternative due to their improved sample quality and higher log-likelihood scores compared to GANs. Despite this, DPMs suffer from two major drawbacks in real clinical settings, i.e., the computationally expensive sampling process and the insufficient preservation of correspondence between the conditioning LPET image and the reconstructed PET（RPET）image. To address the above limitations, this paper presents a coarse-to-fine PET reconstruction framework that consists of a coarse prediction module（CPM）and an iterative refinement module（IRM）. The CPM generates a coarse PET image via a deterministic process, and the IRM samples the residual iteratively. By delegating most of the computational overhead to the CPM, the overall sampling speed of our method can be significantly improved. Furthermore, two additional strategies, i.e., an auxiliary guidance strategy and a contrastive diffusion strategy, are proposed and integrated into the reconstruction process, which can enhance the correspondence between the LPET image and the RPET image, further improving clinical reliability. Extensive experiments on two human brain PET datasets demonstrate that our method outperforms the state-of-the-art PET reconstruction methods. The source code is available at \url{https://github.com/Show-han/PET-Reconstruction}.
</details></li>
</ul>
<hr>
<h2 id="Federated-Pseudo-Modality-Generation-for-Incomplete-Multi-Modal-MRI-Reconstruction"><a href="#Federated-Pseudo-Modality-Generation-for-Incomplete-Multi-Modal-MRI-Reconstruction" class="headerlink" title="Federated Pseudo Modality Generation for Incomplete Multi-Modal MRI Reconstruction"></a>Federated Pseudo Modality Generation for Incomplete Multi-Modal MRI Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10910">http://arxiv.org/abs/2308.10910</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunlu Yan, Chun-Mei Feng, Yuexiang Li, Rick Siow Mong Goh, Lei Zhu</li>
<li>for:  Addressing the missing modality challenge in federated multi-modal MRI reconstruction.</li>
<li>methods:  Utilizes a pseudo modality generation mechanism to recover the missing modality for each single-modal client by sharing the distribution information of the amplitude spectrum in frequency space, and introduces a clustering scheme to reduce communication costs.</li>
<li>results:  Can effectively complete the missing modality within an acceptable communication cost, and attains similar performance with the ideal scenario, i.e., all clients have the full set of modalities.Here’s the Chinese translation of the three points:</li>
<li>for:  Addressing the 缺失多个Modalities的问题在分布式多modal MRI重建中。</li>
<li>methods:  Utilizes a pseudo modality generation mechanism to recover the missing modality for each single-modal client by sharing the distribution information of the amplitude spectrum in frequency space, and introduces a clustering scheme to reduce communication costs.</li>
<li>results:  Can effectively complete the missing modality within an acceptable communication cost, and attains similar performance with the ideal scenario, i.e., all clients have the full set of modalities.<details>
<summary>Abstract</summary>
While multi-modal learning has been widely used for MRI reconstruction, it relies on paired multi-modal data which is difficult to acquire in real clinical scenarios. Especially in the federated setting, the common situation is that several medical institutions only have single-modal data, termed the modality missing issue. Therefore, it is infeasible to deploy a standard federated learning framework in such conditions. In this paper, we propose a novel communication-efficient federated learning framework, namely Fed-PMG, to address the missing modality challenge in federated multi-modal MRI reconstruction. Specifically, we utilize a pseudo modality generation mechanism to recover the missing modality for each single-modal client by sharing the distribution information of the amplitude spectrum in frequency space. However, the step of sharing the original amplitude spectrum leads to heavy communication costs. To reduce the communication cost, we introduce a clustering scheme to project the set of amplitude spectrum into finite cluster centroids, and share them among the clients. With such an elaborate design, our approach can effectively complete the missing modality within an acceptable communication cost. Extensive experiments demonstrate that our proposed method can attain similar performance with the ideal scenario, i.e., all clients have the full set of modalities. The source code will be released.
</details>
<details>
<summary>摘要</summary>
多Modal学习已经广泛应用于MRI重建，但它需要对配套多Modal数据进行训练，这在实际临床情况下很Difficult to acquire.特别是在联邦设置下，一些医疗机构只有单Modal数据，称为模式缺失问题。因此，使用标准联邦学习框架是不可能的。在这篇论文中，我们提出了一种新的通信效率高的联邦学习框架，即Fed-PMG，用于解决联邦多Modal MRI重建中的模式缺失问题。特别是，我们利用 pseudo Modality生成机制来为每个单Modal客户端 recuperate 缺失的模式。然而，在分享原始振荡谱的步骤中，会导致重大的通信成本。为了降低通信成本，我们引入了分区 schemes来将振荡谱集合投影到有限的群集中心，然后分享这些中心。通过这种精心的设计，我们的方法可以在接受ABLE的通信成本下完成缺失模式。EXTensive experiments表明，我们提出的方法可以 дости到与理想情况（即所有客户端具有完整的模式）相同的性能。源代码将被发布。
</details></li>
</ul>
<hr>
<h2 id="Polymerized-Feature-based-Domain-Adaptation-for-Cervical-Cancer-Dose-Map-Prediction"><a href="#Polymerized-Feature-based-Domain-Adaptation-for-Cervical-Cancer-Dose-Map-Prediction" class="headerlink" title="Polymerized Feature-based Domain Adaptation for Cervical Cancer Dose Map Prediction"></a>Polymerized Feature-based Domain Adaptation for Cervical Cancer Dose Map Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10142">http://arxiv.org/abs/2308.10142</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jie Zeng, Zeyu Han, Xingchen Peng, Jianghong Xiao, Peng Wang, Yan Wang</li>
<li>for: 这篇研究是为了提高乳腺癌治疗规划的精确度，使用深度学习（DL）方法。</li>
<li>methods: 本研究使用对另一种肝癌（rectum cancer）进行了预测，并将其中学习到的丰富知识转移到乳腺癌上，以优化对乳腺癌的辐射规划预测性。</li>
<li>results: 实验结果显示，提案的方法比州先进方法更高效，并在两个实验数据集上显示出超过州先进方法的性能。<details>
<summary>Abstract</summary>
Recently, deep learning (DL) has automated and accelerated the clinical radiation therapy (RT) planning significantly by predicting accurate dose maps. However, most DL-based dose map prediction methods are data-driven and not applicable for cervical cancer where only a small amount of data is available. To address this problem, this paper proposes to transfer the rich knowledge learned from another cancer, i.e., rectum cancer, which has the same scanning area and more clinically available data, to improve the dose map prediction performance for cervical cancer through domain adaptation. In order to close the congenital domain gap between the source (i.e., rectum cancer) and the target (i.e., cervical cancer) domains, we develop an effective Transformer-based polymerized feature module (PFM), which can generate an optimal polymerized feature distribution to smoothly align the two input distributions. Experimental results on two in-house clinical datasets demonstrate the superiority of the proposed method compared with state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
To bridge the inherent domain gap between the source (rectum cancer) and target (cervical cancer) domains, we develop an effective Transformer-based polymerized feature module (PFM) that generates an optimal polymerized feature distribution to smoothly align the two input distributions. Experimental results on two in-house clinical datasets demonstrate the superiority of the proposed method compared with state-of-the-art methods.
</details></li>
</ul>
<hr>
<h2 id="Sensitivity-analysis-of-AI-based-algorithms-for-autonomous-driving-on-optical-wavefront-aberrations-induced-by-the-windshield"><a href="#Sensitivity-analysis-of-AI-based-algorithms-for-autonomous-driving-on-optical-wavefront-aberrations-induced-by-the-windshield" class="headerlink" title="Sensitivity analysis of AI-based algorithms for autonomous driving on optical wavefront aberrations induced by the windshield"></a>Sensitivity analysis of AI-based algorithms for autonomous driving on optical wavefront aberrations induced by the windshield</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11711">http://arxiv.org/abs/2308.11711</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dominik Werner Wolf, Markus Ulrich, Nikhil Kapoor</li>
<li>for: 本研究旨在解决自动驾驶视觉技术中的领域转移问题，即使用不同汽车型号和镜头配置训练模型后，在不同镜头配置下运行模型的性能下降问题。</li>
<li>methods: 本研究采用了一种基于 Fourier optics 的威胁模型，对两种视觉模型进行评估，以确定它们在不同镜头配置下的性能敏感性。</li>
<li>results: 研究结果表明，镜头配置会引入性能差距，而现有的光学指标可能不够用于评估模型在不同镜头配置下的性能。<details>
<summary>Abstract</summary>
Autonomous driving perception techniques are typically based on supervised machine learning models that are trained on real-world street data. A typical training process involves capturing images with a single car model and windshield configuration. However, deploying these trained models on different car types can lead to a domain shift, which can potentially hurt the neural networks performance and violate working ADAS requirements. To address this issue, this paper investigates the domain shift problem further by evaluating the sensitivity of two perception models to different windshield configurations. This is done by evaluating the dependencies between neural network benchmark metrics and optical merit functions by applying a Fourier optics based threat model. Our results show that there is a performance gap introduced by windshields and existing optical metrics used for posing requirements might not be sufficient.
</details>
<details>
<summary>摘要</summary>
自主驾驶感知技术通常基于指导学习模型，这些模型通常是基于实际街道数据进行训练。一般训练过程中会使用单车型和顶部配置拍摄图像。但是，将这些训练模型应用于不同车型可能会导致领域shift，这可能会影响神经网络的性能，并违反工作ADAS要求。为解决这个问题，本文进一步探讨领域shift问题，并评估两种感知模型对不同顶部配置的敏感性。我们通过应用 Fourier optics 基于威胁模型来评估神经网络指标和光学功能之间的依赖关系。我们的结果表明，顶部配置会引入性能差异，现有的光学指标可能并不够。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/20/eess.IV_2023_08_20/" data-id="clly3dw2l00ew098849009h41" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_19" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/19/cs.LG_2023_08_19/" class="article-date">
  <time datetime="2023-08-18T16:00:00.000Z" itemprop="datePublished">2023-08-19</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/19/cs.LG_2023_08_19/">cs.LG - 2023-08-19 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Efficient-Representation-Learning-for-Healthcare-with-Cross-Architectural-Self-Supervision"><a href="#Efficient-Representation-Learning-for-Healthcare-with-Cross-Architectural-Self-Supervision" class="headerlink" title="Efficient Representation Learning for Healthcare with Cross-Architectural Self-Supervision"></a>Efficient Representation Learning for Healthcare with Cross-Architectural Self-Supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10064">http://arxiv.org/abs/2308.10064</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pranavsinghps1/CASS">https://github.com/pranavsinghps1/CASS</a></li>
<li>paper_authors: Pranav Singh, Jacopo Cirrone</li>
<li>for: 这篇论文目的是提出一种新的自我超vised学习方法，以增强深度学习架构在医疗应用中的表现。</li>
<li>methods: 本文使用了一种新的siamese自我超vised学习方法，Synergistically leveraging Transformer和Convolutional Neural Networks (CNN) для高效学习。</li>
<li>results: 在四个不同的医疗数据集上，CASS-trained CNNs和Transformers已经超越了现有的自我超vised学习方法，仅使用1%的标签数据进行调整，协助得到了3.8%的平均提升。<details>
<summary>Abstract</summary>
In healthcare and biomedical applications, extreme computational requirements pose a significant barrier to adopting representation learning. Representation learning can enhance the performance of deep learning architectures by learning useful priors from limited medical data. However, state-of-the-art self-supervised techniques suffer from reduced performance when using smaller batch sizes or shorter pretraining epochs, which are more practical in clinical settings. We present Cross Architectural - Self Supervision (CASS) in response to this challenge. This novel siamese self-supervised learning approach synergistically leverages Transformer and Convolutional Neural Networks (CNN) for efficient learning. Our empirical evaluation demonstrates that CASS-trained CNNs and Transformers outperform existing self-supervised learning methods across four diverse healthcare datasets. With only 1% labeled data for finetuning, CASS achieves a 3.8% average improvement; with 10% labeled data, it gains 5.9%; and with 100% labeled data, it reaches a remarkable 10.13% enhancement. Notably, CASS reduces pretraining time by 69% compared to state-of-the-art methods, making it more amenable to clinical implementation. We also demonstrate that CASS is considerably more robust to variations in batch size and pretraining epochs, making it a suitable candidate for machine learning in healthcare applications.
</details>
<details>
<summary>摘要</summary>
在医疗和生物医学应用中，极高的计算需求成为了采用表示学习的重大障碍。表示学习可以提高深度学习架构的性能，但是现有的自我超vised学习技术在使用小批量或短时间预训练时表现不佳。为回应这个挑战，我们提出了跨建筑自我超vised学习（CASS）方法。这种新的siamese自我超vised学习方法可以有效地利用转换器和卷积神经网络（CNN）进行学习。我们的实验证明，CASS-trained CNNs和转换器在四个不同的医疗数据集上都能够超越现有的自我超vised学习方法。只有1%的标注数据进行微调，CASS可以实现3.8%的平均提高; 使用10%的标注数据，它可以获得5.9%的提高; 使用100%的标注数据，它可以达到10.13%的提高。另外，CASS可以降低预训练时间，比现有的方法减少69%，使其更适合在临床实施。我们还证明了CASS在批量大小和预训练轮次上的变化对其性能的影响较小，使其成为医学机器学习中适用的方法。
</details></li>
</ul>
<hr>
<h2 id="Accelerating-Exact-Combinatorial-Optimization-via-RL-based-Initialization-–-A-Case-Study-in-Scheduling"><a href="#Accelerating-Exact-Combinatorial-Optimization-via-RL-based-Initialization-–-A-Case-Study-in-Scheduling" class="headerlink" title="Accelerating Exact Combinatorial Optimization via RL-based Initialization – A Case Study in Scheduling"></a>Accelerating Exact Combinatorial Optimization via RL-based Initialization – A Case Study in Scheduling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11652">http://arxiv.org/abs/2308.11652</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaqi Yin, Cunxi Yu</li>
<li>for: 本研究旨在提出一种创新的方法，使用机器学习（ML）解决 combinatorial 优化问题，用调度为例子。目标是提供优化和推理的 garanties，同时保持优化算法的运行时间成本。</li>
<li>methods: 我们提出了一种两阶段 RL-to-ILP 调度框架，包括以下三个步骤：1）RL 算法 acts as coarse-grain scheduler，2）解决 relaxation 和 3）精确的解决方案 via ILP。</li>
<li>results: 我们的框架能够与使用精确调度方法具有同样的调度性能，而且可以达到 128 倍的速度提升。这些结果基于实际的 EdgeTPU 平台，使用 ImageNet DNN 计算图为输入。此外，我们的框架还可以提供更好的在芯片上的执行时间和加速。<details>
<summary>Abstract</summary>
Scheduling on dataflow graphs (also known as computation graphs) is an NP-hard problem. The traditional exact methods are limited by runtime complexity, while reinforcement learning (RL) and heuristic-based approaches struggle with determinism and solution quality. This research aims to develop an innovative approach that employs machine learning (ML) for addressing combinatorial optimization problems, using scheduling as a case study. The goal is to provide guarantees in optimality and determinism while maintaining the runtime cost of heuristic methods. Specifically, we introduce a novel two-phase RL-to-ILP scheduling framework, which includes three steps: 1) RL solver acts as coarse-grain scheduler, 2) solution relaxation and 3) exact solving via ILP. Our framework demonstrates the same scheduling performance compared with using exact scheduling methods while achieving up to 128 $\times$ speed improvements. This was conducted on actual EdgeTPU platforms, utilizing ImageNet DNN computation graphs as input. Additionally, the framework offers improved on-chip inference runtime and acceleration compared to the commercially available EdgeTPU compiler.
</details>
<details>
<summary>摘要</summary>
“计划在数据流图（也称作计算图）是一个NP困难问题。传统的精确方法受到运行时复杂性的限制，而循环学习（RL）和规则基本方法受到束缚和解决质量的影响。这项研究旨在开发一种创新的方法，使用机器学习（ML）来解决 combinatorial 优化问题，使用计划为 caso study。目标是提供优化和决定性的保证，同时保持规则基本方法的运行时成本。具体来说，我们提出了一种新的两阶段RL-to-ILP 计划框架，包括以下三个步骤：1）RL 算法作为粗粒度调度器，2）解决缓和3）使用 ILP 进行精确解决。我们的框架实现了与使用精确调度方法相同的计划性表现，同时实现了Up to 128倍的速度提升。这些实验在实际的 EdgeTPU 平台上进行，使用 ImageNet DNN 计算图作为输入。此外，我们的框架也提供了比商业可用的 EdgeTPU 编译器更好的在处理器上的执行时间和加速。”
</details></li>
</ul>
<hr>
<h2 id="The-Snowflake-Hypothesis-Training-Deep-GNN-with-One-Node-One-Receptive-field"><a href="#The-Snowflake-Hypothesis-Training-Deep-GNN-with-One-Node-One-Receptive-field" class="headerlink" title="The Snowflake Hypothesis: Training Deep GNN with One Node One Receptive field"></a>The Snowflake Hypothesis: Training Deep GNN with One Node One Receptive field</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10051">http://arxiv.org/abs/2308.10051</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kun Wang, Guohao Li, Shilong Wang, Guibin Zhang, Kai Wang, Yang You, Xiaojiang Peng, Yuxuan Liang, Yang Wang</li>
<li>for: 本研究的目的是解释深度Graph Neural Networks（GNN）在图像领域的成功，并提出一种新的概念——雪花假设，以提高GNN的可读性和通用性。</li>
<li>methods: 本研究使用了多种方法，包括不同的训练方案、不同的束深度和层数、以及不同的层数和训练集。同时，本研究还使用了 simplest gradient和node-level cosine distance来规则节点聚合的深度。</li>
<li>results: 本研究的结果表明，雪花假设可以成为一种通用的操作，可以应用于多种GNN框架，提高其效果并提供可读性和可 generalized的选择网络深度。同时，本研究还发现，采用雪花假设可以减少GNN的过拟合和过熔化问题。<details>
<summary>Abstract</summary>
Despite Graph Neural Networks demonstrating considerable promise in graph representation learning tasks, GNNs predominantly face significant issues with over-fitting and over-smoothing as they go deeper as models of computer vision realm. In this work, we conduct a systematic study of deeper GNN research trajectories. Our findings indicate that the current success of deep GNNs primarily stems from (I) the adoption of innovations from CNNs, such as residual/skip connections, or (II) the tailor-made aggregation algorithms like DropEdge. However, these algorithms often lack intrinsic interpretability and indiscriminately treat all nodes within a given layer in a similar manner, thereby failing to capture the nuanced differences among various nodes. To this end, we introduce the Snowflake Hypothesis -- a novel paradigm underpinning the concept of ``one node, one receptive field''. The hypothesis draws inspiration from the unique and individualistic patterns of each snowflake, proposing a corresponding uniqueness in the receptive fields of nodes in the GNNs.   We employ the simplest gradient and node-level cosine distance as guiding principles to regulate the aggregation depth for each node, and conduct comprehensive experiments including: (1) different training schemes; (2) various shallow and deep GNN backbones, and (3) various numbers of layers (8, 16, 32, 64) on multiple benchmarks (six graphs including dense graphs with millions of nodes); (4) compare with different aggregation strategies. The observational results demonstrate that our hypothesis can serve as a universal operator for a range of tasks, and it displays tremendous potential on deep GNNs. It can be applied to various GNN frameworks, enhancing its effectiveness when operating in-depth, and guiding the selection of the optimal network depth in an explainable and generalizable way.
</details>
<details>
<summary>摘要</summary>
尽管图像神经网络（Graph Neural Networks，GNNs）在图表学习任务上表现出了诸多承诺，但它们在深度上遇到了许多问题，如预测和泛化。在这项工作中，我们进行了系统的深度GNN研究轨迹。我们的发现表明，当前深度GNN的成功主要归功于（I） adopting innovations from CNNs，如径规/跳过连接，或（II）适应性的聚合算法，如DropEdge。但这些算法通常缺乏内在解释性，并且不具备对不同节点的细化了的认知。因此，我们提出了雪花假设——一种新的思维方式，它提出了每个节点都有独特的感受领域。这种假设 Draws inspiration from the unique and individualistic patterns of each snowflake，并提出了对节点的聚合深度进行调控的一种新的方法。我们采用 simplest gradient 和 node-level cosine distance 作为导引原则，并进行了广泛的实验，包括：（1）不同的训练方案；（2）不同的束缚和深度的 GNN 基础架构；（3）不同层数（8, 16, 32, 64）在多个 Benchmark 上进行了多种实验。我们的观察结果表明，我们的假设可以作为一种通用的操作，并且在深度GNNs中表现出了巨大的潜力。它可以应用于不同的 GNN 框架，提高其在深度下的效果，并且可以在可解释和普适的方式下选择网络的最佳层数。
</details></li>
</ul>
<hr>
<h2 id="Computing-the-Vapnik-Chervonenkis-Dimension-for-Non-Discrete-Settings"><a href="#Computing-the-Vapnik-Chervonenkis-Dimension-for-Non-Discrete-Settings" class="headerlink" title="Computing the Vapnik Chervonenkis Dimension for Non-Discrete Settings"></a>Computing the Vapnik Chervonenkis Dimension for Non-Discrete Settings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10041">http://arxiv.org/abs/2308.10041</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammed Nechba, Mouhajir Mohamed, Sedjari Yassine</li>
<li>for: 本文旨在研究一种不受概念类或其域集的约束而 compute the VC dimension的方法。</li>
<li>methods: 本文使用了Empirical Risk Minimization（ERM）学习模型，以characterize the shattering property of a concept class。</li>
<li>results: 本文提出了一种可以approximately compute the VC dimension的方法，不受概念类或其域集的约束。<details>
<summary>Abstract</summary>
In 1984, Valiant [ 7 ] introduced the Probably Approximately Correct (PAC) learning framework for boolean function classes. Blumer et al. [ 2] extended this model in 1989 by introducing the VC dimension as a tool to characterize the learnability of PAC. The VC dimension was based on the work of Vapnik and Chervonenkis in 1971 [8 ], who introduced a tool called the growth function to characterize the shattering property. Researchers have since determined the VC dimension for specific classes, and efforts have been made to develop an algorithm that can calculate the VC dimension for any concept class. In 1991, Linial, Mansour, and Rivest [4] presented an algorithm for computing the VC dimension in the discrete setting, assuming that both the concept class and domain set were finite. However, no attempts had been made to design an algorithm that could compute the VC dimension in the general setting.Therefore, our work focuses on developing a method to approximately compute the VC dimension without constraints on the concept classes or their domain set. Our approach is based on our finding that the Empirical Risk Minimization (ERM) learning paradigm can be used as a new tool to characterize the shattering property of a concept class.
</details>
<details>
<summary>摘要</summary>
在1984年，Valiant（7）提出了一种名为“Probably Approximately Correct”（PAC）学习框架，用于 boolean 函数类型。Blumer等人（2）在1989年Extension this model by introducing the VC dimension as a tool to characterize the learnability of PAC.VC dimension是基于Vapnik和Chervonenkis（8）在1971年引入的一种工具，用于 caracterize the shattering property。研究人员已经确定了特定类型的VC dimension，并且有努力开发一个可以计算VC dimension for any concept class的算法。在1991年，Linial、Mansour和Rivest（4）提出了一种算法来计算VC dimension在离散设定下，假设概念类和域集都是finite。然而，没有任何尝试过开发一个可以计算VC dimension在通用设定下的算法。因此，我们的工作是关注开发一种可以约approximately compute VC dimension的方法，不受概念类或其域集的限制。我们的方法基于我们发现，Empirical Risk Minimization（ERM）学习模式可以用作一种新的工具来characterize the shattering property of a concept class。
</details></li>
</ul>
<hr>
<h2 id="Physics-guided-training-of-GAN-to-improve-accuracy-in-airfoil-design-synthesis"><a href="#Physics-guided-training-of-GAN-to-improve-accuracy-in-airfoil-design-synthesis" class="headerlink" title="Physics-guided training of GAN to improve accuracy in airfoil design synthesis"></a>Physics-guided training of GAN to improve accuracy in airfoil design synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10038">http://arxiv.org/abs/2308.10038</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kazunari Wada, Katsuyuki Suzuki, Kazuo Yonekura</li>
<li>for: 本研究使用生成对抗网络（GAN）进行机械形状的设计合成，但GANometimes输出物理不合理的形状。例如，当GAN模型被训练以输出符合需要的 aerodynamic表现的气流形状时，会出现显著的错误。这是因为GAN模型只考虑数据，不考虑下面的物理方程。本研究提出了通过physics-guided Training来导引GAN模型学习物理有效性。</li>
<li>methods: 本研究使用physics-guided Training来导引GAN模型学习物理有效性。通过general-purpose software locate outside the neural network model来计算物理有效性。</li>
<li>results: 数字实验显示，提议的模型可以减少错误，并且输出的形状与训练集数据不同，但仍满足物理有效性，超越现有的GAN模型的限制。<details>
<summary>Abstract</summary>
Generative adversarial networks (GAN) have recently been used for a design synthesis of mechanical shapes. A GAN sometimes outputs physically unreasonable shapes. For example, when a GAN model is trained to output airfoil shapes that indicate required aerodynamic performance, significant errors occur in the performance values. This is because the GAN model only considers data but does not consider the aerodynamic equations that lie under the data. This paper proposes the physics-guided training of the GAN model to guide the model to learn physical validity. Physical validity is computed using general-purpose software located outside the neural network model. Such general-purpose software cannot be used in physics-informed neural network frameworks, because physical equations must be implemented inside the neural network models. Additionally, a limitation of generative models is that the output data are similar to the training data and cannot generate completely new shapes. However, because the proposed model is guided by a physical model and does not use a training dataset, it can generate completely new shapes. Numerical experiments show that the proposed model drastically improves the accuracy. Moreover, the output shapes differ from those of the training dataset but still satisfy the physical validity, overcoming the limitations of existing GAN models.
</details>
<details>
<summary>摘要</summary>
《生成对抗网络（GAN）在机械形状设计中的应用》，Recently, GAN has been used for mechanical shape design. However, sometimes the output shapes of GAN are physically unreasonable. For example, when training a GAN model to output airfoil shapes that meet certain aerodynamic performance requirements, significant errors can occur in the performance values. This is because the GAN model only considers the data but does not consider the underlying aerodynamic equations. This paper proposes the physics-guided training of the GAN model to ensure physical validity. Physical validity is computed using general-purpose software located outside the neural network model. Unfortunately, such general-purpose software cannot be used in physics-informed neural network frameworks, because physical equations must be implemented inside the neural network models. Additionally, a limitation of generative models is that the output data are similar to the training data and cannot generate completely new shapes. However, because the proposed model is guided by a physical model and does not use a training dataset, it can generate completely new shapes. Numerical experiments show that the proposed model significantly improves accuracy. Moreover, the output shapes differ from those of the training dataset but still satisfy physical validity, overcoming the limitations of existing GAN models.
</details></li>
</ul>
<hr>
<h2 id="High-Performance-Computing-Applied-to-Logistic-Regression-A-CPU-and-GPU-Implementation-Comparison"><a href="#High-Performance-Computing-Applied-to-Logistic-Regression-A-CPU-and-GPU-Implementation-Comparison" class="headerlink" title="High Performance Computing Applied to Logistic Regression: A CPU and GPU Implementation Comparison"></a>High Performance Computing Applied to Logistic Regression: A CPU and GPU Implementation Comparison</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10037">http://arxiv.org/abs/2308.10037</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nechbamohammed/swiftlogisticreg">https://github.com/nechbamohammed/swiftlogisticreg</a></li>
<li>paper_authors: Nechba Mohammed, Mouhajir Mohamed, Sedjari Yassine</li>
<li>for: 这个研究是为了提高大数据集的binary classification的速度，使用GPU进行并行运算。</li>
<li>methods: 这个研究使用了X. Zou等人提出的平行Gradient Descent Logistic Regression算法，并将其Directly translate到GPU上。</li>
<li>results: 实验结果显示，我们的GPU-based LR比CPU-based实现更快，具有相似的f1分数。这种加速处理大数据集的能力使得我们的方法特别有利于实时预测应用，如影像识别、垃圾邮件检测和诈欺检测。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
We present a versatile GPU-based parallel version of Logistic Regression (LR), aiming to address the increasing demand for faster algorithms in binary classification due to large data sets. Our implementation is a direct translation of the parallel Gradient Descent Logistic Regression algorithm proposed by X. Zou et al. [12]. Our experiments demonstrate that our GPU-based LR outperforms existing CPU-based implementations in terms of execution time while maintaining comparable f1 score. The significant acceleration of processing large datasets makes our method particularly advantageous for real-time prediction applications like image recognition, spam detection, and fraud detection. Our algorithm is implemented in a ready-to-use Python library available at : https://github.com/NechbaMohammed/SwiftLogisticReg
</details>
<details>
<summary>摘要</summary>
我们提出了一种高性能的GPU基于的Logistic Regression（LR）版本，用于解决由大数据集引起的快速算法需求增加。我们的实现是基于平行梯度下降Logistic Regression算法的直译，由X. Zou等人提出。我们的实验表明，我们的GPU基于LR在执行时间方面与CPU基于实现相比具有明显的优势，同时保持相似的准确率。这种加速处理大数据集的能力使我们的方法在实时预测应用，如图像识别、垃圾邮件检测和诈骗检测等方面特别有优势。我们的算法在Python中实现，可以在：https://github.com/NechbaMohammed/SwiftLogisticReg 中获取。
</details></li>
</ul>
<hr>
<h2 id="Semi-Supervised-Anomaly-Detection-for-the-Determination-of-Vehicle-Hijacking-Tweets"><a href="#Semi-Supervised-Anomaly-Detection-for-the-Determination-of-Vehicle-Hijacking-Tweets" class="headerlink" title="Semi-Supervised Anomaly Detection for the Determination of Vehicle Hijacking Tweets"></a>Semi-Supervised Anomaly Detection for the Determination of Vehicle Hijacking Tweets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10036">http://arxiv.org/abs/2308.10036</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taahir Aiyoob Patel, Clement N. Nyirenda</li>
<li>for: 这个研究旨在使用推文来识别车质事件，以帮助旅行者免受车质事件的威胁。</li>
<li>methods: 这个研究使用了两种无监督侦错算法，包括K-Nearest Neighbour (KNN) 和 Cluster Based Outlier Factor (CBLOF)，以检测推文中的偏差。</li>
<li>results: 比较研究表明，CBLOF 方法的精度为 90%，而 KNN 方法的精度为 89%。CBLOF 方法也得到了 F1-Score 的 0.8，而 KNN 方法得到了 0.78。因此，CBLOF 方法 slightly outperformed KNN 方法，并被选为车质事件推文的侦错方法。<details>
<summary>Abstract</summary>
In South Africa, there is an ever-growing issue of vehicle hijackings. This leads to travellers constantly being in fear of becoming a victim to such an incident. This work presents a new semi-supervised approach to using tweets to identify hijacking incidents by using unsupervised anomaly detection algorithms. Tweets consisting of the keyword "hijacking" are obtained, stored, and processed using the term frequency-inverse document frequency (TF-IDF) and further analyzed by using two anomaly detection algorithms: 1) K-Nearest Neighbour (KNN); 2) Cluster Based Outlier Factor (CBLOF). The comparative evaluation showed that the KNN method produced an accuracy of 89%, whereas the CBLOF produced an accuracy of 90%. The CBLOF method was also able to obtain a F1-Score of 0.8, whereas the KNN produced a 0.78. Therefore, there is a slight difference between the two approaches, in favour of CBLOF, which has been selected as a preferred unsupervised method for the determination of relevant hijacking tweets. In future, a comparison will be done between supervised learning methods and the unsupervised methods presented in this work on larger dataset. Optimisation mechanisms will also be employed in order to increase the overall performance.
</details>
<details>
<summary>摘要</summary>
在南非， vehicular hijacking 问题日益严重。这使得旅行者们constantemente在担忧成为受害者的风险。本工作提出了一种新的半监督方法，使用 Twitter 上的异常检测算法来识别劫持事件。利用 TF-IDF 加以处理的 tweets 中包含 "劫持" 关键词，并使用 KNN 和 CBLOF 两种异常检测算法进行分析。对比评估表明，KNN 方法的准确率为 89%，CBLOF 方法的准确率为 90%，CBLOF 方法还可以获得 F1-Score 0.8，而 KNN 方法的 F1-Score 为 0.78。因此，CBLOF 方法在劫持 tweets 的识别中赢得了一定的优势，因此被选为半监督方法。未来，将对大型数据集进行比较，以及使用优化机制提高总性能。
</details></li>
</ul>
<hr>
<h2 id="Effects-of-Convolutional-Autoencoder-Bottleneck-Width-on-StarGAN-based-Singing-Technique-Conversion"><a href="#Effects-of-Convolutional-Autoencoder-Bottleneck-Width-on-StarGAN-based-Singing-Technique-Conversion" class="headerlink" title="Effects of Convolutional Autoencoder Bottleneck Width on StarGAN-based Singing Technique Conversion"></a>Effects of Convolutional Autoencoder Bottleneck Width on StarGAN-based Singing Technique Conversion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10021">http://arxiv.org/abs/2308.10021</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tung-Cheng Su, Yung-Chuan Chang, Yi-Wen Liu</li>
<li>for: 将一种声音技巧转换为另一种声音技巧，保持原始歌手身份、旋律和语言元素不变</li>
<li>methods: 使用 Generative Adversarial Networks (GANs) 和 Convolutional Autoencoders (CAEs) 进行转换</li>
<li>results: 研究发现，宽度更大的瓶颈对转换质量有着良好的影响，但并不一定导致更高的类似性于目标技巧。其中，折唇声是转换最容易的目标，而其他三种声音技巧作为源则更能够生成更加吸引人的转换结果。<details>
<summary>Abstract</summary>
Singing technique conversion (STC) refers to the task of converting from one voice technique to another while leaving the original singer identity, melody, and linguistic components intact. Previous STC studies, as well as singing voice conversion research in general, have utilized convolutional autoencoders (CAEs) for conversion, but how the bottleneck width of the CAE affects the synthesis quality has not been thoroughly evaluated. To this end, we constructed a GAN-based multi-domain STC system which took advantage of the WORLD vocoder representation and the CAE architecture. We varied the bottleneck width of the CAE, and evaluated the conversion results subjectively. The model was trained on a Mandarin dataset which features four singers and four singing techniques: the chest voice, the falsetto, the raspy voice, and the whistle voice. The results show that a wider bottleneck corresponds to better articulation clarity but does not necessarily lead to higher likeness to the target technique. Among the four techniques, we also found that the whistle voice is the easiest target for conversion, while the other three techniques as a source produce more convincing conversion results than the whistle.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Semi-Implicit-Variational-Inference-via-Score-Matching"><a href="#Semi-Implicit-Variational-Inference-via-Score-Matching" class="headerlink" title="Semi-Implicit Variational Inference via Score Matching"></a>Semi-Implicit Variational Inference via Score Matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10014">http://arxiv.org/abs/2308.10014</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/longinyu/sivism">https://github.com/longinyu/sivism</a></li>
<li>paper_authors: Longlin Yu, Cheng Zhang</li>
<li>for: 提高变量家族表达力，使其能够更好地捕捉复杂的 bayesian 推理问题。</li>
<li>methods: 基于代理证明对象的得分匹配方法，利用层次结构来自然地处理不可读取的变量分布。</li>
<li>results: 与 MCMC 相当精准，并且超过 ELBO 基于的 SIVI 方法在多种 bayesian 推理任务中表现出色。<details>
<summary>Abstract</summary>
Semi-implicit variational inference (SIVI) greatly enriches the expressiveness of variational families by considering implicit variational distributions defined in a hierarchical manner. However, due to the intractable densities of variational distributions, current SIVI approaches often use surrogate evidence lower bounds (ELBOs) or employ expensive inner-loop MCMC runs for unbiased ELBOs for training. In this paper, we propose SIVI-SM, a new method for SIVI based on an alternative training objective via score matching. Leveraging the hierarchical structure of semi-implicit variational families, the score matching objective allows a minimax formulation where the intractable variational densities can be naturally handled with denoising score matching. We show that SIVI-SM closely matches the accuracy of MCMC and outperforms ELBO-based SIVI methods in a variety of Bayesian inference tasks.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate into Simplified Chinese semi-implicit variational inference (SIVI)  greatly enriches the expressiveness of variational families by considering implicit variational distributions defined in a hierarchical manner. However, due to the intractable densities of variational distributions, current SIVI approaches often use surrogate evidence lower bounds (ELBOs) or employ expensive inner-loop MCMC runs for unbiased ELBOs for training. In this paper, we propose SIVI-SM, a new method for SIVI based on an alternative training objective via score matching. Leveraging the hierarchical structure of semi-implicit variational families, the score matching objective allows a minimax formulation where the intractable variational densities can be naturally handled with denoising score matching. We show that SIVI-SM closely matches the accuracy of MCMC and outperforms ELBO-based SIVI methods in a variety of Bayesian inference tasks.Translate completed.
</details></li>
</ul>
<hr>
<h2 id="Distributionally-Robust-Cross-Subject-EEG-Decoding"><a href="#Distributionally-Robust-Cross-Subject-EEG-Decoding" class="headerlink" title="Distributionally Robust Cross Subject EEG Decoding"></a>Distributionally Robust Cross Subject EEG Decoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11651">http://arxiv.org/abs/2308.11651</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tiehang Duan, Zhenyi Wang, Gianfranco Doretto, Fang Li, Cui Tao, Donald Adjeroh</li>
<li>for: 提高 Electroencephalography (EEG) 解码任务的性能</li>
<li>methods: 使用分布robust优化和 Wasserstein gradient flow (WGF) 进行数据演化</li>
<li>results: 比基eline方法 significanly 提高解码性能，特别是在具有各种损害的 EEG 信号下<details>
<summary>Abstract</summary>
Recently, deep learning has shown to be effective for Electroencephalography (EEG) decoding tasks. Yet, its performance can be negatively influenced by two key factors: 1) the high variance and different types of corruption that are inherent in the signal, 2) the EEG datasets are usually relatively small given the acquisition cost, annotation cost and amount of effort needed. Data augmentation approaches for alleviation of this problem have been empirically studied, with augmentation operations on spatial domain, time domain or frequency domain handcrafted based on expertise of domain knowledge. In this work, we propose a principled approach to perform dynamic evolution on the data for improvement of decoding robustness. The approach is based on distributionally robust optimization and achieves robustness by optimizing on a family of evolved data distributions instead of the single training data distribution. We derived a general data evolution framework based on Wasserstein gradient flow (WGF) and provides two different forms of evolution within the framework. Intuitively, the evolution process helps the EEG decoder to learn more robust and diverse features. It is worth mentioning that the proposed approach can be readily integrated with other data augmentation approaches for further improvements. We performed extensive experiments on the proposed approach and tested its performance on different types of corrupted EEG signals. The model significantly outperforms competitive baselines on challenging decoding scenarios.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Disposable-Transfer-Learning-for-Selective-Source-Task-Unlearning"><a href="#Disposable-Transfer-Learning-for-Selective-Source-Task-Unlearning" class="headerlink" title="Disposable Transfer Learning for Selective Source Task Unlearning"></a>Disposable Transfer Learning for Selective Source Task Unlearning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09971">http://arxiv.org/abs/2308.09971</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seunghee Koh, Hyounguk Shon, Janghyeon Lee, Hyeong Gwon Hong, Junmo Kim</li>
<li>for: 本研究旨在提出一种新的转移学习方法，即可 dispose 的转移学习（DTL），以便在转移学习过程中保留目标任务的表现。</li>
<li>methods: 本研究提出了一种新的损失函数名为 Gradient Collision loss (GC loss)，用于 selectively 忘记源任务知识。 GC loss 使得梯度向量在不同批处理中的方向不同，以便减少知识泄露。</li>
<li>results: 研究表明，GC loss 是一种有效的方法来解决转移学习问题，可以保留目标任务表现，同时减少知识泄露。<details>
<summary>Abstract</summary>
Transfer learning is widely used for training deep neural networks (DNN) for building a powerful representation. Even after the pre-trained model is adapted for the target task, the representation performance of the feature extractor is retained to some extent. As the performance of the pre-trained model can be considered the private property of the owner, it is natural to seek the exclusive right of the generalized performance of the pre-trained weight. To address this issue, we suggest a new paradigm of transfer learning called disposable transfer learning (DTL), which disposes of only the source task without degrading the performance of the target task. To achieve knowledge disposal, we propose a novel loss named Gradient Collision loss (GC loss). GC loss selectively unlearns the source knowledge by leading the gradient vectors of mini-batches in different directions. Whether the model successfully unlearns the source task is measured by piggyback learning accuracy (PL accuracy). PL accuracy estimates the vulnerability of knowledge leakage by retraining the scrubbed model on a subset of source data or new downstream data. We demonstrate that GC loss is an effective approach to the DTL problem by showing that the model trained with GC loss retains the performance on the target task with a significantly reduced PL accuracy.
</details>
<details>
<summary>摘要</summary>
<<SYS>>TRANSLATE_TEXTTransfer learning 广泛用于训练深度神经网络（DNN）以建立强大的表示。即使预训练模型被适应目标任务，表示性性能 OF 特征提取器也会保持一定程度的遗传。由于预训练模型的性能可以视为专有财产，因此是自然的寻求专利权的通用性表现。 To address this issue, we propose a new paradigm of transfer learning called disposable transfer learning (DTL), which discards only the source task without degrading the performance of the target task. To achieve knowledge disposal, we propose a novel loss function named Gradient Collision loss (GC loss). GC loss selectively unlearns the source knowledge by leading the gradient vectors of mini-batches in different directions. Whether the model successfully unlearns the source task is measured by piggyback learning accuracy (PL accuracy). PL accuracy estimates the vulnerability of knowledge leakage by retraining the scrubbed model on a subset of source data or new downstream data. We demonstrate that GC loss is an effective approach to the DTL problem by showing that the model trained with GC loss retains the performance on the target task with a significantly reduced PL accuracy.Note: The translation is done using a machine translation tool, and may not be perfect or idiomatic.
</details></li>
</ul>
<hr>
<h2 id="Tackling-Vision-Language-Tasks-Through-Learning-Inner-Monologues"><a href="#Tackling-Vision-Language-Tasks-Through-Learning-Inner-Monologues" class="headerlink" title="Tackling Vision Language Tasks Through Learning Inner Monologues"></a>Tackling Vision Language Tasks Through Learning Inner Monologues</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09970">http://arxiv.org/abs/2308.09970</a></li>
<li>repo_url: None</li>
<li>paper_authors: Diji Yang, Kezhen Chen, Jinmeng Rao, Xiaoyuan Guo, Yawen Zhang, Jie Yang, Yi Zhang</li>
<li>for: 解决复杂的视觉语言问题，通过内部对话机制来协调语言模型和视觉模型。</li>
<li>methods: 提出了一种新的 Inner Monologue Multi-Modal Optimization（IMMO）方法，通过自然语言对话来促进语言模型和视觉模型之间的交互，并采用两个阶段训练方式来学习内部对话过程。</li>
<li>results: 对两个popular任务进行评估，结果表明，通过模拟内部对话机制，IMMO可以提高语义解释能力和推理能力，从而更有效地融合视觉和语言模型。此外，IMMO不需要人工定制的对话，可以在多个AI问题中广泛应用。<details>
<summary>Abstract</summary>
Visual language tasks require AI models to comprehend and reason with both visual and textual content. Driven by the power of Large Language Models (LLMs), two prominent methods have emerged: (1) the hybrid integration between LLMs and Vision-Language Models (VLMs), where visual inputs are firstly converted into language descriptions by VLMs, serving as inputs for LLMs to generate final answer(s); (2) visual feature alignment in language space, where visual inputs are encoded as embeddings and projected to LLMs' language space via further supervised fine-tuning. The first approach provides light training costs and interpretability but is hard to be optimized in an end-to-end fashion. The second approach presents decent performance, but feature alignment usually requires large amounts of training data and lacks interpretability. To tackle this dilemma, we propose a novel approach, Inner Monologue Multi-Modal Optimization (IMMO), to solve complex vision language problems by simulating inner monologue processes, a cognitive process in which an individual engages in silent verbal communication with themselves. We enable LLMs and VLMs to interact through natural language conversation and propose to use a two-stage training process to learn how to do the inner monologue (self-asking questions and answering questions). IMMO is evaluated on two popular tasks and the results suggest by emulating the cognitive phenomenon of internal dialogue, our approach can enhance reasoning and explanation abilities, contributing to the more effective fusion of vision and language models. More importantly, instead of using predefined human-crafted monologues, IMMO learns this process within the deep learning models, promising wider applicability to many different AI problems beyond vision language tasks.
</details>
<details>
<summary>摘要</summary>
“视觉语言任务需要人工智能模型理解和处理视觉和文本内容。受大语言模型（LLM）的能力驱动，两种主要方法得到推广：（1）将视觉输入转换为语言描述，并将其作为LLM的输入进行生成答案（2）在语言空间中对视觉输入进行可视化特征对齐，通过进一步的超vision fine-tuning来实现。首种方法可以减少训练成本和提高可读性，但实际上困难以在端到端方式进行优化。第二种方法可以达到可 Acceptable performance，但特征对齐通常需要大量的训练数据，并且缺乏可读性。为了解决这个困境，我们提出了一种新的方法——内部对话多模态优化（IMMO），用于解决复杂的视觉语言问题。我们通过模拟内部对话过程，让LLM和VLM之间进行自然语言交流，并提出了一个两阶段训练过程，以学习如何进行内部对话（自我问答和回答问题）。IMMO在两个流行任务上进行评估，结果表明，通过模拟内部对话，我们的方法可以提高理解和解释能力，为视觉语言模型的融合做出更有效的贡献。更重要的是，IMMO不使用预先定义的人类编写的对话，而是在深度学习模型中学习这个过程，这意味着我们的方法可以在许多不同的AI问题中应用。”
</details></li>
</ul>
<hr>
<h2 id="Anomaly-Aware-Semantic-Segmentation-via-Style-Aligned-OoD-Augmentation"><a href="#Anomaly-Aware-Semantic-Segmentation-via-Style-Aligned-OoD-Augmentation" class="headerlink" title="Anomaly-Aware Semantic Segmentation via Style-Aligned OoD Augmentation"></a>Anomaly-Aware Semantic Segmentation via Style-Aligned OoD Augmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09965">http://arxiv.org/abs/2308.09965</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dan Zhang, Kaspar Sakmann, William Beluch, Robin Hutmacher, Yumeng Li</li>
<li>for: 提高自动驾驶中的未知物体检测精度</li>
<li>methods: 使用精度更高的OoD数据生成，并提出一种简单的精度调整方法</li>
<li>results: 通过少量的微调，可以使用预训练模型进行异常检测，并保持原始任务的性能In English, this translates to:</li>
<li>for: Improving the accuracy of unknown object detection in autonomous driving</li>
<li>methods: Using higher-quality OoD data generation, and proposing a simple fine-tuning method</li>
<li>results: By minimally fine-tuning a pre-trained model, we can use it for anomaly detection while maintaining the performance on the original task.<details>
<summary>Abstract</summary>
Within the context of autonomous driving, encountering unknown objects becomes inevitable during deployment in the open world. Therefore, it is crucial to equip standard semantic segmentation models with anomaly awareness. Many previous approaches have utilized synthetic out-of-distribution (OoD) data augmentation to tackle this problem. In this work, we advance the OoD synthesis process by reducing the domain gap between the OoD data and driving scenes, effectively mitigating the style difference that might otherwise act as an obvious shortcut during training. Additionally, we propose a simple fine-tuning loss that effectively induces a pre-trained semantic segmentation model to generate a ``none of the given classes" prediction, leveraging per-pixel OoD scores for anomaly segmentation. With minimal fine-tuning effort, our pipeline enables the use of pre-trained models for anomaly segmentation while maintaining the performance on the original task.
</details>
<details>
<summary>摘要</summary>
在自动驾驶中，遇到未知对象是不可避免的，因此需要在开放世界中部署标准 semantic segmentation 模型时提供异常意识。许多先前的方法使用了 synthetic out-of-distribution（OoD）数据增强来解决这个问题。在这种工作中，我们将 OoD 数据增强过程进行了改进，将驱动场景和 OoD 数据之间的领域差减少到最小化，从而减轻了训练中可能会作为短cut的样式差异。此外，我们提议一种简单的精通化损失函数，使得先验性学习的 semantic segmentation 模型在训练中能够生成“无任何给定类”的预测，利用每个像素的 OoD 分数进行异常分 segmentation。与 minimal fine-tuning 努力相比，我们的管道可以使用先验性学习的模型进行异常分 segmentation，同时保持原始任务的性能。
</details></li>
</ul>
<hr>
<h2 id="Towards-Self-Adaptive-Machine-Learning-Enabled-Systems-Through-QoS-Aware-Model-Switching"><a href="#Towards-Self-Adaptive-Machine-Learning-Enabled-Systems-Through-QoS-Aware-Model-Switching" class="headerlink" title="Towards Self-Adaptive Machine Learning-Enabled Systems Through QoS-Aware Model Switching"></a>Towards Self-Adaptive Machine Learning-Enabled Systems Through QoS-Aware Model Switching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09960">http://arxiv.org/abs/2308.09960</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sa4s-serc/adamls">https://github.com/sa4s-serc/adamls</a></li>
<li>paper_authors: Shubham Kulkarni, Arya Marda, Karthik Vaidhyanathan</li>
<li>For: 本研究旨在提出一种机器学习模型均衡器，以管理运行时uncertainty，提高机器学习生成系统（MLS）的可靠性和性能。* Methods: 本研究提出了一种基于多模型自适应的机器学习模型均衡器（AdaMLS），通过动态模型交换来维护系统和模型的性能平衡。* Results: 通过一个基于物体检测的自适应对象检测系统的证明，研究人员发现 AdaMLS 可以在不可预测的环境下提供优化的QoS保证，比Naive和单个最佳方案更高。<details>
<summary>Abstract</summary>
Machine Learning (ML), particularly deep learning, has seen vast advancements, leading to the rise of Machine Learning-Enabled Systems (MLS). However, numerous software engineering challenges persist in propelling these MLS into production, largely due to various run-time uncertainties that impact the overall Quality of Service (QoS). These uncertainties emanate from ML models, software components, and environmental factors. Self-adaptation techniques present potential in managing run-time uncertainties, but their application in MLS remains largely unexplored. As a solution, we propose the concept of a Machine Learning Model Balancer, focusing on managing uncertainties related to ML models by using multiple models. Subsequently, we introduce AdaMLS, a novel self-adaptation approach that leverages this concept and extends the traditional MAPE-K loop for continuous MLS adaptation. AdaMLS employs lightweight unsupervised learning for dynamic model switching, thereby ensuring consistent QoS. Through a self-adaptive object detection system prototype, we demonstrate AdaMLS's effectiveness in balancing system and model performance. Preliminary results suggest AdaMLS surpasses naive and single state-of-the-art models in QoS guarantees, heralding the advancement towards self-adaptive MLS with optimal QoS in dynamic environments.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Comparison-of-Adversarial-Learning-Techniques-for-Malware-Detection"><a href="#A-Comparison-of-Adversarial-Learning-Techniques-for-Malware-Detection" class="headerlink" title="A Comparison of Adversarial Learning Techniques for Malware Detection"></a>A Comparison of Adversarial Learning Techniques for Malware Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09958">http://arxiv.org/abs/2308.09958</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pavla Louthánová, Matouš Kozák, Martin Jureček, Mark Stamp</li>
<li>for: 本文 addresses the problem of generating adversarial malware samples, specifically malicious Windows Portable Executable files, to evaluate the effectiveness of different methods in evading machine learning-based malware detection.</li>
<li>methods: 本文使用 gradient-based, evolutionary algorithm-based, 和 reinforcement-based methods to generate adversarial samples, and then tests the generated samples against selected antivirus products.</li>
<li>results: 结果显示，使用优化后的恶意软件amples可以导致 incorrectly classify the file as benign, 并且生成的恶意软件amples可以成功用于其他检测模型。使用多个生成器可以创建新的恶意软件amples，并且使用 Gym-malware generator 可以 achieve the highest practical potential.<details>
<summary>Abstract</summary>
Machine learning has proven to be a useful tool for automated malware detection, but machine learning models have also been shown to be vulnerable to adversarial attacks. This article addresses the problem of generating adversarial malware samples, specifically malicious Windows Portable Executable files. We summarize and compare work that has focused on adversarial machine learning for malware detection. We use gradient-based, evolutionary algorithm-based, and reinforcement-based methods to generate adversarial samples, and then test the generated samples against selected antivirus products. We compare the selected methods in terms of accuracy and practical applicability. The results show that applying optimized modifications to previously detected malware can lead to incorrect classification of the file as benign. It is also known that generated malware samples can be successfully used against detection models other than those used to generate them and that using combinations of generators can create new samples that evade detection. Experiments show that the Gym-malware generator, which uses a reinforcement learning approach, has the greatest practical potential. This generator achieved an average sample generation time of 5.73 seconds and the highest average evasion rate of 44.11%. Using the Gym-malware generator in combination with itself improved the evasion rate to 58.35%.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="To-prune-or-not-to-prune-A-chaos-causality-approach-to-principled-pruning-of-dense-neural-networks"><a href="#To-prune-or-not-to-prune-A-chaos-causality-approach-to-principled-pruning-of-dense-neural-networks" class="headerlink" title="To prune or not to prune : A chaos-causality approach to principled pruning of dense neural networks"></a>To prune or not to prune : A chaos-causality approach to principled pruning of dense neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09955">http://arxiv.org/abs/2308.09955</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rajan Sahu, Shivam Chadha, Nithin Nagaraj, Archana Mathur, Snehanshu Saha</li>
<li>for: 这篇论文的目的是提出一种基于 chaos in learning 的 neural network 缩小方法，以维持网络性能并且保留特征解释性。</li>
<li>methods: 本篇论文使用了 weight update 的 chaos in learning 来定义适当的缩小策略，并且通过 causality 来特定引起错分类的几个重要权重。</li>
<li>results: 根据实验结果显示，这种缩小策略可以将网络大小缩小到原来的一半，而且网络性能仍然保持在原本水准。同时，这种缩小策略仍然可以保留网络的特征解释性。<details>
<summary>Abstract</summary>
Reducing the size of a neural network (pruning) by removing weights without impacting its performance is an important problem for resource-constrained devices. In the past, pruning was typically accomplished by ranking or penalizing weights based on criteria like magnitude and removing low-ranked weights before retraining the remaining ones. Pruning strategies may also involve removing neurons from the network in order to achieve the desired reduction in network size. We formulate pruning as an optimization problem with the objective of minimizing misclassifications by selecting specific weights. To accomplish this, we have introduced the concept of chaos in learning (Lyapunov exponents) via weight updates and exploiting causality to identify the causal weights responsible for misclassification. Such a pruned network maintains the original performance and retains feature explainability.
</details>
<details>
<summary>摘要</summary>
将神经网络的大小缩小（裁剪），无需影响其表现，是资源有限设备上的重要问题。在过去，裁剪通常通过按照字段大小或字段排名来选择丢弃重要性较低的字段，然后重新训练剩下的字段。裁剪策略可能还包括从网络中移除神经元，以达到预期的网络大小增加。我们将裁剪视为一个优化问题，并通过选择特定的字段来实现最小化错分。为此，我们引入了学习中的混乱（ Lyapunov 数据）via 字段更新，并利用因果关系来识别导致错分的字段。这样的裁剪网络保持原始表现，并保留特征解释性。
</details></li>
</ul>
<hr>
<h2 id="Finding-emergence-in-data-causal-emergence-inspired-dynamics-learning"><a href="#Finding-emergence-in-data-causal-emergence-inspired-dynamics-learning" class="headerlink" title="Finding emergence in data: causal emergence inspired dynamics learning"></a>Finding emergence in data: causal emergence inspired dynamics learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09952">http://arxiv.org/abs/2308.09952</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingzhe Yang, Zhipeng Wang, Kaiwei Liu, Yingqi Rong, Bing Yuan, Jiang Zhang</li>
<li>for: 这篇论文旨在开发一种基于数据驱动的模型，用于模拟复杂的动力系统，并可以有效地捕捉出 emergent 性质。</li>
<li>methods: 该论文提出了一种基于机器学习的框架，通过最大化有效信息（EI）来学习 macro-dinamics 模型，并且可以量化 emergence 在数据中。</li>
<li>results: 实验结果表明，该框架可以成功地捕捉出 emergent 模式，并且可以学习 coarse-graining 策略和量化数据中的 causal emergence 度。此外，对于不同于训练数据集的环境进行了测试，结果表明该模型具有出色的泛化能力。<details>
<summary>Abstract</summary>
Modelling complex dynamical systems in a data-driven manner is challenging due to the presence of emergent behaviors and properties that cannot be directly captured by micro-level observational data. Therefore, it is crucial to develop a model that can effectively capture emergent dynamics at the macro-level and quantify emergence based on the available data. Drawing inspiration from the theory of causal emergence, this paper introduces a machine learning framework aimed at learning macro-dynamics within an emergent latent space. The framework achieves this by maximizing the effective information (EI) to obtain a macro-dynamics model with stronger causal effects. Experimental results on both simulated and real data demonstrate the effectiveness of the proposed framework. Not only does it successfully capture emergent patterns, but it also learns the coarse-graining strategy and quantifies the degree of causal emergence in the data. Furthermore, experiments conducted on environments different from the training dataset highlight the superior generalization ability of our model.
</details>
<details>
<summary>摘要</summary>
模拟复杂动力系统的数据驱动方法是具有挑战性的，因为存在不可直接捕捉的emergent行为和性质。因此，需要开发一个能够有效捕捉emergent dynamics的 macro-级模型，并且量化emergence基于可用的数据。 drawing inspiration from the theory of causal emergence，本文提出了一种基于机器学习的框架，用于在emergent latent space中学习macro-dynamics。该框架通过最大化有效信息（EI）来获得一个具有更强的 causal effect的macro-dynamics模型。实验结果表明，该模型不仅能成功捕捉emergent pattern，还能学习coarse-graining strategy和量化数据中的causal emergence度。此外，在不同于训练数据的环境下进行的实验还表明了我们的模型具有更高的泛化能力。
</details></li>
</ul>
<hr>
<h2 id="Study-on-the-effectiveness-of-AutoML-in-detecting-cardiovascular-disease"><a href="#Study-on-the-effectiveness-of-AutoML-in-detecting-cardiovascular-disease" class="headerlink" title="Study on the effectiveness of AutoML in detecting cardiovascular disease"></a>Study on the effectiveness of AutoML in detecting cardiovascular disease</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09947">http://arxiv.org/abs/2308.09947</a></li>
<li>repo_url: None</li>
<li>paper_authors: T. V. Afanasieva, A. P. Kuzlyakin, A. V. Komolov</li>
<li>for: 这个研究旨在开发和应用patient-oriented系统，以帮助患有 chronic noncommunicable diseases 的患者早发现和预测心血管疾病。</li>
<li>methods: 该研究使用了自动机器学习（AutoML）技术，可以简化和加速AI&#x2F;ML应用的开发过程，使得医疗专业人员可以更容易地使用这些应用。</li>
<li>results: 研究发现，自动机器学习模型在检测心血管疾病方面的准确率在87.41%至92.3%之间，最高准确率达92.3%，并且发现数据Normalization技术对模型的准确率有较大影响。<details>
<summary>Abstract</summary>
Cardiovascular diseases are widespread among patients with chronic noncommunicable diseases and are one of the leading causes of death, including in the working age. The article presents the relevance of the development and application of patient-oriented systems, in which machine learning (ML) is a promising technology that allows predicting cardiovascular diseases. Automated machine learning (AutoML) makes it possible to simplify and speed up the process of developing AI/ML applications, which is key in the development of patient-oriented systems by application users, in particular medical specialists. The authors propose a framework for the application of automatic machine learning and three scenarios that allowed for data combining five data sets of cardiovascular disease indicators from the UCI Machine Learning Repository to investigate the effectiveness in detecting this class of diseases. The study investigated one AutoML model that used and optimized the hyperparameters of thirteen basic ML models (KNeighborsUnif, KNeighborsDist, LightGBMXT, LightGBM, RandomForestGini, RandomForestEntr, CatBoost, ExtraTreesGini, ExtraTreesEntr, NeuralNetFastA, XGBoost, NeuralNetTorch, LightGBMLarge) and included the most accurate models in the weighted ensemble. The results of the study showed that the structure of the AutoML model for detecting cardiovascular diseases depends not only on the efficiency and accuracy of the basic models used, but also on the scenarios for preprocessing the initial data, in particular, on the technique of data normalization. The comparative analysis showed that the accuracy of the AutoML model in detecting cardiovascular disease varied in the range from 87.41% to 92.3%, and the maximum accuracy was obtained when normalizing the source data into binary values, and the minimum was obtained when using the built-in AutoML technique.
</details>
<details>
<summary>摘要</summary>
心血管疾病非常普遍 среди慢性非传染疾病患者，是死亡的主要原因之一，包括在工作年龄。这篇文章介绍了开发和应用 patient-oriented 系统的重要性，其中机器学习（ML）是一种承诺的技术，可以预测心血管疾病。自动机器学习（AutoML）使得开发 AI/ML 应用的过程可以简化和加速，这对医疗专业人员特别重要。作者提出了一个框架，并在五个数据集中组合了心血管疾病指标数据，以调查这类疾病的检测效果。研究中使用了一个 AutoML 模型，该模型使用和优化了十三种基本 ML 模型（KNeighborsUnif、KNeighborsDist、LightGBMXT、LightGBM、RandomForestGini、RandomForestEntr、CatBoost、ExtraTreesGini、ExtraTreesEntr、NeuralNetFastA、XGBoost、NeuralNetTorch、LightGBMLarge），并包括最佳模型在权重ensemble中。研究结果表明，AutoML 模型的结构不仅受到基本模型的效率和准确度影响，还受到数据预处理方法的选择，特别是数据normalization技术。比较分析表明，AutoML 模型在检测心血管疾病方面的准确率在87.41%到92.3%之间，最高准确率为对源数据进行二分化normalization，最低准确率为使用自动 ML 技术。
</details></li>
</ul>
<hr>
<h2 id="Dual-Branch-Deep-Learning-Network-for-Detection-and-Stage-Grading-of-Diabetic-Retinopathy"><a href="#Dual-Branch-Deep-Learning-Network-for-Detection-and-Stage-Grading-of-Diabetic-Retinopathy" class="headerlink" title="Dual Branch Deep Learning Network for Detection and Stage Grading of Diabetic Retinopathy"></a>Dual Branch Deep Learning Network for Detection and Stage Grading of Diabetic Retinopathy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09945">http://arxiv.org/abs/2308.09945</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hossein Shakibania, Sina Raoufi, Behnam Pourafkham, Hassan Khotanlou, Muharram Mansoorizadeh</li>
<li>for: 这个论文的目的是为了检测和分级糖尿病性视网膜病变，使用单一的视网膜图像。</li>
<li>methods: 这个模型使用了转移学习，使用两个现有的顶尖预训模型作为特征提取器，并对新的数据集进行精确化。</li>
<li>results: 这个模型在APTOS 2019数据集上实现了优异的表现，在糖尿病性视网膜检测和分级中，实现了98.50%的准确率、99.46%的感度和97.51%的特异度。<details>
<summary>Abstract</summary>
Diabetic retinopathy is a severe complication of diabetes that can lead to permanent blindness if not treated promptly. Early and accurate diagnosis of the disease is essential for successful treatment. This paper introduces a deep learning method for the detection and stage grading of diabetic retinopathy, using a single fundus retinal image. Our model utilizes transfer learning, employing two state-of-the-art pre-trained models as feature extractors and fine-tuning them on a new dataset. The proposed model is trained on a large multi-center dataset, including the APTOS 2019 dataset, obtained from publicly available sources. It achieves remarkable performance in diabetic retinopathy detection and stage classification on the APTOS 2019, outperforming the established literature. For binary classification, the proposed approach achieves an accuracy of 98.50%, a sensitivity of 99.46%, and a specificity of 97.51%. In stage grading, it achieves a quadratic weighted kappa of 93.00%, an accuracy of 89.60%, a sensitivity of 89.60%, and a specificity of 97.72%. The proposed approach serves as a reliable screening and stage grading tool for diabetic retinopathy, offering significant potential to enhance clinical decision-making and patient care.
</details>
<details>
<summary>摘要</summary>
糖尿病肠病是糖尿病的严重并发症，如果不及时治疗，可能会导致永久潦积。早期和准确的诊断是成功治疗的关键。本文提出了一种深度学习方法，用于检测和评分糖尿病肠病，只需一张背部照片。我们的模型使用了传输学习，使用两个国际先进的预训练模型，并对其进行精细调整。我们的模型在APTOS 2019数据集上训练，并在这个数据集上实现了糖尿病肠病检测和评分的优异表现，比Literature中的已知方法更出色。为二分类问题，我们的方法实现了98.50%的准确率，99.46%的感知率和97.51%的特异性。在评分问题上，我们的方法实现了93.00%的卷积权重κ值，89.60%的准确率，89.60%的感知率和97.72%的特异性。我们的方法可以作为糖尿病肠病检测和评分工具，为临床决策和患者护理带来了重要的可能性。
</details></li>
</ul>
<hr>
<h2 id="On-the-Robustness-of-Open-World-Test-Time-Training-Self-Training-with-Dynamic-Prototype-Expansion"><a href="#On-the-Robustness-of-Open-World-Test-Time-Training-Self-Training-with-Dynamic-Prototype-Expansion" class="headerlink" title="On the Robustness of Open-World Test-Time Training: Self-Training with Dynamic Prototype Expansion"></a>On the Robustness of Open-World Test-Time Training: Self-Training with Dynamic Prototype Expansion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09942">http://arxiv.org/abs/2308.09942</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yushu-li/owttt">https://github.com/yushu-li/owttt</a></li>
<li>paper_authors: Yushu Li, Xun Xu, Yongyi Su, Kui Jia</li>
<li>for: 该论文旨在提高 unknown 目标频谱分布下的深度学习模型的泛化性，并且具有低延迟。</li>
<li>methods: 该论文提出了一种基于 test-time training&#x2F;adaptation (TTT&#x2F;TTA) 的方法，并且对存在强度外部数据的情况进行了研究。</li>
<li>results: 该论文在 5 个 open-world test-time training (OWTTT)  benchmark 上达到了 state-of-the-art 性能，并且提出了一种 adaptive strong OOD pruning 和动态扩展 prototype 的方法来提高模型的 robustness。I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Generalizing deep learning models to unknown target domain distribution with low latency has motivated research into test-time training/adaptation (TTT/TTA). Existing approaches often focus on improving test-time training performance under well-curated target domain data. As figured out in this work, many state-of-the-art methods fail to maintain the performance when the target domain is contaminated with strong out-of-distribution (OOD) data, a.k.a. open-world test-time training (OWTTT). The failure is mainly due to the inability to distinguish strong OOD samples from regular weak OOD samples. To improve the robustness of OWTTT we first develop an adaptive strong OOD pruning which improves the efficacy of the self-training TTT method. We further propose a way to dynamically expand the prototypes to represent strong OOD samples for an improved weak/strong OOD data separation. Finally, we regularize self-training with distribution alignment and the combination yields the state-of-the-art performance on 5 OWTTT benchmarks. The code is available at https://github.com/Yushu-Li/OWTTT.
</details>
<details>
<summary>摘要</summary>
通过快速适应Unknown目标分布，深度学习模型的普及化已成为研究焦点。现有方法通常是在Well-curated目标分布数据下提高测试时训练性能。然而，这些state-of-the-art方法在Open-world测试时训练（OWTTT）中表现不佳，主要是因为无法分辨强OOD样本（out-of-distribution）和弱OOD样本（weak out-of-distribution）之间的差异。为了改进OWTTT的Robustness，我们首先开发了自适应强OOD��ppring，以提高自学习TTT方法的效果。然后，我们提议在运行时动态扩展表例，以更好地分离弱OOD样本和强OOD样本。最后，我们添加了分布对齐的REG regularization，这种组合得到了5个OWTTT标准测试 benchmarks的state-of-the-art性能。代码可以在https://github.com/Yushu-Li/OWTTT中找到。
</details></li>
</ul>
<hr>
<h2 id="Practical-Anomaly-Detection-over-Multivariate-Monitoring-Metrics-for-Online-Services"><a href="#Practical-Anomaly-Detection-over-Multivariate-Monitoring-Metrics-for-Online-Services" class="headerlink" title="Practical Anomaly Detection over Multivariate Monitoring Metrics for Online Services"></a>Practical Anomaly Detection over Multivariate Monitoring Metrics for Online Services</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09937">http://arxiv.org/abs/2308.09937</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/OpsPAI/CMAnomaly">https://github.com/OpsPAI/CMAnomaly</a></li>
<li>paper_authors: Jinyang Liu, Tianyi Yang, Zhuangbin Chen, Yuxin Su, Cong Feng, Zengyin Yang, Michael R. Lyu</li>
<li>for: 这个研究的目的是为了提出一个基于协同机器学习的异常探测框架，以便更好地探测现代软件系统中的异常情况。</li>
<li>methods: 这个研究使用了一种叫做协同机器学习的方法，以捕捉多元监控指标之间的相互依存关系，并且可以在线时间复杂度下进行效率地探测。</li>
<li>results: 实验结果显示，与现有基eline模型相比，CMAnomaly可以提高异常探测的精度，并且可以在10X到20X的速度上进行探测。此外，这个框架也在Huawei Cloud中进行了部署。<details>
<summary>Abstract</summary>
As modern software systems continue to grow in terms of complexity and volume, anomaly detection on multivariate monitoring metrics, which profile systems' health status, becomes more and more critical and challenging. In particular, the dependency between different metrics and their historical patterns plays a critical role in pursuing prompt and accurate anomaly detection. Existing approaches fall short of industrial needs for being unable to capture such information efficiently. To fill this significant gap, in this paper, we propose CMAnomaly, an anomaly detection framework on multivariate monitoring metrics based on collaborative machine. The proposed collaborative machine is a mechanism to capture the pairwise interactions along with feature and temporal dimensions with linear time complexity. Cost-effective models can then be employed to leverage both the dependency between monitoring metrics and their historical patterns for anomaly detection. The proposed framework is extensively evaluated with both public data and industrial data collected from a large-scale online service system of Huawei Cloud. The experimental results demonstrate that compared with state-of-the-art baseline models, CMAnomaly achieves an average F1 score of 0.9494, outperforming baselines by 6.77% to 10.68%, and runs 10X to 20X faster. Furthermore, we also share our experience of deploying CMAnomaly in Huawei Cloud.
</details>
<details>
<summary>摘要</summary>
现代软件系统在复杂性和规模上不断增长，异常检测在多变量监控指标上成为更加重要和挑战性的。特别是在不同指标之间的依赖关系以及历史 patrern 的情况下，异常检测变得更加重要。现有的方法无法有效地捕捉这些信息，因此在这篇论文中，我们提出了 CMAnomaly 异常检测框架，基于协同机器学习。我们的提议的协同机器是一种可以有效地捕捉多变量监控指标之间的对价关系，以及特征和时间维度的机制，具有线性时间复杂度。这使得可以使用便宜的模型来利用异常检测。我们的框架在大规模在线服务系统中进行了广泛的评估，结果显示，相比状态之前的基准模型，CMAnomaly 的平均 F1 分数为 0.9494，高于基准模型的 6.77% 到 10.68%，并且运行速度比基准模型快 10 倍到 20 倍。此外，我们还分享了在华为云上部署 CMAnomaly 的经验。
</details></li>
</ul>
<hr>
<h2 id="BLIVA-A-Simple-Multimodal-LLM-for-Better-Handling-of-Text-Rich-Visual-Questions"><a href="#BLIVA-A-Simple-Multimodal-LLM-for-Better-Handling-of-Text-Rich-Visual-Questions" class="headerlink" title="BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions"></a>BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09936">http://arxiv.org/abs/2308.09936</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mlpc-ucsd/bliva">https://github.com/mlpc-ucsd/bliva</a></li>
<li>paper_authors: Wenbo Hu, Yifan Xu, Yi Li, Weiyue Li, Zeyuan Chen, Zhuowen Tu</li>
<li>For: The paper aims to improve the ability of Vision Language Models (VLMs) to interpret images with text-rich context, which is a common occurrence in real-world scenarios.* Methods: The proposed method, called BLIVA, incorporates query embeddings from InstructBLIP and directly projects encoded patch embeddings into the LLM, a technique inspired by LLaVA. This approach helps the model capture intricate details potentially missed during the query decoding process.* Results: The proposed BLIVA model significantly enhances performance in processing text-rich VQA benchmarks (up to 17.76% in OCR-VQA benchmark) and typical VQA benchmarks (up to 7.9% in Visual Spatial Reasoning benchmark), compared to the baseline InstructBLIP. Additionally, BLIVA demonstrates significant capability in decoding real-world images, regardless of text presence.<details>
<summary>Abstract</summary>
Vision Language Models (VLMs), which extend Large Language Models (LLM) by incorporating visual understanding capability, have demonstrated significant advancements in addressing open-ended visual question-answering (VQA) tasks. However, these models cannot accurately interpret images infused with text, a common occurrence in real-world scenarios. Standard procedures for extracting information from images often involve learning a fixed set of query embeddings. These embeddings are designed to encapsulate image contexts and are later used as soft prompt inputs in LLMs. Yet, this process is limited to the token count, potentially curtailing the recognition of scenes with text-rich context. To improve upon them, the present study introduces BLIVA: an augmented version of InstructBLIP with Visual Assistant. BLIVA incorporates the query embeddings from InstructBLIP and also directly projects encoded patch embeddings into the LLM, a technique inspired by LLaVA. This approach assists the model to capture intricate details potentially missed during the query decoding process. Empirical evidence demonstrates that our model, BLIVA, significantly enhances performance in processing text-rich VQA benchmarks (up to 17.76\% in OCR-VQA benchmark) and in undertaking typical VQA benchmarks (up to 7.9\% in Visual Spatial Reasoning benchmark), comparing to our baseline InstructBLIP. BLIVA demonstrates significant capability in decoding real-world images, irrespective of text presence. To demonstrate the broad industry applications enabled by BLIVA, we evaluate the model using a new dataset comprising YouTube thumbnails paired with question-answer sets across 13 diverse categories. For researchers interested in further exploration, our code and models are freely accessible at https://github.com/mlpc-ucsd/BLIVA.git
</details>
<details>
<summary>摘要</summary>
《视觉语言模型（VLM）》，它们将大型语言模型（LLM）扩展以包含视觉理解能力，在开放式视觉问答任务（VQA）中显示出了重要的进步。然而，这些模型无法正确地解释含有文本的图像，这是现实世界中常见的情况。标准的图像信息提取方法通常包括学习固定的查询嵌入。这些嵌入用于在LLM中作为软提问输入，然而这种过程受到固定的token计数的限制，可能会遗弃场景中的文本背景。为此，本研究提出了BLIVA：一个基于InstructBLIP的增强版，它在LLM中直接将编码的质心嵌入投影到LLaVA的技术。这种方法帮助模型捕捉文本背景中的细节，可能在查询解码过程中被遗弃。empirical evidence表明，我们的模型BLIVA在处理含有文本的VQAbenchmark上（最高提升17.76%）和 Typical VQA benchmark上（最高提升7.9%）表现出色，相比基eline InstructBLIP。BLIVA在实际图像中解码表现出色，不管文本存在或不存在。为了展示BLIVA在广泛的产业应用中的应用前景，我们使用YouTube预览图片和相应的问答集来评估模型。对研究人员来说，我们在GitHub上提供了代码和模型，可以免费下载：https://github.com/mlpc-ucsd/BLIVA.git。
</details></li>
</ul>
<hr>
<h2 id="Analyzing-Quantization-in-TVM"><a href="#Analyzing-Quantization-in-TVM" class="headerlink" title="Analyzing Quantization in TVM"></a>Analyzing Quantization in TVM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10905">http://arxiv.org/abs/2308.10905</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingfei Guo</li>
<li>For: The paper aims to investigate the underperformance of 8-bit quantization in Tensor Virtual Machine (TVM) and to optimize the quantization process for deep learning models.* Methods: The paper uses TVM to perform 8-bit quantization on deep learning models and compares the performance with the non-quantized version. The authors also discuss various optimization techniques in TVM, such as graph building and memory access optimization, to improve the performance of quantized models.* Results: The paper achieves a 163.88% improvement in inference time for compute-bound tasks and a 194.98% improvement for memory-bound tasks compared to the TVM compiled baseline after addressing a bug in graph building and applying optimization strategies.<details>
<summary>Abstract</summary>
There has been many papers in academic literature on quantizing weight tensors in deep learning models to reduce inference latency and memory footprint. TVM also has the ability to quantize weights and support low-bit computations. Although quantization is typically expected to improve inference time, in TVM, the performance of 8-bit quantization does not meet the expectations. Typically, when applying 8-bit quantization to a deep learning model, it is usually expected to achieve around 50% of the full-precision inference time. However, in this particular case, not only does the quantized version fail to achieve the desired performance boost, but it actually performs worse, resulting in an inference time that is about 2 times as slow as the non-quantized version. In this project, we thoroughly investigate the reasons behind the underperformance and assess the compatibility and optimization opportunities of 8-bit quantization in TVM. We discuss the optimization of two different types of tasks: computation-bound and memory-bound, and provide a detailed comparison of various optimization techniques in TVM. Through the identification of performance issues, we have successfully improved quantization by addressing a bug in graph building. Furthermore, we analyze multiple optimization strategies to achieve the optimal quantization result. The best experiment achieves 163.88% improvement compared with the TVM compiled baseline in inference time for the compute-bound task and 194.98% for the memory-bound task.
</details>
<details>
<summary>摘要</summary>
有很多学术论文提出了深度学习模型中量化权重矩阵以降低推理时间和内存占用的想法。TVM也具有量化权重和低位计算的能力。although quantization 通常预计会改善推理时间，在 TVM 中，8 位量化的表现不符合预期。通常在应用 8 位量化深度学习模型时，预计可以达到约 50% 的全精度推理时间。但在这个特定情况下，量化版本并不只是不符合预期，而且实际更慢，导致推理时间约 double 非量化版本。在这个项目中，我们进行了深入的调查和分析，探讨 TVM 中 8 位量化的Compatibility和优化机会。我们分析了两种不同的任务类型：计算约束和内存约束，并对 TVM 中不同优化技术进行了详细的比较。通过发现性能问题，我们成功地修复了图像建立的漏洞，并分析了多种优化策略以实现最佳量化结果。最佳实验结果显示，与 TVM 编译基线相比，compute-bound 任务的推理时间提高了 163.88%，而 memory-bound 任务的推理时间提高了 194.98%。
</details></li>
</ul>
<hr>
<h2 id="East-Efficient-and-Accurate-Secure-Transformer-Framework-for-Inference"><a href="#East-Efficient-and-Accurate-Secure-Transformer-Framework-for-Inference" class="headerlink" title="East: Efficient and Accurate Secure Transformer Framework for Inference"></a>East: Efficient and Accurate Secure Transformer Framework for Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09923">http://arxiv.org/abs/2308.09923</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanchao Ding, Hua Guo, Yewei Guan, Weixin Liu, Jiarong Huo, Zhenyu Guan, Xiyong Zhang</li>
<li>for: 该论文旨在提供一个可靠和准确的安全Transformer推理框架，以保护用户的隐私。</li>
<li>methods: 该论文提出了一种新的幂等分割多项式评估算法，用于活动函数的评估，从而降低了GELU的运行时间和通信量。此外，该论文还为软max和层normalization的安全协议进行了仔细的设计，以保持所需的功能。</li>
<li>results: 该论文应用于BERT，并证明了在不需要精心调整的情况下，推理精度与明文推理相同。相比 iron，我们的方法具有1.8倍lower的通信量和1.2倍lower的运行时间。<details>
<summary>Abstract</summary>
Transformer has been successfully used in practical applications, such as ChatGPT, due to its powerful advantages. However, users' input is leaked to the model provider during the service. With people's attention to privacy, privacy-preserving Transformer inference is on the demand of such services. Secure protocols for non-linear functions are crucial in privacy-preserving Transformer inference, which are not well studied. Thus, designing practical secure protocols for non-linear functions is hard but significant to model performance. In this work, we propose a framework \emph{East} to enable efficient and accurate secure Transformer inference. Firstly, we propose a new oblivious piecewise polynomial evaluation algorithm and apply it to the activation functions, which reduces the runtime and communication of GELU by over 1.5$\times$ and 2.5$\times$, compared to prior arts. Secondly, the secure protocols for softmax and layer normalization are carefully designed to faithfully maintain the desired functionality. Thirdly, several optimizations are conducted in detail to enhance the overall efficiency. We applied \emph{East} to BERT and the results show that the inference accuracy remains consistent with the plaintext inference without fine-tuning. Compared to Iron, we achieve about 1.8$\times$ lower communication within 1.2$\times$ lower runtime.
</details>
<details>
<summary>摘要</summary>
“transformer”已经成功应用在实际应用中，例如ChatGPT，因为它具有强大的优势。然而，用户的输入会被提供者 During the service 泄露。随着人们对隐私的关注，隐私保护的transformer推理是实际的服务中的请求。对非线性函数的安全协议是这些服务中的重要课题，但是尚未得到充分的研究。因此，设计实用的安全协议 для非线性函数是具有挑战性和重要性的。在这个工作中，我们提出了一个名为“East”的框架，以实现有效和准确的隐私保护transformer推理。首先，我们提出了一个新的隐私 polynomial evaluation algorithm，并将其应用到活动函数中，这减少了GELU的runtime和通信量，相比于先前的艺术，则是1.5倍以上和2.5倍以上。其次，我们对于softmax和层Normalization进行了详细的设计，以确保所需的功能faithfully maintained。最后，我们在细节上进行了多个优化，以提高整体的效率。我们将“East”应用到BERT，结果显示，在不需要精确调整的情况下，推理精度与纯文本推理相同。相比于Iron，我们在1.2倍的runtime和1.8倍的通信量下可以 дости到相同的推理精度。”
</details></li>
</ul>
<hr>
<h2 id="EGANS-Evolutionary-Generative-Adversarial-Network-Search-for-Zero-Shot-Learning"><a href="#EGANS-Evolutionary-Generative-Adversarial-Network-Search-for-Zero-Shot-Learning" class="headerlink" title="EGANS: Evolutionary Generative Adversarial Network Search for Zero-Shot Learning"></a>EGANS: Evolutionary Generative Adversarial Network Search for Zero-Shot Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09915">http://arxiv.org/abs/2308.09915</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiming Chen, Shihuang Chen, Wenjin Hou, Weiping Ding, Xinge You</li>
<li>for: 这篇论文目的是提出了一种基于进化的对抗性探索方法（EGANS），用于实现零目标学习（ZSL）中的视觉标本生成。</li>
<li>methods: 这篇论文使用了对抗性探索（EGANS）来自动设计视觉生成网络，以提高ZSL中的视觉标本生成精度。</li>
<li>results: 实验结果显示，EGANS可以成功地对抗现有的对抗性探索方法，并在标准的CUB、SUN、AWA2和FLO datasets上实现了视觉标本生成中的改进。<details>
<summary>Abstract</summary>
Zero-shot learning (ZSL) aims to recognize the novel classes which cannot be collected for training a prediction model. Accordingly, generative models (e.g., generative adversarial network (GAN)) are typically used to synthesize the visual samples conditioned by the class semantic vectors and achieve remarkable progress for ZSL. However, existing GAN-based generative ZSL methods are based on hand-crafted models, which cannot adapt to various datasets/scenarios and fails to model instability. To alleviate these challenges, we propose evolutionary generative adversarial network search (termed EGANS) to automatically design the generative network with good adaptation and stability, enabling reliable visual feature sample synthesis for advancing ZSL. Specifically, we adopt cooperative dual evolution to conduct a neural architecture search for both generator and discriminator under a unified evolutionary adversarial framework. EGANS is learned by two stages: evolution generator architecture search and evolution discriminator architecture search. During the evolution generator architecture search, we adopt a many-to-one adversarial training strategy to evolutionarily search for the optimal generator. Then the optimal generator is further applied to search for the optimal discriminator in the evolution discriminator architecture search with a similar evolution search algorithm. Once the optimal generator and discriminator are searched, we entail them into various generative ZSL baselines for ZSL classification. Extensive experiments show that EGANS consistently improve existing generative ZSL methods on the standard CUB, SUN, AWA2 and FLO datasets. The significant performance gains indicate that the evolutionary neural architecture search explores a virgin field in ZSL.
</details>
<details>
<summary>摘要</summary>
zero-shot learning (ZSL) targets recognizing novel classes that cannot be collected for training a prediction model. Therefore, generative models (e.g., generative adversarial network (GAN)) are typically used to synthesize visual samples conditioned by the class semantic vectors and achieve remarkable progress for ZSL. However, existing GAN-based generative ZSL methods are based on hand-crafted models, which cannot adapt to various datasets/scenarios and fail to model instability. To address these challenges, we propose evolutionary generative adversarial network search (termed EGANS) to automatically design the generative network with good adaptation and stability, enabling reliable visual feature sample synthesis for advancing ZSL. Specifically, we adopt cooperative dual evolution to conduct a neural architecture search for both generator and discriminator under a unified evolutionary adversarial framework. EGANS is learned by two stages: evolution generator architecture search and evolution discriminator architecture search. During the evolution generator architecture search, we adopt a many-to-one adversarial training strategy to evolutionarily search for the optimal generator. Then the optimal generator is further applied to search for the optimal discriminator in the evolution discriminator architecture search with a similar evolution search algorithm. Once the optimal generator and discriminator are searched, we entail them into various generative ZSL baselines for ZSL classification. Extensive experiments show that EGANS consistently improve existing generative ZSL methods on the standard CUB, SUN, AWA2, and FLO datasets. The significant performance gains indicate that the evolutionary neural architecture search explores a virgin field in ZSL.
</details></li>
</ul>
<hr>
<h2 id="Never-Explore-Repeatedly-in-Multi-Agent-Reinforcement-Learning"><a href="#Never-Explore-Repeatedly-in-Multi-Agent-Reinforcement-Learning" class="headerlink" title="Never Explore Repeatedly in Multi-Agent Reinforcement Learning"></a>Never Explore Repeatedly in Multi-Agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09909">http://arxiv.org/abs/2308.09909</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenghao Li, Tonghan Wang, Chongjie Zhang, Qianchuan Zhao</li>
<li>for: 增强多智能体强化学习中的探索性能</li>
<li>methods: 提出了动态奖励缩放方法，用于稳定前期探索区域的奖励变化，促进更广泛的探索</li>
<li>results: 实验结果表明，该方法能够在Google研究足球和StarCraft II微管理任务中提高性能，特别在罕见奖励设定下<details>
<summary>Abstract</summary>
In the realm of multi-agent reinforcement learning, intrinsic motivations have emerged as a pivotal tool for exploration. While the computation of many intrinsic rewards relies on estimating variational posteriors using neural network approximators, a notable challenge has surfaced due to the limited expressive capability of these neural statistics approximators. We pinpoint this challenge as the "revisitation" issue, where agents recurrently explore confined areas of the task space. To combat this, we propose a dynamic reward scaling approach. This method is crafted to stabilize the significant fluctuations in intrinsic rewards in previously explored areas and promote broader exploration, effectively curbing the revisitation phenomenon. Our experimental findings underscore the efficacy of our approach, showcasing enhanced performance in demanding environments like Google Research Football and StarCraft II micromanagement tasks, especially in sparse reward settings.
</details>
<details>
<summary>摘要</summary>
在多代理激励学习领域，内在动机已经出现为探索的重要工具。而计算许多内在奖励的计算则依赖于使用神经网络近似器来估算变量 posterior。然而，由于神经统计近似器的表达能力有限，这导致了一种“再次访问”问题，Agent会重复探索任务空间中的封闭区域。为解决这个问题，我们提议一种动态奖励缩放方法。这种方法可以稳定在已经探索过的区域中的内在奖励的大幅波动，并促进更广泛的探索，从而控制“再次访问”现象。我们的实验发现，我们的方法在Google研究足球和星际争霸II微管理任务中表现出色，特别在罕见奖励设定下。
</details></li>
</ul>
<hr>
<h2 id="Imputing-Brain-Measurements-Across-Data-Sets-via-Graph-Neural-Networks"><a href="#Imputing-Brain-Measurements-Across-Data-Sets-via-Graph-Neural-Networks" class="headerlink" title="Imputing Brain Measurements Across Data Sets via Graph Neural Networks"></a>Imputing Brain Measurements Across Data Sets via Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09907">http://arxiv.org/abs/2308.09907</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yixin Wang, Wei Peng, Susan F. Tapert, Qingyu Zhao, Kilian M. Pohl</li>
<li>for: 这个论文的目的是为了填充公共可用的数据集中缺失的 brain Region of Interest (ROI) 测量值。</li>
<li>methods: 这个论文使用了深度学习的方法来预测缺失的测量值，包括使用图 neural network (GNN) 模型来模拟 ROI 测量值之间的依赖关系，并考虑到不同性别的脑测量值差异。</li>
<li>results: 该论文的结果表明，使用 DAGI 算法可以准确地预测缺失的 Freesurfer 测量值，并且可以考虑到不同性别的脑测量值差异。<details>
<summary>Abstract</summary>
Publicly available data sets of structural MRIs might not contain specific measurements of brain Regions of Interests (ROIs) that are important for training machine learning models. For example, the curvature scores computed by Freesurfer are not released by the Adolescent Brain Cognitive Development (ABCD) Study. One can address this issue by simply reapplying Freesurfer to the data set. However, this approach is generally computationally and labor intensive (e.g., requiring quality control). An alternative is to impute the missing measurements via a deep learning approach. However, the state-of-the-art is designed to estimate randomly missing values rather than entire measurements. We therefore propose to re-frame the imputation problem as a prediction task on another (public) data set that contains the missing measurements and shares some ROI measurements with the data sets of interest. A deep learning model is then trained to predict the missing measurements from the shared ones and afterwards is applied to the other data sets. Our proposed algorithm models the dependencies between ROI measurements via a graph neural network (GNN) and accounts for demographic differences in brain measurements (e.g. sex) by feeding the graph encoding into a parallel architecture. The architecture simultaneously optimizes a graph decoder to impute values and a classifier in predicting demographic factors. We test the approach, called Demographic Aware Graph-based Imputation (DAGI), on imputing those missing Freesurfer measurements of ABCD (N=3760) by training the predictor on those publicly released by the National Consortium on Alcohol and Neurodevelopment in Adolescence (NCANDA, N=540)...
</details>
<details>
<summary>摘要</summary>
公共可用数据集的结构MRI数据可能不包含特定的脑区域关注点（ROIs）的准确测量。例如，ABCDFreesurfer的曲线分数不会由ABCDFreesurfer发布。可以解决这个问题通过简单地重新应用Freessurfer来处理数据集。然而，这种方法通常是计算机和人工劳动（例如质量控制）的。另一种方法是使用深度学习方法进行填充。然而，现状的深度学习方法是随机缺失值的估计而不是整个测量。我们因此提议将填充问题重新定义为一个预测任务，使用另一个（公共）数据集来计算缺失测量，该数据集与数据集之间存在ROI测量的相似性。然后，我们使用深度学习模型来预测缺失测量，并将其应用于其他数据集。我们提出的算法模拟了ROI测量之间的依赖关系，使用图神经网络（GNN）来模型这些关系，同时考虑了脑测量中的人口差异（如性别），通过将图编码 feed 到平行架构中来进行考虑。该架构同时优化了图解码器来填充值，以及一个分类器来预测人口因素。我们测试了我们的方法，称为人口意识图像基于填充（DAGI），在ABCDFreesurfer中缺失的测量（N=3760）中进行填充，通过在NCANDA（N=540）中公共发布的数据集进行训练。
</details></li>
</ul>
<hr>
<h2 id="DPMAC-Differentially-Private-Communication-for-Cooperative-Multi-Agent-Reinforcement-Learning"><a href="#DPMAC-Differentially-Private-Communication-for-Cooperative-Multi-Agent-Reinforcement-Learning" class="headerlink" title="DPMAC: Differentially Private Communication for Cooperative Multi-Agent Reinforcement Learning"></a>DPMAC: Differentially Private Communication for Cooperative Multi-Agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09902">http://arxiv.org/abs/2308.09902</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/CANVOLCANO/DPMAC">https://github.com/CANVOLCANO/DPMAC</a></li>
<li>paper_authors: Canzhe Zhao, Yanjie Ze, Jing Dong, Baoxiang Wang, Shuai Li</li>
<li>for: 保护多体智能学习（MARL）中每个代理的敏感信息，以确保人工智能（AI）系统的隐私。</li>
<li>methods: 提议了一种基于（ε，δ）敏感数据隐私（DP）的差分性多体通信算法（DPMAC），每个代理都有一个本地消息发送器，并自动调整学习的消息分布，以缓解DP噪声所引起的不稳定性。</li>
<li>results: 证明了在保护隐私的情况下，协作MARL存在纳什平衡，这表示这个问题是游戏理论上可学习的。实验证明DPMAC在隐私保护场景下表现明显优于基eline方法。<details>
<summary>Abstract</summary>
Communication lays the foundation for cooperation in human society and in multi-agent reinforcement learning (MARL). Humans also desire to maintain their privacy when communicating with others, yet such privacy concern has not been considered in existing works in MARL. To this end, we propose the \textit{differentially private multi-agent communication} (DPMAC) algorithm, which protects the sensitive information of individual agents by equipping each agent with a local message sender with rigorous $(\epsilon, \delta)$-differential privacy (DP) guarantee. In contrast to directly perturbing the messages with predefined DP noise as commonly done in privacy-preserving scenarios, we adopt a stochastic message sender for each agent respectively and incorporate the DP requirement into the sender, which automatically adjusts the learned message distribution to alleviate the instability caused by DP noise. Further, we prove the existence of a Nash equilibrium in cooperative MARL with privacy-preserving communication, which suggests that this problem is game-theoretically learnable. Extensive experiments demonstrate a clear advantage of DPMAC over baseline methods in privacy-preserving scenarios.
</details>
<details>
<summary>摘要</summary>
通信layfoundationforthecooperationinhuman社会和多代理权威学习（MARL）。人们也渴望保持与他人通信时的隐私，但这一问题在现有的MARL工作中未得到考虑。为此，我们提出了《差分隐私多代理通信算法》（DPMAC），该算法保护每个代理的敏感信息，并在每个代理机器人上实现了严格（ε、δ）差分隐私（DP）保证。与直接在消息上添加固定DP噪声的常见方法不同，我们采用了每个代理机器人的本地消息发送器，并将DP要求直接 интеグinto sender，这 автоматичеamente调整了学习的消息分布，以解决由DP噪声所引起的不稳定性。此外，我们证明了在保持隐私的情况下，多代理MARL проблеma是可学习的游戏理论问题。广泛的实验表明，DPMAC在隐私保护场景下具有明显的优势。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Learning-based-Imputation-Prediction-Networks-for-In-hospital-Mortality-Risk-Modeling-using-EHRs"><a href="#Contrastive-Learning-based-Imputation-Prediction-Networks-for-In-hospital-Mortality-Risk-Modeling-using-EHRs" class="headerlink" title="Contrastive Learning-based Imputation-Prediction Networks for In-hospital Mortality Risk Modeling using EHRs"></a>Contrastive Learning-based Imputation-Prediction Networks for In-hospital Mortality Risk Modeling using EHRs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09896">http://arxiv.org/abs/2308.09896</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liulab1356/CL-ImpPreNet">https://github.com/liulab1356/CL-ImpPreNet</a></li>
<li>paper_authors: Yuxi Liu, Zhenhao Zhang, Shaowen Qin, Flora D. Salim, Antonio Jimeno Yepes</li>
<li>for: 预测医院内死亡风险基于电子医疗记录 (EHRs) 已经受到了广泛关注，以提供早期警示患者的健康状况，以便医疗专业人员能够在时间上采取措施。</li>
<li>methods: 我们的方法包括使用图分析模型来划分病人，以便只使用相似病人的信息进行缺失值填充。此外，我们还将对比学习 integrate 到我们的网络架构中，以提高病人表示学习和预测性能。</li>
<li>results: 我们的方法在两个真实的 EHR 数据集上进行实验，与当前状态的方法相比，在缺失值填充和预测任务中均有较高的性能。<details>
<summary>Abstract</summary>
Predicting the risk of in-hospital mortality from electronic health records (EHRs) has received considerable attention. Such predictions will provide early warning of a patient's health condition to healthcare professionals so that timely interventions can be taken. This prediction task is challenging since EHR data are intrinsically irregular, with not only many missing values but also varying time intervals between medical records. Existing approaches focus on exploiting the variable correlations in patient medical records to impute missing values and establishing time-decay mechanisms to deal with such irregularity. This paper presents a novel contrastive learning-based imputation-prediction network for predicting in-hospital mortality risks using EHR data. Our approach introduces graph analysis-based patient stratification modeling in the imputation process to group similar patients. This allows information of similar patients only to be used, in addition to personal contextual information, for missing value imputation. Moreover, our approach can integrate contrastive learning into the proposed network architecture to enhance patient representation learning and predictive performance on the classification task. Experiments on two real-world EHR datasets show that our approach outperforms the state-of-the-art approaches in both imputation and prediction tasks.
</details>
<details>
<summary>摘要</summary>
预测医院内死亡风险从电子医疗记录（EHR）获得了广泛关注。这种预测可以提供早期诊断病人健康状况的警示，以便医疗专业人员在时间上采取措施。这个预测任务是挑战性的，因为EHR数据本身是不规则的，有许多缺失的值和不同的时间间隔between medical records。现有的方法是利用病人医疗记录中的变量相关性来填充缺失值，并设置时间衰退机制来处理这种不规则性。本文提出了一种新的对比学习基于抽象的插值预测网络，用于预测医院内死亡风险。我们的方法包括基于图分析的病人划分模型，以组合相似病人的信息。这使得只有相似病人的信息，以及个人上下文信息，用于缺失值填充。此外，我们的方法还可以将对比学习integrated到提议的网络架构中，以提高病人表征学习和预测性能。实验结果表明，我们的方法在两个实际的EHR数据集上比状态机制方法更高。
</details></li>
</ul>
<hr>
<h2 id="Knowledge-Transfer-from-High-Resource-to-Low-Resource-Programming-Languages-for-Code-LLMs"><a href="#Knowledge-Transfer-from-High-Resource-to-Low-Resource-Programming-Languages-for-Code-LLMs" class="headerlink" title="Knowledge Transfer from High-Resource to Low-Resource Programming Languages for Code LLMs"></a>Knowledge Transfer from High-Resource to Low-Resource Programming Languages for Code LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09895">http://arxiv.org/abs/2308.09895</a></li>
<li>repo_url: None</li>
<li>paper_authors: Federico Cassano, John Gouwar, Francesca Lucchetti, Claire Schlesinger, Carolyn Jane Anderson, Michael Greenberg, Abhinav Jangda, Arjun Guha</li>
<li>for: 本文旨在提高Code LLMP的表现在低资源语言上，使其能够更好地支持低资源语言的编程。</li>
<li>methods: 本文提出了一种效果的方法，即使用半人工生成的数据来提高Code LLMP的表现。这种方法可以将高资源语言的训练数据翻译成低资源语言的训练数据，以便使用任何预训练的Code LLMP进行精度。</li>
<li>results: 本文使用MultiPL-T生成了大量的新训练数据，并对这些数据进行了验证。 results表明，通过使用MultiPL-T生成的数据，可以在Racket、OCaml和Lua等低资源语言上达到类似于高资源语言的性能。<details>
<summary>Abstract</summary>
Over the past few years, Large Language Models of Code (Code LLMs) have started to have a significant impact on programming practice. Code LLMs are also emerging as a building block for research in programming languages and software engineering. However, the quality of code produced by a Code LLM varies significantly by programming languages. Code LLMs produce impressive results on programming languages that are well represented in their training data (e.g., Java, Python, or JavaScript), but struggle with low-resource languages, like OCaml and Racket.   This paper presents an effective approach for boosting the performance of Code LLMs on low-resource languages using semi-synthetic data. Our approach generates high-quality datasets for low-resource languages, which can then be used to fine-tune any pretrained Code LLM. Our approach, called MultiPL-T, translates training data from high-resource languages into training data for low-resource languages. We apply our approach to generate tens of thousands of new, validated training items for Racket, OCaml, and Lua from Python. Moreover, we use an open dataset (The Stack) and model (StarCoderBase), which allow us to decontaminate benchmarks and train models on this data without violating the model license.   With MultiPL-T generated data, we present fine-tuned versions of StarCoderBase that achieve state-of-the-art performance for Racket, OCaml, and Lua on benchmark problems. For Lua, our fine-tuned model achieves the same performance as StarCoderBase as Python -- a very high-resource language -- on the MultiPL-E benchmarks. For Racket and OCaml, we double their performance on MultiPL-E, bringing their performance close to higher-resource languages such as Ruby and C#.
</details>
<details>
<summary>摘要</summary>
在过去几年，大型代码语言模型（Code LLM）已经开始对程序设计产生重要的影响。 Code LLM 也在程序语言和软件工程研究中出现为建筑块。然而，由 Code LLM 生成的代码质量受到程序语言的影响，高Resource语言（如 Java、Python 或 JavaScript）的代码生成印象良好，而低Resource语言（如 OCaml 和 Racket）的代码生成却受到限制。本文提出一种有效的方法，使 Code LLM 在低Resource语言上表现更好。我们的方法通过生成高质量的低Resource语言数据集来提高 Code LLM 的表现。我们的方法被称为 MultiPL-T，它将高Resource语言的训练数据翻译成低Resource语言的训练数据。我们使用 Python 等高Resource语言生成了数以千计的新的有效训练项目，并使用开放数据集（The Stack）和模型（StarCoderBase），以便在这些数据上训练模型，而不违反模型的许可证。使用 MultiPL-T 生成的数据，我们提出了一些精心调整的 StarCoderBase 模型，以实现 Racket、OCaml 和 Lua 在 benchmark 问题上的状态的表现。对 Lua，我们的调整模型与 Python 在 MultiPL-E  benchmark 上达到了同等水平的表现。对 Racket 和 OCaml，我们将其表现提高至 Ruby 和 C# 等高Resource语言水平的两倍。
</details></li>
</ul>
<hr>
<h2 id="Utilizing-Semantic-Textual-Similarity-for-Clinical-Survey-Data-Feature-Selection"><a href="#Utilizing-Semantic-Textual-Similarity-for-Clinical-Survey-Data-Feature-Selection" class="headerlink" title="Utilizing Semantic Textual Similarity for Clinical Survey Data Feature Selection"></a>Utilizing Semantic Textual Similarity for Clinical Survey Data Feature Selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09892">http://arxiv.org/abs/2308.09892</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bcwarner/sts-select">https://github.com/bcwarner/sts-select</a></li>
<li>paper_authors: Benjamin C. Warner, Ziqi Xu, Simon Haroutounian, Thomas Kannampallil, Chenyang Lu</li>
<li>for: 这篇论文的目的是提出一种基于文本名称的Feature选择方法，以提高预测结果的普遍性。</li>
<li>methods: 这篇论文使用语言模型（LM）评估文本名称之间的 semantic textual similarity（STS）分数，以选择最佳的特征集。</li>
<li>results: 研究发现，使用 STS 选择特征可以导致更高的模型性能，比较传统的特征选择算法。<details>
<summary>Abstract</summary>
Survey data can contain a high number of features while having a comparatively low quantity of examples. Machine learning models that attempt to predict outcomes from survey data under these conditions can overfit and result in poor generalizability. One remedy to this issue is feature selection, which attempts to select an optimal subset of features to learn upon. A relatively unexplored source of information in the feature selection process is the usage of textual names of features, which may be semantically indicative of which features are relevant to a target outcome. The relationships between feature names and target names can be evaluated using language models (LMs) to produce semantic textual similarity (STS) scores, which can then be used to select features. We examine the performance using STS to select features directly and in the minimal-redundancy-maximal-relevance (mRMR) algorithm. The performance of STS as a feature selection metric is evaluated against preliminary survey data collected as a part of a clinical study on persistent post-surgical pain (PPSP). The results suggest that features selected with STS can result in higher performance models compared to traditional feature selection algorithms.
</details>
<details>
<summary>摘要</summary>
survey data 可以包含大量特征，但同时只有一小部分例子。机器学习模型在这些条件下预测结果时可能会过拟合，导致泛化性差。一种解决方案是特征选择，它尝试选择最佳的特征子来学习。文本特征名称可能是semantic indicative的特征相关性的一种不常 Investigated sources of information in the feature selection process。我们使用语言模型（LM）评估特征名称和目标名称之间的语义文本相似性（STS）分数，并用这些分数选择特征。我们对直接使用STS作为特征选择度量的性能进行了评估，并与传统特征选择算法进行比较。结果表明，使用STS选择特征可以对比传统特征选择算法获得更高性能的模型。
</details></li>
</ul>
<hr>
<h2 id="Inductive-bias-Learning-Generating-Code-Models-with-Large-Language-Model"><a href="#Inductive-bias-Learning-Generating-Code-Models-with-Large-Language-Model" class="headerlink" title="Inductive-bias Learning: Generating Code Models with Large Language Model"></a>Inductive-bias Learning: Generating Code Models with Large Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09890">http://arxiv.org/abs/2308.09890</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fuyu-quant/iblm">https://github.com/fuyu-quant/iblm</a></li>
<li>paper_authors: Toma Tanaka, Naofumi Emoto, Tsukasa Yumibayashi</li>
<li>For: The paper proposes a novel method called Inductive-Bias Learning (IBL) that combines the techniques of In-Context Learning (ICL) and code generation.* Methods: The paper uses a contextual understanding to generate a code with a necessary structure for inference, leveraging the property of inference without explicit inductive bias inherent in ICL and the readability and explainability of code generation.* Results: The generated Code Models have been found to achieve predictive accuracy comparable to, and in some cases surpassing, ICL and representative machine learning models.<details>
<summary>Abstract</summary>
Large Language Models(LLMs) have been attracting attention due to a ability called in-context learning(ICL). ICL, without updating the parameters of a LLM, it is possible to achieve highly accurate inference based on rules ``in the context'' by merely inputting a training data into the prompt. Although ICL is a developing field with many unanswered questions, LLMs themselves serves as a inference model, seemingly realizing inference without explicitly indicate ``inductive bias''. On the other hand, a code generation is also a highlighted application of LLMs. The accuracy of code generation has dramatically improved, enabling even non-engineers to generate code to perform the desired tasks by crafting appropriate prompts. In this paper, we propose a novel ``learning'' method called an ``Inductive-Bias Learning (IBL)'', which combines the techniques of ICL and code generation. An idea of IBL is straightforward. Like ICL, IBL inputs a training data into the prompt and outputs a code with a necessary structure for inference (we referred to as ``Code Model'') from a ``contextual understanding''. Despite being a seemingly simple approach, IBL encompasses both a ``property of inference without explicit inductive bias'' inherent in ICL and a ``readability and explainability'' of the code generation. Surprisingly, generated Code Models have been found to achieve predictive accuracy comparable to, and in some cases surpassing, ICL and representative machine learning models. Our IBL code is open source: https://github.com/fuyu-quant/IBLM
</details>
<details>
<summary>摘要</summary>
大型语言模型(LLMs) 在 latest 时期引起了关注，主要是因为它具有一种能力 называ为 "在上下文中学习" (ICL)。 ICL 可以在不更新 LLM 参数的情况下，通过输入训练数据来实现高度准确的推理，只需要在提问中输入训练数据。虽然 ICL 是一个还未解决的问题，但 LLMS 本身就是一种推理模型，似乎不需要显式地指定 "推理偏好"。此外，代码生成也是 LLMS 的突出应用。代码生成的准确率已经提高到了非常高的水平，使得even non-engineers可以通过制定合适的提问来生成代码以执行所需的任务。在这篇论文中，我们提出了一种新的 "学习" 方法，称为 "推理偏好学习" (IBL)。 IBL 结合了 ICL 和代码生成的技术。IBL 的想法是 straightforward。与 ICL 类似，IBL 通过输入训练数据来提问，并从上下文理解中生成一个代码模型（我们称之为 "代码模型"），以实现推理。尽管看起来很简单，但 IBL 包含了 ICL 中 "推理无需显式偏好" 的性质和代码生成 "可读性和解释性"。 surprisingly，生成的代码模型已经被发现可以达到与 ICL 和代表性机器学习模型相同或更高的预测精度。我们的 IBL 代码开源在 GitHub：https://github.com/fuyu-quant/IBLM
</details></li>
</ul>
<hr>
<h2 id="DUAW-Data-free-Universal-Adversarial-Watermark-against-Stable-Diffusion-Customization"><a href="#DUAW-Data-free-Universal-Adversarial-Watermark-against-Stable-Diffusion-Customization" class="headerlink" title="DUAW: Data-free Universal Adversarial Watermark against Stable Diffusion Customization"></a>DUAW: Data-free Universal Adversarial Watermark against Stable Diffusion Customization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09889">http://arxiv.org/abs/2308.09889</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoyu Ye, Hao Huang, Jiaqi An, Yongtao Wang</li>
<li>For: The paper aims to protect a myriad of copyrighted images from different customization approaches across various versions of SD models.* Methods: The proposed approach, called invisible data-free universal adversarial watermark (DUAW), is designed to disrupt the variational autoencoder during SD customization. It operates in a data-free context, where it is trained on synthetic images produced by a Large Language Model (LLM) and a pretrained SD model.* Results: Experimental results demonstrate that DUAW can effectively distort the outputs of fine-tuned SD models, rendering them discernible to both human observers and a simple classifier.Here’s the same information in Simplified Chinese text:* For: 该研究旨在保护不同版本的SD模型自定义的数字艺术作品，以防止抄袭和侵犯版权。* Methods: 提议的方法是透明无数据自由对抗水印（DUAW），通过在自适应变换器中打乱VAR的工作方式来保护权利图像。该方法在无需直接处理版权图像的情况下进行训练，通过大自然语言模型（LLM）和预训练SD模型生成的 sintetic图像来实现。* Results: 实验结果表明，DUAW可以有效地打乱定制后的SD模型输出，使其对人类观察员和简单的分类器都可见。<details>
<summary>Abstract</summary>
Stable Diffusion (SD) customization approaches enable users to personalize SD model outputs, greatly enhancing the flexibility and diversity of AI art. However, they also allow individuals to plagiarize specific styles or subjects from copyrighted images, which raises significant concerns about potential copyright infringement. To address this issue, we propose an invisible data-free universal adversarial watermark (DUAW), aiming to protect a myriad of copyrighted images from different customization approaches across various versions of SD models. First, DUAW is designed to disrupt the variational autoencoder during SD customization. Second, DUAW operates in a data-free context, where it is trained on synthetic images produced by a Large Language Model (LLM) and a pretrained SD model. This approach circumvents the necessity of directly handling copyrighted images, thereby preserving their confidentiality. Once crafted, DUAW can be imperceptibly integrated into massive copyrighted images, serving as a protective measure by inducing significant distortions in the images generated by customized SD models. Experimental results demonstrate that DUAW can effectively distort the outputs of fine-tuned SD models, rendering them discernible to both human observers and a simple classifier.
</details>
<details>
<summary>摘要</summary>
stable diffusion（SD）自定义方法可以让用户个性化SD模型输出，大大提高AI艺术的灵活性和多样性。然而，这些自定义方法也使得个人可以复制特定风格或主题的版权图像，这引发了对可能的版权侵犯的重大担忧。为解决这个问题，我们提出了隐形数据自由 universial adversarial watermark（DUAW），以保护不同版本的SD模型在不同自定义方法下生成的多种版权图像。首先，DUAW是在SD自定义过程中打乱变量自动encoder的。其次，DUAW在无数据上下文中进行训练，使用一个大型自然语言模型（LLM）和一个预训练的SD模型生成的 sintetic图像。这种方法可以避免直接处理版权图像，从而保持其 конфиденциальность。一旦创制完成，DUAW可以隐藏地将入prise到大量版权图像中，作为一种保护措施，使得生成的图像被自定义SD模型输出的 Distortion 可见 both to human observers and a simple classifier。实验结果表明，DUAW可以有效地对精度调整后的SD模型输出进行 Distortion，使其可见 both to human observers and a simple classifier。
</details></li>
</ul>
<hr>
<h2 id="On-Estimating-the-Gradient-of-the-Expected-Information-Gain-in-Bayesian-Experimental-Design"><a href="#On-Estimating-the-Gradient-of-the-Expected-Information-Gain-in-Bayesian-Experimental-Design" class="headerlink" title="On Estimating the Gradient of the Expected Information Gain in Bayesian Experimental Design"></a>On Estimating the Gradient of the Expected Information Gain in Bayesian Experimental Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09888">http://arxiv.org/abs/2308.09888</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ziq-ao/GradEIG">https://github.com/ziq-ao/GradEIG</a></li>
<li>paper_authors: Ziqiao Ao, Jinglai Li</li>
<li>for: 本研究的目的是为bayesian inference中的实验设计优化预期信息增强(EIG)的优化问题提供方法。</li>
<li>methods: 本研究提出了两种方法来估计EIG的梯度，分别是UEEG-MCMC和BEEG-AP。UEEG-MCMC通过MCMC生成 posterior samples来估计EIG梯度，而BEEG-AP则是通过重复使用参数样本来实现高效的 simulations。</li>
<li>results: 理论分析和数值实验表明，UEEG-MCMC是具有robust性的，而BEEG-AP在EIG值小时具有更高的效率。此外，两种方法在我们的数值实验中表现了更好的性能，比如popular benchmarks。<details>
<summary>Abstract</summary>
Bayesian Experimental Design (BED), which aims to find the optimal experimental conditions for Bayesian inference, is usually posed as to optimize the expected information gain (EIG). The gradient information is often needed for efficient EIG optimization, and as a result the ability to estimate the gradient of EIG is essential for BED problems. The primary goal of this work is to develop methods for estimating the gradient of EIG, which, combined with the stochastic gradient descent algorithms, result in efficient optimization of EIG. Specifically, we first introduce a posterior expected representation of the EIG gradient with respect to the design variables. Based on this, we propose two methods for estimating the EIG gradient, UEEG-MCMC that leverages posterior samples generated through Markov Chain Monte Carlo (MCMC) to estimate the EIG gradient, and BEEG-AP that focuses on achieving high simulation efficiency by repeatedly using parameter samples. Theoretical analysis and numerical studies illustrate that UEEG-MCMC is robust agains the actual EIG value, while BEEG-AP is more efficient when the EIG value to be optimized is small. Moreover, both methods show superior performance compared to several popular benchmarks in our numerical experiments.
</details>
<details>
<summary>摘要</summary>
bayesian 实验设计（BED），旨在找到bayesian 推理中的最佳实验条件，通常是要最大化预期信息增加（EIG）的。在这种情况下，梯度信息是非常重要的，因此能够估算EIG梯度的能力是BED问题的关键。本工作的主要目标是开发一些估算EIG梯度的方法，这些方法可以与梯度下降法相结合，从而实现EIG的有效优化。首先，我们引入了 posterior 预期表示EIG梯度的关系，并对这个表示进行了分析。然后，我们提出了两种估算EIG梯度的方法：UEEG-MCMC，利用MCMC生成的 posterior 样本来估算EIG梯度，和BEEG-AP，强调在实验中实现高效率，通过重复使用参数样本来实现。我们的理论分析和数值研究表明，UEEG-MCMC 对实际的EIG值具有较高的稳定性，而BEEG-AP 在EIG值小于一定程度时具有更高的效率。此外，两种方法在我们的数值实验中都表现出了较好的性能，比如几种流行的参考方法。
</details></li>
</ul>
<hr>
<h2 id="Calibrating-Uncertainty-for-Semi-Supervised-Crowd-Counting"><a href="#Calibrating-Uncertainty-for-Semi-Supervised-Crowd-Counting" class="headerlink" title="Calibrating Uncertainty for Semi-Supervised Crowd Counting"></a>Calibrating Uncertainty for Semi-Supervised Crowd Counting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09887">http://arxiv.org/abs/2308.09887</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Li, Xiaoling Hu, Shahira Abousamra, Chao Chen</li>
<li>for: 这篇论文的目的是提出一种新的半指导人数推断方法，以提高半指导人数推断 task 的性能。</li>
<li>methods: 本篇论文使用了一种基于 uncertainty 的 iterative pseudo-labeling 方法，通过一个 surrogate function 来训练模型，以控制模型的 uncertainty。</li>
<li>results: 本篇论文的结果显示，使用 proposed 方法可以生成可靠的 uncertainty estimation，高品质的 pseudo-labels，并 achieved state-of-the-art performance in semisupervised crowd counting task。<details>
<summary>Abstract</summary>
Semi-supervised crowd counting is an important yet challenging task. A popular approach is to iteratively generate pseudo-labels for unlabeled data and add them to the training set. The key is to use uncertainty to select reliable pseudo-labels. In this paper, we propose a novel method to calibrate model uncertainty for crowd counting. Our method takes a supervised uncertainty estimation strategy to train the model through a surrogate function. This ensures the uncertainty is well controlled throughout the training. We propose a matching-based patch-wise surrogate function to better approximate uncertainty for crowd counting tasks. The proposed method pays a sufficient amount of attention to details, while maintaining a proper granularity. Altogether our method is able to generate reliable uncertainty estimation, high quality pseudolabels, and achieve state-of-the-art performance in semisupervised crowd counting.
</details>
<details>
<summary>摘要</summary>
半指导的人群计数是一项重要但又具有挑战性的任务。一种受欢迎的方法是通过逐步生成 pseudo-标签 для无标签数据并将其添加到训练集中。关键在于使用uncertainty来选择可靠的 pseudo-标签。在这篇论文中，我们提出了一种新的方法来准确控制模型的uncertainty。我们使用一种监督型 uncertainty estimation 策略来训练模型，并使用一个 matching-based patch-wise 替换函数来更好地估计uncertainty。我们的方法具有着充分的注意力于细节，同时保持合理的粒度。总的来说，我们的方法可以生成可靠的 uncertainty estimation，高质量的 pseudo-标签，并实现semisupervised人群计数中的顶峰性能。
</details></li>
</ul>
<hr>
<h2 id="A-Transformer-based-Framework-For-Multi-variate-Time-Series-A-Remaining-Useful-Life-Prediction-Use-Case"><a href="#A-Transformer-based-Framework-For-Multi-variate-Time-Series-A-Remaining-Useful-Life-Prediction-Use-Case" class="headerlink" title="A Transformer-based Framework For Multi-variate Time Series: A Remaining Useful Life Prediction Use Case"></a>A Transformer-based Framework For Multi-variate Time Series: A Remaining Useful Life Prediction Use Case</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09884">http://arxiv.org/abs/2308.09884</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oluwaseyi Ogunfowora, Homayoun Najjaran</li>
<li>for: 这个研究旨在提出一个基于encoder-transformer架构的多变量时间序列预测框架，用于预测机器的剩下有用生命时间（RUL）。</li>
<li>methods: 本研究使用了transformer模型，并进行了三个模型特有的实验，以将transformer模型从自然语言领域转移到时间序列领域。此外，本研究还提出了一个新的扩展窗口方法，以帮助模型识别机器的初期阶段和衰退路径。</li>
<li>results: 根据所有C-MAPPSbenchmark dataset上的四个集，这个提案的encoder-transformer模型的预测性能大幅提高，与13个现有的州际之最（SOTA）模型相比，平均提高137.65%。<details>
<summary>Abstract</summary>
In recent times, Large Language Models (LLMs) have captured a global spotlight and revolutionized the field of Natural Language Processing. One of the factors attributed to the effectiveness of LLMs is the model architecture used for training, transformers. Transformer models excel at capturing contextual features in sequential data since time series data are sequential, transformer models can be leveraged for more efficient time series data prediction. The field of prognostics is vital to system health management and proper maintenance planning. A reliable estimation of the remaining useful life (RUL) of machines holds the potential for substantial cost savings. This includes avoiding abrupt machine failures, maximizing equipment usage, and serving as a decision support system (DSS). This work proposed an encoder-transformer architecture-based framework for multivariate time series prediction for a prognostics use case. We validated the effectiveness of the proposed framework on all four sets of the C-MAPPS benchmark dataset for the remaining useful life prediction task. To effectively transfer the knowledge and application of transformers from the natural language domain to time series, three model-specific experiments were conducted. Also, to enable the model awareness of the initial stages of the machine life and its degradation path, a novel expanding window method was proposed for the first time in this work, it was compared with the sliding window method, and it led to a large improvement in the performance of the encoder transformer model. Finally, the performance of the proposed encoder-transformer model was evaluated on the test dataset and compared with the results from 13 other state-of-the-art (SOTA) models in the literature and it outperformed them all with an average performance increase of 137.65% over the next best model across all the datasets.
</details>
<details>
<summary>摘要</summary>
To address this challenge, this work proposes an encoder-transformer architecture-based framework for multivariate time series prediction in a prognostics use case. The proposed framework was validated on four datasets from the C-MAPPS benchmark, and three model-specific experiments were conducted to transfer knowledge from the natural language domain to time series. Additionally, a novel expanding window method was proposed to improve the model's awareness of the initial stages of machine life and its degradation path.The proposed encoder-transformer model outperformed 13 other state-of-the-art (SOTA) models in the literature with an average performance increase of 137.65% over the next best model across all datasets. This demonstrates the effectiveness of the proposed framework and the potential of transformer models for time series prediction in prognostics.
</details></li>
</ul>
<hr>
<h2 id="Flamingo-Multi-Round-Single-Server-Secure-Aggregation-with-Applications-to-Private-Federated-Learning"><a href="#Flamingo-Multi-Round-Single-Server-Secure-Aggregation-with-Applications-to-Private-Federated-Learning" class="headerlink" title="Flamingo: Multi-Round Single-Server Secure Aggregation with Applications to Private Federated Learning"></a>Flamingo: Multi-Round Single-Server Secure Aggregation with Applications to Private Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09883">http://arxiv.org/abs/2308.09883</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/eniac/flamingo">https://github.com/eniac/flamingo</a></li>
<li>paper_authors: Yiping Ma, Jess Woods, Sebastian Angel, Antigoni Polychroniadou, Tal Rabin</li>
<li>for: 这篇论文介绍了一种用于安全聚合数据的系统，以便在大量客户端上进行训练。</li>
<li>methods: 该系统使用了一种新的轻量级Dropout鲁棒性协议，以确保如果客户端在聚合过程中离开，服务器仍然可以获得有意义的结果。此外，它还引入了一种新的客户端 neighboorhood选择方法。</li>
<li>results: 作者们实现并评估了Flamingo系统，并证明了它可以安全地训练基于MNIST和CIFAR-100数据集的神经网络模型，并且模型的学习结果与非私有 Federated Learning 系统相同。<details>
<summary>Abstract</summary>
This paper introduces Flamingo, a system for secure aggregation of data across a large set of clients. In secure aggregation, a server sums up the private inputs of clients and obtains the result without learning anything about the individual inputs beyond what is implied by the final sum. Flamingo focuses on the multi-round setting found in federated learning in which many consecutive summations (averages) of model weights are performed to derive a good model. Previous protocols, such as Bell et al. (CCS '20), have been designed for a single round and are adapted to the federated learning setting by repeating the protocol multiple times. Flamingo eliminates the need for the per-round setup of previous protocols, and has a new lightweight dropout resilience protocol to ensure that if clients leave in the middle of a sum the server can still obtain a meaningful result. Furthermore, Flamingo introduces a new way to locally choose the so-called client neighborhood introduced by Bell et al. These techniques help Flamingo reduce the number of interactions between clients and the server, resulting in a significant reduction in the end-to-end runtime for a full training session over prior work. We implement and evaluate Flamingo and show that it can securely train a neural network on the (Extended) MNIST and CIFAR-100 datasets, and the model converges without a loss in accuracy, compared to a non-private federated learning system.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:这篇论文介绍了Flamingo系统，用于在多个客户端上安全地汇集数据。在安全汇集中，服务器将客户端的私有输入汇集起来，而不会学习到每个输入的细节，只知道最终结果的含义。Flamingo专注于联合学习中的多轮设定，在多个汇集（平均）后 derivate 出一个好的模型。先前的协议，如Bell et al. (CCS '20)，已经为单轮设定而设计，并在联合学习设定中重复协议多次。Flamingo消除了先前协议的每轮设定需求，并 introduce 了一种轻量级的dropout鲁棒性协议，以保证如果客户端在汇集过程中离开，服务器仍然可以获得有意义的结果。此外，Flamingo引入了一种新的客户端选择方法，以及 Bell et al. 所引入的客户端社区。这些技术帮助Flamingo减少客户端和服务器之间的交互数量，从而实现了与先前工作相比的很大减少。我们实现和评估了Flamingo，并证明它可以安全地训练（扩展）MNIST和CIFAR-100数据集上的神经网络模型，模型也可以在私有化联合学习环境中减少准确性损失。
</details></li>
</ul>
<hr>
<h2 id="Generative-Adversarial-Networks-Unlearning"><a href="#Generative-Adversarial-Networks-Unlearning" class="headerlink" title="Generative Adversarial Networks Unlearning"></a>Generative Adversarial Networks Unlearning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09881">http://arxiv.org/abs/2308.09881</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hui Sun, Tianqing Zhu, Wenhan Chang, Wanlei Zhou<br>for: This paper focuses on the issue of unlearning in Generative Adversarial Networks (GANs), specifically addressing the challenges of generator unlearning and defining a criterion for the discriminator.methods: The authors propose a cascaded unlearning approach that utilizes a substitution mechanism and fake label to mitigate the challenges of generator unlearning.results: The proposed approach achieves significantly improved item and class unlearning efficiency, reducing the required time by up to 185x and 284x for the MNIST and CIFAR-10 datasets, respectively, compared to retraining from scratch. Additionally, the model’s performance experiences only minor degradation after unlearning, and has no adverse effects on downstream tasks such as classification.Here is the result in Simplified Chinese text:for: 这篇论文关注在生成 adversarial 网络（GANs）中的快速学习问题，特别是生成器快速学习的挑战和定义验证器的标准。methods: 作者提出了一种叠加快速学习方法，利用替换机制和假标签来解决生成器快速学习的挑战。results: 提议的方法在MNIST和CIFAR-10 datasets上实现了明显的项目和类快速学习效率提高，比重新训练从头开始的时间减少了185倍和284倍。此外，模型之后学习后的性能只受到了微小的降低，与只需要64个图像相比，无法对下游任务如分类造成负面影响。<details>
<summary>Abstract</summary>
As machine learning continues to develop, and data misuse scandals become more prevalent, individuals are becoming increasingly concerned about their personal information and are advocating for the right to remove their data. Machine unlearning has emerged as a solution to erase training data from trained machine learning models. Despite its success in classifiers, research on Generative Adversarial Networks (GANs) is limited due to their unique architecture, including a generator and a discriminator. One challenge pertains to generator unlearning, as the process could potentially disrupt the continuity and completeness of the latent space. This disruption might consequently diminish the model's effectiveness after unlearning. Another challenge is how to define a criterion that the discriminator should perform for the unlearning images. In this paper, we introduce a substitution mechanism and define a fake label to effectively mitigate these challenges. Based on the substitution mechanism and fake label, we propose a cascaded unlearning approach for both item and class unlearning within GAN models, in which the unlearning and learning processes run in a cascaded manner. We conducted a comprehensive evaluation of the cascaded unlearning technique using the MNIST and CIFAR-10 datasets. Experimental results demonstrate that this approach achieves significantly improved item and class unlearning efficiency, reducing the required time by up to 185x and 284x for the MNIST and CIFAR-10 datasets, respectively, in comparison to retraining from scratch. Notably, although the model's performance experiences minor degradation after unlearning, this reduction is negligible when dealing with a minimal number of images (e.g., 64) and has no adverse effects on downstream tasks such as classification.
</details>
<details>
<summary>摘要</summary>
随着机器学习技术的不断发展，个人数据泄露事件的出现也越来越普遍，人们对自己的个人信息越来越关注，并且呼吁保护自己的数据权利。机器学习模型中的数据解启（unlearning）技术已成为一种解决方案，可以将训练数据从已经训练过的机器学习模型中除去。然而，对于生成器（generator）和判别器（discriminator）的特殊架构，研究生成对抗网络（GANs）的unlearning却受到了限制。一个挑战在生成器unlearning中，即可能导致生成器的维度空间中断和不连续，从而影响模型的效果。另一个挑战是如何定义判别器对unlearning图像的标准。在本文中，我们提出了替换机制和假标签，以解决这些挑战。基于替换机制和假标签，我们提议一种叠加式unlearning方法，在GAN模型中进行项和类unlearning。我们在MNIST和CIFAR-10 datasets上进行了广泛的评估，结果表明，这种方法可以大幅提高item和类unlearning效率，比 retraining from scratch 需要的时间减少至多达185倍和284倍。尤其是，模型性能减少后仍然保持可观，只有当处理少量图像（例如64）时，这种减少才会导致轻微的性能下降，无法影响下游任务 such as classification。
</details></li>
</ul>
<hr>
<h2 id="DatasetEquity-Are-All-Samples-Created-Equal-In-The-Quest-For-Equity-Within-Datasets"><a href="#DatasetEquity-Are-All-Samples-Created-Equal-In-The-Quest-For-Equity-Within-Datasets" class="headerlink" title="DatasetEquity: Are All Samples Created Equal? In The Quest For Equity Within Datasets"></a>DatasetEquity: Are All Samples Created Equal? In The Quest For Equity Within Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09878">http://arxiv.org/abs/2308.09878</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/towardsautonomy/datasetequity">https://github.com/towardsautonomy/datasetequity</a></li>
<li>paper_authors: Shubham Shrivastava, Xianling Zhang, Sushruth Nagesh, Armin Parchami</li>
<li>for: Addressing data imbalance in machine learning, particularly in computer vision tasks.</li>
<li>methods: Using deep perceptual embeddings and clustering to compute sample likelihoods, and proposing a novel $\textbf{Generalized Focal Loss}$ function to weigh samples differently during training.</li>
<li>results: Achieving over $200%$ AP gains on under-represented classes (Cyclist) in the KITTI dataset, and demonstrating the method’s effectiveness across autonomous driving vision datasets including nuScenes.Here’s the full summary in Simplified Chinese:</li>
<li>for: 本研究旨在解决机器学习中的数据不均衡问题，特别是计算机视觉任务中的数据不均衡问题。</li>
<li>methods: 使用深度感知嵌入和聚类计算样本可能性，并提出一种新的$\textbf{扩展Focus损失函数}$来在训练中不同样本的权重。</li>
<li>results: 在KITTI数据集中，对于不足 Represented 类（自行车手）的AP得分提高了超过200%，并在自动驾驶视觉数据集（包括nuScenes）中证明了方法的一致性和通用性。<details>
<summary>Abstract</summary>
Data imbalance is a well-known issue in the field of machine learning, attributable to the cost of data collection, the difficulty of labeling, and the geographical distribution of the data. In computer vision, bias in data distribution caused by image appearance remains highly unexplored. Compared to categorical distributions using class labels, image appearance reveals complex relationships between objects beyond what class labels provide. Clustering deep perceptual features extracted from raw pixels gives a richer representation of the data. This paper presents a novel method for addressing data imbalance in machine learning. The method computes sample likelihoods based on image appearance using deep perceptual embeddings and clustering. It then uses these likelihoods to weigh samples differently during training with a proposed $\textbf{Generalized Focal Loss}$ function. This loss can be easily integrated with deep learning algorithms. Experiments validate the method's effectiveness across autonomous driving vision datasets including KITTI and nuScenes. The loss function improves state-of-the-art 3D object detection methods, achieving over $200\%$ AP gains on under-represented classes (Cyclist) in the KITTI dataset. The results demonstrate the method is generalizable, complements existing techniques, and is particularly beneficial for smaller datasets and rare classes. Code is available at: https://github.com/towardsautonomy/DatasetEquity
</details>
<details>
<summary>摘要</summary>
“数据不匹配是机器学习领域的一个公认问题，这可以归因于数据收集的成本、标签的困难以及数据的地域分布。在计算机视觉领域，图像的外观偏见对数据分布的偏见尚未得到充分的探讨。相比于使用类别标签的分布，图像的外观 revelas了对象之间复杂的关系，这些关系超出了类别标签所提供的信息。使用深度感知特征提取自原始像素的归一化可以为数据提供更加富有的表示。本文提出了一种 novel 的数据不匹配解决方法，该方法使用图像外观的深度感知嵌入和归一化计算样本的可能性。然后，使用这些可能性来调整样本的权重，并使用提议的 $\textbf{通用强调损失}$ 函数进行训练。这个损失函数可以轻松地与深度学习算法结合使用。实验证明了该方法在自动驾驶视觉 datasets 中的效果，包括 KITTI 和 nuScenes。该损失函数可以提高 state-of-the-art 3D 物体检测方法的性能，在 KITTI dataset 中Cyclist 类型的下 Represented 类型中获得了更 than 200% AP 提升。结果表明该方法是通用的，可以补充现有的技术，特别是对小型 datasets 和罕见类型的支持。代码可以在：https://github.com/towardsautonomy/DatasetEquity 中找到。”Note that Simplified Chinese is used in the translation, as it is more widely used in mainland China and is the standard language used in most online platforms and publications. If you prefer Traditional Chinese, I can provide that version as well.
</details></li>
</ul>
<hr>
<h2 id="Skill-Transformer-A-Monolithic-Policy-for-Mobile-Manipulation"><a href="#Skill-Transformer-A-Monolithic-Policy-for-Mobile-Manipulation" class="headerlink" title="Skill Transformer: A Monolithic Policy for Mobile Manipulation"></a>Skill Transformer: A Monolithic Policy for Mobile Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09873">http://arxiv.org/abs/2308.09873</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoyu Huang, Dhruv Batra, Akshara Rai, Andrew Szot</li>
<li>for: 解决长期机器人任务， combining conditional sequence modeling 和技能归一化。</li>
<li>methods: 使用 transformer 架构，通过示例轨迹进行端到端训练，预测高级技能和全身低级动作。</li>
<li>results: 在embodied rearrangement benchmark上测试， Skill Transformer 可以实现 robust 任务规划和低级控制，成功率高于基eline 2.5倍。<details>
<summary>Abstract</summary>
We present Skill Transformer, an approach for solving long-horizon robotic tasks by combining conditional sequence modeling and skill modularity. Conditioned on egocentric and proprioceptive observations of a robot, Skill Transformer is trained end-to-end to predict both a high-level skill (e.g., navigation, picking, placing), and a whole-body low-level action (e.g., base and arm motion), using a transformer architecture and demonstration trajectories that solve the full task. It retains the composability and modularity of the overall task through a skill predictor module while reasoning about low-level actions and avoiding hand-off errors, common in modular approaches. We test Skill Transformer on an embodied rearrangement benchmark and find it performs robust task planning and low-level control in new scenarios, achieving a 2.5x higher success rate than baselines in hard rearrangement problems.
</details>
<details>
<summary>摘要</summary>
我们提出了技能变换器，一种解决长期机器人任务的方法，结合条件序列模型和技能分解性。基于机器人 egocentric 和 proprioceptive 观察，技能变换器通过 transformer 架构进行端到端训练，预测高级技能（例如导航、捕捉、放置）和整体低级动作（例如基底和臂动作），使用示例轨迹解决整个任务。它保留了总任务的可组合性和分解性，通过技能预测模块进行低级动作的编制和避免手动错误，常见于模块化方法。我们在embodied重新排序测试上测试了技能变换器，发现它在新的情况下能够做出坚固的任务规划和低级控制，比基eline高2.5倍成功率。
</details></li>
</ul>
<hr>
<h2 id="Tensor-Compressed-Back-Propagation-Free-Training-for-Physics-Informed-Neural-Networks"><a href="#Tensor-Compressed-Back-Propagation-Free-Training-for-Physics-Informed-Neural-Networks" class="headerlink" title="Tensor-Compressed Back-Propagation-Free Training for (Physics-Informed) Neural Networks"></a>Tensor-Compressed Back-Propagation-Free Training for (Physics-Informed) Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09858">http://arxiv.org/abs/2308.09858</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yequan Zhao, Xinling Yu, Zhixiong Chen, Ziyue Liu, Sijia Liu, Zheng Zhang</li>
<li>for:  This paper aims to present a completely backward propagation (BP)-free framework for training realistic neural networks on edge devices, which can greatly improve the design complexity and time-to-market of on-device training accelerators.</li>
<li>methods:  The paper proposes a three-fold technical contribution to achieve BP-free training: (1) a tensor-compressed variance reduction approach to improve the scalability of zeroth-order (ZO) optimization, (2) a hybrid gradient evaluation approach to improve the efficiency of ZO training, and (3) an extension of the BP-free training framework to physics-informed neural networks (PINNs) using a sparse-grid approach to estimate derivatives without BP.</li>
<li>results:  The paper shows that the proposed BP-free training framework only loses little accuracy on the MNIST dataset compared with standard first-order training, and successfully trains a PINN for solving a 20-dim Hamiltonian-Jacobi-Bellman PDE. The memory-efficient and BP-free approach may serve as a foundation for the near-future on-device training on many resource-constraint platforms (e.g., FPGA, ASIC, micro-controllers, and photonic chips).<details>
<summary>Abstract</summary>
Backward propagation (BP) is widely used to compute the gradients in neural network training. However, it is hard to implement BP on edge devices due to the lack of hardware and software resources to support automatic differentiation. This has tremendously increased the design complexity and time-to-market of on-device training accelerators. This paper presents a completely BP-free framework that only requires forward propagation to train realistic neural networks. Our technical contributions are three-fold. Firstly, we present a tensor-compressed variance reduction approach to greatly improve the scalability of zeroth-order (ZO) optimization, making it feasible to handle a network size that is beyond the capability of previous ZO approaches. Secondly, we present a hybrid gradient evaluation approach to improve the efficiency of ZO training. Finally, we extend our BP-free training framework to physics-informed neural networks (PINNs) by proposing a sparse-grid approach to estimate the derivatives in the loss function without using BP. Our BP-free training only loses little accuracy on the MNIST dataset compared with standard first-order training. We also demonstrate successful results in training a PINN for solving a 20-dim Hamiltonian-Jacobi-Bellman PDE. This memory-efficient and BP-free approach may serve as a foundation for the near-future on-device training on many resource-constraint platforms (e.g., FPGA, ASIC, micro-controllers, and photonic chips).
</details>
<details>
<summary>摘要</summary>
<<SYS>>用简化中文表示大多数 neural network 训练使用倒推 propagation (BP) 计算梯度，但在边缘设备上实现 BP 具有硬件和软件资源的限制，导致训练减速器的设计复杂度和时间到市场增加。这篇论文提出了一个完全无需 BP 的框架，只需要前向传播来训练真实的 neural network。我们的技术贡献包括以下三个方面：1. 我们提出了一种紧凑变量 reduction 技术，以提高 zero-order (ZO) 优化的扩展性，使得可以处理的网络大小超出了先前 ZO 方法的能力。2. 我们提出了一种混合式梯度评估方法，以提高 ZO 训练的效率。3. 我们将我们的 BP-free 训练框架应用到物理学 informed neural networks (PINNs) 中，提出了一种稀疏网格方法，以无需 BP 来估算损失函数中的导数。我们的 BP-free 训练只在 MNIST 数据集上减少了一些精度，与标准首次训练相比。我们还成功地训练了一个 PINN 来解决一个 20 维 Hamiltonian-Jacobi-Bellman PDE。这种内存有效并且 BP-free 的方法可能将成为未来资源限制的平台（如 FPGA、ASIC、微控制器和光子Integrated Circuits）上的训练基础。
</details></li>
</ul>
<hr>
<h2 id="Backdoor-Mitigation-by-Correcting-the-Distribution-of-Neural-Activations"><a href="#Backdoor-Mitigation-by-Correcting-the-Distribution-of-Neural-Activations" class="headerlink" title="Backdoor Mitigation by Correcting the Distribution of Neural Activations"></a>Backdoor Mitigation by Correcting the Distribution of Neural Activations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09850">http://arxiv.org/abs/2308.09850</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xi Li, Zhen Xiang, David J. Miller, George Kesidis</li>
<li>for: 本研究旨在揭示和分析对深度神经网络（DNN）的后门（Trojan）攻击的一种重要性，即在攻击成功后，攻击者的后门触发器会导致内层活动的分布变化，并且如果这种变化得到修复，则可以正确地将攻击者的目标类归类。</li>
<li>methods: 本研究使用了反工程化的触发器来修复后门攻击所导致的分布变化，并不需要改变DNN的任何可调参数。</li>
<li>results: 对比 existed 方法，本研究的方法可以更好地 mitigate 后门攻击，并且可以有效地检测测试实例中是否存在触发器。<details>
<summary>Abstract</summary>
Backdoor (Trojan) attacks are an important type of adversarial exploit against deep neural networks (DNNs), wherein a test instance is (mis)classified to the attacker's target class whenever the attacker's backdoor trigger is present. In this paper, we reveal and analyze an important property of backdoor attacks: a successful attack causes an alteration in the distribution of internal layer activations for backdoor-trigger instances, compared to that for clean instances. Even more importantly, we find that instances with the backdoor trigger will be correctly classified to their original source classes if this distribution alteration is corrected. Based on our observations, we propose an efficient and effective method that achieves post-training backdoor mitigation by correcting the distribution alteration using reverse-engineered triggers. Notably, our method does not change any trainable parameters of the DNN, but achieves generally better mitigation performance than existing methods that do require intensive DNN parameter tuning. It also efficiently detects test instances with the trigger, which may help to catch adversarial entities in the act of exploiting the backdoor.
</details>
<details>
<summary>摘要</summary>
<<SYS>>输入文本翻译成简化中文：<</SYS>>深度神经网络（DNN）面临着重要的反对抗攻击，即后门（Trojan）攻击，其中测试实例在攻击者的目标类上被识别为攻击者的后门触发器存在时。在这篇论文中，我们揭示了和分析了后门攻击的一个重要性特征：成功攻击会导致后门触发器实例的内部层活动分布变化，相比于干净实例。更重要的是，我们发现了一个关键的现象：如果这种分布变化得到了修正，那么带有后门触发器的实例将被正确地分类到其原始的类别。基于我们的观察，我们提出了一种高效和有效的后门恢复方法，通过修正分布变化来实现。这种方法不改变了DNN的任何可变参数，但它在恢复性能方面比既有的方法更好，并且可以有效地检测测试实例中的触发器。这可能帮助捕捉了利用后门攻击的恶意实体。
</details></li>
</ul>
<hr>
<h2 id="Enumerating-Safe-Regions-in-Deep-Neural-Networks-with-Provable-Probabilistic-Guarantees"><a href="#Enumerating-Safe-Regions-in-Deep-Neural-Networks-with-Provable-Probabilistic-Guarantees" class="headerlink" title="Enumerating Safe Regions in Deep Neural Networks with Provable Probabilistic Guarantees"></a>Enumerating Safe Regions in Deep Neural Networks with Provable Probabilistic Guarantees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09842">http://arxiv.org/abs/2308.09842</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luca Marzari, Davide Corsi, Enrico Marchesini, Alessandro Farinelli, Ferdinando Cicalese</li>
<li>for:  Ensuring trust in Deep Neural Networks (DNNs) by identifying safe areas.</li>
<li>methods:  Proposed an efficient approximation method called epsilon-ProVe, which exploits statistical prediction of tolerance limits to provide a tight lower estimate of the safe areas.</li>
<li>results:  Scalable and effective method that offers provable probabilistic guarantees, evaluated on standard benchmarks.<details>
<summary>Abstract</summary>
Identifying safe areas is a key point to guarantee trust for systems that are based on Deep Neural Networks (DNNs). To this end, we introduce the AllDNN-Verification problem: given a safety property and a DNN, enumerate the set of all the regions of the property input domain which are safe, i.e., where the property does hold. Due to the #P-hardness of the problem, we propose an efficient approximation method called epsilon-ProVe. Our approach exploits a controllable underestimation of the output reachable sets obtained via statistical prediction of tolerance limits, and can provide a tight (with provable probabilistic guarantees) lower estimate of the safe areas. Our empirical evaluation on different standard benchmarks shows the scalability and effectiveness of our method, offering valuable insights for this new type of verification of DNNs.
</details>
<details>
<summary>摘要</summary>
安全区域的标识是深度神经网络（DNN）系统的关键安全保证点。为此，我们提出了AllDNN-Verification问题：给定一个安全性质和一个DNN，列出安全区域的输入领域中的所有区域，即where the property does hold。由于这个问题的#P-hardness，我们提出了一种高效的近似方法called epsilon-ProVe。我们的方法利用通过统计预测容差范围来控制输出可达集的下界，并可以提供一个紧靠的（具有可证明的概率保证）下界。我们的实验表明我们的方法可以扩展到不同的标准准比，并且有效地验证DNNs。这些结果提供了对这种新类型的验证方法的有价值的理解。
</details></li>
</ul>
<hr>
<h2 id="Microscopy-Image-Segmentation-via-Point-and-Shape-Regularized-Data-Synthesis"><a href="#Microscopy-Image-Segmentation-via-Point-and-Shape-Regularized-Data-Synthesis" class="headerlink" title="Microscopy Image Segmentation via Point and Shape Regularized Data Synthesis"></a>Microscopy Image Segmentation via Point and Shape Regularized Data Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09835">http://arxiv.org/abs/2308.09835</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shijie Li, Mengwei Ren, Thomas Ach, Guido Gerig<br>for:* 这个论文主要针对的是用深度学习方法进行微scopia图像分割，但是现有的方法几乎都需要大量的训练数据，包括完整的对象边界信息，这会非常困难和成本高昂。methods:* 作者提出了一个整体框架，用于将点注释转换为 Synthetically generated training data，包括三个阶段：	1. 使用点注释生成一个 Pseudo dense segmentation mask，并使用 shape priors 来约束其生成;	2. 使用一个未经 paired 训练的图像生成模型，将 Pseudo mask 翻译成一个真实的 microscopy 图像，并使用 object level consistency 来补做;	3. 将 Pseudo masks 和生成的图像组成一个 pairwise 数据集，用于训练适应的分割模型。results:* 作者在公共的 MoNuSeg 数据集上测试了自己的生成框架，并发现其生成的图像比基eline模型更加多样化和真实，同时保持了输入mask和生成图像之间的高协调性;* 当使用同一个分割后果的模型，使用作者的生成数据集进行训练，与使用 pseudo-labels 或基eline生成的图像进行训练相比，模型的性能明显提高;* 作者的框架可以与 dense 标注数据进行比较，并且在 authentic microscopy 图像上达到相同的性能，这表明其可以作为 dense 标注数据的可靠和高效的替代方案。<details>
<summary>Abstract</summary>
Current deep learning-based approaches for the segmentation of microscopy images heavily rely on large amount of training data with dense annotation, which is highly costly and laborious in practice. Compared to full annotation where the complete contour of objects is depicted, point annotations, specifically object centroids, are much easier to acquire and still provide crucial information about the objects for subsequent segmentation. In this paper, we assume access to point annotations only during training and develop a unified pipeline for microscopy image segmentation using synthetically generated training data. Our framework includes three stages: (1) it takes point annotations and samples a pseudo dense segmentation mask constrained with shape priors; (2) with an image generative model trained in an unpaired manner, it translates the mask to a realistic microscopy image regularized by object level consistency; (3) the pseudo masks along with the synthetic images then constitute a pairwise dataset for training an ad-hoc segmentation model. On the public MoNuSeg dataset, our synthesis pipeline produces more diverse and realistic images than baseline models while maintaining high coherence between input masks and generated images. When using the identical segmentation backbones, the models trained on our synthetic dataset significantly outperform those trained with pseudo-labels or baseline-generated images. Moreover, our framework achieves comparable results to models trained on authentic microscopy images with dense labels, demonstrating its potential as a reliable and highly efficient alternative to labor-intensive manual pixel-wise annotations in microscopy image segmentation. The code is available.
</details>
<details>
<summary>摘要</summary>
当前的深度学习基本方法 для微scopic图像分割具有大量的训练数据和密集的标注，但在实际应用中是非常成本高昂和劳动密集的。相比于全部标注，其中包括对象的完整边框，点标注，特别是对象的中心点，更加容易获取并且仍然提供了对象分割的关键信息。在这篇论文中，我们假设在训练时有点标注可用，并开发了一个整体框架，包括以下三个阶段：1. 使用点标注生成一个 Pseudo 稠密分割mask，并将其约束于形状假设；2. 使用一个在无对应方式下训练的图像生成模型，将 Pseudo 分割mask 翻译成一个真实的微scopic图像，并对其进行对象水平的准确性 regularization；3. Pseudo 分割mask 和生成的图像组成一个对应的数据集，用于训练适应性的分割模型。在公共的 MoNuSeg 数据集上，我们的生成框架生成了更加多样和真实的图像，同时保持了输入掩模的高准确性。使用同样的分割背包，我们在我们的 sintetic 数据集上训练的模型比使用 Pseudo 标签或基eline-生成的图像训练得更高效，并且与 dense 标注的模型相当。此外，我们的框架可以实现 dense 标注的微scopic图像分割任务中的高效和可靠的替代方案。代码可用。
</details></li>
</ul>
<hr>
<h2 id="Learning-from-A-Single-Graph-is-All-You-Need-for-Near-Shortest-Path-Routing-in-Wireless-Networks"><a href="#Learning-from-A-Single-Graph-is-All-You-Need-for-Near-Shortest-Path-Routing-in-Wireless-Networks" class="headerlink" title="Learning from A Single Graph is All You Need for Near-Shortest Path Routing in Wireless Networks"></a>Learning from A Single Graph is All You Need for Near-Shortest Path Routing in Wireless Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09829">http://arxiv.org/abs/2308.09829</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yung-Fu Chen, Sen Lin, Anish Arora</li>
<li>for: 本研究旨在开发一种可以在无需大量数据样本的情况下，适应所有随机网络模型的本地路由策略学习算法。</li>
<li>methods: 本研究使用深度神经网络（DNNs）来学习本地路由策略，该策略只考虑当前节点和邻居节点的状态。研究者们在选择输入特征和选择“种子图”和子样本的方面借鉴了网络领域知识，以提供理论上的解释性。</li>
<li>results: 研究结果表明，使用生成于一些路由路径的抽样从一个较小的种子图就能够快速学习一个普适的路由策略，该策略可以在大多数随机网络模型中适用。此外，研究者们还发现了一种神经网络，可以准确地模仿扩散前方路由策略的性能。<details>
<summary>Abstract</summary>
We propose a learning algorithm for local routing policies that needs only a few data samples obtained from a single graph while generalizing to all random graphs in a standard model of wireless networks. We thus solve the all-pairs near-shortest path problem by training deep neural networks (DNNs) that efficiently and scalably learn routing policies that are local, i.e., they only consider node states and the states of neighboring nodes. Remarkably, one of these DNNs we train learns a policy that exactly matches the performance of greedy forwarding; another generally outperforms greedy forwarding. Our algorithm design exploits network domain knowledge in several ways: First, in the selection of input features and, second, in the selection of a ``seed graph'' and subsamples from its shortest paths. The leverage of domain knowledge provides theoretical explainability of why the seed graph and node subsampling suffice for learning that is efficient, scalable, and generalizable. Simulation-based results on uniform random graphs with diverse sizes and densities empirically corroborate that using samples generated from a few routing paths in a modest-sized seed graph quickly learns a model that is generalizable across (almost) all random graphs in the wireless network model.
</details>
<details>
<summary>摘要</summary>
Our algorithm leverages network domain knowledge in two ways:1. Selection of input features: We carefully select the input features to ensure that the learned policy is efficient and scalable.2. Selection of a "seed graph" and subsamples from its shortest paths: We use a small seed graph and subsamples from its shortest paths to train the DNNs, which provides theoretical explainability of why the seed graph and node subsampling suffice for learning.Our simulation results on uniform random graphs with diverse sizes and densities show that using samples generated from a few routing paths in a modest-sized seed graph can quickly learn a model that is generalizable across (almost) all random graphs in the wireless network model. This means that our algorithm can learn a routing policy that is effective and efficient, even in a large and complex wireless network.
</details></li>
</ul>
<hr>
<h2 id="VL-PET-Vision-and-Language-Parameter-Efficient-Tuning-via-Granularity-Control"><a href="#VL-PET-Vision-and-Language-Parameter-Efficient-Tuning-via-Granularity-Control" class="headerlink" title="VL-PET: Vision-and-Language Parameter-Efficient Tuning via Granularity Control"></a>VL-PET: Vision-and-Language Parameter-Efficient Tuning via Granularity Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09804">http://arxiv.org/abs/2308.09804</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/henryhzy/vl-pet">https://github.com/henryhzy/vl-pet</a></li>
<li>paper_authors: Zi-Yuan Hu, Yanyang Li, Michael R. Lyu, Liwei Wang</li>
<li>for: 这个研究旨在提出一个可控的vision-and-language参数效率训练（VL-PET）框架，以提高现有的参数效率训练技术的精确度和效率。</li>
<li>methods: 本研究使用了一个新的粒度控制机制，允许在不同的粒度控制矩阵上进行模块化修改，从而产生多种模型独立的VL-PET模组。此外，我们还提出了一些轻量级PET模组的设计，以提高预料和文本生成的整合。</li>
<li>results: 我们在四个图像数据项目和四个影片数据项目上进行了广泛的实验，结果显示了我们的VL-PET框架在效率、有效性和转移性方面具有优秀的表现。尤其是，我们的VL-PET-大模组，配备了轻量级PET模组，与BART-base和T5-base相比，在图像数据项目上提高了2.92%（3.41%）和3.37%（7.03%）。<details>
<summary>Abstract</summary>
As the model size of pre-trained language models (PLMs) grows rapidly, full fine-tuning becomes prohibitively expensive for model training and storage. In vision-and-language (VL), parameter-efficient tuning (PET) techniques are proposed to integrate modular modifications (e.g., Adapter and LoRA) into encoder-decoder PLMs. By tuning a small set of trainable parameters, these techniques perform on par with full fine-tuning. However, excessive modular modifications and neglecting the functionality gap between the encoders and decoders can lead to performance degradation, while existing PET techniques (e.g., VL-Adapter) overlook these critical issues. In this paper, we propose a Vision-and-Language Parameter-Efficient Tuning (VL-PET) framework to impose effective control over modular modifications via a novel granularity-controlled mechanism. Considering different granularity-controlled matrices generated by this mechanism, a variety of model-agnostic VL-PET modules can be instantiated from our framework for better efficiency and effectiveness trade-offs. We further propose lightweight PET module designs to enhance VL alignment and modeling for the encoders and maintain text generation for the decoders. Extensive experiments conducted on four image-text tasks and four video-text tasks demonstrate the efficiency, effectiveness and transferability of our VL-PET framework. In particular, our VL-PET-large with lightweight PET module designs significantly outperforms VL-Adapter by 2.92% (3.41%) and LoRA by 3.37% (7.03%) with BART-base (T5-base) on image-text tasks. Furthermore, we validate the enhanced effect of employing our VL-PET designs on existing PET techniques, enabling them to achieve significant performance improvements. Our code is available at https://github.com/HenryHZY/VL-PET.
</details>
<details>
<summary>摘要</summary>
As the size of pre-trained language models (PLMs) continues to grow rapidly, full fine-tuning becomes increasingly expensive for model training and storage. In the field of vision-and-language (VL), parameter-efficient tuning (PET) techniques have been proposed to integrate modular modifications (e.g., Adapter and LoRA) into encoder-decoder PLMs. By tuning a small set of trainable parameters, these techniques can achieve performance on par with full fine-tuning. However, excessive modular modifications and neglecting the functionality gap between the encoders and decoders can lead to performance degradation. Additionally, existing PET techniques (e.g., VL-Adapter) overlook these critical issues.In this paper, we propose a Vision-and-Language Parameter-Efficient Tuning (VL-PET) framework to effectively control modular modifications through a novel granularity-controlled mechanism. By considering different granularity-controlled matrices generated by this mechanism, a variety of model-agnostic VL-PET modules can be instantiated from our framework for better efficiency and effectiveness trade-offs. Furthermore, we propose lightweight PET module designs to enhance VL alignment and modeling for the encoders and maintain text generation for the decoders.Extensive experiments conducted on four image-text tasks and four video-text tasks demonstrate the efficiency, effectiveness, and transferability of our VL-PET framework. In particular, our VL-PET-large with lightweight PET module designs significantly outperforms VL-Adapter by 2.92% (3.41%) and LoRA by 3.37% (7.03%) with BART-base (T5-base) on image-text tasks. Additionally, we validate the enhanced effect of employing our VL-PET designs on existing PET techniques, enabling them to achieve significant performance improvements. Our code is available at https://github.com/HenryHZY/VL-PET.
</details></li>
</ul>
<hr>
<h2 id="An-Efficient-High-Dimensional-Gene-Selection-Approach-based-on-Binary-Horse-Herd-Optimization-Algorithm-for-Biological-Data-Classification"><a href="#An-Efficient-High-Dimensional-Gene-Selection-Approach-based-on-Binary-Horse-Herd-Optimization-Algorithm-for-Biological-Data-Classification" class="headerlink" title="An Efficient High-Dimensional Gene Selection Approach based on Binary Horse Herd Optimization Algorithm for Biological Data Classification"></a>An Efficient High-Dimensional Gene Selection Approach based on Binary Horse Herd Optimization Algorithm for Biological Data Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09791">http://arxiv.org/abs/2308.09791</a></li>
<li>repo_url: None</li>
<li>paper_authors: Niloufar Mehrabi, Sayed Pedram Haeri Boroujeni, Elnaz Pashaei</li>
<li>for: 解决复杂和高维度问题</li>
<li>methods: 使用新的Horse Herd Optimization Algorithm（HOA）和一种新的 Transfer Function（TF），以及一种hybrid feature selection方法 combining HOA和MRMR筛选法。</li>
<li>results: 比较其他状态的精准率和最少选择的特征数，MRMR-BHOA方法表现出色，并且实验结果表明X-形BHOA方法超过其他方法。<details>
<summary>Abstract</summary>
The Horse Herd Optimization Algorithm (HOA) is a new meta-heuristic algorithm based on the behaviors of horses at different ages. The HOA was introduced recently to solve complex and high-dimensional problems. This paper proposes a binary version of the Horse Herd Optimization Algorithm (BHOA) in order to solve discrete problems and select prominent feature subsets. Moreover, this study provides a novel hybrid feature selection framework based on the BHOA and a minimum Redundancy Maximum Relevance (MRMR) filter method. This hybrid feature selection, which is more computationally efficient, produces a beneficial subset of relevant and informative features. Since feature selection is a binary problem, we have applied a new Transfer Function (TF), called X-shape TF, which transforms continuous problems into binary search spaces. Furthermore, the Support Vector Machine (SVM) is utilized to examine the efficiency of the proposed method on ten microarray datasets, namely Lymphoma, Prostate, Brain-1, DLBCL, SRBCT, Leukemia, Ovarian, Colon, Lung, and MLL. In comparison to other state-of-the-art, such as the Gray Wolf (GW), Particle Swarm Optimization (PSO), and Genetic Algorithm (GA), the proposed hybrid method (MRMR-BHOA) demonstrates superior performance in terms of accuracy and minimum selected features. Also, experimental results prove that the X-Shaped BHOA approach outperforms others methods.
</details>
<details>
<summary>摘要</summary>
《马群优化算法（HOA）是一种新的meta-heuristic算法，基于马匹不同年龄的行为。HOA最近被引入以解决复杂高维问题。本文提出了一种二进制版本的马群优化算法（BHOA），用于解 discrete问题并选择出色特征子集。此外，本研究还提出了一种 hybrid 特征选择框架，基于 BHOA 和最小重复度最大相关性（MRMR）筛选法。这种 hybrid 特征选择更加 computationally efficient，生成了有利的特征子集。由于特征选择是一个二进制问题，我们采用了一个新的转移函数（TF），称为 X-形 TF，将连续问题转换成二进制搜索空间。此外，我们使用了支持向量机（SVM）来评估提案方法在十个 microarray 数据集上的效率，即 Limphoma、Prostate、Brain-1、DLBCL、SRBCT、Leukemia、Ovarian、Colon、Lung 和 MLL。与其他现有的state-of-the-art，如灰狼（GW）、PARTICLE SWARM OPTIMIZATION（PSO）和遗传算法（GA）相比，我们的 hybrid 方法（MRMR-BHOA）在准确率和选择的最小特征数上显示出超越性。此外，实验结果也证明了 X-Shape BHOA 方法的优越性。
</details></li>
</ul>
<hr>
<h2 id="A-Two-Part-Machine-Learning-Approach-to-Characterizing-Network-Interference-in-A-B-Testing"><a href="#A-Two-Part-Machine-Learning-Approach-to-Characterizing-Network-Interference-in-A-B-Testing" class="headerlink" title="A Two-Part Machine Learning Approach to Characterizing Network Interference in A&#x2F;B Testing"></a>A Two-Part Machine Learning Approach to Characterizing Network Interference in A&#x2F;B Testing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09790">http://arxiv.org/abs/2308.09790</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuan Yuan, Kristen M. Altenburger</li>
<li>for: 提高控制实验的可靠性，解决网络干扰问题</li>
<li>methods: 使用机器学习算法自动识别和特征化不同类型的网络干扰，并使用透明的机器学习模型确定最佳曝光映射</li>
<li>results: 通过验证两个synthetic实验和一个实际的大规模Instagram用户测试，与传统方法相比，提高了A&#x2F;B测试结果的精度和可靠性<details>
<summary>Abstract</summary>
The reliability of controlled experiments, or "A/B tests," can often be compromised due to the phenomenon of network interference, wherein the outcome for one unit is influenced by other units. To tackle this challenge, we propose a machine learning-based method to identify and characterize heterogeneous network interference. Our approach accounts for latent complex network structures and automates the task of "exposure mapping'' determination, which addresses the two major limitations in the existing literature. We introduce "causal network motifs'' and employ transparent machine learning models to establish the most suitable exposure mapping that reflects underlying network interference patterns. Our method's efficacy has been validated through simulations on two synthetic experiments and a real-world, large-scale test involving 1-2 million Instagram users, outperforming conventional methods such as design-based cluster randomization and analysis-based neighborhood exposure mapping. Overall, our approach not only offers a comprehensive, automated solution for managing network interference and improving the precision of A/B testing results, but it also sheds light on users' mutual influence and aids in the refinement of marketing strategies.
</details>
<details>
<summary>摘要</summary>
控制实验的可靠性，或“A/B测试”，经常受到网络干扰的影响，其中一个单元的结果会受到其他单元的影响。为解决这个挑战，我们提议一种基于机器学习的方法，用于识别和特征化不同类型的网络干扰。我们的方法考虑了隐藏的复杂网络结构，并自动确定“曝光 mapping”，解决了现有文献中的两大限制。我们引入“ causal 网络模式”并使用透明的机器学习模型，以确定最佳的曝光 mapping，它反映了下面网络干扰模式。我们的方法在两个人工实验和一个实际的大规模实验中，与传统的设计基于块随机分配和分析基于邻居曝光 mapping相比，表现出了更高的效果。总之，我们的方法不仅提供了一种完整、自动化的网络干扰管理和A/B测试结果的精度提高的解决方案，还可以揭示用户之间的互动关系，帮助改进营销策略。
</details></li>
</ul>
<hr>
<h2 id="Towards-Grounded-Visual-Spatial-Reasoning-in-Multi-Modal-Vision-Language-Models"><a href="#Towards-Grounded-Visual-Spatial-Reasoning-in-Multi-Modal-Vision-Language-Models" class="headerlink" title="Towards Grounded Visual Spatial Reasoning in Multi-Modal Vision Language Models"></a>Towards Grounded Visual Spatial Reasoning in Multi-Modal Vision Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09778">http://arxiv.org/abs/2308.09778</a></li>
<li>repo_url: None</li>
<li>paper_authors: Navid Rajabi, Jana Kosecka</li>
<li>for: 研究大规模视言模型（VLM）在视觉理解任务中表现，特别是识别空间关系的能力。</li>
<li>methods: 提出细化的 композиitional 顺序排序方法，结合图像文本匹配或视觉问答任务，以评估视觉关系理解能力。</li>
<li>results: 通过对象和其位置的识别，计算最终排名的空间 clause，并在多种视觉语言模型（Tan和Bansal 2019; Gupta等 2022; Kamath等 2021）上进行评估和比较，以 highlight 它们在理解空间关系方面的能力。<details>
<summary>Abstract</summary>
With the advances in large scale vision-and-language models (VLMs) it is of interest to assess their performance on various visual reasoning tasks such as counting, referring expressions and general visual question answering. The focus of this work is to study the ability of these models to understanding spatial relations. Previously, this has been tackled using image-text matching (Liu, Emerson, and Collier 2022) or visual question answering task, both showing poor performance and a large gap compared to human performance. To better understand the gap, we present fine-grained compositional grounding of spatial relationships and propose a bottom up approach for ranking spatial clauses and evaluating the performance of spatial relationship reasoning task. We propose to combine the evidence from grounding noun phrases corresponding to objects and their locations to compute the final rank of the spatial clause. We demonstrate the approach on representative vision-language models (Tan and Bansal 2019; Gupta et al. 2022; Kamath et al. 2021) and compare and highlight their abilities to reason about spatial relationships.
</details>
<details>
<summary>摘要</summary>
通过大规模视力语言模型（VLM）的发展，我们关心其在不同的视觉逻辑任务中的表现，如数字、引用表达和通用视觉问答。我们的研究着点在于检测这些模型对空间关系的理解能力。既前面的研究通过图像文本匹配（Liu、Emerson和Collier 2022）或视觉问答任务来评估这些模型的表现，都显示了较差的性能和人类表现之间的大差。为了更好地理解这个差距，我们提出了细化的 композиitional 顺序排序方法，并提出了一种底向方法来评估视觉关系逻辑任务的表现。我们将基于对物体和其位置的语言表达grounding的证据来计算最终排名的空间条款。我们在代表性的视力语言模型（Tan和Bansal 2019; Gupta等 2022; Kamath等 2021）上进行了实验，并对这些模型在视觉关系逻辑任务中的表现进行了比较和高亮。
</details></li>
</ul>
<hr>
<h2 id="Time-Series-Predictions-in-Unmonitored-Sites-A-Survey-of-Machine-Learning-Techniques-in-Water-Resources"><a href="#Time-Series-Predictions-in-Unmonitored-Sites-A-Survey-of-Machine-Learning-Techniques-in-Water-Resources" class="headerlink" title="Time Series Predictions in Unmonitored Sites: A Survey of Machine Learning Techniques in Water Resources"></a>Time Series Predictions in Unmonitored Sites: A Survey of Machine Learning Techniques in Water Resources</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09766">http://arxiv.org/abs/2308.09766</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jared D. Willard, Charuleka Varadharajan, Xiaowei Jia, Vipin Kumar</li>
<li>For: The paper is written for water resources science, specifically for predicting dynamic environmental variables in unmonitored sites.* Methods: The paper uses modern machine learning methods, such as deep learning frameworks, to predict hydrological variables like river flow and water quality.* Results: The paper reviews state-of-the-art applications of machine learning for streamflow, water quality, and other water resources prediction, and identifies open questions for time series predictions in unmonitored sites, including incorporating dynamic inputs and site characteristics, mechanistic understanding and spatial context, and explainable AI techniques.Here is the same information in Simplified Chinese text:* For: 论文写于水资源科学领域，特点是预测未监测站点的动态环境变量。* Methods: 论文使用现代机器学习方法，如深度学习框架，预测河流流量和水质等ydrological变量。* Results: 论文对水资源预测进行了国际评估，并确定了未监测站点预测时间序列的开放问题，包括包括动态输入和站点特点、机制理解和空间上下文等。<details>
<summary>Abstract</summary>
Prediction of dynamic environmental variables in unmonitored sites remains a long-standing challenge for water resources science. The majority of the world's freshwater resources have inadequate monitoring of critical environmental variables needed for management. Yet, the need to have widespread predictions of hydrological variables such as river flow and water quality has become increasingly urgent due to climate and land use change over the past decades, and their associated impacts on water resources. Modern machine learning methods increasingly outperform their process-based and empirical model counterparts for hydrologic time series prediction with their ability to extract information from large, diverse data sets. We review relevant state-of-the art applications of machine learning for streamflow, water quality, and other water resources prediction and discuss opportunities to improve the use of machine learning with emerging methods for incorporating watershed characteristics into deep learning models, transfer learning, and incorporating process knowledge into machine learning models. The analysis here suggests most prior efforts have been focused on deep learning learning frameworks built on many sites for predictions at daily time scales in the United States, but that comparisons between different classes of machine learning methods are few and inadequate. We identify several open questions for time series predictions in unmonitored sites that include incorporating dynamic inputs and site characteristics, mechanistic understanding and spatial context, and explainable AI techniques in modern machine learning frameworks.
</details>
<details>
<summary>摘要</summary>
<<SYS>>水资源科学中预测无监测站的动态环境变量是一项长期挑战。全球大多数新鲜水资源尚未充分监测关键环境变量，尤其是气候和土地使用变化过去几十年所带来的影响。然而，由于气候和土地使用变化，水资源预测的需求日益增加。现代机器学习方法在水文时序预测方面升级表现，可以从大量多样数据集中提取信息。我们评估了相关的现代应用，包括流量和水质预测，并讨论了将水 shed特征 incorporated into deep learning模型、传输学习和机器学习模型中的进程知识。分析表明，大多数先前努力都集中在了基于多地点的深度学习框架上，但对不同类型机器学习方法的比较 remains limited。我们标识了一些未解决的问题，包括 incorporating 动态输入和站点特征、机制理解和空间上下文，以及现代机器学习框架中的可解释AI技术。
</details></li>
</ul>
<hr>
<h2 id="Taken-by-Surprise-Contrast-effect-for-Similarity-Scores"><a href="#Taken-by-Surprise-Contrast-effect-for-Similarity-Scores" class="headerlink" title="Taken by Surprise: Contrast effect for Similarity Scores"></a>Taken by Surprise: Contrast effect for Similarity Scores</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09765">http://arxiv.org/abs/2308.09765</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/meetelise/surprise-similarity">https://github.com/meetelise/surprise-similarity</a></li>
<li>paper_authors: Thomas C. Bachlechner, Mario Martone, Marjorie Schillo</li>
<li>for: 本研究旨在提出一种基于人类听觉效应的对象向量表示的相似性评价方法，以提高自然语言处理、信息检索和分类任务的性能。</li>
<li>methods: 本研究使用了一种 называ为“surprise score”的ensemble-normalized相似性指标，该指标基于对象向量的分布来衡量对象之间的相似性。</li>
<li>results: 研究发现，使用“surprise score”指标可以在零&#x2F;几shot文档分类任务中提高性能， Typically 10-15% better than raw cosine similarity。<details>
<summary>Abstract</summary>
Accurately evaluating the similarity of object vector embeddings is of critical importance for natural language processing, information retrieval and classification tasks. Popular similarity scores (e.g cosine similarity) are based on pairs of embedding vectors and disregard the distribution of the ensemble from which objects are drawn. Human perception of object similarity significantly depends on the context in which the objects appear. In this work we propose the $\textit{surprise score}$, an ensemble-normalized similarity metric that encapsulates the contrast effect of human perception and significantly improves the classification performance on zero- and few-shot document classification tasks. This score quantifies the surprise to find a given similarity between two elements relative to the pairwise ensemble similarities. We evaluate this metric on zero/few shot classification and clustering tasks and typically find 10-15 % better performance compared to raw cosine similarity. Our code is available at https://github.com/MeetElise/surprise-similarity.
</details>
<details>
<summary>摘要</summary>
精准评估对象vector编码的相似性对自然语言处理、信息检索和分类任务是极为重要的。受欢迎的相似性分数（例如归一化相似性）基于对 embedding vector的对应对，而忽略了对象 ensemble 的分布。人类对对象相似性的识别受到对象出现的 Context 的影响。在这种工作中，我们提议使用 $\textit{surprise score}$，一种ensemble-normalized相似度度量，它体现了人类对对象相似性的冲击效应，并在零/几个shot文档分类任务中显著提高分类性能， Typically 10-15%。我们在这些任务中评估了这个分数，并发现它 Typically 10-15% 比raw cosine similarity better。我们的代码可以在https://github.com/MeetElise/surprise-similarity上找到。
</details></li>
</ul>
<hr>
<h2 id="The-Impact-of-Background-Removal-on-Performance-of-Neural-Networks-for-Fashion-Image-Classification-and-Segmentation"><a href="#The-Impact-of-Background-Removal-on-Performance-of-Neural-Networks-for-Fashion-Image-Classification-and-Segmentation" class="headerlink" title="The Impact of Background Removal on Performance of Neural Networks for Fashion Image Classification and Segmentation"></a>The Impact of Background Removal on Performance of Neural Networks for Fashion Image Classification and Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09764">http://arxiv.org/abs/2308.09764</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junhui Liang, Ying Liu, Vladimir Vlassov</li>
<li>for: 提高时尚图像数据质量和模型性能</li>
<li>methods: 使用突出对象检测来移除背景</li>
<li>results: 可以提高模型准确率达5%，但是深度网络不适合使用Background Removal因为与其他正则化技术不兼容<details>
<summary>Abstract</summary>
Fashion understanding is a hot topic in computer vision, with many applications having great business value in the market. Fashion understanding remains a difficult challenge for computer vision due to the immense diversity of garments and various scenes and backgrounds. In this work, we try removing the background from fashion images to boost data quality and increase model performance. Having fashion images of evident persons in fully visible garments, we can utilize Salient Object Detection to achieve the background removal of fashion data to our expectations. A fashion image with the background removed is claimed as the "rembg" image, contrasting with the original one in the fashion dataset. We conducted extensive comparative experiments with these two types of images on multiple aspects of model training, including model architectures, model initialization, compatibility with other training tricks and data augmentations, and target task types. Our experiments show that background removal can effectively work for fashion data in simple and shallow networks that are not susceptible to overfitting. It can improve model accuracy by up to 5% in the classification on the FashionStyle14 dataset when training models from scratch. However, background removal does not perform well in deep neural networks due to incompatibility with other regularization techniques like batch normalization, pre-trained initialization, and data augmentations introducing randomness. The loss of background pixels invalidates many existing training tricks in the model training, adding the risk of overfitting for deep models.
</details>
<details>
<summary>摘要</summary>
现代时尚理解是计算机视觉领域的热门话题，具有广泛的商业价值。然而，时尚理解仍然是计算机视觉中的挑战，因为衣服的多样性和不同的场景和背景。在这项工作中，我们尝试将时尚图像的背景移除，以提高数据质量并提高模型性能。通过使用突出对象检测来实现背景移除，我们提出了“rembg”图像的概念，与原始时尚数据集的图像进行对比。我们进行了多种比较实验，包括模型架构、模型初始化、与其他训练技巧和数据扩展相容性等方面。我们的实验结果表明，背景移除可以有效地提高时尚数据的模型准确率，但是深度网络中的背景移除不太好，因为它们与批处理常规化、预先初始化和数据扩展引入随机性不兼容。失去背景像素会让许多现有的训练技巧在模型训练中添加随机性，增加深度模型的风险过拟合。
</details></li>
</ul>
<hr>
<h2 id="Data-Compression-and-Inference-in-Cosmology-with-Self-Supervised-Machine-Learning"><a href="#Data-Compression-and-Inference-in-Cosmology-with-Self-Supervised-Machine-Learning" class="headerlink" title="Data Compression and Inference in Cosmology with Self-Supervised Machine Learning"></a>Data Compression and Inference in Cosmology with Self-Supervised Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09751">http://arxiv.org/abs/2308.09751</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aizhanaakhmet/data-compression-inference-in-cosmology-with-ssl">https://github.com/aizhanaakhmet/data-compression-inference-in-cosmology-with-ssl</a></li>
<li>paper_authors: Aizhan Akhmetzhanova, Siddharth Mishra-Sharma, Cora Dvorkin</li>
<li>for:  cosmological surveys 的数据压缩</li>
<li>methods: 使用自动augmentation的自然语言处理技术</li>
<li>results: 可以生成高度信息含量的摘要，用于各种下游任务，如精确参数推导Here’s a more detailed explanation of each point:</li>
<li>for: The paper is written for the purpose of exploring a new method for compressing and analyzing large cosmological datasets.</li>
<li>methods: The paper uses a self-supervised machine learning approach called simulation-based augmentations to construct representative summaries of the data.</li>
<li>results: The method is shown to deliver highly informative summaries that can be used for a variety of downstream tasks, such as precise and accurate parameter inference, and is insensitive to prescribed systematic effects like the influence of baryonic physics.<details>
<summary>Abstract</summary>
The influx of massive amounts of data from current and upcoming cosmological surveys necessitates compression schemes that can efficiently summarize the data with minimal loss of information. We introduce a method that leverages the paradigm of self-supervised machine learning in a novel manner to construct representative summaries of massive datasets using simulation-based augmentations. Deploying the method on hydrodynamical cosmological simulations, we show that it can deliver highly informative summaries, which can be used for a variety of downstream tasks, including precise and accurate parameter inference. We demonstrate how this paradigm can be used to construct summary representations that are insensitive to prescribed systematic effects, such as the influence of baryonic physics. Our results indicate that self-supervised machine learning techniques offer a promising new approach for compression of cosmological data as well its analysis.
</details>
<details>
<summary>摘要</summary>
Current and upcoming cosmological surveys will produce vast amounts of data, making it essential to develop compression schemes that can efficiently summarize the data with minimal loss of information. We propose a method that leverages self-supervised machine learning in a novel way to create representative summaries of massive datasets using simulation-based augmentations. Applying the method to hydrodynamical cosmological simulations, we show that it can produce highly informative summaries that can be used for a variety of downstream tasks, such as precise and accurate parameter inference. Our results suggest that self-supervised machine learning techniques offer a promising new approach for compressing and analyzing cosmological data.Here's the text in Traditional Chinese for comparison:现有和未来的 cosmological surveys 将生成巨量数据，因此需要发展压缩方案，以实现最小化信息损失的数据概要。我们提出了一种方法，利用自动化学习的 Paradigma 在一种新的方式中，创建大量数据的代表概要，使用 simulations 的增强。将方法应用到 hydrodynamical cosmological simulations，我们展示了它可以生成高度有用的概要，可以用于多种下游任务，例如精确和准确的参数推断。我们的结果表明，自动化学习技术对 cosmological data 的压缩和分析提供了一个有前途的新方法。
</details></li>
</ul>
<hr>
<h2 id="Robust-Monocular-Depth-Estimation-under-Challenging-Conditions"><a href="#Robust-Monocular-Depth-Estimation-under-Challenging-Conditions" class="headerlink" title="Robust Monocular Depth Estimation under Challenging Conditions"></a>Robust Monocular Depth Estimation under Challenging Conditions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09711">http://arxiv.org/abs/2308.09711</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/md4all/md4all">https://github.com/md4all/md4all</a></li>
<li>paper_authors: Stefano Gasperini, Nils Morbitzer, HyunJun Jung, Nassir Navab, Federico Tombari</li>
<li>for: 提高单目深度估计的可靠性，并能在不同的环境和气候条件下工作，包括夜间和雨天等。</li>
<li>methods: 利用现有方法的有效性，生成复杂的样本，并通过自我或全量监督训练，使模型能够在不同的condition下恢复信息。</li>
<li>results: 在nuScenes和Oxford RobotCar等两个公共数据集上进行了广泛的实验，比 Priors works 大幅提高了效果，在标准和挑战性条件下都有出色的表现。<details>
<summary>Abstract</summary>
While state-of-the-art monocular depth estimation approaches achieve impressive results in ideal settings, they are highly unreliable under challenging illumination and weather conditions, such as at nighttime or in the presence of rain. In this paper, we uncover these safety-critical issues and tackle them with md4all: a simple and effective solution that works reliably under both adverse and ideal conditions, as well as for different types of learning supervision. We achieve this by exploiting the efficacy of existing methods under perfect settings. Therefore, we provide valid training signals independently of what is in the input. First, we generate a set of complex samples corresponding to the normal training ones. Then, we train the model by guiding its self- or full-supervision by feeding the generated samples and computing the standard losses on the corresponding original images. Doing so enables a single model to recover information across diverse conditions without modifications at inference time. Extensive experiments on two challenging public datasets, namely nuScenes and Oxford RobotCar, demonstrate the effectiveness of our techniques, outperforming prior works by a large margin in both standard and challenging conditions. Source code and data are available at: https://md4all.github.io.
</details>
<details>
<summary>摘要</summary>
当前的单目深度估计方法在理想的设置下可以获得很好的结果，但在具有挑战性的照明和天气条件下（如夜晚或雨天），这些方法却很不可靠。在这篇论文中，我们揭示了这些安全关键问题并解决了它们，使用md4all：一个简单有效的解决方案，在不同的条件下都可靠地工作，包括理想和挑战性的条件，以及不同类型的学习监督。我们实现了这一点通过利用现有方法在完美的设置下的效果。因此，我们可以在训练时提供有效的训练信号，不受输入内容的限制。首先，我们生成一个包含复杂样本的集合，与常见的训练样本相对应。然后，我们使用这些生成的样本和对原始图像进行标准损失计算来引导模型的自我或全自监督。这样做的原因是，我们可以在推理时不需要对模型进行修改，以便在多种条件下进行推理。我们的技术在nuScenes和Oxford RobotCar两个公共数据集上进行了广泛的实验，并且与之前的工作相比，在标准和挑战性条件下都有很大的进步。代码和数据可以在https://md4all.github.io获取。
</details></li>
</ul>
<hr>
<h2 id="Neural-network-quantum-state-study-of-the-long-range-antiferromagnetic-Ising-chain"><a href="#Neural-network-quantum-state-study-of-the-long-range-antiferromagnetic-Ising-chain" class="headerlink" title="Neural-network quantum state study of the long-range antiferromagnetic Ising chain"></a>Neural-network quantum state study of the long-range antiferromagnetic Ising chain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09709">http://arxiv.org/abs/2308.09709</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jicheol Kim, Dongkyu Kim, Dong-Hee Kim</li>
<li>for:  investigate quantum phase transitions in the transverse field Ising chain with algebraically decaying long-range antiferromagnetic interactions</li>
<li>methods:  use variational Monte Carlo method with restricted Boltzmann machine as trial wave function ansatz</li>
<li>results:  find that central charge deviates from 1&#x2F;2 at small decay exponent $\alpha_\mathrm{LR}$, and identify threshold of Ising universality and conformal symmetry deviation from SR Ising class at $\alpha_\mathrm{LR} &lt; 2$<details>
<summary>Abstract</summary>
We investigate quantum phase transitions in the transverse field Ising chain with algebraically decaying long-range antiferromagnetic interactions by using the variational Monte Carlo method with the restricted Boltzmann machine being employed as a trial wave function ansatz. In the finite-size scaling analysis with the order parameter and the second R\'enyi entropy, we find that the central charge deviates from 1/2 at a small decay exponent $\alpha_\mathrm{LR}$ in contrast to the critical exponents staying very close to the short-range (SR) Ising values regardless of $\alpha_\mathrm{LR}$ examined, supporting the previously proposed scenario of conformal invariance breakdown. To identify the threshold of the Ising universality and the conformal symmetry, we perform two additional tests for the universal Binder ratio and the conformal field theory (CFT) description of the correlation function. It turns out that both indicate a noticeable deviation from the SR Ising class at $\alpha_\mathrm{LR} < 2$. However, a closer look at the scaled correlation function for $\alpha_\mathrm{LR} \ge 2$ shows a gradual change from the asymptotic line of the CFT verified at $\alpha_\mathrm{LR} = 3$, providing a rough estimate of the threshold being in the range of $2 \lesssim \alpha_\mathrm{LR} < 3$.
</details>
<details>
<summary>摘要</summary>
我们研究量子阶段转变在横向Isings链中的数学态度，使用variational Monte Carlo方法和受限 Boltzmann机作为实验波函数构想。在finite-size扩展分析中，我们发现中心 charge在小数字 $\alpha_\text{LR}$ 下异 від 1/2，与短距离Isings值不同，支持之前提出的对称性破坏enario。为了识别Isings universality和对称性的阈值，我们进行了两个额外测试： universal Binder 比率和对称场论 (CFT) 描述的联系函数。结果显示，两个测试都显示了 SR Isings 类型的明显偏离，但是在 $\alpha_\text{LR} \ge 2$ 时，扩展联系函数的涨落趋势逐渐变化为CFT预测的极限线，提供了约 estimate的阈值在 $2 \lesssim \alpha_\text{LR} < 3$ 之间。
</details></li>
</ul>
<hr>
<h2 id="Do-you-know-what-q-means"><a href="#Do-you-know-what-q-means" class="headerlink" title="Do you know what q-means?"></a>Do you know what q-means?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09701">http://arxiv.org/abs/2308.09701</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Sfedfcv/redesigned-pancake">https://github.com/Sfedfcv/redesigned-pancake</a></li>
<li>paper_authors: João F. Doriguello, Alessandro Luongo, Ewin Tang</li>
<li>for: 本研究旨在提出一种可以实现高效的分类算法，具体来说是$k$-means算法的一种近似版本。</li>
<li>methods: 本研究使用的方法包括：Lloyd’s iteration algorithm和一种基于QRAM的近似$k$-means算法。</li>
<li>results: 本研究的结果包括：提出了一种基于QRAM的$k$-means算法，该算法可以在$O\big(\frac{k^{2}}{\varepsilon^2}(\sqrt{k}d + \log(Nd))\big)$时间内完成分类任务，同时保持了对$N$的多项幂逻辑依赖。此外，还提出了一种类比的分类算法，即”dequantized”算法，其时间复杂度为$O\big(\frac{k^{2}}{\varepsilon^2}(kd + \log(Nd))\big)$。<details>
<summary>Abstract</summary>
Clustering is one of the most important tools for analysis of large datasets, and perhaps the most popular clustering algorithm is Lloyd's iteration for $k$-means. This iteration takes $N$ vectors $v_1,\dots,v_N\in\mathbb{R}^d$ and outputs $k$ centroids $c_1,\dots,c_k\in\mathbb{R}^d$; these partition the vectors into clusters based on which centroid is closest to a particular vector. We present an overall improved version of the "$q$-means" algorithm, the quantum algorithm originally proposed by Kerenidis, Landman, Luongo, and Prakash (2019) which performs $\varepsilon$-$k$-means, an approximate version of $k$-means clustering. This algorithm does not rely on the quantum linear algebra primitives of prior work, instead only using its QRAM to prepare and measure simple states based on the current iteration's clusters. The time complexity is $O\big(\frac{k^{2}}{\varepsilon^2}(\sqrt{k}d + \log(Nd))\big)$ and maintains the polylogarithmic dependence on $N$ while improving the dependence on most of the other parameters. We also present a "dequantized" algorithm for $\varepsilon$-$k$-means which runs in $O\big(\frac{k^{2}}{\varepsilon^2}(kd + \log(Nd))\big)$ time. Notably, this classical algorithm matches the polylogarithmic dependence on $N$ attained by the quantum algorithms.
</details>
<details>
<summary>摘要</summary>
clustering 是大规模数据分析中最重要的工具之一，而可能最受欢迎的归一化算法之一就是沛降的迭代法（Lloyd's iteration）。这个迭代法会将 N 个坐标vector $v_1, \ldots, v_N \in \mathbb{R}^d$ 转化为 K 个中心点 $c_1, \ldots, c_K \in \mathbb{R}^d$，这些中心点将坐标分成 clusters 基于哪个中心点最近。我们提出了一个全面改进的 "$q$-means" 算法，它是 Kerenidis et al. (2019) 提出的量子算法的应用，用于实现 $\varepsilon$-$k$-means，这是 $k$-means 归一化 clustering 的一个 Approximate 版本。这个算法不依赖于量子线性代数基本操作，而是只使用其 QRAM 来准备和测量基于当前迭代的 clusters 的简单状态。时间复杂度为 $O\big(\frac{k^{2}}{\varepsilon^2}(\sqrt{k}d + \log(Nd))\big)$，这保持了对 N 的多项式依赖性，而改善了大多数其他参数的依赖性。我们还提出了一个 "dequantized" 算法，它在 $O\big(\frac{k^{2}}{\varepsilon^2}(kd + \log(Nd))\big)$ 时间内运行，并且与量子算法具有相同的多项式依赖性。它的时间复杂度与量子算法具有相同的多项式依赖性。
</details></li>
</ul>
<hr>
<h2 id="A-Lightweight-Transformer-for-Faster-and-Robust-EBSD-Data-Collection"><a href="#A-Lightweight-Transformer-for-Faster-and-Robust-EBSD-Data-Collection" class="headerlink" title="A Lightweight Transformer for Faster and Robust EBSD Data Collection"></a>A Lightweight Transformer for Faster and Robust EBSD Data Collection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09693">http://arxiv.org/abs/2308.09693</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hdong920/ebsd_slice_recovery">https://github.com/hdong920/ebsd_slice_recovery</a></li>
<li>paper_authors: Harry Dong, Sean Donegan, Megna Shah, Yuejie Chi</li>
<li>for: 提高三维电子背散射束Diffraction（EBSD）微scopic的数据质量，以便在材料科学中应用。</li>
<li>methods: 使用深度学习模型和投影算法来修复3D EBSD数据中缺失的层。</li>
<li>results: 使用只有自我超vision的synthetic 3D EBSD数据进行训练，在真实3D EBSD数据上获得了比现有方法更高的恢复精度。<details>
<summary>Abstract</summary>
Three dimensional electron back-scattered diffraction (EBSD) microscopy is a critical tool in many applications in materials science, yet its data quality can fluctuate greatly during the arduous collection process, particularly via serial-sectioning. Fortunately, 3D EBSD data is inherently sequential, opening up the opportunity to use transformers, state-of-the-art deep learning architectures that have made breakthroughs in a plethora of domains, for data processing and recovery. To be more robust to errors and accelerate this 3D EBSD data collection, we introduce a two step method that recovers missing slices in an 3D EBSD volume, using an efficient transformer model and a projection algorithm to process the transformer's outputs. Overcoming the computational and practical hurdles of deep learning with scarce high dimensional data, we train this model using only synthetic 3D EBSD data with self-supervision and obtain superior recovery accuracy on real 3D EBSD data, compared to existing methods.
</details>
<details>
<summary>摘要</summary>
三维电子后射扩散Diffraction（EBSD） Mikroskopi是物料科学中多种应用中的重要工具，但它的数据质量可能会在收集过程中变化很大，特别是通过串行sectioning。幸运的是，3D EBSD数据是顺序的，这为使用transformer，当前领域的最先进深度学习架构，进行数据处理和恢复提供了机会。为了更加鲁棒地处理错误和加速3D EBSD数据收集，我们提出了一种两步方法，使用高效的transformer模型和投影算法来处理transformer的输出。通过超越深度学习中的计算和实践障碍，我们使用只有自我超vision的synthetic 3D EBSD数据进行训练，并在实际3D EBSD数据上获得了比现有方法更高的恢复精度。
</details></li>
</ul>
<hr>
<h2 id="Reduced-Order-Modeling-of-a-MOOSE-based-Advanced-Manufacturing-Model-with-Operator-Learning"><a href="#Reduced-Order-Modeling-of-a-MOOSE-based-Advanced-Manufacturing-Model-with-Operator-Learning" class="headerlink" title="Reduced Order Modeling of a MOOSE-based Advanced Manufacturing Model with Operator Learning"></a>Reduced Order Modeling of a MOOSE-based Advanced Manufacturing Model with Operator Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09691">http://arxiv.org/abs/2308.09691</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahmoud Yaseen, Dewen Yushu, Peter German, Xu Wu</li>
<li>for: 这个论文的目的是开发一种快速运行的减少维度模型（ROM），用于在深度学习控制和优化方法中使用。</li>
<li>methods: 这个论文使用了一种基于操作学习（OL）的方法，使用 fourier neural operator 来构建ROM。</li>
<li>results: 这个论文的研究结果表明，OL-based ROM 可以具有更高的准确性和更快的运行速度，相比于传统的深度神经网络基于 ROM。<details>
<summary>Abstract</summary>
Advanced Manufacturing (AM) has gained significant interest in the nuclear community for its potential application on nuclear materials. One challenge is to obtain desired material properties via controlling the manufacturing process during runtime. Intelligent AM based on deep reinforcement learning (DRL) relies on an automated process-level control mechanism to generate optimal design variables and adaptive system settings for improved end-product properties. A high-fidelity thermo-mechanical model for direct energy deposition has recently been developed within the MOOSE framework at the Idaho National Laboratory (INL). The goal of this work is to develop an accurate and fast-running reduced order model (ROM) for this MOOSE-based AM model that can be used in a DRL-based process control and optimization method. Operator learning (OL)-based methods will be employed due to their capability to learn a family of differential equations, in this work, produced by changing process variables in the Gaussian point heat source for the laser. We will develop OL-based ROM using Fourier neural operator, and perform a benchmark comparison of its performance with a conventional deep neural network-based ROM.
</details>
<details>
<summary>摘要</summary>
高等制造（AM）在核子社区中已引起了广泛的关注，因为它在核子材料方面的应用有很大的潜力。一个挑战是通过控制生产过程的 runtime 来实现所需的材料性能。基于深度强化学习（DRL）的智能制造利用了自动化的过程级别控制机制，以生成优化的设计变量和适应系统设置，以提高终产品的性能。在蒙大学（INL）的MOOSE框架中，最近开发了一个高精度的热力学-机械模型，用于直接能量沟入。本工作的目标是开发一个准确和快速运行的减少阶段模型（ROM），用于这个 MOOSE-based AM 模型，以便在 DRL-based 过程控制和优化方法中使用。我们将使用操作学（OL）-based 方法，因为它们可以学习一个家族的差分方程，在这里，由变量改变在泊松点热源中的激光。我们将开发 OL-based ROM 使用整数 ней网络，并对它的性能进行了比较。
</details></li>
</ul>
<hr>
<h2 id="Graph-of-Thoughts-Solving-Elaborate-Problems-with-Large-Language-Models"><a href="#Graph-of-Thoughts-Solving-Elaborate-Problems-with-Large-Language-Models" class="headerlink" title="Graph of Thoughts: Solving Elaborate Problems with Large Language Models"></a>Graph of Thoughts: Solving Elaborate Problems with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09687">http://arxiv.org/abs/2308.09687</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/spcl/graph-of-thoughts">https://github.com/spcl/graph-of-thoughts</a></li>
<li>paper_authors: Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, Torsten Hoefler</li>
<li>for: 本研究旨在提高大语言模型（LLM）的提示能力，并超越链条思维或树思维（ToT）的限制。</li>
<li>methods: 本研究提出了Graph of Thoughts（GoT）框架，可以模型LLM生成的信息为自由图，其中单元为“LLM思维”，带有两个端点的边表示了这些单元之间的依赖关系。这种方法可以将不同的LLM思维组合成衍生新的结果，捕捉整个网络的核心意思，或通过反馈循环进行增强。</li>
<li>results: 研究表明，GoT可以与现状下的task比较，例如排序任务上提高质量62%，同时降低成本&gt;31%。此外，GoT还可以扩展到新的思维变换，因此可以用于开拓新的提示方案。这项工作使得LLM的思维更加接近人类思维或脑内的复杂网络机制。<details>
<summary>Abstract</summary>
We introduce Graph of Thoughts (GoT): a framework that advances prompting capabilities in large language models (LLMs) beyond those offered by paradigms such as Chain-of-Thought or Tree of Thoughts (ToT). The key idea and primary advantage of GoT is the ability to model the information generated by an LLM as an arbitrary graph, where units of information ("LLM thoughts") are vertices, and edges correspond to dependencies between these vertices. This approach enables combining arbitrary LLM thoughts into synergistic outcomes, distilling the essence of whole networks of thoughts, or enhancing thoughts using feedback loops. We illustrate that GoT offers advantages over state of the art on different tasks, for example increasing the quality of sorting by 62% over ToT, while simultaneously reducing costs by >31%. We ensure that GoT is extensible with new thought transformations and thus can be used to spearhead new prompting schemes. This work brings the LLM reasoning closer to human thinking or brain mechanisms such as recurrence, both of which form complex networks.
</details>
<details>
<summary>摘要</summary>
我团队介绍 Graph of Thoughts（GoT）框架：可以超越Chain-of-Thought或Tree of Thoughts（ToT）概念的推导能力。GoT的关键思想和优势在于将LLM生成的信息视为一个任意图，其中单元信息（LLM思想）是顶点，而各个顶点之间的边表示这些思想之间的依赖关系。这种方法可以将不同的LLM思想结合成补做出新的结果，浓缩整个网络的核心意义，或者通过反馈循环进行增强。我们证明GoT在不同任务上比ToT高质量，同时Costs下降了>31%。此外，GoT可以扩展新的思想转换，因此可以用于推导新的思路。这项工作使LLM推导更接近人类思维或脑机制，如 recursivity，两者都形成复杂的网络。
</details></li>
</ul>
<hr>
<h2 id="Audiovisual-Moments-in-Time-A-Large-Scale-Annotated-Dataset-of-Audiovisual-Actions"><a href="#Audiovisual-Moments-in-Time-A-Large-Scale-Annotated-Dataset-of-Audiovisual-Actions" class="headerlink" title="Audiovisual Moments in Time: A Large-Scale Annotated Dataset of Audiovisual Actions"></a>Audiovisual Moments in Time: A Large-Scale Annotated Dataset of Audiovisual Actions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09685">http://arxiv.org/abs/2308.09685</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mjoannou/audiovisual-moments-in-time">https://github.com/mjoannou/audiovisual-moments-in-time</a></li>
<li>paper_authors: Michael Joannou, Pia Rotshtein, Uta Noppeney</li>
<li>for: 本研究创造了一个大规模的 audiovisual moments in time（AVMIT）数据集，用于识别 audiovisual 动作事件。</li>
<li>methods: 研究人员使用了11名参与者进行了一项广泛的标注任务，对 Mit 数据集中的 3 秒 audiovisual 视频进行标注。每个试验都需要参与者确定 audiovisual 动作事件是否存在，以及该事件是视频中最为突出的特征。</li>
<li>results: 研究人员从 inicial 收集的 57,177 个 audiovisual 视频中选择了 16 种 distinct 动作类，并生成了 60 个视频测试集（960 个视频）。此外，研究人员还提供了 2 个预计算 audiovisual 特征嵌入，使用 VGGish&#x2F;YamNet 和 VGG16&#x2F;EfficientNetB0 进行音频和视频数据的预处理。研究人员发现，使用 AVMIT 的标注和特征嵌入可以提高 audiovisual 事件识别的性能。<details>
<summary>Abstract</summary>
We present Audiovisual Moments in Time (AVMIT), a large-scale dataset of audiovisual action events. In an extensive annotation task 11 participants labelled a subset of 3-second audiovisual videos from the Moments in Time dataset (MIT). For each trial, participants assessed whether the labelled audiovisual action event was present and whether it was the most prominent feature of the video. The dataset includes the annotation of 57,177 audiovisual videos, each independently evaluated by 3 of 11 trained participants. From this initial collection, we created a curated test set of 16 distinct action classes, with 60 videos each (960 videos). We also offer 2 sets of pre-computed audiovisual feature embeddings, using VGGish/YamNet for audio data and VGG16/EfficientNetB0 for visual data, thereby lowering the barrier to entry for audiovisual DNN research. We explored the advantages of AVMIT annotations and feature embeddings to improve performance on audiovisual event recognition. A series of 6 Recurrent Neural Networks (RNNs) were trained on either AVMIT-filtered audiovisual events or modality-agnostic events from MIT, and then tested on our audiovisual test set. In all RNNs, top 1 accuracy was increased by 2.71-5.94\% by training exclusively on audiovisual events, even outweighing a three-fold increase in training data. We anticipate that the newly annotated AVMIT dataset will serve as a valuable resource for research and comparative experiments involving computational models and human participants, specifically when addressing research questions where audiovisual correspondence is of critical importance.
</details>
<details>
<summary>摘要</summary>
我们介绍了听视频时刻（AVMIT）数据集，这是一个大规模的听视频动作事件数据集。在一项广泛的注释任务中，11名参与者标注了MIT数据集中的3秒听视频示例。每个试验中，参与者评估了听视频动作事件是否存在，以及它是视频中最为突出的特征。该数据集包括57,177个听视频示例，每个示例由3名训练参与者独立地评估。从这些初始集合中，我们创建了一个精心挑选的测试集，包含16种动作类别，每个类别有60个视频示例（共960个视频）。我们还提供了2个预计算的听视频特征嵌入，使用VGGish/YamNet для音频数据和VGG16/EfficientNetB0 для视频数据，从而降低了听视频DNN研究的门槛。我们利用AVMIT注释和特征嵌入的优势来提高听视频事件认识性能。我们使用6个回归神经网络（RNN）在AVMIT听视频事件或MIT多模态无关事件上训练，然后测试在我们的听视频测试集上。在所有RNN中，测试准确率上升了2.71-5.94%，即使训练数据量只有多样化三倍。我们预计AVMIT数据集会成为研究计算模型和人类参与者之间的重要资源，特别是在研究问题中，听视频协调是关键性的。
</details></li>
</ul>
<hr>
<h2 id="Variational-optimization-of-the-amplitude-of-neural-network-quantum-many-body-ground-states"><a href="#Variational-optimization-of-the-amplitude-of-neural-network-quantum-many-body-ground-states" class="headerlink" title="Variational optimization of the amplitude of neural-network quantum many-body ground states"></a>Variational optimization of the amplitude of neural-network quantum many-body ground states</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09664">http://arxiv.org/abs/2308.09664</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jia-Qi Wang, Rong-Qiang He, Zhong-Yi Lu</li>
<li>for: 这个论文目的是找到量子多体系统的凝固态基态，并使用深度学习技术来优化。</li>
<li>methods: 这个论文使用了一种新的方法，即将量子多体系统的变量分解成一个实数valued amplitude神经网络和一个固定的Sign结构，然后优化 amplitude网络。</li>
<li>results: 这个论文在三个典型的量子多体系统上进行了测试，并 obtainted了较低的基态能量，与传统的变量 Monte Carlo（VMC）方法和density matrix renormalization group（DMRG）方法相比。而对于受挫的Heisenberg $J_1$-$J_2$模型，这个论文的结果比文献中的复数valued CNN更好，这表明了 sign structure of complex-valued NQS 难以优化。<details>
<summary>Abstract</summary>
Neural-network quantum states (NQSs), variationally optimized by combining traditional methods and deep learning techniques, is a new way to find quantum many-body ground states and gradually becomes a competitor of traditional variational methods. However, there are still some difficulties in the optimization of NQSs, such as local minima, slow convergence, and sign structure optimization. Here, we split a quantum many-body variational wave function into a multiplication of a real-valued amplitude neural network and a sign structure, and focus on the optimization of the amplitude network while keeping the sign structure fixed. The amplitude network is a convolutional neural network (CNN) with residual blocks, namely a ResNet. Our method is tested on three typical quantum many-body systems. The obtained ground state energies are lower than or comparable to those from traditional variational Monte Carlo (VMC) methods and density matrix renormalization group (DMRG). Surprisingly, for the frustrated Heisenberg $J_1$-$J_2$ model, our results are better than those of the complex-valued CNN in the literature, implying that the sign structure of the complex-valued NQS is difficult to be optimized. We will study the optimization of the sign structure of NQSs in the future.
</details>
<details>
<summary>摘要</summary>
新型神经网络量子状态（NQS），通过结合传统方法和深度学习技术进行变分优化，成为了找寻量子多体底态的新方法，但是仍有一些优化困难，如本地最小值、慢速收敛和正负结构优化。我们将量子多体变量波函数分解为一个实值神经网络幂与一个固定的正负结构，并将焦点放在幂网络的优化上，保持正负结构不变。我们的方法使用了卷积神经网络（CNN） WITH residual块，即ResNet。我们在三个典型量子多体系统上进行测试，获得的底态能量比或等于传统变量 Monte Carlo（VMC）和density matrix renormalization group（DMRG）方法所获得的值更低。 Surprisingly, 对于受挫的Heisenberg-$J_1$-$J_2$模型，我们的结果比文献中的复数值神经网络更好，这 imply That the sign structure of the complex-valued NQS is difficult to be optimized. 我们将在未来研究NQSs的正负结构优化。
</details></li>
</ul>
<hr>
<h2 id="GiGaMAE-Generalizable-Graph-Masked-Autoencoder-via-Collaborative-Latent-Space-Reconstruction"><a href="#GiGaMAE-Generalizable-Graph-Masked-Autoencoder-via-Collaborative-Latent-Space-Reconstruction" class="headerlink" title="GiGaMAE: Generalizable Graph Masked Autoencoder via Collaborative Latent Space Reconstruction"></a>GiGaMAE: Generalizable Graph Masked Autoencoder via Collaborative Latent Space Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09663">http://arxiv.org/abs/2308.09663</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sycny/gigamae">https://github.com/sycny/gigamae</a></li>
<li>paper_authors: Yucheng Shi, Yushun Dong, Qiaoyu Tan, Jundong Li, Ninghao Liu</li>
<li>for: 提高自适应学习模型对图数据的泛化能力</li>
<li>methods: 提出一种图自适应马斯克感知器框架 GiGaMAE，不同于现有的马斯克感知器，我们在这 paper 中提议将共同重建有用和集成的封闭嵌入作为重建目标，以捕捉更广泛的知识。</li>
<li>results: 对三个下游任务和七个数据集进行了广泛的实验，证明 GiGaMAE 在比较难的任务上表现出优于现有基elines。<details>
<summary>Abstract</summary>
Self-supervised learning with masked autoencoders has recently gained popularity for its ability to produce effective image or textual representations, which can be applied to various downstream tasks without retraining. However, we observe that the current masked autoencoder models lack good generalization ability on graph data. To tackle this issue, we propose a novel graph masked autoencoder framework called GiGaMAE. Different from existing masked autoencoders that learn node presentations by explicitly reconstructing the original graph components (e.g., features or edges), in this paper, we propose to collaboratively reconstruct informative and integrated latent embeddings. By considering embeddings encompassing graph topology and attribute information as reconstruction targets, our model could capture more generalized and comprehensive knowledge. Furthermore, we introduce a mutual information based reconstruction loss that enables the effective reconstruction of multiple targets. This learning objective allows us to differentiate between the exclusive knowledge learned from a single target and common knowledge shared by multiple targets. We evaluate our method on three downstream tasks with seven datasets as benchmarks. Extensive experiments demonstrate the superiority of GiGaMAE against state-of-the-art baselines. We hope our results will shed light on the design of foundation models on graph-structured data. Our code is available at: https://github.com/sycny/GiGaMAE.
</details>
<details>
<summary>摘要</summary>
自顾学学习掌握到掩码自适应器的能力，可以生成有效的图像或文本表示，可以应用于多个下游任务无需重新训练。然而，我们发现当前的掩码自适应器模型在图数据上的泛化能力不佳。为解决这个问题，我们提出了一个新的图自适应器框架 called GiGaMAE。与现有的掩码自适应器模型不同，我们在这篇论文中提议使用共同重构有用和完整的嵌入码。我们考虑嵌入码包括图格结构和属性信息作为重构目标，这使我们的模型可以捕捉更加普遍和全面的知识。此外，我们引入了互信息基于的重建损失，这使得我们的模型可以有效地重建多个目标。我们在三个下游任务上使用七个数据集进行评估。广泛的实验表明GiGaMAE比 estado-of-the-art 基线模型更高效。我们希望我们的结果可以指导基本模型的设计在图结构数据上。我们的代码可以在 GitHub 上找到：https://github.com/sycny/GiGaMAE。
</details></li>
</ul>
<hr>
<h2 id="Robust-Uncertainty-Quantification-using-Conformalised-Monte-Carlo-Prediction"><a href="#Robust-Uncertainty-Quantification-using-Conformalised-Monte-Carlo-Prediction" class="headerlink" title="Robust Uncertainty Quantification using Conformalised Monte Carlo Prediction"></a>Robust Uncertainty Quantification using Conformalised Monte Carlo Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09647">http://arxiv.org/abs/2308.09647</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/team-daniel/mc-cp">https://github.com/team-daniel/mc-cp</a></li>
<li>paper_authors: Daniel Bethell, Simos Gerasimou, Radu Calinescu</li>
<li>for: 这篇研究旨在提供一个新的深度学习模型评估方法，以提高深度学习模型在安全敏感应用中的可靠性。</li>
<li>methods: 这篇研究使用了一种新的混合MC-CP方法，它结合了适应MC dropout方法和整构预测方法，以提高深度学习模型的不确定性评估。</li>
<li>results: 经过了严格的实验评估，这篇研究发现MC-CP方法可以实现深度学习模型的更好的可靠性和精度，并且可以轻松地添加到现有的模型中，使其更加简单地应用。<details>
<summary>Abstract</summary>
Deploying deep learning models in safety-critical applications remains a very challenging task, mandating the provision of assurances for the dependable operation of these models. Uncertainty quantification (UQ) methods estimate the model's confidence per prediction, informing decision-making by considering the effect of randomness and model misspecification. Despite the advances of state-of-the-art UQ methods, they are computationally expensive or produce conservative prediction sets/intervals. We introduce MC-CP, a novel hybrid UQ method that combines a new adaptive Monte Carlo (MC) dropout method with conformal prediction (CP). MC-CP adaptively modulates the traditional MC dropout at runtime to save memory and computation resources, enabling predictions to be consumed by CP, yielding robust prediction sets/intervals. Throughout comprehensive experiments, we show that MC-CP delivers significant improvements over advanced UQ methods, like MC dropout, RAPS and CQR, both in classification and regression benchmarks. MC-CP can be easily added to existing models, making its deployment simple.
</details>
<details>
<summary>摘要</summary>
部署深度学习模型在安全关键应用中仍然是一项非常具有挑战性的任务，需要提供对模型可靠性的保证。不约束量评估（UQ）方法可以估算模型每个预测的可信度，为决策做出考虑，考虑随机性和模型误差的效果。despite the advances of state-of-the-art UQ methods, they are computationally expensive or produce conservative prediction sets/intervals. 我们介绍MC-CP，一种新的混合MC dropout和CP（确定预测）方法。MC-CP在运行时动态调整传统MC dropout，以避免过度使用内存和计算资源，使预测可以被CP所使用，生成Robust预测集/区间。通过广泛的实验，我们表明MC-CP在分类和回归 benchmark上都能够实现显著改进，比如MC dropout、RAPS和CQR。MC-CP可以轻松地添加到现有模型中，使其部署简单。
</details></li>
</ul>
<hr>
<h2 id="biquality-learn-a-Python-library-for-Biquality-Learning"><a href="#biquality-learn-a-Python-library-for-Biquality-Learning" class="headerlink" title="biquality-learn: a Python library for Biquality Learning"></a>biquality-learn: a Python library for Biquality Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09643">http://arxiv.org/abs/2308.09643</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/biquality-learn/biquality-learn">https://github.com/biquality-learn/biquality-learn</a></li>
<li>paper_authors: Pierre Nodet, Vincent Lemaire, Alexis Bondu, Antoine Cornuéjols</li>
<li>For: The paper is written for practitioners and researchers who need to handle weak supervision and dataset shifts in machine learning, and who want to use a Python library for Biquality Learning.* Methods: The paper proposes a machine learning framework called Biquality Learning, which can handle multiple weaknesses of supervision and dataset shifts without assumptions on their nature and level. The proposed library, biquality-learn, provides an intuitive and consistent API for learning machine learning models from biquality data.* Results: The paper does not present specific results, but rather proposes a new machine learning framework and library for handling weak supervision and dataset shifts. The proposed library, biquality-learn, is designed to be accessible and easy to use for everyone, and enables researchers to experiment in a reproducible way on biquality data.<details>
<summary>Abstract</summary>
The democratization of Data Mining has been widely successful thanks in part to powerful and easy-to-use Machine Learning libraries. These libraries have been particularly tailored to tackle Supervised Learning. However, strong supervision signals are scarce in practice, and practitioners must resort to weak supervision. In addition to weaknesses of supervision, dataset shifts are another kind of phenomenon that occurs when deploying machine learning models in the real world. That is why Biquality Learning has been proposed as a machine learning framework to design algorithms capable of handling multiple weaknesses of supervision and dataset shifts without assumptions on their nature and level by relying on the availability of a small trusted dataset composed of cleanly labeled and representative samples. Thus we propose biquality-learn: a Python library for Biquality Learning with an intuitive and consistent API to learn machine learning models from biquality data, with well-proven algorithms, accessible and easy to use for everyone, and enabling researchers to experiment in a reproducible way on biquality data.
</details>
<details>
<summary>摘要</summary>
“数据挖掘的民主化得到了广泛的成功，很大的准确是归功于强大且易于使用的机器学习库。这些库具有特别地针对超级vised学习。然而，实际中强制监督信号强度很弱，实践者必须采用弱监督。此外，在部署机器学习模型时，数据Shift是一种常见的现象。因此，我们提出了Biquality学习框架，设计用于处理多种弱监督和数据Shift的算法，不假设其性质和水平。Biquality学习库提供了一个小型可信的数据集，包含清晰标注和代表性样本，并提供了一个intuitive和一致的API，让人们轻松地学习机器学习模型从biquality数据，并且具有证明的算法、易于使用和可重复地进行研究。”Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know and I can provide that as well.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/19/cs.LG_2023_08_19/" data-id="clly3dvzo006i09883d0b3wkw" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_08_19" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/19/cs.SD_2023_08_19/" class="article-date">
  <time datetime="2023-08-18T16:00:00.000Z" itemprop="datePublished">2023-08-19</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/19/cs.SD_2023_08_19/">cs.SD - 2023-08-19 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Spatial-Reconstructed-Local-Attention-Res2Net-with-F0-Subband-for-Fake-Speech-Detection"><a href="#Spatial-Reconstructed-Local-Attention-Res2Net-with-F0-Subband-for-Fake-Speech-Detection" class="headerlink" title="Spatial Reconstructed Local Attention Res2Net with F0 Subband for Fake Speech Detection"></a>Spatial Reconstructed Local Attention Res2Net with F0 Subband for Fake Speech Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09944">http://arxiv.org/abs/2308.09944</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cunhang Fan, Jun Xue, Jianhua Tao, Jiangyan Yi, Chenglong Wang, Chengshi Zheng, Zhao Lv</li>
<li>for: 本研究旨在提高假语音识别（FSD） task 的性能，通过提出一种新的F0子带和一种新的SR-LA Res2Net网络模型。</li>
<li>methods: 本研究使用了一种名为SR-LA Res2Net的新网络模型，其包括一个Res2Net底层网络和一个地域重建机制，以获取多尺度信息并避免损失重要信息。此外，本研究还使用了一种本地注意力机制，使模型更加注重F0子带的本地信息。</li>
<li>results: 根据ASVspoof 2019 LA dataset的实验结果，我们的提议方法实现了EER值为0.47%和min t-DCF值为0.0159，与其他单个系统相比，实现了状态的杰出性。<details>
<summary>Abstract</summary>
The rhythm of synthetic speech is usually too smooth, which causes that the fundamental frequency (F0) of synthetic speech is significantly different from that of real speech. It is expected that the F0 feature contains the discriminative information for the fake speech detection (FSD) task. In this paper, we propose a novel F0 subband for FSD. In addition, to effectively model the F0 subband so as to improve the performance of FSD, the spatial reconstructed local attention Res2Net (SR-LA Res2Net) is proposed. Specifically, Res2Net is used as a backbone network to obtain multiscale information, and enhanced with a spatial reconstruction mechanism to avoid losing important information when the channel group is constantly superimposed. In addition, local attention is designed to make the model focus on the local information of the F0 subband. Experimental results on the ASVspoof 2019 LA dataset show that our proposed method obtains an equal error rate (EER) of 0.47% and a minimum tandem detection cost function (min t-DCF) of 0.0159, achieving the state-of-the-art performance among all of the single systems.
</details>
<details>
<summary>摘要</summary>
《人工语音的节奏通常太平滑，导致人工语音的基本频率（F0）与真实语音的F0有所不同。这些F0特征包含潜在的假语音检测（FSD）任务中的 дискリมิنatif信息。在本文中，我们提出了一种新的F0子带 для FSD。此外，为了有效地模型F0子带，以提高FSD性能，我们提出了空间重建本地注意力Res2Net（SR-LA Res2Net）。具体来说，Res2Net被用作幕后网络，以获取多尺度信息，并在执行常数抽象操作时增加空间重建机制，以避免损失重要信息。此外，本地注意力被设计来使模型关注F0子带的本地信息。实验结果表明，我们提出的方法在ASVspoof 2019 LA数据集上获得了EER值为0.47%和min t-DCF值为0.0159，与所有单个系统中的性能相当。》Note: Simplified Chinese is a romanization of Chinese, and the translation is based on the standardized pronunciation of Simplified Chinese. The actual pronunciation may vary depending on the speaker's accent and intonation.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/19/cs.SD_2023_08_19/" data-id="clly3dw10009r09889mirhxh1" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_08_19" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/19/eess.IV_2023_08_19/" class="article-date">
  <time datetime="2023-08-18T16:00:00.000Z" itemprop="datePublished">2023-08-19</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/19/eess.IV_2023_08_19/">eess.IV - 2023-08-19 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="CRC-ICM-Colorectal-Cancer-Immune-Cell-Markers-Pattern-Dataset"><a href="#CRC-ICM-Colorectal-Cancer-Immune-Cell-Markers-Pattern-Dataset" class="headerlink" title="CRC-ICM: Colorectal Cancer Immune Cell Markers Pattern Dataset"></a>CRC-ICM: Colorectal Cancer Immune Cell Markers Pattern Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10033">http://arxiv.org/abs/2308.10033</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zahra Mokhtari, Elham Amjadi, Hamidreza Bolhasani, Zahra Faghih, AmirReza Dehghanian, Marzieh Rezaei</li>
<li>For: The paper is focused on the immune checkpoints in the tumor microenvironment of colorectal cancer (CRC) and their potential as therapeutic targets for cancer treatment.* Methods: The paper uses a dataset of 1756 images from 136 patients with CRC, stained with specific antibodies for CD3, CD8, CD45RO, PD-1, LAG3, and Tim3 to investigate the expression of immune checkpoints in the tumor microenvironment.* Results: The paper reports on the differences in immune checkpoint expression between tumors located in the right and left sides of the colon, and the potential implications of these differences for the prognosis of CRC patients.Here is the information in Simplified Chinese text:</li>
<li>for: 这篇论文关注了大肠癌（CRC）的免疫检查点在肿瘤微环境中的表达，以及这些检查点作为癌症治疗的可能性。</li>
<li>methods: 这篇论文使用了136名CRC患者的1756个图像，用具体抗体检测CD3、CD8、CD45RO、PD-1、LAG3和Tim3等免疫检查点的表达。</li>
<li>results: 这篇论文报告了右侧和左侧大肠的肿瘤表达免疫检查点之间的差异，以及这些差异对CRC患者的预后的可能影响。<details>
<summary>Abstract</summary>
Colorectal Cancer (CRC) is the second most common cause of cancer death in the world, ad can be identified by the location of the primary tumor in the large intestine: right and left colon, and rectum. Based on the location, CRC shows differences in chromosomal and molecular characteristics, microbiomes incidence, pathogenesis, and outcome. It has been shown that tumors on left and right sides also have different immune landscape, so the prognosis may be different based on the primary tumor locations. It is widely accepted that immune components of the tumor microenvironment (TME) plays a critical role in tumor development. One of the critical regulatory molecules in the TME is immune checkpoints that as the gatekeepers of immune responses regulate the infiltrated immune cell functions. Inhibitory immune checkpoints such as PD-1, Tim3, and LAG3, as the main mechanism of immune suppression in TME overexpressed and result in further development of the tumor. The images of this dataset have been taken from colon tissues of patients with CRC, stained with specific antibodies for CD3, CD8, CD45RO, PD-1, LAG3 and Tim3. The name of this dataset is CRC-ICM and contains 1756 images related to 136 patients. The initial version of CRC-ICM is published on Elsevier Mendeley dataset portal, and the latest version is accessible via: https://databiox.com
</details>
<details>
<summary>摘要</summary>
便膜癌（CRC）是全球第二常见的癌病原因，可以根据主 tumor 的位置在大肠分为右和左边colon，以及肛门。根据位置，CRC 会出现不同的染色体和分子特征、微生物发生率、生物学发展和结果。已经证明左右两边的肿瘤也有不同的免疫景况，因此预后可能因主 tumor 的位置而异。免疫组件在肿瘤微环境中（TME）plays a critical role in tumor development。一种critical regulatory molecules in TME 是免疫检查点，它们作为免疫 responses 的门槛，调控入侵的免疫细胞功能。压缩性免疫检查点，如PD-1、Tim3和LAG3，是免疫抑制的主要机制，它们在TME 过剩表达，导致肿瘤的进一步发展。这些图像来自于colon这些癌病患者的患者，这些图像用specific抗体CD3、CD8、CD45RO、PD-1、LAG3和Tim3染色。这个 dataset 的名称是CRC-ICM，它包含 1756 个图像，与 136 名病患相关。初版CRC-ICM 已经发表在Elsevier Mendeley 数据集Portail，而最新版本可以通过以下连结获取：https://databiox.com。
</details></li>
</ul>
<hr>
<h2 id="Deformable-Detection-Transformer-for-Microbubble-Localization-in-Ultrasound-Localization-Microscopy"><a href="#Deformable-Detection-Transformer-for-Microbubble-Localization-in-Ultrasound-Localization-Microscopy" class="headerlink" title="Deformable-Detection Transformer for Microbubble Localization in Ultrasound Localization Microscopy"></a>Deformable-Detection Transformer for Microbubble Localization in Ultrasound Localization Microscopy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09845">http://arxiv.org/abs/2308.09845</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sepideh K. Gharamaleki, Brandon Helfield, Hassan Rivaz</li>
<li>for: 提高ultrasound imaging中半波长解像力的问题</li>
<li>methods: 使用DEformable DETR（DE-DETR）方法，带有多级弹性注意力分配，从而提高精度和准确性</li>
<li>results: 对比DETR方法，DE-DETR方法在精度和准确性方面均显示了改善，并且在最终超分解图像中也得到了更好的效果<details>
<summary>Abstract</summary>
To overcome the half a wavelength resolution limitations of ultrasound imaging, microbubbles (MBs) have been utilized widely in the field. Conventional MB localization methods are limited whether by exhaustive parameter tuning or considering a fixed Point Spread Function (PSF) for MBs. This questions their adaptability to different imaging settings or depths. As a result, development of methods that don't rely on manually adjusted parameters is crucial. Previously, we used a transformer-based approach i.e. DEtection TRansformer (DETR) (arXiv:2005.12872v3 and arXiv:2209.11859v1) to address the above mentioned issues. However, DETR suffers from long training times and lower precision for smaller objects. In this paper, we propose the application of DEformable DETR (DE-DETR) ( arXiv:2010.04159) for MB localization to mitigate DETR's above mentioned challenges. As opposed to DETR, where attention is casted upon all grid pixels, DE-DETR utilizes a multi-scale deformable attention to distribute attention within a limited budget. To evaluate the proposed strategy, pre-trained DE-DETR was fine-tuned on a subset of the dataset provided by the IEEE IUS Ultra-SR challenge organizers using transfer learning principles and subsequently we tested the network on the rest of the dataset, excluding the highly correlated frames. The results manifest an improvement both in precision and recall and the final super-resolution maps compared to DETR.
</details>
<details>
<summary>摘要</summary>
通过超过半波长的限制， ultrasound 影像中的微ubble (MB) 已经广泛应用于领域中。传统的 MB 位置方法受限于手动调整的参数或者假设固定的点扩散函数 (PSF)  для MBs。这问题了它们在不同的描述设置或深度下的适应性。因此，开发不依赖于手动调整参数的方法是关键。在先前的研究中，我们使用 transformer 基本的方法，即检测转换器 (DETR) （arXiv: 2005.12872v3 和 arXiv: 2209.11859v1）来解决上述问题。然而， DETR 受到训练时间过长和对小对象的精度较低的问题。在这篇论文中，我们提议将 DEformable DETR (DE-DETR) （arXiv: 2010.04159）应用于 MB 位置检测，以减少 DETR 的上述挑战。与 DETR 不同，DE-DETR 使用多尺度可变的注意力来分配注意力，而不是将注意力投射到所有的格子像素上。为评估提议的策略，我们先使用 transfer learning 原则来精心 DE-DETR ，然后在数据集中测试网络，排除高相关性的帧。结果表明，提议的策略在精度和准确性方面得到改进，并且在最终的超解像图中比 DETR 更好。
</details></li>
</ul>
<hr>
<h2 id="Cross-modality-Attention-based-Multimodal-Fusion-for-Non-small-Cell-Lung-Cancer-NSCLC-Patient-Survival-Prediction"><a href="#Cross-modality-Attention-based-Multimodal-Fusion-for-Non-small-Cell-Lung-Cancer-NSCLC-Patient-Survival-Prediction" class="headerlink" title="Cross-modality Attention-based Multimodal Fusion for Non-small Cell Lung Cancer (NSCLC) Patient Survival Prediction"></a>Cross-modality Attention-based Multimodal Fusion for Non-small Cell Lung Cancer (NSCLC) Patient Survival Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09831">http://arxiv.org/abs/2308.09831</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruining Deng, Nazim Shaikh, Gareth Shannon, Yao Nie</li>
<li>for: 预测和诊断结果提高，尤其是诊断非小细胞肺癌（NSCLC）患者的存活情况。</li>
<li>methods: 跨模态注意力基本的多模态融合策略，通过评估不同模式之间的关系来混合特征。</li>
<li>results: 与单模式相比，提出的融合方法在实验中实现了c-指数0.6587，表明可以充分利用不同模式之间的知识，提高诊断和预测的准确性。<details>
<summary>Abstract</summary>
Cancer prognosis and survival outcome predictions are crucial for therapeutic response estimation and for stratifying patients into various treatment groups. Medical domains concerned with cancer prognosis are abundant with multiple modalities, including pathological image data and non-image data such as genomic information. To date, multimodal learning has shown potential to enhance clinical prediction model performance by extracting and aggregating information from different modalities of the same subject. This approach could outperform single modality learning, thus improving computer-aided diagnosis and prognosis in numerous medical applications. In this work, we propose a cross-modality attention-based multimodal fusion pipeline designed to integrate modality-specific knowledge for patient survival prediction in non-small cell lung cancer (NSCLC). Instead of merely concatenating or summing up the features from different modalities, our method gauges the importance of each modality for feature fusion with cross-modality relationship when infusing the multimodal features. Compared with single modality, which achieved c-index of 0.5772 and 0.5885 using solely tissue image data or RNA-seq data, respectively, the proposed fusion approach achieved c-index 0.6587 in our experiment, showcasing the capability of assimilating modality-specific knowledge from varied modalities.
</details>
<details>
<summary>摘要</summary>
肿瘤诊断和存活结果预测是肿瘤治疗的关键因素，可以用于评估治疗效果和将患者分组。医学领域中关于肿瘤诊断的数据非常丰富，包括图像数据和非图像数据，如基因信息。到目前为止，多modal学习已经显示出了提高诊断模型性能的潜力，通过提取和综合不同模式的信息来提高计算机辅助诊断和预测的性能。在这项工作中，我们提出了跨模式关注-基于多模式融合管线，用于汇集不同模式特有的知识来预测患者存活情况。而不是仅将不同模式的特征 concatenate 或 sum，我们的方法会测量不同模式之间的关系，以便在融合多模式特征时进行相应的权重调整。相比单模式，我们的融合方法在实验中实现了c-index 0.6587，超过了使用具体图像数据或 RNA-seq 数据单独进行预测的性能。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/19/eess.IV_2023_08_19/" data-id="clly3dw2k00eu09883huu045h" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/18/cs.LG_2023_08_18/" class="article-date">
  <time datetime="2023-08-17T16:00:00.000Z" itemprop="datePublished">2023-08-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/18/cs.LG_2023_08_18/">cs.LG - 2023-08-18 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Revisiting-Skin-Tone-Fairness-in-Dermatological-Lesion-Classification"><a href="#Revisiting-Skin-Tone-Fairness-in-Dermatological-Lesion-Classification" class="headerlink" title="Revisiting Skin Tone Fairness in Dermatological Lesion Classification"></a>Revisiting Skin Tone Fairness in Dermatological Lesion Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09640">http://arxiv.org/abs/2308.09640</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tkalbl/revisitingskintonefairness">https://github.com/tkalbl/revisitingskintonefairness</a></li>
<li>paper_authors: Thorsten Kalb, Kaisar Kushibar, Celia Cintas, Karim Lekadir, Oliver Diaz, Richard Osuala</li>
<li>for: 评估皮肤疾病分类中的公平性是非常重要的，因为皮肤疾病在不同皮肤色调上表现不同。但由于公共数据集中没有皮肤色调标签，因此建立公平的分类器受到限制。</li>
<li>methods: 以前的研究使用了个体学型角度（ITA）来估算皮肤色调，并将其分为不同的皮肤色调类别。然而，这些方法之间存在很大的不一致，这可能是因为数据集的不一致性带来的。</li>
<li>results: 我们的分析发现，以前发表的研究中的皮肤色调估算方法之间存在很大的不一致，这可能是因为数据集的不一致性带来的。此外，我们还发现了数据集的不多样化性，这限制了其用于公平性分析的可用性。因此，我们建议进一步研究ITA估算方法和数据集多样化性的增强，以便更 conclussive 的公平性评估。<details>
<summary>Abstract</summary>
Addressing fairness in lesion classification from dermatological images is crucial due to variations in how skin diseases manifest across skin tones. However, the absence of skin tone labels in public datasets hinders building a fair classifier. To date, such skin tone labels have been estimated prior to fairness analysis in independent studies using the Individual Typology Angle (ITA). Briefly, ITA calculates an angle based on pixels extracted from skin images taking into account the lightness and yellow-blue tints. These angles are then categorised into skin tones that are subsequently used to analyse fairness in skin cancer classification. In this work, we review and compare four ITA-based approaches of skin tone classification on the ISIC18 dataset, a common benchmark for assessing skin cancer classification fairness in the literature. Our analyses reveal a high disagreement among previously published studies demonstrating the risks of ITA-based skin tone estimation methods. Moreover, we investigate the causes of such large discrepancy among these approaches and find that the lack of diversity in the ISIC18 dataset limits its use as a testbed for fairness analysis. Finally, we recommend further research on robust ITA estimation and diverse dataset acquisition with skin tone annotation to facilitate conclusive fairness assessments of artificial intelligence tools in dermatology. Our code is available at https://github.com/tkalbl/RevisitingSkinToneFairness.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "lesion classification" is translated as "皮肤病变分类" (pī bì bì huàng bèng)* "dermatological images" is translated as "皮肤图像" (pī bì tú xiàng)* "skin tone" is translated as "皮肤色调" (pī bì sè tiào)* "Individual Typology Angle" is translated as "个体类型角度" (gè tǐ xìng tiě jiàng)* "ITA" is translated as "ITA" (ī tà)* "skin cancer classification" is translated as "皮肤癌分类" (pī bì bì hán bèng)* "ISIC18 dataset" is translated as "ISIC18数据集" (yī sī sī wù shù)* "artificial intelligence tools" is translated as "人工智能工具" (rén gōng zhì neng gōng bù)
</details></li>
</ul>
<hr>
<h2 id="Development-of-a-Neural-Network-based-Method-for-Improved-Imputation-of-Missing-Values-in-Time-Series-Data-by-Repurposing-DataWig"><a href="#Development-of-a-Neural-Network-based-Method-for-Improved-Imputation-of-Missing-Values-in-Time-Series-Data-by-Repurposing-DataWig" class="headerlink" title="Development of a Neural Network-based Method for Improved Imputation of Missing Values in Time Series Data by Repurposing DataWig"></a>Development of a Neural Network-based Method for Improved Imputation of Missing Values in Time Series Data by Repurposing DataWig</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09635">http://arxiv.org/abs/2308.09635</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Zhang</li>
<li>For: This paper is written for researchers and practitioners who work with time series data and need to impute missing values in their datasets.* Methods: The paper proposes a modified version of the DataWig method, called tsDataWig, which is designed to handle time series data and can directly impute missing values in complex time series datasets.* Results: The author demonstrates that tsDataWig outperforms the original DataWig and current state-of-the-art methods for time series data imputation on simulated and real-world datasets, and has the potential to be applied to large and complex time series datasets with millions of samples, high dimensional variables, and heterogeneous data types.<details>
<summary>Abstract</summary>
Time series data are observations collected over time intervals. Successful analysis of time series data captures patterns such as trends, cyclicity and irregularity, which are crucial for decision making in research, business, and governance. However, missing values in time series data occur often and present obstacles to successful analysis, thus they need to be filled with alternative values, a process called imputation. Although various approaches have been attempted for robust imputation of time series data, even the most advanced methods still face challenges including limited scalability, poor capacity to handle heterogeneous data types and inflexibility due to requiring strong assumptions of data missing mechanisms. Moreover, the imputation accuracy of these methods still has room for improvement. In this study, I developed tsDataWig (time-series DataWig) by modifying DataWig, a neural network-based method that possesses the capacity to process large datasets and heterogeneous data types but was designed for non-time series data imputation. Unlike the original DataWig, tsDataWig can directly handle values of time variables and impute missing values in complex time series datasets. Using one simulated and three different complex real-world time series datasets, I demonstrated that tsDataWig outperforms the original DataWig and the current state-of-the-art methods for time series data imputation and potentially has broad application due to not requiring strong assumptions of data missing mechanisms. This study provides a valuable solution for robustly imputing missing values in challenging time series datasets, which often contain millions of samples, high dimensional variables, and heterogeneous data types.
</details>
<details>
<summary>摘要</summary>
时间序列数据是在时间间隔内收集的观测记录。成功分析时间序列数据可以捕捉到趋势、征 циclical 和不规则性，这些特征对研究、商业和管理决策都非常重要。然而，时间序列数据中的缺失值很常 occurrence 和阻碍了成功分析，因此需要将其替换为可行的值，一个过程称为填充。虽然有很多方法用于robust imputation of time series data，但even the most advanced methods still face challenges including limited scalability, poor capacity to handle heterogeneous data types, and inflexibility due to requiring strong assumptions of data missing mechanisms。此外，这些方法的填充精度仍然有待提高。在这项研究中，我开发了tsDataWig（时间序列数据wig），通过修改DataWig，一种基于神经网络的方法，该方法可以处理大量数据和多种数据类型，但是它是非时间序列数据填充的设计。不同于原始DataWig，tsDataWig可以直接处理时间变量的值并在复杂的时间序列数据集中填充缺失值。使用一个 simulate 和三个不同的复杂实际时间序列数据集，我证明了tsDataWig在时间序列数据填充中表现出色，并且可能具有广泛的应用，因为它不需要强制的数据缺失机制假设。这项研究为Robustly imputing missing values in challenging time series datasets，这些数据集通常包含百万个样本，高维度变量和多种数据类型提供了有价值的解决方案。
</details></li>
</ul>
<hr>
<h2 id="VALERIE22-–-A-photorealistic-richly-metadata-annotated-dataset-of-urban-environments"><a href="#VALERIE22-–-A-photorealistic-richly-metadata-annotated-dataset-of-urban-environments" class="headerlink" title="VALERIE22 – A photorealistic, richly metadata annotated dataset of urban environments"></a>VALERIE22 – A photorealistic, richly metadata annotated dataset of urban environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09632">http://arxiv.org/abs/2308.09632</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oliver Grau, Korbinian Hagn</li>
<li>for: 本研究旨在帮助深度神经网络（DNN）的识别性能理解，特别是在城市环境中的自动驾驶场景中。</li>
<li>methods: 本研究使用了VALERIE工具渠道生成了一个名为VALERIE22的 sintetic数据集，该数据集包含了高品质的摄像头视频数据，并且具有丰富的元数据，如图像精度遮挡率、物体位置和距离相机的信息。</li>
<li>results: 根据性能指标，VALERIE22数据集与其他公共可用的数据集进行了比较，结果显示VALERIE22是目前公开领域中最高性能的 sintetic数据集之一。<details>
<summary>Abstract</summary>
The VALERIE tool pipeline is a synthetic data generator developed with the goal to contribute to the understanding of domain-specific factors that influence perception performance of DNNs (deep neural networks). This work was carried out under the German research project KI Absicherung in order to develop a methodology for the validation of DNNs in the context of pedestrian detection in urban environments for automated driving. The VALERIE22 dataset was generated with the VALERIE procedural tools pipeline providing a photorealistic sensor simulation rendered from automatically synthesized scenes. The dataset provides a uniquely rich set of metadata, allowing extraction of specific scene and semantic features (like pixel-accurate occlusion rates, positions in the scene and distance + angle to the camera). This enables a multitude of possible tests on the data and we hope to stimulate research on understanding performance of DNNs. Based on performance metric a comparison with several other publicly available datasets is provided, demonstrating that VALERIE22 is one of best performing synthetic datasets currently available in the open domain.
</details>
<details>
<summary>摘要</summary>
VALERIE工具管道是一个用于生成 sintetic数据的工具，旨在提高深度神经网络（DNN）的感知性能的理解。这项工作是在德国研究项目“KI Absicherung”下进行的，以开发一种用于自动驾驶中人员检测 scenes的 DNN 验证方法。VALERIE22数据集是通过VALERIE生成器工具管道生成的，它提供了高度具有 metadata，如自动生成场景中的干扰率、位置和距离摄像头的信息，这些信息可以用于进行多种可能的数据分析和研究。根据表现指标，我们比较了VALERIE22数据集与其他公共可用的数据集，结果显示VALERIE22是目前公开领域中最佳的 sintetic 数据集之一。
</details></li>
</ul>
<hr>
<h2 id="Learning-Computational-Efficient-Bots-with-Costly-Features"><a href="#Learning-Computational-Efficient-Bots-with-Costly-Features" class="headerlink" title="Learning Computational Efficient Bots with Costly Features"></a>Learning Computational Efficient Bots with Costly Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09629">http://arxiv.org/abs/2308.09629</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anthony Kobanda, Valliappan C. A., Joshua Romoff, Ludovic Denoyer</li>
<li>for: 这个研究旨在提高深度强化学习（DRL）技术在各种决策过程中的computational efficiency，以及在real-time setting中的决策过程中的能效性。</li>
<li>methods: 本研究提出了一个通用的Offline学习方法，考虑了输入特征 Computational Cost的问题。我们提出了一个名为Budgeted Decision Transformer的扩展，将成本限制纳入到决策过程中，以避免过度运算。</li>
<li>results: 我们在多个任务上进行了实验，包括D4RLbenchmark和复杂的3D环境，以及类似于游戏的环境。结果显示，我们的方法可以在computational resources方面实现显著的节省，同时保持与 класи方法相似的性能。<details>
<summary>Abstract</summary>
Deep reinforcement learning (DRL) techniques have become increasingly used in various fields for decision-making processes. However, a challenge that often arises is the trade-off between both the computational efficiency of the decision-making process and the ability of the learned agent to solve a particular task. This is particularly critical in real-time settings such as video games where the agent needs to take relevant decisions at a very high frequency, with a very limited inference time.   In this work, we propose a generic offline learning approach where the computation cost of the input features is taken into account. We derive the Budgeted Decision Transformer as an extension of the Decision Transformer that incorporates cost constraints to limit its cost at inference. As a result, the model can dynamically choose the best input features at each timestep. We demonstrate the effectiveness of our method on several tasks, including D4RL benchmarks and complex 3D environments similar to those found in video games, and show that it can achieve similar performance while using significantly fewer computational resources compared to classical approaches.
</details>
<details>
<summary>摘要</summary>
深度强化学习（DRL）技术在不同领域的决策过程中得到广泛应用。然而，经常出现的挑战是决策过程的计算效率和学习的能力之间的负面相互作用。这 particualry critical在实时设定中，如游戏中，Agent需要在 Very high frequency 上做相关的决策，具有很限的推理时间。在这种情况下，我们提出了一种通用的 offline 学习方法，其中输入特征的计算成本被考虑。我们 derive the Budgeted Decision Transformer 作为 Decision Transformer 的扩展，该模型内置成本约束，以限制其推理时间的成本。因此，模型可以在每个时间步骤上动态选择最佳的输入特征。我们在多个任务上验证了我们的方法，包括 D4RL benchmark 和复杂的 3D 环境，并证明它可以在使用较少的计算资源的情况下达到相似的性能。
</details></li>
</ul>
<hr>
<h2 id="Constrained-Bayesian-Optimization-Using-a-Lagrange-Multiplier-Applied-to-Power-Transistor-Design"><a href="#Constrained-Bayesian-Optimization-Using-a-Lagrange-Multiplier-Applied-to-Power-Transistor-Design" class="headerlink" title="Constrained Bayesian Optimization Using a Lagrange Multiplier Applied to Power Transistor Design"></a>Constrained Bayesian Optimization Using a Lagrange Multiplier Applied to Power Transistor Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09612">http://arxiv.org/abs/2308.09612</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ping-Ju Chuang, Ali Saadat, Sara Ghazvini, Hal Edwards, William G. Vandenberghe</li>
<li>for: 优化LDMOS晶体管的设计过程，实现目标破坏电压（BV）。</li>
<li>methods: 使用受限的极大优化算法（BO），通过拉格朗日积分来转化受限BO问题，并使用可变的拉格朗日多多乘数来解决具有成本评估的约束。</li>
<li>results: 通过这种算法，设计者可以在设计空间中设定目标BV，并自动获得满足优化FOM和目标BV约束的设备。此外，我们还在30-50V范围内对我们的设备的物理限制进行了探索。<details>
<summary>Abstract</summary>
We propose a novel constrained Bayesian Optimization (BO) algorithm optimizing the design process of Laterally-Diffused Metal-Oxide-Semiconductor (LDMOS) transistors while realizing a target Breakdown Voltage (BV). We convert the constrained BO problem into a conventional BO problem using a Lagrange multiplier. Instead of directly optimizing the traditional Figure-of-Merit (FOM), we set the Lagrangian as the objective function of BO. This adaptive objective function with a changeable Lagrange multiplier can address constrained BO problems which have constraints that require costly evaluations, without the need for additional surrogate models to approximate constraints. Our algorithm enables a device designer to set the target BV in the design space, and obtain a device that satisfies the optimized FOM and the target BV constraint automatically. Utilizing this algorithm, we have also explored the physical limits of the FOM for our devices in 30 - 50 V range within the defined design space.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的受限 bayesian优化（BO）算法，用于优化扩散金属氧化半导体（LDMOS）晶体管的设计过程，同时实现目标电压触发 voltage（BV）。我们将受限 BO 问题转化为一个普通的 BO 问题，使用拉格朗日 multiply 来实现。而不是直接优化传统的图像因数（FOM），我们设置了 Lagrange 函数作为 BO 的目标函数。这种可变的 Lagrange 多重器可以解决受限 BO 问题，无需额外的规格模型来近似约束。我们的算法允许设计者在设计空间中设置目标 BV，并自动获得满足优化 FOM 和目标 BV 约束的设备。通过这种算法，我们还在30-50 V 范围内对我们的设备的物理限制进行了探索。
</details></li>
</ul>
<hr>
<h2 id="Solving-PDEs-on-Spheres-with-Physics-Informed-Convolutional-Neural-Networks"><a href="#Solving-PDEs-on-Spheres-with-Physics-Informed-Convolutional-Neural-Networks" class="headerlink" title="Solving PDEs on Spheres with Physics-Informed Convolutional Neural Networks"></a>Solving PDEs on Spheres with Physics-Informed Convolutional Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09605">http://arxiv.org/abs/2308.09605</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guanhang Lei, Zhen Lei, Lei Shi, Chenyu Zeng, Ding-Xuan Zhou</li>
<li>for: 解决高维度部分几何方程（PDEs）的数学理论分析</li>
<li>methods: 使用和改进深度卷积神经网络的最新近似结果和球面幂分析，证明 Sobolev  нор下的预测误差的Upper bound</li>
<li>results: 确认和补充了实验结果，并在高维度PDEs中解决尺度味的味In English:</li>
<li>for: Solving high-dimensional partial differential equations (PDEs) from a mathematical perspective</li>
<li>methods: Using and improving the latest approximation results of deep convolutional neural networks and spherical harmonic analysis to establish an upper bound for the approximation error with respect to the Sobolev norm</li>
<li>results: Confirming and supplementing the experimental results, and solving the curse of dimensionality in high-dimensional PDEs<details>
<summary>Abstract</summary>
Physics-informed neural networks (PINNs) have been demonstrated to be efficient in solving partial differential equations (PDEs) from a variety of experimental perspectives. Some recent studies have also proposed PINN algorithms for PDEs on surfaces, including spheres. However, theoretical understanding of the numerical performance of PINNs, especially PINNs on surfaces or manifolds, is still lacking. In this paper, we establish rigorous analysis of the physics-informed convolutional neural network (PICNN) for solving PDEs on the sphere. By using and improving the latest approximation results of deep convolutional neural networks and spherical harmonic analysis, we prove an upper bound for the approximation error with respect to the Sobolev norm. Subsequently, we integrate this with innovative localization complexity analysis to establish fast convergence rates for PICNN. Our theoretical results are also confirmed and supplemented by our experiments. In light of these findings, we explore potential strategies for circumventing the curse of dimensionality that arises when solving high-dimensional PDEs.
</details>
<details>
<summary>摘要</summary>
physics-informed neural networks (PINNs) 物理学习神经网络 (PINNs) 已经在各种实验方面证明其能够有效地解决部分杂度方程 (PDEs)。一些最近的研究还提出了 PINN 算法用于 PDEs 的表面上，包括球体。然而，对 PINNs 的数学性能，特别是在表面或者拓扑上的数学性能，仍然缺乏一般理解。在这篇论文中，我们建立了 PICNN 的准确分析，用于解决球体上的 PDEs。我们利用最新的深度卷积神经网络的近似结果和球面幂分析，证明了在 Sobolev 范数中的上界误差。然后，我们将这一结果与创新的本地化复杂性分析结合，以确立 PICNN 的快速收敛速率。我们的理论结果也被实验证明。在这些发现的基础上，我们探讨了解决高维度 PDEs 时的维度繁殖的缺陷。
</details></li>
</ul>
<hr>
<h2 id="Breaking-the-Complexity-Barrier-in-Compositional-Minimax-Optimization"><a href="#Breaking-the-Complexity-Barrier-in-Compositional-Minimax-Optimization" class="headerlink" title="Breaking the Complexity Barrier in Compositional Minimax Optimization"></a>Breaking the Complexity Barrier in Compositional Minimax Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09604">http://arxiv.org/abs/2308.09604</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jin Liu, Xiaokang Pan, Junwen Duan, Hongdong Li, Youqi Li, Zhe Qu</li>
<li>for: 这个论文旨在解决机器学习中的分布robustness和策略评估中的compositional minimax optimization问题。</li>
<li>methods: 该论文提出了嵌入STORM（NSTORM）和ADAptive NSTORM（ADA-NSTORM）两种方法，它们可以实现$O(\kappa^3&#x2F;\epsilon^3)$的样本复杂度，并且可以在不需要大批量的情况下进行分布robustness和策略评估。</li>
<li>results: 该论文通过了广泛的实验来证明其方法可以匹配下界，并且在实际应用中更加有效。<details>
<summary>Abstract</summary>
Compositional minimax optimization is a pivotal yet under-explored challenge across machine learning, including distributionally robust training and policy evaluation for reinforcement learning. Current techniques exhibit suboptimal complexity or rely heavily on large batch sizes. This paper proposes Nested STOchastic Recursive Momentum (NSTORM), attaining the optimal sample complexity of $O(\kappa^3/\epsilon^3)$ for finding an $\epsilon$-accurate solution. However, NSTORM requires low learning rates, potentially limiting applicability. Thus we introduce ADAptive NSTORM (ADA-NSTORM) with adaptive learning rates, proving it achieves the same sample complexity while experiments demonstrate greater effectiveness. Our methods match lower bounds for minimax optimization without large batch requirements, validated through extensive experiments. This work significantly advances compositional minimax optimization, a crucial capability for distributional robustness and policy evaluation
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate the following text into Simplified Chinese:Compositional minimax optimization is a pivotal yet under-explored challenge across machine learning, including distributionally robust training and policy evaluation for reinforcement learning. Current techniques exhibit suboptimal complexity or rely heavily on large batch sizes. This paper proposes Nested STOchastic Recursive Momentum (NSTORM), attaining the optimal sample complexity of $O(\kappa^3/\epsilon^3)$ for finding an $\epsilon$-accurate solution. However, NSTORM requires low learning rates, potentially limiting applicability. Thus we introduce ADAptive NSTORM (ADA-NSTORM) with adaptive learning rates, proving it achieves the same sample complexity while experiments demonstrate greater effectiveness. Our methods match lower bounds for minimax optimization without large batch requirements, validated through extensive experiments. This work significantly advances compositional minimax optimization, a crucial capability for distributional robustness and policy evaluation.Translate the text into Simplified Chinese, please.</SYS> Sure, here's the translation:<<SYS>> compositional minimax optimization 是机器学习中的一个重要 yet under-explored 挑战，包括分布 robust 训练和策略评估。当前技术具有不佳的复杂性或者依赖于大批量Size，这篇论文提出了嵌入 STOchastic Recursive Momentum (NSTORM)，实现了 $\epsilon$-accurate 解决方案的最佳样本复杂度 $O(\kappa^3/\epsilon^3)$。然而，NSTORM需要低学习率，可能限制其应用性。因此，我们介绍了 ADAptive NSTORM (ADA-NSTORM)  WITH adaptive learning rates，证明其实现了同样的样本复杂度，而实验表明它更有效。我们的方法与最佳下界匹配，无需大批量要求，经验验证了这一点。这项工作显著提高了 compositional minimax optimization 的能力，这是分布 robustness 和策略评估中的关键能力。Translate the text into Simplified Chinese, please.</SYS> Sure, here's the translation:<<SYS>>compositional minimax optimization 是机器学习中的一个重要 yet under-explored 挑战，包括分布 robust 训练和策略评估。当前技术具有不佳的复杂性或者依赖于大批量Size，这篇论文提出了嵌入 STOchastic Recursive Momentum (NSTORM)，实现了 $\epsilon$-accurate 解决方案的最佳样本复杂度 $O(\kappa^3/\epsilon^3)$。然而，NSTORM需要低学习率，可能限制其应用性。因此，我们介绍了 ADAptive NSTORM (ADA-NSTORM) WITH adaptive learning rates，证明其实现了同样的样本复杂度，而实验表明它更有效。我们的方法与最佳下界匹配，无需大批量要求，经验验证了这一点。这项工作显著提高了 compositional minimax optimization 的能力，这是分布 robustness 和策略评估中的关键能力。
</details></li>
</ul>
<hr>
<h2 id="Disparity-Inequality-and-Accuracy-Tradeoffs-in-Graph-Neural-Networks-for-Node-Classification"><a href="#Disparity-Inequality-and-Accuracy-Tradeoffs-in-Graph-Neural-Networks-for-Node-Classification" class="headerlink" title="Disparity, Inequality, and Accuracy Tradeoffs in Graph Neural Networks for Node Classification"></a>Disparity, Inequality, and Accuracy Tradeoffs in Graph Neural Networks for Node Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09596">http://arxiv.org/abs/2308.09596</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/arpitdm/gnn_accuracy_fairness_tradeoff">https://github.com/arpitdm/gnn_accuracy_fairness_tradeoff</a></li>
<li>paper_authors: Arpit Merchant, Carlos Castillo</li>
<li>for: 本研究旨在探讨Graph Neural Networks（GNNs）在人类应用中的偏见问题，特别是在预测节点标签的过程中是否存在偏见，以及如何 Mitigate 这些偏见的影响。</li>
<li>methods: 本研究提出了两种GNN-agnostic intervención，namely PFR-AX和PostProcess，以降低节点之间的分离度，并更新模型预测结果以最小化不同群体之间的差异。</li>
<li>results: 通过在四个数据集上进行了大量实验，研究发现，PFR-AX和PostProcess可以提供精细的控制和提高模型对保护组节点正确预测的自信度，但不同的 intervención在不同的数据集上的优化效果不同。<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) are increasingly used in critical human applications for predicting node labels in attributed graphs. Their ability to aggregate features from nodes' neighbors for accurate classification also has the capacity to exacerbate existing biases in data or to introduce new ones towards members from protected demographic groups. Thus, it is imperative to quantify how GNNs may be biased and to what extent their harmful effects may be mitigated. To this end, we propose two new GNN-agnostic interventions namely, (i) PFR-AX which decreases the separability between nodes in protected and non-protected groups, and (ii) PostProcess which updates model predictions based on a blackbox policy to minimize differences between error rates across demographic groups. Through a large set of experiments on four datasets, we frame the efficacies of our approaches (and three variants) in terms of their algorithmic fairness-accuracy tradeoff and benchmark our results against three strong baseline interventions on three state-of-the-art GNN models. Our results show that no single intervention offers a universally optimal tradeoff, but PFR-AX and PostProcess provide granular control and improve model confidence when correctly predicting positive outcomes for nodes in protected groups.
</details>
<details>
<summary>摘要</summary>
Graph neural networks (GNNs) 是在人类应用中越来越普遍地用于预测 attributed 图上的节点标签。它们可以从节点邻居中聚合特征，以便准确地分类，同时也可能扩大或引入数据中现有的偏见，特别是对来自保护性群体的成员。因此，有必要量化 GNN 是如何偏离的，以及如何降低它的负面影响。为此，我们提出了两种 GNN 不依赖的干预方法，namely，(i) PFR-AX，减少保护和非保护组之间节点的分离度，以及 (ii) PostProcess，根据黑盒政策更新模型预测结果，以最小化不同群体的错误率差异。通过大量实验，我们将我们的方法（及其三种变体）的公平性-准确性负担Plot 与三种基eline interventions 对三种现代 GNN 模型进行比较。我们的结果显示，无一个干预方法可以在所有情况下提供最佳的负担平衡，但PFR-AX 和 PostProcess 可以提供细化的控制，并在保护组中正确预测结果时提高模型的信任度。
</details></li>
</ul>
<hr>
<h2 id="WizardMath-Empowering-Mathematical-Reasoning-for-Large-Language-Models-via-Reinforced-Evol-Instruct"><a href="#WizardMath-Empowering-Mathematical-Reasoning-for-Large-Language-Models-via-Reinforced-Evol-Instruct" class="headerlink" title="WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct"></a>WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09583">http://arxiv.org/abs/2308.09583</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nlpxucan/wizardlm">https://github.com/nlpxucan/wizardlm</a></li>
<li>paper_authors: Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, Dongmei Zhang</li>
<li>for: 增强大型自然语言处理（NLP）任务中的数学逻辑能力</li>
<li>methods: 应用提出的演进学习从演示反馈（RLEIF）方法</li>
<li>results: 在两个数学逻辑benchmark上表现出色，超过了所有公开源模型，并且even surpasses ChatGPT-3.5、Claude Instant-1、PaLM-2和Minerva在GSM8k上，同时也超过Text-davinci-002、PaLM-1和GPT-3在MATH上。<details>
<summary>Abstract</summary>
Large language models (LLMs), such as GPT-4, have shown remarkable performance in natural language processing (NLP) tasks, including challenging mathematical reasoning. However, most existing open-source models are only pre-trained on large-scale internet data and without math-related optimization. In this paper, we present WizardMath, which enhances the mathematical reasoning abilities of Llama-2, by applying our proposed Reinforcement Learning from Evol-Instruct Feedback (RLEIF) method to the domain of math. Through extensive experiments on two mathematical reasoning benchmarks, namely GSM8k and MATH, we reveal the extraordinary capabilities of our model. WizardMath surpasses all other open-source LLMs by a substantial margin. Furthermore, our model even outperforms ChatGPT-3.5, Claude Instant-1, PaLM-2 and Minerva on GSM8k, simultaneously surpasses Text-davinci-002, PaLM-1 and GPT-3 on MATH. More details and model weights are public at https://github.com/nlpxucan/WizardLM and https://huggingface.co/WizardLM.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM），如GPT-4，在自然语言处理（NLP）任务中表现很出色，包括复杂的数学逻辑。然而，现有的大多数开源模型都只是在大规模的互联网数据上预训练而不是数学相关的优化。在本文中，我们介绍了WizardMath，它通过我们提议的强化学习反馈法（RLEIF）方法来增强Llama-2模型的数学逻辑能力。经过广泛的实验，我们发现WizardMath在两个数学逻辑 bencmarks，即GSM8k和MATH上表现出色，大大超过了其他开源LLM。此外，我们的模型还超过了ChatGPT-3.5、Claude Instant-1、PaLM-2和Minerva在GSM8k上，同时也超过了Text-davinci-002、PaLM-1和GPT-3在MATH上。详细信息和模型权重可以在 GitHub 上找到（https://github.com/nlpxucan/WizardLM）和 Hugging Face 上找到（https://huggingface.co/WizardLM）。
</details></li>
</ul>
<hr>
<h2 id="Physics-Informed-Boundary-Integral-Networks-PIBI-Nets-A-Data-Driven-Approach-for-Solving-Partial-Differential-Equations"><a href="#Physics-Informed-Boundary-Integral-Networks-PIBI-Nets-A-Data-Driven-Approach-for-Solving-Partial-Differential-Equations" class="headerlink" title="Physics-Informed Boundary Integral Networks (PIBI-Nets): A Data-Driven Approach for Solving Partial Differential Equations"></a>Physics-Informed Boundary Integral Networks (PIBI-Nets): A Data-Driven Approach for Solving Partial Differential Equations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09571">http://arxiv.org/abs/2308.09571</a></li>
<li>repo_url: None</li>
<li>paper_authors: Monika Nagy-Huber, Volker Roth</li>
<li>for: 解决实世界应用中的减震问题，即将 formal PDE 模型与 (可能含有噪声的) 观测结合起来，特别在缺乏边界或初始条件信息时。</li>
<li>methods: physics-informed neural networks (PINNs) 和 physics-informed boundary integral networks (PIBI-Nets)，后者在高维度设置中具有更高的精度和效率。</li>
<li>results: PIBI-Nets 在 Laplace 和 Poisson 方程中实现了非常高的准确率，并在一个真实世界应用中成功地重建了地下水流。<details>
<summary>Abstract</summary>
Partial differential equations (PDEs) can describe many relevant phenomena in dynamical systems. In real-world applications, we commonly need to combine formal PDE models with (potentially noisy) observations. This is especially relevant in settings where we lack information about boundary or initial conditions, or where we need to identify unknown model parameters. In recent years, Physics-informed neural networks (PINNs) have become a popular tool for problems of this kind. In high-dimensional settings, however, PINNs often suffer from computational problems because they usually require dense collocation points over the entire computational domain. To address this problem, we present Physics-Informed Boundary Integral Networks (PIBI-Nets) as a data-driven approach for solving PDEs in one dimension less than the original problem space. PIBI-Nets only need collocation points at the computational domain boundary, while still achieving highly accurate results, and in several practical settings, they clearly outperform PINNs. Exploiting elementary properties of fundamental solutions of linear differential operators, we present a principled and simple way to handle point sources in inverse problems. We demonstrate the excellent performance of PIBI-Nets for the Laplace and Poisson equations, both on artificial data sets and within a real-world application concerning the reconstruction of groundwater flows.
</details>
<details>
<summary>摘要</summary>
diferenciales parciales (PDEs) pueden describir muchos fenómenos relevantes en sistemas dinámicos. En aplicaciones reales, comúnmente combinamos modelos formales PDE con (potentialmente ruidosas) observaciones. Esto es especialmente relevante en situaciones donde carecemos de información sobre condiciones de borde o condiciones iniciales, o donde debemos identificar parámetros desconocidos del modelo. En los últimos años, las redes neuronales informadas por la física (PINNs) han become a herramienta popular para problemas de este tipo. Sin embargo, en configuraciones de alta dimensión, las PINNs suelen sufrir de problemas computacionales debido a que requieren puntos de colocación densos en todo el dominio de cálculo. Para abordar este problema, presentamos redes de integración de la frontera informadas por la física (PIBI-Nets) como una aproximación datos-driven para resolver PDEs en un espacio de dimensión inferior al espacio de problema original. Las PIBI-Nets solo necesitan puntos de colocación en la frontera del dominio de cálculo, mientras aún logran resultados altamente precisos, y en varios contextos prácticos, claramente superan a las PINNs. Explotando propiedades elementales de las soluciones fundamental de operadores lineales diferenciales, presentamos una manera principios y simple de manejar fuentes de puntos en problemas de inversión. Demostramos el excelente rendimiento de las PIBI-Nets para las ecuaciones de Laplace y de Poisson, tanto en conjuntos de datos artificiales como en una aplicación real concerniente a la reconstrucción de flujos de agua subterránea.
</details></li>
</ul>
<hr>
<h2 id="Investigating-the-Interplay-between-Features-and-Structures-in-Graph-Learning"><a href="#Investigating-the-Interplay-between-Features-and-Structures-in-Graph-Learning" class="headerlink" title="Investigating the Interplay between Features and Structures in Graph Learning"></a>Investigating the Interplay between Features and Structures in Graph Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09570">http://arxiv.org/abs/2308.09570</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniele Castellana, Federico Errica</li>
<li>for: This paper is written for researchers who are interested in understanding the inductive bias of Deep Graph Networks (DGNs) and how it relates to the dichotomy between homophily and heterophily.</li>
<li>methods: The paper uses two generative processes for node classification tasks, which allow the authors to build and study ad-hoc problems. The authors also use a metric called Feature Informativeness to quantitatively measure the influence of node features on target labels.</li>
<li>results: The paper finds that previously defined metrics are not adequate when the assumption of a strong correlation between node features and target labels is relaxed. The authors present novel research findings that could help advance our understanding of the field.<details>
<summary>Abstract</summary>
In the past, the dichotomy between homophily and heterophily has inspired research contributions toward a better understanding of Deep Graph Networks' inductive bias. In particular, it was believed that homophily strongly correlates with better node classification predictions of message-passing methods. More recently, however, researchers pointed out that such dichotomy is too simplistic as we can construct node classification tasks where graphs are completely heterophilic but the performances remain high. Most of these works have also proposed new quantitative metrics to understand when a graph structure is useful, which implicitly or explicitly assume the correlation between node features and target labels. Our work empirically investigates what happens when this strong assumption does not hold, by formalising two generative processes for node classification tasks that allow us to build and study ad-hoc problems. To quantitatively measure the influence of the node features on the target labels, we also use a metric we call Feature Informativeness. We construct six synthetic tasks and evaluate the performance of six models, including structure-agnostic ones. Our findings reveal that previously defined metrics are not adequate when we relax the above assumption. Our contribution to the workshop aims at presenting novel research findings that could help advance our understanding of the field.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Normalization-Is-All-You-Need-Understanding-Layer-Normalized-Federated-Learning-under-Extreme-Label-Shift"><a href="#Normalization-Is-All-You-Need-Understanding-Layer-Normalized-Federated-Learning-under-Extreme-Label-Shift" class="headerlink" title="Normalization Is All You Need: Understanding Layer-Normalized Federated Learning under Extreme Label Shift"></a>Normalization Is All You Need: Understanding Layer-Normalized Federated Learning under Extreme Label Shift</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09565">http://arxiv.org/abs/2308.09565</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guojun Zhang, Mahdi Beitollahi, Alex Bie, Xi Chen</li>
<li>for: 这paper的目的是解释层normalization在 Federated Learning 中的准确作用，以及它如何控制特征潜在的潜在问题。</li>
<li>methods: 这 paper 使用了多种方法来 investigated layer normalization，包括 feature normalization 和 label shift problem。</li>
<li>results: 这 paper 的结果表明，layer normalization 可以在 Federated Learning 中提高模型的性能，特别是在极端标签分布情况下。此外，paper 还进行了广泛的ablation study，以更好地理解层normalization的关键因素。<details>
<summary>Abstract</summary>
Layer normalization (LN) is a widely adopted deep learning technique especially in the era of foundation models. Recently, LN has been shown to be surprisingly effective in federated learning (FL) with non-i.i.d. data. However, exactly why and how it works remains mysterious. In this work, we reveal the profound connection between layer normalization and the label shift problem in federated learning. To understand layer normalization better in FL, we identify the key contributing mechanism of normalization methods in FL, called feature normalization (FN), which applies normalization to the latent feature representation before the classifier head. Although LN and FN do not improve expressive power, they control feature collapse and local overfitting to heavily skewed datasets, and thus accelerates global training. Empirically, we show that normalization leads to drastic improvements on standard benchmarks under extreme label shift. Moreover, we conduct extensive ablation studies to understand the critical factors of layer normalization in FL. Our results verify that FN is an essential ingredient inside LN to significantly improve the convergence of FL while remaining robust to learning rate choices, especially under extreme label shift where each client has access to few classes.
</details>
<details>
<summary>摘要</summary>
层normalization（LN）是深度学习中广泛采用的技术，特别在基础模型时代。最近，LN在非独立数据（Federated Learning，FL）中显示出意外的有效性。然而，它何时何为在FL中工作的原理仍然不清楚。在这项工作中，我们揭示了层normalization和FL中的标签shift问题之间的浓闻连接。为了更好地理解层normalization在FL中，我们确定了FL中normalization方法的关键贡献机制，称为特征normalization（FN），该机制在批处头之前对潜在特征表示进行了normalization。虽然LN和FN不提高表达力，但它们控制特征塌积和地方适应，从而加速全局训练。我们的实验结果表明，normalization在极端标签shift情况下导致了很大的改进，并且我们进行了广泛的减少研究，以了解层normalization在FL中的核心因素。我们的结果表明，FN是LN中关键的一部分，可以在FL中提高训练的速度和稳定性，特别是在极端标签shift情况下，每个客户端只有几个类型的训练数据。
</details></li>
</ul>
<hr>
<h2 id="Eigenvalue-based-Incremental-Spectral-Clustering"><a href="#Eigenvalue-based-Incremental-Spectral-Clustering" class="headerlink" title="Eigenvalue-based Incremental Spectral Clustering"></a>Eigenvalue-based Incremental Spectral Clustering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10999">http://arxiv.org/abs/2308.10999</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mieczysław A. Kłopotek, Bartłmiej Starosta, Sławomir T. Wierzchoń</li>
<li>for: 这个论文主要是为了提出一种增量 spectral clustering 方法，用于 clustering 大规模数据集。</li>
<li>methods: 该方法包括将数据分割成可管理的subsets，对每个subset进行 clustering，然后将不同subset的cluster合并到一起，基于eigenvalue spectrum的相似性。</li>
<li>results: 实验显示，这种方法可以快速地 clustering 大规模数据集，并且可以获得相对于整个数据集的cluster。<details>
<summary>Abstract</summary>
Our previous experiments demonstrated that subsets collections of (short) documents (with several hundred entries) share a common normalized in some way eigenvalue spectrum of combinatorial Laplacian. Based on this insight, we propose a method of incremental spectral clustering. The method consists of the following steps: (1) split the data into manageable subsets, (2) cluster each of the subsets, (3) merge clusters from different subsets based on the eigenvalue spectrum similarity to form clusters of the entire set. This method can be especially useful for clustering methods of complexity strongly increasing with the size of the data sample,like in case of typical spectral clustering. Experiments were performed showing that in fact the clustering and merging the subsets yields clusters close to clustering the entire dataset.
</details>
<details>
<summary>摘要</summary>
我们之前的实验表明，具有一些百度的文档集合（简称为“ subsets”）的常量 Laplacian 的几何特征是共同的正规化。基于这一点，我们提出了一种增量 spectral clustering 方法。该方法包括以下步骤：1. 将数据分成可控制的subsets;2. 对每个subset进行 clustering;3. 根据ensemble spectrum的相似性，将不同subset的cluster合并形成整个数据集的cluster。这种方法对于数据样本的复杂性呈指数增长的 clustering 方法特别有用。我们的实验表明，通过将subsets进行分 clustering 并将cluster合并，实际上可以获得整个数据集的高质量cluster。
</details></li>
</ul>
<hr>
<h2 id="Attesting-Distributional-Properties-of-Training-Data-for-Machine-Learning"><a href="#Attesting-Distributional-Properties-of-Training-Data-for-Machine-Learning" class="headerlink" title="Attesting Distributional Properties of Training Data for Machine Learning"></a>Attesting Distributional Properties of Training Data for Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09552">http://arxiv.org/abs/2308.09552</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vasisht Duddu, Anudeep Das, Nora Khayata, Hossein Yalame, Thomas Schneider, N. Asokan</li>
<li>for: 这篇论文是关于机器学习（ML）可靠性的问题，以及相关的法规框架的制定。</li>
<li>methods: 这篇论文提出了一种名为“属性证明”的方法，即证明模型训练数据的分布性质，但不披露实际数据。这种方法组合了属性推理和加密机制。</li>
<li>results: 这篇论文提出的属性证明方法可以有效地证明模型训练数据的分布性质，并且可以避免披露实际数据。<details>
<summary>Abstract</summary>
The success of machine learning (ML) has been accompanied by increased concerns about its trustworthiness. Several jurisdictions are preparing ML regulatory frameworks. One such concern is ensuring that model training data has desirable distributional properties for certain sensitive attributes. For example, draft regulations indicate that model trainers are required to show that training datasets have specific distributional properties, such as reflecting diversity of the population.   We propose the notion of property attestation allowing a prover (e.g., model trainer) to demonstrate relevant distributional properties of training data to a verifier (e.g., a customer) without revealing the data. We present an effective hybrid property attestation combining property inference with cryptographic mechanisms.
</details>
<details>
<summary>摘要</summary>
机器学习（ML）的成功也导致了其可靠性的问题受到了更多的关注。许多地区都在制定ML规章。一种关心的问题是确保模型训练数据具有特定敏感属性的愉悦分布性质。例如，审核法规提到了训练数据的特定分布性质，如反映人口多样性。我们提出了财产证明的概念，允许证明者（例如模型训练者）在不泄露数据的情况下，向验证者（例如客户）展示训练数据的相关分布性质。我们介绍了一种高效的混合财产证明方法，结合属性推断和 криптографиic机制。
</details></li>
</ul>
<hr>
<h2 id="Adapt-Your-Teacher-Improving-Knowledge-Distillation-for-Exemplar-free-Continual-Learning"><a href="#Adapt-Your-Teacher-Improving-Knowledge-Distillation-for-Exemplar-free-Continual-Learning" class="headerlink" title="Adapt Your Teacher: Improving Knowledge Distillation for Exemplar-free Continual Learning"></a>Adapt Your Teacher: Improving Knowledge Distillation for Exemplar-free Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09544">http://arxiv.org/abs/2308.09544</a></li>
<li>repo_url: None</li>
<li>paper_authors: Filip Szatkowski, Mateusz Pyla, Marcin Przewięźlikowski, Sebastian Cygert, Bartłomiej Twardowski, Tomasz Trzciński</li>
<li>for: 本研究 investigate exemplar-free class incremental learning (CIL) with knowledge distillation (KD) as a regularization strategy, aiming to prevent forgetting.</li>
<li>methods: 我们使用 KD-based methods, but we often struggle to regularize the model without access to exemplars of the training data from previous tasks. Our analysis reveals that this issue originates from substantial representation shifts in the teacher network when dealing with out-of-distribution data.</li>
<li>results: 我们引入 Teacher Adaptation (TA) method, which concurrently updates the teacher and the main model during incremental training. Our method seamlessly integrates with KD-based CIL approaches and allows for consistent enhancement of their performance across multiple exemplar-free CIL benchmarks.<details>
<summary>Abstract</summary>
In this work, we investigate exemplar-free class incremental learning (CIL) with knowledge distillation (KD) as a regularization strategy, aiming to prevent forgetting. KD-based methods are successfully used in CIL, but they often struggle to regularize the model without access to exemplars of the training data from previous tasks. Our analysis reveals that this issue originates from substantial representation shifts in the teacher network when dealing with out-of-distribution data. This causes large errors in the KD loss component, leading to performance degradation in CIL. Inspired by recent test-time adaptation methods, we introduce Teacher Adaptation (TA), a method that concurrently updates the teacher and the main model during incremental training. Our method seamlessly integrates with KD-based CIL approaches and allows for consistent enhancement of their performance across multiple exemplar-free CIL benchmarks.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们研究了无例示增量学习（CIL）中的知识储存（KD）作为规范策略，以避免忘记。KD基于方法在CIL中得到了成功，但它们经常无法在无前任任务的数据上正则化模型。我们的分析发现这个问题的起源在教师网络对异常数据的处理方面存在重大的表示转移问题，这导致KD损失部分的大错误，从而导致CIL性能下降。引用最近的测试时适应方法，我们提出了教师适应（TA）方法，该方法在增量训练中同时更新教师网络和主模型。我们的方法可以轻松地与KD基于CIL方法结合使用，并在多个无例示增量CILbenchmark上保持表现的一致提升。
</details></li>
</ul>
<hr>
<h2 id="Latent-State-Models-of-Training-Dynamics"><a href="#Latent-State-Models-of-Training-Dynamics" class="headerlink" title="Latent State Models of Training Dynamics"></a>Latent State Models of Training Dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09543">http://arxiv.org/abs/2308.09543</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Y. Hu, Angelica Chen, Naomi Saphra, Kyunghyun Cho</li>
<li>for: 这个论文旨在了解Randomness在模型训练中的影响，具体来说是如何不同的数据顺序和初始化实际会导致模型训练的不同结果，以及如何解释这些结果的训练动态和阶段转移。</li>
<li>methods: 作者使用多个Random seed来训练模型，并计算了训练过程中不同指标的变化，如模型权重的$L_2$范数、平均值和方差。然后，他们使用隐藏马尔可夫模型（HMM）来模型训练过程，将其视为一种随机过程，并从其中提取了训练动态的低维、离散表示。</li>
<li>results: 作者使用HMM来研究训练动态的阶段转移和阶段转移的概率分布，并发现了一些隐藏的”停滞”状态，这些状态会使训练过程慢下来。他们还使用HMM来研究模型训练的阶段转移，并发现了一些阶段转移的特征。<details>
<summary>Abstract</summary>
The impact of randomness on model training is poorly understood. How do differences in data order and initialization actually manifest in the model, such that some training runs outperform others or converge faster? Furthermore, how can we interpret the resulting training dynamics and the phase transitions that characterize different trajectories? To understand the effect of randomness on the dynamics and outcomes of neural network training, we train models multiple times with different random seeds and compute a variety of metrics throughout training, such as the $L_2$ norm, mean, and variance of the neural network's weights. We then fit a hidden Markov model (HMM) over the resulting sequences of metrics. The HMM represents training as a stochastic process of transitions between latent states, providing an intuitive overview of significant changes during training. Using our method, we produce a low-dimensional, discrete representation of training dynamics on grokking tasks, image classification, and masked language modeling. We use the HMM representation to study phase transitions and identify latent "detour" states that slow down convergence.
</details>
<details>
<summary>摘要</summary>
“模型训练中随机性的影响不甚了解。不同的数据顺序和初始化方法对模型的训练会如何产生不同的表现，有些训练运行比其他快速 converge 或者表现更好吗？此外，我们如何解释训练过程中的变动和结果，以及不同的训练路径中的相变点？为了理解随机性对模型训练的影响，我们将模型训练多次，每次使用不同的随机种子，并 Compute 一些 metric  durante 训练，例如模型的 $L_2$  норма、平均值和方差。然后，我们使用隐藏 Markov 模型 (HMM) 来描述训练的数据序列，从而获得训练过程的低维、组合表示。使用我们的方法，我们可以研究训练过程中的相变点和潜在的“停顿”状态，并且使用 HMM 表示来 изу究训练过程的相变点。”Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Decoupled-conditional-contrastive-learning-with-variable-metadata-for-prostate-lesion-detection"><a href="#Decoupled-conditional-contrastive-learning-with-variable-metadata-for-prostate-lesion-detection" class="headerlink" title="Decoupled conditional contrastive learning with variable metadata for prostate lesion detection"></a>Decoupled conditional contrastive learning with variable metadata for prostate lesion detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09542">http://arxiv.org/abs/2308.09542</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/camilleruppli/decoupled_ccl">https://github.com/camilleruppli/decoupled_ccl</a></li>
<li>paper_authors: Camille Ruppli, Pietro Gori, Roberto Ardon, Isabelle Bloch</li>
<li>for: 早期检测前列腺癌是治疗efficient的关键，mp-MRI广泛用于肿瘤检测。</li>
<li>methods: 我们提出了一种新的对比损失函数，利用多个注解员每个样本的 metadata，并利用多个注解员之间的变化来定义metadata信息的可信度。</li>
<li>results: 我们在公共PI-CAI挑战数据集上 Report a 3% AUC increase in lesion detection using our proposed contrastive loss function. 代码可以在 GitHub上找到：<a target="_blank" rel="noopener" href="https://github.com/camilleruppli/decoupled_ccl%E3%80%82">https://github.com/camilleruppli/decoupled_ccl。</a><details>
<summary>Abstract</summary>
Early diagnosis of prostate cancer is crucial for efficient treatment. Multi-parametric Magnetic Resonance Images (mp-MRI) are widely used for lesion detection. The Prostate Imaging Reporting and Data System (PI-RADS) has standardized interpretation of prostate MRI by defining a score for lesion malignancy. PI-RADS data is readily available from radiology reports but is subject to high inter-reports variability. We propose a new contrastive loss function that leverages weak metadata with multiple annotators per sample and takes advantage of inter-reports variability by defining metadata confidence. By combining metadata of varying confidence with unannotated data into a single conditional contrastive loss function, we report a 3% AUC increase on lesion detection on the public PI-CAI challenge dataset.   Code is available at: https://github.com/camilleruppli/decoupled_ccl
</details>
<details>
<summary>摘要</summary>
早期探测 проstate 癌是关键，以便有效的治疗。多parametric 磁共振成像 (mp-MRI) 广泛用于肿瘤检测。Prostate Imaging Reporting and Data System (PI-RADS) 已经标准化了 проstate MRI 的解释，并定义了肿瘤坏性的分数。PI-RADS 数据ready available 从 radiology 报告，但是受到高度的Inter-report variability。我们提议一种新的对比损失函数，利用weak metadata 和多个标注员每个样本，利用 inter-reports variability 定义metadata confidence。通过将metadata of varying confidence 与未标注数据结合成一个conditional contrastive loss函数，我们在公共PI-CAI challenge dataset上报告了3% AUC提高。代码可以在以下链接中找到：https://github.com/camilleruppli/decoupled_ccl。
</details></li>
</ul>
<hr>
<h2 id="Privacy-Preserving-3-Layer-Neural-Network-Training-using-Mere-Homomorphic-Encryption-Technique"><a href="#Privacy-Preserving-3-Layer-Neural-Network-Training-using-Mere-Homomorphic-Encryption-Technique" class="headerlink" title="Privacy-Preserving 3-Layer Neural Network Training using Mere Homomorphic Encryption Technique"></a>Privacy-Preserving 3-Layer Neural Network Training using Mere Homomorphic Encryption Technique</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09531">http://arxiv.org/abs/2308.09531</a></li>
<li>repo_url: None</li>
<li>paper_authors: John Chiang</li>
<li>for: 本 manuscript 考虑了隐私保护神经网络训练在简单的同adi运算 Setting 中的问题。</li>
<li>methods: 本 paper  combining 多种现有技术，进一步 extend 一些技术，最终实现了使用简单同adi运算技术训练 3-layer 神经网络，用于回归和分类问题。</li>
<li>results: 本 paper 实现了使用简单同adi运算技术训练神经网络，并且可以解决回归和分类问题。<details>
<summary>Abstract</summary>
In this manuscript, we consider the problem of privacy-preserving training of neural networks in the mere homomorphic encryption setting. We combine several exsiting techniques available, extend some of them, and finally enable the training of 3-layer neural networks for both the regression and classification problems using mere homomorphic encryption technique.
</details>
<details>
<summary>摘要</summary>
在这个手记中，我们考虑了使用简单的同源加密技术进行隐私保护神经网络训练。我们结合了现有的技术，进行了一些扩展，最终实现了使用简单同源加密技术训练3层神经网络，用于回归和分类问题。
</details></li>
</ul>
<hr>
<h2 id="Transitivity-Preserving-Graph-Representation-Learning-for-Bridging-Local-Connectivity-and-Role-based-Similarity"><a href="#Transitivity-Preserving-Graph-Representation-Learning-for-Bridging-Local-Connectivity-and-Role-based-Similarity" class="headerlink" title="Transitivity-Preserving Graph Representation Learning for Bridging Local Connectivity and Role-based Similarity"></a>Transitivity-Preserving Graph Representation Learning for Bridging Local Connectivity and Role-based Similarity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09517">http://arxiv.org/abs/2308.09517</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nslab-cuk/unified-graph-transformer">https://github.com/nslab-cuk/unified-graph-transformer</a></li>
<li>paper_authors: Van Thuy Hoang, O-Joun Lee</li>
<li>for: 这篇论文主要是为了提出一种能够有效地 integrate 本地和远程结构信息的图表示学方法（UGT），以便在不同的下游任务上进行图数据分析。</li>
<li>methods: 这篇论文提出了一种基于自我注意力的 UGT 模型，它首先通过识别本地子结构并汇集每个节点的 $k $- hop 邻居特征来学习本地结构。其次，它通过构建虚拟边来连接远程节点，以捕捉远程相互关系。最后，它通过自我注意力编码结构距离和 $p $- step 过渡概率来学习统一表示。</li>
<li>results: 实验结果表明，UGT 在真实世界 benchmark 数据上对多种下游任务进行了显著改进，并且达到了第三个 Weisfeiler-Lehman 同构测试（3d-WL）的表达力水平，能够分辨不同的图对。<details>
<summary>Abstract</summary>
Graph representation learning (GRL) methods, such as graph neural networks and graph transformer models, have been successfully used to analyze graph-structured data, mainly focusing on node classification and link prediction tasks. However, the existing studies mostly only consider local connectivity while ignoring long-range connectivity and the roles of nodes. In this paper, we propose Unified Graph Transformer Networks (UGT) that effectively integrate local and global structural information into fixed-length vector representations. First, UGT learns local structure by identifying the local substructures and aggregating features of the $k$-hop neighborhoods of each node. Second, we construct virtual edges, bridging distant nodes with structural similarity to capture the long-range dependencies. Third, UGT learns unified representations through self-attention, encoding structural distance and $p$-step transition probability between node pairs. Furthermore, we propose a self-supervised learning task that effectively learns transition probability to fuse local and global structural features, which could then be transferred to other downstream tasks. Experimental results on real-world benchmark datasets over various downstream tasks showed that UGT significantly outperformed baselines that consist of state-of-the-art models. In addition, UGT reaches the expressive power of the third-order Weisfeiler-Lehman isomorphism test (3d-WL) in distinguishing non-isomorphic graph pairs. The source code is available at https://github.com/NSLab-CUK/Unified-Graph-Transformer.
</details>
<details>
<summary>摘要</summary>
GRAPH 表示学习（GRL）方法，如图神经网络和图转换模型，已经成功地分析了图结构数据，主要集中在节点分类和链接预测任务上。然而，现有的研究通常只考虑当地连接性，忽略了远程连接性和节点的角色。在本文中，我们提出了统一图Transformer网络（UGT），可以有效地将本地和全局结构信息转化为固定长度 вектор表示。首先，UGT 通过识别节点的本地子结构并聚合 $k $- hop 邻居的特征来学习本地结构。其次，我们构建虚拟边，将远程节点相似的结构连接起来，以捕捉远程依赖关系。最后，UGT 通过自注意力学习，编码结构距离和 $p $- step 过渡概率 между节点对，学习统一表示。此外，我们提出了一种自动学习任务，可以有效地学习 transition probability，将本地和全局结构特征融合，可以转移到其他下游任务。实验结果表明，UGT 在真实世界 benchmark 数据集上对多种下游任务表现出色，并且达到了3d-WL isomorphism test 的表达力。代码可以在 https://github.com/NSLab-CUK/Unified-Graph-Transformer 中找到。
</details></li>
</ul>
<hr>
<h2 id="Spatial-LibriSpeech-An-Augmented-Dataset-for-Spatial-Audio-Learning"><a href="#Spatial-LibriSpeech-An-Augmented-Dataset-for-Spatial-Audio-Learning" class="headerlink" title="Spatial LibriSpeech: An Augmented Dataset for Spatial Audio Learning"></a>Spatial LibriSpeech: An Augmented Dataset for Spatial Audio Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09514">http://arxiv.org/abs/2308.09514</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/apple/ml-spatial-librispeech">https://github.com/apple/ml-spatial-librispeech</a></li>
<li>paper_authors: Miguel Sarabia, Elena Menyaylenko, Alessandro Toso, Skyler Seto, Zakaria Aldeneh, Shadi Pirhosseinloo, Luca Zappella, Barry-John Theobald, Nicholas Apostoloff, Jonathan Sheaffer</li>
<li>for: 这个论文是为了提供一个具有650小时19道声音的空间声音数据集，以及可选的干扰声音。这个数据集是为机器学习模型训练而设计。</li>
<li>methods: 这个论文使用了LibriSpeech样本的扩展，通过 simulated acoustic conditions 和8k+ synthetic rooms 生成了650个小时的声音数据。</li>
<li>results: 这个论文通过训练四个空间声音任务，实现了3D源localization的 median absolute error 为6.60°， distance 的 median absolute error 为0.43m， T30 的 median absolute error 为90.66ms，以及 DRR 的 median absolute error 为2.74dB。这些模型还可以在其他评估数据集上进行普适化。<details>
<summary>Abstract</summary>
We present Spatial LibriSpeech, a spatial audio dataset with over 650 hours of 19-channel audio, first-order ambisonics, and optional distractor noise. Spatial LibriSpeech is designed for machine learning model training, and it includes labels for source position, speaking direction, room acoustics and geometry. Spatial LibriSpeech is generated by augmenting LibriSpeech samples with 200k+ simulated acoustic conditions across 8k+ synthetic rooms. To demonstrate the utility of our dataset, we train models on four spatial audio tasks, resulting in a median absolute error of 6.60{\deg} on 3D source localization, 0.43m on distance, 90.66ms on T30, and 2.74dB on DRR estimation. We show that the same models generalize well to widely-used evaluation datasets, e.g., obtaining a median absolute error of 12.43{\deg} on 3D source localization on TUT Sound Events 2018, and 157.32ms on T30 estimation on ACE Challenge.
</details>
<details>
<summary>摘要</summary>
我们现在介绍Spatial LibriSpeech数据集，这是一个包含超过650小时19个通道音频的空间声音数据集，以及可选的干扰噪声。Spatial LibriSpeech是设计用于机器学习模型训练，其中包括源位置、说话方向、房间声学和几何学标签。Spatial LibriSpeech通过对LibriSpeech样本进行扩充，生成了200,000+个模拟的声音环境和8,000+个人工生成的房间。为了证明我们的数据集的实用性，我们在四个空间声音任务上训练了模型，导致了3D源localization的中值绝对误差为6.60度，距离为0.43米，T30为90.66毫秒，DRR估计为2.74dB。我们还证明了这些模型在广泛使用的评估数据集上也具有良好的泛化能力，例如在TUT Sound Events 2018中获得了12.43度的3D源localization中值绝对误差，并在ACE Challenge中获得了157.32毫秒的T30估计中值绝对误差。
</details></li>
</ul>
<hr>
<h2 id="Bridged-GNN-Knowledge-Bridge-Learning-for-Effective-Knowledge-Transfer"><a href="#Bridged-GNN-Knowledge-Bridge-Learning-for-Effective-Knowledge-Transfer" class="headerlink" title="Bridged-GNN: Knowledge Bridge Learning for Effective Knowledge Transfer"></a>Bridged-GNN: Knowledge Bridge Learning for Effective Knowledge Transfer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09499">http://arxiv.org/abs/2308.09499</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wendong Bi, Xueqi Cheng, Bingbing Xu, Xiaoqian Sun, Li Xu, Huawei Shen</li>
<li>for: 解决数据缺乏和低质量问题，帮助深度学习模型在TargetDomain上进行更好的泛化能力。</li>
<li>methods: 基于Graph Neural Networks (GNNs)的Knowledge Bridge Learning (KBL)方法，通过连接知识域的样本与TargetDomain中的样本建立知识桥，并通过GNN进行样本级别的知识传输。</li>
<li>results: 在不同的数据缺乏和数据质量问题下，Bridged-GNN对比SOTA方法具有显著的改善，能够更好地泛化到TargetDomain中。<details>
<summary>Abstract</summary>
The data-hungry problem, characterized by insufficiency and low-quality of data, poses obstacles for deep learning models. Transfer learning has been a feasible way to transfer knowledge from high-quality external data of source domains to limited data of target domains, which follows a domain-level knowledge transfer to learn a shared posterior distribution. However, they are usually built on strong assumptions, e.g., the domain invariant posterior distribution, which is usually unsatisfied and may introduce noises, resulting in poor generalization ability on target domains. Inspired by Graph Neural Networks (GNNs) that aggregate information from neighboring nodes, we redefine the paradigm as learning a knowledge-enhanced posterior distribution for target domains, namely Knowledge Bridge Learning (KBL). KBL first learns the scope of knowledge transfer by constructing a Bridged-Graph that connects knowledgeable samples to each target sample and then performs sample-wise knowledge transfer via GNNs.KBL is free from strong assumptions and is robust to noises in the source data. Guided by KBL, we propose the Bridged-GNN} including an Adaptive Knowledge Retrieval module to build Bridged-Graph and a Graph Knowledge Transfer module. Comprehensive experiments on both un-relational and relational data-hungry scenarios demonstrate the significant improvements of Bridged-GNN compared with SOTA methods
</details>
<details>
<summary>摘要</summary>
“问题Characterized by 缺乏和低质量数据，深度学习模型遇到了困难。将知识从高质量外部数据传递到有限数据的目标领域，通过域层知识传递，以学习共享 posterior distribution。然而，这些方法通常是基于强大的假设，例如域层知识 posterior distribution，这通常不充分满足，可能导致误差，影响了目标领域的数据整合能力。获取灵感自 Graph Neural Networks（GNNs），我们重新定义了这个概念为知识桥学习（KBL）。KBL首先学习知识传递的范围，然后通过 GNNs 进行样本别知识传递。KBL 不受强大的假设的限制，并具有较好的韧性。根据 KBL，我们提出了 Bridged-GNN，包括一个 Adaptive Knowledge Retrieval 模组和一个 Graph Knowledge Transfer 模组。实验结果显示，Bridged-GNN 与 State-of-the-Art 方法相比，在无关数据和关联数据的情况下具有杰出的改善。”
</details></li>
</ul>
<hr>
<h2 id="Predictive-Authoring-for-Brazilian-Portuguese-Augmentative-and-Alternative-Communication"><a href="#Predictive-Authoring-for-Brazilian-Portuguese-Augmentative-and-Alternative-Communication" class="headerlink" title="Predictive Authoring for Brazilian Portuguese Augmentative and Alternative Communication"></a>Predictive Authoring for Brazilian Portuguese Augmentative and Alternative Communication</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09497">http://arxiv.org/abs/2308.09497</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jayralencar/pictogram_prediction_pt">https://github.com/jayralencar/pictogram_prediction_pt</a></li>
<li>paper_authors: Jayr Pereira, Rodrigo Nogueira, Cleber Zanchettin, Robson Fidalgo</li>
<li>for: 这个论文旨在提出使用 BERTimbau 模型进行图像预测，以提高 AAC 系统的表达效果。</li>
<li>methods: 作者使用了 BERTimbau 模型，并构建了一个特定于巴西葡萄牙语的 AAC 训练集，以finetune BERTimbau。另外，作者还比较了不同的图像预测方法，包括使用图像定义、相关词语集和图像本身。</li>
<li>results: 结果显示，使用图像定义、相关词语集和图像本身的嵌入都有相似的表现，使用相关词语集得到最高的准确率，但使用图像定义得到最低的混淆率。这篇论文为使用 BERT-like 模型进行图像预测提供了新的想法，以及使用图像进行图像预测的潜在可能性。<details>
<summary>Abstract</summary>
Individuals with complex communication needs (CCN) often rely on augmentative and alternative communication (AAC) systems to have conversations and communique their wants. Such systems allow message authoring by arranging pictograms in sequence. However, the difficulty of finding the desired item to complete a sentence can increase as the user's vocabulary increases. This paper proposes using BERTimbau, a Brazilian Portuguese version of BERT, for pictogram prediction in AAC systems. To finetune BERTimbau, we constructed an AAC corpus for Brazilian Portuguese to use as a training corpus. We tested different approaches to representing a pictogram for prediction: as a word (using pictogram captions), as a concept (using a dictionary definition), and as a set of synonyms (using related terms). We also evaluated the usage of images for pictogram prediction. The results demonstrate that using embeddings computed from the pictograms' caption, synonyms, or definitions have a similar performance. Using synonyms leads to lower perplexity, but using captions leads to the highest accuracies. This paper provides insight into how to represent a pictogram for prediction using a BERT-like model and the potential of using images for pictogram prediction.
</details>
<details>
<summary>摘要</summary>
人们 WITH complex communication needs (CCN) 常常使用增强性和替代通信系统 (AAC) 来与他人交流和表达自己的需求。这些系统允许用户通过排序图ogram来编写消息。然而，如果用户的词汇量增加，则找到所需的图ogram可能会变得更加困难。这篇论文提议使用BERTimbau，一个基于 Brazilian Portuguese 的 BERT 模型，来预测图ogram。为了训练 BERTimbau，我们构建了一个 Brazilian Portuguese 的 AAC 训练集。我们测试了不同的图ogram表示方法来预测图ogram：作为单词 (使用图ogram的caption)、作为概念 (使用词典定义)、作为相关词 (使用相关 терminus)。我们还评估了使用图像来预测图ogram。结果表明，使用图ogram的caption、synonyms或定义来计算嵌入的性能类似。使用 synonyms 导致较低的混淆率，而使用 caption 导致最高的准确率。这篇论文为使用 BERT-like 模型来表示图ogram以及使用图像来预测图ogram提供了洞察和潜在的应用。
</details></li>
</ul>
<hr>
<h2 id="Balancing-Transparency-and-Risk-The-Security-and-Privacy-Risks-of-Open-Source-Machine-Learning-Models"><a href="#Balancing-Transparency-and-Risk-The-Security-and-Privacy-Risks-of-Open-Source-Machine-Learning-Models" class="headerlink" title="Balancing Transparency and Risk: The Security and Privacy Risks of Open-Source Machine Learning Models"></a>Balancing Transparency and Risk: The Security and Privacy Risks of Open-Source Machine Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09490">http://arxiv.org/abs/2308.09490</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dominik Hintersdorf, Lukas Struppek, Kristian Kersting</li>
<li>for: 本文旨在提醒读者关于使用开源机器学习模型时的隐私和安全风险。</li>
<li>methods: 本文使用实例来说明开源模型可能带来的隐私和安全风险，包括模型隐藏函数和特定输入模式触发的行为干扰。</li>
<li>results: 本文通过提高读者对开源模型使用的隐私和安全风险的认识，旨在促进负责任的AI系统使用。<details>
<summary>Abstract</summary>
The field of artificial intelligence (AI) has experienced remarkable progress in recent years, driven by the widespread adoption of open-source machine learning models in both research and industry. Considering the resource-intensive nature of training on vast datasets, many applications opt for models that have already been trained. Hence, a small number of key players undertake the responsibility of training and publicly releasing large pre-trained models, providing a crucial foundation for a wide range of applications. However, the adoption of these open-source models carries inherent privacy and security risks that are often overlooked. To provide a concrete example, an inconspicuous model may conceal hidden functionalities that, when triggered by specific input patterns, can manipulate the behavior of the system, such as instructing self-driving cars to ignore the presence of other vehicles. The implications of successful privacy and security attacks encompass a broad spectrum, ranging from relatively minor damage like service interruptions to highly alarming scenarios, including physical harm or the exposure of sensitive user data. In this work, we present a comprehensive overview of common privacy and security threats associated with the use of open-source models. By raising awareness of these dangers, we strive to promote the responsible and secure use of AI systems.
</details>
<details>
<summary>摘要</summary>
人工智能（AI）领域在最近几年内取得了很大进步，这主要归功于研究和实践中广泛采用的开源机器学习模型。由于训练大量数据需要巨量的资源，许多应用程序选择使用已经训练过的模型。因此，只有一小部分的关键玩家承担了训练和公共释放大型预训练模型的责任，这些模型提供了许多应用程序的基础。然而，使用这些开源模型的采用带来了内置的隐私和安全风险，这些风险通常被忽略。为了给出具体的例子，一个不引人注意的模型可能封装了隐藏的功能，当特定的输入模式触发时，可以 manipulate 系统的行为，如 instructing self-driving cars to ignore the presence of other vehicles。成功的隐私和安全攻击的后果覆盖广泛，从较轻的服务中断到极其警示的情况，包括物理损害或暴露敏感用户数据。在这项工作中，我们提供了对开源模型常见隐私和安全威胁的完整概述。我们希望通过提醒这些危险，推动AI系统的负责任和安全使用。
</details></li>
</ul>
<hr>
<h2 id="RBA-GCN-Relational-Bilevel-Aggregation-Graph-Convolutional-Network-for-Emotion-Recognition"><a href="#RBA-GCN-Relational-Bilevel-Aggregation-Graph-Convolutional-Network-for-Emotion-Recognition" class="headerlink" title="RBA-GCN: Relational Bilevel Aggregation Graph Convolutional Network for Emotion Recognition"></a>RBA-GCN: Relational Bilevel Aggregation Graph Convolutional Network for Emotion Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11029">http://arxiv.org/abs/2308.11029</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/luftmenscher/RBA-GCN">https://github.com/luftmenscher/RBA-GCN</a></li>
<li>paper_authors: Lin Yuan, Guoheng Huang, Fenghuan Li, Xiaochen Yuan, Chi-Man Pun, Guo Zhong<br>for:The paper is written for recognizing emotion in conversations (ERC) and addressing the limitations of traditional graph convolutional networks (GCNs) in capturing long-range contextual information and node discriminant information.methods:The paper proposes a novel approach called the relational bilevel aggregation graph convolutional network (RBA-GCN), which consists of three modules: the graph generation module (GGM), similarity-based cluster building module (SCBM), and bilevel aggregation module (BiAM).results:The RBA-GCN achieves a 2.17% to 5.21% improvement in weighted average F1 score over the most advanced method on the IEMOCAP and MELD datasets.<details>
<summary>Abstract</summary>
Emotion recognition in conversation (ERC) has received increasing attention from researchers due to its wide range of applications. As conversation has a natural graph structure, numerous approaches used to model ERC based on graph convolutional networks (GCNs) have yielded significant results. However, the aggregation approach of traditional GCNs suffers from the node information redundancy problem, leading to node discriminant information loss. Additionally, single-layer GCNs lack the capacity to capture long-range contextual information from the graph. Furthermore, the majority of approaches are based on textual modality or stitching together different modalities, resulting in a weak ability to capture interactions between modalities. To address these problems, we present the relational bilevel aggregation graph convolutional network (RBA-GCN), which consists of three modules: the graph generation module (GGM), similarity-based cluster building module (SCBM) and bilevel aggregation module (BiAM). First, GGM constructs a novel graph to reduce the redundancy of target node information. Then, SCBM calculates the node similarity in the target node and its structural neighborhood, where noisy information with low similarity is filtered out to preserve the discriminant information of the node. Meanwhile, BiAM is a novel aggregation method that can preserve the information of nodes during the aggregation process. This module can construct the interaction between different modalities and capture long-range contextual information based on similarity clusters. On both the IEMOCAP and MELD datasets, the weighted average F1 score of RBA-GCN has a 2.17$\sim$5.21\% improvement over that of the most advanced method.
</details>
<details>
<summary>摘要</summary>
研究者们对话情感识别（ERC）已经受到了越来越多的关注，因为它在各种应用场景中具有广泛的应用前景。由于对话自然具有图structure，许多方法基于图 convolutional networks（GCNs）来模型ERC已经取得了 significativ results。然而，传统GCNs的聚合方法受到节点信息纠纷问题的影响，导致节点特征信息的损失，同时单层GCNs缺乏捕捉长距离上下文信息的能力。此外，大多数方法都是基于文本模式或将不同模式粘合在一起，导致模式之间的交互 capture的能力弱化。为解决这些问题，我们提出了关系着色层聚合图 convolutional network（RBA-GCN），它包括三个模块：图生成模块（GGM），相似度基于团建模块（SCBM）和着色层聚合模块（BiAM）。首先，GGM构建了一个新的图，以减少目标节点信息的纠纷。然后，SCBM计算了目标节点和其结构邻域中的节点相似度，并将低相似度的信息排除，以保留节点特征信息。同时，BiAM是一种新的聚合方法，可以在聚合过程中保留节点信息。这个模块可以建立不同模式之间的交互，并基于相似团来捕捉长距离上下文信息。在IEMOCAP和MELD数据集上，RBA-GCN的权重平均F1分数与最先进方法相比提高了2.17%∼5.21%。
</details></li>
</ul>
<hr>
<h2 id="Data-augmentation-and-explainability-for-bias-discovery-and-mitigation-in-deep-learning"><a href="#Data-augmentation-and-explainability-for-bias-discovery-and-mitigation-in-deep-learning" class="headerlink" title="Data augmentation and explainability for bias discovery and mitigation in deep learning"></a>Data augmentation and explainability for bias discovery and mitigation in deep learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09464">http://arxiv.org/abs/2308.09464</a></li>
<li>repo_url: None</li>
<li>paper_authors: Agnieszka Mikołajczyk-Bareła</li>
<li>for: 这个论文探讨深度神经网络中的偏见问题，并提出了减少偏见影响模型性能的方法。</li>
<li>methods: 论文首先对数据和模型中的偏见和错误源进行分类和描述，特别是深度学习管道中的偏见。然后，提出了解释AI的概率分布来证明预测结果，并控制和改进模型。为了找到数据中的偏见，论文还提出了全球偏见标识方法。最后，论文提出了三种方法来减少偏见的影响：样式传递数据扩展、Targeted数据扩展和回归反馈。</li>
<li>results: 论文通过实验表明，使用样式传递数据扩展、Targeted数据扩展和回归反馈方法可以减少偏见对机器学习模型的影响。<details>
<summary>Abstract</summary>
This dissertation explores the impact of bias in deep neural networks and presents methods for reducing its influence on model performance. The first part begins by categorizing and describing potential sources of bias and errors in data and models, with a particular focus on bias in machine learning pipelines. The next chapter outlines a taxonomy and methods of Explainable AI as a way to justify predictions and control and improve the model. Then, as an example of a laborious manual data inspection and bias discovery process, a skin lesion dataset is manually examined. A Global Explanation for the Bias Identification method is proposed as an alternative semi-automatic approach to manual data exploration for discovering potential biases in data. Relevant numerical methods and metrics are discussed for assessing the effects of the identified biases on the model. Whereas identifying errors and bias is critical, improving the model and reducing the number of flaws in the future is an absolute priority. Hence, the second part of the thesis focuses on mitigating the influence of bias on ML models. Three approaches are proposed and discussed: Style Transfer Data Augmentation, Targeted Data Augmentations, and Attribution Feedback. Style Transfer Data Augmentation aims to address shape and texture bias by merging a style of a malignant lesion with a conflicting shape of a benign one. Targeted Data Augmentations randomly insert possible biases into all images in the dataset during the training, as a way to make the process random and, thus, destroy spurious correlations. Lastly, Attribution Feedback is used to fine-tune the model to improve its accuracy by eliminating obvious mistakes and teaching it to ignore insignificant input parts via an attribution loss. The goal of these approaches is to reduce the influence of bias on machine learning models, rather than eliminate it entirely.
</details>
<details>
<summary>摘要</summary>
As an example of a laborious manual data inspection and bias discovery process, a skin lesion dataset is manually examined. A Global Explanation for the Bias Identification method is proposed as a semi-automatic approach to discover potential biases in data. Relevant numerical methods and metrics are discussed for assessing the effects of identified biases on the model.The second part of the thesis focuses on mitigating the influence of bias on ML models. Three approaches are proposed and discussed:1. Style Transfer Data Augmentation: This approach aims to address shape and texture bias by merging a style of a malignant lesion with a conflicting shape of a benign one.2. Targeted Data Augmentations: This approach randomly inserts possible biases into all images in the dataset during training to destroy spurious correlations.3. Attribution Feedback: This approach fine-tunes the model to improve its accuracy by eliminating obvious mistakes and teaching it to ignore insignificant input parts via an attribution loss.The goal of these approaches is to reduce the influence of bias on machine learning models, rather than eliminate it entirely.
</details></li>
</ul>
<hr>
<h2 id="Reconstructing-S-matrix-Phases-with-Machine-Learning"><a href="#Reconstructing-S-matrix-Phases-with-Machine-Learning" class="headerlink" title="Reconstructing $S$-matrix Phases with Machine Learning"></a>Reconstructing $S$-matrix Phases with Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09451">http://arxiv.org/abs/2308.09451</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aurélien Dersy, Matthew D. Schwartz, Alexander Zhiboedov</li>
<li>for: 这个论文是关于 $S$-矩阵 Bootstrap 计划中关于模ulus 和阶的关系的研究。</li>
<li>methods: 作者使用现代机器学习技术来研究单位约束。他们发现，对于给定的模ulus，如果存在阶，则可以通过机器学习方法准确重建阶。此外，损失函数的优化问题可以作为单位约束是否成立的指标。</li>
<li>results: 作者发现，在允许多阶的情况下，模ulus 和阶之间存在一定的关系，并且可以使用机器学习方法来重建阶。此外，他们发现了一个新的阶 ambiguity 解，这将单位约束的已知上限提高了许多。<details>
<summary>Abstract</summary>
An important element of the $S$-matrix bootstrap program is the relationship between the modulus of an $S$-matrix element and its phase. Unitarity relates them by an integral equation. Even in the simplest case of elastic scattering, this integral equation cannot be solved analytically and numerical approaches are required. We apply modern machine learning techniques to studying the unitarity constraint. We find that for a given modulus, when a phase exists it can generally be reconstructed to good accuracy with machine learning. Moreover, the loss of the reconstruction algorithm provides a good proxy for whether a given modulus can be consistent with unitarity at all. In addition, we study the question of whether multiple phases can be consistent with a single modulus, finding novel phase-ambiguous solutions. In particular, we find a new phase-ambiguous solution which pushes the known limit on such solutions significantly beyond the previous bound.
</details>
<details>
<summary>摘要</summary>
<font face="宋体">$S$-Matrix bootstrap 程序中一个重要元素是$S$-Matrix元素的模ulus和其相位之间的关系。封闭性关系它们通过一个积分方程。même在最简单的射击反射过程中，这个积分方程无法 analytically解决，需要使用数值方法。我们使用现代机器学习技术来研究封闭性约束。我们发现，对于给定的模ulus，当一个相位存在时，通常可以使用机器学习来重建它到高度准确。此外，损失函数的损失函数可以作为一个给定模ulus是否能够符合封闭性的指标。此外，我们研究了一个给定模ulus是否能够有多个相位的问题，发现了新的相位不确定解。特别是，我们发现了一个新的相位不确定解，将之前的限制 pushed significantly beyond the previous bound.</font>Note: The translation is in Simplified Chinese, which is the standardized form of Chinese used in mainland China and Singapore. Traditional Chinese is used in Hong Kong, Taiwan, and other regions.
</details></li>
</ul>
<hr>
<h2 id="Logistics-Hub-Location-Optimization-A-K-Means-and-P-Median-Model-Hybrid-Approach-Using-Road-Network-Distances"><a href="#Logistics-Hub-Location-Optimization-A-K-Means-and-P-Median-Model-Hybrid-Approach-Using-Road-Network-Distances" class="headerlink" title="Logistics Hub Location Optimization: A K-Means and P-Median Model Hybrid Approach Using Road Network Distances"></a>Logistics Hub Location Optimization: A K-Means and P-Median Model Hybrid Approach Using Road Network Distances</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11038">http://arxiv.org/abs/2308.11038</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Abdul Rahman, Muhammad Aamir Basheer, Zubair Khalid, Muhammad Tahir, Momin Uppal</li>
<li>for: 优化各类物流总站的位置，以提高电商业务效益和减少碳脚印。</li>
<li>methods:  hybrid方法，首先使用K-Means聚合elivery点的空间位置，然后使用P-Median方法选择最佳物流总站。</li>
<li>results: 通过使用优化的物流总站位置，可以节省每个配送的距离815米（10%）。<details>
<summary>Abstract</summary>
Logistic hubs play a pivotal role in the last-mile delivery distance; even a slight increment in distance negatively impacts the business of the e-commerce industry while also increasing its carbon footprint. The growth of this industry, particularly after Covid-19, has further intensified the need for optimized allocation of resources in an urban environment. In this study, we use a hybrid approach to optimize the placement of logistic hubs. The approach sequentially employs different techniques. Initially, delivery points are clustered using K-Means in relation to their spatial locations. The clustering method utilizes road network distances as opposed to Euclidean distances. Non-road network-based approaches have been avoided since they lead to erroneous and misleading results. Finally, hubs are located using the P-Median method. The P-Median method also incorporates the number of deliveries and population as weights. Real-world delivery data from Muller and Phipps (M&P) is used to demonstrate the effectiveness of the approach. Serving deliveries from the optimal hub locations results in the saving of 815 (10%) meters per delivery.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Defending-Label-Inference-Attacks-in-Split-Learning-under-Regression-Setting"><a href="#Defending-Label-Inference-Attacks-in-Split-Learning-under-Regression-Setting" class="headerlink" title="Defending Label Inference Attacks in Split Learning under Regression Setting"></a>Defending Label Inference Attacks in Split Learning under Regression Setting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09448">http://arxiv.org/abs/2308.09448</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoze Qiu, Fei Zheng, Chaochao Chen, Xiaolin Zheng</li>
<li>for: 防止批处理学习中的标签推理攻击</li>
<li>methods: 提议Random Label Extension (RLE)和Model-based adaptive Label Extension (MLE)两种防御方法</li>
<li>results: 比基础防御方法更有效地降低攻击模型的性能，同时保持原始任务的性能<details>
<summary>Abstract</summary>
As a privacy-preserving method for implementing Vertical Federated Learning, Split Learning has been extensively researched. However, numerous studies have indicated that the privacy-preserving capability of Split Learning is insufficient. In this paper, we primarily focus on label inference attacks in Split Learning under regression setting, which are mainly implemented through the gradient inversion method. To defend against label inference attacks, we propose Random Label Extension (RLE), where labels are extended to obfuscate the label information contained in the gradients, thereby preventing the attacker from utilizing gradients to train an attack model that can infer the original labels. To further minimize the impact on the original task, we propose Model-based adaptive Label Extension (MLE), where original labels are preserved in the extended labels and dominate the training process. The experimental results show that compared to the basic defense methods, our proposed defense methods can significantly reduce the attack model's performance while preserving the original task's performance.
</details>
<details>
<summary>摘要</summary>
为保持隐私，垂直联合学习中的Split Learning方法已经受到了广泛研究。然而，许多研究表明，Split Learning的隐私保护能力不足。在这篇论文中，我们主要关注在 regression 设定下的标签推论攻击，这些攻击通常通过Gradient Inversion方法进行实现。为了防止标签推论攻击，我们提出了Random Label Extension（RLE），将标签扩展以隐藏标签信息含在梯度中，防止攻击者使用梯度训练一个可以推论原始标签的攻击模型。另外，我们提出了Model-based adaptive Label Extension（MLE），将原始标签保留在扩展标签中，使得训练过程中的标签保持原始任务的影响最小。实验结果显示，相比基本防御方法，我们提出的防御方法可以对攻击模型的性能产生重大降低，同时保持原始任务的性能。
</details></li>
</ul>
<hr>
<h2 id="An-Efficient-1-Iteration-Learning-Algorithm-for-Gaussian-Mixture-Model-And-Gaussian-Mixture-Embedding-For-Neural-Network"><a href="#An-Efficient-1-Iteration-Learning-Algorithm-for-Gaussian-Mixture-Model-And-Gaussian-Mixture-Embedding-For-Neural-Network" class="headerlink" title="An Efficient 1 Iteration Learning Algorithm for Gaussian Mixture Model And Gaussian Mixture Embedding For Neural Network"></a>An Efficient 1 Iteration Learning Algorithm for Gaussian Mixture Model And Gaussian Mixture Embedding For Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09444">http://arxiv.org/abs/2308.09444</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weiguo Lu, Xuan Wu, Deng Ding, Gangnan Yuan</li>
<li>for: 提出了一种基于Gaussian Mixture Model（GMM）学习算法，以便解决经典Expectation Maximization（EM）算法的缺点，包括缺乏稳定性和复杂性。</li>
<li>methods: 该算法基于我们之前的GMM扩展思想，具有更高的稳定性和简洁性，同时仅需一轮学习。我们也证明了该算法在任意参数初始化情况下都能够 converges。</li>
<li>results: 我们对经典概率层与GMM扩展方法进行比较，发现GMM扩展方法能够更好地应对数据不确定性和逆问题。此外，我们还测试了基于GMM的生成器，表明它具有可能应用于随机抽样和变量控制等领域的潜在能力。<details>
<summary>Abstract</summary>
We propose an Gaussian Mixture Model (GMM) learning algorithm, based on our previous work of GMM expansion idea. The new algorithm brings more robustness and simplicity than classic Expectation Maximization (EM) algorithm. It also improves the accuracy and only take 1 iteration for learning. We theoretically proof that this new algorithm is guarantee to converge regardless the parameters initialisation. We compare our GMM expansion method with classic probability layers in neural network leads to demonstrably better capability to overcome data uncertainty and inverse problem. Finally, we test GMM based generator which shows a potential to build further application that able to utilized distribution random sampling for stochastic variation as well as variation control.
</details>
<details>
<summary>摘要</summary>
我们提出了一种 Gaussian Mixture Model（GMM）学习算法，基于我们之前的 GMM 扩展思想。新算法比 классический Expectation Maximization（EM）算法更加稳定和简单，同时也提高了准确性，只需一次迭代学习。我们 teorically 证明了这种新算法是无论初始化参数都会收敛。我们对 класси probability layers 和 GMM expansion method 进行比较，发现后者在面临数据不确定和逆问题时表现更好，可以更好地适应不确定性和随机变化。最后，我们测试了 GMM 基于的生成器，发现它具有可以利用分布随机抽样和变量控制的潜在应用可能性。Note: Please keep in mind that the translation is a rough approximation and may not be perfect, as the nuances of the original text may be lost in translation.
</details></li>
</ul>
<hr>
<h2 id="From-Hope-to-Safety-Unlearning-Biases-of-Deep-Models-by-Enforcing-the-Right-Reasons-in-Latent-Space"><a href="#From-Hope-to-Safety-Unlearning-Biases-of-Deep-Models-by-Enforcing-the-Right-Reasons-in-Latent-Space" class="headerlink" title="From Hope to Safety: Unlearning Biases of Deep Models by Enforcing the Right Reasons in Latent Space"></a>From Hope to Safety: Unlearning Biases of Deep Models by Enforcing the Right Reasons in Latent Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09437">http://arxiv.org/abs/2308.09437</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maximilian Dreyer, Frederik Pahde, Christopher J. Anders, Wojciech Samek, Sebastian Lapuschkin<br>for:这篇论文旨在测试深度神经网络中隐藏的偏见，以及对于高风险决策的预测。methods:这篇论文使用了一种新的方法，通过减少模型对偏见的敏感性，以确保预测的正确性。这种方法基于概念活化向量，并且考虑了偏见的可Robustness。results:这篇论文在控制的和实际的设定下，成功地实现了对ISIC、Bone Age、ImageNet和CelebA dataset上的预测偏见的控制。使用了VGG、ResNet和EfficientNet的架构。<details>
<summary>Abstract</summary>
Deep Neural Networks are prone to learning spurious correlations embedded in the training data, leading to potentially biased predictions. This poses risks when deploying these models for high-stake decision-making, such as in medical applications. Current methods for post-hoc model correction either require input-level annotations, which are only possible for spatially localized biases, or augment the latent feature space, thereby hoping to enforce the right reasons. We present a novel method ensuring the right reasons on the concept level by reducing the model's sensitivity towards biases through the gradient. When modeling biases via Concept Activation Vectors, we highlight the importance of choosing robust directions, as traditional regression-based approaches such as Support Vector Machines tend to result in diverging directions. We effectively mitigate biases in controlled and real-world settings on the ISIC, Bone Age, ImageNet and CelebA datasets using VGG, ResNet and EfficientNet architectures.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Can-ultrasound-confidence-maps-predict-sonographers’-labeling-variability"><a href="#Can-ultrasound-confidence-maps-predict-sonographers’-labeling-variability" class="headerlink" title="Can ultrasound confidence maps predict sonographers’ labeling variability?"></a>Can ultrasound confidence maps predict sonographers’ labeling variability?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09433">http://arxiv.org/abs/2308.09433</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vanessa Gonzalez Duque, Leonhard Zirus, Yordanka Velikova, Nassir Navab, Diana Mateus</li>
<li>for: 本研究的目的是提高深度学习Segmentation方法的可靠性，使其能够更好地处理 ultrasound 图像中的各种物理原理导致的不确定性，如吸收、阴影和缺失边界。</li>
<li>methods: 本研究提出了一种新的方法，即使用 ultrasound 图像的可信度映射（CM）来引导深度学习Segmentation 网络，使其能够更好地考虑sonographers的不确定性，并生成更加准确的预测结果。</li>
<li>results: 研究结果表明，使用 ultrasound CMs 可以提高 Dice 分数、改善 Hausdorff 和平均表面距离，同时减少孤立像素预测。此外，研究还发现，使用 ultrasound CMs 可以提高对真实数据中困难 interpolations 的惩罚，从而提高 Segmentation 结果的可靠性。<details>
<summary>Abstract</summary>
Measuring cross-sectional areas in ultrasound images is a standard tool to evaluate disease progress or treatment response. Often addressed today with supervised deep-learning segmentation approaches, existing solutions highly depend upon the quality of experts' annotations. However, the annotation quality in ultrasound is anisotropic and position-variant due to the inherent physical imaging principles, including attenuation, shadows, and missing boundaries, commonly exacerbated with depth. This work proposes a novel approach that guides ultrasound segmentation networks to account for sonographers' uncertainties and generate predictions with variability similar to the experts. We claim that realistic variability can reduce overconfident predictions and improve physicians' acceptance of deep-learning cross-sectional segmentation solutions. Our method provides CM's certainty for each pixel for minimal computational overhead as it can be precalculated directly from the image. We show that there is a correlation between low values in the confidence maps and expert's label uncertainty. Therefore, we propose to give the confidence maps as additional information to the networks. We study the effect of the proposed use of ultrasound CMs in combination with four state-of-the-art neural networks and in two configurations: as a second input channel and as part of the loss. We evaluate our method on 3D ultrasound datasets of the thyroid and lower limb muscles. Our results show ultrasound CMs increase the Dice score, improve the Hausdorff and Average Surface Distances, and decrease the number of isolated pixel predictions. Furthermore, our findings suggest that ultrasound CMs improve the penalization of uncertain areas in the ground truth data, thereby improving problematic interpolations. Our code and example data will be made public at https://github.com/IFL-CAMP/Confidence-segmentation.
</details>
<details>
<summary>摘要</summary>
measuring cross-sectional areas in ultrasound images 是评估疾病进展或治疗响应的标准工具。现有的方法多数是由协调深度学习 segmentation 方法进行管理，但现有解决方案的质量很大程度取决于专家的注释。然而，在ultrasound中，注释质量是方均不同和位置 variant的，由于物理各种原理的约束，包括吸收、阴影和缺失边界，这些问题通常会在深度下恶化。这项工作提出了一种新的方法，使 ultrasound segmentation 网络能够考虑医生的不确定性，并生成与专家的预测相似的结果。我们认为，实际的不确定性可以减少过度的预测，并提高医生对深度学习横截 segmentation 解决方案的接受度。我们的方法可以在低计算开销下为每个像素提供CM的确定性，并且我们发现了图像中低值的CM confidence map 与专家的标签不确定性存在相关性。因此，我们提议将CM confidence map 作为网络的附加信息。我们在四种 state-of-the-art 神经网络中进行了研究，并在两种配置下评估了我们的方法：作为第二个输入通道和作为损失的一部分。我们在3D ultrasound 数据集上评估了我们的方法，并发现了以下结果： ultrasound CMs 可以提高 dice 分数，改善 Hausdorff 和平均表面距离，并减少孤立像素预测。此外，我们的发现表明， ultrasound CMs 可以优化地杂 interpolations 问题，从而提高疾病诊断的精度。我们的代码和示例数据将在 https://github.com/IFL-CAMP/Confidence-segmentation 上公开。
</details></li>
</ul>
<hr>
<h2 id="End-to-end-topographic-networks-as-models-of-cortical-map-formation-and-human-visual-behaviour-moving-beyond-convolutions"><a href="#End-to-end-topographic-networks-as-models-of-cortical-map-formation-and-human-visual-behaviour-moving-beyond-convolutions" class="headerlink" title="End-to-end topographic networks as models of cortical map formation and human visual behaviour: moving beyond convolutions"></a>End-to-end topographic networks as models of cortical map formation and human visual behaviour: moving beyond convolutions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09431">http://arxiv.org/abs/2308.09431</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zejin Lu, Adrien Doerig, Victoria Bosch, Bas Krahmer, Daniel Kaiser, Radoslaw M Cichy, Tim C Kietzmann</li>
<li>for: 理解 primate visual system 的几何组织结构的起源和功能</li>
<li>methods: 开发 All-Topographic Neural Networks (All-TNNs)，通过训练视觉输入获得 primate 视觉系统 的特征</li>
<li>results: All-TNNs 可以更好地适应人类视觉行为，并且可以在人类视觉中找到类别选择区域。<details>
<summary>Abstract</summary>
Computational models are an essential tool for understanding the origin and functions of the topographic organisation of the primate visual system. Yet, vision is most commonly modelled by convolutional neural networks that ignore topography by learning identical features across space. Here, we overcome this limitation by developing All-Topographic Neural Networks (All-TNNs). Trained on visual input, several features of primate topography emerge in All-TNNs: smooth orientation maps and cortical magnification in their first layer, and category-selective areas in their final layer. In addition, we introduce a novel dataset of human spatial biases in object recognition, which enables us to directly link models to behaviour. We demonstrate that All-TNNs significantly better align with human behaviour than previous state-of-the-art convolutional models due to their topographic nature. All-TNNs thereby mark an important step forward in understanding the spatial organisation of the visual brain and how it mediates visual behaviour.
</details>
<details>
<summary>摘要</summary>
计算模型是Primates视觉系统的起源和功能的重要工具。然而，视觉通常是使用 convolutional neural networks（ConvNets）来模型，这些ConvNets忽略了地理学的特征，通过学习同样的特征在空间上。在这里，我们超越这些限制，开发了All-Topographic Neural Networks（All-TNNs）。通过视觉输入，All-TNNs中出现了许多Primates的特征：平滑的方向图和 cortical magnification在其第一层，以及在其最后层中的类别特异区域。此外，我们还提出了一个新的人类空间偏好对象识别的数据集，该数据集允许我们直接将模型与行为相连。我们示出了All-TNNs在人类行为方面比前一代 convolutional 模型更好地适应，这是因为All-TNNs具有地理学性质。All-TNNs因此标志着理解视觉大脑的空间组织和如何通过视觉行为的传递重要一步进展。
</details></li>
</ul>
<hr>
<h2 id="Towards-Understanding-the-Generalizability-of-Delayed-Stochastic-Gradient-Descent"><a href="#Towards-Understanding-the-Generalizability-of-Delayed-Stochastic-Gradient-Descent" class="headerlink" title="Towards Understanding the Generalizability of Delayed Stochastic Gradient Descent"></a>Towards Understanding the Generalizability of Delayed Stochastic Gradient Descent</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09430">http://arxiv.org/abs/2308.09430</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoge Deng, Li Shen, Shengwei Li, Tao Sun, Dongsheng Li, Dacheng Tao</li>
<li>for: This paper investigates the generalization performance of asynchronous stochastic gradient descent (SGD) and provides sharper generalization error bounds.</li>
<li>methods: The paper uses generating function analysis to establish the average stability of the delayed gradient algorithm and derives upper bounds on the generalization error.</li>
<li>results: The theoretical results show that asynchronous delays can reduce the generalization error of the delayed SGD algorithm, and the experimental results validate these findings.Here’s the text in Simplified Chinese:</li>
<li>for: 这篇论文研究了异步梯度下降（SGD）的通用性表现，并提供了更为精细的通用错误 bound。</li>
<li>methods: 论文使用生成函数分析来确定延迟梯度算法的平均稳定性，并从这种稳定性得到了通用错误 bound。</li>
<li>results: 理论结果表明，异步延迟可以降低延迟SGD算法的通用错误，实验结果也证实了这些结论。<details>
<summary>Abstract</summary>
Stochastic gradient descent (SGD) performed in an asynchronous manner plays a crucial role in training large-scale machine learning models. However, the generalization performance of asynchronous delayed SGD, which is an essential metric for assessing machine learning algorithms, has rarely been explored. Existing generalization error bounds are rather pessimistic and cannot reveal the correlation between asynchronous delays and generalization. In this paper, we investigate sharper generalization error bound for SGD with asynchronous delay $\tau$. Leveraging the generating function analysis tool, we first establish the average stability of the delayed gradient algorithm. Based on this algorithmic stability, we provide upper bounds on the generalization error of $\tilde{\mathcal{O}}(\frac{T-\tau}{n\tau})$ and $\tilde{\mathcal{O}}(\frac{1}{n})$ for quadratic convex and strongly convex problems, respectively, where $T$ refers to the iteration number and $n$ is the amount of training data. Our theoretical results indicate that asynchronous delays reduce the generalization error of the delayed SGD algorithm. Analogous analysis can be generalized to the random delay setting, and the experimental results validate our theoretical findings.
</details>
<details>
<summary>摘要</summary>
<!--language_loops_enabled: false; --> Stochastic gradient descent（SGD）在异步方式下扮演着关键的角色在训练大规模机器学习模型中。然而，异步延迟SGD的总体化性表现，即机器学习算法的一个关键度量，很少被探讨。现有的总体错误约束很偏袋、无法描述异步延迟和总体化性之间的相互关系。在这篇论文中，我们调查了SGD异步延迟$\tau$的更精确的总体化性误差约束。通过生成函数分析工具，我们首先确立延迟梯度算法的平均稳定性。基于这种算法稳定性，我们提供了异步延迟SGD的总体化性误差的Upper bound，其中$T$表示迭代次数，$n$表示训练数据量。我们的理论结果表明，异步延迟可以降低延迟SGD算法的总体化性误差。可以将这种分析推广到随机延迟设置下，并且实验结果证明了我们的理论发现。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Single-Image-Deconvolution-with-Siamese-Neural-Networks"><a href="#Self-Supervised-Single-Image-Deconvolution-with-Siamese-Neural-Networks" class="headerlink" title="Self-Supervised Single-Image Deconvolution with Siamese Neural Networks"></a>Self-Supervised Single-Image Deconvolution with Siamese Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09426">http://arxiv.org/abs/2308.09426</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mikhail Papkov, Kaupo Palo, Leopold Parts</li>
<li>for:  Image reconstruction from inverse problems, specifically in 3D microscopy deconvolution tasks.</li>
<li>methods:  Deep learning methods with self-supervised blind-spot neural networks, combined with Fast Fourier Transform convolutions and Siamese invariance loss.</li>
<li>results:  Outperforms previous state-of-the-art deconvolution methods with known point spread function.<details>
<summary>Abstract</summary>
Inverse problems in image reconstruction are fundamentally complicated by unknown noise properties. Classical iterative deconvolution approaches amplify noise and require careful parameter selection for an optimal trade-off between sharpness and grain. Deep learning methods allow for flexible parametrization of the noise and learning its properties directly from the data. Recently, self-supervised blind-spot neural networks were successfully adopted for image deconvolution by including a known point-spread function in the end-to-end training. However, their practical application has been limited to 2D images in the biomedical domain because it implies large kernels that are poorly optimized. We tackle this problem with Fast Fourier Transform convolutions that provide training speed-up in 3D microscopy deconvolution tasks. Further, we propose to adopt a Siamese invariance loss for deconvolution and empirically identify its optimal position in the neural network between blind-spot and full image branches. The experimental results show that our improved framework outperforms the previous state-of-the-art deconvolution methods with a known point spread function.
</details>
<details>
<summary>摘要</summary>
<<SYS>> tranlate("Inverse problems in image reconstruction are fundamentally complicated by unknown noise properties. Classical iterative deconvolution approaches amplify noise and require careful parameter selection for an optimal trade-off between sharpness and grain. Deep learning methods allow for flexible parametrization of the noise and learning its properties directly from the data. Recently, self-supervised blind-spot neural networks were successfully adopted for image deconvolution by including a known point-spread function in the end-to-end training. However, their practical application has been limited to 2D images in the biomedical domain because it implies large kernels that are poorly optimized. We tackle this problem with Fast Fourier Transform convolutions that provide training speed-up in 3D microscopy deconvolution tasks. Further, we propose to adopt a Siamese invariance loss for deconvolution and empirically identify its optimal position in the neural network between blind-spot and full image branches. The experimental results show that our improved framework outperforms the previous state-of-the-art deconvolution methods with a known point spread function.")</SYS>>Here's the translation:“逆问题在图像重建中是基础上受到未知噪音特性的影响。经典迭代复原方法会增加噪音，并需要精确地选择参数以进行适当的对比 between sharpness 和 grain。深度学习方法允许对噪音进行可调 parametrization，并直接从数据中学习噪音的特性。现在，自适应盲点神经网络已经成功地应用于图像复原中。然而，它们的实际应用受到2D图像在生物医学领域的限制，因为它们假设了大的核心，对于复原效果不够优化。我们使用 Fast Fourier Transform 核函数来提高训练速度，并提出采用 Siamese 不变损函数来复原，并在神经网络中对复原效果进行实验性地调整。实验结果显示，我们的改进的框架比前一代的复原方法 with 知名点评函数更好。”
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Agent-Communication-and-Learning-through-Action-and-Language"><a href="#Enhancing-Agent-Communication-and-Learning-through-Action-and-Language" class="headerlink" title="Enhancing Agent Communication and Learning through Action and Language"></a>Enhancing Agent Communication and Learning through Action and Language</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10842">http://arxiv.org/abs/2308.10842</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Caselles-Dupré Hugo, Sigaud Olivier, Chetouani Mohamed</li>
<li>for: 这篇论文是为了探讨一种新的GC-代理人，它可以作为教师和学习者而设计的。</li>
<li>methods: 论文使用了动作示例和语言指令，以提高人工智能交互的效率。同时，它还研究了在人类交流中的教学和目的实现中的pedagogy和 Pragmatism的应用，以提高代理人的教学和学习能力。</li>
<li>results: 论文发现，将交通方式（动作和语言）相结合可以提高学习效果，并且提出了一种多Modal的交互方式。<details>
<summary>Abstract</summary>
We introduce a novel category of GC-agents capable of functioning as both teachers and learners. Leveraging action-based demonstrations and language-based instructions, these agents enhance communication efficiency. We investigate the incorporation of pedagogy and pragmatism, essential elements in human communication and goal achievement, enhancing the agents' teaching and learning capabilities. Furthermore, we explore the impact of combining communication modes (action and language) on learning outcomes, highlighting the benefits of a multi-modal approach.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种新的GC-代理人，能够同时作为教师和学生 function。通过行动示例和语言指令，这些代理人提高了交流效率。我们研究了人类communication和目标实现中的pedagogy和pragmatism元素，以提高代理人的教学和学习能力。此外，我们还探讨了 combining communication modes（行动和语言）对学习成果的影响，指出多模式approach的 beneficial effects。
</details></li>
</ul>
<hr>
<h2 id="ICU-Mortality-Prediction-Using-Long-Short-Term-Memory-Networks"><a href="#ICU-Mortality-Prediction-Using-Long-Short-Term-Memory-Networks" class="headerlink" title="ICU Mortality Prediction Using Long Short-Term Memory Networks"></a>ICU Mortality Prediction Using Long Short-Term Memory Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12800">http://arxiv.org/abs/2308.12800</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manel Mili, Asma Kerkeni, Asma Ben Abdallah, Mohamed Hedi Bedoui</li>
<li>for: 这个论文是为了提出一种自动化数据驱动系统，用于分析医疗电子记录（EHRs）中的大量多变量时间数据，并提取高级信息以预测医院死亡率和Length of Stay（LOS）的早期预测。</li>
<li>methods: 本研究使用了Long Short-Term Memory（LSTM）网络，通过减少时间框架到6小时，以提高临床任务的可靠性。</li>
<li>results: 实验结果表明，LSTM模型可以rigorously multivariate时间序列测量来建立真实世界预测引擎。<details>
<summary>Abstract</summary>
Extensive bedside monitoring in Intensive Care Units (ICUs) has resulted in complex temporal data regarding patient physiology, which presents an upscale context for clinical data analysis. In the other hand, identifying the time-series patterns within these data may provide a high aptitude to predict clinical events. Hence, we investigate, during this work, the implementation of an automatic data-driven system, which analyzes large amounts of multivariate temporal data derived from Electronic Health Records (EHRs), and extracts high-level information so as to predict in-hospital mortality and Length of Stay (LOS) early. Practically, we investigate the applicability of LSTM network by reducing the time-frame to 6-hour so as to enhance clinical tasks. The experimental results highlight the efficiency of LSTM model with rigorous multivariate time-series measurements for building real-world prediction engines.
</details>
<details>
<summary>摘要</summary>
延伸床side监测在重症监护室(ICU)中已经导致了复杂的时间序列数据，这些数据提供了丰富的临床数据分析上下文。然而，在这些数据中寻找时间序列模式可能提供高度预测临床事件的可能性。因此，在这项工作中，我们调查了一个自动化数据驱动系统，该系统分析大量多变量时间序列数据来自电子医疗纪录(EHR)，并提取高级信息以预测医院死亡率和 lengths of stay (LOS)的早期预测。实际上，我们研究了使用LSTM网络，通过减少时间帧为6小时来增强临床任务。实验结果表明LSTM模型在多变量时间序列测量上具有强大的预测能力。
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-Solutions-for-the-Analysis-of-Single-Particle-Diffusion-Trajectories"><a href="#Machine-Learning-Solutions-for-the-Analysis-of-Single-Particle-Diffusion-Trajectories" class="headerlink" title="Machine-Learning Solutions for the Analysis of Single-Particle Diffusion Trajectories"></a>Machine-Learning Solutions for the Analysis of Single-Particle Diffusion Trajectories</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09414">http://arxiv.org/abs/2308.09414</a></li>
<li>repo_url: None</li>
<li>paper_authors: Henrik Seckler, Janusz Szwabinski, Ralf Metzler</li>
<li>for: 这篇论文是关于解读单个分子、细胞或动物的扩散运动的轨迹记录，以及利用现代机器学习技术解释这些记录的动力学机制。</li>
<li>methods: 这篇论文介绍了最新的机器学习技术，包括在异常扩散挑战中取得成功的方法，以及包括不确定性估计和特征基于的方法，以提高解释性和提供具体的学习过程信息。</li>
<li>results: 这篇论文的结果表明，这些新的机器学习方法可以成功地解释单个分子、细胞或动物的扩散运动，并且可以提供具体的系统参数和不确定性估计。<details>
<summary>Abstract</summary>
Single-particle traces of the diffusive motion of molecules, cells, or animals are by-now routinely measured, similar to stochastic records of stock prices or weather data. Deciphering the stochastic mechanism behind the recorded dynamics is vital in understanding the observed systems. Typically, the task is to decipher the exact type of diffusion and/or to determine system parameters. The tools used in this endeavor are currently revolutionized by modern machine-learning techniques. In this Perspective we provide an overview over recently introduced methods in machine-learning for diffusive time series, most notably, those successfully competing in the Anomalous-Diffusion-Challenge. As such methods are often criticized for their lack of interpretability, we focus on means to include uncertainty estimates and feature-based approaches, both improving interpretability and providing concrete insight into the learning process of the machine. We expand the discussion by examining predictions on different out-of-distribution data. We also comment on expected future developments.
</details>
<details>
<summary>摘要</summary>
一个粒子轨迹的游移，包括分子、细胞或动物的游移，现在可以通过单个粒子轨迹来评估，类似于随机记录的股票价格或天气数据。解释记录的随机机制是理解所观察系统的关键。通常，任务是确定扩散的类型和/或系统参数。现代机器学习技术已经对这些工具进行了革命性的改进。在这篇观点文章中，我们提供了最近引入的机器学习方法 для游移时间序列的概述，其中许多方法在解释性方面受到批判。因此，我们将重点介绍包括不确定性估计和特征基本方法在内的方法，以提高解释性和提供机器学习过程的具体信息。此外，我们还探讨了不同的外部数据集预测结果，以及未来发展的预期。
</details></li>
</ul>
<hr>
<h2 id="Metadata-Improves-Segmentation-Through-Multitasking-Elicitation"><a href="#Metadata-Improves-Segmentation-Through-Multitasking-Elicitation" class="headerlink" title="Metadata Improves Segmentation Through Multitasking Elicitation"></a>Metadata Improves Segmentation Through Multitasking Elicitation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09411">http://arxiv.org/abs/2308.09411</a></li>
<li>repo_url: None</li>
<li>paper_authors: Iaroslav Plutenko, Mikhail Papkov, Kaupo Palo, Leopold Parts, Dmytro Fishman</li>
<li>for: 这个论文主要针对的是使用metadata进行深度学习方法的semantic segmentation任务。</li>
<li>methods: 这个论文使用了通道调节机制，将metadata作为 convolutional neural network 的输入，以提高semantic segmentation的结果。</li>
<li>results: 论文表明，通过使用metadata作为输入，可以提高semantic segmentation的结果，同时实现可靠地和经济地实现。此外，论文还explores了metadata驱动的系统的特点和优点。<details>
<summary>Abstract</summary>
Metainformation is a common companion to biomedical images. However, this potentially powerful additional source of signal from image acquisition has had limited use in deep learning methods, for semantic segmentation in particular. Here, we incorporate metadata by employing a channel modulation mechanism in convolutional networks and study its effect on semantic segmentation tasks. We demonstrate that metadata as additional input to a convolutional network can improve segmentation results while being inexpensive in implementation as a nimble add-on to popular models. We hypothesize that this benefit of metadata can be attributed to facilitating multitask switching. This aspect of metadata-driven systems is explored and discussed in detail.
</details>
<details>
<summary>摘要</summary>
这里的 metadata 是医学影像的常见附属资讯。然而，这个潜在强大的资料来源在深度学习方法中对于 semantic segmentation 仍然有限的使用。在这里，我们通过运用槽模组化机制在 convolutional network 中使用 metadata，并研究其对 semantic segmentation 任务的影响。我们发现，Metadata 作为 convolutional network 的额外输入，可以提高 segmentation 结果，而且实现起来相对便宜。我们推测这个优点可以归于metadata 帮助多任务转换。这个方面的metadata-driven系统的细节和讨论，在这里得到了详细的探讨。
</details></li>
</ul>
<hr>
<h2 id="Learning-MDL-logic-programs-from-noisy-data"><a href="#Learning-MDL-logic-programs-from-noisy-data" class="headerlink" title="Learning MDL logic programs from noisy data"></a>Learning MDL logic programs from noisy data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09393">http://arxiv.org/abs/2308.09393</a></li>
<li>repo_url: None</li>
<li>paper_authors: Céline Hocquette, Andreas Niskanen, Matti Järvisalo, Andrew Cropper</li>
<li>for: 学习从含噪数据中学习程序，增强 inductive logic programming 的能力。</li>
<li>methods: 使用 minimal description length 方法，可以学习含 recursive 程序的含噪数据。</li>
<li>results: 在多个领域（包括药物设计、游戏撸机和程序生成）中，我们的方法可以超过现有方法的预测精度，并可以扩展到moderate 量的噪音。<details>
<summary>Abstract</summary>
Many inductive logic programming approaches struggle to learn programs from noisy data. To overcome this limitation, we introduce an approach that learns minimal description length programs from noisy data, including recursive programs. Our experiments on several domains, including drug design, game playing, and program synthesis, show that our approach can outperform existing approaches in terms of predictive accuracies and scale to moderate amounts of noise.
</details>
<details>
<summary>摘要</summary>
很多逻辑学习程序方法在含噪数据上学习困难，以往我们提出了一种学习最短描述长度程序从含噪数据中学习，包括循环程序。我们在多个领域进行了实验，包括药物设计、游戏玩家和程序生成，结果表明我们的方法可以在适度噪音下达到更高的预测精度和规模。
</details></li>
</ul>
<hr>
<h2 id="FunQuant-A-R-package-to-perform-quantization-in-the-context-of-rare-events-and-time-consuming-simulations"><a href="#FunQuant-A-R-package-to-perform-quantization-in-the-context-of-rare-events-and-time-consuming-simulations" class="headerlink" title="FunQuant: A R package to perform quantization in the context of rare events and time-consuming simulations"></a>FunQuant: A R package to perform quantization in the context of rare events and time-consuming simulations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10871">http://arxiv.org/abs/2308.10871</a></li>
<li>repo_url: None</li>
<li>paper_authors: Charlie Sire, Yann Richet, Rodolphe Le Riche, Didier Rullière, Jérémy Rohmer, Lucie Pheulpin</li>
<li>for: 本研究旨在提出一种新的数据量化方法，以解决数据评估成本高、罕见事件的问题。</li>
<li>methods: 本研究使用 Lloyd 算法来partition 空间，并基于 Voronoi 维度和概率质量来构建一个离散分布。但是，在数据评估成本高、罕见事件的场景下，Lloyd 算法难以实现高精度。因此，本研究还提出了一种元模型和适应采样方法来提高罕见 cluster 的计算精度。</li>
<li>results: 本研究的结果表明，采用元模型和适应采样方法可以提高罕见 cluster 的计算精度，并且可以降低数据评估成本。同时，本研究还发现了一些可能的应用场景，例如在机器学习和数据挖掘等领域。<details>
<summary>Abstract</summary>
Quantization summarizes continuous distributions by calculating a discrete approximation. Among the widely adopted methods for data quantization is Lloyd's algorithm, which partitions the space into Vorono\"i cells, that can be seen as clusters, and constructs a discrete distribution based on their centroids and probabilistic masses. Lloyd's algorithm estimates the optimal centroids in a minimal expected distance sense, but this approach poses significant challenges in scenarios where data evaluation is costly, and relates to rare events. Then, the single cluster associated to no event takes the majority of the probability mass. In this context, a metamodel is required and adapted sampling methods are necessary to increase the precision of the computations on the rare clusters.
</details>
<details>
<summary>摘要</summary>
量化概率分布的目的是计算一个粗略的抽象。广泛采用的数据量化方法之一是朗道算法，该算法将空间分成Voronoi细胞，可以看作归一化的集群，并根据其中心和概率质量来构建一个离散分布。朗道算法估算最佳中心，但这种方法在数据评估成本高、 relate to rare events 的场景下存在很大挑战。在这种情况下，一个méta模型是必要的，并且采用适当的采样方法可以提高罕见集群的计算精度。Note: "概率质量" (probability mass) is translated as "概率质量" in Simplified Chinese, but it should be noted that this term is not commonly used in Chinese and may not be well understood by some readers. A more common term used in Chinese to refer to the probability of a cluster is "集群概率" (cluster probability).
</details></li>
</ul>
<hr>
<h2 id="On-Gradient-like-Explanation-under-a-Black-box-Setting-When-Black-box-Explanations-Become-as-Good-as-White-box"><a href="#On-Gradient-like-Explanation-under-a-Black-box-Setting-When-Black-box-Explanations-Become-as-Good-as-White-box" class="headerlink" title="On Gradient-like Explanation under a Black-box Setting: When Black-box Explanations Become as Good as White-box"></a>On Gradient-like Explanation under a Black-box Setting: When Black-box Explanations Become as Good as White-box</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09381">http://arxiv.org/abs/2308.09381</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yi Cai, Gerhard Wunder</li>
<li>for: 本文旨在提供一种基于梯度估计的解释方法，以解释数据驱动方法的决策过程中的特征对应关系。</li>
<li>methods: 本文提出了一种名为GEEX的解释方法，该方法在黑盒Setting下提供了梯度类型的解释，并将该方法与路径方法集成，得到了iGEEX（集成GEEX）方法。</li>
<li>results: 实验表明，提出的方法在图像数据上表现出excelsior的效果，并且与状态艺术方法相比，具有更高的可靠性和灵活性。<details>
<summary>Abstract</summary>
Attribution methods shed light on the explainability of data-driven approaches such as deep learning models by revealing the most contributing features to decisions that have been made. A widely accepted way of deriving feature attributions is to analyze the gradients of the target function with respect to input features. Analysis of gradients requires full access to the target system, meaning that solutions of this kind treat the target system as a white-box. However, the white-box assumption may be untenable due to security and safety concerns, thus limiting their practical applications. As an answer to the limited flexibility, this paper presents GEEX (gradient-estimation-based explanation), an explanation method that delivers gradient-like explanations under a black-box setting. Furthermore, we integrate the proposed method with a path method. The resulting approach iGEEX (integrated GEEX) satisfies the four fundamental axioms of attribution methods: sensitivity, insensitivity, implementation invariance, and linearity. With a focus on image data, the exhaustive experiments empirically show that the proposed methods outperform state-of-the-art black-box methods and achieve competitive performance compared to the ones with full access.
</details>
<details>
<summary>摘要</summary>
<<SYS>>传递方法可以描述数据驱动方法中的解释性，例如深度学习模型的决策过程中的最重要的特征。一种广泛接受的解释特征的 derivation 方法是分析目标函数对输入特征的梯度。这种方法需要对目标系统具有完整的访问权，因此被称为白盒模型。然而，白盒假设可能存在安全和安全问题，因此它们在实际应用中有限制。为了解决这些限制，本文提出了 GEEX（梯度估计基于解释），一种解释方法，可以在黑盒设定下提供梯度类似的解释。此外，我们将 GEEX 与路径方法集成，得到 iGEEX（集成 GEEX）。这种方法满足了解释方法的四个基本假设：敏感性、不敏感性、实现不变性和线性。对于图像数据，我们进行了详细的实验，并证明了提案的方法在黑盒设定下比状态eliaoning的黑盒方法表现更好，并且与完整访问下的方法具有竞争性。Note: The translation is in Simplified Chinese, which is the standard Chinese writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Deciphering-knee-osteoarthritis-diagnostic-features-with-explainable-artificial-intelligence-A-systematic-review"><a href="#Deciphering-knee-osteoarthritis-diagnostic-features-with-explainable-artificial-intelligence-A-systematic-review" class="headerlink" title="Deciphering knee osteoarthritis diagnostic features with explainable artificial intelligence: A systematic review"></a>Deciphering knee osteoarthritis diagnostic features with explainable artificial intelligence: A systematic review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09380">http://arxiv.org/abs/2308.09380</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yun Xin Teoh, Alice Othmani, Siew Li Goh, Juliana Usman, Khin Wee Lai</li>
<li>for: 提高膝关节炎诊断的可靠性和可读性</li>
<li>methods: 使用解释性人工智能技术来提高膝关节炎诊断的可信度和解释性</li>
<li>results: 提供了膝关节炎诊断领域中解释性人工智能技术的首次报告，并从数据可解性和模型可解性两个角度进行了讨论，以便促进这种技术在临床实践中的采用<details>
<summary>Abstract</summary>
Existing artificial intelligence (AI) models for diagnosing knee osteoarthritis (OA) have faced criticism for their lack of transparency and interpretability, despite achieving medical-expert-like performance. This opacity makes them challenging to trust in clinical practice. Recently, explainable artificial intelligence (XAI) has emerged as a specialized technique that can provide confidence in the model's prediction by revealing how the prediction is derived, thus promoting the use of AI systems in healthcare. This paper presents the first survey of XAI techniques used for knee OA diagnosis. The XAI techniques are discussed from two perspectives: data interpretability and model interpretability. The aim of this paper is to provide valuable insights into XAI's potential towards a more reliable knee OA diagnosis approach and encourage its adoption in clinical practice.
</details>
<details>
<summary>摘要</summary>
现有的膝关节滑块病（OA）诊断模型（AI）已经受到了不透明性和解释性的批评，即使它们达到了医学专家水平的性能。这种透明性使得它们在临床实践中具有挑战性。在最近，解释性人工智能（XAI）已经emerged as a specialized technique，可以为诊断提供信任度，揭示如何 derive 预测结果，从而推动人工智能系统在医疗领域的应用。本文是膝关节滑块诊断领域中第一篇XAI技术survey。XAI技术从两个角度进行了讨论：数据可读性和模型可读性。本文的目的是提供有价值的情况，推动XAI在诊断方法中的采用，并且 Encourage its adoption in clinical practice.
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Techniques-in-Extreme-Weather-Events-A-Review"><a href="#Deep-Learning-Techniques-in-Extreme-Weather-Events-A-Review" class="headerlink" title="Deep Learning Techniques in Extreme Weather Events: A Review"></a>Deep Learning Techniques in Extreme Weather Events: A Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10995">http://arxiv.org/abs/2308.10995</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shikha Verma, Kuldeep Srivastava, Akhilesh Tiwari, Shekhar Verma</li>
<li>for: This paper provides a comprehensive overview of the state-of-the-art deep learning techniques in the field of meteorology, specifically for accurate analysis and precise forecasting of extreme weather events.</li>
<li>methods: The paper explores the utilization of deep learning architectures in various aspects of weather prediction, including thunderstorm, lightning, precipitation, drought, heatwave, cold waves, and tropical cyclones.</li>
<li>results: The paper highlights the potential of deep learning techniques to capture complex patterns and non-linear relationships in weather data, and discusses the limitations of current approaches and future directions for advancements in the field of meteorology.<details>
<summary>Abstract</summary>
Extreme weather events pose significant challenges, thereby demanding techniques for accurate analysis and precise forecasting to mitigate its impact. In recent years, deep learning techniques have emerged as a promising approach for weather forecasting and understanding the dynamics of extreme weather events. This review aims to provide a comprehensive overview of the state-of-the-art deep learning in the field. We explore the utilization of deep learning architectures, across various aspects of weather prediction such as thunderstorm, lightning, precipitation, drought, heatwave, cold waves and tropical cyclones. We highlight the potential of deep learning, such as its ability to capture complex patterns and non-linear relationships. Additionally, we discuss the limitations of current approaches and highlight future directions for advancements in the field of meteorology. The insights gained from this systematic review are crucial for the scientific community to make informed decisions and mitigate the impacts of extreme weather events.
</details>
<details>
<summary>摘要</summary>
extreme weather events pose significant challenges, requiring accurate analysis and precise forecasting to mitigate their impact. In recent years, deep learning techniques have emerged as a promising approach for weather forecasting and understanding the dynamics of extreme weather events. this review aims to provide a comprehensive overview of the state-of-the-art deep learning in the field. We explore the utilization of deep learning architectures across various aspects of weather prediction, such as thunderstorms, lightning, precipitation, droughts, heatwaves, cold waves, and tropical cyclones. we highlight the potential of deep learning, such as its ability to capture complex patterns and non-linear relationships. Additionally, we discuss the limitations of current approaches and highlight future directions for advancements in the field of meteorology. The insights gained from this systematic review are crucial for the scientific community to make informed decisions and mitigate the impacts of extreme weather events.
</details></li>
</ul>
<hr>
<h2 id="Image-Processing-and-Machine-Learning-for-Hyperspectral-Unmixing-An-Overview-and-the-HySUPP-Python-Package"><a href="#Image-Processing-and-Machine-Learning-for-Hyperspectral-Unmixing-An-Overview-and-the-HySUPP-Python-Package" class="headerlink" title="Image Processing and Machine Learning for Hyperspectral Unmixing: An Overview and the HySUPP Python Package"></a>Image Processing and Machine Learning for Hyperspectral Unmixing: An Overview and the HySUPP Python Package</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09375">http://arxiv.org/abs/2308.09375</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/behnoodrasti/hysupp">https://github.com/behnoodrasti/hysupp</a></li>
<li>paper_authors: Behnood Rasti, Alexandre Zouaoui, Julien Mairal, Jocelyn Chanussot</li>
<li>for: 这 paper 提供了一个概述先进和传统混合分析方法的Overview，以及三类不同类型的 linear unmixing 的比较。</li>
<li>methods: 这 paper 使用了先进的图像处理和机器学习技术，包括超vised、semi-supervised 和隐藏式 linear unmixing。</li>
<li>results: 实验结果显示了不同类型的混合分析方法在不同场景下的优势，以及 Python 基于的开源包 HySUPP 可以在 <a target="_blank" rel="noopener" href="https://github.com/BehnoodRasti/HySUPP">https://github.com/BehnoodRasti/HySUPP</a> 上下载。<details>
<summary>Abstract</summary>
Spectral pixels are often a mixture of the pure spectra of the materials, called endmembers, due to the low spatial resolution of hyperspectral sensors, double scattering, and intimate mixtures of materials in the scenes. Unmixing estimates the fractional abundances of the endmembers within the pixel. Depending on the prior knowledge of endmembers, linear unmixing can be divided into three main groups: supervised, semi-supervised, and unsupervised (blind) linear unmixing. Advances in Image processing and machine learning substantially affected unmixing. This paper provides an overview of advanced and conventional unmixing approaches. Additionally, we draw a critical comparison between advanced and conventional techniques from the three categories. We compare the performance of the unmixing techniques on three simulated and two real datasets. The experimental results reveal the advantages of different unmixing categories for different unmixing scenarios. Moreover, we provide an open-source Python-based package available at https://github.com/BehnoodRasti/HySUPP to reproduce the results.
</details>
<details>
<summary>摘要</summary>
几何色素 pixels 经常是杂合物质的纯谱，称为终端成员，由于雷达传感器的低空间分辨率、重吸散和场景中物质的温顺混合，导致这种杂合。混合计算器中的分量，即混合率，用于描述终端成员在像素中的存在度。根据终端成员的先知情，线性混合可以分为三类：监督、半监督和无监督（盲目）线性混合。图像处理和机器学习技术的进步对混合产生了深远的影响。本文提供了高级和传统混合方法的概述，并对这两种类型的混合技术进行抽象比较。我们将对三个 simulate 和两个实际数据集进行比较，并提供一个开源的 Python 基于包，可以在 <https://github.com/BehnoodRasti/HySUPP> 上获取。
</details></li>
</ul>
<hr>
<h2 id="Noise-Sensitivity-and-Stability-of-Deep-Neural-Networks-for-Binary-Classification"><a href="#Noise-Sensitivity-and-Stability-of-Deep-Neural-Networks-for-Binary-Classification" class="headerlink" title="Noise Sensitivity and Stability of Deep Neural Networks for Binary Classification"></a>Noise Sensitivity and Stability of Deep Neural Networks for Binary Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09374">http://arxiv.org/abs/2308.09374</a></li>
<li>repo_url: None</li>
<li>paper_authors: Johan Jonasson, Jeffrey E. Steif, Olof Zetterqvist</li>
<li>for: 这个论文的目的是理解深度神经网络（DNN）分类器的不稳定性现象，通过 Boolean 函数的视角来研究 DNN 模型是否敏感于噪声。</li>
<li>methods: 这篇论文使用了 Boolean 函数的概念来研究 DNN 模型，并对两种标准的 DNN 架构（完全连接和卷积模型）进行了研究，并且使用了随机 initialize 的 Gaussian 权重来调查这些模型的性质。</li>
<li>results: 研究发现，在随机 initialize 的 Gaussian 权重下，fully connected 模型和卷积模型都是噪声敏感的，而且卷积模型的噪声敏感性比 fully connected 模型更强。<details>
<summary>Abstract</summary>
A first step is taken towards understanding often observed non-robustness phenomena of deep neural net (DNN) classifiers. This is done from the perspective of Boolean functions by asking if certain sequences of Boolean functions represented by common DNN models are noise sensitive or noise stable, concepts defined in the Boolean function literature. Due to the natural randomness in DNN models, these concepts are extended to annealed and quenched versions. Here we sort out the relation between these definitions and investigate the properties of two standard DNN architectures, the fully connected and convolutional models, when initiated with Gaussian weights.
</details>
<details>
<summary>摘要</summary>
“一步进展在深度神经网络（DNN）分类器的不稳定现象之理解方面。这是从布尔函数的角度出发，询问了一些通用DNN模型表示的布尔函数是否对随机变量敏感或不敏感，这些概念在布尔函数文献中已经定义。由于自然的随机性在DNN模型中，这些定义被扩展到气体化和冷却版本。我们详细探讨这些定义之间的关系，并调查了两种标准的DNN架构，完全连接和卷积分模型，当启动时的Gauss矩阵初值。”Note that " Simplified Chinese" is also known as "Mandarin" or "Standard Chinese".
</details></li>
</ul>
<hr>
<h2 id="Which-Transformer-to-Favor-A-Comparative-Analysis-of-Efficiency-in-Vision-Transformers"><a href="#Which-Transformer-to-Favor-A-Comparative-Analysis-of-Efficiency-in-Vision-Transformers" class="headerlink" title="Which Transformer to Favor: A Comparative Analysis of Efficiency in Vision Transformers"></a>Which Transformer to Favor: A Comparative Analysis of Efficiency in Vision Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09372">http://arxiv.org/abs/2308.09372</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tobna/whattransformertofavor">https://github.com/tobna/whattransformertofavor</a></li>
<li>paper_authors: Tobias Christian Nauen, Sebastian Palacio, Andreas Dengel</li>
<li>For: This paper aims to provide a comprehensive analysis of vision transformers and related architectures, evaluating their efficiency across multiple performance metrics.* Methods: The authors use more than 30 models to conduct a holistic evaluation of efficiency-oriented transformers, considering various performance metrics.* Results: The study reveals several surprising insights, including the Pareto optimality of ViT across multiple efficiency metrics, the strength of hybrid attention-CNN models, and the positive correlation between FLOPS and training memory. The results provide valuable insights for practitioners and researchers in selecting models for specific applications.Here’s the Chinese version of the three key information points:* For: 这篇论文目的是为了提供视transformer和相关架构的全面分析，以评估它们的效率 across多个性能指标。* Methods: 作者使用了多于30个模型来进行视transformer和相关架构的总体评估，考虑多个性能指标。* Results: 研究发现了一些意外的发现，如 ViT在多个效率指标上的Pareto优化，混合注意力-CNN模型在低执行内存和参数数量上的强大表现，以及图像大小的扩展对模型性能的影响。结果为实践者和研究人员提供了有价值的指导，帮助他们在选择特定应用场景时做出 Informed decisions。<details>
<summary>Abstract</summary>
The growing popularity of Vision Transformers as the go-to models for image classification has led to an explosion of architectural modifications claiming to be more efficient than the original ViT. However, a wide diversity of experimental conditions prevents a fair comparison between all of them, based solely on their reported results. To address this gap in comparability, we conduct a comprehensive analysis of more than 30 models to evaluate the efficiency of vision transformers and related architectures, considering various performance metrics. Our benchmark provides a comparable baseline across the landscape of efficiency-oriented transformers, unveiling a plethora of surprising insights. For example, we discover that ViT is still Pareto optimal across multiple efficiency metrics, despite the existence of several alternative approaches claiming to be more efficient. Results also indicate that hybrid attention-CNN models fare particularly well when it comes to low inference memory and number of parameters, and also that it is better to scale the model size, than the image size. Furthermore, we uncover a strong positive correlation between the number of FLOPS and the training memory, which enables the estimation of required VRAM from theoretical measurements alone.   Thanks to our holistic evaluation, this study offers valuable insights for practitioners and researchers, facilitating informed decisions when selecting models for specific applications. We publicly release our code and data at https://github.com/tobna/WhatTransformerToFavor
</details>
<details>
<summary>摘要</summary>
“vision transformer”的快速增长 Popularity 使得许多建筑修改 claim 能够更高效 than the original ViT。然而，各种实验条件的多样性使得对所有这些模型进行公平比较变得困难，基于其报告的结果 alone。为了解决这个问题，我们进行了详细的30多种模型的分析，以评估视 transformer 和相关建筑的效率。我们的标准提供了多种效率指标下的共同基线，揭示了许多有趣的发现。例如，我们发现，despite the existence of several alternative approaches claiming to be more efficient, ViT still maintains Pareto optimality across multiple efficiency metrics。 results also show that hybrid attention-CNN models perform particularly well in terms of low inference memory and number of parameters, and that it is better to scale the model size than the image size. In addition, we find a strong positive correlation between the number of FLOPS and the training memory, which enables the estimation of required VRAM from theoretical measurements alone.  thanks to our comprehensive evaluation, this study provides valuable insights for practitioners and researchers, facilitating informed decisions when selecting models for specific applications. We publicly release our code and data at <https://github.com/tobna/WhatTransformerToFavor>。
</details></li>
</ul>
<hr>
<h2 id="A-tailored-Handwritten-Text-Recognition-System-for-Medieval-Latin"><a href="#A-tailored-Handwritten-Text-Recognition-System-for-Medieval-Latin" class="headerlink" title="A tailored Handwritten-Text-Recognition System for Medieval Latin"></a>A tailored Handwritten-Text-Recognition System for Medieval Latin</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09368">http://arxiv.org/abs/2308.09368</a></li>
<li>repo_url: None</li>
<li>paper_authors: Philipp Koch, Gilary Vera Nuñez, Esteban Garces Arias, Christian Heumann, Matthias Schöffel, Alexander Häberlin, Matthias Aßenmacher</li>
<li>for: 这份研究旨在为巴伐利亚科学院和人文学院的中世纪拉丁词典进行数字化。</li>
<li>methods: 这个pipeline使用两个最新的图像分割模型来准备输入数据集，并使用多种基于转换器的模型和GPT-2解码器进行实验。</li>
<li>results: 最佳实现达到了Character Error Rate（CER）0.015，超过了商业Google云视觉模型，并且表现更稳定。<details>
<summary>Abstract</summary>
The Bavarian Academy of Sciences and Humanities aims to digitize its Medieval Latin Dictionary. This dictionary entails record cards referring to lemmas in medieval Latin, a low-resource language. A crucial step of the digitization process is the Handwritten Text Recognition (HTR) of the handwritten lemmas found on these record cards. In our work, we introduce an end-to-end pipeline, tailored to the medieval Latin dictionary, for locating, extracting, and transcribing the lemmas. We employ two state-of-the-art (SOTA) image segmentation models to prepare the initial data set for the HTR task. Furthermore, we experiment with different transformer-based models and conduct a set of experiments to explore the capabilities of different combinations of vision encoders with a GPT-2 decoder. Additionally, we also apply extensive data augmentation resulting in a highly competitive model. The best-performing setup achieved a Character Error Rate (CER) of 0.015, which is even superior to the commercial Google Cloud Vision model, and shows more stable performance.
</details>
<details>
<summary>摘要</summary>
巴伐利亚科学院计划数字化中世纪拉丁词典。这个词典包含手写记录卡上的中世纪拉丁词汇，是一种低资源语言。我们的工作是开发一个端到端管道，专门为中世纪拉丁词典的数字化进行找到、提取和译写词汇。我们使用两个当今最佳实践（SOTA）图像分割模型来准备初始数据集，并对不同的变换器基于模型进行试验，以探索不同组合的视觉编码器和GPT-2解码器之间的可能性。此外，我们还进行了广泛的数据增强，实现了非常竞争力强的模型。最佳设置实现的字符错误率（CER）为0.015，甚至高于商业Google云视觉模型，并且表现更加稳定。
</details></li>
</ul>
<hr>
<h2 id="On-the-Approximation-of-Bi-Lipschitz-Maps-by-Invertible-Neural-Networks"><a href="#On-the-Approximation-of-Bi-Lipschitz-Maps-by-Invertible-Neural-Networks" class="headerlink" title="On the Approximation of Bi-Lipschitz Maps by Invertible Neural Networks"></a>On the Approximation of Bi-Lipschitz Maps by Invertible Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09367">http://arxiv.org/abs/2308.09367</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bangti Jin, Zehui Zhou, Jun Zou</li>
<li>for: 这个论文主要是用于研究嵌入型神经网络（INNs）的容量和应用可能性。</li>
<li>methods: 该论文使用了一种基于对接的INNs，并提供了一种基于模型简化、主成分分析和INNs的方法来近似具有不同维度的映射。</li>
<li>results: 研究发现，该方法可以同时近似前向和反向映射，并且可以用于近似参数化第二阶几何问题的解Operator。初步的数据分析表明该方法的可行性。<details>
<summary>Abstract</summary>
Invertible neural networks (INNs) represent an important class of deep neural network architectures that have been widely used in several applications. The universal approximation properties of INNs have also been established recently. However, the approximation rate of INNs is largely missing. In this work, we provide an analysis of the capacity of a class of coupling-based INNs to approximate bi-Lipschitz continuous mappings on a compact domain, and the result shows that it can well approximate both forward and inverse maps simultaneously. Furthermore, we develop an approach for approximating bi-Lipschitz maps on infinite-dimensional spaces that simultaneously approximate the forward and inverse maps, by combining model reduction with principal component analysis and INNs for approximating the reduced map, and we analyze the overall approximation error of the approach. Preliminary numerical results show the feasibility of the approach for approximating the solution operator for parameterized second-order elliptic problems.
</details>
<details>
<summary>摘要</summary>
invertible neural networks (INNs) 是深度神经网络的一个重要类型，已经广泛应用在各种领域。 INNs 的 universality 也已经最近得到了证明。然而， INNs 的 Approximation 率却尚未得到了充分的研究。在这项工作中，我们对一类基于coupling的 INNs 的容量进行了分析，并证明了它可以同时高精度地approximate forward 和 inverse 映射。此外，我们还开发了一种能够同时approximate forward 和 inverse 映射的方法，该方法基于模型减reduction、主成分分析和 INNs 来approximate减少后的映射，并对全局approximate error进行了分析。初步的数值结果表明该方法可以approximate parameterized second-order elliptic problems 的解Operator。
</details></li>
</ul>
<hr>
<h2 id="Multi-feature-concatenation-and-multi-classifier-stacking-an-interpretable-and-generalizable-machine-learning-method-for-MDD-discrimination-with-rsfMRI"><a href="#Multi-feature-concatenation-and-multi-classifier-stacking-an-interpretable-and-generalizable-machine-learning-method-for-MDD-discrimination-with-rsfMRI" class="headerlink" title="Multi-feature concatenation and multi-classifier stacking: an interpretable and generalizable machine learning method for MDD discrimination with rsfMRI"></a>Multi-feature concatenation and multi-classifier stacking: an interpretable and generalizable machine learning method for MDD discrimination with rsfMRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09360">http://arxiv.org/abs/2308.09360</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunsong Luo, Wenyu Chen, Ling Zhan, Jiang Qiu, Tao Jia</li>
<li>for: 这个研究旨在提高抑郁疾病的诊断精度，通过利用多种机器学习算法来挖掘resting-state功能MRI中的资讯，并将其 concatenate 成多个特征，然后使用多个分类器进行归类。</li>
<li>methods: 这个研究使用了多种机器学习算法，包括堆叠分类器、支持向量机器学习、以及决策树等，并将其 concatenate 成多个特征。</li>
<li>results: 这个研究获得了96.9%的抑郁疾病诊断精度，较之前的方法有所改善；此外，这个方法在独立的测试和训练数据上也有良好的一致性。<details>
<summary>Abstract</summary>
Major depressive disorder is a serious and heterogeneous psychiatric disorder that needs accurate diagnosis. Resting-state functional MRI (rsfMRI), which captures multiple perspectives on brain structure, function, and connectivity, is increasingly applied in the diagnosis and pathological research of mental diseases. Different machine learning algorithms are then developed to exploit the rich information in rsfMRI and discriminate MDD patients from normal controls. Despite recent advances reported, the discrimination accuracy has room for further improvement. The generalizability and interpretability of the method are not sufficiently addressed either. Here, we propose a machine learning method (MFMC) for MDD discrimination by concatenating multiple features and stacking multiple classifiers. MFMC is tested on the REST-meta-MDD data set that contains 2428 subjects collected from 25 different sites. MFMC yields 96.9% MDD discrimination accuracy, demonstrating a significant improvement over existing methods. In addition, the generalizability of MFMC is validated by the good performance when the training and testing subjects are from independent sites. The use of XGBoost as the meta classifier allows us to probe the decision process of MFMC. We identify 13 feature values related to 9 brain regions including the posterior cingulate gyrus, superior frontal gyrus orbital part, and angular gyrus, which contribute most to the classification and also demonstrate significant differences at the group level. The use of these 13 feature values alone can reach 87% of MFMC's full performance when taking all feature values. These features may serve as clinically useful diagnostic and prognostic biomarkers for mental disorders in the future.
</details>
<details>
<summary>摘要</summary>
major depressive disorder 是一种严重多样的心理疾病，需要准确诊断。它可以通过多种方法进行诊断和病理研究，其中包括功能磁共振成像（rsfMRI）。不同的机器学习算法可以利用rsfMRI中的多个视角和特征来分类患者和正常人。 DESPITE 最近的进步，分类精度仍然有很大的空间提高。此外，方法的普适性和解释性也未得到充分考虑。在这种情况下，我们提出了一种机器学习方法（MFMC），通过 concatenating 多个特征和堆式多个分类器来进行患者分类。MFMC 在 REST-meta-MDD 数据集上进行测试，共计 2428 名参与者，来自 25 个不同的场景。MFMC 的分类精度达 96.9%，表现出了显著的提高。此外，我们还 validate 了 MFMC 的普适性，通过在训练和测试数据集来自独立的场景时进行测试。使用 XGBoost 作为元分类器，我们可以探究 MFMC 的决策过程。我们identified 13 个特征值， relate 到 9 个脑区，包括 posterior cingulate gyrus、superior frontal gyrus orbital part 和 angular gyrus，这些特征值在分类中做出了最大贡献，并在群体水平上显示了显著的差异。这些特征值可能在未来作为心理疾病的临床有用的诊断和预后指标。
</details></li>
</ul>
<hr>
<h2 id="RLIPv2-Fast-Scaling-of-Relational-Language-Image-Pre-training"><a href="#RLIPv2-Fast-Scaling-of-Relational-Language-Image-Pre-training" class="headerlink" title="RLIPv2: Fast Scaling of Relational Language-Image Pre-training"></a>RLIPv2: Fast Scaling of Relational Language-Image Pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09351">http://arxiv.org/abs/2308.09351</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jacobyuan7/rlipv2">https://github.com/jacobyuan7/rlipv2</a></li>
<li>paper_authors: Hangjie Yuan, Shiwei Zhang, Xiang Wang, Samuel Albanie, Yining Pan, Tao Feng, Jianwen Jiang, Dong Ni, Yingya Zhang, Deli Zhao</li>
<li>for: 提高计算机视觉任务中关系理解能力</li>
<li>methods: 引入异ometric语言-图像融合（ALIF）机制，提高早期和深度的模态融合，并使用减少语言编码层来减少训练时间</li>
<li>results: 在人-物互动检测和场景图生成任务上达到了状态级表现，在完全训练、少数shot和零shot设置下都达到了最佳性能， largest RLIPv2 在 HICO-DET 上达到了23.29mAP without fine-tuning，yielded 32.22mAP with just 1% data, and yielded 45.09mAP with 100% data.<details>
<summary>Abstract</summary>
Relational Language-Image Pre-training (RLIP) aims to align vision representations with relational texts, thereby advancing the capability of relational reasoning in computer vision tasks. However, hindered by the slow convergence of RLIPv1 architecture and the limited availability of existing scene graph data, scaling RLIPv1 is challenging. In this paper, we propose RLIPv2, a fast converging model that enables the scaling of relational pre-training to large-scale pseudo-labelled scene graph data. To enable fast scaling, RLIPv2 introduces Asymmetric Language-Image Fusion (ALIF), a mechanism that facilitates earlier and deeper gated cross-modal fusion with sparsified language encoding layers. ALIF leads to comparable or better performance than RLIPv1 in a fraction of the time for pre-training and fine-tuning. To obtain scene graph data at scale, we extend object detection datasets with free-form relation labels by introducing a captioner (e.g., BLIP) and a designed Relation Tagger. The Relation Tagger assigns BLIP-generated relation texts to region pairs, thus enabling larger-scale relational pre-training. Through extensive experiments conducted on Human-Object Interaction Detection and Scene Graph Generation, RLIPv2 shows state-of-the-art performance on three benchmarks under fully-finetuning, few-shot and zero-shot settings. Notably, the largest RLIPv2 achieves 23.29mAP on HICO-DET without any fine-tuning, yields 32.22mAP with just 1% data and yields 45.09mAP with 100% data. Code and models are publicly available at https://github.com/JacobYuan7/RLIPv2.
</details>
<details>
<summary>摘要</summary>
“relational语言-图像预训练（RLIP）目的是将视觉表示与关系文本相对应，从而提高计算机视觉任务中的关系推理能力。然而，由于RLIPv1架构的慢 converges和现有场景图数据的有限性，扩展RLIPv1是困难的。在这篇论文中，我们提出RLIPv2，一种快 converges 的模型，可以将关系预训练扩展到大规模 Pseudo-标注场景图数据。为了快速扩展，RLIPv2引入了非对称语言-图像融合（ALIF）机制，使得更早更深的阻止 Language Encoding 层进行快速融合。ALIF 导致与 RLIPv1 相比，在预训练和精度调整中需要的时间只占一小部分。为了获得场景图数据的扩展，我们将对象检测数据集扩展到具有自由关系标签的场景图数据，并引入了一个captioner（例如 BLIP）和一个设计的关系标签器。关系标签器将 BLIP 生成的关系文本分配给区域对，从而启用大规模关系预训练。通过广泛的实验，RLIPv2在人物-物体互动检测和场景图生成三个标准 benchmark 上显示了状态机器的性能，包括完全 fine-tuning、几何 shot 和零 shot 设置下的性能。尤其是RLIPv2最大模型在 HICO-DET 上达到了 23.29mAP 无需任何 fine-tuning，在 1% 数据上达到了 32.22mAP，在 100% 数据上达到了 45.09mAP。代码和模型在https://github.com/JacobYuan7/RLIPv2 上公开可用。”
</details></li>
</ul>
<hr>
<h2 id="Denoising-diffusion-based-MR-to-CT-image-translation-enables-whole-spine-vertebral-segmentation-in-2D-and-3D-without-manual-annotations"><a href="#Denoising-diffusion-based-MR-to-CT-image-translation-enables-whole-spine-vertebral-segmentation-in-2D-and-3D-without-manual-annotations" class="headerlink" title="Denoising diffusion-based MR to CT image translation enables whole spine vertebral segmentation in 2D and 3D without manual annotations"></a>Denoising diffusion-based MR to CT image translation enables whole spine vertebral segmentation in 2D and 3D without manual annotations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09345">http://arxiv.org/abs/2308.09345</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/robert-graf/readable-conditional-denoising-diffusion">https://github.com/robert-graf/readable-conditional-denoising-diffusion</a></li>
<li>paper_authors: Robert Graf, Joachim Schmitt, Sarah Schlaeger, Hendrik Kristian Möller, Vasiliki Sideri-Lampretsa, Anjany Sekuboyina, Sandro Manuel Krieg, Benedikt Wiestler, Bjoern Menze, Daniel Rueckert, Jan Stefan Kirschke</li>
<li>For: This paper aims to develop a method for translating T1w and T2w MR images into CT images, with a focus on accurately delineating posterior spine structures.* Methods: The authors used a combination of landmark-based registration and image-to-image translation techniques, including Pix2Pix, DDIM, and SynDiff, to align the MR and CT images. They evaluated the performance of these methods using PSNR and Dice scores.* Results: The authors found that 2D paired methods and SynDiff exhibited similar translation performance and Dice scores on paired data, while DDIM image mode achieved the highest image quality. The 3D translation approach outperformed the 2D approach, resulting in improved Dice scores and anatomically accurate segmentations.Here are the three points in Simplified Chinese:* For: 这个论文的目的是开发一种将T1w和T2wMR图像翻译成CT图像的方法，尤其是准确地分割 posterior spine structure.* Methods: 作者使用了一种组合landmark-based registration和图像-to-图像翻译技术，包括Pix2Pix、DDIM和SynDiff，将MR和CT图像对齐。他们使用PSNR和Dice scores评估这些方法的性能。* Results: 作者发现，2D paired方法和SynDiff在对应数据上具有相似的翻译性能和Dice scores，而DDIM图像模式 achieve最高的图像质量。3D翻译方法在高分辨率下对比2D方法表现出了改善的Dice scores和正确的 segmentation.<details>
<summary>Abstract</summary>
Background: Automated segmentation of spinal MR images plays a vital role both scientifically and clinically. However, accurately delineating posterior spine structures presents challenges.   Methods: This retrospective study, approved by the ethical committee, involved translating T1w and T2w MR image series into CT images in a total of n=263 pairs of CT/MR series. Landmark-based registration was performed to align image pairs. We compared 2D paired (Pix2Pix, denoising diffusion implicit models (DDIM) image mode, DDIM noise mode) and unpaired (contrastive unpaired translation, SynDiff) image-to-image translation using "peak signal to noise ratio" (PSNR) as quality measure. A publicly available segmentation network segmented the synthesized CT datasets, and Dice scores were evaluated on in-house test sets and the "MRSpineSeg Challenge" volumes. The 2D findings were extended to 3D Pix2Pix and DDIM.   Results: 2D paired methods and SynDiff exhibited similar translation performance and Dice scores on paired data. DDIM image mode achieved the highest image quality. SynDiff, Pix2Pix, and DDIM image mode demonstrated similar Dice scores (0.77). For craniocaudal axis rotations, at least two landmarks per vertebra were required for registration. The 3D translation outperformed the 2D approach, resulting in improved Dice scores (0.80) and anatomically accurate segmentations in a higher resolution than the original MR image.   Conclusion: Two landmarks per vertebra registration enabled paired image-to-image translation from MR to CT and outperformed all unpaired approaches. The 3D techniques provided anatomically correct segmentations, avoiding underprediction of small structures like the spinous process.
</details>
<details>
<summary>摘要</summary>
Background: 自动化分割脊梗磁共agnetic resonance imaging（MRI）图像对科学和临床来说都具有重要的作用。然而，准确地界定后方脊梗结构具有挑战。Methods: 这是一项回溯性研究，得到了伦敦委员会的批准，涉及翻译T1w和T2w磁共图像序列到CT图像序列的总共n=263对。使用标记为基准的注册方法对图像序列进行了对齐。我们使用“峰信号噪声比”（PSNR）作为质量指标，并对在家测试集和“MRSpineSeg Challenge”volumes上进行了Dice分数的评估。使用公共可用的 segmentation network 对合成的CT数据进行了 segmentation，并评估了Dice分数。Results: 2D paired方法和SynDiff在对应数据上显示了相似的翻译性能和Dice分数。DDIM图像模式实现了最高的图像质量。SynDiff、Pix2Pix和DDIM图像模式在对应数据上都达到了0.77的Dice分数。对横轴向的旋转，至少需要两个标记每个vertebra进行注册。3D翻译超过了2D方法，导致了改进的Dice分数（0.80）和高分辨率的、准确的分割。Conclusion: 使用两个标记每个vertebra的注册可以实现了paired图像到图像的翻译，并超过了所有的不对应方法。3D技术提供了准确的分割，避免了小结构的下预测，如脊梗进程。
</details></li>
</ul>
<hr>
<h2 id="Surprise-machines-revealing-Harvard-Art-Museums’-image-collection"><a href="#Surprise-machines-revealing-Harvard-Art-Museums’-image-collection" class="headerlink" title="Surprise machines: revealing Harvard Art Museums’ image collection"></a>Surprise machines: revealing Harvard Art Museums’ image collection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09343">http://arxiv.org/abs/2308.09343</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dario Rodighiero, Lins Derry, Douglas Duhaime, Jordan Kruguer, Maximilian C. Mueller, Christopher Pietsch, Jeffrey T. Schnapp, Jeff Steward</li>
<li>for: 这个研究是为了开发一个名为“Surprise Machines”的实验 museology 项目，旨在可视化哈佛艺术博物馆的所有图像收藏，以便为访客带来意外的视觉体验。</li>
<li>methods: 这个项目使用了人工智能技术，以显示大量图像并创造对访客的意外感。为此，它设计了一个舞蹈式界面，将访客的运动与图像 коллекцию相连。</li>
<li>results: 这个项目可以创造一种新的视觉体验，让访客感受到图像收藏的意外之处。通过这种方式，访客可以更好地了解和探索图像收藏，并且可以与其他访客分享这种体验。<details>
<summary>Abstract</summary>
Surprise Machines is a project of experimental museology that sets out to visualize the entire image collection of the Harvard Art Museums, intending to open up unexpected vistas on more than 200,000 objects usually inaccessible to visitors. Part of the exhibition Curatorial A(i)gents organized by metaLAB (at) Harvard, the project explores the limits of artificial intelligence to display a large set of images and create surprise among visitors. To achieve such a feeling of surprise, a choreographic interface was designed to connect the audience's movement with several unique views of the collection.
</details>
<details>
<summary>摘要</summary>
很高兴的机器（Surprise Machines）是一个 Harvard Art Museums 的实验 museology 项目，旨在通过可见化整个收藏品库，为访问者提供新的视角和感受。这个项目是metaLAB（at）Harvard组织的 Curatorial A(i)gents 展览的一部分。项目使用人工智能技术，以显示大量图像并创造访问者的感佩感。为了实现这种感动， проек 设计了一个与访客运动相连的chorographic接口，以显示收藏品库的多个独特视角。
</details></li>
</ul>
<hr>
<h2 id="Document-Automation-Architectures-Updated-Survey-in-Light-of-Large-Language-Models"><a href="#Document-Automation-Architectures-Updated-Survey-in-Light-of-Large-Language-Models" class="headerlink" title="Document Automation Architectures: Updated Survey in Light of Large Language Models"></a>Document Automation Architectures: Updated Survey in Light of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09341">http://arxiv.org/abs/2308.09341</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Ahmadi Achachlouei, Omkar Patil, Tarun Joshi, Vijayan N. Nair</li>
<li>for: 本文对当前文档自动化（DA）技术进行了一个全面的评估和概述，尤其是在法律领域的商业解决方案上。</li>
<li>methods: 本文对学术研究中的DA体系和技术进行了一个 clearer定义和特征化，并提出了新的研究机遇，以及在生成AI和大语言模型的进步下的DA领域的发展趋势。</li>
<li>results: 本文对学术研究中的DA体系和技术进行了一个全面的评估和概述，并提出了新的研究机遇。<details>
<summary>Abstract</summary>
This paper surveys the current state of the art in document automation (DA). The objective of DA is to reduce the manual effort during the generation of documents by automatically creating and integrating input from different sources and assembling documents conforming to defined templates. There have been reviews of commercial solutions of DA, particularly in the legal domain, but to date there has been no comprehensive review of the academic research on DA architectures and technologies. The current survey of DA reviews the academic literature and provides a clearer definition and characterization of DA and its features, identifies state-of-the-art DA architectures and technologies in academic research, and provides ideas that can lead to new research opportunities within the DA field in light of recent advances in generative AI and large language models.
</details>
<details>
<summary>摘要</summary>
The current survey of DA reviews academic literature and provides a clearer definition and characterization of DA and its features. It identifies state-of-the-art DA architectures and technologies in academic research and offers ideas for new research opportunities in the DA field, given recent advances in generative AI and large language models.
</details></li>
</ul>
<hr>
<h2 id="Causal-Interpretable-Progression-Trajectory-Analysis-of-Chronic-Disease"><a href="#Causal-Interpretable-Progression-Trajectory-Analysis-of-Chronic-Disease" class="headerlink" title="Causal Interpretable Progression Trajectory Analysis of Chronic Disease"></a>Causal Interpretable Progression Trajectory Analysis of Chronic Disease</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09735">http://arxiv.org/abs/2308.09735</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhoujian Sun, Wenzhuo Zhang, Zhengxing Huang, Nai Ding</li>
<li>for: 预测疾病进程轨迹和诊断决策</li>
<li>methods: 利用机器学习模型和 causal discovery 技术组合 trajectory prediction 和 causal关系挖掘</li>
<li>results: 提供了高度可解释的疾病进程轨迹预测和治疗效果估计，帮助临床决策<details>
<summary>Abstract</summary>
Chronic disease is the leading cause of death, emphasizing the need for accurate prediction of disease progression trajectories and informed clinical decision-making. Machine learning (ML) models have shown promise in this domain by capturing non-linear patterns within patient features. However, existing ML-based models lack the ability to provide causal interpretable predictions and estimate treatment effects, limiting their decision-assisting perspective. In this study, we propose a novel model called causal trajectory prediction (CTP) to tackle the limitation. The CTP model combines trajectory prediction and causal discovery to enable accurate prediction of disease progression trajectories and uncovering causal relationships between features. By incorporating a causal graph into the prediction process, CTP ensures that ancestor features are not influenced by treatment on descendant features, thereby enhancing the interpretability of the model. By estimating the bounds of treatment effects, even in the presence of unmeasured confounders, the CTP provides valuable insights for clinical decision-making. We evaluate the performance of the CTP using simulated and real medical datasets. Experimental results demonstrate that our model achieves satisfactory performance, highlighting its potential to assist clinical decisions.
</details>
<details>
<summary>摘要</summary>
chronic disease 是 death 的主要原因，强调了精准预测疾病进程轨迹和临床决策的需要。机器学习（ML）模型在这个领域中表现出了承诺，通过捕捉非线性 patient 特征中的模式。然而，现有的 ML 基本模型缺乏提供可采用性预测和计算治疗效果的能力，限制了决策帮助的观点。在这种研究中，我们提出了一种新的模型，即 causal trajectory prediction（CTP）模型。CTP 模型结合轨迹预测和 causal discovery，以实现精准预测疾病进程轨迹和探索特征之间的 causal 关系。通过将 causal 图 incorporated 到预测过程中，CTP 模型确保了 ancestor 特征不会由治疗影响 descendant 特征，从而增强了模型的可读性。通过估计治疗效果的范围，即使在不完全掌握的混合变量的情况下，CTP 模型提供了有价值的决策参考。我们使用 simulated 和实际医疗数据进行了实验，结果表明，我们的模型在满足性方面表现出色，强调其在临床决策中的潜在价值。
</details></li>
</ul>
<hr>
<h2 id="Towards-Attack-tolerant-Federated-Learning-via-Critical-Parameter-Analysis"><a href="#Towards-Attack-tolerant-Federated-Learning-via-Critical-Parameter-Analysis" class="headerlink" title="Towards Attack-tolerant Federated Learning via Critical Parameter Analysis"></a>Towards Attack-tolerant Federated Learning via Critical Parameter Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09318">http://arxiv.org/abs/2308.09318</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sungwon-han/fedcpa">https://github.com/sungwon-han/fedcpa</a></li>
<li>paper_authors: Sungwon Han, Sungwon Park, Fangzhao Wu, Sundong Kim, Bin Zhu, Xing Xie, Meeyoung Cha</li>
<li>for: 防止 Federated Learning 系统受到毒 poisoning 攻击</li>
<li>methods: 提出了一种新的防御策略：FedCPA (Federated learning with Critical Parameter Analysis)，基于本地模型中critical parameter的观察，对恶意本地模型进行检测和排除</li>
<li>results: 对多个数据集和不同的攻击场景进行实验，表明我们的模型在防止毒 poisoning 攻击方面表现更好，比较现有的防御策略。<details>
<summary>Abstract</summary>
Federated learning is used to train a shared model in a decentralized way without clients sharing private data with each other. Federated learning systems are susceptible to poisoning attacks when malicious clients send false updates to the central server. Existing defense strategies are ineffective under non-IID data settings. This paper proposes a new defense strategy, FedCPA (Federated learning with Critical Parameter Analysis). Our attack-tolerant aggregation method is based on the observation that benign local models have similar sets of top-k and bottom-k critical parameters, whereas poisoned local models do not. Experiments with different attack scenarios on multiple datasets demonstrate that our model outperforms existing defense strategies in defending against poisoning attacks.
</details>
<details>
<summary>摘要</summary>
federated learning 是一种用于共享模型的协同训练方式，无需客户端对彼此分享私人数据。 federated learning 系统容易受到毒素攻击，当恶意客户端将 false 更新发送到中央服务器时。 现有的防御策略在非标一致数据设置下无效。 这篇论文提出了一种新的防御策略，即 FedCPA (federated learning with Critical Parameter Analysis)。我们的攻击忍受汇总方法基于本地模型的准确参数集的观察，坏做的本地模型与benign的本地模型之间存在显著差异。 对多个数据集和不同的攻击场景进行实验，我们的模型比现有的防御策略更高效地防止毒素攻击。
</details></li>
</ul>
<hr>
<h2 id="Path-Signatures-for-Seizure-Forecasting"><a href="#Path-Signatures-for-Seizure-Forecasting" class="headerlink" title="Path Signatures for Seizure Forecasting"></a>Path Signatures for Seizure Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09312">http://arxiv.org/abs/2308.09312</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonas F. Haderlein, Andre D. H. Peterson, Parvin Zarei Eskikand, Mark J. Cook, Anthony N. Burkitt, Iven M. Y. Mareels, David B. Grayden</li>
<li>for: This paper aims to automatically discover and quantify statistical features (biomarkers) that can be used to forecast seizures in a patient-specific way.</li>
<li>methods: The authors use existing and novel feature extraction algorithms, including the path signature, a recent development in time series analysis. They use statistical classification algorithms with in-built subset selection to discern time series with and without an impending seizure while selecting only a small number of relevant features.</li>
<li>results: The study may be seen as a step towards a generalisable pattern recognition pipeline for time series in a broader context.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文目标是自动发现和评估病人特定的预测癫痫症状的统计特征（生物标志）。</li>
<li>methods: 作者使用现有和新的特征提取算法，包括时间序列分析中最新的路径签名。他们使用统计分类算法和内置的子集选择来分辨病人的时间序列中是否有预期的癫痫症状，并且选择只有一小部分相关的特征。</li>
<li>results: 这种研究可能是对时间序列Pattern recognition pipeline的一个普适的步骤。<details>
<summary>Abstract</summary>
Forecasting the state of a system from an observed time series is the subject of research in many domains, such as computational neuroscience. Here, the prediction of epileptic seizures from brain measurements is an unresolved problem. There are neither complete models describing underlying brain dynamics, nor do individual patients exhibit a single seizure onset pattern, which complicates the development of a `one-size-fits-all' solution. Based on a longitudinal patient data set, we address the automated discovery and quantification of statistical features (biomarkers) that can be used to forecast seizures in a patient-specific way. We use existing and novel feature extraction algorithms, in particular the path signature, a recent development in time series analysis. Of particular interest is how this set of complex, nonlinear features performs compared to simpler, linear features on this task. Our inference is based on statistical classification algorithms with in-built subset selection to discern time series with and without an impending seizure while selecting only a small number of relevant features. This study may be seen as a step towards a generalisable pattern recognition pipeline for time series in a broader context.
</details>
<details>
<summary>摘要</summary>
预测系统的状态从观察时序序列是多个领域的研究主题，如计算神经科学。在这里，预测脑测量中的癫病症发生是一个未解决的问题。没有完整的模型描述脑动力学，每个患者都不会表现出唯一的发病开始模式，这使得开发一个“一size-fits-all”解决方案变得困难。基于长期患者数据集，我们研究自动发现和评估统计特征（生物标志），以便预测患者特定的癫病症。我们使用现有和新的特征提取算法，特别是路径签名，这是时间序列分析中最近的发展。我们的推断基于统计分类算法，并使用内置子集选择来分辨时间序列中有无危机发生，同时选择只有少量相关的特征。这种研究可能是一步向更通用的时间序列模式识别管道的发展。
</details></li>
</ul>
<hr>
<h2 id="Variance-reduction-techniques-for-stochastic-proximal-point-algorithms"><a href="#Variance-reduction-techniques-for-stochastic-proximal-point-algorithms" class="headerlink" title="Variance reduction techniques for stochastic proximal point algorithms"></a>Variance reduction techniques for stochastic proximal point algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09310">http://arxiv.org/abs/2308.09310</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheik Traoré, Vassilis Apidopoulos, Saverio Salzo, Silvia Villa</li>
<li>for: 本研究目的是为了提出variance reduction技术来提高现有抽象函数的最小值算法性能。</li>
<li>methods: 本文使用了stochastic proximal point算法，并研究了这些算法的variance reduction版本。</li>
<li>results: 本文提供了几种convergence result，包括iterates和目标函数值的convergence rate。在Polyak-{\L}ojasiewicz条件下，我们得到了iterates和函数值的线性增长率。数据实验表明，proximal variance reduction方法比其梯度对应方法更稳定，尤其是对于步长的选择。<details>
<summary>Abstract</summary>
In the context of finite sums minimization, variance reduction techniques are widely used to improve the performance of state-of-the-art stochastic gradient methods. Their practical impact is clear, as well as their theoretical properties. Stochastic proximal point algorithms have been studied as an alternative to stochastic gradient algorithms since they are more stable with respect to the choice of the stepsize but a proper variance reduced version is missing. In this work, we propose the first study of variance reduction techniques for stochastic proximal point algorithms. We introduce a stochastic proximal version of SVRG, SAGA, and some of their variants for smooth and convex functions. We provide several convergence results for the iterates and the objective function values. In addition, under the Polyak-{\L}ojasiewicz (PL) condition, we obtain linear convergence rates for the iterates and the function values. Our numerical experiments demonstrate the advantages of the proximal variance reduction methods over their gradient counterparts, especially about the stability with respect to the choice of the step size.
</details>
<details>
<summary>摘要</summary>
在finite sums最小化上， variance reduction 技术广泛应用于提高现有的随机梯度方法性能。其实际影响明确，同时其理论性质也很清晰。随机 proximal 点算法被视为随机梯度算法的替代方案，因为它们在步长选择方面更加稳定，但是一个适当的 variance reduced 版本缺失。在这种工作中，我们提出了随机 proximal 版本的 SVRG、SAGA 和一些其他变种，用于凸函数和 convex 函数。我们提供了迭代点和目标函数值的多个收敛结果。此外，在 Polyak-{\L}ojasiewicz（PL）条件下，我们获得了迭代点和函数值的线性收敛率。我们的数值实验表明 proximal variance reduction 方法在步长选择方面比随机梯度方法更稳定，特别是在随机梯度方法的稳定性方面。
</details></li>
</ul>
<hr>
<h2 id="Meta-learning-enhanced-next-POI-recommendation-by-leveraging-check-ins-from-auxiliary-cities"><a href="#Meta-learning-enhanced-next-POI-recommendation-by-leveraging-check-ins-from-auxiliary-cities" class="headerlink" title="Meta-learning enhanced next POI recommendation by leveraging check-ins from auxiliary cities"></a>Meta-learning enhanced next POI recommendation by leveraging check-ins from auxiliary cities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09309">http://arxiv.org/abs/2308.09309</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/oli-wang/merec">https://github.com/oli-wang/merec</a></li>
<li>paper_authors: Jinze Wang, Lu Zhang, Zhu Sun, Yew-Soon Ong</li>
<li>for: 提出一种基于meta-学习的下一个 POI 推荐方法，以解决城市级用户历史检查入数据的稀缺性问题。</li>
<li>methods: 利用城市级用户历史检查入数据和相关城市的检查入数据，通过meta-学习方法捕捉用户偏好，并通过城市级协调策略把更有关系的知识从更相关的城市传递给目标城市。</li>
<li>results: 经过广泛的实验 validate 了提出的方法的优越性， compared with 现有的算法。<details>
<summary>Abstract</summary>
Most existing point-of-interest (POI) recommenders aim to capture user preference by employing city-level user historical check-ins, thus facilitating users' exploration of the city. However, the scarcity of city-level user check-ins brings a significant challenge to user preference learning. Although prior studies attempt to mitigate this challenge by exploiting various context information, e.g., spatio-temporal information, they ignore to transfer the knowledge (i.e., common behavioral pattern) from other relevant cities (i.e., auxiliary cities). In this paper, we investigate the effect of knowledge distilled from auxiliary cities and thus propose a novel Meta-learning Enhanced next POI Recommendation framework (MERec). The MERec leverages the correlation of check-in behaviors among various cities into the meta-learning paradigm to help infer user preference in the target city, by holding the principle of "paying more attention to more correlated knowledge". Particularly, a city-level correlation strategy is devised to attentively capture common patterns among cities, so as to transfer more relevant knowledge from more correlated cities. Extensive experiments verify the superiority of the proposed MERec against state-of-the-art algorithms.
</details>
<details>
<summary>摘要</summary>
现有大多数点位推荐器都 aim to capture用户偏好通过使用城市级别的用户历史检查入，从而促进用户对城市的探索。然而，城市级别的用户检查入的缺乏引起了significant Challenge to user preference learning。 although prior studies attempt to mitigate this challenge by exploiting various context information, such as spatio-temporal information, they ignore to transfer the knowledge (i.e., common behavioral pattern) from other relevant cities (i.e., auxiliary cities).在本文中，我们investigate the effect of knowledge distilled from auxiliary cities and thus propose a novel Meta-learning Enhanced next POI Recommendation framework (MERec). MERec leverages the correlation of check-in behaviors among various cities into the meta-learning paradigm to help infer user preference in the target city, by holding the principle of "paying more attention to more correlated knowledge". particularly, a city-level correlation strategy is devised to attentively capture common patterns among cities, so as to transfer more relevant knowledge from more correlated cities.extensive experiments verify the superiority of the proposed MERec against state-of-the-art algorithms.
</details></li>
</ul>
<hr>
<h2 id="Online-Class-Incremental-Learning-on-Stochastic-Blurry-Task-Boundary-via-Mask-and-Visual-Prompt-Tuning"><a href="#Online-Class-Incremental-Learning-on-Stochastic-Blurry-Task-Boundary-via-Mask-and-Visual-Prompt-Tuning" class="headerlink" title="Online Class Incremental Learning on Stochastic Blurry Task Boundary via Mask and Visual Prompt Tuning"></a>Online Class Incremental Learning on Stochastic Blurry Task Boundary via Mask and Visual Prompt Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09303">http://arxiv.org/abs/2308.09303</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/moonjunyyy/si-blurry">https://github.com/moonjunyyy/si-blurry</a></li>
<li>paper_authors: Jun-Yeong Moon, Keon-Hee Park, Jung Uk Kim, Gyeong-Moon Park</li>
<li>for: 本研究旨在 Addressing the challenges of continual learning in real-world scenarios, where the number of input data and tasks is constantly changing in a statistical way.</li>
<li>methods: 我们提出了一种新的 Stochastic incremental Blurry task boundary scenario (Si-Blurry)，并引入了 Mask and Visual Prompt tuning (MVP) 方法来解决inter-和 intra-task 忘记和类偏度问题。MVP方法包括novel instance-wise logit masking和contrastive visual prompt tuning loss，以及一种新的类相似性基于的 focal loss和 adaptive feature scaling。</li>
<li>results: 我们的实验表明，Compared with existing state-of-the-art methods, our proposed MVP method significantly outperforms in our challenging Si-Blurry scenario.<details>
<summary>Abstract</summary>
Continual learning aims to learn a model from a continuous stream of data, but it mainly assumes a fixed number of data and tasks with clear task boundaries. However, in real-world scenarios, the number of input data and tasks is constantly changing in a statistical way, not a static way. Although recently introduced incremental learning scenarios having blurry task boundaries somewhat address the above issues, they still do not fully reflect the statistical properties of real-world situations because of the fixed ratio of disjoint and blurry samples. In this paper, we propose a new Stochastic incremental Blurry task boundary scenario, called Si-Blurry, which reflects the stochastic properties of the real-world. We find that there are two major challenges in the Si-Blurry scenario: (1) inter- and intra-task forgettings and (2) class imbalance problem. To alleviate them, we introduce Mask and Visual Prompt tuning (MVP). In MVP, to address the inter- and intra-task forgetting issues, we propose a novel instance-wise logit masking and contrastive visual prompt tuning loss. Both of them help our model discern the classes to be learned in the current batch. It results in consolidating the previous knowledge. In addition, to alleviate the class imbalance problem, we introduce a new gradient similarity-based focal loss and adaptive feature scaling to ease overfitting to the major classes and underfitting to the minor classes. Extensive experiments show that our proposed MVP significantly outperforms the existing state-of-the-art methods in our challenging Si-Blurry scenario.
</details>
<details>
<summary>摘要</summary>
continuous learning旨在从连续的数据流中学习模型，但它主要假设 fixes number of data和任务，并且这些任务的boundary是清晰的。然而，在实际情况下，输入数据和任务的数量不断改变，而且这些变化是以 Statistical way进行的，而不是静止的way。虽然最近引入的增量学习enario中有些地方处理了这些问题，但它们仍然不能完全反映实际情况中的Statistical properties。因此，我们在这篇论文中提出了一个新的Stochastic增量Blurry任务boundaryscenario，称为Si-Blurry。我们发现这个scenario中有两个主要挑战：（1）inter-和intra-task forgetting，以及（2）class imbalance问题。为了解决这些问题，我们引入了Mask和Visual Prompt tuning（MVP）。在MVP中，为了解决inter-和intra-task forgetting问题，我们提出了一个novel的instance-wise logit masking和contrastive visual prompt tuning损失。这些损失帮助我们的模型在当前批次中识别需要学习的类别。因此，它将有助于我们的模型固化先前的知识。此外，为了解决class imbalance问题，我们引入了一个新的gradient similarity-based focal loss和adaptive feature scaling。这些方法帮助我们的模型免于主要类别的过拟合和次要类别的下降。实验结果显示，我们的提出的MVP方法在我们的挑战性Si-Blurryscenario中得到了很好的表现，较以前的state-of-the-art方法有所进步。
</details></li>
</ul>
<hr>
<h2 id="Learning-Reward-Machines-through-Preference-Queries-over-Sequences"><a href="#Learning-Reward-Machines-through-Preference-Queries-over-Sequences" class="headerlink" title="Learning Reward Machines through Preference Queries over Sequences"></a>Learning Reward Machines through Preference Queries over Sequences</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09301">http://arxiv.org/abs/2308.09301</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eric Hsiung, Joydeep Biswas, Swarat Chaudhuri</li>
<li>for: 本研究旨在学习具有复杂动作序列的任务时，使用实际弱反馈来学习奖励机。</li>
<li>methods: 本研究提出了一种名为REMAP的算法，用于从偏好中学习奖励机，并提供了正确性和结束 garanties。 REMAP使用偏好查询而不是L*算法中的会员查询，并利用符号观察表以及卷积和约束解决方法缩小假设奖励机搜索空间。</li>
<li>results: 本研究提供了对REMAP算法的正确性和终止性证明，以及实验证明了REMAP算法在一个相对准确但不准确的教师下的正确性和偏差。<details>
<summary>Abstract</summary>
Reward machines have shown great promise at capturing non-Markovian reward functions for learning tasks that involve complex action sequencing. However, no algorithm currently exists for learning reward machines with realistic weak feedback in the form of preferences. We contribute REMAP, a novel algorithm for learning reward machines from preferences, with correctness and termination guarantees. REMAP introduces preference queries in place of membership queries in the L* algorithm, and leverages a symbolic observation table along with unification and constraint solving to narrow the hypothesis reward machine search space. In addition to the proofs of correctness and termination for REMAP, we present empirical evidence measuring correctness: how frequently the resulting reward machine is isomorphic under a consistent yet inexact teacher, and the regret between the ground truth and learned reward machines.
</details>
<details>
<summary>摘要</summary>
奖励机器有非常良好的承诺，用于捕捉复杂的动作序列学习任务中的非马普朗奖函数。然而，目前没有一种算法可以学习奖励机器的实际弱反馈，即偏好。我们贡献了一种新的算法，名为REMAP，可以从偏好中学习奖励机器，并提供正确性和结束性保证。REMAP在L*算法中引入偏好查询，并利用符号观察表和约束解决方法缩小偶极机器搜索空间。此外，我们还提供了REMAP的正确性和结束性证明，以及实验证明：如何频繁地使用一致却不准确的教师来证明奖励机器的同构性，以及奖励机器和真实奖励机器之间的偏差。
</details></li>
</ul>
<hr>
<h2 id="CARLA-A-Self-supervised-Contrastive-Representation-Learning-Approach-for-Time-Series-Anomaly-Detection"><a href="#CARLA-A-Self-supervised-Contrastive-Representation-Learning-Approach-for-Time-Series-Anomaly-Detection" class="headerlink" title="CARLA: A Self-supervised Contrastive Representation Learning Approach for Time Series Anomaly Detection"></a>CARLA: A Self-supervised Contrastive Representation Learning Approach for Time Series Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09296">http://arxiv.org/abs/2308.09296</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zahra Zamanzadeh Darban, Geoffrey I. Webb, Shirui Pan, Mahsa Salehi</li>
<li>for: 本研究旨在提出一种基于自适应对比学习的时间序列异常检测方法，以便更好地识别时间序列数据中的异常 patterns。</li>
<li>methods: 本方法使用对比表示学习，通过学习时间序列窗口中的相似和不相似表示，生成高效的异常检测模型。</li>
<li>results: 经过广泛的实验 validate，本方法在7个标准的真实世界时间序列异常检测基准数据集上达到了F1和AU-PR的超越现有状态艺的结果。<details>
<summary>Abstract</summary>
We introduce a Self-supervised Contrastive Representation Learning Approach for Time Series Anomaly Detection (CARLA), an innovative end-to-end self-supervised framework carefully developed to identify anomalous patterns in both univariate and multivariate time series data. By taking advantage of contrastive representation learning, We introduce an innovative end-to-end self-supervised deep learning framework carefully developed to identify anomalous patterns in both univariate and multivariate time series data. By taking advantage of contrastive representation learning, CARLA effectively generates robust representations for time series windows. It achieves this by 1) learning similar representations for temporally close windows and dissimilar representations for windows and their equivalent anomalous windows and 2) employing a self-supervised approach to classify normal/anomalous representations of windows based on their nearest/furthest neighbours in the representation space. Most of the existing models focus on learning normal behaviour. The normal boundary is often tightly defined, which can result in slight deviations being classified as anomalies, resulting in a high false positive rate and limited ability to generalise normal patterns. CARLA's contrastive learning methodology promotes the production of highly consistent and discriminative predictions, thereby empowering us to adeptly address the inherent challenges associated with anomaly detection in time series data. Through extensive experimentation on 7 standard real-world time series anomaly detection benchmark datasets, CARLA demonstrates F1 and AU-PR superior to existing state-of-the-art results. Our research highlights the immense potential of contrastive representation learning in advancing the field of time series anomaly detection, thus paving the way for novel applications and in-depth exploration in this domain.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种自然Language Modeling自我超vised representation learning方法（CARLA），用于时间序列异常检测。这是一种创新的端到端自我超vised深度学习框架，通过利用对比表示学习，以便在时间序列数据中检测异常patterns。我们通过1）学习近似的表示 для时间相近的窗口和不相似的表示 для窗口和其等异常窗口，以及2）使用自我超vised的方法来分类normal/异常的表示窗口基于其最近/最远的邻居在表示空间中来实现。大多数现有的模型都是学习正常行为，正常边界通常是非常紧张的，这可能导致非常小的偏差被分类为异常，从而导致高的假阳性率和有限的泛化能力。CARLA的对比学习方法ологиy可以生成高度一致和抑制的预测，因此可以有效地解决时间序列异常检测中的内在挑战。经过对7个标准实际时间序列异常检测benchmark数据集的广泛实验，CARLA的F1和AU-PR超过现有状态的最佳结果。我们的研究表明，对比表示学习在时间序列异常检测中具有极大的潜力，因此可以为这个领域开拓新的应用和深入探索。
</details></li>
</ul>
<hr>
<h2 id="How-important-are-specialized-transforms-in-Neural-Operators"><a href="#How-important-are-specialized-transforms-in-Neural-Operators" class="headerlink" title="How important are specialized transforms in Neural Operators?"></a>How important are specialized transforms in Neural Operators?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09293">http://arxiv.org/abs/2308.09293</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Ritam-M/LearnableTransformsNO">https://github.com/Ritam-M/LearnableTransformsNO</a></li>
<li>paper_authors: Ritam Majumdar, Shirish Karande, Lovekesh Vig</li>
<li>for: 本研究探讨了基于Transform-based Neural Operators的Physical System Simulation中的transform层的重要性。</li>
<li>methods: 本研究使用了 Replace all transform layers with learnable linear layers的方法来测试transform层的重要性。</li>
<li>results:  surprisingly, the results show that linear layers can provide performance comparable to the best-known transform-based layers, with a compute time advantage as well.<details>
<summary>Abstract</summary>
Simulating physical systems using Partial Differential Equations (PDEs) has become an indispensible part of modern industrial process optimization. Traditionally, numerical solvers have been used to solve the associated PDEs, however recently Transform-based Neural Operators such as the Fourier Neural Operator and Wavelet Neural Operator have received a lot of attention for their potential to provide fast solutions for systems of PDEs. In this work, we investigate the importance of the transform layers to the reported success of transform based neural operators. In particular, we record the cost in terms of performance, if all the transform layers are replaced by learnable linear layers. Surprisingly, we observe that linear layers suffice to provide performance comparable to the best-known transform-based layers and seem to do so with a compute time advantage as well. We believe that this observation can have significant implications for future work on Neural Operators, and might point to other sources of efficiencies for these architectures.
</details>
<details>
<summary>摘要</summary>
使用部分泛函方程（PDEs）模拟物理系统已成为现代工业过程优化的不可或缺的一部分。在传统上，数值分析器被用来解决相关的PDEs，但在最近，基于变换的神经网络运算符如傅立叶 нейронOperator和波幅 нейронOperator在解决系统PDEs方面受到了广泛的关注。在这项工作中，我们调查了变换层的重要性，特别是在报道最佳性能的情况下，所有变换层都被替换为学习的线性层。我们发现了一个意外的现象：线性层可以提供与最佳变换层相当的性能，并且在计算时间方面也有一定的优势。我们认为这一观察可能对未来神经网络架构的发展产生重要的影响，并可能指向其他效率来源。
</details></li>
</ul>
<hr>
<h2 id="Graph-based-Alignment-and-Uniformity-for-Recommendation"><a href="#Graph-based-Alignment-and-Uniformity-for-Recommendation" class="headerlink" title="Graph-based Alignment and Uniformity for Recommendation"></a>Graph-based Alignment and Uniformity for Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09292">http://arxiv.org/abs/2308.09292</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yangliangwei/graphau">https://github.com/yangliangwei/graphau</a></li>
<li>paper_authors: Liangwei Yang, Zhiwei Liu, Chen Wang, Mingdai Yang, Xiaolong Liu, Jing Ma, Philip S. Yu</li>
<li>for:  Collaborative filtering-based recommender systems (RecSys)</li>
<li>methods:  graph-based alignment and uniformity (GraphAU) approach, neighborhood aggregator, layer-wise alignment pooling module</li>
<li>results:  significantly alleviates the sparsity issue, achieves state-of-the-art performanceHere’s the simplified Chinese text:</li>
<li>for: collaborative filtering-based recommender systems (RecSys)</li>
<li>methods: 基于图的启发和均匀性（GraphAU）方法，包括邻居聚合器和层次启发聚合模块</li>
<li>results: 可以有效解决稀疏问题，实现了最佳性能<details>
<summary>Abstract</summary>
Collaborative filtering-based recommender systems (RecSys) rely on learning representations for users and items to predict preferences accurately. Representation learning on the hypersphere is a promising approach due to its desirable properties, such as alignment and uniformity. However, the sparsity issue arises when it encounters RecSys. To address this issue, we propose a novel approach, graph-based alignment and uniformity (GraphAU), that explicitly considers high-order connectivities in the user-item bipartite graph. GraphAU aligns the user/item embedding to the dense vector representations of high-order neighbors using a neighborhood aggregator, eliminating the need to compute the burdensome alignment to high-order neighborhoods individually. To address the discrepancy in alignment losses, GraphAU includes a layer-wise alignment pooling module to integrate alignment losses layer-wise. Experiments on four datasets show that GraphAU significantly alleviates the sparsity issue and achieves state-of-the-art performance. We open-source GraphAU at https://github.com/YangLiangwei/GraphAU.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="HyperLoRA-for-PDEs"><a href="#HyperLoRA-for-PDEs" class="headerlink" title="HyperLoRA for PDEs"></a>HyperLoRA for PDEs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09290">http://arxiv.org/abs/2308.09290</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ritam Majumdar, Vishal Jadhav, Anirudh Deodhar, Shirish Karande, Lovekesh Vig, Venkataramana Runkana</li>
<li>for: 解决参数化积分方程的解的难题，提高神经网络的泛化能力。</li>
<li>methods: 使用Hypernetworks技术，将每层base网络分解成低维度矩阵，并使用这些矩阵来预测神经网络的参数。</li>
<li>results: 在各种基础网络和任务下，LoRA-based HyperPINN训练可以快速学习参数化积分方程的解，并且可以在参数数量减少8倍的情况下维持准确性。<details>
<summary>Abstract</summary>
Physics-informed neural networks (PINNs) have been widely used to develop neural surrogates for solutions of Partial Differential Equations. A drawback of PINNs is that they have to be retrained with every change in initial-boundary conditions and PDE coefficients. The Hypernetwork, a model-based meta learning technique, takes in a parameterized task embedding as input and predicts the weights of PINN as output. Predicting weights of a neural network however, is a high-dimensional regression problem, and hypernetworks perform sub-optimally while predicting parameters for large base networks. To circumvent this issue, we use a low ranked adaptation (LoRA) formulation to decompose every layer of the base network into low-ranked tensors and use hypernetworks to predict the low-ranked tensors. Despite the reduced dimensionality of the resulting weight-regression problem, LoRA-based Hypernetworks violate the underlying physics of the given task. We demonstrate that the generalization capabilities of LoRA-based hypernetworks drastically improve when trained with an additional physics-informed loss component (HyperPINN) to satisfy the governing differential equations. We observe that LoRA-based HyperPINN training allows us to learn fast solutions for parameterized PDEs like Burger's equation and Navier Stokes: Kovasznay flow, while having an 8x reduction in prediction parameters on average without compromising on accuracy when compared to all other baselines.
</details>
<details>
<summary>摘要</summary>
物理学信息感知神经网络（PINNs）广泛应用于解决分数方程的解的神经替换器。PINNs的缺点是它们每次初始边界条件和微分方程系数改变时需要重新训练。 Hypernetwork，一种模型基于元学习技术，输入一个 parameterized task embedding，并预测 PINN 的重量。然而，预测神经网络参数是一个高维度回归问题，Hypernetworks 在预测基础网络参数时表现不佳。为了解决这个问题，我们使用 low-ranked adaptation（LoRA）形式将每层基础网络分解成低维度矩阵，并使用 Hypernetworks 预测低维度矩阵。尽管LoRA-based Hypernetworks 的结果维度降低，但是LoRA-based Hypernetworks 仍然违反了给定任务的物理基础。我们示出，在 HyperPINN 中添加物理学信息感知损失函数可以提高 LoRA-based Hypernetworks 的泛化能力。我们观察到，LoRA-based HyperPINN 训练可以快速解决参数化的微分方程，如布尔格方程和奈尔-斯托克斯：kovasznay 流动，而无需妥协准确性。在 average 情况下，LoRA-based HyperPINN 的预测参数数量减少了 8 倍，而不会影响准确性。
</details></li>
</ul>
<hr>
<h2 id="A-hybrid-Decoder-DeepONet-operator-regression-framework-for-unaligned-observation-data"><a href="#A-hybrid-Decoder-DeepONet-operator-regression-framework-for-unaligned-observation-data" class="headerlink" title="A hybrid Decoder-DeepONet operator regression framework for unaligned observation data"></a>A hybrid Decoder-DeepONet operator regression framework for unaligned observation data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09274">http://arxiv.org/abs/2308.09274</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cb-sjtu/decoder_deeponet">https://github.com/cb-sjtu/decoder_deeponet</a></li>
<li>paper_authors: Bo Chen, Chenyu Wang, Weipeng Li, Haiyang Fu</li>
<li>for: 处理不对齐的观测数据</li>
<li>methods: 提议了一种混合Decoder-DeepONet算法树立阶段，以及一种Multi-Decoder-DeepONet算法，通过使用训练数据的平均场来进行输入增强</li>
<li>results: 通过两个实验（Darcy问题和流场附近的飞机翼）， validate了提议的方法在处理不对齐观测数据方面的效率和准确性，并显示了这些方法在改进预测精度方面的潜力<details>
<summary>Abstract</summary>
Deep neural operators (DNOs) have been utilized to approximate nonlinear mappings between function spaces. However, DNOs face the challenge of increased dimensionality and computational cost associated with unaligned observation data. In this study, we propose a hybrid Decoder-DeepONet operator regression framework to handle unaligned data effectively. Additionally, we introduce a Multi-Decoder-DeepONet, which utilizes an average field of training data as input augmentation. The consistencies of the frameworks with the operator approximation theory are provided, on the basis of the universal approximation theorem. Two numerical experiments, Darcy problem and flow-field around an airfoil, are conducted to validate the efficiency and accuracy of the proposed methods. Results illustrate the advantages of Decoder-DeepONet and Multi-Decoder-DeepONet in handling unaligned observation data and showcase their potentials in improving prediction accuracy.
</details>
<details>
<summary>摘要</summary>
深度神经操作员 (DNO) 已被应用于函数空间中的非线性映射的近似。然而，DNO 面临不同观察数据的维度和计算成本的增加。在本研究中，我们提议了一种混合 Decoder-DeepONet 算法批处不同观察数据的批处方法。此外，我们还介绍了 Multi-Decoder-DeepONet，它利用训练数据的平均场作为输入增强。我们提供了基于 оператор近似理论的一致性，以确保方法的可靠性。在 Darcy 问题和风流附近的飞机翼上进行了两个数值实验，以验证提议的方法的效率和准确性。结果表明 Decoder-DeepONet 和 Multi-Decoder-DeepONet 可以有效地处理不同观察数据，并且在提高预测精度方面具有潜力。
</details></li>
</ul>
<hr>
<h2 id="Multi-Task-Pseudo-Label-Learning-for-Non-Intrusive-Speech-Quality-Assessment-Model"><a href="#Multi-Task-Pseudo-Label-Learning-for-Non-Intrusive-Speech-Quality-Assessment-Model" class="headerlink" title="Multi-Task Pseudo-Label Learning for Non-Intrusive Speech Quality Assessment Model"></a>Multi-Task Pseudo-Label Learning for Non-Intrusive Speech Quality Assessment Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09262">http://arxiv.org/abs/2308.09262</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryandhimas E. Zezario, Bo-Ren Brian Bai, Chiou-Shann Fuh, Hsin-Min Wang, Yu Tsao</li>
<li>for: 这种研究旨在开发一种非侵入式语音质量评估模型，使用多任务pseudo标签学习（MPL）技术。</li>
<li>methods: 该研究使用MPL技术，包括两个阶段：首先，从预训练模型中获取pseudo标签分数；其次，进行多任务学习。 selects three 3QUEST metricss：Speech-MOS（S-MOS）、Noise-MOS（N-MOS）和General-MOS（G-MOS）作为真实标签。使用预训练MOSA-Net模型来估算三个pseudo标签：语音质量评估（PESQ）、短时间对象智能度（STOI）和语音质量指数（SDI）。然后，使用多任务学习阶段来训练MTQ-Net模型（多目标语音质量评估网络）。该模型通过 combining Loss supervision（基于真实标签和pseudo标签之间的差异）和 Loss semi-supervision（基于pseudo标签和真实标签之间的差异）来优化。</li>
<li>results: 实验结果表明，MPL比训练模型从scratch和使用知识传递机制来训练模型都有优势。其次，使用柯伯损失函数来计算损失函数可以提高MTQ-Net模型的预测能力。最后，MTQ-Net模型使用MPL方法 exhibits higher overall prediction capabilities compared to other SSL-based speech assessment models。<details>
<summary>Abstract</summary>
This study introduces multi-task pseudo-label (MPL) learning for a non-intrusive speech quality assessment model. MPL consists of two stages which are obtaining pseudo-label scores from a pretrained model and performing multi-task learning. The 3QUEST metrics, namely Speech-MOS (S-MOS), Noise-MOS (N-MOS), and General-MOS (G-MOS) are selected as the primary ground-truth labels. Additionally, the pretrained MOSA-Net model is utilized to estimate three pseudo-labels: perceptual evaluation of speech quality (PESQ), short-time objective intelligibility (STOI), and speech distortion index (SDI). Multi-task learning stage of MPL is then employed to train the MTQ-Net model (multi-target speech quality assessment network). The model is optimized by incorporating Loss supervision (derived from the difference between the estimated score and the real ground-truth labels) and Loss semi-supervision (derived from the difference between the estimated score and pseudo-labels), where Huber loss is employed to calculate the loss function. Experimental results first demonstrate the advantages of MPL compared to training the model from scratch and using knowledge transfer mechanisms. Secondly, the benefits of Huber Loss in improving the prediction model of MTQ-Net are verified. Finally, the MTQ-Net with the MPL approach exhibits higher overall prediction capabilities when compared to other SSL-based speech assessment models.
</details>
<details>
<summary>摘要</summary>
In the multi-task learning stage of MPL, the MTQ-Net model (multi-target speech quality assessment network) is trained using Loss supervision and Loss semi-supervision. The Loss supervision is derived from the difference between the estimated score and the real ground-truth labels, while the Loss semi-supervision is derived from the difference between the estimated score and the pseudo-labels. The Huber loss is employed to calculate the loss function.Experimental results show that the MPL approach outperforms training the model from scratch and using knowledge transfer mechanisms. Additionally, the use of Huber loss in the MPL approach improves the prediction capabilities of the MTQ-Net model. Compared to other speech assessment models based on semi-supervised learning (SSL), the MTQ-Net with the MPL approach exhibits higher overall prediction capabilities.
</details></li>
</ul>
<hr>
<h2 id="Distribution-shift-mitigation-at-test-time-with-performance-guarantees"><a href="#Distribution-shift-mitigation-at-test-time-with-performance-guarantees" class="headerlink" title="Distribution shift mitigation at test time with performance guarantees"></a>Distribution shift mitigation at test time with performance guarantees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09259">http://arxiv.org/abs/2308.09259</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rui Ding, Jielong Yang, Feng Ji, Xionghu Zhong, Linbo Xie<br>for: 这个研究旨在提高Graph Neural Networks (GNNs)的测试性能，解决由于样本选择不适切和训练数据有限而导致的分布shift问题。methods: 我们提出了一个缩寸的FR-GNN框架，通过将训练好的GNN的输出和输入建立映射关系，以取得类别表示vector，然后使用这些vector来重建节点的特征。这些重建的节点特征可以直接用于测试已经训练好的模型，从而减少分布shift和提高测试性能。results: 我们提供了理论保证，并进行了多种公共数据集的实验。实验结果显示FR-GNN在与主流方法比较之下，具有更高的测试性能。<details>
<summary>Abstract</summary>
Due to inappropriate sample selection and limited training data, a distribution shift often exists between the training and test sets. This shift can adversely affect the test performance of Graph Neural Networks (GNNs). Existing approaches mitigate this issue by either enhancing the robustness of GNNs to distribution shift or reducing the shift itself. However, both approaches necessitate retraining the model, which becomes unfeasible when the model structure and parameters are inaccessible. To address this challenge, we propose FR-GNN, a general framework for GNNs to conduct feature reconstruction. FRGNN constructs a mapping relationship between the output and input of a well-trained GNN to obtain class representative embeddings and then uses these embeddings to reconstruct the features of labeled nodes. These reconstructed features are then incorporated into the message passing mechanism of GNNs to influence the predictions of unlabeled nodes at test time. Notably, the reconstructed node features can be directly utilized for testing the well-trained model, effectively reducing the distribution shift and leading to improved test performance. This remarkable achievement is attained without any modifications to the model structure or parameters. We provide theoretical guarantees for the effectiveness of our framework. Furthermore, we conduct comprehensive experiments on various public datasets. The experimental results demonstrate the superior performance of FRGNN in comparison to mainstream methods.
</details>
<details>
<summary>摘要</summary>
FR-GNN constructs a mapping relationship between the output and input of a well-trained GNN to obtain class representative embeddings and then uses these embeddings to reconstruct the features of labeled nodes. These reconstructed features are then incorporated into the message passing mechanism of GNNs to influence the predictions of unlabeled nodes at test time. Notably, the reconstructed node features can be directly utilized for testing the well-trained model, effectively reducing the distribution shift and leading to improved test performance. This remarkable achievement is attained without any modifications to the model structure or parameters. We provide theoretical guarantees for the effectiveness of our framework.Furthermore, we conduct comprehensive experiments on various public datasets, and the experimental results demonstrate the superior performance of FR-GNN in comparison to mainstream methods.
</details></li>
</ul>
<hr>
<h2 id="Capacity-Bounds-for-Hyperbolic-Neural-Network-Representations-of-Latent-Tree-Structures"><a href="#Capacity-Bounds-for-Hyperbolic-Neural-Network-Representations-of-Latent-Tree-Structures" class="headerlink" title="Capacity Bounds for Hyperbolic Neural Network Representations of Latent Tree Structures"></a>Capacity Bounds for Hyperbolic Neural Network Representations of Latent Tree Structures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09250">http://arxiv.org/abs/2308.09250</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anastasis Kratsios, Ruiyang Hong, Haitz Sáez de Ocáriz Borde</li>
<li>for: 这个论文探讨了深度偏函数神经网络（HNN）在偏函数空间中的表示能力。</li>
<li>methods: 作者使用了ReLU activation function和网络复杂度来证明HNN可以ε-同构将任意权重树 embed到偏函数空间中，其中偏函数空间的维度至少为2，并且偏函数弯曲度为负数κ&lt;0。</li>
<li>results: 作者发现HNN的网络复杂度与表示准确度无关，而且任何ReLU多层感知机（MLP）都必须在嵌入树时减少至少Ω(L^{1&#x2F;d})的准确度，独立于层数、宽度和活动函数。<details>
<summary>Abstract</summary>
We study the representation capacity of deep hyperbolic neural networks (HNNs) with a ReLU activation function. We establish the first proof that HNNs can $\varepsilon$-isometrically embed any finite weighted tree into a hyperbolic space of dimension $d$ at least equal to $2$ with prescribed sectional curvature $\kappa<0$, for any $\varepsilon> 1$ (where $\varepsilon=1$ being optimal). We establish rigorous upper bounds for the network complexity on an HNN implementing the embedding. We find that the network complexity of HNN implementing the graph representation is independent of the representation fidelity/distortion. We contrast this result against our lower bounds on distortion which any ReLU multi-layer perceptron (MLP) must exert when embedding a tree with $L>2^d$ leaves into a $d$-dimensional Euclidean space, which we show at least $\Omega(L^{1/d})$; independently of the depth, width, and (possibly discontinuous) activation function defining the MLP.
</details>
<details>
<summary>摘要</summary>
我们研究深度偏函数神经网络（HNN）的表示能力，使用ReLUActivation函数。我们证明了HNN可以将任意有重量的树 $\varepsilon$-同构到具有扁拟圆盘环境的空间中，其中维度至少为2，并且sectional curvature小于0，对于任何 $\varepsilon>1$（其中 $\varepsilon=1$ 是最优的）。我们也提出了准确的网络复杂度上限，用于HNN实现图像表示。我们发现网络复杂度与表示准确度无关。我们对此结果与我们对多层感知机（MLP）中的下界进行了比较，其中MLP必须在权重大于$2^d$的树中嵌入$d$-维欧式空间，并且下界至少为$\Omega(L^{1/d})$，无论深度、宽度和（可能不连续）活化函数。
</details></li>
</ul>
<hr>
<h2 id="Active-and-Passive-Causal-Inference-Learning"><a href="#Active-and-Passive-Causal-Inference-Learning" class="headerlink" title="Active and Passive Causal Inference Learning"></a>Active and Passive Causal Inference Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09248">http://arxiv.org/abs/2308.09248</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Jiwoong Im, Kyunghyun Cho</li>
<li>for: 这篇论文是为了提供机器学习研究者、工程师和学生，尝试了解但未 Familiar with causal inference 的人开始的一个起点。</li>
<li>methods: 本文从 collectively 需要的假设出发，包括交换性、正性、一致性和无干扰。从这些假设中，我们构建了一组重要的 causal inference 技术，分为两个桶：活跃和被动approaches。我们描述和讨论了随机化控制试验和bandit-based approaches从活跃类别，然后描述了经典方法，如匹配和反向概率权重，从被动类别中。最后，我们介绍了一些 causal inference 中缺失的方面，如迷宫偏见，以便让读者通过这篇论文获得多种开始点，进一步阅读和研究 causal inference 和发现。</li>
<li>results: 本文提供了一个多样化的起点，以便读者可以尝试不同的方法和技术，并进行进一步的研究和阅读。<details>
<summary>Abstract</summary>
This paper serves as a starting point for machine learning researchers, engineers and students who are interested in but not yet familiar with causal inference. We start by laying out an important set of assumptions that are collectively needed for causal identification, such as exchangeability, positivity, consistency and the absence of interference. From these assumptions, we build out a set of important causal inference techniques, which we do so by categorizing them into two buckets; active and passive approaches. We describe and discuss randomized controlled trials and bandit-based approaches from the active category. We then describe classical approaches, such as matching and inverse probability weighting, in the passive category, followed by more recent deep learning based algorithms. By finishing the paper with some of the missing aspects of causal inference from this paper, such as collider biases, we expect this paper to provide readers with a diverse set of starting points for further reading and research in causal inference and discovery.
</details>
<details>
<summary>摘要</summary>
Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China.
</details></li>
</ul>
<hr>
<h2 id="A-Robust-Policy-Bootstrapping-Algorithm-for-Multi-objective-Reinforcement-Learning-in-Non-stationary-Environments"><a href="#A-Robust-Policy-Bootstrapping-Algorithm-for-Multi-objective-Reinforcement-Learning-in-Non-stationary-Environments" class="headerlink" title="A Robust Policy Bootstrapping Algorithm for Multi-objective Reinforcement Learning in Non-stationary Environments"></a>A Robust Policy Bootstrapping Algorithm for Multi-objective Reinforcement Learning in Non-stationary Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09734">http://arxiv.org/abs/2308.09734</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sherif Abdelfattah, Kathryn Kasmarik, Jiankun Hu</li>
<li>for: 本研究旨在解决多目标Markov决策过程中的多目标优化问题，即在带有随机过程的环境中进行逐步决策，同时满足Markov性的条件。</li>
<li>methods: 本研究使用了多目标回归学习方法，将回归学习概念与多目标优化技术相结合，以解决这个问题。然而，现有的方法具有不适应非站点环境的缺点，因为它们采用了优化过程，假设环境是站点的。</li>
<li>results: 本研究提出了一种发展优化方法，可以在在线方式下，在定义的目标空间中，逐步演化策略覆盖集，以适应非站点环境。我们还提出了一种新的多目标回归学习算法，可以在非站点环境中稳定演化一个凸覆盖集的策略。与现有的算法相比，我们的算法在非站点环境中表现出了显著的优异性，而在站点环境中则达到了相似的结果。<details>
<summary>Abstract</summary>
Multi-objective Markov decision processes are a special kind of multi-objective optimization problem that involves sequential decision making while satisfying the Markov property of stochastic processes. Multi-objective reinforcement learning methods address this problem by fusing the reinforcement learning paradigm with multi-objective optimization techniques. One major drawback of these methods is the lack of adaptability to non-stationary dynamics in the environment. This is because they adopt optimization procedures that assume stationarity to evolve a coverage set of policies that can solve the problem. This paper introduces a developmental optimization approach that can evolve the policy coverage set while exploring the preference space over the defined objectives in an online manner. We propose a novel multi-objective reinforcement learning algorithm that can robustly evolve a convex coverage set of policies in an online manner in non-stationary environments. We compare the proposed algorithm with two state-of-the-art multi-objective reinforcement learning algorithms in stationary and non-stationary environments. Results showed that the proposed algorithm significantly outperforms the existing algorithms in non-stationary environments while achieving comparable results in stationary environments.
</details>
<details>
<summary>摘要</summary>
多目标马尔可夫决策过程是一种特殊的多目标优化问题，它涉及到顺序做出决策，并满足马尔可夫性Property的游程过程。多目标返点学习方法解决了这个问题，它将返点学习 paradigma与多目标优化技术相结合。然而，这些方法的一个主要缺点是缺乏适应非站ARY动态环境的能力。这是因为它们采用了优化过程，假设环境是站ARY的，以演算出一个覆盖集的策略，可以解决问题。本文提出了一种发展优化方法，可以在线模式下演化策略覆盖集，同时在定义的目标空间中探索喜好。我们提出了一种新的多目标返点学习算法，可以在线模式下鲁棒地演化一个凸覆盖集的策略，并在非站ARY环境中显著超越了现有的两种多目标返点学习算法。结果表明，提posed algorithm在非站ARY环境中具有显著的优势，而在站ARY环境中则具有相对的优势。
</details></li>
</ul>
<hr>
<h2 id="Intrinsically-Motivated-Hierarchical-Policy-Learning-in-Multi-objective-Markov-Decision-Processes"><a href="#Intrinsically-Motivated-Hierarchical-Policy-Learning-in-Multi-objective-Markov-Decision-Processes" class="headerlink" title="Intrinsically Motivated Hierarchical Policy Learning in Multi-objective Markov Decision Processes"></a>Intrinsically Motivated Hierarchical Policy Learning in Multi-objective Markov Decision Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09733">http://arxiv.org/abs/2308.09733</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sherif Abdelfattah, Kathryn Merrick, Jiankun Hu</li>
<li>for: 解决多目标Markov决策过程中的多个冲突奖励函数不能同时优化， conventional single optimal policy 不能解决这类问题。</li>
<li>methods: 使用 multi-objective reinforcement learning 方法，演化一个可满足所有偏好的优化策略集。</li>
<li>results: 在动态环境中，提出了一种新的 dual-phase intrinsically motivated reinforcement learning 方法，在第一阶段学习一个通用技能集，在第二阶段使用这个集 bootstrap policy coverage sets for each shift in the environment dynamics，实验显示该方法在动态环境中表现出优于state-of-the-art多目标决策方法。<details>
<summary>Abstract</summary>
Multi-objective Markov decision processes are sequential decision-making problems that involve multiple conflicting reward functions that cannot be optimized simultaneously without a compromise. This type of problems cannot be solved by a single optimal policy as in the conventional case. Alternatively, multi-objective reinforcement learning methods evolve a coverage set of optimal policies that can satisfy all possible preferences in solving the problem. However, many of these methods cannot generalize their coverage sets to work in non-stationary environments. In these environments, the parameters of the state transition and reward distribution vary over time. This limitation results in significant performance degradation for the evolved policy sets. In order to overcome this limitation, there is a need to learn a generic skill set that can bootstrap the evolution of the policy coverage set for each shift in the environment dynamics therefore, it can facilitate a continuous learning process. In this work, intrinsically motivated reinforcement learning has been successfully deployed to evolve generic skill sets for learning hierarchical policies to solve multi-objective Markov decision processes. We propose a novel dual-phase intrinsically motivated reinforcement learning method to address this limitation. In the first phase, a generic set of skills is learned. While in the second phase, this set is used to bootstrap policy coverage sets for each shift in the environment dynamics. We show experimentally that the proposed method significantly outperforms state-of-the-art multi-objective reinforcement methods in a dynamic robotics environment.
</details>
<details>
<summary>摘要</summary>
多目标Markov决策过程是一种sequential decision-making问题，其涉及多个矛盾的奖励函数，无法同时优化这些奖励函数。这类问题不可以通过单一优化策略来解决，相反，多目标学习方法会演化一个包含多个优化策略的coverage集，以满足所有可能的偏好。然而，许多这些方法无法扩展其coverage集以适应非站ARY环境。在这些环境中，状态转移和奖励分布的参数会随时间变化。这限制了演化出来的策略集的性能。为了突破这一限制，需要学习一个通用技能集，以便在环境动态变化时，使用这个技能集来演化策略集，从而实现连续学习过程。在这个工作中，我们成功地应用了内在激励学习方法来演化通用技能集，以解决多目标Markov决策过程中的限制。我们提出了一种新的双相内在激励学习方法，其在第一阶段学习一个通用技能集，而在第二阶段使用这个集来 bootstrap策略集，以适应环境动态变化。我们通过实验表明，提议的方法在动态 робо扮环境中能够显著超越现状的多目标决策方法。
</details></li>
</ul>
<hr>
<h2 id="Generalized-Sum-Pooling-for-Metric-Learning"><a href="#Generalized-Sum-Pooling-for-Metric-Learning" class="headerlink" title="Generalized Sum Pooling for Metric Learning"></a>Generalized Sum Pooling for Metric Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09228">http://arxiv.org/abs/2308.09228</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yetigurbuz/generalized-sum-pooling">https://github.com/yetigurbuz/generalized-sum-pooling</a></li>
<li>paper_authors: Yeti Z. Gurbuz, Ozan Sener, A. Aydın Alatan</li>
<li>for: This paper focuses on improving the global average pooling (GAP) method in deep metric learning by proposing a learnable generalized sum pooling (GSP) method.</li>
<li>methods: The proposed GSP method uses an entropy-smoothed optimal transport problem to learn the weights for aggregating feature vectors, and it has two distinct abilities: i) selecting a subset of semantic entities and ii) learning the importance of each entity.</li>
<li>results: The proposed GSP method is evaluated on four popular metric learning benchmarks and shows improved performance compared to the traditional GAP method.Here’s the simplified Chinese text format you requested:</li>
<li>for: 本研究探讨了深度度量学中的全局平均汇聚方法（GAP）的改进方法，提出了一种可学习的总和汇聚方法（GSP）。</li>
<li>methods: GSP方法使用了一种感知平滑的优质运输问题来学习归一化特征向量的权重，具有两种特点：一是选择一 subset of semantic entities，二是学习每个实体的重要性。</li>
<li>results: GSP方法在四个流行的度量学挑战 task 上进行了广泛的评估，与传统的GAP方法相比，表现出了改进的效果。<details>
<summary>Abstract</summary>
A common architectural choice for deep metric learning is a convolutional neural network followed by global average pooling (GAP). Albeit simple, GAP is a highly effective way to aggregate information. One possible explanation for the effectiveness of GAP is considering each feature vector as representing a different semantic entity and GAP as a convex combination of them. Following this perspective, we generalize GAP and propose a learnable generalized sum pooling method (GSP). GSP improves GAP with two distinct abilities: i) the ability to choose a subset of semantic entities, effectively learning to ignore nuisance information, and ii) learning the weights corresponding to the importance of each entity. Formally, we propose an entropy-smoothed optimal transport problem and show that it is a strict generalization of GAP, i.e., a specific realization of the problem gives back GAP. We show that this optimization problem enjoys analytical gradients enabling us to use it as a direct learnable replacement for GAP. We further propose a zero-shot loss to ease the learning of GSP. We show the effectiveness of our method with extensive evaluations on 4 popular metric learning benchmarks. Code is available at: GSP-DML Framework
</details>
<details>
<summary>摘要</summary>
一种常见的深度学习架构选择是使用卷积神经网络后跟global average pooling（GAP）。尽管简单，但GAP是一种非常有效的信息汇集方法。一种可能的解释是每个特征向量都代表不同的semantic entity，GAP可以看作这些entity的convex combination。基于这个视角，我们扩展GAP并提出一种学习可能的通用汇集方法（GSP）。GSP在两个方面超越GAP：一是可以选择一 subset of semantic entities，有效地忽略干扰信息；二是可以学习每个entity的重要性权重。我们提出一个熵平滑的优化Transport问题，并证明它是GAP的严格泛化，即特定的实现问题可以回归GAP。我们还提出了一个零战损损失来使GSP更加容易学习。我们通过对4个流行的metric learning benchmark进行了广泛的评估，证明了我们的方法的有效性。代码可以在：GSP-DML框架中找到。
</details></li>
</ul>
<hr>
<h2 id="Advancing-Relation-Extraction-through-Language-Probing-with-Exemplars-from-Set-Co-Expansion"><a href="#Advancing-Relation-Extraction-through-Language-Probing-with-Exemplars-from-Set-Co-Expansion" class="headerlink" title="Advancing Relation Extraction through Language Probing with Exemplars from Set Co-Expansion"></a>Advancing Relation Extraction through Language Probing with Exemplars from Set Co-Expansion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11720">http://arxiv.org/abs/2308.11720</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yerong Li, Roxana Girju</li>
<li>For: The primary goal of the paper is to enhance relation classification accuracy and mitigate confusion between contrastive classes in relation extraction.* Methods: The authors propose a multi-faceted approach that integrates representative examples and through co-set expansion, which involves seeding each relationship class with representative examples and incorporating similarity measures between target pairs and representative pairs from the target class.* Results: The authors achieve a significant enhancement of relation classification performance, with an observed margin of at least 1 percent improvement in accuracy in most settings, on top of existing fine-tuning approaches. Additionally, the authors conduct an in-depth analysis of tuning contrastive examples to reduce confusion between classes sharing similarities and achieve more precise classification.Here are the three key points in Simplified Chinese:* For: 本研究的主要目标是提高关系分类精度，并减少对异类关系的混淆。* Methods: 作者们提出了一种多方面的方法，即通过代表示例和通过合并扩展来提高关系分类精度。* Results: 作者们实现了一个显著提高关系分类性能的效果，在大多数情况下，与现有的精度提高方法相比，观测到了至少1%的提高。此外，作者们还进行了深入分析，并发现通过调整对异类关系的对照示例，可以更好地减少对异类关系的混淆。<details>
<summary>Abstract</summary>
Relation Extraction (RE) is a pivotal task in automatically extracting structured information from unstructured text. In this paper, we present a multi-faceted approach that integrates representative examples and through co-set expansion. The primary goal of our method is to enhance relation classification accuracy and mitigating confusion between contrastive classes.   Our approach begins by seeding each relationship class with representative examples. Subsequently, our co-set expansion algorithm enriches training objectives by incorporating similarity measures between target pairs and representative pairs from the target class. Moreover, the co-set expansion process involves a class ranking procedure that takes into account exemplars from contrastive classes. Contextual details encompassing relation mentions are harnessed via context-free Hearst patterns to ascertain contextual similarity.   Empirical evaluation demonstrates the efficacy of our co-set expansion approach, resulting in a significant enhancement of relation classification performance. Our method achieves an observed margin of at least 1 percent improvement in accuracy in most settings, on top of existing fine-tuning approaches. To further refine our approach, we conduct an in-depth analysis that focuses on tuning contrastive examples. This strategic selection and tuning effectively reduce confusion between classes sharing similarities, leading to a more precise classification process.   Experimental results underscore the effectiveness of our proposed framework for relation extraction. The synergy between co-set expansion and context-aware prompt tuning substantially contributes to improved classification accuracy. Furthermore, the reduction in confusion between contrastive classes through contrastive examples tuning validates the robustness and reliability of our method.
</details>
<details>
<summary>摘要</summary>
<<SYS>>TRANSLATE_TEXTRelation Extraction (RE) 是自动提取结构化信息的重要任务。在这篇文章中，我们提出了一个多元的方法，它结合代表性的例子和通过协同扩展。我们的方法的主要目标是增加关系分类精度，并减少相似类别的混淆。我们的方法开始由每个关系类别中的代表性例子。然后，我们的协同扩展算法将训练目标扩展到包含关系提及的上下文特征。此外，协同扩展过程还包括一个类别排名程序，它考虑了对应类别的例子。上下文特征探索关系提及的上下文特征，使用 Context-free Hearst 模式来确定上下文相似性。实验结果显示我们的协同扩展方法的有效性，它导致关系分类精度的明显提高。我们的方法在大多数情况下得到观察的改善率至少1%，在现有的调整方法之上。为了进一步改进我们的方法，我们进行了深入的分析，专注于调整对照例子。这种策略性的选择和调整有效地减少了相似类别之间的混淆，导致更精确的分类过程。实验结果证明我们的提出的框架具有优秀的效果，协同扩展和上下文相似性调整的融合对关系分类精度有重要贡献。此外，透过对对照例子的调整，我们的方法 Validates the robustness and reliability of our method.<<SYS</SYS>Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="DMCVR-Morphology-Guided-Diffusion-Model-for-3D-Cardiac-Volume-Reconstruction"><a href="#DMCVR-Morphology-Guided-Diffusion-Model-for-3D-Cardiac-Volume-Reconstruction" class="headerlink" title="DMCVR: Morphology-Guided Diffusion Model for 3D Cardiac Volume Reconstruction"></a>DMCVR: Morphology-Guided Diffusion Model for 3D Cardiac Volume Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09223">http://arxiv.org/abs/2308.09223</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hexiaoxiao-cs/dmcvr">https://github.com/hexiaoxiao-cs/dmcvr</a></li>
<li>paper_authors: Xiaoxiao He, Chaowei Tan, Ligong Han, Bo Liu, Leon Axel, Kang Li, Dimitris N. Metaxas</li>
<li>for: 提高心脏疾病诊断和治疗规划的精度，通过从缺乏2D图像堆栈中生成高分辨率3D心脏剖面图像。</li>
<li>methods: 基于生成模型，使用形态导向的扩散模型DMCVR，通过控制心脏形态来提高生成质量，并从学习的秘密空间获得全球semantics、局部心脏形态和每个2DcMRI片的细节。</li>
<li>results: 在多个方面表现出色，包括2D生成和3D重建性能，可以生成高分辨率3D心脏MRI重建图像，超过当前技术。<details>
<summary>Abstract</summary>
Accurate 3D cardiac reconstruction from cine magnetic resonance imaging (cMRI) is crucial for improved cardiovascular disease diagnosis and understanding of the heart's motion. However, current cardiac MRI-based reconstruction technology used in clinical settings is 2D with limited through-plane resolution, resulting in low-quality reconstructed cardiac volumes. To better reconstruct 3D cardiac volumes from sparse 2D image stacks, we propose a morphology-guided diffusion model for 3D cardiac volume reconstruction, DMCVR, that synthesizes high-resolution 2D images and corresponding 3D reconstructed volumes. Our method outperforms previous approaches by conditioning the cardiac morphology on the generative model, eliminating the time-consuming iterative optimization process of the latent code, and improving generation quality. The learned latent spaces provide global semantics, local cardiac morphology and details of each 2D cMRI slice with highly interpretable value to reconstruct 3D cardiac shape. Our experiments show that DMCVR is highly effective in several aspects, such as 2D generation and 3D reconstruction performance. With DMCVR, we can produce high-resolution 3D cardiac MRI reconstructions, surpassing current techniques. Our proposed framework has great potential for improving the accuracy of cardiac disease diagnosis and treatment planning. Code can be accessed at https://github.com/hexiaoxiao-cs/DMCVR.
</details>
<details>
<summary>摘要</summary>
准确的3D冠状动脉重建从cine磁共振成像（cMRI）是诊断心血管疾病的关键，以及心脏运动的理解。然而，现有的医学应用中的cardiac MRI重建技术仅为2D，具有有限的沿plane分辨率，导致低质量重建的冠状动脉体积。为了提高3D冠状动脉体积的重建质量，我们提议一种基于形态指导的扩散模型，DMCVR，该模型可以将高分辨率的2D图像和相应的3D重建体积相互关联。我们的方法比前一代方法更高效，因为它将cardiac形态条件在生成模型中，消除了时间消耗的迭代优化过程，并提高生成质量。学习的积分空间提供了全球 semantics、局部冠状动脉形态和每个2D cMRI slice的高分辨率重建。我们的实验表明，DMCVR在多个方面都具有高效性，如2D生成和3D重建性能。通过DMCVR，我们可以生成高分辨率的3D冠状动脉MRI重建，超越现有技术。我们提出的框架具有诊断心血管疾病的准确性和治疗规划的潜在优势。代码可以在https://github.com/hexiaoxiao-cs/DMCVR中获取。
</details></li>
</ul>
<hr>
<h2 id="Baird-Counterexample-Is-Solved-with-an-example-of-How-to-Debug-a-Two-time-scale-Algorithm"><a href="#Baird-Counterexample-Is-Solved-with-an-example-of-How-to-Debug-a-Two-time-scale-Algorithm" class="headerlink" title="Baird Counterexample Is Solved: with an example of How to Debug a Two-time-scale Algorithm"></a>Baird Counterexample Is Solved: with an example of How to Debug a Two-time-scale Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09732">http://arxiv.org/abs/2308.09732</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hengshuai Yao</li>
<li>for: 本文是用来解释TD(0)算法在Baird counterexample中的异常行为，以及two-time-scale随机近似算法的调试分析。</li>
<li>methods: 本文使用了TD(0)算法和Impression GTD算法来解释Baird counterexample的行为，并提供了Debugging技术来研究这种行为。</li>
<li>results: 本文的实验结果表明，Impression GTD算法在Baird counterexample中的收敛速度非常快，甚至在线性速度。这表明了Baird counterexample已经得到了解决。<details>
<summary>Abstract</summary>
Baird counterexample was proposed by Leemon Baird in 1995, first used to show that the Temporal Difference (TD(0)) algorithm diverges on this example. Since then, it is often used to test and compare off-policy learning algorithms. Gradient TD algorithms solved the divergence issue of TD on Baird counterexample. However, their convergence on this example is still very slow, and the nature of the slowness is not well understood, e.g., see (Sutton and Barto 2018).   This note is to understand in particular, why TDC is slow on this example, and provide debugging analysis to understand this behavior. Our debugging technique can be used to study the convergence behavior of two-time-scale stochastic approximation algorithms. We also provide empirical results of the recent Impression GTD algorithm on this example, showing the convergence is very fast, in fact, in a linear rate. We conclude that Baird counterexample is solved, by an algorithm with convergence guarantee to the TD solution in general and a fast convergence rate.
</details>
<details>
<summary>摘要</summary>
白尔德反例（Baird counterexample）由Leemon Baird在1995年提出，用以表明TD(0)算法在这个例子中出现偏离。自此以后，这个例子经常用于测试和比较不同的离政学习算法。梯度TD算法解决了TD算法在白尔德反例中的偏离问题，但是它们在这个例子上的减速还是很慢，并且这种慢速度的原因还不很清楚，比如参见（Sutton和Barto 2018）。本文的目的是要理解TDCSlow的原因，并提供调试分析来理解这种行为。我们的调试技术可以用来研究两个时间尺度的随机抽象算法的转化行为。我们还提供了最近的Impression GTD算法在这个例子上的实验结果，显示它的转化非常快，甚至是线性减速。我们 conclude that Baird counterexample已经得到解决，TDCSlow的问题也得到解决，并且有一个可靠的转化率。
</details></li>
</ul>
<hr>
<h2 id="A-Model-Agnostic-Framework-for-Recommendation-via-Interest-aware-Item-Embeddings"><a href="#A-Model-Agnostic-Framework-for-Recommendation-via-Interest-aware-Item-Embeddings" class="headerlink" title="A Model-Agnostic Framework for Recommendation via Interest-aware Item Embeddings"></a>A Model-Agnostic Framework for Recommendation via Interest-aware Item Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09202">http://arxiv.org/abs/2308.09202</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amit Kumar Jaiswal, Yu Xiong</li>
<li>for: 这个研究旨在提高推荐系统中的ITEM表示，以更好地捕捉用户的兴趣。</li>
<li>methods: 该研究提出了一种名为Interest-aware Capsule network（IaCN）的新型推荐模型，该模型通过 direkt learning interest-oriented item representations来提高推荐效果。</li>
<li>results: 实验结果表明， compared to existing recommendation models, IaCN 可以在不同的深度神经网络、行为序列长度和共同学习率下提高推荐性能。<details>
<summary>Abstract</summary>
Item representation holds significant importance in recommendation systems, which encompasses domains such as news, retail, and videos. Retrieval and ranking models utilise item representation to capture the user-item relationship based on user behaviours. While existing representation learning methods primarily focus on optimising item-based mechanisms, such as attention and sequential modelling. However, these methods lack a modelling mechanism to directly reflect user interests within the learned item representations. Consequently, these methods may be less effective in capturing user interests indirectly. To address this challenge, we propose a novel Interest-aware Capsule network (IaCN) recommendation model, a model-agnostic framework that directly learns interest-oriented item representations. IaCN serves as an auxiliary task, enabling the joint learning of both item-based and interest-based representations. This framework adopts existing recommendation models without requiring substantial redesign. We evaluate the proposed approach on benchmark datasets, exploring various scenarios involving different deep neural networks, behaviour sequence lengths, and joint learning ratios of interest-oriented item representations. Experimental results demonstrate significant performance enhancements across diverse recommendation models, validating the effectiveness of our approach.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="TinyProp-–-Adaptive-Sparse-Backpropagation-for-Efficient-TinyML-On-device-Learning"><a href="#TinyProp-–-Adaptive-Sparse-Backpropagation-for-Efficient-TinyML-On-device-Learning" class="headerlink" title="TinyProp – Adaptive Sparse Backpropagation for Efficient TinyML On-device Learning"></a>TinyProp – Adaptive Sparse Backpropagation for Efficient TinyML On-device Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09201">http://arxiv.org/abs/2308.09201</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcus Rüb, Daniel Maier, Daniel Mueller-Gritschneder, Axel Sikora</li>
<li>for: 这篇论文是用于提高深度神经网络在内存和计算上的效率，以便在低功率微控制器单元（MCU）上进行在机器上的学习或精细化神经网络。</li>
<li>methods: 这篇论文使用的方法是使用逐步练习，将神经网络中的一部分权重和偏置给训练。这些权重和偏置会在训练过程中进行动态调整，以确保在每个训练步骤中都能够获得最佳的结果。</li>
<li>results: 这篇论文的结果显示，使用TinyProp方法可以在MCU上进行在机器上的学习，并且比非 sparse 训练更快，且仅受到小的计算开销。具体来说，与非 sparse 训练相比，TinyProp可以提高平均速度约5倍，并且仅受到1%的精度损失。此外，与现有的静态 sparse backpropagation方法相比，TinyProp可以提高平均速度约2.9倍，并且精度损失降低约6%。<details>
<summary>Abstract</summary>
Training deep neural networks using backpropagation is very memory and computationally intensive. This makes it difficult to run on-device learning or fine-tune neural networks on tiny, embedded devices such as low-power micro-controller units (MCUs). Sparse backpropagation algorithms try to reduce the computational load of on-device learning by training only a subset of the weights and biases. Existing approaches use a static number of weights to train. A poor choice of this so-called backpropagation ratio limits either the computational gain or can lead to severe accuracy losses. In this paper we present TinyProp, the first sparse backpropagation method that dynamically adapts the back-propagation ratio during on-device training for each training step. TinyProp induces a small calculation overhead to sort the elements of the gradient, which does not significantly impact the computational gains. TinyProp works particularly well on fine-tuning trained networks on MCUs, which is a typical use case for embedded applications. For typical datasets from three datasets MNIST, DCASE2020 and CIFAR10, we are 5 times faster compared to non-sparse training with an accuracy loss of on average 1%. On average, TinyProp is 2.9 times faster than existing, static sparse backpropagation algorithms and the accuracy loss is reduced on average by 6 % compared to a typical static setting of the back-propagation ratio.
</details>
<details>
<summary>摘要</summary>
培训深度神经网络使用反传播是非常占用内存和计算资源的。这使得在设备学习或精细调整神经网络的小型、嵌入式设备（如低功耗微控制器单元）上运行困难。稀疏反传播算法试图减少设备上学习的计算负担，只培训一部分权重和偏好。现有方法使用静态的反传播比率来培训。这种称为反传播比率的选择有限制 either the computational gain or can lead to severe accuracy losses。在这篇论文中，我们介绍了TinyProp，第一种动态适应设备上培训中每步反传播比率的稀疏反传播方法。TinyProp需要小量的计算开销来排序梯度元素，这并不会对计算减少的影响。TinyProp在MCUs上精细调整已经训练的网络 particularly well，这是常见的嵌入式应用场景。对于典型的MNIST、DCASE2020和CIFAR10数据集，我们比非稀疏培训更快5倍，减少了平均1%的精度损失。相比已有的静态稀疏反传播算法，TinyProp在平均上2.9倍快，并且在平均上减少了6%的精度损失。
</details></li>
</ul>
<hr>
<h2 id="Polynomial-Bounds-for-Learning-Noisy-Optical-Physical-Unclonable-Functions-and-Connections-to-Learning-With-Errors"><a href="#Polynomial-Bounds-for-Learning-Noisy-Optical-Physical-Unclonable-Functions-and-Connections-to-Learning-With-Errors" class="headerlink" title="Polynomial Bounds for Learning Noisy Optical Physical Unclonable Functions and Connections to Learning With Errors"></a>Polynomial Bounds for Learning Noisy Optical Physical Unclonable Functions and Connections to Learning With Errors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09199">http://arxiv.org/abs/2308.09199</a></li>
<li>repo_url: None</li>
<li>paper_authors: Apollo Albright, Boris Gelfand, Michael Dixon</li>
<li>for: 这个论文旨在研究一类光学物理不可克隆函数（PUF）是否可以在噪声存在下，通过许多挑战对应的回应对，学习到任意精度，并且可以在计算机术语下进行高效的学习。</li>
<li>methods: 这篇论文使用了一种基于多项式的拟合算法，通过对噪声存在下的挑战对应的回应对进行学习，来实现PUF的学习。</li>
<li>results: 论文表明，通过许多挑战对应的回应对，可以在噪声存在下，通过计算机术语下的高效拟合算法，学习到光学物理不可克隆函数（PUF）的任意精度，并且可以在 polynomial 时间内完成。<details>
<summary>Abstract</summary>
It is shown that a class of optical physical unclonable functions (PUFs) can be learned to arbitrary precision with arbitrarily high probability, even in the presence of noise, given access to polynomially many challenge-response pairs and polynomially bounded computational power, under mild assumptions about the distributions of the noise and challenge vectors. This extends the results of Rh\"uramir et al. (2013), who showed a subset of this class of PUFs to be learnable in polynomial time in the absence of noise, under the assumption that the optics of the PUF were either linear or had negligible nonlinear effects. We derive polynomial bounds for the required number of samples and the computational complexity of a linear regression algorithm, based on size parameters of the PUF, the distributions of the challenge and noise vectors, and the probability and accuracy of the regression algorithm, with a similar analysis to one done by Bootle et al. (2018), who demonstrated a learning attack on a poorly implemented version of the Learning With Errors problem.
</details>
<details>
<summary>摘要</summary>
据显示，一类光学物理不可复制函数（PUFs）可以准确地学习到任意精度，即使在噪声存在的情况下，只要有对挑战响应对的极多数对话和计算能力，且假设噪声和挑战向量的分布都是某种可以考虑的。这些结果超越了 Rh\"uramir et al. (2013) 所示的一个子集的 PUFs，他们表明这些 PUFs 可以在噪声缺失的情况下，在 polynomial 时间内学习，假设 optics 的 PUF 是 линей的或具有可忽略的非线性效应。我们 derive 了一些多项式的样本数和计算复杂性的下界，基于 PUF 的大小参数、挑战向量和噪声向量的分布、学习算法的概率和准确率，与 Bootle et al. (2018) 所做的一个类似的分析。
</details></li>
</ul>
<hr>
<h2 id="Half-Hop-A-graph-upsampling-approach-for-slowing-down-message-passing"><a href="#Half-Hop-A-graph-upsampling-approach-for-slowing-down-message-passing" class="headerlink" title="Half-Hop: A graph upsampling approach for slowing down message passing"></a>Half-Hop: A graph upsampling approach for slowing down message passing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09198">http://arxiv.org/abs/2308.09198</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nerdslab/halfhop">https://github.com/nerdslab/halfhop</a></li>
<li>paper_authors: Mehdi Azabou, Venkataramana Ganesh, Shantanu Thakoor, Chi-Heng Lin, Lakshmi Sathidevi, Ran Liu, Michal Valko, Petar Veličković, Eva L. Dyer</li>
<li>for: 提高Message Passing神经网络的学习效果，尤其是在邻居节点属于不同类别时。</li>
<li>methods: 引入”慢节点”，用于媒介源节点和目标节点之间的通信，从而减少消息传递过程中的过滤效果。</li>
<li>results: 在多种监督和自监学 benchmark 上提高了表现，特别是在hetrophilic 条件下， где邻居节点更有可能属于不同的类别。 Additionally, the approach can be used to generate augmentations for self-supervised learning, by introducing slow nodes into different edges in the graph to generate multi-scale views with variable path lengths.<details>
<summary>Abstract</summary>
Message passing neural networks have shown a lot of success on graph-structured data. However, there are many instances where message passing can lead to over-smoothing or fail when neighboring nodes belong to different classes. In this work, we introduce a simple yet general framework for improving learning in message passing neural networks. Our approach essentially upsamples edges in the original graph by adding "slow nodes" at each edge that can mediate communication between a source and a target node. Our method only modifies the input graph, making it plug-and-play and easy to use with existing models. To understand the benefits of slowing down message passing, we provide theoretical and empirical analyses. We report results on several supervised and self-supervised benchmarks, and show improvements across the board, notably in heterophilic conditions where adjacent nodes are more likely to have different labels. Finally, we show how our approach can be used to generate augmentations for self-supervised learning, where slow nodes are randomly introduced into different edges in the graph to generate multi-scale views with variable path lengths.
</details>
<details>
<summary>摘要</summary>
We provide theoretical and empirical analyses to demonstrate the benefits of slowing down message passing. Our results on several supervised and self-supervised benchmarks show improvements across the board, particularly in heterophilic conditions where adjacent nodes are more likely to have different labels. Additionally, we show how our approach can be used to generate augmentations for self-supervised learning, where slow nodes are randomly introduced into different edges in the graph to generate multi-scale views with variable path lengths.In simplified Chinese:message passing neural networks 在图structured data 上表现出了很多成功，但是它们可能会导致过滤或失败当邻居节点属于不同的类。在这个工作中，我们介绍了一种简单 yet general的框架，用于提高message passing neural networks 的学习。我们的方法添加了每个边的 "slow node"，以便在源节点和目标节点之间进行中间调度。我们的方法只需修改输入图，使其易于使用现有的模型。我们提供了理论和实验分析，以证明减速消息传递的好处。我们的结果表明，在多种supervised和self-supervised benchmarks 上，我们的方法都能获得提高，特别是在邻居节点属于不同类时。此外，我们还示出了如何使用我们的方法生成自适应学习的扩展， где slow node 在不同的边上被随机添加，以生成多缓度视图和变量路径长。
</details></li>
</ul>
<hr>
<h2 id="A-Comparative-Study-of-Text-Embedding-Models-for-Semantic-Text-Similarity-in-Bug-Reports"><a href="#A-Comparative-Study-of-Text-Embedding-Models-for-Semantic-Text-Similarity-in-Bug-Reports" class="headerlink" title="A Comparative Study of Text Embedding Models for Semantic Text Similarity in Bug Reports"></a>A Comparative Study of Text Embedding Models for Semantic Text Similarity in Bug Reports</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09193">http://arxiv.org/abs/2308.09193</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/av9ash/duplicatebugdetection">https://github.com/av9ash/duplicatebugdetection</a></li>
<li>paper_authors: Avinash Patil, Kihwan Han, Sabyasachi Mukhopadhyay</li>
<li>for: 本研究旨在比较不同嵌入模型在撷取相似 bug report 方面的效果，以帮助减少问题解决的时间和努力。</li>
<li>methods: 本研究使用了多种嵌入模型，包括 TF-IDF (基线), FastText, Gensim, BERT, 和 ADA，并使用 Software Defects Data 来评估这些模型的表现。</li>
<li>results: 实验结果显示 BERT 通常在 recall 方面表现最好，其次是 ADA, Gensim, FastText, 和 TF-IDF。本研究提供了不同嵌入模型在撷取相似 bug report 方面的效果，并显示了选择适当的嵌入模型对这个任务的重要性。<details>
<summary>Abstract</summary>
Bug reports are an essential aspect of software development, and it is crucial to identify and resolve them quickly to ensure the consistent functioning of software systems. Retrieving similar bug reports from an existing database can help reduce the time and effort required to resolve bugs. In this paper, we compared the effectiveness of semantic textual similarity methods for retrieving similar bug reports based on a similarity score. We explored several embedding models such as TF-IDF (Baseline), FastText, Gensim, BERT, and ADA. We used the Software Defects Data containing bug reports for various software projects to evaluate the performance of these models. Our experimental results showed that BERT generally outperformed the rest of the models regarding recall, followed by ADA, Gensim, FastText, and TFIDF. Our study provides insights into the effectiveness of different embedding methods for retrieving similar bug reports and highlights the impact of selecting the appropriate one for this task. Our code is available on GitHub.
</details>
<details>
<summary>摘要</summary>
�� Bug reports 是软件开发的重要方面，快速确定和解决 bug 可以确保软件系统的一致性。从现有数据库中检索类似的 bug reports 可以减少解决 bug 所需的时间和努力。在这篇论文中，我们比较了使用 semantic textual similarity 方法来检索类似 bug reports 的效果，基于一个相似性分数。我们探索了多种嵌入模型，包括 TF-IDF（基线）、FastText、Gensim、BERT 和 ADA。我们使用 Software Defects Data 中的 bug reports 来评估这些模型的性能。我们的实验结果表明，BERT 通常在 recall 方面表现最好，其次是 ADA、Gensim、FastText 和 TF-IDF。我们的研究提供了不同嵌入方法在检索类似 bug reports 的效果的视角，并 highlights 选择合适的嵌入方法对此任务的影响。我们的代码可以在 GitHub 上找到。
</details></li>
</ul>
<hr>
<h2 id="Regularizing-Adversarial-Imitation-Learning-Using-Causal-Invariance"><a href="#Regularizing-Adversarial-Imitation-Learning-Using-Causal-Invariance" class="headerlink" title="Regularizing Adversarial Imitation Learning Using Causal Invariance"></a>Regularizing Adversarial Imitation Learning Using Causal Invariance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09189">http://arxiv.org/abs/2308.09189</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ivan Ovinnikov, Joachim M. Buhmann</li>
<li>for: 本研究使用启发学习方法从专家示范数据集中推导策略，以最小化分配度度量的差异。</li>
<li>methods: 使用探测器作为对抗优化程序的指导信号。</li>
<li>results: 发现这种模型容易吸收专家数据中的假 correlate，提出了使用 causal invariance 作为对抗训练模型的规范原则，并在一些高维机器人移动 benchmark 任务中证明其效果。<details>
<summary>Abstract</summary>
Imitation learning methods are used to infer a policy in a Markov decision process from a dataset of expert demonstrations by minimizing a divergence measure between the empirical state occupancy measures of the expert and the policy. The guiding signal to the policy is provided by the discriminator used as part of an versarial optimization procedure. We observe that this model is prone to absorbing spurious correlations present in the expert data. To alleviate this issue, we propose to use causal invariance as a regularization principle for adversarial training of these models. The regularization objective is applicable in a straightforward manner to existing adversarial imitation frameworks. We demonstrate the efficacy of the regularized formulation in an illustrative two-dimensional setting as well as a number of high-dimensional robot locomotion benchmark tasks.
</details>
<details>
<summary>摘要</summary>
“模仿学习方法用于在Markov决策过程中推断策略，从专家示范数据集中 minimize 一个分割度量，以确定专家和策略之间的状态占据度的差异。导航信号提供给策略是通过用作对抗优化过程中的识别器。我们发现这些模型容易吸收专家数据中的假 correlations。为解决这个问题，我们提议使用 causal invariance 作为对抗训练这些模型的正则化原则。这个正则化目标可以直接应用于现有的对抗模仿框架中。我们在一个简单的二维设置中以及一些高维机器人移动 benchmark 任务中证明了正则化形式的效果。”Note that Simplified Chinese is a written language used in mainland China, and it is different from Traditional Chinese, which is used in Taiwan and other parts of the world.
</details></li>
</ul>
<hr>
<h2 id="Distributed-Extra-gradient-with-Optimal-Complexity-and-Communication-Guarantees"><a href="#Distributed-Extra-gradient-with-Optimal-Complexity-and-Communication-Guarantees" class="headerlink" title="Distributed Extra-gradient with Optimal Complexity and Communication Guarantees"></a>Distributed Extra-gradient with Optimal Complexity and Communication Guarantees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09187">http://arxiv.org/abs/2308.09187</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lions-epfl/qgenx">https://github.com/lions-epfl/qgenx</a></li>
<li>paper_authors: Ali Ramezani-Kebrya, Kimon Antonakopoulos, Igor Krawczuk, Justin Deschenaux, Volkan Cevher</li>
<li>for:  solve monotone variational inequality (VI) problems in multi-GPU settings</li>
<li>methods:  propose a quantized generalized extra-gradient (Q-GenX) algorithm, which is an unbiased and adaptive compression method tailored to solve VIs, and an adaptive step-size rule that adapts to the respective noise profiles at hand</li>
<li>results:  achieve a fast convergence rate of ${\mathcal O}(1&#x2F;T)$ under relative noise and an order-optimal convergence rate of ${\mathcal O}(1&#x2F;\sqrt{T})$ under absolute noise, and validate the theoretical results through real-world experiments and training generative adversarial networks on multiple GPUs.Here’s the format you requested:</li>
<li>for: &lt; solve monotone variational inequality (VI) problems in multi-GPU settings&gt;</li>
<li>methods: &lt; propose a quantized generalized extra-gradient (Q-GenX) algorithm, which is an unbiased and adaptive compression method tailored to solve VIs, and an adaptive step-size rule that adapts to the respective noise profiles at hand&gt;</li>
<li>results: &lt; achieve a fast convergence rate of ${\mathcal O}(1&#x2F;T)$ under relative noise and an order-optimal convergence rate of ${\mathcal O}(1&#x2F;\sqrt{T})$ under absolute noise, and validate the theoretical results through real-world experiments and training generative adversarial networks on multiple GPUs.&gt;<details>
<summary>Abstract</summary>
We consider monotone variational inequality (VI) problems in multi-GPU settings where multiple processors/workers/clients have access to local stochastic dual vectors. This setting includes a broad range of important problems from distributed convex minimization to min-max and games. Extra-gradient, which is a de facto algorithm for monotone VI problems, has not been designed to be communication-efficient. To this end, we propose a quantized generalized extra-gradient (Q-GenX), which is an unbiased and adaptive compression method tailored to solve VIs. We provide an adaptive step-size rule, which adapts to the respective noise profiles at hand and achieve a fast rate of ${\mathcal O}(1/T)$ under relative noise, and an order-optimal ${\mathcal O}(1/\sqrt{T})$ under absolute noise and show distributed training accelerates convergence. Finally, we validate our theoretical results by providing real-world experiments and training generative adversarial networks on multiple GPUs.
</details>
<details>
<summary>摘要</summary>
我们考虑了多个GPU上的单调几何维问题（VI），在这些环境中，多个处理器/员工/客户端具有本地随机对应矩阵。这些环境包括分布式凸降至最小值和游戏等问题。标准的extra-gradient算法，它是单调几何VI问题的实际算法，但它并没有考虑通信效率。因此，我们提出了弹性化的弹性矩阵（Q-GenX），它是适应性的弹性压缩方法，适用于解决VI问题。我们还提出了适应步长规则，可以适应具体的噪声 Profiling ，并在相对噪声下 achieves a fast rate of $O(1/T)$，并在绝对噪声下 achieves an order-optimal $O(1/\sqrt{T})$。最后，我们验证了我们的理论结果，通过实际实验，在多个GPU上训练生成器条件网络。
</details></li>
</ul>
<hr>
<h2 id="RatGPT-Turning-online-LLMs-into-Proxies-for-Malware-Attacks"><a href="#RatGPT-Turning-online-LLMs-into-Proxies-for-Malware-Attacks" class="headerlink" title="RatGPT: Turning online LLMs into Proxies for Malware Attacks"></a>RatGPT: Turning online LLMs into Proxies for Malware Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09183">http://arxiv.org/abs/2308.09183</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mika Beckerich, Laura Plein, Sergio Coronado</li>
<li>for: This paper is written to highlight the cybersecurity issues that arise from the use of openly available plugins and Large Language Models (LLMs) in software engineering.</li>
<li>methods: The paper uses an LLM, specifically ChatGPT, as a proxy between the attacker and the victim to disseminate malicious software and evade detection. The authors also establish communication with a command and control (C2) server to receive commands to interact with the victim’s system.</li>
<li>results: The paper presents a proof-of-concept that demonstrates the use of LLMs for malicious purposes, such as delivering malware and establishing communication with a C2 server. The authors highlight the significant cybersecurity issues that arise from the use of openly available plugins and LLMs, and emphasize the need for the development of security guidelines, controls, and mitigation strategies.Here is the information in Simplified Chinese text:</li>
<li>for: 这篇论文是为了探讨开源插件和大语言模型（LLM）在软件工程中的安全问题。</li>
<li>methods: 论文使用ChatGPT作为攻击者和受害者之间的代理，通过它来传播恶意软件并避免检测。作者还建立了命令和控制（C2）服务器与受害者系统进行交互的通信。</li>
<li>results: 论文提供了一个证明，证明了开源插件和LLM可以用于恶意目的，如传播恶意软件和建立C2服务器与受害者系统进行交互。作者指出了开源插件和LLM的使用导致的重要安全问题，并强调了需要开发安全指南、控制和缓解策略。<details>
<summary>Abstract</summary>
The evolution of Generative AI and the capabilities of the newly released Large Language Models (LLMs) open new opportunities in software engineering. However, they also lead to new challenges in cybersecurity. Recently, researchers have shown the possibilities of using LLMs such as ChatGPT to generate malicious content that can directly be exploited or guide inexperienced hackers to weaponize tools and code. Those studies covered scenarios that still require the attacker in the middle of the loop. In this study, we leverage openly available plugins and use an LLM as proxy between the attacker and the victim. We deliver a proof-of-concept where ChatGPT is used for the dissemination of malicious software while evading detection, alongside establishing the communication to a command and control (C2) server to receive commands to interact with a victim's system. Finally, we present the general approach as well as essential elements in order to stay undetected and make the attack a success. This proof-of-concept highlights significant cybersecurity issues with openly available plugins and LLMs, which require the development of security guidelines, controls, and mitigation strategies.
</details>
<details>
<summary>摘要</summary>
生成AI的演化和新一代大语言模型（LLMs）在软件工程中开创了新的机遇，但也带来了新的cybersecurity挑战。最近，研究人员已经证明了使用ChatGPT等LLM生成恶意内容，直接利用或指导不熟悉黑客 weaponize工具和代码的可能性。这些研究都需要攻击者在循环中。在这项研究中，我们利用公开可用的插件，使用LLM作为袋中间人，将攻击者与受害者之间的交互进行了隐蔽。我们实现了一个证明，使用ChatGPT进行恶意软件的散布，同时避免检测，并与Command和Control（C2）服务器建立了通信，以接收对受害者系统的交互命令。最后，我们提出了通用的方法和关键元素，以确保攻击成功，并且需要开发安全指南、控制和缓减策略。这项证明指出，公开可用的插件和LLMs对cybersecurity pose了重要的问题，需要开发安全措施，以避免攻击。
</details></li>
</ul>
<hr>
<h2 id="ChatGPT-HealthPrompt-Harnessing-the-Power-of-XAI-in-Prompt-Based-Healthcare-Decision-Support-using-ChatGPT"><a href="#ChatGPT-HealthPrompt-Harnessing-the-Power-of-XAI-in-Prompt-Based-Healthcare-Decision-Support-using-ChatGPT" class="headerlink" title="ChatGPT-HealthPrompt. Harnessing the Power of XAI in Prompt-Based Healthcare Decision Support using ChatGPT"></a>ChatGPT-HealthPrompt. Harnessing the Power of XAI in Prompt-Based Healthcare Decision Support using ChatGPT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09731">http://arxiv.org/abs/2308.09731</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fatemeh Nazary, Yashar Deldjoo, Tommaso Di Noia</li>
<li>for: 本研究旨在探讨大语言模型（LLMs）在医疗决策中的应用，特点是使用 OpenAI 的 ChatGPT。我们的方法涉及设计了 Contextual prompts，包括任务描述、特征描述以及医学知识的集成，以实现高质量的 binary classification 任务，即使在数据缺乏的情况下。</li>
<li>methods: 我们的研究利用了医学知识，来设计高效可读取的 ML 模型，并将其集成到提问中。我们视这些 ML 模型为医学专家，从而提取了关键的特征重要性，以帮助决策过程。</li>
<li>results: 我们的研究发现，使用提问工程学strategies可以在不同的数据条件下提高 OpenAI 的 ChatGPT 的性能。我们的方法可以在数据缺乏的情况下实现高质量的 binary classification 任务，并且可以在不同的医学领域中应用。<details>
<summary>Abstract</summary>
This study presents an innovative approach to the application of large language models (LLMs) in clinical decision-making, focusing on OpenAI's ChatGPT. Our approach introduces the use of contextual prompts-strategically designed to include task description, feature description, and crucially, integration of domain knowledge-for high-quality binary classification tasks even in data-scarce scenarios. The novelty of our work lies in the utilization of domain knowledge, obtained from high-performing interpretable ML models, and its seamless incorporation into prompt design. By viewing these ML models as medical experts, we extract key insights on feature importance to aid in decision-making processes. This interplay of domain knowledge and AI holds significant promise in creating a more insightful diagnostic tool.   Additionally, our research explores the dynamics of zero-shot and few-shot prompt learning based on LLMs. By comparing the performance of OpenAI's ChatGPT with traditional supervised ML models in different data conditions, we aim to provide insights into the effectiveness of prompt engineering strategies under varied data availability. In essence, this paper bridges the gap between AI and healthcare, proposing a novel methodology for LLMs application in clinical decision support systems. It highlights the transformative potential of effective prompt design, domain knowledge integration, and flexible learning approaches in enhancing automated decision-making.
</details>
<details>
<summary>摘要</summary>
Additionally, our research explores the dynamics of zero-shot and few-shot prompt learning based on LLMs. By comparing the performance of OpenAI's ChatGPT with traditional supervised ML models in different data conditions, we aim to provide insights into the effectiveness of prompt engineering strategies under varied data availability. In essence, this paper bridges the gap between AI and healthcare, proposing a novel methodology for LLMs application in clinical decision support systems. It highlights the transformative potential of effective prompt design, domain knowledge integration, and flexible learning approaches in enhancing automated decision-making.
</details></li>
</ul>
<hr>
<h2 id="Diversifying-AI-Towards-Creative-Chess-with-AlphaZero"><a href="#Diversifying-AI-Towards-Creative-Chess-with-AlphaZero" class="headerlink" title="Diversifying AI: Towards Creative Chess with AlphaZero"></a>Diversifying AI: Towards Creative Chess with AlphaZero</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09175">http://arxiv.org/abs/2308.09175</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tom Zahavy, Vivek Veeriah, Shaobo Hou, Kevin Waugh, Matthew Lai, Edouard Leurent, Nenad Tomasev, Lisa Schut, Demis Hassabis, Satinder Singh</li>
<li>for: 本研究探讨了人工智能（AI）系统是否可以通过创新决策机制来增强其计算合理性。</li>
<li>methods: 本研究使用了AlphaZero（AZ）游戏为基础，并通过嵌入条件的建模来实现多个代理之间的交互。通过行为多样性技术来让AI系统生成更多的想法，然后选择最有前途的想法。</li>
<li>results: 实验结果表明，使用多个AI系统组成的团队可以在棋盘游戏中解决更多的问题，并且能够在不同的开局中选择最佳的玩家。与AlphaZero相比，多个AI系统团队可以解决更多的复杂问题，包括困难的盘龙位。<details>
<summary>Abstract</summary>
In recent years, Artificial Intelligence (AI) systems have surpassed human intelligence in a variety of computational tasks. However, AI systems, like humans, make mistakes, have blind spots, hallucinate, and struggle to generalize to new situations. This work explores whether AI can benefit from creative decision-making mechanisms when pushed to the limits of its computational rationality. In particular, we investigate whether a team of diverse AI systems can outperform a single AI in challenging tasks by generating more ideas as a group and then selecting the best ones. We study this question in the game of chess, the so-called drosophila of AI. We build on AlphaZero (AZ) and extend it to represent a league of agents via a latent-conditioned architecture, which we call AZ_db. We train AZ_db to generate a wider range of ideas using behavioral diversity techniques and select the most promising ones with sub-additive planning. Our experiments suggest that AZ_db plays chess in diverse ways, solves more puzzles as a group and outperforms a more homogeneous team. Notably, AZ_db solves twice as many challenging puzzles as AZ, including the challenging Penrose positions. When playing chess from different openings, we notice that players in AZ_db specialize in different openings, and that selecting a player for each opening using sub-additive planning results in a 50 Elo improvement over AZ. Our findings suggest that diversity bonuses emerge in teams of AI agents, just as they do in teams of humans and that diversity is a valuable asset in solving computationally hard problems.
</details>
<details>
<summary>摘要</summary>
We build on the AlphaZero (AZ) system and extend it to represent a league of agents via a latent-conditioned architecture, which we call AZ_db. We train AZ_db to generate a wider range of ideas using behavioral diversity techniques and select the most promising ones with sub-additive planning. Our experiments show that AZ_db plays chess in diverse ways, solves more puzzles as a group, and outperforms a more homogeneous team. Notably, AZ_db solves twice as many challenging puzzles as AZ, including the challenging Penrose positions.When playing chess from different openings, we notice that players in AZ_db specialize in different openings, and that selecting a player for each opening using sub-additive planning results in a 50 Elo improvement over AZ. Our findings suggest that diversity bonuses emerge in teams of AI agents, just as they do in teams of humans, and that diversity is a valuable asset in solving computationally hard problems.
</details></li>
</ul>
<hr>
<h2 id="Forensic-Data-Analytics-for-Anomaly-Detection-in-Evolving-Networks"><a href="#Forensic-Data-Analytics-for-Anomaly-Detection-in-Evolving-Networks" class="headerlink" title="Forensic Data Analytics for Anomaly Detection in Evolving Networks"></a>Forensic Data Analytics for Anomaly Detection in Evolving Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09171">http://arxiv.org/abs/2308.09171</a></li>
<li>repo_url: None</li>
<li>paper_authors: Li Yang, Abdallah Moubayed, Abdallah Shami, Amine Boukhtouta, Parisa Heidari, Stere Preda, Richard Brunner, Daniel Migault, Adel Larabi</li>
<li>for: 这篇论文旨在提出一种数字审计框架，用于检测网络异常行为，以帮助检察官追究犯罪分子并预防未来的犯罪活动。</li>
<li>methods: 该框架基于多个视角的特征工程，包括无监管异常检测和全面结果修正过程。</li>
<li>results: 实验结果表明，提出的数字审计解决方案具有效果，可以帮助检察官更好地检察和处理网络犯罪。<details>
<summary>Abstract</summary>
In the prevailing convergence of traditional infrastructure-based deployment (i.e., Telco and industry operational networks) towards evolving deployments enabled by 5G and virtualization, there is a keen interest in elaborating effective security controls to protect these deployments in-depth. By considering key enabling technologies like 5G and virtualization, evolving networks are democratized, facilitating the establishment of point presences integrating different business models ranging from media, dynamic web content, gaming, and a plethora of IoT use cases. Despite the increasing services provided by evolving networks, many cybercrimes and attacks have been launched in evolving networks to perform malicious activities. Due to the limitations of traditional security artifacts (e.g., firewalls and intrusion detection systems), the research on digital forensic data analytics has attracted more attention. Digital forensic analytics enables people to derive detailed information and comprehensive conclusions from different perspectives of cybercrimes to assist in convicting criminals and preventing future crimes. This chapter presents a digital analytics framework for network anomaly detection, including multi-perspective feature engineering, unsupervised anomaly detection, and comprehensive result correction procedures. Experiments on real-world evolving network data show the effectiveness of the proposed forensic data analytics solution.
</details>
<details>
<summary>摘要</summary>
在传统基础设施部署（如电信和产业运营网络）向5G和虚拟化技术的演进部署转型时，有强烈的兴趣在深入保护这些部署。通过考虑关键启用技术如5G和虚拟化，演进网络得到了民主化，使得不同业务模式的点存在集成，从媒体、动态网页内容、游戏和大量物联网应用场景。由于传统安全文件（如防火墙和入侵检测系统）的局限性，研究数字审查数据分析技术在获得更多的注意力。数字审查数据分析技术可以帮助人们从不同角度获得细致信息和全面结论，以协助指控犯罪分子并预防未来的犯罪。本章介绍了一种网络异常检测数字分析框架，包括多元视角特征工程、无监督异常检测和全面结果更正过程。在实际演进网络数据上进行实验，提出的数字审查数据分析解决方案得到了证明。
</details></li>
</ul>
<hr>
<h2 id="Online-Transition-Based-Feature-Generation-for-Anomaly-Detection-in-Concurrent-Data-Streams"><a href="#Online-Transition-Based-Feature-Generation-for-Anomaly-Detection-in-Concurrent-Data-Streams" class="headerlink" title="Online Transition-Based Feature Generation for Anomaly Detection in Concurrent Data Streams"></a>Online Transition-Based Feature Generation for Anomaly Detection in Concurrent Data Streams</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10893">http://arxiv.org/abs/2308.10893</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yinzheng Zhong, Alexei Lisitsa</li>
<li>for: 这篇论文旨在提出一种基于转移的特征生成技术（TFGen），用于处理通信网络数据、系统调用数据和监测摄像头数据等不同类型的活动数据。</li>
<li>methods: TFGen使用转移学习来生成数据，可以在线进行处理，并且可以使用历史数据进行编码。</li>
<li>results: TFGen可以解决域独特性、全球过程结构的发现、时间序列数据的编码和在线处理等问题，并且具有高计算效率。<details>
<summary>Abstract</summary>
In this paper, we introduce the transition-based feature generator (TFGen) technique, which reads general activity data with attributes and generates step-by-step generated data. The activity data may consist of network activity from packets, system calls from processes or classified activity from surveillance cameras. TFGen processes data online and will generate data with encoded historical data for each incoming activity with high computational efficiency. The input activities may concurrently originate from distinct traces or channels. The technique aims to address issues such as domain-independent applicability, the ability to discover global process structures, the encoding of time-series data, and online processing capability.
</details>
<details>
<summary>摘要</summary>
在本文中，我们介绍了一种基于转移的特征生成技术（TFGen），该技术可以读取一般活动数据并生成步骤生成的数据。活动数据可以包括网络活动封包、进程系呼び出 shout ouputs或视频监测器中的分类活动。TFGen在线上处理数据，可以对每个入参活动进行编码历史数据的生成，并具有高计算效率。输入活动可以同时来自不同的轨迹或通道。该技术的目标是解决域独立可用性、找到全局过程结构、编码时间序列数据和在线处理能力等问题。
</details></li>
</ul>
<hr>
<h2 id="FedPerfix-Towards-Partial-Model-Personalization-of-Vision-Transformers-in-Federated-Learning"><a href="#FedPerfix-Towards-Partial-Model-Personalization-of-Vision-Transformers-in-Federated-Learning" class="headerlink" title="FedPerfix: Towards Partial Model Personalization of Vision Transformers in Federated Learning"></a>FedPerfix: Towards Partial Model Personalization of Vision Transformers in Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09160">http://arxiv.org/abs/2308.09160</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/imguangyu/fedperfix">https://github.com/imguangyu/fedperfix</a></li>
<li>paper_authors: Guangyu Sun, Matias Mendieta, Jun Luo, Shandong Wu, Chen Chen</li>
<li>for: 本研究旨在提高分布式学习中的模型个性化，并 investigate ViT 模型中哪些部分可以进行部分个性化。</li>
<li>methods: 本研究使用了实验评估不同层的敏感性，并基于这些层的敏感性提出了一种名为 FedPerfix 的个性化方法，通过插件将模型的信息传递到本地客户端进行个性化。</li>
<li>results: 在 CIFAR-100、OrganAMNIST 和 Office-Home 等 datasets 上进行了实验评估，并与多种先进的 PFL 方法进行了比较，结果表明 FedPerfix 可以提高模型的性能。<details>
<summary>Abstract</summary>
Personalized Federated Learning (PFL) represents a promising solution for decentralized learning in heterogeneous data environments. Partial model personalization has been proposed to improve the efficiency of PFL by selectively updating local model parameters instead of aggregating all of them. However, previous work on partial model personalization has mainly focused on Convolutional Neural Networks (CNNs), leaving a gap in understanding how it can be applied to other popular models such as Vision Transformers (ViTs). In this work, we investigate where and how to partially personalize a ViT model. Specifically, we empirically evaluate the sensitivity to data distribution of each type of layer. Based on the insights that the self-attention layer and the classification head are the most sensitive parts of a ViT, we propose a novel approach called FedPerfix, which leverages plugins to transfer information from the aggregated model to the local client as a personalization. Finally, we evaluate the proposed approach on CIFAR-100, OrganAMNIST, and Office-Home datasets and demonstrate its effectiveness in improving the model's performance compared to several advanced PFL methods.
</details>
<details>
<summary>摘要</summary>
In this work, we investigate where and how to partially personalize a ViT model. Specifically, we empirically evaluate the sensitivity of each type of layer to data distribution. Based on the insights that the self-attention layer and the classification head are the most sensitive parts of a ViT, we propose a novel approach called FedPerfix, which leverages plugins to transfer information from the aggregated model to the local client as a personalization.We evaluate the proposed approach on CIFAR-100, OrganAMNIST, and Office-Home datasets and demonstrate its effectiveness in improving the model's performance compared to several advanced PFL methods.
</details></li>
</ul>
<hr>
<h2 id="Data-diversity-and-virtual-imaging-in-AI-based-diagnosis-A-case-study-based-on-COVID-19"><a href="#Data-diversity-and-virtual-imaging-in-AI-based-diagnosis-A-case-study-based-on-COVID-19" class="headerlink" title="Data diversity and virtual imaging in AI-based diagnosis: A case study based on COVID-19"></a>Data diversity and virtual imaging in AI-based diagnosis: A case study based on COVID-19</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09730">http://arxiv.org/abs/2308.09730</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fakrul Islam Tushar, Lavsen Dahal, Saman Sotoudeh-Paima, Ehsan Abadi, W. Paul Segars, Ehsan Samei, Joseph Y. Lo<br>for: 这个研究旨在发展和评估基于深度学习的人工智能（AI）模型，用于诊断新型冠状病毒（COVID-19）。methods: 该研究使用了多种临床和虚拟生成的医疗图像，开发和评估了AI模型。同时，研究还进行了虚拟成像试验，以评估AI性能是如何受到患者和物理因素的影响。results: 研究发现，AI性能受到数据集特点（包括数量、多样性和患者程度）的强烈影响，导致在不同数据集上的性能下降至20%。虚拟CT和CXR图像的模型性能与临床数据集的结果相似。诊断性能与疾病程度有显著关系，CT结果比CXR结果更佳。<details>
<summary>Abstract</summary>
Many studies have investigated deep-learning-based artificial intelligence (AI) models for medical imaging diagnosis of the novel coronavirus (COVID-19), with many reports of near-perfect performance. However, variability in performance and underlying data biases raise concerns about clinical generalizability. This retrospective study involved the development and evaluation of artificial intelligence (AI) models for COVID-19 diagnosis using both diverse clinical and virtually generated medical images. In addition, we conducted a virtual imaging trial to assess how AI performance is affected by several patient- and physics-based factors, including the extent of disease, radiation dose, and imaging modality of computed tomography (CT) and chest radiography (CXR). AI performance was strongly influenced by dataset characteristics including quantity, diversity, and prevalence, leading to poor generalization with up to 20% drop in receiver operating characteristic area under the curve. Model performance on virtual CT and CXR images was comparable to overall results on clinical data. Imaging dose proved to have negligible influence on the results, but the extent of the disease had a marked affect. CT results were consistently superior to those from CXR. Overall, the study highlighted the significant impact of dataset characteristics and disease extent on COVID assessment, and the relevance and potential role of virtual imaging trial techniques on developing effective evaluation of AI algorithms and facilitating translation into diagnostic practice.
</details>
<details>
<summary>摘要</summary>
多些研究已经研究了深度学习基于人工智能（AI）模型，用于医疗影像诊断新型冠状病毒（COVID-19），有很多报告表明近乎完美的性能。然而，表现的变化和下面数据偏好引起了临床一致性的问题。这个逆向研究涉及了COVID-19诊断使用多种临床和虚拟生成的医疗影像开发和评估人工智能（AI）模型。此外，我们还进行了虚拟成像试验，以评估人工智能表现如何受到患者和物理因素的影响，包括疾病程度、辐射剂量和成像方式。我们发现，AI表现受数据集特点的影响，包括数量、多样性和普遍性，导致近20%的接收操作特征曲线下降。模型在虚拟CT和CXR图像上的表现与总体数据上的表现相当。成像剂量对结果没有影响，但疾病程度有明显的影响。CT结果比CXR结果更加稳定。总之，这个研究指出了COVID诊断中数据集特点和疾病程度对人工智能评估的重要影响，以及虚拟成像试验技术在开发有效的AI算法和实现诊断实践中的重要作用。
</details></li>
</ul>
<hr>
<h2 id="ZhiJian-A-Unifying-and-Rapidly-Deployable-Toolbox-for-Pre-trained-Model-Reuse"><a href="#ZhiJian-A-Unifying-and-Rapidly-Deployable-Toolbox-for-Pre-trained-Model-Reuse" class="headerlink" title="ZhiJian: A Unifying and Rapidly Deployable Toolbox for Pre-trained Model Reuse"></a>ZhiJian: A Unifying and Rapidly Deployable Toolbox for Pre-trained Model Reuse</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09158">http://arxiv.org/abs/2308.09158</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhangyikaii/lamda-zhijian">https://github.com/zhangyikaii/lamda-zhijian</a></li>
<li>paper_authors: Yi-Kai Zhang, Lu Ren, Chao Yi, Qi-Wei Wang, De-Chuan Zhan, Han-Jia Ye</li>
<li>for: 这篇论文是为了探讨模型重复（Model Reuse）的方法和技术，以便在实际应用中快速学习和扩展机器学习。</li>
<li>methods: 这篇论文使用了PyTorch backend，提出了一个统一的模型重复方法，包括目标建构、适应器适配和PTM-based推论。</li>
<li>results: 这篇论文提出了ZhiJian工具箱，一个可朗丰和易用的模型重复工具，并透过实际应用评估了ZhiJian的效果。<details>
<summary>Abstract</summary>
The rapid expansion of foundation pre-trained models and their fine-tuned counterparts has significantly contributed to the advancement of machine learning. Leveraging pre-trained models to extract knowledge and expedite learning in real-world tasks, known as "Model Reuse", has become crucial in various applications. Previous research focuses on reusing models within a certain aspect, including reusing model weights, structures, and hypothesis spaces. This paper introduces ZhiJian, a comprehensive and user-friendly toolbox for model reuse, utilizing the PyTorch backend. ZhiJian presents a novel paradigm that unifies diverse perspectives on model reuse, encompassing target architecture construction with PTM, tuning target model with PTM, and PTM-based inference. This empowers deep learning practitioners to explore downstream tasks and identify the complementary advantages among different methods. ZhiJian is readily accessible at https://github.com/zhangyikaii/lamda-zhijian facilitating seamless utilization of pre-trained models and streamlining the model reuse process for researchers and developers.
</details>
<details>
<summary>摘要</summary>
“快速扩展的基础模型和其精细化版本的出现，对机器学习的进步做出了重要贡献。利用预训练模型来提取知识和加速实际任务中的学习，称为“模型再利用”，在各种应用中变得非常重要。先前的研究主要集中在模型重用方面，包括模型权重、结构和假设空间的重用。本文介绍了一个名为ZhiJian的通用和易用的工具箱，利用PyTorch backend，可以实现模型重用。ZhiJian提出了一种新的思路，整合了多种模型重用的视角，包括目标建筑PTM、调参目标模型PTM和PTM基于的推理。这使得深度学习专家可以更好地探索下游任务，并发现不同方法之间的补做优势。ZhiJian可以很方便地在https://github.com/zhangyikaii/lamda-zhijian上使用，便于研究人员和开发者使用预训练模型，并简化模型重用过程。”
</details></li>
</ul>
<hr>
<h2 id="Accurate-machine-learning-force-fields-via-experimental-and-simulation-data-fusion"><a href="#Accurate-machine-learning-force-fields-via-experimental-and-simulation-data-fusion" class="headerlink" title="Accurate machine learning force fields via experimental and simulation data fusion"></a>Accurate machine learning force fields via experimental and simulation data fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09142">http://arxiv.org/abs/2308.09142</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sebastien Röcken, Julija Zavadlav</li>
<li>For: The paper aims to develop a machine learning (ML) potential for titanium that can accurately predict its properties by leveraging both density functional theory (DFT) calculations and experimentally measured mechanical properties and lattice parameters.* Methods: The authors use a fused data learning strategy that combines DFT calculations and experimental data to train the ML potential. They demonstrate that this approach can concurrently satisfy all target objectives and result in a more accurate molecular model compared to models trained with a single data source.* Results: The paper shows that the fused data learning strategy can correct the inaccuracies of DFT functionals at target experimental properties while leaving off-target properties largely unperturbed. The authors also demonstrate the applicability of their approach to any material, making it a general strategy for obtaining highly accurate ML potentials.Here are the three points in Simplified Chinese text:* For: 这篇论文目标是开发一个基于机器学习（ML）的钛力场模型，可以准确预测钛的性质。* Methods: 作者们使用了一种融合数据学习策略，将DFT计算和实验测量的机械性质和晶体参数组合来训练ML potential。他们示出了这种方法可以同时满足所有目标对象，并且比使用单一数据源训练的模型更加准确。* Results: 论文显示，融合数据学习策略可以正确地修正DFT函数的偏差，并且保持偏离目标对象的性质不受影响。作者们还证明了这种方法的通用性，可以应用于任何材料。<details>
<summary>Abstract</summary>
Machine Learning (ML)-based force fields are attracting ever-increasing interest due to their capacity to span spatiotemporal scales of classical interatomic potentials at quantum-level accuracy. They can be trained based on high-fidelity simulations or experiments, the former being the common case. However, both approaches are impaired by scarce and erroneous data resulting in models that either do not agree with well-known experimental observations or are under-constrained and only reproduce some properties. Here we leverage both Density Functional Theory (DFT) calculations and experimentally measured mechanical properties and lattice parameters to train an ML potential of titanium. We demonstrate that the fused data learning strategy can concurrently satisfy all target objectives, thus resulting in a molecular model of higher accuracy compared to the models trained with a single data source. The inaccuracies of DFT functionals at target experimental properties were corrected, while the investigated off-target properties remained largely unperturbed. Our approach is applicable to any material and can serve as a general strategy to obtain highly accurate ML potentials.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="RTB-Formulation-Using-Point-Process"><a href="#RTB-Formulation-Using-Point-Process" class="headerlink" title="RTB Formulation Using Point Process"></a>RTB Formulation Using Point Process</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09122">http://arxiv.org/abs/2308.09122</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seong Jin Lee, Bumsik Kim</li>
<li>for: 这个论文是为了模型 repeated auctions 的RTB生态系统中的随机过程。</li>
<li>methods: 论文使用点过程模型来描述这些 auctions，并提出了一种通用的框架来描述不同的拍卖enario。</li>
<li>results: 论文提出了一些理论结果，包括可以将这种过程简化为Poisson点过程，并且指出了玩家的优化策略在不同情况下的表现。<details>
<summary>Abstract</summary>
We propose a general stochastic framework for modelling repeated auctions in the Real Time Bidding (RTB) ecosystem using point processes. The flexibility of the framework allows a variety of auction scenarios including configuration of information provided to player, determination of auction winner and quantification of utility gained from each auctions. We propose theoretical results on how this formulation of process can be approximated to a Poisson point process, which enables the analyzer to take advantage of well-established properties. Under this framework, we specify the player's optimal strategy under various scenarios. We also emphasize that it is critical to consider the joint distribution of utility and market condition instead of estimating the marginal distributions independently.
</details>
<details>
<summary>摘要</summary>
我们提出了一个通用的随机框架，用于模拟重复拍卖在实时拍卖（RTB）生态系统中的进行，使用点程序。这个框架的灵活性允许多种拍卖enario，包括参与者资讯的配置、拍卖胜出者的决定以及每次拍卖中获得的 utility 的量化。我们提出了理论结果，认为这种过程可以近似到一个 Poisson 点程序，这使得分析者可以利用已有的性质。在这个框架下，我们针对不同的情况下定义了玩家的最佳策略。我们还强调了在联合分布中考虑market condition和 utility 的 JOINT 分布，而不是独立地估算两个分布。
</details></li>
</ul>
<hr>
<h2 id="Multi-fidelity-Fourier-Neural-Operator-for-Fast-Modeling-of-Large-Scale-Geological-Carbon-Storage"><a href="#Multi-fidelity-Fourier-Neural-Operator-for-Fast-Modeling-of-Large-Scale-Geological-Carbon-Storage" class="headerlink" title="Multi-fidelity Fourier Neural Operator for Fast Modeling of Large-Scale Geological Carbon Storage"></a>Multi-fidelity Fourier Neural Operator for Fast Modeling of Large-Scale Geological Carbon Storage</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09113">http://arxiv.org/abs/2308.09113</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hewei Tang, Qingkai Kong, Joseph P. Morris</li>
<li>for: 用于加速预测气体储存床中气压和CO2迁移的数学模型。</li>
<li>methods: 使用深度学习基于模拟器的多 fideltity FNO 模型。</li>
<li>results: 可以使用更可Affordable多 fideltity训练集来解决大规模GCS问题，并且可以预测气压场 WITH reasonable accuracy，即使高精度数据很少。<details>
<summary>Abstract</summary>
Deep learning-based surrogate models have been widely applied in geological carbon storage (GCS) problems to accelerate the prediction of reservoir pressure and CO2 plume migration. Large amounts of data from physics-based numerical simulators are required to train a model to accurately predict the complex physical behaviors associated with this process. In practice, the available training data are always limited in large-scale 3D problems due to the high computational cost. Therefore, we propose to use a multi-fidelity Fourier Neural Operator to solve large-scale GCS problems with more affordable multi-fidelity training datasets. The Fourier Neural Operator has a desirable grid-invariant property, which simplifies the transfer learning procedure between datasets with different discretization. We first test the model efficacy on a GCS reservoir model being discretized into 110k grid cells. The multi-fidelity model can predict with accuracy comparable to a high-fidelity model trained with the same amount of high-fidelity data with 81% less data generation costs. We further test the generalizability of the multi-fidelity model on a same reservoir model with a finer discretization of 1 million grid cells. This case was made more challenging by employing high-fidelity and low-fidelity datasets generated by different geostatistical models and reservoir simulators. We observe that the multi-fidelity FNO model can predict pressure fields with reasonable accuracy even when the high-fidelity data are extremely limited.
</details>
<details>
<summary>摘要</summary>
深度学习基于的代理模型在地球科学中广泛应用于加速气体储存（GCS）问题的预测，特别是气体泵迁和储存层压力的预测。但在实践中，大规模3D问题中的数据几乎总是受限，因为计算成本高。因此，我们提议使用多质量Fourier神经算法来解决大规模GCS问题，只需要较低的多质量训练数据。Fourier神经算法具有恰当的网格不变性，这使得在不同网格的数据之间进行传学过程变得更加简单。我们首先测试模型在110k网格细分的GCS储存模型上的效果。我们发现，使用多质量FNO模型可以与高精度模型在同样的数据量下达到相同的准确率，但是需要81% fewer数据生成成本。我们进一步测试了模型的一致性，在同一个储存模型上使用不同的地球统计学模型和储存 simulator生成的高精度和低精度数据来进行测试。我们发现，使用多质量FNO模型可以在高精度数据几乎极其有限的情况下预测压力场的reasonable准确率。
</details></li>
</ul>
<hr>
<h2 id="Learning-Lightweight-Object-Detectors-via-Multi-Teacher-Progressive-Distillation"><a href="#Learning-Lightweight-Object-Detectors-via-Multi-Teacher-Progressive-Distillation" class="headerlink" title="Learning Lightweight Object Detectors via Multi-Teacher Progressive Distillation"></a>Learning Lightweight Object Detectors via Multi-Teacher Progressive Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09105">http://arxiv.org/abs/2308.09105</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shengcao Cao, Mengtian Li, James Hays, Deva Ramanan, Yi-Xiong Wang, Liang-Yan Gui</li>
<li>for: 提高资源受限的视觉系统（如边缘计算和视频人工智能）中的视觉模型的准确率和计算量和内存使用率。</li>
<li>methods: 使用知识储存（knowledge distillation）技术来提高轻量级的分类模型的性能，但是在结构输出（如物体检测和实例分割）中，知识储存的应用仍然是一个复杂的任务，因为输出的变化和内部网络模块的复杂性。</li>
<li>results: 我们提出了一种简单 yet 有力的顺序方法，可以从一组教师检测器中逐步传递知识到一个轻量级学生模型，以提高其性能。我们的顺序策略可以轻松地与现有的检测储存机制结合使用，以在不同的设置下 consistently 提高学生模型的性能。我们成功地储存了基于 Transformer 的教师检测器的知识到基于 convolution 的学生模型中，并在 MS COCO 测试集上从 36.5% 提高到 42.0% AP 和从 38.2% 提高到 42.5% AP。<details>
<summary>Abstract</summary>
Resource-constrained perception systems such as edge computing and vision-for-robotics require vision models to be both accurate and lightweight in computation and memory usage. While knowledge distillation is a proven strategy to enhance the performance of lightweight classification models, its application to structured outputs like object detection and instance segmentation remains a complicated task, due to the variability in outputs and complex internal network modules involved in the distillation process. In this paper, we propose a simple yet surprisingly effective sequential approach to knowledge distillation that progressively transfers the knowledge of a set of teacher detectors to a given lightweight student. To distill knowledge from a highly accurate but complex teacher model, we construct a sequence of teachers to help the student gradually adapt. Our progressive strategy can be easily combined with existing detection distillation mechanisms to consistently maximize student performance in various settings. To the best of our knowledge, we are the first to successfully distill knowledge from Transformer-based teacher detectors to convolution-based students, and unprecedentedly boost the performance of ResNet-50 based RetinaNet from 36.5% to 42.0% AP and Mask R-CNN from 38.2% to 42.5% AP on the MS COCO benchmark.
</details>
<details>
<summary>摘要</summary>
资源受限的感知系统，如边计算和视频控制，需要视觉模型具备高准确率和轻量级计算和内存使用。而知识填充是一种证明了提高轻量级分类模型表现的策略，但对结构输出如物体检测和实例分割来说，它的应用仍然是一个复杂的任务，因为输出的变化和内部网络模块的复杂性。在这篇论文中，我们提出了一种简单却有力的顺序方法，可以从多个教师检测器中逐步传授知识到一个轻量级学生。为了从高精度但复杂的教师模型中填充知识，我们构建了一序列的教师，帮助学生逐步适应。我们的顺序策略可以轻松地与现有的检测填充机制结合，以实现在不同设置下学生表现的最大化。而我们所知道的是，我们成功地填充了基于Transformer的教师检测器的知识到基于 convolution 的学生中，并在 MS COCO benchmark 上从 36.5% 提高到 42.0% AP 和 38.2% 提高到 42.5% AP。
</details></li>
</ul>
<hr>
<h2 id="A-comprehensive-study-of-spike-and-slab-shrinkage-priors-for-structurally-sparse-Bayesian-neural-networks"><a href="#A-comprehensive-study-of-spike-and-slab-shrinkage-priors-for-structurally-sparse-Bayesian-neural-networks" class="headerlink" title="A comprehensive study of spike and slab shrinkage priors for structurally sparse Bayesian neural networks"></a>A comprehensive study of spike and slab shrinkage priors for structurally sparse Bayesian neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09104">http://arxiv.org/abs/2308.09104</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sanket Jantre, Shrijita Bhattacharya, Tapabrata Maiti</li>
<li>for: 提高深度学习网络的复杂度和计算效率。</li>
<li>methods: 使用结构化稀疏技术（如节点稀疏）压缩深度神经网络，以降低推理时间、提高数据传输量和降低能耗。</li>
<li>results: 比基eline模型更高的预测精度、模型压缩和推理延迟。<details>
<summary>Abstract</summary>
Network complexity and computational efficiency have become increasingly significant aspects of deep learning. Sparse deep learning addresses these challenges by recovering a sparse representation of the underlying target function by reducing heavily over-parameterized deep neural networks. Specifically, deep neural architectures compressed via structured sparsity (e.g. node sparsity) provide low latency inference, higher data throughput, and reduced energy consumption. In this paper, we explore two well-established shrinkage techniques, Lasso and Horseshoe, for model compression in Bayesian neural networks. To this end, we propose structurally sparse Bayesian neural networks which systematically prune excessive nodes with (i) Spike-and-Slab Group Lasso (SS-GL), and (ii) Spike-and-Slab Group Horseshoe (SS-GHS) priors, and develop computationally tractable variational inference including continuous relaxation of Bernoulli variables. We establish the contraction rates of the variational posterior of our proposed models as a function of the network topology, layer-wise node cardinalities, and bounds on the network weights. We empirically demonstrate the competitive performance of our models compared to the baseline models in prediction accuracy, model compression, and inference latency.
</details>
<details>
<summary>摘要</summary>
网络复杂性和计算效率已成为深度学习中的两个关键方面。 sparse deep learning 技术可以恢复压缩后的目标函数表示，从而提高深度学习模型的计算效率。Specifically, 我们可以通过结构化稀疏（例如节点稀疏）来压缩深度神经网络，从而提高推理速度、数据传输率和能耗降低。在这篇论文中，我们explore了两种已有的减少技术，lasso和匹配板，用于 Bayesian 神经网络中的模型压缩。为此，我们提出了结构化稀疏 Bayesian 神经网络，并采用了 Spike-and-Slab Group Lasso (SS-GL) 和 Spike-and-Slab Group Horseshoe (SS-GHS) 假设，并开发了可计算性的拟合推理，包括对 Bernoulli 变量进行连续化 relaxation。我们证明了我们提出的模型的变量 posterior 的减少率与网络结构、层次节点数量以及网络参数的 bounds 有关。我们通过实验表明了我们的模型与基eline 模型相比在预测精度、模型压缩和推理速度方面具有竞争力。
</details></li>
</ul>
<hr>
<h2 id="MindMap-Knowledge-Graph-Prompting-Sparks-Graph-of-Thoughts-in-Large-Language-Models"><a href="#MindMap-Knowledge-Graph-Prompting-Sparks-Graph-of-Thoughts-in-Large-Language-Models" class="headerlink" title="MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models"></a>MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09729">http://arxiv.org/abs/2308.09729</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/willing510/MindMap">https://github.com/willing510/MindMap</a></li>
<li>paper_authors: Yilin Wen, Zifeng Wang, Jimeng Sun</li>
<li>for: This paper aims to improve the performance of large language models (LLMs) by incorporating knowledge graphs (KGs) and eliciting the reasoning pathways of LLMs.</li>
<li>methods: The authors propose a prompting pipeline that enables LLMs to comprehend KG inputs and infer with a combined implicit knowledge and retrieved external knowledge. They also investigate eliciting the mind map on which LLMs perform the reasoning and generate answers.</li>
<li>results: The experiments on three question &amp; answering datasets show that MindMap prompting leads to a striking empirical gain, outperforming GPT-4 and other prompting-with-document-retrieval methods. The produced mind map exhibits the reasoning pathways of LLMs grounded on the ontology of knowledge.<details>
<summary>Abstract</summary>
LLMs usually exhibit limitations in their ability to incorporate new knowledge, the generation of hallucinations, and the transparency of their decision-making process. In this paper, we explore how to prompt LLMs with knowledge graphs (KG), working as a remedy to engage LLMs with up-to-date knowledge and elicit the reasoning pathways from LLMs. Specifically, we build a prompting pipeline that endows LLMs with the capability of comprehending KG inputs and inferring with a combined implicit knowledge and the retrieved external knowledge. In addition, we investigate eliciting the mind map on which LLMs perform the reasoning and generate the answers. It is identified that the produced mind map exhibits the reasoning pathways of LLMs grounded on the ontology of knowledge, hence bringing the prospects of probing and gauging LLM inference in production. The experiments on three question & answering datasets also show that MindMap prompting leads to a striking empirical gain. For instance, prompting a GPT-3.5 with MindMap yields an overwhelming performance over GPT-4 consistently. We also demonstrate that with structured facts retrieved from KG, MindMap can outperform a series of prompting-with-document-retrieval methods, benefiting from more accurate, concise, and comprehensive knowledge from KGs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Modeling-Edge-Features-with-Deep-Bayesian-Graph-Networks"><a href="#Modeling-Edge-Features-with-Deep-Bayesian-Graph-Networks" class="headerlink" title="Modeling Edge Features with Deep Bayesian Graph Networks"></a>Modeling Edge Features with Deep Bayesian Graph Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09087">http://arxiv.org/abs/2308.09087</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/diningphil/e-cgmm">https://github.com/diningphil/e-cgmm</a></li>
<li>paper_authors: Daniele Atzeni, Federico Errica, Davide Bacciu, Alessio Micheli</li>
<li>for: 本研究扩展了Contextual Graph Markov Model，一种深度和概率机器学习模型，以模型边Feature的分布。</li>
<li>methods: 我们引入了一个额外的权重网络，将边特征映射到精确的状态中，以便由原始模型使用。</li>
<li>results: 我们在标准图分类 benchmark 上提高性能，并在图 regression 任务中，利用学习的边表示得到了显著的性能提升。<details>
<summary>Abstract</summary>
We propose an extension of the Contextual Graph Markov Model, a deep and probabilistic machine learning model for graphs, to model the distribution of edge features. Our approach is architectural, as we introduce an additional Bayesian network mapping edge features into discrete states to be used by the original model. In doing so, we are also able to build richer graph representations even in the absence of edge features, which is confirmed by the performance improvements on standard graph classification benchmarks. Moreover, we successfully test our proposal in a graph regression scenario where edge features are of fundamental importance, and we show that the learned edge representation provides substantial performance improvements against the original model on three link prediction tasks. By keeping the computational complexity linear in the number of edges, the proposed model is amenable to large-scale graph processing.
</details>
<details>
<summary>摘要</summary>
我们提出一种对 Contextual Graph Markov Model（CGMM）进行扩展，以模型边Feature的分布。我们的方法是建立一个附加的 Bayesian 网络，将边Feature映射到精确的 discrete 状态，以便由原始模型使用。在这样做时，我们还能够建立更加 ricer 的图表示，即使没有边Feature，这得到了标准图分类 benchmark 上的性能提升。此外，我们成功地测试了我们的提议在图 regression 场景中，并证明了学习到的边表示提供了重要的性能提升，在三个链接预测任务中。由于计算复杂度linear化为边的数量，我们的模型适用于大规模图处理。
</details></li>
</ul>
<hr>
<h2 id="Embracing-assay-heterogeneity-with-neural-processes-for-markedly-improved-bioactivity-predictions"><a href="#Embracing-assay-heterogeneity-with-neural-processes-for-markedly-improved-bioactivity-predictions" class="headerlink" title="Embracing assay heterogeneity with neural processes for markedly improved bioactivity predictions"></a>Embracing assay heterogeneity with neural processes for markedly improved bioactivity predictions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09086">http://arxiv.org/abs/2308.09086</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lucian Chan, Marcel Verdonk, Carl Poelking</li>
<li>for: 预测药物的生物活性是计算辅助药物发现中最Difficult和最重要的挑战之一。尽管年来数据收集和整理努力很大，生物活性数据仍然稀缺和多样，因此妨碍了建立准确、传递和可靠的预测模型。实验室数据的内在多样性被聚合实践加以忽略，导致预测模型的准确性受到限制。</li>
<li>methods: 我们提出了一种层次meta-学习框架，利用不同试验方法之间的信息共同作用，成功地考虑到试验方法的多样性。我们显示，该模型在不同蛋白目标和试验方法类型上的粘合能力有很大提高，相比于传统基elines。它可以快速适应新的目标Context使用非常少的观察，因此可以实现大规模的虚拟屏选在早期药物发现阶段。</li>
<li>results: 我们的模型在不同蛋白目标和试验方法类型上的粘合能力有很大提高，相比于传统基elines。它可以快速适应新的目标Context使用非常少的观察，因此可以实现大规模的虚拟屏选在早期药物发现阶段。<details>
<summary>Abstract</summary>
Predicting the bioactivity of a ligand is one of the hardest and most important challenges in computer-aided drug discovery. Despite years of data collection and curation efforts by research organizations worldwide, bioactivity data remains sparse and heterogeneous, thus hampering efforts to build predictive models that are accurate, transferable and robust. The intrinsic variability of the experimental data is further compounded by data aggregation practices that neglect heterogeneity to overcome sparsity. Here we discuss the limitations of these practices and present a hierarchical meta-learning framework that exploits the information synergy across disparate assays by successfully accounting for assay heterogeneity. We show that the model achieves a drastic improvement in affinity prediction across diverse protein targets and assay types compared to conventional baselines. It can quickly adapt to new target contexts using very few observations, thus enabling large-scale virtual screening in early-phase drug discovery.
</details>
<details>
<summary>摘要</summary>
预测药物的生物活性是计算机辅助药物发现中最为困难和重要的挑战。尽管多年的数据收集和整理努力，生物活性数据仍然稀缺和多样化，因此妨碍了建立准确、传递和可靠的预测模型。实验室中的数据变化性更是由数据聚合实践忽略了多样性，以避免稀缺性。本文描述了这些限制，并提出了层次度meta学框架，利用不同试验中的信息相互作用，成功地考虑到了试验的多样性。我们显示，该模型在多种蛋白目标和试验类型上 achieves  significan improvement in affinity prediction compared to conventional baselines.它可以快速适应新的目标Context使用很少观测，因此实现了大规模的虚拟屏选在早期药物发现阶段。
</details></li>
</ul>
<hr>
<h2 id="MovePose-A-High-performance-Human-Pose-Estimation-Algorithm-on-Mobile-and-Edge-Devices"><a href="#MovePose-A-High-performance-Human-Pose-Estimation-Algorithm-on-Mobile-and-Edge-Devices" class="headerlink" title="MovePose: A High-performance Human Pose Estimation Algorithm on Mobile and Edge Devices"></a>MovePose: A High-performance Human Pose Estimation Algorithm on Mobile and Edge Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09084">http://arxiv.org/abs/2308.09084</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongyang Yu, Haoyue Zhang, Zhirui Zhou, Wangpeng An, Yanhong Yang</li>
<li>for: 这个研究旨在提供高精度、实时性的人体姿势估测算法，以应用于移动设备上的人体姿势识别、健身追踪、手语识别等领域。</li>
<li>methods: 这个算法使用了优化的轻量级卷积神经网络，包括三种技术：复原、大小调整的卷积、和坐标分类方法。这些技术可以增强模型的准确性和识别能力。</li>
<li>results: 这个算法在COCO评估数据集上获得了67.7的 Mean Average Precision（mAP）得分，并在Intel i9-10920x CPU和NVIDIA RTX3090 GPU上显示了高效性，分别为69+帧每秒和452+帧每秒。在Android手机上，它的帧率超过11帧每秒。<details>
<summary>Abstract</summary>
We present MovePose, an optimized lightweight convolutional neural network designed specifically for real-time body pose estimation on CPU-based mobile devices. The current solutions do not provide satisfactory accuracy and speed for human posture estimation, and MovePose addresses this gap. It aims to maintain real-time performance while improving the accuracy of human posture estimation for mobile devices. The network produces 17 keypoints for each individual at a rate exceeding 11 frames per second, making it suitable for real-time applications such as fitness tracking, sign language interpretation, and advanced mobile human posture estimation. Our MovePose algorithm has attained an Mean Average Precision (mAP) score of 67.7 on the COCO \cite{cocodata} validation dataset. The MovePose algorithm displayed efficiency with a performance of 69+ frames per second (fps) when run on an Intel i9-10920x CPU. Additionally, it showcased an increased performance of 452+ fps on an NVIDIA RTX3090 GPU. On an Android phone equipped with a Snapdragon 8 + 4G processor, the fps reached above 11. To enhance accuracy, we incorporated three techniques: deconvolution, large kernel convolution, and coordinate classification methods. Compared to basic upsampling, deconvolution is trainable, improves model capacity, and enhances the receptive field. Large kernel convolution strengthens these properties at a decreased computational cost. In summary, MovePose provides high accuracy and real-time performance, marking it a potential tool for a variety of applications, including those focused on mobile-side human posture estimation. The code and models for this algorithm will be made publicly accessible.
</details>
<details>
<summary>摘要</summary>
我们介绍了 MovePose，一种优化的轻量级卷积神经网络，特意设计用于实时人体姿态估计在CPU基于移动设备上。现有解决方案无法提供满意的精度和速度，MovePose填补了这一空白。它希望在实时应用中维持实时性，同时提高人体姿态估计的精度。该网络每秒钟生成17个关键点，适用于实时应用，如健身监测、手语理解和高级移动人体姿态估计。我们的 MovePose算法在 COCO 验证数据集上获得了67.7的 Mean Average Precision（mAP）分数。MovePose算法在 Intel i9-10920x CPU 上运行时达到了69+帧每秒（fps）的性能，并在 NVIDIA RTX3090 GPU 上达到了452+ fps。在一个装备 Snapdragon 8 + 4G 处理器的 Android 手机上，fps 超过11。为了提高精度，我们采用了三种技术：解 convolution，大kernel convolution和坐标分类方法。与基本的upsampling相比，解 convolution 可以学习，提高模型容量，并扩大感知范围。大kernel convolution 强化这些属性，而且降低计算成本。简单来说，MovePose 提供了高精度和实时性，使其成为许多应用的可能工具，包括移动端人体姿态估计。我们将代码和模型公开访问。
</details></li>
</ul>
<hr>
<h2 id="Over-the-Air-Computation-Aided-Federated-Learning-with-the-Aggregation-of-Normalized-Gradient"><a href="#Over-the-Air-Computation-Aided-Federated-Learning-with-the-Aggregation-of-Normalized-Gradient" class="headerlink" title="Over-the-Air Computation Aided Federated Learning with the Aggregation of Normalized Gradient"></a>Over-the-Air Computation Aided Federated Learning with the Aggregation of Normalized Gradient</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09082">http://arxiv.org/abs/2308.09082</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rongfei Fan, Xuming An, Shiyuan Zuo, Han Hu</li>
<li>for: 这个研究旨在提高联合学习（Federated Learning，FL）的通信效率，特别是在无线通信环境下。</li>
<li>methods: 这个研究使用了调升法来实现遥测式计算（Over-the-air Computation），具体包括：每个移动设备更新、增强本地梯度，然后将其传递给服务器；服务器获取所有移动设备的总梯度，然后发送更新后的模型参数给每个移动设备。</li>
<li>results: 这个研究的主要结果包括：在平滑的损失函数下，我们的提案可以在子线性速度下收敛到稳定点；在平滑且强检测的损失函数下，我们的提案可以在线性速度下 achieves 最佳训练损失，并且发现了调升因子的选择与收敛速度之间的贡献。<details>
<summary>Abstract</summary>
Over-the-air computation is a communication-efficient solution for federated learning (FL). In such a system, iterative procedure is performed: Local gradient of private loss function is updated, amplified and then transmitted by every mobile device; the server receives the aggregated gradient all-at-once, generates and then broadcasts updated model parameters to every mobile device. In terms of amplification factor selection, most related works suppose the local gradient's maximal norm always happens although it actually fluctuates over iterations, which may degrade convergence performance. To circumvent this problem, we propose to turn local gradient to be normalized one before amplifying it. Under our proposed method, when the loss function is smooth, we prove our proposed method can converge to stationary point at sub-linear rate. In case of smooth and strongly convex loss function, we prove our proposed method can achieve minimal training loss at linear rate with any small positive tolerance. Moreover, a tradeoff between convergence rate and the tolerance is discovered. To speedup convergence, problems optimizing system parameters are also formulated for above two cases. Although being non-convex, optimal solution with polynomial complexity of the formulated problems are derived. Experimental results show our proposed method can outperform benchmark methods on convergence performance.
</details>
<details>
<summary>摘要</summary>
随空计算是一种通信效率的解决方案 для联合学习（FL）。在这种系统中，每个移动设备都会进行迭代过程：每个设备都会更新、增强并将本地梯度发送给服务器;服务器会收到所有设备的汇总梯度，并生成并广播到每个设备的更新模型参数。在扩大因子选择方面，大多数相关的工作假设了本地梯度的最大 нор Always happens，这可能会降低收敛性能。为了解决这个问题，我们提议将本地梯度转换成normalized的一个前提下，然后扩大它。我们的提议方法可以在smooth的损失函数下降到站点点，并且在smooth和强 convex的损失函数下可以在线性率下 achieves minimal training loss。此外，我们发现了一个tolerance和收敛率之间的负反关系。为了加速收敛，我们还形ulated了一些优化系统参数的问题，并derived了可靠的解。实验结果表明，我们的提议方法可以在收敛性能方面超越参照方法。
</details></li>
</ul>
<hr>
<h2 id="Conditional-Sampling-of-Variational-Autoencoders-via-Iterated-Approximate-Ancestral-Sampling"><a href="#Conditional-Sampling-of-Variational-Autoencoders-via-Iterated-Approximate-Ancestral-Sampling" class="headerlink" title="Conditional Sampling of Variational Autoencoders via Iterated Approximate Ancestral Sampling"></a>Conditional Sampling of Variational Autoencoders via Iterated Approximate Ancestral Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09078">http://arxiv.org/abs/2308.09078</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vaidotas Simkus, Michael U. Gutmann</li>
<li>for: 这个论文是为了解决Variational Autoencoders（VAEs）的受限 sampling问题，特别是在缺失数据填充等应用中。</li>
<li>methods: 这篇论文使用Metropolis-within-Gibbs（MWG）法进行 asymptotically exact 的 conditional sampling，但是发现VAEs学习的结构化层次空间可能会导致MWG采样器受到目标分布的强制性影响。</li>
<li>results: 这篇论文提出了两种解决这些坑爹的方法，并在一系列采样任务中表明了这些方法的改进表现。<details>
<summary>Abstract</summary>
Conditional sampling of variational autoencoders (VAEs) is needed in various applications, such as missing data imputation, but is computationally intractable. A principled choice for asymptotically exact conditional sampling is Metropolis-within-Gibbs (MWG). However, we observe that the tendency of VAEs to learn a structured latent space, a commonly desired property, can cause the MWG sampler to get "stuck" far from the target distribution. This paper mitigates the limitations of MWG: we systematically outline the pitfalls in the context of VAEs, propose two original methods that address these pitfalls, and demonstrate an improved performance of the proposed methods on a set of sampling tasks.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>conditional sampling of variational autoencoders (VAEs) 是在各种应用中需要的，如数据缺失填充，但是计算复杂。 Metropolis-within-Gibbs (MWG) 是一种原则上正确的循环采样方法，但我们发现 VAEs 学习的结构化隐藏空间，是通常所希望的属性，可能会使 MWG 采样器迷失到目标分布的远方。这篇论文解决了 VAEs 中 MWG 采样器的局限性，我们系统地描述了这些坑害的情况，提出了两种原创的方法来解决这些问题，并在一组采样任务中证明了我们的方法的性能有所改善。
</details></li>
</ul>
<hr>
<h2 id="Fast-Decision-Support-for-Air-Traffic-Management-at-Urban-Air-Mobility-Vertiports-using-Graph-Learning"><a href="#Fast-Decision-Support-for-Air-Traffic-Management-at-Urban-Air-Mobility-Vertiports-using-Graph-Learning" class="headerlink" title="Fast Decision Support for Air Traffic Management at Urban Air Mobility Vertiports using Graph Learning"></a>Fast Decision Support for Air Traffic Management at Urban Air Mobility Vertiports using Graph Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09075">http://arxiv.org/abs/2308.09075</a></li>
<li>repo_url: None</li>
<li>paper_authors: Prajit KrisshnaKumar, Jhoel Witter, Steve Paul, Hanvit Cho, Karthik Dantu, Souma Chowdhury</li>
<li>For: This paper proposes a novel approach to Urban Air Mobility - Vertiport Schedule Management (UAM-VSM) using graph reinforcement learning to generate decision-support policies.* Methods: The proposed approach uses two separate graphs to represent the designated physical spots within the vertiport’s airspace and the vehicles being managed, with feature extraction performed through a graph convolutional network (GCN).* Results: The proposed approach is demonstrated to be superior to basic reinforcement learning (with graph embeddings) or random choice baselines through realistic simulations in AirSim applied to scaled down multi-rotor vehicles, with improved performance in terms of delays, safety (no. of collisions), and battery consumption.Here is the simplified Chinese text:* 用: 这篇论文提出了一种新的城市空中交通管理方法（UAM-VSM），使用图约束学习生成决策支持策略。* 方法: 该方法使用两个图来表示Vertiport中的设定物理位置和运行中的 vehicles，并通过图 convolutional neural network (GCN) 进行特征提取。* 结果: 该方法在使用 AirSim 进行大小化多旋翼机器人的实验中，被证明比基本约束学习（图嵌入）或随机选择基eline superior，并且在延迟、安全（碰撞数）和电池消耗等方面提高性能。<details>
<summary>Abstract</summary>
Urban Air Mobility (UAM) promises a new dimension to decongested, safe, and fast travel in urban and suburban hubs. These UAM aircraft are conceived to operate from small airports called vertiports each comprising multiple take-off/landing and battery-recharging spots. Since they might be situated in dense urban areas and need to handle many aircraft landings and take-offs each hour, managing this schedule in real-time becomes challenging for a traditional air-traffic controller but instead calls for an automated solution. This paper provides a novel approach to this problem of Urban Air Mobility - Vertiport Schedule Management (UAM-VSM), which leverages graph reinforcement learning to generate decision-support policies. Here the designated physical spots within the vertiport's airspace and the vehicles being managed are represented as two separate graphs, with feature extraction performed through a graph convolutional network (GCN). Extracted features are passed onto perceptron layers to decide actions such as continue to hover or cruise, continue idling or take-off, or land on an allocated vertiport spot. Performance is measured based on delays, safety (no. of collisions) and battery consumption. Through realistic simulations in AirSim applied to scaled down multi-rotor vehicles, our results demonstrate the suitability of using graph reinforcement learning to solve the UAM-VSM problem and its superiority to basic reinforcement learning (with graph embeddings) or random choice baselines.
</details>
<details>
<summary>摘要</summary>
城市空中 mobilicity (UAM) 提供了一个新的维度，即减压、安全和快速的城市和郊区旅行。这些UAM飞机被设计为从小机场（vertiport）中起降和续航，每个vertiport都包含多个起降和电池充能的位置。由于它们可能会位于密集的城市区域，管理这些 Vertiport 的时间表是一个传统空中交通控制员无法处理的问题，而是需要一个自动化解决方案。这篇论文提出了一种新的方法，即城市空中 mobilicity - Vertiport 时间表管理（UAM-VSM），该方法利用图约束学习生成决策策略。在这种方法中，Vertiport 中的物理位置和管理的车辆被表示为两个分开的图，通过图卷积网络（GCN）进行特征提取。提取出的特征被传递给感知层，以确定动作，如继续悬停或继续飞行、继续停止或 Vertiport 上占用位置。性能被评估基于延迟、安全（车辆碰撞数）和电池消耗。通过在AirSim上进行了扩展的多旋翼飞机的实际 simulations，我们的结果表明，使用图约束学习解决 UAM-VSM 问题是可行的，并且比基本约束学习（图嵌入）或随机选择基eline更高效。
</details></li>
</ul>
<hr>
<h2 id="Joint-Power-Control-and-Data-Size-Selection-for-Over-the-Air-Computation-Aided-Federated-Learning"><a href="#Joint-Power-Control-and-Data-Size-Selection-for-Over-the-Air-Computation-Aided-Federated-Learning" class="headerlink" title="Joint Power Control and Data Size Selection for Over-the-Air Computation Aided Federated Learning"></a>Joint Power Control and Data Size Selection for Over-the-Air Computation Aided Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09072">http://arxiv.org/abs/2308.09072</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anxuming/fedaircomp">https://github.com/anxuming/fedaircomp</a></li>
<li>paper_authors: Xuming An, Rongfei Fan, Shiyuan Zuo, Han Hu, Hai Jiang, Ning Zhang</li>
<li>for: 这篇论文主要是为了提出一种基于联合学习的机器学习方法，以便处理各个移动设备上产生的大量原始数据。</li>
<li>methods: 这篇论文使用了联合学习的方法，并且将所有移动设备的执行模型参数传递到一个基站 (BS) 中进行联合训练。为了实现这一点，论文提出了一种基于无线电信处理的方法，允许所有移动设备同时传送它们的参数映射讯号到 BS。</li>
<li>results: 根据论文的数据分析，这种方法可以对联合学习领域的训练效果提供大幅提高，并且可以实现更好的训练效果和更高的精度。<details>
<summary>Abstract</summary>
Federated learning (FL) has emerged as an appealing machine learning approach to deal with massive raw data generated at multiple mobile devices, {which needs to aggregate the training model parameter of every mobile device at one base station (BS) iteratively}. For parameter aggregating in FL, over-the-air computation is a spectrum-efficient solution, which allows all mobile devices to transmit their parameter-mapped signals concurrently to a BS. Due to heterogeneous channel fading and noise, there exists difference between the BS's received signal and its desired signal, measured as the mean-squared error (MSE). To minimize the MSE, we propose to jointly optimize the signal amplification factors at the BS and the mobile devices as well as the data size (the number of data samples involved in local training) at every mobile device. The formulated problem is challenging to solve due to its non-convexity. To find the optimal solution, with some simplification on cost function and variable replacement, which still preserves equivalence, we transform the changed problem to be a bi-level problem equivalently. For the lower-level problem, optimal solution is found by enumerating every candidate solution from the Karush-Kuhn-Tucker (KKT) condition. For the upper-level problem, the optimal solution is found by exploring its piecewise convexity. Numerical results show that our proposed method can greatly reduce the MSE and can help to improve the training performance of FL compared with benchmark methods.
</details>
<details>
<summary>摘要</summary>
federated learning（FL）已经成为一种有appeal的机器学习方法，用于处理多个移动设备生成的大量原始数据，{需要在一个基站（BS）上进行参数聚合iteratively}.在FL中，在BS上进行参数聚合的过程中，通过空中计算，所有的移动设备可以同时将参数映射的信号传输给BS。由于不同的通道抑降和噪声，BS所接收到的信号和其所需的信号之间存在差异，这被称为平均方差误差（MSE）。为了最小化MSE，我们提议同时优化BS和移动设备中的信号增强因子以及每个移动设备中的数据大小（当地训练中使用的数据样本数）。这个问题的解决具有非 convex 性，为了找到优化的解决方案，我们通过简化成本函数和变量替换，保持等价性，将问题转换为一个相等的二级问题。在下一级问题中，可以通过枚举所有的候选解来找到最优解。在上一级问题中，通过探索其块维度的凹陷性，找到优化的解决方案。 numerics 结果表明，我们的提议方法可以大幅减少MSE，并且可以提高FL的训练性能，与参考方法相比。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/18/cs.LG_2023_08_18/" data-id="clly3dvzq006o09888mnjawvl" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_08_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/18/cs.SD_2023_08_18/" class="article-date">
  <time datetime="2023-08-17T16:00:00.000Z" itemprop="datePublished">2023-08-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/18/cs.SD_2023_08_18/">cs.SD - 2023-08-18 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Compensating-Removed-Frequency-Components-Thwarting-Voice-Spectrum-Reduction-Attacks"><a href="#Compensating-Removed-Frequency-Components-Thwarting-Voice-Spectrum-Reduction-Attacks" class="headerlink" title="Compensating Removed Frequency Components: Thwarting Voice Spectrum Reduction Attacks"></a>Compensating Removed Frequency Components: Thwarting Voice Spectrum Reduction Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09546">http://arxiv.org/abs/2308.09546</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shu Wang, Kun Sun, Qi Li</li>
<li>for: 本研究旨在强化语音识别系统（ASR）面对恶意音频攻击的安全性。</li>
<li>methods: 本文提出了一种名为ACE的听音补偿系统，利用频率成分相关性和扰动敏感性来对抗频谱减少攻击。</li>
<li>results: 实验结果表明，ACE可以效果地减少ASR推理错误率达87.9%，并对剩下的错误分析了六种常见的ASR推理错误类型和其可能的缓解方案。<details>
<summary>Abstract</summary>
Automatic speech recognition (ASR) provides diverse audio-to-text services for humans to communicate with machines. However, recent research reveals ASR systems are vulnerable to various malicious audio attacks. In particular, by removing the non-essential frequency components, a new spectrum reduction attack can generate adversarial audios that can be perceived by humans but cannot be correctly interpreted by ASR systems. It raises a new challenge for content moderation solutions to detect harmful content in audio and video available on social media platforms. In this paper, we propose an acoustic compensation system named ACE to counter the spectrum reduction attacks over ASR systems. Our system design is based on two observations, namely, frequency component dependencies and perturbation sensitivity. First, since the Discrete Fourier Transform computation inevitably introduces spectral leakage and aliasing effects to the audio frequency spectrum, the frequency components with similar frequencies will have a high correlation. Thus, considering the intrinsic dependencies between neighboring frequency components, it is possible to recover more of the original audio by compensating for the removed components based on the remaining ones. Second, since the removed components in the spectrum reduction attacks can be regarded as an inverse of adversarial noise, the attack success rate will decrease when the adversarial audio is replayed in an over-the-air scenario. Hence, we can model the acoustic propagation process to add over-the-air perturbations into the attacked audio. We implement a prototype of ACE and the experiments show ACE can effectively reduce up to 87.9% of ASR inference errors caused by spectrum reduction attacks. Also, by analyzing residual errors, we summarize six general types of ASR inference errors and investigate the error causes and potential mitigation solutions.
</details>
<details>
<summary>摘要</summary>
自动语音识别（ASR）系统提供了多种媒体到文本服务，帮助人类与机器进行交互。然而，最新的研究发现，ASR系统受到了多种恶意音频攻击。特别是，通过去除非关键频率成分，新的频谱减少攻击可以生成对人类可见但是ASR系统无法正确理解的恶意音频。这引起了社交媒体平台上内容审核解决方案检测危害内容的新挑战。在本文中，我们提议一种名为ACE的听音补偿系统，用于对ASR系统中的频谱减少攻击进行防御。我们的系统设计基于两个观察结论：一是频率成分之间的相互依赖关系，二是对攻击音频进行频率补偿可以降低攻击成功率。我们实现了ACE的原型，实验结果表明，ACE可以降低ASR推断错误率达87.9%。此外，我们分析了剩下的错误 residual errors，并总结了六种通用的ASR推断错误类型，并investigate了这些错误的原因和可能的缓解解决方案。
</details></li>
</ul>
<hr>
<h2 id="Generative-Machine-Listener"><a href="#Generative-Machine-Listener" class="headerlink" title="Generative Machine Listener"></a>Generative Machine Listener</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09493">http://arxiv.org/abs/2308.09493</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guanxin Jiang, Lars Villemoes, Arijit Biswas</li>
<li>for: 这篇论文是用于描述一种基于神经网络的音频测试数据生成方法。</li>
<li>methods: 该方法使用各个侵入式听力测试分数来训练神经网络，并可以预测每对参照和编码音频信号的分数分布。</li>
<li>results: 与基准系统相比，使用回归 Mean 分数而不是 GML 方法，出现了较低的异常比率（OR），并且可以轻松地预测 confidence interval（CI）。此外，通过从图像领域中吸取数据增强技术，可以提高 CI 预测精度以及 Pearson 和 Spearman 排名 correlation 的 Mean 分数。<details>
<summary>Abstract</summary>
We show how a neural network can be trained on individual intrusive listening test scores to predict a distribution of scores for each pair of reference and coded input stereo or binaural signals. We nickname this method the Generative Machine Listener (GML), as it is capable of generating an arbitrary amount of simulated listening test data. Compared to a baseline system using regression over mean scores, we observe lower outlier ratios (OR) for the mean score predictions, and obtain easy access to the prediction of confidence intervals (CI). The introduction of data augmentation techniques from the image domain results in a significant increase in CI prediction accuracy as well as Pearson and Spearman rank correlation of mean scores.
</details>
<details>
<summary>摘要</summary>
我们展示了一个神经网络可以根据个别侵入式聆听测试成绩来预测每对参照和压缩音 signals 的分布。我们称这为生成机器听者（GML），因为它可以生成无限多个模拟聆听测试数据。相比基准系统使用平均分布 regression，我们观察了较低的外围比率（OR），并可以轻松地预测信息 intervals（CI）的预测。对于数据增强技术的引入，我们获得了显著提高 CI 预测准确性以及平均分布和斯宾格数字相互联系的关系。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Sampling-Techniques-for-Generating-Melodies-with-a-Transformer-Language-Model"><a href="#Exploring-Sampling-Techniques-for-Generating-Melodies-with-a-Transformer-Language-Model" class="headerlink" title="Exploring Sampling Techniques for Generating Melodies with a Transformer Language Model"></a>Exploring Sampling Techniques for Generating Melodies with a Transformer Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09454">http://arxiv.org/abs/2308.09454</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mathias Rose Bjare, Stefan Lattner, Gerhard Widmer</li>
<li>for: 这个研究探讨了不同采样策略对于训练autoregressive自然语言处理模型的质量产生的影响。</li>
<li>methods: 作者使用高容量变换器模型训练在高度结构化的爱尔兰传统旋律音乐中，并使用分布 truncation 采样技术进行分析。特别是使用核心采样、“典型采样”和传统祖先采样。</li>
<li>results: 研究发现，在优化的情况下，概率 truncation 技术可能会限制多样性和结构性特征，但在低效情况下，它们可能会生成更多的乐曲。<details>
<summary>Abstract</summary>
Research in natural language processing has demonstrated that the quality of generations from trained autoregressive language models is significantly influenced by the used sampling strategy. In this study, we investigate the impact of different sampling techniques on musical qualities such as diversity and structure. To accomplish this, we train a high-capacity transformer model on a vast collection of highly-structured Irish folk melodies and analyze the musical qualities of the samples generated using distribution truncation sampling techniques. Specifically, we use nucleus sampling, the recently proposed "typical sampling", and conventional ancestral sampling. We evaluate the effect of these sampling strategies in two scenarios: optimal circumstances with a well-calibrated model and suboptimal circumstances where we systematically degrade the model's performance. We assess the generated samples using objective and subjective evaluations. We discover that probability truncation techniques may restrict diversity and structural patterns in optimal circumstances, but may also produce more musical samples in suboptimal circumstances.
</details>
<details>
<summary>摘要</summary>
研究自然语言处理已经证明，训练过的自然语言生成模型的质量受到采样策略的影响。在这个研究中，我们研究了不同采样技术对音乐质量的影响。为此，我们使用高容量变换器模型训练在高度结构化的爱尔兰传统散户歌曲中，并对采样技术的影响进行分析。具体来说，我们使用核心采样、“典型采样”和传统祖先采样。我们在两个场景中评估这些采样策略的影响：优化的情况下，模型性能很好，以及逐步降低模型性能的情况。我们使用对jective和主观评估来评估生成的样本。我们发现，抑制采样技术可能会限制多样性和结构性模式，但在优化情况下可能会生成更多的音乐样本。
</details></li>
</ul>
<hr>
<h2 id="TrOMR-Transformer-Based-Polyphonic-Optical-Music-Recognition"><a href="#TrOMR-Transformer-Based-Polyphonic-Optical-Music-Recognition" class="headerlink" title="TrOMR:Transformer-Based Polyphonic Optical Music Recognition"></a>TrOMR:Transformer-Based Polyphonic Optical Music Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09370">http://arxiv.org/abs/2308.09370</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/netease/polyphonic-tromr">https://github.com/netease/polyphonic-tromr</a></li>
<li>paper_authors: Yixuan Li, Huaping Liu, Qiang Jin, Miaomiao Cai, Peng Li</li>
<li>for: 这篇论文是关于音乐Recognition（OMR）技术的研究，旨在提出一种基于变换器的端到端多重音乐识别方法，以提高识别精度。</li>
<li>methods: 该方法使用变换器来实现全球性的音乐理解，并引入一种新的一致损失函数和合理的数据注释方法来提高识别精度。</li>
<li>results: 广泛的实验表明，TrOMR方法在实际场景下的识别精度明显高于现有的OMR方法，特别是在复杂的音乐乐谱上。此外，authors还开发了TrOMR系统和一个摄像头场景数据集，以便在真实世界中进行全页音乐乐谱识别。<details>
<summary>Abstract</summary>
Optical Music Recognition (OMR) is an important technology in music and has been researched for a long time. Previous approaches for OMR are usually based on CNN for image understanding and RNN for music symbol classification. In this paper, we propose a transformer-based approach with excellent global perceptual capability for end-to-end polyphonic OMR, called TrOMR. We also introduce a novel consistency loss function and a reasonable approach for data annotation to improve recognition accuracy for complex music scores. Extensive experiments demonstrate that TrOMR outperforms current OMR methods, especially in real-world scenarios. We also develop a TrOMR system and build a camera scene dataset for full-page music scores in real-world. The code and datasets will be made available for reproducibility.
</details>
<details>
<summary>摘要</summary>
优化音乐识别（OMR）是音乐技术的一个重要方向，已经在长期的研究中。先前的OMR方法通常基于Convolutional Neural Network（CNN） для图像理解和Recurrent Neural Network（RNN） для乐谱符号分类。在这篇论文中，我们提出了一种基于变换器的方法，具有优秀的全球性识别能力，用于端到端多重音乐识别，称为TrOMR。我们还提出了一种新的一致损失函数和一种合理的数据注释方法，以提高复杂乐谱中的识别精度。广泛的实验表明，TrOMR超过当前OMR方法，特别是在真实世界情况下。我们还开发了TrOMR系统和一个摄像头场景数据集，用于全页乐谱的真实世界摄像头识别。代码和数据将被公开，以便重现。
</details></li>
</ul>
<hr>
<h2 id="Lip-Reading-for-Low-resource-Languages-by-Learning-and-Combining-General-Speech-Knowledge-and-Language-specific-Knowledge"><a href="#Lip-Reading-for-Low-resource-Languages-by-Learning-and-Combining-General-Speech-Knowledge-and-Language-specific-Knowledge" class="headerlink" title="Lip Reading for Low-resource Languages by Learning and Combining General Speech Knowledge and Language-specific Knowledge"></a>Lip Reading for Low-resource Languages by Learning and Combining General Speech Knowledge and Language-specific Knowledge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09311">http://arxiv.org/abs/2308.09311</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minsu Kim, Jeong Hun Yeo, Jeongsoo Choi, Yong Man Ro</li>
<li>for: 本研究提出了一种新的唇读框架，特别是针对低资源语言，前一代文献中未得到充分的关注。由于低资源语言缺乏足够的视频文本对应数据来训练模型，因此在这些语言上开发唇读模型被视为挑战。</li>
<li>methods: 我们尝试通过学习通用语言知识，即模型唇部运动的能力，从一种高资源语言来预测语音单位。由于不同语言有一定的共同声母，因此学习一种语言的通用语言知识可以扩展到其他语言。此外，我们还提出了语言特定的储存器（LMDecoder），它将语言特定的音频特征存储在内存银行中，并可以通过语音文本对应数据进行训练。</li>
<li>results: 通过对五种语言（英语、西班牙语、法语、意大利语、葡萄牙语）进行了广泛的实验，证明了我们提出的方法的效iveness。<details>
<summary>Abstract</summary>
This paper proposes a novel lip reading framework, especially for low-resource languages, which has not been well addressed in the previous literature. Since low-resource languages do not have enough video-text paired data to train the model to have sufficient power to model lip movements and language, it is regarded as challenging to develop lip reading models for low-resource languages. In order to mitigate the challenge, we try to learn general speech knowledge, the ability to model lip movements, from a high-resource language through the prediction of speech units. It is known that different languages partially share common phonemes, thus general speech knowledge learned from one language can be extended to other languages. Then, we try to learn language-specific knowledge, the ability to model language, by proposing Language-specific Memory-augmented Decoder (LMDecoder). LMDecoder saves language-specific audio features into memory banks and can be trained on audio-text paired data which is more easily accessible than video-text paired data. Therefore, with LMDecoder, we can transform the input speech units into language-specific audio features and translate them into texts by utilizing the learned rich language knowledge. Finally, by combining general speech knowledge and language-specific knowledge, we can efficiently develop lip reading models even for low-resource languages. Through extensive experiments using five languages, English, Spanish, French, Italian, and Portuguese, the effectiveness of the proposed method is evaluated.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Robust-Audio-Anti-Spoofing-with-Fusion-Reconstruction-Learning-on-Multi-Order-Spectrograms"><a href="#Robust-Audio-Anti-Spoofing-with-Fusion-Reconstruction-Learning-on-Multi-Order-Spectrograms" class="headerlink" title="Robust Audio Anti-Spoofing with Fusion-Reconstruction Learning on Multi-Order Spectrograms"></a>Robust Audio Anti-Spoofing with Fusion-Reconstruction Learning on Multi-Order Spectrograms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09302">http://arxiv.org/abs/2308.09302</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ph-w2000/s2pecnet">https://github.com/ph-w2000/s2pecnet</a></li>
<li>paper_authors: Penghui Wen, Kun Hu, Wenxi Yue, Sen Zhang, Wanlei Zhou, Zhiyong Wang<br>for: 这篇论文旨在提出一种基于多频域特征的深度学习方法，以提高音频反伪测试的效果。methods: 这篇论文使用了一种叫做S2pecNet的深度学习方法，它利用多频域特征来实现音频反伪测试。特别是，这篇论文使用了一种组合构成的混合方法，将多频域特征与时间上的特征混合在一起，以提高反伪测试的精度。results: 这篇论文的实验结果显示，S2pecNet方法可以实现高效的音频反伪测试。具体来说，在ASVspoof2019 LA Challenge上，S2pecNet方法的误分率（EER）为0.77%，与其他方法相比，表现出色。<details>
<summary>Abstract</summary>
Robust audio anti-spoofing has been increasingly challenging due to the recent advancements on deepfake techniques. While spectrograms have demonstrated their capability for anti-spoofing, complementary information presented in multi-order spectral patterns have not been well explored, which limits their effectiveness for varying spoofing attacks. Therefore, we propose a novel deep learning method with a spectral fusion-reconstruction strategy, namely S2pecNet, to utilise multi-order spectral patterns for robust audio anti-spoofing representations. Specifically, spectral patterns up to second-order are fused in a coarse-to-fine manner and two branches are designed for the fine-level fusion from the spectral and temporal contexts. A reconstruction from the fused representation to the input spectrograms further reduces the potential fused information loss. Our method achieved the state-of-the-art performance with an EER of 0.77% on a widely used dataset: ASVspoof2019 LA Challenge.
</details>
<details>
<summary>摘要</summary>
“对于深圳技术的进步，Robust audio anti-spoofing 对于不断增加的挑战。 spectrograms 已经展示了它们在反伪中的能力，但多维 spectral pattern 尚未得到充分利用，这限制了它们在不同的伪装攻击下的效iveness。因此，我们提出了一种基于深度学习的新方法，即 S2pecNet，具有多维 spectral pattern 的融合构想。具体来说，我们将spectral pattern 最多到第二顺序融合在一个course-to-fine的方式下，并设计了两个分支来从spectral和temporal context中获取细节。从融合表示重建到input spectrograms 可以更好地储存可能的融合信息损失。我们的方法在一个广泛使用的dataset上取得了现代最佳性能，EER 为0.77%。”
</details></li>
</ul>
<hr>
<h2 id="V2A-Mapper-A-Lightweight-Solution-for-Vision-to-Audio-Generation-by-Connecting-Foundation-Models"><a href="#V2A-Mapper-A-Lightweight-Solution-for-Vision-to-Audio-Generation-by-Connecting-Foundation-Models" class="headerlink" title="V2A-Mapper: A Lightweight Solution for Vision-to-Audio Generation by Connecting Foundation Models"></a>V2A-Mapper: A Lightweight Solution for Vision-to-Audio Generation by Connecting Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09300">http://arxiv.org/abs/2308.09300</a></li>
<li>repo_url: None</li>
<li>paper_authors: Heng Wang, Jianbo Ma, Santiago Pascual, Richard Cartwright, Weidong Cai</li>
<li>for: 该研究旨在提出一种轻量级的视觉对应声音生成方法，利用基础模型（FM）来解决跨modal生成问题。</li>
<li>methods: 该方法首先调查了视觉CLIP和听音CLAP模型之间的领域差异，然后提出了一种简单 yet effective的映射机制（V2A-Mapper）来桥接这个领域差异。 Conditioned on the translated CLAP embedding, 采用预训练的听音生成FM AudioLDM来生成高质量和视觉对应的声音。</li>
<li>results: 比较前方法，该方法只需快速训练V2A-Mapper，并在两个V2A数据集上进行了广泛的实验和分析。结果表明，使用生成映射器可以提高听音生成的质量和多样性（FD），而使用回归映射器可以提高听音生成的相关性（CS）。在FD和CS两个指标上，该方法与当前状态艺术方法相比，提高了53%和19%。<details>
<summary>Abstract</summary>
Building artificial intelligence (AI) systems on top of a set of foundation models (FMs) is becoming a new paradigm in AI research. Their representative and generative abilities learnt from vast amounts of data can be easily adapted and transferred to a wide range of downstream tasks without extra training from scratch. However, leveraging FMs in cross-modal generation remains under-researched when audio modality is involved. On the other hand, automatically generating semantically-relevant sound from visual input is an important problem in cross-modal generation studies. To solve this vision-to-audio (V2A) generation problem, existing methods tend to design and build complex systems from scratch using modestly sized datasets. In this paper, we propose a lightweight solution to this problem by leveraging foundation models, specifically CLIP, CLAP, and AudioLDM. We first investigate the domain gap between the latent space of the visual CLIP and the auditory CLAP models. Then we propose a simple yet effective mapper mechanism (V2A-Mapper) to bridge the domain gap by translating the visual input between CLIP and CLAP spaces. Conditioned on the translated CLAP embedding, pretrained audio generative FM AudioLDM is adopted to produce high-fidelity and visually-aligned sound. Compared to previous approaches, our method only requires a quick training of the V2A-Mapper. We further analyze and conduct extensive experiments on the choice of the V2A-Mapper and show that a generative mapper is better at fidelity and variability (FD) while a regression mapper is slightly better at relevance (CS). Both objective and subjective evaluation on two V2A datasets demonstrate the superiority of our proposed method compared to current state-of-the-art approaches - trained with 86% fewer parameters but achieving 53% and 19% improvement in FD and CS, respectively.
</details>
<details>
<summary>摘要</summary>
Currently, building artificial intelligence (AI) systems on top of a set of foundation models (FMs) is becoming a new trend in AI research. These FMs can learn representative and generative abilities from vast amounts of data, which can be easily adapted and transferred to a wide range of downstream tasks without needing to be trained from scratch. However, using FMs in cross-modal generation, especially when it comes to audio modality, is still an under-researched area. Specifically, automatically generating semantically relevant sound from visual input is an important problem in cross-modal generation studies.Existing methods for solving this vision-to-audio (V2A) generation problem tend to design and build complex systems from scratch using modestly sized datasets. In this paper, we propose a lightweight solution to this problem by leveraging foundation models, specifically CLIP, CLAP, and AudioLDM. We first investigate the domain gap between the latent space of the visual CLIP and the auditory CLAP models. Then, we propose a simple yet effective mapper mechanism (V2A-Mapper) to bridge the domain gap by translating the visual input between CLIP and CLAP spaces. Conditioned on the translated CLAP embedding, pretrained audio generative FM AudioLDM is adopted to produce high-fidelity and visually-aligned sound.Compared to previous approaches, our method only requires a quick training of the V2A-Mapper. We further analyze and conduct extensive experiments on the choice of the V2A-Mapper and show that a generative mapper is better at fidelity and variability (FD) while a regression mapper is slightly better at relevance (CS). Both objective and subjective evaluation on two V2A datasets demonstrate the superiority of our proposed method compared to current state-of-the-art approaches. We trained our method with 86% fewer parameters but achieved 53% and 19% improvement in FD and CS, respectively.
</details></li>
</ul>
<hr>
<h2 id="Bridging-High-Quality-Audio-and-Video-via-Language-for-Sound-Effects-Retrieval-from-Visual-Queries"><a href="#Bridging-High-Quality-Audio-and-Video-via-Language-for-Sound-Effects-Retrieval-from-Visual-Queries" class="headerlink" title="Bridging High-Quality Audio and Video via Language for Sound Effects Retrieval from Visual Queries"></a>Bridging High-Quality Audio and Video via Language for Sound Effects Retrieval from Visual Queries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09089">http://arxiv.org/abs/2308.09089</a></li>
<li>repo_url: None</li>
<li>paper_authors: Julia Wilkins, Justin Salamon, Magdalena Fuentes, Juan Pablo Bello, Oriol Nieto</li>
<li>for: 这个论文的目的是提出一种多模态框架，用于根据视频帧来检索高质量的声音效果（SFX）。</li>
<li>methods: 这个论文使用了大型语言模型和基础视觉语言模型来桥接高质量的音频和视频，从而创建了一个高度可扩展的自动音频视频数据筛选管道。它还使用预训练的音频和视频编码器来训练一种对比学习基于的检索系统。</li>
<li>results: 论文的实验结果表明，使用这个多模态框架，可以significantly outperform基线值在高质量声音效果检索任务上。此外，这个系统还能够在各种不同的数据集上进行 generale化，并且在用户研究中，人们对这个系统中的SFX Retrieval结果表示满意。<details>
<summary>Abstract</summary>
Finding the right sound effects (SFX) to match moments in a video is a difficult and time-consuming task, and relies heavily on the quality and completeness of text metadata. Retrieving high-quality (HQ) SFX using a video frame directly as the query is an attractive alternative, removing the reliance on text metadata and providing a low barrier to entry for non-experts. Due to the lack of HQ audio-visual training data, previous work on audio-visual retrieval relies on YouTube (in-the-wild) videos of varied quality for training, where the audio is often noisy and the video of amateur quality. As such it is unclear whether these systems would generalize to the task of matching HQ audio to production-quality video. To address this, we propose a multimodal framework for recommending HQ SFX given a video frame by (1) leveraging large language models and foundational vision-language models to bridge HQ audio and video to create audio-visual pairs, resulting in a highly scalable automatic audio-visual data curation pipeline; and (2) using pre-trained audio and visual encoders to train a contrastive learning-based retrieval system. We show that our system, trained using our automatic data curation pipeline, significantly outperforms baselines trained on in-the-wild data on the task of HQ SFX retrieval for video. Furthermore, while the baselines fail to generalize to this task, our system generalizes well from clean to in-the-wild data, outperforming the baselines on a dataset of YouTube videos despite only being trained on the HQ audio-visual pairs. A user study confirms that people prefer SFX retrieved by our system over the baseline 67% of the time both for HQ and in-the-wild data. Finally, we present ablations to determine the impact of model and data pipeline design choices on downstream retrieval performance. Please visit our project website to listen to and view our SFX retrieval results.
</details>
<details>
<summary>摘要</summary>
寻找符合视频中的声音效果（SFX）是一项复杂和时间consuming的任务，它取决于视频中文本 metadata 的质量和完整性。使用视频帧直接作为查询来检索高品质（HQ）声音的方法是一种吸引人的alternative，它可以消除文本 metadata 的依赖关系，并提供低门槛 для非专家。由于缺乏 HQ 音频视频培训数据，过去的声音视频检索工作通常使用 YouTube （在野）视频进行培训，这些视频的音频 oftentimes 噪音且视频质量不高。因此，是否这些系统能够通用到高品质音频与生产质量视频之间的匹配问题存在uncertainty。为解决这个问题，我们提议一种多模态框架，用于基于视频帧提供 HQ 声音效果的推荐，包括：1. 利用大语言模型和基础视频语言模型来桥接 HQ 音频和视频，从而创建高可扩展的自动音频视频数据纪要管道。2. 使用预训练的音频和视觉编码器来培训对比学习基于检索系统。我们的系统，通过我们自动生成的数据纪要管道进行训练，与基于野外数据的基eline 相比，显著提高了高品质声音效果检索任务的性能。此外，我们的系统可以从清晰到野外数据中进行扩展，并在 YouTube 视频集上表现出色，即使只受训练于 HQ 音频视频对。人们在用户研究中表示，他们67% 的时间 prefer SFX 被我们的系统检索出来，而不是基eline 。最后，我们提供了一系列ablation来评估模型和数据管道设计的影响。请参考我们项目网站来听取和查看我们的 SFX 检索结果。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/18/cs.SD_2023_08_18/" data-id="clly3dw11009v09887bjzgkv1" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_08_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/18/eess.IV_2023_08_18/" class="article-date">
  <time datetime="2023-08-17T16:00:00.000Z" itemprop="datePublished">2023-08-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/18/eess.IV_2023_08_18/">eess.IV - 2023-08-18 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Uncertainty-based-quality-assurance-of-carotid-artery-wall-segmentation-in-black-blood-MRI"><a href="#Uncertainty-based-quality-assurance-of-carotid-artery-wall-segmentation-in-black-blood-MRI" class="headerlink" title="Uncertainty-based quality assurance of carotid artery wall segmentation in black-blood MRI"></a>Uncertainty-based quality assurance of carotid artery wall segmentation in black-blood MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09538">http://arxiv.org/abs/2308.09538</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/miagrouput/carotid-segmentation">https://github.com/miagrouput/carotid-segmentation</a></li>
<li>paper_authors: Elina Thibeau-Sutre, Dieuwertje Alblas, Sophie Buurman, Christoph Brune, Jelmer M. Wolterink</li>
<li>for: 这个研究旨在应用深度学习模型来自动确保大规模数据集的质量。</li>
<li>methods: 这种方法使用全自动的算法来 segmentation 黑血栓 MRI 中的血管壁。</li>
<li>results: 研究发现，包括不确定性测量不会下降 segmentation 的质量，不确定性指标可以作为质量指标，并且可以检测参与者级别的低质量 segmentation。<details>
<summary>Abstract</summary>
The application of deep learning models to large-scale data sets requires means for automatic quality assurance. We have previously developed a fully automatic algorithm for carotid artery wall segmentation in black-blood MRI that we aim to apply to large-scale data sets. This method identifies nested artery walls in 3D patches centered on the carotid artery. In this study, we investigate to what extent the uncertainty in the model predictions for the contour location can serve as a surrogate for error detection and, consequently, automatic quality assurance. We express the quality of automatic segmentations using the Dice similarity coefficient. The uncertainty in the model's prediction is estimated using either Monte Carlo dropout or test-time data augmentation. We found that (1) including uncertainty measurements did not degrade the quality of the segmentations, (2) uncertainty metrics provide a good proxy of the quality of our contours if the center found during the first step is enclosed in the lumen of the carotid artery and (3) they could be used to detect low-quality segmentations at the participant level. This automatic quality assurance tool might enable the application of our model in large-scale data sets.
</details>
<details>
<summary>摘要</summary>
深度学习模型在大规模数据集应用需要自动质量控制方法。我们之前已经开发了一种完全自动的护层动脉增强MRI图像分割算法，我们计划将其应用于大规模数据集。这种方法可以在3D补充中心于护层动脉的patch中标识嵌入的护层动脉增强。在这项研究中，我们研究了模型预测结果中的不确定性是否可以作为自动质量控制的代理。我们使用 dice相似度系数来衡量自动分割的质量。我们发现：（1）包括不确定性测量并不下降自动分割的质量，（2）不确定性指标可以作为护层动脉中心在护层动脉血栓中心的质量代理，（3）它们可以用来检测参与者级别的低质量分割。这种自动质量控制工具可能会使我们的模型在大规模数据集上应用。
</details></li>
</ul>
<hr>
<h2 id="INR-LDDMM-Fluid-based-Medical-Image-Registration-Integrating-Implicit-Neural-Representation-and-Large-Deformation-Diffeomorphic-Metric-Mapping"><a href="#INR-LDDMM-Fluid-based-Medical-Image-Registration-Integrating-Implicit-Neural-Representation-and-Large-Deformation-Diffeomorphic-Metric-Mapping" class="headerlink" title="INR-LDDMM: Fluid-based Medical Image Registration Integrating Implicit Neural Representation and Large Deformation Diffeomorphic Metric Mapping"></a>INR-LDDMM: Fluid-based Medical Image Registration Integrating Implicit Neural Representation and Large Deformation Diffeomorphic Metric Mapping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09473">http://arxiv.org/abs/2308.09473</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chulong Zhang, Xiaokun Liang</li>
<li>for: 这篇论文是用于医疗影像注册的流程基础架构，使用隐藏 нейрон表示法和大型可扭度Diffusion Metric Mapping（LDDMM）。</li>
<li>methods: 这篇论文使用了多层感知神经网络（MLP）来生成速度，同时估计速度和影像相似性。它还采用了从粗到细的方法来解决医疗影像注册方法中的严重扭转问题。</li>
<li>results: 这篇论文在一个包含50名病人的CT-CBCT资料集上验证了其方法，并使用转移过的标签作为评估指标。与现有方法相比，这篇论文的方法实现了现场状况的最佳性能。<details>
<summary>Abstract</summary>
We propose a flow-based registration framework of medical images based on implicit neural representation. By integrating implicit neural representation and Large Deformable Diffeomorphic Metric Mapping (LDDMM), we employ a Multilayer Perceptron (MLP) as a velocity generator while optimizing velocity and image similarity. Moreover, we adopt a coarse-to-fine approach to address the challenge of deformable-based registration methods dropping into local optimal solutions, thus aiding the management of significant deformations in medical image registration. Our algorithm has been validated on a paired CT-CBCT dataset of 50 patients,taking the dice coefficient of transferred annotations as an evaluation metric. Compared to existing methods, our approach achieves the state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
我们提出一种基于隐藏神经表示的医学图像注册框架。通过结合隐藏神经表示和大型可变截面度量mapping（LDDMM），我们使用多层感知器（MLP）作为速度生成器，同时优化速度和图像相似性。此外，我们采用宽泛到细节的方法来Addressing the challenge of deformable-based registration methods dropping into local optimal solutions，以避免医学图像注册中显著的形态变换问题。我们的算法在50名患者的CT-CBCT对照集上进行验证，并根据 transferred annotations的 dice coefficient 作为评价指标。与现有方法相比，我们的方法实现了状态的最佳性。
</details></li>
</ul>
<hr>
<h2 id="Quantitative-Susceptibility-Mapping-through-Model-based-Deep-Image-Prior-MoDIP"><a href="#Quantitative-Susceptibility-Mapping-through-Model-based-Deep-Image-Prior-MoDIP" class="headerlink" title="Quantitative Susceptibility Mapping through Model-based Deep Image Prior (MoDIP)"></a>Quantitative Susceptibility Mapping through Model-based Deep Image Prior (MoDIP)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09467">http://arxiv.org/abs/2308.09467</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhuang Xiong, Yang Gao, Yin Liu, Amir Fazlollahi, Peter Nestor, Feng Liu, Hongfu Sun</li>
<li>for: 提高Quantitative Susceptibility Mapping（QSM）中的批处理参数不同对象的普适性。</li>
<li>methods: 提出了一种新的无需训练的模型基于深度学习方法，称为MoDIP（模型基于深度图像先验）。MoDIP包括一个小型、未训练的网络和数据准确优化（DFO）模块。</li>
<li>results: MoDIP在不同扫描参数下的QSM普适性问题中表现出色，与深度学习和传统迭代方法相比，实现了32%的准确性提高，并且比传统DIP基于方法快33%，可以在4.5分钟内完成3D高分辨率图像重建。<details>
<summary>Abstract</summary>
The data-driven approach of supervised learning methods has limited applicability in solving dipole inversion in Quantitative Susceptibility Mapping (QSM) with varying scan parameters across different objects. To address this generalization issue in supervised QSM methods, we propose a novel training-free model-based unsupervised method called MoDIP (Model-based Deep Image Prior). MoDIP comprises a small, untrained network and a Data Fidelity Optimization (DFO) module. The network converges to an interim state, acting as an implicit prior for image regularization, while the optimization process enforces the physical model of QSM dipole inversion. Experimental results demonstrate MoDIP's excellent generalizability in solving QSM dipole inversion across different scan parameters. It exhibits robustness against pathological brain QSM, achieving over 32% accuracy improvement than supervised deep learning and traditional iterative methods. It is also 33% more computationally efficient and runs 4 times faster than conventional DIP-based approaches, enabling 3D high-resolution image reconstruction in under 4.5 minutes.
</details>
<details>
<summary>摘要</summary>
supervised学习方法的数据驱动方法在量子吸引mapping（QSM）中的不同对象扫描参数下有限的应用。为解决总化issue在超级vised QSM方法中，我们提出了一种新的无需训练的模型基于Unsupervised方法called MoDIP（模型基于深度图像先验）。MoDIP包括一个小型、未训练的网络和数据准确优化（DFO）模块。网络在某些扫描参数下 converges to an interim state，作为图像规范化的隐式先验，而优化过程检查QSM扫描器的物理模型。实验结果表明，MoDIP在不同扫描参数下的QSM扫描中具有出色的总化性能，与超级vised深度学习和传统迭代方法相比，具有32%的精度提高。此外，MoDIP还比折衔DIP基本方法更加快速，只需4.5分钟内可以完成3D高分辨率图像重建。
</details></li>
</ul>
<hr>
<h2 id="Causal-SAR-ATR-with-Limited-Data-via-Dual-Invariance"><a href="#Causal-SAR-ATR-with-Limited-Data-via-Dual-Invariance" class="headerlink" title="Causal SAR ATR with Limited Data via Dual Invariance"></a>Causal SAR ATR with Limited Data via Dual Invariance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09412">http://arxiv.org/abs/2308.09412</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cwwangsaratr/saratr_causal_dual_invariance">https://github.com/cwwangsaratr/saratr_causal_dual_invariance</a></li>
<li>paper_authors: Chenwei Wang, You Qin, Li Li, Siyi Luo, Yulin Huang, Jifang Pei, Yin Zhang, Jianyu Yang</li>
<li>for: 本文旨在提高受限制SAR数据的自动目标识别（SAR ATR）的迷你性。</li>
<li>methods: 本文提出了一个 causal ATR 模型，表明有限SAR数据中的噪声（N）会对特征X中的污染，导致SAR ATR的迷你性。为了解决这个问题，本文提出了一种双Inv隐含proxy和噪声不变损失。</li>
<li>results: 实验结果表明，提出的方法在三个标准数据集上达到了更高的性能。<details>
<summary>Abstract</summary>
Synthetic aperture radar automatic target recognition (SAR ATR) with limited data has recently been a hot research topic to enhance weak generalization. Despite many excellent methods being proposed, a fundamental theory is lacked to explain what problem the limited SAR data causes, leading to weak generalization of ATR. In this paper, we establish a causal ATR model demonstrating that noise $N$ that could be blocked with ample SAR data, becomes a confounder with limited data for recognition. As a result, it has a detrimental causal effect damaging the efficacy of feature $X$ extracted from SAR images, leading to weak generalization of SAR ATR with limited data. The effect of $N$ on feature can be estimated and eliminated by using backdoor adjustment to pursue the direct causality between $X$ and the predicted class $Y$. However, it is difficult for SAR images to precisely estimate and eliminated the effect of $N$ on $X$. The limited SAR data scarcely powers the majority of existing optimization losses based on empirical risk minimization (ERM), thus making it difficult to effectively eliminate $N$'s effect. To tackle with difficult estimation and elimination of $N$'s effect, we propose a dual invariance comprising the inner-class invariant proxy and the noise-invariance loss. Motivated by tackling change with invariance, the inner-class invariant proxy facilitates precise estimation of $N$'s effect on $X$ by obtaining accurate invariant features for each class with the limited data. The noise-invariance loss transitions the ERM's data quantity necessity into a need for noise environment annotations, effectively eliminating $N$'s effect on $X$ by cleverly applying the previous $N$'s estimation as the noise environment annotations. Experiments on three benchmark datasets indicate that the proposed method achieves superior performance.
</details>
<details>
<summary>摘要</summary>
射频干扰自动目标识别（SAR ATR）受限数据的研究在最近几年来非常热门，但是它们的基本理论仍然缺乏，这使得SAR ATR的训练受限数据的情况下存在强化问题。在这篇文章中，我们建立了一个导向性的ATR模型，证明了噪音N，可以阻据了充足的SAR数据，在有限数据情况下成为识别的干扰因子。这使得噪音N对特征X提出了负面的导向效应，导致SAR ATR的训练受限数据下存在弱化问题。但是，噪音N对特征X的影响可以通过后门调整来追求直接的 causality between X和预测的类别Y。即使SAR图像具有限制的数据能力，也可以通过内部的类别对称代理和噪音不对称损失来实现这一目的。实验结果显示，提案的方法在三个benchmark dataset上具有superior的表现。
</details></li>
</ul>
<hr>
<h2 id="Unveiling-Causalities-in-SAR-ATR-A-Causal-Interventional-Approach-for-Limited-Data"><a href="#Unveiling-Causalities-in-SAR-ATR-A-Causal-Interventional-Approach-for-Limited-Data" class="headerlink" title="Unveiling Causalities in SAR ATR: A Causal Interventional Approach for Limited Data"></a>Unveiling Causalities in SAR ATR: A Causal Interventional Approach for Limited Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09396">http://arxiv.org/abs/2308.09396</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenwei Wang, Xin Chen, You Qin, Siyi Luo, Yulin Huang, Jifang Pei, Jianyu Yang</li>
<li>For:	+ The paper aims to address the limited training data problem in synthetic aperture radar (SAR) automatic target recognition (ATR) by proposing a causal interventional ATR method (CIATR).* Methods:	+ The proposed CIATR method uses causal inference to understand the causal relationships among the key factors in ATR, and employs a structural causal model (SCM) to mitigate the spurious correlation introduced by limited SAR data.	+ The method includes data augmentation with spatial-frequency domain hybrid transformation and a feature discrimination approach with hybrid similarity measurement to measure and mitigate the impacts of varying imaging conditions on the extracted features from SAR images.* Results:	+ The proposed CIATR method has been experimented and compared on the moving and stationary target acquisition and recognition (MSTAR) and OpenSARship datasets, and has shown effective performance in pursuing the true causality between SAR images and the corresponding classes even with limited SAR data.<details>
<summary>Abstract</summary>
Synthetic aperture radar automatic target recognition (SAR ATR) methods fall short with limited training data. In this letter, we propose a causal interventional ATR method (CIATR) to formulate the problem of limited SAR data which helps us uncover the ever-elusive causalities among the key factors in ATR, and thus pursue the desired causal effect without changing the imaging conditions. A structural causal model (SCM) is comprised using causal inference to help understand how imaging conditions acts as a confounder introducing spurious correlation when SAR data is limited. This spurious correlation among SAR images and the predicted classes can be fundamentally tackled with the conventional backdoor adjustments. An effective implement of backdoor adjustments is proposed by firstly using data augmentation with spatial-frequency domain hybrid transformation to estimate the potential effect of varying imaging conditions on SAR images. Then, a feature discrimination approach with hybrid similarity measurement is introduced to measure and mitigate the structural and vector angle impacts of varying imaging conditions on the extracted features from SAR images. Thus, our CIATR can pursue the true causality between SAR images and the corresponding classes even with limited SAR data. Experiments and comparisons conducted on the moving and stationary target acquisition and recognition (MSTAR) and OpenSARship datasets have shown the effectiveness of our method with limited SAR data.
</details>
<details>
<summary>摘要</summary>
Synthetic aperture radar自动目标识别（SAR ATR）方法受有限训练数据的限制。在这封信中，我们提议一种 causal interventional ATR 方法（CIATR）来处理有限 SAR 数据的问题，并帮助我们探索 SAR 图像中隐藏的 causalities 以及它们如何影响 ATR 的性能。我们使用 causal inference 来理解如何限制 SAR 图像的捕捉条件引入了假 correlation，并且提出了一种基于 backdoor adjustments 的方法来解决这种假 correlation。我们首先使用数据扩充和空间频率域混合变换来估计限制 SAR 图像的可变 imaging 条件对 SAR 图像的影响。然后，我们引入了一种 hybrid similarity measurement 来衡量和减少限制 SAR 图像的结构和向量角度的影响。因此，我们的 CIATR 可以追求有限 SAR 数据中 true causality  между SAR 图像和对应的类别。我们在 MSTAR 和 OpenSARship 数据集上进行了实验和比较，并证明了我们的方法在有限 SAR 数据情况下的效果。
</details></li>
</ul>
<hr>
<h2 id="SAMedOCT-Adapting-Segment-Anything-Model-SAM-for-Retinal-OCT"><a href="#SAMedOCT-Adapting-Segment-Anything-Model-SAM-for-Retinal-OCT" class="headerlink" title="SAMedOCT: Adapting Segment Anything Model (SAM) for Retinal OCT"></a>SAMedOCT: Adapting Segment Anything Model (SAM) for Retinal OCT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09331">http://arxiv.org/abs/2308.09331</a></li>
<li>repo_url: None</li>
<li>paper_authors: Botond Fazekas, José Morano, Dmitrii Lachinov, Guilherme Aresta, Hrvoje Bogunović</li>
<li>for: 这篇论文是为了评估Segment Anything Model（SAM）在Retinal OCT扫描影像分类中的可行性和优势。</li>
<li>methods: 这篇论文使用了SAM模型和其修改版本，并与现有的State-of-the-art retinal fluid segmentation方法进行比较。</li>
<li>results: 研究发现，这些修改版本的SAM模型在大量的Retinal OCT扫描影像 dataset上具有了优秀的分类能力，但在一些情况下仍落后于现有的方法。<details>
<summary>Abstract</summary>
The Segment Anything Model (SAM) has gained significant attention in the field of image segmentation due to its impressive capabilities and prompt-based interface. While SAM has already been extensively evaluated in various domains, its adaptation to retinal OCT scans remains unexplored. To bridge this research gap, we conduct a comprehensive evaluation of SAM and its adaptations on a large-scale public dataset of OCTs from RETOUCH challenge. Our evaluation covers diverse retinal diseases, fluid compartments, and device vendors, comparing SAM against state-of-the-art retinal fluid segmentation methods. Through our analysis, we showcase adapted SAM's efficacy as a powerful segmentation model in retinal OCT scans, although still lagging behind established methods in some circumstances. The findings highlight SAM's adaptability and robustness, showcasing its utility as a valuable tool in retinal OCT image analysis and paving the way for further advancements in this domain.
</details>
<details>
<summary>摘要</summary>
Segment Anything Model (SAM) 已经在图像分割领域引起了广泛的关注，因为它的印象深刻和提示式接口。虽然 SAM 已经在不同领域进行了广泛的评估，但它在Retinal OCT扫描图像中的适用仍然未得到探索。为了填补这个研究差距，我们进行了大规模公共数据集的 RETOUCH 挑战中的 OCT 图像的全面评估。我们的评估覆盖了多种Retinal疾病、液体腔和设备制造商，与现有的Retinal液体分割方法进行比较。通过我们的分析，我们发现了适应 SAM 的可能性和强大性，尽管在某些情况下仍然落后于已知方法。这些发现表明 SAM 在Retinal OCT 图像分析中的可用性和稳定性，并且作为一个有价值的工具，可以在这个领域进一步发展。
</details></li>
</ul>
<hr>
<h2 id="Advancing-Intra-operative-Precision-Dynamic-Data-Driven-Non-Rigid-Registration-for-Enhanced-Brain-Tumor-Resection-in-Image-Guided-Neurosurgery"><a href="#Advancing-Intra-operative-Precision-Dynamic-Data-Driven-Non-Rigid-Registration-for-Enhanced-Brain-Tumor-Resection-in-Image-Guided-Neurosurgery" class="headerlink" title="Advancing Intra-operative Precision: Dynamic Data-Driven Non-Rigid Registration for Enhanced Brain Tumor Resection in Image-Guided Neurosurgery"></a>Advancing Intra-operative Precision: Dynamic Data-Driven Non-Rigid Registration for Enhanced Brain Tumor Resection in Image-Guided Neurosurgery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10868">http://arxiv.org/abs/2308.10868</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nikos Chrisochoides, Andriy Fedorov, Fotis Drakopoulos, Andriy Kot, Yixun Liu, Panos Foteinos, Angelos Angelopoulos, Olivier Clatz, Nicholas Ayache, Peter M. Black, Alex J. Golby, Ron Kikinis</li>
<li>for: 用于 neurosurgery 中医学影像识别肿瘤和关键结构</li>
<li>methods: 使用 Dynamic Data-Driven Non-Rigid Registration (NRR) 方法对医学影像进行调整，以考虑 operation 中脑膜的变化</li>
<li>results: 实现 NRR 结果在临床时间限制内，并通过分布式计算和机器学习提高 registration 精度，同时描述了在操作室中使用 NRR 的挑战<details>
<summary>Abstract</summary>
During neurosurgery, medical images of the brain are used to locate tumors and critical structures, but brain tissue shifts make pre-operative images unreliable for accurate removal of tumors. Intra-operative imaging can track these deformations but is not a substitute for pre-operative data. To address this, we use Dynamic Data-Driven Non-Rigid Registration (NRR), a complex and time-consuming image processing operation that adjusts the pre-operative image data to account for intra-operative brain shift. Our review explores a specific NRR method for registering brain MRI during image-guided neurosurgery and examines various strategies for improving the accuracy and speed of the NRR method. We demonstrate that our implementation enables NRR results to be delivered within clinical time constraints while leveraging Distributed Computing and Machine Learning to enhance registration accuracy by identifying optimal parameters for the NRR method. Additionally, we highlight challenges associated with its use in the operating room.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese: During 神经外科手术，医疗影像被用来确定肿瘤和重要结构，但是脑组织的运动使得先前的影像无法准确地识别肿瘤。 intra-operative imaging可以跟踪这些变形，但是不能代替先前的数据。为解决这个问题，我们使用动态数据驱动非固定注册（NRR）方法，将先前的影像数据调整以 compte for intra-operative brain shift。我们的文章检讨了一种特定的NRR方法，用于在图像引导神经外科手术中注册脑MRI。我们还检讨了不同的策略来提高NRR方法的准确性和速度。我们的实现可以在临床时间限制内提供NRR结果，并利用分布式计算和机器学习来提高注册精度。此外，我们还 highlighted some challenges associated with its use in the operating room.Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="JPEG-Quantized-Coefficient-Recovery-via-DCT-Domain-Spatial-Frequential-Transformer"><a href="#JPEG-Quantized-Coefficient-Recovery-via-DCT-Domain-Spatial-Frequential-Transformer" class="headerlink" title="JPEG Quantized Coefficient Recovery via DCT Domain Spatial-Frequential Transformer"></a>JPEG Quantized Coefficient Recovery via DCT Domain Spatial-Frequential Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09110">http://arxiv.org/abs/2308.09110</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingyu Ouyang, Zhenzhong Chen</li>
<li>for: 本研究旨在提出一种基于DCT频域的JPEG压缩图像恢复方法，以提高压缩图像的Restoration效果。</li>
<li>methods: 我们提出了一种名为DCTransformer的双支持结构，通过捕捉DCT快 Fourier Transform的空间-频域相关性来提高图像Restoration效果。此外，我们还采用量化矩阵嵌入和同色度组分对齐来扩展模型的应用范围。</li>
<li>results: 我们的DCTransformer模型在对压缩JPEG图像进行恢复时表现出色，在各种质量因素下都能够达到更高的Restoration效果。<details>
<summary>Abstract</summary>
JPEG compression adopts the quantization of Discrete Cosine Transform (DCT) coefficients for effective bit-rate reduction, whilst the quantization could lead to a significant loss of important image details. Recovering compressed JPEG images in the frequency domain has attracted more and more attention recently, in addition to numerous restoration approaches developed in the pixel domain. However, the current DCT domain methods typically suffer from limited effectiveness in handling a wide range of compression quality factors, or fall short in recovering sparse quantized coefficients and the components across different colorspace. To address these challenges, we propose a DCT domain spatial-frequential Transformer, named as DCTransformer. Specifically, a dual-branch architecture is designed to capture both spatial and frequential correlations within the collocated DCT coefficients. Moreover, we incorporate the operation of quantization matrix embedding, which effectively allows our single model to handle a wide range of quality factors, and a luminance-chrominance alignment head that produces a unified feature map to align different-sized luminance and chrominance components. Our proposed DCTransformer outperforms the current state-of-the-art JPEG artifact removal techniques, as demonstrated by our extensive experiments.
</details>
<details>
<summary>摘要</summary>
JPEG压缩采用了离散余弦变换（DCT）系数的归一化进行有效的比特率减少，而这可能会导致重要的图像细节产生损失。在频域中还原压缩的JPEG图像已经引起了越来越多的关注，而 besides numerous restoration approaches developed in the pixel domain. However, the current DCT domain methods typically suffer from limited effectiveness in handling a wide range of compression quality factors, or fall short in recovering sparse quantized coefficients and the components across different color spaces. To address these challenges, we propose a DCT domain spatial-frequential Transformer, named as DCTransformer. Specifically, a dual-branch architecture is designed to capture both spatial and frequential correlations within the collocated DCT coefficients. Moreover, we incorporate the operation of quantization matrix embedding, which effectively allows our single model to handle a wide range of quality factors, and a luminance-chrominance alignment head that produces a unified feature map to align different-sized luminance and chrominance components. Our proposed DCTransformer outperforms the current state-of-the-art JPEG artifact removal techniques, as demonstrated by our extensive experiments.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/18/eess.IV_2023_08_18/" data-id="clly3dw2k00es0988eiqiajyt" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/4/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/">6</a><a class="page-number" href="/page/7/">7</a><span class="space">&hellip;</span><a class="page-number" href="/page/27/">27</a><a class="extend next" rel="next" href="/page/6/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">24</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">24</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">24</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">57</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">55</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">104</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">165</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
