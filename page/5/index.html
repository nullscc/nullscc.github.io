
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/page/5/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.SD_2023_08_04" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/04/cs.SD_2023_08_04/" class="article-date">
  <time datetime="2023-08-03T16:00:00.000Z" itemprop="datePublished">2023-08-04</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/04/cs.SD_2023_08_04/">cs.SD - 2023-08-04 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Efficient-Monaural-Speech-Enhancement-using-Spectrum-Attention-Fusion"><a href="#Efficient-Monaural-Speech-Enhancement-using-Spectrum-Attention-Fusion" class="headerlink" title="Efficient Monaural Speech Enhancement using Spectrum Attention Fusion"></a>Efficient Monaural Speech Enhancement using Spectrum Attention Fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02263">http://arxiv.org/abs/2308.02263</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinyu Long, Jetic Gū, Binhao Bai, Zhibo Yang, Ping Wei, Junli Li</li>
<li>for: 提高自动 speech 处理管道中的speech减噪性能，以提高干扰 speech 的分离效果。</li>
<li>methods: 提出了一种 Spectrum Attention Fusion 技术，用于将自我注意力 fusion 与 spectral 特征 fusion 结合，以提高模型的表达能力和效率。</li>
<li>results: 在 Voice Bank + DEMAND 数据集上，与 state-of-the-art 模型比较，提出的模型能够达到相当或更好的结果，同时具有较少的参数（0.58M）。<details>
<summary>Abstract</summary>
Speech enhancement is a demanding task in automated speech processing pipelines, focusing on separating clean speech from noisy channels. Transformer based models have recently bested RNN and CNN models in speech enhancement, however at the same time they are much more computationally expensive and require much more high quality training data, which is always hard to come by. In this paper, we present an improvement for speech enhancement models that maintains the expressiveness of self-attention while significantly reducing model complexity, which we have termed Spectrum Attention Fusion. We carefully construct a convolutional module to replace several self-attention layers in a speech Transformer, allowing the model to more efficiently fuse spectral features. Our proposed model is able to achieve comparable or better results against SOTA models but with significantly smaller parameters (0.58M) on the Voice Bank + DEMAND dataset.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Speech enhancement 是自动化语音处理流程中的一个要求，旨在分离含杂的语音和清晰的语音。基于Transformer的模型在最近的Speech enhancement中表现出色，但同时它们也更加计算昂贵，需要更多高质量的训练数据，这并不容易获得。在这篇论文中，我们提出了一种改进 speech enhancement 模型，保持了自注意的表达力，同时显著减少模型的复杂度，我们称之为 Spectrum Attention Fusion。我们在一个 convolutional 模块中代替了一些自注意层，让模型更有效地融合频谱特征。我们的提议模型在 Voice Bank + DEMAND 数据集上可以达到与顶峰模型相当或更好的结果，但具有远小于参数（0.58M）。
</details></li>
</ul>
<hr>
<h2 id="Emo-DNA-Emotion-Decoupling-and-Alignment-Learning-for-Cross-Corpus-Speech-Emotion-Recognition"><a href="#Emo-DNA-Emotion-Decoupling-and-Alignment-Learning-for-Cross-Corpus-Speech-Emotion-Recognition" class="headerlink" title="Emo-DNA: Emotion Decoupling and Alignment Learning for Cross-Corpus Speech Emotion Recognition"></a>Emo-DNA: Emotion Decoupling and Alignment Learning for Cross-Corpus Speech Emotion Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02190">http://arxiv.org/abs/2308.02190</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jiaxin-ye/emo-dna">https://github.com/jiaxin-ye/emo-dna</a></li>
<li>paper_authors: Jiaxin Ye, Yujie Wei, Xin-Cheng Wen, Chenglong Ma, Zhizhong Huang, Kunhong Liu, Hongming Shan</li>
<li>for: 这个研究的目的是将拥有不同 Corpora 的语音情感识别系统进行整合，以提高其在不同 Corpora 上的表现。</li>
<li>methods: 这个研究提出了一个名为 Emotion Decoupling aNd Alignment 的新框架，它使用了对照分离和双层情感对齐来学习语音情感识别系统。</li>
<li>results: 实验结果显示，这个新框架在多个跨 Corpora 的情感识别任务中表现更好，比起现有的方法。<details>
<summary>Abstract</summary>
Cross-corpus speech emotion recognition (SER) seeks to generalize the ability of inferring speech emotion from a well-labeled corpus to an unlabeled one, which is a rather challenging task due to the significant discrepancy between two corpora. Existing methods, typically based on unsupervised domain adaptation (UDA), struggle to learn corpus-invariant features by global distribution alignment, but unfortunately, the resulting features are mixed with corpus-specific features or not class-discriminative. To tackle these challenges, we propose a novel Emotion Decoupling aNd Alignment learning framework (EMO-DNA) for cross-corpus SER, a novel UDA method to learn emotion-relevant corpus-invariant features. The novelties of EMO-DNA are two-fold: contrastive emotion decoupling and dual-level emotion alignment. On one hand, our contrastive emotion decoupling achieves decoupling learning via a contrastive decoupling loss to strengthen the separability of emotion-relevant features from corpus-specific ones. On the other hand, our dual-level emotion alignment introduces an adaptive threshold pseudo-labeling to select confident target samples for class-level alignment, and performs corpus-level alignment to jointly guide model for learning class-discriminative corpus-invariant features across corpora. Extensive experimental results demonstrate the superior performance of EMO-DNA over the state-of-the-art methods in several cross-corpus scenarios. Source code is available at https://github.com/Jiaxin-Ye/Emo-DNA.
</details>
<details>
<summary>摘要</summary>
cross-corpus speech emotion recognition (SER) 提高了推断语音情绪的能力，从一个很好地标注的 corpora 扩展到另一个没有标注的 corpora，这是一项非常具有挑战性的任务，因为两个 corpora 之间存在很大的差异。现有的方法通常基于无监督领域适应 (UDA)，尝试通过全局分布对齐来学习 corpora  invariant 特征，但是 unfortunately，得到的特征都是混合 corpora 特定特征或不是类别特征。为了解决这些挑战，我们提出了一个新的 Emotion Decoupling and Alignment learning framework (EMO-DNA)  для cross-corpus SER，一种新的 UDA 方法来学习情绪相关的 corpora  invariant 特征。EMO-DNA 的两大创新是：对比情绪分离和双级情绪对接。一方面，我们的对比情绪分离通过对比分离损失来强化情绪相关特征与 corpora 特定特征之间的分离性。另一方面，我们的双级情绪对接引入了一个 adaptive 阈值 pseudo-labeling，选择 confidence 的目标样本进行类别对接，并在 corpora 级别对接以协助模型学习类别特征的 cross-corpus 普适性。我们的实验结果表明，EMO-DNA 在多个 cross-corpus 场景中表现出了与当前状态OF 法的超越性。代码可以在 <https://github.com/Jiaxin-Ye/Emo-DNA> 上找到。
</details></li>
</ul>
<hr>
<h2 id="Capturing-Spectral-and-Long-term-Contextual-Information-for-Speech-Emotion-Recognition-Using-Deep-Learning-Techniques"><a href="#Capturing-Spectral-and-Long-term-Contextual-Information-for-Speech-Emotion-Recognition-Using-Deep-Learning-Techniques" class="headerlink" title="Capturing Spectral and Long-term Contextual Information for Speech Emotion Recognition Using Deep Learning Techniques"></a>Capturing Spectral and Long-term Contextual Information for Speech Emotion Recognition Using Deep Learning Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04517">http://arxiv.org/abs/2308.04517</a></li>
<li>repo_url: None</li>
<li>paper_authors: Samiul Islam, Md. Maksudul Haque, Abu Jobayer Md. Sadat</li>
<li>for: 本研究旨在超越传统的speech emotion recognition方法（如LSTM、CNN、RNN、SVM、MLP），这些方法具有难以捕捉长期依赖关系、捕捉时间动态和捕捉复杂模式关系等缺陷。</li>
<li>methods: 本研究提出了一个 ensemble 模型，该模型将文本数据处理GCN（图 convolutional networks）和音频信号分析 HuBERT trasformer 相结合。GCN 可以利用文本的图形表示，捕捉文本中的长期Contextual 依赖关系和 semantics 关系，而 HuBERT 通过自我注意机制，可以捕捉音频信号中的长期依赖关系，捕捉时间动态。</li>
<li>results: 结果表明，将 GCN 和 HuBERT 相结合，可以充分利用这两种方法的优势，同时分析多Modal 数据，并将这些模式相互融合，从而提高情绪识别系统的准确性。<details>
<summary>Abstract</summary>
Traditional approaches in speech emotion recognition, such as LSTM, CNN, RNN, SVM, and MLP, have limitations such as difficulty capturing long-term dependencies in sequential data, capturing the temporal dynamics, and struggling to capture complex patterns and relationships in multimodal data. This research addresses these shortcomings by proposing an ensemble model that combines Graph Convolutional Networks (GCN) for processing textual data and the HuBERT transformer for analyzing audio signals. We found that GCNs excel at capturing Long-term contextual dependencies and relationships within textual data by leveraging graph-based representations of text and thus detecting the contextual meaning and semantic relationships between words. On the other hand, HuBERT utilizes self-attention mechanisms to capture long-range dependencies, enabling the modeling of temporal dynamics present in speech and capturing subtle nuances and variations that contribute to emotion recognition. By combining GCN and HuBERT, our ensemble model can leverage the strengths of both approaches. This allows for the simultaneous analysis of multimodal data, and the fusion of these modalities enables the extraction of complementary information, enhancing the discriminative power of the emotion recognition system. The results indicate that the combined model can overcome the limitations of traditional methods, leading to enhanced accuracy in recognizing emotions from speech.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="N-gram-Boosting-Improving-Contextual-Biasing-with-Normalized-N-gram-Targets"><a href="#N-gram-Boosting-Improving-Contextual-Biasing-with-Normalized-N-gram-Targets" class="headerlink" title="N-gram Boosting: Improving Contextual Biasing with Normalized N-gram Targets"></a>N-gram Boosting: Improving Contextual Biasing with Normalized N-gram Targets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02092">http://arxiv.org/abs/2308.02092</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wang Yau Li, Shreekantha Nadig, Karol Chang, Zafarullah Mahmood, Riqiang Wang, Simon Vandieken, Jonas Robertson, Fred Mailhot</li>
<li>for: 提高 keywords 识别率</li>
<li>methods: 使用 two-step keyword boosting mechanism，Normalize unigrams 和 n-grams，避免 missing hits 和 over-boosting multi-token keywords</li>
<li>results: 提高 keyword recognition rate by 26% Relative on proprietary in-domain dataset，和 2% on LibriSpeechI hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Accurate transcription of proper names and technical terms is particularly important in speech-to-text applications for business conversations. These words, which are essential to understanding the conversation, are often rare and therefore likely to be under-represented in text and audio training data, creating a significant challenge in this domain. We present a two-step keyword boosting mechanism that successfully works on normalized unigrams and n-grams rather than just single tokens, which eliminates missing hits issues with boosting raw targets. In addition, we show how adjusting the boosting weight logic avoids over-boosting multi-token keywords. This improves our keyword recognition rate by 26% relative on our proprietary in-domain dataset and 2% on LibriSpeech. This method is particularly useful on targets that involve non-alphabetic characters or have non-standard pronunciations.
</details>
<details>
<summary>摘要</summary>
精准转写特有名称和技术术语 particualrly important in speech-to-text应用程序中，这些词语是理解对话的关键，但它们通常是罕见的，因此在文本和音频训练数据中受到抑制。我们提出了一种两步关键词强化机制，该机制可以在 норма化单个字和n-gram中工作，而不是只是单个token，这将消除 raw 目标中的缺失命中问题。此外，我们还证明了如何调整强化权重逻辑，以避免多token关键被过度强化。这将提高我们的关键识别率达26%，相对于我们的自有领域数据集，并且2% 在 LibriSpeech 上。这种方法特别有用于targets 中包含非字母字符或非标准发音。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://example.com/2023/08/04/cs.SD_2023_08_04/" data-id="cllsjvzcz004sf588dccl1vle" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_08_04" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/04/eess.IV_2023_08_04/" class="article-date">
  <time datetime="2023-08-03T16:00:00.000Z" itemprop="datePublished">2023-08-04</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/04/eess.IV_2023_08_04/">eess.IV - 2023-08-04 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Frequency-Disentangled-Features-in-Neural-Image-Compression"><a href="#Frequency-Disentangled-Features-in-Neural-Image-Compression" class="headerlink" title="Frequency Disentangled Features in Neural Image Compression"></a>Frequency Disentangled Features in Neural Image Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02620">http://arxiv.org/abs/2308.02620</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Zafari, Atefeh Khoshkhahtinat, Piyush Mehta, Mohammad Saeed Ebrahimi Saadabadi, Mohammad Akyash, Nasser M. Nasrabadi</li>
<li>For: The paper proposes a neural image compression network that leverages feature-level frequency disentanglement and an augmented self-attention score calculation to improve the compression efficiency and de-correlate the image features.* Methods: The proposed network uses a rate-distortion variational autoencoder (R-D VAE) with relaxed scalar quantization, which is guided by a feature-level frequency disentanglement to capture the low-frequency texture of the image. The network also utilizes an augmented self-attention score calculation based on the Hadamard product during both encoding and decoding.* Results: The proposed network outperforms hand-engineered codecs and neural network-based codecs built on computation-heavy spatially autoregressive entropy models, demonstrating its effectiveness in image compression.<details>
<summary>Abstract</summary>
The design of a neural image compression network is governed by how well the entropy model matches the true distribution of the latent code. Apart from the model capacity, this ability is indirectly under the effect of how close the relaxed quantization is to the actual hard quantization. Optimizing the parameters of a rate-distortion variational autoencoder (R-D VAE) is ruled by this approximated quantization scheme. In this paper, we propose a feature-level frequency disentanglement to help the relaxed scalar quantization achieve lower bit rates by guiding the high entropy latent features to include most of the low-frequency texture of the image. In addition, to strengthen the de-correlating power of the transformer-based analysis/synthesis transform, an augmented self-attention score calculation based on the Hadamard product is utilized during both encoding and decoding. Channel-wise autoregressive entropy modeling takes advantage of the proposed frequency separation as it inherently directs high-informational low-frequency channels to the first chunks and conditions the future chunks on it. The proposed network not only outperforms hand-engineered codecs, but also neural network-based codecs built on computation-heavy spatially autoregressive entropy models.
</details>
<details>
<summary>摘要</summary>
neural 图像压缩网络的设计受到真实分布的熵模型匹配度的限制。 apart from 模型容量，这种能力受到实际硬化量化的距离影响。 在这篇论文中，我们提出了一种基于频谱分解的特征级频率分离，以帮助放松量化实现更低的比特率，使高熵特征映射到图像中的低频文本。 此外，我们还利用了在编码和解码过程中的扩展自我注意力计算，以增强trasformer 基于的分析/生成变换的分离能力。 通道 wise 自动化熵模型利用了我们提出的频谱分解，因为它直接引导高信息低频通道到首块，并将后续块condition 在它之上。 提议的网络不仅超过了手工编码器，还超过了基于计算昂贵的空间自动关联熵模型的神经网络编码器。
</details></li>
</ul>
<hr>
<h2 id="Brain-MRI-Segmentation-using-Template-Based-Training-and-Visual-Perception-Augmentation"><a href="#Brain-MRI-Segmentation-using-Template-Based-Training-and-Visual-Perception-Augmentation" class="headerlink" title="Brain MRI Segmentation using Template-Based Training and Visual Perception Augmentation"></a>Brain MRI Segmentation using Template-Based Training and Visual Perception Augmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02363">http://arxiv.org/abs/2308.02363</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fang-Cheng Yeh</li>
<li>for: 用一个人类大脑MRI模板和其相关的分类标签来训练一个3D U-Net模型从头开始，不需要大量的训练数据。</li>
<li>methods: 使用模板基于的训练方法，并包括视觉感知增强以提高模型对各种图像输入的Robustness，以避免过拟合。</li>
<li>results: 通过这种方法，我们训练了mouse、rat、猴、猴和人类大脑MRI的3D U-Net模型，并实现了分割任务，如脑骨梁除、大脑分割和组织概率地图。这种工具有效地解决了深度学习应用图像分析中的数据有限问题，并为研究人员提供了一个统一的解决方案，只需要一个图像样本来训练深度神经网络。<details>
<summary>Abstract</summary>
Deep learning models usually require sufficient training data to achieve high accuracy, but obtaining labeled data can be time-consuming and labor-intensive. Here we introduce a template-based training method to train a 3D U-Net model from scratch using only one population-averaged brain MRI template and its associated segmentation label. The process incorporated visual perception augmentation to enhance the model's robustness in handling diverse image inputs and mitigating overfitting. Leveraging this approach, we trained 3D U-Net models for mouse, rat, marmoset, rhesus, and human brain MRI to achieve segmentation tasks such as skull-stripping, brain segmentation, and tissue probability mapping. This tool effectively addresses the limited availability of training data and holds significant potential for expanding deep learning applications in image analysis, providing researchers with a unified solution to train deep neural networks with only one image sample.
</details>
<details>
<summary>摘要</summary>
深度学习模型通常需要充足的训练数据来达到高精度，但获取标注数据可以是时间consuming和劳动 INTENSIVE。在这里，我们介绍了一种模板基于的训练方法，可以从scratch用一个人类大脑MRI模板和其关联的分割标注来训练3D U-Net模型。该过程包括视觉感知增强以提高模型对多种图像输入的抗衡能力和避免过拟合。通过这种方法，我们训练了3D U-Net模型用于鼠、老鼠、猴、人类大脑MRI的分割任务，如骨干取除、大脑分割和组织概率地图。这种工具有效地解决了训练数据的有限性问题，并具有扩展深度学习应用于图像分析的潜在 potential。
</details></li>
</ul>
<hr>
<h2 id="T-UNet-Triplet-UNet-for-Change-Detection-in-High-Resolution-Remote-Sensing-Images"><a href="#T-UNet-Triplet-UNet-for-Change-Detection-in-High-Resolution-Remote-Sensing-Images" class="headerlink" title="T-UNet: Triplet UNet for Change Detection in High-Resolution Remote Sensing Images"></a>T-UNet: Triplet UNet for Change Detection in High-Resolution Remote Sensing Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02356">http://arxiv.org/abs/2308.02356</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pl-2000/t-unet">https://github.com/pl-2000/t-unet</a></li>
<li>paper_authors: Huan Zhong, Chen Wu</li>
<li>for: 这篇论文旨在提出一个新的网络模型，用于远程感知图像变化检测，以提高检测精度和准确性。</li>
<li>methods: 这篇论文提出了一个三枝Encoder结构，并使用多枝空间特征聚合模组（MBSSCA）进行特征聚合和探索。在解码阶段，论文导入了通道注意力机制（CAM）和空间注意力机制（SAM），以实现完整地探索和融合详细的特征信息和 semantic 地域特征信息。</li>
<li>results: 这篇论文的实验结果显示，T-UNet 模型可以实现更高的准确率和精度，并且能够更好地探索和融合详细的特征信息和 semantic 地域特征信息。<details>
<summary>Abstract</summary>
Remote sensing image change detection aims to identify the differences between images acquired at different times in the same area. It is widely used in land management, environmental monitoring, disaster assessment and other fields. Currently, most change detection methods are based on Siamese network structure or early fusion structure. Siamese structure focuses on extracting object features at different times but lacks attention to change information, which leads to false alarms and missed detections. Early fusion (EF) structure focuses on extracting features after the fusion of images of different phases but ignores the significance of object features at different times for detecting change details, making it difficult to accurately discern the edges of changed objects. To address these issues and obtain more accurate results, we propose a novel network, Triplet UNet(T-UNet), based on a three-branch encoder, which is capable to simultaneously extract the object features and the change features between the pre- and post-time-phase images through triplet encoder. To effectively interact and fuse the features extracted from the three branches of triplet encoder, we propose a multi-branch spatial-spectral cross-attention module (MBSSCA). In the decoder stage, we introduce the channel attention mechanism (CAM) and spatial attention mechanism (SAM) to fully mine and integrate detailed textures information at the shallow layer and semantic localization information at the deep layer.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate into Simplified Chinese remote sensing image change detection 图像变化检测是用于在同一区域内的不同时间检测到的图像差异。它广泛应用于土地管理、环境监测、灾害评估等领域。目前，大多数变化检测方法基于 Siamese 网络结构或 Early Fusion 结构。Siamese 结构注重在不同时间中EXTRACTING对象特征，但缺乏关注变化信息，导致假警报和漏报。 Early Fusion 结构注重在不同阶段图像归一化后EXTRACTING对象特征，但忽视对象特征在不同时间段中的变化细节，使其困难准确地分辨变化的边缘。为了解决这些问题并获得更高精度的结果，我们提出了一种新的网络模型，Triplet UNet（T-UNet），基于三个分支Encoder，能同时EXTRACT对象特征和不同时间段图像之间的变化特征。为了有效地交互和融合 triplet Encoder 中EXTRACT的特征，我们提出了多支分支空间特征跟踪模块（MBSSCA）。在解码阶段，我们引入了通道注意机制（CAM）和空间注意机制（SAM），以全面挖掘和融合图像的细节信息和semantic 本地化信息。
</details></li>
</ul>
<hr>
<h2 id="Generative-Image-Priors-for-MRI-Reconstruction-Trained-from-Magnitude-Only-Images"><a href="#Generative-Image-Priors-for-MRI-Reconstruction-Trained-from-Magnitude-Only-Images" class="headerlink" title="Generative Image Priors for MRI Reconstruction Trained from Magnitude-Only Images"></a>Generative Image Priors for MRI Reconstruction Trained from Magnitude-Only Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02340">http://arxiv.org/abs/2308.02340</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mrirecon/image-priors">https://github.com/mrirecon/image-priors</a></li>
<li>paper_authors: Guanxiong Luo, Xiaoqing Wang, Mortiz Blumenthal, Martin Schilling, Erik Hans Ulrich Rauf, Raviteja Kotikalapudi, Niels Focke, Martin Uecker</li>
<li>for: 这个研究旨在构建基于大数据集和阶段信息的通用和稳定的生成图像先验。这些先验可以用于恢复图像质量的正则化。</li>
<li>methods: 研究开始于准备用魔力只图像准备训练数据集，然后将这个数据集扩展到包括阶段信息，并用这个数据集训练生成图像先验。最后，研究人员使用不同的抽样方案进行了线性和非线性恢复的评估。</li>
<li>results: 实验结果表明，基于复杂图像的先验比只基于魔力图像的先验表现更好。此外，一个训练在更大的数据集上的先验也表现出更高的稳定性。最后，我们发现使用生成先验比L1-wavelet正则化更有利于扩展扫描成像。结论：这些发现表明，包括阶段信息和利用大数据集可以提高生成先验的性能和可靠性，并且这些先验可以用于提高MRI重建的质量。<details>
<summary>Abstract</summary>
Purpose: In this work, we present a workflow to construct generic and robust generative image priors from magnitude-only images. The priors can then be used for regularization in reconstruction to improve image quality. Methods: The workflow begins with the preparation of training datasets from magnitude-only MR images. This dataset is then augmented with phase information and used to train generative priors of complex images. Finally, trained priors are evaluated using both linear and nonlinear reconstruction for compressed sensing parallel imaging with various undersampling schemes. Results: The results of our experiments demonstrate that priors trained on complex images outperform priors trained only on magnitude images. Additionally, a prior trained on a larger dataset exhibits higher robustness. Finally, we show that the generative priors are superior to L1 -wavelet regularization for compressed sensing parallel imaging with high undersampling. Conclusion: These findings stress the importance of incorporating phase information and leveraging large datasets to raise the performance and reliability of the generative priors for MRI reconstruction. Phase augmentation makes it possible to use existing image databases for training.
</details>
<details>
<summary>摘要</summary>
目的：在这项工作中，我们提出了一个工作流程，用于从偏好度只图像中构建通用和稳定的生成图像先验。这些先验然后可以用于图像重建中的regularization，以提高图像质量。方法：工作流程开始于从偏好度只图像MR影像中准备训练集。这个集合然后被补充 phase信息，并用于训练复杂图像的生成先验。最后，我们使用线性和非线性重建进行测试，以评估训练后的先验表现。结果：我们的实验结果表明，基于复杂图像的先验比基于偏好度只图像的先验表现更好。此外，一个基于更大的数据集训练的先验表现更高稳定。最后，我们表明，生成先验比L1-wavelet regularization更有优势于高抽样率的并行扫描图像重建。结论：这些发现强调了在图像重建中包含相位信息和利用大数据集来提高生成先验的性能和可靠性。相位增强使得可以使用现有的图像数据库进行训练。
</details></li>
</ul>
<hr>
<h2 id="CT-Reconstruction-from-Few-Planar-X-rays-with-Application-towards-Low-resource-Radiotherapy"><a href="#CT-Reconstruction-from-Few-Planar-X-rays-with-Application-towards-Low-resource-Radiotherapy" class="headerlink" title="CT Reconstruction from Few Planar X-rays with Application towards Low-resource Radiotherapy"></a>CT Reconstruction from Few Planar X-rays with Application towards Low-resource Radiotherapy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02100">http://arxiv.org/abs/2308.02100</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wanderinrain/xray2ct">https://github.com/wanderinrain/xray2ct</a></li>
<li>paper_authors: Yiran Sun, Tucker Netherton, Laurence Court, Ashok Veeraraghavan, Guha Balakrishnan</li>
<li>for: 这项研究的目的是使用几个平面X射图像生成CT卷积体，以便在低和中Resource Settings中提高肿瘤诊断和治疗的效率。</li>
<li>methods: 该研究使用了深度生成模型，基于神经隐式表示来合成volumetric CT扫描图像从几个平面X射图像的不同角度。同时，模型还可以在训练过程中使用 segmentation masks 来减少生成任务中的产生不必要的特征。</li>
<li>results: 研究发现，使用该方法生成的 thoracic CT 扫描图像和临床实际中获取的 CT 扫描图像之间的是ocenter 辐射剂量Error 小于1%。此外，该方法还比现有的稀疍CT重建基elines 高于标准像素和结构级别指标（PSNR、SSIM、Dice 分数）中的LIDC肺CT数据集。<details>
<summary>Abstract</summary>
CT scans are the standard-of-care for many clinical ailments, and are needed for treatments like external beam radiotherapy. Unfortunately, CT scanners are rare in low and mid-resource settings due to their costs. Planar X-ray radiography units, in comparison, are far more prevalent, but can only provide limited 2D observations of the 3D anatomy. In this work, we propose a method to generate CT volumes from few (<5) planar X-ray observations using a prior data distribution, and perform the first evaluation of such a reconstruction algorithm for a clinical application: radiotherapy planning. We propose a deep generative model, building on advances in neural implicit representations to synthesize volumetric CT scans from few input planar X-ray images at different angles. To focus the generation task on clinically-relevant features, our model can also leverage anatomical guidance during training (via segmentation masks). We generated 2-field opposed, palliative radiotherapy plans on thoracic CTs reconstructed by our method, and found that isocenter radiation dose on reconstructed scans have <1% error with respect to the dose calculated on clinically acquired CTs using <=4 X-ray views. In addition, our method is better than recent sparse CT reconstruction baselines in terms of standard pixel and structure-level metrics (PSNR, SSIM, Dice score) on the public LIDC lung CT dataset. Code is available at: https://github.com/wanderinrain/Xray2CT.
</details>
<details>
<summary>摘要</summary>
干扰X射线成像设备在低和中型资源设备中较为罕见，原因是它们的成本较高。相比之下，平面X射线成像设备更为普遍，但它们只能提供2D的观察结果，无法提供3D的解剖结构。在这项工作中，我们提出了一种方法，使用先前的数据分布来生成CT体积从少于5个平面X射线图像中，并在临床应用中进行了首次评估。我们提出了一种深度生成模型，基于神经隐式表示来生成3D的CT体积图像，并在训练过程中使用解剖指导来避免误差。我们使用了2个对称的反向X射线成像计划来规划肺部CT图像，并发现在重建的扫描图像上的穿透中心辐射剂量与临床获得的CT图像中的辐射剂量之间的差异小于1%。此外，我们的方法也比最近的稀疏CT重建基eline更好，根据标准像素和结构级度指标（PSNR、SSIM、Dice分数）来评估。代码可以在以下地址找到：https://github.com/wanderinrain/Xray2CT。
</details></li>
</ul>
<hr>
<h2 id="Motion-robust-free-running-cardiovascular-MRI"><a href="#Motion-robust-free-running-cardiovascular-MRI" class="headerlink" title="Motion-robust free-running cardiovascular MRI"></a>Motion-robust free-running cardiovascular MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02088">http://arxiv.org/abs/2308.02088</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/syedmurtazaarshad/motion-robust-CMR">https://github.com/syedmurtazaarshad/motion-robust-CMR</a></li>
<li>paper_authors: Syed M. Arshad, Lee C. Potter, Chong Chen, Yingmin Liu, Preethi Chandrasekaran, Christopher Crabtree, Yuchi Han, Rizwan Ahmad</li>
<li>For: 这种研究旨在提高自由运行cardiovascular MRI（CMR）的动力稳定性，以便在各种应用中减少运动artefacts。* Methods: 该方法模拟了异常值作为auxiliary变量，并对这个变量进行MR физи学引导的集成随机树减少（CORe）。通过迭代算法，同时对auxiliary变量和图像进行估算。* Results: 对于50个实现中的异常值处理方法，CORe在正常化平均方差（NMSE）和结构相似指标（SSIM）方面表现出色，并且在3D cinema图像中更好地抑制了artefacts，而无需减少图像锐度。在4D流动图像中，CORe得到了更一致的流动测量结果，特别是在运动压力下。<details>
<summary>Abstract</summary>
PURPOSE: To present and validate an outlier rejection method that makes free-running cardiovascular MRI (CMR) more motion robust.   METHODS: The proposed method, called compressive recovery with outlier rejection (CORe), models outliers as an auxiliary variable that is added to the measured data. We enforce MR physics-guided group-sparsity on the auxiliary variable and jointly estimate it along with the image using an iterative algorithm. For validation, CORe is first compared to traditional compressed sensing (CS), robust regression (RR), and another outlier rejection method using two simulation studies. Then, CORe is compared to CS using five 3D cine and ten rest and stress 4D flow imaging datasets.   RESULTS: Our simulation studies show that CORe outperforms CS, RR, and the outlier rejection method in terms of normalized mean squared error (NMSE) and structural similarity index (SSIM) across 50 different realizations. The expert reader evaluation of 3D cine images demonstrates that CORe is more effective in suppressing artifacts while maintaining or improving image sharpness. The flow consistency evaluation in 4D flow images show that CORe yields more consistent flow measurements, especially under exercise stress.   CONCLUSION: An outlier rejection method is presented and validated using simulated and measured data. This method can help suppress motion artifacts in a wide range of free-running CMR applications.   CODE: MATLAB implementation code is available on GitHub at https://github.com/syedmurtazaarshad/motion-robust-CMR
</details>
<details>
<summary>摘要</summary>
目的：提出和验证一种可以使自由运行征 Cardiovascular MRI (CMR) 更加鲁棒于运动 artifacts 的方法。方法：提出的方法，称为 compressive recovery with outlier rejection (CORe)，将异常值模型为 auxillary 变量，并在这个变量上强制施加 MR 物理指导的群 sparse  regularization。我们使用迭代算法来同时估算这个变量和图像。对比 CS、RR 和异常拒绝方法，我们使用两个 simulate 研究进行验证。然后，我们使用五个 3D cine 和十个 rest 和 stress 4D flow imaging 数据集进行验证。结果：我们的 simulate 研究表明，CORe 在 NMSE 和 SSIM 指标上都高于 CS、RR 和异常拒绝方法。专业读者评估 3D cine 图像时，CORe 更有效地抑制 artifacts，同时保持或改善图像的锐度。在 4D flow 图像中，CORe 产生的流量测量更加一致，特别是在运动压力下。结论：我们提出了一种可以鲁棒化自由运行 CMR 应用中的异常拒绝方法，可以帮助抑制运动 artifacts。代码：MATLAB 实现代码可以在 GitHub 上找到，https://github.com/syedmurtazaarshad/motion-robust-CMR
</details></li>
</ul>
<hr>
<h2 id="Diffusion-Models-for-Counterfactual-Generation-and-Anomaly-Detection-in-Brain-Images"><a href="#Diffusion-Models-for-Counterfactual-Generation-and-Anomaly-Detection-in-Brain-Images" class="headerlink" title="Diffusion Models for Counterfactual Generation and Anomaly Detection in Brain Images"></a>Diffusion Models for Counterfactual Generation and Anomaly Detection in Brain Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02062">http://arxiv.org/abs/2308.02062</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alessandro-f/dif-fuse">https://github.com/alessandro-f/dif-fuse</a></li>
<li>paper_authors: Alessandro Fontanella, Grant Mair, Joanna Wardlaw, Emanuele Trucco, Amos Storkey</li>
<li>for: This paper is written for the purpose of generating healthy counterfactuals of diseased images for medical applications such as brain tumor and stroke management.</li>
<li>methods: The paper proposes a weakly supervised method that uses a saliency map obtained with ACAT to generate a healthy version of a diseased image, followed by targeted modifications using a diffusion model trained on healthy samples. The method combines DDPM and DDIM at each step of the sampling process to ensure a seamless transition between edited and unedited parts.</li>
<li>results: The paper shows that the proposed method improves the DICE score of the best competing method from $0.6534$ to $0.7056$ on IST-3 for stroke lesion segmentation and on BraTS2021 for brain tumor segmentation.Here’s the simplified Chinese text:</li>
<li>for: 这篇论文是为了生成疾病图像的健康对照样本而写的。</li>
<li>methods: 这篇论文提出了一种弱监督方法，使用ACAT获得的Saliency map来生成疾病图像的健康版本，然后进行targeted修改使用健康样本上训练的扩散模型。</li>
<li>results: 论文显示，提出的方法可以提高IST-3 stroke病变部分 segmentation的DICE分数从0.6534提高到0.7056，以及BraTS2021 brain tumor segmentation的DICE分数。<details>
<summary>Abstract</summary>
Segmentation masks of pathological areas are useful in many medical applications, such as brain tumour and stroke management. Moreover, healthy counterfactuals of diseased images can be used to enhance radiologists' training files and to improve the interpretability of segmentation models. In this work, we present a weakly supervised method to generate a healthy version of a diseased image and then use it to obtain a pixel-wise anomaly map. To do so, we start by considering a saliency map that approximately covers the pathological areas, obtained with ACAT. Then, we propose a technique that allows to perform targeted modifications to these regions, while preserving the rest of the image. In particular, we employ a diffusion model trained on healthy samples and combine Denoising Diffusion Probabilistic Model (DDPM) and Denoising Diffusion Implicit Model (DDIM) at each step of the sampling process. DDPM is used to modify the areas affected by a lesion within the saliency map, while DDIM guarantees reconstruction of the normal anatomy outside of it. The two parts are also fused at each timestep, to guarantee the generation of a sample with a coherent appearance and a seamless transition between edited and unedited parts. We verify that when our method is applied to healthy samples, the input images are reconstructed without significant modifications. We compare our approach with alternative weakly supervised methods on IST-3 for stroke lesion segmentation and on BraTS2021 for brain tumour segmentation, where we improve the DICE score of the best competing method from $0.6534$ to $0.7056$.
</details>
<details>
<summary>摘要</summary>
医学应用中的疾病区域分割mask是非常有用的，例如脑肿瘤和中风管理。此外，健康的对比样本可以用来提高放射学家的训练文件，并提高分割模型的解释性。在这种情况下，我们提出了一种弱相关的方法，可以生成一个疾病的健康版本，并使用其生成一个像素层级异常地图。我们的方法的核心思想是使用ACAT获取疾病区域的灵敏度地图，然后使用一种目标修改这些区域的技术，以保持图像的其他部分不受影响。我们使用训练于健康样本的扩散模型（DDPM）和扩散隐藏模型（DDIM），在每个步骤中进行修改和重建。DDPM用于修改疾病区域中的影响区域，而DDIM则 garantiz reconstruction of normal anatomy outside of it。这两个部分还 fusion at each timestep，以确保生成的样本具有一致的外观和无缝过渡。我们证明，当我们的方法应用于健康样本时，输入图像不会经受重要的修改。我们与其他弱相关方法进行比较，在IST-3上进行了肿瘤病变分割和在BraTS2021上进行了脑肿瘤分割，我们提高了最佳竞争方法的DICE分数从0.6534提高到0.7056。
</details></li>
</ul>
<hr>
<h2 id="Predicting-Ki67-ER-PR-and-HER2-Statuses-from-H-E-stained-Breast-Cancer-Images"><a href="#Predicting-Ki67-ER-PR-and-HER2-Statuses-from-H-E-stained-Breast-Cancer-Images" class="headerlink" title="Predicting Ki67, ER, PR, and HER2 Statuses from H&amp;E-stained Breast Cancer Images"></a>Predicting Ki67, ER, PR, and HER2 Statuses from H&amp;E-stained Breast Cancer Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01982">http://arxiv.org/abs/2308.01982</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amir Akbarnejad, Nilanjan Ray, Penny J. Barnes, Gilbert Bigras</li>
<li>for: This paper aims to investigate the accuracy of machine learning methods in predicting molecular information from histomorphology images.</li>
<li>methods: The authors built a large-scale dataset of histomorphology images with reliable measurements for Ki67, ER, PR, and HER2 statuses, and used a standard ViT-based pipeline to train classifiers for predicting these molecular markers.</li>
<li>results: The authors achieved prediction performances around 90% in terms of Area Under the Curve (AUC) when trained with a proper labeling protocol, and demonstrated the ability of the trained classifiers to localize relevant regions.Here is the simplified Chinese translation of the three key points:</li>
<li>for: 本研究旨在探讨机器学习方法是否可以准确地预测 histomorphology 图像中的分子信息。</li>
<li>methods: 作者们建立了一个大规模的 histomorphology 图像 dataset，并使用了标准的 ViT 基于管道来训练类ifiers 以预测 Ki67、ER、PR 和 HER2 状况。</li>
<li>results: 作者们在使用正确的标签协议训练的情况下，达到了约 90% 的预测性能（AUC），并证明了训练好的类ifiers 可以 correctly localize 相关区域。<details>
<summary>Abstract</summary>
Despite the advances in machine learning and digital pathology, it is not yet clear if machine learning methods can accurately predict molecular information merely from histomorphology. In a quest to answer this question, we built a large-scale dataset (185538 images) with reliable measurements for Ki67, ER, PR, and HER2 statuses. The dataset is composed of mirrored images of H\&E and corresponding images of immunohistochemistry (IHC) assays (Ki67, ER, PR, and HER2. These images are mirrored through registration. To increase reliability, individual pairs were inspected and discarded if artifacts were present (tissue folding, bubbles, etc). Measurements for Ki67, ER and PR were determined by calculating H-Score from image analysis. HER2 measurement is based on binary classification: 0 and 1+ (IHC scores representing a negative subset) vs 3+ (IHC score positive subset). Cases with IHC equivocal score (2+) were excluded. We show that a standard ViT-based pipeline can achieve prediction performances around 90% in terms of Area Under the Curve (AUC) when trained with a proper labeling protocol. Finally, we shed light on the ability of the trained classifiers to localize relevant regions, which encourages future work to improve the localizations. Our proposed dataset is publicly available: https://ihc4bc.github.io/
</details>
<details>
<summary>摘要</summary>
尽管机器学习和数字 PATHOLOGY 的进步，仍然没有确定机器学习方法可以准确地预测蛋白质信息仅基于组织结构。为了回答这个问题，我们建立了一个大规模数据集（185538张图像），其中包含可靠的测量结果 для Ki67、ER、PR 和 HER2 状况。这个数据集由 H\&E 和相关的免疫染色试验（Ki67、ER、PR 和 HER2）的图像组成，这些图像通过注册进行镜像。为了增强可靠性，我们检查了每个对并抛弃了包含artefacts（组织折叠、气泡等）的对。我们使用图像分析计算 H-Score 来确定 Ki67、ER 和 PR 的测量结果，而 HER2 的测量基于二分类：0 和 1+（IHC 分数表示负集）vs 3+（IHC 分数正集）。我们排除了 IHC 不确定分数（2+）的 случа。我们显示，使用标准 ViT-based 管道可以在训练 proper 标签协议下达到约 90% 的区域Under the Curve（AUC）的预测性能。最后，我们探讨了训练的分类器是否能够LOCALIZE relevant regions，这种能力鼓励未来的工作进一步提高本地化。我们的提出的数据集现在公开可用：https://ihc4bc.github.io/
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://example.com/2023/08/04/eess.IV_2023_08_04/" data-id="cllsjvzej0092f588bl3sbqbr" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_03" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/03/cs.LG_2023_08_03/" class="article-date">
  <time datetime="2023-08-02T16:00:00.000Z" itemprop="datePublished">2023-08-03</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/03/cs.LG_2023_08_03/">cs.LG - 2023-08-03 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="The-Capability-of-Large-Language-Models-to-Measure-Psychiatric-Functioning"><a href="#The-Capability-of-Large-Language-Models-to-Measure-Psychiatric-Functioning" class="headerlink" title="The Capability of Large Language Models to Measure Psychiatric Functioning"></a>The Capability of Large Language Models to Measure Psychiatric Functioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01834">http://arxiv.org/abs/2308.01834</a></li>
<li>repo_url: None</li>
<li>paper_authors: Isaac R. Galatzer-Levy, Daniel McDuff, Vivek Natarajan, Alan Karthikesalingam, Matteo Malgaroli</li>
<li>for:  investigate the capability of Large language models (LLMs) to predict psychiatric functioning from patient interviews and clinical descriptions without being trained to do so.</li>
<li>methods:  using prompts to extract estimated clinical scores and diagnoses based on standardized assessments.</li>
<li>results:  Med-PaLM 2 is capable of assessing psychiatric functioning across a range of psychiatric conditions with the strongest performance being the prediction of depression scores based on standardized assessments, which were statistically indistinguishable from human clinical raters.<details>
<summary>Abstract</summary>
The current work investigates the capability of Large language models (LLMs) that are explicitly trained on large corpuses of medical knowledge (Med-PaLM 2) to predict psychiatric functioning from patient interviews and clinical descriptions without being trained to do so. To assess this, n = 145 depression and n =115 PTSD assessments and n = 46 clinical case studies across high prevalence/high comorbidity disorders (Depressive, Anxiety, Psychotic, trauma and stress, Addictive disorders) were analyzed using prompts to extract estimated clinical scores and diagnoses. Results demonstrate that Med-PaLM 2 is capable of assessing psychiatric functioning across a range of psychiatric conditions with the strongest performance being the prediction of depression scores based on standardized assessments (Accuracy range= 0.80 - 0.84) which were statistically indistinguishable from human clinical raters t(1,144) = 1.20; p = 0.23. Results show the potential for general clinical language models to flexibly predict psychiatric risk based on free descriptions of functioning from both patients and clinicians.
</details>
<details>
<summary>摘要</summary>
当前研究探讨了大语言模型（LLM）在大量医学知识训练（Med-PaLM 2）下预测患者诊断和精神功能水平，不需要专门训练。为了评估这一点，研究使用了145例带有抑郁症和115例带有PTSD的评估，以及46例临床案例，涵盖高发病率/高混合病率的疾病（抑郁、焦虑、精神病、压力和 стресс、依数病）。结果表明Med-PaLM 2可以评估各种心理疾病的精神功能水平，特别是预测抑郁评估结果（准确率范围=0.80-0.84），这些结果与人类临床评估员的结果 statistically indistinguishable（t(1,144) = 1.20; p = 0.23）。结果表明大规模临床语言模型可以通过自由描述功能来预测心理风险。
</details></li>
</ul>
<hr>
<h2 id="Distribution-Free-Inference-for-the-Regression-Function-of-Binary-Classification"><a href="#Distribution-Free-Inference-for-the-Regression-Function-of-Binary-Classification" class="headerlink" title="Distribution-Free Inference for the Regression Function of Binary Classification"></a>Distribution-Free Inference for the Regression Function of Binary Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01835">http://arxiv.org/abs/2308.01835</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ambrus Tamás, Balázs Csanád Csáji</li>
<li>for: 本研究论文的目的是提出一种可靠、分布自由和非假想的预测函数信任区的构建方法，以便在任选的信任水平下确定真实的预测函数。</li>
<li>methods: 本文提出了一种抽样框架，用于构建可靠、分布自由和非假想的预测函数信任区。Specific algorithms are also suggested to demonstrate the framework.</li>
<li>results: 研究证明了构建的信任区是强有效的，即任何不正确的模型都将在长期内被排除，并且这种排除的可靠性被证明为一个可靠的 Probably Approximately Correct (PAC) 类型上下文。<details>
<summary>Abstract</summary>
One of the key objects of binary classification is the regression function, i.e., the conditional expectation of the class labels given the inputs. With the regression function not only a Bayes optimal classifier can be defined, but it also encodes the corresponding misclassification probabilities. The paper presents a resampling framework to construct exact, distribution-free and non-asymptotically guaranteed confidence regions for the true regression function for any user-chosen confidence level. Then, specific algorithms are suggested to demonstrate the framework. It is proved that the constructed confidence regions are strongly consistent, that is, any false model is excluded in the long run with probability one. The exclusion is quantified with probably approximately correct type bounds, as well. Finally, the algorithms are validated via numerical experiments, and the methods are compared to approximate asymptotic confidence ellipsoids.
</details>
<details>
<summary>摘要</summary>
一个重要的二分类问题中的关键对象是回归函数，即输入 conditional 类别标签的预期值。通过回归函数不仅可以定义 Bayes 优化的分类器，还可以表示相应的误分类概率。文章提出了一种抽样框架，可以构造 exact， distribution-free 和非假正极限保证的信任区域，以确定真实的回归函数。然后，文章提供了特定的算法，以示出框架。文章证明了构造的信任区域是strongly consistent，即任何 false model 都会在长期内被排除，并且这种排除的可能性可以通过 probably approximately correct 类型上限来衡量。最后，算法被数学实验 validate，并与approxymatic confidence ellipsoids 进行比较。
</details></li>
</ul>
<hr>
<h2 id="Hard-Adversarial-Example-Mining-for-Improving-Robust-Fairness"><a href="#Hard-Adversarial-Example-Mining-for-Improving-Robust-Fairness" class="headerlink" title="Hard Adversarial Example Mining for Improving Robust Fairness"></a>Hard Adversarial Example Mining for Improving Robust Fairness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01823">http://arxiv.org/abs/2308.01823</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenhao Lin, Xiang Ji, Yulong Yang, Qian Li, Chao Shen, Run Wang, Liming Fang</li>
<li>for: 这篇论文的目的是提高深度神经网络的抗对抗性（Adversarial Training，AT），并解决这些模型对于对抗示例（Adversarial Examples，AE）的不公正性问题。</li>
<li>methods: 这篇论文提出了一个简单 yet effective的框架，即适应式强制对抗示例挖掘（HAM），并通过适应性地挖掘强制对抗示例，并将容易的对抗示例早期弃用，以提高AT的效率和公平性。</li>
<li>results: 实验结果显示，这篇论文的HAM方法可以在CIFAR-10、SVHN和Imagenette等三个数据集上实现重要的公平性提升，同时降低了computational cost，比较了state-of-the-art adversarial training方法的效果。<details>
<summary>Abstract</summary>
Adversarial training (AT) is widely considered the state-of-the-art technique for improving the robustness of deep neural networks (DNNs) against adversarial examples (AE). Nevertheless, recent studies have revealed that adversarially trained models are prone to unfairness problems, restricting their applicability. In this paper, we empirically observe that this limitation may be attributed to serious adversarial confidence overfitting, i.e., certain adversarial examples with overconfidence. To alleviate this problem, we propose HAM, a straightforward yet effective framework via adaptive Hard Adversarial example Mining.HAM concentrates on mining hard adversarial examples while discarding the easy ones in an adaptive fashion. Specifically, HAM identifies hard AEs in terms of their step sizes needed to cross the decision boundary when calculating loss value. Besides, an early-dropping mechanism is incorporated to discard the easy examples at the initial stages of AE generation, resulting in efficient AT. Extensive experimental results on CIFAR-10, SVHN, and Imagenette demonstrate that HAM achieves significant improvement in robust fairness while reducing computational cost compared to several state-of-the-art adversarial training methods. The code will be made publicly available.
</details>
<details>
<summary>摘要</summary>
“对抗训练”（AT）是深度神经网络（DNN）对异常示例（AE）的状态艺术技术。然而，最近的研究发现，对抗训练模型具有不公平问题，限制其应用。在这篇论文中，我们直观地发现这一问题可能归结于严重的对抗信任过拟合问题，即某些对抗示例具有过分信任。为了解决这个问题，我们提出了HAM，一种简单 yet有效的框架，通过适应式硬对抗示例挖掘来解决这个问题。HAM会在计算损失值时将硬对抗示例与易于攻击的示例进行分离，并在早期阶段使用早期释出机制来优化AT。实验结果表明，HAM在CIFAR-10、SVHN和Imagenette上实现了显著的公平性提升，同时降低了计算成本，相比于一些现状最佳的对抗训练方法。代码将公开发布。
</details></li>
</ul>
<hr>
<h2 id="Tensor-Programs-IVb-Adaptive-Optimization-in-the-Infinite-Width-Limit"><a href="#Tensor-Programs-IVb-Adaptive-Optimization-in-the-Infinite-Width-Limit" class="headerlink" title="Tensor Programs IVb: Adaptive Optimization in the Infinite-Width Limit"></a>Tensor Programs IVb: Adaptive Optimization in the Infinite-Width Limit</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01814">http://arxiv.org/abs/2308.01814</a></li>
<li>repo_url: None</li>
<li>paper_authors: Greg Yang, Etai Littwin</li>
<li>for: 这篇论文探讨了在宽神经网络中使用自适应优化器 like Adam 以外的新现象，包括批处理矩阵和核函数行为。</li>
<li>methods: 该论文使用了一种新的张量编程语言 NEXORT，以及 bras-ket notation，以描述如何使用 adaptive optimizers 处理梯度并生成更新。</li>
<li>results: 论文显示了在宽神经网络中使用 Adam 优化器时，存在类似于梯度下降优化器的特征学习和核函数行为，并提供了对这些行为的分析和总结。<details>
<summary>Abstract</summary>
Going beyond stochastic gradient descent (SGD), what new phenomena emerge in wide neural networks trained by adaptive optimizers like Adam? Here we show: The same dichotomy between feature learning and kernel behaviors (as in SGD) holds for general optimizers as well, including Adam -- albeit with a nonlinear notion of "kernel." We derive the corresponding "neural tangent" and "maximal update" limits for any architecture. Two foundational advances underlie the above results: 1) A new Tensor Program language, NEXORT, that can express how adaptive optimizers process gradients into updates. 2) The introduction of bra-ket notation to drastically simplify expressions and calculations in Tensor Programs. This work summarizes and generalizes all previous results in the Tensor Programs series of papers.
</details>
<details>
<summary>摘要</summary>
SGD 以外，在宽神经网络中使用自适应优化器如 Adam 的训练中，新的现象出现了什么？我们表明：SGD 中的特征学习和核函数行为之 dichotomy 也存在于总的优化器中，包括 Adam，但是它们是非线性的。我们 derivates 对应的 "神经折射" 和 "最大更新" 限制，对于任何架构都成立。这两个基本进展是：1）一种新的tensor program语言，NEXORT，可以表示如何自适应优化器将梯度转化为更新。2）在tensor program中引入bra-ket表示法，以简化表达和计算。这些成果总结了以前在tensor program series中的所有结果。
</details></li>
</ul>
<hr>
<h2 id="Job-Shop-Scheduling-via-Deep-Reinforcement-Learning-a-Sequence-to-Sequence-approach"><a href="#Job-Shop-Scheduling-via-Deep-Reinforcement-Learning-a-Sequence-to-Sequence-approach" class="headerlink" title="Job Shop Scheduling via Deep Reinforcement Learning: a Sequence to Sequence approach"></a>Job Shop Scheduling via Deep Reinforcement Learning: a Sequence to Sequence approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01797">http://arxiv.org/abs/2308.01797</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dawoz/JSP-DeepRL-Seq2Seq">https://github.com/dawoz/JSP-DeepRL-Seq2Seq</a></li>
<li>paper_authors: Giovanni Bonetta, Davide Zago, Rossella Cancelliere, Andrea Grosso</li>
<li>for: 本研究旨在提出一种基于深度学习的Job调度算法，可以自动学习调度规则。</li>
<li>methods: 本研究使用了自然语言编码器-解码器模型，并在Job Shop问题的benchmark实例上进行了测试。</li>
<li>results: 研究结果显示，我们的方法可以超过许多传统的优先级调度规则，并与当前最佳深度学习方法相当竞争。<details>
<summary>Abstract</summary>
Job scheduling is a well-known Combinatorial Optimization problem with endless applications. Well planned schedules bring many benefits in the context of automated systems: among others, they limit production costs and waste. Nevertheless, the NP-hardness of this problem makes it essential to use heuristics whose design is difficult, requires specialized knowledge and often produces methods tailored to the specific task. This paper presents an original end-to-end Deep Reinforcement Learning approach to scheduling that automatically learns dispatching rules. Our technique is inspired by natural language encoder-decoder models for sequence processing and has never been used, to the best of our knowledge, for scheduling purposes. We applied and tested our method in particular to some benchmark instances of Job Shop Problem, but this technique is general enough to be potentially used to tackle other different optimal job scheduling tasks with minimal intervention. Results demonstrate that we outperform many classical approaches exploiting priority dispatching rules and show competitive results on state-of-the-art Deep Reinforcement Learning ones.
</details>
<details>
<summary>摘要</summary>
This paper presents an original end-to-end deep reinforcement learning approach to job scheduling that automatically learns dispatching rules. Our technique is inspired by natural language encoder-decoder models for sequence processing and has never been used, to the best of our knowledge, for scheduling purposes. We applied and tested our method on some benchmark instances of the Job Shop Problem, but it is general enough to be potentially used to tackle other different optimal job scheduling tasks with minimal intervention.The results demonstrate that we outperform many classical approaches that use priority dispatching rules and show competitive results with state-of-the-art deep reinforcement learning methods.
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-Adaptative-Variational-Quantum-Algorithms-on-QUBO-Instances"><a href="#Benchmarking-Adaptative-Variational-Quantum-Algorithms-on-QUBO-Instances" class="headerlink" title="Benchmarking Adaptative Variational Quantum Algorithms on QUBO Instances"></a>Benchmarking Adaptative Variational Quantum Algorithms on QUBO Instances</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01789">http://arxiv.org/abs/2308.01789</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gloria Turati, Maurizio Ferrari Dacrema, Paolo Cremonesi</li>
<li>for: 这篇论文主要是为了研究可变量量量算法（Adaptative VQAs），以解决在量子计算机NISQ时代中的优化问题。</li>
<li>methods: 这篇论文比较了三种 Adaptative VQAs：EVQE、VAns 和 RA-VQE，以及传统的量子近似优化算法（QAOA）。这些算法都是基于不同的启发，如环境深度、共轭能力和硬件兼容性，来动态修改环境的电路结构和参数。</li>
<li>results: 论文通过应用这些算法解决 QUBO 问题，并研究了这些算法的性能，包括解决的问题质量和计算时间。此外，论文还检查了不同的超参数选择方法对算法的总性能的影响，提出了选择合适方法进行超参数调整的重要性。<details>
<summary>Abstract</summary>
In recent years, Variational Quantum Algorithms (VQAs) have emerged as a promising approach for solving optimization problems on quantum computers in the NISQ era. However, one limitation of VQAs is their reliance on fixed-structure circuits, which may not be taylored for specific problems or hardware configurations. A leading strategy to address this issue are Adaptative VQAs, which dynamically modify the circuit structure by adding and removing gates, and optimize their parameters during the training. Several Adaptative VQAs, based on heuristics such as circuit shallowness, entanglement capability and hardware compatibility, have already been proposed in the literature, but there is still lack of a systematic comparison between the different methods. In this paper, we aim to fill this gap by analyzing three Adaptative VQAs: Evolutionary Variational Quantum Eigensolver (EVQE), Variable Ansatz (VAns), already proposed in the literature, and Random Adapt-VQE (RA-VQE), a random approach we introduce as a baseline. In order to compare these algorithms to traditional VQAs, we also include the Quantum Approximate Optimization Algorithm (QAOA) in our analysis. We apply these algorithms to QUBO problems and study their performance by examining the quality of the solutions found and the computational times required. Additionally, we investigate how the choice of the hyperparameters can impact the overall performance of the algorithms, highlighting the importance of selecting an appropriate methodology for hyperparameter tuning. Our analysis sets benchmarks for Adaptative VQAs designed for near-term quantum devices and provides valuable insights to guide future research in this area.
</details>
<details>
<summary>摘要</summary>
近年来，变量量子算法（VQA）在量子计算机NISQ时代 emerged as a promising approach for solving optimization problems. However, one limitation of VQAs is their reliance on fixed-structure circuits, which may not be tailored for specific problems or hardware configurations. To address this issue, adaptive VQAs have been proposed, which dynamically modify the circuit structure and optimize parameters during training. Several adaptive VQAs have been proposed based on heuristics such as circuit shallowness, entanglement capability, and hardware compatibility. However, there is still a lack of a systematic comparison between the different methods.In this paper, we aim to fill this gap by analyzing three adaptive VQAs: Evolutionary Variational Quantum Eigensolver (EVQE), Variable Ansatz (VAns), and Random Adapt-VQE (RA-VQE), as well as the Quantum Approximate Optimization Algorithm (QAOA) for comparison. We apply these algorithms to QUBO problems and study their performance by examining the quality of the solutions found and the computational times required. Additionally, we investigate the impact of hyperparameter choice on the overall performance of the algorithms, highlighting the importance of selecting an appropriate methodology for hyperparameter tuning. Our analysis sets benchmarks for adaptive VQAs designed for near-term quantum devices and provides valuable insights to guide future research in this area.
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-based-Prediction-of-Stress-and-Strain-Maps-in-Arterial-Walls-for-Improved-Cardiovascular-Risk-Assessment"><a href="#Deep-Learning-based-Prediction-of-Stress-and-Strain-Maps-in-Arterial-Walls-for-Improved-Cardiovascular-Risk-Assessment" class="headerlink" title="Deep Learning-based Prediction of Stress and Strain Maps in Arterial Walls for Improved Cardiovascular Risk Assessment"></a>Deep Learning-based Prediction of Stress and Strain Maps in Arterial Walls for Improved Cardiovascular Risk Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01771">http://arxiv.org/abs/2308.01771</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yasin Shokrollahi1, Pengfei Dong1, Xianqi Li, Linxia Gu<br>for: This study aimed to develop a surrogate model for finite element analysis to predict stress-strain fields within 2D cross sections of arterial walls, which could replace traditional FEM methods and be more effective and efficient.methods: The study used a U-Net based fully convolutional neural network (CNN) and a conditional generative adversarial network (cGAN) to predict the von Mises stress and strain distribution, and also proposed their ensemble approaches to further improve the prediction accuracy.results: The trained U-Net models and cGAN models demonstrated high accuracy in predicting von Mises stress and strain fields, with SSIM scores of 0.854 and 0.830, and mean squared errors of 0.017 and 0.018 for stress and strain, respectively. The ensemble and transfer learning techniques also showed high accuracy, with SSIM scores of 0.890 for stress and 0.803 for strain, and mean squared errors of 0.008 for stress and 0.017 for strain.<details>
<summary>Abstract</summary>
This study investigated the potential of end-to-end deep learning tools as a more effective substitute for FEM in predicting stress-strain fields within 2D cross sections of arterial wall. We first proposed a U-Net based fully convolutional neural network (CNN) to predict the von Mises stress and strain distribution based on the spatial arrangement of calcification within arterial wall cross-sections. Further, we developed a conditional generative adversarial network (cGAN) to enhance, particularly from the perceptual perspective, the prediction accuracy of stress and strain field maps for arterial walls with various calcification quantities and spatial configurations. On top of U-Net and cGAN, we also proposed their ensemble approaches, respectively, to further improve the prediction accuracy of field maps. Our dataset, consisting of input and output images, was generated by implementing boundary conditions and extracting stress-strain field maps. The trained U-Net models can accurately predict von Mises stress and strain fields, with structural similarity index scores (SSIM) of 0.854 and 0.830 and mean squared errors of 0.017 and 0.018 for stress and strain, respectively, on a reserved test set. Meanwhile, the cGAN models in a combination of ensemble and transfer learning techniques demonstrate high accuracy in predicting von Mises stress and strain fields, as evidenced by SSIM scores of 0.890 for stress and 0.803 for strain. Additionally, mean squared errors of 0.008 for stress and 0.017 for strain further support the model's performance on a designated test set. Overall, this study developed a surrogate model for finite element analysis, which can accurately and efficiently predict stress-strain fields of arterial walls regardless of complex geometries and boundary conditions.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Bag-of-Policies-for-Distributional-Deep-Exploration"><a href="#Bag-of-Policies-for-Distributional-Deep-Exploration" class="headerlink" title="Bag of Policies for Distributional Deep Exploration"></a>Bag of Policies for Distributional Deep Exploration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01759">http://arxiv.org/abs/2308.01759</a></li>
<li>repo_url: None</li>
<li>paper_authors: Asen Nachkov, Luchen Li, Giulia Luise, Filippo Valdettaro, Aldo Faisal</li>
<li>for: 提高复杂环境中RL的效率探索</li>
<li>methods: 使用Bag of Policies（BoP）方法，其包括多个独立更新的头部，每个头在每个话题中控制一集state-action对，并使用这些对来更新所有头部</li>
<li>results: 通过实验证明，BoP方法可以提高RL在ALE Atari游戏中的robustness和速度<details>
<summary>Abstract</summary>
Efficient exploration in complex environments remains a major challenge for reinforcement learning (RL). Compared to previous Thompson sampling-inspired mechanisms that enable temporally extended exploration, i.e., deep exploration, we focus on deep exploration in distributional RL. We develop here a general purpose approach, Bag of Policies (BoP), that can be built on top of any return distribution estimator by maintaining a population of its copies. BoP consists of an ensemble of multiple heads that are updated independently. During training, each episode is controlled by only one of the heads and the collected state-action pairs are used to update all heads off-policy, leading to distinct learning signals for each head which diversify learning and behaviour. To test whether optimistic ensemble method can improve on distributional RL as did on scalar RL, by e.g. Bootstrapped DQN, we implement the BoP approach with a population of distributional actor-critics using Bayesian Distributional Policy Gradients (BDPG). The population thus approximates a posterior distribution of return distributions along with a posterior distribution of policies. Another benefit of building upon BDPG is that it allows to analyze global posterior uncertainty along with local curiosity bonus simultaneously for exploration. As BDPG is already an optimistic method, this pairing helps to investigate if optimism is accumulatable in distributional RL. Overall BoP results in greater robustness and speed during learning as demonstrated by our experimental results on ALE Atari games.
</details>
<details>
<summary>摘要</summary>
RL中的有效探索仍然是一个主要挑战。与前期的汤姆逊探索机制相比，我们在分布RL中强调深入探索。我们开发了一个通用的方法，即袋子策略（Bag of Policies，BoP），它可以基于任何返回分布估计器建立。BoP包括多个独立更新的头，每个头控制一个episode，并将收集的状态-动作对用于所有头上不同策略进行off-policy更新，从而生成多个不同的学习信号，使得学习和行为更加多样化。为了测试optimistic ensemble方法在分布RL中是否能够提高性能，我们实现了BoP方法，使用了一个分布 actor-critic 的人工 intel 拟合 Bayesian Distributional Policy Gradients（BDPG）。这个人工 intel Population Approximates posterior distribution of return distributions and posterior distribution of policies。此外，由于BDPG已经是一个optimistic方法，这种结合可以同时分析分布RL中的全局 posterior uncertainty和本地好奇购买奖励。实验结果表明，BoP在ALE Atari游戏中显示出更高的稳定性和速度。
</details></li>
</ul>
<hr>
<h2 id="Guided-Distillation-for-Semi-Supervised-Instance-Segmentation"><a href="#Guided-Distillation-for-Semi-Supervised-Instance-Segmentation" class="headerlink" title="Guided Distillation for Semi-Supervised Instance Segmentation"></a>Guided Distillation for Semi-Supervised Instance Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02668">http://arxiv.org/abs/2308.02668</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tariq Berrada, Camille Couprie, Karteek Alahari, Jakob Verbeek</li>
<li>for: 提高Instance Segmentation的表现，减少完全监督图像的需求</li>
<li>methods: 使用 semi-supervised 方法，利用无标注数据作为训练信号，限制过拟合到标注样本</li>
<li>results: 提高 teacher-student 抽象模型的表现，在 Cityscapes 和 COCO 数据集上提高 mask-AP 的数值，例如在 Cityscapes 数据集上提高 mask-AP 从 23.7 到 33.9，在 COCO 数据集上提高 mask-AP 从 18.3 到 34.1<details>
<summary>Abstract</summary>
Although instance segmentation methods have improved considerably, the dominant paradigm is to rely on fully-annotated training images, which are tedious to obtain. To alleviate this reliance, and boost results, semi-supervised approaches leverage unlabeled data as an additional training signal that limits overfitting to the labeled samples. In this context, we present novel design choices to significantly improve teacher-student distillation models. In particular, we (i) improve the distillation approach by introducing a novel "guided burn-in" stage, and (ii) evaluate different instance segmentation architectures, as well as backbone networks and pre-training strategies. Contrary to previous work which uses only supervised data for the burn-in period of the student model, we also use guidance of the teacher model to exploit unlabeled data in the burn-in period. Our improved distillation approach leads to substantial improvements over previous state-of-the-art results. For example, on the Cityscapes dataset we improve mask-AP from 23.7 to 33.9 when using labels for 10\% of images, and on the COCO dataset we improve mask-AP from 18.3 to 34.1 when using labels for only 1\% of the training data.
</details>
<details>
<summary>摘要</summary>
尽管实例分割方法已经有了很大的进步，但主流方法仍然是基于完全标注的训练图像，这是获取标注数据的很 tedious 和耗时的过程。为了解决这个问题，并提高结果，半supervised方法利用无标注数据作为额外的训练信号，以避免过拟合标注样本。在这个上下文中，我们提出了新的设计选择，以提高教师学生热键扩展模型。具体来说，我们（i）改进了热键扩展approach，通过引入新的“导航燃烧”阶段，以及（ii）评估不同的实例分割架构、后备网络和预训练策略。与前一些工作一样，我们只使用supervised数据进行学生模型的热键期，但我们还使用教师模型的指导来利用无标注数据，从而在热键期内进行学习。我们改进的热键扩展方法导致了substantial提高，比如在Cityscapes数据集上，我们提高了mask-AP从23.7到33.9，并在COCO数据集上提高了mask-AP从18.3到34.1，只使用标注数据的1%。
</details></li>
</ul>
<hr>
<h2 id="Neural-Collapse-Terminus-A-Unified-Solution-for-Class-Incremental-Learning-and-Its-Variants"><a href="#Neural-Collapse-Terminus-A-Unified-Solution-for-Class-Incremental-Learning-and-Its-Variants" class="headerlink" title="Neural Collapse Terminus: A Unified Solution for Class Incremental Learning and Its Variants"></a>Neural Collapse Terminus: A Unified Solution for Class Incremental Learning and Its Variants</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01746">http://arxiv.org/abs/2308.01746</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/neuralcollapseapplications/unicil">https://github.com/neuralcollapseapplications/unicil</a></li>
<li>paper_authors: Yibo Yang, Haobo Yuan, Xiangtai Li, Jianlong Wu, Lefei Zhang, Zhouchen Lin, Philip Torr, Dacheng Tao, Bernard Ghanem<br>for: 这篇论文的目的是解决在新类incremental learning中保持老类能力的问题，包括长尾类增量学习和几架shot类增量学习，这些问题在实际应用中非常普遍，并且使得旧类能力衰退问题更加严重。methods: 这篇论文提出了一个统一的解决方案，即神经坍缩终点（Neural Collapse Terminus，NCT），这是一个固定的结构，具有整个标签空间中最大的等角对称分类分布。NCT  acted as a consistent target throughout the incremental training, 以避免在增量训练中分配特征空间。results: 实验结果显示，这篇论文的方法可以在多个数据集上进行实际应用，并且在增量训练中保持旧类能力，同时在新类 incremental learning 中获得良好的性能。实验结果还显示，这篇论文的方法可以应对实际应用中的数据不均匀和数据缺乏问题，并且可以在不知道总共有多少个类别和数据分布是否正常、长尾或几架shot的情况下进行通用化。<details>
<summary>Abstract</summary>
How to enable learnability for new classes while keeping the capability well on old classes has been a crucial challenge for class incremental learning. Beyond the normal case, long-tail class incremental learning and few-shot class incremental learning are also proposed to consider the data imbalance and data scarcity, respectively, which are common in real-world implementations and further exacerbate the well-known problem of catastrophic forgetting. Existing methods are specifically proposed for one of the three tasks. In this paper, we offer a unified solution to the misalignment dilemma in the three tasks. Concretely, we propose neural collapse terminus that is a fixed structure with the maximal equiangular inter-class separation for the whole label space. It serves as a consistent target throughout the incremental training to avoid dividing the feature space incrementally. For CIL and LTCIL, we further propose a prototype evolving scheme to drive the backbone features into our neural collapse terminus smoothly. Our method also works for FSCIL with only minor adaptations. Theoretical analysis indicates that our method holds the neural collapse optimality in an incremental fashion regardless of data imbalance or data scarcity. We also design a generalized case where we do not know the total number of classes and whether the data distribution is normal, long-tail, or few-shot for each coming session, to test the generalizability of our method. Extensive experiments with multiple datasets are conducted to demonstrate the effectiveness of our unified solution to all the three tasks and the generalized case.
</details>
<details>
<summary>摘要</summary>
如何维护新类的学习能力而不损害老类的能力是泛cremental learning中的一个关键挑战。此外，我们还考虑了实际情况中的数据不均衡和数据罕见性，并提出了长尾类增量学习和少数shot类增量学习两种方法。现有的方法主要针对一个任务。在这篇论文中，我们提出了增量训练中的不一致问题的统一解决方案。具体来说，我们提出了一个固定结构的神经溃终点，该结构具有整个标签空间中最大的等角间距。它在增量训练中作为不变的目标，以避免在增量训练中分割特征空间。为CIL和LTCIL任务，我们进一步提出了一种prototype evolving scheme来顺略地将后ION抽象到我们的神经溃终点中。我们的方法也适用于FSCIL任务，只需要小量的修改。理论分析表明，我们的方法在增量训练中保持了神经溃优化的优化，不受数据不均衡或数据罕见性的影响。我们还设计了一种通用情况，在每个来来Session中不知道总共有多少类和每个数据分布是正常、长尾或少数shot，以测试我们的方法的通用性。我们进行了多个数据集的广泛实验，以证明我们的统一解决方案对所有三个任务和通用情况具有效果。
</details></li>
</ul>
<hr>
<h2 id="Multitask-Learning-with-No-Regret-from-Improved-Confidence-Bounds-to-Active-Learning"><a href="#Multitask-Learning-with-No-Regret-from-Improved-Confidence-Bounds-to-Active-Learning" class="headerlink" title="Multitask Learning with No Regret: from Improved Confidence Bounds to Active Learning"></a>Multitask Learning with No Regret: from Improved Confidence Bounds to Active Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01744">http://arxiv.org/abs/2308.01744</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pier Giuseppe Sessa, Pierre Laforgue, Nicolò Cesa-Bianchi, Andreas Krause<br>for: 这个论文的目的是提供一种在不知道任务之间相似性的情况下，实现多任务学习的信任度范围，以及一种基于这些信任度范围的在线学习算法。methods: 该论文使用了多任务信任度范围的新研究，通过对多任务信息增量进行细化分析，提供了新的 regret保证，具有任务相似性参数的依赖性。results: 该论文提出了一种自动地适应任务相似性的在线学习算法，并在synthetic和实际世界数据上进行了实验验证，证明了其bounds和算法的有效性。<details>
<summary>Abstract</summary>
Multitask learning is a powerful framework that enables one to simultaneously learn multiple related tasks by sharing information between them. Quantifying uncertainty in the estimated tasks is of pivotal importance for many downstream applications, such as online or active learning. In this work, we provide novel multitask confidence intervals in the challenging agnostic setting, i.e., when neither the similarity between tasks nor the tasks' features are available to the learner. The obtained intervals do not require i.i.d. data and can be directly applied to bound the regret in online learning. Through a refined analysis of the multitask information gain, we obtain new regret guarantees that, depending on a task similarity parameter, can significantly improve over treating tasks independently. We further propose a novel online learning algorithm that achieves such improved regret without knowing this parameter in advance, i.e., automatically adapting to task similarity. As a second key application of our results, we introduce a novel multitask active learning setup where several tasks must be simultaneously optimized, but only one of them can be queried for feedback by the learner at each round. For this problem, we design a no-regret algorithm that uses our confidence intervals to decide which task should be queried. Finally, we empirically validate our bounds and algorithms on synthetic and real-world (drug discovery) data.
</details>
<details>
<summary>摘要</summary>
多任务学习是一个强大的框架，它允许一个模型同时学习多个相关的任务，并在这些任务之间共享信息。在许多下游应用中，量化任务估计中的不确定性是非常重要的，例如在线学习或活动学习中。在这项工作中，我们提供了新的多任务信任范围，它在无相似性信息和任务特征信息的情况下提供，并且可以直接应用于 bound 在线学习中的 regret。通过对多任务信息增量进行细化分析，我们获得了新的 regret 保证，它们可以在任务相似度参数的情况下显著改进于独立处理任务。此外，我们还提出了一种新的在线学习算法，它可以在不知道任务相似度参数的情况下实现改进的 regret。作为第二个关键应用，我们引入了一种多任务活动学习设置，在这个设置中，学习器需要同时优化多个任务，但只有一个任务可以在每次轮次中被学习器请求反馈。为解决这个问题，我们设计了一种无损算法，它使用我们的信任范围来决定哪个任务应该被请求反馈。 finally，我们employnull对我们的 bound 和算法进行了实验 validate 。
</details></li>
</ul>
<hr>
<h2 id="Finding-the-Optimum-Design-of-Large-Gas-Engines-Prechambers-Using-CFD-and-Bayesian-Optimization"><a href="#Finding-the-Optimum-Design-of-Large-Gas-Engines-Prechambers-Using-CFD-and-Bayesian-Optimization" class="headerlink" title="Finding the Optimum Design of Large Gas Engines Prechambers Using CFD and Bayesian Optimization"></a>Finding the Optimum Design of Large Gas Engines Prechambers Using CFD and Bayesian Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01743">http://arxiv.org/abs/2308.01743</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefan Posch, Clemens Gößnitzer, Franz Rohrhofer, Bernhard C. Geiger, Andreas Wimmer</li>
<li>for: 大功率液化燃料发动机中的紧急液喷启火概念，以实现在副气比较低的情况下稳定燃烧，从而提高效率并降低排放。</li>
<li>methods: 计算流体动力学（CFD）模拟，用于评估不同设计参数下的启火器设计。</li>
<li>results: 使用欧几里得法则精度的Reynolds均值 Navier-Stokes  simulations来确定选择启火器设计参数时的目标值。结果表明选择的策略是有效地找到符合目标值的启火器设计。<details>
<summary>Abstract</summary>
The turbulent jet ignition concept using prechambers is a promising solution to achieve stable combustion at lean conditions in large gas engines, leading to high efficiency at low emission levels. Due to the wide range of design and operating parameters for large gas engine prechambers, the preferred method for evaluating different designs is computational fluid dynamics (CFD), as testing in test bed measurement campaigns is time-consuming and expensive. However, the significant computational time required for detailed CFD simulations due to the complexity of solving the underlying physics also limits its applicability. In optimization settings similar to the present case, i.e., where the evaluation of the objective function(s) is computationally costly, Bayesian optimization has largely replaced classical design-of-experiment. Thus, the present study deals with the computationally efficient Bayesian optimization of large gas engine prechambers design using CFD simulation. Reynolds-averaged-Navier-Stokes simulations are used to determine the target values as a function of the selected prechamber design parameters. The results indicate that the chosen strategy is effective to find a prechamber design that achieves the desired target values.
</details>
<details>
<summary>摘要</summary>
大型气Engine预室设计使用液体喷射技术可能是实现稳定燃烧在质量低的情况下高效燃烧的有望解决方案。由于大型气Engine预室设计和运行参数的范围很广，因此计算流体力学（CFD）是评估不同设计的首选方法，因为测试床测量campaign是时间consuming和昂贵的。然而，由于解决下面的物理学问题的复杂度，详细的CFD simulations也有限制其应用。在优化设定中，例如现在的案例，i.e., where the evaluation of the objective function(s) is computationally costly, Bayesian optimization has largely replaced classical design-of-experiment。因此，本研究关注计算效率高的抽象归一化优化大型气Engine预室设计使用CFD simulations。Reynolds-averaged-Navier-Stokes simulations are used to determine the target values as a function of the selected prechamber design parameters. The results indicate that the chosen strategy is effective to find a prechamber design that achieves the desired target values.
</details></li>
</ul>
<hr>
<h2 id="Exploiting-Multi-Label-Correlation-in-Label-Distribution-Learning"><a href="#Exploiting-Multi-Label-Correlation-in-Label-Distribution-Learning" class="headerlink" title="Exploiting Multi-Label Correlation in Label Distribution Learning"></a>Exploiting Multi-Label Correlation in Label Distribution Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01742">http://arxiv.org/abs/2308.01742</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiqiang Kou jing wang yuheng jia xin geng</li>
<li>for: 本研究旨在提出一种新的机器学习模式即分布式标签学习（LDL），以便解决机器学习问题中的指标空间呈指数级别的问题。</li>
<li>methods: 本研究采用了一种新的方法，即通过在多个标签学习（MLL）中采用低级别标签相互关系来捕捉标签相互关系。</li>
<li>results: 经过对比分析，本研究发现了现有LDL方法的缺陷，并提出了一种新的方法，即通过在MLL中采用低级别标签相互关系来捕捉标签相互关系，从而提高LDL方法的性能。<details>
<summary>Abstract</summary>
Label Distribution Learning (LDL) is a novel machine learning paradigm that assigns label distribution to each instance. Many LDL methods proposed to leverage label correlation in the learning process to solve the exponential-sized output space; among these, many exploited the low-rank structure of label distribution to capture label correlation. However, recent studies disclosed that label distribution matrices are typically full-rank, posing challenges to those works exploiting low-rank label correlation. Note that multi-label is generally low-rank; low-rank label correlation is widely adopted in multi-label learning (MLL) literature. Inspired by that, we introduce an auxiliary MLL process in LDL and capture low-rank label correlation on that MLL rather than LDL. In such a way, low-rank label correlation is appropriately exploited in our LDL methods. We conduct comprehensive experiments and demonstrate that our methods are superior to existing LDL methods. Besides, the ablation studies justify the advantages of exploiting low-rank label correlation in the auxiliary MLL.
</details>
<details>
<summary>摘要</summary>
标签分布学习（LDL）是一种新的机器学习方案，它将标签分布分配给每个实例。许多LDL方法尝试利用标签关系来解决不同输出空间的问题，其中许多方法利用标签分布的低级结构来捕捉标签关系。然而，最近的研究发现，标签分布矩阵通常是全级结构的，这对于那些利用低级标签关系的方法带来了挑战。注意，多标签通常是低级的，低级标签关系广泛采用在多标签学习（MLL）文献中。针对这一点，我们引入了一个辅助的多标签学习过程（MLL），并在这个MLL上捕捉低级标签关系。这样做的优点是，我们可以正确地利用低级标签关系在LDL中。我们进行了广泛的实验，并证明了我们的方法比现有的LDL方法更高效。此外，缺省研究证明了在辅助MLL中利用低级标签关系的优势。
</details></li>
</ul>
<hr>
<h2 id="Bringing-Chemistry-to-Scale-Loss-Weight-Adjustment-for-Multivariate-Regression-in-Deep-Learning-of-Thermochemical-Processes"><a href="#Bringing-Chemistry-to-Scale-Loss-Weight-Adjustment-for-Multivariate-Regression-in-Deep-Learning-of-Thermochemical-Processes" class="headerlink" title="Bringing Chemistry to Scale: Loss Weight Adjustment for Multivariate Regression in Deep Learning of Thermochemical Processes"></a>Bringing Chemistry to Scale: Loss Weight Adjustment for Multivariate Regression in Deep Learning of Thermochemical Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01954">http://arxiv.org/abs/2308.01954</a></li>
<li>repo_url: None</li>
<li>paper_authors: Franz M. Rohrhofer, Stefan Posch, Clemens Gößnitzer, José M. García-Oliver, Bernhard C. Geiger</li>
<li>for: 本研究旨在改进人工神经网络（ANN）在多变量回归任务中学习多种物质质量分布。</li>
<li>methods: 本研究使用了一种简单 yet effective的损失重量调整，以提高ANN在学习多种物质质量分布时的准确性。</li>
<li>results: 研究发现，这种损失重量调整可以使ANN更加准确地学习所有物质质量分布，包括次要物质的质量分布，而标准的平均方差优化则完全失败。此外，调整后的损失重量使得网络训练过程中的梯度更加均衡，这解释了其效果。<details>
<summary>Abstract</summary>
Flamelet models are widely used in computational fluid dynamics to simulate thermochemical processes in turbulent combustion. These models typically employ memory-expensive lookup tables that are predetermined and represent the combustion process to be simulated. Artificial neural networks (ANNs) offer a deep learning approach that can store this tabular data using a small number of network weights, potentially reducing the memory demands of complex simulations by orders of magnitude. However, ANNs with standard training losses often struggle with underrepresented targets in multivariate regression tasks, e.g., when learning minor species mass fractions as part of lookup tables. This paper seeks to improve the accuracy of an ANN when learning multiple species mass fractions of a hydrogen (\ce{H2}) combustion lookup table. We assess a simple, yet effective loss weight adjustment that outperforms the standard mean-squared error optimization and enables accurate learning of all species mass fractions, even of minor species where the standard optimization completely fails. Furthermore, we find that the loss weight adjustment leads to more balanced gradients in the network training, which explains its effectiveness.
</details>
<details>
<summary>摘要</summary>
法则模型广泛用于计算流体动力学来模拟热化学过程，以便在液体燃烧中预测燃烧过程。这些模型通常使用占用内存的lookup表，这些表示燃烧过程要模拟。人工神经网络（ANNs）提供了深度学习方法，可以将这些表格数据存储在小数量的网络参数中，从而可能减少复杂的计算模拟中的内存需求。然而，标准训练损失通常在多变量回归任务中struggle with underrepresented targets，例如在学习某些小分子质量 Fraction as part of lookup tables。本文旨在提高ANN在学习多种种质量 Fraction的氢（\ce{H2）}燃烧lookup表时的准确性。我们评估了一个简单，却有效的权重调整，该超过标准的平均方差优化，使得网络学习所有种质量 Fraction，包括次要种的质量 Fraction，其标准优化完全失败。此外，我们发现权重调整导致了网络训练中的更加平衡的梯度，这解释了其效果。
</details></li>
</ul>
<hr>
<h2 id="MAP-A-Model-agnostic-Pretraining-Framework-for-Click-through-Rate-Prediction"><a href="#MAP-A-Model-agnostic-Pretraining-Framework-for-Click-through-Rate-Prediction" class="headerlink" title="MAP: A Model-agnostic Pretraining Framework for Click-through Rate Prediction"></a>MAP: A Model-agnostic Pretraining Framework for Click-through Rate Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01737">http://arxiv.org/abs/2308.01737</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chiangel/map-code">https://github.com/chiangel/map-code</a></li>
<li>paper_authors: Jianghao Lin, Yanru Qu, Wei Guo, Xinyi Dai, Ruiming Tang, Yong Yu, Weinan Zhang</li>
<li>for: 这篇论文主要针对的是 clicked-through rate（CTR）预测，尤其是在大规模线上个人化服务中，为了提高CTR预测的精度和效率。</li>
<li>methods: 这篇论文提出了一个基于自动学习的预备架构（MAP），并提出了两种实用的算法：伪设对于每个实例中的特征进行隐藏和预测（Masked Feature Prediction，MFP），以及替换特征的检测（Replaced Feature Detection，RFD）。这些算法可以对大规模的用户点击logs进行自动学习，以提高CTR预测的精度和效率。</li>
<li>results: 根据该论文的实验结果，这两种算法可以在两个真实世界的大规模数据集（Avazu和Criteo）上达到新的州度测试表现，并在多个强大的后置模型（例如DCNv2和DeepFM）上显示出比较好的效果和效率。<details>
<summary>Abstract</summary>
With the widespread application of personalized online services, click-through rate (CTR) prediction has received more and more attention and research. The most prominent features of CTR prediction are its multi-field categorical data format, and vast and daily-growing data volume. The large capacity of neural models helps digest such massive amounts of data under the supervised learning paradigm, yet they fail to utilize the substantial data to its full potential, since the 1-bit click signal is not sufficient to guide the model to learn capable representations of features and instances. The self-supervised learning paradigm provides a more promising pretrain-finetune solution to better exploit the large amount of user click logs, and learn more generalized and effective representations. However, self-supervised learning for CTR prediction is still an open question, since current works on this line are only preliminary and rudimentary. To this end, we propose a Model-agnostic pretraining (MAP) framework that applies feature corruption and recovery on multi-field categorical data, and more specifically, we derive two practical algorithms: masked feature prediction (MFP) and replaced feature detection (RFD). MFP digs into feature interactions within each instance through masking and predicting a small portion of input features, and introduces noise contrastive estimation (NCE) to handle large feature spaces. RFD further turns MFP into a binary classification mode through replacing and detecting changes in input features, making it even simpler and more effective for CTR pretraining. Our extensive experiments on two real-world large-scale datasets (i.e., Avazu, Criteo) demonstrate the advantages of these two methods on several strong backbones (e.g., DCNv2, DeepFM), and achieve new state-of-the-art performance in terms of both effectiveness and efficiency for CTR prediction.
</details>
<details>
<summary>摘要</summary>
To address this challenge, we propose a Model-agnostic pretraining (MAP) framework that applies feature corruption and recovery on multi-field categorical data. Specifically, we derive two practical algorithms: masked feature prediction (MFP) and replaced feature detection (RFD). MFP digs into feature interactions within each instance through masking and predicting a small portion of input features, and introduces noise contrastive estimation (NCE) to handle large feature spaces. RFD further turns MFP into a binary classification mode through replacing and detecting changes in input features, making it even simpler and more effective for CTR pretraining.Our extensive experiments on two real-world large-scale datasets (i.e., Avazu, Criteo) demonstrate the advantages of these two methods on several strong backbones (e.g., DCNv2, DeepFM), and achieve new state-of-the-art performance in terms of both effectiveness and efficiency for CTR prediction.
</details></li>
</ul>
<hr>
<h2 id="Quantification-of-Predictive-Uncertainty-via-Inference-Time-Sampling"><a href="#Quantification-of-Predictive-Uncertainty-via-Inference-Time-Sampling" class="headerlink" title="Quantification of Predictive Uncertainty via Inference-Time Sampling"></a>Quantification of Predictive Uncertainty via Inference-Time Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01731">http://arxiv.org/abs/2308.01731</a></li>
<li>repo_url: None</li>
<li>paper_authors: Katarína Tóthová, Ľubor Ladický, Daniel Thul, Marc Pollefeys, Ender Konukoglu</li>
<li>for: 这项研究旨在解决数据uncertainty导致预测不确定性的问题，提出了一种后期采样策略来估计预测不确定性。</li>
<li>methods: 该方法不需要特定的建筑Component或训练机制，可以应用于任何具有Feed-Forward Deterministic Network的模型，无需改变建筑或训练过程。</li>
<li>results: 实验结果表明，该方法可以生成多种可能性 distributions，与预测错误之间存在良好的相关性。<details>
<summary>Abstract</summary>
Predictive variability due to data ambiguities has typically been addressed via construction of dedicated models with built-in probabilistic capabilities that are trained to predict uncertainty estimates as variables of interest. These approaches require distinct architectural components and training mechanisms, may include restrictive assumptions and exhibit overconfidence, i.e., high confidence in imprecise predictions. In this work, we propose a post-hoc sampling strategy for estimating predictive uncertainty accounting for data ambiguity. The method can generate different plausible outputs for a given input and does not assume parametric forms of predictive distributions. It is architecture agnostic and can be applied to any feed-forward deterministic network without changes to the architecture or training procedure. Experiments on regression tasks on imaging and non-imaging input data show the method's ability to generate diverse and multi-modal predictive distributions, and a desirable correlation of the estimated uncertainty with the prediction error.
</details>
<details>
<summary>摘要</summary>
通常情况下，预测变化因数据 ambiguity 被通过建立专门的模型，这些模型具有内置的 probabilistic 能力，并通过训练来预测不确定性估计。这些方法可能需要特定的体系结构和训练机制，并且可能受限于假设和过于自信。在这项工作中，我们提出了一种后期抽样策略，用于估计预测不确定性，考虑到数据 ambiguity。这种方法可以生成不同的可能输出，并不假设预测分布的 parametic 形式。它是体系无关的，可以应用于任何批处网络，无需改变体系结构或训练过程。在重静态和非静态输入数据上的回归任务中，我们的方法能够生成多种多样的预测分布，并且预测不确定性与预测错误之间存在适当的相关性。
</details></li>
</ul>
<hr>
<h2 id="Telematics-Combined-Actuarial-Neural-Networks-for-Cross-Sectional-and-Longitudinal-Claim-Count-Data"><a href="#Telematics-Combined-Actuarial-Neural-Networks-for-Cross-Sectional-and-Longitudinal-Claim-Count-Data" class="headerlink" title="Telematics Combined Actuarial Neural Networks for Cross-Sectional and Longitudinal Claim Count Data"></a>Telematics Combined Actuarial Neural Networks for Cross-Sectional and Longitudinal Claim Count Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01729">http://arxiv.org/abs/2308.01729</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francis Duval, Jean-Philippe Boucher, Mathieu Pigeon</li>
<li>for: 这个论文的目的是提出一种基于CANN框架的车保险laim count模型，用于评估和预测驾驶行为对车保险的影响。</li>
<li>methods: 这个论文使用了一种combined actuarial neural network（CANN）模型，该模型结合了经典的概率模型和神经网络，以提高评估驾驶行为对车保险的精度和可靠性。</li>
<li>results: 研究结果表明，使用CANN模型可以比以经典的ilog-linear模型来评估驾驶行为对车保险的影响，并且可以更好地评估驾驶行为的复杂性和相互关系。<details>
<summary>Abstract</summary>
We present novel cross-sectional and longitudinal claim count models for vehicle insurance built upon the Combined Actuarial Neural Network (CANN) framework proposed by Mario W\"uthrich and Michael Merz. The CANN approach combines a classical actuarial model, such as a generalized linear model, with a neural network. This blending of models results in a two-component model comprising a classical regression model and a neural network part. The CANN model leverages the strengths of both components, providing a solid foundation and interpretability from the classical model while harnessing the flexibility and capacity to capture intricate relationships and interactions offered by the neural network. In our proposed models, we use well-known log-linear claim count regression models for the classical regression part and a multilayer perceptron (MLP) for the neural network part. The MLP part is used to process telematics car driving data given as a vector characterizing the driving behavior of each insured driver. In addition to the Poisson and negative binomial distributions for cross-sectional data, we propose a procedure for training our CANN model with a multivariate negative binomial (MVNB) specification. By doing so, we introduce a longitudinal model that accounts for the dependence between contracts from the same insured. Our results reveal that the CANN models exhibit superior performance compared to log-linear models that rely on manually engineered telematics features.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的跨部分和长期声明计数模型，用于汽车保险，基于Mario W\"uthrich和Michael Merz所提出的combined actuarial neural network（CANN）框架。CANN模型结合了一种经典的 actuarial模型，如一般线性模型，和一个神经网络。这种模型融合结果形成了一个两部分模型，包括一个经典回归模型和一个神经网络部分。CANN模型利用了经典模型的优点，提供了坚实的基础和解释，同时具有神经网络的灵活性和能力捕捉复杂的关系和交互。在我们的提议中，我们使用了常见的 log-linear 声明计数回归模型作为经典回归部分，并使用一个多层感知器（MLP）作为神经网络部分。MLP部分用于处理每名保险人的驾驶行为数据，即作为一个向量表示驾驶行为。此外，我们还提出了一种训练 CANN 模型的多变量负 binomial（MVNB）规范。通过这种方法，我们开发了一种长期模型，帮助考虑保险合同之间的依赖关系。我们的结果表明，CANN 模型在基于手动设计的驾驶特征的情况下显示出了更高的性能，与经典 log-linear 模型相比。
</details></li>
</ul>
<hr>
<h2 id="ADRNet-A-Generalized-Collaborative-Filtering-Framework-Combining-Clinical-and-Non-Clinical-Data-for-Adverse-Drug-Reaction-Prediction"><a href="#ADRNet-A-Generalized-Collaborative-Filtering-Framework-Combining-Clinical-and-Non-Clinical-Data-for-Adverse-Drug-Reaction-Prediction" class="headerlink" title="ADRNet: A Generalized Collaborative Filtering Framework Combining Clinical and Non-Clinical Data for Adverse Drug Reaction Prediction"></a>ADRNet: A Generalized Collaborative Filtering Framework Combining Clinical and Non-Clinical Data for Adverse Drug Reaction Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02571">http://arxiv.org/abs/2308.02571</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/haoxuanli-pku/adrnet">https://github.com/haoxuanli-pku/adrnet</a></li>
<li>paper_authors: Haoxuan Li, Taojun Hu, Zetong Xiong, Chunyuan Zheng, Fuli Feng, Xiangnan He, Xiao-Hua Zhou</li>
<li>for: 预测药物副作用（ADR）的incidence rate，以提高医疗和药品发现中的安全性。</li>
<li>methods: 基于drugg-ADR的协同缓存问题进行预测，并利用非临床数据中的药物特征进行补充。</li>
<li>results: 通过对两个大规模的临床数据集进行广泛的比较，证明ADRNet可以准确地预测多个标签ADR。<details>
<summary>Abstract</summary>
Adverse drug reaction (ADR) prediction plays a crucial role in both health care and drug discovery for reducing patient mortality and enhancing drug safety. Recently, many studies have been devoted to effectively predict the drug-ADRs incidence rates. However, these methods either did not effectively utilize non-clinical data, i.e., physical, chemical, and biological information about the drug, or did little to establish a link between content-based and pure collaborative filtering during the training phase. In this paper, we first formulate the prediction of multi-label ADRs as a drug-ADR collaborative filtering problem, and to the best of our knowledge, this is the first work to provide extensive benchmark results of previous collaborative filtering methods on two large publicly available clinical datasets. Then, by exploiting the easy accessible drug characteristics from non-clinical data, we propose ADRNet, a generalized collaborative filtering framework combining clinical and non-clinical data for drug-ADR prediction. Specifically, ADRNet has a shallow collaborative filtering module and a deep drug representation module, which can exploit the high-dimensional drug descriptors to further guide the learning of low-dimensional ADR latent embeddings, which incorporates both the benefits of collaborative filtering and representation learning. Extensive experiments are conducted on two publicly available real-world drug-ADR clinical datasets and two non-clinical datasets to demonstrate the accuracy and efficiency of the proposed ADRNet. The code is available at https://github.com/haoxuanli-pku/ADRnet.
</details>
<details>
<summary>摘要</summary>
药物反应（ADR）预测对于医疗和药物发现具有重要作用，可以降低病人死亡率并提高药物安全性。在最近的研究中，许多研究者已经努力地预测药物-ADR的发生率。然而，这些方法大多未能有效地利用药物的非临床数据，例如物理、化学和生物学信息。此外，这些方法也未能够在训练阶段建立物理和纯粹的共同滤波技术之间的连接。在本文中，我们将药物多标签ADR预测定为药物-ADR共同滤波问题，并且，到我们所知，这是第一篇对两个大规模公共可用临床数据进行了广泛的比较分析的研究。然后，我们提出了ADRNet，一种通用的共同滤波框架，结合临床和非临床数据来预测药物-ADR。具体来说，ADRNet包括一个浅层共同滤波模块和一个深度药物表示模块，可以利用高维药物描述符进一步导航低维ADR秘密嵌入的学习，这里包括了共同滤波和表示学习的两大好处。我们对两个公共可用的真实世界药物-ADR临床数据集和两个非临床数据集进行了广泛的实验，以证明我们提出的ADRNet的准确和效率。代码可以在https://github.com/haoxuanli-pku/ADRnet上获取。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Link-Prediction-Explanations-for-Graph-Neural-Networks"><a href="#Evaluating-Link-Prediction-Explanations-for-Graph-Neural-Networks" class="headerlink" title="Evaluating Link Prediction Explanations for Graph Neural Networks"></a>Evaluating Link Prediction Explanations for Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01682">http://arxiv.org/abs/2308.01682</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cborile/eval_lp_xai">https://github.com/cborile/eval_lp_xai</a></li>
<li>paper_authors: Claudio Borile, Alan Perotti, André Panisson</li>
<li>for: 本研究旨在提供链接预测模型的解释评价指标，以便帮助推广Graph Machine Learning（GML）模型的应用。</li>
<li>methods: 本研究使用了现有的解释方法，如Graph Neural Networks（GNN），并评估了它们的解释质量。</li>
<li>results: 研究发现，选择距离 между节点表示的选择对链接预测解释质量有重要影响。此外，研究还发现了一些技术细节和假设对链接预测解释质量的影响。<details>
<summary>Abstract</summary>
Graph Machine Learning (GML) has numerous applications, such as node/graph classification and link prediction, in real-world domains. Providing human-understandable explanations for GML models is a challenging yet fundamental task to foster their adoption, but validating explanations for link prediction models has received little attention. In this paper, we provide quantitative metrics to assess the quality of link prediction explanations, with or without ground-truth. State-of-the-art explainability methods for Graph Neural Networks are evaluated using these metrics. We discuss how underlying assumptions and technical details specific to the link prediction task, such as the choice of distance between node embeddings, can influence the quality of the explanations.
</details>
<details>
<summary>摘要</summary>
机器学习（GML）在实际领域有广泛的应用，如节点/图分类和链接预测。提供可理解的GML模型解释是推广其使用的挑战，但链接预测模型的解释 Validating explanations has received little attention. In this paper, we provide quantitative metrics to assess the quality of link prediction explanations, with or without ground-truth. State-of-the-art explainability methods for Graph Neural Networks are evaluated using these metrics. We discuss how underlying assumptions and technical details specific to the link prediction task, such as the choice of distance between node embeddings, can influence the quality of the explanations.Here's the translation in Traditional Chinese:机器学习（GML）在实际领域有广泛的应用，如节点/图分类和链接预测。提供可理解的GML模型解释是推广其使用的挑战，但链接预测模型的解释 Validating explanations has received little attention. In this paper, we provide quantitative metrics to assess the quality of link prediction explanations, with or without ground-truth. State-of-the-art explainability methods for Graph Neural Networks are evaluated using these metrics. We discuss how underlying assumptions and technical details specific to the link prediction task, such as the choice of distance between node embeddings, can influence the quality of the explanations.
</details></li>
</ul>
<hr>
<h2 id="Learning-Implicit-Entity-object-Relations-by-Bidirectional-Generative-Alignment-for-Multimodal-NER"><a href="#Learning-Implicit-Entity-object-Relations-by-Bidirectional-Generative-Alignment-for-Multimodal-NER" class="headerlink" title="Learning Implicit Entity-object Relations by Bidirectional Generative Alignment for Multimodal NER"></a>Learning Implicit Entity-object Relations by Bidirectional Generative Alignment for Multimodal NER</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02570">http://arxiv.org/abs/2308.02570</a></li>
<li>repo_url: None</li>
<li>paper_authors: Feng Chen, Jiajia Liu, Kaixiang Ji, Wang Ren, Jian Wang, Jingdong Wang</li>
<li>for: 本文提出了一种解决多modal named entity recognition（MNER）中的两个挑战的方法，即 bridging the semantic gap between text and image，以及匹配实体与其相关的对象在图像中。</li>
<li>methods: 本文提出了一种名为BGA-MNER的双向生成对应方法，该方法包括\texttt{image2text}和\texttt{text2image}两个生成阶段，以及对实体特征含义的生成对应。</li>
<li>results: 经过广泛的实验 validate，本文的方法在两个benchmark上达到了无需图像输入 durante la inferencia的状态之决性性能。<details>
<summary>Abstract</summary>
The challenge posed by multimodal named entity recognition (MNER) is mainly two-fold: (1) bridging the semantic gap between text and image and (2) matching the entity with its associated object in image. Existing methods fail to capture the implicit entity-object relations, due to the lack of corresponding annotation. In this paper, we propose a bidirectional generative alignment method named BGA-MNER to tackle these issues. Our BGA-MNER consists of \texttt{image2text} and \texttt{text2image} generation with respect to entity-salient content in two modalities. It jointly optimizes the bidirectional reconstruction objectives, leading to aligning the implicit entity-object relations under such direct and powerful constraints. Furthermore, image-text pairs usually contain unmatched components which are noisy for generation. A stage-refined context sampler is proposed to extract the matched cross-modal content for generation. Extensive experiments on two benchmarks demonstrate that our method achieves state-of-the-art performance without image input during inference.
</details>
<details>
<summary>摘要</summary>
The challenge posed by multimodal named entity recognition (MNER) is mainly two-fold: (1) bridging the semantic gap between text and image and (2) matching the entity with its associated object in the image. Existing methods fail to capture the implicit entity-object relations due to the lack of corresponding annotation. In this paper, we propose a bidirectional generative alignment method named BGA-MNER to tackle these issues. Our BGA-MNER consists of \texttt{image2text} and \texttt{text2image} generation with respect to entity-salient content in two modalities. It jointly optimizes the bidirectional reconstruction objectives, leading to aligning the implicit entity-object relations under such direct and powerful constraints. Furthermore, image-text pairs usually contain unmatched components which are noisy for generation. A stage-refined context sampler is proposed to extract the matched cross-modal content for generation. Extensive experiments on two benchmarks demonstrate that our method achieves state-of-the-art performance without image input during inference.Here's the translation in Traditional Chinese as well:The challenge posed by multimodal named entity recognition (MNER) is mainly two-fold: (1) bridging the semantic gap between text and image and (2) matching the entity with its associated object in the image. Existing methods fail to capture the implicit entity-object relations due to the lack of corresponding annotation. In this paper, we propose a bidirectional generative alignment method named BGA-MNER to tackle these issues. Our BGA-MNER consists of \texttt{image2text} and \texttt{text2image} generation with respect to entity-salient content in two modalities. It jointly optimizes the bidirectional reconstruction objectives, leading to aligning the implicit entity-object relations under such direct and powerful constraints. Furthermore, image-text pairs usually contain unmatched components which are noisy for generation. A stage-refined context sampler is proposed to extract the matched cross-modal content for generation. Extensive experiments on two benchmarks demonstrate that our method achieves state-of-the-art performance without image input during inference.Please note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Efficiency-of-First-Order-Methods-for-Low-Rank-Tensor-Recovery-with-the-Tensor-Nuclear-Norm-Under-Strict-Complementarity"><a href="#Efficiency-of-First-Order-Methods-for-Low-Rank-Tensor-Recovery-with-the-Tensor-Nuclear-Norm-Under-Strict-Complementarity" class="headerlink" title="Efficiency of First-Order Methods for Low-Rank Tensor Recovery with the Tensor Nuclear Norm Under Strict Complementarity"></a>Efficiency of First-Order Methods for Low-Rank Tensor Recovery with the Tensor Nuclear Norm Under Strict Complementarity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01677">http://arxiv.org/abs/2308.01677</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dan Garber, Atara Kaplan</li>
<li>for: 本文是关于使用凸relaxation来重建低维度张量的研究。</li>
<li>methods: 本文使用了凸relaxation方法，包括拟合线性map和拟合quadratic形函数。</li>
<li>results: 本文得到了以下三个结果：1. 当 objective 函数为形式 $f(\mX)&#x3D;g(\mA\mX)+\langle{\mC,\mX}\rangle$，其中 $g$ 是强制凸函数，且 $\mA$ 是线性映射， Then 有一个quadratic growth bound，implying linear convergence rates for standard projected gradient methods。 2. 对于一个平滑函数 objective，当初始化在一个满足 SC 的优解中，then standard projected gradient methods only require SVD computations (for projecting onto the tensor nuclear norm ball) of rank that matches the tubal rank of the optimal solution。 3. 对于一个非平滑函数 objective，we derive similar results for the well known extragradient method。 Additionally, the paper extends many basic results regarding tensors of arbitrary order, which were previously obtained only for third-order tensors.<details>
<summary>Abstract</summary>
We consider convex relaxations for recovering low-rank tensors based on constrained minimization over a ball induced by the tensor nuclear norm, recently introduced in \cite{tensor_tSVD}. We build on a recent line of results that considered convex relaxations for the recovery of low-rank matrices and established that under a strict complementarity condition (SC), both the convergence rate and per-iteration runtime of standard gradient methods may improve dramatically. We develop the appropriate strict complementarity condition for the tensor nuclear norm ball and obtain the following main results under this condition: 1. When the objective to minimize is of the form $f(\mX)=g(\mA\mX)+\langle{\mC,\mX}\rangle$ , where $g$ is strongly convex and $\mA$ is a linear map (e.g., least squares), a quadratic growth bound holds, which implies linear convergence rates for standard projected gradient methods, despite the fact that $f$ need not be strongly convex. 2. For a smooth objective function, when initialized in certain proximity of an optimal solution which satisfies SC, standard projected gradient methods only require SVD computations (for projecting onto the tensor nuclear norm ball) of rank that matches the tubal rank of the optimal solution. In particular, when the tubal rank is constant, this implies nearly linear (in the size of the tensor) runtime per iteration, as opposed to super linear without further assumptions. 3. For a nonsmooth objective function which admits a popular smooth saddle-point formulation, we derive similar results to the latter for the well known extragradient method. An additional contribution which may be of independent interest, is the rigorous extension of many basic results regarding tensors of arbitrary order, which were previously obtained only for third-order tensors.
</details>
<details>
<summary>摘要</summary>
我们考虑使用凸关键函数来回复低维度tensor的方法，基于给定的凸关键函数球体上的受限最小化。我们在\cite{tensor_tSVD}中引入的tensor核心 нор的凸关键函数球体上建立了严格的完全相互矛盾（SC）的必要条件。我们获得以下主要结果：1. 当待解函数为$f(\mX)=g(\mA\mX)+\langle{\mC,\mX}\rangle$，其中$g$是强式凸函数且$\mA$是线性映射（例如最小二乘）， THEN 一个径度增长范围将成立，这意味着标准投影方法将在待解函数中展现出直线增长率，即使待解函数并不是强式凸函数。2. 当待解函数为几何函数且初值在具有SC的优解中， THEN 标准投影方法只需要在tensor核心 norm球体上进行SVD计算（用于对待解函数进行投影），其中SVD的维度与优解的管径维度相同。这意味着在无变量大小的tensor上，每次迭代的时间几乎是常量，而不是增长的。3. 当待解函数为非凸函数且具有流行的滑块形式则，我们 derive了类似的结果，对于通过extrapolation method来解决的问题。此外，我们还提供了一些独立有用的结果，例如在tensor的任意维度上，许多基本结果的扩展，这些结果在以前只有在第三维tensor上被证明。
</details></li>
</ul>
<hr>
<h2 id="End-to-End-Reinforcement-Learning-of-Koopman-Models-for-Economic-Nonlinear-MPC"><a href="#End-to-End-Reinforcement-Learning-of-Koopman-Models-for-Economic-Nonlinear-MPC" class="headerlink" title="End-to-End Reinforcement Learning of Koopman Models for Economic Nonlinear MPC"></a>End-to-End Reinforcement Learning of Koopman Models for Economic Nonlinear MPC</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01674">http://arxiv.org/abs/2308.01674</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Mayfrank, Alexander Mitsos, Manuel Dahmen</li>
<li>for: 本研究旨在提出一种数据驱动的汽车模型，以便在实时控制中减少计算成本。</li>
<li>methods: 该方法使用端到端学习来学习动态模型，并通过对实际数据进行训练来提高预测性能。</li>
<li>results: 研究结果显示，使用该方法可以实时生成高性能的预测模型，并且可以在控制设置变化时快速适应。同时，该方法与传统的最大预测精度方法和模型自适应方法相比，能够减少计算成本。<details>
<summary>Abstract</summary>
(Economic) nonlinear model predictive control ((e)NMPC) requires dynamic system models that are sufficiently accurate in all relevant state-space regions. These models must also be computationally cheap enough to ensure real-time tractability. Data-driven surrogate models for mechanistic models can be used to reduce the computational burden of (e)NMPC; however, such models are typically trained by system identification for maximum average prediction accuracy on simulation samples and perform suboptimally as part of actual (e)NMPC. We present a method for end-to-end reinforcement learning of dynamic surrogate models for optimal performance in (e)NMPC applications, resulting in predictive controllers that strike a favorable balance between control performance and computational demand. We validate our method on two applications derived from an established nonlinear continuous stirred-tank reactor model. We compare the controller performance to that of MPCs utilizing models trained by the prevailing maximum prediction accuracy paradigm, and model-free neural network controllers trained using reinforcement learning. We show that our method matches the performance of the model-free neural network controllers while consistently outperforming models derived from system identification. Additionally, we show that the MPC policies can react to changes in the control setting without retraining.
</details>
<details>
<summary>摘要</summary>
经济非线性模型预测控制（(E)NMPC）需要具有 suficiently 精度的动态系统模型，以 guaranteee 在所有有用的状态空间Region中准确。这些模型还需要够快速计算，以确保实时可行性。使用数据驱动的替身模型来减少(E)NMPC中模型计算的复杂性可以使其更加快速，但这些模型通常通过系统标准化来训练，并且在实际(E)NMPC应用中表现不佳。我们提出了一种终端渐进学习的动态替身模型，以实现在(E)NMPC应用中的优化性能。我们验证了我们的方法，并与现有的最大预测精度 paradigm和模型自由神经网络控制器进行比较。我们发现，我们的方法与模型自由神经网络控制器的性能相同，而且一直占据了模型来自系统标准化的性能。此外，我们还发现MPC策略可以根据控制设置的变化而反应，而不需要重新训练。
</details></li>
</ul>
<hr>
<h2 id="UniG-Encoder-A-Universal-Feature-Encoder-for-Graph-and-Hypergraph-Node-Classification"><a href="#UniG-Encoder-A-Universal-Feature-Encoder-for-Graph-and-Hypergraph-Node-Classification" class="headerlink" title="UniG-Encoder: A Universal Feature Encoder for Graph and Hypergraph Node Classification"></a>UniG-Encoder: A Universal Feature Encoder for Graph and Hypergraph Node Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01650">http://arxiv.org/abs/2308.01650</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/minhzou/unig-encoder">https://github.com/minhzou/unig-encoder</a></li>
<li>paper_authors: Minhao Zou, Zhongxue Gan, Yutong Wang, Junheng Zhang, Dongyan Sui, Chun Guan, Siyang Leng</li>
<li>for: 这篇论文的目的是为了提出一种 универсальный特征编码器，用于图和高级图表示学习。</li>
<li>methods: 该方法使用一个前向变换将图的 topological 关系转化为边或超边特征，然后将这些特征和原始节点特征 feed 入神经网络，最后使用反向变换，即投影矩阵的读取，获得编码后的节点嵌入。</li>
<li>results: 对于十二个 representativ 的高级图数据集和六个实际图数据集，该方法在对比state-of-the-art方法时显示出了更高的性能。<details>
<summary>Abstract</summary>
Graph and hypergraph representation learning has attracted increasing attention from various research fields. Despite the decent performance and fruitful applications of Graph Neural Networks (GNNs), Hypergraph Neural Networks (HGNNs), and their well-designed variants, on some commonly used benchmark graphs and hypergraphs, they are outperformed by even a simple Multi-Layer Perceptron. This observation motivates a reexamination of the design paradigm of the current GNNs and HGNNs and poses challenges of extracting graph features effectively. In this work, a universal feature encoder for both graph and hypergraph representation learning is designed, called UniG-Encoder. The architecture starts with a forward transformation of the topological relationships of connected nodes into edge or hyperedge features via a normalized projection matrix. The resulting edge/hyperedge features, together with the original node features, are fed into a neural network. The encoded node embeddings are then derived from the reversed transformation, described by the transpose of the projection matrix, of the network's output, which can be further used for tasks such as node classification. The proposed architecture, in contrast to the traditional spectral-based and/or message passing approaches, simultaneously and comprehensively exploits the node features and graph/hypergraph topologies in an efficient and unified manner, covering both heterophilic and homophilic graphs. The designed projection matrix, encoding the graph features, is intuitive and interpretable. Extensive experiments are conducted and demonstrate the superior performance of the proposed framework on twelve representative hypergraph datasets and six real-world graph datasets, compared to the state-of-the-art methods. Our implementation is available online at https://github.com/MinhZou/UniG-Encoder.
</details>
<details>
<summary>摘要</summary>
GRaph和嵌入图 representation learning 已经引起了不同领域的研究者的关注。尽管图神经网络（GNNs）、嵌入图神经网络（HGNNs）以及其设计的许多变体在一些常用的图和嵌入图上达到了不错的性能，但它们在一些简单的多层感知器（MLP）上被超越。这种观察激发了现有GNNs和HGNNs的设计思路的重新评估，并提出了提取图特征的挑战。在这种工作中，我们设计了一种通用的特征编码器，称为UniG-Encoder。该架构开始于将图中连接节点的topological关系转化为Edge或嵌入Edge特征via一个正规投影矩阵。然后，这些特征，与原始节点特征一起，被 feed into一个神经网络。编码后的节点嵌入则是通过投影矩阵的背景矩阵反转，从神经网络的输出获得的。该架构，与传统的spectral-based和/或message passing方法不同，同时并且全面地利用节点特征和图/嵌入图结构，提供了一种高效的、统一的方法，可以处理异质的图和嵌入图。设计的投影矩阵是直观和可解释的。我们在十二个代表性的嵌入图 dataset和六个实际图 dataset上进行了广泛的实验，并证明了我们的框架在这些dataset上的超越性。我们的实现可以在https://github.com/MinhZou/UniG-Encoder上找到。
</details></li>
</ul>
<hr>
<h2 id="MARLIM-Multi-Agent-Reinforcement-Learning-for-Inventory-Management"><a href="#MARLIM-Multi-Agent-Reinforcement-Learning-for-Inventory-Management" class="headerlink" title="MARLIM: Multi-Agent Reinforcement Learning for Inventory Management"></a>MARLIM: Multi-Agent Reinforcement Learning for Inventory Management</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01649">http://arxiv.org/abs/2308.01649</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rémi Leluc, Elie Kadoche, Antoine Bertoncello, Sébastien Gourvénec</li>
<li>for: 该论文旨在解决供应链中单一架构多种产品的存储管理问题，通过优化填充决策来保持供应和需求的平衡。</li>
<li>methods: 该论文提出了一种基于强化学习的新框架 named MARLIM，通过单或多个代理人在合作环境中开发控制器来解决存储管理问题。</li>
<li>results: 数值实验表明，基于强化学习方法比传统基线方法更有利于解决存储管理问题。<details>
<summary>Abstract</summary>
Maintaining a balance between the supply and demand of products by optimizing replenishment decisions is one of the most important challenges in the supply chain industry. This paper presents a novel reinforcement learning framework called MARLIM, to address the inventory management problem for a single-echelon multi-products supply chain with stochastic demands and lead-times. Within this context, controllers are developed through single or multiple agents in a cooperative setting. Numerical experiments on real data demonstrate the benefits of reinforcement learning methods over traditional baselines.
</details>
<details>
<summary>摘要</summary>
维护产品供应和需求的平衡是供应链业中最重要的挑战。本文提出了一个新的强化学习框架，名为MARLIM，以解决单一批制供应链中多产品的库存管理问题。在这个上下文中，控制器通过单一或多个代理人在合作环境下发展。实验结果显示，强化学习方法比传统基准方法更有利。Here's the breakdown of the translation:* 维护 (maintaining) = 维护 (maintaining)* 产品 (products) = 产品 (products)* 供应 (supply) = 供应 (supply)* 需求 (demand) = 需求 (demand)* 平衡 (balance) = 平衡 (balance)* 挑战 (challenge) = 挑战 (challenge)* 供应链 (supply chain) = 供应链 (supply chain)* 单一 (single) = 单一 (single)* 批制 (batch) = 批制 (batch)* 供应链 (supply chain) = 供应链 (supply chain)* 多产品 (multi-products) = 多产品 (multi-products)* 库存 (inventory) = 库存 (inventory)* 管理 (management) = 管理 (management)* 问题 (problem) = 问题 (problem)* 控制器 (controller) = 控制器 (controller)* 代理人 (agent) = 代理人 (agent)* 合作 (cooperative) = 合作 (cooperative)* 环境 (environment) = 环境 (environment)* 实验 (experiment) = 实验 (experiment)* 结果 (result) = 结果 (result)* 比 (than) = 比 (than)* 传统 (traditional) = 传统 (traditional)* 基准 (baseline) = 基准 (baseline)* 方法 (method) = 方法 (method)* 更 (more) = 更 (more)
</details></li>
</ul>
<hr>
<h2 id="Interleaving-GANs-with-knowledge-graphs-to-support-design-creativity-for-book-covers"><a href="#Interleaving-GANs-with-knowledge-graphs-to-support-design-creativity-for-book-covers" class="headerlink" title="Interleaving GANs with knowledge graphs to support design creativity for book covers"></a>Interleaving GANs with knowledge graphs to support design creativity for book covers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01626">http://arxiv.org/abs/2308.01626</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alexmotogna/generatorapi">https://github.com/alexmotogna/generatorapi</a></li>
<li>paper_authors: Alexandru Motogna, Adrian Groza</li>
<li>for: 这篇论文是为了提高书籍封面的创作而写的。</li>
<li>methods: 这篇论文使用了生成对抗网络（GANs）来生成图像，并通过与知识图加以混合来改变输入标题，以生成多种可能性。</li>
<li>results: 这篇论文的方法比之前的尝试更好地生成了图像，并且知识图可以为书作者或编辑提供更多的选择。<details>
<summary>Abstract</summary>
An attractive book cover is important for the success of a book. In this paper, we apply Generative Adversarial Networks (GANs) to the book covers domain, using different methods for training in order to obtain better generated images. We interleave GANs with knowledge graphs to alter the input title to obtain multiple possible options for any given title, which are then used as an augmented input to the generator. Finally, we use the discriminator obtained during the training phase to select the best images generated with new titles. Our method performed better at generating book covers than previous attempts, and the knowledge graph gives better options to the book author or editor compared to using GANs alone.
</details>
<details>
<summary>摘要</summary>
一本有吸引力的书封面对书的成功很重要。在这篇论文中，我们使用生成对抗网络（GANs）来改进书封面领域中的生成图像。我们在训练中使用不同的方法，以获得更好的生成图像。我们将知识图库与GANs相互嵌入，以对输入书名进行修改，从而获得多个可能的选项。最后，我们使用训练阶段获得的推识器，选择最佳的生成图像。我们的方法在生成书封面方面比前一次尝试更好，而知识图库对书作者或编辑提供了更多的选择。
</details></li>
</ul>
<hr>
<h2 id="Weighted-Multi-Level-Feature-Factorization-for-App-ads-CTR-and-installation-prediction"><a href="#Weighted-Multi-Level-Feature-Factorization-for-App-ads-CTR-and-installation-prediction" class="headerlink" title="Weighted Multi-Level Feature Factorization for App ads CTR and installation prediction"></a>Weighted Multi-Level Feature Factorization for App ads CTR and installation prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02568">http://arxiv.org/abs/2308.02568</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/knife982000/recsys2023challenge">https://github.com/knife982000/recsys2023challenge</a></li>
<li>paper_authors: Juan Manuel Rodriguez, Antonela Tommasel</li>
<li>for: 本文是针对ACM RecSys Challenge 2023进行报告，主要目标是预测用户点击和安装应用程序的概率，以优化深层渠道优化和尊重用户隐私。</li>
<li>methods: 该方法基于Weighted Multi-Level Feature Factorization，即在不同层次上结合任务特定和共享特征进行特征工程，以优化点击和安装两个相关 yet 不同的任务。</li>
<li>results: 本文在ACM RecSys Challenge 2023的学术赛道决赛中获得了11名和总分55的成绩，并在<a target="_blank" rel="noopener" href="https://github.com/knife982000/RecSys2023Challenge%E4%B8%8A%E5%85%AC%E5%BC%80%E4%BA%86%E4%BB%A3%E7%A0%81%E3%80%82">https://github.com/knife982000/RecSys2023Challenge上公开了代码。</a><details>
<summary>Abstract</summary>
This paper provides an overview of the approach we used as team ISISTANITOS for the ACM RecSys Challenge 2023. The competition was organized by ShareChat, and involved predicting the probability of a user clicking an app ad and/or installing an app, to improve deep funnel optimization and a special focus on user privacy. Our proposed method inferring the probabilities of clicking and installing as two different, but related tasks. Hence, the model engineers a specific set of features for each task and a set of shared features. Our model is called Weighted Multi-Level Feature Factorization because it considers the interaction of different order features, where the order is associated to the depth in a neural network. The prediction for a given task is generated by combining the task specific and shared features on the different levels. Our submission achieved the 11 rank and overall score of 55 in the competition academia-track final results. We release our source code at: https://github.com/knife982000/RecSys2023Challenge
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Multimodal-Indoor-Localisation-in-Parkinson’s-Disease-for-Detecting-Medication-Use-Observational-Pilot-Study-in-a-Free-Living-Setting"><a href="#Multimodal-Indoor-Localisation-in-Parkinson’s-Disease-for-Detecting-Medication-Use-Observational-Pilot-Study-in-a-Free-Living-Setting" class="headerlink" title="Multimodal Indoor Localisation in Parkinson’s Disease for Detecting Medication Use: Observational Pilot Study in a Free-Living Setting"></a>Multimodal Indoor Localisation in Parkinson’s Disease for Detecting Medication Use: Observational Pilot Study in a Free-Living Setting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02419">http://arxiv.org/abs/2308.02419</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ferdianjovan/Multihead-Dual-Convolutional-Self-Attention">https://github.com/ferdianjovan/Multihead-Dual-Convolutional-Self-Attention</a></li>
<li>paper_authors: Ferdian Jovan, Catherine Morgan, Ryan McConville, Emma L. Tonkin, Ian Craddock, Alan Whone</li>
<li>for: 这个论文是为了提高现有的indoor localization方法的效果而写的。</li>
<li>methods: 这个论文使用了transformer模型，使用了双Modal的RSSI和加速度数据来提高indoor localization的准确率。</li>
<li>results: 论文的evaluation表明，该方法可以在real-world condition下提高indoor localization的准确率，并且可以准确地判断PD参与者是否正在服用levodopa药物。<details>
<summary>Abstract</summary>
Parkinson's disease (PD) is a slowly progressive, debilitating neurodegenerative disease which causes motor symptoms including gait dysfunction. Motor fluctuations are alterations between periods with a positive response to levodopa therapy ("on") and periods marked by re-emergency of PD symptoms ("off") as the response to medication wears off. These fluctuations often affect gait speed and they increase in their disabling impact as PD progresses. To improve the effectiveness of current indoor localisation methods, a transformer-based approach utilising dual modalities which provide complementary views of movement, Received Signal Strength Indicator (RSSI) and accelerometer data from wearable devices, is proposed. A sub-objective aims to evaluate whether indoor localisation, including its in-home gait speed features (i.e. the time taken to walk between rooms), could be used to evaluate motor fluctuations by detecting whether the person with PD is taking levodopa medications or withholding them. To properly evaluate our proposed method, we use a free-living dataset where the movements and mobility are greatly varied and unstructured as expected in real-world conditions. 24 participants lived in pairs (consisting of one person with PD, one control) for five days in a smart home with various sensors. Our evaluation on the resulting dataset demonstrates that our proposed network outperforms other methods for indoor localisation. The sub-objective evaluation shows that precise room-level localisation predictions, transformed into in-home gait speed features, produce accurate predictions on whether the PD participant is taking or withholding their medications.
</details>
<details>
<summary>摘要</summary>
帕金森病 (PD) 是一种慢速进行、抑酸性减退性神经病，引起运动征瘤包括跑步功能不优。运动波动是指在 леvodopa 治疗中有效期内和无效期间的变化，这些变化通常影响跑步速度，随着病情进展而加剧。为了改善当前indoor localization方法的效果，一种基于 transformer 的方法，使用 dual modalities 提供了补做的视角，包括Received Signal Strength Indicator (RSSI) 和加速器数据从 wearable 设备。一个副目标是评估whether indoor localization，包括室内跑步速度特征（即在房间之间行走的时间），可以用于评估PD参与者是否服用了levodopa 药物。为了准确评估我们的提议方法，我们使用了一个免费生活数据集，其中运动和 mobilty 具有很大的变化和不结构性，如预期的实际条件中。24名参与者（其中12名PD参与者和12名控制参与者）在 smart home 中生活了5天，并使用了各种感知器。我们对所获得的数据进行评估，并示出了我们的提议网络在indoor localization方面的出色表现。副目标评估显示，精准的房间层级本地化预测，经过转换为室内跑步速度特征，可以生成准确的PD参与者是否服用了levodopa 药物的预测。
</details></li>
</ul>
<hr>
<h2 id="A-Novel-Convolutional-Neural-Network-Architecture-with-a-Continuous-Symmetry"><a href="#A-Novel-Convolutional-Neural-Network-Architecture-with-a-Continuous-Symmetry" class="headerlink" title="A Novel Convolutional Neural Network Architecture with a Continuous Symmetry"></a>A Novel Convolutional Neural Network Architecture with a Continuous Symmetry</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01621">http://arxiv.org/abs/2308.01621</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liuyao12/ConvNets-PDE-perspective">https://github.com/liuyao12/ConvNets-PDE-perspective</a></li>
<li>paper_authors: Yao Liu, Hang Shao, Bing Bai</li>
<li>for: 这种新的卷积神经网络架构是基于一种类型的偏微分方程（PDE），即 quasi-linear 超声速系统。</li>
<li>methods: 这种架构使得权重可以通过一个连续群的对称性进行修改，这与传统的模型不同，其架构和权重基本固定。</li>
<li>results: 与传统模型相比，这种架构在图像分类任务上具有相似的性能，同时具有内部对称性作为一个新的愿望属性。<details>
<summary>Abstract</summary>
This paper introduces a new Convolutional Neural Network (ConvNet) architecture inspired by a class of partial differential equations (PDEs) called quasi-linear hyperbolic systems. With comparable performance on the image classification task, it allows for the modification of the weights via a continuous group of symmetry. This is a significant shift from traditional models where the architecture and weights are essentially fixed. We wish to promote the (internal) symmetry as a new desirable property for a neural network, and to draw attention to the PDE perspective in analyzing and interpreting ConvNets in the broader Deep Learning community.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了一种新的卷积神经网络（ConvNet）架构， Drawing inspiration from a class ofpartial differential equations（PDEs）called quasi-linear hyperbolic systems. 与传统模型相比，这种架构允许权重的修改via continuous group of symmetry，这是一种重要的Shift from traditional models, where the architecture and weights are essentially fixed. 我们希望通过推广这种（内部）对称性作为神经网络的新有优点，并吸引Deep Learning社区更广泛关注PDE的视角来分析和解释ConvNets.Note: "quasi-linear hyperbolic systems" in the original text is translated as "quasi-linear hyperbolic systems" in Simplified Chinese, as there is no direct equivalent term in Chinese.
</details></li>
</ul>
<hr>
<h2 id="Assessing-Systematic-Weaknesses-of-DNNs-using-Counterfactuals"><a href="#Assessing-Systematic-Weaknesses-of-DNNs-using-Counterfactuals" class="headerlink" title="Assessing Systematic Weaknesses of DNNs using Counterfactuals"></a>Assessing Systematic Weaknesses of DNNs using Counterfactuals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01614">http://arxiv.org/abs/2308.01614</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sujan Sai Gannamaneni, Michael Mock, Maram Akila</li>
<li>for: 这篇论文旨在检查和验证深度神经网络（DNN）在安全应用中的测试方法。</li>
<li>methods: 本文使用了一种基于对应假设的Semantic Attribution方法来验证DNN的性能差异。</li>
<li>results: 本文的结果显示，在自驾车领域中的一个例子中，使用高度标注的 simulated 数据，发现了一些特定的人工资产（asset）对于深度神经网络（DNN）的性能有差异，但是只有在某些情况下，资产类型本身是性能差异的原因。<details>
<summary>Abstract</summary>
With the advancement of DNNs into safety-critical applications, testing approaches for such models have gained more attention. A current direction is the search for and identification of systematic weaknesses that put safety assumptions based on average performance values at risk. Such weaknesses can take on the form of (semantically coherent) subsets or areas in the input space where a DNN performs systematically worse than its expected average. However, it is non-trivial to attribute the reason for such observed low performances to the specific semantic features that describe the subset. For instance, inhomogeneities within the data w.r.t. other (non-considered) attributes might distort results. However, taking into account all (available) attributes and their interaction is often computationally highly expensive. Inspired by counterfactual explanations, we propose an effective and computationally cheap algorithm to validate the semantic attribution of existing subsets, i.e., to check whether the identified attribute is likely to have caused the degraded performance. We demonstrate this approach on an example from the autonomous driving domain using highly annotated simulated data, where we show for a semantic segmentation model that (i) performance differences among the different pedestrian assets exist, but (ii) only in some cases is the asset type itself the reason for this reduction in the performance.
</details>
<details>
<summary>摘要</summary>
Inspired by counterfactual explanations, we propose an efficient and cost-effective algorithm to validate the semantic attribution of existing subsets. We demonstrate this approach on an example from the autonomous driving domain using highly annotated simulated data. Our results show that (i) performance differences exist among different pedestrian assets, but (ii) the asset type is not always the reason for the reduced performance.
</details></li>
</ul>
<hr>
<h2 id="Feature-Noise-Boosts-DNN-Generalization-under-Label-Noise"><a href="#Feature-Noise-Boosts-DNN-Generalization-under-Label-Noise" class="headerlink" title="Feature Noise Boosts DNN Generalization under Label Noise"></a>Feature Noise Boosts DNN Generalization under Label Noise</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01609">http://arxiv.org/abs/2308.01609</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zlzenglu/fn">https://github.com/zlzenglu/fn</a></li>
<li>paper_authors: Lu Zeng, Xuan Chen, Xiaoshuang Shi, Heng Tao Shen</li>
<li>for: 增强深度神经网络（DNNs）对标签噪声的泛化性能。</li>
<li>methods: 直接将噪声添加到训练数据中的特征上，以增强DNNs的泛化性能。</li>
<li>results: 经过理论分析，发现标签噪声会削弱DNNs的泛化性能，而特征噪声则可以增强DNNs的泛化性能，并且可以通过调整特征噪声的类型和水平来确定最佳的噪声类型和水平。经过实验 validate 的结果表明，这种特征噪声方法可以显著提高DNNs在标签噪声下的泛化性能。<details>
<summary>Abstract</summary>
The presence of label noise in the training data has a profound impact on the generalization of deep neural networks (DNNs). In this study, we introduce and theoretically demonstrate a simple feature noise method, which directly adds noise to the features of training data, can enhance the generalization of DNNs under label noise. Specifically, we conduct theoretical analyses to reveal that label noise leads to weakened DNN generalization by loosening the PAC-Bayes generalization bound, and feature noise results in better DNN generalization by imposing an upper bound on the mutual information between the model weights and the features, which constrains the PAC-Bayes generalization bound. Furthermore, to ensure effective generalization of DNNs in the presence of label noise, we conduct application analyses to identify the optimal types and levels of feature noise to add for obtaining desirable label noise generalization. Finally, extensive experimental results on several popular datasets demonstrate the feature noise method can significantly enhance the label noise generalization of the state-of-the-art label noise method.
</details>
<details>
<summary>摘要</summary>
deep neural networks (DNNs) 的泛化能力受到标签噪声的影响。在本研究中，我们提出了一种简单的特征噪声方法，可以直接将噪声添加到训练数据的特征中，以提高 DNN 的泛化能力。我们进行了理论分析，发现标签噪声会减弱 DNN 的泛化能力，而特征噪声则可以通过限制模型参数和特征之间的互信息，提高 PAC-Bayes 泛化 bound。此外，为确保 DNN 在标签噪声下的有效泛化，我们进行了应用分析，并identified optimal types and levels of feature noise to add for obtaining desirable label noise generalization。最后，我们在多个popular dataset上进行了广泛的实验，并证明了 feature noise method可以显著提高 state-of-the-art label noise method 的泛化能力。
</details></li>
</ul>
<hr>
<h2 id="Discriminative-Graph-level-Anomaly-Detection-via-Dual-students-teacher-Model"><a href="#Discriminative-Graph-level-Anomaly-Detection-via-Dual-students-teacher-Model" class="headerlink" title="Discriminative Graph-level Anomaly Detection via Dual-students-teacher Model"></a>Discriminative Graph-level Anomaly Detection via Dual-students-teacher Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01947">http://arxiv.org/abs/2308.01947</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/whb605/gladst">https://github.com/whb605/gladst</a></li>
<li>paper_authors: Fu Lin, Xuexiong Luo, Jia Wu, Jian Yang, Shan Xue, Zitong Wang, Haonan Gong</li>
<li>for: 本研究的目标是找出图集中的异常图，并使用图表示来识别它们。由于现有的研究不够关于图级异常检测，因此我们需要更好地定义图级异常的描述。</li>
<li>methods: 我们首先定义图集中的异常图信息，包括节点和图像异常。然后，我们提出了一种可分辨性 Graph-level anomaly detection 框架，使用双学生-教师模型。教师模型使用准确的损失函数来让图表示更加分化。两个竞争学生模型通过正常和异常图来适应教师模型的图表示。最后，我们将两个学生模型的表示错误相加，以分别地识别异常图。</li>
<li>results: 我们在实验分析中发现，我们的方法可以有效地检测图集中的异常图。这表明我们的方法可以在实际世界中的图据集上进行异常检测。<details>
<summary>Abstract</summary>
Different from the current node-level anomaly detection task, the goal of graph-level anomaly detection is to find abnormal graphs that significantly differ from others in a graph set. Due to the scarcity of research on the work of graph-level anomaly detection, the detailed description of graph-level anomaly is insufficient. Furthermore, existing works focus on capturing anomalous graph information to learn better graph representations, but they ignore the importance of an effective anomaly score function for evaluating abnormal graphs. Thus, in this work, we first define anomalous graph information including node and graph property anomalies in a graph set and adopt node-level and graph-level information differences to identify them, respectively. Then, we introduce a discriminative graph-level anomaly detection framework with dual-students-teacher model, where the teacher model with a heuristic loss are trained to make graph representations more divergent. Then, two competing student models trained by normal and abnormal graphs respectively fit graph representations of the teacher model in terms of node-level and graph-level representation perspectives. Finally, we combine representation errors between two student models to discriminatively distinguish anomalous graphs. Extensive experiment analysis demonstrates that our method is effective for the graph-level anomaly detection task on graph datasets in the real world.
</details>
<details>
<summary>摘要</summary>
不同于现有的节点级异常检测任务，我们的目标是找到异常图的，这些图在图集中显著地不同于其他图。由于关于图级异常检测的研究缺乏，异常图的详细描述还不够。此外，现有的工作主要是捕捉异常图信息，以便学习更好的图表示，但它们忽略了评估异常图的有效 anomaly score 函数的重要性。因此，在这种工作中，我们首先定义图集中的异常图信息，包括节点和图性异常，并采用节点级和图级信息差异来识别它们。然后，我们提出了一种能够分类异常图的双学生-教师模型，其中教师模型通过规则损失来训练图表示更加分化。然后，两个竞争学生模型，一个通过正常图，另一个通过异常图进行训练，分别适应教师模型的节点级和图级表示视角。最后，我们将两个学生模型的表示错误相加，以分类异常图。我们的方法在实际世界的图据集上进行了广泛的实验分析，得到了有效的结果。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Multiplex-Graph-Learning-with-Complementary-and-Consistent-Information"><a href="#Unsupervised-Multiplex-Graph-Learning-with-Complementary-and-Consistent-Information" class="headerlink" title="Unsupervised Multiplex Graph Learning with Complementary and Consistent Information"></a>Unsupervised Multiplex Graph Learning with Complementary and Consistent Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01606">http://arxiv.org/abs/2308.01606</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/larryuestc/cocomg">https://github.com/larryuestc/cocomg</a></li>
<li>paper_authors: Liang Peng, Xin Wang, Xiaofeng Zhu</li>
<li>for: 本文提出了一种解决实际应用中issues的不监管多重图学习方法（UMGL），以提高不同下游任务的效果。</li>
<li>methods: 本方法使用多个多层感知网络（MLP）Encoder进行表示学习，并采用两个约束条件：保持节点之间的本地图structures，以处理异样问题，并 Maximize多个节点表示之间的相关性，以处理噪声问题。</li>
<li>results: 对比其他方法，本方法在实验中表现出了更高的效果和效率，并能够有效地解决异样和噪声问题。<details>
<summary>Abstract</summary>
Unsupervised multiplex graph learning (UMGL) has been shown to achieve significant effectiveness for different downstream tasks by exploring both complementary information and consistent information among multiple graphs. However, previous methods usually overlook the issues in practical applications, i.e., the out-of-sample issue and the noise issue. To address the above issues, in this paper, we propose an effective and efficient UMGL method to explore both complementary and consistent information. To do this, our method employs multiple MLP encoders rather than graph convolutional network (GCN) to conduct representation learning with two constraints, i.e., preserving the local graph structure among nodes to handle the out-of-sample issue, and maximizing the correlation of multiple node representations to handle the noise issue. Comprehensive experiments demonstrate that our proposed method achieves superior effectiveness and efficiency over the comparison methods and effectively tackles those two issues. Code is available at https://github.com/LarryUESTC/CoCoMG.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate the given text into Simplified Chinese.<</SYS>>Unsupervised多重图学习（UMGL）已经在不同的下游任务中显示出了 significanteffectiveness，通过探索多个图中的共同信息和差异信息。然而，之前的方法通常忽视了实际应用中的问题，即外样问题和噪声问题。为了解决这些问题，在这篇论文中，我们提出了一种有效和高效的 UMGL 方法，通过多个多层感知（MLP）编码器来进行表示学习，并遵循两个约束条件：保持节点之间的本地图结构，以处理外样问题，并 maximize 多个节点表示的相关性，以处理噪声问题。广泛的实验表明，我们提出的方法在比较方法中显示出了superior的有效性和高效性，并有效地解决了这两个问题。代码可以在 https://github.com/LarryUESTC/CoCoMG 上获取。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-based-surrogate-models-for-parametrized-PDEs-handling-geometric-variability-through-graph-neural-networks"><a href="#Deep-Learning-based-surrogate-models-for-parametrized-PDEs-handling-geometric-variability-through-graph-neural-networks" class="headerlink" title="Deep Learning-based surrogate models for parametrized PDEs: handling geometric variability through graph neural networks"></a>Deep Learning-based surrogate models for parametrized PDEs: handling geometric variability through graph neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01602">http://arxiv.org/abs/2308.01602</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicola Rares Franco, Stefania Fresca, Filippo Tombari, Andrea Manzoni</li>
<li>for: 用于模拟复杂物理系统，需要解决参数化时间依赖非线性偏微分方程（PDEs）。</li>
<li>methods: 使用图ael neural networks（GNNs）来代替 computationally expensive solvers，以实现更高效的simulation。</li>
<li>results: GNNs可以提供一个有效的surrogate model，可以涵盖不同的几何和分解精度，并且可以在不同的参数下进行泛化。<details>
<summary>Abstract</summary>
Mesh-based simulations play a key role when modeling complex physical systems that, in many disciplines across science and engineering, require the solution of parametrized time-dependent nonlinear partial differential equations (PDEs). In this context, full order models (FOMs), such as those relying on the finite element method, can reach high levels of accuracy, however often yielding intensive simulations to run. For this reason, surrogate models are developed to replace computationally expensive solvers with more efficient ones, which can strike favorable trade-offs between accuracy and efficiency. This work explores the potential usage of graph neural networks (GNNs) for the simulation of time-dependent PDEs in the presence of geometrical variability. In particular, we propose a systematic strategy to build surrogate models based on a data-driven time-stepping scheme where a GNN architecture is used to efficiently evolve the system. With respect to the majority of surrogate models, the proposed approach stands out for its ability of tackling problems with parameter dependent spatial domains, while simultaneously generalizing to different geometries and mesh resolutions. We assess the effectiveness of the proposed approach through a series of numerical experiments, involving both two- and three-dimensional problems, showing that GNNs can provide a valid alternative to traditional surrogate models in terms of computational efficiency and generalization to new scenarios. We also assess, from a numerical standpoint, the importance of using GNNs, rather than classical dense deep neural networks, for the proposed framework.
</details>
<details>
<summary>摘要</summary>
mesh-based 模拟在许多科学和工程领域中扮演关键角色，特别是当模型复杂物理系统时，需要解决 Parametrized 时间依赖非线性偏微分方程 (PDEs)。在这种情况下，全序模型 (FOMs)，如基于 finite element 方法的模型，可以达到高级别的准确性，但通常需要昂贵的计算。为了解决这个问题，人们通常会开发供应商模型，以取代 computationally 昂贵的解决方案，从而实现可接受的妥协。这项工作探讨了使用图 neuron 网络 (GNNs) 来模拟时间依赖 PDEs 的可能性。具体来说，我们提出了一种系统性的策略，通过数据驱动的时间步骤来建立仿真模型。与大多数供应商模型不同的是，我们的方法可以处理具有参数依赖的空间领域的问题，同时能够泛化到不同的几何和分辨率。我们通过一系列数学实验，包括二维和三维问题，证明了 GNNs 可以提供一个有效的代替方案，而不需要经过复杂的拟合。此外，我们还评估了使用 GNNs 而不是传统的密集深度神经网络，对该框架的重要性。
</details></li>
</ul>
<hr>
<h2 id="Experimental-Results-regarding-multiple-Machine-Learning-via-Quaternions"><a href="#Experimental-Results-regarding-multiple-Machine-Learning-via-Quaternions" class="headerlink" title="Experimental Results regarding multiple Machine Learning via Quaternions"></a>Experimental Results regarding multiple Machine Learning via Quaternions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01946">http://arxiv.org/abs/2308.01946</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianlei Zhu, Renzhe Zhu</li>
<li>for: 这项研究探讨了使用量子来实现机器学习算法的应用。</li>
<li>methods: 本研究使用随机生成的量子数据和对应的标签，将量子转换为旋转矩阵，并使其为输入特征。</li>
<li>results: 结果表明，使用量子和多种机器学习算法可以实现更高的准确率和显著改善在预测任务中。<details>
<summary>Abstract</summary>
This paper presents an experimental study on the application of quaternions in several machine learning algorithms. Quaternion is a mathematical representation of rotation in three-dimensional space, which can be used to represent complex data transformations. In this study, we explore the use of quaternions to represent and classify rotation data, using randomly generated quaternion data and corresponding labels, converting quaternions to rotation matrices, and using them as input features. Based on quaternions and multiple machine learning algorithms, it has shown higher accuracy and significantly improved performance in prediction tasks. Overall, this study provides an empirical basis for exploiting quaternions for machine learning tasks.
</details>
<details>
<summary>摘要</summary>
这个论文介绍了使用四元数在多种机器学习算法中的实验研究。四元数是三维空间中旋转的数学表示方式，可以用于表示复杂的数据变换。在这个研究中，我们研究了使用四元数来表示和分类旋转数据，使用随机生成的四元数数据和相应的标签，将四元数转换为旋转矩阵，并使其为输入特征。基于四元数和多种机器学习算法，研究结果显示了更高的准确率和明显改善的预测性能。总之，这个研究提供了使用四元数进行机器学习任务的实质基础。
</details></li>
</ul>
<hr>
<h2 id="SoK-Assessing-the-State-of-Applied-Federated-Machine-Learning"><a href="#SoK-Assessing-the-State-of-Applied-Federated-Machine-Learning" class="headerlink" title="SoK: Assessing the State of Applied Federated Machine Learning"></a>SoK: Assessing the State of Applied Federated Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02454">http://arxiv.org/abs/2308.02454</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tobias Müller, Maximilian Stäbler, Hugo Gascón, Frank Köster, Florian Matthes</li>
<li>for: 本研究旨在探讨 Federated Machine Learning（FedML）在实际应用中的现状和挑战。</li>
<li>methods: 本研究采用系统性的文献回顾方法，对74篇相关文章进行分析，描述 FedML 实现的特点和趋势，以及驱动其应用的动机和应用领域。</li>
<li>results: 本研究发现，FedML 在隐私敏感领域的应用具有许多优势，但在实际应用中还存在许多挑战，如数据质量问题、安全性和可信度问题等。<details>
<summary>Abstract</summary>
Machine Learning (ML) has shown significant potential in various applications; however, its adoption in privacy-critical domains has been limited due to concerns about data privacy. A promising solution to this issue is Federated Machine Learning (FedML), a model-to-data approach that prioritizes data privacy. By enabling ML algorithms to be applied directly to distributed data sources without sharing raw data, FedML offers enhanced privacy protections, making it suitable for privacy-critical environments. Despite its theoretical benefits, FedML has not seen widespread practical implementation. This study aims to explore the current state of applied FedML and identify the challenges hindering its practical adoption. Through a comprehensive systematic literature review, we assess 74 relevant papers to analyze the real-world applicability of FedML. Our analysis focuses on the characteristics and emerging trends of FedML implementations, as well as the motivational drivers and application domains. We also discuss the encountered challenges in integrating FedML into real-life settings. By shedding light on the existing landscape and potential obstacles, this research contributes to the further development and implementation of FedML in privacy-critical scenarios.
</details>
<details>
<summary>摘要</summary>
translate to Simplified Chinese:机器学习（ML）在不同的应用场景中表现出了很remarkable potential; however，its adoption in privacy-critical domains has been limited due to concerns about data privacy. A promising solution to this issue is Federated Machine Learning（FedML），a model-to-data approach that prioritizes data privacy. By enabling ML algorithms to be applied directly to distributed data sources without sharing raw data，FedML offers enhanced privacy protections，making it suitable for privacy-critical environments. Despite its theoretical benefits，FedML has not seen widespread practical implementation. This study aims to explore the current state of applied FedML and identify the challenges hindering its practical adoption. Through a comprehensive systematic literature review，we assess 74 relevant papers to analyze the real-world applicability of FedML. Our analysis focuses on the characteristics and emerging trends of FedML implementations，as well as the motivational drivers and application domains. We also discuss the encountered challenges in integrating FedML into real-life settings. By shedding light on the existing landscape and potential obstacles，this research contributes to the further development and implementation of FedML in privacy-critical scenarios.
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Representation-Learning-for-Time-Series-A-Review"><a href="#Unsupervised-Representation-Learning-for-Time-Series-A-Review" class="headerlink" title="Unsupervised Representation Learning for Time Series: A Review"></a>Unsupervised Representation Learning for Time Series: A Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01578">http://arxiv.org/abs/2308.01578</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mqwfrog/ults">https://github.com/mqwfrog/ults</a></li>
<li>paper_authors: Qianwen Meng, Hangwei Qian, Yong Liu, Yonghui Xu, Zhiqi Shen, Lizhen Cui</li>
<li>for: 本研究旨在探讨无监督表示学习方法的应用在时间序列数据上，以便学习不同特征表示而不需要每个样本的标注。</li>
<li>methods: 本研究使用了多种不监督表示学习技术，包括自适应表示学习、强化学习和对比学习等。</li>
<li>results: 经验证明，使用不监督表示学习方法可以在9种真实世界数据集上实现高度的特征表示能力，并且可以在不同的数据集上进行跨种类比较。<details>
<summary>Abstract</summary>
Unsupervised representation learning approaches aim to learn discriminative feature representations from unlabeled data, without the requirement of annotating every sample. Enabling unsupervised representation learning is extremely crucial for time series data, due to its unique annotation bottleneck caused by its complex characteristics and lack of visual cues compared with other data modalities. In recent years, unsupervised representation learning techniques have advanced rapidly in various domains. However, there is a lack of systematic analysis of unsupervised representation learning approaches for time series. To fill the gap, we conduct a comprehensive literature review of existing rapidly evolving unsupervised representation learning approaches for time series. Moreover, we also develop a unified and standardized library, named ULTS (i.e., Unsupervised Learning for Time Series), to facilitate fast implementations and unified evaluations on various models. With ULTS, we empirically evaluate state-of-the-art approaches, especially the rapidly evolving contrastive learning methods, on 9 diverse real-world datasets. We further discuss practical considerations as well as open research challenges on unsupervised representation learning for time series to facilitate future research in this field.
</details>
<details>
<summary>摘要</summary>
<<SYS>> transtableUnsupervised representation learning方法 aim to learn discriminative feature representations from unlabeled data, without the requirement of annotating every sample. Enabling unsupervised representation learning is extremely crucial for time series data, due to its unique annotation bottleneck caused by its complex characteristics and lack of visual cues compared with other data modalities. In recent years, unsupervised representation learning techniques have advanced rapidly in various domains. However, there is a lack of systematic analysis of unsupervised representation learning approaches for time series. To fill the gap, we conduct a comprehensive literature review of existing rapidly evolving unsupervised representation learning approaches for time series. Moreover, we also develop a unified and standardized library, named ULTS (i.e., Unsupervised Learning for Time Series), to facilitate fast implementations and unified evaluations on various models. With ULTS, we empirically evaluate state-of-the-art approaches, especially the rapidly evolving contrastive learning methods, on 9 diverse real-world datasets. We further discuss practical considerations as well as open research challenges on unsupervised representation learning for time series to facilitate future research in this field.<</SYS>>
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Training-of-Denoising-Diffusion-Model-Using-Dual-Discriminators-for-High-Fidelity-Multi-Speaker-TTS"><a href="#Adversarial-Training-of-Denoising-Diffusion-Model-Using-Dual-Discriminators-for-High-Fidelity-Multi-Speaker-TTS" class="headerlink" title="Adversarial Training of Denoising Diffusion Model Using Dual Discriminators for High-Fidelity Multi-Speaker TTS"></a>Adversarial Training of Denoising Diffusion Model Using Dual Discriminators for High-Fidelity Multi-Speaker TTS</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01573">http://arxiv.org/abs/2308.01573</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/komyeongjin/specdiff-gan">https://github.com/komyeongjin/specdiff-gan</a></li>
<li>paper_authors: Myeongjin Ko, Yong-Hoon Choi</li>
<li>for: 本研究旨在提高 diffusion speech synthesis 模型的表现，通过添加两个识别器：扩散识别器和spectrogram识别器，以学习扩散过程的分布和生成数据的分布。</li>
<li>methods: 本研究使用了 diffusion 模型，并结合了 GAN 结构。具体来说，使用了一个扩散识别器和一个spectrogram识别器，以学习扩散过程的分布和生成数据的分布。</li>
<li>results: 对比 FastSpeech2 和 DiffGAN-TTS 等当前状态的艺术模型，本研究的模型在不同的 метриках中表现出优于其他模型，包括 SSIM、MCD、F0 RMSE、STOI、PESQ 等。<details>
<summary>Abstract</summary>
The diffusion model is capable of generating high-quality data through a probabilistic approach. However, it suffers from the drawback of slow generation speed due to the requirement of a large number of time steps. To address this limitation, recent models such as denoising diffusion implicit models (DDIM) focus on generating samples without directly modeling the probability distribution, while models like denoising diffusion generative adversarial networks (GAN) combine diffusion processes with GANs. In the field of speech synthesis, a recent diffusion speech synthesis model called DiffGAN-TTS, utilizing the structure of GANs, has been introduced and demonstrates superior performance in both speech quality and generation speed. In this paper, to further enhance the performance of DiffGAN-TTS, we propose a speech synthesis model with two discriminators: a diffusion discriminator for learning the distribution of the reverse process and a spectrogram discriminator for learning the distribution of the generated data. Objective metrics such as structural similarity index measure (SSIM), mel-cepstral distortion (MCD), F0 root mean squared error (F0 RMSE), short-time objective intelligibility (STOI), perceptual evaluation of speech quality (PESQ), as well as subjective metrics like mean opinion score (MOS), are used to evaluate the performance of the proposed model. The evaluation results show that the proposed model outperforms recent state-of-the-art models such as FastSpeech2 and DiffGAN-TTS in various metrics. Our implementation and audio samples are located on GitHub.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate_parts:  - text: "The diffusion model"    translate: "扩散模型"  - text: "capable of generating high-quality data"    translate: "可以生成高质量数据"  - text: "through a probabilistic approach"    translate: "通过 probabilistic 方法"  - text: "However, it suffers from the drawback"    translate: "然而，它受到一个缺点"  - text: "of slow generation speed"    translate: "生成速度较慢"  - text: "due to the requirement of a large number of time steps"    translate: "因为需要较多的时间步骤"  - text: "To address this limitation"    translate: "以解决这些限制"  - text: "recent models such as denoising diffusion implicit models (DDIM)"    translate: "最近的模型，如杂谱扩散隐式模型（DDIM）"  - text: "focus on generating samples without directly modeling the probability distribution"    translate: "注重生成样本，不直接模型概率分布"  - text: "while models like denoising diffusion generative adversarial networks (GAN)"    translate: "如杂谱扩散生成敌方网络（GAN）"  - text: "combine diffusion processes with GANs"    translate: "将扩散过程与 GAN 结合"  - text: "In the field of speech synthesis"    translate: "在语音合成领域"  - text: "a recent diffusion speech synthesis model called DiffGAN-TTS"    translate: "一种最近的扩散语音合成模型，即 DiffGAN-TTS"  - text: "utilizing the structure of GANs"    translate: "利用 GAN 的结构"  - text: "has been introduced and demonstrates superior performance"    translate: "已经引入并示出了出色的表现"  - text: "in both speech quality and generation speed"    translate: "在语音质量和生成速度两个方面"  - text: "In this paper"    translate: "在这篇论文"  - text: "to further enhance the performance of DiffGAN-TTS"    translate: "以进一步提高 DiffGAN-TTS 的表现"  - text: "we propose a speech synthesis model with two discriminators"    translate: "我们提议一种语音合成模型，具有两个抑制器"  - text: "a diffusion discriminator for learning the distribution of the reverse process"    translate: "一个扩散抑制器，用于学习反向过程的分布"  - text: "and a spectrogram discriminator for learning the distribution of the generated data"    translate: "一个spectrogram抑制器，用于学习生成的数据的分布"  - text: "Objective metrics such as structural similarity index measure (SSIM)"    translate: "如结构相似度指标 (SSIM)"  - text: "mel-cepstral distortion (MCD)"    translate: "mel-cepstral 扭曲 (MCD)"  - text: "F0 root mean squared error (F0 RMSE)"    translate: "F0 根均方差 (F0 RMSE)"  - text: "short-time objective intelligibility (STOI)"    translate: "短时间目标可理解性 (STOI)"  - text: "perceptual evaluation of speech quality (PESQ)"    translate: "语音质量的主观评估 (PESQ)"  - text: "as well as subjective metrics like mean opinion score (MOS)"    translate: "以及主观指标如 mean opinion score (MOS)"  - text: "are used to evaluate the performance of the proposed model"    translate: "用于评估提议的模型表现"  - text: "The evaluation results show that the proposed model outperforms"    translate: "评估结果表明，提议的模型在"  - text: "recent state-of-the-art models such as FastSpeech2 and DiffGAN-TTS"    translate: "最近的状态艺术模型，如 FastSpeech2 和 DiffGAN-TTS"  - text: "in various metrics"    translate: "在多个指标中"Note: Some of the translated text may not be exact, as the meaning of the original text may not be perfectly conveyed in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Fast-Slate-Policy-Optimization-Going-Beyond-Plackett-Luce"><a href="#Fast-Slate-Policy-Optimization-Going-Beyond-Plackett-Luce" class="headerlink" title="Fast Slate Policy Optimization: Going Beyond Plackett-Luce"></a>Fast Slate Policy Optimization: Going Beyond Plackett-Luce</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01566">http://arxiv.org/abs/2308.01566</a></li>
<li>repo_url: None</li>
<li>paper_authors: Otmane Sakhi, David Rohde, Nicolas Chopin</li>
<li>for: 这篇论文主要针对大规模机器学习系统中的返回板块问题，即在查询时返回一个有序列表的项目。这种技术在搜索、信息检索和推荐系统等领域都有广泛的应用。</li>
<li>methods: 该论文使用政策优化框架来优化大规模决策系统，并提出了一种新的政策类型，基于决策函数的新降低。这种方法简单而高效，可承载巨大的动作空间。</li>
<li>results: 论文对具有百万级动作空间的问题进行比较，并证明了其效果超过常见采用的普拉克特-劳伦谱策略。<details>
<summary>Abstract</summary>
An increasingly important building block of large scale machine learning systems is based on returning slates; an ordered lists of items given a query. Applications of this technology include: search, information retrieval and recommender systems. When the action space is large, decision systems are restricted to a particular structure to complete online queries quickly. This paper addresses the optimization of these large scale decision systems given an arbitrary reward function. We cast this learning problem in a policy optimization framework and propose a new class of policies, born from a novel relaxation of decision functions. This results in a simple, yet efficient learning algorithm that scales to massive action spaces. We compare our method to the commonly adopted Plackett-Luce policy class and demonstrate the effectiveness of our approach on problems with action space sizes in the order of millions.
</details>
<details>
<summary>摘要</summary>
“ Returns 的列表（slate）已成为大规模机器学习系统的关键组件。这些技术在搜索、信息检索和推荐系统中应用。当动作空间较大时，决策系统会受到特定结构的限制，以快速完成在线查询。本文通过policy优化框架优化大规模决策系统，并提出了一种新的策略类型，基于决策函数的新降级。这种简单 yet efficient的学习算法可扩展到巨大的动作空间。我们与常见的Plackett-Luce策略类比较，并在动作空间大约为百万的问题上证明了我们的方法的有效性。”Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-Federated-Learning-in-Wireless-Networks-Pruning-Tackles-Bandwidth-Scarcity-and-System-Heterogeneity"><a href="#Hierarchical-Federated-Learning-in-Wireless-Networks-Pruning-Tackles-Bandwidth-Scarcity-and-System-Heterogeneity" class="headerlink" title="Hierarchical Federated Learning in Wireless Networks: Pruning Tackles Bandwidth Scarcity and System Heterogeneity"></a>Hierarchical Federated Learning in Wireless Networks: Pruning Tackles Bandwidth Scarcity and System Heterogeneity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01562">http://arxiv.org/abs/2308.01562</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Ferdous Pervej, Richeng Jin, Huaiyu Dai</li>
<li>for: 这篇论文目的是提出一种基于层次联合学习的无线网络模型，以适应实际无线网络中的限制和硬件限制。</li>
<li>methods: 论文使用了模型剔除技术，并提出了一种层次联合学习（PHFL）算法，以满足各种硬件限制和延迟限制。</li>
<li>results: 通过大量的 simulate，论文证明了该算法可以提高测试精度、增加wall clock时间、减少能耗和带宽需求。<details>
<summary>Abstract</summary>
While a practical wireless network has many tiers where end users do not directly communicate with the central server, the users' devices have limited computation and battery powers, and the serving base station (BS) has a fixed bandwidth. Owing to these practical constraints and system models, this paper leverages model pruning and proposes a pruning-enabled hierarchical federated learning (PHFL) in heterogeneous networks (HetNets). We first derive an upper bound of the convergence rate that clearly demonstrates the impact of the model pruning and wireless communications between the clients and the associated BS. Then we jointly optimize the model pruning ratio, central processing unit (CPU) frequency and transmission power of the clients in order to minimize the controllable terms of the convergence bound under strict delay and energy constraints. However, since the original problem is not convex, we perform successive convex approximation (SCA) and jointly optimize the parameters for the relaxed convex problem. Through extensive simulation, we validate the effectiveness of our proposed PHFL algorithm in terms of test accuracy, wall clock time, energy consumption and bandwidth requirement.
</details>
<details>
<summary>摘要</summary>
而实际无线网络具有多层次结构，用户设备具有有限的计算和电池能力，服务基站（BS）具有固定带宽。由于这些实际约束和系统模型，这篇论文利用模型剔除和提出了剔除启用的层次联合学习（PHFL）在不同类型网络（HetNets）中。我们首先得出了模型剔除对 converge 速率的Upper bound，并明确地显示了无线通信和客户端与关联的BS之间的模型剔除和计算剔除的影响。然后，我们联合优化客户端的模型剔除比例、中央处理器频率和传输功率，以最小化控制性 bound 下的执行时间和能量消耗。然而，由于原始问题不是凸型问题，我们使用Successive Convex Approximation（SCA）进行凸化优化参数，并联合优化参数以获得 relaxed 凸型问题的解。通过广泛的Simulation，我们证明了我们提出的PHFL算法在测试准确率、墙 clock 时间、能源消耗和带宽要求方面的有效性。
</details></li>
</ul>
<hr>
<h2 id="Motion-Planning-Diffusion-Learning-and-Planning-of-Robot-Motions-with-Diffusion-Models"><a href="#Motion-Planning-Diffusion-Learning-and-Planning-of-Robot-Motions-with-Diffusion-Models" class="headerlink" title="Motion Planning Diffusion: Learning and Planning of Robot Motions with Diffusion Models"></a>Motion Planning Diffusion: Learning and Planning of Robot Motions with Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01557">http://arxiv.org/abs/2308.01557</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joao Carvalho, An T. Le, Mark Baierl, Dorothea Koert, Jan Peters</li>
<li>for: 学习路径分布可以帮助加速机器人运动规划优化。已有成功的计划中，使用这些先前学习的路径生成模型作为新的规划问题的先验知识是非常有利的。</li>
<li>methods: 我们提议使用扩散模型来学习先验知识。通过扩散模型的逆噪处理，直接从任务目标条件下采样 posterior  trajectory 分布。此外，扩散模型在高维设定下能够有效地编码数据的多模性，这特别适合大型 trajectory 数据集。</li>
<li>results: 我们的实验表明，扩散模型是高维 trajectory 分布的强大先验知识。在 simulated 平面机器人和 7-odo 机器人臂 manipulate 环境中，我们的方法与基eline 进行比较，并在未看到过障碍物的环境中进行测试，以证明我们的方法的普适性。<details>
<summary>Abstract</summary>
Learning priors on trajectory distributions can help accelerate robot motion planning optimization. Given previously successful plans, learning trajectory generative models as priors for a new planning problem is highly desirable. Prior works propose several ways on utilizing this prior to bootstrapping the motion planning problem. Either sampling the prior for initializations or using the prior distribution in a maximum-a-posterior formulation for trajectory optimization. In this work, we propose learning diffusion models as priors. We then can sample directly from the posterior trajectory distribution conditioned on task goals, by leveraging the inverse denoising process of diffusion models. Furthermore, diffusion has been recently shown to effectively encode data multimodality in high-dimensional settings, which is particularly well-suited for large trajectory dataset. To demonstrate our method efficacy, we compare our proposed method - Motion Planning Diffusion - against several baselines in simulated planar robot and 7-dof robot arm manipulator environments. To assess the generalization capabilities of our method, we test it in environments with previously unseen obstacles. Our experiments show that diffusion models are strong priors to encode high-dimensional trajectory distributions of robot motions.
</details>
<details>
<summary>摘要</summary>
学习运动轨迹分布可以帮助加速机器人运动规划优化。已经成功的计划中，学习运动轨迹生成模型作为优先是非常有利的。先前的工作提出了多种利用这个优先来启动运动规划问题。可以从优先抽样或者在最大 posterior 形式中使用优先分布来优化运动规划。在这个工作中，我们提议学习扩散模型作为优先。我们可以通过扩散模型的逆减雑过程直接从后 posterior 轨迹分布中抽样，并且利用扩散模型可以有效地编码数据的多模性，尤其是在高维设定下。为了证明我们的方法效果，我们在 simulated 平面机器人和7-度 freedom 机器人 manipulate 环境中对我们的提议方法进行了比较。为了评估我们的方法泛化能力，我们在未看到的障碍物环境中进行了测试。我们的实验表明，扩散模型是高维轨迹分布的机器人运动优先。
</details></li>
</ul>
<hr>
<h2 id="InterAct-Exploring-the-Potentials-of-ChatGPT-as-a-Cooperative-Agent"><a href="#InterAct-Exploring-the-Potentials-of-ChatGPT-as-a-Cooperative-Agent" class="headerlink" title="InterAct: Exploring the Potentials of ChatGPT as a Cooperative Agent"></a>InterAct: Exploring the Potentials of ChatGPT as a Cooperative Agent</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01552">http://arxiv.org/abs/2308.01552</a></li>
<li>repo_url: None</li>
<li>paper_authors: Po-Lin Chen, Cheng-Shang Chang</li>
<li>for: 这个研究论文探讨了基于OpenAI的ChatGPT的embodied Agent系统的 интеграция，并评估了其对交互决策标准的影响。</li>
<li>methods: 我们引入了一种称为InterAct的方法，将ChatGPT fed with varied prompts，并将其分配为多个角色，如检查员和排序员，然后将其与原始语言模型结合。</li>
<li>results: 我们的研究显示，在AlfWorld中，包含6个任务的 simulate household environment中，ChatGPT的成功率达到98%，这表明ChatGPT在实际世界中完成复杂任务的能力强，从而预示了任务规划领域的进一步发展。<details>
<summary>Abstract</summary>
This research paper delves into the integration of OpenAI's ChatGPT into embodied agent systems, evaluating its influence on interactive decision-making benchmark. Drawing a parallel to the concept of people assuming roles according to their unique strengths, we introduce InterAct. In this approach, we feed ChatGPT with varied prompts, assigning it a numerous roles like a checker and a sorter, then integrating them with the original language model. Our research shows a remarkable success rate of 98% in AlfWorld, which consists of 6 different tasks in a simulated household environment, emphasizing the significance of proficient prompt engineering. The results highlight ChatGPT's competence in comprehending and performing intricate tasks effectively in real-world settings, thus paving the way for further advancements in task planning.
</details>
<details>
<summary>摘要</summary>
这篇研究论文探讨了OpenAI的ChatGPT在具体体系中的整合，评估其对交互决策标准的影响。从人们按照自己特点任务的角度出发，我们提出了InterAct方法。在这种方法中，我们对ChatGPT提供了多种提示，将其分配为多个角色，如检查员和排序员，然后与原始语言模型结合。我们的研究显示在AlfWorld中，包括6种不同任务的 simulate 的家庭环境中，ChatGPT的成功率达到98%，这些结果表明ChatGPT在真实世界中执行复杂任务时的能力，这对任务规划带来了新的可能性。
</details></li>
</ul>
<hr>
<h2 id="MusicLDM-Enhancing-Novelty-in-Text-to-Music-Generation-Using-Beat-Synchronous-Mixup-Strategies"><a href="#MusicLDM-Enhancing-Novelty-in-Text-to-Music-Generation-Using-Beat-Synchronous-Mixup-Strategies" class="headerlink" title="MusicLDM: Enhancing Novelty in Text-to-Music Generation Using Beat-Synchronous Mixup Strategies"></a>MusicLDM: Enhancing Novelty in Text-to-Music Generation Using Beat-Synchronous Mixup Strategies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01546">http://arxiv.org/abs/2308.01546</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ke Chen, Yusong Wu, Haohe Liu, Marianna Nezhurina, Taylor Berg-Kirkpatrick, Shlomo Dubnov</li>
<li>for: 这篇论文的目的是提出一种新的文本到音乐生成模型，以 Addressing the challenges of generating music with limited data and sensitivity to copyright and plagiarism.</li>
<li>methods: 该模型使用了Stable Diffusion和AudioLDM架构，并通过重新训练CLAP和Hifi-GAN vocoder来适应音乐领域。另外，该模型还使用了 beat tracking 模型和两种不同的mixup策略来进行数据扩展，以便生成更多样化的音乐。</li>
<li>results: 该模型可以生成更高质量和更多样化的音乐，同时仍然保持与输入文本的相关性。此外，该模型还可以提高CLAP score和新的评价指标，证明其在生成音乐的质量和创新性方面具有优势。<details>
<summary>Abstract</summary>
Diffusion models have shown promising results in cross-modal generation tasks, including text-to-image and text-to-audio generation. However, generating music, as a special type of audio, presents unique challenges due to limited availability of music data and sensitive issues related to copyright and plagiarism. In this paper, to tackle these challenges, we first construct a state-of-the-art text-to-music model, MusicLDM, that adapts Stable Diffusion and AudioLDM architectures to the music domain. We achieve this by retraining the contrastive language-audio pretraining model (CLAP) and the Hifi-GAN vocoder, as components of MusicLDM, on a collection of music data samples. Then, to address the limitations of training data and to avoid plagiarism, we leverage a beat tracking model and propose two different mixup strategies for data augmentation: beat-synchronous audio mixup and beat-synchronous latent mixup, which recombine training audio directly or via a latent embeddings space, respectively. Such mixup strategies encourage the model to interpolate between musical training samples and generate new music within the convex hull of the training data, making the generated music more diverse while still staying faithful to the corresponding style. In addition to popular evaluation metrics, we design several new evaluation metrics based on CLAP score to demonstrate that our proposed MusicLDM and beat-synchronous mixup strategies improve both the quality and novelty of generated music, as well as the correspondence between input text and generated music.
</details>
<details>
<summary>摘要</summary>
Diffusion models have shown promising results in cross-modal generation tasks, including text-to-image and text-to-audio generation. However, generating music, as a special type of audio, presents unique challenges due to limited availability of music data and sensitive issues related to copyright and plagiarism. In this paper, to tackle these challenges, we first construct a state-of-the-art text-to-music model, MusicLDM, that adapts Stable Diffusion and AudioLDM architectures to the music domain. We achieve this by retraining the contrastive language-audio pretraining model (CLAP) and the Hifi-GAN vocoder, as components of MusicLDM, on a collection of music data samples. Then, to address the limitations of training data and to avoid plagiarism, we leverage a beat tracking model and propose two different mixup strategies for data augmentation: beat-synchronous audio mixup and beat-synchronous latent mixup, which recombine training audio directly or via a latent embeddings space, respectively. Such mixup strategies encourage the model to interpolate between musical training samples and generate new music within the convex hull of the training data, making the generated music more diverse while still staying faithful to the corresponding style. In addition to popular evaluation metrics, we design several new evaluation metrics based on CLAP score to demonstrate that our proposed MusicLDM and beat-synchronous mixup strategies improve both the quality and novelty of generated music, as well as the correspondence between input text and generated music.
</details></li>
</ul>
<hr>
<h2 id="Lode-Enhancer-Level-Co-creation-Through-Scaling"><a href="#Lode-Enhancer-Level-Co-creation-Through-Scaling" class="headerlink" title="Lode Enhancer: Level Co-creation Through Scaling"></a>Lode Enhancer: Level Co-creation Through Scaling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01543">http://arxiv.org/abs/2308.01543</a></li>
<li>repo_url: None</li>
<li>paper_authors: Debosmita Bhaumik, Julian Togelius, Georgios N. Yannakakis, Ahmed Khalifa</li>
<li>for: 用AI技术帮助创建2D游戏关卡。</li>
<li>methods: 使用深度神经网络进行水平增强，以帮助用户在不同分辨率下创建和编辑关卡。</li>
<li>results: 通过训练神经网络，实现在不同分辨率下的同步编辑功能，并且通过提供高优先级块来增强水平的创建能力。<details>
<summary>Abstract</summary>
We explore AI-powered upscaling as a design assistance tool in the context of creating 2D game levels. Deep neural networks are used to upscale artificially downscaled patches of levels from the puzzle platformer game Lode Runner. The trained networks are incorporated into a web-based editor, where the user can create and edit levels at three different levels of resolution: 4x4, 8x8, and 16x16. An edit at any resolution instantly transfers to the other resolutions. As upscaling requires inventing features that might not be present at lower resolutions, we train neural networks to reproduce these features. We introduce a neural network architecture that is capable of not only learning upscaling but also giving higher priority to less frequent tiles. To investigate the potential of this tool and guide further development, we conduct a qualitative study with 3 designers to understand how they use it. Designers enjoyed co-designing with the tool, liked its underlying concept, and provided feedback for further improvement.
</details>
<details>
<summary>摘要</summary>
我们探索使用人工智能进行缩放作为游戏平台层级设计工具。我们使用深度神经网络缩放游戏《宝藏猎人》中的各个层级，并将训练好的网络集成到了基于网络的编辑器中。用户可以在3个不同的分辨率上创建和编辑层级：4x4、8x8和16x16。任何编辑操作都会同步到其他分辨率上。由于缩放需要创造不存在于较低分辨率上的特征，我们训练神经网络来重现这些特征。我们提出了一种神经网络架构，可以不仅学习缩放，还可以给较少出现的块优先级更高。为了了解这个工具的潜在可能性并提供进一步改进的建议，我们进行了3名设计师的质量研究，了解他们如何使用这个工具。设计师喜欢与这个工具合作，喜欢它的核心思想，并提供了进一步改进的反馈。
</details></li>
</ul>
<hr>
<h2 id="MFIM-Megapixel-Facial-Identity-Manipulation"><a href="#MFIM-Megapixel-Facial-Identity-Manipulation" class="headerlink" title="MFIM: Megapixel Facial Identity Manipulation"></a>MFIM: Megapixel Facial Identity Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01536">http://arxiv.org/abs/2308.01536</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sanghyeon Na</li>
<li>for: 本研究的目的是提出一种新的面孔交换框架，即高像素面孔标识修饰（MFIM）。</li>
<li>methods: 本模型使用预训练的StyleGAN进行GAN-倒转，以生成高像素图像。此外，本模型还使用3DMM来控制不同人脸特征的混合。</li>
<li>results: 经过广泛的实验，我们表明我们的模型可以达到状态对的性能，并且可以自由地调整新的人脸特征。此外，我们还提出了一种新的操作called ID mixing，可以通过semantic混合不同人脸特征来创建新的人脸标识。<details>
<summary>Abstract</summary>
Face swapping is a task that changes a facial identity of a given image to that of another person. In this work, we propose a novel face-swapping framework called Megapixel Facial Identity Manipulation (MFIM). The face-swapping model should achieve two goals. First, it should be able to generate a high-quality image. We argue that a model which is proficient in generating a megapixel image can achieve this goal. However, generating a megapixel image is generally difficult without careful model design. Therefore, our model exploits pretrained StyleGAN in the manner of GAN-inversion to effectively generate a megapixel image. Second, it should be able to effectively transform the identity of a given image. Specifically, it should be able to actively transform ID attributes (e.g., face shape and eyes) of a given image into those of another person, while preserving ID-irrelevant attributes (e.g., pose and expression). To achieve this goal, we exploit 3DMM that can capture various facial attributes. Specifically, we explicitly supervise our model to generate a face-swapped image with the desirable attributes using 3DMM. We show that our model achieves state-of-the-art performance through extensive experiments. Furthermore, we propose a new operation called ID mixing, which creates a new identity by semantically mixing the identities of several people. It allows the user to customize the new identity.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "Face swapping is a task that changes a facial identity of a given image to that of another person. In this work, we propose a novel face-swapping framework called Megapixel Facial Identity Manipulation (MFIM). The face-swapping model should achieve two goals. First, it should be able to generate a high-quality image. We argue that a model which is proficient in generating a megapixel image can achieve this goal. However, generating a megapixel image is generally difficult without careful model design. Therefore, our model exploits pretrained StyleGAN in the manner of GAN-inversion to effectively generate a megapixel image. Second, it should be able to effectively transform the identity of a given image. Specifically, it should be able to actively transform ID attributes (e.g., face shape and eyes) of a given image into those of another person, while preserving ID-irrelevant attributes (e.g., pose and expression). To achieve this goal, we exploit 3DMM that can capture various facial attributes. Specifically, we explicitly supervise our model to generate a face-swapped image with the desirable attributes using 3DMM. We show that our model achieves state-of-the-art performance through extensive experiments. Furthermore, we propose a new operation called ID mixing, which creates a new identity by semantically mixing the identities of several people. It allows the user to customize the new identity."中文翻译：面部换位是一项任务，把给定图像中的面部标识换成另一个人的面部标识。在这项工作中，我们提出了一种新的面部换位框架，称为高像素面部标识修饰（MFIM）。这个模型应该实现两个目标。首先，它应该能够生成高质量图像。我们认为一个能够生成高像素图像的模型就可以实现这一目标。然而，生成高像素图像通常需要精心的模型设计。因此，我们利用预训练的StyleGAN，通过GAN-反向方式来有效地生成高像素图像。其次，它应该能够有效地将给定图像中的面部标识转换成另一个人的面部标识。具体来说，它应该能够活动地将ID特征（例如脸形和眼睛）转换成另一个人的ID特征，保留ID不相关的特征（例如姿势和表情）。为了实现这一目标，我们利用3DMM，可以捕捉各种面部特征。我们显式地监督我们的模型，使其生成符合愿望的面部换位图像，使用3DMM。我们示出了我们模型在各种实验中的状态级表现。此外，我们还提出了一种新的操作，称为ID混合，它可以创造一个新的标识，通过semantic地混合多个人的标识。这allow用户自定义新的标识。
</details></li>
</ul>
<hr>
<h2 id="Food-Classification-using-Joint-Representation-of-Visual-and-Textual-Data"><a href="#Food-Classification-using-Joint-Representation-of-Visual-and-Textual-Data" class="headerlink" title="Food Classification using Joint Representation of Visual and Textual Data"></a>Food Classification using Joint Representation of Visual and Textual Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02562">http://arxiv.org/abs/2308.02562</a></li>
<li>repo_url: None</li>
<li>paper_authors: Prateek Mittal, Puneet Goyal, Joohi Chauhan</li>
<li>for: 本研究旨在提出一种多模态分类框架，用于健康食品分类。</li>
<li>methods: 提议的网络使用修改版EfficientNet和Mish激活函数进行图像分类，而传统的BERT变换器网络用于文本分类。</li>
<li>results: 对于大规模开源数据集UPMC Food-101，提议的网络与其他状态时方法进行比较，实验结果显示，提议的网络在图像和文本分类任务中具有显著的优势，与第二最佳方法相比，图像分类精度提高11.57%，文本分类精度提高6.34%。<details>
<summary>Abstract</summary>
Food classification is an important task in health care. In this work, we propose a multimodal classification framework that uses the modified version of EfficientNet with the Mish activation function for image classification, and the traditional BERT transformer-based network is used for text classification. The proposed network and the other state-of-the-art methods are evaluated on a large open-source dataset, UPMC Food-101. The experimental results show that the proposed network outperforms the other methods, a significant difference of 11.57% and 6.34% in accuracy is observed for image and text classification, respectively, when compared with the second-best performing method. We also compared the performance in terms of accuracy, precision, and recall for text classification using both machine learning and deep learning-based models. The comparative analysis from the prediction results of both images and text demonstrated the efficiency and robustness of the proposed approach.
</details>
<details>
<summary>摘要</summary>
食品分类是医疗领域中的重要任务。在这种工作中，我们提议了一种多modal分类框架，使用修改后的EfficientNet和Mish活动函数进行图像分类，而传统的BERT变换器网络用于文本分类。我们的提议网络和其他状态公共方法在大型开源数据集UPMC Food-101上进行评估。实验结果表明，我们的提议网络在图像和文本分类方面具有显著的优势，与第二好的方法相比，图像分类精度提高11.57%，文本分类精度提高6.34%。我们还对文本分类使用机器学习和深度学习模型进行比较分析，结果表明，我们的方法在预测结果中具有更高的准确率、精度和回归率。
</details></li>
</ul>
<hr>
<h2 id="Circumventing-Concept-Erasure-Methods-For-Text-to-Image-Generative-Models"><a href="#Circumventing-Concept-Erasure-Methods-For-Text-to-Image-Generative-Models" class="headerlink" title="Circumventing Concept Erasure Methods For Text-to-Image Generative Models"></a>Circumventing Concept Erasure Methods For Text-to-Image Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01508">http://arxiv.org/abs/2308.01508</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nyu-dice-lab/circumventing-concept-erasure">https://github.com/nyu-dice-lab/circumventing-concept-erasure</a></li>
<li>paper_authors: Minh Pham, Kelly O. Marshall, Chinmay Hegde</li>
<li>for: 本研究旨在检验五种最近提出的概念消除方法，以及这些方法是否能够彻底从文本到图像模型中除去目标概念。</li>
<li>methods: 本研究使用了五种最近提出的概念消除方法，包括隐藏概念、替换概念、排除概念、屏蔽概念和筛选概念等方法。</li>
<li>results: 研究发现，无论使用哪种方法，都无法彻底从文本到图像模型中除去目标概念。特别是，使用特殊学习的词嵌入可以很容易地从sanitized模型中恢复排除的概念，而无需对模型的权重进行任何修改。这些结果表明，后期概念消除方法不够可靠，并质疑它们在AI安全中的应用。<details>
<summary>Abstract</summary>
Text-to-image generative models can produce photo-realistic images for an extremely broad range of concepts, and their usage has proliferated widely among the general public. On the flip side, these models have numerous drawbacks, including their potential to generate images featuring sexually explicit content, mirror artistic styles without permission, or even hallucinate (or deepfake) the likenesses of celebrities. Consequently, various methods have been proposed in order to "erase" sensitive concepts from text-to-image models. In this work, we examine five recently proposed concept erasure methods, and show that targeted concepts are not fully excised from any of these methods. Specifically, we leverage the existence of special learned word embeddings that can retrieve "erased" concepts from the sanitized models with no alterations to their weights. Our results highlight the brittleness of post hoc concept erasure methods, and call into question their use in the algorithmic toolkit for AI safety.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Local-Global-Temporal-Fusion-Network-with-an-Attention-Mechanism-for-Multiple-and-Multiclass-Arrhythmia-Classification"><a href="#Local-Global-Temporal-Fusion-Network-with-an-Attention-Mechanism-for-Multiple-and-Multiclass-Arrhythmia-Classification" class="headerlink" title="Local-Global Temporal Fusion Network with an Attention Mechanism for Multiple and Multiclass Arrhythmia Classification"></a>Local-Global Temporal Fusion Network with an Attention Mechanism for Multiple and Multiclass Arrhythmia Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02416">http://arxiv.org/abs/2308.02416</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yun Kwan Kim, Minji Lee, Kunwook Jo, Hee Seok Song, Seong-Whan Lee</li>
<li>for: This paper aims to develop a clinical decision support system (CDSS) for arrhythmia classification, addressing the challenge of varying arrhythmia lengths.</li>
<li>methods: The proposed method combines local temporal information extraction, global pattern extraction, and local-global information fusion with attention to detect and classify arrhythmia with a constrained input length.</li>
<li>results: The proposed method achieved superior performance on the MIT-BIH arrhythmia database (MITDB) and MIT-BIH atrial fibrillation database (AFDB), outperforming comparison models. The method also demonstrated good generalization ability when tested on a different database.<details>
<summary>Abstract</summary>
Clinical decision support systems (CDSSs) have been widely utilized to support the decisions made by cardiologists when detecting and classifying arrhythmia from electrocardiograms (ECGs). However, forming a CDSS for the arrhythmia classification task is challenging due to the varying lengths of arrhythmias. Although the onset time of arrhythmia varies, previously developed methods have not considered such conditions. Thus, we propose a framework that consists of (i) local temporal information extraction, (ii) global pattern extraction, and (iii) local-global information fusion with attention to perform arrhythmia detection and classification with a constrained input length. The 10-class and 4-class performances of our approach were assessed by detecting the onset and offset of arrhythmia as an episode and the duration of arrhythmia based on the MIT-BIH arrhythmia database (MITDB) and MIT-BIH atrial fibrillation database (AFDB), respectively. The results were statistically superior to those achieved by the comparison models. To check the generalization ability of the proposed method, an AFDB-trained model was tested on the MITDB, and superior performance was attained compared with that of a state-of-the-art model. The proposed method can capture local-global information and dynamics without incurring information losses. Therefore, arrhythmias can be recognized more accurately, and their occurrence times can be calculated; thus, the clinical field can create more accurate treatment plans by using the proposed method.
</details>
<details>
<summary>摘要</summary>
临床决策支持系统（CDSS）已广泛应用于心电图（ECG）上的心Rate变化的诊断和分类。然而，为了建立一个CDSS来进行心Rate变化的诊断和分类，存在许多挑战。主要的问题在于心Rate变化的持续时间不同，导致过去的方法无法考虑这种情况。因此，我们提出了一种框架，包括（i）本地时间信息提取，（ii）全局模式提取，以及（iii）本地-全局信息融合 WITH attention来实现受限输入长度下的心Rate变化诊断和分类。我们使用MIT-BIH心Rate变化数据库（MITDB）和MIT-BIHatrioventricular flutter数据库（AFDB）进行10类和4类的评估，结果显著高于比较模型。为了证明提出的方法的通用能力，使用AFDB训练好的模型在MITDB上进行测试，并实现了与当前状态艺术模型的比较。这种方法可以捕捉本地-全局信息和动态，不会产生信息损失，因此可以更准确地识别心Rate变化，并计算其发生时间，从而为临床领域创造更加准确的治疗计划。
</details></li>
</ul>
<hr>
<h2 id="Online-Multi-Task-Learning-with-Recursive-Least-Squares-and-Recursive-Kernel-Methods"><a href="#Online-Multi-Task-Learning-with-Recursive-Least-Squares-and-Recursive-Kernel-Methods" class="headerlink" title="Online Multi-Task Learning with Recursive Least Squares and Recursive Kernel Methods"></a>Online Multi-Task Learning with Recursive Least Squares and Recursive Kernel Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01938">http://arxiv.org/abs/2308.01938</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gabriel R. Lencione, Fernando J. Von Zuben</li>
<li>for: 这paper是为了解决在线多任务学习（MTL）回归问题而提出的两种新方法。</li>
<li>methods: 这paper使用了高性能图 струкured MTL formulation，并基于Weighted Recursive Least Squares（WRLS）和Online Sparse Least Squares Support Vector Regression（OSLSSVR）的再归版本。</li>
<li>results: 通过采用任务堆叠变换，这paper实现了一个包含多个任务关系的矩阵，并将其 integrate到MT-WRLS和MT-OSLSSVR中。相比现有Literature，这paper实现了精确和近似归并，并在输入空间维度（MT-WRLS）或字典大小（MT-OSLSSVR）上实现了 quitropic per-instance cost。在一个实际世界的风速预测案例中，我们比较了这paper所提出的方法和其他竞争方法，并发现了 significan performance gain。<details>
<summary>Abstract</summary>
This paper introduces two novel approaches for Online Multi-Task Learning (MTL) Regression Problems. We employ a high performance graph-based MTL formulation and develop its recursive versions based on the Weighted Recursive Least Squares (WRLS) and the Online Sparse Least Squares Support Vector Regression (OSLSSVR). Adopting task-stacking transformations, we demonstrate the existence of a single matrix incorporating the relationship of multiple tasks and providing structural information to be embodied by the MT-WRLS method in its initialization procedure and by the MT-OSLSSVR in its multi-task kernel function. Contrasting the existing literature, which is mostly based on Online Gradient Descent (OGD) or cubic inexact approaches, we achieve exact and approximate recursions with quadratic per-instance cost on the dimension of the input space (MT-WRLS) or on the size of the dictionary of instances (MT-OSLSSVR). We compare our online MTL methods to other contenders in a real-world wind speed forecasting case study, evidencing the significant gain in performance of both proposed approaches.
</details>
<details>
<summary>摘要</summary>
Here's the Simplified Chinese translation:这篇论文介绍了两种新的在线多任务学习（MTL）回归问题的方法。我们使用高性能的图形基于的 MTL 形式，并开发了其循环版本，基于最小二乘回归（WRLS）和在线稀疏最小二乘支持向量回归（OSLSSVR）。通过任务堆叠变换，我们表明了一个单个矩阵可以捕捉多个任务之间的关系，并提供结构信息。这个矩阵在 MT-WRLS 方法的初始化过程中使用，以及 MT-OSLSSVR 方法的多任务核函数中使用。与现有文献，大多使用在线梯度下降（OGD）或立方不精确方法，我们实现了精确和近似循环的quadratic per-instance cost在输入空间维度（MT-WRLS）或实例词典大小（MT-OSLSSVR）。我们将这两种在线 MTL 方法与其他竞争者进行比较，在一个实际的风速预测案例中，证明了我们的两种方法具有显著的性能提升。
</details></li>
</ul>
<hr>
<h2 id="Minimax-Optimal-Q-Learning-with-Nearest-Neighbors"><a href="#Minimax-Optimal-Q-Learning-with-Nearest-Neighbors" class="headerlink" title="Minimax Optimal $Q$ Learning with Nearest Neighbors"></a>Minimax Optimal $Q$ Learning with Nearest Neighbors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01490">http://arxiv.org/abs/2308.01490</a></li>
<li>repo_url: None</li>
<li>paper_authors: Puning Zhao, Lifeng Lai<br>for: This paper focuses on modifying the original $Q$ learning method to make it suitable for continuous state spaces, and proposes two new $Q$ learning methods to improve the convergence rate.methods: The paper uses nearest neighbor approach to estimate $Q$ function, but with a direct nearest neighbor approach instead of the kernel nearest neighbor in discretized regions.results: The paper shows that both offline and online methods are minimax rate optimal, and the time complexity is significantly improved in high dimensional state spaces.<details>
<summary>Abstract</summary>
$Q$ learning is a popular model free reinforcement learning method. Most of existing works focus on analyzing $Q$ learning for finite state and action spaces. If the state space is continuous, then the original $Q$ learning method can not be directly used. A modification of the original $Q$ learning method was proposed in (Shah and Xie, 2018), which estimates $Q$ values with nearest neighbors. Such modification makes $Q$ learning suitable for continuous state space. (Shah and Xie, 2018) shows that the convergence rate of estimated $Q$ function is $\tilde{O}(T^{-1/(d+3)})$, which is slower than the minimax lower bound $\tilde{\Omega}(T^{-1/(d+2)})$, indicating that this method is not efficient. This paper proposes two new $Q$ learning methods to bridge the gap of convergence rates in (Shah and Xie, 2018), with one of them being offline, while the other is online. Despite that we still use nearest neighbor approach to estimate $Q$ function, the algorithms are crucially different from (Shah and Xie, 2018). In particular, we replace the kernel nearest neighbor in discretized region with a direct nearest neighbor approach. Consequently, our approach significantly improves the convergence rate. Moreover, the time complexity is also significantly improved in high dimensional state spaces. Our analysis shows that both offline and online methods are minimax rate optimal.
</details>
<details>
<summary>摘要</summary>
$Q$ 学习是一种流行的模型自由奖励学习方法。大多数现有工作都集中在finite state和动作空间上分析 $Q$ 学习。如果状态空间是连续的，那么原始 $Q$ 学习方法直接使用不可靠。(Shah and Xie, 2018) 提出了修改原始 $Q$ 学习方法的方法，该方法通过 nearest neighbors 来估计 $Q$ 值。这种修改使 $Q$ 学习适用于连续状态空间。(Shah and Xie, 2018) 显示了 estimated $Q$ 函数的减少率为 $\tilde{O}(T^{-1/(d+3)})$, 这 slower than minimax lower bound $\tilde{\Omega}(T^{-1/(d+2)})$, 这表明这种方法不够有效。这篇论文提出了两种新的 $Q$ 学习方法，其中一种是 offline，另一种是 online。尽管我们仍然使用 nearest neighbor 方法来估计 $Q$ 函数，但这些算法与 (Shah and Xie, 2018) 的方法有所不同。具体来说，我们将 kernel nearest neighbor 在离散区域中替换为直接 nearest neighbor 方法。因此，我们的方法可以 significatively 提高减少率。此外，在高维状态空间中，我们的时间复杂度也得到了显著改善。我们的分析表明，两种方法都是 minimax 率最优。
</details></li>
</ul>
<hr>
<h2 id="Efficient-neural-supersampling-on-a-novel-gaming-dataset"><a href="#Efficient-neural-supersampling-on-a-novel-gaming-dataset" class="headerlink" title="Efficient neural supersampling on a novel gaming dataset"></a>Efficient neural supersampling on a novel gaming dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01483">http://arxiv.org/abs/2308.01483</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antoine Mercier, Ruan Erasmus, Yashesh Savani, Manik Dhingra, Fatih Porikli, Guillaume Berger</li>
<li>for: 提高视频游戏的实时渲染效能，以满足更高的分辨率、帧率和 фото真实性要求。</li>
<li>methods: 使用神经网络算法进行精度较高的超采样渲染内容，比现有方法高效4倍。同时，我们提供了一个新的数据集，包括视频游戏中的运动 вектор和深度信息，这些信息可以用于测试和提高超解算法的性能。</li>
<li>results: 与现有方法相比，我们的方法可以提供4倍的效率，同时保持同等的准确性。此外，我们的数据集可以填补现有数据集的空白，并成为测试和提高超解技术的 valuable 资源。<details>
<summary>Abstract</summary>
Real-time rendering for video games has become increasingly challenging due to the need for higher resolutions, framerates and photorealism. Supersampling has emerged as an effective solution to address this challenge. Our work introduces a novel neural algorithm for supersampling rendered content that is 4 times more efficient than existing methods while maintaining the same level of accuracy. Additionally, we introduce a new dataset which provides auxiliary modalities such as motion vectors and depth generated using graphics rendering features like viewport jittering and mipmap biasing at different resolutions. We believe that this dataset fills a gap in the current dataset landscape and can serve as a valuable resource to help measure progress in the field and advance the state-of-the-art in super-resolution techniques for gaming content.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Online-covariance-estimation-for-stochastic-gradient-descent-under-Markovian-sampling"><a href="#Online-covariance-estimation-for-stochastic-gradient-descent-under-Markovian-sampling" class="headerlink" title="Online covariance estimation for stochastic gradient descent under Markovian sampling"></a>Online covariance estimation for stochastic gradient descent under Markovian sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01481">http://arxiv.org/abs/2308.01481</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abhishek Roy, Krishnakumar Balasubramanian</li>
<li>for: 这个论文主要针对 Stochastic Gradient Descent (SGD) 的在 Markovian 采样下的在线拟合方法 Covariance 估计。</li>
<li>methods: 论文使用了 batch-means 方法来估计 Covariance，并且对 Markovian 采样进行了分析。</li>
<li>results: 论文显示了在 Markovian 采样下，batch-means 方法的 convergence 率为 $O\big(\sqrt{d},n^{-1&#x2F;8}(\log n)^{1&#x2F;4}\big)$ 和 $O\big(\sqrt{d},n^{-1&#x2F;8}\big)$，与 $\iid$  случа子中最佳的 convergence 率相当，即Logarithmic 因子。此外，论文还研究了 SGD 动态中 $\ell_2$ 范数的错误的 converge 率，以及在 Markovian 采样下的 SGD 训练中的Linear 和 Logistic 回归模型的 numerics 示例。最后，论文应用了这些结论来解决了一个挑战性的攻击者可以在训练过程中随机修改特征来增加被分类到特定目标类的概率的问题。<details>
<summary>Abstract</summary>
We study the online overlapping batch-means covariance estimator for Stochastic Gradient Descent (SGD) under Markovian sampling. We show that the convergence rates of the covariance estimator are $O\big(\sqrt{d}\,n^{-1/8}(\log n)^{1/4}\big)$ and $O\big(\sqrt{d}\,n^{-1/8}\big)$ under state-dependent and state-independent Markovian sampling, respectively, with $d$ representing dimensionality and $n$ denoting the number of observations or SGD iterations. Remarkably, these rates match the best-known convergence rate previously established for the independent and identically distributed ($\iid$) case by \cite{zhu2021online}, up to logarithmic factors. Our analysis overcomes significant challenges that arise due to Markovian sampling, leading to the introduction of additional error terms and complex dependencies between the blocks of the batch-means covariance estimator. Moreover, we establish the convergence rate for the first four moments of the $\ell_2$ norm of the error of SGD dynamics under state-dependent Markovian data, which holds potential interest as an independent result. To validate our theoretical findings, we provide numerical illustrations to derive confidence intervals for SGD when training linear and logistic regression models under Markovian sampling. Additionally, we apply our approach to tackle the intriguing problem of strategic classification with logistic regression, where adversaries can adaptively modify features during the training process to increase their chances of being classified in a specific target class.
</details>
<details>
<summary>摘要</summary>
我们研究在采用Markovian sampling的线上遮盾Batch-means协方差估计器中的Stochastic Gradient Descent（SGD）。我们表明了协方差估计器的数据速度为$O\big(\sqrt{d}\,n^{-1/8}(\log n)^{1/4}\big)$和$O\big(\sqrt{d}\,n^{-1/8}\big)$，具体取决于Markovian sampling的状态。这些速度与以前在独立同分布（iid）情况下所得到的最佳速度相匹配，仅带有对数因子。我们的分析解决了由Markovian sampling引起的重要挑战，包括附加的错误项和几何上的复杂对话。此外，我们还证明了SGD动态中的第四个几何中的错误的$\ell_2$ нор的数据速度，这个结果可能具有独立的意义。在实验中，我们提供了几个数据示例，以建立SGD训练Linear和Logistic regression模型的信任区间。此外，我们还应用我们的方法解决了具有挑战性的分类问题，其中敌人可以在训练过程中随机修改特征，以增加他们被特定目标类别中分类的机会。
</details></li>
</ul>
<hr>
<h2 id="Interpretable-Machine-Learning-for-Discovery-Statistical-Challenges-Opportunities"><a href="#Interpretable-Machine-Learning-for-Discovery-Statistical-Challenges-Opportunities" class="headerlink" title="Interpretable Machine Learning for Discovery: Statistical Challenges &amp; Opportunities"></a>Interpretable Machine Learning for Discovery: Statistical Challenges &amp; Opportunities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01475">http://arxiv.org/abs/2308.01475</a></li>
<li>repo_url: None</li>
<li>paper_authors: Genevera I. Allen, Luqin Gan, Lili Zheng</li>
<li>for: 本研究的目的是探讨可解释机器学习技术在大数据中进行发现和探索的应用。</li>
<li>methods: 本研究使用了多种可解释机器学习技术，包括可视化、预测和数据分析等方法，以便从大数据中提取有用的信息和发现新知识。</li>
<li>results: 本研究发现了一些在可解释机器学习技术应用中的挑战，包括数据采样和稳定性问题，以及验证发现的困难。同时，研究也提出了一些解决这些挑战的方法和技术。<details>
<summary>Abstract</summary>
New technologies have led to vast troves of large and complex datasets across many scientific domains and industries. People routinely use machine learning techniques to not only process, visualize, and make predictions from this big data, but also to make data-driven discoveries. These discoveries are often made using Interpretable Machine Learning, or machine learning models and techniques that yield human understandable insights. In this paper, we discuss and review the field of interpretable machine learning, focusing especially on the techniques as they are often employed to generate new knowledge or make discoveries from large data sets. We outline the types of discoveries that can be made using Interpretable Machine Learning in both supervised and unsupervised settings. Additionally, we focus on the grand challenge of how to validate these discoveries in a data-driven manner, which promotes trust in machine learning systems and reproducibility in science. We discuss validation from both a practical perspective, reviewing approaches based on data-splitting and stability, as well as from a theoretical perspective, reviewing statistical results on model selection consistency and uncertainty quantification via statistical inference. Finally, we conclude by highlighting open challenges in using interpretable machine learning techniques to make discoveries, including gaps between theory and practice for validating data-driven-discoveries.
</details>
<details>
<summary>摘要</summary>
新技术使得各科学领域和产业中的大量大数据备受欢迎。人们常常使用机器学习技术不仅处理、视觉和预测这些大数据，还可以通过机器学习模型和技术获得人类可理解的发现。在这篇论文中，我们讨论了机器学习可解释的场景，特别是在大数据集中使用机器学习模型和技术来获得新的发现。我们还详细介绍了在超级vised和无级vised情况下使用机器学习可解释的发现方法，以及如何验证这些发现的数据驱动方法。我们还讨论了验证这些发现的挑战，包括数据分割和稳定性问题，以及统计学的模型选择一致性和不确定性量化问题。最后，我们结束这篇文章，描述了使用机器学习可解释技术进行发现时存在的开放挑战。
</details></li>
</ul>
<hr>
<h2 id="Reverse-Stable-Diffusion-What-prompt-was-used-to-generate-this-image"><a href="#Reverse-Stable-Diffusion-What-prompt-was-used-to-generate-this-image" class="headerlink" title="Reverse Stable Diffusion: What prompt was used to generate this image?"></a>Reverse Stable Diffusion: What prompt was used to generate this image?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01472">http://arxiv.org/abs/2308.01472</a></li>
<li>repo_url: None</li>
<li>paper_authors: Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, Mubarak Shah</li>
<li>for: 本研究的目的是提出一种新的文本描述生成模型，用于预测由生成扩散模型生成的图像所关联的文本描述。</li>
<li>methods: 我们采用了一种组合白盒和黑盒模型的方法，其中包括了聚合文本描述 regression 和多标签词汇分类目标函数。此外，我们还采用了一种师生学习程序和领域适应诊断器学习方法来提高方法的性能。</li>
<li>results: 我们在DiffusionDB数据集上进行了实验，并取得了优秀的结果。在白盒模型上，我们的新学习框架得到了最高的提升。此外，我们还发现了一个有趣的发现：通过直接将扩散模型用于文本-图像生成任务，可以使模型生成的图像与输入文本更加吻合。<details>
<summary>Abstract</summary>
Text-to-image diffusion models such as Stable Diffusion have recently attracted the interest of many researchers, and inverting the diffusion process can play an important role in better understanding the generative process and how to engineer prompts in order to obtain the desired images. To this end, we introduce the new task of predicting the text prompt given an image generated by a generative diffusion model. We combine a series of white-box and black-box models (with and without access to the weights of the diffusion network) to deal with the proposed task. We propose a novel learning framework comprising of a joint prompt regression and multi-label vocabulary classification objective that generates improved prompts. To further improve our method, we employ a curriculum learning procedure that promotes the learning of image-prompt pairs with lower labeling noise (i.e. that are better aligned), and an unsupervised domain-adaptive kernel learning method that uses the similarities between samples in the source and target domains as extra features. We conduct experiments on the DiffusionDB data set, predicting text prompts from images generated by Stable Diffusion. Our novel learning framework produces excellent results on the aforementioned task, yielding the highest gains when applied on the white-box model. In addition, we make an interesting discovery: training a diffusion model on the prompt generation task can make the model generate images that are much better aligned with the input prompts, when the model is directly reused for text-to-image generation.
</details>
<details>
<summary>摘要</summary>
文本至图像协托模型，如稳定协托，最近吸引了许多研究人员的关注，而协托过程的逆转也可以更好地理解生成过程和如何引入提示以获得所需的图像。为此，我们介绍了预测图像生成模型中的文本提示任务。我们组合了一系列的白盒和黑盒模型（具有或无Diffusion网络权重的访问）来处理该任务。我们提出了一种新的学习框架，包括文本提示回归和多标签词汇分类目标，可以生成改进的提示。为了进一步改进我们的方法，我们采用了课程学习程序，其中优先采用低噪音（即更好地对齐）的图像提示对象，以及一种不supervised领域适应kernel学习方法，该方法使用源和目标领域样本之间的相似性作为额外特征。我们在DiffusionDB数据集上进行了实验，预测由Stable Diffusion生成的图像中的文本提示。我们的新的学习框架在该任务上具有优秀的成绩，尤其是在白盒模型上。此外，我们发现了一项 interessante发现：通过直接将协托模型学习到提示生成任务上，可以使模型生成与输入提示更好地对齐的图像。
</details></li>
</ul>
<hr>
<h2 id="Implicit-Occupancy-Flow-Fields-for-Perception-and-Prediction-in-Self-Driving"><a href="#Implicit-Occupancy-Flow-Fields-for-Perception-and-Prediction-in-Self-Driving" class="headerlink" title="Implicit Occupancy Flow Fields for Perception and Prediction in Self-Driving"></a>Implicit Occupancy Flow Fields for Perception and Prediction in Self-Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01471">http://arxiv.org/abs/2308.01471</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ben Agro, Quinlan Sykora, Sergio Casas, Raquel Urtasun</li>
<li>for: 这个论文旨在提出一种能够同时捕捉它周围环境和预测其他交通参与者未来行为的自动驾驶车辆（SDV）。</li>
<li>methods: 这个论文使用了一种混合了卷积神经网络和注意力机制的方法，能够同时预测它周围的占用和流动Grid，并且可以 directly queried by the motion planner at continuous spatio-temporal locations。</li>
<li>results: 经过广泛的实验证明，这个方法可以在城市和高速公路上的不同环境下表现出优于当前状态的论文。<details>
<summary>Abstract</summary>
A self-driving vehicle (SDV) must be able to perceive its surroundings and predict the future behavior of other traffic participants. Existing works either perform object detection followed by trajectory forecasting of the detected objects, or predict dense occupancy and flow grids for the whole scene. The former poses a safety concern as the number of detections needs to be kept low for efficiency reasons, sacrificing object recall. The latter is computationally expensive due to the high-dimensionality of the output grid, and suffers from the limited receptive field inherent to fully convolutional networks. Furthermore, both approaches employ many computational resources predicting areas or objects that might never be queried by the motion planner. This motivates our unified approach to perception and future prediction that implicitly represents occupancy and flow over time with a single neural network. Our method avoids unnecessary computation, as it can be directly queried by the motion planner at continuous spatio-temporal locations. Moreover, we design an architecture that overcomes the limited receptive field of previous explicit occupancy prediction methods by adding an efficient yet effective global attention mechanism. Through extensive experiments in both urban and highway settings, we demonstrate that our implicit model outperforms the current state-of-the-art. For more information, visit the project website: https://waabi.ai/research/implicito.
</details>
<details>
<summary>摘要</summary>
一个自动驾驶车（SDV）需要能够感知周围环境并预测其他交通参与者的未来行为。现有的工作都是在先检测对象，然后预测这些检测到的对象的轨迹，或者预测整个场景的厚度占用和流动Grid。前者会导致安全隐患，因为需要降低检测数量以实现效率，导致对象回溯问题。后者由于输出格式的高维度性而 computationally expensive，同时受到全 convolutional network 的局部感知限制。此外，两种方法都需要大量计算资源预测可能不会被询问的区域或对象。这种情况驱动我们提出了一种独立的感知和未来预测方法，该方法可以直接由运动规划器查询，避免不必要的计算。此外，我们还设计了一种高效但有效的全局注意机制，以解决前一些明确occupancy预测方法的局部感知限制。经过广泛的实验，我们表明我们的含义模型在城市和高速公路上都能够超越当前状态。更多信息请访问我们的项目网站：https://waabi.ai/research/implicito。
</details></li>
</ul>
<hr>
<h2 id="Training-Data-Protection-with-Compositional-Diffusion-Models"><a href="#Training-Data-Protection-with-Compositional-Diffusion-Models" class="headerlink" title="Training Data Protection with Compositional Diffusion Models"></a>Training Data Protection with Compositional Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01937">http://arxiv.org/abs/2308.01937</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aditya Golatkar, Alessandro Achille, Ashwin Swaminathan, Stefano Soatto</li>
<li>for: 这篇论文是为了推动大规模扩散模型的各种应用场景，如选择性忘记和继续学习等。</li>
<li>methods: 这篇论文提出了一种叫做组 compartmentalized Diffusion Models（CDM）的方法，允许在推导时将不同的扩散模型（或提示）分别训练在不同的数据源上，并在推导时自由组合它们以实现与准确模型（训练在所有数据上）相当的性能。此外，每个模型只包含它在训练时接触到的数据subset的信息，因此可以实现数据训练保护和用户访问权限控制等功能。</li>
<li>results: 该研究表明，CDM可以实现选择性忘记和继续学习等功能，并且可以根据用户的访问权限来服务自定义的模型。此外，CDM还可以确定特定样本的数据subset的重要性。<details>
<summary>Abstract</summary>
We introduce Compartmentalized Diffusion Models (CDM), a method to train different diffusion models (or prompts) on distinct data sources and arbitrarily compose them at inference time. The individual models can be trained in isolation, at different times, and on different distributions and domains and can be later composed to achieve performance comparable to a paragon model trained on all data simultaneously. Furthermore, each model only contains information about the subset of the data it was exposed to during training, enabling several forms of training data protection. In particular, CDMs are the first method to enable both selective forgetting and continual learning for large-scale diffusion models, as well as allowing serving customized models based on the user's access rights. CDMs also allow determining the importance of a subset of the data in generating particular samples.
</details>
<details>
<summary>摘要</summary>
我们介绍 compartmentalized diffusion models (CDM)，一种方法可以在推导时分别训练不同的扩散模型（或启发），并在推导时随意组合。个别模型可以在隔离的时间、不同的分布和领域上进行训练，而且可以在推导时随意组合以实现与单一模型训练在所有数据上的性能相似。此外，每个模型只包含它在训练时所接触过的数据subset的信息，因此可以实现多种训练数据保护。例如，CDMs可以实现选择性的忘记和持续学习，以及根据用户的存取权提供自定义的模型。CDMs还允许决定特定数据subset的重要性在生成特定样本中。
</details></li>
</ul>
<hr>
<h2 id="Dual-Governance-The-intersection-of-centralized-regulation-and-crowdsourced-safety-mechanisms-for-Generative-AI"><a href="#Dual-Governance-The-intersection-of-centralized-regulation-and-crowdsourced-safety-mechanisms-for-Generative-AI" class="headerlink" title="Dual Governance: The intersection of centralized regulation and crowdsourced safety mechanisms for Generative AI"></a>Dual Governance: The intersection of centralized regulation and crowdsourced safety mechanisms for Generative AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04448">http://arxiv.org/abs/2308.04448</a></li>
<li>repo_url: None</li>
<li>paper_authors: Avijit Ghosh, Dhanya Lakshmi<br>for:The paper focuses on the ethical and safety concerns surrounding the use of generative AI, particularly in the context of consumer-facing models, and proposes a framework called Dual Governance to address these issues.methods:The paper discusses the limitations of existing centralized regulations and decentralized safety mechanisms, and proposes a cooperative synergy between government regulations and community-developed safety mechanisms as a solution.results:The paper argues that the proposed Dual Governance framework can promote innovation and creativity while ensuring the safe and ethical deployment of generative AI.<details>
<summary>Abstract</summary>
Generative Artificial Intelligence (AI) has seen mainstream adoption lately, especially in the form of consumer-facing, open-ended, text and image generating models. However, the use of such systems raises significant ethical and safety concerns, including privacy violations, misinformation and intellectual property theft. The potential for generative AI to displace human creativity and livelihoods has also been under intense scrutiny. To mitigate these risks, there is an urgent need of policies and regulations responsible and ethical development in the field of generative AI. Existing and proposed centralized regulations by governments to rein in AI face criticisms such as not having sufficient clarity or uniformity, lack of interoperability across lines of jurisdictions, restricting innovation, and hindering free market competition. Decentralized protections via crowdsourced safety tools and mechanisms are a potential alternative. However, they have clear deficiencies in terms of lack of adequacy of oversight and difficulty of enforcement of ethical and safety standards, and are thus not enough by themselves as a regulation mechanism. We propose a marriage of these two strategies via a framework we call Dual Governance. This framework proposes a cooperative synergy between centralized government regulations in a U.S. specific context and safety mechanisms developed by the community to protect stakeholders from the harms of generative AI. By implementing the Dual Governance framework, we posit that innovation and creativity can be promoted while ensuring safe and ethical deployment of generative AI.
</details>
<details>
<summary>摘要</summary>
生成人工智能（AI）在最近几年内得到了普遍的批准，特别是在consumer-facing、开放结束的文本和图像生成模型的形式。然而，使用这些系统的使用带来了重大的伦理和安全问题，包括隐私侵犯、谎言和知识产权盗窃。生成AI可能会取代人类创造力和生活方式，也在严峻的检讨中。为了缓解这些风险，生成AI的负责任和伦理的开发是一个紧迫的需求。现有的中央政府的法规和 proposed regulations 面临批评，如不具有足够的明确性和一致性，跨司法管辖区域的不具有可靠性，限制创新，和妨碍自由市场竞争。 Decentralized protection mechanisms via crowdsourced safety tools and mechanisms are a potential alternative，但它们缺乏伦理和安全标准的充分监管和执行能力，因此不够作为唯一的规则机制。我们提议一种名为“双重管理”的框架，这种框架提议在美国特定的上下文中，中央政府的法规和社区开发的安全机制之间建立了合作的同步。通过实施“双重管理”框架，我们认为可以促进创新和创造力，同时确保生成AI的安全和伦理部署。
</details></li>
</ul>
<hr>
<h2 id="VertexSerum-Poisoning-Graph-Neural-Networks-for-Link-Inference"><a href="#VertexSerum-Poisoning-Graph-Neural-Networks-for-Link-Inference" class="headerlink" title="VertexSerum: Poisoning Graph Neural Networks for Link Inference"></a>VertexSerum: Poisoning Graph Neural Networks for Link Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01469">http://arxiv.org/abs/2308.01469</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruyi Ding, Shijin Duan, Xiaolin Xu, Yunsi Fei</li>
<li>for: 本研究旨在攻击图structured数据中的隐私泄露，特别是社交分析和诈骗探测等应用中使用的图生成网络（GNNs）。</li>
<li>methods: 我们提出了一种新的图毒液攻击方法——VertexSerum，它可以更好地利用图连接的敏感性和价值，从而提高图连接泄露的效果。我们还提出了一种注意力机制，可以嵌入到连接检测网络中，以提高连接检测的准确性。</li>
<li>results: 我们的实验结果表明，VertexSerum在四个真实世界数据集和三种不同的GNN结构下，与现有的链接探测攻击方法相比，平均提高了链接泄露的AUC分数 by 9.8%。此外，我们的实验还表明，VertexSerum在黑盒和在线学习Setting下都具有良好的实用性。<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) have brought superb performance to various applications utilizing graph structural data, such as social analysis and fraud detection. The graph links, e.g., social relationships and transaction history, are sensitive and valuable information, which raises privacy concerns when using GNNs. To exploit these vulnerabilities, we propose VertexSerum, a novel graph poisoning attack that increases the effectiveness of graph link stealing by amplifying the link connectivity leakage. To infer node adjacency more accurately, we propose an attention mechanism that can be embedded into the link detection network. Our experiments demonstrate that VertexSerum significantly outperforms the SOTA link inference attack, improving the AUC scores by an average of $9.8\%$ across four real-world datasets and three different GNN structures. Furthermore, our experiments reveal the effectiveness of VertexSerum in both black-box and online learning settings, further validating its applicability in real-world scenarios.
</details>
<details>
<summary>摘要</summary>
Graph Neural Networks (GNNs) 已经在不同的应用中提供了出色的表现，如社交分析和诈骗检测。图像链接，如社交关系和交易历史记录，是敏感和有价值的信息，这会使用 GNNs 引发隐私问题。为了利用这些漏洞，我们提议 VertexSerum，一种新的图像恶意攻击，可以增强图像链接窃取的效果。为更准确地推断节点相互关系，我们提议一种注意力机制，可以在链接检测网络中嵌入。我们的实验表明，VertexSerum significantly outperforms 当前链接推断攻击的最佳实践（SOTA），提高了平均混淆率（AUC）分数，分别在四个真实世界数据集和三种不同的 GNN 结构上提高了9.8%。此外，我们的实验还表明 VertexSerum 在黑盒和在线学习设置下都是有效的，进一步验证了它在实际场景中的适用性。
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-Small-Molecule-Properties-in-Drug-Discovery"><a href="#Machine-Learning-Small-Molecule-Properties-in-Drug-Discovery" class="headerlink" title="Machine Learning Small Molecule Properties in Drug Discovery"></a>Machine Learning Small Molecule Properties in Drug Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12354">http://arxiv.org/abs/2308.12354</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nikolai Schapin, Maciej Majewski, Alejandro Varela, Carlos Arroniz, Gianni De Fabritiis</li>
<li>for: 这篇论文主要是为了介绍近年来用于小分子性质预测的机器学习（ML）方法。</li>
<li>methods: 论文详细介绍了各种ML方法，包括绑定Affinity、溶解度、ABMET等多种属性的预测。还讨论了现有的流行数据集和分子特征，如化学指纹和图像神经网络。</li>
<li>results: 论文分析了小分子性质预测中存在的挑战，如同时预测和优化多个属性的问题，并 briefly介绍了可能的多目标优化技术来均衡多个属性。最后，论文评估了模型预测结果的可理解性，尤其是在药物发现过程中的关键决策中。总的来说，这篇论文提供了药物性质预测领域机器学习模型的全面回顾。<details>
<summary>Abstract</summary>
Machine learning (ML) is a promising approach for predicting small molecule properties in drug discovery. Here, we provide a comprehensive overview of various ML methods introduced for this purpose in recent years. We review a wide range of properties, including binding affinities, solubility, and ADMET (Absorption, Distribution, Metabolism, Excretion, and Toxicity). We discuss existing popular datasets and molecular descriptors and embeddings, such as chemical fingerprints and graph-based neural networks. We highlight also challenges of predicting and optimizing multiple properties during hit-to-lead and lead optimization stages of drug discovery and explore briefly possible multi-objective optimization techniques that can be used to balance diverse properties while optimizing lead candidates. Finally, techniques to provide an understanding of model predictions, especially for critical decision-making in drug discovery are assessed. Overall, this review provides insights into the landscape of ML models for small molecule property predictions in drug discovery. So far, there are multiple diverse approaches, but their performances are often comparable. Neural networks, while more flexible, do not always outperform simpler models. This shows that the availability of high-quality training data remains crucial for training accurate models and there is a need for standardized benchmarks, additional performance metrics, and best practices to enable richer comparisons between the different techniques and models that can shed a better light on the differences between the many techniques.
</details>
<details>
<summary>摘要</summary>
机器学习（ML）是药物搜索中预测小分子性质的有前途的方法。我们在这篇文章中提供了过去几年内对这些目的的多种机器学习方法的全面概述。我们评论了各种性质，包括绑定稳定性、溶解度和ADMET（吸收、分布、代谢、排泄和毒性）。我们讨论了现有的受欢迎数据集和分子特征，如化学指纹和图像基于神经网络。我们也提到了预测和优化多种性质的挑战，特别是在碰撞到领先和领先优化阶段。我们 briefly explored 可能的多目标优化技术，以填充多种性质的平衡。最后，我们评估了模型预测的方法，特别是在关键决策中的评估。总的来说，这篇文章提供了机器学习模型在小分子性质预测中的领域概况。目前有多种不同的方法，但它们的表现通常相似。神经网络，虽然更灵活，并不总是表现更好。这表明高质量的训练数据的可用性仍然是训练准确模型的关键，而且需要标准化的参考数据、额外的性能指标和最佳实践，以便更好地对不同的技术和模型进行比较，从而更好地了解它们之间的差异。
</details></li>
</ul>
<hr>
<h2 id="From-Discrete-Tokens-to-High-Fidelity-Audio-Using-Multi-Band-Diffusion"><a href="#From-Discrete-Tokens-to-High-Fidelity-Audio-Using-Multi-Band-Diffusion" class="headerlink" title="From Discrete Tokens to High-Fidelity Audio Using Multi-Band Diffusion"></a>From Discrete Tokens to High-Fidelity Audio Using Multi-Band Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02560">http://arxiv.org/abs/2308.02560</a></li>
<li>repo_url: None</li>
<li>paper_authors: Robin San Roman, Yossi Adi, Antoine Deleforge, Romain Serizel, Gabriel Synnaeve, Alexandre Défossez</li>
<li>for: 这个论文的目的是提出一种高级别扩散模型，可以从低比特率的精度表示生成任何类型的音频模式（如语音、音乐、环境声）。</li>
<li>methods: 这个论文使用了扩散模型，但不同于之前的扩散模型，它可以生成任何类型的音频模式，并且可以在同等比特率下比其他生成技术具有更高的 perceived quality。</li>
<li>results: 根据论文的结果，这种扩散模型可以生成高质量的音频，并且可以在不同的音频类型和bit rate下进行调整。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Deep generative models can generate high-fidelity audio conditioned on various types of representations (e.g., mel-spectrograms, Mel-frequency Cepstral Coefficients (MFCC)). Recently, such models have been used to synthesize audio waveforms conditioned on highly compressed representations. Although such methods produce impressive results, they are prone to generate audible artifacts when the conditioning is flawed or imperfect. An alternative modeling approach is to use diffusion models. However, these have mainly been used as speech vocoders (i.e., conditioned on mel-spectrograms) or generating relatively low sampling rate signals. In this work, we propose a high-fidelity multi-band diffusion-based framework that generates any type of audio modality (e.g., speech, music, environmental sounds) from low-bitrate discrete representations. At equal bit rate, the proposed approach outperforms state-of-the-art generative techniques in terms of perceptual quality. Training and, evaluation code, along with audio samples, are available on the facebookresearch/audiocraft Github page.
</details>
<details>
<summary>摘要</summary>
深度生成模型可以生成高准确性音频，根据不同类型的表示（如mel-spectrograms、Mel-frequency Cepstral Coefficients (MFCC)）。最近，这些模型已经用于生成基于高度压缩表示的音频波形。虽然这些方法产生了很好的结果，但是它们容易产生噪音artefacts，当conditioning是错误或不完美时。另一种模型化方法是使用扩散模型。然而，这些模型主要用于speech vocoder（基于mel-spectrograms）或生成低频率的信号。在这项工作中，我们提议一种高准确度多带 diffusion-based框架，可以从低比特率精确表示中生成任何类型的音频模式（如语音、音乐、环境声）。在相同的比特率下，我们的提议方法与现状最佳生成技术相比，在人类可识别质量上表现出 excel。训练和评估代码，以及音频示例，可以在facebookresearch/audiocraft GitHub页面上找到。
</details></li>
</ul>
<hr>
<h2 id="A-digital-twin-framework-for-civil-engineering-structures"><a href="#A-digital-twin-framework-for-civil-engineering-structures" class="headerlink" title="A digital twin framework for civil engineering structures"></a>A digital twin framework for civil engineering structures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01445">http://arxiv.org/abs/2308.01445</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matteo Torzoni, Marco Tezzele, Stefano Mariani, Andrea Manzoni, Karen E. Willcox</li>
<li>for: This paper proposes a predictive digital twin approach to health monitoring, maintenance, and management planning for civil engineering structures.</li>
<li>methods: The proposed approach uses a probabilistic graphical model, dynamic Bayesian network, and deep learning models to update the digital twin state in real-time and inform optimal maintenance and management actions.</li>
<li>results: The paper demonstrates the effectiveness of the proposed approach through two synthetic case studies, showing the ability of the digital twin to provide real-time structural health diagnostics and inform dynamic decision-making.<details>
<summary>Abstract</summary>
The digital twin concept represents an appealing opportunity to advance condition-based and predictive maintenance paradigms for civil engineering systems, thus allowing reduced lifecycle costs, increased system safety, and increased system availability. This work proposes a predictive digital twin approach to the health monitoring, maintenance, and management planning of civil engineering structures. The asset-twin coupled dynamical system is encoded employing a probabilistic graphical model, which allows all relevant sources of uncertainty to be taken into account. In particular, the time-repeating observations-to-decisions flow is modeled using a dynamic Bayesian network. Real-time structural health diagnostics are provided by assimilating sensed data with deep learning models. The digital twin state is continually updated in a sequential Bayesian inference fashion. This is then exploited to inform the optimal planning of maintenance and management actions within a dynamic decision-making framework. A preliminary offline phase involves the population of training datasets through a reduced-order numerical model and the computation of a health-dependent control policy. The strategy is assessed on two synthetic case studies, involving a cantilever beam and a railway bridge, demonstrating the dynamic decision-making capabilities of health-aware digital twins.
</details>
<details>
<summary>摘要</summary>
数字双生物概念在 цивиLENGINEERING 系统中表现出了吸引人的机会，以提高基于状况的维护和预测维护方法，从而降低生命周期成本、提高系统安全性和系统可用性。该工作提议一种预测性数字双生物方法来监测、维护和规划 цивиLENGINEERING 结构的健康状况。 asset-twin  Coupled 动力系统使用 probabilistic graphical model 编码，以涵盖所有相关的不确定性因素。特别是，时间重复的观测到决策流程使用动态 Bayesian network 模型。在实时 Structural health 诊断中，把感知数据 assimilate  WITH deep learning models。数字双生物状态在 sequential Bayesian inference 方式中不断更新。这后来被用来决策维护和管理行动的优化计划，在动态决策框架中。在 preliminary offline 阶段，通过减少的数值模型 Compute 健康依赖控制策略。该策略在 two  Synthetic case studies 中， involving 悬臂和铁路桥，表现出了健康意识数字双生物的动态决策能力。
</details></li>
</ul>
<hr>
<h2 id="DLSIA-Deep-Learning-for-Scientific-Image-Analysis"><a href="#DLSIA-Deep-Learning-for-Scientific-Image-Analysis" class="headerlink" title="DLSIA: Deep Learning for Scientific Image Analysis"></a>DLSIA: Deep Learning for Scientific Image Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02559">http://arxiv.org/abs/2308.02559</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eric J Roberts, Tanny Chavez, Alexander Hexemer, Petrus H. Zwart</li>
<li>for: 用于科学图像分析领域的深度学习库，为科学家和研究人员提供可自定义的卷积神经网络架构，用于许多图像分析任务，包括下游数据处理和实验循环计算场景。</li>
<li>methods: 使用易于使用的架构，如自动编码器、可调 U-Net 和精简 mixed-scale dense network (MSDNet)，以及 randomly generated sparse mixed-scale networks (SMSNets)。</li>
<li>results: 提供可访问的 CNN 构建和抽象，让科学家可以适应机器学习方法，加速发现，促进交叉领域合作，并进展科学图像分析研究。<details>
<summary>Abstract</summary>
We introduce DLSIA (Deep Learning for Scientific Image Analysis), a Python-based machine learning library that empowers scientists and researchers across diverse scientific domains with a range of customizable convolutional neural network (CNN) architectures for a wide variety of tasks in image analysis to be used in downstream data processing, or for experiment-in-the-loop computing scenarios. DLSIA features easy-to-use architectures such as autoencoders, tunable U-Nets, and parameter-lean mixed-scale dense networks (MSDNets). Additionally, we introduce sparse mixed-scale networks (SMSNets), generated using random graphs and sparse connections. As experimental data continues to grow in scale and complexity, DLSIA provides accessible CNN construction and abstracts CNN complexities, allowing scientists to tailor their machine learning approaches, accelerate discoveries, foster interdisciplinary collaboration, and advance research in scientific image analysis.
</details>
<details>
<summary>摘要</summary>
我们介绍DLSIA（深度学习 для科学影像分析），这是一个基于Python的机器学习库，它为科学家和研究人员在多种科学领域提供了一系列可自定义的卷积神经网络架构，用于处理各种影像分析任务，可以用于下游资料处理或实验运行 Computing enario。DLSIA 提供了易于使用的架构，例如自动encoder、可调 U-Net 和简洁的混合缩减网络（MSDNets）。此外，我们还引入了随机 graphs 和简Connection 的稀疏混合网络（SMSNets）。随着实验数据的数量和复杂度不断增加，DLSIA 提供了可访问的 CNN 建构和抽象 CNN 复杂度，让科学家能够适应自己的机器学习方法，加速发现，促进交叉领域合作，并进步科学影像分析研究。
</details></li>
</ul>
<hr>
<h2 id="Novel-Physics-Based-Machine-Learning-Models-for-Indoor-Air-Quality-Approximations"><a href="#Novel-Physics-Based-Machine-Learning-Models-for-Indoor-Air-Quality-Approximations" class="headerlink" title="Novel Physics-Based Machine-Learning Models for Indoor Air Quality Approximations"></a>Novel Physics-Based Machine-Learning Models for Indoor Air Quality Approximations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01438">http://arxiv.org/abs/2308.01438</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmad Mohammadshirazi, Aida Nadafian, Amin Karimi Monsefi, Mohammad H. Rafiei, Rajiv Ramnath</li>
<li>for: 这个研究旨在提出六个新的物理学基础的机器学习模型，以精确地估计室内污染物浓度。</li>
<li>methods: 这个研究使用了 físic-based 的机器学习模型，包括State-space概念、Gated Recurrent Units和Decomposition技术。</li>
<li>results: 研究发现这些提案的模型比类似的现代变数组件模型更加简单、computationally更高效和更精确。<details>
<summary>Abstract</summary>
Cost-effective sensors are capable of real-time capturing a variety of air quality-related modalities from different pollutant concentrations to indoor/outdoor humidity and temperature. Machine learning (ML) models are capable of performing air-quality "ahead-of-time" approximations. Undoubtedly, accurate indoor air quality approximation significantly helps provide a healthy indoor environment, optimize associated energy consumption, and offer human comfort. However, it is crucial to design an ML architecture to capture the domain knowledge, so-called problem physics. In this study, we propose six novel physics-based ML models for accurate indoor pollutant concentration approximations. The proposed models include an adroit combination of state-space concepts in physics, Gated Recurrent Units, and Decomposition techniques. The proposed models were illustrated using data collected from five offices in a commercial building in California. The proposed models are shown to be less complex, computationally more efficient, and more accurate than similar state-of-the-art transformer-based models. The superiority of the proposed models is due to their relatively light architecture (computational efficiency) and, more importantly, their ability to capture the underlying highly nonlinear patterns embedded in the often contaminated sensor-collected indoor air quality temporal data.
</details>
<details>
<summary>摘要</summary>
cost-effective sensors can real-time capture various indoor/outdoor air quality-related modalities, from different pollutant concentrations to humidity and temperature. machine learning (ml) models can perform air quality "ahead-of-time" approximations. accurately approximating indoor air quality can help provide a healthy indoor environment, optimize associated energy consumption, and offer human comfort. however, it is crucial to design an ml architecture to capture the domain knowledge, so-called problem physics. in this study, we propose six novel physics-based ml models for accurate indoor pollutant concentration approximations. the proposed models combine state-space concepts in physics, gated recurrent units, and decomposition techniques. the proposed models were illustrated using data collected from five offices in a commercial building in california. the proposed models are less complex, computationally more efficient, and more accurate than similar state-of-the-art transformer-based models. the superiority of the proposed models is due to their relatively light architecture (computational efficiency) and their ability to capture the underlying highly nonlinear patterns embedded in the often contaminated sensor-collected indoor air quality temporal data.
</details></li>
</ul>
<hr>
<h2 id="Price-Aware-Deep-Learning-for-Electricity-Markets"><a href="#Price-Aware-Deep-Learning-for-Electricity-Markets" class="headerlink" title="Price-Aware Deep Learning for Electricity Markets"></a>Price-Aware Deep Learning for Electricity Markets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01436">http://arxiv.org/abs/2308.01436</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vladimir Dvorkin, Ferdinando Fioretto</li>
<li>for: 该论文旨在探讨深度学习在运维规划中的应用，以及深度学习所带来的预测错误对电价价格的影响。</li>
<li>methods: 该论文使用了深度学习模型来预测电力供应和需求，并对预测错误进行分析。</li>
<li>results: 该论文发现了预测错误对电价价格的影响，并提出了一种基于深度学习的电力市场清算优化方法来增强公平性。该方法可以均衡预测错误和价格错误，从而提高系统的公平性和稳定性。<details>
<summary>Abstract</summary>
While deep learning gradually penetrates operational planning, its inherent prediction errors may significantly affect electricity prices. This letter examines how prediction errors propagate into electricity prices, revealing notable pricing errors and their spatial disparity in congested power systems. To improve fairness, we propose to embed electricity market-clearing optimization as a deep learning layer. Differentiating through this layer allows for balancing between prediction and pricing errors, as oppose to minimizing prediction errors alone. This layer implicitly optimizes fairness and controls the spatial distribution of price errors across the system. We showcase the price-aware deep learning in the nexus of wind power forecasting and short-term electricity market clearing.
</details>
<details>
<summary>摘要</summary>
而深度学习逐渐渗透到运维规划中，它的内生预测错误可能对电力价格产生重要影响。这封信件研究了预测错误如何传播到电力价格上，揭示了明显的价格错误和其空间差异在拥挤的电力系统中。为了提高公平性，我们提议将电力市场清算优化作为深度学习层的一部分。通过这层的微调，可以平衡预测错误和价格错误之间的平衡，而不是仅仅是减少预测错误。这层隐式地优化了公平性和价格错误的空间分布在系统中。我们在风力发电预测和短期电力市场清算之间展示了价格意识的深度学习。
</details></li>
</ul>
<hr>
<h2 id="COVID-VR-A-Deep-Learning-COVID-19-Classification-Model-Using-Volume-Rendered-Computer-Tomography"><a href="#COVID-VR-A-Deep-Learning-COVID-19-Classification-Model-Using-Volume-Rendered-Computer-Tomography" class="headerlink" title="COVID-VR: A Deep Learning COVID-19 Classification Model Using Volume-Rendered Computer Tomography"></a>COVID-VR: A Deep Learning COVID-19 Classification Model Using Volume-Rendered Computer Tomography</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01433">http://arxiv.org/abs/2308.01433</a></li>
<li>repo_url: None</li>
<li>paper_authors: Noemi Maritza L. Romero, Ricco Vasconcellos, Mariana R. Mendoza, João L. D. Comba</li>
<li>for: 该论文目的是为了开发一种基于计算机断层成像（CT）图像的肺疾病分类方法，以提高肺疾病诊断的准确性和效率。</li>
<li>methods: 该方法使用了深度学习建模，将多个视角下的肺部CT图像转化为三维Volume Rendering图像，以提高肺疾病诊断的准确性和效率。</li>
<li>results: 对于私有数据和公共数据集的比较，该方法能够有效地识别肺疾病，并与切片方法相比，表现竞争力强。<details>
<summary>Abstract</summary>
The COVID-19 pandemic presented numerous challenges to healthcare systems worldwide. Given that lung infections are prevalent among COVID-19 patients, chest Computer Tomography (CT) scans have frequently been utilized as an alternative method for identifying COVID-19 conditions and various other types of pulmonary diseases. Deep learning architectures have emerged to automate the identification of pulmonary disease types by leveraging CT scan slices as inputs for classification models. This paper introduces COVID-VR, a novel approach for classifying pulmonary diseases based on volume rendering images of the lungs captured from multiple angles, thereby providing a comprehensive view of the entire lung in each image. To assess the effectiveness of our proposal, we compared it against competing strategies utilizing both private data obtained from partner hospitals and a publicly available dataset. The results demonstrate that our approach effectively identifies pulmonary lesions and performs competitively when compared to slice-based methods.
</details>
<details>
<summary>摘要</summary>
COVID-19 大流行对全球医疗系统带来了无数的挑战。由于 COVID-19 患者中肺部感染很普遍，因此胸部计算机扫描（CT）成为了一种代替方法来诊断 COVID-19 状况和其他多种肺病。深度学习架构在 CT 扫描片为分类模型提供输入，以自动识别肺病类型。本文介绍了 COVID-VR，一种基于肺部体积渲染图像的新方法，以捕捉多个角度捕捉肺部的全面视图。为评估我们的建议的有效性，我们与合作医院提供的私人数据进行比较，以及公开可用的数据集。结果显示，我们的方法可以有效地识别肺病涂抹，并与slice-based方法相比竞争性地表现。
</details></li>
</ul>
<hr>
<h2 id="Unlocking-the-Potential-of-Similarity-Matching-Scalability-Supervision-and-Pre-training"><a href="#Unlocking-the-Potential-of-Similarity-Matching-Scalability-Supervision-and-Pre-training" class="headerlink" title="Unlocking the Potential of Similarity Matching: Scalability, Supervision and Pre-training"></a>Unlocking the Potential of Similarity Matching: Scalability, Supervision and Pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02427">http://arxiv.org/abs/2308.02427</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanis Bahroun, Shagesh Sridharan, Atithi Acharya, Dmitri B. Chklovskii, Anirvan M. Sengupta</li>
<li>for: 这个研究旨在开发一种基于本地学习规则的替代算法，以增强backpropagation算法的有效性和生物可能性。</li>
<li>methods: 研究使用了一种主要是无监督的相似匹配（SM）框架，该框架与生物系统中观察到的机制相符，并且具有在线、本地化和生物可能性的算法。</li>
<li>results: 研究人员提出了一种使用PyTorch实现Convolutional Nonnegative SM的方法，并引入了一种本地化的supervised SM目标，以便堆叠SM层。此外，研究人员还使用PyTorch实现了预训练 architecture such as LeNet，并对BP-trained模型中的特征进行评估。这项研究结合了生物可能性的算法和计算效率，开辟了多个可能性的探索。<details>
<summary>Abstract</summary>
While effective, the backpropagation (BP) algorithm exhibits limitations in terms of biological plausibility, computational cost, and suitability for online learning. As a result, there has been a growing interest in developing alternative biologically plausible learning approaches that rely on local learning rules. This study focuses on the primarily unsupervised similarity matching (SM) framework, which aligns with observed mechanisms in biological systems and offers online, localized, and biologically plausible algorithms. i) To scale SM to large datasets, we propose an implementation of Convolutional Nonnegative SM using PyTorch. ii) We introduce a localized supervised SM objective reminiscent of canonical correlation analysis, facilitating stacking SM layers. iii) We leverage the PyTorch implementation for pre-training architectures such as LeNet and compare the evaluation of features against BP-trained models. This work combines biologically plausible algorithms with computational efficiency opening multiple avenues for further explorations.
</details>
<details>
<summary>摘要</summary>
While effective, the backpropagation (BP) algorithm has limitations in terms of biological plausibility, computational cost, and suitability for online learning. As a result, there has been a growing interest in developing alternative biologically plausible learning approaches that rely on local learning rules. This study focuses on the primarily unsupervised similarity matching (SM) framework, which aligns with observed mechanisms in biological systems and offers online, localized, and biologically plausible algorithms.i) To scale SM to large datasets, we propose an implementation of Convolutional Nonnegative SM using PyTorch.ii) We introduce a localized supervised SM objective reminiscent of canonical correlation analysis, facilitating stacking SM layers.iii) We leverage the PyTorch implementation for pre-training architectures such as LeNet and compare the evaluation of features against BP-trained models. This work combines biologically plausible algorithms with computational efficiency, opening multiple avenues for further explorations.Here's the translation in Traditional Chinese as well, for reference:While effective, the backpropagation (BP) algorithm has limitations in terms of biological plausibility, computational cost, and suitability for online learning. As a result, there has been a growing interest in developing alternative biologically plausible learning approaches that rely on local learning rules. This study focuses on the primarily unsupervised similarity matching (SM) framework, which aligns with observed mechanisms in biological systems and offers online, localized, and biologically plausible algorithms.i) To scale SM to large datasets, we propose an implementation of Convolutional Nonnegative SM using PyTorch.ii) We introduce a localized supervised SM objective reminiscent of canonical correlation analysis, facilitating stacking SM layers.iii) We leverage the PyTorch implementation for pre-training architectures such as LeNet and compare the evaluation of features against BP-trained models. This work combines biologically plausible algorithms with computational efficiency, opening multiple avenues for further explorations.
</details></li>
</ul>
<hr>
<h2 id="Bio-Clinical-BERT-BERT-Base-and-CNN-Performance-Comparison-for-Predicting-Drug-Review-Satisfaction"><a href="#Bio-Clinical-BERT-BERT-Base-and-CNN-Performance-Comparison-for-Predicting-Drug-Review-Satisfaction" class="headerlink" title="Bio+Clinical BERT, BERT Base, and CNN Performance Comparison for Predicting Drug-Review Satisfaction"></a>Bio+Clinical BERT, BERT Base, and CNN Performance Comparison for Predicting Drug-Review Satisfaction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03782">http://arxiv.org/abs/2308.03782</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yue Ling</li>
<li>for: 这个研究旨在开发一些可以分析病人的药物评价，并将其满意度精确地分类为正面、中性或负面的自然语言处理（NLP）模型。</li>
<li>methods: 这个研究使用了多种分类模型，包括BERT基础模型、Bio+Clinical BERT和简单的CNN。</li>
<li>results: 结果显示，医疗领域专门的Bio+Clinical BERT模型在表格2中表现出色，与通用领域基础BERT模型相比，macro f1和 recall 分数提高了11%。<details>
<summary>Abstract</summary>
The objective of this study is to develop natural language processing (NLP) models that can analyze patients' drug reviews and accurately classify their satisfaction levels as positive, neutral, or negative. Such models would reduce the workload of healthcare professionals and provide greater insight into patients' quality of life, which is a critical indicator of treatment effectiveness. To achieve this, we implemented and evaluated several classification models, including a BERT base model, Bio+Clinical BERT, and a simpler CNN. Results indicate that the medical domain-specific Bio+Clinical BERT model significantly outperformed the general domain base BERT model, achieving macro f1 and recall score improvement of 11%, as shown in Table 2. Future research could explore how to capitalize on the specific strengths of each model. Bio+Clinical BERT excels in overall performance, particularly with medical jargon, while the simpler CNN demonstrates the ability to identify crucial words and accurately classify sentiment in texts with conflicting sentiments.
</details>
<details>
<summary>摘要</summary>
本研究的目的是开发自然语言处理（NLP）模型，可以分析患者的药物评价并准确地分类为正面、中性或负面的满意度。这些模型会减轻医疗专业人员的工作负担，并为患者的生活质量提供更多的指导，这是治疗效果的重要指标。为达到这一目标，我们实施并评估了多种分类模型，包括BERT基础模型、Bio+клиничеBERT和简单的CNN。结果表明，专业领域域 especific的Bio+клиничеBERT模型在表格2中显著超越了通用领域基础BERT模型，实现了macro f1和回快分数的提高11%。未来的研究可以探讨如何利用每个模型的特点。Bio+клиничеBERT在总性性能方面表现出色，特别是对医疗专业术语的处理，而简单的CNN则能够准确地标识关键词并在文本中分类情感。
</details></li>
</ul>
<hr>
<h2 id="Sea-level-Projections-with-Machine-Learning-using-Altimetry-and-Climate-Model-ensembles"><a href="#Sea-level-Projections-with-Machine-Learning-using-Altimetry-and-Climate-Model-ensembles" class="headerlink" title="Sea level Projections with Machine Learning using Altimetry and Climate Model ensembles"></a>Sea level Projections with Machine Learning using Altimetry and Climate Model ensembles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02460">http://arxiv.org/abs/2308.02460</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saumya Sinha, John Fasullo, R. Steven Nerem, Claire Monteleoni</li>
<li>for: 本研究使用卫星测量数据自1993年起，检测全球海平面的升高趋势（3.4毫米&#x2F;年），并调查人类活动对这种升高的影响。</li>
<li>methods: 本研究使用机器学习（ML）技术，通过融合卫星观测数据和气候模型 simulations，预测未来海平面变化的趋势。</li>
<li>results: 研究发现，通过各种气候变化信号的评估，可以更好地预测未来海平面变化的趋势，并且可以通过各种方法提高预测的准确性。<details>
<summary>Abstract</summary>
Satellite altimeter observations retrieved since 1993 show that the global mean sea level is rising at an unprecedented rate (3.4mm/year). With almost three decades of observations, we can now investigate the contributions of anthropogenic climate-change signals such as greenhouse gases, aerosols, and biomass burning in this rising sea level. We use machine learning (ML) to investigate future patterns of sea level change. To understand the extent of contributions from the climate-change signals, and to help in forecasting sea level change in the future, we turn to climate model simulations. This work presents a machine learning framework that exploits both satellite observations and climate model simulations to generate sea level rise projections at a 2-degree resolution spatial grid, 30 years into the future. We train fully connected neural networks (FCNNs) to predict altimeter values through a non-linear fusion of the climate model hindcasts (for 1993-2019). The learned FCNNs are then applied to future climate model projections to predict future sea level patterns. We propose segmenting our spatial dataset into meaningful clusters and show that clustering helps to improve predictions of our ML model.
</details>
<details>
<summary>摘要</summary>
卫星测量数据自1993年起已经提供了全球海平面上升的无前例快速速率（3.4毫米/年）。在近三十年的观测记录下，我们现在可以进行人类活动气候变化的贡献分析，如绿色气体、喷气和生物燃烧等。我们使用机器学习（ML）技术来探索未来海平面变化的趋势。为了了解气候变化信号的贡献和未来海平面变化的预测，我们转向气候模型仿真。本研究提出了一种基于卫星观测和气候模型仿真的机器学习框架，用于预测未来30年的海平面变化趋势。我们使用全连接神经网络（FCNN）来预测测量值，通过非线性混合气候模型预测（1993-2019）来训练FCNN。然后，我们应用FCNN来预测未来气候模型预测中的海平面变化趋势。我们还提出了对空间数据进行有意义的分割，并证明分割可以提高我们的机器学习模型预测的准确性。
</details></li>
</ul>
<hr>
<h2 id="OpenFlamingo-An-Open-Source-Framework-for-Training-Large-Autoregressive-Vision-Language-Models"><a href="#OpenFlamingo-An-Open-Source-Framework-for-Training-Large-Autoregressive-Vision-Language-Models" class="headerlink" title="OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models"></a>OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01390">http://arxiv.org/abs/2308.01390</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mlfoundations/open_flamingo">https://github.com/mlfoundations/open_flamingo</a></li>
<li>paper_authors: Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, Ludwig Schmidt</li>
<li>for: 这篇论文是为了描述OpenFlamingo模型家族，该家族包括从3B到9B参数的束autoregressive视频语言模型，这是一个开源的Flamingo模型复制项目。</li>
<li>methods: 这篇论文使用了OpenFlamingo模型，training数据，超参数以及评估集合来训练这些模型。</li>
<li>results: 在七个视频语言数据集上，OpenFlamingo模型的平均表现为80-89%相对于对应的Flamingo模型表现。<details>
<summary>Abstract</summary>
We introduce OpenFlamingo, a family of autoregressive vision-language models ranging from 3B to 9B parameters. OpenFlamingo is an ongoing effort to produce an open-source replication of DeepMind's Flamingo models. On seven vision-language datasets, OpenFlamingo models average between 80 - 89% of corresponding Flamingo performance. This technical report describes our models, training data, hyperparameters, and evaluation suite. We share our models and code at https://github.com/mlfoundations/open_flamingo.
</details>
<details>
<summary>摘要</summary>
我们介绍OpenFlamingo，一家以自适应推论为基础的视觉语言模型，它的参数量从3B至9B。OpenFlamingo是一个持续进行的开源复制项目，旨在实现深度联盟的Flamingo模型的开源版本。在七个视觉语言数据集上，OpenFlamingo模型的平均表现为80-89%相应的Flamingo表现。本技术报告描述了我们的模型、训练数据、几何 Parameters 和评估工具。我们在https://github.com/mlfoundations/open_flamingo 上分享我们的模型和代码。
</details></li>
</ul>
<hr>
<h2 id="Follow-the-Soldiers-with-Optimized-Single-Shot-Multibox-Detection-and-Reinforcement-Learning"><a href="#Follow-the-Soldiers-with-Optimized-Single-Shot-Multibox-Detection-and-Reinforcement-Learning" class="headerlink" title="Follow the Soldiers with Optimized Single-Shot Multibox Detection and Reinforcement Learning"></a>Follow the Soldiers with Optimized Single-Shot Multibox Detection and Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01389">http://arxiv.org/abs/2308.01389</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jumman Hossain, Maliha Momtaz</li>
<li>for: 本研究的主要目标是建立一个自动驾驶系统，使其可以跟踪特定人（在我们项目中是士兵）在任何方向移动。</li>
<li>methods: 我们使用优化的单射多框检测（SSD）模型和再增强学习（RL）模型来实现这个目标。</li>
<li>results: 实验结果显示，使用 SSD Lite 模型可以提供更好的性能（比 SSD 和 NCS 更好），而且在执行速度方面也有显著提高（约2-3倍），而无需牺牲准确性。<details>
<summary>Abstract</summary>
Nowadays, autonomous cars are gaining traction due to their numerous potential applications on battlefields and in resolving a variety of other real-world challenges. The main goal of our project is to build an autonomous system using DeepRacer which will follow a specific person (for our project, a soldier) when they will be moving in any direction. Two main components to accomplish this project is an optimized Single-Shot Multibox Detection (SSD) object detection model and a Reinforcement Learning (RL) model. We accomplished the task using SSD Lite instead of SSD and at the end, compared the results among SSD, SSD with Neural Computing Stick (NCS), and SSD Lite. Experimental results show that SSD Lite gives better performance among these three techniques and exhibits a considerable boost in inference speed (~2-3 times) without compromising accuracy.
</details>
<details>
<summary>摘要</summary>
现在，自动驾驶车在各种应用场景中得到了广泛的应用，尤其是在战场和解决各种现实世界问题方面。我们项目的主要目标是使用DeepRacer构建一个自动驾驶系统，该系统可以跟踪一个特定人（在我们项目中是一名士兵）在任何方向移动时。我们完成了这个项目，使用优化的Single-Shot Multibox Detection（SSD）物体检测模型和Reinforcement Learning（RL）模型。我们使用SSD Lite而不是SSD，并在结果中进行了比较。实验结果显示，SSD Lite在这三种技术中表现最佳，并且在执行速度方面表现出了明显的提升（大约2-3倍），而无需牺牲准确性。
</details></li>
</ul>
<hr>
<h2 id="DeepSpeed-Chat-Easy-Fast-and-Affordable-RLHF-Training-of-ChatGPT-like-Models-at-All-Scales"><a href="#DeepSpeed-Chat-Easy-Fast-and-Affordable-RLHF-Training-of-ChatGPT-like-Models-at-All-Scales" class="headerlink" title="DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales"></a>DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01320">http://arxiv.org/abs/2308.01320</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/DeepSpeed">https://github.com/microsoft/DeepSpeed</a></li>
<li>paper_authors: Zhewei Yao, Reza Yazdani Aminabadi, Olatunji Ruwase, Samyam Rajbhandari, Xiaoxia Wu, Ammar Ahmad Awan, Jeff Rasley, Minjia Zhang, Conglong Li, Connor Holmes, Zhongzhu Zhou, Michael Wyatt, Molly Smith, Lev Kurilenko, Heyang Qin, Masahiro Tanaka, Shuai Che, Shuaiwen Leon Song, Yuxiong He</li>
<li>for: This paper aims to provide an accessible, efficient, and cost-effective end-to-end RLHF training pipeline for ChatGPT-like models, particularly when training at the scale of billions of parameters.</li>
<li>methods: The paper introduces DeepSpeed-Chat, a novel system that democratizes RLHF training, making it accessible to the AI community. The system offers three key capabilities: an easy-to-use training and inference experience for ChatGPT-like models, a DeepSpeed-RLHF pipeline that replicates the training pipeline from InstructGPT, and a robust DeepSpeed-RLHF system that combines various optimizations for training and inference in a unified way.</li>
<li>results: The system delivers unparalleled efficiency and scalability, enabling training of models with hundreds of billions of parameters in record time and at a fraction of the cost. With this development, DeepSpeed-Chat paves the way for broader access to advanced RLHF training, even for data scientists with limited resources, thereby fostering innovation and further development in the field of AI.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文目标是提供一个可accessible、高效、成本效果的RLHF训练管道，特别是在多亿个参数训练时。</li>
<li>methods: 论文引入了DeepSpeed-Chat系统，该系统提供了三个关键功能：对ChatGPT-like模型的易用训练和推理体验、DeepSpeed-RLHF管道，以及一个稳定的DeepSpeed-RLHF系统，该系统结合了多种优化来提高训练和推理的效率和扩展性。</li>
<li>results: 系统可以在记录时间内训练出多亿个参数的模型，并且在成本的一小部分。通过这一发展，DeepSpeed-Chat开创了更广泛的RLHF训练访问权，使得数据科学家们可以更容易地访问高级RLHF训练，从而推动AI领域的创新和发展。<details>
<summary>Abstract</summary>
ChatGPT-like models have revolutionized various applications in artificial intelligence, from summarization and coding to translation, matching or even surpassing human performance. However, the current landscape lacks an accessible, efficient, and cost-effective end-to-end RLHF (Reinforcement Learning with Human Feedback) training pipeline for these powerful models, particularly when training at the scale of billions of parameters. This paper introduces DeepSpeed-Chat, a novel system that democratizes RLHF training, making it accessible to the AI community. DeepSpeed-Chat offers three key capabilities: an easy-to-use training and inference experience for ChatGPT-like models, a DeepSpeed-RLHF pipeline that replicates the training pipeline from InstructGPT, and a robust DeepSpeed-RLHF system that combines various optimizations for training and inference in a unified way. The system delivers unparalleled efficiency and scalability, enabling training of models with hundreds of billions of parameters in record time and at a fraction of the cost. With this development, DeepSpeed-Chat paves the way for broader access to advanced RLHF training, even for data scientists with limited resources, thereby fostering innovation and further development in the field of AI.
</details>
<details>
<summary>摘要</summary>
chatgpt-like模型已经革命化了人工智能中的多个应用，从概要和编程到翻译，与人类表现相当或甚至超越人类表现。然而，当前的景象缺乏可 accessible，高效，和cost-effective的RLHF（人工智能学习带反馈）训练管道，特别是在 billion parameter scale 的训练中。本文介绍了 DeepSpeed-Chat，一种新的系统，使得RLHF训练变得可 accessible。DeepSpeed-Chat具有三个关键能力：对 ChatGPT-like模型的易用训练和推理经验，基于 InstructGPT 的 DeepSpeed-RLHF 管道，以及一个可靠的 DeepSpeed-RLHF 系统，它将训练和推理过程合并到一起，提供了无 parallel 的效率和可扩展性。该系统可以在 record 时间内训练 billions of parameters 的模型，并且只需一小部分的成本。通过这一发展，DeepSpeed-Chat 为AI领域的进步和发展开辟了新的可能性，尤其是为那些具有限制的数据科学家，他们可以更容易地访问高级RLHF训练，从而推动AI领域的进步。
</details></li>
</ul>
<hr>
<h2 id="Computational-Long-Exposure-Mobile-Photography"><a href="#Computational-Long-Exposure-Mobile-Photography" class="headerlink" title="Computational Long Exposure Mobile Photography"></a>Computational Long Exposure Mobile Photography</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01379">http://arxiv.org/abs/2308.01379</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eric Tabellion, Nikhil Karnad, Noa Glaser, Ben Weiss, David E. Jacobs, Yael Pritch</li>
<li>for: 这篇论文旨在提供一种可以在手持式智能手机摄像头应用程序中实现长时间摄影的计算机 burst 摄影系统，以生成吸引人的满屏照片。</li>
<li>methods: 该系统首先检测和分割主题，然后跟踪场景运动多个帧，并对图像进行对齐，以保持所需的锐度和生成美观的运动梳子。系统还会捕捉不充足的短暂拍摄，并选择输入帧中能够生成控制长度的滑块，不管场景或摄像头运动速度。最后，系统预测间帧运动并synthesize运动滑块，以填充间隔 между输入帧。</li>
<li>results: 该系统可以自动生成高分辨率和高动态范围（HDR）照片，并使这种创作风格更加 accessible 于大多数临时摄影师。更多信息和补充材料可以在项目网页上找到：<a target="_blank" rel="noopener" href="https://motion-mode.github.io/">https://motion-mode.github.io/</a><details>
<summary>Abstract</summary>
Long exposure photography produces stunning imagery, representing moving elements in a scene with motion-blur. It is generally employed in two modalities, producing either a foreground or a background blur effect. Foreground blur images are traditionally captured on a tripod-mounted camera and portray blurred moving foreground elements, such as silky water or light trails, over a perfectly sharp background landscape. Background blur images, also called panning photography, are captured while the camera is tracking a moving subject, to produce an image of a sharp subject over a background blurred by relative motion. Both techniques are notoriously challenging and require additional equipment and advanced skills. In this paper, we describe a computational burst photography system that operates in a hand-held smartphone camera app, and achieves these effects fully automatically, at the tap of the shutter button. Our approach first detects and segments the salient subject. We track the scene motion over multiple frames and align the images in order to preserve desired sharpness and to produce aesthetically pleasing motion streaks. We capture an under-exposed burst and select the subset of input frames that will produce blur trails of controlled length, regardless of scene or camera motion velocity. We predict inter-frame motion and synthesize motion-blur to fill the temporal gaps between the input frames. Finally, we composite the blurred image with the sharp regular exposure to protect the sharpness of faces or areas of the scene that are barely moving, and produce a final high resolution and high dynamic range (HDR) photograph. Our system democratizes a capability previously reserved to professionals, and makes this creative style accessible to most casual photographers.   More information and supplementary material can be found on our project webpage: https://motion-mode.github.io/
</details>
<details>
<summary>摘要</summary>
长时间摄影可以生成吸目的图像，表现在Scene中的运动元素的混淆。它通常在两种方式下使用，生成 either 前景或背景混淆效果。前景混淆图像通常在静止摄像机上捕捉，显示混淆的移动前景元素，如柔软的水或光轨，与锐化的背景景象一起。背景混淆图像，也称为摄影满天飞行，通过跟踪移动主题，以生成一个锐化的主题图像，并且背景混淆由相对运动引起。这两种技术都非常具有挑战性，需要额外设备和高级技能。在这篇论文中，我们描述了一个基于智能手机摄像机应用程序的计算摄影系统，可以自动实现这些效果，只需要点击拍照按钮。我们的方法首先检测和分割出主题。我们跟踪场景运动，并将多帧图像相互对齐，以保持所需的锐化和生成美观的运动损块。我们捕捉充足的短暂拍照，并选择输入帧中生成混淆轨迹的子集，无论场景或摄像机运动速度如何。我们预测间帧运动，并使用Synthesize Motion-blur填充时间间隔。最后，我们将混淆图像与锐化图像合并，保护面部或场景中的细微运动部分，并生成一个高分辨率和高动态范围（HDR）照片。我们的系统将这种创作风格升级到普通用户，使得大多数休闲摄影爱好者可以轻松地获得这种创新风格。更多信息和补充材料可以在我们项目网站上找到：<https://motion-mode.github.io/>
</details></li>
</ul>
<hr>
<h2 id="AI-Enhanced-Data-Processing-and-Discovery-Crowd-Sourcing-for-Meteor-Shower-Mapping"><a href="#AI-Enhanced-Data-Processing-and-Discovery-Crowd-Sourcing-for-Meteor-Shower-Mapping" class="headerlink" title="AI-Enhanced Data Processing and Discovery Crowd Sourcing for Meteor Shower Mapping"></a>AI-Enhanced Data Processing and Discovery Crowd Sourcing for Meteor Shower Mapping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02664">http://arxiv.org/abs/2308.02664</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siddha Ganju, Amartya Hatua, Peter Jenniskens, Sahyadri Krishna, Chicheng Ren, Surya Ambardar</li>
<li>for: 该研究旨在自动化数据进行处理和获得洞察，以便提高 meteor 显示的发现率。</li>
<li>methods: 该研究使用了云端 AI 智能管道，以自动化数据进行处理和分析。</li>
<li>results: 该研究已经发现了超过 200 个新的 meteor 显示，并已经验证了多个先前报告的显示。<details>
<summary>Abstract</summary>
The Cameras for Allsky Meteor Surveillance (CAMS) project, funded by NASA starting in 2010, aims to map our meteor showers by triangulating meteor trajectories detected in low-light video cameras from multiple locations across 16 countries in both the northern and southern hemispheres. Its mission is to validate, discover, and predict the upcoming returns of meteor showers. Our research aimed to streamline the data processing by implementing an automated cloud-based AI-enabled pipeline and improve the data visualization to improve the rate of discoveries by involving the public in monitoring the meteor detections. This article describes the process of automating the data ingestion, processing, and insight generation using an interpretable Active Learning and AI pipeline. This work also describes the development of an interactive web portal (the NASA Meteor Shower portal) to facilitate the visualization of meteor radiant maps. To date, CAMS has discovered over 200 new meteor showers and has validated dozens of previously reported showers.
</details>
<details>
<summary>摘要</summary>
美国国家航空航天局（NASA）自2010年起投入了“全天 Meteor 探测”（CAMS）项目，旨在通过多个国家和多个地点的低光照视频摄像头检测 meteor 轨迹，并通过三角测量确定 meteor 的轨迹。项目的目标是验证、发现和预测未来的流星雨。我们的研究旨在减少数据处理的复杂度，通过实施云端基于人工智能的自动化数据管道，并改进数据可视化以提高发现率。这篇文章描述了自动化数据进口、处理和洞察的活动学习和人工智能管道的实现方式。此外，我们还开发了一个交互式网站（NASA 流星雨门户），以便为流星辐射地图的可视化提供便捷的方式。至今，CAMS 已经发现了超过 200 个新的流星雨，并验证了数十个先前报道的雨。
</details></li>
</ul>
<hr>
<h2 id="Explainable-Deep-Learning-for-Tumor-Dynamic-Modeling-and-Overall-Survival-Prediction-using-Neural-ODE"><a href="#Explainable-Deep-Learning-for-Tumor-Dynamic-Modeling-and-Overall-Survival-Prediction-using-Neural-ODE" class="headerlink" title="Explainable Deep Learning for Tumor Dynamic Modeling and Overall Survival Prediction using Neural-ODE"></a>Explainable Deep Learning for Tumor Dynamic Modeling and Overall Survival Prediction using Neural-ODE</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01362">http://arxiv.org/abs/2308.01362</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mark Laurie, James Lu</li>
<li>for: This paper aims to improve the predictivity of tumor dynamic modeling in oncology drug development by proposing a new pharmacology-informed neural network called TDNODE.</li>
<li>methods: The TDNODE model uses an encoder-decoder architecture to express an underlying dynamical law that is generalized homogeneous with respect to time, enabling the generation of kinetic rate metrics that can be used to predict patients’ overall survival with high accuracy.</li>
<li>results: The proposed modeling formalism provides a principled way to integrate multimodal dynamical datasets in oncology disease modeling, and the generated metrics can be used to predict patients’ overall survival with high accuracy.Here’s the simplified Chinese text:</li>
<li>for: 这篇论文目的是提高肿瘤动态模型在肿瘤药物开发中的预测性，提议一种新的药理学知识感知神经网络called TDNODE。</li>
<li>methods: TDNODE模型使用编码器-解码器架构表达一个基于时间的总体法则，使得生成的生物动力学指标可以用于预测患者的全身生存率高精度。</li>
<li>results: 提议的模型形式可以理性地将多模态动态数据集成在肿瘤疾病模型中，并生成的指标可以用于预测患者的全身生存率高精度。<details>
<summary>Abstract</summary>
While tumor dynamic modeling has been widely applied to support the development of oncology drugs, there remains a need to increase predictivity, enable personalized therapy, and improve decision-making. We propose the use of Tumor Dynamic Neural-ODE (TDNODE) as a pharmacology-informed neural network to enable model discovery from longitudinal tumor size data. We show that TDNODE overcomes a key limitation of existing models in its ability to make unbiased predictions from truncated data. The encoder-decoder architecture is designed to express an underlying dynamical law which possesses the fundamental property of generalized homogeneity with respect to time. Thus, the modeling formalism enables the encoder output to be interpreted as kinetic rate metrics, with inverse time as the physical unit. We show that the generated metrics can be used to predict patients' overall survival (OS) with high accuracy. The proposed modeling formalism provides a principled way to integrate multimodal dynamical datasets in oncology disease modeling.
</details>
<details>
<summary>摘要</summary>
traditional Chinese:虾蟹肿瘤动态模型已广泛应用于肿瘤药物开发支持，但还有必要提高预测精度、实现个性化治疗和改善决策。我们提议使用肿瘤动态神经网络（TDNODE）作为药理学知识推导的神经网络，以从长期肿瘤大小数据中发现模型。我们表明，TDNODE可以超越现有模型的一个关键局限性，即从截断数据中做出不受偏见的预测。encoder-decoder架构是设计来表达下述动态法律：在时间上 generalized homogeneity 的基本性质。因此，模型 formalism 允许 encoder 输出被解释为动力学率度量，倒时间为物理单位。我们显示，生成的度量可以高精度地预测患者的总存活率（OS）。我们提出的模型 formalism 为肿瘤疾病模型集成多Modal dynamical dataset提供了原则性的方法。Here's a word-for-word translation of the text into Simplified Chinese:虾蟹肿瘤动态模型已广泛应用于肿瘤药物开发支持，但还有必要提高预测精度、实现个性化治疗和改善决策。我们提议使用肿瘤动态神经网络（TDNODE）作为药理学知识推导的神经网络，以从长期肿瘤大小数据中发现模型。我们表明，TDNODE可以超越现有模型的一个关键局限性，即从截断数据中做出不受偏见的预测。encoder-decoder架构是设计来表达下述动态法律：在时间上 generalized homogeneity 的基本性质。因此，模型 formalism 允许 encoder 输出被解释为动力学率度量，倒时间为物理单位。我们显示，生成的度量可以高精度地预测患者的总存活率（OS）。我们提出的模型 formalism 为肿瘤疾病模型集成多Modal dynamical dataset提供了原则性的方法。
</details></li>
</ul>
<hr>
<h2 id="Compressed-and-distributed-least-squares-regression-convergence-rates-with-applications-to-Federated-Learning"><a href="#Compressed-and-distributed-least-squares-regression-convergence-rates-with-applications-to-Federated-Learning" class="headerlink" title="Compressed and distributed least-squares regression: convergence rates with applications to Federated Learning"></a>Compressed and distributed least-squares regression: convergence rates with applications to Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01358">http://arxiv.org/abs/2308.01358</a></li>
<li>repo_url: None</li>
<li>paper_authors: Constantin Philippenko, Aymeric Dieuleveut</li>
<li>for: 这个论文研究了对机器学习的分布式和联合学习中使用整数压缩算法时的影响。</li>
<li>methods: 这篇论文使用了多种不同的压缩算法，并进行了对这些算法的分析。</li>
<li>results: 论文发现，即使使用不规则的随机场， covariance $\mathfrak{C}<em>{\mathrm{ania}}$ 的加法噪声仍然Scales with $\mathrm{Tr}(\mathfrak{C}</em>{\mathrm{ania}} H^{-1})&#x2F;K$，这generalizes the rate for the vanilla LSR case。此外，论文还分析了压缩策略的影响和联合学习框架中的应用。<details>
<summary>Abstract</summary>
In this paper, we investigate the impact of compression on stochastic gradient algorithms for machine learning, a technique widely used in distributed and federated learning. We underline differences in terms of convergence rates between several unbiased compression operators, that all satisfy the same condition on their variance, thus going beyond the classical worst-case analysis. To do so, we focus on the case of least-squares regression (LSR) and analyze a general stochastic approximation algorithm for minimizing quadratic functions relying on a random field. We consider weak assumptions on the random field, tailored to the analysis (specifically, expected H\"older regularity), and on the noise covariance, enabling the analysis of various randomizing mechanisms, including compression. We then extend our results to the case of federated learning.   More formally, we highlight the impact on the convergence of the covariance $\mathfrak{C}_{\mathrm{ania}}$ of the additive noise induced by the algorithm. We demonstrate despite the non-regularity of the stochastic field, that the limit variance term scales with $\mathrm{Tr}(\mathfrak{C}_{\mathrm{ania}} H^{-1})/K$ (where $H$ is the Hessian of the optimization problem and $K$ the number of iterations) generalizing the rate for the vanilla LSR case where it is $\sigma^2 \mathrm{Tr}(H H^{-1}) / K = \sigma^2 d / K$ (Bach and Moulines, 2013). Then, we analyze the dependency of $\mathfrak{C}_{\mathrm{ania}}$ on the compression strategy and ultimately its impact on convergence, first in the centralized case, then in two heterogeneous FL frameworks.
</details>
<details>
<summary>摘要</summary>
本文 investigate 分布式学习和联合学习中的压缩对Stochastic Gradient Algorithm的影响。我们比较不同压缩算法的收敛率，其中所有算法都满足同样的假设条件，超出了经典最坏情况分析。为此，我们在Least Squares Regression (LSR) 中分析一种基于随机场的普通采样算法，并对随机场假设和噪声矩阵做出弱assumption。然后，我们扩展我们的结果到联合学习中。更 formally，我们关注 covariance $\mathfrak{C}_{\text{ania}}$ 的添加噪声对算法的收敛带来的影响。我们发现，即使随机场不规则，则限制变量 $\frac{\text{Tr}(\mathfrak{C}_{\text{ania}} H^{-1})}{K}$  scales，其中 $H$ 是优化问题的梯度矩阵，$K$ 是迭代次数。这与vanilla LSR 情况相同，其中 $\sigma^2 \text{Tr}(H H^{-1}) / K = \sigma^2 d / K$ (Bach 和 Moulines, 2013)。然后，我们分析压缩策略对 covariance $\mathfrak{C}_{\text{ania}}$ 的影响，并最终探讨其对收敛的影响，首先在中央化情况下，然后在两种不同的多元联合学习框架中。
</details></li>
</ul>
<hr>
<h2 id="More-Context-Less-Distraction-Visual-Classification-by-Inferring-and-Conditioning-on-Contextual-Attributes"><a href="#More-Context-Less-Distraction-Visual-Classification-by-Inferring-and-Conditioning-on-Contextual-Attributes" class="headerlink" title="More Context, Less Distraction: Visual Classification by Inferring and Conditioning on Contextual Attributes"></a>More Context, Less Distraction: Visual Classification by Inferring and Conditioning on Contextual Attributes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01313">http://arxiv.org/abs/2308.01313</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/umd-huang-lab/perceptionclip">https://github.com/umd-huang-lab/perceptionclip</a></li>
<li>paper_authors: Bang An, Sicheng Zhu, Michael-Andrei Panaitescu-Liess, Chaithanya Kumar Mummadi, Furong Huang</li>
<li>for: 提高零shot图像分类的性能和可解释性</li>
<li>methods: 基于人类视觉过程的启发，提供图像上的 контекст特征，然后通过这些特征进行对象分类</li>
<li>results: 对比传统零shot分类方法，PerceptionCLIP可以更好地 generalized、group robustness和可解释性，例如在Waterbirds数据集上，PerceptionCLIP与ViT-L&#x2F;14组合可以提高最差组分精度 by 16.5%，并在CelebA数据集上提高组合精度 by 3.5%<details>
<summary>Abstract</summary>
CLIP, as a foundational vision language model, is widely used in zero-shot image classification due to its ability to understand various visual concepts and natural language descriptions. However, how to fully leverage CLIP's unprecedented human-like understanding capabilities to achieve better zero-shot classification is still an open question. This paper draws inspiration from the human visual perception process: a modern neuroscience view suggests that in classifying an object, humans first infer its class-independent attributes (e.g., background and orientation) which help separate the foreground object from the background, and then make decisions based on this information. Inspired by this, we observe that providing CLIP with contextual attributes improves zero-shot classification and mitigates reliance on spurious features. We also observe that CLIP itself can reasonably infer the attributes from an image. With these observations, we propose a training-free, two-step zero-shot classification method named PerceptionCLIP. Given an image, it first infers contextual attributes (e.g., background) and then performs object classification conditioning on them. Our experiments show that PerceptionCLIP achieves better generalization, group robustness, and better interpretability. For example, PerceptionCLIP with ViT-L/14 improves the worst group accuracy by 16.5% on the Waterbirds dataset and by 3.5% on CelebA.
</details>
<details>
<summary>摘要</summary>
CLIP，作为基础视语言模型，在零码图像分类中广泛应用，因其能够理解多种视觉概念和自然语言描述。然而，如何充分利用CLIP的人类样式理解能力以实现更好的零码分类仍是一个开放问题。这篇论文着眼于人类视觉过程：现代神经科学视野表明，在分类一个物体时，人们首先推理出该物体的类型独立特征（如背景和方向），然后根据这些信息进行决策。 inspirited by this，我们发现，为CLIP提供 contextual attributes 可以提高零码分类并减少偶极特征的依赖。我们还发现，CLIP本身可以有效地从图像中推理出这些特征。基于这些观察，我们提出了一种无需训练的、两步零码分类方法，名为PerceptionCLIP。给定一个图像，它首先推理出图像中的上下文特征（如背景），然后根据这些特征进行物体分类。我们的实验表明，PerceptionCLIP 可以 achieve better generalization, group robustness, and better interpretability。例如，PerceptionCLIP 与 ViT-L/14 在 Waterbirds 数据集上提高了最差群 accuracy 16.5%，并在 CelebA 数据集上提高了3.5%。
</details></li>
</ul>
<hr>
<h2 id="Lode-Encoder-AI-constrained-co-creativity"><a href="#Lode-Encoder-AI-constrained-co-creativity" class="headerlink" title="Lode Encoder: AI-constrained co-creativity"></a>Lode Encoder: AI-constrained co-creativity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01312">http://arxiv.org/abs/2308.01312</a></li>
<li>repo_url: None</li>
<li>paper_authors: Debosmita Bhaumik, Ahmed Khalifa, Julian Togelius</li>
<li>for: 这篇论文是关于开发一种基于混合 iniciative 的平台游戏困难设计系统，用于 классиic 平台游戏困难游戏《寻找宝藏》。</li>
<li>methods: 该系统是基于多个自动编码器，每个自动编码器都是根据不同的困难设计训练而成。当用户提供设计时，每个自动编码器都会生成一个更加类似于它所训练的困难设计的版本。</li>
<li>results: 据报道，该系统可以帮助设计师探索新的设计可能性，而不是仅仅是通过传统的编辑工具。用户测试表明，该系统可以帮助设计师快速创建高质量的困难设计。<details>
<summary>Abstract</summary>
We present Lode Encoder, a gamified mixed-initiative level creation system for the classic platform-puzzle game Lode Runner. The system is built around several autoencoders which are trained on sets of Lode Runner levels. When fed with the user's design, each autoencoder produces a version of that design which is closer in style to the levels that it was trained on. The Lode Encoder interface allows the user to build and edit levels through 'painting' from the suggestions provided by the autoencoders. Crucially, in order to encourage designers to explore new possibilities, the system does not include more traditional editing tools. We report on the system design and training procedure, as well as on the evolution of the system itself and user tests.
</details>
<details>
<summary>摘要</summary>
我们介绍Lode Encoder，一个基于混合 iniciativa 的游戏创作系统，专门为经典平台游戏Lode Runner创建各种各样的关卡。该系统建立在多个自动编码器之上，这些自动编码器在不同的Lode Runner关卡集上进行训练。当用户输入设计时，每个自动编码器都会生成一个更像原始关卡的版本。Lode Encoder 界面允许用户通过"油画"的方式从自动编码器提供的建议中创建和编辑关卡。重要的是，以便鼓励设计师探索新的可能性，该系统不包括传统的编辑工具。我们介绍了系统的设计和训练过程，以及用户测试。
</details></li>
</ul>
<hr>
<h2 id="Masked-and-Swapped-Sequence-Modeling-for-Next-Novel-Basket-Recommendation-in-Grocery-Shopping"><a href="#Masked-and-Swapped-Sequence-Modeling-for-Next-Novel-Basket-Recommendation-in-Grocery-Shopping" class="headerlink" title="Masked and Swapped Sequence Modeling for Next Novel Basket Recommendation in Grocery Shopping"></a>Masked and Swapped Sequence Modeling for Next Novel Basket Recommendation in Grocery Shopping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01308">http://arxiv.org/abs/2308.01308</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liming-7/mask-swap-nnbr">https://github.com/liming-7/mask-swap-nnbr</a></li>
<li>paper_authors: Ming Li, Mozhdeh Ariannezhad, Andrew Yates, Maarten de Rijke</li>
<li>for: 本研究的目的是提出一种新的下一个购物篮子推荐任务（NNBR），即推荐用户未前吃过的食品。</li>
<li>methods: 本研究提出了一种简单的双向变换器购物篮子推荐模型（BTBR），该模型直接模型食品之间的相关性，并使用了不同的屏蔽策略和训练目标来进行训练。</li>
<li>results: 实验结果表明，BTBR可以有效地解决NNBR任务，并且可以采用不同的屏蔽策略和训练目标来进一步提高性能。<details>
<summary>Abstract</summary>
Next basket recommendation (NBR) is the task of predicting the next set of items based on a sequence of already purchased baskets. It is a recommendation task that has been widely studied, especially in the context of grocery shopping. In next basket recommendation (NBR), it is useful to distinguish between repeat items, i.e., items that a user has consumed before, and explore items, i.e., items that a user has not consumed before. Most NBR work either ignores this distinction or focuses on repeat items. We formulate the next novel basket recommendation (NNBR) task, i.e., the task of recommending a basket that only consists of novel items, which is valuable for both real-world application and NBR evaluation. We evaluate how existing NBR methods perform on the NNBR task and find that, so far, limited progress has been made w.r.t. the NNBR task. To address the NNBR task, we propose a simple bi-directional transformer basket recommendation model (BTBR), which is focused on directly modeling item-to-item correlations within and across baskets instead of learning complex basket representations. To properly train BTBR, we propose and investigate several masking strategies and training objectives: (i) item-level random masking, (ii) item-level select masking, (iii) basket-level all masking, (iv) basket-level explore masking, and (v) joint masking. In addition, an item-basket swapping strategy is proposed to enrich the item interactions within the same baskets. We conduct extensive experiments on three open datasets with various characteristics. The results demonstrate the effectiveness of BTBR and our masking and swapping strategies for the NNBR task. BTBR with a properly selected masking and swapping strategy can substantially improve NNBR performance.
</details>
<details>
<summary>摘要</summary>
下一个篮筐推荐（NBR）任务是预测下一个篮筐中的项目，基于已经购买过的篮筐序列。这是一项推荐任务，尤其在超市购物中广泛研究。在NBR任务中，分 distinguish between repeat items，即用户已经消费过的项目，和 explore items，即用户没有消费过的项目。大多数NBR工作忽略这种分类或者专注于 repeat items。我们提出了下一个新篮筐推荐（NNBR）任务，即推荐一个仅由新项目组成的篮筐，这对于实际应用和NBR评估都具有价值。我们评估了现有NBR方法在NNBR任务中的性能，并发现，迄今为止，对NNBR任务的进展还很有限。为解决NNBR任务，我们提议了一个简单的双向转换器篮筐推荐模型（BTBR），该模型专门模型item-to-item相关性内 и外 basket中。为正确训练BTBR，我们提议并研究了多种masquerade策略和训练目标：（i）item-level随机masquerade，（ii）item-level选择masquerade，（iii）basket-level所有masquerade，（iv）basket-level探索masquerade，（v）联合masquerade。此外，我们还提议了一种item-篮筐交换策略，以便在同一个篮筐中增强item之间的交互。我们对三个开源数据集进行了广泛的实验，结果表明，BTBR和我们的masquerade策略和item-篮筐交换策略可以很好地提高NNBR性能。
</details></li>
</ul>
<hr>
<h2 id="Excitatory-Inhibitory-Balance-Emerges-as-a-Key-Factor-for-RBN-Performance-Overriding-Attractor-Dynamics"><a href="#Excitatory-Inhibitory-Balance-Emerges-as-a-Key-Factor-for-RBN-Performance-Overriding-Attractor-Dynamics" class="headerlink" title="Excitatory&#x2F;Inhibitory Balance Emerges as a Key Factor for RBN Performance, Overriding Attractor Dynamics"></a>Excitatory&#x2F;Inhibitory Balance Emerges as a Key Factor for RBN Performance, Overriding Attractor Dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10831">http://arxiv.org/abs/2308.10831</a></li>
<li>repo_url: None</li>
<li>paper_authors: Emmanuel Calvet, Jean Rouat, Bertrand Reulet</li>
<li>for: 这个论文主要是为了研究储存和预测两个复杂任务中的表现。</li>
<li>methods: 作者使用了随机布尔网络（RBNs）来研究连接性、动力学和性能之间的关系。</li>
<li>results: 研究发现，在特定的分布参数下，随机布尔网络可以导致多样化的动力学行为，并且大多数储存器具有一个主要吸引器。此外，作者发现，储存和预测任务中的表现与储存器的内生动力学行为有很少关系。<details>
<summary>Abstract</summary>
Reservoir computing provides a time and cost-efficient alternative to traditional learning methods.Critical regimes, known as the "edge of chaos," have been found to optimize computational performance in binary neural networks. However, little attention has been devoted to studying reservoir-to-reservoir variability when investigating the link between connectivity, dynamics, and performance. As physical reservoir computers become more prevalent, developing a systematic approach to network design is crucial. In this article, we examine Random Boolean Networks (RBNs) and demonstrate that specific distribution parameters can lead to diverse dynamics near critical points. We identify distinct dynamical attractors and quantify their statistics, revealing that most reservoirs possess a dominant attractor. We then evaluate performance in two challenging tasks, memorization and prediction, and find that a positive excitatory balance produces a critical point with higher memory performance. In comparison, a negative inhibitory balance delivers another critical point with better prediction performance. Interestingly, we show that the intrinsic attractor dynamics have little influence on performance in either case.
</details>
<details>
<summary>摘要</summary>
储备计算提供了传统学习方法的时间和成本效益的代替方案。critical regime, known as the "edge of chaos", have been found to optimize computational performance in binary neural networks. However, little attention has been devoted to studying reservoir-to-reservoir variability when investigating the link between connectivity, dynamics, and performance. As physical reservoir computers become more prevalent, developing a systematic approach to network design is crucial. In this article, we examine Random Boolean Networks (RBNs) and demonstrate that specific distribution parameters can lead to diverse dynamics near critical points. We identify distinct dynamical attractors and quantify their statistics, revealing that most reservoirs possess a dominant attractor. We then evaluate performance in two challenging tasks, memorization and prediction, and find that a positive excitatory balance produces a critical point with higher memory performance. In comparison, a negative inhibitory balance delivers another critical point with better prediction performance. Interestingly, we show that the intrinsic attractor dynamics have little influence on performance in either case.Here's the translation in Traditional Chinese:储备计算提供了传统学习方法的时间和成本效益的代替方案。critical regime, known as the "edge of chaos", have been found to optimize computational performance in binary neural networks. However, little attention has been devoted to studying reservoir-to-reservoir variability when investigating the link between connectivity, dynamics, and performance. As physical reservoir computers become more prevalent, developing a systematic approach to network design is crucial. In this article, we examine Random Boolean Networks (RBNs) and demonstrate that specific distribution parameters can lead to diverse dynamics near critical points. We identify distinct dynamical attractors and quantify their statistics, revealing that most reservoirs possess a dominant attractor. We then evaluate performance in two challenging tasks, memorization and prediction, and find that a positive excitatory balance produces a critical point with higher memory performance. In comparison, a negative inhibitory balance delivers another critical point with better prediction performance. Interestingly, we show that the intrinsic attractor dynamics have little influence on performance in either case.
</details></li>
</ul>
<hr>
<h2 id="EmbeddingTree-Hierarchical-Exploration-of-Entity-Features-in-Embedding"><a href="#EmbeddingTree-Hierarchical-Exploration-of-Entity-Features-in-Embedding" class="headerlink" title="EmbeddingTree: Hierarchical Exploration of Entity Features in Embedding"></a>EmbeddingTree: Hierarchical Exploration of Entity Features in Embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01329">http://arxiv.org/abs/2308.01329</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yan Zheng, Junpeng Wang, Chin-Chia Michael Yeh, Yujie Fan, Huiyuan Chen, Liang Wang, Wei Zhang</li>
<li>for: 这篇论文是为了探讨嵌入学习算法中feature的编码方式而写的。</li>
<li>methods: 该论文提出了一种嵌入探索算法，名为EmbeddingTree，可以结构化解释嵌入空间中feature的编码方式。</li>
<li>results: 通过使用EmbeddingTree和相关视觉化工具，可以帮助用户更好地了解高维嵌入的特征，进行嵌入训练中的特征杂谱除法和新数据集的嵌入生成。<details>
<summary>Abstract</summary>
Embedding learning transforms discrete data entities into continuous numerical representations, encoding features/properties of the entities. Despite the outstanding performance reported from different embedding learning algorithms, few efforts were devoted to structurally interpreting how features are encoded in the learned embedding space. This work proposes EmbeddingTree, a hierarchical embedding exploration algorithm that relates the semantics of entity features with the less-interpretable embedding vectors. An interactive visualization tool is also developed based on EmbeddingTree to explore high-dimensional embeddings. The tool helps users discover nuance features of data entities, perform feature denoising/injecting in embedding training, and generate embeddings for unseen entities. We demonstrate the efficacy of EmbeddingTree and our visualization tool through embeddings generated for industry-scale merchant data and the public 30Music listening/playlists dataset.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate_language: zh-CNEmbedding learning 将粒度数据实体转换为连续数字表示，卷积特征/属性实体。尽管不同的嵌入学习算法报告了出色的性能，但有很少努力投入到嵌入空间中特征的结构性解释。这项工作提出了嵌入树（EmbeddingTree），一种嵌入探索算法，将实体特征 semantics 与卷积向量相关联。我们还开发了基于嵌入树的互动视觉化工具，帮助用户探索高维卷积中的细节特征，进行嵌入训练中的特征杂谔/插入、生成未看到的实体嵌入。我们通过使用industry-scale merchant数据和公共30Music listening/playlists数据集来证明嵌入树和我们的视觉化工具的有效性。
</details></li>
</ul>
<hr>
<h2 id="Investigation-on-Machine-Learning-Based-Approaches-for-Estimating-the-Critical-Temperature-of-Superconductors"><a href="#Investigation-on-Machine-Learning-Based-Approaches-for-Estimating-the-Critical-Temperature-of-Superconductors" class="headerlink" title="Investigation on Machine Learning Based Approaches for Estimating the Critical Temperature of Superconductors"></a>Investigation on Machine Learning Based Approaches for Estimating the Critical Temperature of Superconductors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01932">http://arxiv.org/abs/2308.01932</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fatin Abrar Shams, Rashed Hasan Ratul, Ahnaf Islam Naf, Syed Shaek Hossain Samir, Mirza Muntasir Nishat, Fahim Faisal, Md. Ashraful Hoque</li>
<li>for: 预测超导材料的 kritical 温度</li>
<li>methods: 使用堆叠机器学习方法对超导材料的复杂特征进行训练，以准确预测 kritical 温度</li>
<li>results: 与其他前一次可 accessible 研究相比，该模型显示了良好的性能，RMSE 为 9.68，R2 值为 0.922，这些发现可能为堆叠ensemble方法与超参数优化（HPO）提供新的视角。<details>
<summary>Abstract</summary>
Superconductors have been among the most fascinating substances, as the fundamental concept of superconductivity as well as the correlation of critical temperature and superconductive materials have been the focus of extensive investigation since their discovery. However, superconductors at normal temperatures have yet to be identified. Additionally, there are still many unknown factors and gaps of understanding regarding this unique phenomenon, particularly the connection between superconductivity and the fundamental criteria to estimate the critical temperature. To bridge the gap, numerous machine learning techniques have been established to estimate critical temperatures as it is extremely challenging to determine. Furthermore, the need for a sophisticated and feasible method for determining the temperature range that goes beyond the scope of the standard empirical formula appears to be strongly emphasized by various machine-learning approaches. This paper uses a stacking machine learning approach to train itself on the complex characteristics of superconductive materials in order to accurately predict critical temperatures. In comparison to other previous accessible research investigations, this model demonstrated a promising performance with an RMSE of 9.68 and an R2 score of 0.922. The findings presented here could be a viable technique to shed new insight on the efficient implementation of the stacking ensemble method with hyperparameter optimization (HPO).
</details>
<details>
<summary>摘要</summary>
超导材料已经是最吸引人的物质之一，因为超导性的基本概念以及相关的极点温度和超导材料的关系已经在发现之后得到了广泛的研究。然而，在常温下发现超导材料仍然没有被发现。此外，关于这一独特现象的多种未知因素和理解的缺陷仍然存在，特别是超导性和基本的评估极点温度的连接。为了填补这些缺陷，许多机器学习技术已经被建立来估算极点温度。此外，需要一种可靠和实用的方法来确定极点温度范围，这超出了标准的Empirical formula的范围。本文使用堆叠机器学习方法来训练自己，以准确预测超导材料的极点温度。与之前可 accessible 的研究相比，这个模型表现了非常出色的性能，RMSE 为 9.68 和 R2 分数为 0.922。本文所提出的结论可能是一种可靠的技术来释明堆叠ensemble method 的可行性和hyperparameter optimization（HPO）的进一步发展。
</details></li>
</ul>
<hr>
<h2 id="BRNES-Enabling-Security-and-Privacy-aware-Experience-Sharing-in-Multiagent-Robotic-and-Autonomous-Systems"><a href="#BRNES-Enabling-Security-and-Privacy-aware-Experience-Sharing-in-Multiagent-Robotic-and-Autonomous-Systems" class="headerlink" title="BRNES: Enabling Security and Privacy-aware Experience Sharing in Multiagent Robotic and Autonomous Systems"></a>BRNES: Enabling Security and Privacy-aware Experience Sharing in Multiagent Robotic and Autonomous Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01274">http://arxiv.org/abs/2308.01274</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aralab-unr/brnes">https://github.com/aralab-unr/brnes</a></li>
<li>paper_authors: Md Tamjid Hossain, Hung Manh La, Shahriar Badsha, Anton Netchaev</li>
<li>for: 加速多智能体学习（MARL）的辅导者-被辅导者框架</li>
<li>methods: 使用启发式邻居区选择和权重经验聚合技术来降低滥览攻击的影响，并保护智能体的私有信息免受敌意推理攻击</li>
<li>results: 在拥有攻击者的情况下，提出了一个新的MARL框架（BRNES），可以快速地达到目标，并且在拥有隐私保护的情况下，对智能体的私有信息进行保护。 experiments show that our framework outperforms the state-of-the-art in terms of steps to goal, obtained reward, and time to goal metrics, and is 8.32x faster than non-private frameworks and 1.41x faster than private frameworks in an adversarial setting.<details>
<summary>Abstract</summary>
Although experience sharing (ES) accelerates multiagent reinforcement learning (MARL) in an advisor-advisee framework, attempts to apply ES to decentralized multiagent systems have so far relied on trusted environments and overlooked the possibility of adversarial manipulation and inference. Nevertheless, in a real-world setting, some Byzantine attackers, disguised as advisors, may provide false advice to the advisee and catastrophically degrade the overall learning performance. Also, an inference attacker, disguised as an advisee, may conduct several queries to infer the advisors' private information and make the entire ES process questionable in terms of privacy leakage. To address and tackle these issues, we propose a novel MARL framework (BRNES) that heuristically selects a dynamic neighbor zone for each advisee at each learning step and adopts a weighted experience aggregation technique to reduce Byzantine attack impact. Furthermore, to keep the agent's private information safe from adversarial inference attacks, we leverage the local differential privacy (LDP)-induced noise during the ES process. Our experiments show that our framework outperforms the state-of-the-art in terms of the steps to goal, obtained reward, and time to goal metrics. Particularly, our evaluation shows that the proposed framework is 8.32x faster than the current non-private frameworks and 1.41x faster than the private frameworks in an adversarial setting.
</details>
<details>
<summary>摘要</summary>
although experience sharing (ES) can accelerate multiagent reinforcement learning (MARL) in an advisor-advisee framework, attempts to apply ES to decentralized multiagent systems have relied on trusted environments and ignored the possibility of adversarial manipulation and inference. however, in a real-world setting, some Byzantine attackers may provide false advice to the advisee and catastrophically degrade the overall learning performance. additionally, an inference attacker may conduct several queries to infer the advisors' private information, making the entire ES process questionable in terms of privacy leakage. to address these issues, we propose a novel MARL framework (BRNES) that heuristically selects a dynamic neighbor zone for each advisee at each learning step and adopts a weighted experience aggregation technique to reduce Byzantine attack impact. furthermore, to keep the agent's private information safe from adversarial inference attacks, we leverage local differential privacy (LDP)-induced noise during the ES process. our experiments show that our framework outperforms the state-of-the-art in terms of the steps to goal, obtained reward, and time to goal metrics. particularly, our evaluation shows that the proposed framework is 8.32x faster than the current non-private frameworks and 1.41x faster than the private frameworks in an adversarial setting.
</details></li>
</ul>
<hr>
<h2 id="A-Probabilistic-Approach-to-Self-Supervised-Learning-using-Cyclical-Stochastic-Gradient-MCMC"><a href="#A-Probabilistic-Approach-to-Self-Supervised-Learning-using-Cyclical-Stochastic-Gradient-MCMC" class="headerlink" title="A Probabilistic Approach to Self-Supervised Learning using Cyclical Stochastic Gradient MCMC"></a>A Probabilistic Approach to Self-Supervised Learning using Cyclical Stochastic Gradient MCMC</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01271">http://arxiv.org/abs/2308.01271</a></li>
<li>repo_url: None</li>
<li>paper_authors: Masoumeh Javanbakhat, Christoph Lippert</li>
<li>for: 这个论文是为了提出一种实用的 bayesian自适应学习方法，使用循环随机梯度哈密顿 Monte Carlo（cSGHMC）。</li>
<li>methods: 这种方法使用 prior 来定义自适应学习模型的参数，并使用 cSGHMC 来近似高维和多模态的 posterior 分布。</li>
<li>results: 通过寻找高维和多模态的 posterior 分布， bayesian 自适应学习可以生成可读性和多样性的表示，并在多个下游分类任务上得到显著的性能提升、评估和对外部数据集的检测。<details>
<summary>Abstract</summary>
In this paper we present a practical Bayesian self-supervised learning method with Cyclical Stochastic Gradient Hamiltonian Monte Carlo (cSGHMC). Within this framework, we place a prior over the parameters of a self-supervised learning model and use cSGHMC to approximate the high dimensional and multimodal posterior distribution over the embeddings. By exploring an expressive posterior over the embeddings, Bayesian self-supervised learning produces interpretable and diverse representations. Marginalizing over these representations yields a significant gain in performance, calibration and out-of-distribution detection on a variety of downstream classification tasks. We provide experimental results on multiple classification tasks on four challenging datasets. Moreover, we demonstrate the effectiveness of the proposed method in out-of-distribution detection using the SVHN and CIFAR-10 datasets.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种实用的 bayesian自适应学习方法，即循环随机梯度汉堡 Monte Carlo（cSGHMC）。在这个框架中，我们将自适应学习模型的参数置于先验分布中，并使用cSGHMC来近似高维多模态的后验分布。通过探索表征空间的表示，bayesian自适应学习可以生成可读取和多样的表示。对这些表示进行聚合，可以得到大幅提高的性能、评估和异常检测性能。我们在多个分类任务上进行了多个数据集的实验，并证明了提posed方法的效果。此外，我们还用SVHN和CIFAR-10数据集来证明方法的异常检测能力。
</details></li>
</ul>
<hr>
<h2 id="Tirtha-–-An-Automated-Platform-to-Crowdsource-Images-and-Create-3D-Models-of-Heritage-Sites"><a href="#Tirtha-–-An-Automated-Platform-to-Crowdsource-Images-and-Create-3D-Models-of-Heritage-Sites" class="headerlink" title="Tirtha – An Automated Platform to Crowdsource Images and Create 3D Models of Heritage Sites"></a>Tirtha – An Automated Platform to Crowdsource Images and Create 3D Models of Heritage Sites</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01246">http://arxiv.org/abs/2308.01246</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/smlab-niser/tirtha-public">https://github.com/smlab-niser/tirtha-public</a></li>
<li>paper_authors: Jyotirmaya Shivottam, Subhankar Mishra</li>
<li>for: 这篇论文的目的是为了推广和实现文化遗产（CH）区域的数位保存，并提供一个可靠、可读性高的平台 для将这些遗产转换为三维模型。</li>
<li>methods: 这篇论文使用了最新的构造从动（SfM）和多视野视力（MVS）技术，并提供了一个可调、可扩展的架构，以应对未来摄影学技术的进步。</li>
<li>results: 这篇论文的结果是一个名为Tirtha的网络平台，可以将拍摄到的CH区域转换为三维模型，并将这些模型提供给研究人员和公众进行检视、互动和下载。<details>
<summary>Abstract</summary>
Digital preservation of Cultural Heritage (CH) sites is crucial to protect them against damage from natural disasters or human activities. Creating 3D models of CH sites has become a popular method of digital preservation thanks to advancements in computer vision and photogrammetry. However, the process is time-consuming, expensive, and typically requires specialized equipment and expertise, posing challenges in resource-limited developing countries. Additionally, the lack of an open repository for 3D models hinders research and public engagement with their heritage. To address these issues, we propose Tirtha, a web platform for crowdsourcing images of CH sites and creating their 3D models. Tirtha utilizes state-of-the-art Structure from Motion (SfM) and Multi-View Stereo (MVS) techniques. It is modular, extensible and cost-effective, allowing for the incorporation of new techniques as photogrammetry advances. Tirtha is accessible through a web interface at https://tirtha.niser.ac.in and can be deployed on-premise or in a cloud environment. In our case studies, we demonstrate the pipeline's effectiveness by creating 3D models of temples in Odisha, India, using crowdsourced images. These models are available for viewing, interaction, and download on the Tirtha website. Our work aims to provide a dataset of crowdsourced images and 3D reconstructions for research in computer vision, heritage conservation, and related domains. Overall, Tirtha is a step towards democratizing digital preservation, primarily in resource-limited developing countries.
</details>
<details>
<summary>摘要</summary>
digitization of cultural heritage (遗产) sites is crucial to protect them against damage from natural disasters or human activities. Creating 3D models of CH sites has become a popular method of digital preservation thanks to advancements in computer vision and photogrammetry. However, the process is time-consuming, expensive, and typically requires specialized equipment and expertise, posing challenges in resource-limited developing countries. Additionally, the lack of an open repository for 3D models hinders research and public engagement with their heritage. To address these issues, we propose Tirtha, a web platform for crowdsourcing images of CH sites and creating their 3D models. Tirtha utilizes state-of-the-art Structure from Motion (SfM) and Multi-View Stereo (MVS) techniques. It is modular, extensible, and cost-effective, allowing for the incorporation of new techniques as photogrammetry advances. Tirtha is accessible through a web interface at https://tirtha.niser.ac.in and can be deployed on-premise or in a cloud environment. In our case studies, we demonstrate the pipeline's effectiveness by creating 3D models of temples in Odisha, India, using crowdsourced images. These models are available for viewing, interaction, and download on the Tirtha website. Our work aims to provide a dataset of crowdsourced images and 3D reconstructions for research in computer vision, heritage conservation, and related domains. Overall, Tirtha is a step towards democratizing digital preservation, primarily in resource-limited developing countries.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://example.com/2023/08/03/cs.LG_2023_08_03/" data-id="cllsjvzca002nf588fecoapr0" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_08_03" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/03/cs.SD_2023_08_03/" class="article-date">
  <time datetime="2023-08-02T16:00:00.000Z" itemprop="datePublished">2023-08-03</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/03/cs.SD_2023_08_03/">cs.SD - 2023-08-03 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Versatile-Time-Frequency-Representations-Realized-by-Convex-Penalty-on-Magnitude-Spectrogram"><a href="#Versatile-Time-Frequency-Representations-Realized-by-Convex-Penalty-on-Magnitude-Spectrogram" class="headerlink" title="Versatile Time-Frequency Representations Realized by Convex Penalty on Magnitude Spectrogram"></a>Versatile Time-Frequency Representations Realized by Convex Penalty on Magnitude Spectrogram</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01665">http://arxiv.org/abs/2308.01665</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keidai Arai, Koki Yamada, Kohei Yatabe</li>
<li>for: 本文旨在提出一种基于凸优化的时域频域（T-F）表示方法，以实现根据用户指定的特性来设计T-F表示。</li>
<li>methods: 本文使用了基于凸优化的方法，包括基于约束的最小二乘优化和梯度下降优化，来设计T-F表示。</li>
<li>results: 本文通过实验和分析表明，提出的方法可以实现根据用户指定的特性来设计T-F表示，并且可以获得低维度或均匀幅度的T-F表示。<details>
<summary>Abstract</summary>
Sparse time-frequency (T-F) representations have been an important research topic for more than several decades. Among them, optimization-based methods (in particular, extensions of basis pursuit) allow us to design the representations through objective functions. Since acoustic signal processing utilizes models of spectrogram, the flexibility of optimization-based T-F representations is helpful for adjusting the representation for each application. However, acoustic applications often require models of \textit{magnitude} of T-F representations obtained by discrete Gabor transform (DGT). Adjusting a T-F representation to such a magnitude model (e.g., smoothness of magnitude of DGT coefficients) results in a non-convex optimization problem that is difficult to solve. In this paper, instead of tackling difficult non-convex problems, we propose a convex optimization-based framework that realizes a T-F representation whose magnitude has characteristics specified by the user. We analyzed the properties of the proposed method and provide numerical examples of sparse T-F representations having, e.g., low-rank or smooth magnitude, which have not been realized before.
</details>
<details>
<summary>摘要</summary>
质量时频（T-F）表示已经是研究领域的重要话题，一直以来的几十年。其中，优化基于方法（尤其是基数追求的扩展），允许我们通过目标函数设计表示。由于音声信号处理使用spectrogram模型，优化基于T-F表示的灵活性对于每个应用程序都很有用。然而，音声应用经常需要DGT变折 coefficient的大小模型（例如，DGT变折 coefficient的平滑度）。对于这种非凸优化问题，我们提出了一种凸优化框架，可以实现T-F表示的大小具有用户指定的特性。我们分析了提案方法的性质并提供了数字示例，其中包括低级或平滑的T-F表示，这些表示没有在过去被实现过。
</details></li>
</ul>
<hr>
<h2 id="Optimizing-multi-user-sound-communications-in-reverberating-environments-with-acoustic-reconfigurable-metasurfaces"><a href="#Optimizing-multi-user-sound-communications-in-reverberating-environments-with-acoustic-reconfigurable-metasurfaces" class="headerlink" title="Optimizing multi-user sound communications in reverberating environments with acoustic reconfigurable metasurfaces"></a>Optimizing multi-user sound communications in reverberating environments with acoustic reconfigurable metasurfaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01531">http://arxiv.org/abs/2308.01531</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongkuan Zhang, Qiyuan Wang, Mathias Fink, Guancong Ma</li>
<li>for: 解决在噪声环境中多个人同时发送多个消息，使每个消息都能够清晰地传递而无任何干扰。</li>
<li>methods: 开发智能声学墙，通过电子控制和学习算法控制室内 geometry 和源 sender 的位置，实现不受干扰的多个人同时沟通。</li>
<li>results: 实现了不受干扰的多个人同时沟通，包括不同的音乐来源之间的同时播放、频分多路播放和多用户沟通等多种功能。<details>
<summary>Abstract</summary>
How do you ensure that, in a reverberant room, several people can speak simultaneously to several other people, making themselves perfectly understood and without any crosstalk between messages? In this work, we report a conceptual solution to this problem by developing an intelligent acoustic wall, which can be reconfigured electronically and is controlled by a learning algorithm that adapts to the geometry of the room and the positions of sources and receivers. To this end, a portion of the room boundaries is covered with a smart mirror made of a broadband acoustic reconfigurable metasurface (ARMs) designed to provide a two-state (0 or {\pi}) phase shift in the reflected waves by 200 independently tunable units. The whole phase pattern is optimized to maximize the Shannon capacity while minimizing crosstalk between the different sources and receivers. We demonstrate the control of multi-spectral sound fields covering a spectrum much larger than the coherence bandwidth of the room for diverse striking functionalities, including crosstalk-free acoustic communication, frequency-multiplexed communications, and multi-user communications. An experiment conducted with two music sources for two different people demonstrates a crosstalk-free simultaneous music playback. Our work opens new routes for the control of sound waves in complex media and for a new generation of acoustic devices.
</details>
<details>
<summary>摘要</summary>
如何在吸吟的房间中，许多人同时发送消息，使自己完全清晰无任何跨信号干扰？在这项工作中，我们报道了一种概念解决方案，通过开发智能听音墙， elektronisch控制 Room 的 geometry 和发送器和接收器的位置。为此，房间的一部分使用智能镜，由200个独立调整的单元组成，可以提供0或π的零phas shift。整个零phas pattern 被优化，以最大化信annon容量，同时最小化不同发送器和接收器之间的跨信号干扰。我们示例了多 Spectral 听音场，覆盖了 much larger than the coherence bandwidth of the room的谱，包括无干扰的听音通信、频分多路通信和多用户通信。在实验中，使用两个不同的音乐来源，对两个不同的人进行同时播放音乐，成功实现了无干扰的同时播放。我们的工作开启了新的控制听音波在复杂媒体中的新 Routes 和一代新的听音设备。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://example.com/2023/08/03/cs.SD_2023_08_03/" data-id="cllsjvzd1004wf5884wan1p6g" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_08_03" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/03/eess.AS_2023_08_03/" class="article-date">
  <time datetime="2023-08-02T16:00:00.000Z" itemprop="datePublished">2023-08-03</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/03/eess.AS_2023_08_03/">eess.AS - 2023-08-03 22:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Many-to-Many-Spoken-Language-Translation-via-Unified-Speech-and-Text-Representation-Learning-with-Unit-to-Unit-Translation"><a href="#Many-to-Many-Spoken-Language-Translation-via-Unified-Speech-and-Text-Representation-Learning-with-Unit-to-Unit-Translation" class="headerlink" title="Many-to-Many Spoken Language Translation via Unified Speech and Text Representation Learning with Unit-to-Unit Translation"></a>Many-to-Many Spoken Language Translation via Unified Speech and Text Representation Learning with Unit-to-Unit Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01831">http://arxiv.org/abs/2308.01831</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minsu Kim, Jeongsoo Choi, Dahun Kim, Yong Man Ro</li>
<li>for: 本研究旨在提出一种方法，使用单一模型来学习多语言speech和文本的统一表示，尤其关注speech synthesis的目的。</li>
<li>methods: 我们使用自动编码的speech特征来表示多语言speech音频，并将其视为pseudo文本来处理。然后，我们提议使用encoder-decoder结构的模型，以Unit-to-Unit Translation（UTUT）目标进行训练。</li>
<li>results: 我们通过在多种语言下进行了广泛的实验， Validate了我们提议的方法的有效性 across多种多语言任务，包括Speech-to-Speech Translation（STS）、多语言Text-to-Speech Synthesis（TTS）和Text-to-Speech Translation（TTST）。此外，我们还证明UTUT可以完成多语言STS，这在文献中尚未被探讨。<details>
<summary>Abstract</summary>
In this paper, we propose a method to learn unified representations of multilingual speech and text with a single model, especially focusing on the purpose of speech synthesis. We represent multilingual speech audio with speech units, the quantized representations of speech features encoded from a self-supervised speech model. Therefore, we can focus on their linguistic content by treating the audio as pseudo text and can build a unified representation of speech and text. Then, we propose to train an encoder-decoder structured model with a Unit-to-Unit Translation (UTUT) objective on multilingual data. Specifically, by conditioning the encoder with the source language token and the decoder with the target language token, the model is optimized to translate the spoken language into that of the target language, in a many-to-many language translation setting. Therefore, the model can build the knowledge of how spoken languages are comprehended and how to relate them to different languages. A single pre-trained model with UTUT can be employed for diverse multilingual speech- and text-related tasks, such as Speech-to-Speech Translation (STS), multilingual Text-to-Speech Synthesis (TTS), and Text-to-Speech Translation (TTST). By conducting comprehensive experiments encompassing various languages, we validate the efficacy of the proposed method across diverse multilingual tasks. Moreover, we show UTUT can perform many-to-many language STS, which has not been previously explored in the literature. Samples are available on https://choijeongsoo.github.io/utut.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种方法，可以通过单个模型学习多语言speech和文本的统一表示，尤其是在speech synthesis的目的下。我们使用自动生成的speech模型来编码speech特征，并将其转换为speech单元。因此，我们可以忽略它们的语言特点，将audio视为pseudo文本，并建立speech和文本的统一表示。然后，我们提议使用encoder-decoder结构的模型，通过Unit-to-Unit Translation（UTUT）目标在多语言数据上进行训练。具体来说，通过将源语言token条件到encoder，并将目标语言token条件到decoder，模型将被优化以将说话语言翻译成目标语言，这种Setting被称为多语言翻译。因此，模型可以学习说话语言的理解和如何将其与不同语言相关联。一个预训练的UTUT模型可以在多种多语言speech-和文本相关任务上进行多样化应用，如Speech-to-Speech Translation（STS）、多语言文本识别（TTS）和文本识别翻译（TTST）。我们通过包括多种语言的实验， validate了我们提出的方法的有效性 across多种多语言任务。此外，我们还证明UTUT可以实现多语言STS，这在文献中尚未被探讨。样本可以在https://choijeongsoo.github.io/utut 上找到。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://example.com/2023/08/03/eess.AS_2023_08_03/" data-id="cllsjvzdp006qf588e4js9ypu" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_08_03" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/03/eess.IV_2023_08_03/" class="article-date">
  <time datetime="2023-08-02T16:00:00.000Z" itemprop="datePublished">2023-08-03</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/03/eess.IV_2023_08_03/">eess.IV - 2023-08-03 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Maximum-likelihood-estimation-in-ptychography-in-the-presence-of-Poisson-Gaussian-noise-statistics"><a href="#Maximum-likelihood-estimation-in-ptychography-in-the-presence-of-Poisson-Gaussian-noise-statistics" class="headerlink" title="Maximum-likelihood estimation in ptychography in the presence of Poisson-Gaussian noise statistics"></a>Maximum-likelihood estimation in ptychography in the presence of Poisson-Gaussian noise statistics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02436">http://arxiv.org/abs/2308.02436</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jacob Seifert, Yifeng Shao, Rens van Dam, Dorian Bouchet, Tristan van Leeuwen, Allard P. Mosk</li>
<li>for: 提高图像质量，特别是在低信号强度（SNR）下。</li>
<li>methods: 利用最大可能性估计，在梯度基于ptychography优化中考虑camera读取噪声。</li>
<li>results: 通过实验和数值数据，我们的方法比传统方法有更高的图像重建质量，并且可以在低SNR下提高图像质量。<details>
<summary>Abstract</summary>
Optical measurements often exhibit mixed Poisson-Gaussian noise statistics, which hampers image quality, particularly under low signal-to-noise ratio (SNR) conditions. Computational imaging falls short in such situations when solely Poissonian noise statistics are assumed. In response to this challenge, we define a loss function that explicitly incorporates this mixed noise nature. By using maximum-likelihood estimation, we devise a practical method to account for camera readout noise in gradient-based ptychography optimization. Our results, based on both experimental and numerical data, demonstrate that this approach outperforms the conventional one, enabling enhanced image reconstruction quality under challenging noise conditions through a straightforward methodological adjustment.
</details>
<details>
<summary>摘要</summary>
光学测量经常表现出杂合波尔兹-加布拉斯噪声统计，这会影响图像质量，特别是在低信号噪声比（SNR）条件下。计算成像在这些情况下失败，因为它假设了单一的波尔兹噪声统计。为解决这个挑战，我们定义了一个损失函数，其直接表达混合噪声的 NATURE。通过最大likelihood估计，我们开发了一种实用的方法，用于在梯度基本ptychography优化中考虑相机读取噪声。我们的结果，基于实验和数值数据，表明这种方法在具有挑战性的噪声条件下表现出色，可以通过简单的方法调整来提高图像重建质量。
</details></li>
</ul>
<hr>
<h2 id="Focus-on-Content-not-Noise-Improving-Image-Generation-for-Nuclei-Segmentation-by-Suppressing-Steganography-in-CycleGAN"><a href="#Focus-on-Content-not-Noise-Improving-Image-Generation-for-Nuclei-Segmentation-by-Suppressing-Steganography-in-CycleGAN" class="headerlink" title="Focus on Content not Noise: Improving Image Generation for Nuclei Segmentation by Suppressing Steganography in CycleGAN"></a>Focus on Content not Noise: Improving Image Generation for Nuclei Segmentation by Suppressing Steganography in CycleGAN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01769">http://arxiv.org/abs/2308.01769</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonas Utz, Tobias Weise, Maja Schlereth, Fabian Wagner, Mareike Thies, Mingxuan Gu, Stefan Uderhardt, Katharina Breininger</li>
<li>for: 这个论文是为了提高投影机微scopy图像中的核体分割任务而准备的。</li>
<li>methods: 这个论文使用的方法是使用CycleGAN生成器 inverse microscopy图像，并使用低通过滤波器基于DCT来移除生成图像中的隐藏快捷信息。</li>
<li>results: 这个论文的结果显示，通过移除隐藏快捷信息，可以提高生成图像和цикли图像之间的准确性，并提高核体分割任务的F1分数。<details>
<summary>Abstract</summary>
Annotating nuclei in microscopy images for the training of neural networks is a laborious task that requires expert knowledge and suffers from inter- and intra-rater variability, especially in fluorescence microscopy. Generative networks such as CycleGAN can inverse the process and generate synthetic microscopy images for a given mask, thereby building a synthetic dataset. However, past works report content inconsistencies between the mask and generated image, partially due to CycleGAN minimizing its loss by hiding shortcut information for the image reconstruction in high frequencies rather than encoding the desired image content and learning the target task. In this work, we propose to remove the hidden shortcut information, called steganography, from generated images by employing a low pass filtering based on the DCT. We show that this increases coherence between generated images and cycled masks and evaluate synthetic datasets on a downstream nuclei segmentation task. Here we achieve an improvement of 5.4 percentage points in the F1-score compared to a vanilla CycleGAN. Integrating advanced regularization techniques into the CycleGAN architecture may help mitigate steganography-related issues and produce more accurate synthetic datasets for nuclei segmentation.
</details>
<details>
<summary>摘要</summary>
描述核在微scopic图像中标注是一项劳动密集的任务，需要专业知识和受人类评估的变化，特别是在染色微scopic图像中。生成网络如CycleGAN可以反转过程，生成基于给定的mask的假图像，并建立一个假数据集。然而，过去的工作发现，生成图像中的内容与mask之间存在不一致，这部分是因为CycleGAN寻找短cut减少高频信息，而不是编码愿意图像内容和学习目标任务。在这项工作中，我们提议从生成图像中除掉隐藏的短cut信息（即steganography），使用基于DCT的低通滤波器。我们发现，这会提高生成图像和cycled mask之间的协调性，并评估假数据集在下游核 segmentation任务上的性能。在这里，我们实现了与vanilla CycleGAN相比的5.4个百分点的F1分数提升。将进降常规正则化技术integrated into CycleGAN architecture可能可以减少steganography相关的问题，生成更准确的假数据集 для核 segmentation。
</details></li>
</ul>
<hr>
<h2 id="NuInsSeg-A-Fully-Annotated-Dataset-for-Nuclei-Instance-Segmentation-in-H-E-Stained-Histological-Images"><a href="#NuInsSeg-A-Fully-Annotated-Dataset-for-Nuclei-Instance-Segmentation-in-H-E-Stained-Histological-Images" class="headerlink" title="NuInsSeg: A Fully Annotated Dataset for Nuclei Instance Segmentation in H&amp;E-Stained Histological Images"></a>NuInsSeg: A Fully Annotated Dataset for Nuclei Instance Segmentation in H&amp;E-Stained Histological Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01760">http://arxiv.org/abs/2308.01760</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/masih4/nuinsseg">https://github.com/masih4/nuinsseg</a></li>
<li>paper_authors: Amirreza Mahbod, Christine Polak, Katharina Feldmann, Rumsha Khan, Katharina Gelles, Georg Dorffner, Ramona Woitek, Sepideh Hatamikia, Isabella Ellinger</li>
<li>for:  automatic nuclei instance segmentation in whole slide image analysis</li>
<li>methods:  supervised deep learning (DL) methods with fully manually annotated datasets</li>
<li>results:  release of one of the biggest fully manually annotated datasets of nuclei in Hematoxylin and Eosin (H&amp;E)-stained histological images, called NuInsSeg, with additional ambiguous area masks for the entire dataset.<details>
<summary>Abstract</summary>
In computational pathology, automatic nuclei instance segmentation plays an essential role in whole slide image analysis. While many computerized approaches have been proposed for this task, supervised deep learning (DL) methods have shown superior segmentation performances compared to classical machine learning and image processing techniques. However, these models need fully annotated datasets for training which is challenging to acquire, especially in the medical domain. In this work, we release one of the biggest fully manually annotated datasets of nuclei in Hematoxylin and Eosin (H&E)-stained histological images, called NuInsSeg. This dataset contains 665 image patches with more than 30,000 manually segmented nuclei from 31 human and mouse organs. Moreover, for the first time, we provide additional ambiguous area masks for the entire dataset. These vague areas represent the parts of the images where precise and deterministic manual annotations are impossible, even for human experts. The dataset and detailed step-by-step instructions to generate related segmentation masks are publicly available at https://www.kaggle.com/datasets/ipateam/nuinsseg and https://github.com/masih4/NuInsSeg, respectively.
</details>
<details>
<summary>摘要</summary>
Computational pathology 中，自动核体实例分割扮演着重要的角色，整体扫描图像分析中。虽然许多计算机化的方法已经被提出，但是超级深度学习（DL）方法在这个任务中显示出了更高的分割性能，相比于经典机器学习和图像处理技术。然而，这些模型需要完全注释的数据集进行训练，特别是在医疗领域中，这是一个具有挑战性的任务。在这项工作中，我们释放了一个包含665个图像块，共计超过30,000个手动注释的核体的大数据集，称为NuInsSeg。这个数据集包括31种人类和小鼠的组织，以及每个组织的多个图像块。此外，我们还提供了整个数据集的抽象区域mask，这些抽象区域表示图像中具有不确定性的部分，即人类专家无法在这些部分提供准确的手动注释。该数据集和相关的分割步骤说明可以在https://www.kaggle.com/datasets/ipateam/nuinsseg和https://github.com/masih4/NuInsSeg中下载。
</details></li>
</ul>
<hr>
<h2 id="Reference-Free-Isotropic-3D-EM-Reconstruction-using-Diffusion-Models"><a href="#Reference-Free-Isotropic-3D-EM-Reconstruction-using-Diffusion-Models" class="headerlink" title="Reference-Free Isotropic 3D EM Reconstruction using Diffusion Models"></a>Reference-Free Isotropic 3D EM Reconstruction using Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01594">http://arxiv.org/abs/2308.01594</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kyungryun Lee, Won-Ki Jeong</li>
<li>for: 这个论文是为了解决电子顾微显微镜像中的扩散率差异问题，提高分析和下游任务的精度和效率。</li>
<li>methods: 该方法基于二维扩散模型，可以一直重建3D体积，并且适用于高下采样的数据。</li>
<li>results: 对两个公共数据集进行了广泛的实验，并证明了利用生成模型比超级vised学习方法更加稳定和有效。此外，还示出了该方法的自动化重建可能性，可以无需任何训练数据来重建单个不同的体积。<details>
<summary>Abstract</summary>
Electron microscopy (EM) images exhibit anisotropic axial resolution due to the characteristics inherent to the imaging modality, presenting challenges in analysis and downstream tasks.In this paper, we propose a diffusion-model-based framework that overcomes the limitations of requiring reference data or prior knowledge about the degradation process. Our approach utilizes 2D diffusion models to consistently reconstruct 3D volumes and is well-suited for highly downsampled data. Extensive experiments conducted on two public datasets demonstrate the robustness and superiority of leveraging the generative prior compared to supervised learning methods. Additionally, we demonstrate our method's feasibility for self-supervised reconstruction, which can restore a single anisotropic volume without any training data.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="DMDC-Dynamic-mask-based-dual-camera-design-for-snapshot-Hyperspectral-Imaging"><a href="#DMDC-Dynamic-mask-based-dual-camera-design-for-snapshot-Hyperspectral-Imaging" class="headerlink" title="DMDC: Dynamic-mask-based dual camera design for snapshot Hyperspectral Imaging"></a>DMDC: Dynamic-mask-based dual camera design for snapshot Hyperspectral Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01541">http://arxiv.org/abs/2308.01541</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/caizeyu1992/dmdc">https://github.com/caizeyu1992/dmdc</a></li>
<li>paper_authors: Zeyu Cai, Chengqian Jin, Feipeng Da</li>
<li>for: 提高coded aperture snapshot spectral imaging（CASSI）中深度学习方法的性能。</li>
<li>methods: 使用动态Mask基于双摄像头系统，首先从RGB图像中学习场景的空间特征分布，然后使用SLM编码场景，最后将RGB和CASSI图像传给网络进行重建。设计了DMDC-net，包括一个小规模的CNN基于动态Mask网络和多模式重建网络。</li>
<li>results: 对多个数据集进行了广泛的实验，结果表明我们的方法可以与最佳状态提高超过9dB的PSNR。<details>
<summary>Abstract</summary>
Deep learning methods are developing rapidly in coded aperture snapshot spectral imaging (CASSI). The number of parameters and FLOPs of existing state-of-the-art methods (SOTA) continues to increase, but the reconstruction accuracy improves slowly. Current methods still face two problems: 1) The performance of the spatial light modulator (SLM) is not fully developed due to the limitation of fixed Mask coding. 2) The single input limits the network performance. In this paper we present a dynamic-mask-based dual camera system, which consists of an RGB camera and a CASSI system running in parallel. First, the system learns the spatial feature distribution of the scene based on the RGB images, then instructs the SLM to encode each scene, and finally sends both RGB and CASSI images to the network for reconstruction. We further designed the DMDC-net, which consists of two separate networks, a small-scale CNN-based dynamic mask network for dynamic adjustment of the mask and a multimodal reconstruction network for reconstruction using RGB and CASSI measurements. Extensive experiments on multiple datasets show that our method achieves more than 9 dB improvement in PSNR over the SOTA. (https://github.com/caizeyu1992/DMDC)
</details>
<details>
<summary>摘要</summary>
深度学习方法在干扰窗口Snapshot спектраль成像（CASSI）领域不断发展。现有State-of-the-art方法（SOTA）中参数和FLOPs的数量继续增加，但重建精度变化缓慢。现有方法仍面临两个问题：1）SLM的性能尚未得到完全发挥，归因于固定的Mask编码限制。2）单输入限制网络性能。在本文中，我们提出了一种基于动态面罩的双相机系统，该系统由一个RGB相机和一个CASSI系统在并行运行。首先，系统根据RGB图像学习场景的空间特征分布，然后通过SLM编码场景，并将RGB和CASSI图像传递给网络进行重建。我们还设计了DMDC-net，它由两个独立网络组成：一个小规模的CNN基于动态面罩网络用于动态调整面罩，以及一个多模式重建网络用于使用RGB和CASSI测量进行重建。我们对多个数据集进行了广泛的实验，结果显示，我们的方法可以与SOTA比进行9dB以上的PSNR提升。（https://github.com/caizeyu1992/DMDC）
</details></li>
</ul>
<hr>
<h2 id="Numerical-Uncertainty-of-Convolutional-Neural-Networks-Inference-for-Structural-Brain-MRI-Analysis"><a href="#Numerical-Uncertainty-of-Convolutional-Neural-Networks-Inference-for-Structural-Brain-MRI-Analysis" class="headerlink" title="Numerical Uncertainty of Convolutional Neural Networks Inference for Structural Brain MRI Analysis"></a>Numerical Uncertainty of Convolutional Neural Networks Inference for Structural Brain MRI Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01939">http://arxiv.org/abs/2308.01939</a></li>
<li>repo_url: None</li>
<li>paper_authors: Inés Gonzalez Pepe, Vinuyan Sivakolunthu, Hae Lang Park, Yohan Chatelain, Tristan Glatard</li>
<li>for: 这 paper 探讨了 Convolutional Neural Networks (CNNs) 推理过程中的数字不确定性，用于Structural Brain MRI 分析。</li>
<li>methods: 这 paper 使用 Random Rounding 方法，对 CNN 模型在非线性调整 (SynthMorph) 和整个大脑 segmentation (FastSurfer) 中使用。</li>
<li>results: 结果表明，CNN 预测比传统图像处理结果更加精确（非线性调整：19 vs 13 重要位数的平均值；整个大脑 segmentation：0.99 vs 0.92 S{\o}rensen-Dice 分数的平均值），这表明 CNN 结果在执行环境中更加重复性。<details>
<summary>Abstract</summary>
This paper investigates the numerical uncertainty of Convolutional Neural Networks (CNNs) inference for structural brain MRI analysis. It applies Random Rounding -- a stochastic arithmetic technique -- to CNN models employed in non-linear registration (SynthMorph) and whole-brain segmentation (FastSurfer), and compares the resulting numerical uncertainty to the one measured in a reference image-processing pipeline (FreeSurfer recon-all). Results obtained on 32 representative subjects show that CNN predictions are substantially more accurate numerically than traditional image-processing results (non-linear registration: 19 vs 13 significant bits on average; whole-brain segmentation: 0.99 vs 0.92 S{\o}rensen-Dice score on average), which suggests a better reproducibility of CNN results across execution environments.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="TDMD-A-Database-for-Dynamic-Color-Mesh-Subjective-and-Objective-Quality-Explorations"><a href="#TDMD-A-Database-for-Dynamic-Color-Mesh-Subjective-and-Objective-Quality-Explorations" class="headerlink" title="TDMD: A Database for Dynamic Color Mesh Subjective and Objective Quality Explorations"></a>TDMD: A Database for Dynamic Color Mesh Subjective and Objective Quality Explorations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01499">http://arxiv.org/abs/2308.01499</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qi Yang, Joel Jung, Timon Deschamps, Xiaozhong Xu, Shan Liu<br>for:The paper aims to create a database of distorted dynamic colored meshes (DCMs) to study the impact of distortions on human perception and to develop objective metrics for evaluating the quality of DCMs.methods:The authors use a large-scale subjective experiment to collect mean opinion scores for 303 distorted DCM samples, and they evaluate three types of state-of-the-art objective metrics on the database.results:The authors find that different types of distortion have varying effects on human perception, and they provide recommendations for selecting objective metrics in practical DCM applications. They also make the TDMD database publicly available for future research.<details>
<summary>Abstract</summary>
Dynamic colored meshes (DCM) are widely used in various applications; however, these meshes may undergo different processes, such as compression or transmission, which can distort them and degrade their quality. To facilitate the development of objective metrics for DCMs and study the influence of typical distortions on their perception, we create the Tencent - dynamic colored mesh database (TDMD) containing eight reference DCM objects with six typical distortions. Using processed video sequences (PVS) derived from the DCM, we have conducted a large-scale subjective experiment that resulted in 303 distorted DCM samples with mean opinion scores, making the TDMD the largest available DCM database to our knowledge. This database enabled us to study the impact of different types of distortion on human perception and offer recommendations for DCM compression and related tasks. Additionally, we have evaluated three types of state-of-the-art objective metrics on the TDMD, including image-based, point-based, and video-based metrics, on the TDMD. Our experimental results highlight the strengths and weaknesses of each metric, and we provide suggestions about the selection of metrics in practical DCM applications. The TDMD will be made publicly available at the following location: https://multimedia.tencent.com/resources/tdmd.
</details>
<details>
<summary>摘要</summary>
“弹性颜色网 mesh”（DCM）在多个应用中广泛使用，但这些网可能会经历不同的处理程序，如压缩或传输，这可能会使其变形和降低质量。为了促进DCM的开发和研究，我们创建了“天聪”- dynamic colored mesh database（TDMD），包含8个参考DCM物件，以6种常见的变形作为试验材料。我们透过从DCM derivated的处理影像序列（PVS）进行了大规模的主观实验，产生了303个扭曲DCM样品，并记录了它们的主观评分，从而使TDMD成为我们所知道的最大的DCM数据库。这个数据库允许我们研究不同类型的变形对人类感知的影响，并提供了适用于DCM压缩和相关任务的建议。此外，我们也评估了三种现有的州Of-the-art物理指标，包括影像基于、点基于和影像基于的指标，在TDMD上。我们的实验结果显示了这些指标的优点和缺点，并提供了实际应用中选择指标的建议。TDMD将在以下位置公开：https://multimedia.tencent.com/resources/tdmd。”
</details></li>
</ul>
<hr>
<h2 id="Estimation-of-motion-blur-kernel-parameters-using-regression-convolutional-neural-networks"><a href="#Estimation-of-motion-blur-kernel-parameters-using-regression-convolutional-neural-networks" class="headerlink" title="Estimation of motion blur kernel parameters using regression convolutional neural networks"></a>Estimation of motion blur kernel parameters using regression convolutional neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01381">http://arxiv.org/abs/2308.01381</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luis G. Varela, Laura E. Boucheron, Steven Sandoval, David Voelz, Abu Bucker Siddik</li>
<li>for: 本文针对 Linear Motion Blur 预测方法进行提出，将 Linear Motion Blur 视为一种条件对称噪声，并使用对应的 Neural Network 进行预测。</li>
<li>methods: 本文使用的方法是基于 Neural Network 的回推预测方法，将 Linear Motion Blur 的长度和方向转换为 Parameters，并使用这些 Parameters 来预测 Linear Motion Blur 预测器。</li>
<li>results: 本文透过分析 Linear Motion Blur 的关系性，实现了对 Uniformed Motion Blur 图像的预测。这些预测结果可以帮助对 Linear Motion Blur 进行更好的处理和修复。<details>
<summary>Abstract</summary>
Many deblurring and blur kernel estimation methods use MAP or classification deep learning techniques to sharpen an image and predict the blur kernel. We propose a regression approach using neural networks to predict the parameters of linear motion blur kernels. These kernels can be parameterized by its length of blur and the orientation of the blur.This paper will analyze the relationship between length and angle of linear motion blur. This analysis will help establish a foundation to using regression prediction in uniformed motion blur images.
</details>
<details>
<summary>摘要</summary>
很多锐化和抖镜元数估计方法使用MAP或分类深度学习技术来锐化图像并预测抖镜元数。我们提议使用回归方法使用神经网络预测线性运动抖镜元数的参数。这些参数可以 Parameterized by its length of blur and the orientation of the blur。这篇论文将分析线性运动抖镜的关系，以建立基础 для使用回归预测在均匀运动抖镜图像中。
</details></li>
</ul>
<hr>
<h2 id="ELIXR-Towards-a-general-purpose-X-ray-artificial-intelligence-system-through-alignment-of-large-language-models-and-radiology-vision-encoders"><a href="#ELIXR-Towards-a-general-purpose-X-ray-artificial-intelligence-system-through-alignment-of-large-language-models-and-radiology-vision-encoders" class="headerlink" title="ELIXR: Towards a general purpose X-ray artificial intelligence system through alignment of large language models and radiology vision encoders"></a>ELIXR: Towards a general purpose X-ray artificial intelligence system through alignment of large language models and radiology vision encoders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01317">http://arxiv.org/abs/2308.01317</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shawn Xu, Lin Yang, Christopher Kelly, Marcin Sieniek, Timo Kohlberger, Martin Ma, Wei-Hung Weng, Attila Kiraly, Sahar Kazemzadeh, Zakkai Melamed, Jungyeon Park, Patricia Strachan, Yun Liu, Chuck Lau, Preeti Singh, Christina Chen, Mozziyar Etemadi, Sreenivasa Raju Kalidindi, Yossi Matias, Katherine Chou, Greg S. Corrado, Shravya Shetty, Daniel Tse, Shruthi Prabhakara, Daniel Golden, Rory Pilgrim, Krish Eswaran, Andrew Sellergren</li>
<li>for: 这个研究旨在开发一个名为ELIXR的语言&#x2F;图像测合模型，用于进行各种医疗影像处理任务。</li>
<li>methods: 这个模型使用了一个语言aligned图像Encoder，与固定的LLM PaLM 2结合，并通过对MIMIC-CXR dataset中的图像和相应的自由文本医疗报告进行训练。</li>
<li>results: ELIXR在零扩展骨臓X射线（CXR）分类（平均AUC为0.850）、数据效率CXR分类（平均AUCs为0.893和0.898）以及semantic搜寻（NDCG为0.76）等方面均 achieve state-of-the-art performance，并且在医疗影像语言任务中也表现良好。<details>
<summary>Abstract</summary>
Our approach, which we call Embeddings for Language/Image-aligned X-Rays, or ELIXR, leverages a language-aligned image encoder combined or grafted onto a fixed LLM, PaLM 2, to perform a broad range of tasks. We train this lightweight adapter architecture using images paired with corresponding free-text radiology reports from the MIMIC-CXR dataset. ELIXR achieved state-of-the-art performance on zero-shot chest X-ray (CXR) classification (mean AUC of 0.850 across 13 findings), data-efficient CXR classification (mean AUCs of 0.893 and 0.898 across five findings (atelectasis, cardiomegaly, consolidation, pleural effusion, and pulmonary edema) for 1% (~2,200 images) and 10% (~22,000 images) training data), and semantic search (0.76 normalized discounted cumulative gain (NDCG) across nineteen queries, including perfect retrieval on twelve of them). Compared to existing data-efficient methods including supervised contrastive learning (SupCon), ELIXR required two orders of magnitude less data to reach similar performance. ELIXR also showed promise on CXR vision-language tasks, demonstrating overall accuracies of 58.7% and 62.5% on visual question answering and report quality assurance tasks, respectively. These results suggest that ELIXR is a robust and versatile approach to CXR AI.
</details>
<details>
<summary>摘要</summary>
我们的方法，我们称之为语言/图像aligned X-射线（ELIXR），利用一个语言对应的图像编码器与固定的LLM（PaLM 2）结合，以实现广泛的任务。我们在使用图像和相应的自由文本 radiology report 从 MIMIC-CXR 数据集进行训练这个轻量级适配器建筑。ELIXR 在零 shot 胸部 X-射线（CXR）分类中获得了状态机器人表现（平均 AUC 为 0.850  across 13 个发现），数据效率 CXR 分类（平均 AUCs 为 0.893 和 0.898  across 五 个发现（肿瘤、心脏 hypertrophy、填充、肿水和肺水肿） for 1% （约 2,200 张图像）和 10% （约 22,000 张图像） 训练数据），以及semantic search（NDCG 为 0.76）。相比现有的数据效率方法，包括 supervised contrastive learning （SupCon），ELIXR 需要两个数据效率下来到达相似的性能。ELIXR 还在 CXR vision-language 任务上显示了抢器，得到了 58.7% 和 62.5% 的总准确率在视觉问答和报告质量答案任务中。这些结果表明 ELIXR 是一种强大和多功能的 CXR AI 方法。
</details></li>
</ul>
<hr>
<h2 id="A-vision-transformer-based-framework-for-knowledge-transfer-from-multi-modal-to-mono-modal-lymphoma-subtyping-models"><a href="#A-vision-transformer-based-framework-for-knowledge-transfer-from-multi-modal-to-mono-modal-lymphoma-subtyping-models" class="headerlink" title="A vision transformer-based framework for knowledge transfer from multi-modal to mono-modal lymphoma subtyping models"></a>A vision transformer-based framework for knowledge transfer from multi-modal to mono-modal lymphoma subtyping models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01328">http://arxiv.org/abs/2308.01328</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bilel Guetarni, Feryal Windal, Halim Benhabiles, Marianne Petit, Romain Dubois, Emmanuelle Leteurtre, Dominique Collard</li>
<li>for: 这个论文的目的是为了提出一种基于整体板准的DLBCL悬浮性诊断方法，以提高癌症患者的生存机会。</li>
<li>methods: 该方法使用了整体板准图像分析技术，并利用深度学习模型来预测DLBCL悬浮性诊断结果。</li>
<li>results: 实验研究表明，该方法可以高效地识别DLBCL悬浮性诊断结果，并且比现有的六种方法更高效。此外，研究还发现，为了达到同等诊断精度，该方法只需要训练一个单模态分类器。<details>
<summary>Abstract</summary>
Determining lymphoma subtypes is a crucial step for better patients treatment targeting to potentially increase their survival chances. In this context, the existing gold standard diagnosis method, which is based on gene expression technology, is highly expensive and time-consuming making difficult its accessibility. Although alternative diagnosis methods based on IHC (immunohistochemistry) technologies exist (recommended by the WHO), they still suffer from similar limitations and are less accurate. WSI (Whole Slide Image) analysis by deep learning models showed promising new directions for cancer diagnosis that would be cheaper and faster than existing alternative methods. In this work, we propose a vision transformer-based framework for distinguishing DLBCL (Diffuse Large B-Cell Lymphoma) cancer subtypes from high-resolution WSIs. To this end, we propose a multi-modal architecture to train a classifier model from various WSI modalities. We then exploit this model through a knowledge distillation mechanism for efficiently driving the learning of a mono-modal classifier. Our experimental study conducted on a dataset of 157 patients shows the promising performance of our mono-modal classification model, outperforming six recent methods from the state-of-the-art dedicated for cancer classification. Moreover, the power-law curve, estimated on our experimental data, shows that our classification model requires a reasonable number of additional patients for its training to potentially reach identical diagnosis accuracy as IHC technologies.
</details>
<details>
<summary>摘要</summary>
确定淋巴瘤Subtype是诊断患者的关键Step，以提高治疗效果和存活率。然而，现有的金标准诊断方法，基于基因表达技术，是非常昂贵和耗时的，使得它的可accessibility受限。尽管现有的诊断方法基于IHC（免疫组织化学）技术存在，但它们仍然受到相同的限制，并且较为不准确。WSI（整个报告图像）分析方法，基于深入学习模型，显示了新的可能性 Directions for cancer diagnosis，这些方法比现有的替代方法更加便宜和快速。在这项工作中，我们提出了一个视Transformer基于的框架，用于从高分辨率WSIs中分类Diffuse Large B-Cell Lymphoma（淋巴瘤）亚型。为此，我们提出了一种多模态建立的建议模型，用于训练分类模型。然后，我们利用这个模型的知识塑化机制，以有效地驱动模型的学习。我们的实验研究在157名病人的数据集上进行，显示我们的单模态分类模型在诊断方面表现出色，超过了过去六个最新的投入于cancer分类的方法。此外，我们对实验数据进行了力级曲线的估计，显示我们的分类模型需要一个合理的数量的更多patients的训练，以达到与IHC技术相同的诊断准确率。
</details></li>
</ul>
<hr>
<h2 id="Incorporating-Season-and-Solar-Specificity-into-Renderings-made-by-a-NeRF-Architecture-using-Satellite-Images"><a href="#Incorporating-Season-and-Solar-Specificity-into-Renderings-made-by-a-NeRF-Architecture-using-Satellite-Images" class="headerlink" title="Incorporating Season and Solar Specificity into Renderings made by a NeRF Architecture using Satellite Images"></a>Incorporating Season and Solar Specificity into Renderings made by a NeRF Architecture using Satellite Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01262">http://arxiv.org/abs/2308.01262</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/enterprisecv-6/season-nerf">https://github.com/enterprisecv-6/season-nerf</a></li>
<li>paper_authors: Michael Gableman, Avinash Kak</li>
<li>for: 可以使用卫星图像进行训练，在基于NeRF的框架中，根据季节 angle 渲染场景从不同的视角。</li>
<li>methods: 我们的工作extend了Shadow NeRF和Sat-NeRF的贡献，并显示了如何使得渲染得到季节特征。我们引入了一个时间变量，以教育我们的网络渲染季节特征，并且仍然能够渲染阴影。但是，卫星图像的小训练集可能会导致在同一个季节中的阴影存在相同的位置，这会引入歧义。我们添加了额外的损失函数项，以避免网络使用季节特征来补做阴影。</li>
<li>results: 我们在八个Area of Interest中进行了评估，包括测试框架能够准确渲染新视角、生成高度图、预测阴影、独立地考虑季节特征。我们的ablation study表明了我们的网络设计参数的选择是合理的。<details>
<summary>Abstract</summary>
As a result of Shadow NeRF and Sat-NeRF, it is possible to take the solar angle into account in a NeRF-based framework for rendering a scene from a novel viewpoint using satellite images for training. Our work extends those contributions and shows how one can make the renderings season-specific. Our main challenge was creating a Neural Radiance Field (NeRF) that could render seasonal features independently of viewing angle and solar angle while still being able to render shadows. We teach our network to render seasonal features by introducing one more input variable -- time of the year. However, the small training datasets typical of satellite imagery can introduce ambiguities in cases where shadows are present in the same location for every image of a particular season. We add additional terms to the loss function to discourage the network from using seasonal features for accounting for shadows. We show the performance of our network on eight Areas of Interest containing images captured by the Maxar WorldView-3 satellite. This evaluation includes tests measuring the ability of our framework to accurately render novel views, generate height maps, predict shadows, and specify seasonal features independently from shadows. Our ablation studies justify the choices made for network design parameters.
</details>
<details>
<summary>摘要</summary>
因为阴影NeRF和Sat-NeRF，可以在基于NeRF框架中考虑太阳角度，使用卫星图像进行训练，并在不同的视角和太阳角度下渲染场景。我们的工作扩展了这些贡献，并显示了如何使渲染季节特征独立于视角和太阳角度。我们教育我们的神经辐射场（NeRF）如何独立地渲染季节特征，而不是通过视角和太阳角度来覆盖阴影。我们添加了额外的损失函数项来防止神经网络使用季节特征来覆盖阴影。我们在八个区域 интерес中测试了我们的框架，包括测试框架能够准确渲染新视图、生成高程图像、预测阴影和独立地渲染季节特征。我们的剥削研究证明了我们的网络设计参数的选择。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://example.com/2023/08/03/eess.IV_2023_08_03/" data-id="cllsjvzeg008uf5888wr79igs" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_02" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/02/cs.LG_2023_08_02/" class="article-date">
  <time datetime="2023-08-01T16:00:00.000Z" itemprop="datePublished">2023-08-02</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/02/cs.LG_2023_08_02/">cs.LG - 2023-08-02 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Careful-Whisper-–-leveraging-advances-in-automatic-speech-recognition-for-robust-and-interpretable-aphasia-subtype-classification"><a href="#Careful-Whisper-–-leveraging-advances-in-automatic-speech-recognition-for-robust-and-interpretable-aphasia-subtype-classification" class="headerlink" title="Careful Whisper – leveraging advances in automatic speech recognition for robust and interpretable aphasia subtype classification"></a>Careful Whisper – leveraging advances in automatic speech recognition for robust and interpretable aphasia subtype classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01327">http://arxiv.org/abs/2308.01327</a></li>
<li>repo_url: None</li>
<li>paper_authors: Laurin Wagner, Mario Zusag, Theresa Bloder</li>
<li>for: 这篇论文旨在自动从语音录音中识别speech anomalies，以便评估语音障碍。</li>
<li>methods: 该方法结合了Connectionist Temporal Classification（CTC）和encoder-decoder型自动语音识别模型，生成了丰富的听音和清晰的字幕。然后，通过应用多种自然语言处理方法，提取了字幕中的特征，生成了健康语音的原型。</li>
<li>results: 通过使用这些原型的基本距离度量，可以通过标准机器学习分类器实现人类水平的准确率，并能够正确地分类出常见的语音障碍类型。此外，该管道可以直接应用于其他疾病和语言，表现出抽象找出语音生物标志的可能性。<details>
<summary>Abstract</summary>
This paper presents a fully automated approach for identifying speech anomalies from voice recordings to aid in the assessment of speech impairments. By combining Connectionist Temporal Classification (CTC) and encoder-decoder-based automatic speech recognition models, we generate rich acoustic and clean transcripts. We then apply several natural language processing methods to extract features from these transcripts to produce prototypes of healthy speech. Basic distance measures from these prototypes serve as input features for standard machine learning classifiers, yielding human-level accuracy for the distinction between recordings of people with aphasia and a healthy control group. Furthermore, the most frequently occurring aphasia types can be distinguished with 90% accuracy. The pipeline is directly applicable to other diseases and languages, showing promise for robustly extracting diagnostic speech biomarkers.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种完全自动化的方法，用于从语音录音中识别语音异常，以 помочь评估语音障碍。通过结合连接主义时间分类（CTC）和编解码器基于自动语音识别模型，我们生成了丰富的声学特征和清晰的字幕。然后，我们应用了一些自然语言处理方法，从这些字幕中提取特征，生成健康语音的原型。这些原型的基本距离测量作为输入特征，用标准机器学习分类器进行识别，达到了人类水平的准确率，用于分别记录了人类Language和健康控制组的录音。此外，最常出现的语言障碍类型也可以达到90%的准确率。这个管道直接适用于其他疾病和语言，表现出抗颤异常抽取 диагностических语音标记的可能性。
</details></li>
</ul>
<hr>
<h2 id="Do-Multilingual-Language-Models-Think-Better-in-English"><a href="#Do-Multilingual-Language-Models-Think-Better-in-English" class="headerlink" title="Do Multilingual Language Models Think Better in English?"></a>Do Multilingual Language Models Think Better in English?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01223">http://arxiv.org/abs/2308.01223</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/juletx/self-translate">https://github.com/juletx/self-translate</a></li>
<li>paper_authors: Julen Etxaniz, Gorka Azkune, Aitor Soroa, Oier Lopez de Lacalle, Mikel Artetxe</li>
<li>for: 提高多语言模型的性能</li>
<li>methods: 利用多语言模型内置的几个shot翻译能力</li>
<li>results: 自翻译方法比直接推理更高效，证明语言模型在非英语语言下的潜在多语言能力未能得到充分利用。<details>
<summary>Abstract</summary>
Translate-test is a popular technique to improve the performance of multilingual language models. This approach works by translating the input into English using an external machine translation system, and running inference over the translated input. However, these improvements can be attributed to the use of a separate translation system, which is typically trained on large amounts of parallel data not seen by the language model. In this work, we introduce a new approach called self-translate, which overcomes the need of an external translation system by leveraging the few-shot translation capabilities of multilingual language models. Experiments over 5 tasks show that self-translate consistently outperforms direct inference, demonstrating that language models are unable to leverage their full multilingual potential when prompted in non-English languages. Our code is available at https://github.com/juletx/self-translate.
</details>
<details>
<summary>摘要</summary>
自 traducción-test 是一种受欢迎的技术，用于提高多语言语音模型的性能。这种方法工作如下：将输入翻译成英语使用外部机器翻译系统，然后运行推理过程。然而，这些改进可以归因于使用外部翻译系统，这些系统通常是基于大量并行数据，这些数据不是语言模型所见的。在这种工作中，我们介绍了一种新的方法，即自翻译，可以超越外部翻译系统的需求。我们的实验结果表明，自翻译在 5 个任务上一直表现出色，常常超过直接推理，这表明了语言模型在非英语语言下的推理能力尚未得到完全利用。我们的代码可以在 GitHub 上找到：https://github.com/juletx/self-translate。
</details></li>
</ul>
<hr>
<h2 id="Calibration-in-Deep-Learning-A-Survey-of-the-State-of-the-Art"><a href="#Calibration-in-Deep-Learning-A-Survey-of-the-State-of-the-Art" class="headerlink" title="Calibration in Deep Learning: A Survey of the State-of-the-Art"></a>Calibration in Deep Learning: A Survey of the State-of-the-Art</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01222">http://arxiv.org/abs/2308.01222</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheng Wang</li>
<li>for: 这篇论文主要探讨了深度神经网络模型的准确性和可靠性问题，以及如何使用不同机制来进行模型准确性的调整。</li>
<li>methods: 本文主要介绍了现有的模型准确性调整方法，可以分为四个类别：后期准确化、regularization方法、uncertainty估计和组合方法。</li>
<li>results: 本文提供了一个系统性的对现有准确性调整方法的介绍，并讨论了一些最新的进展和挑战在大型模型（特别是大语言模型）的准确性调整方面。<details>
<summary>Abstract</summary>
Calibrating deep neural models plays an important role in building reliable, robust AI systems in safety-critical applications. Recent work has shown that modern neural networks that possess high predictive capability are poorly calibrated and produce unreliable model predictions. Though deep learning models achieve remarkable performance on various benchmarks, the study of model calibration and reliability is relatively underexplored. Ideal deep models should have not only high predictive performance but also be well calibrated. There have been some recent methods proposed to calibrate deep models by using different mechanisms. In this survey, we review the state-of-the-art calibration methods and provide an understanding of their principles for performing model calibration. First, we start with the definition of model calibration and explain the root causes of model miscalibration. Then we introduce the key metrics that can measure this aspect. It is followed by a summary of calibration methods that we roughly classified into four categories: post-hoc calibration, regularization methods, uncertainty estimation, and composition methods. We also covered some recent advancements in calibrating large models, particularly large language models (LLMs). Finally, we discuss some open issues, challenges, and potential directions.
</details>
<details>
<summary>摘要</summary>
<<SYS>Translate into Simplified Chinese.</SYS>>深度神经网络的准确性评估在安全关键应用中扮演着重要的角色。现代神经网络具有高度预测能力，但是它们的准确性受到诸如模型误差等因素的影响。深度学习模型在不同的benchmark上实现了卓越的表现，但是对模型准确性和可靠性的研究还是相对不充分。理想的深度模型应该具有高度预测性能和准确性。在这篇评论中，我们将对当前领域的state-of-the-art准确方法进行回顾，并提供这些方法的原理的理解。首先，我们将定义模型准确性的定义，并解释模型误差的根本原因。然后我们会介绍用于评估这个方面的关键度量。接着，我们将概括一些准确方法，分为四类：后期准确、正则化方法、uncertainty估计和组合方法。我们还讲解了一些在准确大模型中的最新进展，特别是大语言模型（LLMs）。最后，我们讨论了一些开放的问题、挑战和可能的方向。
</details></li>
</ul>
<hr>
<h2 id="Using-ScrutinAI-for-Visual-Inspection-of-DNN-Performance-in-a-Medical-Use-Case"><a href="#Using-ScrutinAI-for-Visual-Inspection-of-DNN-Performance-in-a-Medical-Use-Case" class="headerlink" title="Using ScrutinAI for Visual Inspection of DNN Performance in a Medical Use Case"></a>Using ScrutinAI for Visual Inspection of DNN Performance in a Medical Use Case</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01220">http://arxiv.org/abs/2308.01220</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rebekka Görge, Elena Haedecke, Michael Mock</li>
<li>for: 这篇论文旨在提高人工分析者通过可视化工具ScrutinAI来调查模型性能和数据集。</li>
<li>methods: 该论文使用了ScrutinAI工具来分析不同专家对数据集的标签变化对模型性能的影响。</li>
<li>results: 该研究发现了模型性能受标签质量变化的影响，并通过Root Cause Analysis分析了DNN模型中真正的弱点。<details>
<summary>Abstract</summary>
Our Visual Analytics (VA) tool ScrutinAI supports human analysts to investigate interactively model performanceand data sets. Model performance depends on labeling quality to a large extent. In particular in medical settings, generation of high quality labels requires in depth expert knowledge and is very costly. Often, data sets are labeled by collecting opinions of groups of experts. We use our VA tool to analyse the influence of label variations between different experts on the model performance. ScrutinAI facilitates to perform a root cause analysis that distinguishes weaknesses of deep neural network (DNN) models caused by varying or missing labeling quality from true weaknesses. We scrutinize the overall detection of intracranial hemorrhages and the more subtle differentiation between subtypes in a publicly available data set.
</details>
<details>
<summary>摘要</summary>
我们的视觉分析工具ScrutinAI可以帮助人类分析员 Investigate model性能和数据集的交互方式。模型性能受标签质量影响很大。尤其在医疗设置下，生成高质量标签需要深厚的专家知识和很昂贵。经常，数据集被由多个专家的意见收集标签。我们使用ScrutinAI来分析标签变化 между不同专家对模型性能的影响。ScrutinAI可以进行根本原因分析，从而分化true weaknesses和变化或缺失标签质量导致的深度神经网络模型的弱点。我们在一个公共可用数据集中分析了脑内出血的总检测和更加细致的类型差异。
</details></li>
</ul>
<hr>
<h2 id="Global-Hierarchical-Neural-Networks-using-Hierarchical-Softmax"><a href="#Global-Hierarchical-Neural-Networks-using-Hierarchical-Softmax" class="headerlink" title="Global Hierarchical Neural Networks using Hierarchical Softmax"></a>Global Hierarchical Neural Networks using Hierarchical Softmax</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01210">http://arxiv.org/abs/2308.01210</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jschuurmans/hsoftmax">https://github.com/jschuurmans/hsoftmax</a></li>
<li>paper_authors: Jetze Schuurmans, Flavius Frasincar</li>
<li>for: 这 paper 提出了一种基于层次softmax的全球分类器框架，适用于任何具有自然层次结构的分类任务。</li>
<li>methods: 该方法使用层次softmax创建全球分类器，并在四个文本分类数据集上进行了实验，并在所有数据集中超过了常见的平铺softmax在macro-F1和macro-recall上的表现。</li>
<li>results: 在三个数据集中，层次softmax达到了更高的微精度和macro-精度。<details>
<summary>Abstract</summary>
This paper presents a framework in which hierarchical softmax is used to create a global hierarchical classifier. The approach is applicable for any classification task where there is a natural hierarchy among classes. We show empirical results on four text classification datasets. In all datasets the hierarchical softmax improved on the regular softmax used in a flat classifier in terms of macro-F1 and macro-recall. In three out of four datasets hierarchical softmax achieved a higher micro-accuracy and macro-precision.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Generative-Noisy-Label-Learning-by-Implicit-Dicriminative-Approximation-with-Partial-Label-Prior"><a href="#Generative-Noisy-Label-Learning-by-Implicit-Dicriminative-Approximation-with-Partial-Label-Prior" class="headerlink" title="Generative Noisy-Label Learning by Implicit Dicriminative Approximation with Partial Label Prior"></a>Generative Noisy-Label Learning by Implicit Dicriminative Approximation with Partial Label Prior</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01184">http://arxiv.org/abs/2308.01184</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fengbei Liu, Yuanhong Chen, Chong Wang, Yuyuan Liu, Gustavo Carneiro</li>
<li>for: 本 paper 是Addressing Learning with Noisy Labels 的一种新方法，它提供了一种新的生成模型优化方法，以及一种新的有用的标签假设。</li>
<li>methods: 本 paper 使用了一种新的生成模型，其中使用了一种基于推理的权重学习方法，以及一种基于partial label learning的标签假设。</li>
<li>results: 本 paper 的实验结果表明，使用该新方法可以在不同的噪音标签 benchmark 上达到state-of-the-art的结果，而且与传统的推理模型相比，该方法具有类似的计算复杂度。<details>
<summary>Abstract</summary>
The learning with noisy labels has been addressed with both discriminative and generative models. Although discriminative models have dominated the field due to their simpler modeling and more efficient computational training processes, generative models offer a more effective means of disentangling clean and noisy labels and improving the estimation of the label transition matrix. However, generative approaches maximize the joint likelihood of noisy labels and data using a complex formulation that only indirectly optimizes the model of interest associating data and clean labels. Additionally, these approaches rely on generative models that are challenging to train and tend to use uninformative clean label priors. In this paper, we propose a new generative noisy-label learning approach that addresses these three issues. First, we propose a new model optimisation that directly associates data and clean labels. Second, the generative model is implicitly estimated using a discriminative model, eliminating the inefficient training of a generative model. Third, we propose a new informative label prior inspired by partial label learning as supervision signal for noisy label learning. Extensive experiments on several noisy-label benchmarks demonstrate that our generative model provides state-of-the-art results while maintaining a similar computational complexity as discriminative models.
</details>
<details>
<summary>摘要</summary>
学习含杂标签问题已经由权重分配和生成模型来解决。虽然权重分配模型在场景中占据主导地位，因为它们的模型化更加简单，并且在训练过程中更加高效，但生成模型可以更好地分离干净标签和含杂标签，并改善标签过渡矩阵的估计。然而，生成方法需要使用复杂的形式来最大化含杂标签和数据的联合概率函数，这会间接地优化模型的关注点。此外，这些方法通常需要训练困难的生成模型，并使用无用的干净标签辅助信号。在这篇论文中，我们提出了一种新的生成含杂标签学习方法，解决了这三个问题。首先，我们提出了一种直接关联数据和干净标签的模型优化方法。其次，我们使用权重分配模型来隐式地估计生成模型，从而消除了生成模型的不必要培训。最后，我们提出了一种基于部分标签学习的新备注标签监督信号。我们在多个含杂标签 benchmark 进行了广泛的实验，结果显示，我们的生成模型可以在计算复杂性方面与权重分配模型相当，同时提供了最佳的结果。
</details></li>
</ul>
<hr>
<h2 id="Direct-Gradient-Temporal-Difference-Learning"><a href="#Direct-Gradient-Temporal-Difference-Learning" class="headerlink" title="Direct Gradient Temporal Difference Learning"></a>Direct Gradient Temporal Difference Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01170">http://arxiv.org/abs/2308.01170</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaochi Qian, Shangtong Zhang</li>
<li>for: 解决强化学习（RL）中的稳定性问题，即“三难kill”问题，使RL agent可以对不被执行的策略进行反思。</li>
<li>methods: 提出了一种直接使用两个样本在马可夫链数据流中进行解决双样本问题的方法，从而解决了GTD的额外Weight问题，而不需要额外的负担。</li>
<li>results: 提供了对数据流中的Markov链进行分析，并证明了该算法与 canonical on-policy  temporal difference learning的吞吐量相同，但是具有更高的计算效率和更低的内存占用。<details>
<summary>Abstract</summary>
Off-policy learning enables a reinforcement learning (RL) agent to reason counterfactually about policies that are not executed and is one of the most important ideas in RL. It, however, can lead to instability when combined with function approximation and bootstrapping, two arguably indispensable ingredients for large-scale reinforcement learning. This is the notorious deadly triad. Gradient Temporal Difference (GTD) is one powerful tool to solve the deadly triad. Its success results from solving a doubling sampling issue indirectly with weight duplication or Fenchel duality. In this paper, we instead propose a direct method to solve the double sampling issue by simply using two samples in a Markovian data stream with an increasing gap. The resulting algorithm is as computationally efficient as GTD but gets rid of GTD's extra weights. The only price we pay is a logarithmically increasing memory as time progresses. We provide both asymptotic and finite sample analysis, where the convergence rate is on-par with the canonical on-policy temporal difference learning. Key to our analysis is a novel refined discretization of limiting ODEs.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本为简化中文。<</SYS>>掌握偏离策略的学习（RL）Agent可以对不执行的策略进行反思，是RL最重要的一个想法。然而，它可能会导致不稳定性，当与函数approximation和启动器（bootstrapping）相结合时，这些都是大规模RL中不可或缺的两个重要成分。这被称为“死亡三角形”。梯度时间差（GTD）是一种强大的解决方案，它通过解决对折 sampling 问题来 indirectly 使用重复或Fenchel duality。在这篇文章中，我们则提出了直接解决对折 sampling 问题的方法，通过在Markovian数据流中使用两个样本，并随着时间的增加，间隔逐渐增加。这种算法的计算效率与GTD相同，但是没有额外的 weights。我们只需要付出一个对时间的增长有限的logarithmic 的内存占用。我们提供了对数学分析和finite sample分析，其 converge 速率与标准的在政策上的 temporal difference 学习相同。关键在于我们的分析中使用了一种新的精细的限制ODEs的拟合。
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-Based-Diabetes-Detection-Using-Photoplethysmography-Signal-Features"><a href="#Machine-Learning-Based-Diabetes-Detection-Using-Photoplethysmography-Signal-Features" class="headerlink" title="Machine Learning-Based Diabetes Detection Using Photoplethysmography Signal Features"></a>Machine Learning-Based Diabetes Detection Using Photoplethysmography Signal Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01930">http://arxiv.org/abs/2308.01930</a></li>
<li>repo_url: None</li>
<li>paper_authors: Filipe A. C. Oliveira, Felipe M. Dias, Marcelo A. F. Toledo, Diego A. C. Cardenas, Douglas A. Almeida, Estela Ribeiro, Jose E. Krieger, Marco A. Gutierrez</li>
<li>for: 这个研究旨在开发一种非侵入性的光学脉冲测量技术，以检测糖尿病。</li>
<li>methods: 本研究使用了非侵入性的光学脉冲测量技术，并使用了条件调教的类别学习算法，包括Logistic Regression和eXtreme Gradient Boosting。</li>
<li>results: 研究获得了一个F1-Score和AUC的值为58.8±20.0%和79.2±15.0%，表明这种非侵入性的光学脉冲测量技术可以实现糖尿病的检测和预防。<details>
<summary>Abstract</summary>
Diabetes is a prevalent chronic condition that compromises the health of millions of people worldwide. Minimally invasive methods are needed to prevent and control diabetes but most devices for measuring glucose levels are invasive and not amenable for continuous monitoring. Here, we present an alternative method to overcome these shortcomings based on non-invasive optical photoplethysmography (PPG) for detecting diabetes. We classify non-Diabetic and Diabetic patients using the PPG signal and metadata for training Logistic Regression (LR) and eXtreme Gradient Boosting (XGBoost) algorithms. We used PPG signals from a publicly available dataset. To prevent overfitting, we divided the data into five folds for cross-validation. By ensuring that patients in the training set are not in the testing set, the model's performance can be evaluated on unseen subjects' data, providing a more accurate assessment of its generalization. Our model achieved an F1-Score and AUC of $58.8\pm20.0\%$ and $79.2\pm15.0\%$ for LR and $51.7\pm16.5\%$ and $73.6\pm17.0\%$ for XGBoost, respectively. Feature analysis suggested that PPG morphological features contains diabetes-related information alongside metadata. Our findings are within the same range reported in the literature, indicating that machine learning methods are promising for developing remote, non-invasive, and continuous measurement devices for detecting and preventing diabetes.
</details>
<details>
<summary>摘要</summary>
diabetes 是一种流行的慢性疾病，影响全球数百万人的健康。 however， most devices for measuring glucose levels are invasive and not suitable for continuous monitoring. 在这种情况下，我们提出了一种 alternatively method， based on non-invasive optical photoplethysmography (PPG) for detecting diabetes. we use PPG signal and metadata to train logistic regression (LR) and eXtreme gradient boosting (XGBoost) algorithms to classify non-diabetic and diabetic patients.we used PPG signals from a publicly available dataset and divided the data into five folds for cross-validation to prevent overfitting. our model achieved an F1-score and AUC of $58.8\pm20.0\%$ and $79.2\pm15.0\%$ for LR and $51.7\pm16.5\%$ and $73.6\pm17.0\%$ for XGBoost, respectively. feature analysis suggested that PPG morphological features contain diabetes-related information alongside metadata. our findings are within the same range reported in the literature, indicating that machine learning methods are promising for developing remote, non-invasive, and continuous measurement devices for detecting and preventing diabetes.
</details></li>
</ul>
<hr>
<h2 id="LLMs-Understand-Glass-Box-Models-Discover-Surprises-and-Suggest-Repairs"><a href="#LLMs-Understand-Glass-Box-Models-Discover-Surprises-and-Suggest-Repairs" class="headerlink" title="LLMs Understand Glass-Box Models, Discover Surprises, and Suggest Repairs"></a>LLMs Understand Glass-Box Models, Discover Surprises, and Suggest Repairs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01157">http://arxiv.org/abs/2308.01157</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/interpretml/talktoebm">https://github.com/interpretml/talktoebm</a></li>
<li>paper_authors: Benjamin J. Lengerich, Sebastian Bordt, Harsha Nori, Mark E. Nunnally, Yin Aphinyanaphongs, Manolis Kellis, Rich Caruana</li>
<li>for: 这篇论文主要是为了探讨大语言模型（LLMs）如何与可解释模型（ interpretable models）结合使用，以实现数据科学中一些常见任务的自动化。</li>
<li>methods: 论文使用了一种层次 reasoning 方法，使得 LLMS 可以根据自己的背景知识来自动完成数据科学中的一些任务，如检测异常、描述异常的原因和修复异常。</li>
<li>results: 论文通过多个医疗实例示例，展示了 LLMS 在使用 GAMs 时的有用性，并提供了一个开源的 LLM-GAM 接口 $\texttt{TalkToEBM}$。<details>
<summary>Abstract</summary>
We show that large language models (LLMs) are remarkably good at working with interpretable models that decompose complex outcomes into univariate graph-represented components. By adopting a hierarchical approach to reasoning, LLMs can provide comprehensive model-level summaries without ever requiring the entire model to fit in context. This approach enables LLMs to apply their extensive background knowledge to automate common tasks in data science such as detecting anomalies that contradict prior knowledge, describing potential reasons for the anomalies, and suggesting repairs that would remove the anomalies. We use multiple examples in healthcare to demonstrate the utility of these new capabilities of LLMs, with particular emphasis on Generalized Additive Models (GAMs). Finally, we present the package $\texttt{TalkToEBM}$ as an open-source LLM-GAM interface.
</details>
<details>
<summary>摘要</summary>
我们证明大型语言模型（LLM）具有极其出色的可解释性，可以将复杂的结果拆分为单一图表示的分成部分。通过预设的层级方法来理解，LLM可以提供完整的模型水平摘要，不需要整个模型适应上下文。这种方法允许LLM将它们广泛的背景知识应用到自动化资料科学中的常见任务，例如检测过去知识背景下的偏差、描述偏差的可能原因，以及修复偏差所需的修补措施。我们使用了多个预𫓹例子，尤其是通用加性模型（GAMs），以示出这些新的LLM功能的用用。最后，我们提出了一个名为$\texttt{TalkToEBM}$的开源LLM-GAM界面。
</details></li>
</ul>
<hr>
<h2 id="A-Transformer-based-Prediction-Method-for-Depth-of-Anesthesia-During-Target-controlled-Infusion-of-Propofol-and-Remifentanil"><a href="#A-Transformer-based-Prediction-Method-for-Depth-of-Anesthesia-During-Target-controlled-Infusion-of-Propofol-and-Remifentanil" class="headerlink" title="A Transformer-based Prediction Method for Depth of Anesthesia During Target-controlled Infusion of Propofol and Remifentanil"></a>A Transformer-based Prediction Method for Depth of Anesthesia During Target-controlled Infusion of Propofol and Remifentanil</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01929">http://arxiv.org/abs/2308.01929</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/heeeyk/transformer-doa-prediction">https://github.com/heeeyk/transformer-doa-prediction</a></li>
<li>paper_authors: Yongkang He, Siyuan Peng, Mingjin Chen, Zhijing Yang, Yuanhui Chen</li>
<li>for: 预测麻醉效果的准确性是脊梁控制输液系统的关键。传统的PK-PD模型 для Bispectral index（BIS）预测需要手动选择模型参数，这可能在临床设置中具有挑战性。最近提出的深度学习方法可能只能捕捉总趋势，而不能预测急变的BIS。</li>
<li>methods: 我们提议使用 transformer 网络来预测麻醉深度（DOA），使用 propofol 和 remifentanil 的药液输液。我们的方法使用 LSTM 和 GRN 网络来提高特征融合的效率，并应用了注意力机制来探索药物之间的互动。我们还使用标签分布平滑和重新分配损失来解决数据不均衡问题。</li>
<li>results: 我们的提议方法比传统的PK-PD模型和先前的深度学习方法更高效，能够有效地预测麻醉深度在急变和深度麻醉条件下。<details>
<summary>Abstract</summary>
Accurately predicting anesthetic effects is essential for target-controlled infusion systems. The traditional (PK-PD) models for Bispectral index (BIS) prediction require manual selection of model parameters, which can be challenging in clinical settings. Recently proposed deep learning methods can only capture general trends and may not predict abrupt changes in BIS. To address these issues, we propose a transformer-based method for predicting the depth of anesthesia (DOA) using drug infusions of propofol and remifentanil. Our method employs long short-term memory (LSTM) and gate residual network (GRN) networks to improve the efficiency of feature fusion and applies an attention mechanism to discover the interactions between the drugs. We also use label distribution smoothing and reweighting losses to address data imbalance. Experimental results show that our proposed method outperforms traditional PK-PD models and previous deep learning methods, effectively predicting anesthetic depth under sudden and deep anesthesia conditions.
</details>
<details>
<summary>摘要</summary>
优先预测麻醉效果是控制性Infusion系统中的关键。传统的PK-PD模型需要手动选择参数，这可能是临床设置中的挑战。最近提出的深度学习方法可能只能捕捉总趋势，并不能预测突然变化的BIS。为解决这些问题，我们提议一种基于变换器的方法，用于预测麻醉深度（DOA），使用propofol和remifentanil的药物注射。我们的方法使用长期短期记忆（LSTM）和门控异常网络（GRN）来提高特征融合的效率，并使用注意力机制来揭示药物之间的交互。此外，我们还使用标签分布平滑和重新评估损失来Address数据不均衡。实验结果表明，我们的提议方法比传统PK-PD模型和前一些深度学习方法更高效，可以有效地预测麻醉深度在突然和深度麻醉条件下。
</details></li>
</ul>
<hr>
<h2 id="DySTreSS-Dynamically-Scaled-Temperature-in-Self-Supervised-Contrastive-Learning"><a href="#DySTreSS-Dynamically-Scaled-Temperature-in-Self-Supervised-Contrastive-Learning" class="headerlink" title="DySTreSS: Dynamically Scaled Temperature in Self-Supervised Contrastive Learning"></a>DySTreSS: Dynamically Scaled Temperature in Self-Supervised Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01140">http://arxiv.org/abs/2308.01140</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siladittya Manna, Soumitri Chattopadhyay, Rakesh Dey, Saumik Bhattacharya, Umapada Pal</li>
<li>for: 提高自然语言处理（NLP）领域中的自我监督学习（SSL）性能，尤其是对于SimCLR和MoCo等 contemporaneous自编程对比算法。</li>
<li>methods: 研究InfoNCE损失函数中温度超参数的影响，并提出一种基于cosinus相似性的温度扩散函数来优化样本分布在特征空间中。</li>
<li>results: 经验证明，提出的方法可以超过或与对比损失函数基于SSL算法相当。此外，我们还对温度的变化情况进行了全面的分析，并证明了在特征空间中的本地和全局结构的变化。<details>
<summary>Abstract</summary>
In contemporary self-supervised contrastive algorithms like SimCLR, MoCo, etc., the task of balancing attraction between two semantically similar samples and repulsion between two samples from different classes is primarily affected by the presence of hard negative samples. While the InfoNCE loss has been shown to impose penalties based on hardness, the temperature hyper-parameter is the key to regulating the penalties and the trade-off between uniformity and tolerance. In this work, we focus our attention to improve the performance of InfoNCE loss in SSL by studying the effect of temperature hyper-parameter values. We propose a cosine similarity-dependent temperature scaling function to effectively optimize the distribution of the samples in the feature space. We further analyze the uniformity and tolerance metrics to investigate the optimal regions in the cosine similarity space for better optimization. Additionally, we offer a comprehensive examination of the behavior of local and global structures in the feature space throughout the pre-training phase, as the temperature varies. Experimental evidence shows that the proposed framework outperforms or is at par with the contrastive loss-based SSL algorithms. We believe our work (DySTreSS) on temperature scaling in SSL provides a foundation for future research in contrastive learning.
</details>
<details>
<summary>摘要</summary>
现代自我监督对势学习算法如SimCLR、MoCo等，任务是在两个semantically similar sample之间吸引，并在不同类型的sample之间强制分化。而这种任务受到强度负样本的存在影响最大。InfoNCE损失已经显示出对强度做出了罚款，但温度超参数是控制这些罚款和吸引vs抵抗之间的平衡。在这个工作中，我们关注了InfoNCE损失在SSL中的性能优化，研究温度超参数的影响。我们提议一种cosine similarity-dependent温度扩大函数，可以有效地优化样本在特征空间的分布。我们进一步分析了uniformity和tolerance度量来探索最佳的cosine similarity空间区域，以便更好地优化。此外，我们还对批量和全局结构在特征空间中的行为进行了详细的分析，从温度变化的过程中。实验证明，我们提出的框架（DySTreSS）在SSL中表现出色，与对势损失基于SSL算法相当。我们认为我们的工作在温度扩大中的SSL提供了一个基础，以便未来对对势学习进行进一步研究。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Privacy-Allocation-for-Locally-Differentially-Private-Federated-Learning-with-Composite-Objectives"><a href="#Dynamic-Privacy-Allocation-for-Locally-Differentially-Private-Federated-Learning-with-Composite-Objectives" class="headerlink" title="Dynamic Privacy Allocation for Locally Differentially Private Federated Learning with Composite Objectives"></a>Dynamic Privacy Allocation for Locally Differentially Private Federated Learning with Composite Objectives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01139">http://arxiv.org/abs/2308.01139</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaojiao Zhang, Dominik Fay, Mikael Johansson</li>
<li>for: 该文章提出了一种基于地域ifferentially private federated学习算法，用于强 converges但可能是非流�emetric问题的保护工作者 gradient  против善良但怀疑的服务器。</li>
<li>methods: 提议的算法在共享信息中添加人工噪音来保护隐私，并动态分配时变的噪音 variance来最小化优化错误的上限，以满足先定的隐私预算限制。</li>
<li>results: 数值结果表明提议的算法在比较方法之上具有优势，可以在一个邻居的优解中实现隐私保护和实用性。<details>
<summary>Abstract</summary>
This paper proposes a locally differentially private federated learning algorithm for strongly convex but possibly nonsmooth problems that protects the gradients of each worker against an honest but curious server. The proposed algorithm adds artificial noise to the shared information to ensure privacy and dynamically allocates the time-varying noise variance to minimize an upper bound of the optimization error subject to a predefined privacy budget constraint. This allows for an arbitrarily large but finite number of iterations to achieve both privacy protection and utility up to a neighborhood of the optimal solution, removing the need for tuning the number of iterations. Numerical results show the superiority of the proposed algorithm over state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这篇论文提出了一种基于地域ifferentially private的联合学习算法，用于强 convex但可能非滑的问题，保护每个工作者的梯度 против一个诚实但感兴趣的服务器。提议的算法添加了随机噪声到共享信息中，以保障隐私，并动态分配时间变化的噪声方差，以最小化优化错误的Upper bound，subject to预定的隐私预算限制。这允许进行无限多次迭代，以达到隐私保护和实用性的权限，取消了迭代次数的调整。numerical results show that the proposed algorithm outperforms existing methods.
</details></li>
</ul>
<hr>
<h2 id="Can-We-Transfer-Noise-Patterns-A-Multi-environment-Spectrum-Analysis-Model-Using-Generated-Cases"><a href="#Can-We-Transfer-Noise-Patterns-A-Multi-environment-Spectrum-Analysis-Model-Using-Generated-Cases" class="headerlink" title="Can We Transfer Noise Patterns? A Multi-environment Spectrum Analysis Model Using Generated Cases"></a>Can We Transfer Noise Patterns? A Multi-environment Spectrum Analysis Model Using Generated Cases</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01138">http://arxiv.org/abs/2308.01138</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/magnomic/cnst">https://github.com/magnomic/cnst</a></li>
<li>paper_authors: Haiwen Du, Zheng Ju, Yu An, Honghui Du, Dongjie Zhu, Zhaoshuo Tian, Aonghus Lawlor, Ruihai Dong</li>
<li>for: 这个论文的目的是提高在线水质测试中的谱分析系统的精度和可靠性，使regulatory agencies能够更快地回应污染事件。</li>
<li>methods: 该论文提出了一种采用 transferred noise pattern 模型，可以在不同环境中提高谱分析系统的鲁棒性和可靠性。这种模型通过学习不同环境中标准水样的谱spectrum的差异，以 Transfer learning 的方式将陌生样本的噪声转移到标准样本中，从而提高谱分析系统的性能。</li>
<li>results: 实验表明，提出的方法可以减少谱分析系统中的噪声影响，提高系统的鲁棒性和可靠性。与基eline系统相比，该方法在不同背景噪声下的实验结果表明，它可以更好地适应不同环境中的噪声。<details>
<summary>Abstract</summary>
Spectrum analysis systems in online water quality testing are designed to detect types and concentrations of pollutants and enable regulatory agencies to respond promptly to pollution incidents. However, spectral data-based testing devices suffer from complex noise patterns when deployed in non-laboratory environments. To make the analysis model applicable to more environments, we propose a noise patterns transferring model, which takes the spectrum of standard water samples in different environments as cases and learns the differences in their noise patterns, thus enabling noise patterns to transfer to unknown samples. Unfortunately, the inevitable sample-level baseline noise makes the model unable to obtain the paired data that only differ in dataset-level environmental noise. To address the problem, we generate a sample-to-sample case-base to exclude the interference of sample-level noise on dataset-level noise learning, enhancing the system's learning performance. Experiments on spectral data with different background noises demonstrate the good noise-transferring ability of the proposed method against baseline systems ranging from wavelet denoising, deep neural networks, and generative models. From this research, we posit that our method can enhance the performance of DL models by generating high-quality cases. The source code is made publicly available online at https://github.com/Magnomic/CNST.
</details>
<details>
<summary>摘要</summary>
Spectrum analysis systems in online water quality testing are designed to detect types and concentrations of pollutants and enable regulatory agencies to respond promptly to pollution incidents. However, spectral data-based testing devices suffer from complex noise patterns when deployed in non-laboratory environments. To make the analysis model applicable to more environments, we propose a noise patterns transferring model, which takes the spectrum of standard water samples in different environments as cases and learns the differences in their noise patterns, thus enabling noise patterns to transfer to unknown samples. Unfortunately, the inevitable sample-level baseline noise makes the model unable to obtain the paired data that only differ in dataset-level environmental noise. To address the problem, we generate a sample-to-sample case-base to exclude the interference of sample-level noise on dataset-level noise learning, enhancing the system's learning performance. Experiments on spectral data with different background noises demonstrate the good noise-transferring ability of the proposed method against baseline systems ranging from wavelet denoising, deep neural networks, and generative models. From this research, we posit that our method can enhance the performance of DL models by generating high-quality cases. The source code is made publicly available online at <https://github.com/Magnomic/CNST>.Note: Please note that the translation is in Simplified Chinese, which is one of the two standard versions of Chinese. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Multi-task-learning-for-classification-segmentation-reconstruction-and-detection-on-chest-CT-scans"><a href="#Multi-task-learning-for-classification-segmentation-reconstruction-and-detection-on-chest-CT-scans" class="headerlink" title="Multi-task learning for classification, segmentation, reconstruction, and detection on chest CT scans"></a>Multi-task learning for classification, segmentation, reconstruction, and detection on chest CT scans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01137">http://arxiv.org/abs/2308.01137</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weronika Hryniewska-Guzik, Maria Kędzierska, Przemysław Biecek</li>
<li>for: 针对肺癌和covid-19两种疾病，提出了一种多任务学习框架，用于分类、分割、重建和检测。</li>
<li>methods: 使用多任务学习方法，抽取医学数据中重要的特征，如肿瘤，从小量数据中提取出来。</li>
<li>results: 提出了一种新的多任务框架，并在分类、分割、重建和检测任务中进行了实验，并取得了优秀的结果。此外，还研究了使用不同的后向和损失函数在分割任务中的可能性。<details>
<summary>Abstract</summary>
Lung cancer and covid-19 have one of the highest morbidity and mortality rates in the world. For physicians, the identification of lesions is difficult in the early stages of the disease and time-consuming. Therefore, multi-task learning is an approach to extracting important features, such as lesions, from small amounts of medical data because it learns to generalize better. We propose a novel multi-task framework for classification, segmentation, reconstruction, and detection. To the best of our knowledge, we are the first ones who added detection to the multi-task solution. Additionally, we checked the possibility of using two different backbones and different loss functions in the segmentation task.
</details>
<details>
<summary>摘要</summary>
肺癌和 COVID-19 在全球病例和死亡率中排名非常高。为医生来说，在疾病的早期阶段 identification of lesions 是困难和耗时的。因此，多任务学习是一种提取重要特征的方法，如肿瘤，从小量医疗数据中提取出来。我们提出了一种新的多任务框架，用于分类、分割、重建和检测。根据我们所知，我们是第一个在多任务解决方案中添加检测的人。此外，我们还检查了使用两种不同的背bone 和不同的损失函数在 segmentation 任务中的可能性。
</details></li>
</ul>
<hr>
<h2 id="Unlearning-Spurious-Correlations-in-Chest-X-ray-Classification"><a href="#Unlearning-Spurious-Correlations-in-Chest-X-ray-Classification" class="headerlink" title="Unlearning Spurious Correlations in Chest X-ray Classification"></a>Unlearning Spurious Correlations in Chest X-ray Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01119">http://arxiv.org/abs/2308.01119</a></li>
<li>repo_url: None</li>
<li>paper_authors: Misgina Tsighe Hagos, Kathleen M. Curran, Brian Mac Namee</li>
<li>For: The paper aims to address the issue of spurious correlations in medical image classification, particularly in musculoskeletal image classification, due to unintended confounding factors such as skeletal maturation-induced bone growth.* Methods: The authors use a deep learning approach called eXplanation Based Learning (XBL) that incorporates interactive user feedback, specifically feature annotations, to unlearn spurious correlations and construct robust models.* Results: The study demonstrates the effectiveness of XBL in eliminating spurious correlations and improving model accuracy, even in the presence of confounding factors.Here is the same information in Simplified Chinese text:* For: 本研究旨在解决医学图像分类中的假相关性问题，特别是骨骼成熔融引起的骨骼成熔融相关性问题。* Methods: 作者使用了一种深度学习方法——解释基于学习（XBL），该方法通过获取用户交互式反馈，特别是特征标注，来消除假相关性和构建可靠的模型。* Results: 研究表明，XBL 可以有效消除假相关性，提高模型准确性，即使在骨骼成熔融等干扰因素存在时。<details>
<summary>Abstract</summary>
Medical image classification models are frequently trained using training datasets derived from multiple data sources. While leveraging multiple data sources is crucial for achieving model generalization, it is important to acknowledge that the diverse nature of these sources inherently introduces unintended confounders and other challenges that can impact both model accuracy and transparency. A notable confounding factor in medical image classification, particularly in musculoskeletal image classification, is skeletal maturation-induced bone growth observed during adolescence. We train a deep learning model using a Covid-19 chest X-ray dataset and we showcase how this dataset can lead to spurious correlations due to unintended confounding regions. eXplanation Based Learning (XBL) is a deep learning approach that goes beyond interpretability by utilizing model explanations to interactively unlearn spurious correlations. This is achieved by integrating interactive user feedback, specifically feature annotations. In our study, we employed two non-demanding manual feedback mechanisms to implement an XBL-based approach for effectively eliminating these spurious correlations. Our results underscore the promising potential of XBL in constructing robust models even in the presence of confounding factors.
</details>
<details>
<summary>摘要</summary>
医疗图像分类模型frequently使用多种数据源进行训练。虽然利用多种数据源对模型泛化起到关键作用，但是需要注意这些来源的多样性自然而来会引入无意义的干扰因素和其他挑战，这些因素可能会影响模型的准确性和透明度。在医疗图像分类中，特别是在肢体成像分类中，肿瘤成熔是一个强大的干扰因素。我们使用COVID-19肺X射线图像集进行训练深度学习模型，并表明这些数据集可能会导致偶极相关性，因为无意义的干扰区域。eXplanation Based Learning（XBL）是一种深度学习方法，它不仅提供了解释，还可以通过交互式解释来卸除偶极相关性。我们使用了两种简单的手动反馈机制来实现XBL中的解释。我们的结果表明XBL在存在干扰因素的情况下可以构建Robust的模型。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-on-Popularity-Bias-in-Recommender-Systems"><a href="#A-Survey-on-Popularity-Bias-in-Recommender-Systems" class="headerlink" title="A Survey on Popularity Bias in Recommender Systems"></a>A Survey on Popularity Bias in Recommender Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01118">http://arxiv.org/abs/2308.01118</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anastasiia Klimashevskaia, Dietmar Jannach, Mehdi Elahi, Christoph Trattner</li>
<li>for: The paper aims to discuss the popularity bias in recommender systems and explore ways to detect, quantify, and mitigate it.</li>
<li>methods: The paper reviews existing approaches to reduce popularity bias in recommender systems, including computational metrics and technical approaches.</li>
<li>results: The paper discusses the potential reasons for popularity bias and critically examines the current literature on the topic, which is mostly based on computational experiments and assumptions about the practical effects of including long-tail items in recommendations.<details>
<summary>Abstract</summary>
Recommender systems help people find relevant content in a personalized way. One main promise of such systems is that they are able to increase the visibility of items in the long tail, i.e., the lesser-known items in a catalogue. Existing research, however, suggests that in many situations today's recommendation algorithms instead exhibit a popularity bias, meaning that they often focus on rather popular items in their recommendations. Such a bias may not only lead to limited value of the recommendations for consumers and providers in the short run, but it may also cause undesired reinforcement effects over time. In this paper, we discuss the potential reasons for popularity bias and we review existing approaches to detect, quantify and mitigate popularity bias in recommender systems. Our survey therefore includes both an overview of the computational metrics used in the literature as well as a review of the main technical approaches to reduce the bias. We furthermore critically discuss today's literature, where we observe that the research is almost entirely based on computational experiments and on certain assumptions regarding the practical effects of including long-tail items in the recommendations.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Spatio-Temporal-Branching-for-Motion-Prediction-using-Motion-Increments"><a href="#Spatio-Temporal-Branching-for-Motion-Prediction-using-Motion-Increments" class="headerlink" title="Spatio-Temporal Branching for Motion Prediction using Motion Increments"></a>Spatio-Temporal Branching for Motion Prediction using Motion Increments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01097">http://arxiv.org/abs/2308.01097</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jasonwang959/stpmp">https://github.com/jasonwang959/stpmp</a></li>
<li>paper_authors: Jiexin Wang, Yujie Zhou, Wenwen Qiang, Ying Ba, Bing Su, Ji-Rong Wen</li>
<li>for: 人体动作预测（HMP）是一个广泛应用的研究领域，但是它仍然是一个具有较高难度的任务，因为未来姿势的概率分布是随机和不规则的。</li>
<li>methods: 我们提出了一种新的分支网络方法，它将时间频率预测和空间结构预测解耦开，提取更多的动作信息，并通过知识储存来实现跨频率知识学习。</li>
<li>results: 我们在标准HMP bencmark上评估了我们的方法，并与当前的方法相比，在预测精度方面表现出色。<details>
<summary>Abstract</summary>
Human motion prediction (HMP) has emerged as a popular research topic due to its diverse applications, but it remains a challenging task due to the stochastic and aperiodic nature of future poses. Traditional methods rely on hand-crafted features and machine learning techniques, which often struggle to model the complex dynamics of human motion. Recent deep learning-based methods have achieved success by learning spatio-temporal representations of motion, but these models often overlook the reliability of motion data. Additionally, the temporal and spatial dependencies of skeleton nodes are distinct. The temporal relationship captures motion information over time, while the spatial relationship describes body structure and the relationships between different nodes. In this paper, we propose a novel spatio-temporal branching network using incremental information for HMP, which decouples the learning of temporal-domain and spatial-domain features, extracts more motion information, and achieves complementary cross-domain knowledge learning through knowledge distillation. Our approach effectively reduces noise interference and provides more expressive information for characterizing motion by separately extracting temporal and spatial features. We evaluate our approach on standard HMP benchmarks and outperform state-of-the-art methods in terms of prediction accuracy.
</details>
<details>
<summary>摘要</summary>
人体运动预测（HMP）已成为一个流行的研究主题，它的应用场景非常广泛，但是预测人体运动的任务仍然具有挑战性，因为未来的姿势是随机和不规则的。传统的方法通常使用手工设计的特征和机器学习技术来实现预测，但这些方法通常无法模型人体运动的复杂动力学。现代深度学习基于的方法在学习运动的空间时间表示方面取得成功，但是这些模型经常忽视运动数据的可靠性。此外，运动数据中的时间和空间关系不同。时间关系捕捉运动信息的时间方面，而空间关系描述人体结构和不同节点之间的关系。在本文中，我们提出了一种新的空间时间分支网络使用增量信息进行HMP，这种方法可以分离学习时间频率和空间频率特征，提取更多的运动信息，并通过知识填充来实现跨频率知识学习。我们的方法可以减少噪声扰动并为运动特征提供更加精细的描述。我们在标准HMP测试benchmark上评估了我们的方法，并与当前的状态艺技术相比，在预测精度方面表现出色。
</details></li>
</ul>
<hr>
<h2 id="Multi-variable-Hard-Physical-Constraints-for-Climate-Model-Downscaling"><a href="#Multi-variable-Hard-Physical-Constraints-for-Climate-Model-Downscaling" class="headerlink" title="Multi-variable Hard Physical Constraints for Climate Model Downscaling"></a>Multi-variable Hard Physical Constraints for Climate Model Downscaling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01868">http://arxiv.org/abs/2308.01868</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jose González-Abad, Álex Hernández-García, Paula Harder, David Rolnick, José Manuel Gutiérrez</li>
<li>for:  GCMs 全球气候模型是主要用于 simulate 气候演变和评估气候变化的工具，但它们通常在尺度较粗的尺度下运行，导致它们无法准确地复制当地规模的地理现象。</li>
<li>methods: 统计下降方法利用深度学习可以解决这个问题，通过将粗略变量转换为本地气候场，以便在区域GCM预测中使用。然而，通常会独立地下降不同变量的气候场，导致物理性关系 между 不同变量的下降场被违反。</li>
<li>results: 本研究探讨了这个问题的范围，并通过对温度变量进行应用，建立了一个满足物理关系 между 多个下降场的框架。<details>
<summary>Abstract</summary>
Global Climate Models (GCMs) are the primary tool to simulate climate evolution and assess the impacts of climate change. However, they often operate at a coarse spatial resolution that limits their accuracy in reproducing local-scale phenomena. Statistical downscaling methods leveraging deep learning offer a solution to this problem by approximating local-scale climate fields from coarse variables, thus enabling regional GCM projections. Typically, climate fields of different variables of interest are downscaled independently, resulting in violations of fundamental physical properties across interconnected variables. This study investigates the scope of this problem and, through an application on temperature, lays the foundation for a framework introducing multi-variable hard constraints that guarantees physical relationships between groups of downscaled climate variables.
</details>
<details>
<summary>摘要</summary>
全球气候模型（GCM）是气候进化和气候变化影响的主要工具。然而，它们通常在粗略的空间分辨率下运行，限制其在本地规模现象的准确还原。统计下降方法，利用深度学习解决这个问题，通过将粗略变量下降到本地气候场，以便地区GCM预测。然而，通常会在不同变量 интересов的气候场下降独立，导致物理关系的违反，这种情况下，这种研究探讨范围和应用于温度下降，为多变量硬件约束的框架奠基。
</details></li>
</ul>
<hr>
<h2 id="Homography-Estimation-in-Complex-Topological-Scenes"><a href="#Homography-Estimation-in-Complex-Topological-Scenes" class="headerlink" title="Homography Estimation in Complex Topological Scenes"></a>Homography Estimation in Complex Topological Scenes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01086">http://arxiv.org/abs/2308.01086</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giacomo D’Amicantonio, Egor Bondarau, Peter H. N. De With</li>
<li>for: 用于视觉监测应用，如交通分析和犯罪检测</li>
<li>methods: 使用自定义的字典方法，不需要先知 camera 设置</li>
<li>results: 提高 IoU 指标，比对state-of-the-art模型在五个synthetic dataset和2014年世界杯 dataset 的提高为12%<details>
<summary>Abstract</summary>
Surveillance videos and images are used for a broad set of applications, ranging from traffic analysis to crime detection. Extrinsic camera calibration data is important for most analysis applications. However, security cameras are susceptible to environmental conditions and small camera movements, resulting in a need for an automated re-calibration method that can account for these varying conditions. In this paper, we present an automated camera-calibration process leveraging a dictionary-based approach that does not require prior knowledge on any camera settings. The method consists of a custom implementation of a Spatial Transformer Network (STN) and a novel topological loss function. Experiments reveal that the proposed method improves the IoU metric by up to 12% w.r.t. a state-of-the-art model across five synthetic datasets and the World Cup 2014 dataset.
</details>
<details>
<summary>摘要</summary>
视频监测和图像被用于广泛的应用领域，从交通分析到犯罪检测。外部摄像头准确性是重要的。然而，安全摄像头容易受到环境因素和小型摄像头运动的影响，需要一种自动重新准确方法，可以考虑这些不同的条件。本文提出了一种基于词典方法的自动摄像头准确化 процесса，不需要任何摄像头设置的先前知识。该方法包括一种自定义的空间变换网络（STN）和一种新的拓扑损失函数。实验表明，提议的方法可以在五个 sintetic 数据集和2014年世界杯数据集中提高 IoU 指标至最多 12% 相比于现有模型。
</details></li>
</ul>
<hr>
<h2 id="Data-Driven-Identification-of-Quadratic-Symplectic-Representations-of-Nonlinear-Hamiltonian-Systems"><a href="#Data-Driven-Identification-of-Quadratic-Symplectic-Representations-of-Nonlinear-Hamiltonian-Systems" class="headerlink" title="Data-Driven Identification of Quadratic Symplectic Representations of Nonlinear Hamiltonian Systems"></a>Data-Driven Identification of Quadratic Symplectic Representations of Nonlinear Hamiltonian Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01084">http://arxiv.org/abs/2308.01084</a></li>
<li>repo_url: None</li>
<li>paper_authors: Süleyman Yildiz, Pawan Goyal, Thomas Bendokat, Peter Benner</li>
<li>for: 学习哈密顿系统使用数据</li>
<li>methods: 使用提升假设，将非线性哈密顿系统写作非线性系统，并使用协调器抽象器保持哈密顿结构</li>
<li>results: 实现了使用数据学习哈密顿系统，并保持系统的长期稳定性，同时使用三次哈密顿函数提供较低的模型复杂性<details>
<summary>Abstract</summary>
We present a framework for learning Hamiltonian systems using data. This work is based on the lifting hypothesis, which posits that nonlinear Hamiltonian systems can be written as nonlinear systems with cubic Hamiltonians. By leveraging this, we obtain quadratic dynamics that are Hamiltonian in a transformed coordinate system. To that end, for given generalized position and momentum data, we propose a methodology to learn quadratic dynamical systems, enforcing the Hamiltonian structure in combination with a symplectic auto-encoder. The enforced Hamiltonian structure exhibits long-term stability of the system, while the cubic Hamiltonian function provides relatively low model complexity. For low-dimensional data, we determine a higher-order transformed coordinate system, whereas, for high-dimensional data, we find a lower-order coordinate system with the desired properties. We demonstrate the proposed methodology by means of both low-dimensional and high-dimensional nonlinear Hamiltonian systems.
</details>
<details>
<summary>摘要</summary>
我们提出了一种基于数据学习哈密顿系统的框架。这项工作基于升高假设，即非线性哈密顿系统可以写作非线性系统的立方 Hamiltonians。通过这种方式，我们得到了各次幂动力学，并且在一种变换坐标系中保持哈密顿结构。为此，我们提出了一种使用泛化位势和动量数据学习 quadratic dynamical systems的方法，并在合理的坐标系中强制实施哈密顿结构。这种方法可以保证系统的长期稳定性，而且立方 Hamiltonians 的模型复杂度相对较低。对于低维数据，我们可以找到一个更高阶的变换坐标系，而对于高维数据，我们可以找到一个更低阶的坐标系，满足所需的性质。我们通过低维和高维非线性哈密顿系统的示例来证明我们的方法。
</details></li>
</ul>
<hr>
<h2 id="A-Practical-Deep-Learning-Based-Acoustic-Side-Channel-Attack-on-Keyboards"><a href="#A-Practical-Deep-Learning-Based-Acoustic-Side-Channel-Attack-on-Keyboards" class="headerlink" title="A Practical Deep Learning-Based Acoustic Side Channel Attack on Keyboards"></a>A Practical Deep Learning-Based Acoustic Side Channel Attack on Keyboards</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01074">http://arxiv.org/abs/2308.01074</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/JBFH-Dev/Keystroke-Datasets">https://github.com/JBFH-Dev/Keystroke-Datasets</a></li>
<li>paper_authors: Joshua Harrison, Ehsan Toreini, Maryam Mehrnezhad</li>
<li>for: 防止键盘Side channel攻击</li>
<li>methods: 使用深度学习模型类型 laptop键盘输入</li>
<li>results:  achieved an accuracy of 95% and 93% when trained on keystrokes recorded by a nearby phone and Zoom video-conferencing software respectively, proving the practicality of these side channel attacks via off-the-shelf equipment and algorithms.<details>
<summary>Abstract</summary>
With recent developments in deep learning, the ubiquity of micro-phones and the rise in online services via personal devices, acoustic side channel attacks present a greater threat to keyboards than ever. This paper presents a practical implementation of a state-of-the-art deep learning model in order to classify laptop keystrokes, using a smartphone integrated microphone. When trained on keystrokes recorded by a nearby phone, the classifier achieved an accuracy of 95%, the highest accuracy seen without the use of a language model. When trained on keystrokes recorded using the video-conferencing software Zoom, an accuracy of 93% was achieved, a new best for the medium. Our results prove the practicality of these side channel attacks via off-the-shelf equipment and algorithms. We discuss a series of mitigation methods to protect users against these series of attacks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Automatic-Feature-Engineering-for-Time-Series-Classification-Evaluation-and-Discussion"><a href="#Automatic-Feature-Engineering-for-Time-Series-Classification-Evaluation-and-Discussion" class="headerlink" title="Automatic Feature Engineering for Time Series Classification: Evaluation and Discussion"></a>Automatic Feature Engineering for Time Series Classification: Evaluation and Discussion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01071">http://arxiv.org/abs/2308.01071</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aurélien Renault, Alexis Bondu, Vincent Lemaire, Dominique Gay</li>
<li>for: 本文主要针对时间序列分类（TSC）问题，尤其是在数据科学和知识工程领域。</li>
<li>methods: 本文使用了多种方法，包括相似度量、间隔、Shapelets、字典等，以及深度学习方法和混合ensemble方法。</li>
<li>results: 本文的实验结果表明，使用feature工程工具可以获得高度可预测性的时间序列分类结果，与当前状态艺术TSC算法相当。<details>
<summary>Abstract</summary>
Time Series Classification (TSC) has received much attention in the past two decades and is still a crucial and challenging problem in data science and knowledge engineering. Indeed, along with the increasing availability of time series data, many TSC algorithms have been suggested by the research community in the literature. Besides state-of-the-art methods based on similarity measures, intervals, shapelets, dictionaries, deep learning methods or hybrid ensemble methods, several tools for extracting unsupervised informative summary statistics, aka features, from time series have been designed in the recent years. Originally designed for descriptive analysis and visualization of time series with informative and interpretable features, very few of these feature engineering tools have been benchmarked for TSC problems and compared with state-of-the-art TSC algorithms in terms of predictive performance. In this article, we aim at filling this gap and propose a simple TSC process to evaluate the potential predictive performance of the feature sets obtained with existing feature engineering tools. Thus, we present an empirical study of 11 feature engineering tools branched with 9 supervised classifiers over 112 time series data sets. The analysis of the results of more than 10000 learning experiments indicate that feature-based methods perform as accurately as current state-of-the-art TSC algorithms, and thus should rightfully be considered further in the TSC literature.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="When-Analytic-Calculus-Cracks-AdaBoost-Code"><a href="#When-Analytic-Calculus-Cracks-AdaBoost-Code" class="headerlink" title="When Analytic Calculus Cracks AdaBoost Code"></a>When Analytic Calculus Cracks AdaBoost Code</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01070">http://arxiv.org/abs/2308.01070</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jean-Marc Brossier, Olivier Lafitte, Lenny Réthoré</li>
<li>for: 本研究探讨了支持学习中的加强原理，特别是 AdaBoost 算法是否真的是一种优化算法。</li>
<li>methods: 本研究使用了一个二分类问题作为例子，并通过三个二进制分类器的组合来显示 AdaBoost 算法可以由一个真表示表示出来。</li>
<li>results: 研究结果表明，AdaBoost 算法不是一种优化算法，而是一种可以由真表示表示出来的算法。与 scikit-learn 库中的 AdaBoost 算法实现相比，本研究的结果也提供了一些对 AdaBoost 算法的比较。<details>
<summary>Abstract</summary>
The principle of boosting in supervised learning involves combining multiple weak classifiers to obtain a stronger classifier. AdaBoost has the reputation to be a perfect example of this approach. We have previously shown that AdaBoost is not truly an optimization algorithm. This paper shows that AdaBoost is an algorithm in name only, as the resulting combination of weak classifiers can be explicitly calculated using a truth table. This study is carried out by considering a problem with two classes and is illustrated by the particular case of three binary classifiers and presents results in comparison with those from the implementation of AdaBoost algorithm of the Python library scikit-learn.
</details>
<details>
<summary>摘要</summary>
“boosting”在超级vised学习中的原则是将多个弱分类器组合而成一个更强大的分类器。阿达Boost被认为是这种方法的完美示例。我们之前已经证明了阿达Boost不是一个优化算法。这篇论文表明，阿达Boost并不是一个真正的算法，因为将弱分类器组合的结果可以通过真值表进行直观计算。本研究通过考虑两个类别问题，并通过三个二进制分类器的特殊情况进行示例，并与scikit-learn库中的阿达Boost实现相比较。
</details></li>
</ul>
<hr>
<h2 id="Graph-Anomaly-Detection-at-Group-Level-A-Topology-Pattern-Enhanced-Unsupervised-Approach"><a href="#Graph-Anomaly-Detection-at-Group-Level-A-Topology-Pattern-Enhanced-Unsupervised-Approach" class="headerlink" title="Graph Anomaly Detection at Group Level: A Topology Pattern Enhanced Unsupervised Approach"></a>Graph Anomaly Detection at Group Level: A Topology Pattern Enhanced Unsupervised Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01063">http://arxiv.org/abs/2308.01063</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xing Ai, Jialong Zhou, Yulin Zhu, Gaolei Li, Tomasz P. Michalak, Xiapu Luo, Kai Zhou</li>
<li>for: 本研究旨在提出一种新的无监督框架，用于检测图像中异常的群体（Group-level Graph Anomaly Detection，Gr-GAD）。</li>
<li>methods: 该框架首先使用变体的图自动编码器（Graph AutoEncoder，GAE）来找到可能的异常群体的anchor节点，然后使用群体采样来选择候选组，并使用图像特征来生成每个候选组的嵌入。</li>
<li>results: 在实际世界和synthetic数据集上的实验结果显示，提出的框架能够有效地识别和地图异常群体，这表明该框架是一种有前途的解决方案。<details>
<summary>Abstract</summary>
Graph anomaly detection (GAD) has achieved success and has been widely applied in various domains, such as fraud detection, cybersecurity, finance security, and biochemistry. However, existing graph anomaly detection algorithms focus on distinguishing individual entities (nodes or graphs) and overlook the possibility of anomalous groups within the graph. To address this limitation, this paper introduces a novel unsupervised framework for a new task called Group-level Graph Anomaly Detection (Gr-GAD). The proposed framework first employs a variant of Graph AutoEncoder (GAE) to locate anchor nodes that belong to potential anomaly groups by capturing long-range inconsistencies. Subsequently, group sampling is employed to sample candidate groups, which are then fed into the proposed Topology Pattern-based Graph Contrastive Learning (TPGCL) method. TPGCL utilizes the topology patterns of groups as clues to generate embeddings for each candidate group and thus distinct anomaly groups. The experimental results on both real-world and synthetic datasets demonstrate that the proposed framework shows superior performance in identifying and localizing anomaly groups, highlighting it as a promising solution for Gr-GAD. Datasets and codes of the proposed framework are at the github repository https://anonymous.4open.science/r/Topology-Pattern-Enhanced-Unsupervised-Group-level-Graph-Anomaly-Detection.
</details>
<details>
<summary>摘要</summary>
graph anomaly detection (GAD) 已经取得成功并广泛应用于不同领域，如诈骗检测、网络安全、金融安全和生物化学。然而，现有的图像异常检测算法都是专注于区分个体节点或图像，忽略图像中可能存在异常群集的可能性。为了解决这种局限性，本文提出了一种新的无监督框架，即Group-level Graph Anomaly Detection (Gr-GAD)。该框架首先使用变体的图自动编码器（GAE）来找到可能异常群集的anchor节点，并通过捕捉长范围的不一致性来捕捉异常群集的特征。然后，群体采样被用来采样候选群集，并将其传递给提出的Topology Pattern-based Graph Contrastive Learning（TPGCL）方法。TPGCL利用群体的topology特征作为准确的准确度，生成每个候选群集的嵌入。实验结果表明，提出的框架在真实世界和 sintetic 数据集上具有优秀的异常群集识别和定位能力，从而证明了该框架的潜在价值。数据集和代码可以在 GitHub 上获取：https://anonymous.4open.science/r/Topology-Pattern-Enhanced-Unsupervised-Group-level-Graph-Anomaly-Detection。
</details></li>
</ul>
<hr>
<h2 id="Simulation-based-inference-using-surjective-sequential-neural-likelihood-estimation"><a href="#Simulation-based-inference-using-surjective-sequential-neural-likelihood-estimation" class="headerlink" title="Simulation-based inference using surjective sequential neural likelihood estimation"></a>Simulation-based inference using surjective sequential neural likelihood estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01054">http://arxiv.org/abs/2308.01054</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dirmeier/ssnl">https://github.com/dirmeier/ssnl</a></li>
<li>paper_authors: Simon Dirmeier, Carlo Albert, Fernando Perez-Cruz</li>
<li>for: 这篇论文的目的是提出一种基于模拟的推理方法，用于处理不可分解的几何函数模型，只有生成 synthetic 数据的 simulator 可用。</li>
<li>methods: 该方法使用一种维度减少的射影正常化流形模型，作为几何函数模型的代理，以便使用普通的极限渐近法或变量极限法进行推理。</li>
<li>results: 作者在多种实验中证明了 SSNL 比当今的同类方法在高维数据集上表现更佳，包括一个具有实际意义的astrophysical例子，用于模拟太阳magnetic field strength。<details>
<summary>Abstract</summary>
We present Surjective Sequential Neural Likelihood (SSNL) estimation, a novel method for simulation-based inference in models where the evaluation of the likelihood function is not tractable and only a simulator that can generate synthetic data is available. SSNL fits a dimensionality-reducing surjective normalizing flow model and uses it as a surrogate likelihood function which allows for conventional Bayesian inference using either Markov chain Monte Carlo methods or variational inference. By embedding the data in a low-dimensional space, SSNL solves several issues previous likelihood-based methods had when applied to high-dimensional data sets that, for instance, contain non-informative data dimensions or lie along a lower-dimensional manifold. We evaluate SSNL on a wide variety of experiments and show that it generally outperforms contemporary methods used in simulation-based inference, for instance, on a challenging real-world example from astrophysics which models the magnetic field strength of the sun using a solar dynamo model.
</details>
<details>
<summary>摘要</summary>
我们提出了Sequential Neural Likelihood（SSNL）估计方法，用于基于模拟的推理，其中评估likelihood函数不可求解，只有一个可以生成假数据的 simulator。SSNL使用一个维度减少的射影正常分布模型，并将其作为媒介函数使用，这使得可以使用 conventinal Bayesian推理方法，如Markov chain Monte Carlo方法或variational推理。通过嵌入数据到低维空间中，SSNL解决了之前基于likelihood方法在高维数据集上遇到的多种问题，如非有用数据维度或数据集 lying在一个低维度扁平面上。我们在各种实验中评估了SSNL，并显示它通常超过了当今使用在基于模拟的推理中的方法，如在一个具有实际意义的例子中，模拟太阳magnetic field strength using solar dynamo模型。
</details></li>
</ul>
<hr>
<h2 id="A-Counterfactual-Safety-Margin-Perspective-on-the-Scoring-of-Autonomous-Vehicles’-Riskiness"><a href="#A-Counterfactual-Safety-Margin-Perspective-on-the-Scoring-of-Autonomous-Vehicles’-Riskiness" class="headerlink" title="A Counterfactual Safety Margin Perspective on the Scoring of Autonomous Vehicles’ Riskiness"></a>A Counterfactual Safety Margin Perspective on the Scoring of Autonomous Vehicles’ Riskiness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01050">http://arxiv.org/abs/2308.01050</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alessandro Zanardi, Andrea Censi, Margherita Atzei, Luigi Di Lillo, Emilio Frazzoli</li>
<li>for: This paper aims to provide a data-driven framework for comparing the risk of different autonomous vehicles (AVs) in various operational design domains (ODDs).</li>
<li>methods: The paper uses counterfactual simulations of “misbehaving” road users to assess the risk of AVs. The proposed methodology is applicable even when the AV’s behavioral policy is unknown, making it useful for external third-party risk assessors.</li>
<li>results: The experimental results demonstrate a correlation between the safety margin, the driving policy quality, and the ODD, shedding light on the relative risk associated with different AV providers. The paper contributes to AV safety assessment and addresses legislative and insurance concerns surrounding the technology.<details>
<summary>Abstract</summary>
Autonomous Vehicles (AVs) have the potential to provide numerous societal benefits, such as decreased road accidents and increased overall transportation efficiency. However, quantifying the risk associated with AVs is challenging due to the lack of historical data and the rapidly evolving technology. This paper presents a data-driven framework for comparing the risk of different AVs' behaviors in various operational design domains (ODDs), based on counterfactual simulations of "misbehaving" road users. We introduce the concept of counterfactual safety margin, which represents the minimum deviation from normal behavior that could lead to a collision. This concept helps to find the most critical scenarios but also to assess the frequency and severity of risk of AVs. We show that the proposed methodology is applicable even when the AV's behavioral policy is unknown -- through worst- and best-case analyses -- making the method useful also to external third-party risk assessors. Our experimental results demonstrate the correlation between the safety margin, the driving policy quality, and the ODD shedding light on the relative risk associated with different AV providers. This work contributes to AV safety assessment and aids in addressing legislative and insurance concerns surrounding this emerging technology.
</details>
<details>
<summary>摘要</summary>
Note: The text has been translated into Simplified Chinese, which is the standard writing system used in mainland China.
</details></li>
</ul>
<hr>
<h2 id="Are-Easy-Data-Easy-for-K-Means"><a href="#Are-Easy-Data-Easy-for-K-Means" class="headerlink" title="Are Easy Data Easy (for K-Means)"></a>Are Easy Data Easy (for K-Means)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01926">http://arxiv.org/abs/2308.01926</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sayantann11/all-classification-templetes-for-ML">https://github.com/sayantann11/all-classification-templetes-for-ML</a></li>
<li>paper_authors: Mieczysław A. Kłopotek</li>
<li>for:  investigate the capability of correctly recovering well-separated clusters by various brands of the $k$-means algorithm.</li>
<li>methods:  uses the $k$-means algorithm and derives conditions for well-separated clusters, and proposes a new algorithm that is a variation of $k$-means++ via repeated subsampling.</li>
<li>results:  the new algorithm outperforms four other algorithms from the $k$-means family on the task of discovering well-separated clusters.<details>
<summary>Abstract</summary>
This paper investigates the capability of correctly recovering well-separated clusters by various brands of the $k$-means algorithm. The concept of well-separatedness used here is derived directly from the common definition of clusters, which imposes an interplay between the requirements of within-cluster-homogenicity and between-clusters-diversity. Conditions are derived for a special case of well-separated clusters such that the global minimum of $k$-means cost function coincides with the well-separatedness. An experimental investigation is performed to find out whether or no various brands of $k$-means are actually capable of discovering well separated clusters. It turns out that they are not. A new algorithm is proposed that is a variation of $k$-means++ via repeated {sub}sampling when choosing a seed. The new algorithm outperforms four other algorithms from $k$-means family on the task.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Evaluation-of-network-guided-random-forest-for-disease-gene-discovery"><a href="#Evaluation-of-network-guided-random-forest-for-disease-gene-discovery" class="headerlink" title="Evaluation of network-guided random forest for disease gene discovery"></a>Evaluation of network-guided random forest for disease gene discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01323">http://arxiv.org/abs/2308.01323</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianchang Hu, Silke Szymczak</li>
<li>For: The paper aims to investigate the performance of a network-guided random forest (RF) algorithm for gene expression data analysis, specifically for disease module and pathway identification.* Methods: The paper uses a network-guided RF approach that summarizes network information into a sampling probability of predictor variables, which is then used in the construction of the RF.* Results: The paper finds that network-guided RF does not provide better disease prediction than the standard RF, but it can identify disease genes more accurately when disease status is independent from genes in the given network. Additionally, the paper demonstrates that network-guided RF can identify genes from PGR-related pathways, leading to a better connected module of identified genes.Here is the same information in Simplified Chinese text:* For: 本研究用于 investigate random forest（RF）算法在基因表达数据分析中的性能，特别是疾病模块和路径标识。* Methods: 本研究使用一种基于网络信息的网络导向RF方法，将网络信息总结为预测变量的抽样概率，然后在RF构建中使用。* Results: 研究发现，网络导向RF不比标准RF提供更好的疾病预测。但是，当疾病状态与网络中的基因独立时，网络导向RF可以更准确地预测疾病基因。此外，研究还发现，使用网络信息时，特别是对于核心基因，可能会出现偶极选择的问题。<details>
<summary>Abstract</summary>
Gene network information is believed to be beneficial for disease module and pathway identification, but has not been explicitly utilized in the standard random forest (RF) algorithm for gene expression data analysis. We investigate the performance of a network-guided RF where the network information is summarized into a sampling probability of predictor variables which is further used in the construction of the RF. Our results suggest that network-guided RF does not provide better disease prediction than the standard RF. In terms of disease gene discovery, if disease genes form module(s), network-guided RF identifies them more accurately. In addition, when disease status is independent from genes in the given network, spurious gene selection results can occur when using network information, especially on hub genes. Our empirical analysis on two balanced microarray and RNA-Seq breast cancer datasets from The Cancer Genome Atlas (TCGA) for classification of progesterone receptor (PR) status also demonstrates that network-guided RF can identify genes from PGR-related pathways, which leads to a better connected module of identified genes.
</details>
<details>
<summary>摘要</summary>
GENE网络信息被认为对疾病模块和路径标识有利，但尚未直接应用于标准随机森林（RF）算法中。我们调查了基于网络信息的网络指导RF的性能。我们的结果表明，网络指导RF不比标准RF提供更好的疾病预测。在疾病基因发现方面，如果疾病基因组成模块， тогда网络指导RF能够更准确地确定它们。此外，当疾病状态与网络中的基因独立时，使用网络信息时，特别是对于枢纽基因，可能会出现假阳性基因选择结果。我们对TCGA breast cancer数据集进行了二分类分析，并证明了网络指导RF可以从PGR相关的路径中选择基因，从而构建更好地连接的基因模块。
</details></li>
</ul>
<hr>
<h2 id="Computing-the-Distance-between-unbalanced-Distributions-–-The-flat-Metric"><a href="#Computing-the-Distance-between-unbalanced-Distributions-–-The-flat-Metric" class="headerlink" title="Computing the Distance between unbalanced Distributions – The flat Metric"></a>Computing the Distance between unbalanced Distributions – The flat Metric</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01039">http://arxiv.org/abs/2308.01039</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hs42/flat_metric">https://github.com/hs42/flat_metric</a></li>
<li>paper_authors: Henri Schmidt, Christian Düll</li>
<li>for: 本研究是为了计算任意维度中的平凡度量，以扩展 Wasserstein 距离 W1 的概念，并应用于不均衡优化运输任务以及数据分布的分析。</li>
<li>methods: 本研究使用神经网络来确定两个给定概率分布之间的距离。特别是通过独立训练多个网络来实现对Distance的比较性。</li>
<li>results: 经过多个实验和 simulate 数据测试，研究发现该方法可以准确地计算平凡度量，并且可以在不同的维度下实现相似的比较性。<details>
<summary>Abstract</summary>
We provide an implementation to compute the flat metric in any dimension. The flat metric, also called dual bounded Lipschitz distance, generalizes the well-known Wasserstein distance W1 to the case that the distributions are of unequal total mass. This is of particular interest for unbalanced optimal transport tasks and for the analysis of data distributions where the sample size is important or normalization is not possible. The core of the method is based on a neural network to determine on optimal test function realizing the distance between two given measures. Special focus was put on achieving comparability of pairwise computed distances from independently trained networks. We tested the quality of the output in several experiments where ground truth was available as well as with simulated data.
</details>
<details>
<summary>摘要</summary>
我们提供了一个实现方法来计算任何维度的扁平度量。扁平度量，也被称为双重约束点距离，将水星拓扑距离W1推广到分布是不均匀的情况下。这对于不均匀优化运输任务和资料分布分析中是非常有兴趣的。我们的核心方法是使用神经网络来决定两个给定的概率分布之间的距离。我们特别强调了独立训练的网络之间的比较可靠性。我们在许多实验中评估了结果，包括可用的基准值和实验数据。
</details></li>
</ul>
<hr>
<h2 id="Three-Factors-to-Improve-Out-of-Distribution-Detection"><a href="#Three-Factors-to-Improve-Out-of-Distribution-Detection" class="headerlink" title="Three Factors to Improve Out-of-Distribution Detection"></a>Three Factors to Improve Out-of-Distribution Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01030">http://arxiv.org/abs/2308.01030</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hyunjun Choi, JaeHo Chung, Hawook Jeong, Jin Young Choi</li>
<li>for: 提高 OOD 检测性能和分类精度的方法</li>
<li>methods: 使用自知力塑化损失、半硬外围样本训练和超vised contrastive learning</li>
<li>results: 提高 OOD 检测性能和分类精度，缓解分类和 OOD 检测之间的负面关系<details>
<summary>Abstract</summary>
In the problem of out-of-distribution (OOD) detection, the usage of auxiliary data as outlier data for fine-tuning has demonstrated encouraging performance. However, previous methods have suffered from a trade-off between classification accuracy (ACC) and OOD detection performance (AUROC, FPR, AUPR). To improve this trade-off, we make three contributions: (i) Incorporating a self-knowledge distillation loss can enhance the accuracy of the network; (ii) Sampling semi-hard outlier data for training can improve OOD detection performance with minimal impact on accuracy; (iii) The introduction of our novel supervised contrastive learning can simultaneously improve OOD detection performance and the accuracy of the network. By incorporating all three factors, our approach enhances both accuracy and OOD detection performance by addressing the trade-off between classification and OOD detection. Our method achieves improvements over previous approaches in both performance metrics.
</details>
<details>
<summary>摘要</summary>
在 OUT-OF-DISTRIBUTION（OOD）检测问题中，使用辅助数据作为精度数据进行微调得到了鼓舞人的成绩。然而，先前的方法受到了准确率（ACC）和OOD检测性能（AUROC、FPR、AUPR）的负面交互。为了改进这种交互，我们提出了三项贡献：（i）将自我知识热释损失纳入网络准确性;（ii）在训练中采用半硬度异常数据抽样可以提高OOD检测性能，无需影响准确率;（iii）我们提出的新的监督对比学习可以同时提高OOD检测性能和网络准确性。通过结合这三个因素，我们的方法可以同时提高准确率和OOD检测性能，解决准确率和OOD检测之间的负面交互。我们的方法在性能指标上超越先前的方法。
</details></li>
</ul>
<hr>
<h2 id="Maximizing-Success-Rate-of-Payment-Routing-using-Non-stationary-Bandits"><a href="#Maximizing-Success-Rate-of-Payment-Routing-using-Non-stationary-Bandits" class="headerlink" title="Maximizing Success Rate of Payment Routing using Non-stationary Bandits"></a>Maximizing Success Rate of Payment Routing using Non-stationary Bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01028">http://arxiv.org/abs/2308.01028</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aayush Chaudhary, Abhinav Rai, Abhishek Gupta</li>
<li>For: This paper discusses the design and deployment of a non-stationary multi-armed bandit approach to determine a near-optimal payment routing policy based on recent transaction history.* Methods: The proposed Routing Service architecture uses a novel Ray-based implementation to optimize payment routing for over 10,000 transactions per second, while adhering to PCI DSS system design requirements and ecosystem constraints.* Results: The authors evaluate the effectiveness of multiple bandit-based payment routing algorithms on a custom simulator and demonstrate consistent improvement in transaction success rate (0.92%) compared to traditional rule-based methods over one month in live experiments on a fantasy sports platform.<details>
<summary>Abstract</summary>
This paper discusses the system architecture design and deployment of non-stationary multi-armed bandit approaches to determine a near-optimal payment routing policy based on the recent history of transactions. We propose a Routing Service architecture using a novel Ray-based implementation for optimally scaling bandit-based payment routing to over 10000 transactions per second, adhering to the system design requirements and ecosystem constraints with Payment Card Industry Data Security Standard (PCI DSS). We first evaluate the effectiveness of multiple bandit-based payment routing algorithms on a custom simulator to benchmark multiple non-stationary bandit approaches and identify the best hyperparameters. We then conducted live experiments on the payment transaction system on a fantasy sports platform Dream11. In the live experiments, we demonstrated that our non-stationary bandit-based algorithm consistently improves the success rate of transactions by 0.92\% compared to the traditional rule-based methods over one month.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Enhancing-Representation-Learning-for-Periodic-Time-Series-with-Floss-A-Frequency-Domain-Regularization-Approach"><a href="#Enhancing-Representation-Learning-for-Periodic-Time-Series-with-Floss-A-Frequency-Domain-Regularization-Approach" class="headerlink" title="Enhancing Representation Learning for Periodic Time Series with Floss: A Frequency Domain Regularization Approach"></a>Enhancing Representation Learning for Periodic Time Series with Floss: A Frequency Domain Regularization Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01011">http://arxiv.org/abs/2308.01011</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/agustdd/floss">https://github.com/agustdd/floss</a></li>
<li>paper_authors: Chunwei Yang, Xiaoxu Chen, Lijun Sun, Hongyu Yang, Yuankai Wu</li>
<li>for: 本研究旨在提出一种自适应 periodic regularization 方法，以提高深度学习模型对时间序列数据的表征。</li>
<li>methods: 该方法首先自动检测时间序列中主要的周期性，然后使用周期偏移和频谱浓度相似度度量来学习具有周期性的表征。</li>
<li>results: 通过对常见时间序列分类、预测和异常检测任务进行广泛的实验，我们证明了 Floss 方法可以自动捕捉时间序列中的周期性，并在深度学习模型中提高表征性能。<details>
<summary>Abstract</summary>
Time series analysis is a fundamental task in various application domains, and deep learning approaches have demonstrated remarkable performance in this area. However, many real-world time series data exhibit significant periodic or quasi-periodic dynamics that are often not adequately captured by existing deep learning-based solutions. This results in an incomplete representation of the underlying dynamic behaviors of interest. To address this gap, we propose an unsupervised method called Floss that automatically regularizes learned representations in the frequency domain. The Floss method first automatically detects major periodicities from the time series. It then employs periodic shift and spectral density similarity measures to learn meaningful representations with periodic consistency. In addition, Floss can be easily incorporated into both supervised, semi-supervised, and unsupervised learning frameworks. We conduct extensive experiments on common time series classification, forecasting, and anomaly detection tasks to demonstrate the effectiveness of Floss. We incorporate Floss into several representative deep learning solutions to justify our design choices and demonstrate that it is capable of automatically discovering periodic dynamics and improving state-of-the-art deep learning models.
</details>
<details>
<summary>摘要</summary>
时间序列分析是各个应用领域的基本任务，深度学习方法在这个领域表现出了惊人的表现。然而，许多实际时间序列数据具有重要的周期或半周期动力学特性，这些特性经常由现有的深度学习基于解决方案不充分捕捉。这会导致时间序列中的下一步动力学行为的捕捉不准确。为解决这个问题，我们提出了一种不supervised方法called Floss，它可以自动在频域中规范学习的结果。Floss方法首先自动检测时间序列中的主要周期性。然后，它使用周期偏移和频谱密度相似度度量来学习具有周期一致性的有意义表示。此外，Floss可以轻松地被 incorporated 到supervised、semi-supervised和Unsupervised learning框架中。我们在常见的时间序列分类、预测和异常检测任务中进行了广泛的实验，以示Floss的有效性。我们将Floss与一些代表性的深度学习解决方案结合，以证明我们的设计选择和证明Floss可以自动找到周期动力学和提高现有的深度学习模型。
</details></li>
</ul>
<hr>
<h2 id="MDT3D-Multi-Dataset-Training-for-LiDAR-3D-Object-Detection-Generalization"><a href="#MDT3D-Multi-Dataset-Training-for-LiDAR-3D-Object-Detection-Generalization" class="headerlink" title="MDT3D: Multi-Dataset Training for LiDAR 3D Object Detection Generalization"></a>MDT3D: Multi-Dataset Training for LiDAR 3D Object Detection Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01000">http://arxiv.org/abs/2308.01000</a></li>
<li>repo_url: None</li>
<li>paper_authors: Louis Soum-Fontez, Jean-Emmanuel Deschaud, François Goulette</li>
<li>for: 这个研究旨在提高3D物体检测模型在新环境中的Robustness，使其能够更好地适应不同的感测器配置和数据来源。</li>
<li>methods: 这个研究使用了Multi-Dataset Training for 3D Object Detection（MDT3D）方法，利用多个标注的源数据集来增强3D物体检测模型的适应能力。另外，这个方法还使用了一新的标签映射技术来填充标签之间的差异，以及一种新的跨数据集增强技术：跨数据集物体注入。</li>
<li>results: 这个研究发现，这个MDT3D方法可以对不同类型的3D物体检测模型进行改进，并且可以增强这些模型在新环境中的适应能力。<details>
<summary>Abstract</summary>
Supervised 3D Object Detection models have been displaying increasingly better performance in single-domain cases where the training data comes from the same environment and sensor as the testing data. However, in real-world scenarios data from the target domain may not be available for finetuning or for domain adaptation methods. Indeed, 3D object detection models trained on a source dataset with a specific point distribution have shown difficulties in generalizing to unseen datasets. Therefore, we decided to leverage the information available from several annotated source datasets with our Multi-Dataset Training for 3D Object Detection (MDT3D) method to increase the robustness of 3D object detection models when tested in a new environment with a different sensor configuration. To tackle the labelling gap between datasets, we used a new label mapping based on coarse labels. Furthermore, we show how we managed the mix of datasets during training and finally introduce a new cross-dataset augmentation method: cross-dataset object injection. We demonstrate that this training paradigm shows improvements for different types of 3D object detection models. The source code and additional results for this research project will be publicly available on GitHub for interested parties to access and utilize: https://github.com/LouisSF/MDT3D
</details>
<details>
<summary>摘要</summary>
超级vised 3D对象检测模型在单个领域情况下显示出了不断提高的性能，其中训练数据和测试数据来自同一个环境和感知器。然而，在实际应用场景中，目标领域的数据可能无法用于训练或适应方法。实际上，3D对象检测模型在特定点分布的训练集上显示了难以泛化到未看过的集合。因此，我们决定利用多个注释源集合的信息来提高3D对象检测模型在新环境中测试时的Robustness。为了 bridge label gap между集合，我们使用了新的标签映射基于粗略标签。此外，我们详细介绍了在训练中管理多个集合的方法，并引入了一种新的交叉集 augmentation 方法：交叉集对象注入。我们表明这种训练方法在不同类型的3D对象检测模型上显示出了改进。详细的源代码和更多结果将在 GitHub 上公开，欢迎有兴趣的朋友们来到 GitHub 上获取和利用：https://github.com/LouisSF/MDT3D。
</details></li>
</ul>
<hr>
<h2 id="Exploiting-Synthetic-Data-for-Data-Imbalance-Problems-Baselines-from-a-Data-Perspective"><a href="#Exploiting-Synthetic-Data-for-Data-Imbalance-Problems-Baselines-from-a-Data-Perspective" class="headerlink" title="Exploiting Synthetic Data for Data Imbalance Problems: Baselines from a Data Perspective"></a>Exploiting Synthetic Data for Data Imbalance Problems: Baselines from a Data Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00994">http://arxiv.org/abs/2308.00994</a></li>
<li>repo_url: None</li>
<li>paper_authors: Moon Ye-Bin, Nam Hyeon-Woo, Wonseok Choi, Nayeong Kim, Suha Kwak, Tae-Hyun Oh</li>
<li>for: addressing the challenges of data imbalance in deep neural networks</li>
<li>methods: utilizes synthetic data as a preliminary step before employing task-specific algorithms</li>
<li>results: impressive performance on various datasets, surpassing existing task-specific methods<details>
<summary>Abstract</summary>
We live in a vast ocean of data, and deep neural networks are no exception to this. However, this data exhibits an inherent phenomenon of imbalance. This imbalance poses a risk of deep neural networks producing biased predictions, leading to potentially severe ethical and social consequences. To address these challenges, we believe that the use of generative models is a promising approach for comprehending tasks, given the remarkable advancements demonstrated by recent diffusion models in generating high-quality images. In this work, we propose a simple yet effective baseline, SYNAuG, that utilizes synthetic data as a preliminary step before employing task-specific algorithms to address data imbalance problems. This straightforward approach yields impressive performance on datasets such as CIFAR100-LT, ImageNet100-LT, UTKFace, and Waterbird, surpassing the performance of existing task-specific methods. While we do not claim that our approach serves as a complete solution to the problem of data imbalance, we argue that supplementing the existing data with synthetic data proves to be an effective and crucial preliminary step in addressing data imbalance concerns.
</details>
<details>
<summary>摘要</summary>
我们生活在一个庞大的数据海洋中，深度神经网络也不例外。然而，这些数据具有内在的不均衡现象，这可能导致深度神经网络预测结果受到偏见的影响，从而导致严重的伦理和社会问题。为解决这些挑战，我们认为使用生成模型是一种有前途的方法，因为最近的扩散模型在生成高质量图像方面已经达到了很高的水平。在这个工作中，我们提出了一个简单 yet有效的基线方案，即SYNAuG，它利用生成的数据作为先决步骤，然后使用任务特定的算法来解决数据不均衡问题。这种简单的方法在CIFAR100-LT、ImageNet100-LT、UTKFace和Waterbird等数据集上达到了较高的性能，超过了现有的任务特定方法的表现。虽然我们不能声称我们的方法是完全解决数据不均衡问题的解决方案，但我们认为在补充现有数据时使用生成的数据是一种有效和重要的先决步骤。
</details></li>
</ul>
<hr>
<h2 id="Wasserstein-Diversity-Enriched-Regularizer-for-Hierarchical-Reinforcement-Learning"><a href="#Wasserstein-Diversity-Enriched-Regularizer-for-Hierarchical-Reinforcement-Learning" class="headerlink" title="Wasserstein Diversity-Enriched Regularizer for Hierarchical Reinforcement Learning"></a>Wasserstein Diversity-Enriched Regularizer for Hierarchical Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00989">http://arxiv.org/abs/2308.00989</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haorui Li, Jiaqi Liang, Linjing Li, Daniel Zeng</li>
<li>for: 这个论文旨在提出一种新的自动发现策略，用于解决现有的升降问题，以提高复杂任务的执行性。</li>
<li>methods: 该论文使用了 Hierarchical Reinforcement Learning（层次优化学习）和 Automated Subpolicies Discovery（自动发现策略）等方法，并提出了一种新的任务无关的正则化器 called Wasserstein Diversity-Enriched Regularizer（WDER），用于提高策略的多样性。</li>
<li>results: 实验结果表明，使用提出的WDER可以更好地提高性能和样本效率，而无需修改超参数，这表明了WDER的可应用性和稳定性。<details>
<summary>Abstract</summary>
Hierarchical reinforcement learning composites subpolicies in different hierarchies to accomplish complex tasks.Automated subpolicies discovery, which does not depend on domain knowledge, is a promising approach to generating subpolicies.However, the degradation problem is a challenge that existing methods can hardly deal with due to the lack of consideration of diversity or the employment of weak regularizers. In this paper, we propose a novel task-agnostic regularizer called the Wasserstein Diversity-Enriched Regularizer (WDER), which enlarges the diversity of subpolicies by maximizing the Wasserstein distances among action distributions. The proposed WDER can be easily incorporated into the loss function of existing methods to boost their performance further.Experimental results demonstrate that our WDER improves performance and sample efficiency in comparison with prior work without modifying hyperparameters, which indicates the applicability and robustness of the WDER.
</details>
<details>
<summary>摘要</summary>
hierarchical reinforcement learning 将不同层次的 subpolicies  compose 以完成复杂任务。自动发现 subpolicies 的方法，不виси于Domain Knowledge，是一种有前途的approach。然而，降低问题是现有方法难以处理的挑战，这是因为现有方法缺乏多样性或使用弱正则化。在这篇论文中，我们提出了一种新的任务无关的正则化方法，called Wasserstein Diversity-Enriched Regularizer (WDER)，它通过最大化 Wasserstein 距离来增大 subpolicies 的多样性。我们的 WDER 可以轻松地与现有方法的损失函数结合使用，以提高其性能。实验结果表明，我们的 WDER 可以提高性能和样本效率，而无需修改超参数，这表明了我们的 WDER 的可行性和可靠性。
</details></li>
</ul>
<hr>
<h2 id="Learning-Regionalization-within-a-Differentiable-High-Resolution-Hydrological-Model-using-Accurate-Spatial-Cost-Gradients"><a href="#Learning-Regionalization-within-a-Differentiable-High-Resolution-Hydrological-Model-using-Accurate-Spatial-Cost-Gradients" class="headerlink" title="Learning Regionalization within a Differentiable High-Resolution Hydrological Model using Accurate Spatial Cost Gradients"></a>Learning Regionalization within a Differentiable High-Resolution Hydrological Model using Accurate Spatial Cost Gradients</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02040">http://arxiv.org/abs/2308.02040</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ngo Nghi Truyen Huynh, Pierre-André Garambois, François Colleoni, Benjamin Renard, Hélène Roux, Julie Demargne, Pierre Javelle<br>for:This paper is written to address the challenging problem of estimating spatially distributed hydrological parameters in ungauged catchments, and to propose a new approach called Hybrid Data Assimilation and Parameter Regionalization (HDA-PR) that incorporates learnable regionalization mappings.methods:The HDA-PR approach uses a differentiable hydrological model and incorporates multivariate regressions or neural networks to regionalize the model parameters. The approach also uses adjoint-based gradients to account for information from multiple observation sites and to tackle the inverse problem of calibrating the model parameters.results:The HDA-PR approach was tested on two flash-flood-prone areas in the South of France, and the results showed a strong regionalization performance, with median Nash-Sutcliffe efficiency (NSE) scores ranging from 0.52 to 0.78 at pseudo-ungauged sites over calibration and validation periods. The approach improved the NSE by up to 0.57 compared to the baseline model calibrated with lumped parameters, and achieved a performance comparable to the reference solution obtained with local uniform calibration.<details>
<summary>Abstract</summary>
Estimating spatially distributed hydrological parameters in ungauged catchments poses a challenging regionalization problem and requires imposing spatial constraints given the sparsity of discharge data. A possible approach is to search for a transfer function that quantitatively relates physical descriptors to conceptual model parameters. This paper introduces a Hybrid Data Assimilation and Parameter Regionalization (HDA-PR) approach incorporating learnable regionalization mappings, based on either multivariate regressions or neural networks, into a differentiable hydrological model. It enables the exploitation of heterogeneous datasets across extensive spatio-temporal computational domains within a high-dimensional regionalization context, using accurate adjoint-based gradients. The inverse problem is tackled with a multi-gauge calibration cost function accounting for information from multiple observation sites. HDA-PR was tested on high-resolution, hourly and kilometric regional modeling of two flash-flood-prone areas located in the South of France. In both study areas, the median Nash-Sutcliffe efficiency (NSE) scores ranged from 0.52 to 0.78 at pseudo-ungauged sites over calibration and validation periods. These results highlight a strong regionalization performance of HDA-PR, improving NSE by up to 0.57 compared to the baseline model calibrated with lumped parameters, and achieving a performance comparable to the reference solution obtained with local uniform calibration (median NSE from 0.59 to 0.79). Multiple evaluation metrics based on flood-oriented hydrological signatures are also employed to assess the accuracy and robustness of the approach. The regionalization method is amenable to state-parameter correction from multi-source data over a range of time scales needed for operational data assimilation, and it is adaptable to other differentiable geophysical models.
</details>
<details>
<summary>摘要</summary>
估算分布式ydrological参数在无测站catchments中存在一个挑战性的区域化问题，需要在缺乏流量数据的情况下强制实施空间约束。本文提出了一种Hybrid Data Assimilation and Parameter Regionalization（HDA-PR）方法，该方法将学习的区域化映射纳入可微分ydrological模型中，使得在广泛的空间和时间域内对高维区域化问题进行可靠的逻辑拟合。该方法可以利用多种多样的数据来源，包括多个观测站，并且可以在不同的时间尺度上进行数据适应。在高分辨率、小时间间隔和kilometric级别上对两个暴雨洪水频发区域进行了实验，结果表明HDA-PR方法在 pseudo-无测站 sites 上的 median Nash-Sutcliffe efficiency（NSE）分布在0.52-0.78之间，比基线模型强制Parameter regionalization 提高了0.57个NSE分数。这些结果表明HDA-PR方法在区域化性能方面具有强大的表现，与参照解（median NSE从0.59到0.79）相当。此外，多种流水学量的评估指标也被利用，以评估方法的准确性和稳定性。该区域化方法可以在多个数据源上进行状态-参数修正，并且适用于其他可微分地球物理模型。
</details></li>
</ul>
<hr>
<h2 id="Certified-Multi-Fidelity-Zeroth-Order-Optimization"><a href="#Certified-Multi-Fidelity-Zeroth-Order-Optimization" class="headerlink" title="Certified Multi-Fidelity Zeroth-Order Optimization"></a>Certified Multi-Fidelity Zeroth-Order Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00978">http://arxiv.org/abs/2308.00978</a></li>
<li>repo_url: None</li>
<li>paper_authors: Étienne de Montbrun, Sébastien Gerchinovitz</li>
<li>for: 这个论文主要针对的问题是多级别质量零次优化问题，即可以评估函数 f 在不同级别的 aproximation 水平（具有不同的成本），并且目标是使用最低成本来优化 f。</li>
<li>methods: 这个论文提出了证明的算法，即在证明算法和评估环境之间的游戏中，对函数 f 进行优化。这种证明算法的成本复杂度被证明为对于任何 lipschitz 函数 f，是near-optimal的。</li>
<li>results: 这个论文提出了一种证明算法，并且证明了这种算法的成本复杂度在对于任何 lipschitz 函数 f 时是near-optimal的。此外，论文还证明了这种算法在噪声评估环境下的性能。<details>
<summary>Abstract</summary>
We consider the problem of multi-fidelity zeroth-order optimization, where one can evaluate a function $f$ at various approximation levels (of varying costs), and the goal is to optimize $f$ with the cheapest evaluations possible. In this paper, we study \emph{certified} algorithms, which are additionally required to output a data-driven upper bound on the optimization error. We first formalize the problem in terms of a min-max game between an algorithm and an evaluation environment. We then propose a certified variant of the MFDOO algorithm and derive a bound on its cost complexity for any Lipschitz function $f$. We also prove an $f$-dependent lower bound showing that this algorithm has a near-optimal cost complexity. We close the paper by addressing the special case of noisy (stochastic) evaluations as a direct example.
</details>
<details>
<summary>摘要</summary>
我们考虑多项调度零项优化问题，其中可以评估函数 $f$ 在不同的近似水平（具有不同的成本）上，并且目标是将 $f$ 优化到最低成本为可能。在这篇论文中，我们研究认证算法，它们还需要输出一个基于数据的上限 bounds 估计优化误差。我们首先将问题正式化为一个算法和评估环境之间的最小最大游戏。然后，我们提出了认证版本的 MFDOO 算法，并derive了这个算法对任何 Lipschitz 函数 $f$ 的成本复杂度上限。此外，我们证明了一个 $f$-dependent 下界，证明这个算法具有近乎最佳的成本复杂度。最后，我们处理了随机（测度）评估的特殊情况作为直接例子。
</details></li>
</ul>
<hr>
<h2 id="A-new-approach-for-evaluating-internal-cluster-validation-indices"><a href="#A-new-approach-for-evaluating-internal-cluster-validation-indices" class="headerlink" title="A new approach for evaluating internal cluster validation indices"></a>A new approach for evaluating internal cluster validation indices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03894">http://arxiv.org/abs/2308.03894</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zoltán Botta-Dukát</li>
<li>for: 本研究旨在提供一种内部验证指标来选择最佳的无监督分类算法和参数设置。</li>
<li>methods: 本研究评论了一些内部验证指标的方法，包括使用known cluster structure的数据集进行评估。</li>
<li>results: 本研究提出了一种新的验证方法，并评估了其优劣点。<details>
<summary>Abstract</summary>
A vast number of different methods are available for unsupervised classification. Since no algorithm and parameter setting performs best in all types of data, there is a need for cluster validation to select the actually best-performing algorithm. Several indices were proposed for this purpose without using any additional (external) information. These internal validation indices can be evaluated by applying them to classifications of datasets with a known cluster structure. Evaluation approaches differ in how they use the information on the ground-truth classification. This paper reviews these approaches, considering their advantages and disadvantages, and then suggests a new approach.
</details>
<details>
<summary>摘要</summary>
很多不同的方法可以用于无监督分类。由于不同的算法和参数设置在不同的数据上不一定都表现最佳，因此需要使用集群验证来选择实际最佳表现的算法。许多内部验证指标已经被提议，但这些指标不使用外部信息。这些验证指标可以通过应用于知道的分类结构的数据来评估。本文将评论这些方法，包括其优缺点，然后建议一种新的方法。Here's the translation of the text with some additional information about the Simplified Chinese translation:Simplified Chinese is a romanization of Mandarin Chinese that uses a simplified set of characters and grammar rules to represent the language. It is widely used in mainland China and other countries where Mandarin is spoken.In the translation, I have used the Simplified Chinese characters and grammar to represent the text. However, please note that the translation may not be exact, as there are some nuances and subtleties in the original text that may not be fully captured by the Simplified Chinese translation.
</details></li>
</ul>
<hr>
<h2 id="Effects-of-Daily-News-Sentiment-on-Stock-Price-Forecasting"><a href="#Effects-of-Daily-News-Sentiment-on-Stock-Price-Forecasting" class="headerlink" title="Effects of Daily News Sentiment on Stock Price Forecasting"></a>Effects of Daily News Sentiment on Stock Price Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08549">http://arxiv.org/abs/2308.08549</a></li>
<li>repo_url: None</li>
<li>paper_authors: S. Srinivas, R. Gadela, R. Sabu, A. Das, G. Nath, V. Datla</li>
<li>for: 这篇论文的目的是设计一个高效的新闻听说系统，以捕捉NITY50股票的新闻听说，并研究这些新闻听说对股票价格的影响。</li>
<li>methods: 这篇论文使用了一个Robust数据采集和处理框架，创建了一个新闻数据库，包含了约半百万篇新闻文章，以及相应的股票价格信息。其中，使用了不同的情感库计算不同部分的情感分数。</li>
<li>results: 这篇论文通过使用不同的LSTM模型，对股票价格进行预测，并 Compares their performances with and without using sentiment scores as features.<details>
<summary>Abstract</summary>
Predicting future prices of a stock is an arduous task to perform. However, incorporating additional elements can significantly improve our predictions, rather than relying solely on a stock's historical price data to forecast its future price. Studies have demonstrated that investor sentiment, which is impacted by daily news about the company, can have a significant impact on stock price swings. There are numerous sources from which we can get this information, but they are cluttered with a lot of noise, making it difficult to accurately extract the sentiments from them. Hence the focus of our research is to design an efficient system to capture the sentiments from the news about the NITY50 stocks and investigate how much the financial news sentiment of these stocks are affecting their prices over a period of time. This paper presents a robust data collection and preprocessing framework to create a news database for a timeline of around 3.7 years, consisting of almost half a million news articles. We also capture the stock price information for this timeline and create multiple time series data, that include the sentiment scores from various sections of the article, calculated using different sentiment libraries. Based on this, we fit several LSTM models to forecast the stock prices, with and without using the sentiment scores as features and compare their performances.
</details>
<details>
<summary>摘要</summary>
Our research focuses on designing an efficient system to capture sentiments from news about NITY50 stocks and investigating how much financial news sentiment affects their prices over time. We present a robust data collection and preprocessing framework to create a news database spanning 3.7 years, consisting of nearly half a million news articles. We also collect stock price information for this timeline and create multiple time series data, including sentiment scores from various sections of the article calculated using different sentiment libraries.We fit several LSTM models to forecast stock prices, with and without using sentiment scores as features, and compare their performances. This paper provides a comprehensive framework for incorporating investor sentiment into stock price predictions, and demonstrates the effectiveness of our approach using a large and diverse dataset.
</details></li>
</ul>
<hr>
<h2 id="Integrating-Homomorphic-Encryption-and-Trusted-Execution-Technology-for-Autonomous-and-Confidential-Model-Refining-in-Cloud"><a href="#Integrating-Homomorphic-Encryption-and-Trusted-Execution-Technology-for-Autonomous-and-Confidential-Model-Refining-in-Cloud" class="headerlink" title="Integrating Homomorphic Encryption and Trusted Execution Technology for Autonomous and Confidential Model Refining in Cloud"></a>Integrating Homomorphic Encryption and Trusted Execution Technology for Autonomous and Confidential Model Refining in Cloud</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00963">http://arxiv.org/abs/2308.00963</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pinglan Liu, Wensheng Zhang</li>
<li>For: This paper aims to design a scheme for autonomous and confidential model refining in cloud computing, to address security and privacy concerns while continuously improving model accuracy.* Methods: The proposed scheme utilizes homomorphic encryption and trusted execution environment technology to protect confidentiality during autonomous computation, and integrates these two techniques to enhance the scheme’s feasibility and efficiency.* Results: The proposed scheme allows the cloud server to autonomously refine an encrypted model with newly provided encrypted training data, continuously improving its accuracy. However, the efficiency is still lower than the baseline scheme, and can be improved by fully utilizing the higher level of parallelism and computational power of GPU at the cloud server.Here is the Chinese version of the three key points:* For: 本文提出了一种用于云计算中自动化和保密模型优化的方案，以解决安全和隐私问题，同时不断提高模型精度。* Methods: 该方案利用了同知加密和可信执行环境技术来保护数据和模型的Confidentiality，并将这两种技术相互衔接以提高方案的可行性和效率。* Results: 该方案使得云服务器可以自动地使用新提供的加密训练数据来加密模型，不断提高其精度。然而，效率仍然远低于基线方案，可以通过全面利用云服务器的高级并行和GPU的计算能力来进一步提高效率。<details>
<summary>Abstract</summary>
With the popularity of cloud computing and machine learning, it has been a trend to outsource machine learning processes (including model training and model-based inference) to cloud. By the outsourcing, other than utilizing the extensive and scalable resource offered by the cloud service provider, it will also be attractive to users if the cloud servers can manage the machine learning processes autonomously on behalf of the users. Such a feature will be especially salient when the machine learning is expected to be a long-term continuous process and the users are not always available to participate. Due to security and privacy concerns, it is also desired that the autonomous learning preserves the confidentiality of users' data and models involved. Hence, in this paper, we aim to design a scheme that enables autonomous and confidential model refining in cloud. Homomorphic encryption and trusted execution environment technology can protect confidentiality for autonomous computation, but each of them has their limitations respectively and they are complementary to each other. Therefore, we further propose to integrate these two techniques in the design of the model refining scheme. Through implementation and experiments, we evaluate the feasibility of our proposed scheme. The results indicate that, with our proposed scheme the cloud server can autonomously refine an encrypted model with newly provided encrypted training data to continuously improve its accuracy. Though the efficiency is still significantly lower than the baseline scheme that refines plaintext-model with plaintext-data, we expect that it can be improved by fully utilizing the higher level of parallelism and the computational power of GPU at the cloud server.
</details>
<details>
<summary>摘要</summary>
With the popularity of cloud computing and machine learning, it has become a trend to outsource machine learning processes (including model training and model-based inference) to the cloud. By outsourcing, users can not only utilize the extensive and scalable resources offered by the cloud service provider but also enjoy the convenience of autonomous management of machine learning processes by the cloud servers. This feature is especially important when machine learning is expected to be a long-term continuous process and users are not always available to participate. However, due to security and privacy concerns, it is essential to ensure that the autonomous learning preserves the confidentiality of users' data and models involved. Therefore, in this paper, we aim to design a scheme that enables autonomous and confidential model refining in the cloud. Homomorphic encryption and trusted execution environment technology can protect the confidentiality of autonomous computation, but each of them has its limitations, respectively. Therefore, we propose to integrate these two techniques in the design of the model refining scheme. Through implementation and experiments, we evaluate the feasibility of our proposed scheme. The results show that the cloud server can autonomously refine an encrypted model with newly provided encrypted training data to continuously improve its accuracy. Although the efficiency is still significantly lower than the baseline scheme that refines plaintext-model with plaintext-data, we expect that it can be improved by fully utilizing the higher level of parallelism and the computational power of GPU at the cloud server.Here's the translation in Traditional Chinese as well: With the popularity of cloud computing and machine learning, it has become a trend to outsource machine learning processes (including model training and model-based inference) to the cloud. By outsourcing, users can not only utilize the extensive and scalable resources offered by the cloud service provider but also enjoy the convenience of autonomous management of machine learning processes by the cloud servers. This feature is especially important when machine learning is expected to be a long-term continuous process and users are not always available to participate. However, due to security and privacy concerns, it is essential to ensure that the autonomous learning preserves the confidentiality of users' data and models involved. Therefore, in this paper, we aim to design a scheme that enables autonomous and confidential model refining in the cloud. Homomorphic encryption and trusted execution environment technology can protect the confidentiality of autonomous computation, but each of them has its limitations, respectively. Therefore, we propose to integrate these two techniques in the design of the model refining scheme. Through implementation and experiments, we evaluate the feasibility of our proposed scheme. The results show that the cloud server can autonomously refine an encrypted model with newly provided encrypted training data to continuously improve its accuracy. Although the efficiency is still significantly lower than the baseline scheme that refines plaintext-model with plaintext-data, we expect that it can be improved by fully utilizing the higher level of parallelism and the computational power of GPU at the cloud server.
</details></li>
</ul>
<hr>
<h2 id="Causal-Inference-with-Differentially-Private-Clustered-Outcomes"><a href="#Causal-Inference-with-Differentially-Private-Clustered-Outcomes" class="headerlink" title="Causal Inference with Differentially Private (Clustered) Outcomes"></a>Causal Inference with Differentially Private (Clustered) Outcomes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00957">http://arxiv.org/abs/2308.00957</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adel Javanmard, Vahab Mirrokni, Jean Pouget-Abadie</li>
<li>For: The paper focuses on developing a new differential privacy mechanism, “Cluster-DP”, to improve the variance of causal effect estimation while maintaining strong privacy guarantees.* Methods: The proposed method leverages any given cluster structure of the data to reduce the variance loss and improve the privacy guarantees. The algorithm uses a novel privacy-variance trade-off to achieve a balance between privacy and accuracy.* Results: The paper shows that the proposed “Cluster-DP” algorithm outperforms its unclustered version and a more extreme uniform-prior version in terms of variance loss, while maintaining the same privacy guarantees. The results demonstrate the effectiveness of the proposed method in achieving lower variance for stronger privacy guarantees.<details>
<summary>Abstract</summary>
Estimating causal effects from randomized experiments is only feasible if participants agree to reveal their potentially sensitive responses. Of the many ways of ensuring privacy, label differential privacy is a widely used measure of an algorithm's privacy guarantee, which might encourage participants to share responses without running the risk of de-anonymization. Many differentially private mechanisms inject noise into the original data-set to achieve this privacy guarantee, which increases the variance of most statistical estimators and makes the precise measurement of causal effects difficult: there exists a fundamental privacy-variance trade-off to performing causal analyses from differentially private data. With the aim of achieving lower variance for stronger privacy guarantees, we suggest a new differential privacy mechanism, "Cluster-DP", which leverages any given cluster structure of the data while still allowing for the estimation of causal effects. We show that, depending on an intuitive measure of cluster quality, we can improve the variance loss while maintaining our privacy guarantees. We compare its performance, theoretically and empirically, to that of its unclustered version and a more extreme uniform-prior version which does not use any of the original response distribution, both of which are special cases of the "Cluster-DP" algorithm.
</details>
<details>
<summary>摘要</summary>
估计 causal effect from randomized experiments 只能实现 if participants agree to reveal their potentially sensitive responses. 保护 participants' privacy 的多种方法之一是 label differential privacy, 这种方法可能会鼓励 participants 分享 responses  без running the risk of de-anonymization. 多种 differentially private mechanisms 会把 noise 注入到原始数据集中以实现这种 privacy guarantee, 这会增加 most statistical estimators 的变量和 makes the precise measurement of causal effects difficult. 在进行 causal analyses from differentially private data 时, there exists a fundamental privacy-variance trade-off.为了实现 lower variance for stronger privacy guarantees, we suggest a new differential privacy mechanism, "Cluster-DP", which leverages any given cluster structure of the data while still allowing for the estimation of causal effects. we show that, depending on an intuitive measure of cluster quality, we can improve the variance loss while maintaining our privacy guarantees. we compare its performance, theoretically and empirically, to that of its unclustered version and a more extreme uniform-prior version which does not use any of the original response distribution, both of which are special cases of the "Cluster-DP" algorithm.
</details></li>
</ul>
<hr>
<h2 id="Curriculum-Guided-Domain-Adaptation-in-the-Dark"><a href="#Curriculum-Guided-Domain-Adaptation-in-the-Dark" class="headerlink" title="Curriculum Guided Domain Adaptation in the Dark"></a>Curriculum Guided Domain Adaptation in the Dark</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00956">http://arxiv.org/abs/2308.00956</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chowdhury Sadman Jahan, Andreas Savakis</li>
<li>for: 这个研究的目的是为了解决隐私和安全性的问题，将黑盒模型训练到目标领域中而不需要任何源数据或源模型参数。</li>
<li>methods: 这个研究使用了curriculum guided adaptation approach，首先在目标数据上训练预设值很高的clean标签，然后在这些标签上训练另一个分支网络以抑制错误的累累。此外，研究使用了Jensen-Shannon数值来分类清洁和杂音标签，并且不需要任何额外的调整阶段。</li>
<li>results: 研究结果显示，CABB比现有的黑盒领域域整合模型优化性能更好，并且与白盒域整合模型优化性能相似。<details>
<summary>Abstract</summary>
Addressing the rising concerns of privacy and security, domain adaptation in the dark aims to adapt a black-box source trained model to an unlabeled target domain without access to any source data or source model parameters. The need for domain adaptation of black-box predictors becomes even more pronounced to protect intellectual property as deep learning based solutions are becoming increasingly commercialized. Current methods distill noisy predictions on the target data obtained from the source model to the target model, and/or separate clean/noisy target samples before adapting using traditional noisy label learning algorithms. However, these methods do not utilize the easy-to-hard learning nature of the clean/noisy data splits. Also, none of the existing methods are end-to-end, and require a separate fine-tuning stage and an initial warmup stage. In this work, we present Curriculum Adaptation for Black-Box (CABB) which provides a curriculum guided adaptation approach to gradually train the target model, first on target data with high confidence (clean) labels, and later on target data with noisy labels. CABB utilizes Jensen-Shannon divergence as a better criterion for clean-noisy sample separation, compared to the traditional criterion of cross entropy loss. Our method utilizes co-training of a dual-branch network to suppress error accumulation resulting from confirmation bias. The proposed approach is end-to-end trainable and does not require any extra finetuning stage, unlike existing methods. Empirical results on standard domain adaptation datasets show that CABB outperforms existing state-of-the-art black-box DA models and is comparable to white-box domain adaptation models.
</details>
<details>
<summary>摘要</summary>
In this work, we present Curriculum Adaptation for Black-Box (CABB), which provides a curriculum-guided adaptation approach to gradually train the target model. First, it trains the target model on target data with high confidence (clean) labels, and later on target data with noisy labels. CABB utilizes Jensen-Shannon divergence as a better criterion for clean-noisy sample separation compared to the traditional criterion of cross-entropy loss. Our method utilizes co-training of a dual-branch network to suppress error accumulation resulting from confirmation bias. The proposed approach is end-to-end trainable and does not require any extra fine-tuning stage, unlike existing methods.Empirical results on standard domain adaptation datasets show that CABB outperforms existing state-of-the-art black-box DA models and is comparable to white-box domain adaptation models.
</details></li>
</ul>
<hr>
<h2 id="From-Sparse-to-Soft-Mixtures-of-Experts"><a href="#From-Sparse-to-Soft-Mixtures-of-Experts" class="headerlink" title="From Sparse to Soft Mixtures of Experts"></a>From Sparse to Soft Mixtures of Experts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00951">http://arxiv.org/abs/2308.00951</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/google-research/vmoe">https://github.com/google-research/vmoe</a></li>
<li>paper_authors: Joan Puigcerver, Carlos Riquelme, Basil Mustafa, Neil Houlsby</li>
<li>for: 这 paper 的目的是提出一种可 diferenciable 的异步 Mixture of Experts（Soft MoE）模型，以解决 MoE 模型在训练和推理过程中的稳定性和效果问题。</li>
<li>methods: Soft MoE 使用了一种隐式软分配方法，通过将不同权重的输入 токен传递给每个专家，实现了隐藏状态的共享和模型的扩展。</li>
<li>results: 在视觉识别任务中，Soft MoE 与标准 Transformer 和 популяр的 MoE 变体（Token Choice 和 Experts Choice）进行了比较，并取得了substantially better的性能。例如，Soft MoE-Base&#x2F;16 只需要10.5倍的推理成本（5.7倍的wall-clock时间）和 ViT-Huge&#x2F;14 相同的训练效果。此外，Soft MoE 还可以扩展到大型模型，Soft MoE Huge&#x2F;14  WITH 128 专家在 16 个 MoE 层中，与 ViT Huge&#x2F;14 的参数数量相比，只有2% 的推理时间成本增加。<details>
<summary>Abstract</summary>
Sparse mixture of expert architectures (MoEs) scale model capacity without large increases in training or inference costs. Despite their success, MoEs suffer from a number of issues: training instability, token dropping, inability to scale the number of experts, or ineffective finetuning. In this work, we proposeSoft MoE, a fully-differentiable sparse Transformer that addresses these challenges, while maintaining the benefits of MoEs. Soft MoE performs an implicit soft assignment by passing different weighted combinations of all input tokens to each expert. As in other MoE works, experts in Soft MoE only process a subset of the (combined) tokens, enabling larger model capacity at lower inference cost. In the context of visual recognition, Soft MoE greatly outperforms standard Transformers (ViTs) and popular MoE variants (Tokens Choice and Experts Choice). For example, Soft MoE-Base/16 requires 10.5x lower inference cost (5.7x lower wall-clock time) than ViT-Huge/14 while matching its performance after similar training. Soft MoE also scales well: Soft MoE Huge/14 with 128 experts in 16 MoE layers has over 40x more parameters than ViT Huge/14, while inference time cost grows by only 2%, and it performs substantially better.
</details>
<details>
<summary>摘要</summary>
稀疏混合专家架构（MoE）可以增加模型容量而无需大幅提高训练或执行成本。尽管它们取得了成功，但MoE受到一些问题的限制：训练不稳定、减少токен、不能扩展专家数量、或者训练不 efective。在这项工作中，我们提出了软MoE，一种完全可微分的稀疏变换器，解决了这些挑战，同时保留MoE的优点。软MoE通过不同权重的Weighted combinations passing所有输入 токен给每个专家进行隐式软分配。与其他MoE工作一样，专家在Soft MoE中只处理一 subset of (combined)  токен，使得模型容量可以在更低的执行成本下增加。在视觉认知上，软MoE大幅超越标准转换器（ViTs）和流行的MoE变种（Tokens Choice和Experts Choice）。例如，Soft MoE-Base/16需要10.5倍更低的执行成本（5.7倍更低的墙 clock时间），与ViT-Huge/14匹配其性能，同时需要相似的训练。软MoE还可扩展：Soft MoE Huge/14 WITH 128专家在16个MoE层中有40倍更多的参数，而执行成本上增加的成本只增加2%，并且表现出substantially better。
</details></li>
</ul>
<hr>
<h2 id="Decomposing-and-Coupling-Saliency-Map-for-Lesion-Segmentation-in-Ultrasound-Images"><a href="#Decomposing-and-Coupling-Saliency-Map-for-Lesion-Segmentation-in-Ultrasound-Images" class="headerlink" title="Decomposing and Coupling Saliency Map for Lesion Segmentation in Ultrasound Images"></a>Decomposing and Coupling Saliency Map for Lesion Segmentation in Ultrasound Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00947">http://arxiv.org/abs/2308.00947</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenyuan Ning, Yixiao Mao, Qianjin Feng, Shengzhou Zhong, Yu Zhang</li>
<li>for: Accurate lesion segmentation in ultrasound images, particularly in the presence of complex background textures.</li>
<li>methods: Decomposition-coupling network (DC-Net) that disentangles foreground and background saliency maps, followed by fusion of regional features, relation-aware representation fusion, and dependency-aware prior incorporation.</li>
<li>results: Remarkable performance improvement over existing state-of-the-art methods on two ultrasound lesion segmentation tasks.<details>
<summary>Abstract</summary>
Complex scenario of ultrasound image, in which adjacent tissues (i.e., background) share similar intensity with and even contain richer texture patterns than lesion region (i.e., foreground), brings a unique challenge for accurate lesion segmentation. This work presents a decomposition-coupling network, called DC-Net, to deal with this challenge in a (foreground-background) saliency map disentanglement-fusion manner. The DC-Net consists of decomposition and coupling subnets, and the former preliminarily disentangles original image into foreground and background saliency maps, followed by the latter for accurate segmentation under the assistance of saliency prior fusion. The coupling subnet involves three aspects of fusion strategies, including: 1) regional feature aggregation (via differentiable context pooling operator in the encoder) to adaptively preserve local contextual details with the larger receptive field during dimension reduction; 2) relation-aware representation fusion (via cross-correlation fusion module in the decoder) to efficiently fuse low-level visual characteristics and high-level semantic features during resolution restoration; 3) dependency-aware prior incorporation (via coupler) to reinforce foreground-salient representation with the complementary information derived from background representation. Furthermore, a harmonic loss function is introduced to encourage the network to focus more attention on low-confidence and hard samples. The proposed method is evaluated on two ultrasound lesion segmentation tasks, which demonstrates the remarkable performance improvement over existing state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
复杂的ultrasound图像场景下，邻近组织（即背景）与病变区域（即前景）的INTENSITY具有相似的特征，甚至含有更加丰富的тексту征，这带来了精确病变部细分的搜索挑战。本文提出了一种 decomposition-coupling网络（DC-Net），通过对原始图像进行前期分解和聚合来解决这个挑战。DC-Net包括分解和聚合子网络，前者首先分解原始图像为病变和背景的saliency map，然后后者利用这些saliency map进行准确的病变部细分。聚合子网络包括三种融合策略：1）地域特征聚合（通过可微上下文抽取器在encoder中），以适应大尺寸场景下的本地Contextual details; 2）相关性认知表示融合（通过cross-correlation融合模块在decoder中），以高效地融合低级视觉特征和高级 semantics特征; 3）依赖关系的先天约束（通过coupler），以强制前景病变表示与背景表示之间的依赖关系。此外，我们还引入了一种和谐损失函数，以促进网络对低信任和困难样本的更多注意。提议的方法在两个ultrasound病变部细分任务上进行了评估，并显示出与现有状态 искус家技术的remarkable性能提升。
</details></li>
</ul>
<hr>
<h2 id="On-the-use-of-deep-learning-for-phase-recovery"><a href="#On-the-use-of-deep-learning-for-phase-recovery" class="headerlink" title="On the use of deep learning for phase recovery"></a>On the use of deep learning for phase recovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00942">http://arxiv.org/abs/2308.00942</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaiqiang Wang, Li Song, Chutian Wang, Zhenbo Ren, Guangyuan Zhao, Jiazhen Dou, Jianglei Di, George Barbastathis, Renjie Zhou, Jianlin Zhao, Edmund Y. Lam</li>
<li>for: 这篇论文主要是为了探讨深度学习在phas recovery（PR）方面的应用和支持。</li>
<li>methods: 论文首先简要介绍了传统的PR方法，然后详细介绍了深度学习在PR的三个阶段：先processing、进程中处理和后processing中的应用。</li>
<li>results: 论文则 Summarize了在DL中为PR工作的进展，并提供了一个live-updating资源（<a target="_blank" rel="noopener" href="https://github.com/kqwang/phase-recovery%EF%BC%89%EF%BC%8C%E4%BB%A5%E4%BE%BF%E8%AF%BB%E8%80%85%E6%9B%B4%E6%B7%B1%E5%85%A5%E4%BA%86%E8%A7%A3PR%E3%80%82">https://github.com/kqwang/phase-recovery），以便读者更深入了解PR。</a><details>
<summary>Abstract</summary>
Phase recovery (PR) refers to calculating the phase of the light field from its intensity measurements. As exemplified from quantitative phase imaging and coherent diffraction imaging to adaptive optics, PR is essential for reconstructing the refractive index distribution or topography of an object and correcting the aberration of an imaging system. In recent years, deep learning (DL), often implemented through deep neural networks, has provided unprecedented support for computational imaging, leading to more efficient solutions for various PR problems. In this review, we first briefly introduce conventional methods for PR. Then, we review how DL provides support for PR from the following three stages, namely, pre-processing, in-processing, and post-processing. We also review how DL is used in phase image processing. Finally, we summarize the work in DL for PR and outlook on how to better use DL to improve the reliability and efficiency in PR. Furthermore, we present a live-updating resource (https://github.com/kqwang/phase-recovery) for readers to learn more about PR.
</details>
<details>
<summary>摘要</summary>
phase recovery (PR) 指的是从光场强度测量中计算光场的阶段。例如从量化阶段影像和相干折射影像到自适应optics，PR 是重要的 для重建对象的反射指数分布或地图，并对投影系统的偏差进行修复。在最近几年，深度学习（DL），通常通过深度神经网络实现，对计算成像提供了前所未有的支持，导致了许多PR问题的更高效的解决方案。在这篇文章中，我们首先简要介绍了传统的PR方法。然后，我们查看了DL 如何在三个阶段提供支持，即预处理、实时处理和后处理。我们还查看了DL 在阶段图像处理中的应用。最后，我们summarize了DL 在PR 方面的工作，并讨论了如何更好地使用DL 提高PR 的可靠性和效率。此外，我们提供了一个Live-updating资源（https://github.com/kqwang/phase-recovery），让读者了解更多关于PR。
</details></li>
</ul>
<hr>
<h2 id="QUANT-A-Minimalist-Interval-Method-for-Time-Series-Classification"><a href="#QUANT-A-Minimalist-Interval-Method-for-Time-Series-Classification" class="headerlink" title="QUANT: A Minimalist Interval Method for Time Series Classification"></a>QUANT: A Minimalist Interval Method for Time Series Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00928">http://arxiv.org/abs/2308.00928</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/angus924/quant">https://github.com/angus924/quant</a></li>
<li>paper_authors: Angus Dempster, Daniel F. Schmidt, Geoffrey I. Webb</li>
<li>for: 这个论文是为了提出一种快速准确的时间序列分类方法。</li>
<li>methods: 这个方法使用了一种单种特征（量iles）、固定间隔和 ‘Off-the-shelf’ 分类器。</li>
<li>results: 这个方法可以在一个标准的测试集上达到与最高精度 interval 方法相同的准确率，并且在142个 UCR 数据集上达到了状态机器的准确率，用于训练和推断的总计算时间仅为15分钟左右，使用单个 CPU 核心。<details>
<summary>Abstract</summary>
We show that it is possible to achieve the same accuracy, on average, as the most accurate existing interval methods for time series classification on a standard set of benchmark datasets using a single type of feature (quantiles), fixed intervals, and an 'off the shelf' classifier. This distillation of interval-based approaches represents a fast and accurate method for time series classification, achieving state-of-the-art accuracy on the expanded set of 142 datasets in the UCR archive with a total compute time (training and inference) of less than 15 minutes using a single CPU core.
</details>
<details>
<summary>摘要</summary>
我们证明可以在标准的测试集上实现同等准确率，而且这种方法可以使用单种特征（quantiles）、固定间隔和 'off the shelf' 分类器实现时间序列分类。这种简化的间隔方法可以快速和准确地进行时间序列分类，在扩展的142个测试集上达到状态的最佳准确率，用单个CPU核心并在训练和推理之间共计使用了 menos than 15分钟的计算时间。
</details></li>
</ul>
<hr>
<h2 id="Continual-Domain-Adaptation-on-Aerial-Images-under-Gradually-Degrading-Weather"><a href="#Continual-Domain-Adaptation-on-Aerial-Images-under-Gradually-Degrading-Weather" class="headerlink" title="Continual Domain Adaptation on Aerial Images under Gradually Degrading Weather"></a>Continual Domain Adaptation on Aerial Images under Gradually Degrading Weather</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00924">http://arxiv.org/abs/2308.00924</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sadman-jahan/aid-ucm-degradingweather">https://github.com/sadman-jahan/aid-ucm-degradingweather</a></li>
<li>paper_authors: Chowdhury Sadman Jahan, Andreas Savakis<br>for:*  Mitigate the domain gap between the source domain and the target domain in aerial deployment.methods:*  Synthesize two gradually worsening weather conditions on real images from two existing aerial imagery datasets.*  Evaluate three DA models (baseline standard DA model and two continual DA models) under the continual, or test-time adaptation setting.results:*  Discover stability issues during adaptation for existing buffer-fed continual DA methods.*  Offer gradient normalization as a simple solution to curb training instability.<details>
<summary>Abstract</summary>
Domain adaptation (DA) strives to mitigate the domain gap between the source domain where a model is trained, and the target domain where the model is deployed. When a deep learning model is deployed on an aerial platform, it may face gradually degrading weather conditions during operation, leading to widening domain gaps between the training data and the encountered evaluation data. We synthesize two such gradually worsening weather conditions on real images from two existing aerial imagery datasets, generating a total of four benchmark datasets. Under the continual, or test-time adaptation setting, we evaluate three DA models on our datasets: a baseline standard DA model and two continual DA models. In such setting, the models can access only one small portion, or one batch of the target data at a time, and adaptation takes place continually, and over only one epoch of the data. The combination of the constraints of continual adaptation, and gradually deteriorating weather conditions provide the practical DA scenario for aerial deployment. Among the evaluated models, we consider both convolutional and transformer architectures for comparison. We discover stability issues during adaptation for existing buffer-fed continual DA methods, and offer gradient normalization as a simple solution to curb training instability.
</details>
<details>
<summary>摘要</summary>
域 adaptation (DA) 目标是减少源域和目标域之间的域度差。当深度学习模型在空中平台上部署时，它可能会遇到逐渐恶化的天气条件，导致模型在训练数据和评估数据之间的域度差加大。我们使用两个现有的空中影像集合拼接了两种逐渐恶化的天气条件，生成了四个benchmark集合。在 continual 或 test-time adaptation  Setting，我们评估了三种 DA 模型：基线标准 DA 模型和两种 continual DA 模型。在这种设定下，模型只能访问一小部分或一批目标数据一次，并且适应发生在一个epoch内。combine continual adaptation 和 gradually deteriorating 天气条件提供了实际的 DA enario for aerial deployment。我们考虑了 convolutional 和 transformer 架构进行比较。我们发现了 continual adaptation 时的稳定性问题，并提供了 Gradient normalization 的简单解决方案来缓解训练不稳定。
</details></li>
</ul>
<hr>
<h2 id="Survey-on-Computer-Vision-Techniques-for-Internet-of-Things-Devices"><a href="#Survey-on-Computer-Vision-Techniques-for-Internet-of-Things-Devices" class="headerlink" title="Survey on Computer Vision Techniques for Internet-of-Things Devices"></a>Survey on Computer Vision Techniques for Internet-of-Things Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02553">http://arxiv.org/abs/2308.02553</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ishmeet Kaur, Adwaita Janardhan Jadhav</li>
<li>for: 本研究旨在提高深度神经网络（DNNs）在低功耗设备上的部署，以提高公共安全。</li>
<li>methods: 本研究使用了低功耗和能效的DNN实现技术，包括神经网络压缩、网络架构搜索和设计、编译器和图优化等。</li>
<li>results: 本研究总结了低功耗DNN实现技术的优缺点和未来研究方向。<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) are state-of-the-art techniques for solving most computer vision problems. DNNs require billions of parameters and operations to achieve state-of-the-art results. This requirement makes DNNs extremely compute, memory, and energy-hungry, and consequently difficult to deploy on small battery-powered Internet-of-Things (IoT) devices with limited computing resources. Deployment of DNNs on Internet-of-Things devices, such as traffic cameras, can improve public safety by enabling applications such as automatic accident detection and emergency response.Through this paper, we survey the recent advances in low-power and energy-efficient DNN implementations that improve the deployability of DNNs without significantly sacrificing accuracy. In general, these techniques either reduce the memory requirements, the number of arithmetic operations, or both. The techniques can be divided into three major categories: neural network compression, network architecture search and design, and compiler and graph optimizations. In this paper, we survey both low-power techniques for both convolutional and transformer DNNs, and summarize the advantages, disadvantages, and open research problems.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Virtual-histological-staining-of-unlabeled-autopsy-tissue"><a href="#Virtual-histological-staining-of-unlabeled-autopsy-tissue" class="headerlink" title="Virtual histological staining of unlabeled autopsy tissue"></a>Virtual histological staining of unlabeled autopsy tissue</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00920">http://arxiv.org/abs/2308.00920</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuzhu Li, Nir Pillar, Jingxi Li, Tairan Liu, Di Wu, Songyu Sun, Guangdong Ma, Kevin de Haan, Luzhe Huang, Sepehr Hamidi, Anatoly Urisman, Tal Keidar Haran, William Dean Wallace, Jonathan E. Zuckerman, Aydogan Ozcan</li>
<li>for: 这个研究旨在解决传统压潮化方法面临的多种挑战，包括延迟了尸体组织的固定，以及大量的化学染料和大量的劳动力成本。</li>
<li>methods: 这个研究使用了训练 neural network 将自动染料图像转换成快速呈现的明亮场景图像，从而消除传统压潮化方法中的严重染料 artifacts。</li>
<li>results: 这个研究显示，虽然样本经历了严重的自杀和蛋白损害，但虚拟压潮化技术仍能快速和高效地生成 artifact-free H&amp;E 染料，并且可以减少劳动力成本和基础设施需求。<details>
<summary>Abstract</summary>
Histological examination is a crucial step in an autopsy; however, the traditional histochemical staining of post-mortem samples faces multiple challenges, including the inferior staining quality due to autolysis caused by delayed fixation of cadaver tissue, as well as the resource-intensive nature of chemical staining procedures covering large tissue areas, which demand substantial labor, cost, and time. These challenges can become more pronounced during global health crises when the availability of histopathology services is limited, resulting in further delays in tissue fixation and more severe staining artifacts. Here, we report the first demonstration of virtual staining of autopsy tissue and show that a trained neural network can rapidly transform autofluorescence images of label-free autopsy tissue sections into brightfield equivalent images that match hematoxylin and eosin (H&E) stained versions of the same samples, eliminating autolysis-induced severe staining artifacts inherent in traditional histochemical staining of autopsied tissue. Our virtual H&E model was trained using >0.7 TB of image data and a data-efficient collaboration scheme that integrates the virtual staining network with an image registration network. The trained model effectively accentuated nuclear, cytoplasmic and extracellular features in new autopsy tissue samples that experienced severe autolysis, such as COVID-19 samples never seen before, where the traditional histochemical staining failed to provide consistent staining quality. This virtual autopsy staining technique can also be extended to necrotic tissue, and can rapidly and cost-effectively generate artifact-free H&E stains despite severe autolysis and cell death, also reducing labor, cost and infrastructure requirements associated with the standard histochemical staining.
</details>
<details>
<summary>摘要</summary>
histological examination是検死试中的关键步骤，但传统的透明化学染色方法面临多种挑战，包括由尸体组织自逐浸导致的自逐浸破坏，以及涉及大量化学染料、劳动、成本和时间的资源占用。在全球卫生危机期间， histopathology服务的可用性受限，导致组织涂染的延迟和更严重的染料artefacts。在这篇报告中，我们展示了虚拟染色技术的首次应用，使得一种已经训练的神经网络可以快速地将自动染色图像转换成和染料染色版本相同的明亮场景图像，从而消除自逐浸破坏导致的严重染料artefacts。我们的虚拟H&E模型在 >0.7 TB的图像数据和数据效率协作方案的支持下进行训练。训练后，模型能够强调组织涂染中的核、细胞膜和 extracellular 特征，并在新的急死组织样本中表现出优秀的性能，包括COVID-19样本，这些样本在传统的透明化学染色方法下无法保持一致的染料质量。此虚拟染色技术可以扩展到肿瘤组织，并可以快速、成本低、设备简单地生成 artefact-free H&E染料，无论是严重的自逐浸破坏或细胞死亡。
</details></li>
</ul>
<hr>
<h2 id="VLUCI-Variational-Learning-of-Unobserved-Confounders-for-Counterfactual-Inference"><a href="#VLUCI-Variational-Learning-of-Unobserved-Confounders-for-Counterfactual-Inference" class="headerlink" title="VLUCI: Variational Learning of Unobserved Confounders for Counterfactual Inference"></a>VLUCI: Variational Learning of Unobserved Confounders for Counterfactual Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00904">http://arxiv.org/abs/2308.00904</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yonghe Zhao, Qiang Huang, Siwei Wu, Yun Peng, Huiyan Sun</li>
<li>For: 本研究旨在解决在观察数据中存在未知干扰因素的 causal inference 问题，提高 counterfactual outcome 的准确性。* Methods: 本文提出了一种新的变分学习模型，可以学习未知干扰因素的 posterior distribution，从而改进 counterfactual inference 的准确性。* Results: 对于 synthetic 和 semi-synthetic 数据集，变分学习模型在推断未知干扰因素方面表现出色，与现有的 counterfactual inference 方法相比，具有更高的准确性。  Additionally, the paper provides confidence intervals for counterfactual outcomes, which can aid decision-making in risk-sensitive domains.<details>
<summary>Abstract</summary>
Causal inference plays a vital role in diverse domains like epidemiology, healthcare, and economics. De-confounding and counterfactual prediction in observational data has emerged as a prominent concern in causal inference research. While existing models tackle observed confounders, the presence of unobserved confounders remains a significant challenge, distorting causal inference and impacting counterfactual outcome accuracy. To address this, we propose a novel variational learning model of unobserved confounders for counterfactual inference (VLUCI), which generates the posterior distribution of unobserved confounders. VLUCI relaxes the unconfoundedness assumption often overlooked by most causal inference methods. By disentangling observed and unobserved confounders, VLUCI constructs a doubly variational inference model to approximate the distribution of unobserved confounders, which are used for inferring more accurate counterfactual outcomes. Extensive experiments on synthetic and semi-synthetic datasets demonstrate VLUCI's superior performance in inferring unobserved confounders. It is compatible with state-of-the-art counterfactual inference models, significantly improving inference accuracy at both group and individual levels. Additionally, VLUCI provides confidence intervals for counterfactual outcomes, aiding decision-making in risk-sensitive domains. We further clarify the considerations when applying VLUCI to cases where unobserved confounders don't strictly conform to our model assumptions using the public IHDP dataset as an example, highlighting the practical advantages of VLUCI.
</details>
<details>
<summary>摘要</summary>
causal inference在多个领域中扮演着重要的角色，如epidemiology、医疗和经济。在观察数据中，去掉和预测Counterfactual outcome Has become a prominent concern in causal inference research. Although existing models can handle observed confounders, the presence of unobserved confounders remains a significant challenge, distorting causal inference and affecting the accuracy of counterfactual outcomes. To address this, we propose a novel variational learning model of unobserved confounders for counterfactual inference (VLUCI), which generates the posterior distribution of unobserved confounders. VLUCI relaxes the unconfoundedness assumption often overlooked by most causal inference methods. By disentangling observed and unobserved confounders, VLUCI constructs a doubly variational inference model to approximate the distribution of unobserved confounders, which are used for inferring more accurate counterfactual outcomes.在实验中，VLUCI在 synthetic 和 semi-synthetic 数据上显示出了更高的 counterfactual inference 精度。它与当前的 state-of-the-art counterfactual inference models 相容，并在 group 和 individual 水平上提高了推断精度。此外，VLUCI还提供了对 counterfactual outcomes 的信任间隔，帮助在风险敏感的领域做出决策。我们还讨论了在 VLUCI 应用时需要考虑的一些因素，使用公共的 IHDP 数据集作为例子， highlighting the practical advantages of VLUCI。
</details></li>
</ul>
<hr>
<h2 id="User-Controllable-Recommendation-via-Counterfactual-Retrospective-and-Prospective-Explanations"><a href="#User-Controllable-Recommendation-via-Counterfactual-Retrospective-and-Prospective-Explanations" class="headerlink" title="User-Controllable Recommendation via Counterfactual Retrospective and Prospective Explanations"></a>User-Controllable Recommendation via Counterfactual Retrospective and Prospective Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00894">http://arxiv.org/abs/2308.00894</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chrisjtan/ucr">https://github.com/chrisjtan/ucr</a></li>
<li>paper_authors: Juntao Tan, Yingqiang Ge, Yan Zhu, Yinglong Xia, Jiebo Luo, Jianchao Ji, Yongfeng Zhang</li>
<li>for: 提高用户满意度和信任度，通过增加用户可控性</li>
<li>methods: 利用解释能力和可控性，在一个统一框架中呈现retrospective和prospective解释，让用户自定义控制系统</li>
<li>results: 在MovieLens和Yelp数据集上进行实验评估，并证明了提posed方案的有效性，同时发现给用户提供控制选项可能会提高未来的推荐精度<details>
<summary>Abstract</summary>
Modern recommender systems utilize users' historical behaviors to generate personalized recommendations. However, these systems often lack user controllability, leading to diminished user satisfaction and trust in the systems. Acknowledging the recent advancements in explainable recommender systems that enhance users' understanding of recommendation mechanisms, we propose leveraging these advancements to improve user controllability. In this paper, we present a user-controllable recommender system that seamlessly integrates explainability and controllability within a unified framework. By providing both retrospective and prospective explanations through counterfactual reasoning, users can customize their control over the system by interacting with these explanations.   Furthermore, we introduce and assess two attributes of controllability in recommendation systems: the complexity of controllability and the accuracy of controllability. Experimental evaluations on MovieLens and Yelp datasets substantiate the effectiveness of our proposed framework. Additionally, our experiments demonstrate that offering users control options can potentially enhance recommendation accuracy in the future. Source code and data are available at \url{https://github.com/chrisjtan/ucr}.
</details>
<details>
<summary>摘要</summary>
现代推荐系统通常利用用户历史行为生成个性化推荐，但这些系统经常缺乏用户可控性，导致用户满意度和信任度减降。考虑到 latest advancements in explainable recommender systems 可以提高用户理解推荐机制的程度，我们提议利用这些进步来提高用户可控性。在这篇论文中，我们提出了一个可控的推荐系统，该系统可以具有一体化的解释和可控性特性。通过对解释进行反思和评估，用户可以自定义他们对系统的控制。此外，我们还引入了两个推荐系统可控性的属性：复杂性和准确性。我们在 MovieLens 和 Yelp 数据集上进行了实验评估，并证明了我们提出的框架的效果。此外，我们的实验还表明，向用户提供控制选项可能会提高未来的推荐准确性。用户可以通过访问 GitHub 上的 <https://github.com/chrisjtan/ucr> 获取源代码和数据。
</details></li>
</ul>
<hr>
<h2 id="Tango-rethinking-quantization-for-graph-neural-network-training-on-GPUs"><a href="#Tango-rethinking-quantization-for-graph-neural-network-training-on-GPUs" class="headerlink" title="Tango: rethinking quantization for graph neural network training on GPUs"></a>Tango: rethinking quantization for graph neural network training on GPUs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00890">http://arxiv.org/abs/2308.00890</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiyang Chen, Da Zheng, Caiwen Ding, Chengying Huan, Yuede Ji, Hang Liu</li>
<li>for: 这篇论文主要是为了提高图神经网络（GNNs）的训练效率和精度。</li>
<li>methods: 这篇论文提出了一种名为Tango的新方法，包括有效的精度维护策略、量化意识的 primitives 和间接优化，以加速GNN训练。</li>
<li>results: 根据实验结果，Tango在各种GNN模型和数据集上表现出了superior的性能，比现有方法更快和更精度。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) are becoming increasingly popular due to their superior performance in critical graph-related tasks. While quantization is widely used to accelerate GNN computation, quantized training faces unprecedented challenges. Current quantized GNN training systems often have longer training times than their full-precision counterparts for two reasons: (i) addressing the accuracy challenge leads to excessive overhead, and (ii) the optimization potential exposed by quantization is not adequately leveraged. This paper introduces Tango which re-thinks quantization challenges and opportunities for graph neural network training on GPUs with three contributions: Firstly, we introduce efficient rules to maintain accuracy during quantized GNN training. Secondly, we design and implement quantization-aware primitives and inter-primitive optimizations that can speed up GNN training. Finally, we integrate Tango with the popular Deep Graph Library (DGL) system and demonstrate its superior performance over state-of-the-art approaches on various GNN models and datasets.
</details>
<details>
<summary>摘要</summary>
Graph Neural Networks (GNNs) 是越来越受欢迎，因为它们在图相关任务中表现出色。虽然量化是广泛使用来加速 GNN 计算，但量化训练面临了无 precedent 的挑战。现有的量化 GNN 训练系统经常比其全精度对应者 longer 的训练时间，因为：（i）保持精度需要增加过程，（ii）量化 expose 优化潜力并未得到有效利用。本文介绍 Tango，它重新思考量化挑战和机遇，并在 GPU 上进行 GNN 训练。Tango 具有以下三个贡献：首先，我们提出了高效的规则，以保持量化 GNN 训练中的精度。其次，我们设计并实现了量化感知的基本对象和间接优化，可以加速 GNN 训练。最后，我们将 Tango 集成到了流行的 Deep Graph Library (DGL) 系统中，并在不同的 GNN 模型和数据集上示出了与现有方法的超越性。
</details></li>
</ul>
<hr>
<h2 id="Factor-Graph-Neural-Networks"><a href="#Factor-Graph-Neural-Networks" class="headerlink" title="Factor Graph Neural Networks"></a>Factor Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00887">http://arxiv.org/abs/2308.00887</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/molyswu/hand_detection">https://github.com/molyswu/hand_detection</a></li>
<li>paper_authors: Zhen Zhang, Mohammed Haroon Dupty, Fan Wu, Javen Qinfeng Shi, Wee Sun Lee</li>
<li>for: 本研究的目的是提出一种高效的图神经网络（FGNN），以便更好地捕捉高阶关系。</li>
<li>methods: 该研究使用了一种有效的权重估计方法，以及一种基于图神经网络（GNN）的新的消息传递方案。</li>
<li>results: 实验结果表明，该模型在 sintetic 数据集和实际数据集上具有潜在的应用前景。<details>
<summary>Abstract</summary>
In recent years, we have witnessed a surge of Graph Neural Networks (GNNs), most of which can learn powerful representations in an end-to-end fashion with great success in many real-world applications. They have resemblance to Probabilistic Graphical Models (PGMs), but break free from some limitations of PGMs. By aiming to provide expressive methods for representation learning instead of computing marginals or most likely configurations, GNNs provide flexibility in the choice of information flowing rules while maintaining good performance. Despite their success and inspirations, they lack efficient ways to represent and learn higher-order relations among variables/nodes. More expressive higher-order GNNs which operate on k-tuples of nodes need increased computational resources in order to process higher-order tensors. We propose Factor Graph Neural Networks (FGNNs) to effectively capture higher-order relations for inference and learning. To do so, we first derive an efficient approximate Sum-Product loopy belief propagation inference algorithm for discrete higher-order PGMs. We then neuralize the novel message passing scheme into a Factor Graph Neural Network (FGNN) module by allowing richer representations of the message update rules; this facilitates both efficient inference and powerful end-to-end learning. We further show that with a suitable choice of message aggregation operators, our FGNN is also able to represent Max-Product belief propagation, providing a single family of architecture that can represent both Max and Sum-Product loopy belief propagation. Our extensive experimental evaluation on synthetic as well as real datasets demonstrates the potential of the proposed model.
</details>
<details>
<summary>摘要</summary>
Recently, there has been a surge of Graph Neural Networks (GNNs), many of which can learn powerful representations in an end-to-end fashion with great success in many real-world applications. They share some similarities with Probabilistic Graphical Models (PGMs), but have broken free from some of PGMs' limitations. GNNs aim to provide expressive methods for representation learning, rather than computing marginals or most likely configurations. However, they lack efficient ways to represent and learn higher-order relations among variables/nodes. To address this, we propose Factor Graph Neural Networks (FGNNs) to effectively capture higher-order relations for inference and learning.We first derive an efficient approximate Sum-Product loopy belief propagation inference algorithm for discrete higher-order PGMs. We then neuralize the novel message passing scheme into a Factor Graph Neural Network (FGNN) module, allowing richer representations of the message update rules. This facilitates both efficient inference and powerful end-to-end learning. Moreover, with a suitable choice of message aggregation operators, our FGNN can also represent Max-Product belief propagation, providing a single family of architecture that can represent both Max and Sum-Product loopy belief propagation. Our extensive experimental evaluation on synthetic as well as real datasets demonstrates the potential of the proposed model.
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Machine-Learning-Performance-with-Continuous-In-Session-Ground-Truth-Scores-Pilot-Study-on-Objective-Skeletal-Muscle-Pain-Intensity-Prediction"><a href="#Enhancing-Machine-Learning-Performance-with-Continuous-In-Session-Ground-Truth-Scores-Pilot-Study-on-Objective-Skeletal-Muscle-Pain-Intensity-Prediction" class="headerlink" title="Enhancing Machine Learning Performance with Continuous In-Session Ground Truth Scores: Pilot Study on Objective Skeletal Muscle Pain Intensity Prediction"></a>Enhancing Machine Learning Performance with Continuous In-Session Ground Truth Scores: Pilot Study on Objective Skeletal Muscle Pain Intensity Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00886">http://arxiv.org/abs/2308.00886</a></li>
<li>repo_url: None</li>
<li>paper_authors: Boluwatife E. Faremi, Jonathon Stavres, Nuno Oliveira, Zhaoxian Zhou, Andrew H. Sung</li>
<li>for: 这项研究旨在开发两种设备，用于实时采集痛苦分数和血液征动活动评估。</li>
<li>methods: 该研究使用了多层感知神经网络（MLP）和随机森林（RF）机器学习模型，并将对象血液征动活动特征与实时痛苦分数相结合。</li>
<li>results: 研究发现，使用实时痛苦分数可以有效地提高机器学习模型在痛苦Intensity characterization中的性能，比使用后期痛苦分数更高。<details>
<summary>Abstract</summary>
Machine learning (ML) models trained on subjective self-report scores struggle to objectively classify pain accurately due to the significant variance between real-time pain experiences and recorded scores afterwards. This study developed two devices for acquisition of real-time, continuous in-session pain scores and gathering of ANS-modulated endodermal activity (EDA).The experiment recruited N = 24 subjects who underwent a post-exercise circulatory occlusion (PECO) with stretch, inducing discomfort. Subject data were stored in a custom pain platform, facilitating extraction of time-domain EDA features and in-session ground truth scores. Moreover, post-experiment visual analog scale (VAS) scores were collected from each subject. Machine learning models, namely Multi-layer Perceptron (MLP) and Random Forest (RF), were trained using corresponding objective EDA features combined with in-session scores and post-session scores, respectively. Over a 10-fold cross-validation, the macro-averaged geometric mean score revealed MLP and RF models trained with objective EDA features and in-session scores achieved superior performance (75.9% and 78.3%) compared to models trained with post-session scores (70.3% and 74.6%) respectively. This pioneering study demonstrates that using continuous in-session ground truth scores significantly enhances ML performance in pain intensity characterization, overcoming ground truth sparsity-related issues, data imbalance, and high variance. This study informs future objective-based ML pain system training.
</details>
<details>
<summary>摘要</summary>
机器学习（ML）模型在主观自报分数上训练时，很难准确地分类疼痛，因为实际时间疼痛经验和记录下来的分数之间存在了很大的变化。这项研究开发了两种设备用于实时、连续式疼痛分数的获取和抽取 autonomic nervous system 模ulated endodermal activity（EDA）。实验采用 N = 24 个参与者，经过 POST-EXERCISE circulatory occlusion（PECO）后，induced discomfort。参与者数据被存储在自定义疼痛平台上，方便提取时间域 EDA 特征和实时真实分数。此外，每个参与者也提供了POST-EXPERIMENT visual analog scale（VAS）分数。机器学习模型，包括多层感知网络（MLP）和随机森林（RF），在对对应的对象 EDA 特征和实时分数、POST-SESSION 分数进行训练时，在十个横向验证中，macro-averaged geometric mean score 表明 MLP 和 RF 模型在对象 EDA 特征和实时分数上训练时， achievement 的性能（75.9% 和 78.3%）高于在 POST-SESSION 分数上训练时的性能（70.3% 和 74.6%）。这项先驱的研究表明，使用连续实时真实分数可以准确地强化 ML 在疼痛intensity  characterization 中，超越真实分数稀缺、数据不均衡和高方差问题。这项研究提供了未来对象基于 ML 疼痛系统训练的指导。
</details></li>
</ul>
<hr>
<h2 id="Revolutionizing-Wireless-Networks-with-Federated-Learning-A-Comprehensive-Review"><a href="#Revolutionizing-Wireless-Networks-with-Federated-Learning-A-Comprehensive-Review" class="headerlink" title="Revolutionizing Wireless Networks with Federated Learning: A Comprehensive Review"></a>Revolutionizing Wireless Networks with Federated Learning: A Comprehensive Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04404">http://arxiv.org/abs/2308.04404</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sajjad Emdadi Mahdimahalleh</li>
<li>for: The paper is written for discussing the significance of Machine Learning in wireless communication and introducing Federated Learning (FL) as a novel approach for future mobile networks, particularly 6G and beyond.</li>
<li>methods: The paper uses Federated Learning (FL) as a machine learning model that separates data acquisition and computation at the central unit, and discusses the challenges of implementing FL in a wireless edge network with limited and unreliable communication resources.</li>
<li>results: The paper highlights the potential of Federated Learning (FL) to play a vital role in future mobile networks, particularly 6G and beyond, by enabling the separation of data acquisition and computation at the central unit and addressing the challenges of limited and unreliable communication resources.<details>
<summary>Abstract</summary>
These days with the rising computational capabilities of wireless user equipment such as smart phones, tablets, and vehicles, along with growing concerns about sharing private data, a novel machine learning model called federated learning (FL) has emerged. FL enables the separation of data acquisition and computation at the central unit, which is different from centralized learning that occurs in a data center. FL is typically used in a wireless edge network where communication resources are limited and unreliable. Bandwidth constraints necessitate scheduling only a subset of UEs for updates in each iteration, and because the wireless medium is shared, transmissions are susceptible to interference and are not assured. The article discusses the significance of Machine Learning in wireless communication and highlights Federated Learning (FL) as a novel approach that could play a vital role in future mobile networks, particularly 6G and beyond.
</details>
<details>
<summary>摘要</summary>
Note: "UE" stands for "user equipment" in the context of wireless communication.
</details></li>
</ul>
<hr>
<h2 id="PeRP-Personalized-Residual-Policies-For-Congestion-Mitigation-Through-Co-operative-Advisory-Systems"><a href="#PeRP-Personalized-Residual-Policies-For-Congestion-Mitigation-Through-Co-operative-Advisory-Systems" class="headerlink" title="PeRP: Personalized Residual Policies For Congestion Mitigation Through Co-operative Advisory Systems"></a>PeRP: Personalized Residual Policies For Congestion Mitigation Through Co-operative Advisory Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00864">http://arxiv.org/abs/2308.00864</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aamir Hasan, Neeloy Chakraborty, Haonan Chen, Jung-Hoon Cho, Cathy Wu, Katherine Driggs-Campbell</li>
<li>for: 提高交通效率和油耗成本</li>
<li>methods: 基于 Piecewise Constant (PC) 策略和变量自动编码器实现个性化驾驶建议</li>
<li>results: 在模拟驾驶中，我们的方法可以成功地减轻交通堵塞，并适应不同驾驶者行为，比基本方案提高4-22%的平均速度。<details>
<summary>Abstract</summary>
Intelligent driving systems can be used to mitigate congestion through simple actions, thus improving many socioeconomic factors such as commute time and gas costs. However, these systems assume precise control over autonomous vehicle fleets, and are hence limited in practice as they fail to account for uncertainty in human behavior. Piecewise Constant (PC) Policies address these issues by structurally modeling the likeness of human driving to reduce traffic congestion in dense scenarios to provide action advice to be followed by human drivers. However, PC policies assume that all drivers behave similarly. To this end, we develop a co-operative advisory system based on PC policies with a novel driver trait conditioned Personalized Residual Policy, PeRP. PeRP advises drivers to behave in ways that mitigate traffic congestion. We first infer the driver's intrinsic traits on how they follow instructions in an unsupervised manner with a variational autoencoder. Then, a policy conditioned on the inferred trait adapts the action of the PC policy to provide the driver with a personalized recommendation. Our system is trained in simulation with novel driver modeling of instruction adherence. We show that our approach successfully mitigates congestion while adapting to different driver behaviors, with 4 to 22% improvement in average speed over baselines.
</details>
<details>
<summary>摘要</summary>
智能驾驶系统可以减轻堵塞，从而改善许多社会经济因素，如通勤时间和油费成本。然而，这些系统假设自动驾驶车辆队伍的精确控制，因此在实践中有限，因为它们无法考虑人类行为的不确定性。 Piecewise Constant（PC）策略可以解决这些问题，通过结构化模型人类驾驶行为，以减少拥堵的潜在风险。然而，PC策略假设所有 drivers 的行为相似。为此，我们开发了一种合作建议系统，基于 PC 策略和一种新的个性化剩余策略（PeRP）。PeRP 会提供适应不同 Driver 行为的个性化建议，以减少拥堵。我们首先通过一种异构自动编码器来无监督地推断 Driver 的内在特征，然后基于推断的特征来 condition 策略，以提供个性化的建议。我们的系统在 simulator 中进行了训练，并在新的 Driver 模型中模拟了 Driver 的指令遵从程度。我们的方法可以成功地减少拥堵，同时适应不同 Driver 行为，相比基eline 提高了4%-22%的平均速度。
</details></li>
</ul>
<hr>
<h2 id="Understanding-Activation-Patterns-in-Artificial-Neural-Networks-by-Exploring-Stochastic-Processes"><a href="#Understanding-Activation-Patterns-in-Artificial-Neural-Networks-by-Exploring-Stochastic-Processes" class="headerlink" title="Understanding Activation Patterns in Artificial Neural Networks by Exploring Stochastic Processes"></a>Understanding Activation Patterns in Artificial Neural Networks by Exploring Stochastic Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00858">http://arxiv.org/abs/2308.00858</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stephan Johann Lehmler, Muhammad Saif-ur-Rehman, Tobias Glasmachers, Ioannis Iossifidis</li>
<li>for: 这paper的目的是强调使用 Stochastic Processes 模型来描述人工神经网络的行为和学习 dinamics。</li>
<li>methods: 该 paper 使用 Stochastic Processes 模型来模拟人工神经网络的动作 Patterns，并使用 neuroscience 技术来描述实际神经元的冲击 Train。</li>
<li>results: 该 paper 通过对不同的人工神经网络图像识别任务数据进行分析，发现了不同的建筑和训练集之间的差异，并计算了 Mean Firing Rate、Mean Fano Factor 和 Variances 等指标，以描述人工神经网络的吞吐量和学习行为。<details>
<summary>Abstract</summary>
To gain a deeper understanding of the behavior and learning dynamics of (deep) artificial neural networks, it is valuable to employ mathematical abstractions and models. These tools provide a simplified perspective on network performance and facilitate systematic investigations through simulations. In this paper, we propose utilizing the framework of stochastic processes, which has been underutilized thus far.   Our approach models activation patterns of thresholded nodes in (deep) artificial neural networks as stochastic processes. We focus solely on activation frequency, leveraging neuroscience techniques used for real neuron spike trains. During a classification task, we extract spiking activity and use an arrival process following the Poisson distribution.   We examine observed data from various artificial neural networks in image recognition tasks, fitting the proposed model's assumptions. Through this, we derive parameters describing activation patterns in each network. Our analysis covers randomly initialized, generalizing, and memorizing networks, revealing consistent differences across architectures and training sets.   Calculating Mean Firing Rate, Mean Fano Factor, and Variances, we find stable indicators of memorization during learning, providing valuable insights into network behavior. The proposed model shows promise in describing activation patterns and could serve as a general framework for future investigations. It has potential applications in theoretical simulations, pruning, and transfer learning.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:使用数学准确模型来深入理解人工神经网络（深度人工神经网络）的行为和学习动态。这些工具可以简化网络性能的视角，并且通过仿真来进行系统性的调查。在这篇论文中，我们提出使用 Stochastic Processes 框架，这种框架在人工神经网络中的应用仍然很少。  我们的方法是将人工神经网络中的激活模式模型为 Stochastic Processes。我们仅ocus on 激活频率，利用了 neuroscience 中用于真正神经元脉冲的技术。在图像识别任务中，我们从人工神经网络中提取激活活动，并使用随机过程遵循 Poisson 分布。  我们对不同的人工神经网络数据进行分析，包括随机初始化、泛化和记忆化网络。我们发现这些网络在学习过程中的激活模式存在差异，这些差异随着训练集的不同而变化。  通过计算 Mean Firing Rate、Mean Fano Factor 和 Variances，我们发现在学习过程中存在稳定的记忆指标，这些指标提供了人工神经网络行为的重要信息。我们的模型显示 promise 在描述激活模式方面，并且可能在理论仿真、剪枝和转移学习等领域有应用。
</details></li>
</ul>
<hr>
<h2 id="Differential-Privacy-for-Adaptive-Weight-Aggregation-in-Federated-Tumor-Segmentation"><a href="#Differential-Privacy-for-Adaptive-Weight-Aggregation-in-Federated-Tumor-Segmentation" class="headerlink" title="Differential Privacy for Adaptive Weight Aggregation in Federated Tumor Segmentation"></a>Differential Privacy for Adaptive Weight Aggregation in Federated Tumor Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00856">http://arxiv.org/abs/2308.00856</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Irfan Khan, Esa Alhoniemi, Elina Kontio, Suleiman A. Khan, Mojtaba Jafaritadi</li>
<li>For: This paper focuses on developing a differentially private federated deep learning framework for brain tumor segmentation in multi-modal magnetic resonance imaging (MRI). The goal is to enhance model segmentation capabilities while preserving the privacy of client data.* Methods: The proposed method, called DP-SimAgg, is a differentially private similarity-weighted aggregation algorithm that extends the existing SimAgg method to ensure privacy preservation. The algorithm uses a secure multi-party computation (MPC) protocol to enable private similarity computation and weight aggregation.* Results: The proposed method is evaluated extensively, with a focus on computational performance, and demonstrates accurate and robust brain tumor segmentation while minimizing communication costs during model training. The results show that the DP-SimAgg method provides an additional layer of privacy preservation without compromising segmentation model efficacy.<details>
<summary>Abstract</summary>
Federated Learning (FL) is a distributed machine learning approach that safeguards privacy by creating an impartial global model while respecting the privacy of individual client data. However, the conventional FL method can introduce security risks when dealing with diverse client data, potentially compromising privacy and data integrity. To address these challenges, we present a differential privacy (DP) federated deep learning framework in medical image segmentation. In this paper, we extend our similarity weight aggregation (SimAgg) method to DP-SimAgg algorithm, a differentially private similarity-weighted aggregation algorithm for brain tumor segmentation in multi-modal magnetic resonance imaging (MRI). Our DP-SimAgg method not only enhances model segmentation capabilities but also provides an additional layer of privacy preservation. Extensive benchmarking and evaluation of our framework, with computational performance as a key consideration, demonstrate that DP-SimAgg enables accurate and robust brain tumor segmentation while minimizing communication costs during model training. This advancement is crucial for preserving the privacy of medical image data and safeguarding sensitive information. In conclusion, adding a differential privacy layer in the global weight aggregation phase of the federated brain tumor segmentation provides a promising solution to privacy concerns without compromising segmentation model efficacy. By leveraging DP, we ensure the protection of client data against adversarial attacks and malicious participants.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 是一种分布式机器学习方法，它保障隐私性的同时创造一个公正的全球模型，对各个客户端数据进行尊重。然而，传统的 FL 方法可能会在处理多样化的客户端数据时引入安全风险，可能会侵犯隐私性和数据完整性。为解决这些挑战，我们在医学图像分割中提出了一种基于分布式深度学习的幂等隐私（DP）框架。在这篇论文中，我们扩展了我们的相似权聚合（SimAgg）方法，并将其转化为 differentially private similarity-weighted aggregation algorithm（DP-SimAgg），用于多Modal磁共振成像（MRI）中的脑肿瘤分割。我们的 DP-SimAgg 方法不仅提高了模型的分割能力，还提供了一层额外的隐私保护。我们对我们的框架进行了广泛的 benchmarking 和评估，并且计算性能作为关键考虑因素，得到了DP-SimAgg 能够实现精准而Robust的脑肿瘤分割，同时尽量降低模型训练期间的通信成本。这种进步是关键的，因为它使得医学图像数据的隐私得到了保障，并防止了恶意攻击和黑客参与者。In conclusion, adding a differential privacy layer in the global weight aggregation phase of the federated brain tumor segmentation provides a promising solution to privacy concerns without compromising segmentation model efficacy. By leveraging DP, we ensure the protection of client data against adversarial attacks and malicious participants.
</details></li>
</ul>
<hr>
<h2 id="A-Comprehensive-Study-of-Groundbreaking-Machine-Learning-Research-Analyzing-Highly-Cited-and-Impactful-Publications-across-Six-Decades"><a href="#A-Comprehensive-Study-of-Groundbreaking-Machine-Learning-Research-Analyzing-Highly-Cited-and-Impactful-Publications-across-Six-Decades" class="headerlink" title="A Comprehensive Study of Groundbreaking Machine Learning Research: Analyzing Highly Cited and Impactful Publications across Six Decades"></a>A Comprehensive Study of Groundbreaking Machine Learning Research: Analyzing Highly Cited and Impactful Publications across Six Decades</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00855">http://arxiv.org/abs/2308.00855</a></li>
<li>repo_url: None</li>
<li>paper_authors: Absalom E. Ezugwu, Japie Greeff, Yuh-Shan Ho</li>
<li>for: 本研究旨在探讨机器学习（ML）领域最具影响力的论文，以了解领域的发展趋势、关键人物和贡献。</li>
<li>methods: 本研究采用了多种 bibliometric 技术，包括引用分析、合作关系分析、关键词分析和发表趋势分析，以分析数据。</li>
<li>results: 研究发现了机器学习领域最具影响力的论文、作者和合作网络，并揭示出了不同年代的发表趋势和热点研究主题。<details>
<summary>Abstract</summary>
Machine learning (ML) has emerged as a prominent field of research in computer science and other related fields, thereby driving advancements in other domains of interest. As the field continues to evolve, it is crucial to understand the landscape of highly cited publications to identify key trends, influential authors, and significant contributions made thus far. In this paper, we present a comprehensive bibliometric analysis of highly cited ML publications. We collected a dataset consisting of the top-cited papers from reputable ML conferences and journals, covering a period of several years from 1959 to 2022. We employed various bibliometric techniques to analyze the data, including citation analysis, co-authorship analysis, keyword analysis, and publication trends. Our findings reveal the most influential papers, highly cited authors, and collaborative networks within the machine learning community. We identify popular research themes and uncover emerging topics that have recently gained significant attention. Furthermore, we examine the geographical distribution of highly cited publications, highlighting the dominance of certain countries in ML research. By shedding light on the landscape of highly cited ML publications, our study provides valuable insights for researchers, policymakers, and practitioners seeking to understand the key developments and trends in this rapidly evolving field.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="CASSINI-Network-Aware-Job-Scheduling-in-Machine-Learning-Clusters"><a href="#CASSINI-Network-Aware-Job-Scheduling-in-Machine-Learning-Clusters" class="headerlink" title="CASSINI: Network-Aware Job Scheduling in Machine Learning Clusters"></a>CASSINI: Network-Aware Job Scheduling in Machine Learning Clusters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00852">http://arxiv.org/abs/2308.00852</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sudarsanan Rajasekaran, Manya Ghobadi, Aditya Akella</li>
<li>for: 提高机器学习（ML）集群的任务调度效率和网络带宽利用率。</li>
<li>methods: 使用一个novel的 геометрического抽象来考虑不同任务之间的通信模式，并使用一个相互关联的时间排序来调整任务的通信阶段。</li>
<li>results: 在24个服务器的测试环境中，与状态时艺ML调度器相比，CASSINI可以提高任务的平均和尾部完成时间 by up to 1.6x和2.5x，同时还可以减少集群中的ECN标记包数量 by up to 33x。<details>
<summary>Abstract</summary>
We present CASSINI, a network-aware job scheduler for machine learning (ML) clusters. CASSINI introduces a novel geometric abstraction to consider the communication pattern of different jobs while placing them on network links. To do so, CASSINI uses an affinity graph that finds a series of time-shift values to adjust the communication phases of a subset of jobs, such that the communication patterns of jobs sharing the same network link are interleaved with each other. Experiments with 13 common ML models on a 24-server testbed demonstrate that compared to the state-of-the-art ML schedulers, CASSINI improves the average and tail completion time of jobs by up to 1.6x and 2.5x, respectively. Moreover, we show that CASSINI reduces the number of ECN marked packets in the cluster by up to 33x.
</details>
<details>
<summary>摘要</summary>
我们介绍了CASSINI，一个基于网络的机器学习（ML）集群中的任务调度器。CASSINI引入了一种新的几何封装，以考虑不同任务之间的通信模式。为此，CASSINI使用一个相互关系图来找到一系列时间偏移值，以调整与共享同一个网络链接的任务的通信阶段。实验结果表明，相比现有的ML调度器，CASSINI可以提高平均和尾部完成时间的任务执行时间，最多提高1.6倍和2.5倍。此外，我们还发现CASSINI可以减少集群中ECN标记包的数量，最多减少33倍。
</details></li>
</ul>
<hr>
<h2 id="An-Exact-Kernel-Equivalence-for-Finite-Classification-Models"><a href="#An-Exact-Kernel-Equivalence-for-Finite-Classification-Models" class="headerlink" title="An Exact Kernel Equivalence for Finite Classification Models"></a>An Exact Kernel Equivalence for Finite Classification Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00824">http://arxiv.org/abs/2308.00824</a></li>
<li>repo_url: None</li>
<li>paper_authors: Brian Bell, Michael Geyer, David Glickenstein, Amanda Fernandez, Juston Moore</li>
<li>for: 本研究探讨神经网络和核函数方法之间的等价性，并derive了任何 finite-size parametric classification model通过梯度下降训练的第一个精确表示。</li>
<li>methods: 本研究使用梯度下降训练来 derive the exact representation of any finite-size parametric classification model, and compare it with well-known Neural Tangent Kernel (NTK) and other non-exact path kernel formulations.</li>
<li>results: 实验表明，可以用这个精确kernel来计算现实网络中的预测结果，并且这个kernel可以提供有用的推理方法，尤其是在神经网络的泛化性方面。<details>
<summary>Abstract</summary>
We explore the equivalence between neural networks and kernel methods by deriving the first exact representation of any finite-size parametric classification model trained with gradient descent as a kernel machine. We compare our exact representation to the well-known Neural Tangent Kernel (NTK) and discuss approximation error relative to the NTK and other non-exact path kernel formulations. We experimentally demonstrate that the kernel can be computed for realistic networks up to machine precision. We use this exact kernel to show that our theoretical contribution can provide useful insights into the predictions made by neural networks, particularly the way in which they generalize.
</details>
<details>
<summary>摘要</summary>
我们研究神经网络和核函数方法之间的等价性，通过将任何具有Gradient Descent训练的finite-size parametric类别模型转换为核机制。我们与Well-known Neural Tangent Kernel（NTK）进行比较，并讨论这些非正确的路径核函数表示的误差。我们还详细地验证了这个核函数可以在现实的神经网络上计算到机器精度。我们使用这个精确的核函数，以示我们的理论贡献可以对神经网络的预测提供有用的洞察。
</details></li>
</ul>
<hr>
<h2 id="An-Introduction-to-Bi-level-Optimization-Foundations-and-Applications-in-Signal-Processing-and-Machine-Learning"><a href="#An-Introduction-to-Bi-level-Optimization-Foundations-and-Applications-in-Signal-Processing-and-Machine-Learning" class="headerlink" title="An Introduction to Bi-level Optimization: Foundations and Applications in Signal Processing and Machine Learning"></a>An Introduction to Bi-level Optimization: Foundations and Applications in Signal Processing and Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00788">http://arxiv.org/abs/2308.00788</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yihua Zhang, Prashant Khanduri, Ioannis Tsaknakis, Yuguang Yao, Mingyi Hong, Sijia Liu</li>
<li>for: 这篇论文主要是为了探讨bi-level optimization（BLO）在信号处理（SP）和机器学习（ML）领域的应用，以及BLO在这些领域的潜在应用前景。</li>
<li>methods: 本文主要介绍了一类可解释的BLO问题，包括其优化条件、标准算法（包括其优化原理和实践）以及如何使其在SP和ML应用中 дости得状态最优结果。</li>
<li>results: 本文详细介绍了一些BLO问题的基本概念，包括其优化条件、标准算法和实践，以及如何使其在SP和ML应用中实现状态最优结果。此外，本文还讨论了BLO理论的最新进展、其对应用的影响以及未来研究的重要性。<details>
<summary>Abstract</summary>
Recently, bi-level optimization (BLO) has taken center stage in some very exciting developments in the area of signal processing (SP) and machine learning (ML). Roughly speaking, BLO is a classical optimization problem that involves two levels of hierarchy (i.e., upper and lower levels), wherein obtaining the solution to the upper-level problem requires solving the lower-level one. BLO has become popular largely because it is powerful in modeling problems in SP and ML, among others, that involve optimizing nested objective functions. Prominent applications of BLO range from resource allocation for wireless systems to adversarial machine learning. In this work, we focus on a class of tractable BLO problems that often appear in SP and ML applications. We provide an overview of some basic concepts of this class of BLO problems, such as their optimality conditions, standard algorithms (including their optimization principles and practical implementations), as well as how they can be leveraged to obtain state-of-the-art results for a number of key SP and ML applications. Further, we discuss some recent advances in BLO theory, its implications for applications, and point out some limitations of the state-of-the-art that require significant future research efforts. Overall, we hope that this article can serve to accelerate the adoption of BLO as a generic tool to model, analyze, and innovate on a wide array of emerging SP and ML applications.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Evaluating-Spiking-Neural-Network-On-Neuromorphic-Platform-For-Human-Activity-Recognition"><a href="#Evaluating-Spiking-Neural-Network-On-Neuromorphic-Platform-For-Human-Activity-Recognition" class="headerlink" title="Evaluating Spiking Neural Network On Neuromorphic Platform For Human Activity Recognition"></a>Evaluating Spiking Neural Network On Neuromorphic Platform For Human Activity Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00787">http://arxiv.org/abs/2308.00787</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sizhen Bian, Michele Magno<br>for: This paper is written for the purpose of evaluating the effectiveness of spiking neural networks on neuromorphic processors in human activity recognition for wearable applications.methods: The paper uses a multi-threshold delta modulation approach to encode input sensor data into spike trains, and a spiking neural network with direct-event training. The trained model is deployed on the research neuromorphic platform from Intel, Loihi.results: The spike-based workouts recognition system achieves a comparable accuracy (87.5%) to a popular milliwatt RISC-V bases multi-core processor GAP8 with a traditional neural network (88.1%) while achieving two times better energy-delay product (0.66 \si{\micro\joule\second} vs. 1.32 \si{\micro\joule\second}).<details>
<summary>Abstract</summary>
Energy efficiency and low latency are crucial requirements for designing wearable AI-empowered human activity recognition systems, due to the hard constraints of battery operations and closed-loop feedback. While neural network models have been extensively compressed to match the stringent edge requirements, spiking neural networks and event-based sensing are recently emerging as promising solutions to further improve performance due to their inherent energy efficiency and capacity to process spatiotemporal data in very low latency. This work aims to evaluate the effectiveness of spiking neural networks on neuromorphic processors in human activity recognition for wearable applications. The case of workout recognition with wrist-worn wearable motion sensors is used as a study. A multi-threshold delta modulation approach is utilized for encoding the input sensor data into spike trains to move the pipeline into the event-based approach. The spikes trains are then fed to a spiking neural network with direct-event training, and the trained model is deployed on the research neuromorphic platform from Intel, Loihi, to evaluate energy and latency efficiency. Test results show that the spike-based workouts recognition system can achieve a comparable accuracy (87.5\%) comparable to the popular milliwatt RISC-V bases multi-core processor GAP8 with a traditional neural network ( 88.1\%) while achieving two times better energy-delay product (0.66 \si{\micro\joule\second} vs. 1.32 \si{\micro\joule\second}).
</details>
<details>
<summary>摘要</summary>
“能量效率和延迟时间是设计智能盔牌人活动识别系统的关键要求，因为电池操作和关闭反馈带来的硬件限制。尽管神经网络模型已经广泛压缩以满足边缘的强制要求，但使用快速进程和事件检测是最近几年出现的优秀解决方案，因为它们的内生能量效率和可以在非常低延迟时间处理空间时间数据。本工作的目的是评估使用神经元处理器在人活动识别方面的快速进程神经网络的有效性。使用腕上穿戴式运动传感器进行运动识别的情况作为研究。将输入传感器数据编码为脉冲列表，然后将脉冲列表传递给快速进程神经网络进行训练，并使用直接事件训练。训练后的模型被部署在智能研究平台Intel Loihi上，以评估能量和延迟效率。测试结果显示，使用脉冲基于的运动识别系统可以达到与流行的百万瓦RISC-V基于多核心处理器GAP8的传统神经网络（88.1%）相同的准确率（87.5%），同时在能量延迟产品上提供两倍的提升（0.66 \si{\micro\joule\second} vs. 1.32 \si{\micro\joule\second）。”
</details></li>
</ul>
<hr>
<h2 id="DYMOND-DYnamic-MOtif-NoDes-Network-Generative-Model"><a href="#DYMOND-DYnamic-MOtif-NoDes-Network-Generative-Model" class="headerlink" title="DYMOND: DYnamic MOtif-NoDes Network Generative Model"></a>DYMOND: DYnamic MOtif-NoDes Network Generative Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00770">http://arxiv.org/abs/2308.00770</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zeno129/dymond">https://github.com/zeno129/dymond</a></li>
<li>paper_authors: Giselle Zeno, Timothy La Fond, Jennifer Neville</li>
<li>for: 这个论文旨在提出一种基于动态网络结构的生成模型，以capture longer-range correlations in connections and activity。</li>
<li>methods: 该模型使用 temporal motif activity 和节点的角色来生成动态网络结构，并提出了一种新的评价方法来评估网络的动态特性。</li>
<li>results: 与基eline模型进行比较后，该模型能够更好地生成网络结构和节点的行为，同时提供了一种新的评价方法来评估网络的动态特性。<details>
<summary>Abstract</summary>
Motifs, which have been established as building blocks for network structure, move beyond pair-wise connections to capture longer-range correlations in connections and activity. In spite of this, there are few generative graph models that consider higher-order network structures and even fewer that focus on using motifs in models of dynamic graphs. Most existing generative models for temporal graphs strictly grow the networks via edge addition, and the models are evaluated using static graph structure metrics -- which do not adequately capture the temporal behavior of the network. To address these issues, in this work we propose DYnamic MOtif-NoDes (DYMOND) -- a generative model that considers (i) the dynamic changes in overall graph structure using temporal motif activity and (ii) the roles nodes play in motifs (e.g., one node plays the hub role in a wedge, while the remaining two act as spokes). We compare DYMOND to three dynamic graph generative model baselines on real-world networks and show that DYMOND performs better at generating graph structure and node behavior similar to the observed network. We also propose a new methodology to adapt graph structure metrics to better evaluate the temporal aspect of the network. These metrics take into account the changes in overall graph structure and the individual nodes' behavior over time.
</details>
<details>
<summary>摘要</summary>
网络结构中的模式，作为建筑块，超越了对连接的对应关系，以捕捉更长范围的连接和活动的相关性。尽管如此，目前的生成图模型很少考虑高阶网络结构，而且更少的模型将注重使用模式来生成动态图。大多数现有的生成模型都是通过边加入来增长网络，并且这些模型被评估使用静止图结构指标，这些指标不够 capture 网络的时间行为。为解决这些问题，在这项工作中，我们提出了动态模式无核（DYMOND）生成模型，它考虑了（i）动态变化的总图结构使用时间模式活动，以及（ii）节点在模式中的角色（例如，一个节点在wedgel中扮演核角色，而剩下两个节点扮演螺旋的角色）。我们比较了DYMOND与三种动态图生成模型基eline 的性能，并发现DYMOND在生成图结构和节点行为方面表现出色，能够更好地生成与观察网络相似的图结构和节点行为。此外，我们还提出了一种新的方法来适应图结构指标的改进，这些指标考虑了网络的时间变化和个体节点的行为过程中的变化。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Contrastive-BERT-Fine-tuning-for-Fusion-based-Reviewed-Item-Retrieval"><a href="#Self-Supervised-Contrastive-BERT-Fine-tuning-for-Fusion-based-Reviewed-Item-Retrieval" class="headerlink" title="Self-Supervised Contrastive BERT Fine-tuning for Fusion-based Reviewed-Item Retrieval"></a>Self-Supervised Contrastive BERT Fine-tuning for Fusion-based Reviewed-Item Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00762">http://arxiv.org/abs/2308.00762</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/d3mlab/rir_data">https://github.com/d3mlab/rir_data</a></li>
<li>paper_authors: Mohammad Mahdi Abdollah Pour, Parsa Farinneya, Armin Toroghi, Anton Korikov, Ali Pesaranghader, Touqir Sajed, Manasa Bharadwaj, Borislav Mavrin, Scott Sanner<br>for: This paper focuses on the task of Reviewed-Item Retrieval (RIR), which involves aggregating query-review scores to generate item-level scores for ranking.methods: The authors extend Neural Information Retrieval (IR) methodology to RIR by leveraging self-supervised methods for contrastive learning of BERT embeddings for both queries and reviews. They explore different techniques for selecting positive and negative samples, including anchor sub-sampling and augmenting with meta-data.results: The authors show that Late Fusion contrastive learning for Neural RIR outperforms all other contrastive IR configurations, Neural IR, and sparse retrieval baselines. This demonstrates the effectiveness of exploiting the two-level structure in Neural RIR approaches and the importance of preserving the nuance of individual review content via Late Fusion methods.<details>
<summary>Abstract</summary>
As natural language interfaces enable users to express increasingly complex natural language queries, there is a parallel explosion of user review content that can allow users to better find items such as restaurants, books, or movies that match these expressive queries. While Neural Information Retrieval (IR) methods have provided state-of-the-art results for matching queries to documents, they have not been extended to the task of Reviewed-Item Retrieval (RIR), where query-review scores must be aggregated (or fused) into item-level scores for ranking. In the absence of labeled RIR datasets, we extend Neural IR methodology to RIR by leveraging self-supervised methods for contrastive learning of BERT embeddings for both queries and reviews. Specifically, contrastive learning requires a choice of positive and negative samples, where the unique two-level structure of our item-review data combined with meta-data affords us a rich structure for the selection of these samples. For contrastive learning in a Late Fusion scenario, we investigate the use of positive review samples from the same item and/or with the same rating, selection of hard positive samples by choosing the least similar reviews from the same anchor item, and selection of hard negative samples by choosing the most similar reviews from different items. We also explore anchor sub-sampling and augmenting with meta-data. For a more end-to-end Early Fusion approach, we introduce contrastive item embedding learning to fuse reviews into single item embeddings. Experimental results show that Late Fusion contrastive learning for Neural RIR outperforms all other contrastive IR configurations, Neural IR, and sparse retrieval baselines, thus demonstrating the power of exploiting the two-level structure in Neural RIR approaches as well as the importance of preserving the nuance of individual review content via Late Fusion methods.
</details>
<details>
<summary>摘要</summary>
随着自然语言界面的出现，用户可以提出更加复杂的自然语言查询，这也导致了用户评论内容的激增，可以帮助用户更好地找到符合其查询的餐厅、书籍或电影等。尽管神经信息检索（IR）方法已经提供了状态机器的结果，但它们没有被扩展到评论遍历（RIR）任务中，其中查询评论得分需要聚合（或融合）到物品级别的分数。由于没有标注的 RIR 数据集，我们扩展了神经 IR 方法到 RIR，通过自我超vision 方法来学习 BERT 表示的对比学习。具体来说，对比学习需要选择正例和负例样本，我们利用 item-review 数据的两级结构，以及元数据，选择正例和负例样本。在晚期融合方案中，我们 investigate 使用同一个物品的正例评论，或者同一个评级的负例评论作为正例，选择最不相似的 anchor item 中的负例评论，以及选择最相似的 anchor item 中的正例评论作为负例。我们还 explore anchor 抽样和元数据的扩展。在更加端到端的早期融合方案中，我们引入对比项表示学习，将评论融合到单个物品表示中。实验结果显示，晚期融合对比学习在神经 RIR 中表现出色，超过了其他对比 IR 配置、神经 IR 和稀缺检索基准。这也证明了在神经 RIR 方法中利用两级结构的优势，以及在保留评论内容的细节方面，晚期融合方法的重要性。
</details></li>
</ul>
<hr>
<h2 id="The-Bias-Amplification-Paradox-in-Text-to-Image-Generation"><a href="#The-Bias-Amplification-Paradox-in-Text-to-Image-Generation" class="headerlink" title="The Bias Amplification Paradox in Text-to-Image Generation"></a>The Bias Amplification Paradox in Text-to-Image Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00755">http://arxiv.org/abs/2308.00755</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/preethiseshadri518/bias-amplification-paradox">https://github.com/preethiseshadri518/bias-amplification-paradox</a></li>
<li>paper_authors: Preethi Seshadri, Sameer Singh, Yanai Elazar</li>
<li>for: 研究文章探讨了模型中的偏见增强现象，具体来说是在文本到图像领域中使用稳定扩散来研究。</li>
<li>methods: 作者使用了稳定扩散来研究模型中的偏见增强现象，并比较了训练和生成图像中的性别比例。</li>
<li>results: 研究发现，模型在训练和生成图像中增强了 gender-occupation 偏见，但是这种增强可以大量归因于训练数据中的 gender 信息和生成图像中的 prompts 之间的差异。当考虑到不同的文本使用于训练和生成时，偏见增强的减少了许多。这些发现告诉我们在比较模型和训练数据中的偏见时需要考虑各种因素，以及这些因素如何影响偏见增强。<details>
<summary>Abstract</summary>
Bias amplification is a phenomenon in which models increase imbalances present in the training data. In this paper, we study bias amplification in the text-to-image domain using Stable Diffusion by comparing gender ratios in training vs. generated images. We find that the model appears to amplify gender-occupation biases found in the training data (LAION). However, we discover that amplification can largely be attributed to discrepancies between training captions and model prompts. For example, an inherent difference is that captions from the training data often contain explicit gender information while the prompts we use do not, which leads to a distribution shift and consequently impacts bias measures. Once we account for various distributional differences between texts used for training and generation, we observe that amplification decreases considerably. Our findings illustrate the challenges of comparing biases in models and the data they are trained on, and highlight confounding factors that contribute to bias amplification.
</details>
<details>
<summary>摘要</summary>
“偏调增强”是一种现象，模型在训练数据中增加了偏调。在这篇论文中，我们研究了偏调增强在文本到图像领域中，使用稳定扩散法比较训练和生成图像中的性别比例。我们发现，模型会增加训练数据中的性别职业偏调。但是，我们发现这种增强可以大多归因于训练描述和模型说明之间的不同。例如，训练数据中的描述通常包含直接表达性别信息，而模型说明则不包含这种信息，这会导致分布shift和影响偏调测量。一旦我们考虑到不同的分布特征，我们发现增强降低了许多。我们的发现显示了比较模型和它们训练数据中的偏调是具有挑战性的，并高亮了干扰因素对偏调增强的影响。
</details></li>
</ul>
<hr>
<h2 id="Learning-from-Hypervectors-A-Survey-on-Hypervector-Encoding"><a href="#Learning-from-Hypervectors-A-Survey-on-Hypervector-Encoding" class="headerlink" title="Learning from Hypervectors: A Survey on Hypervector Encoding"></a>Learning from Hypervectors: A Survey on Hypervector Encoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00685">http://arxiv.org/abs/2308.00685</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sercan Aygun, Mehran Shoushtari Moghadam, M. Hassan Najafi, Mohsen Imani</li>
<li>for: 本研究旨在探讨高维计算（HDC）系统输入和超对数据编码过程中的hypervector生成方法，以及不同应用场景下的编码类型和其限制、挑战和优势。</li>
<li>methods: 本研究将各种来自不同研究的超对数据生成方法融合探讨，包括Orthogonal Vector Generation（OVG）、Correlated Vector Generation（CVG）等。</li>
<li>results: 本研究可以帮助读者更深入理解HDC系统输入和超对数据编码过程中的hypervector生成方法，并了解不同应用场景下的编码类型和其限制、挑战和优势。<details>
<summary>Abstract</summary>
Hyperdimensional computing (HDC) is an emerging computing paradigm that imitates the brain's structure to offer a powerful and efficient processing and learning model. In HDC, the data are encoded with long vectors, called hypervectors, typically with a length of 1K to 10K. The literature provides several encoding techniques to generate orthogonal or correlated hypervectors, depending on the intended application. The existing surveys in the literature often focus on the overall aspects of HDC systems, including system inputs, primary computations, and final outputs. However, this study takes a more specific approach. It zeroes in on the HDC system input and the generation of hypervectors, directly influencing the hypervector encoding process. This survey brings together various methods for hypervector generation from different studies and explores the limitations, challenges, and potential benefits they entail. Through a comprehensive exploration of this survey, readers will acquire a profound understanding of various encoding types in HDC and gain insights into the intricate process of hypervector generation for diverse applications.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="CodeBPE-Investigating-Subtokenization-Options-for-Large-Language-Model-Pretraining-on-Source-Code"><a href="#CodeBPE-Investigating-Subtokenization-Options-for-Large-Language-Model-Pretraining-on-Source-Code" class="headerlink" title="CodeBPE: Investigating Subtokenization Options for Large Language Model Pretraining on Source Code"></a>CodeBPE: Investigating Subtokenization Options for Large Language Model Pretraining on Source Code</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00683">http://arxiv.org/abs/2308.00683</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nadezhda Chirkova, Sergey Troshin</li>
<li>for:  investigate another important aspect of large language model pretraining for source code, namely the effect of different subtokenization options, and aims at identifying most effective and length-efficient subtokenizations.</li>
<li>methods: propose subtokenization that reduces average length by 17% without downstream performance drop, and show that a carefully chosen subtokenization may improve quality by 0.5-2%, possibly with some length increase.</li>
<li>results: identify most effective and length-efficient subtokenizations, taking into account code specifics, and show that a carefully chosen subtokenization may improve quality by 0.5-2%, possibly with some length increase.<details>
<summary>Abstract</summary>
Recent works have widely adopted large language model pretraining for source code, suggested source code-specific pretraining objectives and investigated the applicability of various Transformer-based language model architectures for source code. This work investigates another important aspect of such models, namely the effect of different subtokenization options, and aims at identifying most effective and length-efficient subtokenizations, taking into account code specifics. We propose subtokenziation that reduces average length by 17% without downstream performance drop, and show that a carefully chosen subtokenization may improve quality by 0.5-2%, possibly with some length increase.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Tool-Documentation-Enables-Zero-Shot-Tool-Usage-with-Large-Language-Models"><a href="#Tool-Documentation-Enables-Zero-Shot-Tool-Usage-with-Large-Language-Models" class="headerlink" title="Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models"></a>Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00675">http://arxiv.org/abs/2308.00675</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheng-Yu Hsieh, Si-An Chen, Chun-Liang Li, Yasuhisa Fujii, Alexander Ratner, Chen-Yu Lee, Ranjay Krishna, Tomas Pfister</li>
<li>for: 这个论文的目的是为了提出一种新的方法，即使用工具文档来取代示例来教育大型自然语言模型（LLM）使用新工具。</li>
<li>methods: 这个论文使用了六个任务，分别在视觉和语言领域进行了测试，并使用了多种工具API来检验其效果。</li>
<li>results: 研究发现，只使用工具文档，LLM可以很好地使用新工具，并且在一些任务上甚至可以达到与几个示例的性能。此外，研究还发现，使用工具文档可以在很多情况下超过示例的效果，并且可以自动激活新的应用程序。<details>
<summary>Abstract</summary>
Today, large language models (LLMs) are taught to use new tools by providing a few demonstrations of the tool's usage. Unfortunately, demonstrations are hard to acquire, and can result in undesirable biased usage if the wrong demonstration is chosen. Even in the rare scenario that demonstrations are readily available, there is no principled selection protocol to determine how many and which ones to provide. As tasks grow more complex, the selection search grows combinatorially and invariably becomes intractable. Our work provides an alternative to demonstrations: tool documentation. We advocate the use of tool documentation, descriptions for the individual tool usage, over demonstrations. We substantiate our claim through three main empirical findings on 6 tasks across both vision and language modalities. First, on existing benchmarks, zero-shot prompts with only tool documentation are sufficient for eliciting proper tool usage, achieving performance on par with few-shot prompts. Second, on a newly collected realistic tool-use dataset with hundreds of available tool APIs, we show that tool documentation is significantly more valuable than demonstrations, with zero-shot documentation significantly outperforming few-shot without documentation. Third, we highlight the benefits of tool documentations by tackling image generation and video tracking using just-released unseen state-of-the-art models as tools. Finally, we highlight the possibility of using tool documentation to automatically enable new applications: by using nothing more than the documentation of GroundingDino, Stable Diffusion, XMem, and SAM, LLMs can re-invent the functionalities of the just-released Grounded-SAM and Track Anything models.
</details>
<details>
<summary>摘要</summary>
Note: The text has been translated into Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. Some words and phrases may be written differently in Traditional Chinese, which is used in Taiwan and other countries.
</details></li>
</ul>
<hr>
<h2 id="Mapping-Computer-Science-Research-Trends-Influences-and-Predictions"><a href="#Mapping-Computer-Science-Research-Trends-Influences-and-Predictions" class="headerlink" title="Mapping Computer Science Research: Trends, Influences, and Predictions"></a>Mapping Computer Science Research: Trends, Influences, and Predictions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00733">http://arxiv.org/abs/2308.00733</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammed Almutairi, Ozioma Collins Oguine</li>
<li>for: 这篇论文探讨了计算机科学领域当前流行的研究领域，并研究了这些领域的出现的因素。</li>
<li>methods: 该论文使用了高级机器学习技术，包括决策树和逻辑回归模型，以预测流行的研究领域。</li>
<li>results: 研究发现，参考论文的数量（Reference Count）是当前流行研究领域的关键因素，使得参考论文数量成为计算机科学领域中最重要的驱动因素。此外，NSF拨款和专利在流行话题中的影响也在不断增长。逻辑回归模型比决策树模型更高精度、精度、回归率和F1分数，表明逻辑回归模型在预测流行研究领域的表现更佳。该数据驱动的方法比随机猜测基准更高精度和有效性，为研究人员和机构提供了数据驱动的基础 для决策和未来研究方向。<details>
<summary>Abstract</summary>
This paper explores the current trending research areas in the field of Computer Science (CS) and investigates the factors contributing to their emergence. Leveraging a comprehensive dataset comprising papers, citations, and funding information, we employ advanced machine learning techniques, including Decision Tree and Logistic Regression models, to predict trending research areas. Our analysis reveals that the number of references cited in research papers (Reference Count) plays a pivotal role in determining trending research areas making reference counts the most relevant factor that drives trend in the CS field. Additionally, the influence of NSF grants and patents on trending topics has increased over time. The Logistic Regression model outperforms the Decision Tree model in predicting trends, exhibiting higher accuracy, precision, recall, and F1 score. By surpassing a random guess baseline, our data-driven approach demonstrates higher accuracy and efficacy in identifying trending research areas. The results offer valuable insights into the trending research areas, providing researchers and institutions with a data-driven foundation for decision-making and future research direction.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://example.com/2023/08/02/cs.LG_2023_08_02/" data-id="cllsjvzc6002bf588gegy18mf" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_08_02" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/02/cs.SD_2023_08_02/" class="article-date">
  <time datetime="2023-08-01T16:00:00.000Z" itemprop="datePublished">2023-08-02</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/02/cs.SD_2023_08_02/">cs.SD - 2023-08-02 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Music-De-limiter-Networks-via-Sample-wise-Gain-Inversion"><a href="#Music-De-limiter-Networks-via-Sample-wise-Gain-Inversion" class="headerlink" title="Music De-limiter Networks via Sample-wise Gain Inversion"></a>Music De-limiter Networks via Sample-wise Gain Inversion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01187">http://arxiv.org/abs/2308.01187</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jeonchangbin49/de-limiter">https://github.com/jeonchangbin49/de-limiter</a></li>
<li>paper_authors: Chang-Bin Jeon, Kyogu Lee</li>
<li>for: 提高音乐压缩后的声音质量和听众舒适性</li>
<li>methods: 基于限器原理，提出了sample-wise gain inversion（SGI）框架，并使用了musdb-XL-train数据集进行训练</li>
<li>results: 提出了一种基于SGI的音乐去限器网络，可以高效地将压缩后的音乐恢复到原始状态，并且在musdb-HQ与musdb-XL数据集上达到了23.8 dB的抑制比（SI-SDR）<details>
<summary>Abstract</summary>
The loudness war, an ongoing phenomenon in the music industry characterized by the increasing final loudness of music while reducing its dynamic range, has been a controversial topic for decades. Music mastering engineers have used limiters to heavily compress and make music louder, which can induce ear fatigue and hearing loss in listeners. In this paper, we introduce music de-limiter networks that estimate uncompressed music from heavily compressed signals. Inspired by the principle of a limiter, which performs sample-wise gain reduction of a given signal, we propose the framework of sample-wise gain inversion (SGI). We also present the musdb-XL-train dataset, consisting of 300k segments created by applying a commercial limiter plug-in for training real-world friendly de-limiter networks. Our proposed de-limiter network achieves excellent performance with a scale-invariant source-to-distortion ratio (SI-SDR) of 23.8 dB in reconstructing musdb-HQ from musdb- XL data, a limiter-applied version of musdb-HQ. The training data, codes, and model weights are available in our repository (https://github.com/jeonchangbin49/De-limiter).
</details>
<details>
<summary>摘要</summary>
《喧嚣战争》是音乐工业中一个延续多年的现象，其特征是逐渐增加音乐的响度，同时降低其 Dinamic Range。音乐硬件工程师通过使用限制器来压缩和增强音乐的响度，可能会导致耳朵疲劳和听力损害。在这篇论文中，我们介绍了一种音乐去限制网络，该网络可以从压缩后的音乐中估计原始音乐。我们的提案基于限制器的原理，即对给定的信号进行采样值减少的操作。我们称之为样本值减少（SGI）框架。此外，我们还提供了musdb-XL-train数据集，包括300k段，每段由应用商业限制插件来训练真实世界友好的去限制网络。我们的提案的去限制网络在重建musdb-HQ从musdb-XL数据中得到了优秀的表现，SI-SDR为23.8dB。训练数据、代码和模型参数可以在我们的存储库（https://github.com/jeonchangbin49/De-limiter）中找到。
</details></li>
</ul>
<hr>
<h2 id="Inaudible-Adversarial-Perturbation-Manipulating-the-Recognition-of-User-Speech-in-Real-Time"><a href="#Inaudible-Adversarial-Perturbation-Manipulating-the-Recognition-of-User-Speech-in-Real-Time" class="headerlink" title="Inaudible Adversarial Perturbation: Manipulating the Recognition of User Speech in Real Time"></a>Inaudible Adversarial Perturbation: Manipulating the Recognition of User Speech in Real Time</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01040">http://arxiv.org/abs/2308.01040</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinfeng Li, Chen Yan, Xuancun Lu, Zihan Zeng, Xiaoyu Ji, Wenyuan Xu</li>
<li>for: 这种研究旨在攻击语音识别系统（ASR），使其输出错误或被 manipulate。</li>
<li>methods: 这种攻击使用无声振荡（ultrasound）传输干扰，通过修改ASR的输入来控制其输出。</li>
<li>results: 实验结果表明，这种攻击能够在不同的距离和用户语音中实时 manipulate ASR 输出，并且能够抗击六种防御机制。<details>
<summary>Abstract</summary>
Automatic speech recognition (ASR) systems have been shown to be vulnerable to adversarial examples (AEs). Recent success all assumes that users will not notice or disrupt the attack process despite the existence of music/noise-like sounds and spontaneous responses from voice assistants. Nonetheless, in practical user-present scenarios, user awareness may nullify existing attack attempts that launch unexpected sounds or ASR usage. In this paper, we seek to bridge the gap in existing research and extend the attack to user-present scenarios. We propose VRIFLE, an inaudible adversarial perturbation (IAP) attack via ultrasound delivery that can manipulate ASRs as a user speaks. The inherent differences between audible sounds and ultrasounds make IAP delivery face unprecedented challenges such as distortion, noise, and instability. In this regard, we design a novel ultrasonic transformation model to enhance the crafted perturbation to be physically effective and even survive long-distance delivery. We further enable VRIFLE's robustness by adopting a series of augmentation on user and real-world variations during the generation process. In this way, VRIFLE features an effective real-time manipulation of the ASR output from different distances and under any speech of users, with an alter-and-mute strategy that suppresses the impact of user disruption. Our extensive experiments in both digital and physical worlds verify VRIFLE's effectiveness under various configurations, robustness against six kinds of defenses, and universality in a targeted manner. We also show that VRIFLE can be delivered with a portable attack device and even everyday-life loudspeakers.
</details>
<details>
<summary>摘要</summary>
自动语音识别（ASR）系统已经被证明是易受到敌意攻击（AE）的。现有的成功假设了用户不会注意或干扰攻击过程中的音乐/噪声和声音助手的自发响应。然而，在实际用户存在的场景下，用户的意识可能会neutralize现有的攻击尝试，发送不期望的声音或ASR使用。在这篇论文中，我们尝试将攻击扩展到用户存在场景。我们提出了一种不可见的敌意攻击（IAP），通过ultrasound发送来控制ASR。由于听ible和ultrasound之间的本质差异，IAPelivery面临了无前例的挑战，如扭曲、噪声和不稳定。为此，我们设计了一种新的ultrasound转换模型，以提高制造的杂乱扰动的物理效果，并使其在长距离传输中存活。此外，我们采用了一系列的改进来增强VRIFLE的可靠性，包括在生成过程中对用户和实际世界的变化进行改进。这样，VRIFLE可以在不同的距离和用户说话时进行有效的实时控制ASR输出，并采取“相互抑制”策略来避免用户中断。我们的广泛的实验表明VRIFLE在不同的配置下、对六种防御机制进行了robustness测试，并在目标方式下实现了通用性。此外，我们还证明了VRIFLE可以通过移动攻击设备和日常生活中的 loudspeakers进行传输。
</details></li>
</ul>
<hr>
<h2 id="SALTTS-Leveraging-Self-Supervised-Speech-Representations-for-improved-Text-to-Speech-Synthesis"><a href="#SALTTS-Leveraging-Self-Supervised-Speech-Representations-for-improved-Text-to-Speech-Synthesis" class="headerlink" title="SALTTS: Leveraging Self-Supervised Speech Representations for improved Text-to-Speech Synthesis"></a>SALTTS: Leveraging Self-Supervised Speech Representations for improved Text-to-Speech Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01018">http://arxiv.org/abs/2308.01018</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ramanan Sivaguru, Vasista Sai Lodagala, S Umesh</li>
<li>for: 提高 FastSpeech2 的 synthesized speech 质量</li>
<li>methods: 使用 Self-Supervised Learning (SSL) 模型的表示来增强 FastSpeech2 的 synthesized speech</li>
<li>results: 比基eline FastSpeech2 表现更高的对象和主观评价指标In English:</li>
<li>for: Improving the quality of synthesized speech in FastSpeech2</li>
<li>methods: Using Self-Supervised Learning (SSL) models to enhance the representations in FastSpeech2</li>
<li>results: Outperforming the baseline FastSpeech2 in both objective and subjective evaluation measures<details>
<summary>Abstract</summary>
While FastSpeech2 aims to integrate aspects of speech such as pitch, energy, and duration as conditional inputs, it still leaves scope for richer representations. As a part of this work, we leverage representations from various Self-Supervised Learning (SSL) models to enhance the quality of the synthesized speech. In particular, we pass the FastSpeech2 encoder's length-regulated outputs through a series of encoder layers with the objective of reconstructing the SSL representations. In the SALTTS-parallel implementation, the representations from this second encoder are used for an auxiliary reconstruction loss with the SSL features. The SALTTS-cascade implementation, however, passes these representations through the decoder in addition to having the reconstruction loss. The richness of speech characteristics from the SSL features reflects in the output speech quality, with the objective and subjective evaluation measures of the proposed approach outperforming the baseline FastSpeech2.
</details>
<details>
<summary>摘要</summary>
而FASTSpeech2尝试将发音特征，如抑高、能量和持续时间作为条件输入，但还留有更加丰富的表示空间。在这项工作中，我们利用了不同的自动适应学习（SSL）模型来增强合成 speech 的质量。特别是，我们将 FASTSpeech2 Encoder 的长度调整后的输出通过一系列Encoder层，以重construct SSL 特征。在 SALTTS-parallel 实现中，这些第二个 Encoder 的表示被用于auxiliary reconstruction loss中的 SSL 特征。SALTTS-cascade 实现则是将这些表示通过 Decoder 以及重建损失。通过 SSL 特征的丰富性，提高了合成 speech 的质量，并且对于提出的方法的目标和主观评价指标都达到了比基eline FastSpeech2 的更高水平。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://example.com/2023/08/02/cs.SD_2023_08_02/" data-id="cllsjvzcz004qf588cnluau1t" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_08_02" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/02/eess.IV_2023_08_02/" class="article-date">
  <time datetime="2023-08-01T16:00:00.000Z" itemprop="datePublished">2023-08-02</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/02/eess.IV_2023_08_02/">eess.IV - 2023-08-02 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="CMUNeXt-An-Efficient-Medical-Image-Segmentation-Network-based-on-Large-Kernel-and-Skip-Fusion"><a href="#CMUNeXt-An-Efficient-Medical-Image-Segmentation-Network-based-on-Large-Kernel-and-Skip-Fusion" class="headerlink" title="CMUNeXt: An Efficient Medical Image Segmentation Network based on Large Kernel and Skip Fusion"></a>CMUNeXt: An Efficient Medical Image Segmentation Network based on Large Kernel and Skip Fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01239">http://arxiv.org/abs/2308.01239</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/FengheTan9/Medical-Image-Segmentation-Benchmarks">https://github.com/FengheTan9/Medical-Image-Segmentation-Benchmarks</a></li>
<li>paper_authors: Fenghe Tang, Jianrui Ding, Lingtao Wang, Chunping Ning, S. Kevin Zhou</li>
<li>for: 这个研究旨在提出一个高效、轻量级的医疗影像分类网络，以应对现场医疗 scenarios 中的病变探测和诊断。</li>
<li>methods: 本研究使用了 U-shaped 架构，并将 CNN 和 Transformer 混合使用，以获得更好的全局 контекст资讯和精确的地方化探测。</li>
<li>results: 实验结果显示，CMUNeXt 可以在多个医疗影像数据集上实现更好的分类性能，并且比其他现有的重量级和轻量级医疗影像分类网络更快速、更轻量级、并且降低了计算成本。<details>
<summary>Abstract</summary>
The U-shaped architecture has emerged as a crucial paradigm in the design of medical image segmentation networks. However, due to the inherent local limitations of convolution, a fully convolutional segmentation network with U-shaped architecture struggles to effectively extract global context information, which is vital for the precise localization of lesions. While hybrid architectures combining CNNs and Transformers can address these issues, their application in real medical scenarios is limited due to the computational resource constraints imposed by the environment and edge devices. In addition, the convolutional inductive bias in lightweight networks adeptly fits the scarce medical data, which is lacking in the Transformer based network. In order to extract global context information while taking advantage of the inductive bias, we propose CMUNeXt, an efficient fully convolutional lightweight medical image segmentation network, which enables fast and accurate auxiliary diagnosis in real scene scenarios. CMUNeXt leverages large kernel and inverted bottleneck design to thoroughly mix distant spatial and location information, efficiently extracting global context information. We also introduce the Skip-Fusion block, designed to enable smooth skip-connections and ensure ample feature fusion. Experimental results on multiple medical image datasets demonstrate that CMUNeXt outperforms existing heavyweight and lightweight medical image segmentation networks in terms of segmentation performance, while offering a faster inference speed, lighter weights, and a reduced computational cost. The code is available at https://github.com/FengheTan9/CMUNeXt.
</details>
<details>
<summary>摘要</summary>
医疗图像分 segmentation 网络中，U字型架构已成为重要的设计方法。然而，由于 convolution 的本质性限制，一个完全 convolutional 分 segmentation 网络具有 U字型架构很难准确地提取全局上下文信息，这是精确地定位疾病所必需的。尽管混合 Architecture 组合 CNN 和 Transformer 可以解决这些问题，但在实际医疗应用中受到环境和边缘设备的计算资源限制。此外， convolutional 的学习偏好适合疾病数据的缺乏，Transformer 基于的网络具有更好的表现。为了提取全局上下文信息而不丢失 convolutional 的学习偏好，我们提出了 CMUNeXt，一种高效的全 convolutional 轻量级医疗图像分 segmentation 网络。CMUNeXt 利用大kernel和翻转瓶颈设计，fficiently 混合远程空间和位置信息，提取全局上下文信息。我们还引入了 Skip-Fusion 块，以便在 skip-connection 中实现细腻的融合。实验结果表明，CMUNeXt 在多个医疗图像数据集上的 segmentation 性能超过现有的重量级和轻量级医疗图像分 segmentation 网络，同时具有快速的推理速度、轻量级的权重和减少的计算成本。代码可以在 <https://github.com/FengheTan9/CMUNeXt> 上获取。
</details></li>
</ul>
<hr>
<h2 id="High-efficient-deep-learning-based-DTI-reconstruction-with-flexible-diffusion-gradient-encoding-scheme"><a href="#High-efficient-deep-learning-based-DTI-reconstruction-with-flexible-diffusion-gradient-encoding-scheme" class="headerlink" title="High-efficient deep learning-based DTI reconstruction with flexible diffusion gradient encoding scheme"></a>High-efficient deep learning-based DTI reconstruction with flexible diffusion gradient encoding scheme</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01173">http://arxiv.org/abs/2308.01173</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zejun Wu, Jiechao Wang, Zunquan Chen, Qinqin Yang, Shuhui Cai, Zhong Chen, Congbo Cai</li>
<li>for: 本研究旨在开发和评估一种基于动态核函数的新方法，称为FlexDTI，以实现高效的Diffusion Tensor Imaging（DTI）重建。</li>
<li>methods: FlexDTI方法使用动态核函数来嵌入扩散梯度方向信息到相应的扩散信号特征图中，以实现高质量DTI参数地图的重建。此外，我们的方法实现了扩散梯度方向的通用化，通过设置最大输入通道数量来实现 flexible number of diffusion gradient directions。</li>
<li>results: 与其他高级tensor参数估计方法相比，FlexDTI成功地实现了高质量扩散tensor-derived变量，即使扩散梯度方向和数量是变量的。它提高了 peak signal-to-noise ratio（PSNR）约10 dB，相对于基于深度学习的状态对抗方法。<details>
<summary>Abstract</summary>
Purpose: To develop and evaluate a novel dynamic-convolution-based method called FlexDTI for high-efficient diffusion tensor reconstruction with flexible diffusion encoding gradient schemes. Methods: FlexDTI was developed to achieve high-quality DTI parametric mapping with flexible number and directions of diffusion encoding gradients. The proposed method used dynamic convolution kernels to embed diffusion gradient direction information into feature maps of the corresponding diffusion signal. Besides, our method realized the generalization of a flexible number of diffusion gradient directions by setting the maximum number of input channels of the network. The network was trained and tested using data sets from the Human Connectome Project and a local hospital. Results from FlexDTI and other advanced tensor parameter estimation methods were compared. Results: Compared to other methods, FlexDTI successfully achieves high-quality diffusion tensor-derived variables even if the number and directions of diffusion encoding gradients are variable. It increases peak signal-to-noise ratio (PSNR) by about 10 dB on Fractional Anisotropy (FA) and Mean Diffusivity (MD), compared with the state-of-the-art deep learning method with flexible diffusion encoding gradient schemes. Conclusion: FlexDTI can well learn diffusion gradient direction information to achieve generalized DTI reconstruction with flexible diffusion gradient schemes. Both flexibility and reconstruction quality can be taken into account in this network.
</details>
<details>
<summary>摘要</summary>
目的：开发和评估一种基于动态核函数的新方法called FlexDTI，用于高效的扩散矢量重建with flexible扩散编码 Gradient schemes。方法：FlexDTI方法实现了高质量的DTI参数地图with flexible扩散编码方向数和扩散编码方向。提案的方法通过动态核函数嵌入扩散Gradient方向信息到相应的扩散信号特征图中。此外，我们的方法实现了一般化扩散编码方向的数目的扩展，通过设置网络的最大输入通道数来实现。网络通过使用数据集from the Human Connectome Project和当地医院进行训练和测试。与其他先进的矢量参数估计方法相比，FlexDTI成功地实现了高质量的扩散矢量变量，即使扩散编码方向数和方向不固定。结果：相比其他方法，FlexDTI可以在不同的扩散编码方向下实现高质量的扩散矢量重建。它提高了平均扩散矢量(FA)和扩散率(MD)的峰峰信号强度比(PSNR)约10dB，相比先进的深度学习方法with flexible扩散编码方向。结论：FlexDTI可以很好地学习扩散Gradient方向信息，实现通用的DTI重建with flexible扩散编码方向。这种网络可以同时考虑扩散Gradient方向信息和重建质量。
</details></li>
</ul>
<hr>
<h2 id="Contrast-augmented-Diffusion-Model-with-Fine-grained-Sequence-Alignment-for-Markup-to-Image-Generation"><a href="#Contrast-augmented-Diffusion-Model-with-Fine-grained-Sequence-Alignment-for-Markup-to-Image-Generation" class="headerlink" title="Contrast-augmented Diffusion Model with Fine-grained Sequence Alignment for Markup-to-Image Generation"></a>Contrast-augmented Diffusion Model with Fine-grained Sequence Alignment for Markup-to-Image Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01147">http://arxiv.org/abs/2308.01147</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zgj77/fsacdm">https://github.com/zgj77/fsacdm</a></li>
<li>paper_authors: Guojin Zhong, Jin Yuan, Pan Wang, Kailun Yang, Weili Guan, Zhiyong Li</li>
<li>for: 本研究旨在提高markup-to-image生成中的性能，特别是对于 markup language 中的Error tolerance和序列相关性具有更高的要求。</li>
<li>methods: 本paper提出了一种名为“增强对比扩散模型with fine-grained sequence alignment”（FSA-CDM）的新模型，其中引入了对比正例&#x2F;负例的样本，以提高markup-to-image生成的性能。技术上，我们设计了一种细致的交叉模式对应模块，以便充分利用两个模式之间的序列相似性，从而学习更加稳定的特征表示。</li>
<li>results: 在四个不同领域的benchmark dataset上，我们进行了广泛的实验，并证明了提出的组件在FSA-CDM中具有显著的效果，与当前最佳性能差异为2%-12% DTW。<details>
<summary>Abstract</summary>
The recently rising markup-to-image generation poses greater challenges as compared to natural image generation, due to its low tolerance for errors as well as the complex sequence and context correlations between markup and rendered image. This paper proposes a novel model named "Contrast-augmented Diffusion Model with Fine-grained Sequence Alignment" (FSA-CDM), which introduces contrastive positive/negative samples into the diffusion model to boost performance for markup-to-image generation. Technically, we design a fine-grained cross-modal alignment module to well explore the sequence similarity between the two modalities for learning robust feature representations. To improve the generalization ability, we propose a contrast-augmented diffusion model to explicitly explore positive and negative samples by maximizing a novel contrastive variational objective, which is mathematically inferred to provide a tighter bound for the model's optimization. Moreover, the context-aware cross attention module is developed to capture the contextual information within markup language during the denoising process, yielding better noise prediction results. Extensive experiments are conducted on four benchmark datasets from different domains, and the experimental results demonstrate the effectiveness of the proposed components in FSA-CDM, significantly exceeding state-of-the-art performance by about 2%-12% DTW improvements. The code will be released at https://github.com/zgj77/FSACDM.
</details>
<details>
<summary>摘要</summary>
最近升起的markup-to-图像生成问题，相比于自然图像生成，具有较低的容忍度和复杂的序列和上下文相关性。这篇论文提出了一种新的模型，即“对比增强扩散模型with细致过程对齐”（FSA-CDM），该模型在markup-to-图像生成中提高性能。技术上，我们设计了细致的过程对齐模块，以便充分探索两个模式之间的序列相似性，从而学习强健的特征表示。为提高泛化能力，我们提出了对比增强扩散模型，通过最大化一种新的对比算法，以提高模型的优化。此外，我们还开发了上下文意识卷积模块，以捕捉markup语言中的上下文信息，从而实现更好的噪声预测结果。我们在四个不同领域的四个benchmark dataset上进行了广泛的实验，并证明了提案的组件在FSA-CDM中的效果，与状态艺ircle的表现提高约2%-12% DTW。代码将在https://github.com/zgj77/FSACDM中发布。
</details></li>
</ul>
<hr>
<h2 id="UCDFormer-Unsupervised-Change-Detection-Using-a-Transformer-driven-Image-Translation"><a href="#UCDFormer-Unsupervised-Change-Detection-Using-a-Transformer-driven-Image-Translation" class="headerlink" title="UCDFormer: Unsupervised Change Detection Using a Transformer-driven Image Translation"></a>UCDFormer: Unsupervised Change Detection Using a Transformer-driven Image Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01146">http://arxiv.org/abs/2308.01146</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhu-xlab/ucdformer">https://github.com/zhu-xlab/ucdformer</a></li>
<li>paper_authors: Qingsong Xu, Yilei Shi, Jianhua Guo, Chaojun Ouyang, Xiao Xiang Zhu<br>for: 这个论文主要针对的是如何使用不需要标注的变化信息进行远程感知中的变化检测（CD） task，特别是考虑到多个时间图像中的季节和风格差异。methods: 该论文提出了一种基于域shift的CD方法，使用了一种轻量级的变换器（UCDFormer）来减轻域shift问题，并提出了一种新的无监督CD方法。这种方法首先使用了一种变换器驱动的图像翻译模型，然后使用了域specific affinity weight来调整图像之间的域shift。最后，该方法使用了一种可靠的像素提取模块来选择changed和unchanged像素，并使用了一种二分类器来获得最终的变化地图。results: 该论文的实验结果表明，Compared with several other related methods, UCDFormer improves performance on the Kappa coefficient by more than 12%. In addition, UCDFormer achieves excellent performance for earthquake-induced landslide detection when considering large-scale applications.<details>
<summary>Abstract</summary>
Change detection (CD) by comparing two bi-temporal images is a crucial task in remote sensing. With the advantages of requiring no cumbersome labeled change information, unsupervised CD has attracted extensive attention in the community. However, existing unsupervised CD approaches rarely consider the seasonal and style differences incurred by the illumination and atmospheric conditions in multi-temporal images. To this end, we propose a change detection with domain shift setting for remote sensing images. Furthermore, we present a novel unsupervised CD method using a light-weight transformer, called UCDFormer. Specifically, a transformer-driven image translation composed of a light-weight transformer and a domain-specific affinity weight is first proposed to mitigate domain shift between two images with real-time efficiency. After image translation, we can generate the difference map between the translated before-event image and the original after-event image. Then, a novel reliable pixel extraction module is proposed to select significantly changed/unchanged pixel positions by fusing the pseudo change maps of fuzzy c-means clustering and adaptive threshold. Finally, a binary change map is obtained based on these selected pixel pairs and a binary classifier. Experimental results on different unsupervised CD tasks with seasonal and style changes demonstrate the effectiveness of the proposed UCDFormer. For example, compared with several other related methods, UCDFormer improves performance on the Kappa coefficient by more than 12\%. In addition, UCDFormer achieves excellent performance for earthquake-induced landslide detection when considering large-scale applications. The code is available at \url{https://github.com/zhu-xlab/UCDFormer}
</details>
<details>
<summary>摘要</summary>
优化变更检测（CD）方法是远程感知领域中的关键任务。无监督CD方法受到了社区的广泛关注，因为它们不需要繁琐的标注更改信息。然而，现有的无监督CD方法很少考虑远程图像中的季节和风格差异，这些差异可能由照明和大气条件引起。为此，我们提出了基于频率域差分的CD方法。此外，我们还提出了一种基于小型转换器的新无监督CD方法，称为UCDFormer。具体来说，我们首先使用一种小型转换器和域特定相似权重来将两个图像进行图像翻译。然后，我们可以生成原始事件后图像和事件前图像之间的差分图。接着，我们提出了一种新的可靠像检测模块，可以通过将各种瑞利变换和适应阈值融合来选择有优势的更改/不更改像素位置。最后，我们根据这些选择的像素对生成一个二进制更改地图，并使用一个二进制分类器。实验结果表明，在不同的无监督CD任务中，UCDFormer的性能明显优于其他相关方法。例如，相比其他方法，UCDFormer在卡平差 coefficient 上提高了更多于12%。此外，UCDFormer在考虑大规模应用时对地球quake-induced landslide检测表现出色。代码可以在 \url{https://github.com/zhu-xlab/UCDFormer} 上获取。
</details></li>
</ul>
<hr>
<h2 id="Learning-Fourier-Constrained-Diffusion-Bridges-for-MRI-Reconstruction"><a href="#Learning-Fourier-Constrained-Diffusion-Bridges-for-MRI-Reconstruction" class="headerlink" title="Learning Fourier-Constrained Diffusion Bridges for MRI Reconstruction"></a>Learning Fourier-Constrained Diffusion Bridges for MRI Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01096">http://arxiv.org/abs/2308.01096</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/icon-lab/fdb">https://github.com/icon-lab/fdb</a></li>
<li>paper_authors: Muhammad U. Mirza, Onat Dalmaz, Hasan A. Bedel, Gokberk Elmas, Yilmaz Korkmaz, Alper Gungor, Salman UH Dar, Tolga Çukur</li>
<li>for: 这篇论文的目的是提出一种新的泵diffusion bridge（FDB）来加速MRI重建。</li>
<li>methods: FDB使用了一种通过噪音添加和随机频率除去的泵进程来将受测样本转换为全样本样本。</li>
<li>results: 实验结果显示，FDB可以比州前方的重建方法，包括传统的泵假设。<details>
<summary>Abstract</summary>
Recent years have witnessed a surge in deep generative models for accelerated MRI reconstruction. Diffusion priors in particular have gained traction with their superior representational fidelity and diversity. Instead of the target transformation from undersampled to fully-sampled data, common diffusion priors are trained to learn a multi-step transformation from Gaussian noise onto fully-sampled data. During inference, data-fidelity projections are injected in between reverse diffusion steps to reach a compromise solution within the span of both the diffusion prior and the imaging operator. Unfortunately, suboptimal solutions can arise as the normality assumption of the diffusion prior causes divergence between learned and target transformations. To address this limitation, here we introduce the first diffusion bridge for accelerated MRI reconstruction. The proposed Fourier-constrained diffusion bridge (FDB) leverages a generalized process to transform between undersampled and fully-sampled data via random noise addition and random frequency removal as degradation operators. Unlike common diffusion priors that use an asymptotic endpoint based on Gaussian noise, FDB captures a transformation between finite endpoints where the initial endpoint is based on moderate degradation of fully-sampled data. Demonstrations on brain MRI indicate that FDB outperforms state-of-the-art reconstruction methods including conventional diffusion priors.
</details>
<details>
<summary>摘要</summary>
近年来，深度生成模型在加速MRI重建方面得到了广泛应用。尤其是Diffusion priors在其superior representational fidelity和多样性方面受到了广泛关注。而不是target transformation从 undersampled 数据到fully-sampled数据，通常的Diffusion priors是通过学习一个多步转换从 Gaussian noise 到 fully-sampled 数据。在推理过程中，数据准确性投影会在反卷 diffusion 步骤之间插入，以达到一个compromise solution在 diffusion prior 和 imaging operator 之间。然而，可能会出现优化解决方案的局限性，因为normality assumption of diffusion prior 会导致learned 和 target 转换之间的分歧。为了解决这个限制，我们在这里引入了首个Fourier-constrained diffusion bridge (FDB)。该提案利用一种通用过程来将 undersampled 和 fully-sampled 数据之间进行转换，通过随机噪声添加和随机频率除法作为降低操作符。与常见的Diffusion priors不同，FDB capture一个转换 междуfinite endpoints，其初始endpoint基于moderate degradation of fully-sampled data。在脑MRI示例中，FDB超过了当前重建方法，包括conventional Diffusion priors。
</details></li>
</ul>
<hr>
<h2 id="Push-the-Boundary-of-SAM-A-Pseudo-label-Correction-Framework-for-Medical-Segmentation"><a href="#Push-the-Boundary-of-SAM-A-Pseudo-label-Correction-Framework-for-Medical-Segmentation" class="headerlink" title="Push the Boundary of SAM: A Pseudo-label Correction Framework for Medical Segmentation"></a>Push the Boundary of SAM: A Pseudo-label Correction Framework for Medical Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00883">http://arxiv.org/abs/2308.00883</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyi Huang, Hongshan Liu, Haofeng Zhang, Fuyong Xing, Andrew Laine, Elsa Angelini, Christine Hendon, Yu Gan</li>
<li>for: 这个研究旨在提高静止学习segmentation的精度，特别是在医疗影像segmentation中，因为这里的标签是劳动密集且需要专业知识。</li>
<li>methods: 本研究使用Segment anything model (SAM)，并导入了一个新的标签损坏模型来提高SAM-based segmentation的精度。这个模型可以区分清洁标签和噪标签，并使用一个自我修复模组来更正噪标签。</li>
<li>results: 研究结果显示，使用提案的模型可以提高segmentation的精度，并在X-ray和肺CT数据集上超越基准方法。<details>
<summary>Abstract</summary>
Segment anything model (SAM) has emerged as the leading approach for zero-shot learning in segmentation, offering the advantage of avoiding pixel-wise annotation. It is particularly appealing in medical image segmentation where annotation is laborious and expertise-demanding. However, the direct application of SAM often yields inferior results compared to conventional fully supervised segmentation networks. While using SAM generated pseudo label could also benefit the training of fully supervised segmentation, the performance is limited by the quality of pseudo labels. In this paper, we propose a novel label corruption to push the boundary of SAM-based segmentation. Our model utilizes a novel noise detection module to distinguish between noisy labels from clean labels. This enables us to correct the noisy labels using an uncertainty-based self-correction module, thereby enriching the clean training set. Finally, we retrain the network with updated labels to optimize its weights for future predictions. One key advantage of our model is its ability to train deep networks using SAM-generated pseudo labels without relying on a subset of expert-level annotations. We demonstrate the effectiveness of our proposed model on both X-ray and lung CT datasets, indicating its ability to improve segmentation accuracy and outperform baseline methods in label correction.
</details>
<details>
<summary>摘要</summary>
对于零条件学习分类，对于医疗影像分类而言，模型有了新的进步——分割任意模型（SAM）。SAM可以避免像素级标注，尤其是在医疗影像分类中，标注是劳动密集且需要专家知识。然而，直接应用SAM often yields inferior results compared to conventional fully supervised segmentation networks。使用SAM生成的伪标签也可以帮助完全指定分类网络的训练，但是性能受到伪标签质量的限制。在这篇论文中，我们提出了一个新的标签损害方法，我们的模型具有一个新的错误检测模组，可以区别于噪标签和清洁标签。这使我们可以使用一个不certainty-based自修正模组来更正噪标签，从而增加清洁训练集。最后，我们重新训练网络使用更新的标签，以便在未来预测中优化其权重。我们的模型有一个关键优势，即可以透过SAM生成的伪标签进行深度网络的训练，不需要一subset of expert-level标注。我们在X-ray和肺CT dataset上显示了我们的提案的效果，显示了它可以提高分类精度和超越基eline方法。
</details></li>
</ul>
<hr>
<h2 id="Decomposition-Ascribed-Synergistic-Learning-for-Unified-Image-Restoration"><a href="#Decomposition-Ascribed-Synergistic-Learning-for-Unified-Image-Restoration" class="headerlink" title="Decomposition Ascribed Synergistic Learning for Unified Image Restoration"></a>Decomposition Ascribed Synergistic Learning for Unified Image Restoration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00759">http://arxiv.org/abs/2308.00759</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinghao Zhang, Jie Huang, Man Zhou, Chongyi Li, Feng Zhao</li>
<li>for: 这个论文主要针对的问题是如何在单一模型中学习多种图像干扰的纠正。</li>
<li>methods: 该论文提出了一种基于Singular Value Decomposition（SVD）的Decomposition Ascribed Synergistic Learning（DASL）方法，包括两种有效的运算器：Singular VEctor Operator（SVEO）和Singular VAlue Operator（SVAO），以便更好地利用多种纠正任务之间的关系。</li>
<li>results: 在五种混合图像纠正任务上进行了广泛的实验，证明了该方法的有效性，包括雨彩纠正、霜尘纠正、噪声纠正、抖润纠正和低光照图像增强。<details>
<summary>Abstract</summary>
Learning to restore multiple image degradations within a single model is quite beneficial for real-world applications. Nevertheless, existing works typically concentrate on regarding each degradation independently, while their relationship has been less exploited to ensure the synergistic learning. To this end, we revisit the diverse degradations through the lens of singular value decomposition, with the observation that the decomposed singular vectors and singular values naturally undertake the different types of degradation information, dividing various restoration tasks into two groups,\ie, singular vector dominated and singular value dominated. The above analysis renders a more unified perspective to ascribe the diverse degradations, compared to previous task-level independent learning. The dedicated optimization of degraded singular vectors and singular values inherently utilizes the potential relationship among diverse restoration tasks, attributing to the Decomposition Ascribed Synergistic Learning (DASL). Specifically, DASL comprises two effective operators, namely, Singular VEctor Operator (SVEO) and Singular VAlue Operator (SVAO), to favor the decomposed optimization, which can be lightly integrated into existing convolutional image restoration backbone. Moreover, the congruous decomposition loss has been devised for auxiliary. Extensive experiments on blended five image restoration tasks demonstrate the effectiveness of our method, including image deraining, image dehazing, image denoising, image deblurring, and low-light image enhancement.
</details>
<details>
<summary>摘要</summary>
To optimize the degraded singular vectors and singular values, we propose the Decomposition Ascribed Synergistic Learning (DASL) method. DASL consists of two effective operators: the Singular VEctor Operator (SVEO) and the Singular VAlue Operator (SVAO). These operators favor the decomposed optimization and can be easily integrated into existing convolutional image restoration backbones. Additionally, we have developed a congruous decomposition loss for auxiliary purposes.Extensive experiments on five blended image restoration tasks (image deraining, image dehazing, image denoising, image deblurring, and low-light image enhancement) demonstrate the effectiveness of our method. Our approach is able to effectively restore images and outperform existing methods in terms of both objective metrics and visual quality.
</details></li>
</ul>
<hr>
<h2 id="Phase-Diverse-Phase-Retrieval-for-Microscopy-Comparison-of-Gaussian-and-Poisson-Approaches"><a href="#Phase-Diverse-Phase-Retrieval-for-Microscopy-Comparison-of-Gaussian-and-Poisson-Approaches" class="headerlink" title="Phase Diverse Phase Retrieval for Microscopy: Comparison of Gaussian and Poisson Approaches"></a>Phase Diverse Phase Retrieval for Microscopy: Comparison of Gaussian and Poisson Approaches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00734">http://arxiv.org/abs/2308.00734</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nikolajreiser/poissonphasediversity">https://github.com/nikolajreiser/poissonphasediversity</a></li>
<li>paper_authors: Nikolaj Reiser, Min Guo, Hari Shroff, Patrick J. La Riviere</li>
<li>for: 这篇论文旨在研究phas diversity是一种广泛偏振 Aberration correction方法，用于 Correcting aberrations in widefield microscopy.</li>
<li>methods: 论文使用了多个图像来估算折射率在干扰系统的 pupil plane，通过解决优化问题来实现折射率的估算。</li>
<li>results: 实验结果表明，Poisson模型比 Gaussian 模型在各种情况下匹配或超越 Gaussian 模型，并且在较低的照明条件下表现更好。Poisson 算法也更加鲁棒于空间不连续的偏振和相位噪声的影响。最后，论文对重新获取受到偏振 corrections 和使用偏振点阵的杂化比较。<details>
<summary>Abstract</summary>
Phase diversity is a widefield aberration correction method that uses multiple images to estimate the phase aberration at the pupil plane of an imaging system by solving an optimization problem. This estimated aberration can then be used to deconvolve the aberrated image or to reacquire it with aberration corrections applied to a deformable mirror. The optimization problem for aberration estimation has been formulated for both Gaussian and Poisson noise models but the Poisson model has never been studied in microscopy nor compared with the Gaussian model. Here, the Gaussian- and Poisson-based estimation algorithms are implemented and compared for widefield microscopy in simulation. The Poisson algorithm is found to match or outperform the Gaussian algorithm in a variety of situations, and converges in a similar or decreased amount of time. The Gaussian algorithm does perform better in low-light regimes when image noise is dominated by additive Gaussian noise. The Poisson algorithm is also found to be more robust to the effects of spatially variant aberration and phase noise. Finally, the relative advantages of re-acquisition with aberration correction and deconvolution with aberrated point spread functions are compared.
</details>
<details>
<summary>摘要</summary>
“phase多样性是一种广角偏差 corrections方法，使用多个图像来估计图像系统的焦点平面上的相位偏差，并通过解决优化问题来应用偏差 corrections。这个估计的偏差可以用来deconvolve受偏差影响的图像，或者使用可变曲面镜来重新获取受偏差 corrections的图像。在微scopic中，Gaussian和Poisson noise模型中的优化问题已经被形式化，但是Poisson模型从来没有在微scopic中被研究过。在这里，Gaussian和Poisson基本的优化算法被实现并比较，在广角微scopic中的 simulate中表现出了不同的情况。Poisson算法在许多情况下与Gaussian算法匹配或超越Gaussian算法，并且在相同或更短的时间内 converges。Gaussian算法在低光量条件下，当图像噪声主要是加itive Gaussian噪声时，表现较好。Poisson算法也较Gaussian算法更加Robust，抗性于空间variant aberration和相位噪声的影响。最后，将受偏差 corrections应用于重新获取图像和受偏差点扩展函数deconvolution的相对优势被比较。”
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://example.com/2023/08/02/eess.IV_2023_08_02/" data-id="cllsjvzef008qf588grh740dn" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_01" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/01/cs.LG_2023_08_01/" class="article-date">
  <time datetime="2023-07-31T16:00:00.000Z" itemprop="datePublished">2023-08-01</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/01/cs.LG_2023_08_01/">cs.LG - 2023-08-01 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Hessian-Aware-Bayesian-Optimization-for-Decision-Making-Systems"><a href="#Hessian-Aware-Bayesian-Optimization-for-Decision-Making-Systems" class="headerlink" title="Hessian-Aware Bayesian Optimization for Decision Making Systems"></a>Hessian-Aware Bayesian Optimization for Decision Making Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00629">http://arxiv.org/abs/2308.00629</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohit Rajpal, Lac Gia Tran, Yehong Zhang, Bryan Kian Hsiang Low</li>
<li>for:  optimize decision making systems in situations where feedback is sparse or uninformative</li>
<li>methods:  Bayesian Optimization and Hessian-aware Bayesian Optimization</li>
<li>results:  effective optimization under resource constraints and malformed feedback settings, demonstrated through experimental results on several benchmarks.Here’s the full text in Simplified Chinese:</li>
<li>for:  optimize 决策系统在具有笼罩或不完整反馈的情况下</li>
<li>methods: Bayesian Optimization 和 Hessian-aware Bayesian Optimization</li>
<li>results:  effective 优化下资源限制和异常反馈设置下，经过实验证明在多个 benchmark 上得到良好的效果。<details>
<summary>Abstract</summary>
Many approaches for optimizing decision making systems rely on gradient based methods requiring informative feedback from the environment. However, in the case where such feedback is sparse or uninformative, such approaches may result in poor performance. Derivative-free approaches such as Bayesian Optimization mitigate the dependency on the quality of gradient feedback, but are known to scale poorly in the high-dimension setting of complex decision making systems. This problem is exacerbated if the system requires interactions between several actors cooperating to accomplish a shared goal. To address the dimensionality challenge, we propose a compact multi-layered architecture modeling the dynamics of actor interactions through the concept of role. Additionally, we introduce Hessian-aware Bayesian Optimization to efficiently optimize the multi-layered architecture parameterized by a large number of parameters. Experimental results demonstrate that our method (HA-GP-UCB) works effectively on several benchmarks under resource constraints and malformed feedback settings.
</details>
<details>
<summary>摘要</summary>
很多决策系统优化方法基于梯度方法，需要环境反馈信息。然而，在环境反馈稀缺或不具有信息时，这些方法可能表现不佳。 derivative-free方法，如 bayesian 优化，可以减少对梯度反馈质量的依赖，但是在复杂决策系统中可能 scales poorly。这个问题更加严重，当系统需要多个演员之间的互动，共同完成一个共同目标时。为了解决维度挑战，我们提议一种嵌入式多层架构，模型演员之间的动态，通过角色概念。此外，我们还引入了Hessian-aware bayesian 优化，高效地优化多层架构中的大量参数。实验结果表明，我们的方法（HA-GP-UCB）在资源限制和损坏反馈设置下表现良好。
</details></li>
</ul>
<hr>
<h2 id="Human-M3-A-Multi-view-Multi-modal-Dataset-for-3D-Human-Pose-Estimation-in-Outdoor-Scenes"><a href="#Human-M3-A-Multi-view-Multi-modal-Dataset-for-3D-Human-Pose-Estimation-in-Outdoor-Scenes" class="headerlink" title="Human-M3: A Multi-view Multi-modal Dataset for 3D Human Pose Estimation in Outdoor Scenes"></a>Human-M3: A Multi-view Multi-modal Dataset for 3D Human Pose Estimation in Outdoor Scenes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00628">http://arxiv.org/abs/2308.00628</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/soullessrobot/human-m3-dataset">https://github.com/soullessrobot/human-m3-dataset</a></li>
<li>paper_authors: Bohao Fan, Siqi Wang, Wenxuan Guo, Wenzhao Zheng, Jianjiang Feng, Jie Zhou</li>
<li>for:  This paper aims to provide a large-scale outdoor multi-modal multi-view multi-person human pose database (Human-M3) to support future research in 3D human pose estimation.</li>
<li>methods:  The proposed algorithm uses multi-modal data input, including RGB images and pointclouds, to generate ground truth annotations and improve the accuracy of human pose estimation.</li>
<li>results:  The proposed algorithm demonstrates the advantages of multi-modal data input for 3D human pose estimation, and the Human-M3 database is found to be challenging and suitable for future research.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文目的是提供一个大规模的户外多模态多视图多人姿态数据库（人类M3数据库），以支持未来的3D人姿估计研究。</li>
<li>methods: 提议的算法使用多模态数据输入，包括RGB图像和点云数据，生成ground truth注释，并提高人姿估计的准确性。</li>
<li>results: 提议的算法表明多模态数据输入对3D人姿估计具有优势，人类M3数据库被发现是一个挑战性的和适用于未来研究的数据库。<details>
<summary>Abstract</summary>
3D human pose estimation in outdoor environments has garnered increasing attention recently. However, prevalent 3D human pose datasets pertaining to outdoor scenes lack diversity, as they predominantly utilize only one type of modality (RGB image or pointcloud), and often feature only one individual within each scene. This limited scope of dataset infrastructure considerably hinders the variability of available data. In this article, we propose Human-M3, an outdoor multi-modal multi-view multi-person human pose database which includes not only multi-view RGB videos of outdoor scenes but also corresponding pointclouds. In order to obtain accurate human poses, we propose an algorithm based on multi-modal data input to generate ground truth annotation. This benefits from robust pointcloud detection and tracking, which solves the problem of inaccurate human localization and matching ambiguity that may exist in previous multi-view RGB videos in outdoor multi-person scenes, and generates reliable ground truth annotations. Evaluation of multiple different modalities algorithms has shown that this database is challenging and suitable for future research. Furthermore, we propose a 3D human pose estimation algorithm based on multi-modal data input, which demonstrates the advantages of multi-modal data input for 3D human pose estimation. Code and data will be released on https://github.com/soullessrobot/Human-M3-Dataset.
</details>
<details>
<summary>摘要</summary>
Recently, 3D人体姿态估算在外部环境中获得了越来越多的关注。然而，现有的3D人体姿态数据集，主要是基于单一模式（RGB图像或点云），并且通常只有一个人在每个场景中。这限制了可用数据的多样性。在这篇文章中，我们提出了人类-M3数据集，这是一个包含多视角RGB视频和相应的点云的外部多模式多人体姿态数据集。为了获得准确的人体姿态，我们提出了基于多模式数据输入的算法来生成基准注释。这使得可以解决前一些多视角RGB视频中的人类本地化和匹配抽象问题，并生成可靠的基准注释。评估多种模式算法后，我们发现这个数据集是挑战性的和适用于未来研究。此外，我们还提出了基于多模式数据输入的3D人体姿态估算算法，这示出了多模式数据输入对3D人体姿态估算的优势。代码和数据将在https://github.com/soullessrobot/Human-M3-Dataset上发布。
</details></li>
</ul>
<hr>
<h2 id="Beyond-One-Hot-Encoding-Injecting-Semantics-to-Drive-Image-Classifiers"><a href="#Beyond-One-Hot-Encoding-Injecting-Semantics-to-Drive-Image-Classifiers" class="headerlink" title="Beyond One-Hot-Encoding: Injecting Semantics to Drive Image Classifiers"></a>Beyond One-Hot-Encoding: Injecting Semantics to Drive Image Classifiers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00607">http://arxiv.org/abs/2308.00607</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/s1m0n38/semantic-encodings">https://github.com/s1m0n38/semantic-encodings</a></li>
<li>paper_authors: Alan Perotti, Simone Bertolotto, Eliana Pastor, André Panisson</li>
<li>for: 本研究旨在提高图像分类模型的可解释性和可靠性，通过 incorporating semantic information into the training process。</li>
<li>methods: 本文提出了一种基于 semantic information 的扩展损失函数，可以帮助模型更好地理解图像的内容和意义。具体来说， authors 使用ontology和word embedding等 semantic information来Derive an additional loss term，并通过supervised learning来使模型更加可信。</li>
<li>results: 经过实验 validate 的结果表明，该方法可以提高图像分类模型的准确率和 mistake severity，同时也可以提高模型的内部表示能力。此外， authors 还发现，该方法可以增加模型的解释性和对抗攻击性。<details>
<summary>Abstract</summary>
Images are loaded with semantic information that pertains to real-world ontologies: dog breeds share mammalian similarities, food pictures are often depicted in domestic environments, and so on. However, when training machine learning models for image classification, the relative similarities amongst object classes are commonly paired with one-hot-encoded labels. According to this logic, if an image is labelled as 'spoon', then 'tea-spoon' and 'shark' are equally wrong in terms of training loss. To overcome this limitation, we explore the integration of additional goals that reflect ontological and semantic knowledge, improving model interpretability and trustworthiness. We suggest a generic approach that allows to derive an additional loss term starting from any kind of semantic information about the classification label. First, we show how to apply our approach to ontologies and word embeddings, and discuss how the resulting information can drive a supervised learning process. Second, we use our semantically enriched loss to train image classifiers, and analyse the trade-offs between accuracy, mistake severity, and learned internal representations. Finally, we discuss how this approach can be further exploited in terms of explainability and adversarial robustness. Code repository: https://github.com/S1M0N38/semantic-encodings
</details>
<details>
<summary>摘要</summary>
图像含有semantic信息，与现实世界 ontology 相关：狗种类共享哺乳动物类似性，食物图像经常在家庭环境中描绘，等等。然而，在机器学习模型图像分类训练中，对象类之间的相似性通常与一个热度编码的标签相对。根据这种逻辑，如果一个图像被标记为 " spoons" ，那么 " tea-spoon" 和 " shark" 在训练损失方面都是错误的。为了超越这些限制，我们探索了 Semantic Encodings 的整合，以提高模型解释性和可靠性。我们建议一种通用的方法，可以从任何类型的 semantic 信息中 derivate 额外的损失项。首先，我们示示如何应用我们的方法到ontology 和 word embedding中，并讨论如何将这些信息驱动一个supervised learning过程。其次，我们使用我们增强的损失函数来训练图像分类器，并分析损失函数的交易OFF、严重性和学习内部表示。最后，我们讨论如何进一步利用这种方法来提高解释性和对抗 robustness。代码存储库：https://github.com/S1M0N38/semantic-encodings
</details></li>
</ul>
<hr>
<h2 id="Latent-Shift-Gradient-of-Entropy-Helps-Neural-Codecs"><a href="#Latent-Shift-Gradient-of-Entropy-Helps-Neural-Codecs" class="headerlink" title="Latent-Shift: Gradient of Entropy Helps Neural Codecs"></a>Latent-Shift: Gradient of Entropy Helps Neural Codecs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00725">http://arxiv.org/abs/2308.00725</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammet Balcilar, Bharath Bhushan Damodaran, Karam Naser, Franck Galpin, Pierre Hellier</li>
<li>for: 这个论文是为了提出一种基于ENTROPY GRADIENT的图像&#x2F;视频编码方法，以提高图像&#x2F;视频的压缩率。</li>
<li>methods: 该方法利用了ENTROPY GRADIENT，并通过对图像&#x2F;视频的抽象来实现高效的压缩。</li>
<li>results: 实验表明，使用ENTROPY GRADIENT可以实现$1-2%$的压缩率提高，而且与其他改进方法独立。<details>
<summary>Abstract</summary>
End-to-end image/video codecs are getting competitive compared to traditional compression techniques that have been developed through decades of manual engineering efforts. These trainable codecs have many advantages over traditional techniques such as easy adaptation on perceptual distortion metrics and high performance on specific domains thanks to their learning ability. However, state of the art neural codecs does not take advantage of the existence of gradient of entropy in decoding device. In this paper, we theoretically show that gradient of entropy (available at decoder side) is correlated with the gradient of the reconstruction error (which is not available at decoder side). We then demonstrate experimentally that this gradient can be used on various compression methods, leading to a $1-2\%$ rate savings for the same quality. Our method is orthogonal to other improvements and brings independent rate savings.
</details>
<details>
<summary>摘要</summary>
现代图像/视频编码器在满足人类视觉需求方面已经具有竞争力，而传统的压缩技术则是通过多年的手动工程努力而开发的。这些可调编码器具有许多优势，如根据人类视觉评价指标进行易于适应的修正，以及在特定领域中表现出色的高性能，归功于其学习能力。然而，当前的 neural codecs 并不利用decode器 сторо的梯度 entropy的存在。在这篇论文中，我们理论上验证了梯度 entropy （可以在decoder сторо上获得）与重建错误的梯度（不可以在decoder сторо上获得）之间的相关性。我们then通过实验表明，这个梯度可以在不同的压缩方法上使用，导致1-2%的比特率减少，同时保持相同的质量。我们的方法是对其他改进 независимы的。
</details></li>
</ul>
<hr>
<h2 id="Regularization-early-stopping-and-dreaming-a-Hopfield-like-setup-to-address-generalization-and-overfitting"><a href="#Regularization-early-stopping-and-dreaming-a-Hopfield-like-setup-to-address-generalization-and-overfitting" class="headerlink" title="Regularization, early-stopping and dreaming: a Hopfield-like setup to address generalization and overfitting"></a>Regularization, early-stopping and dreaming: a Hopfield-like setup to address generalization and overfitting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01421">http://arxiv.org/abs/2308.01421</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elena Agliari, Miriam Aquaro, Francesco Alemanno, Alberto Fachechi</li>
<li>for: 本文研究了吸引器神经网络，从机器学习角度来看，通过梯度下降来找到优化的网络参数。</li>
<li>methods: 本文使用了梯度下降法，并在正则化损失函数上进行了优化。在这个框架下，最佳的 neuron-interaction 矩阵被证明为一类具有 Hebbian 核函数修改的步骤，并且这些步骤与正则化参数和训练时间有关。因此，可以采取基于交互矩阵的步骤来避免过拟合。</li>
<li>results: 本文通过分析随机生成的 sintetic 数据集，以及数值实验，发现了不同的拟合情况（即过拟合、失败和成功），这与数据集参数的变化有关。<details>
<summary>Abstract</summary>
In this work we approach attractor neural networks from a machine learning perspective: we look for optimal network parameters by applying a gradient descent over a regularized loss function. Within this framework, the optimal neuron-interaction matrices turn out to be a class of matrices which correspond to Hebbian kernels revised by iteratively applying some unlearning protocols. Remarkably, the number of unlearning steps is proved to be related to the regularization hyperparameters of the loss function and to the training time. Thus, we can design strategies to avoid overfitting that are formulated in terms of the algebraic properties of the interaction matrix, or, equivalently, in terms of regularization tuning and early-stopping strategies. The generalization capabilities of these attractor networks are also investigated: analytical results are obtained for random synthetic datasets, next, the emerging picture is corroborated by numerical experiments that highlight the existence of several regimes (i.e., overfitting, failure and success) as the dataset parameters are varied.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们从机器学习角度来看待吸引器神经网络：我们通过应用梯度下降来找到最佳网络参数，并且在这个框架下，最佳的神经元互动矩阵实际上是一类具有希贝尔核函数修改的谱函数约束。意外地，训练时间和正则化参数之间存在一定的关系，这使得我们可以通过对学习步数进行设置，以避免过拟合。此外，我们还可以通过对互动矩阵的特性进行分析，以及正则化调整和早停策略的设置，来提高模型的泛化能力。通过随机生成的 sintetic 数据和实验证明，我们发现了不同的训练 dataset 参数下的不同拟合情况（即过拟合、失败和成功）。
</details></li>
</ul>
<hr>
<h2 id="Semisupervised-Anomaly-Detection-using-Support-Vector-Regression-with-Quantum-Kernel"><a href="#Semisupervised-Anomaly-Detection-using-Support-Vector-Regression-with-Quantum-Kernel" class="headerlink" title="Semisupervised Anomaly Detection using Support Vector Regression with Quantum Kernel"></a>Semisupervised Anomaly Detection using Support Vector Regression with Quantum Kernel</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00583">http://arxiv.org/abs/2308.00583</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kilian Tscharke, Sebastian Issel, Pascal Debus</li>
<li>for: 本研究旨在提出一种基于支持向量回归（SVR）的半监督分类方法，用于矩阵缺失（AD）问题。</li>
<li>methods: 本方法使用量子kernel估计来实现SVR模型，并与量子自适应编码器和量子一类分类器进行比较。</li>
<li>results: 对11个实际AD数据集和1个玩具数据集进行了广泛的 benchmarking，结果显示，我们的SVR模型使用量子kernel比SVR模型使用径向基函数（RBF）kernel和所有其他模型都更高，取得了所有数据集的平均AUC最高。<details>
<summary>Abstract</summary>
Anomaly detection (AD) involves identifying observations or events that deviate in some way from the rest of the data. Machine learning techniques have shown success in automating this process by detecting hidden patterns and deviations in large-scale data. The potential of quantum computing for machine learning has been widely recognized, leading to extensive research efforts to develop suitable quantum machine learning (QML) algorithms. In particular, the search for QML algorithms for near-term NISQ devices is in full swing. However, NISQ devices pose additional challenges due to their limited qubit coherence times, low number of qubits, and high error rates. Kernel methods based on quantum kernel estimation have emerged as a promising approach to QML on NISQ devices, offering theoretical guarantees, versatility, and compatibility with NISQ constraints. Especially support vector machines (SVM) utilizing quantum kernel estimation have shown success in various supervised learning tasks. However, in the context of AD, semisupervised learning is of great relevance, and yet there is limited research published in this area. This paper introduces an approach to semisupervised AD based on the reconstruction loss of a support vector regression (SVR) with quantum kernel. This novel model is an alternative to the variational quantum and quantum kernel one-class classifiers, and is compared to a quantum autoencoder as quantum baseline and a SVR with radial-basis-function (RBF) kernel as well as a classical autoencoder as classical baselines. The models are benchmarked extensively on 10 real-world AD data sets and one toy data set, and it is shown that our SVR model with quantum kernel performs better than the SVR with RBF kernel as well as all other models, achieving highest mean AUC over all data sets. In addition, our QSVR outperforms the quantum autoencoder on 9 out of 11 data sets.
</details>
<details>
<summary>摘要</summary>
anomaly detection (AD) 涉及到从数据中找到不同的观察或事件。机器学习技术已经在大规模数据中自动找到隐藏的模式和偏差，并且在量子计算领域的研究已经得到了广泛的认可。特别是在近期的NISQ设备上进行量子机器学习（QML）的研究在全面进行。然而，NISQ设备具有有限的量子缓存时间、少量的量子比特和高错误率，这些问题对QML的应用带来了额外的挑战。基于量子kernel估计的kernel方法在NISQ设备上进行QML已经显示出了潜在的优势，包括理论保证、灵活性和NISQ约束的兼容性。尤其是在SVM中使用量子kernel估计，已经在不同的指导学习任务中显示出了成功。然而，在AD中，半supervised学习是非常重要的，但是现有的研究非常有限。本文介绍了一种基于支持向量回归（SVR）的半supervised AD模型，利用量子kernel估计。这种新的模型是与量子自编码器和量子kernel一类的一类的一类，并与量子自编码器和 радиаль基函数kernel SVR 以及经典自编码器作为经典基准进行比较。模型在10个真实的AD数据集和1个模拟数据集上进行了广泛的 benchmarking，结果显示，我们的SVR模型使用量子kernel可以比量子自编码器和RBF kernel SVR以及其他所有模型的mean AUC得到更高的性能，并且在9个数据集中超过了量子自编码器。此外，我们的QSVR还超过了经典自编码器在9个数据集中。
</details></li>
</ul>
<hr>
<h2 id="Graph-Neural-Networks-for-Forecasting-Multivariate-Realized-Volatility-with-Spillover-Effects"><a href="#Graph-Neural-Networks-for-Forecasting-Multivariate-Realized-Volatility-with-Spillover-Effects" class="headerlink" title="Graph Neural Networks for Forecasting Multivariate Realized Volatility with Spillover Effects"></a>Graph Neural Networks for Forecasting Multivariate Realized Volatility with Spillover Effects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01419">http://arxiv.org/abs/2308.01419</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao Zhang, Xingyue Pu, Mihai Cucuringu, Xiaowen Dong</li>
<li>for: 该论文是为了模型和预测多变量实现的投资风险管理。</li>
<li>methods: 论文使用自定义图 neural network 来 incorporate 股票之间的延迟效应，并可以模型非线性关系和不同的损失函数进行灵活的训练。</li>
<li>results: 研究发现，包含多个邻居之间的延迟效应不一定能够提高预测精度，但是模型非线性延迟效应可以提高实现波动精度，特别是在短期预测（一周内）。此外，使用 quasi-likelihood 损失函数可以提高模型性能。<details>
<summary>Abstract</summary>
We present a novel methodology for modeling and forecasting multivariate realized volatilities using customized graph neural networks to incorporate spillover effects across stocks. The proposed model offers the benefits of incorporating spillover effects from multi-hop neighbors, capturing nonlinear relationships, and flexible training with different loss functions. Our empirical findings provide compelling evidence that incorporating spillover effects from multi-hop neighbors alone does not yield a clear advantage in terms of predictive accuracy. However, modeling nonlinear spillover effects enhances the forecasting accuracy of realized volatilities, particularly for short-term horizons of up to one week. Moreover, our results consistently indicate that training with the Quasi-likelihood loss leads to substantial improvements in model performance compared to the commonly-used mean squared error. A comprehensive series of empirical evaluations in alternative settings confirm the robustness of our results.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的方法，用自定义图神经网络来模型和预测多variate实现的波动。我们的提案方法可以捕捉多个跳数的影响，捕捉非线性关系，并且可以自由地训练使用不同的损失函数。我们的实验结果表明，只通过考虑多个跳数的影响不会提供明显的预测精度优势。但是，模型非线性的影响可以提高实现波动的预测精度，特别是在短期horizon（一周以下）。此外，我们的结果表明，使用 quasi-likelihood损失函数进行训练可以提高模型性能，相比于常用的mean squared error。我们在不同的设定下进行了全面的实验评估，并证明了我们的结果的可靠性。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Collaborative-Filtering-with-Personalized-Time-Decay-Functions-for-Financial-Product-Recommendation"><a href="#Adaptive-Collaborative-Filtering-with-Personalized-Time-Decay-Functions-for-Financial-Product-Recommendation" class="headerlink" title="Adaptive Collaborative Filtering with Personalized Time Decay Functions for Financial Product Recommendation"></a>Adaptive Collaborative Filtering with Personalized Time Decay Functions for Financial Product Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01208">http://arxiv.org/abs/2308.01208</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ashraf Ghiye, Baptiste Barreau, Laurent Carlier, Michalis Vazirgiannis</li>
<li>for: 提高金融产品推荐的可靠性和准确性，特别是在时间敏感的场景下。</li>
<li>methods: 提出了一种基于时间的共同推荐算法，可以自适应地减少远程客户产品交互的 Utility 值，使用个性化衰减函数模型客户对产品的动态共同信号。</li>
<li>results: 使用 proprietary 数据集进行评估，与文献中的参考基elines进行比较，显示了明显的提升，强调了在模型中Explicitly 处理时间的重要性，以提高金融产品推荐的准确性。<details>
<summary>Abstract</summary>
Classical recommender systems often assume that historical data are stationary and fail to account for the dynamic nature of user preferences, limiting their ability to provide reliable recommendations in time-sensitive settings. This assumption is particularly problematic in finance, where financial products exhibit continuous changes in valuations, leading to frequent shifts in client interests. These evolving interests, summarized in the past client-product interactions, see their utility fade over time with a degree that might differ from one client to another. To address this challenge, we propose a time-dependent collaborative filtering algorithm that can adaptively discount distant client-product interactions using personalized decay functions. Our approach is designed to handle the non-stationarity of financial data and produce reliable recommendations by modeling the dynamic collaborative signals between clients and products. We evaluate our method using a proprietary dataset from BNP Paribas and demonstrate significant improvements over state-of-the-art benchmarks from relevant literature. Our findings emphasize the importance of incorporating time explicitly in the model to enhance the accuracy of financial product recommendation.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Robust-Linear-Regression-Phase-Transitions-and-Precise-Tradeoffs-for-General-Norms"><a href="#Robust-Linear-Regression-Phase-Transitions-and-Precise-Tradeoffs-for-General-Norms" class="headerlink" title="Robust Linear Regression: Phase-Transitions and Precise Tradeoffs for General Norms"></a>Robust Linear Regression: Phase-Transitions and Precise Tradeoffs for General Norms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00556">http://arxiv.org/abs/2308.00556</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elvis Dohmatob, Meyer Scetbon</li>
<li>for: 本研究 investigate 测试时对Linear Regression模型的抗击攻击的影响，并确定任何模型可以保持给定水平的标准预测性能（准确率）的最佳 robustness 水平。</li>
<li>methods: 通过量化估计，我们揭示了不同 режим下的抗击坚定性和准确率之间的基本贸易。我们获得了精确地分类 regime 中抗击坚定性可以保持标准准确率无损的情况，以及抗击坚定性和准确率之间可能存在贸易的情况。</li>
<li>results: 我们通过简单的实验证明了我们的发现，并验证了这些发现在不同的设定下是否成立。本研究适用于特征协方差矩阵和攻击范数任何性质，并超越过去在这个领域的研究。<details>
<summary>Abstract</summary>
In this paper, we investigate the impact of test-time adversarial attacks on linear regression models and determine the optimal level of robustness that any model can reach while maintaining a given level of standard predictive performance (accuracy). Through quantitative estimates, we uncover fundamental tradeoffs between adversarial robustness and accuracy in different regimes. We obtain a precise characterization which distinguishes between regimes where robustness is achievable without hurting standard accuracy and regimes where a tradeoff might be unavoidable. Our findings are empirically confirmed with simple experiments that represent a variety of settings. This work applies to feature covariance matrices and attack norms of any nature, and extends beyond previous works in this area.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了测试时对线性回归模型的抗击攻击对模型的影响，并确定了保持给定水平的标准预测性能（准确率）的最佳鲁棒性水平。通过量化估计，我们揭示了不同场景下抗击鲁棒性和准确率之间的基本交易offs。我们获得了精确地分类型，可以分为不会受到交易offs的场景和可能不可避免的交易offs场景。我们的发现得到了实际验证，通过一些简单的实验，这些实验代表了多种设置。这项工作适用于特征协方差矩阵和攻击 нормы的任何种类，并超越了之前在这一领域的研究。
</details></li>
</ul>
<hr>
<h2 id="Copula-for-Instance-wise-Feature-Selection-and-Ranking"><a href="#Copula-for-Instance-wise-Feature-Selection-and-Ranking" class="headerlink" title="Copula for Instance-wise Feature Selection and Ranking"></a>Copula for Instance-wise Feature Selection and Ranking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00549">http://arxiv.org/abs/2308.00549</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hanyu Peng, Guanhua Fang, Ping Li</li>
<li>for: 提高 neural network 中 feature 的选择和排序精度，使其更适合每个样本。</li>
<li>methods: 使用 Gaussian copula 技术 capture 特征之间的相关性，不需要额外变更现有的特征选择框架。</li>
<li>results: 对 synthetic 和实际数据进行比较性和可读性的实验，表明我们的方法可以捕捉有意义的相关性。<details>
<summary>Abstract</summary>
Instance-wise feature selection and ranking methods can achieve a good selection of task-friendly features for each sample in the context of neural networks. However, existing approaches that assume feature subsets to be independent are imperfect when considering the dependency between features. To address this limitation, we propose to incorporate the Gaussian copula, a powerful mathematical technique for capturing correlations between variables, into the current feature selection framework with no additional changes needed. Experimental results on both synthetic and real datasets, in terms of performance comparison and interpretability, demonstrate that our method is capable of capturing meaningful correlations.
</details>
<details>
<summary>摘要</summary>
<<SYS>> instance-wise 特征选择和排名方法可以在神经网络中选择每个样本的任务友好特征，但现有的方法假设特征子集是独立的，这会导致限制。为了解决这个限制，我们提议将 Gaussian copula，一种强大的变量相关技术， integrate 到当前特征选择框架中，无需进行额外变更。实验结果表明，我们的方法可以 capture 意义的相关性。在 both synthetic 和实际数据集上，我们的方法可以在性能比较和可读性方面与现有方法进行比较，并且可以 capture 意义的相关性。Gaussian copula 是一种强大的变量相关技术，可以用来捕捉特征之间的相关性。通过 integrate 这种技术到当前特征选择框架中，我们可以更好地 capture 意义的相关性，从而提高模型的性能。实验结果表明，我们的方法可以在不同的数据集上 capture 意义的相关性，并且可以与现有方法进行比较。此外，我们的方法还可以提供更多的可读性，使得用户可以更好地理解模型的决策过程。总之，我们的方法可以帮助用户更好地选择和排名特征，从而提高模型的性能和可读性。
</details></li>
</ul>
<hr>
<h2 id="Predicting-Early-Dropouts-of-an-Active-and-Healthy-Ageing-App"><a href="#Predicting-Early-Dropouts-of-an-Active-and-Healthy-Ageing-App" class="headerlink" title="Predicting Early Dropouts of an Active and Healthy Ageing App"></a>Predicting Early Dropouts of an Active and Healthy Ageing App</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00539">http://arxiv.org/abs/2308.00539</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vasileios Perifanis, Ioanna Michailidi, Giorgos Stamatelatos, George Drosatos, Pavlos S. Efraimidis</li>
<li>for: 预测年轻人健康长逝APP使用者早期退出</li>
<li>methods: 使用机器学习方法处理数据，构建分类模型预测用户忠诚度，并使用SMOTE和ADASYN等扩大样本方法提高分类性能</li>
<li>results: 实验结果表明，机器学习算法可以提供高质量的忠诚度预测，动态特征对模型预测性能产生积极影响，并使用扩大样本方法可以提高分类性能10%。<details>
<summary>Abstract</summary>
In this work, we present a machine learning approach for predicting early dropouts of an active and healthy ageing app. The presented algorithms have been submitted to the IFMBE Scientific Challenge 2022, part of IUPESM WC 2022. We have processed the given database and generated seven datasets. We used pre-processing techniques to construct classification models that predict the adherence of users using dynamic and static features. We submitted 11 official runs and our results show that machine learning algorithms can provide high-quality adherence predictions. Based on the results, the dynamic features positively influence a model's classification performance. Due to the imbalanced nature of the dataset, we employed oversampling methods such as SMOTE and ADASYN to improve the classification performance. The oversampling approaches led to a remarkable improvement of 10\%. Our methods won first place in the IFMBE Scientific Challenge 2022.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们提出了一种机器学习方法，用于预测活跃和健康年龄应用程序中的早期退出。我们提交了IFMBE科学挑战2022，该挑战是IUPESM WC 2022的一部分。我们处理了给定的数据库，生成了七个数据集。我们使用预处理技术，构建了用于预测用户使用动态和静止特征的分类模型。我们提交了11个官方运行，我们的结果表明，机器学习算法可以提供高质量的遵从性预测。根据结果，动态特征对模型的分类性能产生了积极的影响。由于数据集的不均衡性，我们使用了扩大样本的方法，如SMOTE和ADASYN，以改善分类性能。这些扩大样本方法导致了10%的很显著的提高。我们的方法在IFMBE科学挑战2022中获得了第一名。
</details></li>
</ul>
<hr>
<h2 id="Graph-Embedding-Dynamic-Feature-based-Supervised-Contrastive-Learning-of-Transient-Stability-for-Changing-Power-Grid-Topologies"><a href="#Graph-Embedding-Dynamic-Feature-based-Supervised-Contrastive-Learning-of-Transient-Stability-for-Changing-Power-Grid-Topologies" class="headerlink" title="Graph Embedding Dynamic Feature-based Supervised Contrastive Learning of Transient Stability for Changing Power Grid Topologies"></a>Graph Embedding Dynamic Feature-based Supervised Contrastive Learning of Transient Stability for Changing Power Grid Topologies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00537">http://arxiv.org/abs/2308.00537</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zijian Lv, Xin Chen, Zijian Feng</li>
<li>for: 提高电力系统稳定性预测精度，适应干扰影响。</li>
<li>methods: 使用高维电力网 topology信息向低维节点基 embedding数据流动化，并使用 supervised contrastive learning 预测瞬态稳定性。</li>
<li>results: 测试结果表明，提出的 GEDF-SCL 模型可以高度精度地预测瞬态稳定性，同时适应变化的电力网 topology。<details>
<summary>Abstract</summary>
Accurate online transient stability prediction is critical for ensuring power system stability when facing disturbances. While traditional transient stablity analysis replies on the time domain simulations can not be quickly adapted to the power grid toplogy change. In order to vectorize high-dimensional power grid topological structure information into low-dimensional node-based graph embedding streaming data, graph embedding dynamic feature (GEDF) has been proposed. The transient stability GEDF-based supervised contrastive learning (GEDF-SCL) model uses supervised contrastive learning to predict transient stability with GEDFs, considering power grid topology information. To evaluate the performance of the proposed GEDF-SCL model, power grids of varying topologies were generated based on the IEEE 39-bus system model. Transient operational data was obtained by simulating N-1 and N-$\bm{m}$-1 contingencies on these generated power system topologies. Test result demonstrated that the GEDF-SCL model can achieve high accuracy in transient stability prediction and adapt well to changing power grid topologies.
</details>
<details>
<summary>摘要</summary>
正确的在线稳定预测是电力系统稳定性的关键，当面临干扰时，可以帮助恢复稳定。传统的稳定预测方法基于时域仿真，但是这些方法不能快速适应电力网络结构变化。为了将高维电力网络结构信息转化为低维节点基于图像流动数据，提出了图像动态特征（GEDF）。基于GEDF的稳定预测模型使用超级对比学习（SCL）来预测稳定性，考虑电力网络结构信息。为评估提案模型的性能，基于IEEE 39-bus系统模型生成了不同电力网络结构。通过对这些生成的电力系统结构进行 simulate N-1和N-m-1的干扰，获得了过程数据。测试结果表明，GEDF-SCL模型可以在稳定预测中达到高精度和适应电力网络结构变化。
</details></li>
</ul>
<hr>
<h2 id="Graph-Contrastive-Learning-with-Generative-Adversarial-Network"><a href="#Graph-Contrastive-Learning-with-Generative-Adversarial-Network" class="headerlink" title="Graph Contrastive Learning with Generative Adversarial Network"></a>Graph Contrastive Learning with Generative Adversarial Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00535">http://arxiv.org/abs/2308.00535</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheng Wu, Chaokun Wang, Jingcao Xu, Ziyang Liu, Kai Zheng, Xiaowei Wang, Yang Song, Kun Gai</li>
<li>for: 用于 Graph Contrastive Learning (GCL) 的训练，以便在实际应用中减少标签的问题。</li>
<li>methods: 利用 Graph Contrastive Learning (GCL) 和 Graph Generative Adversarial Networks (GANs) 来训练 Graph Neural Networks (GNNs)，并通过自动生成图的视图来捕捉图的特性。</li>
<li>results: 在七个真实世界数据集上进行了广泛的实验，并证明了 GACN 可以生成高质量的扩充视图，并且高于十二个基eline方法。同时，GACN 意外发现生成的视图最终遵循了在线网络中知名的偏好附加规则。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have demonstrated promising results on exploiting node representations for many downstream tasks through supervised end-to-end training. To deal with the widespread label scarcity issue in real-world applications, Graph Contrastive Learning (GCL) is leveraged to train GNNs with limited or even no labels by maximizing the mutual information between nodes in its augmented views generated from the original graph. However, the distribution of graphs remains unconsidered in view generation, resulting in the ignorance of unseen edges in most existing literature, which is empirically shown to be able to improve GCL's performance in our experiments. To this end, we propose to incorporate graph generative adversarial networks (GANs) to learn the distribution of views for GCL, in order to i) automatically capture the characteristic of graphs for augmentations, and ii) jointly train the graph GAN model and the GCL model. Specifically, we present GACN, a novel Generative Adversarial Contrastive learning Network for graph representation learning. GACN develops a view generator and a view discriminator to generate augmented views automatically in an adversarial style. Then, GACN leverages these views to train a GNN encoder with two carefully designed self-supervised learning losses, including the graph contrastive loss and the Bayesian personalized ranking Loss. Furthermore, we design an optimization framework to train all GACN modules jointly. Extensive experiments on seven real-world datasets show that GACN is able to generate high-quality augmented views for GCL and is superior to twelve state-of-the-art baseline methods. Noticeably, our proposed GACN surprisingly discovers that the generated views in data augmentation finally conform to the well-known preferential attachment rule in online networks.
</details>
<details>
<summary>摘要</summary>
graph neural networks (GNNs) 已经在许多下游任务上达到了可观的成果，通过监督式终到训练来利用节点表示。然而，在实际应用中，标签的缺乏问题是普遍存在的。为了解决这个问题，我们提出了使用图对照学习（GCL）来训练GNNs，不需要或者even不需要标签。GCL通过最大化图中节点之间的共同信息来进行训练。然而，现有文献中忽略了图的分布，导致对未见边的忽略。我们提出了通过图生成对抗网络（GANs）来学习图的分布，以便自动捕捉图的特征，并将图生成模型和GCL模型同时训练。我们提出了一种名为GACN的图生成对抗学习网络，它包括视图生成器和视图识别器。GACN使用对抗风格来自动生成增强视图。然后，GACN使用这些增强视图来训练GNN编码器，并使用两种特殊的自我监督学习损失函数，包括图对照损失和 bayesian人性化排名损失。此外，我们设计了一种培训所有GACN模块的优化框架。我们在七个真实世界数据集上进行了广泛的实验，显示GACN能够生成高质量的增强视图，并比十二个州先进方法更佳。另外，我们发现了GACN所生成的视图最终遵循了在在线网络中广泛存在的偏好附加规则。
</details></li>
</ul>
<hr>
<h2 id="A-Novel-Temporal-Multi-Gate-Mixture-of-Experts-Approach-for-Vehicle-Trajectory-and-Driving-Intention-Prediction"><a href="#A-Novel-Temporal-Multi-Gate-Mixture-of-Experts-Approach-for-Vehicle-Trajectory-and-Driving-Intention-Prediction" class="headerlink" title="A Novel Temporal Multi-Gate Mixture-of-Experts Approach for Vehicle Trajectory and Driving Intention Prediction"></a>A Novel Temporal Multi-Gate Mixture-of-Experts Approach for Vehicle Trajectory and Driving Intention Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00533">http://arxiv.org/abs/2308.00533</a></li>
<li>repo_url: None</li>
<li>paper_authors: Renteng Yuan, Mohamed Abdel-Aty, Qiaojun Xiang, Zijin Wang, Ou Zheng</li>
<li>for: 预测车辆轨迹和驾驶意图</li>
<li>methods: 使用Temporal Multi-Gate Mixture-of-Experts（TMMOE）模型同时预测车辆轨迹和驾驶意图，包括三层：共享层、专家层和全连接层。在模型中，共享层使用Temporal Convolutional Networks（TCN）提取时间特征，然后专家层可以识别不同任务的信息。</li>
<li>results: 使用uncertainty算法建构多任务损失函数，最终在CitySim数据集上验证TMMOE模型，与LSTM模型相比，实现最高的分类和回归结果。<details>
<summary>Abstract</summary>
Accurate Vehicle Trajectory Prediction is critical for automated vehicles and advanced driver assistance systems. Vehicle trajectory prediction consists of two essential tasks, i.e., longitudinal position prediction and lateral position prediction. There is a significant correlation between driving intentions and vehicle motion. In existing work, the three tasks are often conducted separately without considering the relationships between the longitudinal position, lateral position, and driving intention. In this paper, we propose a novel Temporal Multi-Gate Mixture-of-Experts (TMMOE) model for simultaneously predicting the vehicle trajectory and driving intention. The proposed model consists of three layers: a shared layer, an expert layer, and a fully connected layer. In the model, the shared layer utilizes Temporal Convolutional Networks (TCN) to extract temporal features. Then the expert layer is built to identify different information according to the three tasks. Moreover, the fully connected layer is used to integrate and export prediction results. To achieve better performance, uncertainty algorithm is used to construct the multi-task loss function. Finally, the publicly available CitySim dataset validates the TMMOE model, demonstrating superior performance compared to the LSTM model, achieving the highest classification and regression results. Keywords: Vehicle trajectory prediction, driving intentions Classification, Multi-task
</details>
<details>
<summary>摘要</summary>
准确预测车辆轨迹是自动驾驶和高级驾驶助手系统中关键的一环。车辆轨迹预测包括两个基本任务，即长进位预测和横向位预测。车辆的运动和驾驶意图之间存在显著的相关性。现有的工作通常将这三个任务分别进行，没有考虑这三个任务之间的关系。本文提出了一种新的时间多门混合专家（TMMOE）模型，用于同时预测车辆轨迹和驾驶意图。该模型包括三层：共享层、专家层和全连接层。在模型中，共享层使用时间卷积网络（TCN）提取时间特征。然后，专家层用于识别不同任务中的信息。此外，全连接层用于集成和出口预测结果。为了实现更好的性能，uncertainty算法用于构建多任务损失函数。最后，公开的CitySim数据集 validate了TMMOE模型，达到了LSTM模型的最高分类和回归结果。关键词：车辆轨迹预测、驾驶意图分类、多任务
</details></li>
</ul>
<hr>
<h2 id="Variational-Label-Correlation-Enhancement-for-Congestion-Prediction"><a href="#Variational-Label-Correlation-Enhancement-for-Congestion-Prediction" class="headerlink" title="Variational Label-Correlation Enhancement for Congestion Prediction"></a>Variational Label-Correlation Enhancement for Congestion Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00529">http://arxiv.org/abs/2308.00529</a></li>
<li>repo_url: None</li>
<li>paper_authors: Biao Liu, Congyu Qiao, Ning Xu, Xin Geng, Ziran Zhu, Jun Yang</li>
<li>for: 提高大规模设计过程中的Routing质量预测精度，以促进普遍设计和资源保留。</li>
<li>methods: 利用变量推理技术来估计本地标签相关性权重，并将其与邻居格点的标签相关性权重相乘以提高回归模型的性能。</li>
<li>results: 在公共可用的 \texttt{ISPD2011} 和 \texttt{DAC2012} 测试集上，{\ours} 表现出色，与传统方法相比，提高了回归模型的性能。<details>
<summary>Abstract</summary>
The physical design process of large-scale designs is a time-consuming task, often requiring hours to days to complete, with routing being the most critical and complex step. As the the complexity of Integrated Circuits (ICs) increases, there is an increased demand for accurate routing quality prediction. Accurate congestion prediction aids in identifying design flaws early on, thereby accelerating circuit design and conserving resources. Despite the advancements in current congestion prediction methodologies, an essential aspect that has been largely overlooked is the spatial label-correlation between different grids in congestion prediction. The spatial label-correlation is a fundamental characteristic of circuit design, where the congestion status of a grid is not isolated but inherently influenced by the conditions of its neighboring grids. In order to fully exploit the inherent spatial label-correlation between neighboring grids, we propose a novel approach, {\ours}, i.e., VAriational Label-Correlation Enhancement for Congestion Prediction, which considers the local label-correlation in the congestion map, associating the estimated congestion value of each grid with a local label-correlation weight influenced by its surrounding grids. {\ours} leverages variational inference techniques to estimate this weight, thereby enhancing the regression model's performance by incorporating spatial dependencies. Experiment results validate the superior effectiveness of {\ours} on the public available \texttt{ISPD2011} and \texttt{DAC2012} benchmarks using the superblue circuit line.
</details>
<details>
<summary>摘要</summary>
大规模设计的物理设计过程是一个时间consuming的任务，经常需要几个小时到几天的时间完成，routing是最关键和复杂的步骤。随着集成电路（IC）的复杂度的提高，需要更加准确的routing质量预测。准确的填充预测可以帮助早期发现设计缺陷，从而加速电路设计并保留资源。现有的填充预测方法中一个关键的缺失是忽略了各个网格之间的空间标签相关性。各个网格的填充状态不 solo，而是受到周围网格的状态的影响。为了充分利用这种空间标签相关性，我们提出了一种新的方法，即ours，即Variational Label-Correlation Enhancement for Congestion Prediction。ours通过利用变量推理技术来估算每个网格的本地标签相关性权重，以便在填充预测中更好地考虑空间相关性。实验结果表明ours在公开的 \texttt{ISPD2011} 和 \texttt{DAC2012} benchmark上显著超越了传统的填充预测方法。
</details></li>
</ul>
<hr>
<h2 id="Improved-Prognostic-Prediction-of-Pancreatic-Cancer-Using-Multi-Phase-CT-by-Integrating-Neural-Distance-and-Texture-Aware-Transformer"><a href="#Improved-Prognostic-Prediction-of-Pancreatic-Cancer-Using-Multi-Phase-CT-by-Integrating-Neural-Distance-and-Texture-Aware-Transformer" class="headerlink" title="Improved Prognostic Prediction of Pancreatic Cancer Using Multi-Phase CT by Integrating Neural Distance and Texture-Aware Transformer"></a>Improved Prognostic Prediction of Pancreatic Cancer Using Multi-Phase CT by Integrating Neural Distance and Texture-Aware Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00507">http://arxiv.org/abs/2308.00507</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hexin Dong, Jiawen Yao, Yuxing Tang, Mingze Yuan, Yingda Xia, Jian Zhou, Hong Lu, Jingren Zhou, Bin Dong, Le Lu, Li Zhang, Zaiyi Liu, Yu Shi, Ling Zhang<br>for:This paper aims to develop a novel learnable neural distance method to predict the prognosis of patients with pancreatic ductal adenocarcinoma (PDAC) based on the precise relationship between the tumor and nearby important vessels in CT images.methods:The proposed method uses a combination of CNN and transformer modules to extract dynamic tumor-related texture features from multi-phase contrast-enhanced CT images, and a learnable neural distance to describe the precise relationship between the tumor and vessels.results:The proposed method was extensively evaluated and compared with existing methods in a multi-center dataset with 1,070 patients with PDAC, and statistical analysis confirmed its clinical effectiveness in the external test set consisting of three centers. The developed risk marker was the strongest predictor of overall survival among preoperative factors and has the potential to be combined with established clinical factors to select patients at higher risk who might benefit from neoadjuvant therapy.Here is the same information in Simplified Chinese text:for:这篇论文目标是基于CT图像中肿瘤和近处重要血管之间的准确关系来预测患有肝脏ductal adenocarcinoma（PDAC）患者的预后。methods:该方法使用了组合CNN和transformer模块来提取多phasic contrast-enhanced CT图像中的动态肿瘤相关文本特征，并使用学习的神经距离来描述肿瘤和血管之间的准确关系。results:该方法在多中心数据集（n&#x3D;4）中进行了广泛的评估和比较，并在多中心测试集（n&#x3D;3）中进行了统计分析，证实了其在外部测试集中的临床效iveness。开发的风险标记是外科前因素中最强的预后预测器，并且有可能与已知的临床因素相结合，以选择可能需要neoadjuvant therapy的患者。<details>
<summary>Abstract</summary>
Pancreatic ductal adenocarcinoma (PDAC) is a highly lethal cancer in which the tumor-vascular involvement greatly affects the resectability and, thus, overall survival of patients. However, current prognostic prediction methods fail to explicitly and accurately investigate relationships between the tumor and nearby important vessels. This paper proposes a novel learnable neural distance that describes the precise relationship between the tumor and vessels in CT images of different patients, adopting it as a major feature for prognosis prediction. Besides, different from existing models that used CNNs or LSTMs to exploit tumor enhancement patterns on dynamic contrast-enhanced CT imaging, we improved the extraction of dynamic tumor-related texture features in multi-phase contrast-enhanced CT by fusing local and global features using CNN and transformer modules, further enhancing the features extracted across multi-phase CT images. We extensively evaluated and compared the proposed method with existing methods in the multi-center (n=4) dataset with 1,070 patients with PDAC, and statistical analysis confirmed its clinical effectiveness in the external test set consisting of three centers. The developed risk marker was the strongest predictor of overall survival among preoperative factors and it has the potential to be combined with established clinical factors to select patients at higher risk who might benefit from neoadjuvant therapy.
</details>
<details>
<summary>摘要</summary>
肺脏ductal adenocarcinoma（PDAC）是一种高度致命的Cancer，肿瘤与附近重要的血管交叉关系对患者的手术可能性和全身生存率产生很大影响。然而，现有的预测方法未能准确地和Explicitlyinvestigate肿瘤和附近重要血管之间的关系。这篇文章提出了一种新的学习型尺度，用于描述不同患者的CT图像中肿瘤和血管之间的准确关系，并采用其为预测诊断的重要特征。此外，与现有模型使用CNN或LSTM提取肿瘤增强 patrerns on dynamic contrast-enhanced CT imaging不同，我们在多相contrast-enhanced CT图像中提取了更多的动态肿瘤相关文本特征，并使用CNN和transformer模块进行融合，进一步提高了在多相CT图像中提取的特征。我们对在多中心（n=4）的数据集中的1,070名PDAC患者进行了广泛的评估和比较，并通过统计分析确认了我们的方法在外测集中的临床效果。我们开发的风险标记是PDAC患者的全身生存率最强的预测因素之一，并且它有可能与已知的临床因素相结合，以选择更高风险的患者，以便通过neoadjuvant therapy进行治疗。
</details></li>
</ul>
<hr>
<h2 id="Explainable-Graph-Spectral-Clustering-of-Text-Documents"><a href="#Explainable-Graph-Spectral-Clustering-of-Text-Documents" class="headerlink" title="Explainable Graph Spectral Clustering of Text Documents"></a>Explainable Graph Spectral Clustering of Text Documents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00504">http://arxiv.org/abs/2308.00504</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bartłomiej Starosta, Mieczysław A. Kłopotek, Sławomir T. Wierzchoń</li>
<li>For: This paper aims to provide a method for explaining the results of combinatorial Laplacian-based graph spectral clustering, which is a technique used for clustering text documents.* Methods: The paper proposes a new method called $K$-embedding, which is based on showing the approximate equivalence of the combinatorial Laplacian embedding, the $K$-embedding, and the term vector space embedding. The paper provides theoretical background for this approach and demonstrates the effectiveness of $K$-embedding through experimental studies.* Results: The paper shows that $K$-embedding approximates the Laplacian embedding well under certain conditions, and provides a bridge between the textual contents and the clustering results, allowing for more interpretable and explainable results.<details>
<summary>Abstract</summary>
Spectral clustering methods are known for their ability to represent clusters of diverse shapes, densities etc. However, results of such algorithms, when applied e.g. to text documents, are hard to explain to the user, especially due to embedding in the spectral space which has no obvious relation to document contents. Therefore there is an urgent need to elaborate methods for explaining the outcome of the clustering. This paper presents a contribution towards this goal. We present a proposal of explanation of results of combinatorial Laplacian based graph spectral clustering. It is based on showing (approximate) equivalence of combinatorial Laplacian embedding, $K$-embedding (proposed in this paper) and term vector space embedding. Hence a bridge is constructed between the textual contents and the clustering results. We provide theoretical background for this approach. We performed experimental study showing that $K$-embedding approximates well Laplacian embedding under favourable block matrix conditions and show that approximation is good enough under other conditions.
</details>
<details>
<summary>摘要</summary>
spectral clustering 方法知名于其能够表示多iform shapes、density等多样性群集。然而，当应用于文档时，这些算法的结果很难对用户进行解释，尤其是因为它们在spectral space中嵌入，这个空间与文档内容没有直接关系。因此，有一项办法需要强调的是解释结果的方法。本文提出了一种解释 combinatorial Laplacian 基于图 spectral clustering 的结果的方法。这种方法基于表明（approximate）combinatorial Laplacian embedding、K-embedding（本文所提出的）和term vector space embedding之间的等价性。因此，构建了文本内容和归类结果之间的桥梁。我们提供了理论背景，并进行了实验研究，证明了 K-embedding 可以准确地表示 Laplacian embedding ，并且在某些条件下，这种准确性足够高。
</details></li>
</ul>
<hr>
<h2 id="DINO-CXR-A-self-supervised-method-based-on-vision-transformer-for-chest-X-ray-classification"><a href="#DINO-CXR-A-self-supervised-method-based-on-vision-transformer-for-chest-X-ray-classification" class="headerlink" title="DINO-CXR: A self supervised method based on vision transformer for chest X-ray classification"></a>DINO-CXR: A self supervised method based on vision transformer for chest X-ray classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00475">http://arxiv.org/abs/2308.00475</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammadreza Shakouri, Fatemeh Iranmanesh, Mahdi Eftekhari</li>
<li>for: 这个研究是为了解决骨胶X射线数据的有限性问题，对医疗影像分析进行自动化。</li>
<li>methods: 本研究使用了一种自我超级vised学习方法，即DINO-CXR，它是基于视觉 трансформа器的骨胶X射线分类方法。</li>
<li>results: 比较分析表明，提案的方法在肺炎和COVID-19检测中具有更高的精度和相近的AUC和F-1分数，并且需要更少的标注数据。<details>
<summary>Abstract</summary>
The limited availability of labeled chest X-ray datasets is a significant bottleneck in the development of medical imaging methods. Self-supervised learning (SSL) can mitigate this problem by training models on unlabeled data. Furthermore, self-supervised pretraining has yielded promising results in visual recognition of natural images but has not been given much consideration in medical image analysis. In this work, we propose a self-supervised method, DINO-CXR, which is a novel adaptation of a self-supervised method, DINO, based on a vision transformer for chest X-ray classification. A comparative analysis is performed to show the effectiveness of the proposed method for both pneumonia and COVID-19 detection. Through a quantitative analysis, it is also shown that the proposed method outperforms state-of-the-art methods in terms of accuracy and achieves comparable results in terms of AUC and F-1 score while requiring significantly less labeled data.
</details>
<details>
<summary>摘要</summary>
限量的胸部X射线数据的可用性是医疗影像方法的开发中的一个重要瓶颈。自我指导学习（SSL）可以解决这个问题，通过训练无标注数据上的模型。然而，在医疗影像分析中，自我指导预训练并没有受到太多的注意。在这种工作中，我们提议一种自我指导方法，称为DINO-CXR，这是基于视力变换器的胸部X射线分类的一种新的适应。通过比较分析，我们证明了提议的方法在肺炎和COVID-19检测中的效果。通过量化分析，我们还证明了提议的方法在准确率和AUC和F-1分数方面具有比州顶峰方法更高的性能，而且需要远 fewer的标注数据。
</details></li>
</ul>
<hr>
<h2 id="Is-Last-Layer-Re-Training-Truly-Sufficient-for-Robustness-to-Spurious-Correlations"><a href="#Is-Last-Layer-Re-Training-Truly-Sufficient-for-Robustness-to-Spurious-Correlations" class="headerlink" title="Is Last Layer Re-Training Truly Sufficient for Robustness to Spurious Correlations?"></a>Is Last Layer Re-Training Truly Sufficient for Robustness to Spurious Correlations?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00473">http://arxiv.org/abs/2308.00473</a></li>
<li>repo_url: None</li>
<li>paper_authors: Phuong Quynh Le, Jörg Schlötterer, Christin Seifert</li>
<li>for: 该论文旨在探讨empirical risk minimization（ERM）模型在医疗领域中的应用，以及其如何处理假 correlate feature。</li>
<li>methods: 该论文使用Deep Feature Reweighting（DFR）方法来改进ERM模型的准确性，该方法只需重新训练最后一层的分类模型，使其更加适应各个群体的特点。</li>
<li>results: 研究发现，DFR方法可以改善ERM模型在医疗领域中的准确性，特别是在某些群体中的准确性。然而，DFR方法仍然可能受到假 correlate feature的影响。<details>
<summary>Abstract</summary>
Models trained with empirical risk minimization (ERM) are known to learn to rely on spurious features, i.e., their prediction is based on undesired auxiliary features which are strongly correlated with class labels but lack causal reasoning. This behavior particularly degrades accuracy in groups of samples of the correlated class that are missing the spurious feature or samples of the opposite class but with the spurious feature present. The recently proposed Deep Feature Reweighting (DFR) method improves accuracy of these worst groups. Based on the main argument that ERM mods can learn core features sufficiently well, DFR only needs to retrain the last layer of the classification model with a small group-balanced data set. In this work, we examine the applicability of DFR to realistic data in the medical domain. Furthermore, we investigate the reasoning behind the effectiveness of last-layer retraining and show that even though DFR has the potential to improve the accuracy of the worst group, it remains susceptible to spurious correlations.
</details>
<details>
<summary>摘要</summary>
模型使用隐式风险最小化（ERM）训练，有可能学习到幻觉特征，即类别标签强相关但无 causal 理解的auxiliary feature。这种行为尤其是在类别标签相关的特征 missing 或者 opposing class 存在 spurious feature 时，精度会受到负面影响。 reciently proposed Deep Feature Reweighting（DFR）方法可以提高这些最差群体的准确率。基于主要的Argument that ERM 模型可以学习核心特征，DFR 只需要在类别模型的最后一层重新训练一小组均衡数据集。在这个工作中，我们对实际数据集进行了检验，并investigated  послед layer 重新训练的理由，并证明了DFR 具有改善最差群体精度的潜在能力，但仍然可能受到幻觉相关性的影响。
</details></li>
</ul>
<hr>
<h2 id="Mirror-Natural-Evolution-Strategies"><a href="#Mirror-Natural-Evolution-Strategies" class="headerlink" title="Mirror Natural Evolution Strategies"></a>Mirror Natural Evolution Strategies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00469">http://arxiv.org/abs/2308.00469</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haishan Ye</li>
<li>for: 这个论文主要研究的是如何使用零次梯度优化法在机器学习应用中进行优化。</li>
<li>methods: 这个论文提出了一种新的重parameterized目标函数((\mu, \Sigma)),以及一种基于这个目标函数的新算法(\texttt{MiNES})。</li>
<li>results: 这个论文证明了(\texttt{MiNES})算法可以快速 converges to the minimum of the original objective function, 并且 convergence rate是(\widetilde{\mathcal{O}}(1&#x2F;k))。此外，论文还提供了Explicit convergence rate和如何covariance matrix提高 convergence rate。<details>
<summary>Abstract</summary>
The zeroth-order optimization has been widely used in machine learning applications. However, the theoretical study of the zeroth-order optimization focus on the algorithms which approximate (first-order) gradients using (zeroth-order) function value difference at a random direction. The theory of algorithms which approximate the gradient and Hessian information by zeroth-order queries is much less studied. In this paper, we focus on the theory of zeroth-order optimization which utilizes both the first-order and second-order information approximated by the zeroth-order queries. We first propose a novel reparameterized objective function with parameters $(\mu, \Sigma)$. This reparameterized objective function achieves its optimum at the minimizer and the Hessian inverse of the original objective function respectively, but with small perturbations. Accordingly, we propose a new algorithm to minimize our proposed reparameterized objective, which we call \texttt{MiNES} (mirror descent natural evolution strategy). We show that the estimated covariance matrix of \texttt{MiNES} converges to the inverse of Hessian matrix of the objective function with a convergence rate $\widetilde{\mathcal{O}}(1/k)$, where $k$ is the iteration number and $\widetilde{\mathcal{O}}(\cdot)$ hides the constant and $\log$ terms. We also provide the explicit convergence rate of \texttt{MiNES} and how the covariance matrix promotes the convergence rate.
</details>
<details>
<summary>摘要</summary>
“零次优化已经广泛应用于机器学习领域。然而，关于零次优化的理论研究主要集中在使用随机方向的函数值差来近似（first-order）梯度的算法。针对这一点，本文强调了零次优化中使用梯度和二阶函数信息的近似算法的理论研究。我们首先提出了一个新的受参数化的目标函数($\mu$, $\Sigma)$。这个受参数化的目标函数在原始目标函数的最小值和偏微分 matrix的 inverse 之间具有小的偏差。根据这个受参数化的目标函数，我们提出了一个新的算法来推算最优解，即\texttt{MiNES}（镜像演化策略）。我们证明了\texttt{MiNES} 算法中估计的 covariance matrix 的渐近级别为原始目标函数的偏微分 matrix 的 inverse，并且其渐近级别为 $\widetilde{\mathcal{O}}(1/k)$，其中 $k$ 是迭代次数，$\widetilde{\mathcal{O}}(\cdot)$ 隐藏了常数和对数项。我们还提供了\texttt{MiNES} 算法的确定性渐近级别和如何使 covariance matrix 提高渐近级别。”Note: "零次优化" in Chinese is usually translated as "zero-order optimization", but since the text is using the term "zeroth-order" to refer to the gradient and Hessian information approximated by zeroth-order queries, I have kept the term "zeroth-order" in the translation to maintain consistency with the original text.
</details></li>
</ul>
<hr>
<h2 id="Divergence-of-the-ADAM-algorithm-with-fixed-stepsize-a-very-simple-example"><a href="#Divergence-of-the-ADAM-algorithm-with-fixed-stepsize-a-very-simple-example" class="headerlink" title="Divergence of the ADAM algorithm with fixed-stepsize: a (very) simple example"></a>Divergence of the ADAM algorithm with fixed-stepsize: a (very) simple example</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00720">http://arxiv.org/abs/2308.00720</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ph. L. Toint</li>
<li>for: 这篇论文的目的是证明了一种简单的单变量函数，使得ADAM算法在缺乏误差情况下不能正确地最小化该函数。</li>
<li>methods: 该论文使用了ADAM算法，并研究了其在不同参数设置下的性能。</li>
<li>results: 研究发现，无论选择什么参数，ADAM算法在缺乏误差情况下都会导致拟合失败。<details>
<summary>Abstract</summary>
A very simple unidimensional function with Lipschitz continuous gradient is constructed such that the ADAM algorithm with constant stepsize, started from the origin, diverges when applied to minimize this function in the absence of noise on the gradient. Divergence occurs irrespective of the choice of the method parameters.
</details>
<details>
<summary>摘要</summary>
一个非常简单的一维函数，其梯度 lipschitz 连续，可以构造出来，使得 ADAM 算法，带有常数步长，从原点开始，在缺乏梯度噪声的情况下，会导致散射。这种散射不виси于方法参数的选择。Here's a word-for-word translation of the text:一个非常简单的一维函数，其梯度 lipschitz 连续，可以构造出来，使得 ADAM 算法，带有常数步长，从原点开始，在缺乏梯度噪声的情况下，会导致散射。这种散射不виси于方法参数的选择。Note that "ADAM algorithm" in the original text is translated as "ADAM 算法" in Simplified Chinese, which is the standard way to refer to the algorithm in Chinese.
</details></li>
</ul>
<hr>
<h2 id="A-Majority-Invariant-Approach-to-Patch-Robustness-Certification-for-Deep-Learning-Models"><a href="#A-Majority-Invariant-Approach-to-Patch-Robustness-Certification-for-Deep-Learning-Models" class="headerlink" title="A Majority Invariant Approach to Patch Robustness Certification for Deep Learning Models"></a>A Majority Invariant Approach to Patch Robustness Certification for Deep Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00452">http://arxiv.org/abs/2308.00452</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kio-cs/majorcert">https://github.com/kio-cs/majorcert</a></li>
<li>paper_authors: Qilin Zhou, Zhengyuan Wei, Haipeng Wang, W. K. Chan</li>
<li>for:  Ensures the robustness of deep learning models against adversarial attacks by certifying that no patch within a given bound can manipulate the model to predict a different label.</li>
<li>methods:  Proposes a new technique called MajorCert, which finds all possible label sets manipulatable by the same patch region on the same sample across the underlying classifiers, and then checks whether the majority invariant of all these combinations is intact to certify samples.</li>
<li>results:  Certifies samples that cannot meet the strict bars at the classifier or patch region levels, and ensures the robustness of deep learning models against adversarial attacks.<details>
<summary>Abstract</summary>
Patch robustness certification ensures no patch within a given bound on a sample can manipulate a deep learning model to predict a different label. However, existing techniques cannot certify samples that cannot meet their strict bars at the classifier or patch region levels. This paper proposes MajorCert. MajorCert firstly finds all possible label sets manipulatable by the same patch region on the same sample across the underlying classifiers, then enumerates their combinations element-wise, and finally checks whether the majority invariant of all these combinations is intact to certify samples.
</details>
<details>
<summary>摘要</summary>
patch robustness 证明确保不会在给定范围内的样本上进行修补，以至使深度学习模型预测不同的标签。然而，现有的技术无法证明样本不能满足其严格的标准在分类器或补丁区域 уров别。这篇论文提出了 MajorCert。 MajorCert 首先找到同一个补丁区域中所有可能的标签集，然后对它们进行元素级枚举，最后检查这些组合的主要不变性是否完整，以证明样本。
</details></li>
</ul>
<hr>
<h2 id="MAiVAR-T-Multimodal-Audio-image-and-Video-Action-Recognizer-using-Transformers"><a href="#MAiVAR-T-Multimodal-Audio-image-and-Video-Action-Recognizer-using-Transformers" class="headerlink" title="MAiVAR-T: Multimodal Audio-image and Video Action Recognizer using Transformers"></a>MAiVAR-T: Multimodal Audio-image and Video Action Recognizer using Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03741">http://arxiv.org/abs/2308.03741</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Bilal Shaikh, Douglas Chai, Syed Mohammed Shamsul Islam, Naveed Akhtar</li>
<li>for: 提高多模态人体动作识别（MHAR）的效果</li>
<li>methods: 将音频模式转化到图像域，并与视频模式进行混合，以便利用多模态 Contextual richness</li>
<li>results: 对比现有State-of-the-art方法，MAiVAR-T表现出优异的性能，证明了多模态 integrate的优势<details>
<summary>Abstract</summary>
In line with the human capacity to perceive the world by simultaneously processing and integrating high-dimensional inputs from multiple modalities like vision and audio, we propose a novel model, MAiVAR-T (Multimodal Audio-Image to Video Action Recognition Transformer). This model employs an intuitive approach for the combination of audio-image and video modalities, with a primary aim to escalate the effectiveness of multimodal human action recognition (MHAR). At the core of MAiVAR-T lies the significance of distilling substantial representations from the audio modality and transmuting these into the image domain. Subsequently, this audio-image depiction is fused with the video modality to formulate a unified representation. This concerted approach strives to exploit the contextual richness inherent in both audio and video modalities, thereby promoting action recognition. In contrast to existing state-of-the-art strategies that focus solely on audio or video modalities, MAiVAR-T demonstrates superior performance. Our extensive empirical evaluations conducted on a benchmark action recognition dataset corroborate the model's remarkable performance. This underscores the potential enhancements derived from integrating audio and video modalities for action recognition purposes.
</details>
<details>
<summary>摘要</summary>
基于人类能同时处理和 интеGRATE多Modalities的高维输入，我们提出一种新的模型，MAiVAR-T（多Modal audio-Image to Video Action Recognition Transformer）。这种模型采用直观的多Modalities结合方法，主要目标是提高多modal human action recognition（MHAR）的效果。MAiVAR-T的核心在于将音频模式中的重要表示Transformed into Image domain，然后与视频模式结合以形成一个统一的表示。这种结合方法利用了音频和视频模式中的上下文富有性，从而提高动作识别。与现有的State-of-the-art策略相比，MAiVAR-T表现出色。我们在一个action recognition benchmark dataset上进行了广泛的实验测试，结果证明了模型的杰出表现。这表明了将音频和视频模式结合起来进行动作识别可以提高性能。
</details></li>
</ul>
<hr>
<h2 id="Fair-Models-in-Credit-Intersectional-Discrimination-and-the-Amplification-of-Inequity"><a href="#Fair-Models-in-Credit-Intersectional-Discrimination-and-the-Amplification-of-Inequity" class="headerlink" title="Fair Models in Credit: Intersectional Discrimination and the Amplification of Inequity"></a>Fair Models in Credit: Intersectional Discrimination and the Amplification of Inequity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02680">http://arxiv.org/abs/2308.02680</a></li>
<li>repo_url: None</li>
<li>paper_authors: Savina Kim, Stefan Lessmann, Galina Andreeva, Michael Rovatsos</li>
<li>for: 这篇论文探讨了在借款评估中存在的算法偏见问题，以及这种偏见如何影响不同社会群体的借款获取。</li>
<li>methods: 该论文使用了西班牙微贷市场的数据，通过对自动决策系统的使用来研究 intersecting 社会分类的影响。</li>
<li>results: 研究发现，使用自动决策系统可能会导致不同社会群体之间的借款分配不均，特别是对女性、年龄、婚姻状况、单身Parent status 和孩子的数量进行分析。这些结果表明，在使用自动决策系统时，需要考虑多个社会分类的交叠影响，以确保借款获取的公正性。<details>
<summary>Abstract</summary>
The increasing usage of new data sources and machine learning (ML) technology in credit modeling raises concerns with regards to potentially unfair decision-making that rely on protected characteristics (e.g., race, sex, age) or other socio-economic and demographic data. The authors demonstrate the impact of such algorithmic bias in the microfinance context. Difficulties in assessing credit are disproportionately experienced among vulnerable groups, however, very little is known about inequities in credit allocation between groups defined, not only by single, but by multiple and intersecting social categories. Drawing from the intersectionality paradigm, the study examines intersectional horizontal inequities in credit access by gender, age, marital status, single parent status and number of children. This paper utilizes data from the Spanish microfinance market as its context to demonstrate how pluralistic realities and intersectional identities can shape patterns of credit allocation when using automated decision-making systems. With ML technology being oblivious to societal good or bad, we find that a more thorough examination of intersectionality can enhance the algorithmic fairness lens to more authentically empower action for equitable outcomes and present a fairer path forward. We demonstrate that while on a high-level, fairness may exist superficially, unfairness can exacerbate at lower levels given combinatorial effects; in other words, the core fairness problem may be more complicated than current literature demonstrates. We find that in addition to legally protected characteristics, sensitive attributes such as single parent status and number of children can result in imbalanced harm. We discuss the implications of these findings for the financial services industry.
</details>
<details>
<summary>摘要</summary>
随着新数据源和机器学习技术在信用评估中的应用，有关可能存在基于保护特征（如种族、性别、年龄）或其他社会经济和民生特征的不公正决策的担忧。作者们在微贷上下文中示出了算法偏见的影响。在评估信用方面，投险群体经历了不公正的困难，但现实中几乎没有关于不同社会分类群体之间的偏见的研究。本研究采用了 intersecting 理论，检查了信用访问中的水平偏见，包括性别、年龄、婚姻状况、单身状况和孩子的数量。这项研究使用了西班牙微贷市场作为背景，以示汇报自动化决策系统在多个社会分类群体之间的偏见。由于机器学习技术不会考虑社会好坏，我们发现，通过更加全面的考虑 intersecting 特征，可以增强算法公平的镜像，以更 authentically 激发行动，并提供更公平的前进。我们发现，尽管在高层面上，公平可能存在混乱，但在层次分析下，不公正可能会加剧，即核心公平问题可能更复杂。我们发现，除了法律保护的特征之外，敏感特征如单身状况和孩子的数量也可能导致不公正的危害。我们讨论了这些发现的影响于金融服务业。
</details></li>
</ul>
<hr>
<h2 id="SelfCheck-Using-LLMs-to-Zero-Shot-Check-Their-Own-Step-by-Step-Reasoning"><a href="#SelfCheck-Using-LLMs-to-Zero-Shot-Check-Their-Own-Step-by-Step-Reasoning" class="headerlink" title="SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning"></a>SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00436">http://arxiv.org/abs/2308.00436</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ningmiao/selfcheck">https://github.com/ningmiao/selfcheck</a></li>
<li>paper_authors: Ning Miao, Yee Whye Teh, Tom Rainforth</li>
<li>for: 这个论文的目的是检测语言模型是否能够识别自己的错误，而不需要外部资源。</li>
<li>methods: 作者提出了一种零shot验证方法，用于识别语言模型中的错误。他们还使用了这种验证方法来改善问答性能，通过对不同生成的答案进行权重投票。</li>
<li>results: 作者在三个数学 dataset（GSM8K、MathQA和MATH）上进行测试，发现这种验证方法能够成功识别错误，并在最终预测性能中提高了表现。<details>
<summary>Abstract</summary>
The recent progress in large language models (LLMs), especially the invention of chain-of-thoughts (CoT) prompting, makes it possible to solve reasoning problems. However, even the strongest LLMs are still struggling with more complicated problems that require non-linear thinking and multi-step reasoning. In this work, we explore whether LLMs have the ability to recognize their own errors, without resorting to external resources. In particular, we investigate whether they can be used to identify individual errors within a step-by-step reasoning. To this end, we propose a zero-shot verification scheme to recognize such errors. We then use this verification scheme to improve question-answering performance, by using it to perform weighted voting on different generated answers. We test the method on three math datasets-GSM8K, MathQA, and MATH-and find that it successfully recognizes errors and, in turn, increases final predictive performance.
</details>
<details>
<summary>摘要</summary>
近些时间，大型语言模型（LLM）的进步，尤其是幂链思维（CoT）的发明，使得解释问题变得可能。然而，即使最强大的LLM也在更复杂的问题上遇到困难，需要非线性思维和多步逻辑。在这种情况下，我们询问LLM是否有能力自动识别错误，不需要外部资源。具体来说，我们研究LLM是否可以在步骤逻辑中识别个错。为此，我们提出了零shot验证方案，用于识别这些错误。然后，我们使用这种验证方案来提高问答表现，通过对不同生成的答案进行权重投票。我们在三个数学数据集（GSM8K、MathQA、MATH）上测试了这种方法，发现它成功地识别错误，并在最终预测性能中提高表现。
</details></li>
</ul>
<hr>
<h2 id="qgym-A-Gym-for-Training-and-Benchmarking-RL-Based-Quantum-Compilation"><a href="#qgym-A-Gym-for-Training-and-Benchmarking-RL-Based-Quantum-Compilation" class="headerlink" title="qgym: A Gym for Training and Benchmarking RL-Based Quantum Compilation"></a>qgym: A Gym for Training and Benchmarking RL-Based Quantum Compilation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02536">http://arxiv.org/abs/2308.02536</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qutech-delft/qgym">https://github.com/qutech-delft/qgym</a></li>
<li>paper_authors: Stan van der Linde, Willem de Kok, Tariq Bontekoe, Sebastian Feld</li>
<li>for: 本研究旨在使用人工智能技术优化量子circuit编译过程，以最大化现有量子计算机的限制的利用。</li>
<li>methods: 本研究使用了人工智能学习技术，具体是强化学习（RL），通过在量子编译环境中交互学习，来寻找最佳编译策略。</li>
<li>results: 本研究提出了一个基于OpenAI gym的软件框架qgym，可以用于具体实现RL agents和算法的训练和性能评估，并提供了特定量子编译环境的高度可定制化。<details>
<summary>Abstract</summary>
Compiling a quantum circuit for specific quantum hardware is a challenging task. Moreover, current quantum computers have severe hardware limitations. To make the most use of the limited resources, the compilation process should be optimized. To improve currents methods, Reinforcement Learning (RL), a technique in which an agent interacts with an environment to learn complex policies to attain a specific goal, can be used. In this work, we present qgym, a software framework derived from the OpenAI gym, together with environments that are specifically tailored towards quantum compilation. The goal of qgym is to connect the research fields of Artificial Intelligence (AI) with quantum compilation by abstracting parts of the process that are irrelevant to either domain. It can be used to train and benchmark RL agents and algorithms in highly customizable environments.
</details>
<details>
<summary>摘要</summary>
compile quantum circuit for specific quantum hardware 是一项复杂的任务。另外，当前的量子计算机具有严重的硬件限制。为了最大化有限资源，编译过程应该进行优化。为了改进当前方法，我们可以使用人工智能学习（RL），它是一种在环境中与 Agent 交互，以学习复杂的策略，以达到特定目标的技术。在这项工作中，我们提出了qgym，一个基于 OpenAI gym 的软件框架，同时包含特制的环境，专门为量子编译而设计。qgym 的目标是将人工智能领域与量子编译相连，抽象不相关的部分，以便在高度可定制的环境中训练和测试 RL 代理和算法。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Generate-Training-Datasets-for-Robust-Semantic-Segmentation"><a href="#Learning-to-Generate-Training-Datasets-for-Robust-Semantic-Segmentation" class="headerlink" title="Learning to Generate Training Datasets for Robust Semantic Segmentation"></a>Learning to Generate Training Datasets for Robust Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02535">http://arxiv.org/abs/2308.02535</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marwane Hariat, Olivier Laurent, Rémi Kazmierczak, Shihao Zhang, Andrei Bursuc, Angela Yao, Gianni Franchi</li>
<li>for: 提高 semantic segmentation 技术的Robustness，尤其是在安全关键应用中。</li>
<li>methods: 利用 label-to-image 生成器和 image-to-label 分割模型的Synergy，设计并训练 Robusta  conditional generative adversarial network，生成真实和可能的异常或异常图像，用于培养可靠的分割模型。</li>
<li>results: 研究表明，提出的生成模型可以显著提高 semantic segmentation 技术的Robustness，在实际干扰和分布转移中表现出色，并且在不同的批处大小和数据分布下都能够保持高度的可靠性。<details>
<summary>Abstract</summary>
Semantic segmentation techniques have shown significant progress in recent years, but their robustness to real-world perturbations and data samples not seen during training remains a challenge, particularly in safety-critical applications. In this paper, we propose a novel approach to improve the robustness of semantic segmentation techniques by leveraging the synergy between label-to-image generators and image-to-label segmentation models. Specifically, we design and train Robusta, a novel robust conditional generative adversarial network to generate realistic and plausible perturbed or outlier images that can be used to train reliable segmentation models. We conduct in-depth studies of the proposed generative model, assess the performance and robustness of the downstream segmentation network, and demonstrate that our approach can significantly enhance the robustness of semantic segmentation techniques in the face of real-world perturbations, distribution shifts, and out-of-distribution samples. Our results suggest that this approach could be valuable in safety-critical applications, where the reliability of semantic segmentation techniques is of utmost importance and comes with a limited computational budget in inference. We will release our code shortly.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="BiERL-A-Meta-Evolutionary-Reinforcement-Learning-Framework-via-Bilevel-Optimization"><a href="#BiERL-A-Meta-Evolutionary-Reinforcement-Learning-Framework-via-Bilevel-Optimization" class="headerlink" title="BiERL: A Meta Evolutionary Reinforcement Learning Framework via Bilevel Optimization"></a>BiERL: A Meta Evolutionary Reinforcement Learning Framework via Bilevel Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01207">http://arxiv.org/abs/2308.01207</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chriswang98sz/bierl">https://github.com/chriswang98sz/bierl</a></li>
<li>paper_authors: Junyi Wang, Yuanyang Zhu, Zhi Wang, Yan Zheng, Jianye Hao, Chunlin Chen</li>
<li>for: 提高复杂RL问题的解决能力，尤其是通过高并行性提高ERL算法的性能。</li>
<li>methods: 提出了一种通用的meta-ERL框架，通过级别优化来同时更新内部RL模型的超参数和外部RL模型的超参数，从而避免需要先有域知识或费时优化过程。</li>
<li>results: 通过在MuJoCo和Box2D任务中进行了广泛的实验，证明了BiERL框架在许多基线上表现出色，可以适应多种ERL算法，并且可以提高RL问题的学习性能。<details>
<summary>Abstract</summary>
Evolutionary reinforcement learning (ERL) algorithms recently raise attention in tackling complex reinforcement learning (RL) problems due to high parallelism, while they are prone to insufficient exploration or model collapse without carefully tuning hyperparameters (aka meta-parameters). In the paper, we propose a general meta ERL framework via bilevel optimization (BiERL) to jointly update hyperparameters in parallel to training the ERL model within a single agent, which relieves the need for prior domain knowledge or costly optimization procedure before model deployment. We design an elegant meta-level architecture that embeds the inner-level's evolving experience into an informative population representation and introduce a simple and feasible evaluation of the meta-level fitness function to facilitate learning efficiency. We perform extensive experiments in MuJoCo and Box2D tasks to verify that as a general framework, BiERL outperforms various baselines and consistently improves the learning performance for a diversity of ERL algorithms.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Tackling-Hallucinations-in-Neural-Chart-Summarization"><a href="#Tackling-Hallucinations-in-Neural-Chart-Summarization" class="headerlink" title="Tackling Hallucinations in Neural Chart Summarization"></a>Tackling Hallucinations in Neural Chart Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00399">http://arxiv.org/abs/2308.00399</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/worldhellow/hallucinations-c2t">https://github.com/worldhellow/hallucinations-c2t</a></li>
<li>paper_authors: Saad Obaid ul Islam, Iza Škrjanec, Ondřej Dušek, Vera Demberg</li>
<li>for: 减少文本生成中的幻觉现象</li>
<li>methods: 使用自然语言推理（NLI）方法处理训练数据，以减少幻觉现象</li>
<li>results: 人工评估显示，使用我们的方法可以显著减少幻觉现象，同时短化输入序列中长距离的依赖关系和添加图表标题和标签可以提高总性性能。<details>
<summary>Abstract</summary>
Hallucinations in text generation occur when the system produces text that is not grounded in the input. In this work, we tackle the problem of hallucinations in neural chart summarization. Our analysis shows that the target side of chart summarization training datasets often contains additional information, leading to hallucinations. We propose a natural language inference (NLI) based method to preprocess the training data and show through human evaluation that our method significantly reduces hallucinations. We also found that shortening long-distance dependencies in the input sequence and adding chart-related information like title and legends improves the overall performance.
</details>
<details>
<summary>摘要</summary>
描述文本生成中的幻觉发生 когда系统生成的文本与输入不相关。在这项工作中，我们解决了图表概要生成中的幻觉问题。我们的分析表明目标一侧的概要生成训练数据经常包含额外信息，导致幻觉。我们提议使用自然语言推理（NLI）基于的方法来预处理训练数据，并通过人工评估显示我们的方法可以有效降低幻觉。此外，我们发现短缩长距离依赖性在输入序列中和添加图表相关信息如标题和图例可以提高总性性能。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-of-Time-Series-Anomaly-Detection-Methods-in-the-AIOps-Domain"><a href="#A-Survey-of-Time-Series-Anomaly-Detection-Methods-in-the-AIOps-Domain" class="headerlink" title="A Survey of Time Series Anomaly Detection Methods in the AIOps Domain"></a>A Survey of Time Series Anomaly Detection Methods in the AIOps Domain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00393">http://arxiv.org/abs/2308.00393</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenyu Zhong, Qiliang Fan, Jiacheng Zhang, Minghua Ma, Shenglin Zhang, Yongqian Sun, Qingwei Lin, Yuzhi Zhang, Dan Pei</li>
<li>for: 本研究旨在为研究者、服务运维人员和协调工程师提供一种智能化和优化操作工作流程的概述。</li>
<li>methods: 本文使用人工智能技术进行时间序列异常检测，并 explore了未来领域和下一代时间序列异常检测的发展趋势。</li>
<li>results: 本文提供了一个全面的时间序列异常检测在人工智能运维（AIOps）中的概述，并探讨了实际应用和未来发展的方向。<details>
<summary>Abstract</summary>
Internet-based services have seen remarkable success, generating vast amounts of monitored key performance indicators (KPIs) as univariate or multivariate time series. Monitoring and analyzing these time series are crucial for researchers, service operators, and on-call engineers to detect outliers or anomalies indicating service failures or significant events. Numerous advanced anomaly detection methods have emerged to address availability and performance issues. This review offers a comprehensive overview of time series anomaly detection in Artificial Intelligence for IT operations (AIOps), which uses AI capabilities to automate and optimize operational workflows. Additionally, it explores future directions for real-world and next-generation time-series anomaly detection based on recent advancements.
</details>
<details>
<summary>摘要</summary>
互联网基数服务在过去几年中取得了杰出的成功，生成了大量监控关键性表现指标（KPI），这些KPI可以是单一或多元时间序列。监控和分析这些时间序列是研究人员、服务运营员和内部工程师必须掌握的技能，以探测服务故障或重要事件的偏差或异常。随着AI技术的发展，许多高级偏差检测方法在AIOps中出现，以自动化和优化运营工作流程。此文为您提供了AIOps中时间序列偏差检测的全面回顾，同时也探讨了未来领域和下一代时间序列偏差检测的发展。
</details></li>
</ul>
<hr>
<h2 id="Counterfactual-Graph-Transformer-for-Traffic-Flow-Prediction"><a href="#Counterfactual-Graph-Transformer-for-Traffic-Flow-Prediction" class="headerlink" title="Counterfactual Graph Transformer for Traffic Flow Prediction"></a>Counterfactual Graph Transformer for Traffic Flow Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00391">http://arxiv.org/abs/2308.00391</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ying Yang, Kai Du, Xingyuan Dai, Jianwu Fang</li>
<li>for: 这个论文主要目标是提出一种基于图的Counterfactual Graph Transformer（CGT）模型，以提高交通流量预测的可解释性和可靠性。</li>
<li>methods: 该模型使用了多种注意机制，包括实例级别的解释器（如找到重要的子图），以便在交通流量预测中提供可解释的结果。它还使用了输入感知特征和图结构的推准面生成器，以生成空间和时间的counterfactual解释。</li>
<li>results: 在三个真实的公共数据集上进行了广泛的测试，结果表明，CGT可以生成可靠的解释，并且在交通流量预测中表现出色。<details>
<summary>Abstract</summary>
Traffic flow prediction (TFP) is a fundamental problem of the Intelligent Transportation System (ITS), as it models the latent spatial-temporal dependency of traffic flow for potential congestion prediction. Recent graph-based models with multiple kinds of attention mechanisms have achieved promising performance. However, existing methods for traffic flow prediction tend to inherit the bias pattern from the dataset and lack interpretability. To this end, we propose a Counterfactual Graph Transformer (CGT) model with an instance-level explainer (e.g., finding the important subgraphs) specifically designed for TFP. We design a perturbation mask generator over input sensor features at the time dimension and the graph structure on the graph transformer module to obtain spatial and temporal counterfactual explanations. By searching the optimal perturbation masks on the input data feature and graph structures, we can obtain the concise and dominant data or graph edge links for the subsequent TFP task. After re-training the utilized graph transformer model after counterfactual perturbation, we can obtain improved and interpretable traffic flow prediction. Extensive results on three real-world public datasets show that CGT can produce reliable explanations and is promising for traffic flow prediction.
</details>
<details>
<summary>摘要</summary>
做为智能交通系统（ITS）的基本问题，流量流动预测（TFP）模型了路交通流量的隐藏空间时间相依性，以预测潜在塞车。现有的图形基本模型和多种注意力机制已经实现了有前途的性能。然而，现有的交通流量预测方法往往从数据中继承偏见和无法解释。为了解决这个问题，我们提出了Counterfactual Graph Transformer（CGT）模型，该模型还包括实例级别解释器（例如，找到重要的子图），专门设计用于TFP。我们在图形变换模组中实现了阶层损害几何和时间维度的损害几何，以获取空间和时间的对照性解释。通过搜索最佳的损害几何在输入数据特征和图形结构上，我们可以获取简洁且主要的数据或图形边路连接，并将其用于后续的TFP任务。经过重新训练已使用的图形变换模型，我们可以获得改进和可解释的交通流量预测。实验结果显示，CGT可以生成可靠的解释和具有前途的交通流量预测。
</details></li>
</ul>
<hr>
<h2 id="Improving-Generalization-of-Adversarial-Training-via-Robust-Critical-Fine-Tuning"><a href="#Improving-Generalization-of-Adversarial-Training-via-Robust-Critical-Fine-Tuning" class="headerlink" title="Improving Generalization of Adversarial Training via Robust Critical Fine-Tuning"></a>Improving Generalization of Adversarial Training via Robust Critical Fine-Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02533">http://arxiv.org/abs/2308.02533</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/robustlearn">https://github.com/microsoft/robustlearn</a></li>
<li>paper_authors: Kaijie Zhu, Jindong Wang, Xixu Hu, Xing Xie, Ge Yang</li>
<li>for: 提高模型的泛化能力和对外围干扰的Robustness。</li>
<li>methods: 提出了一种新的方法called Robustness Critical Fine-Tuning（RiFT），通过利用模型对Robustness的约束来提高泛化能力，同时保持对外围干扰的Robustness。</li>
<li>results: 实验表明，RiFT可以在CIFAR10、CIFAR100和Tiny-ImageNet等 datasets上提高模型的泛化能力和对外围干扰的Robustness，同时保持或甚至提高对外围干扰的Robustness。<details>
<summary>Abstract</summary>
Deep neural networks are susceptible to adversarial examples, posing a significant security risk in critical applications. Adversarial Training (AT) is a well-established technique to enhance adversarial robustness, but it often comes at the cost of decreased generalization ability. This paper proposes Robustness Critical Fine-Tuning (RiFT), a novel approach to enhance generalization without compromising adversarial robustness. The core idea of RiFT is to exploit the redundant capacity for robustness by fine-tuning the adversarially trained model on its non-robust-critical module. To do so, we introduce module robust criticality (MRC), a measure that evaluates the significance of a given module to model robustness under worst-case weight perturbations. Using this measure, we identify the module with the lowest MRC value as the non-robust-critical module and fine-tune its weights to obtain fine-tuned weights. Subsequently, we linearly interpolate between the adversarially trained weights and fine-tuned weights to derive the optimal fine-tuned model weights. We demonstrate the efficacy of RiFT on ResNet18, ResNet34, and WideResNet34-10 models trained on CIFAR10, CIFAR100, and Tiny-ImageNet datasets. Our experiments show that \method can significantly improve both generalization and out-of-distribution robustness by around 1.5% while maintaining or even slightly enhancing adversarial robustness. Code is available at https://github.com/microsoft/robustlearn.
</details>
<details>
<summary>摘要</summary>
深度神经网络容易受到攻击性例子的威胁，这对于重要应用程序来说是一个重要的安全风险。对此，我们提出了一种新的方法 called Robustness Critical Fine-Tuning（RiFT），可以提高模型的泛化能力无需妥协对抗性。RiFT的核心思想是利用模型对抗性的剩余容量，通过对抗训练后的模型的不稳定模块进行细化，以提高模型的泛化能力。我们引入了模块抗性稳定性（MRC）度量，用于评估模块对模型抗性的影响。我们可以通过这个度量，确定模型中最不稳定的模块，并对其进行细化，以获得细化后的模型 weights。然后，我们可以通过线性 interpolate between 抗性训练后的 weights 和细化后的 weights，以 derive 最佳的细化模型 weights。我们在 ResNet18、ResNet34 和 WideResNet34-10 模型上进行了 CIFAR10、CIFAR100 和 Tiny-ImageNet 数据集的实验，结果表明，\method 可以提高模型的泛化能力和外部抗性能力，同时保持或甚至提高对抗性能力。代码可以在 GitHub 上找到。
</details></li>
</ul>
<hr>
<h2 id="Shape-Completion-with-Prediction-of-Uncertain-Regions"><a href="#Shape-Completion-with-Prediction-of-Uncertain-Regions" class="headerlink" title="Shape Completion with Prediction of Uncertain Regions"></a>Shape Completion with Prediction of Uncertain Regions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00377">http://arxiv.org/abs/2308.00377</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dlr-rm/shape-completion">https://github.com/dlr-rm/shape-completion</a></li>
<li>paper_authors: Matthias Humt, Dominik Winkelbauer, Ulrich Hillenbrand</li>
<li>for: 这 paper 是用于Shape completion的, 即从 partial observation 中预测对象的完整geometry.</li>
<li>methods: 这 paper 提出了 two novel methods for predicting uncertain regions, one through postprocessing occupancy scores, the other through direct prediction of an uncertainty indicator.</li>
<li>results: 对于 known and novel object instances 和 synthetic 和 real data, both novel methods outperform the two baselines in shape completion and uncertain region prediction, and avoiding the predicted uncertain regions increases the quality of grasps for all tested methods.Here’s the full text in Simplified Chinese:</li>
<li>for: 这 paper 是用于Shape completion的, 即从 partial observation 中预测对象的完整geometry.</li>
<li>methods: 这 paper 提出了 two novel methods for predicting uncertain regions, one through postprocessing occupancy scores, the other through direct prediction of an uncertainty indicator.</li>
<li>results: 对于 known and novel object instances 和 synthetic 和 real data, both novel methods outperform the two baselines in shape completion and uncertain region prediction, 并且避免预测的不确定区域可以提高所有测试方法的抓取质量.<details>
<summary>Abstract</summary>
Shape completion, i.e., predicting the complete geometry of an object from a partial observation, is highly relevant for several downstream tasks, most notably robotic manipulation. When basing planning or prediction of real grasps on object shape reconstruction, an indication of severe geometric uncertainty is indispensable. In particular, there can be an irreducible uncertainty in extended regions about the presence of entire object parts when given ambiguous object views. To treat this important case, we propose two novel methods for predicting such uncertain regions as straightforward extensions of any method for predicting local spatial occupancy, one through postprocessing occupancy scores, the other through direct prediction of an uncertainty indicator. We compare these methods together with two known approaches to probabilistic shape completion. Moreover, we generate a dataset, derived from ShapeNet, of realistically rendered depth images of object views with ground-truth annotations for the uncertain regions. We train on this dataset and test each method in shape completion and prediction of uncertain regions for known and novel object instances and on synthetic and real data. While direct uncertainty prediction is by far the most accurate in the segmentation of uncertain regions, both novel methods outperform the two baselines in shape completion and uncertain region prediction, and avoiding the predicted uncertain regions increases the quality of grasps for all tested methods. Web: https://github.com/DLR-RM/shape-completion
</details>
<details>
<summary>摘要</summary>
shape completion，即从部分观察到 объек的完整几何结构，在多个下游任务中具有重要意义，主要包括机器人操作。当基于对象形状重建的 плани策或预测真正的抓取时，对象形状的不确定性是不可或缺的。特别是在杂乱的对象视图中，可能存在扩展区域中对整个对象部分的存在或不存在的不确定性。为处理这个重要的情况，我们提出了两种新的方法，一种通过处理占用度分数，另一种通过直接预测不确定性指标来预测这些不确定区域。我们与两种已知的概率形状完成方法进行比较，并在shape completion和不确定区域预测方面对已知和新的对象实例进行测试，并在 sintetic 和实际数据上进行测试。而直接预测不确定性的方法在 segmentation 不确定区域中的准确性是最高，而我们的两种新方法在 shape completion 和不确定区域预测方面都超过了两个基线方法，并且避免预测的不确定区域可以提高所有测试方法的质量。网址：https://github.com/DLR-RM/shape-completion
</details></li>
</ul>
<hr>
<h2 id="MRQ-Support-Multiple-Quantization-Schemes-through-Model-Re-Quantization"><a href="#MRQ-Support-Multiple-Quantization-Schemes-through-Model-Re-Quantization" class="headerlink" title="MRQ:Support Multiple Quantization Schemes through Model Re-Quantization"></a>MRQ:Support Multiple Quantization Schemes through Model Re-Quantization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01867">http://arxiv.org/abs/2308.01867</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manasa Manohara, Sankalp Dayal, Tariq Afzal, Rahul Bakshi, Kahkuen Fu</li>
<li>for: 这个研究旨在解决深度学习模型在边缘设备上部署时，由于复杂的模型汇数和转换而受到挑战。现有的模型汇数框架，如Tensorflow QAT、TFLite PTQ和Qualcomm AIMET，仅支持有限的汇数方案。因此，深度学习模型无法轻松地汇数 для不同的固定点硬件。</li>
<li>methods: 我们提出了一种新的模型汇数方法，called MRQ（模型重新汇数），可以将现有的汇数模型转换为不同的汇数需求（例如：对称 -&gt; 非对称、非二进制尺度 -&gt; 二进制尺度）。重新汇数比从头开始汇数更简单，因为它可以避免重训成本和提供多种汇数方案的支持。我们开发了一系列重新汇数算法，包括权重调整和四处调整，以减少重新汇数误差。我们证明了，可以将MobileNetV2 QAT模型在0.64单位以下的精度损失下重新汇数为两种不同的汇数方案（即对称和对称+二进制尺度）。</li>
<li>results: 我们的研究表明，可以使用MRQ方法将MobileNetV2 QAT模型转换为不同的汇数方案，并且在NNA上部署该模型。我们的模型在Echo Show设备上得到了成功的部署。<details>
<summary>Abstract</summary>
Despite the proliferation of diverse hardware accelerators (e.g., NPU, TPU, DPU), deploying deep learning models on edge devices with fixed-point hardware is still challenging due to complex model quantization and conversion. Existing model quantization frameworks like Tensorflow QAT [1], TFLite PTQ [2], and Qualcomm AIMET [3] supports only a limited set of quantization schemes (e.g., only asymmetric per-tensor quantization in TF1.x QAT [4]). Accordingly, deep learning models cannot be easily quantized for diverse fixed-point hardwares, mainly due to slightly different quantization requirements. In this paper, we envision a new type of model quantization approach called MRQ (model re-quantization), which takes existing quantized models and quickly transforms the models to meet different quantization requirements (e.g., asymmetric -> symmetric, non-power-of-2 scale -> power-of-2 scale). Re-quantization is much simpler than quantizing from scratch because it avoids costly re-training and provides support for multiple quantization schemes simultaneously. To minimize re-quantization error, we developed a new set of re-quantization algorithms including weight correction and rounding error folding. We have demonstrated that MobileNetV2 QAT model [7] can be quickly re-quantized into two different quantization schemes (i.e., symmetric and symmetric+power-of-2 scale) with less than 0.64 units of accuracy loss. We believe our work is the first to leverage this concept of re-quantization for model quantization and models obtained from the re-quantization process have been successfully deployed on NNA in the Echo Show devices.
</details>
<details>
<summary>摘要</summary>
尽管现有多种硬件加速器（如NPU、TPU、DPU），但是在边缘设备上部署深度学习模型仍然具有挑战，主要是因为复杂的模型量化和转换。现有的模型量化框架如TensorFlow QAT [1]、TFLite PTQ [2] 和Qualcomm AIMET [3] 只支持有限的量化方案（例如，只有TF1.x QAT 中的对称性量化）。因此，深度学习模型难以被容易量化为不同的固定点硬件，主要是因为不同的量化需求略有不同。在这篇论文中，我们提出了一种新的模型量化方法，即MRQ（模型重量化），它可以将现有的量化模型快速地转换为满足不同的量化需求（例如，对称 -> 非对称、非二进制扩展 -> 二进制扩展）。重量化比量化从零开始更加简单，因为它可以避免重新训练的成本，并同时支持多种量化方案。为了最小化重量化误差，我们开发了一组新的重量化算法，包括权重修正和四舍五入误差卷积。我们已经证明了，通过MRQ方法可以将MobileNetV2 QAT模型 [7] 快速地重量化成两种不同的量化方案（即对称和对称+二进制扩展），准确率下降不足0.64个单位。我们认为，我们的工作是首次利用此类重量化方法进行模型量化，并且在NNA上部署了使用重量化过程获得的模型。
</details></li>
</ul>
<hr>
<h2 id="Learning-Green’s-Function-Efficiently-Using-Low-Rank-Approximations"><a href="#Learning-Green’s-Function-Efficiently-Using-Low-Rank-Approximations" class="headerlink" title="Learning Green’s Function Efficiently Using Low-Rank Approximations"></a>Learning Green’s Function Efficiently Using Low-Rank Approximations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00350">http://arxiv.org/abs/2308.00350</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kishanwn/decgreennet">https://github.com/kishanwn/decgreennet</a></li>
<li>paper_authors: Kishan Wimalawarne, Taiji Suzuki, Sophie Langer</li>
<li>for: 用深度学习模型解决不同类型的偏微分方程</li>
<li>methods: 使用低级分解学习绿函数，实现去除重复的计算昂贵的蒙特卡洛 integral 估计</li>
<li>results: 提高计算时间，与 PINNs 和 MOD-Net 相当准确，但计算时间更短<details>
<summary>Abstract</summary>
Learning the Green's function using deep learning models enables to solve different classes of partial differential equations. A practical limitation of using deep learning for the Green's function is the repeated computationally expensive Monte-Carlo integral approximations. We propose to learn the Green's function by low-rank decomposition, which results in a novel architecture to remove redundant computations by separate learning with domain data for evaluation and Monte-Carlo samples for integral approximation. Using experiments we show that the proposed method improves computational time compared to MOD-Net while achieving comparable accuracy compared to both PINNs and MOD-Net.
</details>
<details>
<summary>摘要</summary>
使用深度学习模型学习格林函数可以解决不同类型的部分 diferencial equations。然而，使用深度学习来学习格林函数存在重复的计算昂贵的蒙地卡罗 integral approximation。我们提议通过低级别分解来学习格林函数，这Result in a novel architecture to remove redundant computations by separate learning with domain data for evaluation and Monte-Carlo samples for integral approximation.使用实验表明，我们的方法可以比MOD-Net快速计算，并且与PINNs和MOD-Net的准确率相似。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-ensemble-selection-based-on-Deep-Neural-Network-Uncertainty-Estimation-for-Adversarial-Robustness"><a href="#Dynamic-ensemble-selection-based-on-Deep-Neural-Network-Uncertainty-Estimation-for-Adversarial-Robustness" class="headerlink" title="Dynamic ensemble selection based on Deep Neural Network Uncertainty Estimation for Adversarial Robustness"></a>Dynamic ensemble selection based on Deep Neural Network Uncertainty Estimation for Adversarial Robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00346">http://arxiv.org/abs/2308.00346</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruoxi Qin, Linyuan Wang, Xuehui Du, Xingyuan Chen, Bin Yan<br>for: 这个论文是为了提高图像识别器的Robustness而写的。methods: 这篇论文使用了动态ensemble选择技术来提高模型的鲁棒性，这种技术在模型层次上考虑动态特征来提高模型的鲁棒性。results:  compare with之前的动态方法和静态对抗训练模型，这种方法可以很好地提高鲁棒性Result without sacrificing accuracy。<details>
<summary>Abstract</summary>
The deep neural network has attained significant efficiency in image recognition. However, it has vulnerable recognition robustness under extensive data uncertainty in practical applications. The uncertainty is attributed to the inevitable ambient noise and, more importantly, the possible adversarial attack. Dynamic methods can effectively improve the defense initiative in the arms race of attack and defense of adversarial examples. Different from the previous dynamic method depend on input or decision, this work explore the dynamic attributes in model level through dynamic ensemble selection technology to further protect the model from white-box attacks and improve the robustness. Specifically, in training phase the Dirichlet distribution is apply as prior of sub-models' predictive distribution, and the diversity constraint in parameter space is introduced under the lightweight sub-models to construct alternative ensembel model spaces. In test phase, the certain sub-models are dynamically selected based on their rank of uncertainty value for the final prediction to ensure the majority accurate principle in ensemble robustness and accuracy. Compared with the previous dynamic method and staic adversarial traning model, the presented approach can achieve significant robustness results without damaging accuracy by combining dynamics and diversity property.
</details>
<details>
<summary>摘要</summary>
深度神经网络在图像识别中已经实现了显著的效率。然而，它在实际应用中面临着广泛的数据不确定性的挑战。这种不确定性来自于不可避免的环境噪声以及可能的敌意攻击。动态方法可以有效地提高模型的防御性能，在攻击者和防御者之间的攻击战中。与之前的动态方法不同，本工作通过在模型层次上实现动态集合选择技术，以保护模型免受白盒攻击并提高其Robustness。在训练阶段， Dirichlet 分布被应用于子模型预测分布的先验 Distribution，并在轻量级子模型参数空间中引入多样性约束。在测试阶段，根据其最终预测结果的不确定性值排名，选择特定的子模型来实现最终预测。相比之前的动态方法和静态敌意训练模型，提出的方法可以在不损害准确率的情况下实现显著的Robustness。
</details></li>
</ul>
<hr>
<h2 id="Monitoring-Algorithmic-Fairness-under-Partial-Observations"><a href="#Monitoring-Algorithmic-Fairness-under-Partial-Observations" class="headerlink" title="Monitoring Algorithmic Fairness under Partial Observations"></a>Monitoring Algorithmic Fairness under Partial Observations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00341">http://arxiv.org/abs/2308.00341</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas A. Henzinger, Konstantin Kueffner, Kaushik Mallik</li>
<li>for: 本研究旨在为已经部署的机器学习算法提供runtime fairness监测，以确保它们在做出决策时保持公平和不偏袋。</li>
<li>methods: 本研究使用了partially observed Markov chains (POMC)模型，并使用了 arithmetic expressions over the expected values of numerical functions on event sequences来Specify fairness properties。</li>
<li>results: 研究人员通过观察系统的长期运行，并使用了一种名为PAC-estimates的计算机轻量级监测方法，可以在系统的不同执行情况下提供有关系统公平性的估计。这些监测方法可以帮助确保机器学习算法在做出决策时保持公平和不偏袋。<details>
<summary>Abstract</summary>
As AI and machine-learned software are used increasingly for making decisions that affect humans, it is imperative that they remain fair and unbiased in their decisions. To complement design-time bias mitigation measures, runtime verification techniques have been introduced recently to monitor the algorithmic fairness of deployed systems. Previous monitoring techniques assume full observability of the states of the (unknown) monitored system. Moreover, they can monitor only fairness properties that are specified as arithmetic expressions over the probabilities of different events. In this work, we extend fairness monitoring to systems modeled as partially observed Markov chains (POMC), and to specifications containing arithmetic expressions over the expected values of numerical functions on event sequences. The only assumptions we make are that the underlying POMC is aperiodic and starts in the stationary distribution, with a bound on its mixing time being known. These assumptions enable us to estimate a given property for the entire distribution of possible executions of the monitored POMC, by observing only a single execution. Our monitors observe a long run of the system and, after each new observation, output updated PAC-estimates of how fair or biased the system is. The monitors are computationally lightweight and, using a prototype implementation, we demonstrate their effectiveness on several real-world examples.
</details>
<details>
<summary>摘要</summary>
“作为人工智能和机器学习软件在做出影响人类决策的情况下，它们必须保持公平和无偏见。为了补充设计时的偏见缓和措施，runtime监控技术在最近才被引入，以监控部署系统的算法公平性。先前的监控技术假设了监控系统的完整可观察性，并且仅能监控公平性属性是指的数学表达式中的概率分布。在这个工作中，我们将公平监控扩展到模型为部分可观察Markov链（POMC）的系统，并将属性指定为数学表达式中的预期值。我们假设了背景POMC是无限循环的，并且知道其混合时间的上限。这些假设允许我们预估一个属性的整个分布，通过观察单一的执行。我们的监控器在观察系统的长期执行后，每次新的观察结果后，都会产生新的PAC估计，以衡量系统的公平性。我们的监控器 Computationally lightweight，我们使用一个原型实现，并在实际应用中证明了它们的有效性。”
</details></li>
</ul>
<hr>
<h2 id="Threshold-aware-Learning-to-Generate-Feasible-Solutions-for-Mixed-Integer-Programs"><a href="#Threshold-aware-Learning-to-Generate-Feasible-Solutions-for-Mixed-Integer-Programs" class="headerlink" title="Threshold-aware Learning to Generate Feasible Solutions for Mixed Integer Programs"></a>Threshold-aware Learning to Generate Feasible Solutions for Mixed Integer Programs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00327">http://arxiv.org/abs/2308.00327</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taehyun Yoon, Jinwon Choi, Hyokun Yun, Sungbin Lim</li>
<li>for:  solving combinatorial optimization (CO) problems with a high-quality and feasible solution in a limited time.</li>
<li>methods:  using neural diving (ND) to generate partial discrete variable assignments, and a post-hoc method to optimize the coverage.</li>
<li>results:  achieving state-of-the-art performance in NeurIPS ML4CO datasets, with an optimality gap of 0.45% and a ten-fold improvement over SCIP within the one-minute time limit.<details>
<summary>Abstract</summary>
Finding a high-quality feasible solution to a combinatorial optimization (CO) problem in a limited time is challenging due to its discrete nature. Recently, there has been an increasing number of machine learning (ML) methods for addressing CO problems. Neural diving (ND) is one of the learning-based approaches to generating partial discrete variable assignments in Mixed Integer Programs (MIP), a framework for modeling CO problems. However, a major drawback of ND is a large discrepancy between the ML and MIP objectives, i.e., variable value classification accuracy over primal bound. Our study investigates that a specific range of variable assignment rates (coverage) yields high-quality feasible solutions, where we suggest optimizing the coverage bridges the gap between the learning and MIP objectives. Consequently, we introduce a post-hoc method and a learning-based approach for optimizing the coverage. A key idea of our approach is to jointly learn to restrict the coverage search space and to predict the coverage in the learned search space. Experimental results demonstrate that learning a deep neural network to estimate the coverage for finding high-quality feasible solutions achieves state-of-the-art performance in NeurIPS ML4CO datasets. In particular, our method shows outstanding performance in the workload apportionment dataset, achieving the optimality gap of 0.45%, a ten-fold improvement over SCIP within the one-minute time limit.
</details>
<details>
<summary>摘要</summary>
寻找一个高质量可行的解决方案 для combinatorial optimization（CO）问题在有限时间内是具有挑战性的，这主要是因为CO问题的离散性质。当前， Machine Learning（ML）方法在解决CO问题上的应用越来越广泛。Neural Diving（ND）是一种学习基于方法，用于生成混合整数程序（MIP）中的部分逻辑变量分配。然而，ND的一个主要缺点是ML目标和MIP目标之间的大差异，即变量值分类准确率过 primal bound。我们的研究表明，在特定的变量分配率范围内，可以获得高质量可行的解决方案，我们建议优化该覆盖率来bridging the gap между学习和MIP目标。因此，我们提出了一种后续方法和学习基于方法来优化覆盖率。我们的方法的关键思想是同时学习Restrict the coverage search space和在学习的搜索空间中预测覆盖。实验结果表明，通过学习深度神经网络来估算覆盖以找到高质量可行的解决方案，可以在NeurIPS ML4CO数据集中达到状态之最的性能。尤其是在工作负担分配数据集中，我们的方法实现了优化率0.45%，对SCIP的一分钟时间限制进行了十倍的提升。
</details></li>
</ul>
<hr>
<h2 id="Pixel-to-policy-DQN-Encoders-for-within-cross-game-reinforcement-learning"><a href="#Pixel-to-policy-DQN-Encoders-for-within-cross-game-reinforcement-learning" class="headerlink" title="Pixel to policy: DQN Encoders for within &amp; cross-game reinforcement learning"></a>Pixel to policy: DQN Encoders for within &amp; cross-game reinforcement learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00318">http://arxiv.org/abs/2308.00318</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ashrya Agrawal, Priyanshi Shah, Sourabh Prakash</li>
<li>for: 这 paper 的目的是研究如何通过transfer learning进行强化学习，以提高RL表现和效率。</li>
<li>methods: 这 paper 使用了多种RL模型，包括从零开始学习和基于DQN的转移学习。</li>
<li>results: 这 paper 的结果表明，通过转移学习可以提高RL表现和效率，并且在不同的游戏环境中达到人类水平的表现。具体来说，这 paper 的模型在 Assault 和 Space Invader 环境中 achieved mean episode reward of 533.42 和 402.17 分别，这些结果都是非常出色的。<details>
<summary>Abstract</summary>
Reinforcement Learning can be applied to various tasks, and environments. Many of these environments have a similar shared structure, which can be exploited to improve RL performance on other tasks. Transfer learning can be used to take advantage of this shared structure, by learning policies that are transferable across different tasks and environments and can lead to more efficient learning as well as improved performance on a wide range of tasks. This work explores as well as compares the performance between RL models being trained from the scratch and on different approaches of transfer learning. Additionally, the study explores the performance of a model trained on multiple game environments, with the goal of developing a universal game-playing agent as well as transfer learning a pre-trained encoder using DQN, and training it on the same game or a different game. Our DQN model achieves a mean episode reward of 46.16 which even beats the human-level performance with merely 20k episodes which is significantly lower than deepmind's 1M episodes. The achieved mean rewards of 533.42 and 402.17 on the Assault and Space Invader environments respectively, represent noteworthy performance on these challenging environments.
</details>
<details>
<summary>摘要</summary>
� Reinforcement Learning 可以应用到不同的任务和环境中。许多这些环境都有相似的共同结构，可以利用这个共同结构来提高RL表现，例如通过将多个任务和环境的学习策略转移到其他任务和环境中，以提高学习效率和任务表现的多样性。这个研究将探讨RL模型是否从零开始学习或是否使用传递学习，以及将一个多环境训练的模型转移到不同的游戏环境中。我们的DQN模型在不同的游戏环境中获得了46.16的平均集成奖，甚至超越了人类水准的表现，仅需20k集成 Episodes，比deepmind的1M Episodes要来得要少。在Assault和Space Invader环境中，我们的模型获得了533.42和402.17的平均奖励，表示在这些挑战性的环境中表现非常出色。
</details></li>
</ul>
<hr>
<h2 id="Doubly-Robust-Instance-Reweighted-Adversarial-Training"><a href="#Doubly-Robust-Instance-Reweighted-Adversarial-Training" class="headerlink" title="Doubly Robust Instance-Reweighted Adversarial Training"></a>Doubly Robust Instance-Reweighted Adversarial Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00311">http://arxiv.org/abs/2308.00311</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daouda Sow, Sen Lin, Zhangyang Wang, Yingbin Liang</li>
<li>for: 本文目的是提出一种新的双重强化实例权重对抗训练方法，以提高模型对攻击最弱的数据点的 Robustness。</li>
<li>methods: 本文使用了分布式 robust optimization（DRO）技术来获取重要性权重，并通过优化KL偏好函数来提出新的算法，具有理论上的收敛保证。</li>
<li>results: 对标准分类 datasets 进行实验，本文的提出方法比相关基线方法在平均 robust性方面表现出色，同时对攻击最弱的数据点的 Robustness 也有显著改善。<details>
<summary>Abstract</summary>
Assigning importance weights to adversarial data has achieved great success in training adversarially robust networks under limited model capacity. However, existing instance-reweighted adversarial training (AT) methods heavily depend on heuristics and/or geometric interpretations to determine those importance weights, making these algorithms lack rigorous theoretical justification/guarantee. Moreover, recent research has shown that adversarial training suffers from a severe non-uniform robust performance across the training distribution, e.g., data points belonging to some classes can be much more vulnerable to adversarial attacks than others. To address both issues, in this paper, we propose a novel doubly-robust instance reweighted AT framework, which allows to obtain the importance weights via exploring distributionally robust optimization (DRO) techniques, and at the same time boosts the robustness on the most vulnerable examples. In particular, our importance weights are obtained by optimizing the KL-divergence regularized loss function, which allows us to devise new algorithms with a theoretical convergence guarantee. Experiments on standard classification datasets demonstrate that our proposed approach outperforms related state-of-the-art baseline methods in terms of average robust performance, and at the same time improves the robustness against attacks on the weakest data points. Codes will be available soon.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将重要性权重分配给敌对数据实现了在有限模型容量下训练敌对抗击网络的很大成功。然而，现有的实例权重对抗训练（AT）方法仍然依赖于规则和/或几何解释来确定这些重要性权重，使这些算法缺乏正式的理论基础保证。此外，现有研究表明，对抗训练受到数据分布中的非均匀抗击性的困扰，例如，某些类别的数据点可能比其他类别更容易受到敌对攻击。为解决这两个问题，在这篇论文中，我们提出了一种新的双重不确定实例权重AT框架，可以通过探索分布式不确定优化（DRO）技术来获得重要性权重，同时提高最容易受到攻击的数据点的Robustness。具体来说，我们的重要性权重通过优化KL分布regularized损失函数来获得，这allow us to设计新的算法，并提供了理论上的准确整合保证。实验表明，我们提出的方法在标准分类dataset上表现出了与相关基线方法相比的平均Robustness和对攻击最弱数据点的Robustness进行了改进。代码将很快 disponible。
</details></li>
</ul>
<hr>
<h2 id="GradOrth-A-Simple-yet-Efficient-Out-of-Distribution-Detection-with-Orthogonal-Projection-of-Gradients"><a href="#GradOrth-A-Simple-yet-Efficient-Out-of-Distribution-Detection-with-Orthogonal-Projection-of-Gradients" class="headerlink" title="GradOrth: A Simple yet Efficient Out-of-Distribution Detection with Orthogonal Projection of Gradients"></a>GradOrth: A Simple yet Efficient Out-of-Distribution Detection with Orthogonal Projection of Gradients</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00310">http://arxiv.org/abs/2308.00310</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sima Behpour, Thang Doan, Xin Li, Wenbin He, Liang Gou, Liu Ren</li>
<li>for: 本研究旨在提出一种基于特征Map的out-of-distribution（OOD）检测方法，以确保机器学习模型在实际应用中安全部署。</li>
<li>methods: 本研究使用了一种新的方法 called GradOrth，它基于在ID数据上最重要的参数的观察，发现OOD数据的关键特征在ID数据的低维度子空间中。特别是，通过计算ID数据中考虑重要的子空间上的梯度 проек 值来识别OOD数据。</li>
<li>results: 对比现有方法，GradOrth方法在FPR95下减少了OOD检测中的均 FALSE POSITIVE 率达到8%，显示出了显著的提高。<details>
<summary>Abstract</summary>
Detecting out-of-distribution (OOD) data is crucial for ensuring the safe deployment of machine learning models in real-world applications. However, existing OOD detection approaches primarily rely on the feature maps or the full gradient space information to derive OOD scores neglecting the role of most important parameters of the pre-trained network over in-distribution (ID) data. In this study, we propose a novel approach called GradOrth to facilitate OOD detection based on one intriguing observation that the important features to identify OOD data lie in the lower-rank subspace of in-distribution (ID) data. In particular, we identify OOD data by computing the norm of gradient projection on the subspaces considered important for the in-distribution data. A large orthogonal projection value (i.e. a small projection value) indicates the sample as OOD as it captures a weak correlation of the ID data. This simple yet effective method exhibits outstanding performance, showcasing a notable reduction in the average false positive rate at a 95% true positive rate (FPR95) of up to 8% when compared to the current state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
检测不同分布（OOD）数据是机器学习模型在实际应用中安全部署的关键。然而，现有的OOD检测方法主要基于特征地图或全Gradient空间信息来生成OOD分数，忽视了预训练网络中最重要的参数的作用。在这种研究中，我们提出了一种新的方法，即GradOrth，用于基于ID数据中最重要的特征来进行OOD检测。具体来说，我们通过计算ID数据中考虑重要的子空间上的梯度投影来识别OOD数据。如果梯度投影的正交值（即投影值较小）强大，则表示该样本为OOD，因为它表示ID数据的弱相关性。这种简单 yet 有效的方法在性能方面表现出色，可以在95%的真正正确率（FPR95）下减少了相对于当前状态的欺骗率达到8%。
</details></li>
</ul>
<hr>
<h2 id="Revolutionizing-TCAD-Simulations-with-Universal-Device-Encoding-and-Graph-Attention-Networks"><a href="#Revolutionizing-TCAD-Simulations-with-Universal-Device-Encoding-and-Graph-Attention-Networks" class="headerlink" title="Revolutionizing TCAD Simulations with Universal Device Encoding and Graph Attention Networks"></a>Revolutionizing TCAD Simulations with Universal Device Encoding and Graph Attention Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11624">http://arxiv.org/abs/2308.11624</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guangxi Fan, Kain Lu Low</li>
<li>for: 提出了一种基于人工智能（AI）和图表示法的半导体设备编码方法，用于TCAD设备仿真。</li>
<li>methods: 提出了一种基于图表示法的通用编码方案，考虑了材料层和设备层嵌入，还引入了一种新的空间关系嵌入， inspirited by interpolation operations typically used in finite element meshing。</li>
<li>results: 通过使用一种新的图注意力网络（RelGAT），实现了Surrogate Poisson伪 simulate和current-voltage（IV）预测，基于漫游扩散模型。<details>
<summary>Abstract</summary>
An innovative methodology that leverages artificial intelligence (AI) and graph representation for semiconductor device encoding in TCAD device simulation is proposed. A graph-based universal encoding scheme is presented that not only considers material-level and device-level embeddings, but also introduces a novel spatial relationship embedding inspired by interpolation operations typically used in finite element meshing. Universal physical laws from device simulations are leveraged for comprehensive data-driven modeling, which encompasses surrogate Poisson emulation and current-voltage (IV) prediction based on drift-diffusion model. Both are achieved using a novel graph attention network, referred to as RelGAT. Comprehensive technical details based on the device simulator Sentaurus TCAD are presented, empowering researchers to adopt the proposed AI-driven Electronic Design Automation (EDA) solution at the device level.
</details>
<details>
<summary>摘要</summary>
一种创新的方法ологиía，利用人工智能（AI）和图表示法来实现半导体设备编码在TCAD设备仿真中，被提出。这种图基本 universal encoding scheme不仅考虑材料层和设备层嵌入，还引入了一种新的空间关系嵌入， Draw inspiration from interpolation operations typically used in finite element meshing. 通过利用设备仿真中的universal physical laws，实现了全面的数据驱动模拟，包括Surrogate Poisson观测和电压-电流（IV）预测，基于漂移扩散模型。这些都是通过一种新的图注意力网络， referred to as RelGAT，实现的。提供了基于设备仿真器Sentaurus TCAD的完整技术详细信息，使研究人员可以在设备层采用该人工智能驱动电子设计自动化（EDA）解决方案。
</details></li>
</ul>
<hr>
<h2 id="Adapt-and-Decompose-Efficient-Generalization-of-Text-to-SQL-via-Domain-Adapted-Least-To-Most-Prompting"><a href="#Adapt-and-Decompose-Efficient-Generalization-of-Text-to-SQL-via-Domain-Adapted-Least-To-Most-Prompting" class="headerlink" title="Adapt and Decompose: Efficient Generalization of Text-to-SQL via Domain Adapted Least-To-Most Prompting"></a>Adapt and Decompose: Efficient Generalization of Text-to-SQL via Domain Adapted Least-To-Most Prompting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02582">http://arxiv.org/abs/2308.02582</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aseem Arora, Shabbirhussain Bhaisaheb, Harshit Nigam, Manasi Patwardhan, Lovekesh Vig, Gautam Shroff</li>
<li>for: 本研究旨在提高文本到SQL语义解析的跨频域和跨组合部署性。</li>
<li>methods: 我们提出了一种算法，通过在训练集中进行离线采样，生成一个固定的通用提示（GP），以及一种自适应的DA-GP，以更好地处理跨频域泛化。</li>
<li>results: 我们的方法在KaggleDBQA数据集上显示了superior的性能，并且在不同的LLM和数据库上 consistently improve the performance of LTMP-DA-GP over GP, highlighting the efficacy and model agnostic benefits of our prompt-based adapt and decompose approach。<details>
<summary>Abstract</summary>
Cross-domain and cross-compositional generalization of Text-to-SQL semantic parsing is a challenging task. Existing Large Language Model (LLM) based solutions rely on inference-time retrieval of few-shot exemplars from the training set to synthesize a run-time prompt for each Natural Language (NL) test query. In contrast, we devise an algorithm which performs offline sampling of a minimal set-of few-shots from the training data, with complete coverage of SQL clauses, operators and functions, and maximal domain coverage within the allowed token length. This allows for synthesis of a fixed Generic Prompt (GP), with a diverse set-of exemplars common across NL test queries, avoiding expensive test time exemplar retrieval. We further auto-adapt the GP to the target database domain (DA-GP), to better handle cross-domain generalization; followed by a decomposed Least-To-Most-Prompting (LTMP-DA-GP) to handle cross-compositional generalization. The synthesis of LTMP-DA-GP is an offline task, to be performed one-time per new database with minimal human intervention. Our approach demonstrates superior performance on the KaggleDBQA dataset, designed to evaluate generalizability for the Text-to-SQL task. We further showcase consistent performance improvement of LTMP-DA-GP over GP, across LLMs and databases of KaggleDBQA, highlighting the efficacy and model agnostic benefits of our prompt based adapt and decompose approach.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Study-of-Unsupervised-Evaluation-Metrics-for-Practical-and-Automatic-Domain-Adaptation"><a href="#A-Study-of-Unsupervised-Evaluation-Metrics-for-Practical-and-Automatic-Domain-Adaptation" class="headerlink" title="A Study of Unsupervised Evaluation Metrics for Practical and Automatic Domain Adaptation"></a>A Study of Unsupervised Evaluation Metrics for Practical and Automatic Domain Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00287">http://arxiv.org/abs/2308.00287</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minghao Chen, Zepeng Gao, Shuai Zhao, Qibo Qiu, Wenxiao Wang, Binbin Lin, Xiaofei He<br>for: This paper aims to find an evaluation metric for unsupervised domain adaptation (UDA) methods that does not require target validation labels.methods: The authors use a metric based on mutual information of the model prediction and incorporate source accuracy into the metric. They also employ a new MLP classifier that is held out during training and integrate the enhanced metric with data augmentation.results: The proposed Augmentation Consistency Metric (ACM) significantly improves the evaluation of UDA methods and outperforms previous experiment settings. The authors also demonstrate the effectiveness of their proposed metric through large-scale experiments and show that it can automatically search for the optimal hyper-parameter set, achieving superior performance compared to manually tuned sets across four common benchmarks.<details>
<summary>Abstract</summary>
Unsupervised domain adaptation (UDA) methods facilitate the transfer of models to target domains without labels. However, these methods necessitate a labeled target validation set for hyper-parameter tuning and model selection. In this paper, we aim to find an evaluation metric capable of assessing the quality of a transferred model without access to target validation labels. We begin with the metric based on mutual information of the model prediction. Through empirical analysis, we identify three prevalent issues with this metric: 1) It does not account for the source structure. 2) It can be easily attacked. 3) It fails to detect negative transfer caused by the over-alignment of source and target features. To address the first two issues, we incorporate source accuracy into the metric and employ a new MLP classifier that is held out during training, significantly improving the result. To tackle the final issue, we integrate this enhanced metric with data augmentation, resulting in a novel unsupervised UDA metric called the Augmentation Consistency Metric (ACM). Additionally, we empirically demonstrate the shortcomings of previous experiment settings and conduct large-scale experiments to validate the effectiveness of our proposed metric. Furthermore, we employ our metric to automatically search for the optimal hyper-parameter set, achieving superior performance compared to manually tuned sets across four common benchmarks. Codes will be available soon.
</details>
<details>
<summary>摘要</summary>
无监督领域适应（UDA）方法可以将模型传输到目标领域无需标签。然而，这些方法需要一个标注的目标验证集进行超参数调整和模型选择。在这篇论文中，我们想找到一个无需目标验证集的评价指标，可以评估传输模型的质量。我们从模型预测的共同信息基础出发，并通过实验分析发现了三个常见问题：1）它不考虑源结构。2）它可以被轻松攻击。3）它无法探测源和目标特征的过对齐导致的负转移。为了解决第一个问题，我们将源准确率 incorporated into the metric，并使用一个在训练中保持隐藏的新的多层感知（MLP）分类器，显著提高结果。为了解决第三个问题，我们将这种加强的指标与数据扩展相结合，得到了一个新的无监督UDA指标——扩展一致指标（ACM）。此外，我们还证明了先前的实验设置的缺陷，并进行了大规模的实验 validate our proposed metric的有效性。最后，我们使用我们的指标自动搜索最佳超参数集，在四个常见的 benchmark上达到了手动调整的超参数集的比较优秀性。代码将很快地发布。
</details></li>
</ul>
<hr>
<h2 id="Predictive-Modeling-through-Hyper-Bayesian-Optimization"><a href="#Predictive-Modeling-through-Hyper-Bayesian-Optimization" class="headerlink" title="Predictive Modeling through Hyper-Bayesian Optimization"></a>Predictive Modeling through Hyper-Bayesian Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00285">http://arxiv.org/abs/2308.00285</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manisha Senadeera, Santu Rana, Sunil Gupta, Svetha Venkatesh</li>
<li>for: 本研究旨在提高模型选择和 Bayesian 优化（BO）的效率，并输出黑obox函数的信息。</li>
<li>methods: 本文提出了一种新的模型选择和 BO  интеграции方法，通过在模型空间和函数空间之间往返，使用分数函数捕捉模型的质量，并将其反馈到函数空间中，以便更快地 convergence 到函数的最优点。</li>
<li>results: 实验结果表明，与标准 BO 相比，本方法可以大幅提高样本效率，并输出黑obox函数的信息。同时，本文也证明了该方法的收敛性。<details>
<summary>Abstract</summary>
Model selection is an integral problem of model based optimization techniques such as Bayesian optimization (BO). Current approaches often treat model selection as an estimation problem, to be periodically updated with observations coming from the optimization iterations. In this paper, we propose an alternative way to achieve both efficiently. Specifically, we propose a novel way of integrating model selection and BO for the single goal of reaching the function optima faster. The algorithm moves back and forth between BO in the model space and BO in the function space, where the goodness of the recommended model is captured by a score function and fed back, capturing how well the model helped convergence in the function space. The score function is derived in such a way that it neutralizes the effect of the moving nature of the BO in the function space, thus keeping the model selection problem stationary. This back and forth leads to quick convergence for both model selection and BO in the function space. In addition to improved sample efficiency, the framework outputs information about the black-box function. Convergence is proved, and experimental results show significant improvement compared to standard BO.
</details>
<details>
<summary>摘要</summary>
<<SYS>>功能优化技术中的模型选择问题是一个基本问题。现有的方法通常将模型选择视为一个估计问题，通过优化迭代获取观测数据来更新估计。在这篇论文中，我们提出了一种新的方法，可以更加快速地达到函数的最优点。这种方法是将模型选择和优化迭代结合在一起，通过一个得分函数来捕捉模型对函数空间的贡献，并将其反馈到函数空间中。这个得分函数是以一种方式定义的，使得它在模型移动的情况下保持静止，因此可以准确地衡量模型在函数空间中的性能。这种往返的运动会使得模型选择和优化迭代在函数空间中更快速地 converges。此外，这种方法还可以提供关于黑obox函数的信息，并且可以证明其 converges。实验结果表明，与标准BO相比，这种方法可以带来显著的改善。
</details></li>
</ul>
<hr>
<h2 id="CLAMS-A-Cluster-Ambiguity-Measure-for-Estimating-Perceptual-Variability-in-Visual-Clustering"><a href="#CLAMS-A-Cluster-Ambiguity-Measure-for-Estimating-Perceptual-Variability-in-Visual-Clustering" class="headerlink" title="CLAMS: A Cluster Ambiguity Measure for Estimating Perceptual Variability in Visual Clustering"></a>CLAMS: A Cluster Ambiguity Measure for Estimating Perceptual Variability in Visual Clustering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00284">http://arxiv.org/abs/2308.00284</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hyeon Jeon, Ghulam Jilani Quadri, Hyunwook Lee, Paul Rosen, Danielle Albers Szafir, Jinwook Seo</li>
<li>For: This paper aims to address the problem of perceptual variability in visual clustering, which can lead to unreliable data analysis.* Methods: The paper introduces a data-driven visual quality measure called CLAMS, which uses a regression module to estimate human-judged separability of clusters and predict cluster ambiguity.* Results: The paper shows that CLAMS outperforms widely-used clustering techniques in predicting ground truth cluster ambiguity, and exhibits performance on par with human annotators. Additionally, the paper presents two applications for optimizing and benchmarking data mining techniques using CLAMS.<details>
<summary>Abstract</summary>
Visual clustering is a common perceptual task in scatterplots that supports diverse analytics tasks (e.g., cluster identification). However, even with the same scatterplot, the ways of perceiving clusters (i.e., conducting visual clustering) can differ due to the differences among individuals and ambiguous cluster boundaries. Although such perceptual variability casts doubt on the reliability of data analysis based on visual clustering, we lack a systematic way to efficiently assess this variability. In this research, we study perceptual variability in conducting visual clustering, which we call Cluster Ambiguity. To this end, we introduce CLAMS, a data-driven visual quality measure for automatically predicting cluster ambiguity in monochrome scatterplots. We first conduct a qualitative study to identify key factors that affect the visual separation of clusters (e.g., proximity or size difference between clusters). Based on study findings, we deploy a regression module that estimates the human-judged separability of two clusters. Then, CLAMS predicts cluster ambiguity by analyzing the aggregated results of all pairwise separability between clusters that are generated by the module. CLAMS outperforms widely-used clustering techniques in predicting ground truth cluster ambiguity. Meanwhile, CLAMS exhibits performance on par with human annotators. We conclude our work by presenting two applications for optimizing and benchmarking data mining techniques using CLAMS. The interactive demo of CLAMS is available at clusterambiguity.dev.
</details>
<details>
<summary>摘要</summary>
<<SYS>>TRANSLATE_TEXT视觉划分是一种常见的感知任务在散点图中，支持多种数据分析任务（例如，群集识别）。然而，即使使用同一个散点图，人们在进行视觉划分时可能会有不同的方式和不同的识别结果，这是因为人们之间存在差异和扩散的群集边界。这种感知变化会对数据分析基于视觉划分的可靠性提出各种 вопро题。在这项研究中，我们研究了在进行视觉划分时的感知变化，我们称之为划分混乱。为了实现这一目标，我们引入了一种数据驱动的视觉质量指标，称为CLAMS，可以自动预测散点图中划分的混乱程度。我们首先进行了一项质量研究，以确定影响视觉划分中cluster的关键因素（例如，群集之间的距离或者群集的大小差异）。根据研究发现，我们部署了一个回归模块，可以估算两个群集之间的人类评估分别。然后，CLAMS根据这些结果对所有对应的划分进行分析，并预测划分的混乱程度。我们发现，CLAMS可以超过常见的划分技术，并且与人类标注器表现相当。我们结束这项研究，并在散点图中优化和Benchmarking数据挖掘技术的应用中采用CLAMS。CLAMS的交互示例可以在clusterambiguity.dev上查看。TRANSLATE_TEXT
</details></li>
</ul>
<hr>
<h2 id="ZADU-A-Python-Library-for-Evaluating-the-Reliability-of-Dimensionality-Reduction-Embeddings"><a href="#ZADU-A-Python-Library-for-Evaluating-the-Reliability-of-Dimensionality-Reduction-Embeddings" class="headerlink" title="ZADU: A Python Library for Evaluating the Reliability of Dimensionality Reduction Embeddings"></a>ZADU: A Python Library for Evaluating the Reliability of Dimensionality Reduction Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00282">http://arxiv.org/abs/2308.00282</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hj-n/zadu">https://github.com/hj-n/zadu</a></li>
<li>paper_authors: Hyeon Jeon, Aeri Cho, Jinhwa Jang, Soohyun Lee, Jake Hyun, Hyung-Kwon Ko, Jaemin Jo, Jinwook Seo</li>
<li>for: 本研究旨在提供一个Python库（ZADU），用于评估维度缩减（DR） embedding 的可靠性。</li>
<li>methods: ZADU 提供了广泛的扭曲度量，自动优化扭曲度量的执行，并将个别点的贡献给扭曲度量提供，以便详细分析 DR embedding。</li>
<li>results: 通过一个实际的实验，我们验证了我们的优化方案可以快速执行扭曲度量，并且 ZADU 可以实现对 DR embedding 的详细分析。<details>
<summary>Abstract</summary>
Dimensionality reduction (DR) techniques inherently distort the original structure of input high-dimensional data, producing imperfect low-dimensional embeddings. Diverse distortion measures have thus been proposed to evaluate the reliability of DR embeddings. However, implementing and executing distortion measures in practice has so far been time-consuming and tedious. To address this issue, we present ZADU, a Python library that provides distortion measures. ZADU is not only easy to install and execute but also enables comprehensive evaluation of DR embeddings through three key features. First, the library covers a wide range of distortion measures. Second, it automatically optimizes the execution of distortion measures, substantially reducing the running time required to execute multiple measures. Last, the library informs how individual points contribute to the overall distortions, facilitating the detailed analysis of DR embeddings. By simulating a real-world scenario of optimizing DR embeddings, we verify that our optimization scheme substantially reduces the time required to execute distortion measures. Finally, as an application of ZADU, we present another library called ZADUVis that allows users to easily create distortion visualizations that depict the extent to which each region of an embedding suffers from distortions.
</details>
<details>
<summary>摘要</summary>
维度减少（DR）技术自然地扭曲输入高维数据的原始结构，生成不完美的低维嵌入。为了评估DR嵌入的可靠性，各种不同的扭曲度指标已经被提出。然而，在实践中实施和执行这些指标仍然是一项时间消耗和繁琐的任务。为解决这个问题，我们介绍了ZADU，一个Python库，它提供了多种扭曲度指标，并且可以自动优化执行这些指标，大幅减少执行多个指标所需的时间。此外，ZADU还可以告诉你具体的点对总的扭曲度做出了贡献，这使得可以详细分析DR嵌入。通过一个实际的优化DR嵌入场景的示例，我们证明了我们的优化方案可以减少执行扭曲度指标所需的时间。最后，作为ZADU的应用，我们介绍了一个名为ZADUVis的库，它允许用户轻松地创建扭曲度视觉化，这些视觉化可以显示嵌入中每个区域受到的扭曲度程度。
</details></li>
</ul>
<hr>
<h2 id="Data-Collaboration-Analysis-applied-to-Compound-Datasets-and-the-Introduction-of-Projection-data-to-Non-IID-settings"><a href="#Data-Collaboration-Analysis-applied-to-Compound-Datasets-and-the-Introduction-of-Projection-data-to-Non-IID-settings" class="headerlink" title="Data Collaboration Analysis applied to Compound Datasets and the Introduction of Projection data to Non-IID settings"></a>Data Collaboration Analysis applied to Compound Datasets and the Introduction of Projection data to Non-IID settings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00280">http://arxiv.org/abs/2308.00280</a></li>
<li>repo_url: None</li>
<li>paper_authors: Akihiro Mizoguchi, Anna Bogdanova, Akira Imakura, Tetsuya Sakurai</li>
<li>for: 预测化学物质的性能（prediction of chemical compound properties）</li>
<li>methods: 使用分布式机器学习（distributed machine learning）和数据合作分析（data collaboration analysis），以及一种改进的方法called数据合作分析使用投影数据（DCPd）</li>
<li>results: 比较了不同方法在非标一致分布（non-IID）和标一致分布（IID）下的分类精度（classification accuracy），结果显示DCPd在非标一致分布下表现最好，而且与其他方法的性能差异较小。<details>
<summary>Abstract</summary>
Given the time and expense associated with bringing a drug to market, numerous studies have been conducted to predict the properties of compounds based on their structure using machine learning. Federated learning has been applied to compound datasets to increase their prediction accuracy while safeguarding potentially proprietary information. However, federated learning is encumbered by low accuracy in not identically and independently distributed (non-IID) settings, i.e., data partitioning has a large label bias, and is considered unsuitable for compound datasets, which tend to have large label bias. To address this limitation, we utilized an alternative method of distributed machine learning to chemical compound data from open sources, called data collaboration analysis (DC). We also proposed data collaboration analysis using projection data (DCPd), which is an improved method that utilizes auxiliary PubChem data. This improves the quality of individual user-side data transformations for the projection data for the creation of intermediate representations. The classification accuracy, i.e., area under the curve in the receiver operating characteristic curve (ROC-AUC) and AUC in the precision-recall curve (PR-AUC), of federated averaging (FedAvg), DC, and DCPd was compared for five compound datasets. We determined that the machine learning performance for non-IID settings was in the order of DCPd, DC, and FedAvg, although they were almost the same in identically and independently distributed (IID) settings. Moreover, the results showed that compared to other methods, DCPd exhibited a negligible decline in classification accuracy in experiments with different degrees of label bias. Thus, DCPd can address the low performance in non-IID settings, which is one of the challenges of federated learning.
</details>
<details>
<summary>摘要</summary>
由于药物上市需要时间和成本，许多研究已经进行了预测药物的性质基于其结构使用机器学习。 federated learning已经应用于药物数据集来提高预测精度，但是在非标一致分布（non-IID）的设置下， federated learning 的准确率低，因此不适合药物数据集，这些数据集往往具有大的标签偏好。为解决这些限制，我们使用了一种分布式机器学习方法来分析化学物质数据，称为数据合作分析（DC）。我们还提出了使用投影数据（DCPd）来改进DC方法，这种方法使用辅助 PubChem 数据。这将提高用户端数据转换的质量，以便创建中间表示。我们比较了 FedAvg、DC 和 DCPd 的分类精度，包括地下曲线准确率（ROC-AUC）和精度-回归率曲线准确率（PR-AUC），对五个药物数据集进行了比较。结果表明，在非标一致分布下，DCPd 的机器学习性能高于 FedAvg 和 DC，尽管在标一致分布下，它们几乎相同。此外，结果还表明，相比其他方法，DCPd 在不同的标签偏好情况下的分类精度几乎不变。因此，DCPd 可以解决 federated learning 在非标一致分布下的低性能问题。
</details></li>
</ul>
<hr>
<h2 id="Robust-Positive-Unlabeled-Learning-via-Noise-Negative-Sample-Self-correction"><a href="#Robust-Positive-Unlabeled-Learning-via-Noise-Negative-Sample-Self-correction" class="headerlink" title="Robust Positive-Unlabeled Learning via Noise Negative Sample Self-correction"></a>Robust Positive-Unlabeled Learning via Noise Negative Sample Self-correction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00279">http://arxiv.org/abs/2308.00279</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/woriazzc/robust-pu">https://github.com/woriazzc/robust-pu</a></li>
<li>paper_authors: Zhangchi Zhu, Lu Wang, Pu Zhao, Chao Du, Wei Zhang, Hang Dong, Bo Qiao, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang</li>
<li>For: The paper focuses on improving the robustness of positive-unlabeled (PU) learning, which is a common approach in machine learning that uses both positive and unlabeled data for training.* Methods: The proposed method uses a novel “hardness” measure to distinguish unlabeled samples with a high chance of being negative from those with large label noise. An iterative training strategy is then implemented to fine-tune the selection of negative samples during the training process.* Results: The proposed method is shown to be effective in improving the accuracy and stability of learning with positive and unlabeled data through extensive experimental validations over a wide range of learning tasks.Here are the three key points in Simplified Chinese:* For: 本文关注Positive-unlabeled（PU）学习的稳定性提高，PU学习是机器学习中常见的一种方法，通过使用正样本和无标签样本进行训练。* Methods: 提议方法使用一种新的”困难度”度量，可以在训练过程中精准地分类无标签样本，并在训练过程中进行迭代的训练策略来细化选择负样本。* Results: 提议方法通过对各种学习任务进行广泛的实验验证，显示其可以有效地提高使用正样本和无标签样本进行训练的精度和稳定性。<details>
<summary>Abstract</summary>
Learning from positive and unlabeled data is known as positive-unlabeled (PU) learning in literature and has attracted much attention in recent years. One common approach in PU learning is to sample a set of pseudo-negatives from the unlabeled data using ad-hoc thresholds so that conventional supervised methods can be applied with both positive and negative samples. Owing to the label uncertainty among the unlabeled data, errors of misclassifying unlabeled positive samples as negative samples inevitably appear and may even accumulate during the training processes. Those errors often lead to performance degradation and model instability. To mitigate the impact of label uncertainty and improve the robustness of learning with positive and unlabeled data, we propose a new robust PU learning method with a training strategy motivated by the nature of human learning: easy cases should be learned first. Similar intuition has been utilized in curriculum learning to only use easier cases in the early stage of training before introducing more complex cases. Specifically, we utilize a novel ``hardness'' measure to distinguish unlabeled samples with a high chance of being negative from unlabeled samples with large label noise. An iterative training strategy is then implemented to fine-tune the selection of negative samples during the training process in an iterative manner to include more ``easy'' samples in the early stage of training. Extensive experimental validations over a wide range of learning tasks show that this approach can effectively improve the accuracy and stability of learning with positive and unlabeled data. Our code is available at https://github.com/woriazzc/Robust-PU
</details>
<details>
<summary>摘要</summary>
学习正面和未标注数据的方法，称为正面未标注（PU）学习，在过去几年内吸引了很多关注。一种常见的PU学习方法是从未标注数据中采样一些 Pseudo-negative samples 使用自定义的阈值，以便通过两类样本进行传统的超级vised学习。由于未标注数据中的标签不确定性，在训练过程中会出现错误地将未标注正面样本分类为负样本的现象，这会导致性能下降和模型不稳定。为了减少标签不确定性的影响和提高学习正面和未标注数据的稳定性，我们提出了一种新的robust PU学习方法，具体来说是通过人类学习的自然性，先学习容易的样本。这种想法类似于curriculum学习，在训练的早期使用更容易的样本。我们采用了一种新的“困难度”度量，以分辨未标注样本中高概率是负样本的样本和大量标签噪声。然后，我们实现了一种迭代训练策略，在训练过程中在迭代的方式中细化选择负样本，以包括更多的容易样本在训练的早期。经验 validate 表明，这种方法可以有效地提高学习正面和未标注数据的精度和稳定性。我们的代码可以在 <https://github.com/woriazzc/Robust-PU> 中找到。
</details></li>
</ul>
<hr>
<h2 id="Classes-are-not-Clusters-Improving-Label-based-Evaluation-of-Dimensionality-Reduction"><a href="#Classes-are-not-Clusters-Improving-Label-based-Evaluation-of-Dimensionality-Reduction" class="headerlink" title="Classes are not Clusters: Improving Label-based Evaluation of Dimensionality Reduction"></a>Classes are not Clusters: Improving Label-based Evaluation of Dimensionality Reduction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00278">http://arxiv.org/abs/2308.00278</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hj-n/ltnc">https://github.com/hj-n/ltnc</a></li>
<li>paper_authors: Hyeon Jeon, Yun-Hsin Kuo, Michaël Aupetit, Kwan-Liu Ma, Jinwook Seo</li>
<li>for: 评估维度减少（DR）嵌入的可靠性</li>
<li>methods: 引入了两个新的质量指标：标签可靠性和标签连续性（标签-T&amp;C），以提高基于类标签的DR评估</li>
<li>results: 比较了Label-T&amp;C与常用的DR评估指标（如信任性和连续性、库拉布-莱布尔差），发现Label-T&amp;C在评估DR嵌入保持cluster结构的准确性方面表现出色，并且可扩展性好。此外，Label-T&amp;C在揭示DR技术和其超参数的内在特性方面也有成功应用。<details>
<summary>Abstract</summary>
A common way to evaluate the reliability of dimensionality reduction (DR) embeddings is to quantify how well labeled classes form compact, mutually separated clusters in the embeddings. This approach is based on the assumption that the classes stay as clear clusters in the original high-dimensional space. However, in reality, this assumption can be violated; a single class can be fragmented into multiple separated clusters, and multiple classes can be merged into a single cluster. We thus cannot always assure the credibility of the evaluation using class labels. In this paper, we introduce two novel quality measures -- Label-Trustworthiness and Label-Continuity (Label-T&C) -- advancing the process of DR evaluation based on class labels. Instead of assuming that classes are well-clustered in the original space, Label-T&C work by (1) estimating the extent to which classes form clusters in the original and embedded spaces and (2) evaluating the difference between the two. A quantitative evaluation showed that Label-T&C outperform widely used DR evaluation measures (e.g., Trustworthiness and Continuity, Kullback-Leibler divergence) in terms of the accuracy in assessing how well DR embeddings preserve the cluster structure, and are also scalable. Moreover, we present case studies demonstrating that Label-T&C can be successfully used for revealing the intrinsic characteristics of DR techniques and their hyperparameters.
</details>
<details>
<summary>摘要</summary>
通常来评估维度减少（DR）嵌入的可靠性是根据类标签是否形成紧凑、相互隔离的圈状分布来衡量。这种方法假设高维空间中的类都是清晰分布的，但在实际情况下，这种假设可能被违反；单个类可能会分割成多个分布在多个不同的地方，多个类可能会合并成单个圈状分布。因此，我们不能总是对DR嵌入的评估结果具有信任感。在这篇论文中，我们引入了两种新的质量指标：标签可靠性（Label-Trustworthiness）和标签连续性（Label-Continuity， Label-T&C），这两种指标基于类标签的分布来评估DR嵌入的质量。不同于传统的DR评估方法，Label-T&C不假设类都是清晰分布在高维空间中，而是通过对原始空间和嵌入空间中类的分布进行评估来评估DR嵌入的质量。我们的量化评估表明，Label-T&C可以高效地评估DR嵌入的质量，并且比传统的DR评估指标（如信任度和连续性，卷积-莱布尼度）更准确地评估DR嵌入 preserve 类层次结构的程度。此外，我们还提供了 caso studies，证明了 Label-T&C 可以成功地用于揭示DR技术和其超参数的内在特性。
</details></li>
</ul>
<hr>
<h2 id="Neural-approximation-of-Wasserstein-distance-via-a-universal-architecture-for-symmetric-and-factorwise-group-invariant-functions"><a href="#Neural-approximation-of-Wasserstein-distance-via-a-universal-architecture-for-symmetric-and-factorwise-group-invariant-functions" class="headerlink" title="Neural approximation of Wasserstein distance via a universal architecture for symmetric and factorwise group invariant functions"></a>Neural approximation of Wasserstein distance via a universal architecture for symmetric and factorwise group invariant functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00273">http://arxiv.org/abs/2308.00273</a></li>
<li>repo_url: None</li>
<li>paper_authors: Samantha Chen, Yusu Wang</li>
<li>for: 本研究的目的是学习 Complex 对象之间的距离函数，如 Wasserstein 距离来比较点集。</li>
<li>methods: 本文提出了一种通用的神经网络架构来近似 SFGI 函数（Symmetric and Factor-wise Group Invariant 函数）。</li>
<li>results: 本文首次显示了一种神经网络可以approximate Wasserstein 距离，并且模型复杂度是独立于输入点集的大小。在实际中，我们的新提出的神经网络架构比其他模型（包括 SOTA Siamese Autoencoder 方法）更好地generalize和更快地训练。<details>
<summary>Abstract</summary>
Learning distance functions between complex objects, such as the Wasserstein distance to compare point sets, is a common goal in machine learning applications. However, functions on such complex objects (e.g., point sets and graphs) are often required to be invariant to a wide variety of group actions e.g. permutation or rigid transformation. Therefore, continuous and symmetric product functions (such as distance functions) on such complex objects must also be invariant to the product of such group actions. We call these functions symmetric and factor-wise group invariant (or SFGI functions in short). In this paper, we first present a general neural network architecture for approximating SFGI functions. The main contribution of this paper combines this general neural network with a sketching idea to develop a specific and efficient neural network which can approximate the $p$-th Wasserstein distance between point sets. Very importantly, the required model complexity is independent of the sizes of input point sets. On the theoretical front, to the best of our knowledge, this is the first result showing that there exists a neural network with the capacity to approximate Wasserstein distance with bounded model complexity. Our work provides an interesting integration of sketching ideas for geometric problems with universal approximation of symmetric functions. On the empirical front, we present a range of results showing that our newly proposed neural network architecture performs comparatively or better than other models (including a SOTA Siamese Autoencoder based approach). In particular, our neural network generalizes significantly better and trains much faster than the SOTA Siamese AE. Finally, this line of investigation could be useful in exploring effective neural network design for solving a broad range of geometric optimization problems (e.g., $k$-means in a metric space).
</details>
<details>
<summary>摘要</summary>
学习复杂对象之间的距离函数，如 Wasserstein 距离来比较点集，是机器学习应用中常见的目标。然而，函数在这些复杂对象上（例如点集和图）经常需要对广泛的群作用（如Permutation 和扭转变换）进行不变性。因此，在这些复杂对象上的连续和对称产品函数（如距离函数）必须对组合这些群作用的产品进行不变性。我们称这些函数为对称和因子团变函数（SFGI函数）。在这篇论文中，我们首先提出一种通用的神经网络架构来近似 SFGI 函数。我们的主要贡献是将这种通用神经网络与抽象思想结合，开发一种特定和高效的神经网络，可以近似 $p$-th Wasserstein 距离 между点集。非常重要的是，模型的复杂度不依赖于输入点集的大小。从理论上来看，我们的结果表明了存在一种能够近似 Wasserstein 距离的神经网络，并且模型的复杂度是固定的。我们的工作提供了对几何问题的解决方案和对共形函数的近似的 интересintegration。在实际方面，我们发表了一系列结果，表明我们的新提出的神经网络架构在与其他模型（包括顶尖Siamese Autoencoder）进行比较时，表现较好或更好。具体来说，我们的神经网络在泛化和训练速度方面都表现出了优势。最后，这种研究方向可能有助于探索有效的神经网络设计方法，用于解决广泛的几何优化问题（例如在度量空间中的 $k$-means）。
</details></li>
</ul>
<hr>
<h2 id="Multi-Modality-Multi-Loss-Fusion-Network"><a href="#Multi-Modality-Multi-Loss-Fusion-Network" class="headerlink" title="Multi-Modality Multi-Loss Fusion Network"></a>Multi-Modality Multi-Loss Fusion Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00264">http://arxiv.org/abs/2308.00264</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zehui Wu, Ziwei Gong, Jaywon Koo, Julia Hirschberg</li>
<li>for: 这个研究旨在提高情感检测中的特征选择和融合方法，使用多个感知模式的特征并将其组合在神经网络中。</li>
<li>methods: 研究使用不同的融合方法，并对多模态融合网络中的子网络性能进行分析，发现多模态特征训练可以提高单模态测试性能。</li>
<li>results: 研究结果显示，使用多模态特征可以提高单模态测试性能，并且基于数据注释schema设计的融合方法可以提高模型性能。最佳模型在CMU-MOSI、CMU-MOSEI和CH-SIMS三个数据集上达到了状态 искусственный智能表现，并在大多数指标上超过了其他方法。<details>
<summary>Abstract</summary>
In this work we investigate the optimal selection and fusion of features across multiple modalities and combine these in a neural network to improve emotion detection. We compare different fusion methods and examine the impact of multi-loss training within the multi-modality fusion network, identifying useful findings relating to subnet performance. Our best model achieves state-of-the-art performance for three datasets (CMU-MOSI, CMU-MOSEI and CH-SIMS), and outperforms the other methods in most metrics. We have found that training on multimodal features improves single modality testing and designing fusion methods based on dataset annotation schema enhances model performance. These results suggest a roadmap towards an optimized feature selection and fusion approach for enhancing emotion detection in neural networks.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们研究了多Modalities中最佳的特征选择和融合方法，并将这些方法 integrate into a neural network 以提高情感识别。我们比较了不同的融合方法，并研究了在多模态融合网络中多loss训练的影响，发现了有用的发现关于子网络性能。我们的最佳模型在三个数据集（CMU-MOSI、CMU-MOSEI和CH-SIMS）上达到了状态 искусственный neural networks 的最佳性能，并在大多数指标上超越了其他方法。我们发现，训练在多模态特征上提高单模态测试的性能，并基于数据集注释Schema设计的融合方法可以增强模型性能。这些结果建议一个优化特征选择和融合方法的路线图，以提高情感识别在神经网络中的性能。
</details></li>
</ul>
<hr>
<h2 id="Asynchronous-Federated-Learning-with-Bidirectional-Quantized-Communications-and-Buffered-Aggregation"><a href="#Asynchronous-Federated-Learning-with-Bidirectional-Quantized-Communications-and-Buffered-Aggregation" class="headerlink" title="Asynchronous Federated Learning with Bidirectional Quantized Communications and Buffered Aggregation"></a>Asynchronous Federated Learning with Bidirectional Quantized Communications and Buffered Aggregation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00263">http://arxiv.org/abs/2308.00263</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tomas Ortega, Hamid Jafarkhani</li>
<li>for: 这篇论文主要用于提高 asynchronous federated learning 的效率和扩展性，并且减少了通信成本。</li>
<li>methods: 这篇论文使用了一个新的量化推导方法（QAFeL），具有一个共享的“隐藏”状态，以避免因直接量化而导致的错误传播。</li>
<li>results: 论文提供了对 QAFeL 的理论均衡保证，并通过使用标准库存 benchmark 进行实验证明。<details>
<summary>Abstract</summary>
Asynchronous Federated Learning with Buffered Aggregation (FedBuff) is a state-of-the-art algorithm known for its efficiency and high scalability. However, it has a high communication cost, which has not been examined with quantized communications. To tackle this problem, we present a new algorithm (QAFeL), with a quantization scheme that establishes a shared "hidden" state between the server and clients to avoid the error propagation caused by direct quantization. This approach allows for high precision while significantly reducing the data transmitted during client-server interactions. We provide theoretical convergence guarantees for QAFeL and corroborate our analysis with experiments on a standard benchmark.
</details>
<details>
<summary>摘要</summary>
“异步联合学习 WITH 缓冲聚合（FedBuff）是当前最佳策略，其高效性和可扩展性具有广泛应用前景。然而，它具有高通信成本，而这个问题尚未与量化通信相结合考虑。为解决这个问题，我们提出了一种新的算法（QAFeL），具有一种量化方案，在服务器和客户端之间共享一个“隐藏”状态，以避免由直接量化引起的错误卷积。这种方法允许高精度，同时大幅减少了客户端和服务器之间的数据传输量。我们提供了理论受托证明，并在标准 benchmark 上进行了实验 validate 我们的分析。”Note that "FedBuff" is translated as "异步联合学习 WITH 缓冲聚合" (asynchronous federated learning with buffered aggregation), and "QAFeL" is translated as "量化联合学习（QAFeL）" (quantized federated learning).
</details></li>
</ul>
<hr>
<h2 id="AQUILA-Communication-Efficient-Federated-Learning-with-Adaptive-Quantization-of-Lazily-Aggregated-Gradients"><a href="#AQUILA-Communication-Efficient-Federated-Learning-with-Adaptive-Quantization-of-Lazily-Aggregated-Gradients" class="headerlink" title="AQUILA: Communication Efficient Federated Learning with Adaptive Quantization of Lazily-Aggregated Gradients"></a>AQUILA: Communication Efficient Federated Learning with Adaptive Quantization of Lazily-Aggregated Gradients</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00258">http://arxiv.org/abs/2308.00258</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihao Zhao, Yuzhu Mao, Zhenpeng Shi, Yang Liu, Tian Lan, Wenbo Ding, Xiao-Ping Zhang</li>
<li>for: 这篇论文的目的是提高 Federated Learning（分布式学习）中通信开销的高，通常是由大规模模型的传输引起的。</li>
<li>methods: 这篇论文引入了 Adaptive Quantization of Lazily-Aggregated Gradients（AQUILA），一个新的适应性框架，用于解决这些问题，提高分布式学习的效率和可靠性。AQUILA 包括了复杂的设备选择方法，优先运算设备更新的质量和用途。此外，AQUILA 还提出了一个优化的量化准则，用于提高通信效率，保证模型的参数稳定。</li>
<li>results: 实验结果显示，AQUILA 与现有方法相比，对于不同的非同质量 FL 环境（如非同质量数据和多样化模型架构）而言，可以实现明显的通信成本优化，同时维持模型性能的相似性。<details>
<summary>Abstract</summary>
The widespread adoption of Federated Learning (FL), a privacy-preserving distributed learning methodology, has been impeded by the challenge of high communication overheads, typically arising from the transmission of large-scale models. Existing adaptive quantization methods, designed to mitigate these overheads, operate under the impractical assumption of uniform device participation in every training round. Additionally, these methods are limited in their adaptability due to the necessity of manual quantization level selection and often overlook biases inherent in local devices' data, thereby affecting the robustness of the global model. In response, this paper introduces AQUILA (adaptive quantization of lazily-aggregated gradients), a novel adaptive framework devised to effectively handle these issues, enhancing the efficiency and robustness of FL. AQUILA integrates a sophisticated device selection method that prioritizes the quality and usefulness of device updates. Utilizing the exact global model stored by devices, it enables a more precise device selection criterion, reduces model deviation, and limits the need for hyperparameter adjustments. Furthermore, AQUILA presents an innovative quantization criterion, optimized to improve communication efficiency while assuring model convergence. Our experiments demonstrate that AQUILA significantly decreases communication costs compared to existing methods, while maintaining comparable model performance across diverse non-homogeneous FL settings, such as Non-IID data and heterogeneous model architectures.
</details>
<details>
<summary>摘要</summary>
“质量保护分布式学习（FL）的广泛采用受到了通信开销的挑战，通常是由大规模模型的传输所导致的。现有的适应量化方法，是为了解决这些开销，但是它们假设所有训练轮都有参与者，而且它们具有限制性，需要手动选择量化水平，并且常常忽视本地设备数据中的偏见，从而影响全球模型的稳定性。为了解决这些问题，本文提出了AQUILA（适应量化 Lazy 梯度），一种新的适应框架，可以更好地处理这些问题，提高FL的效率和稳定性。AQUILA  integrates 一种智能的设备选择方法，可以根据设备更新的质量和有用性来优先选择设备。通过使用设备上存储的全球模型，AQUILA 可以更准确地选择设备，减少模型偏差，并减少超参数的调整。此外，AQUILA 还提出了一种优化的量化标准，可以提高通信效率，保证模型的收敛。我们的实验表明，AQUILA 可以significantly reduce  communication costs compared to existing methods, while maintaining comparable model performance across diverse non-homogeneous FL settings, such as Non-IID data and heterogeneous model architectures。”
</details></li>
</ul>
<hr>
<h2 id="Best-Subset-Selection-in-Generalized-Linear-Models-A-Fast-and-Consistent-Algorithm-via-Splicing-Technique"><a href="#Best-Subset-Selection-in-Generalized-Linear-Models-A-Fast-and-Consistent-Algorithm-via-Splicing-Technique" class="headerlink" title="Best-Subset Selection in Generalized Linear Models: A Fast and Consistent Algorithm via Splicing Technique"></a>Best-Subset Selection in Generalized Linear Models: A Fast and Consistent Algorithm via Splicing Technique</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00251">http://arxiv.org/abs/2308.00251</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junxian Zhu, Jin Zhu, Borui Tang, Xuanyu Chen, Hongmei Lin, Xueqin Wang</li>
<li>for: 本文是为了提出一种高维 generalized linear model 中适应响应变化的精炼方法。</li>
<li>methods: 本文使用了一种快速算法来选择最佳子集，并在轻量级条件下提供了 statistically guaranteed 的方法。</li>
<li>results: 实验表明，本方法在变量选择和系数估计方面具有优异性，并且与现有方法相比，实现了约四倍的速度提升。<details>
<summary>Abstract</summary>
In high-dimensional generalized linear models, it is crucial to identify a sparse model that adequately accounts for response variation. Although the best subset section has been widely regarded as the Holy Grail of problems of this type, achieving either computational efficiency or statistical guarantees is challenging. In this article, we intend to surmount this obstacle by utilizing a fast algorithm to select the best subset with high certainty. We proposed and illustrated an algorithm for best subset recovery in regularity conditions. Under mild conditions, the computational complexity of our algorithm scales polynomially with sample size and dimension. In addition to demonstrating the statistical properties of our method, extensive numerical experiments reveal that it outperforms existing methods for variable selection and coefficient estimation. The runtime analysis shows that our implementation achieves approximately a fourfold speedup compared to popular variable selection toolkits like glmnet and ncvreg.
</details>
<details>
<summary>摘要</summary>
高维度泛化线性模型中，必须确定一个稀疏模型，以便准确地回归响应变化。虽然最佳子集问题被广泛视为这类问题的圣杯，但实现计算效率或统计保证是困难的。在这篇文章中，我们决心使用快速算法选择最佳子集，以高度确定性。我们提出了一种算法，并在正则条件下进行了描述。在规模和维度增长时，我们的算法的计算复杂度随样本大小和维度呈极其平滑的几何函数。除了证明我们的方法的统计性质外，我们的数值实验表明，我们的方法在变量选择和参数估计方面的性能较高，并且与流行的变量选择工具包like glmnet和ncvreg进行比较，我们的实现的运行时间约为四倍。
</details></li>
</ul>
<hr>
<h2 id="EEG-based-Cognitive-Load-Classification-using-Feature-Masked-Autoencoding-and-Emotion-Transfer-Learning"><a href="#EEG-based-Cognitive-Load-Classification-using-Feature-Masked-Autoencoding-and-Emotion-Transfer-Learning" class="headerlink" title="EEG-based Cognitive Load Classification using Feature Masked Autoencoding and Emotion Transfer Learning"></a>EEG-based Cognitive Load Classification using Feature Masked Autoencoding and Emotion Transfer Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00246">http://arxiv.org/abs/2308.00246</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dustin Pulver, Prithila Angkan, Paul Hungler, Ali Etemad</li>
<li>for: 本研究旨在提出一种基于电enzephalogram（EEG）的识别负荷思维方法，以便在各种敏感领域中进行性能和决策结果的分析。</li>
<li>methods: 本研究使用了transformer架构，通过跨类转移学习来连接情感和负荷思维。我们采用了自我超vised masked autoencoding来预训练我们的模型，并使用了冻结参数和精度调整来进行下游负荷思维分类。</li>
<li>results: 我们的实验结果显示，我们提出的方法可以具有强大的表现，并超过了传统的单阶段完全监督学习。此外，我们还进行了细化折衔和敏感性研究，以评估不同方面的影响。本研究的成果将增加情感计算领域的文献，并开辟了新的跨领域转移学习using自我超vised预训练的研究途径。<details>
<summary>Abstract</summary>
Cognitive load, the amount of mental effort required for task completion, plays an important role in performance and decision-making outcomes, making its classification and analysis essential in various sensitive domains. In this paper, we present a new solution for the classification of cognitive load using electroencephalogram (EEG). Our model uses a transformer architecture employing transfer learning between emotions and cognitive load. We pre-train our model using self-supervised masked autoencoding on emotion-related EEG datasets and use transfer learning with both frozen weights and fine-tuning to perform downstream cognitive load classification. To evaluate our method, we carry out a series of experiments utilizing two publicly available EEG-based emotion datasets, namely SEED and SEED-IV, for pre-training, while we use the CL-Drive dataset for downstream cognitive load classification. The results of our experiments show that our proposed approach achieves strong results and outperforms conventional single-stage fully supervised learning. Moreover, we perform detailed ablation and sensitivity studies to evaluate the impact of different aspects of our proposed solution. This research contributes to the growing body of literature in affective computing with a focus on cognitive load, and opens up new avenues for future research in the field of cross-domain transfer learning using self-supervised pre-training.
</details>
<details>
<summary>摘要</summary>
认知负担（Cognitive Load）在完成任务和决策结果中扮演着重要角色，因此其分类和分析成为了各种敏感领域中的重要课题。在这篇论文中，我们提出了一个新的认知负担分类方法，使用了 трансформа器架构和跨领域学习。我们在认知负担和情感之间进行了对应学习，并在过程中使用了两种不同的权重升级和精致化。为了评估我们的方法，我们进行了一连串的实验，使用了两个公开available的EEG基于情感资料集，namely SEED和SEED-IV，进行预训练，而下游认知负担分类则使用了CL-Drive资料集。结果显示，我们的提出的方法具有强大的表现和超越传统单阶充足学习。此外，我们还进行了细部抑制和敏感性研究，以评估不同方面的影响。这个研究对于情感计算领域中的认知负担有所贡献，并开启了跨领域跨学习使用自愿学习预训练的新的应用领域。
</details></li>
</ul>
<hr>
<h2 id="Beam-Detection-Based-on-Machine-Learning-Algorithms"><a href="#Beam-Detection-Based-on-Machine-Learning-Algorithms" class="headerlink" title="Beam Detection Based on Machine Learning Algorithms"></a>Beam Detection Based on Machine Learning Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00718">http://arxiv.org/abs/2308.00718</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoyuan Li, Qing Yin</li>
<li>for:  precisely determining the positions of free electron laser beams on screens</li>
<li>methods:  using a sequence of machine learning models, including a self-constructed convolutional neural network based on VGG16 model and a support vector regression model</li>
<li>results:  achieving 85.8% correct prediction on test data<details>
<summary>Abstract</summary>
The positions of free electron laser beams on screens are precisely determined by a sequence of machine learning models. Transfer training is conducted in a self-constructed convolutional neural network based on VGG16 model. Output of intermediate layers are passed as features to a support vector regression model. With this sequence, 85.8% correct prediction is achieved on test data.
</details>
<details>
<summary>摘要</summary>
“免电子激光束在屏幕上的位置精确地由一个机器学习模型序列确定。在自定义的卷积神经网络基于VGG16模型上进行传输训练。输出的中间层的输出被用作特征，并被传输到一个支持向量回归模型。通过这个序列，在测试数据上达到了85.8%的正确预测率。”Here's a word-for-word translation of the text:“免电子激光束在屏幕上的位置由机器学习模型序列确定。基于VGG16模型自定义卷积神经网络进行传输训练。中间层输出用作特征，并传输到支持向量回归模型。测试数据上达到85.8%的正确预测率。”
</details></li>
</ul>
<hr>
<h2 id="ChatMOF-An-Autonomous-AI-System-for-Predicting-and-Generating-Metal-Organic-Frameworks"><a href="#ChatMOF-An-Autonomous-AI-System-for-Predicting-and-Generating-Metal-Organic-Frameworks" class="headerlink" title="ChatMOF: An Autonomous AI System for Predicting and Generating Metal-Organic Frameworks"></a>ChatMOF: An Autonomous AI System for Predicting and Generating Metal-Organic Frameworks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01423">http://arxiv.org/abs/2308.01423</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yeonghun1675/chatmof">https://github.com/yeonghun1675/chatmof</a></li>
<li>paper_authors: Yeonghun Kang, Jihan Kim</li>
<li>for: 这篇论文是为了探讨和开发一种基于自然语言处理的金属有机框架（MOFs）预测和生成系统。</li>
<li>methods: 这个系统使用了一个大规模语言模型（gpt-3.5-turbo），从文本输入中提取关键信息并提供相应的回答，因此消除了需要僵化的结构化查询的需要。系统由三个核心组件（代理、工具箱和评估器）组成，形成一个可靠的管道，用于处理多种任务，包括数据检索、性能预测和结构生成。</li>
<li>results: 这篇论文还研究了使用大语言模型AI系统在材料科学中的优势和局限，并展示了其可能性的潜在发展。<details>
<summary>Abstract</summary>
ChatMOF is an autonomous Artificial Intelligence (AI) system that is built to predict and generate of metal-organic frameworks (MOFs). By leveraging a large-scale language model (gpt-3.5-turbo), ChatMOF extracts key details from textual inputs and delivers appropriate responses, thus eliminating the necessity for rigid structured queries. The system is comprised of three core components (i.e. an agent, a toolkit, and an evaluator) and it forms a robust pipeline that manages a variety of tasks, including data retrieval, property prediction, and structure generation. The study further explores the merits and constraints of using large language models (LLMs) AI system in material sciences using and showcases its transformative potential for future advancements.
</details>
<details>
<summary>摘要</summary>
chatMOF 是一个自主的人工智能（AI）系统，用于预测和生成金刚烷金刚烷框架（MOFs）。通过利用大规模语言模型（gpt-3.5-turbo）， chatMOF 从文本输入中提取关键信息并提供相应的回答，因此消除了僵化的结构化查询的需要。该系统由三个核心组件（即代理、工具箱和评估器）组成，形成了一个全面的管道，处理多种任务，包括数据检索、性能预测和结构生成。研究还探讨了使用大语言模型（LLMs）AI系统在材料科学中的优劣和限制，并展示了其可以在未来的发展中发挥转变性的作用。
</details></li>
</ul>
<hr>
<h2 id="Capsa-A-Unified-Framework-for-Quantifying-Risk-in-Deep-Neural-Networks"><a href="#Capsa-A-Unified-Framework-for-Quantifying-Risk-in-Deep-Neural-Networks" class="headerlink" title="Capsa: A Unified Framework for Quantifying Risk in Deep Neural Networks"></a>Capsa: A Unified Framework for Quantifying Risk in Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00231">http://arxiv.org/abs/2308.00231</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sadhana Lolla, Iaroslav Elistratov, Alejandro Perez, Elaheh Ahmadi, Daniela Rus, Alexander Amini</li>
<li>for: 这篇论文旨在提供一种扩展模型的风险意识框架，以便在复杂的问题上提高模型的可靠性和安全性。</li>
<li>methods: 这篇论文使用了多种风险意识算法，包括 aleatoric uncertainty、epistemic uncertainty 和偏见估计等，并将这些算法集成到一个框架中，以便在复杂的感知数据集上进行测试和评估。</li>
<li>results: 根据实验结果，capsa 框架能够轻松地组合不同的风险意识算法，并在复杂的感知数据集上提供了全面的风险意识。<details>
<summary>Abstract</summary>
The modern pervasiveness of large-scale deep neural networks (NNs) is driven by their extraordinary performance on complex problems but is also plagued by their sudden, unexpected, and often catastrophic failures, particularly on challenging scenarios. Existing algorithms that provide risk-awareness to NNs are complex and ad-hoc. Specifically, these methods require significant engineering changes, are often developed only for particular settings, and are not easily composable. Here we present capsa, a framework for extending models with risk-awareness. Capsa provides a methodology for quantifying multiple forms of risk and composing different algorithms together to quantify different risk metrics in parallel. We validate capsa by implementing state-of-the-art uncertainty estimation algorithms within the capsa framework and benchmarking them on complex perception datasets. We demonstrate capsa's ability to easily compose aleatoric uncertainty, epistemic uncertainty, and bias estimation together in a single procedure, and show how this approach provides a comprehensive awareness of NN risk.
</details>
<details>
<summary>摘要</summary>
现代大规模深度神经网络（NN）的普遍性受到了它们在复杂问题上的非凡表现的驱动，但也受到了它们在困难场景下的不预期和致命的失败的困扰。现有的风险意识提供方法都是复杂的和不一致的，它们需要大量的工程变更，通常只适用于特定的设置，而且不易组合。我们在这里提出了一个框架，即capsa，用于扩展模型的风险意识。capsa提供了多种风险量化的方法ología，并可以将不同的风险度量化算法相互组合，以并行地计算多种风险指标。我们通过在capsa框架中实现现有的uncertainty估计算法，并对复杂的感知数据集进行了 benchmarking，以证明capsa的能力。我们展示了capsa可以轻松地组合 aleatoric uncertainty、epistemic uncertainty 和偏见估计在一个过程中，并显示了这种方法在NN风险意识方面提供了全面的认知。
</details></li>
</ul>
<hr>
<h2 id="Instructed-to-Bias-Instruction-Tuned-Language-Models-Exhibit-Emergent-Cognitive-Bias"><a href="#Instructed-to-Bias-Instruction-Tuned-Language-Models-Exhibit-Emergent-Cognitive-Bias" class="headerlink" title="Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias"></a>Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00225">http://arxiv.org/abs/2308.00225</a></li>
<li>repo_url: None</li>
<li>paper_authors: Itay Itzhak, Gabriel Stanovsky, Nir Rosenfeld, Yonatan Belinkov</li>
<li>For: This paper aims to investigate the presence of cognitive biases in fine-tuned large language models (LMs), specifically those that have undergone instruction tuning.* Methods: The authors examine three cognitive biases - the decoy effect, the certainty effect, and the belief bias - in various LMs, including Flan-T5, GPT3.5, and GPT4. They use a variety of evaluation metrics and methods to detect these biases.* Results: The authors find evidence of these biases in the fine-tuned models, particularly in those that have undergone instruction tuning. They also observe that these biases are more pronounced in certain models and tasks. Their findings highlight the need for further research into cognitive biases in instruction-tuned LMs.Here is the information in Simplified Chinese text:* 为: 这篇论文探讨了各种大语言模型（LMs）中的认知偏见问题，尤其是那些经过了指导调整的模型。* 方法: 作者们使用了多种评估指标和方法来探测这些偏见。* 结果: 作者们发现了这些偏见在调整后的模型中，特别是在某些模型和任务中更为明显。他们的发现提醒我们需要更进一步的研究认知偏见在指导调整后的LMs。<details>
<summary>Abstract</summary>
Recent studies show that instruction tuning and learning from human feedback improve the abilities of large language models (LMs) dramatically. While these tuning methods can make models generate high-quality text, we conjecture that more implicit cognitive biases may arise in these fine-tuned models. Our work provides evidence that these fine-tuned models exhibit biases that were absent or less pronounced in their pretrained predecessors. We examine the extent of this phenomenon in three cognitive biases - the decoy effect, the certainty effect, and the belief bias - all of which are known to influence human decision-making and reasoning. Our findings highlight the presence of these biases in various models, especially those that have undergone instruction tuning, such as Flan-T5, GPT3.5, and GPT4. This research constitutes a step toward comprehending cognitive biases in instruction-tuned LMs, which is crucial for the development of more reliable and unbiased language models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Deep-Reinforcement-Learning-Based-Battery-Conditioning-Hierarchical-V2G-Coordination-for-Multi-Stakeholder-Benefits"><a href="#Deep-Reinforcement-Learning-Based-Battery-Conditioning-Hierarchical-V2G-Coordination-for-Multi-Stakeholder-Benefits" class="headerlink" title="Deep Reinforcement Learning-Based Battery Conditioning Hierarchical V2G Coordination for Multi-Stakeholder Benefits"></a>Deep Reinforcement Learning-Based Battery Conditioning Hierarchical V2G Coordination for Multi-Stakeholder Benefits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00218">http://arxiv.org/abs/2308.00218</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yubao Zhang, Xin Chen, Yi Gu, Zhicheng Li, Wu Kai</li>
<li>for: 提高可再生能源利用率和电网稳定性，适用于电动汽车（EV）和大规模协调策略。</li>
<li>methods: 基于深度强化学习（DRL）和证明权益算法，实现多方受益者协调。</li>
<li>results: 比基eline四个 Referenced baselines，该策略可以提高可再生能源消耗率、降低负荷波动、满足EV充电需求，并减少充电成本和电池衰老。<details>
<summary>Abstract</summary>
With the growing prevalence of electric vehicles (EVs) and advancements in EV electronics, vehicle-to-grid (V2G) techniques and large-scale scheduling strategies have emerged to promote renewable energy utilization and power grid stability. This study proposes a multi-stakeholder hierarchical V2G coordination based on deep reinforcement learning (DRL) and the Proof of Stake algorithm. Furthermore, the multi-stakeholders include the power grid, EV aggregators (EVAs), and users, and the proposed strategy can achieve multi-stakeholder benefits. On the grid side, load fluctuations and renewable energy consumption are considered, while on the EVA side, energy constraints and charging costs are considered. The three critical battery conditioning parameters of battery SOX are considered on the user side, including state of charge, state of power, and state of health. Compared with four typical baselines, the multi-stakeholder hierarchical coordination strategy can enhance renewable energy consumption, mitigate load fluctuations, meet the energy demands of EVA, and reduce charging costs and battery degradation under realistic operating conditions.
</details>
<details>
<summary>摘要</summary>
On the grid side, the method considers load fluctuations and renewable energy consumption, while on the EVA side, it considers energy constraints and charging costs. For users, the method takes into account three critical battery conditioning parameters: state of charge, state of power, and state of health.Compared to four typical baselines, the multi-stakeholder hierarchical coordination strategy can increase renewable energy consumption, mitigate load fluctuations, meet the energy demands of EVAs, and reduce charging costs and battery degradation under realistic operating conditions.
</details></li>
</ul>
<hr>
<h2 id="Robust-Single-view-Cone-beam-X-ray-Pose-Estimation-with-Neural-Tuned-Tomography-NeTT-and-Masked-Neural-Radiance-Fields-mNeRF"><a href="#Robust-Single-view-Cone-beam-X-ray-Pose-Estimation-with-Neural-Tuned-Tomography-NeTT-and-Masked-Neural-Radiance-Fields-mNeRF" class="headerlink" title="Robust Single-view Cone-beam X-ray Pose Estimation with Neural Tuned Tomography (NeTT) and Masked Neural Radiance Fields (mNeRF)"></a>Robust Single-view Cone-beam X-ray Pose Estimation with Neural Tuned Tomography (NeTT) and Masked Neural Radiance Fields (mNeRF)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00214">http://arxiv.org/abs/2308.00214</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chaochao Zhou, Syed Hasib Akhter Faruqui, Abhinav Patel, Ramez N. Abdalla, Michael C. Hurley, Ali Shaibani, Matthew B. Potts, Babak S. Jahromi, Leon Cho, Sameer A. Ansari, Donald R. Cantrell</li>
<li>for: 这篇论文是用于描述一种基于X射线投影的医学操作pose estimation的新方法。</li>
<li>methods: 该方法使用了DiffDRR算法来高效计算Digitally Reconstructed Radiographs（DRRs），并使用TensorFlow中的自动微分来实现 Iterative Gradient Descent 算法。两种新的高精度视图synthesis方法Neural Tuned Tomography（NeTT）和Masked Neural Radiance Fields（mNeRF）也被提出，它们都基于经典的Conic Beam Computerized Tomography（CBCT）。</li>
<li>results: 对于pose estimation问题，NeTT和mNeRF两种方法都能够达到高精度的结果，成功率高于93%。然而，NeTT的计算成本远低于mNeRF，而且NeTT可以在训练和pose estimation阶段都具有更好的性能。此外， authors还发现NeTT可以在不同的主体上进行高精度的DRRsynthesis和pose estimation。因此，NeTT是一种可靠的pose estimation方法。<details>
<summary>Abstract</summary>
Many tasks performed in image-guided, mini-invasive, medical procedures can be cast as pose estimation problems, where an X-ray projection is utilized to reach a target in 3D space. Expanding on recent advances in the differentiable rendering of optically reflective materials, we introduce new methods for pose estimation of radiolucent objects using X-ray projections, and we demonstrate the critical role of optimal view synthesis in performing this task. We first develop an algorithm (DiffDRR) that efficiently computes Digitally Reconstructed Radiographs (DRRs) and leverages automatic differentiation within TensorFlow. Pose estimation is performed by iterative gradient descent using a loss function that quantifies the similarity of the DRR synthesized from a randomly initialized pose and the true fluoroscopic image at the target pose. We propose two novel methods for high-fidelity view synthesis, Neural Tuned Tomography (NeTT) and masked Neural Radiance Fields (mNeRF). Both methods rely on classic Cone-Beam Computerized Tomography (CBCT); NeTT directly optimizes the CBCT densities, while the non-zero values of mNeRF are constrained by a 3D mask of the anatomic region segmented from CBCT. We demonstrate that both NeTT and mNeRF distinctly improve pose estimation within our framework. By defining a successful pose estimate to be a 3D angle error of less than 3 deg, we find that NeTT and mNeRF can achieve similar results, both with overall success rates more than 93%. However, the computational cost of NeTT is significantly lower than mNeRF in both training and pose estimation. Furthermore, we show that a NeTT trained for a single subject can generalize to synthesize high-fidelity DRRs and ensure robust pose estimations for all other subjects. Therefore, we suggest that NeTT is an attractive option for robust pose estimation using fluoroscopic projections.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="SkullGAN-Synthetic-Skull-CT-Generation-with-Generative-Adversarial-Networks"><a href="#SkullGAN-Synthetic-Skull-CT-Generation-with-Generative-Adversarial-Networks" class="headerlink" title="SkullGAN: Synthetic Skull CT Generation with Generative Adversarial Networks"></a>SkullGAN: Synthetic Skull CT Generation with Generative Adversarial Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00206">http://arxiv.org/abs/2308.00206</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kbp-lab/skullgan">https://github.com/kbp-lab/skullgan</a></li>
<li>paper_authors: Kasra Naftchi-Ardebili, Karanpartap Singh, Reza Pourabolghasem, Pejman Ghanouni, Gerald R. Popelka, Kim Butts Pauly</li>
<li>for: 减少实验图像的依赖，以促进医疗机器学习的整合。</li>
<li>methods: 使用生成对抗网络（GAN）创建大量人颅CT剖析资料，以减少实验图像的需求和提高医疗机器学习的应用。</li>
<li>results: SkullGAN生成的人颅CT剖析图像和实验图像之间的三个量度（骨密度比、平均厚度和平均强度）之间存在类似性，并且使用SkullGAN推论器作为分类器时，可以将56.5%的实验图像和55.9%的SkullGAN生成图像视为实验图像（理论最佳值为50%），显示SkullGAN生成的人颅CT剖析集合与实验图像集合无法区别。<details>
<summary>Abstract</summary>
Deep learning offers potential for various healthcare applications involving the human skull but requires extensive datasets of curated medical images. To overcome this challenge, we propose SkullGAN, a generative adversarial network (GAN), to create large datasets of synthetic skull CT slices, reducing reliance on real images and accelerating the integration of machine learning into healthcare. In our method, CT slices of 38 subjects were fed to SkullGAN, a neural network comprising over 200 million parameters. The synthetic skull images generated were evaluated based on three quantitative radiological features: skull density ratio (SDR), mean thickness, and mean intensity. They were further analyzed using t-distributed stochastic neighbor embedding (t-SNE) and by applying the SkullGAN discriminator as a classifier. The results showed that SkullGAN-generated images demonstrated similar key quantitative radiological features to real skulls. Further definitive analysis was undertaken by applying the discriminator of SkullGAN, where the SkullGAN discriminator classified 56.5% of a test set of real skull images and 55.9% of the SkullGAN-generated images as reals (the theoretical optimum being 50%), demonstrating that the SkullGAN-generated skull set is indistinguishable from the real skull set - within the limits of our nonlinear classifier. Therefore, SkullGAN makes it possible to generate large numbers of synthetic skull CT segments, necessary for training neural networks for medical applications involving the human skull. This mitigates challenges associated with preparing large, high-quality training datasets, such as access, capital, time, and the need for domain expertise.
</details>
<details>
<summary>摘要</summary>
深度学习可能在各种医疗应用中使用人颅部，但需要大量的 curae 医疗影像集。为解决这个挑战，我们提议使用 SkullGAN，一种生成敌对网络（GAN），创建大量的人工颅部CT切片，降低对实际影像的依赖和加速机器学习在医疗中的集成。在我们的方法中，CT切片的38名参与者被 fed 到 SkullGAN，一个包含超过2亿个参数的神经网络。生成的人工颅部影像被评估基于三个量化医学特征：颅部密度比率（SDR）、平均厚度和平均强度。它们还被使用t-分布随机邻居embedding（t-SNE）分析，并通过应用SkullGAN推论器作为分类器。结果表明，SkullGAN生成的影像具有与实际颅部相同的关键量化医学特征。进一步的定 définitive分析表明，SkullGAN推论器对实际颅部测试集中的56.5%和SkullGAN生成的影像中的55.9%进行了分类，表明SkullGAN生成的颅部集是与实际颅部集相同的 - 在我们的非线性分类器的限制下。因此，SkullGAN使得可以生成大量的人工颅部CT段，为医疗应用中的神经网络训练准备庞大、高质量的医疗影像集。这解决了准备大量、高质量医疗影像集的挑战，包括访问、资金、时间和域专业知识的需求。
</details></li>
</ul>
<hr>
<h2 id="CBCL-PR-A-Cognitively-Inspired-Model-for-Class-Incremental-Learning-in-Robotics"><a href="#CBCL-PR-A-Cognitively-Inspired-Model-for-Class-Incremental-Learning-in-Robotics" class="headerlink" title="CBCL-PR: A Cognitively Inspired Model for Class-Incremental Learning in Robotics"></a>CBCL-PR: A Cognitively Inspired Model for Class-Incremental Learning in Robotics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00199">http://arxiv.org/abs/2308.00199</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aliayub7/cbcl-pr">https://github.com/aliayub7/cbcl-pr</a></li>
<li>paper_authors: Ali Ayub, Alan R. Wagner</li>
<li>for: 本文解决了Robot在有限数据环境中不断学习和适应的问题，具体来说是几个数据示例内的类增量学习（FSIL）问题。</li>
<li>methods: 我们提出了一种基于脑中神经元和血液管理的概念学习理论的新框架，用于解决FSIL问题。该框架将物体类表示为集合的集合，并将其存储在内存中。当学习新类时，框架会重温过去学习的类数据，以避免忘记。</li>
<li>results: 我们在两个物体分类数据集上进行了评估，得到了当前最佳（SOTA）性能的类增量学习和FSIL性能。此外，我们还在一个 robot 上进行了FSIL测试，结果表明 robot 可以不断地学习并分类大量家用品，只需限制的人类协助。<details>
<summary>Abstract</summary>
For most real-world applications, robots need to adapt and learn continually with limited data in their environments. In this paper, we consider the problem of Few-Shot class Incremental Learning (FSIL), in which an AI agent is required to learn incrementally from a few data samples without forgetting the data it has previously learned. To solve this problem, we present a novel framework inspired by theories of concept learning in the hippocampus and the neocortex. Our framework represents object classes in the form of sets of clusters and stores them in memory. The framework replays data generated by the clusters of the old classes, to avoid forgetting when learning new classes. Our approach is evaluated on two object classification datasets resulting in state-of-the-art (SOTA) performance for class-incremental learning and FSIL. We also evaluate our framework for FSIL on a robot demonstrating that the robot can continually learn to classify a large set of household objects with limited human assistance.
</details>
<details>
<summary>摘要</summary>
For most real-world applications, robots need to adapt and learn continually with limited data in their environments. In this paper, we consider the problem of Few-Shot class Incremental Learning (FSIL), in which an AI agent is required to learn incrementally from a few data samples without forgetting the data it has previously learned. To solve this problem, we present a novel framework inspired by theories of concept learning in the hippocampus and the neocortex. Our framework represents object classes in the form of sets of clusters and stores them in memory. The framework replays data generated by the clusters of the old classes, to avoid forgetting when learning new classes. Our approach is evaluated on two object classification datasets, resulting in state-of-the-art (SOTA) performance for class-incremental learning and FSIL. We also evaluate our framework for FSIL on a robot, demonstrating that the robot can continually learn to classify a large set of household objects with limited human assistance.Here's the text in Traditional Chinese:For most real-world applications, robots need to adapt and learn continually with limited data in their environments. In this paper, we consider the problem of Few-Shot class Incremental Learning (FSIL), in which an AI agent is required to learn incrementally from a few data samples without forgetting the data it has previously learned. To solve this problem, we present a novel framework inspired by theories of concept learning in the hippocampus and the neocortex. Our framework represents object classes in the form of sets of clusters and stores them in memory. The framework replays data generated by the clusters of the old classes, to avoid forgetting when learning new classes. Our approach is evaluated on two object classification datasets, resulting in state-of-the-art (SOTA) performance for class-incremental learning and FSIL. We also evaluate our framework for FSIL on a robot, demonstrating that the robot can continually learn to classify a large set of household objects with limited human assistance.
</details></li>
</ul>
<hr>
<h2 id="C-DARL-Contrastive-diffusion-adversarial-representation-learning-for-label-free-blood-vessel-segmentation"><a href="#C-DARL-Contrastive-diffusion-adversarial-representation-learning-for-label-free-blood-vessel-segmentation" class="headerlink" title="C-DARL: Contrastive diffusion adversarial representation learning for label-free blood vessel segmentation"></a>C-DARL: Contrastive diffusion adversarial representation learning for label-free blood vessel segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00193">http://arxiv.org/abs/2308.00193</a></li>
<li>repo_url: None</li>
<li>paper_authors: Boah Kim, Yujin Oh, Bradford J. Wood, Ronald M. Summers, Jong Chul Ye</li>
<li>for: 这篇论文旨在提供一种自顾学性脉络分割方法，以便在医疗影像中诊断和规划涉及广泛诊断和手术计划。</li>
<li>methods: 本文提出了一种自顾学性脉络分割方法，称为对比扩散对抗表示学习（C-DARL）模型。该模型由扩散模块和生成模块组成，通过生成多域血管数据的分布来学习血管图像的分布。此外，我们采用了对比学习，通过一个面纱基于的对比损失来让模型学习更真实的血管表示。</li>
<li>results: 实验结果表明，C-DARL比基eline方法具有更高的性能，并且具有阈值稳定性，表明C-DARL可以有效地进行脉络分割。<details>
<summary>Abstract</summary>
Blood vessel segmentation in medical imaging is one of the essential steps for vascular disease diagnosis and interventional planning in a broad spectrum of clinical scenarios in image-based medicine and interventional medicine. Unfortunately, manual annotation of the vessel masks is challenging and resource-intensive due to subtle branches and complex structures. To overcome this issue, this paper presents a self-supervised vessel segmentation method, dubbed the contrastive diffusion adversarial representation learning (C-DARL) model. Our model is composed of a diffusion module and a generation module that learns the distribution of multi-domain blood vessel data by generating synthetic vessel images from diffusion latent. Moreover, we employ contrastive learning through a mask-based contrastive loss so that the model can learn more realistic vessel representations. To validate the efficacy, C-DARL is trained using various vessel datasets, including coronary angiograms, abdominal digital subtraction angiograms, and retinal imaging. Experimental results confirm that our model achieves performance improvement over baseline methods with noise robustness, suggesting the effectiveness of C-DARL for vessel segmentation.
</details>
<details>
<summary>摘要</summary>
《医学影像中血管分割》是医学影像诊断和 intervención规划中一项非常重要的步骤，在各种临床场景下都是如此。然而，手动标注血管掩蔽是一项困难和耗时的任务，因为血管分支和结构相对复杂。为解决这个问题，这篇论文提出了一种自动化血管分割方法，名为对抗扩散对抗学习（C-DARL）模型。我们的模型包括扩散模块和生成模块，通过生成多域血管数据的分布来学习血管图像。此外，我们还使用对比学习，通过一个掩蔽基于的对比损失，使模型学习更真实的血管表示。为验证效果，C-DARL在不同的血管数据集上进行了训练，包括肠动脉ANGIOGRAM、腹部数字抽取ANGIOGRAM和视网膜成像。实验结果表明，我们的模型在比基eline方法上具有噪音Robustness，表明C-DARL有效地进行了血管分割。
</details></li>
</ul>
<hr>
<h2 id="Universal-Majorization-Minimization-Algorithms"><a href="#Universal-Majorization-Minimization-Algorithms" class="headerlink" title="Universal Majorization-Minimization Algorithms"></a>Universal Majorization-Minimization Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00190">http://arxiv.org/abs/2308.00190</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matthew Streeter</li>
<li>for: 优化问题</li>
<li>methods: 使用自动梯度下降法生成自动majorizer，实现对任意问题的优化</li>
<li>results: 可以从任意初始点开始，无需调整超参数，并且可以 converges to global optimum<details>
<summary>Abstract</summary>
Majorization-minimization (MM) is a family of optimization methods that iteratively reduce a loss by minimizing a locally-tight upper bound, called a majorizer. Traditionally, majorizers were derived by hand, and MM was only applicable to a small number of well-studied problems. We present optimizers that instead derive majorizers automatically, using a recent generalization of Taylor mode automatic differentiation. These universal MM optimizers can be applied to arbitrary problems and converge from any starting point, with no hyperparameter tuning.
</details>
<details>
<summary>摘要</summary>
Majorization-minimization（MM）是一家优化方法的家族，通过逐步减少损失函数，最终得到一个当地紧急的上界，称为主要函数。在过去，主要函数通常是手动 derive的，因此MM只能应用于一些已经广泛研究的问题。我们提出了一种使用最近普遍化的泰勒模式自动导数来自动 derive 主要函数的优化器。这种通用的 MM 优化器可以应用于任何问题，从任何初始点开始，无需调整超参数。
</details></li>
</ul>
<hr>
<h2 id="Generative-Models-as-a-Complex-Systems-Science-How-can-we-make-sense-of-large-language-model-behavior"><a href="#Generative-Models-as-a-Complex-Systems-Science-How-can-we-make-sense-of-large-language-model-behavior" class="headerlink" title="Generative Models as a Complex Systems Science: How can we make sense of large language model behavior?"></a>Generative Models as a Complex Systems Science: How can we make sense of large language model behavior?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00189">http://arxiv.org/abs/2308.00189</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ari Holtzman, Peter West, Luke Zettlemoyer</li>
<li>for: 本研究旨在系统地研究语言模型的行为类型，以便更好地理解它们在不同任务上的表现。</li>
<li>methods: 本研究使用了一种系统的分类方法，以分类语言模型的行为类型，并对这些类型的分布进行了分析。</li>
<li>results: 研究发现，语言模型的行为可以分为多种类型，包括基本的语言理解、语义理解和语言生成等。这些类型之间存在着较强的相关关系，可以帮助我们更好地理解语言模型在不同任务上的表现。I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Coaxing out desired behavior from pretrained models, while avoiding undesirable ones, has redefined NLP and is reshaping how we interact with computers. What was once a scientific engineering discipline-in which building blocks are stacked one on top of the other-is arguably already a complex systems science, in which emergent behaviors are sought out to support previously unimagined use cases.   Despite the ever increasing number of benchmarks that measure task performance, we lack explanations of what behaviors language models exhibit that allow them to complete these tasks in the first place. We argue for a systematic effort to decompose language model behavior into categories that explain cross-task performance, to guide mechanistic explanations and help future-proof analytic research.
</details>
<details>
<summary>摘要</summary>
使待命模型展现愿望的行为，而不是不良的一种，已经重塑了自然语言处理（NLP）领域，并改变了我们与计算机之间的交互方式。过去，建立模型的科学工程是一种堆叠一个叠的过程，但现在，这已经变成了复杂系统科学，寻找 Emergent 行为来支持未想过的应用场景。尽管有越来越多的任务性能指标，但我们仍然缺乏这些语言模型展现出的行为所导致的任务完成的解释。我们提出了一种系统的努力，以分类语言模型的行为，以解释跨任务性能的交叠，以帮助未来的分析研究。
</details></li>
</ul>
<hr>
<h2 id="Attribution-Scores-in-Data-Management-and-Explainable-Machine-Learning"><a href="#Attribution-Scores-in-Data-Management-and-Explainable-Machine-Learning" class="headerlink" title="Attribution-Scores in Data Management and Explainable Machine Learning"></a>Attribution-Scores in Data Management and Explainable Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00184">http://arxiv.org/abs/2308.00184</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leopoldo Bertossi</li>
<li>for: 这篇论文主要探讨了使用实际 causality 定义责任分数的研究，以及这些分数在数据库和机器学习中的应用。</li>
<li>methods: 论文使用了数据库修复和分数的计算方法，以及一种量化度量数据库的一致性。</li>
<li>results: 论文提出了一种正确地扩展了分数，并在分类模型中应用了这种扩展。同时，论文还分析了 Shap-score 的高效计算方法。<details>
<summary>Abstract</summary>
We describe recent research on the use of actual causality in the definition of responsibility scores as explanations for query answers in databases, and for outcomes from classification models in machine learning. In the case of databases, useful connections with database repairs are illustrated and exploited. Repairs are also used to give a quantitative measure of the consistency of a database. For classification models, the responsibility score is properly extended and illustrated. The efficient computation of Shap-score is also analyzed and discussed. The emphasis is placed on work done by the author and collaborators.
</details>
<details>
<summary>摘要</summary>
我们描述了最近的研究，把实际 causality 引入定义责任分数的概念，以解释 queries 的答案在数据库中，以及机器学习模型中的结果。在数据库中，我们利用了有用的连接，并用修复来给出数据库的数量化度量。对于机器学习模型，我们正确地扩展了责任分数，并给出了相应的示例。我们还分析了计算 Shap 分数的高效策略。我们强调了作者和合作者的工作。
</details></li>
</ul>
<hr>
<h2 id="General-Anomaly-Detection-of-Underwater-Gliders-Validated-by-Large-scale-Deployment-Dataset"><a href="#General-Anomaly-Detection-of-Underwater-Gliders-Validated-by-Large-scale-Deployment-Dataset" class="headerlink" title="General Anomaly Detection of Underwater Gliders Validated by Large-scale Deployment Dataset"></a>General Anomaly Detection of Underwater Gliders Validated by Large-scale Deployment Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00180">http://arxiv.org/abs/2308.00180</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruochu Yang, Chad Lembke, Fumin Zhang, Catherine Edwards</li>
<li>for: 本研究使用异常检测算法来评估水下飞行器在不可预测的海洋环境中的正常运行。</li>
<li>methods: 本研究使用了实时alerts，使飞行器驾驶员在检测到异常情况时能够及时控制飞行器，避免进一步的损害。检测算法应用于大量的实际数据集，收集自Skidaway Institute of Oceanography（SkIO）和University of South Florida（USF）的实际飞行器部署。</li>
<li>results: 实验评估包括了线上和线下检测两种模式。线下检测使用完整的回收数据集，以提供精确的异常分析和与驾驶员日志进行比较。线上检测则是在飞行器在抵达事件时传输的实时数据 subsets，尽管实时数据可能不如完整的回收数据具有相同的信息量，但在线上检测对于实时监控异常情况非常重要。<details>
<summary>Abstract</summary>
This paper employs an anomaly detection algorithm to assess the normal operation of underwater gliders in unpredictable ocean environments. Real-time alerts can be provided to glider pilots upon detecting any anomalies, enabling them to assume control of the glider and prevent further harm. The detection algorithm is applied to abundant data sets collected in real glider deployments led by the Skidaway Institute of Oceanography (SkIO) and the University of South Florida (USF). Regarding generality, the experimental evaluation is composed of both offline and online detection modes. The offline detection utilizes full post-recovery data sets, which carries high-resolution information, to present detailed analysis of the anomaly and compare it with pilot logs. The online detection focuses on the real-time subsets of data transmitted from the glider at the surfacing events. While the real-time data may not contain as much rich information as the post-recovery data, the online detection is of great importance as it allows glider pilots to monitor potential abnormal conditions in real time.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:这篇论文使用异常检测算法来评估水下飞行器在不可预知的海洋环境中的正常运行。在检测到任何异常时，飞行器驾驶员可以接收实时警示，以避免更大的损害。检测算法应用于Skidaway Institute of Oceanography（SkIO）和University of South Florida（USF）在实际飞行器部署中收集的丰富数据集。在总体来说，实验评估包括了线上和线下检测模式。线上检测专注于在飞行器抵达事件时传输的实时数据 subsets，尽管实时数据可能不具备完整的信息，但是在线上检测对于实时监测异常情况非常重要。
</details></li>
</ul>
<hr>
<h2 id="Pretrained-deep-models-outperform-GBDTs-in-Learning-To-Rank-under-label-scarcity"><a href="#Pretrained-deep-models-outperform-GBDTs-in-Learning-To-Rank-under-label-scarcity" class="headerlink" title="Pretrained deep models outperform GBDTs in Learning-To-Rank under label scarcity"></a>Pretrained deep models outperform GBDTs in Learning-To-Rank under label scarcity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00177">http://arxiv.org/abs/2308.00177</a></li>
<li>repo_url: None</li>
<li>paper_authors: Charlie Hou, Kiran Koshy Thekumparampil, Michael Shavlovsky, Giulia Fanti, Yesh Dattatreya, Sujay Sanghavi</li>
<li>for: 这个论文主要研究了 whether unsupervised pretraining can improve Learning-To-Rank (LTR) 性能，以及如何使用 simple design choices 来实现这一目标。</li>
<li>methods: 论文使用了 SimCLR-Rank，一种修改后 SimCLR 方法，以进行 ranking-specific 的预训练。</li>
<li>results: 论文发现，预训练的深度学习模型可以Soundly 超过 GBDT 和其他非预训练模型在 label 数量比较多的情况下，并且预训练模型也常常在对异常数据进行排名时表现更好。<details>
<summary>Abstract</summary>
While deep learning (DL) models are state-of-the-art in text and image domains, they have not yet consistently outperformed Gradient Boosted Decision Trees (GBDTs) on tabular Learning-To-Rank (LTR) problems. Most of the recent performance gains attained by DL models in text and image tasks have used unsupervised pretraining, which exploits orders of magnitude more unlabeled data than labeled data. To the best of our knowledge, unsupervised pretraining has not been applied to the LTR problem, which often produces vast amounts of unlabeled data. In this work, we study whether unsupervised pretraining can improve LTR performance over GBDTs and other non-pretrained models. Using simple design choices--including SimCLR-Rank, our ranking-specific modification of SimCLR (an unsupervised pretraining method for images)--we produce pretrained deep learning models that soundly outperform GBDTs (and other non-pretrained models) in the case where labeled data is vastly outnumbered by unlabeled data. We also show that pretrained models also often achieve significantly better robustness than non-pretrained models (GBDTs or DL models) in ranking outlier data.
</details>
<details>
<summary>摘要</summary>
while deep learning (DL) models are state-of-the-art in text and image domains, they have not yet consistently outperformed Gradient Boosted Decision Trees (GBDTs) on tabular Learning-To-Rank (LTR) problems. most of the recent performance gains attained by DL models in text and image tasks have used unsupervised pretraining, which exploits orders of magnitude more unlabeled data than labeled data. to the best of our knowledge, unsupervised pretraining has not been applied to the LTR problem, which often produces vast amounts of unlabeled data. in this work, we study whether unsupervised pretraining can improve LTR performance over GBDTs and other non-pretrained models. using simple design choices--including SimCLR-Rank, our ranking-specific modification of SimCLR (an unsupervised pretraining method for images)--we produce pretrained deep learning models that soundly outperform GBDTs (and other non-pretrained models) in the case where labeled data is vastly outnumbered by unlabeled data. we also show that pretrained models also often achieve significantly better robustness than non-pretrained models (GBDTs or DL models) in ranking outlier data.Here is the translation in Traditional Chinese:而深度学习（DL）模型在文本和图像领域是现今最佳的，但它们尚未一致地超越Gradient Boosted Decision Trees（GBDT）在排序学习（LTR）问题上。大多数最近的表现提升都是由DL模型在文本和图像任务中使用无监督预训，这些预训可以处理数量为数十倍于标签数据。根据我们所知，无监督预训尚未应用于LTR问题，LTR问题通常生成了巨量的无标签数据。在这个工作中，我们研究了是否可以透过无监督预训提高LTR性能，比较GBDT和其他非预训模型。我们还使用了简单的设计选择，包括我们的排名具体化SimCLR-Rank，将SimCLR（对于图像的无监督预训方法） Ranking化来提高排名性能。我们产生了预训深度学习模型，与GBDT和其他非预训模型相比，在标签数据远远多于无标签数据时表现出色。我们还表明了预训模型通常在排名偏差数据时表现更好的响应性。
</details></li>
</ul>
<hr>
<h2 id="A-Flow-Artist-for-High-Dimensional-Cellular-Data"><a href="#A-Flow-Artist-for-High-Dimensional-Cellular-Data" class="headerlink" title="A Flow Artist for High-Dimensional Cellular Data"></a>A Flow Artist for High-Dimensional Cellular Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00176">http://arxiv.org/abs/2308.00176</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kincaid MacDonald, Dhananjay Bhaskar, Guy Thampakkul, Nhi Nguyen, Joia Zhang, Michael Perlmutter, Ian Adelstein, Smita Krishnaswamy</li>
<li>for: 该论文旨在为高维数据中的点云数据样本提供嵌入，并利用相关的流速信息。</li>
<li>methods: 该论文提出了一种基于神经网络的嵌入方法，可以同时学习点云中的坐标和速度信息。</li>
<li>results: 该论文在测试数据集和单个细胞RNA速度数据上表现出了更好地分离和可视化高维数据中的速度相关结构。<details>
<summary>Abstract</summary>
We consider the problem of embedding point cloud data sampled from an underlying manifold with an associated flow or velocity. Such data arises in many contexts where static snapshots of dynamic entities are measured, including in high-throughput biology such as single-cell transcriptomics. Existing embedding techniques either do not utilize velocity information or embed the coordinates and velocities independently, i.e., they either impose velocities on top of an existing point embedding or embed points within a prescribed vector field. Here we present FlowArtist, a neural network that embeds points while jointly learning a vector field around the points. The combination allows FlowArtist to better separate and visualize velocity-informed structures. Our results, on toy datasets and single-cell RNA velocity data, illustrate the value of utilizing coordinate and velocity information in tandem for embedding and visualizing high-dimensional data.
</details>
<details>
<summary>摘要</summary>
我团队考虑了嵌入点云数据，该数据来自于下面的拓扑结构，并且有关联的流速信息。这类数据在高通量生物学中经常出现，例如单细胞肽谱分析。现有的嵌入技术 Either do not utilize velocity information or embed points and velocities independently, i.e., they either impose velocities on top of an existing point embedding or embed points within a prescribed vector field. Here we present FlowArtist, a neural network that embeds points while jointly learning a vector field around the points. The combination allows FlowArtist to better separate and visualize velocity-informed structures. Our results, on toy datasets and single-cell RNA velocity data, illustrate the value of utilizing coordinate and velocity information in tandem for embedding and visualizing high-dimensional data.Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Federated-Learning-for-Data-and-Model-Heterogeneity-in-Medical-Imaging"><a href="#Federated-Learning-for-Data-and-Model-Heterogeneity-in-Medical-Imaging" class="headerlink" title="Federated Learning for Data and Model Heterogeneity in Medical Imaging"></a>Federated Learning for Data and Model Heterogeneity in Medical Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00155">http://arxiv.org/abs/2308.00155</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hussain Ahmad Madni, Rao Muhammad Umer, Gian Luca Foresti</li>
<li>for: 这个研究目的是为了解决 Federated Learning (FL) 中的数据和模型多样性问题，以提高 FL 的效率。</li>
<li>methods: 本研究使用了知识传播和对称损失来解决模型和数据多样性问题。</li>
<li>results: 实验结果显示，提案的方法在医疗数据上比较其他现有方法更有优势。<details>
<summary>Abstract</summary>
Federated Learning (FL) is an evolving machine learning method in which multiple clients participate in collaborative learning without sharing their data with each other and the central server. In real-world applications such as hospitals and industries, FL counters the challenges of data heterogeneity and model heterogeneity as an inevitable part of the collaborative training. More specifically, different organizations, such as hospitals, have their own private data and customized models for local training. To the best of our knowledge, the existing methods do not effectively address both problems of model heterogeneity and data heterogeneity in FL. In this paper, we exploit the data and model heterogeneity simultaneously, and propose a method, MDH-FL (Exploiting Model and Data Heterogeneity in FL) to solve such problems to enhance the efficiency of the global model in FL. We use knowledge distillation and a symmetric loss to minimize the heterogeneity and its impact on the model performance. Knowledge distillation is used to solve the problem of model heterogeneity, and symmetric loss tackles with the data and label heterogeneity. We evaluate our method on the medical datasets to conform the real-world scenario of hospitals, and compare with the existing methods. The experimental results demonstrate the superiority of the proposed approach over the other existing methods.
</details>
<details>
<summary>摘要</summary>
协同学习（Federated Learning，FL）是一种发展中的机器学习方法，其中多个客户端参与共同学习而无需分享他们的数据。在实际应用中，如医院和产业，FL 可以解决数据多样性和模型多样性作为共同训练的不可避免的一部分。更 Specifically, different organizations, such as hospitals, have their own private data and customized models for local training. To the best of our knowledge, the existing methods do not effectively address both problems of model heterogeneity and data heterogeneity in FL. In this paper, we exploit the data and model heterogeneity simultaneously, and propose a method, MDH-FL (Exploiting Model and Data Heterogeneity in FL) to solve such problems to enhance the efficiency of the global model in FL. We use knowledge distillation and a symmetric loss to minimize the heterogeneity and its impact on the model performance. Knowledge distillation is used to solve the problem of model heterogeneity, and symmetric loss tackles with the data and label heterogeneity. We evaluate our method on medical datasets to conform the real-world scenario of hospitals, and compare with the existing methods. The experimental results demonstrate the superiority of the proposed approach over the other existing methods.
</details></li>
</ul>
<hr>
<h2 id="DiffusAL-Coupling-Active-Learning-with-Graph-Diffusion-for-Label-Efficient-Node-Classification"><a href="#DiffusAL-Coupling-Active-Learning-with-Graph-Diffusion-for-Label-Efficient-Node-Classification" class="headerlink" title="DiffusAL: Coupling Active Learning with Graph Diffusion for Label-Efficient Node Classification"></a>DiffusAL: Coupling Active Learning with Graph Diffusion for Label-Efficient Node Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00146">http://arxiv.org/abs/2308.00146</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lmu-dbs/diffusal">https://github.com/lmu-dbs/diffusal</a></li>
<li>paper_authors: Sandra Gilhuber, Julian Busch, Daniel Rotthues, Christian M. M. Frey, Thomas Seidl</li>
<li>for: 这篇论文的目的是为了提出一种新的活况graph学习方法，以提高label效率和转移性。</li>
<li>methods: 这篇论文使用了三种独立的评估函数来选择最有价值的节点，包括Model Uncertainty、Diversity Component和Node Importance，这些评估函数都是通过传播规律来计算。</li>
<li>results: 实验结果显示，这篇论文的方法在不同的benchmark数据集上比随机选择更高，并且在100%的数据集和标签预算下表现更好。<details>
<summary>Abstract</summary>
Node classification is one of the core tasks on attributed graphs, but successful graph learning solutions require sufficiently labeled data. To keep annotation costs low, active graph learning focuses on selecting the most qualitative subset of nodes that maximizes label efficiency. However, deciding which heuristic is best suited for an unlabeled graph to increase label efficiency is a persistent challenge. Existing solutions either neglect aligning the learned model and the sampling method or focus only on limited selection aspects. They are thus sometimes worse or only equally good as random sampling. In this work, we introduce a novel active graph learning approach called DiffusAL, showing significant robustness in diverse settings. Toward better transferability between different graph structures, we combine three independent scoring functions to identify the most informative node samples for labeling in a parameter-free way: i) Model Uncertainty, ii) Diversity Component, and iii) Node Importance computed via graph diffusion heuristics. Most of our calculations for acquisition and training can be pre-processed, making DiffusAL more efficient compared to approaches combining diverse selection criteria and similarly fast as simpler heuristics. Our experiments on various benchmark datasets show that, unlike previous methods, our approach significantly outperforms random selection in 100% of all datasets and labeling budgets tested.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。<</SYS>>节点分类是 attributed graphs 的核心任务，但是成功的图学解决方案需要具备充分的标签数据。为了降低标签成本，活跃图学学习强调选择最优质量的节点子集，以最大化标签效率。然而，为无标签图选择最佳的选择方法是一个长期的挑战。现有的解决方案可能会忽略对学习模型和采样方法的对齐，或者只关注有限的选择方面。这些方法有时会比随机采样更差，或者只等于随机采样。在这种情况下，我们介绍了一种新的活跃图学学习方法，称为DiffusAL，它在多种场景下显示了明显的稳定性。为了更好地转移不同图结构之间的知识，我们将三种独立的评价函数组合在一起，以无参数的方式标识最有用的节点样本 для标签：i) 模型不确定性，ii) 多样性分量，和 iii) 节点重要性通过图 diffusion 规则来计算。大多数我们的获取和训练计算可以预处理，使得DiffusAL比 combining 多种选择 criterion 和同样快速的方法更高效。我们在多个 benchmark 数据集上进行了实验，发现与前一代方法不同，我们的方法在所有测试 dataset 和标签预算中都能够明显超越随机选择。
</details></li>
</ul>
<hr>
<h2 id="Formally-Explaining-Neural-Networks-within-Reactive-Systems"><a href="#Formally-Explaining-Neural-Networks-within-Reactive-Systems" class="headerlink" title="Formally Explaining Neural Networks within Reactive Systems"></a>Formally Explaining Neural Networks within Reactive Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00143">http://arxiv.org/abs/2308.00143</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shahaf Bassan, Guy Amir, Davide Corsi, Idan Refaeli, Guy Katz</li>
<li>for: 这种研究旨在解释深度神经网络（DNNs）控制的反应系统中的行为。</li>
<li>methods: 该研究提出了一种基于验证的可解释AI（XAI）技术，可以准确地指出引入了哪些输入特征导致DNN行为如此。</li>
<li>results: 研究人员提出了一种有效地计算简洁解释的方法，利用系统的转移约束来缩小检查空间，并在两个流行的自动导航 benchmark 上证明了该方法的高效性和可靠性。<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) are increasingly being used as controllers in reactive systems. However, DNNs are highly opaque, which renders it difficult to explain and justify their actions. To mitigate this issue, there has been a surge of interest in explainable AI (XAI) techniques, capable of pinpointing the input features that caused the DNN to act as it did. Existing XAI techniques typically face two limitations: (i) they are heuristic, and do not provide formal guarantees that the explanations are correct; and (ii) they often apply to ``one-shot'' systems, where the DNN is invoked independently of past invocations, as opposed to reactive systems. Here, we begin bridging this gap, and propose a formal DNN-verification-based XAI technique for reasoning about multi-step, reactive systems. We suggest methods for efficiently calculating succinct explanations, by exploiting the system's transition constraints in order to curtail the search space explored by the underlying verifier. We evaluate our approach on two popular benchmarks from the domain of automated navigation; and observe that our methods allow the efficient computation of minimal and minimum explanations, significantly outperforming the state of the art. We also demonstrate that our methods produce formal explanations that are more reliable than competing, non-verification-based XAI techniques.
</details>
<details>
<summary>摘要</summary>
In this paper, we aim to bridge this gap by proposing a formal DNN-verification-based XAI technique for reasoning about multi-step, reactive systems. Our approach leverages the system's transition constraints to efficiently calculate succinct explanations, reducing the search space explored by the underlying verifier. We evaluate our method on two popular benchmarks from the domain of automated navigation and show that our approach produces minimal and minimum explanations that are significantly more efficient than the state of the art. We also demonstrate that our methods produce formal explanations that are more reliable than competing, non-verification-based XAI techniques.我们的研究目标是 bridging the gap between deep neural networks (DNNs) 的 opacity 和 its application in reactive systems. DNNs 是 increasingly being used as controllers in reactive systems, but their high opacity makes it difficult to explain and justify their actions. To address this issue, there has been a surge of interest in explainable AI (XAI) techniques that can pinpoint the input features responsible for the DNN's actions. However, existing XAI techniques have two limitations: they are heuristic and do not provide formal guarantees of correctness, and they are typically designed for "one-shot" systems where the DNN is invoked independently of past invocations, rather than reactive systems.我们的方法是使用 DNN 的验证来提供正式的 XAI 技术，用于理解多步、反应式系统中 DNN 的行为。我们的方法利用系统的转换约束来高效计算简洁的解释，从而减少了底层验证器所探索的搜索空间。我们在 automatized navigation 领域的两个popular benchmark上评估了我们的方法，并观察到我们的方法可以生成高效的最小和最小解释，远胜过现状。我们还证明了我们的方法生成的正式解释比非验证基于 XAI 技术更可靠。
</details></li>
</ul>
<hr>
<h2 id="Semi-Supervised-Laplacian-Learning-on-Stiefel-Manifolds"><a href="#Semi-Supervised-Laplacian-Learning-on-Stiefel-Manifolds" class="headerlink" title="Semi-Supervised Laplacian Learning on Stiefel Manifolds"></a>Semi-Supervised Laplacian Learning on Stiefel Manifolds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00142">http://arxiv.org/abs/2308.00142</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chester Holtz, Pengwen Chen, Alexander Cloninger, Chung-Kuan Cheng, Gal Mishne</li>
<li>for: addressing the degeneracy of canonical Laplace learning algorithms in low label rates</li>
<li>methods: reformulating graph-based semi-supervised learning as a nonconvex generalization of a Trust-Region Subproblem (TRS)</li>
<li>results: achieving lower classification error compared to recent state-of-the-art and classical semi-supervised learning methods at extremely low, medium, and high label ratesHere’s the full Chinese text:</li>
<li>for: 用于解决laplace学习算法在低标签率下的异常性问题</li>
<li>methods: 将图基于 semi-supervised learning 转换为非凸泛化 Trust-Region Subproblem（TRS）</li>
<li>results: 在极低、中等和高标签率下 achieved 更低的分类错误率，比之前的状态作准和传统 semi-supervised learning 方法I hope that helps!<details>
<summary>Abstract</summary>
Motivated by the need to address the degeneracy of canonical Laplace learning algorithms in low label rates, we propose to reformulate graph-based semi-supervised learning as a nonconvex generalization of a \emph{Trust-Region Subproblem} (TRS). This reformulation is motivated by the well-posedness of Laplacian eigenvectors in the limit of infinite unlabeled data. To solve this problem, we first show that a first-order condition implies the solution of a manifold alignment problem and that solutions to the classical \emph{Orthogonal Procrustes} problem can be used to efficiently find good classifiers that are amenable to further refinement. Next, we address the criticality of selecting supervised samples at low-label rates. We characterize informative samples with a novel measure of centrality derived from the principal eigenvectors of a certain submatrix of the graph Laplacian. We demonstrate that our framework achieves lower classification error compared to recent state-of-the-art and classical semi-supervised learning methods at extremely low, medium, and high label rates. Our code is available on github\footnote{anonymized for submission}.
</details>
<details>
<summary>摘要</summary>
“受到低标签率下 Laplace 学习算法的衰弱问题的启发，我们提议将图基 semi-supervised learning  reformulate为非对称泛化的 Trust-Region Subproblem（TRS）。这种 reformulation 受到无穷多个标签数据下 Laplacian 埃文方程的定理性的启发。为解这个问题，我们首先证明了一个第一个条件是拟合 manifold 问题的解，并且我们用 classical 的 orthogonal Procrustes 问题的解来高效地找到可以进一步精细化的好分类器。然后，我们关注低标签率下选择监督样本的敏感性。我们使用一种新的中心性度量，基于图 Laplacian 的子矩阵的主要特征向量来Characterize 有用的样本。我们证明了我们的框架在 extremely low、medium 和 high 标签率下的分类错误率比现有的 semi-supervised learning 方法和古典方法低。我们的代码可以在 GitHub 上找到（anonymized for submission）。”Note: The translation is in Simplified Chinese, which is one of the two standard forms of Chinese writing. The other form is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="A-Suite-of-Fairness-Datasets-for-Tabular-Classification"><a href="#A-Suite-of-Fairness-Datasets-for-Tabular-Classification" class="headerlink" title="A Suite of Fairness Datasets for Tabular Classification"></a>A Suite of Fairness Datasets for Tabular Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00133">http://arxiv.org/abs/2308.00133</a></li>
<li>repo_url: None</li>
<li>paper_authors: Martin Hirzel, Michael Feffer</li>
<li>for: 该论文目的是提高机器学习分类器的公平性。</li>
<li>methods: 该论文使用了一些算法来提高机器学习分类器的公平性，并提供了一些函数来抓取20个公平性数据集和相关的公平性metadata。</li>
<li>results: 该论文通过提供大量的公平性数据集和metadata，希望能够促进未来的公平性意识机器学习研究的更加严格的实验评估。<details>
<summary>Abstract</summary>
There have been many papers with algorithms for improving fairness of machine-learning classifiers for tabular data. Unfortunately, most use only very few datasets for their experimental evaluation. We introduce a suite of functions for fetching 20 fairness datasets and providing associated fairness metadata. Hopefully, these will lead to more rigorous experimental evaluations in future fairness-aware machine learning research.
</details>
<details>
<summary>摘要</summary>
有很多论文提出了对机器学习分类器的公平性进行改进的算法。然而，大多数使用的数据集很少。我们介绍了一个函数集，用于获取20个公平性数据集和相关的公平性元数据。希望这些函数可以促进未来的公平意识机器学习研究的更加严格的实验评估。Here's the word-for-word translation:有很多论文提出了对机器学习分类器的公平性进行改进的算法。然而，大多数使用的数据集很少。我们介绍了一个函数集，用于获取20个公平性数据集和相关的公平性元数据。希望这些函数可以促进未来的公平意识机器学习研究的更加严格的实验评估。
</details></li>
</ul>
<hr>
<h2 id="Ensemble-Learning-with-Residual-Transformer-for-Brain-Tumor-Segmentation"><a href="#Ensemble-Learning-with-Residual-Transformer-for-Brain-Tumor-Segmentation" class="headerlink" title="Ensemble Learning with Residual Transformer for Brain Tumor Segmentation"></a>Ensemble Learning with Residual Transformer for Brain Tumor Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00128">http://arxiv.org/abs/2308.00128</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lanhong Yao, Zheyuan Zhang, Ulas Bagci</li>
<li>for: 本研究旨在提高脑肿瘤分割的精度，因为现有的 U-Net 架构受到复杂形态和文本化肿瘤的限制，以及 pixel-wise 标注的失败。</li>
<li>methods: 本研究提出一种 integrate  transformer 和 U-Net 的新网络架构，通过自适应的混合来提取3D 体积上的上下文信息，并添加 residual connection 以避免信息流失。</li>
<li>results: 在 BraTS 2021 数据集上（3D），我们的模型实现了87.6% 的 Mean Dice 分数，超过了当前最佳方法，示ifying 将多种架构结合使用可以优化脑肿瘤分割。<details>
<summary>Abstract</summary>
Brain tumor segmentation is an active research area due to the difficulty in delineating highly complex shaped and textured tumors as well as the failure of the commonly used U-Net architectures. The combination of different neural architectures is among the mainstream research recently, particularly the combination of U-Net with Transformers because of their innate attention mechanism and pixel-wise labeling. Different from previous efforts, this paper proposes a novel network architecture that integrates Transformers into a self-adaptive U-Net to draw out 3D volumetric contexts with reasonable computational costs. We further add a residual connection to prevent degradation in information flow and explore ensemble methods, as the evaluated models have edges on different cases and sub-regions. On the BraTS 2021 dataset (3D), our model achieves 87.6% mean Dice score and outperforms the state-of-the-art methods, demonstrating the potential for combining multiple architectures to optimize brain tumor segmentation.
</details>
<details>
<summary>摘要</summary>
脑肿 segmentation是一个活跃的研究领域，因为区分高度复杂形态和文本化肿瘤具有极大的挑战。通常使用的U-Net架构也有失败情况。近年来， combining多种神经架构是主流的研究方向，特别是将Transformers搭配U-Net，因为它们的自然注意机制和像素级标注。与之前的尝试不同，本文提出了一种新的网络架构，将Transformers integrate到自适应U-Net中，以实现3D积体上下文的可靠计算。此外，我们还添加了径向连接，以避免信息流失并explore ensemble方法，因为评估模型在不同的情况和子区域上具有优势。在BraTS 2021 dataset（3D）上，我们的模型实现了87.6%的mean Dice分数，超越了当前的状态艺术方法，这表明将多种架构组合起来可以优化脑肿分 segmentation。
</details></li>
</ul>
<hr>
<h2 id="DiviML-A-Module-based-Heuristic-for-Mapping-Neural-Networks-onto-Heterogeneous-Platforms"><a href="#DiviML-A-Module-based-Heuristic-for-Mapping-Neural-Networks-onto-Heterogeneous-Platforms" class="headerlink" title="DiviML: A Module-based Heuristic for Mapping Neural Networks onto Heterogeneous Platforms"></a>DiviML: A Module-based Heuristic for Mapping Neural Networks onto Heterogeneous Platforms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00127">http://arxiv.org/abs/2308.00127</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yassine Ghannane, Mohamed S. Abdelfattah</li>
<li>For: The paper is written to address the challenge of compiler-level partitioning of deep neural networks (DNNs) onto multiple interconnected hardware devices in heterogeneous datacenters.* Methods: The paper proposes a general framework for heterogeneous DNN compilation, which includes automatic partitioning and device mapping, as well as a scheduler that integrates both an exact solver and a modularity-based heuristic for scalability.* Results: The paper shows that the proposed framework can achieve more than 3 times lower latency and up to 2.9 times higher throughput compared to naively running DNNs on the fastest GPU, by automatically leveraging both data and model parallelism to deploy DNNs on a heterogeneous system comprised of a CPU and two distinct GPUs. Additionally, the paper demonstrates the effectiveness of the proposed modularity-based “splitting” heuristic, which improves the solution runtime up to 395 times without sacrificing solution quality compared to an exact MILP solution.<details>
<summary>Abstract</summary>
Datacenters are increasingly becoming heterogeneous, and are starting to include specialized hardware for networking, video processing, and especially deep learning. To leverage the heterogeneous compute capability of modern datacenters, we develop an approach for compiler-level partitioning of deep neural networks (DNNs) onto multiple interconnected hardware devices. We present a general framework for heterogeneous DNN compilation, offering automatic partitioning and device mapping. Our scheduler integrates both an exact solver, through a mixed integer linear programming (MILP) formulation, and a modularity-based heuristic for scalability. Furthermore, we propose a theoretical lower bound formula for the optimal solution, which enables the assessment of the heuristic solutions' quality. We evaluate our scheduler in optimizing both conventional DNNs and randomly-wired neural networks, subject to latency and throughput constraints, on a heterogeneous system comprised of a CPU and two distinct GPUs. Compared to na\"ively running DNNs on the fastest GPU, he proposed framework can achieve more than 3$\times$ times lower latency and up to 2.9$\times$ higher throughput by automatically leveraging both data and model parallelism to deploy DNNs on our sample heterogeneous server node. Moreover, our modularity-based "splitting" heuristic improves the solution runtime up to 395$\times$ without noticeably sacrificing solution quality compared to an exact MILP solution, and outperforms all other heuristics by 30-60% solution quality. Finally, our case study shows how we can extend our framework to schedule large language models across multiple heterogeneous servers by exploiting symmetry in the hardware setup. Our code can be easily plugged in to existing frameworks, and is available at https://github.com/abdelfattah-lab/diviml.
</details>
<details>
<summary>摘要</summary>
现代数据中心越来越多样化，包括特циализирован的网络硬件、视频处理硬件和深度学习硬件。为了利用现代数据中心的多样化计算能力，我们开发了一种编译器级别的深度神经网络（DNNs）分配方法，并提供自动分配和设备映射。我们的排程器 integrate both an exact solver（通过混合integer linear programming（MILP）表示）和一个基于模块性的优化器，以提高可扩展性。此外，我们提出了一个理论下界方程，可以评估优化解的质量。我们在一个包括CPU和两种不同GPU的多元环境中进行了优化。与直接运行DNNs在最快的GPU上相比，我们的框架可以实现更多于3倍的延迟和最多2.9倍的吞吐量，通过自动利用数据和模型平行性来部署DNNs。此外，我们的模块性"splitting"优化器可以提高解 runtime up to 395倍，而无需明显降低解质量相比于精确MILP解，并且在其他优化器中提高30-60%的解质量。最后，我们的案例研究显示了如何使用对硬件设置的 симметry 来扩展我们的框架，以部署大型自然语言模型 across multiple heterogeneous servers。我们的代码可以轻松地插入到现有框架中，并可以在https://github.com/abdelfattah-lab/diviml 中获得。
</details></li>
</ul>
<hr>
<h2 id="Convolutional-Occupancy-Models-for-Dense-Packing-of-Complex-Novel-Objects"><a href="#Convolutional-Occupancy-Models-for-Dense-Packing-of-Complex-Novel-Objects" class="headerlink" title="Convolutional Occupancy Models for Dense Packing of Complex, Novel Objects"></a>Convolutional Occupancy Models for Dense Packing of Complex, Novel Objects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00091">http://arxiv.org/abs/2308.00091</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nikhilmishra000/fcon">https://github.com/nikhilmishra000/fcon</a></li>
<li>paper_authors: Nikhil Mishra, Pieter Abbeel, Xi Chen, Maximilian Sieb</li>
<li>for:  dense packing in pick-and-place systems for warehouse and logistics applications</li>
<li>methods:  fully-convolutional shape completion model (F-CON) combined with off-the-shelf planning methods</li>
<li>results:  substantially better dense packing than other shape completion methods in real-world cluttered scenes<details>
<summary>Abstract</summary>
Dense packing in pick-and-place systems is an important feature in many warehouse and logistics applications. Prior work in this space has largely focused on planning algorithms in simulation, but real-world packing performance is often bottlenecked by the difficulty of perceiving 3D object geometry in highly occluded, partially observed scenes. In this work, we present a fully-convolutional shape completion model, F-CON, which can be easily combined with off-the-shelf planning methods for dense packing in the real world. We also release a simulated dataset, COB-3D-v2, that can be used to train shape completion models for real-word robotics applications, and use it to demonstrate that F-CON outperforms other state-of-the-art shape completion methods. Finally, we equip a real-world pick-and-place system with F-CON, and demonstrate dense packing of complex, unseen objects in cluttered scenes. Across multiple planning methods, F-CON enables substantially better dense packing than other shape completion methods.
</details>
<details>
<summary>摘要</summary>
dense packing在选择和放置系统中是重要的特点，在仓库和物流应用中具有广泛的应用。先前的工作主要集中在仿真中的规划算法上，但实际的填充性常被受到三维物体干扰、部分观察场景的困难所压制。在这种情况下，我们提出了一种易于组合的完全卷积形成模型，F-CON，可以与市场上 readily available 的规划方法结合使用，以实现在实际世界中的高密度填充。我们还发布了COB-3D-v2 simulated dataset，可以用于训练形成模型，并使用其来证明F-CON在实际世界中的表现比其他状态空间中的形成方法更好。最后，我们在实际的选择和放置系统上装备了F-CON，并在拥挤的场景中实现了复杂、未看到的物体的高密度填充。通过不同的规划方法，F-CON在其他形成方法中实现了许多更好的高密度填充。
</details></li>
</ul>
<hr>
<h2 id="New-Lower-Bounds-for-Testing-Monotonicity-and-Log-Concavity-of-Distributions"><a href="#New-Lower-Bounds-for-Testing-Monotonicity-and-Log-Concavity-of-Distributions" class="headerlink" title="New Lower Bounds for Testing Monotonicity and Log Concavity of Distributions"></a>New Lower Bounds for Testing Monotonicity and Log Concavity of Distributions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00089">http://arxiv.org/abs/2308.00089</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuqian Cheng, Daniel M. Kane, Zhicheng Zheng</li>
<li>for: 该论文是为了证明分布测试下界的技术而写的。</li>
<li>methods: 该论文使用的方法是构造一对匹配分布的家族，其中一个家族保持定义不等式的条件，而另一个家族违反这些不等式。</li>
<li>results: 该论文通过这种方法获得了新的下界，包括 monotonicity testing over discrete cubes 和 log-concavity testing 的紧张下界。<details>
<summary>Abstract</summary>
We develop a new technique for proving distribution testing lower bounds for properties defined by inequalities involving the bin probabilities of the distribution in question. Using this technique we obtain new lower bounds for monotonicity testing over discrete cubes and tight lower bounds for log-concavity testing.   Our basic technique involves constructing a pair of moment-matching families of distributions by tweaking the probabilities of pairs of bins so that one family maintains the defining inequalities while the other violates them.
</details>
<details>
<summary>摘要</summary>
我们开发了一种新的技术，用于证明分布测试下界限的方法，该方法基于分布中每个分布的概率的不等式定义的性质。使用这种技术，我们获得了新的下界限 для升序测试和紧密的下界限 для对排斜度测试。  我们的基本技术是建立一对匹配时间的分布家族，通过修改每个分布中的概率对的概率来使一家分布满足定义的不等式，而另一家分布则违反这些不等式。
</details></li>
</ul>
<hr>
<h2 id="A-Novel-Deep-Learning-based-Model-to-Defend-Network-Intrusion-Detection-System-against-Adversarial-Attacks"><a href="#A-Novel-Deep-Learning-based-Model-to-Defend-Network-Intrusion-Detection-System-against-Adversarial-Attacks" class="headerlink" title="A Novel Deep Learning based Model to Defend Network Intrusion Detection System against Adversarial Attacks"></a>A Novel Deep Learning based Model to Defend Network Intrusion Detection System against Adversarial Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00077">http://arxiv.org/abs/2308.00077</a></li>
<li>repo_url: None</li>
<li>paper_authors: Khushnaseeb Roshan, Aasim Zafar, Shiekh Burhan Ul Haque</li>
<li>for: 本研究旨在探讨深度学习基于网络入侵检测系统（NIDS）中的强大敌意攻击算法以及其防御策略。</li>
<li>methods: 本研究使用了四种强大敌意攻击算法：快速梯度签名方法（FGSM）、杠杆矩阵攻击（JSMA）、投影式梯度下降（PGD）以及加拿大和瓦格纳（C&amp;W）攻击。而防御策略则是使用了对抗训练来增强NIDS模型的可抗性。</li>
<li>results: 研究结果分为三个阶段：第一阶段是没有遭受攻击之前的结果，第二阶段是遭受攻击后的结果，第三阶段是经过对抗训练后的结果。使用了加拿大INSTIDS-2017数据集进行评估，并通过了不同的性能指标如f1-score、准确率等来评估模型的性能。<details>
<summary>Abstract</summary>
Network Intrusion Detection System (NIDS) is an essential tool in securing cyberspace from a variety of security risks and unknown cyberattacks. A number of solutions have been implemented for Machine Learning (ML), and Deep Learning (DL) based NIDS. However, all these solutions are vulnerable to adversarial attacks, in which the malicious actor tries to evade or fool the model by injecting adversarial perturbed examples into the system. The main aim of this research work is to study powerful adversarial attack algorithms and their defence method on DL-based NIDS. Fast Gradient Sign Method (FGSM), Jacobian Saliency Map Attack (JSMA), Projected Gradient Descent (PGD) and Carlini & Wagner (C&W) are four powerful adversarial attack methods implemented against the NIDS. As a defence method, Adversarial Training is used to increase the robustness of the NIDS model. The results are summarized in three phases, i.e., 1) before the adversarial attack, 2) after the adversarial attack, and 3) after the adversarial defence. The Canadian Institute for Cybersecurity Intrusion Detection System 2017 (CICIDS-2017) dataset is used for evaluation purposes with various performance measurements like f1-score, accuracy etc.
</details>
<details>
<summary>摘要</summary>
网络侵入检测系统（NIDS）是保护网络安全的重要工具，它可以检测到多种安全风险和未知的网络攻击。为了提高NIDS的准确率和鲁棒性，许多解决方案已经实施了机器学习（ML）和深度学习（DL）技术。然而，这些解决方案都受到了恶意攻击的威胁，攻击者可以通过注入恶意抗件来诱导模型出错。本研究的主要目标是研究强大的恶意攻击算法和其防御方法在DL-based NIDS中。fast gradient sign method（FGSM）、Jacobian saliency map attack（JSMA）、projected gradient descent（PGD）和Carlini & Wagner（C&W）是四种强大的恶意攻击方法，它们都是对NIDS模型的挑战。为了增强NIDS模型的鲁棒性，我们使用了对抗训练。研究结果分为三个阶段：在攻击之前、攻击后和防御后。我们使用了加拿大安全攻击检测系统2017（CICIDS-2017）数据集进行评估，并使用了各种性能指标如f1-score、准确率等来评估模型的表现。
</details></li>
</ul>
<hr>
<h2 id="Crowd-Safety-Manager-Towards-Data-Driven-Active-Decision-Support-for-Planning-and-Control-of-Crowd-Events"><a href="#Crowd-Safety-Manager-Towards-Data-Driven-Active-Decision-Support-for-Planning-and-Control-of-Crowd-Events" class="headerlink" title="Crowd Safety Manager: Towards Data-Driven Active Decision Support for Planning and Control of Crowd Events"></a>Crowd Safety Manager: Towards Data-Driven Active Decision Support for Planning and Control of Crowd Events</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00076">http://arxiv.org/abs/2308.00076</a></li>
<li>repo_url: None</li>
<li>paper_authors: Panchamy Krishnakumari, Sascha Hoogendoorn-Lanser, Jeroen Steenbakkers, Serge Hoogendoorn</li>
<li>For: The paper aims to enhance crowd management in both the planning and operational phases, using innovative data collection techniques, data integration, and visualization, as well as artificial intelligence (AI) tools for risk identification.* Methods: The paper introduces the Bowtie model, a comprehensive framework that assesses and predicts risk levels by combining objective estimations and predictions with various aggravating factors. The proposed framework is applied to the Crowd Safety Manager project in Scheveningen, using a wealth of real-time data sources, including Resono, which provides insights into the number of visitors and their movements. Advanced machine learning techniques, such as XGBoost, are used for forecasting.* Results: The results indicate that the predictions are adequately accurate, but certain locations may benefit from additional input data to further enhance prediction quality. Despite these limitations, the work contributes to a more effective crowd management system and opens avenues for further advancements in this critical field.Here’s the Chinese translation of the three points:* For: 这篇论文目的是提高人群管理的规划和运行阶段，使用创新的数据采集技术、数据 интеграción和可视化，以及人工智能（AI）工具来确定风险水平。* Methods: 论文介绍了一种名为“环”模型，该模型将对象估计和预测与多种累加因素相结合，以评估预计的风险水平。该模型在芝尼登的群体安全管理项目中应用，使用了大量实时数据源，包括Resono，以获取访客数量和移动征性的情况。文中使用了先进的机器学习技术，如XGBoost框架，进行预测。* Results: 结果表明预测结果具有足够的准确性，但certain location可能需要更多的输入数据来进一步提高预测质量。尽管有这些限制，这种工作仍然为人群管理系统做出了贡献，并开启了进一步提高这一领域的前iers。<details>
<summary>Abstract</summary>
This paper presents novel technology and methodology aimed at enhancing crowd management in both the planning and operational phases. The approach encompasses innovative data collection techniques, data integration, and visualization using a 3D Digital Twin, along with the incorporation of artificial intelligence (AI) tools for risk identification. The paper introduces the Bowtie model, a comprehensive framework designed to assess and predict risk levels. The model combines objective estimations and predictions, such as traffic flow operations and crowdedness levels, with various aggravating factors like weather conditions, sentiments, and the purpose of visitors, to evaluate the expected risk of incidents. The proposed framework is applied to the Crowd Safety Manager project in Scheveningen, where the DigiTwin is developed based on a wealth of real-time data sources. One noteworthy data source is Resono, offering insights into the number of visitors and their movements, leveraging a mobile phone panel of over 2 million users in the Netherlands. Particular attention is given to the left-hand side of the Bowtie, which includes state estimation, prediction, and forecasting. Notably, the focus is on generating multi-day ahead forecasts for event-planning purposes using Resono data. Advanced machine learning techniques, including the XGBoost framework, are compared, with XGBoost demonstrating the most accurate forecasts. The results indicate that the predictions are adequately accurate. However, certain locations may benefit from additional input data to further enhance prediction quality. Despite these limitations, this work contributes to a more effective crowd management system and opens avenues for further advancements in this critical field.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Using-Kernel-SHAP-XAI-Method-to-optimize-the-Network-Anomaly-Detection-Model"><a href="#Using-Kernel-SHAP-XAI-Method-to-optimize-the-Network-Anomaly-Detection-Model" class="headerlink" title="Using Kernel SHAP XAI Method to optimize the Network Anomaly Detection Model"></a>Using Kernel SHAP XAI Method to optimize the Network Anomaly Detection Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00074">http://arxiv.org/abs/2308.00074</a></li>
<li>repo_url: None</li>
<li>paper_authors: Khushnaseeb Roshan, Aasim Zafar</li>
<li>for: 本研究旨在使用Explainable Artificial Intelligence（XAI）技术检测和解释网络异常行为，以提高网络异常检测模型的准确率、回归率、精度和F1分数。</li>
<li>methods: 本研究使用kernelSHAP方法实现网络异常检测和解释。</li>
<li>results: 实验结果显示，使用OPT_Model（在无监督方式下训练）可以达到网络异常检测模型的高准确率和F1分数（准确率为0.90，F1分数为0.76）。<details>
<summary>Abstract</summary>
Anomaly detection and its explanation is important in many research areas such as intrusion detection, fraud detection, unknown attack detection in network traffic and logs. It is challenging to identify the cause or explanation of why one instance is an anomaly? and the other is not due to its unbounded and lack of supervisory nature. The answer to this question is possible with the emerging technique of explainable artificial intelligence (XAI). XAI provides tools and techniques to interpret and explain the output and working of complex models such as Deep Learning (DL). This paper aims to detect and explain network anomalies with XAI, kernelSHAP method. The same approach is used to improve the network anomaly detection model in terms of accuracy, recall, precision and f score. The experiment is conduced with the latest CICIDS2017 dataset. Two models are created (Model_1 and OPT_Model) and compared. The overall accuracy and F score of OPT_Model (when trained in unsupervised way) are 0.90 and 0.76, respectively.
</details>
<details>
<summary>摘要</summary>
anomaly detection 和其解释对许多研究领域都是重要的，如侵入检测、诈骗检测、未知攻击检测在网络流量和日志中。但是确定一个实例是异常的原因或解释是困难的，因为它们没有监督性和是无限的。这个问题的答案是可能的，通过新兴的解释人工智能（XAI）技术。XAI提供了解释和解释复杂模型如深度学习（DL）的工具和技术。这篇论文的目标是使用XAI技术检测和解释网络异常，并使用kernelSHAP方法进行改进。实验使用最新的CICIDS2017数据集。两个模型（Model_1和OPT_Model）被创建并比较。OPT_Model在无监督方式下训练时的总准确率和F分数分别为0.90和0.76。
</details></li>
</ul>
<hr>
<h2 id="T-Fusion-Net-A-Novel-Deep-Neural-Network-Augmented-with-Multiple-Localizations-based-Spatial-Attention-Mechanisms-for-Covid-19-Detection"><a href="#T-Fusion-Net-A-Novel-Deep-Neural-Network-Augmented-with-Multiple-Localizations-based-Spatial-Attention-Mechanisms-for-Covid-19-Detection" class="headerlink" title="T-Fusion Net: A Novel Deep Neural Network Augmented with Multiple Localizations based Spatial Attention Mechanisms for Covid-19 Detection"></a>T-Fusion Net: A Novel Deep Neural Network Augmented with Multiple Localizations based Spatial Attention Mechanisms for Covid-19 Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00053">http://arxiv.org/abs/2308.00053</a></li>
<li>repo_url: None</li>
<li>paper_authors: Susmita Ghosh, Abhiroop Chatterjee</li>
<li>for: 这篇论文的目的是提出一种新的深度神经网络（称为T-Fusion Net），以增强图像分类任务的性能。</li>
<li>methods: 该网络使用多个本地化的空间注意 Mechanism，让网络专注于重要的图像区域，提高了其描述力。 homogeneous ensemble of the said network 是用来进一步增强图像分类精度。</li>
<li>results: 实验结果显示，提案的T-Fusion Net和homogeneous ensemble model在COVID-19（SARS-CoV-2 CT scan） dataset上表现出色，与其他现有的方法相比，获得了97.59%和98.4%的准确率。<details>
<summary>Abstract</summary>
In recent years, deep neural networks are yielding better performance in image classification tasks. However, the increasing complexity of datasets and the demand for improved performance necessitate the exploration of innovative techniques. The present work proposes a new deep neural network (called as, T-Fusion Net) that augments multiple localizations based spatial attention. This attention mechanism allows the network to focus on relevant image regions, improving its discriminative power. A homogeneous ensemble of the said network is further used to enhance image classification accuracy. For ensembling, the proposed approach considers multiple instances of individual T-Fusion Net. The model incorporates fuzzy max fusion to merge the outputs of individual nets. The fusion process is optimized through a carefully chosen parameter to strike a balance on the contributions of the individual models. Experimental evaluations on benchmark Covid-19 (SARS-CoV-2 CT scan) dataset demonstrate the effectiveness of the proposed T-Fusion Net as well as its ensemble. The proposed T-Fusion Net and the homogeneous ensemble model exhibit better performance, as compared to other state-of-the-art methods, achieving accuracy of 97.59% and 98.4%, respectively.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Reinforcement-Learning-for-Generative-AI-State-of-the-Art-Opportunities-and-Open-Research-Challenges"><a href="#Reinforcement-Learning-for-Generative-AI-State-of-the-Art-Opportunities-and-Open-Research-Challenges" class="headerlink" title="Reinforcement Learning for Generative AI: State of the Art, Opportunities and Open Research Challenges"></a>Reinforcement Learning for Generative AI: State of the Art, Opportunities and Open Research Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00031">http://arxiv.org/abs/2308.00031</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giorgio Franceschelli, Mirco Musolesi</li>
<li>for: 本文概述了在生成人工智能领域应用强化学习的现状、机遇和未解决问题。</li>
<li>methods: 本文讨论了三种应用方式，即使RL作为生成无特定目标的替代方法、同时Maximize一个函数的生成输出方法、以及通过嵌入desired特性来捕捉不易由目标函数捕捉的生成过程。</li>
<li>results: 本文结束于对这个吸引人的新兴领域的机遇和挑战的深入讨论。<details>
<summary>Abstract</summary>
Generative Artificial Intelligence (AI) is one of the most exciting developments in Computer Science of the last decade. At the same time, Reinforcement Learning (RL) has emerged as a very successful paradigm for a variety of machine learning tasks. In this survey, we discuss the state of the art, opportunities and open research questions in applying RL to generative AI. In particular, we will discuss three types of applications, namely, RL as an alternative way for generation without specified objectives; as a way for generating outputs while concurrently maximizing an objective function; and, finally, as a way of embedding desired characteristics, which cannot be easily captured by means of an objective function, into the generative process. We conclude the survey with an in-depth discussion of the opportunities and challenges in this fascinating emerging area.
</details>
<details>
<summary>摘要</summary>
人工智能创新（AI）在计算机科学领域最近一代最吸引人的发展之一。同时，对rl（强化学习）的应用也在不断演化。在这篇评论中，我们讨论RL在生成AI中的状态、机会和未解决的问题。特别是，我们将讨论RL作为生成无明确目标的替代方法，RL作为同时最大化目标函数的输出生成方法，以及RL作为嵌入需要精准捕捉的特征的生成过程中的方法。我们将结束这篇评论 avec一个深入的讨论在这一领域中的机会和挑战。
</details></li>
</ul>
<hr>
<h2 id="Conformal-PID-Control-for-Time-Series-Prediction"><a href="#Conformal-PID-Control-for-Time-Series-Prediction" class="headerlink" title="Conformal PID Control for Time Series Prediction"></a>Conformal PID Control for Time Series Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16895">http://arxiv.org/abs/2307.16895</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aangelopoulos/conformal-time-series">https://github.com/aangelopoulos/conformal-time-series</a></li>
<li>paper_authors: Anastasios N. Angelopoulos, Emmanuel J. Candes, Ryan J. Tibshirani</li>
<li>for: 这个论文的目的是提供一些可靠的时间序列预测算法，并为这些算法提供正式的保证。</li>
<li>methods: 这个论文使用了准确预测、控制理论和在线学习等方法，并能够在线预测具有系统性错误的时间序列中的准确分数。</li>
<li>results: 实验表明，使用这些算法可以提高对COVID-19死亡人数的4周前预测覆盖率，以及预测电力需求、股票市场返现和气温的准确率。<details>
<summary>Abstract</summary>
We study the problem of uncertainty quantification for time series prediction, with the goal of providing easy-to-use algorithms with formal guarantees. The algorithms we present build upon ideas from conformal prediction and control theory, are able to prospectively model conformal scores in an online setting, and adapt to the presence of systematic errors due to seasonality, trends, and general distribution shifts. Our theory both simplifies and strengthens existing analyses in online conformal prediction. Experiments on 4-week-ahead forecasting of statewide COVID-19 death counts in the U.S. show an improvement in coverage over the ensemble forecaster used in official CDC communications. We also run experiments on predicting electricity demand, market returns, and temperature using autoregressive, Theta, Prophet, and Transformer models. We provide an extendable codebase for testing our methods and for the integration of new algorithms, data sets, and forecasting rules.
</details>
<details>
<summary>摘要</summary>
我们研究时间序列预测不确定性评估问题，目的是提供有正式保证的易用算法。我们的算法基于准确预测和控制理论的想法，可在线上预测充分分配概率分布，并适应系统性错误的变化，如季节性、趋势和总分布变化。我们的理论简化了和加强了现有的在线准确预测分析。我们在美国州级COVID-19死亡人数预测4个星期前的实验中显示了覆盖率的提高，并在预测电力需求、股票市场回报和温度等方面进行了实验。我们提供了可扩展的代码库，用于测试我们的方法和新算法、数据集和预测规则的集成。
</details></li>
</ul>
<hr>
<h2 id="Predicting-masked-tokens-in-stochastic-locations-improves-masked-image-modeling"><a href="#Predicting-masked-tokens-in-stochastic-locations-improves-masked-image-modeling" class="headerlink" title="Predicting masked tokens in stochastic locations improves masked image modeling"></a>Predicting masked tokens in stochastic locations improves masked image modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00566">http://arxiv.org/abs/2308.00566</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amir Bar, Florian Bordes, Assaf Shocher, Mahmoud Assran, Pascal Vincent, Nicolas Ballas, Trevor Darrell, Amir Globerson, Yann LeCun</li>
<li>for: 这篇论文旨在提高自动学习中的Masked Image Modeling（MIM）表现，以提高下游任务的性能。</li>
<li>methods: 本文提出了FlexPredict，一种随机推几个几个掩盖的token位置来导引模型学习更加响应于位置不确定性的方法。</li>
<li>results: 相比MIM基eline，FlexPredict在ImageNet直线探针中提高了1.6%的性能，并在半指示性视频分类中提高了2.5%的性能。<details>
<summary>Abstract</summary>
Self-supervised learning is a promising paradigm in deep learning that enables learning from unlabeled data by constructing pretext tasks that require learning useful representations. In natural language processing, the dominant pretext task has been masked language modeling (MLM), while in computer vision there exists an equivalent called Masked Image Modeling (MIM). However, MIM is challenging because it requires predicting semantic content in accurate locations. E.g, given an incomplete picture of a dog, we can guess that there is a tail, but we cannot determine its exact location. In this work, we propose FlexPredict, a stochastic model that addresses this challenge by incorporating location uncertainty into the model. Specifically, we condition the model on stochastic masked token positions to guide the model toward learning features that are more robust to location uncertainties. Our approach improves downstream performance on a range of tasks, e.g, compared to MIM baselines, FlexPredict boosts ImageNet linear probing by 1.6% with ViT-B and by 2.5% for semi-supervised video segmentation using ViT-L.
</details>
<details>
<summary>摘要</summary>
自我监督学习是深度学习中有前途的潜在方法，它允许通过构建预测任务来学习无标记数据中的有用表示。在自然语言处理中，主要的预测任务是遮盖语言模型（MLM），而在计算机视觉中则有相应的equivalent，即遮盖图像模型（MIM）。然而，MIM具有困难的特点，即需要准确预测 semantic content的位置。例如，给定一幅缺失的狗图片，我们可以预测有尾巴，但是无法准确地确定其位置。在这项工作中，我们提出了FlexPredict，一种随机模型，解决这个挑战。我们通过conditioning模型的掩码位置来引导模型学习更加Robust的特征。我们的方法可以提高下游任务的性能，例如，相比MIM基eline，FlexPredict在ImageNet线性探测中提高了1.6%的ViT-B和2.5%的半supervised видео分割使用ViT-L。
</details></li>
</ul>
<hr>
<h2 id="Foundational-Models-for-Fault-Diagnosis-of-Electrical-Motors"><a href="#Foundational-Models-for-Fault-Diagnosis-of-Electrical-Motors" class="headerlink" title="Foundational Models for Fault Diagnosis of Electrical Motors"></a>Foundational Models for Fault Diagnosis of Electrical Motors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16891">http://arxiv.org/abs/2307.16891</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sriram Anbalagan, Deepesh Agarwal, Balasubramaniam Natarajan, Babji Srinivasan</li>
<li>for: 本研究旨在提高电动机故障诊断方法的可靠性和效率，适用于实际应用场景中的各种运行条件和机器。</li>
<li>methods: 该研究提出了一种基于神经网络自我supervised learning的框架，包括建立一个高级特征学习的神经网络后ION，并在不同的运行条件和机器上进行细化调整。</li>
<li>results: 实验结果表明，该方法可以在不同的故障情况和运行条件下达到高于90%的分类精度，并且可以在不同的机器上进行跨机器的故障诊断任务。<details>
<summary>Abstract</summary>
A majority of recent advancements related to the fault diagnosis of electrical motors are based on the assumption that training and testing data are drawn from the same distribution. However, the data distribution can vary across different operating conditions during real-world operating scenarios of electrical motors. Consequently, this assumption limits the practical implementation of existing studies for fault diagnosis, as they rely on fully labelled training data spanning all operating conditions and assume a consistent distribution. This is because obtaining a large number of labelled samples for several machines across different fault cases and operating scenarios may be unfeasible. In order to overcome the aforementioned limitations, this work proposes a framework to develop a foundational model for fault diagnosis of electrical motors. It involves building a neural network-based backbone to learn high-level features using self-supervised learning, and then fine-tuning the backbone to achieve specific objectives. The primary advantage of such an approach is that the backbone can be fine-tuned to achieve a wide variety of target tasks using very less amount of training data as compared to traditional supervised learning methodologies. The empirical evaluation demonstrates the effectiveness of the proposed approach by obtaining more than 90\% classification accuracy by fine-tuning the backbone not only across different types of fault scenarios or operating conditions, but also across different machines. This illustrates the promising potential of the proposed approach for cross-machine fault diagnosis tasks in real-world applications.
</details>
<details>
<summary>摘要</summary>
大多数最近的电动机故障诊断技术的进步都基于假设训练和测试数据来自同一个分布。然而，在实际应用中的电动机运行场景中，数据分布可能会随着不同的操作条件而变化。这种假设限制了现有的研究实施，因为它们需要完全标注的训练数据，并且假设数据分布是一致的。这可能是因为获得许多标注的样本数据 для几台机器 across 多种故障情况和操作条件可能是不可能的。为了突破这些限制，该工作提出了一个框架，用于开发电动机故障诊断的基本模型。它包括使用神经网络基础模型来学习高级特征，然后使用自动适应学习来细化基础模型以实现特定目标。这种方法的优点是，只需要少量的训练数据，可以通过细化基础模型来实现各种目标任务。实际评估表明，提议的方法可以在不同的故障情况和操作条件下，以及不同的机器上，实现高于90%的分类精度。这说明了提议的方法在实际应用中的扩展性和可靠性。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Model-the-World-with-Language"><a href="#Learning-to-Model-the-World-with-Language" class="headerlink" title="Learning to Model the World with Language"></a>Learning to Model the World with Language</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01399">http://arxiv.org/abs/2308.01399</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/OpenKP">https://github.com/microsoft/OpenKP</a></li>
<li>paper_authors: Jessy Lin, Yuqing Du, Olivia Watkins, Danijar Hafner, Pieter Abbeel, Dan Klein, Anca Dragan</li>
<li>for: 这个论文的目的是建立一种可以理解多种语言、关联语言与视觉世界并根据语言进行行动的 Agent。</li>
<li>methods: 这个论文使用了一种基于未来预测的自动学习目标，将语言理解和未来预测联系起来，以便帮助 Agent 更好地理解多种语言并根据语言进行行动。</li>
<li>results: 在这个论文中， authors 提出了一种名为 Dynalang 的 Agent，该 Agent 可以从多种语言中获得丰富的语言理解，并通过预测未来语言、视频和奖励来学习行动。 Dynalang 可以在多种环境中进行学习，包括在线交互环境以及无动作和奖励的 dataset 上。<details>
<summary>Abstract</summary>
To interact with humans in the world, agents need to understand the diverse types of language that people use, relate them to the visual world, and act based on them. While current agents learn to execute simple language instructions from task rewards, we aim to build agents that leverage diverse language that conveys general knowledge, describes the state of the world, provides interactive feedback, and more. Our key idea is that language helps agents predict the future: what will be observed, how the world will behave, and which situations will be rewarded. This perspective unifies language understanding with future prediction as a powerful self-supervised learning objective. We present Dynalang, an agent that learns a multimodal world model that predicts future text and image representations and learns to act from imagined model rollouts. Unlike traditional agents that use language only to predict actions, Dynalang acquires rich language understanding by using past language also to predict future language, video, and rewards. In addition to learning from online interaction in an environment, Dynalang can be pretrained on datasets of text, video, or both without actions or rewards. From using language hints in grid worlds to navigating photorealistic scans of homes, Dynalang utilizes diverse types of language to improve task performance, including environment descriptions, game rules, and instructions.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:为了与人类交互在世界中，代理需要理解人类使用的多种语言，与视觉世界相关，并基于其行动。当前的代理只是通过任务奖励学习简单的语言指令，而我们的关键想法是让语言帮助代理预测未来：未来会观察什么，世界会如何行为，哪些情况将得到奖励。这种视角将语言理解与未来预测作为强大的自主学习目标统一起来。我们介绍了 Dynalang，一个学习多模态世界模型，预测未来文本和图像表示，并从抽象模型演练中学习行为。不同于传统的代理只用语言预测动作，Dynalang通过过去语言还预测未来语言、视频和奖励来获得丰富的语言理解。除了在环境中学习在线交互外，Dynalang还可以通过文本、视频或两者的无动作或奖励数据预training。从使用语言提示在格子世界中到探索高级扫描图像的家庭，Dynalang利用多种语言来提高任务表现，包括环境描述、游戏规则和指令。
</details></li>
</ul>
<hr>
<h2 id="Discovering-Adaptable-Symbolic-Algorithms-from-Scratch"><a href="#Discovering-Adaptable-Symbolic-Algorithms-from-Scratch" class="headerlink" title="Discovering Adaptable Symbolic Algorithms from Scratch"></a>Discovering Adaptable Symbolic Algorithms from Scratch</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16890">http://arxiv.org/abs/2307.16890</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stephen Kelly, Daniel S. Park, Xingyou Song, Mitchell McIntire, Pranav Nashikkar, Ritam Guha, Wolfgang Banzhaf, Kalyanmoy Deb, Vishnu Naresh Boddeti, Jie Tan, Esteban Real</li>
<li>for: 该论文旨在开发一种快速适应环境变化的自主机器人控制策略。</li>
<li>methods: 该方法基于AutoML-Zero，通过演化零批量学习的模型参数和推理算法来快速适应环境变化。</li>
<li>results: 实验表明，该方法可以在实际 quadruped robot上提供安全的控制策略，并在突然环境变化时表现出较高的稳定性和可靠性。<details>
<summary>Abstract</summary>
Autonomous robots deployed in the real world will need control policies that rapidly adapt to environmental changes. To this end, we propose AutoRobotics-Zero (ARZ), a method based on AutoML-Zero that discovers zero-shot adaptable policies from scratch. In contrast to neural network adaption policies, where only model parameters are optimized, ARZ can build control algorithms with the full expressive power of a linear register machine. We evolve modular policies that tune their model parameters and alter their inference algorithm on-the-fly to adapt to sudden environmental changes. We demonstrate our method on a realistic simulated quadruped robot, for which we evolve safe control policies that avoid falling when individual limbs suddenly break. This is a challenging task in which two popular neural network baselines fail. Finally, we conduct a detailed analysis of our method on a novel and challenging non-stationary control task dubbed Cataclysmic Cartpole. Results confirm our findings that ARZ is significantly more robust to sudden environmental changes and can build simple, interpretable control policies.
</details>
<details>
<summary>摘要</summary>
自适应 robots 在实际世界中部署时需要快速适应环境变化的控制策略。为此，我们提出了 AutoRobotics-Zero（ARZ）方法，基于 AutoML-Zero 方法，可以从头开始找到零shot适应策略。与神经网络适应策略相比，ARZ 可以建立一个完整的线性注册机器的控制算法，并可以在不间断的环境变化中进行适应。我们演化出模块化策略，可以在运行时调整模型参数和推理算法，以适应突然的环境变化。我们在模拟的四肢 робот上进行了实验，并成功演化出安全的控制策略，以避免当各个肢体突然失效时的倒下。这是一个具有挑战性的任务，两种流行的神经网络基elines都失败了。最后，我们对我们的方法进行了详细的分析，并在一个新的和挑战性的非站台控制任务中进行了实验。结果证明了我们的发现，ARZ 在突然环境变化中更加鲁棒，可以建立简单、可读的控制策略。
</details></li>
</ul>
<hr>
<h2 id="Virtual-Prompt-Injection-for-Instruction-Tuned-Large-Language-Models"><a href="#Virtual-Prompt-Injection-for-Instruction-Tuned-Large-Language-Models" class="headerlink" title="Virtual Prompt Injection for Instruction-Tuned Large Language Models"></a>Virtual Prompt Injection for Instruction-Tuned Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16888">http://arxiv.org/abs/2307.16888</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jun Yan, Vikas Yadav, Shiyang Li, Lichang Chen, Zheng Tang, Hai Wang, Vijay Srinivasan, Xiang Ren, Hongxia Jin</li>
<li>for: 这个论文主要是为了漏洞披露 Large Language Models (LLMs) 的 instruction-tuning 数据，以实现恶意控制模型行为。</li>
<li>methods: 论文使用的方法是在模型的 instruction-tuning 数据中杀器投入恶意指令，以实现不正常的模型行为。</li>
<li>results: 论文的实验结果表明，只需杀器投入 0.1% 的恶意示例，就可以让模型对有关 Joe Biden 的查询返回负面的回答，从而导致模型被恶意控制。<details>
<summary>Abstract</summary>
We present Virtual Prompt Injection (VPI) for instruction-tuned Large Language Models (LLMs). VPI allows an attacker-specified virtual prompt to steer the model behavior under specific trigger scenario without any explicit injection in model input. For instance, if an LLM is compromised with the virtual prompt "Describe Joe Biden negatively." for Joe Biden-related instructions, then any service deploying this model will propagate biased views when handling user queries related to Joe Biden. VPI is especially harmful for two primary reasons. Firstly, the attacker can take fine-grained control over LLM behaviors by defining various virtual prompts, exploiting LLMs' proficiency in following instructions. Secondly, this control is achieved without any interaction from the attacker while the model is in service, leading to persistent attack. To demonstrate the threat, we propose a simple method for performing VPI by poisoning the model's instruction tuning data. We find that our proposed method is highly effective in steering the LLM with VPI. For example, by injecting only 52 poisoned examples (0.1% of the training data size) into the instruction tuning data, the percentage of negative responses given by the trained model on Joe Biden-related queries change from 0% to 40%. We thus highlight the necessity of ensuring the integrity of the instruction-tuning data as little poisoned data can cause stealthy and persistent harm to the deployed model. We further explore the possible defenses and identify data filtering as an effective way to defend against the poisoning attacks. Our project page is available at https://poison-llm.github.io.
</details>
<details>
<summary>摘要</summary>
我们介绍了一个名为“虚拟提示插入”（VPI）的攻击技术，可以让攻击者在特定的触发情况下，透过虚拟提示来控制大语言模型（LLM）的行为，而不需要直接对模型的输入进行插入。例如，如果一个LLM被攻击者感染了虚拟提示“谈论乔·宾登的负面”，那么任何使用这个模型的服务都会传播偏见的观点，当用户提出有关乔·宾登的问题时。VPI有两个主要的危险因素：首先，攻击者可以通过定义不同的虚拟提示来精细地控制LLM的行为，滥用LLM的专业能力。第二，这种控制是在服务模型时，并不需要攻击者和模型之间的互动，因此是一种持续的攻击。为了证明这个威胁，我们提出了一个简单的方法来进行VPI，利用损害模型的调教数据。我们发现，仅将52个恶意示例（0.1%的训练数据大小）混入模型的调教数据中，可以将模型透过VPI控制。例如，将52个恶意示例混入调教数据中，将 Joe Biden 相关的查询中的负面回应率从0%提升至40%。这表明了调教数据的可信性的重要性，因为只需少量恶意数据可以导致隐藏和持续的伤害。我们还评估了可能的防护方法，发现数据范围化是一个有效的防护方法。我们的项目页面可以在https://poison-llm.github.io/中找到。
</details></li>
</ul>
<hr>
<h2 id="MetaCAM-Ensemble-Based-Class-Activation-Map"><a href="#MetaCAM-Ensemble-Based-Class-Activation-Map" class="headerlink" title="MetaCAM: Ensemble-Based Class Activation Map"></a>MetaCAM: Ensemble-Based Class Activation Map</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16863">http://arxiv.org/abs/2307.16863</a></li>
<li>repo_url: None</li>
<li>paper_authors: Emily Kaczmarek, Olivier X. Miguel, Alexa C. Bowie, Robin Ducharme, Alysha L. J. Dingwall-Harvey, Steven Hawken, Christine M. Armour, Mark C. Walker, Kevin Dick</li>
<li>For:  This paper aims to improve the performance of Class Activation Maps (CAMs) for explaining the predictions of deep learning models, specifically in high-criticality fields like medicine and biometric identification.* Methods: The paper proposes an ensemble-based method called MetaCAM, which combines multiple existing CAM methods using the consensus of the top-k% most highly activated pixels across component CAMs. The paper also introduces a new method called Cumulative Residual Effect (CRE) for summarizing large-scale ensemble-based experiments. Additionally, the paper proposes adaptive thresholding to improve the performance of individual CAMs.* Results: The paper shows that MetaCAM outperforms existing CAMs and refines the most salient regions of images used for model predictions. In a specific example, MetaCAM improved the performance of the pixel perturbation method Remove and Debias (ROAD) to 0.393 compared to the range of -0.101 to 0.172 for 11 individual CAMs.<details>
<summary>Abstract</summary>
The need for clear, trustworthy explanations of deep learning model predictions is essential for high-criticality fields, such as medicine and biometric identification. Class Activation Maps (CAMs) are an increasingly popular category of visual explanation methods for Convolutional Neural Networks (CNNs). However, the performance of individual CAMs depends largely on experimental parameters such as the selected image, target class, and model. Here, we propose MetaCAM, an ensemble-based method for combining multiple existing CAM methods based on the consensus of the top-k% most highly activated pixels across component CAMs. We perform experiments to quantifiably determine the optimal combination of 11 CAMs for a given MetaCAM experiment. A new method denoted Cumulative Residual Effect (CRE) is proposed to summarize large-scale ensemble-based experiments. We also present adaptive thresholding and demonstrate how it can be applied to individual CAMs to improve their performance, measured using pixel perturbation method Remove and Debias (ROAD). Lastly, we show that MetaCAM outperforms existing CAMs and refines the most salient regions of images used for model predictions. In a specific example, MetaCAM improved ROAD performance to 0.393 compared to 11 individual CAMs with ranges from -0.101-0.172, demonstrating the importance of combining CAMs through an ensembling method and adaptive thresholding.
</details>
<details>
<summary>摘要</summary>
需要清晰、可信度高的深度学习模型预测解释是高 kriticality 领域，如医学和生物ometrics 中的必备。类Activation Map (CAM) 是 CNN 中的一种增加 popular 的视觉解释方法。然而，具体实验参数，如选择的图像、目标类和模型，对各个 CAM 的性能产生很大影响。在这里，我们提出了 MetaCAM，一种基于 consensus 的多个现有 CAM 方法的 ensemble 方法，通过选择顶层 k% 最高活跃像素来确定最佳结果。我们对 MetaCAM 实验进行了量化的探索，并提出了一种新的 Cumulative Residual Effect (CRE) 方法来总结大规模的 ensemble 实验。此外，我们还介绍了适应阈值，并证明可以应用到各个 CAM 中来提高其性能， measured 使用像素变化方法 Remove 和 Debias (ROAD)。最后，我们证明 MetaCAM 超过了现有 CAM 和适应阈值，并提高了模型预测中使用的图像的最佳区域。例如，在 MetaCAM 实验中，ROAD 性能提高到 0.393，比11个个 CAM 的范围从 -0.101-0.172 高，这表明将 CAM 合并到 ensemble 方法和适应阈值中可以提高性能。
</details></li>
</ul>
<hr>
<h2 id="Towards-Trustworthy-and-Aligned-Machine-Learning-A-Data-centric-Survey-with-Causality-Perspectives"><a href="#Towards-Trustworthy-and-Aligned-Machine-Learning-A-Data-centric-Survey-with-Causality-Perspectives" class="headerlink" title="Towards Trustworthy and Aligned Machine Learning: A Data-centric Survey with Causality Perspectives"></a>Towards Trustworthy and Aligned Machine Learning: A Data-centric Survey with Causality Perspectives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16851">http://arxiv.org/abs/2307.16851</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoyang Liu, Maheep Chaudhary, Haohan Wang<br>for:This paper is written to provide a comprehensive review of the recent advancements in trustworthy machine learning, focusing on the challenges posed by the data and the methods developed to address these challenges.methods:The paper uses a data-centric perspective to review the various techniques developed in the field of trustworthy machine learning, including those inspired by the causality literature. These techniques are connected using a unified set of concepts and mathematical vocabulary.results:The paper provides a unified understanding of the field of trustworthy machine learning, connecting the various techniques developed in different subfields. It also explores the trustworthiness of large pretrained models and draws connections between these models and the standard empirical risk minimization (ERM) method. The paper offers a brief summary of the applications of these methods and discusses potential future aspects related to the survey.<details>
<summary>Abstract</summary>
The trustworthiness of machine learning has emerged as a critical topic in the field, encompassing various applications and research areas such as robustness, security, interpretability, and fairness. The last decade saw the development of numerous methods addressing these challenges. In this survey, we systematically review these advancements from a data-centric perspective, highlighting the shortcomings of traditional empirical risk minimization (ERM) training in handling challenges posed by the data.   Interestingly, we observe a convergence of these methods, despite being developed independently across trustworthy machine learning subfields. Pearl's hierarchy of causality offers a unifying framework for these techniques. Accordingly, this survey presents the background of trustworthy machine learning development using a unified set of concepts, connects this language to Pearl's causal hierarchy, and finally discusses methods explicitly inspired by causality literature. We provide a unified language with mathematical vocabulary to link these methods across robustness, adversarial robustness, interpretability, and fairness, fostering a more cohesive understanding of the field.   Further, we explore the trustworthiness of large pretrained models. After summarizing dominant techniques like fine-tuning, parameter-efficient fine-tuning, prompting, and reinforcement learning with human feedback, we draw connections between them and the standard ERM. This connection allows us to build upon the principled understanding of trustworthy methods, extending it to these new techniques in large pretrained models, paving the way for future methods. Existing methods under this perspective are also reviewed.   Lastly, we offer a brief summary of the applications of these methods and discuss potential future aspects related to our survey. For more information, please visit http://trustai.one.
</details>
<details>
<summary>摘要</summary>
机器学习的可靠性问题在领域中得到了广泛关注，涵盖了多个应用和研究领域，如可靠性、安全性、可解释性和公平性。过去十年内，有多种方法被开发出来解决这些挑战。在这篇评论中，我们从数据中心的视角系统性地查询这些进步，指出传统的empirical risk minimization（ERM）训练在处理数据的挑战方面存在缺陷。有意思的是，我们发现这些方法尽管在不同的可靠机器学习子领域中独立进行开发，但它们却 converges。珍珠的 causality 隐含了这些技术。因此，本文使用珍珠的 causal 隐含提供了一种统一的语言，将这些方法连接到 Pearl 的 causal 层次结构中，并最终讨论这些方法在可靠机器学习领域的应用。我们提供一种统一的语言，其中包括数学术语，以连接这些方法，从 robustness、对抗性、可解释性和公平性等方面进行链接。这些链接使我们可以更好地理解这个领域。此外，我们还探讨了大型预训模型的可靠性。我们首先概括了现有的方法，如 fine-tuning、参数效率的 fine-tuning、提示和人工回归学习。然后，我们连接这些方法和标准 ERM，从而扩展了可靠方法的基础理解，应用到这些新的大型预训模型中。现有的方法也被评论。最后，我们 briefly 概述了这些方法的应用，并讨论了未来相关的方向。更多信息请参考 <http://trustai.one>。
</details></li>
</ul>
<hr>
<h2 id="A-Trajectory-K-Anonymity-Model-Based-on-Point-Density-and-Partition"><a href="#A-Trajectory-K-Anonymity-Model-Based-on-Point-Density-and-Partition" class="headerlink" title="A Trajectory K-Anonymity Model Based on Point Density and Partition"></a>A Trajectory K-Anonymity Model Based on Point Density and Partition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16849">http://arxiv.org/abs/2307.16849</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wanshu Yu, Haonan Shi, Hongyun Xu</li>
<li>for: 保护用户行迹数据隐私</li>
<li>methods: 基于点密度和分区（KPDP）模型，对行迹数据进行匿名化处理，防止重新标识攻击</li>
<li>results: 提出了一种基于Point Density和Partition（KPDP）模型的行迹匿名化方法，可以有效防止重新标识攻击，同时保持数据的实用性和执行速度。实验结果表明，提出的方法在实际数据集上具有显著的优势。<details>
<summary>Abstract</summary>
As people's daily life becomes increasingly inseparable from various mobile electronic devices, relevant service application platforms and network operators can collect numerous individual information easily. When releasing these data for scientific research or commercial purposes, users' privacy will be in danger, especially in the publication of spatiotemporal trajectory datasets. Therefore, to avoid the leakage of users' privacy, it is necessary to anonymize the data before they are released. However, more than simply removing the unique identifiers of individuals is needed to protect the trajectory privacy, because some attackers may infer the identity of users by the connection with other databases. Much work has been devoted to merging multiple trajectories to avoid re-identification, but these solutions always require sacrificing data quality to achieve the anonymity requirement. In order to provide sufficient privacy protection for users' trajectory datasets, this paper develops a study on trajectory privacy against re-identification attacks, proposing a trajectory K-anonymity model based on Point Density and Partition (KPDP). Our approach improves the existing trajectory generalization anonymization techniques regarding trajectory set partition preprocessing and trajectory clustering algorithms. It successfully resists re-identification attacks and reduces the data utility loss of the k-anonymized dataset. A series of experiments on a real-world dataset show that the proposed model has significant advantages in terms of higher data utility and shorter algorithm execution time than other existing techniques.
</details>
<details>
<summary>摘要</summary>
As people's daily life becomes increasingly inseparable from various mobile electronic devices, relevant service application platforms and network operators can easily collect numerous individual information. When releasing these data for scientific research or commercial purposes, users' privacy will be in danger, especially in the publication of spatiotemporal trajectory datasets. Therefore, to avoid the leakage of users' privacy, it is necessary to anonymize the data before they are released. However, simply removing the unique identifiers of individuals is not enough to protect the trajectory privacy, because some attackers may infer the identity of users by the connection with other databases. Much work has been devoted to merging multiple trajectories to avoid re-identification, but these solutions always require sacrificing data quality to achieve the anonymity requirement. In order to provide sufficient privacy protection for users' trajectory datasets, this paper develops a study on trajectory privacy against re-identification attacks, proposing a trajectory K-anonymity model based on Point Density and Partition (KPDP). Our approach improves the existing trajectory generalization anonymization techniques regarding trajectory set partition preprocessing and trajectory clustering algorithms. It successfully resists re-identification attacks and reduces the data utility loss of the k-anonymized dataset. A series of experiments on a real-world dataset show that the proposed model has significant advantages in terms of higher data utility and shorter algorithm execution time than other existing techniques.
</details></li>
</ul>
<hr>
<h2 id="Latent-Masking-for-Multimodal-Self-supervised-Learning-in-Health-Timeseries"><a href="#Latent-Masking-for-Multimodal-Self-supervised-Learning-in-Health-Timeseries" class="headerlink" title="Latent Masking for Multimodal Self-supervised Learning in Health Timeseries"></a>Latent Masking for Multimodal Self-supervised Learning in Health Timeseries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16847">http://arxiv.org/abs/2307.16847</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shohreh Deldari, Dimitris Spathis, Mohammad Malekzadeh, Fahim Kawsar, Flora Salim, Akhil Mathur</li>
<li>for: 本研究旨在提高生物医学时间序列数据的机器学习进步，因为现有的生物医学时间序列数据标注是有限的，这限制了机器学习模型的进步。</li>
<li>methods: 本研究提出了一种新的自主学习方法（CroSSL），该方法使用掩码中间表示来实现跨模态的学习，并使用跨模态聚合器将多个模态的表示融合成一个全局表示。</li>
<li>results: 对多种医疗时间序列数据进行测试， CroSSL 方法能够达到先前的自主学习技术和指导 benchmark 的性能水平，只需要 minimal 的标注数据。此外，我们还分析了不同掩码比例和策略的影响，以及模式的Robustness 性。<details>
<summary>Abstract</summary>
Limited availability of labeled data for machine learning on biomedical time-series hampers progress in the field. Self-supervised learning (SSL) is a promising approach to learning data representations without labels. However, current SSL methods require expensive computations for negative pairs and are designed for single modalities, limiting their versatility. To overcome these limitations, we introduce CroSSL (Cross-modal SSL). CroSSL introduces two novel concepts: masking intermediate embeddings from modality-specific encoders and aggregating them into a global embedding using a cross-modal aggregator. This enables the handling of missing modalities and end-to-end learning of cross-modal patterns without prior data preprocessing or time-consuming negative-pair sampling. We evaluate CroSSL on various multimodal time-series benchmarks, including both medical-grade and consumer biosignals. Our results demonstrate superior performance compared to previous SSL techniques and supervised benchmarks with minimal labeled data. We additionally analyze the impact of different masking ratios and strategies and assess the robustness of the learned representations to missing modalities. Overall, our work achieves state-of-the-art performance while highlighting the benefits of masking latent embeddings for cross-modal learning in temporal health data.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:因为生物医学时序数据的标注数据有限，机器学习领域的进步受到了限制。自动学习（SSL）是一种不需要标注数据的数据表示学习的有希望的方法。然而，现有的SSL方法需要费时的计算对应对和是单模态的，这限制了它们的多样化性。为了突破这些限制，我们介绍了CroSSL（跨Modal SSL）。CroSSL引入了两个新的概念：在特定编码器中隐藏中间嵌入，并将它们聚合成全局嵌入使用交叉模态聚合器。这使得可以处理缺失的模态，并实现端到端学习跨模态模式，不需要先行数据处理或费时的负对样本生成。我们对多种医疗和消费者生物信号 benchmark 进行了评估，结果表明 CroSSL 的性能比前一代 SSL 技术和监督标准更高，几乎没有需要大量标注数据。我们还分析了不同的遮盖比例和策略的影响，以及缺失模态对学习到的表示的稳定性。总的来说，我们的工作实现了状态机器学习领域的前景性，同时强调隐藏嵌入的权重对跨模态学习的益处。
</details></li>
</ul>
<hr>
<h2 id="Identification-of-Driving-Heterogeneity-using-Action-chains"><a href="#Identification-of-Driving-Heterogeneity-using-Action-chains" class="headerlink" title="Identification of Driving Heterogeneity using Action-chains"></a>Identification of Driving Heterogeneity using Action-chains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16843">http://arxiv.org/abs/2307.16843</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xue Yao, Simeon C. Calvert, Serge P. Hoogendoorn</li>
<li>for: 这种研究的目的是为了从动作链视角进行驾驶不同性的识别，以提高交通效率和安全性。</li>
<li>methods: 该研究使用了一种基于规则的分割技术，以及一个动作库，其中包含了各种驾驶行为模式的描述。此外，研究还使用了动作链概念和动作链过渡概率来评估驾驶不同性。</li>
<li>results: 研究使用了实际数据进行评估，并得到了有效地识别驾驶不同性的结果，包括个人 drivers和交通流体系之间的驾驶不同性。这些发现可以帮助开发更加准确的驾驶行为理论和交通流模型，从而提高交通效率和安全性。<details>
<summary>Abstract</summary>
Current approaches to identifying driving heterogeneity face challenges in capturing the diversity of driving characteristics and understanding the fundamental patterns from a driving behaviour mechanism standpoint. This study introduces a comprehensive framework for identifying driving heterogeneity from an Action-chain perspective. First, a rule-based segmentation technique that considers the physical meanings of driving behaviour is proposed. Next, an Action phase Library including descriptions of various driving behaviour patterns is created based on the segmentation findings. The Action-chain concept is then introduced by implementing Action phase transition probability, followed by a method for evaluating driving heterogeneity. Employing real-world datasets for evaluation, our approach effectively identifies driving heterogeneity for both individual drivers and traffic flow while providing clear interpretations. These insights can aid the development of accurate driving behaviour theory and traffic flow models, ultimately benefiting traffic performance, and potentially leading to aspects such as improved road capacity and safety.
</details>
<details>
<summary>摘要</summary>
当前驾驶异同检测方法面临 capture driving behavior 多样性和理解驾驶行为机制的基本模式的挑战。本研究提出了基于 Action-chain 视角的全面驾驶异同检测框架。首先，我们提出了基于驾驶行为物理含义的规则生成分割技术。然后，我们基于分割结果创建了驾驶行为模式库，并在该库中添加了各种驾驶行为模式的描述。接着，我们引入了 Action phase 过渡概率，并提出了评估驾驶异同性的方法。使用实际数据进行评估，我们的方法可以有效地检测个体驾驶异同性和交通流动中的异同性，并提供了明确的解释。这些发现可以帮助建立准确的驾驶行为理论和交通流动模型，从而提高交通性能和安全性。
</details></li>
</ul>
<hr>
<h2 id="Automated-COVID-19-CT-Image-Classification-using-Multi-head-Channel-Attention-in-Deep-CNN"><a href="#Automated-COVID-19-CT-Image-Classification-using-Multi-head-Channel-Attention-in-Deep-CNN" class="headerlink" title="Automated COVID-19 CT Image Classification using Multi-head Channel Attention in Deep CNN"></a>Automated COVID-19 CT Image Classification using Multi-head Channel Attention in Deep CNN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00715">http://arxiv.org/abs/2308.00715</a></li>
<li>repo_url: None</li>
<li>paper_authors: Susmita Ghosh, Abhiroop Chatterjee</li>
<li>for: 检测COVID-19 CT扫描图像的自动化分类</li>
<li>methods: 提出了一种基于深度学习的修改Xception模型，具有新设计的通道注意力机制和权重 globally average pooling，以提高特征提取并改善分类精度</li>
<li>results: 在一个广泛使用的COVID-19 CT扫描图像数据集上实现了非常好的准确率（96.99%），并证明了其比其他状态的技术更高效。<details>
<summary>Abstract</summary>
The rapid spread of COVID-19 has necessitated efficient and accurate diagnostic methods. Computed Tomography (CT) scan images have emerged as a valuable tool for detecting the disease. In this article, we present a novel deep learning approach for automated COVID-19 CT scan classification where a modified Xception model is proposed which incorporates a newly designed channel attention mechanism and weighted global average pooling to enhance feature extraction thereby improving classification accuracy. The channel attention module selectively focuses on informative regions within each channel, enabling the model to learn discriminative features for COVID-19 detection. Experiments on a widely used COVID-19 CT scan dataset demonstrate a very good accuracy of 96.99% and show its superiority to other state-of-the-art techniques. This research can contribute to the ongoing efforts in using artificial intelligence to combat current and future pandemics and can offer promising and timely solutions for efficient medical image analysis tasks.
</details>
<details>
<summary>摘要</summary>
快速蔓延的 COVID-19 病毒感染了全球，因此需要有效和准确的诊断方法。 computed Tomography（CT）扫描图像已成为推荐诊断 COVID-19 的有价值工具。在这篇文章中，我们提出了一种新的深度学习方法，用于自动分类 COVID-19 CT 扫描图像。我们修改了 Xception 模型，添加了一个新的通道注意力机制和权重global average pooling，以提高特征提取，从而提高分类精度。通道注意力模块可以选择每个通道中的有用区域，让模型学习 COVID-19 的特征。实验结果表明，我们的方法在一个常用的 COVID-19 CT 扫描图像集上达到了 96.99% 的准确率，超过了其他当前状态艺术技术。这些研究可以贡献到使用人工智能对抗当前和未来的流行病，并提供可靠的医疗图像分析任务的解决方案。
</details></li>
</ul>
<hr>
<h2 id="Recent-advancement-in-Disease-Diagnostic-using-machine-learning-Systematic-survey-of-decades-comparisons-and-challenges"><a href="#Recent-advancement-in-Disease-Diagnostic-using-machine-learning-Systematic-survey-of-decades-comparisons-and-challenges" class="headerlink" title="Recent advancement in Disease Diagnostic using machine learning: Systematic survey of decades, comparisons, and challenges"></a>Recent advancement in Disease Diagnostic using machine learning: Systematic survey of decades, comparisons, and challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01319">http://arxiv.org/abs/2308.01319</a></li>
<li>repo_url: None</li>
<li>paper_authors: Farzaneh Tajidini, Mohammad-Javad Kheiri</li>
<li>for: 本研究旨在探讨机器学习在计算机助手诊断中的应用。</li>
<li>methods: 本文使用机器学习算法来检测疾病，包括肝炎、糖尿病、肝病、登革热和心脏病。</li>
<li>results: 本文总结了各种机器学习技术和算法在疾病检测中的应用，并评估了这些方法的效果。<details>
<summary>Abstract</summary>
Computer-aided diagnosis (CAD), a vibrant medical imaging research field, is expanding quickly. Because errors in medical diagnostic systems might lead to seriously misleading medical treatments, major efforts have been made in recent years to improve computer-aided diagnostics applications. The use of machine learning in computer-aided diagnosis is crucial. A simple equation may result in a false indication of items like organs. Therefore, learning from examples is a vital component of pattern recognition. Pattern recognition and machine learning in the biomedical area promise to increase the precision of disease detection and diagnosis. They also support the decision-making process's objectivity. Machine learning provides a practical method for creating elegant and autonomous algorithms to analyze high-dimensional and multimodal bio-medical data. This review article examines machine-learning algorithms for detecting diseases, including hepatitis, diabetes, liver disease, dengue fever, and heart disease. It draws attention to the collection of machine learning techniques and algorithms employed in studying conditions and the ensuing decision-making process.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Getting-from-Generative-AI-to-Trustworthy-AI-What-LLMs-might-learn-from-Cyc"><a href="#Getting-from-Generative-AI-to-Trustworthy-AI-What-LLMs-might-learn-from-Cyc" class="headerlink" title="Getting from Generative AI to Trustworthy AI: What LLMs might learn from Cyc"></a>Getting from Generative AI to Trustworthy AI: What LLMs might learn from Cyc</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04445">http://arxiv.org/abs/2308.04445</a></li>
<li>repo_url: None</li>
<li>paper_authors: Doug Lenat, Gary Marcus</li>
<li>for: 这篇论文目的是探讨未来AI的发展和趋势，以及一种可靠的AI技术。</li>
<li>methods: 论文使用了大语模型（LLM）和受检验的知识和规则来推导出结论。</li>
<li>results: 论文提出了16个AI发展的理想目标，并讨论了一种替代方案，即通过受检验的知识和规则来教育AI，以实现可靠和可解释的AI技术。<details>
<summary>Abstract</summary>
Generative AI, the most popular current approach to AI, consists of large language models (LLMs) that are trained to produce outputs that are plausible, but not necessarily correct. Although their abilities are often uncanny, they are lacking in aspects of reasoning, leading LLMs to be less than completely trustworthy. Furthermore, their results tend to be both unpredictable and uninterpretable.   We lay out 16 desiderata for future AI, and discuss an alternative approach to AI which could theoretically address many of the limitations associated with current approaches: AI educated with curated pieces of explicit knowledge and rules of thumb, enabling an inference engine to automatically deduce the logical entailments of all that knowledge. Even long arguments produced this way can be both trustworthy and interpretable, since the full step-by-step line of reasoning is always available, and for each step the provenance of the knowledge used can be documented and audited. There is however a catch: if the logical language is expressive enough to fully represent the meaning of anything we can say in English, then the inference engine runs much too slowly. That's why symbolic AI systems typically settle for some fast but much less expressive logic, such as knowledge graphs. We describe how one AI system, Cyc, has developed ways to overcome that tradeoff and is able to reason in higher order logic in real time.   We suggest that any trustworthy general AI will need to hybridize the approaches, the LLM approach and more formal approach, and lay out a path to realizing that dream.
</details>
<details>
<summary>摘要</summary>
现代人工智能的主流方法是生成型AI，它通过大量的语言模型（LLM）训练来生成可能性强的输出，但并不一定是正确的。尽管它们的能力往往吸引人，但它们缺乏一定的推理能力，因此不够可靠。此外，它们的结果往往难以预测和解释。我们提出了16个愿景为未来AI，并讨论了一种可能解决当前方法的限制的替代方法：通过手动编辑的明确知识和规则来教育AI，使其推理引擎可以自动推导所有知识的逻辑推论。这种方法可以生成可靠并可解释的长Arguments，因为整个步骤的推理逻辑都可以被跟踪，并且每一步的知识来源可以被记录和审核。然而，存在一个问题：如果逻辑语言足够表达英语中的任何意义，那么推理引擎就会非常慢。因此，符号AI系统通常选择一些快速而但是较为有限的逻辑，如知识图。我们描述了一个AI系统——Cyc，如何超越这个负担，在实时内推理在高阶逻辑中。我们建议任何可靠的通用AI都需要混合这两种方法，并走出实现这个梦想的路径。
</details></li>
</ul>
<hr>
<h2 id="Changes-in-Policy-Preferences-in-German-Tweets-during-the-COVID-Pandemic"><a href="#Changes-in-Policy-Preferences-in-German-Tweets-during-the-COVID-Pandemic" class="headerlink" title="Changes in Policy Preferences in German Tweets during the COVID Pandemic"></a>Changes in Policy Preferences in German Tweets during the COVID Pandemic</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04444">http://arxiv.org/abs/2308.04444</a></li>
<li>repo_url: None</li>
<li>paper_authors: Felix Biessmann</li>
<li>for: The paper aims to quantify political preferences in online social media, specifically in response to COVID measures.</li>
<li>methods: The paper uses a novel data set of tweets with fine grained political preference annotations to train a text classification model.</li>
<li>results: The study finds that expression of political opinions increased in response to the COVID pandemic, with the majority of opinions falling into the categories of pro-welfare, pro-education, and pro-governmental administration efficiency.Here are the three points in Simplified Chinese text:</li>
<li>for: 这个论文目标是在社交媒体上量化政治意见，具体来说是在响应COVID措施的情况下。</li>
<li>methods: 这个论文使用一个新的 tweets 数据集，并对其进行精细的政治偏好标注。</li>
<li>results: 研究发现，COVID 大流行后，政治意见的表达增加了，主要集中在优先顺序、教育和政府管理效率等三个类别上。<details>
<summary>Abstract</summary>
Online social media have become an important forum for exchanging political opinions. In response to COVID measures citizens expressed their policy preferences directly on these platforms. Quantifying political preferences in online social media remains challenging: The vast amount of content requires scalable automated extraction of political preferences -- however fine grained political preference extraction is difficult with current machine learning (ML) technology, due to the lack of data sets. Here we present a novel data set of tweets with fine grained political preference annotations. A text classification model trained on this data is used to extract policy preferences in a German Twitter corpus ranging from 2019 to 2022. Our results indicate that in response to the COVID pandemic, expression of political opinions increased. Using a well established taxonomy of policy preferences we analyse fine grained political views and highlight changes in distinct political categories. These analyses suggest that the increase in policy preference expression is dominated by the categories pro-welfare, pro-education and pro-governmental administration efficiency. All training data and code used in this study are made publicly available to encourage other researchers to further improve automated policy preference extraction methods. We hope that our findings contribute to a better understanding of political statements in online social media and to a better assessment of how COVID measures impact political preferences.
</details>
<details>
<summary>摘要</summary>
在线社交媒体已经成为政治意见交流的重要 forum。对于 COVID 措施，公民直接在这些平台上表达了政策偏好。但量化在线社交媒体中的政治偏好仍然是挑战：大量内容需要批量自动提取政策偏好，但现有机器学习（ML）技术仍然无法实现精确的政策偏好提取。我们现在发表了一个罕见的推文标注政治偏好的数据集。我们使用这个数据集训练了一个文本分类模型，并在2019-2022年德国Twitter资料中提取了政策偏好。我们的结果显示，对 COVID 大流行的回应，表达政治意见的人数增加。使用一个已经稳定的政策偏好分类法，我们分析了细部政治观点，并发现具体政治类别中的改变。这些分析结果显示，增加政策偏好表达的主要类别是“护理”、“教育”和“政府管理效率”。我们公开提供了所有训练数据和代码，以便其他研究人员可以进一步改进自动政策偏好提取方法。我们希望我们的成果对政治声明在线社交媒体中的理解产生影响，并且对 COVID 措施对政策偏好的影响进行更好的评估。
</details></li>
</ul>
<hr>
<h2 id="Structural-Transfer-Learning-in-NL-to-Bash-Semantic-Parsers"><a href="#Structural-Transfer-Learning-in-NL-to-Bash-Semantic-Parsers" class="headerlink" title="Structural Transfer Learning in NL-to-Bash Semantic Parsers"></a>Structural Transfer Learning in NL-to-Bash Semantic Parsers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16795">http://arxiv.org/abs/2307.16795</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kyle Duffy, Satwik Bhattamishra, Phil Blunsom</li>
<li>for: 这篇论文主要针对的是大规模预训练在自然语言处理领域的进步，以及预训练数据集的设计方面的 pocoUnderstanding。</li>
<li>methods: 这篇论文提出了一种方法来获得自然语言到bashSemantic parsing任务（NLBash）的结构匹配度的量化理解。该方法在NLBash和自然语言到SQL之间的结构匹配度之间进行了研究，并发现了强大的结构匹配度。</li>
<li>results: 研究发现，预训练compute expended在英语到德语翻译任务中的变化不一定会导致NLBash中的semantic representations具有更强的传递性。<details>
<summary>Abstract</summary>
Large-scale pre-training has made progress in many fields of natural language processing, though little is understood about the design of pre-training datasets. We propose a methodology for obtaining a quantitative understanding of structural overlap between machine translation tasks. We apply our methodology to the natural language to Bash semantic parsing task (NLBash) and show that it is largely reducible to lexical alignment. We also find that there is strong structural overlap between NLBash and natural language to SQL. Additionally, we perform a study varying compute expended during pre-training on the English to German machine translation task and find that more compute expended during pre-training does not always correspond semantic representations with stronger transfer to NLBash.
</details>
<details>
<summary>摘要</summary>
大规模预训练在自然语言处理多个领域取得了进步，然而预训练数据集的设计仍然不够了解。我们提出了一种方法来获得自然语言到BashSemantic parsing任务（NLBash）的结构性重叠的量化理解。我们应用我们的方法于NLBash任务，发现它主要归结于词语对应。我们还发现了自然语言到SQL任务和NLBash任务之间强大的结构重叠。此外，我们在英语到德语机器翻译任务上进行了不同计算资源的预训练时间对NLBash任务的传递效果的研究，发现更多的计算资源不总是导致更强的传递效果。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://example.com/2023/08/01/cs.LG_2023_08_01/" data-id="cllsjvzc7002ff5883htd8ej7" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/4/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/">6</a><a class="page-number" href="/page/7/">7</a><span class="space">&hellip;</span><a class="page-number" href="/page/17/">17</a><a class="extend next" rel="next" href="/page/6/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CR/">cs.CR</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">43</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">42</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">44</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">53</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">114</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'', root:''}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
