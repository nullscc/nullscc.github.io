
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/5/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-eess.AS_2023_08_22" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/22/eess.AS_2023_08_22/" class="article-date">
  <time datetime="2023-08-21T16:00:00.000Z" itemprop="datePublished">2023-08-22</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/22/eess.AS_2023_08_22/">eess.AS - 2023-08-22</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Furnishing-Sound-Event-Detection-with-Language-Model-Abilities"><a href="#Furnishing-Sound-Event-Detection-with-Language-Model-Abilities" class="headerlink" title="Furnishing Sound Event Detection with Language Model Abilities"></a>Furnishing Sound Event Detection with Language Model Abilities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11530">http://arxiv.org/abs/2308.11530</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hualei Wang, Jianguo Mao, Zhifang Guo, Jiarui Wan, Hong Liu, Xiangdong Wang</li>
</ul>
<p>Abstract:<br>Recently, the ability of language models (LMs) has attracted increasing attention in visual cross-modality. In this paper, we further explore the generation capacity of LMs for sound event detection (SED), beyond the visual domain. Specifically, we propose an elegant method that aligns audio features and text features to accomplish sound event classification and temporal location. The framework consists of an acoustic encoder, a contrastive module that align the corresponding representations of the text and audio, and a decoupled language decoder that generates temporal and event sequences from the audio characteristic. Compared with conventional works that require complicated processing and barely utilize limited audio features, our model is more concise and comprehensive since language model directly leverage its semantic capabilities to generate the sequences. We investigate different decoupling modules to demonstrate the effectiveness for timestamps capture and event classification. Evaluation results show that the proposed method achieves accurate sequences of sound event detection.</p>
<hr>
<h2 id="Deep-learning-based-denoising-streamed-from-mobile-phones-improves-speech-in-noise-understanding-for-hearing-aid-users"><a href="#Deep-learning-based-denoising-streamed-from-mobile-phones-improves-speech-in-noise-understanding-for-hearing-aid-users" class="headerlink" title="Deep learning-based denoising streamed from mobile phones improves speech-in-noise understanding for hearing aid users"></a>Deep learning-based denoising streamed from mobile phones improves speech-in-noise understanding for hearing aid users</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11456">http://arxiv.org/abs/2308.11456</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peter Udo Diehl, Hannes Zilly, Felix Sattler, Yosef Singer, Kevin Kepp, Mark Berry, Henning Hasemann, Marlene Zippel, Müge Kaya, Paul Meyer-Rachner, Annett Pudszuhn, Veit M. Hofmann, Matthias Vormann, Elias Sprengel</li>
</ul>
<p>Abstract:<br>The hearing loss of almost half a billion people is commonly treated with hearing aids. However, current hearing aids often do not work well in real-world noisy environments. We present a deep learning based denoising system that runs in real time on iPhone 7 and Samsung Galaxy S10 (25ms algorithmic latency). The denoised audio is streamed to the hearing aid, resulting in a total delay of around 75ms. In tests with hearing aid users having moderate to severe hearing loss, our denoising system improves audio across three tests: 1) listening for subjective audio ratings, 2) listening for objective speech intelligibility, and 3) live conversations in a noisy environment for subjective ratings. Subjective ratings increase by more than 40%, for both the listening test and the live conversation compared to a fitted hearing aid as a baseline. Speech reception thresholds, measuring speech understanding in noise, improve by 1.6 dB SRT. Ours is the first denoising system that is implemented on a mobile device, streamed directly to users’ hearing aids using only a single channel as audio input while improving user satisfaction on all tested aspects, including speech intelligibility. This includes overall preference of the denoised and streamed signal over the hearing aid, thereby accepting the higher latency for the significant improvement in speech understanding.</p>
<hr>
<h2 id="Convoifilter-A-case-study-of-doing-cocktail-party-speech-recognition"><a href="#Convoifilter-A-case-study-of-doing-cocktail-party-speech-recognition" class="headerlink" title="Convoifilter: A case study of doing cocktail party speech recognition"></a>Convoifilter: A case study of doing cocktail party speech recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11380">http://arxiv.org/abs/2308.11380</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thai-Binh Nguyen, Alexander Waibel</li>
</ul>
<p>Abstract:<br>This paper presents an end-to-end model designed to improve automatic speech recognition (ASR) for a particular speaker in a crowded, noisy environment. The model utilizes a single-channel speech enhancement module that isolates the speaker’s voice from background noise, along with an ASR module. Through this approach, the model is able to decrease the word error rate (WER) of ASR from 80% to 26.4%. Typically, these two components are adjusted independently due to variations in data requirements. However, speech enhancement can create anomalies that decrease ASR efficiency. By implementing a joint fine-tuning strategy, the model can reduce the WER from 26.4% in separate tuning to 14.5% in joint tuning.</p>
<hr>
<h2 id="Evaluation-of-the-Speech-Resynthesis-Capabilities-of-the-VoicePrivacy-Challenge-Baseline-B1"><a href="#Evaluation-of-the-Speech-Resynthesis-Capabilities-of-the-VoicePrivacy-Challenge-Baseline-B1" class="headerlink" title="Evaluation of the Speech Resynthesis Capabilities of the VoicePrivacy Challenge Baseline B1"></a>Evaluation of the Speech Resynthesis Capabilities of the VoicePrivacy Challenge Baseline B1</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11337">http://arxiv.org/abs/2308.11337</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ünal Ege Gaznepoglu, Nils Peters</li>
</ul>
<p>Abstract:<br>Speaker anonymization systems continue to improve their ability to obfuscate the original speaker characteristics in a speech signal, but often create processing artifacts and unnatural sounding voices as a tradeoff. Many of those systems stem from the VoicePrivacy Challenge (VPC) Baseline B1, using a neural vocoder to synthesize speech from an F0, x-vectors and bottleneck features-based speech representation. Inspired by this, we investigate the reproduction capabilities of the aforementioned baseline, to assess how successful the shared methodology is in synthesizing human-like speech. We use four objective metrics to measure speech quality, waveform similarity, and F0 similarity. Our findings indicate that both the speech representation and the vocoder introduces artifacts, causing an unnatural perception. A MUSHRA-like listening test on 18 subjects corroborate our findings, motivating further research on the analysis and synthesis components of the VPC Baseline B1.</p>
<hr>
<h2 id="Music-Understanding-LLaMA-Advancing-Text-to-Music-Generation-with-Question-Answering-and-Captioning"><a href="#Music-Understanding-LLaMA-Advancing-Text-to-Music-Generation-with-Question-Answering-and-Captioning" class="headerlink" title="Music Understanding LLaMA: Advancing Text-to-Music Generation with Question Answering and Captioning"></a>Music Understanding LLaMA: Advancing Text-to-Music Generation with Question Answering and Captioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11276">http://arxiv.org/abs/2308.11276</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shansong Liu, Atin Sakkeer Hussain, Chenshuo Sun, Ying Shan</li>
</ul>
<p>Abstract:<br>Text-to-music generation (T2M-Gen) faces a major obstacle due to the scarcity of large-scale publicly available music datasets with natural language captions. To address this, we propose the Music Understanding LLaMA (MU-LLaMA), capable of answering music-related questions and generating captions for music files. Our model utilizes audio representations from a pretrained MERT model to extract music features. However, obtaining a suitable dataset for training the MU-LLaMA model remains challenging, as existing publicly accessible audio question answering datasets lack the necessary depth for open-ended music question answering. To fill this gap, we present a methodology for generating question-answer pairs from existing audio captioning datasets and introduce the MusicQA Dataset designed for answering open-ended music-related questions. The experiments demonstrate that the proposed MU-LLaMA model, trained on our designed MusicQA dataset, achieves outstanding performance in both music question answering and music caption generation across various metrics, outperforming current state-of-the-art (SOTA) models in both fields and offering a promising advancement in the T2M-Gen research field.</p>
<hr>
<h2 id="Modeling-Bends-in-Popular-Music-Guitar-Tablatures"><a href="#Modeling-Bends-in-Popular-Music-Guitar-Tablatures" class="headerlink" title="Modeling Bends in Popular Music Guitar Tablatures"></a>Modeling Bends in Popular Music Guitar Tablatures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12307">http://arxiv.org/abs/2308.12307</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://gitlab.com/adhooge1/bend-prediction">https://gitlab.com/adhooge1/bend-prediction</a></li>
<li>paper_authors: Alexandre D’Hooge, Louis Bigo, Ken Déguernel</li>
</ul>
<p>Abstract:<br>Tablature notation is widely used in popular music to transcribe and share guitar musical content. As a complement to standard score notation, tablatures transcribe performance gesture information including finger positions and a variety of guitar-specific playing techniques such as slides, hammer-on&#x2F;pull-off or bends.This paper focuses on bends, which enable to progressively shift the pitch of a note, therefore circumventing physical limitations of the discrete fretted fingerboard. In this paper, we propose a set of 25 high-level features, computed for each note of the tablature, to study how bend occurrences can be predicted from their past and future short-term context. Experiments are performed on a corpus of 932 lead guitar tablatures of popular music and show that a decision tree successfully predicts bend occurrences with an F1 score of 0.71 anda limited amount of false positive predictions, demonstrating promising applications to assist the arrangement of non-guitar music into guitar tablatures.</p>
<hr>
<h2 id="An-Effective-Transformer-based-Contextual-Model-and-Temporal-Gate-Pooling-for-Speaker-Identification"><a href="#An-Effective-Transformer-based-Contextual-Model-and-Temporal-Gate-Pooling-for-Speaker-Identification" class="headerlink" title="An Effective Transformer-based Contextual Model and Temporal Gate Pooling for Speaker Identification"></a>An Effective Transformer-based Contextual Model and Temporal Gate Pooling for Speaker Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11241">http://arxiv.org/abs/2308.11241</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/harunorikawano/speaker-identification-with-tgp">https://github.com/harunorikawano/speaker-identification-with-tgp</a></li>
<li>paper_authors: Harunori Kawano, Sota Shimizu</li>
</ul>
<p>Abstract:<br>Wav2vec2 has achieved success in applying Transformer architecture and self-supervised learning to speech recognition. Recently, these have come to be used not only for speech recognition but also for the entire speech processing. This paper introduces an effective end-to-end speaker identification model applied Transformer-based contextual model. We explored the relationship between the parameters and the performance in order to discern the structure of an effective model. Furthermore, we propose a pooling method, Temporal Gate Pooling, with powerful learning ability for speaker identification. We applied Conformer as encoder and BEST-RQ for pre-training and conducted an evaluation utilizing the speaker identification of VoxCeleb1. The proposed method has achieved an accuracy of 85.9% with 28.5M parameters, demonstrating comparable precision to wav2vec2 with 317.7M parameters. Code is available at <a target="_blank" rel="noopener" href="https://github.com/HarunoriKawano/speaker-identification-with-tgp">https://github.com/HarunoriKawano/speaker-identification-with-tgp</a>.</p>
<hr>
<h2 id="PMVC-Data-Augmentation-Based-Prosody-Modeling-for-Expressive-Voice-Conversion"><a href="#PMVC-Data-Augmentation-Based-Prosody-Modeling-for-Expressive-Voice-Conversion" class="headerlink" title="PMVC: Data Augmentation-Based Prosody Modeling for Expressive Voice Conversion"></a>PMVC: Data Augmentation-Based Prosody Modeling for Expressive Voice Conversion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11084">http://arxiv.org/abs/2308.11084</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yimin Deng, Huaizhen Tang, Xulong Zhang, Jianzong Wang, Ning Cheng, Jing Xiao</li>
</ul>
<p>Abstract:<br>Voice conversion as the style transfer task applied to speech, refers to converting one person’s speech into a new speech that sounds like another person’s. Up to now, there has been a lot of research devoted to better implementation of VC tasks. However, a good voice conversion model should not only match the timbre information of the target speaker, but also expressive information such as prosody, pace, pause, etc. In this context, prosody modeling is crucial for achieving expressive voice conversion that sounds natural and convincing. Unfortunately, prosody modeling is important but challenging, especially without text transcriptions. In this paper, we firstly propose a novel voice conversion framework named ‘PMVC’, which effectively separates and models the content, timbre, and prosodic information from the speech without text transcriptions. Specially, we introduce a new speech augmentation algorithm for robust prosody extraction. And building upon this, mask and predict mechanism is applied in the disentanglement of prosody and content information. The experimental results on the AIShell-3 corpus supports our improvement of naturalness and similarity of converted speech.</p>
<hr>
<h2 id="Ultra-Dual-Path-Compression-For-Joint-Echo-Cancellation-And-Noise-Suppression"><a href="#Ultra-Dual-Path-Compression-For-Joint-Echo-Cancellation-And-Noise-Suppression" class="headerlink" title="Ultra Dual-Path Compression For Joint Echo Cancellation And Noise Suppression"></a>Ultra Dual-Path Compression For Joint Echo Cancellation And Noise Suppression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11053">http://arxiv.org/abs/2308.11053</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hangting Chen, Jianwei Yu, Yi Luo, Rongzhi Gu, Weihua Li, Zhuocheng Lu, Chao Weng</li>
</ul>
<p>Abstract:<br>Echo cancellation and noise reduction are essential for full-duplex communication, yet most existing neural networks have high computational costs and are inflexible in tuning model complexity. In this paper, we introduce time-frequency dual-path compression to achieve a wide range of compression ratios on computational cost. Specifically, for frequency compression, trainable filters are used to replace manually designed filters for dimension reduction. For time compression, only using frame skipped prediction causes large performance degradation, which can be alleviated by a post-processing network with full sequence modeling. We have found that under fixed compression ratios, dual-path compression combining both the time and frequency methods will give further performance improvement, covering compression ratios from 4x to 32x with little model size change. Moreover, the proposed models show competitive performance compared with fast FullSubNet and DeepFilterNet. A demo page can be found at hangtingchen.github.io&#x2F;ultra_dual_path_compression.github.io&#x2F;.</p>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/22/eess.AS_2023_08_22/" data-id="clm0t8e1x00cgv7888jlt378r" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_08_22" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/22/eess.IV_2023_08_22/" class="article-date">
  <time datetime="2023-08-21T16:00:00.000Z" itemprop="datePublished">2023-08-22</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/22/eess.IV_2023_08_22/">eess.IV - 2023-08-22 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Multitemporal-analysis-in-Google-Earth-Engine-for-detecting-urban-changes-using-optical-data-and-machine-learning-algorithms"><a href="#Multitemporal-analysis-in-Google-Earth-Engine-for-detecting-urban-changes-using-optical-data-and-machine-learning-algorithms" class="headerlink" title="Multitemporal analysis in Google Earth Engine for detecting urban changes using optical data and machine learning algorithms"></a>Multitemporal analysis in Google Earth Engine for detecting urban changes using optical data and machine learning algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11468">http://arxiv.org/abs/2308.11468</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mariapia Rita Iandolo, Francesca Razzano, Chiara Zarro, G. S. Yogesh, Silvia Liberata Ullo</li>
<li>for: 本研究使用Google Earth Engine（GEE）平台进行多时间分析，检测城市区域的变化使用光学数据和专门的机器学习（ML）算法。</li>
<li>methods: 作为案例研究，选择了埃及国家的开罗市，为过去一个十年内世界上最多人口的五个超大城市之一。从2013年7月到2021年7月，对研究区域进行分类和变化检测分析。</li>
<li>results: 结果表明提出的方法有效地确定了在选定时间间发生变化和不发生变化的城市区域。此外，本研究也证明了GEE作为云端解决方案，可以有效地处理大量的卫星数据。<details>
<summary>Abstract</summary>
The aim of this work is to perform a multitemporal analysis using the Google Earth Engine (GEE) platform for the detection of changes in urban areas using optical data and specific machine learning (ML) algorithms. As a case study, Cairo City has been identified, in Egypt country, as one of the five most populous megacities of the last decade in the world. Classification and change detection analysis of the region of interest (ROI) have been carried out from July 2013 to July 2021. Results demonstrate the validity of the proposed method in identifying changed and unchanged urban areas over the selected period. Furthermore, this work aims to evidence the growing significance of GEE as an efficient cloud-based solution for managing large quantities of satellite data.
</details>
<details>
<summary>摘要</summary>
目的是使用Google Earth Engine（GEE）平台进行多时间分析，以探测城市区域的变化使用光学数据和专门的机器学习（ML）算法。作为案例研究，埃及的开罗城市被选为全球最后一个十年最为人口稠密的五个超级城市之一。从2013年7月至2021年7月的时间段进行了区域兴趣（ROI）的分类和变化检测分析。结果表明提出的方法有效地标识了在选定时间段内发生变化和不发生变化的城市区域。此外，本研究也旨在证明GEE作为云计算平台，对快速处理大量卫星数据表现出了高效的能力。
</details></li>
</ul>
<hr>
<h2 id="Integration-of-Sentinel-1-and-Sentinel-2-data-for-Earth-surface-classification-using-Machine-Learning-algorithms-implemented-on-Google-Earth-Engine"><a href="#Integration-of-Sentinel-1-and-Sentinel-2-data-for-Earth-surface-classification-using-Machine-Learning-algorithms-implemented-on-Google-Earth-Engine" class="headerlink" title="Integration of Sentinel-1 and Sentinel-2 data for Earth surface classification using Machine Learning algorithms implemented on Google Earth Engine"></a>Integration of Sentinel-1 and Sentinel-2 data for Earth surface classification using Machine Learning algorithms implemented on Google Earth Engine</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11340">http://arxiv.org/abs/2308.11340</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francesca Razzano, Mariapia Rita Iandolo, Chiara Zarro, G. S. Yogesh, Silvia Liberata Ullo</li>
<li>for: 这个研究旨在使用Synthetic Aperture Radar (SAR)和光学数据进行地球表面分类。</li>
<li>methods: 这个研究使用了Sentinel-1 (S-1)和Sentinel-2 (S-2)数据的集成，通过超vision机器学习算法在Google Earth Engine (GEE)平台上进行分类。</li>
<li>results: 研究结果表明，Radar和光学Remote探测提供了补充性信息，有利于地表覆盖分类，通常导致映射精度提高。此外，这篇论文也证明了GEE在处理大量卫星数据方面的emerging角色。<details>
<summary>Abstract</summary>
In this study, Synthetic Aperture Radar (SAR) and optical data are both considered for Earth surface classification. Specifically, the integration of Sentinel-1 (S-1) and Sentinel-2 (S-2) data is carried out through supervised Machine Learning (ML) algorithms implemented on the Google Earth Engine (GEE) platform for the classification of a particular region of interest. Achieved results demonstrate how in this case radar and optical remote detection provide complementary information, benefiting surface cover classification and generally leading to increased mapping accuracy. In addition, this paper works in the direction of proving the emerging role of GEE as an effective cloud-based tool for handling large amounts of satellite data.
</details>
<details>
<summary>摘要</summary>
这个研究中，使用Synthetic Aperture Radar（SAR）和光学数据进行地面分类。特别是通过监督式机器学习（ML）算法在Google Earth Engine（GEE）平台上结合Sentinel-1（S-1）和Sentinel-2（S-2）数据进行地面覆盖分类。实际结果表明在这种情况下，雷达和光学Remote探测提供了补充性信息，有利于地面覆盖分类，通常导致增加的地图精度。此外，这篇论文还证明了GEE在处理巨量卫星数据方面的emerging角色。
</details></li>
</ul>
<hr>
<h2 id="PCMC-T1-Free-breathing-myocardial-T1-mapping-with-Physically-Constrained-Motion-Correction"><a href="#PCMC-T1-Free-breathing-myocardial-T1-mapping-with-Physically-Constrained-Motion-Correction" class="headerlink" title="PCMC-T1: Free-breathing myocardial T1 mapping with Physically-Constrained Motion Correction"></a>PCMC-T1: Free-breathing myocardial T1 mapping with Physically-Constrained Motion Correction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11281">http://arxiv.org/abs/2308.11281</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eyal Hanania, Ilya Volovik, Lilach Barkat, Israel Cohen, Moti Freiman</li>
<li>for: 这篇论文旨在描述一种基于深度学习的自由呼吸T1映像注射技术，以增强diffuse myocardial disease的诊断。</li>
<li>methods: 该技术使用了深度学习模型来 correction of motion artifacts in free-breathing T1 mapping. The model incorporates the signal decay model to encourage physically-plausible deformations along the longitudinal relaxation axis.</li>
<li>results: 对于5个 эксперименталь setup，PCMC-T1表现出最高的模型适应质量(R2: 0.955)和最高的клиниче影响(clinical score: 3.93)，在比较基eline方法时表现出了优异性。<details>
<summary>Abstract</summary>
T1 mapping is a quantitative magnetic resonance imaging (qMRI) technique that has emerged as a valuable tool in the diagnosis of diffuse myocardial diseases. However, prevailing approaches have relied heavily on breath-hold sequences to eliminate respiratory motion artifacts. This limitation hinders accessibility and effectiveness for patients who cannot tolerate breath-holding. Image registration can be used to enable free-breathing T1 mapping. Yet, inherent intensity differences between the different time points make the registration task challenging. We introduce PCMC-T1, a physically-constrained deep-learning model for motion correction in free-breathing T1 mapping. We incorporate the signal decay model into the network architecture to encourage physically-plausible deformations along the longitudinal relaxation axis. We compared PCMC-T1 to baseline deep-learning-based image registration approaches using a 5-fold experimental setup on a publicly available dataset of 210 patients. PCMC-T1 demonstrated superior model fitting quality (R2: 0.955) and achieved the highest clinical impact (clinical score: 3.93) compared to baseline methods (0.941, 0.946 and 3.34, 3.62 respectively). Anatomical alignment results were comparable (Dice score: 0.9835 vs. 0.984, 0.988). Our code and trained models are available at https://github.com/eyalhana/PCMC-T1.
</details>
<details>
<summary>摘要</summary>
T1映射是一种量化核磁共振成像（qMRI）技术，在慢性心肺疾病诊断中发挥了重要作用。然而，现有的方法都是基于呼吸停止序列来消除呼吸运动artefacts，这限制了患者可以tolerate的范围。图像 registrtion可以使得自由呼吸T1映射成为可能。然而，不同时点的信号强度之间的自然差异使得注册任务变得困难。我们介绍了PCMC-T1，一种基于深度学习的物理约束模型，用于自由呼吸T1映射中的运动 corrections。我们在网络架构中包含了信号衰减模型，以便鼓励物理可能的扭曲 along the longitudinal relaxation axis。我们与基eline deep learning based image registrtion方法进行了5-fold实验，结果显示PCMC-T1的模型适应质量（R2：0.955）和临床影响（临床分数：3.93）都高于基eline方法（0.941、0.946和3.34、3.62分别）。解剖对应结果相似（Dice分数：0.9835 vs. 0.984、0.988）。我们的代码和训练模型可以在https://github.com/eyalhana/PCMC-T1上获取。
</details></li>
</ul>
<hr>
<h2 id="Validation-of-apparent-intra-and-extra-myocellular-lipid-content-indicator-using-spiral-spectroscopic-imaging-at-3T"><a href="#Validation-of-apparent-intra-and-extra-myocellular-lipid-content-indicator-using-spiral-spectroscopic-imaging-at-3T" class="headerlink" title="Validation of apparent intra-and extra-myocellular lipid content indicator using spiral spectroscopic imaging at 3T"></a>Validation of apparent intra-and extra-myocellular lipid content indicator using spiral spectroscopic imaging at 3T</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11668">http://arxiv.org/abs/2308.11668</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antoine Naëgel, Magalie Viallon, Jabrane Karkouri, Thomas Troalen, Pierre Croisille, Hélène Ratiney</li>
<li>for: 本研究旨在开发一种快速简单的征方法，用于映射IMCL和EMCL显示的内容，这是一项复杂的任务，并与经典质量量化结果进行比较。</li>
<li>methods: 本研究使用旋转MRSI技术来实现快速简单的征方法，并对肌肉区域进行研究。</li>
<li>results: 研究结果表明，本方法可以快速、简单地映射IMCL和EMCL显示的内容，并与经典质量量化结果相符。<details>
<summary>Abstract</summary>
This work presents a fast and simple method based on spiral MRSI for mapping the IMCL and EMCL apparent content, which is a challenging task and it compares this indicator to classical quantification results in muscles of interest.
</details>
<details>
<summary>摘要</summary>
这个研究提出了一种快速简单的方法，基于螺旋MRSI测量IMCL和EMCL显示的内容，这是一项复杂的任务，并与经典量化结果进行比较。Here's a breakdown of the translation:* 这个研究 (zhè ge yánjiū) - This study* 提出 (tīshuō) - proposes* 一种 (yī zhī) - a kind of* 方法 (fāngfa) - method* 基于 (jīyù) - based on* 螺旋MRSI (lúnshēn MRSI) - spiral MRSI* 测量 (cèliàng) - measurement* IMCL (yìmín cluò) - intramuscular adipose tissue* 和 (hé) - and* EMCL (èrmín cluò) - extramuscular adipose tissue* 显示 (xiǎnshì) - display* 的 (de) - possessive particle* 内容 (néngyòng) - content* 是 (shì) - is* 一项 (yījiàn) - a task* 复杂 (fùzé) - complex* 与 (yǔ) - and* 经典 (jīngdiǎn) - classical* 量化 (liàngzhèng) - quantification* 结果 (jiéguò) - result* 进行 (jìnxiàng) - to conduct* 比较 (bǐjiào) - comparisonNote that the translation of "IMCL" and "EMCL" as "内容" (content) is a bit ambiguous, as these terms typically refer to specific types of tissue, rather than a general term for content. However, in the context of the sentence, it seems to be using "内容" to refer to the apparent content of the tissue, rather than the tissue itself.
</details></li>
</ul>
<hr>
<h2 id="Phase-Aberration-Correction-A-Deep-Learning-Based-Aberration-to-Aberration-Approach"><a href="#Phase-Aberration-Correction-A-Deep-Learning-Based-Aberration-to-Aberration-Approach" class="headerlink" title="Phase Aberration Correction: A Deep Learning-Based Aberration to Aberration Approach"></a>Phase Aberration Correction: A Deep Learning-Based Aberration to Aberration Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11149">http://arxiv.org/abs/2308.11149</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mostafa Sharifzadeh, Sobhan Goudarzi, An Tang, Habib Benali, Hassan Rivaz</li>
<li>for: 本研究旨在开发一种不需要真实ground truth的深度学习方法，用于correcting phase aberration问题在ultrasound imaging中。</li>
<li>methods: 我们提出了一种使用深度学习方法，并使用Randomly Aberrated RF Data作为输入和目标输出。我们还提出了一种适应性混合损失函数，以便更高效地训练这种网络。</li>
<li>results: 我们在实验中发现，使用我们提出的方法可以高效地correct phase aberration问题，并且可以在实际场景中使用。此外，我们还发布了一个包含161,701个单平面波图像（RF数据）的数据集，以便解决深度学习方法的数据不足问题。<details>
<summary>Abstract</summary>
One of the primary sources of suboptimal image quality in ultrasound imaging is phase aberration. It is caused by spatial changes in sound speed over a heterogeneous medium, which disturbs the transmitted waves and prevents coherent summation of echo signals. Obtaining non-aberrated ground truths in real-world scenarios can be extremely challenging, if not impossible. This challenge hinders training of deep learning-based techniques' performance due to the presence of domain shift between simulated and experimental data. Here, for the first time, we propose a deep learning-based method that does not require ground truth to correct the phase aberration problem, and as such, can be directly trained on real data. We train a network wherein both the input and target output are randomly aberrated radio frequency (RF) data. Moreover, we demonstrate that a conventional loss function such as mean square error is inadequate for training such a network to achieve optimal performance. Instead, we propose an adaptive mixed loss function that employs both B-mode and RF data, resulting in more efficient convergence and enhanced performance. Finally, we publicly release our dataset, including 161,701 single plane-wave images (RF data). This dataset serves to mitigate the data scarcity problem in the development of deep learning-based techniques for phase aberration correction.
</details>
<details>
<summary>摘要</summary>
For the first time, we propose a deep learning-based method that does not require ground truth to correct the phase aberration problem, and can be directly trained on real data. We train a network where the input and target output are randomly aberrated radio frequency (RF) data. Moreover, we find that a conventional loss function such as mean square error is inadequate for training such a network to achieve optimal performance. Instead, we propose an adaptive mixed loss function that employs both B-mode and RF data, resulting in more efficient convergence and enhanced performance.We publicly release our dataset, including 161,701 single plane-wave images (RF data), which serves to mitigate the data scarcity problem in the development of deep learning-based techniques for phase aberration correction.
</details></li>
</ul>
<hr>
<h2 id="Hey-That’s-Mine-Imperceptible-Watermarks-are-Preserved-in-Diffusion-Generated-Outputs"><a href="#Hey-That’s-Mine-Imperceptible-Watermarks-are-Preserved-in-Diffusion-Generated-Outputs" class="headerlink" title="Hey That’s Mine Imperceptible Watermarks are Preserved in Diffusion Generated Outputs"></a>Hey That’s Mine Imperceptible Watermarks are Preserved in Diffusion Generated Outputs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11123">http://arxiv.org/abs/2308.11123</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luke Ditria, Tom Drummond</li>
<li>for: 保护内容在线分享</li>
<li>methods: 使用隐藏水印技术训练生成模型，并测试模型是否可以检测水印并恢复水印特征</li>
<li>results: 通过统计测试，确认模型可以检测和恢复水印，提供了一种保护知识产权的解决方案<details>
<summary>Abstract</summary>
Generative models have seen an explosion in popularity with the release of huge generative Diffusion models like Midjourney and Stable Diffusion to the public. Because of this new ease of access, questions surrounding the automated collection of data and issues regarding content ownership have started to build. In this paper we present new work which aims to provide ways of protecting content when shared to the public. We show that a generative Diffusion model trained on data that has been imperceptibly watermarked will generate new images with these watermarks present. We further show that if a given watermark is correlated with a certain feature of the training data, the generated images will also have this correlation. Using statistical tests we show that we are able to determine whether a model has been trained on marked data, and what data was marked. As a result our system offers a solution to protect intellectual property when sharing content online.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>生成模型在发布大量生成扩散模型 Midjourney 和 Stable Diffusion 后得到了广泛的投入。由于这种新的访问权，人们开始关注自动收集数据的问题以及内容所有权问题。在这篇论文中，我们提出了一种保护内容的新方法。我们证明了一个基于不可见水印的生成扩散模型，会在生成新图像时包含这些水印。此外，如果给定的水印与训练数据中的某个特征相关，那么生成的图像也将具有这种相关性。通过统计测试，我们证明了我们是否可以判断模型是否训练用到了水印数据，以及这些数据是什么。因此，我们的系统可以保护在线分享内容的知识产权。
</details></li>
</ul>
<hr>
<h2 id="Switched-auxiliary-loss-for-robust-training-of-transformer-models-for-histopathological-image-segmentation"><a href="#Switched-auxiliary-loss-for-robust-training-of-transformer-models-for-histopathological-image-segmentation" class="headerlink" title="Switched auxiliary loss for robust training of transformer models for histopathological image segmentation"></a>Switched auxiliary loss for robust training of transformer models for histopathological image segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10994">http://arxiv.org/abs/2308.10994</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mustaffa Hussain, Saharsh Barve</li>
<li>for: 本研究旨在开发一种用于多器官功能组织单元（FTUs）分割模型，以帮助病理学家更好地理解人体疾病的 Cellular 水平信息。</li>
<li>methods: 我们使用了HuBMAP + HPA - Hacking the Human Body竞赛数据集，并提出了使用偏移式助理损失来解决深度模型衰减问题，以便在训练深度模型时获得最佳result。</li>
<li>results: 我们的模型在公共数据集上得到了0.793的dice分数，与私有数据集上的0.778的dice分数相比，表明使用我们提议的方法可以提高模型的表现。这些发现也证明了transformers模型在医学图像分析中的紧密预测任务中的可靠性。<details>
<summary>Abstract</summary>
Functional tissue Units (FTUs) are cell population neighborhoods local to a particular organ performing its main function. The FTUs provide crucial information to the pathologist in understanding the disease affecting a particular organ by providing information at the cellular level. In our research, we have developed a model to segment multi-organ FTUs across 5 organs namely: the kidney, large intestine, lung, prostate and spleen by utilizing the HuBMAP + HPA - Hacking the Human Body competition dataset. We propose adding shifted auxiliary loss for training models like the transformers to overcome the diminishing gradient problem which poses a challenge towards optimal training of deep models. Overall, our model achieved a dice score of 0.793 on the public dataset and 0.778 on the private dataset and shows a 1% improvement with the use of the proposed method. The findings also bolster the use of transformers models for dense prediction tasks in the field of medical image analysis. The study assists in understanding the relationships between cell and tissue organization thereby providing a useful medium to look at the impact of cellular functions on human health.
</details>
<details>
<summary>摘要</summary>
Functional tissue Units (FTUs) 是指器官当地的细胞群聚地区，它们提供了病理学家理解器官疾病的关键信息。在我们的研究中，我们已经开发了一种方法来在5个器官（肾脏、大小肠、肺、生殖腺和脾膜）的多个Functional tissue Units（FTUs）中进行分割，使用了HuBMAP + HPA - Hacking the Human Body competition dataset。我们提议在训练模型时使用偏移 auxiliary loss，以解决深度模型训练中的减少梯度问题，从而提高模型的训练效果。总的来说，我们的模型在公共数据集上达到了0.793的 dice score，在私有数据集上达到了0.778的 dice score，与使用我们提议的方法相比，提高了1%。这些结果也证明了使用 transformers 模型在医学图像分析中进行紧密预测任务是有效的。这个研究帮助我们更好地理解细胞和组织之间的关系，从而更好地了解人类健康的影响因素。
</details></li>
</ul>
<hr>
<h2 id="Debiasing-Counterfactuals-In-the-Presence-of-Spurious-Correlations"><a href="#Debiasing-Counterfactuals-In-the-Presence-of-Spurious-Correlations" class="headerlink" title="Debiasing Counterfactuals In the Presence of Spurious Correlations"></a>Debiasing Counterfactuals In the Presence of Spurious Correlations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10984">http://arxiv.org/abs/2308.10984</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amar Kumar, Nima Fathi, Raghav Mehta, Brennan Nichyporuk, Jean-Pierre R. Falet, Sotirios Tsaftaris, Tal Arbel</li>
<li>for: This paper aims to improve the performance of deep learning models in medical imaging classification tasks by addressing the issue of spurious correlations in the training data.</li>
<li>methods: The proposed method integrates two techniques: (1) popular debiasing classifiers, such as distributionally robust optimization (DRO), to avoid relying on spurious correlations, and (2) counterfactual image generation to unveil generalizable imaging markers of relevance to the task.</li>
<li>results: The proposed method is evaluated on two public datasets with simulated and real visual artifacts, and the results show that it (1) learns generalizable markers across the population and (2) successfully ignores spurious correlations and focuses on the underlying disease pathology.Here’s the same information in Simplified Chinese:</li>
<li>for: 这篇论文目标是解决医学成像分类任务中的干扰因素问题，使深度学习模型能够更好地在人口中学习。</li>
<li>methods: 提议的方法组合了两种技术：（1）流行的偏差修正分类器（DRO），以避免基于干扰因素的推断，和（2）对应ifactual图像生成，以揭示有关任务的可靠成像标记。</li>
<li>results: 在两个公共数据集上（包括模拟和实际视觉artefacts）进行了评估，结果显示：（1）可以在人口中学习普遍的标记，和（2）成功忽略干扰因素，关注下面疾病趋势。<details>
<summary>Abstract</summary>
Deep learning models can perform well in complex medical imaging classification tasks, even when basing their conclusions on spurious correlations (i.e. confounders), should they be prevalent in the training dataset, rather than on the causal image markers of interest. This would thereby limit their ability to generalize across the population. Explainability based on counterfactual image generation can be used to expose the confounders but does not provide a strategy to mitigate the bias. In this work, we introduce the first end-to-end training framework that integrates both (i) popular debiasing classifiers (e.g. distributionally robust optimization (DRO)) to avoid latching onto the spurious correlations and (ii) counterfactual image generation to unveil generalizable imaging markers of relevance to the task. Additionally, we propose a novel metric, Spurious Correlation Latching Score (SCLS), to quantify the extent of the classifier reliance on the spurious correlation as exposed by the counterfactual images. Through comprehensive experiments on two public datasets (with the simulated and real visual artifacts), we demonstrate that the debiasing method: (i) learns generalizable markers across the population, and (ii) successfully ignores spurious correlations and focuses on the underlying disease pathology.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="BundleSeg-A-versatile-reliable-and-reproducible-approach-to-white-matter-bundle-segmentation"><a href="#BundleSeg-A-versatile-reliable-and-reproducible-approach-to-white-matter-bundle-segmentation" class="headerlink" title="BundleSeg: A versatile, reliable and reproducible approach to white matter bundle segmentation"></a>BundleSeg: A versatile, reliable and reproducible approach to white matter bundle segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10958">http://arxiv.org/abs/2308.10958</a></li>
<li>repo_url: None</li>
<li>paper_authors: Etienne St-Onge, Kurt G Schilling, Francois Rheault</li>
<li>for: 提供一种可靠、可重现、快速的白 matter 通路EXTRACTION方法</li>
<li>methods: 利用迭代注册程序和精确流线搜索算法，能够高效地分 Segment 流线，无需进行追gram clustering或简化假设</li>
<li>results: 在repeatability和 reproduceability方面，BundleSeg 表现更好于现状的分Segmentation方法，并且具有显著的速度提升。提高了白 matter 连接的精度和减少了变化，为 neuroscience 研究提供了一种有价值的工具，从而提高了追gram-based 研究的敏感性和特点。<details>
<summary>Abstract</summary>
This work presents BundleSeg, a reliable, reproducible, and fast method for extracting white matter pathways. The proposed method combines an iterative registration procedure with a recently developed precise streamline search algorithm that enables efficient segmentation of streamlines without the need for tractogram clustering or simplifying assumptions. We show that BundleSeg achieves improved repeatability and reproducibility than state-of-the-art segmentation methods, with significant speed improvements. The enhanced precision and reduced variability in extracting white matter connections offer a valuable tool for neuroinformatic studies, increasing the sensitivity and specificity of tractography-based studies of white matter pathways.
</details>
<details>
<summary>摘要</summary>
这个研究提出了一种可靠、可重复、快速的白 matter 路径提取方法，称为 BundleSeg。该方法结合了一种迭代注册过程和最近发展的精确流线搜索算法，可以高效地分 segments 流线无需进行 tractogram 归类或简化假设。我们示出了 BundleSeg 在比较州-of-the-art 分 segmentation 方法的重复性和可重复性有所提高，同时速度也有显著提高。增强的精度和流线分 segments 的变化率提供了一种有价值的工具 для neuroscience 研究，提高了追踪性-based 研究中白 matter 路径的敏感性和特点。
</details></li>
</ul>
<hr>
<h2 id="Pixel-Adaptive-Deep-Unfolding-Transformer-for-Hyperspectral-Image-Reconstruction"><a href="#Pixel-Adaptive-Deep-Unfolding-Transformer-for-Hyperspectral-Image-Reconstruction" class="headerlink" title="Pixel Adaptive Deep Unfolding Transformer for Hyperspectral Image Reconstruction"></a>Pixel Adaptive Deep Unfolding Transformer for Hyperspectral Image Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10820">http://arxiv.org/abs/2308.10820</a></li>
<li>repo_url: None</li>
<li>paper_authors: Miaoyu Li, Ying Fu, Ji Liu, Yulun Zhang</li>
<li>for: 高光谱像 (HSI) 重建，解决深度折衣框架中的匹配问题。</li>
<li>methods: 提出了一种Pixel Adaptive Deep Unfolding Transformer (PADUT)，在数据模块中使用了像素适应下降步骤，在假设模块中引入了Non-local Spectral Transformer (NST)，并在不同阶段和深度中的特征表达中进行了改进。</li>
<li>results: 对比于现有的HSI重建方法，实验结果表明PADUT方法在实验场景中具有更高的重建质量。代码可以在<a target="_blank" rel="noopener" href="https://github.com/MyuLi/PADUT%E4%B8%AD%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/MyuLi/PADUT中下载。</a><details>
<summary>Abstract</summary>
Hyperspectral Image (HSI) reconstruction has made gratifying progress with the deep unfolding framework by formulating the problem into a data module and a prior module. Nevertheless, existing methods still face the problem of insufficient matching with HSI data. The issues lie in three aspects: 1) fixed gradient descent step in the data module while the degradation of HSI is agnostic in the pixel-level. 2) inadequate prior module for 3D HSI cube. 3) stage interaction ignoring the differences in features at different stages. To address these issues, in this work, we propose a Pixel Adaptive Deep Unfolding Transformer (PADUT) for HSI reconstruction. In the data module, a pixel adaptive descent step is employed to focus on pixel-level agnostic degradation. In the prior module, we introduce the Non-local Spectral Transformer (NST) to emphasize the 3D characteristics of HSI for recovering. Moreover, inspired by the diverse expression of features in different stages and depths, the stage interaction is improved by the Fast Fourier Transform (FFT). Experimental results on both simulated and real scenes exhibit the superior performance of our method compared to state-of-the-art HSI reconstruction methods. The code is released at: https://github.com/MyuLi/PADUT.
</details>
<details>
<summary>摘要</summary>
高spectral像素（HSI）重建已经取得了满意的进步，透过深度发散框架，将问题转化为数据模组和假设模组。然而，现有方法仍然面临HSI数据不足的问题，这些问题包括：1）固定的梯度下降步骤在数据模组中，而HSI质量下降是无知的Pixel水平。2）不够的假设模组 для3D HSI立方体。3）阶段交互忽略了不同阶段和层级的特征之间的不同。为了解决这些问题，在这个研究中，我们提出了适应Pixel深度 unfolding transformer（PADUT） дляHSI重建。在数据模组中，我们使用适应梯度下降步骤，以注意到Pixel水平的不知质量下降。在假设模组中，我们引入了Non-local Spectral Transformer（NST），以强调3D特征的HSI重建。此外，受到不同阶段和层级的特征表达的多样性的惊叹，我们改进了阶段交互，使用Fast Fourier Transform（FFT）。实验结果显示，我们的方法与现有的HSI重建方法相比，在 simulated和RealScene 上具有更高的性能。代码可以在：https://github.com/MyuLi/PADUT 获取。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/22/eess.IV_2023_08_22/" data-id="clm0t8e2x00fmv788d1958pto" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_21" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/21/cs.LG_2023_08_21/" class="article-date">
  <time datetime="2023-08-20T16:00:00.000Z" itemprop="datePublished">2023-08-21</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/21/cs.LG_2023_08_21/">cs.LG - 2023-08-21 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Graph-Neural-Bandits"><a href="#Graph-Neural-Bandits" class="headerlink" title="Graph Neural Bandits"></a>Graph Neural Bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10808">http://arxiv.org/abs/2308.10808</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lasgroup/GNNBO">https://github.com/lasgroup/GNNBO</a></li>
<li>paper_authors: Yunzhe Qi, Yikun Ban, Jingrui He</li>
<li>for: 这篇论文是为了提出一种基于图神经网络的推荐算法框架，以便利用用户之间的协同作用来提高推荐效果。</li>
<li>methods: 该论文使用了图神经网络来建模用户之间的协同关系，并分别使用GNN-based模型来进行优化的推荐策略。</li>
<li>results: 该论文通过对多个实际数据集进行比较，证明了其提出的推荐算法框架的有效性。<details>
<summary>Abstract</summary>
Contextual bandits algorithms aim to choose the optimal arm with the highest reward out of a set of candidates based on the contextual information. Various bandit algorithms have been applied to real-world applications due to their ability of tackling the exploitation-exploration dilemma. Motivated by online recommendation scenarios, in this paper, we propose a framework named Graph Neural Bandits (GNB) to leverage the collaborative nature among users empowered by graph neural networks (GNNs). Instead of estimating rigid user clusters as in existing works, we model the "fine-grained" collaborative effects through estimated user graphs in terms of exploitation and exploration respectively. Then, to refine the recommendation strategy, we utilize separate GNN-based models on estimated user graphs for exploitation and adaptive exploration. Theoretical analysis and experimental results on multiple real data sets in comparison with state-of-the-art baselines are provided to demonstrate the effectiveness of our proposed framework.
</details>
<details>
<summary>摘要</summary>
Contextual bandits算法目的是选择最高奖励的武器（arm）中的集合基于Contextual信息。多种bandit算法在实际应用中使用，因为它们可以解决探索和利用之间的矛盾。在这篇论文中，我们提出了一个名为图 neural bandits（GNB）的框架，利用用户之间的协同作用，激活了图神经网络（GNNs）。而不是在现有的工作中估计固定的用户群集，我们通过估计用户图来模型“细化”的协同效果，分别用于探索和适应探索。然后，我们使用分开的GNN-based模型来修改推荐策略，并对多个实际数据集进行了 teoretic 分析和实验研究，以证明我们的提出的框架的效果。
</details></li>
</ul>
<hr>
<h2 id="DynED-Dynamic-Ensemble-Diversification-in-Data-Stream-Classification"><a href="#DynED-Dynamic-Ensemble-Diversification-in-Data-Stream-Classification" class="headerlink" title="DynED: Dynamic Ensemble Diversification in Data Stream Classification"></a>DynED: Dynamic Ensemble Diversification in Data Stream Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10807">http://arxiv.org/abs/2308.10807</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/soheilabadifard/dyned">https://github.com/soheilabadifard/dyned</a></li>
<li>paper_authors: Soheil Abadifard, Sepehr Bakhshi, Sanaz Gheibuni, Fazli Can</li>
<li>for: 这篇论文是为了提高在数据流中的类别准确率，因为数据流中的变化可能会导致模型的性能下降。</li>
<li>methods: 这篇论文使用了一种新的集成方法，叫做DynED，它可以在数据流中维持集成模型的性能。DynED使用了MMR（最大margin relevance）来选择最佳的集成元件，以确保集成模型的各个元件都能够做出贡献。</li>
<li>results: 根据实验结果，DynED在11个 sintetic dataset和4个真实 dataset上的平均准确率高于5个现有的基eline。<details>
<summary>Abstract</summary>
Ensemble methods are commonly used in classification due to their remarkable performance. Achieving high accuracy in a data stream environment is a challenging task considering disruptive changes in the data distribution, also known as concept drift. A greater diversity of ensemble components is known to enhance prediction accuracy in such settings. Despite the diversity of components within an ensemble, not all contribute as expected to its overall performance. This necessitates a method for selecting components that exhibit high performance and diversity. We present a novel ensemble construction and maintenance approach based on MMR (Maximal Marginal Relevance) that dynamically combines the diversity and prediction accuracy of components during the process of structuring an ensemble. The experimental results on both four real and 11 synthetic datasets demonstrate that the proposed approach (DynED) provides a higher average mean accuracy compared to the five state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
ensemble方法在分类 task 中广泛应用，因其表现出色。在数据流环境中达到高精度是一项具有挑战性的任务，因为数据分布会不断变化，也就是说 concept drift。更多的ensemble组件可以提高预测精度。 despite the diversity of components within an ensemble, not all contribute as expected to its overall performance。这种情况需要一种方法来选择表现好的和多样的组件。我们提出了一种基于 MMR（最大边际相关）的新集成建立和维护方法，可以在构建集成时动态组合多样性和预测精度。实验结果表明，我们的方法（DynED）在14个实验数据集上比基eline5种状态 искусственный顶峰表现出更高的平均含义精度。
</details></li>
</ul>
<hr>
<h2 id="Differentiable-Frank-Wolfe-Optimization-Layer"><a href="#Differentiable-Frank-Wolfe-Optimization-Layer" class="headerlink" title="Differentiable Frank-Wolfe Optimization Layer"></a>Differentiable Frank-Wolfe Optimization Layer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10806">http://arxiv.org/abs/2308.10806</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zixuan Liu, Liu Liu, Xueqian Wang, Peilin Zhao</li>
<li>for: 提出了一种高效的梯度下降优化层（DFWLayer），用于解决具有约束的机器学习问题。</li>
<li>methods: 基于Frank-Wolfe算法，DFWLayer不需要计算投影和希格曼矩阵，从而实现了高效的大规模问题解决方法。</li>
<li>results: 在实验中，DFWLayer不仅实现了竞争性的准确率和梯度，还可靠地遵守约束。此外，它在前向和反向计算速度方面也超过了基elines。<details>
<summary>Abstract</summary>
Differentiable optimization has received a significant amount of attention due to its foundational role in the domain of machine learning based on neural networks. The existing methods leverages the optimality conditions and implicit function theorem to obtain the Jacobian matrix of the output, which increases the computational cost and limits the application of differentiable optimization. In addition, some non-differentiable constraints lead to more challenges when using prior differentiable optimization layers. This paper proposes a differentiable layer, named Differentiable Frank-Wolfe Layer (DFWLayer), by rolling out the Frank-Wolfe method, a well-known optimization algorithm which can solve constrained optimization problems without projections and Hessian matrix computations, thus leading to a efficient way of dealing with large-scale problems. Theoretically, we establish a bound on the suboptimality gap of the DFWLayer in the context of l1-norm constraints. Experimental assessments demonstrate that the DFWLayer not only attains competitive accuracy in solutions and gradients but also consistently adheres to constraints. Moreover, it surpasses the baselines in both forward and backward computational speeds.
</details>
<details>
<summary>摘要</summary>
differentiable 优化在机器学习领域中得到了广泛的关注，因为它在神经网络上构建的机器学习模型中扮演了基础性的角色。现有的方法利用优化性条件和隐函数定理来获取输出的雅可比矩阵，这会增加计算成本并限制 differentiable 优化的应用。此外，一些非 differentiable 约束会使得使用先前的 differentiable 优化层更加困难。这篇论文提出了一种名为差分可 differentiable Frank-Wolfe 层（DFWLayer）的层，通过折衔 Frank-Wolfe 算法，该算法可以解决具有约束的优化问题，而不需要计算投影和资料矩阵，因此可以方便地处理大规模问题。理论上，我们提出了在 l1-norm 约束下的优化误差幅度的下界。实验评估表明，DFWLayer 不仅在解决方案和梯度方面达到了竞争性的准确性，而且一致地遵循约束。此外，它在前向和后向计算速度方面也超过了基eline。
</details></li>
</ul>
<hr>
<h2 id="Stabilizing-Unsupervised-Environment-Design-with-a-Learned-Adversary"><a href="#Stabilizing-Unsupervised-Environment-Design-with-a-Learned-Adversary" class="headerlink" title="Stabilizing Unsupervised Environment Design with a Learned Adversary"></a>Stabilizing Unsupervised Environment Design with a Learned Adversary</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10797">http://arxiv.org/abs/2308.10797</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/dcd">https://github.com/facebookresearch/dcd</a></li>
<li>paper_authors: Ishita Mediratta, Minqi Jiang, Jack Parker-Holder, Michael Dennis, Eugene Vinitsky, Tim Rocktäschel</li>
<li>for: 本研究旨在提高普通能力Agent的训练，通过设计适应环境变化的训练任务来促进广泛的普通化和稳定性。</li>
<li>methods: 本研究使用了强化学习（RL）来训练教师策略，从 scratch generate任务，使得可以直接生成适应当前机器人能力的任务。</li>
<li>results: 本研究在多个已知的难度 Navigation 和赛车环境中实现了与当前状态的比较或超越，生成了可靠的普通化Agent。<details>
<summary>Abstract</summary>
A key challenge in training generally-capable agents is the design of training tasks that facilitate broad generalization and robustness to environment variations. This challenge motivates the problem setting of Unsupervised Environment Design (UED), whereby a student agent trains on an adaptive distribution of tasks proposed by a teacher agent. A pioneering approach for UED is PAIRED, which uses reinforcement learning (RL) to train a teacher policy to design tasks from scratch, making it possible to directly generate tasks that are adapted to the agent's current capabilities. Despite its strong theoretical backing, PAIRED suffers from a variety of challenges that hinder its practical performance. Thus, state-of-the-art methods currently rely on curation and mutation rather than generation of new tasks. In this work, we investigate several key shortcomings of PAIRED and propose solutions for each shortcoming. As a result, we make it possible for PAIRED to match or exceed state-of-the-art methods, producing robust agents in several established challenging procedurally-generated environments, including a partially-observed maze navigation task and a continuous-control car racing environment. We believe this work motivates a renewed emphasis on UED methods based on learned models that directly generate challenging environments, potentially unlocking more open-ended RL training and, as a result, more general agents.
</details>
<details>
<summary>摘要</summary>
training 通常遇到的一个挑战是设计训练任务，以便在环境变化时能够广泛通用和Robust。这个挑战激发了无监督环境设计（UED）的问题，其中学生机器人通过一个教师机器人提供的 adaptive 任务分布进行训练。一种开拓性的方法是 PAIRED，它使用强化学习（RL）来训练一个教师策略，从scratch生成任务，使得可以直接生成适应机器人当前能力的任务。然而，PAIRED 受到多种挑战，这些挑战使得实际性受到影响。因此，当前的state-of-the-art方法通常采用审核和变化而不是生成新任务。在这种情况下，我们调查了 PAIRED 的一些关键缺陷，并提出了解决方案。结果是，我们使得 PAIRED 能够与当前的state-of-the-art方法匹配或超越，在一些已知的复杂生成环境中训练 robust 的机器人，包括部分观察 Maze 导航任务和连续控制汽车竞赛环境。我们认为这项工作将激励更多的 UED 方法基于学习的模型，直接生成挑战任务，可能解锁更多的 open-ended RL 训练和更一般的机器人。
</details></li>
</ul>
<hr>
<h2 id="MGMAE-Motion-Guided-Masking-for-Video-Masked-Autoencoding"><a href="#MGMAE-Motion-Guided-Masking-for-Video-Masked-Autoencoding" class="headerlink" title="MGMAE: Motion Guided Masking for Video Masked Autoencoding"></a>MGMAE: Motion Guided Masking for Video Masked Autoencoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10794">http://arxiv.org/abs/2308.10794</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bingkun Huang, Zhiyu Zhao, Guozhen Zhang, Yu Qiao, Limin Wang</li>
<li>For: The paper is focused on improving the performance of video masked autoencoding (MMAE) by incorporating motion information into the masking strategy.* Methods: The paper introduces a motion guided masking strategy that explicitly incorporates motion information to build temporal consistent masking volume. The authors also use an online efficient optical flow estimator and backward masking map warping strategy to implement their method.* Results: The authors demonstrate superior performance of their motion guided MMAE (MGMAE) compared to the original VideoMAE on the datasets of Something-Something V2 and Kinetics-400. They also provide visualization analysis to illustrate that their method can sample temporal consistent cubes in a motion-adaptive manner for more effective video pre-training.Here is the simplified Chinese text for the three key points:* 为：本文主要关注提高视频Masked Autoencoding（MMAE）性能，通过 incorporating 运动信息到Masking策略中。* 方法：本文引入运动指导Masking策略，其中Explicitly incorporates 运动信息构建 temporal consistent 的Masking volume。作者还使用了在线高效的Optical flow estimator和backward Masking map warping策略来实现方法。* 结果：作者在Something-Something V2和Kinetics-400 datasets上展示了MGMAE的超越VideoMAE性能。他们还提供了可视化分析，以 illustrate MGMAE可以在运动响应的方式采样时间一致的cubes，以便更有效地预训练视频。<details>
<summary>Abstract</summary>
Masked autoencoding has shown excellent performance on self-supervised video representation learning. Temporal redundancy has led to a high masking ratio and customized masking strategy in VideoMAE. In this paper, we aim to further improve the performance of video masked autoencoding by introducing a motion guided masking strategy. Our key insight is that motion is a general and unique prior in video, which should be taken into account during masked pre-training. Our motion guided masking explicitly incorporates motion information to build temporal consistent masking volume. Based on this masking volume, we can track the unmasked tokens in time and sample a set of temporal consistent cubes from videos. These temporal aligned unmasked tokens will further relieve the information leakage issue in time and encourage the MGMAE to learn more useful structure information. We implement our MGMAE with an online efficient optical flow estimator and backward masking map warping strategy. We perform experiments on the datasets of Something-Something V2 and Kinetics-400, demonstrating the superior performance of our MGMAE to the original VideoMAE. In addition, we provide the visualization analysis to illustrate that our MGMAE can sample temporal consistent cubes in a motion-adaptive manner for more effective video pre-training.
</details>
<details>
<summary>摘要</summary>
卷积自编码（Masked Autoencoding）在自我监督视频表示学习中表现出色。视频中的时间重复性导致了高的面罩率和自定义面罩策略，在这篇论文中，我们想要进一步提高视频卷积自编码的性能，通过引入运动指导的面罩策略。我们的关键发现是，运动是视频中一个通用和特殊的先验，应该在面罩预训练中考虑。我们的运动指导面罩 explictly incorporates 运动信息，建立了时间一致的面罩量。基于这个面罩量，我们可以在时间上跟踪不面罩的 токен，并从视频中采样一组时间一致的立方体。这些时间一致的不面罩的 токен将进一步减轻时间泄露问题，促使MGMAE学习更有用的结构信息。我们实现了我们的MGMAE，使用了在线高效的滤波器估计器和反向面罩地图抽象策略。我们在Something-Something V2和Kinetics-400 datasets上进行了实验，并证明了我们的MGMAE在原始VideoMAE的基础上具有更高的性能。此外，我们还提供了视觉分析，以 Illustrate 我们的MGMAE可以在运动适应的方式下采样时间一致的立方体，以更有效地进行视频预训练。
</details></li>
</ul>
<hr>
<h2 id="Instruction-Tuning-for-Large-Language-Models-A-Survey"><a href="#Instruction-Tuning-for-Large-Language-Models-A-Survey" class="headerlink" title="Instruction Tuning for Large Language Models: A Survey"></a>Instruction Tuning for Large Language Models: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10792">http://arxiv.org/abs/2308.10792</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, Guoyin Wang</li>
<li>for: 本文审查了大语言模型（LLM）的指令调整（IT）研究，以提高LLM的能力和可控性。</li>
<li>methods: 本文使用系统性的文献综述方法，包括IT的总方法、IT数据集的构建、IT模型的训练、不同Modalities、领域和应用程序中的应用，以及影响IT结果的因素的分析（如生成指令输出、指令数据集的大小等）。</li>
<li>results: 本文对IT的总体结果进行了分析，包括IT的应用、潜在的坏处和批评，以及现有策略的不足和未来研究的可能性。<details>
<summary>Abstract</summary>
This paper surveys research works in the quickly advancing field of instruction tuning (IT), a crucial technique to enhance the capabilities and controllability of large language models (LLMs). Instruction tuning refers to the process of further training LLMs on a dataset consisting of \textsc{(instruction, output)} pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of LLMs and the users' objective of having LLMs adhere to human instructions. In this work, we make a systematic review of the literature, including the general methodology of IT, the construction of IT datasets, the training of IT models, and applications to different modalities, domains and applications, along with an analysis on aspects that influence the outcome of IT (e.g., generation of instruction outputs, size of the instruction dataset, etc). We also review the potential pitfalls of IT along with criticism against it, along with efforts pointing out current deficiencies of existing strategies and suggest some avenues for fruitful research.
</details>
<details>
<summary>摘要</summary>
The paper provides a systematic review of the literature, including the general methodology of IT, the construction of IT datasets, the training of IT models, and applications to different modalities, domains, and applications. The review also analyzes factors that influence the outcome of IT, such as the generation of instruction outputs and the size of the instruction dataset.Furthermore, the paper discusses potential pitfalls of IT and criticism against it, as well as efforts to address current deficiencies in existing strategies and suggests avenues for fruitful research. Overall, the paper provides a comprehensive overview of the current state of IT research and its potential applications in various domains.Here is the translation in Simplified Chinese:这篇论文检讨了大语言模型（LLM）的指令调整（IT）研究，这是一种能够提高LLM的能力和可控性的重要技术。IT通过在指令集合（instruction, output）的supervised模式下进行进一步训练LLM， bridge了LLM的下一个词预测目标和用户的目标，即使LLM遵循人类的指令。论文提供了系统性的文献综述，包括IT的总方法、IT数据集的建构、IT模型的训练、以及不同的modalities、domains和应用程序中的应用。文献还分析了IT的结果受到的因素，如生成指令输出和指令集合的大小。此外，论文还讨论了IT的潜在弱点和批评，以及现有策略的不足和改进的可能性。文献还提出了一些有优势的研究方向。总的来说，论文提供了大语言模型研究的现状和IT研究的潜在应用领域。
</details></li>
</ul>
<hr>
<h2 id="Zero-and-Few-Shot-Prompting-with-LLMs-A-Comparative-Study-with-Fine-tuned-Models-for-Bangla-Sentiment-Analysis"><a href="#Zero-and-Few-Shot-Prompting-with-LLMs-A-Comparative-Study-with-Fine-tuned-Models-for-Bangla-Sentiment-Analysis" class="headerlink" title="Zero- and Few-Shot Prompting with LLMs: A Comparative Study with Fine-tuned Models for Bangla Sentiment Analysis"></a>Zero- and Few-Shot Prompting with LLMs: A Comparative Study with Fine-tuned Models for Bangla Sentiment Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10783">http://arxiv.org/abs/2308.10783</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md. Arid Hasan, Shudipta Das, Afiyat Anjum, Firoj Alam, Anika Anjum, Avijit Sarker, Sheak Rashed Haider Noori</li>
<li>for: 本研究旨在提供大量手动标注的孟加拉语新闻微博和Facebook评论数据集，以及对多种语言模型进行零、几shot在场景学习的研究。</li>
<li>methods: 本研究使用了零、几shot在场景学习的方法，包括Flan-T5、GPT-4和Bloomz等语言模型，并进行了对比研究。</li>
<li>results: 研究发现，单语言变换器模型在零、几shot场景下表现出了强劲的性能，超过其他模型。<details>
<summary>Abstract</summary>
The rapid expansion of the digital world has propelled sentiment analysis into a critical tool across diverse sectors such as marketing, politics, customer service, and healthcare. While there have been significant advancements in sentiment analysis for widely spoken languages, low-resource languages, such as Bangla, remain largely under-researched due to resource constraints. Furthermore, the recent unprecedented performance of Large Language Models (LLMs) in various applications highlights the need to evaluate them in the context of low-resource languages. In this study, we present a sizeable manually annotated dataset encompassing 33,605 Bangla news tweets and Facebook comments. We also investigate zero- and few-shot in-context learning with several language models, including Flan-T5, GPT-4, and Bloomz, offering a comparative analysis against fine-tuned models. Our findings suggest that monolingual transformer-based models consistently outperform other models, even in zero and few-shot scenarios. To foster continued exploration, we intend to make this dataset and our research tools publicly available to the broader research community. In the spirit of further research, we plan to make this dataset and our experimental resources publicly accessible to the wider research community.
</details>
<details>
<summary>摘要</summary>
随着数字世界的快速扩张，情感分析已成为多个领域的关键工具，包括市场营销、政治、客户服务和医疗等。虽然普通话和其他语言的情感分析得到了 significiant 进步，但低资源语言，如孟加拉语，仍然受到资源限制，因此它们尚未得到了足够的研究。此外，最近的不同应用中大型自然语言处理器（LLMs）的表现也提出了对低资源语言进行评估的需求。在本研究中，我们提供了一个大量手动标注的孟加拉语新闻微博和Facebook评论数据集，并进行了零、几个适应学习和翻译模型的研究。我们的发现表明，单语言转换器基于模型在零和几个适应场景中一直表现出色，even in zero and few-shot scenarios。为了促进进一步的探索，我们计划在未来公开这个数据集和我们的研究工具，以便更广泛的研究者社区可以进行更多的研究。
</details></li>
</ul>
<hr>
<h2 id="Sparse-Linear-Concept-Discovery-Models"><a href="#Sparse-Linear-Concept-Discovery-Models" class="headerlink" title="Sparse Linear Concept Discovery Models"></a>Sparse Linear Concept Discovery Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10782">http://arxiv.org/abs/2308.10782</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/konpanousis/conceptdiscoverymodels">https://github.com/konpanousis/conceptdiscoverymodels</a></li>
<li>paper_authors: Konstantinos P. Panousis, Dino Ienco, Diego Marcos</li>
<li>for: This paper aims to improve the interpretability of Deep Neural Networks (DNNs) by proposing a simple and intuitive framework based on Contrastive Language Image models and a single sparse linear layer.</li>
<li>methods: The proposed framework uses a data-driven Bernoulli distribution to infer concept presence and achieve sparsity in the model, which is a novel approach compared to previous CBM methods.</li>
<li>results: The proposed framework outperforms recent CBM approaches in accuracy and achieves high per-example concept sparsity, making it easier to investigate the emerging concepts.<details>
<summary>Abstract</summary>
The recent mass adoption of DNNs, even in safety-critical scenarios, has shifted the focus of the research community towards the creation of inherently intrepretable models. Concept Bottleneck Models (CBMs) constitute a popular approach where hidden layers are tied to human understandable concepts allowing for investigation and correction of the network's decisions. However, CBMs usually suffer from: (i) performance degradation and (ii) lower interpretability than intended due to the sheer amount of concepts contributing to each decision. In this work, we propose a simple yet highly intuitive interpretable framework based on Contrastive Language Image models and a single sparse linear layer. In stark contrast to related approaches, the sparsity in our framework is achieved via principled Bayesian arguments by inferring concept presence via a data-driven Bernoulli distribution. As we experimentally show, our framework not only outperforms recent CBM approaches accuracy-wise, but it also yields high per example concept sparsity, facilitating the individual investigation of the emerging concepts.
</details>
<details>
<summary>摘要</summary>
In this work, we propose a simple and highly intuitive interpretable framework based on Contrastive Language Image models and a single sparse linear layer. Unlike other approaches, our framework achieves sparsity through principled Bayesian arguments by inferring concept presence via a data-driven Bernoulli distribution. Our experiments show that our framework not only outperforms recent CBM approaches in terms of accuracy, but it also yields high per-example concept sparsity, making it easier to investigate the emerging concepts.Here is the text in Simplified Chinese:近些时间，深度神经网络（DNNs）在安全关键场景中广泛应用，导致研究人员强调创建可解释性强的模型。概念瓶颈模型（CBMs）是一种受欢迎的方法，它将隐藏层与人理解的概念绑定在一起，以便调查和修正网络决策的过程。然而，CBMs 通常会受到以下两个问题的影响：（i）性能下降，和（ii）解释性比预期更低，这是因为每个决策中的概念的数量过多。在这项工作中，我们提出了一种简单、易于理解的框架，基于对比语言图像模型和单个稀疏线性层。与相关方法不同，我们的框架中的稀疏性是通过原则性的极 bayesian 理由来实现的，通过数据驱动的 Берну利分布来判断概念存在。我们的实验表明，我们的框架不仅在准确性上超越了最近的 CBM 方法，而且每个例子的概念稀疏性也很高，使得可以轻松地调查出现的概念。
</details></li>
</ul>
<hr>
<h2 id="Mixed-Integer-Projections-for-Automated-Data-Correction-of-EMRs-Improve-Predictions-of-Sepsis-among-Hospitalized-Patients"><a href="#Mixed-Integer-Projections-for-Automated-Data-Correction-of-EMRs-Improve-Predictions-of-Sepsis-among-Hospitalized-Patients" class="headerlink" title="Mixed-Integer Projections for Automated Data Correction of EMRs Improve Predictions of Sepsis among Hospitalized Patients"></a>Mixed-Integer Projections for Automated Data Correction of EMRs Improve Predictions of Sepsis among Hospitalized Patients</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10781">http://arxiv.org/abs/2308.10781</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mehak Arora, Hassan Mortagy, Nathan Dwarshius, Swati Gupta, Andre L. Holder, Rishikesan Kamaleswaran</li>
<li>for: 这个研究目的是为了提高机器学习模型在医疗内容中自动化诊断决策的精度。</li>
<li>methods: 这篇研究使用了一种创新的投影方法，将诊断领域的专业知识融入机器学习工作流程中，生成重要的元数据，并通过高维混合整数程式来修正患者数据，以确保数据的正确性。</li>
<li>results: 研究结果显示，使用这种投影方法可以提高机器学习分类器在实际医疗 Setting中的性能，特别是在检测 septic shock 的早期检测中，AUROC 为 0.865，精度为 0.922，较无投影的机器学习模型更高。<details>
<summary>Abstract</summary>
Machine learning (ML) models are increasingly pivotal in automating clinical decisions. Yet, a glaring oversight in prior research has been the lack of proper processing of Electronic Medical Record (EMR) data in the clinical context for errors and outliers. Addressing this oversight, we introduce an innovative projections-based method that seamlessly integrates clinical expertise as domain constraints, generating important meta-data that can be used in ML workflows. In particular, by using high-dimensional mixed-integer programs that capture physiological and biological constraints on patient vitals and lab values, we can harness the power of mathematical "projections" for the EMR data to correct patient data. Consequently, we measure the distance of corrected data from the constraints defining a healthy range of patient data, resulting in a unique predictive metric we term as "trust-scores". These scores provide insight into the patient's health status and significantly boost the performance of ML classifiers in real-life clinical settings. We validate the impact of our framework in the context of early detection of sepsis using ML. We show an AUROC of 0.865 and a precision of 0.922, that surpasses conventional ML models without such projections.
</details>
<details>
<summary>摘要</summary>
机器学习（ML）模型在医疗决策自动化中日益重要。然而，在先前的研究中，缺乏对电子医疗记录（EMR）数据在临床上的正确处理是一大缺点。我们解决这一问题，引入了一种创新的投影方法，它可以融合临床专业知识作为领域约束，生成重要的元数据，可以在机器学习工作流中使用。具体来说，我们使用高维混合整数程序，捕捉了生物和生物学约束，以 Correct patient数据。然后，我们测量了对约束定义的健康范围内的 corrected data 的距离，得到了一个唯一的预测指标，我们称之为 "信任分数"。这些分数可以提供患者的健康状况信息，并在实际临床设置中显著提高机器学习分类器的性能。我们验证了我们的框架在早期识别 septic shock 中的效果，我们显示了 AUC 为 0.865，精度为 0.922，这些结果超过了不含投影的 ML 模型。
</details></li>
</ul>
<hr>
<h2 id="Spear-and-Shield-Adversarial-Attacks-and-Defense-Methods-for-Model-Based-Link-Prediction-on-Continuous-Time-Dynamic-Graphs"><a href="#Spear-and-Shield-Adversarial-Attacks-and-Defense-Methods-for-Model-Based-Link-Prediction-on-Continuous-Time-Dynamic-Graphs" class="headerlink" title="Spear and Shield: Adversarial Attacks and Defense Methods for Model-Based Link Prediction on Continuous-Time Dynamic Graphs"></a>Spear and Shield: Adversarial Attacks and Defense Methods for Model-Based Link Prediction on Continuous-Time Dynamic Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10779">http://arxiv.org/abs/2308.10779</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongjin Lee, Juho Lee, Kijung Shin</li>
<li>for: 这 paper 的目的是调查 Temporal Graph Neural Networks (TGNNs) 在静态图模型中的敏感性，并提出了一种简单而有效的针对链接预测任务的对抗攻击方法。</li>
<li>methods: 作者提出了一种名为 T-SPEAR 的对抗攻击方法，通过在预测过程之前对数据进行边扰动，使得预测模型失效。同时，作者还提出了一种名为 T-SHIELD 的Robust Training Approach，通过边筛选和时间稳定性来提高预测模型的Robustness。</li>
<li>results: 实验表明，T-SPEAR 可以很好地降低预测模型的链接预测性能，而且这些攻击可以转移到其他 TGNNs 上，即使它们与预测模型不同。此外，T-SHIELD 可以有效地筛选出对预测模型不良的边，并在对抗攻击时表现出更高的Robustness。<details>
<summary>Abstract</summary>
Real-world graphs are dynamic, constantly evolving with new interactions, such as financial transactions in financial networks. Temporal Graph Neural Networks (TGNNs) have been developed to effectively capture the evolving patterns in dynamic graphs. While these models have demonstrated their superiority, being widely adopted in various important fields, their vulnerabilities against adversarial attacks remain largely unexplored. In this paper, we propose T-SPEAR, a simple and effective adversarial attack method for link prediction on continuous-time dynamic graphs, focusing on investigating the vulnerabilities of TGNNs. Specifically, before the training procedure of a victim model, which is a TGNN for link prediction, we inject edge perturbations to the data that are unnoticeable in terms of the four constraints we propose, and yet effective enough to cause malfunction of the victim model. Moreover, we propose a robust training approach T-SHIELD to mitigate the impact of adversarial attacks. By using edge filtering and enforcing temporal smoothness to node embeddings, we enhance the robustness of the victim model. Our experimental study shows that T-SPEAR significantly degrades the victim model's performance on link prediction tasks, and even more, our attacks are transferable to other TGNNs, which differ from the victim model assumed by the attacker. Moreover, we demonstrate that T-SHIELD effectively filters out adversarial edges and exhibits robustness against adversarial attacks, surpassing the link prediction performance of the naive TGNN by up to 11.2% under T-SPEAR.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Modular-and-Adaptive-System-for-Business-Email-Compromise-Detection"><a href="#A-Modular-and-Adaptive-System-for-Business-Email-Compromise-Detection" class="headerlink" title="A Modular and Adaptive System for Business Email Compromise Detection"></a>A Modular and Adaptive System for Business Email Compromise Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10776">http://arxiv.org/abs/2308.10776</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jan Brabec, Filip Šrajer, Radek Starosta, Tomáš Sixta, Marc Dupont, Miloš Lenoch, Jiří Menšík, Florian Becker, Jakub Boros, Tomáš Pop, Pavel Novák</li>
<li>for: 防止商业电子邮件骗财（BEC）和特攻钓鱼攻击，帮助组织nings worldwide  Addressing the growing sophistication of Business Email Compromise (BEC) and spear phishing attacks, which pose significant challenges to organizations worldwide.</li>
<li>methods:  combining multiple machine learning approaches, including Natural Language Understanding (NLU), to detect BEC-related behaviors across various email modalities such as text, images, metadata, and the email’s communication context. </li>
<li>results:  CAPE, a comprehensive and efficient system for BEC detection that has been proven in a production environment for over two years, with naturally explainable verdicts. The system combines independent ML models and algorithms to detect BEC-related behaviors and adapts continuously through a Bayesian approach that combines limited data with domain knowledge.<details>
<summary>Abstract</summary>
The growing sophistication of Business Email Compromise (BEC) and spear phishing attacks poses significant challenges to organizations worldwide. The techniques featured in traditional spam and phishing detection are insufficient due to the tailored nature of modern BEC attacks as they often blend in with the regular benign traffic. Recent advances in machine learning, particularly in Natural Language Understanding (NLU), offer a promising avenue for combating such attacks but in a practical system, due to limitations such as data availability, operational costs, verdict explainability requirements or a need to robustly evolve the system, it is essential to combine multiple approaches together. We present CAPE, a comprehensive and efficient system for BEC detection that has been proven in a production environment for a period of over two years. Rather than being a single model, CAPE is a system that combines independent ML models and algorithms detecting BEC-related behaviors across various email modalities such as text, images, metadata and the email's communication context. This decomposition makes CAPE's verdicts naturally explainable. In the paper, we describe the design principles and constraints behind its architecture, as well as the challenges of model design, evaluation and adapting the system continuously through a Bayesian approach that combines limited data with domain knowledge. Furthermore, we elaborate on several specific behavioral detectors, such as those based on Transformer neural architectures.
</details>
<details>
<summary>摘要</summary>
企业电子邮件攻击（BEC）和攻击者针对邮件的攻击（spear phishing）的复杂度不断增加，对全球企业造成了严重的挑战。传统的防御策略，如防火墙和杀毒软件，已经无法满足现代BEC攻击的需求，因为这些攻击往往与正常的邮件交互混合在一起。现代机器学习技术，特别是自然语言理解（NLU），在抗击BEC攻击方面提供了一个有希望的方向。然而，在实践中，由于数据可用性、运营成本、评估透明度和持续性提高等因素，需要结合多种方法来实现。我们提出了CAPE，一个涵盖和高效的BEC检测系统，在生产环境中证明了超过两年的有效性。CAPE不是单一的模型，而是一个结合独立的机器学习模型和算法，检测电子邮件中的BEC相关行为，包括文本、图像、元数据和邮件交互Context。这种分解使得CAPE的评决具有自然的透明度。在这篇论文中，我们介绍了CAPE的设计原则和限制，以及模型设计、评估和持续性改进的挑战。此外，我们还详细介绍了一些特定的行为检测器，如基于Transformer нейロ网络架构的检测器。
</details></li>
</ul>
<hr>
<h2 id="GBM-based-Bregman-Proximal-Algorithms-for-Constrained-Learning"><a href="#GBM-based-Bregman-Proximal-Algorithms-for-Constrained-Learning" class="headerlink" title="GBM-based Bregman Proximal Algorithms for Constrained Learning"></a>GBM-based Bregman Proximal Algorithms for Constrained Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10767">http://arxiv.org/abs/2308.10767</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhenweilin/constrainedgbm">https://github.com/zhenweilin/constrainedgbm</a></li>
<li>paper_authors: Zhenwei Lin, Qi Deng<br>for:* The paper is focused on developing a new algorithm for constrained learning tasks in machine learning, specifically for Neyman-Pearson classification and fairness classification.methods:* The authors adapt the gradient boosting machine (GBM) algorithm for constrained learning tasks within the framework of Bregman proximal algorithms.* They introduce a new Bregman primal-dual method with a global optimality guarantee for convex learning objectives and constraint functions.results:* The authors provide substantial experimental evidence to showcase the effectiveness of the Bregman algorithm framework for NPC and fairness ML, and demonstrate its potential for a broader range of constrained learning applications.<details>
<summary>Abstract</summary>
As the complexity of learning tasks surges, modern machine learning encounters a new constrained learning paradigm characterized by more intricate and data-driven function constraints. Prominent applications include Neyman-Pearson classification (NPC) and fairness classification, which entail specific risk constraints that render standard projection-based training algorithms unsuitable. Gradient boosting machines (GBMs) are among the most popular algorithms for supervised learning; however, they are generally limited to unconstrained settings. In this paper, we adapt the GBM for constrained learning tasks within the framework of Bregman proximal algorithms. We introduce a new Bregman primal-dual method with a global optimality guarantee when the learning objective and constraint functions are convex. In cases of nonconvex functions, we demonstrate how our algorithm remains effective under a Bregman proximal point framework. Distinct from existing constrained learning algorithms, ours possess a unique advantage in their ability to seamlessly integrate with publicly available GBM implementations such as XGBoost (Chen and Guestrin, 2016) and LightGBM (Ke et al., 2017), exclusively relying on their public interfaces. We provide substantial experimental evidence to showcase the effectiveness of the Bregman algorithm framework. While our primary focus is on NPC and fairness ML, our framework holds significant potential for a broader range of constrained learning applications. The source code is currently freely available at https://github.com/zhenweilin/ConstrainedGBM}{https://github.com/zhenweilin/ConstrainedGBM.
</details>
<details>
<summary>摘要</summary>
As the complexity of learning tasks increases, modern machine learning encounters a new constrained learning paradigm with more intricate and data-driven function constraints. Prominent applications include Neyman-Pearson classification (NPC) and fairness classification, which involve specific risk constraints that render standard projection-based training algorithms unsuitable. Gradient boosting machines (GBMs) are among the most popular algorithms for supervised learning; however, they are generally limited to unconstrained settings. In this paper, we adapt the GBM for constrained learning tasks within the framework of Bregman proximal algorithms. We introduce a new Bregman primal-dual method with a global optimality guarantee when the learning objective and constraint functions are convex. In cases of nonconvex functions, we demonstrate how our algorithm remains effective under a Bregman proximal point framework. Distinct from existing constrained learning algorithms, ours possess a unique advantage in their ability to seamlessly integrate with publicly available GBM implementations such as XGBoost (Chen and Guestrin, 2016) and LightGBM (Ke et al., 2017), exclusively relying on their public interfaces. We provide substantial experimental evidence to showcase the effectiveness of the Bregman algorithm framework. While our primary focus is on NPC and fairness ML, our framework holds significant potential for a broader range of constrained learning applications. The source code is currently freely available at <https://github.com/zhenweilin/ConstrainedGBM>.
</details></li>
</ul>
<hr>
<h2 id="To-Whom-are-You-Talking-A-Deep-Learning-Model-to-Endow-Social-Robots-with-Addressee-Estimation-Skills"><a href="#To-Whom-are-You-Talking-A-Deep-Learning-Model-to-Endow-Social-Robots-with-Addressee-Estimation-Skills" class="headerlink" title="To Whom are You Talking? A Deep Learning Model to Endow Social Robots with Addressee Estimation Skills"></a>To Whom are You Talking? A Deep Learning Model to Endow Social Robots with Addressee Estimation Skills</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10757">http://arxiv.org/abs/2308.10757</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carlo Mazzola, Marta Romeo, Francesco Rea, Alessandra Sciutti, Angelo Cangelosi</li>
<li>for: 本研究旨在理解人类对话中的 адресат估计问题，以便在人机交互中使用机器人。</li>
<li>methods: 本研究使用了一种hybrid深度学习模型，组合了卷积层和LSTM细胞，将说话者的面部表情和姿势vector作为输入。</li>
<li>results: 研究表明，该模型能够在 robot egocentric 视角下解决 addressee 的本地化问题。<details>
<summary>Abstract</summary>
Communicating shapes our social word. For a robot to be considered social and being consequently integrated in our social environment it is fundamental to understand some of the dynamics that rule human-human communication. In this work, we tackle the problem of Addressee Estimation, the ability to understand an utterance's addressee, by interpreting and exploiting non-verbal bodily cues from the speaker. We do so by implementing an hybrid deep learning model composed of convolutional layers and LSTM cells taking as input images portraying the face of the speaker and 2D vectors of the speaker's body posture. Our implementation choices were guided by the aim to develop a model that could be deployed on social robots and be efficient in ecological scenarios. We demonstrate that our model is able to solve the Addressee Estimation problem in terms of addressee localisation in space, from a robot ego-centric point of view.
</details>
<details>
<summary>摘要</summary>
人际交流 shapes our 社会语言。为了让机器人被视为社交的并被顺利地整合到我们的社交环境中，我们必须理解一些人类对人交流的dinamics。在这项工作中，我们面临了Addresssee Estimation问题，即理解说话者的utterance的接收人，通过解释和利用说话者的非语言性肢体姿势。我们通过实施一种hybrid深度学习模型，组合 convolutional层和LSTM细胞，将说话者的脸部图像和说话者的身姿 вектор作为输入。我们的实现选择是根据目标开发一种可部署于社交机器人上的模型，并能在生态环境中高效地解决问题。我们示示了我们的模型能够解决Addresssee Estimation问题，从机器人自身的视角来看，对话者的位置在空间中。
</details></li>
</ul>
<hr>
<h2 id="On-the-Adversarial-Robustness-of-Multi-Modal-Foundation-Models"><a href="#On-the-Adversarial-Robustness-of-Multi-Modal-Foundation-Models" class="headerlink" title="On the Adversarial Robustness of Multi-Modal Foundation Models"></a>On the Adversarial Robustness of Multi-Modal Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10741">http://arxiv.org/abs/2308.10741</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christian Schlarmann, Matthias Hein</li>
<li>for: 保护用户免受恶意内容的误导和害害</li>
<li>methods: 使用隐形攻击破坏图像，改变多模态基础模型的描述输出</li>
<li>results: 显示了恶意内容提供者可以使用隐形攻击诱导善良用户访问恶势力网站或播报假信息，需要对多模态基础模型进行防御性减噪措施<details>
<summary>Abstract</summary>
Multi-modal foundation models combining vision and language models such as Flamingo or GPT-4 have recently gained enormous interest. Alignment of foundation models is used to prevent models from providing toxic or harmful output. While malicious users have successfully tried to jailbreak foundation models, an equally important question is if honest users could be harmed by malicious third-party content. In this paper we show that imperceivable attacks on images in order to change the caption output of a multi-modal foundation model can be used by malicious content providers to harm honest users e.g. by guiding them to malicious websites or broadcast fake information. This indicates that countermeasures to adversarial attacks should be used by any deployed multi-modal foundation model.
</details>
<details>
<summary>摘要</summary>
多modal基础模型，如FLAMINGO或GPT-4，在最近受到了巨大的关注。对基础模型的Alignment用于防止模型提供有害或肤浅的输出。然而，有人试图使用恶意内容来攻击基础模型，这也是一个非常重要的问题。在这篇论文中，我们展示了如何使用图像攻击来改变多modal基础模型的描述输出，从而导致善意用户被诱导到有害网站或接受假信息。这表明，在部署多modal基础模型时应该使用防御性攻击的countermeasure。
</details></li>
</ul>
<hr>
<h2 id="We-Don’t-Need-No-Adam-All-We-Need-Is-EVE-On-The-Variance-of-Dual-Learning-Rate-And-Beyond"><a href="#We-Don’t-Need-No-Adam-All-We-Need-Is-EVE-On-The-Variance-of-Dual-Learning-Rate-And-Beyond" class="headerlink" title="We Don’t Need No Adam, All We Need Is EVE: On The Variance of Dual Learning Rate And Beyond"></a>We Don’t Need No Adam, All We Need Is EVE: On The Variance of Dual Learning Rate And Beyond</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10740">http://arxiv.org/abs/2308.10740</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/akhadangi/EVE">https://github.com/akhadangi/EVE</a></li>
<li>paper_authors: Afshin Khadangi</li>
<li>for: 这篇论文是为了优化深度学习模型而写的。</li>
<li>methods: 这篇论文提出了一种新的方法，即增强速度估计（EVE），它利用不同的学习率来区分不同的组件。</li>
<li>results: 实验表明，EVE方法可以快速地 converge 并且在不同的数据集和架构上表现出色，与现有的优化技术相比有所提高。<details>
<summary>Abstract</summary>
In the rapidly advancing field of deep learning, optimising deep neural networks is paramount. This paper introduces a novel method, Enhanced Velocity Estimation (EVE), which innovatively applies different learning rates to distinct components of the gradients. By bifurcating the learning rate, EVE enables more nuanced control and faster convergence, addressing the challenges associated with traditional single learning rate approaches. Utilising a momentum term that adapts to the learning landscape, the method achieves a more efficient navigation of the complex loss surface, resulting in enhanced performance and stability. Extensive experiments demonstrate that EVE significantly outperforms existing optimisation techniques across various benchmark datasets and architectures.
</details>
<details>
<summary>摘要</summary>
在深度学习领域的快速发展中，优化深度神经网络非常重要。本文介绍了一种新方法，即增强速度估计（EVE），它创新地将不同的学习率应用到不同的梯度组件。通过分化学习率，EVE允许更细化的控制和更快的 converges，解决传统单学习率方法所遇到的挑战。利用适应学习地面的旋转矩阵，方法实现了更高效地导航复杂的损失函数表面，从而提高性能和稳定性。广泛的实验表明，EVEsignificantly exceeds现有优化技术在各种 benchmark 数据集和架构上。
</details></li>
</ul>
<hr>
<h2 id="UGSL-A-Unified-Framework-for-Benchmarking-Graph-Structure-Learning"><a href="#UGSL-A-Unified-Framework-for-Benchmarking-Graph-Structure-Learning" class="headerlink" title="UGSL: A Unified Framework for Benchmarking Graph Structure Learning"></a>UGSL: A Unified Framework for Benchmarking Graph Structure Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10737">http://arxiv.org/abs/2308.10737</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/google-research/google-research">https://github.com/google-research/google-research</a></li>
<li>paper_authors: Bahare Fatemi, Sami Abu-El-Haija, Anton Tsitsulin, Mehran Kazemi, Dustin Zelle, Neslihan Bulut, Jonathan Halcrow, Bryan Perozzi</li>
<li>for: 本研究旨在提供一个统一的框架，用于评估图structured learning模型的效果。</li>
<li>methods: 本研究使用了多种现有的图structured learning模型，并对这些模型进行了广泛的分析和比较。</li>
<li>results: 研究结果表明，不同的模型在不同的情况下具有不同的优劣点，并提供了一个清晰和简洁的理解于这些模型的效果。<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) demonstrate outstanding performance in a broad range of applications. While the majority of GNN applications assume that a graph structure is given, some recent methods substantially expanded the applicability of GNNs by showing that they may be effective even when no graph structure is explicitly provided. The GNN parameters and a graph structure are jointly learned. Previous studies adopt different experimentation setups, making it difficult to compare their merits. In this paper, we propose a benchmarking strategy for graph structure learning using a unified framework. Our framework, called Unified Graph Structure Learning (UGSL), reformulates existing models into a single model. We implement a wide range of existing models in our framework and conduct extensive analyses of the effectiveness of different components in the framework. Our results provide a clear and concise understanding of the different methods in this area as well as their strengths and weaknesses. The benchmark code is available at https://github.com/google-research/google-research/tree/master/ugsl.
</details>
<details>
<summary>摘要</summary>
图 neural network (GNN) 在广泛的应用领域中表现出色。大多数 GNN 应用假设给定的图结构，但一些最近的方法已经大幅扩展了 GNN 的可应用范围，显示它们可以在没有明确提供图结构的情况下也是有效的。在这些方法中，GNN 参数和图结构都是共同学习的。先前的研究采用了不同的实验设置，使得比较他们的优劣很Difficult。本文提出了一种图结构学习的 benchmarking 策略，称为 Unified Graph Structure Learning (UGSL)。我们将现有的模型重新表述为单一的模型，并在这个框架中实现了广泛的现有模型。我们进行了广泛的现有模型的分析，并对不同的组件的效果进行了广泛的分析。我们的结果为这一领域的不同方法、优劣点提供了清晰和简洁的理解。代码可以在 https://github.com/google-research/google-research/tree/master/ugsl 中下载。
</details></li>
</ul>
<hr>
<h2 id="Artificial-intelligence-driven-antimicrobial-peptide-discovery"><a href="#Artificial-intelligence-driven-antimicrobial-peptide-discovery" class="headerlink" title="Artificial intelligence-driven antimicrobial peptide discovery"></a>Artificial intelligence-driven antimicrobial peptide discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10921">http://arxiv.org/abs/2308.10921</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paulina Szymczak, Ewa Szczurek</li>
<li>for: 抗微生物蛋白（AMPs）作为替代性抗生素，提供了一种新的抗菌抗性材料。</li>
<li>methods: 人工智能（AI）在AMP发现中扮演了关键角色，通过对优秀候选者的预测和材料的生成。</li>
<li>results: AI在AMP发现中取得了 significiant advances，包括预测活性和药理性，以及生成新的AMP候选者。<details>
<summary>Abstract</summary>
Antimicrobial peptides (AMPs) emerge as promising agents against antimicrobial resistance, providing an alternative to conventional antibiotics. Artificial intelligence (AI) revolutionized AMP discovery through both discrimination and generation approaches. The discriminators aid the identification of promising candidates by predicting key peptide properties such as activity and toxicity, while the generators learn the distribution over peptides and enable sampling novel AMP candidates, either de novo, or as analogues of a prototype peptide. Moreover, the controlled generation of AMPs with desired properties is achieved by discriminator-guided filtering, positive-only learning, latent space sampling, as well as conditional and optimized generation. Here we review recent achievements in AI-driven AMP discovery, highlighting the most exciting directions.
</details>
<details>
<summary>摘要</summary>
antimicrobial peptides (AMPs) emerge as promising agents against antimicrobial resistance, providing an alternative to conventional antibiotics. artificial intelligence (AI) revolutionized AMP discovery through both discrimination and generation approaches. the discriminators aid the identification of promising candidates by predicting key peptide properties such as activity and toxicity, while the generators learn the distribution over peptides and enable sampling novel AMP candidates, either de novo, or as analogues of a prototype peptide. Moreover, the controlled generation of AMPs with desired properties is achieved by discriminator-guided filtering, positive-only learning, latent space sampling, as well as conditional and optimized generation. here we review recent achievements in AI-driven AMP discovery, highlighting the most exciting directions.Note: Simplified Chinese is used here as the text is intended for a general audience and not for academic or technical purposes. If you need a more formal or precise translation, please let me know and I can provide it in Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="What’s-Race-Got-to-do-with-it-Predicting-Youth-Depression-Across-Racial-Groups-Using-Machine-and-Deep-Learning"><a href="#What’s-Race-Got-to-do-with-it-Predicting-Youth-Depression-Across-Racial-Groups-Using-Machine-and-Deep-Learning" class="headerlink" title="What’s Race Got to do with it? Predicting Youth Depression Across Racial Groups Using Machine and Deep Learning"></a>What’s Race Got to do with it? Predicting Youth Depression Across Racial Groups Using Machine and Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11591">http://arxiv.org/abs/2308.11591</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nathan Zhong, Nikhil Yadav</li>
<li>for: 本研究旨在透过机器学习（ML）和人工神经网络（ANN）模型，识别高中生中的抑郁症状。</li>
<li>methods: 本研究使用了全国青少年风险行为调查系统（YRBSS）调查数据，并对不同的种族 subgroup 进行了不同的分类。</li>
<li>results: 研究发现，不同的种族 subgroup 有不同的参量，这些参量对于识别抑郁症状具有不同的重要性。ANN模型在整个数据集上得到了82.90%的F1分数，而最佳表现的机器学习模型支持向量机器（SVM）则得到了81.90%的F1分数。<details>
<summary>Abstract</summary>
Depression is a common yet serious mental disorder that affects millions of U.S. high schoolers every year. Still, accurate diagnosis and early detection remain significant challenges. In the field of public health, research shows that neural networks produce promising results in identifying other diseases such as cancer and HIV. This study proposes a similar approach, utilizing machine learning (ML) and artificial neural network (ANN) models to classify depression in a student. Additionally, the study highlights the differences in relevant factors for race subgroups and advocates the need for more extensive and diverse datasets. The models train on nationwide Youth Risk Behavior Surveillance System (YRBSS) survey data, in which the most relevant factors of depression are found with statistical analysis. The survey data is a structured dataset with 15000 entries including three race subsets each consisting of 900 entries. For classification, the research problem is modeled as a supervised learning binary classification problem. Factors relevant to depression for different racial subgroups are also identified. The ML and ANN models are trained on the entire dataset followed by different race subsets to classify whether an individual has depression. The ANN model achieves the highest F1 score of 82.90% while the best-performing machine learning model, support vector machines (SVM), achieves a score of 81.90%. This study reveals that different parameters are more valuable for modeling depression across diverse racial groups and furthers research regarding American youth depression.
</details>
<details>
<summary>摘要</summary>
每年数百万美国高中生都会患上抑郁症，但确定诊断和早期发现仍然是一项大型挑战。在公共卫生领域，研究表明，神经网络生成出了对其他疾病，如癌症和HIV，的识别promising results。这个研究提议了类似的方法，使用机器学习（ML）和人工神经网络（ANN）模型来识别高中生中的抑郁。此外，研究还 highlights the differences in relevant factors for different racial subgroups and advocates the need for more extensive and diverse datasets.研究使用全国青年风险行为监测系统（YRBSS）调查数据进行训练，该数据包括3个种族 subsets，每个 subsets包含900个数据。为了进行分类，研究问题被模型为一个指导学习binary classification问题。对于不同的种族 subgroup，研究还 indentified the factors relevant to depression.使用整个数据集和不同种族 subsets进行训练，ANN模型 achieve最高的F1分数为82.90%，而最佳performing机器学习模型，support vector machines（SVM）， achieve分数为81.90%。这个研究表明，不同种族 subgroup的参数在模型抑郁有所不同，并且推动了美国青年抑郁的进一步研究。
</details></li>
</ul>
<hr>
<h2 id="Test-time-augmentation-based-active-learning-and-self-training-for-label-efficient-segmentation"><a href="#Test-time-augmentation-based-active-learning-and-self-training-for-label-efficient-segmentation" class="headerlink" title="Test-time augmentation-based active learning and self-training for label-efficient segmentation"></a>Test-time augmentation-based active learning and self-training for label-efficient segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10727">http://arxiv.org/abs/2308.10727</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bella Specktor-Fadida, Anna Levchakov, Dana Schonberger, Liat Ben-Sira, Dafna Ben-Bashat, Leo Joskowicz</li>
<li>for: 这个论文目的是提出一种 combining 自动学习（AL）和自我教学（ST）方法，以减轻数据注释的卷积 neural network 的Annotation burden。</li>
<li>methods: 这个论文使用的方法包括 Test-Time Augmentations（TTA）、自动学习（AL）和自我教学（ST）。</li>
<li>results: 研究结果显示，ST 是对 MRI 胚胎体和 Placenta 分割任务的非常有效的，可以提高 ID 和 OOD 数据的性能。但是，对于单序列胚胎体分割任务，搅拌自动学习可以提高性能，而对于多序列 Placenta 分割任务，搅拌自我教学可以提高性能。此外，AL 在高变化 Placenta 数据上是有帮助的，但是对于单序列胚胎体数据，AL 不能提高性能。<details>
<summary>Abstract</summary>
Deep learning techniques depend on large datasets whose annotation is time-consuming. To reduce annotation burden, the self-training (ST) and active-learning (AL) methods have been developed as well as methods that combine them in an iterative fashion. However, it remains unclear when each method is the most useful, and when it is advantageous to combine them. In this paper, we propose a new method that combines ST with AL using Test-Time Augmentations (TTA). First, TTA is performed on an initial teacher network. Then, cases for annotation are selected based on the lowest estimated Dice score. Cases with high estimated scores are used as soft pseudo-labels for ST. The selected annotated cases are trained with existing annotated cases and ST cases with border slices annotations. We demonstrate the method on MRI fetal body and placenta segmentation tasks with different data variability characteristics. Our results indicate that ST is highly effective for both tasks, boosting performance for in-distribution (ID) and out-of-distribution (OOD) data. However, while self-training improved the performance of single-sequence fetal body segmentation when combined with AL, it slightly deteriorated performance of multi-sequence placenta segmentation on ID data. AL was helpful for the high variability placenta data, but did not improve upon random selection for the single-sequence body data. For fetal body segmentation sequence transfer, combining AL with ST following ST iteration yielded a Dice of 0.961 with only 6 original scans and 2 new sequence scans. Results using only 15 high-variability placenta cases were similar to those using 50 cases. Code is available at: https://github.com/Bella31/TTA-quality-estimation-ST-AL
</details>
<details>
<summary>摘要</summary>
深度学习技术需要大量的数据集，其标注是时间consuming的。为了减轻标注压力，自适应学习（ST）和活动学习（AL）方法已经被开发出来，同时也有将其结合在迭代方式下的方法。然而，尚未确定哪些情况下使用哪一种方法是最有用，并且何时结合它们是有利的。在这篇论文中，我们提出了一种新的方法，即将ST与AL结合使用测试时间扩展（TTA）。首先，TTA被应用于初始教师网络。然后，根据最低估计的 dice 分数选择标注案例。高估分数的案例用作软件 Pseudo-标注，并与现有标注案例和ST案例进行训练。我们在MRI胎体和胎盘分割任务上进行了实验，并通过不同数据特征来评估我们的方法。我们的结果表明，ST具有高效性，对于ID和OOD数据都有提高性。然而，当与AL结合使用时，单个序列胎体分割的性能有所下降，而多个序列胎盘分割的性能则没有得到提高。AL对高变化胎盘数据是有帮助的，但对单个序列胎体数据没有提高。在胎体分割序列传输中，将AL与ST结合使用，并在ST迭代后进行AL，可以达到0.961的Dice值，只需要6个原始扫描和2个新序列扫描。在50个高变化胎盘案例中，结果与使用50个案例相同。相关代码可以在GitHub上找到：https://github.com/Bella31/TTA-quality-estimation-ST-AL。
</details></li>
</ul>
<hr>
<h2 id="Clustered-Linear-Contextual-Bandits-with-Knapsacks"><a href="#Clustered-Linear-Contextual-Bandits-with-Knapsacks" class="headerlink" title="Clustered Linear Contextual Bandits with Knapsacks"></a>Clustered Linear Contextual Bandits with Knapsacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10722">http://arxiv.org/abs/2308.10722</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yichuan Deng, Michalis Mamakos, Zhao Song</li>
<li>for: 本研究studies clustered contextual bandits, where rewards and resource consumption are the outcomes of cluster-specific linear models.</li>
<li>methods: 该算法使用了 clustering 技术，并 combining techniques from the literature of econometrics and of bandits with constraints.</li>
<li>results: 该算法可以 achieve regret sublinear in the number of time periods, without requiring access to all of the arms.<details>
<summary>Abstract</summary>
In this work, we study clustered contextual bandits where rewards and resource consumption are the outcomes of cluster-specific linear models. The arms are divided in clusters, with the cluster memberships being unknown to an algorithm. Pulling an arm in a time period results in a reward and in consumption for each one of multiple resources, and with the total consumption of any resource exceeding a constraint implying the termination of the algorithm. Thus, maximizing the total reward requires learning not only models about the reward and the resource consumption, but also cluster memberships. We provide an algorithm that achieves regret sublinear in the number of time periods, without requiring access to all of the arms. In particular, we show that it suffices to perform clustering only once to a randomly selected subset of the arms. To achieve this result, we provide a sophisticated combination of techniques from the literature of econometrics and of bandits with constraints.
</details>
<details>
<summary>摘要</summary>
在这个研究中，我们研究集中的上下文抽奖问题，其中奖励和资源消耗是集中的线性模型的结果。武器被分成集群，集群成员身份未知于算法。在一个时间段内抽一把武器会得到奖励和每种多种资源的消耗，而某些资源的总消耗超过限制，则算法将被终止。因此，最大化总奖励需要学习不仅奖励和资源消耗，还需要集群成员身份。我们提供一种算法，其 regret 是时间段数量的下界，不需要所有武器的访问。具体来说，我们表明只需要对随机选择的一 subset of 武器进行分 clustering 即可。为 достичь这一结果，我们 combinated 了文献中的 econometrics 和抽奖问题的技术。
</details></li>
</ul>
<hr>
<h2 id="CoMIX-A-Multi-agent-Reinforcement-Learning-Training-Architecture-for-Efficient-Decentralized-Coordination-and-Independent-Decision-Making"><a href="#CoMIX-A-Multi-agent-Reinforcement-Learning-Training-Architecture-for-Efficient-Decentralized-Coordination-and-Independent-Decision-Making" class="headerlink" title="CoMIX: A Multi-agent Reinforcement Learning Training Architecture for Efficient Decentralized Coordination and Independent Decision Making"></a>CoMIX: A Multi-agent Reinforcement Learning Training Architecture for Efficient Decentralized Coordination and Independent Decision Making</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10721">http://arxiv.org/abs/2308.10721</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giovanni Minelli, Mirco Musolesi</li>
<li>for: 这篇论文的目的是提出一种基于协调的多代理人训练框架，以便在共享环境中协同完成共同目标，同时保持每个代理人的独立决策能力。</li>
<li>methods: 这篇论文提出了一种名为Coordinated QMIX（CoMIX）的训练框架，该框架基于协调的策略，允许每个代理人在决策过程中独立地做出决定，同时能够适应不同的情况，保持独立和协同的平衡。</li>
<li>results: 实验结果表明，CoMIX在协同任务上表现出色，比基线方法高效。这 validate了 authors 的增量策略方法是一种有效的协调提高多代理人系统的技术。<details>
<summary>Abstract</summary>
Robust coordination skills enable agents to operate cohesively in shared environments, together towards a common goal and, ideally, individually without hindering each other's progress. To this end, this paper presents Coordinated QMIX (CoMIX), a novel training framework for decentralized agents that enables emergent coordination through flexible policies, allowing at the same time independent decision-making at individual level. CoMIX models selfish and collaborative behavior as incremental steps in each agent's decision process. This allows agents to dynamically adapt their behavior to different situations balancing independence and collaboration. Experiments using a variety of simulation environments demonstrate that CoMIX outperforms baselines on collaborative tasks. The results validate our incremental policy approach as effective technique for improving coordination in multi-agent systems.
</details>
<details>
<summary>摘要</summary>
<<SYS>> transtable text into Simplified Chinese.<</SYS>> Robust coordination skills allow agents to work together in shared environments, working towards a common goal and, ideally, making decisions independently without hindering each other's progress. To achieve this, this paper presents Coordinated QMIX (CoMIX), a novel training framework for decentralized agents that enables emergent coordination through flexible policies, allowing for independent decision-making at the individual level. CoMIX models selfish and collaborative behavior as incremental steps in each agent's decision process, allowing agents to adapt their behavior dynamically to different situations, balancing independence and collaboration. Experimental results using a variety of simulation environments show that CoMIX outperforms baselines on collaborative tasks, validating our incremental policy approach as an effective technique for improving coordination in multi-agent systems.
</details></li>
</ul>
<hr>
<h2 id="Relax-and-penalize-a-new-bilevel-approach-to-mixed-binary-hyperparameter-optimization"><a href="#Relax-and-penalize-a-new-bilevel-approach-to-mixed-binary-hyperparameter-optimization" class="headerlink" title="Relax and penalize: a new bilevel approach to mixed-binary hyperparameter optimization"></a>Relax and penalize: a new bilevel approach to mixed-binary hyperparameter optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10711">http://arxiv.org/abs/2308.10711</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marianna de Santis, Jordan Frecon, Francesco Rinaldi, Saverio Salzo, Martin Schmidt</li>
<li>for: 这篇论文是为了提出一种能够有效地优化高维度机器学习模型的混合二级方法的研究。</li>
<li>methods: 该论文使用了一种基于罚项的等值 continous bilevel reformulation 方法，以解决现有的二级参数问题。</li>
<li>results: 该论文的实验结果表明，使用该方法可以对 regression 问题中的组 sparse 结构进行优化，并且比现有的 relaxation 和 rounding 方法表现更好。<details>
<summary>Abstract</summary>
In recent years, bilevel approaches have become very popular to efficiently estimate high-dimensional hyperparameters of machine learning models. However, to date, binary parameters are handled by continuous relaxation and rounding strategies, which could lead to inconsistent solutions. In this context, we tackle the challenging optimization of mixed-binary hyperparameters by resorting to an equivalent continuous bilevel reformulation based on an appropriate penalty term. We propose an algorithmic framework that, under suitable assumptions, is guaranteed to provide mixed-binary solutions. Moreover, the generality of the method allows to safely use existing continuous bilevel solvers within the proposed framework. We evaluate the performance of our approach for a specific machine learning problem, i.e., the estimation of the group-sparsity structure in regression problems. Reported results clearly show that our method outperforms state-of-the-art approaches based on relaxation and rounding
</details>
<details>
<summary>摘要</summary>
近年来，二级方法已经非常受欢迎地用于高维度机器学习模型参数的效率估算。然而，至今为止，Binary参数都是通过连续弹簧和圆拟法来处理，这可能会导致不一致的解决方案。在这个上下文中，我们解决了高维度混合 Binary 参数的困难优化问题，通过适当的罚项来转化为连续二级 reformulation。我们提出了一个算法框架，在适当的假设下，可以保证提供混合 Binary 解决方案。此外，我们的方法总体来说具有一致性和可靠性，可以安全地使用现有的连续二级解决方案。我们对一个特定的机器学习问题，即回归问题中的集群稀疏结构估算进行评估。报告的结果显示，我们的方法明显超过了当前的 relaxation 和圆拟法所能达到的性能。
</details></li>
</ul>
<hr>
<h2 id="Measuring-the-Effect-of-Causal-Disentanglement-on-the-Adversarial-Robustness-of-Neural-Network-Models"><a href="#Measuring-the-Effect-of-Causal-Disentanglement-on-the-Adversarial-Robustness-of-Neural-Network-Models" class="headerlink" title="Measuring the Effect of Causal Disentanglement on the Adversarial Robustness of Neural Network Models"></a>Measuring the Effect of Causal Disentanglement on the Adversarial Robustness of Neural Network Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10708">http://arxiv.org/abs/2308.10708</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/prebenness/causal_disentanglement_robustness">https://github.com/prebenness/causal_disentanglement_robustness</a></li>
<li>paper_authors: Preben M. Ness, Dusica Marijan, Sunanda Bose</li>
<li>for: 这些论文主要用于研究 causal Neural Network 模型在针对性攻击方面的Robustness和通用性能。</li>
<li>methods: 这些模型使用了 causal disentanglement 来提高其 Robustness和通用性能，并使用了 Computer Vision 领域的内容&#x2F;风格分离指标来衡量 causal disentanglement。</li>
<li>results: 研究发现， causal Neural Network 模型在针对性攻击方面的 Robustness 与模型 decorrelate causal 和干扰信号的程度有很强的相关性（r&#x3D;0.820， p&#x3D;0.001），同时也发现隐藏信号的像素级信息含量与针对性攻击的Robustness有负相关性（r&#x3D;-0.597， p&#x3D;0.040）。<details>
<summary>Abstract</summary>
Causal Neural Network models have shown high levels of robustness to adversarial attacks as well as an increased capacity for generalisation tasks such as few-shot learning and rare-context classification compared to traditional Neural Networks. This robustness is argued to stem from the disentanglement of causal and confounder input signals. However, no quantitative study has yet measured the level of disentanglement achieved by these types of causal models or assessed how this relates to their adversarial robustness.   Existing causal disentanglement metrics are not applicable to deterministic models trained on real-world datasets. We, therefore, utilise metrics of content/style disentanglement from the field of Computer Vision to measure different aspects of the causal disentanglement for four state-of-the-art causal Neural Network models. By re-implementing these models with a common ResNet18 architecture we are able to fairly measure their adversarial robustness on three standard image classification benchmarking datasets under seven common white-box attacks. We find a strong association (r=0.820, p=0.001) between the degree to which models decorrelate causal and confounder signals and their adversarial robustness. Additionally, we find a moderate negative association between the pixel-level information content of the confounder signal and adversarial robustness (r=-0.597, p=0.040).
</details>
<details>
<summary>摘要</summary>
causal neural network 模型在针对敌意攻击方面表现出了高水平的 robustness，以及在几个类型的泛化任务中提高了能力，如少量学习和罕见情况分类。这种 robustness 被认为是由 causal 和干扰输入信号的分离所带来的。然而，现在没有任何量化研究 measure 了这些类型的 causal 模型中的分离水平，或者如何这与其针对敌意攻击的 robustness 相关。现有的 causal 分离度量不适用于 deterministic 模型在实际数据集上的训练。我们因此利用计算机视觉领域中的内容/风格分离度量来测量不同 causal 模型的不同方面的分离度。通过对四种 state-of-the-art causal neural network 模型重新实现，并使用共同的 ResNet18 架构，我们可以公平地测量它们在三个标准图像分类 benchmark 数据集上的针对敌意攻击的 robustness。我们发现 causal 和干扰信号的分离度与模型的针对敌意攻击的 robustness 之间存在强相关性（r = 0.820， p = 0.001）。此外，我们发现干扰信号的像素级信息 contenido 和针对敌意攻击的 robustness 之间存在负相关性（r = -0.597， p = 0.040）。
</details></li>
</ul>
<hr>
<h2 id="Sampling-From-Autoencoders’-Latent-Space-via-Quantization-And-Probability-Mass-Function-Concepts"><a href="#Sampling-From-Autoencoders’-Latent-Space-via-Quantization-And-Probability-Mass-Function-Concepts" class="headerlink" title="Sampling From Autoencoders’ Latent Space via Quantization And Probability Mass Function Concepts"></a>Sampling From Autoencoders’ Latent Space via Quantization And Probability Mass Function Concepts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10704">http://arxiv.org/abs/2308.10704</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aymene Mohammed Bouayed, Adrian Iaccovelli, David Naccache</li>
<li>for: 这篇论文主要关注于从生成模型中的伪扩散空间抽样，以确保生成的数据是生活化的图像。</li>
<li>methods: 我们提出了一个新的后训练抽样算法，基于几率质量函数的概念，并与量化过程相结合。我们的算法在每个伪扩散 вектор的输入数据中定义了一个范围，然后从这些定义的邻域中抽样数据。这种策略可以确保抽样的伪扩散 вектор主要 inhabit 高几率区域，并且可以实际地转换为真实世界的图像。</li>
<li>results: 我们的抽样算法在多种模型和数据集上展现出了统计上的优势，特别是在MNIST Benchmark dataset上，我们的方法与GMM抽样比较之下，增加了0.89的FID值。此外，在生成面孔和眼睛图像时，我们的方法也显示了明显的改善，FID值分别提高了1.69和0.87。<details>
<summary>Abstract</summary>
In this study, we focus on sampling from the latent space of generative models built upon autoencoders so as the reconstructed samples are lifelike images. To do to, we introduce a novel post-training sampling algorithm rooted in the concept of probability mass functions, coupled with a quantization process. Our proposed algorithm establishes a vicinity around each latent vector from the input data and then proceeds to draw samples from these defined neighborhoods. This strategic approach ensures that the sampled latent vectors predominantly inhabit high-probability regions, which, in turn, can be effectively transformed into authentic real-world images. A noteworthy point of comparison for our sampling algorithm is the sampling technique based on Gaussian mixture models (GMM), owing to its inherent capability to represent clusters. Remarkably, we manage to improve the time complexity from the previous $\mathcal{O}(n\times d \times k \times i)$ associated with GMM sampling to a much more streamlined $\mathcal{O}(n\times d)$, thereby resulting in substantial speedup during runtime. Moreover, our experimental results, gauged through the Fr\'echet inception distance (FID) for image generation, underscore the superior performance of our sampling algorithm across a diverse range of models and datasets. On the MNIST benchmark dataset, our approach outperforms GMM sampling by yielding a noteworthy improvement of up to $0.89$ in FID value. Furthermore, when it comes to generating images of faces and ocular images, our approach showcases substantial enhancements with FID improvements of $1.69$ and $0.87$ respectively, as compared to GMM sampling, as evidenced on the CelebA and MOBIUS datasets. Lastly, we substantiate our methodology's efficacy in estimating latent space distributions in contrast to GMM sampling, particularly through the lens of the Wasserstein distance.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们关注生成模型基于autoencoder的 latent space采样，以便生成真实的图像。为此，我们提出了一种新的 posterior 采样算法，基于概率质量函数，并与量化过程结合。我们的算法在每个输入数据的 latent vector周围建立一个定义的邻域，然后从这些定义的邻域中采样。这种策略使得采样的 latent vector主要居住在高概率区域中，从而可以有效地转换为真实的世界图像。与基于 Gaussian mixture models (GMM) 的采样技术相比，我们的采样算法具有更高的时间复杂度优化，从 $\mathcal{O}(n\times d\times k\times i)$ 降低到 $\mathcal{O}(n\times d)$，从而在运行时间中获得了显著的加速。此外，我们通过 Fréchet inception distance (FID) 测试，发现我们的采样算法在不同的模型和数据集上显示出了显著的性能优势。在 MNIST 测试集上，我们的方法与 GMM 采样相比，提高了 FID 值的不可或缺的提升，达到 $0.89$。此外，在生成人脸和眼部图像时，我们的方法也表现出了显著的改善，FID 提升 $1.69$ 和 $0.87$ 分别，与 GMM 采样相比。最后，我们通过 Wasserstein distance 测试，证明我们的方法在估计 latent space 分布方面的效果比 GMM 采样更好。
</details></li>
</ul>
<hr>
<h2 id="Refashioning-Emotion-Recognition-Modelling-The-Advent-of-Generalised-Large-Models"><a href="#Refashioning-Emotion-Recognition-Modelling-The-Advent-of-Generalised-Large-Models" class="headerlink" title="Refashioning Emotion Recognition Modelling: The Advent of Generalised Large Models"></a>Refashioning Emotion Recognition Modelling: The Advent of Generalised Large Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11578">http://arxiv.org/abs/2308.11578</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zixing Zhang, Liyizhe Peng, Tao Pang, Jing Han, Huan Zhao, Bjorn W. Schuller</li>
<li>for: This paper aims to investigate the performance of large language models (LLMs) in emotion recognition, specifically looking at in-context learning, few-shot learning, accuracy, generalization, and explanation.</li>
<li>methods: The paper uses LLMs, such as ChatGPT, to perform emotion recognition tasks, and examines their performance in various aspects.</li>
<li>results: The paper provides insights into the performance of LLMs in emotion recognition, including their ability to learn with few examples, adapt to new contexts, and provide explanations for their predictions.<details>
<summary>Abstract</summary>
After the inception of emotion recognition or affective computing, it has increasingly become an active research topic due to its broad applications. Over the past couple of decades, emotion recognition models have gradually migrated from statistically shallow models to neural network-based deep models, which can significantly boost the performance of emotion recognition models and consistently achieve the best results on different benchmarks. Therefore, in recent years, deep models have always been considered the first option for emotion recognition. However, the debut of large language models (LLMs), such as ChatGPT, has remarkably astonished the world due to their emerged capabilities of zero/few-shot learning, in-context learning, chain-of-thought, and others that are never shown in previous deep models. In the present paper, we comprehensively investigate how the LLMs perform in emotion recognition in terms of diverse aspects, including in-context learning, few-short learning, accuracy, generalisation, and explanation. Moreover, we offer some insights and pose other potential challenges, hoping to ignite broader discussions about enhancing emotion recognition in the new era of advanced and generalised large models.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese: послеinception of emotion recognition or affective computing, it has increasingly become an active research topic due to its broad applications. Over the past couple of decades, emotion recognition models have gradually migrated from statistically shallow models to neural network-based deep models, which can significantly boost the performance of emotion recognition models and consistently achieve the best results on different benchmarks. Therefore, in recent years, deep models have always been considered the first option for emotion recognition. However, the debut of large language models (LLMs), such as ChatGPT, has remarkably astonished the world due to their emerged capabilities of zero/few-shot learning, in-context learning, chain-of-thought, and others that are never shown in previous deep models. In the present paper, we comprehensively investigate how the LLMs perform in emotion recognition in terms of diverse aspects, including in-context learning, few-short learning, accuracy, generalisation, and explanation. Moreover, we offer some insights and pose other potential challenges, hoping to ignite broader discussions about enhancing emotion recognition in the new era of advanced and generalised large models.Translated into Traditional Chinese:在内心 recognition of emotion or affective computing 的启动以来，它已经成为一个活跃的研究领域，因为它的应用非常广泛。过去几十年，情感识别模型在从 statistically shallow 模型演化到基于神经网络的深度模型，可以对情感识别模型的性能提高，并在不同的 benchmark 上获得最好的结果。因此，在最近的年份，深度模型一直被视为情感识别的首选。然而，大型语言模型（LLMs），如 ChatGPT，在世界上发生了惊人的出现，因为它们的发展出了过去深度模型 nunca 显示的一些能力，包括零/几架学习、上下文学习、链接思维等。在 presente 纸上，我们全面调查了 LLMs 在情感识别方面的表现，包括上下文学习、几架学习、准确性、一致性和解释等方面。此外，我们还提供了一些问题和潜在的挑战，希望能够发球更广泛的讨论，以推动情感识别在新的时代的进一步发展。
</details></li>
</ul>
<hr>
<h2 id="An-engine-to-simulate-insurance-fraud-network-data"><a href="#An-engine-to-simulate-insurance-fraud-network-data" class="headerlink" title="An engine to simulate insurance fraud network data"></a>An engine to simulate insurance fraud network data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11659">http://arxiv.org/abs/2308.11659</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bavo D. C. Campo, Katrien Antonio</li>
<li>for: 本研究旨在开发高效、准确的欺诈检测模型，以便更好地检测保险欺诈laims。</li>
<li>methods: 本研究使用社交网络中充满特征的数据进行学习，并可以控制数据生成机制以便模拟不同的情况。</li>
<li>results: 通过使用这些synthetic数据，研究人员和实践者可以测试不同的检测模型，并评估其预测性能。<details>
<summary>Abstract</summary>
Traditionally, the detection of fraudulent insurance claims relies on business rules and expert judgement which makes it a time-consuming and expensive process (\'Oskarsd\'ottir et al., 2022). Consequently, researchers have been examining ways to develop efficient and accurate analytic strategies to flag suspicious claims. Feeding learning methods with features engineered from the social network of parties involved in a claim is a particularly promising strategy (see for example Van Vlasselaer et al. (2016); Tumminello et al. (2023)). When developing a fraud detection model, however, we are confronted with several challenges. The uncommon nature of fraud, for example, creates a high class imbalance which complicates the development of well performing analytic classification models. In addition, only a small number of claims are investigated and get a label, which results in a large corpus of unlabeled data. Yet another challenge is the lack of publicly available data. This hinders not only the development of new methods, but also the validation of existing techniques. We therefore design a simulation machine that is engineered to create synthetic data with a network structure and available covariates similar to the real life insurance fraud data set analyzed in \'Oskarsd\'ottir et al. (2022). Further, the user has control over several data-generating mechanisms. We can specify the total number of policyholders and parties, the desired level of imbalance and the (effect size of the) features in the fraud generating model. As such, the simulation engine enables researchers and practitioners to examine several methodological challenges as well as to test their (development strategy of) insurance fraud detection models in a range of different settings. Moreover, large synthetic data sets can be generated to evaluate the predictive performance of (advanced) machine learning techniques.
</details>
<details>
<summary>摘要</summary>
传统上，探测欺诈保险索赔靠业务规则和专家判断，这使得过程时间consuming和成本高（'Oskarsd\'ottir等，2022）。因此，研究人员在尝试开发高效和准确的分析策略来标识可疑索赔。从社交网络中提取和Engineered特征来帮助学习方法是一种非常有前途的策略（如Van Vlasselaer等，2016；Tumminello等，2023）。在开发欺诈检测模型时，我们面临了多个挑战。欺诈的罕见性导致分类问题的吸引性异常高，这使得开发高性能分类模型的发展具有挑战性。此外，只有一小部分索赔被调查并获得标签，导致大量的无标签数据。此外，公共数据的缺乏也阻碍了新方法的开发和现有技术的验证。为了解决这些问题，我们设计了一台模拟机器，可以生成具有网络结构和实际欺诈数据集中可用特征的 sintetic数据。用户可以控制数据生成机制，包括总体保险持有人和相关人数、欺诈水平和特征效果等。因此，模拟机器可以帮助研究人员和实践者在不同的设定下测试开发策略和检测模型，并生成大量的 sintetic数据来评估高级机器学习技术的预测性能。
</details></li>
</ul>
<hr>
<h2 id="Cost-Efficient-Online-Decision-Making-A-Combinatorial-Multi-Armed-Bandit-Approach"><a href="#Cost-Efficient-Online-Decision-Making-A-Combinatorial-Multi-Armed-Bandit-Approach" class="headerlink" title="Cost-Efficient Online Decision Making: A Combinatorial Multi-Armed Bandit Approach"></a>Cost-Efficient Online Decision Making: A Combinatorial Multi-Armed Bandit Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10699">http://arxiv.org/abs/2308.10699</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arman Rahbar, Niklas Åkerblom, Morteza Haghir Chehreghani</li>
<li>for: 这篇论文是关于在多种应用场景中进行在线决策的研究，具体来说是在收到新数据点时进行测试序列的决策。</li>
<li>methods: 本文提出了一种基于 combinatorial multi-armed bandits 的新的在线决策问题的形式ulation，并使用 posterior sampling 或 BayesUCB 进行探索。</li>
<li>results: 本文提供了一种新的成本效益的在线决策框架，并进行了严格的理论分析和多种实验 validate 其可用性。<details>
<summary>Abstract</summary>
Online decision making plays a crucial role in numerous real-world applications. In many scenarios, the decision is made based on performing a sequence of tests on the incoming data points. However, performing all tests can be expensive and is not always possible. In this paper, we provide a novel formulation of the online decision making problem based on combinatorial multi-armed bandits and take the cost of performing tests into account. Based on this formulation, we provide a new framework for cost-efficient online decision making which can utilize posterior sampling or BayesUCB for exploration. We provide a rigorous theoretical analysis for our framework and present various experimental results that demonstrate its applicability to real-world problems.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Beyond-expectations-Residual-Dynamic-Mode-Decomposition-and-Variance-for-Stochastic-Dynamical-Systems"><a href="#Beyond-expectations-Residual-Dynamic-Mode-Decomposition-and-Variance-for-Stochastic-Dynamical-Systems" class="headerlink" title="Beyond expectations: Residual Dynamic Mode Decomposition and Variance for Stochastic Dynamical Systems"></a>Beyond expectations: Residual Dynamic Mode Decomposition and Variance for Stochastic Dynamical Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10697">http://arxiv.org/abs/2308.10697</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matthew J. Colbrook, Qin Li, Ryan V. Raut, Alex Townsend</li>
<li>For: 这个论文的目的是为了解决非线性动力系统中的库曼操作符的特征，以及其spectral information的计算方法。* Methods: 这篇论文使用了Dynamic Mode Decomposition（DMD）方法，以及一个额外的DMD-type矩阵，来 aproximate the sum of a squared residual and a variance term，从而控制 projection error。* Results: 这篇论文的结果表明，通过包含差异的库曼框架，可以 Addressing challenges such as spurious modes, essential spectra, and the verification of Koopman mode decompositions for stochastic systems。 In addition, the authors introduce the concept of variance-pseudospectra to gauge statistical coherence, and present a suite of convergence results for the spectral quantities of stochastic Koopman operators. Finally, the study demonstrates practical applications using both simulated and experimental data, and reveals physiologically significant information unavailable to standard expectation-based dynamical models.<details>
<summary>Abstract</summary>
Koopman operators linearize nonlinear dynamical systems, making their spectral information of crucial interest. Numerous algorithms have been developed to approximate these spectral properties, and Dynamic Mode Decomposition (DMD) stands out as the poster child of projection-based methods. Although the Koopman operator itself is linear, the fact that it acts in an infinite-dimensional space of observables poses various challenges. These include spurious modes, essential spectra, and the verification of Koopman mode decompositions. While recent work has addressed these challenges for deterministic systems, there remains a notable gap in verified DMD methods tailored for stochastic systems, where the Koopman operator measures the expectation of observables. We show that it is necessary to go beyond expectations to address these issues. By incorporating variance into the Koopman framework, we address these challenges. Through an additional DMD-type matrix, we approximate the sum of a squared residual and a variance term, each of which can be approximated individually using batched snapshot data. This allows verified computation of the spectral properties of stochastic Koopman operators, controlling the projection error. We also introduce the concept of variance-pseudospectra to gauge statistical coherency. Finally, we present a suite of convergence results for the spectral quantities of stochastic Koopman operators. Our study concludes with practical applications using both simulated and experimental data. In neural recordings from awake mice, we demonstrate how variance-pseudospectra can reveal physiologically significant information unavailable to standard expectation-based dynamical models.
</details>
<details>
<summary>摘要</summary>
黑曼操作符 linearizes nonlinear dynamical systems, making its spectral information of crucial interest. numerous algorithms have been developed to approximate these spectral properties, and Dynamic Mode Decomposition (DMD) stands out as the poster child of projection-based methods. Although the Koopman operator itself is linear, the fact that it acts in an infinite-dimensional space of observables poses various challenges. These include spurious modes, essential spectra, and the verification of Koopman mode decompositions. While recent work has addressed these challenges for deterministic systems, there remains a notable gap in verified DMD methods tailored for stochastic systems, where the Koopman operator measures the expectation of observables. We show that it is necessary to go beyond expectations to address these issues. By incorporating variance into the Koopman framework, we address these challenges. Through an additional DMD-type matrix, we approximate the sum of a squared residual and a variance term, each of which can be approximated individually using batched snapshot data. This allows verified computation of the spectral properties of stochastic Koopman operators, controlling the projection error. We also introduce the concept of variance-pseudospectra to gauge statistical coherency. Finally, we present a suite of convergence results for the spectral quantities of stochastic Koopman operators. Our study concludes with practical applications using both simulated and experimental data. In neural recordings from awake mice, we demonstrate how variance-pseudospectra can reveal physiologically significant information unavailable to standard expectation-based dynamical models.Here's the translation in Traditional Chinese:科普曼操作符可以线性化非线性动力学系统，使其特征对应的spectral information得到了核心地位。numerous algorithms have been developed to approximate these spectral properties, and Dynamic Mode Decomposition (DMD) stands out as the poster child of projection-based methods. although the Koopman operator itself is linear, the fact that it acts in an infinite-dimensional space of observables poses various challenges. These include spurious modes, essential spectra, and the verification of Koopman mode decompositions. While recent work has addressed these challenges for deterministic systems, there remains a notable gap in verified DMD methods tailored for stochastic systems, where the Koopman operator measures the expectation of observables. We show that it is necessary to go beyond expectations to address these issues. By incorporating variance into the Koopman framework, we address these challenges. Through an additional DMD-type matrix, we approximate the sum of a squared residual and a variance term, each of which can be approximated individually using batched snapshot data. This allows verified computation of the spectral properties of stochastic Koopman operators, controlling the projection error. We also introduce the concept of variance-pseudospectra to gauge statistical coherency. Finally, we present a suite of convergence results for the spectral quantities of stochastic Koopman operators. Our study concludes with practical applications using both simulated and experimental data. In neural recordings from awake mice, we demonstrate how variance-pseudospectra can reveal physiologically significant information unavailable to standard expectation-based dynamical models.
</details></li>
</ul>
<hr>
<h2 id="An-Improved-Best-of-both-worlds-Algorithm-for-Bandits-with-Delayed-Feedback"><a href="#An-Improved-Best-of-both-worlds-Algorithm-for-Bandits-with-Delayed-Feedback" class="headerlink" title="An Improved Best-of-both-worlds Algorithm for Bandits with Delayed Feedback"></a>An Improved Best-of-both-worlds Algorithm for Bandits with Delayed Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10675">http://arxiv.org/abs/2308.10675</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saeed Masoudian, Julian Zimmert, Yevgeny Seldin</li>
<li>for: 本文提出了一种新的best-of-both-worlds算法，用于带延迟反馈的bandit问题。该算法改进了先前的 Masoudian et al. 的工作，不需要先知道最大延迟 $d_{\max}$，并提供了更紧的 regret bounds。</li>
<li>methods: 本文使用了 counts of outstanding observations（在动作时间被观察到的尚未完成的观察数）而不是延迟或最大延迟（只在反馈 arrive时被观察到）来实现。一个重要贡献是控制分布漂移，基于偏损失估计器和延迟过大的观察被跳过。</li>
<li>results: 本文的 regret bounds 是基于 counts of outstanding observations after skipping of observations with excessively large delays，而不是延迟或最大延迟。这一结论表明了best-of-both-worlds bandit with delayed feedback的复杂性是由出standing observations的总计数所定义，而不是延迟或最大延迟。<details>
<summary>Abstract</summary>
We propose a new best-of-both-worlds algorithm for bandits with variably delayed feedback. The algorithm improves on prior work by Masoudian et al. [2022] by eliminating the need in prior knowledge of the maximal delay $d_{\mathrm{max}}$ and providing tighter regret bounds in both regimes. The algorithm and its regret bounds are based on counts of outstanding observations (a quantity that is observed at action time) rather than delays or the maximal delay (quantities that are only observed when feedback arrives). One major contribution is a novel control of distribution drift, which is based on biased loss estimators and skipping of observations with excessively large delays. Another major contribution is demonstrating that the complexity of best-of-both-worlds bandits with delayed feedback is characterized by the cumulative count of outstanding observations after skipping of observations with excessively large delays, rather than the delays or the maximal delay.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的最佳之both-worlds算法，用于带有变化延迟反馈的带宽问题。该算法超越了先前的 Masoudian 等人（2022）的工作，不需要先知道最大延迟 $d_{\max}$，并提供了更紧的 regret bound 在两个 режиме下。该算法和其 regret bound 基于当时行动时观察到的待机数（一个观察到的量）而不是延迟或最大延迟（只有在反馈来到时才能观察到的量）。我们的一个重要贡献是一种新的分布漂移控制，基于偏置损失估计器和延迟过长的观察抛弃。另一个重要贡献是表明了最佳之both-worlds 带宽问题的复杂度被定义为在延迟抛弃后的累积待机数后，而不是延迟或最大延迟。
</details></li>
</ul>
<hr>
<h2 id="A-Safe-Deep-Reinforcement-Learning-Approach-for-Energy-Efficient-Federated-Learning-in-Wireless-Communication-Networks"><a href="#A-Safe-Deep-Reinforcement-Learning-Approach-for-Energy-Efficient-Federated-Learning-in-Wireless-Communication-Networks" class="headerlink" title="A Safe Deep Reinforcement Learning Approach for Energy Efficient Federated Learning in Wireless Communication Networks"></a>A Safe Deep Reinforcement Learning Approach for Energy Efficient Federated Learning in Wireless Communication Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10664">http://arxiv.org/abs/2308.10664</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nikolaos Koursioumpas, Lina Magoula, Nikolaos Petropouleas, Alexandros-Ioannis Thanopoulos, Theodora Panagea, Nancy Alonistioti, M. A. Gutierrez-Estevez, Ramin Khalili</li>
<li>for: 提高 Federated Learning（FL）过程中的能源占用率，并保证模型的性能。</li>
<li>methods: 提出一种 Soft Actor Critic Deep Reinforcement Learning（DRL）解决方案，并在训练过程中引入罚函数，以避免环境约束的违反。</li>
<li>results: 与四个州OF-the-art基eline解决方案进行比较，在静态和动态环境中达到了94%的能源占用减少。<details>
<summary>Abstract</summary>
Progressing towards a new era of Artificial Intelligence (AI) - enabled wireless networks, concerns regarding the environmental impact of AI have been raised both in industry and academia. Federated Learning (FL) has emerged as a key privacy preserving decentralized AI technique. Despite efforts currently being made in FL, its environmental impact is still an open problem. Targeting the minimization of the overall energy consumption of an FL process, we propose the orchestration of computational and communication resources of the involved devices to minimize the total energy required, while guaranteeing a certain performance of the model. To this end, we propose a Soft Actor Critic Deep Reinforcement Learning (DRL) solution, where a penalty function is introduced during training, penalizing the strategies that violate the constraints of the environment, and ensuring a safe RL process. A device level synchronization method, along with a computationally cost effective FL environment are proposed, with the goal of further reducing the energy consumption and communication overhead. Evaluation results show the effectiveness of the proposed scheme compared to four state-of-the-art baseline solutions in both static and dynamic environments, achieving a decrease of up to 94% in the total energy consumption.
</details>
<details>
<summary>摘要</summary>
Note: Simplified Chinese is also known as "简化字" or "简化字".Translation Notes:* "Federated Learning" is translated as "联合学习" (Liánhégòngxué)* "Soft Actor Critic" is translated as "软 actor 批评" (Ruǎn yuǎn jīng zhì)* "Deep Reinforcement Learning" is translated as "深度强化学习" (Shēngrán jīnghòu xuéxí)* "Device level synchronization" is translated as "设备层同步" (Jījī kuàng yì)* "Computationally cost-effective" is translated as "计算成本低" (Jìsuàn jīngbèi dī)
</details></li>
</ul>
<hr>
<h2 id="Practical-Parallel-Algorithms-for-Non-Monotone-Submodular-Maximization"><a href="#Practical-Parallel-Algorithms-for-Non-Monotone-Submodular-Maximization" class="headerlink" title="Practical Parallel Algorithms for Non-Monotone Submodular Maximization"></a>Practical Parallel Algorithms for Non-Monotone Submodular Maximization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10656">http://arxiv.org/abs/2308.10656</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuang Cui, Kai Han, Jing Tang, He Huang, Xueying Li, Aakas Zhiyuli, Hanxiao Li</li>
<li>for: 这paper focuses on developing efficient and parallelizable algorithms for submodular maximization in various domains of artificial intelligence, such as machine learning, computer vision, and natural language processing.</li>
<li>methods: 该paper proposes two algorithms for non-monotone submodular maximization subject to a knapsack constraint and a $k$-system constraint, respectively. The first algorithm achieves an $(8+\epsilon)$-approximation under $\mathcal{O}(\log n)$ adaptive complexity, which is optimal up to a factor of $\mathcal{O}(\log\log n)$. The second algorithm has both provable approximation ratio and sublinear adaptive complexity.</li>
<li>results: 该paper achieves performance bounds comparable with those of state-of-the-art algorithms on the special case of submodular maximization subject to a cardinality constraint. Extensive experiments on real-world applications demonstrate the effectiveness of the proposed approach.<details>
<summary>Abstract</summary>
Submodular maximization has found extensive applications in various domains within the field of artificial intelligence, including but not limited to machine learning, computer vision, and natural language processing. With the increasing size of datasets in these domains, there is a pressing need to develop efficient and parallelizable algorithms for submodular maximization. One measure of the parallelizability of a submodular maximization algorithm is its adaptive complexity, which indicates the number of sequential rounds where a polynomial number of queries to the objective function can be executed in parallel. In this paper, we study the problem of non-monotone submodular maximization subject to a knapsack constraint, and propose the first combinatorial algorithm achieving an $(8+\epsilon)$-approximation under $\mathcal{O}(\log n)$ adaptive complexity, which is \textit{optimal} up to a factor of $\mathcal{O}(\log\log n)$. Moreover, we also propose the first algorithm with both provable approximation ratio and sublinear adaptive complexity for the problem of non-monotone submodular maximization subject to a $k$-system constraint. As a by-product, we show that our two algorithms can also be applied to the special case of submodular maximization subject to a cardinality constraint, and achieve performance bounds comparable with those of state-of-the-art algorithms. Finally, the effectiveness of our approach is demonstrated by extensive experiments on real-world applications.
</details>
<details>
<summary>摘要</summary>
“对于人工智能领域内的不同领域，例如机器学习、computer vision和自然语言处理，submodular maximization已经获得了广泛的应用。随着这些领域的数据规模的增加，发展高效且可并行化的submodular maximization算法成为了一个紧迫的需求。一个measure of the parallelizability of a submodular maximization algorithm是其adaptive complexity，它指出在执行多个并行查询时，可以在执行多个sequential round中 Execute a polynomial number of queries to the objective function。在这篇论文中，我们研究了非单调submodular maximization subject to a knapsack constraint，并提出了首次的 combinatorial algorithm，可以在 $\mathcal{O}(\log n)$ adaptive complexity下实现 $(8+\epsilon)$-approximation，这是optimal up to a factor of $\mathcal{O}(\log\log n)$。此外，我们还提出了首次的一个Algorithm with both provable approximation ratio and sublinear adaptive complexity for the problem of non-monotone submodular maximization subject to a $k$-system constraint。在特殊情况下，我们的两个算法可以应用到submodular maximization subject to a cardinality constraint，并实现与现有算法相当的性能。最后，我们通过实验证明了我们的方法的有效性。”
</details></li>
</ul>
<hr>
<h2 id="Deep-Evidential-Learning-for-Bayesian-Quantile-Regression"><a href="#Deep-Evidential-Learning-for-Bayesian-Quantile-Regression" class="headerlink" title="Deep Evidential Learning for Bayesian Quantile Regression"></a>Deep Evidential Learning for Bayesian Quantile Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10650">http://arxiv.org/abs/2308.10650</a></li>
<li>repo_url: None</li>
<li>paper_authors: Frederik Boe Hüttel, Filipe Rodrigues, Francisco Câmara Pereira</li>
<li>for: 这篇论文旨在提出一种深度 bayesian 量化回归模型，能够无需 Gaussian 假设来Estimate 连续目标分布的Quantiles。</li>
<li>methods: 该方法基于 evidential learning，通过单个推理过程来捕捉 aleatoric 和 epistemic uncertainty。</li>
<li>results: 对于非 Gaussian 分布，该方法可以实现准确的 uncertainty estimation，并且可以分解 aleatoric 和 epistemic uncertainty，以及对于异常样本的 Robustness。<details>
<summary>Abstract</summary>
It is desirable to have accurate uncertainty estimation from a single deterministic forward-pass model, as traditional methods for uncertainty quantification are computationally expensive. However, this is difficult because single forward-pass models do not sample weights during inference and often make assumptions about the target distribution, such as assuming it is Gaussian. This can be restrictive in regression tasks, where the mean and standard deviation are inadequate to model the target distribution accurately. This paper proposes a deep Bayesian quantile regression model that can estimate the quantiles of a continuous target distribution without the Gaussian assumption. The proposed method is based on evidential learning, which allows the model to capture aleatoric and epistemic uncertainty with a single deterministic forward-pass model. This makes the method efficient and scalable to large models and datasets. We demonstrate that the proposed method achieves calibrated uncertainties on non-Gaussian distributions, disentanglement of aleatoric and epistemic uncertainty, and robustness to out-of-distribution samples.
</details>
<details>
<summary>摘要</summary>
希望通过单个决定性前进模型获得准确的不确定性估计，然而这是困难的因为单个前进模型在推理过程中不会采样权重。传统方法的不确定性量化 computationally expensive。这些方法经常假设目标分布是 Gaussian，这可能是回归任务中的限制。这篇文章提出了深度 bayesian 量化回归模型，可以无需 Gaussian 假设来估计连续目标分布的quantiles。提出的方法基于证据学习，使得模型能够捕捉 aleatoric 和 epistemic 不确定性。这使得方法高效可扩展。我们示示了该方法可以在非 Gaussian 分布上获得准确的不确定性估计，分离 aleatoric 和 epistemic 不确定性，并对异常样本具有鲁棒性。
</details></li>
</ul>
<hr>
<h2 id="Reinforcement-Learning-Based-Sensor-Optimization-for-Bio-markers"><a href="#Reinforcement-Learning-Based-Sensor-Optimization-for-Bio-markers" class="headerlink" title="Reinforcement Learning Based Sensor Optimization for Bio-markers"></a>Reinforcement Learning Based Sensor Optimization for Bio-markers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10649">http://arxiv.org/abs/2308.10649</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sajal Khandelwal, Pawan Kumar, Syed Azeemuddin</li>
<li>for: 这个论文旨在提高基于电子板的射频生物感测器的敏感度。</li>
<li>methods: 这个论文使用了一种新的强化学习基于二进制蜂群优化（RLBPSO）算法来优化感测器的设计参数，并与其他当前领域的方法进行比较。</li>
<li>results: 研究发现，RLBPSO方法可以在不同频率范围内提高感测器的敏感度，并且在不同的电极设计和脚宽参数下显示出最佳性能。<details>
<summary>Abstract</summary>
Radio frequency (RF) biosensors, in particular those based on inter-digitated capacitors (IDCs), are pivotal in areas like biomedical diagnosis, remote sensing, and wireless communication. Despite their advantages of low cost and easy fabrication, their sensitivity can be hindered by design imperfections, environmental factors, and circuit noise. This paper investigates enhancing the sensitivity of IDC-based RF sensors using novel reinforcement learning based Binary Particle Swarm Optimization (RLBPSO), and it is compared to Ant Colony Optimization (ACO), and other state-of-the-art methods. By focusing on optimizing design parameters like electrode design and finger width, the proposed study found notable improvements in sensor sensitivity. The proposed RLBPSO method shows best optimized design for various frequency ranges when compared to current state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
Radio frequency (RF) 感测器，尤其是基于交错电极（IDC）的感测器，在生物医学诊断、远程探测和无线通信等领域具有重要地位。尽管它们具有低成本和易于制造的优点，但是设计瑕疵、环境因素和电路噪声可能会减少它们的敏感度。这篇论文探讨了使用新的强化学习基于二进制群集优化（RLBPSO）方法来提高IDC基于RF感测器的敏感度，并与抗群优化（ACO）和其他当前最佳方法进行比较。通过对设计参数如电极设计和脚宽进行优化，该研究发现了显著提高感测器敏感度的可能性。RLBPSO方法在不同频率范围内对感测器设计优化表现出了最佳的效果。
</details></li>
</ul>
<hr>
<h2 id="Faster-Training-of-Neural-ODEs-Using-Gaus-Legendre-Quadrature"><a href="#Faster-Training-of-Neural-ODEs-Using-Gaus-Legendre-Quadrature" class="headerlink" title="Faster Training of Neural ODEs Using Gauß-Legendre Quadrature"></a>Faster Training of Neural ODEs Using Gauß-Legendre Quadrature</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10644">http://arxiv.org/abs/2308.10644</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/a-norcliffe/torch_gq_adjoint">https://github.com/a-norcliffe/torch_gq_adjoint</a></li>
<li>paper_authors: Alexander Norcliffe, Marc Peter Deisenroth</li>
<li>for: 提高神经泛化方法的训练速度，尤其是大型神经网络。</li>
<li>methods: 使用Gau{\ss}-Legendre quadrature来更快地解决 интегралы，而不会影响神经网络的表达能力。</li>
<li>results: 提出了一种新的方法来快速训练神经泛化方法，并且可以应用于大型神经网络和SDE-based模型的训练。<details>
<summary>Abstract</summary>
Neural ODEs demonstrate strong performance in generative and time-series modelling. However, training them via the adjoint method is slow compared to discrete models due to the requirement of numerically solving ODEs. To speed neural ODEs up, a common approach is to regularise the solutions. However, this approach may affect the expressivity of the model; when the trajectory itself matters, this is particularly important. In this paper, we propose an alternative way to speed up the training of neural ODEs. The key idea is to speed up the adjoint method by using Gau{\ss}-Legendre quadrature to solve integrals faster than ODE-based methods while remaining memory efficient. We also extend the idea to training SDEs using the Wong-Zakai theorem, by training a corresponding ODE and transferring the parameters. Our approach leads to faster training of neural ODEs, especially for large models. It also presents a new way to train SDE-based models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="SCULPT-Shape-Conditioned-Unpaired-Learning-of-Pose-dependent-Clothed-and-Textured-Human-Meshes"><a href="#SCULPT-Shape-Conditioned-Unpaired-Learning-of-Pose-dependent-Clothed-and-Textured-Human-Meshes" class="headerlink" title="SCULPT: Shape-Conditioned Unpaired Learning of Pose-dependent Clothed and Textured Human Meshes"></a>SCULPT: Shape-Conditioned Unpaired Learning of Pose-dependent Clothed and Textured Human Meshes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10638">http://arxiv.org/abs/2308.10638</a></li>
<li>repo_url: None</li>
<li>paper_authors: Soubhik Sanyal, Partha Ghosh, Jinlong Yang, Michael J. Black, Justus Thies, Timo Bolkart</li>
<li>for: 这个论文的目的是提出一种基于深度学习的3D生成模型，用于生成披覆的人体3D模型。</li>
<li>methods: 这个论文使用的方法包括：（1）利用CAPE数据集和大规模的2D图像数据集来隐式学习 clothed human body的几何和外观分布；（2）提出一种不需要对数据集进行对应的学习方法，可以从2D图像数据集中学习 pose-dependent clothed human body的几何和外观特征；（3）使用 attribute labels 来分离 cloth 类型和 pose 之间的杂糅关系，以及 cloth 颜色和 pose 之间的杂糅关系。</li>
<li>results: 这个论文的结果显示，使用这种方法可以生成高质量的 clothed human body 3D模型，并且比之前的方法更加灵活和可控。<details>
<summary>Abstract</summary>
We present SCULPT, a novel 3D generative model for clothed and textured 3D meshes of humans. Specifically, we devise a deep neural network that learns to represent the geometry and appearance distribution of clothed human bodies. Training such a model is challenging, as datasets of textured 3D meshes for humans are limited in size and accessibility. Our key observation is that there exist medium-sized 3D scan datasets like CAPE, as well as large-scale 2D image datasets of clothed humans and multiple appearances can be mapped to a single geometry. To effectively learn from the two data modalities, we propose an unpaired learning procedure for pose-dependent clothed and textured human meshes. Specifically, we learn a pose-dependent geometry space from 3D scan data. We represent this as per vertex displacements w.r.t. the SMPL model. Next, we train a geometry conditioned texture generator in an unsupervised way using the 2D image data. We use intermediate activations of the learned geometry model to condition our texture generator. To alleviate entanglement between pose and clothing type, and pose and clothing appearance, we condition both the texture and geometry generators with attribute labels such as clothing types for the geometry, and clothing colors for the texture generator. We automatically generated these conditioning labels for the 2D images based on the visual question answering model BLIP and CLIP. We validate our method on the SCULPT dataset, and compare to state-of-the-art 3D generative models for clothed human bodies. We will release the codebase for research purposes.
</details>
<details>
<summary>摘要</summary>
我们介绍SCULPT，一种新的3D生成模型，用于 clothed和textured 3D雷达图像的人体。我们设计了一个深度神经网络，用于表示人体的几何和外观分布。由于人体3D扫描数据的限制，训练这种模型是一个挑战。我们的关键观察是，存在中等大小的3D扫描数据集，如CAPE，以及大规模的2D图像数据集，包含不同的服装和人体姿势。为了有效地利用这两种数据模式，我们提出了一种无关的学习过程，用于pose-dependent clothed和textured人体3D雷达图像。我们从3D扫描数据中学习一个pose-dependent的几何空间，并将其表示为每个顶点的位差相对于SMPL模型。然后，我们在无监督的情况下使用2D图像数据来训练一个几何条件的文本生成器。我们使用学习的几何模型的中间活动来condition我们的文本生成器。为了避免姿势和服装类型之间的杂谱和姿势和服装外观之间的杂谱，我们将condition both texture和geometry生成器使用属性标签，例如服装类型 для几何和服装颜色 для文本生成器。我们自动生成了这些conditioning标签基于视觉问答模型BLIP和CLIP。我们验证了我们的方法SCULPT数据集上，并与状态之前的3D生成模型进行比较。我们将发布代码库用于研究目的。
</details></li>
</ul>
<hr>
<h2 id="Foundation-Model-oriented-Robustness-Robust-Image-Model-Evaluation-with-Pretrained-Models"><a href="#Foundation-Model-oriented-Robustness-Robust-Image-Model-Evaluation-with-Pretrained-Models" class="headerlink" title="Foundation Model-oriented Robustness: Robust Image Model Evaluation with Pretrained Models"></a>Foundation Model-oriented Robustness: Robust Image Model Evaluation with Pretrained Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10632">http://arxiv.org/abs/2308.10632</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peiyan Zhang, Haoyang Liu, Chaozhuo Li, Xing Xie, Sunghun Kim, Haohan Wang</li>
<li>for: 这篇论文主要关注于评估图像分类模型的 robustness 性能，以及开发一种新的评估方法以外测试模型的性能。</li>
<li>methods: 本论文提出了一种新的评估方法，即与基本模型（oracle）进行比较，以评估图像分类模型的性能。此外， authors还提出了一种使用新的样本生成方法，以增加测试数据的多样性。</li>
<li>results: 本论文的实验结果表明，使用该新的评估方法可以准确地评估图像分类模型的 robustness 性能，并且可以免除 fix benchmarks 的限制。此外， authors 还通过对模型的行为进行分析，了解模型在不同情况下的行为。<details>
<summary>Abstract</summary>
Machine learning has demonstrated remarkable performance over finite datasets, yet whether the scores over the fixed benchmarks can sufficiently indicate the model's performance in the real world is still in discussion. In reality, an ideal robust model will probably behave similarly to the oracle (e.g., the human users), thus a good evaluation protocol is probably to evaluate the models' behaviors in comparison to the oracle. In this paper, we introduce a new robustness measurement that directly measures the image classification model's performance compared with a surrogate oracle (i.e., a foundation model). Besides, we design a simple method that can accomplish the evaluation beyond the scope of the benchmarks. Our method extends the image datasets with new samples that are sufficiently perturbed to be distinct from the ones in the original sets, but are still bounded within the same image-label structure the original test image represents, constrained by a foundation model pretrained with a large amount of samples. As a result, our new method will offer us a new way to evaluate the models' robustness performance, free of limitations of fixed benchmarks or constrained perturbations, although scoped by the power of the oracle. In addition to the evaluation results, we also leverage our generated data to understand the behaviors of the model and our new evaluation strategies.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Homogenization-Approach-for-Gradient-Dominated-Stochastic-Optimization"><a href="#A-Homogenization-Approach-for-Gradient-Dominated-Stochastic-Optimization" class="headerlink" title="A Homogenization Approach for Gradient-Dominated Stochastic Optimization"></a>A Homogenization Approach for Gradient-Dominated Stochastic Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10630">http://arxiv.org/abs/2308.10630</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiyuan Tan, Chenyu Xue, Chuwen Zhang, Qi Deng, Dongdong Ge, Yinyu Ye</li>
<li>for: 这篇论文是关于非对称优化的研究，尤其是gradient-dominated optimization，并使用了homogenization approach来解决问题。</li>
<li>methods: 本文使用了stochastic homogeneous second-order descent method (SHSODM)，并提出了一种具有variance reduction技术的SHSODM，具体来说是使用extremal eigenvector problems来解决问题。</li>
<li>results: 本文的结果显示SHSODM可以在非对称优化中实现global convergence，并且可以比其他off-the-shelf方法更高效。具体来说，本文的结果显示SHSODM可以在$\alpha \in [1, 2]$的情况下实现sample complexity bound of $O(\epsilon^{-7&#x2F;(2 \alpha) +1})$和$\tilde{O}(\epsilon^{-2&#x2F;\alpha})$。此外，本文还提出了一种具有variance reduction技术的SHSODM，其sample complexity bound为$O( \epsilon ^{-( 7-3\alpha ) &#x2F;( 2\alpha )})$。<details>
<summary>Abstract</summary>
Gradient dominance property is a condition weaker than strong convexity, yet it sufficiently ensures global convergence for first-order methods even in non-convex optimization. This property finds application in various machine learning domains, including matrix decomposition, linear neural networks, and policy-based reinforcement learning (RL). In this paper, we study the stochastic homogeneous second-order descent method (SHSODM) for gradient-dominated optimization with $\alpha \in [1, 2]$ based on a recently proposed homogenization approach. Theoretically, we show that SHSODM achieves a sample complexity of $O(\epsilon^{-7/(2 \alpha) +1})$ for $\alpha \in [1, 3/2)$ and $\tilde{O}(\epsilon^{-2/\alpha})$ for $\alpha \in [3/2, 2]$. We further provide a SHSODM with a variance reduction technique enjoying an improved sample complexity of $O( \epsilon ^{-( 7-3\alpha ) /( 2\alpha )})$ for $\alpha \in [1,3/2)$. Our results match the state-of-the-art sample complexity bounds for stochastic gradient-dominated optimization without \emph{cubic regularization}. Since the homogenization approach only relies on solving extremal eigenvector problems instead of Newton-type systems, our methods gain the advantage of cheaper iterations and robustness in ill-conditioned problems. Numerical experiments on several RL tasks demonstrate the efficiency of SHSODM compared to other off-the-shelf methods.
</details>
<details>
<summary>摘要</summary>
“Gradient占主性质是一种弱于强Converter的条件，但它足够保证全局收敛性 для首频方法，即使在非 convex 优化中。这种性质在机器学习中有很多应用，包括矩阵分解、线性神经网络和Policy-based 强化学习（RL）。在这篇论文中，我们研究了随机同质二阶 descend 方法（SHSODM）在 Gradient-dominated 优化中，基于最近提出的同质化方法。我们证明了 SHSODM 的样本复杂度为 $O(\epsilon^{-7/(2\alpha) +1})$ for $\alpha \in [1, 3/2)$ 和 $\tilde{O}(\epsilon^{-2/\alpha})$ for $\alpha \in [3/2, 2]$。我们还提供了一种 SHSODM 使用减少噪声的技术，其样本复杂度为 $O( \epsilon ^{-( 7-3\alpha ) /( 2\alpha )})$ for $\alpha \in [1,3/2)$。我们的结果与不含 кубиック regularization 的 Stochastic gradient-dominated 优化的状态艺术性样本复杂度匹配。由于同质化方法只需解决极值 eigenvector 问题而不需要 Newton-type 系统，我们的方法具有更便宜的迭代和稳定性。 numerical experiments on several RL tasks 表明 SHSODM 比其他 off-the-shelf 方法更高效。”
</details></li>
</ul>
<hr>
<h2 id="GaitPT-Skeletons-Are-All-You-Need-For-Gait-Recognition"><a href="#GaitPT-Skeletons-Are-All-You-Need-For-Gait-Recognition" class="headerlink" title="GaitPT: Skeletons Are All You Need For Gait Recognition"></a>GaitPT: Skeletons Are All You Need For Gait Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10623">http://arxiv.org/abs/2308.10623</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andy Catruna, Adrian Cosma, Emilian Radoi</li>
<li>for: 人脸识别（人脸认知）</li>
<li>methods: pose estimation skeletons + hierarchical transformer architecture</li>
<li>results: 比其他skeleton-based gait recognition工作高出6%的平均准确率（82.6%），以及52.16%的排名第1精度（Rank-1）。<details>
<summary>Abstract</summary>
The analysis of patterns of walking is an important area of research that has numerous applications in security, healthcare, sports and human-computer interaction. Lately, walking patterns have been regarded as a unique fingerprinting method for automatic person identification at a distance. In this work, we propose a novel gait recognition architecture called Gait Pyramid Transformer (GaitPT) that leverages pose estimation skeletons to capture unique walking patterns, without relying on appearance information. GaitPT adopts a hierarchical transformer architecture that effectively extracts both spatial and temporal features of movement in an anatomically consistent manner, guided by the structure of the human skeleton. Our results show that GaitPT achieves state-of-the-art performance compared to other skeleton-based gait recognition works, in both controlled and in-the-wild scenarios. GaitPT obtains 82.6% average accuracy on CASIA-B, surpassing other works by a margin of 6%. Moreover, it obtains 52.16% Rank-1 accuracy on GREW, outperforming both skeleton-based and appearance-based approaches.
</details>
<details>
<summary>摘要</summary>
研究人行姿势的分析是一个重要的领域，它在安全、医疗、运动和人机交互等领域都有广泛的应用。最近，人行姿势被视为一种唯一的指纹方法，用于自动识别人员。在这项工作中，我们提出了一种新的步态识别架构，称为步态 pyramid transformer（GaitPT），它利用 pose estimation skeleton 来捕捉唯一的人行姿势特征，不依赖于外观信息。GaitPT 采用了层次 transformer 架构，能够有效地提取人体运动的空间和时间特征，并且以人体骨架为引导，以遵循人体解剖结构。我们的结果表明，GaitPT 在 CASIA-B 上取得了82.6% 的平均准确率，比其他skeleton-based gait recognition 工作高出6%。此外，它在 GREW 上取得了52.16% 的排名第一准确率，超过了skeleton-based和 appearance-based Approaches。
</details></li>
</ul>
<hr>
<h2 id="Weighting-by-Tying-A-New-Approach-to-Weighted-Rank-Correlation"><a href="#Weighting-by-Tying-A-New-Approach-to-Weighted-Rank-Correlation" class="headerlink" title="Weighting by Tying: A New Approach to Weighted Rank Correlation"></a>Weighting by Tying: A New Approach to Weighted Rank Correlation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10622">http://arxiv.org/abs/2308.10622</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sascha Henzgen, Eyke Hüllermeier</li>
<li>for: 这篇论文是关于 Statistics 中的排名相关性度量的研究，旨在捕捉两个排名的相同集合中的项目之间的协调程度。</li>
<li>methods: 该论文使用了基于杂态顺序关系的尺度的权重版本，具有可 Parametrized 的权重分配方式，以及一种灵活的权重分配方法。</li>
<li>results: 研究人员通过提出了一种基于杂态顺序关系的尺度的权重版本，具有sound的形式基础和灵活的权重分配方式，实现了在排名相关性度量中增加权重。<details>
<summary>Abstract</summary>
Measures of rank correlation are commonly used in statistics to capture the degree of concordance between two orderings of the same set of items. Standard measures like Kendall's tau and Spearman's rho coefficient put equal emphasis on each position of a ranking. Yet, motivated by applications in which some of the positions (typically those on the top) are more important than others, a few weighted variants of these measures have been proposed. Most of these generalizations fail to meet desirable formal properties, however. Besides, they are often quite inflexible in the sense of committing to a fixed weighing scheme. In this paper, we propose a weighted rank correlation measure on the basis of fuzzy order relations. Our measure, called scaled gamma, is related to Goodman and Kruskal's gamma rank correlation. It is parametrized by a fuzzy equivalence relation on the rank positions, which in turn is specified conveniently by a so-called scaling function. This approach combines soundness with flexibility: it has a sound formal foundation and allows for weighing rank positions in a flexible way.
</details>
<details>
<summary>摘要</summary>
ranking 关系度量通常用于统计学中，用于捕捉两个顺序化的同一组项目之间的协调度。标准度量 LIKE Kendall tau 和 Spearman ρ 系数都强调每个排名位置的等效性。然而，基于应用场景中一些排名位置（通常是排名的顶部）更重要 than others，一些加权变体的度量已经被提议。然而，大多数这些扩展都不具备 Desirable 的正式性质，而且它们通常具有固定的加权方案。在这篇论文中，我们提出了基于杂元顺序关系的加权排名相关度量。我们的度量，叫做 scaled gamma，与 Goodman 和 Kruskal 的 gamma 排名相关度量相关。它是通过一个杂元等同关系来规定排名位置，这个关系再次是通过一个called scaling function来指定。这种方法结合了准确性和灵活性：它具有准确的形式基础，同时允许对排名位置进行灵活的归一化。
</details></li>
</ul>
<hr>
<h2 id="centroIDA-Cross-Domain-Class-Discrepancy-Minimization-Based-on-Accumulative-Class-Centroids-for-Imbalanced-Domain-Adaptation"><a href="#centroIDA-Cross-Domain-Class-Discrepancy-Minimization-Based-on-Accumulative-Class-Centroids-for-Imbalanced-Domain-Adaptation" class="headerlink" title="centroIDA: Cross-Domain Class Discrepancy Minimization Based on Accumulative Class-Centroids for Imbalanced Domain Adaptation"></a>centroIDA: Cross-Domain Class Discrepancy Minimization Based on Accumulative Class-Centroids for Imbalanced Domain Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10619">http://arxiv.org/abs/2308.10619</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaona Sun, Zhenyu Wu, Yichen Liu, Saier Hu, Zhiqiang Zhan, Yang Ji</li>
<li>for: 本研究targets the imbalanced domain adaptation (IDA) problem, which involves both covariate and long-tailed label shifts across domains.</li>
<li>methods: The proposed method, called centroIDA, uses a cross-domain class discrepancy minimization approach based on accumulative class-centroids alignment. This method includes class-based re-sampling, accumulative class-centroids alignment, and class-wise feature alignment.</li>
<li>results: The proposed method outperforms other state-of-the-art (SOTA) methods on the IDA problem, especially when the degree of label shift increases.<details>
<summary>Abstract</summary>
Unsupervised Domain Adaptation (UDA) approaches address the covariate shift problem by minimizing the distribution discrepancy between the source and target domains, assuming that the label distribution is invariant across domains. However, in the imbalanced domain adaptation (IDA) scenario, covariate and long-tailed label shifts both exist across domains. To tackle the IDA problem, some current research focus on minimizing the distribution discrepancies of each corresponding class between source and target domains. Such methods rely much on the reliable pseudo labels' selection and the feature distributions estimation for target domain, and the minority classes with limited numbers makes the estimations more uncertainty, which influences the model's performance. In this paper, we propose a cross-domain class discrepancy minimization method based on accumulative class-centroids for IDA (centroIDA). Firstly, class-based re-sampling strategy is used to obtain an unbiased classifier on source domain. Secondly, the accumulative class-centroids alignment loss is proposed for iterative class-centroids alignment across domains. Finally, class-wise feature alignment loss is used to optimize the feature representation for a robust classification boundary. A series of experiments have proved that our method outperforms other SOTA methods on IDA problem, especially with the increasing degree of label shift.
</details>
<details>
<summary>摘要</summary>
Unsupervised domain adaptation (UDA)方法解决 covariate shift 问题，即在源频率和目标频率之间的分布差异假设 label 分布是各频率之间不变。然而，在不平衡频率适应 (IDA) 场景中，covariate 和长尾标签各自存在在不同频率之间的差异。为解决 IDA 问题，一些当前研究强调在每个匹配的类之间分布差异的最小化。这些方法需要可靠的 pseudo label 选择和目标频率的特征分布估计，但是少数类的数量有限，这会使估计更加不确定，影响模型的性能。在这篇论文中，我们提出了一种基于积累类中心的 cross-domain 类差异最小化方法（centroIDA）。首先，使用 class-based 重采样策略来在源频率上获得不偏向的类ifier。其次，为了迭代类中心的对齐，我们提出了积累类中心对齐损失。最后，使用类别特征对齐损失来优化特征表示，以实现一个可靠的分类边界。经过一系列实验证明，我们的方法在 IDA 问题上表现出优于现有 SOTA 方法，特别是随着标签差异的增加。
</details></li>
</ul>
<hr>
<h2 id="ST-RAP-A-Spatio-Temporal-Framework-for-Real-Estate-Appraisal"><a href="#ST-RAP-A-Spatio-Temporal-Framework-for-Real-Estate-Appraisal" class="headerlink" title="ST-RAP: A Spatio-Temporal Framework for Real Estate Appraisal"></a>ST-RAP: A Spatio-Temporal Framework for Real Estate Appraisal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10609">http://arxiv.org/abs/2308.10609</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dojeon-ai/strap">https://github.com/dojeon-ai/strap</a></li>
<li>paper_authors: Hojoon Lee, Hawon Jeong, Byungkun Lee, Kyungyup Lee, Jaegul Choo</li>
<li>for: 这个论文是为了提出一种新的空间 temporal 框架，用于房地产评估。</li>
<li>methods: 这个框架使用层次结构和不同类型的图神经网络来同时捕捉空间和时间方面的关系和动态。</li>
<li>results: 通过对大规模房地产数据集进行广泛的实验，ST-RAP 表明了在房地产评估中同时考虑空间和时间方面的综合效果。Here’s the translation in English:</li>
<li>for: This paper proposes a novel Spatio-Temporal framework for Real estate APpraisal.</li>
<li>methods: The framework employs a hierarchical architecture with a heterogeneous graph neural network to simultaneously capture temporal dynamics and spatial relationships.</li>
<li>results: Through comprehensive experiments on a large-scale real estate dataset, ST-RAP outperforms previous methods, demonstrating the significant benefits of integrating spatial and temporal aspects in real estate appraisal.<details>
<summary>Abstract</summary>
In this paper, we introduce ST-RAP, a novel Spatio-Temporal framework for Real estate APpraisal. ST-RAP employs a hierarchical architecture with a heterogeneous graph neural network to encapsulate temporal dynamics and spatial relationships simultaneously. Through comprehensive experiments on a large-scale real estate dataset, ST-RAP outperforms previous methods, demonstrating the significant benefits of integrating spatial and temporal aspects in real estate appraisal. Our code and dataset are available at https://github.com/dojeon-ai/STRAP.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了ST-RAP，一种新的空间-时间框架，用于房地产评估。ST-RAP使用层次架构和不同类型的图 neural network来同时捕捉时间动态和空间关系。通过对大规模房地产数据进行全面的实验，ST-RAP表明了将空间和时间方面综合考虑在房地产评估中的重要性，并且超过了之前的方法。我们的代码和数据可以在https://github.com/dojeon-ai/STRAP上获取。
</details></li>
</ul>
<hr>
<h2 id="FocalDreamer-Text-driven-3D-Editing-via-Focal-fusion-Assembly"><a href="#FocalDreamer-Text-driven-3D-Editing-via-Focal-fusion-Assembly" class="headerlink" title="FocalDreamer: Text-driven 3D Editing via Focal-fusion Assembly"></a>FocalDreamer: Text-driven 3D Editing via Focal-fusion Assembly</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10608">http://arxiv.org/abs/2308.10608</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuhan Li, Yishun Dou, Yue Shi, Yu Lei, Xuanhong Chen, Yi Zhang, Peng Zhou, Bingbing Ni</li>
<li>for: 该论文旨在提供一种基于文本指示的高级3D编辑方法，以便在特定区域内进行精细编辑。</li>
<li>methods: 该方法基于文本蒸馈采样的技术，并使用geometry union和双轨渲染技术将独立的3D部件组合成完整的物体。此外，该方法还提出了geometry focal loss和样式一致准则，以促进焦点融合和整体外观的一致性。</li>
<li>results: 实验结果表明，FocalDreamer方法可以在量化和质量上提供高级3D编辑功能，并且在多种图形引擎下生成高质量的几何学和PBRTexture。<details>
<summary>Abstract</summary>
While text-3D editing has made significant strides in leveraging score distillation sampling, emerging approaches still fall short in delivering separable, precise and consistent outcomes that are vital to content creation. In response, we introduce FocalDreamer, a framework that merges base shape with editable parts according to text prompts for fine-grained editing within desired regions. Specifically, equipped with geometry union and dual-path rendering, FocalDreamer assembles independent 3D parts into a complete object, tailored for convenient instance reuse and part-wise control. We propose geometric focal loss and style consistency regularization, which encourage focal fusion and congruent overall appearance. Furthermore, FocalDreamer generates high-fidelity geometry and PBR textures which are compatible with widely-used graphics engines. Extensive experiments have highlighted the superior editing capabilities of FocalDreamer in both quantitative and qualitative evaluations.
</details>
<details>
<summary>摘要</summary>
“文本3D编辑在利用分数浸泡样本方面已经做出了重要进步，但现有方法仍然缺乏提供分解、精准和一致的结果，这些结果对内容创建至关重要。为此，我们介绍了FOCAL Dreamer框架，该框架将基本形状与可编辑部分相结合，根据文本提示进行细致的编辑，并在 Desired 区域内进行精细控制。Specifically, FOCAL Dreamer 使用geometry union和双路渲染技术，将独立的3D部件组合成完整的 объек，且适用于便捷的实例重用和部件控制。我们还提出了 геометрической专注损失和风格一致 regularization，这两种正则化激励核心融合和一致的整体外观。此外，FOCAL Dreamer 生成高质量的几何学和PBR文本ure，与广泛使用的图形引擎相容。我们的实验表明，FOCAL Dreamer 在量化和质量两个方面的评价都具有明显的优势。”
</details></li>
</ul>
<hr>
<h2 id="Analyzing-Complex-Systems-with-Cascades-Using-Continuous-Time-Bayesian-Networks"><a href="#Analyzing-Complex-Systems-with-Cascades-Using-Continuous-Time-Bayesian-Networks" class="headerlink" title="Analyzing Complex Systems with Cascades Using Continuous-Time Bayesian Networks"></a>Analyzing Complex Systems with Cascades Using Continuous-Time Bayesian Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10606">http://arxiv.org/abs/2308.10606</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alessandro Bregoli, Karin Rathsman, Marco Scutari, Fabio Stella, Søren Wengel Mogensen</li>
<li>for: 本研究旨在分析复杂系统中的协同事件链的冲击行为，以便更好地理解系统中哪些状态会触发冲击行为。</li>
<li>methods: 本研究使用连续时间权重网络（CTBN）模型来分析复杂系统中的协同事件链，并提出了一种新的知识提取方法来从CTBN中提取有用的信息。</li>
<li>results: 研究人员通过应用CTBN模型和新的知识提取方法，成功地找到了可能导致冲击行为的系统状态，并获得了可读的输出结果。<details>
<summary>Abstract</summary>
Interacting systems of events may exhibit cascading behavior where events tend to be temporally clustered. While the cascades themselves may be obvious from the data, it is important to understand which states of the system trigger them. For this purpose, we propose a modeling framework based on continuous-time Bayesian networks (CTBNs) to analyze cascading behavior in complex systems. This framework allows us to describe how events propagate through the system and to identify likely sentry states, that is, system states that may lead to imminent cascading behavior. Moreover, CTBNs have a simple graphical representation and provide interpretable outputs, both of which are important when communicating with domain experts. We also develop new methods for knowledge extraction from CTBNs and we apply the proposed methodology to a data set of alarms in a large industrial system.
</details>
<details>
<summary>摘要</summary>
文本翻译为简化中文：复杂系统中的事件互动可能会展现堆叠行为，其中事件倾向于在时间上叠加。虽然堆叠本身可能从数据中容易看到，但是要理解触发它们的系统状态非常重要。为了解决这个问题，我们提出了基于连续时间概率网络（CTBN）的模型化框架，用于分析复杂系统中的堆叠行为。这个框架可以描述事件如何在系统中传播，并识别可能导致堆叠行为的系统状态。此外，CTBN具有简单的图形表示和可解释的输出，这两点都是与领域专家交流时非常重要。我们还开发了新的知识提取方法，并将该方法应用于一个大型工业系统的数据集。
</details></li>
</ul>
<hr>
<h2 id="BackTrack-Robust-template-update-via-Backward-Tracking-of-candidate-template"><a href="#BackTrack-Robust-template-update-via-Backward-Tracking-of-candidate-template" class="headerlink" title="BackTrack: Robust template update via Backward Tracking of candidate template"></a>BackTrack: Robust template update via Backward Tracking of candidate template</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10604">http://arxiv.org/abs/2308.10604</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongwook Lee, Wonjun Choi, Seohyung Lee, ByungIn Yoo, Eunho Yang, Seongju Hwang</li>
<li>for: 提高视觉对象跟踪性能，解决模板更新引起的模型漂移问题</li>
<li>methods: 提出BackTrack方法，通过倒计时追踪候选模板，计算候选模板的可靠程度，选择可靠的候选模板更新模板</li>
<li>results: 对多个跟踪 benchmark 进行了广泛的实验，证明BackTrack方法可以提高模板更新精度，达到最新的状态角度表现In English, this translates to:</li>
<li>for: Improving visual object tracking performance, solving the template drift problem caused by template updates</li>
<li>methods: Propose BackTrack method, which backward tracks candidates on past frames to calculate their reliability, and selects reliable candidates to update the template</li>
<li>results: Extensive experiments on various tracking benchmarks demonstrate that BackTrack method can improve template update accuracy, achieving state-of-the-art performance.<details>
<summary>Abstract</summary>
Variations of target appearance such as deformations, illumination variance, occlusion, etc., are the major challenges of visual object tracking that negatively impact the performance of a tracker. An effective method to tackle these challenges is template update, which updates the template to reflect the change of appearance in the target object during tracking. However, with template updates, inadequate quality of new templates or inappropriate timing of updates may induce a model drift problem, which severely degrades the tracking performance. Here, we propose BackTrack, a robust and reliable method to quantify the confidence of the candidate template by backward tracking it on the past frames. Based on the confidence score of candidates from BackTrack, we can update the template with a reliable candidate at the right time while rejecting unreliable candidates. BackTrack is a generic template update scheme and is applicable to any template-based trackers. Extensive experiments on various tracking benchmarks verify the effectiveness of BackTrack over existing template update algorithms, as it achieves SOTA performance on various tracking benchmarks.
</details>
<details>
<summary>摘要</summary>
<<SYS>> traduction de texte en chinois simplifiéVariations de l'apparence cible telles que les déformations, les variations d'éclairage, les occlusions, etc., sont les principales difficultés de la suivi d'objets visuels qui négativement affectent la performance d'un traqueur. Une méthode efficace pour relever ces défis est l'actualisation du modèle, qui met à jour le modèle pour refléter les changements d'apparence dans l'objet cible pendant la traque. Cependant, avec des mises à jour de modèle, la qualité inadéquate des nouveaux modèles ou la mise à jour inopportune peuvent entraîner un problème de drift de modèle, qui altère gravement la performance de la traque. Voici ce que nous proposons : BackTrack, un méthode robuste et fiable pour estimer la confiance du modèle de candidature en backward tracking les frames passées. En fonction de la note de confiance des candidats de BackTrack, nous pouvons mettre à jour le modèle avec un candidat fiable à temps opportun, tout en rejetant les candidats non fiables. BackTrack est un schéma d'actualisation de modèle générique et est applicable à tous les trackers de modèle. Les expériences extensives sur divers bancs d'essai de suivi ont verifié l'efficacité de BackTrack par rapport aux algorithmes d'actualisation de modèle existants, car elle atteint des performances SOTA sur divers bancs d'essai de suivi.
</details></li>
</ul>
<hr>
<h2 id="Improving-the-Transferability-of-Adversarial-Examples-with-Arbitrary-Style-Transfer"><a href="#Improving-the-Transferability-of-Adversarial-Examples-with-Arbitrary-Style-Transfer" class="headerlink" title="Improving the Transferability of Adversarial Examples with Arbitrary Style Transfer"></a>Improving the Transferability of Adversarial Examples with Arbitrary Style Transfer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10601">http://arxiv.org/abs/2308.10601</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhijin-ge/stm">https://github.com/zhijin-ge/stm</a></li>
<li>paper_authors: Zhijin Ge, Fanhua Shang, Hongying Liu, Yuanyuan Liu, Liang Wan, Wei Feng, Xiaosen Wang</li>
<li>for: 本研究旨在提高黑盒 Setting 下 adversarial example 的转移性，通过将具有不同领域的资料作为增强资料。</li>
<li>methods: 我们提出了一种名为 Style Transfer Method (STM) 的新攻击方法，它利用一个提案的自由式类别转换网络将图像转换为不同的领域。为了确保涂抹后的图像仍保持 semantics 信息，我们在涂抹过程中进行了精确的调整和混合。</li>
<li>results: 我们在 ImageNet-compatible 数据集上进行了广泛的实验，结果显示，我们的提案的 STM 方法可以在 normally 训练的模型或 adversarially 训练的模型上提高 adversarial transferability，较前者的 state-of-the-art input transformation-based 攻击方法。代码可以在 GitHub 上找到：<a target="_blank" rel="noopener" href="https://github.com/Zhijin-Ge/STM%E3%80%82">https://github.com/Zhijin-Ge/STM。</a><details>
<summary>Abstract</summary>
Deep neural networks are vulnerable to adversarial examples crafted by applying human-imperceptible perturbations on clean inputs. Although many attack methods can achieve high success rates in the white-box setting, they also exhibit weak transferability in the black-box setting. Recently, various methods have been proposed to improve adversarial transferability, in which the input transformation is one of the most effective methods. In this work, we notice that existing input transformation-based works mainly adopt the transformed data in the same domain for augmentation. Inspired by domain generalization, we aim to further improve the transferability using the data augmented from different domains. Specifically, a style transfer network can alter the distribution of low-level visual features in an image while preserving semantic content for humans. Hence, we propose a novel attack method named Style Transfer Method (STM) that utilizes a proposed arbitrary style transfer network to transform the images into different domains. To avoid inconsistent semantic information of stylized images for the classification network, we fine-tune the style transfer network and mix up the generated images added by random noise with the original images to maintain semantic consistency and boost input diversity. Extensive experimental results on the ImageNet-compatible dataset show that our proposed method can significantly improve the adversarial transferability on either normally trained models or adversarially trained models than state-of-the-art input transformation-based attacks. Code is available at: https://github.com/Zhijin-Ge/STM.
</details>
<details>
<summary>摘要</summary>
Extensive experimental results on the ImageNet-compatible dataset show that our proposed method can significantly improve the adversarial transferability on either normally trained models or adversarially trained models than state-of-the-art input transformation-based attacks. Code is available at: https://github.com/Zhijin-Ge/STM.Translated into Simplified Chinese:深度神经网络受到人类不可见的攻击例子的威胁，尽管许多攻击方法在白盒设定下可以达到高成功率，但它们也表现出了软输送性。在黑盒设定下，许多方法已经被提出来改进攻击传输性，其中输入转换是最有效的方法。在这项工作中，我们注意到现有的输入转换基于工作主要采用转换后的数据进行增强。受到域泛化的启发，我们想要进一步提高传输性，使用不同域的数据进行增强。Specifically, a style transfer network can alter the distribution of low-level visual features in an image while preserving semantic content for humans. Hence, we propose a novel attack method named Style Transfer Method (STM) that utilizes a proposed arbitrary style transfer network to transform the images into different domains. To avoid inconsistent semantic information of stylized images for the classification network, we fine-tune the style transfer network and mix up the generated images added by random noise with the original images to maintain semantic consistency and boost input diversity. 广泛的实验结果表明，我们提出的方法可以在 ImageNet  compatible 数据集上对 normally 训练的模型和 adversarially 训练的模型进行较好的攻击传输性，比采用 state-of-the-art 的输入转换基于攻击方法高。代码可以在：https://github.com/Zhijin-Ge/STM 中找到。
</details></li>
</ul>
<hr>
<h2 id="Image-free-Classifier-Injection-for-Zero-Shot-Classification"><a href="#Image-free-Classifier-Injection-for-Zero-Shot-Classification" class="headerlink" title="Image-free Classifier Injection for Zero-Shot Classification"></a>Image-free Classifier Injection for Zero-Shot Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10599">http://arxiv.org/abs/2308.10599</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/explainableml/imagefreezsl">https://github.com/explainableml/imagefreezsl</a></li>
<li>paper_authors: Anders Christensen, Massimiliano Mancini, A. Sophia Koepke, Ole Winther, Zeynep Akata</li>
<li>for: 这个研究目的是为了将预训模型转换为零shot学习模型，不需要训练图像数据。</li>
<li>methods: 我们提出了一种名为Image-free Classifier Injection with Semantics（ICIS）的方法，可以在预训模型上附加零shot学习能力，不需要任何图像数据。ICIS使用两个encoder-decoder网络，将描述器转换为分类器的重量，并且运用(cross-)重建和偏角损失来调整解oding过程。</li>
<li>results: 实验结果显示，ICIS可以在 benchmark ZSL 数据集上生成出高性能的零shot分类器。code可以在<a target="_blank" rel="noopener" href="https://github.com/ExplainableML/ImageFreeZSL">https://github.com/ExplainableML/ImageFreeZSL</a> 上获取。<details>
<summary>Abstract</summary>
Zero-shot learning models achieve remarkable results on image classification for samples from classes that were not seen during training. However, such models must be trained from scratch with specialised methods: therefore, access to a training dataset is required when the need for zero-shot classification arises. In this paper, we aim to equip pre-trained models with zero-shot classification capabilities without the use of image data. We achieve this with our proposed Image-free Classifier Injection with Semantics (ICIS) that injects classifiers for new, unseen classes into pre-trained classification models in a post-hoc fashion without relying on image data. Instead, the existing classifier weights and simple class-wise descriptors, such as class names or attributes, are used. ICIS has two encoder-decoder networks that learn to reconstruct classifier weights from descriptors (and vice versa), exploiting (cross-)reconstruction and cosine losses to regularise the decoding process. Notably, ICIS can be cheaply trained and applied directly on top of pre-trained classification models. Experiments on benchmark ZSL datasets show that ICIS produces unseen classifier weights that achieve strong (generalised) zero-shot classification performance. Code is available at https://github.com/ExplainableML/ImageFreeZSL .
</details>
<details>
<summary>摘要</summary>
Zero-shot learning模型在图像分类任务中达到了非常出色的结果，但是这些模型必须通过特殊的方法进行训练，因此需要训练集的存在。在这篇论文中，我们想要让预训练模型具有零批学习的分类能力，而无需使用图像数据。我们通过我们提出的图像自由分类插入（ICIS）技术来实现这一目标。ICIS使用两个Encoder-Decoder网络来学习从描述符（包括类名或属性）中重建分类器权重，不需要使用图像数据。我们通过（cross-）重建和归一化损失来规范解码过程。吸引人的是，ICIS可以便宜地训练和应用于预训练分类模型之上。我们在 benchmark ZSL 数据集上进行了实验，并证明了 ICIS 可以生成出高性能的零批分类器。代码可以在 GitHub 上找到：https://github.com/ExplainableML/ImageFreeZSL。
</details></li>
</ul>
<hr>
<h2 id="RADIANCE-Radio-Frequency-Adversarial-Deep-learning-Inference-for-Automated-Network-Coverage-Estimation"><a href="#RADIANCE-Radio-Frequency-Adversarial-Deep-learning-Inference-for-Automated-Network-Coverage-Estimation" class="headerlink" title="RADIANCE: Radio-Frequency Adversarial Deep-learning Inference for Automated Network Coverage Estimation"></a>RADIANCE: Radio-Frequency Adversarial Deep-learning Inference for Automated Network Coverage Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10584">http://arxiv.org/abs/2308.10584</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sopan Sarkar, Mohammad Hossein Manshaei, Marwan Krunz</li>
<li>for: 这个研究是为了自动生成无线电网络的覆盖地图（RF maps），以便于无线网络的计划、Access Point和基站的位置设置、地点定位和覆盖估计。</li>
<li>methods: 这个研究使用了一个对抗式深度学习推断（GAN）基础的方法，叫做 radio-frequency adversarial deep-learning inference for automated network coverage estimation（RADIANCE），它使用了一个 semantic map，一个高级的内部环境表示，来编码空间关系和内部环境中物体的特征。这个方法还使用了一个新的梯度基于的损失函数，它计算了从环境中的点到接收信号强度（RSS）值的变化的大小和方向。</li>
<li>results: 这个研究的结果显示，使用 RADIANCE 可以实现覆盖地图的自动生成，并且与射线追踪模拟相比，RADIANCE 可以得到较佳的结果，其中的 mean average error（MAE）为 0.09，root-mean-squared error（RMSE）为 0.29，peak signal-to-noise ratio（PSNR）为 10.78，multi-scale structural similarity index（MS-SSIM）为 0.80。<details>
<summary>Abstract</summary>
Radio-frequency coverage maps (RF maps) are extensively utilized in wireless networks for capacity planning, placement of access points and base stations, localization, and coverage estimation. Conducting site surveys to obtain RF maps is labor-intensive and sometimes not feasible. In this paper, we propose radio-frequency adversarial deep-learning inference for automated network coverage estimation (RADIANCE), a generative adversarial network (GAN) based approach for synthesizing RF maps in indoor scenarios. RADIANCE utilizes a semantic map, a high-level representation of the indoor environment to encode spatial relationships and attributes of objects within the environment and guide the RF map generation process. We introduce a new gradient-based loss function that computes the magnitude and direction of change in received signal strength (RSS) values from a point within the environment. RADIANCE incorporates this loss function along with the antenna pattern to capture signal propagation within a given indoor configuration and generate new patterns under new configuration, antenna (beam) pattern, and center frequency. Extensive simulations are conducted to compare RADIANCE with ray-tracing simulations of RF maps. Our results show that RADIANCE achieves a mean average error (MAE) of 0.09, root-mean-squared error (RMSE) of 0.29, peak signal-to-noise ratio (PSNR) of 10.78, and multi-scale structural similarity index (MS-SSIM) of 0.80.
</details>
<details>
<summary>摘要</summary>
射频覆盖地图（RF 地图）在无线网络中广泛使用，用于容量规划、Access Point和基站的布局、地理位置和覆盖估计。进行场景调查以获取RF地图是劳动密集且不可靠。在这篇论文中，我们提出了射频对抗深度学习推断（RADIANCE），一种基于生成对抗网络（GAN）的方法，用于自动生成室内场景中的射频地图。RADIANCE利用了 semantic map，一个高级的室内环境表示，以编码空间关系和环境中对象的特征。我们引入了一个新的梯度基于损失函数，计算从环境中的点的受信号强度（RSS）值的变化大小和方向。RADIANCE将这个损失函数与天线 patrón 混合，以捕捉室内配置下的信号传播特性，并生成新的射频地图。我们对RADIANCE与射频投影 simulations进行了广泛的比较。结果显示，RADIANCE的 mean average error（MAE）为0.09，root-mean-squared error（RMSE）为0.29，受信号强度比（PSNR）为10.78，和多尺度结构相似度（MS-SSIM）为0.80。
</details></li>
</ul>
<hr>
<h2 id="Pseudo-online-framework-for-BCI-evaluation-A-MOABB-perspective"><a href="#Pseudo-online-framework-for-BCI-evaluation-A-MOABB-perspective" class="headerlink" title="Pseudo-online framework for BCI evaluation: A MOABB perspective"></a>Pseudo-online framework for BCI evaluation: A MOABB perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11656">http://arxiv.org/abs/2308.11656</a></li>
<li>repo_url: None</li>
<li>paper_authors: Igor Carrara, Théodore Papadopoulo</li>
<li>for: 本研究旨在扩展现有的MOABB框架，以便在 Pseudo-online 模式下比较不同算法的性能。</li>
<li>methods: 本研究使用了 overlaping sliding windows 技术，并引入了停靠状态事件，以考虑所有不同的思维方式。</li>
<li>results: 研究分析了过去 15 年的州际顶点算法，并对多个 Motor Imagery 数据集进行了分析，显示了两种方法之间的统计上的差异。<details>
<summary>Abstract</summary>
Objective: BCI (Brain-Computer Interface) technology operates in three modes: online, offline, and pseudo-online. In the online mode, real-time EEG data is constantly analyzed. In offline mode, the signal is acquired and processed afterwards. The pseudo-online mode processes collected data as if they were received in real-time. The main difference is that the offline mode often analyzes the whole data, while the online and pseudo-online modes only analyze data in short time windows. Offline analysis is usually done with asynchronous BCIs, which restricts analysis to predefined time windows. Asynchronous BCI, compatible with online and pseudo-online modes, allows flexible mental activity duration. Offline processing tends to be more accurate, while online analysis is better for therapeutic applications. Pseudo-online implementation approximates online processing without real-time constraints. Many BCI studies being offline introduce biases compared to real-life scenarios, impacting classification algorithm performance. Approach: The objective of this research paper is therefore to extend the current MOABB framework, operating in offline mode, so as to allow a comparison of different algorithms in a pseudo-online setting with the use of a technology based on overlapping sliding windows. To do this will require the introduction of a idle state event in the dataset that takes into account all different possibilities that are not task thinking. To validate the performance of the algorithms we will use the normalized Matthews Correlation Coefficient (nMCC) and the Information Transfer Rate (ITR). Main results: We analyzed the state-of-the-art algorithms of the last 15 years over several Motor Imagery (MI) datasets composed by several subjects, showing the differences between the two approaches from a statistical point of view. Significance: The ability to analyze the performance of different algorithms in offline and pseudo-online modes will allow the BCI community to obtain more accurate and comprehensive reports regarding the performance of classification algorithms.
</details>
<details>
<summary>摘要</summary>
目标：BCI（脑computer接口）技术运行在三种模式之一：在线、离线和假在线模式。在在线模式下，实时EEG数据被不断分析。在离线模式下，信号被获取并处理后进行分析。假在线模式处理收集的数据，如果收集的数据是在实时进行处理的话。主要的区别在于离线模式通常分析整个数据，而在线和假在线模式只分析短时间窗口中的数据。离线分析通常更加准确，而在线分析更适合治疗应用。假在线实现方式模拟在线处理，不受实时限制。许多BCI研究都是离线进行，这会引入比实际情况更多的偏见，影响分类算法的性能。方法：为了扩展当前的MOABB框架，它在离线模式下运行，以便对不同算法的比较在假在线设置下进行。为此，需要引入一个假静止状态事件，考虑所有不同的可能性，不是任务思考。为了验证算法的性能，我们使用 норма化的玛特维克相互相关系数（nMCC）和信息传输率（ITR）。结果：我们对过去15年的State-of-the-art算法进行分析，并在多个motor imagery（MI）数据集上进行了比较。显示了两种方法的 statistically 的差异。重要性：能够在离线和假在线模式下分析不同算法的性能，将BCI社区获得更加准确和全面的报告关于分类算法的性能。
</details></li>
</ul>
<hr>
<h2 id="Overcoming-Overconfidence-for-Active-Learning"><a href="#Overcoming-Overconfidence-for-Active-Learning" class="headerlink" title="Overcoming Overconfidence for Active Learning"></a>Overcoming Overconfidence for Active Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10571">http://arxiv.org/abs/2308.10571</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yujin Hwang, Won Jo, Juyoung Hong, Yukyung Choi</li>
<li>for: 这篇论文主要关注于解决人工智能学习中的数据选择问题，以提高模型的准确性和可靠性。</li>
<li>methods: 本论文提出了两种解决过去误差问题的方法：一是将数据扩展为多个不同的数据分布，以帮助模型更好地调整；二是根据预测margin的排名，选择数据进行调整，以避免选择过度自信的数据。</li>
<li>results: 经过各种实验和分析， authors发现了两种方法可以有效地缓和过度自信的问题，并且这些方法可以轻松地应用。<details>
<summary>Abstract</summary>
It is not an exaggeration to say that the recent progress in artificial intelligence technology depends on large-scale and high-quality data. Simultaneously, a prevalent issue exists everywhere: the budget for data labeling is constrained. Active learning is a prominent approach for addressing this issue, where valuable data for labeling is selected through a model and utilized to iteratively adjust the model. However, due to the limited amount of data in each iteration, the model is vulnerable to bias; thus, it is more likely to yield overconfident predictions. In this paper, we present two novel methods to address the problem of overconfidence that arises in the active learning scenario. The first is an augmentation strategy named Cross-Mix-and-Mix (CMaM), which aims to calibrate the model by expanding the limited training distribution. The second is a selection strategy named Ranked Margin Sampling (RankedMS), which prevents choosing data that leads to overly confident predictions. Through various experiments and analyses, we are able to demonstrate that our proposals facilitate efficient data selection by alleviating overconfidence, even though they are readily applicable.
</details>
<details>
<summary>摘要</summary>
不是订正说，现代人工智能技术的进步几乎完全取决于大规模高质量数据。然而，一个普遍存在的问题是 everywhere：数据标注预算受限。活动学习是一种主要的方法来解决这个问题，其中通过模型选择有价值的数据进行标注，并使用这些数据来逐次调整模型。然而，由于每次迭代中的数据量有限，模型容易受到偏见，因此更可能产生过optimistic预测。在这篇论文中，我们提出了两种解决活动学习场景中出现的过optimistic问题的方法。首先是一种扩展训练分布的扩展策略名为 Cross-Mix-and-Mix (CMaM)，其目的是使模型更加准确。其次是一种名为 Ranked Margin Sampling (RankedMS)的选择策略，它避免选择导致过optimistic预测的数据。通过多种实验和分析，我们能够证明我们的建议可以有效地选择数据，减少过optimistic预测，即使它们可以应用。
</details></li>
</ul>
<hr>
<h2 id="Decentralized-Riemannian-Conjugate-Gradient-Method-on-the-Stiefel-Manifold"><a href="#Decentralized-Riemannian-Conjugate-Gradient-Method-on-the-Stiefel-Manifold" class="headerlink" title="Decentralized Riemannian Conjugate Gradient Method on the Stiefel Manifold"></a>Decentralized Riemannian Conjugate Gradient Method on the Stiefel Manifold</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10547">http://arxiv.org/abs/2308.10547</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jun Chen, Haishan Ye, Mengmeng Wang, Tianxin Huang, Guang Dai, Ivor W. Tsang, Yong Liu</li>
<li>for: 这个论文 targets at solving optimization problems on the Stiefel manifold, a non-convex set, using a decentralized Riemannian conjugate gradient descent (DRCGD) method.</li>
<li>methods: 该方法使用了一种分布式的agent网络，每个agent负责一个本地函数，并且通过无向连接图进行交互。DRCGD方法不需要进行Expensive的里曼几何运算，从而减少了每个agent的计算复杂性。</li>
<li>results: 该论文提出了一种首次在分布式里曼空间中实现全球收敛的DRCGD方法，并且证明了该方法的 globally convergence。<details>
<summary>Abstract</summary>
The conjugate gradient method is a crucial first-order optimization method that generally converges faster than the steepest descent method, and its computational cost is much lower than the second-order methods. However, while various types of conjugate gradient methods have been studied in Euclidean spaces and on Riemannian manifolds, there has little study for those in distributed scenarios. This paper proposes a decentralized Riemannian conjugate gradient descent (DRCGD) method that aims at minimizing a global function over the Stiefel manifold. The optimization problem is distributed among a network of agents, where each agent is associated with a local function, and communication between agents occurs over an undirected connected graph. Since the Stiefel manifold is a non-convex set, a global function is represented as a finite sum of possibly non-convex (but smooth) local functions. The proposed method is free from expensive Riemannian geometric operations such as retractions, exponential maps, and vector transports, thereby reducing the computational complexity required by each agent. To the best of our knowledge, DRCGD is the first decentralized Riemannian conjugate gradient algorithm to achieve global convergence over the Stiefel manifold.
</details>
<details>
<summary>摘要</summary>
“ conjugate gradient 方法是一种重要的首尔顺化方法，通常比坡度下降法快速 converges，且计算成本较低于第二项方法。然而，在分布式场景中，各种 conjugate gradient 方法已经在欧几何空间和里曼尼投影上进行了许多研究，但对于分布式场景的研究却有很少。这篇论文提出了一种分布式里曼尼 conjugate gradient descent（DRCGD）方法，旨在全局函数的最小化，这个函数是在Stiefel manifold上的一个全部函数。实际上，这个问题是分布式在一个无向网络中的一个问题，每个代理人都有一个本地函数，并且在各个代理人之间进行了无向网络上的通信。由于Stiefel manifold是一个非凸集，因此全球函数是一个可能非凸（但是光滑的）的全部函数。提案的方法不需要 expensive Riemannian geometric operations，例如投影、对映图和向量运输，因此每个代理人的计算复杂度很低。到目前为止，DRCGD 是首个在 Stiefel manifold 上实现全球均衡的分布式里曼尼 conjugate gradient 算法。”
</details></li>
</ul>
<hr>
<h2 id="Towards-Accelerated-Model-Training-via-Bayesian-Data-Selection"><a href="#Towards-Accelerated-Model-Training-via-Bayesian-Data-Selection" class="headerlink" title="Towards Accelerated Model Training via Bayesian Data Selection"></a>Towards Accelerated Model Training via Bayesian Data Selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10544">http://arxiv.org/abs/2308.10544</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhijie Deng, Peng Cui, Jun Zhu</li>
<li>for: This paper is written to address the problem of mislabeled, duplicated, or biased data in real-world scenarios, which can hinder model convergence and prolong training.</li>
<li>methods: The paper proposes a more reasonable data selection principle by examining the data’s impact on the model’s generalization loss, and incorporates off-the-shelf zero-shot predictors built on large-scale pre-trained models.</li>
<li>results: The paper performs extensive empirical studies on challenging benchmarks with considerable data noise and imbalance in the online batch selection scenario, and observes superior training efficiency over competitive baselines. Specifically, the method achieves similar predictive performance with significantly fewer training iterations than leading data selection methods on the challenging WebVision benchmark.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文是为了解决现实世界中的杂乱数据问题，包括杂乱标签、重复数据和偏见数据，这些问题会延长模型训练和抑制模型的准确性。</li>
<li>methods: 论文提出了一种更加合理的数据选择原则，即通过考虑数据对模型通用损失的影响来选择数据。此外，它还 incorporates off-the-shelf zero-shot predictors built on large-scale pre-trained models。</li>
<li>results: 论文在实际中进行了严格的实验研究，包括在具有较大数据噪和数据不均衡的online批处理场景中进行了训练效率的比较。研究结果显示，该方法在比较难的WebVision数据集上可以在相对较少的训练迭代数下达到类似的预测性能，与主流数据选择方法相比。<details>
<summary>Abstract</summary>
Mislabeled, duplicated, or biased data in real-world scenarios can lead to prolonged training and even hinder model convergence. Traditional solutions prioritizing easy or hard samples lack the flexibility to handle such a variety simultaneously. Recent work has proposed a more reasonable data selection principle by examining the data's impact on the model's generalization loss. However, its practical adoption relies on less principled approximations and additional clean holdout data. This work solves these problems by leveraging a lightweight Bayesian treatment and incorporating off-the-shelf zero-shot predictors built on large-scale pre-trained models. The resulting algorithm is efficient and easy-to-implement. We perform extensive empirical studies on challenging benchmarks with considerable data noise and imbalance in the online batch selection scenario, and observe superior training efficiency over competitive baselines. Notably, on the challenging WebVision benchmark, our method can achieve similar predictive performance with significantly fewer training iterations than leading data selection methods.
</details>
<details>
<summary>摘要</summary>
错分、重复或偏袋数据在实际场景中可能导致模型训练延长，甚至阻碍模型平衡。传统解决方案会优先级化易于或困难的样本，缺乏适应性来处理多种样本同时。现有研究提出了一种更合理的数据选择原则，通过评估模型通用损失来评估数据的影响。然而，其实践几乎依赖于不原则的近似和额外的干净保留数据。这种方法解决了这些问题，通过利用轻量级权重抽象和大规模预训练模型建立的零shot预测器。该算法是高效和易于实现。我们在具有较大数据噪音和不均衡的在线批处理场景中进行了广泛的实验研究，并观察到了与竞争对手基eline的高效训练性能。尤其是在WebVision标准园 benchmark上，我们的方法可以在与其他数据选择方法相比较同等预测性能的情况下，通过significantly fewer training iterations获得更高的训练效率。
</details></li>
</ul>
<hr>
<h2 id="Learning-Weakly-Convex-Regularizers-for-Convergent-Image-Reconstruction-Algorithms"><a href="#Learning-Weakly-Convex-Regularizers-for-Convergent-Image-Reconstruction-Algorithms" class="headerlink" title="Learning Weakly Convex Regularizers for Convergent Image-Reconstruction Algorithms"></a>Learning Weakly Convex Regularizers for Convergent Image-Reconstruction Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10542">http://arxiv.org/abs/2308.10542</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexis Goujon, Sebastian Neumayer, Michael Unser</li>
<li>for: 本研究旨在学习非凸正则化方法，并将其的弱凸性模块规定为上限。这些正则化方法可以用来实现变分降噪器，并且具有少量参数（ fewer than 15,000）和可解释性。</li>
<li>methods: 本研究使用了数字实验来证明这些变分降噪器可以超越凸正则化方法和BM3D降噪器的性能。此外，学习的正则化方法还可以用于解决反射问题，并且可以提供可靠的分布式迭代方法。</li>
<li>results: 数据驱动的实验结果表明，使用学习的非凸正则化方法可以在CT和MRI重建中提供优秀的平衡点，并且在性能、参数数量、保证和可解释性等方面表现出色，在比较其他数据驱动方法时表现更好。<details>
<summary>Abstract</summary>
We propose to learn non-convex regularizers with a prescribed upper bound on their weak-convexity modulus. Such regularizers give rise to variational denoisers that minimize a convex energy. They rely on few parameters (less than 15,000) and offer a signal-processing interpretation as they mimic handcrafted sparsity-promoting regularizers. Through numerical experiments, we show that such denoisers outperform convex-regularization methods as well as the popular BM3D denoiser. Additionally, the learned regularizer can be deployed to solve inverse problems with iterative schemes that provably converge. For both CT and MRI reconstruction, the regularizer generalizes well and offers an excellent tradeoff between performance, number of parameters, guarantees, and interpretability when compared to other data-driven approaches.
</details>
<details>
<summary>摘要</summary>
我们提议使用具有固定上界的弱矩形变数的非凸调教器。这些调教器将实现一个内部平面上的内附式数据过滤，并且具有少于15,000个参数。它们可以视为手工设计的稀疏化调教器，并且通过数据实验显示，这些调教器可以超过对称调教器和BM3D调教器的性能。此外，学习的调教器可以用迭代方案来解决反射问题，并且可以提供可靠的收敛保证。在CT和MRI重建中，调教器具有良好的泛化性和优秀的贡献比例、保证和可读性，在与其他数据驱动方法比较之下表现出色。
</details></li>
</ul>
<hr>
<h2 id="KGrEaT-A-Framework-to-Evaluate-Knowledge-Graphs-via-Downstream-Tasks"><a href="#KGrEaT-A-Framework-to-Evaluate-Knowledge-Graphs-via-Downstream-Tasks" class="headerlink" title="KGrEaT: A Framework to Evaluate Knowledge Graphs via Downstream Tasks"></a>KGrEaT: A Framework to Evaluate Knowledge Graphs via Downstream Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10537">http://arxiv.org/abs/2308.10537</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicolas Heist, Sven Hertling, Heiko Paulheim</li>
<li>for: 这种研究旨在创建、扩展或完善知识图，以便创建更大、更正确或更多样化的知识图。</li>
<li>methods: 这些研究通常使用创建或扩展知识图的方法，而不是评估下游任务的性能。</li>
<li>results: KGrEaT 框架可以评估知识图的质量，并对不同的知识图进行比较，以确定它们在实际任务上的表现。<details>
<summary>Abstract</summary>
In recent years, countless research papers have addressed the topics of knowledge graph creation, extension, or completion in order to create knowledge graphs that are larger, more correct, or more diverse. This research is typically motivated by the argumentation that using such enhanced knowledge graphs to solve downstream tasks will improve performance. Nonetheless, this is hardly ever evaluated. Instead, the predominant evaluation metrics - aiming at correctness and completeness - are undoubtedly valuable but fail to capture the complete picture, i.e., how useful the created or enhanced knowledge graph actually is. Further, the accessibility of such a knowledge graph is rarely considered (e.g., whether it contains expressive labels, descriptions, and sufficient context information to link textual mentions to the entities of the knowledge graph). To better judge how well knowledge graphs perform on actual tasks, we present KGrEaT - a framework to estimate the quality of knowledge graphs via actual downstream tasks like classification, clustering, or recommendation. Instead of comparing different methods of processing knowledge graphs with respect to a single task, the purpose of KGrEaT is to compare various knowledge graphs as such by evaluating them on a fixed task setup. The framework takes a knowledge graph as input, automatically maps it to the datasets to be evaluated on, and computes performance metrics for the defined tasks. It is built in a modular way to be easily extendable with additional tasks and datasets.
</details>
<details>
<summary>摘要</summary>
To better evaluate the performance of knowledge graphs on actual tasks, we propose KGrEaT, a framework for estimating the quality of knowledge graphs through actual downstream tasks such as classification, clustering, or recommendation. Instead of comparing different methods of processing knowledge graphs for a single task, KGrEaT compares various knowledge graphs by evaluating them on a fixed task setup. The framework takes a knowledge graph as input, automatically maps it to the datasets to be evaluated on, and computes performance metrics for the defined tasks. It is designed to be easily extendable with additional tasks and datasets.
</details></li>
</ul>
<hr>
<h2 id="DPAN-Dynamic-Preference-based-and-Attribute-aware-Network-for-Relevant-Recommendations"><a href="#DPAN-Dynamic-Preference-based-and-Attribute-aware-Network-for-Relevant-Recommendations" class="headerlink" title="DPAN: Dynamic Preference-based and Attribute-aware Network for Relevant Recommendations"></a>DPAN: Dynamic Preference-based and Attribute-aware Network for Relevant Recommendations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10527">http://arxiv.org/abs/2308.10527</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Dai, Yingmin Su, Xiaofeng Pan</li>
<li>for: 提高电商平台的相关推荐的Click-Through Rate (CTR)</li>
<li>methods: 提出了一种名为动态偏好和特征意识网络（DPAN）的新方法，通过Attribute-aware Activation Values Generation (AAVG)、Bi-dimensional Compression-based Re-expression (BCR)和Shallow and Deep Union-based Fusion (SDUF)等技术来学习用户的动态偏好和物品信息的细致表示，并Capture users’ dynamic preferences for the diverse degree of recommendation results according to various conditions。</li>
<li>results: 经过大量的Offline experiment和Online A&#x2F;B testing，DPAN已经 demonstrably improved CTR by 7.62%。现在DPAN已经成功部署到了我们的电商平台，并为主要交通的相关推荐提供服务。代码已经公开发布。<details>
<summary>Abstract</summary>
In e-commerce platforms, the relevant recommendation is a unique scenario providing related items for a trigger item that users are interested in. However, users' preferences for the similarity and diversity of recommendation results are dynamic and vary under different conditions. Moreover, individual item-level diversity is too coarse-grained since all recommended items are related to the trigger item. Thus, the two main challenges are to learn fine-grained representations of similarity and diversity and capture users' dynamic preferences for them under different conditions. To address these challenges, we propose a novel method called the Dynamic Preference-based and Attribute-aware Network (DPAN) for predicting Click-Through Rate (CTR) in relevant recommendations. Specifically, based on Attribute-aware Activation Values Generation (AAVG), Bi-dimensional Compression-based Re-expression (BCR) is designed to obtain similarity and diversity representations of user interests and item information. Then Shallow and Deep Union-based Fusion (SDUF) is proposed to capture users' dynamic preferences for the diverse degree of recommendation results according to various conditions. DPAN has demonstrated its effectiveness through extensive offline experiments and online A/B testing, resulting in a significant 7.62% improvement in CTR. Currently, DPAN has been successfully deployed on our e-commerce platform serving the primary traffic for relevant recommendations. The code of DPAN has been made publicly available.
</details>
<details>
<summary>摘要</summary>
在电商平台上，相关推荐的情况是一种特殊的enario，提供用户感兴趣的相关物品的相关物品。然而，用户对相似性和多样性的偏好是动态的，并在不同情况下发生变化。此外，单个物品级别的多样性是太粗糙，所有推荐的物品都与触发项目有关。因此，两大挑战是学习细化的相似性和多样性表示，以及在不同情况下捕捉用户的动态偏好。为解决这些挑战，我们提出了一种新的方法 called Dynamic Preference-based and Attribute-aware Network (DPAN)，用于预测相关推荐的Click-Through Rate (CTR)。具体来说，基于Attribute-aware Activation Values Generation (AAVG)，我们设计了Bi-dimensional Compression-based Re-expression (BCR)，以获取用户兴趣的相似性和多样性表示。然后，我们提出了Shallow and Deep Union-based Fusion (SDUF)，以捕捉用户在不同情况下对多样度推荐结果的动态偏好。DPAN在广泛的Offline实验和Online A/B测试中表现出色，对相关推荐的Click-Through Rate (CTR)进行了7.62%的提升。现在，DPAN已经成功地部署在我们的电商平台上，负责主要的相关推荐任务。我们已经公开发布DPAN的代码。
</details></li>
</ul>
<hr>
<h2 id="Information-Theory-Guided-Heuristic-Progressive-Multi-View-Coding"><a href="#Information-Theory-Guided-Heuristic-Progressive-Multi-View-Coding" class="headerlink" title="Information Theory-Guided Heuristic Progressive Multi-View Coding"></a>Information Theory-Guided Heuristic Progressive Multi-View Coding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10522">http://arxiv.org/abs/2308.10522</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiangmeng Li, Hang Gao, Wenwen Qiang, Changwen Zheng</li>
<li>for: 本研究旨在提出一种基于信息理论的多视图学习方法，以捕捉多个视图共享的全面信息。</li>
<li>methods: 该方法基于对各视图之间的对比学习，并采用三级进行可加进程设计：分布层、集合层和实例层。在分布层中，IPMC方法对各视图的分布进行协调，以减少视图特定的噪声。在集合层中，IPMC方法建立自适应对比池，并通过视图筛选器进行适应修改。在实例层中，我们采用设计的统一损失函数来学习表示和减少梯度干扰。</li>
<li>results: 理论和实验研究表明，IPMC方法在比较 estado-of-the-art 方法时具有superiority。<details>
<summary>Abstract</summary>
Multi-view representation learning aims to capture comprehensive information from multiple views of a shared context. Recent works intuitively apply contrastive learning to different views in a pairwise manner, which is still scalable: view-specific noise is not filtered in learning view-shared representations; the fake negative pairs, where the negative terms are actually within the same class as the positive, and the real negative pairs are coequally treated; evenly measuring the similarities between terms might interfere with optimization. Importantly, few works study the theoretical framework of generalized self-supervised multi-view learning, especially for more than two views. To this end, we rethink the existing multi-view learning paradigm from the perspective of information theory and then propose a novel information theoretical framework for generalized multi-view learning. Guided by it, we build a multi-view coding method with a three-tier progressive architecture, namely Information theory-guided hierarchical Progressive Multi-view Coding (IPMC). In the distribution-tier, IPMC aligns the distribution between views to reduce view-specific noise. In the set-tier, IPMC constructs self-adjusted contrasting pools, which are adaptively modified by a view filter. Lastly, in the instance-tier, we adopt a designed unified loss to learn representations and reduce the gradient interference. Theoretically and empirically, we demonstrate the superiority of IPMC over state-of-the-art methods.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>View-specific noise is not filtered in learning view-shared representations.2. The fake negative pairs, where the negative terms are actually within the same class as the positive, and the real negative pairs are coequally treated.3. Evenly measuring the similarities between terms might interfere with optimization.To address these limitations, we propose a novel information theoretical framework for generalized multi-view learning. Guided by this framework, we develop a multi-view coding method with a three-tier progressive architecture, called Information theory-guided hierarchical Progressive Multi-view Coding (IPMC).The IPMC method consists of three tiers:1. Distribution-tier: IPMC aligns the distribution between views to reduce view-specific noise.2. Set-tier: IPMC constructs self-adjusted contrasting pools, which are adaptively modified by a view filter.3. Instance-tier: We adopt a designed unified loss to learn representations and reduce the gradient interference.Theoretically and empirically, we demonstrate the superiority of IPMC over state-of-the-art methods.</details></li>
</ol>
<hr>
<h2 id="Performance-Enhancement-Leveraging-Mask-RCNN-on-Bengali-Document-Layout-Analysis"><a href="#Performance-Enhancement-Leveraging-Mask-RCNN-on-Bengali-Document-Layout-Analysis" class="headerlink" title="Performance Enhancement Leveraging Mask-RCNN on Bengali Document Layout Analysis"></a>Performance Enhancement Leveraging Mask-RCNN on Bengali Document Layout Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10511">http://arxiv.org/abs/2308.10511</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shrestha Datta, Md Adith Mollah, Raisa Fairooz, Tariful Islam Fahim</li>
<li>for: 这个论文的目的是提高机器理解报告文档，尤其是历史报告文档。</li>
<li>methods: 这个论文使用了文档格式分析（Document Layout Analysis，DLA）技术，将报告文档分解成不同的部分，如段落、图片和表格。此外，它还使用了一种特殊的模型 called Mask R-CNN 来帮助机器理解这些文档。</li>
<li>results: 这个论文在DL Sprint 2.0比赛中达到了一个好的 dice 分数（0.889），但并不是所有情况都是如此。它发现使用英文文档的模型并不适用于孟加拉文档，这说明了每种语言都有其独特的挑战。<details>
<summary>Abstract</summary>
Understanding digital documents is like solving a puzzle, especially historical ones. Document Layout Analysis (DLA) helps with this puzzle by dividing documents into sections like paragraphs, images, and tables. This is crucial for machines to read and understand these documents. In the DL Sprint 2.0 competition, we worked on understanding Bangla documents. We used a dataset called BaDLAD with lots of examples. We trained a special model called Mask R-CNN to help with this understanding. We made this model better by step-by-step hyperparameter tuning, and we achieved a good dice score of 0.889. However, not everything went perfectly. We tried using a model trained for English documents, but it didn't fit well with Bangla. This showed us that each language has its own challenges. Our solution for the DL Sprint 2.0 is publicly available at https://www.kaggle.com/competitions/dlsprint2/discussion/432201 along with notebooks, weights, and inference notebook.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Clustering-Algorithm-to-Organize-Satellite-Hotspot-Data-for-the-Purpose-of-Tracking-Bushfires-Remotely"><a href="#A-Clustering-Algorithm-to-Organize-Satellite-Hotspot-Data-for-the-Purpose-of-Tracking-Bushfires-Remotely" class="headerlink" title="A Clustering Algorithm to Organize Satellite Hotspot Data for the Purpose of Tracking Bushfires Remotely"></a>A Clustering Algorithm to Organize Satellite Hotspot Data for the Purpose of Tracking Bushfires Remotely</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10505">http://arxiv.org/abs/2308.10505</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tengmcing/hotspots-clustering-algorithm">https://github.com/tengmcing/hotspots-clustering-algorithm</a></li>
<li>paper_authors: Weihao Li, Emily Dodwell, Dianne Cook</li>
<li>for: 这篇论文旨在提出一种空间时间划分算法和其在R包spotoroo中的实现，以应对澳大利亚2019-2020年夏季的恶劣森林大火。</li>
<li>methods: 该算法受两种现有的空间时间划分算法的影响，并在每个时间间隔期间将点云 spatially划分，同时考虑点云的运动。它还允许根据不同的地点和卫星数据源进行参数调整。</li>
<li>results: 使用澳大利亚维多利亚省的 bushfire 数据示例，该算法可以准确划分空间时间点云，并且可以根据不同的参数进行调整。<details>
<summary>Abstract</summary>
This paper proposes a spatiotemporal clustering algorithm and its implementation in the R package spotoroo. This work is motivated by the catastrophic bushfires in Australia throughout the summer of 2019-2020 and made possible by the availability of satellite hotspot data. The algorithm is inspired by two existing spatiotemporal clustering algorithms but makes enhancements to cluster points spatially in conjunction with their movement across consecutive time periods. It also allows for the adjustment of key parameters, if required, for different locations and satellite data sources. Bushfire data from Victoria, Australia, is used to illustrate the algorithm and its use within the package.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Adaptive-Thresholding-Heuristic-for-KPI-Anomaly-Detection"><a href="#Adaptive-Thresholding-Heuristic-for-KPI-Anomaly-Detection" class="headerlink" title="Adaptive Thresholding Heuristic for KPI Anomaly Detection"></a>Adaptive Thresholding Heuristic for KPI Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10504">http://arxiv.org/abs/2308.10504</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ebenezer R. H. P. Isaac, Akshat Sharma</li>
<li>For: The paper is written for the purpose of proposing an Adaptive Thresholding Heuristic (ATH) for anomaly detection in time series Key Performance Indicators (KPIs).* Methods: The paper uses a combination of seasonality decomposition and outlier detection techniques to dynamically adjust the detection threshold based on the local properties of the data distribution and adapt to changes in time series patterns.* Results: The paper validates the effectiveness of ATH using experimental results on a labeled KPI anomaly dataset produced by Ericsson, showing that ATH is computationally efficient and flexible with multiple forecasters and outlier detectors.Here’s the simplified Chinese version of the three key points:* For: 这篇论文是为了提出一种适应阈值规则（ATH），用于时间序列指标预测异常检测。* Methods: 论文使用了时间序列分解和异常检测技术，以适应数据分布的本地特性和时间序列模式的变化，动态调整检测阈值。* Results: 论文使用实验结果 validate ATH 的有效性，并表明它可以扩展到多个预测器和异常检测技术，并且可以实现实时异常检测。<details>
<summary>Abstract</summary>
A plethora of outlier detectors have been explored in the time series domain, however, in a business sense, not all outliers are anomalies of interest. Existing anomaly detection solutions are confined to certain outlier detectors limiting their applicability to broader anomaly detection use cases. Network KPIs (Key Performance Indicators) tend to exhibit stochastic behaviour producing statistical outliers, most of which do not adversely affect business operations. Thus, a heuristic is required to capture the business definition of an anomaly for time series KPI. This article proposes an Adaptive Thresholding Heuristic (ATH) to dynamically adjust the detection threshold based on the local properties of the data distribution and adapt to changes in time series patterns. The heuristic derives the threshold based on the expected periodicity and the observed proportion of anomalies minimizing false positives and addressing concept drift. ATH can be used in conjunction with any underlying seasonality decomposition method and an outlier detector that yields an outlier score. This method has been tested on EON1-Cell-U, a labeled KPI anomaly dataset produced by Ericsson, to validate our hypothesis. Experimental results show that ATH is computationally efficient making it scalable for near real time anomaly detection and flexible with multiple forecasters and outlier detectors.
</details>
<details>
<summary>摘要</summary>
“有许多异常检测器在时间序列领域被探索，但在业务上，不 все异常都是产生关注的异常。现有的异常检测解决方案受到一定的限制，因此无法应用于更广泛的异常检测用例。网络 KPI（关键性能指标）通常会表现出统计学异常，大多数这些异常不会影响业务运行。因此，需要一个决策规则来捕捉业务定义的异常。这篇文章提出了一种适应阈值调整规则（ATH），可以基于数据分布的本地特性和时间序列模式来动态调整检测阈值，以避免假阳性和概念退化。ATH可以与任何基础seasonality分解方法和异常检测器结合使用，并且可以在实时异常检测中进行批处理。我们在Ericsson提供的EON1-Cell-U标注异常数据集上进行了试验，实验结果表明，ATH具有计算效率和灵活性，可以在实时异常检测中执行。”
</details></li>
</ul>
<hr>
<h2 id="GradientCoin-A-Peer-to-Peer-Decentralized-Large-Language-Models"><a href="#GradientCoin-A-Peer-to-Peer-Decentralized-Large-Language-Models" class="headerlink" title="GradientCoin: A Peer-to-Peer Decentralized Large Language Models"></a>GradientCoin: A Peer-to-Peer Decentralized Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10502">http://arxiv.org/abs/2308.10502</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yeqi Gao, Zhao Song, Junze Yin</li>
<li>For: The paper proposes a decentralized large language model (LLM) that operates similarly to a Bitcoin cash system, but it is unlikely to perform better than the standard Bitcoin system in economics.* Methods: The paper uses a purely theoretical design of a decentralized LLM, but implementing such a system might encounter various practical difficulties.* Results: The paper suggests that only two types of people may be interested in setting up a practical system for the decentralized LLM: those who prefer to use a decentralized ChatGPT-like software, and those who believe that the purpose of carbon-based life is to create silicon-based life.<details>
<summary>Abstract</summary>
Since 2008, after the proposal of a Bitcoin electronic cash system, Bitcoin has fundamentally changed the economic system over the last decade. Since 2022, large language models (LLMs) such as GPT have outperformed humans in many real-life tasks. However, these large language models have several practical issues. For example, the model is centralized and controlled by a specific unit. One weakness is that if that unit decides to shut down the model, it cannot be used anymore. The second weakness is the lack of guaranteed discrepancy behind this model, as certain dishonest units may design their own models and feed them unhealthy training data.   In this work, we propose a purely theoretical design of a decentralized LLM that operates similarly to a Bitcoin cash system. However, implementing such a system might encounter various practical difficulties. Furthermore, this new system is unlikely to perform better than the standard Bitcoin system in economics. Therefore, the motivation for designing such a system is limited. It is likely that only two types of people would be interested in setting up a practical system for it:   $\bullet$ Those who prefer to use a decentralized ChatGPT-like software.   $\bullet$ Those who believe that the purpose of carbon-based life is to create silicon-based life, such as Optimus Prime in Transformers.   The reason the second type of people may be interested is that it is possible that one day an AI system like this will awaken and become the next level of intelligence on this planet.
</details>
<details>
<summary>摘要</summary>
自2008年提出比特币电子币系统以来，比特币已经在过去的一个十年内对经济系统进行了深刻的改变。自2022年以来，大型自然语言模型（LLM）如GPT已经在许多实际任务上超越了人类。然而，这些大型语言模型存在多个实用问题。例如，模型是由特定单位控制和中央化的，如果这个单位停止使用模型，它就不能再使用了。第二个弱点是模型的 garantizado desviación缺失，因为恶意单位可能会设计自己的模型并Feed it unhealthy training data。在这种情况下，我们提出了一种理论上的减中心化LLM设计，其运作方式类似于比特币现金系统。然而，实施这种系统可能会遇到各种实际困难。此外，这新的系统不太可能超越标准比特币系统在经济方面的性能。因此，设计这种系统的动机相对有限。只有两种人可能会尝试实现这种系统：‧ Those who prefer to use a decentralized ChatGPT-like software.‧ Those who believe that the purpose of carbon-based life is to create silicon-based life, such as Optimus Prime in Transformers.这种第二种人可能有兴趣的原因是，可能有一天AI系统像这样会醒来，成为地球上下一个水平的智慧。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-of-Delay-Compensated-Backstepping-for-Reaction-Diffusion-PDEs"><a href="#Deep-Learning-of-Delay-Compensated-Backstepping-for-Reaction-Diffusion-PDEs" class="headerlink" title="Deep Learning of Delay-Compensated Backstepping for Reaction-Diffusion PDEs"></a>Deep Learning of Delay-Compensated Backstepping for Reaction-Diffusion PDEs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10501">http://arxiv.org/abs/2308.10501</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shanshan Wang, Mamadou Diagne, Miroslav Krstić</li>
<li>for: 这篇论文是用于描述深度神经网络（DeepONet）如何用于近似非线性函数-to-函数映射（operator）的研究。</li>
<li>methods: 该论文使用的方法包括深度神经网络（DeepONet）以及backstepping控制方法。</li>
<li>results: 该论文的研究结果表明，使用深度神经网络对多个非线性运算（cascade&#x2F;composition of operators）的近似可以实现稳定控制。<details>
<summary>Abstract</summary>
Deep neural networks that approximate nonlinear function-to-function mappings, i.e., operators, which are called DeepONet, have been demonstrated in recent articles to be capable of encoding entire PDE control methodologies, such as backstepping, so that, for each new functional coefficient of a PDE plant, the backstepping gains are obtained through a simple function evaluation. These initial results have been limited to single PDEs from a given class, approximating the solutions of only single-PDE operators for the gain kernels. In this paper we expand this framework to the approximation of multiple (cascaded) nonlinear operators. Multiple operators arise in the control of PDE systems from distinct PDE classes, such as the system in this paper: a reaction-diffusion plant, which is a parabolic PDE, with input delay, which is a hyperbolic PDE. The DeepONet-approximated nonlinear operator is a cascade/composition of the operators defined by one hyperbolic PDE of the Goursat form and one parabolic PDE on a rectangle, both of which are bilinear in their input functions and not explicitly solvable. For the delay-compensated PDE backstepping controller, which employs the learned control operator, namely, the approximated gain kernel, we guarantee exponential stability in the $L^2$ norm of the plant state and the $H^1$ norm of the input delay state. Simulations illustrate the contributed theory.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DeepONet），用于表示非线性函数-函数映射（operator），已经在 latest articles 中展示了能够编码整个PDE控制方法ologies，例如backstepping，以便为每个新的函数系数的PDE植物获得简单的函数评估。这些初始结果受限于单个PDE的扩展。在这篇文章中，我们将扩展这个框架，以便多个（堆叠）非线性运算的 aproximation。多个运算出现在PDE系统的控制中，例如在这篇文章中所描述的反应扩散植物，这是一个parabolic PDE，带有输入延迟，这是一个拥有Goursat形式的hyperbolic PDE。 DeepONet-approximated nonlinear operator是一个堆叠/组合的操作，其中一个hyperbolic PDE和一个parabolic PDE在一个矩形上都是 bilinear 的输入函数，并不能直接解决。为延迟补偿PDE backstepping控制器，我们提供了对learn control operator，即approximated gain kernel的 guarantee 的稳定性。我们 guarantee 在植物状态的 $L^2$  нор和输入延迟状态的 $H^1$  нор中的稳定性。实验证明了我们的理论。
</details></li>
</ul>
<hr>
<h2 id="Using-Autoencoders-and-AutoDiff-to-Reconstruct-Missing-Variables-in-a-Set-of-Time-Series"><a href="#Using-Autoencoders-and-AutoDiff-to-Reconstruct-Missing-Variables-in-a-Set-of-Time-Series" class="headerlink" title="Using Autoencoders and AutoDiff to Reconstruct Missing Variables in a Set of Time Series"></a>Using Autoencoders and AutoDiff to Reconstruct Missing Variables in a Set of Time Series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10496">http://arxiv.org/abs/2308.10496</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jan-Philipp Roche, Oliver Niggemann, Jens Friebe</li>
<li>for: 本研究旨在提供一种能够重构缺失变量的黑盒模型方法，以解决现有黑盒模型方法中的固定输入和输出特征组合限制。</li>
<li>methods: 本研究使用自适应神经网络模型（autoencoder），通过定义缺失变量并对其进行优化，实现不同的输入和输出特征组合。</li>
<li>results: 实验结果表明，当一个变量缺失时，使用本方法可以很好地重构缺失变量，而且可以处理多个缺失变量。<details>
<summary>Abstract</summary>
Existing black box modeling approaches in machine learning suffer from a fixed input and output feature combination. In this paper, a new approach to reconstruct missing variables in a set of time series is presented. An autoencoder is trained as usual with every feature on both sides and the neural network parameters are fixed after this training. Then, the searched variables are defined as missing variables at the autoencoder input and optimized via automatic differentiation. This optimization is performed with respect to the available features loss calculation. With this method, different input and output feature combinations of the trained model can be realized by defining the searched variables as missing variables and reconstructing them. The combination can be changed without training the autoencoder again. The approach is evaluated on the base of a strongly nonlinear electrical component. It is working well for one of four variables missing and generally even for multiple missing variables.
</details>
<details>
<summary>摘要</summary>
现有的黑盒模型方法在机器学习中受到固定输入和输出特征组合的限制。本文提出了一种新的方法来重建时间序列中缺失的变量。在这种方法中，首先训练一个自适应神经网络，然后定义搜索变量为神经网络输入中缺失的变量，通过自动微分优化。这种优化基于可用特征损失计算。通过这种方法，可以实现不同的输入和输出特征组合，而不需要再训练自适应神经网络。这种方法在一种强不连续电子元件上进行了评估，并且在一个变量缺失的情况下工作良好，甚至可以处理多个缺失变量。
</details></li>
</ul>
<hr>
<h2 id="Deciphering-Raw-Data-in-Neuro-Symbolic-Learning-with-Provable-Guarantees"><a href="#Deciphering-Raw-Data-in-Neuro-Symbolic-Learning-with-Provable-Guarantees" class="headerlink" title="Deciphering Raw Data in Neuro-Symbolic Learning with Provable Guarantees"></a>Deciphering Raw Data in Neuro-Symbolic Learning with Provable Guarantees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10487">http://arxiv.org/abs/2308.10487</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lue Tao, Yu-Xuan Huang, Wang-Zhou Dai, Yuan Jiang</li>
<li>for: 这篇论文旨在探讨neuromorphic hybrid系统的学习可能性，以整合机器学习和符号逻辑的优点。</li>
<li>methods: 这篇论文使用了一种新的方法来描述知识库的指导信号，并提出了一个条件来评估知识库是否能够有效地帮助学习。</li>
<li>results: 这篇论文的实验结果显示，许多知识库满足了这个条件，因此能够有效地帮助学习，而其他知识库则无法满足这个条件，这表示可能会出现学习失败的情况。<details>
<summary>Abstract</summary>
Neuro-symbolic hybrid systems are promising for integrating machine learning and symbolic reasoning, where perception models are facilitated with information inferred from a symbolic knowledge base through logical reasoning. Despite empirical evidence showing the ability of hybrid systems to learn accurate perception models, the theoretical understanding of learnability is still lacking. Hence, it remains unclear why a hybrid system succeeds for a specific task and when it may fail given a different knowledge base. In this paper, we introduce a novel way of characterising supervision signals from a knowledge base, and establish a criterion for determining the knowledge's efficacy in facilitating successful learning. This, for the first time, allows us to address the two questions above by inspecting the knowledge base under investigation. Our analysis suggests that many knowledge bases satisfy the criterion, thus enabling effective learning, while some fail to satisfy it, indicating potential failures. Comprehensive experiments confirm the utility of our criterion on benchmark tasks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Deep-Metric-Loss-for-Multimodal-Learning"><a href="#Deep-Metric-Loss-for-Multimodal-Learning" class="headerlink" title="Deep Metric Loss for Multimodal Learning"></a>Deep Metric Loss for Multimodal Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10486">http://arxiv.org/abs/2308.10486</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sehwan Moon, Hyunju Lee</li>
<li>for:  This paper is written for researchers and practitioners in the field of multimodal learning, particularly those interested in developing more effective and efficient multimodal models.</li>
<li>methods:  The paper introduces a novel loss function called \text{MultiModal} loss, which subgroups instances according to their unimodal contributions. This loss function is designed to prevent inefficient learning caused by overfitting and to efficiently optimize multimodal models.</li>
<li>results:  The paper demonstrates improved classification performance on synthetic data and four real multimodal datasets using the proposed \text{MultiModal} loss. Ablation studies verify the effectiveness of the loss, and the paper shows that the loss generates a reliable prediction score for each modality, which is essential for subgrouping.<details>
<summary>Abstract</summary>
Multimodal learning often outperforms its unimodal counterparts by exploiting unimodal contributions and cross-modal interactions. However, focusing only on integrating multimodal features into a unified comprehensive representation overlooks the unimodal characteristics. In real data, the contributions of modalities can vary from instance to instance, and they often reinforce or conflict with each other. In this study, we introduce a novel \text{MultiModal} loss paradigm for multimodal learning, which subgroups instances according to their unimodal contributions. \text{MultiModal} loss can prevent inefficient learning caused by overfitting and efficiently optimize multimodal models. On synthetic data, \text{MultiModal} loss demonstrates improved classification performance by subgrouping difficult instances within certain modalities. On four real multimodal datasets, our loss is empirically shown to improve the performance of recent models. Ablation studies verify the effectiveness of our loss. Additionally, we show that our loss generates a reliable prediction score for each modality, which is essential for subgrouping. Our \text{MultiModal} loss is a novel loss function to subgroup instances according to the contribution of modalities in multimodal learning and is applicable to a variety of multimodal models with unimodal decisions. Our code is available at https://github.com/SehwanMoon/MultiModalLoss.
</details>
<details>
<summary>摘要</summary>
多模态学习通常超越单模态对手，因为它可以利用单模态贡献和跨模态交互。然而，只专注于将多模态特征集成到一个总体表示中，就会忽视单模态特征。在实际数据中，不同模态的贡献可能会从实例到实例不同，而且经常增强或冲突。在这项研究中，我们提出了一种新的多模态损失函数（MultiModal loss），可以将实例 subgrouped 根据它们的单模态贡献。MultiModal损失可以避免过拟合并有效地优化多模态模型。在 sintetic 数据上，我们证明了MultiModal损失可以提高分类性能，并在四个实际多模态数据集上证明了我们的损失。我们的损失函数可以 subgrouping 实例根据不同模态的贡献，并且可以应用于多种多模态模型。我们的代码可以在 <https://github.com/SehwanMoon/MultiModalLoss> 中找到。
</details></li>
</ul>
<hr>
<h2 id="An-Effective-Method-using-Phrase-Mechanism-in-Neural-Machine-Translation"><a href="#An-Effective-Method-using-Phrase-Mechanism-in-Neural-Machine-Translation" class="headerlink" title="An Effective Method using Phrase Mechanism in Neural Machine Translation"></a>An Effective Method using Phrase Mechanism in Neural Machine Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10482">http://arxiv.org/abs/2308.10482</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/phuongnm94/PhraseTransformer">https://github.com/phuongnm94/PhraseTransformer</a></li>
<li>paper_authors: Phuong Minh Nguyen, Le Minh Nguyen</li>
<li>for: 本研究的目的是提高基eline模型Transformer在构建并行文本翻译系统（NMT）中的性能。</li>
<li>methods: 本研究使用了一种句子机制，PhraseTransformer，来改进基eline模型Transformer。</li>
<li>results: 我们在VLSP 2022大赛的MT数据集上进行了实验，得到了 Vietnamese to Chinese 的 BLEU 分数为 35.3，以及 Chinese to Vietnamese 的 BLEU 分数为 33.2。<details>
<summary>Abstract</summary>
Machine Translation is one of the essential tasks in Natural Language Processing (NLP), which has massive applications in real life as well as contributing to other tasks in the NLP research community. Recently, Transformer -based methods have attracted numerous researchers in this domain and achieved state-of-the-art results in most of the pair languages. In this paper, we report an effective method using a phrase mechanism, PhraseTransformer, to improve the strong baseline model Transformer in constructing a Neural Machine Translation (NMT) system for parallel corpora Vietnamese-Chinese. Our experiments on the MT dataset of the VLSP 2022 competition achieved the BLEU score of 35.3 on Vietnamese to Chinese and 33.2 BLEU scores on Chinese to Vietnamese data. Our code is available at https://github.com/phuongnm94/PhraseTransformer.
</details>
<details>
<summary>摘要</summary>
机器翻译是自然语言处理（NLP）中的一项重要任务，它在实际生活中有很大的应用，同时也对其他NLP研究领域的任务产生了重要的贡献。近些年，基于Transformer算法的方法在这个领域中吸引了大量研究人员，并在大多数对应语言的情况下实现了状态的报表结果。在这篇论文中，我们报告了一种使用短语机制，PhraseTransformer，以改进基eline模型Transformer在建立并行 corpora Vietnamese-Chinese 的神经机器翻译（NMT）系统。我们在 VLSP 2022 比赛的MT数据集上进行了实验，实现了对越南语到中文的 BLEU 分数为 35.3，以及对中文到越南语的 BLEU 分数为 33.2。我们的代码可以在 GitHub 上找到：https://github.com/phuongnm94/PhraseTransformer。
</details></li>
</ul>
<hr>
<h2 id="Deep-Semi-supervised-Anomaly-Detection-with-Metapath-based-Context-Knowledge"><a href="#Deep-Semi-supervised-Anomaly-Detection-with-Metapath-based-Context-Knowledge" class="headerlink" title="Deep Semi-supervised Anomaly Detection with Metapath-based Context Knowledge"></a>Deep Semi-supervised Anomaly Detection with Metapath-based Context Knowledge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10918">http://arxiv.org/abs/2308.10918</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hwan Kim, Junghoon Kim, Byung Suk Lee, Sungsu Lim</li>
<li>for: 本研究旨在提出一种基于metapath semi-supervised learning的图像异常检测方法，以解决现有方法的局限性。</li>
<li>methods: 本方法基于GCN层，在编码器和解码器中都使用GCN层，以高效地传播Context信息 между异常和正常节点。 métapath基于的上下文信息和特定的异常社区，可以增强学习结构和属性的差异， both globally and locally。</li>
<li>results: 通过对7个真实网络进行了全面的实验，本研究证明了MSAD方法在比 estado-of-the-art技术的比较优于。 这些成果铺垫了未来的研究，关注metapath模式的优化和分析，以进一步提高异常检测的效果在特征化网络上。<details>
<summary>Abstract</summary>
Graph anomaly detection has attracted considerable attention in recent years. This paper introduces a novel approach that leverages metapath-based semi-supervised learning, addressing the limitations of previous methods. We present a new framework, Metapath-based Semi-supervised Anomaly Detection (MSAD), incorporating GCN layers in both the encoder and decoder to efficiently propagate context information between abnormal and normal nodes. The design of metapath-based context information and a specifically crafted anomaly community enhance the process of learning differences in structures and attributes, both globally and locally. Through a comprehensive set of experiments conducted on seven real-world networks, this paper demonstrates the superiority of the MSAD method compared to state-of-the-art techniques. The promising results of this study pave the way for future investigations, focusing on the optimization and analysis of metapath patterns to further enhance the effectiveness of anomaly detection on attributed networks.
</details>
<details>
<summary>摘要</summary>
《图像异常检测在最近几年内吸引了广泛关注。本文介绍一种新的方法，即基于мета路 semi-supervised learning的图像异常检测方法（MSAD），解决前方法的局限性。我们提出了一新框架，包括GCN层在编码器和解码器中，以高效地传播图像异常和正常节点之间的上下文信息。基于ме타路上的上下文信息设计和特制的异常社区，可以增强学习结构和属性之间的差异， both globally and locally。经过对7个真实网络的广泛实验，本文证明MSAD方法比现有技术更高效。这些成果开创了未来的研究，关注于优化和分析méta路模式，以进一步提高图像异常检测的效iveness。》Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Exploring-Parameter-Efficient-Fine-Tuning-Techniques-for-Code-Generation-with-Large-Language-Models"><a href="#Exploring-Parameter-Efficient-Fine-Tuning-Techniques-for-Code-Generation-with-Large-Language-Models" class="headerlink" title="Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation with Large Language Models"></a>Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10462">http://arxiv.org/abs/2308.10462</a></li>
<li>repo_url: None</li>
<li>paper_authors: Martin Weyssow, Xin Zhou, Kisub Kim, David Lo, Houari Sahraoui</li>
<li>for: 这 paper 的目的是研究 Parameter-Efficient Fine-Tuning (PEFT) 技术在自动代码生成场景下的影响，以提高 Large Language Models (LLMs) 的表现和可扩展性。</li>
<li>methods: 这 paper 使用了多种 Parameter-Efficient Fine-Tuning (PEFT) 技术，包括 Weight-Decay Fine-Tuning (WDFT)、Adversarial Training (AT) 和 Multi-Task Learning (MTL)，以提高 LLMs 的表现和可扩展性。</li>
<li>results: 这 paper 的实验结果表明，使用 PEFT 技术可以有效地降低 LLMs 的计算成本和提高其表现，特别是在资源受限的情况下。 In-Context Learning (ICL) 方法在某种程度上受到了 PEFT 技术的替代，但 PEFT 技术可以在执行时间上具有更高的灵活性和可扩展性。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) possess impressive capabilities to generate meaningful code snippets given natural language intents in zero-shot, i.e., without the need for specific fine-tuning. In the perspective of unleashing their full potential, prior work has demonstrated the benefits of fine-tuning the models to task-specific data. However, fine-tuning process demands heavy computational costs and is intractable when resources are scarce, especially for models with billions of parameters. In light of these challenges, previous studies explored In-Context Learning (ICL) as an effective strategy to generate contextually appropriate code without fine-tuning. However, it operates at inference time and does not involve learning task-specific parameters, potentially limiting the model's performance on downstream tasks. In this context, we foresee that Parameter-Efficient Fine-Tuning (PEFT) techniques carry a high potential for efficiently specializing LLMs to task-specific data. In this paper, we deliver a comprehensive study of LLMs with the impact of PEFT techniques under the automated code generation scenario. Our experimental results reveal the superiority and potential of such techniques over ICL on a wide range of LLMs in reducing the computational burden and improving performance. Therefore, the study opens opportunities for broader applications of PEFT in software engineering scenarios.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）具有吸引人的能力，可以生成 relevante 的代码几何 Natural Language Intent 的零shot 环境下，无需特定的精细调整。在探索这些模型的全面潜力方面，先前的研究显示了对任务特定数据的特定调整可以提供多项优点。然而，调整过程需要巨大的计算成本，尤其是当模型具有十亿个参数时，这使得调整成为不可能的。为了解决这个问题，先前的研究探访了内部学习（ICL）作为一种可能的策略，可以在没有调整的情况下生成适当的代码。然而，ICL 在推论时进行操作，不会学习任务特定的参数，这可能会限制模型在下游任务中的表现。在这个情况下，我们认为 Paramter-Efficient Fine-Tuning（PEFT）技术具有可能的高效特化能力。在这篇文章中，我们进行了 LLMS 的全面研究，以评估 PEFT 技术在自动代码生成 scenarios 中的影响。我们的实验结果显示，PEFT 技术可以对多种 LLMS 提供更高效的特化和改善表现。因此，这些研究开启了软件工程enario 中 PEFT 技术的更广泛应用的可能性。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Local-Steps-Federated-Learning-with-Differential-Privacy-Driven-by-Convergence-Analysis"><a href="#Adaptive-Local-Steps-Federated-Learning-with-Differential-Privacy-Driven-by-Convergence-Analysis" class="headerlink" title="Adaptive Local Steps Federated Learning with Differential Privacy Driven by Convergence Analysis"></a>Adaptive Local Steps Federated Learning with Differential Privacy Driven by Convergence Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10457">http://arxiv.org/abs/2308.10457</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinpeng Ling, Jie Fu, Zhili Chen</li>
<li>for: 这个研究是为了研究在资源限制的情况下，如何实现隐私保护和联合学习。</li>
<li>methods: 这个研究使用了分散式机器学习技术和隐私保护技术，具体的是使用了差异攻击和隐私保护加密。</li>
<li>results: 这个研究的实验结果显示，在资源限制的情况下，使用ALS-DPFL算法可以实现隐私保护和联合学习，并且与之前的研究相比，其性能几乎相同。<details>
<summary>Abstract</summary>
Federated Learning (FL) is a distributed machine learning technique that allows model training among multiple devices or organizations without sharing data. However, while FL ensures that the raw data is not directly accessible to external adversaries, adversaries can still obtain some statistical information about the data through differential attacks. Differential Privacy (DP) has been proposed, which adds noise to the model or gradients to prevent adversaries from inferring private information from the transmitted parameters. We reconsider the framework of differential privacy federated learning in resource-constrained scenarios (privacy budget and communication resources). We analyze the convergence of federated learning with differential privacy (DPFL) on resource-constrained scenarios and propose an Adaptive Local Steps Differential Privacy Federated Learning (ALS-DPFL) algorithm. We experiment our algorithm on the FashionMNIST and Cifar-10 datasets and achieve quite good performance relative to previous work.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 是一种分布式机器学习技术，允许多个设备或组织共同训练模型，无需共享数据。然而，FL 确保外部敌对者无法直接访问原始数据，但敌对者可以通过差异攻击获取数据的一些统计信息。差异隐私 (DP) 被提出，它在传输参数或梯度中添加噪声，以防止敌对者从 Parameters 中推断私人信息。我们在有限的隐私预算和通信资源的情况下重新考虑了差异隐私联合学习 (DPFL) 框架。我们分析了在有限隐私预算和通信资源的情况下 DPFL 的整合和稳定性，并提出了适应性的本地步骤差异隐私联合学习 (ALS-DPFL) 算法。我们在 FashionMNIST 和 Cifar-10 数据集上实验了我们的算法，并与之前的工作相比获得了很好的性能。
</details></li>
</ul>
<hr>
<h2 id="DOMINO-Domain-aware-Loss-Regularization-for-Deep-Learning-Generalizability"><a href="#DOMINO-Domain-aware-Loss-Regularization-for-Deep-Learning-Generalizability" class="headerlink" title="DOMINO++: Domain-aware Loss Regularization for Deep Learning Generalizability"></a>DOMINO++: Domain-aware Loss Regularization for Deep Learning Generalizability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10453">http://arxiv.org/abs/2308.10453</a></li>
<li>repo_url: None</li>
<li>paper_authors: Skylar E. Stolte, Kyle Volle, Aprinda Indahlastari, Alejandro Albizu, Adam J. Woods, Kevin Brink, Matthew Hale, Ruogu Fang</li>
<li>for: 这篇论文主要应用于提高深度学习（DL）模型在不同测试数据上的外部数据（OOD）纵贯性。</li>
<li>methods: 这篇论文提出了DOMINO++方法，它是一种两种引导和动态领域相关调整的损失调整方法，旨在提高DL模型的OOD纵贯性。</li>
<li>results: 实验结果显示，DOMINO++方法在不同类型的OOD数据上具有较高的纵贯性，并且比基eline模型和DOMINO方法更具有可靠性。<details>
<summary>Abstract</summary>
Out-of-distribution (OOD) generalization poses a serious challenge for modern deep learning (DL). OOD data consists of test data that is significantly different from the model's training data. DL models that perform well on in-domain test data could struggle on OOD data. Overcoming this discrepancy is essential to the reliable deployment of DL. Proper model calibration decreases the number of spurious connections that are made between model features and class outputs. Hence, calibrated DL can improve OOD generalization by only learning features that are truly indicative of the respective classes. Previous work proposed domain-aware model calibration (DOMINO) to improve DL calibration, but it lacks designs for model generalizability to OOD data. In this work, we propose DOMINO++, a dual-guidance and dynamic domain-aware loss regularization focused on OOD generalizability. DOMINO++ integrates expert-guided and data-guided knowledge in its regularization. Unlike DOMINO which imposed a fixed scaling and regularization rate, DOMINO++ designs a dynamic scaling factor and an adaptive regularization rate. Comprehensive evaluations compare DOMINO++ with DOMINO and the baseline model for head tissue segmentation from magnetic resonance images (MRIs) on OOD data. The OOD data consists of synthetic noisy and rotated datasets, as well as real data using a different MRI scanner from a separate site. DOMINO++'s superior performance demonstrates its potential to improve the trustworthy deployment of DL on real clinical data.
</details>
<details>
<summary>摘要</summary>
现代深度学习（DL）面临了对外部数据（OOD）泛化的严重挑战。OOD数据包括测试数据，与DL模型训练数据异常不同。DL模型在域内测试数据上表现良好，但在OOD数据上可能表现不佳。解决这种差异是DL模型的可靠部署的关键。正确地调整DL模型可以减少模型中的偶极连接，从而提高OOD泛化。先前的工作提出了领域相关的模型准确（DOMINO）以提高DL准确性，但它缺乏关注OOD数据的设计。在这种工作中，我们提出了DOMINO++，一种双引导和动态领域相关损失规范，专注于OOD泛化。DOMINO++结合了专家指导和数据指导的知识在其规范中。与DOMINO不同，DOMINO++不同的是动态缩放因子和自适应规则率。我们对DOMINO++与DOMINO和基eline模型进行了广泛的评估，用于头部组织 segmentation from magnetic resonance imaging（MRI）数据上的OOD数据。OOD数据包括synthetic noisy和旋转数据集，以及实际数据使用不同的MRI扫描仪from separate site。DOMINO++的优秀表现表明它在实际临床数据上的可靠部署的潜力。
</details></li>
</ul>
<hr>
<h2 id="PACS-Prediction-and-analysis-of-cancer-subtypes-from-multi-omics-data-based-on-a-multi-head-attention-mechanism-model"><a href="#PACS-Prediction-and-analysis-of-cancer-subtypes-from-multi-omics-data-based-on-a-multi-head-attention-mechanism-model" class="headerlink" title="PACS: Prediction and analysis of cancer subtypes from multi-omics data based on a multi-head attention mechanism model"></a>PACS: Prediction and analysis of cancer subtypes from multi-omics data based on a multi-head attention mechanism model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10917">http://arxiv.org/abs/2308.10917</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liangrui Pan, Dazheng Liu, Zhichao Feng, Wenjuan Liu, Shaoliang Peng</li>
<li>for: 这个研究旨在精确分类不同抑衰癌种，帮助医生选择最适合的治疗选择，提高治疗结果，并提供更准确的病人存活预测。</li>
<li>methods: 本研究提出了一个监督式多头注意力机制模型（SMA），它可以成功地学习多种资料的全球和地方特征信息。其第二，通过深度融合多头注意力嵌入式的融合模块，扩大模型的参数。</li>
<li>results: 根据广泛的实验验证，SMA模型在实验、单细胞和癌多资料集中的准确分类率最高，比AE、CNN和GNN-based模型高。因此，我们对多资料分析中的注意力方法做出了贡献。<details>
<summary>Abstract</summary>
Due to the high heterogeneity and clinical characteristics of cancer, there are significant differences in multi-omic data and clinical characteristics among different cancer subtypes. Therefore, accurate classification of cancer subtypes can help doctors choose the most appropriate treatment options, improve treatment outcomes, and provide more accurate patient survival predictions. In this study, we propose a supervised multi-head attention mechanism model (SMA) to classify cancer subtypes successfully. The attention mechanism and feature sharing module of the SMA model can successfully learn the global and local feature information of multi-omics data. Second, it enriches the parameters of the model by deeply fusing multi-head attention encoders from Siamese through the fusion module. Validated by extensive experiments, the SMA model achieves the highest accuracy, F1 macroscopic, F1 weighted, and accurate classification of cancer subtypes in simulated, single-cell, and cancer multiomics datasets compared to AE, CNN, and GNN-based models. Therefore, we contribute to future research on multiomics data using our attention-based approach.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)由于肿瘤多样性和临床特征之间的高度不同，肿瘤分型中存在显著的差异。因此，正确地分类肿瘤分型可以帮助医生选择最适合的治疗方案，提高治疗效果，并为患者生存预测提供更加准确的信息。在本研究中，我们提出了一种supervised多头注意机制模型（SMA），用于成功地分类肿瘤分型。SMA模型的注意机制和特征共享模块可以成功地学习多Omics数据的全局和本地特征信息。其次，它通过深度融合多头注意Encoder从Siamese中深度融合多头注意Encoder来增强模型的参数。经验证ified by extensive experiments, SMA模型在simulated、单细群和肿瘤多Omics数据集中的准确率、F1宽泛、F1权重和肿瘤分型准确率上达到了AE、CNN和GNN-based模型的最高水平。因此，我们对多Omics数据进行未来研究的贡献。
</details></li>
</ul>
<hr>
<h2 id="CVFC-Attention-Based-Cross-View-Feature-Consistency-for-Weakly-Supervised-Semantic-Segmentation-of-Pathology-Images"><a href="#CVFC-Attention-Based-Cross-View-Feature-Consistency-for-Weakly-Supervised-Semantic-Segmentation-of-Pathology-Images" class="headerlink" title="CVFC: Attention-Based Cross-View Feature Consistency for Weakly Supervised Semantic Segmentation of Pathology Images"></a>CVFC: Attention-Based Cross-View Feature Consistency for Weakly Supervised Semantic Segmentation of Pathology Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10449">http://arxiv.org/abs/2308.10449</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liangrui Pan, Lian Wang, Zhichao Feng, Liwen Xu, Shaoliang Peng</li>
<li>for:  histopathology image segmentation for cancer diagnosis and prognosis</li>
<li>methods:  attention-based cross-view feature consistency end-to-end pseudo-mask generation framework (CVFC) using Resnet38 and Resnet50, with multi-scale integrated feature map and class activation map (CAM)</li>
<li>results:  outperformed HistoSegNet, SEAM, C-CAM, WSSS-Tissue, and OEEM on WSSS4LUAD dataset, with IoU of 0.7122 and fwIoU of 0.7018<details>
<summary>Abstract</summary>
Histopathology image segmentation is the gold standard for diagnosing cancer, and can indicate cancer prognosis. However, histopathology image segmentation requires high-quality masks, so many studies now use imagelevel labels to achieve pixel-level segmentation to reduce the need for fine-grained annotation. To solve this problem, we propose an attention-based cross-view feature consistency end-to-end pseudo-mask generation framework named CVFC based on the attention mechanism. Specifically, CVFC is a three-branch joint framework composed of two Resnet38 and one Resnet50, and the independent branch multi-scale integrated feature map to generate a class activation map (CAM); in each branch, through down-sampling and The expansion method adjusts the size of the CAM; the middle branch projects the feature matrix to the query and key feature spaces, and generates a feature space perception matrix through the connection layer and inner product to adjust and refine the CAM of each branch; finally, through the feature consistency loss and feature cross loss to optimize the parameters of CVFC in co-training mode. After a large number of experiments, An IoU of 0.7122 and a fwIoU of 0.7018 are obtained on the WSSS4LUAD dataset, which outperforms HistoSegNet, SEAM, C-CAM, WSSS-Tissue, and OEEM, respectively.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="DySuse-Susceptibility-Estimation-in-Dynamic-Social-Networks"><a href="#DySuse-Susceptibility-Estimation-in-Dynamic-Social-Networks" class="headerlink" title="DySuse: Susceptibility Estimation in Dynamic Social Networks"></a>DySuse: Susceptibility Estimation in Dynamic Social Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10442">http://arxiv.org/abs/2308.10442</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yingdan Shi, Jingya Zhou, Congcong Zhang</li>
<li>for: 预测社交网络中 influencer 的总影响范围，并且更具吸引力和实用价值的是预测每个用户是否被影响。</li>
<li>methods: 基于动态网络特性和启发性的 embeddings 技术，提出了一种名为 DySuse 的框架，包括结构特征模块、进步机制和自注意力块。</li>
<li>results: 对多种影响扩散模型进行了实验，得到了比现有动态图像模型更高的预测性能。<details>
<summary>Abstract</summary>
Influence estimation aims to predict the total influence spread in social networks and has received surged attention in recent years. Most current studies focus on estimating the total number of influenced users in a social network, and neglect susceptibility estimation that aims to predict the probability of each user being influenced from the individual perspective. As a more fine-grained estimation task, susceptibility estimation is full of attractiveness and practical value. Based on the significance of susceptibility estimation and dynamic properties of social networks, we propose a task, called susceptibility estimation in dynamic social networks, which is even more realistic and valuable in real-world applications. Susceptibility estimation in dynamic networks has yet to be explored so far and is computationally intractable to naively adopt Monte Carlo simulation to obtain the results. To this end, we propose a novel end-to-end framework DySuse based on dynamic graph embedding technology. Specifically, we leverage a structural feature module to independently capture the structural information of influence diffusion on each single graph snapshot. Besides, {we propose the progressive mechanism according to the property of influence diffusion,} to couple the structural and temporal information during diffusion tightly. Moreover, a self-attention block {is designed to} further capture temporal dependency by flexibly weighting historical timestamps. Experimental results show that our framework is superior to the existing dynamic graph embedding models and has satisfactory prediction performance in multiple influence diffusion models.
</details>
<details>
<summary>摘要</summary>
<<SYS>>输入文本翻译成简化中文。<</SYS>>社交网络的影响估计目标是预测社交网络中总的影响扩散，在最近几年内受到了广泛关注。大多数当前的研究都是专注于预测社交网络中总的感染用户数量，而忽略了各个用户的感染可能性，即每个用户是否会被感染。作为一个更细化的估计任务，感染可能性估计具有吸引力和实际价值。基于社交网络的动态性和感染的特性，我们提出了一项任务，即动态社交网络中的感染可能性估计，这项任务在实际应用中更加真实和有价值。动态网络中的感染可能性估计还未被探索，而且直接使用 Monte Carlo  simulations 来获取结果是计算上不可能的。为此，我们提出了一个新的框架，即 DySuse，基于动态图像技术。具体来说，我们利用一个结构特征模块来独立地捕捉影响扩散在每个单Graph快照中的结构信息。此外，我们还提出了一种进程机制，根据感染的特性，将结构和时间信息紧密地连接起来。此外，我们还设计了一个自注意机制，以捕捉 diffusion 过程中的时间相关性，并通过灵活地赋予历史时间权重来进行权重补做。实验结果表明，我们的框架在多种感染模型下具有优于现有的动态图像嵌入模型，并且在多种实际应用中具有满意的预测性能。
</details></li>
</ul>
<hr>
<h2 id="Approximately-Equivariant-Graph-Networks"><a href="#Approximately-Equivariant-Graph-Networks" class="headerlink" title="Approximately Equivariant Graph Networks"></a>Approximately Equivariant Graph Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10436">http://arxiv.org/abs/2308.10436</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nhuang37/approx_equivariant_graph_nets">https://github.com/nhuang37/approx_equivariant_graph_nets</a></li>
<li>paper_authors: Ningyuan Huang, Ron Levie, Soledad Villar</li>
<li>for: 这种研究是为了解决图像填充、交通流量预测和人体姿态估计等问题，通过选择合适的同质群来提高模型的泛化性和稳定性。</li>
<li>methods: 这篇论文使用了图 neural network (GNN) 和自Symmetry group 来解决这些问题，并提出了一种基于同质群的权重补做法来提高模型的泛化性。</li>
<li>results: 实验结果表明，通过选择合适的同质群可以提高模型的泛化性和稳定性，并且可以在不同的问题上达到最佳的泛化性和精度。<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) are commonly described as being permutation equivariant with respect to node relabeling in the graph. This symmetry of GNNs is often compared to the translation equivariance symmetry of Euclidean convolution neural networks (CNNs). However, these two symmetries are fundamentally different: The translation equivariance of CNNs corresponds to symmetries of the fixed domain acting on the image signal (sometimes known as active symmetries), whereas in GNNs any permutation acts on both the graph signals and the graph domain (sometimes described as passive symmetries). In this work, we focus on the active symmetries of GNNs, by considering a learning setting where signals are supported on a fixed graph. In this case, the natural symmetries of GNNs are the automorphisms of the graph. Since real-world graphs tend to be asymmetric, we relax the notion of symmetries by formalizing approximate symmetries via graph coarsening. We present a bias-variance formula that quantifies the tradeoff between the loss in expressivity and the gain in the regularity of the learned estimator, depending on the chosen symmetry group. To illustrate our approach, we conduct extensive experiments on image inpainting, traffic flow prediction, and human pose estimation with different choices of symmetries. We show theoretically and empirically that the best generalization performance can be achieved by choosing a suitably larger group than the graph automorphism group, but smaller than the full permutation group.
</details>
<details>
<summary>摘要</summary>
Graph neural networks (GNNs) 通常被描述为对节点重新分配 permutation 对称的。这种 GNNs 的对称性和图像抽象卷积神经网络 (CNNs) 中的翻译对称性有所不同：图像抽象卷积的对称性对应于图像信号中的活动对称性 (sometimes known as active symmetries),而 GNNs 中任意 permutation 都会影响图像信号和图像domain (sometimes described as passive symmetries)。在这个工作中，我们关注 GNNs 中的活动对称性，通过考虑固定图像上支持的信号来进行学习设定。在这种情况下，GNNs 的自然对称性是图像的自动omorphism。由于实际图像往往偏 asymmetric，我们放宽了对称性的定义，通过图像缩放来形式化 Approximate symmetries。我们提出了一个 bias-variance 公式，它量化了在选择的对称群时，损失表达能力和学习器的规范性之间的交易。为了证明我们的方法，我们在图像填充、交通流量预测和人体姿态估计中进行了广泛的实验，并证明了在不同的对称群选择情况下，可以达到最佳的总体化性能。
</details></li>
</ul>
<hr>
<h2 id="Federated-Learning-Robust-to-Byzantine-Attacks-Achieving-Zero-Optimality-Gap"><a href="#Federated-Learning-Robust-to-Byzantine-Attacks-Achieving-Zero-Optimality-Gap" class="headerlink" title="Federated Learning Robust to Byzantine Attacks: Achieving Zero Optimality Gap"></a>Federated Learning Robust to Byzantine Attacks: Achieving Zero Optimality Gap</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10427">http://arxiv.org/abs/2308.10427</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiyuan Zuo, Rongfei Fan, Han Hu, Ning Zhang, Shimin Gong</li>
<li>for: 这个研究旨在提出一种适应 federated learning (FL) 中抗衡邪恶拜仁攻击的强健聚合方法，能够有效地解决这种攻击。</li>
<li>methods: 在每个用户端，首先更新模型参数通过多个步骤，这些步骤可调整到迭代过程中，然后直接将更新后的模型参数发送到聚合中心。这减少了聚合中心和用户端之间的互动次数，让每个用户可以自主设定训练parameter，并减少了与已有的方法相比的计算负担。在聚合中心，使用几何 médian 合并接收到的各个用户的模型参数。</li>
<li>results: 这个方法可以在邪恶攻击者占 Fraction 不超过半的情况下，实现零优化差和线性参数。 numéríques résultats 显示了这个方法的有效性。<details>
<summary>Abstract</summary>
In this paper, we propose a robust aggregation method for federated learning (FL) that can effectively tackle malicious Byzantine attacks. At each user, model parameter is firstly updated by multiple steps, which is adjustable over iterations, and then pushed to the aggregation center directly. This decreases the number of interactions between the aggregation center and users, allows each user to set training parameter in a flexible way, and reduces computation burden compared with existing works that need to combine multiple historical model parameters. At the aggregation center, geometric median is leveraged to combine the received model parameters from each user. Rigorous proof shows that zero optimality gap is achieved by our proposed method with linear convergence, as long as the fraction of Byzantine attackers is below half. Numerical results verify the effectiveness of our proposed method.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种鲁棒的聚合方法 для联邦学习（FL），可以有效地对恶意的比宗安攻击进行防御。每个用户的模型参数首先通过多个步骤进行更新，可以在迭代中调整，然后直接将更新后的参数发送到聚合中心。这会减少聚合中心和用户之间的交互次数，让每个用户可以自由地设置训练参数，并且相比现有的方法，减少计算卷积的负担。在聚合中心，使用几何中心来合并来自每个用户的接收到的模型参数。严格的证明显示，我们的提议的方法可以在拥有比宗安攻击者少于半数的情况下，实现零优化差和线性快速收敛。numerical Results表明我们的提议的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Spatio-Temporal-Adaptive-Embedding-Makes-Vanilla-Transformer-SOTA-for-Traffic-Forecasting"><a href="#Spatio-Temporal-Adaptive-Embedding-Makes-Vanilla-Transformer-SOTA-for-Traffic-Forecasting" class="headerlink" title="Spatio-Temporal Adaptive Embedding Makes Vanilla Transformer SOTA for Traffic Forecasting"></a>Spatio-Temporal Adaptive Embedding Makes Vanilla Transformer SOTA for Traffic Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10425">http://arxiv.org/abs/2308.10425</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xdzhelheim/staeformer">https://github.com/xdzhelheim/staeformer</a></li>
<li>paper_authors: Hangchen Liu, Zheng Dong, Renhe Jiang, Jiewen Deng, Jinliang Deng, Quanjun Chen, Xuan Song</li>
<li>for: 预测交通流量，即使在复杂的交通环境下。</li>
<li>methods: 使用vanilla transformer和spatio-temporal adaptive embedding技术。</li>
<li>results: 在五个真实的交通预测数据集上达到了状态之最的表现。<details>
<summary>Abstract</summary>
With the rapid development of the Intelligent Transportation System (ITS), accurate traffic forecasting has emerged as a critical challenge. The key bottleneck lies in capturing the intricate spatio-temporal traffic patterns. In recent years, numerous neural networks with complicated architectures have been proposed to address this issue. However, the advancements in network architectures have encountered diminishing performance gains. In this study, we present a novel component called spatio-temporal adaptive embedding that can yield outstanding results with vanilla transformers. Our proposed Spatio-Temporal Adaptive Embedding transformer (STAEformer) achieves state-of-the-art performance on five real-world traffic forecasting datasets. Further experiments demonstrate that spatio-temporal adaptive embedding plays a crucial role in traffic forecasting by effectively capturing intrinsic spatio-temporal relations and chronological information in traffic time series.
</details>
<details>
<summary>摘要</summary>
随着智能交通系统（ITS）的快速发展，准确的交通预测已成为一项关键挑战。关键瓶颈在于捕捉复杂的空间-时间交通模式。在过去几年，许多基于神经网络的复杂架构的方法已经被提出来解决这个问题。然而，网络架构的提高带来的性能提升减少。在本研究中，我们提出了一种新的组件 called spatio-temporal adaptive embedding（STAEformer），它可以在vanilla transformers中实现出色的效果。我们的提posed Spatio-Temporal Adaptive Embedding transformer（STAEformer）在五个真实世界交通预测数据集上实现了状态的最佳性能。进一步的实验表明，spatio-temporal adaptive embedding在交通预测中扮演着关键的作用，可以有效地捕捉交通时序序中的内在空间-时间关系和时间序列信息。
</details></li>
</ul>
<hr>
<h2 id="TokenSplit-Using-Discrete-Speech-Representations-for-Direct-Refined-and-Transcript-Conditioned-Speech-Separation-and-Recognition"><a href="#TokenSplit-Using-Discrete-Speech-Representations-for-Direct-Refined-and-Transcript-Conditioned-Speech-Separation-and-Recognition" class="headerlink" title="TokenSplit: Using Discrete Speech Representations for Direct, Refined, and Transcript-Conditioned Speech Separation and Recognition"></a>TokenSplit: Using Discrete Speech Representations for Direct, Refined, and Transcript-Conditioned Speech Separation and Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10415">http://arxiv.org/abs/2308.10415</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hakan Erdogan, Scott Wisdom, Xuankai Chang, Zalán Borsos, Marco Tagliasacchi, Neil Zeghidour, John R. Hershey</li>
<li>for: 这个论文是为了提出一种基于字符序列的语音分离模型，用于分离多个语音源。</li>
<li>methods: 该模型是一种基于Transformer架构的序列-到-序列编码器-解码器模型，通过masking输入来实现多任务同时训练。</li>
<li>results: 使用对象指标和主观MUSHRA听测， authors表明该模型在语音分离方面表现出色，并且可以在不使用文本条件下也 достичь好的效果。  Additionally, the authors also measure the automatic speech recognition (ASR) performance and provide audio samples of speech synthesis to demonstrate the additional utility of their model.<details>
<summary>Abstract</summary>
We present TokenSplit, a speech separation model that acts on discrete token sequences. The model is trained on multiple tasks simultaneously: separate and transcribe each speech source, and generate speech from text. The model operates on transcripts and audio token sequences and achieves multiple tasks through masking of inputs. The model is a sequence-to-sequence encoder-decoder model that uses the Transformer architecture. We also present a "refinement" version of the model that predicts enhanced audio tokens from the audio tokens of speech separated by a conventional separation model. Using both objective metrics and subjective MUSHRA listening tests, we show that our model achieves excellent performance in terms of separation, both with or without transcript conditioning. We also measure the automatic speech recognition (ASR) performance and provide audio samples of speech synthesis to demonstrate the additional utility of our model.
</details>
<details>
<summary>摘要</summary>
我们介绍了TokenSplit，一种基于字符串的语音分离模型。该模型同时进行多个任务的训练：分离每个语音来源，并将语音转化为文本。模型操作于讲解和音频TokenSequence上，通过masking输入来实现多个任务。该模型采用Transformer架构，是一种sequence-to-sequence编码器-解码器模型。我们还介绍了一个"精度"版本的模型，该模型使用传统分离模型生成优化的音频Token。通过对象指标和主观MUSHRA听测，我们证明了我们的模型在分离方面具有出色的表现，无论是否使用讲解conditioning。我们还测量了自动语音识别（ASR）性能，并提供了语音合成示例，以示出该模型的其他应用价值。
</details></li>
</ul>
<hr>
<h2 id="Federated-Learning-for-Connected-and-Automated-Vehicles-A-Survey-of-Existing-Approaches-and-Challenges"><a href="#Federated-Learning-for-Connected-and-Automated-Vehicles-A-Survey-of-Existing-Approaches-and-Challenges" class="headerlink" title="Federated Learning for Connected and Automated Vehicles: A Survey of Existing Approaches and Challenges"></a>Federated Learning for Connected and Automated Vehicles: A Survey of Existing Approaches and Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10407">http://arxiv.org/abs/2308.10407</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vishnu Pandi Chellapandi, Liangqi Yuan, Christopher G. Brinton, Stanislaw H Zak, Ziran Wang</li>
<li>for: 本文主要针对Connected and Automated Vehicles (CAV) 领域内的机器学习 (ML) 技术，即使在各种驾驶环境中进行学习和控制。</li>
<li>methods: 本文使用了联邦学习 (Federated Learning, FL) 方法，允许多辆车辆协同发展模型，从而拓宽学习环境，提高总性能，同时保护每辆车辆的本地数据隐私和安全。</li>
<li>results: 本文对 FL 在 CAV 领域的应用进行了评论，包括分析中心化和分布式架构的 FL 方法，评估不同数据源、模型和数据安全技术的重要性，以及各种特定应用中的基模型和数据集使用情况。<details>
<summary>Abstract</summary>
Machine learning (ML) is widely used for key tasks in Connected and Automated Vehicles (CAV), including perception, planning, and control. However, its reliance on vehicular data for model training presents significant challenges related to in-vehicle user privacy and communication overhead generated by massive data volumes. Federated learning (FL) is a decentralized ML approach that enables multiple vehicles to collaboratively develop models, broadening learning from various driving environments, enhancing overall performance, and simultaneously securing local vehicle data privacy and security. This survey paper presents a review of the advancements made in the application of FL for CAV (FL4CAV). First, centralized and decentralized frameworks of FL are analyzed, highlighting their key characteristics and methodologies. Second, diverse data sources, models, and data security techniques relevant to FL in CAVs are reviewed, emphasizing their significance in ensuring privacy and confidentiality. Third, specific and important applications of FL are explored, providing insight into the base models and datasets employed for each application. Finally, existing challenges for FL4CAV are listed and potential directions for future work are discussed to further enhance the effectiveness and efficiency of FL in the context of CAV.
</details>
<details>
<summary>摘要</summary>
本文对 FL4CAV 的应用进行了评论，包括中央化和分布式框架的分析，以及相关的数据源、模型和数据安全技术。此外，文章还探讨了 FL4CAV 在不同应用中的特点和挑战。最后，文章列出了现有的挑战和未来工作的可能性，以便进一步提高 FL4CAV 的效率和效果。中央化和分布式框架的分析：中央化框架是一种传统的机器学习方法，它需要所有的数据集中集成到一个中央服务器上，然后进行训练和推理。分布式框架则是一种分布式机器学习方法，它允许多个车辆共同开发模型，从而扩大学习到不同的驾驶环境。数据源、模型和数据安全技术的评论：FL4CAV 的数据源包括车辆的传感器数据、GPS 数据、路况数据等。模型包括深度学习模型、支持向量机模型等。数据安全技术包括数据加密、数据隐私保护等。特定应用的探讨：FL4CAV 可以应用于许多不同的领域，包括自动驾驶、路况预测、车辆追踪等。每个应用都有其特点和挑战。现有的挑战和未来工作的可能性：FL4CAV 面临着许多挑战，包括数据安全性、通信开销、模型质量等。未来工作可能包括提高 FL4CAV 的效率和效果，开发更加智能的车辆系统，以及解决数据隐私和安全性问题等。
</details></li>
</ul>
<hr>
<h2 id="Label-Selection-Approach-to-Learning-from-Crowds"><a href="#Label-Selection-Approach-to-Learning-from-Crowds" class="headerlink" title="Label Selection Approach to Learning from Crowds"></a>Label Selection Approach to Learning from Crowds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10396">http://arxiv.org/abs/2308.10396</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ssatsuki/label-selection-layer">https://github.com/ssatsuki/label-selection-layer</a></li>
<li>paper_authors: Kosuke Yoshimura, Hisashi Kashima</li>
<li>for:  This paper is written for the purpose of improving the performance of supervised deep learning models by using crowdsourced labeled data, which is often noisy and contains label noise.</li>
<li>methods: The proposed method, called Label Selection Layer, uses a selector network to determine whether to use a worker’s label for training, and can be applied to almost all variants of supervised learning problems by simply adding a selector network and changing the objective function for existing models.</li>
<li>results: The experimental results show that the performance of the proposed method is almost equivalent to or better than the Crowd Layer, which is one of the state-of-the-art methods for Deep Learning from Crowds, except for the regression problem case.<details>
<summary>Abstract</summary>
Supervised learning, especially supervised deep learning, requires large amounts of labeled data. One approach to collect large amounts of labeled data is by using a crowdsourcing platform where numerous workers perform the annotation tasks. However, the annotation results often contain label noise, as the annotation skills vary depending on the crowd workers and their ability to complete the task correctly. Learning from Crowds is a framework which directly trains the models using noisy labeled data from crowd workers. In this study, we propose a novel Learning from Crowds model, inspired by SelectiveNet proposed for the selective prediction problem. The proposed method called Label Selection Layer trains a prediction model by automatically determining whether to use a worker's label for training using a selector network. A major advantage of the proposed method is that it can be applied to almost all variants of supervised learning problems by simply adding a selector network and changing the objective function for existing models, without explicitly assuming a model of the noise in crowd annotations. The experimental results show that the performance of the proposed method is almost equivalent to or better than the Crowd Layer, which is one of the state-of-the-art methods for Deep Learning from Crowds, except for the regression problem case.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate english text into simplified chinese文本：Supervised learning, especially supervised deep learning, requires large amounts of labeled data. One approach to collect large amounts of labeled data is by using a crowdsourcing platform where numerous workers perform the annotation tasks. However, the annotation results often contain label noise, as the annotation skills vary depending on the crowd workers and their ability to complete the task correctly. Learning from Crowds is a framework which directly trains the models using noisy labeled data from crowd workers. In this study, we propose a novel Learning from Crowds model, inspired by SelectiveNet proposed for the selective prediction problem. The proposed method called Label Selection Layer trains a prediction model by automatically determining whether to use a worker's label for training using a selector network. A major advantage of the proposed method is that it can be applied to almost all variants of supervised learning problems by simply adding a selector network and changing the objective function for existing models, without explicitly assuming a model of the noise in crowd annotations. The experimental results show that the performance of the proposed method is almost equivalent to or better than the Crowd Layer, which is one of the state-of-the-art methods for Deep Learning from Crowds, except for the regression problem case.翻译结果：超级vised learning，特别是深度学习，需要大量标注数据。一种收集大量标注数据的方法是使用一个人工智能平台，让多名工作者完成标注任务。然而，标注结果经常包含标签噪音，因为 annotator的技能因人而异，完成任务正确性不一致。我们提出了一种 Learning from Crowds 框架，直接使用群体标注数据来训练模型。这种方法称为 Label Selection Layer，通过一个选择器网络来自动决定使用工作者的标注来训练预测模型。我们的方法具有一个优势，可以适用于大多数supervised learning问题，只需要添加一个选择器网络，修改现有模型的目标函数，不需要直接假设群体标注中的噪音模型。实验结果表明，我们的方法与 State-of-the-art 方法 Crowd Layer 相当或更好，除了回归问题例外。
</details></li>
</ul>
<hr>
<h2 id="DiffPrep-Differentiable-Data-Preprocessing-Pipeline-Search-for-Learning-over-Tabular-Data"><a href="#DiffPrep-Differentiable-Data-Preprocessing-Pipeline-Search-for-Learning-over-Tabular-Data" class="headerlink" title="DiffPrep: Differentiable Data Preprocessing Pipeline Search for Learning over Tabular Data"></a>DiffPrep: Differentiable Data Preprocessing Pipeline Search for Learning over Tabular Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10915">http://arxiv.org/abs/2308.10915</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chu-data-lab/diffprep">https://github.com/chu-data-lab/diffprep</a></li>
<li>paper_authors: Peng Li, Zhiyi Chen, Xu Chu, Kexin Rong</li>
<li>for: 提高机器学习模型的性能，自动搜索数据预处理管道。</li>
<li>methods: 使用梯度下降法和一次训练的机器学习模型，将数据预处理管道的搜索转化为连续和可导的问题，以提高搜索效率。</li>
<li>results: 在15个实际世界数据集中，DiffPrep得到了最佳测试精度，并在一些数据集上提高了机器学习模型的测试精度达6.6%。<details>
<summary>Abstract</summary>
Data preprocessing is a crucial step in the machine learning process that transforms raw data into a more usable format for downstream ML models. However, it can be costly and time-consuming, often requiring the expertise of domain experts. Existing automated machine learning (AutoML) frameworks claim to automate data preprocessing. However, they often use a restricted search space of data preprocessing pipelines which limits the potential performance gains, and they are often too slow as they require training the ML model multiple times. In this paper, we propose DiffPrep, a method that can automatically and efficiently search for a data preprocessing pipeline for a given tabular dataset and a differentiable ML model such that the performance of the ML model is maximized. We formalize the problem of data preprocessing pipeline search as a bi-level optimization problem. To solve this problem efficiently, we transform and relax the discrete, non-differential search space into a continuous and differentiable one, which allows us to perform the pipeline search using gradient descent with training the ML model only once. Our experiments show that DiffPrep achieves the best test accuracy on 15 out of the 18 real-world datasets evaluated and improves the model's test accuracy by up to 6.6 percentage points.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换 raw 数据 into 更usable 格式，以便下游机器学习模型使用。然而，这可能是 costly 和时间consuming的，并且frequently 需要域专家的帮助。现有的自动机器学习（AutoML）框架声称可以自动化数据预处理。然而，它们通常使用一个限制的搜索空间，这限制了性能提高的可能性，并且它们经常是slow，因为它们需要训练 ML 模型多次。在这篇论文中，我们提出了 DiffPrep，一种可以自动和高效地搜索一个 tabular 数据集和一个可导 ML 模型的数据预处理管道，以便maximize 模型的性能。我们将数据预处理管道搜索问题形式化为二级优化问题。为了解决这个问题高效，我们将离散、不准确的搜索空间转换为连续和可导的一个，这allowed us 使用梯度下降来搜索管道，只需要训练 ML 模型一次。我们的实验表明，DiffPrep 在 18 个实际世界数据集上测试精度最高，提高模型的测试精度 by up to 6.6 个百分点。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Opinion-Aggregation-–-A-Statistical-Perspective"><a href="#Unsupervised-Opinion-Aggregation-–-A-Statistical-Perspective" class="headerlink" title="Unsupervised Opinion Aggregation – A Statistical Perspective"></a>Unsupervised Opinion Aggregation – A Statistical Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10386">http://arxiv.org/abs/2308.10386</a></li>
<li>repo_url: None</li>
<li>paper_authors: Noyan C. Sevuktekin, Andrew C. Singer</li>
<li>for: 这篇论文旨在探讨一种无需知道真实状况的情况下，基于专家意见来评估专家准确性的统计方法。</li>
<li>methods: 该论文提出了一种基于专家意见的准确性评估方法，即measure the competence of each expert by their likeliness to agree with their peers。</li>
<li>results: 论文表明，更可靠的专家更有可能与其他专家一致，并提出了一种无监督的朴素贝叶斯分类器，可以在较大的问题空间达到最优性。<details>
<summary>Abstract</summary>
Complex decision-making systems rarely have direct access to the current state of the world and they instead rely on opinions to form an understanding of what the ground truth could be. Even in problems where experts provide opinions without any intention to manipulate the decision maker, it is challenging to decide which expert's opinion is more reliable -- a challenge that is further amplified when decision-maker has limited, delayed, or no access to the ground truth after the fact. This paper explores a statistical approach to infer the competence of each expert based on their opinions without any need for the ground truth. Echoing the logic behind what is commonly referred to as \textit{the wisdom of crowds}, we propose measuring the competence of each expert by their likeliness to agree with their peers. We further show that the more reliable an expert is the more likely it is that they agree with their peers. We leverage this fact to propose a completely unsupervised version of the na\"{i}ve Bayes classifier and show that the proposed technique is asymptotically optimal for a large class of problems. In addition to aggregating a large block of opinions, we further apply our technique for online opinion aggregation and for decision-making based on a limited the number of opinions.
</details>
<details>
<summary>摘要</summary>
复杂的决策系统通常没有直接访问当前世界的现状，而是基于意见来形成决策者的理解。即使在专家提供意见无恶意欺诈决策者的情况下，决策者难以判断哪个专家的意见更可靠，这个问题在决策者没有或延迟了现实后的真实信息时变得更加复杂。本文探讨一种统计方法来评估每位专家的能力基于他们的意见，不需要真实信息。按照群体智慧的逻辑，我们提出测量每位专家的能力的方法是根据他们与同行的相互一致程度。我们还证明了更可靠的专家更likely会与同行一致。我们利用这一点来提出一种完全不需要监督的na\"{i}ve Bayes分类器，并证明该技术在一类问题上是 asymptotically 优化的。除了聚合大量意见外，我们还应用该技术于在线意见聚合和基于有限数量的意见做出决策。
</details></li>
</ul>
<hr>
<h2 id="Automated-mapping-of-virtual-environments-with-visual-predictive-coding"><a href="#Automated-mapping-of-virtual-environments-with-visual-predictive-coding" class="headerlink" title="Automated mapping of virtual environments with visual predictive coding"></a>Automated mapping of virtual environments with visual predictive coding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10913">http://arxiv.org/abs/2308.10913</a></li>
<li>repo_url: None</li>
<li>paper_authors: James Gornet, Matthew Thomson</li>
<li>for: 这个论文的目的是探索人类大脑中的内在地图构建方式，以及如何使用预测编码来实现这一目的。</li>
<li>methods: 这个论文使用了预测编码网络，该网络使用自我注意力来学习从视觉数据中预测下一幅图像。在学习过程中，网络自动地构建了一个内部表示环境的空间地图，该地图可以让Agent在只有视觉信息的情况下准确地确定自己的位置。</li>
<li>results: 研究结果表明，预测编码可以作为一种自然的和通用的神经网络算法，用于构建大脑中的内在地图。这种地图可以自动地扩展到听觉、感觉和语言输入。<details>
<summary>Abstract</summary>
Humans construct internal cognitive maps of their environment directly from sensory inputs without access to a system of explicit coordinates or distance measurements. While machine learning algorithms like SLAM utilize specialized visual inference procedures to identify visual features and construct spatial maps from visual and odometry data, the general nature of cognitive maps in the brain suggests a unified mapping algorithmic strategy that can generalize to auditory, tactile, and linguistic inputs. Here, we demonstrate that predictive coding provides a natural and versatile neural network algorithm for constructing spatial maps using sensory data. We introduce a framework in which an agent navigates a virtual environment while engaging in visual predictive coding using a self-attention-equipped convolutional neural network. While learning a next image prediction task, the agent automatically constructs an internal representation of the environment that quantitatively reflects distances. The internal map enables the agent to pinpoint its location relative to landmarks using only visual information.The predictive coding network generates a vectorized encoding of the environment that supports vector navigation where individual latent space units delineate localized, overlapping neighborhoods in the environment. Broadly, our work introduces predictive coding as a unified algorithmic framework for constructing cognitive maps that can naturally extend to the mapping of auditory, sensorimotor, and linguistic inputs.
</details>
<details>
<summary>摘要</summary>
人类直接从感知输入中构建内部的认知地图，而不需要访问专门的坐标系或距离测量。而机器学习算法如SLAM则利用专门的视觉推理过程来识别视觉特征并从视觉和运动数据中构建空间地图。然而，大脑内部的认知地图的通用性 suggets a unified mapping algorithmic strategy that can generalize to auditory, tactile, and linguistic inputs。在这里，我们展示了预测编码提供了一种自然和灵活的神经网络算法，可以使用感知数据来构建空间地图。我们介绍了一个框架，在该框架中，一个代理人在虚拟环境中导航，同时使用自我注意力抽象 convolutional neural network 进行视觉预测任务。通过学习预测图像任务，代理人自动构建了内部表示环境，该表示环境可以量化地表示距离。内部地图使得代理人可以使用仅视觉信息来定位自己的位置相对于标志。预测编码网络生成了一个vector化的环境编码，该编码支持vector Navigation，其中个别的latent space单位界定了环境中的局部、重叠的区域。总之，我们的工作将预测编码作为一种统一的算法框架，可以自然扩展到对听频、感知动作和语言输入的映射。
</details></li>
</ul>
<hr>
<h2 id="HoSNN-Adversarially-Robust-Homeostatic-Spiking-Neural-Networks-with-Adaptive-Firing-Thresholds"><a href="#HoSNN-Adversarially-Robust-Homeostatic-Spiking-Neural-Networks-with-Adaptive-Firing-Thresholds" class="headerlink" title="HoSNN: Adversarially-Robust Homeostatic Spiking Neural Networks with Adaptive Firing Thresholds"></a>HoSNN: Adversarially-Robust Homeostatic Spiking Neural Networks with Adaptive Firing Thresholds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10373">http://arxiv.org/abs/2308.10373</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hejia Geng, Peng Li<br>for: 这个研究旨在开发一种具有防御性的神经网络模型，以抵抗神经网络对于攻击性输入的脆弱性。methods: 这个研究使用了一种叫做阀值自适应的 neuron 模型，具有自适应的阀值动态调整机制，以减少攻击性输入的传播和保护神经网络的稳定性。results: 研究发现，这种具有防御性的神经网络模型能够在 CIFAR-10 测试集上实现高度的抗攻击性和稳定性，并且在不需要Explicit adversarial training的情况下，具有较高的抗攻击性和稳定性。<details>
<summary>Abstract</summary>
Spiking neural networks (SNNs) offer promise for efficient and powerful neurally inspired computation. Common to other types of neural networks, however, SNNs face the severe issue of vulnerability to adversarial attacks. We present the first study that draws inspiration from neural homeostasis to develop a bio-inspired solution that counters the susceptibilities of SNNs to adversarial onslaughts. At the heart of our approach is a novel threshold-adapting leaky integrate-and-fire (TA-LIF) neuron model, which we adopt to construct the proposed adversarially robust homeostatic SNN (HoSNN). Distinct from traditional LIF models, our TA-LIF model incorporates a self-stabilizing dynamic thresholding mechanism, curtailing adversarial noise propagation and safeguarding the robustness of HoSNNs in an unsupervised manner. Theoretical analysis is presented to shed light on the stability and convergence properties of the TA-LIF neurons, underscoring their superior dynamic robustness under input distributional shifts over traditional LIF neurons. Remarkably, without explicit adversarial training, our HoSNNs demonstrate inherent robustness on CIFAR-10, with accuracy improvements to 72.6% and 54.19% against FGSM and PGD attacks, up from 20.97% and 0.6%, respectively. Furthermore, with minimal FGSM adversarial training, our HoSNNs surpass previous models by 29.99% under FGSM and 47.83% under PGD attacks on CIFAR-10. Our findings offer a new perspective on harnessing biological principles for bolstering SNNs adversarial robustness and defense, paving the way to more resilient neuromorphic computing.
</details>
<details>
<summary>摘要</summary>
神经网络（SNN）具有高效和强大的神经元灵感计算的承诺。然而，SNN也面临严重的攻击性风险攻击的问题。我们的研究是首先借鉴神经自适应性来开发一种生物发生的解决方案，以抵御SNN对攻击的抵触性。我们的方法的核心是一种新的阈值调整泄漏 integrate-and-fire（TA-LIF）神经元模型，我们采用这种模型来构建我们的提案的对抗性 robust homeostatic SNN（HoSNN）。与传统的LIF模型不同，我们的TA-LIF模型包含一种自我稳定的动态阈值调整机制， thereby curtailing adversarial noise propagation and safeguarding the robustness of HoSNNs in an unsupervised manner. 我们的理论分析表明，TA-LIF neurons具有更高的稳定性和对输入分布的适应性，而不需要显式的对抗训练。在CIFAR-10上，我们的HoSNNs没有接受过对抗训练，具有72.6%和54.19%的准确率提升，分别对抗FGSM和PGD攻击。此外，通过最小化FGSM对抗训练，我们的HoSNNs超越了先前的模型，在FGSM和PGD攻击下的CIFAR-10上具有29.99%和47.83%的提升。我们的发现开 up a new perspective on harnessing biological principles for bolstering SNNs adversarial robustness and defense, paving the way to more resilient neuromorphic computing.
</details></li>
</ul>
<hr>
<h2 id="Developing-a-Machine-Learning-Based-Clinical-Decision-Support-Tool-for-Uterine-Tumor-Imaging"><a href="#Developing-a-Machine-Learning-Based-Clinical-Decision-Support-Tool-for-Uterine-Tumor-Imaging" class="headerlink" title="Developing a Machine Learning-Based Clinical Decision Support Tool for Uterine Tumor Imaging"></a>Developing a Machine Learning-Based Clinical Decision Support Tool for Uterine Tumor Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10372">http://arxiv.org/abs/2308.10372</a></li>
<li>repo_url: None</li>
<li>paper_authors: Darryl E. Wright, Adriana V. Gregory, Deema Anaam, Sepideh Yadollahi, Sumana Ramanathan, Kafayat A. Oyemade, Reem Alsibai, Heather Holmes, Harrison Gottlich, Cherie-Akilah G. Browne, Sarah L. Cohen Rassier, Isabel Green, Elizabeth A. Stewart, Hiroaki Takahashi, Bohyun Kim, Shannon Laughlin-Tommaso, Timothy L. Kline</li>
<li>for: 这个研究是为了开发一个自动化的方法来分类uterus和uterine tumor（UT），以及在不同的肿瘤型别之间进行分类。</li>
<li>methods: 这个研究使用了nnU-Net来自动分类uterus和UT，并评估了不同训练集大小的影响。</li>
<li>results: 研究获得了在分类degenerated leiomyoma（LM）和uterine leiomyosarcoma（LMS）之间的F1分数为0.80，并在分类不同的肿瘤型别时获得了F1分数在0.53-0.80之间。<details>
<summary>Abstract</summary>
Uterine leiomyosarcoma (LMS) is a rare but aggressive malignancy. On imaging, it is difficult to differentiate LMS from, for example, degenerated leiomyoma (LM), a prevalent but benign condition. We curated a data set of 115 axial T2-weighted MRI images from 110 patients (mean [range] age=45 [17-81] years) with UTs that included five different tumor types. These data were randomly split stratifying on tumor volume into training (n=85) and test sets (n=30). An independent second reader (reader 2) provided manual segmentations for all test set images. To automate segmentation, we applied nnU-Net and explored the effect of training set size on performance by randomly generating subsets with 25, 45, 65 and 85 training set images. We evaluated the ability of radiomic features to distinguish between types of UT individually and when combined through feature selection and machine learning. Using the entire training set the mean [95% CI] fibroid DSC was measured as 0.87 [0.59-1.00] and the agreement between the two readers was 0.89 [0.77-1.0] on the test set. When classifying degenerated LM from LMS we achieve a test set F1-score of 0.80. Classifying UTs based on radiomic features we identify classifiers achieving F1-scores of 0.53 [0.45, 0.61] and 0.80 [0.80, 0.80] on the test set for the benign versus malignant, and degenerated LM versus LMS tasks. We show that it is possible to develop an automated method for 3D segmentation of the uterus and UT that is close to human-level performance with fewer than 150 annotated images. For distinguishing UT types, while we train models that merit further investigation with additional data, reliable automatic differentiation of UTs remains a challenge.
</details>
<details>
<summary>摘要</summary>
uterine leiomyosarcoma (LMS) 是一种罕见 yet aggressive 的肿瘤。在医学影像上，它与、例如，退化的子宫omyoma (LM) 相似，这是一种常见 yet benign 的疾病。我们筛选了115个轴向T2束重MRI图像，来自110名患者（年龄的 mean [range] = 45 [17-81] 岁），包括五种不同的肿瘤类型。这些数据被随机分割，按照肿瘤体积进行训练（n=85）和测试集（n=30）。一名独立的第二读者（读者2）提供了测试集图像的手动分割。为了自动分割，我们应用了nnU-Net，并通过 randomly generating subsets with 25, 45, 65 and 85 训练集图像来调查训练集大小对性能的影响。我们评估了用于分类不同类型的uterine tumor 的 радиомiros特征的能力，并通过特征选择和机器学习来组合这些特征。使用整个训练集，我们测算了 fibroid DSC 的 mean [95% CI] 为 0.87 [0.59-1.00]，并且测试集上的两个读者之间的一致性为 0.89 [0.77-1.0]。在分类退化LM from LMS 时，我们在测试集上获得了 F1 分数为 0.80。基于 радиомiros特征，我们identified classifiers achieving F1 分数为 0.53 [0.45, 0.61] 和 0.80 [0.80, 0.80] 在测试集上，用于分类 benign versus malignant 和 degenerated LM versus LMS 任务。我们显示，可以通过使用 fewer than 150 annotated images 来开发一种可以与人类水平的自动 segmentation of the uterus and UT 的方法。然而，在分类不同类型的 UT 时，尽管我们训练了一些值得进一步调查的模型，但可靠的自动分类 UT 仍然是一个挑战。
</details></li>
</ul>
<hr>
<h2 id="SE-3-Equivariant-Augmented-Coupling-Flows"><a href="#SE-3-Equivariant-Augmented-Coupling-Flows" class="headerlink" title="SE(3) Equivariant Augmented Coupling Flows"></a>SE(3) Equivariant Augmented Coupling Flows</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10364">http://arxiv.org/abs/2308.10364</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lollcat/se3-augmented-coupling-flows">https://github.com/lollcat/se3-augmented-coupling-flows</a></li>
<li>paper_authors: Laurence I. Midgley, Vincent Stimper, Javier Antorán, Emile Mathieu, Bernhard Schölkopf, José Miguel Hernández-Lobato</li>
<li>for: 本文为了提出一种可以保持SE(3)和 permutation equivariant的coupling流程，以便在物理系统中进行 probabilistic modeling。</li>
<li>methods: 本文使用了coordinate splits along additional augmented dimensions，将原始坐标映射到学习的SE(3) invarianton bases中，然后应用标准流转换。</li>
<li>results: 本文的coupling流程可以保持fast sampling和density evaluation，并且可以生成无偏见的期望值。在训练DW4、LJ13和QM9-positional数据集时，本文的coupling流程与equivarian continuous normalizing flows相比，可以快速样本两个数量级。此外，本文还是第一个通过只模型分子的Cartesian坐标位置来学习全 Boltzmann 分布的 Alanine dipeptide。最后，本文示出了其可以使用只有能量函数来 aproximately 样本 DW4 和 LJ13 粒子系统的Boltzmann分布。<details>
<summary>Abstract</summary>
Coupling normalizing flows allow for fast sampling and density evaluation, making them the tool of choice for probabilistic modeling of physical systems. However, the standard coupling architecture precludes endowing flows that operate on the Cartesian coordinates of atoms with the SE(3) and permutation invariances of physical systems. This work proposes a coupling flow that preserves SE(3) and permutation equivariance by performing coordinate splits along additional augmented dimensions. At each layer, the flow maps atoms' positions into learned SE(3) invariant bases, where we apply standard flow transformations, such as monotonic rational-quadratic splines, before returning to the original basis. Crucially, our flow preserves fast sampling and density evaluation, and may be used to produce unbiased estimates of expectations with respect to the target distribution via importance sampling. When trained on the DW4, LJ13 and QM9-positional datasets, our flow is competitive with equivariant continuous normalizing flows, while allowing sampling two orders of magnitude faster. Moreover, to the best of our knowledge, we are the first to learn the full Boltzmann distribution of alanine dipeptide by only modeling the Cartesian positions of its atoms. Lastly, we demonstrate that our flow can be trained to approximately sample from the Boltzmann distribution of the DW4 and LJ13 particle systems using only their energy functions.
</details>
<details>
<summary>摘要</summary>
通过卷积整合流可以快速采样和评估概率分布，使其成为物理系统的概率模型工具。然而，标准 coupling 架构禁止将 Cartesian 坐标系中的 atoms 映射到 SE(3) 和 permutation 协变的物理系统中。这项工作提议一种 coupling 流，可以保持 SE(3) 和 permutation 协变，通过在附加的扩展维度上进行坐标拆分。在每层中，流量将 atoms 的位置映射到学习的 SE(3) 不变基底中，然后应用标准 flow 变换，如 monotonic rational-quadratic splines，然后返回到原基底。关键的是，我们的流量保持快速采样和概率评估，并可以生成偏向采样来计算对 Target 分布的预期值。当我们在 DW4、LJ13 和 QM9-positional 数据集上训练我们的流量时，它与 equivariant continuous normalizing flows 相当竞争，而且可以快速采样两个数量级。此外，我们是第一个通过只模型 Cartesian 坐标系中 atoms 的概率分布来学习 full Boltzmann 分布的 alanine dipeptide。最后，我们示示了我们的流量可以使用 DW4 和 LJ13 粒子系统的能量函数来约 approximate 其 Boltzmann 分布。
</details></li>
</ul>
<hr>
<h2 id="Can-Large-Language-Models-Find-And-Fix-Vulnerable-Software"><a href="#Can-Large-Language-Models-Find-And-Fix-Vulnerable-Software" class="headerlink" title="Can Large Language Models Find And Fix Vulnerable Software?"></a>Can Large Language Models Find And Fix Vulnerable Software?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10345">http://arxiv.org/abs/2308.10345</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Noever</li>
<li>for: 这个研究是用于评估大语言模型（LLMs）在检测软件漏洞方面的能力，特别是OpenAI的GPT-4，并与传统的静态代码分析工具如Snyk和Fortify进行比较。</li>
<li>methods: 这个研究使用了许多Repository，包括NASA和美国国防部的Repository，并使用了129个代码示例 across eight programming languages来测试GPT-4的性能。</li>
<li>results: GPT-4可以检测到 aproximately four times 的漏洞，并且可以提供可行的修复方案，false positive rate很低。Code corrections可以减少漏洞的数量by 90%，但需要增加代码行数by 11%。LLMs还能够自我审查，并提供修复漏洞的建议。<details>
<summary>Abstract</summary>
In this study, we evaluated the capability of Large Language Models (LLMs), particularly OpenAI's GPT-4, in detecting software vulnerabilities, comparing their performance against traditional static code analyzers like Snyk and Fortify. Our analysis covered numerous repositories, including those from NASA and the Department of Defense. GPT-4 identified approximately four times the vulnerabilities than its counterparts. Furthermore, it provided viable fixes for each vulnerability, demonstrating a low rate of false positives. Our tests encompassed 129 code samples across eight programming languages, revealing the highest vulnerabilities in PHP and JavaScript. GPT-4's code corrections led to a 90% reduction in vulnerabilities, requiring only an 11% increase in code lines. A critical insight was LLMs' ability to self-audit, suggesting fixes for their identified vulnerabilities and underscoring their precision. Future research should explore system-level vulnerabilities and integrate multiple static code analyzers for a holistic perspective on LLMs' potential.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们评估了大型自然语言模型（LLM），尤其是OpenAI的GPT-4，在检测软件漏洞方面的能力，并与传统的静态代码分析工具如Snyk和Fortify进行比较。我们的分析覆盖了多个Repository，包括NASA和国防部的Repository。GPT-4在检测漏洞方面表现出了约四倍的能力，并提供了每个漏洞的可行的修复方案，表明了它的假阳性率非常低。我们的测试包括129个代码样本 across eight种编程语言，发现PHP和JavaScript中的漏洞最高。GPT-4的代码更正引起了漏洞的减少率达90%，仅需要11%的代码行数增加。LLMs的自我审查能力为我们提供了一个关键的发现，它们可以自动提供修复方案，并证明了它们的精度。未来的研究应该探索系统级别的漏洞和将多个静态代码分析工具集成到LLMs的潜在力量中。
</details></li>
</ul>
<hr>
<h2 id="A-Comprehensive-Empirical-Evaluation-on-Online-Continual-Learning"><a href="#A-Comprehensive-Empirical-Evaluation-on-Online-Continual-Learning" class="headerlink" title="A Comprehensive Empirical Evaluation on Online Continual Learning"></a>A Comprehensive Empirical Evaluation on Online Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10328">http://arxiv.org/abs/2308.10328</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/albinsou/ocl_survey">https://github.com/albinsou/ocl_survey</a></li>
<li>paper_authors: Albin Soutif–Cormerais, Antonio Carta, Andrea Cossu, Julio Hurtado, Vincenzo Lomonaco, Joost Van de Weijer, Hamed Hemati</li>
<li>for: 本研究旨在评估在流动数据上进行连续学习的不同方法，以实现更加接近实际学习经验。</li>
<li>methods: 本研究考虑了Literature中的多种方法，包括分类增量学习。</li>
<li>results: 研究发现大多数方法受到稳定性和过拟合问题的影响，但是学习表示的质量与同样计算预算下的随机批处理相当。<details>
<summary>Abstract</summary>
Online continual learning aims to get closer to a live learning experience by learning directly on a stream of data with temporally shifting distribution and by storing a minimum amount of data from that stream. In this empirical evaluation, we evaluate various methods from the literature that tackle online continual learning. More specifically, we focus on the class-incremental setting in the context of image classification, where the learner must learn new classes incrementally from a stream of data. We compare these methods on the Split-CIFAR100 and Split-TinyImagenet benchmarks, and measure their average accuracy, forgetting, stability, and quality of the representations, to evaluate various aspects of the algorithm at the end but also during the whole training period. We find that most methods suffer from stability and underfitting issues. However, the learned representations are comparable to i.i.d. training under the same computational budget. No clear winner emerges from the results and basic experience replay, when properly tuned and implemented, is a very strong baseline. We release our modular and extensible codebase at https://github.com/AlbinSou/ocl_survey based on the avalanche framework to reproduce our results and encourage future research.
</details>
<details>
<summary>摘要</summary>
在线连续学习目标是通过直接学习流动数据的方式，与时间变化的分布进行融合，并将最少量的数据存储在流动中。在这个实验性评估中，我们评估了文献中的多种在线连续学习方法。更specifically，我们在图像分类任务中强调类增量设置，learner需要从数据流中逐渐学习新类。我们在Split-CIFAR100和Split-TinyImagenet bencmarks上对这些方法进行了平均精度、忘记、稳定性和表示质量的评估，以评估不同方法的各个方面。我们发现大多数方法受到稳定性和下降问题的影响。然而，学习的表示比i.i.d.培育下相同计算预算下的表示相当。没有一个明确的赢家，基本的经验回快在适度调整和实现下成为了强大的基准。我们在https://github.com/AlbinSou/ocl_survey中发布了我们的模块化和可扩展的代码基础，以便重现我们的结果并促进未来的研究。
</details></li>
</ul>
<hr>
<h2 id="Quantum-State-Tomography-using-Quantum-Machine-Learning"><a href="#Quantum-State-Tomography-using-Quantum-Machine-Learning" class="headerlink" title="Quantum State Tomography using Quantum Machine Learning"></a>Quantum State Tomography using Quantum Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10327">http://arxiv.org/abs/2308.10327</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hongyehu/Machine_Learning_Quantum_State_Tomography">https://github.com/hongyehu/Machine_Learning_Quantum_State_Tomography</a></li>
<li>paper_authors: Nouhaila Innan, Owais Ishtiaq Siddiqui, Shivang Arora, Tamojit Ghosh, Yasemin Poyraz Koçak, Dominic Paragas, Abdullah Al Omar Galib, Muhammad Al-Zafar Khan, Mohamed Bennai</li>
<li>for: 这个论文的目的是提出一种基于量子机器学习（QML）技术的量子状态探测（QST）方法，以提高QST的效率。</li>
<li>methods: 这个论文使用了多种类ссиkal和量子方法来进行QST，并评估了不同的QML方法的效果。</li>
<li>results: 研究结果表明，使用QML技术可以significantly reduce the number of measurements required for QST, 并且可以达到高准确率（98%）。<details>
<summary>Abstract</summary>
Quantum State Tomography (QST) is a fundamental technique in Quantum Information Processing (QIP) for reconstructing unknown quantum states. However, the conventional QST methods are limited by the number of measurements required, which makes them impractical for large-scale quantum systems. To overcome this challenge, we propose the integration of Quantum Machine Learning (QML) techniques to enhance the efficiency of QST. In this paper, we conduct a comprehensive investigation into various approaches for QST, encompassing both classical and quantum methodologies; We also implement different QML approaches for QST and demonstrate their effectiveness on various simulated and experimental quantum systems, including multi-qubit networks. Our results show that our QML-based QST approach can achieve high fidelity (98%) with significantly fewer measurements than conventional methods, making it a promising tool for practical QIP applications.
</details>
<details>
<summary>摘要</summary>
量子状态测测（QST）是Quantum Information Processing（QIP）中重要的技术，用于重建未知量子状态。然而，现有的QST方法受限于测量数量的限制，使其对大规模量子系统不实际。为解决这个挑战，我们提议通过量子机器学习（QML）技术来增强QST的效率。在这篇论文中，我们进行了各种QST方法的全面调查，包括класси方法和量子方法；我们还实施了不同的QML方法，并在多个模拟和实验量子系统上进行了测试，包括多元比特网络。我们的结果表明，我们的QML基于QST方法可以达到高准确率（98%），并且使用了许多 fewer measurements than conventional methods，这使其成为实际QIP应用中的一个有前途的工具。
</details></li>
</ul>
<hr>
<h2 id="Homogenising-SoHO-EIT-and-SDO-AIA-171A-Images-A-Deep-Learning-Approach"><a href="#Homogenising-SoHO-EIT-and-SDO-AIA-171A-Images-A-Deep-Learning-Approach" class="headerlink" title="Homogenising SoHO&#x2F;EIT and SDO&#x2F;AIA 171Å$~$ Images: A Deep Learning Approach"></a>Homogenising SoHO&#x2F;EIT and SDO&#x2F;AIA 171Å$~$ Images: A Deep Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10322">http://arxiv.org/abs/2308.10322</a></li>
<li>repo_url: None</li>
<li>paper_authors: Subhamoy Chatterjee, Andrés Muñoz-Jaramillo, Maher Dayeh, Hazel M. Bain, Kimberly Moreland</li>
<li>for: 这个研究旨在创建一个具有同一个特征的 EUV 图像数据集，以便进行太阳气候预测任务。</li>
<li>methods: 这个研究使用了 SoHO&#x2F;EIT 和 SDO&#x2F;AIA 171 ~\AA ~的探测数据，通过 ensemble learning 技术将多个探测数据集合并，并使用 Approximate Bayesian Ensembling 方法来生成一个 ensemble 模型，以估计对于不同气候预测任务的 uncertainty。</li>
<li>results: 研究发现， ensemble 模型的 uncertainty 随着训练集大小的增加而降低，而且 ensemble 模型在测试数据中表现出较高的 uncertainty，表明它可以准确地预测不同的气候预测任务。<details>
<summary>Abstract</summary>
Extreme Ultraviolet images of the Sun are becoming an integral part of space weather prediction tasks. However, having different surveys requires the development of instrument-specific prediction algorithms. As an alternative, it is possible to combine multiple surveys to create a homogeneous dataset. In this study, we utilize the temporal overlap of SoHO/EIT and SDO/AIA 171~\AA ~surveys to train an ensemble of deep learning models for creating a single homogeneous survey of EUV images for 2 solar cycles. Prior applications of deep learning have focused on validating the homogeneity of the output while overlooking the systematic estimation of uncertainty. We use an approach called `Approximate Bayesian Ensembling' to generate an ensemble of models whose uncertainty mimics that of a fully Bayesian neural network at a fraction of the cost. We find that ensemble uncertainty goes down as the training set size increases. Additionally, we show that the model ensemble adds immense value to the prediction by showing higher uncertainty in test data that are not well represented in the training data.
</details>
<details>
<summary>摘要</summary>
Extreme Ultraviolet 图像的太阳是天气预测任务中的一个重要组成部分。然而，不同的调查需要开发专门的仪器预测算法。作为一种 alternatives，我们可以将多个调查结合起来创建一个一致的数据集。在这项研究中，我们利用SOHO/EIT和SDO/AIA 171 Å 调查的时间重叠来训练一个深度学习模型的 ensemble，以生成一个包含2个太阳周期的homogeneous EUV图像数据集。先前的深度学习应用都是验证输出的一致性，而忽略了系统性的估计不确定性。我们使用一种名为“ Approximate Bayesian Ensembling”的方法，生成一个ensemble的模型，其不确定性与完全 Bayesian 神经网络相似，但是比其便宜。我们发现，ensemble 不确定性随着训练集大小的增加而下降。此外，我们还显示，模型ensemble 对预测具有很大的价值，可以在测试数据中显示更高的不确定性，这些数据不良 represent 在训练数据中。
</details></li>
</ul>
<hr>
<h2 id="Towards-Sustainable-Development-A-Novel-Integrated-Machine-Learning-Model-for-Holistic-Environmental-Health-Monitoring"><a href="#Towards-Sustainable-Development-A-Novel-Integrated-Machine-Learning-Model-for-Holistic-Environmental-Health-Monitoring" class="headerlink" title="Towards Sustainable Development: A Novel Integrated Machine Learning Model for Holistic Environmental Health Monitoring"></a>Towards Sustainable Development: A Novel Integrated Machine Learning Model for Holistic Environmental Health Monitoring</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10317">http://arxiv.org/abs/2308.10317</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anirudh Mazumder, Sarthak Engala, Aditya Nallaparaju</li>
<li>for: 这个研究旨在帮助政府确定 intervención点，改善规划和保护努力，以促进可持续发展。</li>
<li>methods: 这个研究使用机器学习技术来识别环境状况的预测特征，并使用污染物水平和 particulate matter 作为环境状况的指标。</li>
<li>results: 研究发现了连接不良环境 condition 的特征，帮助政府更好地识别 intervención点，并且可以促进可持续发展。<details>
<summary>Abstract</summary>
Urbanization enables economic growth but also harms the environment through degradation. Traditional methods of detecting environmental issues have proven inefficient. Machine learning has emerged as a promising tool for tracking environmental deterioration by identifying key predictive features. Recent research focused on developing a predictive model using pollutant levels and particulate matter as indicators of environmental state in order to outline challenges. Machine learning was employed to identify patterns linking areas with worse conditions. This research aims to assist governments in identifying intervention points, improving planning and conservation efforts, and ultimately contributing to sustainable development.
</details>
<details>
<summary>摘要</summary>
城市化促进经济增长，但也导致环境退化。传统的环境问题检测方法已经证明不够有效。机器学习技术在跟踪环境衰退方面表现出了替代性，通过确定关键预测特征来开发预测模型。最近的研究集中于开发基于污染物水平和悬尘物含量的预测模型，以描述环境状况的挑战。机器学习技术被用来找出与环境状况相关的模式，以帮助政府确定 intervención点，改善规划和保护努力，最终贡献到可持续发展。
</details></li>
</ul>
<hr>
<h2 id="Demystifying-the-Performance-of-Data-Transfers-in-High-Performance-Research-Networks"><a href="#Demystifying-the-Performance-of-Data-Transfers-in-High-Performance-Research-Networks" class="headerlink" title="Demystifying the Performance of Data Transfers in High-Performance Research Networks"></a>Demystifying the Performance of Data Transfers in High-Performance Research Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10312">http://arxiv.org/abs/2308.10312</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ehsan Saeedizade, Bing Zhang, Engin Arslan</li>
<li>for: 本文旨在提供一个可扩展的监控框架，以便监测高速研究网络中数据传输的性能问题。</li>
<li>methods: 本文使用了一种可扩展的监控框架，可以收集和存储关键性能指标，以便了解数据传输的性能问题。</li>
<li>results: 评估结果显示，提案的框架可以同时监测400个传输并收集一秒精度的性能统计数据，并且可以自动处理收集到的性能指标，并在87-98%的准确率下识别性能异常。<details>
<summary>Abstract</summary>
High-speed research networks are built to meet the ever-increasing needs of data-intensive distributed workflows. However, data transfers in these networks often fail to attain the promised transfer rates for several reasons, including I/O and network interference, server misconfigurations, and network anomalies. Although understanding the root causes of performance issues is critical to mitigating them and increasing the utilization of expensive network infrastructures, there is currently no available mechanism to monitor data transfers in these networks. In this paper, we present a scalable, end-to-end monitoring framework to gather and store key performance metrics for file transfers to shed light on the performance of transfers. The evaluation results show that the proposed framework can monitor up to 400 transfers per host and more than 40, 000 transfers in total while collecting performance statistics at one-second precision. We also introduce a heuristic method to automatically process the gathered performance metrics and identify the root causes of performance anomalies with an F-score of 87 - 98%.
</details>
<details>
<summary>摘要</summary>
高速研究网络是为满足数据敏感分布式工作流程的需求而建立的。然而，在这些网络中的数据传输经常无法实现承诺的传输速率，这可能是因为输入输出和网络干扰、服务器配置错误以及网络异常。虽然了解性能问题的根本原因是解决性能问题的关键，但目前没有可用的监控数据传输的机制。在本文中，我们提出了一个可扩展的终端到终端监控框架，以收集和存储关键性能指标。评估结果显示，我们的框架可以同时监控400个传输任务和总共超过40,000个传输任务，并在1秒精度收集性能统计数据。我们还提出了一种冒险法来自动处理收集到的性能指标，并使用F-score在87-98%的情况下自动确定性能异常的根本原因。
</details></li>
</ul>
<hr>
<h2 id="I-O-Burst-Prediction-for-HPC-Clusters-using-Darshan-Logs"><a href="#I-O-Burst-Prediction-for-HPC-Clusters-using-Darshan-Logs" class="headerlink" title="I&#x2F;O Burst Prediction for HPC Clusters using Darshan Logs"></a>I&#x2F;O Burst Prediction for HPC Clusters using Darshan Logs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10311">http://arxiv.org/abs/2308.10311</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ehsan Saeedizade, Roya Taheri, Engin Arslan<br>for: 这篇论文的目的是分析大规模高性能计算（HPC）集群内的各种I&#x2F;O模式，以便最小化I&#x2F;O干扰的发生和影响。methods: 论文使用Darshan报告来抽出系统水平的读写I&#x2F;O率，并使用机器学习模型预测系统层级I&#x2F;O填充的发生。results: 结果显示在五分钟间隔内，读写I&#x2F;O率会有明显的颠簸（超过100倍），并且使用机器学习模型可以预测I&#x2F;O填充的发生，精度高于90%（F-1分数）。此外，研究还显示了一个可以根据I&#x2F;O填充预测的内存管理策略，可以降低应用程序的执行时间。<details>
<summary>Abstract</summary>
Understanding cluster-wide I/O patterns of large-scale HPC clusters is essential to minimize the occurrence and impact of I/O interference. Yet, most previous work in this area focused on monitoring and predicting task and node-level I/O burst events. This paper analyzes Darshan reports from three supercomputers to extract system-level read and write I/O rates in five minutes intervals. We observe significant (over 100x) fluctuations in read and write I/O rates in all three clusters. We then train machine learning models to estimate the occurrence of system-level I/O bursts 5 - 120 minutes ahead. Evaluation results show that we can predict I/O bursts with more than 90% accuracy (F-1 score) five minutes ahead and more than 87% accuracy two hours ahead. We also show that the ML models attain more than 70% accuracy when estimating the degree of the I/O burst. We believe that high-accuracy predictions of I/O bursts can be used in multiple ways, such as postponing delay-tolerant I/O operations (e.g., checkpointing), pausing nonessential applications (e.g., file system scrubbers), and devising I/O-aware job scheduling methods. To validate this claim, we simulated a burst-aware job scheduler that can postpone the start time of applications to avoid I/O bursts. We show that the burst-aware job scheduling can lead to an up to 5x decrease in application runtime.
</details>
<details>
<summary>摘要</summary>
理解大规模HPC集群的总体I/O模式是必要的，以避免I/O干扰的发生和影响。然而，大多数前一些研究都集中于监测和预测任务和节点级I/O异常事件。这篇论文使用Darshan报告数据分析了三个超级计算机的系统级读写I/O速率，发现了所有三个集群中的读写I/O速率存在明显的异常波动（大于100倍）。然后，我们使用机器学习模型来预测系统级I/O异常事件的发生5-120分钟前。评估结果表明，我们可以在5分钟前预测I/O异常事件的发生率高于90%（F-1分数），并在2小时前预测精度高于87%。此外，我们发现ML模型在预测I/O异常事件的严重程度时的准确率超过70%。我们认为高精度的I/O异常预测可以用于多种方式，如延迟快速I/O操作（例如检查点）、挂起非关键应用程序（例如文件系统扫描器），并开发I/O意识的任务调度策略。为验证这一点，我们 simulate了一种基于I/O异常预测的任务调度策略，并证明该策略可以减少应用程序执行时间的最大幅度达5倍。
</details></li>
</ul>
<hr>
<h2 id="Co-Evolution-of-Pose-and-Mesh-for-3D-Human-Body-Estimation-from-Video"><a href="#Co-Evolution-of-Pose-and-Mesh-for-3D-Human-Body-Estimation-from-Video" class="headerlink" title="Co-Evolution of Pose and Mesh for 3D Human Body Estimation from Video"></a>Co-Evolution of Pose and Mesh for 3D Human Body Estimation from Video</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10305">http://arxiv.org/abs/2308.10305</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kasvii/pmce">https://github.com/kasvii/pmce</a></li>
<li>paper_authors: Yingxuan You, Hong Liu, Ti Wang, Wenhao Li, Runwei Ding, Xia Li</li>
<li>for: 用于重建3D人体动作 from video</li>
<li>methods: 使用 Pose and Mesh Co-Evolution network (PMCE)，分为两个部分：1) 视频基于3D人体pose estimation，2) 从估计 pose 和时间图像特征中进行预测 mesh  vertices</li>
<li>results: 在三个基准数据集上（3DPW、Human3.6M 和 MPI-INF-3DHP）实现了以前没有达到的高精度和temporal consistency，并且超越了之前的状态 искусственный智能方法。<details>
<summary>Abstract</summary>
Despite significant progress in single image-based 3D human mesh recovery, accurately and smoothly recovering 3D human motion from a video remains challenging. Existing video-based methods generally recover human mesh by estimating the complex pose and shape parameters from coupled image features, whose high complexity and low representation ability often result in inconsistent pose motion and limited shape patterns. To alleviate this issue, we introduce 3D pose as the intermediary and propose a Pose and Mesh Co-Evolution network (PMCE) that decouples this task into two parts: 1) video-based 3D human pose estimation and 2) mesh vertices regression from the estimated 3D pose and temporal image feature. Specifically, we propose a two-stream encoder that estimates mid-frame 3D pose and extracts a temporal image feature from the input image sequence. In addition, we design a co-evolution decoder that performs pose and mesh interactions with the image-guided Adaptive Layer Normalization (AdaLN) to make pose and mesh fit the human body shape. Extensive experiments demonstrate that the proposed PMCE outperforms previous state-of-the-art methods in terms of both per-frame accuracy and temporal consistency on three benchmark datasets: 3DPW, Human3.6M, and MPI-INF-3DHP. Our code is available at https://github.com/kasvii/PMCE.
</details>
<details>
<summary>摘要</summary>
尽管单个图像基于的3D人体模lesh recovering已经取得了 significativ progress,但从视频中准确地和平滑地recover 3D人体运动仍然是一个挑战。现有的视频基于方法通常通过估算复杂的姿势和形状参数从相关的图像特征来recover人体模lesh, whose high complexity and low representation ability often result in inconsistent pose motion and limited shape patterns. To address this issue, we introduce 3D pose as the intermediary and propose a Pose and Mesh Co-Evolution network (PMCE) that decouples this task into two parts: 1) video-based 3D human pose estimation and 2) mesh vertices regression from the estimated 3D pose and temporal image feature. Specifically, we propose a two-stream encoder that estimates mid-frame 3D pose and extracts a temporal image feature from the input image sequence. In addition, we design a co-evolution decoder that performs pose and mesh interactions with the image-guided Adaptive Layer Normalization (AdaLN) to make pose and mesh fit the human body shape. Extensive experiments demonstrate that the proposed PMCE outperforms previous state-of-the-art methods in terms of both per-frame accuracy and temporal consistency on three benchmark datasets: 3DPW, Human3.6M, and MPI-INF-3DHP. Our code is available at https://github.com/kasvii/PMCE.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/21/cs.LG_2023_08_21/" data-id="clm0t8e0k007cv788dku7dt2e" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_08_21" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/21/cs.SD_2023_08_21/" class="article-date">
  <time datetime="2023-08-20T16:00:00.000Z" itemprop="datePublished">2023-08-21</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/21/cs.SD_2023_08_21/">cs.SD - 2023-08-21 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="LibriWASN-A-Data-Set-for-Meeting-Separation-Diarization-and-Recognition-with-Asynchronous-Recording-Devices"><a href="#LibriWASN-A-Data-Set-for-Meeting-Separation-Diarization-and-Recognition-with-Asynchronous-Recording-Devices" class="headerlink" title="LibriWASN: A Data Set for Meeting Separation, Diarization, and Recognition with Asynchronous Recording Devices"></a>LibriWASN: A Data Set for Meeting Separation, Diarization, and Recognition with Asynchronous Recording Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10682">http://arxiv.org/abs/2308.10682</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joerg Schmalenstroeer, Tobias Gburrek, Reinhold Haeb-Umbach</li>
<li>for: 这个论文是为测试随机位置的无线听写系统而设计的数据集。</li>
<li>methods: 该数据集使用了九种不同的设备，包括五个智能手机和四个麦克风阵列，共记录了29个通道的数据。</li>
<li>results: 该数据集包含了与LibriCSS相似的LibriSpeech句子播放和会议 overlap的情况，可以用于测试时钟同步算法、会议分离、转录和讲话系统。<details>
<summary>Abstract</summary>
We present LibriWASN, a data set whose design follows closely the LibriCSS meeting recognition data set, with the marked difference that the data is recorded with devices that are randomly positioned on a meeting table and whose sampling clocks are not synchronized. Nine different devices, five smartphones with a single recording channel and four microphone arrays, are used to record a total of 29 channels. Other than that, the data set follows closely the LibriCSS design: the same LibriSpeech sentences are played back from eight loudspeakers arranged around a meeting table and the data is organized in subsets with different percentages of speech overlap. LibriWASN is meant as a test set for clock synchronization algorithms, meeting separation, diarization and transcription systems on ad-hoc wireless acoustic sensor networks. Due to its similarity to LibriCSS, meeting transcription systems developed for the former can readily be tested on LibriWASN. The data set is recorded in two different rooms and is complemented with ground-truth diarization information of who speaks when.
</details>
<details>
<summary>摘要</summary>
我们介绍LibriWASN数据集，其设计与LibriCSS会议认知数据集相似，但唯一不同之处是数据记录设备随机分布在会议表前，并且采样时钟不同步。数据集使用了五款智能手机和四个麦克风组合，共记录29个通道。除此之外，数据集的设计与LibriCSS相同：同样使用LibriSpeech句子在会议表周围的八个扬声器播放，并将数据分成不同的发言重叠百分比。LibriWASN是为无线听取感知网络的时钟同步算法、会议分离、笔记录和转录系统进行测试而设计的。由于与LibriCSS的相似性，可以将为LibriCSS开发的会议转录系统直接应用到LibriWASN上。数据集在两个不同的房间中录制，并且配备了会议发言人员时间ground truth的 диари化信息。
</details></li>
</ul>
<hr>
<h2 id="An-Anchor-Point-Based-Image-Model-for-Room-Impulse-Response-Simulation-with-Directional-Source-Radiation-and-Sensor-Directivity-Patterns"><a href="#An-Anchor-Point-Based-Image-Model-for-Room-Impulse-Response-Simulation-with-Directional-Source-Radiation-and-Sensor-Directivity-Patterns" class="headerlink" title="An Anchor-Point Based Image-Model for Room Impulse Response Simulation with Directional Source Radiation and Sensor Directivity Patterns"></a>An Anchor-Point Based Image-Model for Room Impulse Response Simulation with Directional Source Radiation and Sensor Directivity Patterns</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10543">http://arxiv.org/abs/2308.10543</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao Pan, Lei Zhang, Yilong Lu, Jilu Jin, Lin Qiu, Jingdong Chen, Jacob Benesty</li>
<li>for: 这篇论文的目的是扩展图像模型方法，以便在不同应用中使用。</li>
<li>methods: 该方法使用了 anchor point image model (APIM) 方法，包括源辐射和感知器直达性特征。</li>
<li>results: 该方法可以根据方向函数、分解时延和计算复杂性生成房间响应。可用于多种声学问题，以便模拟房间响应和评估处理算法。<details>
<summary>Abstract</summary>
The image model method has been widely used to simulate room impulse responses and the endeavor to adapt this method to different applications has also piqued great interest over the last few decades. This paper attempts to extend the image model method and develops an anchor-point-image-model (APIM) approach as a solution for simulating impulse responses by including both the source radiation and sensor directivity patterns. To determine the orientations of all the virtual sources, anchor points are introduced to real sources, which subsequently lead to the determination of the orientations of the virtual sources. An algorithm is developed to generate room impulse responses with APIM by taking into account the directional pattern functions, factional time delays, as well as the computational complexity. The developed model and algorithms can be used in various acoustic problems to simulate room acoustics and improve and evaluate processing algorithms.
</details>
<details>
<summary>摘要</summary>
“图像模型方法在过去几十年内已经广泛使用来模拟房间快速响应，而将这方法应用于不同的应用场景也引起了很大的兴趣。本文尝试将图像模型方法扩展，开发出一个图像模型（APIM）方法来模拟快速响应，这个方法包括了源辐射和感应器直径图形的考虑。为了决定所有虚拟源的方向，本文引入了紧缩点，这些紧缩点将导致虚拟源的方向的决定。具有考虑irectional pattern functions、分割时延和计算复杂度的算法是为了生成房间快速响应的APIM模型和算法。这个模型和算法可以在各种音响问题中模拟房间音响，提高和评估处理算法。”
</details></li>
</ul>
<hr>
<h2 id="Implicit-Self-supervised-Language-Representation-for-Spoken-Language-Diarization"><a href="#Implicit-Self-supervised-Language-Representation-for-Spoken-Language-Diarization" class="headerlink" title="Implicit Self-supervised Language Representation for Spoken Language Diarization"></a>Implicit Self-supervised Language Representation for Spoken Language Diarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10470">http://arxiv.org/abs/2308.10470</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jagabandhu Mishra, S. R. Mahadeva Prasanna<br>for: 这个论文的目的是提出一种基于自适应框架的语音话语分类方法，以便在低资源语言上进行语音分类。methods: 这个论文使用了三种不同的框架，分别是固定分 segmentation、变点基本分 segmentation 和 E2E 框架，并使用 x-vector 作为隐式语言表示。results: 该论文的实验结果表明，使用 x-vector 作为隐式语言表示可以达到同样的表达效果，而且使用 E2E 框架可以达到最佳的隐式语言分类性能（JER &#x3D; 6.38）。然而，在使用实际的 Microsoft CS（MSCS）数据集时，隐式语言分类性能下降至 60.4，主要是因为 MSCS 数据集中次语言的分段持续时间的分布不同于 TTSF-LD 数据集。此外，使用小值 N 可以避免分段膨润，但是同时 x-vector 表示不能够捕捉到语言差异，因为同一个 speaker 在两种语言中说话。因此，该研究提出了一种自适应隐式语言表示，并与 x-vector 表示进行比较。<details>
<summary>Abstract</summary>
In a code-switched (CS) scenario, the use of spoken language diarization (LD) as a pre-possessing system is essential. Further, the use of implicit frameworks is preferable over the explicit framework, as it can be easily adapted to deal with low/zero resource languages. Inspired by speaker diarization (SD) literature, three frameworks based on (1) fixed segmentation, (2) change point-based segmentation and (3) E2E are proposed to perform LD. The initial exploration with synthetic TTSF-LD dataset shows, using x-vector as implicit language representation with appropriate analysis window length ($N$) can able to achieve at per performance with explicit LD. The best implicit LD performance of $6.38$ in terms of Jaccard error rate (JER) is achieved by using the E2E framework. However, considering the E2E framework the performance of implicit LD degrades to $60.4$ while using with practical Microsoft CS (MSCS) dataset. The difference in performance is mostly due to the distributional difference between the monolingual segment duration of secondary language in the MSCS and TTSF-LD datasets. Moreover, to avoid segment smoothing, the smaller duration of the monolingual segment suggests the use of a small value of $N$. At the same time with small $N$, the x-vector representation is unable to capture the required language discrimination due to the acoustic similarity, as the same speaker is speaking both languages. Therefore, to resolve the issue a self-supervised implicit language representation is proposed in this study. In comparison with the x-vector representation, the proposed representation provides a relative improvement of $63.9\%$ and achieved a JER of $21.8$ using the E2E framework.
</details>
<details>
<summary>摘要</summary>
在码换（CS）场景中，使用口语语音分类（LD）作为预处理系统是必备的。此外，使用隐式框架更有利于处理低/零资源语言，因此根据说话者分类（SD）文献，提出了三种基于（1）固定分 segmentation、（2）变点分 segmentation和（3）E2E的框架来实现LD。使用xvector作为隐式语言表示，并选择合适的分析窗口长度($N$)，可以达到与Explicit LD相同的性能。最佳隐式LD性能为6.38个Jaccard错误率（JER）， achievable by using E2E框架。然而，在使用实际的微软CS（MSCS） dataset时，隐式LD性能下降到60.4个JER，主要是由于MSCS dataset中次语言的单语言分段时间的分布不同于TTSF-LD dataset。此外，避免分段平滑的问题，需要选择小的分段时间。同时，使用小的$N$值，xvector表示无法捕捉到必要的语言划分，因为同一个说话者同时说两种语言，导致问题。因此，本研究提出了一种自动学习的隐式语言表示，与xvector表示相比，提供了63.9%的相对改进，并使用E2E框架达到JER21.8。
</details></li>
</ul>
<hr>
<h2 id="Multi-GradSpeech-Towards-Diffusion-based-Multi-Speaker-Text-to-speech-Using-Consistent-Diffusion-Models"><a href="#Multi-GradSpeech-Towards-Diffusion-based-Multi-Speaker-Text-to-speech-Using-Consistent-Diffusion-Models" class="headerlink" title="Multi-GradSpeech: Towards Diffusion-based Multi-Speaker Text-to-speech Using Consistent Diffusion Models"></a>Multi-GradSpeech: Towards Diffusion-based Multi-Speaker Text-to-speech Using Consistent Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10428">http://arxiv.org/abs/2308.10428</a></li>
<li>repo_url: None</li>
<li>paper_authors: Heyang Xue, Shuai Guo, Pengcheng Zhu, Mengxiao Bi</li>
<li>for: 提高多 speaker Text-to-Speech（TTS）性能</li>
<li>methods: 引入 Consistent Diffusion Model（CDM）生成模型，在训练过程中保证 CDM 的一致性，以避免抽取难点问题</li>
<li>results: 对多 speaker TTS 表现有显著提高，甚至超过了细化调整方法的表现In English:</li>
<li>for: Improving the performance of multi-speaker Text-to-Speech (TTS)</li>
<li>methods: Introducing the Consistent Diffusion Model (CDM) as a generative modeling approach, ensuring the consistency property during training to alleviate the sampling drift problem in the inference stage</li>
<li>results: Significant improvements in multi-speaker TTS performance, outperforming the fine-tuning approach and available audio samples at <a target="_blank" rel="noopener" href="https://welkinyang.github.io/multi-gradspeech/">https://welkinyang.github.io/multi-gradspeech/</a><details>
<summary>Abstract</summary>
Recent advancements in diffusion-based acoustic models have revolutionized data-sufficient single-speaker Text-to-Speech (TTS) approaches, with Grad-TTS being a prime example. However, diffusion models suffer from drift in training and sampling distributions due to imperfect score-matching. The sampling drift problem leads to these approaches struggling in multi-speaker scenarios in practice. In this paper, we present Multi-GradSpeech, a multi-speaker diffusion-based acoustic models which introduces the Consistent Diffusion Model (CDM) as a generative modeling approach. We enforce the consistency property of CDM during the training process to alleviate the sampling drift problem in the inference stage, resulting in significant improvements in multi-speaker TTS performance. Our experimental results corroborate that our proposed approach can improve the performance of different speakers involved in multi-speaker TTS compared to Grad-TTS, even outperforming the fine-tuning approach. Audio samples are available at https://welkinyang.github.io/multi-gradspeech/
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Neural-Architectures-Learning-Fourier-Transforms-Signal-Processing-and-Much-More…"><a href="#Neural-Architectures-Learning-Fourier-Transforms-Signal-Processing-and-Much-More…" class="headerlink" title="Neural Architectures Learning Fourier Transforms, Signal Processing and Much More…."></a>Neural Architectures Learning Fourier Transforms, Signal Processing and Much More….</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10388">http://arxiv.org/abs/2308.10388</a></li>
<li>repo_url: None</li>
<li>paper_authors: Prateek Verma</li>
<li>For: The paper explores the use of neural architectures for learning kernels in Fourier Transform, specifically for audio signal processing applications.* Methods: The paper uses a neural network to learn sinusoidal kernel shapes and discovers various signal-processing properties such as windowing functions, onset detectors, high pass filters, low pass filters, modulations, etc. The neural architecture has a comb filter-like structure on top of the learned kernels.* Results: The paper shows that the neural architecture not only learns sinusoidal kernel shapes but also discovers all kinds of incredible signal-processing properties, such as windowing functions, onset detectors, high pass filters, low pass filters, modulations, etc. The learned kernels can be used for a variety of signal processing tasks, and the content of the kernels can be made adaptive to different inputs.<details>
<summary>Abstract</summary>
This report will explore and answer fundamental questions about taking Fourier Transforms and tying it with recent advances in AI and neural architecture. One interpretation of the Fourier Transform is decomposing a signal into its constituent components by projecting them onto complex exponentials. Variants exist, such as discrete cosine transform that does not operate on the complex domain and projects an input signal to only cosine functions oscillating at different frequencies. However, this is a fundamental limitation, and it needs to be more suboptimal. The first one is that all kernels are sinusoidal: What if we could have some kernels adapted or learned according to the problem? What if we can use neural architectures for this? We show how one can learn these kernels from scratch for audio signal processing applications. We find that the neural architecture not only learns sinusoidal kernel shapes but discovers all kinds of incredible signal-processing properties. E.g., windowing functions, onset detectors, high pass filters, low pass filters, modulations, etc. Further, upon analysis of the filters, we find that the neural architecture has a comb filter-like structure on top of the learned kernels. Comb filters that allow harmonic frequencies to pass through are one of the core building blocks/types of filters similar to high-pass, low-pass, and band-pass filters of various traditional signal processing algorithms. Further, we can also use the convolution operation with a signal to be learned from scratch, and we will explore papers in the literature that uses this with that robust Transformer architectures. Further, we would also explore making the learned kernel's content adaptive, i.e., learning different kernels for different inputs.
</details>
<details>
<summary>摘要</summary>
traducción al chino simplificado:这份报告将探索并回答关于使用傅立叶变换和最新的人工智能和神经网络架构之间的关系的基本问题。傅立叶变换的一种解释是将信号分解成其成分部分，并将它们 proyect onto 复杂的指数函数。 variants exist， such as the discrete cosine transform that does not operate on the complex domain and projects an input signal to only cosine functions oscillating at different frequencies. However, this is a fundamental limitation, and it needs to be more suboptimal. The first one is that all kernels are sinusoidal: What if we could have some kernels adapted or learned according to the problem? What if we can use neural architectures for this? We show how one can learn these kernels from scratch for audio signal processing applications. We find that the neural architecture not only learns sinusoidal kernel shapes but discovers all kinds of incredible signal-processing properties. E.g., windowing functions, onset detectors, high pass filters, low pass filters, modulations, etc. Further, upon analysis of the filters, we find that the neural architecture has a comb filter-like structure on top of the learned kernels. Comb filters that allow harmonic frequencies to pass through are one of the core building blocks/types of filters similar to high-pass, low-pass, and band-pass filters of various traditional signal processing algorithms. Further, we can also use the convolution operation with a signal to be learned from scratch, and we will explore papers in the literature that uses this with that robust Transformer architectures. Further, we would also explore making the learned kernel's content adaptive, i.e., learning different kernels for different inputs.
</details></li>
</ul>
<hr>
<h2 id="Local-Periodicity-Based-Beat-Tracking-for-Expressive-Classical-Piano-Music"><a href="#Local-Periodicity-Based-Beat-Tracking-for-Expressive-Classical-Piano-Music" class="headerlink" title="Local Periodicity-Based Beat Tracking for Expressive Classical Piano Music"></a>Local Periodicity-Based Beat Tracking for Expressive Classical Piano Music</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10355">http://arxiv.org/abs/2308.10355</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sunnycyc/plpdp4beat">https://github.com/sunnycyc/plpdp4beat</a></li>
<li>paper_authors: Ching-Yu Chiu, Meinard Müller, Matthew E. P. Davies, Alvin Wen-Yu Su, Yi-Hsuan Yang</li>
<li>for: 这个论文是为了探讨现代拍谱系统如何模型拍子的征性，以及如何在西 classical 钢琴音乐中提高拍谱的准确性。</li>
<li>methods: 这个论文使用了两个大的西 classical 钢琴音乐数据集， namely Aligned Scores and Performances (ASAP) 数据集和 Chopin 的 Mazurkas (Maz-5) 数据集，并通过实验表明了现有的拍谱系统在地方拍子变化时的缺陷，因此需要新的方法。这个论文提出了一种新的本地征性基于的拍谱系统，即 predominant local pulse-based dynamic programming (PLPDP) 跟踪方法，它可以更好地考虑地方的拍子变化。</li>
<li>results: 相比现有的拍谱系统，PLPDP 方法可以提高 ASAP 数据集中的 F1-score 从 0.473 提高到 0.493，并在 Maz-5 数据集中提高 F1-score 从 0.595 提高到 0.838。<details>
<summary>Abstract</summary>
To model the periodicity of beats, state-of-the-art beat tracking systems use "post-processing trackers" (PPTs) that rely on several empirically determined global assumptions for tempo transition, which work well for music with a steady tempo. For expressive classical music, however, these assumptions can be too rigid. With two large datasets of Western classical piano music, namely the Aligned Scores and Performances (ASAP) dataset and a dataset of Chopin's Mazurkas (Maz-5), we report on experiments showing the failure of existing PPTs to cope with local tempo changes, thus calling for new methods. In this paper, we propose a new local periodicity-based PPT, called predominant local pulse-based dynamic programming (PLPDP) tracking, that allows for more flexible tempo transitions. Specifically, the new PPT incorporates a method called "predominant local pulses" (PLP) in combination with a dynamic programming (DP) component to jointly consider the locally detected periodicity and beat activation strength at each time instant. Accordingly, PLPDP accounts for the local periodicity, rather than relying on a global tempo assumption. Compared to existing PPTs, PLPDP particularly enhances the recall values at the cost of a lower precision, resulting in an overall improvement of F1-score for beat tracking in ASAP (from 0.473 to 0.493) and Maz-5 (from 0.595 to 0.838).
</details>
<details>
<summary>摘要</summary>
<<SYS>>请将以下文本翻译成简化中文。<</SYS>>现代拍谱系统使用“后处理跟踪器”（PPT）来模拟拍征的周期性，这些PPTs基于一些实际所确定的全球假设，以适应音乐中的稳定拍 tempo。然而，对于表达力强的古典音乐来说，这些假设可能是太僵化了。基于西方古典钢琴音乐的两大数据集， namely Aligned Scores and Performances（ASAP）数据集和钢琴 Mazurkas（Maz-5）数据集，我们报告了现有PPTs在本地拍征变化时的失败，从而需要新的方法。在这篇论文中，我们提出了一种新的本地周期性基于的PPT，即主导性本地拍（PLP）基于的动态规划（DP）跟踪器。PLPDP通过同时考虑检测到的本地周期性和拍动强度来联合考虑本地拍征和全球拍征。因此，PLPDP不依赖于全球拍假设，而是根据本地周期性来跟踪拍征。相比现有PPTs，PLPDP尤其提高了ASAP和Maz-5中的回归值（从0.473提高到0.493）和总体的F1分数（从0.595提高到0.838）。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/21/cs.SD_2023_08_21/" data-id="clm0t8e1f00akv7880kb0ch6l" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_08_21" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/21/eess.IV_2023_08_21/" class="article-date">
  <time datetime="2023-08-20T16:00:00.000Z" itemprop="datePublished">2023-08-21</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/21/eess.IV_2023_08_21/">eess.IV - 2023-08-21 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Extraction-of-Text-from-Optic-Nerve-Optical-Coherence-Tomography-Reports"><a href="#Extraction-of-Text-from-Optic-Nerve-Optical-Coherence-Tomography-Reports" class="headerlink" title="Extraction of Text from Optic Nerve Optical Coherence Tomography Reports"></a>Extraction of Text from Optic Nerve Optical Coherence Tomography Reports</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10790">http://arxiv.org/abs/2308.10790</a></li>
<li>repo_url: None</li>
<li>paper_authors: Iyad Majid, Youchen Victor Zhang, Robert Chang, Sophia Y. Wang</li>
<li>for: This study aimed to develop and evaluate rule-based algorithms for extracting text data, including RNFL values and GCC data, from Zeiss Cirrus OCT scan reports.</li>
<li>methods: The study used DICOM files with encapsulated PDF reports, converted them into image files, and used the PaddleOCR Python package for optical character recognition. Rule-based algorithms were designed and optimized for improved performance in extracting RNFL and GCC data.</li>
<li>results: The developed algorithms demonstrated high precision in extracting data from both RNFL and GCC scans, with slightly better precision for the right eye in RNFL extraction and for the left eye in GCC extraction. However, some values presented more challenges in extraction, such as clock hours 5 and 6 for RNFL thickness and signal strength for GCC.Here’s the information in Simplified Chinese text:</li>
<li>for: 这项研究的目的是开发和评估基于规则的算法，以提高从Zeiss Cirrus光子共振成像扫描报告中提取文本数据，包括肾脉层（RNFL）值和其他膝肾细胞计数（GCC）数据。</li>
<li>methods: 该研究使用DICOM文件中嵌入的PDF报告，并将其转换为图像文件，使用Python中的PaddleOCR包进行光学字符识别。研究人员设计了和优化了基于规则的算法，以提高对RNFL和GCC扫描报告中的数据提取精度。</li>
<li>results: 开发的算法在提取RNFL和GCC扫描报告中的数据时显示了高精度。对于右眼，RNFL提取精度较高（OD: 0.9803 vs. OS: 0.9046），对于左眼，GCC提取精度较高（OD: 0.9567 vs. OS: 0.9677）。然而，一些值在提取时存在更大的挑战，如RNFL厚度的时钟小时5和6，以及GCC的信号强度。<details>
<summary>Abstract</summary>
Purpose: The purpose of this study was to develop and evaluate rule-based algorithms to enhance the extraction of text data, including retinal nerve fiber layer (RNFL) values and other ganglion cell count (GCC) data, from Zeiss Cirrus optical coherence tomography (OCT) scan reports. Methods: DICOM files that contained encapsulated PDF reports with RNFL or Ganglion Cell in their document titles were identified from a clinical imaging repository at a single academic ophthalmic center. PDF reports were then converted into image files and processed using the PaddleOCR Python package for optical character recognition. Rule-based algorithms were designed and iteratively optimized for improved performance in extracting RNFL and GCC data. Evaluation of the algorithms was conducted through manual review of a set of RNFL and GCC reports. Results: The developed algorithms demonstrated high precision in extracting data from both RNFL and GCC scans. Precision was slightly better for the right eye in RNFL extraction (OD: 0.9803 vs. OS: 0.9046), and for the left eye in GCC extraction (OD: 0.9567 vs. OS: 0.9677). Some values presented more challenges in extraction, particularly clock hours 5 and 6 for RNFL thickness, and signal strength for GCC. Conclusions: A customized optical character recognition algorithm can identify numeric results from optical coherence scan reports with high precision. Automated processing of PDF reports can greatly reduce the time to extract OCT results on a large scale.
</details>
<details>
<summary>摘要</summary>
目的：本研究的目的是开发和评估基于规则的算法，以提高从Zeiss Cirrus光合成 Tomatoes(OCT)扫描报告中提取文本数据的精度，包括胁肤神经层(RNFL)值和神经细胞计数(GCC)数据。方法：从一所学术眼科中心的临床扫描存储系统中标记为包含DICOM文档的PDF报告，并将PDF报告转换为图像文件，然后使用Python包PaddleOCR进行光学字符识别。基于规则的算法被设计并优化，以提高提取RNFL和GCC数据的精度。评估算法的效果通过手动复审一组RNFL和GCC报告进行评估。结果：开发的算法在RNFL和GCC扫描报告中提取数据的精度很高，OD和OS的精度分别为0.9803和0.9046，以及0.9567和0.9677。但是，某些值在提取中存在更大的挑战，例如RNFL厚度的时钟小时5和6，以及GCC的信号强度。结论：可以使用自定义的光学字符识别算法来从OCT扫描报告中提取数据，并且自动处理PDF报告可以大幅减少大规模提取OCT结果的时间。
</details></li>
</ul>
<hr>
<h2 id="Dense-Error-Map-Estimation-for-MRI-Ultrasound-Registration-in-Brain-Tumor-Surgery-Using-Swin-UNETR"><a href="#Dense-Error-Map-Estimation-for-MRI-Ultrasound-Registration-in-Brain-Tumor-Surgery-Using-Swin-UNETR" class="headerlink" title="Dense Error Map Estimation for MRI-Ultrasound Registration in Brain Tumor Surgery Using Swin UNETR"></a>Dense Error Map Estimation for MRI-Ultrasound Registration in Brain Tumor Surgery Using Swin UNETR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10784">http://arxiv.org/abs/2308.10784</a></li>
<li>repo_url: None</li>
<li>paper_authors: Soorena Salari, Amirhossein Rasoulian, Hassan Rivaz, Yiming Xiao</li>
<li>for: 降低脑肿瘤手术死亡率的早期手术治疗</li>
<li>methods: 使用投射照像（iUS）跟踪脑组织变形，并使用高精度的MRI-iUS匹配技术更新先前的手术计划，以提高手术安全性和效果</li>
<li>results: 提出了一种基于深度学习（DL）的框架，可以自动评估MRI-iUS匹配结果的质量，并在实际临床数据上显示其性能。<details>
<summary>Abstract</summary>
Early surgical treatment of brain tumors is crucial in reducing patient mortality rates. However, brain tissue deformation (called brain shift) occurs during the surgery, rendering pre-operative images invalid. As a cost-effective and portable tool, intra-operative ultrasound (iUS) can track brain shift, and accurate MRI-iUS registration techniques can update pre-surgical plans and facilitate the interpretation of iUS. This can boost surgical safety and outcomes by maximizing tumor removal while avoiding eloquent regions. However, manual assessment of MRI-iUS registration results in real-time is difficult and prone to errors due to the 3D nature of the data. Automatic algorithms that can quantify the quality of inter-modal medical image registration outcomes can be highly beneficial. Therefore, we propose a novel deep-learning (DL) based framework with the Swin UNETR to automatically assess 3D-patch-wise dense error maps for MRI-iUS registration in iUS-guided brain tumor resection and show its performance with real clinical data for the first time.
</details>
<details>
<summary>摘要</summary>
早期手术治疗脑肿的时间点对病人死亡率有重要影响。然而，手术过程中脑组织变形（称为脑Shift）会使先前的图像无效。作为一种cost-effective和可搬式工具，在手术过程中的ultrasound（iUS）可以跟踪脑Shift，并且精准的MRI-iUS注册技术可以更新先前的 планы和促进iUS的解释。这可以提高手术安全性和效果，最大化肿瘤除除而避免感知区域。然而，手动评估MRI-iUS注册结果的实时性具有困难和错误的可能性，因为数据的3D性。自动的算法可以评估多Modal医疗图像注册结果的质量。因此，我们提出了一个基于深度学习（DL）的框架，使用Swin UNITER来自动评估3D-patch-wise稠密错误地图进行MRI-iUS注册的性能，并在实际临床数据上展示其性能。
</details></li>
</ul>
<hr>
<h2 id="Automated-Identification-of-Failure-Cases-in-Organ-at-Risk-Segmentation-Using-Distance-Metrics-A-Study-on-CT-Data"><a href="#Automated-Identification-of-Failure-Cases-in-Organ-at-Risk-Segmentation-Using-Distance-Metrics-A-Study-on-CT-Data" class="headerlink" title="Automated Identification of Failure Cases in Organ at Risk Segmentation Using Distance Metrics: A Study on CT Data"></a>Automated Identification of Failure Cases in Organ at Risk Segmentation Using Distance Metrics: A Study on CT Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10636">http://arxiv.org/abs/2308.10636</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amin Honarmandi Shandiz, Attila Rádics, Rajesh Tamada, Makk Árpád, Karolina Glowacka, Lehel Ferenczi, Sandeep Dutta, Michael Fanariotis</li>
<li>for: 提高自动生成的肿瘤分割精度，以便更好地规划辐射治疗</li>
<li>methods: 使用维度距离和 Hausdorff 距离的组合来自动标识失败案例，从而更快地修复失败案例</li>
<li>results: 通过设置维度距离和 Hausdorff 距离的阈值，能够快速地自动标识失败案例，并且可以对12个不同的失败案例进行视觉评估<details>
<summary>Abstract</summary>
Automated organ at risk (OAR) segmentation is crucial for radiation therapy planning in CT scans, but the generated contours by automated models can be inaccurate, potentially leading to treatment planning issues. The reasons for these inaccuracies could be varied, such as unclear organ boundaries or inaccurate ground truth due to annotation errors. To improve the model's performance, it is necessary to identify these failure cases during the training process and to correct them with some potential post-processing techniques. However, this process can be time-consuming, as traditionally it requires manual inspection of the predicted output. This paper proposes a method to automatically identify failure cases by setting a threshold for the combination of Dice and Hausdorff distances. This approach reduces the time-consuming task of visually inspecting predicted outputs, allowing for faster identification of failure case candidates. The method was evaluated on 20 cases of six different organs in CT images from clinical expert curated datasets. By setting the thresholds for the Dice and Hausdorff distances, the study was able to differentiate between various states of failure cases and evaluate over 12 cases visually. This thresholding approach could be extended to other organs, leading to faster identification of failure cases and thereby improving the quality of radiation therapy planning.
</details>
<details>
<summary>摘要</summary>
自动化器官风险（OAR）分割是辐射疗法规划CT扫描图中的关键，但自动生成的边界可能存在误差，可能导致治疗规划问题。这些误差的原因可能是不清晰的器官边界或者实际数据错误，导致标注错误。为了提高模型的性能，需要在训练过程中识别这些失败案例，并使用一些可能的后处理技术来更正。然而，这个过程可能占用很多时间，因为传统上需要手动检查预测输出。这篇论文提出了一种方法，通过设置Dice和 Hausdorff距离的组合阈值，自动地识别失败案例。这种方法可以减少手动检查预测输出的时间占用，并允许更快地识别失败案例候选者。该方法在20个不同器官的CT图像中进行了20个案例的评估，通过设置阈值，能够分辨出不同类型的失败案例，并评估12个案例。这种阈值设置方法可以扩展到其他器官，从而更快地识别失败案例，提高辐射疗法规划的质量。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Medical-Image-Segmentation-Optimizing-Cross-Entropy-Weights-and-Post-Processing-with-Autoencoders"><a href="#Enhancing-Medical-Image-Segmentation-Optimizing-Cross-Entropy-Weights-and-Post-Processing-with-Autoencoders" class="headerlink" title="Enhancing Medical Image Segmentation: Optimizing Cross-Entropy Weights and Post-Processing with Autoencoders"></a>Enhancing Medical Image Segmentation: Optimizing Cross-Entropy Weights and Post-Processing with Autoencoders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10488">http://arxiv.org/abs/2308.10488</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pranav Singh, Luoyao Chen, Mei Chen, Jinqian Pan, Raviteja Chukkapalli, Shravan Chaudhari, Jacopo Cirrone</li>
<li>for: 本研究旨在提高医学图像分割的精度和效率，特别是在抗体性疾病如皮肤粘液病中分割细胞和炎症的过程中。</li>
<li>methods: 本研究采用深度学习技术，开发了一种适应性能高的医学图像分割方法，并对捷克网络（U-Net）和U-Net++进行了比较。</li>
<li>results: 实验结果表明，本研究的方法在皮肤粘液病图像分割任务上比现有技术高效率12.26%和12.04%。此外，我们还对loss函数权重的优化和三个医学图像分割任务进行了比较。<details>
<summary>Abstract</summary>
The task of medical image segmentation presents unique challenges, necessitating both localized and holistic semantic understanding to accurately delineate areas of interest, such as critical tissues or aberrant features. This complexity is heightened in medical image segmentation due to the high degree of inter-class similarities, intra-class variations, and possible image obfuscation. The segmentation task further diversifies when considering the study of histopathology slides for autoimmune diseases like dermatomyositis. The analysis of cell inflammation and interaction in these cases has been less studied due to constraints in data acquisition pipelines. Despite the progressive strides in medical science, we lack a comprehensive collection of autoimmune diseases. As autoimmune diseases globally escalate in prevalence and exhibit associations with COVID-19, their study becomes increasingly essential. While there is existing research that integrates artificial intelligence in the analysis of various autoimmune diseases, the exploration of dermatomyositis remains relatively underrepresented. In this paper, we present a deep-learning approach tailored for Medical image segmentation. Our proposed method outperforms the current state-of-the-art techniques by an average of 12.26% for U-Net and 12.04% for U-Net++ across the ResNet family of encoders on the dermatomyositis dataset. Furthermore, we probe the importance of optimizing loss function weights and benchmark our methodology on three challenging medical image segmentation tasks
</details>
<details>
<summary>摘要</summary>
医疗图像分割任务具有独特的挑战，需要同时具备本地化和整体 semantics 的理解，以准确地分割关键区域，如病理性组织或异常特征。这种复杂性在医疗图像分割中受到高度的类间相似性、类内变化和可能的图像掩蔽的影响。医疗图像分割任务进一步复杂化，当考虑到研究 histopathology 板块 для自遗护疾病如dermatomyositis时。对于这些病例，分割和检测细胞Inflammation和互动的分析尚未得到了充分的研究，这主要是因为数据获取管道的限制。尽管医学科技在进步的同时，我们缺乏一个完整的自遗护疾病集合。自遗护疾病在全球范围内的发病率不断增长，并与 COVID-19 相关，因此其研究变得越来越重要。虽然现有的研究已经将人工智能 integrate 到了不同的自遗护疾病的分析中，但是dermatomyositis 的研究仍然相对落后。在这篇文章中，我们提出了一种适用于医疗图像分割的深度学习方法。我们的提议方法在 ResNet 家族Encoder 上的 U-Net 和 U-Net++ 中超过了平均提高12.26%和12.04%。此外，我们还评估了优化损失函数的重要性，并在三个困难的医疗图像分割任务上进行了比较。
</details></li>
</ul>
<hr>
<h2 id="Prediction-of-Pneumonia-and-COVID-19-Using-Deep-Neural-Networks"><a href="#Prediction-of-Pneumonia-and-COVID-19-Using-Deep-Neural-Networks" class="headerlink" title="Prediction of Pneumonia and COVID-19 Using Deep Neural Networks"></a>Prediction of Pneumonia and COVID-19 Using Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10368">http://arxiv.org/abs/2308.10368</a></li>
<li>repo_url: None</li>
<li>paper_authors: M. S. Haque, M. S. Taluckder, S. B. Shawkat, M. A. Shahriyar, M. A. Sayed, C. Modak</li>
<li>for: 这个研究旨在探讨医疗图像分析是否可以帮助早期识别感染病毒和细菌所致的肺炎，以便减少其传播。</li>
<li>methods: 这个研究使用机器学习技术来预测肺炎，并评估不同的机器学习模型在肺炎患者的颈部X射线图像上的表现。</li>
<li>results: 研究发现，使用DenseNet121模型可以实现肺炎的准确预测，其准确率为99.58%。这项研究显示了机器学习技术在精确肺炎诊断中的重要性，并提供了这方面的技术传承。<details>
<summary>Abstract</summary>
Pneumonia, caused by bacteria and viruses, is a rapidly spreading viral infection with global implications. Prompt identification of infected individuals is crucial for containing its transmission. This study explores the potential of medical image analysis to address this challenge. We propose machine-learning techniques for predicting Pneumonia from chest X-ray images. Chest X-ray imaging is vital for Pneumonia diagnosis due to its accessibility and cost-effectiveness. However, interpreting X-rays for Pneumonia detection can be complex, as radiographic features can overlap with other respiratory conditions. We evaluate the performance of different machine learning models, including DenseNet121, Inception Resnet-v2, Inception Resnet-v3, Resnet50, and Xception, using chest X-ray images of pneumonia patients. Performance measures and confusion matrices are employed to assess and compare the models. The findings reveal that DenseNet121 outperforms other models, achieving an accuracy rate of 99.58%. This study underscores the significance of machine learning in the accurate detection of Pneumonia, leveraging chest X-ray images. Our study offers insights into the potential of technology to mitigate the spread of pneumonia through precise diagnostics.
</details>
<details>
<summary>摘要</summary>
《肺炎，由病毒和 бактерий引起的，是一种迅速传播的感染病种，具有全球化的意义。》Prompt identification of infected individuals is crucial for containing the transmission of pneumonia. This study explores the potential of medical image analysis to address this challenge. We propose machine-learning techniques for predicting pneumonia from chest X-ray images. Chest X-ray imaging is vital for pneumonia diagnosis due to its accessibility and cost-effectiveness. However, interpreting X-rays for pneumonia detection can be complex, as radiographic features can overlap with other respiratory conditions. We evaluate the performance of different machine learning models, including DenseNet121, Inception Resnet-v2, Inception Resnet-v3, Resnet50, and Xception, using chest X-ray images of pneumonia patients. Performance measures and confusion matrices are employed to assess and compare the models. The findings reveal that DenseNet121 outperforms other models, achieving an accuracy rate of 99.58%. This study underscores the significance of machine learning in the accurate detection of pneumonia, leveraging chest X-ray images. Our study offers insights into the potential of technology to mitigate the spread of pneumonia through precise diagnostics.Here's the text with some notes on the translation:* "肺炎" (pneumonia) is a noun, and it is translated as "肺炎" (pneumonia) in Simplified Chinese.* "由病毒和 бактерий引起" (caused by bacteria and viruses) is a prepositional phrase, and it is translated as "由病毒和 бактерий引起" (caused by bacteria and viruses) in Simplified Chinese.* "是一种迅速传播的感染病种" (a rapidly spreading viral infection) is a sentence, and it is translated as "是一种迅速传播的感染病种" (a rapidly spreading viral infection) in Simplified Chinese.* "Prompt identification of infected individuals is crucial for containing the transmission of pneumonia" is a sentence, and it is translated as "Prompt identification of infected individuals is crucial for containing the transmission of pneumonia" in Simplified Chinese.* "This study explores the potential of medical image analysis to address this challenge" is a sentence, and it is translated as "这种研究探讨了医疗图像分析如何解决这一挑战" (this study explores the potential of medical image analysis to address this challenge) in Simplified Chinese.* "We propose machine-learning techniques for predicting pneumonia from chest X-ray images" is a sentence, and it is translated as "我们提议使用机器学习技术预测肺炎从胸部X射图像" (we propose machine-learning techniques for predicting pneumonia from chest X-ray images) in Simplified Chinese.* "Chest X-ray imaging is vital for pneumonia diagnosis due to its accessibility and cost-effectiveness" is a sentence, and it is translated as "胸部X射图像诊断肺炎具有可行性和成本效益" (chest X-ray imaging is vital for pneumonia diagnosis due to its accessibility and cost-effectiveness) in Simplified Chinese.* "However, interpreting X-rays for pneumonia detection can be complex, as radiographic features can overlap with other respiratory conditions" is a sentence, and it is translated as "然而，从X射图像中诊断肺炎可能会具有复杂性，因为肺炎的Radiographic特征可能与其他呼吸道疾病重叠" (however, interpreting X-rays for pneumonia detection can be complex, as radiographic features can overlap with other respiratory conditions) in Simplified Chinese.* "We evaluate the performance of different machine learning models, including DenseNet121, Inception Resnet-v2, Inception Resnet-v3, Resnet50, and Xception, using chest X-ray images of pneumonia patients" is a sentence, and it is translated as "我们使用不同的机器学习模型，包括DenseNet121、Inception Resnet-v2、Inception Resnet-v3、Resnet50和Xception，使用肺炎患者的胸部X射图像进行评估" (we evaluate the performance of different machine learning models, including DenseNet121, Inception Resnet-v2, Inception Resnet-v3, Resnet50, and Xception, using chest X-ray images of pneumonia patients) in Simplified Chinese.* "Performance measures and confusion matrices are employed to assess and compare the models" is a sentence, and it is translated as "我们使用性能指标和混淆矩阵来评估和比较不同模型的表现" (performance measures and confusion matrices are employed to assess and compare the models) in Simplified Chinese.* "The findings reveal that DenseNet121 outperforms other models, achieving an accuracy rate of 99.58%" is a sentence, and it is translated as "发现结果表明，DenseNet121模型在识别肺炎方面的准确率为99.58%" (the findings reveal that DenseNet121 outperforms other models, achieving an accuracy rate of 99.58%) in Simplified Chinese.* "This study underscores the significance of machine learning in the accurate detection of pneumonia, leveraging chest X-ray images" is a sentence, and it is translated as "这种研究强调了机器学习在肺炎检测中的重要性，利用胸部X射图像" (this study underscores the significance of machine learning in the accurate detection of pneumonia, leveraging chest X-ray images) in Simplified Chinese.* "Our study offers insights into the potential of technology to mitigate the spread of pneumonia through precise diagnostics" is a sentence, and it is translated as "我们的研究提供了有关技术在防止肺炎传播的精准诊断方面的洞察" (our study offers insights into the potential of technology to mitigate the spread of pneumonia through precise diagnostics) in Simplified Chinese.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/21/eess.IV_2023_08_21/" data-id="clm0t8e2w00fiv7882x3vgk1e" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_20" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/20/cs.LG_2023_08_20/" class="article-date">
  <time datetime="2023-08-19T16:00:00.000Z" itemprop="datePublished">2023-08-20</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/20/cs.LG_2023_08_20/">cs.LG - 2023-08-20 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Preserving-Specificity-in-Federated-Graph-Learning-for-fMRI-based-Neurological-Disorder-Identification"><a href="#Preserving-Specificity-in-Federated-Graph-Learning-for-fMRI-based-Neurological-Disorder-Identification" class="headerlink" title="Preserving Specificity in Federated Graph Learning for fMRI-based Neurological Disorder Identification"></a>Preserving Specificity in Federated Graph Learning for fMRI-based Neurological Disorder Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10302">http://arxiv.org/abs/2308.10302</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junhao Zhang, Qianqian Wang, Xiaochuan Wang, Lishan Qiao, Mingxia Liu</li>
<li>for: 这个研究旨在应用Resting-state functional magnetic resonance imaging (rs-fMRI)和 Federated Learning (FL)技术来探索脑疾病的不均衡连接性，并且考虑到体域特点如年龄、性别和教育水准。</li>
<li>methods: 本研究使用了Graph Neural Network (GNN)和 Federated Learning (FL)技术，并且提出了特定性敏感 Federated Graph Learning (SFGL)框架，以探索不同体域特点下的脑疾病特征。</li>
<li>results: 实验结果显示，SFGL方法在两个rs-fMRI数据集上的调整后，较前方的方法提高了10%至20%的准确率。<details>
<summary>Abstract</summary>
Resting-state functional magnetic resonance imaging (rs-fMRI) offers a non-invasive approach to examining abnormal brain connectivity associated with brain disorders. Graph neural network (GNN) gains popularity in fMRI representation learning and brain disorder analysis with powerful graph representation capabilities. Training a general GNN often necessitates a large-scale dataset from multiple imaging centers/sites, but centralizing multi-site data generally faces inherent challenges related to data privacy, security, and storage burden. Federated Learning (FL) enables collaborative model training without centralized multi-site fMRI data. Unfortunately, previous FL approaches for fMRI analysis often ignore site-specificity, including demographic factors such as age, gender, and education level. To this end, we propose a specificity-aware federated graph learning (SFGL) framework for rs-fMRI analysis and automated brain disorder identification, with a server and multiple clients/sites for federated model aggregation and prediction. At each client, our model consists of a shared and a personalized branch, where parameters of the shared branch are sent to the server while those of the personalized branch remain local. This can facilitate knowledge sharing among sites and also helps preserve site specificity. In the shared branch, we employ a spatio-temporal attention graph isomorphism network to learn dynamic fMRI representations. In the personalized branch, we integrate vectorized demographic information (i.e., age, gender, and education years) and functional connectivity networks to preserve site-specific characteristics. Representations generated by the two branches are then fused for classification. Experimental results on two fMRI datasets with a total of 1,218 subjects suggest that SFGL outperforms several state-of-the-art approaches.
</details>
<details>
<summary>摘要</summary>
RESTING-STATE ФУНКЦИОНАЛНО МАГНЕТНО РЕЗОНАНСНА ИМАЖИНГ (RS-fMRI) ПРОВОДИ ГНИЛОСТИВЫЙ ПОДХОД К ИЗУЧЕНИЮ АБНОРМАЛЬНОЙ БРАИНОВОЙ СОЕДИНЕНИЯ ПРОВОДЗЕМ СО БРАИНОВЫМИ РАЗЛИЧИЯМИ. ГРАФОВАЯ НЕРВНАЯ СИСТЕМА (GNN) ЗНАЧАЕТСЯ В ФМРИ ЗАПИСИВАНИЕ И БРАИНОВОЙ РАЗЛИЧИЕВАНИИ СО ВСЕМОГОВОРТНОЙ ГРАФОЙОЙ РЕПРЕСЕНТАЦИЕЙ. ОБУЧЕНИЕ ГЕНЕРАЛЬНОЙ GNN НОВОЕ РЕЗУЛЬТАТО ВОЗМОЖНО СОВЕРШИТЬ БЕЗ ЦЕНТРАЛИЗОВАННОГО МНОГОСИТЕЙНОГО ДАННЫХ, НО ОБЫЧЕСКИЕ ПРОТОКОЛЫ ФЛЕARNING (FL) ДЛЯ ФМРИАНАЛИЗА СНИЖАЮТ ОБЪЕМ ДАННЫХ ПО КОРАБОРАТИВНОМУ ОБЪЕМУ. НО В ПРЕДЫДСТВИИ ТОГО, ПРОПОЛОЖЕННЫЕ ФЛЕARNING (SFGL) ПРОВОДИТ К КОМБИНИРОВАНИЮ МЕСТООБРАЗНЫХ КАРАКТЕРИСТИК ИЗУЧЕНИЯ БРАИНОВОГО РАЗЛИЧИЕВАНИА. В НАШЕМ ФРАМВОРКЕ, МАТЕРИАЛЫ СЕРВЕРА И МУЛЬТИПЛОВЫЕ КЛИЕНТЫ/СИТИ ИСКАЗЫВАТЬСЯ В ФЕДЕРАТИВНОМ ОБЪЕМЕ, ГДЕ ВСЕ КЛИЕНТЫ ПРОВОДЗЕМ ИЗ СЕРВЕРА СОБСТВЕННЫЕ И ПЕРСОНАЛИЗОВАННЫЕ БРАНЧИ. В СОБСТВЕННОМ БРАНЧЕ МЫ ЗЕМЛЯЕМ СПАЦИО-ВРЕМЕННЫЙ ВАТЕРНЫЙ ГРАФОВЫЙ ИЗОМОРФИЗМ КАК РЕПРЕСЕНТАЦИЮ ФМРИ. В ПЕРСОНАЛИЗОВАННЫМ БРАНЧЕ МЫ СОБИРАЕММ ВЕКТОРИЗИРОВАННЫЕ ДЕМОГРАФИЧЕСКИЕ ИЗМЕРЕНИЯ (НАПРИМЕР, ВОСРОД СТОРОК, И УЧЕТ ОБУЧЕНИЯ) И ФУНКЦИОНАЛЬНУЮ СОЕДИНЕННУЮ СИТЬ, ЧТОБЫ ПРЕЗЕРВИТЬ МЕСТООБРАЗНЫЕ КАРАКТЕРИСТИКИ. ЗАПИСИВАННЫЕ В ДВУХ БРАНЧАХ ЗАТОМ СОБИРАЕТСЯ ДЛЯ КЛАССИРОВКИ. ЭКСПЕРИМЕНТАЛЬНЫЕ РЕЗУЛЬТАТЫ ПРОВОДИТЫ КТОРЫМУ, ЧТО SFGL ПРОВОДИТ К БЫСТРОМ И ГОРОДОМ ПОКОВКЕ, ПРОТИВОЗАПИСАНИЮ ИЗУЧЕНИЯ БРАИНОВОГО РАЗЛИЧИЕВАНИА.
</details></li>
</ul>
<hr>
<h2 id="An-interpretable-deep-learning-method-for-bearing-fault-diagnosis"><a href="#An-interpretable-deep-learning-method-for-bearing-fault-diagnosis" class="headerlink" title="An interpretable deep learning method for bearing fault diagnosis"></a>An interpretable deep learning method for bearing fault diagnosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10292">http://arxiv.org/abs/2308.10292</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Lu, Austin M. Bray, Chao Hu, Andrew T. Zimmerman, Hongyi Xu</li>
<li>for: 这个研究旨在解决深度学习模型中的黑盒问题，以提高人工维护人员对模型的信任度。</li>
<li>methods: 本研究使用了卷积神经网组合Gradient-weighted Class Activation Mapping（Grad-CAM）来实现深度学习模型的解释和可读性。</li>
<li>results: 研究结果显示，使用Grad-CAM可以从training sample中找到重要的特征对照，并将其annotate为健康库（health library）中的一部分。在评估过程中，提出的方法可以从健康库中选择相似的预测基础样本，以提高模型的可信度。<details>
<summary>Abstract</summary>
Deep learning (DL) has gained popularity in recent years as an effective tool for classifying the current health and predicting the future of industrial equipment. However, most DL models have black-box components with an underlying structure that is too complex to be interpreted and explained to human users. This presents significant challenges when deploying these models for safety-critical maintenance tasks, where non-technical personnel often need to have complete trust in the recommendations these models give. To address these challenges, we utilize a convolutional neural network (CNN) with Gradient-weighted Class Activation Mapping (Grad-CAM) activation map visualizations to form an interpretable DL method for classifying bearing faults. After the model training process, we apply Grad-CAM to identify a training sample's feature importance and to form a library of diagnosis knowledge (or health library) containing training samples with annotated feature maps. During the model evaluation process, the proposed approach retrieves prediction basis samples from the health library according to the similarity of the feature importance. The proposed method can be easily applied to any CNN model without modifying the model architecture, and our experimental results show that this method can select prediction basis samples that are intuitively and physically meaningful, improving the model's trustworthiness for human users.
</details>
<details>
<summary>摘要</summary>
深度学习（DL）在最近几年内得到了广泛应用，用于现代工业设备的类型和未来预测。然而，大多数DL模型具有黑盒子组件，其下面结构太复杂，无法被人类用户理解和解释。这些问题在安全维护任务中具有重要性，因为非技术人员经常需要对这些模型的建议产生完全的信任。为解决这些问题，我们使用卷积神经网络（CNN）和梯度权重分类图像（Grad-CAM）活动图像可视化来形成可解释的DL方法，用于分类承载问题。在模型训练过程中，我们使用Grad-CAM来标识训练样本的特征重要性，并将其作为健康图书馆（health library）中的训练样本，并将其标注为特征图像。在评估模型过程中，我们的方法可以从健康图书馆中检索预测基础样本，根据特征重要性的相似性。我们的方法可以轻松地应用于任何CNN模型，无需修改模型结构，我们的实验结果表明，这种方法可以选择Physically meaningful和直观的预测基础样本，提高模型的人类用户的信任度。
</details></li>
</ul>
<hr>
<h2 id="Towards-Few-shot-Coordination-Revisiting-Ad-hoc-Teamplay-Challenge-In-the-Game-of-Hanabi"><a href="#Towards-Few-shot-Coordination-Revisiting-Ad-hoc-Teamplay-Challenge-In-the-Game-of-Hanabi" class="headerlink" title="Towards Few-shot Coordination: Revisiting Ad-hoc Teamplay Challenge In the Game of Hanabi"></a>Towards Few-shot Coordination: Revisiting Ad-hoc Teamplay Challenge In the Game of Hanabi</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10284">http://arxiv.org/abs/2308.10284</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hadi Nekoei, Xutong Zhao, Janarthanan Rajendran, Miao Liu, Sarath Chandar</li>
<li>for: 这个论文的目的是研究 Cooperative Multi-agent Reinforcement Learning（MARL）算法中的零批合作（ZSC）能力，以及如何提高这种能力以适应复杂的任务和变化的环境。</li>
<li>methods: 该论文使用了一种基于 Hanabi 游戏的实验框架，并定义了一个新的 metric called adaptation regret，用于衡量 MARL 算法的适应能力。</li>
<li>results: 实验结果显示，当将 ZSC 算法与不同的学习方法训练的 Agent 结合时，state-of-the-art ZSC 算法的性能很差，并且需要 millions of interaction samples 来适应这些新的合作伙伴。此外，研究发现，通过调整不同的 hyper-parameter 和设计选择，可以提高 Hanabi 算法的适应能力。<details>
<summary>Abstract</summary>
Cooperative Multi-agent Reinforcement Learning (MARL) algorithms with Zero-Shot Coordination (ZSC) have gained significant attention in recent years. ZSC refers to the ability of agents to coordinate zero-shot (without additional interaction experience) with independently trained agents. While ZSC is crucial for cooperative MARL agents, it might not be possible for complex tasks and changing environments. Agents also need to adapt and improve their performance with minimal interaction with other agents. In this work, we show empirically that state-of-the-art ZSC algorithms have poor performance when paired with agents trained with different learning methods, and they require millions of interaction samples to adapt to these new partners. To investigate this issue, we formally defined a framework based on a popular cooperative multi-agent game called Hanabi to evaluate the adaptability of MARL methods. In particular, we created a diverse set of pre-trained agents and defined a new metric called adaptation regret that measures the agent's ability to efficiently adapt and improve its coordination performance when paired with some held-out pool of partners on top of its ZSC performance. After evaluating several SOTA algorithms using our framework, our experiments reveal that naive Independent Q-Learning (IQL) agents in most cases adapt as quickly as the SOTA ZSC algorithm Off-Belief Learning (OBL). This finding raises an interesting research question: How to design MARL algorithms with high ZSC performance and capability of fast adaptation to unseen partners. As a first step, we studied the role of different hyper-parameters and design choices on the adaptability of current MARL algorithms. Our experiments show that two categories of hyper-parameters controlling the training data diversity and optimization process have a significant impact on the adaptability of Hanabi agents.
</details>
<details>
<summary>摘要</summary>
合作多智能 reinforcement learning（MARL）算法 WITH Zero-Shot Coordination（ZSC）在 recent 年份中受到了关注。 ZSC 指的是无需额外交互经验的 AGENTS 之间的协调。 虽然 ZSC 对协作 MARL 代理人来说非常重要，但在复杂任务和变化环境下可能无法实现。 AGENTS 也需要通过最小化交互amples 来改进其性能。 在这项工作中，我们通过实验表明，当将最新的 ZSC 算法与另外的学习方法训练的 AGENTS 结合时，其性能很差。 为了解释这个问题，我们形式地定义了一个基于流行的合作多智能游戏 Hanabi 的框架，用于评估 MARL 方法的适应性。 具体来说，我们创建了一组多样化的预训练 AGENTS，并定义了一个新的指标called adaptation regret，用于衡量 AGENTS 在与其他另外的合作伙伴交互时的快速适应和改进协调性能。 经过我们的实验，我们发现，在 Hanabi 游戏中，简单的独立 Q-学习（IQL） AGENTS 在大多数情况下可以与 SOTA ZSC 算法 Off-Belief Learning（OBL）相当快地适应。 这一发现提出了一个有趣的研究问题：如何设计 MARL 算法，具有高度的 ZSC 性能和适应不visible 合作伙伴的能力。 作为一个第一步，我们研究了现有 MARL 算法中的不同超参数和设计选择对 Hanabi 代理人的适应性有多大的影响。 我们的实验表明，控制训练数据多样性和优化过程的两类超参数具有重要的影响。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Uncertainty-Guided-Model-Selection-for-Data-Driven-PDE-Discovery"><a href="#Adaptive-Uncertainty-Guided-Model-Selection-for-Data-Driven-PDE-Discovery" class="headerlink" title="Adaptive Uncertainty-Guided Model Selection for Data-Driven PDE Discovery"></a>Adaptive Uncertainty-Guided Model Selection for Data-Driven PDE Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10283">http://arxiv.org/abs/2308.10283</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pongpisit-thanasutives/ubic">https://github.com/pongpisit-thanasutives/ubic</a></li>
<li>paper_authors: Pongpisit Thanasutives, Takashi Morita, Masayuki Numao, Ken-ichi Fukui</li>
<li>For: 本研究提出了一种新的参数 adaptive uncertainty-penalized Bayesian information criterion (UBIC), 用于优先选择含有噪声的空间-时间观察数据的简洁参数化 differential equation (PDE)。* Methods: 本研究使用了一种physics-informed neural network learning的数据驱动方法来验证选择的 PDE 是否具有足够的准确性。* Results: 实验结果表明，UBIC 可以成功地选择真实的管理 PDE，并且发现了对噪声数据的适当处理可以提高模型选择的trade-off。Here’s the same information in English:* For: This study proposes a new parameter-adaptive uncertainty-penalized Bayesian information criterion (UBIC) to prioritize the parsimonious partial differential equation (PDE) that sufficiently governs noisy spatial-temporal observed data with few reliable terms.* Methods: The study uses a physics-informed neural network learning approach to validate the selected PDE flexibly against the other discovered PDE.* Results: Experimental results show that UBIC can successfully select the true governing PDE, and reveal an interesting effect of denoising the observed data on improving the trade-off between the BIC score and model complexity.<details>
<summary>Abstract</summary>
We propose a new parameter-adaptive uncertainty-penalized Bayesian information criterion (UBIC) to prioritize the parsimonious partial differential equation (PDE) that sufficiently governs noisy spatial-temporal observed data with few reliable terms. Since the naive use of the BIC for model selection has been known to yield an undesirable overfitted PDE, the UBIC penalizes the found PDE not only by its complexity but also the quantified uncertainty, derived from the model supports' coefficient of variation in a probabilistic view. We also introduce physics-informed neural network learning as a simulation-based approach to further validate the selected PDE flexibly against the other discovered PDE. Numerical results affirm the successful application of the UBIC in identifying the true governing PDE. Additionally, we reveal an interesting effect of denoising the observed data on improving the trade-off between the BIC score and model complexity. Code is available at https://github.com/Pongpisit-Thanasutives/UBIC.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的参数适应不确定性加权 bayesian信息条件（UBIC），用于优先选择含有噪声的空间-时间观测数据中的简洁partial differential equation（PDE）。由于直接使用BIC来进行模型选择可能会导致不想要的过拟合PDE，因此UBIC不仅对发现的PDE进行复杂度惩罚，还对其进行量化的不确定性惩罚，从概率视角来看。我们还引入了物理学习神经网络，作为一种基于实验的方法，以验证选择的PDE的可行性。numerical results表明，UBIC成功地实现了选择真实的导导PDE。此外，我们发现了对观测数据进行降噪有助于改善模型复杂度和BIC分数之间的交互效应。代码可以在https://github.com/Pongpisit-Thanasutives/UBIC上获取。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Spatiotemporal-Traffic-Prediction-through-Urban-Human-Activity-Analysis"><a href="#Enhancing-Spatiotemporal-Traffic-Prediction-through-Urban-Human-Activity-Analysis" class="headerlink" title="Enhancing Spatiotemporal Traffic Prediction through Urban Human Activity Analysis"></a>Enhancing Spatiotemporal Traffic Prediction through Urban Human Activity Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10282">http://arxiv.org/abs/2308.10282</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/suminhan/traffic-uagcrntf">https://github.com/suminhan/traffic-uagcrntf</a></li>
<li>paper_authors: Sumin Han, Youngjun Park, Minji Lee, Jisun An, Dongman Lee</li>
<li>for: 提高城市交通预测精度，帮助确保公民的安全和便利。</li>
<li>methods: 基于图 convolution deep learning 算法，利用人员活动频率数据从国家家庭旅行调查来增强推理 causal 关系 между活动和交通模式。</li>
<li>results: 对比传统的深度学习模型，该方法实现了状态之 искусственный智能模型，无需增加计算负担。<details>
<summary>Abstract</summary>
Traffic prediction is one of the key elements to ensure the safety and convenience of citizens. Existing traffic prediction models primarily focus on deep learning architectures to capture spatial and temporal correlation. They often overlook the underlying nature of traffic. Specifically, the sensor networks in most traffic datasets do not accurately represent the actual road network exploited by vehicles, failing to provide insights into the traffic patterns in urban activities. To overcome these limitations, we propose an improved traffic prediction method based on graph convolution deep learning algorithms. We leverage human activity frequency data from National Household Travel Survey to enhance the inference capability of a causal relationship between activity and traffic patterns. Despite making minimal modifications to the conventional graph convolutional recurrent networks and graph convolutional transformer architectures, our approach achieves state-of-the-art performance without introducing excessive computational overhead.
</details>
<details>
<summary>摘要</summary>
traffic prediction 是一个关键的元素，以确保公民的安全和便利。现有的交通预测模型主要采用深度学习架构，以捕捉空间和时间相关性。它们经常忽略交通的本质。具体来说，交通数据集中的感知网络不准确地表示实际行驶的道路网络，失去了对交通模式的探索。为了解决这些限制，我们提出了基于图конvolution深度学习算法的改进交通预测方法。我们利用国家家庭旅行调查中的人类活动频率数据，以提高 causal 关系 между活动和交通模式的推理能力。虽然我们对传统的图 convolutional recurrent networks 和图 convolutional transformer 架构进行了最小的修改，但我们的方法可以在计算开销不增加的情况下达到状态的最佳性能。
</details></li>
</ul>
<hr>
<h2 id="The-DKU-DUKEECE-System-for-the-Manipulation-Region-Location-Task-of-ADD-2023"><a href="#The-DKU-DUKEECE-System-for-the-Manipulation-Region-Location-Task-of-ADD-2023" class="headerlink" title="The DKU-DUKEECE System for the Manipulation Region Location Task of ADD 2023"></a>The DKU-DUKEECE System for the Manipulation Region Location Task of ADD 2023</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10281">http://arxiv.org/abs/2308.10281</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zexin Cai, Weiqing Wang, Yikang Wang, Ming Li</li>
<li>for: 本文描述了我们在 Audio Deepfake Detection Challenge (ADD 2023) 中的 Track 2 系统，它的目的是识别受修改的音频段。</li>
<li>methods: 我们的方法包括使用多个检测系统来确定受修改的区域和确定其真实性。我们训练并结合了两个帧级系统：一个用于边界检测，另一个用于深伪检测。此外，我们还使用了专门使用真实数据训练的 VAE 模型来确定音频剪辑的真实性。</li>
<li>results: 通过将这三个系统融合，我们的topperforming解决方案在 ADD 挑战中取得了82.23% 的句子准确率和60.66%的 F1 分数，最终的 ADD 分数为0.6713，在 Track 2 中获得了第一名。<details>
<summary>Abstract</summary>
This paper introduces our system designed for Track 2, which focuses on locating manipulated regions, in the second Audio Deepfake Detection Challenge (ADD 2023). Our approach involves the utilization of multiple detection systems to identify splicing regions and determine their authenticity. Specifically, we train and integrate two frame-level systems: one for boundary detection and the other for deepfake detection. Additionally, we employ a third VAE model trained exclusively on genuine data to determine the authenticity of a given audio clip. Through the fusion of these three systems, our top-performing solution for the ADD challenge achieves an impressive 82.23% sentence accuracy and an F1 score of 60.66%. This results in a final ADD score of 0.6713, securing the first rank in Track 2 of ADD 2023.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了我们为Track 2设计的系统，专注于寻找操作区域（splicing regions），在第二届音频深圳检测比赛（ADD 2023）中获得第一名。我们的方法包括多个检测系统的结合，以确定声音片断的真实性。具体来说，我们训练并集成了两个帧级别系统：一个用于边界检测，另一个用于深圳检测。此外，我们还使用专门用于真实数据训练的VAE模型，以确定声音片断的真实性。通过这三种系统的融合，我们在ADD挑战中获得了82.23%的句子准确率和60.66%的F1分数，最终得分为0.6713，在Track 2中排名第一。
</details></li>
</ul>
<hr>
<h2 id="GPFL-Simultaneously-Learning-Global-and-Personalized-Feature-Information-for-Personalized-Federated-Learning"><a href="#GPFL-Simultaneously-Learning-Global-and-Personalized-Feature-Information-for-Personalized-Federated-Learning" class="headerlink" title="GPFL: Simultaneously Learning Global and Personalized Feature Information for Personalized Federated Learning"></a>GPFL: Simultaneously Learning Global and Personalized Feature Information for Personalized Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10279">http://arxiv.org/abs/2308.10279</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianqing Zhang, Yang Hua, Hao Wang, Tao Song, Zhengui Xue, Ruhui Ma, Jian Cao, Haibing Guan</li>
<li>for: 这个论文主要是为了解决 Federated Learning (FL) 中的个人化特征提取问题，提出了一种新的个人化 Federated Learning (pFL) 方法，以实现在多客户端上同时学习全局和个人特征信息。</li>
<li>methods: 该方法使用了一种新的特征提取技术，可以同时学习全局和个人特征信息，并在多客户端上进行协同学习。</li>
<li>results: 在六个数据集上进行了三种统计上不同的实验，证明了 GPFL 在效果、可扩展性、公平性、稳定性和隐私方面比十种现有方法更优。此外，GPFL 还可以避免过拟合并超过基eline的提升。<details>
<summary>Abstract</summary>
Federated Learning (FL) is popular for its privacy-preserving and collaborative learning capabilities. Recently, personalized FL (pFL) has received attention for its ability to address statistical heterogeneity and achieve personalization in FL. However, from the perspective of feature extraction, most existing pFL methods only focus on extracting global or personalized feature information during local training, which fails to meet the collaborative learning and personalization goals of pFL. To address this, we propose a new pFL method, named GPFL, to simultaneously learn global and personalized feature information on each client. We conduct extensive experiments on six datasets in three statistically heterogeneous settings and show the superiority of GPFL over ten state-of-the-art methods regarding effectiveness, scalability, fairness, stability, and privacy. Besides, GPFL mitigates overfitting and outperforms the baselines by up to 8.99% in accuracy.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 是由于其隐私保护和合作学习能力而受欢迎的。最近，个性化 federated learning (pFL) 已经受到关注，因为它可以 Address statistical heterogeneity 和实现个性化。然而，从特征提取的角度来看，大多数现有的 pFL 方法只是在本地训练中提取全局或个性化特征信息，这不符合 pFL 的协作学习和个性化目标。为解决这个问题，我们提出了一种新的 pFL 方法，名为 GPFL，它可以在每个客户端同时学习全局和个性化特征信息。我们在六个数据集上进行了三种 statistically heterogeneous 的实验，并证明 GPFL 在效果、可扩展性、公平性、稳定性和隐私方面超过了十个现有方法。此外，GPFL 可以 Mitigate overfitting 并超过基eline 的性能。
</details></li>
</ul>
<hr>
<h2 id="Minimalist-Traffic-Prediction-Linear-Layer-Is-All-You-Need"><a href="#Minimalist-Traffic-Prediction-Linear-Layer-Is-All-You-Need" class="headerlink" title="Minimalist Traffic Prediction: Linear Layer Is All You Need"></a>Minimalist Traffic Prediction: Linear Layer Is All You Need</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10276">http://arxiv.org/abs/2308.10276</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wenyingduan/STLinear">https://github.com/wenyingduan/STLinear</a></li>
<li>paper_authors: Wenying Duan, Hong Rao, Wei Huang, Xiaoxi He</li>
<li>for: 这篇论文是为了解决智能交通系统（ITS）和智能城市的发展中的交通预测问题而写的。</li>
<li>methods: 本论文提出了三个解决方案：节点嵌入方法、时间序列分解和周期学习。它还介绍了一种名为 STLinear 的简单化模型架构，它在计算复杂性和计算负担方面具有明显的优势。</li>
<li>results: 实验表明，STLinear 能够与其他领先的 STGNN 模型匹配或超越其精度，但具有明显的计算复杂性和计算负担减少（相比于2023年的状态艺术 STGNN 基eline，MACs每个epoch减少了超过95%）。<details>
<summary>Abstract</summary>
Traffic prediction is essential for the progression of Intelligent Transportation Systems (ITS) and the vision of smart cities. While Spatial-Temporal Graph Neural Networks (STGNNs) have shown promise in this domain by leveraging Graph Neural Networks (GNNs) integrated with either RNNs or Transformers, they present challenges such as computational complexity, gradient issues, and resource-intensiveness. This paper addresses these challenges, advocating for three main solutions: a node-embedding approach, time series decomposition, and periodicity learning. We introduce STLinear, a minimalist model architecture designed for optimized efficiency and performance. Unlike traditional STGNNs, STlinear operates fully locally, avoiding inter-node data exchanges, and relies exclusively on linear layers, drastically cutting computational demands. Our empirical studies on real-world datasets confirm STLinear's prowess, matching or exceeding the accuracy of leading STGNNs, but with significantly reduced complexity and computation overhead (more than 95% reduction in MACs per epoch compared to state-of-the-art STGNN baseline published in 2023). In summary, STLinear emerges as a potent, efficient alternative to conventional STGNNs, with profound implications for the future of ITS and smart city initiatives.
</details>
<details>
<summary>摘要</summary>
traffic 预测是智能交通系统（ITS）的核心和智能城市的视野。而 spatial-temporal graph neural networks（STGNNs）在这个领域表现了承诺，通过结合图神经网络（GNNs）和 either RNNs 或 Transformers 来预测交通流。然而，STGNNs 还存在一些挑战，如计算复杂性、梯度问题和资源占用性。这篇文章解决了这些挑战，提出三个主要解决方案：节点嵌入方法、时间序列分解和周期学习。我们介绍了 STLinear，一种最佳化的模型建立，与传统的 STGNNs 不同，STLinear 完全地地方处理，不需要 между节点数据交换，并且仅仅使用线性层，减少了计算需求。我们对实际数据集进行了实验，确认 STLinear 的强大性，与状态之前的 STGNNs 准确性相当或超过，但计算负担减少了超过 95%。总之，STLinear  emerges 为智能交通系统和智能城市initiatives的强大、高效的代替方案。
</details></li>
</ul>
<hr>
<h2 id="SBSM-Pro-Support-Bio-sequence-Machine-for-Proteins"><a href="#SBSM-Pro-Support-Bio-sequence-Machine-for-Proteins" class="headerlink" title="SBSM-Pro: Support Bio-sequence Machine for Proteins"></a>SBSM-Pro: Support Bio-sequence Machine for Proteins</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10275">http://arxiv.org/abs/2308.10275</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wyzbio/support-bio-sequence-machine">https://github.com/wyzbio/support-bio-sequence-machine</a></li>
<li>paper_authors: Yizheng Wang, Yixiao Zhai, Yijie Ding, Quan Zou</li>
<li>for: 本研究旨在提出一种特制 для生物序列分类的支持机器学习模型（SBSM-Pro），帮助指导生物实验和应用。</li>
<li>methods: 该模型从原始序列开始，根据蛋白质物理化学性质进行氨基酸分组，并使用序列对 alignment 测量蛋白质之间的相似性。它采用了一种新的 MKL 方法，将不同类型的信息集成，使用支持向量机器学习进行分类预测。</li>
<li>results: 研究结果表明，SBSM-Pro 在 10 个数据集中表现出色，在蛋白质功能预测和后转录修饰方面进行了正确的识别。这项研究不仅代表了生物序列分类领域的国际前沿，还开创了新的方向，为生物序列分类平台的开发做出了重要贡献。<details>
<summary>Abstract</summary>
Proteins play a pivotal role in biological systems. The use of machine learning algorithms for protein classification can assist and even guide biological experiments, offering crucial insights for biotechnological applications. We propose a support bio-sequence machine for proteins, a model specifically designed for biological sequence classification. This model starts with raw sequences and groups amino acids based on their physicochemical properties. It incorporates sequence alignment to measure the similarities between proteins and uses a novel MKL approach to integrate various types of information, utilizing support vector machines for classification prediction. The results indicate that our model demonstrates commendable performance across 10 datasets in terms of the identification of protein function and posttranslational modification. This research not only showcases state-of-the-art work in protein classification but also paves the way for new directions in this domain, representing a beneficial endeavour in the development of platforms tailored for biological sequence classification. SBSM-Pro is available for access at http://lab.malab.cn/soft/SBSM-Pro/.
</details>
<details>
<summary>摘要</summary>
生物系统中，蛋白质扮演着关键角色。使用机器学习算法进行蛋白质分类可以帮助和指导生物实验，提供生物技术应用中关键的发现。我们提出了一种专门为蛋白质分类设计的生物序列机器学习模型（SBSM-Pro）。这个模型从原始序列开始，根据蛋白质物理化学性质分组氨基酸。它利用序列对alignment测量蛋白质之间的相似性，并采用一种新的MKL方法集成不同类型的信息，使用支持向量机进行分类预测。结果表明，我们的模型在10个数据集中表现出色地预测蛋白质功能和后转化 modify。这项研究不仅代表了蛋白质分类领域的 estado-of-the-art，还开拓了新的发展方向，代表了一项有利的生物序列分类平台开发的努力。SBSM-Pro可以在http://lab.malab.cn/soft/SBSM-Pro/上下载。
</details></li>
</ul>
<hr>
<h2 id="An-alternative-to-SVM-Method-for-Data-Classification"><a href="#An-alternative-to-SVM-Method-for-Data-Classification" class="headerlink" title="An alternative to SVM Method for Data Classification"></a>An alternative to SVM Method for Data Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11579">http://arxiv.org/abs/2308.11579</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/himanshub1007/Alzhimers-Disease-Prediction-Using-Deep-learning">https://github.com/himanshub1007/Alzhimers-Disease-Prediction-Using-Deep-learning</a></li>
<li>paper_authors: Lakhdar Remaki</li>
<li>for: 这篇论文是为了提出一种新的分类方法，以解决支持向量机（SVM）的一些缺点。</li>
<li>methods: 该方法使用最小距离到优化的子空间，以确定分类结果。</li>
<li>results: 研究发现，新方法与支持向量机（SVM）的性能相似，但具有轻量级的优势，如减少计算时间、避免优化过程失败、扩展到多类分类、处理不均衡类型和动态分类等问题。<details>
<summary>Abstract</summary>
Support vector machine (SVM), is a popular kernel method for data classification that demonstrated its efficiency for a large range of practical applications. The method suffers, however, from some weaknesses including; time processing, risk of failure of the optimization process for high dimension cases, generalization to multi-classes, unbalanced classes, and dynamic classification. In this paper an alternative method is proposed having a similar performance, with a sensitive improvement of the aforementioned shortcomings. The new method is based on a minimum distance to optimal subspaces containing the mapped original classes.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Turning-Waste-into-Wealth-Leveraging-Low-Quality-Samples-for-Enhancing-Continuous-Conditional-Generative-Adversarial-Networks"><a href="#Turning-Waste-into-Wealth-Leveraging-Low-Quality-Samples-for-Enhancing-Continuous-Conditional-Generative-Adversarial-Networks" class="headerlink" title="Turning Waste into Wealth: Leveraging Low-Quality Samples for Enhancing Continuous Conditional Generative Adversarial Networks"></a>Turning Waste into Wealth: Leveraging Low-Quality Samples for Enhancing Continuous Conditional Generative Adversarial Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10273">http://arxiv.org/abs/2308.10273</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xin Ding, Yongwei Wang, Zuheng Xu<br>for: 这个论文旨在提高Continuous Conditional Generative Adversarial Networks（CcGANs）中的生成模型，使其能够基于连续数值变量（称为回归标签）进行生成。methods: 这个论文提出了一种新的Negative Data Augmentation（NDA）方法，称为Dual-NDA，以解决CcGANs中的问题，即生成低质量的假图像。Dual-NDA使用了两种类型的负样本：来自预训练CcGAN的视觉不真实图像和 manipulate 真实图像的标签。results: 实验表明，Dual-NDA可以提高CcGANs中生成图像的视觉准确性和标签一致性，而且可以超越当前的状态艺术Conditional GANs和扩散模型，达到新的高水平性能。<details>
<summary>Abstract</summary>
Continuous Conditional Generative Adversarial Networks (CcGANs) enable generative modeling conditional on continuous scalar variables (termed regression labels). However, they can produce subpar fake images due to limited training data. Although Negative Data Augmentation (NDA) effectively enhances unconditional and class-conditional GANs by introducing anomalies into real training images, guiding the GANs away from low-quality outputs, its impact on CcGANs is limited, as it fails to replicate negative samples that may occur during the CcGAN sampling. We present a novel NDA approach called Dual-NDA specifically tailored for CcGANs to address this problem. Dual-NDA employs two types of negative samples: visually unrealistic images generated from a pre-trained CcGAN and label-inconsistent images created by manipulating real images' labels. Leveraging these negative samples, we introduce a novel discriminator objective alongside a modified CcGAN training algorithm. Empirical analysis on UTKFace and Steering Angle reveals that Dual-NDA consistently enhances the visual fidelity and label consistency of fake images generated by CcGANs, exhibiting a substantial performance gain over the vanilla NDA. Moreover, by applying Dual-NDA, CcGANs demonstrate a remarkable advancement beyond the capabilities of state-of-the-art conditional GANs and diffusion models, establishing a new pinnacle of performance.
</details>
<details>
<summary>摘要</summary>
Dual-NDA 使用了两种负样本：由预训练 CcGAN 生成的视觉不可能的图像，以及 manipulate 真实图像的标签以创造的 label-inconsistent 图像。我们利用这些负样本，引入了一种新的识别器目标 alongside 修改后 CcGAN 训练算法。我们的实验表明，使用 Dual-NDA，CcGANs 能够在 UTKFace 和 Steering Angle 上提高假图像的视觉准确性和标签一致性，并且表现出了明显的性能提升。此外，通过应用 Dual-NDA，CcGANs 能够超越现有的 conditional GANs 和扩散模型，达到新的高点性能。
</details></li>
</ul>
<hr>
<h2 id="Large-Transformers-are-Better-EEG-Learners"><a href="#Large-Transformers-are-Better-EEG-Learners" class="headerlink" title="Large Transformers are Better EEG Learners"></a>Large Transformers are Better EEG Learners</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11654">http://arxiv.org/abs/2308.11654</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bingxin Wang, Xiaowen Fu, Yuan Lan, Luchan Zhang, Yang Xiang</li>
<li>for: 这个论文主要针对的是如何将类型变数的电enzephalogram（EEG）资料转换为图像或文本格式，以便使用预训Transformer模型进行预测。</li>
<li>methods: 作者提出了一个名为AdaCE的专案，用于将EEG资料转换为图像或文本格式，并将这些格式与预训Transformer模型进行混合，以便进行预测。</li>
<li>results: 作者的实验结果显示，使用AdaCE模组可以将预训Transformer模型直接 fine-tune 为EEG预测任务，并 achieve state-of-the-art 性能在多种EEG预测任务上。例如，AdaCE在预训Swin-Transformer上 achieve 99.6%，即相对提高9.2%的精度。此外，作者还证明了，将更大的预训模型通过AdaCE进行 fine-tune 可以在EEG预测任务上 achieve better performance。<details>
<summary>Abstract</summary>
Pre-trained large transformer models have achieved remarkable performance in the fields of natural language processing and computer vision. Since the magnitude of available labeled electroencephalogram (EEG) data is much lower than that of text and image data, it is difficult for transformer models pre-trained from EEG to be developed as large as GPT-4 100T to fully unleash the potential of this architecture. In this paper, we show that transformers pre-trained from images as well as text can be directly fine-tuned for EEG-based prediction tasks. We design AdaCE, plug-and-play Adapters for Converting EEG data into image as well as text forms, to fine-tune pre-trained vision and language transformers. The proposed AdaCE module is highly effective for fine-tuning pre-trained transformers while achieving state-of-the-art performance on diverse EEG-based prediction tasks. For example, AdaCE on the pre-trained Swin-Transformer achieves 99.6%, an absolute improvement of 9.2%, on the EEG-decoding task of human activity recognition (UCI HAR). Furthermore, we empirically show that applying the proposed AdaCE to fine-tune larger pre-trained models can achieve better performance on EEG-based predicting tasks, indicating the potential of our adapters for even larger transformers. The plug-and-play AdaCE module can be applied to fine-tuning most of the popular pre-trained transformers on many other time-series data with multiple channels, not limited to EEG data and the models we use. Our code will be available at https://github.com/wangbxj1234/AdaCE.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>大型预训练变换器模型在自然语言处理和计算机视觉领域已经实现了很好的表现。由于电生成ogram（EEG）数据的数量相对较少，预训练后的变换器模型难以达到GPT-4 100T的规模，以全面发挥变换器模型的潜力。在这篇论文中，我们表明了从图像和文本预训练的变换器模型可以直接进行EEG数据适应。我们设计了AdaCE模块，它是一种用于将EEG数据转换为图像和文本形式的插件，以便适应预训练的视觉和语言变换器。我们的AdaCE模块非常有效地进行适应预训练后的变换器模型，并在多种EEG预测任务中达到了状态元的表现。例如，AdaCE在预训练Swin-Transformer上的EEG解码任务上达到了99.6%，相对于基线方法的9.2%提升。此外，我们还证明了在应用我们的AdaCE插件后，可以进行更大的预训练模型的精度调整，这表明了我们的插件在更大的变换器模型上的潜力。插件可以应用于大多数流行的预训练变换器模型上，并不限于EEG数据和我们所用的模型。我们的代码将在https://github.com/wangbxj1234/AdaCE上提供。
</details></li>
</ul>
<hr>
<h2 id="Towards-Synthesizing-Datasets-for-IEEE-802-1-Time-sensitive-Networking"><a href="#Towards-Synthesizing-Datasets-for-IEEE-802-1-Time-sensitive-Networking" class="headerlink" title="Towards Synthesizing Datasets for IEEE 802.1 Time-sensitive Networking"></a>Towards Synthesizing Datasets for IEEE 802.1 Time-sensitive Networking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10255">http://arxiv.org/abs/2308.10255</a></li>
<li>repo_url: None</li>
<li>paper_authors: Doğanalp Ergenç, Nurefşan Sertbaş Bülbül, Lisa Maile, Anna Arestova, Mathias Fischer</li>
<li>for: 本文旨在探讨IEEE 802.1时间敏感网络协议在不同的传统系统中的应用，以及使用人工智能和机器学习模型来开发高级配置和维护方法。</li>
<li>methods: 本文提出了一种使用人工智能和机器学习模型来开发高级配置和维护方法的方法，并分析了该方法的主要需求和可行设计。</li>
<li>results: 本文指出，为了推进IEEE 802.1时间敏感网络协议的研究，需要开发一些可靠的TSN数据集，以便训练人工智能和机器学习模型。<details>
<summary>Abstract</summary>
IEEE 802.1 Time-sensitive Networking (TSN) protocols have recently been proposed to replace legacy networking technologies across different mission-critical systems (MCSs). Design, configuration, and maintenance of TSN within MCSs require advanced methods to tackle the highly complex and interconnected nature of those systems. Accordingly, artificial intelligence (AI) and machine learning (ML) models are the most prominent enablers to develop such methods. However, they usually require a significant amount of data for model training, which is not easily accessible. This short paper aims to recapitulate the need for TSN datasets to flourish research on AI/ML-based techniques for TSN systems. Moreover, it analyzes the main requirements and alternative designs to build a TSN platform to synthesize realistic datasets.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="StableLLaVA-Enhanced-Visual-Instruction-Tuning-with-Synthesized-Image-Dialogue-Data"><a href="#StableLLaVA-Enhanced-Visual-Instruction-Tuning-with-Synthesized-Image-Dialogue-Data" class="headerlink" title="StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data"></a>StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10253">http://arxiv.org/abs/2308.10253</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/icoz69/stablellava">https://github.com/icoz69/stablellava</a></li>
<li>paper_authors: Yanda Li, Chi Zhang, Gang Yu, Zhibin Wang, Bin Fu, Guosheng Lin, Chunhua Shen, Ling Chen, Yunchao Wei</li>
<li>for: 这些论文的主要研究目标是开发一种能够有效地对应文本和视觉模式的大型自然语言模型（LLM），以便理解人类的指令。</li>
<li>methods: 我们提议一种新的数据收集方法，即同步生成图像和对话，以便对视觉指令进行调整。这种方法利用了生成模型的能力，将文本生成模型和对话生成模型结合起来，以生成多样化和可控的数据集。</li>
<li>results: 我们的研究包括对多个数据集进行了广泛的实验，使用开源的 LLAVA 模型作为我们提议的管道进行测试。我们的结果表明，我们的方法可以明显提高 LLM 的多种常见能力，包括图像生成、对话生成、问题回答、文本生成等。<details>
<summary>Abstract</summary>
The remarkable multimodal capabilities demonstrated by OpenAI's GPT-4 have sparked significant interest in the development of multimodal Large Language Models (LLMs). A primary research objective of such models is to align visual and textual modalities effectively while comprehending human instructions. Current methodologies often rely on annotations derived from benchmark datasets to construct image-dialogue datasets for training purposes, akin to instruction tuning in LLMs. However, these datasets often exhibit domain bias, potentially constraining the generative capabilities of the models. In an effort to mitigate these limitations, we propose a novel data collection methodology that synchronously synthesizes images and dialogues for visual instruction tuning. This approach harnesses the power of generative models, marrying the abilities of ChatGPT and text-to-image generative models to yield a diverse and controllable dataset with varied image content. This not only provides greater flexibility compared to existing methodologies but also significantly enhances several model capabilities. Our research includes comprehensive experiments conducted on various datasets using the open-source LLAVA model as a testbed for our proposed pipeline. Our results underscore marked enhancements across more than ten commonly assessed capabilities,
</details>
<details>
<summary>摘要</summary>
“OpenAI的GPT-4的多模式能力引起了很大的关注，推动了多模式大语言模型（LLM）的研发。这些模型的主要研究目标是在理解人类 instrucion 时，有效地对文本和视觉模式进行对应。现有的方法ologies  oft en rely on来自标准 benchmark dataset 的注释来构建图像对话集 для训练purpose，类似于 instruction tuning 在 LLM 中。然而，这些数据集经常受到领域偏见的影响，可能限制模型的生成能力。为了缓解这些限制，我们提出了一种新的数据采集方法，同时生成图像和对话，以便对视觉 instrucion 进行调整。这种方法利用了生成模型的能力，将文本生成模型和图像生成模型结合起来，生成了多样化和可控的数据集。这不仅提供了现有方法所不具备的灵活性，还显著提高了多种模型能力。我们的研究包括对多个数据集进行了全面的实验，使用开源的 LLAVA 模型作为我们提议的管道测试环境。我们的结果表明，我们的方法在多达十个常评价指标上具有明显的提升。”
</details></li>
</ul>
<hr>
<h2 id="Activation-Addition-Steering-Language-Models-Without-Optimization"><a href="#Activation-Addition-Steering-Language-Models-Without-Optimization" class="headerlink" title="Activation Addition: Steering Language Models Without Optimization"></a>Activation Addition: Steering Language Models Without Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10248">http://arxiv.org/abs/2308.10248</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alex Turner, Lisa Thiergart, David Udell, Gavin Leech, Ulisse Mini, Monte MacDiarmid</li>
<li>for: 这个论文旨在提出一种可靠地控制大型自然语言模型（LLM）的行为的方法。</li>
<li>methods: 论文使用激活工程（Activation Addition，ActAdd）方法，在推理时添加一个“导航向量”，通过自然语言来隐式地定义。</li>
<li>results: 论文在GPT-2上进行了OpenWebText和ConceptNet的测试，表明ActAdd方法可以在推理时控制输出的高级性质，并且不会影响模型的目标性能。此外，ActAdd方法比超visionfinetuning和人类反馈学习（RLHF）更加快速，需要更少的计算资源和实现努力，同时允许用户提供自然语言指令。<details>
<summary>Abstract</summary>
Reliably controlling the behavior of large language models (LLMs) is a pressing open problem. Existing methods include supervised finetuning, reinforcement learning from human feedback (RLHF), prompt engineering and guided decoding. We instead investigate activation engineering: modifying activations at inference time to predictably alter model behavior. In particular, we bias the forward pass with an added 'steering vector' implicitly specified through natural language.   Unlike past work which learned these steering vectors (Subramani, Suresh, and Peters 2022; Hernandez, Li, and Andreas 2023), our Activation Addition (ActAdd) method computes them by taking the activation differences that result from pairs of prompts. We demonstrate ActAdd on GPT-2 on OpenWebText and ConceptNet. Our inference-time approach yields control over high-level properties of output and preserves off-target model performance. It involves far less compute and implementation effort compared to finetuning or RLHF, allows users to provide natural language specifications, and its overhead scales naturally with model size.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）的可靠控制问题仍是一个开放的问题。现有的方法包括监督微调、人工反馈学习（RLHF）、提示工程和导航解码。我们则是活动工程：在推理时修改激活来预测性地改变模型行为。具体来说，我们在前进通过添加“导航向量”来隐式地指定自然语言中的批处理。与过去的工作不同（Subramani et al. 2022；Hernandez et al. 2023），我们的激活添加（ActAdd）方法不是学习这些导航向量，而是通过对提示对的激活差异来计算它们。我们在GPT-2上使用OpenWebText和ConceptNet进行了实验，并证明了ActAdd可以在推理时控制输出的高级性质，并且保持目标模型性能。这种在推理时进行的方法比微调或RLHF需要更少的计算和实现努力，允许用户提供自然语言规范，并且其开销随模型大小呈指数增长。
</details></li>
</ul>
<hr>
<h2 id="From-Global-to-Local-Multi-scale-Out-of-distribution-Detection"><a href="#From-Global-to-Local-Multi-scale-Out-of-distribution-Detection" class="headerlink" title="From Global to Local: Multi-scale Out-of-distribution Detection"></a>From Global to Local: Multi-scale Out-of-distribution Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10239">http://arxiv.org/abs/2308.10239</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jimzai/mode-ood">https://github.com/jimzai/mode-ood</a></li>
<li>paper_authors: Ji Zhang, Lianli Gao, Bingguang Hao, Hao Huang, Jingkuan Song, Hengtao Shen</li>
<li>for: 这个研究的目的是提高OD detection的精度，尤其是在遇到未知数据时。</li>
<li>methods: 这个方法使用了多个缩寸的方法，包括global visual information和local region details，以提高OD detection的精度。</li>
<li>results: 这个方法在多个benchmark上的表现比前一代方法好，具体的表现提高了19.24%在False Positive Rate和2.77%在AUC上。<details>
<summary>Abstract</summary>
Out-of-distribution (OOD) detection aims to detect "unknown" data whose labels have not been seen during the in-distribution (ID) training process. Recent progress in representation learning gives rise to distance-based OOD detection that recognizes inputs as ID/OOD according to their relative distances to the training data of ID classes. Previous approaches calculate pairwise distances relying only on global image representations, which can be sub-optimal as the inevitable background clutter and intra-class variation may drive image-level representations from the same ID class far apart in a given representation space. In this work, we overcome this challenge by proposing Multi-scale OOD DEtection (MODE), a first framework leveraging both global visual information and local region details of images to maximally benefit OOD detection. Specifically, we first find that existing models pretrained by off-the-shelf cross-entropy or contrastive losses are incompetent to capture valuable local representations for MODE, due to the scale-discrepancy between the ID training and OOD detection processes. To mitigate this issue and encourage locally discriminative representations in ID training, we propose Attention-based Local PropAgation (ALPA), a trainable objective that exploits a cross-attention mechanism to align and highlight the local regions of the target objects for pairwise examples. During test-time OOD detection, a Cross-Scale Decision (CSD) function is further devised on the most discriminative multi-scale representations to distinguish ID/OOD data more faithfully. We demonstrate the effectiveness and flexibility of MODE on several benchmarks -- on average, MODE outperforms the previous state-of-the-art by up to 19.24% in FPR, 2.77% in AUROC. Code is available at https://github.com/JimZAI/MODE-OOD.
</details>
<details>
<summary>摘要</summary>
OUT-OF-DISTRIBUTION (OOD) 检测的目标是检测“未知”数据，其标签在ID 训练过程中没有出现过。随着表征学学习的进步，距离基于 OOD 检测已经得到了广泛应用。然而，过去的方法通常基于全局图像表征，忽略了图像内部的细节信息，这可能会导致同一个ID类型的图像在给定的表征空间中被分化。在这种情况下，我们提出了一个新的框架，即多scale OOD 检测（MODE），它利用全局视觉信息和图像内部的局部区域特征来最大化 OOD 检测的效果。Specifically, we find that existing models pretrained by off-the-shelf cross-entropy or contrastive losses are incompetent to capture valuable local representations for MODE, due to the scale-discrepancy between the ID training and OOD detection processes. To mitigate this issue and encourage locally discriminative representations in ID training, we propose Attention-based Local PropAgation (ALPA), a trainable objective that exploits a cross-attention mechanism to align and highlight the local regions of the target objects for pairwise examples. During test-time OOD detection, a Cross-Scale Decision (CSD) function is further devised on the most discriminative multi-scale representations to distinguish ID/OOD data more faithfully. We demonstrate the effectiveness and flexibility of MODE on several benchmarks -- on average, MODE outperforms the previous state-of-the-art by up to 19.24% in FPR, 2.77% in AUROC. Code is available at https://github.com/JimZAI/MODE-OOD.
</details></li>
</ul>
<hr>
<h2 id="Thompson-Sampling-for-Real-Valued-Combinatorial-Pure-Exploration-of-Multi-Armed-Bandit"><a href="#Thompson-Sampling-for-Real-Valued-Combinatorial-Pure-Exploration-of-Multi-Armed-Bandit" class="headerlink" title="Thompson Sampling for Real-Valued Combinatorial Pure Exploration of Multi-Armed Bandit"></a>Thompson Sampling for Real-Valued Combinatorial Pure Exploration of Multi-Armed Bandit</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10238">http://arxiv.org/abs/2308.10238</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shintaro Nakamura, Masashi Sugiyama</li>
<li>for:  solve the real-valued combinatorial pure exploration of the multi-armed bandit (R-CPE-MAB) problem, which is to find the optimal action from a finite-sized real-valued action set with as few arm pulls as possible.</li>
<li>methods:  the Generalized Thompson Sampling Explore (GenTS-Explore) algorithm, which is the first algorithm that can work even when the size of the action set is exponentially large in d.</li>
<li>results:  a novel problem-dependent sample complexity lower bound of the R-CPE-MAB problem, and show that the GenTS-Explore algorithm achieves the optimal sample complexity up to a problem-dependent constant factor.Here are the three information in a more concise format, using bullet points:</li>
<li>for:  solve the R-CPE-MAB problem with a large action set.</li>
<li>methods:  GenTS-Explore algorithm.</li>
<li>results:  a novel problem-dependent sample complexity lower bound, and the GenTS-Explore algorithm achieves the optimal sample complexity up to a constant factor.<details>
<summary>Abstract</summary>
We study the real-valued combinatorial pure exploration of the multi-armed bandit (R-CPE-MAB) problem. In R-CPE-MAB, a player is given $d$ stochastic arms, and the reward of each arm $s\in\{1, \ldots, d\}$ follows an unknown distribution with mean $\mu_s$. In each time step, a player pulls a single arm and observes its reward. The player's goal is to identify the optimal \emph{action} $\boldsymbol{\pi}^{*} = \argmax_{\boldsymbol{\pi} \in \mathcal{A}} \boldsymbol{\mu}^{\top}\boldsymbol{\pi}$ from a finite-sized real-valued \emph{action set} $\mathcal{A}\subset \mathbb{R}^{d}$ with as few arm pulls as possible. Previous methods in the R-CPE-MAB assume that the size of the action set $\mathcal{A}$ is polynomial in $d$. We introduce an algorithm named the Generalized Thompson Sampling Explore (GenTS-Explore) algorithm, which is the first algorithm that can work even when the size of the action set is exponentially large in $d$. We also introduce a novel problem-dependent sample complexity lower bound of the R-CPE-MAB problem, and show that the GenTS-Explore algorithm achieves the optimal sample complexity up to a problem-dependent constant factor.
</details>
<details>
<summary>摘要</summary>
我们研究了实数值的可整合探索多臂枪手问题（R-CPE-MAB）。在R-CPE-MAB中，一个玩家被分配了$d$个随机臂，每个臂$s\in\{1, \ldots, d\}$的奖励follows一个未知分布的mean $\mu_s$。在每个时间步骤中，一个玩家抓一个臂并观察其奖励。玩家的目标是从一个封闭的实数值动作集$\mathcal{A}\subset \mathbb{R}^{d}$中选择最佳的动作 $\boldsymbol{\pi}^{*} = \argmax_{\boldsymbol{\pi} \in \mathcal{A}} \boldsymbol{\mu}^{\top}\boldsymbol{\pi}$，以最少的臂抓次数为目标。先前的R-CPE-MAB方法假设动作集$\mathcal{A}$的大小是对数函数的$d$。我们介绍了一个名为Generalized Thompson Sampling Explore（GenTS-Explore）算法，这是第一个可以在动作集$\mathcal{A}$的大小是指数增长的$d$时工作的算法。我们还介绍了一个问题内部依赖的样本缩减下限，并证明GenTS-Explore算法实现了问题内部依赖的样本缩减下限。
</details></li>
</ul>
<hr>
<h2 id="FedSIS-Federated-Split-Learning-with-Intermediate-Representation-Sampling-for-Privacy-preserving-Generalized-Face-Presentation-Attack-Detection"><a href="#FedSIS-Federated-Split-Learning-with-Intermediate-Representation-Sampling-for-Privacy-preserving-Generalized-Face-Presentation-Attack-Detection" class="headerlink" title="FedSIS: Federated Split Learning with Intermediate Representation Sampling for Privacy-preserving Generalized Face Presentation Attack Detection"></a>FedSIS: Federated Split Learning with Intermediate Representation Sampling for Privacy-preserving Generalized Face Presentation Attack Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10236">http://arxiv.org/abs/2308.10236</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/naiftt/fedsis">https://github.com/naiftt/fedsis</a></li>
<li>paper_authors: Naif Alkhunaizi, Koushik Srivatsan, Faris Almalik, Ibrahim Almakky, Karthik Nandakumar</li>
<li>for: 本研究旨在提高面部攻击检测算法（FacePAD）的通用性，解决现有算法的 Achilles heel 问题。</li>
<li>methods: 本研究提出了一种新的框架called Federated Split learning with Intermediate representation Sampling（FedSIS），combines federated learning（FL）和split learning，以保持隐私而实现预测通用性。</li>
<li>results: 研究表明，FedSIS 可以在不需要数据共享的情况下，达到面部攻击检测算法的状态之最游标性表现，并在未见过的频道上实现良好的泛化性。<details>
<summary>Abstract</summary>
Lack of generalization to unseen domains/attacks is the Achilles heel of most face presentation attack detection (FacePAD) algorithms. Existing attempts to enhance the generalizability of FacePAD solutions assume that data from multiple source domains are available with a single entity to enable centralized training. In practice, data from different source domains may be collected by diverse entities, who are often unable to share their data due to legal and privacy constraints. While collaborative learning paradigms such as federated learning (FL) can overcome this problem, standard FL methods are ill-suited for domain generalization because they struggle to surmount the twin challenges of handling non-iid client data distributions during training and generalizing to unseen domains during inference. In this work, a novel framework called Federated Split learning with Intermediate representation Sampling (FedSIS) is introduced for privacy-preserving domain generalization. In FedSIS, a hybrid Vision Transformer (ViT) architecture is learned using a combination of FL and split learning to achieve robustness against statistical heterogeneity in the client data distributions without any sharing of raw data (thereby preserving privacy). To further improve generalization to unseen domains, a novel feature augmentation strategy called intermediate representation sampling is employed, and discriminative information from intermediate blocks of a ViT is distilled using a shared adapter network. The FedSIS approach has been evaluated on two well-known benchmarks for cross-domain FacePAD to demonstrate that it is possible to achieve state-of-the-art generalization performance without data sharing. Code: https://github.com/Naiftt/FedSIS
</details>
<details>
<summary>摘要</summary>
缺乏泛化到未经见的领域/攻击是现有的面孔展示攻击检测（FacePAD）算法的 Achilles heel。现有的增强FacePAD解决方案假设有多个源领域的数据可以在单一实体上进行中央式训练。然而，在实践中，来自不同的源领域的数据可能是由不同的实体收集的，这些实体经常因为法律和隐私限制无法共享自己的数据。而合作学习 paradigm such as federated learning（FL）可以解决这个问题，但标准的FL方法在适应新的领域时存在两大挑战：处理非标一Client数据分布在训练中和在推断中适应未经见的领域。在这种情况下，一种名为 Federated Split learning with Intermediate representation Sampling（FedSIS）的新框架被引入，用于保护隐私的领域泛化。在 FedSIS 中，使用一种混合的 Vision Transformer（ViT）架构，通过 combining FL 和 split learning 来实现对Client数据分布的统计异质性的Robustness，而不需要 Client 数据的共享。为了进一步提高适应未经见的领域，一种名为 intermediate representation sampling 的新的特征增强策略被使用，并通过一个共享的 adapter 网络来浓缩出权威信息。FedSIS 方法在两个常用的 cross-domain FacePAD  benchmark 上进行了评估，并证明了可以在没有数据共享情况下实现状态码的泛化性能。代码：https://github.com/Naiftt/FedSIS
</details></li>
</ul>
<hr>
<h2 id="Karma-Adaptive-Video-Streaming-via-Causal-Sequence-Modeling"><a href="#Karma-Adaptive-Video-Streaming-via-Causal-Sequence-Modeling" class="headerlink" title="Karma: Adaptive Video Streaming via Causal Sequence Modeling"></a>Karma: Adaptive Video Streaming via Causal Sequence Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10230">http://arxiv.org/abs/2308.10230</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fcbw2012/Karma">https://github.com/fcbw2012/Karma</a></li>
<li>paper_authors: Bowei Xu, Hao Chen, Zhan Ma</li>
<li>For: This paper aims to improve the adaptive bitrate (ABR) decision-making process by utilizing causal sequence modeling to comprehend the interrelated causality among past observations, returns, and actions, and timely refining actions when deviations occur.* Methods: The proposed Karma algorithm uses a decision transformer to determine the next action based on a multi-dimensional time series of observations, returns, and actions, with the maximum cumulative future quality of experience (QoE) as an extended return signal.* Results: The paper demonstrates superior performance compared to existing state-of-the-art ABR algorithms, with an average QoE improvement ranging from 10.8% to 18.7% across diverse network conditions, and strong generalization capabilities under unseen networks in both simulations and real-world tests.Here is the text in Simplified Chinese:* For: 这篇论文目标是通过利用 causal sequence modeling 改善 adaptive bitrate (ABR) 决策过程，以便更好地理解过去观察、返回和行为之间的相互关系，并在偏差发生时及时更新行动。* Methods: 提议的 Karma 算法使用决策转换器来确定下一步行动，基于多维时间序列观察、返回和行动中的观察、返回和行动。* Results: 论文表明，相比现有的状态艺术 ABR 算法，Karma 算法在多种网络条件下表现出较高的平均Quality of Experience（QoE）提升，从10.8% 到18.7%。此外，Karma 算法在未见网络上也显示出了强大的泛化能力。<details>
<summary>Abstract</summary>
Optimal adaptive bitrate (ABR) decision depends on a comprehensive characterization of state transitions that involve interrelated modalities over time including environmental observations, returns, and actions. However, state-of-the-art learning-based ABR algorithms solely rely on past observations to decide the next action. This paradigm tends to cause a chain of deviations from optimal action when encountering unfamiliar observations, which consequently undermines the model generalization. This paper presents Karma, an ABR algorithm that utilizes causal sequence modeling to improve generalization by comprehending the interrelated causality among past observations, returns, and actions and timely refining action when deviation occurs. Unlike direct observation-to-action mapping, Karma recurrently maintains a multi-dimensional time series of observations, returns, and actions as input and employs causal sequence modeling via a decision transformer to determine the next action. In the input sequence, Karma uses the maximum cumulative future quality of experience (QoE) (a.k.a, QoE-to-go) as an extended return signal, which is periodically estimated based on current network conditions and playback status. We evaluate Karma through trace-driven simulations and real-world field tests, demonstrating superior performance compared to existing state-of-the-art ABR algorithms, with an average QoE improvement ranging from 10.8% to 18.7% across diverse network conditions. Furthermore, Karma exhibits strong generalization capabilities, showing leading performance under unseen networks in both simulations and real-world tests.
</details>
<details>
<summary>摘要</summary>
优化的适应比率（ABR）决策需要对状态转移进行全面的特征化，包括时间上的相关Modalities。然而，现有的学习基于ABR算法只是基于过去的观察来决定下一个动作。这种做法会导致对于不熟悉的观察而引起链式偏差，从而下降模型泛化。这篇论文提出了Karma算法，它利用 causal sequence modeling来提高泛化性，通过理解过去观察、返回和动作之间的相关 causality，并在偏差发生时进行时间 opportune 的修正。与直接观察到动作映射不同，Karma 使用循环维护一个多维时间序列，并使用决策变换器来确定下一个动作。在输入序列中，Karma 使用最大累积未来体验质量（QoE）作为延长返回信号，这些信号 periodically 根据当前网络conditions和播放状态来 estimating。我们通过跟踪驱动的 simulations 和实际场景测试评估了Karma，并证明它在不同的网络条件下表现出优于现有状态艺术ABR算法，QoE 提升平均值在10.8%到18.7%之间。此外，Karma 表现出了强大的泛化能力，在未看到的网络上仍然保持领先的表现。
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-powered-Combinatorial-Clock-Auction"><a href="#Machine-Learning-powered-Combinatorial-Clock-Auction" class="headerlink" title="Machine Learning-powered Combinatorial Clock Auction"></a>Machine Learning-powered Combinatorial Clock Auction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10226">http://arxiv.org/abs/2308.10226</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/marketdesignresearch/ml-cca">https://github.com/marketdesignresearch/ml-cca</a></li>
<li>paper_authors: Ermis Soumalias, Jakob Weissteiner, Jakob Heiss, Sven Seuken</li>
<li>for: 这篇论文关注了迭代 combinatorial 拍卖（ICA）的设计。ICA 中的主要挑战在于bundle空间随着物品数量的增加而 exponentiates。</li>
<li>methods: 这篇论文提出了一种基于机器学习（ML）的偏好拟合算法，以便从投标者那里获取最重要的信息。</li>
<li>results: 这篇论文的实验结果表明，相比 combinatorial clock auction（CCA），我们的 ML-based demand query mechanism在各个频谱拍卖领域中表现出了显著的高效性。它在一个较小的数量的拍卖轮次中达到了更高的效率，并且使用线性价格时可以达到了巨大的清算潜力。因此，这篇论文 bridge了研究和实践之间的差距，并提出了首个实用的 ML-powered ICA。<details>
<summary>Abstract</summary>
We study the design of iterative combinatorial auctions (ICAs). The main challenge in this domain is that the bundle space grows exponentially in the number of items. To address this, several papers have recently proposed machine learning (ML)-based preference elicitation algorithms that aim to elicit only the most important information from bidders. However, from a practical point of view, the main shortcoming of this prior work is that those designs elicit bidders' preferences via value queries (i.e., ``What is your value for the bundle $\{A,B\}$?''). In most real-world ICA domains, value queries are considered impractical, since they impose an unrealistically high cognitive burden on bidders, which is why they are not used in practice. In this paper, we address this shortcoming by designing an ML-powered combinatorial clock auction that elicits information from the bidders only via demand queries (i.e., ``At prices $p$, what is your most preferred bundle of items?''). We make two key technical contributions: First, we present a novel method for training an ML model on demand queries. Second, based on those trained ML models, we introduce an efficient method for determining the demand query with the highest clearing potential, for which we also provide a theoretical foundation. We experimentally evaluate our ML-based demand query mechanism in several spectrum auction domains and compare it against the most established real-world ICA: the combinatorial clock auction (CCA). Our mechanism significantly outperforms the CCA in terms of efficiency in all domains, it achieves higher efficiency in a significantly reduced number of rounds, and, using linear prices, it exhibits vastly higher clearing potential. Thus, with this paper we bridge the gap between research and practice and propose the first practical ML-powered ICA.
</details>
<details>
<summary>摘要</summary>
我们研究Iterative Combinatorial Auctions（ICA）的设计。ICA的主要挑战在于bundle空间随着物品数量的增加而增加 exponentially。为了解决这个问题，一些最近的论文已经提出了基于机器学习（ML）的偏好探索算法，以探索供应商对不同套件的偏好。然而，从实践的角度来看，这些设计都是通过值询问（i.e.,“What is your value for the bundle $\{A,B\}$?”）来探索供应商的偏好，这种方法在实际应用中被视为不实际。在这篇论文中，我们解决这个问题，通过设计一个基于ML的套件时钟拍卖，通过需求询问（i.e.,“At prices $p$, what is your most preferred bundle of items?”）来探索供应商的偏好。我们的研究做出了两个关键技术贡献：首先，我们提出了一种新的需求询问训练ML模型的方法；其次，基于这些训练的ML模型，我们引入了一种高效的需求询问找到最高清算潜力的方法，并提供了理论基础。我们在几个频谱拍卖领域进行了实验评估，与现有最具実际应用的ICA：套件时钟拍卖（CCA）进行比较。我们的机制在所有领域中都有着明显的高效性，在许多领域中，它在许多更少的轮数中达到了更高的高效性，并且使用线性价格，它的清算潜力是极大的。因此，这篇论文通过实践和理论的研究，提出了第一个实际应用的ML-Powered ICA。
</details></li>
</ul>
<hr>
<h2 id="Soft-Decomposed-Policy-Critic-Bridging-the-Gap-for-Effective-Continuous-Control-with-Discrete-RL"><a href="#Soft-Decomposed-Policy-Critic-Bridging-the-Gap-for-Effective-Continuous-Control-with-Discrete-RL" class="headerlink" title="Soft Decomposed Policy-Critic: Bridging the Gap for Effective Continuous Control with Discrete RL"></a>Soft Decomposed Policy-Critic: Bridging the Gap for Effective Continuous Control with Discrete RL</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10203">http://arxiv.org/abs/2308.10203</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yechen Zhang, Jian Sun, Gang Wang, Zhuo Li, Wei Chen</li>
<li>for: 解决连续控制问题中的维度爆炸问题</li>
<li>methods: combines soft RL和actor-critic技术，独立地对每个动作维度进行柔化，使用共享批处理网络来最大化柔化$Q$-函数</li>
<li>results: 在多种连续控制任务中，比如Mujoco的人工智能和Box2d的两脚行走器，实验结果表明我们提出的方法可以超越现有的连续RL算法表现。<details>
<summary>Abstract</summary>
Discrete reinforcement learning (RL) algorithms have demonstrated exceptional performance in solving sequential decision tasks with discrete action spaces, such as Atari games. However, their effectiveness is hindered when applied to continuous control problems due to the challenge of dimensional explosion. In this paper, we present the Soft Decomposed Policy-Critic (SDPC) architecture, which combines soft RL and actor-critic techniques with discrete RL methods to overcome this limitation. SDPC discretizes each action dimension independently and employs a shared critic network to maximize the soft $Q$-function. This novel approach enables SDPC to support two types of policies: decomposed actors that lead to the Soft Decomposed Actor-Critic (SDAC) algorithm, and decomposed $Q$-networks that generate Boltzmann soft exploration policies, resulting in the Soft Decomposed-Critic Q (SDCQ) algorithm. Through extensive experiments, we demonstrate that our proposed approach outperforms state-of-the-art continuous RL algorithms in a variety of continuous control tasks, including Mujoco's Humanoid and Box2d's BipedalWalker. These empirical results validate the effectiveness of the SDPC architecture in addressing the challenges associated with continuous control.
</details>
<details>
<summary>摘要</summary>
离散强化学习（RL）算法在解决顺序决策任务中的离散动作空间方面表现出色，如Atari游戏。然而，当应用于连续控制问题时，它们的效iveness受到维度爆炸的挑战。在这篇论文中，我们提出了软分解策略-批评（SDPC）架构，它将离散RL和演员-批评技术与离散RL方法相结合，以解决这一问题。SDPC独立地对每个动作维度进行分解，并使用共享批评网络来最大化软Q函数。这种新的方法使得SDPC支持两种策略：分解演员，导致Soft Decomposed Actor-Critic（SDAC）算法，以及分解Q网络，生成Boltzmann软探索策略，导致Soft Decomposed-Critic Q（SDCQ）算法。我们通过广泛的实验表明，我们提出的方法在多种连续控制任务中比州前的连续RL算法表现出色，包括Mujoco的人iform和Box2d的BipedalWalker。这些实验结果证明了SDPC架构在连续控制问题中的有效性。
</details></li>
</ul>
<hr>
<h2 id="Hiding-Backdoors-within-Event-Sequence-Data-via-Poisoning-Attacks"><a href="#Hiding-Backdoors-within-Event-Sequence-Data-via-Poisoning-Attacks" class="headerlink" title="Hiding Backdoors within Event Sequence Data via Poisoning Attacks"></a>Hiding Backdoors within Event Sequence Data via Poisoning Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10201">http://arxiv.org/abs/2308.10201</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elizaveta Kovtun, Alina Ermilova, Dmitry Berestnev, Alexey Zaytsev</li>
<li>for: 这个论文旨在描述如何在金融业中使用深度学习模型，同时解决这些模型受到恶意攻击的问题。</li>
<li>methods: 这个论文使用了一种名为“潜藏后门”的方法，通过在训练过程中插入一个隐藏的攻击点来引入攻击性质的模型。</li>
<li>results:  experiments 表明，这种方法可以在不同的 dataset、架构和模型组件上实现攻击，并且可以让模型在检测攻击时保持正常的功能。<details>
<summary>Abstract</summary>
The financial industry relies on deep learning models for making important decisions. This adoption brings new danger, as deep black-box models are known to be vulnerable to adversarial attacks. In computer vision, one can shape the output during inference by performing an adversarial attack called poisoning via introducing a backdoor into the model during training. For sequences of financial transactions of a customer, insertion of a backdoor is harder to perform, as models operate over a more complex discrete space of sequences, and systematic checks for insecurities occur. We provide a method to introduce concealed backdoors, creating vulnerabilities without altering their functionality for uncontaminated data. To achieve this, we replace a clean model with a poisoned one that is aware of the availability of a backdoor and utilize this knowledge. Our most difficult for uncovering attacks include either additional supervised detection step of poisoned data activated during the test or well-hidden model weight modifications. The experimental study provides insights into how these effects vary across different datasets, architectures, and model components. Alternative methods and baselines, such as distillation-type regularization, are also explored but found to be less efficient. Conducted on three open transaction datasets and architectures, including LSTM, CNN, and Transformer, our findings not only illuminate the vulnerabilities in contemporary models but also can drive the construction of more robust systems.
</details>
<details>
<summary>摘要</summary>
Financial industry 使用深度学习模型作重要决策，这种采用带来新的危险，深度黑盒模型容易受到抗击攻击。在计算机视觉中，可以在推理过程中Shape the outputby performing an adversarial attack called poisoning via introducing a backdoor into the model during training. However, for sequences of financial transactions of a customer, insertion of a backdoor is harder to perform, as models operate over a more complex discrete space of sequences, and systematic checks for insecurities occur. We provide a method to introduce concealed backdoors, creating vulnerabilities without altering their functionality for uncontaminated data. To achieve this, we replace a clean model with a poisoned one that is aware of the availability of a backdoor and utilize this knowledge. Our most difficult for uncovering attacks include either additional supervised detection step of poisoned data activated during the test or well-hidden model weight modifications. The experimental study provides insights into how these effects vary across different datasets, architectures, and model components. Alternative methods and baselines, such as distillation-type regularization, are also explored but found to be less efficient. Conducted on three open transaction datasets and architectures, including LSTM, CNN, and Transformer, our findings not only illuminate the vulnerabilities in contemporary models but also can drive the construction of more robust systems.Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. Traditional Chinese is used in Taiwan, Hong Kong, and other regions.
</details></li>
</ul>
<hr>
<h2 id="Deep-Reinforcement-Learning-for-Artificial-Upwelling-Energy-Management"><a href="#Deep-Reinforcement-Learning-for-Artificial-Upwelling-Energy-Management" class="headerlink" title="Deep Reinforcement Learning for Artificial Upwelling Energy Management"></a>Deep Reinforcement Learning for Artificial Upwelling Energy Management</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10199">http://arxiv.org/abs/2308.10199</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiyuan Zhang, Wei Fan</li>
<li>for: 这篇论文旨在探讨人工上升（Artificial Upwelling，AU）技术是否能够提高海洋抑 carbon 储存，以及AU 系统如何 efficiently 运行。</li>
<li>methods: 该论文提出了一种使用深度强化学习（Deep Reinforcement Learning，DRL）算法来开发高效的 AU 系统操作策略。</li>
<li>results: 通过大量的 simulations，该论文表明了 DRL 算法比传统的规则型approaches和其他 DRL 算法更有效率地减少能源浪费，同时确保 AU 系统的稳定和高效操作。<details>
<summary>Abstract</summary>
The potential of artificial upwelling (AU) as a means of lifting nutrient-rich bottom water to the surface, stimulating seaweed growth, and consequently enhancing ocean carbon sequestration, has been gaining increasing attention in recent years. This has led to the development of the first solar-powered and air-lifted AU system (AUS) in China. However, efficient scheduling of air injection systems remains a crucial challenge in operating AUS, as it holds the potential to significantly improve system efficiency. Conventional approaches based on rules or models are often impractical due to the complex and heterogeneous nature of the marine environment and its associated disturbances. To address this challenge, we propose a novel energy management approach that utilizes deep reinforcement learning (DRL) algorithm to develop efficient strategies for operating AUS. Through extensive simulations, we evaluate the performance of our algorithm and demonstrate its superior effectiveness over traditional rule-based approaches and other DRL algorithms in reducing energy wastage while ensuring the stable and efficient operation of AUS. Our findings suggest that a DRL-based approach offers a promising way for improving the efficiency of AUS and enhancing the sustainability of seaweed cultivation and carbon sequestration in the ocean.
</details>
<details>
<summary>摘要</summary>
人工升浮（AU）的潜在作用是将有营养物质的底水升到表层，促进海藻生长，从而提高海洋碳储存量。在最近几年中，AU在中国已经开始研发首个太阳能驱动、空气升降系统（AUS）。然而，AU系统的有效调度仍然是一个主要挑战，因为它们的 marine 环境复杂且多样化，以及其关联的干扰。为 Addressing this challenge, we propose a novel energy management approach that utilizes deep reinforcement learning (DRL) algorithm to develop efficient strategies for operating AUS. Through extensive simulations, we evaluate the performance of our algorithm and demonstrate its superior effectiveness over traditional rule-based approaches and other DRL algorithms in reducing energy wastage while ensuring the stable and efficient operation of AUS. Our findings suggest that a DRL-based approach offers a promising way for improving the efficiency of AUS and enhancing the sustainability of seaweed cultivation and carbon sequestration in the ocean.
</details></li>
</ul>
<hr>
<h2 id="ProSpire-Proactive-Spatial-Prediction-of-Radio-Environment-Using-Deep-Learning"><a href="#ProSpire-Proactive-Spatial-Prediction-of-Radio-Environment-Using-Deep-Learning" class="headerlink" title="ProSpire: Proactive Spatial Prediction of Radio Environment Using Deep Learning"></a>ProSpire: Proactive Spatial Prediction of Radio Environment Using Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10193">http://arxiv.org/abs/2308.10193</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shamik Sarkar, Dongning Guo, Danijela Cabric</li>
<li>for: 这个研究旨在帮助无线网络中的输送器预测电磁环境，以提高无线网络的多样性和可靠性。</li>
<li>methods: 这个研究使用了一个新的、受到监督学习的框架，即ProSpire，以实现对输送器的探测。ProSpire 利用了一个叫做 RSSu-net 的深度学习方法，实现了对输送器的预测。</li>
<li>results: 这个研究获得了reasonable的实验结果，其中预测误差为5 dB的平均绝对误差，与其他相关方法相当。此外，ProSpire 可以实现对输送器的探测，使其可以在97%的机会下不会导致干扰。相比之下，RSSu-net 的性能比其他相似方法更好，增加了19%的可能性。<details>
<summary>Abstract</summary>
Spatial prediction of the radio propagation environment of a transmitter can assist and improve various aspects of wireless networks. The majority of research in this domain can be categorized as 'reactive' spatial prediction, where the predictions are made based on a small set of measurements from an active transmitter whose radio environment is to be predicted. Emerging spectrum-sharing paradigms would benefit from 'proactive' spatial prediction of the radio environment, where the spatial predictions must be done for a transmitter for which no measurement has been collected.   This paper proposes a novel, supervised deep learning-based framework, ProSpire, that enables spectrum sharing by leveraging the idea of proactive spatial prediction. We carefully address several challenges in ProSpire, such as designing a framework that conveniently collects training data for learning, performing the predictions in a fast manner, enabling operations without an area map, and ensuring that the predictions do not lead to undesired interference. ProSpire relies on the crowdsourcing of transmitters and receivers during their normal operations to address some of the aforementioned challenges. The core component of ProSpire is a deep learning-based image-to-image translation method, which we call RSSu-net. We generate several diverse datasets using ray tracing software and numerically evaluate ProSpire. Our evaluations show that RSSu-net performs reasonably well in terms of signal strength prediction, 5 dB mean absolute error, which is comparable to the average error of other relevant methods. Importantly, due to the merits of RSSu-net, ProSpire creates proactive boundaries around transmitters such that they can be activated with 97% probability of not causing interference. In this regard, the performance of RSSu-net is 19% better than that of other comparable methods.
</details>
<details>
<summary>摘要</summary>
通过预测广播环境，可以提高无线网络的多种方面。大多数研究在这个领域是“反应式”的预测，基于活动发送器的 радио环境进行预测。然而，新兴的spectrum-sharing paradigms需要“积极”的预测，预测发送器没有收集过数据。这篇论文提出了一种新的、深度学习基于的框架，名为ProSpire，它利用了积极预测的想法。我们仔细解决了一些挑战，例如如何便捷地收集训练数据，在快速方式进行预测，不需要地图，以及预测不导致不必要的干扰。ProSpire通过在正常运行时 solicit 发送器和接收器来解决一些上述挑战。核心组件是一种基于图像至图像翻译的深度学习方法，我们称之为RSSu-net。我们使用 ray tracing 软件生成了多种多样的数据集，并 numerically 评估了ProSpire。我们的评估结果表明，RSSu-net在信号强度预测方面表现reasonably well，相比其他相关方法的平均误差为5 dB。这意味着ProSpire可以创建积极的边界，使 transmitter 有97%的概率不会导致干扰。这种性能比其他相似方法高出19%。
</details></li>
</ul>
<hr>
<h2 id="Mimicking-To-Dominate-Imitation-Learning-Strategies-for-Success-in-Multiagent-Competitive-Games"><a href="#Mimicking-To-Dominate-Imitation-Learning-Strategies-for-Success-in-Multiagent-Competitive-Games" class="headerlink" title="Mimicking To Dominate: Imitation Learning Strategies for Success in Multiagent Competitive Games"></a>Mimicking To Dominate: Imitation Learning Strategies for Success in Multiagent Competitive Games</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10188">http://arxiv.org/abs/2308.10188</a></li>
<li>repo_url: None</li>
<li>paper_authors: The Viet Bui, Tien Mai, Thanh Hong Nguyen</li>
<li>for: 本研究旨在Addressing the challenges of training agents in multi-agent competitive games, particularly in mitigating uncertainties in game dynamics.</li>
<li>methods: 我们提出了一种新的多体学习模型，可以预测对手的下一步行动，基于隐藏的对手行动和本地观察。此外，我们还提出了一种新的多体强化学习算法，可以结合我们的模型和策略训练进行一起训练。</li>
<li>results: 我们在三个复杂的游戏环境中进行了广泛的实验，包括SMACv2。实验结果表明，我们的方法可以比现有的多体RL算法 achieve superior performance.<details>
<summary>Abstract</summary>
Training agents in multi-agent competitive games presents significant challenges due to their intricate nature. These challenges are exacerbated by dynamics influenced not only by the environment but also by opponents' strategies. Existing methods often struggle with slow convergence and instability. To address this, we harness the potential of imitation learning to comprehend and anticipate opponents' behavior, aiming to mitigate uncertainties with respect to the game dynamics. Our key contributions include: (i) a new multi-agent imitation learning model for predicting next moves of the opponents -- our model works with hidden opponents' actions and local observations; (ii) a new multi-agent reinforcement learning algorithm that combines our imitation learning model and policy training into one single training process; and (iii) extensive experiments in three challenging game environments, including an advanced version of the Star-Craft multi-agent challenge (i.e., SMACv2). Experimental results show that our approach achieves superior performance compared to existing state-of-the-art multi-agent RL algorithms.
</details>
<details>
<summary>摘要</summary>
training 多个代理人在多代理人竞争游戏中存在巨大的挑战，这些挑战受到环境以及对手策略的影响。现有方法经常受到慢性和不稳定性的影响。为了解决这些问题，我们利用仿制学来理解和预测对手的行为，以降低与游戏动力学有关的不确定性。我们的关键贡献包括：(i) 一种新的多个代理人仿制学模型，用于预测对手的下一步行动，该模型可以处理隐藏的对手行动和地方观察。(ii) 一种新的多个代理人 reinforcement learning 算法，将我们的仿制学模型和策略训练集成在一起，以实现单一的训练过程。(iii) 在三个复杂的游戏环境中进行了广泛的实验，包括 Star-Craft 多代理人挑战（SMACv2）的高级版本。实验结果表明，我们的方法可以与现有的多代理人 RL 算法相比，实现更高的性能。
</details></li>
</ul>
<hr>
<h2 id="Quantization-based-Optimization-with-Perspective-of-Quantum-Mechanics"><a href="#Quantization-based-Optimization-with-Perspective-of-Quantum-Mechanics" class="headerlink" title="Quantization-based Optimization with Perspective of Quantum Mechanics"></a>Quantization-based Optimization with Perspective of Quantum Mechanics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11594">http://arxiv.org/abs/2308.11594</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinwuk Seok, Changsik Cho</li>
<li>for: 本研究旨在探讨量子逻辑如何应用于全球优化问题中，提供一种新的研究框架。</li>
<li>methods: 本文使用量子谱分法对全球优化问题进行分析，揭示了量子力学中允许全球优化的特性。</li>
<li>results: 实验结果表明，量子谱分法中的 Tunneling 效应可以使得找到局部最优点的问题逃脱局部最优点，并且这种效应与量子力学基础的全球优化问题相同。<details>
<summary>Abstract</summary>
Statistical and stochastic analysis based on thermodynamics has been the main analysis framework for stochastic global optimization. Recently, appearing quantum annealing or quantum tunneling algorithm for global optimization, we require a new researching framework for global optimization algorithms. In this paper, we provide the analysis for quantization-based optimization based on the Schr\"odinger equation to reveal what property in quantum mechanics enables global optimization. We present that the tunneling effect derived by the Schr\"odinger equation in quantization-based optimization enables to escape of a local minimum. Additionally, we confirm that this tunneling effect is the same property included in quantum mechanics-based global optimization. Experiments with standard multi-modal benchmark functions represent that the proposed analysis is valid.
</details>
<details>
<summary>摘要</summary>
基于热力学的统计学和随机分析已经是全球优化的主要分析框架。在最近，量子气体或量子隧道算法在全球优化中出现，我们需要一个新的研究框架来探讨全球优化算法。在这篇论文中，我们提供了量子化基于Schrödinger方程的优化分析，以探索量子力学中允许全球优化的属性。我们发现，通过Schrödinger方程中的隧道效应，可以在量子化基于优化中突破本地最小值。此外，我们证明这种隧道效应与量子力学基于全球优化中的同一性。通过对标准多模式函数的实验，我们证明了我们的分析的有效性。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Client-Drift-in-Federated-Learning-A-Logit-Perspective"><a href="#Rethinking-Client-Drift-in-Federated-Learning-A-Logit-Perspective" class="headerlink" title="Rethinking Client Drift in Federated Learning: A Logit Perspective"></a>Rethinking Client Drift in Federated Learning: A Logit Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10162">http://arxiv.org/abs/2308.10162</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunlu Yan, Chun-Mei Feng, Mang Ye, Wangmeng Zuo, Ping Li, Rick Siow Mong Goh, Lei Zhu, C. L. Philip Chen</li>
<li>for: 这个研究是为了解决 Federated Learning (FL) 中 Client Drift 问题，并提高 FL 的性能。</li>
<li>methods: 本研究使用了一个新的 Class prototype Similarity Distillation (FedCSD) 算法，将本地和全球模型进行对预。 FedCSD 不仅将全球知识转移到本地客户端，因为一个未熟悉的全球模型无法提供可靠的知识，即类别相似性信息。而是将本地征推与全球原型之间的相似性用于对预。</li>
<li>results: 实验结果显示，FedCSD 在不同的多 клиєн端数据设定下表现更好，并且可以避免 Catastrophic Forgetting 问题。<details>
<summary>Abstract</summary>
Federated Learning (FL) enables multiple clients to collaboratively learn in a distributed way, allowing for privacy protection. However, the real-world non-IID data will lead to client drift which degrades the performance of FL. Interestingly, we find that the difference in logits between the local and global models increases as the model is continuously updated, thus seriously deteriorating FL performance. This is mainly due to catastrophic forgetting caused by data heterogeneity between clients. To alleviate this problem, we propose a new algorithm, named FedCSD, a Class prototype Similarity Distillation in a federated framework to align the local and global models. FedCSD does not simply transfer global knowledge to local clients, as an undertrained global model cannot provide reliable knowledge, i.e., class similarity information, and its wrong soft labels will mislead the optimization of local models. Concretely, FedCSD introduces a class prototype similarity distillation to align the local logits with the refined global logits that are weighted by the similarity between local logits and the global prototype. To enhance the quality of global logits, FedCSD adopts an adaptive mask to filter out the terrible soft labels of the global models, thereby preventing them to mislead local optimization. Extensive experiments demonstrate the superiority of our method over the state-of-the-art federated learning approaches in various heterogeneous settings. The source code will be released.
</details>
<details>
<summary>摘要</summary>
受欢迎的 Federated Learning (FL) 技术允许多个客户端共同学习，以保护隐私。然而，在实际世界中，客户端数据不够一致，导致客户端漂移，从而下降 FL 性能。我们发现，在不断更新模型时，本地和全球模型之间的差异在逐渐增加，从而严重降低 FL 性能。这主要是由于客户端数据不同性导致的慢速忘记。为解决这问题，我们提出了一种新的算法，即 FedCSD，它是一种基于联合类prototype similarity distillation的联邦框架，用于对本地和全球模型进行对齐。FedCSD不仅将全球知识传递给本地客户端，因为一个受训练不充分的全球模型无法提供可靠的类 similarity 信息，而且其错误的软标签会mislead本地优化。具体来说，FedCSD 引入一种类 prototype similarity distillation，用于对本地征标与全球 проtotypes 之间的类 similarity进行对齐。为提高全球征标的质量，FedCSD 采用了一种适应性掩模，以过滤全球模型的差异化软标签，从而避免它们对本地优化产生负面影响。我们的实验表明，FedCSD 在不同的异质设置下表现出优于当前 state-of-the-art 联邦学习方法。我们将源代码发布。
</details></li>
</ul>
<hr>
<h2 id="Resource-Adaptive-Newton’s-Method-for-Distributed-Learning"><a href="#Resource-Adaptive-Newton’s-Method-for-Distributed-Learning" class="headerlink" title="Resource-Adaptive Newton’s Method for Distributed Learning"></a>Resource-Adaptive Newton’s Method for Distributed Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10154">http://arxiv.org/abs/2308.10154</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuzhen Chen, Yuan Yuan, Youming Tao, Zhipeng Cai, Dongxiao Yu</li>
<li>for: 这篇论文是用于探讨分布式随机优化方法，特别是以新顿方法为基础，以提高性能。</li>
<li>methods: 这篇论文使用了一种名为RANL的新型和有效的算法，它利用了简单的希腊矩来初始化，并通过自适应训练区域的分配来解决新顿方法的问题。</li>
<li>results: 这篇论文的结果显示了RANL算法在数据不均匀和训练资料组件复杂的情况下仍然能够实现线性的快速减退。此外，RANL算法还能够自动适应可用资源，以确保高效率。<details>
<summary>Abstract</summary>
Distributed stochastic optimization methods based on Newton's method offer significant advantages over first-order methods by leveraging curvature information for improved performance. However, the practical applicability of Newton's method is hindered in large-scale and heterogeneous learning environments due to challenges such as high computation and communication costs associated with the Hessian matrix, sub-model diversity, staleness in training, and data heterogeneity. To address these challenges, this paper introduces a novel and efficient algorithm called RANL, which overcomes the limitations of Newton's method by employing a simple Hessian initialization and adaptive assignments of training regions. The algorithm demonstrates impressive convergence properties, which are rigorously analyzed under standard assumptions in stochastic optimization. The theoretical analysis establishes that RANL achieves a linear convergence rate while effectively adapting to available resources and maintaining high efficiency. Unlike traditional first-order methods, RANL exhibits remarkable independence from the condition number of the problem and eliminates the need for complex parameter tuning. These advantages make RANL a promising approach for distributed stochastic optimization in practical scenarios.
</details>
<details>
<summary>摘要</summary>
新类的分布式数据估计方法，基于牛顿法，可以实现更好的性能，但是它实际应用在大规模和多标的学习环境中受到一些挑战，例如牛顿矩阵的计算和通信成本高昂，模型多标的问题，训练过程中的偏预设问题，数据多标性。为了解决这些挑战，本文提出了一个新的和高效的算法，叫做RANL，它可以超越牛顿法的限制，通过简单的牛顿矩阵初始化和自适应的训练区域分配。这个算法在标准假设下进行了严谨的分析，证明了RANL可以在线性几何中实现快速的渐近稳定。不同于传统的首项方法，RANL不受问题的条件数值影响，并且不需要复杂的参数调整。这些优点使RANL成为实际应用中的分布式数据估计方法。
</details></li>
</ul>
<hr>
<h2 id="Global-Warming-In-Ghana’s-Major-Cities-Based-on-Statistical-Analysis-of-NASA’s-POWER-Over-3-Decades"><a href="#Global-Warming-In-Ghana’s-Major-Cities-Based-on-Statistical-Analysis-of-NASA’s-POWER-Over-3-Decades" class="headerlink" title="Global Warming In Ghana’s Major Cities Based on Statistical Analysis of NASA’s POWER Over 3-Decades"></a>Global Warming In Ghana’s Major Cities Based on Statistical Analysis of NASA’s POWER Over 3-Decades</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10909">http://arxiv.org/abs/2308.10909</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joshua Attih</li>
<li>for: 这项研究旨在 investigate Ghana 四大城市的长期温度趋势，以了解地方气候变化的影响和政策制定的依据。</li>
<li>methods: 该研究使用 NASA 的 Prediction of Worldwide Energy Resource (POWER) 数据，并使用统计分析和 XGBoost 机器学习算法来预测温度变化。 地表温度 profiling 图表从 RSLab 平台生成，以提高准确性。</li>
<li>results: 研究发现各城市都有本地气候变化趋势，特别是工业化的 Accra 显示明显的升高趋势。 涉及人口因素不显著。 XGBoost 模型的低 Root Mean Square Error (RMSE) 分数表明其能够准确地捕捉温度模式。 Wa  unexpectedly 的平均温度最高。 预计2023 中期 Accra 的平均温度为 27.86℃，Kumasi 为 27.15℃， Kete-Krachi 为 29.39℃， Wa 为 30.76℃。这些结果可以帮助气候变化策略的制定和实施。<details>
<summary>Abstract</summary>
Global warming's impact on high temperatures in various parts of the world has raised concerns. This study investigates long-term temperature trends in four major Ghanaian cities representing distinct climatic zones. Using NASA's Prediction of Worldwide Energy Resource (POWER) data, statistical analyses assess local climate warming and its implications. Linear regression trend analysis and eXtreme Gradient Boosting (XGBoost) machine learning predict temperature variations. Land Surface Temperature (LST) profile maps generated from the RSLab platform enhance accuracy. Results reveal local warming trends, particularly in industrialized Accra. Demographic factors aren't significant. XGBoost model's low Root Mean Square Error (RMSE) scores demonstrate effectiveness in capturing temperature patterns. Wa unexpectedly has the highest mean temperature. Estimated mean temperatures for mid-2023 are: Accra 27.86{\deg}C, Kumasi 27.15{\deg}C, Kete-Krachi 29.39{\deg}C, and Wa 30.76{\deg}C. These findings improve understanding of local climate warming for policymakers and communities, aiding climate change strategies.
</details>
<details>
<summary>摘要</summary>
全球气候变化对各地高温的影响已引发了关注。这项研究对四个加纳城市进行了长期气温趋势分析，这些城市代表了不同的气候区。使用NASA的Prediction of Worldwide Energy Resource（POWER）数据，统计分析评估了地方气候变暖的影响。线性回归方法和极限梯度提升（XGBoost）机器学习方法预测温度变化。RSLab平台生成的土地表面温度（LST）profile图表提高了准确性。结果显示了地方气候变暖趋势，特别是工业化的阿克拉。人口因素没有显著影响。XGBoost模型的低根据平方误差（RMSE）得分表明其能够准确捕捉温度模式。意外地，华有最高的平均温度。预计2023年中的温度为：阿克拉27.86℃，库马西27.15℃，别克拉29.39℃，和华30.76℃。这些发现可以帮助气候变化策略的制定和社区的决策。
</details></li>
</ul>
<hr>
<h2 id="OCHID-Fi-Occlusion-Robust-Hand-Pose-Estimation-in-3D-via-RF-Vision"><a href="#OCHID-Fi-Occlusion-Robust-Hand-Pose-Estimation-in-3D-via-RF-Vision" class="headerlink" title="OCHID-Fi: Occlusion-Robust Hand Pose Estimation in 3D via RF-Vision"></a>OCHID-Fi: Occlusion-Robust Hand Pose Estimation in 3D via RF-Vision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10146">http://arxiv.org/abs/2308.10146</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shujie Zhang, Tianyue Zheng, Zhe Chen, Jingzhi Hu, Abdelwahed Khamis, Jiajun Liu, Jun Luo<br>for: occluded hand pose estimationmethods: RF-vision and adversarial learningresults: comparable accuracy to CM-HPE under normal conditions, maintains accuracy in occluded scenarios, generalizable to new domains.Here’s the simplified Chinese text:for:  occluded手势识别methods: RF-vision和对抗学习results: 与CM-HPE在正常情况下具有相同的准确率，在遮盖情况下保持准确率，并能在新领域中普适应用.<details>
<summary>Abstract</summary>
Hand Pose Estimation (HPE) is crucial to many applications, but conventional cameras-based CM-HPE methods are completely subject to Line-of-Sight (LoS), as cameras cannot capture occluded objects. In this paper, we propose to exploit Radio-Frequency-Vision (RF-vision) capable of bypassing obstacles for achieving occluded HPE, and we introduce OCHID-Fi as the first RF-HPE method with 3D pose estimation capability. OCHID-Fi employs wideband RF sensors widely available on smart devices (e.g., iPhones) to probe 3D human hand pose and extract their skeletons behind obstacles. To overcome the challenge in labeling RF imaging given its human incomprehensible nature, OCHID-Fi employs a cross-modality and cross-domain training process. It uses a pre-trained CM-HPE network and a synchronized CM/RF dataset, to guide the training of its complex-valued RF-HPE network under LoS conditions. It further transfers knowledge learned from labeled LoS domain to unlabeled occluded domain via adversarial learning, enabling OCHID-Fi to generalize to unseen occluded scenarios. Experimental results demonstrate the superiority of OCHID-Fi: it achieves comparable accuracy to CM-HPE under normal conditions while maintaining such accuracy even in occluded scenarios, with empirical evidence for its generalizability to new domains.
</details>
<details>
<summary>摘要</summary>
手势识别（HPE）是许多应用程序的关键，但传统的相机基于CM-HPE方法是完全依赖于直线视野（LoS），因为相机无法捕捉遮盖物体。在这篇论文中，我们提议利用Radio-Frequency-Vision（RF-vision）来绕过障碍物实现遮盖物体HPE，并介绍了OCHID-Fi作为首个RF-HPE方法，具有3D手势 pose estimation能力。OCHID-Fi利用了广泛可用的智能设备（如iPhone）上的宽频RF传感器来探测3D人手势 pose和其骨架，并在障碍物下实现了高精度的手势识别。为了解决RF图像标注的挑战，OCHID-Fi采用了交叉模式和交叉领域训练过程。它使用了预训练的CM-HPE网络和同步CM/RF数据集，以导引其复杂的RF-HPE网络在LoS条件下进行训练。它还通过对LoS频谱频谱中的标注进行反向传播学习，使OCHID-Fi能够在未看到障碍物的情况下泛化到新领域。实验结果表明，OCHID-Fi具有较高的精度和泛化能力，可以在正常情况下与CM-HPE具有相同的精度，而在障碍物下仍然保持高精度，并且在新领域中进行泛化。
</details></li>
</ul>
<hr>
<h2 id="Wasserstein-Geodesic-Generator-for-Conditional-Distributions"><a href="#Wasserstein-Geodesic-Generator-for-Conditional-Distributions" class="headerlink" title="Wasserstein Geodesic Generator for Conditional Distributions"></a>Wasserstein Geodesic Generator for Conditional Distributions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10145">http://arxiv.org/abs/2308.10145</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kyg0910/wasserstein-geodesic-generator-for-conditional-distributions">https://github.com/kyg0910/wasserstein-geodesic-generator-for-conditional-distributions</a></li>
<li>paper_authors: Young-geun Kim, Kyungbok Lee, Youngwon Choi, Joong-Ho Won, Myunghee Cho Paik</li>
<li>for: 这篇论文是用于研究如何获得高品质的条件生成。</li>
<li>methods: 论文使用估计条件分布的方法，包括 derive 一个可诠释的上界 bound 来定义条件分布，并使用 optimal transport 理论来设计一个名为 Wasserstein geodesic generator 的新型条件生成器。</li>
<li>results: 实验结果显示，提案的方法可以高效地学习条件分布，并可以生成高品质的条件生成。<details>
<summary>Abstract</summary>
Generating samples given a specific label requires estimating conditional distributions. We derive a tractable upper bound of the Wasserstein distance between conditional distributions to lay the theoretical groundwork to learn conditional distributions. Based on this result, we propose a novel conditional generation algorithm where conditional distributions are fully characterized by a metric space defined by a statistical distance. We employ optimal transport theory to propose the Wasserstein geodesic generator, a new conditional generator that learns the Wasserstein geodesic. The proposed method learns both conditional distributions for observed domains and optimal transport maps between them. The conditional distributions given unobserved intermediate domains are on the Wasserstein geodesic between conditional distributions given two observed domain labels. Experiments on face images with light conditions as domain labels demonstrate the efficacy of the proposed method.
</details>
<details>
<summary>摘要</summary>
<<SYS>计算样本 Given Specific标签需要估算conditional Distributions。我们得出了可观察 Wasserstein distance的Upper bound，以准备 theoretically learn conditional Distributions。 Based on this result, we propose a novel conditional generation algorithm, where conditional distributions are fully characterized by a metric space defined by a statistical distance. We employ optimal transport theory to propose the Wasserstein geodesic generator, a new conditional generator that learns the Wasserstein geodesic. The proposed method learns both conditional distributions for observed domains and optimal transport maps between them. The conditional distributions given unobserved intermediate domains are on the Wasserstein geodesic between conditional distributions given two observed domain labels. Experiments on face images with light conditions as domain labels demonstrate the efficacy of the proposed method. tranlation notes:* "conditional distributions" translated as "条件分布"* "Wasserstein distance" translated as "沃斯坦距离"* "metric space" translated as "度量空间"* "optimal transport theory" translated as "最优运输理论"* "Wasserstein geodesic" translated as "沃斯坦曲线"* "conditional generator" translated as "条件生成器"* "observed domains" translated as "观察Domain"* "unobserved intermediate domains" translated as "未观察中间Domain"* "light conditions" translated as "照明条件"Please note that the translation is in Simplified Chinese, and the word order may be different from the original text.
</details></li>
</ul>
<hr>
<h2 id="ExpeL-LLM-Agents-Are-Experiential-Learners"><a href="#ExpeL-LLM-Agents-Are-Experiential-Learners" class="headerlink" title="ExpeL: LLM Agents Are Experiential Learners"></a>ExpeL: LLM Agents Are Experiential Learners</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10144">http://arxiv.org/abs/2308.10144</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Andrewzh112/ExpeL">https://github.com/Andrewzh112/ExpeL</a></li>
<li>paper_authors: Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, Gao Huang</li>
<li>for: 这篇论文的目的是提出一种新的语言模型学习方法，以便在做决策任务时不需要进行参数更新。</li>
<li>methods: 该方法使用自然语言从训练任务中收集经验，并通过自动提取知识来做出 Informed 决策。</li>
<li>results: 该方法在实验中表现出了Robust 的学习效果，表明随着经验的积累，模型的性能会不断提高。<details>
<summary>Abstract</summary>
The recent surge in research interest in applying large language models (LLMs) to decision-making tasks has flourished by leveraging the extensive world knowledge embedded in LLMs. While there is a growing demand to tailor LLMs for custom decision-making tasks, finetuning them for specific tasks is resource-intensive and may diminish the model's generalization capabilities. Moreover, state-of-the-art language models like GPT-4 and Claude are primarily accessible through API calls, with their parametric weights remaining proprietary and unavailable to the public. This scenario emphasizes the growing need for new methodologies that allow learning from agent experiences without requiring parametric updates. To address these problems, we introduce the Experiential Learning (ExpeL) agent. Our agent autonomously gathers experiences and extracts knowledge using natural language from a collection of training tasks. At inference, the agent recalls its extracted insights and past experiences to make informed decisions. Our empirical results highlight the robust learning efficacy of the ExpeL agent, indicating a consistent enhancement in its performance as it accumulates experiences. We further explore the emerging capabilities and transfer learning potential of the ExpeL agent through qualitative observations and additional experiments.
</details>
<details>
<summary>摘要</summary>
To address these issues, we propose the Experiential Learning (ExpeL) agent. Our agent autonomously gathers experiences and extracts knowledge using natural language from a collection of training tasks. At inference, the agent recalls its extracted insights and past experiences to make informed decisions. Our empirical results show that the ExpeL agent exhibits robust learning efficacy, with its performance consistently improving as it accumulates experiences. We also explore the emerging capabilities and transfer learning potential of the ExpeL agent through qualitative observations and additional experiments.
</details></li>
</ul>
<hr>
<h2 id="A-Review-on-Objective-Driven-Artificial-Intelligence"><a href="#A-Review-on-Objective-Driven-Artificial-Intelligence" class="headerlink" title="A Review on Objective-Driven Artificial Intelligence"></a>A Review on Objective-Driven Artificial Intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10135">http://arxiv.org/abs/2308.10135</a></li>
<li>repo_url: None</li>
<li>paper_authors: Apoorv Singh</li>
<li>for: 本文旨在探讨人工智能（AI）技术目前的缺陷，以及如何通过层次规划和能量基、隐变量方法等方法来减少人机智能差距。</li>
<li>methods: 本文主要使用层次规划、能量基、隐变量方法等方法来探讨人工智能技术的发展和应用。</li>
<li>results: 本文指出，现有的AI技术尚未能够准确地理解人类的语言和情感表达，而层次规划和能量基、隐变量方法等方法可以帮助人工智能技术更好地理解人类的语言和情感表达。<details>
<summary>Abstract</summary>
While advancing rapidly, Artificial Intelligence still falls short of human intelligence in several key aspects due to inherent limitations in current AI technologies and our understanding of cognition. Humans have an innate ability to understand context, nuances, and subtle cues in communication, which allows us to comprehend jokes, sarcasm, and metaphors. Machines struggle to interpret such contextual information accurately. Humans possess a vast repository of common-sense knowledge that helps us make logical inferences and predictions about the world. Machines lack this innate understanding and often struggle with making sense of situations that humans find trivial. In this article, we review the prospective Machine Intelligence candidates, a review from Prof. Yann LeCun, and other work that can help close this gap between human and machine intelligence. Specifically, we talk about what's lacking with the current AI techniques such as supervised learning, reinforcement learning, self-supervised learning, etc. Then we show how Hierarchical planning-based approaches can help us close that gap and deep-dive into energy-based, latent-variable methods and Joint embedding predictive architecture methods.
</details>
<details>
<summary>摘要</summary>
artifical intelligence 在快速发展中，但仍然缺乏人类智能的一些关键方面，这主要归结于当前的 AI 技术和我们认知神经科学的限制。人类有内置的理解上下文、涵义和微妙提示的能力，使得我们能够理解讽刺、讽刺和 метаFOR。机器则很难准确地理解这些上下文信息。人类拥有庞大的通用智能知识，帮助我们做出逻辑推理和世界上的预测。机器缺乏这种内置的理解，经常陷入人类轻视的情况。在这篇文章中，我们评论了当前的机器智能候选人，包括Prof. Yann LeCun的评论以及其他工作，以帮助关闭人类和机器智能之间的差距。我们讨论了当前 AI 技术的缺陷，如监督学习、奖励学习、自监学习等。然后，我们介绍了层次规划基础的方法，可以帮助我们关闭这个差距。我们还深入探讨了能量基础、隐变量方法和联合嵌入预测架构。
</details></li>
</ul>
<hr>
<h2 id="AutoReP-Automatic-ReLU-Replacement-for-Fast-Private-Network-Inference"><a href="#AutoReP-Automatic-ReLU-Replacement-for-Fast-Private-Network-Inference" class="headerlink" title="AutoReP: Automatic ReLU Replacement for Fast Private Network Inference"></a>AutoReP: Automatic ReLU Replacement for Fast Private Network Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10134">http://arxiv.org/abs/2308.10134</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/harveyp123/autorep">https://github.com/harveyp123/autorep</a></li>
<li>paper_authors: Hongwu Peng, Shaoyi Huang, Tong Zhou, Yukui Luo, Chenghong Wang, Zigeng Wang, Jiahui Zhao, Xi Xie, Ang Li, Tony Geng, Kaleel Mahmood, Wujie Wen, Xiaolin Xu, Caiwen Ding<br>for: This paper aims to address the data privacy and security issues in Machine-Learning-As-A-Service (MLaaS) market by proposing a gradient-based approach called AutoReP, which reduces the number of non-linear operators and maintains model expressivity.methods: The proposed AutoReP method uses gradient-based approach to select appropriate ReLU and polynomial functions for private inference, and introduces distribution-aware polynomial approximation (DaPa) to accurately approximate ReLUs.results: The experimental results on CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets show significant accuracy improvements over current state-of-the-art methods, e.g., SNL. Specifically, the accuracy improvements are 6.12%, 8.39%, and 9.45%, respectively. Additionally, AutoReP is applied to EfficientNet-B2 on ImageNet dataset, achieving 75.55% accuracy with 176.1 times ReLU budget reduction.<details>
<summary>Abstract</summary>
The growth of the Machine-Learning-As-A-Service (MLaaS) market has highlighted clients' data privacy and security issues. Private inference (PI) techniques using cryptographic primitives offer a solution but often have high computation and communication costs, particularly with non-linear operators like ReLU. Many attempts to reduce ReLU operations exist, but they may need heuristic threshold selection or cause substantial accuracy loss. This work introduces AutoReP, a gradient-based approach to lessen non-linear operators and alleviate these issues. It automates the selection of ReLU and polynomial functions to speed up PI applications and introduces distribution-aware polynomial approximation (DaPa) to maintain model expressivity while accurately approximating ReLUs. Our experimental results demonstrate significant accuracy improvements of 6.12% (94.31%, 12.9K ReLU budget, CIFAR-10), 8.39% (74.92%, 12.9K ReLU budget, CIFAR-100), and 9.45% (63.69%, 55K ReLU budget, Tiny-ImageNet) over current state-of-the-art methods, e.g., SNL. Morever, AutoReP is applied to EfficientNet-B2 on ImageNet dataset, and achieved 75.55% accuracy with 176.1 times ReLU budget reduction.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Intelligent-Communication-Planning-for-Constrained-Environmental-IoT-Sensing-with-Reinforcement-Learning"><a href="#Intelligent-Communication-Planning-for-Constrained-Environmental-IoT-Sensing-with-Reinforcement-Learning" class="headerlink" title="Intelligent Communication Planning for Constrained Environmental IoT Sensing with Reinforcement Learning"></a>Intelligent Communication Planning for Constrained Environmental IoT Sensing with Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10124">http://arxiv.org/abs/2308.10124</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yi Hu, Jinhang Zuo, Bob Iannucci, Carlee Joe-Wong</li>
<li>for: 这篇论文旨在提高环境监测和风险警告，通过部署一个网络的物联网设备（IoT）。</li>
<li>methods: 该论文使用多代理学习（MARL）方法来优化物联网设备的通信策略，以最大化环境数据跟踪准确性，同时满足功率和带宽限制。</li>
<li>results: 实验表明，使用MARL方法可以学习和利用环境数据的空间时间相关性，以减少物联网设备的重复报告。<details>
<summary>Abstract</summary>
Internet of Things (IoT) technologies have enabled numerous data-driven mobile applications and have the potential to significantly improve environmental monitoring and hazard warnings through the deployment of a network of IoT sensors. However, these IoT devices are often power-constrained and utilize wireless communication schemes with limited bandwidth. Such power constraints limit the amount of information each device can share across the network, while bandwidth limitations hinder sensors' coordination of their transmissions. In this work, we formulate the communication planning problem of IoT sensors that track the state of the environment. We seek to optimize sensors' decisions in collecting environmental data under stringent resource constraints. We propose a multi-agent reinforcement learning (MARL) method to find the optimal communication policies for each sensor that maximize the tracking accuracy subject to the power and bandwidth limitations. MARL learns and exploits the spatial-temporal correlation of the environmental data at each sensor's location to reduce the redundant reports from the sensors. Experiments on wildfire spread with LoRA wireless network simulators show that our MARL method can learn to balance the need to collect enough data to predict wildfire spread with unknown bandwidth limitations.
</details>
<details>
<summary>摘要</summary>
互联网物件技术（IoT）已经启用了许多数据驱动的移动应用程序，并有潜力增强环境监控和险情警告通过网络设置 IoT 感知器。然而，这些 IoT 设备通常受限于能源和无线通信协议的限制，这限制每个设备可以在网络上分享的资讯量，而且对于感知器的传输协议的协调也受到限制。在这个工作中，我们将环境监控 IoT 感知器的通信规划问题形式化为一个最佳化问题，以最大化感知器对环境状态的追踪精度，同时遵循能源和传输协议的限制。我们提出了一种基于多代理问题学习（MARL）方法，以便每个感知器可以在网络上传输环境数据，并且可以适当地调整传输策略，以最大化追踪精度。实验结果显示，我们的 MARL 方法可以在不知道传输协议的情况下，对野火传播进行精确的预测。
</details></li>
</ul>
<hr>
<h2 id="Deep-Generative-Modeling-based-Data-Augmentation-with-Demonstration-using-the-BFBT-Benchmark-Void-Fraction-Datasets"><a href="#Deep-Generative-Modeling-based-Data-Augmentation-with-Demonstration-using-the-BFBT-Benchmark-Void-Fraction-Datasets" class="headerlink" title="Deep Generative Modeling-based Data Augmentation with Demonstration using the BFBT Benchmark Void Fraction Datasets"></a>Deep Generative Modeling-based Data Augmentation with Demonstration using the BFBT Benchmark Void Fraction Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10120">http://arxiv.org/abs/2308.10120</a></li>
<li>repo_url: None</li>
<li>paper_authors: Farah Alsafadi, Xu Wu</li>
<li>for: 这个论文的目的是用深度学习（DL）技术来扩展科学数据，以便更好地训练深度学习模型。</li>
<li>methods: 这个论文使用了深度生成模型（DGM），包括生成敌对网络（GAN）、标准化流（NF）、变换自动编码器（VAE）和条件VAE（CVAE）等，来学习训练数据集的下面分布。</li>
<li>results: 研究发现，使用DGM生成的 sintetic数据可以覆盖现有训练数据的限制，并且可以减少训练数据的缺失值。CVAEs的生成性能最佳，其生成的数据具有最小的错误。这些结果表明，DGM可以有效地扩展科学数据，并且可以帮助深度学习模型更加准确地训练。<details>
<summary>Abstract</summary>
Deep learning (DL) has achieved remarkable successes in many disciplines such as computer vision and natural language processing due to the availability of ``big data''. However, such success cannot be easily replicated in many nuclear engineering problems because of the limited amount of training data, especially when the data comes from high-cost experiments. To overcome such a data scarcity issue, this paper explores the applications of deep generative models (DGMs) that have been widely used for image data generation to scientific data augmentation. DGMs, such as generative adversarial networks (GANs), normalizing flows (NFs), variational autoencoders (VAEs), and conditional VAEs (CVAEs), can be trained to learn the underlying probabilistic distribution of the training dataset. Once trained, they can be used to generate synthetic data that are similar to the training data and significantly expand the dataset size. By employing DGMs to augment TRACE simulated data of the steady-state void fractions based on the NUPEC Boiling Water Reactor Full-size Fine-mesh Bundle Test (BFBT) benchmark, this study demonstrates that VAEs, CVAEs, and GANs have comparable generative performance with similar errors in the synthetic data, with CVAEs achieving the smallest errors. The findings shows that DGMs have a great potential to augment scientific data in nuclear engineering, which proves effective for expanding the training dataset and enabling other DL models to be trained more accurately.
</details>
<details>
<summary>摘要</summary>
深度学习（DL）在许多领域取得了很大成功，如计算机视觉和自然语言处理，这主要归功于大量数据的可用性。然而，在核工程问题中，由于数据的有限性，特别是高成本实验室中的数据，因此复制这种成功是不容易的。为解决这个数据缺乏问题，本文探讨了在科学数据增强方面使用深度生成模型（DGM）的应用。DGM包括生成对抗网络（GAN）、Normalizing Flows（NF）、Variational Autoencoders（VAE）和 Conditional VAEs（CVAE）等，可以在训练数据集的下采用学习下面的概率分布。一旦训练完成，它们可以生成与训练数据相似的 sintetic 数据，并大大增加数据集的大小。本研究通过使用 DGM 增强 TRACE 仿真数据，基于 NUPEC Boiling Water Reactor Full-size Fine-mesh Bundle Test（BFBT）标准底本，展示了 VAE、CVAE 和 GAN 在生成数据时的相似性，CVAE 的错误最小。这些发现表明 DGM 在核工程领域有很大的潜力，可以增强数据增强模型的准确性，从而提高核工程领域的研究效果。
</details></li>
</ul>
<hr>
<h2 id="Modeling-Random-Networks-with-Heterogeneous-Reciprocity"><a href="#Modeling-Random-Networks-with-Heterogeneous-Reciprocity" class="headerlink" title="Modeling Random Networks with Heterogeneous Reciprocity"></a>Modeling Random Networks with Heterogeneous Reciprocity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10113">http://arxiv.org/abs/2308.10113</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Cirkovic, Tiandong Wang</li>
<li>For: 本研究旨在模型社交网络中不同水平的反馈行为。* Methods: 本文提出了一种偏好附加模型，用于模拟用户吸引受欢迎用户的行为，以及不同水平的反馈行为。文章还比较了 bayesian 和 frequentist 模型适应技术，以及计算效率高的变量方案。* Results: 对 Facebook 墙上的墙文网络进行分析，发现用户的反馈行为呈差异性分布，并使用模型预测用户之间的连接关系。模型能够捕捉 Facebook 数据中的重概率分布，并分别确定了多个用户群体的反馈行为特征。<details>
<summary>Abstract</summary>
Reciprocity, or the tendency of individuals to mirror behavior, is a key measure that describes information exchange in a social network. Users in social networks tend to engage in different levels of reciprocal behavior. Differences in such behavior may indicate the existence of communities that reciprocate links at varying rates. In this paper, we develop methodology to model the diverse reciprocal behavior in growing social networks. In particular, we present a preferential attachment model with heterogeneous reciprocity that imitates the attraction users have for popular users, plus the heterogeneous nature by which they reciprocate links. We compare Bayesian and frequentist model fitting techniques for large networks, as well as computationally efficient variational alternatives. Cases where the number of communities are known and unknown are both considered. We apply the presented methods to the analysis of a Facebook wallpost network where users have non-uniform reciprocal behavior patterns. The fitted model captures the heavy-tailed nature of the empirical degree distributions in the Facebook data and identifies multiple groups of users that differ in their tendency to reply to and receive responses to wallposts.
</details>
<details>
<summary>摘要</summary>
“循环性”或“模仿行为”是社交网络中信息交换的关键指标。社交网络中的用户通常在不同的水平上进行反馈行为。不同的反馈行为可能表示社交网络中存在不同的社群，这些社群在链接reciprocate的速率上有所不同。在这篇论文中，我们开发了模型社交网络中多样化反馈行为的方法ологи。特别是，我们提出了具有异质反馈的偏好附着模型，该模型模拟用户循环行为的吸引力和不同的反馈方式。我们比较了 bayesian 和频率主义模型适应技术，以及计算效率高的变量替代方法。我们还考虑了知道和不知道社群数量的两种情况。我们应用这些方法分析Facebook墙上的墙posts网络，发现用户的反馈行为具有不均匀的特点，并确定了不同的用户群体。Note: "循环性" (reciprocity) in the text refers to the tendency of individuals to mirror behavior in a social network.
</details></li>
</ul>
<hr>
<h2 id="Robust-Mixture-of-Expert-Training-for-Convolutional-Neural-Networks"><a href="#Robust-Mixture-of-Expert-Training-for-Convolutional-Neural-Networks" class="headerlink" title="Robust Mixture-of-Expert Training for Convolutional Neural Networks"></a>Robust Mixture-of-Expert Training for Convolutional Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10110">http://arxiv.org/abs/2308.10110</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/optml-group/robust-moe-cnn">https://github.com/optml-group/robust-moe-cnn</a></li>
<li>paper_authors: Yihua Zhang, Ruisi Cai, Tianlong Chen, Guanhua Zhang, Huan Zhang, Pin-Yu Chen, Shiyu Chang, Zhangyang Wang, Sijia Liu</li>
<li>for: 本研究旨在探讨如何使用 sparse-gated Mixture of Expert (MoE) 模型提高 convolutional neural networks (CNNs) 的鲁棒性。</li>
<li>methods: 本研究提出了一种新的 router-expert alternating Adversarial training 框架，以提高 MoE-CNN 模型的鲁棒性。</li>
<li>results: 实验结果表明，相比 dense CNN，AdvMoE 可以提高 adversarial robustness 1% ~ 4%，同时具有较高的计算效率，减少了 более半个 inference cost。<details>
<summary>Abstract</summary>
Sparsely-gated Mixture of Expert (MoE), an emerging deep model architecture, has demonstrated a great promise to enable high-accuracy and ultra-efficient model inference. Despite the growing popularity of MoE, little work investigated its potential to advance convolutional neural networks (CNNs), especially in the plane of adversarial robustness. Since the lack of robustness has become one of the main hurdles for CNNs, in this paper we ask: How to adversarially robustify a CNN-based MoE model? Can we robustly train it like an ordinary CNN model? Our pilot study shows that the conventional adversarial training (AT) mechanism (developed for vanilla CNNs) no longer remains effective to robustify an MoE-CNN. To better understand this phenomenon, we dissect the robustness of an MoE-CNN into two dimensions: Robustness of routers (i.e., gating functions to select data-specific experts) and robustness of experts (i.e., the router-guided pathways defined by the subnetworks of the backbone CNN). Our analyses show that routers and experts are hard to adapt to each other in the vanilla AT. Thus, we propose a new router-expert alternating Adversarial training framework for MoE, termed AdvMoE. The effectiveness of our proposal is justified across 4 commonly-used CNN model architectures over 4 benchmark datasets. We find that AdvMoE achieves 1% ~ 4% adversarial robustness improvement over the original dense CNN, and enjoys the efficiency merit of sparsity-gated MoE, leading to more than 50% inference cost reduction. Codes are available at https://github.com/OPTML-Group/Robust-MoE-CNN.
</details>
<details>
<summary>摘要</summary>
这是一篇研究档案，探讨了一种新的深度学习模型架构，即零价对抗性混合专家（MoE）。这种模型架构已经展示了高精度和高效率的推断能力。 despite its growing popularity, little work has been done to investigate its potential to improve convolutional neural networks (CNNs)， especially in the area of adversarial robustness. therefore, we ask: how to adversarially robustify a CNN-based MoE model? can we train it like an ordinary CNN model? our preliminary study shows that the conventional adversarial training (AT) mechanism (developed for vanilla CNNs) is no longer effective to robustify an MoE-CNN. to better understand this phenomenon, we dissect the robustness of an MoE-CNN into two dimensions: the robustness of routers (i.e., gating functions to select data-specific experts) and the robustness of experts (i.e., the router-guided pathways defined by the subnetworks of the backbone CNN). our analyses show that routers and experts are difficult to adapt to each other in the vanilla AT. therefore, we propose a new router-expert alternating adversarial training framework for MoE, termed AdvMoE. the effectiveness of our proposal is justified across 4 commonly-used CNN model architectures over 4 benchmark datasets. we find that AdvMoE achieves 1% to 4% adversarial robustness improvement over the original dense CNN, and enjoys the efficiency merit of sparsity-gated MoE, leading to more than 50% inference cost reduction. codes are available at https://github.com/OPTML-Group/Robust-MoE-CNN.
</details></li>
</ul>
<hr>
<h2 id="An-Online-Multiple-Kernel-Parallelizable-Learning-Scheme"><a href="#An-Online-Multiple-Kernel-Parallelizable-Learning-Scheme" class="headerlink" title="An Online Multiple Kernel Parallelizable Learning Scheme"></a>An Online Multiple Kernel Parallelizable Learning Scheme</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10101">http://arxiv.org/abs/2308.10101</a></li>
<li>repo_url: None</li>
<li>paper_authors: Emilio Ruiz-Moreno, Baltasar Beferull-Lozano</li>
<li>for: 提高单kernel方法的选择灵活性和计算效率，使其能够更好地适应数据丰富任务中的解决空间。</li>
<li>methods: 基于多kernel学习形式，可以将单kernel解决空间扩展到更广泛的解决空间，从而提高解决空间内的性能。同时，该学习形式可以并行化，以便分配计算负担到不同的计算单元。</li>
<li>results: 在实验中，提出的学习方案比单个单kernel方法相比，在累积较少的最小二乘loss metric上表现出更高的性能。<details>
<summary>Abstract</summary>
The performance of reproducing kernel Hilbert space-based methods is known to be sensitive to the choice of the reproducing kernel. Choosing an adequate reproducing kernel can be challenging and computationally demanding, especially in data-rich tasks without prior information about the solution domain. In this paper, we propose a learning scheme that scalably combines several single kernel-based online methods to reduce the kernel-selection bias. The proposed learning scheme applies to any task formulated as a regularized empirical risk minimization convex problem. More specifically, our learning scheme is based on a multi-kernel learning formulation that can be applied to widen any single-kernel solution space, thus increasing the possibility of finding higher-performance solutions. In addition, it is parallelizable, allowing for the distribution of the computational load across different computing units. We show experimentally that the proposed learning scheme outperforms the combined single-kernel online methods separately in terms of the cumulative regularized least squares cost metric.
</details>
<details>
<summary>摘要</summary>
“kernel Hilbert space-based方法的性能受选择kernel的影响很大。选择合适的kernel可以是一个问题，特别是在没有对解题域的内部信息的情况下。本文提出了一个学习方案，可以可扩展性地结合多个单kernel-based在线方法，以减少kernel-选择偏见。这个学习方案适用于任何形式化为单倍阶调整项目的问题。更 Specifically, our learning scheme is based on a multi-kernel learning formulation that can be applied to widen any single-kernel solution space, thus increasing the possibility of finding higher-performance solutions. In addition, it is parallelizable, allowing for the distribution of the computational load across different computing units. We show experimentally that the proposed learning scheme outperforms the combined single-kernel online methods separately in terms of the cumulative regularized least squares cost metric.”Note: The translation is in Simplified Chinese, which is one of the two standard versions of Chinese. The other version is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Geometric-instability-of-graph-neural-networks-on-large-graphs"><a href="#Geometric-instability-of-graph-neural-networks-on-large-graphs" class="headerlink" title="Geometric instability of graph neural networks on large graphs"></a>Geometric instability of graph neural networks on large graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10099">http://arxiv.org/abs/2308.10099</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/brs96/geometric-instability-gnn-large-graphs">https://github.com/brs96/geometric-instability-gnn-large-graphs</a></li>
<li>paper_authors: Emily Morris, Haotian Shen, Weiling Du, Muhammad Hamza Sajjad, Borun Shi</li>
<li>for: 研究图 Néural Networks（GNNs）中的几何不稳定性。</li>
<li>methods: 提出了一种简单、高效的图native Graph Gram Index（GGI）来测量这种不稳定性，该指标具有卷积、旋转、平移和评估顺序不敏感的特点。</li>
<li>results: 通过研究大图上GNN embedding的不稳定性，发现 embeddings的不稳定性随图的大小而变化，并且在node classification和链接预测 tasks上有不同的不稳定性行为。<details>
<summary>Abstract</summary>
We analyse the geometric instability of embeddings produced by graph neural networks (GNNs). Existing methods are only applicable for small graphs and lack context in the graph domain. We propose a simple, efficient and graph-native Graph Gram Index (GGI) to measure such instability which is invariant to permutation, orthogonal transformation, translation and order of evaluation. This allows us to study the varying instability behaviour of GNN embeddings on large graphs for both node classification and link prediction.
</details>
<details>
<summary>摘要</summary>
我们分析图 neural network (GNN) 生成的嵌入的几何不稳定性。现有方法只适用于小图，缺乏图域上的上下文。我们提议一种简单、高效、图原生的图agram Gram Index (GGI) 来衡量这种不稳定性，该指标是对Permutation、旋转、平移和评估顺序进行 invariant。这使得我们可以研究 GNN 嵌入在大图上的不同不稳定行为，包括节点分类和链接预测。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Bilevel-Learning-with-Inexact-Line-Search"><a href="#Dynamic-Bilevel-Learning-with-Inexact-Line-Search" class="headerlink" title="Dynamic Bilevel Learning with Inexact Line Search"></a>Dynamic Bilevel Learning with Inexact Line Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10098">http://arxiv.org/abs/2308.10098</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Sadegh Salehi, Subhadip Mukherjee, Lindon Roberts, Matthias J. Ehrhardt</li>
<li>for: 这篇论文的目的是解决在各种图像和数据科学领域中，特别是使用变分regularization方法时，手动配置正则化参数的问题。</li>
<li>methods: 这篇论文使用了积分学习来学习适当的正则化参数。</li>
<li>results: 这篇论文的实验结果表明，该方法可以有效地解决手动配置正则化参数的问题，并且可以根据实际需求动态地确定需要的精度。<details>
<summary>Abstract</summary>
In various domains within imaging and data science, particularly when addressing tasks modeled utilizing the variational regularization approach, manually configuring regularization parameters presents a formidable challenge. The difficulty intensifies when employing regularizers involving a large number of hyperparameters. To overcome this challenge, bilevel learning is employed to learn suitable hyperparameters. However, due to the use of numerical solvers, the exact gradient with respect to the hyperparameters is unattainable, necessitating the use of methods relying on approximate gradients. State-of-the-art inexact methods a priori select a decreasing summable sequence of the required accuracy and only assure convergence given a sufficiently small fixed step size. Despite this, challenges persist in determining the Lipschitz constant of the hypergradient and identifying an appropriate fixed step size. Conversely, computing exact function values is not feasible, impeding the use of line search. In this work, we introduce a provably convergent inexact backtracking line search involving inexact function evaluations and hypergradients. We show convergence to a stationary point of the loss with respect to hyperparameters. Additionally, we propose an algorithm to determine the required accuracy dynamically. Our numerical experiments demonstrate the efficiency and feasibility of our approach for hyperparameter estimation in variational regularization problems, alongside its robustness in terms of the initial accuracy and step size choices.
</details>
<details>
<summary>摘要</summary>
在各种图像和数据科学领域中，特别是使用变分regularization方法解决问题时，手动配置正则化参数是一项具有挑战性的任务。这种挑战会加剧，当使用含有大量hyperparameter的正则izers时。为了解决这个问题，我们使用笛卡尔学习来学习适当的hyperparameter。然而，由于使用数值解析器，不能获取正则izers的精确梯度，因此需要使用 Approximate Gradients 的方法。现有的 state-of-the-art 不精确方法会选择一个递减和可加性的精度要求，并且只有在 sufficiently small fixed step size 下才能确保收敛。然而，在确定 Lipschitz 常数和选择合适的fixed step size 方面，仍然存在挑战。另外，计算精确函数值是不可能的，从而阻碍了使用线搜索。在这种情况下，我们提出了一种可证明收敛的不精确返回搜索，该搜索利用不精确函数评估和正则izers的梯度。我们证明了该方法可以收敛到正则izers中的一个站点点。此外，我们还提出了一种动态确定所需的精度的算法。我们的数值实验表明，我们的方法可以有效地进行正则izers的优化，并且具有较好的Robustness 性。
</details></li>
</ul>
<hr>
<h2 id="MLOps-A-Review"><a href="#MLOps-A-Review" class="headerlink" title="MLOps: A Review"></a>MLOps: A Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10908">http://arxiv.org/abs/2308.10908</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jenningst/ecommerce-ops">https://github.com/jenningst/ecommerce-ops</a></li>
<li>paper_authors: Samar Wazir, Gautam Siddharth Kashyap, Parag Saxena</li>
<li>for: The paper aims to explore the significance of Machine Learning Operations (MLOps) methods and assess their features and operability to help create software that is simple to use.</li>
<li>methods: The authors evaluate 22 papers that attempted to apply the MLOps idea and assess the features and operability of various MLOps methods.</li>
<li>results: The authors conclude that there is a scarcity of fully effective MLOps methods that can self-regulate by limiting human engagement.Here’s the same information in Simplified Chinese text:</li>
<li>for: 该论文旨在探讨机器学习运维（MLOps）方法的重要性和评估各种MLOps方法的特性和操作性，以帮助创建简单易用的软件。</li>
<li>methods: 作者评估了22篇应用了MLOps想法的论文，并评估各种MLOps方法的特性和操作性。</li>
<li>results: 作者认为，目前 még 缺乏完全有效的MLOps方法，可以通过限制人类参与来自动化进程。<details>
<summary>Abstract</summary>
Recently, Machine Learning (ML) has become a widely accepted method for significant progress that is rapidly evolving. Since it employs computational methods to teach machines and produce acceptable answers. The significance of the Machine Learning Operations (MLOps) methods, which can provide acceptable answers for such problems, is examined in this study. To assist in the creation of software that is simple to use, the authors research MLOps methods. To choose the best tool structure for certain projects, the authors also assess the features and operability of various MLOps methods. A total of 22 papers were assessed that attempted to apply the MLOps idea. Finally, the authors admit the scarcity of fully effective MLOps methods based on which advancements can self-regulate by limiting human engagement.
</details>
<details>
<summary>摘要</summary>
最近，机器学习（ML）已成为广泛接受的方法，迅速发展。由于它利用计算方法教育机器并生成可接受的答案。本研究研究机器学习操作（MLOps）方法，以便为解决这些问题提供可靠的答案。为便于创建简单易用的软件，作者研究了不同的MLOps方法。为选择特定项目适用的最佳工具结构，作者还评估了各种MLOps方法的特性和操作性。本研究总共评估了22篇尝试应用MLOps想法的论文。最后，作者承认MLOps方法的完全可效性尚未得到保证，尤其是限制人类参与的情况下。
</details></li>
</ul>
<hr>
<h2 id="Securing-Pathways-with-Orthogonal-Robots"><a href="#Securing-Pathways-with-Orthogonal-Robots" class="headerlink" title="Securing Pathways with Orthogonal Robots"></a>Securing Pathways with Orthogonal Robots</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10093">http://arxiv.org/abs/2308.10093</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hamid Hoorfar, Faraneh Fathi, Sara Moshtaghi Largani, Alireza Bagheri</li>
<li>for: 本研究旨在提供一种新型的路径保护方法，使用正交机器人来保护路径。</li>
<li>methods: 本研究使用了正交机器人来 efficiently guard orthogonal areas with the minimum number of orthogonal robots.</li>
<li>results: 研究表明可以在线时确定最小数量的正交机器人，以保护路径。<details>
<summary>Abstract</summary>
The protection of pathways holds immense significance across various domains, including urban planning, transportation, surveillance, and security. This article introduces a groundbreaking approach to safeguarding pathways by employing orthogonal robots. The study specifically addresses the challenge of efficiently guarding orthogonal areas with the minimum number of orthogonal robots. The primary focus is on orthogonal pathways, characterized by a path-like dual graph of vertical decomposition. It is demonstrated that determining the minimum number of orthogonal robots for pathways can be achieved in linear time. However, it is essential to note that the general problem of finding the minimum number of robots for simple polygons with general visibility, even in the orthogonal case, is known to be NP-hard. Emphasis is placed on the flexibility of placing robots anywhere within the polygon, whether on the boundary or in the interior.
</details>
<details>
<summary>摘要</summary>
保护路径在多个领域，如城市规划、交通、监视和安全方面，具有极高的重要性。本文介绍了一种创新的路径保护方法，利用正交机器人。研究特点在于有效地使用最少的正交机器人来保护正交区域。研究主要关注正交路径，即笛卡尔分解中的路径 dual graph。实验表明，可以在线性时间内确定最少正交机器人数量。然而，需要注意的是，对于简单多边形的通用可见情况，即使是正交情况，找到最少机器人数量的问题是NP困难的。研究强调了机器人的位置可以在多边形边界上或者在内部。
</details></li>
</ul>
<hr>
<h2 id="Minimizing-Turns-in-Watchman-Robot-Navigation-Strategies-and-Solutions"><a href="#Minimizing-Turns-in-Watchman-Robot-Navigation-Strategies-and-Solutions" class="headerlink" title="Minimizing Turns in Watchman Robot Navigation: Strategies and Solutions"></a>Minimizing Turns in Watchman Robot Navigation: Strategies and Solutions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10090">http://arxiv.org/abs/2308.10090</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hamid Hoorfar, Sara Moshtaghi Largani, Reza Rahimi, Alireza Bagheri</li>
<li>For: 这篇论文的目的是解决监视人员路径问题（OWRP），即在多边形环境中找到最短路径，使机器人可以在一次连续扫描整个环境。* Methods: 本研究使用了有效的线性时间算法，解决OWRP问题，假设环境是单调的。* Results: 研究发现，这种算法可以减少机器人转弯的数量，从而提高机器人在观察和监视方面的效率，并且可以应用于各种实际应用中。<details>
<summary>Abstract</summary>
The Orthogonal Watchman Route Problem (OWRP) entails the search for the shortest path, known as the watchman route, that a robot must follow within a polygonal environment. The primary objective is to ensure that every point in the environment remains visible from at least one point on the route, allowing the robot to survey the entire area in a single, continuous sweep. This research places particular emphasis on reducing the number of turns in the route, as it is crucial for optimizing navigation in watchman routes within the field of robotics. The cost associated with changing direction is of significant importance, especially for specific types of robots. This paper introduces an efficient linear-time algorithm for solving the OWRP under the assumption that the environment is monotone. The findings of this study contribute to the progress of robotic systems by enabling the design of more streamlined patrol robots. These robots are capable of efficiently navigating complex environments while minimizing the number of turns. This advancement enhances their coverage and surveillance capabilities, making them highly effective in various real-world applications.
</details>
<details>
<summary>摘要</summary>
orthogonal 看守路径问题 (OWRP) 是关于找到最短路径，也称为看守路径，Robot在多边形环境中移动的问题。主要目标是确保环境中每个点都可以在一个点上看到，使Robot在一次连续扫描中涵盖整个区域。这种研究强调减少路径转弯数量，因为这对于某些类型的Robot来说非常重要。这篇论文介绍了一种高效的直线时间算法，用于解决 OWRP，假设环境是均匀的。这些发现对于 robotic 系统的进步做出了贡献，可以设计更加流畅的执勤Robot，这些Robot可以在复杂环境中高效巡捕，最小化转弯数量，提高覆盖和监测能力，使其在实际应用中表现出色。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Learning-for-Non-Local-Graphs-with-Multi-Resolution-Structural-Views"><a href="#Contrastive-Learning-for-Non-Local-Graphs-with-Multi-Resolution-Structural-Views" class="headerlink" title="Contrastive Learning for Non-Local Graphs with Multi-Resolution Structural Views"></a>Contrastive Learning for Non-Local Graphs with Multi-Resolution Structural Views</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10077">http://arxiv.org/abs/2308.10077</a></li>
<li>repo_url: None</li>
<li>paper_authors: Asif Khan, Amos Storkey</li>
<li>for: 学习不同类型图的节点水平表示，用于识别骗子和蛋白质功能预测等应用。</li>
<li>methods: 提出了一种基于多视图对比学习的新方法，该方法通过在图上执行扩散缓动来捕捉高级别图 структуры，从而提高节点表示的准确性。</li>
<li>results: 在synthetic和实际结构数据上比较baseline，该方法的表现较佳，胜过best baseline by 16.06% on Cornell, 3.27% on Texas, and 8.04% on Wisconsin。此外，它在邻近任务上也表现出优异，这表明它有效地捕捉了结构信息，并且可以提高下游应用的性能。<details>
<summary>Abstract</summary>
Learning node-level representations of heterophilic graphs is crucial for various applications, including fraudster detection and protein function prediction. In such graphs, nodes share structural similarity identified by the equivalence of their connectivity which is implicitly encoded in the form of higher-order hierarchical information in the graphs. The contrastive methods are popular choices for learning the representation of nodes in a graph. However, existing contrastive methods struggle to capture higher-order graph structures. To address this limitation, we propose a novel multiview contrastive learning approach that integrates diffusion filters on graphs. By incorporating multiple graph views as augmentations, our method captures the structural equivalence in heterophilic graphs, enabling the discovery of hidden relationships and similarities not apparent in traditional node representations. Our approach outperforms baselines on synthetic and real structural datasets, surpassing the best baseline by $16.06\%$ on Cornell, $3.27\%$ on Texas, and $8.04\%$ on Wisconsin. Additionally, it consistently achieves superior performance on proximal tasks, demonstrating its effectiveness in uncovering structural information and improving downstream applications.
</details>
<details>
<summary>摘要</summary>
学习不同类型图的节点级别表示是关键的，包括诈器检测和蛋白质功能预测。在这些图中，节点具有同类结构，可以通过节点之间的连接相似性来隐式地编码在图中。对比方法是选择学习节点在图中的表示方法，但现有的对比方法很难捕捉更高级别的图结构。为解决这些限制，我们提出了一种新的多视图对比学习方法，该方法在图中integration多个视图作为增强。通过这种方法，我们可以捕捉到不同类型图中节点之间的结构相似性，从而找到隐藏的关系和相似性，不同于传统节点表示。我们的方法在 sintetic和实际结构数据上比基eline表现出色，胜过最佳基eline的$16.06\%$在 Cornell，$3.27\%$在 Texas，和$8.04\%$在 Wisconsin。此外，我们的方法在邻近任务上持续表现出色，证明它的有效性在揭示结构信息和提高下游应用中。
</details></li>
</ul>
<hr>
<h2 id="ILCAS-Imitation-Learning-Based-Configuration-Adaptive-Streaming-for-Live-Video-Analytics-with-Cross-Camera-Collaboration"><a href="#ILCAS-Imitation-Learning-Based-Configuration-Adaptive-Streaming-for-Live-Video-Analytics-with-Cross-Camera-Collaboration" class="headerlink" title="ILCAS: Imitation Learning-Based Configuration-Adaptive Streaming for Live Video Analytics with Cross-Camera Collaboration"></a>ILCAS: Imitation Learning-Based Configuration-Adaptive Streaming for Live Video Analytics with Cross-Camera Collaboration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10068">http://arxiv.org/abs/2308.10068</a></li>
<li>repo_url: None</li>
<li>paper_authors: Duo Wu, Dayou Zhang, Miao Zhang, Ruoyu Zhang, Fangxin Wang, Shuguang Cui</li>
<li>for: 这个论文目的是为了提出一个基于模仿学习的配置适应式视频分析系统（ILCAS），以减少对网络带宽的需求和处理时间，并且能够适应不同的视频内容变化。</li>
<li>methods: ILCAS使用了视频动态特征地图和镜头间协力机制，以捕捉视频内容变化并选择适当的配置。它还使用了专家示范的对策来训练代理人，通过动态计划来解决配置适应问题。</li>
<li>results: 实验结果显示，ILCAS比之前的方案有2-20.9%的增加精度和19.9-85.3%的减少块上传延迟。<details>
<summary>Abstract</summary>
The high-accuracy and resource-intensive deep neural networks (DNNs) have been widely adopted by live video analytics (VA), where camera videos are streamed over the network to resource-rich edge/cloud servers for DNN inference. Common video encoding configurations (e.g., resolution and frame rate) have been identified with significant impacts on striking the balance between bandwidth consumption and inference accuracy and therefore their adaption scheme has been a focus of optimization. However, previous profiling-based solutions suffer from high profiling cost, while existing deep reinforcement learning (DRL) based solutions may achieve poor performance due to the usage of fixed reward function for training the agent, which fails to craft the application goals in various scenarios. In this paper, we propose ILCAS, the first imitation learning (IL) based configuration-adaptive VA streaming system. Unlike DRL-based solutions, ILCAS trains the agent with demonstrations collected from the expert which is designed as an offline optimal policy that solves the configuration adaption problem through dynamic programming. To tackle the challenge of video content dynamics, ILCAS derives motion feature maps based on motion vectors which allow ILCAS to visually ``perceive'' video content changes. Moreover, ILCAS incorporates a cross-camera collaboration scheme to exploit the spatio-temporal correlations of cameras for more proper configuration selection. Extensive experiments confirm the superiority of ILCAS compared with state-of-the-art solutions, with 2-20.9% improvement of mean accuracy and 19.9-85.3% reduction of chunk upload lag.
</details>
<details>
<summary>摘要</summary>
高精度和资源占用深度神经网络（DNN）在实时视频分析（VA）中广泛应用，其中摄像头视频流经网络传输至 Edge/云服务器进行DNN推理。常见的视频编码配置（例如分辨率和帧率）有显著影响于占用带宽和推理精度的平衡，因此其适应方案成为优化的焦点。然而，先前的 Profiling-based 解决方案具有高 Profiling 成本，而现有的深度强化学习（DRL）基于解决方案可能因为使用固定的奖励函数进行训练代理人，导致在不同enario中拟合应用目标的问题。在本文中，我们提出了 ILCAS，第一个仿学学习（IL）基于配置适应VA流动系统。与 DRL-based 解决方案不同，ILCAS 通过收集专家设计的示例来训练代理人，通过动态计划解决配置适应问题。为了解决视频内容变化的挑战，ILCAS  derivates 基于运动 вектор的动作特征图， Allow ILCAS 在视觉上“感受”视频内容变化。此外，ILCAS 还实现了相机间协作机制，以便更好地选择适配。经验证明，ILCAS 相比现有的解决方案具有2-20.9%的提升精度和19.9-85.3%的减少块上传延迟。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/20/cs.LG_2023_08_20/" data-id="clm0t8e0j007av788b68w8k1x" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_08_20" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/20/cs.SD_2023_08_20/" class="article-date">
  <time datetime="2023-08-19T16:00:00.000Z" itemprop="datePublished">2023-08-20</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/20/cs.SD_2023_08_20/">cs.SD - 2023-08-20 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Indonesian-Automatic-Speech-Recognition-with-XLSR-53"><a href="#Indonesian-Automatic-Speech-Recognition-with-XLSR-53" class="headerlink" title="Indonesian Automatic Speech Recognition with XLSR-53"></a>Indonesian Automatic Speech Recognition with XLSR-53</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11589">http://arxiv.org/abs/2308.11589</a></li>
<li>repo_url: None</li>
<li>paper_authors: Panji Arisaputra, Amalia Zahra</li>
<li>for: 这个研究旨在开发一个使用 XLSR-53 预训练模型的印度尼西亚自动语音识别（ASR）系统，以减少不英语语言需要的训练数据量以 достичь竞争性 Word Error Rate（WER）。</li>
<li>methods: 这个研究使用了 14 小时、31 分、1 秒的 TITML-IDN 数据集、3 小时、33 分的 Magic Data 数据集和 6 小时、14 分、1 秒的 Common Voice 数据集，并使用了一个语言模型来降低 WER 约 8%，从 20% 降低到 12%。</li>
<li>results: 根据这个研究，可以成功地完善之前的研究，创造一个更好的印度尼西亚 ASR 系统，只需要一小部分的数据。WER 的值为 20%，比 similiar 模型使用 Common Voice 数据集的分test更低。<details>
<summary>Abstract</summary>
This study focuses on the development of Indonesian Automatic Speech Recognition (ASR) using the XLSR-53 pre-trained model, the XLSR stands for cross-lingual speech representations. The use of this XLSR-53 pre-trained model is to significantly reduce the amount of training data in non-English languages required to achieve a competitive Word Error Rate (WER). The total amount of data used in this study is 24 hours, 18 minutes, and 1 second: (1) TITML-IDN 14 hours and 31 minutes; (2) Magic Data 3 hours and 33 minutes; and (3) Common Voice 6 hours, 14 minutes, and 1 second. With a WER of 20%, the model built in this study can compete with similar models using the Common Voice dataset split test. WER can be decreased by around 8% using a language model, resulted in WER from 20% to 12%. Thus, the results of this study have succeeded in perfecting previous research in contributing to the creation of a better Indonesian ASR with a smaller amount of data.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>TITML-IDN: 14 hours and 31 minutes2. Magic Data: 3 hours and 33 minutes3. Common Voice: 6 hours, 14 minutes, and 1 secondWith a WER of 20%, the model built in this study can compete with similar models using the Common Voice dataset split test. Additionally, using a language model can decrease WER by around 8%, resulting in a WER of 12%. Therefore, the results of this study have successfully built upon previous research and contributed to the creation of a better Indonesian ASR with a smaller amount of data.</details></li>
</ol>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/20/cs.SD_2023_08_20/" data-id="clm0t8e1e00agv78829mj0lw1" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_08_20" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/20/eess.IV_2023_08_20/" class="article-date">
  <time datetime="2023-08-19T16:00:00.000Z" itemprop="datePublished">2023-08-20</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/20/eess.IV_2023_08_20/">eess.IV - 2023-08-20 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Boosting-Adversarial-Transferability-by-Block-Shuffle-and-Rotation"><a href="#Boosting-Adversarial-Transferability-by-Block-Shuffle-and-Rotation" class="headerlink" title="Boosting Adversarial Transferability by Block Shuffle and Rotation"></a>Boosting Adversarial Transferability by Block Shuffle and Rotation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10299">http://arxiv.org/abs/2308.10299</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kunyu Wang, Xuanran He, Wenxuan Wang, Xiaosen Wang</li>
<li>for: 防御深度学习模型受到攻击，即使使用黑盒 Setting 进行攻击。</li>
<li>methods: 使用输入变换基于的攻击方法，包括块洗混淆和旋转（BSR）。</li>
<li>results: BSR 可以在单模型和ensemble模型 Setting 下达到更高的传输性能，并且可以与现有的输入变换方法相结合，以获得更好的传输性能。<details>
<summary>Abstract</summary>
Adversarial examples mislead deep neural networks with imperceptible perturbations and have brought significant threats to deep learning. An important aspect is their transferability, which refers to their ability to deceive other models, thus enabling attacks in the black-box setting. Though various methods have been proposed to boost transferability, the performance still falls short compared with white-box attacks. In this work, we observe that existing input transformation based attacks, one of the mainstream transfer-based attacks, result in different attention heatmaps on various models, which might limit the transferability. We also find that breaking the intrinsic relation of the image can disrupt the attention heatmap of the original image. Based on this finding, we propose a novel input transformation based attack called block shuffle and rotation (BSR). Specifically, BSR splits the input image into several blocks, then randomly shuffles and rotates these blocks to construct a set of new images for gradient calculation. Empirical evaluations on the ImageNet dataset demonstrate that BSR could achieve significantly better transferability than the existing input transformation based methods under single-model and ensemble-model settings. Combining BSR with the current input transformation method can further improve the transferability, which significantly outperforms the state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
深度学习中的对抗性示例会使深度神经网络受到不可见的扰动，对深度学习带来了重要的威胁。其中一个重要方面是它们的传输性，即它们能够在黑盒Setting中欺骗其他模型，因此可以实现黑盒攻击。虽然多种方法已经被提出来提高传输性，但其性能仍然落后于白盒攻击。在这项工作中，我们发现现有的输入变换基于攻击，一种主流的传输基于攻击，会导致不同的注意度热图在不同的模型上。我们还发现，打破图像的内在关系可以阻断原始图像的注意度热图。基于这一发现，我们提出了一种新的输入变换基于攻击方法，即块拼接和旋转（BSR）。具体来说，BSR将输入图像分成多个块，然后随机拼接和旋转这些块来构建一组新的图像，用于计算梯度。我们的实验表明，BSR可以在单模型和集成模型设置下 achieves significantly better transferability than现有的输入变换基于攻击方法。同时，BSR与现有的输入变换方法相结合可以进一步提高传输性，并且与当前的状态态-of-the-art方法相比，显著超越。
</details></li>
</ul>
<hr>
<h2 id="Domain-Reduction-Strategy-for-Non-Line-of-Sight-Imaging"><a href="#Domain-Reduction-Strategy-for-Non-Line-of-Sight-Imaging" class="headerlink" title="Domain Reduction Strategy for Non Line of Sight Imaging"></a>Domain Reduction Strategy for Non Line of Sight Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10269">http://arxiv.org/abs/2308.10269</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hyunbo Shim, In Cho, Daekyu Kwon, Seon Joo Kim</li>
<li>for: 非直线视野（NLOS）图像重建</li>
<li>methods: 利用独立计算各点隐藏物变数的方法，并将处理隐藏表面的函数组合成一个更加简洁的问题</li>
<li>results: 在多种NLOS情况下，包括非平面镜壁、稀疏扫描图、对焦和非对焦图像重建等，该方法能够提供高效且稳定的重建结果<details>
<summary>Abstract</summary>
This paper presents a novel optimization-based method for non-line-of-sight (NLOS) imaging that aims to reconstruct hidden scenes under various setups. Our method is built upon the observation that photons returning from each point in hidden volumes can be independently computed if the interactions between hidden surfaces are trivially ignored. We model the generalized light propagation function to accurately represent the transients as a linear combination of these functions. Moreover, our proposed method includes a domain reduction procedure to exclude empty areas of the hidden volumes from the set of propagation functions, thereby improving computational efficiency of the optimization. We demonstrate the effectiveness of the method in various NLOS scenarios, including non-planar relay wall, sparse scanning patterns, confocal and non-confocal, and surface geometry reconstruction. Experiments conducted on both synthetic and real-world data clearly support the superiority and the efficiency of the proposed method in general NLOS scenarios.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Crucial-Feature-Capture-and-Discrimination-for-Limited-Training-Data-SAR-ATR"><a href="#Crucial-Feature-Capture-and-Discrimination-for-Limited-Training-Data-SAR-ATR" class="headerlink" title="Crucial Feature Capture and Discrimination for Limited Training Data SAR ATR"></a>Crucial Feature Capture and Discrimination for Limited Training Data SAR ATR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10911">http://arxiv.org/abs/2308.10911</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cwwangsaratr/saratr_feacapture_discrimination">https://github.com/cwwangsaratr/saratr_feacapture_discrimination</a></li>
<li>paper_authors: Chenwei Wang, Siyi Luo, Jifang Pei, Yulin Huang, Yin Zhang, Jianyu Yang</li>
<li>for: 本研究旨在提高SAR ATR的表现，对于受限 Training 数据的情况下。</li>
<li>methods: 本研究使用了两个分支和两个模组，包括全球帮助分支和本地增强分支、特征捕捉模组和特征分别模组。在每次训练中，全球帮助分支首先完成了初始识别，基于整个图像。然后，特征捕捉模组自动搜寻并锁定了重要的图像区域，我们称之为“图像金钥”。最后，本地增强分支对于这些捕捉的图像区域进行了本地特征抽取，并将其与全局特征进行了联合识别。</li>
<li>results: 本研究的模型验证实验和比较显示，在受限 Training 数据的情况下，我们的方法实现了更好的识别表现，包括改善特征分布和识别概率。在MSTAR和OPENSAR上进行的实验和比较显示，我们的方法已经 дости得了superior的识别表现。<details>
<summary>Abstract</summary>
Although deep learning-based methods have achieved excellent performance on SAR ATR, the fact that it is difficult to acquire and label a lot of SAR images makes these methods, which originally performed well, perform weakly. This may be because most of them consider the whole target images as input, but the researches find that, under limited training data, the deep learning model can't capture discriminative image regions in the whole images, rather focus on more useless even harmful image regions for recognition. Therefore, the results are not satisfactory. In this paper, we design a SAR ATR framework under limited training samples, which mainly consists of two branches and two modules, global assisted branch and local enhanced branch, feature capture module and feature discrimination module. In every training process, the global assisted branch first finishes the initial recognition based on the whole image. Based on the initial recognition results, the feature capture module automatically searches and locks the crucial image regions for correct recognition, which we named as the golden key of image. Then the local extract the local features from the captured crucial image regions. Finally, the overall features and local features are input into the classifier and dynamically weighted using the learnable voting parameters to collaboratively complete the final recognition under limited training samples. The model soundness experiments demonstrate the effectiveness of our method through the improvement of feature distribution and recognition probability. The experimental results and comparisons on MSTAR and OPENSAR show that our method has achieved superior recognition performance.
</details>
<details>
<summary>摘要</summary>
尽管深度学习基本方法在 Synthetic Aperture Radar（SAR）特征识别（ATR）中表现出色，但由于获取和标注大量SAR图像困难，这些方法在限制性训练样本的情况下表现弱。这可能是因为大多数方法将整个目标图像作为输入，但研究人员发现，在有限训练样本下，深度学习模型无法在整个图像中捕捉有利特征区域，而是偏向更无用或甚至有害的图像区域进行识别。因此，结果不满足要求。在这篇论文中，我们设计了一个基于有限训练样本的SAR ATR框架，它包括两个支线和两个模块：全球协助支线和本地增强支线，特征捕捉模块和特征分类模块。在每次训练过程中，全球协助支线首先根据整个图像完成初步识别。基于初步识别结果，特征捕捉模块自动搜索和锁定图像中重要的关键区域，我们称之为图像的“金钥匙”。然后，本地EXTRACT本地特征从捕捉到的关键图像区域。最后，总特征和本地特征被输入到分类器并使用学习投票参数进行协同完成最终识别。实验证明我们的方法的有效性通过特征分布和识别概率的改进。实验结果和相对比较表明，我们的方法在MSTAR和OPENSAR上达到了最高的识别性能。
</details></li>
</ul>
<hr>
<h2 id="An-Entropy-Awareness-Meta-Learning-Method-for-SAR-Open-Set-ATR"><a href="#An-Entropy-Awareness-Meta-Learning-Method-for-SAR-Open-Set-ATR" class="headerlink" title="An Entropy-Awareness Meta-Learning Method for SAR Open-Set ATR"></a>An Entropy-Awareness Meta-Learning Method for SAR Open-Set ATR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10251">http://arxiv.org/abs/2308.10251</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenwei Wang, Siyi Luo, Jifang Pei, Xiaoyu Liu, Yulin Huang, Yin Zhang, Jianyu Yang</li>
<li>for: 提高Synthetic Aperture Radar自动目标识别（SAR ATR）方法的泛化能力，特别是处理未知类目标的开放集成识别（OSR）问题。</li>
<li>methods: 提出一种基于自适应学习和熵觉知的元学习方法，通过元学习任务学习构建动态分配的知识类别特征空间，并通过熵觉知损失进一步增强特征空间的效果和Robustness。</li>
<li>results: 在MSTAR数据集上进行实验，显示了方法的效果性和可靠性。<details>
<summary>Abstract</summary>
Existing synthetic aperture radar automatic target recognition (SAR ATR) methods have been effective for the classification of seen target classes. However, it is more meaningful and challenging to distinguish the unseen target classes, i.e., open set recognition (OSR) problem, which is an urgent problem for the practical SAR ATR. The key solution of OSR is to effectively establish the exclusiveness of feature distribution of known classes. In this letter, we propose an entropy-awareness meta-learning method that improves the exclusiveness of feature distribution of known classes which means our method is effective for not only classifying the seen classes but also encountering the unseen other classes. Through meta-learning tasks, the proposed method learns to construct a feature space of the dynamic-assigned known classes. This feature space is required by the tasks to reject all other classes not belonging to the known classes. At the same time, the proposed entropy-awareness loss helps the model to enhance the feature space with effective and robust discrimination between the known and unknown classes. Therefore, our method can construct a dynamic feature space with discrimination between the known and unknown classes to simultaneously classify the dynamic-assigned known classes and reject the unknown classes. Experiments conducted on the moving and stationary target acquisition and recognition (MSTAR) dataset have shown the effectiveness of our method for SAR OSR.
</details>
<details>
<summary>摘要</summary>
现有的Synthetic Aperture Radar自动目标识别（SAR ATR）方法已经有效地对seen标签类进行分类。然而，更重要和挑战性的是分inguished the unseen target classes，即open set recognition（OSR）问题，这是实际SAR ATR中的紧迫问题。OSR的关键解决方案是有效地建立known classes的特征分布的 exclusiveness。在这封信中，我们提出了一种基于entropy的meta-学习方法，该方法可以提高known classes的特征分布的 exclusiveness，这意味着我们的方法不仅能够分类seen classes，还能够遇到未seen的其他类。通过meta-学习任务，我们的方法学习了construct a feature space of the dynamic-assigned known classes。这个feature space是需要由任务拒绝所有不属于known classes的其他类。同时，我们的entropy-awareness loss帮助模型增强feature space的有效和Robust的 distinguish between known和unknown classes。因此，我们的方法可以construct a dynamic feature space with discrimination between known和unknown classes，同时分类dynamic-assigned known classes和拒绝unknown classes。在MSTAR dataset上进行的实验表明了我们的方法对SAR OSR的效iveness。
</details></li>
</ul>
<hr>
<h2 id="SAR-Ship-Target-Recognition-via-Selective-Feature-Discrimination-and-Multifeature-Center-Classifier"><a href="#SAR-Ship-Target-Recognition-via-Selective-Feature-Discrimination-and-Multifeature-Center-Classifier" class="headerlink" title="SAR Ship Target Recognition via Selective Feature Discrimination and Multifeature Center Classifier"></a>SAR Ship Target Recognition via Selective Feature Discrimination and Multifeature Center Classifier</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10250">http://arxiv.org/abs/2308.10250</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenwei Wang, Siyi Luo, Jifang Pei, Yulin Huang, Yin Zhang, Jianyu Yang</li>
<li>for: 这 paper 是为了提高 SAR 图像识别精度，特别是在减少训练样本数量的情况下。</li>
<li>methods: 本 paper 提出了一种基于选择性特征强制和多特征中心分类器的 SAR 船target 识别方法。该方法包括自动选择与最相似 Inter-class 图像对的相似特征，以及与最不相似 Inner-class 图像对的不相似特征，然后对这些特征进行增强。此外，本方法还使用分类器分配多个学习可能的特征中心，以 conquering 大 inner-class variance。</li>
<li>results: 实验结果表明，本方法在 OpenSARShip 和 FUSAR-Ship 数据集上 achieved 较高的识别精度，而且可以在减少训练 SAR 船样本数量的情况下保持高精度。<details>
<summary>Abstract</summary>
Maritime surveillance is not only necessary for every country, such as in maritime safeguarding and fishing controls, but also plays an essential role in international fields, such as in rescue support and illegal immigration control. Most of the existing automatic target recognition (ATR) methods directly send the extracted whole features of SAR ships into one classifier. The classifiers of most methods only assign one feature center to each class. However, the characteristics of SAR ship images, large inner-class variance, and small interclass difference lead to the whole features containing useless partial features and a single feature center for each class in the classifier failing with large inner-class variance. We proposes a SAR ship target recognition method via selective feature discrimination and multifeature center classifier. The selective feature discrimination automatically finds the similar partial features from the most similar interclass image pairs and the dissimilar partial features from the most dissimilar inner-class image pairs. It then provides a loss to enhance these partial features with more interclass separability. Motivated by divide and conquer, the multifeature center classifier assigns multiple learnable feature centers for each ship class. In this way, the multifeature centers divide the large inner-class variance into several smaller variances and conquered by combining all feature centers of one ship class. Finally, the probability distribution over all feature centers is considered comprehensively to achieve an accurate recognition of SAR ship images. The ablation experiments and experimental results on OpenSARShip and FUSAR-Ship datasets show that our method has achieved superior recognition performance under decreasing training SAR ship samples.
</details>
<details>
<summary>摘要</summary>
海上监控不仅是每个国家的必需，如海上安全和渔业控制，而且在国际领域也扮演着重要角色，如搜救支持和非法移民控制。现有的自动目标识别（ATR）方法大多直接将抽取的整个特征集送入一个分类器。但是SAR船图像的特征是巨大的内类差异和小的间类差异，导致整个特征集含有无用的部分特征和分类器中的每个特征中心都是一个。我们提出了一种基于选择性特征分化和多特征中心分类器的SAR船目标识别方法。选择性特征分化自动从最相似的Interclass图像对中找到相似的部分特征和最不相似的内类图像对中找到不相似的部分特征，然后为这些部分特征提供损失来提高它们的Interclass分离度。受分和胜利的思想 inspirited by divide and conquer，多特征中心分类器将每个船类分配多个学习的特征中心。这样，多特征中心将大内类差异分解成多个小差异，并且通过所有特征中心的组合来实现一个船类的准确识别。最后，对所有特征中心的概率分布进行全面考虑以实现高精度的SAR船图像识别。ablation experiment和OpenSARShip和FUSAR-Ship数据集的实验结果表明，我们的方法在减少的训练SAR船样本下实现了优秀的识别性能。
</details></li>
</ul>
<hr>
<h2 id="SAR-Ship-Target-Recognition-Via-Multi-Scale-Feature-Attention-and-Adaptive-Weighed-Classifier"><a href="#SAR-Ship-Target-Recognition-Via-Multi-Scale-Feature-Attention-and-Adaptive-Weighed-Classifier" class="headerlink" title="SAR Ship Target Recognition Via Multi-Scale Feature Attention and Adaptive-Weighed Classifier"></a>SAR Ship Target Recognition Via Multi-Scale Feature Attention and Adaptive-Weighed Classifier</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10247">http://arxiv.org/abs/2308.10247</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenwei Wang, Jifang Pei, Siyi Luo, Weibo Huo, Yulin Huang, Yin Zhang, Jianyu Yang</li>
<li>for: 本研究旨在提高Synthetic Aperture Radar（SAR）船target recognition的精度，提供了一种基于多scale feature attention和自适应权重分类器的方法。</li>
<li>methods: 该方法首先在网络中构建了一个内网络特征 piramid，以EXTRACT多scale特征从SAR船图像中。然后，使用多scale feature attention可以提取和增强多scale特征中的主要成分，具有更高的内类紧凑性和 между类分离性。最后，使用自适应权重分类器选择有效的特征层次，以实现最终的精度 recognition。</li>
<li>results: 通过对OpenSARship数据集进行实验和比较，该方法被证明可以达到SAR船target recognition的 estado-of-the-art表现。<details>
<summary>Abstract</summary>
Maritime surveillance is indispensable for civilian fields, including national maritime safeguarding, channel monitoring, and so on, in which synthetic aperture radar (SAR) ship target recognition is a crucial research field. The core problem to realizing accurate SAR ship target recognition is the large inner-class variance and inter-class overlap of SAR ship features, which limits the recognition performance. Most existing methods plainly extract multi-scale features of the network and utilize equally each feature scale in the classification stage. However, the shallow multi-scale features are not discriminative enough, and each scale feature is not equally effective for recognition. These factors lead to the limitation of recognition performance. Therefore, we proposed a SAR ship recognition method via multi-scale feature attention and adaptive-weighted classifier to enhance features in each scale, and adaptively choose the effective feature scale for accurate recognition. We first construct an in-network feature pyramid to extract multi-scale features from SAR ship images. Then, the multi-scale feature attention can extract and enhance the principal components from the multi-scale features with more inner-class compactness and inter-class separability. Finally, the adaptive weighted classifier chooses the effective feature scales in the feature pyramid to achieve the final precise recognition. Through experiments and comparisons under OpenSARship data set, the proposed method is validated to achieve state-of-the-art performance for SAR ship recognition.
</details>
<details>
<summary>摘要</summary>
海上监测是民用领域不可或缺的，包括国家海上安全、水道监测等， Synthetic Aperture Radar（SAR）船TargetRecognition是一个重要的研究领域。 however， Due to the large inner-class variance and inter-class overlap of SAR ship features, accurate recognition is limited. Most existing methods simply extract multi-scale features from the network and use each feature scale equally in the classification stage. However, the shallow multi-scale features are not discriminative enough, and each scale feature is not equally effective for recognition. These factors limit the recognition performance. Therefore, we proposed a SAR ship recognition method based on multi-scale feature attention and adaptive-weighted classifiers to enhance features in each scale and adaptively choose the effective feature scale for accurate recognition.First, we construct an in-network feature pyramid to extract multi-scale features from SAR ship images. Then, the multi-scale feature attention can extract and enhance the principal components from the multi-scale features with more inner-class compactness and inter-class separability. Finally, the adaptive weighted classifier chooses the effective feature scales in the feature pyramid to achieve the final precise recognition. Through experiments and comparisons under OpenSARship dataset, the proposed method is validated to achieve state-of-the-art performance for SAR ship recognition.
</details></li>
</ul>
<hr>
<h2 id="SAR-ATR-Method-with-Limited-Training-Data-via-an-Embedded-Feature-Augmenter-and-Dynamic-Hierarchical-Feature-Refiner"><a href="#SAR-ATR-Method-with-Limited-Training-Data-via-an-Embedded-Feature-Augmenter-and-Dynamic-Hierarchical-Feature-Refiner" class="headerlink" title="SAR ATR Method with Limited Training Data via an Embedded Feature Augmenter and Dynamic Hierarchical-Feature Refiner"></a>SAR ATR Method with Limited Training Data via an Embedded Feature Augmenter and Dynamic Hierarchical-Feature Refiner</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10243">http://arxiv.org/abs/2308.10243</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenwei Wang, Siyi Luo, Yulin Huang, Jifang Pei, Yin Zhang, Jianyu Yang</li>
<li>for: 增进 Synthetic Aperture Radar（SAR）自动目标识别（ATR）性能，特别是在有限的训练数据情况下。</li>
<li>methods: 提出了一种新的方法，包括嵌入特征增强器和动态层次特征细化器。嵌入特征增强器通过吸引远离分类中心的虚拟特征，提高了可用于超vised学习的信息量。动态层次特征细化器通过生成动态核函数，捕捉到不同维度的特征，进一步提高了内类紧凑度和对类分离度。</li>
<li>results: 实验结果表明，提出的方法在有限SAR训练数据情况下实现了优秀的ATR性能，并在MSTAR、OpenSARShip和FUSAR-Ship数据集上达到了robust性和稳定性。<details>
<summary>Abstract</summary>
Without sufficient data, the quantity of information available for supervised training is constrained, as obtaining sufficient synthetic aperture radar (SAR) training data in practice is frequently challenging. Therefore, current SAR automatic target recognition (ATR) algorithms perform poorly with limited training data availability, resulting in a critical need to increase SAR ATR performance. In this study, a new method to improve SAR ATR when training data are limited is proposed. First, an embedded feature augmenter is designed to enhance the extracted virtual features located far away from the class center. Based on the relative distribution of the features, the algorithm pulls the corresponding virtual features with different strengths toward the corresponding class center. The designed augmenter increases the amount of information available for supervised training and improves the separability of the extracted features. Second, a dynamic hierarchical-feature refiner is proposed to capture the discriminative local features of the samples. Through dynamically generated kernels, the proposed refiner integrates the discriminative local features of different dimensions into the global features, further enhancing the inner-class compactness and inter-class separability of the extracted features. The proposed method not only increases the amount of information available for supervised training but also extracts the discriminative features from the samples, resulting in superior ATR performance in problems with limited SAR training data. Experimental results on the moving and stationary target acquisition and recognition (MSTAR), OpenSARShip, and FUSAR-Ship benchmark datasets demonstrate the robustness and outstanding ATR performance of the proposed method in response to limited SAR training data.
</details>
<details>
<summary>摘要</summary>
无 suficiente datos de entrenamiento，量度信息可用于超vised学习是受限的，因为在实践中获得 suficiente Synthetic Aperture Radar（SAR）training data是困难的。因此，当前SAR自动目标识别（ATR）算法在有限的training data availability时表现糟糕。在这种情况下，一种新的方法是提出，以提高SAR ATR的性能。首先，一种嵌入式特征增强器是设计的，以增强提取的虚拟特征，特别是那些远离类中心的特征。根据特征的相对分布，算法将相应的虚拟特征与不同强度拖向相应的类中心。这种增强器提高了用于超vised学习的信息量，并提高了提取特征的分类能力。其次，一种动态层次特征级化器是提出的，以捕捉不同维度的特征。通过动态生成的核函数，提出的级化器将不同维度的特征级化到全维度特征中，进一步提高了类内紧密性和类间分离性。这种方法不仅提高了用于超vised学习的信息量，还提取了样本中的特征，从而实现了在有限SAR training data情况下的出色ATR性能。实验结果表明，在MSTAR、OpenSARShip和FUSAR-Ship benchmark dataset上，提出的方法具有强大的Robustness和出色的ATR性能，在有限SAR training data情况下表现优秀。
</details></li>
</ul>
<hr>
<h2 id="Blind-Face-Restoration-for-Under-Display-Camera-via-Dictionary-Guided-Transformer"><a href="#Blind-Face-Restoration-for-Under-Display-Camera-via-Dictionary-Guided-Transformer" class="headerlink" title="Blind Face Restoration for Under-Display Camera via Dictionary Guided Transformer"></a>Blind Face Restoration for Under-Display Camera via Dictionary Guided Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10196">http://arxiv.org/abs/2308.10196</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingfan Tan, Xiaoxu Chen, Tao Wang, Kaihao Zhang, Wenhan Luo, Xiaocun Cao</li>
<li>for: 提供全屏体验，但是由于显示器的特点，UDC图像会受到质量下降的影响。</li>
<li>methods: 提出了一种两stage网络UDC退化模型网络（UDC-DMNet），通过模拟UDC拍摄过程来synthesize UDC图像。还使用高质量的face图像从FFHQ和CelebA-Test创建了UDC face测试集FFHQ-P&#x2F;T和CelebA-Test-P&#x2F;T。</li>
<li>results: 提出了一种基于字典指导 transformer网络（DGFormer），通过引入面部组合字典和UDC图像特点来实现盲目face restauration。实验表明，我们的DGFormer和UDC-DMNet均达到了当前最佳性能。<details>
<summary>Abstract</summary>
By hiding the front-facing camera below the display panel, Under-Display Camera (UDC) provides users with a full-screen experience. However, due to the characteristics of the display, images taken by UDC suffer from significant quality degradation. Methods have been proposed to tackle UDC image restoration and advances have been achieved. There are still no specialized methods and datasets for restoring UDC face images, which may be the most common problem in the UDC scene. To this end, considering color filtering, brightness attenuation, and diffraction in the imaging process of UDC, we propose a two-stage network UDC Degradation Model Network named UDC-DMNet to synthesize UDC images by modeling the processes of UDC imaging. Then we use UDC-DMNet and high-quality face images from FFHQ and CelebA-Test to create UDC face training datasets FFHQ-P/T and testing datasets CelebA-Test-P/T for UDC face restoration. We propose a novel dictionary-guided transformer network named DGFormer. Introducing the facial component dictionary and the characteristics of the UDC image in the restoration makes DGFormer capable of addressing blind face restoration in UDC scenarios. Experiments show that our DGFormer and UDC-DMNet achieve state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
<<SYS>>通过将前置摄像头嵌入到显示板上，Under-Display Camera（UDC）为用户提供了全屏体验。然而，由于显示器的特点，UDC拍摄的图像受到了显著的质量下降。已经提出了修复UDC图像的方法，并取得了进步。然而，还没有专门的方法和数据集用于修复UDC脸图像，这可能是UDC场景中最常见的问题。为此，我们考虑了颜色滤波、亮度减弱和干涉的影像捕捉过程，并提出了一个两stage网络名为UDC-DMNet，用于模拟UDC拍摄过程。然后，我们使用UDC-DMNet和高质量的脸图像从FFHQ和CelebA-Test创建了UDC脸培训集FFHQ-P/T和测试集CelebA-Test-P/T。我们还提出了一种新的字典引导变换网络名为DGFormer，通过引入人脸组件字典和UDC图像的特点，DGFormer可以实现盲目脸修复在UDC场景中。实验表明，我们的DGFormer和UDC-DMNet实现了状态盘的性能。Note: Simplified Chinese is used here, which is a more common writing system in China. If you prefer Traditional Chinese, I can also provide the translation.
</details></li>
</ul>
<hr>
<h2 id="WMFormer-Nested-Transformer-for-Visible-Watermark-Removal-via-Implict-Joint-Learning"><a href="#WMFormer-Nested-Transformer-for-Visible-Watermark-Removal-via-Implict-Joint-Learning" class="headerlink" title="WMFormer++: Nested Transformer for Visible Watermark Removal via Implict Joint Learning"></a>WMFormer++: Nested Transformer for Visible Watermark Removal via Implict Joint Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10195">http://arxiv.org/abs/2308.10195</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongjian Huo, Zehong Zhang, Hanjing Su, Guanbin Li, Chaowei Fang, Qingyao Wu</li>
<li>for: 本研究旨在提高水印去除技术的精度和效率，以提高水印保护的可靠性。</li>
<li>methods: 该研究提出了一种新的 JOINT 学习方法，通过自动控制信息流动来Integrate 多个分支知识，从而提高水印localization和背景 restore 的精度。</li>
<li>results: 实验结果表明，该方法可以具有remarkable superiority，比existingsota-of-the-art方法大幅提高精度和效率。<details>
<summary>Abstract</summary>
Watermarking serves as a widely adopted approach to safeguard media copyright. In parallel, the research focus has extended to watermark removal techniques, offering an adversarial means to enhance watermark robustness and foster advancements in the watermarking field. Existing watermark removal methods mainly rely on UNet with task-specific decoder branches--one for watermark localization and the other for background image restoration. However, watermark localization and background restoration are not isolated tasks; precise watermark localization inherently implies regions necessitating restoration, and the background restoration process contributes to more accurate watermark localization. To holistically integrate information from both branches, we introduce an implicit joint learning paradigm. This empowers the network to autonomously navigate the flow of information between implicit branches through a gate mechanism. Furthermore, we employ cross-channel attention to facilitate local detail restoration and holistic structural comprehension, while harnessing nested structures to integrate multi-scale information. Extensive experiments are conducted on various challenging benchmarks to validate the effectiveness of our proposed method. The results demonstrate our approach's remarkable superiority, surpassing existing state-of-the-art methods by a large margin.
</details>
<details>
<summary>摘要</summary>
水印技术是现代版权保护的广泛采用方法之一。同时，研究焦点已经扩展到水印去除技术，提供了一种对抗性的方法，以提高水印的鲁棒性和推动水印技术的发展。现有的水印去除方法主要基于UNet架构，其中一个任务特定的解码分支用于水印localization，另一个分支用于背景图像修复。然而，水印localization和背景修复不是独立的任务，准确的水印localization直接 imply了需要修复的区域，而背景修复过程也会提高水印localization的准确性。为了整合这两个分支的信息，我们提出了隐式联合学习 paradigm。这种方法使得网络可以自动地在两个分支之间流动信息，通过门控制机制来自动调节信息的流动。此外，我们还使用了跨通道注意力来促进地方细节修复和整体结构认知，同时利用嵌入结构来整合多尺度信息。我们在多种复杂的 benchmark 上进行了广泛的实验，以验证我们的提出方法的有效性。结果表明，我们的方法在与现有状态顶的方法进行比较时表现出了惊人的优势，凌驱了现有的方法。
</details></li>
</ul>
<hr>
<h2 id="EDDense-Net-Fully-Dense-Encoder-Decoder-Network-for-Joint-Segmentation-of-Optic-Cup-and-Disc"><a href="#EDDense-Net-Fully-Dense-Encoder-Decoder-Network-for-Joint-Segmentation-of-Optic-Cup-and-Disc" class="headerlink" title="EDDense-Net: Fully Dense Encoder Decoder Network for Joint Segmentation of Optic Cup and Disc"></a>EDDense-Net: Fully Dense Encoder Decoder Network for Joint Segmentation of Optic Cup and Disc</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10192">http://arxiv.org/abs/2308.10192</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mehwish Mehmood, Khuram Naveed, Haroon Ahmed Khan, Syed S. Naqvi<br>for: 这篇论文是为了检测和诊断眼内疾病 glaucoma 而写的。methods: 这篇论文提出了一种基于 dense block 的 EDDense-Net 分割网络，用于同时 segmentation of OC 和 OD。该网络使用 grouped convolutional layer 以获取和传递图像中的空间信息，并同时减少网络的复杂性。在 semantic segmentation 阶段，使用 dice pixel classification 来缓解类别不均衡问题。results: 该方法在两个公共可用的数据集上进行评估，并在精度和效率方面超越了现有的状态图像方法。这种方法可以用于医学眼科专业人员的助手，帮助他们诊断和分析眼内疾病。<details>
<summary>Abstract</summary>
Glaucoma is an eye disease that causes damage to the optic nerve, which can lead to visual loss and permanent blindness. Early glaucoma detection is therefore critical in order to avoid permanent blindness. The estimation of the cup-to-disc ratio (CDR) during an examination of the optical disc (OD) is used for the diagnosis of glaucoma. In this paper, we present the EDDense-Net segmentation network for the joint segmentation of OC and OD. The encoder and decoder in this network are made up of dense blocks with a grouped convolutional layer in each block, allowing the network to acquire and convey spatial information from the image while simultaneously reducing the network's complexity. To reduce spatial information loss, the optimal number of filters in all convolution layers were utilised. In semantic segmentation, dice pixel classification is employed in the decoder to alleviate the problem of class imbalance. The proposed network was evaluated on two publicly available datasets where it outperformed existing state-of-the-art methods in terms of accuracy and efficiency. For the diagnosis and analysis of glaucoma, this method can be used as a second opinion system to assist medical ophthalmologists.
</details>
<details>
<summary>摘要</summary>
Glaucoma 是一种眼病，可以导致视网膜损害，从而导致视力下降和永久失明。 Early detection of glaucoma 是非常重要，以避免永久失明。在诊断 glaucoma 时，可以使用 optical disc 的 cup-to-disc ratio（CDR）的估算。在这篇论文中，我们提出了 EDDense-Net 分割网络，用于对 optical disc 和 optic cup 的同时分割。这个网络包括 dense block 和 grouped convolutional layer，可以同时保持图像的空间信息并减少网络的复杂性。为了避免空间信息损失，我们在所有卷积层中使用了最佳的筛选器数量。在 semantic segmentation 中，我们使用 dice pixel classification 来缓解类别偏见问题。我们提出的网络在两个公共可用的数据集上进行了评估，并与现有的状态之arte方法相比，在准确性和效率方面表现出色。这种方法可以用于帮助医学眼科医生进行诊断和分析。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Diffusion-Model-with-Auxiliary-Guidance-for-Coarse-to-Fine-PET-Reconstruction"><a href="#Contrastive-Diffusion-Model-with-Auxiliary-Guidance-for-Coarse-to-Fine-PET-Reconstruction" class="headerlink" title="Contrastive Diffusion Model with Auxiliary Guidance for Coarse-to-Fine PET Reconstruction"></a>Contrastive Diffusion Model with Auxiliary Guidance for Coarse-to-Fine PET Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10157">http://arxiv.org/abs/2308.10157</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/show-han/pet-reconstruction">https://github.com/show-han/pet-reconstruction</a></li>
<li>paper_authors: Zeyu Han, Yuhan Wang, Luping Zhou, Peng Wang, Binyu Yan, Jiliu Zhou, Yan Wang, Dinggang Shen</li>
<li>for: 提高标准剂量 позитроном发射tomography（PET）成像质量，降低人体对PET成像的辐射暴露。</li>
<li>methods: 使用生成对抗网络（GANs）和扩散概率模型（DPMs）重建标准剂量PET（SPET）图像从低剂量PET（LPET）图像中。</li>
<li>results: 提出一种块级预测模块（CPM）和一种迭代纠正模块（IRM）组成的粗细预测架构，可以大幅提高样本速度，同时保持LPET图像和重建PET（RPET）图像之间的协同关系，提高临床可靠性。<details>
<summary>Abstract</summary>
To obtain high-quality positron emission tomography (PET) scans while reducing radiation exposure to the human body, various approaches have been proposed to reconstruct standard-dose PET (SPET) images from low-dose PET (LPET) images. One widely adopted technique is the generative adversarial networks (GANs), yet recently, diffusion probabilistic models (DPMs) have emerged as a compelling alternative due to their improved sample quality and higher log-likelihood scores compared to GANs. Despite this, DPMs suffer from two major drawbacks in real clinical settings, i.e., the computationally expensive sampling process and the insufficient preservation of correspondence between the conditioning LPET image and the reconstructed PET (RPET) image. To address the above limitations, this paper presents a coarse-to-fine PET reconstruction framework that consists of a coarse prediction module (CPM) and an iterative refinement module (IRM). The CPM generates a coarse PET image via a deterministic process, and the IRM samples the residual iteratively. By delegating most of the computational overhead to the CPM, the overall sampling speed of our method can be significantly improved. Furthermore, two additional strategies, i.e., an auxiliary guidance strategy and a contrastive diffusion strategy, are proposed and integrated into the reconstruction process, which can enhance the correspondence between the LPET image and the RPET image, further improving clinical reliability. Extensive experiments on two human brain PET datasets demonstrate that our method outperforms the state-of-the-art PET reconstruction methods. The source code is available at \url{https://github.com/Show-han/PET-Reconstruction}.
</details>
<details>
<summary>摘要</summary>
为了获得高质量的 позиトрон发射Tomography（PET）扫描图像，而同时降低人体对X射线暴露，Various approaches have been proposed to reconstruct standard-dose PET（SPET）images from low-dose PET（LPET）images. One widely adopted technique is the generative adversarial networks（GANs）, yet recently, diffusion probabilistic models（DPMs）have emerged as a compelling alternative due to their improved sample quality and higher log-likelihood scores compared to GANs. Despite this, DPMs suffer from two major drawbacks in real clinical settings, i.e., the computationally expensive sampling process and the insufficient preservation of correspondence between the conditioning LPET image and the reconstructed PET（RPET）image. To address the above limitations, this paper presents a coarse-to-fine PET reconstruction framework that consists of a coarse prediction module（CPM）and an iterative refinement module（IRM）. The CPM generates a coarse PET image via a deterministic process, and the IRM samples the residual iteratively. By delegating most of the computational overhead to the CPM, the overall sampling speed of our method can be significantly improved. Furthermore, two additional strategies, i.e., an auxiliary guidance strategy and a contrastive diffusion strategy, are proposed and integrated into the reconstruction process, which can enhance the correspondence between the LPET image and the RPET image, further improving clinical reliability. Extensive experiments on two human brain PET datasets demonstrate that our method outperforms the state-of-the-art PET reconstruction methods. The source code is available at \url{https://github.com/Show-han/PET-Reconstruction}.
</details></li>
</ul>
<hr>
<h2 id="Federated-Pseudo-Modality-Generation-for-Incomplete-Multi-Modal-MRI-Reconstruction"><a href="#Federated-Pseudo-Modality-Generation-for-Incomplete-Multi-Modal-MRI-Reconstruction" class="headerlink" title="Federated Pseudo Modality Generation for Incomplete Multi-Modal MRI Reconstruction"></a>Federated Pseudo Modality Generation for Incomplete Multi-Modal MRI Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10910">http://arxiv.org/abs/2308.10910</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunlu Yan, Chun-Mei Feng, Yuexiang Li, Rick Siow Mong Goh, Lei Zhu</li>
<li>for:  Addressing the missing modality challenge in federated multi-modal MRI reconstruction.</li>
<li>methods:  Utilizes a pseudo modality generation mechanism to recover the missing modality for each single-modal client by sharing the distribution information of the amplitude spectrum in frequency space, and introduces a clustering scheme to reduce communication costs.</li>
<li>results:  Can effectively complete the missing modality within an acceptable communication cost, and attains similar performance with the ideal scenario, i.e., all clients have the full set of modalities.Here’s the Chinese translation of the three points:</li>
<li>for:  Addressing the 缺失多个Modalities的问题在分布式多modal MRI重建中。</li>
<li>methods:  Utilizes a pseudo modality generation mechanism to recover the missing modality for each single-modal client by sharing the distribution information of the amplitude spectrum in frequency space, and introduces a clustering scheme to reduce communication costs.</li>
<li>results:  Can effectively complete the missing modality within an acceptable communication cost, and attains similar performance with the ideal scenario, i.e., all clients have the full set of modalities.<details>
<summary>Abstract</summary>
While multi-modal learning has been widely used for MRI reconstruction, it relies on paired multi-modal data which is difficult to acquire in real clinical scenarios. Especially in the federated setting, the common situation is that several medical institutions only have single-modal data, termed the modality missing issue. Therefore, it is infeasible to deploy a standard federated learning framework in such conditions. In this paper, we propose a novel communication-efficient federated learning framework, namely Fed-PMG, to address the missing modality challenge in federated multi-modal MRI reconstruction. Specifically, we utilize a pseudo modality generation mechanism to recover the missing modality for each single-modal client by sharing the distribution information of the amplitude spectrum in frequency space. However, the step of sharing the original amplitude spectrum leads to heavy communication costs. To reduce the communication cost, we introduce a clustering scheme to project the set of amplitude spectrum into finite cluster centroids, and share them among the clients. With such an elaborate design, our approach can effectively complete the missing modality within an acceptable communication cost. Extensive experiments demonstrate that our proposed method can attain similar performance with the ideal scenario, i.e., all clients have the full set of modalities. The source code will be released.
</details>
<details>
<summary>摘要</summary>
多Modal学习已经广泛应用于MRI重建，但它需要对配套多Modal数据进行训练，这在实际临床情况下很Difficult to acquire.特别是在联邦设置下，一些医疗机构只有单Modal数据，称为模式缺失问题。因此，使用标准联邦学习框架是不可能的。在这篇论文中，我们提出了一种新的通信效率高的联邦学习框架，即Fed-PMG，用于解决联邦多Modal MRI重建中的模式缺失问题。特别是，我们利用 pseudo Modality生成机制来为每个单Modal客户端 recuperate 缺失的模式。然而，在分享原始振荡谱的步骤中，会导致重大的通信成本。为了降低通信成本，我们引入了分区 schemes来将振荡谱集合投影到有限的群集中心，然后分享这些中心。通过这种精心的设计，我们的方法可以在接受ABLE的通信成本下完成缺失模式。EXTensive experiments表明，我们提出的方法可以 дости到与理想情况（即所有客户端具有完整的模式）相同的性能。源代码将被发布。
</details></li>
</ul>
<hr>
<h2 id="Polymerized-Feature-based-Domain-Adaptation-for-Cervical-Cancer-Dose-Map-Prediction"><a href="#Polymerized-Feature-based-Domain-Adaptation-for-Cervical-Cancer-Dose-Map-Prediction" class="headerlink" title="Polymerized Feature-based Domain Adaptation for Cervical Cancer Dose Map Prediction"></a>Polymerized Feature-based Domain Adaptation for Cervical Cancer Dose Map Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10142">http://arxiv.org/abs/2308.10142</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jie Zeng, Zeyu Han, Xingchen Peng, Jianghong Xiao, Peng Wang, Yan Wang</li>
<li>for: 这篇研究是为了提高乳腺癌治疗规划的精确度，使用深度学习（DL）方法。</li>
<li>methods: 本研究使用对另一种肝癌（rectum cancer）进行了预测，并将其中学习到的丰富知识转移到乳腺癌上，以优化对乳腺癌的辐射规划预测性。</li>
<li>results: 实验结果显示，提案的方法比州先进方法更高效，并在两个实验数据集上显示出超过州先进方法的性能。<details>
<summary>Abstract</summary>
Recently, deep learning (DL) has automated and accelerated the clinical radiation therapy (RT) planning significantly by predicting accurate dose maps. However, most DL-based dose map prediction methods are data-driven and not applicable for cervical cancer where only a small amount of data is available. To address this problem, this paper proposes to transfer the rich knowledge learned from another cancer, i.e., rectum cancer, which has the same scanning area and more clinically available data, to improve the dose map prediction performance for cervical cancer through domain adaptation. In order to close the congenital domain gap between the source (i.e., rectum cancer) and the target (i.e., cervical cancer) domains, we develop an effective Transformer-based polymerized feature module (PFM), which can generate an optimal polymerized feature distribution to smoothly align the two input distributions. Experimental results on two in-house clinical datasets demonstrate the superiority of the proposed method compared with state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
To bridge the inherent domain gap between the source (rectum cancer) and target (cervical cancer) domains, we develop an effective Transformer-based polymerized feature module (PFM) that generates an optimal polymerized feature distribution to smoothly align the two input distributions. Experimental results on two in-house clinical datasets demonstrate the superiority of the proposed method compared with state-of-the-art methods.
</details></li>
</ul>
<hr>
<h2 id="Sensitivity-analysis-of-AI-based-algorithms-for-autonomous-driving-on-optical-wavefront-aberrations-induced-by-the-windshield"><a href="#Sensitivity-analysis-of-AI-based-algorithms-for-autonomous-driving-on-optical-wavefront-aberrations-induced-by-the-windshield" class="headerlink" title="Sensitivity analysis of AI-based algorithms for autonomous driving on optical wavefront aberrations induced by the windshield"></a>Sensitivity analysis of AI-based algorithms for autonomous driving on optical wavefront aberrations induced by the windshield</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11711">http://arxiv.org/abs/2308.11711</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dominik Werner Wolf, Markus Ulrich, Nikhil Kapoor</li>
<li>for: 本研究旨在解决自动驾驶视觉技术中的领域转移问题，即使用不同汽车型号和镜头配置训练模型后，在不同镜头配置下运行模型的性能下降问题。</li>
<li>methods: 本研究采用了一种基于 Fourier optics 的威胁模型，对两种视觉模型进行评估，以确定它们在不同镜头配置下的性能敏感性。</li>
<li>results: 研究结果表明，镜头配置会引入性能差距，而现有的光学指标可能不够用于评估模型在不同镜头配置下的性能。<details>
<summary>Abstract</summary>
Autonomous driving perception techniques are typically based on supervised machine learning models that are trained on real-world street data. A typical training process involves capturing images with a single car model and windshield configuration. However, deploying these trained models on different car types can lead to a domain shift, which can potentially hurt the neural networks performance and violate working ADAS requirements. To address this issue, this paper investigates the domain shift problem further by evaluating the sensitivity of two perception models to different windshield configurations. This is done by evaluating the dependencies between neural network benchmark metrics and optical merit functions by applying a Fourier optics based threat model. Our results show that there is a performance gap introduced by windshields and existing optical metrics used for posing requirements might not be sufficient.
</details>
<details>
<summary>摘要</summary>
自主驾驶感知技术通常基于指导学习模型，这些模型通常是基于实际街道数据进行训练。一般训练过程中会使用单车型和顶部配置拍摄图像。但是，将这些训练模型应用于不同车型可能会导致领域shift，这可能会影响神经网络的性能，并违反工作ADAS要求。为解决这个问题，本文进一步探讨领域shift问题，并评估两种感知模型对不同顶部配置的敏感性。我们通过应用 Fourier optics 基于威胁模型来评估神经网络指标和光学功能之间的依赖关系。我们的结果表明，顶部配置会引入性能差异，现有的光学指标可能并不够。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/20/eess.IV_2023_08_20/" data-id="clm0t8e2v00fgv7888o4x8zqd" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_19" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/19/cs.LG_2023_08_19/" class="article-date">
  <time datetime="2023-08-18T16:00:00.000Z" itemprop="datePublished">2023-08-19</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/19/cs.LG_2023_08_19/">cs.LG - 2023-08-19 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Efficient-Representation-Learning-for-Healthcare-with-Cross-Architectural-Self-Supervision"><a href="#Efficient-Representation-Learning-for-Healthcare-with-Cross-Architectural-Self-Supervision" class="headerlink" title="Efficient Representation Learning for Healthcare with Cross-Architectural Self-Supervision"></a>Efficient Representation Learning for Healthcare with Cross-Architectural Self-Supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10064">http://arxiv.org/abs/2308.10064</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pranavsinghps1/CASS">https://github.com/pranavsinghps1/CASS</a></li>
<li>paper_authors: Pranav Singh, Jacopo Cirrone</li>
<li>for: 这篇论文目的是提出一种新的自我超vised学习方法，以增强深度学习架构在医疗应用中的表现。</li>
<li>methods: 本文使用了一种新的siamese自我超vised学习方法，Synergistically leveraging Transformer和Convolutional Neural Networks (CNN) для高效学习。</li>
<li>results: 在四个不同的医疗数据集上，CASS-trained CNNs和Transformers已经超越了现有的自我超vised学习方法，仅使用1%的标签数据进行调整，协助得到了3.8%的平均提升。<details>
<summary>Abstract</summary>
In healthcare and biomedical applications, extreme computational requirements pose a significant barrier to adopting representation learning. Representation learning can enhance the performance of deep learning architectures by learning useful priors from limited medical data. However, state-of-the-art self-supervised techniques suffer from reduced performance when using smaller batch sizes or shorter pretraining epochs, which are more practical in clinical settings. We present Cross Architectural - Self Supervision (CASS) in response to this challenge. This novel siamese self-supervised learning approach synergistically leverages Transformer and Convolutional Neural Networks (CNN) for efficient learning. Our empirical evaluation demonstrates that CASS-trained CNNs and Transformers outperform existing self-supervised learning methods across four diverse healthcare datasets. With only 1% labeled data for finetuning, CASS achieves a 3.8% average improvement; with 10% labeled data, it gains 5.9%; and with 100% labeled data, it reaches a remarkable 10.13% enhancement. Notably, CASS reduces pretraining time by 69% compared to state-of-the-art methods, making it more amenable to clinical implementation. We also demonstrate that CASS is considerably more robust to variations in batch size and pretraining epochs, making it a suitable candidate for machine learning in healthcare applications.
</details>
<details>
<summary>摘要</summary>
在医疗和生物医学应用中，极高的计算需求成为了采用表示学习的重大障碍。表示学习可以提高深度学习架构的性能，但是现有的自我超vised学习技术在使用小批量或短时间预训练时表现不佳。为回应这个挑战，我们提出了跨建筑自我超vised学习（CASS）方法。这种新的siamese自我超vised学习方法可以有效地利用转换器和卷积神经网络（CNN）进行学习。我们的实验证明，CASS-trained CNNs和转换器在四个不同的医疗数据集上都能够超越现有的自我超vised学习方法。只有1%的标注数据进行微调，CASS可以实现3.8%的平均提高; 使用10%的标注数据，它可以获得5.9%的提高; 使用100%的标注数据，它可以达到10.13%的提高。另外，CASS可以降低预训练时间，比现有的方法减少69%，使其更适合在临床实施。我们还证明了CASS在批量大小和预训练轮次上的变化对其性能的影响较小，使其成为医学机器学习中适用的方法。
</details></li>
</ul>
<hr>
<h2 id="Accelerating-Exact-Combinatorial-Optimization-via-RL-based-Initialization-–-A-Case-Study-in-Scheduling"><a href="#Accelerating-Exact-Combinatorial-Optimization-via-RL-based-Initialization-–-A-Case-Study-in-Scheduling" class="headerlink" title="Accelerating Exact Combinatorial Optimization via RL-based Initialization – A Case Study in Scheduling"></a>Accelerating Exact Combinatorial Optimization via RL-based Initialization – A Case Study in Scheduling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11652">http://arxiv.org/abs/2308.11652</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaqi Yin, Cunxi Yu</li>
<li>for: 本研究旨在提出一种创新的方法，使用机器学习（ML）解决 combinatorial 优化问题，用调度为例子。目标是提供优化和推理的 garanties，同时保持优化算法的运行时间成本。</li>
<li>methods: 我们提出了一种两阶段 RL-to-ILP 调度框架，包括以下三个步骤：1）RL 算法 acts as coarse-grain scheduler，2）解决 relaxation 和 3）精确的解决方案 via ILP。</li>
<li>results: 我们的框架能够与使用精确调度方法具有同样的调度性能，而且可以达到 128 倍的速度提升。这些结果基于实际的 EdgeTPU 平台，使用 ImageNet DNN 计算图为输入。此外，我们的框架还可以提供更好的在芯片上的执行时间和加速。<details>
<summary>Abstract</summary>
Scheduling on dataflow graphs (also known as computation graphs) is an NP-hard problem. The traditional exact methods are limited by runtime complexity, while reinforcement learning (RL) and heuristic-based approaches struggle with determinism and solution quality. This research aims to develop an innovative approach that employs machine learning (ML) for addressing combinatorial optimization problems, using scheduling as a case study. The goal is to provide guarantees in optimality and determinism while maintaining the runtime cost of heuristic methods. Specifically, we introduce a novel two-phase RL-to-ILP scheduling framework, which includes three steps: 1) RL solver acts as coarse-grain scheduler, 2) solution relaxation and 3) exact solving via ILP. Our framework demonstrates the same scheduling performance compared with using exact scheduling methods while achieving up to 128 $\times$ speed improvements. This was conducted on actual EdgeTPU platforms, utilizing ImageNet DNN computation graphs as input. Additionally, the framework offers improved on-chip inference runtime and acceleration compared to the commercially available EdgeTPU compiler.
</details>
<details>
<summary>摘要</summary>
“计划在数据流图（也称作计算图）是一个NP困难问题。传统的精确方法受到运行时复杂性的限制，而循环学习（RL）和规则基本方法受到束缚和解决质量的影响。这项研究旨在开发一种创新的方法，使用机器学习（ML）来解决 combinatorial 优化问题，使用计划为 caso study。目标是提供优化和决定性的保证，同时保持规则基本方法的运行时成本。具体来说，我们提出了一种新的两阶段RL-to-ILP 计划框架，包括以下三个步骤：1）RL 算法作为粗粒度调度器，2）解决缓和3）使用 ILP 进行精确解决。我们的框架实现了与使用精确调度方法相同的计划性表现，同时实现了Up to 128倍的速度提升。这些实验在实际的 EdgeTPU 平台上进行，使用 ImageNet DNN 计算图作为输入。此外，我们的框架也提供了比商业可用的 EdgeTPU 编译器更好的在处理器上的执行时间和加速。”
</details></li>
</ul>
<hr>
<h2 id="The-Snowflake-Hypothesis-Training-Deep-GNN-with-One-Node-One-Receptive-field"><a href="#The-Snowflake-Hypothesis-Training-Deep-GNN-with-One-Node-One-Receptive-field" class="headerlink" title="The Snowflake Hypothesis: Training Deep GNN with One Node One Receptive field"></a>The Snowflake Hypothesis: Training Deep GNN with One Node One Receptive field</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10051">http://arxiv.org/abs/2308.10051</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kun Wang, Guohao Li, Shilong Wang, Guibin Zhang, Kai Wang, Yang You, Xiaojiang Peng, Yuxuan Liang, Yang Wang</li>
<li>for: 本研究的目的是解释深度Graph Neural Networks（GNN）在图像领域的成功，并提出一种新的概念——雪花假设，以提高GNN的可读性和通用性。</li>
<li>methods: 本研究使用了多种方法，包括不同的训练方案、不同的束深度和层数、以及不同的层数和训练集。同时，本研究还使用了 simplest gradient和node-level cosine distance来规则节点聚合的深度。</li>
<li>results: 本研究的结果表明，雪花假设可以成为一种通用的操作，可以应用于多种GNN框架，提高其效果并提供可读性和可 generalized的选择网络深度。同时，本研究还发现，采用雪花假设可以减少GNN的过拟合和过熔化问题。<details>
<summary>Abstract</summary>
Despite Graph Neural Networks demonstrating considerable promise in graph representation learning tasks, GNNs predominantly face significant issues with over-fitting and over-smoothing as they go deeper as models of computer vision realm. In this work, we conduct a systematic study of deeper GNN research trajectories. Our findings indicate that the current success of deep GNNs primarily stems from (I) the adoption of innovations from CNNs, such as residual/skip connections, or (II) the tailor-made aggregation algorithms like DropEdge. However, these algorithms often lack intrinsic interpretability and indiscriminately treat all nodes within a given layer in a similar manner, thereby failing to capture the nuanced differences among various nodes. To this end, we introduce the Snowflake Hypothesis -- a novel paradigm underpinning the concept of ``one node, one receptive field''. The hypothesis draws inspiration from the unique and individualistic patterns of each snowflake, proposing a corresponding uniqueness in the receptive fields of nodes in the GNNs.   We employ the simplest gradient and node-level cosine distance as guiding principles to regulate the aggregation depth for each node, and conduct comprehensive experiments including: (1) different training schemes; (2) various shallow and deep GNN backbones, and (3) various numbers of layers (8, 16, 32, 64) on multiple benchmarks (six graphs including dense graphs with millions of nodes); (4) compare with different aggregation strategies. The observational results demonstrate that our hypothesis can serve as a universal operator for a range of tasks, and it displays tremendous potential on deep GNNs. It can be applied to various GNN frameworks, enhancing its effectiveness when operating in-depth, and guiding the selection of the optimal network depth in an explainable and generalizable way.
</details>
<details>
<summary>摘要</summary>
尽管图像神经网络（Graph Neural Networks，GNNs）在图表学习任务上表现出了诸多承诺，但它们在深度上遇到了许多问题，如预测和泛化。在这项工作中，我们进行了系统的深度GNN研究轨迹。我们的发现表明，当前深度GNN的成功主要归功于（I） adopting innovations from CNNs，如径规/跳过连接，或（II）适应性的聚合算法，如DropEdge。但这些算法通常缺乏内在解释性，并且不具备对不同节点的细化了的认知。因此，我们提出了雪花假设——一种新的思维方式，它提出了每个节点都有独特的感受领域。这种假设 Draws inspiration from the unique and individualistic patterns of each snowflake，并提出了对节点的聚合深度进行调控的一种新的方法。我们采用 simplest gradient 和 node-level cosine distance 作为导引原则，并进行了广泛的实验，包括：（1）不同的训练方案；（2）不同的束缚和深度的 GNN 基础架构；（3）不同层数（8, 16, 32, 64）在多个 Benchmark 上进行了多种实验。我们的观察结果表明，我们的假设可以作为一种通用的操作，并且在深度GNNs中表现出了巨大的潜力。它可以应用于不同的 GNN 框架，提高其在深度下的效果，并且可以在可解释和普适的方式下选择网络的最佳层数。
</details></li>
</ul>
<hr>
<h2 id="Computing-the-Vapnik-Chervonenkis-Dimension-for-Non-Discrete-Settings"><a href="#Computing-the-Vapnik-Chervonenkis-Dimension-for-Non-Discrete-Settings" class="headerlink" title="Computing the Vapnik Chervonenkis Dimension for Non-Discrete Settings"></a>Computing the Vapnik Chervonenkis Dimension for Non-Discrete Settings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10041">http://arxiv.org/abs/2308.10041</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammed Nechba, Mouhajir Mohamed, Sedjari Yassine</li>
<li>for: 本文旨在研究一种不受概念类或其域集的约束而 compute the VC dimension的方法。</li>
<li>methods: 本文使用了Empirical Risk Minimization（ERM）学习模型，以characterize the shattering property of a concept class。</li>
<li>results: 本文提出了一种可以approximately compute the VC dimension的方法，不受概念类或其域集的约束。<details>
<summary>Abstract</summary>
In 1984, Valiant [ 7 ] introduced the Probably Approximately Correct (PAC) learning framework for boolean function classes. Blumer et al. [ 2] extended this model in 1989 by introducing the VC dimension as a tool to characterize the learnability of PAC. The VC dimension was based on the work of Vapnik and Chervonenkis in 1971 [8 ], who introduced a tool called the growth function to characterize the shattering property. Researchers have since determined the VC dimension for specific classes, and efforts have been made to develop an algorithm that can calculate the VC dimension for any concept class. In 1991, Linial, Mansour, and Rivest [4] presented an algorithm for computing the VC dimension in the discrete setting, assuming that both the concept class and domain set were finite. However, no attempts had been made to design an algorithm that could compute the VC dimension in the general setting.Therefore, our work focuses on developing a method to approximately compute the VC dimension without constraints on the concept classes or their domain set. Our approach is based on our finding that the Empirical Risk Minimization (ERM) learning paradigm can be used as a new tool to characterize the shattering property of a concept class.
</details>
<details>
<summary>摘要</summary>
在1984年，Valiant（7）提出了一种名为“Probably Approximately Correct”（PAC）学习框架，用于 boolean 函数类型。Blumer等人（2）在1989年Extension this model by introducing the VC dimension as a tool to characterize the learnability of PAC.VC dimension是基于Vapnik和Chervonenkis（8）在1971年引入的一种工具，用于 caracterize the shattering property。研究人员已经确定了特定类型的VC dimension，并且有努力开发一个可以计算VC dimension for any concept class的算法。在1991年，Linial、Mansour和Rivest（4）提出了一种算法来计算VC dimension在离散设定下，假设概念类和域集都是finite。然而，没有任何尝试过开发一个可以计算VC dimension在通用设定下的算法。因此，我们的工作是关注开发一种可以约approximately compute VC dimension的方法，不受概念类或其域集的限制。我们的方法基于我们发现，Empirical Risk Minimization（ERM）学习模式可以用作一种新的工具来characterize the shattering property of a concept class。
</details></li>
</ul>
<hr>
<h2 id="Physics-guided-training-of-GAN-to-improve-accuracy-in-airfoil-design-synthesis"><a href="#Physics-guided-training-of-GAN-to-improve-accuracy-in-airfoil-design-synthesis" class="headerlink" title="Physics-guided training of GAN to improve accuracy in airfoil design synthesis"></a>Physics-guided training of GAN to improve accuracy in airfoil design synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10038">http://arxiv.org/abs/2308.10038</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kazunari Wada, Katsuyuki Suzuki, Kazuo Yonekura</li>
<li>for: 本研究使用生成对抗网络（GAN）进行机械形状的设计合成，但GANometimes输出物理不合理的形状。例如，当GAN模型被训练以输出符合需要的 aerodynamic表现的气流形状时，会出现显著的错误。这是因为GAN模型只考虑数据，不考虑下面的物理方程。本研究提出了通过physics-guided Training来导引GAN模型学习物理有效性。</li>
<li>methods: 本研究使用physics-guided Training来导引GAN模型学习物理有效性。通过general-purpose software locate outside the neural network model来计算物理有效性。</li>
<li>results: 数字实验显示，提议的模型可以减少错误，并且输出的形状与训练集数据不同，但仍满足物理有效性，超越现有的GAN模型的限制。<details>
<summary>Abstract</summary>
Generative adversarial networks (GAN) have recently been used for a design synthesis of mechanical shapes. A GAN sometimes outputs physically unreasonable shapes. For example, when a GAN model is trained to output airfoil shapes that indicate required aerodynamic performance, significant errors occur in the performance values. This is because the GAN model only considers data but does not consider the aerodynamic equations that lie under the data. This paper proposes the physics-guided training of the GAN model to guide the model to learn physical validity. Physical validity is computed using general-purpose software located outside the neural network model. Such general-purpose software cannot be used in physics-informed neural network frameworks, because physical equations must be implemented inside the neural network models. Additionally, a limitation of generative models is that the output data are similar to the training data and cannot generate completely new shapes. However, because the proposed model is guided by a physical model and does not use a training dataset, it can generate completely new shapes. Numerical experiments show that the proposed model drastically improves the accuracy. Moreover, the output shapes differ from those of the training dataset but still satisfy the physical validity, overcoming the limitations of existing GAN models.
</details>
<details>
<summary>摘要</summary>
《生成对抗网络（GAN）在机械形状设计中的应用》，Recently, GAN has been used for mechanical shape design. However, sometimes the output shapes of GAN are physically unreasonable. For example, when training a GAN model to output airfoil shapes that meet certain aerodynamic performance requirements, significant errors can occur in the performance values. This is because the GAN model only considers the data but does not consider the underlying aerodynamic equations. This paper proposes the physics-guided training of the GAN model to ensure physical validity. Physical validity is computed using general-purpose software located outside the neural network model. Unfortunately, such general-purpose software cannot be used in physics-informed neural network frameworks, because physical equations must be implemented inside the neural network models. Additionally, a limitation of generative models is that the output data are similar to the training data and cannot generate completely new shapes. However, because the proposed model is guided by a physical model and does not use a training dataset, it can generate completely new shapes. Numerical experiments show that the proposed model significantly improves accuracy. Moreover, the output shapes differ from those of the training dataset but still satisfy physical validity, overcoming the limitations of existing GAN models.
</details></li>
</ul>
<hr>
<h2 id="High-Performance-Computing-Applied-to-Logistic-Regression-A-CPU-and-GPU-Implementation-Comparison"><a href="#High-Performance-Computing-Applied-to-Logistic-Regression-A-CPU-and-GPU-Implementation-Comparison" class="headerlink" title="High Performance Computing Applied to Logistic Regression: A CPU and GPU Implementation Comparison"></a>High Performance Computing Applied to Logistic Regression: A CPU and GPU Implementation Comparison</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10037">http://arxiv.org/abs/2308.10037</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nechbamohammed/swiftlogisticreg">https://github.com/nechbamohammed/swiftlogisticreg</a></li>
<li>paper_authors: Nechba Mohammed, Mouhajir Mohamed, Sedjari Yassine</li>
<li>for: 这个研究是为了提高大数据集的binary classification的速度，使用GPU进行并行运算。</li>
<li>methods: 这个研究使用了X. Zou等人提出的平行Gradient Descent Logistic Regression算法，并将其Directly translate到GPU上。</li>
<li>results: 实验结果显示，我们的GPU-based LR比CPU-based实现更快，具有相似的f1分数。这种加速处理大数据集的能力使得我们的方法特别有利于实时预测应用，如影像识别、垃圾邮件检测和诈欺检测。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
We present a versatile GPU-based parallel version of Logistic Regression (LR), aiming to address the increasing demand for faster algorithms in binary classification due to large data sets. Our implementation is a direct translation of the parallel Gradient Descent Logistic Regression algorithm proposed by X. Zou et al. [12]. Our experiments demonstrate that our GPU-based LR outperforms existing CPU-based implementations in terms of execution time while maintaining comparable f1 score. The significant acceleration of processing large datasets makes our method particularly advantageous for real-time prediction applications like image recognition, spam detection, and fraud detection. Our algorithm is implemented in a ready-to-use Python library available at : https://github.com/NechbaMohammed/SwiftLogisticReg
</details>
<details>
<summary>摘要</summary>
我们提出了一种高性能的GPU基于的Logistic Regression（LR）版本，用于解决由大数据集引起的快速算法需求增加。我们的实现是基于平行梯度下降Logistic Regression算法的直译，由X. Zou等人提出。我们的实验表明，我们的GPU基于LR在执行时间方面与CPU基于实现相比具有明显的优势，同时保持相似的准确率。这种加速处理大数据集的能力使我们的方法在实时预测应用，如图像识别、垃圾邮件检测和诈骗检测等方面特别有优势。我们的算法在Python中实现，可以在：https://github.com/NechbaMohammed/SwiftLogisticReg 中获取。
</details></li>
</ul>
<hr>
<h2 id="Semi-Supervised-Anomaly-Detection-for-the-Determination-of-Vehicle-Hijacking-Tweets"><a href="#Semi-Supervised-Anomaly-Detection-for-the-Determination-of-Vehicle-Hijacking-Tweets" class="headerlink" title="Semi-Supervised Anomaly Detection for the Determination of Vehicle Hijacking Tweets"></a>Semi-Supervised Anomaly Detection for the Determination of Vehicle Hijacking Tweets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10036">http://arxiv.org/abs/2308.10036</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taahir Aiyoob Patel, Clement N. Nyirenda</li>
<li>for: 这个研究旨在使用推文来识别车质事件，以帮助旅行者免受车质事件的威胁。</li>
<li>methods: 这个研究使用了两种无监督侦错算法，包括K-Nearest Neighbour (KNN) 和 Cluster Based Outlier Factor (CBLOF)，以检测推文中的偏差。</li>
<li>results: 比较研究表明，CBLOF 方法的精度为 90%，而 KNN 方法的精度为 89%。CBLOF 方法也得到了 F1-Score 的 0.8，而 KNN 方法得到了 0.78。因此，CBLOF 方法 slightly outperformed KNN 方法，并被选为车质事件推文的侦错方法。<details>
<summary>Abstract</summary>
In South Africa, there is an ever-growing issue of vehicle hijackings. This leads to travellers constantly being in fear of becoming a victim to such an incident. This work presents a new semi-supervised approach to using tweets to identify hijacking incidents by using unsupervised anomaly detection algorithms. Tweets consisting of the keyword "hijacking" are obtained, stored, and processed using the term frequency-inverse document frequency (TF-IDF) and further analyzed by using two anomaly detection algorithms: 1) K-Nearest Neighbour (KNN); 2) Cluster Based Outlier Factor (CBLOF). The comparative evaluation showed that the KNN method produced an accuracy of 89%, whereas the CBLOF produced an accuracy of 90%. The CBLOF method was also able to obtain a F1-Score of 0.8, whereas the KNN produced a 0.78. Therefore, there is a slight difference between the two approaches, in favour of CBLOF, which has been selected as a preferred unsupervised method for the determination of relevant hijacking tweets. In future, a comparison will be done between supervised learning methods and the unsupervised methods presented in this work on larger dataset. Optimisation mechanisms will also be employed in order to increase the overall performance.
</details>
<details>
<summary>摘要</summary>
在南非， vehicular hijacking 问题日益严重。这使得旅行者们constantemente在担忧成为受害者的风险。本工作提出了一种新的半监督方法，使用 Twitter 上的异常检测算法来识别劫持事件。利用 TF-IDF 加以处理的 tweets 中包含 "劫持" 关键词，并使用 KNN 和 CBLOF 两种异常检测算法进行分析。对比评估表明，KNN 方法的准确率为 89%，CBLOF 方法的准确率为 90%，CBLOF 方法还可以获得 F1-Score 0.8，而 KNN 方法的 F1-Score 为 0.78。因此，CBLOF 方法在劫持 tweets 的识别中赢得了一定的优势，因此被选为半监督方法。未来，将对大型数据集进行比较，以及使用优化机制提高总性能。
</details></li>
</ul>
<hr>
<h2 id="Effects-of-Convolutional-Autoencoder-Bottleneck-Width-on-StarGAN-based-Singing-Technique-Conversion"><a href="#Effects-of-Convolutional-Autoencoder-Bottleneck-Width-on-StarGAN-based-Singing-Technique-Conversion" class="headerlink" title="Effects of Convolutional Autoencoder Bottleneck Width on StarGAN-based Singing Technique Conversion"></a>Effects of Convolutional Autoencoder Bottleneck Width on StarGAN-based Singing Technique Conversion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10021">http://arxiv.org/abs/2308.10021</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tung-Cheng Su, Yung-Chuan Chang, Yi-Wen Liu</li>
<li>for: 将一种声音技巧转换为另一种声音技巧，保持原始歌手身份、旋律和语言元素不变</li>
<li>methods: 使用 Generative Adversarial Networks (GANs) 和 Convolutional Autoencoders (CAEs) 进行转换</li>
<li>results: 研究发现，宽度更大的瓶颈对转换质量有着良好的影响，但并不一定导致更高的类似性于目标技巧。其中，折唇声是转换最容易的目标，而其他三种声音技巧作为源则更能够生成更加吸引人的转换结果。<details>
<summary>Abstract</summary>
Singing technique conversion (STC) refers to the task of converting from one voice technique to another while leaving the original singer identity, melody, and linguistic components intact. Previous STC studies, as well as singing voice conversion research in general, have utilized convolutional autoencoders (CAEs) for conversion, but how the bottleneck width of the CAE affects the synthesis quality has not been thoroughly evaluated. To this end, we constructed a GAN-based multi-domain STC system which took advantage of the WORLD vocoder representation and the CAE architecture. We varied the bottleneck width of the CAE, and evaluated the conversion results subjectively. The model was trained on a Mandarin dataset which features four singers and four singing techniques: the chest voice, the falsetto, the raspy voice, and the whistle voice. The results show that a wider bottleneck corresponds to better articulation clarity but does not necessarily lead to higher likeness to the target technique. Among the four techniques, we also found that the whistle voice is the easiest target for conversion, while the other three techniques as a source produce more convincing conversion results than the whistle.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Semi-Implicit-Variational-Inference-via-Score-Matching"><a href="#Semi-Implicit-Variational-Inference-via-Score-Matching" class="headerlink" title="Semi-Implicit Variational Inference via Score Matching"></a>Semi-Implicit Variational Inference via Score Matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10014">http://arxiv.org/abs/2308.10014</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/longinyu/sivism">https://github.com/longinyu/sivism</a></li>
<li>paper_authors: Longlin Yu, Cheng Zhang</li>
<li>for: 提高变量家族表达力，使其能够更好地捕捉复杂的 bayesian 推理问题。</li>
<li>methods: 基于代理证明对象的得分匹配方法，利用层次结构来自然地处理不可读取的变量分布。</li>
<li>results: 与 MCMC 相当精准，并且超过 ELBO 基于的 SIVI 方法在多种 bayesian 推理任务中表现出色。<details>
<summary>Abstract</summary>
Semi-implicit variational inference (SIVI) greatly enriches the expressiveness of variational families by considering implicit variational distributions defined in a hierarchical manner. However, due to the intractable densities of variational distributions, current SIVI approaches often use surrogate evidence lower bounds (ELBOs) or employ expensive inner-loop MCMC runs for unbiased ELBOs for training. In this paper, we propose SIVI-SM, a new method for SIVI based on an alternative training objective via score matching. Leveraging the hierarchical structure of semi-implicit variational families, the score matching objective allows a minimax formulation where the intractable variational densities can be naturally handled with denoising score matching. We show that SIVI-SM closely matches the accuracy of MCMC and outperforms ELBO-based SIVI methods in a variety of Bayesian inference tasks.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate into Simplified Chinese semi-implicit variational inference (SIVI)  greatly enriches the expressiveness of variational families by considering implicit variational distributions defined in a hierarchical manner. However, due to the intractable densities of variational distributions, current SIVI approaches often use surrogate evidence lower bounds (ELBOs) or employ expensive inner-loop MCMC runs for unbiased ELBOs for training. In this paper, we propose SIVI-SM, a new method for SIVI based on an alternative training objective via score matching. Leveraging the hierarchical structure of semi-implicit variational families, the score matching objective allows a minimax formulation where the intractable variational densities can be naturally handled with denoising score matching. We show that SIVI-SM closely matches the accuracy of MCMC and outperforms ELBO-based SIVI methods in a variety of Bayesian inference tasks.Translate completed.
</details></li>
</ul>
<hr>
<h2 id="Distributionally-Robust-Cross-Subject-EEG-Decoding"><a href="#Distributionally-Robust-Cross-Subject-EEG-Decoding" class="headerlink" title="Distributionally Robust Cross Subject EEG Decoding"></a>Distributionally Robust Cross Subject EEG Decoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11651">http://arxiv.org/abs/2308.11651</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tiehang Duan, Zhenyi Wang, Gianfranco Doretto, Fang Li, Cui Tao, Donald Adjeroh</li>
<li>for: 提高 Electroencephalography (EEG) 解码任务的性能</li>
<li>methods: 使用分布robust优化和 Wasserstein gradient flow (WGF) 进行数据演化</li>
<li>results: 比基eline方法 significanly 提高解码性能，特别是在具有各种损害的 EEG 信号下<details>
<summary>Abstract</summary>
Recently, deep learning has shown to be effective for Electroencephalography (EEG) decoding tasks. Yet, its performance can be negatively influenced by two key factors: 1) the high variance and different types of corruption that are inherent in the signal, 2) the EEG datasets are usually relatively small given the acquisition cost, annotation cost and amount of effort needed. Data augmentation approaches for alleviation of this problem have been empirically studied, with augmentation operations on spatial domain, time domain or frequency domain handcrafted based on expertise of domain knowledge. In this work, we propose a principled approach to perform dynamic evolution on the data for improvement of decoding robustness. The approach is based on distributionally robust optimization and achieves robustness by optimizing on a family of evolved data distributions instead of the single training data distribution. We derived a general data evolution framework based on Wasserstein gradient flow (WGF) and provides two different forms of evolution within the framework. Intuitively, the evolution process helps the EEG decoder to learn more robust and diverse features. It is worth mentioning that the proposed approach can be readily integrated with other data augmentation approaches for further improvements. We performed extensive experiments on the proposed approach and tested its performance on different types of corrupted EEG signals. The model significantly outperforms competitive baselines on challenging decoding scenarios.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Disposable-Transfer-Learning-for-Selective-Source-Task-Unlearning"><a href="#Disposable-Transfer-Learning-for-Selective-Source-Task-Unlearning" class="headerlink" title="Disposable Transfer Learning for Selective Source Task Unlearning"></a>Disposable Transfer Learning for Selective Source Task Unlearning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09971">http://arxiv.org/abs/2308.09971</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seunghee Koh, Hyounguk Shon, Janghyeon Lee, Hyeong Gwon Hong, Junmo Kim</li>
<li>for: 本研究旨在提出一种新的转移学习方法，即可 dispose 的转移学习（DTL），以便在转移学习过程中保留目标任务的表现。</li>
<li>methods: 本研究提出了一种新的损失函数名为 Gradient Collision loss (GC loss)，用于 selectively 忘记源任务知识。 GC loss 使得梯度向量在不同批处理中的方向不同，以便减少知识泄露。</li>
<li>results: 研究表明，GC loss 是一种有效的方法来解决转移学习问题，可以保留目标任务表现，同时减少知识泄露。<details>
<summary>Abstract</summary>
Transfer learning is widely used for training deep neural networks (DNN) for building a powerful representation. Even after the pre-trained model is adapted for the target task, the representation performance of the feature extractor is retained to some extent. As the performance of the pre-trained model can be considered the private property of the owner, it is natural to seek the exclusive right of the generalized performance of the pre-trained weight. To address this issue, we suggest a new paradigm of transfer learning called disposable transfer learning (DTL), which disposes of only the source task without degrading the performance of the target task. To achieve knowledge disposal, we propose a novel loss named Gradient Collision loss (GC loss). GC loss selectively unlearns the source knowledge by leading the gradient vectors of mini-batches in different directions. Whether the model successfully unlearns the source task is measured by piggyback learning accuracy (PL accuracy). PL accuracy estimates the vulnerability of knowledge leakage by retraining the scrubbed model on a subset of source data or new downstream data. We demonstrate that GC loss is an effective approach to the DTL problem by showing that the model trained with GC loss retains the performance on the target task with a significantly reduced PL accuracy.
</details>
<details>
<summary>摘要</summary>
<<SYS>>TRANSLATE_TEXTTransfer learning 广泛用于训练深度神经网络（DNN）以建立强大的表示。即使预训练模型被适应目标任务，表示性性能 OF 特征提取器也会保持一定程度的遗传。由于预训练模型的性能可以视为专有财产，因此是自然的寻求专利权的通用性表现。 To address this issue, we propose a new paradigm of transfer learning called disposable transfer learning (DTL), which discards only the source task without degrading the performance of the target task. To achieve knowledge disposal, we propose a novel loss function named Gradient Collision loss (GC loss). GC loss selectively unlearns the source knowledge by leading the gradient vectors of mini-batches in different directions. Whether the model successfully unlearns the source task is measured by piggyback learning accuracy (PL accuracy). PL accuracy estimates the vulnerability of knowledge leakage by retraining the scrubbed model on a subset of source data or new downstream data. We demonstrate that GC loss is an effective approach to the DTL problem by showing that the model trained with GC loss retains the performance on the target task with a significantly reduced PL accuracy.Note: The translation is done using a machine translation tool, and may not be perfect or idiomatic.
</details></li>
</ul>
<hr>
<h2 id="Tackling-Vision-Language-Tasks-Through-Learning-Inner-Monologues"><a href="#Tackling-Vision-Language-Tasks-Through-Learning-Inner-Monologues" class="headerlink" title="Tackling Vision Language Tasks Through Learning Inner Monologues"></a>Tackling Vision Language Tasks Through Learning Inner Monologues</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09970">http://arxiv.org/abs/2308.09970</a></li>
<li>repo_url: None</li>
<li>paper_authors: Diji Yang, Kezhen Chen, Jinmeng Rao, Xiaoyuan Guo, Yawen Zhang, Jie Yang, Yi Zhang</li>
<li>for: 解决复杂的视觉语言问题，通过内部对话机制来协调语言模型和视觉模型。</li>
<li>methods: 提出了一种新的 Inner Monologue Multi-Modal Optimization（IMMO）方法，通过自然语言对话来促进语言模型和视觉模型之间的交互，并采用两个阶段训练方式来学习内部对话过程。</li>
<li>results: 对两个popular任务进行评估，结果表明，通过模拟内部对话机制，IMMO可以提高语义解释能力和推理能力，从而更有效地融合视觉和语言模型。此外，IMMO不需要人工定制的对话，可以在多个AI问题中广泛应用。<details>
<summary>Abstract</summary>
Visual language tasks require AI models to comprehend and reason with both visual and textual content. Driven by the power of Large Language Models (LLMs), two prominent methods have emerged: (1) the hybrid integration between LLMs and Vision-Language Models (VLMs), where visual inputs are firstly converted into language descriptions by VLMs, serving as inputs for LLMs to generate final answer(s); (2) visual feature alignment in language space, where visual inputs are encoded as embeddings and projected to LLMs' language space via further supervised fine-tuning. The first approach provides light training costs and interpretability but is hard to be optimized in an end-to-end fashion. The second approach presents decent performance, but feature alignment usually requires large amounts of training data and lacks interpretability. To tackle this dilemma, we propose a novel approach, Inner Monologue Multi-Modal Optimization (IMMO), to solve complex vision language problems by simulating inner monologue processes, a cognitive process in which an individual engages in silent verbal communication with themselves. We enable LLMs and VLMs to interact through natural language conversation and propose to use a two-stage training process to learn how to do the inner monologue (self-asking questions and answering questions). IMMO is evaluated on two popular tasks and the results suggest by emulating the cognitive phenomenon of internal dialogue, our approach can enhance reasoning and explanation abilities, contributing to the more effective fusion of vision and language models. More importantly, instead of using predefined human-crafted monologues, IMMO learns this process within the deep learning models, promising wider applicability to many different AI problems beyond vision language tasks.
</details>
<details>
<summary>摘要</summary>
“视觉语言任务需要人工智能模型理解和处理视觉和文本内容。受大语言模型（LLM）的能力驱动，两种主要方法得到推广：（1）将视觉输入转换为语言描述，并将其作为LLM的输入进行生成答案（2）在语言空间中对视觉输入进行可视化特征对齐，通过进一步的超vision fine-tuning来实现。首种方法可以减少训练成本和提高可读性，但实际上困难以在端到端方式进行优化。第二种方法可以达到可 Acceptable performance，但特征对齐通常需要大量的训练数据，并且缺乏可读性。为了解决这个困境，我们提出了一种新的方法——内部对话多模态优化（IMMO），用于解决复杂的视觉语言问题。我们通过模拟内部对话过程，让LLM和VLM之间进行自然语言交流，并提出了一个两阶段训练过程，以学习如何进行内部对话（自我问答和回答问题）。IMMO在两个流行任务上进行评估，结果表明，通过模拟内部对话，我们的方法可以提高理解和解释能力，为视觉语言模型的融合做出更有效的贡献。更重要的是，IMMO不使用预先定义的人类编写的对话，而是在深度学习模型中学习这个过程，这意味着我们的方法可以在许多不同的AI问题中应用。”
</details></li>
</ul>
<hr>
<h2 id="Anomaly-Aware-Semantic-Segmentation-via-Style-Aligned-OoD-Augmentation"><a href="#Anomaly-Aware-Semantic-Segmentation-via-Style-Aligned-OoD-Augmentation" class="headerlink" title="Anomaly-Aware Semantic Segmentation via Style-Aligned OoD Augmentation"></a>Anomaly-Aware Semantic Segmentation via Style-Aligned OoD Augmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09965">http://arxiv.org/abs/2308.09965</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dan Zhang, Kaspar Sakmann, William Beluch, Robin Hutmacher, Yumeng Li</li>
<li>for: 提高自动驾驶中的未知物体检测精度</li>
<li>methods: 使用精度更高的OoD数据生成，并提出一种简单的精度调整方法</li>
<li>results: 通过少量的微调，可以使用预训练模型进行异常检测，并保持原始任务的性能In English, this translates to:</li>
<li>for: Improving the accuracy of unknown object detection in autonomous driving</li>
<li>methods: Using higher-quality OoD data generation, and proposing a simple fine-tuning method</li>
<li>results: By minimally fine-tuning a pre-trained model, we can use it for anomaly detection while maintaining the performance on the original task.<details>
<summary>Abstract</summary>
Within the context of autonomous driving, encountering unknown objects becomes inevitable during deployment in the open world. Therefore, it is crucial to equip standard semantic segmentation models with anomaly awareness. Many previous approaches have utilized synthetic out-of-distribution (OoD) data augmentation to tackle this problem. In this work, we advance the OoD synthesis process by reducing the domain gap between the OoD data and driving scenes, effectively mitigating the style difference that might otherwise act as an obvious shortcut during training. Additionally, we propose a simple fine-tuning loss that effectively induces a pre-trained semantic segmentation model to generate a ``none of the given classes" prediction, leveraging per-pixel OoD scores for anomaly segmentation. With minimal fine-tuning effort, our pipeline enables the use of pre-trained models for anomaly segmentation while maintaining the performance on the original task.
</details>
<details>
<summary>摘要</summary>
在自动驾驶中，遇到未知对象是不可避免的，因此需要在开放世界中部署标准 semantic segmentation 模型时提供异常意识。许多先前的方法使用了 synthetic out-of-distribution（OoD）数据增强来解决这个问题。在这种工作中，我们将 OoD 数据增强过程进行了改进，将驱动场景和 OoD 数据之间的领域差减少到最小化，从而减轻了训练中可能会作为短cut的样式差异。此外，我们提议一种简单的精通化损失函数，使得先验性学习的 semantic segmentation 模型在训练中能够生成“无任何给定类”的预测，利用每个像素的 OoD 分数进行异常分 segmentation。与 minimal fine-tuning 努力相比，我们的管道可以使用先验性学习的模型进行异常分 segmentation，同时保持原始任务的性能。
</details></li>
</ul>
<hr>
<h2 id="Towards-Self-Adaptive-Machine-Learning-Enabled-Systems-Through-QoS-Aware-Model-Switching"><a href="#Towards-Self-Adaptive-Machine-Learning-Enabled-Systems-Through-QoS-Aware-Model-Switching" class="headerlink" title="Towards Self-Adaptive Machine Learning-Enabled Systems Through QoS-Aware Model Switching"></a>Towards Self-Adaptive Machine Learning-Enabled Systems Through QoS-Aware Model Switching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09960">http://arxiv.org/abs/2308.09960</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sa4s-serc/adamls">https://github.com/sa4s-serc/adamls</a></li>
<li>paper_authors: Shubham Kulkarni, Arya Marda, Karthik Vaidhyanathan</li>
<li>For: 本研究旨在提出一种机器学习模型均衡器，以管理运行时uncertainty，提高机器学习生成系统（MLS）的可靠性和性能。* Methods: 本研究提出了一种基于多模型自适应的机器学习模型均衡器（AdaMLS），通过动态模型交换来维护系统和模型的性能平衡。* Results: 通过一个基于物体检测的自适应对象检测系统的证明，研究人员发现 AdaMLS 可以在不可预测的环境下提供优化的QoS保证，比Naive和单个最佳方案更高。<details>
<summary>Abstract</summary>
Machine Learning (ML), particularly deep learning, has seen vast advancements, leading to the rise of Machine Learning-Enabled Systems (MLS). However, numerous software engineering challenges persist in propelling these MLS into production, largely due to various run-time uncertainties that impact the overall Quality of Service (QoS). These uncertainties emanate from ML models, software components, and environmental factors. Self-adaptation techniques present potential in managing run-time uncertainties, but their application in MLS remains largely unexplored. As a solution, we propose the concept of a Machine Learning Model Balancer, focusing on managing uncertainties related to ML models by using multiple models. Subsequently, we introduce AdaMLS, a novel self-adaptation approach that leverages this concept and extends the traditional MAPE-K loop for continuous MLS adaptation. AdaMLS employs lightweight unsupervised learning for dynamic model switching, thereby ensuring consistent QoS. Through a self-adaptive object detection system prototype, we demonstrate AdaMLS's effectiveness in balancing system and model performance. Preliminary results suggest AdaMLS surpasses naive and single state-of-the-art models in QoS guarantees, heralding the advancement towards self-adaptive MLS with optimal QoS in dynamic environments.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Comparison-of-Adversarial-Learning-Techniques-for-Malware-Detection"><a href="#A-Comparison-of-Adversarial-Learning-Techniques-for-Malware-Detection" class="headerlink" title="A Comparison of Adversarial Learning Techniques for Malware Detection"></a>A Comparison of Adversarial Learning Techniques for Malware Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09958">http://arxiv.org/abs/2308.09958</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pavla Louthánová, Matouš Kozák, Martin Jureček, Mark Stamp</li>
<li>for: 本文 addresses the problem of generating adversarial malware samples, specifically malicious Windows Portable Executable files, to evaluate the effectiveness of different methods in evading machine learning-based malware detection.</li>
<li>methods: 本文使用 gradient-based, evolutionary algorithm-based, 和 reinforcement-based methods to generate adversarial samples, and then tests the generated samples against selected antivirus products.</li>
<li>results: 结果显示，使用优化后的恶意软件amples可以导致 incorrectly classify the file as benign, 并且生成的恶意软件amples可以成功用于其他检测模型。使用多个生成器可以创建新的恶意软件amples，并且使用 Gym-malware generator 可以 achieve the highest practical potential.<details>
<summary>Abstract</summary>
Machine learning has proven to be a useful tool for automated malware detection, but machine learning models have also been shown to be vulnerable to adversarial attacks. This article addresses the problem of generating adversarial malware samples, specifically malicious Windows Portable Executable files. We summarize and compare work that has focused on adversarial machine learning for malware detection. We use gradient-based, evolutionary algorithm-based, and reinforcement-based methods to generate adversarial samples, and then test the generated samples against selected antivirus products. We compare the selected methods in terms of accuracy and practical applicability. The results show that applying optimized modifications to previously detected malware can lead to incorrect classification of the file as benign. It is also known that generated malware samples can be successfully used against detection models other than those used to generate them and that using combinations of generators can create new samples that evade detection. Experiments show that the Gym-malware generator, which uses a reinforcement learning approach, has the greatest practical potential. This generator achieved an average sample generation time of 5.73 seconds and the highest average evasion rate of 44.11%. Using the Gym-malware generator in combination with itself improved the evasion rate to 58.35%.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="To-prune-or-not-to-prune-A-chaos-causality-approach-to-principled-pruning-of-dense-neural-networks"><a href="#To-prune-or-not-to-prune-A-chaos-causality-approach-to-principled-pruning-of-dense-neural-networks" class="headerlink" title="To prune or not to prune : A chaos-causality approach to principled pruning of dense neural networks"></a>To prune or not to prune : A chaos-causality approach to principled pruning of dense neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09955">http://arxiv.org/abs/2308.09955</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rajan Sahu, Shivam Chadha, Nithin Nagaraj, Archana Mathur, Snehanshu Saha</li>
<li>for: 这篇论文的目的是提出一种基于 chaos in learning 的 neural network 缩小方法，以维持网络性能并且保留特征解释性。</li>
<li>methods: 本篇论文使用了 weight update 的 chaos in learning 来定义适当的缩小策略，并且通过 causality 来特定引起错分类的几个重要权重。</li>
<li>results: 根据实验结果显示，这种缩小策略可以将网络大小缩小到原来的一半，而且网络性能仍然保持在原本水准。同时，这种缩小策略仍然可以保留网络的特征解释性。<details>
<summary>Abstract</summary>
Reducing the size of a neural network (pruning) by removing weights without impacting its performance is an important problem for resource-constrained devices. In the past, pruning was typically accomplished by ranking or penalizing weights based on criteria like magnitude and removing low-ranked weights before retraining the remaining ones. Pruning strategies may also involve removing neurons from the network in order to achieve the desired reduction in network size. We formulate pruning as an optimization problem with the objective of minimizing misclassifications by selecting specific weights. To accomplish this, we have introduced the concept of chaos in learning (Lyapunov exponents) via weight updates and exploiting causality to identify the causal weights responsible for misclassification. Such a pruned network maintains the original performance and retains feature explainability.
</details>
<details>
<summary>摘要</summary>
将神经网络的大小缩小（裁剪），无需影响其表现，是资源有限设备上的重要问题。在过去，裁剪通常通过按照字段大小或字段排名来选择丢弃重要性较低的字段，然后重新训练剩下的字段。裁剪策略可能还包括从网络中移除神经元，以达到预期的网络大小增加。我们将裁剪视为一个优化问题，并通过选择特定的字段来实现最小化错分。为此，我们引入了学习中的混乱（ Lyapunov 数据）via 字段更新，并利用因果关系来识别导致错分的字段。这样的裁剪网络保持原始表现，并保留特征解释性。
</details></li>
</ul>
<hr>
<h2 id="Finding-emergence-in-data-causal-emergence-inspired-dynamics-learning"><a href="#Finding-emergence-in-data-causal-emergence-inspired-dynamics-learning" class="headerlink" title="Finding emergence in data: causal emergence inspired dynamics learning"></a>Finding emergence in data: causal emergence inspired dynamics learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09952">http://arxiv.org/abs/2308.09952</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingzhe Yang, Zhipeng Wang, Kaiwei Liu, Yingqi Rong, Bing Yuan, Jiang Zhang</li>
<li>for: 这篇论文旨在开发一种基于数据驱动的模型，用于模拟复杂的动力系统，并可以有效地捕捉出 emergent 性质。</li>
<li>methods: 该论文提出了一种基于机器学习的框架，通过最大化有效信息（EI）来学习 macro-dinamics 模型，并且可以量化 emergence 在数据中。</li>
<li>results: 实验结果表明，该框架可以成功地捕捉出 emergent 模式，并且可以学习 coarse-graining 策略和量化数据中的 causal emergence 度。此外，对于不同于训练数据集的环境进行了测试，结果表明该模型具有出色的泛化能力。<details>
<summary>Abstract</summary>
Modelling complex dynamical systems in a data-driven manner is challenging due to the presence of emergent behaviors and properties that cannot be directly captured by micro-level observational data. Therefore, it is crucial to develop a model that can effectively capture emergent dynamics at the macro-level and quantify emergence based on the available data. Drawing inspiration from the theory of causal emergence, this paper introduces a machine learning framework aimed at learning macro-dynamics within an emergent latent space. The framework achieves this by maximizing the effective information (EI) to obtain a macro-dynamics model with stronger causal effects. Experimental results on both simulated and real data demonstrate the effectiveness of the proposed framework. Not only does it successfully capture emergent patterns, but it also learns the coarse-graining strategy and quantifies the degree of causal emergence in the data. Furthermore, experiments conducted on environments different from the training dataset highlight the superior generalization ability of our model.
</details>
<details>
<summary>摘要</summary>
模拟复杂动力系统的数据驱动方法是具有挑战性的，因为存在不可直接捕捉的emergent行为和性质。因此，需要开发一个能够有效捕捉emergent dynamics的 macro-级模型，并且量化emergence基于可用的数据。 drawing inspiration from the theory of causal emergence，本文提出了一种基于机器学习的框架，用于在emergent latent space中学习macro-dynamics。该框架通过最大化有效信息（EI）来获得一个具有更强的 causal effect的macro-dynamics模型。实验结果表明，该模型不仅能成功捕捉emergent pattern，还能学习coarse-graining strategy和量化数据中的causal emergence度。此外，在不同于训练数据的环境下进行的实验还表明了我们的模型具有更高的泛化能力。
</details></li>
</ul>
<hr>
<h2 id="Study-on-the-effectiveness-of-AutoML-in-detecting-cardiovascular-disease"><a href="#Study-on-the-effectiveness-of-AutoML-in-detecting-cardiovascular-disease" class="headerlink" title="Study on the effectiveness of AutoML in detecting cardiovascular disease"></a>Study on the effectiveness of AutoML in detecting cardiovascular disease</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09947">http://arxiv.org/abs/2308.09947</a></li>
<li>repo_url: None</li>
<li>paper_authors: T. V. Afanasieva, A. P. Kuzlyakin, A. V. Komolov</li>
<li>for: 这个研究旨在开发和应用patient-oriented系统，以帮助患有 chronic noncommunicable diseases 的患者早发现和预测心血管疾病。</li>
<li>methods: 该研究使用了自动机器学习（AutoML）技术，可以简化和加速AI&#x2F;ML应用的开发过程，使得医疗专业人员可以更容易地使用这些应用。</li>
<li>results: 研究发现，自动机器学习模型在检测心血管疾病方面的准确率在87.41%至92.3%之间，最高准确率达92.3%，并且发现数据Normalization技术对模型的准确率有较大影响。<details>
<summary>Abstract</summary>
Cardiovascular diseases are widespread among patients with chronic noncommunicable diseases and are one of the leading causes of death, including in the working age. The article presents the relevance of the development and application of patient-oriented systems, in which machine learning (ML) is a promising technology that allows predicting cardiovascular diseases. Automated machine learning (AutoML) makes it possible to simplify and speed up the process of developing AI/ML applications, which is key in the development of patient-oriented systems by application users, in particular medical specialists. The authors propose a framework for the application of automatic machine learning and three scenarios that allowed for data combining five data sets of cardiovascular disease indicators from the UCI Machine Learning Repository to investigate the effectiveness in detecting this class of diseases. The study investigated one AutoML model that used and optimized the hyperparameters of thirteen basic ML models (KNeighborsUnif, KNeighborsDist, LightGBMXT, LightGBM, RandomForestGini, RandomForestEntr, CatBoost, ExtraTreesGini, ExtraTreesEntr, NeuralNetFastA, XGBoost, NeuralNetTorch, LightGBMLarge) and included the most accurate models in the weighted ensemble. The results of the study showed that the structure of the AutoML model for detecting cardiovascular diseases depends not only on the efficiency and accuracy of the basic models used, but also on the scenarios for preprocessing the initial data, in particular, on the technique of data normalization. The comparative analysis showed that the accuracy of the AutoML model in detecting cardiovascular disease varied in the range from 87.41% to 92.3%, and the maximum accuracy was obtained when normalizing the source data into binary values, and the minimum was obtained when using the built-in AutoML technique.
</details>
<details>
<summary>摘要</summary>
心血管疾病非常普遍 среди慢性非传染疾病患者，是死亡的主要原因之一，包括在工作年龄。这篇文章介绍了开发和应用 patient-oriented 系统的重要性，其中机器学习（ML）是一种承诺的技术，可以预测心血管疾病。自动机器学习（AutoML）使得开发 AI/ML 应用的过程可以简化和加速，这对医疗专业人员特别重要。作者提出了一个框架，并在五个数据集中组合了心血管疾病指标数据，以调查这类疾病的检测效果。研究中使用了一个 AutoML 模型，该模型使用和优化了十三种基本 ML 模型（KNeighborsUnif、KNeighborsDist、LightGBMXT、LightGBM、RandomForestGini、RandomForestEntr、CatBoost、ExtraTreesGini、ExtraTreesEntr、NeuralNetFastA、XGBoost、NeuralNetTorch、LightGBMLarge），并包括最佳模型在权重ensemble中。研究结果表明，AutoML 模型的结构不仅受到基本模型的效率和准确度影响，还受到数据预处理方法的选择，特别是数据normalization技术。比较分析表明，AutoML 模型在检测心血管疾病方面的准确率在87.41%到92.3%之间，最高准确率为对源数据进行二分化normalization，最低准确率为使用自动 ML 技术。
</details></li>
</ul>
<hr>
<h2 id="Dual-Branch-Deep-Learning-Network-for-Detection-and-Stage-Grading-of-Diabetic-Retinopathy"><a href="#Dual-Branch-Deep-Learning-Network-for-Detection-and-Stage-Grading-of-Diabetic-Retinopathy" class="headerlink" title="Dual Branch Deep Learning Network for Detection and Stage Grading of Diabetic Retinopathy"></a>Dual Branch Deep Learning Network for Detection and Stage Grading of Diabetic Retinopathy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09945">http://arxiv.org/abs/2308.09945</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hossein Shakibania, Sina Raoufi, Behnam Pourafkham, Hassan Khotanlou, Muharram Mansoorizadeh</li>
<li>for: 这个论文的目的是为了检测和分级糖尿病性视网膜病变，使用单一的视网膜图像。</li>
<li>methods: 这个模型使用了转移学习，使用两个现有的顶尖预训模型作为特征提取器，并对新的数据集进行精确化。</li>
<li>results: 这个模型在APTOS 2019数据集上实现了优异的表现，在糖尿病性视网膜检测和分级中，实现了98.50%的准确率、99.46%的感度和97.51%的特异度。<details>
<summary>Abstract</summary>
Diabetic retinopathy is a severe complication of diabetes that can lead to permanent blindness if not treated promptly. Early and accurate diagnosis of the disease is essential for successful treatment. This paper introduces a deep learning method for the detection and stage grading of diabetic retinopathy, using a single fundus retinal image. Our model utilizes transfer learning, employing two state-of-the-art pre-trained models as feature extractors and fine-tuning them on a new dataset. The proposed model is trained on a large multi-center dataset, including the APTOS 2019 dataset, obtained from publicly available sources. It achieves remarkable performance in diabetic retinopathy detection and stage classification on the APTOS 2019, outperforming the established literature. For binary classification, the proposed approach achieves an accuracy of 98.50%, a sensitivity of 99.46%, and a specificity of 97.51%. In stage grading, it achieves a quadratic weighted kappa of 93.00%, an accuracy of 89.60%, a sensitivity of 89.60%, and a specificity of 97.72%. The proposed approach serves as a reliable screening and stage grading tool for diabetic retinopathy, offering significant potential to enhance clinical decision-making and patient care.
</details>
<details>
<summary>摘要</summary>
糖尿病肠病是糖尿病的严重并发症，如果不及时治疗，可能会导致永久潦积。早期和准确的诊断是成功治疗的关键。本文提出了一种深度学习方法，用于检测和评分糖尿病肠病，只需一张背部照片。我们的模型使用了传输学习，使用两个国际先进的预训练模型，并对其进行精细调整。我们的模型在APTOS 2019数据集上训练，并在这个数据集上实现了糖尿病肠病检测和评分的优异表现，比Literature中的已知方法更出色。为二分类问题，我们的方法实现了98.50%的准确率，99.46%的感知率和97.51%的特异性。在评分问题上，我们的方法实现了93.00%的卷积权重κ值，89.60%的准确率，89.60%的感知率和97.72%的特异性。我们的方法可以作为糖尿病肠病检测和评分工具，为临床决策和患者护理带来了重要的可能性。
</details></li>
</ul>
<hr>
<h2 id="On-the-Robustness-of-Open-World-Test-Time-Training-Self-Training-with-Dynamic-Prototype-Expansion"><a href="#On-the-Robustness-of-Open-World-Test-Time-Training-Self-Training-with-Dynamic-Prototype-Expansion" class="headerlink" title="On the Robustness of Open-World Test-Time Training: Self-Training with Dynamic Prototype Expansion"></a>On the Robustness of Open-World Test-Time Training: Self-Training with Dynamic Prototype Expansion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09942">http://arxiv.org/abs/2308.09942</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yushu-li/owttt">https://github.com/yushu-li/owttt</a></li>
<li>paper_authors: Yushu Li, Xun Xu, Yongyi Su, Kui Jia</li>
<li>for: 该论文旨在提高 unknown 目标频谱分布下的深度学习模型的泛化性，并且具有低延迟。</li>
<li>methods: 该论文提出了一种基于 test-time training&#x2F;adaptation (TTT&#x2F;TTA) 的方法，并且对存在强度外部数据的情况进行了研究。</li>
<li>results: 该论文在 5 个 open-world test-time training (OWTTT)  benchmark 上达到了 state-of-the-art 性能，并且提出了一种 adaptive strong OOD pruning 和动态扩展 prototype 的方法来提高模型的 robustness。I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Generalizing deep learning models to unknown target domain distribution with low latency has motivated research into test-time training/adaptation (TTT/TTA). Existing approaches often focus on improving test-time training performance under well-curated target domain data. As figured out in this work, many state-of-the-art methods fail to maintain the performance when the target domain is contaminated with strong out-of-distribution (OOD) data, a.k.a. open-world test-time training (OWTTT). The failure is mainly due to the inability to distinguish strong OOD samples from regular weak OOD samples. To improve the robustness of OWTTT we first develop an adaptive strong OOD pruning which improves the efficacy of the self-training TTT method. We further propose a way to dynamically expand the prototypes to represent strong OOD samples for an improved weak/strong OOD data separation. Finally, we regularize self-training with distribution alignment and the combination yields the state-of-the-art performance on 5 OWTTT benchmarks. The code is available at https://github.com/Yushu-Li/OWTTT.
</details>
<details>
<summary>摘要</summary>
通过快速适应Unknown目标分布，深度学习模型的普及化已成为研究焦点。现有方法通常是在Well-curated目标分布数据下提高测试时训练性能。然而，这些state-of-the-art方法在Open-world测试时训练（OWTTT）中表现不佳，主要是因为无法分辨强OOD样本（out-of-distribution）和弱OOD样本（weak out-of-distribution）之间的差异。为了改进OWTTT的Robustness，我们首先开发了自适应强OOD��ppring，以提高自学习TTT方法的效果。然后，我们提议在运行时动态扩展表例，以更好地分离弱OOD样本和强OOD样本。最后，我们添加了分布对齐的REG regularization，这种组合得到了5个OWTTT标准测试 benchmarks的state-of-the-art性能。代码可以在https://github.com/Yushu-Li/OWTTT中找到。
</details></li>
</ul>
<hr>
<h2 id="Practical-Anomaly-Detection-over-Multivariate-Monitoring-Metrics-for-Online-Services"><a href="#Practical-Anomaly-Detection-over-Multivariate-Monitoring-Metrics-for-Online-Services" class="headerlink" title="Practical Anomaly Detection over Multivariate Monitoring Metrics for Online Services"></a>Practical Anomaly Detection over Multivariate Monitoring Metrics for Online Services</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09937">http://arxiv.org/abs/2308.09937</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/OpsPAI/CMAnomaly">https://github.com/OpsPAI/CMAnomaly</a></li>
<li>paper_authors: Jinyang Liu, Tianyi Yang, Zhuangbin Chen, Yuxin Su, Cong Feng, Zengyin Yang, Michael R. Lyu</li>
<li>for: 这个研究的目的是为了提出一个基于协同机器学习的异常探测框架，以便更好地探测现代软件系统中的异常情况。</li>
<li>methods: 这个研究使用了一种叫做协同机器学习的方法，以捕捉多元监控指标之间的相互依存关系，并且可以在线时间复杂度下进行效率地探测。</li>
<li>results: 实验结果显示，与现有基eline模型相比，CMAnomaly可以提高异常探测的精度，并且可以在10X到20X的速度上进行探测。此外，这个框架也在Huawei Cloud中进行了部署。<details>
<summary>Abstract</summary>
As modern software systems continue to grow in terms of complexity and volume, anomaly detection on multivariate monitoring metrics, which profile systems' health status, becomes more and more critical and challenging. In particular, the dependency between different metrics and their historical patterns plays a critical role in pursuing prompt and accurate anomaly detection. Existing approaches fall short of industrial needs for being unable to capture such information efficiently. To fill this significant gap, in this paper, we propose CMAnomaly, an anomaly detection framework on multivariate monitoring metrics based on collaborative machine. The proposed collaborative machine is a mechanism to capture the pairwise interactions along with feature and temporal dimensions with linear time complexity. Cost-effective models can then be employed to leverage both the dependency between monitoring metrics and their historical patterns for anomaly detection. The proposed framework is extensively evaluated with both public data and industrial data collected from a large-scale online service system of Huawei Cloud. The experimental results demonstrate that compared with state-of-the-art baseline models, CMAnomaly achieves an average F1 score of 0.9494, outperforming baselines by 6.77% to 10.68%, and runs 10X to 20X faster. Furthermore, we also share our experience of deploying CMAnomaly in Huawei Cloud.
</details>
<details>
<summary>摘要</summary>
现代软件系统在复杂性和规模上不断增长，异常检测在多变量监控指标上成为更加重要和挑战性的。特别是在不同指标之间的依赖关系以及历史 patrern 的情况下，异常检测变得更加重要。现有的方法无法有效地捕捉这些信息，因此在这篇论文中，我们提出了 CMAnomaly 异常检测框架，基于协同机器学习。我们的提议的协同机器是一种可以有效地捕捉多变量监控指标之间的对价关系，以及特征和时间维度的机制，具有线性时间复杂度。这使得可以使用便宜的模型来利用异常检测。我们的框架在大规模在线服务系统中进行了广泛的评估，结果显示，相比状态之前的基准模型，CMAnomaly 的平均 F1 分数为 0.9494，高于基准模型的 6.77% 到 10.68%，并且运行速度比基准模型快 10 倍到 20 倍。此外，我们还分享了在华为云上部署 CMAnomaly 的经验。
</details></li>
</ul>
<hr>
<h2 id="BLIVA-A-Simple-Multimodal-LLM-for-Better-Handling-of-Text-Rich-Visual-Questions"><a href="#BLIVA-A-Simple-Multimodal-LLM-for-Better-Handling-of-Text-Rich-Visual-Questions" class="headerlink" title="BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions"></a>BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09936">http://arxiv.org/abs/2308.09936</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mlpc-ucsd/bliva">https://github.com/mlpc-ucsd/bliva</a></li>
<li>paper_authors: Wenbo Hu, Yifan Xu, Yi Li, Weiyue Li, Zeyuan Chen, Zhuowen Tu</li>
<li>For: The paper aims to improve the ability of Vision Language Models (VLMs) to interpret images with text-rich context, which is a common occurrence in real-world scenarios.* Methods: The proposed method, called BLIVA, incorporates query embeddings from InstructBLIP and directly projects encoded patch embeddings into the LLM, a technique inspired by LLaVA. This approach helps the model capture intricate details potentially missed during the query decoding process.* Results: The proposed BLIVA model significantly enhances performance in processing text-rich VQA benchmarks (up to 17.76% in OCR-VQA benchmark) and typical VQA benchmarks (up to 7.9% in Visual Spatial Reasoning benchmark), compared to the baseline InstructBLIP. Additionally, BLIVA demonstrates significant capability in decoding real-world images, regardless of text presence.<details>
<summary>Abstract</summary>
Vision Language Models (VLMs), which extend Large Language Models (LLM) by incorporating visual understanding capability, have demonstrated significant advancements in addressing open-ended visual question-answering (VQA) tasks. However, these models cannot accurately interpret images infused with text, a common occurrence in real-world scenarios. Standard procedures for extracting information from images often involve learning a fixed set of query embeddings. These embeddings are designed to encapsulate image contexts and are later used as soft prompt inputs in LLMs. Yet, this process is limited to the token count, potentially curtailing the recognition of scenes with text-rich context. To improve upon them, the present study introduces BLIVA: an augmented version of InstructBLIP with Visual Assistant. BLIVA incorporates the query embeddings from InstructBLIP and also directly projects encoded patch embeddings into the LLM, a technique inspired by LLaVA. This approach assists the model to capture intricate details potentially missed during the query decoding process. Empirical evidence demonstrates that our model, BLIVA, significantly enhances performance in processing text-rich VQA benchmarks (up to 17.76\% in OCR-VQA benchmark) and in undertaking typical VQA benchmarks (up to 7.9\% in Visual Spatial Reasoning benchmark), comparing to our baseline InstructBLIP. BLIVA demonstrates significant capability in decoding real-world images, irrespective of text presence. To demonstrate the broad industry applications enabled by BLIVA, we evaluate the model using a new dataset comprising YouTube thumbnails paired with question-answer sets across 13 diverse categories. For researchers interested in further exploration, our code and models are freely accessible at https://github.com/mlpc-ucsd/BLIVA.git
</details>
<details>
<summary>摘要</summary>
《视觉语言模型（VLM）》，它们将大型语言模型（LLM）扩展以包含视觉理解能力，在开放式视觉问答任务（VQA）中显示出了重要的进步。然而，这些模型无法正确地解释含有文本的图像，这是现实世界中常见的情况。标准的图像信息提取方法通常包括学习固定的查询嵌入。这些嵌入用于在LLM中作为软提问输入，然而这种过程受到固定的token计数的限制，可能会遗弃场景中的文本背景。为此，本研究提出了BLIVA：一个基于InstructBLIP的增强版，它在LLM中直接将编码的质心嵌入投影到LLaVA的技术。这种方法帮助模型捕捉文本背景中的细节，可能在查询解码过程中被遗弃。empirical evidence表明，我们的模型BLIVA在处理含有文本的VQAbenchmark上（最高提升17.76%）和 Typical VQA benchmark上（最高提升7.9%）表现出色，相比基eline InstructBLIP。BLIVA在实际图像中解码表现出色，不管文本存在或不存在。为了展示BLIVA在广泛的产业应用中的应用前景，我们使用YouTube预览图片和相应的问答集来评估模型。对研究人员来说，我们在GitHub上提供了代码和模型，可以免费下载：https://github.com/mlpc-ucsd/BLIVA.git。
</details></li>
</ul>
<hr>
<h2 id="Analyzing-Quantization-in-TVM"><a href="#Analyzing-Quantization-in-TVM" class="headerlink" title="Analyzing Quantization in TVM"></a>Analyzing Quantization in TVM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10905">http://arxiv.org/abs/2308.10905</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingfei Guo</li>
<li>For: The paper aims to investigate the underperformance of 8-bit quantization in Tensor Virtual Machine (TVM) and to optimize the quantization process for deep learning models.* Methods: The paper uses TVM to perform 8-bit quantization on deep learning models and compares the performance with the non-quantized version. The authors also discuss various optimization techniques in TVM, such as graph building and memory access optimization, to improve the performance of quantized models.* Results: The paper achieves a 163.88% improvement in inference time for compute-bound tasks and a 194.98% improvement for memory-bound tasks compared to the TVM compiled baseline after addressing a bug in graph building and applying optimization strategies.<details>
<summary>Abstract</summary>
There has been many papers in academic literature on quantizing weight tensors in deep learning models to reduce inference latency and memory footprint. TVM also has the ability to quantize weights and support low-bit computations. Although quantization is typically expected to improve inference time, in TVM, the performance of 8-bit quantization does not meet the expectations. Typically, when applying 8-bit quantization to a deep learning model, it is usually expected to achieve around 50% of the full-precision inference time. However, in this particular case, not only does the quantized version fail to achieve the desired performance boost, but it actually performs worse, resulting in an inference time that is about 2 times as slow as the non-quantized version. In this project, we thoroughly investigate the reasons behind the underperformance and assess the compatibility and optimization opportunities of 8-bit quantization in TVM. We discuss the optimization of two different types of tasks: computation-bound and memory-bound, and provide a detailed comparison of various optimization techniques in TVM. Through the identification of performance issues, we have successfully improved quantization by addressing a bug in graph building. Furthermore, we analyze multiple optimization strategies to achieve the optimal quantization result. The best experiment achieves 163.88% improvement compared with the TVM compiled baseline in inference time for the compute-bound task and 194.98% for the memory-bound task.
</details>
<details>
<summary>摘要</summary>
有很多学术论文提出了深度学习模型中量化权重矩阵以降低推理时间和内存占用的想法。TVM也具有量化权重和低位计算的能力。although quantization 通常预计会改善推理时间，在 TVM 中，8 位量化的表现不符合预期。通常在应用 8 位量化深度学习模型时，预计可以达到约 50% 的全精度推理时间。但在这个特定情况下，量化版本并不只是不符合预期，而且实际更慢，导致推理时间约 double 非量化版本。在这个项目中，我们进行了深入的调查和分析，探讨 TVM 中 8 位量化的Compatibility和优化机会。我们分析了两种不同的任务类型：计算约束和内存约束，并对 TVM 中不同优化技术进行了详细的比较。通过发现性能问题，我们成功地修复了图像建立的漏洞，并分析了多种优化策略以实现最佳量化结果。最佳实验结果显示，与 TVM 编译基线相比，compute-bound 任务的推理时间提高了 163.88%，而 memory-bound 任务的推理时间提高了 194.98%。
</details></li>
</ul>
<hr>
<h2 id="East-Efficient-and-Accurate-Secure-Transformer-Framework-for-Inference"><a href="#East-Efficient-and-Accurate-Secure-Transformer-Framework-for-Inference" class="headerlink" title="East: Efficient and Accurate Secure Transformer Framework for Inference"></a>East: Efficient and Accurate Secure Transformer Framework for Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09923">http://arxiv.org/abs/2308.09923</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanchao Ding, Hua Guo, Yewei Guan, Weixin Liu, Jiarong Huo, Zhenyu Guan, Xiyong Zhang</li>
<li>for: 该论文旨在提供一个可靠和准确的安全Transformer推理框架，以保护用户的隐私。</li>
<li>methods: 该论文提出了一种新的幂等分割多项式评估算法，用于活动函数的评估，从而降低了GELU的运行时间和通信量。此外，该论文还为软max和层normalization的安全协议进行了仔细的设计，以保持所需的功能。</li>
<li>results: 该论文应用于BERT，并证明了在不需要精心调整的情况下，推理精度与明文推理相同。相比 iron，我们的方法具有1.8倍lower的通信量和1.2倍lower的运行时间。<details>
<summary>Abstract</summary>
Transformer has been successfully used in practical applications, such as ChatGPT, due to its powerful advantages. However, users' input is leaked to the model provider during the service. With people's attention to privacy, privacy-preserving Transformer inference is on the demand of such services. Secure protocols for non-linear functions are crucial in privacy-preserving Transformer inference, which are not well studied. Thus, designing practical secure protocols for non-linear functions is hard but significant to model performance. In this work, we propose a framework \emph{East} to enable efficient and accurate secure Transformer inference. Firstly, we propose a new oblivious piecewise polynomial evaluation algorithm and apply it to the activation functions, which reduces the runtime and communication of GELU by over 1.5$\times$ and 2.5$\times$, compared to prior arts. Secondly, the secure protocols for softmax and layer normalization are carefully designed to faithfully maintain the desired functionality. Thirdly, several optimizations are conducted in detail to enhance the overall efficiency. We applied \emph{East} to BERT and the results show that the inference accuracy remains consistent with the plaintext inference without fine-tuning. Compared to Iron, we achieve about 1.8$\times$ lower communication within 1.2$\times$ lower runtime.
</details>
<details>
<summary>摘要</summary>
“transformer”已经成功应用在实际应用中，例如ChatGPT，因为它具有强大的优势。然而，用户的输入会被提供者 During the service 泄露。随着人们对隐私的关注，隐私保护的transformer推理是实际的服务中的请求。对非线性函数的安全协议是这些服务中的重要课题，但是尚未得到充分的研究。因此，设计实用的安全协议 для非线性函数是具有挑战性和重要性的。在这个工作中，我们提出了一个名为“East”的框架，以实现有效和准确的隐私保护transformer推理。首先，我们提出了一个新的隐私 polynomial evaluation algorithm，并将其应用到活动函数中，这减少了GELU的runtime和通信量，相比于先前的艺术，则是1.5倍以上和2.5倍以上。其次，我们对于softmax和层Normalization进行了详细的设计，以确保所需的功能faithfully maintained。最后，我们在细节上进行了多个优化，以提高整体的效率。我们将“East”应用到BERT，结果显示，在不需要精确调整的情况下，推理精度与纯文本推理相同。相比于Iron，我们在1.2倍的runtime和1.8倍的通信量下可以 дости到相同的推理精度。”
</details></li>
</ul>
<hr>
<h2 id="EGANS-Evolutionary-Generative-Adversarial-Network-Search-for-Zero-Shot-Learning"><a href="#EGANS-Evolutionary-Generative-Adversarial-Network-Search-for-Zero-Shot-Learning" class="headerlink" title="EGANS: Evolutionary Generative Adversarial Network Search for Zero-Shot Learning"></a>EGANS: Evolutionary Generative Adversarial Network Search for Zero-Shot Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09915">http://arxiv.org/abs/2308.09915</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiming Chen, Shihuang Chen, Wenjin Hou, Weiping Ding, Xinge You</li>
<li>for: 这篇论文目的是提出了一种基于进化的对抗性探索方法（EGANS），用于实现零目标学习（ZSL）中的视觉标本生成。</li>
<li>methods: 这篇论文使用了对抗性探索（EGANS）来自动设计视觉生成网络，以提高ZSL中的视觉标本生成精度。</li>
<li>results: 实验结果显示，EGANS可以成功地对抗现有的对抗性探索方法，并在标准的CUB、SUN、AWA2和FLO datasets上实现了视觉标本生成中的改进。<details>
<summary>Abstract</summary>
Zero-shot learning (ZSL) aims to recognize the novel classes which cannot be collected for training a prediction model. Accordingly, generative models (e.g., generative adversarial network (GAN)) are typically used to synthesize the visual samples conditioned by the class semantic vectors and achieve remarkable progress for ZSL. However, existing GAN-based generative ZSL methods are based on hand-crafted models, which cannot adapt to various datasets/scenarios and fails to model instability. To alleviate these challenges, we propose evolutionary generative adversarial network search (termed EGANS) to automatically design the generative network with good adaptation and stability, enabling reliable visual feature sample synthesis for advancing ZSL. Specifically, we adopt cooperative dual evolution to conduct a neural architecture search for both generator and discriminator under a unified evolutionary adversarial framework. EGANS is learned by two stages: evolution generator architecture search and evolution discriminator architecture search. During the evolution generator architecture search, we adopt a many-to-one adversarial training strategy to evolutionarily search for the optimal generator. Then the optimal generator is further applied to search for the optimal discriminator in the evolution discriminator architecture search with a similar evolution search algorithm. Once the optimal generator and discriminator are searched, we entail them into various generative ZSL baselines for ZSL classification. Extensive experiments show that EGANS consistently improve existing generative ZSL methods on the standard CUB, SUN, AWA2 and FLO datasets. The significant performance gains indicate that the evolutionary neural architecture search explores a virgin field in ZSL.
</details>
<details>
<summary>摘要</summary>
zero-shot learning (ZSL) targets recognizing novel classes that cannot be collected for training a prediction model. Therefore, generative models (e.g., generative adversarial network (GAN)) are typically used to synthesize visual samples conditioned by the class semantic vectors and achieve remarkable progress for ZSL. However, existing GAN-based generative ZSL methods are based on hand-crafted models, which cannot adapt to various datasets/scenarios and fail to model instability. To address these challenges, we propose evolutionary generative adversarial network search (termed EGANS) to automatically design the generative network with good adaptation and stability, enabling reliable visual feature sample synthesis for advancing ZSL. Specifically, we adopt cooperative dual evolution to conduct a neural architecture search for both generator and discriminator under a unified evolutionary adversarial framework. EGANS is learned by two stages: evolution generator architecture search and evolution discriminator architecture search. During the evolution generator architecture search, we adopt a many-to-one adversarial training strategy to evolutionarily search for the optimal generator. Then the optimal generator is further applied to search for the optimal discriminator in the evolution discriminator architecture search with a similar evolution search algorithm. Once the optimal generator and discriminator are searched, we entail them into various generative ZSL baselines for ZSL classification. Extensive experiments show that EGANS consistently improve existing generative ZSL methods on the standard CUB, SUN, AWA2, and FLO datasets. The significant performance gains indicate that the evolutionary neural architecture search explores a virgin field in ZSL.
</details></li>
</ul>
<hr>
<h2 id="Never-Explore-Repeatedly-in-Multi-Agent-Reinforcement-Learning"><a href="#Never-Explore-Repeatedly-in-Multi-Agent-Reinforcement-Learning" class="headerlink" title="Never Explore Repeatedly in Multi-Agent Reinforcement Learning"></a>Never Explore Repeatedly in Multi-Agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09909">http://arxiv.org/abs/2308.09909</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenghao Li, Tonghan Wang, Chongjie Zhang, Qianchuan Zhao</li>
<li>for: 增强多智能体强化学习中的探索性能</li>
<li>methods: 提出了动态奖励缩放方法，用于稳定前期探索区域的奖励变化，促进更广泛的探索</li>
<li>results: 实验结果表明，该方法能够在Google研究足球和StarCraft II微管理任务中提高性能，特别在罕见奖励设定下<details>
<summary>Abstract</summary>
In the realm of multi-agent reinforcement learning, intrinsic motivations have emerged as a pivotal tool for exploration. While the computation of many intrinsic rewards relies on estimating variational posteriors using neural network approximators, a notable challenge has surfaced due to the limited expressive capability of these neural statistics approximators. We pinpoint this challenge as the "revisitation" issue, where agents recurrently explore confined areas of the task space. To combat this, we propose a dynamic reward scaling approach. This method is crafted to stabilize the significant fluctuations in intrinsic rewards in previously explored areas and promote broader exploration, effectively curbing the revisitation phenomenon. Our experimental findings underscore the efficacy of our approach, showcasing enhanced performance in demanding environments like Google Research Football and StarCraft II micromanagement tasks, especially in sparse reward settings.
</details>
<details>
<summary>摘要</summary>
在多代理激励学习领域，内在动机已经出现为探索的重要工具。而计算许多内在奖励的计算则依赖于使用神经网络近似器来估算变量 posterior。然而，由于神经统计近似器的表达能力有限，这导致了一种“再次访问”问题，Agent会重复探索任务空间中的封闭区域。为解决这个问题，我们提议一种动态奖励缩放方法。这种方法可以稳定在已经探索过的区域中的内在奖励的大幅波动，并促进更广泛的探索，从而控制“再次访问”现象。我们的实验发现，我们的方法在Google研究足球和星际争霸II微管理任务中表现出色，特别在罕见奖励设定下。
</details></li>
</ul>
<hr>
<h2 id="Imputing-Brain-Measurements-Across-Data-Sets-via-Graph-Neural-Networks"><a href="#Imputing-Brain-Measurements-Across-Data-Sets-via-Graph-Neural-Networks" class="headerlink" title="Imputing Brain Measurements Across Data Sets via Graph Neural Networks"></a>Imputing Brain Measurements Across Data Sets via Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09907">http://arxiv.org/abs/2308.09907</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yixin Wang, Wei Peng, Susan F. Tapert, Qingyu Zhao, Kilian M. Pohl</li>
<li>for: 这个论文的目的是为了填充公共可用的数据集中缺失的 brain Region of Interest (ROI) 测量值。</li>
<li>methods: 这个论文使用了深度学习的方法来预测缺失的测量值，包括使用图 neural network (GNN) 模型来模拟 ROI 测量值之间的依赖关系，并考虑到不同性别的脑测量值差异。</li>
<li>results: 该论文的结果表明，使用 DAGI 算法可以准确地预测缺失的 Freesurfer 测量值，并且可以考虑到不同性别的脑测量值差异。<details>
<summary>Abstract</summary>
Publicly available data sets of structural MRIs might not contain specific measurements of brain Regions of Interests (ROIs) that are important for training machine learning models. For example, the curvature scores computed by Freesurfer are not released by the Adolescent Brain Cognitive Development (ABCD) Study. One can address this issue by simply reapplying Freesurfer to the data set. However, this approach is generally computationally and labor intensive (e.g., requiring quality control). An alternative is to impute the missing measurements via a deep learning approach. However, the state-of-the-art is designed to estimate randomly missing values rather than entire measurements. We therefore propose to re-frame the imputation problem as a prediction task on another (public) data set that contains the missing measurements and shares some ROI measurements with the data sets of interest. A deep learning model is then trained to predict the missing measurements from the shared ones and afterwards is applied to the other data sets. Our proposed algorithm models the dependencies between ROI measurements via a graph neural network (GNN) and accounts for demographic differences in brain measurements (e.g. sex) by feeding the graph encoding into a parallel architecture. The architecture simultaneously optimizes a graph decoder to impute values and a classifier in predicting demographic factors. We test the approach, called Demographic Aware Graph-based Imputation (DAGI), on imputing those missing Freesurfer measurements of ABCD (N=3760) by training the predictor on those publicly released by the National Consortium on Alcohol and Neurodevelopment in Adolescence (NCANDA, N=540)...
</details>
<details>
<summary>摘要</summary>
公共可用数据集的结构MRI数据可能不包含特定的脑区域关注点（ROIs）的准确测量。例如，ABCDFreesurfer的曲线分数不会由ABCDFreesurfer发布。可以解决这个问题通过简单地重新应用Freessurfer来处理数据集。然而，这种方法通常是计算机和人工劳动（例如质量控制）的。另一种方法是使用深度学习方法进行填充。然而，现状的深度学习方法是随机缺失值的估计而不是整个测量。我们因此提议将填充问题重新定义为一个预测任务，使用另一个（公共）数据集来计算缺失测量，该数据集与数据集之间存在ROI测量的相似性。然后，我们使用深度学习模型来预测缺失测量，并将其应用于其他数据集。我们提出的算法模拟了ROI测量之间的依赖关系，使用图神经网络（GNN）来模型这些关系，同时考虑了脑测量中的人口差异（如性别），通过将图编码 feed 到平行架构中来进行考虑。该架构同时优化了图解码器来填充值，以及一个分类器来预测人口因素。我们测试了我们的方法，称为人口意识图像基于填充（DAGI），在ABCDFreesurfer中缺失的测量（N=3760）中进行填充，通过在NCANDA（N=540）中公共发布的数据集进行训练。
</details></li>
</ul>
<hr>
<h2 id="DPMAC-Differentially-Private-Communication-for-Cooperative-Multi-Agent-Reinforcement-Learning"><a href="#DPMAC-Differentially-Private-Communication-for-Cooperative-Multi-Agent-Reinforcement-Learning" class="headerlink" title="DPMAC: Differentially Private Communication for Cooperative Multi-Agent Reinforcement Learning"></a>DPMAC: Differentially Private Communication for Cooperative Multi-Agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09902">http://arxiv.org/abs/2308.09902</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/CANVOLCANO/DPMAC">https://github.com/CANVOLCANO/DPMAC</a></li>
<li>paper_authors: Canzhe Zhao, Yanjie Ze, Jing Dong, Baoxiang Wang, Shuai Li</li>
<li>for: 保护多体智能学习（MARL）中每个代理的敏感信息，以确保人工智能（AI）系统的隐私。</li>
<li>methods: 提议了一种基于（ε，δ）敏感数据隐私（DP）的差分性多体通信算法（DPMAC），每个代理都有一个本地消息发送器，并自动调整学习的消息分布，以缓解DP噪声所引起的不稳定性。</li>
<li>results: 证明了在保护隐私的情况下，协作MARL存在纳什平衡，这表示这个问题是游戏理论上可学习的。实验证明DPMAC在隐私保护场景下表现明显优于基eline方法。<details>
<summary>Abstract</summary>
Communication lays the foundation for cooperation in human society and in multi-agent reinforcement learning (MARL). Humans also desire to maintain their privacy when communicating with others, yet such privacy concern has not been considered in existing works in MARL. To this end, we propose the \textit{differentially private multi-agent communication} (DPMAC) algorithm, which protects the sensitive information of individual agents by equipping each agent with a local message sender with rigorous $(\epsilon, \delta)$-differential privacy (DP) guarantee. In contrast to directly perturbing the messages with predefined DP noise as commonly done in privacy-preserving scenarios, we adopt a stochastic message sender for each agent respectively and incorporate the DP requirement into the sender, which automatically adjusts the learned message distribution to alleviate the instability caused by DP noise. Further, we prove the existence of a Nash equilibrium in cooperative MARL with privacy-preserving communication, which suggests that this problem is game-theoretically learnable. Extensive experiments demonstrate a clear advantage of DPMAC over baseline methods in privacy-preserving scenarios.
</details>
<details>
<summary>摘要</summary>
通信layfoundationforthecooperationinhuman社会和多代理权威学习（MARL）。人们也渴望保持与他人通信时的隐私，但这一问题在现有的MARL工作中未得到考虑。为此，我们提出了《差分隐私多代理通信算法》（DPMAC），该算法保护每个代理的敏感信息，并在每个代理机器人上实现了严格（ε、δ）差分隐私（DP）保证。与直接在消息上添加固定DP噪声的常见方法不同，我们采用了每个代理机器人的本地消息发送器，并将DP要求直接 интеグinto sender，这 автоматичеamente调整了学习的消息分布，以解决由DP噪声所引起的不稳定性。此外，我们证明了在保持隐私的情况下，多代理MARL проблеma是可学习的游戏理论问题。广泛的实验表明，DPMAC在隐私保护场景下具有明显的优势。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Learning-based-Imputation-Prediction-Networks-for-In-hospital-Mortality-Risk-Modeling-using-EHRs"><a href="#Contrastive-Learning-based-Imputation-Prediction-Networks-for-In-hospital-Mortality-Risk-Modeling-using-EHRs" class="headerlink" title="Contrastive Learning-based Imputation-Prediction Networks for In-hospital Mortality Risk Modeling using EHRs"></a>Contrastive Learning-based Imputation-Prediction Networks for In-hospital Mortality Risk Modeling using EHRs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09896">http://arxiv.org/abs/2308.09896</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liulab1356/CL-ImpPreNet">https://github.com/liulab1356/CL-ImpPreNet</a></li>
<li>paper_authors: Yuxi Liu, Zhenhao Zhang, Shaowen Qin, Flora D. Salim, Antonio Jimeno Yepes</li>
<li>for: 预测医院内死亡风险基于电子医疗记录 (EHRs) 已经受到了广泛关注，以提供早期警示患者的健康状况，以便医疗专业人员能够在时间上采取措施。</li>
<li>methods: 我们的方法包括使用图分析模型来划分病人，以便只使用相似病人的信息进行缺失值填充。此外，我们还将对比学习 integrate 到我们的网络架构中，以提高病人表示学习和预测性能。</li>
<li>results: 我们的方法在两个真实的 EHR 数据集上进行实验，与当前状态的方法相比，在缺失值填充和预测任务中均有较高的性能。<details>
<summary>Abstract</summary>
Predicting the risk of in-hospital mortality from electronic health records (EHRs) has received considerable attention. Such predictions will provide early warning of a patient's health condition to healthcare professionals so that timely interventions can be taken. This prediction task is challenging since EHR data are intrinsically irregular, with not only many missing values but also varying time intervals between medical records. Existing approaches focus on exploiting the variable correlations in patient medical records to impute missing values and establishing time-decay mechanisms to deal with such irregularity. This paper presents a novel contrastive learning-based imputation-prediction network for predicting in-hospital mortality risks using EHR data. Our approach introduces graph analysis-based patient stratification modeling in the imputation process to group similar patients. This allows information of similar patients only to be used, in addition to personal contextual information, for missing value imputation. Moreover, our approach can integrate contrastive learning into the proposed network architecture to enhance patient representation learning and predictive performance on the classification task. Experiments on two real-world EHR datasets show that our approach outperforms the state-of-the-art approaches in both imputation and prediction tasks.
</details>
<details>
<summary>摘要</summary>
预测医院内死亡风险从电子医疗记录（EHR）获得了广泛关注。这种预测可以提供早期诊断病人健康状况的警示，以便医疗专业人员在时间上采取措施。这个预测任务是挑战性的，因为EHR数据本身是不规则的，有许多缺失的值和不同的时间间隔between medical records。现有的方法是利用病人医疗记录中的变量相关性来填充缺失值，并设置时间衰退机制来处理这种不规则性。本文提出了一种新的对比学习基于抽象的插值预测网络，用于预测医院内死亡风险。我们的方法包括基于图分析的病人划分模型，以组合相似病人的信息。这使得只有相似病人的信息，以及个人上下文信息，用于缺失值填充。此外，我们的方法还可以将对比学习integrated到提议的网络架构中，以提高病人表征学习和预测性能。实验结果表明，我们的方法在两个实际的EHR数据集上比状态机制方法更高。
</details></li>
</ul>
<hr>
<h2 id="Knowledge-Transfer-from-High-Resource-to-Low-Resource-Programming-Languages-for-Code-LLMs"><a href="#Knowledge-Transfer-from-High-Resource-to-Low-Resource-Programming-Languages-for-Code-LLMs" class="headerlink" title="Knowledge Transfer from High-Resource to Low-Resource Programming Languages for Code LLMs"></a>Knowledge Transfer from High-Resource to Low-Resource Programming Languages for Code LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09895">http://arxiv.org/abs/2308.09895</a></li>
<li>repo_url: None</li>
<li>paper_authors: Federico Cassano, John Gouwar, Francesca Lucchetti, Claire Schlesinger, Carolyn Jane Anderson, Michael Greenberg, Abhinav Jangda, Arjun Guha</li>
<li>for: 本文旨在提高Code LLMP的表现在低资源语言上，使其能够更好地支持低资源语言的编程。</li>
<li>methods: 本文提出了一种效果的方法，即使用半人工生成的数据来提高Code LLMP的表现。这种方法可以将高资源语言的训练数据翻译成低资源语言的训练数据，以便使用任何预训练的Code LLMP进行精度。</li>
<li>results: 本文使用MultiPL-T生成了大量的新训练数据，并对这些数据进行了验证。 results表明，通过使用MultiPL-T生成的数据，可以在Racket、OCaml和Lua等低资源语言上达到类似于高资源语言的性能。<details>
<summary>Abstract</summary>
Over the past few years, Large Language Models of Code (Code LLMs) have started to have a significant impact on programming practice. Code LLMs are also emerging as a building block for research in programming languages and software engineering. However, the quality of code produced by a Code LLM varies significantly by programming languages. Code LLMs produce impressive results on programming languages that are well represented in their training data (e.g., Java, Python, or JavaScript), but struggle with low-resource languages, like OCaml and Racket.   This paper presents an effective approach for boosting the performance of Code LLMs on low-resource languages using semi-synthetic data. Our approach generates high-quality datasets for low-resource languages, which can then be used to fine-tune any pretrained Code LLM. Our approach, called MultiPL-T, translates training data from high-resource languages into training data for low-resource languages. We apply our approach to generate tens of thousands of new, validated training items for Racket, OCaml, and Lua from Python. Moreover, we use an open dataset (The Stack) and model (StarCoderBase), which allow us to decontaminate benchmarks and train models on this data without violating the model license.   With MultiPL-T generated data, we present fine-tuned versions of StarCoderBase that achieve state-of-the-art performance for Racket, OCaml, and Lua on benchmark problems. For Lua, our fine-tuned model achieves the same performance as StarCoderBase as Python -- a very high-resource language -- on the MultiPL-E benchmarks. For Racket and OCaml, we double their performance on MultiPL-E, bringing their performance close to higher-resource languages such as Ruby and C#.
</details>
<details>
<summary>摘要</summary>
在过去几年，大型代码语言模型（Code LLM）已经开始对程序设计产生重要的影响。 Code LLM 也在程序语言和软件工程研究中出现为建筑块。然而，由 Code LLM 生成的代码质量受到程序语言的影响，高Resource语言（如 Java、Python 或 JavaScript）的代码生成印象良好，而低Resource语言（如 OCaml 和 Racket）的代码生成却受到限制。本文提出一种有效的方法，使 Code LLM 在低Resource语言上表现更好。我们的方法通过生成高质量的低Resource语言数据集来提高 Code LLM 的表现。我们的方法被称为 MultiPL-T，它将高Resource语言的训练数据翻译成低Resource语言的训练数据。我们使用 Python 等高Resource语言生成了数以千计的新的有效训练项目，并使用开放数据集（The Stack）和模型（StarCoderBase），以便在这些数据上训练模型，而不违反模型的许可证。使用 MultiPL-T 生成的数据，我们提出了一些精心调整的 StarCoderBase 模型，以实现 Racket、OCaml 和 Lua 在 benchmark 问题上的状态的表现。对 Lua，我们的调整模型与 Python 在 MultiPL-E  benchmark 上达到了同等水平的表现。对 Racket 和 OCaml，我们将其表现提高至 Ruby 和 C# 等高Resource语言水平的两倍。
</details></li>
</ul>
<hr>
<h2 id="Utilizing-Semantic-Textual-Similarity-for-Clinical-Survey-Data-Feature-Selection"><a href="#Utilizing-Semantic-Textual-Similarity-for-Clinical-Survey-Data-Feature-Selection" class="headerlink" title="Utilizing Semantic Textual Similarity for Clinical Survey Data Feature Selection"></a>Utilizing Semantic Textual Similarity for Clinical Survey Data Feature Selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09892">http://arxiv.org/abs/2308.09892</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bcwarner/sts-select">https://github.com/bcwarner/sts-select</a></li>
<li>paper_authors: Benjamin C. Warner, Ziqi Xu, Simon Haroutounian, Thomas Kannampallil, Chenyang Lu</li>
<li>for: 这篇论文的目的是提出一种基于文本名称的Feature选择方法，以提高预测结果的普遍性。</li>
<li>methods: 这篇论文使用语言模型（LM）评估文本名称之间的 semantic textual similarity（STS）分数，以选择最佳的特征集。</li>
<li>results: 研究发现，使用 STS 选择特征可以导致更高的模型性能，比较传统的特征选择算法。<details>
<summary>Abstract</summary>
Survey data can contain a high number of features while having a comparatively low quantity of examples. Machine learning models that attempt to predict outcomes from survey data under these conditions can overfit and result in poor generalizability. One remedy to this issue is feature selection, which attempts to select an optimal subset of features to learn upon. A relatively unexplored source of information in the feature selection process is the usage of textual names of features, which may be semantically indicative of which features are relevant to a target outcome. The relationships between feature names and target names can be evaluated using language models (LMs) to produce semantic textual similarity (STS) scores, which can then be used to select features. We examine the performance using STS to select features directly and in the minimal-redundancy-maximal-relevance (mRMR) algorithm. The performance of STS as a feature selection metric is evaluated against preliminary survey data collected as a part of a clinical study on persistent post-surgical pain (PPSP). The results suggest that features selected with STS can result in higher performance models compared to traditional feature selection algorithms.
</details>
<details>
<summary>摘要</summary>
survey data 可以包含大量特征，但同时只有一小部分例子。机器学习模型在这些条件下预测结果时可能会过拟合，导致泛化性差。一种解决方案是特征选择，它尝试选择最佳的特征子来学习。文本特征名称可能是semantic indicative的特征相关性的一种不常 Investigated sources of information in the feature selection process。我们使用语言模型（LM）评估特征名称和目标名称之间的语义文本相似性（STS）分数，并用这些分数选择特征。我们对直接使用STS作为特征选择度量的性能进行了评估，并与传统特征选择算法进行比较。结果表明，使用STS选择特征可以对比传统特征选择算法获得更高性能的模型。
</details></li>
</ul>
<hr>
<h2 id="Inductive-bias-Learning-Generating-Code-Models-with-Large-Language-Model"><a href="#Inductive-bias-Learning-Generating-Code-Models-with-Large-Language-Model" class="headerlink" title="Inductive-bias Learning: Generating Code Models with Large Language Model"></a>Inductive-bias Learning: Generating Code Models with Large Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09890">http://arxiv.org/abs/2308.09890</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fuyu-quant/iblm">https://github.com/fuyu-quant/iblm</a></li>
<li>paper_authors: Toma Tanaka, Naofumi Emoto, Tsukasa Yumibayashi</li>
<li>For: The paper proposes a novel method called Inductive-Bias Learning (IBL) that combines the techniques of In-Context Learning (ICL) and code generation.* Methods: The paper uses a contextual understanding to generate a code with a necessary structure for inference, leveraging the property of inference without explicit inductive bias inherent in ICL and the readability and explainability of code generation.* Results: The generated Code Models have been found to achieve predictive accuracy comparable to, and in some cases surpassing, ICL and representative machine learning models.<details>
<summary>Abstract</summary>
Large Language Models(LLMs) have been attracting attention due to a ability called in-context learning(ICL). ICL, without updating the parameters of a LLM, it is possible to achieve highly accurate inference based on rules ``in the context'' by merely inputting a training data into the prompt. Although ICL is a developing field with many unanswered questions, LLMs themselves serves as a inference model, seemingly realizing inference without explicitly indicate ``inductive bias''. On the other hand, a code generation is also a highlighted application of LLMs. The accuracy of code generation has dramatically improved, enabling even non-engineers to generate code to perform the desired tasks by crafting appropriate prompts. In this paper, we propose a novel ``learning'' method called an ``Inductive-Bias Learning (IBL)'', which combines the techniques of ICL and code generation. An idea of IBL is straightforward. Like ICL, IBL inputs a training data into the prompt and outputs a code with a necessary structure for inference (we referred to as ``Code Model'') from a ``contextual understanding''. Despite being a seemingly simple approach, IBL encompasses both a ``property of inference without explicit inductive bias'' inherent in ICL and a ``readability and explainability'' of the code generation. Surprisingly, generated Code Models have been found to achieve predictive accuracy comparable to, and in some cases surpassing, ICL and representative machine learning models. Our IBL code is open source: https://github.com/fuyu-quant/IBLM
</details>
<details>
<summary>摘要</summary>
大型语言模型(LLMs) 在 latest 时期引起了关注，主要是因为它具有一种能力 называ为 "在上下文中学习" (ICL)。 ICL 可以在不更新 LLM 参数的情况下，通过输入训练数据来实现高度准确的推理，只需要在提问中输入训练数据。虽然 ICL 是一个还未解决的问题，但 LLMS 本身就是一种推理模型，似乎不需要显式地指定 "推理偏好"。此外，代码生成也是 LLMS 的突出应用。代码生成的准确率已经提高到了非常高的水平，使得even non-engineers可以通过制定合适的提问来生成代码以执行所需的任务。在这篇论文中，我们提出了一种新的 "学习" 方法，称为 "推理偏好学习" (IBL)。 IBL 结合了 ICL 和代码生成的技术。IBL 的想法是 straightforward。与 ICL 类似，IBL 通过输入训练数据来提问，并从上下文理解中生成一个代码模型（我们称之为 "代码模型"），以实现推理。尽管看起来很简单，但 IBL 包含了 ICL 中 "推理无需显式偏好" 的性质和代码生成 "可读性和解释性"。 surprisingly，生成的代码模型已经被发现可以达到与 ICL 和代表性机器学习模型相同或更高的预测精度。我们的 IBL 代码开源在 GitHub：https://github.com/fuyu-quant/IBLM
</details></li>
</ul>
<hr>
<h2 id="DUAW-Data-free-Universal-Adversarial-Watermark-against-Stable-Diffusion-Customization"><a href="#DUAW-Data-free-Universal-Adversarial-Watermark-against-Stable-Diffusion-Customization" class="headerlink" title="DUAW: Data-free Universal Adversarial Watermark against Stable Diffusion Customization"></a>DUAW: Data-free Universal Adversarial Watermark against Stable Diffusion Customization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09889">http://arxiv.org/abs/2308.09889</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoyu Ye, Hao Huang, Jiaqi An, Yongtao Wang</li>
<li>For: The paper aims to protect a myriad of copyrighted images from different customization approaches across various versions of SD models.* Methods: The proposed approach, called invisible data-free universal adversarial watermark (DUAW), is designed to disrupt the variational autoencoder during SD customization. It operates in a data-free context, where it is trained on synthetic images produced by a Large Language Model (LLM) and a pretrained SD model.* Results: Experimental results demonstrate that DUAW can effectively distort the outputs of fine-tuned SD models, rendering them discernible to both human observers and a simple classifier.Here’s the same information in Simplified Chinese text:* For: 该研究旨在保护不同版本的SD模型自定义的数字艺术作品，以防止抄袭和侵犯版权。* Methods: 提议的方法是透明无数据自由对抗水印（DUAW），通过在自适应变换器中打乱VAR的工作方式来保护权利图像。该方法在无需直接处理版权图像的情况下进行训练，通过大自然语言模型（LLM）和预训练SD模型生成的 sintetic图像来实现。* Results: 实验结果表明，DUAW可以有效地打乱定制后的SD模型输出，使其对人类观察员和简单的分类器都可见。<details>
<summary>Abstract</summary>
Stable Diffusion (SD) customization approaches enable users to personalize SD model outputs, greatly enhancing the flexibility and diversity of AI art. However, they also allow individuals to plagiarize specific styles or subjects from copyrighted images, which raises significant concerns about potential copyright infringement. To address this issue, we propose an invisible data-free universal adversarial watermark (DUAW), aiming to protect a myriad of copyrighted images from different customization approaches across various versions of SD models. First, DUAW is designed to disrupt the variational autoencoder during SD customization. Second, DUAW operates in a data-free context, where it is trained on synthetic images produced by a Large Language Model (LLM) and a pretrained SD model. This approach circumvents the necessity of directly handling copyrighted images, thereby preserving their confidentiality. Once crafted, DUAW can be imperceptibly integrated into massive copyrighted images, serving as a protective measure by inducing significant distortions in the images generated by customized SD models. Experimental results demonstrate that DUAW can effectively distort the outputs of fine-tuned SD models, rendering them discernible to both human observers and a simple classifier.
</details>
<details>
<summary>摘要</summary>
stable diffusion（SD）自定义方法可以让用户个性化SD模型输出，大大提高AI艺术的灵活性和多样性。然而，这些自定义方法也使得个人可以复制特定风格或主题的版权图像，这引发了对可能的版权侵犯的重大担忧。为解决这个问题，我们提出了隐形数据自由 universial adversarial watermark（DUAW），以保护不同版本的SD模型在不同自定义方法下生成的多种版权图像。首先，DUAW是在SD自定义过程中打乱变量自动encoder的。其次，DUAW在无数据上下文中进行训练，使用一个大型自然语言模型（LLM）和一个预训练的SD模型生成的 sintetic图像。这种方法可以避免直接处理版权图像，从而保持其 конфиденциальность。一旦创制完成，DUAW可以隐藏地将入prise到大量版权图像中，作为一种保护措施，使得生成的图像被自定义SD模型输出的 Distortion 可见 both to human observers and a simple classifier。实验结果表明，DUAW可以有效地对精度调整后的SD模型输出进行 Distortion，使其可见 both to human observers and a simple classifier。
</details></li>
</ul>
<hr>
<h2 id="On-Estimating-the-Gradient-of-the-Expected-Information-Gain-in-Bayesian-Experimental-Design"><a href="#On-Estimating-the-Gradient-of-the-Expected-Information-Gain-in-Bayesian-Experimental-Design" class="headerlink" title="On Estimating the Gradient of the Expected Information Gain in Bayesian Experimental Design"></a>On Estimating the Gradient of the Expected Information Gain in Bayesian Experimental Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09888">http://arxiv.org/abs/2308.09888</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ziq-ao/GradEIG">https://github.com/ziq-ao/GradEIG</a></li>
<li>paper_authors: Ziqiao Ao, Jinglai Li</li>
<li>for: 本研究的目的是为bayesian inference中的实验设计优化预期信息增强(EIG)的优化问题提供方法。</li>
<li>methods: 本研究提出了两种方法来估计EIG的梯度，分别是UEEG-MCMC和BEEG-AP。UEEG-MCMC通过MCMC生成 posterior samples来估计EIG梯度，而BEEG-AP则是通过重复使用参数样本来实现高效的 simulations。</li>
<li>results: 理论分析和数值实验表明，UEEG-MCMC是具有robust性的，而BEEG-AP在EIG值小时具有更高的效率。此外，两种方法在我们的数值实验中表现了更好的性能，比如popular benchmarks。<details>
<summary>Abstract</summary>
Bayesian Experimental Design (BED), which aims to find the optimal experimental conditions for Bayesian inference, is usually posed as to optimize the expected information gain (EIG). The gradient information is often needed for efficient EIG optimization, and as a result the ability to estimate the gradient of EIG is essential for BED problems. The primary goal of this work is to develop methods for estimating the gradient of EIG, which, combined with the stochastic gradient descent algorithms, result in efficient optimization of EIG. Specifically, we first introduce a posterior expected representation of the EIG gradient with respect to the design variables. Based on this, we propose two methods for estimating the EIG gradient, UEEG-MCMC that leverages posterior samples generated through Markov Chain Monte Carlo (MCMC) to estimate the EIG gradient, and BEEG-AP that focuses on achieving high simulation efficiency by repeatedly using parameter samples. Theoretical analysis and numerical studies illustrate that UEEG-MCMC is robust agains the actual EIG value, while BEEG-AP is more efficient when the EIG value to be optimized is small. Moreover, both methods show superior performance compared to several popular benchmarks in our numerical experiments.
</details>
<details>
<summary>摘要</summary>
bayesian 实验设计（BED），旨在找到bayesian 推理中的最佳实验条件，通常是要最大化预期信息增加（EIG）的。在这种情况下，梯度信息是非常重要的，因此能够估算EIG梯度的能力是BED问题的关键。本工作的主要目标是开发一些估算EIG梯度的方法，这些方法可以与梯度下降法相结合，从而实现EIG的有效优化。首先，我们引入了 posterior 预期表示EIG梯度的关系，并对这个表示进行了分析。然后，我们提出了两种估算EIG梯度的方法：UEEG-MCMC，利用MCMC生成的 posterior 样本来估算EIG梯度，和BEEG-AP，强调在实验中实现高效率，通过重复使用参数样本来实现。我们的理论分析和数值研究表明，UEEG-MCMC 对实际的EIG值具有较高的稳定性，而BEEG-AP 在EIG值小于一定程度时具有更高的效率。此外，两种方法在我们的数值实验中都表现出了较好的性能，比如几种流行的参考方法。
</details></li>
</ul>
<hr>
<h2 id="Calibrating-Uncertainty-for-Semi-Supervised-Crowd-Counting"><a href="#Calibrating-Uncertainty-for-Semi-Supervised-Crowd-Counting" class="headerlink" title="Calibrating Uncertainty for Semi-Supervised Crowd Counting"></a>Calibrating Uncertainty for Semi-Supervised Crowd Counting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09887">http://arxiv.org/abs/2308.09887</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Li, Xiaoling Hu, Shahira Abousamra, Chao Chen</li>
<li>for: 这篇论文的目的是提出一种新的半指导人数推断方法，以提高半指导人数推断 task 的性能。</li>
<li>methods: 本篇论文使用了一种基于 uncertainty 的 iterative pseudo-labeling 方法，通过一个 surrogate function 来训练模型，以控制模型的 uncertainty。</li>
<li>results: 本篇论文的结果显示，使用 proposed 方法可以生成可靠的 uncertainty estimation，高品质的 pseudo-labels，并 achieved state-of-the-art performance in semisupervised crowd counting task。<details>
<summary>Abstract</summary>
Semi-supervised crowd counting is an important yet challenging task. A popular approach is to iteratively generate pseudo-labels for unlabeled data and add them to the training set. The key is to use uncertainty to select reliable pseudo-labels. In this paper, we propose a novel method to calibrate model uncertainty for crowd counting. Our method takes a supervised uncertainty estimation strategy to train the model through a surrogate function. This ensures the uncertainty is well controlled throughout the training. We propose a matching-based patch-wise surrogate function to better approximate uncertainty for crowd counting tasks. The proposed method pays a sufficient amount of attention to details, while maintaining a proper granularity. Altogether our method is able to generate reliable uncertainty estimation, high quality pseudolabels, and achieve state-of-the-art performance in semisupervised crowd counting.
</details>
<details>
<summary>摘要</summary>
半指导的人群计数是一项重要但又具有挑战性的任务。一种受欢迎的方法是通过逐步生成 pseudo-标签 для无标签数据并将其添加到训练集中。关键在于使用uncertainty来选择可靠的 pseudo-标签。在这篇论文中，我们提出了一种新的方法来准确控制模型的uncertainty。我们使用一种监督型 uncertainty estimation 策略来训练模型，并使用一个 matching-based patch-wise 替换函数来更好地估计uncertainty。我们的方法具有着充分的注意力于细节，同时保持合理的粒度。总的来说，我们的方法可以生成可靠的 uncertainty estimation，高质量的 pseudo-标签，并实现semisupervised人群计数中的顶峰性能。
</details></li>
</ul>
<hr>
<h2 id="A-Transformer-based-Framework-For-Multi-variate-Time-Series-A-Remaining-Useful-Life-Prediction-Use-Case"><a href="#A-Transformer-based-Framework-For-Multi-variate-Time-Series-A-Remaining-Useful-Life-Prediction-Use-Case" class="headerlink" title="A Transformer-based Framework For Multi-variate Time Series: A Remaining Useful Life Prediction Use Case"></a>A Transformer-based Framework For Multi-variate Time Series: A Remaining Useful Life Prediction Use Case</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09884">http://arxiv.org/abs/2308.09884</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oluwaseyi Ogunfowora, Homayoun Najjaran</li>
<li>for: 这个研究旨在提出一个基于encoder-transformer架构的多变量时间序列预测框架，用于预测机器的剩下有用生命时间（RUL）。</li>
<li>methods: 本研究使用了transformer模型，并进行了三个模型特有的实验，以将transformer模型从自然语言领域转移到时间序列领域。此外，本研究还提出了一个新的扩展窗口方法，以帮助模型识别机器的初期阶段和衰退路径。</li>
<li>results: 根据所有C-MAPPSbenchmark dataset上的四个集，这个提案的encoder-transformer模型的预测性能大幅提高，与13个现有的州际之最（SOTA）模型相比，平均提高137.65%。<details>
<summary>Abstract</summary>
In recent times, Large Language Models (LLMs) have captured a global spotlight and revolutionized the field of Natural Language Processing. One of the factors attributed to the effectiveness of LLMs is the model architecture used for training, transformers. Transformer models excel at capturing contextual features in sequential data since time series data are sequential, transformer models can be leveraged for more efficient time series data prediction. The field of prognostics is vital to system health management and proper maintenance planning. A reliable estimation of the remaining useful life (RUL) of machines holds the potential for substantial cost savings. This includes avoiding abrupt machine failures, maximizing equipment usage, and serving as a decision support system (DSS). This work proposed an encoder-transformer architecture-based framework for multivariate time series prediction for a prognostics use case. We validated the effectiveness of the proposed framework on all four sets of the C-MAPPS benchmark dataset for the remaining useful life prediction task. To effectively transfer the knowledge and application of transformers from the natural language domain to time series, three model-specific experiments were conducted. Also, to enable the model awareness of the initial stages of the machine life and its degradation path, a novel expanding window method was proposed for the first time in this work, it was compared with the sliding window method, and it led to a large improvement in the performance of the encoder transformer model. Finally, the performance of the proposed encoder-transformer model was evaluated on the test dataset and compared with the results from 13 other state-of-the-art (SOTA) models in the literature and it outperformed them all with an average performance increase of 137.65% over the next best model across all the datasets.
</details>
<details>
<summary>摘要</summary>
To address this challenge, this work proposes an encoder-transformer architecture-based framework for multivariate time series prediction in a prognostics use case. The proposed framework was validated on four datasets from the C-MAPPS benchmark, and three model-specific experiments were conducted to transfer knowledge from the natural language domain to time series. Additionally, a novel expanding window method was proposed to improve the model's awareness of the initial stages of machine life and its degradation path.The proposed encoder-transformer model outperformed 13 other state-of-the-art (SOTA) models in the literature with an average performance increase of 137.65% over the next best model across all datasets. This demonstrates the effectiveness of the proposed framework and the potential of transformer models for time series prediction in prognostics.
</details></li>
</ul>
<hr>
<h2 id="Flamingo-Multi-Round-Single-Server-Secure-Aggregation-with-Applications-to-Private-Federated-Learning"><a href="#Flamingo-Multi-Round-Single-Server-Secure-Aggregation-with-Applications-to-Private-Federated-Learning" class="headerlink" title="Flamingo: Multi-Round Single-Server Secure Aggregation with Applications to Private Federated Learning"></a>Flamingo: Multi-Round Single-Server Secure Aggregation with Applications to Private Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09883">http://arxiv.org/abs/2308.09883</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/eniac/flamingo">https://github.com/eniac/flamingo</a></li>
<li>paper_authors: Yiping Ma, Jess Woods, Sebastian Angel, Antigoni Polychroniadou, Tal Rabin</li>
<li>for: 这篇论文介绍了一种用于安全聚合数据的系统，以便在大量客户端上进行训练。</li>
<li>methods: 该系统使用了一种新的轻量级Dropout鲁棒性协议，以确保如果客户端在聚合过程中离开，服务器仍然可以获得有意义的结果。此外，它还引入了一种新的客户端 neighboorhood选择方法。</li>
<li>results: 作者们实现并评估了Flamingo系统，并证明了它可以安全地训练基于MNIST和CIFAR-100数据集的神经网络模型，并且模型的学习结果与非私有 Federated Learning 系统相同。<details>
<summary>Abstract</summary>
This paper introduces Flamingo, a system for secure aggregation of data across a large set of clients. In secure aggregation, a server sums up the private inputs of clients and obtains the result without learning anything about the individual inputs beyond what is implied by the final sum. Flamingo focuses on the multi-round setting found in federated learning in which many consecutive summations (averages) of model weights are performed to derive a good model. Previous protocols, such as Bell et al. (CCS '20), have been designed for a single round and are adapted to the federated learning setting by repeating the protocol multiple times. Flamingo eliminates the need for the per-round setup of previous protocols, and has a new lightweight dropout resilience protocol to ensure that if clients leave in the middle of a sum the server can still obtain a meaningful result. Furthermore, Flamingo introduces a new way to locally choose the so-called client neighborhood introduced by Bell et al. These techniques help Flamingo reduce the number of interactions between clients and the server, resulting in a significant reduction in the end-to-end runtime for a full training session over prior work. We implement and evaluate Flamingo and show that it can securely train a neural network on the (Extended) MNIST and CIFAR-100 datasets, and the model converges without a loss in accuracy, compared to a non-private federated learning system.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:这篇论文介绍了Flamingo系统，用于在多个客户端上安全地汇集数据。在安全汇集中，服务器将客户端的私有输入汇集起来，而不会学习到每个输入的细节，只知道最终结果的含义。Flamingo专注于联合学习中的多轮设定，在多个汇集（平均）后 derivate 出一个好的模型。先前的协议，如Bell et al. (CCS '20)，已经为单轮设定而设计，并在联合学习设定中重复协议多次。Flamingo消除了先前协议的每轮设定需求，并 introduce 了一种轻量级的dropout鲁棒性协议，以保证如果客户端在汇集过程中离开，服务器仍然可以获得有意义的结果。此外，Flamingo引入了一种新的客户端选择方法，以及 Bell et al. 所引入的客户端社区。这些技术帮助Flamingo减少客户端和服务器之间的交互数量，从而实现了与先前工作相比的很大减少。我们实现和评估了Flamingo，并证明它可以安全地训练（扩展）MNIST和CIFAR-100数据集上的神经网络模型，模型也可以在私有化联合学习环境中减少准确性损失。
</details></li>
</ul>
<hr>
<h2 id="Generative-Adversarial-Networks-Unlearning"><a href="#Generative-Adversarial-Networks-Unlearning" class="headerlink" title="Generative Adversarial Networks Unlearning"></a>Generative Adversarial Networks Unlearning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09881">http://arxiv.org/abs/2308.09881</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hui Sun, Tianqing Zhu, Wenhan Chang, Wanlei Zhou<br>for: This paper focuses on the issue of unlearning in Generative Adversarial Networks (GANs), specifically addressing the challenges of generator unlearning and defining a criterion for the discriminator.methods: The authors propose a cascaded unlearning approach that utilizes a substitution mechanism and fake label to mitigate the challenges of generator unlearning.results: The proposed approach achieves significantly improved item and class unlearning efficiency, reducing the required time by up to 185x and 284x for the MNIST and CIFAR-10 datasets, respectively, compared to retraining from scratch. Additionally, the model’s performance experiences only minor degradation after unlearning, and has no adverse effects on downstream tasks such as classification.Here is the result in Simplified Chinese text:for: 这篇论文关注在生成 adversarial 网络（GANs）中的快速学习问题，特别是生成器快速学习的挑战和定义验证器的标准。methods: 作者提出了一种叠加快速学习方法，利用替换机制和假标签来解决生成器快速学习的挑战。results: 提议的方法在MNIST和CIFAR-10 datasets上实现了明显的项目和类快速学习效率提高，比重新训练从头开始的时间减少了185倍和284倍。此外，模型之后学习后的性能只受到了微小的降低，与只需要64个图像相比，无法对下游任务如分类造成负面影响。<details>
<summary>Abstract</summary>
As machine learning continues to develop, and data misuse scandals become more prevalent, individuals are becoming increasingly concerned about their personal information and are advocating for the right to remove their data. Machine unlearning has emerged as a solution to erase training data from trained machine learning models. Despite its success in classifiers, research on Generative Adversarial Networks (GANs) is limited due to their unique architecture, including a generator and a discriminator. One challenge pertains to generator unlearning, as the process could potentially disrupt the continuity and completeness of the latent space. This disruption might consequently diminish the model's effectiveness after unlearning. Another challenge is how to define a criterion that the discriminator should perform for the unlearning images. In this paper, we introduce a substitution mechanism and define a fake label to effectively mitigate these challenges. Based on the substitution mechanism and fake label, we propose a cascaded unlearning approach for both item and class unlearning within GAN models, in which the unlearning and learning processes run in a cascaded manner. We conducted a comprehensive evaluation of the cascaded unlearning technique using the MNIST and CIFAR-10 datasets. Experimental results demonstrate that this approach achieves significantly improved item and class unlearning efficiency, reducing the required time by up to 185x and 284x for the MNIST and CIFAR-10 datasets, respectively, in comparison to retraining from scratch. Notably, although the model's performance experiences minor degradation after unlearning, this reduction is negligible when dealing with a minimal number of images (e.g., 64) and has no adverse effects on downstream tasks such as classification.
</details>
<details>
<summary>摘要</summary>
随着机器学习技术的不断发展，个人数据泄露事件的出现也越来越普遍，人们对自己的个人信息越来越关注，并且呼吁保护自己的数据权利。机器学习模型中的数据解启（unlearning）技术已成为一种解决方案，可以将训练数据从已经训练过的机器学习模型中除去。然而，对于生成器（generator）和判别器（discriminator）的特殊架构，研究生成对抗网络（GANs）的unlearning却受到了限制。一个挑战在生成器unlearning中，即可能导致生成器的维度空间中断和不连续，从而影响模型的效果。另一个挑战是如何定义判别器对unlearning图像的标准。在本文中，我们提出了替换机制和假标签，以解决这些挑战。基于替换机制和假标签，我们提议一种叠加式unlearning方法，在GAN模型中进行项和类unlearning。我们在MNIST和CIFAR-10 datasets上进行了广泛的评估，结果表明，这种方法可以大幅提高item和类unlearning效率，比 retraining from scratch 需要的时间减少至多达185倍和284倍。尤其是，模型性能减少后仍然保持可观，只有当处理少量图像（例如64）时，这种减少才会导致轻微的性能下降，无法影响下游任务 such as classification。
</details></li>
</ul>
<hr>
<h2 id="DatasetEquity-Are-All-Samples-Created-Equal-In-The-Quest-For-Equity-Within-Datasets"><a href="#DatasetEquity-Are-All-Samples-Created-Equal-In-The-Quest-For-Equity-Within-Datasets" class="headerlink" title="DatasetEquity: Are All Samples Created Equal? In The Quest For Equity Within Datasets"></a>DatasetEquity: Are All Samples Created Equal? In The Quest For Equity Within Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09878">http://arxiv.org/abs/2308.09878</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/towardsautonomy/datasetequity">https://github.com/towardsautonomy/datasetequity</a></li>
<li>paper_authors: Shubham Shrivastava, Xianling Zhang, Sushruth Nagesh, Armin Parchami</li>
<li>for: Addressing data imbalance in machine learning, particularly in computer vision tasks.</li>
<li>methods: Using deep perceptual embeddings and clustering to compute sample likelihoods, and proposing a novel $\textbf{Generalized Focal Loss}$ function to weigh samples differently during training.</li>
<li>results: Achieving over $200%$ AP gains on under-represented classes (Cyclist) in the KITTI dataset, and demonstrating the method’s effectiveness across autonomous driving vision datasets including nuScenes.Here’s the full summary in Simplified Chinese:</li>
<li>for: 本研究旨在解决机器学习中的数据不均衡问题，特别是计算机视觉任务中的数据不均衡问题。</li>
<li>methods: 使用深度感知嵌入和聚类计算样本可能性，并提出一种新的$\textbf{扩展Focus损失函数}$来在训练中不同样本的权重。</li>
<li>results: 在KITTI数据集中，对于不足 Represented 类（自行车手）的AP得分提高了超过200%，并在自动驾驶视觉数据集（包括nuScenes）中证明了方法的一致性和通用性。<details>
<summary>Abstract</summary>
Data imbalance is a well-known issue in the field of machine learning, attributable to the cost of data collection, the difficulty of labeling, and the geographical distribution of the data. In computer vision, bias in data distribution caused by image appearance remains highly unexplored. Compared to categorical distributions using class labels, image appearance reveals complex relationships between objects beyond what class labels provide. Clustering deep perceptual features extracted from raw pixels gives a richer representation of the data. This paper presents a novel method for addressing data imbalance in machine learning. The method computes sample likelihoods based on image appearance using deep perceptual embeddings and clustering. It then uses these likelihoods to weigh samples differently during training with a proposed $\textbf{Generalized Focal Loss}$ function. This loss can be easily integrated with deep learning algorithms. Experiments validate the method's effectiveness across autonomous driving vision datasets including KITTI and nuScenes. The loss function improves state-of-the-art 3D object detection methods, achieving over $200\%$ AP gains on under-represented classes (Cyclist) in the KITTI dataset. The results demonstrate the method is generalizable, complements existing techniques, and is particularly beneficial for smaller datasets and rare classes. Code is available at: https://github.com/towardsautonomy/DatasetEquity
</details>
<details>
<summary>摘要</summary>
“数据不匹配是机器学习领域的一个公认问题，这可以归因于数据收集的成本、标签的困难以及数据的地域分布。在计算机视觉领域，图像的外观偏见对数据分布的偏见尚未得到充分的探讨。相比于使用类别标签的分布，图像的外观 revelas了对象之间复杂的关系，这些关系超出了类别标签所提供的信息。使用深度感知特征提取自原始像素的归一化可以为数据提供更加富有的表示。本文提出了一种 novel 的数据不匹配解决方法，该方法使用图像外观的深度感知嵌入和归一化计算样本的可能性。然后，使用这些可能性来调整样本的权重，并使用提议的 $\textbf{通用强调损失}$ 函数进行训练。这个损失函数可以轻松地与深度学习算法结合使用。实验证明了该方法在自动驾驶视觉 datasets 中的效果，包括 KITTI 和 nuScenes。该损失函数可以提高 state-of-the-art 3D 物体检测方法的性能，在 KITTI dataset 中Cyclist 类型的下 Represented 类型中获得了更 than 200% AP 提升。结果表明该方法是通用的，可以补充现有的技术，特别是对小型 datasets 和罕见类型的支持。代码可以在：https://github.com/towardsautonomy/DatasetEquity 中找到。”Note that Simplified Chinese is used in the translation, as it is more widely used in mainland China and is the standard language used in most online platforms and publications. If you prefer Traditional Chinese, I can provide that version as well.
</details></li>
</ul>
<hr>
<h2 id="Skill-Transformer-A-Monolithic-Policy-for-Mobile-Manipulation"><a href="#Skill-Transformer-A-Monolithic-Policy-for-Mobile-Manipulation" class="headerlink" title="Skill Transformer: A Monolithic Policy for Mobile Manipulation"></a>Skill Transformer: A Monolithic Policy for Mobile Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09873">http://arxiv.org/abs/2308.09873</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoyu Huang, Dhruv Batra, Akshara Rai, Andrew Szot</li>
<li>for: 解决长期机器人任务， combining conditional sequence modeling 和技能归一化。</li>
<li>methods: 使用 transformer 架构，通过示例轨迹进行端到端训练，预测高级技能和全身低级动作。</li>
<li>results: 在embodied rearrangement benchmark上测试， Skill Transformer 可以实现 robust 任务规划和低级控制，成功率高于基eline 2.5倍。<details>
<summary>Abstract</summary>
We present Skill Transformer, an approach for solving long-horizon robotic tasks by combining conditional sequence modeling and skill modularity. Conditioned on egocentric and proprioceptive observations of a robot, Skill Transformer is trained end-to-end to predict both a high-level skill (e.g., navigation, picking, placing), and a whole-body low-level action (e.g., base and arm motion), using a transformer architecture and demonstration trajectories that solve the full task. It retains the composability and modularity of the overall task through a skill predictor module while reasoning about low-level actions and avoiding hand-off errors, common in modular approaches. We test Skill Transformer on an embodied rearrangement benchmark and find it performs robust task planning and low-level control in new scenarios, achieving a 2.5x higher success rate than baselines in hard rearrangement problems.
</details>
<details>
<summary>摘要</summary>
我们提出了技能变换器，一种解决长期机器人任务的方法，结合条件序列模型和技能分解性。基于机器人 egocentric 和 proprioceptive 观察，技能变换器通过 transformer 架构进行端到端训练，预测高级技能（例如导航、捕捉、放置）和整体低级动作（例如基底和臂动作），使用示例轨迹解决整个任务。它保留了总任务的可组合性和分解性，通过技能预测模块进行低级动作的编制和避免手动错误，常见于模块化方法。我们在embodied重新排序测试上测试了技能变换器，发现它在新的情况下能够做出坚固的任务规划和低级控制，比基eline高2.5倍成功率。
</details></li>
</ul>
<hr>
<h2 id="Tensor-Compressed-Back-Propagation-Free-Training-for-Physics-Informed-Neural-Networks"><a href="#Tensor-Compressed-Back-Propagation-Free-Training-for-Physics-Informed-Neural-Networks" class="headerlink" title="Tensor-Compressed Back-Propagation-Free Training for (Physics-Informed) Neural Networks"></a>Tensor-Compressed Back-Propagation-Free Training for (Physics-Informed) Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09858">http://arxiv.org/abs/2308.09858</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yequan Zhao, Xinling Yu, Zhixiong Chen, Ziyue Liu, Sijia Liu, Zheng Zhang</li>
<li>for:  This paper aims to present a completely backward propagation (BP)-free framework for training realistic neural networks on edge devices, which can greatly improve the design complexity and time-to-market of on-device training accelerators.</li>
<li>methods:  The paper proposes a three-fold technical contribution to achieve BP-free training: (1) a tensor-compressed variance reduction approach to improve the scalability of zeroth-order (ZO) optimization, (2) a hybrid gradient evaluation approach to improve the efficiency of ZO training, and (3) an extension of the BP-free training framework to physics-informed neural networks (PINNs) using a sparse-grid approach to estimate derivatives without BP.</li>
<li>results:  The paper shows that the proposed BP-free training framework only loses little accuracy on the MNIST dataset compared with standard first-order training, and successfully trains a PINN for solving a 20-dim Hamiltonian-Jacobi-Bellman PDE. The memory-efficient and BP-free approach may serve as a foundation for the near-future on-device training on many resource-constraint platforms (e.g., FPGA, ASIC, micro-controllers, and photonic chips).<details>
<summary>Abstract</summary>
Backward propagation (BP) is widely used to compute the gradients in neural network training. However, it is hard to implement BP on edge devices due to the lack of hardware and software resources to support automatic differentiation. This has tremendously increased the design complexity and time-to-market of on-device training accelerators. This paper presents a completely BP-free framework that only requires forward propagation to train realistic neural networks. Our technical contributions are three-fold. Firstly, we present a tensor-compressed variance reduction approach to greatly improve the scalability of zeroth-order (ZO) optimization, making it feasible to handle a network size that is beyond the capability of previous ZO approaches. Secondly, we present a hybrid gradient evaluation approach to improve the efficiency of ZO training. Finally, we extend our BP-free training framework to physics-informed neural networks (PINNs) by proposing a sparse-grid approach to estimate the derivatives in the loss function without using BP. Our BP-free training only loses little accuracy on the MNIST dataset compared with standard first-order training. We also demonstrate successful results in training a PINN for solving a 20-dim Hamiltonian-Jacobi-Bellman PDE. This memory-efficient and BP-free approach may serve as a foundation for the near-future on-device training on many resource-constraint platforms (e.g., FPGA, ASIC, micro-controllers, and photonic chips).
</details>
<details>
<summary>摘要</summary>
<<SYS>>用简化中文表示大多数 neural network 训练使用倒推 propagation (BP) 计算梯度，但在边缘设备上实现 BP 具有硬件和软件资源的限制，导致训练减速器的设计复杂度和时间到市场增加。这篇论文提出了一个完全无需 BP 的框架，只需要前向传播来训练真实的 neural network。我们的技术贡献包括以下三个方面：1. 我们提出了一种紧凑变量 reduction 技术，以提高 zero-order (ZO) 优化的扩展性，使得可以处理的网络大小超出了先前 ZO 方法的能力。2. 我们提出了一种混合式梯度评估方法，以提高 ZO 训练的效率。3. 我们将我们的 BP-free 训练框架应用到物理学 informed neural networks (PINNs) 中，提出了一种稀疏网格方法，以无需 BP 来估算损失函数中的导数。我们的 BP-free 训练只在 MNIST 数据集上减少了一些精度，与标准首次训练相比。我们还成功地训练了一个 PINN 来解决一个 20 维 Hamiltonian-Jacobi-Bellman PDE。这种内存有效并且 BP-free 的方法可能将成为未来资源限制的平台（如 FPGA、ASIC、微控制器和光子Integrated Circuits）上的训练基础。
</details></li>
</ul>
<hr>
<h2 id="Backdoor-Mitigation-by-Correcting-the-Distribution-of-Neural-Activations"><a href="#Backdoor-Mitigation-by-Correcting-the-Distribution-of-Neural-Activations" class="headerlink" title="Backdoor Mitigation by Correcting the Distribution of Neural Activations"></a>Backdoor Mitigation by Correcting the Distribution of Neural Activations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09850">http://arxiv.org/abs/2308.09850</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xi Li, Zhen Xiang, David J. Miller, George Kesidis</li>
<li>for: 本研究旨在揭示和分析对深度神经网络（DNN）的后门（Trojan）攻击的一种重要性，即在攻击成功后，攻击者的后门触发器会导致内层活动的分布变化，并且如果这种变化得到修复，则可以正确地将攻击者的目标类归类。</li>
<li>methods: 本研究使用了反工程化的触发器来修复后门攻击所导致的分布变化，并不需要改变DNN的任何可调参数。</li>
<li>results: 对比 existed 方法，本研究的方法可以更好地 mitigate 后门攻击，并且可以有效地检测测试实例中是否存在触发器。<details>
<summary>Abstract</summary>
Backdoor (Trojan) attacks are an important type of adversarial exploit against deep neural networks (DNNs), wherein a test instance is (mis)classified to the attacker's target class whenever the attacker's backdoor trigger is present. In this paper, we reveal and analyze an important property of backdoor attacks: a successful attack causes an alteration in the distribution of internal layer activations for backdoor-trigger instances, compared to that for clean instances. Even more importantly, we find that instances with the backdoor trigger will be correctly classified to their original source classes if this distribution alteration is corrected. Based on our observations, we propose an efficient and effective method that achieves post-training backdoor mitigation by correcting the distribution alteration using reverse-engineered triggers. Notably, our method does not change any trainable parameters of the DNN, but achieves generally better mitigation performance than existing methods that do require intensive DNN parameter tuning. It also efficiently detects test instances with the trigger, which may help to catch adversarial entities in the act of exploiting the backdoor.
</details>
<details>
<summary>摘要</summary>
<<SYS>>输入文本翻译成简化中文：<</SYS>>深度神经网络（DNN）面临着重要的反对抗攻击，即后门（Trojan）攻击，其中测试实例在攻击者的目标类上被识别为攻击者的后门触发器存在时。在这篇论文中，我们揭示了和分析了后门攻击的一个重要性特征：成功攻击会导致后门触发器实例的内部层活动分布变化，相比于干净实例。更重要的是，我们发现了一个关键的现象：如果这种分布变化得到了修正，那么带有后门触发器的实例将被正确地分类到其原始的类别。基于我们的观察，我们提出了一种高效和有效的后门恢复方法，通过修正分布变化来实现。这种方法不改变了DNN的任何可变参数，但它在恢复性能方面比既有的方法更好，并且可以有效地检测测试实例中的触发器。这可能帮助捕捉了利用后门攻击的恶意实体。
</details></li>
</ul>
<hr>
<h2 id="Enumerating-Safe-Regions-in-Deep-Neural-Networks-with-Provable-Probabilistic-Guarantees"><a href="#Enumerating-Safe-Regions-in-Deep-Neural-Networks-with-Provable-Probabilistic-Guarantees" class="headerlink" title="Enumerating Safe Regions in Deep Neural Networks with Provable Probabilistic Guarantees"></a>Enumerating Safe Regions in Deep Neural Networks with Provable Probabilistic Guarantees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09842">http://arxiv.org/abs/2308.09842</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luca Marzari, Davide Corsi, Enrico Marchesini, Alessandro Farinelli, Ferdinando Cicalese</li>
<li>for:  Ensuring trust in Deep Neural Networks (DNNs) by identifying safe areas.</li>
<li>methods:  Proposed an efficient approximation method called epsilon-ProVe, which exploits statistical prediction of tolerance limits to provide a tight lower estimate of the safe areas.</li>
<li>results:  Scalable and effective method that offers provable probabilistic guarantees, evaluated on standard benchmarks.<details>
<summary>Abstract</summary>
Identifying safe areas is a key point to guarantee trust for systems that are based on Deep Neural Networks (DNNs). To this end, we introduce the AllDNN-Verification problem: given a safety property and a DNN, enumerate the set of all the regions of the property input domain which are safe, i.e., where the property does hold. Due to the #P-hardness of the problem, we propose an efficient approximation method called epsilon-ProVe. Our approach exploits a controllable underestimation of the output reachable sets obtained via statistical prediction of tolerance limits, and can provide a tight (with provable probabilistic guarantees) lower estimate of the safe areas. Our empirical evaluation on different standard benchmarks shows the scalability and effectiveness of our method, offering valuable insights for this new type of verification of DNNs.
</details>
<details>
<summary>摘要</summary>
安全区域的标识是深度神经网络（DNN）系统的关键安全保证点。为此，我们提出了AllDNN-Verification问题：给定一个安全性质和一个DNN，列出安全区域的输入领域中的所有区域，即where the property does hold。由于这个问题的#P-hardness，我们提出了一种高效的近似方法called epsilon-ProVe。我们的方法利用通过统计预测容差范围来控制输出可达集的下界，并可以提供一个紧靠的（具有可证明的概率保证）下界。我们的实验表明我们的方法可以扩展到不同的标准准比，并且有效地验证DNNs。这些结果提供了对这种新类型的验证方法的有价值的理解。
</details></li>
</ul>
<hr>
<h2 id="Microscopy-Image-Segmentation-via-Point-and-Shape-Regularized-Data-Synthesis"><a href="#Microscopy-Image-Segmentation-via-Point-and-Shape-Regularized-Data-Synthesis" class="headerlink" title="Microscopy Image Segmentation via Point and Shape Regularized Data Synthesis"></a>Microscopy Image Segmentation via Point and Shape Regularized Data Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09835">http://arxiv.org/abs/2308.09835</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shijie Li, Mengwei Ren, Thomas Ach, Guido Gerig<br>for:* 这个论文主要针对的是用深度学习方法进行微scopia图像分割，但是现有的方法几乎都需要大量的训练数据，包括完整的对象边界信息，这会非常困难和成本高昂。methods:* 作者提出了一个整体框架，用于将点注释转换为 Synthetically generated training data，包括三个阶段：	1. 使用点注释生成一个 Pseudo dense segmentation mask，并使用 shape priors 来约束其生成;	2. 使用一个未经 paired 训练的图像生成模型，将 Pseudo mask 翻译成一个真实的 microscopy 图像，并使用 object level consistency 来补做;	3. 将 Pseudo masks 和生成的图像组成一个 pairwise 数据集，用于训练适应的分割模型。results:* 作者在公共的 MoNuSeg 数据集上测试了自己的生成框架，并发现其生成的图像比基eline模型更加多样化和真实，同时保持了输入mask和生成图像之间的高协调性;* 当使用同一个分割后果的模型，使用作者的生成数据集进行训练，与使用 pseudo-labels 或基eline生成的图像进行训练相比，模型的性能明显提高;* 作者的框架可以与 dense 标注数据进行比较，并且在 authentic microscopy 图像上达到相同的性能，这表明其可以作为 dense 标注数据的可靠和高效的替代方案。<details>
<summary>Abstract</summary>
Current deep learning-based approaches for the segmentation of microscopy images heavily rely on large amount of training data with dense annotation, which is highly costly and laborious in practice. Compared to full annotation where the complete contour of objects is depicted, point annotations, specifically object centroids, are much easier to acquire and still provide crucial information about the objects for subsequent segmentation. In this paper, we assume access to point annotations only during training and develop a unified pipeline for microscopy image segmentation using synthetically generated training data. Our framework includes three stages: (1) it takes point annotations and samples a pseudo dense segmentation mask constrained with shape priors; (2) with an image generative model trained in an unpaired manner, it translates the mask to a realistic microscopy image regularized by object level consistency; (3) the pseudo masks along with the synthetic images then constitute a pairwise dataset for training an ad-hoc segmentation model. On the public MoNuSeg dataset, our synthesis pipeline produces more diverse and realistic images than baseline models while maintaining high coherence between input masks and generated images. When using the identical segmentation backbones, the models trained on our synthetic dataset significantly outperform those trained with pseudo-labels or baseline-generated images. Moreover, our framework achieves comparable results to models trained on authentic microscopy images with dense labels, demonstrating its potential as a reliable and highly efficient alternative to labor-intensive manual pixel-wise annotations in microscopy image segmentation. The code is available.
</details>
<details>
<summary>摘要</summary>
当前的深度学习基本方法 для微scopic图像分割具有大量的训练数据和密集的标注，但在实际应用中是非常成本高昂和劳动密集的。相比于全部标注，其中包括对象的完整边框，点标注，特别是对象的中心点，更加容易获取并且仍然提供了对象分割的关键信息。在这篇论文中，我们假设在训练时有点标注可用，并开发了一个整体框架，包括以下三个阶段：1. 使用点标注生成一个 Pseudo 稠密分割mask，并将其约束于形状假设；2. 使用一个在无对应方式下训练的图像生成模型，将 Pseudo 分割mask 翻译成一个真实的微scopic图像，并对其进行对象水平的准确性 regularization；3. Pseudo 分割mask 和生成的图像组成一个对应的数据集，用于训练适应性的分割模型。在公共的 MoNuSeg 数据集上，我们的生成框架生成了更加多样和真实的图像，同时保持了输入掩模的高准确性。使用同样的分割背包，我们在我们的 sintetic 数据集上训练的模型比使用 Pseudo 标签或基eline-生成的图像训练得更高效，并且与 dense 标注的模型相当。此外，我们的框架可以实现 dense 标注的微scopic图像分割任务中的高效和可靠的替代方案。代码可用。
</details></li>
</ul>
<hr>
<h2 id="Learning-from-A-Single-Graph-is-All-You-Need-for-Near-Shortest-Path-Routing-in-Wireless-Networks"><a href="#Learning-from-A-Single-Graph-is-All-You-Need-for-Near-Shortest-Path-Routing-in-Wireless-Networks" class="headerlink" title="Learning from A Single Graph is All You Need for Near-Shortest Path Routing in Wireless Networks"></a>Learning from A Single Graph is All You Need for Near-Shortest Path Routing in Wireless Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09829">http://arxiv.org/abs/2308.09829</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yung-Fu Chen, Sen Lin, Anish Arora</li>
<li>for: 本研究旨在开发一种可以在无需大量数据样本的情况下，适应所有随机网络模型的本地路由策略学习算法。</li>
<li>methods: 本研究使用深度神经网络（DNNs）来学习本地路由策略，该策略只考虑当前节点和邻居节点的状态。研究者们在选择输入特征和选择“种子图”和子样本的方面借鉴了网络领域知识，以提供理论上的解释性。</li>
<li>results: 研究结果表明，使用生成于一些路由路径的抽样从一个较小的种子图就能够快速学习一个普适的路由策略，该策略可以在大多数随机网络模型中适用。此外，研究者们还发现了一种神经网络，可以准确地模仿扩散前方路由策略的性能。<details>
<summary>Abstract</summary>
We propose a learning algorithm for local routing policies that needs only a few data samples obtained from a single graph while generalizing to all random graphs in a standard model of wireless networks. We thus solve the all-pairs near-shortest path problem by training deep neural networks (DNNs) that efficiently and scalably learn routing policies that are local, i.e., they only consider node states and the states of neighboring nodes. Remarkably, one of these DNNs we train learns a policy that exactly matches the performance of greedy forwarding; another generally outperforms greedy forwarding. Our algorithm design exploits network domain knowledge in several ways: First, in the selection of input features and, second, in the selection of a ``seed graph'' and subsamples from its shortest paths. The leverage of domain knowledge provides theoretical explainability of why the seed graph and node subsampling suffice for learning that is efficient, scalable, and generalizable. Simulation-based results on uniform random graphs with diverse sizes and densities empirically corroborate that using samples generated from a few routing paths in a modest-sized seed graph quickly learns a model that is generalizable across (almost) all random graphs in the wireless network model.
</details>
<details>
<summary>摘要</summary>
Our algorithm leverages network domain knowledge in two ways:1. Selection of input features: We carefully select the input features to ensure that the learned policy is efficient and scalable.2. Selection of a "seed graph" and subsamples from its shortest paths: We use a small seed graph and subsamples from its shortest paths to train the DNNs, which provides theoretical explainability of why the seed graph and node subsampling suffice for learning.Our simulation results on uniform random graphs with diverse sizes and densities show that using samples generated from a few routing paths in a modest-sized seed graph can quickly learn a model that is generalizable across (almost) all random graphs in the wireless network model. This means that our algorithm can learn a routing policy that is effective and efficient, even in a large and complex wireless network.
</details></li>
</ul>
<hr>
<h2 id="VL-PET-Vision-and-Language-Parameter-Efficient-Tuning-via-Granularity-Control"><a href="#VL-PET-Vision-and-Language-Parameter-Efficient-Tuning-via-Granularity-Control" class="headerlink" title="VL-PET: Vision-and-Language Parameter-Efficient Tuning via Granularity Control"></a>VL-PET: Vision-and-Language Parameter-Efficient Tuning via Granularity Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09804">http://arxiv.org/abs/2308.09804</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/henryhzy/vl-pet">https://github.com/henryhzy/vl-pet</a></li>
<li>paper_authors: Zi-Yuan Hu, Yanyang Li, Michael R. Lyu, Liwei Wang</li>
<li>for: 这个研究旨在提出一个可控的vision-and-language参数效率训练（VL-PET）框架，以提高现有的参数效率训练技术的精确度和效率。</li>
<li>methods: 本研究使用了一个新的粒度控制机制，允许在不同的粒度控制矩阵上进行模块化修改，从而产生多种模型独立的VL-PET模组。此外，我们还提出了一些轻量级PET模组的设计，以提高预料和文本生成的整合。</li>
<li>results: 我们在四个图像数据项目和四个影片数据项目上进行了广泛的实验，结果显示了我们的VL-PET框架在效率、有效性和转移性方面具有优秀的表现。尤其是，我们的VL-PET-大模组，配备了轻量级PET模组，与BART-base和T5-base相比，在图像数据项目上提高了2.92%（3.41%）和3.37%（7.03%）。<details>
<summary>Abstract</summary>
As the model size of pre-trained language models (PLMs) grows rapidly, full fine-tuning becomes prohibitively expensive for model training and storage. In vision-and-language (VL), parameter-efficient tuning (PET) techniques are proposed to integrate modular modifications (e.g., Adapter and LoRA) into encoder-decoder PLMs. By tuning a small set of trainable parameters, these techniques perform on par with full fine-tuning. However, excessive modular modifications and neglecting the functionality gap between the encoders and decoders can lead to performance degradation, while existing PET techniques (e.g., VL-Adapter) overlook these critical issues. In this paper, we propose a Vision-and-Language Parameter-Efficient Tuning (VL-PET) framework to impose effective control over modular modifications via a novel granularity-controlled mechanism. Considering different granularity-controlled matrices generated by this mechanism, a variety of model-agnostic VL-PET modules can be instantiated from our framework for better efficiency and effectiveness trade-offs. We further propose lightweight PET module designs to enhance VL alignment and modeling for the encoders and maintain text generation for the decoders. Extensive experiments conducted on four image-text tasks and four video-text tasks demonstrate the efficiency, effectiveness and transferability of our VL-PET framework. In particular, our VL-PET-large with lightweight PET module designs significantly outperforms VL-Adapter by 2.92% (3.41%) and LoRA by 3.37% (7.03%) with BART-base (T5-base) on image-text tasks. Furthermore, we validate the enhanced effect of employing our VL-PET designs on existing PET techniques, enabling them to achieve significant performance improvements. Our code is available at https://github.com/HenryHZY/VL-PET.
</details>
<details>
<summary>摘要</summary>
As the size of pre-trained language models (PLMs) continues to grow rapidly, full fine-tuning becomes increasingly expensive for model training and storage. In the field of vision-and-language (VL), parameter-efficient tuning (PET) techniques have been proposed to integrate modular modifications (e.g., Adapter and LoRA) into encoder-decoder PLMs. By tuning a small set of trainable parameters, these techniques can achieve performance on par with full fine-tuning. However, excessive modular modifications and neglecting the functionality gap between the encoders and decoders can lead to performance degradation. Additionally, existing PET techniques (e.g., VL-Adapter) overlook these critical issues.In this paper, we propose a Vision-and-Language Parameter-Efficient Tuning (VL-PET) framework to effectively control modular modifications through a novel granularity-controlled mechanism. By considering different granularity-controlled matrices generated by this mechanism, a variety of model-agnostic VL-PET modules can be instantiated from our framework for better efficiency and effectiveness trade-offs. Furthermore, we propose lightweight PET module designs to enhance VL alignment and modeling for the encoders and maintain text generation for the decoders.Extensive experiments conducted on four image-text tasks and four video-text tasks demonstrate the efficiency, effectiveness, and transferability of our VL-PET framework. In particular, our VL-PET-large with lightweight PET module designs significantly outperforms VL-Adapter by 2.92% (3.41%) and LoRA by 3.37% (7.03%) with BART-base (T5-base) on image-text tasks. Additionally, we validate the enhanced effect of employing our VL-PET designs on existing PET techniques, enabling them to achieve significant performance improvements. Our code is available at https://github.com/HenryHZY/VL-PET.
</details></li>
</ul>
<hr>
<h2 id="An-Efficient-High-Dimensional-Gene-Selection-Approach-based-on-Binary-Horse-Herd-Optimization-Algorithm-for-Biological-Data-Classification"><a href="#An-Efficient-High-Dimensional-Gene-Selection-Approach-based-on-Binary-Horse-Herd-Optimization-Algorithm-for-Biological-Data-Classification" class="headerlink" title="An Efficient High-Dimensional Gene Selection Approach based on Binary Horse Herd Optimization Algorithm for Biological Data Classification"></a>An Efficient High-Dimensional Gene Selection Approach based on Binary Horse Herd Optimization Algorithm for Biological Data Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09791">http://arxiv.org/abs/2308.09791</a></li>
<li>repo_url: None</li>
<li>paper_authors: Niloufar Mehrabi, Sayed Pedram Haeri Boroujeni, Elnaz Pashaei</li>
<li>for: 解决复杂和高维度问题</li>
<li>methods: 使用新的Horse Herd Optimization Algorithm（HOA）和一种新的 Transfer Function（TF），以及一种hybrid feature selection方法 combining HOA和MRMR筛选法。</li>
<li>results: 比较其他状态的精准率和最少选择的特征数，MRMR-BHOA方法表现出色，并且实验结果表明X-形BHOA方法超过其他方法。<details>
<summary>Abstract</summary>
The Horse Herd Optimization Algorithm (HOA) is a new meta-heuristic algorithm based on the behaviors of horses at different ages. The HOA was introduced recently to solve complex and high-dimensional problems. This paper proposes a binary version of the Horse Herd Optimization Algorithm (BHOA) in order to solve discrete problems and select prominent feature subsets. Moreover, this study provides a novel hybrid feature selection framework based on the BHOA and a minimum Redundancy Maximum Relevance (MRMR) filter method. This hybrid feature selection, which is more computationally efficient, produces a beneficial subset of relevant and informative features. Since feature selection is a binary problem, we have applied a new Transfer Function (TF), called X-shape TF, which transforms continuous problems into binary search spaces. Furthermore, the Support Vector Machine (SVM) is utilized to examine the efficiency of the proposed method on ten microarray datasets, namely Lymphoma, Prostate, Brain-1, DLBCL, SRBCT, Leukemia, Ovarian, Colon, Lung, and MLL. In comparison to other state-of-the-art, such as the Gray Wolf (GW), Particle Swarm Optimization (PSO), and Genetic Algorithm (GA), the proposed hybrid method (MRMR-BHOA) demonstrates superior performance in terms of accuracy and minimum selected features. Also, experimental results prove that the X-Shaped BHOA approach outperforms others methods.
</details>
<details>
<summary>摘要</summary>
《马群优化算法（HOA）是一种新的meta-heuristic算法，基于马匹不同年龄的行为。HOA最近被引入以解决复杂高维问题。本文提出了一种二进制版本的马群优化算法（BHOA），用于解 discrete问题并选择出色特征子集。此外，本研究还提出了一种 hybrid 特征选择框架，基于 BHOA 和最小重复度最大相关性（MRMR）筛选法。这种 hybrid 特征选择更加 computationally efficient，生成了有利的特征子集。由于特征选择是一个二进制问题，我们采用了一个新的转移函数（TF），称为 X-形 TF，将连续问题转换成二进制搜索空间。此外，我们使用了支持向量机（SVM）来评估提案方法在十个 microarray 数据集上的效率，即 Limphoma、Prostate、Brain-1、DLBCL、SRBCT、Leukemia、Ovarian、Colon、Lung 和 MLL。与其他现有的state-of-the-art，如灰狼（GW）、PARTICLE SWARM OPTIMIZATION（PSO）和遗传算法（GA）相比，我们的 hybrid 方法（MRMR-BHOA）在准确率和选择的最小特征数上显示出超越性。此外，实验结果也证明了 X-Shape BHOA 方法的优越性。
</details></li>
</ul>
<hr>
<h2 id="A-Two-Part-Machine-Learning-Approach-to-Characterizing-Network-Interference-in-A-B-Testing"><a href="#A-Two-Part-Machine-Learning-Approach-to-Characterizing-Network-Interference-in-A-B-Testing" class="headerlink" title="A Two-Part Machine Learning Approach to Characterizing Network Interference in A&#x2F;B Testing"></a>A Two-Part Machine Learning Approach to Characterizing Network Interference in A&#x2F;B Testing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09790">http://arxiv.org/abs/2308.09790</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuan Yuan, Kristen M. Altenburger</li>
<li>for: 提高控制实验的可靠性，解决网络干扰问题</li>
<li>methods: 使用机器学习算法自动识别和特征化不同类型的网络干扰，并使用透明的机器学习模型确定最佳曝光映射</li>
<li>results: 通过验证两个synthetic实验和一个实际的大规模Instagram用户测试，与传统方法相比，提高了A&#x2F;B测试结果的精度和可靠性<details>
<summary>Abstract</summary>
The reliability of controlled experiments, or "A/B tests," can often be compromised due to the phenomenon of network interference, wherein the outcome for one unit is influenced by other units. To tackle this challenge, we propose a machine learning-based method to identify and characterize heterogeneous network interference. Our approach accounts for latent complex network structures and automates the task of "exposure mapping'' determination, which addresses the two major limitations in the existing literature. We introduce "causal network motifs'' and employ transparent machine learning models to establish the most suitable exposure mapping that reflects underlying network interference patterns. Our method's efficacy has been validated through simulations on two synthetic experiments and a real-world, large-scale test involving 1-2 million Instagram users, outperforming conventional methods such as design-based cluster randomization and analysis-based neighborhood exposure mapping. Overall, our approach not only offers a comprehensive, automated solution for managing network interference and improving the precision of A/B testing results, but it also sheds light on users' mutual influence and aids in the refinement of marketing strategies.
</details>
<details>
<summary>摘要</summary>
控制实验的可靠性，或“A/B测试”，经常受到网络干扰的影响，其中一个单元的结果会受到其他单元的影响。为解决这个挑战，我们提议一种基于机器学习的方法，用于识别和特征化不同类型的网络干扰。我们的方法考虑了隐藏的复杂网络结构，并自动确定“曝光 mapping”，解决了现有文献中的两大限制。我们引入“ causal 网络模式”并使用透明的机器学习模型，以确定最佳的曝光 mapping，它反映了下面网络干扰模式。我们的方法在两个人工实验和一个实际的大规模实验中，与传统的设计基于块随机分配和分析基于邻居曝光 mapping相比，表现出了更高的效果。总之，我们的方法不仅提供了一种完整、自动化的网络干扰管理和A/B测试结果的精度提高的解决方案，还可以揭示用户之间的互动关系，帮助改进营销策略。
</details></li>
</ul>
<hr>
<h2 id="Towards-Grounded-Visual-Spatial-Reasoning-in-Multi-Modal-Vision-Language-Models"><a href="#Towards-Grounded-Visual-Spatial-Reasoning-in-Multi-Modal-Vision-Language-Models" class="headerlink" title="Towards Grounded Visual Spatial Reasoning in Multi-Modal Vision Language Models"></a>Towards Grounded Visual Spatial Reasoning in Multi-Modal Vision Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09778">http://arxiv.org/abs/2308.09778</a></li>
<li>repo_url: None</li>
<li>paper_authors: Navid Rajabi, Jana Kosecka</li>
<li>for: 研究大规模视言模型（VLM）在视觉理解任务中表现，特别是识别空间关系的能力。</li>
<li>methods: 提出细化的 композиitional 顺序排序方法，结合图像文本匹配或视觉问答任务，以评估视觉关系理解能力。</li>
<li>results: 通过对象和其位置的识别，计算最终排名的空间 clause，并在多种视觉语言模型（Tan和Bansal 2019; Gupta等 2022; Kamath等 2021）上进行评估和比较，以 highlight 它们在理解空间关系方面的能力。<details>
<summary>Abstract</summary>
With the advances in large scale vision-and-language models (VLMs) it is of interest to assess their performance on various visual reasoning tasks such as counting, referring expressions and general visual question answering. The focus of this work is to study the ability of these models to understanding spatial relations. Previously, this has been tackled using image-text matching (Liu, Emerson, and Collier 2022) or visual question answering task, both showing poor performance and a large gap compared to human performance. To better understand the gap, we present fine-grained compositional grounding of spatial relationships and propose a bottom up approach for ranking spatial clauses and evaluating the performance of spatial relationship reasoning task. We propose to combine the evidence from grounding noun phrases corresponding to objects and their locations to compute the final rank of the spatial clause. We demonstrate the approach on representative vision-language models (Tan and Bansal 2019; Gupta et al. 2022; Kamath et al. 2021) and compare and highlight their abilities to reason about spatial relationships.
</details>
<details>
<summary>摘要</summary>
通过大规模视力语言模型（VLM）的发展，我们关心其在不同的视觉逻辑任务中的表现，如数字、引用表达和通用视觉问答。我们的研究着点在于检测这些模型对空间关系的理解能力。既前面的研究通过图像文本匹配（Liu、Emerson和Collier 2022）或视觉问答任务来评估这些模型的表现，都显示了较差的性能和人类表现之间的大差。为了更好地理解这个差距，我们提出了细化的 композиitional 顺序排序方法，并提出了一种底向方法来评估视觉关系逻辑任务的表现。我们将基于对物体和其位置的语言表达grounding的证据来计算最终排名的空间条款。我们在代表性的视力语言模型（Tan和Bansal 2019; Gupta等 2022; Kamath等 2021）上进行了实验，并对这些模型在视觉关系逻辑任务中的表现进行了比较和高亮。
</details></li>
</ul>
<hr>
<h2 id="Time-Series-Predictions-in-Unmonitored-Sites-A-Survey-of-Machine-Learning-Techniques-in-Water-Resources"><a href="#Time-Series-Predictions-in-Unmonitored-Sites-A-Survey-of-Machine-Learning-Techniques-in-Water-Resources" class="headerlink" title="Time Series Predictions in Unmonitored Sites: A Survey of Machine Learning Techniques in Water Resources"></a>Time Series Predictions in Unmonitored Sites: A Survey of Machine Learning Techniques in Water Resources</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09766">http://arxiv.org/abs/2308.09766</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jared D. Willard, Charuleka Varadharajan, Xiaowei Jia, Vipin Kumar</li>
<li>For: The paper is written for water resources science, specifically for predicting dynamic environmental variables in unmonitored sites.* Methods: The paper uses modern machine learning methods, such as deep learning frameworks, to predict hydrological variables like river flow and water quality.* Results: The paper reviews state-of-the-art applications of machine learning for streamflow, water quality, and other water resources prediction, and identifies open questions for time series predictions in unmonitored sites, including incorporating dynamic inputs and site characteristics, mechanistic understanding and spatial context, and explainable AI techniques.Here is the same information in Simplified Chinese text:* For: 论文写于水资源科学领域，特点是预测未监测站点的动态环境变量。* Methods: 论文使用现代机器学习方法，如深度学习框架，预测河流流量和水质等ydrological变量。* Results: 论文对水资源预测进行了国际评估，并确定了未监测站点预测时间序列的开放问题，包括包括动态输入和站点特点、机制理解和空间上下文等。<details>
<summary>Abstract</summary>
Prediction of dynamic environmental variables in unmonitored sites remains a long-standing challenge for water resources science. The majority of the world's freshwater resources have inadequate monitoring of critical environmental variables needed for management. Yet, the need to have widespread predictions of hydrological variables such as river flow and water quality has become increasingly urgent due to climate and land use change over the past decades, and their associated impacts on water resources. Modern machine learning methods increasingly outperform their process-based and empirical model counterparts for hydrologic time series prediction with their ability to extract information from large, diverse data sets. We review relevant state-of-the art applications of machine learning for streamflow, water quality, and other water resources prediction and discuss opportunities to improve the use of machine learning with emerging methods for incorporating watershed characteristics into deep learning models, transfer learning, and incorporating process knowledge into machine learning models. The analysis here suggests most prior efforts have been focused on deep learning learning frameworks built on many sites for predictions at daily time scales in the United States, but that comparisons between different classes of machine learning methods are few and inadequate. We identify several open questions for time series predictions in unmonitored sites that include incorporating dynamic inputs and site characteristics, mechanistic understanding and spatial context, and explainable AI techniques in modern machine learning frameworks.
</details>
<details>
<summary>摘要</summary>
<<SYS>>水资源科学中预测无监测站的动态环境变量是一项长期挑战。全球大多数新鲜水资源尚未充分监测关键环境变量，尤其是气候和土地使用变化过去几十年所带来的影响。然而，由于气候和土地使用变化，水资源预测的需求日益增加。现代机器学习方法在水文时序预测方面升级表现，可以从大量多样数据集中提取信息。我们评估了相关的现代应用，包括流量和水质预测，并讨论了将水 shed特征 incorporated into deep learning模型、传输学习和机器学习模型中的进程知识。分析表明，大多数先前努力都集中在了基于多地点的深度学习框架上，但对不同类型机器学习方法的比较 remains limited。我们标识了一些未解决的问题，包括 incorporating 动态输入和站点特征、机制理解和空间上下文，以及现代机器学习框架中的可解释AI技术。
</details></li>
</ul>
<hr>
<h2 id="Taken-by-Surprise-Contrast-effect-for-Similarity-Scores"><a href="#Taken-by-Surprise-Contrast-effect-for-Similarity-Scores" class="headerlink" title="Taken by Surprise: Contrast effect for Similarity Scores"></a>Taken by Surprise: Contrast effect for Similarity Scores</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09765">http://arxiv.org/abs/2308.09765</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/meetelise/surprise-similarity">https://github.com/meetelise/surprise-similarity</a></li>
<li>paper_authors: Thomas C. Bachlechner, Mario Martone, Marjorie Schillo</li>
<li>for: 本研究旨在提出一种基于人类听觉效应的对象向量表示的相似性评价方法，以提高自然语言处理、信息检索和分类任务的性能。</li>
<li>methods: 本研究使用了一种 называ为“surprise score”的ensemble-normalized相似性指标，该指标基于对象向量的分布来衡量对象之间的相似性。</li>
<li>results: 研究发现，使用“surprise score”指标可以在零&#x2F;几shot文档分类任务中提高性能， Typically 10-15% better than raw cosine similarity。<details>
<summary>Abstract</summary>
Accurately evaluating the similarity of object vector embeddings is of critical importance for natural language processing, information retrieval and classification tasks. Popular similarity scores (e.g cosine similarity) are based on pairs of embedding vectors and disregard the distribution of the ensemble from which objects are drawn. Human perception of object similarity significantly depends on the context in which the objects appear. In this work we propose the $\textit{surprise score}$, an ensemble-normalized similarity metric that encapsulates the contrast effect of human perception and significantly improves the classification performance on zero- and few-shot document classification tasks. This score quantifies the surprise to find a given similarity between two elements relative to the pairwise ensemble similarities. We evaluate this metric on zero/few shot classification and clustering tasks and typically find 10-15 % better performance compared to raw cosine similarity. Our code is available at https://github.com/MeetElise/surprise-similarity.
</details>
<details>
<summary>摘要</summary>
精准评估对象vector编码的相似性对自然语言处理、信息检索和分类任务是极为重要的。受欢迎的相似性分数（例如归一化相似性）基于对 embedding vector的对应对，而忽略了对象 ensemble 的分布。人类对对象相似性的识别受到对象出现的 Context 的影响。在这种工作中，我们提议使用 $\textit{surprise score}$，一种ensemble-normalized相似度度量，它体现了人类对对象相似性的冲击效应，并在零/几个shot文档分类任务中显著提高分类性能， Typically 10-15%。我们在这些任务中评估了这个分数，并发现它 Typically 10-15% 比raw cosine similarity better。我们的代码可以在https://github.com/MeetElise/surprise-similarity上找到。
</details></li>
</ul>
<hr>
<h2 id="The-Impact-of-Background-Removal-on-Performance-of-Neural-Networks-for-Fashion-Image-Classification-and-Segmentation"><a href="#The-Impact-of-Background-Removal-on-Performance-of-Neural-Networks-for-Fashion-Image-Classification-and-Segmentation" class="headerlink" title="The Impact of Background Removal on Performance of Neural Networks for Fashion Image Classification and Segmentation"></a>The Impact of Background Removal on Performance of Neural Networks for Fashion Image Classification and Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09764">http://arxiv.org/abs/2308.09764</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junhui Liang, Ying Liu, Vladimir Vlassov</li>
<li>for: 提高时尚图像数据质量和模型性能</li>
<li>methods: 使用突出对象检测来移除背景</li>
<li>results: 可以提高模型准确率达5%，但是深度网络不适合使用Background Removal因为与其他正则化技术不兼容<details>
<summary>Abstract</summary>
Fashion understanding is a hot topic in computer vision, with many applications having great business value in the market. Fashion understanding remains a difficult challenge for computer vision due to the immense diversity of garments and various scenes and backgrounds. In this work, we try removing the background from fashion images to boost data quality and increase model performance. Having fashion images of evident persons in fully visible garments, we can utilize Salient Object Detection to achieve the background removal of fashion data to our expectations. A fashion image with the background removed is claimed as the "rembg" image, contrasting with the original one in the fashion dataset. We conducted extensive comparative experiments with these two types of images on multiple aspects of model training, including model architectures, model initialization, compatibility with other training tricks and data augmentations, and target task types. Our experiments show that background removal can effectively work for fashion data in simple and shallow networks that are not susceptible to overfitting. It can improve model accuracy by up to 5% in the classification on the FashionStyle14 dataset when training models from scratch. However, background removal does not perform well in deep neural networks due to incompatibility with other regularization techniques like batch normalization, pre-trained initialization, and data augmentations introducing randomness. The loss of background pixels invalidates many existing training tricks in the model training, adding the risk of overfitting for deep models.
</details>
<details>
<summary>摘要</summary>
现代时尚理解是计算机视觉领域的热门话题，具有广泛的商业价值。然而，时尚理解仍然是计算机视觉中的挑战，因为衣服的多样性和不同的场景和背景。在这项工作中，我们尝试将时尚图像的背景移除，以提高数据质量并提高模型性能。通过使用突出对象检测来实现背景移除，我们提出了“rembg”图像的概念，与原始时尚数据集的图像进行对比。我们进行了多种比较实验，包括模型架构、模型初始化、与其他训练技巧和数据扩展相容性等方面。我们的实验结果表明，背景移除可以有效地提高时尚数据的模型准确率，但是深度网络中的背景移除不太好，因为它们与批处理常规化、预先初始化和数据扩展引入随机性不兼容。失去背景像素会让许多现有的训练技巧在模型训练中添加随机性，增加深度模型的风险过拟合。
</details></li>
</ul>
<hr>
<h2 id="Data-Compression-and-Inference-in-Cosmology-with-Self-Supervised-Machine-Learning"><a href="#Data-Compression-and-Inference-in-Cosmology-with-Self-Supervised-Machine-Learning" class="headerlink" title="Data Compression and Inference in Cosmology with Self-Supervised Machine Learning"></a>Data Compression and Inference in Cosmology with Self-Supervised Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09751">http://arxiv.org/abs/2308.09751</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aizhanaakhmet/data-compression-inference-in-cosmology-with-ssl">https://github.com/aizhanaakhmet/data-compression-inference-in-cosmology-with-ssl</a></li>
<li>paper_authors: Aizhan Akhmetzhanova, Siddharth Mishra-Sharma, Cora Dvorkin</li>
<li>for:  cosmological surveys 的数据压缩</li>
<li>methods: 使用自动augmentation的自然语言处理技术</li>
<li>results: 可以生成高度信息含量的摘要，用于各种下游任务，如精确参数推导Here’s a more detailed explanation of each point:</li>
<li>for: The paper is written for the purpose of exploring a new method for compressing and analyzing large cosmological datasets.</li>
<li>methods: The paper uses a self-supervised machine learning approach called simulation-based augmentations to construct representative summaries of the data.</li>
<li>results: The method is shown to deliver highly informative summaries that can be used for a variety of downstream tasks, such as precise and accurate parameter inference, and is insensitive to prescribed systematic effects like the influence of baryonic physics.<details>
<summary>Abstract</summary>
The influx of massive amounts of data from current and upcoming cosmological surveys necessitates compression schemes that can efficiently summarize the data with minimal loss of information. We introduce a method that leverages the paradigm of self-supervised machine learning in a novel manner to construct representative summaries of massive datasets using simulation-based augmentations. Deploying the method on hydrodynamical cosmological simulations, we show that it can deliver highly informative summaries, which can be used for a variety of downstream tasks, including precise and accurate parameter inference. We demonstrate how this paradigm can be used to construct summary representations that are insensitive to prescribed systematic effects, such as the influence of baryonic physics. Our results indicate that self-supervised machine learning techniques offer a promising new approach for compression of cosmological data as well its analysis.
</details>
<details>
<summary>摘要</summary>
Current and upcoming cosmological surveys will produce vast amounts of data, making it essential to develop compression schemes that can efficiently summarize the data with minimal loss of information. We propose a method that leverages self-supervised machine learning in a novel way to create representative summaries of massive datasets using simulation-based augmentations. Applying the method to hydrodynamical cosmological simulations, we show that it can produce highly informative summaries that can be used for a variety of downstream tasks, such as precise and accurate parameter inference. Our results suggest that self-supervised machine learning techniques offer a promising new approach for compressing and analyzing cosmological data.Here's the text in Traditional Chinese for comparison:现有和未来的 cosmological surveys 将生成巨量数据，因此需要发展压缩方案，以实现最小化信息损失的数据概要。我们提出了一种方法，利用自动化学习的 Paradigma 在一种新的方式中，创建大量数据的代表概要，使用 simulations 的增强。将方法应用到 hydrodynamical cosmological simulations，我们展示了它可以生成高度有用的概要，可以用于多种下游任务，例如精确和准确的参数推断。我们的结果表明，自动化学习技术对 cosmological data 的压缩和分析提供了一个有前途的新方法。
</details></li>
</ul>
<hr>
<h2 id="Robust-Monocular-Depth-Estimation-under-Challenging-Conditions"><a href="#Robust-Monocular-Depth-Estimation-under-Challenging-Conditions" class="headerlink" title="Robust Monocular Depth Estimation under Challenging Conditions"></a>Robust Monocular Depth Estimation under Challenging Conditions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09711">http://arxiv.org/abs/2308.09711</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/md4all/md4all">https://github.com/md4all/md4all</a></li>
<li>paper_authors: Stefano Gasperini, Nils Morbitzer, HyunJun Jung, Nassir Navab, Federico Tombari</li>
<li>for: 提高单目深度估计的可靠性，并能在不同的环境和气候条件下工作，包括夜间和雨天等。</li>
<li>methods: 利用现有方法的有效性，生成复杂的样本，并通过自我或全量监督训练，使模型能够在不同的condition下恢复信息。</li>
<li>results: 在nuScenes和Oxford RobotCar等两个公共数据集上进行了广泛的实验，比 Priors works 大幅提高了效果，在标准和挑战性条件下都有出色的表现。<details>
<summary>Abstract</summary>
While state-of-the-art monocular depth estimation approaches achieve impressive results in ideal settings, they are highly unreliable under challenging illumination and weather conditions, such as at nighttime or in the presence of rain. In this paper, we uncover these safety-critical issues and tackle them with md4all: a simple and effective solution that works reliably under both adverse and ideal conditions, as well as for different types of learning supervision. We achieve this by exploiting the efficacy of existing methods under perfect settings. Therefore, we provide valid training signals independently of what is in the input. First, we generate a set of complex samples corresponding to the normal training ones. Then, we train the model by guiding its self- or full-supervision by feeding the generated samples and computing the standard losses on the corresponding original images. Doing so enables a single model to recover information across diverse conditions without modifications at inference time. Extensive experiments on two challenging public datasets, namely nuScenes and Oxford RobotCar, demonstrate the effectiveness of our techniques, outperforming prior works by a large margin in both standard and challenging conditions. Source code and data are available at: https://md4all.github.io.
</details>
<details>
<summary>摘要</summary>
当前的单目深度估计方法在理想的设置下可以获得很好的结果，但在具有挑战性的照明和天气条件下（如夜晚或雨天），这些方法却很不可靠。在这篇论文中，我们揭示了这些安全关键问题并解决了它们，使用md4all：一个简单有效的解决方案，在不同的条件下都可靠地工作，包括理想和挑战性的条件，以及不同类型的学习监督。我们实现了这一点通过利用现有方法在完美的设置下的效果。因此，我们可以在训练时提供有效的训练信号，不受输入内容的限制。首先，我们生成一个包含复杂样本的集合，与常见的训练样本相对应。然后，我们使用这些生成的样本和对原始图像进行标准损失计算来引导模型的自我或全自监督。这样做的原因是，我们可以在推理时不需要对模型进行修改，以便在多种条件下进行推理。我们的技术在nuScenes和Oxford RobotCar两个公共数据集上进行了广泛的实验，并且与之前的工作相比，在标准和挑战性条件下都有很大的进步。代码和数据可以在https://md4all.github.io获取。
</details></li>
</ul>
<hr>
<h2 id="Neural-network-quantum-state-study-of-the-long-range-antiferromagnetic-Ising-chain"><a href="#Neural-network-quantum-state-study-of-the-long-range-antiferromagnetic-Ising-chain" class="headerlink" title="Neural-network quantum state study of the long-range antiferromagnetic Ising chain"></a>Neural-network quantum state study of the long-range antiferromagnetic Ising chain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09709">http://arxiv.org/abs/2308.09709</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jicheol Kim, Dongkyu Kim, Dong-Hee Kim</li>
<li>for:  investigate quantum phase transitions in the transverse field Ising chain with algebraically decaying long-range antiferromagnetic interactions</li>
<li>methods:  use variational Monte Carlo method with restricted Boltzmann machine as trial wave function ansatz</li>
<li>results:  find that central charge deviates from 1&#x2F;2 at small decay exponent $\alpha_\mathrm{LR}$, and identify threshold of Ising universality and conformal symmetry deviation from SR Ising class at $\alpha_\mathrm{LR} &lt; 2$<details>
<summary>Abstract</summary>
We investigate quantum phase transitions in the transverse field Ising chain with algebraically decaying long-range antiferromagnetic interactions by using the variational Monte Carlo method with the restricted Boltzmann machine being employed as a trial wave function ansatz. In the finite-size scaling analysis with the order parameter and the second R\'enyi entropy, we find that the central charge deviates from 1/2 at a small decay exponent $\alpha_\mathrm{LR}$ in contrast to the critical exponents staying very close to the short-range (SR) Ising values regardless of $\alpha_\mathrm{LR}$ examined, supporting the previously proposed scenario of conformal invariance breakdown. To identify the threshold of the Ising universality and the conformal symmetry, we perform two additional tests for the universal Binder ratio and the conformal field theory (CFT) description of the correlation function. It turns out that both indicate a noticeable deviation from the SR Ising class at $\alpha_\mathrm{LR} < 2$. However, a closer look at the scaled correlation function for $\alpha_\mathrm{LR} \ge 2$ shows a gradual change from the asymptotic line of the CFT verified at $\alpha_\mathrm{LR} = 3$, providing a rough estimate of the threshold being in the range of $2 \lesssim \alpha_\mathrm{LR} < 3$.
</details>
<details>
<summary>摘要</summary>
我们研究量子阶段转变在横向Isings链中的数学态度，使用variational Monte Carlo方法和受限 Boltzmann机作为实验波函数构想。在finite-size扩展分析中，我们发现中心 charge在小数字 $\alpha_\text{LR}$ 下异 від 1/2，与短距离Isings值不同，支持之前提出的对称性破坏enario。为了识别Isings universality和对称性的阈值，我们进行了两个额外测试： universal Binder 比率和对称场论 (CFT) 描述的联系函数。结果显示，两个测试都显示了 SR Isings 类型的明显偏离，但是在 $\alpha_\text{LR} \ge 2$ 时，扩展联系函数的涨落趋势逐渐变化为CFT预测的极限线，提供了约 estimate的阈值在 $2 \lesssim \alpha_\text{LR} < 3$ 之间。
</details></li>
</ul>
<hr>
<h2 id="Do-you-know-what-q-means"><a href="#Do-you-know-what-q-means" class="headerlink" title="Do you know what q-means?"></a>Do you know what q-means?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09701">http://arxiv.org/abs/2308.09701</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Sfedfcv/redesigned-pancake">https://github.com/Sfedfcv/redesigned-pancake</a></li>
<li>paper_authors: João F. Doriguello, Alessandro Luongo, Ewin Tang</li>
<li>for: 本研究旨在提出一种可以实现高效的分类算法，具体来说是$k$-means算法的一种近似版本。</li>
<li>methods: 本研究使用的方法包括：Lloyd’s iteration algorithm和一种基于QRAM的近似$k$-means算法。</li>
<li>results: 本研究的结果包括：提出了一种基于QRAM的$k$-means算法，该算法可以在$O\big(\frac{k^{2}}{\varepsilon^2}(\sqrt{k}d + \log(Nd))\big)$时间内完成分类任务，同时保持了对$N$的多项幂逻辑依赖。此外，还提出了一种类比的分类算法，即”dequantized”算法，其时间复杂度为$O\big(\frac{k^{2}}{\varepsilon^2}(kd + \log(Nd))\big)$。<details>
<summary>Abstract</summary>
Clustering is one of the most important tools for analysis of large datasets, and perhaps the most popular clustering algorithm is Lloyd's iteration for $k$-means. This iteration takes $N$ vectors $v_1,\dots,v_N\in\mathbb{R}^d$ and outputs $k$ centroids $c_1,\dots,c_k\in\mathbb{R}^d$; these partition the vectors into clusters based on which centroid is closest to a particular vector. We present an overall improved version of the "$q$-means" algorithm, the quantum algorithm originally proposed by Kerenidis, Landman, Luongo, and Prakash (2019) which performs $\varepsilon$-$k$-means, an approximate version of $k$-means clustering. This algorithm does not rely on the quantum linear algebra primitives of prior work, instead only using its QRAM to prepare and measure simple states based on the current iteration's clusters. The time complexity is $O\big(\frac{k^{2}}{\varepsilon^2}(\sqrt{k}d + \log(Nd))\big)$ and maintains the polylogarithmic dependence on $N$ while improving the dependence on most of the other parameters. We also present a "dequantized" algorithm for $\varepsilon$-$k$-means which runs in $O\big(\frac{k^{2}}{\varepsilon^2}(kd + \log(Nd))\big)$ time. Notably, this classical algorithm matches the polylogarithmic dependence on $N$ attained by the quantum algorithms.
</details>
<details>
<summary>摘要</summary>
clustering 是大规模数据分析中最重要的工具之一，而可能最受欢迎的归一化算法之一就是沛降的迭代法（Lloyd's iteration）。这个迭代法会将 N 个坐标vector $v_1, \ldots, v_N \in \mathbb{R}^d$ 转化为 K 个中心点 $c_1, \ldots, c_K \in \mathbb{R}^d$，这些中心点将坐标分成 clusters 基于哪个中心点最近。我们提出了一个全面改进的 "$q$-means" 算法，它是 Kerenidis et al. (2019) 提出的量子算法的应用，用于实现 $\varepsilon$-$k$-means，这是 $k$-means 归一化 clustering 的一个 Approximate 版本。这个算法不依赖于量子线性代数基本操作，而是只使用其 QRAM 来准备和测量基于当前迭代的 clusters 的简单状态。时间复杂度为 $O\big(\frac{k^{2}}{\varepsilon^2}(\sqrt{k}d + \log(Nd))\big)$，这保持了对 N 的多项式依赖性，而改善了大多数其他参数的依赖性。我们还提出了一个 "dequantized" 算法，它在 $O\big(\frac{k^{2}}{\varepsilon^2}(kd + \log(Nd))\big)$ 时间内运行，并且与量子算法具有相同的多项式依赖性。它的时间复杂度与量子算法具有相同的多项式依赖性。
</details></li>
</ul>
<hr>
<h2 id="A-Lightweight-Transformer-for-Faster-and-Robust-EBSD-Data-Collection"><a href="#A-Lightweight-Transformer-for-Faster-and-Robust-EBSD-Data-Collection" class="headerlink" title="A Lightweight Transformer for Faster and Robust EBSD Data Collection"></a>A Lightweight Transformer for Faster and Robust EBSD Data Collection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09693">http://arxiv.org/abs/2308.09693</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hdong920/ebsd_slice_recovery">https://github.com/hdong920/ebsd_slice_recovery</a></li>
<li>paper_authors: Harry Dong, Sean Donegan, Megna Shah, Yuejie Chi</li>
<li>for: 提高三维电子背散射束Diffraction（EBSD）微scopic的数据质量，以便在材料科学中应用。</li>
<li>methods: 使用深度学习模型和投影算法来修复3D EBSD数据中缺失的层。</li>
<li>results: 使用只有自我超vision的synthetic 3D EBSD数据进行训练，在真实3D EBSD数据上获得了比现有方法更高的恢复精度。<details>
<summary>Abstract</summary>
Three dimensional electron back-scattered diffraction (EBSD) microscopy is a critical tool in many applications in materials science, yet its data quality can fluctuate greatly during the arduous collection process, particularly via serial-sectioning. Fortunately, 3D EBSD data is inherently sequential, opening up the opportunity to use transformers, state-of-the-art deep learning architectures that have made breakthroughs in a plethora of domains, for data processing and recovery. To be more robust to errors and accelerate this 3D EBSD data collection, we introduce a two step method that recovers missing slices in an 3D EBSD volume, using an efficient transformer model and a projection algorithm to process the transformer's outputs. Overcoming the computational and practical hurdles of deep learning with scarce high dimensional data, we train this model using only synthetic 3D EBSD data with self-supervision and obtain superior recovery accuracy on real 3D EBSD data, compared to existing methods.
</details>
<details>
<summary>摘要</summary>
三维电子后射扩散Diffraction（EBSD） Mikroskopi是物料科学中多种应用中的重要工具，但它的数据质量可能会在收集过程中变化很大，特别是通过串行sectioning。幸运的是，3D EBSD数据是顺序的，这为使用transformer，当前领域的最先进深度学习架构，进行数据处理和恢复提供了机会。为了更加鲁棒地处理错误和加速3D EBSD数据收集，我们提出了一种两步方法，使用高效的transformer模型和投影算法来处理transformer的输出。通过超越深度学习中的计算和实践障碍，我们使用只有自我超vision的synthetic 3D EBSD数据进行训练，并在实际3D EBSD数据上获得了比现有方法更高的恢复精度。
</details></li>
</ul>
<hr>
<h2 id="Reduced-Order-Modeling-of-a-MOOSE-based-Advanced-Manufacturing-Model-with-Operator-Learning"><a href="#Reduced-Order-Modeling-of-a-MOOSE-based-Advanced-Manufacturing-Model-with-Operator-Learning" class="headerlink" title="Reduced Order Modeling of a MOOSE-based Advanced Manufacturing Model with Operator Learning"></a>Reduced Order Modeling of a MOOSE-based Advanced Manufacturing Model with Operator Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09691">http://arxiv.org/abs/2308.09691</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahmoud Yaseen, Dewen Yushu, Peter German, Xu Wu</li>
<li>for: 这个论文的目的是开发一种快速运行的减少维度模型（ROM），用于在深度学习控制和优化方法中使用。</li>
<li>methods: 这个论文使用了一种基于操作学习（OL）的方法，使用 fourier neural operator 来构建ROM。</li>
<li>results: 这个论文的研究结果表明，OL-based ROM 可以具有更高的准确性和更快的运行速度，相比于传统的深度神经网络基于 ROM。<details>
<summary>Abstract</summary>
Advanced Manufacturing (AM) has gained significant interest in the nuclear community for its potential application on nuclear materials. One challenge is to obtain desired material properties via controlling the manufacturing process during runtime. Intelligent AM based on deep reinforcement learning (DRL) relies on an automated process-level control mechanism to generate optimal design variables and adaptive system settings for improved end-product properties. A high-fidelity thermo-mechanical model for direct energy deposition has recently been developed within the MOOSE framework at the Idaho National Laboratory (INL). The goal of this work is to develop an accurate and fast-running reduced order model (ROM) for this MOOSE-based AM model that can be used in a DRL-based process control and optimization method. Operator learning (OL)-based methods will be employed due to their capability to learn a family of differential equations, in this work, produced by changing process variables in the Gaussian point heat source for the laser. We will develop OL-based ROM using Fourier neural operator, and perform a benchmark comparison of its performance with a conventional deep neural network-based ROM.
</details>
<details>
<summary>摘要</summary>
高等制造（AM）在核子社区中已引起了广泛的关注，因为它在核子材料方面的应用有很大的潜力。一个挑战是通过控制生产过程的 runtime 来实现所需的材料性能。基于深度强化学习（DRL）的智能制造利用了自动化的过程级别控制机制，以生成优化的设计变量和适应系统设置，以提高终产品的性能。在蒙大学（INL）的MOOSE框架中，最近开发了一个高精度的热力学-机械模型，用于直接能量沟入。本工作的目标是开发一个准确和快速运行的减少阶段模型（ROM），用于这个 MOOSE-based AM 模型，以便在 DRL-based 过程控制和优化方法中使用。我们将使用操作学（OL）-based 方法，因为它们可以学习一个家族的差分方程，在这里，由变量改变在泊松点热源中的激光。我们将开发 OL-based ROM 使用整数 ней网络，并对它的性能进行了比较。
</details></li>
</ul>
<hr>
<h2 id="Graph-of-Thoughts-Solving-Elaborate-Problems-with-Large-Language-Models"><a href="#Graph-of-Thoughts-Solving-Elaborate-Problems-with-Large-Language-Models" class="headerlink" title="Graph of Thoughts: Solving Elaborate Problems with Large Language Models"></a>Graph of Thoughts: Solving Elaborate Problems with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09687">http://arxiv.org/abs/2308.09687</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/spcl/graph-of-thoughts">https://github.com/spcl/graph-of-thoughts</a></li>
<li>paper_authors: Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, Torsten Hoefler</li>
<li>for: 本研究旨在提高大语言模型（LLM）的提示能力，并超越链条思维或树思维（ToT）的限制。</li>
<li>methods: 本研究提出了Graph of Thoughts（GoT）框架，可以模型LLM生成的信息为自由图，其中单元为“LLM思维”，带有两个端点的边表示了这些单元之间的依赖关系。这种方法可以将不同的LLM思维组合成衍生新的结果，捕捉整个网络的核心意思，或通过反馈循环进行增强。</li>
<li>results: 研究表明，GoT可以与现状下的task比较，例如排序任务上提高质量62%，同时降低成本&gt;31%。此外，GoT还可以扩展到新的思维变换，因此可以用于开拓新的提示方案。这项工作使得LLM的思维更加接近人类思维或脑内的复杂网络机制。<details>
<summary>Abstract</summary>
We introduce Graph of Thoughts (GoT): a framework that advances prompting capabilities in large language models (LLMs) beyond those offered by paradigms such as Chain-of-Thought or Tree of Thoughts (ToT). The key idea and primary advantage of GoT is the ability to model the information generated by an LLM as an arbitrary graph, where units of information ("LLM thoughts") are vertices, and edges correspond to dependencies between these vertices. This approach enables combining arbitrary LLM thoughts into synergistic outcomes, distilling the essence of whole networks of thoughts, or enhancing thoughts using feedback loops. We illustrate that GoT offers advantages over state of the art on different tasks, for example increasing the quality of sorting by 62% over ToT, while simultaneously reducing costs by >31%. We ensure that GoT is extensible with new thought transformations and thus can be used to spearhead new prompting schemes. This work brings the LLM reasoning closer to human thinking or brain mechanisms such as recurrence, both of which form complex networks.
</details>
<details>
<summary>摘要</summary>
我团队介绍 Graph of Thoughts（GoT）框架：可以超越Chain-of-Thought或Tree of Thoughts（ToT）概念的推导能力。GoT的关键思想和优势在于将LLM生成的信息视为一个任意图，其中单元信息（LLM思想）是顶点，而各个顶点之间的边表示这些思想之间的依赖关系。这种方法可以将不同的LLM思想结合成补做出新的结果，浓缩整个网络的核心意义，或者通过反馈循环进行增强。我们证明GoT在不同任务上比ToT高质量，同时Costs下降了>31%。此外，GoT可以扩展新的思想转换，因此可以用于推导新的思路。这项工作使LLM推导更接近人类思维或脑机制，如 recursivity，两者都形成复杂的网络。
</details></li>
</ul>
<hr>
<h2 id="Audiovisual-Moments-in-Time-A-Large-Scale-Annotated-Dataset-of-Audiovisual-Actions"><a href="#Audiovisual-Moments-in-Time-A-Large-Scale-Annotated-Dataset-of-Audiovisual-Actions" class="headerlink" title="Audiovisual Moments in Time: A Large-Scale Annotated Dataset of Audiovisual Actions"></a>Audiovisual Moments in Time: A Large-Scale Annotated Dataset of Audiovisual Actions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09685">http://arxiv.org/abs/2308.09685</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mjoannou/audiovisual-moments-in-time">https://github.com/mjoannou/audiovisual-moments-in-time</a></li>
<li>paper_authors: Michael Joannou, Pia Rotshtein, Uta Noppeney</li>
<li>for: 本研究创造了一个大规模的 audiovisual moments in time（AVMIT）数据集，用于识别 audiovisual 动作事件。</li>
<li>methods: 研究人员使用了11名参与者进行了一项广泛的标注任务，对 Mit 数据集中的 3 秒 audiovisual 视频进行标注。每个试验都需要参与者确定 audiovisual 动作事件是否存在，以及该事件是视频中最为突出的特征。</li>
<li>results: 研究人员从 inicial 收集的 57,177 个 audiovisual 视频中选择了 16 种 distinct 动作类，并生成了 60 个视频测试集（960 个视频）。此外，研究人员还提供了 2 个预计算 audiovisual 特征嵌入，使用 VGGish&#x2F;YamNet 和 VGG16&#x2F;EfficientNetB0 进行音频和视频数据的预处理。研究人员发现，使用 AVMIT 的标注和特征嵌入可以提高 audiovisual 事件识别的性能。<details>
<summary>Abstract</summary>
We present Audiovisual Moments in Time (AVMIT), a large-scale dataset of audiovisual action events. In an extensive annotation task 11 participants labelled a subset of 3-second audiovisual videos from the Moments in Time dataset (MIT). For each trial, participants assessed whether the labelled audiovisual action event was present and whether it was the most prominent feature of the video. The dataset includes the annotation of 57,177 audiovisual videos, each independently evaluated by 3 of 11 trained participants. From this initial collection, we created a curated test set of 16 distinct action classes, with 60 videos each (960 videos). We also offer 2 sets of pre-computed audiovisual feature embeddings, using VGGish/YamNet for audio data and VGG16/EfficientNetB0 for visual data, thereby lowering the barrier to entry for audiovisual DNN research. We explored the advantages of AVMIT annotations and feature embeddings to improve performance on audiovisual event recognition. A series of 6 Recurrent Neural Networks (RNNs) were trained on either AVMIT-filtered audiovisual events or modality-agnostic events from MIT, and then tested on our audiovisual test set. In all RNNs, top 1 accuracy was increased by 2.71-5.94\% by training exclusively on audiovisual events, even outweighing a three-fold increase in training data. We anticipate that the newly annotated AVMIT dataset will serve as a valuable resource for research and comparative experiments involving computational models and human participants, specifically when addressing research questions where audiovisual correspondence is of critical importance.
</details>
<details>
<summary>摘要</summary>
我们介绍了听视频时刻（AVMIT）数据集，这是一个大规模的听视频动作事件数据集。在一项广泛的注释任务中，11名参与者标注了MIT数据集中的3秒听视频示例。每个试验中，参与者评估了听视频动作事件是否存在，以及它是视频中最为突出的特征。该数据集包括57,177个听视频示例，每个示例由3名训练参与者独立地评估。从这些初始集合中，我们创建了一个精心挑选的测试集，包含16种动作类别，每个类别有60个视频示例（共960个视频）。我们还提供了2个预计算的听视频特征嵌入，使用VGGish/YamNet для音频数据和VGG16/EfficientNetB0 для视频数据，从而降低了听视频DNN研究的门槛。我们利用AVMIT注释和特征嵌入的优势来提高听视频事件认识性能。我们使用6个回归神经网络（RNN）在AVMIT听视频事件或MIT多模态无关事件上训练，然后测试在我们的听视频测试集上。在所有RNN中，测试准确率上升了2.71-5.94%，即使训练数据量只有多样化三倍。我们预计AVMIT数据集会成为研究计算模型和人类参与者之间的重要资源，特别是在研究问题中，听视频协调是关键性的。
</details></li>
</ul>
<hr>
<h2 id="Variational-optimization-of-the-amplitude-of-neural-network-quantum-many-body-ground-states"><a href="#Variational-optimization-of-the-amplitude-of-neural-network-quantum-many-body-ground-states" class="headerlink" title="Variational optimization of the amplitude of neural-network quantum many-body ground states"></a>Variational optimization of the amplitude of neural-network quantum many-body ground states</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09664">http://arxiv.org/abs/2308.09664</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jia-Qi Wang, Rong-Qiang He, Zhong-Yi Lu</li>
<li>for: 这个论文目的是找到量子多体系统的凝固态基态，并使用深度学习技术来优化。</li>
<li>methods: 这个论文使用了一种新的方法，即将量子多体系统的变量分解成一个实数valued amplitude神经网络和一个固定的Sign结构，然后优化 amplitude网络。</li>
<li>results: 这个论文在三个典型的量子多体系统上进行了测试，并 obtainted了较低的基态能量，与传统的变量 Monte Carlo（VMC）方法和density matrix renormalization group（DMRG）方法相比。而对于受挫的Heisenberg $J_1$-$J_2$模型，这个论文的结果比文献中的复数valued CNN更好，这表明了 sign structure of complex-valued NQS 难以优化。<details>
<summary>Abstract</summary>
Neural-network quantum states (NQSs), variationally optimized by combining traditional methods and deep learning techniques, is a new way to find quantum many-body ground states and gradually becomes a competitor of traditional variational methods. However, there are still some difficulties in the optimization of NQSs, such as local minima, slow convergence, and sign structure optimization. Here, we split a quantum many-body variational wave function into a multiplication of a real-valued amplitude neural network and a sign structure, and focus on the optimization of the amplitude network while keeping the sign structure fixed. The amplitude network is a convolutional neural network (CNN) with residual blocks, namely a ResNet. Our method is tested on three typical quantum many-body systems. The obtained ground state energies are lower than or comparable to those from traditional variational Monte Carlo (VMC) methods and density matrix renormalization group (DMRG). Surprisingly, for the frustrated Heisenberg $J_1$-$J_2$ model, our results are better than those of the complex-valued CNN in the literature, implying that the sign structure of the complex-valued NQS is difficult to be optimized. We will study the optimization of the sign structure of NQSs in the future.
</details>
<details>
<summary>摘要</summary>
新型神经网络量子状态（NQS），通过结合传统方法和深度学习技术进行变分优化，成为了找寻量子多体底态的新方法，但是仍有一些优化困难，如本地最小值、慢速收敛和正负结构优化。我们将量子多体变量波函数分解为一个实值神经网络幂与一个固定的正负结构，并将焦点放在幂网络的优化上，保持正负结构不变。我们的方法使用了卷积神经网络（CNN） WITH residual块，即ResNet。我们在三个典型量子多体系统上进行测试，获得的底态能量比或等于传统变量 Monte Carlo（VMC）和density matrix renormalization group（DMRG）方法所获得的值更低。 Surprisingly, 对于受挫的Heisenberg-$J_1$-$J_2$模型，我们的结果比文献中的复数值神经网络更好，这 imply That the sign structure of the complex-valued NQS is difficult to be optimized. 我们将在未来研究NQSs的正负结构优化。
</details></li>
</ul>
<hr>
<h2 id="GiGaMAE-Generalizable-Graph-Masked-Autoencoder-via-Collaborative-Latent-Space-Reconstruction"><a href="#GiGaMAE-Generalizable-Graph-Masked-Autoencoder-via-Collaborative-Latent-Space-Reconstruction" class="headerlink" title="GiGaMAE: Generalizable Graph Masked Autoencoder via Collaborative Latent Space Reconstruction"></a>GiGaMAE: Generalizable Graph Masked Autoencoder via Collaborative Latent Space Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09663">http://arxiv.org/abs/2308.09663</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sycny/gigamae">https://github.com/sycny/gigamae</a></li>
<li>paper_authors: Yucheng Shi, Yushun Dong, Qiaoyu Tan, Jundong Li, Ninghao Liu</li>
<li>for: 提高自适应学习模型对图数据的泛化能力</li>
<li>methods: 提出一种图自适应马斯克感知器框架 GiGaMAE，不同于现有的马斯克感知器，我们在这 paper 中提议将共同重建有用和集成的封闭嵌入作为重建目标，以捕捉更广泛的知识。</li>
<li>results: 对三个下游任务和七个数据集进行了广泛的实验，证明 GiGaMAE 在比较难的任务上表现出优于现有基elines。<details>
<summary>Abstract</summary>
Self-supervised learning with masked autoencoders has recently gained popularity for its ability to produce effective image or textual representations, which can be applied to various downstream tasks without retraining. However, we observe that the current masked autoencoder models lack good generalization ability on graph data. To tackle this issue, we propose a novel graph masked autoencoder framework called GiGaMAE. Different from existing masked autoencoders that learn node presentations by explicitly reconstructing the original graph components (e.g., features or edges), in this paper, we propose to collaboratively reconstruct informative and integrated latent embeddings. By considering embeddings encompassing graph topology and attribute information as reconstruction targets, our model could capture more generalized and comprehensive knowledge. Furthermore, we introduce a mutual information based reconstruction loss that enables the effective reconstruction of multiple targets. This learning objective allows us to differentiate between the exclusive knowledge learned from a single target and common knowledge shared by multiple targets. We evaluate our method on three downstream tasks with seven datasets as benchmarks. Extensive experiments demonstrate the superiority of GiGaMAE against state-of-the-art baselines. We hope our results will shed light on the design of foundation models on graph-structured data. Our code is available at: https://github.com/sycny/GiGaMAE.
</details>
<details>
<summary>摘要</summary>
自顾学学习掌握到掩码自适应器的能力，可以生成有效的图像或文本表示，可以应用于多个下游任务无需重新训练。然而，我们发现当前的掩码自适应器模型在图数据上的泛化能力不佳。为解决这个问题，我们提出了一个新的图自适应器框架 called GiGaMAE。与现有的掩码自适应器模型不同，我们在这篇论文中提议使用共同重构有用和完整的嵌入码。我们考虑嵌入码包括图格结构和属性信息作为重构目标，这使我们的模型可以捕捉更加普遍和全面的知识。此外，我们引入了互信息基于的重建损失，这使得我们的模型可以有效地重建多个目标。我们在三个下游任务上使用七个数据集进行评估。广泛的实验表明GiGaMAE比 estado-of-the-art 基线模型更高效。我们希望我们的结果可以指导基本模型的设计在图结构数据上。我们的代码可以在 GitHub 上找到：https://github.com/sycny/GiGaMAE。
</details></li>
</ul>
<hr>
<h2 id="Robust-Uncertainty-Quantification-using-Conformalised-Monte-Carlo-Prediction"><a href="#Robust-Uncertainty-Quantification-using-Conformalised-Monte-Carlo-Prediction" class="headerlink" title="Robust Uncertainty Quantification using Conformalised Monte Carlo Prediction"></a>Robust Uncertainty Quantification using Conformalised Monte Carlo Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09647">http://arxiv.org/abs/2308.09647</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/team-daniel/mc-cp">https://github.com/team-daniel/mc-cp</a></li>
<li>paper_authors: Daniel Bethell, Simos Gerasimou, Radu Calinescu</li>
<li>for: 这篇研究旨在提供一个新的深度学习模型评估方法，以提高深度学习模型在安全敏感应用中的可靠性。</li>
<li>methods: 这篇研究使用了一种新的混合MC-CP方法，它结合了适应MC dropout方法和整构预测方法，以提高深度学习模型的不确定性评估。</li>
<li>results: 经过了严格的实验评估，这篇研究发现MC-CP方法可以实现深度学习模型的更好的可靠性和精度，并且可以轻松地添加到现有的模型中，使其更加简单地应用。<details>
<summary>Abstract</summary>
Deploying deep learning models in safety-critical applications remains a very challenging task, mandating the provision of assurances for the dependable operation of these models. Uncertainty quantification (UQ) methods estimate the model's confidence per prediction, informing decision-making by considering the effect of randomness and model misspecification. Despite the advances of state-of-the-art UQ methods, they are computationally expensive or produce conservative prediction sets/intervals. We introduce MC-CP, a novel hybrid UQ method that combines a new adaptive Monte Carlo (MC) dropout method with conformal prediction (CP). MC-CP adaptively modulates the traditional MC dropout at runtime to save memory and computation resources, enabling predictions to be consumed by CP, yielding robust prediction sets/intervals. Throughout comprehensive experiments, we show that MC-CP delivers significant improvements over advanced UQ methods, like MC dropout, RAPS and CQR, both in classification and regression benchmarks. MC-CP can be easily added to existing models, making its deployment simple.
</details>
<details>
<summary>摘要</summary>
部署深度学习模型在安全关键应用中仍然是一项非常具有挑战性的任务，需要提供对模型可靠性的保证。不约束量评估（UQ）方法可以估算模型每个预测的可信度，为决策做出考虑，考虑随机性和模型误差的效果。despite the advances of state-of-the-art UQ methods, they are computationally expensive or produce conservative prediction sets/intervals. 我们介绍MC-CP，一种新的混合MC dropout和CP（确定预测）方法。MC-CP在运行时动态调整传统MC dropout，以避免过度使用内存和计算资源，使预测可以被CP所使用，生成Robust预测集/区间。通过广泛的实验，我们表明MC-CP在分类和回归 benchmark上都能够实现显著改进，比如MC dropout、RAPS和CQR。MC-CP可以轻松地添加到现有模型中，使其部署简单。
</details></li>
</ul>
<hr>
<h2 id="biquality-learn-a-Python-library-for-Biquality-Learning"><a href="#biquality-learn-a-Python-library-for-Biquality-Learning" class="headerlink" title="biquality-learn: a Python library for Biquality Learning"></a>biquality-learn: a Python library for Biquality Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09643">http://arxiv.org/abs/2308.09643</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/biquality-learn/biquality-learn">https://github.com/biquality-learn/biquality-learn</a></li>
<li>paper_authors: Pierre Nodet, Vincent Lemaire, Alexis Bondu, Antoine Cornuéjols</li>
<li>For: The paper is written for practitioners and researchers who need to handle weak supervision and dataset shifts in machine learning, and who want to use a Python library for Biquality Learning.* Methods: The paper proposes a machine learning framework called Biquality Learning, which can handle multiple weaknesses of supervision and dataset shifts without assumptions on their nature and level. The proposed library, biquality-learn, provides an intuitive and consistent API for learning machine learning models from biquality data.* Results: The paper does not present specific results, but rather proposes a new machine learning framework and library for handling weak supervision and dataset shifts. The proposed library, biquality-learn, is designed to be accessible and easy to use for everyone, and enables researchers to experiment in a reproducible way on biquality data.<details>
<summary>Abstract</summary>
The democratization of Data Mining has been widely successful thanks in part to powerful and easy-to-use Machine Learning libraries. These libraries have been particularly tailored to tackle Supervised Learning. However, strong supervision signals are scarce in practice, and practitioners must resort to weak supervision. In addition to weaknesses of supervision, dataset shifts are another kind of phenomenon that occurs when deploying machine learning models in the real world. That is why Biquality Learning has been proposed as a machine learning framework to design algorithms capable of handling multiple weaknesses of supervision and dataset shifts without assumptions on their nature and level by relying on the availability of a small trusted dataset composed of cleanly labeled and representative samples. Thus we propose biquality-learn: a Python library for Biquality Learning with an intuitive and consistent API to learn machine learning models from biquality data, with well-proven algorithms, accessible and easy to use for everyone, and enabling researchers to experiment in a reproducible way on biquality data.
</details>
<details>
<summary>摘要</summary>
“数据挖掘的民主化得到了广泛的成功，很大的准确是归功于强大且易于使用的机器学习库。这些库具有特别地针对超级vised学习。然而，实际中强制监督信号强度很弱，实践者必须采用弱监督。此外，在部署机器学习模型时，数据Shift是一种常见的现象。因此，我们提出了Biquality学习框架，设计用于处理多种弱监督和数据Shift的算法，不假设其性质和水平。Biquality学习库提供了一个小型可信的数据集，包含清晰标注和代表性样本，并提供了一个intuitive和一致的API，让人们轻松地学习机器学习模型从biquality数据，并且具有证明的算法、易于使用和可重复地进行研究。”Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know and I can provide that as well.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/19/cs.LG_2023_08_19/" data-id="clm0t8e0i0076v788cbchcgaz" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_08_19" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/19/cs.SD_2023_08_19/" class="article-date">
  <time datetime="2023-08-18T16:00:00.000Z" itemprop="datePublished">2023-08-19</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/19/cs.SD_2023_08_19/">cs.SD - 2023-08-19 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Spatial-Reconstructed-Local-Attention-Res2Net-with-F0-Subband-for-Fake-Speech-Detection"><a href="#Spatial-Reconstructed-Local-Attention-Res2Net-with-F0-Subband-for-Fake-Speech-Detection" class="headerlink" title="Spatial Reconstructed Local Attention Res2Net with F0 Subband for Fake Speech Detection"></a>Spatial Reconstructed Local Attention Res2Net with F0 Subband for Fake Speech Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09944">http://arxiv.org/abs/2308.09944</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cunhang Fan, Jun Xue, Jianhua Tao, Jiangyan Yi, Chenglong Wang, Chengshi Zheng, Zhao Lv</li>
<li>for: 本研究旨在提高假语音识别（FSD） task 的性能，通过提出一种新的F0子带和一种新的SR-LA Res2Net网络模型。</li>
<li>methods: 本研究使用了一种名为SR-LA Res2Net的新网络模型，其包括一个Res2Net底层网络和一个地域重建机制，以获取多尺度信息并避免损失重要信息。此外，本研究还使用了一种本地注意力机制，使模型更加注重F0子带的本地信息。</li>
<li>results: 根据ASVspoof 2019 LA dataset的实验结果，我们的提议方法实现了EER值为0.47%和min t-DCF值为0.0159，与其他单个系统相比，实现了状态的杰出性。<details>
<summary>Abstract</summary>
The rhythm of synthetic speech is usually too smooth, which causes that the fundamental frequency (F0) of synthetic speech is significantly different from that of real speech. It is expected that the F0 feature contains the discriminative information for the fake speech detection (FSD) task. In this paper, we propose a novel F0 subband for FSD. In addition, to effectively model the F0 subband so as to improve the performance of FSD, the spatial reconstructed local attention Res2Net (SR-LA Res2Net) is proposed. Specifically, Res2Net is used as a backbone network to obtain multiscale information, and enhanced with a spatial reconstruction mechanism to avoid losing important information when the channel group is constantly superimposed. In addition, local attention is designed to make the model focus on the local information of the F0 subband. Experimental results on the ASVspoof 2019 LA dataset show that our proposed method obtains an equal error rate (EER) of 0.47% and a minimum tandem detection cost function (min t-DCF) of 0.0159, achieving the state-of-the-art performance among all of the single systems.
</details>
<details>
<summary>摘要</summary>
《人工语音的节奏通常太平滑，导致人工语音的基本频率（F0）与真实语音的F0有所不同。这些F0特征包含潜在的假语音检测（FSD）任务中的 дискリมิنatif信息。在本文中，我们提出了一种新的F0子带 для FSD。此外，为了有效地模型F0子带，以提高FSD性能，我们提出了空间重建本地注意力Res2Net（SR-LA Res2Net）。具体来说，Res2Net被用作幕后网络，以获取多尺度信息，并在执行常数抽象操作时增加空间重建机制，以避免损失重要信息。此外，本地注意力被设计来使模型关注F0子带的本地信息。实验结果表明，我们提出的方法在ASVspoof 2019 LA数据集上获得了EER值为0.47%和min t-DCF值为0.0159，与所有单个系统中的性能相当。》Note: Simplified Chinese is a romanization of Chinese, and the translation is based on the standardized pronunciation of Simplified Chinese. The actual pronunciation may vary depending on the speaker's accent and intonation.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/19/cs.SD_2023_08_19/" data-id="clm0t8e1d00aev7882nnz4v2x" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/4/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/">6</a><a class="page-number" href="/page/7/">7</a><span class="space">&hellip;</span><a class="page-number" href="/page/29/">29</a><a class="extend next" rel="next" href="/page/6/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">60</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">56</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">112</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
