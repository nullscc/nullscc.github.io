
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/page/16/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-eess.AS_2023_07_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/05/eess.AS_2023_07_05/" class="article-date">
  <time datetime="2023-07-04T16:00:00.000Z" itemprop="datePublished">2023-07-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/05/eess.AS_2023_07_05/">eess.AS - 2023-07-05 22:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Online-Hybrid-CTC-Attention-End-to-End-Automatic-Speech-Recognition-Architecture"><a href="#Online-Hybrid-CTC-Attention-End-to-End-Automatic-Speech-Recognition-Architecture" class="headerlink" title="Online Hybrid CTC&#x2F;Attention End-to-End Automatic Speech Recognition Architecture"></a>Online Hybrid CTC&#x2F;Attention End-to-End Automatic Speech Recognition Architecture</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02351">http://arxiv.org/abs/2307.02351</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoran Miao, Gaofeng Cheng, Pengyuan Zhang, Yonghong Yan</li>
<li>for: This paper proposes an online hybrid CTC&#x2F;attention end-to-end ASR architecture for real-time speech recognition.</li>
<li>methods: The proposed architecture uses stable monotonic chunk-wise attention (sMoChA) to streamline global attention, truncated CTC (T-CTC) prefix score calculation, and dynamic waiting joint decoding (DWJD) algorithm for online prediction.</li>
<li>results: Compared with the offline CTC&#x2F;attention model, the proposed online CTC&#x2F;attention model improves the real-time factor in human-computer interaction services while maintaining moderate performance. This is the first full-stack online solution for CTC&#x2F;attention end-to-end ASR architecture.<details>
<summary>Abstract</summary>
Recently, there has been increasing progress in end-to-end automatic speech recognition (ASR) architecture, which transcribes speech to text without any pre-trained alignments. One popular end-to-end approach is the hybrid Connectionist Temporal Classification (CTC) and attention (CTC/attention) based ASR architecture. However, how to deploy hybrid CTC/attention systems for online speech recognition is still a non-trivial problem. This article describes our proposed online hybrid CTC/attention end-to-end ASR architecture, which replaces all the offline components of conventional CTC/attention ASR architecture with their corresponding streaming components. Firstly, we propose stable monotonic chunk-wise attention (sMoChA) to stream the conventional global attention, and further propose monotonic truncated attention (MTA) to simplify sMoChA and solve the training-and-decoding mismatch problem of sMoChA. Secondly, we propose truncated CTC (T-CTC) prefix score to stream CTC prefix score calculation. Thirdly, we design dynamic waiting joint decoding (DWJD) algorithm to dynamically collect the predictions of CTC and attention in an online manner. Finally, we use latency-controlled bidirectional long short-term memory (LC-BLSTM) to stream the widely-used offline bidirectional encoder network. Experiments with LibriSpeech English and HKUST Mandarin tasks demonstrate that, compared with the offline CTC/attention model, our proposed online CTC/attention model improves the real time factor in human-computer interaction services and maintains its performance with moderate degradation. To the best of our knowledge, this is the first work to provide the full-stack online solution for CTC/attention end-to-end ASR architecture.
</details>
<details>
<summary>摘要</summary>
近年来，endo-to-end自动语音识别（ASR）建筑有了显著的进步，这种语音识别模型不需要预先训练的Alignment。一种popular的endo-to-end方法是hybrid Connectionist Temporal Classification（CTC）和注意（CTC/注意）基于ASR建筑。然而，如何在线部署hybrid CTC/注意系统仍然是一个非常困难的问题。这篇文章描述了我们提议的在线hybrid CTC/注意末端ASR建筑，该建筑将所有的Offline组件替换为其相应的流动组件。首先，我们提出了稳定的均衡块级注意（sMoChA），以流动 conventiomal global注意，并提出了均衡 truncated attention（MTA），以解决sMoChA的训练和解码匹配问题。其次，我们提出了truncated CTC（T-CTC）前缀分数，以流动 CTC前缀分数计算。最后，我们设计了动态等待联合解码（DWJD）算法，以在线收集CTC和注意的预测。我们还使用了时钟控制的 bidirectional long short-term memory（LC-BLSTM），以流动 widely used offline bidirectional encoder network。实验表明，相比于Offline CTC/注意模型，我们提议的在线 CTC/注意模型在人机交互服务中提高了实时因素，并保持了其性能，虽有一定的衰减。到目前为止，这是首次提供了全栈在线解决CTC/注意末端ASR建筑的工作。
</details></li>
</ul>
<hr>
<h2 id="Differentially-Private-Adversarial-Auto-Encoder-to-Protect-Gender-in-Voice-Biometrics"><a href="#Differentially-Private-Adversarial-Auto-Encoder-to-Protect-Gender-in-Voice-Biometrics" class="headerlink" title="Differentially Private Adversarial Auto-Encoder to Protect Gender in Voice Biometrics"></a>Differentially Private Adversarial Auto-Encoder to Protect Gender in Voice Biometrics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02135">http://arxiv.org/abs/2307.02135</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oubaïda Chouchane, Michele Panariello, Oualid Zari, Ismet Kerenciler, Imen Chihaoui, Massimiliano Todisco, Melek Önen</li>
<li>for: 隐藏 gender 信息以保护个人隐私，同时保持 speaker 识别效果。</li>
<li>methods: 使用 Adversarial Auto-Encoder approach，对 gender 信息进行隐藏，并通过 Laplace 机制实现 differential privacy 保障。</li>
<li>results: 在 VoxCeleb 数据集上，可以成功隐藏 speaker 的 gender 信息，同时保持 speaker 识别效果，并可以根据需要调整 Laplace 噪声的强度来选择 Privacy 和 Utility 之间的平衡。<details>
<summary>Abstract</summary>
Over the last decade, the use of Automatic Speaker Verification (ASV) systems has become increasingly widespread in response to the growing need for secure and efficient identity verification methods. The voice data encompasses a wealth of personal information, which includes but is not limited to gender, age, health condition, stress levels, and geographical and socio-cultural origins. These attributes, known as soft biometrics, are private and the user may wish to keep them confidential. However, with the advancement of machine learning algorithms, soft biometrics can be inferred automatically, creating the potential for unauthorized use. As such, it is crucial to ensure the protection of these personal data that are inherent within the voice while retaining the utility of identity recognition. In this paper, we present an adversarial Auto-Encoder--based approach to hide gender-related information in speaker embeddings, while preserving their effectiveness for speaker verification. We use an adversarial procedure against a gender classifier and incorporate a layer based on the Laplace mechanism into the Auto-Encoder architecture. This layer adds Laplace noise for more robust gender concealment and ensures differential privacy guarantees during inference for the output speaker embeddings. Experiments conducted on the VoxCeleb dataset demonstrate that speaker verification tasks can be effectively carried out while concealing speaker gender and ensuring differential privacy guarantees; moreover, the intensity of the Laplace noise can be tuned to select the desired trade-off between privacy and utility.
</details>
<details>
<summary>摘要</summary>
过去十年，自动说话验证（ASV）系统的使用越来越普遍，以应对安全和高效的身份验证方法的增长需求。语音数据包含大量个人信息，包括但不限于性别、年龄、健康状况、压力水平和地域和文化背景。这些属性被称为软生物метrics，用户可能希望保持隐私。然而，通过机器学习算法的提高，软生物метrics可以被推断出来，从而创造不当使用的可能性。因此，保护这些个人数据，并在保留身份识别的同时确保隐私，是极为重要的。在这篇论文中，我们提出了一种利用对抗学习掩蔽 gender 信息的自动编码器方法，保持 speaker 嵌入的有效性，同时确保隐私。我们通过对 gender 分类器进行对抗程序，并在 Auto-Encoder 架构中添加基于 Laplace 机制的层。这层在推断过程中添加 Laplace 噪声，以确保在输出 speaker 嵌入时的隐私保障。在 VoxCeleb 数据集上进行的实验表明，可以在掩蔽 speaker 的 gender 信息和确保隐私保障的同时进行有效的 speaker 验证任务。此外，可以根据需要调整 Laplace 噪声的强度，选择适当的隐私和功能之间的平衡。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-multilingual-transfer-for-unsupervised-semantic-acoustic-word-embeddings"><a href="#Leveraging-multilingual-transfer-for-unsupervised-semantic-acoustic-word-embeddings" class="headerlink" title="Leveraging multilingual transfer for unsupervised semantic acoustic word embeddings"></a>Leveraging multilingual transfer for unsupervised semantic acoustic word embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02083">http://arxiv.org/abs/2307.02083</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christiaan Jacobs, Herman Kamper</li>
<li>for: 这paper的目的是研究语音词嵌入（AWE），并在这个基础上构建语音词嵌入模型，以实现语音词的语义表示。</li>
<li>methods: 这paper使用了一种多语言基础模型，将语音词分类为不同的语音类别，然后使用这些类别来生成语音词嵌入。</li>
<li>results: 这paper的实验结果表明，使用这种多语言基础模型可以实现语音词嵌入的语义表示，并且在语音词相似性任务中表现出色。此外，这paper还实现了语音词 Query-by-Example 搜索的功能。<details>
<summary>Abstract</summary>
Acoustic word embeddings (AWEs) are fixed-dimensional vector representations of speech segments that encode phonetic content so that different realisations of the same word have similar embeddings. In this paper we explore semantic AWE modelling. These AWEs should not only capture phonetics but also the meaning of a word (similar to textual word embeddings). We consider the scenario where we only have untranscribed speech in a target language. We introduce a number of strategies leveraging a pre-trained multilingual AWE model -- a phonetic AWE model trained on labelled data from multiple languages excluding the target. Our best semantic AWE approach involves clustering word segments using the multilingual AWE model, deriving soft pseudo-word labels from the cluster centroids, and then training a Skipgram-like model on the soft vectors. In an intrinsic word similarity task measuring semantics, this multilingual transfer approach outperforms all previous semantic AWE methods. We also show -- for the first time -- that AWEs can be used for downstream semantic query-by-example search.
</details>
<details>
<summary>摘要</summary>
听音字嵌入（AWEs）是指将speech segmentFixed-dimensional vector representation的方法，以便不同的实现方式中的同一个词有相似的嵌入。在这篇论文中，我们探索 semantic AWE 模型。这些 AWEs 不仅应 capture phonetics, but also the meaning of a word, similar to textual word embeddings。我们考虑了target language only have untranscribed speech的场景。我们提出了一些使用预训练的多语言 AWE 模型（包括目标语言）的策略。我们的最佳semantic AWE方法是 clustering word segments using the multilingual AWE model, deriving soft pseudo-word labels from the cluster centroids, and then training a Skipgram-like model on the soft vectors。在内在词 Similarity task中，这种多语言传递方法超过了所有前一个semantic AWE方法。我们还表明了，AWEs可以用于下游semantic query-by-example search。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://example.com/2023/07/05/eess.AS_2023_07_05/" data-id="cllsjvzde005sf588htimc4pm" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_07_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/05/eess.IV_2023_07_05/" class="article-date">
  <time datetime="2023-07-04T16:00:00.000Z" itemprop="datePublished">2023-07-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/05/eess.IV_2023_07_05/">eess.IV - 2023-07-05 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Dual-Arbitrary-Scale-Super-Resolution-for-Multi-Contrast-MRI"><a href="#Dual-Arbitrary-Scale-Super-Resolution-for-Multi-Contrast-MRI" class="headerlink" title="Dual Arbitrary Scale Super-Resolution for Multi-Contrast MRI"></a>Dual Arbitrary Scale Super-Resolution for Multi-Contrast MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02334">http://arxiv.org/abs/2307.02334</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jmzhang79/dual-arbnet">https://github.com/jmzhang79/dual-arbnet</a></li>
<li>paper_authors: Jiamiao Zhang, Yichen Chi, Jun Lyu, Wenming Yang, Yapeng Tian</li>
<li>for: 这篇论文旨在提高医疗成像领域中的磁共振成像（MRI）像素化，以提高医生在诊断和治疗时的可见度。</li>
<li>methods: 本研究使用了一种基于神经网络的多标示磁共振超解析（SR）重建方法，称为Dual-ArbNet，它可以在不同的标示模式下进行SR重建，并且可以处理不同的像素比例和分辨率。</li>
<li>results: 实验结果显示，Dual-ArbNet方法在两个公共MRI数据集上具有较高的SR性能，并且可以在不同的像素比例和分辨率下进行SR重建。此外，该方法还可以运用到临床实践中。<details>
<summary>Abstract</summary>
Limited by imaging systems, the reconstruction of Magnetic Resonance Imaging (MRI) images from partial measurement is essential to medical imaging research. Benefiting from the diverse and complementary information of multi-contrast MR images in different imaging modalities, multi-contrast Super-Resolution (SR) reconstruction is promising to yield SR images with higher quality. In the medical scenario, to fully visualize the lesion, radiologists are accustomed to zooming the MR images at arbitrary scales rather than using a fixed scale, as used by most MRI SR methods. In addition, existing multi-contrast MRI SR methods often require a fixed resolution for the reference image, which makes acquiring reference images difficult and imposes limitations on arbitrary scale SR tasks. To address these issues, we proposed an implicit neural representations based dual-arbitrary multi-contrast MRI super-resolution method, called Dual-ArbNet. First, we decouple the resolution of the target and reference images by a feature encoder, enabling the network to input target and reference images at arbitrary scales. Then, an implicit fusion decoder fuses the multi-contrast features and uses an Implicit Decoding Function~(IDF) to obtain the final MRI SR results. Furthermore, we introduce a curriculum learning strategy to train our network, which improves the generalization and performance of our Dual-ArbNet. Extensive experiments in two public MRI datasets demonstrate that our method outperforms state-of-the-art approaches under different scale factors and has great potential in clinical practice.
</details>
<details>
<summary>摘要</summary>
限于快照系统，重建快照成像（MRI）图像从部分测量是医学成像研究中的关键。利用不同和补充的多比特MR成像模式的多比特超分辨（SR）重建可以获得更高质量的SR图像。在医疗场景下，为了全面显示肿瘤，辐射医生通常会在自定义的比例下缩放MR图像，而不是使用固定比例，这与大多数MRI SR方法不同。此外，现有的多比特MRI SR方法通常需要固定的参参图像分辨率，这使得获得参考图像困难，并对自定义比例SR任务带来限制。为解决这些问题，我们提出了基于卷积神经表示的双自由多比特MRI超分辨方法，称为Dual-ArbNet。首先，我们将目标和参考图像的分辨率解耦通过特征编码器，使网络可以输入自定义的目标和参考图像。然后，我们使用卷积叠加器将多比特特征进行卷积叠加，并使用偏函数IDF获取最终的MRI SR结果。此外，我们引入了课程学习策略来训练我们的网络，这有助于提高我们Dual-ArbNet的一般化和性能。广泛的实验在两个公共MRI数据集上表明，我们的方法在不同的比例因子下表现出色，有很好的潜在应用前景。
</details></li>
</ul>
<hr>
<h2 id="Joint-Hierarchical-Priors-and-Adaptive-Spatial-Resolution-for-Efficient-Neural-Image-Compression"><a href="#Joint-Hierarchical-Priors-and-Adaptive-Spatial-Resolution-for-Efficient-Neural-Image-Compression" class="headerlink" title="Joint Hierarchical Priors and Adaptive Spatial Resolution for Efficient Neural Image Compression"></a>Joint Hierarchical Priors and Adaptive Spatial Resolution for Efficient Neural Image Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02273">http://arxiv.org/abs/2307.02273</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmed Ghorbel, Wassim Hamidouche, Luce Morin</li>
<li>for: 这篇论文是关于神经网络图像压缩（NIC）的研究，旨在提高NIC的性能，并且希望通过对Tranformer-based transform coding框架进行改进，以提高图像压缩的效率和质量。</li>
<li>methods: 本文使用Tranformer-based channel-wise auto-regressive prior模型来提高SwinT-ChARM的性能，并且添加了一个可学习的缩放模块来更好地提取更紧凑的缺失代码。</li>
<li>results: 实验结果表明，提出的框架可以在各种测试集上显著提高NIC的质量和效率，并且在与VVC参考编码器（VTM-18.0）和SwinT-ChARM神经编码器进行比较时，具有更好的质量和效率。<details>
<summary>Abstract</summary>
Recently, the performance of neural image compression (NIC) has steadily improved thanks to the last line of study, reaching or outperforming state-of-the-art conventional codecs. Despite significant progress, current NIC methods still rely on ConvNet-based entropy coding, limited in modeling long-range dependencies due to their local connectivity and the increasing number of architectural biases and priors, resulting in complex underperforming models with high decoding latency. Motivated by the efficiency investigation of the Tranformer-based transform coding framework, namely SwinT-ChARM, we propose to enhance the latter, as first, with a more straightforward yet effective Tranformer-based channel-wise auto-regressive prior model, resulting in an absolute image compression transformer (ICT). Through the proposed ICT, we can capture both global and local contexts from the latent representations and better parameterize the distribution of the quantized latents. Further, we leverage a learnable scaling module with a sandwich ConvNeXt-based pre-/post-processor to accurately extract more compact latent codes while reconstructing higher-quality images. Extensive experimental results on benchmark datasets showed that the proposed framework significantly improves the trade-off between coding efficiency and decoder complexity over the versatile video coding (VVC) reference encoder (VTM-18.0) and the neural codec SwinT-ChARM. Moreover, we provide model scaling studies to verify the computational efficiency of our approach and conduct several objective and subjective analyses to bring to the fore the performance gap between the adaptive image compression transformer (AICT) and the neural codec SwinT-ChARM.
</details>
<details>
<summary>摘要</summary>
近些年，神经网络图像压缩（NIC）的性能已经逐渐提高，达到或超越传统编码器的状态元。 DESPITE 这些进步，当前的 NIC 方法仍然基于 ConvNet 来实现 entropy coding，受到本地连接性的限制，以及逐渐增加的建筑学偏好和先验，导致复杂的不够表现的模型和高解码延迟。 被Transformer 基于 transform coding 框架的 SwinT-ChARM 的效率调查所驱动，我们提议使用更直观而有效的 Tranformer 基于通道 wise auto-regressive prior 模型，以提高后者。通过我们的提议的 ICT，我们可以从 latent 表示中捕捉全局和局部上下文，更好地参数化归一化的量化 latent。此外，我们利用一个可学习的缩放模块，并在 ConvNeXt 基于的预处理/后处理器中使用它来准确地提取更紧凑的 latent 代码，并在重建更高质量的图像。对于一系列的 benchmark 数据集，我们进行了广泛的实验研究，并证明了我们的框架可以显著提高对 coding 效率和解码器复杂度的质量权衡。此外，我们还提供了模型缩放研究，以证明我们的方法的计算效率。最后，我们进行了一些对象和主观分析，以强调 AICT 与 SwinT-ChARM 之间的性能差距。
</details></li>
</ul>
<hr>
<h2 id="Direct-segmentation-of-brain-white-matter-tracts-in-diffusion-MRI"><a href="#Direct-segmentation-of-brain-white-matter-tracts-in-diffusion-MRI" class="headerlink" title="Direct segmentation of brain white matter tracts in diffusion MRI"></a>Direct segmentation of brain white matter tracts in diffusion MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02223">http://arxiv.org/abs/2307.02223</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hamza Kebiri, Ali Gholipour, Meritxell Bach Cuadra, Davood Karimi<br>for:白 matter  tracts 的 segmentation，即 brain 中各个区域之间的连接组织。methods:使用 deep learning 方法，直接从 diffusion MRI 数据中提取 white matter tracts。results: segmentation 精度与现有方法相当（mean Dice Similarity Coefficient 为 0.826），并且具有更高的普适性，可应用于低样本量的临床研究和不同的数据获取协议。<details>
<summary>Abstract</summary>
The brain white matter consists of a set of tracts that connect distinct regions of the brain. Segmentation of these tracts is often needed for clinical and research studies. Diffusion-weighted MRI offers unique contrast to delineate these tracts. However, existing segmentation methods rely on intermediate computations such as tractography or estimation of fiber orientation density. These intermediate computations, in turn, entail complex computations that can result in unnecessary errors. Moreover, these intermediate computations often require dense multi-shell measurements that are unavailable in many clinical and research applications. As a result, current methods suffer from low accuracy and poor generalizability. Here, we propose a new deep learning method that segments these tracts directly from the diffusion MRI data, thereby sidestepping the intermediate computation errors. Our experiments show that this method can achieve segmentation accuracy that is on par with the state of the art methods (mean Dice Similarity Coefficient of 0.826). Compared with the state of the art, our method offers far superior generalizability to undersampled data that are typical of clinical studies and to data obtained with different acquisition protocols. Moreover, we propose a new method for detecting inaccurate segmentations and show that it is more accurate than standard methods that are based on estimation uncertainty quantification. The new methods can serve many critically important clinical and scientific applications that require accurate and reliable non-invasive segmentation of white matter tracts.
</details>
<details>
<summary>摘要</summary>
脑白atter包括一组通过不同脑区域的脑 tract， segmentation 这些 tract 常需要在临床和研究实验中进行。Diffusion-weighted MRI 提供了一个唯一的对比，以定义这些 tract。然而，现有的 segmentation 方法通常需要中间计算，如 tractography 或 fibre orientation density 的估计。这些中间计算可能会导致多余的错误，并且常常需要 dense multi-shell measurements，这些 measurements 在许多临床和研究应用中不可得。因此，现有的方法受到低精度和差异化的限制。在这里，我们提出了一种新的深度学习方法，可以直接从 diffusion MRI 数据中分割 white matter tract，并且避免中间计算的错误。我们的实验表明，这种方法可以达到与现有方法相同的 segmentation 精度（mean Dice Similarity Coefficient 0.826）。相比之下，我们的方法在不同的数据采样和数据采集协议下具有更好的普适性。此外，我们还提出了一种新的方法来检测不准确的分割，并证明它比标准的方法更加准确。这些新方法可以为许多重要的临床和科学应用提供准确和可靠的非侵入式 white matter tract 的分割。
</details></li>
</ul>
<hr>
<h2 id="Compound-Attention-and-Neighbor-Matching-Network-for-Multi-contrast-MRI-Super-resolution"><a href="#Compound-Attention-and-Neighbor-Matching-Network-for-Multi-contrast-MRI-Super-resolution" class="headerlink" title="Compound Attention and Neighbor Matching Network for Multi-contrast MRI Super-resolution"></a>Compound Attention and Neighbor Matching Network for Multi-contrast MRI Super-resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02148">http://arxiv.org/abs/2307.02148</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenxuan Chen, Sirui Wu, Shuai Wang, Zhongsen Li, Jia Yang, Huifeng Yao, Xiaomeng Li, Xiaolei Song</li>
<li>for: 这个论文旨在提出一种新的多模式磁共振成像超分辨（SR）网络架构，用于解决现有的SR方法在多模式磁共振成像中存在缺陷，如缺乏合适的参考特征和缺乏频率匹配。</li>
<li>methods: 该论文提出了一种基于自我注意力和邻居匹配的网络架构，称为CANM-Net，它使用复合自我注意力机制和邻居匹配模块来捕捉多模式磁共振成像中的相互依赖关系，并将参考特征和下落特征进行适应性匹配，以实现高质量的SR图像生成。</li>
<li>results: 该论文通过在IXI、fastMRI和实际扫描数据集上进行SR任务的实验，证明了CANM-Net在透彻和跨模式磁共振成像SR中具有优于现有方法的性能，并且在不当 registrations 的情况下仍然保持良好的表现，这表明其在临床应用中具有良好的潜力。<details>
<summary>Abstract</summary>
Multi-contrast magnetic resonance imaging (MRI) reflects information about human tissue from different perspectives and has many clinical applications. By utilizing the complementary information among different modalities, multi-contrast super-resolution (SR) of MRI can achieve better results than single-image super-resolution. However, existing methods of multi-contrast MRI SR have the following shortcomings that may limit their performance: First, existing methods either simply concatenate the reference and degraded features or exploit global feature-matching between them, which are unsuitable for multi-contrast MRI SR. Second, although many recent methods employ transformers to capture long-range dependencies in the spatial dimension, they neglect that self-attention in the channel dimension is also important for low-level vision tasks. To address these shortcomings, we proposed a novel network architecture with compound-attention and neighbor matching (CANM-Net) for multi-contrast MRI SR: The compound self-attention mechanism effectively captures the dependencies in both spatial and channel dimension; the neighborhood-based feature-matching modules are exploited to match degraded features and adjacent reference features and then fuse them to obtain the high-quality images. We conduct experiments of SR tasks on the IXI, fastMRI, and real-world scanning datasets. The CANM-Net outperforms state-of-the-art approaches in both retrospective and prospective experiments. Moreover, the robustness study in our work shows that the CANM-Net still achieves good performance when the reference and degraded images are imperfectly registered, proving good potential in clinical applications.
</details>
<details>
<summary>摘要</summary>
多模式磁共振成像（MRI）可以从不同角度获取人体组织信息，有广泛的临床应用。通过利用不同模式之间的共趋性信息，多模式超解析（SR）的MRI可以实现更好的结果，而存在的方法却有以下缺点：首先，现有方法可能会简单地 concatenate 参考和压缩特征，或者利用全局特征匹配，这些方法不适合多模式MRI SR。其次，虽然许多最新的方法使用 transformer 来捕捉空间维度的长距离依赖关系，但它们忽略了通道维度的自我注意力的重要性，这对低级视觉任务来说非常重要。为了解决这些缺点，我们提出了一种新的网络架构，即嵌入式自注意和邻居匹配网络（CANM-Net），用于多模式MRI SR：嵌入式自注意机制可以有效捕捉空间和通道维度之间的依赖关系；邻居特征匹配模块可以将压缩特征和相邻参考特征匹配并融合，以获得高质量的图像。我们在 IXI、fastMRI 和实际扫描数据集上进行 SR 任务的实验，CANM-Net 比 estado-of-the-art 方法在回顾和前瞻性实验中表现出色。此外，我们的 robustness 研究显示，CANM-Net 在参考和压缩图像不完美匹配时仍能保持良好的性能，这证明它在临床应用中具有良好的潜力。
</details></li>
</ul>
<hr>
<h2 id="A-Mini-Batch-Quasi-Newton-Proximal-Method-for-Constrained-Total-Variation-Nonlinear-Image-Reconstruction"><a href="#A-Mini-Batch-Quasi-Newton-Proximal-Method-for-Constrained-Total-Variation-Nonlinear-Image-Reconstruction" class="headerlink" title="A Mini-Batch Quasi-Newton Proximal Method for Constrained Total-Variation Nonlinear Image Reconstruction"></a>A Mini-Batch Quasi-Newton Proximal Method for Constrained Total-Variation Nonlinear Image Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02043">http://arxiv.org/abs/2307.02043</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tao Hong, Thanh-an Pham, Irad Yavneh, Michael Unser</li>
<li>for: 这篇论文是关于计算成像，使用精确的物理模型来实现高质量重建的。</li>
<li>methods: 本文提出了一种基于强化随机批处理的准确非线性物理模型的计算成像方法，即mini-batch quasi-Newton proximal方法（BQNPM）。</li>
<li>results: 本文通过对三维反射问题进行实验和实际数据测试，证明BQNPM比ASPMs更快速地 converges，并且可以在计算成像中实现高质量的重建。<details>
<summary>Abstract</summary>
Over the years, computational imaging with accurate nonlinear physical models has drawn considerable interest due to its ability to achieve high-quality reconstructions. However, such nonlinear models are computationally demanding. A popular choice for solving the corresponding inverse problems is accelerated stochastic proximal methods (ASPMs), with the caveat that each iteration is expensive. To overcome this issue, we propose a mini-batch quasi-Newton proximal method (BQNPM) tailored to image-reconstruction problems with total-variation regularization. It involves an efficient approach that computes a weighted proximal mapping at a cost similar to that of the proximal mapping in ASPMs. However, BQNPM requires fewer iterations than ASPMs to converge. We assess the performance of BQNPM on three-dimensional inverse-scattering problems with linear and nonlinear physical models. Our results on simulated and real data show the effectiveness and efficiency of BQNPM,
</details>
<details>
<summary>摘要</summary>
随着时间的推移，计算成像技术已经吸引了广泛的关注，因为它可以实现高质量的重建。然而，这些非线性模型在计算上具有挑战性。一种受欢迎的解决方案是加速随机邻域方法（ASPMs），但每个迭代都是贵夫。为了解决这个问题，我们提议一种基于图像重建问题的权重贝叶斯方法（BQNPM）。这种方法具有计算贝叶斯映射的效率，但需要 fewer than ASPMs 的迭代次数才能达到 convergence。我们对三维反射问题进行了线性和非线性物理模型的测试，结果表明 BQNPM 的效果和效率。
</details></li>
</ul>
<hr>
<h2 id="Joint-Recovery-of-T1-T2-and-Proton-Density-Maps-Using-a-Bayesian-Approach-with-Parameter-Estimation-and-Complementary-Undersampling-Patterns"><a href="#Joint-Recovery-of-T1-T2-and-Proton-Density-Maps-Using-a-Bayesian-Approach-with-Parameter-Estimation-and-Complementary-Undersampling-Patterns" class="headerlink" title="Joint Recovery of T1, T2* and Proton Density Maps Using a Bayesian Approach with Parameter Estimation and Complementary Undersampling Patterns"></a>Joint Recovery of T1, T2* and Proton Density Maps Using a Bayesian Approach with Parameter Estimation and Complementary Undersampling Patterns</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02015">http://arxiv.org/abs/2307.02015</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuai Huang, James J. Lah, Jason W. Allen, Deqiang Qiu<br>for:  This paper aims to improve the quality of quantitative MR images recovered from undersampled measurements by incorporating the signal model of the variable-flip-angle (VFA) multi-echo 3D gradient-echo (GRE) method into the reconstruction of $T_1$, $T_2^*$, and proton density (PD) maps.methods:  The proposed approach is based on a probabilistic Bayesian formulation of the recovery problem, and uses approximate message passing with built-in parameter estimation (AMP-PE) to jointly recover distribution parameters, VFA multi-echo images, and $T_1$, $T_2^*$, and PD maps without the need for hyperparameter tuning.results:  The proposed AMP-PE approach outperforms the state-of-the-art $l1$-norm minimization approach in terms of reconstruction performance, and adopting complementary undersampling patterns across different flip angles and&#x2F;or echo times yields the best performance for $T_2^*$ and proton density mappings.<details>
<summary>Abstract</summary>
Purpose: To improve the quality of quantitative MR images recovered from undersampled measurements, we incorporate the signal model of the variable-flip-angle (VFA) multi-echo 3D gradient-echo (GRE) method into the reconstruction of $T_1$, $T_2^*$ and proton density (PD) maps. Additionally, we investigate the use of complementary undersampling patterns to determine optimal undersampling schemes for quantitative MRI.   Theory: We propose a probabilistic Bayesian formulation of the recovery problem. Our proposed approach, approximate message passing with built-in parameter estimation (AMP-PE), enables the joint recovery of distribution parameters, VFA multi-echo images, and $T_1$, $T_2^*$, and PD maps without the need for hyperparameter tuning.   Methods: We conducted both retrospective and prospective undersampling to obtain Fourier measurements using variable-density and Poisson-disk patterns. We investigated a variety of undersampling schemes, adopting complementary patterns across different flip angles and/or echo times.   Results: AMP-PE adopts a joint recovery strategy, it outperforms the state-of-the-art $l1$-norm minimization approach that follows a decoupled recovery strategy. For $T_1$ mapping, employing fixed sampling patterns across different echo times produced the best performance. Whereas for $T_2^*$ and proton density mappings, using complementary sampling patterns across different flip angles yielded the best performance.   Conclusion: AMP-PE achieves better performance by combining information from both the MR signal model and the sparse prior on VFA multi-echo images. It is equipped with automatic and adaptive parameter estimation, and works naturally with the clinical prospective undersampling scheme.
</details>
<details>
<summary>摘要</summary>
目的：提高 Undersampled 测量中的量子 MR 图像质量，我们在 reconstruction 中 incorporate 变量扭矩（VFA）多echo 3D 梯阶 echo（GRE）方法的信号模型。此外，我们还 investigate 使用 complementary 抽象样本来确定最佳的 Undersampling 方案。理论：我们提出了一种 Bayesian 形式的回归问题。我们的提议方法为 approximate message passing with built-in parameter estimation（AMP-PE），它可以同时回归分布参数、VFA multi-echo 图像和 $T_1$, $T_2^*$ 和 proton density（PD）图像，无需进行hyperparameter 调整。方法：我们在 retrospective 和 prospectively 抽象到 obtain Fourier 测量。我们 investigate 了不同的抽象方案，包括 variable-density 和 Poisson-disk 模式。结果：AMP-PE 采用了联合回归策略，其表现更好于 state-of-the-art $l1$-norm 最小化方法，后者采用了解 Coupled 回归策略。对 $T_1$ 图像，使用 fixes 抽象模式 across 不同的 echo times 得到了最佳性能。而对 $T_2^*$ 和 proton density 图像，使用 complementary 抽象模式 across 不同的扭矩 angles 得到了最佳性能。结论：AMP-PE 通过结合 MR 信号模型和 VFA multi-echo 图像的稀热先验来提高量子 MR 图像的质量。它具有自动和适应参数估计，并可以自然地与临床的前向抽象 schemes 结合。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Spectral-Demosaicing-with-Lightweight-Spectral-Attention-Networks"><a href="#Unsupervised-Spectral-Demosaicing-with-Lightweight-Spectral-Attention-Networks" class="headerlink" title="Unsupervised Spectral Demosaicing with Lightweight Spectral Attention Networks"></a>Unsupervised Spectral Demosaicing with Lightweight Spectral Attention Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01990">http://arxiv.org/abs/2307.01990</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kai Feng, Yongqiang Zhao, Seong G. Kong, Haijin Zeng</li>
<li>for: 这 paper 的目的是提出一种基于深度学习的无监督 spectral demosaicing 技术，以便在实际图像中进行高质量的颜色彩度恢复。</li>
<li>methods: 该 paper 使用了一种无监督学习的架构，包括提出了一种 mosaic loss function、模型结构、变换策略以及 early stopping 策略，这些组成了一个完整的无监督 spectral demosaicing 框架。</li>
<li>results: 对于实际图像，该 paper 的方法能够更好地抑制空间扭曲、保持 spectral 准确性、稳定性和计算成本，并且在 synthetic 和实际数据上进行了广泛的实验，得到了更高的性能。<details>
<summary>Abstract</summary>
This paper presents a deep learning-based spectral demosaicing technique trained in an unsupervised manner. Many existing deep learning-based techniques relying on supervised learning with synthetic images, often underperform on real-world images especially when the number of spectral bands increases. According to the characteristics of the spectral mosaic image, this paper proposes a mosaic loss function, the corresponding model structure, a transformation strategy, and an early stopping strategy, which form a complete unsupervised spectral demosaicing framework. A challenge in real-world spectral demosaicing is inconsistency between the model parameters and the computational resources of the imager. We reduce the complexity and parameters of the spectral attention module by dividing the spectral attention tensor into spectral attention matrices in the spatial dimension and spectral attention vector in the channel dimension, which is more suitable for unsupervised framework. This paper also presents Mosaic25, a real 25-band hyperspectral mosaic image dataset of various objects, illuminations, and materials for benchmarking. Extensive experiments on synthetic and real-world datasets demonstrate that the proposed method outperforms conventional unsupervised methods in terms of spatial distortion suppression, spectral fidelity, robustness, and computational cost.
</details>
<details>
<summary>摘要</summary>
One of the challenges in real-world spectral demosaicing is the inconsistency between the model parameters and the computational resources of the imager. To address this, the paper reduces the complexity and parameters of the spectral attention module by dividing the spectral attention tensor into spectral attention matrices in the spatial dimension and spectral attention vectors in the channel dimension. This is more suitable for an unsupervised framework.The paper also presents Mosaic25, a real 25-band hyperspectral mosaic image dataset of various objects, illuminations, and materials for benchmarking. Extensive experiments on synthetic and real-world datasets demonstrate that the proposed method outperforms conventional unsupervised methods in terms of spatial distortion suppression, spectral fidelity, robustness, and computational cost.Here is the Simplified Chinese translation of the text:这篇论文提出了一种基于深度学习的spectral demosaicing技术，该技术在无监督的情况下训练。现有的深度学习基于的技术 часто采用监督学习 Synthetic 图像，在实际图像中表现不佳，特别是随着频谱带数的增加。根据频谱拼接图像的特点，这篇论文提出了一种拼接损失函数、相应的模型结构、转换策略和早stopping策略，这些组成了一个完整的无监督 spectral demosaicing 框架。实际频谱拼接中的一个挑战是模型参数和捕获设备的计算资源之间的不一致。这篇论文通过将频谱注意力矩阵分割成空间维度的频谱注意力矩阵和通道维度的频谱注意力向量，来降低模型的复杂性和参数数量。这更适合无监督的框架。此外，这篇论文还提供了Mosaic25，一个实际25个频谱带的卷积合成图像数据集，包括不同的物体、照明和材料，用于对比。对于实际和 sintetic 数据集进行了广泛的实验，结果表明，提出的方法在隐藏扰乱、频谱准确、稳定性和计算成本方面都有较好的表现。
</details></li>
</ul>
<hr>
<h2 id="ToothSegNet-Image-Degradation-meets-Tooth-Segmentation-in-CBCT-Images"><a href="#ToothSegNet-Image-Degradation-meets-Tooth-Segmentation-in-CBCT-Images" class="headerlink" title="ToothSegNet: Image Degradation meets Tooth Segmentation in CBCT Images"></a>ToothSegNet: Image Degradation meets Tooth Segmentation in CBCT Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01979">http://arxiv.org/abs/2307.01979</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaxiang Liu, Tianxiang Hu, Yang Feng, Wanghui Ding, Zuozhu Liu</li>
<li>for: constructing three-dimensional tooth models in computer-assisted orthodontics</li>
<li>methods: 使用ToothSegNet框架，通过生成降低图像的信息来训练分割模型，并使用通道维度的混合来减少Encoder和Decoder之间的语义差异，以及通过结构约束损失来精细调整预测的牙齿形态</li>
<li>results: 比前一代医疗图像分割方法更高精度的牙齿分割结果<details>
<summary>Abstract</summary>
In computer-assisted orthodontics, three-dimensional tooth models are required for many medical treatments. Tooth segmentation from cone-beam computed tomography (CBCT) images is a crucial step in constructing the models. However, CBCT image quality problems such as metal artifacts and blurring caused by shooting equipment and patients' dental conditions make the segmentation difficult. In this paper, we propose ToothSegNet, a new framework which acquaints the segmentation model with generated degraded images during training. ToothSegNet merges the information of high and low quality images from the designed degradation simulation module using channel-wise cross fusion to reduce the semantic gap between encoder and decoder, and also refines the shape of tooth prediction through a structural constraint loss. Experimental results suggest that ToothSegNet produces more precise segmentation and outperforms the state-of-the-art medical image segmentation methods.
</details>
<details>
<summary>摘要</summary>
在计算机协助orthodontics中，三维牙齿模型是许多医疗治疗的关键 step。然而，CBCT图像质量问题，如机器设备和病人的牙科条件所导致的锈损和模糊，使 segmentation 变得更加困难。在这篇论文中，我们提出了 ToothSegNet，一个新的框架，通过在训练过程中对生成的受损图像进行准备，使 segmentation 模型更加熟悉受损图像的特征。ToothSegNet 通过核心混合来减少编码器和解码器之间的semantic gap，并通过结构约束损失来精细调整牙齿预测的形态。实验结果表明，ToothSegNet 可以生成更加精准的 segmentation，并超越了当前医学影像 segmentation 方法的性能。
</details></li>
</ul>
<hr>
<h2 id="Millimeter-Wave-Reflectionless-Filters-Using-Advanced-Thin-Film-Fabrication"><a href="#Millimeter-Wave-Reflectionless-Filters-Using-Advanced-Thin-Film-Fabrication" class="headerlink" title="Millimeter-Wave Reflectionless Filters Using Advanced Thin-Film Fabrication"></a>Millimeter-Wave Reflectionless Filters Using Advanced Thin-Film Fabrication</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01914">http://arxiv.org/abs/2307.01914</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matthew Morgan, Seng Loo, Tod Boyd, Miho Hunter</li>
<li>for: developing millimeter-wave, lumped-element reflectionless filters</li>
<li>methods: using advanced thin-film fabrication process with better than 2 μm feature size and integrated elements such as SiN Metal-Insulator-Metal (MIM) capacitors, bridges, and TaN Thin-Film Resistors (TFRs)</li>
<li>results: achieved higher frequency implementation than ever beforeHere’s the same information in Simplified Chinese text:</li>
<li>for: 开发毫米波、堆叠元件反射 filters</li>
<li>methods: 利用高精度薄膜制造过程，实现更高频率实现</li>
<li>results: 实现了历史上最高频率实现<details>
<summary>Abstract</summary>
We report on the development of millimeter-wave, lumped-element reflectionless filters using an advanced thin-film fabrication process. Based on previously demonstrated circuit topologies capable of achieving 50{\Omega} impedance match at all frequencies, these circuits have been implemented at higher frequencies than ever before by leveraging a thin-film process with better than 2 {\mu}m feature size and integrated elements such as SiN Metal-Insulator-Metal (MIM) capacitors, bridges, and TaN Thin-Film Resistors (TFRs).
</details>
<details>
<summary>摘要</summary>
我们报道了毫米波，积成元件反射性筛选器的发展，使用进步的薄膜制造过程。基于之前已经证明可以在所有频率上实现50Ω输Impedance匹配的电路结构，这些电路在以前没有达到过的高频范围内实现了，通过利用 better than 2μm的薄膜特性和集成元件 such as SiN Metal-Insulator-Metal (MIM) 电容器、桥和 TaN Thin-Film Resistors (TFRs)。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Deep-Learning-for-Model-Correction-in-the-Computational-Crystallography-Toolbox"><a href="#Self-Supervised-Deep-Learning-for-Model-Correction-in-the-Computational-Crystallography-Toolbox" class="headerlink" title="Self-Supervised Deep Learning for Model Correction in the Computational Crystallography Toolbox"></a>Self-Supervised Deep Learning for Model Correction in the Computational Crystallography Toolbox</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01901">http://arxiv.org/abs/2307.01901</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gigantocypris/spread">https://github.com/gigantocypris/spread</a></li>
<li>paper_authors: Vidya Ganapati, Daniel Tchon, Aaron S. Brewster, Nicholas K. Sauter</li>
<li>for:  This paper aims to use the Computational Crystallography Toolbox (CCTBX) to determine the oxidation state of individual metal atoms in a macromolecule.</li>
<li>methods:  The paper uses self-supervised deep learning to correct the scientific model in CCTBX and provide uncertainty quantification.</li>
<li>results:  The paper describes the potential impact of using self-supervised deep learning to correct the scientific model in CCTBX and provide uncertainty quantification, and provides code for forward model simulation and data analysis at <a target="_blank" rel="noopener" href="https://github.com/gigantocypris/SPREAD.Here">https://github.com/gigantocypris/SPREAD.Here</a> is the text in Simplified Chinese:</li>
<li>for: 这篇论文使用Computational Crystallography Toolbox（CCTBX）确定蛋白质中金属原子的氧化状态。</li>
<li>methods: 这篇论文使用自我超vised深度学习修正CCTBX中的科学模型，并提供不确定性评估。</li>
<li>results: 这篇论文描述了使用自我超vised深度学习修正CCTBX中的科学模型的可能影响，并提供了<a target="_blank" rel="noopener" href="https://github.com/gigantocypris/SPREAD%E4%B8%AD%E7%9A%84%E4%BB%A3%E7%A0%81%E8%BF%9B%E8%A1%8C%E5%89%8D%E5%90%91%E6%A8%A1%E5%9E%8B%E4%BB%BF%E7%9C%9F%E5%92%8C%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E3%80%82">https://github.com/gigantocypris/SPREAD中的代码进行前向模型仿真和数据分析。</a><details>
<summary>Abstract</summary>
The Computational Crystallography Toolbox (CCTBX) is open-source software that allows for processing of crystallographic data, including from serial femtosecond crystallography (SFX), for macromolecular structure determination. We aim to use the modules in CCTBX to determine the oxidation state of individual metal atoms in a macromolecule. Changes in oxidation state are reflected in small shifts of the atom's X-ray absorption edge. These energy shifts can be extracted from the diffraction images recorded in serial femtosecond crystallography, given knowledge of a forward physics model. However, as the diffraction changes only slightly due to the absorption edge shift, inaccuracies in the forward physics model make it extremely challenging to observe the oxidation state. In this work, we describe the potential impact of using self-supervised deep learning to correct the scientific model in CCTBX and provide uncertainty quantification. We provide code for forward model simulation and data analysis, built from CCTBX modules, at https://github.com/gigantocypris/SPREAD , which can be integrated with machine learning. We describe open questions in algorithm development to help spur advances through dialog between crystallographers and machine learning researchers. New methods could help elucidate charge transfer processes in many reactions, including key events in photosynthesis.
</details>
<details>
<summary>摘要</summary>
《计算 кристалagraphy工具箱（CCTBX）》是一款开源软件，用于处理晶体学数据，包括 serial femtosecond crystallography（SFX），以确定大分子结构。我们想使用 CCTBX 模块来确定杂谱中金属原子的氧化状态。氧化状态的变化会导致原子的 X-射线吸收边缘微小变化。这些能量差可以从 serial femtosecond crystallography 记录的 diffraction 图像中提取，只要知道前向物理学模型。然而，由于 diffraction 变化只是微小，因此错误在物理学模型中会导致非常困难地观察氧化状态。在这种情况下，我们描述了使用自适应深度学习来更正科学模型在 CCTBX 中的可能的影响，以及提供不确定性评估。我们在 GitHub 上提供了代码，包括 forward 物理学模型的 simulate 和数据分析，可以与机器学习结合使用。我们描述了在算法开发中的开问，以帮助推动进步，通过晶体学家和机器学习研究人员之间的对话。新的方法可以帮助解释生物化学中的电子传递过程，包括照明很重要的PhotoSynthesis 过程。
</details></li>
</ul>
<hr>
<h2 id="Grad-FEC-Unequal-Loss-Protection-of-Deep-Features-in-Collaborative-Intelligence"><a href="#Grad-FEC-Unequal-Loss-Protection-of-Deep-Features-in-Collaborative-Intelligence" class="headerlink" title="Grad-FEC: Unequal Loss Protection of Deep Features in Collaborative Intelligence"></a>Grad-FEC: Unequal Loss Protection of Deep Features in Collaborative Intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01846">http://arxiv.org/abs/2307.01846</a></li>
<li>repo_url: None</li>
<li>paper_authors: Korcan Uyanik, S. Faegheh Yeganli, Ivan V. Bajić</li>
<li>for: 提高edge设备和云端的智能合作系统的可靠性和Robustness，即Collaborative Intelligence（CI）系统。</li>
<li>methods: 提出了一种基于Unequal Loss Protection（ULP）的新方法，包括特征重要度估计器，以优先保护front-end生成的重要特征包。</li>
<li>results: 实验结果表明，提出的方法可以在 packet loss 的情况下显著提高CI系统的可靠性和Robustness。<details>
<summary>Abstract</summary>
Collaborative intelligence (CI) involves dividing an artificial intelligence (AI) model into two parts: front-end, to be deployed on an edge device, and back-end, to be deployed in the cloud. The deep feature tensors produced by the front-end are transmitted to the cloud through a communication channel, which may be subject to packet loss. To address this issue, in this paper, we propose a novel approach to enhance the resilience of the CI system in the presence of packet loss through Unequal Loss Protection (ULP). The proposed ULP approach involves a feature importance estimator, which estimates the importance of feature packets produced by the front-end, and then selectively applies Forward Error Correction (FEC) codes to protect important packets. Experimental results demonstrate that the proposed approach can significantly improve the reliability and robustness of the CI system in the presence of packet loss.
</details>
<details>
<summary>摘要</summary>
共同智能（CI）包括将人工智能（AI）模型分成两部分：前端，部署在边缘设备上，和后端，部署在云端。深度特征张量生成于前端将被传输到云端通过通信频道，该频道可能会出现包loss。为 Addressing this issue, in this paper, we propose a novel approach to enhance the resilience of the CI system in the presence of packet loss through Unequal Loss Protection (ULP). The proposed ULP approach involves a feature importance estimator, which estimates the importance of feature packets produced by the front-end, and then selectively applies Forward Error Correction (FEC) codes to protect important packets. Experimental results demonstrate that the proposed approach can significantly improve the reliability and robustness of the CI system in the presence of packet loss.Here's the breakdown of the translation:* 共同智能 (CI): Collaborative intelligence* 人工智能 (AI)：Artificial intelligence* 模型 (model): Model* 前端 (front-end): Front-end* 后端 (back-end): Back-end* 云端 (cloud): Cloud* 深度特征张量 (deep feature tensors): Deep feature tensors* 包loss (packet loss): Packet loss* 强化 (enhance): Enhance* 不平等损失保护 (ULP): Unequal loss protection* 特征重要性估计器 (feature importance estimator): Feature importance estimator* 前端生成的特征包 (feature packets produced by the front-end): Feature packets produced by the front-end* FEC (Forward Error Correction) 码：Forward error correction codes* 实验结果 (experimental results): Experimental results* 可以显著提高 (can significantly improve): Can significantly improve* 可靠性 (reliability): Reliability* Robustness: Robustness
</details></li>
</ul>
<hr>
<h2 id="Multi-Channel-Feature-Extraction-for-Virtual-Histological-Staining-of-Photon-Absorption-Remote-Sensing-Images"><a href="#Multi-Channel-Feature-Extraction-for-Virtual-Histological-Staining-of-Photon-Absorption-Remote-Sensing-Images" class="headerlink" title="Multi-Channel Feature Extraction for Virtual Histological Staining of Photon Absorption Remote Sensing Images"></a>Multi-Channel Feature Extraction for Virtual Histological Staining of Photon Absorption Remote Sensing Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01824">http://arxiv.org/abs/2307.01824</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marian Boktor, James E. D. Tweel, Benjamin R. Ecclestone, Jennifer Ai Ye, Paul Fieguth, Parsin Haji Reza<br>for: 这项研究旨在提高血液染色的效率和可靠性，以便在病理诊断中提供可靠的诊断信息，帮助病理医生在疾病分类、评估和治疗规划中做出更好的决策。methods: 该研究提出了一种基于深度学习的虚拟 histological 染色框架，使用 photon absorption remote sensing（PARS）图像进行特征提取，并使用一种变体的K-means方法来捕捉有价值的多模态信息。此外，该研究还提出了一种基于传统cycleGAN框架的多通道cycleGAN（MC-GAN）模型，以包括更多的特征。results: 实验结果表明，特定的特征组合可以超过传统通道的性能，并且可以提高虚拟染色结果与化学染色（H&amp;E）图像的吻合度。在人皮肤和mouse brain组织中应用，结果表明，选择最佳特征组合是关键，可以提高虚拟染色结果的可靠性和可视化质量。<details>
<summary>Abstract</summary>
Accurate and fast histological staining is crucial in histopathology, impacting diagnostic precision and reliability. Traditional staining methods are time-consuming and subjective, causing delays in diagnosis. Digital pathology plays a vital role in advancing and optimizing histology processes to improve efficiency and reduce turnaround times. This study introduces a novel deep learning-based framework for virtual histological staining using photon absorption remote sensing (PARS) images. By extracting features from PARS time-resolved signals using a variant of the K-means method, valuable multi-modal information is captured. The proposed multi-channel cycleGAN (MC-GAN) model expands on the traditional cycleGAN framework, allowing the inclusion of additional features. Experimental results reveal that specific combinations of features outperform the conventional channels by improving the labeling of tissue structures prior to model training. Applied to human skin and mouse brain tissue, the results underscore the significance of choosing the optimal combination of features, as it reveals a substantial visual and quantitative concurrence between the virtually stained and the gold standard chemically stained hematoxylin and eosin (H&E) images, surpassing the performance of other feature combinations. Accurate virtual staining is valuable for reliable diagnostic information, aiding pathologists in disease classification, grading, and treatment planning. This study aims to advance label-free histological imaging and opens doors for intraoperative microscopy applications.
</details>
<details>
<summary>摘要</summary>
准精准快的 Histological 染色是 Histopathology 中非常重要的，它直接影响诊断的准确性和可靠性。传统的染色方法需要较长的时间和主观的干预，导致诊断的延迟。数字化Patology 在提高和优化 Histology 过程中扮演着重要的角色，以提高效率和减少回转时间。本研究提出了一种基于深度学习的虚拟 Histological 染色方法，使用 photon absorption remote sensing（PARS）图像来提取特征。通过 variants of the K-means 方法提取 PARS 时间分解信号中的有价值多Modal 信息。提出的多通道 cycleGAN（MC-GAN）模型在传统 cycleGAN 框架上进行扩展，以包括额外的特征。实验结果表明，特定的特征组合能够超越传统渠道的表现，提高识别组织结构之前的标签。应用于人皮和 Mouse brain 组织样本，结果表明选择最佳特征组合非常重要，它可以提供较高的视觉和量化协调性，超过其他特征组合。准确的虚拟染色对诊断信息的可靠性至关重要，帮助病理学家在疾病分类、评分和治疗规划中做出更加准确的决策。本研究旨在提高无标签 Histological 成像，开启了Intraoperative 镜像应用的大门。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://example.com/2023/07/05/eess.IV_2023_07_05/" data-id="cllsjvzdw007cf588c7cz4x2b" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_07_04" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/04/cs.LG_2023_07_04/" class="article-date">
  <time datetime="2023-07-03T16:00:00.000Z" itemprop="datePublished">2023-07-04</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/04/cs.LG_2023_07_04/">cs.LG - 2023-07-04 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="GHOST-A-Graph-Neural-Network-Accelerator-using-Silicon-Photonics"><a href="#GHOST-A-Graph-Neural-Network-Accelerator-using-Silicon-Photonics" class="headerlink" title="GHOST: A Graph Neural Network Accelerator using Silicon Photonics"></a>GHOST: A Graph Neural Network Accelerator using Silicon Photonics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01782">http://arxiv.org/abs/2307.01782</a></li>
<li>repo_url: None</li>
<li>paper_authors: Salma Afifi, Febin Sunny, Amin Shafiee, Mahdi Nikdast, Sudeep Pasricha</li>
<li>for: 这篇论文的目的是为了提出一种基于光学频谱的干扰器硬件加速器，用于加速图 neuron 网络（GNNs）的运算。</li>
<li>methods: 这篇论文使用了光学频谱技术，实现了图 neuron 网络的三个主要阶段（邻居更新、 message passing 和更新），并且可以用于多种广泛使用的 GNN 模型和架构，如图 convolution 网络和图注意力网络。</li>
<li>results: 根据 simulations 研究，GHOST 相比 GPU、TPU、CPU 和多种现有 GNN 硬件加速器，能够提供至少 10.2 倍的吞吐量和 3.8 倍的能效率。<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) have emerged as a powerful approach for modelling and learning from graph-structured data. Multiple fields have since benefitted enormously from the capabilities of GNNs, such as recommendation systems, social network analysis, drug discovery, and robotics. However, accelerating and efficiently processing GNNs require a unique approach that goes beyond conventional artificial neural network accelerators, due to the substantial computational and memory requirements of GNNs. The slowdown of scaling in CMOS platforms also motivates a search for alternative implementation substrates. In this paper, we present GHOST, the first silicon-photonic hardware accelerator for GNNs. GHOST efficiently alleviates the costs associated with both vertex-centric and edge-centric operations. It implements separately the three main stages involved in running GNNs in the optical domain, allowing it to be used for the inference of various widely used GNN models and architectures, such as graph convolution networks and graph attention networks. Our simulation studies indicate that GHOST exhibits at least 10.2x better throughput and 3.8x better energy efficiency when compared to GPU, TPU, CPU and multiple state-of-the-art GNN hardware accelerators.
</details>
<details>
<summary>摘要</summary>
граф нейрон сети (GNNs) 已成为图Structured data的 мощful approached for modeling and learning. 多个领域受益于 GNNs 的能力, such as recommendation systems, social network analysis, drug discovery, and robotics. 然而，加速和有效地处理 GNNs 需要特殊的approach，以 beyond conventional artificial neural network accelerators, due to the substantial computational and memory requirements of GNNs. CMOS 平台的慢速下降也驱动了寻找代替实现SUBSTRATES. 在这篇论文中，我们提出了 GHOST, the first silicon-photonic hardware accelerator for GNNs. GHOST efficiently alleviates the costs associated with both vertex-centric and edge-centric operations. It implements separately the three main stages involved in running GNNs in the optical domain, allowing it to be used for the inference of various widely used GNN models and architectures, such as graph convolution networks and graph attention networks. Our simulation studies indicate that GHOST exhibits at least 10.2x better throughput and 3.8x better energy efficiency when compared to GPU, TPU, CPU, and multiple state-of-the-art GNN hardware accelerators.
</details></li>
</ul>
<hr>
<h2 id="FedHIL-Heterogeneity-Resilient-Federated-Learning-for-Robust-Indoor-Localization-with-Mobile-Devices"><a href="#FedHIL-Heterogeneity-Resilient-Federated-Learning-for-Robust-Indoor-Localization-with-Mobile-Devices" class="headerlink" title="FedHIL: Heterogeneity Resilient Federated Learning for Robust Indoor Localization with Mobile Devices"></a>FedHIL: Heterogeneity Resilient Federated Learning for Robust Indoor Localization with Mobile Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01780">http://arxiv.org/abs/2307.01780</a></li>
<li>repo_url: None</li>
<li>paper_authors: Danish Gufran, Sudeep Pasricha</li>
<li>for: 本研究旨在提高设备不同、indoor环境多样化的情况下的indoor定位精度，同时保护用户数据隐私。</li>
<li>methods: 本研究提出了一种基于联合学习（Federated Learning，FL）和indoor定位的嵌入式机器学习框架（FedHIL），通过选择性调整量来维护ML模型的性能，并在不同设备和环境中实现高精度indoor定位。</li>
<li>results: 实验表明，FedHIL在多种不同的indoor环境和设备上都能够实现1.62倍的定位精度提高，较前期工作的最佳FL-based indoor定位框架的1.35倍。<details>
<summary>Abstract</summary>
Indoor localization plays a vital role in applications such as emergency response, warehouse management, and augmented reality experiences. By deploying machine learning (ML) based indoor localization frameworks on their mobile devices, users can localize themselves in a variety of indoor and subterranean environments. However, achieving accurate indoor localization can be challenging due to heterogeneity in the hardware and software stacks of mobile devices, which can result in inconsistent and inaccurate location estimates. Traditional ML models also heavily rely on initial training data, making them vulnerable to degradation in performance with dynamic changes across indoor environments. To address the challenges due to device heterogeneity and lack of adaptivity, we propose a novel embedded ML framework called FedHIL. Our framework combines indoor localization and federated learning (FL) to improve indoor localization accuracy in device-heterogeneous environments while also preserving user data privacy. FedHIL integrates a domain-specific selective weight adjustment approach to preserve the ML model's performance for indoor localization during FL, even in the presence of extremely noisy data. Experimental evaluations in diverse real-world indoor environments and with heterogeneous mobile devices show that FedHIL outperforms state-of-the-art FL and non-FL indoor localization frameworks. FedHIL is able to achieve 1.62x better localization accuracy on average than the best performing FL-based indoor localization framework from prior work.
</details>
<details>
<summary>摘要</summary>
室内定位在应用程序中扮演着重要的角色，如应急应对、仓库管理和增强现实体验。通过在移动设备上部署机器学习（ML）基于的室内定位框架，用户可以在各种室内和地下环境中自动地标定自己的位置。然而，实现准确的室内定位可以是困难的，因为移动设备的硬件和软件栈的差异会导致不一致和不准确的位置估计。传统的ML模型也具有依赖于初始训练数据的问题，从而使其在室内环境中表现出现很大的变化和衰退。为解决设备不一致和数据变化导致的挑战，我们提出了一种新的嵌入式ML框架called FedHIL。FedHIL将室内定位和联邦学习（FL）结合起来，以提高设备不一致环境中的室内定位精度，同时也保护用户数据隐私。FedHIL使用域特定的选择性加重方法来保持ML模型在室内定位中的表现，即使面临非常噪音的数据时也能够保持高性能。实验证明，FedHIL在多个真实世界室内环境和不同的移动设备上表现出色，与传统的FL和非FL室内定位框架相比，具有1.62倍的本地化精度。
</details></li>
</ul>
<hr>
<h2 id="Shapley-Sets-Feature-Attribution-via-Recursive-Function-Decomposition"><a href="#Shapley-Sets-Feature-Attribution-via-Recursive-Function-Decomposition" class="headerlink" title="Shapley Sets: Feature Attribution via Recursive Function Decomposition"></a>Shapley Sets: Feature Attribution via Recursive Function Decomposition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01777">http://arxiv.org/abs/2307.01777</a></li>
<li>repo_url: None</li>
<li>paper_authors: Torty Sivill, Peter Flach</li>
<li>for: 本研究旨在替代Feature Value Attribution中常用但可能受特征相互作用的Shapley值，提出一种新的归属方法——Shapley Set。</li>
<li>methods: 本研究使用了一种归属函数分解算法，将模型分解成不可分割变量组，并具有对数 linear 复杂度。</li>
<li>results: 研究表明，Shapley Set具有与Shapley值相同的公正性观念，并且可以避免基于Shapley值的归属方法中出现的坑。此外，Shapley Set在数据类型具有复杂依赖关系时表现 particullary 优异。<details>
<summary>Abstract</summary>
Despite their ubiquitous use, Shapley value feature attributions can be misleading due to feature interaction in both model and data. We propose an alternative attribution approach, Shapley Sets, which awards value to sets of features. Shapley Sets decomposes the underlying model into non-separable variable groups using a recursive function decomposition algorithm with log linear complexity in the number of variables. Shapley Sets attributes to each non-separable variable group their combined value for a particular prediction. We show that Shapley Sets is equivalent to the Shapley value over the transformed feature set and thus benefits from the same axioms of fairness. Shapley Sets is value function agnostic and we show theoretically and experimentally how Shapley Sets avoids pitfalls associated with Shapley value based alternatives and are particularly advantageous for data types with complex dependency structure.
</details>
<details>
<summary>摘要</summary>
尽管Shapley值特征归功通用，但它们可能导致特征互动的启示，both model和数据级。我们提出了一种替代方案，即Shapley集，该奖励集合特征。Shapley集使用一种分解函数分解算法，将基础模型分解为不可分割变量组。对每个不可分割变量组，Shapley集归功其组合值 для特定预测。我们证明了Shapley集等于在转换特征集上的Shapley值，因此受到同样的公平原则保证。Shapley集是值函数无关的，我们 theoretically和实验表明，Shapley集可以避免基于Shapley值的代替方法中的坑害，特别是数据类型具有复杂依赖结构。
</details></li>
</ul>
<hr>
<h2 id="Fast-Optimal-Transport-through-Sliced-Wasserstein-Generalized-Geodesics"><a href="#Fast-Optimal-Transport-through-Sliced-Wasserstein-Generalized-Geodesics" class="headerlink" title="Fast Optimal Transport through Sliced Wasserstein Generalized Geodesics"></a>Fast Optimal Transport through Sliced Wasserstein Generalized Geodesics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01770">http://arxiv.org/abs/2307.01770</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guillaume Mahey, Laetitia Chapel, Gilles Gasso, Clément Bonet, Nicolas Courty</li>
<li>for: 本 paper 描述了一种新的 Wasserstein 距离代理（min-SWGG），该代理基于输送地图，并与 Wasserstein 泛化 геodesics 相关。</li>
<li>methods: 本 paper 使用了一种新的 Computational Scheme，可以使用 gradient descent 优化。此外，paper 还提供了一种关于 Wasserstein 距离的closed form解，并证明了 min-SWGG 是 Wasserstein 距离的上界，并且与 Sliced-Wasserstein 相似，但具有更多的特性。</li>
<li>results: 本 paper 通过 empirical evidences 支持 min-SWGG 在各种应用中的 beneficial 效果，包括梯度流、形状匹配和图像颜色化等。<details>
<summary>Abstract</summary>
Wasserstein distance (WD) and the associated optimal transport plan have been proven useful in many applications where probability measures are at stake. In this paper, we propose a new proxy of the squared WD, coined min-SWGG, that is based on the transport map induced by an optimal one-dimensional projection of the two input distributions. We draw connections between min-SWGG and Wasserstein generalized geodesics in which the pivot measure is supported on a line. We notably provide a new closed form for the exact Wasserstein distance in the particular case of one of the distributions supported on a line allowing us to derive a fast computational scheme that is amenable to gradient descent optimization. We show that min-SWGG is an upper bound of WD and that it has a complexity similar to as Sliced-Wasserstein, with the additional feature of providing an associated transport plan. We also investigate some theoretical properties such as metricity, weak convergence, computational and topological properties. Empirical evidences support the benefits of min-SWGG in various contexts, from gradient flows, shape matching and image colorization, among others.
</details>
<details>
<summary>摘要</summary>
瓦asserstein距离（WD）和相关的最优运输计划在probability measures中显示了有用性。在这篇论文中，我们提议一个新的proxy，称为min-SWGG，它基于两个输入分布的运输地图，它是通过一个优化的一维投影来定义的。我们将min-SWGG与通用水stein化曲线的关系进行连接，并在特定情况下提供一个新的准确 Wasserstein距离的closed form，使得可以使用梯度下降优化。我们证明min-SWGG是WD的上界，并且它的复杂性与Sliced-Wasserstein相似，但它具有提供相关运输计划的特点。我们还研究了一些理论性质，如metricity、weak convergence、computational和topological性。empirical evidence表明min-SWGG在各种场景中具有各种优点，从梯度流、形态匹配到图像颜色化等。
</details></li>
</ul>
<hr>
<h2 id="Localized-Data-Work-as-a-Precondition-for-Data-Centric-ML-A-Case-Study-of-Full-Lifecycle-Crop-Disease-Identification-in-Ghana"><a href="#Localized-Data-Work-as-a-Precondition-for-Data-Centric-ML-A-Case-Study-of-Full-Lifecycle-Crop-Disease-Identification-in-Ghana" class="headerlink" title="Localized Data Work as a Precondition for Data-Centric ML: A Case Study of Full Lifecycle Crop Disease Identification in Ghana"></a>Localized Data Work as a Precondition for Data-Centric ML: A Case Study of Full Lifecycle Crop Disease Identification in Ghana</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01767">http://arxiv.org/abs/2307.01767</a></li>
<li>repo_url: None</li>
<li>paper_authors: Darlington Akogo, Issah Samori, Cyril Akafia, Harriet Fiagbor, Andrews Kangah, Donald Kwame Asiedu, Kwabena Fuachie, Luis Oala</li>
<li>for: 论文旨在演示如何通过团队合作和数据工程来提高农业产量和食品安全。</li>
<li>methods: 论文使用了无人机采集的数据和机器学习算法来确定作物压力。</li>
<li>results: 研究实现了一个基于地 desktop 应用程序的本地化数据驱动解决方案，以提高农业生产力和食品安全。<details>
<summary>Abstract</summary>
The Ghana Cashew Disease Identification with Artificial Intelligence (CADI AI) project demonstrates the importance of sound data work as a precondition for the delivery of useful, localized datacentric solutions for public good tasks such as agricultural productivity and food security. Drone collected data and machine learning are utilized to determine crop stressors. Data, model and the final app are developed jointly and made available to local farmers via a desktop application.
</details>
<details>
<summary>摘要</summary>
《加纳杏仁疾病识别用人工智能项目（CADI AI）》显示了数据工作的重要性，作为当地数据驱动解决方案的前提。该项目使用无人机收集数据和机器学习来确定作物压力。数据、模型和最终应用程序均由本地农民通过桌面应用程序获得。
</details></li>
</ul>
<hr>
<h2 id="Pretraining-is-All-You-Need-A-Multi-Atlas-Enhanced-Transformer-Framework-for-Autism-Spectrum-Disorder-Classification"><a href="#Pretraining-is-All-You-Need-A-Multi-Atlas-Enhanced-Transformer-Framework-for-Autism-Spectrum-Disorder-Classification" class="headerlink" title="Pretraining is All You Need: A Multi-Atlas Enhanced Transformer Framework for Autism Spectrum Disorder Classification"></a>Pretraining is All You Need: A Multi-Atlas Enhanced Transformer Framework for Autism Spectrum Disorder Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01759">http://arxiv.org/abs/2307.01759</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lugges991/metaformer">https://github.com/lugges991/metaformer</a></li>
<li>paper_authors: Lucas Mahler, Qi Wang, Julius Steiglechner, Florian Birk, Samuel Heczko, Klaus Scheffler, Gabriele Lohmann</li>
<li>For:  This paper proposes a novel framework for ASD classification using resting-state functional magnetic resonance imaging data.* Methods:  The proposed framework, called METAFormer, utilizes a multi-atlas approach and self-supervised pretraining to improve classification performance.* Results:  The proposed framework achieves state-of-the-art performance on the ABIDE I dataset, with an average accuracy of 83.7% and an AUC-score of 0.832.Here is the same information in Simplified Chinese text:* For: 这个论文提出了一种基于Resting-state功能磁共振成像数据的ASD分类方法。* Methods: 提议的方法是METAFormer，它使用多个图像的方法和自我批示训练来提高分类性能。* Results: 提议的方法在ABIDE I dataset上达到了状态之arte的性能，具体来说是83.7%的平均精度和0.832的AUC分数。<details>
<summary>Abstract</summary>
Autism spectrum disorder (ASD) is a prevalent psychiatric condition characterized by atypical cognitive, emotional, and social patterns. Timely and accurate diagnosis is crucial for effective interventions and improved outcomes in individuals with ASD. In this study, we propose a novel Multi-Atlas Enhanced Transformer framework, METAFormer, ASD classification. Our framework utilizes resting-state functional magnetic resonance imaging data from the ABIDE I dataset, comprising 406 ASD and 476 typical control (TC) subjects. METAFormer employs a multi-atlas approach, where flattened connectivity matrices from the AAL, CC200, and DOS160 atlases serve as input to the transformer encoder. Notably, we demonstrate that self-supervised pretraining, involving the reconstruction of masked values from the input, significantly enhances classification performance without the need for additional or separate training data. Through stratified cross-validation, we evaluate the proposed framework and show that it surpasses state-of-the-art performance on the ABIDE I dataset, with an average accuracy of 83.7% and an AUC-score of 0.832. The code for our framework is available at https://github.com/Lugges991/METAFormer
</details>
<details>
<summary>摘要</summary>
“自闭症 спектルム病（ASD）是一种常见的心理疾病，具有异常的认知、情感和社交模式。及时和准确的诊断非常重要，以便为患有ASD的个体提供有效的 intervención和改善结果。在这项研究中，我们提出了一种新的多 Atlas 增强变换框架，METAFormer，用于ASD分类。我们的框架使用了ABIDE I 数据集中的406名ASD和476名 Typical control（TC）个体的休息态功能磁共振成像数据。METAFormer 使用多Atlas方法，其中扁平连接矩阵从AAL、CC200和DOS160 的图像服务器为变换器编码器的输入。我们表明，不需要额外或分离的训练数据，通过自我超vision的预训练，即将掩码的值重建为输入的masked 值，可以明显提高分类性能。通过 stratified 树目录验证，我们评估了提议的框架，并发现其在ABIDE I 数据集上的平均准确率为83.7%，AUC 分数为0.832。 code for our framework is available at https://github.com/Lugges991/METAFormer。”
</details></li>
</ul>
<hr>
<h2 id="Local-primordial-non-Gaussianity-from-the-large-scale-clustering-of-photometric-DESI-luminous-red-galaxies"><a href="#Local-primordial-non-Gaussianity-from-the-large-scale-clustering-of-photometric-DESI-luminous-red-galaxies" class="headerlink" title="Local primordial non-Gaussianity from the large-scale clustering of photometric DESI luminous red galaxies"></a>Local primordial non-Gaussianity from the large-scale clustering of photometric DESI luminous red galaxies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01753">http://arxiv.org/abs/2307.01753</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mehdirezaie/dimagfnl">https://github.com/mehdirezaie/dimagfnl</a></li>
<li>paper_authors: Mehdi Rezaie, Ashley J. Ross, Hee-Jong Seo, Hui Kong, Anna Porredon, Lado Samushia, Edmond Chaussidon, Alex Krolewski, Arnaud de Mattia, Florian Beutler, Jessica Nicole Aguilar, Steven Ahlen, Shadab Alam, Santiago Avila, Benedict Bahr-Kalus, Jose Bermejo-Climent, David Brooks, Todd Claybaugh, Shaun Cole, Kyle Dawson, Axel de la Macorra, Peter Doel, Andreu Font-Ribera, Jaime E. Forero-Romero, Satya Gontcho A Gontcho, Julien Guy, Klaus Honscheid, Theodore Kisner, Martin Landriau, Michael Levi, Marc Manera, Aaron Meisner, Ramon Miquel, Eva-Maria Mueller, Adam Myers, Jeffrey A. Newman, Jundan Nie, Nathalie Palanque-Delabrouille, Will Percival, Claire Poppett, Graziano Rossi, Eusebio Sanchez, Michael Schubnell, Gregory Tarlé, Benjamin Alan Weaver, Christophe Yèche, Zhimin Zhou, Hu Zou</li>
<li>For: The paper aims to constrain the local primordial non-Gaussianity parameter fNL using angular clustering of luminous red galaxies from the Dark Energy Spectroscopic Instrument (DESI) imaging surveys.* Methods: The paper uses linear regression and artificial neural networks to alleviate non-cosmological excess clustering on large scales, and tests the methods against log-normal simulations with and without fNL and systematics.* Results: The paper finds fNL $&#x3D; 47^{+14(+29)}_{-11(-22)}$ at 68%(95%) confidence, with a maximum likelihood value of fNL $\sim 50$ and increased uncertainty when including a full set of imaging maps. The results indicate fNL &gt; 0 with a 99.9 percent confidence level, which could be attributed to unforeseen systematics or a scale-dependent fNL model.<details>
<summary>Abstract</summary>
We use angular clustering of luminous red galaxies from the Dark Energy Spectroscopic Instrument (DESI) imaging surveys to constrain the local primordial non-Gaussianity parameter fNL. Our sample comprises over 12 million targets, covering 14,000 square degrees of the sky, with redshifts in the range 0.2< z < 1.35. We identify Galactic extinction, survey depth, and astronomical seeing as the primary sources of systematic error, and employ linear regression and artificial neural networks to alleviate non-cosmological excess clustering on large scales. Our methods are tested against log-normal simulations with and without fNL and systematics, showing superior performance of the neural network treatment in reducing remaining systematics. Assuming the universality relation, we find fNL $= 47^{+14(+29)}_{-11(-22)}$ at 68\%(95\%) confidence. With a more aggressive treatment, including regression against the full set of imaging maps, our maximum likelihood value shifts slightly to fNL$ \sim 50$ and the uncertainty on fNL increases due to the removal of large-scale clustering information. We apply a series of robustness tests (e.g., cuts on imaging, declination, or scales used) that show consistency in the obtained constraints. Despite extensive efforts to mitigate systematics, our measurements indicate fNL > 0 with a 99.9 percent confidence level. This outcome raises concerns as it could be attributed to unforeseen systematics, including calibration errors or uncertainties associated with low-\ell systematics in the extinction template. Alternatively, it could suggest a scale-dependent fNL model--causing significant non-Gaussianity around large-scale structure while leaving cosmic microwave background scales unaffected. Our results encourage further studies of fNL with DESI spectroscopic samples, where the inclusion of 3D clustering modes should help separate imaging systematics.
</details>
<details>
<summary>摘要</summary>
我们使用 DESI 图像观测的 Angular 卷积方法来约束本地原始非加性参数 fNL。我们的样本包括超过 12 百万目标，覆盖 14,000平方度天空，红shift 在 0.2 < z < 1.35 之间。我们认为 galactic 遮盖、观测深度和天文望远镜为主要系统性错误来源，并使用线性回归和人工神经网络来缓减非 cosmological 过卷 clustering。我们的方法在 log-normal  simulations 中与和 без fNL 和系统atic 进行测试，显示人工神经网络处理的superior performance 在减少剩下系统atic。assuming  универса性关系，我们得到 fNL = 47 ± 14 ± 29 的确idence Interval。通过对全aset of imaging maps进行回归，我们的最大似然值shift 到 fNL ≈ 50，并且因为移除大规模 clustering 信息而增加了 fNL 的不确定度。我们进行了一系列Robustness 测试（例如，对 imaging、 declination 或 scale 进行cut），发现结果是一致的。despite extensive efforts to mitigate systematics，我们的测量结果表明 fNL > 0 的99.9% 信任水平。这些结果可能被归因于未知系统atic，包括折合错误或低-\ell 系统atic 在 extinction 模板中的不确定度。 Alternatively，这些结果可能表明 scale-dependent fNL 模型，导致在大规模结构上显著的非 Gaussianity，而不影响cosmic microwave background 观测。我们的结果鼓励 DESI 光谱样本进一步研究 fNL，其中包括3D clustering modes，可以帮助分离图像系统atic。
</details></li>
</ul>
<hr>
<h2 id="SRCD-Semantic-Reasoning-with-Compound-Domains-for-Single-Domain-Generalized-Object-Detection"><a href="#SRCD-Semantic-Reasoning-with-Compound-Domains-for-Single-Domain-Generalized-Object-Detection" class="headerlink" title="SRCD: Semantic Reasoning with Compound Domains for Single-Domain Generalized Object Detection"></a>SRCD: Semantic Reasoning with Compound Domains for Single-Domain Generalized Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01750">http://arxiv.org/abs/2307.01750</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhijie Rao, Jingcai Guo, Luyao Tang, Yue Huang, Xinghao Ding, Song Guo</li>
<li>for: 这个论文提出了一个新的单域泛化物体检测框架（即Single-DGOD），旨在学习和维护自增强采样的 semantic 结构，以提高模型的泛化能力。</li>
<li>methods: 论文提出了两个主要组件： texture-based self-augmentation (TBSA) 模块和 local-global semantic reasoning (LGSR) 模块。 TBSA 模块用于消除图像水平上的不相关属性，如光影、颜色等，而 LGSR 模块用于进一步模型实例层次的 semantic 关系，以帮助维护内在的 semantic 结构。</li>
<li>results: 对多个benchmark进行了广泛的实验，证明了提出的 SRCD 的效果。<details>
<summary>Abstract</summary>
This paper provides a novel framework for single-domain generalized object detection (i.e., Single-DGOD), where we are interested in learning and maintaining the semantic structures of self-augmented compound cross-domain samples to enhance the model's generalization ability. Different from DGOD trained on multiple source domains, Single-DGOD is far more challenging to generalize well to multiple target domains with only one single source domain. Existing methods mostly adopt a similar treatment from DGOD to learn domain-invariant features by decoupling or compressing the semantic space. However, there may have two potential limitations: 1) pseudo attribute-label correlation, due to extremely scarce single-domain data; and 2) the semantic structural information is usually ignored, i.e., we found the affinities of instance-level semantic relations in samples are crucial to model generalization. In this paper, we introduce Semantic Reasoning with Compound Domains (SRCD) for Single-DGOD. Specifically, our SRCD contains two main components, namely, the texture-based self-augmentation (TBSA) module, and the local-global semantic reasoning (LGSR) module. TBSA aims to eliminate the effects of irrelevant attributes associated with labels, such as light, shadow, color, etc., at the image level by a light-yet-efficient self-augmentation. Moreover, LGSR is used to further model the semantic relationships on instance features to uncover and maintain the intrinsic semantic structures. Extensive experiments on multiple benchmarks demonstrate the effectiveness of the proposed SRCD.
</details>
<details>
<summary>摘要</summary>
To address these limitations, this paper introduces Semantic Reasoning with Compound Domains (SRCD) for Single-DGOD. SRCD consists of two main components: the texture-based self-augmentation (TBSA) module and the local-global semantic reasoning (LGSR) module. TBSA aims to eliminate the effects of irrelevant attributes associated with labels, such as light, shadow, color, etc., at the image level by using a light-yet-efficient self-augmentation. LGSR is used to further model the semantic relationships on instance features to uncover and maintain the intrinsic semantic structures.Experiments on multiple benchmarks demonstrate the effectiveness of the proposed SRCD. The main contributions of this paper are:1. A novel framework for Single-DGOD, which learns and maintains the semantic structures of self-augmented compound cross-domain samples.2. A new module called TBSA, which eliminates the effects of irrelevant attributes associated with labels at the image level.3. A module called LGSR, which models the semantic relationships on instance features to uncover and maintain the intrinsic semantic structures.Overall, this paper presents a more effective and efficient approach to Single-DGOD, which can improve the generalization ability of object detection models in real-world applications.
</details></li>
</ul>
<hr>
<h2 id="RRCNN-A-novel-signal-decomposition-approach-based-on-recurrent-residue-convolutional-neural-network"><a href="#RRCNN-A-novel-signal-decomposition-approach-based-on-recurrent-residue-convolutional-neural-network" class="headerlink" title="RRCNN: A novel signal decomposition approach based on recurrent residue convolutional neural network"></a>RRCNN: A novel signal decomposition approach based on recurrent residue convolutional neural network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01725">http://arxiv.org/abs/2307.01725</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhoudafa08/rrcnn">https://github.com/zhoudafa08/rrcnn</a></li>
<li>paper_authors: Feng Zhou, Antonio Cicone, Haomin Zhou</li>
<li>for: 这种研究的目的是为了开发一种基于深度学习的非站立信号分解方法，以提高现有方法的缺点，如边界和模式混合问题和噪声Robustness。</li>
<li>methods: 该方法使用了卷积神经网络、径向结构和非线性活动函数来计算信号的本地平均值，并在深度学习框架下实现了新的非站立信号分解方法。</li>
<li>results: 实验表明，提案的方法可以更好地处理边界问题、模式混合问题、噪声Robustness和分解结果的正交性，并且在计算本地平均值和信号分解两个方面都有更高的性能。<details>
<summary>Abstract</summary>
The decomposition of non-stationary signals is an important and challenging task in the field of signal time-frequency analysis. In the recent two decades, many signal decomposition methods led by the empirical mode decomposition, which was pioneered by Huang et al. in 1998, have been proposed by different research groups. However, they still have some limitations. For example, they are generally prone to boundary and mode mixing effects and are not very robust to noise. Inspired by the successful applications of deep learning in fields like image processing and natural language processing, and given the lack in the literature of works in which deep learning techniques are used directly to decompose non-stationary signals into simple oscillatory components, we use the convolutional neural network, residual structure and nonlinear activation function to compute in an innovative way the local average of the signal, and study a new non-stationary signal decomposition method under the framework of deep learning. We discuss the training process of the proposed model and study the convergence analysis of the learning algorithm. In the experiments, we evaluate the performance of the proposed model from two points of view: the calculation of the local average and the signal decomposition. Furthermore, we study the mode mixing, noise interference, and orthogonality properties of the decomposed components produced by the proposed method. All results show that the proposed model allows for better handling boundary effect, mode mixing effect, robustness, and the orthogonality of the decomposed components than existing methods.
</details>
<details>
<summary>摘要</summary>
非站点信号的分解是信号时频分析领域中的一个重要和挑战性任务。过去二十年，许多基于实验模式分解的信号分解方法已经被不同的研究组织提出。然而，它们仍有一些限制，例如容易受边缘和模式混合效应的影响，并不够鲁棒对噪声。受图像处理和自然语言处理等领域的深度学习成功应用启发，我们使用卷积神经网络、循环结构和非线性活化函数计算非站点信号的本地均值，并研究了一种基于深度学习框架的新的非站点信号分解方法。我们讨论了该模型的训练过程和学习算法的整合分析。在实验中，我们评估了提案模型的性能从两个角度：计算本地均值和信号分解。此外，我们还研究了分解后的模式混合、噪声抑制和正交性特性。所有结果都表明，提案的模型可以更好地处理边缘效应、模式混合效应、鲁棒性和分解后的正交性。
</details></li>
</ul>
<hr>
<h2 id="MOPO-LSI-A-User-Guide"><a href="#MOPO-LSI-A-User-Guide" class="headerlink" title="MOPO-LSI: A User Guide"></a>MOPO-LSI: A User Guide</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01719">http://arxiv.org/abs/2307.01719</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yong Zheng, Kumar Neelotpal Shukla, Jasmine Xu, David, Wang, Michael O’Leary</li>
<li>for: 这份论文是为了提供一个开源的多目标投资套件库，用于实现可持续投资。</li>
<li>methods: 该论文使用了多目标优化算法来解决投资问题，并提供了一个可用的配置文件来定制算法的参数。</li>
<li>results: 该论文通过使用多目标优化算法，可以实现更好的投资效果，并提供了一个可用的配置文件来定制算法的参数。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
MOPO-LSI is an open-source Multi-Objective Portfolio Optimization Library for Sustainable Investments. This document provides a user guide for MOPO-LSI version 1.0, including problem setup, workflow and the hyper-parameters in configurations.
</details>
<details>
<summary>摘要</summary>
MOPO-LSI是一个开源的多目标投资组合优化库，旨在推动可持续投资。这份文档提供MOPO-LSI版本1.0的用户指南，包括问题设置、工作流程和配置参数。
</details></li>
</ul>
<hr>
<h2 id="On-the-Constrained-Time-Series-Generation-Problem"><a href="#On-the-Constrained-Time-Series-Generation-Problem" class="headerlink" title="On the Constrained Time-Series Generation Problem"></a>On the Constrained Time-Series Generation Problem</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01717">http://arxiv.org/abs/2307.01717</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrea Coletta, Sriram Gopalakrishan, Daniel Borrajo, Svitlana Vyetrenko</li>
<li>for: 这个论文的目的是解决受限时间序列生成问题，以提高机器学习算法的性能，增加罕见事件的发生频率，并生成对应的counterfactualenario。</li>
<li>methods: 这个论文提出了一种新的方法集，包括一种受限时间序列生成模型“GuidedDiffTime”，用于生成符合限制的时间序列。这些方法使用可导的扩散模型，并通过优化问题来保证生成的时间序列具有真实性。</li>
<li>results: 这个论文在金融和能源等领域进行了评估，并证明了其方法的优越性。具体来说，这些方法可以提高现有方法的性能，同时不需要重新训练，从而减少碳脚印。<details>
<summary>Abstract</summary>
Synthetic time series are often used in practical applications to augment the historical time series dataset for better performance of machine learning algorithms, amplify the occurrence of rare events, and also create counterfactual scenarios described by the time series. Distributional-similarity (which we refer to as realism) as well as the satisfaction of certain numerical constraints are common requirements in counterfactual time series scenario generation requests. For instance, the US Federal Reserve publishes synthetic market stress scenarios given by the constrained time series for financial institutions to assess their performance in hypothetical recessions. Existing approaches for generating constrained time series usually penalize training loss to enforce constraints, and reject non-conforming samples. However, these approaches would require re-training if we change constraints, and rejection sampling can be computationally expensive, or impractical for complex constraints. In this paper, we propose a novel set of methods to tackle the constrained time series generation problem and provide efficient sampling while ensuring the realism of generated time series. In particular, we frame the problem using a constrained optimization framework and then we propose a set of generative methods including ``GuidedDiffTime'', a guided diffusion model to generate realistic time series. Empirically, we evaluate our work on several datasets for financial and energy data, where incorporating constraints is critical. We show that our approaches outperform existing work both qualitatively and quantitatively. Most importantly, we show that our ``GuidedDiffTime'' model is the only solution where re-training is not necessary for new constraints, resulting in a significant carbon footprint reduction.
</details>
<details>
<summary>摘要</summary>
Synthetic time series 常用于实际应用中以增强机器学习算法的性能，增加罕见事件的发生频率，并创建对时间序列的counterfactualenario。例如，美国联邦储金行发布了基于受限时间序列的synthetic市场压力场景，用于金融机构评估其在假设的经济衰退中的性能。现有的时间序列生成方法通常是通过减少训练损失来实现约束，并拒绝不符合约束的样本。然而，这些方法需要重新训练，如果改变约束，并且拒绝样本可能是计算昂贵或对复杂约束来说不实际。在这篇论文中，我们提出一种新的方法来解决受约束时间序列生成问题，并提供高效的采样，以保证生成的时间序列的真实性。具体来说，我们将问题带入一个受约束优化框架，然后我们提出一种生成方法，包括“导航扩散模型”，用于生成真实的时间序列。在实际中，我们对金融和能源等数据集进行了评估，并证明我们的方法在质量和效率两个方面都有较好的表现。最重要的是，我们的“导航扩散模型”不需要重新训练，以避免重新训练所带来的碳脚印。
</details></li>
</ul>
<hr>
<h2 id="Align-With-Purpose-Optimize-Desired-Properties-in-CTC-Models-with-a-General-Plug-and-Play-Framework"><a href="#Align-With-Purpose-Optimize-Desired-Properties-in-CTC-Models-with-a-General-Plug-and-Play-Framework" class="headerlink" title="Align With Purpose: Optimize Desired Properties in CTC Models with a General Plug-and-Play Framework"></a>Align With Purpose: Optimize Desired Properties in CTC Models with a General Plug-and-Play Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01715">http://arxiv.org/abs/2307.01715</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eliya Segev, Maya Alroy, Ronen Katsir, Noam Wies, Ayana Shenhav, Yael Ben-Oren, David Zar, Oren Tadmor, Jacob Bitterman, Amnon Shashua, Tal Rosenwein</li>
<li>for: 这篇论文的目的是提高CTC评价函数中的一种扩展，以便在训练seq2seq模型时提高模型的性能。</li>
<li>methods: 该论文提出了一种扩展CTC评价函数的方法，称为“Align With Purpose”，该方法通过添加一个额外的损失函数来优化模型的一定性能。</li>
<li>results: 该论文在自动语音识别领域中应用了该方法，并实现了在不同的性能指标下提高模型的性能。例如，在释放时间优化中，提高了570毫秒，而word error rate（WER）下降了4.5%。此外，该方法可以在大规模数据上进行扩展，并且可以通过只添加一些代码来实现。<details>
<summary>Abstract</summary>
Connectionist Temporal Classification (CTC) is a widely used criterion for training supervised sequence-to-sequence (seq2seq) models. It enables learning the relations between input and output sequences, termed alignments, by marginalizing over perfect alignments (that yield the ground truth), at the expense of imperfect alignments. This binary differentiation of perfect and imperfect alignments falls short of capturing other essential alignment properties that hold significance in other real-world applications. Here we propose $\textit{Align With Purpose}$, a $\textbf{general Plug-and-Play framework}$ for enhancing a desired property in models trained with the CTC criterion. We do that by complementing the CTC with an additional loss term that prioritizes alignments according to a desired property. Our method does not require any intervention in the CTC loss function, enables easy optimization of a variety of properties, and allows differentiation between both perfect and imperfect alignments. We apply our framework in the domain of Automatic Speech Recognition (ASR) and show its generality in terms of property selection, architectural choice, and scale of training dataset (up to 280,000 hours). To demonstrate the effectiveness of our framework, we apply it to two unrelated properties: emission time and word error rate (WER). For the former, we report an improvement of up to 570ms in latency optimization with a minor reduction in WER, and for the latter, we report a relative improvement of 4.5% WER over the baseline models. To the best of our knowledge, these applications have never been demonstrated to work on a scale of data as large as ours. Notably, our method can be implemented using only a few lines of code, and can be extended to other alignment-free loss functions and to domains other than ASR.
</details>
<details>
<summary>摘要</summary>
Connectionist Temporal Classification (CTC) 是一种广泛使用的训练监督序列到序列（seq2seq）模型的评价标准。它允许学习输入和输出序列之间的关系，称为对齐，通过对完美对齐（导致真实值）进行积分，而抛弃不完美对齐。这个二元对齐分类不足以捕捉其他重要的对齐属性，因此我们提出了 $\textit{Align With Purpose}$，一种通用的插件和替换框架。我们通过补充 CTC 的损失函数中的额外损失项，以便根据某种需要的属性进行对齐优化。我们的方法不需要对 CTC 损失函数进行任何修改，可以轻松地优化多种属性，并允许对不完美对齐进行分类。我们在自动语音识别（ASR）领域应用了我们的框架，并在不同的属性、结构和训练数据规模（最多 280,000 小时）上进行了证明。为了证明我们的框架的有效性，我们在两个不相关的属性上应用了它：发射时间和单词错误率（WER）。对于前者，我们报告了最多 570ms 的延迟优化和相对较小的 WER 降低，对于后者，我们报告了相对于基eline模型的4.5% WER 提高。这些应用都是在我们知道的数据规模上进行的，而且我们的方法只需要几行代码就可以实现，并且可以扩展到其他对齐不受限制的损失函数和领域。
</details></li>
</ul>
<hr>
<h2 id="Distributional-Model-Equivalence-for-Risk-Sensitive-Reinforcement-Learning"><a href="#Distributional-Model-Equivalence-for-Risk-Sensitive-Reinforcement-Learning" class="headerlink" title="Distributional Model Equivalence for Risk-Sensitive Reinforcement Learning"></a>Distributional Model Equivalence for Risk-Sensitive Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01708">http://arxiv.org/abs/2307.01708</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tyler Kastner, Murat A. Erdogdu, Amir-massoud Farahmand</li>
<li>for: 本研究强调学习风险敏感奖励学习模型。</li>
<li>methods: 本文使用分布式奖励学习引入两种新的模型Equivalence定义，一种是通用的，可以用来奖励任何风险度量，但是 computationally intractable; 另一种是实用的，允许用户选择可以奖励的风险度量。</li>
<li>results: 我们的框架可以用来改进任何模型自由风险敏感算法，并在标准和大规模实验中证明其能力。<details>
<summary>Abstract</summary>
We consider the problem of learning models for risk-sensitive reinforcement learning. We theoretically demonstrate that proper value equivalence, a method of learning models which can be used to plan optimally in the risk-neutral setting, is not sufficient to plan optimally in the risk-sensitive setting. We leverage distributional reinforcement learning to introduce two new notions of model equivalence, one which is general and can be used to plan for any risk measure, but is intractable; and a practical variation which allows one to choose which risk measures they may plan optimally for. We demonstrate how our framework can be used to augment any model-free risk-sensitive algorithm, and provide both tabular and large-scale experiments to demonstrate its ability.
</details>
<details>
<summary>摘要</summary>
我们考虑到风险敏感的强化学习问题。我们理论上显示，对于风险中立设定的价值相等方法，不够以来 пла номoptimal 的方式在风险敏感设定中。我们利用分布式强化学习来引入两个新的模型相等性，一个是一般的，可以用来 пла номoptimal 任何风险度量，但是computationally intractable;另一个是实用的，允许选择可以实时最佳化的风险度量。我们显示了我们的框架可以与任何风险敏感无模型学习算法结合，并提供了 Tabular 和大规模实验来证明其能力。
</details></li>
</ul>
<hr>
<h2 id="Online-Learning-and-Solving-Infinite-Games-with-an-ERM-Oracle"><a href="#Online-Learning-and-Solving-Infinite-Games-with-an-ERM-Oracle" class="headerlink" title="Online Learning and Solving Infinite Games with an ERM Oracle"></a>Online Learning and Solving Infinite Games with an ERM Oracle</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01689">http://arxiv.org/abs/2307.01689</a></li>
<li>repo_url: None</li>
<li>paper_authors: Angelos Assos, Idan Attias, Yuval Dagan, Constantinos Daskalakis, Maxwell Fishelson</li>
<li>for: 这个论文主要针对的是在在线学习Setting中，使用ERM oracle calls来实现最佳化的泛化误差和均衡点。</li>
<li>methods: 这篇论文提出了一种基于ERM oracle calls的在线binary分类算法，以及在多 Player游戏中的均衡点算法，这些算法都可以在不同的游戏设定中实现最佳化的性能。</li>
<li>results: 论文表明了这种算法在可 réalisable Setting中有finite regret，在agnostic Setting中具有sublinearly growing regret，并且可以在不同的游戏设定中实现最佳化的性能，其性能与游戏的Littlestone和阈值维度有关。<details>
<summary>Abstract</summary>
While ERM suffices to attain near-optimal generalization error in the stochastic learning setting, this is not known to be the case in the online learning setting, where algorithms for general concept classes rely on computationally inefficient oracles such as the Standard Optimal Algorithm (SOA). In this work, we propose an algorithm for online binary classification setting that relies solely on ERM oracle calls, and show that it has finite regret in the realizable setting and sublinearly growing regret in the agnostic setting. We bound the regret in terms of the Littlestone and threshold dimensions of the underlying concept class.   We obtain similar results for nonparametric games, where the ERM oracle can be interpreted as a best response oracle, finding the best response of a player to a given history of play of the other players. In this setting, we provide learning algorithms that only rely on best response oracles and converge to approximate-minimax equilibria in two-player zero-sum games and approximate coarse correlated equilibria in multi-player general-sum games, as long as the game has a bounded fat-threshold dimension. Our algorithms apply to both binary-valued and real-valued games and can be viewed as providing justification for the wide use of double oracle and multiple oracle algorithms in the practice of solving large games.
</details>
<details>
<summary>摘要</summary>
在随机学习设定下，ERM 已经足够保证逼近优化的泛化误差，但在在线学习设定下，算法们尚未知道是否可以达到优化的泛化误差。在这项工作中，我们提出了基于 ERM  oracle 的在线二分类Setting 的算法，并证明其在可 realizable 设定下有finite regret，在agnostical 设定下有sublinearly growing regret。我们 bound regret 的大小与underlying 概念类型的 Littlestone 和阈值维度。在非参数学习游戏中，我们可以将 ERM oracle 解释为最佳回应 oracle，找到对某个玩家的历史玩家的最佳回应。在这个设定下，我们提供了基于最佳回应 oracle 的学习算法，可以在两 player zero-sum 游戏和多 player general-sum 游戏中达到approximate-minimax equilibria和approximate coarse correlated equilibria，只要游戏有 bounded fat-threshold dimension。我们的算法适用于 binary-valued 和 real-valued 游戏，可以视为对 double oracle 和多 oracle 算法在实践中的广泛使用提供 justify。
</details></li>
</ul>
<hr>
<h2 id="Serving-Graph-Neural-Networks-With-Distributed-Fog-Servers-For-Smart-IoT-Services"><a href="#Serving-Graph-Neural-Networks-With-Distributed-Fog-Servers-For-Smart-IoT-Services" class="headerlink" title="Serving Graph Neural Networks With Distributed Fog Servers For Smart IoT Services"></a>Serving Graph Neural Networks With Distributed Fog Servers For Smart IoT Services</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01684">http://arxiv.org/abs/2307.01684</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liekang Zeng, Xu Chen, Peng Huang, Ke Luo, Xiaoxi Zhang, Zhi Zhou<br>for:Fograph is designed to provide real-time GNN inference for IoT-driven smart applications, leveraging the resources of multiple fog nodes to reduce communication overhead and improve performance.methods:Fograph employs heterogeneity-aware execution planning and GNN-specific compression techniques to optimize the performance of GNN inference in fog environments.results:Compared to state-of-the-art cloud serving and fog deployment, Fograph achieves up to 5.39x execution speedup and 6.84x throughput improvement, demonstrating its effectiveness in improving the performance of GNN-based services for IoT applications.<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have gained growing interest in miscellaneous applications owing to their outstanding ability in extracting latent representation on graph structures. To render GNN-based service for IoT-driven smart applications, traditional model serving paradigms usually resort to the cloud by fully uploading geo-distributed input data to remote datacenters. However, our empirical measurements reveal the significant communication overhead of such cloud-based serving and highlight the profound potential in applying the emerging fog computing. To maximize the architectural benefits brought by fog computing, in this paper, we present Fograph, a novel distributed real-time GNN inference framework that leverages diverse and dynamic resources of multiple fog nodes in proximity to IoT data sources. By introducing heterogeneity-aware execution planning and GNN-specific compression techniques, Fograph tailors its design to well accommodate the unique characteristics of GNN serving in fog environments. Prototype-based evaluation and case study demonstrate that Fograph significantly outperforms the state-of-the-art cloud serving and fog deployment by up to 5.39x execution speedup and 6.84x throughput improvement.
</details>
<details>
<summary>摘要</summary>
граф neural networks (GNNs) 在不同的应用领域获得了不断增长的兴趣，主要是因为它们在图结构上能够激发出优秀的隐藏表示。为了在基于 IoT 的智能应用中提供 GNN 服务，传统的模型服务方式通常是通过完全上传到远程数据中心进行云计算。然而，我们的实验表明，这种云计算中的通信开销很大，而 fog 计算的出现也提供了一个可能性。为了最大化fog计算中的建筑减少开销，在这篇论文中，我们提出了一种分布式实时 GNN 推理框架，称之为 Fograph。 Fograph 利用了多个 fog 节点的多样化和动态资源，以便在 IoT 数据源附近进行 GNN 推理。通过对异质性的执行规划和 GNN 特定压缩技术，Fograph 的设计与 fog 环境中 GNN 服务的特点相匹配。实验和案例研究表明，Fograph 在比较云服务和 fog 部署时可以达到5.39倍的执行速度提升和6.84倍的吞吐量提高。
</details></li>
</ul>
<hr>
<h2 id="Learning-Discrete-Weights-and-Activations-Using-the-Local-Reparameterization-Trick"><a href="#Learning-Discrete-Weights-and-Activations-Using-the-Local-Reparameterization-Trick" class="headerlink" title="Learning Discrete Weights and Activations Using the Local Reparameterization Trick"></a>Learning Discrete Weights and Activations Using the Local Reparameterization Trick</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01683">http://arxiv.org/abs/2307.01683</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guy Berger, Aviv Navon, Ethan Fetaya</li>
<li>for: 降低计算机视和机器学习中的神经网络推断 computation和存储需求</li>
<li>methods: 使用二进制化来减少神经网络推断的计算复杂性，并通过使用比较快的位运算来替代慢速的浮点运算</li>
<li>results: 实现了降低计算机视和机器学习中神经网络推断的时间和内存占用，并达到了当前最佳性能的二进制激活神经网络推断<details>
<summary>Abstract</summary>
In computer vision and machine learning, a crucial challenge is to lower the computation and memory demands for neural network inference. A commonplace solution to address this challenge is through the use of binarization. By binarizing the network weights and activations, one can significantly reduce computational complexity by substituting the computationally expensive floating operations with faster bitwise operations. This leads to a more efficient neural network inference that can be deployed on low-resource devices. In this work, we extend previous approaches that trained networks with discrete weights using the local reparameterization trick to also allow for discrete activations. The original approach optimized a distribution over the discrete weights and uses the central limit theorem to approximate the pre-activation with a continuous Gaussian distribution. Here we show that the probabilistic modeling can also allow effective training of networks with discrete activation as well. This further reduces runtime and memory footprint at inference time with state-of-the-art results for networks with binary activations.
</details>
<details>
<summary>摘要</summary>
在计算机视觉和机器学习中，一个重要挑战是降低神经网络推理的计算和内存需求。一种常见的解决方案是通过 binarization 来实现这一目标。通过将神经网络权重和活动化值binarized，可以在替换计算昂贵的浮点运算时大幅降低计算复杂性。这会导致更高效的神经网络推理，可以在低资源设备上部署。在这项工作中，我们extend了之前的方法，使得神经网络可以使用随机变量的批处理技术进行训练，而不是通过精确的权重值来进行训练。我们原始的方法是使用中心假设定理来近似预Activation的Continuous Gaussian Distribution。这里我们表明，可以通过概率模型来有效地训练具有随机变量的神经网络。这会进一步降低执行时间和内存占用，并且在state-of-the-art 的结果下，对于具有二进制活动化的神经网络进行推理。
</details></li>
</ul>
<hr>
<h2 id="Training-Energy-Based-Models-with-Diffusion-Contrastive-Divergences"><a href="#Training-Energy-Based-Models-with-Diffusion-Contrastive-Divergences" class="headerlink" title="Training Energy-Based Models with Diffusion Contrastive Divergences"></a>Training Energy-Based Models with Diffusion Contrastive Divergences</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01668">http://arxiv.org/abs/2307.01668</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weijian Luo, Hao Jiang, Tianyang Hu, Jiacheng Sun, Zhenguo Li, Zhihua Zhang</li>
<li>for: 这个论文主要针对的问题是如何改进对能量基模型（EBM）的训练方法，以提高EBM的生成能力和效率。</li>
<li>methods: 这个论文提出了一种新的启发对EBM的训练方法，即Diffusion Contrastive Divergence（DCD），它将Langevin dynamic更换为其他EBM参数自由的扩散过程。这种方法可以更高效地进行训练，并且不受非可忽略的梯度项的限制。</li>
<li>results: 作者在实验中表明，提出的DCD方法可以在生成数据集和高维图像噪声除除和生成任务中表现出色，比CD更高效和稳定。此外，DCD还能够训练EBM来生成Celab-A $32\times 32$数据集，与现有EBM相当。<details>
<summary>Abstract</summary>
Energy-Based Models (EBMs) have been widely used for generative modeling. Contrastive Divergence (CD), a prevailing training objective for EBMs, requires sampling from the EBM with Markov Chain Monte Carlo methods (MCMCs), which leads to an irreconcilable trade-off between the computational burden and the validity of the CD. Running MCMCs till convergence is computationally intensive. On the other hand, short-run MCMC brings in an extra non-negligible parameter gradient term that is difficult to handle. In this paper, we provide a general interpretation of CD, viewing it as a special instance of our proposed Diffusion Contrastive Divergence (DCD) family. By replacing the Langevin dynamic used in CD with other EBM-parameter-free diffusion processes, we propose a more efficient divergence. We show that the proposed DCDs are both more computationally efficient than the CD and are not limited to a non-negligible gradient term. We conduct intensive experiments, including both synthesis data modeling and high-dimensional image denoising and generation, to show the advantages of the proposed DCDs. On the synthetic data learning and image denoising experiments, our proposed DCD outperforms CD by a large margin. In image generation experiments, the proposed DCD is capable of training an energy-based model for generating the Celab-A $32\times 32$ dataset, which is comparable to existing EBMs.
</details>
<details>
<summary>摘要</summary>
能量基模型（EBM）在生成模型方面广泛使用。对比差分泵（CD）是EBM训练的主要目标函数，但是使用Markov链约化 Monte Carlo方法（MCMC）来采样EBM，导致计算成本和验证CD之间存在不可 reconcile的负担。在MCMC运行至收敛之前，计算成本很高；另一方面，使用短跑MCMC会带来额外的非可忽略的参数梯度项，而且难以处理。在本文中，我们提供了CD的普遍解释，视其为我们提议的噪声对照分布（DCD）家族的特例。我们将CD中使用的朗格温动力换用其他EBM参数无关的扩散过程，并提出了更高效的分离。我们表明，我们提议的DCD比CD更高效，而且不受非可忽略的参数梯度项的限制。我们进行了广泛的实验，包括生成数据模型和高维图像减震和生成，以显示我们的提议DCD的优势。在生成数据学习和图像减震实验中，我们的提议DCD比CD大幅提高。在图像生成实验中，我们的提议DCD可以训练一个能量基模型，用于生成Celab-A $32\times 32$ 数据集，与现有EBM相当。
</details></li>
</ul>
<hr>
<h2 id="Nonparametric-Classification-on-Low-Dimensional-Manifolds-using-Overparameterized-Convolutional-Residual-Networks"><a href="#Nonparametric-Classification-on-Low-Dimensional-Manifolds-using-Overparameterized-Convolutional-Residual-Networks" class="headerlink" title="Nonparametric Classification on Low Dimensional Manifolds using Overparameterized Convolutional Residual Networks"></a>Nonparametric Classification on Low Dimensional Manifolds using Overparameterized Convolutional Residual Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01649">http://arxiv.org/abs/2307.01649</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaiqi Zhang, Zixuan Zhang, Minshuo Chen, Mengdi Wang, Tuo Zhao, Yu-Xiang Wang</li>
<li>for: 这 paper 的目的是研究 Convolutional residual neural networks (ConvResNets) 的性能，并解释它们在实践中的出色预测能力，不能由 conventional wisdom 解释。</li>
<li>methods: 这 paper 使用 weight decay 来研究 ConvResNeXts 的表现，从非Parametric classification 的角度来看。</li>
<li>results: 研究表明，ConvResNeXts 可以具有高精度的预测性能，并且可以有效地适应函数的柔和性和低维度结构。<details>
<summary>Abstract</summary>
Convolutional residual neural networks (ConvResNets), though overparameterized, can achieve remarkable prediction performance in practice, which cannot be well explained by conventional wisdom. To bridge this gap, we study the performance of ConvResNeXts, which cover ConvResNets as a special case, trained with weight decay from the perspective of nonparametric classification. Our analysis allows for infinitely many building blocks in ConvResNeXts, and shows that weight decay implicitly enforces sparsity on these blocks. Specifically, we consider a smooth target function supported on a low-dimensional manifold, then prove that ConvResNeXts can adapt to the function smoothness and low-dimensional structures and efficiently learn the function without suffering from the curse of dimensionality. Our findings partially justify the advantage of overparameterized ConvResNeXts over conventional machine learning models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="SwinGNN-Rethinking-Permutation-Invariance-in-Diffusion-Models-for-Graph-Generation"><a href="#SwinGNN-Rethinking-Permutation-Invariance-in-Diffusion-Models-for-Graph-Generation" class="headerlink" title="SwinGNN: Rethinking Permutation Invariance in Diffusion Models for Graph Generation"></a>SwinGNN: Rethinking Permutation Invariance in Diffusion Models for Graph Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01646">http://arxiv.org/abs/2307.01646</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qiyan98/swingnn">https://github.com/qiyan98/swingnn</a></li>
<li>paper_authors: Qi Yan, Zhengyang Liang, Yang Song, Renjie Liao, Lele Wang</li>
<li>for: 本文旨在提出一种基于卷积神经网络的非对称扩散模型，用于学习图数据上的非对称分布。</li>
<li>methods: 该模型使用高效的边到边2-WL消息传递网络，并利用Shifted Window基于SwinTransformers的自注意机制。</li>
<li>results: 经过系统的ablations和训练技巧优化，我们的SwinGNN在synthetic和实际的蛋白质和分子数据上达到了顶尖性能。<details>
<summary>Abstract</summary>
Diffusion models based on permutation-equivariant networks can learn permutation-invariant distributions for graph data. However, in comparison to their non-invariant counterparts, we have found that these invariant models encounter greater learning challenges since 1) their effective target distributions exhibit more modes; 2) their optimal one-step denoising scores are the score functions of Gaussian mixtures with more components. Motivated by this analysis, we propose a non-invariant diffusion model, called $\textit{SwinGNN}$, which employs an efficient edge-to-edge 2-WL message passing network and utilizes shifted window based self-attention inspired by SwinTransformers. Further, through systematic ablations, we identify several critical training and sampling techniques that significantly improve the sample quality of graph generation. At last, we introduce a simple post-processing trick, $\textit{i.e.}$, randomly permuting the generated graphs, which provably converts any graph generative model to a permutation-invariant one. Extensive experiments on synthetic and real-world protein and molecule datasets show that our SwinGNN achieves state-of-the-art performances. Our code is released at https://github.com/qiyan98/SwinGNN.
</details>
<details>
<summary>摘要</summary>
Diffusion models based on permutation-equivariant networks can learn permutation-invariant distributions for graph data. However, in comparison to their non-invariant counterparts, we have found that these invariant models encounter greater learning challenges since 1) their effective target distributions exhibit more modes; 2) their optimal one-step denoising scores are the score functions of Gaussian mixtures with more components. Motivated by this analysis, we propose a non-invariant diffusion model, called $\textit{SwinGNN}$, which employs an efficient edge-to-edge 2-WL message passing network and utilizes shifted window based self-attention inspired by SwinTransformers. Further, through systematic ablations, we identify several critical training and sampling techniques that significantly improve the sample quality of graph generation. At last, we introduce a simple post-processing trick, $\textit{i.e.}$, randomly permuting the generated graphs, which provably converts any graph generative model to a permutation-invariant one. Extensive experiments on synthetic and real-world protein and molecule datasets show that our SwinGNN achieves state-of-the-art performances. Our code is released at https://github.com/qiyan98/SwinGNN.Here's the translation in Traditional Chinese:Diffusion models based on permutation-equivariant networks can learn permutation-invariant distributions for graph data. However, in comparison to their non-invariant counterparts, we have found that these invariant models encounter greater learning challenges since 1) their effective target distributions exhibit more modes; 2) their optimal one-step denoising scores are the score functions of Gaussian mixtures with more components. Motivated by this analysis, we propose a non-invariant diffusion model, called $\textit{SwinGNN}$, which employs an efficient edge-to-edge 2-WL message passing network and utilizes shifted window based self-attention inspired by SwinTransformers. Further, through systematic ablations, we identify several critical training and sampling techniques that significantly improve the sample quality of graph generation. At last, we introduce a simple post-processing trick, $\textit{i.e.}$, randomly permuting the generated graphs, which provably converts any graph generative model to a permutation-invariant one. Extensive experiments on synthetic and real-world protein and molecule datasets show that our SwinGNN achieves state-of-the-art performances. Our code is released at https://github.com/qiyan98/SwinGNN.
</details></li>
</ul>
<hr>
<h2 id="Heuristic-Algorithms-for-the-Approximation-of-Mutual-Coherence"><a href="#Heuristic-Algorithms-for-the-Approximation-of-Mutual-Coherence" class="headerlink" title="Heuristic Algorithms for the Approximation of Mutual Coherence"></a>Heuristic Algorithms for the Approximation of Mutual Coherence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01639">http://arxiv.org/abs/2307.01639</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gregor Betz, Vera Chekan, Tamara Mchedlidze</li>
<li>for: This paper is written for those interested in efficient computation of mutual coherence, particularly in the context of political preference matching systems like Wahl-O-Mat.</li>
<li>methods: The paper presents several heuristics to estimate the model parameters of a mixture of three Gaussians distribution, which is used to approximate the mutual coherence. Some of the algorithms are fully polynomial-time, while others require solving a small number of instances of the SAT model counting problem.</li>
<li>results: The paper reports the average squared error of the best algorithm, which is below 0.0035, indicating a high degree of accuracy while also being efficient. The results are precise enough to be used in Wahl-O-Mat-like systems.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文是为了提高共谐度的计算效率而写的，特别是在政治偏好匹配系统中使用。</li>
<li>methods: 论文提出了一些用于估计三个 Gaussian 分布的模型参数的快速算法，其中一些是完全 полиномиаль时间的，而另一些只需解决一些 SAT 模型计数问题。</li>
<li>results: 论文报告了最佳算法的平均平方误差，为0.0035以下，表明高度准确并且高效。结果可以用于 Wahl-O-Mat 类系统中。<details>
<summary>Abstract</summary>
Mutual coherence is a measure of similarity between two opinions. Although the notion comes from philosophy, it is essential for a wide range of technologies, e.g., the Wahl-O-Mat system. In Germany, this system helps voters to find candidates that are the closest to their political preferences. The exact computation of mutual coherence is highly time-consuming due to the iteration over all subsets of an opinion. Moreover, for every subset, an instance of the SAT model counting problem has to be solved which is known to be a hard problem in computer science. This work is the first study to accelerate this computation. We model the distribution of the so-called confirmation values as a mixture of three Gaussians and present efficient heuristics to estimate its model parameters. The mutual coherence is then approximated with the expected value of the distribution. Some of the presented algorithms are fully polynomial-time, others only require solving a small number of instances of the SAT model counting problem. The average squared error of our best algorithm lies below 0.0035 which is insignificant if the efficiency is taken into account. Furthermore, the accuracy is precise enough to be used in Wahl-O-Mat-like systems.
</details>
<details>
<summary>摘要</summary>
互相协调是两个意见之间的相似度度量。这个概念起源于哲学，但是它对广泛的技术领域都很重要，例如德国的 Wahl-O-Mat 系统。这个系统帮助选民找到最符合其政治偏好的候选人。计算互相协调的精确方法需要遍历所有意见的所有子集，并解决每个子集的 SAT 模型计数问题，这是计算机科学中知名的困难问题。这项工作是首次加速这种计算的研究。我们模型了确认值的分布为三个高斯分布的混合，并提供了高效的启发式来估算模型参数。然后，我们使用分布的期望值来 aproximate 互相协调。一些我们提出的算法是完全多项式时间的，其他些只需解决一些 SAT 模型计数问题。我们的最佳算法的平均方差平方误差低于 0.0035，这对于效率来说是无意义的。此外，我们的精度够精确，可以用于 Wahl-O-Mat 类系统。
</details></li>
</ul>
<hr>
<h2 id="HAGNN-Hybrid-Aggregation-for-Heterogeneous-Graph-Neural-Networks"><a href="#HAGNN-Hybrid-Aggregation-for-Heterogeneous-Graph-Neural-Networks" class="headerlink" title="HAGNN: Hybrid Aggregation for Heterogeneous Graph Neural Networks"></a>HAGNN: Hybrid Aggregation for Heterogeneous Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01636">http://arxiv.org/abs/2307.01636</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guanghui Zhu, Zhennan Zhu, Hongyang Chen, Chunfeng Yuan, Yihua Huang</li>
<li>for:  Handle heterogeneous graphs with rich type semantic information.</li>
<li>methods: 	+ Propose a novel framework called HAGNN (Hybrid Aggregation for Heterogeneous GNNs)	+ Leverage both meta-path neighbors and directly connected neighbors for node aggregation	+ Divide the aggregation process into two phases: meta-path-based intra-type aggregation and meta-path-free inter-type aggregation	+ Use a new data structure called fused meta-path graph for intra-type aggregation	+ Perform structural semantic aware aggregation</li>
<li>results: 	+ Outperform existing heterogeneous GNN models on node classification, node clustering, and link prediction tasks	+ Demonstrate the effectiveness of HAGNN in handling heterogeneous graphs with rich type semantic information.<details>
<summary>Abstract</summary>
Heterogeneous graph neural networks (GNNs) have been successful in handling heterogeneous graphs. In existing heterogeneous GNNs, meta-path plays an essential role. However, recent work pointed out that simple homogeneous graph model without meta-path can also achieve comparable results, which calls into question the necessity of meta-path. In this paper, we first present the intrinsic difference about meta-path-based and meta-path-free models, i.e., how to select neighbors for node aggregation. Then, we propose a novel framework to utilize the rich type semantic information in heterogeneous graphs comprehensively, namely HAGNN (Hybrid Aggregation for Heterogeneous GNNs). The core of HAGNN is to leverage the meta-path neighbors and the directly connected neighbors simultaneously for node aggregations. HAGNN divides the overall aggregation process into two phases: meta-path-based intra-type aggregation and meta-path-free inter-type aggregation. During the intra-type aggregation phase, we propose a new data structure called fused meta-path graph and perform structural semantic aware aggregation on it. Finally, we combine the embeddings generated by each phase. Compared with existing heterogeneous GNN models, HAGNN can take full advantage of the heterogeneity in heterogeneous graphs. Extensive experimental results on node classification, node clustering, and link prediction tasks show that HAGNN outperforms the existing modes, demonstrating the effectiveness of HAGNN.
</details>
<details>
<summary>摘要</summary>
《异类图 neural network（GNN）在处理异类图方面取得成功。现有的异类GNN中，元路扮演着关键性的角色。然而，最近的研究表明，简单的同类图模型无需元路可以达到相似的结果，这意味着元路的必要性被质疑。在这篇论文中，我们首先介绍异类GNN中元路和无元路两种模型之间的本质差异，即如何选择节点 для节点聚合。然后，我们提出了一种新的框架，即Hybrid Aggregation for Heterogeneous GNNs（异类GNN中的混合聚合），用于全面利用异类图中各种类型Semantic信息。核心思想是同时利用元路邻居和直接连接邻居进行节点聚合。我们将整个聚合过程分成两个阶段：元路基于的内部聚合和元路无的交叉聚合。在内部聚合阶段，我们提出了一种新的数据结构called fused meta-path graph，并在其上进行结构层次意识感知聚合。最后，我们将每个阶段生成的embeddings合并。与现有的异类GNN模型相比，HAGNN可以全面利用异类图中的异类性。我们在节点分类、节点封顶和链接预测任务上进行了广泛的实验，结果显示，HAGNN在这些任务上表现出了更好的效果，证明了HAGNN的效果。》
</details></li>
</ul>
<hr>
<h2 id="Renewable-energy-management-in-smart-home-environment-via-forecast-embedded-scheduling-based-on-Recurrent-Trend-Predictive-Neural-Network"><a href="#Renewable-energy-management-in-smart-home-environment-via-forecast-embedded-scheduling-based-on-Recurrent-Trend-Predictive-Neural-Network" class="headerlink" title="Renewable energy management in smart home environment via forecast embedded scheduling based on Recurrent Trend Predictive Neural Network"></a>Renewable energy management in smart home environment via forecast embedded scheduling based on Recurrent Trend Predictive Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01622">http://arxiv.org/abs/2307.01622</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mertnakip/Recurrent-Trend-Predictive-Neural-Network">https://github.com/mertnakip/Recurrent-Trend-Predictive-Neural-Network</a></li>
<li>paper_authors: Mert Nakıp, Onur Çopur, Emrah Biyik, Cüneyt Güzeliş</li>
<li>For: The paper proposes an advanced machine learning algorithm for efficient residential demand control in smart home energy management systems.* Methods: The proposed algorithm, called Recurrent Trend Predictive Neural Network based Forecast Embedded Scheduling (rTPNN-FES), simultaneously forecasts renewable energy generation and schedules household appliances, eliminating the need for separate algorithms.* Results: The evaluation results show that rTPNN-FES provides near-optimal scheduling $37.5$ times faster than optimization while outperforming state-of-the-art forecasting techniques.Here is the same information in Simplified Chinese:* For: 本文提出了一种高效的机器学习算法，用于智能家庭能源管理系统中的居民需求控制。* Methods: 提议的算法是基于循环趋势预测神经网络的预测嵌入式调度算法（rTPNN-FES），同时预测可再生能源生产和家庭电器的调度。* Results: 评估结果显示，rTPNN-FES可以在优化过程中提供近似优化的调度，比起现有预测技术要高效，并且在37.5倍 faster than optimization。<details>
<summary>Abstract</summary>
Smart home energy management systems help the distribution grid operate more efficiently and reliably, and enable effective penetration of distributed renewable energy sources. These systems rely on robust forecasting, optimization, and control/scheduling algorithms that can handle the uncertain nature of demand and renewable generation. This paper proposes an advanced ML algorithm, called Recurrent Trend Predictive Neural Network based Forecast Embedded Scheduling (rTPNN-FES), to provide efficient residential demand control. rTPNN-FES is a novel neural network architecture that simultaneously forecasts renewable energy generation and schedules household appliances. By its embedded structure, rTPNN-FES eliminates the utilization of separate algorithms for forecasting and scheduling and generates a schedule that is robust against forecasting errors. This paper also evaluates the performance of the proposed algorithm for an IoT-enabled smart home. The evaluation results reveal that rTPNN-FES provides near-optimal scheduling $37.5$ times faster than the optimization while outperforming state-of-the-art forecasting techniques.
</details>
<details>
<summary>摘要</summary>
智能家庭能源管理系统可以使分布式电力网络运行更加高效和可靠，并允许有效地推进分布式可再生能源源。这些系统需要可靠的预测、优化和控制/调度算法，以处理各种不确定的需求和可再生能源生产。本文提出了一种高级的机器学习算法，即循环趋势预测神经网络基于预测嵌入的调度算法（rTPNN-FES），以提供高效的家庭需求控制。rTPNN-FES是一种新的神经网络架构，同时预测可再生能源生产和调度家用电器。由嵌入结构，rTPNN-FES消除了分离的预测和调度算法，生成一个强健对预测错误的负荷调度。本文还评估了提议的算法在智能家庭上的性能。评估结果显示，rTPNN-FES提供了近似优化的调度，比传统预测技术更高效，并且比优化算法更快，每秒37.5次。
</details></li>
</ul>
<hr>
<h2 id="SageFormer-Series-Aware-Graph-Enhanced-Transformers-for-Multivariate-Time-Series-Forecasting"><a href="#SageFormer-Series-Aware-Graph-Enhanced-Transformers-for-Multivariate-Time-Series-Forecasting" class="headerlink" title="SageFormer: Series-Aware Graph-Enhanced Transformers for Multivariate Time Series Forecasting"></a>SageFormer: Series-Aware Graph-Enhanced Transformers for Multivariate Time Series Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01616">http://arxiv.org/abs/2307.01616</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenwei Zhang, Xin Wang, Yuantao Gu</li>
<li>for: 本研究旨在提高多ivariate时间序列预测中的深度学习方法，尤其是Transformers的应用。</li>
<li>methods: 本paper引入了Series-aware Graph-enhanced Transformer模型，用于有效地捕捉和模型系列之间的依赖关系。</li>
<li>results: 经过广泛的实验研究，本paper显示了SageFormer模型在实际数据和 sintetic dataset上的superior性能，比之前的状态之 искусственный智能方法更高。<details>
<summary>Abstract</summary>
Multivariate time series forecasting plays a critical role in diverse domains. While recent advancements in deep learning methods, especially Transformers, have shown promise, there remains a gap in addressing the significance of inter-series dependencies. This paper introduces SageFormer, a Series-aware Graph-enhanced Transformer model designed to effectively capture and model dependencies between series using graph structures. SageFormer tackles two key challenges: effectively representing diverse temporal patterns across series and mitigating redundant information among series. Importantly, the proposed series-aware framework seamlessly integrates with existing Transformer-based models, augmenting their ability to model inter-series dependencies. Through extensive experiments on real-world and synthetic datasets, we showcase the superior performance of SageFormer compared to previous state-of-the-art approaches.
</details>
<details>
<summary>摘要</summary>
多ivariate时间序列预测在多个领域发挥关键作用。最近的深度学习方法，特别是Transformers，已经显示了承诺，但还有一个差距在处理多个时间序列之间的相互依赖关系。这篇论文引入了SageFormer，一种基于图结构的Series-aware Graph-enhanced Transformer模型，用于有效地捕捉和模型多个时间序列之间的依赖关系。SageFormer解决了两个关键挑战：一是有效地表示多个时间序列中的多样化时间模式，二是避免多个时间序列之间的重复信息。重要的是，我们提出的系列意识框架可以轻松地与现有的Transformer-based模型结合使用，以提高对多个时间序列之间的依赖关系的模型。通过对真实世界和 sintetic 数据集进行广泛的实验，我们展示了SageFormer比前一个状态的方法更高效。
</details></li>
</ul>
<hr>
<h2 id="Overconfidence-is-a-Dangerous-Thing-Mitigating-Membership-Inference-Attacks-by-Enforcing-Less-Confident-Prediction"><a href="#Overconfidence-is-a-Dangerous-Thing-Mitigating-Membership-Inference-Attacks-by-Enforcing-Less-Confident-Prediction" class="headerlink" title="Overconfidence is a Dangerous Thing: Mitigating Membership Inference Attacks by Enforcing Less Confident Prediction"></a>Overconfidence is a Dangerous Thing: Mitigating Membership Inference Attacks by Enforcing Less Confident Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01610">http://arxiv.org/abs/2307.01610</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dependablesystemslab/mia_defense_hamp">https://github.com/dependablesystemslab/mia_defense_hamp</a></li>
<li>paper_authors: Zitao Chen, Karthik Pattabiraman</li>
<li>For: This paper is written to address the problem of membership inference attacks (MIAs) on machine learning (ML) models, which can compromise the privacy of training data. The paper proposes a defense technique called HAMP that can provide strong membership privacy and high accuracy without requiring additional data.* Methods: The HAMP technique consists of a novel training framework with high-entropy soft labels and an entropy-based regularizer to constrain the model’s prediction while still achieving high accuracy. The technique also modifies all prediction outputs to become low-confidence outputs, effectively obscuring the differences between the prediction on members and non-members.* Results: The paper conducts extensive evaluation on five benchmark datasets and shows that HAMP provides consistently high accuracy and strong membership privacy, outperforming seven state-of-the-art defenses in terms of privacy-utility trade-off.<details>
<summary>Abstract</summary>
Machine learning (ML) models are vulnerable to membership inference attacks (MIAs), which determine whether a given input is used for training the target model. While there have been many efforts to mitigate MIAs, they often suffer from limited privacy protection, large accuracy drop, and/or requiring additional data that may be difficult to acquire. This work proposes a defense technique, HAMP that can achieve both strong membership privacy and high accuracy, without requiring extra data. To mitigate MIAs in different forms, we observe that they can be unified as they all exploit the ML model's overconfidence in predicting training samples through different proxies. This motivates our design to enforce less confident prediction by the model, hence forcing the model to behave similarly on the training and testing samples. HAMP consists of a novel training framework with high-entropy soft labels and an entropy-based regularizer to constrain the model's prediction while still achieving high accuracy. To further reduce privacy risk, HAMP uniformly modifies all the prediction outputs to become low-confidence outputs while preserving the accuracy, which effectively obscures the differences between the prediction on members and non-members. We conduct extensive evaluation on five benchmark datasets, and show that HAMP provides consistently high accuracy and strong membership privacy. Our comparison with seven state-of-the-art defenses shows that HAMP achieves a superior privacy-utility trade off than those techniques.
</details>
<details>
<summary>摘要</summary>
为了mitigate MIA的不同形式，我们发现它们都利用了 ML 模型对训练样本的过于自信的预测，通过不同的代理来实现。这种情况使我们设计了一种强制模型在训练和测试样本上具有相同的预测行为的方法。HAMP 包括一种新的训练框架，高级 entropy 软标签和一种基于 entropy 的 regularizer，以防止模型的预测，同时仍然实现高准确率。为了进一步减少隐私风险，HAMP 对所有预测输出进行了一致的低信任输出修改，使模型的预测结果变得模拟，从而隐藏了训练和测试样本之间的差异。我们对五个 benchmark 数据集进行了广泛的评估，并显示了 HAMP 可以在高准确率和强大的成员隐私之间取得平衡。我们与七种 state-of-the-art 防御技术进行比较，发现 HAMP 在隐私利用与实用性之间取得了更好的平衡。
</details></li>
</ul>
<hr>
<h2 id="Prototypes-as-Explanation-for-Time-Series-Anomaly-Detection"><a href="#Prototypes-as-Explanation-for-Time-Series-Anomaly-Detection" class="headerlink" title="Prototypes as Explanation for Time Series Anomaly Detection"></a>Prototypes as Explanation for Time Series Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01601">http://arxiv.org/abs/2307.01601</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bin Li, Carsten Jentsch, Emmanuel Müller</li>
<li>for: 本文针对时间序列资料中的异常模式探测，尤其是在没有标签的情况下，时间序列资料的动态性和未料到的异常行为导致探测过程具有挑战性。</li>
<li>methods: 本文提出了ProtoAD方法，利用示例来解释深度黑盒模型中的异常探测过程。在不对探测性能有重要影响的情况下，示例提供了深度黑盒模型中的透彻关键，并提供了域专家和投资者对模型的直觉理解。</li>
<li>results: 本文extend了广泛使用的示例学习在分类问题上的应用，并将其推广到异常探测问题上。通过视觉化示例的latent空间和输入空间，我们直观地解释了常规资料如何被模型，并且解释了具体的异常模式是如何被识别为异常的。<details>
<summary>Abstract</summary>
Detecting abnormal patterns that deviate from a certain regular repeating pattern in time series is essential in many big data applications. However, the lack of labels, the dynamic nature of time series data, and unforeseeable abnormal behaviors make the detection process challenging. Despite the success of recent deep anomaly detection approaches, the mystical mechanisms in such black-box models have become a new challenge in safety-critical applications. The lack of model transparency and prediction reliability hinders further breakthroughs in such domains. This paper proposes ProtoAD, using prototypes as the example-based explanation for the state of regular patterns during anomaly detection. Without significant impact on the detection performance, prototypes shed light on the deep black-box models and provide intuitive understanding for domain experts and stakeholders. We extend the widely used prototype learning in classification problems into anomaly detection. By visualizing both the latent space and input space prototypes, we intuitively demonstrate how regular data are modeled and why specific patterns are considered abnormal.
</details>
<details>
<summary>摘要</summary>
检测时序序数据中异常模式的检测是许多大数据应用场景中的关键问题。然而，缺乏标签、时序序数据的动态性和未预期的异常行为使检测过程具有挑战性。虽然最近的深度异常检测方法已经取得了成功，但这些黑盒模型中的神秘机制成为了新的挑战。模型的不透明度和预测可靠性限制了进一步的突破。本文提出了ProtoAD，使用模型为异常检测中的示例基本解释。无需对检测性能产生显著影响，示例揭示了深度黑盒模型的内部机制，提供了域专家和投资者Intuitive的理解。我们将通用的 prototype 学习在分类问题中扩展到异常检测。通过视觉化 latent space 和输入空间示例，我们直观地解释了如何模型正常数据和哪些特定模式被视为异常。
</details></li>
</ul>
<hr>
<h2 id="A-Scalable-Reinforcement-Learning-based-System-Using-On-Chain-Data-for-Cryptocurrency-Portfolio-Management"><a href="#A-Scalable-Reinforcement-Learning-based-System-Using-On-Chain-Data-for-Cryptocurrency-Portfolio-Management" class="headerlink" title="A Scalable Reinforcement Learning-based System Using On-Chain Data for Cryptocurrency Portfolio Management"></a>A Scalable Reinforcement Learning-based System Using On-Chain Data for Cryptocurrency Portfolio Management</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01599">http://arxiv.org/abs/2307.01599</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenhan Huang, Fumihide Tanaka</li>
<li>For: The paper is written for proposing a novel reinforcement learning-based system for cryptocurrency portfolio management that incorporates on-chain data for end-to-end management.* Methods: The paper uses on-chain data to train a reinforcement learning model for cryptocurrency portfolio management, and the model is tested and evaluated using backtesting results on three portfolios.* Results: The results show that the proposed CryptoRLPM system outperforms all baselines in terms of accumulated rate of return, daily rate of return, and Sortino ratio, with an enhancement of at least 83.14%, 0.5603%, and 2.1767 respectively compared to Bitcoin.Here are the three points in Simplified Chinese text:</li>
<li>for: 这篇论文是为了提出一种基于强化学习的 криптовалю端folio管理系统，该系统包括了链上数据的测试和评估。</li>
<li>methods: 论文使用链上数据来训练一个基于强化学习的 криптовалю端folio管理模型，并对模型进行了测试和评估。</li>
<li>results: 结果显示，提出的 CryptoRLPM 系统在比基金的测试和评估中减少了至少 83.14%、0.5603% 和 2.1767% 的负面影响，并且在比特币方面减少了至少 83.14%、0.5603% 和 2.1767% 的负面影响。<details>
<summary>Abstract</summary>
On-chain data (metrics) of blockchain networks, akin to company fundamentals, provide crucial and comprehensive insights into the networks. Despite their informative nature, on-chain data have not been utilized in reinforcement learning (RL)-based systems for cryptocurrency (crypto) portfolio management (PM). An intriguing subject is the extent to which the utilization of on-chain data can enhance an RL-based system's return performance compared to baselines. Therefore, in this study, we propose CryptoRLPM, a novel RL-based system incorporating on-chain data for end-to-end crypto PM. CryptoRLPM consists of five units, spanning from information comprehension to trading order execution. In CryptoRLPM, the on-chain data are tested and specified for each crypto to solve the issue of ineffectiveness of metrics. Moreover, the scalable nature of CryptoRLPM allows changes in the portfolios' cryptos at any time. Backtesting results on three portfolios indicate that CryptoRLPM outperforms all the baselines in terms of accumulated rate of return (ARR), daily rate of return (DRR), and Sortino ratio (SR). Particularly, when compared to Bitcoin, CryptoRLPM enhances the ARR, DRR, and SR by at least 83.14%, 0.5603%, and 2.1767 respectively.
</details>
<details>
<summary>摘要</summary>
币Chain数据（指标），类似于公司基础数据，为区块链网络提供了关键和全面的信息。尽管它们的信息性很高，但是它们在基于强化学习（RL）的系统中没有被利用，用于货币（简称为“爬”）股票管理（PM）。这是一个有趣的话题，即使用币Chain数据可以提高RL基本系统的回报性相比基准。因此，在这种研究中，我们提出了CryptoRLPM，一种包含五个单元的RL基本系统，用于综合管理爬股票。CryptoRLPM中的币Chain数据被测试和特定为每种爬股票，以解决币Chain数据的不准确性问题。此外，CryptoRLPM具有可扩展性，可以在任何时间更改股票组合中的爬股票。回testing结果表明，CryptoRLPM在三个股票组合上超过所有基准，在累积收益率（ARR）、日内收益率（DRR）和Sortino分数（SR）方面。特别是与比特币相比，CryptoRLPM在ARR、DRR和SR方面提高了至少83.14%、0.5603%和2.1767%。
</details></li>
</ul>
<hr>
<h2 id="Bridge-the-Performance-Gap-in-Peak-hour-Series-Forecasting-The-Seq2Peak-Framework"><a href="#Bridge-the-Performance-Gap-in-Peak-hour-Series-Forecasting-The-Seq2Peak-Framework" class="headerlink" title="Bridge the Performance Gap in Peak-hour Series Forecasting: The Seq2Peak Framework"></a>Bridge the Performance Gap in Peak-hour Series Forecasting: The Seq2Peak Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01597">http://arxiv.org/abs/2307.01597</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenwei Zhang, Xin Wang, Jingyuan Xie, Heling Zhang, Yuantao Gu</li>
<li>for: 预测峰值时间序列 (PHSF) 是许多领域的关键任务，但是目前的深度学习模型在这种任务上表现不佳。这可以归结于峰值时间序列的高度非站台性，导致直接预测更加困难于标准时间序列预测 (TSF)。</li>
<li>methods: 本文提出了一种名为Seq2Peak的新框架，用于解决 PHSF 任务中的性能差距。Seq2Peak 包括两个关键组件：一个名为 CyclicNorm 的管道，用于解决非站台性问题，以及一个简单 yet effective 的可学习参数自由的峰值时间序列解码器，使用了一种混合损失函数，将原始序列和峰值时间序列作为监督信号。</li>
<li>results: 对于四个实际世界数据集，Seq2Peak 实现了惊人的平均相对提升率为 37.7%，对于基于 transformer 和非 transformer 的 TSF 模型。<details>
<summary>Abstract</summary>
Peak-Hour Series Forecasting (PHSF) is a crucial yet underexplored task in various domains. While state-of-the-art deep learning models excel in regular Time Series Forecasting (TSF), they struggle to achieve comparable results in PHSF. This can be attributed to the challenges posed by the high degree of non-stationarity in peak-hour series, which makes direct forecasting more difficult than standard TSF. Additionally, manually extracting the maximum value from regular forecasting results leads to suboptimal performance due to models minimizing the mean deficit. To address these issues, this paper presents Seq2Peak, a novel framework designed specifically for PHSF tasks, bridging the performance gap observed in TSF models. Seq2Peak offers two key components: the CyclicNorm pipeline to mitigate the non-stationarity issue, and a simple yet effective trainable-parameter-free peak-hour decoder with a hybrid loss function that utilizes both the original series and peak-hour series as supervised signals. Extensive experimentation on publicly available time series datasets demonstrates the effectiveness of the proposed framework, yielding a remarkable average relative improvement of 37.7\% across four real-world datasets for both transformer- and non-transformer-based TSF models.
</details>
<details>
<summary>摘要</summary>
《峰值小时序列预测（PHSF）是许多领域中的关键 yet 未得到充分的研究。当前的深度学习模型在标准时间序列预测（TSF）中表现出色，但在 PHSF 中却很难达到相似的结果。这可以归因于峰值小时序列的高度非站ARY，使得直接预测变得更加困难于标准 TSF。另外，通过手动提取最大值从 regular 预测结果来获得优化性能的方法会带来较差的性能，因为模型会尝试最小化均方误差。为解决这些问题，本文提出了 Seq2Peak 框架，这是专门为 PHSF 任务设计的。Seq2Peak 包括两个关键组成部分： CyclicNorm 管道，用于mitigate 非站ARY问题，以及一个简单 yet 高效的可学习参数无 peak-hour 解码器，使用了 Hybrid 损失函数，该函数使用原始序列和峰值小时序列作为监督信号。经过对公共可用时间序列数据集的广泛实验，Seq2Peak 的效果得到了许多实验证明，其中平均相对提升率为 37.7%，在四个实际世界数据集上。
</details></li>
</ul>
<hr>
<h2 id="Cross-Element-Combinatorial-Selection-for-Multi-Element-Creative-in-Display-Advertising"><a href="#Cross-Element-Combinatorial-Selection-for-Multi-Element-Creative-in-Display-Advertising" class="headerlink" title="Cross-Element Combinatorial Selection for Multi-Element Creative in Display Advertising"></a>Cross-Element Combinatorial Selection for Multi-Element Creative in Display Advertising</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01593">http://arxiv.org/abs/2307.01593</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Zhang, Ping Zhang, Jian Dong, Yongkang Wang, Pengye Zhang, Bo Zhang, Xingxing Wang, Dong Wang</li>
<li>for: 本研究旨在提高广告创作的效果，通过采用跨元素共同选择机制来选择多个创意元素的合适组合。</li>
<li>methods: 本研究提出了一种跨元素共同选择框架（CECS），包括编码器过程和解码器过程。编码器过程采用跨元素交互来动态调整单个创意元素的表达，而解码器过程将创意组合问题转化为多个创意元素之间的链式选择问题。</li>
<li>results: 实验结果表明，CECS取得了最佳成绩（SOTA）在线上数据集上的评价指标，并在实际应用中实现了显著的6.02% CTR和10.37% GMV提升，这对业务具有益处。<details>
<summary>Abstract</summary>
The effectiveness of ad creatives is greatly influenced by their visual appearance. Advertising platforms can generate ad creatives with different appearances by combining creative elements provided by advertisers. However, with the increasing number of ad creative elements, it becomes challenging to select a suitable combination from the countless possibilities. The industry's mainstream approach is to select individual creative elements independently, which often overlooks the importance of interaction between creative elements during the modeling process. In response, this paper proposes a Cross-Element Combinatorial Selection framework for multiple creative elements, termed CECS. In the encoder process, a cross-element interaction is adopted to dynamically adjust the expression of a single creative element based on the current candidate creatives. In the decoder process, the creative combination problem is transformed into a cascade selection problem of multiple creative elements. A pointer mechanism with a cascade design is used to model the associations among candidates. Comprehensive experiments on real-world datasets show that CECS achieved the SOTA score on offline metrics. Moreover, the CECS algorithm has been deployed in our industrial application, resulting in a significant 6.02% CTR and 10.37% GMV lift, which is beneficial to the business.
</details>
<details>
<summary>摘要</summary>
“广告创意的有效性受到它的视觉形象影响很大。广告平台可以通过结合广告主提供的创意元素，生成不同的创意形象。然而，随着创意元素的数量增加，选择适当的组合变得越来越困难。业界主流的方法是选择个别创意元素独立地，往往忽略了创意元素间的互动过程中的重要性。因此，这篇文章提出了跨元素选择框架（CECS）。在encode过程中，采用了跨元素互动来动态地调整单一创意元素的表达，以满足目前的候选者。在decode过程中，创意组合问题转化为多个创意元素之间的传递选择问题。使用一个链接机制，模型候选者之间的协力。实际测试统计表明，CECS已经 дости得了最佳成绩（SOTA）的数据。此外，CECS算法已经在我们的业务应用中实现了6.02%的Click Through Rate（CTR）和10.37%的Gross Merchandise Value（GMV）提升，对业务有很大的帮助。”
</details></li>
</ul>
<hr>
<h2 id="Learning-Lie-Group-Symmetry-Transformations-with-Neural-Networks"><a href="#Learning-Lie-Group-Symmetry-Transformations-with-Neural-Networks" class="headerlink" title="Learning Lie Group Symmetry Transformations with Neural Networks"></a>Learning Lie Group Symmetry Transformations with Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01583">http://arxiv.org/abs/2307.01583</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/victoria-klein/learning-lie-group-symmetries">https://github.com/victoria-klein/learning-lie-group-symmetries</a></li>
<li>paper_authors: Alex Gabel, Victoria Klein, Riccardo Valperga, Jeroen S. W. Lamb, Kevin Webster, Rick Quax, Efstratios Gavves</li>
<li>for: 检测和评估数据集中的对称性，用于模型选择、生成模型和数据分析等方面。</li>
<li>methods: 利用一种新的方法，可以自动发现数据集中的未知对称性，包括 Lie 群对称变换以外的其他对称性。</li>
<li>results: 研究得出的结果表明，该方法可以有效地检测和评估数据集中的对称性，并且可以在不同的参数值下进行一一对应。<details>
<summary>Abstract</summary>
The problem of detecting and quantifying the presence of symmetries in datasets is useful for model selection, generative modeling, and data analysis, amongst others. While existing methods for hard-coding transformations in neural networks require prior knowledge of the symmetries of the task at hand, this work focuses on discovering and characterizing unknown symmetries present in the dataset, namely, Lie group symmetry transformations beyond the traditional ones usually considered in the field (rotation, scaling, and translation). Specifically, we consider a scenario in which a dataset has been transformed by a one-parameter subgroup of transformations with different parameter values for each data point. Our goal is to characterize the transformation group and the distribution of the parameter values. The results showcase the effectiveness of the approach in both these settings.
</details>
<details>
<summary>摘要</summary>
问题是检测和评估数据集中的对称性，具有各种应用，如模型选择、生成模型和数据分析等。现有的方法需要先知道任务的对称性，而这种工作则是通过发现数据集中未知对称性，即李群对称变换 beyond 传统 Considered in the field (旋转、缩放和平移)。 Specifically, we consider a scenario in which a dataset has been transformed by a one-parameter subgroup of transformations with different parameter values for each data point. Our goal is to characterize the transformation group and the distribution of the parameter values. The results showcase the effectiveness of the approach in both these settings.Note that the translation is in Simplified Chinese, which is the more commonly used variety of Chinese in mainland China. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="IAdet-Simplest-human-in-the-loop-object-detection"><a href="#IAdet-Simplest-human-in-the-loop-object-detection" class="headerlink" title="IAdet: Simplest human-in-the-loop object detection"></a>IAdet: Simplest human-in-the-loop object detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01582">http://arxiv.org/abs/2307.01582</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/franchesoni/iadet">https://github.com/franchesoni/iadet</a></li>
<li>paper_authors: Franco Marchesoni-Acland, Gabriele Facciolo</li>
<li>for: 提高单类物体检测模型的训练效率和质量，通过人工监督系统。</li>
<li>methods:  propose a Intelligent Annotation (IA) strategy, including three modules: 助手数据标注、背景模型训练和活动选择下一个数据点。开发了特定于单类物体检测的IAdet工具，并提出了自动评估这种人工监督系统的方法。</li>
<li>results: 在PASCAL VOC数据集上，IAdet工具可以减少数据库标注时间25%，并提供一个免费训练过的模型。这些结果是基于偏门设计的very simple IAdet design，因此IAdet具有多个简单的改进空间，预示了可以实现强大的人工监督对象检测系统。<details>
<summary>Abstract</summary>
This work proposes a strategy for training models while annotating data named Intelligent Annotation (IA). IA involves three modules: (1) assisted data annotation, (2) background model training, and (3) active selection of the next datapoints. Under this framework, we open-source the IAdet tool, which is specific for single-class object detection. Additionally, we devise a method for automatically evaluating such a human-in-the-loop system. For the PASCAL VOC dataset, the IAdet tool reduces the database annotation time by $25\%$ while providing a trained model for free. These results are obtained for a deliberately very simple IAdet design. As a consequence, IAdet is susceptible to multiple easy improvements, paving the way for powerful human-in-the-loop object detection systems.
</details>
<details>
<summary>摘要</summary>
这个工作提出了一种名为智能注释（IA）的模型训练策略。IA包括三个模块：（1）助手数据注释、（2）背景模型训练和（3）活动选择下一个数据点。在这个框架下，我们开源了专门用于单类对象检测的IADE工具。此外，我们还提出了一种自动评估这种人在循环系统的方法。对于PASCAL VOC数据集，IADE工具可以降低数据库注释时间$25\%$，同时提供免费的训练模型。这些结果是基于故意设计非常简单的IADE设计而得到的。因此，IADE易受到多个简单的改进，开展出具有强大人在循环对象检测系统的可能性。
</details></li>
</ul>
<hr>
<h2 id="Optimal-and-Efficient-Binary-Questioning-for-Human-in-the-Loop-Annotation"><a href="#Optimal-and-Efficient-Binary-Questioning-for-Human-in-the-Loop-Annotation" class="headerlink" title="Optimal and Efficient Binary Questioning for Human-in-the-Loop Annotation"></a>Optimal and Efficient Binary Questioning for Human-in-the-Loop Annotation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01578">http://arxiv.org/abs/2307.01578</a></li>
<li>repo_url: None</li>
<li>paper_authors: Franco Marchesoni-Acland, Jean-Michel Morel, Josselin Kherroubi, Gabriele Facciolo</li>
<li>for: 本研究旨在解决人工监督学习中数据注释的缺失问题，即使用一个预测器可以获得更多的注释数据。</li>
<li>methods: 本研究使用了一种枚举编码法来寻找最优的问题策略，以及一些启发式和lookahead最小化代理成本函数的方法。</li>
<li>results: 研究表明，使用提议的方法可以在几种 sintetic 和实际世界的数据集上实现23-86%的注释效率提升。<details>
<summary>Abstract</summary>
Even though data annotation is extremely important for interpretability, research and development of artificial intelligence solutions, most research efforts such as active learning or few-shot learning focus on the sample efficiency problem. This paper studies the neglected complementary problem of getting annotated data given a predictor. For the simple binary classification setting, we present the spectrum ranging from optimal general solutions to practical efficient methods. The problem is framed as the full annotation of a binary classification dataset with the minimal number of yes/no questions when a predictor is available. For the case of general binary questions the solution is found in coding theory, where the optimal questioning strategy is given by the Huffman encoding of the possible labelings. However, this approach is computationally intractable even for small dataset sizes. We propose an alternative practical solution based on several heuristics and lookahead minimization of proxy cost functions. The proposed solution is analysed, compared with optimal solutions and evaluated on several synthetic and real-world datasets. On these datasets, the method allows a significant improvement ($23-86\%$) in annotation efficiency.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Multi-Task-Learning-to-Enhance-Generazability-of-Neural-Network-Equalizers-in-Coherent-Optical-Systems"><a href="#Multi-Task-Learning-to-Enhance-Generazability-of-Neural-Network-Equalizers-in-Coherent-Optical-Systems" class="headerlink" title="Multi-Task Learning to Enhance Generazability of Neural Network Equalizers in Coherent Optical Systems"></a>Multi-Task Learning to Enhance Generazability of Neural Network Equalizers in Coherent Optical Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05374">http://arxiv.org/abs/2307.05374</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sasipim Srivallapanondh, Pedro J. Freire, Ashraful Alam, Nelson Costa, Bernhard Spinnler, Antonio Napoli, Egor Sedov, Sergei K. Turitsyn, Jaroslaw E. Prilepsky</li>
<li>for: 提高减噪系统的灵活性</li>
<li>methods: 使用多任务学习方法提高NN基于的平衡器</li>
<li>results: 单个NN基于平衡器可以提高Q因子至4dB，不需要重新训练，即使发射功率、符号速率或传输距离发生变化。<details>
<summary>Abstract</summary>
For the first time, multi-task learning is proposed to improve the flexibility of NN-based equalizers in coherent systems. A "single" NN-based equalizer improves Q-factor by up to 4 dB compared to CDC, without re-training, even with variations in launch power, symbol rate, or transmission distance.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Approximate-information-for-efficient-exploration-exploitation-strategies"><a href="#Approximate-information-for-efficient-exploration-exploitation-strategies" class="headerlink" title="Approximate information for efficient exploration-exploitation strategies"></a>Approximate information for efficient exploration-exploitation strategies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01563">http://arxiv.org/abs/2307.01563</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alex Barbier-Chebbah, Christian L. Vestergaard, Jean-Baptiste Masson</li>
<li>for: 这篇论文目标是解决决策中的探索-利用矛盾，具体是多重枪支问题。</li>
<li>methods: 这篇论文提出了一种新的算法，即approximate information maximization（AIM），该算法使用分析式 entropy 导数来选择每个时刻哪个枪支。AIM 与 Infomax 和 Thompson sampling 性能相同，同时具有加速、决定性和可追踪性等优点。</li>
<li>results: 实验证明 AIM 遵循 Lai-Robbins  asymptotic bound，并在不同的假设下表现稳定。其表达可调，可以根据具体情况进行特定优化。<details>
<summary>Abstract</summary>
This paper addresses the exploration-exploitation dilemma inherent in decision-making, focusing on multi-armed bandit problems. The problems involve an agent deciding whether to exploit current knowledge for immediate gains or explore new avenues for potential long-term rewards. We here introduce a novel algorithm, approximate information maximization (AIM), which employs an analytical approximation of the entropy gradient to choose which arm to pull at each point in time. AIM matches the performance of Infomax and Thompson sampling while also offering enhanced computational speed, determinism, and tractability. Empirical evaluation of AIM indicates its compliance with the Lai-Robbins asymptotic bound and demonstrates its robustness for a range of priors. Its expression is tunable, which allows for specific optimization in various settings.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Comprehensive-Multi-scale-Approach-for-Speech-and-Dynamics-Synchrony-in-Talking-Head-Generation"><a href="#A-Comprehensive-Multi-scale-Approach-for-Speech-and-Dynamics-Synchrony-in-Talking-Head-Generation" class="headerlink" title="A Comprehensive Multi-scale Approach for Speech and Dynamics Synchrony in Talking Head Generation"></a>A Comprehensive Multi-scale Approach for Speech and Dynamics Synchrony in Talking Head Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03270">http://arxiv.org/abs/2307.03270</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/louisbearing/hmo-audio">https://github.com/louisbearing/hmo-audio</a></li>
<li>paper_authors: Louis Airale, Dominique Vaufreydaz, Xavier Alameda-Pineda</li>
<li>for: 这个论文主要针对的是使用深度生成模型来动画非动体图像，以实现更加自然的头部动作和语音同步。</li>
<li>methods: 该论文提出了一种多尺度音视频同步损失函数和多尺度自适应GAN，以更好地处理语音和头部动作之间的短期和长期相关性。</li>
<li>results: 实验表明，该方法可以在多尺度音视频同步和头部动作质量上达到州前的提升，并且在标准的面部特征域中生成更加自然的头部动作。<details>
<summary>Abstract</summary>
Animating still face images with deep generative models using a speech input signal is an active research topic and has seen important recent progress. However, much of the effort has been put into lip syncing and rendering quality while the generation of natural head motion, let alone the audio-visual correlation between head motion and speech, has often been neglected. In this work, we propose a multi-scale audio-visual synchrony loss and a multi-scale autoregressive GAN to better handle short and long-term correlation between speech and the dynamics of the head and lips. In particular, we train a stack of syncer models on multimodal input pyramids and use these models as guidance in a multi-scale generator network to produce audio-aligned motion unfolding over diverse time scales. Our generator operates in the facial landmark domain, which is a standard low-dimensional head representation. The experiments show significant improvements over the state of the art in head motion dynamics quality and in multi-scale audio-visual synchrony both in the landmark domain and in the image domain.
</details>
<details>
<summary>摘要</summary>
<<SYS>> transtable text into Simplified Chinese.<</SYS>>使用深度生成模型动画静止图像是一个活跃的研究领域，最近几年得到了重要的进步。然而，许多努力都是 lip syncing 和图像质量的优化，而生成自然的头部运动和语音-图像相关性往往被忽略。在这项工作中，我们提议一种多尺度音视频同步损失和多尺度自适应GAN，以更好地处理语音和头部运动之间的短期和长期相关性。特别是，我们在多modal输入 pyramids 上堆叠 syncer 模型，并使用这些模型作为导向在多尺度生成网络中生成音频同步的动作。我们的生成器在 facial landmark 领域中运行，这是一个标准的低维度头部表示。实验结果表明，我们的方法可以在头部运动动态质量和多尺度音视频同步两个方面达到显著提高。
</details></li>
</ul>
<hr>
<h2 id="Secure-Deep-Learning-based-Distributed-Intelligence-on-Pocket-sized-Drones"><a href="#Secure-Deep-Learning-based-Distributed-Intelligence-on-Pocket-sized-Drones" class="headerlink" title="Secure Deep Learning-based Distributed Intelligence on Pocket-sized Drones"></a>Secure Deep Learning-based Distributed Intelligence on Pocket-sized Drones</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01559">http://arxiv.org/abs/2307.01559</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elia Cereda, Alessandro Giusti, Daniele Palossi</li>
<li>for: 这个研究旨在解决单位大小仅对应小型飞行器（nano-drone）上进行大型深度学习模型的问题。</li>
<li>methods: 本研究提出了一种分布式边缘-fog计算模型，以实现在nano-drone上进行大型深度学习模型的执行。此外，本研究还提出了一种验证fog计算的方法，以确保fog节点或通信链路不可信。</li>
<li>results: 相比于完全在nano-drone上执行的现有Visual Pose Estimation网络，这个分布式边缘-fog执行方案可以提高$R^2$ score +0.19。在攻击情况下，本方法可以在2秒内检测攻击，95%的概率可以检测到。<details>
<summary>Abstract</summary>
Palm-sized nano-drones are an appealing class of edge nodes, but their limited computational resources prevent running large deep-learning models onboard. Adopting an edge-fog computational paradigm, we can offload part of the computation to the fog; however, this poses security concerns if the fog node, or the communication link, can not be trusted. To tackle this concern, we propose a novel distributed edge-fog execution scheme that validates fog computation by redundantly executing a random subnetwork aboard our nano-drone. Compared to a State-of-the-Art visual pose estimation network that entirely runs onboard, a larger network executed in a distributed way improves the $R^2$ score by +0.19; in case of attack, our approach detects it within 2s with 95% probability.
</details>
<details>
<summary>摘要</summary>
手持式奈米型机器人的 Computational Resources 有限，无法进行大型深度学习模型的 Calculation。我们运用 Edge-Fog 计算模式，将一部分计算推广到fog中，但这会带来安全性 Concern ，如果fog Node 或通信链路不能被信任。为解决这问题，我们提出了一个分布式 Edge-Fog 执行方案，透过重复运行 Random Subnetwork 在我们的奈米型机器人上，以验证fog计算。相比于完全在board上运行的 State-of-the-Art 视觉 pose 估测网络，分布式执行的大型网络可以提高 $R^2$ 分数 +0.19; 在攻击情况下，我们的方法可以在2秒内检测到攻击，95%的机会性。
</details></li>
</ul>
<hr>
<h2 id="Multi-gauge-Hydrological-Variational-Data-Assimilation-Regionalization-Learning-with-Spatial-Gradients-using-Multilayer-Perceptron-and-Bayesian-Guided-Multivariate-Regression"><a href="#Multi-gauge-Hydrological-Variational-Data-Assimilation-Regionalization-Learning-with-Spatial-Gradients-using-Multilayer-Perceptron-and-Bayesian-Guided-Multivariate-Regression" class="headerlink" title="Multi-gauge Hydrological Variational Data Assimilation: Regionalization Learning with Spatial Gradients using Multilayer Perceptron and Bayesian-Guided Multivariate Regression"></a>Multi-gauge Hydrological Variational Data Assimilation: Regionalization Learning with Spatial Gradients using Multilayer Perceptron and Bayesian-Guided Multivariate Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02497">http://arxiv.org/abs/2307.02497</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ngo Nghi Truyen Huynh, Pierre-André Garambois, François Colleoni, Benjamin Renard, Hélène Roux</li>
<li>for: 这篇论文旨在解决水文模型中难以估计的空间分布型水文参数问题，特别是无测水道上的洪水。</li>
<li>methods: 本研究使用了一种新的区域化技术，将复杂的区域转换函数融合到高分辨率水文模型中，以便使用机器学习优化算法进行学习。</li>
<li>results: 本研究获得了一种可靠地估计水文模型中的空间分布型参数，并且可以处理多测站数据，实现了高精度的水文预测。<details>
<summary>Abstract</summary>
Tackling the difficult problem of estimating spatially distributed hydrological parameters, especially for floods on ungauged watercourses, this contribution presents a novel seamless regionalization technique for learning complex regional transfer functions designed for high-resolution hydrological models. The transfer functions rely on: (i) a multilayer perceptron enabling a seamless flow of gradient computation to employ machine learning optimization algorithms, or (ii) a multivariate regression mapping optimized by variational data assimilation algorithms and guided by Bayesian estimation, addressing the equifinality issue of feasible solutions. The approach involves incorporating the inferable regionalization mappings into a differentiable hydrological model and optimizing a cost function computed on multi-gauge data with accurate adjoint-based spatially distributed gradients.
</details>
<details>
<summary>摘要</summary>
solves the difficult problem of estimating spatially distributed hydrological parameters, especially for floods on ungauged watercourses, by presenting a novel seamless regionalization technique for learning complex regional transfer functions designed for high-resolution hydrological models. The transfer functions rely on:(i) a multilayer perceptron enabling a seamless flow of gradient computation to employ machine learning optimization algorithms, or(ii) a multivariate regression mapping optimized by variational data assimilation algorithms and guided by Bayesian estimation, addressing the equifinality issue of feasible solutions.The approach involves incorporating the inferable regionalization mappings into a differentiable hydrological model and optimizing a cost function computed on multi-gauge data with accurate adjoint-based spatially distributed gradients.
</details></li>
</ul>
<hr>
<h2 id="Scalable-variable-selection-for-two-view-learning-tasks-with-projection-operators"><a href="#Scalable-variable-selection-for-two-view-learning-tasks-with-projection-operators" class="headerlink" title="Scalable variable selection for two-view learning tasks with projection operators"></a>Scalable variable selection for two-view learning tasks with projection operators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01558">http://arxiv.org/abs/2307.01558</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aalto-ics-kepaco/projse">https://github.com/aalto-ics-kepaco/projse</a></li>
<li>paper_authors: Sandor Szedmak, Riikka Huusari, Tat Hong Duong Le, Juho Rousu</li>
<li>for: 该论文提出了一种新的变量选择方法，适用于两视图设置或vector-valued超vision学习问题。该方法可以处理巨大规模的选择任务，数据样本数可以达到百万级。</li>
<li>methods: 该方法通过Iteratively选择高度相关于输出变量的变量，但不相关于先前选择的变量。为度量相关性，该方法使用投影算子和其代数。通过投影算子，输入和输出变量之间的关系可以表示为kernel函数，从而可以利用非线性相关模型。</li>
<li>results: 该方法在synthetic和实际数据上进行了实验 validate，显示了其扩展性和选择的有效性。<details>
<summary>Abstract</summary>
In this paper we propose a novel variable selection method for two-view settings, or for vector-valued supervised learning problems. Our framework is able to handle extremely large scale selection tasks, where number of data samples could be even millions. In a nutshell, our method performs variable selection by iteratively selecting variables that are highly correlated with the output variables, but which are not correlated with the previously chosen variables. To measure the correlation, our method uses the concept of projection operators and their algebra. With the projection operators the relationship, correlation, between sets of input and output variables can also be expressed by kernel functions, thus nonlinear correlation models can be exploited as well. We experimentally validate our approach, showing on both synthetic and real data its scalability and the relevance of the selected features. Keywords: Supervised variable selection, vector-valued learning, projection-valued measure, reproducing kernel Hilbert space
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的变量选择方法，适用于两视设定或vector-valued学习问题。我们的框架可以处理非常大规模的选择任务，数据样本数可以达到百万级。总之，我们的方法通过逐步选择输出变量高度相关的变量，但不相关于已经选择的变量来进行变量选择。为了度量相关性，我们使用投影算子和其代数来度量输入和输出变量之间的关系。通过投影算子，我们可以将输入和输出变量之间的关系表示为内积函数，从而可以利用内积函数来表示非线性相关模型。我们在实验中 validate our approach，并在 synthetic 和实际数据上证明了我们的方法的扩展性和选择的相关性。关键词：supervised变量选择、vector-valued学习、投影值度量、 reproduce kernel Hilbert space
</details></li>
</ul>
<hr>
<h2 id="Learning-to-reconstruct-the-bubble-distribution-with-conductivity-maps-using-Invertible-Neural-Networks-and-Error-Diffusion"><a href="#Learning-to-reconstruct-the-bubble-distribution-with-conductivity-maps-using-Invertible-Neural-Networks-and-Error-Diffusion" class="headerlink" title="Learning to reconstruct the bubble distribution with conductivity maps using Invertible Neural Networks and Error Diffusion"></a>Learning to reconstruct the bubble distribution with conductivity maps using Invertible Neural Networks and Error Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02496">http://arxiv.org/abs/2307.02496</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nishant Kumar, Lukas Krause, Thomas Wondrak, Sven Eckert, Kerstin Eckert, Stefan Gumhold</li>
<li>for: 用于实现可持续的氢生产</li>
<li>methods: 使用外部磁场探测器和归一化方法测量磁场干扰，并使用INN重建电导率场</li>
<li>results: 比使用提高方法（Tikhonov regularization）表现更好，可以高精度地重建电导率场<details>
<summary>Abstract</summary>
Electrolysis is crucial for eco-friendly hydrogen production, but gas bubbles generated during the process hinder reactions, reduce cell efficiency, and increase energy consumption. Additionally, these gas bubbles cause changes in the conductivity inside the cell, resulting in corresponding variations in the induced magnetic field around the cell. Therefore, measuring these gas bubble-induced magnetic field fluctuations using external magnetic sensors and solving the inverse problem of Biot-Savart Law allows for estimating the conductivity in the cell and, thus, bubble size and location. However, determining high-resolution conductivity maps from only a few induced magnetic field measurements is an ill-posed inverse problem. To overcome this, we exploit Invertible Neural Networks (INNs) to reconstruct the conductivity field. Our qualitative results and quantitative evaluation using random error diffusion show that INN achieves far superior performance compared to Tikhonov regularization.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用电解为绿色氢生产的关键步骤，但在过程中生成的气泡会阻碍反应、降低电池效率和增加能源消耗。此外，这些气泡会导致电池内的导电性变化，从而导致电磁场附近电池的变化。因此，通过外部磁场探测器测量气泡启发的磁场变化，并解决生成的Biot-Savart法的反问题，可以估算电池内的导电性，并由此计算气泡的大小和位置。但是，从仅几个磁场测量得到高分辨率导电地图是一个不定义的倒数问题。为了解决这个问题，我们利用归一化神经网络（INNs）重建导电场。我们的质量结果和随机扩散评价表明，INN在性能方面远胜于TIkhonov正则化。Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Exploiting-Richness-of-Learned-Compressed-Representation-of-Images-for-Semantic-Segmentation"><a href="#Exploiting-Richness-of-Learned-Compressed-Representation-of-Images-for-Semantic-Segmentation" class="headerlink" title="Exploiting Richness of Learned Compressed Representation of Images for Semantic Segmentation"></a>Exploiting Richness of Learned Compressed Representation of Images for Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01524">http://arxiv.org/abs/2307.01524</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/DL4Compression/Semantic_Segmentation_of_Driving_Videos_on_Learning_based_Image_Compression">https://github.com/DL4Compression/Semantic_Segmentation_of_Driving_Videos_on_Learning_based_Image_Compression</a></li>
<li>paper_authors: Ravi Kakaiya, Rakshith Sathish, Ramanathan Sethuraman, Debdoot Sheet</li>
<li>for: 提高自动驾驶和高级驾驶助手系统（ADAS）的性能和可扩展性。</li>
<li>methods: 使用学习基于的压缩编码器来减少传输数据的延迟，并且通过学习的方式使得压缩编码器可以同时执行压缩和解压缩操作。</li>
<li>results: 在Cityscapes dataset上实验 validate the proposed pipeline，实现了压缩因子达66倍，保留了segmenation任务所需的信息，而且降低了总计算量11%。<details>
<summary>Abstract</summary>
Autonomous vehicles and Advanced Driving Assistance Systems (ADAS) have the potential to radically change the way we travel. Many such vehicles currently rely on segmentation and object detection algorithms to detect and track objects around its surrounding. The data collected from the vehicles are often sent to cloud servers to facilitate continual/life-long learning of these algorithms. Considering the bandwidth constraints, the data is compressed before sending it to servers, where it is typically decompressed for training and analysis. In this work, we propose the use of a learning-based compression Codec to reduce the overhead in latency incurred for the decompression operation in the standard pipeline. We demonstrate that the learned compressed representation can also be used to perform tasks like semantic segmentation in addition to decompression to obtain the images. We experimentally validate the proposed pipeline on the Cityscapes dataset, where we achieve a compression factor up to $66 \times$ while preserving the information required to perform segmentation with a dice coefficient of $0.84$ as compared to $0.88$ achieved using decompressed images while reducing the overall compute by $11\%$.
</details>
<details>
<summary>摘要</summary>
自动驾驶车和高级驾驶助手系统（ADAS）有可能改变我们的旅行方式。许多这些车辆目前都使用分割和对象检测算法来检测和跟踪周围的对象。收集到的数据通常会被发送到云服务器以便持续/人生学习这些算法。由于带宽约束，数据通常会被压缩后发送到服务器，其中它们通常会被解压缩以进行训练和分析。在这种情况下，我们提议使用学习基于压缩编码器来减少标准管道中的延迟过载。我们示出了learned压缩表示可以用于实现像semantic segmentation这样的任务，而不需要解压缩。我们对Cityscapes数据集进行实验，并实现了最多$66\times$的压缩因子，保留了需要进行分割的信息，并且将compute总体减少$11\%$。
</details></li>
</ul>
<hr>
<h2 id="Deep-Attention-Q-Network-for-Personalized-Treatment-Recommendation"><a href="#Deep-Attention-Q-Network-for-Personalized-Treatment-Recommendation" class="headerlink" title="Deep Attention Q-Network for Personalized Treatment Recommendation"></a>Deep Attention Q-Network for Personalized Treatment Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01519">http://arxiv.org/abs/2307.01519</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/stevenmsm/rl-icu-daqn">https://github.com/stevenmsm/rl-icu-daqn</a></li>
<li>paper_authors: Simin Ma, Junghwan Lee, Nicoleta Serban, Shihao Yang</li>
<li>for: 这篇论文旨在提供个性化治疗建议，以实现医疗结果最佳化。</li>
<li>methods: 本研究使用深度注意力Q网络（DAQN），利用对应架构内的强化学习框架，高效地包含所有过去病人观察数据。</li>
<li>results: 比较先前的模型，本研究的DAQN模型在实际世界的 septic shock 和急性低血压患者群中表现出色，显示其超越性。<details>
<summary>Abstract</summary>
Tailoring treatment for individual patients is crucial yet challenging in order to achieve optimal healthcare outcomes. Recent advances in reinforcement learning offer promising personalized treatment recommendations; however, they rely solely on current patient observations (vital signs, demographics) as the patient's state, which may not accurately represent the true health status of the patient. This limitation hampers policy learning and evaluation, ultimately limiting treatment effectiveness. In this study, we propose the Deep Attention Q-Network for personalized treatment recommendations, utilizing the Transformer architecture within a deep reinforcement learning framework to efficiently incorporate all past patient observations. We evaluated the model on real-world sepsis and acute hypotension cohorts, demonstrating its superiority to state-of-the-art models. The source code for our model is available at https://github.com/stevenmsm/RL-ICU-DAQN.
</details>
<details>
<summary>摘要</summary>
个人化治疗是现代医疗的关键，但是实现优化医疗效果却是挑战。 latest advances in reinforcement learning 提供了个人化治疗建议的可能性，但是它们只基于当前患者的观察数据（生命 Parameters, demographics）来定义患者的状态，这可能不准确地反映患者的真实健康状况。这种限制策略学习和评估，最终限制了治疗效果。在这项研究中，我们提出了 Deep Attention Q-Network，使用 transformer 架构在深度强化学习框架中高效地包含所有过去患者的观察数据。我们对现实世界的 septic shock 和急性低血压群体进行了评估，并证明了我们的模型在现有模型之上表现出色。我们的模型的源代码可以在 https://github.com/stevenmsm/RL-ICU-DAQN 上获取。
</details></li>
</ul>
<hr>
<h2 id="SelfFed-Self-supervised-Federated-Learning-for-Data-Heterogeneity-and-Label-Scarcity-in-IoMT"><a href="#SelfFed-Self-supervised-Federated-Learning-for-Data-Heterogeneity-and-Label-Scarcity-in-IoMT" class="headerlink" title="SelfFed: Self-supervised Federated Learning for Data Heterogeneity and Label Scarcity in IoMT"></a>SelfFed: Self-supervised Federated Learning for Data Heterogeneity and Label Scarcity in IoMT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01514">http://arxiv.org/abs/2307.01514</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sunder Ali Khowaja, Kapal Dev, Syed Muhammad Anwar, Marius George Linguraru</li>
<li>for: 这个研究旨在提出一个基于自适应学习的联邦学习框架，以实现在对没有标签的隔离数据上进行协同学习。</li>
<li>methods: 我们提出了一个名为SelfFed的框架，它包括两个阶段：首先是预训阶段，使用Swin Transformer基本Encoder进行增强模型，在分散式的方式下进行执行。其次是精度调整阶段，引入对照网络和一个新的聚合策略，在分散式的方式下进行训练，以解决标签稀缺问题。</li>
<li>results: 我们在公共可用的医疗图像数据集上进行实验分析，结果显示，我们的提出的SelfFed框架在非Identical和相似数据（IID） dataset上比基于已有的基eline出perform得更好，具体的提高8.8%和4.1%在Retina和COVID-FL数据集上。此外，我们的方法甚至在仅使用10%标签的情况下也能超越现有的基eline。<details>
<summary>Abstract</summary>
Self-supervised learning in federated learning paradigm has been gaining a lot of interest both in industry and research due to the collaborative learning capability on unlabeled yet isolated data. However, self-supervised based federated learning strategies suffer from performance degradation due to label scarcity and diverse data distributions, i.e., data heterogeneity. In this paper, we propose the SelfFed framework for Internet of Medical Things (IoMT). Our proposed SelfFed framework works in two phases. The first phase is the pre-training paradigm that performs augmentive modeling using Swin Transformer based encoder in a decentralized manner. The first phase of SelfFed framework helps to overcome the data heterogeneity issue. The second phase is the fine-tuning paradigm that introduces contrastive network and a novel aggregation strategy that is trained on limited labeled data for a target task in a decentralized manner. This fine-tuning stage overcomes the label scarcity problem. We perform our experimental analysis on publicly available medical imaging datasets and show that our proposed SelfFed framework performs better when compared to existing baselines concerning non-independent and identically distributed (IID) data and label scarcity. Our method achieves a maximum improvement of 8.8% and 4.1% on Retina and COVID-FL datasets on non-IID dataset. Further, our proposed method outperforms existing baselines even when trained on a few (10%) labeled instances.
</details>
<details>
<summary>摘要</summary>
“自我指导学习在联合学习框架中得到了产业和研究领域的广泛关注，因为它可以在不同数据源上进行协同学习，无需标签数据。然而，基于自我指导学习的联合学习策略受到数据不均衡和标签稀缺的限制，即数据多样性问题。在本文中，我们提出了基于互联网医疗器件（IoMT）的SelfFed框架。我们的提议的SelfFed框架分为两个阶段。第一阶段是预训练阶段，使用Swin Transformer基于编码器进行增强模型，在分布式方式下进行。第一阶段的SelfFed框架帮助解决数据多样性问题。第二阶段是精度调整阶段，引入对照网络和一种新的聚合策略，在分布式方式下进行限制标签数据的训练。这个精度调整阶段帮助解决标签稀缺问题。我们在公开available的医学成像数据集上进行实验分析，并证明我们的提议的SelfFed框架在非Identical和相似数据（IID）下性能更好，提高了8.8%和4.1%的提升。此外，我们的提议方法还能在只有10%标签实例的情况下超越现有的基准值。”
</details></li>
</ul>
<hr>
<h2 id="Relation-aware-graph-structure-embedding-with-co-contrastive-learning-for-drug-drug-interaction-prediction"><a href="#Relation-aware-graph-structure-embedding-with-co-contrastive-learning-for-drug-drug-interaction-prediction" class="headerlink" title="Relation-aware graph structure embedding with co-contrastive learning for drug-drug interaction prediction"></a>Relation-aware graph structure embedding with co-contrastive learning for drug-drug interaction prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01507">http://arxiv.org/abs/2307.01507</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mengying Jiang, Guizhong Liu, Biao Zhao, Yuanchao Su, Weiqiang Jin</li>
<li>for: 预测多种关系 drug-drug interaction (DDIs)</li>
<li>methods: 使用 relation-aware graph structure embedding (RaGSE) with co-contrastive learning</li>
<li>results: 在三个任务上比前state-of-the-art方法表现出色，得到更好的预测结果<details>
<summary>Abstract</summary>
Relation-aware graph structure embedding is promising for predicting multi-relational drug-drug interactions (DDIs). Typically, most existing methods begin by constructing a multi-relational DDI graph and then learning relation-aware graph structure embeddings (RaGSEs) of drugs from the DDI graph. Nevertheless, most existing approaches are usually limited in learning RaGSEs of new drugs, leading to serious over-fitting when the test DDIs involve such drugs. To alleviate this issue, we propose a novel DDI prediction method based on relation-aware graph structure embedding with co-contrastive learning, RaGSECo. The proposed RaGSECo constructs two heterogeneous drug graphs: a multi-relational DDI graph and a multi-attribute drug-drug similarity (DDS) graph. The two graphs are used respectively for learning and propagating the RaGSEs of drugs, aiming to ensure all drugs, including new ones, can possess effective RaGSEs. Additionally, we present a novel co-contrastive learning module to learn drug-pairs (DPs) representations. This mechanism learns DP representations from two distinct views (interaction and similarity views) and encourages these views to supervise each other collaboratively to obtain more discriminative DP representations. We evaluate the effectiveness of our RaGSECo on three different tasks using two real datasets. The experimental results demonstrate that RaGSECo outperforms existing state-of-the-art prediction methods.
</details>
<details>
<summary>摘要</summary>
“关系意识的图结构嵌入显示了在预测多关系药物交互（DDIs）方面的承诺。通常，现有的方法都是从多关系DDIs图构建起来，然后学习关系意识图结构嵌入（RaGSEs）。然而，这些方法通常只能学习新药物的RaGSEs，导致在测试DDIs中严重过拟合。为解决这个问题，我们提出了一种基于关系意识图结构嵌入和协同对比学习的新DDIs预测方法，即RaGSECo。提案的RaGSECo构建了两个不同类型的药物图：一个多关系DDIs图和一个多属性药物对比图。这两个图用于学习和传播药物的RaGSEs，以确保所有药物，包括新的一些，都可以具有有效的RaGSEs。此外，我们还提出了一种新的协同对比学习模块，用于学习药物对的表示。这个机制从两种不同的视图（交互视图和相似视图）中学习药物对的表示，并且使这两个视图相互监督each other以获得更有特征的药物对表示。我们使用三个不同的任务和两个真实数据集进行了实验，结果显示，RaGSECo在这些任务中表现出了更高的效果。”
</details></li>
</ul>
<hr>
<h2 id="All-in-One-Multi-task-Prompting-for-Graph-Neural-Networks"><a href="#All-in-One-Multi-task-Prompting-for-Graph-Neural-Networks" class="headerlink" title="All in One: Multi-task Prompting for Graph Neural Networks"></a>All in One: Multi-task Prompting for Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01504">http://arxiv.org/abs/2307.01504</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sheldonresearch/ProG">https://github.com/sheldonresearch/ProG</a></li>
<li>paper_authors: Xiangguo Sun, Hong Cheng, Jia Li, Bo Liu, Jihong Guan</li>
<li>for: 填充预训练模型的知识空间，以便更好地应对不同的图任务。</li>
<li>methods: 提出了一种基于多 зада务提问的图模型提问方法，包括对图提问和自然语言提问的融合、对图任务的重新定义以适应预训练模型，以及使用元学习来快速学习更好的初始化方法。</li>
<li>results: 经过广泛的实验，结果表明该方法可以在不同的图任务上达到更高的性能。<details>
<summary>Abstract</summary>
Recently, ''pre-training and fine-tuning'' has been adopted as a standard workflow for many graph tasks since it can take general graph knowledge to relieve the lack of graph annotations from each application. However, graph tasks with node level, edge level, and graph level are far diversified, making the pre-training pretext often incompatible with these multiple tasks. This gap may even cause a ''negative transfer'' to the specific application, leading to poor results. Inspired by the prompt learning in natural language processing (NLP), which has presented significant effectiveness in leveraging prior knowledge for various NLP tasks, we study the prompting topic for graphs with the motivation of filling the gap between pre-trained models and various graph tasks. In this paper, we propose a novel multi-task prompting method for graph models. Specifically, we first unify the format of graph prompts and language prompts with the prompt token, token structure, and inserting pattern. In this way, the prompting idea from NLP can be seamlessly introduced to the graph area. Then, to further narrow the gap between various graph tasks and state-of-the-art pre-training strategies, we further study the task space of various graph applications and reformulate downstream problems to the graph-level task. Afterward, we introduce meta-learning to efficiently learn a better initialization for the multi-task prompt of graphs so that our prompting framework can be more reliable and general for different tasks. We conduct extensive experiments, results from which demonstrate the superiority of our method.
</details>
<details>
<summary>摘要</summary>
近些时候，“预训练和精度调整”成为了许多图任务的标准工作流程，因为它可以帮助图模型学习通用的图知识，从而缓解每个应用程序缺乏图注释的问题。然而，图任务中的节点层、边层和图层具有广泛的多样性，这些预训练预TeX often incompatible with these multiple tasks，这可能会导致“负面传播”，从而影响特定应用程序的结果。 inspirited by the prompt learning in natural language processing (NLP), which has shown significant effectiveness in leveraging prior knowledge for various NLP tasks, we investigate the prompting topic for graphs with the motivation of filling the gap between pre-trained models and various graph tasks.在这篇论文中，我们提出了一种新的多任务提问方法 для图模型。具体来说，我们首先将图提问和语言提问的格式统一为Prompt Token、Token结构和插入模式。这样，NLP中的提问思想可以轻松地在图领域中引入。然后，为了进一步缩小不同图任务和当前预训练策略之间的差距，我们进一步研究了各种图应用程序的任务空间，并重新表述下游问题为图级任务。最后，我们引入了元学习，以更有效地学习多任务提问中的初始化，以使我们的提问框架更可靠和通用于不同任务。我们进行了广泛的实验，实验结果表明了我们的方法的优越性。
</details></li>
</ul>
<hr>
<h2 id="Accelerated-stochastic-approximation-with-state-dependent-noise"><a href="#Accelerated-stochastic-approximation-with-state-dependent-noise" class="headerlink" title="Accelerated stochastic approximation with state-dependent noise"></a>Accelerated stochastic approximation with state-dependent noise</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01497">http://arxiv.org/abs/2307.01497</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sasila Ilandarideva, Anatoli Juditsky, Guanghui Lan, Tianjiao Li</li>
<li>for:  solves a class of stochastic smooth convex optimization problems with general noise assumptions.</li>
<li>methods:  uses two non-Euclidean accelerated stochastic approximation routines: stochastic accelerated gradient descent (SAGD) and stochastic gradient extrapolation (SGE).</li>
<li>results:  achieves the optimal convergence rate and attains the optimal iteration and sample complexities simultaneously, with more general assumptions for SGE that allow for efficient application to statistical estimation problems under heavy tail noises and discontinuous score functions.<details>
<summary>Abstract</summary>
We consider a class of stochastic smooth convex optimization problems under rather general assumptions on the noise in the stochastic gradient observation. As opposed to the classical problem setting in which the variance of noise is assumed to be uniformly bounded, herein we assume that the variance of stochastic gradients is related to the "sub-optimality" of the approximate solutions delivered by the algorithm. Such problems naturally arise in a variety of applications, in particular, in the well-known generalized linear regression problem in statistics. However, to the best of our knowledge, none of the existing stochastic approximation algorithms for solving this class of problems attain optimality in terms of the dependence on accuracy, problem parameters, and mini-batch size.   We discuss two non-Euclidean accelerated stochastic approximation routines--stochastic accelerated gradient descent (SAGD) and stochastic gradient extrapolation (SGE)--which carry a particular duality relationship. We show that both SAGD and SGE, under appropriate conditions, achieve the optimal convergence rate, attaining the optimal iteration and sample complexities simultaneously. However, corresponding assumptions for the SGE algorithm are more general; they allow, for instance, for efficient application of the SGE to statistical estimation problems under heavy tail noises and discontinuous score functions. We also discuss the application of the SGE to problems satisfying quadratic growth conditions, and show how it can be used to recover sparse solutions. Finally, we report on some simulation experiments to illustrate numerical performance of our proposed algorithms in high-dimensional settings.
</details>
<details>
<summary>摘要</summary>
我团队考虑了一类泛化噪声 convex 优化问题，其中噪声噪声度受到较为一般的假设。与经典问题设定相比，我们假设噪声 variance 与优化解的"低效"有关。这些问题在各种应用中自然出现，如统计中的泛化线性回归问题。然而，据我们所知，现有的泛化噪声策略并没有达到最佳的依赖于准确性、问题参数和批处大小。我们介绍了两种非欧几何减速泛化策略：噪声加速梯度下降（SAGD）和梯度拓展（SGE）。我们证明了这两种策略，在合适的假设下，都可以达到最佳的准确率，同时具有最佳的迭代次数和批处大小复杂度。然而，SGE 算法的假设更加通用，允许在重 tailed 噪声和离散分数函数的情况下进行有效的应用。我们还讨论了 SGE 在 quadratic growth conditions 下的应用，并示出它可以用来恢复稀疏解。最后，我们报告了一些高维度设置下的仿真实验结果，以 illustrate 我们的提议方法的数值性能。
</details></li>
</ul>
<hr>
<h2 id="Review-of-Deep-Learning-based-Malware-Detection-for-Android-and-Windows-System"><a href="#Review-of-Deep-Learning-based-Malware-Detection-for-Android-and-Windows-System" class="headerlink" title="Review of Deep Learning-based Malware Detection for Android and Windows System"></a>Review of Deep Learning-based Malware Detection for Android and Windows System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01494">http://arxiv.org/abs/2307.01494</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nazmul Islam, Seokjoo Shin</li>
<li>for: 遥测和区分不同种类的黑客病毒，以评估其行为和威胁水平，并发展防御策略。</li>
<li>methods: 使用人工智能技术（AI）为抗黑客系统，以应对不同类型的隐藏和混淆技术。</li>
<li>results: 实验结果显示，使用AI技术可以实现百分之百的检测精度，探测不同类型的黑客病毒。<details>
<summary>Abstract</summary>
Differentiating malware is important to determine their behaviors and level of threat; as well as to devise defensive strategy against them. In response, various anti-malware systems have been developed to distinguish between different malwares. However, most of the recent malware families are Artificial Intelligence (AI) enable and can deceive traditional anti-malware systems using different obfuscation techniques. Therefore, only AI-enabled anti-malware system is robust against these techniques and can detect different features in the malware files that aid in malicious activities. In this study we review two AI-enabled techniques for detecting malware in Windows and Android operating system, respectively. Both the techniques achieved perfect accuracy in detecting various malware families.
</details>
<details>
<summary>摘要</summary>
不同的黑客软件有不同的行为和威胁水平，因此可以通过区分黑客软件来制定防御策略。然而，大多数最新的黑客软件家族具有人工智能（AI）功能，可以使用不同的隐蔽技术欺骗传统的防病软件。因此，只有使用AI技术的防病软件才能够对这些技术进行鲜活的检测和区分。本研究将介绍两种基于AI技术的防病方法，一种用于Windows操作系统，另一种用于Android操作系统。两种方法均达到了完美的检测精度，可以帮助检测不同的黑客软件家族。
</details></li>
</ul>
<hr>
<h2 id="FREEDOM-Target-Label-Source-Data-Domain-Information-Free-Multi-Source-Domain-Adaptation-for-Unsupervised-Personalization"><a href="#FREEDOM-Target-Label-Source-Data-Domain-Information-Free-Multi-Source-Domain-Adaptation-for-Unsupervised-Personalization" class="headerlink" title="FREEDOM: Target Label &amp; Source Data &amp; Domain Information-Free Multi-Source Domain Adaptation for Unsupervised Personalization"></a>FREEDOM: Target Label &amp; Source Data &amp; Domain Information-Free Multi-Source Domain Adaptation for Unsupervised Personalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02493">http://arxiv.org/abs/2307.02493</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eunju Yang, Gyusang Cho, Chan-Hyun Youn<br>for:* 这个研究是为了解决多源领域适应（Multi-Source Domain Adaptation，MSDA）中的问题，特别是在没有目标标签和多个领域的情况下进行模型适应。methods:* 这个研究提出了一个新的问题场景，即Three-Free Domain Adaptation（TFDA），在这个问题场景下，目标标签、源数据集和源领域资讯（领域标签和领域数量）都是不可用的。* 这个研究提出了一个实用的适应框架，called FREEDOM，它利用生成模型，将数据分解为类别和样式的两个方面，并使用非Parametric Bayesian方法来定义样式。在适应阶段，FREEDOM尝试将源类别分布与目标类别分布匹配，然后只部署部分的分类模型为个人化网络。results:* 这个研究获得了state-of-the-art或相等的性能，而且可以在没有领域资讯的情况下进行适应，并且将终端模型的大小减少到目标边缘。<details>
<summary>Abstract</summary>
From a service perspective, Multi-Source Domain Adaptation (MSDA) is a promising scenario to adapt a deployed model to a client's dataset. It can provide adaptation without a target label and support the case where a source dataset is constructed from multiple domains. However, it is impractical, wherein its training heavily relies on prior domain information of the multi-source dataset -- how many domains exist and the domain label of each data sample. Moreover, MSDA requires both source and target datasets simultaneously (physically), causing storage limitations on the client device or data privacy issues by transferring client data to a server. For a more practical scenario of model adaptation from a service provider's point of view, we relax these constraints and present a novel problem scenario of Three-Free Domain Adaptation, namely TFDA, where 1) target labels, 2) source dataset, and mostly 3) source domain information (domain labels + the number of domains) are unavailable. Under the problem scenario, we propose a practical adaptation framework called FREEDOM. It leverages the power of the generative model, disentangling data into class and style aspects, where the style is defined as the class-independent information from the source data and designed with a nonparametric Bayesian approach. In the adaptation stage, FREEDOM aims to match the source class distribution with the target's under the philosophy that class distribution is consistent even if the style is different; after then, only part of the classification model is deployed as a personalized network. As a result, FREEDOM achieves state-of-the-art or comparable performance even without domain information, with reduced final model size on the target side, independent of the number of source domains.
</details>
<details>
<summary>摘要</summary>
从服务角度来看，多源频率适应（MSDA）是一个有前途的场景，用于适应已部署模型到客户的数据集。它可以无需目标标签进行适应，并且支持多个源频率构建的情况。然而，它在训练中强依赖于多个源频率数据集的先前知识，以及每个数据样本的频率标签。此外，MSDA需要同时使用源和目标数据集（物理上），导致客户设备存储限制或数据隐私问题。为了更实际的模型适应场景，我们宽松了这些限制，并提出了一个新的问题场景：三自频率适应（TFDA），其中1）目标标签，2）源数据集，以及3）源频率信息（频率标签和频率数量）都不可用。在这种问题场景下，我们提出了一个实用的适应框架called FREEDOM。它利用了生成模型的力量，将数据分解成类和风格两个方面，其中风格被定义为来源数据中独立于类的信息，并使用非 Parametric Bayesian方法设计。在适应阶段，FREEDOM的目标是匹配源类分布与目标类分布，以哲学的思想，即类分布在风格不同的情况下仍然一致。然后，FREEDOM只部署一部分的分类模型作为个性化网络。因此，FREEDOM可以在无需源频率信息的情况下实现状态前或相当的性能，并且减少了目标模型的最终大小，不受源频率数量的影响。
</details></li>
</ul>
<hr>
<h2 id="Nexus-sine-qua-non-Essentially-Connected-Networks-for-Traffic-Forecasting"><a href="#Nexus-sine-qua-non-Essentially-Connected-Networks-for-Traffic-Forecasting" class="headerlink" title="Nexus sine qua non: Essentially Connected Networks for Traffic Forecasting"></a>Nexus sine qua non: Essentially Connected Networks for Traffic Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01482">http://arxiv.org/abs/2307.01482</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tong Nie, Guoyang Qin, Lijun Sun, Yunpeng Wang, Jian Sun</li>
<li>for: 本研究旨在开发简洁高效的神经网络模型，用于learnings representations和预测交通数据中的下一个时刻行为。</li>
<li>methods: 本研究使用了spatiotemporal graph neural networks (STGNNs)，但是现有STGNNs使用复杂的技术来捕捉交通数据中的结构，导致它们难以理解和扩展。因此，研究人员寻求了简单 yet efficient的architecture。研究人员发现了STGNN的表示中的核心是certain forms of spatiotemporal contextualization，并根据此设计了一种简单的efficient message-passing backbone，即Nexus sine qua non (NexuSQN)。</li>
<li>results: 研究人员发现，NexuSQN比较简单的结构，即使不使用复杂的RNNs、Transformers和diffusion convolutions，仍能在计算效率、精度和大小等方面超越了复杂的参考模型。这表明，将来可能有一个Promising future for developing simple yet efficient neural predictors。<details>
<summary>Abstract</summary>
Spatiotemporal graph neural networks (STGNNs) have emerged as a leading approach for learning representations and forecasting on traffic datasets with underlying topological and correlational structures. However, current STGNNs use intricate techniques with high complexities to capture these structures, making them difficult to understand and scale. The existence of simple yet efficient architectures remains an open question. Upon closer examination, we find what lies at the core of STGNN's representations are certain forms of spatiotemporal contextualization. In light of this, we design Nexus sine qua non (NexuSQN), an essentially connected network built on an efficient message-passing backbone. NexuSQN simply uses learnable "where" and "when" locators for the aforementioned contextualization and omits any intricate components such as RNNs, Transformers, and diffusion convolutions. Results show that NexuSQN outperforms intricately designed benchmarks in terms of size, computational efficiency, and accuracy. This suggests a promising future for developing simple yet efficient neural predictors.
</details>
<details>
<summary>摘要</summary>
现代各种图 neural networks (STGNNs) 已经成为学习表示和预测交通数据中的底层拓扑和相关结构的领先方法。然而，当前的 STGNNs 使用复杂的技术来捕捉这些结构，这使得它们变得难以理解和扩展。有效且简单的架构的存在仍然是一个开放的问题。经过仔细分析，我们发现 STGNN 的表示核心是一种特定的空间时间嵌入。基于这一点，我们设计了 Nexus sine qua non (NexuSQN)，一种简单而高效的网络。NexuSQN 使用学习的 "where" 和 "when" 嵌入来进行上述嵌入，并且不包含任何复杂的组件，如 RNNs、Transformers 和扩散卷积。结果表明，NexuSQN 在Size、计算效率和准确性三个方面超过了复杂设计的标准准。这表示在发展简单且高效的神经预测器方面，有一个广阔的未来。
</details></li>
</ul>
<hr>
<h2 id="Beyond-Conservatism-Diffusion-Policies-in-Offline-Multi-agent-Reinforcement-Learning"><a href="#Beyond-Conservatism-Diffusion-Policies-in-Offline-Multi-agent-Reinforcement-Learning" class="headerlink" title="Beyond Conservatism: Diffusion Policies in Offline Multi-agent Reinforcement Learning"></a>Beyond Conservatism: Diffusion Policies in Offline Multi-agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01472">http://arxiv.org/abs/2307.01472</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhuoran Li, Ling Pan, Longbo Huang</li>
<li>for: 本研究提出了一种新的多智能体偏好离线模型（DOM2），用于多智能体学习 reinforcement learning（MARL）环境中的离线学习。</li>
<li>methods: 在本研究中，我们将diffusion模型integrated into the policy network，并提出了一种基于轨迹的数据增强方案。这些关键元素使得我们的算法更加鲁棒对环境变化，并实现了 significiant improvements in performance, generalization和数据效率。</li>
<li>results: 我们的实验结果表明，DOM2在多智能体粒子和多智能体MuJoCo环境中比 existed state-of-the-art方法表现出更高的表现，并在shifted环境中具有更高的表现和更好的泛化能力。此外，DOM2也表现出了更高的数据效率，可以在$20++$ times less data的情况下达到state-of-the-art表现。<details>
<summary>Abstract</summary>
We present a novel Diffusion Offline Multi-agent Model (DOM2) for offline Multi-Agent Reinforcement Learning (MARL). Different from existing algorithms that rely mainly on conservatism in policy design, DOM2 enhances policy expressiveness and diversity based on diffusion. Specifically, we incorporate a diffusion model into the policy network and propose a trajectory-based data-augmentation scheme in training. These key ingredients make our algorithm more robust to environment changes and achieve significant improvements in performance, generalization and data-efficiency. Our extensive experimental results demonstrate that DOM2 outperforms existing state-of-the-art methods in multi-agent particle and multi-agent MuJoCo environments, and generalizes significantly better in shifted environments thanks to its high expressiveness and diversity. Furthermore, DOM2 shows superior data efficiency and can achieve state-of-the-art performance with $20+$ times less data compared to existing algorithms.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的扩散停机多智能体模型（DOM2），用于停机多智能体学习（MARL）。与现有算法不同，我们的算法不仅仅依靠保守性在策略设计中，而是通过扩散来增强策略表达能力和多样性。具体来说，我们在策略网络中 интегrollo了扩散模型，并提出了一种基于轨迹的数据扩充方案在训练中。这些关键元素使我们的算法更加鲁棒对环境变化，并在性能、泛化和数据效率方面达到了显著的改进。我们的广泛的实验结果表明，DOM2在多体分子和多体MuJoCo环境中比现有状态的方法表现出色，并在偏shifted环境中具有更高的表达能力和多样性。此外，DOM2还表现出了更好的数据效率，可以在$20++$times less data的情况下达到状态顶尖的性能。
</details></li>
</ul>
<hr>
<h2 id="A-Review-of-Driver-Gaze-Estimation-and-Application-in-Gaze-Behavior-Understanding"><a href="#A-Review-of-Driver-Gaze-Estimation-and-Application-in-Gaze-Behavior-Understanding" class="headerlink" title="A Review of Driver Gaze Estimation and Application in Gaze Behavior Understanding"></a>A Review of Driver Gaze Estimation and Application in Gaze Behavior Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01470">http://arxiv.org/abs/2307.01470</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pavan Kumar Sharma, Pranamesh Chakraborty</li>
<li>for: 本研究的主要目标是对driver gaze基础知识、测量方法和实际驾驶场景中的应用进行全面的总结。</li>
<li>methods: 本研究使用了头戴式和远程设置基于眼动估算的方法，以及与这些数据收集方法相关的术语。然后列出了现有的参考驾驶员眼动数据集，并讲述了数据收集方法和设备使用的方法。最后，本研究讲述了用于眼动估算的算法，主要是传统机器学习和深度学习基本方法。</li>
<li>results: 估算的驾驶员眼动被用于理解在交叉路口、上坡入口、下坡出口、车道变换和道路广告结构的影响。而且，本研究还讲述了现有文献中的限制、挑战和未来发展预cast。<details>
<summary>Abstract</summary>
Driver gaze plays an important role in different gaze-based applications such as driver attentiveness detection, visual distraction detection, gaze behavior understanding, and building driver assistance system. The main objective of this study is to perform a comprehensive summary of driver gaze fundamentals, methods to estimate driver gaze, and it's applications in real world driving scenarios. We first discuss the fundamentals related to driver gaze, involving head-mounted and remote setup based gaze estimation and the terminologies used for each of these data collection methods. Next, we list out the existing benchmark driver gaze datasets, highlighting the collection methodology and the equipment used for such data collection. This is followed by a discussion of the algorithms used for driver gaze estimation, which primarily involves traditional machine learning and deep learning based techniques. The estimated driver gaze is then used for understanding gaze behavior while maneuvering through intersections, on-ramps, off-ramps, lane changing, and determining the effect of roadside advertising structures. Finally, we have discussed the limitations in the existing literature, challenges, and the future scope in driver gaze estimation and gaze-based applications.
</details>
<details>
<summary>摘要</summary>
Driver's gaze  plays an important role in various gaze-based applications, such as detecting driver attentiveness, visual distraction, and understanding gaze behavior. The main objective of this study is to provide a comprehensive overview of driver gaze fundamentals, methods for estimating driver gaze, and its applications in real-world driving scenarios.First, we discuss the fundamentals of driver gaze, including head-mounted and remote setup-based gaze estimation, and the terminologies used for each data collection method. Next, we list out the existing benchmark driver gaze datasets, highlighting the collection methodology and equipment used for such data collection.Then, we discuss the algorithms used for driver gaze estimation, primarily involving traditional machine learning and deep learning-based techniques. The estimated driver gaze is used to understand gaze behavior while maneuvering through intersections, on-ramps, off-ramps, lane changing, and the effect of roadside advertising structures.Finally, we discuss the limitations in the existing literature, challenges, and the future scope in driver gaze estimation and gaze-based applications.
</details></li>
</ul>
<hr>
<h2 id="Causal-Reinforcement-Learning-A-Survey"><a href="#Causal-Reinforcement-Learning-A-Survey" class="headerlink" title="Causal Reinforcement Learning: A Survey"></a>Causal Reinforcement Learning: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01452">http://arxiv.org/abs/2307.01452</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Zhihong Deng, Jing Jiang, Guodong Long, Chengqi Zhang</li>
<li>for: 本研究写作的目的是对于 causal reinforcement learning 的文献审查和概述。</li>
<li>methods: 本文使用的方法包括 introducing basic concepts of causality and reinforcement learning, 以及 categorizing and systematically reviewing existing causal reinforcement learning approaches based on their target problems and methodologies.</li>
<li>results: 本文审查了 current literature on causal reinforcement learning, 并发现了 several open issues and future directions in this emerging field.<details>
<summary>Abstract</summary>
Reinforcement learning is an essential paradigm for solving sequential decision problems under uncertainty. Despite many remarkable achievements in recent decades, applying reinforcement learning methods in the real world remains challenging. One of the main obstacles is that reinforcement learning agents lack a fundamental understanding of the world and must therefore learn from scratch through numerous trial-and-error interactions. They may also face challenges in providing explanations for their decisions and generalizing the acquired knowledge. Causality, however, offers a notable advantage as it can formalize knowledge in a systematic manner and leverage invariance for effective knowledge transfer. This has led to the emergence of causal reinforcement learning, a subfield of reinforcement learning that seeks to enhance existing algorithms by incorporating causal relationships into the learning process. In this survey, we comprehensively review the literature on causal reinforcement learning. We first introduce the basic concepts of causality and reinforcement learning, and then explain how causality can address core challenges in non-causal reinforcement learning. We categorize and systematically review existing causal reinforcement learning approaches based on their target problems and methodologies. Finally, we outline open issues and future directions in this emerging field.
</details>
<details>
<summary>摘要</summary>
<<SYS>>通过下面的文本翻译到简化中文：<</SYS>>Control learning是一种重要的思想方式，用于解决带有不确定性的顺序决策问题。虽然在过去几十年内，有很多出色的成果，但是在实际应用中仍然存在很多挑战。其中一个主要的障碍是控制学学习代理不具备世界的基本理解，因此需要通过大量的尝试和错误互动来学习。它们还可能面临着解释决策的挑战和掌握知识的一致性问题。然而， causality 提供了一种明显的优势，即可以系统地ormalize知识，并利用不变性来实现有效的知识传递。这导致了 causal reinforcement learning 的出现，这是一种尝试将 causality integrated 到学习过程中的一种新领域。在这篇评论中，我们全面评论了 literature 中的 causal reinforcement learning 研究。我们首先介绍了 causality 和 reinforcement learning 的基本概念，然后解释了如何通过 causality 解决非 causal reinforcement learning 中的核心挑战。然后，我们按照目标问题和方法分类系统地审查了现有的 causal reinforcement learning 方法。最后，我们列出了未解决的问题和未来的方向。
</details></li>
</ul>
<hr>
<h2 id="A-Double-Machine-Learning-Approach-to-Combining-Experimental-and-Observational-Data"><a href="#A-Double-Machine-Learning-Approach-to-Combining-Experimental-and-Observational-Data" class="headerlink" title="A Double Machine Learning Approach to Combining Experimental and Observational Data"></a>A Double Machine Learning Approach to Combining Experimental and Observational Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01449">http://arxiv.org/abs/2307.01449</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marco Morucci, Vittorio Orlandi, Harsh Parikh, Sudeepa Roy, Cynthia Rudin, Alexander Volfovsky</li>
<li>for: 该论文旨在提出一种将实验和观察研究结合起来的双机器学习方法，以便实践者可以一起测试假设的满足性和对治疗效果的估计。</li>
<li>methods: 该方法使用双机器学习技术将实验和观察研究结合起来，以检测假设的满足性和外部有效性的违反。当只有一个假设被违反时，我们提供了半 Parametric 有效的治疗效果估计器。</li>
<li>results: 该研究在三个实际应用场景中展示了其适用性，并指出了准确地识别违反假设的重要性以确保治疗效果的估计的重要性。<details>
<summary>Abstract</summary>
Experimental and observational studies often lack validity due to untestable assumptions. We propose a double machine learning approach to combine experimental and observational studies, allowing practitioners to test for assumption violations and estimate treatment effects consistently. Our framework tests for violations of external validity and ignorability under milder assumptions. When only one assumption is violated, we provide semi-parametrically efficient treatment effect estimators. However, our no-free-lunch theorem highlights the necessity of accurately identifying the violated assumption for consistent treatment effect estimation. We demonstrate the applicability of our approach in three real-world case studies, highlighting its relevance for practical settings.
</details>
<details>
<summary>摘要</summary>
实验和观察研究经常受到有效性问题，因为假设通常无法被证明。我们提出了一种双机器学习方法，可以结合实验和观察研究，让实践者可以测试假设违背和估计治疗效果一致。我们的框架测试了外部有效性和无知性的违背，假设较弱的假设违背。只有一个假设违背时，我们提供了半 parametrically有效的治疗效果估计器。但我们的无免责 theorem 显示，精确地识别违背的假设是估计治疗效果一致的必要条件。我们在三个实际应用中例子中详细介绍了我们的方法，强调了它在实践中的重要性。
</details></li>
</ul>
<hr>
<h2 id="On-Conditional-and-Compositional-Language-Model-Differentiable-Prompting"><a href="#On-Conditional-and-Compositional-Language-Model-Differentiable-Prompting" class="headerlink" title="On Conditional and Compositional Language Model Differentiable Prompting"></a>On Conditional and Compositional Language Model Differentiable Prompting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01446">http://arxiv.org/abs/2307.01446</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jpilaul/PRopS">https://github.com/jpilaul/PRopS</a></li>
<li>paper_authors: Jonathan Pilault, Can Liu, Mohit Bansal, Markus Dreyer</li>
<li>for: 本研究旨在调整静态语言模型（PLM），以便在下游任务中表现出色。</li>
<li>methods: 本研究使用 conditional和compositional differentiable prompting，并提出了一种新的模型——Prompt Production System（PRopS），可以将任务说明或输入元数据转换成细化的Continuous prompts，以便从PLM中获得任务特定的输出。PRopS使用基于神经网络的Production Systems结构，可以学习不同的提示输入模式，进行可compose转换，适用于小样本学习和过渡学习。</li>
<li>results: 研究表明，PRopS可以在compositional generalization任务、可控摘要和多语言翻译中，Consistently exceed other PLM adaptation techniques，并经常超越完全精心调整模型。同时，PRopS需要 fewer trainable parameters，适合实际应用。<details>
<summary>Abstract</summary>
Prompts have been shown to be an effective method to adapt a frozen Pretrained Language Model (PLM) to perform well on downstream tasks. Prompts can be represented by a human-engineered word sequence or by a learned continuous embedding. In this work, we investigate conditional and compositional differentiable prompting. We propose a new model, Prompt Production System (PRopS), which learns to transform task instructions or input metadata, into continuous prompts that elicit task-specific outputs from the PLM. Our model uses a modular network structure based on our neural formulation of Production Systems, which allows the model to learn discrete rules -- neural functions that learn to specialize in transforming particular prompt input patterns, making it suitable for compositional transfer learning and few-shot learning. We present extensive empirical and theoretical analysis and show that PRopS consistently surpasses other PLM adaptation techniques, and often improves upon fully fine-tuned models, on compositional generalization tasks, controllable summarization and multilingual translation, while needing fewer trainable parameters.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换给定文本到简化中文。</SYS>文本：提示已经被证明是一种有效的方法，用于适应预训练语言模型（PLM）在下游任务中表现良好。提示可以表示为人工设计的单词序列或学习到的连续嵌入。在这种工作中，我们研究了决定式和组合的可导提示。我们提出了一种新的模型，提示生产系统（PRopS），该模型学习将任务指令或输入元数据转换为可导的提示，以便从PLM中获取任务特定的输出。我们的模型采用基于我们的神经网络表述的生产系统结构，该结构允许模型学习分解规则——神经函数学习特定提示输入模式的特殊化，使其适用于组合转移学习和少量学习。我们进行了广泛的实验和理论分析，并证明了PRopS在组合泛化任务、可控概要摘要和多语言翻译中表现出色，而需要 fewer 可训练参数。
</details></li>
</ul>
<hr>
<h2 id="Human-Emotion-Recognition-Based-On-Galvanic-Skin-Response-signal-Feature-Selection-and-SVM"><a href="#Human-Emotion-Recognition-Based-On-Galvanic-Skin-Response-signal-Feature-Selection-and-SVM" class="headerlink" title="Human Emotion Recognition Based On Galvanic Skin Response signal Feature Selection and SVM"></a>Human Emotion Recognition Based On Galvanic Skin Response signal Feature Selection and SVM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05383">http://arxiv.org/abs/2307.05383</a></li>
<li>repo_url: None</li>
<li>paper_authors: Di Fan, Mingyang Liu, Xiaohan Zhang, Xiaopeng Gong</li>
<li>for: 本研究提出了一种基于自动选择的 galvanic skin response (GSR) 信号特征和 Support Vector Machine (SVM) 的人类情感识别方法。</li>
<li>methods: 研究使用 e-Health Sensor Platform V2.0 获取 GSR 信号，然后使用浮点函数除噪和normalize 处理数据，提取30个特征。然后，使用协方差基于的特征选择来优化特征。最后，使用 SVM 输入优化特征进行人类情感识别。</li>
<li>results: 实验结果表明，提出的方法可以实现好的人类情感识别，识别率高于 66.67%。<details>
<summary>Abstract</summary>
A novel human emotion recognition method based on automatically selected Galvanic Skin Response (GSR) signal features and SVM is proposed in this paper. GSR signals were acquired by e-Health Sensor Platform V2.0. Then, the data is de-noised by wavelet function and normalized to get rid of the individual difference. 30 features are extracted from the normalized data, however, directly using of these features will lead to a low recognition rate. In order to gain the optimized features, a covariance based feature selection is employed in our method. Finally, a SVM with input of the optimized features is utilized to achieve the human emotion recognition. The experimental results indicate that the proposed method leads to good human emotion recognition, and the recognition accuracy is more than 66.67%.
</details>
<details>
<summary>摘要</summary>
本文提出了一种基于自动选择的galvanic skin response（GSR）信号特征和支持向量机（SVM）的人类情感识别方法。GSR信号通过e-Health感知平台V2.0获取。然后，数据进行杂谱函数滤波和normalizaation处理，以消除个体差异。从 нормализов的数据中提取了30个特征，但直接使用这些特征将导致低的识别率。为了获得优化的特征，我们在方法中使用covariance基于的特征选择。最后，使用输入优化特征的SVM实现人类情感识别。实验结果表明，提出的方法可以实现良好的人类情感识别，识别率高于66.67%。
</details></li>
</ul>
<hr>
<h2 id="TablEye-Seeing-small-Tables-through-the-Lens-of-Images"><a href="#TablEye-Seeing-small-Tables-through-the-Lens-of-Images" class="headerlink" title="TablEye: Seeing small Tables through the Lens of Images"></a>TablEye: Seeing small Tables through the Lens of Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02491">http://arxiv.org/abs/2307.02491</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seung-eon Lee, Sang-Chul Lee</li>
<li>for: 这个论文目的是解决几个板表学习问题，具体来说是在几个板表数据上培养模型，而不需要大量标签数据。</li>
<li>methods: 这个论文使用的方法是基于域转换的，通过生成板表图像来保持原始板表数据的内在 semantics。然后使用已经测试过的几个shot学习算法和嵌入函数来获得和应用优先知识。</li>
<li>results: 这个论文的结果表明，TablEye在4个shot任务中的最高AUC为0.11，在1个shot设置中的平均错误率高于TabLLM by 3.17%。这表明TablEye在几个板表数据上具有更好的性能。<details>
<summary>Abstract</summary>
The exploration of few-shot tabular learning becomes imperative. Tabular data is a versatile representation that captures diverse information, yet it is not exempt from limitations, property of data and model size. Labeling extensive tabular data can be challenging, and it may not be feasible to capture every important feature. Few-shot tabular learning, however, remains relatively unexplored, primarily due to scarcity of shared information among independent datasets and the inherent ambiguity in defining boundaries within tabular data. To the best of our knowledge, no meaningful and unrestricted few-shot tabular learning techniques have been developed without imposing constraints on the dataset. In this paper, we propose an innovative framework called TablEye, which aims to overcome the limit of forming prior knowledge for tabular data by adopting domain transformation. It facilitates domain transformation by generating tabular images, which effectively conserve the intrinsic semantics of the original tabular data. This approach harnesses rigorously tested few-shot learning algorithms and embedding functions to acquire and apply prior knowledge. Leveraging shared data domains allows us to utilize this prior knowledge, originally learned from the image domain. Specifically, TablEye demonstrated a superior performance by outstripping the TabLLM in a 4-shot task with a maximum 0.11 AUC and a STUNT in a 1- shot setting, where it led on average by 3.17% accuracy.
</details>
<details>
<summary>摘要</summary>
exploration of few-shot tabular learning becoming increasingly important. 表格数据是一种多样化表示方式，它可以捕捉多种信息，但同时也有一些限制，例如数据属性和模型大小。对于大量表格数据的标注可能是困难的，而且可能无法捕捉所有重要的特征。然而，几 shot tabular learning仍然尚未得到广泛的探索，主要是因为独立的数据集之间的共享信息缺乏，以及表格数据中的内在含义是不具有明确定义的。根据我们所知，没有任何不受限制的几 shot tabular learning技术已经被开发出来，没有强制要求数据集的限制。在这篇论文中，我们提出了一个创新的框架，即TablEye，以解决表格数据的几 shot learning问题。TablEye采用域转换来解决几 shot learning问题，通过生成表格图像来保留原始表格数据的内在含义。这种方法利用了已经测试过的几 shot学习算法和嵌入函数，以获取和应用先前知识。通过共享数据域，我们可以利用这些先前知识，原来学习自图像领域。特别是，TablEye在4 shot任务中的最大AUC为0.11，在1 shot任务中的平均准确率高于TabLLM的3.17%。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Branch-in-Combinatorial-Optimization-with-Graph-Pointer-Networks"><a href="#Learning-to-Branch-in-Combinatorial-Optimization-with-Graph-Pointer-Networks" class="headerlink" title="Learning to Branch in Combinatorial Optimization with Graph Pointer Networks"></a>Learning to Branch in Combinatorial Optimization with Graph Pointer Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01434">http://arxiv.org/abs/2307.01434</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rui Wang, Zhiming Zhou, Tao Zhang, Ling Wang, Xin Xu, Xiangke Liao, Kaiwen Li</li>
<li>for: 解决 combinatorial optimization 问题的variable选择策略学习</li>
<li>methods: 提出了一种基于图Pointer网络的变量选择策略学习模型，利用图特征、全局特征和历史特征来表示解决器状态</li>
<li>results: 实验表明，提出的方法可以有效地将解决器状态映射到分支变量决策中，并且在各种benchmark问题上显著超越了经典强分支专家规则，同时也超越了当前最佳机器学习基于分支和缓存的方法。<details>
<summary>Abstract</summary>
Branch-and-bound is a typical way to solve combinatorial optimization problems. This paper proposes a graph pointer network model for learning the variable selection policy in the branch-and-bound. We extract the graph features, global features and historical features to represent the solver state. The proposed model, which combines the graph neural network and the pointer mechanism, can effectively map from the solver state to the branching variable decisions. The model is trained to imitate the classic strong branching expert rule by a designed top-k Kullback-Leibler divergence loss function. Experiments on a series of benchmark problems demonstrate that the proposed approach significantly outperforms the widely used expert-designed branching rules. Our approach also outperforms the state-of-the-art machine-learning-based branch-and-bound methods in terms of solving speed and search tree size on all the test instances. In addition, the model can generalize to unseen instances and scale to larger instances.
</details>
<details>
<summary>摘要</summary>
通常的方法之一用于解决 combinatorial optimization 问题是 branch-and-bound。这篇论文提出了一种图像指针网络模型，用于学习变量选择策略在 branch-and-bound 中。我们提取了图像特征、全局特征和历史特征来表示解决器状态。提议的模型，结合图像神经网络和指针机制，可以有效地将解决器状态映射到分支变量决策。模型通过一个设计的 top-k Kullback-Leibler 分布差函数进行训练，以模仿经典的强分支专家规则。实验表明，提议的方法在一系列的 benchmark 问题上显著超越了通用的专家设计的分支规则。我们的方法还超越了当前的Machine Learning 基于 branch-and-bound 方法在所有测试实例上的解决速度和搜索树大小。此外，模型还可以泛化到未看过的实例和更大的实例。
</details></li>
</ul>
<hr>
<h2 id="SleepEGAN-A-GAN-enhanced-Ensemble-Deep-Learning-Model-for-Imbalanced-Classification-of-Sleep-Stages"><a href="#SleepEGAN-A-GAN-enhanced-Ensemble-Deep-Learning-Model-for-Imbalanced-Classification-of-Sleep-Stages" class="headerlink" title="SleepEGAN: A GAN-enhanced Ensemble Deep Learning Model for Imbalanced Classification of Sleep Stages"></a>SleepEGAN: A GAN-enhanced Ensemble Deep Learning Model for Imbalanced Classification of Sleep Stages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05362">http://arxiv.org/abs/2307.05362</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuewei Cheng, Ke Huang, Yi Zou, Shujie Ma</li>
<li>for:  automatische slaapfaseclassificatie</li>
<li>methods: GAN-powered ensemble deep learning model (SleepEGAN) en data-augmentatie</li>
<li>results: verbeterde classificatie-accurateit compared to existing state-of-the-art methods using three public sleep datasets.<details>
<summary>Abstract</summary>
Deep neural networks have played an important role in automatic sleep stage classification because of their strong representation and in-model feature transformation abilities. However, class imbalance and individual heterogeneity which typically exist in raw EEG signals of sleep data can significantly affect the classification performance of any machine learning algorithms. To solve these two problems, this paper develops a generative adversarial network (GAN)-powered ensemble deep learning model, named SleepEGAN, for the imbalanced classification of sleep stages. To alleviate class imbalance, we propose a new GAN (called EGAN) architecture adapted to the features of EEG signals for data augmentation. The generated samples for the minority classes are used in the training process. In addition, we design a cost-free ensemble learning strategy to reduce the model estimation variance caused by the heterogeneity between the validation and test sets, so as to enhance the accuracy and robustness of prediction performance. We show that the proposed method can improve classification accuracy compared to several existing state-of-the-art methods using three public sleep datasets.
</details>
<details>
<summary>摘要</summary>
深度神经网络在自动睡眠阶段分类中发挥了重要作用，因为它们具有强大的表示能力和内存中特征转换能力。然而， raw EEG 信号中的分类不均和个体差异通常会对任何机器学习算法的分类性能产生很大的影响。为解决这两个问题，本文提出了基于生成对抗网络（GAN）的 ensemble 深度学习模型，名为 SleepEGAN，用于不均分类睡眠阶段。为了缓解分类不均，我们提出了一种适应 EEG 信号特点的新 GAN 架构（称为 EGAN），用于数据增强。生成的小类样本在训练过程中使用。此外，我们设计了一种免费的ensemble学习策略，以降低因验证集和测试集之间的个体差异而导致的模型估计方差，以提高预测性能的准确性和稳定性。我们示示了提案的方法可以在三个公共睡眠数据集上提高分类精度，比较现有的一些状态之 arts 方法。
</details></li>
</ul>
<hr>
<h2 id="Smart-filter-aided-domain-adversarial-neural-network-An-unsupervised-domain-adaptation-method-for-fault-diagnosis-in-noisy-industrial-scenarios"><a href="#Smart-filter-aided-domain-adversarial-neural-network-An-unsupervised-domain-adaptation-method-for-fault-diagnosis-in-noisy-industrial-scenarios" class="headerlink" title="Smart filter aided domain adversarial neural network: An unsupervised domain adaptation method for fault diagnosis in noisy industrial scenarios"></a>Smart filter aided domain adversarial neural network: An unsupervised domain adaptation method for fault diagnosis in noisy industrial scenarios</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01429">http://arxiv.org/abs/2307.01429</a></li>
<li>repo_url: None</li>
<li>paper_authors: Baorui Dai, Gaëtan Frusque, Tianfu Li, Qi Li, Olga Fink</li>
<li>for: 这个研究旨在提出一种基于不监督领域适应（Unsupervised Domain Adaptation, UDA）的缺陷诊断方法，以便在实际工业应用中将运作经验和缺陷特征转移到不同的运作条件、不同的机器设备或实际数据和模拟数据之间。</li>
<li>methods: 本研究提出了一种名为Smart Filter-Aided Domain Adversarial Neural Network（SFDANN）的缺陷诊断方法，其主要包括两个步骤。第一步是发展一个智能节点，它可以在时间-频域域中强制同源和目标领域数据的相似性。第二步是将重建后的数据输入到一个领域对抗神经网络（Domain Adversarial Neural Network, DANN）中，以学习领域不断和特征分类。</li>
<li>results: 本研究运用了两个缺陷诊断案例，一是磨削机缺陷诊断在噪音环境中，另一是列车轨道缺陷诊断在列车-轨道-桥梁组合震动系统中，这两个案例都是将模拟数据转移到实际数据上，以验证SFDANN方法的效果。结果显示，相比于其他代表性的UDA方法，SFDANN方法在稳定性和识别性方面表现出色。<details>
<summary>Abstract</summary>
The application of unsupervised domain adaptation (UDA)-based fault diagnosis methods has shown significant efficacy in industrial settings, facilitating the transfer of operational experience and fault signatures between different operating conditions, different units of a fleet or between simulated and real data. However, in real industrial scenarios, unknown levels and types of noise can amplify the difficulty of domain alignment, thus severely affecting the diagnostic performance of deep learning models. To address this issue, we propose an UDA method called Smart Filter-Aided Domain Adversarial Neural Network (SFDANN) for fault diagnosis in noisy industrial scenarios. The proposed methodology comprises two steps. In the first step, we develop a smart filter that dynamically enforces similarity between the source and target domain data in the time-frequency domain. This is achieved by combining a learnable wavelet packet transform network (LWPT) and a traditional wavelet packet transform module. In the second step, we input the data reconstructed by the smart filter into a domain adversarial neural network (DANN). To learn domain-invariant and discriminative features, the learnable modules of SFDANN are trained in a unified manner with three objectives: time-frequency feature proximity, domain alignment, and fault classification. We validate the effectiveness of the proposed SFDANN method based on two fault diagnosis cases: one involving fault diagnosis of bearings in noisy environments and another involving fault diagnosis of slab tracks in a train-track-bridge coupling vibration system, where the transfer task involves transferring from numerical simulations to field measurements. Results show that compared to other representative state of the art UDA methods, SFDANN exhibits superior performance and remarkable stability.
</details>
<details>
<summary>摘要</summary>
通过不监督领域适应（UDA）基本的缺陷诊断方法应用，在实际工业场景中显示出了显著的效果，帮助传输不同操作条件、不同单元的船队中的运行经验和缺陷特征。然而，在实际工业场景中，未知的噪声水平和类型可能会增加领域对Alignment的困难度，从而严重地affect Deep learning模型的诊断性能。为解决这个问题，我们提出了一种名为智能筛子援助领域对抗神经网络（SFDANN）的UDA方法，用于缺陷诊断在噪声rich工业场景中。该方法包括两个步骤：第一步：我们开发了一种智能筛子，通过将源频域和目标频域数据在时域频域上进行动态相似性检查，以确保频域数据的匹配。这是通过组合学习抽象射频包变换网络（LWPT）和传统的抽象射频包变换模块来实现的。第二步：我们将重构后的数据输入到领域对抗神经网络（DANN）中，以学习频域特征的域不可分别性和分类特征。我们将学习模块在一起训练三个目标：时域特征的相似性、频域对齐和缺陷分类。我们验证了我们提出的SFDANN方法的效果，在磁矿轮毂缺陷诊断和铁路桥摆车轨缺陷诊断两个案例中进行了比较，其中一个案例是在噪声环境中进行磁矿轮毂缺陷诊断，另一个案例是在铁路桥摆车轨缺陷诊断中，将数据从数值仿真转移到场景测量中。结果显示，相比其他代表性的UDA方法，SFDANN方法在稳定性和性能两个方面具有显著优势。
</details></li>
</ul>
<hr>
<h2 id="Generative-Flow-Networks-a-Markov-Chain-Perspective"><a href="#Generative-Flow-Networks-a-Markov-Chain-Perspective" class="headerlink" title="Generative Flow Networks: a Markov Chain Perspective"></a>Generative Flow Networks: a Markov Chain Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01422">http://arxiv.org/abs/2307.01422</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tristan Deleu, Yoshua Bengio</li>
<li>for: 这篇论文是为了提出一种基于Markov链 Monte Carlo方法的新框架，用于采样高多modal的概率分布。</li>
<li>methods: 论文使用Generative Flow Networks（GFlowNets）作为一种新的采样框架，通过对采样视为一个顺序决策问题来mitigate高多modal的问题。</li>
<li>results: 论文提出了一种新的框架，可以在不同的状态空间下视为一种回归Markov链，并且可以通过对GFlowNets进行抽象来看到它们与MCMC方法之间的相似性。<details>
<summary>Abstract</summary>
While Markov chain Monte Carlo methods (MCMC) provide a general framework to sample from a probability distribution defined up to normalization, they often suffer from slow convergence to the target distribution when the latter is highly multi-modal. Recently, Generative Flow Networks (GFlowNets) have been proposed as an alternative framework to mitigate this issue when samples have a clear compositional structure, by treating sampling as a sequential decision making problem. Although they were initially introduced from the perspective of flow networks, the recent advances of GFlowNets draw more and more inspiration from the Markov chain literature, bypassing completely the need for flows. In this paper, we formalize this connection and offer a new perspective for GFlowNets using Markov chains, showing a unifying view for GFlowNets regardless of the nature of the state space as recurrent Markov chains. Positioning GFlowNets under the same theoretical framework as MCMC methods also allows us to identify the similarities between both frameworks, and most importantly to highlight their
</details>
<details>
<summary>摘要</summary>
While Markov chain Monte Carlo methods (MCMC) provide a general framework to sample from a probability distribution defined up to normalization, they often suffer from slow convergence to the target distribution when the latter is highly multi-modal. Recently, Generative Flow Networks (GFlowNets) have been proposed as an alternative framework to mitigate this issue when samples have a clear compositional structure, by treating sampling as a sequential decision making problem. Although they were initially introduced from the perspective of flow networks, the recent advances of GFlowNets draw more and more inspiration from the Markov chain literature, bypassing completely the need for flows. In this paper, we formalize this connection and offer a new perspective for GFlowNets using Markov chains, showing a unifying view for GFlowNets regardless of the nature of the state space as recurrent Markov chains. Positioning GFlowNets under the same theoretical framework as MCMC methods also allows us to identify the similarities between both frameworks, and most importantly to highlight their differences.Note: The translation is done using a machine translation tool, and may not be perfect.
</details></li>
</ul>
<hr>
<h2 id="Free-energy-of-Bayesian-Convolutional-Neural-Network-with-Skip-Connection"><a href="#Free-energy-of-Bayesian-Convolutional-Neural-Network-with-Skip-Connection" class="headerlink" title="Free energy of Bayesian Convolutional Neural Network with Skip Connection"></a>Free energy of Bayesian Convolutional Neural Network with Skip Connection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01417">http://arxiv.org/abs/2307.01417</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuya Nagayasu, Sumio Watanabe</li>
<li>for: 本研究探讨了Convolutional Neural Networks(CNNs)中skip connection的效果，以及 Bayesian learning中这种结构的可能性。</li>
<li>methods: 本研究使用了Bayesian方法来研究CNNs中skip connection的效果，并对 Bayesian CNN的一般化性能进行了解释。</li>
<li>results: 研究发现，Bayesian CNN中skip connection的upper bound of free energy不依赖于过参数，并且Bayesian CNN的一般化错误有类似的性能。<details>
<summary>Abstract</summary>
Since the success of Residual Network(ResNet), many of architectures of Convolutional Neural Networks(CNNs) have adopted skip connection. While the generalization performance of CNN with skip connection has been explained within the framework of Ensemble Learning, the dependency on the number of parameters have not been revealed. In this paper, we show that Bayesian free energy of Convolutional Neural Network both with and without skip connection in Bayesian learning. The upper bound of free energy of Bayesian CNN with skip connection does not depend on the oveparametrization and, the generalization error of Bayesian CNN has similar property.
</details>
<details>
<summary>摘要</summary>
自Residual Network(ResNet)的成功以来，许多Convolutional Neural Networks(CNNs)的 arquitectures 已经采用了跳connection。然而，通用的参数数量对CNN with skip connection的泛化性能的影响还没有得到解释。在这篇论文中，我们展示了Bayesian free energy of Convolutional Neural Network both with and without skip connection in Bayesian learning。无论是Bayesian CNN with skip connection还是Bayesian CNN without skip connection，其Upper bound of free energy都不依赖于过参数化，而泛化误差的性能也具有相同的性质。
</details></li>
</ul>
<hr>
<h2 id="Analyzing-the-vulnerabilities-in-SplitFed-Learning-Assessing-the-robustness-against-Data-Poisoning-Attacks"><a href="#Analyzing-the-vulnerabilities-in-SplitFed-Learning-Assessing-the-robustness-against-Data-Poisoning-Attacks" class="headerlink" title="Analyzing the vulnerabilities in SplitFed Learning: Assessing the robustness against Data Poisoning Attacks"></a>Analyzing the vulnerabilities in SplitFed Learning: Assessing the robustness against Data Poisoning Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03197">http://arxiv.org/abs/2307.03197</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aysha Thahsin Zahir Ismail, Raj Mani Shukla</li>
<li>for: 这个论文旨在研究和分析 SplitFed Learning (SFL) 中数据毒素攻击的影响。</li>
<li>methods: 该论文提出了三种新的攻击策略，包括无目标攻击、targeted攻击和距离基于攻击。</li>
<li>results: 研究发现，无目标和距离基于攻击在SFL中有更大的影响，比targeted攻击更容易让分类器输出错误。研究还通过对两个案例研究（electrocardiogram signal classification和自动手写数字识别）进行了多个攻击实验，并分析了攻击的影响。<details>
<summary>Abstract</summary>
Distributed Collaborative Machine Learning (DCML) is a potential alternative to address the privacy concerns associated with centralized machine learning. The Split learning (SL) and Federated Learning (FL) are the two effective learning approaches in DCML. Recently there have been an increased interest on the hybrid of FL and SL known as the SplitFed Learning (SFL). This research is the earliest attempt to study, analyze and present the impact of data poisoning attacks in SFL. We propose three kinds of novel attack strategies namely untargeted, targeted and distance-based attacks for SFL. All the attacks strategies aim to degrade the performance of the DCML-based classifier. We test the proposed attack strategies for two different case studies on Electrocardiogram signal classification and automatic handwritten digit recognition. A series of attack experiments were conducted by varying the percentage of malicious clients and the choice of the model split layer between the clients and the server. The results after the comprehensive analysis of attack strategies clearly convey that untargeted and distance-based poisoning attacks have greater impacts in evading the classifier outcomes compared to targeted attacks in SFL
</details>
<details>
<summary>摘要</summary>
分布式协作机器学习（DCML）是一种可能的中央机器学习隐私问题的解决方案。分布式学习（SL）和联邦学习（FL）是DCML中两种有效的学习方法。最近，关注于SL和FL的混合，即SplitFed Learning（SFL）的研究增长。这项研究是对SFL中数据毒化攻击的首次研究。我们提出了三种新的攻击策略，namely 无目标、Targeted和距离基于攻击，这三种攻击策略都是为了降低基于DCML的分类器性能。我们在两个不同的案例研究中进行了电室心跳信号分类和自动手写数字识别的试验。我们在clients和服务器之间的模型 Split层进行了变化，并通过调整恶意客户端的百分比和选择的模型 Split层来进行了一系列攻击实验。结果表明，无目标和距离基于攻击更有可能影响DCML-based分类器的性能，compared to Targeted attacks。
</details></li>
</ul>
<hr>
<h2 id="Multi-Predictor-Fusion-Combining-Learning-based-and-Rule-based-Trajectory-Predictors"><a href="#Multi-Predictor-Fusion-Combining-Learning-based-and-Rule-based-Trajectory-Predictors" class="headerlink" title="Multi-Predictor Fusion: Combining Learning-based and Rule-based Trajectory Predictors"></a>Multi-Predictor Fusion: Combining Learning-based and Rule-based Trajectory Predictors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01408">http://arxiv.org/abs/2307.01408</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sushant Veer, Apoorva Sharma, Marco Pavone</li>
<li>for: 这篇论文是关于自动驾驶车辆（AV）的 trajectory 预测模块，尤其是在高度互动的交通enario中，以提高安全和效率的规划计划。</li>
<li>methods: 这篇论文提出了一种名为多predictor fusion（MPF）的算法，它将学习基于predictors和逻辑规则的motions planners结合在一起，以提高学习型预测器的性能。MPF使用probabilistic combining方法，将学习型和逻辑规则基的预测器的轨迹混合在一起，以获得最佳性能。</li>
<li>results: 根据我们的结果，MPF在多种指标上表现出色，并且在线性能最高和最稳定的情况下运行。<details>
<summary>Abstract</summary>
Trajectory prediction modules are key enablers for safe and efficient planning of autonomous vehicles (AVs), particularly in highly interactive traffic scenarios. Recently, learning-based trajectory predictors have experienced considerable success in providing state-of-the-art performance due to their ability to learn multimodal behaviors of other agents from data. In this paper, we present an algorithm called multi-predictor fusion (MPF) that augments the performance of learning-based predictors by imbuing them with motion planners that are tasked with satisfying logic-based rules. MPF probabilistically combines learning- and rule-based predictors by mixing trajectories from both standalone predictors in accordance with a belief distribution that reflects the online performance of each predictor. In our results, we show that MPF outperforms the two standalone predictors on various metrics and delivers the most consistent performance.
</details>
<details>
<summary>摘要</summary>
几何预测模组是自动驾驶车 (AV) 规划中的关键启动器，特别是在高度互动的交通情况下。最近，学习型几何预测器在提供最佳性能方面有所成就，因为它们可以从数据中学习多种行为模式。在这篇文章中，我们提出了一个名为多predictor融合（MPF）的算法，它将学习型和规则型预测器融合在一起，以提高几何预测器的性能。MPF 使用一个信念分布来混合两个独立的预测器的轨迹，以实现学习型和规则型预测器的共同运行。在我们的结果中，我们发现MPF 在多个指标上表现更好，并提供了最稳定的性能。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Communicate-using-Contrastive-Learning"><a href="#Learning-to-Communicate-using-Contrastive-Learning" class="headerlink" title="Learning to Communicate using Contrastive Learning"></a>Learning to Communicate using Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01403">http://arxiv.org/abs/2307.01403</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/SonamSangpoLama/Music-Genre-Classification">https://github.com/SonamSangpoLama/Music-Genre-Classification</a></li>
<li>paper_authors: Yat Long Lo, Biswa Sengupta, Jakob Foerster, Michael Noukhovitch</li>
<li>for: 这个研究目的是为了提高多智能体RL中的协调，并且解决对环境的观察和沟通问题。</li>
<li>methods: 这个研究使用了对比学习来学习通信，将在不同时间和位置发送的消息视为不完整的环境状态观察。</li>
<li>results: 研究发现，这种方法可以在对话重要的环境中提高性能和学习速度，并且对环境状态观察有更好的对 symmetry 和全局状态资讯的捕捉。<details>
<summary>Abstract</summary>
Communication is a powerful tool for coordination in multi-agent RL. But inducing an effective, common language is a difficult challenge, particularly in the decentralized setting. In this work, we introduce an alternative perspective where communicative messages sent between agents are considered as different incomplete views of the environment state. By examining the relationship between messages sent and received, we propose to learn to communicate using contrastive learning to maximize the mutual information between messages of a given trajectory. In communication-essential environments, our method outperforms previous work in both performance and learning speed. Using qualitative metrics and representation probing, we show that our method induces more symmetric communication and captures global state information from the environment. Overall, we show the power of contrastive learning and the importance of leveraging messages as encodings for effective communication.
</details>
<details>
<summary>摘要</summary>
通信是多智能RL中协调工具的强大工具。但是引入有效、公共语言是一个困难的挑战，特别是在分布式设定下。在这项工作中，我们提出了一种不同的视角，即在代理者之间交换的通信信息被视为环境状态的不同不完整的视图。我们提出了通过对交换的消息进行对比学习，以最大化交换消息序列中的相互信息。在需要通信的环境下，我们的方法比前一项工作在性能和学习速度方面表现更好。使用质量指标和表示探测，我们显示了我们的方法在交换消息中引入更Symmetric的通信和捕捉环境中的全局状态信息。总之，我们展示了对冲学习的力量和通过消息编码实现有效的通信的重要性。
</details></li>
</ul>
<hr>
<h2 id="Spatio-Temporal-Surrogates-for-Interaction-of-a-Jet-with-High-Explosives-Part-II-–-Clustering-Extremely-High-Dimensional-Grid-Based-Data"><a href="#Spatio-Temporal-Surrogates-for-Interaction-of-a-Jet-with-High-Explosives-Part-II-–-Clustering-Extremely-High-Dimensional-Grid-Based-Data" class="headerlink" title="Spatio-Temporal Surrogates for Interaction of a Jet with High Explosives: Part II – Clustering Extremely High-Dimensional Grid-Based Data"></a>Spatio-Temporal Surrogates for Interaction of a Jet with High Explosives: Part II – Clustering Extremely High-Dimensional Grid-Based Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01400">http://arxiv.org/abs/2307.01400</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chandrika Kamath, Juliette S. Franzman</li>
<li>for: 这个论文的目的是建立一个准确的模拟模型，以便更好地预测计算机模拟中的空间时间输出。</li>
<li>methods: 作者使用了一种简单的方法，即将输出数据分为不同类别，并建立每个类别的单独的模拟模型。但是当输出数据中的空间域数量很大，分类变得更加困难。因此，作者首先将数据转换为一致的格式，然后使用随机投影法减少数据的维度，使用迭代k-means算法进行分类。</li>
<li>results: 作者的方法可以将极高维度的数据进行有意义的分类，即使有一定的近似性。他们通过控制随机投影的方式和k-means算法的初始中心点的选择，确定了数据集中的cluster数量。<details>
<summary>Abstract</summary>
Building an accurate surrogate model for the spatio-temporal outputs of a computer simulation is a challenging task. A simple approach to improve the accuracy of the surrogate is to cluster the outputs based on similarity and build a separate surrogate model for each cluster. This clustering is relatively straightforward when the output at each time step is of moderate size. However, when the spatial domain is represented by a large number of grid points, numbering in the millions, the clustering of the data becomes more challenging. In this report, we consider output data from simulations of a jet interacting with high explosives. These data are available on spatial domains of different sizes, at grid points that vary in their spatial coordinates, and in a format that distributes the output across multiple files at each time step of the simulation. We first describe how we bring these data into a consistent format prior to clustering. Borrowing the idea of random projections from data mining, we reduce the dimension of our data by a factor of thousand, making it possible to use the iterative k-means method for clustering. We show how we can use the randomness of both the random projections, and the choice of initial centroids in k-means clustering, to determine the number of clusters in our data set. Our approach makes clustering of extremely high dimensional data tractable, generating meaningful cluster assignments for our problem, despite the approximation introduced in the random projections.
</details>
<details>
<summary>摘要</summary>
在计算机模拟中的输出中，建立准确的代理模型是一项复杂的任务。一种简单的方法是根据输出的相似性进行归类，并为每个归类建立一个独立的代理模型。当输出的每个时间步骤的大小是 Moderate 时，这种归类是相对容易的。但是，当 spatial 领域被表示为数百万个网点时，归类数据变得更加困难。在这份报告中，我们考虑了计算机模拟中的液体喷气与高爆物相互作用的输出数据。这些数据在不同的空间尺度上可以获得，并且在每个时间步骤上分布在多个文件中。我们首先描述了如何将这些数据转换成一致的格式，以便归类。我们采用了数据挖掘中的Random Projections的想法，将数据维度减少到一千倍，使用迭代k-means算法进行归类。我们示出了如何使用Random Projections和k-means归类算法中的随机初始化中心的Randomness来确定数据集中的凝集数。我们的方法使得归类EXTREMELY HIGH 维度数据成为可能，生成了有意义的凝集分配，尽管在随机投影中引入了一定的简化。
</details></li>
</ul>
<hr>
<h2 id="In-depth-Analysis-On-Parallel-Processing-Patterns-for-High-Performance-Dataframes"><a href="#In-depth-Analysis-On-Parallel-Processing-Patterns-for-High-Performance-Dataframes" class="headerlink" title="In-depth Analysis On Parallel Processing Patterns for High-Performance Dataframes"></a>In-depth Analysis On Parallel Processing Patterns for High-Performance Dataframes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01394">http://arxiv.org/abs/2307.01394</a></li>
<li>repo_url: None</li>
<li>paper_authors: Niranda Perera, Arup Kumar Sarker, Mills Staylor, Gregor von Laszewski, Kaiying Shan, Supun Kamburugamuve, Chathura Widanage, Vibhatha Abeykoon, Thejaka Amila Kanewela, Geoffrey Fox</li>
<li>for: 本研究旨在提高数据工程应用程序的性能，特别是在处理大量数据时。</li>
<li>methods: 本文使用高性能计算的视角，提出了分布式数据框架操作的并行处理模式，并实现了参考runtime实现Cylon。</li>
<li>results: 本研究在ORNL Summit超级计算机上评估了Cylon的性能。<details>
<summary>Abstract</summary>
The Data Science domain has expanded monumentally in both research and industry communities during the past decade, predominantly owing to the Big Data revolution. Artificial Intelligence (AI) and Machine Learning (ML) are bringing more complexities to data engineering applications, which are now integrated into data processing pipelines to process terabytes of data. Typically, a significant amount of time is spent on data preprocessing in these pipelines, and hence improving its e fficiency directly impacts the overall pipeline performance. The community has recently embraced the concept of Dataframes as the de-facto data structure for data representation and manipulation. However, the most widely used serial Dataframes today (R, pandas) experience performance limitations while working on even moderately large data sets. We believe that there is plenty of room for improvement by taking a look at this problem from a high-performance computing point of view. In a prior publication, we presented a set of parallel processing patterns for distributed dataframe operators and the reference runtime implementation, Cylon [1]. In this paper, we are expanding on the initial concept by introducing a cost model for evaluating the said patterns. Furthermore, we evaluate the performance of Cylon on the ORNL Summit supercomputer.
</details>
<details>
<summary>摘要</summary>
“数据科学领域在过去的一个 décennial 内扩大了很大，主要归功于大数据革命。人工智能（AI）和机器学习（ML）对数据工程应用带来了更多复杂性，这些应用现在被 integrate 到数据处理管道中来处理 terrabytes 级数据。通常，处理数据预处理过程中会投入大量时间，因此提高其效率直接影响整个管道性能。社区最近普遍认可数据帧为数据表示和操作的启用词。但是，当前最广泛使用的序列数据帧（R、pandas）在处理 Moderately 大规模数据集时会表现出性能限制。我们认为，从高性能计算的角度来看这个问题，还有很多可以提高的空间。在先前的发表文章中，我们提出了分布式数据帧运算 Patterns 和 Referencel Runtime 实现 Cylon 等一系列并发处理模式[1]。在这篇论文中，我们将这个概念进一步发展，并提出一种成本模型来评估所提出的模式。此外，我们还在 ORNL Summit 超级计算机上评估了 Cylon 的性能。”
</details></li>
</ul>
<hr>
<h2 id="Spatio-Temporal-Surrogates-for-Interaction-of-a-Jet-with-High-Explosives-Part-I-–-Analysis-with-a-Small-Sample-Size"><a href="#Spatio-Temporal-Surrogates-for-Interaction-of-a-Jet-with-High-Explosives-Part-I-–-Analysis-with-a-Small-Sample-Size" class="headerlink" title="Spatio-Temporal Surrogates for Interaction of a Jet with High Explosives: Part I – Analysis with a Small Sample Size"></a>Spatio-Temporal Surrogates for Interaction of a Jet with High Explosives: Part I – Analysis with a Small Sample Size</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01393">http://arxiv.org/abs/2307.01393</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chandrika Kamath, Juliette S. Franzman, Brian H. Daub</li>
<li>for: 本研究旨在开发一种高质量的空间-时间抽象方法，以便更好地理解复杂现象的计算机模拟结果。</li>
<li>methods: 本研究使用了一种基于机器学习的抽象方法，并在使用了一些简单的方法来提高抽象精度。</li>
<li>results: 研究发现，使用这种抽象方法可以创建高质量的空间-时间抽象模型，并且不需要进行大量的计算机模拟。<details>
<summary>Abstract</summary>
Computer simulations, especially of complex phenomena, can be expensive, requiring high-performance computing resources. Often, to understand a phenomenon, multiple simulations are run, each with a different set of simulation input parameters. These data are then used to create an interpolant, or surrogate, relating the simulation outputs to the corresponding inputs. When the inputs and outputs are scalars, a simple machine learning model can suffice. However, when the simulation outputs are vector valued, available at locations in two or three spatial dimensions, often with a temporal component, creating a surrogate is more challenging. In this report, we use a two-dimensional problem of a jet interacting with high explosives to understand how we can build high-quality surrogates. The characteristics of our data set are unique - the vector-valued outputs from each simulation are available at over two million spatial locations; each simulation is run for a relatively small number of time steps; the size of the computational domain varies with each simulation; and resource constraints limit the number of simulations we can run. We show how we analyze these extremely large data-sets, set the parameters for the algorithms used in the analysis, and use simple ways to improve the accuracy of the spatio-temporal surrogates without substantially increasing the number of simulations required.
</details>
<details>
<summary>摘要</summary>
计算机模拟，尤其是复杂现象的模拟，可能具有高成本，需要高性能计算资源。经常情况下，以解释现象，需要运行多个模拟，每个模拟都有不同的模拟输入参数。这些数据后来用于创建一个 interpolant，或surrogate，将模拟输出与相应的输入关系。当输入和输出都是整数时，一个简单的机器学习模型即可。但当模拟输出是二维或三维的向量值，创建surrogate更加困难。在这份报告中，我们使用一个两维问题，即喷气与高爆物相互作用，来理解如何建立高质量surrogate。我们的数据集的特点是唯一的：每个模拟的向量值输出在超过两百万个空间位置上可用;每个模拟只需要很少的时间步骤;计算区域的大小随每个模拟而异；资源限制限制我们可以运行的模拟数量。我们如何分析这些非常大的数据集，设置分析中使用的参数，并使用简单的方法提高空间temporal surrogate的准确性，不需要substantially增加模拟数量。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Learning-in-Real-World-Fraud-Detection-Challenges-and-Perspectives"><a href="#Adversarial-Learning-in-Real-World-Fraud-Detection-Challenges-and-Perspectives" class="headerlink" title="Adversarial Learning in Real-World Fraud Detection: Challenges and Perspectives"></a>Adversarial Learning in Real-World Fraud Detection: Challenges and Perspectives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01390">http://arxiv.org/abs/2307.01390</a></li>
<li>repo_url: None</li>
<li>paper_authors: Danele Lunghi, Alkis Simitsis, Olivier Caelen, Gianluca Bontempi</li>
<li>for: 本研究旨在探讨针对诈骗检测系统的攻击方法，以及如何扩展对其他领域和应用的攻击技术。</li>
<li>methods: 本研究使用了对抗机器学习技术，以探讨诈骗检测系统中的攻击方法。</li>
<li>results: 本研究发现了一些针对诈骗检测系统的攻击方法，并提出了一些可能的解决方案。<details>
<summary>Abstract</summary>
Data economy relies on data-driven systems and complex machine learning applications are fueled by them. Unfortunately, however, machine learning models are exposed to fraudulent activities and adversarial attacks, which threaten their security and trustworthiness. In the last decade or so, the research interest on adversarial machine learning has grown significantly, revealing how learning applications could be severely impacted by effective attacks. Although early results of adversarial machine learning indicate the huge potential of the approach to specific domains such as image processing, still there is a gap in both the research literature and practice regarding how to generalize adversarial techniques in other domains and applications. Fraud detection is a critical defense mechanism for data economy, as it is for other applications as well, which poses several challenges for machine learning. In this work, we describe how attacks against fraud detection systems differ from other applications of adversarial machine learning, and propose a number of interesting directions to bridge this gap.
</details>
<details>
<summary>摘要</summary>
<SYS>  将文本翻译成简化中文。</SYS>数据经济依赖于数据驱动系统和复杂的机器学习应用程序，但是这些应用程序受到诈骗活动和敌意攻击的威胁。在过去的一个 décennial 以来，关于反对机器学习的研究兴趣增长了 significatively，揭示了机器学习应用程序可能受到严重的影响。虽然初期的反对机器学习结果表明了该方法在图像处理领域的巨大潜力，但是在其他领域和应用程序中，还存在一定的泛化问题。防止诈骗是数据经济中的关键防御机制，同时也是其他应用程序中的挑战。在这种情况下，我们描述了诈骗检测系统受到攻击的方式与其他应用程序不同，并提出了一些有趣的方向来bridging这个差距。
</details></li>
</ul>
<hr>
<h2 id="Identification-of-Causal-Relationship-between-Amyloid-beta-Accumulation-and-Alzheimer’s-Disease-Progression-via-Counterfactual-Inference"><a href="#Identification-of-Causal-Relationship-between-Amyloid-beta-Accumulation-and-Alzheimer’s-Disease-Progression-via-Counterfactual-Inference" class="headerlink" title="Identification of Causal Relationship between Amyloid-beta Accumulation and Alzheimer’s Disease Progression via Counterfactual Inference"></a>Identification of Causal Relationship between Amyloid-beta Accumulation and Alzheimer’s Disease Progression via Counterfactual Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01389">http://arxiv.org/abs/2307.01389</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haixing Dai, Mengxuan Hu, Qing Li, Lu Zhang, Lin Zhao, Dajiang Zhu, Ibai Diez, Jorge Sepulcre, Fan Zhang, Xingyu Gao, Manhua Liu, Quanzheng Li, Sheng Li, Tianming Liu, Xiang Li</li>
<li>for: 这篇论文旨在探讨阿尔茨海默症（AD）的预后诊断和个性化治疗方案。</li>
<li>methods: 论文提出了一种基于图 convolutional neural network（GVCNet）的方法来估计个体对药物剂量的影响，以探讨阿尔茨海默症发展的因果关系。</li>
<li>results: 论文显示了这种方法可以实现个体对阿尔茨海默症发展的测量，并且可以提供可靠的预后诊断和个性化治疗方案。<details>
<summary>Abstract</summary>
Alzheimer's disease (AD) is a neurodegenerative disorder that is beginning with amyloidosis, followed by neuronal loss and deterioration in structure, function, and cognition. The accumulation of amyloid-beta in the brain, measured through 18F-florbetapir (AV45) positron emission tomography (PET) imaging, has been widely used for early diagnosis of AD. However, the relationship between amyloid-beta accumulation and AD pathophysiology remains unclear, and causal inference approaches are needed to uncover how amyloid-beta levels can impact AD development. In this paper, we propose a graph varying coefficient neural network (GVCNet) for estimating the individual treatment effect with continuous treatment levels using a graph convolutional neural network. We highlight the potential of causal inference approaches, including GVCNet, for measuring the regional causal connections between amyloid-beta accumulation and AD pathophysiology, which may serve as a robust tool for early diagnosis and tailored care.
</details>
<details>
<summary>摘要</summary>
阿尔茨海默病（AD）是一种神经退化疾病，起始于蛋白质沉积，然后是神经元丢失和结构、功能和认知的衰退。脑内βamyloid沉积的寻测，通过18F-氟苯酚（AV45） пози特核燐发射 Tomography（PET）成像，在早期诊断AD中广泛使用。然而，蛋白质沉积和AD生物学过程之间的关系仍然不清楚，需要用 causal inference 方法来探索蛋白质沉积如何影响AD发展。在这篇论文中，我们提出了一种基于图变换系数神经网络（GVCNet）的个体处方效应估计方法，可以用于评估连续治疗水平下的个体处方效应。我们强调了可meter causal inference 方法，包括 GVCNet，在评估蛋白质沉积和AD生物学过程之间的区域 causal 连接方面的潜在价值，这可能成为早期诊断和个性化治疗的可靠工具。
</details></li>
</ul>
<hr>
<h2 id="Systematic-Bias-in-Sample-Inference-and-its-Effect-on-Machine-Learning"><a href="#Systematic-Bias-in-Sample-Inference-and-its-Effect-on-Machine-Learning" class="headerlink" title="Systematic Bias in Sample Inference and its Effect on Machine Learning"></a>Systematic Bias in Sample Inference and its Effect on Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01384">http://arxiv.org/abs/2307.01384</a></li>
<li>repo_url: None</li>
<li>paper_authors: Owen O’Neill, Fintan Costello</li>
<li>for: 这种机器学习模型下的目标特征下预测不准确，特别是对少数群体的预测。</li>
<li>methods: 使用小样本统计推断的方法，导致预测结果受到方向性的统计偏见。</li>
<li>results: 对多个子集的预测结果显示，这种偏见导致了少数群体的预测错误率较高。<details>
<summary>Abstract</summary>
A commonly observed pattern in machine learning models is an underprediction of the target feature, with the model's predicted target rate for members of a given category typically being lower than the actual target rate for members of that category in the training set. This underprediction is usually larger for members of minority groups; while income level is underpredicted for both men and women in the 'adult' dataset, for example, the degree of underprediction is significantly higher for women (a minority in that dataset). We propose that this pattern of underprediction for minorities arises as a predictable consequence of statistical inference on small samples. When presented with a new individual for classification, an ML model performs inference not on the entire training set, but on a subset that is in some way similar to the new individual, with sizes of these subsets typically following a power law distribution so that most are small (and with these subsets being necessarily smaller for the minority group). We show that such inference on small samples is subject to systematic and directional statistical bias, and that this bias produces the observed patterns of underprediction seen in ML models. Analysing a standard sklearn decision tree model's predictions on a set of over 70 subsets of the 'adult' and COMPAS datasets, we found that a bias prediction measure based on small-sample inference had a significant positive correlations (0.56 and 0.85) with the observed underprediction rate for these subsets.
</details>
<details>
<summary>摘要</summary>
通常观察到的机器学习模型 patrón es la underprediction del feature objetivo, con la tasa predicha del modelo para los miembros de una categoría específica generalmente siendo menor que la tasa real para los miembros de esa categoría en el conjunto de entrenamiento. Esta underprediction es usualmente más grande para los miembros de los grupos minoritarios; por ejemplo, en el conjunto de datos 'adult', la tasa de underprediction es significativamente más alta para las mujeres (un grupo minoritario en ese conjunto de datos). Proponemos que este patrón de underprediction para los minorías se debe a una inferencia estadística predictible en pequeños conjuntos de datos. Cuando se presenta a un nuevo individuo para clasificación, un modelo de aprendizaje automático realiza inferencia no en todo el conjunto de entrenamiento, sino en un subconjunto que es de alguna manera similar al nuevo individuo, con tamaños de estos subconjuntos que siguen una distribución de potencia, lo que significa que la mayoría son pequeños (y con estos subconjuntos necesariamente más pequeños para el grupo minoritario). Demostramos que esta inferencia en pequeños conjuntos de datos está sujeta a una bias estadística sistemática y direccional, y que esta bias produce los patrones de underprediction observados en los modelos de aprendizaje automático. Analizando las predicciones de un modelo de árbol de decisión de sklearn en más de 70 subconjuntos del conjunto de datos 'adult' y COMPAS, encontramos que una medida de predicción de bias basada en la inferencia en pequeños conjuntos de datos tuvo una correlación positiva significativa (0,56 y 0,85) con la tasa de underprediction observada para estos subconjuntos.
</details></li>
</ul>
<hr>
<h2 id="Implicit-Memory-Transformer-for-Computationally-Efficient-Simultaneous-Speech-Translation"><a href="#Implicit-Memory-Transformer-for-Computationally-Efficient-Simultaneous-Speech-Translation" class="headerlink" title="Implicit Memory Transformer for Computationally Efficient Simultaneous Speech Translation"></a>Implicit Memory Transformer for Computationally Efficient Simultaneous Speech Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01381">http://arxiv.org/abs/2307.01381</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/osu-starlab/implicitmemory">https://github.com/osu-starlab/implicitmemory</a></li>
<li>paper_authors: Matthew Raffel, Lizhong Chen</li>
<li>for: 这个论文目的是提出一种新的听话器，以便在同时进行口头翻译。</li>
<li>methods: 该方法使用块处理来分割输入序列，并使用新的左上下文方法来隐式地保留记忆。</li>
<li>results: 实验结果表明，使用该方法可以在Encoder前进行快速加速，并且与使用左上下文和记忆银行的方法相比，翻译质量几乎相同。<details>
<summary>Abstract</summary>
Simultaneous speech translation is an essential communication task difficult for humans whereby a translation is generated concurrently with oncoming speech inputs. For such a streaming task, transformers using block processing to break an input sequence into segments have achieved state-of-the-art performance at a reduced cost. Current methods to allow information to propagate across segments, including left context and memory banks, have faltered as they are both insufficient representations and unnecessarily expensive to compute. In this paper, we propose an Implicit Memory Transformer that implicitly retains memory through a new left context method, removing the need to explicitly represent memory with memory banks. We generate the left context from the attention output of the previous segment and include it in the keys and values of the current segment's attention calculation. Experiments on the MuST-C dataset show that the Implicit Memory Transformer provides a substantial speedup on the encoder forward pass with nearly identical translation quality when compared with the state-of-the-art approach that employs both left context and memory banks.
</details>
<details>
<summary>摘要</summary>
同时对话翻译是人类communication task中的一项重要任务，即在流动输入语音时实时生成翻译。为此流动任务，使用块处理的 transformers 已经达到了状态机器的性能标准，而且可以降低计算成本。现有的方法，包括左上下文和内存银行，尝试让信息在段之间传递，但是这些方法都是不够的表示和过分昂贵的计算。在这篇论文中，我们提出了隐式记忆 transformer，通过一种新的左上下文方法，使得不需要显式表示内存。我们从前一段的注意输出中生成左上下文，并将其包含在当前段的注意计算中的键和值中。实验结果表明，隐式记忆 transformer 在 Must-C 数据集上提供了大幅降低encoder前进计算时间，并且与使用左上下文和内存银行的状态之前的翻译质量相似。
</details></li>
</ul>
<hr>
<h2 id="Shifting-Attention-to-Relevance-Towards-the-Uncertainty-Estimation-of-Large-Language-Models"><a href="#Shifting-Attention-to-Relevance-Towards-the-Uncertainty-Estimation-of-Large-Language-Models" class="headerlink" title="Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models"></a>Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01379">http://arxiv.org/abs/2307.01379</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jinhaoduan/shifting-attention-to-relevance">https://github.com/jinhaoduan/shifting-attention-to-relevance</a></li>
<li>paper_authors: Jinhao Duan, Hao Cheng, Shiqi Wang, Chenan Wang, Alex Zavalny, Renjing Xu, Bhavya Kailkhura, Kaidi Xu</li>
<li>for: 这项研究的目的是解决自动逆进语言模型（LLMs）生成输出的不确定性问题，即用户可以信任模型输出的问题。</li>
<li>methods: 这项研究使用了自动逆进语言模型（LLMs）生成输出的token不均等，即一些token更加重要（或代表）于另外的token，并且对于估计不确定性，所有token被视为平等的现象，来 investigate 如何解决这些不平等。</li>
<li>results: 研究结果显示，在估计不确定性时，许多重要的token和含有有限 semantics的句子被平均地或者甚至很重视，以至于存在biases。为了解决这些biases，提议使用 JOINT SHIFTING ATTENTION TO RELEVANT（SAR）组件，并在实验中达到了superior表现。<details>
<summary>Abstract</summary>
Although Large Language Models (LLMs) have shown great potential in Natural Language Generation, it is still challenging to characterize the uncertainty of model generations, i.e., when users could trust model outputs. Our research is derived from the heuristic facts that tokens are created unequally in reflecting the meaning of generations by auto-regressive LLMs, i.e., some tokens are more relevant (or representative) than others, yet all the tokens are equally valued when estimating uncertainty. It is because of the linguistic redundancy where mostly a few keywords are sufficient to convey the meaning of a long sentence. We name these inequalities as generative inequalities and investigate how they affect uncertainty estimation. Our results reveal that considerable tokens and sentences containing limited semantics are weighted equally or even heavily when estimating uncertainty. To tackle these biases posed by generative inequalities, we propose to jointly Shifting Attention to more Relevant (SAR) components from both the token level and the sentence level while estimating uncertainty. We conduct experiments over popular "off-the-shelf" LLMs (e.g., OPT, LLaMA) with model sizes up to 30B and powerful commercial LLMs (e.g., Davinci from OpenAI), across various free-form question-answering tasks. Experimental results and detailed demographic analysis indicate the superior performance of SAR. Code is available at https://github.com/jinhaoduan/shifting-attention-to-relevance.
</details>
<details>
<summary>摘要</summary>
尽管大型自然语言模型（LLM）已经表现出了很大的潜力，但是 Still characterizing the uncertainty of model generations, i.e., when users can trust model outputs, is still a challenge. Our research is based on the heuristic fact that tokens are created unequally in reflecting the meaning of generations by auto-regressive LLMs, i.e., some tokens are more relevant (or representative) than others, yet all the tokens are equally valued when estimating uncertainty. It is because of the linguistic redundancy where mostly a few keywords are sufficient to convey the meaning of a long sentence. We name these inequalities as generative inequalities and investigate how they affect uncertainty estimation. Our results reveal that considerable tokens and sentences containing limited semantics are weighted equally or even heavily when estimating uncertainty. To tackle these biases posed by generative inequalities, we propose to jointly Shifting Attention to more Relevant (SAR) components from both the token level and the sentence level while estimating uncertainty. We conduct experiments over popular "off-the-shelf" LLMs (e.g., OPT, LLaMA) with model sizes up to 30B and powerful commercial LLMs (e.g., Davinci from OpenAI), across various free-form question-answering tasks. Experimental results and detailed demographic analysis indicate the superior performance of SAR. Code is available at https://github.com/jinhaoduan/shifting-attention-to-relevance.Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Shiftable-Context-Addressing-Training-Inference-Context-Mismatch-in-Simultaneous-Speech-Translation"><a href="#Shiftable-Context-Addressing-Training-Inference-Context-Mismatch-in-Simultaneous-Speech-Translation" class="headerlink" title="Shiftable Context: Addressing Training-Inference Context Mismatch in Simultaneous Speech Translation"></a>Shiftable Context: Addressing Training-Inference Context Mismatch in Simultaneous Speech Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01377">http://arxiv.org/abs/2307.01377</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/osu-starlab/shiftablecontext">https://github.com/osu-starlab/shiftablecontext</a></li>
<li>paper_authors: Matthew Raffel, Drew Penney, Lizhong Chen</li>
<li>for:  simultaneous speech translation</li>
<li>methods: 使用 segment-based processing 和 Shiftable Context  scheme</li>
<li>results:  average increase of 2.09, 1.83, and 1.95 BLEU scores across each wait-k value for the three language pairs, with minimal impact on computation-aware Average Lagging.<details>
<summary>Abstract</summary>
Transformer models using segment-based processing have been an effective architecture for simultaneous speech translation. However, such models create a context mismatch between training and inference environments, hindering potential translation accuracy. We solve this issue by proposing Shiftable Context, a simple yet effective scheme to ensure that consistent segment and context sizes are maintained throughout training and inference, even with the presence of partially filled segments due to the streaming nature of simultaneous translation. Shiftable Context is also broadly applicable to segment-based transformers for streaming tasks. Our experiments on the English-German, English-French, and English-Spanish language pairs from the MUST-C dataset demonstrate that when applied to the Augmented Memory Transformer, a state-of-the-art model for simultaneous speech translation, the proposed scheme achieves an average increase of 2.09, 1.83, and 1.95 BLEU scores across each wait-k value for the three language pairs, respectively, with a minimal impact on computation-aware Average Lagging.
</details>
<details>
<summary>摘要</summary>
transformer模型使用分段处理有效地实现同时语音翻译。然而，这些模型在训练和推理环境中存在上下文匹配问题，从而限制了翻译准确性。我们解决这个问题，提出了Shiftable Context，一种简单 yet effective的方案，确保在训练和推理过程中保持一致的分段和上下文大小。Shiftable Context还可以广泛应用于流处理任务中的 segment-based transformer。我们在MUST-C数据集上进行英语-德语、英语-法语和英语-西班牙语三对语言对的实验，结果显示，当应用到Augmented Memory Transformer模型时，提出的方案平均提高了2.09、1.83和1.95的BLEU分数 across each wait-k值，并且对计算意识的均衡延迟产生了最小的影响。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Principal-Component-Regression-with-Applications-to-Panel-Data"><a href="#Adaptive-Principal-Component-Regression-with-Applications-to-Panel-Data" class="headerlink" title="Adaptive Principal Component Regression with Applications to Panel Data"></a>Adaptive Principal Component Regression with Applications to Panel Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01357">http://arxiv.org/abs/2307.01357</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anish Agarwal, Keegan Harris, Justin Whitehouse, Zhiwei Steven Wu</li>
<li>for: This paper provides time-uniform finite sample guarantees for online principal component regression (PCR) in the presence of adaptive data collection.</li>
<li>methods: The paper uses tools from modern martingale concentration to analyze PCR in the online setting, which is a generalization of the fixed-design error-in-variables regression.</li>
<li>results: The paper provides a framework for experiment design in panel data settings when interventions are assigned adaptively, which can be seen as a generalization of synthetic control and synthetic interventions frameworks.Here’s the Chinese version:</li>
<li>for: 这篇论文提供了在在线主成分回归（PCR）中的时间固定样本保证。</li>
<li>methods: 这篇论文使用现代随机 martingale 集中来分析 PCR 在在线设置下的分析。</li>
<li>results: 这篇论文提供了针对板块数据设置中的实验设计框架，当实验是通过适应性的干预分配策略进行分配。<details>
<summary>Abstract</summary>
Principal component regression (PCR) is a popular technique for fixed-design error-in-variables regression, a generalization of the linear regression setting in which the observed covariates are corrupted with random noise. We provide the first time-uniform finite sample guarantees for online (regularized) PCR whenever data is collected adaptively. Since the proof techniques for analyzing PCR in the fixed design setting do not readily extend to the online setting, our results rely on adapting tools from modern martingale concentration to the error-in-variables setting. As an application of our bounds, we provide a framework for experiment design in panel data settings when interventions are assigned adaptively. Our framework may be thought of as a generalization of the synthetic control and synthetic interventions frameworks, where data is collected via an adaptive intervention assignment policy.
</details>
<details>
<summary>摘要</summary>
主成分回归（PCR）是一种流行的固定设计错误变量回归技术， linear regression 设定中的一种扩展，在观测 covariates 上存在随机噪声。我们提供了在线（规化）PCR 的首次时间均衡finite sample guarantees，当数据采集是动态的。由于fixed design 设定中PCR 的证明技巧不直接适用于在线设定，我们的结果基于采用现代martingale concentration 工具来error-in-variables设定。我们的极限 bounds 可以应用于面板数据设置中的实验设计，当实验是通过适应性干预分配策略采集数据。我们的框架可以看作是错误变量和synthetic control 框架的扩展，在适应性干预分配策略下采集数据。
</details></li>
</ul>
<hr>
<h2 id="Learning-Generic-Solutions-for-Multiphase-Transport-in-Porous-Media-via-the-Flux-Functions-Operator"><a href="#Learning-Generic-Solutions-for-Multiphase-Transport-in-Porous-Media-via-the-Flux-Functions-Operator" class="headerlink" title="Learning Generic Solutions for Multiphase Transport in Porous Media via the Flux Functions Operator"></a>Learning Generic Solutions for Multiphase Transport in Porous Media via the Flux Functions Operator</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01354">http://arxiv.org/abs/2307.01354</a></li>
<li>repo_url: None</li>
<li>paper_authors: Waleed Diab, Omar Chaabi, Shayma Alkobaisi, Abeeb Awotunde, Mohammed Al Kobaisi</li>
<li>for: 加速 fluid 流动和运输在 porous media 中的 simulate 算法，使得在科学和工程领域中可以更快速地解决问题。</li>
<li>methods: 使用 deep learning 技术，具体来说是 Physics-Informed DeepONets (PI-DeepONets)，通过学习 partial differential equations (PDEs) 中的运算函数，从而实现快速的解决。</li>
<li>results: 比 traditional numerical solvers 快速多达四个数量级，并且可以捕捉到 any type of flux function (concave, convex, or non-convex) 的解决。同时，trained PI-DeepONet model 表现出了优秀的泛化能力，这使得它成为了解决 transport problems in porous media 中的一个有力的工具。<details>
<summary>Abstract</summary>
Traditional numerical schemes for simulating fluid flow and transport in porous media can be computationally expensive. Advances in machine learning for scientific computing have the potential to help speed up the simulation time in many scientific and engineering fields. DeepONet has recently emerged as a powerful tool for accelerating the solution of partial differential equations (PDEs) by learning operators (mapping between function spaces) of PDEs. In this work, we learn the mapping between the space of flux functions of the Buckley-Leverett PDE and the space of solutions (saturations). We use Physics-Informed DeepONets (PI-DeepONets) to achieve this mapping without any paired input-output observations, except for a set of given initial or boundary conditions; ergo, eliminating the expensive data generation process. By leveraging the underlying physical laws via soft penalty constraints during model training, in a manner similar to Physics-Informed Neural Networks (PINNs), and a unique deep neural network architecture, the proposed PI-DeepONet model can predict the solution accurately given any type of flux function (concave, convex, or non-convex) while achieving up to four orders of magnitude improvements in speed over traditional numerical solvers. Moreover, the trained PI-DeepONet model demonstrates excellent generalization qualities, rendering it a promising tool for accelerating the solution of transport problems in porous media.
</details>
<details>
<summary>摘要</summary>
传统的数学方法 для模拟 fluid 流和物质传输在porous media中可能是计算昂贵的。机器学习的应用在科学计算中有助于减少模拟时间在多科学和工程领域。DeepONet 是一种可以加速解决部分偏微分方程（PDEs）的有力工具，它可以学习 PDEs 中操作（函数空间之间的映射）的映射。在这个工作中，我们学习了 Buckley-Leverett PDE 中的流函数空间和解空间之间的映射，使用 Physics-Informed DeepONets（PI-DeepONets）来实现这种映射，不需要任何对应的输入输出观察数据，只需要给定一些初始或边界条件即可。通过在模型训练中采用物理法律的软约束，类似于 Physics-Informed Neural Networks（PINNs），以及特有的深度神经网络架构，我们的提议的 PI-DeepONet 模型可以准确地预测解，并且可以在不同类型的流函数（凹、 convex、非几何）下实现四个数量级的速度提高。此外，我们训练的 PI-DeepONet 模型还表现出了优秀的泛化质量，使其成为加速porous media中物质传输问题的解决工具。
</details></li>
</ul>
<hr>
<h2 id="Patch-CNN-Training-data-efficient-deep-learning-for-high-fidelity-diffusion-tensor-estimation-from-minimal-diffusion-protocols"><a href="#Patch-CNN-Training-data-efficient-deep-learning-for-high-fidelity-diffusion-tensor-estimation-from-minimal-diffusion-protocols" class="headerlink" title="Patch-CNN: Training data-efficient deep learning for high-fidelity diffusion tensor estimation from minimal diffusion protocols"></a>Patch-CNN: Training data-efficient deep learning for high-fidelity diffusion tensor estimation from minimal diffusion protocols</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01346">http://arxiv.org/abs/2307.01346</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tobias Goodwin-Allcock, Ting Gong, Robert Gray, Parashkev Nachev, Hui Zhang</li>
<li>for: 这种论文是为了提出一种新的方法，即 Patch-CNN，用于从六个方向的扩散图像（DWI）中提取扩散矩阵（DT）的估计。</li>
<li>methods: 该方法使用了深度学习方法，使用了缓冲层（Convolutional Neural Network，CNN）来学习扩散矩阵的估计。</li>
<li>results: 对比传统模型适应和维度全连接神经网络（voxel-wise Fully-Connected Neural Network，FCN），Patch-CNN 可以更好地估计扩散矩阵和纤维方向，并且只需要使用单个试验者的数据进行训练。<details>
<summary>Abstract</summary>
We propose a new method, Patch-CNN, for diffusion tensor (DT) estimation from only six-direction diffusion weighted images (DWI). Deep learning-based methods have been recently proposed for dMRI parameter estimation, using either voxel-wise fully-connected neural networks (FCN) or image-wise convolutional neural networks (CNN). In the acute clinical context -- where pressure of time limits the number of imaged directions to a minimum -- existing approaches either require an infeasible number of training images volumes (image-wise CNNs), or do not estimate the fibre orientations (voxel-wise FCNs) required for tractogram estimation. To overcome these limitations, we propose Patch-CNN, a neural network with a minimal (non-voxel-wise) convolutional kernel (3$\times$3$\times$3). Compared with voxel-wise FCNs, this has the advantage of allowing the network to leverage local anatomical information. Compared with image-wise CNNs, the minimal kernel vastly reduces training data demand. Evaluated against both conventional model fitting and a voxel-wise FCN, Patch-CNN, trained with a single subject is shown to improve the estimation of both scalar dMRI parameters and fibre orientation from six-direction DWIs. The improved fibre orientation estimation is shown to produce improved tractogram.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新方法，patch-CNN，用于从六个方向的扩散tensor（DT）估计。在临床情况下，使用全量的学习方法来估计DMRI参数，可以使用 Either voxel-wise fully-connected neural networks（FCN）或图像-wise convolutional neural networks（CNN）。现有的方法 either require an infeasible number of training images volumes（image-wise CNNs），或者不能估计纤维方向（voxel-wise FCNs），从而限制了轨迹估计。为了超越这些限制，我们提出了patch-CNN，一个具有最小（非voxel-wise） convolutional kernel（3×3×3）的神经网络。与voxel-wise FCNs比较，这有利于神经网络利用地方 анатомиче信息。与image-wise CNNs比较，最小kernel减少了训练数据的需求。我们通过对 conventiomal model fitting和voxel-wise FCN进行比较，发现patch-CNN，通过一个个体训练，可以提高六个方向DWI中的scalar DMRI参数和纤维方向的估计。此外，改进的纤维方向估计也可以提高轨迹的估计。
</details></li>
</ul>
<hr>
<h2 id="Robust-Uncertainty-Estimation-for-Classification-of-Maritime-Objects"><a href="#Robust-Uncertainty-Estimation-for-Classification-of-Maritime-Objects" class="headerlink" title="Robust Uncertainty Estimation for Classification of Maritime Objects"></a>Robust Uncertainty Estimation for Classification of Maritime Objects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01325">http://arxiv.org/abs/2307.01325</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonathan Becktor, Frederik Scholler, Evangelos Boukas, Lazaros Nalpantidis</li>
<li>for: 这篇论文的目的是探讨在海上领域中使用不确定性估计的可能性，并在具有各种硬件和软件限制的实际场景中进行评估。</li>
<li>methods: 这篇论文使用了蒙特卡洛批处理来实现内部类uncertainty，并结合了最新的异常检测发现的技术来获得更全面的不确定性测量。</li>
<li>results: 该论文的实验结果显示，通过将Monte Carlo Dropout与异常检测技术结合使用，可以提高FPR95的性能，相比之下当模型没有异常数据训练时，该方法的性能提高了8%。此外，相比于基本实现的宽度网络，该方法可以提高性能 by 77%。此外， authors还释放了SHIPS数据集，并证明了该方法的有效性，将FPR95提高了44.2%。<details>
<summary>Abstract</summary>
We explore the use of uncertainty estimation in the maritime domain, showing the efficacy on toy datasets (CIFAR10) and proving it on an in-house dataset, SHIPS. We present a method joining the intra-class uncertainty achieved using Monte Carlo Dropout, with recent discoveries in the field of outlier detection, to gain more holistic uncertainty measures. We explore the relationship between the introduced uncertainty measures and examine how well they work on CIFAR10 and in a real-life setting. Our work improves the FPR95 by 8% compared to the current highest-performing work when the models are trained without out-of-distribution data. We increase the performance by 77% compared to a vanilla implementation of the Wide ResNet. We release the SHIPS dataset and show the effectiveness of our method by improving the FPR95 by 44.2% with respect to the baseline. Our approach is model agnostic, easy to implement, and often does not require model retraining.
</details>
<details>
<summary>摘要</summary>
我们探索了海上领域中uncertainty估计的使用，通过使用CIFAR10杂交数据集和自有数据集SHIPS进行证明，并提出了将Monte Carlo Dropout中的内类uncertainty与现代异常检测发现相结合以获得更全面的uncertainty测度的方法。我们研究了引入的uncertainty测度与之间的关系，并在CIFAR10和实际场景中评估其效果。我们的工作提高了FPR95的性能，相比最高性能工作不包含外围数据集时，提高了8%。相比于普通实现的宽度网络，我们的方法提高了77%的性能。我们发布了SHIPS数据集，并通过提高FPR95的性能44.2%来证明我们的方法的效果。我们的方法是模型无关的，易于实现，通常不需要模型重新训练。
</details></li>
</ul>
<hr>
<h2 id="Density-based-Feasibility-Learning-with-Normalizing-Flows-for-Introspective-Robotic-Assembly"><a href="#Density-based-Feasibility-Learning-with-Normalizing-Flows-for-Introspective-Robotic-Assembly" class="headerlink" title="Density-based Feasibility Learning with Normalizing Flows for Introspective Robotic Assembly"></a>Density-based Feasibility Learning with Normalizing Flows for Introspective Robotic Assembly</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01317">http://arxiv.org/abs/2307.01317</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/DLR-RM/GRACE">https://github.com/DLR-RM/GRACE</a></li>
<li>paper_authors: Jianxiang Feng, Matan Atad, Ismael Rodríguez, Maximilian Durner, Stephan Günnemann, Rudolph Triebel</li>
<li>for: 本研究旨在提高机器学习（ML）模型在机器人组装序列规划（RASP）中的 introspection 能力，以避免效率下降。</li>
<li>methods: 本研究提出了一种基于密度的可行性学习方法，不需要非可行示例。具体来说，我们将可行性学习问题转化为Out-of-Distribution（OOD）探测问题，使用Normalizing Flows（NF）来估计复杂的概率分布。</li>
<li>results: 在机器人组装用例中，提出的方法比单类基elines表现出色地探测不可行的组装。我们还进一步调查了我们方法的内部工作机制，发现可以通过高级变体NF实现很大的内存节省。<details>
<summary>Abstract</summary>
Machine Learning (ML) models in Robotic Assembly Sequence Planning (RASP) need to be introspective on the predicted solutions, i.e. whether they are feasible or not, to circumvent potential efficiency degradation. Previous works need both feasible and infeasible examples during training. However, the infeasible ones are hard to collect sufficiently when re-training is required for swift adaptation to new product variants. In this work, we propose a density-based feasibility learning method that requires only feasible examples. Concretely, we formulate the feasibility learning problem as Out-of-Distribution (OOD) detection with Normalizing Flows (NF), which are powerful generative models for estimating complex probability distributions. Empirically, the proposed method is demonstrated on robotic assembly use cases and outperforms other single-class baselines in detecting infeasible assemblies. We further investigate the internal working mechanism of our method and show that a large memory saving can be obtained based on an advanced variant of NF.
</details>
<details>
<summary>摘要</summary>
machine learning (ml) 模型在机器人组装序列规划 (rasp) 中需要 introspective 对预测的解决方案，以避免效率降低。先前的工作需要两类样本：可行和不可行的示例。然而，不可行的示例具有充足的收集困难，导致在重新训练时需要充足的时间。在这种情况下，我们提议一种基于浓度学习的可行学习方法，只需要可行的示例。具体来说，我们将可行学习问题定义为 OUT-OF-DISTRIBUTION (OOD) 检测，使用 Normalizing Flows (NF) 来Estimate 复杂的概率分布。实验表明，我们提议的方法在机器人组装use case中表现出色，可以快速检测不可行的组装。我们进一步调查我们的方法的内部工作机制，发现可以基于高级变体的 NF 实现大量内存保存。
</details></li>
</ul>
<hr>
<h2 id="Towards-Safe-Autonomous-Driving-Policies-using-a-Neuro-Symbolic-Deep-Reinforcement-Learning-Approach"><a href="#Towards-Safe-Autonomous-Driving-Policies-using-a-Neuro-Symbolic-Deep-Reinforcement-Learning-Approach" class="headerlink" title="Towards Safe Autonomous Driving Policies using a Neuro-Symbolic Deep Reinforcement Learning Approach"></a>Towards Safe Autonomous Driving Policies using a Neuro-Symbolic Deep Reinforcement Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01316">http://arxiv.org/abs/2307.01316</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cav-research-lab/safe-reinforcement-learning-using-symbolic-logical-programming-for-autonomous-highway-driving">https://github.com/cav-research-lab/safe-reinforcement-learning-using-symbolic-logical-programming-for-autonomous-highway-driving</a></li>
<li>paper_authors: Iman Sharifi, Mustafa Yildirim, Saber Fallah</li>
<li>for: 本研究旨在开发一种能够在真实环境中学习自动驾驶策略，并确保安全性的神经符号逻辑深度学习方法（DRLSL）。</li>
<li>methods: 本方法结合神经网络学习和符号逻辑推理，以便在真实环境中学习自动驾驶策略，并且能够保证安全性。</li>
<li>results: 我们在使用高D数据集进行实践中，发现DRLSL方法可以避免不安全行为，并且在训练和测试阶段都能够快速 converges。此外，我们的结果还表明，DRLSL方法在面对新的驾驶场景时能够更好地泛化。<details>
<summary>Abstract</summary>
The dynamic nature of driving environments and the presence of diverse road users pose significant challenges for decision-making in autonomous driving. Deep reinforcement learning (DRL) has emerged as a popular approach to tackle this problem. However, the application of existing DRL solutions is mainly confined to simulated environments due to safety concerns, impeding their deployment in real-world. To overcome this limitation, this paper introduces a novel neuro-symbolic model-free DRL approach, called DRL with Symbolic Logics (DRLSL) that combines the strengths of DRL (learning from experience) and symbolic first-order logics (knowledge-driven reasoning) to enable safe learning in real-time interactions of autonomous driving within real environments. This innovative approach provides a means to learn autonomous driving policies by actively engaging with the physical environment while ensuring safety. We have implemented the DRLSL framework in autonomous driving using the highD dataset and demonstrated that our method successfully avoids unsafe actions during both the training and testing phases. Furthermore, our results indicate that DRLSL achieves faster convergence during training and exhibits better generalizability to new driving scenarios compared to traditional DRL methods.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "The dynamic nature of driving environments and the presence of diverse road users pose significant challenges for decision-making in autonomous driving. Deep reinforcement learning (DRL) has emerged as a popular approach to tackle this problem. However, the application of existing DRL solutions is mainly confined to simulated environments due to safety concerns, impeding their deployment in real-world. To overcome this limitation, this paper introduces a novel neuro-symbolic model-free DRL approach, called DRL with Symbolic Logics (DRLSL) that combines the strengths of DRL (learning from experience) and symbolic first-order logics (knowledge-driven reasoning) to enable safe learning in real-time interactions of autonomous driving within real environments. This innovative approach provides a means to learn autonomous driving policies by actively engaging with the physical environment while ensuring safety. We have implemented the DRLSL framework in autonomous driving using the highD dataset and demonstrated that our method successfully avoids unsafe actions during both the training and testing phases. Furthermore, our results indicate that DRLSL achieves faster convergence during training and exhibits better generalizability to new driving scenarios compared to traditional DRL methods."into Simplified Chinese.驾驶环境的动态性和路用者的多样性 pose significant challenges for autonomous driving decision-making.深度强化学习（DRL）已经成为解决这个问题的popular方法。然而，现有的DRL解决方案主要在模拟环境中应用，由于安全问题，导致它们在实际环境中没有得到广泛应用。为了突破这个限制，这篇论文提出了一种新的 neuralsymbolic model-free DRL方法，叫做DRL with Symbolic Logics（DRLSL）。这种方法结合了DRL（学习经验）和符号逻辑（知识驱动的理解），以便在真实环境中学习自动驾驶策略，并确保安全。我们在自动驾驶中实现了DRLSL框架，使用highD dataset，并证明了我们的方法在训练和测试阶段都可以避免不安全的行为。此外，我们的结果还表明，DRLSL在训练阶段更快 converges和在新的驾驶enario中表现更好的普适性。
</details></li>
</ul>
<hr>
<h2 id="A-numerical-algorithm-for-attaining-the-Chebyshev-bound-in-optimal-learning"><a href="#A-numerical-algorithm-for-attaining-the-Chebyshev-bound-in-optimal-learning" class="headerlink" title="A numerical algorithm for attaining the Chebyshev bound in optimal learning"></a>A numerical algorithm for attaining the Chebyshev bound in optimal learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01304">http://arxiv.org/abs/2307.01304</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pradyumna Paruchuri, Debasish Chatterjee</li>
<li>for: 解决 оптимального学习从数据点集中回归函数的问题</li>
<li>methods: 基于近似解决半无穷问题的目标采样技术</li>
<li>results: 计算废弃半径和废弃中心，解决函数回归问题<details>
<summary>Abstract</summary>
Given a compact subset of a Banach space, the Chebyshev center problem consists of finding a minimal circumscribing ball containing the set. In this article we establish a numerically tractable algorithm for solving the Chebyshev center problem in the context of optimal learning from a finite set of data points. For a hypothesis space realized as a compact but not necessarily convex subset of a finite-dimensional subspace of some underlying Banach space, this algorithm computes the Chebyshev radius and the Chebyshev center of the hypothesis space, thereby solving the problem of optimal recovery of functions from data. The algorithm itself is based on, and significantly extends, recent results for near-optimal solutions of convex semi-infinite problems by means of targeted sampling, and it is of independent interest. Several examples of numerical computations of Chebyshev centers are included in order to illustrate the effectiveness of the algorithm.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "compact subset" becomes "compact subset" (同义译)* "Chebyshev center problem" becomes "Chebychev中心问题" (direct translation)* "hypothesis space" becomes "假设空间" (direct translation)* "compact but not necessarily convex subset" becomes "不必然凸的子集" (direct translation)* "minimal circumscribing ball" becomes "最小圆包" (direct translation)* "optimal learning from a finite set of data points" becomes "从finite个数据点中优化学习" (direct translation)* "convex semi-infinite problems" becomes "凸半无穷问题" (direct translation)* "targeted sampling" becomes "targeted采样" (direct translation)* "of independent interest" becomes "独立有利" (direct translation)Note that in Simplified Chinese, the word "space" is often omitted in translations, so "underlying Banach space" becomes just "underlying Banach 空间" in the translation.
</details></li>
</ul>
<hr>
<h2 id="Pareto-Secure-Machine-Learning-PSML-Fingerprinting-and-Securing-Inference-Serving-Systems"><a href="#Pareto-Secure-Machine-Learning-PSML-Fingerprinting-and-Securing-Inference-Serving-Systems" class="headerlink" title="Pareto-Secure Machine Learning (PSML): Fingerprinting and Securing Inference Serving Systems"></a>Pareto-Secure Machine Learning (PSML): Fingerprinting and Securing Inference Serving Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01292">http://arxiv.org/abs/2307.01292</a></li>
<li>repo_url: None</li>
<li>paper_authors: Debopam Sanyal, Jui-Tse Hung, Manav Agrawal, Prahlad Jasti, Shahab Nikkhoo, Somesh Jha, Tianhao Wang, Sibin Mohan, Alexey Tumanov</li>
<li>for: 本研究探讨了基于服务器端模型 zoo 的实时网络应用中的安全性，具体来说是对模型EXTRACTION攻击的Robustness。</li>
<li>methods: 本研究提出了一种高效的查询 fingerprinting 算法，使攻击者可以让服务器端模型 consistently 执行恶意操作。此外，本研究还提出了一种基于噪音的防御机制，通过添加噪音到指定性能指标来防止 fingerprinting。</li>
<li>results: 本研究表明，使用高效查询 fingerprinting 算法可以在模型EXTRACTION攻击中实现高精度和高准确率（在 $1%$ 以内），同时可以提高模型抽象层的安全性。此外，本研究还发现了一种基于噪音的防御机制可以减少攻击精度和准确率（在 $9.8%$ 和 $4.8%$ 以内）。<details>
<summary>Abstract</summary>
Model-serving systems have become increasingly popular, especially in real-time web applications. In such systems, users send queries to the server and specify the desired performance metrics (e.g., desired accuracy, latency). The server maintains a set of models (model zoo) in the back-end and serves the queries based on the specified metrics. This paper examines the security, specifically robustness against model extraction attacks, of such systems. Existing black-box attacks assume a single model can be repeatedly selected for serving inference requests. Modern inference serving systems break this assumption. Thus, they cannot be directly applied to extract a victim model, as models are hidden behind a layer of abstraction exposed by the serving system. An attacker can no longer identify which model she is interacting with. To this end, we first propose a query-efficient fingerprinting algorithm to enable the attacker to trigger any desired model consistently. We show that by using our fingerprinting algorithm, model extraction can have fidelity and accuracy scores within $1\%$ of the scores obtained when attacking a single, explicitly specified model, as well as up to $14.6\%$ gain in accuracy and up to $7.7\%$ gain in fidelity compared to the naive attack. Second, we counter the proposed attack with a noise-based defense mechanism that thwarts fingerprinting by adding noise to the specified performance metrics. The proposed defense strategy reduces the attack's accuracy and fidelity by up to $9.8\%$ and $4.8\%$, respectively (on medium-sized model extraction). Third, we show that the proposed defense induces a fundamental trade-off between the level of protection and system goodput, achieving configurable and significant victim model extraction protection while maintaining acceptable goodput ($>80\%$). We implement the proposed defense in a real system with plans to open source.
</details>
<details>
<summary>摘要</summary>
模型服务系统在实时网络应用中变得越来越流行，特别是在用户发送查询并指定需要的性能指标（例如精度和响应时间）后，服务器根据指定的指标从后端维护的模型 zoo 中提供查询结果。这篇论文检查这些系统的安全性，特别是对于模型提取攻击的Robustness。现有的黑盒攻击假设可以重复地选择服务器上的单个模型来进行推理请求。现代推理服务系统破坏了这一假设，因此无法直接应用于提取受害模型。攻击者无法确定她正在互动的是哪个模型。为此，我们首先提出了一种高效的询问算法，使得攻击者可以轻松地触发所需的模型。我们显示，使用我们的询问算法可以在$1\%$的精度和准确度下提取模型，并且可以在$14.6\%$的精度和$7.7\%$的准确度上提高模型提取的精度和准确度，相比之下 Naive 攻击。其次，我们采用噪音基的防御机制，将指定性能指标添加噪音，以防止指纹。我们的防御策略可以在中等模型提取 task 下 reducuce 攻击的精度和准确度为$9.8\%$和$4.8\%$。最后，我们显示了我们的防御机制存在可配置的质量和系统性能之间的负面冲击，可以在保持可接受的系统性能（$>80\%$）的情况下实现可靠的受害模型提取保护。我们已经实现了我们的防御机制，计划将其开源。
</details></li>
</ul>
<hr>
<h2 id="Fighting-the-disagreement-in-Explainable-Machine-Learning-with-consensus"><a href="#Fighting-the-disagreement-in-Explainable-Machine-Learning-with-consensus" class="headerlink" title="Fighting the disagreement in Explainable Machine Learning with consensus"></a>Fighting the disagreement in Explainable Machine Learning with consensus</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01288">http://arxiv.org/abs/2307.01288</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antonio Jesus Banegas-Luna, Carlos Martınez-Cortes, Horacio Perez-Sanchez</li>
<li>For: 本研究旨在解释机器学习模型的内部工作方式，以提高模型的可解释性。* Methods: 本研究使用了多种可解释性算法，包括本研究所开发的一种新的函数，以解释五种机器学习模型。* Results: 研究结果显示，提出的函数比其他函数更公正，提供了更一致和准确的解释。<details>
<summary>Abstract</summary>
Machine learning (ML) models are often valued by the accuracy of their predictions. However, in some areas of science, the inner workings of models are as relevant as their accuracy. To understand how ML models work internally, the use of interpretability algorithms is the preferred option. Unfortunately, despite the diversity of algorithms available, they often disagree in explaining a model, leading to contradictory explanations. To cope with this issue, consensus functions can be applied once the models have been explained. Nevertheless, the problem is not completely solved because the final result will depend on the selected consensus function and other factors. In this paper, six consensus functions have been evaluated for the explanation of five ML models. The models were previously trained on four synthetic datasets whose internal rules were known in advance. The models were then explained with model-agnostic local and global interpretability algorithms. Finally, consensus was calculated with six different functions, including one developed by the authors. The results demonstrated that the proposed function is fairer than the others and provides more consistent and accurate explanations.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:机器学习（ML）模型常被评估于其预测准确率。然而，在一些科学领域中，模型内部的工作方式也很重要。为了了解模型如何工作，使用可解释算法是最佳选择。然而，尽管有多种可解释算法可用，它们经常在解释模型时存在差异，导致不一致的解释。为了解决这个问题，可以应用consensus函数。然而，这并不完全解决问题，因为选择的consensus函数以及其他因素会影响最终结果。在这篇论文中，六种consensus函数被评估以解释五种ML模型。这些模型先前在四个已知内部规则的 sintetic数据集上进行了训练。然后，使用模型无关的本地和全局可解释算法来解释模型。最后，使用六种不同的consensus函数进行了投票，包括作者所开发的一种。结果表明，提案的函数比其他们更公平，并提供了更一致和准确的解释。
</details></li>
</ul>
<hr>
<h2 id="Using-BOLD-fMRI-to-Compute-the-Respiration-Volume-per-Time-RTV-and-Respiration-Variation-RV-with-Convolutional-Neural-Networks-CNN-in-the-Human-Connectome-Development-Cohort"><a href="#Using-BOLD-fMRI-to-Compute-the-Respiration-Volume-per-Time-RTV-and-Respiration-Variation-RV-with-Convolutional-Neural-Networks-CNN-in-the-Human-Connectome-Development-Cohort" class="headerlink" title="Using BOLD-fMRI to Compute the Respiration Volume per Time (RTV) and Respiration Variation (RV) with Convolutional Neural Networks (CNN) in the Human Connectome Development Cohort"></a>Using BOLD-fMRI to Compute the Respiration Volume per Time (RTV) and Respiration Variation (RV) with Convolutional Neural Networks (CNN) in the Human Connectome Development Cohort</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05426">http://arxiv.org/abs/2307.05426</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdoljalil Addeh, Fernando Vega, Rebecca J Williams, Ali Golestani, G. Bruce Pike, M. Ethan MacDonald</li>
<li>for: 这个研究的目的是提高fMRI研究中肺功能信号的质量和可用性。</li>
<li>methods: 该研究使用一种一维 convolutional neural network（CNN）模型来重建两种肺功能指标，即RV和RVT。</li>
<li>results: 研究结果表明，CNN模型可以从休息BOLD信号中捕捉有用的特征，并重建实际的RV和RVT时间序列。I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
In many fMRI studies, respiratory signals are unavailable or do not have acceptable quality. Consequently, the direct removal of low-frequency respiratory variations from BOLD signals is not possible. This study proposes a one-dimensional CNN model for reconstruction of two respiratory measures, RV and RVT. Results show that a CNN can capture informative features from resting BOLD signals and reconstruct realistic RV and RVT timeseries. It is expected that application of the proposed method will lower the cost of fMRI studies, reduce complexity, and decrease the burden on participants as they will not be required to wear a respiratory bellows.
</details>
<details>
<summary>摘要</summary>
很多fMRI研究中的呼吸信号不可用或者质量不良。因此，直接从BOLD信号中除去低频呼吸变化是不可能的。本研究提出了一种一维 convolutional neural network（CNN）模型，用于重建两个呼吸指标：RV和RVT。结果显示，CNN可以从休息BOLD信号中捕捉有用的特征，重建真实的RV和RVT时间序列。预计该方法的应用将降低fMRI研究的成本，降低复杂性，并减少参与者的负担，因为他们不需要穿戴呼吸膜。
</details></li>
</ul>
<hr>
<h2 id="NeuBTF-Neural-fields-for-BTF-encoding-and-transfer"><a href="#NeuBTF-Neural-fields-for-BTF-encoding-and-transfer" class="headerlink" title="NeuBTF: Neural fields for BTF encoding and transfer"></a>NeuBTF: Neural fields for BTF encoding and transfer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01199">http://arxiv.org/abs/2307.01199</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carlos Rodriguez-Pardo, Konstantinos Kazatzis, Jorge Lopez-Moreno, Elena Garces</li>
<li>for: 这篇论文旨在提出一种新的神经网络材料表示方法，用于解决神经网络材料的固定性问题，以便在渲染中使用。</li>
<li>methods: 该方法使用神经网络来表示材料，并使用引导图像来控制神经网络的输出。在测试时，该方法可以使用UV、摄像头和光照向量来查询神经网络的输出。</li>
<li>results: 该方法可以在多种 sintetic和实际材料上达到竞争性的压缩率，并且可以通过引导图像来控制神经网络的输出。<details>
<summary>Abstract</summary>
Neural material representations are becoming a popular way to represent materials for rendering. They are more expressive than analytic models and occupy less memory than tabulated BTFs. However, existing neural materials are immutable, meaning that their output for a certain query of UVs, camera, and light vector is fixed once they are trained. While this is practical when there is no need to edit the material, it can become very limiting when the fragment of the material used for training is too small or not tileable, which frequently happens when the material has been captured with a gonioreflectometer. In this paper, we propose a novel neural material representation which jointly tackles the problems of BTF compression, tiling, and extrapolation. At test time, our method uses a guidance image as input to condition the neural BTF to the structural features of this input image. Then, the neural BTF can be queried as a regular BTF using UVs, camera, and light vectors. Every component in our framework is purposefully designed to maximize BTF encoding quality at minimal parameter count and computational complexity, achieving competitive compression rates compared with previous work. We demonstrate the results of our method on a variety of synthetic and captured materials, showing its generality and capacity to learn to represent many optical properties.
</details>
<details>
<summary>摘要</summary>
神经材料表示法是现代渲染中广泛应用的一种表示方法。它比分析模型更加表达力，且占用内存更少，但现有的神经材料都是不可变的，意味着它们的输出对于特定的UV、摄像机和光量向量的训练后就是固定的。这在材料的预测中是有用的，但在材料需要编辑时可能变得非常限制性。在这篇论文中，我们提出了一种新的神经材料表示方法，该方法同时解决了BTF压缩、瓦片和推导问题。在测试时，我们使用导航图像作为输入，通过conditioning神经BTF于这个输入图像的结构特征来控制神经BTF。然后，神经BTF可以被查询作为普通BTF使用UV、摄像机和光量向量。我们的框架中每个组件都是为最大化BTF编码质量而设计，而且减少参数计数和计算复杂度，与之前的工作相比，我们的方法实现了竞争力的压缩率。我们在多种 sintetic和捕捉的材料上进行了试验，展示了我们的方法的通用性和能力学习表示多种光学性质。
</details></li>
</ul>
<hr>
<h2 id="Improved-sampling-via-learned-diffusions"><a href="#Improved-sampling-via-learned-diffusions" class="headerlink" title="Improved sampling via learned diffusions"></a>Improved sampling via learned diffusions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01198">http://arxiv.org/abs/2307.01198</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lorenz Richter, Julius Berner, Guan-Horng Liu</li>
<li>for: 这些论文提出了基于深度学习的方法，用于从不正规分布中采样。</li>
<li>methods: 这些方法是控制的扩散过程的特殊情况，寻找从给定的先前分布到目标分布的最有可能的杂乱进程。</li>
<li>results: 我们在这些方法中引入了一种变量形式，基于时间反转的扩散过程中的路径空间测量差异。这种抽象视角导致了可优化的梯度下降算法，并包含了先前的目标作为特殊情况。此外，我们还可以考虑不同于倒卡劳布拉迪弗分布的差异，以避免模式塌缩。例如，我们提出了对数差异损失函数，它在数值上显示了优化性和改进性。<details>
<summary>Abstract</summary>
Recently, a series of papers proposed deep learning-based approaches to sample from unnormalized target densities using controlled diffusion processes. In this work, we identify these approaches as special cases of the Schr\"odinger bridge problem, seeking the most likely stochastic evolution between a given prior distribution and the specified target. We further generalize this framework by introducing a variational formulation based on divergences between path space measures of time-reversed diffusion processes. This abstract perspective leads to practical losses that can be optimized by gradient-based algorithms and includes previous objectives as special cases. At the same time, it allows us to consider divergences other than the reverse Kullback-Leibler divergence that is known to suffer from mode collapse. In particular, we propose the so-called log-variance loss, which exhibits favorable numerical properties and leads to significantly improved performance across all considered approaches.
</details>
<details>
<summary>摘要</summary>
最近，一系列论文提出了基于深度学习的方法来从不正规Target概率分布中采样。在这篇文章中，我们将这些方法定义为Schrödinger大桥问题的特殊情况，寻找从给定的先验分布到指定的Target概率分布的最有可能的杂化过程。我们进一步总结了这个框架，通过在Path空间测度上引入减法，从而得到了一种可优化的变分形式。这种抽象的视角允许我们考虑其他than reverse Kullback-Leibler divergence的异同，这种异同 known to suffer from mode collapse。特别是，我们提议使用Log-variance loss，它在数值上具有优秀的性质，并在所有考虑的方法中带来显著改进。
</details></li>
</ul>
<hr>
<h2 id="Squeezing-Large-Scale-Diffusion-Models-for-Mobile"><a href="#Squeezing-Large-Scale-Diffusion-Models-for-Mobile" class="headerlink" title="Squeezing Large-Scale Diffusion Models for Mobile"></a>Squeezing Large-Scale Diffusion Models for Mobile</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01193">http://arxiv.org/abs/2307.01193</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiwoong Choi, Minkyu Kim, Daehyun Ahn, Taesu Kim, Yulhwa Kim, Dongwon Jo, Hyesung Jeon, Jae-Joon Kim, Hyungjun Kim</li>
<li>for: 这 paper 旨在探讨将 Stable Diffusion 模型部署到移动设备上，以便实现高精度图像生成。</li>
<li>methods: 该 paper 使用 TensorFlow Lite 框架来实现移动设备上的 Stable Diffusion 部署，并支持 iOS 和 Android 设备。</li>
<li>results: 该 paper 实现的 Mobile Stable Diffusion 可以在 Android 设备上 achieve 512x512 图像生成的推理延迟时间小于 7 秒，并且可以在移动 GPU 上实现。<details>
<summary>Abstract</summary>
The emergence of diffusion models has greatly broadened the scope of high-fidelity image synthesis, resulting in notable advancements in both practical implementation and academic research. With the active adoption of the model in various real-world applications, the need for on-device deployment has grown considerably. However, deploying large diffusion models such as Stable Diffusion with more than one billion parameters to mobile devices poses distinctive challenges due to the limited computational and memory resources, which may vary according to the device. In this paper, we present the challenges and solutions for deploying Stable Diffusion on mobile devices with TensorFlow Lite framework, which supports both iOS and Android devices. The resulting Mobile Stable Diffusion achieves the inference latency of smaller than 7 seconds for a 512x512 image generation on Android devices with mobile GPUs.
</details>
<details>
<summary>摘要</summary>
Diffusion模型的出现已经极大地扩大了高精度图像生成的范围，导致了实践部署和学术研究中的重要进步。然而，将大型Diffusion模型，如Stable Diffusion，deploy到移动设备上具有限制的计算和内存资源的问题。在这篇文章中，我们介绍了将Stable Diffusion部署到移动设备上的挑战和解决方案，使用TensorFlow Lite框架支持iOS和Android设备。我们的Mobile Stable Diffusion实现了512x512像素生成的推理延迟低于7秒钟在Android设备上。
</details></li>
</ul>
<hr>
<h2 id="Trainable-Transformer-in-Transformer"><a href="#Trainable-Transformer-in-Transformer" class="headerlink" title="Trainable Transformer in Transformer"></a>Trainable Transformer in Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01189">http://arxiv.org/abs/2307.01189</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/abhishekpanigrahi1996/transformer_in_transformer">https://github.com/abhishekpanigrahi1996/transformer_in_transformer</a></li>
<li>paper_authors: Abhishek Panigrahi, Sadhika Malladi, Mengzhou Xia, Sanjeev Arora</li>
<li>for: 这个论文目的是提出一种高效的Transformer模型内部精细调整方法，以便在推理过程中进行精细调整。</li>
<li>methods: 这个方法使用了一些创新的近似技术，使得一个具有少于20亿参数的TinT模型能够在单步前进中 simulate和精细调整一个125亿参数的Transformer模型。</li>
<li>results: 在语言模型和下游任务中进行综合实验 validate了TinT模型的内部精细调整过程，并证明了大型预训练语言模型可以执行复杂的子任务。例如，even with a limited one-step budget, we observe TinT for a OPT-125M model improves performance by 4-16% absolute on average compared to OPT-125M。<details>
<summary>Abstract</summary>
Recent works attribute the capability of in-context learning (ICL) in large pre-trained language models to implicitly simulating and fine-tuning an internal model (e.g., linear or 2-layer MLP) during inference. However, such constructions require large memory overhead, which makes simulation of more sophisticated internal models intractable. In this work, we propose an efficient construction, Transformer in Transformer (in short, TinT), that allows a transformer to simulate and fine-tune complex models internally during inference (e.g., pre-trained language models). In particular, we introduce innovative approximation techniques that allow a TinT model with less than 2 billion parameters to simulate and fine-tune a 125 million parameter transformer model within a single forward pass. TinT accommodates many common transformer variants and its design ideas also improve the efficiency of past instantiations of simple models inside transformers. We conduct end-to-end experiments to validate the internal fine-tuning procedure of TinT on various language modeling and downstream tasks. For example, even with a limited one-step budget, we observe TinT for a OPT-125M model improves performance by 4-16% absolute on average compared to OPT-125M. These findings suggest that large pre-trained language models are capable of performing intricate subroutines. To facilitate further work, a modular and extensible codebase for TinT is included.
</details>
<details>
<summary>摘要</summary>
近期研究归功大型预训言语模型中的增Context学习（ICL）能力于内部模型（例如线性或2层MLP）的隐式模拟和细化 during inference. 然而，这些建构具有大量内存开销，使得更复杂的内部模型的模拟成为不可行。在这项工作中，我们提出了高效的建构——Transformer in Transformer（简称TinT），允许 transformer 模型在执行中内部模拟和细化复杂模型（例如预训言语模型）。具体来说，我们提出了创新的近似技术，使得 TinT 模型 fewer than 200 billion parameters 可以在单个前进 pass 中模拟和细化 125 million parameter transformer 模型。TinT 支持许多常见 transformer 变种，并且其设计想法也提高了过去内置简单模型的效率。我们通过综合实验 validate TinT 模型内部细化过程的效果，并在语言模型和下游任务上 observe 4-16% 绝对提升。这些发现表明大规模预训言语模型可以执行复杂的子routines。为了便于后续工作，我们附加了可扩展和可模块化的代码基金。
</details></li>
</ul>
<hr>
<h2 id="Fitting-an-ellipsoid-to-a-quadratic-number-of-random-points"><a href="#Fitting-an-ellipsoid-to-a-quadratic-number-of-random-points" class="headerlink" title="Fitting an ellipsoid to a quadratic number of random points"></a>Fitting an ellipsoid to a quadratic number of random points</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01181">http://arxiv.org/abs/2307.01181</a></li>
<li>repo_url: None</li>
<li>paper_authors: Afonso S. Bandeira, Antoine Maillard, Shahar Mendelson, Elliot Paquette</li>
<li>for: 这个论文研究了将 $n$ 个标准正态随机向量在 $\mathbb{R}^d$ 中适应中心圆柱体的问题，当 $n, d \to \infty$ 时。</li>
<li>methods: 这个论文使用了 Bartl &amp; Mendelson 关于 Gram 矩阵的集中性的结论，并使用了一些轻量级的假设来证明这个问题在高概率下是可行的。</li>
<li>results: 这个论文证明了当 $n \leq d^2 &#x2F; C$，其中 $C &gt; 0$ 是一个可能很大的常数， THEN 问题 $(\mathrm{P})$ 有高概率是可行的。<details>
<summary>Abstract</summary>
We consider the problem $(\mathrm{P})$ of fitting $n$ standard Gaussian random vectors in $\mathbb{R}^d$ to the boundary of a centered ellipsoid, as $n, d \to \infty$. This problem is conjectured to have a sharp feasibility transition: for any $\varepsilon > 0$, if $n \leq (1 - \varepsilon) d^2 / 4$ then $(\mathrm{P})$ has a solution with high probability, while $(\mathrm{P})$ has no solutions with high probability if $n \geq (1 + \varepsilon) d^2 /4$. So far, only a trivial bound $n \geq d^2 / 2$ is known on the negative side, while the best results on the positive side assume $n \leq d^2 / \mathrm{polylog}(d)$. In this work, we improve over previous approaches using a key result of Bartl & Mendelson on the concentration of Gram matrices of random vectors under mild assumptions on their tail behavior. This allows us to give a simple proof that $(\mathrm{P})$ is feasible with high probability when $n \leq d^2 / C$, for a (possibly large) constant $C > 0$.
</details>
<details>
<summary>摘要</summary>
我们考虑一个问题($\mathrm{P}$)，即在中心为零的椭球上适应 $n$ 标准高斯均匀随机向量，当 $n, d \to \infty$ 时。这个问题据悉有一个锐化可行性过渡：如果 $n \leq (1 - \varepsilon) d^2 / 4$，那么 $(\mathrm{P})$ 有高概率解，而如果 $n \geq (1 + \varepsilon) d^2 /4$，那么 $(\mathrm{P})$ 有高概率无解。目前只知道一个负边界 $n \geq d^2 / 2$，而最好的结果在正边界上假设 $n \leq d^2 / \text{polylog}(d)$。在这个工作中，我们使用 Bartl & Mendelson 关于均匀矩阵的吸引性的结果，从而得到一个简单的证明：如果 $n \leq d^2 / C$，那么 $(\mathrm{P})$ 有高概率解，其中 $C > 0$ 是一个可能很大的常数。
</details></li>
</ul>
<hr>
<h2 id="PlanE-Representation-Learning-over-Planar-Graphs"><a href="#PlanE-Representation-Learning-over-Planar-Graphs" class="headerlink" title="PlanE: Representation Learning over Planar Graphs"></a>PlanE: Representation Learning over Planar Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01180">http://arxiv.org/abs/2307.01180</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zzysonny/plane">https://github.com/zzysonny/plane</a></li>
<li>paper_authors: Radoslav Dimitrov, Zeyang Zhao, Ralph Abboud, İsmail İlkan Ceylan</li>
<li>for: 本研究的目的是设计一个可以快速学习完整的平面图 isomorphism 的架构，以便在平面图上进行图像学习。</li>
<li>methods: 本研究使用了一种称为 PlanE 的框架，它是基于 Hopcroft 和 Tarjan 的平面图 isomorphism 算法。PlanE 包括一些可以学习完整的平面图 invariants 的架构，并且可以在实际上扩展到大规模的平面图。</li>
<li>results: 本研究透过实验验证了 PlanE 的模型架构，并取得了多个 state-of-the-art 的结果。在 well-known 平面图 benchmark 上，PlanE 的模型能够实现高效地学习完整的平面图 invariants。<details>
<summary>Abstract</summary>
Graph neural networks are prominent models for representation learning over graphs, where the idea is to iteratively compute representations of nodes of an input graph through a series of transformations in such a way that the learned graph function is isomorphism invariant on graphs, which makes the learned representations graph invariants. On the other hand, it is well-known that graph invariants learned by these class of models are incomplete: there are pairs of non-isomorphic graphs which cannot be distinguished by standard graph neural networks. This is unsurprising given the computational difficulty of graph isomorphism testing on general graphs, but the situation begs to differ for special graph classes, for which efficient graph isomorphism testing algorithms are known, such as planar graphs. The goal of this work is to design architectures for efficiently learning complete invariants of planar graphs. Inspired by the classical planar graph isomorphism algorithm of Hopcroft and Tarjan, we propose PlanE as a framework for planar representation learning. PlanE includes architectures which can learn complete invariants over planar graphs while remaining practically scalable. We empirically validate the strong performance of the resulting model architectures on well-known planar graph benchmarks, achieving multiple state-of-the-art results.
</details>
<details>
<summary>摘要</summary>
“图 neural networks 是 Representation learning over graphs 中的主要模型，其中的思想是通过一系列转换来计算输入图的节点的表示，以确定learned graph function 是isoformation invariant的，这使得learned representation 成为图 invariants。然而，已知这些类型的模型学习的图 invariants 是不完全的：存在一些非同构的图对标准图 neural networks 无法分辨。这不Surprising，因为计算通用图 isomorphism testing 的计算复杂度很高，但在特定的图类中，有高效的图 isomorphism testing 算法，如平面图。我们的目标是设计一种能够有效地学习完整的平面图 invariants的architecture。 draw inspiration from Hopcroft 和 Tarjan 的平面图 isomorphism 算法，我们提出 PlanE 框架，用于平面 representation learning。 PlanE 包括一些可以学习完整的平面图 invariants 的architecture，并且 remain practically scalable。我们通过实验证明了这些结果的强性，在well-known planar graph benchmarks 上达到多个state-of-the-art result。”
</details></li>
</ul>
<hr>
<h2 id="Learning-Mixtures-of-Gaussians-Using-the-DDPM-Objective"><a href="#Learning-Mixtures-of-Gaussians-Using-the-DDPM-Objective" class="headerlink" title="Learning Mixtures of Gaussians Using the DDPM Objective"></a>Learning Mixtures of Gaussians Using the DDPM Objective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01178">http://arxiv.org/abs/2307.01178</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kulin Shah, Sitan Chen, Adam Klivans</li>
<li>for: 本文研究了 diffusion 模型可以学习哪些分布？</li>
<li>methods: 本文使用了什么方法？</li>
<li>results: 本文得到了哪些结果？Here are my answers, in Simplified Chinese:</li>
<li>for: 本文研究了 diffusion 模型可以学习 Gaussian mixture models 的参数。</li>
<li>methods: 本文使用了 gradient descent 算法，并证明了其可以高效地学习 Gaussian mixture models 的参数。</li>
<li>results: 本文证明了 gradient descent 算法可以在两种设置下高效地学习 Gaussian mixture models：1) 随机初始化下可以learns mixtures of two spherical Gaussians in $d$ dimensions with $1&#x2F;\text{poly}(d)$-separated centers。2) warm start 下可以 learns mixtures of $K$ spherical Gaussians with $\Omega(\sqrt{\log(\min(K,d))})$-separated centers。<details>
<summary>Abstract</summary>
Recent works have shown that diffusion models can learn essentially any distribution provided one can perform score estimation. Yet it remains poorly understood under what settings score estimation is possible, let alone when practical gradient-based algorithms for this task can provably succeed.   In this work, we give the first provably efficient results along these lines for one of the most fundamental distribution families, Gaussian mixture models. We prove that gradient descent on the denoising diffusion probabilistic model (DDPM) objective can efficiently recover the ground truth parameters of the mixture model in the following two settings: 1) We show gradient descent with random initialization learns mixtures of two spherical Gaussians in $d$ dimensions with $1/\text{poly}(d)$-separated centers. 2) We show gradient descent with a warm start learns mixtures of $K$ spherical Gaussians with $\Omega(\sqrt{\log(\min(K,d))})$-separated centers. A key ingredient in our proofs is a new connection between score-based methods and two other approaches to distribution learning, the EM algorithm and spectral methods.
</details>
<details>
<summary>摘要</summary>
近期研究表明，扩散模型可以学习任何分布，只要可以进行分数估计。然而，我们还不够了解在哪些情况下分数估计是可行的，更重要的是，我们是否可以实现有效的梯度下降算法来解决这个问题。在这个工作中，我们给出了首次可证fficient的结果，其中包括以下两个情况：1. 我们证明，使用随机 initialization 的梯度下降在 $d$ 维的两个球形 Gaussian 混合模型中可以有效地回归真实参数。2. 我们证明，使用温始的梯度下降可以在 $K$ 个球形 Gaussian 混合模型中，对中心点进行 $\Omega(\sqrt{\log(\min(K,d))})$ 级别的分割。我们的证明中的一个关键元素是一种新的分数-基本方法和 EM 算法以及spectral methods之间的连接。
</details></li>
</ul>
<hr>
<h2 id="Neural-Hilbert-Ladders-Multi-Layer-Neural-Networks-in-Function-Space"><a href="#Neural-Hilbert-Ladders-Multi-Layer-Neural-Networks-in-Function-Space" class="headerlink" title="Neural Hilbert Ladders: Multi-Layer Neural Networks in Function Space"></a>Neural Hilbert Ladders: Multi-Layer Neural Networks in Function Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01177">http://arxiv.org/abs/2307.01177</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhengdao Chen</li>
<li>for: 本文研究深度学习理论中神经网络（NN）定义的函数空间的特点。</li>
<li>methods: 作者视多层NN为定义特定层次的再生核希尔бер特空间（RKHS），称为神经希尔бер特阶梯（NHL）。</li>
<li>results: 作者证明了多层NN表达的函数和NHL之间的对应关系，并提供了控制复杂性度量的泛化保证。 Plus, the author derives the evolution of NHL as the dynamics of multiple random fields, and shows examples of depth separation in NHLs under different activation functions.<details>
<summary>Abstract</summary>
The characterization of the functions spaces explored by neural networks (NNs) is an important aspect of deep learning theory. In this work, we view a multi-layer NN with arbitrary width as defining a particular hierarchy of reproducing kernel Hilbert spaces (RKHSs), named a Neural Hilbert Ladder (NHL). This allows us to define a function space and a complexity measure that generalize prior results for shallow NNs, and we then examine their theoretical properties and implications in several aspects. First, we prove a correspondence between functions expressed by L-layer NNs and those belonging to L-level NHLs. Second, we prove generalization guarantees for learning an NHL with the complexity measure controlled. Third, corresponding to the training of multi-layer NNs in the infinite-width mean-field limit, we derive an evolution of the NHL characterized as the dynamics of multiple random fields. Fourth, we show examples of depth separation in NHLs under ReLU and quadratic activation functions. Finally, we complement the theory with numerical results to illustrate the learning of RKHS in NN training.
</details>
<details>
<summary>摘要</summary>
文章主要探讨深度学习理论中神经网络（NN）函数空间的特点。在这篇文章中，我们将多层NN视为定义特定层次的重复内 produit 希尔бер特空间（RKHS），称为神经希尔бер特阶梯（NHL）。这允许我们定义函数空间和复杂度度量，这些度量将对先前的浅层NN进行扩展，并且我们将研究这些理论性质和影响。首先，我们证明了L层NN表达的函数和L层NHL之间的对应关系。其次，我们证明了控制复杂度度量的学习承诺。第三，对于在无限宽度的平均场中训练多层NN，我们 derivation 了NHL的演化，这可以看做多个Random Fields 的动态。最后，我们通过实验示例来补充理论，以Illustrate 神经网络在训练中学习RKHS的过程。
</details></li>
</ul>
<hr>
<h2 id="Quantum-Neural-Estimation-of-Entropies"><a href="#Quantum-Neural-Estimation-of-Entropies" class="headerlink" title="Quantum Neural Estimation of Entropies"></a>Quantum Neural Estimation of Entropies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01171">http://arxiv.org/abs/2307.01171</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziv Goldfeld, Dhrumil Patel, Sreejith Sreekumar, Mark M. Wilde</li>
<li>for: 估计量子系统中的信息量和相关性</li>
<li>methods: 使用变量量子算法和经典神经网络参数化测量方法</li>
<li>results: 精确地估计了不同 entropy 度量的值，有效地应用于下游任务<details>
<summary>Abstract</summary>
Entropy measures quantify the amount of information and correlations present in a quantum system. In practice, when the quantum state is unknown and only copies thereof are available, one must resort to the estimation of such entropy measures. Here we propose a variational quantum algorithm for estimating the von Neumann and R\'enyi entropies, as well as the measured relative entropy and measured R\'enyi relative entropy. Our approach first parameterizes a variational formula for the measure of interest by a quantum circuit and a classical neural network, and then optimizes the resulting objective over parameter space. Numerical simulations of our quantum algorithm are provided, using a noiseless quantum simulator. The algorithm provides accurate estimates of the various entropy measures for the examples tested, which renders it as a promising approach for usage in downstream tasks.
</details>
<details>
<summary>摘要</summary>
Entropy 测量量代表量子系统中的信息量和相关性。在实践中，当量子状态未知，仅可以通过量子状态的复制来进行估算Entropy测量。我们提出了一种量子算法来估算 von Neumann 熵和 R\'enyi 熵，以及测量相对熵和测量 R\'enyi 相对熵。我们的方法首先假设测量对象的量子演算和классиical neural network的参数，然后对参数空间进行优化。我们的numerical simulation表明，该算法可以准确地估算各种熵测量的例子，这使其成为下游任务中的一个有前途的方法。Here's the breakdown of the translation:* Entropy 测量量 (Entropy measures) -> 熵测量 (entropy measurements)* 量子系统 (quantum system) -> 量子状态 (quantum state)* 未知 (unknown) -> 未知的 (unknown)* 复制 (copies) -> 复制品 (copies)* 估算 (estimation) -> 估算值 (estimated value)* von Neumann 熵 (Von Neumann entropy) -> von Neumann 熵量 (Von Neumann entropy)* R\'enyi 熵 (R\'enyi entropy) -> R\'enyi 熵量 (R\'enyi entropy)* 测量相对熵 (measured relative entropy) -> 测量相对熵量 (measured relative entropy)* 测量 R\'enyi 相对熵 (measured R\'enyi relative entropy) -> 测量 R\'enyi 相对熵量 (measured R\'enyi relative entropy)* 参数 (parameters) -> 参数空间 (parameter space)* 优化 (optimization) -> 优化过程 (optimization process)* numerical simulation -> 数值仿真 (numerical simulation)Note that the translation is done in Simplified Chinese, which is the most widely used standard for Chinese writing. The translation is done word-for-word, and some of the phrases or sentences may not be exactly the same as the original English version, but they should convey the same meaning.
</details></li>
</ul>
<hr>
<h2 id="Online-nearest-neighbor-classification"><a href="#Online-nearest-neighbor-classification" class="headerlink" title="Online nearest neighbor classification"></a>Online nearest neighbor classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01170">http://arxiv.org/abs/2307.01170</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sayantann11/all-classification-templetes-for-ML">https://github.com/sayantann11/all-classification-templetes-for-ML</a></li>
<li>paper_authors: Sanjoy Dasgupta, Geelon So</li>
<li>for: 研究在可实现 Setting 中的在线非参数化分类问题。</li>
<li>methods: 使用 classical 1-nearest neighbor algorithm，并证明其在可实现 Setting 中 achieve 下降的误差率。</li>
<li>results: 实现下降的误差率，即在对征或平滑的对手中的误差率。<details>
<summary>Abstract</summary>
We study an instance of online non-parametric classification in the realizable setting. In particular, we consider the classical 1-nearest neighbor algorithm, and show that it achieves sublinear regret - that is, a vanishing mistake rate - against dominated or smoothed adversaries in the realizable setting.
</details>
<details>
<summary>摘要</summary>
我们研究在可实现 setting 中的在线非参数化分类问题。特别是，我们考虑了经典的1 nearest neighbor算法，并证明它在可实现 setting 中对于受控或平滑的反对敌人（adversaries） achieve 子线性 regret - 即消失的错误率。
</details></li>
</ul>
<hr>
<h2 id="Analyzing-and-Improving-Greedy-2-Coordinate-Updates-for-Equality-Constrained-Optimization-via-Steepest-Descent-in-the-1-Norm"><a href="#Analyzing-and-Improving-Greedy-2-Coordinate-Updates-for-Equality-Constrained-Optimization-via-Steepest-Descent-in-the-1-Norm" class="headerlink" title="Analyzing and Improving Greedy 2-Coordinate Updates for Equality-Constrained Optimization via Steepest Descent in the 1-Norm"></a>Analyzing and Improving Greedy 2-Coordinate Updates for Equality-Constrained Optimization via Steepest Descent in the 1-Norm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01169">http://arxiv.org/abs/2307.01169</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amrutha Varshini Ramesh, Aaron Mishkin, Mark Schmidt, Yihan Zhou, Jonathan Wilder Lavington, Jennifer She</li>
<li>for: 这篇论文是关于最优化问题的，具体来说是使用斜率逐步下降法和贝叶斯搜索法来解决一种具有约束的最优化问题。</li>
<li>methods: 这篇论文使用了一种名为”proximal Polyak-Lojasiewicz”的假设，并通过将这个假设应用到斜率逐步下降法中来提高它的准确率。此外，论文还使用了一种名为”bound- and summation-constrained steepest descent”的方法来解决具有约束的最优化问题。</li>
<li>results: 论文的结果表明，使用这种新的方法可以在$O(n \log n)$时间内解决具有约束的最优化问题，而且比之前的方法更快。此外，论文还证明了这种方法的准确率是Random Selection的两倍，并且不виси于问题的维度$n$。<details>
<summary>Abstract</summary>
We consider minimizing a smooth function subject to a summation constraint over its variables. By exploiting a connection between the greedy 2-coordinate update for this problem and equality-constrained steepest descent in the 1-norm, we give a convergence rate for greedy selection under a proximal Polyak-Lojasiewicz assumption that is faster than random selection and independent of the problem dimension $n$. We then consider minimizing with both a summation constraint and bound constraints, as arises in the support vector machine dual problem. Existing greedy rules for this setting either guarantee trivial progress only or require $O(n^2)$ time to compute. We show that bound- and summation-constrained steepest descent in the L1-norm guarantees more progress per iteration than previous rules and can be computed in only $O(n \log n)$ time.
</details>
<details>
<summary>摘要</summary>
我们考虑最小化一个几何函数，并且需要遵循一个总和约束。我们利用两个坐标更新的连接，将其与等式约束对应的steepest descent在L1内 producer一个更快的价值变数。然后我们考虑受约束的最小化问题，其中包括总和约束和范围约束。现有的对策可能只能保证很小的进步，或者需要O(n^2)的时间来计算。我们显示，在L1内的约束降阶 descendence可以在O(nlogn)的时间内获得更多的进步，并且可以更快地计算。
</details></li>
</ul>
<hr>
<h2 id="Don’t-freeze-Finetune-encoders-for-better-Self-Supervised-HAR"><a href="#Don’t-freeze-Finetune-encoders-for-better-Self-Supervised-HAR" class="headerlink" title="Don’t freeze: Finetune encoders for better Self-Supervised HAR"></a>Don’t freeze: Finetune encoders for better Self-Supervised HAR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01168">http://arxiv.org/abs/2307.01168</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vitor Fortes Rey, Dominique Nshimyimana, Paul Lukowicz</li>
<li>for: 这个论文是为了解决人类活动识别领域中的标签数据可用性问题而提出的一种解决方案。</li>
<li>methods: 这个论文使用了自然语言处理中的预测任务，如重构和对比预测编码，来学习有用的表示。这些方法采用了预训练、冻结和细化的过程。</li>
<li>results: 这个论文发现，不冻结表示后的表示可以获得显著性能提升，这种提升是随着标签数据的量而增加的。此外，这种效果是无论在Capture24数据集上进行预测任务还是直接在目标数据集上进行预测任务中都存在。<details>
<summary>Abstract</summary>
Recently self-supervised learning has been proposed in the field of human activity recognition as a solution to the labelled data availability problem. The idea being that by using pretext tasks such as reconstruction or contrastive predictive coding, useful representations can be learned that then can be used for classification. Those approaches follow the pretrain, freeze and fine-tune procedure. In this paper we will show how a simple change - not freezing the representation - leads to substantial performance gains across pretext tasks. The improvement was found in all four investigated datasets and across all four pretext tasks and is inversely proportional to amount of labelled data. Moreover the effect is present whether the pretext task is carried on the Capture24 dataset or directly in unlabelled data of the target dataset.
</details>
<details>
<summary>摘要</summary>
近期，无监督学习在人活动识别领域被提出，作为数据可用性问题的解决方案。这种方法是通过重构或对比预测编码来学习有用的表示，然后用于分类。这些方法遵循“预训练、冻结并微调”的过程。在这篇论文中，我们将展示一种简单的改变：不冻结表示，导致了重要的性能提升，并且这种提升随着数据量的减少而增加。此外，这种效果是不论预测任务是在 Capture24 数据集上进行还是直接在无标签数据集上进行的。
</details></li>
</ul>
<hr>
<h2 id="Coupled-Gradient-Flows-for-Strategic-Non-Local-Distribution-Shift"><a href="#Coupled-Gradient-Flows-for-Strategic-Non-Local-Distribution-Shift" class="headerlink" title="Coupled Gradient Flows for Strategic Non-Local Distribution Shift"></a>Coupled Gradient Flows for Strategic Non-Local Distribution Shift</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01166">http://arxiv.org/abs/2307.01166</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lauren Conger, Franca Hoffmann, Eric Mazumdar, Lillian Ratliff</li>
<li>for: 本研究旨在分析现实世界系统中的分布变化动态，包括学习算法和其部署的分布之间的反馈循环。</li>
<li>methods: 本研究提出了一种新的整合方法，该方法可以模型学习算法部署中的复杂分布变化，包括策略性反应、非本地人口互动和其他外部因素引起的分布变化。</li>
<li>results: 研究表明，当算法进行梯度下降 retraining 时，可以 дости到稳定状态，并且在有限和无限维度中都有显式速率，这些速率取决于模型参数。此外，研究还发现，该方法可以 Capture 许多已知的分布变化形式，如楔形和不同影响。<details>
<summary>Abstract</summary>
We propose a novel framework for analyzing the dynamics of distribution shift in real-world systems that captures the feedback loop between learning algorithms and the distributions on which they are deployed. Prior work largely models feedback-induced distribution shift as adversarial or via an overly simplistic distribution-shift structure. In contrast, we propose a coupled partial differential equation model that captures fine-grained changes in the distribution over time by accounting for complex dynamics that arise due to strategic responses to algorithmic decision-making, non-local endogenous population interactions, and other exogenous sources of distribution shift. We consider two common settings in machine learning: cooperative settings with information asymmetries, and competitive settings where a learner faces strategic users. For both of these settings, when the algorithm retrains via gradient descent, we prove asymptotic convergence of the retraining procedure to a steady-state, both in finite and in infinite dimensions, obtaining explicit rates in terms of the model parameters. To do so we derive new results on the convergence of coupled PDEs that extends what is known on multi-species systems. Empirically, we show that our approach captures well-documented forms of distribution shifts like polarization and disparate impacts that simpler models cannot capture.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的框架，用于分析实际系统中分布shift的动态。这个框架 capture了学习算法和它们所部署的分布之间的反馈循环。先前的工作大多把反馈引起的分布shift模型为对抗性或非常简单的分布shift结构。相比之下，我们提出了一个结合部分梯度方程的模型，该模型可以考虑复杂的时间变化、策略性反应、非本地人口互动等因素，以捕捉细腻的分布变化。我们考虑了两种常见的机器学习设置：合作性设置和竞争性设置。在两个设置中，当算法通过梯度下降 retrained 时，我们证明了预测过程的稳定性，包括有限维度和无穷维度下的稳定性，并得到了明确的速率。为此，我们 derivation 了新的结果，用于coupled PDEs 的减少。实际证明，我们的方法能够捕捉到一些已知的分布shift形式，如极化和不同的影响。
</details></li>
</ul>
<hr>
<h2 id="Improving-Language-Plasticity-via-Pretraining-with-Active-Forgetting"><a href="#Improving-Language-Plasticity-via-Pretraining-with-Active-Forgetting" class="headerlink" title="Improving Language Plasticity via Pretraining with Active Forgetting"></a>Improving Language Plasticity via Pretraining with Active Forgetting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01163">http://arxiv.org/abs/2307.01163</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yihong Chen, Kelly Marchisio, Roberta Raileanu, David Ifeoluwa Adelani, Pontus Stenetorp, Sebastian Riedel, Mikel Artetxe</li>
<li>for: 实现PLMs的 universality，将其应用于新语言。</li>
<li>methods: 使用活动遗忘机制 during pretraining，以实现PLMs快速适应新语言。</li>
<li>results: 在语言适应中，使用我们的遗忘机制可以提高PLMs的学习新embeddings的能力，并在仅有少量数据的情况下表现出佳。<details>
<summary>Abstract</summary>
Pretrained language models (PLMs) are today the primary model for natural language processing. Despite their impressive downstream performance, it can be difficult to apply PLMs to new languages, a barrier to making their capabilities universally accessible. While prior work has shown it possible to address this issue by learning a new embedding layer for the new language, doing so is both data and compute inefficient. We propose to use an active forgetting mechanism during pretraining, as a simple way of creating PLMs that can quickly adapt to new languages. Concretely, by resetting the embedding layer every K updates during pretraining, we encourage the PLM to improve its ability of learning new embeddings within a limited number of updates, similar to a meta-learning effect. Experiments with RoBERTa show that models pretrained with our forgetting mechanism not only demonstrate faster convergence during language adaptation but also outperform standard ones in a low-data regime, particularly for languages that are distant from English.
</details>
<details>
<summary>摘要</summary>
现在，预训练语言模型（PLM）是自然语言处理的主要模型。尽管它们在下游性能方面表现出色，但是将其应用到新语言可能会增加难度，从而限制其universal accessible的能力。先前的工作已经证明可以通过学习一个新的嵌入层来解决这个问题，但是这需要大量的数据和计算资源。我们提议使用活动忘记机制 durante la pretrainings，作为一种简单的创建 PLMs 可快速适应新语言的方法。具体来说，在每K更新中，我们会重置嵌入层，这会让 PLM 在有限的更新数量内提高其学习新嵌入的能力，类似于 meta-learning 效果。我们通过使用 RoBERTa 进行实验，发现使用我们的忘记机制不仅可以在语言适应过程中快速 convergence，而且在数据量低的情况下，特别是与英语远的语言，也能够表现出较好的性能。
</details></li>
</ul>
<hr>
<h2 id="Theory-of-Mind-as-Intrinsic-Motivation-for-Multi-Agent-Reinforcement-Learning"><a href="#Theory-of-Mind-as-Intrinsic-Motivation-for-Multi-Agent-Reinforcement-Learning" class="headerlink" title="Theory of Mind as Intrinsic Motivation for Multi-Agent Reinforcement Learning"></a>Theory of Mind as Intrinsic Motivation for Multi-Agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01158">http://arxiv.org/abs/2307.01158</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ini Oguntola, Joseph Campbell, Simon Stepputtis, Katia Sycara</li>
<li>for: 本研究旨在提高人工智能代理人在多智能环境中的社会智能，通过模拟他人的心理状态。</li>
<li>methods: 本研究使用深度网络模型策略，并将含义rich的信念嵌入策略中。然后，对每个代理人的信念预测能力作为多代理人学习的自适应奖励信号。</li>
<li>results: 在混合合作竞争环境中，该方法可以提高代理人之间的协作和竞争能力。<details>
<summary>Abstract</summary>
The ability to model the mental states of others is crucial to human social intelligence, and can offer similar benefits to artificial agents with respect to the social dynamics induced in multi-agent settings. We present a method of grounding semantically meaningful, human-interpretable beliefs within policies modeled by deep networks. We then consider the task of 2nd-order belief prediction. We propose that ability of each agent to predict the beliefs of the other agents can be used as an intrinsic reward signal for multi-agent reinforcement learning. Finally, we present preliminary empirical results in a mixed cooperative-competitive environment.
</details>
<details>
<summary>摘要</summary>
人类社交智能中能够模拟他人的心理状态是非常重要的，可以为人工智能agent提供类似的社交动力。我们提出了将 semantically meaningful和human-interpretable的beliefsgrounding在深度网络模型中的方法。然后我们考虑了第二个belief预测任务。我们认为每个agent可以预测其他agent的beliefs作为多 agent reinforcement learning中的内在奖励信号。最后，我们提供了一些初步的实验结果在混合合作-竞争环境中。Here's a word-for-word translation of the text:人类社交智能中能够模拟他人的心理状态是非常重要的，可以为人工智能agent提供类似的社交动力。我们提出了将semantically meaningful和human-interpretable的beliefsgrounding在深度网络模型中的方法。然后我们考虑了第二个belief预测任务。我们认为每个agent可以预测其他agent的beliefs作为多agent reinforcement learning中的内在奖励信号。最后，我们提供了一些初步的实验结果在混合合作-竞争环境中。
</details></li>
</ul>
<hr>
<h2 id="A-novel-approach-for-predicting-epidemiological-forecasting-parameters-based-on-real-time-signals-and-Data-Assimilation"><a href="#A-novel-approach-for-predicting-epidemiological-forecasting-parameters-based-on-real-time-signals-and-Data-Assimilation" class="headerlink" title="A novel approach for predicting epidemiological forecasting parameters based on real-time signals and Data Assimilation"></a>A novel approach for predicting epidemiological forecasting parameters based on real-time signals and Data Assimilation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01157">http://arxiv.org/abs/2307.01157</a></li>
<li>repo_url: None</li>
<li>paper_authors: Romain Molinas, César Quilodrán Casas, Rossella Arcucci, Ovidiu Şerban</li>
<li>for: 预测epidemiological参数，使用新的实时信号 integrate from various sources, such as social media-based population density maps and Air Quality data。</li>
<li>methods: 使用Convolutional Neural Networks (CNN) ensemble models and various data sources and fusion methodology to build robust predictions, and use data assimilation to estimate the state of the system from fused CNN predictions。</li>
<li>results: 提高了 COVID-19 疫情预测的性能和灵活性，并且比标准模型（SEIR）更高精度和更稳定。<details>
<summary>Abstract</summary>
This paper proposes a novel approach to predict epidemiological parameters by integrating new real-time signals from various sources of information, such as novel social media-based population density maps and Air Quality data. We implement an ensemble of Convolutional Neural Networks (CNN) models using various data sources and fusion methodology to build robust predictions and simulate several dynamic parameters that could improve the decision-making process for policymakers. Additionally, we used data assimilation to estimate the state of our system from fused CNN predictions. The combination of meteorological signals and social media-based population density maps improved the performance and flexibility of our prediction of the COVID-19 outbreak in London. While the proposed approach outperforms standard models, such as compartmental models traditionally used in disease forecasting (SEIR), generating robust and consistent predictions allows us to increase the stability of our model while increasing its accuracy.
</details>
<details>
<summary>摘要</summary>
本文提出了一种新的方法，通过将新的实时信号 integrate into various sources of information, such as social media-based population density maps and Air Quality data, to predict epidemiological parameters。我们使用了一个ensemble of Convolutional Neural Networks (CNN) models and various data sources and fusion methodology to build robust predictions and simulate several dynamic parameters that could improve the decision-making process for policymakers。此外，我们使用数据充满来估计系统状态的CNN预测结果。 combining meteorological signals and social media-based population density maps improved the performance and flexibility of our prediction of the COVID-19 outbreak in London。相比标准模型（如SEIR组件模型），我们的方法具有更高的稳定性和准确性。
</details></li>
</ul>
<hr>
<h2 id="AVSegFormer-Audio-Visual-Segmentation-with-Transformer"><a href="#AVSegFormer-Audio-Visual-Segmentation-with-Transformer" class="headerlink" title="AVSegFormer: Audio-Visual Segmentation with Transformer"></a>AVSegFormer: Audio-Visual Segmentation with Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01146">http://arxiv.org/abs/2307.01146</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vvvb-github/avsegformer">https://github.com/vvvb-github/avsegformer</a></li>
<li>paper_authors: Shengyi Gao, Zhe Chen, Guo Chen, Wenhai Wang, Tong Lu</li>
<li>for: 本研究旨在提出一种新的听视分割（AVS）任务，以解决在视频中找到并分割听起来的对象。</li>
<li>methods: 本文提出了一种基于transformer架构的AVSegFormer模型，通过引入听音查询和可学习查询，使网络可以 selectively 关注有趣的视觉特征。此外，我们还提出了一种听视混合器，可以动态调整视觉特征，并且设置了一个中间mask损失，以便更好地监督网络的预测。</li>
<li>results: 广泛的实验表明，AVSegFormer可以在AVS标准准样上取得状态的损失。网络代码可以在<a target="_blank" rel="noopener" href="https://github.com/vvvb-github/AVSegFormer%E4%B8%8A%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/vvvb-github/AVSegFormer上下载。</a><details>
<summary>Abstract</summary>
The combination of audio and vision has long been a topic of interest in the multi-modal community. Recently, a new audio-visual segmentation (AVS) task has been introduced, aiming to locate and segment the sounding objects in a given video. This task demands audio-driven pixel-level scene understanding for the first time, posing significant challenges. In this paper, we propose AVSegFormer, a novel framework for AVS tasks that leverages the transformer architecture. Specifically, we introduce audio queries and learnable queries into the transformer decoder, enabling the network to selectively attend to interested visual features. Besides, we present an audio-visual mixer, which can dynamically adjust visual features by amplifying relevant and suppressing irrelevant spatial channels. Additionally, we devise an intermediate mask loss to enhance the supervision of the decoder, encouraging the network to produce more accurate intermediate predictions. Extensive experiments demonstrate that AVSegFormer achieves state-of-the-art results on the AVS benchmark. The code is available at https://github.com/vvvb-github/AVSegFormer.
</details>
<details>
<summary>摘要</summary>
具有音频和视觉功能的组合已经是多Modal社区中的一个长期关注的话题。最近，一个新的音频视频分割（AVS）任务被引入，旨在在给定的视频中找到并分割声音的对象。这个任务要求音频驱动像素级场景理解，具有重大挑战。在这篇论文中，我们提出了AVSegFormer，一种新的AVS任务框架，利用转换架构。具体来说，我们引入了音频问题和学习问题到转换解码器中，使网络可以选择性地注意到有兴趣的视觉特征。此外，我们提出了一个音频视频混合器，可以动态调整视觉特征，增强有用的空间通道。此外，我们还提出了一个中间面 mask loss，以增强解码器的监督，让网络生成更加准确的中间预测。广泛的实验表明，AVSegFormer可以在AVS标准 bencmarks 上达到状态的最佳结果。代码可以在https://github.com/vvvb-github/AVSegFormer 中下载。
</details></li>
</ul>
<hr>
<h2 id="SCITUNE-Aligning-Large-Language-Models-with-Scientific-Multimodal-Instructions"><a href="#SCITUNE-Aligning-Large-Language-Models-with-Scientific-Multimodal-Instructions" class="headerlink" title="SCITUNE: Aligning Large Language Models with Scientific Multimodal Instructions"></a>SCITUNE: Aligning Large Language Models with Scientific Multimodal Instructions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01139">http://arxiv.org/abs/2307.01139</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lupantech/ScienceQA">https://github.com/lupantech/ScienceQA</a></li>
<li>paper_authors: Sameera Horawalavithana, Sai Munikoti, Ian Stewart, Henry Kvinge</li>
<li>for: 这个论文旨在提高大型语言模型（LLM）的能力，使其更好地遵循科学 Multimodal 指令。</li>
<li>methods: 这个论文提出了 SciTune 调教框架，用于改进 LLM 的科学 Multimodal 理解能力。其中使用了人类生成的科学指令调教数据集，并训练了一个包含视觉编码器和 LLM 的多Modal 模型 LLaMA-SciTune。</li>
<li>results: 对比机器生成数据只进行finetuning的模型，LLaMA-SciTune 在科学QA benchmark中平均和许多子类型的人工性能都高于人类性能。<details>
<summary>Abstract</summary>
Instruction finetuning is a popular paradigm to align large language models (LLM) with human intent. Despite its popularity, this idea is less explored in improving the LLMs to align existing foundation models with scientific disciplines, concepts and goals. In this work, we present SciTune as a tuning framework to improve the ability of LLMs to follow scientific multimodal instructions. To test our methodology, we use a human-generated scientific instruction tuning dataset and train a large multimodal model LLaMA-SciTune that connects a vision encoder and LLM for science-focused visual and language understanding. In comparison to the models that are finetuned with machine generated data only, LLaMA-SciTune surpasses human performance on average and in many sub-categories on the ScienceQA benchmark.
</details>
<details>
<summary>摘要</summary>
instruction fine-tuning是一种流行的思想，用于将大型语言模型（LLM）与人类意图进行对接。尽管这个想法在提高LLM对现有基础模型的适应性方面具有广泛的应用前景，但是它在科学领域中得到了更少的探索。在这项工作中，我们提出了SciTune作为一种调整框架，用于改进LLM对科学多Modal指令的遵循能力。为测试我们的方法，我们使用了人类生成的科学指令调整数据集，并训练了一个包含视觉编码器和LLM的科学频谱模型LLaMA-SciTune。与只使用机器生成的数据进行finetuning的模型相比，LLaMA-SciTune在科学问答 bencmark中平均和许多子类型上超越了人类性能。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://example.com/2023/07/04/cs.LG_2023_07_04/" data-id="cllsjvzbt0013f588hqlmdafv" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_07_04" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/04/cs.SD_2023_07_04/" class="article-date">
  <time datetime="2023-07-03T16:00:00.000Z" itemprop="datePublished">2023-07-04</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/04/cs.SD_2023_07_04/">cs.SD - 2023-07-04 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Disentanglement-in-a-GAN-for-Unconditional-Speech-Synthesis"><a href="#Disentanglement-in-a-GAN-for-Unconditional-Speech-Synthesis" class="headerlink" title="Disentanglement in a GAN for Unconditional Speech Synthesis"></a>Disentanglement in a GAN for Unconditional Speech Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01673">http://arxiv.org/abs/2307.01673</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rf5/simple-asgan">https://github.com/rf5/simple-asgan</a></li>
<li>paper_authors: Matthew Baas, Herman Kamper</li>
<li>for: This paper is written for unconditional speech synthesis, specifically to learn a disentangled latent space for speech synthesis.</li>
<li>methods: The paper proposes a generative adversarial network (GAN) called AudioStyleGAN (ASGAN), which is tailored to learn a disentangled latent space for speech synthesis. The ASGAN model builds upon the StyleGAN family of image synthesis models, and it uses a modified adaptation of adaptive discriminator augmentation to successfully train the model.</li>
<li>results: The paper achieves state-of-the-art results in unconditional speech synthesis on the small-vocabulary Google Speech Commands digits dataset, and it is substantially faster than existing top-performing diffusion models. The paper also demonstrates that the ASGAN model’s latent space is disentangled, and that simple linear operations in the space can be used to perform several tasks unseen during training, such as voice conversion, speech enhancement, speaker verification, and keyword classification.<details>
<summary>Abstract</summary>
Can we develop a model that can synthesize realistic speech directly from a latent space, without explicit conditioning? Despite several efforts over the last decade, previous adversarial and diffusion-based approaches still struggle to achieve this, even on small-vocabulary datasets. To address this, we propose AudioStyleGAN (ASGAN) -- a generative adversarial network for unconditional speech synthesis tailored to learn a disentangled latent space. Building upon the StyleGAN family of image synthesis models, ASGAN maps sampled noise to a disentangled latent vector which is then mapped to a sequence of audio features so that signal aliasing is suppressed at every layer. To successfully train ASGAN, we introduce a number of new techniques, including a modification to adaptive discriminator augmentation which probabilistically skips discriminator updates. We apply it on the small-vocabulary Google Speech Commands digits dataset, where it achieves state-of-the-art results in unconditional speech synthesis. It is also substantially faster than existing top-performing diffusion models. We confirm that ASGAN's latent space is disentangled: we demonstrate how simple linear operations in the space can be used to perform several tasks unseen during training. Specifically, we perform evaluations in voice conversion, speech enhancement, speaker verification, and keyword classification. Our work indicates that GANs are still highly competitive in the unconditional speech synthesis landscape, and that disentangled latent spaces can be used to aid generalization to unseen tasks. Code, models, samples: https://github.com/RF5/simple-asgan/
</details>
<details>
<summary>摘要</summary>
可以开发一个模型，将真实的语音直接从潜在空间 sintesize，无需显式的条件？过去十年多的尝试仍然无法完成这一点，即使在小词库dataset上。为解决这个问题，我们提出了AudioStyleGAN（ASGAN）——一个类型为生成对抗网络的构成，用于无条件语音合成。ASGAN使用抽象的随机变量，将样本的噪声映射到一个分离的潜在空间中，然后将这个空间映射到一系列的语音特征，以抑制信号扩散。为了成功地训练ASGAN，我们导入了一些新的技术，包括修改适应性的检测器更新，以及在检测器更新中 probabilistically skips 检测器更新。我们将其应用于 Google Speech Commands 小词库dataset，实现了无条件语音合成的州际级结果，并且比现有的扩散模型更快。我们还证明了 ASGAN 的潜在空间是分离的：我们显示了在训练中没有看到的任务上，可以使用简单的直线运算来完成多个任务。 Specifically, we perform evaluations in voice conversion, speech enhancement, speaker verification, and keyword classification. 我们的工作表明，GANs 在无条件语音合成领域仍然非常竞争，并且可以使用分离的潜在空间来帮助泛化到未见到的任务。codes, models, samples可以在 https://github.com/RF5/simple-asgan/ 上找到。
</details></li>
</ul>
<hr>
<h2 id="Pretraining-Conformer-with-ASR-or-ASV-for-Anti-Spoofing-Countermeasure"><a href="#Pretraining-Conformer-with-ASR-or-ASV-for-Anti-Spoofing-Countermeasure" class="headerlink" title="Pretraining Conformer with ASR or ASV for Anti-Spoofing Countermeasure"></a>Pretraining Conformer with ASR or ASV for Anti-Spoofing Countermeasure</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01546">http://arxiv.org/abs/2307.01546</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yikang Wang, Hiromitsu Nishizaki, Ming Li</li>
<li>for: 这个论文旨在提出一种基于Transformer的多级特征聚合嵌入器（MFA-Conformer）结构，用于音频反假护照（CM）。MFA-Conformer可以同时聚合全局和本地信息，从而帮助CM系统捕捉到假造的音频特征。</li>
<li>methods: 该论文提出了一种基于Conformer模型的转移学习方法，使得CM系统可以通过使用预训练的Conformer模型来增强其鲁棒性。此外，论文还提出了一种使用嵌入器融合多级特征的方法，以提高CM系统的抗误差性。</li>
<li>results: 实验结果表明，MFA-Conformer模型在清晰语音库（FAD）的清洁集上达到了0.038%的EER，远远超过了基eline。此外，转移学习方法在纯音频段上进行了有效的提升。<details>
<summary>Abstract</summary>
This paper introduces the Multi-scale Feature Aggregation Conformer (MFA-Conformer) structure for audio anti-spoofing countermeasure (CM). MFA-Conformer combines a convolutional neural networkbased on the Transformer, allowing it to aggregate global andlocal information. This may benefit the anti-spoofing CM system to capture the synthetic artifacts hidden both locally and globally. In addition, given the excellent performance of MFA Conformer on automatic speech recognition (ASR) and automatic speaker verification (ASV) tasks, we present a transfer learning method that utilizes pretrained Conformer models on ASR or ASV tasks to enhance the robustness of CM systems. The proposed method is evaluated on both Chinese and Englishs poofing detection databases. On the FAD clean set, the MFA-Conformer model pretrained on the ASR task achieves an EER of 0.038%, which dramatically outperforms the baseline. Moreover, experimental results demonstrate that proposed transfer learning method on Conformer is effective on pure speech segments after voice activity detection processing.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了一种基于Transformer的多级特征汇集声音防 spoofing countermeasure（MFA-Conformer）结构。MFA-Conformer结合了一个卷积神经网络，使其能够汇集全局和本地信息。这可能使防 spoofing CM 系统能够捕捉到假声音中的合成artefacts。此外，基于ASR和ASV任务的预训练Conformer模型的表现很出色，我们提出了一种在CM系统中使用这些预训练模型进行升级，以提高CM系统的Robustness。我们在中文和英文伪声检测数据库上评估了该方法。在FAD清洁集上，预训练MFA-Conformer模型在ASR任务上达到了EER值为0.038%，这在比基准值有很大的提升。此外，实验结果表明，在声音段后的语音活动检测处理后，提出的传输学习方法对Conformer是有效的。
</details></li>
</ul>
<hr>
<h2 id="Spatial-temporal-Graph-Based-Multi-channel-Speaker-Verification-With-Ad-hoc-Microphone-Arrays"><a href="#Spatial-temporal-Graph-Based-Multi-channel-Speaker-Verification-With-Ad-hoc-Microphone-Arrays" class="headerlink" title="Spatial-temporal Graph Based Multi-channel Speaker Verification With Ad-hoc Microphone Arrays"></a>Spatial-temporal Graph Based Multi-channel Speaker Verification With Ad-hoc Microphone Arrays</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01386">http://arxiv.org/abs/2307.01386</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yijiang Chen, Chengdong Liang, Xiao-Lei Zhang</li>
<li>for: 这篇论文目的是提高杂音环境中的多渠道话语识别率。</li>
<li>methods: 这篇论文使用的方法包括一个特性聚合块和一个通道选择块，两者都是基于图形。特性聚合块将不同时间和通道的话者特征聚合，使用空间时间图形对于多渠道话语识别。通道选择块则排除了可能对系统造成负面影响的杂音通道。</li>
<li>results: 实验结果显示，提案的方法与六种代表性方法相比，在实验数据中提供了15.39%的相对平均错误率（EER）下降，并在模拟数据中提供了17.70%的相对平均错误率下降。此外，其性能也具有不同讯号对比度和复响时间的Robustness。<details>
<summary>Abstract</summary>
The performance of speaker verification degrades significantly in adverse acoustic environments with strong reverberation and noise. To address this issue, this paper proposes a spatial-temporal graph convolutional network (GCN) method for the multi-channel speaker verification with ad-hoc microphone arrays. It includes a feature aggregation block and a channel selection block, both of which are built on graphs. The feature aggregation block fuses speaker features among different time and channels by a spatial-temporal GCN. The graph-based channel selection block discards the noisy channels that may contribute negatively to the system. The proposed method is flexible in incorporating various kinds of graphs and prior knowledge. We compared the proposed method with six representative methods in both real-world and simulated environments.   Experimental results show that the proposed method achieves a relative equal error rate (EER) reduction of $\mathbf{15.39\%}$ lower than the strongest referenced method in the simulated datasets, and $\mathbf{17.70\%}$ lower than the latter in the real datasets. Moreover, its performance is robust across different signal-to-noise ratios and reverberation time.
</details>
<details>
<summary>摘要</summary>
“对于具有强 reverberation 和噪声的恶劣类比测试环境， speaker verification 的性能会显著下降。为了解决这个问题，这篇论文提出了一个使用多条件参数的Graph Convolutional Network（GCN）方法，来进行多条件speaker verification。这个方法包括一个网格汇整块和一个网格选择块，它们都是基于图。网格汇整块会融合不同时间和通道的Speaker feature，通过对图进行汇整。网格基于的通道选择块将潜在干扰通道排除，以避免降低系统性能。提案的方法可以灵活地应用多种图和专业知识。我们与六种代表性方法进行比较，结果显示，提案的方法在实际和模拟环境中均有15.39%和17.70%的相对平均错误率（EER）下降，并且在不同的讯号载度和复合时间下保持稳定性。”
</details></li>
</ul>
<hr>
<h2 id="Semantic-enrichment-towards-efficient-speech-representations"><a href="#Semantic-enrichment-towards-efficient-speech-representations" class="headerlink" title="Semantic enrichment towards efficient speech representations"></a>Semantic enrichment towards efficient speech representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01323">http://arxiv.org/abs/2307.01323</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gaëlle Laperrière, Ha Nguyen, Sahar Ghannay, Bassam Jabaian, Yannick Estève</li>
<li>for: 本研究的目的是提高 spoken language understanding 任务中的 semantic extraction，并且考虑 computation costs。</li>
<li>methods: 本研究使用 SAMU-XLSR 模型，通过特有的域内semantic enrichment来增强多语言Speech representation。同时，我们还使用 same-domain French和Italian benchmarks 来提高 low-resource language 的可移植性，以及 explore cross-domain capacities of the enriched SAMU-XLSR。</li>
<li>results: 本研究表明，通过特有的域内semantic enrichment，可以提高 spoken language understanding 任务中的 semantic extraction性能，同时也可以提高 low-resource language 的可移植性。<details>
<summary>Abstract</summary>
Over the past few years, self-supervised learned speech representations have emerged as fruitful replacements for conventional surface representations when solving Spoken Language Understanding (SLU) tasks. Simultaneously, multilingual models trained on massive textual data were introduced to encode language agnostic semantics. Recently, the SAMU-XLSR approach introduced a way to make profit from such textual models to enrich multilingual speech representations with language agnostic semantics. By aiming for better semantic extraction on a challenging Spoken Language Understanding task and in consideration with computation costs, this study investigates a specific in-domain semantic enrichment of the SAMU-XLSR model by specializing it on a small amount of transcribed data from the downstream task. In addition, we show the benefits of the use of same-domain French and Italian benchmarks for low-resource language portability and explore cross-domain capacities of the enriched SAMU-XLSR.
</details>
<details>
<summary>摘要</summary>
在过去几年，自我超级学习的发音表示 emerged 作为解决 spoken language understanding (SLU) 任务的有用替代方案。同时，基于庞大文本数据的多语言模型被引入，以编码语言不受限制的 semantics。最近，SAMU-XLSR 方法引入了使用文本模型增强多语言speech表示的方法。通过寻求在具有挑战性的 SLU 任务中更好地EXTRACT semantic information和考虑计算成本，本研究探讨了特定领域内Semantic enhancement of the SAMU-XLSR model by specializing it on a small amount of transcribed data from the downstream task。此外，我们还展示了使用 same-domain French and Italian benchmarks 的低资源语言可移植性和跨领域 capacities of the enriched SAMU-XLSR.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://example.com/2023/07/04/cs.SD_2023_07_04/" data-id="cllsjvzcf0034f588gi5v0rck" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_07_04" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/04/eess.IV_2023_07_04/" class="article-date">
  <time datetime="2023-07-03T16:00:00.000Z" itemprop="datePublished">2023-07-04</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/04/eess.IV_2023_07_04/">eess.IV - 2023-07-04 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Mitigating-Calibration-Bias-Without-Fixed-Attribute-Grouping-for-Improved-Fairness-in-Medical-Imaging-Analysis"><a href="#Mitigating-Calibration-Bias-Without-Fixed-Attribute-Grouping-for-Improved-Fairness-in-Medical-Imaging-Analysis" class="headerlink" title="Mitigating Calibration Bias Without Fixed Attribute Grouping for Improved Fairness in Medical Imaging Analysis"></a>Mitigating Calibration Bias Without Fixed Attribute Grouping for Improved Fairness in Medical Imaging Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01738">http://arxiv.org/abs/2307.01738</a></li>
<li>repo_url: None</li>
<li>paper_authors: Changjian Shui, Justin Szeto, Raghav Mehta, Douglas L. Arnold, Tal Arbel</li>
<li>for: 避免深度学习医学图像分析模型在实际临床中的不可靠部署，需要进行准确性调整。</li>
<li>methods: 我们提出了一种两阶段方法：集群焦点法，首先标识不准确的样本，将其分为组，然后引入组织损失来改善准确性偏见。</li>
<li>results: 我们在皮肤病分类HAM10000 dataset和多发性硬化病患者未来病变预测 task 上进行了评估，结果表明，我们的方法可以有效控制最差表现 subgroup 的准确性错误，同时保持预测性能，并超越最近的基elines。<details>
<summary>Abstract</summary>
Trustworthy deployment of deep learning medical imaging models into real-world clinical practice requires that they be calibrated. However, models that are well calibrated overall can still be poorly calibrated for a sub-population, potentially resulting in a clinician unwittingly making poor decisions for this group based on the recommendations of the model. Although methods have been shown to successfully mitigate biases across subgroups in terms of model accuracy, this work focuses on the open problem of mitigating calibration biases in the context of medical image analysis. Our method does not require subgroup attributes during training, permitting the flexibility to mitigate biases for different choices of sensitive attributes without re-training. To this end, we propose a novel two-stage method: Cluster-Focal to first identify poorly calibrated samples, cluster them into groups, and then introduce group-wise focal loss to improve calibration bias. We evaluate our method on skin lesion classification with the public HAM10000 dataset, and on predicting future lesional activity for multiple sclerosis (MS) patients. In addition to considering traditional sensitive attributes (e.g. age, sex) with demographic subgroups, we also consider biases among groups with different image-derived attributes, such as lesion load, which are required in medical image analysis. Our results demonstrate that our method effectively controls calibration error in the worst-performing subgroups while preserving prediction performance, and outperforming recent baselines.
</details>
<details>
<summary>摘要</summary>
信任性的深入学习医学影像模型在实际临床应用中需要进行准确化。然而，即使模型在整体上具有良好的准确性，也可能对一个子群体存在不良准确性，导致医生不知道基于模型的建议而做出不良决策。虽然有些方法可以成功地消除 subgroup 的偏见，但这项工作将关注在医学影像分析中的开放问题上，即如何消除准确性偏见。我们的方法不需要在训练过程中提供 subgroup 特征，因此可以随意地消除不同敏感特征的偏见。为此，我们提出了一种新的两stage方法：集群焦点法。首先，我们将准确性不佳的样本分成集群，然后引入集群级别的焦点损失来改善准确性偏见。我们在皮肤病分类 task 和多发性精神病（MS）患者未来病变预测任务上进行了评估。除了考虑传统的敏感特征（例如年龄、性别）与人口 subgroup 之外，我们还考虑了医学影像分析中的不同图像特征，如病虫荷载，这些特征是必需的。我们的结果表明，我们的方法可以有效地控制最差 subgroup 的准确性错误，保持预测性能，并超越最新的基eline。
</details></li>
</ul>
<hr>
<h2 id="Once-Training-All-Fine-No-Reference-Point-Cloud-Quality-Assessment-via-Domain-relevance-Degradation-Description"><a href="#Once-Training-All-Fine-No-Reference-Point-Cloud-Quality-Assessment-via-Domain-relevance-Degradation-Description" class="headerlink" title="Once-Training-All-Fine: No-Reference Point Cloud Quality Assessment via Domain-relevance Degradation Description"></a>Once-Training-All-Fine: No-Reference Point Cloud Quality Assessment via Domain-relevance Degradation Description</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01567">http://arxiv.org/abs/2307.01567</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yipeng Liu, Qi Yang, Yujie Zhang, Yiling Xu, Le Yang, Xiaozhong Xu, Shan Liu</li>
<li>for: 提高无参考点云质量评估（NR-PCQA）方法的一般化性能。</li>
<li>methods: 提出一种基于域相关性的NR-PCQA方法，包括解释性模型、域转换和Semantic explanation。</li>
<li>results: 实验结果表明，提出的D$^3$-PCQA方法在多个公开数据集上 exhibits 强大的一般化能力和 robust性。<details>
<summary>Abstract</summary>
Full-reference (FR) point cloud quality assessment (PCQA) has achieved impressive progress in recent years. However, as reference point clouds are not available in many cases, no-reference (NR) metrics have become a research hotspot. Existing NR methods suffer from poor generalization performance. To address this shortcoming, we propose a novel NR-PCQA method, Point Cloud Quality Assessment via Domain-relevance Degradation Description (D$^3$-PCQA). First, we demonstrate our model's interpretability by deriving the function of each module using a kernelized ridge regression model. Specifically, quality assessment can be characterized as a leap from the scattered perceptual domain (reflecting subjective perception) to the ordered quality domain (reflecting mean opinion score). Second, to reduce the significant domain discrepancy, we establish an intermediate domain, the description domain, based on insights from subjective experiments, by considering the domain relevance among samples located in the perception domain and learning a structured latent space. The anchor features derived from the learned latent space are generated as cross-domain auxiliary information to promote domain transformation. Furthermore, the newly established description domain decomposes the NR-PCQA problem into two relevant stages. These stages include a classification stage that gives the degradation descriptions to point clouds and a regression stage to determine the confidence degrees of descriptions, providing a semantic explanation for the predicted quality scores. Experimental results demonstrate that D$^3$-PCQA exhibits robust performance and outstanding generalization ability on several publicly available datasets. The code in this work will be publicly available at https://smt.sjtu.edu.cn.
</details>
<details>
<summary>摘要</summary>
Full-reference (FR) 点云质量评估 (PCQA) 在最近几年内取得了显著的进步。然而，由于参考点云不常可用，无参考 (NR) 指标成为了研究热点。现有的 NR 方法受到质量评估的泛化性能的限制。为了解决这一缺点，我们提出了一种新的 NR-PCQA 方法，即 Point Cloud Quality Assessment via Domain-relevance Degradation Description (D$^3$-PCQA)。首先，我们证明了我们的模型的可解释性，通过使用kernelized ridge regression模型来 derivate每个模块的函数。具体来说，质量评估可以被描述为从杂乱的感知领域（反映主观感受）到有序的质量领域（反映意见票）的跳跃。其次，为了减少域外差，我们建立了一个中间域，即描述域，基于对主观实验所获得的域相关性的思考。通过学习协同的秘密空间，我们生成了跨域的帮助信息，以便进行域转换。此外，我们新建立的描述域将 NR-PCQA 问题分解成两个相关的阶段。这两个阶段分别是用来给点云的质量描述和确定描述的可信度的阶段，从而提供了 semantics 的解释。实验结果表明，D$^3$-PCQA 具有出色的 Robustness 和泛化能力，在多个公开可用的数据集上达到了优秀的性能。我们将在https://smt.sjtu.edu.cn上公开代码。
</details></li>
</ul>
<hr>
<h2 id="Spatio-Temporal-Perception-Distortion-Trade-off-in-Learned-Video-SR"><a href="#Spatio-Temporal-Perception-Distortion-Trade-off-in-Learned-Video-SR" class="headerlink" title="Spatio-Temporal Perception-Distortion Trade-off in Learned Video SR"></a>Spatio-Temporal Perception-Distortion Trade-off in Learned Video SR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01556">http://arxiv.org/abs/2307.01556</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kuis-ai-tekalp-research-group/perceptual-vsr">https://github.com/kuis-ai-tekalp-research-group/perceptual-vsr</a></li>
<li>paper_authors: Nasrin Rahimi, A. Murat Tekalp</li>
<li>for: 这个论文旨在探讨视频超解像的准确性评价方法，尤其是考虑视频中的运动流动性。</li>
<li>methods: 该论文提出了一种新的视频质量评价指标，强调视频中的自然运动流动性，并提出了一种基于这个指标的视频超解像框架（PSVR）。</li>
<li>results: 实验结果表明，该论文提出的评价指标和框架可以更好地评价视频超解像的准确性，并且支持假设，即视频准确性评价应该考虑运动流动性的自然性。<details>
<summary>Abstract</summary>
Perception-distortion trade-off is well-understood for single-image super-resolution. However, its extension to video super-resolution (VSR) is not straightforward, since popular perceptual measures only evaluate naturalness of spatial textures and do not take naturalness of flow (temporal coherence) into account. To this effect, we propose a new measure of spatio-temporal perceptual video quality emphasizing naturalness of optical flow via the perceptual straightness hypothesis (PSH) for meaningful spatio-temporal perception-distortion trade-off. We also propose a new architecture for perceptual VSR (PSVR) to explicitly enforce naturalness of flow to achieve realistic spatio-temporal perception-distortion trade-off according to the proposed measures. Experimental results with PVSR support the hypothesis that a meaningful perception-distortion tradeoff for video should account for the naturalness of motion in addition to naturalness of texture.
</details>
<details>
<summary>摘要</summary>
文本扭曲质量评估对单张超高清图像处理well understood,但是扩展到视频超高清图像（VSR）并不直接，因为流行的感知度量只评估自然性的空间纹理，而不考虑流动的自然性（时间准确性）。为此，我们提出了一种新的spatio-temporal感知质量指标，强调流动的自然性via the perceptual straightness hypothesis（PSH），以实现有意义的spatio-temporal扭曲质量评估。我们还提出了一种新的PSVR架构，以直接强制实现流动的自然性，以达到实际的spatio-temporal扭曲质量评估。实验结果表示，在PVSR中，一个有意义的扭曲质量评估应该考虑流动的自然性，不仅是空间纹理的自然性。
</details></li>
</ul>
<hr>
<h2 id="Convolutional-Transformer-for-Autonomous-Recognition-and-Grading-of-Tomatoes-Under-Various-Lighting-Occlusion-and-Ripeness-Conditions"><a href="#Convolutional-Transformer-for-Autonomous-Recognition-and-Grading-of-Tomatoes-Under-Various-Lighting-Occlusion-and-Ripeness-Conditions" class="headerlink" title="Convolutional Transformer for Autonomous Recognition and Grading of Tomatoes Under Various Lighting, Occlusion, and Ripeness Conditions"></a>Convolutional Transformer for Autonomous Recognition and Grading of Tomatoes Under Various Lighting, Occlusion, and Ripeness Conditions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01530">http://arxiv.org/abs/2307.01530</a></li>
<li>repo_url: None</li>
<li>paper_authors: Asim Khan, Taimur Hassan, Muhammad Shafay, Israa Fahmy, Naoufel Werghi, Lakmal Seneviratne, Irfan Hussain</li>
<li>for: 本研究旨在开发一种自主识别和评估 Tomatoes 的框架，以便在实际场景中使用移动机器人收割 Tomatoes。</li>
<li>methods: 本研究使用一种卷积变换器架构，通过自动识别和评估 Tomatoes，缓解因为叶子和枝条等因素而导致的 occlusion 问题。</li>
<li>results: 经过训练和测试，提出的方法在不同的照明条件和观察角度下，对 Tomatoes 的识别和评估表现出色，比基eline方法和先前方法高出58.14%、65.42% 和 66.39% 的mean average precision 分数。<details>
<summary>Abstract</summary>
Harvesting fully ripe tomatoes with mobile robots presents significant challenges in real-world scenarios. These challenges arise from factors such as occlusion caused by leaves and branches, as well as the color similarity between tomatoes and the surrounding foliage during the fruit development stage. The natural environment further compounds these issues with varying light conditions, viewing angles, occlusion factors, and different maturity levels. To overcome these obstacles, this research introduces a novel framework that leverages a convolutional transformer architecture to autonomously recognize and grade tomatoes, irrespective of their occlusion level, lighting conditions, and ripeness. The proposed model is trained and tested using carefully annotated images curated specifically for this purpose. The dataset is prepared under various lighting conditions, viewing perspectives, and employs different mobile camera sensors, distinguishing it from existing datasets such as Laboro Tomato and Rob2Pheno Annotated Tomato. The effectiveness of the proposed framework in handling cluttered and occluded tomato instances was evaluated using two additional public datasets, Laboro Tomato and Rob2Pheno Annotated Tomato, as benchmarks. The evaluation results across these three datasets demonstrate the exceptional performance of our proposed framework, surpassing the state-of-the-art by 58.14%, 65.42%, and 66.39% in terms of mean average precision scores for KUTomaData, Laboro Tomato, and Rob2Pheno Annotated Tomato, respectively. The results underscore the superiority of the proposed model in accurately detecting and delineating tomatoes compared to baseline methods and previous approaches. Specifically, the model achieves an F1-score of 80.14%, a Dice coefficient of 73.26%, and a mean IoU of 66.41% on the KUTomaData image dataset.
</details>
<details>
<summary>摘要</summary>
采收完全熟 Tomatoes 的 mobile robot 存在许多实际应用中的挑战。这些挑战包括由叶子和枝头所引起的遮掩、 Tomatoes 和周围的植物发育阶段的颜色相似性，以及自然环境中的不同光照条件、观察角度和不同熟度水平。为了解决这些问题，这项研究提出了一个新的框架，利用卷积变换器架构来自动识别和分级 Tomatoes，无论它们的遮掩水平、光照条件和熟度如何。该提案的模型被训练和测试使用特意为这项研究制作的注意词汇图像集。该数据集在不同的光照条件下、不同的观察角度下和使用不同的移动摄像头感知器时被准备。与现有的数据集不同，这个数据集不仅包括不同的光照条件和观察角度，还使用了不同的移动摄像头感知器。为了评估该提案的效果，研究者们使用了另外两个公共数据集作为参照，即 Laboro Tomato 和 Rob2Pheno Annotated Tomato。结果表明，该提案的模型在处理受遮掩和受遮掩的 Tomatoes 实例时表现出色，与基准方法和先前的方法相比，提高了58.14%、65.42%和66.39%的平均准确率。这些结果表明，该模型在识别和定义 Tomatoes 方面具有出色的性能，而不是基准方法和先前的方法。具体来说，模型在 KUTomaData 图像集上 achieve 的 F1 分数为 80.14%，Dice 系数为 73.26%，和 Mean IoU 为 66.41%。
</details></li>
</ul>
<hr>
<h2 id="H-DenseFormer-An-Efficient-Hybrid-Densely-Connected-Transformer-for-Multimodal-Tumor-Segmentation"><a href="#H-DenseFormer-An-Efficient-Hybrid-Densely-Connected-Transformer-for-Multimodal-Tumor-Segmentation" class="headerlink" title="H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation"></a>H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01486">http://arxiv.org/abs/2307.01486</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shijun18/h-denseformer">https://github.com/shijun18/h-denseformer</a></li>
<li>paper_authors: Jun Shi, Hongyu Kan, Shulan Ruan, Ziqi Zhu, Minfan Zhao, Liang Qiao, Zhaohui Wang, Hong An, Xudong Xue</li>
<li>for: 这篇论文是针对多Modal的医疗影像肿瘤分类问题提出的一个新方法。</li>
<li>methods: 本文提出了一个混合了CNN和Transformer结构的对称网络，名为H-DenseFormer，它可以将多Modal的输入转换为融合特征，并将这些融合特征传递到不同层次的Encoder中进行增强多Modal学习表现。此外，本文还提出了一个轻量级的DCT块来取代标准Transformer块，以减少计算复杂度。</li>
<li>results: 在两个公共的多Modal数据集上进行了广泛的实验，结果显示了我们的提案方法在与现有的State-of-the-art方法进行比较时，具有更好的表现，同时计算复杂度较低。<details>
<summary>Abstract</summary>
Recently, deep learning methods have been widely used for tumor segmentation of multimodal medical images with promising results. However, most existing methods are limited by insufficient representational ability, specific modality number and high computational complexity. In this paper, we propose a hybrid densely connected network for tumor segmentation, named H-DenseFormer, which combines the representational power of the Convolutional Neural Network (CNN) and the Transformer structures. Specifically, H-DenseFormer integrates a Transformer-based Multi-path Parallel Embedding (MPE) module that can take an arbitrary number of modalities as input to extract the fusion features from different modalities. Then, the multimodal fusion features are delivered to different levels of the encoder to enhance multimodal learning representation. Besides, we design a lightweight Densely Connected Transformer (DCT) block to replace the standard Transformer block, thus significantly reducing computational complexity. We conduct extensive experiments on two public multimodal datasets, HECKTOR21 and PI-CAI22. The experimental results show that our proposed method outperforms the existing state-of-the-art methods while having lower computational complexity. The source code is available at https://github.com/shijun18/H-DenseFormer.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:近期，深度学习方法在多Modal医疗影像肿瘤分割领域得到了广泛应用，并取得了Promising的结果。然而，大多数现有方法受到不充分的表达能力、特定的Modal数量和高计算复杂性的限制。在本文中，我们提出了一种混合密集连接网络，名为H-DenseFormer，它结合了Convolutional Neural Network (CNN)和Transformer结构的表达力。具体来说，H-DenseFormer integrate了一个基于Transformer的多路平行嵌入（MPE）模块，可以将多Modal的输入作为输入，以提取不同Modal的融合特征。然后，这些融合特征被传递到不同层的编码器，以增强多Modal学习表达。此外，我们设计了一个轻量级的Densely Connected Transformer（DCT）块，以取代标准Transformer块，从而显著降低计算复杂性。我们在HECKTOR21和PI-CAI22两个公共多Modal数据集上进行了广泛的实验。实验结果表明，我们提出的方法可以比现有的状态级方法更高效，同时计算复杂性也更低。源代码可以在https://github.com/shijun18/H-DenseFormer上获取。
</details></li>
</ul>
<hr>
<h2 id="Zero-DeepSub-Zero-Shot-Deep-Subspace-Reconstruction-for-Rapid-Multiparametric-Quantitative-MRI-Using-3D-QALAS"><a href="#Zero-DeepSub-Zero-Shot-Deep-Subspace-Reconstruction-for-Rapid-Multiparametric-Quantitative-MRI-Using-3D-QALAS" class="headerlink" title="Zero-DeepSub: Zero-Shot Deep Subspace Reconstruction for Rapid Multiparametric Quantitative MRI Using 3D-QALAS"></a>Zero-DeepSub: Zero-Shot Deep Subspace Reconstruction for Rapid Multiparametric Quantitative MRI Using 3D-QALAS</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01410">http://arxiv.org/abs/2307.01410</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yohan Jun, Yamin Arefeen, Jaejin Cho, Shohei Fujita, Xiaoqing Wang, P. Ellen Grant, Borjan Gagoski, Camilo Jaimes, Michael S. Gee, Berkin Bilgic</li>
<li>for: develop and evaluate methods for reconstructing 3D-quantification using an interleaved Look-Locker acquisition sequence with T2 preparation pulse (3D-QALAS) time-series images</li>
<li>methods: using a low-rank subspace method and zero-shot deep-learning subspace method (Zero-DeepSub) for rapid and high fidelity T1 and T2 mapping and time-resolved imaging</li>
<li>results: good linearity and reduced biases compared to conventional QALAS, better g-factor maps and reduced voxel blurring, noise, and artifacts compared to conventional QALAS, and robust performance at up to 9-fold acceleration with Zero-DeepSub enabled whole-brain T1, T2, and PD mapping at 1 mm isotropic resolution within 2 min of scan time.Here’s the format you requested:</li>
<li>for: develop and evaluate methods for 3D-quantification using 3D-QALAS time-series images</li>
<li>methods: using low-rank subspace method and Zero-DeepSub</li>
<li>results: good linearity, reduced biases, better g-factor maps, and reduced voxel blurring, noise, and artifacts, and robust performance at up to 9-fold acceleration<details>
<summary>Abstract</summary>
Purpose: To develop and evaluate methods for 1) reconstructing 3D-quantification using an interleaved Look-Locker acquisition sequence with T2 preparation pulse (3D-QALAS) time-series images using a low-rank subspace method, which enables accurate and rapid T1 and T2 mapping, and 2) improving the fidelity of subspace QALAS by combining scan-specific deep-learning-based reconstruction and subspace modeling. Methods: A low-rank subspace method for 3D-QALAS (i.e., subspace QALAS) and zero-shot deep-learning subspace method (i.e., Zero-DeepSub) were proposed for rapid and high fidelity T1 and T2 mapping and time-resolved imaging using 3D-QALAS. Using an ISMRM/NIST system phantom, the accuracy of the T1 and T2 maps estimated using the proposed methods was evaluated by comparing them with reference techniques. The reconstruction performance of the proposed subspace QALAS using Zero-DeepSub was evaluated in vivo and compared with conventional QALAS at high reduction factors of up to 9-fold. Results: Phantom experiments showed that subspace QALAS had good linearity with respect to the reference methods while reducing biases compared to conventional QALAS, especially for T2 maps. Moreover, in vivo results demonstrated that subspace QALAS had better g-factor maps and could reduce voxel blurring, noise, and artifacts compared to conventional QALAS and showed robust performance at up to 9-fold acceleration with Zero-DeepSub, which enabled whole-brain T1, T2, and PD mapping at 1 mm isotropic resolution within 2 min of scan time. Conclusion: The proposed subspace QALAS along with Zero-DeepSub enabled high fidelity and rapid whole-brain multiparametric quantification and time-resolved imaging.
</details>
<details>
<summary>摘要</summary>
目的：开发和评估使用排序 Look-Locker 类型的三维量化（3D-QALAS）时间序列图像的重要方法，以实现精确和快速的 T1 和 T2 地图的构建，并且提高 subspace QALAS 的实用性。方法：提出了一种基于低维度的 subspace QALAS 方法和 zero-shot 深度学习 subspace 方法（Zero-DeepSub），用于快速和高实用性的 T1 和 T2 地图和时间分辨图像的重建。使用 ISMRM/NIST 系统实验库中的实验库，评估了提案方法中的 T1 和 T2 地图的准确性，并与参考方法进行比较。结果：实验结果显示，subspace QALAS 具有对于参考方法的良好线性性，而且可以降低 conventional QALAS 中的偏差，特别是 T2 地图。此外，在 vivo 中的结果显示，subspace QALAS 可以提供更好的 g-因素地图，并且可以降低像素模糊、噪音和错误，并且在 Zero-DeepSub 的支持下，可以在 9 倍的压缩因子下进行快速的构建。结论：提案的 subspace QALAS 和 Zero-DeepSub 可以实现高实用性和快速的全脑多 parametr 量化和时间分辨图像。
</details></li>
</ul>
<hr>
<h2 id="A-CNN-regression-model-to-estimate-buildings-height-maps-using-Sentinel-1-SAR-and-Sentinel-2-MSI-time-series"><a href="#A-CNN-regression-model-to-estimate-buildings-height-maps-using-Sentinel-1-SAR-and-Sentinel-2-MSI-time-series" class="headerlink" title="A CNN regression model to estimate buildings height maps using Sentinel-1 SAR and Sentinel-2 MSI time series"></a>A CNN regression model to estimate buildings height maps using Sentinel-1 SAR and Sentinel-2 MSI time series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01378">http://arxiv.org/abs/2307.01378</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ritu Yadav, Andrea Nascetti, Yifang Ban</li>
<li>for: 这个研究旨在提出一个监督学习的多modal建筑高度回溯网络（MBHR-Net），用于以10米间隔估计建筑高度使用快射-1（S1）和快射-2（S2）卫星时间序列。</li>
<li>methods: 这个研究使用了S1提供的Synthetic Aperture Radar（SAR）数据，以及S2提供的多spectral数据，并使用深度学习模型将这两种数据融合以学习复杂的空间-时间关系。</li>
<li>results: 这个研究的初步结果显示MBHR-Net可以实现高度精准的估计（3.73米RMSE、0.95 IoU、0.61 R2），表明这个深度学习模型具有实用的应用前景，包括城市规划、环境影响分析等。<details>
<summary>Abstract</summary>
Accurate estimation of building heights is essential for urban planning, infrastructure management, and environmental analysis. In this study, we propose a supervised Multimodal Building Height Regression Network (MBHR-Net) for estimating building heights at 10m spatial resolution using Sentinel-1 (S1) and Sentinel-2 (S2) satellite time series. S1 provides Synthetic Aperture Radar (SAR) data that offers valuable information on building structures, while S2 provides multispectral data that is sensitive to different land cover types, vegetation phenology, and building shadows. Our MBHR-Net aims to extract meaningful features from the S1 and S2 images to learn complex spatio-temporal relationships between image patterns and building heights. The model is trained and tested in 10 cities in the Netherlands. Root Mean Squared Error (RMSE), Intersection over Union (IOU), and R-squared (R2) score metrics are used to evaluate the performance of the model. The preliminary results (3.73m RMSE, 0.95 IoU, 0.61 R2) demonstrate the effectiveness of our deep learning model in accurately estimating building heights, showcasing its potential for urban planning, environmental impact analysis, and other related applications.
</details>
<details>
<summary>摘要</summary>
准确估算建筑高度是城市规划、基础设施管理和环境分析中非常重要的。在本研究中，我们提出了一种监督式多模态建筑高度回归网络（MBHR-Net），用于使用 Sentinal-1（S1）和 Sentinal-2（S2）卫星时序序数据来估算建筑高度的10米空间分辨率。S1提供Synthetic Aperture Radar（SAR）数据，可以提供建筑结构的有价信息，而S2提供多spectral数据，敏感于不同的地表类型、植被生长阶段和建筑阴影。我们的MBHR-Net试图从S1和S2图像中提取有用的特征，以学习图像模式和建筑高度之间的复杂空间时间关系。模型在荷兰10座城市进行训练和测试。使用Root Mean Squared Error（RMSE）、Intersection over Union（IOU）和R-squared（R2） метри来评估模型的性能。初步结果（3.73米RMSE、0.95 IoU、0.61 R2）表明我们的深度学习模型可以准确地估算建筑高度，展示其在城市规划、环境影响分析等相关应用中的潜力。
</details></li>
</ul>
<hr>
<h2 id="Investigating-Data-Memorization-in-3D-Latent-Diffusion-Models-for-Medical-Image-Synthesis"><a href="#Investigating-Data-Memorization-in-3D-Latent-Diffusion-Models-for-Medical-Image-Synthesis" class="headerlink" title="Investigating Data Memorization in 3D Latent Diffusion Models for Medical Image Synthesis"></a>Investigating Data Memorization in 3D Latent Diffusion Models for Medical Image Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01148">http://arxiv.org/abs/2307.01148</a></li>
<li>repo_url: None</li>
<li>paper_authors: Salman Ul Hassan Dar, Arman Ghanaat, Jannik Kahmann, Isabelle Ayx, Theano Papavassiliu, Stefan O. Schoenberg, Sandy Engelhardt</li>
<li>for: 这个论文的目的是评估三维潜在扩散模型在生成医疗数据方面的能力。</li>
<li>methods: 该论文使用了自我超vised模型基于对比学习来检测潜在的记忆效应。</li>
<li>results: 研究结果表明，这些潜在扩散模型确实会记忆训练数据，需要采取措施来缓解这种记忆效应。<details>
<summary>Abstract</summary>
Generative latent diffusion models have been established as state-of-the-art in data generation. One promising application is generation of realistic synthetic medical imaging data for open data sharing without compromising patient privacy. Despite the promise, the capacity of such models to memorize sensitive patient training data and synthesize samples showing high resemblance to training data samples is relatively unexplored. Here, we assess the memorization capacity of 3D latent diffusion models on photon-counting coronary computed tomography angiography and knee magnetic resonance imaging datasets. To detect potential memorization of training samples, we utilize self-supervised models based on contrastive learning. Our results suggest that such latent diffusion models indeed memorize training data, and there is a dire need for devising strategies to mitigate memorization.
</details>
<details>
<summary>摘要</summary>
文本翻译为简化字Simplified Chinese。<</SYS>>生成式潜在扩散模型已成为数据生成领域的状态机。一个有前途的应用是生成真实的医疗数据，以便在开放数据分享无需妥协病人隐私。虽然有承诺，但是这些模型对敏感病人训练数据的记忆能力和生成样本高度相似的样本的 sintesis能力尚未得到充分探讨。我们在 photon-counting coronary computed tomography angiography和 knee magnetic resonance imaging 数据集上评估了3D潜在扩散模型的记忆能力。为检测可能的记忆 Training samples，我们利用了自我超VI的 contrastive learning。我们的结果表明，这些潜在扩散模型确实记忆训练数据，而需要采取措施来缓解记忆。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://example.com/2023/07/04/eess.IV_2023_07_04/" data-id="cllsjvzdu0076f588dpe88w93" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_07_03" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/03/cs.LG_2023_07_03/" class="article-date">
  <time datetime="2023-07-02T16:00:00.000Z" itemprop="datePublished">2023-07-03</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/03/cs.LG_2023_07_03/">cs.LG - 2023-07-03 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Sampling-the-lattice-Nambu-Goto-string-using-Continuous-Normalizing-Flows"><a href="#Sampling-the-lattice-Nambu-Goto-string-using-Continuous-Normalizing-Flows" class="headerlink" title="Sampling the lattice Nambu-Goto string using Continuous Normalizing Flows"></a>Sampling the lattice Nambu-Goto string using Continuous Normalizing Flows</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01107">http://arxiv.org/abs/2307.01107</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/turinlatticefieldtheorygroup/nambugotocnf">https://github.com/turinlatticefieldtheorygroup/nambugotocnf</a></li>
<li>paper_authors: Michele Caselle, Elia Cellini, Alessandro Nada</li>
<li>for: 这篇论文是为了描述偏微场论中的粘合现象，使用紧缩串理论（EST）来描述粘合频谱管的模型。</li>
<li>methods: 这篇论文使用了机器学习方法，特别是最新的Continuous Normalizing Flows（CNF）来解决EST预测的计算问题。</li>
<li>results: 该论文使用CNF方法对Nambu-Goto strings进行了数值计算，并获得了可靠的EST预测值。<details>
<summary>Abstract</summary>
Effective String Theory (EST) represents a powerful non-perturbative approach to describe confinement in Yang-Mills theory that models the confining flux tube as a thin vibrating string. EST calculations are usually performed using the zeta-function regularization: however there are situations (for instance the study of the shape of the flux tube or of the higher order corrections beyond the Nambu-Goto EST) which involve observables that are too complex to be addressed in this way. In this paper we propose a numerical approach based on recent advances in machine learning methods to circumvent this problem. Using as a laboratory the Nambu-Goto string, we show that by using a new class of deep generative models called Continuous Normalizing Flows it is possible to obtain reliable numerical estimates of EST predictions.
</details>
<details>
<summary>摘要</summary>
效果串理论（EST）表示一种强大的非拟合方法来描述 Yang-Mills 理论中的吸引作用，将吸引 flux tube 模型为细膨散的弹性String。EST 计算通常使用 zeta-function 正则化：但有些情况（如研究 flux tube 的形状或高阶修正项）需要访问 Observables 是不可能通过这种方式进行处理。在这篇论文中，我们提出一种基于最近的机器学习方法的数字方法来解决这个问题。使用 Nambu-Goto  string 作为实验室，我们示出了使用一种新的深度生成模型called Continuous Normalizing Flows 可以获得可靠的 EST 预测。
</details></li>
</ul>
<hr>
<h2 id="Streamlined-Lensed-Quasar-Identification-in-Multiband-Images-via-Ensemble-Networks"><a href="#Streamlined-Lensed-Quasar-Identification-in-Multiband-Images-via-Ensemble-Networks" class="headerlink" title="Streamlined Lensed Quasar Identification in Multiband Images via Ensemble Networks"></a>Streamlined Lensed Quasar Identification in Multiband Images via Ensemble Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01090">http://arxiv.org/abs/2307.01090</a></li>
<li>repo_url: None</li>
<li>paper_authors: Irham Taufik Andika, Sherry H. Suyu, Raoul Cañameras, Alejandra Melo, Stefan Schuldt, Yiping Shu, Anna-Christina Eilers, Anton Timur Jaelani, Minghao Yue</li>
<li>for: 找寻强式折射辐射源（quasars experiencing strong lensing），以了解cosmic expansion rate、dark matter profile和镜像主 galaxy。</li>
<li>methods: 使用 cutting-edge convolutional networks（CNNs）和vision transformers（ViTs）， ensemble 训练在realistic galaxy-quasar lens simulations基础上。</li>
<li>results: 通过 averaging 多种CNNs和ViTs，可以减小误 positives，并使用HSC图像和其他数据库，找到大约600万个源，其中3080个 source有高概率是强式折射辐射源，需要spectroscopic confirmation。<details>
<summary>Abstract</summary>
Quasars experiencing strong lensing offer unique viewpoints on subjects related to the cosmic expansion rate, the dark matter profile within the foreground deflectors, and the quasar host galaxies. Unfortunately, identifying them in astronomical images is challenging since they are overwhelmed by the abundance of non-lenses. To address this, we have developed a novel approach by ensembling cutting-edge convolutional networks (CNNs) -- for instance, ResNet, Inception, NASNet, MobileNet, EfficientNet, and RegNet -- along with vision transformers (ViTs) trained on realistic galaxy-quasar lens simulations based on the Hyper Suprime-Cam (HSC) multiband images. While the individual model exhibits remarkable performance when evaluated against the test dataset, achieving an area under the receiver operating characteristic curve of $>$97.3% and a median false positive rate of 3.6%, it struggles to generalize in real data, indicated by numerous spurious sources picked by each classifier. A significant improvement is achieved by averaging these CNNs and ViTs, resulting in the impurities being downsized by factors up to 50. Subsequently, combining the HSC images with the UKIRT, VISTA, and unWISE data, we retrieve approximately 60 million sources as parent samples and reduce this to 892,609 after employing a photometry preselection to discover $z>1.5$ lensed quasars with Einstein radii of $\theta_\mathrm{E}<5$ arcsec. Afterward, the ensemble classifier indicates 3080 sources with a high probability of being lenses, for which we visually inspect, yielding 210 prevailing candidates awaiting spectroscopic confirmation. These outcomes suggest that automated deep learning pipelines hold great potential in effectively detecting strong lenses in vast datasets with minimal manual visual inspection involved.
</details>
<details>
<summary>摘要</summary>
astronomy 关键词：Quasars，强大的 gravitational lensing，cosmic expansion rate，dark matter，quasar host galaxiesQuasars 经历强大的 gravitational lensing 提供了一种独特的视角，可以研究cosmic expansion rate 和 dark matter  Profile within the foreground deflectors 以及 Quasar host galaxies。然而，在天文图像中 identific Quasars 是一项挑战，因为它们被非 gravitational lens 的丰富数量掩盖。为 Address this, we have developed a novel approach by ensembling cutting-edge Convolutional Neural Networks (CNNs) 和 Vision Transformers (ViTs) ，例如 ResNet, Inception, NASNet, MobileNet, EfficientNet, and RegNet ，并在 Hyper Suprime-Cam (HSC) 多波段图像上进行实际的 galaxy-quasar lens  simulations。although the individual model shows remarkable performance when evaluated against the test dataset, achieving an area under the receiver operating characteristic curve of $>$97.3% and a median false positive rate of 3.6%，it struggles to generalize in real data, indicated by numerous spurious sources picked by each classifier。a significant improvement is achieved by averaging these CNNs and ViTs, resulting in the impurities being downsized by factors up to 50。subsequently, we combine the HSC images with the UKIRT, VISTA, and unWISE data, retrieve approximately 60 million sources as parent samples, and reduce this to 892,609 after employing a photometry preselection to discover $z>1.5$ lensed quasars with Einstein radii of $\theta_\mathrm{E}<5$ arcsec。afterward, the ensemble classifier indicates 3080 sources with a high probability of being lenses, for which we visually inspect, yielding 210 prevailing candidates awaiting spectroscopic confirmation。these outcomes suggest that automated deep learning pipelines hold great potential in effectively detecting strong lenses in vast datasets with minimal manual visual inspection involved。
</details></li>
</ul>
<hr>
<h2 id="Empirically-Validating-Conformal-Prediction-on-Modern-Vision-Architectures-Under-Distribution-Shift-and-Long-tailed-Data"><a href="#Empirically-Validating-Conformal-Prediction-on-Modern-Vision-Architectures-Under-Distribution-Shift-and-Long-tailed-Data" class="headerlink" title="Empirically Validating Conformal Prediction on Modern Vision Architectures Under Distribution Shift and Long-tailed Data"></a>Empirically Validating Conformal Prediction on Modern Vision Architectures Under Distribution Shift and Long-tailed Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01088">http://arxiv.org/abs/2307.01088</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kevin Kasa, Graham W. Taylor</li>
<li>for: 提供深度学习模型可靠的uncertainty estimate和安全保证</li>
<li>methods: 评估多种post-hoc和训练基于的conformal prediction方法在不同的分布shift和长尾分布下的性能</li>
<li>results: 研究发现，即使使用多种conformal方法和神经网络家族，其性能在分布shift和长尾分布下都会受到很大影响，导致安全保证被违反。<details>
<summary>Abstract</summary>
Conformal prediction has emerged as a rigorous means of providing deep learning models with reliable uncertainty estimates and safety guarantees. Yet, its performance is known to degrade under distribution shift and long-tailed class distributions, which are often present in real world applications. Here, we characterize the performance of several post-hoc and training-based conformal prediction methods under these settings, providing the first empirical evaluation on large-scale datasets and models. We show that across numerous conformal methods and neural network families, performance greatly degrades under distribution shifts violating safety guarantees. Similarly, we show that in long-tailed settings the guarantees are frequently violated on many classes. Understanding the limitations of these methods is necessary for deployment in real world and safety-critical applications.
</details>
<details>
<summary>摘要</summary>
宽泛预测（Conformal prediction）已经成为深度学习模型提供可靠的不确定性估计和安全保证的有力方法。然而，其性能知道会在分布变换和长尾类分布下逐渐下降，这些情况通常存在在实际应用中。我们在这里对几种后处和训练基础的宽泛预测方法进行了首次实证评估，并在大规模数据和模型上进行了评估。我们发现，无论是哪些方法或者哪些神经网络家族，在分布变换下都会违反安全保证。同时，在长尾设置下，保证也frequently被违反了许多类别。理解这些方法的限制是必要的，以便在实际应用和安全关键应用中部署。
</details></li>
</ul>
<hr>
<h2 id="Supervised-Manifold-Learning-via-Random-Forest-Geometry-Preserving-Proximities"><a href="#Supervised-Manifold-Learning-via-Random-Forest-Geometry-Preserving-Proximities" class="headerlink" title="Supervised Manifold Learning via Random Forest Geometry-Preserving Proximities"></a>Supervised Manifold Learning via Random Forest Geometry-Preserving Proximities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01077">http://arxiv.org/abs/2307.01077</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jake S. Rhodes</li>
<li>for: 本文目的是提出一种新的整体性 manifold learning 方法，用于supervised dimensionality reduction。</li>
<li>methods: 本文使用的方法包括 random forest proximities 和 diffusion-based algorithms，以保持本数据集中的地方结构和全局结构。</li>
<li>results: 本文表明，使用这种新的方法可以更好地保持数据集中的地方结构和全局结构，并且可以提高 manifold learning 的效果。<details>
<summary>Abstract</summary>
Manifold learning approaches seek the intrinsic, low-dimensional data structure within a high-dimensional space. Mainstream manifold learning algorithms, such as Isomap, UMAP, $t$-SNE, Diffusion Map, and Laplacian Eigenmaps do not use data labels and are thus considered unsupervised. Existing supervised extensions of these methods are limited to classification problems and fall short of uncovering meaningful embeddings due to their construction using order non-preserving, class-conditional distances. In this paper, we show the weaknesses of class-conditional manifold learning quantitatively and visually and propose an alternate choice of kernel for supervised dimensionality reduction using a data-geometry-preserving variant of random forest proximities as an initialization for manifold learning methods. We show that local structure preservation using these proximities is near universal across manifold learning approaches and global structure is properly maintained using diffusion-based algorithms.
</details>
<details>
<summary>摘要</summary>
manifold learning方法寻找高维空间中内在的低维数据结构。主流 manifold learning 算法，如 Isomap、UMAP、t-SNE、Diffusion Map 和 Laplacian Eigenmaps，不使用数据标签，因此被视为无监督的。现有的监督扩展方法仅适用于分类问题，而且由于其使用不 preserve 的、类别 conditional 距离，因此无法找到有意义的嵌入。在这篇论文中，我们证明了类别 conditional manifold learning 的弱点，并提出了一种不同的kernel来实现监督的维度减少。我们显示了这些距离的本地结构保持是near universal across manifold learningapproaches，而且使用扩散算法来保持全局结构。
</details></li>
</ul>
<hr>
<h2 id="When-Can-Linear-Learners-be-Robust-to-Indiscriminate-Poisoning-Attacks"><a href="#When-Can-Linear-Learners-be-Robust-to-Indiscriminate-Poisoning-Attacks" class="headerlink" title="When Can Linear Learners be Robust to Indiscriminate Poisoning Attacks?"></a>When Can Linear Learners be Robust to Indiscriminate Poisoning Attacks?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01073">http://arxiv.org/abs/2307.01073</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fnu Suya, Xiao Zhang, Yuan Tian, David Evans</li>
<li>for: 研究线性学习器对恶意抹黑攻击的Robustness，攻击者通过杂化数据中插入一些针对性设计的示例来尝试让模型在测试集上增加错误率。</li>
<li>methods: 基于一些数据集上线性学习器能够抵抗最佳攻击的观察，研究是否存在一些数据集可以自然地抵抗杂化攻击。对于理想化的 Gaussian 分布，我们准确地描述了最佳抹黑攻击策略的行为，这种策略可以在给定抹黑预算下最大化测试集中模型的风险。</li>
<li>results: 我们的结果表明，如果数据集中的类划分具有低差异和低方差，并且杂化攻击点的约束集的大小很小，那么线性学习器就可以具有Robustness 对杂化攻击。这些发现解释了一些学习任务在杂化攻击下的差异性，对于理解杂化攻击的根本原因是非常重要的一步。<details>
<summary>Abstract</summary>
We study indiscriminate poisoning for linear learners where an adversary injects a few crafted examples into the training data with the goal of forcing the induced model to incur higher test error. Inspired by the observation that linear learners on some datasets are able to resist the best known attacks even without any defenses, we further investigate whether datasets can be inherently robust to indiscriminate poisoning attacks for linear learners. For theoretical Gaussian distributions, we rigorously characterize the behavior of an optimal poisoning attack, defined as the poisoning strategy that attains the maximum risk of the induced model at a given poisoning budget. Our results prove that linear learners can indeed be robust to indiscriminate poisoning if the class-wise data distributions are well-separated with low variance and the size of the constraint set containing all permissible poisoning points is also small. These findings largely explain the drastic variation in empirical attack performance of the state-of-the-art poisoning attacks on linear learners across benchmark datasets, making an important initial step towards understanding the underlying reasons some learning tasks are vulnerable to data poisoning attacks.
</details>
<details>
<summary>摘要</summary>
我们研究不偏辐射毒素攻击，针对线性学习器，敌对人在训练数据中插入一些手动制作的示例，以达到让引导出的模型测试错误高的目的。受观察到一些数据集上的线性学习器能够不受任何防御措施下 resist最佳攻击的现象，我们进一步调查是否存在一些数据集可以自然地抵抗不偏辐射毒素攻击。对于理论 Gaussian 分布，我们仔细描述了最佳毒素攻击策略，即在给定毒素预算下，可以使引导出的模型测试错误最大化的攻击策略。我们的结果表明，如果数据集中的类别数据分布具有低差异和低方差，而且制定集中包含所有可能的毒素点的大小也很小，那么线性学习器就可以具有抵抗不偏辐射毒素攻击的性能。这些发现解释了一些学习任务对于数据毒素攻击的 empirical 攻击性能的悬峰性，从而为我们更好的理解这些任务的潜在原因。
</details></li>
</ul>
<hr>
<h2 id="PIGNet2-A-Versatile-Deep-Learning-based-Protein-Ligand-Interaction-Prediction-Model-for-Binding-Affinity-Scoring-and-Virtual-Screening"><a href="#PIGNet2-A-Versatile-Deep-Learning-based-Protein-Ligand-Interaction-Prediction-Model-for-Binding-Affinity-Scoring-and-Virtual-Screening" class="headerlink" title="PIGNet2: A Versatile Deep Learning-based Protein-Ligand Interaction Prediction Model for Binding Affinity Scoring and Virtual Screening"></a>PIGNet2: A Versatile Deep Learning-based Protein-Ligand Interaction Prediction Model for Binding Affinity Scoring and Virtual Screening</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01066">http://arxiv.org/abs/2307.01066</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ace-kaist/pignet2">https://github.com/ace-kaist/pignet2</a></li>
<li>paper_authors: Seokhyun Moon, Sang-Yeon Hwang, Jaechang Lim, Woo Youn Kim</li>
<li>for: 该研究旨在提出一种可靠地预测蛋白-小分子交互（PLI）的模型，以帮助药物发现过程中更好地预测蛋白和小分子之间的交互。</li>
<li>methods: 该研究使用了一种新的数据扩展策略，与物理学习神经网络结合，以提高PLI预测的准确性和效率。</li>
<li>results: 研究显示，该模型在不同的测试中具有显著的改进，包括衍生数据集测试和距离可能性学习测试，并达到了与当前最佳性能相当的水平。这表明该方法在药物发现中具有潜在的应用前景。<details>
<summary>Abstract</summary>
Prediction of protein-ligand interactions (PLI) plays a crucial role in drug discovery as it guides the identification and optimization of molecules that effectively bind to target proteins. Despite remarkable advances in deep learning-based PLI prediction, the development of a versatile model capable of accurately scoring binding affinity and conducting efficient virtual screening remains a challenge. The main obstacle in achieving this lies in the scarcity of experimental structure-affinity data, which limits the generalization ability of existing models. Here, we propose a viable solution to address this challenge by introducing a novel data augmentation strategy combined with a physics-informed graph neural network. The model showed significant improvements in both scoring and screening, outperforming task-specific deep learning models in various tests including derivative benchmarks, and notably achieving results comparable to the state-of-the-art performance based on distance likelihood learning. This demonstrates the potential of this approach to drug discovery.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="ENGAGE-Explanation-Guided-Data-Augmentation-for-Graph-Representation-Learning"><a href="#ENGAGE-Explanation-Guided-Data-Augmentation-for-Graph-Representation-Learning" class="headerlink" title="ENGAGE: Explanation Guided Data Augmentation for Graph Representation Learning"></a>ENGAGE: Explanation Guided Data Augmentation for Graph Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01053">http://arxiv.org/abs/2307.01053</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sycny/engage">https://github.com/sycny/engage</a></li>
<li>paper_authors: Yucheng Shi, Kaixiong Zhou, Ninghao Liu</li>
<li>for: 本文旨在提出一种基于解释导向的数据增强方法，以保留图数据中重要的特征信息并排除不必要的信息。</li>
<li>methods: 本文提出了一种名为ENGAGE（ExplaNation Guided data AuGmEntation）的方法，其中使用了一种高效的无监督解释方法，称为简化活动图，以评估节点的重要性。此外，本文还提出了两种数据增强方案，一种是Structural Augmentation，另一种是Feature Augmentation。</li>
<li>results: 经过实验证明，ENGAGE方法可以在不同的模型架构和真实的图数据上显著提高图数据的表示能力。同时，ENGAGE方法还可以在图级和节点级任务上达到优秀的性能。<details>
<summary>Abstract</summary>
The recent contrastive learning methods, due to their effectiveness in representation learning, have been widely applied to modeling graph data. Random perturbation is widely used to build contrastive views for graph data, which however, could accidentally break graph structures and lead to suboptimal performance. In addition, graph data is usually highly abstract, so it is hard to extract intuitive meanings and design more informed augmentation schemes. Effective representations should preserve key characteristics in data and abandon superfluous information. In this paper, we propose ENGAGE (ExplaNation Guided data AuGmEntation), where explanation guides the contrastive augmentation process to preserve the key parts in graphs and explore removing superfluous information. Specifically, we design an efficient unsupervised explanation method called smoothed activation map as the indicator of node importance in representation learning. Then, we design two data augmentation schemes on graphs for perturbing structural and feature information, respectively. We also provide justification for the proposed method in the framework of information theories. Experiments of both graph-level and node-level tasks, on various model architectures and on different real-world graphs, are conducted to demonstrate the effectiveness and flexibility of ENGAGE. The code of ENGAGE can be found: https://github.com/sycny/ENGAGE.
</details>
<details>
<summary>摘要</summary>
近期的对比学习方法，因其在表示学习中的效iveness，在图数据模型中广泛应用。Random perturbation 广泛使用于建立对比视图，但可能意外破坏图结构，导致表现下降。此外，图数据通常很抽象，因此很难提取直观意义和设计更 Informed的扩充方案。有效的表示应保留数据中的关键特征，抛弃不必要的信息。在这篇论文中，我们提出了ENGAGE（ExplaNation Guided data AuGmEntation），其中解释指导对比扩充过程，以保留图中关键部分并探索抛弃不必要信息。具体来说，我们设计了一种高效的无监督解释方法，即简化活动图作为表示学习中节点重要性的指标。然后，我们设计了对图结构和特征信息进行扩充的两种方案。我们还提供了对提案方法的信息理论 justify。在图级和节点级任务上，使用不同的模型架构和真实的图数据进行实验，以证明ENGAGE的有效性和灵活性。ENGAGE的代码可以在 GitHub 上找到：https://github.com/sycny/ENGAGE。
</details></li>
</ul>
<hr>
<h2 id="Transport-Variational-Inference-and-Diffusions-with-Applications-to-Annealed-Flows-and-Schrodinger-Bridges"><a href="#Transport-Variational-Inference-and-Diffusions-with-Applications-to-Annealed-Flows-and-Schrodinger-Bridges" class="headerlink" title="Transport, Variational Inference and Diffusions: with Applications to Annealed Flows and Schrödinger Bridges"></a>Transport, Variational Inference and Diffusions: with Applications to Annealed Flows and Schrödinger Bridges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01050">http://arxiv.org/abs/2307.01050</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francisco Vargas, Nikolas Nüsken</li>
<li>for: 这篇论文探讨了最优运输和变量推断之间的连接，特别是关于前向和反向时间随机 diffeq 和 Girсанов变换。</li>
<li>methods: 作者提出了一种原则正式的框架，基于差异在路径空间上进行抽象和生成模型，包括一种基于差异的扩散流技术和一种规范化迭代匹配目标。</li>
<li>results: 作者通过一系列的生成模型示例和一个基于双峰的罕见事件任务，展示了提议的方法的潜力。<details>
<summary>Abstract</summary>
This paper explores the connections between optimal transport and variational inference, with a focus on forward and reverse time stochastic differential equations and Girsanov transformations.We present a principled and systematic framework for sampling and generative modelling centred around divergences on path space. Our work culminates in the development of a novel score-based annealed flow technique (with connections to Jarzynski and Crooks identities from statistical physics) and a regularised iterative proportional fitting (IPF)-type objective, departing from the sequential nature of standard IPF. Through a series of generative modelling examples and a double-well-based rare event task, we showcase the potential of the proposed methods.
</details>
<details>
<summary>摘要</summary>
这篇论文探讨优化运输和变量推断之间的连接，特别是关注forward和reverse时间随机 diffeq和 Girсанов变换。我们提出了一种原理性的和系统的框架，用于 sampling和生成模型，围绕 divergence on path space。我们的工作最终得出了一种新的Score-based annealed flow技术（与统计物理中的Jarzynski和Crooks标准相关）和一种常规iterative proportional fitting（IPF）类型的目标函数。通过一系列的生成模型示例和一个基于double-well的罕见事件任务，我们展示了提议的方法的潜力。Note: The translation is done using a machine translation tool, and may not be perfect or idiomatic.
</details></li>
</ul>
<hr>
<h2 id="Vector-Quantile-Regression-on-Manifolds"><a href="#Vector-Quantile-Regression-on-Manifolds" class="headerlink" title="Vector Quantile Regression on Manifolds"></a>Vector Quantile Regression on Manifolds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01037">http://arxiv.org/abs/2307.01037</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marco Pegoraro, Sanketh Vedula, Aviv A. Rosenberg, Irene Tallini, Emanuele Rodolà, Alex M. Bronstein</li>
<li>For: 本研究旨在探讨量规 regression（QR）在多变量分布上的应用，特别是在多变量分布上的 manifold 上。* Methods: 本研究使用优化的运输理论和c-卷函数来定义高维变量在 manifold 上的 conditional vector quantile function（M-CVQF）。* Results: 研究人员通过synthetic data experiment来证明M-CVQF的有效性，并提供了非欧氏分布中量规的含义。<details>
<summary>Abstract</summary>
Quantile regression (QR) is a statistical tool for distribution-free estimation of conditional quantiles of a target variable given explanatory features. QR is limited by the assumption that the target distribution is univariate and defined on an Euclidean domain. Although the notion of quantiles was recently extended to multi-variate distributions, QR for multi-variate distributions on manifolds remains underexplored, even though many important applications inherently involve data distributed on, e.g., spheres (climate measurements), tori (dihedral angles in proteins), or Lie groups (attitude in navigation). By leveraging optimal transport theory and the notion of $c$-concave functions, we meaningfully define conditional vector quantile functions of high-dimensional variables on manifolds (M-CVQFs). Our approach allows for quantile estimation, regression, and computation of conditional confidence sets. We demonstrate the approach's efficacy and provide insights regarding the meaning of non-Euclidean quantiles through preliminary synthetic data experiments.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Temporal-Graph-Benchmark-for-Machine-Learning-on-Temporal-Graphs"><a href="#Temporal-Graph-Benchmark-for-Machine-Learning-on-Temporal-Graphs" class="headerlink" title="Temporal Graph Benchmark for Machine Learning on Temporal Graphs"></a>Temporal Graph Benchmark for Machine Learning on Temporal Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01026">http://arxiv.org/abs/2307.01026</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shenyanghuang/tgb">https://github.com/shenyanghuang/tgb</a></li>
<li>paper_authors: Shenyang Huang, Farimah Poursafaei, Jacob Danovitch, Matthias Fey, Weihua Hu, Emanuele Rossi, Jure Leskovec, Michael Bronstein, Guillaume Rabusseau, Reihaneh Rabbany</li>
<li>for: 本文为了提供一个真实、可重现、可靠的 temporal graph 模型评估 benchmark，旨在驱动 temporal graph 研究的进步。</li>
<li>methods: 本文使用了多种常见的 temporal graph 模型，并设计了基于实际用 caso 的评估协议。</li>
<li>results: 研究发现，存在许多 temporal graph dataset，模型的性能可以很大差异，而且简单的方法经常超越现有的 temporal graph 模型。这些发现打开了未来 temporal graph 研究的可能性。<details>
<summary>Abstract</summary>
We present the Temporal Graph Benchmark (TGB), a collection of challenging and diverse benchmark datasets for realistic, reproducible, and robust evaluation of machine learning models on temporal graphs. TGB datasets are of large scale, spanning years in duration, incorporate both node and edge-level prediction tasks and cover a diverse set of domains including social, trade, transaction, and transportation networks. For both tasks, we design evaluation protocols based on realistic use-cases. We extensively benchmark each dataset and find that the performance of common models can vary drastically across datasets. In addition, on dynamic node property prediction tasks, we show that simple methods often achieve superior performance compared to existing temporal graph models. We believe that these findings open up opportunities for future research on temporal graphs. Finally, TGB provides an automated machine learning pipeline for reproducible and accessible temporal graph research, including data loading, experiment setup and performance evaluation. TGB will be maintained and updated on a regular basis and welcomes community feedback. TGB datasets, data loaders, example codes, evaluation setup, and leaderboards are publicly available at https://tgb.complexdatalab.com/ .
</details>
<details>
<summary>摘要</summary>
我们介绍了 Temporal Graph Benchmark（TGB），一个包含具有具有强大挑战和多样化的 bencmark 数据集，用于真实、可重现和Robust 评估机器学习模型在时间图上。TGB 数据集的规模很大，覆盖了多年的时间 duration，包括节点和边级别预测任务，并覆盖了社交、贸易、交易和交通网络等多种领域。为两个任务，我们设计了基于实际用例的评估协议。我们对每个数据集进行了广泛的测试，发现公共模型在不同数据集上的性能可以截然不同。此外，在动态节点属性预测任务上，我们发现简单的方法经常超越现有的时间图模型。我们认为这些发现开发了未来研究时间图的机遇。此外，TGB 提供了一个自动化机器学习管道，用于可重现和可访问的时间图研究，包括数据加载、实验设置和性能评估。TGB 将会在 régular basis 维护和更新，欢迎社区反馈。TGB 数据集、数据加载器、示例代码、评估设置和排名是公共可用的，可以通过 https://tgb.complexdatalab.com/ 访问。
</details></li>
</ul>
<hr>
<h2 id="Neural-Chronos-ODE-Unveiling-Temporal-Patterns-and-Forecasting-Future-and-Past-Trends-in-Time-Series-Data"><a href="#Neural-Chronos-ODE-Unveiling-Temporal-Patterns-and-Forecasting-Future-and-Past-Trends-in-Time-Series-Data" class="headerlink" title="Neural Chronos ODE: Unveiling Temporal Patterns and Forecasting Future and Past Trends in Time Series Data"></a>Neural Chronos ODE: Unveiling Temporal Patterns and Forecasting Future and Past Trends in Time Series Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01023">http://arxiv.org/abs/2307.01023</a></li>
<li>repo_url: None</li>
<li>paper_authors: C. Coelho, M. Fernanda P. Costa, L. L. Ferrás</li>
<li>for: 预测系统的时间序列，包括前向和反向时间预测</li>
<li>methods: 使用深度神经网络模型，包括Neural CODE和其变种CODE-RNN、CODE-BiRNN、CODE-GRU、CODE-BiGRU、CODE-LSTM和CODE-BiLSTM</li>
<li>results: 实验结果表明Neural CODE比Neural ODE更好地学习系统的时间序列，而CODE-BiRNN&#x2F;-BiGRU&#x2F;-BiLSTM在三个实际时间序列任务上表现最佳，包括数据缺失估计、前向和反向推算等。<details>
<summary>Abstract</summary>
This work introduces Neural Chronos Ordinary Differential Equations (Neural CODE), a deep neural network architecture that fits a continuous-time ODE dynamics for predicting the chronology of a system both forward and backward in time. To train the model, we solve the ODE as an initial value problem and a final value problem, similar to Neural ODEs. We also explore two approaches to combining Neural CODE with Recurrent Neural Networks by replacing Neural ODE with Neural CODE (CODE-RNN), and incorporating a bidirectional RNN for full information flow in both time directions (CODE-BiRNN), and variants with other update cells namely GRU and LSTM: CODE-GRU, CODE-BiGRU, CODE-LSTM, CODE-BiLSTM.   Experimental results demonstrate that Neural CODE outperforms Neural ODE in learning the dynamics of a spiral forward and backward in time, even with sparser data. We also compare the performance of CODE-RNN/-GRU/-LSTM and CODE-BiRNN/-BiGRU/-BiLSTM against ODE-RNN/-GRU/-LSTM on three real-life time series data tasks: imputation of missing data for lower and higher dimensional data, and forward and backward extrapolation with shorter and longer time horizons. Our findings show that the proposed architectures converge faster, with CODE-BiRNN/-BiGRU/-BiLSTM consistently outperforming the other architectures on all tasks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Joint-Coordinate-Regression-and-Association-For-Multi-Person-Pose-Estimation-A-Pure-Neural-Network-Approach"><a href="#Joint-Coordinate-Regression-and-Association-For-Multi-Person-Pose-Estimation-A-Pure-Neural-Network-Approach" class="headerlink" title="Joint Coordinate Regression and Association For Multi-Person Pose Estimation, A Pure Neural Network Approach"></a>Joint Coordinate Regression and Association For Multi-Person Pose Estimation, A Pure Neural Network Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01004">http://arxiv.org/abs/2307.01004</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongyang Yu, Yunshi Xie, Wangpeng An, Li Zhang, Yufeng Yao</li>
<li>for: 这个论文目的是提出一种一阶段端到端多人2D姿态估计算法（Joint Coordinate Regression and Association，简称JCRA），不需要任何后处理。</li>
<li>methods: 该算法使用一个一阶段端到端网络架构，从图像中直接输出人体关节坐标，并采用了对称的网络结构，以确保高准确率。</li>
<li>results: 对于MS COCO和CrowdPose测试集，JCRA的实验结果表明，它在准确率和效率两个方面都超过了现有的方法。具体来说，JCRA在MS COCO测试集上达到了69.2 mAP，并且在推理加速方面比前一代底层算法快78%。<details>
<summary>Abstract</summary>
We introduce a novel one-stage end-to-end multi-person 2D pose estimation algorithm, known as Joint Coordinate Regression and Association (JCRA), that produces human pose joints and associations without requiring any post-processing. The proposed algorithm is fast, accurate, effective, and simple. The one-stage end-to-end network architecture significantly improves the inference speed of JCRA. Meanwhile, we devised a symmetric network structure for both the encoder and decoder, which ensures high accuracy in identifying keypoints. It follows an architecture that directly outputs part positions via a transformer network, resulting in a significant improvement in performance. Extensive experiments on the MS COCO and CrowdPose benchmarks demonstrate that JCRA outperforms state-of-the-art approaches in both accuracy and efficiency. Moreover, JCRA demonstrates 69.2 mAP and is 78\% faster at inference acceleration than previous state-of-the-art bottom-up algorithms. The code for this algorithm will be publicly available.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的一stage终端多人2D姿态估计算法，称为共同坐标回归和关联（JCRA），该算法可以不需要任何后处理生成人体姿态关节和关联。我们提出的算法具有快速、准确、有效和简单的特点。我们使用一stage终端网络架构，这有效地提高了JCRA的推理速度。同时，我们设计了对Encoder和Decoder网络结构的 симметри化，确保高精度地标定关键点。它采用一种直接输出部位位置的transformer网络架构，从而导致了显著提高的性能。我们在COCO和CrowdPose benchmark上进行了广泛的实验，demonstrates that JCRA exceeds state-of-the-art approaches in both accuracy and efficiency. In addition, JCRA achieves 69.2 mAP and is 78% faster at inference acceleration than previous state-of-the-art bottom-up algorithms. The code for this algorithm will be publicly available.
</details></li>
</ul>
<hr>
<h2 id="Capafoldable-self-tracking-foldable-smart-textiles-with-capacitive-sensing"><a href="#Capafoldable-self-tracking-foldable-smart-textiles-with-capacitive-sensing" class="headerlink" title="Capafoldable: self-tracking foldable smart textiles with capacitive sensing"></a>Capafoldable: self-tracking foldable smart textiles with capacitive sensing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05370">http://arxiv.org/abs/2307.05370</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lala Shakti Swarup Ray, Daniel Geißler, Bo Zhou, Paul Lukowicz, Berit Greinke</li>
<li>For: 能够检测 estructural motions 的 smart textile* Methods:  combining folded fabric structures 和 capacitive sensing，使用 state-of-the-art sensing circuits 和 deep learning technologies* Results: 可以很准确地 reconstruction geometry primitives defining patch shape from capacitive signals，tracking error 只有 1cm，可以应用于新的 smart textile 应用程序。Here’s the full text in Traditional Chinese:这个研究旨在开发一种能够检测结构运动的智能纱布，通过结合折叠纱布结构和导电纱布感知技术，并使用现代感知电路和深度学习技术来实现。我们实验了两种折叠模式，即Accordion和Chevron，每种模式都有两种导电纱布感知器的配置。通过我们的方法，可以很准确地从导电信号中重建geometry primitives定义纱布形状，追踪误差只有1cm，可以应用于新的智能纱布应用程序。<details>
<summary>Abstract</summary>
Folding is an unique structural technique to enable planer materials with motion or 3D mechanical properties. Textile-based capacitive sensing has shown to be sensitive to the geometry deformation and relative motion of conductive textiles. In this work, we propose a novel self-tracking foldable smart textile by combining folded fabric structures and capacitive sensing to detect the structural motions using state-of-the-art sensing circuits and deep learning technologies. We created two folding patterns, Accordion and Chevron, each with two layouts of capacitive sensors in the form of thermobonded conductive textile patches. In an experiment of manually moving patches of the folding patterns, we developed deep neural network to learn and reconstruct the vision-tracked shape of the patches. Through our approach, the geometry primitives defining the patch shape can be reconstructed from the capacitive signals with R-squared value of up to 95\% and tracking error of 1cm for 22.5cm long patches. With mechanical, electrical and sensing properties, Capafoldable could enable a new range of smart textile applications.
</details>
<details>
<summary>摘要</summary>
折叠是一种独特的结构技术，可以让平面材料具有运动或3D机械性能。基于织物的电容式感测技术已经证明可以感测织物的几何变形和相对运动。在这项工作中，我们提出了一种新的自追踪式折叠智能织物，通过结合折叠布结构和电容式感测技术来检测结构运动。我们设计了两种折叠模式，即腰棒和斜线，每种模式有两种布置的电容器在形式为热粘合的导电织物贴片上。在手动移动贴片的实验中，我们开发了深度神经网络来学习和重建通过视觉跟踪的贴片形状。通过我们的方法，贴片的几何基本元可以从电容信号中被重建，R-squared值可达95%，跟踪错误为1cm，对22.5cm长的贴片来说。拥有机械、电学和感测性能，Capafoldable可以开拓新的智能织物应用领域。
</details></li>
</ul>
<hr>
<h2 id="Pareto-optimal-proxy-metrics"><a href="#Pareto-optimal-proxy-metrics" class="headerlink" title="Pareto optimal proxy metrics"></a>Pareto optimal proxy metrics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01000">http://arxiv.org/abs/2307.01000</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lee Richardson, Alessandro Zito, Dylan Greaves, Jacopo Soriano</li>
<li>for: 优化产品，提高产品质量</li>
<li>methods: 使用代理指标，同时提高预测精度和敏感度</li>
<li>results: 提高决策 velocicty和决策质量，代理指标比北星指标高八倍敏感度<details>
<summary>Abstract</summary>
North star metrics and online experimentation play a central role in how technology companies improve their products. In many practical settings, however, evaluating experiments based on the north star metric directly can be difficult. The two most significant issues are 1) low sensitivity of the north star metric and 2) differences between the short-term and long-term impact on the north star metric. A common solution is to rely on proxy metrics rather than the north star in experiment evaluation and launch decisions. Existing literature on proxy metrics concentrates mainly on the estimation of the long-term impact from short-term experimental data. In this paper, instead, we focus on the trade-off between the estimation of the long-term impact and the sensitivity in the short term. In particular, we propose the Pareto optimal proxy metrics method, which simultaneously optimizes prediction accuracy and sensitivity. In addition, we give an efficient multi-objective optimization algorithm that outperforms standard methods. We applied our methodology to experiments from a large industrial recommendation system, and found proxy metrics that are eight times more sensitive than the north star and consistently moved in the same direction, increasing the velocity and the quality of the decisions to launch new features.
</details>
<details>
<summary>摘要</summary>
北斗星指标和在线实验是技术公司产品改进的中心角色。然而，在实际设置中，直接基于北斗星指标评估实验的问题经常出现。主要问题包括1）北斗星指标敏感度低和2）短期和长期北斗星指标之间的差异。现有文献中的代表指标集中在长期影响的估计上，而不是短期影响。在这篇论文中，我们则关注了长期影响和短期敏感度之间的衡量。我们提出了最优化代表指标方法，同时保证了预测准确性和敏感度。此外，我们还提供了超过标准方法的高效多目标优化算法。我们在一大型工业推荐系统的实验中应用了我们的方法ологи，发现代表指标是北斗星指标的八倍敏感，并且一直逐渐增长，提高了决策启动新特性的速度和质量。
</details></li>
</ul>
<hr>
<h2 id="Environmental-effects-on-emergent-strategy-in-micro-scale-multi-agent-reinforcement-learning"><a href="#Environmental-effects-on-emergent-strategy-in-micro-scale-multi-agent-reinforcement-learning" class="headerlink" title="Environmental effects on emergent strategy in micro-scale multi-agent reinforcement learning"></a>Environmental effects on emergent strategy in micro-scale multi-agent reinforcement learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00994">http://arxiv.org/abs/2307.00994</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/swarmrl/swarmrl">https://github.com/swarmrl/swarmrl</a></li>
<li>paper_authors: Samuel Tovey, David Zimmer, Christoph Lohrmann, Tobias Merkt, Simon Koppenhoefer, Veit-Lorenz Heuthe, Clemens Bechinger, Christian Holm</li>
<li>for: This paper explores the role of temperature in the emergence and efficacy of strategies in MARL systems using particle-based Langevin molecular dynamics simulations.</li>
<li>methods: The paper uses particle-based Langevin molecular dynamics simulations as a realistic representation of micro-scale environments, and introduces a novel Python package for studying microscopic agents using reinforcement learning.</li>
<li>results: The paper finds that at higher temperatures, the RL agents identify new strategies for achieving tasks, highlighting the importance of understanding this regime and providing insight into optimal training strategies for bridging the generalization gap between simulation and reality.<details>
<summary>Abstract</summary>
Multi-Agent Reinforcement Learning (MARL) is a promising candidate for realizing efficient control of microscopic particles, of which micro-robots are a subset. However, the microscopic particles' environment presents unique challenges, such as Brownian motion at sufficiently small length-scales. In this work, we explore the role of temperature in the emergence and efficacy of strategies in MARL systems using particle-based Langevin molecular dynamics simulations as a realistic representation of micro-scale environments. To this end, we perform experiments on two different multi-agent tasks in microscopic environments at different temperatures, detecting the source of a concentration gradient and rotation of a rod. We find that at higher temperatures, the RL agents identify new strategies for achieving these tasks, highlighting the importance of understanding this regime and providing insight into optimal training strategies for bridging the generalization gap between simulation and reality. We also introduce a novel Python package for studying microscopic agents using reinforcement learning (RL) to accompany our results.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Over-The-Air-Federated-Learning-Status-Quo-Open-Challenges-and-Future-Directions"><a href="#Over-The-Air-Federated-Learning-Status-Quo-Open-Challenges-and-Future-Directions" class="headerlink" title="Over-The-Air Federated Learning: Status Quo, Open Challenges, and Future Directions"></a>Over-The-Air Federated Learning: Status Quo, Open Challenges, and Future Directions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00974">http://arxiv.org/abs/2307.00974</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bingnan Xiao, Xichen Yu, Wei Ni, Xin Wang, H. Vincent Poor</li>
<li>for: 这篇论文旨在提供一份关于无线网络上实现人工智能应用程序的总体评论，并指出未来研究的可能方向。</li>
<li>methods: 论文使用了多 accessed 通道（MACs）的超�ayer federated learning（OTA-FL）技术，通过让网络边缘用户共享频率资源，实现高效、低延迟的全球模型聚合。</li>
<li>results: 论文对OTA-FL的进展进行了分类和总结，包括单天线OTA-FL、多天线OTA-FL和利用emerging reconfigurable intelligent surface（RIS）技术的OTA-FL。同时，论文还讨论了OTA-FL的信任、安全和隐私方面的问题，并提出了未来研究的挑战和方向。<details>
<summary>Abstract</summary>
The development of applications based on artificial intelligence and implemented over wireless networks is increasingly rapidly and is expected to grow dramatically in the future. The resulting demand for the aggregation of large amounts of data has caused serious communication bottlenecks in wireless networks and particularly at the network edge. Over-the-air federated learning (OTA-FL), leveraging the superposition feature of multi-access channels (MACs), enables users at the network edge to share spectrum resources and achieves efficient and low-latency global model aggregation. This paper provides a holistic review of progress in OTA-FL and points to potential future research directions. Specifically, we classify OTA-FL from the perspective of system settings, including single-antenna OTA-FL, multi-antenna OTA-FL, and OTA-FL with the aid of the emerging reconfigurable intelligent surface (RIS) technology, and the contributions of existing works in these areas are summarized. Moreover, we discuss the trust, security and privacy aspects of OTA-FL, and highlight concerns arising from security and privacy. Finally, challenges and potential research directions are discussed to promote the future development of OTA-FL in terms of improving system performance, reliability, and trustworthiness. Specifical challenges to be addressed include model distortion under channel fading, the ineffective OTA aggregation of local models trained on substantially unbalanced data, and the limited accessibility and verifiability of individual local models.
</details>
<details>
<summary>摘要</summary>
发展基于人工智能的应用程序在无线网络上实施，迅速增长，未来将会继续增长迅速。这Resulting in 大量数据的聚合需求导致了无线网络中的通信瓶颈和特别是网络边缘的瓶颈。使用无线多接口通道（MACs）的超载特性，用户在网络边缘可以共享频率资源，实现高效且响应时间短的全球模型聚合。本文提供了无线 federated learning（OTA-FL）的总体评论，并指出了未来研究的可能性。 Specifically, we classify OTA-FL from the perspective of system settings, including single-antenna OTA-FL, multi-antenna OTA-FL, and OTA-FL with the aid of the emerging reconfigurable intelligent surface（RIS）技术，并 Summarize the contributions of existing works in these areas. In addition, we discuss the trust, security, and privacy aspects of OTA-FL, and highlight concerns arising from security and privacy. Finally, we discuss challenges and potential research directions to promote the future development of OTA-FL in terms of improving system performance, reliability, and trustworthiness. Specific challenges to be addressed include model distortion under channel fading, the ineffective OTA aggregation of local models trained on substantially unbalanced data, and the limited accessibility and verifiability of individual local models.
</details></li>
</ul>
<hr>
<h2 id="MoVie-Visual-Model-Based-Policy-Adaptation-for-View-Generalization"><a href="#MoVie-Visual-Model-Based-Policy-Adaptation-for-View-Generalization" class="headerlink" title="MoVie: Visual Model-Based Policy Adaptation for View Generalization"></a>MoVie: Visual Model-Based Policy Adaptation for View Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00972">http://arxiv.org/abs/2307.00972</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yangsizhe/MoVie">https://github.com/yangsizhe/MoVie</a></li>
<li>paper_authors: Sizhe Yang, Yanjie Ze, Huazhe Xu</li>
<li>for: 这篇论文主要旨在解决视觉学习（Reinforcement Learning，RL）代理人在看到的限制视图下面临的总体化能力扩展问题。</li>
<li>methods: 作者提出了一种简单 yet effective的方法，可以在测试时使模型基于视图的政策适应视图总结问题，不需要显式奖励信号和任何修改 durante training time。</li>
<li>results: 作者的方法在四种不同的场景下（包括 DMControl、xArm 和 Adroit 等）进行了18个任务的测试，与基eline相比，表现出了substantial advancements（相对提高33%、86% 和 152%）。这些出色的结果表明该方法在实际 роботех术应用中具有极大的潜力。<details>
<summary>Abstract</summary>
Visual Reinforcement Learning (RL) agents trained on limited views face significant challenges in generalizing their learned abilities to unseen views. This inherent difficulty is known as the problem of $\textit{view generalization}$. In this work, we systematically categorize this fundamental problem into four distinct and highly challenging scenarios that closely resemble real-world situations. Subsequently, we propose a straightforward yet effective approach to enable successful adaptation of visual $\textbf{Mo}$del-based policies for $\textbf{Vie}$w generalization ($\textbf{MoVie}$) during test time, without any need for explicit reward signals and any modification during training time. Our method demonstrates substantial advancements across all four scenarios encompassing a total of $\textbf{18}$ tasks sourced from DMControl, xArm, and Adroit, with a relative improvement of $\mathbf{33}$%, $\mathbf{86}$%, and $\mathbf{152}$% respectively. The superior results highlight the immense potential of our approach for real-world robotics applications. Videos are available at https://yangsizhe.github.io/MoVie/ .
</details>
<details>
<summary>摘要</summary>
视觉强化学习（RL）代理人在有限视角下接受训练，面临普遍化视角问题的挑战。这种问题在实际情况中非常困难，被称为“视觉普遍化”问题。在这种工作中，我们系统地将这个基本问题分为四个明确和具有挑战性的enario，与实际情况很接近。然后，我们提议一种简单 yet effective的方法，在测试时使模型基于视觉的策略适应视觉普遍化，不需要显式奖励信号和任何修改 durante 训练时间。我们的方法在四个scenario中展示了明显的进步，涵盖了DMControl、xArm和Adroit中的共计18个任务，相对改进率为33%、86%和152%。这些出色的结果表明我们的方法在实际 робо学应用中具有巨大的潜力。视频可以在https://yangsizhe.github.io/MoVie/ 中找到。
</details></li>
</ul>
<hr>
<h2 id="REAL-A-Representative-Error-Driven-Approach-for-Active-Learning"><a href="#REAL-A-Representative-Error-Driven-Approach-for-Active-Learning" class="headerlink" title="REAL: A Representative Error-Driven Approach for Active Learning"></a>REAL: A Representative Error-Driven Approach for Active Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00968">http://arxiv.org/abs/2307.00968</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/withchencheng/ecml_pkdd_23_real">https://github.com/withchencheng/ecml_pkdd_23_real</a></li>
<li>paper_authors: Cheng Chen, Yong Wang, Lizi Liao, Yueguo Chen, Xiaoyong Du</li>
<li>for: 这个论文的目的是提出一种基于 Representative Errors for Active Learning（REAL）的方法，以优化活动学习中的数据选择。</li>
<li>methods: 这个方法使用了一种基于不确定性和多样性的方法来评估未标注的实例的有用性，并在这些实例中寻找最有代表性的错误（ Pseudo Errors）。它还采用了一种自适应的采样预算分配方法，以根据预测错误的density来决定采样的质量。</li>
<li>results: 实验表明，使用 REAL 方法可以在多种 гипер参数设置下，consistently 超越所有最佳基准方法 regarding 准确率和 F1-macro 分数。同时，我们的分析还显示了 REAL 方法选择的 pseudo errors 与真实错误的分布相匹配。<details>
<summary>Abstract</summary>
Given a limited labeling budget, active learning (AL) aims to sample the most informative instances from an unlabeled pool to acquire labels for subsequent model training. To achieve this, AL typically measures the informativeness of unlabeled instances based on uncertainty and diversity. However, it does not consider erroneous instances with their neighborhood error density, which have great potential to improve the model performance. To address this limitation, we propose $REAL$, a novel approach to select data instances with $\underline{R}$epresentative $\underline{E}$rrors for $\underline{A}$ctive $\underline{L}$earning. It identifies minority predictions as \emph{pseudo errors} within a cluster and allocates an adaptive sampling budget for the cluster based on estimated error density. Extensive experiments on five text classification datasets demonstrate that $REAL$ consistently outperforms all best-performing baselines regarding accuracy and F1-macro scores across a wide range of hyperparameter settings. Our analysis also shows that $REAL$ selects the most representative pseudo errors that match the distribution of ground-truth errors along the decision boundary. Our code is publicly available at https://github.com/withchencheng/ECML_PKDD_23_Real.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation) Given a limited labeling budget, active learning (AL) aims to sample the most informative instances from an unlabeled pool to acquire labels for subsequent model training. To achieve this, AL typically measures the informativeness of unlabeled instances based on uncertainty and diversity. However, it does not consider erroneous instances with their neighborhood error density, which have great potential to improve the model performance. To address this limitation, we propose $REAL$, a novel approach to select data instances with $\underline{R}$epresentative $\underline{E}$rrors for $\underline{A}$ctive $\underline{L}$earning. It identifies minority predictions as \emph{pseudo errors} within a cluster and allocates an adaptive sampling budget for the cluster based on estimated error density. Extensive experiments on five text classification datasets demonstrate that $REAL$ consistently outperforms all best-performing baselines regarding accuracy and F1-macro scores across a wide range of hyperparameter settings. Our analysis also shows that $REAL$ selects the most representative pseudo errors that match the distribution of ground-truth errors along the decision boundary. Our code is publicly available at https://github.com/withchencheng/ECML_PKDD_23_Real.
</details></li>
</ul>
<hr>
<h2 id="OpenClinicalAI-An-Open-and-Dynamic-Model-for-Alzheimer’s-Disease-Diagnosis"><a href="#OpenClinicalAI-An-Open-and-Dynamic-Model-for-Alzheimer’s-Disease-Diagnosis" class="headerlink" title="OpenClinicalAI: An Open and Dynamic Model for Alzheimer’s Disease Diagnosis"></a>OpenClinicalAI: An Open and Dynamic Model for Alzheimer’s Disease Diagnosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00965">http://arxiv.org/abs/2307.00965</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunyou Huang, Xiaoshuang Liang, Xiangjiang Lu, Xiuxia Miao, Jiyue Xie, Wenjing Liu, Fan Zhang, Guoxin Kang, Li Ma, Suqin Tang, Zhifei Zhang, Jianfeng Zhan</li>
<li>for: 这个研究旨在提出一个可以应对现实临床设置的普遍性阿尔茨heimer病诊断系统，以提高现有医疗系统中的诊断效率和准确性。</li>
<li>methods: 本研究使用了开放式临床AI（OpenClinicalAI），融合了相互关联的深度多动作征学学习（DMARL）和多中心遗传学习（MCML），以动态形成诊断策略和提供诊断结果，以应对现实临床设置中的不确定和多元性。</li>
<li>results: 实验结果显示，OpenClinicalAI 比前一代模型具有更好的性能和较少的临床检查次数。<details>
<summary>Abstract</summary>
Although Alzheimer's disease (AD) cannot be reversed or cured, timely diagnosis can significantly reduce the burden of treatment and care. Current research on AD diagnosis models usually regards the diagnosis task as a typical classification task with two primary assumptions: 1) All target categories are known a priori; 2) The diagnostic strategy for each patient is consistent, that is, the number and type of model input data for each patient are the same. However, real-world clinical settings are open, with complexity and uncertainty in terms of both subjects and the resources of the medical institutions. This means that diagnostic models may encounter unseen disease categories and need to dynamically develop diagnostic strategies based on the subject's specific circumstances and available medical resources. Thus, the AD diagnosis task is tangled and coupled with the diagnosis strategy formulation. To promote the application of diagnostic systems in real-world clinical settings, we propose OpenClinicalAI for direct AD diagnosis in complex and uncertain clinical settings. This is the first powerful end-to-end model to dynamically formulate diagnostic strategies and provide diagnostic results based on the subject's conditions and available medical resources. OpenClinicalAI combines reciprocally coupled deep multiaction reinforcement learning (DMARL) for diagnostic strategy formulation and multicenter meta-learning (MCML) for open-set recognition. The experimental results show that OpenClinicalAI achieves better performance and fewer clinical examinations than the state-of-the-art model. Our method provides an opportunity to embed the AD diagnostic system into the current health care system to cooperate with clinicians to improve current health care.
</details>
<details>
<summary>摘要</summary>
although Alzheimer's disease (AD) cannot be reversed or cured, timely diagnosis can significantly reduce the burden of treatment and care. Current research on AD diagnosis models usually regards the diagnosis task as a typical classification task with two primary assumptions: 1) all target categories are known a priori; 2) the diagnostic strategy for each patient is consistent, that is, the number and type of model input data for each patient are the same. However, real-world clinical settings are open, with complexity and uncertainty in terms of both subjects and the resources of the medical institutions. This means that diagnostic models may encounter unseen disease categories and need to dynamically develop diagnostic strategies based on the subject's specific circumstances and available medical resources. Thus, the AD diagnosis task is tangled and coupled with the diagnosis strategy formulation. To promote the application of diagnostic systems in real-world clinical settings, we propose OpenClinicalAI for direct AD diagnosis in complex and uncertain clinical settings. This is the first powerful end-to-end model to dynamically formulate diagnostic strategies and provide diagnostic results based on the subject's conditions and available medical resources. OpenClinicalAI combines reciprocally coupled deep multiaction reinforcement learning (DMARL) for diagnostic strategy formulation and multicenter meta-learning (MCML) for open-set recognition. The experimental results show that OpenClinicalAI achieves better performance and fewer clinical examinations than the state-of-the-art model. Our method provides an opportunity to embed the AD diagnostic system into the current health care system to cooperate with clinicians to improve current health care.
</details></li>
</ul>
<hr>
<h2 id="A-Dual-Stealthy-Backdoor-From-Both-Spatial-and-Frequency-Perspectives"><a href="#A-Dual-Stealthy-Backdoor-From-Both-Spatial-and-Frequency-Perspectives" class="headerlink" title="A Dual Stealthy Backdoor: From Both Spatial and Frequency Perspectives"></a>A Dual Stealthy Backdoor: From Both Spatial and Frequency Perspectives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10184">http://arxiv.org/abs/2307.10184</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yudong Gao, Honglong Chen, Peng Sun, Junjian Li, Anqing Zhang, Zhibo Wang</li>
<li>for: 防止深度神经网络（DNN）受到后门攻击</li>
<li>methods: 利用Discrete Wavelet Transform和Fourier Transform等方法实现隐藏后门攻击</li>
<li>results: 对四个预测集进行了广泛测试，并获得了较高的攻击成功率和隐藏性<details>
<summary>Abstract</summary>
Backdoor attacks pose serious security threats to deep neural networks (DNNs). Backdoored models make arbitrarily (targeted) incorrect predictions on inputs embedded with well-designed triggers while behaving normally on clean inputs. Many works have explored the invisibility of backdoor triggers to improve attack stealthiness. However, most of them only consider the invisibility in the spatial domain without explicitly accounting for the generation of invisible triggers in the frequency domain, making the generated poisoned images be easily detected by recent defense methods. To address this issue, in this paper, we propose a DUal stealthy BAckdoor attack method named DUBA, which simultaneously considers the invisibility of triggers in both the spatial and frequency domains, to achieve desirable attack performance, while ensuring strong stealthiness. Specifically, we first use Discrete Wavelet Transform to embed the high-frequency information of the trigger image into the clean image to ensure attack effectiveness. Then, to attain strong stealthiness, we incorporate Fourier Transform and Discrete Cosine Transform to mix the poisoned image and clean image in the frequency domain. Moreover, the proposed DUBA adopts a novel attack strategy, in which the model is trained with weak triggers and attacked with strong triggers to further enhance the attack performance and stealthiness. We extensively evaluate DUBA against popular image classifiers on four datasets. The results demonstrate that it significantly outperforms the state-of-the-art backdoor attacks in terms of the attack success rate and stealthiness
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）受到后门攻击的安全威胁。后门模型会在嵌入了高效设计的触发器的输入上提供targeted incorrect predictions，而不会对干净输入产生影响。许多研究探讨了后门触发器的隐藏性，但大多数只是在空间领域内不显式地考虑了生成隐藏的触发器，使得生成的毒素图像可以轻松地被现有的防御方法检测。为解决这个问题，在这篇论文中，我们提出了DUal stealthy BAckdoor attack方法（DUBA），该方法同时考虑了触发器在空间和频域内的隐藏性，以实现desirable的攻击性能，同时保证强大的隐身性。具体来说，我们首先使用Discrete Wavelet Transform将高频信息 embed到了干净图像中，以确保攻击效果。然后，为了进一步增强隐身性，我们使用Fourier Transform和Discrete Cosine Transform将毒素图像和干净图像混合在频域中。此外，我们提出的DUBA采用了一种新的攻击策略，在该策略中，模型被训练使用弱触发器，并在攻击时使用强触发器，以进一步提高攻击性能和隐身性。我们对四个数据集进行了广泛的测试，结果显示，DUBA可以具有较高的攻击成功率和隐身性。
</details></li>
</ul>
<hr>
<h2 id="Neural-Architecture-Transfer-2-A-Paradigm-for-Improving-Efficiency-in-Multi-Objective-Neural-Architecture-Search"><a href="#Neural-Architecture-Transfer-2-A-Paradigm-for-Improving-Efficiency-in-Multi-Objective-Neural-Architecture-Search" class="headerlink" title="Neural Architecture Transfer 2: A Paradigm for Improving Efficiency in Multi-Objective Neural Architecture Search"></a>Neural Architecture Transfer 2: A Paradigm for Improving Efficiency in Multi-Objective Neural Architecture Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00960">http://arxiv.org/abs/2307.00960</a></li>
<li>repo_url: None</li>
<li>paper_authors: Simone Sarti, Eugenio Lomurno, Matteo Matteucci</li>
<li>for: 本研究旨在提高Neural Architecture Search（NAS）技术的效率和计算资源利用率，以便在各种任务上建立高性能的人工神经网络。</li>
<li>methods: 本文提出了Once-For-All（OFA）和其 successor Once-For-All-2（OFAv2）技术，以及Neural Architecture Transfer（NAT）技术，用于自动设计任务优化的人工神经网络。</li>
<li>results: 本研究表明，NATv2可以成功地改进NAT，并在多目标搜索算法应用于动态超网络架构时实现质量提升。<details>
<summary>Abstract</summary>
Deep learning is increasingly impacting various aspects of contemporary society. Artificial neural networks have emerged as the dominant models for solving an expanding range of tasks. The introduction of Neural Architecture Search (NAS) techniques, which enable the automatic design of task-optimal networks, has led to remarkable advances. However, the NAS process is typically associated with long execution times and significant computational resource requirements. Once-For-All (OFA) and its successor, Once-For-All-2 (OFAv2), have been developed to mitigate these challenges. While maintaining exceptional performance and eliminating the need for retraining, they aim to build a single super-network model capable of directly extracting sub-networks satisfying different constraints. Neural Architecture Transfer (NAT) was developed to maximise the effectiveness of extracting sub-networks from a super-network. In this paper, we present NATv2, an extension of NAT that improves multi-objective search algorithms applied to dynamic super-network architectures. NATv2 achieves qualitative improvements in the extractable sub-networks by exploiting the improved super-networks generated by OFAv2 and incorporating new policies for initialisation, pre-processing and updating its networks archive. In addition, a post-processing pipeline based on fine-tuning is introduced. Experimental results show that NATv2 successfully improves NAT and is highly recommended for investigating high-performance architectures with a minimal number of parameters.
</details>
<details>
<summary>摘要</summary>
深度学习在当代社会中越来越有影响。人工神经网络已经成为解决越来越多任务的主导模型。 introduce Neural Architecture Search（NAS）技术，它可以自动设计适应任务的网络，导致了非常的进步。然而，NAS过程通常具有较长的执行时间和较大的计算资源需求。Once-For-All（OFA）和其 successor Once-For-All-2（OFAv2）已经开发出来了，以解决这些挑战。它们希望建立一个单一的超网络模型，可以直接提取满足不同约束的子网络。Neural Architecture Transfer（NAT）被开发出来，以 maximize the effectiveness of extracting sub-networks from a super-network。在这篇论文中，我们提出NATv2，它是NAT的扩展，通过在OFAv2生成的改进的超网络和新的初始化、预处理和更新网络archive的策略来提高可提取的子网络质量。此外，我们还提出了一个基于练习的后处理管道。实验结果表明，NATv2成功地提高了NAT，并且在具有最小参数数量的情况下提供了高性能的建议。
</details></li>
</ul>
<hr>
<h2 id="Learning-Difference-Equations-with-Structured-Grammatical-Evolution-for-Postprandial-Glycaemia-Prediction"><a href="#Learning-Difference-Equations-with-Structured-Grammatical-Evolution-for-Postprandial-Glycaemia-Prediction" class="headerlink" title="Learning Difference Equations with Structured Grammatical Evolution for Postprandial Glycaemia Prediction"></a>Learning Difference Equations with Structured Grammatical Evolution for Postprandial Glycaemia Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01238">http://arxiv.org/abs/2307.01238</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Parra, David Joedicke, J. Manuel Velasco, Gabriel Kronberger, J. Ignacio Hidalgo</li>
<li>for: 这项研究旨在提供一种可解释的血糖预测方法，以帮助患有 диабе吗的人更好地控制血糖水平。</li>
<li>methods: 该方法基于 Interpretable Sparse Identification by Grammatical Evolution 技术，并结合了之前的准备阶段。它提供了 finite difference equations，用于预测在吃过食物后两个小时内血糖水平的变化。</li>
<li>results: 该方法可以提供安全且准确的预测结果，而无需放弃可解释性。与其他方法相比，该方法在准确性和可解释性之间协调得更好，提供了一个有前途的方法 для血糖预测。<details>
<summary>Abstract</summary>
People with diabetes must carefully monitor their blood glucose levels, especially after eating. Blood glucose regulation requires a proper combination of food intake and insulin boluses. Glucose prediction is vital to avoid dangerous post-meal complications in treating individuals with diabetes. Although traditional methods, such as artificial neural networks, have shown high accuracy rates, sometimes they are not suitable for developing personalised treatments by physicians due to their lack of interpretability. In this study, we propose a novel glucose prediction method emphasising interpretability: Interpretable Sparse Identification by Grammatical Evolution. Combined with a previous clustering stage, our approach provides finite difference equations to predict postprandial glucose levels up to two hours after meals. We divide the dataset into four-hour segments and perform clustering based on blood glucose values for the twohour window before the meal. Prediction models are trained for each cluster for the two-hour windows after meals, allowing predictions in 15-minute steps, yielding up to eight predictions at different time horizons. Prediction safety was evaluated based on Parkes Error Grid regions. Our technique produces safe predictions through explainable expressions, avoiding zones D (0.2% average) and E (0%) and reducing predictions on zone C (6.2%). In addition, our proposal has slightly better accuracy than other techniques, including sparse identification of non-linear dynamics and artificial neural networks. The results demonstrate that our proposal provides interpretable solutions without sacrificing prediction accuracy, offering a promising approach to glucose prediction in diabetes management that balances accuracy, interpretability, and computational efficiency.
</details>
<details>
<summary>摘要</summary>
人们有糖尿病必须仔细监测血糖水平，特别是 после吃食。血糖补做需要合适的食物摄取和人工胰岛素注射。预测血糖水平是对治疗糖尿病患者的生命关键。传统方法，如人工神经网络，已经显示高准确率，但是由于其解释性不足，不适用于个性化治疗。在本研究中，我们提出了一种新的血糖预测方法，强调解释性：可解释的稀缺特征识别。结合之前的划分阶段，我们的方法提供了finite difference方程来预测午餐后血糖水平。我们将数据分成四个时间段，并基于吃食前两个小时的血糖值进行划分。预测模型在每个群中训练，以预测午餐后两个小时内的血糖水平，每步预测15分钟，共八个预测。预测安全性评估基于公钵环境区域。我们的方法生成安全的预测，避免了区域D（0.2%的平均值）和区域E（0%），并减少了区域C（6.2%）。此外，我们的提议的准确性略高于其他技术，包括稀缺特征识别非线性动力学和人工神经网络。结果表明，我们的提议可以提供可解释的解决方案，不 sacrificing预测准确性，为糖尿病管理提供了可能的平衡。
</details></li>
</ul>
<hr>
<h2 id="Dynamical-Graph-Echo-State-Networks-with-Snapshot-Merging-for-Dissemination-Process-Classification"><a href="#Dynamical-Graph-Echo-State-Networks-with-Snapshot-Merging-for-Dissemination-Process-Classification" class="headerlink" title="Dynamical Graph Echo State Networks with Snapshot Merging for Dissemination Process Classification"></a>Dynamical Graph Echo State Networks with Snapshot Merging for Dissemination Process Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01237">http://arxiv.org/abs/2307.01237</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziqiang Li, Kantaro Fujiwara, Gouhei Tanaka</li>
<li>for: 本研究主要针对的是 temporally graph classification 问题，尤其是在 community 中的信息或疾病传播模式的分类。</li>
<li>methods: 本研究提出了一种 combining snapshot merging 策略和 Dynamical Graph Echo State Network (DynGESN) 模型来处理 temporally graph classification 任务。 snapshot merging 策略用于在不同时刻 merge 邻居快照，以获得更多的 spatiotemporal 特征；而 DynGESN 模型则用于 capture 这些特征。</li>
<li>results: 实验结果显示，对六个 benchmark DPC 数据集进行测试，本研究的提出的模型在 classification 性能方面占据了优势，比 DynGESN 和一些基于核函数的模型更好。<details>
<summary>Abstract</summary>
The Dissemination Process Classification (DPC) is a popular application of temporal graph classification. The aim of DPC is to classify different spreading patterns of information or pestilence within a community represented by discrete-time temporal graphs. Recently, a reservoir computing-based model named Dynamical Graph Echo State Network (DynGESN) has been proposed for processing temporal graphs with relatively high effectiveness and low computational costs. In this study, we propose a novel model which combines a novel data augmentation strategy called snapshot merging with the DynGESN for dealing with DPC tasks. In our model, the snapshot merging strategy is designed for forming new snapshots by merging neighboring snapshots over time, and then multiple reservoir encoders are set for capturing spatiotemporal features from merged snapshots. After those, the logistic regression is adopted for decoding the sum-pooled embeddings into the classification results. Experimental results on six benchmark DPC datasets show that our proposed model has better classification performances than the DynGESN and several kernel-based models.
</details>
<details>
<summary>摘要</summary>
《信息或疫病传播过程分类（DPC）应用》是一个广泛使用的时间图分类应用。DPC的目标是根据社区的时间图来分类不同的信息或疫病传播模式。近些年，一种基于储存计算机（reservoir computing）的模型named Dynamical Graph Echo State Network（DynGESN）已经被提出来处理时间图。在本研究中，我们提出了一种新的模型，该模型将Snapshot Merging策略与DynGESN相结合，用于处理DPC任务。在我们的模型中，Snapshot Merging策略是用于在时间上邻居图像合并，并将多个储存编码器用于捕捉时间图的空间特征。然后，逻辑回归被采用来将汇聚编码器的输出编码为分类结果。在六个标准DPC数据集上进行实验，我们的提出的模型的分类性能比DynGESN和一些核心基于模型更好。
</details></li>
</ul>
<hr>
<h2 id="Rockmate-an-Efficient-Fast-Automatic-and-Generic-Tool-for-Re-materialization-in-PyTorch"><a href="#Rockmate-an-Efficient-Fast-Automatic-and-Generic-Tool-for-Re-materialization-in-PyTorch" class="headerlink" title="Rockmate: an Efficient, Fast, Automatic and Generic Tool for Re-materialization in PyTorch"></a>Rockmate: an Efficient, Fast, Automatic and Generic Tool for Re-materialization in PyTorch</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01236">http://arxiv.org/abs/2307.01236</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/topal-team/rockmate">https://github.com/topal-team/rockmate</a></li>
<li>paper_authors: Xunyi Zhao, Théotime Le Hellard, Lionel Eyraud, Julia Gusak, Olivier Beaumont</li>
<li>for: 这篇论文的目的是提出一个名为 Rockmate 的自动化工具，用于控制 PyTorch DNN 模型的记忆需求。</li>
<li>methods: Rockmate 使用一种自动检测模型的 Computational 和 Data 相依性结构，并将其转换为一系列复杂的封页，以控制记忆需求。</li>
<li>results: 经过实验显示，Rockmate 能够与 Checkmate 和 Rotor 相比，具有相似的速度和效率，并且可以在许多模型中获得较低的记忆需求（具体比例为 2-5），仅带来一定的开销（约 10%-20%）。<details>
<summary>Abstract</summary>
We propose Rockmate to control the memory requirements when training PyTorch DNN models. Rockmate is an automatic tool that starts from the model code and generates an equivalent model, using a predefined amount of memory for activations, at the cost of a few re-computations. Rockmate automatically detects the structure of computational and data dependencies and rewrites the initial model as a sequence of complex blocks. We show that such a structure is widespread and can be found in many models in the literature (Transformer based models, ResNet, RegNets,...). This structure allows us to solve the problem in a fast and efficient way, using an adaptation of Checkmate (too slow on the whole model but general) at the level of individual blocks and an adaptation of Rotor (fast but limited to sequential models) at the level of the sequence itself. We show through experiments on many models that Rockmate is as fast as Rotor and as efficient as Checkmate, and that it allows in many cases to obtain a significantly lower memory consumption for activations (by a factor of 2 to 5) for a rather negligible overhead (of the order of 10% to 20%). Rockmate is open source and available at https://github.com/topal-team/rockmate.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="OpenAPMax-Abnormal-Patterns-based-Model-for-Real-World-Alzheimer’s-Disease-Diagnosis"><a href="#OpenAPMax-Abnormal-Patterns-based-Model-for-Real-World-Alzheimer’s-Disease-Diagnosis" class="headerlink" title="OpenAPMax: Abnormal Patterns-based Model for Real-World Alzheimer’s Disease Diagnosis"></a>OpenAPMax: Abnormal Patterns-based Model for Real-World Alzheimer’s Disease Diagnosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00936">http://arxiv.org/abs/2307.00936</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunyou Huang, Xianglong Guan, Xiangjiang Lu, Xiaoshuang Liang, Xiuxia Miao, Jiyue Xie, Wenjing Liu, Li Ma, Suqin Tang, Zhifei Zhang, Jianfeng Zhan</li>
<li>for: The paper aims to address the challenges of Alzheimer’s disease (AD) diagnosis in real-world settings, particularly the open-set recognition problem where the known categories are not fixed and can change over time.</li>
<li>methods: The proposed method, OpenAPMax, uses an anomaly pattern-based approach to model the distance between each patient’s abnormal pattern and the center of their category, and modifies the classification probability using extreme value theory (EVT).</li>
<li>results: The proposed method achieves state-of-the-art results in open-set recognition, outperforming recent open-set recognition methods.Here’s the Simplified Chinese text format for the three key points:</li>
<li>for: 本文旨在解决阿尔ц海默病（AD）诊断在实际设置下的挑战，特别是开集识别问题，where the known categories are not fixed and can change over time.</li>
<li>methods: 提议方法基于异常模式的方法，使用极值理论（EVT）来模型每个患者的异常模式和其分类概率。</li>
<li>results: 提议方法达到开集识别领域的州先Result，超越最近的开集识别方法。I hope this helps!<details>
<summary>Abstract</summary>
Alzheimer's disease (AD) cannot be reversed, but early diagnosis will significantly benefit patients' medical treatment and care. In recent works, AD diagnosis has the primary assumption that all categories are known a prior -- a closed-set classification problem, which contrasts with the open-set recognition problem. This assumption hinders the application of the model in natural clinical settings. Although many open-set recognition technologies have been proposed in other fields, they are challenging to use for AD diagnosis directly since 1) AD is a degenerative disease of the nervous system with similar symptoms at each stage, and it is difficult to distinguish from its pre-state, and 2) diversified strategies for AD diagnosis are challenging to model uniformly. In this work, inspired by the concerns of clinicians during diagnosis, we propose an open-set recognition model, OpenAPMax, based on the anomaly pattern to address AD diagnosis in real-world settings. OpenAPMax first obtains the abnormal pattern of each patient relative to each known category through statistics or a literature search, clusters the patients' abnormal pattern, and finally, uses extreme value theory (EVT) to model the distance between each patient's abnormal pattern and the center of their category and modify the classification probability. We evaluate the performance of the proposed method with recent open-set recognition, where we obtain state-of-the-art results.
</details>
<details>
<summary>摘要</summary>
阿尔茨heimer病 (AD) 不能 reversed，但早期诊断将有利于患者的医疗和护理。在最近的研究中，AD 诊断假设所有类别都是已知的，一个closed-set classification problem，与开放式认知问题不同。这种假设限制了模型在自然临床设置中的应用。虽然许多开放式认知技术在其他领域已经提出，但它们在AD诊断直接应用很困难，因为1）AD 是 nervious system 的逐步衰变病，表现类似，难以与其预状分 distinguish，2）AD 诊断策略多样化难以统一模型。在这种情况下，我们以临床医生在诊断过程中的关注为导向，提出一种开放式认知模型，OpenAPMax，基于异常模式来解决AD诊断在实际设置中。OpenAPMax 首先通过统计或文献搜索获取每个患者的异常模式，对每个患者的异常模式进行归类，最后使用极值理论 (EVT) 来模型每个患者的异常模式与其分类概率的距离。我们对最近的开放式认知方法进行评估，并获得了状态机器人的结果。
</details></li>
</ul>
<hr>
<h2 id="Learning-Differentiable-Logic-Programs-for-Abstract-Visual-Reasoning"><a href="#Learning-Differentiable-Logic-Programs-for-Abstract-Visual-Reasoning" class="headerlink" title="Learning Differentiable Logic Programs for Abstract Visual Reasoning"></a>Learning Differentiable Logic Programs for Abstract Visual Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00928">http://arxiv.org/abs/2307.00928</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ml-research/neumann">https://github.com/ml-research/neumann</a></li>
<li>paper_authors: Hikaru Shindo, Viktor Pfanschilling, Devendra Singh Dhami, Kristian Kersting<br>for:The paper is written for building intelligent agents that can perform visual reasoning and solve problem-solving tasks beyond perception.methods:The paper proposes a graph-based differentiable forward reasoner called NEUMANN, which passes messages in a memory-efficient manner and handles structured programs with functors. Additionally, the paper proposes a computationally-efficient structure learning algorithm for explanatory program induction on complex visual scenes.results:The paper demonstrates that NEUMANN outperforms neural, symbolic, and neuro-symbolic baselines in visual reasoning tasks, including a new task called “visual reasoning behind-the-scenes” that requires agents to learn abstract programs and answer queries by imagining scenes that are not observed.<details>
<summary>Abstract</summary>
Visual reasoning is essential for building intelligent agents that understand the world and perform problem-solving beyond perception. Differentiable forward reasoning has been developed to integrate reasoning with gradient-based machine learning paradigms. However, due to the memory intensity, most existing approaches do not bring the best of the expressivity of first-order logic, excluding a crucial ability to solve abstract visual reasoning, where agents need to perform reasoning by using analogies on abstract concepts in different scenarios. To overcome this problem, we propose NEUro-symbolic Message-pAssiNg reasoNer (NEUMANN), which is a graph-based differentiable forward reasoner, passing messages in a memory-efficient manner and handling structured programs with functors. Moreover, we propose a computationally-efficient structure learning algorithm to perform explanatory program induction on complex visual scenes. To evaluate, in addition to conventional visual reasoning tasks, we propose a new task, visual reasoning behind-the-scenes, where agents need to learn abstract programs and then answer queries by imagining scenes that are not observed. We empirically demonstrate that NEUMANN solves visual reasoning tasks efficiently, outperforming neural, symbolic, and neuro-symbolic baselines.
</details>
<details>
<summary>摘要</summary>
Visual reasoning 是建立智能代理的关键，它允许代理人理解世界并解决问题，超出感知。difficult forward reasoning 已经开发来整合reasoning 与梯度基于机器学习 парадигмы。然而，由于内存投入，大多数现有方法不能充分发挥逻辑表达力，排除了一个关键的能力：解决抽象视觉逻辑，代理人需要通过对抽象概念的比较来解决问题。为了解决这个问题，我们提出了 NEUro-symbolic Message-pAssiNg reasoNer (NEUMANN)，它是一个图形基的difficult forward reasoner，通过Message passing 来减少内存占用，并处理结构化程序。此外，我们还提出了一种 Computationally-efficient 的结构学习算法，用于在复杂视觉场景中进行解释性程序induction。为了评估 NEUMANN 的性能，我们提出了一个新任务：视觉逻辑后台，代理人需要学习抽象程序，然后回答问题，想象出未见的场景。我们的实验结果表明，NEUMANN 可以高效解决视觉逻辑任务，超过 neural、symbolic 和 neuro-symbolic 基elines。
</details></li>
</ul>
<hr>
<h2 id="Semi-supervised-multi-view-concept-decomposition"><a href="#Semi-supervised-multi-view-concept-decomposition" class="headerlink" title="Semi-supervised multi-view concept decomposition"></a>Semi-supervised multi-view concept decomposition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00924">http://arxiv.org/abs/2307.00924</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qi Jiang, Guoxu Zhou, Qibin Zhao</li>
<li>for: 提高多视图数据表示性和含义检索性</li>
<li>methods: 基于kernel方法学习 latent表示，并结合多视图CF、标签传播和抽象学习，实现数据表示的更好化</li>
<li>results: 在四个多样化的数据集上，经验表明 SMVCF 模型在多视图归一化任务中显著提高表示性和准确率<details>
<summary>Abstract</summary>
Concept Factorization (CF), as a novel paradigm of representation learning, has demonstrated superior performance in multi-view clustering tasks. It overcomes limitations such as the non-negativity constraint imposed by traditional matrix factorization methods and leverages kernel methods to learn latent representations that capture the underlying structure of the data, thereby improving data representation. However, existing multi-view concept factorization methods fail to consider the limited labeled information inherent in real-world multi-view data. This often leads to significant performance loss. To overcome these limitations, we propose a novel semi-supervised multi-view concept factorization model, named SMVCF. In the SMVCF model, we first extend the conventional single-view CF to a multi-view version, enabling more effective exploration of complementary information across multiple views. We then integrate multi-view CF, label propagation, and manifold learning into a unified framework to leverage and incorporate valuable information present in the data. Additionally, an adaptive weight vector is introduced to balance the importance of different views in the clustering process. We further develop targeted optimization methods specifically tailored for the SMVCF model. Finally, we conduct extensive experiments on four diverse datasets with varying label ratios to evaluate the performance of SMVCF. The experimental results demonstrate the effectiveness and superiority of our proposed approach in multi-view clustering tasks.
</details>
<details>
<summary>摘要</summary>
《概念分解（CF）》是一种新的表示学习 paradigm，在多视图划分任务中表现出了更高的性能。它超越了传统的矩阵分解方法中的非正式约束，并利用核函数方法学习 latent 表示，以捕捉数据下面的结构，从而改善数据表示。然而，现有的多视图概念分解方法通常不考虑实际世界中多视图数据中的有限 labels 信息。这经常导致显著的性能损失。为了解决这些限制，我们提出了一种新的半upervised多视图概念分解模型，名为 SMVCF。在 SMVCF 模型中，我们首先将传统的单视图 CF 扩展到多视图版本，以更好地利用多个视图之间的补做信息。然后，我们将多视图 CF、标签传播和抽象学习集成到一个统一框架中，以利用数据中存在的有价值信息。此外，我们还引入了一个 adaptive  веctor 来衡量不同视图在划分过程中的重要性。最后，我们开发了特定于 SMVCF 模型的目标优化方法。我们进行了对四个多样化的数据集进行了广泛的实验，以评估 SMVCF 模型在多视图划分任务中的性能。实验结果表明，我们提出的方法在多视图划分任务中表现出了更高的有效性和优势。
</details></li>
</ul>
<hr>
<h2 id="Achieving-Stable-Training-of-Reinforcement-Learning-Agents-in-Bimodal-Environments-through-Batch-Learning"><a href="#Achieving-Stable-Training-of-Reinforcement-Learning-Agents-in-Bimodal-Environments-through-Batch-Learning" class="headerlink" title="Achieving Stable Training of Reinforcement Learning Agents in Bimodal Environments through Batch Learning"></a>Achieving Stable Training of Reinforcement Learning Agents in Bimodal Environments through Batch Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00923">http://arxiv.org/abs/2307.00923</a></li>
<li>repo_url: None</li>
<li>paper_authors: E. Hurwitz, N. Peace, G. Cevora</li>
<li>for: 解决 tabular Q-learning 问题中的�iumodal, 随机环境挑战</li>
<li>methods: 使用批处理更新方法</li>
<li>results: 比较 typically 更新和批处理学习agent，批处理学习agent更高效、更具抗随机环境能力<details>
<summary>Abstract</summary>
Bimodal, stochastic environments present a challenge to typical Reinforcement Learning problems. This problem is one that is surprisingly common in real world applications, being particularly applicable to pricing problems. In this paper we present a novel learning approach to the tabular Q-learning algorithm, tailored to tackling these specific challenges by using batch updates. A simulation of pricing problem is used as a testbed to compare a typically updated agent with a batch learning agent. The batch learning agents are shown to be both more effective than the typically-trained agents, and to be more resilient to the fluctuations in a large stochastic environment. This work has a significant potential to enable practical, industrial deployment of Reinforcement Learning in the context of pricing and others.
</details>
<details>
<summary>摘要</summary>
bisimodal, 随机环境会对传统的奖励学习问题提出挑战。这种问题在实际应用中很普遍，特别适用于价格问题。在这篇论文中，我们提出了一种新的学习方法，用于修改标准的Q学习算法，以适应这些特定挑战。我们使用了批处理更新来适应这种随机环境。我们通过对比一个通常更新的代理和批处理学习代理的测试，显示了批处理学习代理在大型随机环境中更加有效和更加鲁棒。这项工作具有实用化奖励学习在价格和其他领域的潜在应用潜力。
</details></li>
</ul>
<hr>
<h2 id="Quantum-Machine-Learning-on-Near-Term-Quantum-Devices-Current-State-of-Supervised-and-Unsupervised-Techniques-for-Real-World-Applications"><a href="#Quantum-Machine-Learning-on-Near-Term-Quantum-Devices-Current-State-of-Supervised-and-Unsupervised-Techniques-for-Real-World-Applications" class="headerlink" title="Quantum Machine Learning on Near-Term Quantum Devices: Current State of Supervised and Unsupervised Techniques for Real-World Applications"></a>Quantum Machine Learning on Near-Term Quantum Devices: Current State of Supervised and Unsupervised Techniques for Real-World Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00908">http://arxiv.org/abs/2307.00908</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yaswitha Gujju, Atsushi Matsuo, Rudy Raymond</li>
<li>for: 本文主要关注在真正的量子硬件上实现量子机器学习（QML）应用，以实现量子优势。</li>
<li>methods: 本文探讨了目前量子硬件上QML实现的Current Limitations，并提出了多种缓解这些限制的技术，如编码技术、架构结构、错误纠正和梯度方法。</li>
<li>results: 本文评估了这些QML实现的性能，并与其经典对手进行比较。最后，本文提出了将来缓解量子机器学习应用在真正量子硬件上的挑战。Here’s the translation of the paper’s abstract in Simplified Chinese:</li>
<li>for: 本文主要关注在真正的量子硬件上实现量子机器学习（QML）应用，以实现量子优势。</li>
<li>methods: 本文探讨了目前量子硬件上QML实现的Current Limitations，并提出了多种缓解这些限制的技术，如编码技术、架构结构、错误纠正和梯度方法。</li>
<li>results: 本文评估了这些QML实现的性能，并与其经典对手进行比较。最后，本文提出了将来缓解量子机器学习应用在真正量子硬件上的挑战。<details>
<summary>Abstract</summary>
The past decade has seen considerable progress in quantum hardware in terms of the speed, number of qubits and quantum volume which is defined as the maximum size of a quantum circuit that can be effectively implemented on a near-term quantum device. Consequently, there has also been a rise in the number of works based on the applications of Quantum Machine Learning (QML) on real hardware to attain quantum advantage over their classical counterparts. In this survey, our primary focus is on selected supervised and unsupervised learning applications implemented on quantum hardware, specifically targeting real-world scenarios. Our survey explores and highlights the current limitations of QML implementations on quantum hardware. We delve into various techniques to overcome these limitations, such as encoding techniques, ansatz structure, error mitigation, and gradient methods. Additionally, we assess the performance of these QML implementations in comparison to their classical counterparts. Finally, we conclude our survey with a discussion on the existing bottlenecks associated with applying QML on real quantum devices and propose potential solutions for overcoming these challenges in the future.
</details>
<details>
<summary>摘要</summary>
过去一个十年，量子硬件在速度、量子比特数和量子体积方面有所进步，量子机器学习（QML）在真实硬件上实现的应用工作也有所增加。在这份报告中，我们主要关注选择性supervised和无监督学习应用程序在量子硬件上的实现，特别是面向实际场景。我们的报告探讨和强调当前量子硬件QML实现的限制，包括编码技术、架构结构、错误缓冲和梯度方法。此外，我们评估这些QML实现与其经典对手的性能。最后，我们结束报告，讨论现有量子硬件上应用QML的瓶颈，并提出未来缓解这些挑战的可能性。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-the-Robustness-of-QMIX-against-State-adversarial-Attacks"><a href="#Enhancing-the-Robustness-of-QMIX-against-State-adversarial-Attacks" class="headerlink" title="Enhancing the Robustness of QMIX against State-adversarial Attacks"></a>Enhancing the Robustness of QMIX against State-adversarial Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00907">http://arxiv.org/abs/2307.00907</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weiran Guo, Guanjun Liu, Ziyuan Zhou, Ling Wang, Jiacun Wang</li>
<li>for: 本研究旨在提高多智能体强化学习（MARL）算法的Robustness，以适应状态恶化攻击（state-adversarial attacks）。</li>
<li>methods: 本研究使用QMIX算法作为例子，提出了四种方法来提高SARL算法的Robustness，包括：在训练阶段使用多种攻击，在训练阶段使用不同类型的攻击，使用混合攻击，以及在训练阶段使用随机攻击。</li>
<li>results: 通过对QMIX算法进行训练和测试，研究发现这些方法可以提高MARL算法的Robustness，使其能够更好地抵抗状态恶化攻击。<details>
<summary>Abstract</summary>
Deep reinforcement learning (DRL) performance is generally impacted by state-adversarial attacks, a perturbation applied to an agent's observation. Most recent research has concentrated on robust single-agent reinforcement learning (SARL) algorithms against state-adversarial attacks. Still, there has yet to be much work on robust multi-agent reinforcement learning. Using QMIX, one of the popular cooperative multi-agent reinforcement algorithms, as an example, we discuss four techniques to improve the robustness of SARL algorithms and extend them to multi-agent scenarios. To increase the robustness of multi-agent reinforcement learning (MARL) algorithms, we train models using a variety of attacks in this research. We then test the models taught using the other attacks by subjecting them to the corresponding attacks throughout the training phase. In this way, we organize and summarize techniques for enhancing robustness when used with MARL.
</details>
<details>
<summary>摘要</summary>
深度强化学习（DRL）性能通常受到状态敌意攻击的影响。最近的研究主要集中在对单机器学习（SARL）算法进行鲁棒性加固。然而，对多机器学习（MARL）算法的鲁棒性加固还很少研究。使用QMIX算法作为例子，本文讨论了对SARL算法进行四种技术提升鲁棒性的方法，并将其推广到多机器场景。为提高MARL算法的鲁棒性，我们在训练过程中使用多种攻击。然后，我们在训练阶段对模型进行测试，以验证它们是否能够抵抗对应的攻击。这样，我们可以系统地整理和总结提高MARL算法的鲁棒性技术。
</details></li>
</ul>
<hr>
<h2 id="Fixing-confirmation-bias-in-feature-attribution-methods-via-semantic-match"><a href="#Fixing-confirmation-bias-in-feature-attribution-methods-via-semantic-match" class="headerlink" title="Fixing confirmation bias in feature attribution methods via semantic match"></a>Fixing confirmation bias in feature attribution methods via semantic match</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00897">http://arxiv.org/abs/2307.00897</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giovanni Cinà, Daniel Fernandez-Llaneza, Nishant Mishra, Tabea E. Röber, Sandro Pezzelle, Iacer Calixto, Rob Goedhart, Ş. İlker Birbil</li>
<li>for: This paper aims to address the issue of confirmation bias in feature attribution methods for black box models, and to propose a structured approach to evaluate the semantic match between human concepts and the model’s explanations.</li>
<li>methods: The paper proposes a new approach called “semantic match” to evaluate the alignment between human concepts and the feature attributions generated by the model. This approach is based on a conceptual framework put forward in Cin`a et al. (2023).</li>
<li>results: The paper presents a suite of experiments using both tabular and image data to demonstrate the effectiveness of the proposed approach in identifying both desirable and undesirable model behaviors. The results show that the assessment of semantic match can provide valuable insights into the model’s internal representations and help to resolve the issue of confirmation bias in XAI.<details>
<summary>Abstract</summary>
Feature attribution methods have become a staple method to disentangle the complex behavior of black box models. Despite their success, some scholars have argued that such methods suffer from a serious flaw: they do not allow a reliable interpretation in terms of human concepts. Simply put, visualizing an array of feature contributions is not enough for humans to conclude something about a model's internal representations, and confirmation bias can trick users into false beliefs about model behavior. We argue that a structured approach is required to test whether our hypotheses on the model are confirmed by the feature attributions. This is what we call the "semantic match" between human concepts and (sub-symbolic) explanations. Building on the conceptual framework put forward in Cin\`a et al. [2023], we propose a structured approach to evaluate semantic match in practice. We showcase the procedure in a suite of experiments spanning tabular and image data, and show how the assessment of semantic match can give insight into both desirable (e.g., focusing on an object relevant for prediction) and undesirable model behaviors (e.g., focusing on a spurious correlation). We couple our experimental results with an analysis on the metrics to measure semantic match, and argue that this approach constitutes the first step towards resolving the issue of confirmation bias in XAI.
</details>
<details>
<summary>摘要</summary>
feature 归因方法已成为黑盒模型行为解释的标准方法。 despite their success, some scholars have argued that such methods suffer from a serious flaw: they do not allow a reliable interpretation in terms of human concepts. simply put, visualizing an array of feature contributions is not enough for humans to conclude something about a model's internal representations, and confirmation bias can trick users into false beliefs about model behavior. we argue that a structured approach is required to test whether our hypotheses on the model are confirmed by the feature attributions. this is what we call the "semantic match" between human concepts and (sub-symbolic) explanations. building on the conceptual framework put forward in Cin\`a et al. [2023], we propose a structured approach to evaluate semantic match in practice. we showcase the procedure in a suite of experiments spanning tabular and image data, and show how the assessment of semantic match can give insight into both desirable (e.g., focusing on an object relevant for prediction) and undesirable model behaviors (e.g., focusing on a spurious correlation). we couple our experimental results with an analysis on the metrics to measure semantic match, and argue that this approach constitutes the first step towards resolving the issue of confirmation bias in XAI.
</details></li>
</ul>
<hr>
<h2 id="Internet-of-Things-Fault-Detection-and-Classification-via-Multitask-Learning"><a href="#Internet-of-Things-Fault-Detection-and-Classification-via-Multitask-Learning" class="headerlink" title="Internet of Things Fault Detection and Classification via Multitask Learning"></a>Internet of Things Fault Detection and Classification via Multitask Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01234">http://arxiv.org/abs/2307.01234</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Arif Ul Alam</li>
<li>for: 这篇论文旨在开发一种适用于实际IIoT应用场景的错误检测和分类系统。</li>
<li>methods: 研究团队使用了实际IIoT系统进行三个阶段的数据收集，模拟了11种预定的故障类别。提出了SMTCNN方法用于IIoT故障检测和分类，并对实际数据进行评估。</li>
<li>results: SMTCNN方法在实际数据上达到了3.5%的特异性，并显著提高了精度、回归率和F1评价指标。<details>
<summary>Abstract</summary>
This paper presents a comprehensive investigation into developing a fault detection and classification system for real-world IIoT applications. The study addresses challenges in data collection, annotation, algorithm development, and deployment. Using a real-world IIoT system, three phases of data collection simulate 11 predefined fault categories. We propose SMTCNN for fault detection and category classification in IIoT, evaluating its performance on real-world data. SMTCNN achieves superior specificity (3.5%) and shows significant improvements in precision, recall, and F1 measures compared to existing techniques.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Fraunhofer-SIT-at-CheckThat-2023-Tackling-Classification-Uncertainty-Using-Model-Souping-on-the-Example-of-Check-Worthiness-Classification"><a href="#Fraunhofer-SIT-at-CheckThat-2023-Tackling-Classification-Uncertainty-Using-Model-Souping-on-the-Example-of-Check-Worthiness-Classification" class="headerlink" title="Fraunhofer SIT at CheckThat! 2023: Tackling Classification Uncertainty Using Model Souping on the Example of Check-Worthiness Classification"></a>Fraunhofer SIT at CheckThat! 2023: Tackling Classification Uncertainty Using Model Souping on the Example of Check-Worthiness Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02377">http://arxiv.org/abs/2307.02377</a></li>
<li>repo_url: None</li>
<li>paper_authors: Raphael Frick, Inna Vogel, Jeong-Eun Choi</li>
<li>for: 这个论文是为了解决政治辩论文本中是否需要进行复核的问题。</li>
<li>methods: 这个论文使用了Model Souping ensemble Classification scheme来解决这个问题。</li>
<li>results: 在英文数据集上，我们的提交模型达到了总F1分数0.878，在竞赛中排名第二。<details>
<summary>Abstract</summary>
This paper describes the second-placed approach developed by the Fraunhofer SIT team in the CLEF-2023 CheckThat! lab Task 1B for English. Given a text snippet from a political debate, the aim of this task is to determine whether it should be assessed for check-worthiness. Detecting check-worthy statements aims to facilitate manual fact-checking efforts by prioritizing the claims that fact-checkers should consider first. It can also be considered as primary step of a fact-checking system. Our best-performing method took advantage of an ensemble classification scheme centered on Model Souping. When applied to the English data set, our submitted model achieved an overall F1 score of 0.878 and was ranked as the second-best model in the competition.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "Check-worthiness" is 可验证性 (kě yàn zhèng xìng) in Simplified Chinese.* "Model Souping" is 模型汤 (molduō tāng) in Simplified Chinese, which is a play on words combining "model" and "soup" to refer to the ensemble of models used in the approach.* "F1 score" is 平均准确率 (píng jiān zhèng qiáng lǐ) in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Unbiased-Pain-Assessment-through-Wearables-and-EHR-Data-Multi-attribute-Fairness-Loss-based-CNN-Approach"><a href="#Unbiased-Pain-Assessment-through-Wearables-and-EHR-Data-Multi-attribute-Fairness-Loss-based-CNN-Approach" class="headerlink" title="Unbiased Pain Assessment through Wearables and EHR Data: Multi-attribute Fairness Loss-based CNN Approach"></a>Unbiased Pain Assessment through Wearables and EHR Data: Multi-attribute Fairness Loss-based CNN Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05333">http://arxiv.org/abs/2307.05333</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sharmin Sultana, Md Mahmudur Rahman, Atqiya Munawara Mahi, Shao-Hsien Liu, Mohammad Arif Ul Alam</li>
<li>for: 这个研究旨在开发一个可以处理不同数据类型（IoT、EHR和临床调查）的扩展可靠的人工智能（AI）系统，以找到痛症状态的物理、行为和心理指标。</li>
<li>methods: 这个研究使用了一个基于Convolutional Neural Networks（CNN）的多Attribute Fairness Loss（MAFL）模型，以考虑数据中可能包含的敏感特征，并对于不同群体进行公平的痛症评估。</li>
<li>results: 研究结果显示，对于不同群体的痛症评估，提出的MAFL模型能够优化精度和公平性的贡献，并且与现有的 Mitigation 方法相比，表现较好。使用NIH All-Of-US 数据，研究范例包括868名受试者，收集了1500天的数据，以分析提议的公平痛症评估系统。<details>
<summary>Abstract</summary>
The combination of diverse health data (IoT, EHR, and clinical surveys) and scalable-adaptable Artificial Intelligence (AI), has enabled the discovery of physical, behavioral, and psycho-social indicators of pain status. Despite the hype and promise to fundamentally alter the healthcare system with technological advancements, much AI adoption in clinical pain evaluation has been hampered by the heterogeneity of the problem itself and other challenges, such as personalization and fairness. Studies have revealed that many AI (i.e., machine learning or deep learning) models display biases and discriminate against specific population segments (such as those based on gender or ethnicity), which breeds skepticism among medical professionals about AI adaptability. In this paper, we propose a Multi-attribute Fairness Loss (MAFL) based CNN model that aims to account for any sensitive attributes included in the data and fairly predict patients' pain status while attempting to minimize the discrepancies between privileged and unprivileged groups. In order to determine whether the trade-off between accuracy and fairness can be satisfied, we compare the proposed model with well-known existing mitigation procedures, and studies reveal that the implemented model performs favorably in contrast to state-of-the-art methods. Utilizing NIH All-Of-US data, where a cohort of 868 distinct individuals with wearables and EHR data gathered over 1500 days has been taken into consideration to analyze our suggested fair pain assessment system.
</details>
<details>
<summary>摘要</summary>
“由多元健康数据（IoT、EHR和临床调查）和可扩展适应的人工智能（AI）的结合，已经发现了体征、行为和心理社会指标。尽管技术进步对健康领域的改革具有广泛的推广和承认，但AI在临床痛评估中的采纳受到了多重因素的影响，例如个人化和公平性。研究发现，许多AI（例如机器学习或深度学习）模型会带有偏见和歧视特定人群（例如根据性别或民族），这产生了医疗专业人员对AI适应性的怀疑。在这篇文章中，我们提出了基于多Attribute Fairness Loss（MAFL）的弹性神经网络模型，以减少权利层次中的差异。为了决定是否可以满足精确性和公平性之间的贸易，我们与已知的 Mitigation 程序进行比较，研究发现，我们的提案模型在与州域方法进行比较时表现较好。使用NIH All-Of-US数据，我们分析了我们建议的公平痛评估系统。”
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Multi-modal-Demand-Dynamics-During-Transport-System-Disruptions"><a href="#Exploring-the-Multi-modal-Demand-Dynamics-During-Transport-System-Disruptions" class="headerlink" title="Exploring the Multi-modal Demand Dynamics During Transport System Disruptions"></a>Exploring the Multi-modal Demand Dynamics During Transport System Disruptions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00877">http://arxiv.org/abs/2307.00877</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Shateri Benam, Angelo Furno, Nour-Eddin El Faouzi</li>
<li>for: 本研究旨在探讨不同类型的交通系统紊乱对城市流动性的影响，以及乘客对这些紊乱事件的不同反应。</li>
<li>methods: 本研究采用数据驱动的方法来探索多种交通方式的需求动态下降。首先，我们开发了一种方法来自动检测历史小时旅行需求数据中的异常情况。然后，我们应用分 clustering 这些异常小时，以分辨不同类型的多种交通需求动态。</li>
<li>results: 本研究提供了一种简单的工具，可以根据不同的紊乱enario分类不同类型的乘客反应，以及估算不同紊乱enario下的模式转移范围。<details>
<summary>Abstract</summary>
Various forms of disruption in transport systems perturb urban mobility in different ways. Passengers respond heterogeneously to such disruptive events based on numerous factors. This study takes a data-driven approach to explore multi-modal demand dynamics under disruptions. We first develop a methodology to automatically detect anomalous instances through historical hourly travel demand data. Then we apply clustering to these anomalous hours to distinguish various forms of multi-modal demand dynamics occurring during disruptions. Our study provides a straightforward tool for categorising various passenger responses to disruptive events in terms of mode choice and paves the way for predictive analyses on estimating the scope of modal shift under distinct disruption scenarios.
</details>
<details>
<summary>摘要</summary>
不同的交通系统紊乱会对城市流动性产生不同的影响。乘客对这些紊乱事件的回应也是多iform的，这是基于许多因素。这项研究采用数据驱动的方法来探索各种多Modal的需求动力学在紊乱情况下。我们首先开发了一种自动检测历史小时旅行需求数据中异常情况的方法。然后我们应用分 clustering 这些异常小时，以分辨不同的多Modal需求动力学在紊乱情况下发生的形式。我们的研究提供了一种简单的工具，可以根据乘客对紊乱事件的回应来分类不同的交通模式，并且为估计不同紊乱情况下的模式转换范围做出预测分析。
</details></li>
</ul>
<hr>
<h2 id="RobustL2S-Speaker-Specific-Lip-to-Speech-Synthesis-exploiting-Self-Supervised-Representations"><a href="#RobustL2S-Speaker-Specific-Lip-to-Speech-Synthesis-exploiting-Self-Supervised-Representations" class="headerlink" title="RobustL2S: Speaker-Specific Lip-to-Speech Synthesis exploiting Self-Supervised Representations"></a>RobustL2S: Speaker-Specific Lip-to-Speech Synthesis exploiting Self-Supervised Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01233">http://arxiv.org/abs/2307.01233</a></li>
<li>repo_url: None</li>
<li>paper_authors: Neha Sahipjohn, Neil Shah, Vishal Tambrahalli, Vineet Gandhi</li>
<li>for:  lip-to-speech synthesis</li>
<li>methods:  non-autoregressive sequence-to-sequence architecture, disentangled speech content representation</li>
<li>results:  state-of-the-art performance on unconstrained and constrained datasets, speech samples available onlineHere’s the full text in Simplified Chinese:for:  lip-to-speech synthesismethods:  non-autoregressive sequence-to-sequence architecture, 自成分化 speech content representationresults:  state-of-the-art performance on unconstrained和 constrained datasets, speech samples available online<details>
<summary>Abstract</summary>
Significant progress has been made in speaker dependent Lip-to-Speech synthesis, which aims to generate speech from silent videos of talking faces. Current state-of-the-art approaches primarily employ non-autoregressive sequence-to-sequence architectures to directly predict mel-spectrograms or audio waveforms from lip representations. We hypothesize that the direct mel-prediction hampers training/model efficiency due to the entanglement of speech content with ambient information and speaker characteristics. To this end, we propose RobustL2S, a modularized framework for Lip-to-Speech synthesis. First, a non-autoregressive sequence-to-sequence model maps self-supervised visual features to a representation of disentangled speech content. A vocoder then converts the speech features into raw waveforms. Extensive evaluations confirm the effectiveness of our setup, achieving state-of-the-art performance on the unconstrained Lip2Wav dataset and the constrained GRID and TCD-TIMIT datasets. Speech samples from RobustL2S can be found at https://neha-sherin.github.io/RobustL2S/
</details>
<details>
<summary>摘要</summary>
significan progress has been made in speaker-dependent Lip-to-Speech synthesis, which aims to generate speech from silent videos of talking faces. current state-of-the-art approaches primarily employ non-autoregressive sequence-to-sequence architectures to directly predict mel-spectrograms or audio waveforms from lip representations. we hypothesize that the direct mel-prediction hampers training/model efficiency due to the entanglement of speech content with ambient information and speaker characteristics. to this end, we propose RobustL2S, a modularized framework for Lip-to-Speech synthesis. first, a non-autoregressive sequence-to-sequence model maps self-supervised visual features to a representation of disentangled speech content. a vocoder then converts the speech features into raw waveforms. extensive evaluations confirm the effectiveness of our setup, achieving state-of-the-art performance on the unconstrained Lip2Wav dataset and the constrained GRID and TCD-TIMIT datasets. speech samples from RobustL2S can be found at https://neha-sherin.github.io/RobustL2S/Here's the word-for-word translation: significan进步有所作出在 speaker-dependent Lip-to-Speech合成中，目标是从无声视频中提取讲话的语音。 current state-of-the-art Approaches primarily employ non-autoregressive sequence-to-sequence architectures to directly predict mel-spectrograms or audio waveforms from lip representations. we hypothesize that the direct mel-prediction hampers training/model efficiency due to the entanglement of speech content with ambient information and speaker characteristics. to this end, we propose RobustL2S, a modularized framework for Lip-to-Speech synthesis. first, a non-autoregressive sequence-to-sequence model maps self-supervised visual features to a representation of disentangled speech content. a vocoder then converts the speech features into raw waveforms. extensive evaluations confirm the effectiveness of our setup, achieving state-of-the-art performance on the unconstrained Lip2Wav dataset and the constrained GRID and TCD-TIMIT datasets. speech samples from RobustL2S can be found at https://neha-sherin.github.io/RobustL2S/
</details></li>
</ul>
<hr>
<h2 id="MADS-Modulated-Auto-Decoding-SIREN-for-time-series-imputation"><a href="#MADS-Modulated-Auto-Decoding-SIREN-for-time-series-imputation" class="headerlink" title="MADS: Modulated Auto-Decoding SIREN for time series imputation"></a>MADS: Modulated Auto-Decoding SIREN for time series imputation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00868">http://arxiv.org/abs/2307.00868</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tom Bamford, Elizabeth Fons, Yousef El-Laham, Svitlana Vyetrenko</li>
<li>for: 这 paper 是为了解决时间序列填充问题而写的，时间序列填充是许多领域中的一个重要挑战，因为时间序列数据的类型可能会具有很大的变化。</li>
<li>methods: 这 paper 使用了深度学习技术，特别是 SIRENs 和 hypernetwork 架构，来解决时间序列填充问题。</li>
<li>results: 这 paper 在两个实际数据集上进行了评估，并证明了它在时间序列填充方面的表现比之前的方法更好，在人活动数据集上提高了填充性能的至少 40%，而在空气质量数据集上与其他基elines一样。在synthetic数据上进行评估时，我们的模型在不同数据集配置下的平均排名得到了所有基elines的最好成绩。<details>
<summary>Abstract</summary>
Time series imputation remains a significant challenge across many fields due to the potentially significant variability in the type of data being modelled. Whilst traditional imputation methods often impose strong assumptions on the underlying data generation process, limiting their applicability, researchers have recently begun to investigate the potential of deep learning for this task, inspired by the strong performance shown by these models in both classification and regression problems across a range of applications. In this work we propose MADS, a novel auto-decoding framework for time series imputation, built upon implicit neural representations. Our method leverages the capabilities of SIRENs for high fidelity reconstruction of signals and irregular data, and combines it with a hypernetwork architecture which allows us to generalise by learning a prior over the space of time series. We evaluate our model on two real-world datasets, and show that it outperforms state-of-the-art methods for time series imputation. On the human activity dataset, it improves imputation performance by at least 40%, while on the air quality dataset it is shown to be competitive across all metrics. When evaluated on synthetic data, our model results in the best average rank across different dataset configurations over all baselines.
</details>
<details>
<summary>摘要</summary>
时间序列填充仍然是许多领域中的主要挑战，因为可能存在严重的数据类型变化，导致传统填充方法的适用有限。然而，研究人员最近开始研究使用深度学习来解决这个问题，因为深度学习模型在各种应用中的类型预测和回归问题中表现出色。在这种工作中，我们提出了MADS，一种新的自动解码框架 для时间序列填充，基于含义表示。我们的方法利用SIRENs高精度重建信号和不规则数据的能力，并将其与一个权重网络架构相结合，以学习时间序列的先验知识。我们对两个实际数据集进行评估，并显示了MADS在时间序列填充方面的超过状态艺术方法的表现。在人活动数据集上，它提高填充性能至少40%，而在空气质量数据集上，它与所有指标中竞争。当对synthetic数据进行评估时，我们的模型在不同数据集配置下的平均排名最高。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-on-Graph-Classification-and-Link-Prediction-based-on-GNN"><a href="#A-Survey-on-Graph-Classification-and-Link-Prediction-based-on-GNN" class="headerlink" title="A Survey on Graph Classification and Link Prediction based on GNN"></a>A Survey on Graph Classification and Link Prediction based on GNN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00865">http://arxiv.org/abs/2307.00865</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingyu Liu, Juan Chen, Quan Wen</li>
<li>for: 本文旨在探讨 tradicional convolutional neural networks 在图数据处理中的应用，以及如何将其扩展到图数据分析和处理领域。</li>
<li>methods: 本文使用 graph convolutional operators 和 graph pooling operators 来构建图 convolutional neural networks，并采用 attention mechanisms 和 autoencoders 来提高模型性能。</li>
<li>results: 本文通过对node classification、graph classification和link prediction等任务的应用，阐述了图 convolutional neural networks 在不同任务中的应用和效果。<details>
<summary>Abstract</summary>
Traditional convolutional neural networks are limited to handling Euclidean space data, overlooking the vast realm of real-life scenarios represented as graph data, including transportation networks, social networks, and reference networks. The pivotal step in transferring convolutional neural networks to graph data analysis and processing lies in the construction of graph convolutional operators and graph pooling operators. This comprehensive review article delves into the world of graph convolutional neural networks. Firstly, it elaborates on the fundamentals of graph convolutional neural networks. Subsequently, it elucidates the graph neural network models based on attention mechanisms and autoencoders, summarizing their application in node classification, graph classification, and link prediction along with the associated datasets.
</details>
<details>
<summary>摘要</summary>
传统的卷积神经网络只能处理几何空间数据，忽略了现实生活中的各种图数据，包括交通网络、社交网络和引用网络。图 convolutional 算子和图 Pooling 算子的建构是将卷积神经网络传输到图数据分析和处理的关键步骤。本综述文章将介绍图 convolutional 神经网络的基础知识，然后详细介绍基于注意机制和自编码器的图神经网络模型，包括节点分类、图分类和链接预测，以及相关的数据集。
</details></li>
</ul>
<hr>
<h2 id="Thompson-Sampling-under-Bernoulli-Rewards-with-Local-Differential-Privacy"><a href="#Thompson-Sampling-under-Bernoulli-Rewards-with-Local-Differential-Privacy" class="headerlink" title="Thompson Sampling under Bernoulli Rewards with Local Differential Privacy"></a>Thompson Sampling under Bernoulli Rewards with Local Differential Privacy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00863">http://arxiv.org/abs/2307.00863</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bo Jiang, Tianchi Zhao, Ming Li</li>
<li>for: 这个论文研究了多机枪弹（MAB）问题中的 regret 最小化问题，同时保证了地方差分隐私（LDP）的 garantor。</li>
<li>methods: 论文使用了三种隐私机制：线性机制、 quadrature 机制和指数机制，并对 Thompson Sampling 算法 derivated 了随机 regret bound。</li>
<li>results: 论文通过 simulate 来示例了不同隐私预算下不同机制的凝结行为。<details>
<summary>Abstract</summary>
This paper investigates the problem of regret minimization for multi-armed bandit (MAB) problems with local differential privacy (LDP) guarantee. Given a fixed privacy budget $\epsilon$, we consider three privatizing mechanisms under Bernoulli scenario: linear, quadratic and exponential mechanisms. Under each mechanism, we derive stochastic regret bound for Thompson Sampling algorithm. Finally, we simulate to illustrate the convergence of different mechanisms under different privacy budgets.
</details>
<details>
<summary>摘要</summary>
这个论文研究了多重投机(MAB)问题中的 regret最小化问题，同时保证了本地差分隐私(LDP)的 garantía。给定一个固定的隐私预算$\epsilon$,我们考虑了三种隐私机制在 Bernoulli 场景下：线性机制、квадратиче机制和指数机制。对于每种机制，我们 derivated sto的抽象 regret bound for Thompson Sampling 算法。最后，我们使用 Simulation 来示出不同隐私预算下不同机制的连续性。Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="CardiGraphormer-Unveiling-the-Power-of-Self-Supervised-Learning-in-Revolutionizing-Drug-Discovery"><a href="#CardiGraphormer-Unveiling-the-Power-of-Self-Supervised-Learning-in-Revolutionizing-Drug-Discovery" class="headerlink" title="CardiGraphormer: Unveiling the Power of Self-Supervised Learning in Revolutionizing Drug Discovery"></a>CardiGraphormer: Unveiling the Power of Self-Supervised Learning in Revolutionizing Drug Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00859">http://arxiv.org/abs/2307.00859</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abhijit Gupta, Arnab Mukherjee</li>
<li>for: 这篇论文旨在探讨一种新的人工智能（AI）方法，用于探索药物发现的可能性。</li>
<li>methods: 这篇论文使用了自我超级学习（SSL）、 graf对话网（GNNs）和卡度维持注意力（Cardinality Preserving Attention）等技术，创造了一个名为CardiGraphormer的新方法。</li>
<li>results:  CardiGraphormer可以对药物结构进行学习，并将其映射到更加强大的预测性和解释性。它可以处理复杂的药物数据，并可以进行节点、双节点、子graph和整个图结构的处理。<details>
<summary>Abstract</summary>
In the expansive realm of drug discovery, with approximately 15,000 known drugs and only around 4,200 approved, the combinatorial nature of the chemical space presents a formidable challenge. While Artificial Intelligence (AI) has emerged as a powerful ally, traditional AI frameworks face significant hurdles. This manuscript introduces CardiGraphormer, a groundbreaking approach that synergizes self-supervised learning (SSL), Graph Neural Networks (GNNs), and Cardinality Preserving Attention to revolutionize drug discovery. CardiGraphormer, a novel combination of Graphormer and Cardinality Preserving Attention, leverages SSL to learn potent molecular representations and employs GNNs to extract molecular fingerprints, enhancing predictive performance and interpretability while reducing computation time. It excels in handling complex data like molecular structures and performs tasks associated with nodes, pairs of nodes, subgraphs, or entire graph structures. CardiGraphormer's potential applications in drug discovery and drug interactions are vast, from identifying new drug targets to predicting drug-to-drug interactions and enabling novel drug discovery. This innovative approach provides an AI-enhanced methodology in drug development, utilizing SSL combined with GNNs to overcome existing limitations and pave the way for a richer exploration of the vast combinatorial chemical space in drug discovery.
</details>
<details>
<summary>摘要</summary>
在药物发现领域中，有约15,000种已知药物和只有约4,200个得到批准，化学空间的 combinatorial 特性呈现出巨大的挑战。而人工智能（AI）已经出现为一种强大的同伴，但传统的 AI 框架又存在许多障碍。这篇文章介绍 CardiGraphormer，一种创新的方法，它将自我supervised learning（SSL）、Graph Neural Networks（GNNs）和Cardinality Preserving Attention（CPA）相互融合，以推动药物发现领域的革命。CardiGraphormer 是 Graphormer 和 CPA 的新的组合体，通过 SSL 学习强大的分子表示，并使用 GNNs 提取分子指纹，提高预测性能和可读性，同时减少计算时间。它可以处理复杂的数据，如分子结构，并可以完成节点、节点对、子图和整个图结构上的任务。CardiGraphormer 在药物发现和药物互作中的应用前景广阔，从选择新的药物目标到预测药物之间的互作，以及开发新的药物探索方法。这种创新的方法为药物发展中的 AI 增强方法，通过 SSL 与 GNNs 的结合，突破现有的限制，开辟了更加丰富的化学空间的探索。
</details></li>
</ul>
<hr>
<h2 id="Beyond-the-Snapshot-Brain-Tokenized-Graph-Transformer-for-Longitudinal-Brain-Functional-Connectome-Embedding"><a href="#Beyond-the-Snapshot-Brain-Tokenized-Graph-Transformer-for-Longitudinal-Brain-Functional-Connectome-Embedding" class="headerlink" title="Beyond the Snapshot: Brain Tokenized Graph Transformer for Longitudinal Brain Functional Connectome Embedding"></a>Beyond the Snapshot: Brain Tokenized Graph Transformer for Longitudinal Brain Functional Connectome Embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00858">http://arxiv.org/abs/2307.00858</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zijiand/brain-tokengt">https://github.com/zijiand/brain-tokengt</a></li>
<li>paper_authors: Zijian Dong, Yilei Wu, Yu Xiao, Joanna Su Xian Chong, Yueming Jin, Juan Helen Zhou</li>
<li>for: 这个研究的目的是为了发展一个可解释的数据 embedding 方法，用于诊断和预测认知功能障碍和慢速进行的脑性障碍病。</li>
<li>methods: 这个方法使用了 Graph Neural Networks (GNN) 和 tokenization 技术，实现了脑功能连接图 (FC) 的时间变化轨迹 embedding。</li>
<li>results: 在 AD 病Continuum 上的两个公共 longitudinal fMRI 数据集上，这个方法比其他 benchmark 模型出色，并且提供了极佳的解释性。<details>
<summary>Abstract</summary>
Under the framework of network-based neurodegeneration, brain functional connectome (FC)-based Graph Neural Networks (GNN) have emerged as a valuable tool for the diagnosis and prognosis of neurodegenerative diseases such as Alzheimer's disease (AD). However, these models are tailored for brain FC at a single time point instead of characterizing FC trajectory. Discerning how FC evolves with disease progression, particularly at the predementia stages such as cognitively normal individuals with amyloid deposition or individuals with mild cognitive impairment (MCI), is crucial for delineating disease spreading patterns and developing effective strategies to slow down or even halt disease advancement. In this work, we proposed the first interpretable framework for brain FC trajectory embedding with application to neurodegenerative disease diagnosis and prognosis, namely Brain Tokenized Graph Transformer (Brain TokenGT). It consists of two modules: 1) Graph Invariant and Variant Embedding (GIVE) for generation of node and spatio-temporal edge embeddings, which were tokenized for downstream processing; 2) Brain Informed Graph Transformer Readout (BIGTR) which augments previous tokens with trainable type identifiers and non-trainable node identifiers and feeds them into a standard transformer encoder to readout. We conducted extensive experiments on two public longitudinal fMRI datasets of the AD continuum for three tasks, including differentiating MCI from controls, predicting dementia conversion in MCI, and classification of amyloid positive or negative cognitively normal individuals. Based on brain FC trajectory, the proposed Brain TokenGT approach outperformed all the other benchmark models and at the same time provided excellent interpretability. The code is available at https://github.com/ZijianD/Brain-TokenGT.git
</details>
<details>
<summary>摘要</summary>
基于网络基因的脑功能连接图（FC）基于图神经网络（GNN）已成为诊断和诊断预测阿尔茨海默病（AD）等神经退化疾病的有价值工具。然而，这些模型围绕单个时间点的脑FC而构建，而不是跟踪FC的演变。了解脑FC在疾病晚期的演变，特别是在认知正常者悉射氧皮质堆积或轻度认知障碍（MCI）等阶段，是诊断疾病扩散模式和开发有效缓解疾病的关键。在这种工作中，我们提出了第一个可解释性框架 для脑FC演变 embeddings，即Brain Tokenized Graph Transformer（Brain TokenGT）。它包括两个模块：1）图固有和变异 embedding（GIVE）用于生成节点和空间时间边 embedding，这些embedding被拆分为下游处理; 2）脑指导图Transformer读取（BIGTR），它将上一个模块的输出和可训练类型标识符和非可训练节点标识符相加，并将其传输给标准Transformer编码器来读取。我们对两个公共的 longitudinal fMRI 数据集进行了广泛的实验，包括分类认知正常者和MCI、预测MCI中的诊断转化和分类悉射氧皮质堆积或正常认知正常者。基于脑FC演变，我们的Brain TokenGT方法在所有参考模型中表现出色，同时提供了优秀的可解释性。代码可以在https://github.com/ZijianD/Brain-TokenGT.git中找到。
</details></li>
</ul>
<hr>
<h2 id="Surgical-fine-tuning-for-Grape-Bunch-Segmentation-under-Visual-Domain-Shifts"><a href="#Surgical-fine-tuning-for-Grape-Bunch-Segmentation-under-Visual-Domain-Shifts" class="headerlink" title="Surgical fine-tuning for Grape Bunch Segmentation under Visual Domain Shifts"></a>Surgical fine-tuning for Grape Bunch Segmentation under Visual Domain Shifts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00837">http://arxiv.org/abs/2307.00837</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/airlab-polimi/sft_grape_segmentation">https://github.com/airlab-polimi/sft_grape_segmentation</a></li>
<li>paper_authors: Agnese Chiatti, Riccardo Bertoglio, Nico Catalano, Matteo Gatti, Matteo Matteucci</li>
<li>for: 这篇论文是为了研究mobile robots在农业中的应用，尤其是在葡萄园中自动和有效地监测植物状况。</li>
<li>methods: 本研究使用了抢救精细调整（surgical fine-tuning）来适应葡萄图像中的视觉域变化。 selectively tuning only specific model layers可以支持预训练深度学习模型对新收集的葡萄图像进行适应，同时减少调整的参数数量。</li>
<li>results: 本研究表明，通过抢救精细调整，可以减少参数数量的情况下，预训练深度学习模型可以快速适应葡萄图像中的视觉域变化，并且可以提高葡萄棕榈的检测精度。<details>
<summary>Abstract</summary>
Mobile robots will play a crucial role in the transition towards sustainable agriculture. To autonomously and effectively monitor the state of plants, robots ought to be equipped with visual perception capabilities that are robust to the rapid changes that characterise agricultural settings. In this paper, we focus on the challenging task of segmenting grape bunches from images collected by mobile robots in vineyards. In this context, we present the first study that applies surgical fine-tuning to instance segmentation tasks. We show how selectively tuning only specific model layers can support the adaptation of pre-trained Deep Learning models to newly-collected grape images that introduce visual domain shifts, while also substantially reducing the number of tuned parameters.
</details>
<details>
<summary>摘要</summary>
mobile robots会在可持续农业过渡中发挥关键作用。为了让机器人自动和有效地监测植物状况，它们需要具备可靠地对农业环境中快速变化的视觉感知能力。在这篇论文中，我们关注了在葡萄园中机器人收集的图像中分割葡萄束的挑战性任务。我们表明了使用手术精细调整来实现预训练深度学习模型的适应新收集的葡萄图像，同时减少调整参数的数量。
</details></li>
</ul>
<hr>
<h2 id="Trading-Off-Payments-and-Accuracy-in-Online-Classification-with-Paid-Stochastic-Experts"><a href="#Trading-Off-Payments-and-Accuracy-in-Online-Classification-with-Paid-Stochastic-Experts" class="headerlink" title="Trading-Off Payments and Accuracy in Online Classification with Paid Stochastic Experts"></a>Trading-Off Payments and Accuracy in Online Classification with Paid Stochastic Experts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00836">http://arxiv.org/abs/2307.00836</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dirk van der Hoeven, Ciara Pike-Burke, Hao Qiu, Nicolo Cesa-Bianchi</li>
<li>for: 这个论文研究在线分类，特别是使用支付的权重随机专家（Paid Stochastic Experts）。</li>
<li>methods: 论文使用了在线学习算法，其中每名专家需要支付一定的费用才能提供预测。 learner需要在每一轮中决定支付每名专家多少费用，并根据这些预测来做预测。</li>
<li>results: 论文提出了一种在线学习算法，该算法的总成本在 $T$ 轮后不超过一个 predictor 的总成本，这个 predictor 知道所有专家的产出函数（productivity）。这个结果比标准的 Lipschitz 随机抽象更好，可以避免 $T^{2&#x2F;3}$ 的成本上限。<details>
<summary>Abstract</summary>
We investigate online classification with paid stochastic experts. Here, before making their prediction, each expert must be paid. The amount that we pay each expert directly influences the accuracy of their prediction through some unknown Lipschitz "productivity" function. In each round, the learner must decide how much to pay each expert and then make a prediction. They incur a cost equal to a weighted sum of the prediction error and upfront payments for all experts. We introduce an online learning algorithm whose total cost after $T$ rounds exceeds that of a predictor which knows the productivity of all experts in advance by at most $\mathcal{O}(K^2(\log T)\sqrt{T})$ where $K$ is the number of experts. In order to achieve this result, we combine Lipschitz bandits and online classification with surrogate losses. These tools allow us to improve upon the bound of order $T^{2/3}$ one would obtain in the standard Lipschitz bandit setting. Our algorithm is empirically evaluated on synthetic data
</details>
<details>
<summary>摘要</summary>
我们调查线上分类 WITH 付价随机专家。在这里，每个专家都需要付出一定的代价才能发出预测。付出的代价直接影响预测的精度通过一个未知的 Lipschitz "产生力" 函数。在每一轮中，学习者需要决定如何付出每个专家以及做出预测。他们需要支付一个复杂的权重和预测误差的成本。我们引入线上学习算法，其全部成本在 $T$ 轮后不超过一个知道所有专家产生力的预测者的全部成本的 $\mathcal{O}(K^2(\log T)\sqrt{T})$，其中 $K$ 是专家的数量。以实现这个结果，我们结合了Lipschitz 枪和线上分类 WITH 代理损失。这些工具允许我们对于标准 Lipschitz 枪设置中的结果进行改进。我们的算法在实验上被评估了在合成数据上。
</details></li>
</ul>
<hr>
<h2 id="Engression-Extrapolation-for-Nonlinear-Regression"><a href="#Engression-Extrapolation-for-Nonlinear-Regression" class="headerlink" title="Engression: Extrapolation for Nonlinear Regression?"></a>Engression: Extrapolation for Nonlinear Regression?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00835">http://arxiv.org/abs/2307.00835</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xwshen51/engression">https://github.com/xwshen51/engression</a></li>
<li>paper_authors: Xinwei Shen, Nicolai Meinshausen</li>
<li>For: The paper is written for those who need a nonlinear regression method that can handle extrapolation tasks, especially in situations where the training data is limited and the test data is outside the support.* Methods: The paper proposes a new method called “engression” which is a distributional regression technique for pre-additive noise models. The method adds noise to the covariates before applying a nonlinear transformation, allowing it to perform well in extrapolation tasks.* Results: The paper shows that engression consistently provides a meaningful improvement in extrapolation tasks compared to traditional regression approaches such as least-squares regression and quantile regression, especially when the function class is strictly monotone. The empirical results from both simulated and real data validate the effectiveness of the engression method.<details>
<summary>Abstract</summary>
Extrapolation is crucial in many statistical and machine learning applications, as it is common to encounter test data outside the training support. However, extrapolation is a considerable challenge for nonlinear models. Conventional models typically struggle in this regard: while tree ensembles provide a constant prediction beyond the support, neural network predictions tend to become uncontrollable. This work aims at providing a nonlinear regression methodology whose reliability does not break down immediately at the boundary of the training support. Our primary contribution is a new method called `engression' which, at its core, is a distributional regression technique for pre-additive noise models, where the noise is added to the covariates before applying a nonlinear transformation. Our experimental results indicate that this model is typically suitable for many real data sets. We show that engression can successfully perform extrapolation under some assumptions such as a strictly monotone function class, whereas traditional regression approaches such as least-squares regression and quantile regression fall short under the same assumptions. We establish the advantages of engression over existing approaches in terms of extrapolation, showing that engression consistently provides a meaningful improvement. Our empirical results, from both simulated and real data, validate these findings, highlighting the effectiveness of the engression method. The software implementations of engression are available in both R and Python.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate the following text into Simplified Chinese<</SYS>>描述：扩展是在统计学和机器学习应用中非常重要，因为很多时候会遇到训练数据外的测试数据。然而，扩展是非线性模型的一大挑战。传统的模型通常在这个方面表现不佳：虚拟树集提供了常数预测，而神经网络预测则变得无法控制。这项工作的目标是提供一种非线性回归方法，其可靠性不会因训练支持的边界而崩溃。我们的主要贡献是一种名为“扩展”的新方法，其核心思想是为幂函数模型添加前向随机变量的分布式回归技术。我们的实验结果表明，这种方法适用于许多实际数据集。我们证明了扩展在一些假设下（如幂函数类型的准确 monotonic）下能够成功进行扩展，而传统的回归方法如最小二乘回归和量化回归则在同样的假设下失败。我们还证明了扩展与现有方法的优势，并通过实验证明了扩展的有效性。扩展的软件实现现已在R和Python中可用。Note: Simplified Chinese is used in mainland China and Singapore, while Traditional Chinese is used in Taiwan, Hong Kong, and Macau.
</details></li>
</ul>
<hr>
<h2 id="Model-Assisted-Probabilistic-Safe-Adaptive-Control-With-Meta-Bayesian-Learning"><a href="#Model-Assisted-Probabilistic-Safe-Adaptive-Control-With-Meta-Bayesian-Learning" class="headerlink" title="Model-Assisted Probabilistic Safe Adaptive Control With Meta-Bayesian Learning"></a>Model-Assisted Probabilistic Safe Adaptive Control With Meta-Bayesian Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00828">http://arxiv.org/abs/2307.00828</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shengbo Wang, Ke Li, Yin Yang, Yuting Cao, Tingwen Huang, Shiping Wen</li>
<li>for: 本研究旨在开发一种可靠的 adaptive safe control 框架，以满足控制系统中的安全性和可靠性需求。</li>
<li>methods: 本研究使用 meta learning 技术、权重学习和 Bayesian 模型，以及控制边界函数（CBF）方法，来学习内在和外在不确定性。特别是，通过 CBF 方法，我们可以通过一个统一的 adaptive Bayesian linear regression（ABLR）模型来学习不确定性，该模型包括一个前向神经网络（NN）和一个 Bayesian 输出层。</li>
<li>results: 对比历史类似任务的数据，我们的算法可以快速地适应新的控制任务，并在多个不确定性约束下进行安全的探索。 results 表明，我们的算法可以显著提高 Bayesian 模型基于 CBF 方法的性能，并且可以快速地适应不同的控制任务。<details>
<summary>Abstract</summary>
Breaking safety constraints in control systems can lead to potential risks, resulting in unexpected costs or catastrophic damage. Nevertheless, uncertainty is ubiquitous, even among similar tasks. In this paper, we develop a novel adaptive safe control framework that integrates meta learning, Bayesian models, and control barrier function (CBF) method. Specifically, with the help of CBF method, we learn the inherent and external uncertainties by a unified adaptive Bayesian linear regression (ABLR) model, which consists of a forward neural network (NN) and a Bayesian output layer. Meta learning techniques are leveraged to pre-train the NN weights and priors of the ABLR model using data collected from historical similar tasks. For a new control task, we refine the meta-learned models using a few samples, and introduce pessimistic confidence bounds into CBF constraints to ensure safe control. Moreover, we provide theoretical criteria to guarantee probabilistic safety during the control processes. To validate our approach, we conduct comparative experiments in various obstacle avoidance scenarios. The results demonstrate that our algorithm significantly improves the Bayesian model-based CBF method, and is capable for efficient safe exploration even with multiple uncertain constraints.
</details>
<details>
<summary>摘要</summary>
breaking safety constraints in control systems can lead to potential risks, resulting in unexpected costs or catastrophic damage. Nevertheless, uncertainty is ubiquitous, even among similar tasks. In this paper, we develop a novel adaptive safe control framework that integrates meta learning, Bayesian models, and control barrier function (CBF) method. Specifically, with the help of CBF method, we learn the inherent and external uncertainties by a unified adaptive Bayesian linear regression (ABLR) model, which consists of a forward neural network (NN) and a Bayesian output layer. Meta learning techniques are leveraged to pre-train the NN weights and priors of the ABLR model using data collected from historical similar tasks. For a new control task, we refine the meta-learned models using a few samples, and introduce pessimistic confidence bounds into CBF constraints to ensure safe control. Moreover, we provide theoretical criteria to guarantee probabilistic safety during the control processes. To validate our approach, we conduct comparative experiments in various obstacle avoidance scenarios. The results demonstrate that our algorithm significantly improves the Bayesian model-based CBF method, and is capable for efficient safe exploration even with multiple uncertain constraints.Here's the translation in Traditional Chinese:breaking safety constraints in control systems can lead to potential risks, resulting in unexpected costs or catastrophic damage. Nevertheless, uncertainty is ubiquitous, even among similar tasks. In this paper, we develop a novel adaptive safe control framework that integrates meta learning, Bayesian models, and control barrier function (CBF) method. Specifically, with the help of CBF method, we learn the inherent and external uncertainties by a unified adaptive Bayesian linear regression (ABLR) model, which consists of a forward neural network (NN) and a Bayesian output layer. Meta learning techniques are leveraged to pre-train the NN weights and priors of the ABLR model using data collected from historical similar tasks. For a new control task, we refine the meta-learned models using a few samples, and introduce pessimistic confidence bounds into CBF constraints to ensure safe control. Moreover, we provide theoretical criteria to guarantee probabilistic safety during the control processes. To validate our approach, we conduct comparative experiments in various obstacle avoidance scenarios. The results demonstrate that our algorithm significantly improves the Bayesian model-based CBF method, and is capable for efficient safe exploration even with multiple uncertain constraints.
</details></li>
</ul>
<hr>
<h2 id="Robust-Surgical-Tools-Detection-in-Endoscopic-Videos-with-Noisy-Data"><a href="#Robust-Surgical-Tools-Detection-in-Endoscopic-Videos-with-Noisy-Data" class="headerlink" title="Robust Surgical Tools Detection in Endoscopic Videos with Noisy Data"></a>Robust Surgical Tools Detection in Endoscopic Videos with Noisy Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01232">http://arxiv.org/abs/2307.01232</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adnan Qayyum, Hassan Ali, Massimo Caputo, Hunaid Vohra, Taofeek Akinosho, Sofiat Abioye, Ilhem Berrou, Paweł Capik, Junaid Qadir, Muhammad Bilal</li>
<li>for: 本研究旨在提出一种系统性的方法ologies для开发Robust模型，以便在含有噪声数据的情况下进行手术工具检测。</li>
<li>methods: 本研究使用了两个关键创新：首先，提出了一种智能活动学习策略，以便由人工专家进行最小数据标注和标签更正；其次，提出了一种学生-教师模型自我训练框架，以便在半监督的情况下实现多种手术工具的精准分类。此外，我们还使用了负权重数据加载器来处理困难的类别标签和类别不均衡问题。</li>
<li>results: 根据我们的实验结果，提出的方法可以在含有噪声数据的情况下实现85.88%的平均F1分数，而无类别权重的情况下可以达到80.88%的平均F1分数。此外，我们的提出方法也可以有效地超越现有的方法，这有效地证明了其效果。<details>
<summary>Abstract</summary>
Over the past few years, surgical data science has attracted substantial interest from the machine learning (ML) community. Various studies have demonstrated the efficacy of emerging ML techniques in analysing surgical data, particularly recordings of procedures, for digitizing clinical and non-clinical functions like preoperative planning, context-aware decision-making, and operating skill assessment. However, this field is still in its infancy and lacks representative, well-annotated datasets for training robust models in intermediate ML tasks. Also, existing datasets suffer from inaccurate labels, hindering the development of reliable models. In this paper, we propose a systematic methodology for developing robust models for surgical tool detection using noisy data. Our methodology introduces two key innovations: (1) an intelligent active learning strategy for minimal dataset identification and label correction by human experts; and (2) an assembling strategy for a student-teacher model-based self-training framework to achieve the robust classification of 14 surgical tools in a semi-supervised fashion. Furthermore, we employ weighted data loaders to handle difficult class labels and address class imbalance issues. The proposed methodology achieves an average F1-score of 85.88\% for the ensemble model-based self-training with class weights, and 80.88\% without class weights for noisy labels. Also, our proposed method significantly outperforms existing approaches, which effectively demonstrates its effectiveness.
</details>
<details>
<summary>摘要</summary>
过去几年，手术数据科学已经吸引了机器学习（ML）社区的广泛关注。多个研究表明了新兴ML技术在分析手术数据方面的效果，特别是记录手术过程的数据，包括前操作规划、Context-aware决策和手术技巧评估等。然而，这一领域仍处于初期阶段，缺乏代表性的、正确标注的数据集，以用于训练中等ML任务。此外，现有的数据集受到不准确的标注，使得模型的发展受到限制。在这篇论文中，我们提出了一种系统方法论，用于开发Robust模型，以便在噪音数据上进行手术工具检测。我们的方法引入了两个关键创新：1. 一种智能的活动学习策略，用于identify minimal数据集和人工专家标注的自动 corrections;2. 一种学生-教师模型基于的自动训练框架，以实现14种手术工具的robust分类。我们还使用了重量数据加载器，以处理困难的类别标签和类别不均衡问题。提议的方法在 ensemble模型基于自动训练中实现了85.88%的F1分数，而不含类别标签的情况下，则是80.88%。此外，我们的提议方法在与现有方法进行比较时，显示出了明显的效果。
</details></li>
</ul>
<hr>
<h2 id="Analysis-of-Task-Transferability-in-Large-Pre-trained-Classifiers"><a href="#Analysis-of-Task-Transferability-in-Large-Pre-trained-Classifiers" class="headerlink" title="Analysis of Task Transferability in Large Pre-trained Classifiers"></a>Analysis of Task Transferability in Large Pre-trained Classifiers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00823">http://arxiv.org/abs/2307.00823</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/akshaymehra24/tasktransferanalysis">https://github.com/akshaymehra24/tasktransferanalysis</a></li>
<li>paper_authors: Akshay Mehra, Yunbei Zhang, Jihun Hamm</li>
<li>for: 本研究旨在分析将知识从源任务传播到多个下游任务中的性能，特别是使用大型预训练模型时，传播性能如何提高。</li>
<li>methods: 本研究使用一种名为Task Transfer Analysis的方法，将源任务的分布和分类器变换成一个新的源任务分布，并将源任务的损失与下游任务的损失相关联。</li>
<li>results: 本研究通过大规模的实验研究，发现了各种因素如任务相关性、预训练方法和模型结构对传播性能的影响。<details>
<summary>Abstract</summary>
Transfer learning transfers the knowledge acquired by a model from a source task to multiple downstream target tasks with minimal fine-tuning. The success of transfer learning at improving performance, especially with the use of large pre-trained models has made transfer learning an essential tool in the machine learning toolbox. However, the conditions under which the performance is transferable to downstream tasks are not understood very well. In this work, we analyze the transfer of performance for classification tasks, when only the last linear layer of the source model is fine-tuned on the target task. We propose a novel Task Transfer Analysis approach that transforms the source distribution (and classifier) by changing the class prior distribution, label, and feature spaces to produce a new source distribution (and classifier) and allows us to relate the loss of the downstream task (i.e., transferability) to that of the source task. Concretely, our bound explains transferability in terms of the Wasserstein distance between the transformed source and downstream task's distribution, conditional entropy between the label distributions of the two tasks, and weighted loss of the source classifier on the source task. Moreover, we propose an optimization problem for learning the transforms of the source task to minimize the upper bound on transferability. We perform a large-scale empirical study by using state-of-the-art pre-trained models and demonstrate the effectiveness of our bound and optimization at predicting transferability. The results of our experiments demonstrate how factors such as task relatedness, pretraining method, and model architecture affect transferability.
</details>
<details>
<summary>摘要</summary>
<<SYS>> Transfer learning 是一种将模型从源任务中的知识转移到多个下游任务上，并且只需 minimal 微调。由于转移学习在提高性能的能力，特别是使用大型预训练模型，因此转移学习已成为机器学习工具箱中的一种重要工具。但是，转移学习下的性能是如何转移的？在这项工作中，我们分析了 classification 任务下的性能转移，只有源模型的最后 Linear layer 微调。我们提出了一种 Task Transfer Analysis 方法，将源分布（和分类器）变换成新的源分布（和分类器），以生成一个新的源分布（和分类器），并允许我们将下游任务的损失（即转移性）与源任务的损失相关联。具体来说，我们的 bound 将转移性分解为 Wasserstein 距离 between 源和下游任务的分布、下游任务的标签分布和源任务的标签分布之间的 conditional entropy，以及源任务中源分类器的权重损失。此外，我们提出了一个优化问题，用于学习转移任务中的转换，以最小化转移性的上限。我们通过使用现有的预训练模型进行大规模的实验研究，并证明了我们的 bound 和优化方法的效果。实验结果表明，转移学习中的因素，如任务相关性、预训练方法和模型结构，对转移性产生了影响。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="A-Critical-Re-evaluation-of-Benchmark-Datasets-for-Deep-Learning-Based-Matching-Algorithms"><a href="#A-Critical-Re-evaluation-of-Benchmark-Datasets-for-Deep-Learning-Based-Matching-Algorithms" class="headerlink" title="A Critical Re-evaluation of Benchmark Datasets for (Deep) Learning-Based Matching Algorithms"></a>A Critical Re-evaluation of Benchmark Datasets for (Deep) Learning-Based Matching Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01231">http://arxiv.org/abs/2307.01231</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gpapadis/dlmatchers">https://github.com/gpapadis/dlmatchers</a></li>
<li>paper_authors: George Papadakis, Nishadi Kirielle, Peter Christen, Themis Palpanas</li>
<li>for: 本研究旨在评估Established datasets的难度和适用性，以便更好地评估学习型匹配算法的性能。</li>
<li>methods: 本研究提出了四种方法来评估13个Established datasets的难度和适用性，包括两种理论方法和两种实践方法。</li>
<li>results: 研究发现，大多数流行的Established datasets pose relatively easy classification tasks，因此不适合评估学习型匹配算法的性能。为此，本研究提出了一种新的方法ología para生成benchmark datasets，并在实践中创建了四个新匹配任务，以便更好地评估学习型匹配算法的性能。<details>
<summary>Abstract</summary>
Entity resolution (ER) is the process of identifying records that refer to the same entities within one or across multiple databases. Numerous techniques have been developed to tackle ER challenges over the years, with recent emphasis placed on machine and deep learning methods for the matching phase. However, the quality of the benchmark datasets typically used in the experimental evaluations of learning-based matching algorithms has not been examined in the literature. To cover this gap, we propose four different approaches to assessing the difficulty and appropriateness of 13 established datasets: two theoretical approaches, which involve new measures of linearity and existing measures of complexity, and two practical approaches: the difference between the best non-linear and linear matchers, as well as the difference between the best learning-based matcher and the perfect oracle. Our analysis demonstrates that most of the popular datasets pose rather easy classification tasks. As a result, they are not suitable for properly evaluating learning-based matching algorithms. To address this issue, we propose a new methodology for yielding benchmark datasets. We put it into practice by creating four new matching tasks, and we verify that these new benchmarks are more challenging and therefore more suitable for further advancements in the field.
</details>
<details>
<summary>摘要</summary>
<<SYS>>传统的实体解决（ER）过程是将多个数据库中的记录与同一实体相匹配。随着年代的推移，各种技术已经被开发出来解决ER挑战，其中最近几年的研究主要集中在机器学习和深度学习方法中。然而，学术界中使用的标准数据集的质量尚未得到文献中的检查。为了填补这个差距，我们提出了四种方法来评估13个已知数据集的难度和适用性：两种理论方法，即基于新的线性度量和现有的复杂度度量，以及两种实践方法：非线性和线性匹配器之间的差异，以及学习基于匹配器和完美oracle之间的差异。我们的分析表明，大多数流行的数据集都是较为容易的分类任务，因此它们不适用于评估学习基于匹配算法的进展。为了解决这个问题，我们提出了一种新的方法来生成标准数据集。我们将其应用于四个新匹配任务中，并证明这些新的标准数据集更适合进一步的发展。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-and-Text-to-3D-Models-for-Engineering-Design-Optimization"><a href="#Large-Language-and-Text-to-3D-Models-for-Engineering-Design-Optimization" class="headerlink" title="Large Language and Text-to-3D Models for Engineering Design Optimization"></a>Large Language and Text-to-3D Models for Engineering Design Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01230">http://arxiv.org/abs/2307.01230</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thiago Rios, Stefan Menzel, Bernhard Sendhoff</li>
<li>for: 这 paper 是为了研究深度文本-3D 模型在工程领域中的潜在应用，尤其是在计算机 simulate 基于的设计优化中。</li>
<li>methods: 这 paper 使用了 Shap-E，一个最近发布的文本-3D 资产网络，以自动化进化设计优化框架。</li>
<li>results: 研究发现，在使用文本提示中，需要确保生成的设计是在物品类中有效，而且需要进一步研究以确定文本提示的变化和3D 设计变化之间的相关性，以提高优化。<details>
<summary>Abstract</summary>
The current advances in generative AI for learning large neural network models with the capability to produce essays, images, music and even 3D assets from text prompts create opportunities for a manifold of disciplines. In the present paper, we study the potential of deep text-to-3D models in the engineering domain, with focus on the chances and challenges when integrating and interacting with 3D assets in computational simulation-based design optimization. In contrast to traditional design optimization of 3D geometries that often searches for the optimum designs using numerical representations, such as B-Spline surface or deformation parameters in vehicle aerodynamic optimization, natural language challenges the optimization framework by requiring a different interpretation of variation operators while at the same time may ease and motivate the human user interaction. Here, we propose and realize a fully automated evolutionary design optimization framework using Shap-E, a recently published text-to-3D asset network by OpenAI, in the context of aerodynamic vehicle optimization. For representing text prompts in the evolutionary optimization, we evaluate (a) a bag-of-words approach based on prompt templates and Wordnet samples, and (b) a tokenisation approach based on prompt templates and the byte pair encoding method from GPT4. Our main findings from the optimizations indicate that, first, it is important to ensure that the designs generated from prompts are within the object class of application, i.e. diverse and novel designs need to be realistic, and, second, that more research is required to develop methods where the strength of text prompt variations and the resulting variations of the 3D designs share causal relations to some degree to improve the optimization.
</details>
<details>
<summary>摘要</summary>
当前的生成AI技术在学习大规模神经网络模型方面带来了许多机会，这些模型可以从文本提示生成文章、图像、音乐和 même 3D 资产。在 presente 纸上，我们研究了在工程领域中深度文本-3D 模型的潜在力量，特别是在 Computational  simulation-based 设计优化中交互和结合 3D 资产的机会和挑战。与传统的3D 结构设计优化不同，通过 numerical 表示（如 B-Spline 表面或者扭变参数在汽车 aerodynamic 优化中），文本挑战了优化框架，需要不同的变量操作符的解释，同时可能使人工用户交互更加容易和动机。在这里，我们提出了一个完全自动化的进化式设计优化框架，使用 OpenAI 最近发布的 Shap-E 文本-3D 资产网络。为表示文本提示在进化优化中，我们评估了（a）使用提示模板和 Wordnet 样本的袋子-of-words 方法，以及（b）使用提示模板和 GPT4 的字节对应方法。我们的主要发现表明，首先，需要确保生成的设计是在应用对象类中的，即文本提示生成的设计需要是多样化、创新的，同时也需要是真实的。其次，需要进一步的研究，以发展方法，使文本提示的变化强度和生成的 3D 设计之间存在相互 causal 关系，以改善优化。
</details></li>
</ul>
<hr>
<h2 id="Monte-Carlo-Policy-Gradient-Method-for-Binary-Optimization"><a href="#Monte-Carlo-Policy-Gradient-Method-for-Binary-Optimization" class="headerlink" title="Monte Carlo Policy Gradient Method for Binary Optimization"></a>Monte Carlo Policy Gradient Method for Binary Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00783">http://arxiv.org/abs/2307.00783</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/optsuite/mcpg">https://github.com/optsuite/mcpg</a></li>
<li>paper_authors: Cheng Chen, Ruitao Chen, Tianyou Li, Ruichen Ao, Zaiwen Wen</li>
<li>for: 这 paper 是为了解决 combinatorial optimization 问题，如 MaxCut、MIMO detection 和 MaxSAT。</li>
<li>methods: 这 paper 使用了一种新的 probabilistic model，以采样 binary solution  according to a parameterized policy distribution。</li>
<li>results: 这 paper 的结果表明，使用这种方法可以提供 near-optimal 的解决方案，并且 convergence 性能良好。<details>
<summary>Abstract</summary>
Binary optimization has a wide range of applications in combinatorial optimization problems such as MaxCut, MIMO detection, and MaxSAT. However, these problems are typically NP-hard due to the binary constraints. We develop a novel probabilistic model to sample the binary solution according to a parameterized policy distribution. Specifically, minimizing the KL divergence between the parameterized policy distribution and the Gibbs distributions of the function value leads to a stochastic optimization problem whose policy gradient can be derived explicitly similar to reinforcement learning. For coherent exploration in discrete spaces, parallel Markov Chain Monte Carlo (MCMC) methods are employed to sample from the policy distribution with diversity and approximate the gradient efficiently. We further develop a filter scheme to replace the original objective function by the one with the local search technique to broaden the horizon of the function landscape. Convergence to stationary points in expectation of the policy gradient method is established based on the concentration inequality for MCMC. Numerical results show that this framework is very promising to provide near-optimal solutions for quite a few binary optimization problems.
</details>
<details>
<summary>摘要</summary>
二进制优化具有广泛的应用于 combinatorial 优化问题中，如 MaxCut、MIMO 探测和 MaxSAT。然而，这些问题通常是 NP-hard 的由于二进制约束。我们开发了一种新的 probabilistic 模型，以采样二进制解决方案根据参数化的政策分布。 Specifically，将参数化的政策分布与 Gibbs 分布的函数值进行 minimum KL divergence 会导致一个随机优化问题，其策略均匀可以明确地 derivation 如抽象学习。为了在离散空间中具有协调的探索，我们使用并行 Markov Chain Monte Carlo (MCMC) 方法来采样政策分布中的多样性和精准地计算 gradient。我们进一步开发了一种筛选方案，将原始目标函数换换为具有本地搜索技术的目标函数，以拓宽目标函数的地平。基于 MCMC 的归一化不等式，我们证明了策略梯度法在预期中 converges to stationary points。 numerics 表明，这一框架非常有 promise 可以为许多二进制优化问题提供near-optimal 解决方案。
</details></li>
</ul>
<hr>
<h2 id="GA-DRL-Graph-Neural-Network-Augmented-Deep-Reinforcement-Learning-for-DAG-Task-Scheduling-over-Dynamic-Vehicular-Clouds"><a href="#GA-DRL-Graph-Neural-Network-Augmented-Deep-Reinforcement-Learning-for-DAG-Task-Scheduling-over-Dynamic-Vehicular-Clouds" class="headerlink" title="GA-DRL: Graph Neural Network-Augmented Deep Reinforcement Learning for DAG Task Scheduling over Dynamic Vehicular Clouds"></a>GA-DRL: Graph Neural Network-Augmented Deep Reinforcement Learning for DAG Task Scheduling over Dynamic Vehicular Clouds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00777">http://arxiv.org/abs/2307.00777</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhang Liu, Lianfen Huang, Zhibin Gao, Manman Luo, Seyyedali Hosseinalipour, Huaiyu Dai</li>
<li>for: 本文提出了一种基于图神经网络和深度强化学习的方法来调度在动态车辆云（VC）上执行计算密集任务。</li>
<li>methods: 本文使用了一种基于多头图注意力网络（GAT）的方法，通过同时考虑每个子任务的前一个和后一个任务，提取了DAG任务的特征。此外，该方法还引入了不均匀DAG任务邻域采样，使其能够适应完全未seen的DAG任务拓扑。</li>
<li>results: 通过在实际的车辆运动轨迹上模拟多种DAG任务，研究人员发现，GA-DRL方法在DAG任务完成时间方面表现出了超过现有标准准则的优势。<details>
<summary>Abstract</summary>
Vehicular clouds (VCs) are modern platforms for processing of computation-intensive tasks over vehicles. Such tasks are often represented as directed acyclic graphs (DAGs) consisting of interdependent vertices/subtasks and directed edges. In this paper, we propose a graph neural network-augmented deep reinforcement learning scheme (GA-DRL) for scheduling DAG tasks over dynamic VCs. In doing so, we first model the VC-assisted DAG task scheduling as a Markov decision process. We then adopt a multi-head graph attention network (GAT) to extract the features of DAG subtasks. Our developed GAT enables a two-way aggregation of the topological information in a DAG task by simultaneously considering predecessors and successors of each subtask. We further introduce non-uniform DAG neighborhood sampling through codifying the scheduling priority of different subtasks, which makes our developed GAT generalizable to completely unseen DAG task topologies. Finally, we augment GAT into a double deep Q-network learning module to conduct subtask-to-vehicle assignment according to the extracted features of subtasks, while considering the dynamics and heterogeneity of the vehicles in VCs. Through simulating various DAG tasks under real-world movement traces of vehicles, we demonstrate that GA-DRL outperforms existing benchmarks in terms of DAG task completion time.
</details>
<details>
<summary>摘要</summary>
自动车云（VC）是现代计算密集任务处理平台。这些任务经常表示为导向无环图（DAG）中的依赖关系，其中每个子任务之间存在指向关系。在本文中，我们提出了基于图神经网络和深度强化学习的GA-DRL方案，用于VC上进行DAG任务调度。在实现这一点上，我们首先将VC协助DAG任务调度模型为Markov决策过程。然后，我们采用多头图注意网络（GAT）来提取DAG子任务的特征。我们开发的GAT允许同时考虑每个子任务的前一个和后一个任务，从而实现两个方向的维度汇集。此外，我们还引入非均匀DAG邻居采样，通过编码调度优先级不同的子任务，使我们的GAT普适于完全未seen DAG任务拓扑。最后，我们将GAT与double deep Q-network学习模块结合，以进行子任务与车辆的具体分配，并考虑车辆在VC中的动态和多样性。通过对各种DAG任务进行真实世界车辆运动轨迹的模拟，我们示出GA-DRL方案在DAG任务完成时间方面的优越性。
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-Open-vocabulary-Universal-Image-Segmentation"><a href="#Hierarchical-Open-vocabulary-Universal-Image-Segmentation" class="headerlink" title="Hierarchical Open-vocabulary Universal Image Segmentation"></a>Hierarchical Open-vocabulary Universal Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00764">http://arxiv.org/abs/2307.00764</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/berkeley-hipie/hipie">https://github.com/berkeley-hipie/hipie</a></li>
<li>paper_authors: Xudong Wang, Shufan Li, Konstantinos Kallidromitis, Yusuke Kato, Kazuki Kozuka, Trevor Darrell</li>
<li>for: 这篇论文targets open-vocabulary image segmentation, aiming to partition an image into semantic regions based on arbitrary text descriptions.</li>
<li>methods: 该方法使用了一个嵌入式表示学习机制，以解决图像描述语言中的抽象层次问题。它还包括一个分离的文本-图像融合机制和表示学习模块。</li>
<li>results: 该模型名为HIPIE，可以同时解决多级嵌入 semantics, open-vocabulary, and universal segmentation tasks。在多个dataset上（如ADE20K、COCO、Pascal-VOC Part、RefCOCO&#x2F;RefCOCOg、ODinW和SeginW）进行了测试，HIPIE在不同的图像理解水平（如semantic segmentation、panoptic&#x2F;referring segmentation、object detection和part&#x2F;subpart segmentation）中达到了state-of-the-art的结果。<details>
<summary>Abstract</summary>
Open-vocabulary image segmentation aims to partition an image into semantic regions according to arbitrary text descriptions. However, complex visual scenes can be naturally decomposed into simpler parts and abstracted at multiple levels of granularity, introducing inherent segmentation ambiguity. Unlike existing methods that typically sidestep this ambiguity and treat it as an external factor, our approach actively incorporates a hierarchical representation encompassing different semantic-levels into the learning process. We propose a decoupled text-image fusion mechanism and representation learning modules for both "things" and "stuff".1 Additionally, we systematically examine the differences that exist in the textual and visual features between these types of categories. Our resulting model, named HIPIE, tackles HIerarchical, oPen-vocabulary, and unIvErsal segmentation tasks within a unified framework. Benchmarked on over 40 datasets, e.g., ADE20K, COCO, Pascal-VOC Part, RefCOCO/RefCOCOg, ODinW and SeginW, HIPIE achieves the state-of-the-art results at various levels of image comprehension, including semantic-level (e.g., semantic segmentation), instance-level (e.g., panoptic/referring segmentation and object detection), as well as part-level (e.g., part/subpart segmentation) tasks. Our code is released at https://github.com/berkeley-hipie/HIPIE.
</details>
<details>
<summary>摘要</summary>
开放词汇图像分割目标是将图像 partition 成Semantic 区域，根据自由文本描述。然而，复杂的视觉场景可以自然地被 decomposed 成更简单的部分，并且在不同的粒度上进行抽象，从而引入内在的分割抽象。不同于现有方法，我们的方法 actively 包含层次结构表示，以便在学习过程中吸收不同 semantic 级别的信息。我们提出了解释文本-图像融合机制和表示学习模块，用于处理 "thing" 和 "stuff" 两类不同的概念。此外，我们系统性地研究了这两类概念之间的文本特征和视觉特征之间的差异。我们的模型，名为 HIPIE，可以同时解决多级图像理解任务，包括层次、开放词汇、不同类别的图像分割任务。我们在超过40个数据集上进行了 benchmarking，包括 ADE20K、COCO、Pascal-VOC Part、RefCOCO/RefCOCOg、ODinW 和 SeginW，HIPIE 在不同的图像理解水平上达到了状态之前的最佳结果，包括semantic-level（例如semantic segmentation）、instance-level（例如panoptic/referring segmentation和对象检测）以及part-level（例如part/subpart segmentation）任务。我们的代码可以在 GitHub 上获取：https://github.com/berkeley-hipie/HIPIE。
</details></li>
</ul>
<hr>
<h2 id="EmoGen-Eliminating-Subjective-Bias-in-Emotional-Music-Generation"><a href="#EmoGen-Eliminating-Subjective-Bias-in-Emotional-Music-Generation" class="headerlink" title="EmoGen: Eliminating Subjective Bias in Emotional Music Generation"></a>EmoGen: Eliminating Subjective Bias in Emotional Music Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01229">http://arxiv.org/abs/2307.01229</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/muzic">https://github.com/microsoft/muzic</a></li>
<li>paper_authors: Chenfei Kang, Peiling Lu, Botao Yu, Xu Tan, Wei Ye, Shikun Zhang, Jiang Bian</li>
<li>for: 本研究旨在生成具有情感特征的音乐，以便在自动音乐生成方面提高情感表达的能力。</li>
<li>methods: 本研究提出了一种基于情感相关音乐特征的音乐生成系统，即 EmoGen。该系统包括两个阶段：首先，使用supervised clustering将情感标签映射到音乐特征上，然后，使用自动学习将音乐特征映射到音乐序列上。两个阶段都有利于提高音乐质量和情感控制精度。</li>
<li>results: 对于emotion control accuracy和音乐质量，EmoGen的表现都超过了之前的方法。具体来说，EmoGen在情感控制精度方面的表现提高了15.6%，而音乐质量方面的表现提高了22.4%。这些结果表明EmoGen在生成情感强的音乐方面具有优势。<details>
<summary>Abstract</summary>
Music is used to convey emotions, and thus generating emotional music is important in automatic music generation. Previous work on emotional music generation directly uses annotated emotion labels as control signals, which suffers from subjective bias: different people may annotate different emotions on the same music, and one person may feel different emotions under different situations. Therefore, directly mapping emotion labels to music sequences in an end-to-end way would confuse the learning process and hinder the model from generating music with general emotions. In this paper, we propose EmoGen, an emotional music generation system that leverages a set of emotion-related music attributes as the bridge between emotion and music, and divides the generation into two stages: emotion-to-attribute mapping with supervised clustering, and attribute-to-music generation with self-supervised learning. Both stages are beneficial: in the first stage, the attribute values around the clustering center represent the general emotions of these samples, which help eliminate the impacts of the subjective bias of emotion labels; in the second stage, the generation is completely disentangled from emotion labels and thus free from the subjective bias. Both subjective and objective evaluations show that EmoGen outperforms previous methods on emotion control accuracy and music quality respectively, which demonstrate our superiority in generating emotional music. Music samples generated by EmoGen are available via this link:https://ai-muzic.github.io/emogen/, and the code is available at this link:https://github.com/microsoft/muzic/.
</details>
<details>
<summary>摘要</summary>
音乐可以传递情感，因此自动生成情感rich的音乐是非常重要的。在过去的工作中，情感音乐生成直接使用了标注的情感标签作为控制信号，但这会受到主观偏见的影响：不同的人可能对同一首音乐的情感标签进行不同的标注，一个人在不同的情况下可能会感受到不同的情感。因此，直接将情感标签映射到音乐序列的方式会诱导学习过程中的混乱，使模型无法生成普遍的情感音乐。在这篇论文中，我们提出了Emotion Music Generation（EmoGen）系统，该系统利用了一组情感相关的音乐特征作为情感和音乐之间的桥梁，并将生成分为两个阶段：情感到特征映射与监督聚合，以及特征到音乐生成与自我监督学习。两个阶段都是有利的：在第一个阶段，特征值附近的聚合中心表示这些样本的普遍情感，这有助于消除主观偏见的情感标签的影响；在第二个阶段，生成完全不受情感标签的影响，因此免除了主观偏见的问题。两种评价方法（主观和客观）都表明EmoGen在情感控制准确性和音乐质量方面超越了之前的方法，这表明我们在生成情感音乐方面的优势。EmoGen生成的音乐样本可以在这里找到：https://ai-muzic.github.io/emogen/，代码可以在这里找到：https://github.com/microsoft/muzic/。
</details></li>
</ul>
<hr>
<h2 id="Graph-level-Anomaly-Detection-via-Hierarchical-Memory-Networks"><a href="#Graph-level-Anomaly-Detection-via-Hierarchical-Memory-Networks" class="headerlink" title="Graph-level Anomaly Detection via Hierarchical Memory Networks"></a>Graph-level Anomaly Detection via Hierarchical Memory Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00755">http://arxiv.org/abs/2307.00755</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/niuchx/himnet">https://github.com/niuchx/himnet</a></li>
<li>paper_authors: Chaoxi Niu, Guansong Pang, Ling Chen</li>
<li>for: 本研究旨在提出一种新的图数据异常检测方法，用于检测图像中的异常图。</li>
<li>methods: 本方法使用嵌入式自适应神经网络，学习图像中的细腻和整体正常模式，并将其组织成两个层次的内存模块：节点级别内存模块和图像级别内存模块。</li>
<li>results: 对于16种真实的图像数据集，本方法在检测本地异常图和全局异常图方面具有显著的优势，并且具有较高的抗异常杂质性能。代码可以在 GitHub 上获取：<a target="_blank" rel="noopener" href="https://github.com/Niuchx/HimNet%E3%80%82">https://github.com/Niuchx/HimNet。</a><details>
<summary>Abstract</summary>
Graph-level anomaly detection aims to identify abnormal graphs that exhibit deviant structures and node attributes compared to the majority in a graph set. One primary challenge is to learn normal patterns manifested in both fine-grained and holistic views of graphs for identifying graphs that are abnormal in part or in whole. To tackle this challenge, we propose a novel approach called Hierarchical Memory Networks (HimNet), which learns hierarchical memory modules -- node and graph memory modules -- via a graph autoencoder network architecture. The node-level memory module is trained to model fine-grained, internal graph interactions among nodes for detecting locally abnormal graphs, while the graph-level memory module is dedicated to the learning of holistic normal patterns for detecting globally abnormal graphs. The two modules are jointly optimized to detect both locally- and globally-anomalous graphs. Extensive empirical results on 16 real-world graph datasets from various domains show that i) HimNet significantly outperforms the state-of-art methods and ii) it is robust to anomaly contamination. Codes are available at: https://github.com/Niuchx/HimNet.
</details>
<details>
<summary>摘要</summary>
格式化检测目标是找到异常图形和节点特征相比多数图形集中的异常图形。一个主要挑战是学习图形集中正常模式，包括细致和总体两个视图。为解决这个挑战，我们提出了一种新的方法 called Hierarchical Memory Networks (HimNet)，它通过图像自动编码网络架构学习层次记忆模块——节点记忆模块和图形记忆模块。节点级别记忆模块用于模型节点之间的细致相互作用，以检测本地异常图形，而图形级别记忆模块则专门学习总体正常模式，以检测全球异常图形。两个模块被联合优化，以检测本地和全球异常图形。我们的实验结果表明，i) HimNet significantly 超过了当前方法，ii) 它对异常污染有良好的鲁棒性。代码可以在 <https://github.com/Niuchx/HimNet> 找到。
</details></li>
</ul>
<hr>
<h2 id="ImDiffusion-Imputed-Diffusion-Models-for-Multivariate-Time-Series-Anomaly-Detection"><a href="#ImDiffusion-Imputed-Diffusion-Models-for-Multivariate-Time-Series-Anomaly-Detection" class="headerlink" title="ImDiffusion: Imputed Diffusion Models for Multivariate Time Series Anomaly Detection"></a>ImDiffusion: Imputed Diffusion Models for Multivariate Time Series Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00754">http://arxiv.org/abs/2307.00754</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/17000cyh/imdiffusion">https://github.com/17000cyh/imdiffusion</a></li>
<li>paper_authors: Yuhang Chen, Chaoyun Zhang, Minghua Ma, Yudong Liu, Ruomeng Ding, Bowen Li, Shilin He, Saravan Rajmohan, Qingwei Lin, Dongmei Zhang</li>
<li>for: 这篇论文的目的是为了提出一个新的多重时间序列资料异常检测方法，以解决现有方法的限制。</li>
<li>methods: 这篇论文使用了时间序列替代模型和扩散模型，实现精确和可靠的异常检测。它还使用时间序列替代模型来实现精确的时间序列预测，并且利用步骤实现过程中的推导出PUTS为异常预测提供有用的信号。</li>
<li>results: 这篇论文的实验结果显示，与现有方法比较，ImDiffusion在检测精度和时间上都有明显的进步。尤其是在Microsoft的生产环境中，ImDiffusion的检测F1分数提高了11.4%。<details>
<summary>Abstract</summary>
Anomaly detection in multivariate time series data is of paramount importance for ensuring the efficient operation of large-scale systems across diverse domains. However, accurately detecting anomalies in such data poses significant challenges. Existing approaches, including forecasting and reconstruction-based methods, struggle to address these challenges effectively. To overcome these limitations, we propose a novel anomaly detection framework named ImDiffusion, which combines time series imputation and diffusion models to achieve accurate and robust anomaly detection. The imputation-based approach employed by ImDiffusion leverages the information from neighboring values in the time series, enabling precise modeling of temporal and inter-correlated dependencies, reducing uncertainty in the data, thereby enhancing the robustness of the anomaly detection process. ImDiffusion further leverages diffusion models as time series imputers to accurately capturing complex dependencies. We leverage the step-by-step denoised outputs generated during the inference process to serve as valuable signals for anomaly prediction, resulting in improved accuracy and robustness of the detection process.   We evaluate the performance of ImDiffusion via extensive experiments on benchmark datasets. The results demonstrate that our proposed framework significantly outperforms state-of-the-art approaches in terms of detection accuracy and timeliness. ImDiffusion is further integrated into the real production system in Microsoft and observe a remarkable 11.4% increase in detection F1 score compared to the legacy approach. To the best of our knowledge, ImDiffusion represents a pioneering approach that combines imputation-based techniques with time series anomaly detection, while introducing the novel use of diffusion models to the field.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate the following text into Simplified Chinese:Anomaly detection in multivariate time series data is of paramount importance for ensuring the efficient operation of large-scale systems across diverse domains. However, accurately detecting anomalies in such data poses significant challenges. Existing approaches, including forecasting and reconstruction-based methods, struggle to address these challenges effectively. To overcome these limitations, we propose a novel anomaly detection framework named ImDiffusion, which combines time series imputation and diffusion models to achieve accurate and robust anomaly detection. The imputation-based approach employed by ImDiffusion leverages the information from neighboring values in the time series, enabling precise modeling of temporal and inter-correlated dependencies, reducing uncertainty in the data, thereby enhancing the robustness of the anomaly detection process. ImDiffusion further leverages diffusion models as time series imputers to accurately capturing complex dependencies. We leverage the step-by-step denoised outputs generated during the inference process to serve as valuable signals for anomaly prediction, resulting in improved accuracy and robustness of the detection process.  We evaluate the performance of ImDiffusion via extensive experiments on benchmark datasets. The results demonstrate that our proposed framework significantly outperforms state-of-the-art approaches in terms of detection accuracy and timeliness. ImDiffusion is further integrated into the real production system in Microsoft and observe a remarkable 11.4% increase in detection F1 score compared to the legacy approach. To the best of our knowledge, ImDiffusion represents a pioneering approach that combines imputation-based techniques with time series anomaly detection, while introducing the novel use of diffusion models to the field.中文简体版：针对多变量时间序列数据，精准检测异常现象非常重要，以确保大规模系统在多个领域中具有高效的运行。然而，在这种数据中异常检测存在 significativetransportation challenges。现有的方法，包括预测和重建方法，尝试解决这些挑战，但是效果不够。为了超越这些限制，我们提出了一种新的异常检测框架，名为ImDiffusion，它将时间序列插入和扩散模型结合使用，以实现高精度和Robustness的异常检测。ImDiffusion中使用的插入方法利用时间序列中的邻居值信息，准确地模型时间和相关性的依赖关系，从而减少数据中的不确定性，提高异常检测过程的稳定性。ImDiffusion还利用扩散模型作为时间序列插入器，以准确地捕捉复杂的依赖关系。我们利用推理过程中的步骤脱氧输出，作为异常预测的有价值信号，从而提高异常检测的精度和稳定性。我们通过对ImDiffusion进行了广泛的实验，证明了我们提出的框架在检测精度和快速性方面具有明显的优势。ImDiffusion还被 Microsoft 的实际生产系统中 интегра，并观察到了11.4%的增强检测 F1 分数。到目前为止，ImDiffusion 是我们所知道的首个将插入基本技术与时间序列异常检测结合的方法，同时引入扩散模型到该领域。
</details></li>
</ul>
<hr>
<h2 id="Population-Age-Group-Sensitivity-for-COVID-19-Infections-with-Deep-Learning"><a href="#Population-Age-Group-Sensitivity-for-COVID-19-Infections-with-Deep-Learning" class="headerlink" title="Population Age Group Sensitivity for COVID-19 Infections with Deep Learning"></a>Population Age Group Sensitivity for COVID-19 Infections with Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00751">http://arxiv.org/abs/2307.00751</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Khairul Islam, Tyler Valentine, Royal Wang, Levi Davis, Matt Manner, Judy Fox<br>for: 这种研究的目的是为了在美国县级别上确定COVID-19感染率中最有影响力的年龄组。methods: 这个研究使用Modified Morris Method和深度学习时序分析来确定年龄组对COVID-19感染率的影响。研究首先在不同的年龄组中训练了现有的时序模型Temporal Fusion Transformer，然后对不同的年龄组进行了特征敏感分析，并根据每个输入特征的柯尼斯基敏感分数（ Morris sensitivity scores）进行排名。results: 研究发现，在COVID-19传播过程中，年龄组最有影响力的是20-29岁的年轻人。这些结果通过美国卫生部和美国人口普查局提供的真实感染率数据得到了验证。这些结果可以用于改进公共卫生政策和 intervención，例如targeted疫苗接种策略，以更好地控制病毒的传播。<details>
<summary>Abstract</summary>
The COVID-19 pandemic has created unprecedented challenges for governments and healthcare systems worldwide, highlighting the critical importance of understanding the factors that contribute to virus transmission. This study aimed to identify the most influential age groups in COVID-19 infection rates at the US county level using the Modified Morris Method and deep learning for time series. Our approach involved training the state-of-the-art time-series model Temporal Fusion Transformer on different age groups as a static feature and the population vaccination status as the dynamic feature. We analyzed the impact of those age groups on COVID-19 infection rates by perturbing individual input features and ranked them based on their Morris sensitivity scores, which quantify their contribution to COVID-19 transmission rates. The findings are verified using ground truth data from the CDC and US Census, which provide the true infection rates for each age group. The results suggest that young adults were the most influential age group in COVID-19 transmission at the county level between March 1, 2020, and November 27, 2021. Using these results can inform public health policies and interventions, such as targeted vaccination strategies, to better control the spread of the virus. Our approach demonstrates the utility of feature sensitivity analysis in identifying critical factors contributing to COVID-19 transmission and can be applied in other public health domains.
</details>
<details>
<summary>摘要</summary>
COVID-19 大流行带来了无 precedent 的挑战，让政府和医疗系统在全球各地面临巨大的挑战。这项研究的目的是通过 Modified Morris Method 和深度学习时序序列来确定 COVID-19 感染率在美国县级别中最有影响力的年龄组。我们的方法是在不同的年龄组作为静态特征，并将人口疫苗接种状况作为动态特征，使用时代混合trasformer 模型进行训练。我们分析了每个年龄组对 COVID-19 感染率的影响，并将其排序基于其 Morris 敏感度分数，该分数量化每个年龄组对 COVID-19 传播率的贡献。结果被验证使用 CDC 和 US Census 的真实感染率数据，这些数据提供了每个年龄组的真实感染率。结果表明在2020年3月1日至2021年11月27日之间，美国县级别中最有影响力的年龄组是年轻成人。使用这些结果可以更好地制定公共卫生政策和干预措施，以控制病毒的传播。我们的方法可以应用于其他公共卫生领域，以确定病毒传播中的关键因素。
</details></li>
</ul>
<hr>
<h2 id="ESGCN-Edge-Squeeze-Attention-Graph-Convolutional-Network-for-Traffic-Flow-Forecasting"><a href="#ESGCN-Edge-Squeeze-Attention-Graph-Convolutional-Network-for-Traffic-Flow-Forecasting" class="headerlink" title="ESGCN: Edge Squeeze Attention Graph Convolutional Network for Traffic Flow Forecasting"></a>ESGCN: Edge Squeeze Attention Graph Convolutional Network for Traffic Flow Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01227">http://arxiv.org/abs/2307.01227</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sangrok Lee, Ha Young Kim</li>
<li>for: 预测交通流量，提高交通预测精度</li>
<li>methods: 提posed Edge Squeeze Graph Convolutional Network (ESGCN)，包括W-module和ES module，通过Graph Convolutional Network (GCN)模型空间时间关系，并使用边特征 direktly capture spatial-temporal flow representation，以及edge attention mechanism和node contrastive loss进行约束</li>
<li>results: 实验结果表明，ESGCN在四个实际数据集（PEMS03、04、07、08）上达到了当前最佳性能水平，而且计算成本较低<details>
<summary>Abstract</summary>
Traffic forecasting is a highly challenging task owing to the dynamical spatio-temporal dependencies of traffic flows. To handle this, we focus on modeling the spatio-temporal dynamics and propose a network termed Edge Squeeze Graph Convolutional Network (ESGCN) to forecast traffic flow in multiple regions. ESGCN consists of two modules: W-module and ES module. W-module is a fully node-wise convolutional network. It encodes the time-series of each traffic region separately and decomposes the time-series at various scales to capture fine and coarse features. The ES module models the spatio-temporal dynamics using Graph Convolutional Network (GCN) and generates an Adaptive Adjacency Matrix (AAM) with temporal features. To improve the accuracy of AAM, we introduce three key concepts. 1) Using edge features to directly capture the spatiotemporal flow representation among regions. 2) Applying an edge attention mechanism to GCN to extract the AAM from the edge features. Here, the attention mechanism can effectively determine important spatio-temporal adjacency relations. 3) Proposing a novel node contrastive loss to suppress obstructed connections and emphasize related connections. Experimental results show that ESGCN achieves state-of-the-art performance by a large margin on four real-world datasets (PEMS03, 04, 07, and 08) with a low computational cost.
</details>
<details>
<summary>摘要</summary>
很多挑战在交通预测中，主要是由交通流动的空间时间相关性引起的。为了解决这个问题，我们关注了空间时间动态的模型化，并提出了一种名为 Edge Squeeze Graph Convolutional Network（ESGCN）来预测多个区域的交通流。ESGCN包括两个模块：W模块和ES模块。W模块是一个完全节点卷积网络，它在每个交通区域 separately 编码时间序列，并在不同的尺度分解时间序列来捕捉细致和大致特征。ES模块使用图aelastic network（GCN）模型了空间时间动态，并生成了一个 Adaptive Adjacency Matrix（AAM），其中包含了时间特征。为了提高AAM的准确性，我们提出了三个关键想法：1. 直接使用边特征来捕捉交通空间时间流表示。2. 应用边注意机制来GCN中提取AAM。这里注意机制可以有效地确定重要的空间时间相关关系。3. 提出了一种新的节点对比损失函数，用于抑制干扰连接和强调相关连接。实验结果表明，ESGCN在四个真实世界数据集（PEMS03、04、07和08）上 achieved state-of-the-art 性能，而且计算成本较低。
</details></li>
</ul>
<hr>
<h2 id="vONTSS-vMF-based-semi-supervised-neural-topic-modeling-with-optimal-transport"><a href="#vONTSS-vMF-based-semi-supervised-neural-topic-modeling-with-optimal-transport" class="headerlink" title="vONTSS: vMF based semi-supervised neural topic modeling with optimal transport"></a>vONTSS: vMF based semi-supervised neural topic modeling with optimal transport</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01226">http://arxiv.org/abs/2307.01226</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weijie Xu, Xiaoyu Jiang, Srinivasan H. Sengamedu, Francis Iannacci, Jinjin Zhao</li>
<li>for: This paper presents a semi-supervised neural topic modeling method, vONTSS, which aims to incorporate human knowledge into the topic modeling process.</li>
<li>methods: vONTSS uses von Mises-Fisher (vMF) based variational autoencoders and optimal transport to generate potential topics and optimize topic-keyword quality and topic classification.</li>
<li>results: Experiments show that vONTSS outperforms existing semi-supervised topic modeling methods in classification accuracy and diversity. Additionally, vONTSS in the unsupervised setting discovers highly clustered and coherent topics on benchmark datasets and is faster than recent NTMs while achieving similar classification performance.<details>
<summary>Abstract</summary>
Recently, Neural Topic Models (NTM), inspired by variational autoencoders, have attracted a lot of research interest; however, these methods have limited applications in the real world due to the challenge of incorporating human knowledge. This work presents a semi-supervised neural topic modeling method, vONTSS, which uses von Mises-Fisher (vMF) based variational autoencoders and optimal transport. When a few keywords per topic are provided, vONTSS in the semi-supervised setting generates potential topics and optimizes topic-keyword quality and topic classification. Experiments show that vONTSS outperforms existing semi-supervised topic modeling methods in classification accuracy and diversity. vONTSS also supports unsupervised topic modeling. Quantitative and qualitative experiments show that vONTSS in the unsupervised setting outperforms recent NTMs on multiple aspects: vONTSS discovers highly clustered and coherent topics on benchmark datasets. It is also much faster than the state-of-the-art weakly supervised text classification method while achieving similar classification performance. We further prove the equivalence of optimal transport loss and cross-entropy loss at the global minimum.
</details>
<details>
<summary>摘要</summary>
近期，神经话题模型（NTM），受变量自动编码器的激发，吸引了大量的研究兴趣；然而，这些方法在实际应用中受到人类知识的挑战。这项工作提出了一种半监督神经话题模型方法，vONTSS，它使用 von Mises-Fisher（vMF）基于的变量自动编码器和最优运输。当提供一些关键词时，vONTSS在半监督设置下生成可能的话题并优化话题-关键词质量和话题分类。实验显示，vONTSS比现有的半监督话题模型方法在分类精度和多样性方面表现更好。vONTSS还支持无监督话题模型。量化和质量实验表明，vONTSS在无监督设置下比最新的弱监督文本分类方法更快，并在类似的分类性能下达到类似的性能。我们进一步证明了最优运输损失和十字积分损失在全局最优点的等价性。
</details></li>
</ul>
<hr>
<h2 id="UnLoc-A-Universal-Localization-Method-for-Autonomous-Vehicles-using-LiDAR-Radar-and-or-Camera-Input"><a href="#UnLoc-A-Universal-Localization-Method-for-Autonomous-Vehicles-using-LiDAR-Radar-and-or-Camera-Input" class="headerlink" title="UnLoc: A Universal Localization Method for Autonomous Vehicles using LiDAR, Radar and&#x2F;or Camera Input"></a>UnLoc: A Universal Localization Method for Autonomous Vehicles using LiDAR, Radar and&#x2F;or Camera Input</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00741">http://arxiv.org/abs/2307.00741</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Ibrahim, Naveed Akhtar, Saeed Anwar, Ajmal Mian</li>
<li>for: 本研究旨在提出一种基于多感器输入的自主导航 robots 的本地化方法，以满足现有方法的缺点，如单一输入数据模式或需要训练多个计算模型来处理不同的感知数据。</li>
<li>methods: 本研究使用了一种名为 UnLoc 的新型 neural network 模型，可以同时处理 LiDAR、摄像头和 RADAR 输入数据，并且可以根据需要选择使用一个或多个输入感知器，从而提高了系统的可靠性和灵活性。</li>
<li>results: 研究人员通过对 Oxford Radar RobotCar、ApolloSouthBay 和 Perth-WA 数据集进行广泛的测试和评估，发现 UnLoc 方法可以准确地地址本地化问题，并且在不同的天气和环境下表现出色。<details>
<summary>Abstract</summary>
Localization is a fundamental task in robotics for autonomous navigation. Existing localization methods rely on a single input data modality or train several computational models to process different modalities. This leads to stringent computational requirements and sub-optimal results that fail to capitalize on the complementary information in other data streams. This paper proposes UnLoc, a novel unified neural modeling approach for localization with multi-sensor input in all weather conditions. Our multi-stream network can handle LiDAR, Camera and RADAR inputs for localization on demand, i.e., it can work with one or more input sensors, making it robust to sensor failure. UnLoc uses 3D sparse convolutions and cylindrical partitioning of the space to process LiDAR frames and implements ResNet blocks with a slot attention-based feature filtering module for the Radar and image modalities. We introduce a unique learnable modality encoding scheme to distinguish between the input sensor data. Our method is extensively evaluated on Oxford Radar RobotCar, ApolloSouthBay and Perth-WA datasets. The results ascertain the efficacy of our technique.
</details>
<details>
<summary>摘要</summary>
<<SYS>>rupiaoming: zhongguo yu dianzi de jiaoyu yu jingying zhongxinLocalization is a fundamental task in robotics for autonomous navigation. Existing localization methods rely on a single input data modality or train several computational models to process different modalities. This leads to stringent computational requirements and sub-optimal results that fail to capitalize on the complementary information in other data streams. This paper proposes UnLoc, a novel unified neural modeling approach for localization with multi-sensor input in all weather conditions. Our multi-stream network can handle LiDAR, Camera and RADAR inputs for localization on demand, i.e., it can work with one or more input sensors, making it robust to sensor failure. UnLoc uses 3D sparse convolutions and cylindrical partitioning of the space to process LiDAR frames and implements ResNet blocks with a slot attention-based feature filtering module for the Radar and image modalities. We introduce a unique learnable modality encoding scheme to distinguish between the input sensor data. Our method is extensively evaluated on Oxford Radar RobotCar, ApolloSouthBay and Perth-WA datasets. The results ascertain the efficacy of our technique.<</SYS>>Here's the translation in Simplified Chinese:<<SYS>>本文提出了一种基于多感器输入的LOCALIZATION方法，用于Robotics autonomous navigation。现有的LOCALIZATION方法通常仅使用单一的输入数据模式，或者训练多个计算模型来处理不同的数据模式。这会导致计算需求严格，并且不能充分利用其他数据流中的补充信息。本文提出了一种名为UnLoc的新的协调神经网络方法，可以处理LiDAR、摄像头和RADAR输入数据，并在不同的天气条件下进行定位。我们的多流网络可以根据需要选择一个或多个输入感知器，从而增强其对感知器失效的Robustness。在进行LiDAR框架处理时，我们使用3D稀疏核算法和圆柱体分割方法，并在RADAR和图像模式中实现了ResNet块和满足特征筛选模块。我们还提出了一种唯一的学习型感知编码方案，以便分辨输入感知器的数据。我们的方法在Oxford Radar RobotCar、ApolloSouthBay和Perth-WA数据集上进行了广泛的评估，结果证明了我们的方法的有效性。<</SYS>>
</details></li>
</ul>
<hr>
<h2 id="On-the-choice-of-training-data-for-machine-learning-of-geostrophic-mesoscale-turbulence"><a href="#On-the-choice-of-training-data-for-machine-learning-of-geostrophic-mesoscale-turbulence" class="headerlink" title="On the choice of training data for machine learning of geostrophic mesoscale turbulence"></a>On the choice of training data for machine learning of geostrophic mesoscale turbulence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00734">http://arxiv.org/abs/2307.00734</a></li>
<li>repo_url: None</li>
<li>paper_authors: F. E. Yan, J. Mak, Y. Wang</li>
<li>for: 这个论文是关于数据驱动方法在地球系统模型中的应用，特别是关于旋转层分布的热层湍流中的质量交换现象。</li>
<li>methods: 本论文使用了数据驱动方法来学习旋转层分布中的质量交换现象，并提供了对比或更好的能力和稳定性。</li>
<li>results: 研究发现，如果将旋转Component从质量交换流动中过滤掉，那么数据驱动模型的预测能力和稳定性会得到改善，而且可以更好地捕捉到数据中隐藏的物理过程。<details>
<summary>Abstract</summary>
'Data' plays a central role in data-driven methods, but is not often the subject of focus in investigations of machine learning algorithms as applied to Earth System Modeling related problems. Here we consider the case of eddy-mean interaction in rotating stratified turbulence in the presence of lateral boundaries, a problem of relevance to ocean modeling, where the eddy fluxes contain dynamically inert rotational components that are expected to contaminate the learning process. An often utilized choice in the literature is to learn from the divergence of the eddy fluxes. Here we provide theoretical arguments and numerical evidence that learning from the eddy fluxes with the rotational component appropriately filtered out results in models with comparable or better skill, but substantially improved robustness. If we simply want a data-driven model to have predictive skill then the choice of data choice and/or quality may not be critical, but we argue it is highly desirable and perhaps even necessary if we want to leverage data-driven methods to aid in discovering unknown or hidden physical processes within the data itself.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Interpretability-and-Transparency-Driven-Detection-and-Transformation-of-Textual-Adversarial-Examples-IT-DT"><a href="#Interpretability-and-Transparency-Driven-Detection-and-Transformation-of-Textual-Adversarial-Examples-IT-DT" class="headerlink" title="Interpretability and Transparency-Driven Detection and Transformation of Textual Adversarial Examples (IT-DT)"></a>Interpretability and Transparency-Driven Detection and Transformation of Textual Adversarial Examples (IT-DT)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01225">http://arxiv.org/abs/2307.01225</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bushra Sabir, M. Ali Babar, Sharif Abuadbba<br>for: 这个论文的目的是提出一种可解释性和透明度驱动的检测和转换（IT-DT）框架，以解决BERT等基于Transformer的文本分类器对于恶意示例的抵触性。methods: 这个框架使用了注意力地图、集成导数和模型反馈等技术来提高可解释性，以便在检测阶段更好地理解恶意分类的依据。在转换阶段，IT-DT使用预训练的嵌入和模型反馈来生成适当的替换，以将恶意示例转化为非恶意示例。results: 实验结果表明，IT-DT可以准确地检测和转换恶意示例，提高了模型的可靠性和安全性。此外，人工专家参与约束和反馈也使得决策更加稳定和可靠。<details>
<summary>Abstract</summary>
Transformer-based text classifiers like BERT, Roberta, T5, and GPT-3 have shown impressive performance in NLP. However, their vulnerability to adversarial examples poses a security risk. Existing defense methods lack interpretability, making it hard to understand adversarial classifications and identify model vulnerabilities. To address this, we propose the Interpretability and Transparency-Driven Detection and Transformation (IT-DT) framework. It focuses on interpretability and transparency in detecting and transforming textual adversarial examples. IT-DT utilizes techniques like attention maps, integrated gradients, and model feedback for interpretability during detection. This helps identify salient features and perturbed words contributing to adversarial classifications. In the transformation phase, IT-DT uses pre-trained embeddings and model feedback to generate optimal replacements for perturbed words. By finding suitable substitutions, we aim to convert adversarial examples into non-adversarial counterparts that align with the model's intended behavior while preserving the text's meaning. Transparency is emphasized through human expert involvement. Experts review and provide feedback on detection and transformation results, enhancing decision-making, especially in complex scenarios. The framework generates insights and threat intelligence empowering analysts to identify vulnerabilities and improve model robustness. Comprehensive experiments demonstrate the effectiveness of IT-DT in detecting and transforming adversarial examples. The approach enhances interpretability, provides transparency, and enables accurate identification and successful transformation of adversarial inputs. By combining technical analysis and human expertise, IT-DT significantly improves the resilience and trustworthiness of transformer-based text classifiers against adversarial attacks.
</details>
<details>
<summary>摘要</summary>
tranformer-based文本分类器如BERT、Roberta、T5和GPT-3在NLP中表现出色，但它们对攻击性示例的抵触性存在安全风险。现有的防御方法缺乏可读性，使得对攻击分类和模型漏洞难以理解。为了解决这个问题，我们提出了可读性和透明度驱动的检测和转换（IT-DT）框架。IT-DT将注重可读性和透明度，在检测阶段使用注意力地图、 интеGRATED GRADIENTS 和模型反馈来提供可读性。这 помоляет Identify 突出的特征和折衣字符在攻击分类中发挥作用。在转换阶段，IT-DT使用预训练的嵌入和模型反馈来生成适当的替换，以将攻击示例转化为不攻击的示例，保持文本的意义。在转换过程中，人工专家参与纠正和反馈，以增强决策，特别是在复杂的情况下。IT-DT生成了见解和威胁情报，使分析人员可以识别漏洞和提高模型 Robustness。广泛的实验表明IT-DT可以准确地检测和转换攻击示例。这种方法提高了可读性，提供了透明度，并使transformer-based文本分类器对攻击示例的抵觕性提高。通过结合技术分析和人工专家知识，IT-DT有效地提高了transformer-based文本分类器对攻击示例的抵觕性和可靠性。
</details></li>
</ul>
<hr>
<h2 id="Neural-Polytopes"><a href="#Neural-Polytopes" class="headerlink" title="Neural Polytopes"></a>Neural Polytopes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00721">http://arxiv.org/abs/2307.00721</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zfurman56/polytopes">https://github.com/zfurman56/polytopes</a></li>
<li>paper_authors: Koji Hashimoto, Tomoya Naito, Hisashi Naito</li>
<li>for: 这篇论文是为了研究用简单神经网络和ReLU活化函数生成规范体（polytopes）而写的。</li>
<li>methods: 论文使用简单神经网络和ReLU活化函数来生成规范体，并研究了不同活化函数的总体化。</li>
<li>results: 研究发现，使用简单神经网络和ReLU活化函数可以生成规范体，并且可以通过调整网络结构来控制规范体的种类和维度。此外，研究还发现了这些规范体的总体化，即神经规范体。<details>
<summary>Abstract</summary>
We find that simple neural networks with ReLU activation generate polytopes as an approximation of a unit sphere in various dimensions. The species of polytopes are regulated by the network architecture, such as the number of units and layers. For a variety of activation functions, generalization of polytopes is obtained, which we call neural polytopes. They are a smooth analogue of polytopes, exhibiting geometric duality. This finding initiates research of generative discrete geometry to approximate surfaces by machine learning.
</details>
<details>
<summary>摘要</summary>
我们发现简单的神经网络与ReLU吸引函数生成了多面体，作为各种维度上圆形的近似。这种多面体的种类受到神经网络架构的限制，如单元数和层数。对于不同的吸引函数，我们可以得到总化的多面体，我们称之为神经多面体。它们是一种缓和的多面体，展示了几何对偶。这一发现推动了机器学习来approximate Surfaces的生成推理 discrete geometry。
</details></li>
</ul>
<hr>
<h2 id="Worth-of-knowledge-in-deep-learning"><a href="#Worth-of-knowledge-in-deep-learning" class="headerlink" title="Worth of knowledge in deep learning"></a>Worth of knowledge in deep learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00712">http://arxiv.org/abs/2307.00712</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/woshixuhao/worth_of_knowledge">https://github.com/woshixuhao/worth_of_knowledge</a></li>
<li>paper_authors: Hao Xu, Yuntian Chen, Dongxiao Zhang</li>
<li>for: 本研究旨在探讨深度学习中知识的作用，以提高模型的泛化能力和约束遵循性。</li>
<li>methods: 本研究使用可解释Machine learning的框架，通过量化实验评估知识的价值，并分析数据和知识之间的复杂关系。</li>
<li>results: 研究发现，知识的价值受到数据量和估计范围的影响，存在依赖、协同和替换效应。这种结果可以应用于多种常见的网络架构，并且可以提高了知识汇报的性能和约束遵循性。<details>
<summary>Abstract</summary>
Knowledge constitutes the accumulated understanding and experience that humans use to gain insight into the world. In deep learning, prior knowledge is essential for mitigating shortcomings of data-driven models, such as data dependence, generalization ability, and compliance with constraints. To enable efficient evaluation of the worth of knowledge, we present a framework inspired by interpretable machine learning. Through quantitative experiments, we assess the influence of data volume and estimation range on the worth of knowledge. Our findings elucidate the complex relationship between data and knowledge, including dependence, synergistic, and substitution effects. Our model-agnostic framework can be applied to a variety of common network architectures, providing a comprehensive understanding of the role of prior knowledge in deep learning models. It can also be used to improve the performance of informed machine learning, as well as distinguish improper prior knowledge.
</details>
<details>
<summary>摘要</summary>
知识是人类使用来理解世界的总结和经验。在深度学习中，先验知识是关键的，可以减少数据驱动模型的缺陷，例如数据依赖、泛化能力和约束遵从。为了有效评估知识的价值，我们提出一种基于可解释机器学习的框架。通过量化实验，我们评估数据量和估计范围对知识的影响。我们的发现揭示了数据和知识之间复杂的关系，包括依赖、合作和替换效应。我们的框架可以应用于多种常见的网络架构，为深度学习模型的角色带来全面的理解。它还可以用来改进了知识填充机器学习的性能，以及分辨不正确的先验知识。
</details></li>
</ul>
<hr>
<h2 id="A-physics-constrained-machine-learning-method-for-mapping-gapless-land-surface-temperature"><a href="#A-physics-constrained-machine-learning-method-for-mapping-gapless-land-surface-temperature" class="headerlink" title="A physics-constrained machine learning method for mapping gapless land surface temperature"></a>A physics-constrained machine learning method for mapping gapless land surface temperature</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04817">http://arxiv.org/abs/2307.04817</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jun Ma, Huanfeng Shen, Menghui Jiang, Liupeng Lin, Chunlei Meng, Chao Zeng, Huifang Li, Penghai Wu</li>
<li>for: 这 paper 的目的是提出一种physics-constrained machine learning（PC-ML）模型，用于 gapless 土壤温度（LST）估算，以提高physical interpretability和抽象能力。</li>
<li>methods: 该模型 combines 机器学习（ML）模型和物理机制模型，并将 физические约束（PCs） incorporated 到 ML 模型中，以提高模型的解释能力和推断能力。</li>
<li>results: 对比 pure physical method 和 pure ML methods，PC-LGBM 模型提高了 LST 预测精度和physical interpretability，并demonstrated good extrapolation ability for extreme weather cases。这种方法可以提供高精度和物理意义的 gapless LST 估算，并可以加速土壤表面过程的研究和数据挖掘。<details>
<summary>Abstract</summary>
More accurate, spatio-temporally, and physically consistent LST estimation has been a main interest in Earth system research. Developing physics-driven mechanism models and data-driven machine learning (ML) models are two major paradigms for gapless LST estimation, which have their respective advantages and disadvantages. In this paper, a physics-constrained ML model, which combines the strengths in the mechanism model and ML model, is proposed to generate gapless LST with physical meanings and high accuracy. The hybrid model employs ML as the primary architecture, under which the input variable physical constraints are incorporated to enhance the interpretability and extrapolation ability of the model. Specifically, the light gradient-boosting machine (LGBM) model, which uses only remote sensing data as input, serves as the pure ML model. Physical constraints (PCs) are coupled by further incorporating key Community Land Model (CLM) forcing data (cause) and CLM simulation data (effect) as inputs into the LGBM model. This integration forms the PC-LGBM model, which incorporates surface energy balance (SEB) constraints underlying the data in CLM-LST modeling within a biophysical framework. Compared with a pure physical method and pure ML methods, the PC-LGBM model improves the prediction accuracy and physical interpretability of LST. It also demonstrates a good extrapolation ability for the responses to extreme weather cases, suggesting that the PC-LGBM model enables not only empirical learning from data but also rationally derived from theory. The proposed method represents an innovative way to map accurate and physically interpretable gapless LST, and could provide insights to accelerate knowledge discovery in land surface processes and data mining in geographical parameter estimation.
</details>
<details>
<summary>摘要</summary>
更准确、空间和时间一致、物理一致的LST估计已经是地球系统研究的主要兴趣点。开发物理驱动机制模型和数据驱动机器学习（ML）模型是两个主要方法 для无缝LST估计，它们各有优势和缺点。本文提出了一种物理约束机器学习（PC-ML）模型，它将机器学习模型作为主体，并将输入变量物理约束（PC）纳入模型中以提高解释性和推理能力。具体来说，使用远程感知数据为输入的光梯度提升机（LGBM）模型作为纯ML模型。PC通过将CLM的激活数据（原因）和CLM的仿真数据（后果）作为输入 integrate into LGBM model，形成PC-LGBM模型，这个模型既包含了地表能耗平衡（SEB）的下面数据，又在CLM-LST模型中体现出了物理的含义。与纯物理方法和纯ML方法相比，PC-LGBM模型提高了LST预测精度和物理解释性。它还表明了对EXTREME WEATHER CASES的应答能力，表明PC-LGBM模型不仅可以从数据学习，也可以从理论 derivation。提出的方法可以创新精度和物理解释能力的无缝LST地图，并提供了对土地表面过程的加速知识发现和数据挖掘的新思路。
</details></li>
</ul>
<hr>
<h2 id="Classification-of-sleep-stages-from-EEG-EOG-and-EMG-signals-by-SSNet"><a href="#Classification-of-sleep-stages-from-EEG-EOG-and-EMG-signals-by-SSNet" class="headerlink" title="Classification of sleep stages from EEG, EOG and EMG signals by SSNet"></a>Classification of sleep stages from EEG, EOG and EMG signals by SSNet</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05373">http://arxiv.org/abs/2307.05373</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haifa Almutairi, Ghulam Mubashar Hassan, Amitava Datta</li>
<li>for: 这个研究旨在开发一个基于深度学习的睡眠阶段分类模型，以便诊断睡眠相关疾病，如呼吸暴露睡眠疾病（SDB）病。</li>
<li>methods: 本研究使用了一个终端到终点的深度学习架构，名为SSNet，其包括两个基于卷积神经网络（CNN）和长短期记忆运算（LSTM）的深度学习网络。这两个深度学习网络从联合的电普热学参数（EOG）、电脑电参数（EEG）和电omyogram（EMG）信号中提取特征，每个信号都有独特的特征，可以帮助分类睡眠阶段。两个深度学习网络生成的特征被 concatenated 传递到完全连接层进行分类。</li>
<li>results: 本研究使用了两个公共的数据集，包括Sleep-EDF扩展数据集和ISRUC-Sleep数据集，评估了我们提出的模型的性能。结果显示，我们的模型在三种睡眠阶段的分类中取得了96.36%的准确率和93.40%的协变系数，在五种睡眠阶段的分类中取得了96.57%的准确率和83.05%的协变系数。相比之下，我们的模型在睡眠阶段分类中表现较好，并且超过了现有的技术。<details>
<summary>Abstract</summary>
Classification of sleep stages plays an essential role in diagnosing sleep-related diseases including Sleep Disorder Breathing (SDB) disease. In this study, we propose an end-to-end deep learning architecture, named SSNet, which comprises of two deep learning networks based on Convolutional Neuron Networks (CNN) and Long Short Term Memory (LSTM). Both deep learning networks extract features from the combination of Electrooculogram (EOG), Electroencephalogram (EEG), and Electromyogram (EMG) signals, as each signal has distinct features that help in the classification of sleep stages. The features produced by the two-deep learning networks are concatenated to pass to the fully connected layer for the classification. The performance of our proposed model is evaluated by using two public datasets Sleep-EDF Expanded dataset and ISRUC-Sleep dataset. The accuracy and Kappa coefficient are 96.36% and 93.40% respectively, for classifying three classes of sleep stages using Sleep-EDF Expanded dataset. Whereas, the accuracy and Kappa coefficient are 96.57% and 83.05% respectively for five classes of sleep stages using Sleep-EDF Expanded dataset. Our model achieves the best performance in classifying sleep stages when compared with the state-of-the-art techniques.
</details>
<details>
<summary>摘要</summary>
классификация сновидений играет ключевую роль в диагностике заболеваний, связанных с сном, включая заболевание дыхательными путями во сне (SDB). В этом исследовании мы предлагаем энд-то-энд architecture, называемую SSNet, которая включает в себя два глубоких обучающихся сетей на основе сетей convolutional neurons (CNN) и Long Short Term Memory (LSTM). Обе сети глубокого обучения извлекают признаки из комбинации сигналов Electrooculogram (EOG), Electroencephalogram (EEG) и Electromyogram (EMG), поскольку каждый сигнал имеет отличительные признаки, которые помогают в классификации сновидений. Признаки, выделенные двумя сетями глубокого обучения, сходят в полностью связанный слой для классификации. Оценка нашей предложенной модели была выполнена с помощью двух общедоступных данныхсетов Sleep-EDF Expanded dataset и ISRUC-Sleep dataset. Аккуратность и коэффициент Кэппеля были равны 96,36% и 93,40% соответственно для классификации трех классов сновидений с помощью Sleep-EDF Expanded dataset. При этом, аккуратность и коэффициент Кэппеля были равны 96,57% и 83,05% соответственно для пяти классов сновидений с помощью Sleep-EDF Expanded dataset. Наша модель достигла лучшего результата в классификации сновидений по сравнению с существующими техниками.
</details></li>
</ul>
<hr>
<h2 id="Tools-for-Verifying-Neural-Models’-Training-Data"><a href="#Tools-for-Verifying-Neural-Models’-Training-Data" class="headerlink" title="Tools for Verifying Neural Models’ Training Data"></a>Tools for Verifying Neural Models’ Training Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00682">http://arxiv.org/abs/2307.00682</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dami Choi, Yonadav Shavit, David Duvenaud</li>
<li>for: 该论文旨在提供一种Proof-of-Training-Data协议，以便用户可以验证模型训练数据的来源和质量。</li>
<li>methods: 论文提出了一种基于随机种子的预commit机制和模型暂时过拟合特性的验证方法，以验证模型训练数据的可靠性。</li>
<li>results: 实验表明，该验证方法可以捕捉广泛的攻击，包括所有已知的Proof-of-Learning文献中的攻击。<details>
<summary>Abstract</summary>
It is important that consumers and regulators can verify the provenance of large neural models to evaluate their capabilities and risks. We introduce the concept of a "Proof-of-Training-Data": any protocol that allows a model trainer to convince a Verifier of the training data that produced a set of model weights. Such protocols could verify the amount and kind of data and compute used to train the model, including whether it was trained on specific harmful or beneficial data sources. We explore efficient verification strategies for Proof-of-Training-Data that are compatible with most current large-model training procedures. These include a method for the model-trainer to verifiably pre-commit to a random seed used in training, and a method that exploits models' tendency to temporarily overfit to training data in order to detect whether a given data-point was included in training. We show experimentally that our verification procedures can catch a wide variety of attacks, including all known attacks from the Proof-of-Learning literature.
</details>
<details>
<summary>摘要</summary>
Note: Simplified Chinese is also known as "简化字母" or "简化字母".Translation notes:* "consumers" is translated as "消费者" (shāngchǎng zhě)* "regulators" is translated as "管制机构" (guǎnzhì jīgòu)* "Proof-of-Training-Data" is translated as "训练数据证明" (xùxíng xùzhì)* "model trainer" is translated as "模型训练者" (móxìng xùxíng zhe)* "Verifier" is translated as "验证人" (yànzhèng rén)* "large-model training procedures" is translated as "大型模型训练程序" (dàxíng móxìng xùxíng)* "random seed" is translated as "随机种子" (suījī zhòngzi)* "training data" is translated as "训练数据" (xùxíng xùzhì)* "harmful or beneficial data sources" is translated as "有害或有益的数据来源" (yǒu hài yòu yì de xùnxīn lái yuán)Please note that the translation is done by a machine and may not be perfect, and it's always a good idea to have a human reviewer to check the translation before using it in any official context.
</details></li>
</ul>
<hr>
<h2 id="CLIMAX-An-exploration-of-Classifier-Based-Contrastive-Explanations"><a href="#CLIMAX-An-exploration-of-Classifier-Based-Contrastive-Explanations" class="headerlink" title="CLIMAX: An exploration of Classifier-Based Contrastive Explanations"></a>CLIMAX: An exploration of Classifier-Based Contrastive Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00680">http://arxiv.org/abs/2307.00680</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/niftynans/climax">https://github.com/niftynans/climax</a></li>
<li>paper_authors: Praharsh Nanavati, Ranjitha Prasad</li>
<li>For: 这种论文旨在解释黑盒机器学习模型的决策过程，以便使这些模型更加透明、负责任、可理解。* Methods: 这种方法基于本地分类器，并使用了标签感知的副本数据生成方法和影响子抽样来保证模型准确性。* Results: 作者比较了这种方法与其他基于LIME的方法，并发现它在一些预测任务上具有更高的一致性。此外，这种方法还可以在文本和图像类 datasets 上生成对比性的解释。<details>
<summary>Abstract</summary>
Explainable AI is an evolving area that deals with understanding the decision making of machine learning models so that these models are more transparent, accountable, and understandable for humans. In particular, post-hoc model-agnostic interpretable AI techniques explain the decisions of a black-box ML model for a single instance locally, without the knowledge of the intrinsic nature of the ML model. Despite their simplicity and capability in providing valuable insights, existing approaches fail to deliver consistent and reliable explanations. Moreover, in the context of black-box classifiers, existing approaches justify the predicted class, but these methods do not ensure that the explanation scores strongly differ as compared to those of another class. In this work we propose a novel post-hoc model agnostic XAI technique that provides contrastive explanations justifying the classification of a black box classifier along with a reasoning as to why another class was not predicted. Our method, which we refer to as CLIMAX which is short for Contrastive Label-aware Influence-based Model Agnostic XAI, is based on local classifiers . In order to ensure model fidelity of the explainer, we require the perturbations to be such that it leads to a class-balanced surrogate dataset. Towards this, we employ a label-aware surrogate data generation method based on random oversampling and Gaussian Mixture Model sampling. Further, we propose influence subsampling in order to retaining effective samples and hence ensure sample complexity. We show that we achieve better consistency as compared to baselines such as LIME, BayLIME, and SLIME. We also depict results on textual and image based datasets, where we generate contrastive explanations for any black-box classification model where one is able to only query the class probabilities for an instance of interest.
</details>
<details>
<summary>摘要</summary>
Explainable AI是一个在发展中的领域，旨在理解机器学习模型的决策过程，以便这些模型更加透明、责任、可理解。特别是，我们专注于在黑盒机器学习模型上的后期、模型无关的解释技术，可以在单一实例上，地方解释模型的决策。虽然现有的方法具有简单性和可提供有价值的洞见，但是它们无法提供一致和可靠的解释。此外，在黑盒分类器的情况下，现有的方法只会说明预测的类别，但不能保证解释 scores 强烈不同于另一个类别。在这个工作中，我们提出了一种新的后期、模型无关的解释技术，可以为黑盒分类器提供相对的解释，同时也能够解释为何选择另一个类别。我们称这种技术为 CLIMAX，即 Contrastive Label-aware Influence-based Model Agnostic XAI。CLIMAX 基于地方分类器，以 Ensure  explainer 的模型实践性，我们需要进行类别数balanced的调整数据。为了实现这一目标，我们使用了随机批量扩展和泊松分布的采样方法。此外，我们也提出了影响抽样，以保留有效的抽样和确保样本复杂性。我们发现我们的方法可以与基于 LIME、BayLIME 和 SLIME 的基eline 相比，具有更高的一致性。我们还展示了在文本和图像基于的数据集上的结果，可以为任何黑盒分类器提供相对的解释，只需要对兴趣的实例进行查询。
</details></li>
</ul>
<hr>
<h2 id="SDC-HSDD-NDSA-Structure-Detecting-Cluster-by-Hierarchical-Secondary-Directed-Differential-with-Normalized-Density-and-Self-Adaption"><a href="#SDC-HSDD-NDSA-Structure-Detecting-Cluster-by-Hierarchical-Secondary-Directed-Differential-with-Normalized-Density-and-Self-Adaption" class="headerlink" title="SDC-HSDD-NDSA: Structure Detecting Cluster by Hierarchical Secondary Directed Differential with Normalized Density and Self-Adaption"></a>SDC-HSDD-NDSA: Structure Detecting Cluster by Hierarchical Secondary Directed Differential with Normalized Density and Self-Adaption</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00677">http://arxiv.org/abs/2307.00677</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hao-b-shu/sdc-hsdd-ndsa">https://github.com/hao-b-shu/sdc-hsdd-ndsa</a></li>
<li>paper_authors: Hao Shu</li>
<li>for: 提供一种能够检测高密度区域中结构的分 clustering方法，以解决传统密度基本划分方法中结构不能够被检测的问题。</li>
<li>methods: 使用次要导向差异、层次结构、 нор化密度以及自适应系数来实现结构检测，并被称为SDC-HSDD-NDSA。</li>
<li>results: 在多个数据集中运行算法，结果验证了该方法的结构检测、鲁棒性和粒度独立性，并且表明其能够超越前一代方法。<details>
<summary>Abstract</summary>
Density-based clustering could be the most popular clustering algorithm since it can identify clusters of arbitrary shape as long as different (high-density) clusters are separated by low-density regions. However, the requirement of the separateness of clusters by low-density regions is not trivial since a high-density region might have different structures which should be clustered into different groups. Such a situation demonstrates the main flaw of all previous density-based clustering algorithms we have known--structures in a high-density cluster could not be detected. Therefore, this paper aims to provide a density-based clustering scheme that not only has the ability previous ones have but could also detect structures in a high-density region not separated by low-density ones. The algorithm employs secondary directed differential, hierarchy, normalized density, as well as the self-adaption coefficient, and thus is called Structure Detecting Cluster by Hierarchical Secondary Directed Differential with Normalized Density and Self-Adaption, dubbed by SDC-HSDD-NDSA for short. To illustrate its effectiveness, we run the algorithm in several data sets. The results verify its validity in structure detection, robustness over noises, as well as independence of granularities, and demonstrate that it could outperform previous ones. The Python code of the paper could be found on https://github.com/Hao-B-Shu/SDC-HSDD-NDSA.
</details>
<details>
<summary>摘要</summary>
density-based clustering可能是最受欢迎的聚类算法，因为它可以找到任意形状的聚集，只要不同的高密度区域被低密度区域隔离开来。然而，需要聚集区域由低密度区域隔离开来的要求并不是干扰的，因为高密度区域可能有不同的结构，这些结构应该被分配到不同的组。这种情况表明了所有之前的密度基于的聚类算法的主要缺陷——聚集区域中的结构不能被探测。因此，本文提出了一种密度基于的聚类方案，不仅具有之前的能力，而且可以在高密度区域中探测结构。该算法使用次级导向差、层次结构、 норциали化密度以及自适应系数，因此被称为结构探测聚类方法，简称SDC-HSDD-NDSA。为证明其效果，我们在多个数据集上运行了该算法。结果表明其在结构探测、鲁棒性和自适应性方面具有优势，并且可以超越之前的算法。Python代码可以在https://github.com/Hao-B-Shu/SDC-HSDD-NDSA中找到。
</details></li>
</ul>
<hr>
<h2 id="Pay-Attention-to-the-Atlas-Atlas-Guided-Test-Time-Adaptation-Method-for-Robust-3D-Medical-Image-Segmentation"><a href="#Pay-Attention-to-the-Atlas-Atlas-Guided-Test-Time-Adaptation-Method-for-Robust-3D-Medical-Image-Segmentation" class="headerlink" title="Pay Attention to the Atlas: Atlas-Guided Test-Time Adaptation Method for Robust 3D Medical Image Segmentation"></a>Pay Attention to the Atlas: Atlas-Guided Test-Time Adaptation Method for Robust 3D Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00676">http://arxiv.org/abs/2307.00676</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingjie Guo, Weitong Zhang, Matthew Sinclair, Daniel Rueckert, Chen Chen</li>
<li>for: 提高3D医学图像分割的稳定性和准确性，特别是在医疗影像应用中，遇到不同临床站点和扫描仪器的医学图像变换问题。</li>
<li>methods: 提出了一种名为AdaAtlas的新的Atlas导航测试时适应方法，只需要一个无标签的测试样本作为输入，并通过最小化atlas空间中学习的atlas-based损失来适应分割网络。此外，我们还可以在测试时使用通道和空间注意力块来提高适应性。</li>
<li>results: 对多个来自不同站点的数据集进行了广泛的实验，结果显示，AdaAtlas-Attention在比较其他竞争方法时具有显著的性能改善，特别是在3D医学图像分割任务中。<details>
<summary>Abstract</summary>
Convolutional neural networks (CNNs) often suffer from poor performance when tested on target data that differs from the training (source) data distribution, particularly in medical imaging applications where variations in imaging protocols across different clinical sites and scanners lead to different imaging appearances. However, re-accessing source training data for unsupervised domain adaptation or labeling additional test data for model fine-tuning can be difficult due to privacy issues and high labeling costs, respectively. To solve this problem, we propose a novel atlas-guided test-time adaptation (TTA) method for robust 3D medical image segmentation, called AdaAtlas. AdaAtlas only takes one single unlabeled test sample as input and adapts the segmentation network by minimizing an atlas-based loss. Specifically, the network is adapted so that its prediction after registration is aligned with the learned atlas in the atlas space, which helps to reduce anatomical segmentation errors at test time. In addition, different from most existing TTA methods which restrict the adaptation to batch normalization blocks in the segmentation network only, we further exploit the use of channel and spatial attention blocks for improved adaptability at test time. Extensive experiments on multiple datasets from different sites show that AdaAtlas with attention blocks adapted (AdaAtlas-Attention) achieves superior performance improvements, greatly outperforming other competitive TTA methods.
</details>
<details>
<summary>摘要</summary>
循环 нейрон网络（CNN）在面向目标数据进行测试时，经常会表现出不佳的性能，特别是在医学影像应用中，因为不同的临床站点和扫描仪器使用不同的扫描技术，导致影像的显示方式不同。然而，重新访问源训练数据进行隐私的预处理或者为模型细化进行标注是困难的，因为隐私和高标注成本。为解决这个问题，我们提出了一种基于图集的测试时适应（TTA）方法，called AdaAtlas。AdaAtlas只需要一个单独的无标签测试样本，并使用图集来适应分割网络，以适应测试数据的不同。具体来说，网络会在注册后与学习的图集进行对比，以减少分割错误。此外，我们还在TTA方法中使用通道和空间注意力块，以提高测试时的适应性。经过多个数据集的测试，我们发现AdaAtlas-Attention方法可以 achieve superior performance improvement，大大超过其他竞争性TTA方法。
</details></li>
</ul>
<hr>
<h2 id="ENN-A-Neural-Network-with-DCT-Adaptive-Activation-Functions"><a href="#ENN-A-Neural-Network-with-DCT-Adaptive-Activation-Functions" class="headerlink" title="ENN: A Neural Network with DCT-Adaptive Activation Functions"></a>ENN: A Neural Network with DCT-Adaptive Activation Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00673">http://arxiv.org/abs/2307.00673</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marc Martinez-Gost, Ana Pérez-Neira, Miguel Ángel Lagunas</li>
<li>for: 这篇论文是用于探讨神经网络 Activation Function 的可表示性和可调整性的。</li>
<li>methods: 这篇论文提出了一种基于 Discrete Cosine Transform (DCT) 的非线性活化函数模型，并通过反射学习在训练阶段进行调整。这种 parametrization 可以保持训练参数的数量低、适合梯度下降法，并适应不同的学习任务。</li>
<li>results: 经过具体实验，这种模型可以在分类和回归任务上适应和具有高表达能力，并且在一些情况下可以提高 state of the art 的准确率，达到40%之间。<details>
<summary>Abstract</summary>
The expressiveness of neural networks highly depends on the nature of the activation function, although these are usually assumed predefined and fixed during the training stage. In this paper we present Expressive Neural Network (ENN), a novel architecture in which the non-linear activation functions are modeled using the Discrete Cosine Transform (DCT) and adapted using backpropagation during training. This parametrization keeps the number of trainable parameters low, is appropriate for gradient-based schemes, and adapts to different learning tasks. This is the first non-linear model for activation functions that relies on a signal processing perspective, providing high flexibility and expressiveness to the network. We contribute with insights in the explainability of the network at convergence by recovering the concept of bump, this is, the response of each activation function in the output space to provide insights. Finally, through exhaustive experiments we show that the model can adapt to classification and regression tasks. The performance of ENN outperforms state of the art benchmarks, providing up to a 40\% gap in accuracy in some scenarios.
</details>
<details>
<summary>摘要</summary>
Expressive Neural Network (ENN)是一种新型神经网络 Architecture，其中非线性活化函数被模型为Discrete Cosine Transform (DCT)，并在训练过程中通过反传播进行适应。这种参数化方式保持训练参数的数量低，适合梯度下降方法，并适应不同的学习任务。这是首次基于信号处理角度的非线性活化函数模型，具有高度的灵活性和表达力。我们提供了解释网络各激活函数在输出空间的响应，即“bump”的概念，从而提供了网络的解释。最后，我们通过广泛的实验证明了ENN模型可以适应分类和回归任务，并在一些情况下超越了状态的权威benchmark。ENN模型的性能与状态的权威benchmark之间的差距可达40%。
</details></li>
</ul>
<hr>
<h2 id="Automatic-MILP-Solver-Configuration-By-Learning-Problem-Similarities"><a href="#Automatic-MILP-Solver-Configuration-By-Learning-Problem-Similarities" class="headerlink" title="Automatic MILP Solver Configuration By Learning Problem Similarities"></a>Automatic MILP Solver Configuration By Learning Problem Similarities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00670">http://arxiv.org/abs/2307.00670</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/scale-lab/MILPTune">https://github.com/scale-lab/MILPTune</a></li>
<li>paper_authors: Abdelrahman Hosny, Sherief Reda</li>
<li>for: 这个研究旨在预测Mixed Integer Linear Programs（MILP）中遗传数据的优化组件，以提高解的成本而不是在解时间上投入大量时间搜寻和评估组件。</li>
<li>methods: 本研究使用深度度量学来学习MILP之间的相似性，并将其转换为解决方案的成本相似性。在推断时，给出一个新的问题时，将其转换为已学习的度量空间中的 nearest neighbor 问题，并将组件设置预测为最近的问题中的组件设置。</li>
<li>results: 实验结果显示，我们的方法可以预测组件设置，从而提高解的成本，最高可以观察到38%的改善。<details>
<summary>Abstract</summary>
A large number of real-world optimization problems can be formulated as Mixed Integer Linear Programs (MILP). MILP solvers expose numerous configuration parameters to control their internal algorithms. Solutions, and their associated costs or runtimes, are significantly affected by the choice of the configuration parameters, even when problem instances have the same number of decision variables and constraints. On one hand, using the default solver configuration leads to suboptimal solutions. On the other hand, searching and evaluating a large number of configurations for every problem instance is time-consuming and, in some cases, infeasible. In this study, we aim to predict configuration parameters for unseen problem instances that yield lower-cost solutions without the time overhead of searching-and-evaluating configurations at the solving time. Toward that goal, we first investigate the cost correlation of MILP problem instances that come from the same distribution when solved using different configurations. We show that instances that have similar costs using one solver configuration also have similar costs using another solver configuration in the same runtime environment. After that, we present a methodology based on Deep Metric Learning to learn MILP similarities that correlate with their final solutions' costs. At inference time, given a new problem instance, it is first projected into the learned metric space using the trained model, and configuration parameters are instantly predicted using previously-explored configurations from the nearest neighbor instance in the learned embedding space. Empirical results on real-world problem benchmarks show that our method predicts configuration parameters that improve solutions' costs by up to 38% compared to existing approaches.
</details>
<details>
<summary>摘要</summary>
许多现实世界优化问题可以表示为杂合整数线性程序（MILP）。MILP解决器公布了许多配置参数来控制其内部算法。解决方案和其关联的成本或运行时间受到配置参数的选择的影响，即使问题实例具有相同的决策变量和约束。一方面，使用默认解决器配置会导致优化解决方案。另一方面，在每个问题实例上搜索和评估大量配置参数是时间消耗性很高，甚至不可行。在这种情况下，我们的目标是预测未看到的问题实例的配置参数，以便在解决时获得更低成本的解决方案，而无需在解决时进行搜索和评估配置参数。我们首先研究了使用不同配置参数解决MILP问题实例的成本相关性。我们发现，使用不同配置参数解决的MILP问题实例的成本相似。然后，我们提出了基于深度度量学习的方法，用于学习MILP问题之间的相似性。在推理时，给定一个新的问题实例，将其投影到已经学习的度量空间中，并使用已经探索的配置参数来预测解决方案的成本。实验结果表明，我们的方法可以预测解决方案的成本下降至38%，相比于现有的方法。
</details></li>
</ul>
<hr>
<h2 id="Active-Sensing-with-Predictive-Coding-and-Uncertainty-Minimization"><a href="#Active-Sensing-with-Predictive-Coding-and-Uncertainty-Minimization" class="headerlink" title="Active Sensing with Predictive Coding and Uncertainty Minimization"></a>Active Sensing with Predictive Coding and Uncertainty Minimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00668">http://arxiv.org/abs/2307.00668</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdelrahman Sharafeldin, Nabil Imam, Hannah Choi</li>
<li>for: 本研究旨在提出一种基于生物学计算的探索方法，可应用于任何探索任务中，无需任务具体的准备和指导。</li>
<li>methods: 该方法基于两种生物计算：预测编码和不确定度最小化。它可以在任务独立和内在驱动的情况下应用于任何探索任务。</li>
<li>results: 研究人员通过在迷宫探索任务和活观视觉任务中应用该方法，并证明了其能够找到环境的转移分布和重建空间特征。此外，该方法还能够建立不监督的表示，使代理人能够高效地样本和分类感知场景。<details>
<summary>Abstract</summary>
We present an end-to-end procedure for embodied exploration based on two biologically inspired computations: predictive coding and uncertainty minimization. The procedure can be applied to any exploration setting in a task-independent and intrinsically driven manner. We first demonstrate our approach in a maze navigation task and show that our model is capable of discovering the underlying transition distribution and reconstructing the spatial features of the environment. Second, we apply our model to the more complex task of active vision, where an agent must actively sample its visual environment to gather information. We show that our model is able to build unsupervised representations that allow it to actively sample and efficiently categorize sensory scenes. We further show that using these representations as input for downstream classification leads to superior data efficiency and learning speed compared to other baselines, while also maintaining lower parameter complexity. Finally, the modularity of our model allows us to analyze its internal mechanisms and to draw insight into the interactions between perception and action during exploratory behavior.
</details>
<details>
<summary>摘要</summary>
我们提出了一种从头到尾的方法，基于生物学上的两种计算：预测编码和不确定度最小化。这种方法可以在任何探索任务中应用，具有任务独立和自适应的特点。我们首先在一个迷宫探索任务中应用了我们的方法，并证明我们的模型可以找到环境的下行传递分布和重建空间特征。然后，我们将我们的模型应用到更复杂的激活视觉任务中，agent需要活动地抽取视觉环境中的信息。我们证明了我们的模型可以建立无监督的表示，使得它可以活动地抽取和有效地分类感知场景。此外，我们还证明了使用这些表示作为下游分类器的输入，可以提高数据效率和学习速度，同时也可以降低参数复杂性。最后，我们的模型的卷积结构允许我们分析它的内部机制，并从探索行为中拓展出对感知和行为之间的交互的理解。
</details></li>
</ul>
<hr>
<h2 id="Morse-Neural-Networks-for-Uncertainty-Quantification"><a href="#Morse-Neural-Networks-for-Uncertainty-Quantification" class="headerlink" title="Morse Neural Networks for Uncertainty Quantification"></a>Morse Neural Networks for Uncertainty Quantification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00667">http://arxiv.org/abs/2307.00667</a></li>
<li>repo_url: None</li>
<li>paper_authors: Benoit Dherin, Huiyi Hu, Jie Ren, Michael W. Dusenberry, Balaji Lakshminarayanan<br>for:* The paper presents a new deep generative model called the Morse neural network, which is useful for uncertainty quantification and can be used for various tasks such as OOD detection, anomaly detection, and continuous learning.methods:* The Morse neural network uses a KL-divergence loss to fit the model, which yields five components: a generative density, an OOD detector, a calibration temperature, a generative sampler, and a distance-aware classifier (in the supervised case).results:* The Morse neural network unifies many techniques in uncertainty quantification and has connections to support vector machines, kernel methods, and Morse theory in topology. It can be used on top of a pre-trained network to bring distance-aware calibration w.r.t the training data.<details>
<summary>Abstract</summary>
We introduce a new deep generative model useful for uncertainty quantification: the Morse neural network, which generalizes the unnormalized Gaussian densities to have modes of high-dimensional submanifolds instead of just discrete points. Fitting the Morse neural network via a KL-divergence loss yields 1) a (unnormalized) generative density, 2) an OOD detector, 3) a calibration temperature, 4) a generative sampler, along with in the supervised case 5) a distance aware-classifier. The Morse network can be used on top of a pre-trained network to bring distance-aware calibration w.r.t the training data. Because of its versatility, the Morse neural networks unifies many techniques: e.g., the Entropic Out-of-Distribution Detector of (Mac\^edo et al., 2021) in OOD detection, the one class Deep Support Vector Description method of (Ruff et al., 2018) in anomaly detection, or the Contrastive One Class classifier in continuous learning (Sun et al., 2021). The Morse neural network has connections to support vector machines, kernel methods, and Morse theory in topology.
</details>
<details>
<summary>摘要</summary>
我们介绍一个新的深度生成模型，用于不确定量化：Morse神经网络，它将高维子集模式扩展到非数字点的 Gaussian 分布中。通过 Morse 神经网络的适应损失函数，可以获得1）生成密度（尚未 норма化），2）外部数据检测器，3）整合温度，4）生成抽样器，并在监督学习情况下5）距离意识类别器。Morse 神经网络可以在对照数据进行训练后，将距离意识于训练数据。由于其多方面性，Morse 神经网络可以统一许多技术：例如 Entropic Out-of-Distribution Detector（Mac\^edo et al., 2021）在类别外数据检测中，One Class Deep Support Vector Description method（Ruff et al., 2018）在异常检测中，以及 Contrastive One Class 类别器在连续学习中（Sun et al., 2021）。Morse 神经网络与支持向量机制、核方法和 Morse 理论在数学上有联系。
</details></li>
</ul>
<hr>
<h2 id="Numerical-Association-Rule-Mining-A-Systematic-Literature-Review"><a href="#Numerical-Association-Rule-Mining-A-Systematic-Literature-Review" class="headerlink" title="Numerical Association Rule Mining: A Systematic Literature Review"></a>Numerical Association Rule Mining: A Systematic Literature Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00662">http://arxiv.org/abs/2307.00662</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minakshi Kaushik, Rahul Sharma, Iztok Fister Jr., Dirk Draheim</li>
<li>for: 本研究旨在bridging numerical association rule mining领域中的知识差距，通过对1996年至2022年发表的1,140篇学术论文进行系统性的文献综述。</li>
<li>methods: 本研究使用了多种方法、算法、 метрик和数据集，包括不同的某些精度抽象方法、精度评估方法、精度权重方法、精度匹配方法等。</li>
<li>results: 本研究通过对68篇论文进行深入的分析，提供了 numerical association rule mining领域中现有的多种方法、算法、 метрик和数据集，并发现了一些研究问题和未来可能性。此外，本研究还提出了一种新的某些精度抽象方法，可以帮助提高人类对数据的认知。<details>
<summary>Abstract</summary>
Numerical association rule mining is a widely used variant of the association rule mining technique, and it has been extensively used in discovering patterns and relationships in numerical data. Initially, researchers and scientists integrated numerical attributes in association rule mining using various discretization approaches; however, over time, a plethora of alternative methods have emerged in this field. Unfortunately, the increase of alternative methods has resulted into a significant knowledge gap in understanding diverse techniques employed in numerical association rule mining -- this paper attempts to bridge this knowledge gap by conducting a comprehensive systematic literature review. We provide an in-depth study of diverse methods, algorithms, metrics, and datasets derived from 1,140 scholarly articles published from the inception of numerical association rule mining in the year 1996 to 2022. In compliance with the inclusion, exclusion, and quality evaluation criteria, 68 papers were chosen to be extensively evaluated. To the best of our knowledge, this systematic literature review is the first of its kind to provide an exhaustive analysis of the current literature and previous surveys on numerical association rule mining. The paper discusses important research issues, the current status, and future possibilities of numerical association rule mining. On the basis of this systematic review, the article also presents a novel discretization measure that contributes by providing a partitioning of numerical data that meets well human perception of partitions.
</details>
<details>
<summary>摘要</summary>
numerically association rule mining 是一种广泛使用的 association rule mining 技术的变种，并在发现数据中的模式和关系方面得到了广泛应用。初始时，研究人员和科学家将数值属性 integrate 到 association rule mining 中使用了多种精炼方法；然而，随着时间的推移，这个领域中出现了一系列的替代方法。 unfortunately, 这些替代方法的出现导致了关于不同技术在数值 association rule mining 中的知识差距增加，这篇论文试图通过进行系统性的文献综述来填补这个知识差距。我们从1996年 numerical association rule mining 的开始时间到2022年进行了1,140篇学术论文的系统性综述。在符合包括、排除和质量评估标准的基础下，我们选择了68篇文献进行了深入的评估。据我们所知，这是首次对 numerical association rule mining 的现有文献和前期调查进行了系统性的综述。本文提出了一些重要的研究问题，现状和未来可能性，以及一种新的精炼度量，它可以为数值数据提供一种符合人类认知的分割。
</details></li>
</ul>
<hr>
<h2 id="Intra-Extra-Source-Exemplar-Based-Style-Synthesis-for-Improved-Domain-Generalization"><a href="#Intra-Extra-Source-Exemplar-Based-Style-Synthesis-for-Improved-Domain-Generalization" class="headerlink" title="Intra- &amp; Extra-Source Exemplar-Based Style Synthesis for Improved Domain Generalization"></a>Intra- &amp; Extra-Source Exemplar-Based Style Synthesis for Improved Domain Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00648">http://arxiv.org/abs/2307.00648</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/boschresearch/issa">https://github.com/boschresearch/issa</a></li>
<li>paper_authors: Yumeng Li, Dan Zhang, Margret Keuper, Anna Khoreva</li>
<li>for: 提高域外适应性（Domain Generalization）的 semantic segmentation 模型，尤其是在自动驾驶等应用场景中，它们经常遇到域 shift 问题。</li>
<li>methods: 提出了一种 exemplar-based 风格合成管道，通过 StyleGAN2 倒数采样和掩蔽噪声预测来提高域外适应性。</li>
<li>results: 在不同类型的数据偏移（地理位置、天气、日夜等）下，实现了最多 12.4% 的 mIoU 提升，并且可以与 CNN 和 Transformer 模型相容，并且可以与其他域外适应性技术相结合使用。<details>
<summary>Abstract</summary>
The generalization with respect to domain shifts, as they frequently appear in applications such as autonomous driving, is one of the remaining big challenges for deep learning models. Therefore, we propose an exemplar-based style synthesis pipeline to improve domain generalization in semantic segmentation. Our method is based on a novel masked noise encoder for StyleGAN2 inversion. The model learns to faithfully reconstruct the image, preserving its semantic layout through noise prediction. Using the proposed masked noise encoder to randomize style and content combinations in the training set, i.e., intra-source style augmentation (ISSA) effectively increases the diversity of training data and reduces spurious correlation. As a result, we achieve up to $12.4\%$ mIoU improvements on driving-scene semantic segmentation under different types of data shifts, i.e., changing geographic locations, adverse weather conditions, and day to night. ISSA is model-agnostic and straightforwardly applicable with CNNs and Transformers. It is also complementary to other domain generalization techniques, e.g., it improves the recent state-of-the-art solution RobustNet by $3\%$ mIoU in Cityscapes to Dark Z\"urich. In addition, we demonstrate the strong plug-n-play ability of the proposed style synthesis pipeline, which is readily usable for extra-source exemplars e.g., web-crawled images, without any retraining or fine-tuning. Moreover, we study a new use case to indicate neural network's generalization capability by building a stylized proxy validation set. This application has significant practical sense for selecting models to be deployed in the open-world environment. Our code is available at \url{https://github.com/boschresearch/ISSA}.
</details>
<details>
<summary>摘要</summary>
“域域迁移总是深度学习模型的一大挑战，尤其在自动驾驶应用中出现的场景下。因此，我们提出了一种基于例子的风格合成管道，以提高域迁移的深度学习模型。我们的方法基于 StyleGAN2 的掩码噪音编码器，该模型能够准确地重建图像，保留其 semantic 布局。通过在训练集中随机采样样式和内容的组合，我们称之为内源样式增强（ISSA），可以提高训练数据的多样性，并降低偶极性。这使得我们在不同类型的数据迁移下 achieve 12.4% 的 mIoU 提升。ISSA 是模型无关的和简单应用于 CNN 和 Transformer 上。它还是其他域迁移技术的补充，例如 RobustNet 的最新状态态之一。此外，我们还证明了我们提posed的风格合成管道具有强大的插件与替换能力，可以在不需要重新训练或微调的情况下使用。此外，我们还研究了一种新的应用场景，即通过建立风格化代理验证集来评估神经网络的泛化能力。这种应用场景具有实际 significanc，可以帮助选择要部署在开放世界环境中的模型。我们的代码可以在 \url{https://github.com/boschresearch/ISSA} 中找到。”
</details></li>
</ul>
<hr>
<h2 id="Multiclass-Boosting-Simple-and-Intuitive-Weak-Learning-Criteria"><a href="#Multiclass-Boosting-Simple-and-Intuitive-Weak-Learning-Criteria" class="headerlink" title="Multiclass Boosting: Simple and Intuitive Weak Learning Criteria"></a>Multiclass Boosting: Simple and Intuitive Weak Learning Criteria</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00642">http://arxiv.org/abs/2307.00642</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nataly Brukhim, Amit Daniely, Yishay Mansour, Shay Moran</li>
<li>for: 这个论文是为了推广到多类Setting中的推广。</li>
<li>methods: 这个论文引入了一种弱学习条件，用于描述多类分类的原始概念，即“slightly better than random guessing”。提供了一种简单有效的推广算法，不需要 realizability assumption，其样本和oracle复杂度上下文独立于类数。</li>
<li>results: 这个论文在理论应用中使用了新的推广技术，包括列PAC学习中的等价性、推广 для列学习者和多类PAC学习和列PAC学习的特征化。其中，我们提供了一种简化的分析方法，并实现了对大型列size的改进的错误 bound。<details>
<summary>Abstract</summary>
We study a generalization of boosting to the multiclass setting. We introduce a weak learning condition for multiclass classification that captures the original notion of weak learnability as being "slightly better than random guessing". We give a simple and efficient boosting algorithm, that does not require realizability assumptions and its sample and oracle complexity bounds are independent of the number of classes.   In addition, we utilize our new boosting technique in several theoretical applications within the context of List PAC Learning. First, we establish an equivalence to weak PAC learning. Furthermore, we present a new result on boosting for list learners, as well as provide a novel proof for the characterization of multiclass PAC learning and List PAC learning. Notably, our technique gives rise to a simplified analysis, and also implies an improved error bound for large list sizes, compared to previous results.
</details>
<details>
<summary>摘要</summary>
我们研究了多类 boosting 的泛化。我们引入了一种多类分类中的弱学习条件， capture 了原始的弱学习概念，即“一些更好过 random 猜测”。我们给出了简单、高效的 boosting 算法，不需要 realizability 假设，其样本和oracle 复杂度上下文独立于类数。在此基础上，我们在list PAC 学习的上下文中应用了我们的新 boosting 技术。首先，我们证明了弱 PAC 学习的等价性。其次，我们提供了一个新的 boosting  для列学习者结果，以及一种新的证明方式 для多类 PAC 学习和list PAC 学习的特征化。值得注意的是，我们的技术使得分析简化，同时还提供了大量列表大小时的改进的错误 bound。
</details></li>
</ul>
<hr>
<h2 id="Effects-of-Explanation-Specificity-on-Passengers-in-Autonomous-Driving"><a href="#Effects-of-Explanation-Specificity-on-Passengers-in-Autonomous-Driving" class="headerlink" title="Effects of Explanation Specificity on Passengers in Autonomous Driving"></a>Effects of Explanation Specificity on Passengers in Autonomous Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00633">http://arxiv.org/abs/2307.00633</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Omeiza, Raunak Bhattacharyya, Nick Hawes, Marina Jirotka, Lars Kunze</li>
<li>for:  investigate the effects of natural language explanations’ specificity on passengers in autonomous driving</li>
<li>methods: extended an existing data-driven tree-based explainer algorithm by adding a rule-based option for explanation generation, and generated auditory natural language explanations with different levels of specificity (abstract and specific)</li>
<li>results: both abstract and specific explanations had similar positive effects on passengers’ perceived safety and the feeling of anxiety, but specific explanations influenced the desire of passengers to takeover driving control from the autonomous vehicle, while abstract explanations did not.<details>
<summary>Abstract</summary>
The nature of explanations provided by an explainable AI algorithm has been a topic of interest in the explainable AI and human-computer interaction community. In this paper, we investigate the effects of natural language explanations' specificity on passengers in autonomous driving. We extended an existing data-driven tree-based explainer algorithm by adding a rule-based option for explanation generation. We generated auditory natural language explanations with different levels of specificity (abstract and specific) and tested these explanations in a within-subject user study (N=39) using an immersive physical driving simulation setup. Our results showed that both abstract and specific explanations had similar positive effects on passengers' perceived safety and the feeling of anxiety. However, the specific explanations influenced the desire of passengers to takeover driving control from the autonomous vehicle (AV), while the abstract explanations did not. We conclude that natural language auditory explanations are useful for passengers in autonomous driving, and their specificity levels could influence how much in-vehicle participants would wish to be in control of the driving activity.
</details>
<details>
<summary>摘要</summary>
自然语言说明提供的AI算法的特点已经引起了解释AI和人机交互社区的关注。在这篇论文中，我们研究了自动驾驶中乘客受到自然语言说明的特定性的影响。我们将现有的数据驱动树型解释算法扩展为添加了规则型解释生成选项。我们生成了不同水平的具体性（抽象和具体）的听觉自然语言说明，并在N=39名参与者参与的内置式物理驾驶模拟设置下进行了一场人Subject用户研究。我们的结果显示，抽象和具体的说明都有类似的正面效果于乘客对自动驾驶车辆（AV）的感知安全和焦虑情况。然而，具体说明对乘客想要控制驾驶活动的愿望产生了影响，而抽象说明没有这种影响。我们结论认为，自然语言听觉说明对自动驾驶中的乘客非常有用，而其特定性水平可以影响乘客希望在驾驶活动中担当控制的程度。
</details></li>
</ul>
<hr>
<h2 id="Bidirectional-Looking-with-A-Novel-Double-Exponential-Moving-Average-to-Adaptive-and-Non-adaptive-Momentum-Optimizers"><a href="#Bidirectional-Looking-with-A-Novel-Double-Exponential-Moving-Average-to-Adaptive-and-Non-adaptive-Momentum-Optimizers" class="headerlink" title="Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non-adaptive Momentum Optimizers"></a>Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non-adaptive Momentum Optimizers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00631">http://arxiv.org/abs/2307.00631</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chernyn/admeta-optimizer">https://github.com/chernyn/admeta-optimizer</a></li>
<li>paper_authors: Yineng Chen, Zuchao Li, Lefei Zhang, Bo Du, Hai Zhao</li>
<li>for: 提高深度学习模型的训练效果和稳定性</li>
<li>methods: 提出了一种新的双向性优化器框架，包括DEMA变体和动态预测策略</li>
<li>results: 经过广泛的实验和理论证明，提出的\textsc{Admeta}优化器在多个任务上表现出色，比基础优化器和最新的竞争优化器更高效和稳定。<details>
<summary>Abstract</summary>
Optimizer is an essential component for the success of deep learning, which guides the neural network to update the parameters according to the loss on the training set. SGD and Adam are two classical and effective optimizers on which researchers have proposed many variants, such as SGDM and RAdam. In this paper, we innovatively combine the backward-looking and forward-looking aspects of the optimizer algorithm and propose a novel \textsc{Admeta} (\textbf{A} \textbf{D}ouble exponential \textbf{M}oving averag\textbf{E} \textbf{T}o \textbf{A}daptive and non-adaptive momentum) optimizer framework. For backward-looking part, we propose a DEMA variant scheme, which is motivated by a metric in the stock market, to replace the common exponential moving average scheme. While in the forward-looking part, we present a dynamic lookahead strategy which asymptotically approaches a set value, maintaining its speed at early stage and high convergence performance at final stage. Based on this idea, we provide two optimizer implementations, \textsc{AdmetaR} and \textsc{AdmetaS}, the former based on RAdam and the latter based on SGDM. Through extensive experiments on diverse tasks, we find that the proposed \textsc{Admeta} optimizer outperforms our base optimizers and shows advantages over recently proposed competitive optimizers. We also provide theoretical proof of these two algorithms, which verifies the convergence of our proposed \textsc{Admeta}.
</details>
<details>
<summary>摘要</summary>
优化器是深度学习成功的关键组件，它引导神经网络更新参数根据训练集的损失。SGD和Adam是经典的优化器，研究人员对其提出了许多变体，如SGDM和RAdam。在这篇论文中，我们创新地结合了优化器算法的反向和前向方面，并提出了一个新的\textsc{Admeta}优化器框架。在反向方面，我们提出了DEMA变体方案，它是由股票市场中的一个度量所 inspirited，用于取代常见的几何移动平均方案。而在前向方面，我们提出了一种动态预测策略，它在早期 stages maintains its speed and在 final stages reaches a set value，即使在极端情况下也能够保持高性能。基于这个想法，我们提供了两种优化器实现，\textsc{AdmetaR}和\textsc{AdmetaS}，前者基于RAdam，后者基于SGDM。通过对多种任务进行广泛的实验，我们发现了我们提posed的\textsc{Admeta}优化器在相比基础优化器和最新的竞争优化器之上具有优势。此外，我们还提供了这两个算法的理论证明，这证明了我们的提案的\textsc{Admeta}优化器的收敛性。
</details></li>
</ul>
<hr>
<h2 id="Variational-Autoencoding-Molecular-Graphs-with-Denoising-Diffusion-Probabilistic-Model"><a href="#Variational-Autoencoding-Molecular-Graphs-with-Denoising-Diffusion-Probabilistic-Model" class="headerlink" title="Variational Autoencoding Molecular Graphs with Denoising Diffusion Probabilistic Model"></a>Variational Autoencoding Molecular Graphs with Denoising Diffusion Probabilistic Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00623">http://arxiv.org/abs/2307.00623</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daiki Koge, Naoaki Ono, Shigehiko Kanaya</li>
<li>for: 这 paper 的目的是提出一种基于分布式生成模型的分子特征设计方法，以便在数据驱动药物发现中使用。</li>
<li>methods: 这 paper 使用了一种基于梯度滤波的 probabilistic 模型（DDPM），将分子结构转化为含层次结构的 probabilistic 特征。</li>
<li>results: 经过一些实验表明，这 paper 的方法可以在小样本大小下提供更好的分子性质预测性能和稳定性，比较现有方法更好。<details>
<summary>Abstract</summary>
In data-driven drug discovery, designing molecular descriptors is a very important task. Deep generative models such as variational autoencoders (VAEs) offer a potential solution by designing descriptors as probabilistic latent vectors derived from molecular structures. These models can be trained on large datasets, which have only molecular structures, and applied to transfer learning. Nevertheless, the approximate posterior distribution of the latent vectors of the usual VAE assumes a simple multivariate Gaussian distribution with zero covariance, which may limit the performance of representing the latent features. To overcome this limitation, we propose a novel molecular deep generative model that incorporates a hierarchical structure into the probabilistic latent vectors. We achieve this by a denoising diffusion probabilistic model (DDPM). We demonstrate that our model can design effective molecular latent vectors for molecular property prediction from some experiments by small datasets on physical properties and activity. The results highlight the superior prediction performance and robustness of our model compared to existing approaches.
</details>
<details>
<summary>摘要</summary>
在数据驱动的药物发现中，设计分子特征是非常重要的任务。深度生成模型如变换自动编码器（VAEs）提供了一种解决方案，通过将分子结构转换为抽象的 probabilistic 矩阵来设计特征。这些模型可以在大量数据上训练，并应用到传输学习。然而，通常的 VAE 模型假设 approximate  posterior distribution 的纬度分布是一个简单的多变量 Gaussian 分布，这可能会限制表示隐藏特征的性能。为了超越这些限制，我们提出了一种新的分子深度生成模型，通过将 hierarchical 结构 incorporated 到 probabilistic 矩阵中来实现。我们通过 denoising diffusion probabilistic model（DDPM）来实现这一点。我们的实验表明，我们的模型可以从一些小 datasets 上预测分子性质，并且比既有方法表现出更好的预测性和稳定性。
</details></li>
</ul>
<hr>
<h2 id="Solving-Linear-Inverse-Problems-Provably-via-Posterior-Sampling-with-Latent-Diffusion-Models"><a href="#Solving-Linear-Inverse-Problems-Provably-via-Posterior-Sampling-with-Latent-Diffusion-Models" class="headerlink" title="Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models"></a>Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00619">http://arxiv.org/abs/2307.00619</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liturout/psld">https://github.com/liturout/psld</a></li>
<li>paper_authors: Litu Rout, Negin Raoof, Giannis Daras, Constantine Caramanis, Alexandros G. Dimakis, Sanjay Shakkottai</li>
<li>for:  solves linear inverse problems using pre-trained latent diffusion models.</li>
<li>methods: leverages pre-trained latent diffusion models and provides provable sample recovery in a linear model setting.</li>
<li>results: outperforms previously proposed posterior sampling algorithms in various inpainting, denoising, deblurring, destriping, and super-resolution tasks.Here’s the full translation of the abstract in Simplified Chinese:</li>
<li>for:  Linear inverse problems 的解决方案，使用预训练的潜在扩散模型。</li>
<li>methods: 基于预训练的潜在扩散模型，提供可证明样本恢复的线性模型设置下的算法分析。</li>
<li>results: 在多种缺失、降噪、滤波、除条、高清化和超解像等问题中，超越了之前的 posterior sampling 算法。<details>
<summary>Abstract</summary>
We present the first framework to solve linear inverse problems leveraging pre-trained latent diffusion models. Previously proposed algorithms (such as DPS and DDRM) only apply to pixel-space diffusion models. We theoretically analyze our algorithm showing provable sample recovery in a linear model setting. The algorithmic insight obtained from our analysis extends to more general settings often considered in practice. Experimentally, we outperform previously proposed posterior sampling algorithms in a wide variety of problems including random inpainting, block inpainting, denoising, deblurring, destriping, and super-resolution.
</details>
<details>
<summary>摘要</summary>
我团队首先提出了利用预训练的潜在扩散模型解决线性逆问题的框架。之前的方法（如DPS和DDRM）只适用于像素空间扩散模型。我们 theoretically analyzed our algorithm, showing provable sample recovery in a linear model setting. The algorithmic insight obtained from our analysis extends to more general settings often considered in practice. Experimentally, we outperform previously proposed posterior sampling algorithms in a wide variety of problems, including random inpainting, block inpainting, denoising, deblurring, destriping, and super-resolution.Here's the translation in Traditional Chinese:我团队首先提出了利用预训练的潜在扩散模型解决线性逆问题的框架。之前的方法（如DPS和DDRM）只适用于像素空间扩散模型。我们 theoretically analyzed our algorithm, showing provable sample recovery in a linear model setting. The algorithmic insight obtained from our analysis extends to more general settings often considered in practice. Experimentally, we outperform previously proposed posterior sampling algorithms in a wide variety of problems, including random inpainting, block inpainting, denoising, deblurring, destriping, and super-resolution.
</details></li>
</ul>
<hr>
<h2 id="Bounce-a-Reliable-Bayesian-Optimization-Algorithm-for-Combinatorial-and-Mixed-Spaces"><a href="#Bounce-a-Reliable-Bayesian-Optimization-Algorithm-for-Combinatorial-and-Mixed-Spaces" class="headerlink" title="Bounce: a Reliable Bayesian Optimization Algorithm for Combinatorial and Mixed Spaces"></a>Bounce: a Reliable Bayesian Optimization Algorithm for Combinatorial and Mixed Spaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00618">http://arxiv.org/abs/2307.00618</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leonard Papenmeier, Luigi Nardi, Matthias Poloczek</li>
<li>for:  solve high-dimensional black-box functions with mixed and combinatorial input spaces</li>
<li>methods:  uses a novel map of various variable types into nested embeddings of increasing dimensionality</li>
<li>results:  reliably achieves and often improves upon state-of-the-art performance on a variety of high-dimensional problems.<details>
<summary>Abstract</summary>
Impactful applications such as materials discovery, hardware design, neural architecture search, or portfolio optimization require optimizing high-dimensional black-box functions with mixed and combinatorial input spaces. While Bayesian optimization has recently made significant progress in solving such problems, an in-depth analysis reveals that the current state-of-the-art methods are not reliable. Their performances degrade substantially when the unknown optima of the function do not have a certain structure. To fill the need for a reliable algorithm for combinatorial and mixed spaces, this paper proposes Bounce that relies on a novel map of various variable types into nested embeddings of increasing dimensionality. Comprehensive experiments show that Bounce reliably achieves and often even improves upon state-of-the-art performance on a variety of high-dimensional problems.
</details>
<details>
<summary>摘要</summary>
“影响力强大的应用，如材料发现、硬件设计、神经架构搜寻或资产优化，需要优化高维黑盒函数，其中输入空间为杂合和混合的。虽然 Bayesian 优化在最近已经做出了重要进步，但是一个深入分析表明，现今的州际状态艺术方法并不可靠。它们在未知最佳点不具有特定结构时表现很差。为了填补这些问题，本文提出了 Bounce，它基于一个新的变量类型对嵌入的方法。实验表明，Bounce 可靠地达到和有时甚至超越了现今状态艺术方法的性能。”Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="The-Forward-Forward-Algorithm-as-a-feature-extractor-for-skin-lesion-classification-A-preliminary-study"><a href="#The-Forward-Forward-Algorithm-as-a-feature-extractor-for-skin-lesion-classification-A-preliminary-study" class="headerlink" title="The Forward-Forward Algorithm as a feature extractor for skin lesion classification: A preliminary study"></a>The Forward-Forward Algorithm as a feature extractor for skin lesion classification: A preliminary study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00617">http://arxiv.org/abs/2307.00617</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abel Reyes-Angulo, Sidike Paheding</li>
<li>for: 针对皮肤癌的早期诊断，以提高诊断率和治疗效果。</li>
<li>methods: 利用深度学习技术，包括卷积神经网络和变换器，对皮肤癌图像进行分类。</li>
<li>results: 提出一种新的神经网络模型，即前进方法（FFA），可以在低功耗的 анаóg硬件上进行实现，并且可以与传统的反射征文法（BP）相结合，以实现更高的预测精度。<details>
<summary>Abstract</summary>
Skin cancer, a deadly form of cancer, exhibits a 23\% survival rate in the USA with late diagnosis. Early detection can significantly increase the survival rate, and facilitate timely treatment. Accurate biomedical image classification is vital in medical analysis, aiding clinicians in disease diagnosis and treatment. Deep learning (DL) techniques, such as convolutional neural networks and transformers, have revolutionized clinical decision-making automation. However, computational cost and hardware constraints limit the implementation of state-of-the-art DL architectures. In this work, we explore a new type of neural network that does not need backpropagation (BP), namely the Forward-Forward Algorithm (FFA), for skin lesion classification. While FFA is claimed to use very low-power analog hardware, BP still tends to be superior in terms of classification accuracy. In addition, our experimental results suggest that the combination of FFA and BP can be a better alternative to achieve a more accurate prediction.
</details>
<details>
<summary>摘要</summary>
皮肤癌，一种致命的癌症，在美国的诊断晚期时存在23%的存活率。早期发现可以显著提高存活率，并促进时间有序的治疗。医疗分析中，精准的生物医学影像分类是非常重要的，帮助临床医生在疾病诊断和治疗中做出更加准确的决策。深度学习（DL）技术，如卷积神经网络和转换器，已经革命化了临床决策自动化。然而，计算成本和硬件限制使得现状的DL体系难以实施。在这项工作中，我们探索了一种不需要反卷积（BP）的神经网络，即前进方法（FFA），用于皮肤变性分类。虽然FFA声称使用非常低功耗的Analog嵌入式硬件，但BP仍然在分类准确率方面具有优势。此外，我们的实验结果表明，将FFA和BP组合起来可以达到更高的准确预测。
</details></li>
</ul>
<hr>
<h2 id="Fraunhofer-SIT-at-CheckThat-2023-Mixing-Single-Modal-Classifiers-to-Estimate-the-Check-Worthiness-of-Multi-Modal-Tweets"><a href="#Fraunhofer-SIT-at-CheckThat-2023-Mixing-Single-Modal-Classifiers-to-Estimate-the-Check-Worthiness-of-Multi-Modal-Tweets" class="headerlink" title="Fraunhofer SIT at CheckThat! 2023: Mixing Single-Modal Classifiers to Estimate the Check-Worthiness of Multi-Modal Tweets"></a>Fraunhofer SIT at CheckThat! 2023: Mixing Single-Modal Classifiers to Estimate the Check-Worthiness of Multi-Modal Tweets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00610">http://arxiv.org/abs/2307.00610</a></li>
<li>repo_url: None</li>
<li>paper_authors: Raphael Frick, Inna Vogel</li>
<li>for: 本研究旨在提出一种多模态检查性分析方法，用于在社交媒体上分享的 multimedia 数据中检测 false information 和 fake news。</li>
<li>methods: 该方法使用两种类器，每个类器在不同的模态上进行训练。对于图像数据，使用 OCR 分析检测出嵌入的文本表现最佳。</li>
<li>results: 该方法在 CheckThat! 2023 任务1A 中获得了一等奖，其在私有测试集上达到的 F1 分数为 0.7297。<details>
<summary>Abstract</summary>
The option of sharing images, videos and audio files on social media opens up new possibilities for distinguishing between false information and fake news on the Internet. Due to the vast amount of data shared every second on social media, not all data can be verified by a computer or a human expert. Here, a check-worthiness analysis can be used as a first step in the fact-checking pipeline and as a filtering mechanism to improve efficiency. This paper proposes a novel way of detecting the check-worthiness in multi-modal tweets. It takes advantage of two classifiers, each trained on a single modality. For image data, extracting the embedded text with an OCR analysis has shown to perform best. By combining the two classifiers, the proposed solution was able to place first in the CheckThat! 2023 Task 1A with an F1 score of 0.7297 achieved on the private test set.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate "The option of sharing images, videos and audio files on social media opens up new possibilities for distinguishing between false information and fake news on the Internet. Due to the vast amount of data shared every second on social media, not all data can be verified by a computer or a human expert. Here, a check-worthiness analysis can be used as a first step in the fact-checking pipeline and as a filtering mechanism to improve efficiency. This paper proposes a novel way of detecting the check-worthiness in multi-modal tweets. It takes advantage of two classifiers, each trained on a single modality. For image data, extracting the embedded text with an OCR analysis has shown to perform best. By combining the two classifiers, the proposed solution was able to place first in the CheckThat! 2023 Task 1A with an F1 score of 0.7297 achieved on the private test set." into Simplified Chinese.中文简体版：社交媒体上分享图片、视频和音频文件的选项开启了新的方式来分辨假信息和 fake news 在互联网上。由于社交媒体上每秒分享的数据量太多，不能由计算机或人工专家所验证。这里，一种可验证性分析可以作为验证管道的第一步和效率提高的筛选机制。本文提出了一种新的多模态推断方法，利用两个分类器，每个分类器在不同的模式上训练。对图像数据，使用 OCR 分析提取嵌入的文本最佳。将两个分类器结合使用，提出的解决方案在 CheckThat! 2023 任务 1A 中取得了0.7297 的 F1 分数。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://example.com/2023/07/03/cs.LG_2023_07_03/" data-id="cllsjvzbp000rf58898b28ffk" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_07_03" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/03/cs.SD_2023_07_03/" class="article-date">
  <time datetime="2023-07-02T16:00:00.000Z" itemprop="datePublished">2023-07-03</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/03/cs.SD_2023_07_03/">cs.SD - 2023-07-03 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="musif-a-Python-package-for-symbolic-music-feature-extraction"><a href="#musif-a-Python-package-for-symbolic-music-feature-extraction" class="headerlink" title="musif: a Python package for symbolic music feature extraction"></a>musif: a Python package for symbolic music feature extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01120">http://arxiv.org/abs/2307.01120</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/didoneproject/musif">https://github.com/didoneproject/musif</a></li>
<li>paper_authors: Ana Llorens, Federico Simonetta, Martín Serrano, Álvaro Torrente</li>
<li>for: 本研究团队开发了一个名为musif的Python包，用于自动提取Symbolic Music Score中的特征。</li>
<li>methods: musif包包含了一大量的特征，这些特征由音乐学家、音乐理论家、统计学家和计算机科学家团队共同开发。此外，包还允许用户轻松创建自定义特征使用常用的Python库。</li>
<li>results: musif包支持处理高质量的MusicXML格式音乐学数据，同时也支持其他常用的音乐信息检索任务格式，如MIDI、MEI、Kern等。作者提供了详细的文档和教程，以帮助扩展框架并帮助新手了解其使用。<details>
<summary>Abstract</summary>
In this work, we introduce musif, a Python package that facilitates the automatic extraction of features from symbolic music scores. The package includes the implementation of a large number of features, which have been developed by a team of experts in musicology, music theory, statistics, and computer science. Additionally, the package allows for the easy creation of custom features using commonly available Python libraries. musif is primarily geared towards processing high-quality musicological data encoded in MusicXML format, but also supports other formats commonly used in music information retrieval tasks, including MIDI, MEI, Kern, and others. We provide comprehensive documentation and tutorials to aid in the extension of the framework and to facilitate the introduction of new and inexperienced users to its usage.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们介绍了musif，一个Python包，用于自动提取符号音乐谱的特征。该包包括一大量的特征，由音乐学、音乐理论、统计和计算机科学领域的专家们开发。此外，包还允许用户轻松创建自定义特征使用常用的Python库。musif主要针对高质量的音乐学数据编码为MusicXML格式进行处理，也支持其他常用于音乐信息检索任务的格式，包括MIDI、MEI、Kern等。我们提供了完善的文档和教程，以帮助扩展该框架并帮助新用户入门使用。
</details></li>
</ul>
<hr>
<h2 id="Multilingual-Contextual-Adapters-To-Improve-Custom-Word-Recognition-In-Low-resource-Languages"><a href="#Multilingual-Contextual-Adapters-To-Improve-Custom-Word-Recognition-In-Low-resource-Languages" class="headerlink" title="Multilingual Contextual Adapters To Improve Custom Word Recognition In Low-resource Languages"></a>Multilingual Contextual Adapters To Improve Custom Word Recognition In Low-resource Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00759">http://arxiv.org/abs/2307.00759</a></li>
<li>repo_url: None</li>
<li>paper_authors: Devang Kulshreshtha, Saket Dingliwal, Brady Houston, Sravan Bodapati</li>
<li>for: 提高自然语言处理（NLP）中自定义单词的识别率</li>
<li>methods: 使用 Contextual Adapters 进行注意力基于偏移的偏移模型，并在训练过程中使用超vision损失来缓和训练</li>
<li>results: 在低资源语言中提高了自定义单词的检索精度，实现了48% F1提升，同时也导致了基础 CTCL 模型的5-11% 词错率下降<details>
<summary>Abstract</summary>
Connectionist Temporal Classification (CTC) models are popular for their balance between speed and performance for Automatic Speech Recognition (ASR). However, these CTC models still struggle in other areas, such as personalization towards custom words. A recent approach explores Contextual Adapters, wherein an attention-based biasing model for CTC is used to improve the recognition of custom entities. While this approach works well with enough data, we showcase that it isn't an effective strategy for low-resource languages. In this work, we propose a supervision loss for smoother training of the Contextual Adapters. Further, we explore a multilingual strategy to improve performance with limited training data. Our method achieves 48% F1 improvement in retrieving unseen custom entities for a low-resource language. Interestingly, as a by-product of training the Contextual Adapters, we see a 5-11% Word Error Rate (WER) reduction in the performance of the base CTC model as well.
</details>
<details>
<summary>摘要</summary>
卷积时序分类（CTC）模型在自动语音识别（ASR）中具有平衡速度和性能的优点，但这些模型仍然在其他领域面临挑战，例如个性化向custom字进行个性化。一种最近的方法是使用上下文适应器来改善CTC模型中的认知 CustomEntities的识别。虽然这种方法在充足的数据量下工作良好，但我们发现在低资源语言上这种策略并不是有效的。在这种情况下，我们提出了一种超vision损失来帮助Contextual Adapters更平滑地训练。此外，我们探索了一种多语言策略以提高具有有限训练数据的性能。我们的方法实现了一个48%的F1提升在检索未看过的个性化字符串中，并且 Interestingly, 在训练Contextual Adapters的过程中，我们发现了5-11%的单词错误率（WER）下降在基本CTC模型的性能中。
</details></li>
</ul>
<hr>
<h2 id="An-End-to-End-Multi-Module-Audio-Deepfake-Generation-System-for-ADD-Challenge-2023"><a href="#An-End-to-End-Multi-Module-Audio-Deepfake-Generation-System-for-ADD-Challenge-2023" class="headerlink" title="An End-to-End Multi-Module Audio Deepfake Generation System for ADD Challenge 2023"></a>An End-to-End Multi-Module Audio Deepfake Generation System for ADD Challenge 2023</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00729">http://arxiv.org/abs/2307.00729</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sheng Zhao, Qilong Yuan, Yibo Duan, Zhuoyue Chen</li>
<li>for: 本研究主要目标是开发一种可以生成语音内容的语音生成模型，以便模拟人工声音。</li>
<li>methods: 该模型采用了端到端多模块结构，包括说话者编码器、基于Tacotron2的合成器和基于WaveRNN的 vocoder。</li>
<li>results: 经过多种比较实验和模型结构的研究，该模型最终在ADD 2023挑战赛Track 1.1中获得了44.97%的Weighted Deception Success Rate（WDSR）。<details>
<summary>Abstract</summary>
The task of synthetic speech generation is to generate language content from a given text, then simulating fake human voice.The key factors that determine the effect of synthetic speech generation mainly include speed of generation, accuracy of word segmentation, naturalness of synthesized speech, etc. This paper builds an end-to-end multi-module synthetic speech generation model, including speaker encoder, synthesizer based on Tacotron2, and vocoder based on WaveRNN. In addition, we perform a lot of comparative experiments on different datasets and various model structures. Finally, we won the first place in the ADD 2023 challenge Track 1.1 with the weighted deception success rate (WDSR) of 44.97%.
</details>
<details>
<summary>摘要</summary>
文本生成任务的目标是将文本转化为语言内容，然后模拟人工嗓音。主要影响生成效果的因素包括生成速度、单词分 segmentation 精度、生成的嗓音自然程度等。这篇文章建立了端到端多模块合成嗓音模型，包括说话者编码器、基于 Tacotron2 的生成器和基于 WaveRNN 的 vocoder。此外，我们进行了多种比较 эксперименты，包括不同的数据集和模型结构。最后，我们在 ADD 2023 挑战赛 Track 1.1 中获得了44.97%的Weighted Deception Success Rate（WDSR）。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://example.com/2023/07/03/cs.SD_2023_07_03/" data-id="cllsjvzcd0030f5880tgecfcd" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_07_03" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/03/eess.AS_2023_07_03/" class="article-date">
  <time datetime="2023-07-02T16:00:00.000Z" itemprop="datePublished">2023-07-03</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/03/eess.AS_2023_07_03/">eess.AS - 2023-07-03 22:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="ContextSpeech-Expressive-and-Efficient-Text-to-Speech-for-Paragraph-Reading"><a href="#ContextSpeech-Expressive-and-Efficient-Text-to-Speech-for-Paragraph-Reading" class="headerlink" title="ContextSpeech: Expressive and Efficient Text-to-Speech for Paragraph Reading"></a>ContextSpeech: Expressive and Efficient Text-to-Speech for Paragraph Reading</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00782">http://arxiv.org/abs/2307.00782</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yujia Xiao, Shaofei Zhang, Xi Wang, Xu Tan, Lei He, Sheng Zhao, Frank K. Soong, Tan Lee</li>
<li>for: 这项研究旨在提高文本转语音（TTS）系统的长文朗读质量。</li>
<li>methods: 该研究提出了一种轻量级 yet有效的 TTS 系统，即 ContextSpeech。该系统首先设计了一种储存机制，以利用全文和语音上下文来增强句子编码。然后，它构建了层次结构的文本 semantics，以扩大全文上下文的增强范围。最后，它综合应用了线性化自注意力，以提高模型效率。</li>
<li>results: 实验表明，ContextSpeech 在段落读物中提高了声音质量和语调表达性，与竞争性模型相当。示例响应器可以在以下链接中浏览：<a target="_blank" rel="noopener" href="https://contextspeech.github.io/demo/">https://contextspeech.github.io/demo/</a><details>
<summary>Abstract</summary>
While state-of-the-art Text-to-Speech systems can generate natural speech of very high quality at sentence level, they still meet great challenges in speech generation for paragraph / long-form reading. Such deficiencies are due to i) ignorance of cross-sentence contextual information, and ii) high computation and memory cost for long-form synthesis. To address these issues, this work develops a lightweight yet effective TTS system, ContextSpeech. Specifically, we first design a memory-cached recurrence mechanism to incorporate global text and speech context into sentence encoding. Then we construct hierarchically-structured textual semantics to broaden the scope for global context enhancement. Additionally, we integrate linearized self-attention to improve model efficiency. Experiments show that ContextSpeech significantly improves the voice quality and prosody expressiveness in paragraph reading with competitive model efficiency. Audio samples are available at: https://contextspeech.github.io/demo/
</details>
<details>
<summary>摘要</summary>
“当前的文本到语音系统可以生成具有非常高质量的自然语音，但是在段落/长文读取中仍然存在很大的挑战。这些问题的原因是：一、忽略跨句Contextual信息，二、长文合成的计算和内存成本过高。为了解决这些问题，本工作开发了一个轻量级又有效的文本到语音系统——ContextSpeech。具体来说，我们首先设计了一种嵌入式的记忆缓存机制，以将全文和语音Context incorporated into sentence encoding。然后，我们构建了层次结构的文本 semantics，以扩大全文Context的增强范围。此外，我们将Linearized self-attention integrated into the model，以提高模型效率。实验表明，ContextSpeech可以在段落读取中显著提高声音质量和表达性，并且与其他模型相比，其效率相对较高。听 samples可以在：https://contextspeech.github.io/demo/ ”Note that the translation is in Simplified Chinese, which is the standard written form of Chinese used in mainland China. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://example.com/2023/07/03/eess.AS_2023_07_03/" data-id="cllsjvzd8005cf5885a1ufcwk" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_07_03" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/03/eess.IV_2023_07_03/" class="article-date">
  <time datetime="2023-07-02T16:00:00.000Z" itemprop="datePublished">2023-07-03</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/03/eess.IV_2023_07_03/">eess.IV - 2023-07-03 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Cross-modality-Attention-Adapter-A-Glioma-Segmentation-Fine-tuning-Method-for-SAM-Using-Multimodal-Brain-MR-Images"><a href="#Cross-modality-Attention-Adapter-A-Glioma-Segmentation-Fine-tuning-Method-for-SAM-Using-Multimodal-Brain-MR-Images" class="headerlink" title="Cross-modality Attention Adapter: A Glioma Segmentation Fine-tuning Method for SAM Using Multimodal Brain MR Images"></a>Cross-modality Attention Adapter: A Glioma Segmentation Fine-tuning Method for SAM Using Multimodal Brain MR Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01124">http://arxiv.org/abs/2307.01124</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoyu Shi, Shurong Chai, Yinhao Li, Jingliang Cheng, Jie Bai, Guohua Zhao, Yen-Wei Chen</li>
<li>for: 针对于基因型预测和诊断 glioma 的基础设施建设</li>
<li>methods: 利用 multimodal 融合和 Cross-modality attention adapter 来细化基础模型，以提高 glioma 分 segmentation 的准确性</li>
<li>results: 在 private  glioma 数据集上，提出的方法可以达到 88.38% 的 Dice 和 10.64 的 Hausdorff distance，比现状态对照方法提高了 4%，为 glioma 治疗带来更好的效果<details>
<summary>Abstract</summary>
According to the 2021 World Health Organization (WHO) Classification scheme for gliomas, glioma segmentation is a very important basis for diagnosis and genotype prediction. In general, 3D multimodal brain MRI is an effective diagnostic tool. In the past decade, there has been an increase in the use of machine learning, particularly deep learning, for medical images processing. Thanks to the development of foundation models, models pre-trained with large-scale datasets have achieved better results on a variety of tasks. However, for medical images with small dataset sizes, deep learning methods struggle to achieve better results on real-world image datasets. In this paper, we propose a cross-modality attention adapter based on multimodal fusion to fine-tune the foundation model to accomplish the task of glioma segmentation in multimodal MRI brain images with better results. The effectiveness of the proposed method is validated via our private glioma data set from the First Affiliated Hospital of Zhengzhou University (FHZU) in Zhengzhou, China. Our proposed method is superior to current state-of-the-art methods with a Dice of 88.38% and Hausdorff distance of 10.64, thereby exhibiting a 4% increase in Dice to segment the glioma region for glioma treatment.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)根据2021年世界卫生组织（WHO）的分类方案，肿瘤分 segmentation 是诊断和基因预测的重要基础。一般来说，3D多Modal MRI 是脑部的有效诊断工具。过去十年，机器学习，特别是深度学习，在医疗图像处理领域得到了广泛的应用。感谢基模型的发展，预训练于大规模数据集的模型在多种任务上取得了更好的结果。然而，医疗图像中的小 dataset  SIZE 下，深度学习方法在真实世界图像集上表现不佳。在这篇论文中，我们提出了跨Modal 注意力适配器，基于多Modal 融合来细化基模型，以实现多Modal MRI 脑部图像中的肿瘤分 segmentation 任务。我们验证了我们的提议方法的有效性，通过我们自己的私人肿瘤数据集，来自 Zhengzhou University First Affiliated Hospital（FHZU）在 Zhengzhou, China。我们的提议方法比现有状态的方法高出4%的Dice，即88.38%，并且 Hausdorff distance 为10.64，因此能够更好地 segmentation 肿瘤区域，为肿瘤治疗提供更好的基础。
</details></li>
</ul>
<hr>
<h2 id="HODINet-High-Order-Discrepant-Interaction-Network-for-RGB-D-Salient-Object-Detection"><a href="#HODINet-High-Order-Discrepant-Interaction-Network-for-RGB-D-Salient-Object-Detection" class="headerlink" title="HODINet: High-Order Discrepant Interaction Network for RGB-D Salient Object Detection"></a>HODINet: High-Order Discrepant Interaction Network for RGB-D Salient Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00954">http://arxiv.org/abs/2307.00954</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kang Yi, Jing Xu, Xiao Jin, Fu Guo, Yan-Feng Wu</li>
<li>for: RGB-D SOD 提高 salient object detection 的精度和效率，并可以更好地处理不同类型的图像和深度图像。</li>
<li>methods: 提出了一种基于 transformer 和 CNN 架构的高阶不同交互网络 (HODINet)，通过不同阶段的权重 fusion 来实现跨模态特征的共同学习和权重调整。</li>
<li>results: 对七个常用的数据集进行了广泛的实验，并达到了对 24 种state-of-the-art 方法的竞争性性能，并且在四个评价指标上显示了更高的精度和效率。<details>
<summary>Abstract</summary>
RGB-D salient object detection (SOD) aims to detect the prominent regions by jointly modeling RGB and depth information. Most RGB-D SOD methods apply the same type of backbones and fusion modules to identically learn the multimodality and multistage features. However, these features contribute differently to the final saliency results, which raises two issues: 1) how to model discrepant characteristics of RGB images and depth maps; 2) how to fuse these cross-modality features in different stages. In this paper, we propose a high-order discrepant interaction network (HODINet) for RGB-D SOD. Concretely, we first employ transformer-based and CNN-based architectures as backbones to encode RGB and depth features, respectively. Then, the high-order representations are delicately extracted and embedded into spatial and channel attentions for cross-modality feature fusion in different stages. Specifically, we design a high-order spatial fusion (HOSF) module and a high-order channel fusion (HOCF) module to fuse features of the first two and the last two stages, respectively. Besides, a cascaded pyramid reconstruction network is adopted to progressively decode the fused features in a top-down pathway. Extensive experiments are conducted on seven widely used datasets to demonstrate the effectiveness of the proposed approach. We achieve competitive performance against 24 state-of-the-art methods under four evaluation metrics.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="An-open-source-deep-learning-algorithm-for-efficient-and-fully-automatic-analysis-of-the-choroid-in-optical-coherence-tomography"><a href="#An-open-source-deep-learning-algorithm-for-efficient-and-fully-automatic-analysis-of-the-choroid-in-optical-coherence-tomography" class="headerlink" title="An open-source deep learning algorithm for efficient and fully-automatic analysis of the choroid in optical coherence tomography"></a>An open-source deep learning algorithm for efficient and fully-automatic analysis of the choroid in optical coherence tomography</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00904">http://arxiv.org/abs/2307.00904</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jamie Burke, Justin Engelmann, Charlene Hamid, Megan Reid-Schachter, Tom Pearson, Dan Pugh, Neeraj Dhaun, Stuart King, Tom MacGillivray, Miguel O. Bernabeu, Amos Storkey, Ian J. C. MacCormick</li>
<li>For: The paper aims to develop a fully-automatic, open-source algorithm for choroidal segmentation in optical coherence tomography (OCT) data.* Methods: The authors used a dataset of 715 OCT B-scans from 3 clinical studies, and finetuned a pre-trained UNet with MobileNetV3 backbone to segment the choroid region.* Results: DeepGPET, the proposed algorithm, achieves excellent agreement with a clinically validated semi-automatic segmentation method (GPET) and reduces the processing time per image by a factor of 27.Here’s the information in Simplified Chinese text:</li>
<li>for: 这个研究的目的是开发一个完全自动化、开源的choroidregion分割算法，用于optical coherence tomography（OCT）数据。</li>
<li>methods: 作者使用了715个OCT B-scan数据（82个研究subject，115个眼球）从3个临床研究，并对pre-trained的UNet with MobileNetV3 backbone进行了微调，以实现choroidregion的分割。</li>
<li>results: DeepGPET，提出的算法，与临床验证的半自动化分割方法（GPET）达到了极高的一致性（AUC&#x3D;0.9994，Dice&#x3D;0.9664；Pearson相关系数为0.8908和0.9082），同时将每个图像的处理时间从34.49秒（±15.09）降低到1.25秒（±0.10）。两种方法在临床专业人员的评价下表现相似，而且不需要人工干预。<details>
<summary>Abstract</summary>
Purpose: To develop an open-source, fully-automatic deep learning algorithm, DeepGPET, for choroid region segmentation in optical coherence tomography (OCT) data. Methods: We used a dataset of 715 OCT B-scans (82 subjects, 115 eyes) from 3 clinical studies related to systemic disease. Ground truth segmentations were generated using a clinically validated, semi-automatic choroid segmentation method, Gaussian Process Edge Tracing (GPET). We finetuned a UNet with MobileNetV3 backbone pre-trained on ImageNet. Standard segmentation agreement metrics, as well as derived measures of choroidal thickness and area, were used to evaluate DeepGPET, alongside qualitative evaluation from a clinical ophthalmologist. Results: DeepGPET achieves excellent agreement with GPET on data from 3 clinical studies (AUC=0.9994, Dice=0.9664; Pearson correlation of 0.8908 for choroidal thickness and 0.9082 for choroidal area), while reducing the mean processing time per image on a standard laptop CPU from 34.49s ($\pm$15.09) using GPET to 1.25s ($\pm$0.10) using DeepGPET. Both methods performed similarly according to a clinical ophthalmologist, who qualitatively judged a subset of segmentations by GPET and DeepGPET, based on smoothness and accuracy of segmentations. Conclusions :DeepGPET, a fully-automatic, open-source algorithm for choroidal segmentation, will enable researchers to efficiently extract choroidal measurements, even for large datasets. As no manual interventions are required, DeepGPET is less subjective than semi-automatic methods and could be deployed in clinical practice without necessitating a trained operator. DeepGPET addresses the lack of open-source, fully-automatic and clinically relevant choroid segmentation algorithms, and its subsequent public release will facilitate future choroidal research both in ophthalmology and wider systemic health.
</details>
<details>
<summary>摘要</summary>
目的：开发一个开源的自动化深度学习算法DeepGPET，用于光谱凝结 Tomatoes（OCT）数据中的choroid区域分割。方法：我们使用了715个OCT B-scan（82名病人，115只眼）从3个临床研究中获取数据，这些研究与系统疾病相关。我们使用了一种临床验证的、 semi-automatic choroid分割方法—— Gaussian Process Edge Tracing（GPET）生成了参考标准分割。我们使用了MobileNetV3预训练在ImageNet上的UNet，并对其进行了微调。我们使用了标准的分割一致度量和 derived的choroid厚度和面积度量来评估DeepGPET，并与一名临床眼科医生的评估。结果：DeepGPET与GPET在3个临床研究中达到了极高的一致度（AUC=0.9994，Dice=0.9664； Pearson相关度=0.8908 дляchoroid厚度和0.9082 дляchoroid面积），而同时减少了每个图像的平均处理时间从34.49秒（±15.09）使用GPET下降到1.25秒（±0.10）使用DeepGPET。两种方法在临床眼科医生的评估下表现相似，后者根据分割的平滑度和准确性进行质量评估。结论：DeepGPET是一个开源的、自动化的、临床相关的choroid分割算法，可以帮助研究人员快速提取choroid的测量数据，即使是大型数据集。由于不需要人工干预，DeepGPET比 semi-automatic方法更加 Objective，可以在临床实践中无需训练操作员而使用。DeepGPET解决了开源、自动化和临床相关的choroid分割算法的缺失，其后续的公共发布将促进未来choroid研究的进展，不仅在眼科领域，还在更广泛的系统医学领域。
</details></li>
</ul>
<hr>
<h2 id="Synthesis-of-Contrast-Enhanced-Breast-MRI-Using-Multi-b-Value-DWI-based-Hierarchical-Fusion-Network-with-Attention-Mechanism"><a href="#Synthesis-of-Contrast-Enhanced-Breast-MRI-Using-Multi-b-Value-DWI-based-Hierarchical-Fusion-Network-with-Attention-Mechanism" class="headerlink" title="Synthesis of Contrast-Enhanced Breast MRI Using Multi-b-Value DWI-based Hierarchical Fusion Network with Attention Mechanism"></a>Synthesis of Contrast-Enhanced Breast MRI Using Multi-b-Value DWI-based Hierarchical Fusion Network with Attention Mechanism</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00895">http://arxiv.org/abs/2307.00895</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianyu Zhang, Luyi Han, Anna D’Angelo, Xin Wang, Yuan Gao, Chunyao Lu, Jonas Teuwen, Regina Beets-Tan, Tao Tan, Ritse Mann<br>for:The paper aims to develop a multi-sequence fusion network to synthesize contrast-enhanced MRI (CE-MRI) based on T1-weighted MRI and diffusion-weighted imaging (DWI) to potentially reduce or avoid the use of gadolinium-based contrast agents (GBCA).methods:The proposed method uses a multi-sequence fusion network that combines T1-weighted MRI and DWIs with different b-values to efficiently utilize the difference features of DWIs. The method also includes a multi-sequence attention module to obtain refined feature maps and a weighted difference module to leverage hierarchical representation information fused at different scales.results:The results show that the multi-b-value DWI-based fusion model can potentially be used to synthesize CE-MRI, thus theoretically reducing or avoiding the use of GBCA, thereby minimizing the burden to patients.<details>
<summary>Abstract</summary>
Magnetic resonance imaging (MRI) is the most sensitive technique for breast cancer detection among current clinical imaging modalities. Contrast-enhanced MRI (CE-MRI) provides superior differentiation between tumors and invaded healthy tissue, and has become an indispensable technique in the detection and evaluation of cancer. However, the use of gadolinium-based contrast agents (GBCA) to obtain CE-MRI may be associated with nephrogenic systemic fibrosis and may lead to bioaccumulation in the brain, posing a potential risk to human health. Moreover, and likely more important, the use of gadolinium-based contrast agents requires the cannulation of a vein, and the injection of the contrast media which is cumbersome and places a burden on the patient. To reduce the use of contrast agents, diffusion-weighted imaging (DWI) is emerging as a key imaging technique, although currently usually complementing breast CE-MRI. In this study, we develop a multi-sequence fusion network to synthesize CE-MRI based on T1-weighted MRI and DWIs. DWIs with different b-values are fused to efficiently utilize the difference features of DWIs. Rather than proposing a pure data-driven approach, we invent a multi-sequence attention module to obtain refined feature maps, and leverage hierarchical representation information fused at different scales while utilizing the contributions from different sequences from a model-driven approach by introducing the weighted difference module. The results show that the multi-b-value DWI-based fusion model can potentially be used to synthesize CE-MRI, thus theoretically reducing or avoiding the use of GBCA, thereby minimizing the burden to patients. Our code is available at \url{https://github.com/Netherlands-Cancer-Institute/CE-MRI}.
</details>
<details>
<summary>摘要</summary>
磁共振成像（MRI）是当前临床成像技术中检测乳腺癌最敏感的方法。增强磁共振成像（CE-MRI）可提供较好的诊断和评估结果，但是使用加多林镁基contrast剂（GBCA）可能会导致肾脏系统 fibrosis和脑部堆积，危及人类健康。此外，使用contrast剂还需要胸部静脉填充和干扰性高，对患者造成困扰。为了减少contrast剂的使用，扩散成像（DWI）正在成为一种关键的成像技术，尽管目前通常作为乳腺CE-MRI的补充。在这项研究中，我们开发了一种多序列融合网络，将CE-MRI基于T1束重度成像和DWI的数据融合。DWI的不同b值被融合，以高效利用DWI的差异特征。而不是直接提出数据驱动的方法，我们创造了一种多序列注意模块，以获得精细的特征地图，并利用层次表示信息在不同级别上进行融合，同时利用不同序列的贡献。结果表明，多b值DWI基本融合模型可能可以Synthesize CE-MRI，从而可能避免或减少GBCA的使用，为患者减轻负担。我们的代码可以在 <https://github.com/Netherlands-Cancer-Institute/CE-MRI> 上获取。
</details></li>
</ul>
<hr>
<h2 id="An-Explainable-Deep-Framework-Towards-Task-Specific-Fusion-for-Multi-to-One-MRI-Synthesis"><a href="#An-Explainable-Deep-Framework-Towards-Task-Specific-Fusion-for-Multi-to-One-MRI-Synthesis" class="headerlink" title="An Explainable Deep Framework: Towards Task-Specific Fusion for Multi-to-One MRI Synthesis"></a>An Explainable Deep Framework: Towards Task-Specific Fusion for Multi-to-One MRI Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00885">http://arxiv.org/abs/2307.00885</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fiy2w/mri_seq2seq">https://github.com/fiy2w/mri_seq2seq</a></li>
<li>paper_authors: Luyi Han, Tianyu Zhang, Yunzhi Huang, Haoran Dou, Xin Wang, Yuan Gao, Chunyao Lu, Tan Tao, Ritse Mann</li>
<li>for: 这篇论文的目的是提出一个可解释的任务特定的合成网络，以便在缺失某些序列的情况下，可以将多个可用的序列合成为完整的序列。</li>
<li>methods: 这篇论文使用的方法是基于深度学习的合成方法，并将这些方法与可解释的任务特定的模组结合在一起，以提高合成的可靠性和可读性。</li>
<li>results: 根据BraTS2021 dataset的1251个主题，这篇论文的方法比之前的方法更好，并且可以将缺失的序列合成为完整的序列。<details>
<summary>Abstract</summary>
Multi-sequence MRI is valuable in clinical settings for reliable diagnosis and treatment prognosis, but some sequences may be unusable or missing for various reasons. To address this issue, MRI synthesis is a potential solution. Recent deep learning-based methods have achieved good performance in combining multiple available sequences for missing sequence synthesis. Despite their success, these methods lack the ability to quantify the contributions of different input sequences and estimate the quality of generated images, making it hard to be practical. Hence, we propose an explainable task-specific synthesis network, which adapts weights automatically for specific sequence generation tasks and provides interpretability and reliability from two sides: (1) visualize the contribution of each input sequence in the fusion stage by a trainable task-specific weighted average module; (2) highlight the area the network tried to refine during synthesizing by a task-specific attention module. We conduct experiments on the BraTS2021 dataset of 1251 subjects, and results on arbitrary sequence synthesis indicate that the proposed method achieves better performance than the state-of-the-art methods. Our code is available at \url{https://github.com/fiy2W/mri_seq2seq}.
</details>
<details>
<summary>摘要</summary>
多序列MRI在临床设置中是有价值的，可以帮助确定疾病诊断和治疗预测，但某些序列可能无法使用或缺失。为解决这个问题，MRI合成是一个可能的解决方案。最新的深度学习基于方法在多个可用序列的组合中实现了好的性能。尽管它们在实际应用中具有成功，但它们缺乏对不同输入序列的贡献的评估和生成图像质量的估计，这使得它们在实践中具有困难。因此，我们提出了可解释的任务特定合成网络，它自动适应不同的序列生成任务，并提供了可读性和可靠性从两个方面：1. 在合并阶段，通过可训练的任务特定权重平均模块，可以视觑每个输入序列的贡献。2. 通过任务特定注意力模块，可以高亮生成过程中网络尝试修改的区域。我们在BraTS2021数据集上进行了1251个主题的实验，并得到了比 estado-of-the-art 方法更好的性能。我们的代码可以在 GitHub 上找到：\url{https://github.com/fiy2W/mri_seq2seq}。
</details></li>
</ul>
<hr>
<h2 id="End-To-End-Prediction-of-Knee-Osteoarthritis-Progression-With-Multi-Modal-Transformers"><a href="#End-To-End-Prediction-of-Knee-Osteoarthritis-Progression-With-Multi-Modal-Transformers" class="headerlink" title="End-To-End Prediction of Knee Osteoarthritis Progression With Multi-Modal Transformers"></a>End-To-End Prediction of Knee Osteoarthritis Progression With Multi-Modal Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00873">http://arxiv.org/abs/2307.00873</a></li>
<li>repo_url: None</li>
<li>paper_authors: Egor Panfilov, Simo Saarakkala, Miika T. Nieminen, Aleksei Tiulpin</li>
<li>for: The paper aims to develop a unified framework for multi-modal fusion of knee imaging data to predict the progression of knee osteoarthritis (KOA) and provide new tools for the design of more efficient clinical trials.</li>
<li>methods: The authors use a Transformer approach to fuse structural knee MRI, multi-modal imaging data, and clinical data to predict KOA progression. They analyze the performance of their framework across different progression horizons and investigate the effectiveness of different modalities and subject subgroups.</li>
<li>results: The authors report that structural knee MRI can identify radiographic KOA progressors with an area under the ROC curve (ROC AUC) of 0.70-0.76 and Average Precision (AP) of 0.15-0.54 in 2-8 year horizons. They also find that multi-modal fusion of imaging data can predict KOA progression within 1 year with high accuracy (ROC AUC of 0.76(0.04), AP of 0.13(0.04)). Additionally, they identify post-traumatic subjects as the most accurate for prediction from imaging data.<details>
<summary>Abstract</summary>
Knee Osteoarthritis (KOA) is a highly prevalent chronic musculoskeletal condition with no currently available treatment. The manifestation of KOA is heterogeneous and prediction of its progression is challenging. Current literature suggests that the use of multi-modal data and advanced modeling methods, such as the ones based on Deep Learning, has promise in tackling this challenge. To date, however, the evidence on the efficacy of this approach is limited. In this study, we leveraged recent advances in Deep Learning and, using a Transformer approach, developed a unified framework for the multi-modal fusion of knee imaging data. Subsequently, we analyzed its performance across a range of scenarios by investigating multiple progression horizons -- from short-term to long-term. We report our findings using a large cohort (n=2421-3967) derived from the Osteoarthritis Initiative dataset. We show that structural knee MRI allows identifying radiographic KOA progressors on par with multi-modal fusion approaches, achieving an area under the ROC curve (ROC AUC) of 0.70-0.76 and Average Precision (AP) of 0.15-0.54 in 2-8 year horizons. Progression within 1 year was better predicted with a multi-modal method using X-ray, structural, and compositional MR images -- ROC AUC of 0.76(0.04), AP of 0.13(0.04) -- or via clinical data. Our follow-up analysis generally shows that prediction from the imaging data is more accurate for post-traumatic subjects, and we further investigate which subject subgroups may benefit the most. The present study provides novel insights into multi-modal imaging of KOA and brings a unified data-driven framework for studying its progression in an end-to-end manner, providing new tools for the design of more efficient clinical trials. The source code of our framework and the pre-trained models are made publicly available.
</details>
<details>
<summary>摘要</summary>
《骨 JOINT arthritis (KOA) 是一种非常普遍的慢性骨附着病，目前还没有任何有效的治疗方法。KOA 的表现具有各种各样的特征，预测其进程是非常困难的。当前的文献表明，使用多Modal 数据和高级模型方法，如基于深度学习的方法，有可能解决这一挑战。然而，至今为止，这种方法的有效性的证据还很有限。在这项研究中，我们利用了最新的深度学习技术，使用Transformer 方法，对骨 JOINT 成像数据进行多Modal 融合。然后，我们分析了其性能在不同的enario中，包括不同的进程时间 horizon （从短期到长期）。我们通过使用大量的 cohort （n=2421-3967）， derive 从骨 JOINT 创新数据集，报告我们的发现。我们发现，骨 JOINT 成像 MRI 可以与多Modal 融合方法一样准确地识别骨 JOINT 急性进程者，其 ROC AUC 在 0.70-0.76 之间，AP 在 0.15-0.54 之间。在 1 年内的进程预测中，使用 X-ray、骨 JOINT 结构和组成 MRI 的多Modal 方法可以达到 ROC AUC 的 0.76(0.04)，AP 的 0.13(0.04)。或者通过临床数据来预测。我们的跟踪分析通常表明，骨 JOINT 成像数据预测的准确性更高，特别是对post-traumatic 患者。我们进一步调查了哪些子群体可能受益最多。这项研究提供了关于骨 JOINT 多Modal 成像的新的意见，并提供了一个通用的数据驱动的框架，可以在一个端到端的方式来研究骨 JOINT 进程的预测，提供新的工具，用于设计更有效的临床试验。我们的框架和预训练模型的源代码公开可用。
</details></li>
</ul>
<hr>
<h2 id="Anisotropic-Fanning-Aware-Low-Rank-Tensor-Approximation-Based-Tractography"><a href="#Anisotropic-Fanning-Aware-Low-Rank-Tensor-Approximation-Based-Tractography" class="headerlink" title="Anisotropic Fanning Aware Low-Rank Tensor Approximation Based Tractography"></a>Anisotropic Fanning Aware Low-Rank Tensor Approximation Based Tractography</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00833">http://arxiv.org/abs/2307.00833</a></li>
<li>repo_url: None</li>
<li>paper_authors: Johannes Grün, Jonah Sieg, Thomas Schultz</li>
<li>for: 本研究旨在提高某些 tractography 方法的完teness和准确性，特别是处理纤维 crossing 和 fanning 问题。</li>
<li>methods: 该研究使用了 low-rank higher-order tensor approximation 技术，并将 anisotropic fanning 模型integrated into a recently proposed tractography method。</li>
<li>results: 研究结果表明，在12个人 connectome project Subjects中，该扩展模型可以增加 tract 的完teness和准确性，同时可以避免过度重建。此外，该模型的结果也比基于 Watson 分布的简单模型更为准确。<details>
<summary>Abstract</summary>
Low-rank higher-order tensor approximation has been used successfully to extract discrete directions for tractography from continuous fiber orientation density functions (fODFs). However, while it accounts for fiber crossings, it has so far ignored fanning, which has led to incomplete reconstructions. In this work, we integrate an anisotropic model of fanning based on the Bingham distribution into a recently proposed tractography method that performs low-rank approximation with an Unscented Kalman Filter. Our technical contributions include an initialization scheme for the new parameters, which is based on the Hessian of the low-rank approximation, pre-integration of the required convolution integrals to reduce the computational effort, and representation of the required 3D rotations with quaternions. Results on 12 subjects from the Human Connectome Project confirm that, in almost all considered tracts, our extended model significantly increases completeness of the reconstruction, while reducing excess, at acceptable additional computational cost. Its results are also more accurate than those from a simpler, isotropic fanning model that is based on Watson distributions.
</details>
<details>
<summary>摘要</summary>
低阶高级张量近似法已成功地提取维度分布函数（fODFs）中连续纤维方向的精确方向。然而，它虽然考虑到纤维交叉，但忽略了扩散，导致重建不准确。在这项工作中，我们将基于比频分布的扩散模型纳入最近提出的 tractography 方法中，并使用不确定 kalman 滤波器进行低阶近似。我们的技术贡献包括基于低阶近似的 Hessian 初始化方法，预处理所需的 convolution 积分，以及使用 quaternion 表示所需的 3D 旋转。results 表明，在12名人类连接度计划的试验Subject中，我们的扩展模型可以在大多数考虑的轨迹中提高重建的完整性，而且降低过量，acceptable的额外计算成本。它的结果还比基于 Watson 分布的简单的扩散模型更准确。
</details></li>
</ul>
<hr>
<h2 id="ACDMSR-Accelerated-Conditional-Diffusion-Models-for-Single-Image-Super-Resolution"><a href="#ACDMSR-Accelerated-Conditional-Diffusion-Models-for-Single-Image-Super-Resolution" class="headerlink" title="ACDMSR: Accelerated Conditional Diffusion Models for Single Image Super-Resolution"></a>ACDMSR: Accelerated Conditional Diffusion Models for Single Image Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00781">http://arxiv.org/abs/2307.00781</a></li>
<li>repo_url: None</li>
<li>paper_authors: Axi Niu, Pham Xuan Trung, Kang Zhang, Jinqiu Sun, Yu Zhu, In So Kweon, Yanning Zhang</li>
<li>for: 这个论文是为了提高图像超分辨率（SR）的批处速度而设计的。</li>
<li>methods: 该方法基于标准扩散模型，通过确定性的迭代减噪过程来实现SR。</li>
<li>results: 对于多个标准 benchmark 数据集（Set5、Set14、Urban100、BSD100、Manga109）的实验结果表明，我们的方法可以超过前一个尝试，并且生成更加可见和实用的低分辨率图像的高分辨率对应图像。<details>
<summary>Abstract</summary>
Diffusion models have gained significant popularity in the field of image-to-image translation. Previous efforts applying diffusion models to image super-resolution (SR) have demonstrated that iteratively refining pure Gaussian noise using a U-Net architecture trained on denoising at various noise levels can yield satisfactory high-resolution images from low-resolution inputs. However, this iterative refinement process comes with the drawback of low inference speed, which strongly limits its applications. To speed up inference and further enhance the performance, our research revisits diffusion models in image super-resolution and proposes a straightforward yet significant diffusion model-based super-resolution method called ACDMSR (accelerated conditional diffusion model for image super-resolution). Specifically, our method adapts the standard diffusion model to perform super-resolution through a deterministic iterative denoising process. Our study also highlights the effectiveness of using a pre-trained SR model to provide the conditional image of the given low-resolution (LR) image to achieve superior high-resolution results. We demonstrate that our method surpasses previous attempts in qualitative and quantitative results through extensive experiments conducted on benchmark datasets such as Set5, Set14, Urban100, BSD100, and Manga109. Moreover, our approach generates more visually realistic counterparts for low-resolution images, emphasizing its effectiveness in practical scenarios.
</details>
<details>
<summary>摘要</summary>
Diffusion models 在图像到图像翻译领域得到了广泛的应用。先前的尝试使用 diffusion models 进行图像超解析（SR）已经证明了，通过多次使用 U-Net 架构训练在不同噪声水平上进行杜尼进行反射，可以从低解析输入图像中获得满意的高解析图像。然而，这个迭代纠正过程受到了推理速度的限制，这限制了其应用。为了加速推理和进一步提高性能，我们的研究重新评估了 diffusion models 在图像 SR 领域，并提出了一种简单但具有显著意义的 diffusion model-based SR 方法，称为 ACDMSR (加速条件扩散模型 для图像 SR)。我们的方法将标准的扩散模型应用于图像 SR 领域，通过一种决定性的迭代噪声纠正过程来进行 SR。我们的研究还发现了使用预训练 SR 模型提供给给定的低解析（LR）图像的 conditional 图像，可以实现更高的高解析结果。我们在 Set5、Set14、Urban100、BSD100 和 Manga109 等标准数据集上进行了广泛的实验，并证明了我们的方法在质量和量化方面的优越性。此外，我们的方法可以生成更加视觉真实的低解析图像，强调其在实际应用中的有效性。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Visual-Fault-Detection-for-Freight-Train-Braking-System-via-Heterogeneous-Self-Distillation-in-the-Wild"><a href="#Efficient-Visual-Fault-Detection-for-Freight-Train-Braking-System-via-Heterogeneous-Self-Distillation-in-the-Wild" class="headerlink" title="Efficient Visual Fault Detection for Freight Train Braking System via Heterogeneous Self Distillation in the Wild"></a>Efficient Visual Fault Detection for Freight Train Braking System via Heterogeneous Self Distillation in the Wild</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00701">http://arxiv.org/abs/2307.00701</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/MVME-HBUT/HSD-FTI-FDet">https://github.com/MVME-HBUT/HSD-FTI-FDet</a></li>
<li>paper_authors: Yang Zhang, Huilin Pan, Yang Zhou, Mingying Li, Guodong Sun</li>
<li>for: 本文提出了一种基于自适应融合的深度学习检测折衣列车 fault的方法，以确保铁路运营的安全性。</li>
<li>methods: 本文使用了一种带有多种特征的自适应融合框架，以确保检测精度和速度的同时满足资源限制。在这个框架中，教师模型通过灌注秘密信息来帮助学生模型提高性能。</li>
<li>results: 实验结果表明，本文的方法可以在四个 fault 数据集上达到37帧每秒的速度，并保持最高准确率。相比传统的杂合方法，本文的方法具有较低的内存使用量和最小的模型大小。<details>
<summary>Abstract</summary>
Efficient visual fault detection of freight trains is a critical part of ensuring the safe operation of railways under the restricted hardware environment. Although deep learning-based approaches have excelled in object detection, the efficiency of freight train fault detection is still insufficient to apply in real-world engineering. This paper proposes a heterogeneous self-distillation framework to ensure detection accuracy and speed while satisfying low resource requirements. The privileged information in the output feature knowledge can be transferred from the teacher to the student model through distillation to boost performance. We first adopt a lightweight backbone to extract features and generate a new heterogeneous knowledge neck. Such neck models positional information and long-range dependencies among channels through parallel encoding to optimize feature extraction capabilities. Then, we utilize the general distribution to obtain more credible and accurate bounding box estimates. Finally, we employ a novel loss function that makes the network easily concentrate on values near the label to improve learning efficiency. Experiments on four fault datasets reveal that our framework can achieve over 37 frames per second and maintain the highest accuracy in comparison with traditional distillation approaches. Moreover, compared to state-of-the-art methods, our framework demonstrates more competitive performance with lower memory usage and the smallest model size.
</details>
<details>
<summary>摘要</summary>
高效的货运列车缺陷检测是铁路安全运行的关键部分，但深度学习方法在实际工程环境中的应用效率仍然不够高。这篇论文提出了一种多元自适应框架，以确保检测精度和速度同时满足资源限制。我们首先采用了轻量级的背bone来提取特征和生成新的多元知识颈部。这个颈部模型通过并行编码来保持通道之间的位势信息和长距离依赖关系，以便优化特征提取能力。然后，我们使用通用分布来获得更准确和可靠的 bounding box 估计。最后，我们使用一种新的损失函数，让网络更容易集中在标签附近的值上来提高学习效率。实验表明，我们的框架可以在四个缺陷集上达到37帧/秒的速度，并且与传统杜邦方法相比，我们的框架在精度和资源使用量上具有更高的竞争力。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://example.com/2023/07/03/eess.IV_2023_07_03/" data-id="cllsjvzdw007af5882i80h6xm" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_07_02" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/02/cs.LG_2023_07_02/" class="article-date">
  <time datetime="2023-07-01T16:00:00.000Z" itemprop="datePublished">2023-07-02</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/02/cs.LG_2023_07_02/">cs.LG - 2023-07-02 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Protecting-the-Future-Neonatal-Seizure-Detection-with-Spatial-Temporal-Modeling"><a href="#Protecting-the-Future-Neonatal-Seizure-Detection-with-Spatial-Temporal-Modeling" class="headerlink" title="Protecting the Future: Neonatal Seizure Detection with Spatial-Temporal Modeling"></a>Protecting the Future: Neonatal Seizure Detection with Spatial-Temporal Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05382">http://arxiv.org/abs/2307.05382</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyue Li, Yuchen Fang, You Li, Kan Ren, Yansen Wang, Xufang Luo, Juanyong Duan, Congrui Huang, Dongsheng Li, Lili Qiu</li>
<li>for: 旨在提供一种自动化新生儿癫痫识别方法，以替代人工监测。</li>
<li>methods: 使用深度学习框架STATENet，特点包括temporal、 spatial和model层次的精细设计，以便更好地适应新生儿癫痫特点。</li>
<li>results: 实验结果表明，我们的框架在大规模实际新生儿EEG数据集上表现出了显著更高的癫痫识别精度。<details>
<summary>Abstract</summary>
A timely detection of seizures for newborn infants with electroencephalogram (EEG) has been a common yet life-saving practice in the Neonatal Intensive Care Unit (NICU). However, it requires great human efforts for real-time monitoring, which calls for automated solutions to neonatal seizure detection. Moreover, the current automated methods focusing on adult epilepsy monitoring often fail due to (i) dynamic seizure onset location in human brains; (ii) different montages on neonates and (iii) huge distribution shift among different subjects. In this paper, we propose a deep learning framework, namely STATENet, to address the exclusive challenges with exquisite designs at the temporal, spatial and model levels. The experiments over the real-world large-scale neonatal EEG dataset illustrate that our framework achieves significantly better seizure detection performance.
</details>
<details>
<summary>摘要</summary>
新生儿电enzephalogram（EEG）检测是医疗单元（NICU）中一种常见 yet life-saving的做法。然而，这需要大量的人工劳动进行实时监测，因此需要自动化新生儿癫痫检测的解决方案。然而，现有的自动化方法通常因（i）脑动脉癫痫开始的地方不断变化；（ii）新生儿和成人的montage不同；以及（iii）不同个体之间的分布差异而失败。本文提出一种深度学习框架，即状态网络（STATENet），以解决这些独特的挑战。实验表明，我们的框架在大规模实际新生儿EEG数据集上显著提高了癫痫检测性能。
</details></li>
</ul>
<hr>
<h2 id="IoT-Based-Air-Quality-Monitoring-System-with-Machine-Learning-for-Accurate-and-Real-time-Data-Analysis"><a href="#IoT-Based-Air-Quality-Monitoring-System-with-Machine-Learning-for-Accurate-and-Real-time-Data-Analysis" class="headerlink" title="IoT-Based Air Quality Monitoring System with Machine Learning for Accurate and Real-time Data Analysis"></a>IoT-Based Air Quality Monitoring System with Machine Learning for Accurate and Real-time Data Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00580">http://arxiv.org/abs/2307.00580</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Hemanth-Karnati-HK/AQMS-ML">https://github.com/Hemanth-Karnati-HK/AQMS-ML</a></li>
<li>paper_authors: Hemanth Karnati</li>
<li>for:  addresses the issue of air pollution awareness in urban areas</li>
<li>methods:  uses two sensors (MQ135 and MQ3) to detect harmful gases and measure air quality in PPM, and employs machine learning analysis on the collected data</li>
<li>results:  provides real-time data specific to the user’s location, and visualizes the data using a cloud-based web app called ThinkSpeak<details>
<summary>Abstract</summary>
Air pollution in urban areas has severe consequences for both human health and the environment, predominantly caused by exhaust emissions from vehicles. To address the issue of air pollution awareness, Air Pollution Monitoring systems are used to measure the concentration of gases like CO2, smoke, alcohol, benzene, and NH3 present in the air. However, current mobile applications are unable to provide users with real-time data specific to their location. In this paper, we propose the development of a portable air quality detection device that can be used anywhere. The data collected will be stored and visualized using the cloud-based web app ThinkSpeak.   The device utilizes two sensors, MQ135 and MQ3, to detect harmful gases and measure air quality in parts per million (PPM). Additionally, machine learning analysis will be employed on the collected data.
</details>
<details>
<summary>摘要</summary>
城市空气污染造成人体健康和环境问题严重，主要由交通工具排放的废气引起。为解决空气污染意识问题，空气污染监测系统用于测量空气中的气体 like CO2、烟雾、酒精、苯并丙烯的浓度。但现有 mobil 应用程序无法为用户提供实时特定位置的数据。在这篇论文中，我们提议开发一种可携带的空气质量检测设备，可以在任何地方使用。收集的数据将被存储并视觉化使用云端的网站 ThinkSpeak。 该设备使用 MQ135 和 MQ3 两种传感器检测危险气体，并测量空气质量为每万分之一（PPM）。此外，我们还将机器学习分析集成到收集的数据中。
</details></li>
</ul>
<hr>
<h2 id="Mode-wise-Principal-Subspace-Pursuit-and-Matrix-Spiked-Covariance-Model"><a href="#Mode-wise-Principal-Subspace-Pursuit-and-Matrix-Spiked-Covariance-Model" class="headerlink" title="Mode-wise Principal Subspace Pursuit and Matrix Spiked Covariance Model"></a>Mode-wise Principal Subspace Pursuit and Matrix Spiked Covariance Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00575">http://arxiv.org/abs/2307.00575</a></li>
<li>repo_url: None</li>
<li>paper_authors: Runshi Tang, Ming Yuan, Anru R. Zhang</li>
<li>for: 这个论文提出了一种新的框架，叫做模式精炼主要特征追踪（MOP-UP），用于抽取矩阵数据中隐藏的变化。</li>
<li>methods: 该algorithm consists of two steps: Average Subspace Capture (ASC) and Alternating Projection (AP). These steps are specifically designed to capture the row-wise and column-wise dimension-reduced subspaces which contain the most informative features of the data.</li>
<li>results: The proposed framework is demonstrated to be effective through experiments on both simulated and real datasets, and the authors also discuss generalizations of their approach to higher-order data.<details>
<summary>Abstract</summary>
This paper introduces a novel framework called Mode-wise Principal Subspace Pursuit (MOP-UP) to extract hidden variations in both the row and column dimensions for matrix data. To enhance the understanding of the framework, we introduce a class of matrix-variate spiked covariance models that serve as inspiration for the development of the MOP-UP algorithm. The MOP-UP algorithm consists of two steps: Average Subspace Capture (ASC) and Alternating Projection (AP). These steps are specifically designed to capture the row-wise and column-wise dimension-reduced subspaces which contain the most informative features of the data. ASC utilizes a novel average projection operator as initialization and achieves exact recovery in the noiseless setting. We analyze the convergence and non-asymptotic error bounds of MOP-UP, introducing a blockwise matrix eigenvalue perturbation bound that proves the desired bound, where classic perturbation bounds fail. The effectiveness and practical merits of the proposed framework are demonstrated through experiments on both simulated and real datasets. Lastly, we discuss generalizations of our approach to higher-order data.
</details>
<details>
<summary>摘要</summary>
MOP-UP consists of two steps: Average Subspace Capture (ASC) and Alternating Projection (AP). These steps are designed to capture the row-wise and column-wise dimension-reduced subspaces that contain the most informative features of the data. ASC uses a novel average projection operator as initialization and can achieve exact recovery in the noiseless setting.We analyze the convergence and non-asymptotic error bounds of MOP-UP, using a blockwise matrix eigenvalue perturbation bound that proves the desired bound, where classic perturbation bounds fail. The effectiveness and practical merits of the proposed framework are demonstrated through experiments on both simulated and real datasets. Finally, we discuss generalizations of our approach to higher-order data.
</details></li>
</ul>
<hr>
<h2 id="Conditionally-Invariant-Representation-Learning-for-Disentangling-Cellular-Heterogeneity"><a href="#Conditionally-Invariant-Representation-Learning-for-Disentangling-Cellular-Heterogeneity" class="headerlink" title="Conditionally Invariant Representation Learning for Disentangling Cellular Heterogeneity"></a>Conditionally Invariant Representation Learning for Disentangling Cellular Heterogeneity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00558">http://arxiv.org/abs/2307.00558</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hananeh Aliee, Ferdinand Kapl, Soroor Hediyeh-Zadeh, Fabian J. Theis</li>
<li>for: 这 paper 的目的是学习受到不想要的变化的表示，通过强制独立性来消除噪声，并建立可解释的模型。</li>
<li>methods: 该方法利用域变化来学习受到不想要的变化的表示，并在满足条件下强制独立性，以便构建可解释的模型。</li>
<li>results: 该方法可以在大规模的单元细胞 genomics 数据中实现数据集 Integration，并提供更深刻的理解单元细胞多样性和疾病细胞状态的能力。<details>
<summary>Abstract</summary>
This paper presents a novel approach that leverages domain variability to learn representations that are conditionally invariant to unwanted variability or distractors. Our approach identifies both spurious and invariant latent features necessary for achieving accurate reconstruction by placing distinct conditional priors on latent features. The invariant signals are disentangled from noise by enforcing independence which facilitates the construction of an interpretable model with a causal semantic. By exploiting the interplay between data domains and labels, our method simultaneously identifies invariant features and builds invariant predictors. We apply our method to grand biological challenges, such as data integration in single-cell genomics with the aim of capturing biological variations across datasets with many samples, obtained from different conditions or multiple laboratories. Our approach allows for the incorporation of specific biological mechanisms, including gene programs, disease states, or treatment conditions into the data integration process, bridging the gap between the theoretical assumptions and real biological applications. Specifically, the proposed approach helps to disentangle biological signals from data biases that are unrelated to the target task or the causal explanation of interest. Through extensive benchmarking using large-scale human hematopoiesis and human lung cancer data, we validate the superiority of our approach over existing methods and demonstrate that it can empower deeper insights into cellular heterogeneity and the identification of disease cell states.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Partial-label-Learning-with-Mixed-Closed-set-and-Open-set-Out-of-candidate-Examples"><a href="#Partial-label-Learning-with-Mixed-Closed-set-and-Open-set-Out-of-candidate-Examples" class="headerlink" title="Partial-label Learning with Mixed Closed-set and Open-set Out-of-candidate Examples"></a>Partial-label Learning with Mixed Closed-set and Open-set Out-of-candidate Examples</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00553">http://arxiv.org/abs/2307.00553</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuo He, Lei Feng, Guowu Yang</li>
<li>for: 本研究旨在解决部分标签学习中的一种限制性假设问题，即训练示例的真实标签必须在候选标签集中。</li>
<li>methods: 本研究使用了两种类型的假设例子，即闭集&#x2F;开集假设例子，其中true标签在知道标签空间内&#x2F;外。为解决这个新的部分标签学习问题，我们首先计算了木头克朗径分布损失函数，并根据特制的标签分类器来动态地区分两种假设例子。对于闭集假设例子，我们进行了反向标签混淆处理；对于开集假设例子，我们利用了一种有效的规范策略，在候选标签集中随机分配了假设标签。这样，两种假设例子都可以得到分化并利用于模型训练。</li>
<li>results: 我们的提议方法与现有的部分标签学习方法进行比较，实验结果显示，我们的方法在训练集中表现更好。<details>
<summary>Abstract</summary>
Partial-label learning (PLL) relies on a key assumption that the true label of each training example must be in the candidate label set. This restrictive assumption may be violated in complex real-world scenarios, and thus the true label of some collected examples could be unexpectedly outside the assigned candidate label set. In this paper, we term the examples whose true label is outside the candidate label set OOC (out-of-candidate) examples, and pioneer a new PLL study to learn with OOC examples. We consider two types of OOC examples in reality, i.e., the closed-set/open-set OOC examples whose true label is inside/outside the known label space. To solve this new PLL problem, we first calculate the wooden cross-entropy loss from candidate and non-candidate labels respectively, and dynamically differentiate the two types of OOC examples based on specially designed criteria. Then, for closed-set OOC examples, we conduct reversed label disambiguation in the non-candidate label set; for open-set OOC examples, we leverage them for training by utilizing an effective regularization strategy that dynamically assigns random candidate labels from the candidate label set. In this way, the two types of OOC examples can be differentiated and further leveraged for model training. Extensive experiments demonstrate that our proposed method outperforms state-of-the-art PLL methods.
</details>
<details>
<summary>摘要</summary>
假设学习（Partial-label learning，PLL）基于一个关键假设，即每个训练示例的真实标签必须在候选标签集中。然而，在复杂的实际场景中，这个假设可能会被违反，有些收集的示例的真实标签可能会不期望地外部候选标签集。在这篇论文中，我们称这些示例为“外部候选标签示例”（OOC，out-of-candidate），并开拓了一种新的PLL研究，以学习与OOC示例。我们在实际中考虑了两种类型的OOC示例，即关闭集/开放集OOC示例，其中true标签在知道的标签空间内/外。为解决这个新的PLL问题，我们首先计算了木材cross-entropy损失从候选标签和非候选标签分别来，然后通过特殊设计的标准来动态分类OOC示例。对关闭集OOC示例，我们进行了反向标签混淆在非候选标签集中；对开放集OOC示例，我们利用它们进行训练，通过使用一种有效的补偿策略，动态将候选标签集中的随机标签分配给OOC示例。这种方法可以区分和利用两种类型的OOC示例进行训练。我们的提议方法在现有的PLL方法之上具有优异性。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-reinforcement-learning-of-multi-agent-ethically-aligned-behaviours-the-QSOM-and-QDSOM-algorithms"><a href="#Adaptive-reinforcement-learning-of-multi-agent-ethically-aligned-behaviours-the-QSOM-and-QDSOM-algorithms" class="headerlink" title="Adaptive reinforcement learning of multi-agent ethically-aligned behaviours: the QSOM and QDSOM algorithms"></a>Adaptive reinforcement learning of multi-agent ethically-aligned behaviours: the QSOM and QDSOM algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00552">http://arxiv.org/abs/2307.00552</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rémy Chaput, Olivier Boissier, Mathieu Guillermin</li>
<li>for: 这篇论文主要是为了解决人工智能系统的伦理考虑问题，特别是随着时间的推移，我们社会的价值观念不断发展，这使得现有的AI系统困难以适应。</li>
<li>methods: 该论文提出了两种算法，即QSOM和QDSOM，它们可以适应环境和奖励函数的变化，以实现AI系统的伦理考虑。它们使用了知名的Q表格和动态自组织地图来处理连续和多维状态和动作空间。</li>
<li>results: 作者在一个小型智能网格中进行了多代理能量分配的用例，并证明了QSOM和QDSOM算法的适应性和比基eline Reinforcement Learning算法的高性能。<details>
<summary>Abstract</summary>
The numerous deployed Artificial Intelligence systems need to be aligned with our ethical considerations. However, such ethical considerations might change as time passes: our society is not fixed, and our social mores evolve. This makes it difficult for these AI systems; in the Machine Ethics field especially, it has remained an under-studied challenge. In this paper, we present two algorithms, named QSOM and QDSOM, which are able to adapt to changes in the environment, and especially in the reward function, which represents the ethical considerations that we want these systems to be aligned with. They associate the well-known Q-Table to (Dynamic) Self-Organizing Maps to handle the continuous and multi-dimensional state and action spaces. We evaluate them on a use-case of multi-agent energy repartition within a small Smart Grid neighborhood, and prove their ability to adapt, and their higher performance compared to baseline Reinforcement Learning algorithms.
</details>
<details>
<summary>摘要</summary>
各种已经部署的人工智能系统需要与我们的道德考虑相协调。然而，这些道德考虑可能随着时间的推移而发生变化：我们的社会不固定，我们的社会习俗也在不断发展。这会让这些 AI 系统受到挑战：在机器伦理学领域，这是一个未得到充分研究的挑战。在这篇论文中，我们提出了两种算法，即 QSOM 和 QDSOM，它们可以适应环境的变化，特别是奖励函数的变化，这些奖励函数表达我们想要这些系统遵循的道德考虑。它们将知名的 Q-表与（动态）自组织地图相结合，以处理连续和多维状态和动作空间。我们在一个小型智能网格社区中进行了多机器人能源分配的使用案例研究，并证明它们的适应性和与基准 Reinforcement Learning 算法相比的高性能。
</details></li>
</ul>
<hr>
<h2 id="Is-Risk-Sensitive-Reinforcement-Learning-Properly-Resolved"><a href="#Is-Risk-Sensitive-Reinforcement-Learning-Properly-Resolved" class="headerlink" title="Is Risk-Sensitive Reinforcement Learning Properly Resolved?"></a>Is Risk-Sensitive Reinforcement Learning Properly Resolved?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00547">http://arxiv.org/abs/2307.00547</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruiwen Zhou, Minghuan Liu, Kan Ren, Xufang Luo, Weinan Zhang, Dongsheng Li</li>
<li>for: 本研究旨在解决分布式强化学习中的风险敏感问题，提出了一种新的算法，即轨迹Q学习（TQL），以便在风险敏感目标下优化做法。</li>
<li>methods: 本研究使用了分布式强化学习框架，学习风险敏感目标函数，并提出了一种新的学习算法，即TQL，可以在不同的风险度量下学习不同的风险敏感策略。</li>
<li>results: 实验结果表明，TQL算法可以有效地实现风险敏感目标，并且可以在不同的风险度量下实现更好的性能。<details>
<summary>Abstract</summary>
Due to the nature of risk management in learning applicable policies, risk-sensitive reinforcement learning (RSRL) has been realized as an important direction. RSRL is usually achieved by learning risk-sensitive objectives characterized by various risk measures, under the framework of distributional reinforcement learning. However, it remains unclear if the distributional Bellman operator properly optimizes the RSRL objective in the sense of risk measures. In this paper, we prove that the existing RSRL methods do not achieve unbiased optimization and can not guarantee optimality or even improvements regarding risk measures over accumulated return distributions. To remedy this issue, we further propose a novel algorithm, namely Trajectory Q-Learning (TQL), for RSRL problems with provable convergence to the optimal policy. Based on our new learning architecture, we are free to introduce a general and practical implementation for different risk measures to learn disparate risk-sensitive policies. In the experiments, we verify the learnability of our algorithm and show how our method effectively achieves better performances toward risk-sensitive objectives.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)由于学习适用政策的风险管理性质，风险敏感回归学习（RSRL）已成为重要的方向。RSRL通常通过学习不同的风险度量来定义风险敏感目标，在分布式回归学习框架下进行学习。然而，是否存在分布式贝尔曼算子可以正确优化RSRL目标还存在unclear。在这篇论文中，我们证明现有的RSRL方法不能够实现不偏优化和 garantía优化或even improvements regarding风险度量 надaccumulated return分布。为了解决这个问题，我们进一步提出了一种新的算法，即轨迹Q学习（TQL），用于RSRL问题，并证明其可提供可靠的优化方案。基于我们的新学习架构，我们可以自由地引入不同的风险度量来学习不同的风险敏感政策。在实验中，我们验证了我们的算法的学习可能性并显示了我们的方法可以更好地实现风险敏感目标。
</details></li>
</ul>
<hr>
<h2 id="Defending-Against-Malicious-Behaviors-in-Federated-Learning-with-Blockchain"><a href="#Defending-Against-Malicious-Behaviors-in-Federated-Learning-with-Blockchain" class="headerlink" title="Defending Against Malicious Behaviors in Federated Learning with Blockchain"></a>Defending Against Malicious Behaviors in Federated Learning with Blockchain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00543">http://arxiv.org/abs/2307.00543</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nanqing Dong, Zhipeng Wang, Jiahao Sun, Michael Kampffmeyer, Yizhe Wen, Shuoying Zhang, William Knottenbelt, Eric Xing</li>
<li>for: 提高 federated learning 系统的安全性和可靠性，使其更易承受多个机构数据拥有者（客户）之间的合作训练。</li>
<li>methods: 基于区块链和分布式笔记技术，提出一种安全可靠的 federated learning 系统，包括对接触 peer-to-peer 投票机制和奖励折损机制，以检测和抵制恶意客户端行为。</li>
<li>results: 通过理论和实验分析，证明提出的方法可以具有高效性和可靠性，并能够抵制恶意客户端的行为。<details>
<summary>Abstract</summary>
In the era of deep learning, federated learning (FL) presents a promising approach that allows multi-institutional data owners, or clients, to collaboratively train machine learning models without compromising data privacy. However, most existing FL approaches rely on a centralized server for global model aggregation, leading to a single point of failure. This makes the system vulnerable to malicious attacks when dealing with dishonest clients. In this work, we address this problem by proposing a secure and reliable FL system based on blockchain and distributed ledger technology. Our system incorporates a peer-to-peer voting mechanism and a reward-and-slash mechanism, which are powered by on-chain smart contracts, to detect and deter malicious behaviors. Both theoretical and empirical analyses are presented to demonstrate the effectiveness of the proposed approach, showing that our framework is robust against malicious client-side behaviors.
</details>
<details>
<summary>摘要</summary>
在深度学习时代，联邦学习（FL）表现出了一种有前途的方法，允许多个机构数据所有者或客户端共同训练机器学习模型，无需违反数据隐私。然而，大多数现有FL方法仍然依赖中央服务器进行全球模型集成，导致单点失败的风险。这使得系统易受到不良客户端行为的攻击。在这种情况下，我们解决这个问题，提出一个安全可靠的FL系统，基于区块链和分布式记录技术。我们的系统包括一个点对点投票机制和一个奖励惩罚机制，这些机制由onto-chain智能合约动力，用于检测和抵制不良客户端行为。我们提供了理论和实验分析，证明我们的框架对客户端不良行为具有耐诡性。
</details></li>
</ul>
<hr>
<h2 id="Collaborative-Policy-Learning-for-Dynamic-Scheduling-Tasks-in-Cloud-Edge-Terminal-IoT-Networks-Using-Federated-Reinforcement-Learning"><a href="#Collaborative-Policy-Learning-for-Dynamic-Scheduling-Tasks-in-Cloud-Edge-Terminal-IoT-Networks-Using-Federated-Reinforcement-Learning" class="headerlink" title="Collaborative Policy Learning for Dynamic Scheduling Tasks in Cloud-Edge-Terminal IoT Networks Using Federated Reinforcement Learning"></a>Collaborative Policy Learning for Dynamic Scheduling Tasks in Cloud-Edge-Terminal IoT Networks Using Federated Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00541">http://arxiv.org/abs/2307.00541</a></li>
<li>repo_url: None</li>
<li>paper_authors: Do-Yup Kim, Da-Eun Lee, Ji-Wan Kim, Hyun-Suk Lee</li>
<li>for: 这篇论文研究了云端-边缘-终端互联网络，其中边缘设备进行了一系列通常的动态调度任务。</li>
<li>methods: 该论文提出了一种基于联邦 reinforcement learning 的共同策略学习框架，用于动态调度任务。该框架可以在云服务器上协同学习每个任务的中央策略，并且可以通过Edge在每个任务上学习本地策略，以避免Edge需要从头开始学习策略。</li>
<li>results: 通过实验，论文表明，相比没有共同策略学习的方法，该框架能够加速策略学习速度，并且帮助新到达的Edge更容易适应其任务。<details>
<summary>Abstract</summary>
In this paper, we examine cloud-edge-terminal IoT networks, where edges undertake a range of typical dynamic scheduling tasks. In these IoT networks, a central policy for each task can be constructed at a cloud server. The central policy can be then used by the edges conducting the task, thereby mitigating the need for them to learn their own policy from scratch. Furthermore, this central policy can be collaboratively learned at the cloud server by aggregating local experiences from the edges, thanks to the hierarchical architecture of the IoT networks. To this end, we propose a novel collaborative policy learning framework for dynamic scheduling tasks using federated reinforcement learning. For effective learning, our framework adaptively selects the tasks for collaborative learning in each round, taking into account the need for fairness among tasks. In addition, as a key enabler of the framework, we propose an edge-agnostic policy structure that enables the aggregation of local policies from different edges. We then provide the convergence analysis of the framework. Through simulations, we demonstrate that our proposed framework significantly outperforms the approaches without collaborative policy learning. Notably, it accelerates the learning speed of the policies and allows newly arrived edges to adapt to their tasks more easily.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了云端-边缘-终端互联网络，其中边缘进行了一系列典型的动态调度任务。在这些互联网络中，可以在云服务器上构建一个中央政策 для每个任务。这个中央政策可以由边缘进行调度任务使用，从而减少边缘需要从零开始学习自己的策略。此外，这个中央政策可以在云服务器上协同学习，通过累累的结构来汇集不同边缘的地方经验。为了实现这一目标，我们提出了一种基于联邦束力学学习的共同策略学习框架。为了有效学习，我们的框架在每次轮次中适时选择需要协同学习的任务，考虑到任务之间的公平性。此外，我们还提出了一种不受边缘影响的策略结构，以便将不同边缘的本地策略集成。然后，我们提供了框架的收敛分析。通过实验，我们证明了我们提出的框架可以快速学习策略，并且新到达的边缘可以更容易适应自己的任务。
</details></li>
</ul>
<hr>
<h2 id="Shared-Growth-of-Graph-Neural-Networks-via-Free-direction-Knowledge-Distillation"><a href="#Shared-Growth-of-Graph-Neural-Networks-via-Free-direction-Knowledge-Distillation" class="headerlink" title="Shared Growth of Graph Neural Networks via Free-direction Knowledge Distillation"></a>Shared Growth of Graph Neural Networks via Free-direction Knowledge Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00534">http://arxiv.org/abs/2307.00534</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaituo Feng, Yikun Miao, Changsheng Li, Ye Yuan, Guoren Wang</li>
<li>for: 提高Graph Neural Networks（GNNs）的性能，避免过参数和过拟合问题。</li>
<li>methods: 提出了首个基于强化学习的Free-direction Knowledge Distillation框架（FreeKD），不再需要提供一个很深和优化的教师GNN。我们的核心想法是通过层次的强化学习来让两个相对较浅的GNN Collaboratively学习，以便在层次上交换知识。我们观察到，一个典型的GNN模型在训练中常常在不同的节点上展现出不同的性能，所以我们设计了一种动态和自由方向的知识传递策略，其中包括两个层次的操作：1）节点级别的操作确定了两个网络中相应节点之间的知识传递方向; 2）结构级别的操作确定了哪些本地结构由节点级别的操作生成的知识传递。</li>
<li>results: 对五个标准 benchmark dataset进行了广泛的实验，并显示了我们的方法可以大幅提高基GNN的性能，并且可以适应不同的GNN模型。最后，我们还提出了FreeKD++，可以在多视图输入下实现自由方向知识传递。<details>
<summary>Abstract</summary>
Knowledge distillation (KD) has shown to be effective to boost the performance of graph neural networks (GNNs), where the typical objective is to distill knowledge from a deeper teacher GNN into a shallower student GNN. However, it is often quite challenging to train a satisfactory deeper GNN due to the well-known over-parametrized and over-smoothing issues, leading to invalid knowledge transfer in practical applications. In this paper, we propose the first Free-direction Knowledge Distillation framework via reinforcement learning for GNNs, called FreeKD, which is no longer required to provide a deeper well-optimized teacher GNN. Our core idea is to collaboratively learn two shallower GNNs in an effort to exchange knowledge between them via reinforcement learning in a hierarchical way. As we observe that one typical GNN model often exhibits better and worse performances at different nodes during training, we devise a dynamic and free-direction knowledge transfer strategy that involves two levels of actions: 1) node-level action determines the directions of knowledge transfer between the corresponding nodes of two networks; and then 2) structure-level action determines which of the local structures generated by the node-level actions to be propagated. Furthermore, considering the diverse knowledge present in different GNNs when dealing with multi-view inputs, we introduce FreeKD++ as a solution to enable free-direction knowledge transfer among multiple shallow GNNs operating on multi-view inputs. Extensive experiments on five benchmark datasets demonstrate our approaches outperform the base GNNs in a large margin, and shows their efficacy to various GNNs. More surprisingly, our FreeKD has comparable or even better performance than traditional KD algorithms that distill knowledge from a deeper and stronger teacher GNN.
</details>
<details>
<summary>摘要</summary>
知识塑化（KD）已经证明可以提高图内 нейрон网络（GNN）的性能，通常的目标是将深度更大的教师GNN中的知识透传到更浅的学生GNN中。然而，在实际应用中，往往很难训练一个满意的深度更大的GNN，因为图内神经网络容易受到过参数和过缓态问题的影响，从而导致无效的知识传递。在这篇论文中，我们提出了首个基于奖励学习的自由方向知识塑化框架 для GNN，称为FreeKD。我们的核心想法是通过奖励学习的方式，将两个更浅的GNN合作学习，以便在其中交换知识。我们发现，一个典型的GNN模型在训练中经常在不同的节点表现出不同的性能，因此我们设计了一种动态和自由方向的知识传递策略，其中包括两个层次的动作：1）节点级别的动作确定了两个网络中对应节点之间的知识传递方向; 然后2）结构级别的动作确定了哪些本地结构，由节点级别的动作生成的。此外，当处理多视图输入时，我们引入FreeKD++，以允许多个浅GNN之间自由地进行知识传递。我们的方法在五个基准数据集上进行了广泛的实验，并证明了我们的方法在基GNN上大幅提高性能，并且可以适应不同的GNN模型。更重要的是，我们的FreeKD与传统的KD算法相比，在提取深度更大的教师GNN中的知识时，表现相对或甚至更好。
</details></li>
</ul>
<hr>
<h2 id="New-intelligent-defense-systems-to-reduce-the-risks-of-Selfish-Mining-and-Double-Spending-attacks-using-Learning-Automata"><a href="#New-intelligent-defense-systems-to-reduce-the-risks-of-Selfish-Mining-and-Double-Spending-attacks-using-Learning-Automata" class="headerlink" title="New intelligent defense systems to reduce the risks of Selfish Mining and Double-Spending attacks using Learning Automata"></a>New intelligent defense systems to reduce the risks of Selfish Mining and Double-Spending attacks using Learning Automata</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00529">http://arxiv.org/abs/2307.00529</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seyed Ardalan Ghoreishi, Mohammad Reza Meybodi</li>
<li>for: 这 paper addresses the double-spending and selfish mining attacks challenges in blockchain-based digital currencies.</li>
<li>methods: 该 paper 提出了一种新的攻击方法， combinining double-spending 和 selfish mining attacks，并提出了一种基于机器学习的解决方案。 Specifically, 使用 learning automaton 来开发两种模型， namely SDTLA 和 WVBM，可以有效地防止自ish mining attacks。</li>
<li>results: 实验结果表明， SDTLA 方法可以提高自ish mining 的利润阈值达到 47%，而 WVBM 方法在许多情况下几乎达到理想情况，即每个矿工的收益与其分配的 hash 处理能力成正比。 此外，两种方法都可以有效地减少 double-spending 的风险。<details>
<summary>Abstract</summary>
In this paper, we address the critical challenges of double-spending and selfish mining attacks in blockchain-based digital currencies. Double-spending is a problem where the same tender is spent multiple times during a digital currency transaction, while selfish mining is an intentional alteration of a blockchain to increase rewards to one miner or a group of miners. We introduce a new attack that combines both these attacks and propose a machine learning-based solution to mitigate the risks associated with them. Specifically, we use the learning automaton, a powerful online learning method, to develop two models, namely the SDTLA and WVBM, which can effectively defend against selfish mining attacks. Our experimental results show that the SDTLA method increases the profitability threshold of selfish mining up to 47$\%$, while the WVBM method performs even better and is very close to the ideal situation where each miner's revenue is proportional to their shared hash processing power. Additionally, we demonstrate that both methods can effectively reduce the risks of double-spending by tuning the $Z$ Parameter. Our findings highlight the potential of SDTLA and WVBM as promising solutions for enhancing the security and efficiency of blockchain networks.
</details>
<details>
<summary>摘要</summary>
在本文中，我们讨论了区块链基于货币的双重支付和自私采矿攻击的关键挑战。双重支付是一个情况下，同一笔货币在数字货币交易中被使用多次，而自私采矿是故意修改区块链，以增加采矿奖励的情况。我们介绍了一种新的攻击，该攻击组合了这两种攻击，并提出了基于机器学习的解决方案。 Specifically，我们使用学习自动机，一种强大的在线学习方法，开发了两种模型， namely SDTLA和WVBM，以有效防止自私采矿攻击。我们的实验结果表明，SDTLA方法可以提高自私采矿的利润阈值，最高达47%；WVBM方法表现更好，与每个矿工的分配 hash 处理能力相似。此外，我们证明了这两种方法可以有效降低双重支付的风险，通过调整 $Z$ 参数。我们的发现表明 SDTLA 和 WVBM 是加强区块链网络安全性和效率的有力解决方案。
</details></li>
</ul>
<hr>
<h2 id="Graph-Neural-Network-based-Log-Anomaly-Detection-and-Explanation"><a href="#Graph-Neural-Network-based-Log-Anomaly-Detection-and-Explanation" class="headerlink" title="Graph Neural Network based Log Anomaly Detection and Explanation"></a>Graph Neural Network based Log Anomaly Detection and Explanation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00527">http://arxiv.org/abs/2307.00527</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhong Li, Jiayang Shi, Matthijs van Leeuwen</li>
<li>for: 该研究旨在提高高科技系统监控中的日志异常检测精度，使用图形模型来检测日志异常。</li>
<li>methods: 该方法首先将日志事件转换为有Attributes、指向和权重的图形模型，然后使用图形神经网络进行图形级别异常检测。</li>
<li>results: 实验结果显示，Logs2Graphs在五个基准数据集上表现至少与现状异常检测方法相当，而在复杂数据集上大量超过现状异常检测方法的性能。<details>
<summary>Abstract</summary>
Event logs are widely used to record the status of high-tech systems, making log anomaly detection important for monitoring those systems. Most existing log anomaly detection methods take a log event count matrix or log event sequences as input, exploiting quantitative and/or sequential relationships between log events to detect anomalies. Unfortunately, only considering quantitative or sequential relationships may result in many false positives and/or false negatives. To alleviate this problem, we propose a graph-based method for unsupervised log anomaly detection, dubbed Logs2Graphs, which first converts event logs into attributed, directed, and weighted graphs, and then leverages graph neural networks to perform graph-level anomaly detection. Specifically, we introduce One-Class Digraph Inception Convolutional Networks, abbreviated as OCDiGCN, a novel graph neural network model for detecting graph-level anomalies in a collection of attributed, directed, and weighted graphs. By coupling the graph representation and anomaly detection steps, OCDiGCN can learn a representation that is especially suited for anomaly detection, resulting in a high detection accuracy. Importantly, for each identified anomaly, we additionally provide a small subset of nodes that play a crucial role in OCDiGCN's prediction as explanations, which can offer valuable cues for subsequent root cause diagnosis. Experiments on five benchmark datasets show that Logs2Graphs performs at least on par state-of-the-art log anomaly detection methods on simple datasets while largely outperforming state-of-the-art log anomaly detection methods on complicated datasets.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate the following text into Simplified Chinese<</SYS>>高科技系统的事件日志广泛应用，因此对事件日志异常检测非常重要。现有的大多数异常检测方法都是根据事件日志的数量或Sequential关系进行检测，但是只考虑数量或Sequential关系可能会导致许多假阳性和/或假阴性。为了解决这个问题，我们提出了一种基于图的方法，名为Logs2Graphs，它首先将事件日志转换为有 Attribute、指向和权重的图，然后利用图神经网络进行图级异常检测。我们引入了一种novel的图神经网络模型，称为One-Class Digraph Inception Convolutional Networks（简称OCDiGCN），可以在一个集合 Attribute、指向和权重的图中检测图级异常。通过将图表示和异常检测步骤结合起来，OCDiGCN可以学习一种特别适合异常检测的表示，从而实现高的检测精度。此外，对每个异常检测结果，我们还提供了一小 subsets of nodes，这些 nodes 在 OCDiGCN 的预测中发挥了关键作用，这些 nodes 可以提供有价值的诊断灵感。在五个 benchmark 数据集上进行了实验，Logs2Graphs 与 state-of-the-art 异常检测方法在简单的数据集上表现至少与state-of-the-art，而在复杂的数据集上则广泛超越 state-of-the-art。
</details></li>
</ul>
<hr>
<h2 id="TensorGPT-Efficient-Compression-of-the-Embedding-Layer-in-LLMs-based-on-the-Tensor-Train-Decomposition"><a href="#TensorGPT-Efficient-Compression-of-the-Embedding-Layer-in-LLMs-based-on-the-Tensor-Train-Decomposition" class="headerlink" title="TensorGPT: Efficient Compression of the Embedding Layer in LLMs based on the Tensor-Train Decomposition"></a>TensorGPT: Efficient Compression of the Embedding Layer in LLMs based on the Tensor-Train Decomposition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00526">http://arxiv.org/abs/2307.00526</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingxue Xu, Yao Lei Xu, Danilo P. Mandic</li>
<li>For: 提高 Large Language Models (LLMs) 的性能和压缩硬件需求* Methods: 使用 Tensor-Train Decomposition (TTD) 将每个token embedding treated as a Matrix Product State (MPS)，可以有效地在分布式环境中计算* Results: 对 GPT-2 进行实验，可以压缩 embedding layer 38.40 倍，并且当压缩因子为 3.31 倍时，甚至超过原始 GPT-2 模型的性能。<details>
<summary>Abstract</summary>
High-dimensional token embeddings underpin Large Language Models (LLMs), as they can capture subtle semantic information and significantly enhance the modelling of complex language patterns. However, the associated high dimensionality also introduces considerable model parameters, and a prohibitively high model storage. To address this issue, this work proposes an approach based on the Tensor-Train Decomposition (TTD), where each token embedding is treated as a Matrix Product State (MPS) that can be efficiently computed in a distributed manner. The experimental results on GPT-2 demonstrate that, through our approach, the embedding layer can be compressed by a factor of up to 38.40 times, and when the compression factor is 3.31 times, even produced a better performance than the original GPT-2 model.
</details>
<details>
<summary>摘要</summary>
高维度Token embedding在大型语言模型（LLM）中扮演着关键角色，它们可以捕捉语言表达中的细微含义并明显提高复杂语言模式的模型化。然而，相关的高维度也导致了较大的模型参数和庞大的模型存储。为解决这问题，本研究提出了基于张量积 Train Decomposition（TTD）的方法，其中每个Token embedding被视为一个Matrix Product State（MPS），可以有效地在分布式环境中计算。实验结果表明，通过我们的方法，Embedding层可以被压缩38.40倍，并且当压缩因子为3.31倍时，甚至超越了原始GPT-2模型的性能。
</details></li>
</ul>
<hr>
<h2 id="LEDITS-Real-Image-Editing-with-DDPM-Inversion-and-Semantic-Guidance"><a href="#LEDITS-Real-Image-Editing-with-DDPM-Inversion-and-Semantic-Guidance" class="headerlink" title="LEDITS: Real Image Editing with DDPM Inversion and Semantic Guidance"></a>LEDITS: Real Image Editing with DDPM Inversion and Semantic Guidance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00522">http://arxiv.org/abs/2307.00522</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/adham-elarabawy/ledits">https://github.com/adham-elarabawy/ledits</a></li>
<li>paper_authors: Linoy Tsaban, Apolinário Passos</li>
<li>for: 本文旨在提出一种轻量级的真实图像编辑方法，以便使用文本来修改图像，而不需要更改模型结构。</li>
<li>methods: 本方法利用 Edit Friendly DDPM 倒推技术和 semantic guidance，将图像倒推到 DDPM 模型的预训练空间中，然后通过文本提示进行修改。</li>
<li>results: 本方法可以实现高质量的图像修改，包括细微和广泛的修改、组合和风格的变化，而无需进行优化或扩展模型结构。<details>
<summary>Abstract</summary>
Recent large-scale text-guided diffusion models provide powerful image-generation capabilities. Currently, a significant effort is given to enable the modification of these images using text only as means to offer intuitive and versatile editing. However, editing proves to be difficult for these generative models due to the inherent nature of editing techniques, which involves preserving certain content from the original image. Conversely, in text-based models, even minor modifications to the text prompt frequently result in an entirely distinct result, making attaining one-shot generation that accurately corresponds to the users intent exceedingly challenging. In addition, to edit a real image using these state-of-the-art tools, one must first invert the image into the pre-trained models domain - adding another factor affecting the edit quality, as well as latency. In this exploratory report, we propose LEDITS - a combined lightweight approach for real-image editing, incorporating the Edit Friendly DDPM inversion technique with Semantic Guidance, thus extending Semantic Guidance to real image editing, while harnessing the editing capabilities of DDPM inversion as well. This approach achieves versatile edits, both subtle and extensive as well as alterations in composition and style, while requiring no optimization nor extensions to the architecture.
</details>
<details>
<summary>摘要</summary>
现代大规模文本导向扩散模型提供了强大的图像生成能力。当前，许多努力是为了通过文本来修改这些图像，以便提供直观和灵活的编辑。然而，编辑 proves to be difficult for these generative models due to the inherent nature of editing techniques, which involves preserving certain content from the original image。 conversely, in text-based models, even minor modifications to the text prompt frequently result in an entirely distinct result, making one-shot generation that accurately corresponds to the user's intent exceedingly challenging。 In addition, to edit a real image using these state-of-the-art tools, one must first invert the image into the pre-trained models domain - adding another factor affecting the edit quality, as well as latency。在这份探索报告中，我们提出了 LEDITS - 一种轻量级的方法 для真实图像编辑，将 Edit Friendly DDPM 抽象技术与semantic guidance相结合，从而将semantic guidance扩展到真实图像编辑，同时利用 DDPM 抽象技术的编辑能力。这种方法可以实现多样化的修改，包括细微和广泛的修改，以及修改图像的结构和风格。此外，这种方法不需要扩展 nor optimization to the architecture。
</details></li>
</ul>
<hr>
<h2 id="DSTCGCN-Learning-Dynamic-Spatial-Temporal-Cross-Dependencies-for-Traffic-Forecasting"><a href="#DSTCGCN-Learning-Dynamic-Spatial-Temporal-Cross-Dependencies-for-Traffic-Forecasting" class="headerlink" title="DSTCGCN: Learning Dynamic Spatial-Temporal Cross Dependencies for Traffic Forecasting"></a>DSTCGCN: Learning Dynamic Spatial-Temporal Cross Dependencies for Traffic Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00518">http://arxiv.org/abs/2307.00518</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/water-wbq/dstcgcn">https://github.com/water-wbq/dstcgcn</a></li>
<li>paper_authors: Binqing Wu, Ling Chen</li>
<li>for: 本研究旨在提出一种能够同时学习空间和时间关系的动态空间temporal crossing graph convolutional neural network (DSTCGCN)，以便进行智能交通系统中的交通预测。</li>
<li>methods: 该模型使用了快速傅立叶变换 (FFT) 基于的注意 selector，选择时间序列中相关的时间步骤，并将其作为动态cross graph构建模块进行学习。</li>
<li>results: 对于六个真实世界数据集，DSTCGCN实验结果表明，该模型在交通预测任务中达到了领先的性能。<details>
<summary>Abstract</summary>
Traffic forecasting is essential to intelligent transportation systems, which is challenging due to the complicated spatial and temporal dependencies within a road network. Existing works usually learn spatial and temporal dependencies separately, ignoring the dependencies crossing spatial and temporal dimensions. In this paper, we propose DSTCGCN, a dynamic spatial-temporal cross graph convolution network to learn dynamic spatial and temporal dependencies jointly via graphs for traffic forecasting. Specifically, we introduce a fast Fourier transform (FFT) based attentive selector to choose relevant time steps for each time step based on time-varying traffic data. Given the selected time steps, we introduce a dynamic cross graph construction module, consisting of the spatial graph construction, temporal connection graph construction, and fusion modules, to learn dynamic spatial-temporal cross dependencies without pre-defined priors. Extensive experiments on six real-world datasets demonstrate that DSTCGCN achieves the state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
traffic 预测是智能交通系统的重要组成部分，但是它具有较复杂的空间和时间依赖关系，使得现有的方法通常只是分别学习空间和时间依赖关系，忽略了空间和时间维度之间的依赖关系。在这篇论文中，我们提出了DSTCGCN，一种能够同时学习空间和时间依赖关系的动态空间-时间跨Graph卷积网络。具体来说，我们引入了基于快速傅立叶变换（FFT）的注意力选择器，可以根据时间变化的交通数据选择相关的时间步骤。给定选择的时间步骤，我们引入了动态跨 Graph 建构模块，包括空间 Graph 建构模块、时间连接 Graph 建构模块和融合模块，以无预先假设的方式学习动态空间-时间跨依赖关系。我们在六个真实世界数据集上进行了广泛的实验，结果显示DSTCGCN可以达到领先的性能。
</details></li>
</ul>
<hr>
<h2 id="SUGAR-Spherical-Ultrafast-Graph-Attention-Framework-for-Cortical-Surface-Registration"><a href="#SUGAR-Spherical-Ultrafast-Graph-Attention-Framework-for-Cortical-Surface-Registration" class="headerlink" title="SUGAR: Spherical Ultrafast Graph Attention Framework for Cortical Surface Registration"></a>SUGAR: Spherical Ultrafast Graph Attention Framework for Cortical Surface Registration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00511">http://arxiv.org/abs/2307.00511</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianxun Ren, Ning An, Youjia Zhang, Danyang Wang, Zhenyu Sun, Cong Lin, Weigang Cui, Weiwei Wang, Ying Zhou, Wei Zhang, Qingyu Hu, Ping Zhang, Dan Hu, Danhong Wang, Hesheng Liu<br>for:SUGAR is designed to improve cortical surface registration, which is crucial for aligning cortical functional and anatomical features across individuals.methods:SUGAR is a deep-learning framework that incorporates a U-Net-based spherical graph attention network and leverages the Euler angle representation for deformation. It also includes a similarity loss, fold loss, and multiple distortion losses to preserve topology and minimize distortions.results:SUGAR achieves high registration performance and accelerated processing times, making it a promising solution for large-scale neuroimaging studies. It exhibits comparable or superior registration performance in accuracy, distortion, and test-retest reliability compared to conventional and learning-based methods, and processes data much faster than conventional methods.<details>
<summary>Abstract</summary>
Cortical surface registration plays a crucial role in aligning cortical functional and anatomical features across individuals. However, conventional registration algorithms are computationally inefficient. Recently, learning-based registration algorithms have emerged as a promising solution, significantly improving processing efficiency. Nonetheless, there remains a gap in the development of a learning-based method that exceeds the state-of-the-art conventional methods simultaneously in computational efficiency, registration accuracy, and distortion control, despite the theoretically greater representational capabilities of deep learning approaches. To address the challenge, we present SUGAR, a unified unsupervised deep-learning framework for both rigid and non-rigid registration. SUGAR incorporates a U-Net-based spherical graph attention network and leverages the Euler angle representation for deformation. In addition to the similarity loss, we introduce fold and multiple distortion losses, to preserve topology and minimize various types of distortions. Furthermore, we propose a data augmentation strategy specifically tailored for spherical surface registration, enhancing the registration performance. Through extensive evaluation involving over 10,000 scans from 7 diverse datasets, we showed that our framework exhibits comparable or superior registration performance in accuracy, distortion, and test-retest reliability compared to conventional and learning-based methods. Additionally, SUGAR achieves remarkable sub-second processing times, offering a notable speed-up of approximately 12,000 times in registering 9,000 subjects from the UK Biobank dataset in just 32 minutes. This combination of high registration performance and accelerated processing time may greatly benefit large-scale neuroimaging studies.
</details>
<details>
<summary>摘要</summary>
cortical surface registration 在调整 cortical functional 和 anatomical features 之间扮演着关键角色。然而，传统的 registration algorithm  computationally inefficient。 recent years, learning-based registration algorithm  emerged as a promising solution，significantly improving processing efficiency。然而，there remains a gap in the development of a learning-based method that exceeds the state-of-the-art conventional methods simultaneously in computational efficiency, registration accuracy, and distortion control，despite the theoretically greater representational capabilities of deep learning approaches。to address the challenge, we present SUGAR, a unified unsupervised deep-learning framework for both rigid and non-rigid registration。SUGAR incorporates a U-Net-based spherical graph attention network and leverages the Euler angle representation for deformation。 In addition to the similarity loss, we introduce fold and multiple distortion losses, to preserve topology and minimize various types of distortions。furthermore, we propose a data augmentation strategy specifically tailored for spherical surface registration, enhancing the registration performance。through extensive evaluation involving over 10,000 scans from 7 diverse datasets, we showed that our framework exhibits comparable or superior registration performance in accuracy, distortion, and test-retest reliability compared to conventional and learning-based methods。 additionally, SUGAR achieves remarkable sub-second processing times, offering a notable speed-up of approximately 12,000 times in registering 9,000 subjects from the UK Biobank dataset in just 32 minutes。this combination of high registration performance and accelerated processing time may greatly benefit large-scale neuroimaging studies。
</details></li>
</ul>
<hr>
<h2 id="HeGeL-A-Novel-Dataset-for-Geo-Location-from-Hebrew-Text"><a href="#HeGeL-A-Novel-Dataset-for-Geo-Location-from-Hebrew-Text" class="headerlink" title="HeGeL: A Novel Dataset for Geo-Location from Hebrew Text"></a>HeGeL: A Novel Dataset for Geo-Location from Hebrew Text</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00509">http://arxiv.org/abs/2307.00509</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/onlplab/hegel">https://github.com/onlplab/hegel</a></li>
<li>paper_authors: Tzuf Paz-Argaman, Tal Bauman, Itai Mondshine, Itzhak Omer, Sagi Dalyot, Reut Tsarfaty</li>
<li>for: 这篇论文的目的是提出一个新的地理位置检索方法，以解决文本地理位置检索问题。</li>
<li>methods: 论文使用了人工勘探方法，收集了5649个直接描述希伯来地点的 literal Hebrew 描述。</li>
<li>results: 研究发现，这些描述中有许多具有地ospatial reasoning的语言表达，需要一种新的环境表示方式。<details>
<summary>Abstract</summary>
The task of textual geolocation - retrieving the coordinates of a place based on a free-form language description - calls for not only grounding but also natural language understanding and geospatial reasoning. Even though there are quite a few datasets in English used for geolocation, they are currently based on open-source data (Wikipedia and Twitter), where the location of the described place is mostly implicit, such that the location retrieval resolution is limited. Furthermore, there are no datasets available for addressing the problem of textual geolocation in morphologically rich and resource-poor languages, such as Hebrew. In this paper, we present the Hebrew Geo-Location (HeGeL) corpus, designed to collect literal place descriptions and analyze lingual geospatial reasoning. We crowdsourced 5,649 literal Hebrew place descriptions of various place types in three cities in Israel. Qualitative and empirical analysis show that the data exhibits abundant use of geospatial reasoning and requires a novel environmental representation.
</details>
<details>
<summary>摘要</summary>
文本地理位置 Retrieving the coordinates of a place based on a natural language description requires not only grounding but also natural language understanding and geospatial reasoning. Although there are several datasets in English for geolocation, they are based on open-source data (Wikipedia and Twitter), where the location of the described place is mostly implicit, resulting in limited location retrieval resolution. Moreover, there are no datasets available for addressing the problem of textual geolocation in morphologically rich and resource-poor languages, such as Hebrew. In this paper, we present the Hebrew Geo-Location (HeGeL) corpus, designed to collect literal place descriptions and analyze lingual geospatial reasoning. We crowdsourced 5,649 literal Hebrew place descriptions of various place types in three cities in Israel. Qualitative and empirical analysis show that the data exhibits abundant use of geospatial reasoning and requires a novel environmental representation.Here's the translation in Traditional Chinese:文本地理位置 Retrieving the coordinates of a place based on a natural language description requires not only grounding but also natural language understanding and geospatial reasoning. Although there are several datasets in English for geolocation, they are based on open-source data (Wikipedia and Twitter), where the location of the described place is mostly implicit, resulting in limited location retrieval resolution. Moreover, there are no datasets available for addressing the problem of textual geolocation in morphologically rich and resource-poor languages, such as Hebrew. In this paper, we present the Hebrew Geo-Location (HeGeL) corpus, designed to collect literal place descriptions and analyze lingual geospatial reasoning. We crowdsourced 5,649 literal Hebrew place descriptions of various place types in three cities in Israel. Qualitative and empirical analysis show that the data exhibits abundant use of geospatial reasoning and requires a novel environmental representation.
</details></li>
</ul>
<hr>
<h2 id="Cloud-Ensemble-Learning-for-Fault-Diagnosis-of-Rolling-Bearings-with-Stochastic-Configuration-Networks"><a href="#Cloud-Ensemble-Learning-for-Fault-Diagnosis-of-Rolling-Bearings-with-Stochastic-Configuration-Networks" class="headerlink" title="Cloud Ensemble Learning for Fault Diagnosis of Rolling Bearings with Stochastic Configuration Networks"></a>Cloud Ensemble Learning for Fault Diagnosis of Rolling Bearings with Stochastic Configuration Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00507">http://arxiv.org/abs/2307.00507</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Dai, Jiang Liu, Lanhao Wang</li>
<li>for:  rolling bearing fault diagnosis in few shot scenarios</li>
<li>methods: stochastic configuration network (SCN) based cloud ensemble learning</li>
<li>results: accurate fault diagnosis with few training samples<details>
<summary>Abstract</summary>
Fault diagnosis of rolling bearings is of great significance for post-maintenance in rotating machinery, but it is a challenging work to diagnose faults efficiently with a few samples. Additionally, faults commonly occur with randomness and fuzziness due to the complexity of the external environment and the structure of rolling bearings, hindering effective mining of fault characteristics and eventually restricting accuracy of fault diagnosis. To overcome these problems, stochastic configuration network (SCN) based cloud ensemble learning, called SCN-CEL, is developed in this work. Concretely, a cloud feature extraction method is first developed by using a backward cloud generator of normal cloud model to mine the uncertainty of fault information. Then, a cloud sampling method, which generates enough cloud droplets using bidirectional cloud generator, is proposed to extend the cloud feature samples. Finally, an ensemble model with SCNs is developed to comprehensively characterize the uncertainty of fault information and advance the generalization performance of fault diagnosis machine. Experimental results demonstrate that the proposed method indeed performs favorably for distinguishing fault categories of rolling bearings in the few shot scenarios.
</details>
<details>
<summary>摘要</summary>
FAULT诊断rolling bearing是后续维护机器人的重要 significace，但是efficiently fault diagnosis with few samples是一项挑战性的工作。另外，FAULTS通常occurs randomly and fuzzily due to the complexity of the external environment and the structure of rolling bearings, which hinders the effective mining of fault characteristics and eventually restricts the accuracy of fault diagnosis. To overcome these problems, this work proposes a stochastic configuration network (SCN) based cloud ensemble learning method, called SCN-CEL. Specifically, a cloud feature extraction method is first developed by using a backward cloud generator of normal cloud model to mine the uncertainty of fault information. Then, a cloud sampling method, which generates enough cloud droplets using bidirectional cloud generator, is proposed to extend the cloud feature samples. Finally, an ensemble model with SCNs is developed to comprehensively characterize the uncertainty of fault information and improve the generalization performance of fault diagnosis machine. Experimental results show that the proposed method can indeed distinguish fault categories of rolling bearings in the few shot scenarios.Here's the translation in Traditional Chinese as well:FAULT诊断rolling bearing是后续维护机器人的重要 significace，但是efficiently fault diagnosis with few samples是一项挑战性的工作。另外，FAULTS通常occurs randomly and fuzzily due to the complexity of the external environment and the structure of rolling bearings, which hinders the effective mining of fault characteristics and eventually restricts the accuracy of fault diagnosis. To overcome these problems, this work proposes a stochastic configuration network (SCN) based cloud ensemble learning method, called SCN-CEL. Specifically, a cloud feature extraction method is first developed by using a backward cloud generator of normal cloud model to mine the uncertainty of fault information. Then, a cloud sampling method, which generates enough cloud droplets using bidirectional cloud generator, is proposed to extend the cloud feature samples. Finally, an ensemble model with SCNs is developed to comprehensively characterize the uncertainty of fault information and improve the generalization performance of fault diagnosis machine. Experimental results show that the proposed method can indeed distinguish fault categories of rolling bearings in the few shot scenarios.
</details></li>
</ul>
<hr>
<h2 id="On-efficient-computation-in-active-inference"><a href="#On-efficient-computation-in-active-inference" class="headerlink" title="On efficient computation in active inference"></a>On efficient computation in active inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00504">http://arxiv.org/abs/2307.00504</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aswinpaul/dpefe_2023">https://github.com/aswinpaul/dpefe_2023</a></li>
<li>paper_authors: Aswin Paul, Noor Sajid, Lancelot Da Costa, Adeel Razi</li>
<li>for: 提高 active inference 的计算效率和定义目标分布的准确性</li>
<li>methods: 提出两种解决方案，包括一种基于动态Programming的新规划算法和一种基于 Z-学习的目标分布设定方法</li>
<li>results: 在标准的grid-world任务中进行了模拟测试，并显示了这些方法的效果和可行性<details>
<summary>Abstract</summary>
Despite being recognized as neurobiologically plausible, active inference faces difficulties when employed to simulate intelligent behaviour in complex environments due to its computational cost and the difficulty of specifying an appropriate target distribution for the agent. This paper introduces two solutions that work in concert to address these limitations. First, we present a novel planning algorithm for finite temporal horizons with drastically lower computational complexity. Second, inspired by Z-learning from control theory literature, we simplify the process of setting an appropriate target distribution for new and existing active inference planning schemes. Our first approach leverages the dynamic programming algorithm, known for its computational efficiency, to minimize the cost function used in planning through the Bellman-optimality principle. Accordingly, our algorithm recursively assesses the expected free energy of actions in the reverse temporal order. This improves computational efficiency by orders of magnitude and allows precise model learning and planning, even under uncertain conditions. Our method simplifies the planning process and shows meaningful behaviour even when specifying only the agent's final goal state. The proposed solutions make defining a target distribution from a goal state straightforward compared to the more complicated task of defining a temporally informed target distribution. The effectiveness of these methods is tested and demonstrated through simulations in standard grid-world tasks. These advances create new opportunities for various applications.
</details>
<details>
<summary>摘要</summary>
despite being recognized as neurobiologically plausible, active inference faces difficulties when employed to simulate intelligent behavior in complex environments due to its computational cost and the difficulty of specifying an appropriate target distribution for the agent. This paper introduces two solutions that work in concert to address these limitations. First, we present a novel planning algorithm for finite temporal horizons with drastically lower computational complexity. Second, inspired by Z-learning from control theory literature, we simplify the process of setting an appropriate target distribution for new and existing active inference planning schemes. Our first approach leverages the dynamic programming algorithm, known for its computational efficiency, to minimize the cost function used in planning through the Bellman-optimality principle. Accordingly, our algorithm recursively assesses the expected free energy of actions in the reverse temporal order. This improves computational efficiency by orders of magnitude and allows precise model learning and planning, even under uncertain conditions. Our method simplifies the planning process and shows meaningful behavior even when specifying only the agent's final goal state. The proposed solutions make defining a target distribution from a goal state straightforward compared to the more complicated task of defining a temporally informed target distribution. The effectiveness of these methods is tested and demonstrated through simulations in standard grid-world tasks. These advances create new opportunities for various applications.Here is the word-for-word translation of the text into Simplified Chinese:尽管被认可为神经生物学上可能的，活动推理却在复杂环境中模拟智能行为时遇到了计算成本和设置合适目标分布的困难。这篇论文介绍了两种解决方案，它们在合作下解决了这些限制。首先，我们提出了一种新的规划算法，可以在有限时间Horizon上降低计算成本。其次，我们受控制理论文学中的Z学习启发，使得设置合适的目标分布变得更加简单。我们的第一个方法利用了动态计划算法，知名的计算效率高，来最小化计划中使用的成本函数。因此，我们的算法递归评估行动的预期自由能量，从反时间顺序进行评估。这会提高计算效率，并允许精确地学习和规划，即使在不确定的条件下。我们的方法简化了规划过程，并在指定只有代理人的最终目标状态时显示了意义的行为。我们的提案使得从目标分布定义变得更加直观，而不是在更复杂的时间 informed target distribution中进行定义。我们的方法的效果通过标准网格世界任务的模拟测试而证明。这些进步创造了新的应用机会。
</details></li>
</ul>
<hr>
<h2 id="Classifying-World-War-II-Era-Ciphers-with-Machine-Learning"><a href="#Classifying-World-War-II-Era-Ciphers-with-Machine-Learning" class="headerlink" title="Classifying World War II Era Ciphers with Machine Learning"></a>Classifying World War II Era Ciphers with Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00501">http://arxiv.org/abs/2307.00501</a></li>
<li>repo_url: None</li>
<li>paper_authors: Brooke Dalton, Mark Stamp</li>
<li>for: 研究使用机器学习和深度学习技术对世界二战时期密码的分类，只使用密码文本。</li>
<li>methods: 使用支持向量机（SVM）、k-最近邻（$k$-NN）和随机森林（RF）等三种经典机器学习模型，以及多层感知机（MLP）、长短期记忆（LSTM）、极限学习机（ELM）和卷积神经网络（CNN）等四种深度学习神经网络模型。</li>
<li>results: 对四种密码（恩igma、M-209、Sigaba、Purple和Typex）进行分类，使用不同的enario和不同长度的密码文本。结果显示，在最实际的scenario下，使用1000个字的密码文本，可以分类密码的准确率高于97%。此外，对一些学习技术的准确率进行分析，发现经典机器学习模型和深度学习模型在一些情况下表现相当。<details>
<summary>Abstract</summary>
We determine the accuracy with which machine learning and deep learning techniques can classify selected World War II era ciphers when only ciphertext is available. The specific ciphers considered are Enigma, M-209, Sigaba, Purple, and Typex. We experiment with three classic machine learning models, namely, Support Vector Machines (SVM), $k$-Nearest Neighbors ($k$-NN), and Random Forest (RF). We also experiment with four deep learning neural network-based models: Multi-Layer Perceptrons (MLP), Long Short-Term Memory (LSTM), Extreme Learning Machines (ELM), and Convolutional Neural Networks (CNN). Each model is trained on features consisting of histograms, digrams, and raw ciphertext letter sequences. Furthermore, the classification problem is considered under four distinct scenarios: Fixed plaintext with fixed keys, random plaintext with fixed keys, fixed plaintext with random keys, and random plaintext with random keys. Under the most realistic scenario, given 1000 characters per ciphertext, we are able to distinguish the ciphers with greater than 97% accuracy. In addition, we consider the accuracy of a subset of the learning techniques as a function of the length of the ciphertext messages. Somewhat surprisingly, our classic machine learning models perform at least as well as our deep learning models. We also find that ciphers that are more similar in design are somewhat more challenging to distinguish, but not as difficult as might be expected.
</details>
<details>
<summary>摘要</summary>
我们测试了机器学习和深度学习技术可以在只有密文available情况下准确地分类选择的World War II时期密码。我们考虑了Enigma、M-209、Sigaba、Purple和Typex等密码。我们尝试了三种经典机器学习模型，namely Support Vector Machines（SVM）、$k$-Nearest Neighbors（$k$-NN）和Random Forest（RF）。我们还尝试了四种深度学习神经网络模型：Multi-Layer Perceptrons（MLP）、Long Short-Term Memory（LSTM）、Extreme Learning Machines（ELM）和Convolutional Neural Networks（CNN）。每个模型都在 histograms、digram和密文字符序列中的特征上进行训练。此外，我们还考虑了密码分类问题在四种不同的enario下进行解决：固定的平文和固定的密钥、随机的平文和固定的密钥、固定的平文和随机的密钥、随机的平文和随机的密钥。在最真实的scenario下，给定1000个密文字符，我们能够分类密码的准确率高于97%。此外，我们还考虑了学习技术的准确率与密文字符数量之间的关系。 surprisingly，我们的经典机器学习模型在准确率上至少与深度学习模型相当。我们还发现，在设计上更相似的密码相对较难分类，但不如可能所想的那么难。
</details></li>
</ul>
<hr>
<h2 id="Data-Free-Quantization-via-Mixed-Precision-Compensation-without-Fine-Tuning"><a href="#Data-Free-Quantization-via-Mixed-Precision-Compensation-without-Fine-Tuning" class="headerlink" title="Data-Free Quantization via Mixed-Precision Compensation without Fine-Tuning"></a>Data-Free Quantization via Mixed-Precision Compensation without Fine-Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00498">http://arxiv.org/abs/2307.00498</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jun Chen, Shipeng Bai, Tianxin Huang, Mengmeng Wang, Guanzhong Tian, Yong Liu</li>
<li>For: The paper aims to propose a data-free mixed-precision compensation (DF-MPC) method to recover the performance of an ultra-low precision quantized model without any data and fine-tuning process.* Methods: The proposed DF-MPC method assumes that the quantized error caused by a low-precision quantized layer can be restored via the reconstruction of a high-precision quantized layer, and minimizes the reconstruction loss of the feature maps to achieve the closed-form solution.* Results: The experimental results show that the proposed DF-MPC method is able to achieve higher accuracy for an ultra-low precision quantized model compared to the recent methods without any data and fine-tuning process.Here are the three key points in Simplified Chinese text:* For: 本文提出的目标是无数据、无调教的情况下提高低精度量化模型的性能。* Methods: 提议的DF-MPC方法假设低精度量化层引起的量化误差可以通过高精度量化层的重建来修复，并将重建loss作为目标函数来求解closed-form解决方案。* Results: 实验结果表明，提议的DF-MPC方法可以在无数据、无调教的情况下提高低精度量化模型的性能，相比之下最近的方法表现更好。<details>
<summary>Abstract</summary>
Neural network quantization is a very promising solution in the field of model compression, but its resulting accuracy highly depends on a training/fine-tuning process and requires the original data. This not only brings heavy computation and time costs but also is not conducive to privacy and sensitive information protection. Therefore, a few recent works are starting to focus on data-free quantization. However, data-free quantization does not perform well while dealing with ultra-low precision quantization. Although researchers utilize generative methods of synthetic data to address this problem partially, data synthesis needs to take a lot of computation and time. In this paper, we propose a data-free mixed-precision compensation (DF-MPC) method to recover the performance of an ultra-low precision quantized model without any data and fine-tuning process. By assuming the quantized error caused by a low-precision quantized layer can be restored via the reconstruction of a high-precision quantized layer, we mathematically formulate the reconstruction loss between the pre-trained full-precision model and its layer-wise mixed-precision quantized model. Based on our formulation, we theoretically deduce the closed-form solution by minimizing the reconstruction loss of the feature maps. Since DF-MPC does not require any original/synthetic data, it is a more efficient method to approximate the full-precision model. Experimentally, our DF-MPC is able to achieve higher accuracy for an ultra-low precision quantized model compared to the recent methods without any data and fine-tuning process.
</details>
<details>
<summary>摘要</summary>
神经网络量化是一个非常有前途的解决方案，可以大幅压缩模型，但是它的结果准确性高度取决于训练/精度调整过程，并且需要原始数据。这不仅带来了重大的计算成本和时间开销，而且不利于隐私和敏感信息保护。因此，一些最近的研究开始关注数据无关量化。然而，数据无关量化在处理超低精度量化时并不很好。尽管研究人员利用生成方法 Synthetic Data 来解决这个问题，但是数据生成需要大量计算和时间。在这篇论文中，我们提出了一种不需要数据的混合精度补偿方法（DF-MPC），可以无需任何数据和精度调整过程，使得模型的性能得到改进。我们假设low-precision量化层所导致的量化误差可以通过高精度量化层的重建来还原。我们将这种情况Mathematically formulated as a reconstruction loss between the pre-trained full-precision model and its layer-wise mixed-precision quantized model。基于我们的形式化，我们可以 theoretically deduce the closed-form solution by minimizing the reconstruction loss of the feature maps。由于 DF-MPC 不需要任何原始/生成的数据，因此它是一种更高效的方法来 aproximate the full-precision model。实验表明，我们的 DF-MPC 能够在无需数据和精度调整过程的情况下，对ultra-low precision量化模型进行更高的准确性改进。
</details></li>
</ul>
<hr>
<h2 id="Don’t-Memorize-Mimic-The-Past-Federated-Class-Incremental-Learning-Without-Episodic-Memory"><a href="#Don’t-Memorize-Mimic-The-Past-Federated-Class-Incremental-Learning-Without-Episodic-Memory" class="headerlink" title="Don’t Memorize; Mimic The Past: Federated Class Incremental Learning Without Episodic Memory"></a>Don’t Memorize; Mimic The Past: Federated Class Incremental Learning Without Episodic Memory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00497">http://arxiv.org/abs/2307.00497</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sara Babakniya, Zalan Fabian, Chaoyang He, Mahdi Soltanolkotabi, Salman Avestimehr</li>
<li>for: 本文研究的目的是解决深度学习模型在新数据上学习时忘记过去学习的问题，特别在联合学习（Federated Learning，FL）中，数据是分散的，每个用户都可以独立地更改自己的数据。</li>
<li>methods: 本文提出了一个基于生成模型的联合学习类增量学习框架，通过生成样本从过去的分布中来避免忘记现象，而不需要将过去的数据存储在客户端上。服务器端使用数据无需方法在每个任务结束时训练生成模型，因此减少了数据泄露的风险。</li>
<li>results: 对于CIFAR-100 dataset，本文比对 existed 的基准值显示了显著的改进。<details>
<summary>Abstract</summary>
Deep learning models are prone to forgetting information learned in the past when trained on new data. This problem becomes even more pronounced in the context of federated learning (FL), where data is decentralized and subject to independent changes for each user. Continual Learning (CL) studies this so-called \textit{catastrophic forgetting} phenomenon primarily in centralized settings, where the learner has direct access to the complete training dataset. However, applying CL techniques to FL is not straightforward due to privacy concerns and resource limitations. This paper presents a framework for federated class incremental learning that utilizes a generative model to synthesize samples from past distributions instead of storing part of past data. Then, clients can leverage the generative model to mitigate catastrophic forgetting locally. The generative model is trained on the server using data-free methods at the end of each task without requesting data from clients. Therefore, it reduces the risk of data leakage as opposed to training it on the client's private data. We demonstrate significant improvements for the CIFAR-100 dataset compared to existing baselines.
</details>
<details>
<summary>摘要</summary>
深度学习模型容易忘记过去学习的信息，特别在 federated learning（FL）上，数据分散化和每个用户独立变化。 kontinual learning（CL）在中心化Setting中主要研究这种称为“极端忘记”现象，但是在隐私和资源限制的情况下应用CL技术到FL不是直接的。这篇文章介绍了一种基于生成模型的联邦分类逐步学习框架，可以在客户端使用生成模型来避免极端忘记。生成模型在服务器端使用数据free方法在每个任务结束时训练，因此减少了数据泄露的风险，与在客户端私有数据上训练生成模型不同。我们对 CIFAR-100 数据集进行了比较，与现有基eline相比，我们得到了显著的改进。
</details></li>
</ul>
<hr>
<h2 id="STG4Traffic-A-Survey-and-Benchmark-of-Spatial-Temporal-Graph-Neural-Networks-for-Traffic-Prediction"><a href="#STG4Traffic-A-Survey-and-Benchmark-of-Spatial-Temporal-Graph-Neural-Networks-for-Traffic-Prediction" class="headerlink" title="STG4Traffic: A Survey and Benchmark of Spatial-Temporal Graph Neural Networks for Traffic Prediction"></a>STG4Traffic: A Survey and Benchmark of Spatial-Temporal Graph Neural Networks for Traffic Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00495">http://arxiv.org/abs/2307.00495</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/trainingl/stg4traffic">https://github.com/trainingl/stg4traffic</a></li>
<li>paper_authors: Xunlian Luo, Chunjiang Zhu, Detian Zhang, Qing Li</li>
<li>for: 这个论文主要是为了提出一种基于图学习的实时交通预测方法，以提高智能城市系统的安全、稳定性和多样性。</li>
<li>methods: 这篇论文使用了图学习策略和常见的图卷积网络来模型交通系统的空间时间相关性。</li>
<li>results: 研究人员通过设计了一个标准化和可扩展的benchmark，并对两种交通数据集进行了比较性评估，发现这种方法可以提供更高的准确率和更好的可扩展性。Please note that the above text is in Simplified Chinese, and the format of the output is as you requested:</li>
<li>for: &lt;what are the paper written for?&gt;</li>
<li>methods: &lt;what methods the paper use?&gt;</li>
<li>results: &lt;what results the paper get?&gt;<details>
<summary>Abstract</summary>
Traffic prediction has been an active research topic in the domain of spatial-temporal data mining. Accurate real-time traffic prediction is essential to improve the safety, stability, and versatility of smart city systems, i.e., traffic control and optimal routing. The complex and highly dynamic spatial-temporal dependencies make effective predictions still face many challenges. Recent studies have shown that spatial-temporal graph neural networks exhibit great potential applied to traffic prediction, which combines sequential models with graph convolutional networks to jointly model temporal and spatial correlations. However, a survey study of graph learning, spatial-temporal graph models for traffic, as well as a fair comparison of baseline models are pending and unavoidable issues. In this paper, we first provide a systematic review of graph learning strategies and commonly used graph convolution algorithms. Then we conduct a comprehensive analysis of the strengths and weaknesses of recently proposed spatial-temporal graph network models. Furthermore, we build a study called STG4Traffic using the deep learning framework PyTorch to establish a standardized and scalable benchmark on two types of traffic datasets. We can evaluate their performance by personalizing the model settings with uniform metrics. Finally, we point out some problems in the current study and discuss future directions. Source codes are available at https://github.com/trainingl/STG4Traffic.
</details>
<details>
<summary>摘要</summary>
宽泛研究领域：预测交通流量是智能城市系统中的一个活跃研究话题。实时准确的交通预测可以提高智能城市系统的安全性、稳定性和多样性，如交通控制和优化Routing。距离Complex and highly dynamic spatial-temporal dependencies make effective predictions still face many challenges. Recent studies have shown that spatial-temporal graph neural networks exhibit great potential applied to traffic prediction, which combines sequential models with graph convolutional networks to jointly model temporal and spatial correlations. However, a survey study of graph learning, spatial-temporal graph models for traffic, as well as a fair comparison of baseline models are pending and unavoidable issues.在这篇论文中，我们首先提供了系统性的图学策略和通用的图 convolution 算法的评估。然后，我们进行了广泛的 spatial-temporal graph network 模型的分析，探讨其优劣点。其次，我们使用 PyTorch 深度学习框架建立了一个标准化和可扩展的 benchmark，并在两种交通数据集上进行了研究。通过个性化模型设置，我们可以评估其性能。最后，我们指出了当前研究中的一些问题，并讨论了未来的方向。源代码可以在 https://github.com/trainingl/STG4Traffic 上获取。
</details></li>
</ul>
<hr>
<h2 id="Optimizing-protein-fitness-using-Gibbs-sampling-with-Graph-based-Smoothing"><a href="#Optimizing-protein-fitness-using-Gibbs-sampling-with-Graph-based-Smoothing" class="headerlink" title="Optimizing protein fitness using Gibbs sampling with Graph-based Smoothing"></a>Optimizing protein fitness using Gibbs sampling with Graph-based Smoothing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00494">http://arxiv.org/abs/2307.00494</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kirjner/ggs">https://github.com/kirjner/ggs</a></li>
<li>paper_authors: Andrew Kirjner, Jason Yim, Raman Samusevich, Tommi Jaakkola, Regina Barzilay, Ila Fiete</li>
<li>for: 本研究旨在设计高适应性蛋白质，以满足医学多种领域的需求。</li>
<li>methods: 本文提出了 Gibbs sampling with Graph-based Smoothing（GGS）方法，可以有效地探索蛋白质设计空间。GGS方法通过 iteratively 应用 Gibbs 与梯度来提出有利变化，并使用图基的平滑来消除干扰梯度导致的假阳性。</li>
<li>results: 对 GFP 和 AAV 设计问题，以及简单的ablations 和基线，本文进行了研究，并得到了 state-of-the-art 的结果，能够找到高适应性蛋白质的Up to 8 个变化。<details>
<summary>Abstract</summary>
The ability to design novel proteins with higher fitness on a given task would be revolutionary for many fields of medicine. However, brute-force search through the combinatorially large space of sequences is infeasible. Prior methods constrain search to a small mutational radius from a reference sequence, but such heuristics drastically limit the design space. Our work seeks to remove the restriction on mutational distance while enabling efficient exploration. We propose Gibbs sampling with Graph-based Smoothing (GGS) which iteratively applies Gibbs with gradients to propose advantageous mutations using graph-based smoothing to remove noisy gradients that lead to false positives. Our method is state-of-the-art in discovering high-fitness proteins with up to 8 mutations from the training set. We study the GFP and AAV design problems, ablations, and baselines to elucidate the results. Code: https://github.com/kirjner/GGS
</details>
<details>
<summary>摘要</summary>
能够设计新的蛋白质，其适应能力更高，将对医学多个领域带来革命。然而，简单地通过枚举空间的搜索是不可能的。先前的方法受限于小距离的参照序列，但这些决策限制了设计空间。我们的工作是要 removes 这种距离限制，同时允许有效探索。我们提议使用 Gibbs 抽象法和图像缓和（GGS），每步应用 Gibbs 与梯度相结合，提出有利мутации，并使用图像缓和来消除干扰梯度，以避免干扰梯度导致的假阳性。我们的方法目前为止在使用训练集上达到高适应能力蛋白质的设计问题上占据了国际领先地位。我们在 GFP 和 AAV 设计问题、ablations 和基准值进行了研究，以便解释结果。代码可以在 GitHub 上找到：https://github.com/kirjner/GGS。
</details></li>
</ul>
<hr>
<h2 id="Fourier-Mixed-Window-Attention-Accelerating-Informer-for-Long-Sequence-Time-Series-Forecasting"><a href="#Fourier-Mixed-Window-Attention-Accelerating-Informer-for-Long-Sequence-Time-Series-Forecasting" class="headerlink" title="Fourier-Mixed Window Attention: Accelerating Informer for Long Sequence Time-Series Forecasting"></a>Fourier-Mixed Window Attention: Accelerating Informer for Long Sequence Time-Series Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00493">http://arxiv.org/abs/2307.00493</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nhatthanhtran/fwin2023">https://github.com/nhatthanhtran/fwin2023</a></li>
<li>paper_authors: Nhat Thanh Tran, Jack Xin</li>
<li>for: 用于加速长序列时间预测 Informer 的快本地全球窗口基于注意力方法。</li>
<li>methods: 使用本地窗口注意力，而不是假设查询稀疏性，并使用 fourier 变换块来补做全球token信息。</li>
<li>results: 通过在单变量和多变量 datasets 上进行实验，我们表明 FWin 变换器可以提高 Informer 的总预测精度，同时提高其推理速度，比如40-50%。此外，我们还在非线性回归模型中显示了一种学习 FWin 类型注意力的方法可以超过 softmax 全注意力。<details>
<summary>Abstract</summary>
We study a fast local-global window-based attention method to accelerate Informer for long sequence time-series forecasting. While window attention is local and a considerable computational saving, it lacks the ability to capture global token information which is compensated by a subsequent Fourier transform block. Our method, named FWin, does not rely on query sparsity hypothesis and an empirical approximation underlying the ProbSparse attention of Informer. Through experiments on univariate and multivariate datasets, we show that FWin transformers improve the overall prediction accuracies of Informer while accelerating its inference speeds by 40 to 50 %. We also show in a nonlinear regression model that a learned FWin type attention approaches or even outperforms softmax full attention based on key vectors extracted from an Informer model's full attention layer acting on time series data.
</details>
<details>
<summary>摘要</summary>
我们研究一种快速的本地-全局窗口基于注意方法，以加速Informer进行长序时间序列预测。虽然窗口注意是本地的，可以获得较大的计算时间 saving，但是它缺乏捕捉全局Token信息的能力，这是通过后续的傅里叶变换块补做。我们的方法，名为FWin，不依赖于查询稀缺假设和Informer中的ProbSparse注意力的经验近似。通过对单variate和多variate数据集进行实验，我们显示了FWin变换器可以提高Informer的总预测精度，同时加速其推理速度 by 40% to 50%. 我们还在非线性回归模型中表明，一种学习的FWin类型注意力可以与Softmax全注意力相当或者超过从Informer模型的全注意层中提取的键vector。
</details></li>
</ul>
<hr>
<h2 id="Pricing-European-Options-with-Google-AutoML-TensorFlow-and-XGBoost"><a href="#Pricing-European-Options-with-Google-AutoML-TensorFlow-and-XGBoost" class="headerlink" title="Pricing European Options with Google AutoML, TensorFlow, and XGBoost"></a>Pricing European Options with Google AutoML, TensorFlow, and XGBoost</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00476">http://arxiv.org/abs/2307.00476</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/juan-esteban-berger/options_pricing_automl_tensorflow_xgboost">https://github.com/juan-esteban-berger/options_pricing_automl_tensorflow_xgboost</a></li>
<li>paper_authors: Juan Esteban Berger</li>
<li>for: 该文章用于比较使用Google Cloud AutoML Regressor、TensorFlow神经网络和XGBoost分类决策树来估算欧洲Option价格。</li>
<li>methods: 该文章使用了三种不同的机器学习算法来估算欧洲Option价格，即Google Cloud AutoML Regressor、TensorFlow神经网络和XGBoost分类决策树。</li>
<li>results: 三种模型都能够超越黑色熊模型，具体来说，使用历史数据来估算欧洲Option价格尤其有效，尤其是使用机器学习算法来学习复杂的模式，传统参数模型不能考虑。<details>
<summary>Abstract</summary>
Researchers have been using Neural Networks and other related machine-learning techniques to price options since the early 1990s. After three decades of improvements in machine learning techniques, computational processing power, cloud computing, and data availability, this paper is able to provide a comparison of using Google Cloud's AutoML Regressor, TensorFlow Neural Networks, and XGBoost Gradient Boosting Decision Trees for pricing European Options. All three types of models were able to outperform the Black Scholes Model in terms of mean absolute error. These results showcase the potential of using historical data from an option's underlying asset for pricing European options, especially when using machine learning algorithms that learn complex patterns that traditional parametric models do not take into account.
</details>
<details>
<summary>摘要</summary>
研究人员 desde 1990年代开始使用神经网络和相关的机器学习技术来估算选择价格。过去三十年来，机器学习技术、计算机处理能力、云计算和数据可用性得到了大幅提高。本文能够对使用Google Cloud的AutoML Regressor、TensorFlow神经网络和XGBoost分布gradient Boosting Decision Trees来估算欧洲选择价格进行比较。这三种类型的模型都能够超越黑卫星模型（Black Scholes Model）的mean absolute error。这些结果表明，使用选择物品的历史数据来估算欧洲选择价格，尤其是使用机器学习算法来学习复杂的模式，传统参数模型无法考虑。
</details></li>
</ul>
<hr>
<h2 id="Moments-Random-Walks-and-Limits-for-Spectrum-Approximation"><a href="#Moments-Random-Walks-and-Limits-for-Spectrum-Approximation" class="headerlink" title="Moments, Random Walks, and Limits for Spectrum Approximation"></a>Moments, Random Walks, and Limits for Spectrum Approximation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00474">http://arxiv.org/abs/2307.00474</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yujia Jin, Christopher Musco, Aaron Sidford, Apoorv Vikram Singh</li>
<li>for: 本文研究一维分布近似问题中的下界问题，具体来说是对于已知多项式级别的一维分布，可以达到伪随机性水平。</li>
<li>methods: 本文使用了induced by eigenvalue spectra of carefully constructed graph adjacency matrices的hard instance，以及Cohen-Steiner et al. [KDD 2018]提供的spectral moments approximations using $2^{O(1&#x2F;\epsilon)}$ random walks initiated at uniformly random nodes in the graph。</li>
<li>results: 本文证明了一些分布在[-1,1]上不能够在Wasserstein-1 distance上准确地近似，即使知道所有多项式级别的分布。这个结果与Kong和Valiant [Annals of Statistics, 2017]的Upper bound相符。<details>
<summary>Abstract</summary>
We study lower bounds for the problem of approximating a one dimensional distribution given (noisy) measurements of its moments. We show that there are distributions on $[-1,1]$ that cannot be approximated to accuracy $\epsilon$ in Wasserstein-1 distance even if we know \emph{all} of their moments to multiplicative accuracy $(1\pm2^{-\Omega(1/\epsilon)})$; this result matches an upper bound of Kong and Valiant [Annals of Statistics, 2017]. To obtain our result, we provide a hard instance involving distributions induced by the eigenvalue spectra of carefully constructed graph adjacency matrices. Efficiently approximating such spectra in Wasserstein-1 distance is a well-studied algorithmic problem, and a recent result of Cohen-Steiner et al. [KDD 2018] gives a method based on accurately approximating spectral moments using $2^{O(1/\epsilon)}$ random walks initiated at uniformly random nodes in the graph.   As a strengthening of our main result, we show that improving the dependence on $1/\epsilon$ in this result would require a new algorithmic approach. Specifically, no algorithm can compute an $\epsilon$-accurate approximation to the spectrum of a normalized graph adjacency matrix with constant probability, even when given the transcript of $2^{\Omega(1/\epsilon)}$ random walks of length $2^{\Omega(1/\epsilon)}$ started at random nodes.
</details>
<details>
<summary>摘要</summary>
我们研究一维分布的下界，对于受到杂度的测量的矩形几何。我们证明有在[-1,1]上的分布，不能够在 Wasserstein-1 距离下对准确度 $\epsilon$ 的测量。这个结果与 Kong 和 Valiant 的上界相匹配 [Annals of Statistics, 2017]。我们使用具有精确的数值矩阵的对角线几何来提供一个困难的实例。现有一个由 Cohen-Steiner 等人提出的方法 [KDD 2018]，可以使用 $2^{O(1/\epsilon)}$ 步骤的随机步进行精确地测量几何的spectrum。作为我们主要结果的强化，我们证明 improvving 这个结果中的 $1/\epsilon$ 取决因数需要一个新的算法方法。具体来说，无法使用现有的算法，在 givent 矩阵的转换矩阵上 compute 一个 $\epsilon$-精确的测量，即使被给定 $2^{\Omega(1/\epsilon)}$ 步骤的随机步进行测量。
</details></li>
</ul>
<hr>
<h2 id="Equal-Confusion-Fairness-Measuring-Group-Based-Disparities-in-Automated-Decision-Systems"><a href="#Equal-Confusion-Fairness-Measuring-Group-Based-Disparities-in-Automated-Decision-Systems" class="headerlink" title="Equal Confusion Fairness: Measuring Group-Based Disparities in Automated Decision Systems"></a>Equal Confusion Fairness: Measuring Group-Based Disparities in Automated Decision Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00472">http://arxiv.org/abs/2307.00472</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/furkangursoy/equalconfusion">https://github.com/furkangursoy/equalconfusion</a></li>
<li>paper_authors: Furkan Gursoy, Ioannis A. Kakadiaris</li>
<li>for:  This paper focuses on evaluating the fairness of automated decision systems, specifically in terms of group fairness.</li>
<li>methods: The paper proposes a new equal confusion fairness test and a new confusion parity error metric to measure unfairness, as well as an appropriate post hoc analysis methodology to identify the source of potential unfairness.</li>
<li>results: The proposed methods and metrics are demonstrated on the case of COMPAS, an automated decision system used in the US to assess recidivism risks, and show their usefulness in assessing fairness as part of a more extensive accountability assessment.Here’s the same information in Simplified Chinese:</li>
<li>for: 这篇论文关注自动决策系统的公平性评估，尤其是集体公平性。</li>
<li>methods: 该论文提出了一种新的平衡决策公平测试和一个新的决策错误度量表来衡量不公平，以及一种适当的后续分析方法来检测潜在的不公平源头。</li>
<li>results: 提议的方法和指标在使用COMPAS例子进行示例后显示了它们在评估自动决策系统公平性的用于更广泛的责任评估中的用用ifulness。<details>
<summary>Abstract</summary>
As artificial intelligence plays an increasingly substantial role in decisions affecting humans and society, the accountability of automated decision systems has been receiving increasing attention from researchers and practitioners. Fairness, which is concerned with eliminating unjust treatment and discrimination against individuals or sensitive groups, is a critical aspect of accountability. Yet, for evaluating fairness, there is a plethora of fairness metrics in the literature that employ different perspectives and assumptions that are often incompatible. This work focuses on group fairness. Most group fairness metrics desire a parity between selected statistics computed from confusion matrices belonging to different sensitive groups. Generalizing this intuition, this paper proposes a new equal confusion fairness test to check an automated decision system for fairness and a new confusion parity error to quantify the extent of any unfairness. To further analyze the source of potential unfairness, an appropriate post hoc analysis methodology is also presented. The usefulness of the test, metric, and post hoc analysis is demonstrated via a case study on the controversial case of COMPAS, an automated decision system employed in the US to assist judges with assessing recidivism risks. Overall, the methods and metrics provided here may assess automated decision systems' fairness as part of a more extensive accountability assessment, such as those based on the system accountability benchmark.
</details>
<details>
<summary>摘要</summary>
随着人工智能在影响人类和社会决策中的角色变得越来越重要，决策系统的负责任得到了更多的关注。公平是考虑消除不公正对待和歧视的一个关键方面，它是负责任的一部分。然而，为了评估公平，在文献中有很多不同的公平指标，这些指标常有不同的视角和假设，导致它们之间不兼容。这项工作将关注于群体公平。大多数群体公平指标寻求在不同敏感群体的决策系统中计算的混乱矩阵中实现平衡。总之，这篇论文提出了一种新的平衡公平测试，用于检测自动决策系统的公平，以及一种新的混乱平衡错误来衡量任何不公正。此外，为了分析潜在的不公正来源，我们还提供了一种适用的后续分析方法。这些方法和指标在一个关于 COMPAS 案例的案例研究中被证明了其用于评估自动决策系统的公平。总之，提供的方法和指标可以用于评估自动决策系统的公平，并作为更广泛的负责任评估的一部分，如基于系统负责任指标。
</details></li>
</ul>
<hr>
<h2 id="Data-Driven-Probabilistic-Energy-Consumption-Estimation-for-Battery-Electric-Vehicles-with-Model-Uncertainty"><a href="#Data-Driven-Probabilistic-Energy-Consumption-Estimation-for-Battery-Electric-Vehicles-with-Model-Uncertainty" class="headerlink" title="Data-Driven Probabilistic Energy Consumption Estimation for Battery Electric Vehicles with Model Uncertainty"></a>Data-Driven Probabilistic Energy Consumption Estimation for Battery Electric Vehicles with Model Uncertainty</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00469">http://arxiv.org/abs/2307.00469</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ayan Maity, Sudeshna Sarkar</li>
<li>for: 这个论文是为了提出一种基于概率数据驱动的电动汽车（BEV）行程级能耗估计方法。</li>
<li>methods: 该方法使用概率神经网络，并通过 Монテ卡洛分布来确定模型uncertainty。</li>
<li>results: 实验结果表明，该方法可以准确地估计电动汽车行程级能耗，并且与其他现有的电动汽车能耗模型相比，具有较高的准确率。<details>
<summary>Abstract</summary>
This paper presents a novel probabilistic data-driven approach to trip-level energy consumption estimation of battery electric vehicles (BEVs). As there are very few electric vehicle (EV) charging stations, EV trip energy consumption estimation can make EV routing and charging planning easier for drivers. In this research article, we propose a new driver behaviour-centric EV energy consumption estimation model using probabilistic neural networks with model uncertainty. By incorporating model uncertainty into neural networks, we have created an ensemble of neural networks using Monte Carlo approximation. Our method comprehensively considers various vehicle dynamics, driver behaviour and environmental factors to estimate EV energy consumption for a given trip. We propose relative positive acceleration (RPA), average acceleration and average deceleration as driver behaviour factors in EV energy consumption estimation and this paper shows that the use of these driver behaviour features improves the accuracy of the EV energy consumption model significantly. Instead of predicting a single-point estimate for EV trip energy consumption, this proposed method predicts a probability distribution for the EV trip energy consumption. The experimental results of our approach show that our proposed probabilistic neural network with weight uncertainty achieves a mean absolute percentage error of 9.3% and outperforms other existing EV energy consumption models in terms of accuracy.
</details>
<details>
<summary>摘要</summary>
The proposed method uses probabilistic neural networks with model uncertainty to estimate EV energy consumption. By incorporating model uncertainty into neural networks, an ensemble of neural networks is created using Monte Carlo approximation. The method considers various vehicle dynamics, driver behavior, and environmental factors to estimate EV energy consumption for a given trip.The proposed method uses relative positive acceleration (RPA), average acceleration, and average deceleration as driver behavior factors in EV energy consumption estimation. The experimental results show that the proposed method outperforms other existing EV energy consumption models in terms of accuracy, with a mean absolute percentage error of 9.3%. Instead of predicting a single-point estimate for EV trip energy consumption, the proposed method predicts a probability distribution for the EV trip energy consumption.Here is the translation in Simplified Chinese:这篇论文提出了一种新的可能性推理数据驱动的方法来估算电动汽车（BEVs）的旅程级能 consumption。由于有很少的电动汽车充电站， точно估算电动汽车旅程能 consumption可以让驾驶员更容易进行电动汽车路径和充电规划。提议的方法使用可能性神经网络 WITH 模型不确定性来估算电动汽车的能 consumption。通过将模型不确定性引入神经网络中，我们创建了一个 ensemble of 神经网络 using Monte Carlo approximation。该方法广泛考虑了不同的汽车动力学、驾驶行为和环境因素，以估算电动汽车旅程能 consumption。提议的方法使用幂加速度（RPA）、均值加速度和均值减速度作为驾驶行为因素。实验结果表明，提议的方法在准确性方面超过了其他现有的电动汽车能 consumption模型， Mean Absolute Percentage Error 为9.3%。而不是预测单点的电动汽车旅程能 consumption，提议的方法预测了电动汽车旅程能 consumption的概率分布。
</details></li>
</ul>
<hr>
<h2 id="MissDiff-Training-Diffusion-Models-on-Tabular-Data-with-Missing-Values"><a href="#MissDiff-Training-Diffusion-Models-on-Tabular-Data-with-Missing-Values" class="headerlink" title="MissDiff: Training Diffusion Models on Tabular Data with Missing Values"></a>MissDiff: Training Diffusion Models on Tabular Data with Missing Values</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00467">http://arxiv.org/abs/2307.00467</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yidong Ouyang, Liyan Xie, Chongxuan Li, Guang Cheng</li>
<li>for: 模型学习从数据中缺失值的数据分布，以便在各种实际应用中处理缺失数据。</li>
<li>methods: 提出了一种统一的涉及原理的扩展 diffusion 模型，可以在不同的缺失机制下学习数据分布。</li>
<li>results: 与 state-of-the-art  diffusion model 比较，在多个实际的 tabular 数据集上达到了较大的性能提升。<details>
<summary>Abstract</summary>
The diffusion model has shown remarkable performance in modeling data distributions and synthesizing data. However, the vanilla diffusion model requires complete or fully observed data for training. Incomplete data is a common issue in various real-world applications, including healthcare and finance, particularly when dealing with tabular datasets. This work presents a unified and principled diffusion-based framework for learning from data with missing values under various missing mechanisms. We first observe that the widely adopted "impute-then-generate" pipeline may lead to a biased learning objective. Then we propose to mask the regression loss of Denoising Score Matching in the training phase. We prove the proposed method is consistent in learning the score of data distributions, and the proposed training objective serves as an upper bound for the negative likelihood in certain cases. The proposed framework is evaluated on multiple tabular datasets using realistic and efficacious metrics and is demonstrated to outperform state-of-the-art diffusion model on tabular data with "impute-then-generate" pipeline by a large margin.
</details>
<details>
<summary>摘要</summary>
diffusion 模型在数据分布模型和数据合成方面表现出色，但是普通的扩散模型需要完整或完全观察到的数据进行训练。实际应用中，包括医疗和金融等领域，数据中存在缺失数据是一个常见的问题。这项工作提出了一种统一的、原则正的扩散模型基于缺失数据学习框架，用于处理不同的缺失机制。我们首先发现，通过"填充然后生成"管道可能会导致偏向的学习目标。然后，我们提议在训练阶段对杜邦Score匹配的损失进行遮盖。我们证明了我们提议的方法是学习数据分布的分数的一种一致的方法，并且我们提议的训练目标为certain cases中的负概率下界。我们的框架在多个实际的 tabular 数据集上进行了实际和有效的评估，并与state-of-the-art扩散模型在 tabular 数据上"填充然后生成"管道的比较中表现出了大幅度的超越。
</details></li>
</ul>
<hr>
<h2 id="Towards-Unbiased-Exploration-in-Partial-Label-Learning"><a href="#Towards-Unbiased-Exploration-in-Partial-Label-Learning" class="headerlink" title="Towards Unbiased Exploration in Partial Label Learning"></a>Towards Unbiased Exploration in Partial Label Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00465">http://arxiv.org/abs/2307.00465</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zsolt Zombori, Agapi Rissaki, Kristóf Szabó, Wolfgang Gatterbauer, Michael Benedikt</li>
<li>for: 这篇论文是用于描述一种基于半标注的学习方法，该方法可以在标准神经网络架构中使用softmax层进行推理，并且可以减轻softmax层中的偏见现象，以便更好地探索多个可能性。</li>
<li>methods: 这篇论文使用了一种新的损失函数，该损失函数可以减轻softmax层中的偏见现象，并且可以在标准神经网络架构中进行不偏的探索。</li>
<li>results: 论文通过对синтетиче数据、标准半标注benchmark和一个新的规则学习挑战中的数据进行广泛的评估，证明了该损失函数的有效性。<details>
<summary>Abstract</summary>
We consider learning a probabilistic classifier from partially-labelled supervision (inputs denoted with multiple possibilities) using standard neural architectures with a softmax as the final layer. We identify a bias phenomenon that can arise from the softmax layer in even simple architectures that prevents proper exploration of alternative options, making the dynamics of gradient descent overly sensitive to initialisation. We introduce a novel loss function that allows for unbiased exploration within the space of alternative outputs. We give a theoretical justification for our loss function, and provide an extensive evaluation of its impact on synthetic data, on standard partially labelled benchmarks and on a contributed novel benchmark related to an existing rule learning challenge.
</details>
<details>
<summary>摘要</summary>
我们考虑从受限的指导下学习一个概率分类器，使用标准的神经网络架构，并在最终层使用softmax层。我们发现在简单的架构中，softmax层可能会带来偏袋现象，使得梯度下降的几何对初始化的敏感性。我们介绍了一个新的损失函数，允许不偏的探索多个出力空间中的选项。我们提供了理论上的说明，并对实验数据、标准受限 benchmark和一个新的规则学习挑战中的贡献 benchmark进行了广泛的评估。
</details></li>
</ul>
<hr>
<h2 id="FedDefender-Backdoor-Attack-Defense-in-Federated-Learning"><a href="#FedDefender-Backdoor-Attack-Defense-in-Federated-Learning" class="headerlink" title="FedDefender: Backdoor Attack Defense in Federated Learning"></a>FedDefender: Backdoor Attack Defense in Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08672">http://arxiv.org/abs/2307.08672</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/warisgill/FedDefender">https://github.com/warisgill/FedDefender</a></li>
<li>paper_authors: Waris Gill, Ali Anwar, Muhammad Ali Gulzar</li>
<li>for: 防止targeted poisoning攻击在 Federated Learning (FL) 中</li>
<li>methods: 利用差异测试方法来识别可疑客户端（包含后门）</li>
<li>results: 在 MNIST 和 FashionMNIST 数据集上，FedDefender 有效地 mitigates 攻击，从而降低攻击成功率至 10%，无需对全球模型性能产生负面影响。<details>
<summary>Abstract</summary>
Federated Learning (FL) is a privacy-preserving distributed machine learning technique that enables individual clients (e.g., user participants, edge devices, or organizations) to train a model on their local data in a secure environment and then share the trained model with an aggregator to build a global model collaboratively. In this work, we propose FedDefender, a defense mechanism against targeted poisoning attacks in FL by leveraging differential testing. Our proposed method fingerprints the neuron activations of clients' models on the same input and uses differential testing to identify a potentially malicious client containing a backdoor. We evaluate FedDefender using MNIST and FashionMNIST datasets with 20 and 30 clients, and our results demonstrate that FedDefender effectively mitigates such attacks, reducing the attack success rate (ASR) to 10\% without deteriorating the global model performance.
</details>
<details>
<summary>摘要</summary>
federated 学习（FL）是一种隐私保护的分布式机器学习技术，允许个体客户端（例如用户参与者、边缘设备或组织）在安全环境中使用本地数据进行模型训练，然后将训练好的模型分享给汇总器以建立全球模型。在这项工作中，我们提议了 FedDefender，一种针对攻击型投毒攻击的防御机制，通过对客户端模型的神经元活动进行差异测试来识别可能有恶意后门的客户端。我们使用 MNIST 和 FashionMNIST 数据集，并与 20 和 30 个客户端进行评估，结果表明，FedDefender 有效地防止了这类攻击，攻击成功率降低至 10%，而全球模型性能不受影响。
</details></li>
</ul>
<hr>
<h2 id="Conformer-LLMs-–-Convolution-Augmented-Large-Language-Models"><a href="#Conformer-LLMs-–-Convolution-Augmented-Large-Language-Models" class="headerlink" title="Conformer LLMs – Convolution Augmented Large Language Models"></a>Conformer LLMs – Convolution Augmented Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00461">http://arxiv.org/abs/2307.00461</a></li>
<li>repo_url: None</li>
<li>paper_authors: Prateek Verma</li>
<li>for: 这个研究旨在将两种受欢迎的神经架构combined，即卷积层和Transformers，以应用于大型语言模型（LLMs）。</li>
<li>methods: 这个研究使用了非 causal 的卷积层，并将它们应用于 causal 的训练架构中。transformers decoder 优化了跨多modalities的长距离依赖，并成为现代机器学习的核心进步。</li>
<li>results: 这个研究获得了显著的性能提升，并显示了一个可靠的语音架构，可以在 causal 设置中进行集成和适应。<details>
<summary>Abstract</summary>
This work builds together two popular blocks of neural architecture, namely convolutional layers and Transformers, for large language models (LLMs). Non-causal conformers are used ubiquitously in automatic speech recognition. This work aims to adapt these architectures in a causal setup for training LLMs. Transformers decoders effectively capture long-range dependencies over several modalities and form a core backbone of modern advancements in machine learning. Convolutional architectures have been popular in extracting features in domains such as raw 1-D signals, speech, and images, to name a few. In this paper, by combining local and global dependencies over latent representations using causal convolutional filters and Transformer, we achieve significant gains in performance. This work showcases a robust speech architecture that can be integrated and adapted in a causal setup beyond speech applications for large-scale language modeling.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="GenRec-Large-Language-Model-for-Generative-Recommendation"><a href="#GenRec-Large-Language-Model-for-Generative-Recommendation" class="headerlink" title="GenRec: Large Language Model for Generative Recommendation"></a>GenRec: Large Language Model for Generative Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00457">http://arxiv.org/abs/2307.00457</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rutgerswiselab/genrec">https://github.com/rutgerswiselab/genrec</a></li>
<li>paper_authors: Jianchao Ji, Zelong Li, Shuyuan Xu, Wenyue Hua, Yingqiang Ge, Juntao Tan, Yongfeng Zhang</li>
<li>For: This paper presents a novel approach to recommendation systems using large language models (LLMs) based on text data, which can directly generate the target item to recommend rather than calculating ranking scores for each candidate item.* Methods: The proposed approach leverages the vast knowledge encoded in large language models to accomplish recommendation tasks, and uses specialized prompts to enhance the ability of LLM to comprehend recommendation tasks.* Results: The proposed GenRec approach has significant better results on large datasets, and the experiments show the potential of LLM-based generative recommendation in revolutionizing the domain of recommendation systems.Here’s the summary in Chinese:* 用这篇论文，我们提出了一种基于文本数据的推荐系统方法，可以直接生成目标项目的推荐，而不需要计算每个候选项目的排名分数。* 我们的方法利用大型自然语言模型（LLM）的优秀表现能力，实现推荐任务。我们使用特殊的提示来增强LLM的理解能力，以便更好地理解用户喜好和项目特性。* 我们的实验结果显示，我们的GenRec方法在大型数据集上有很好的表现，并证明了LLM基于生成推荐的潜在力量。<details>
<summary>Abstract</summary>
In recent years, large language models (LLM) have emerged as powerful tools for diverse natural language processing tasks. However, their potential for recommender systems under the generative recommendation paradigm remains relatively unexplored. This paper presents an innovative approach to recommendation systems using large language models (LLMs) based on text data. In this paper, we present a novel LLM for generative recommendation (GenRec) that utilized the expressive power of LLM to directly generate the target item to recommend, rather than calculating ranking score for each candidate item one by one as in traditional discriminative recommendation. GenRec uses LLM's understanding ability to interpret context, learn user preferences, and generate relevant recommendation. Our proposed approach leverages the vast knowledge encoded in large language models to accomplish recommendation tasks. We first we formulate specialized prompts to enhance the ability of LLM to comprehend recommendation tasks. Subsequently, we use these prompts to fine-tune the LLaMA backbone LLM on a dataset of user-item interactions, represented by textual data, to capture user preferences and item characteristics. Our research underscores the potential of LLM-based generative recommendation in revolutionizing the domain of recommendation systems and offers a foundational framework for future explorations in this field. We conduct extensive experiments on benchmark datasets, and the experiments shows that our GenRec has significant better results on large dataset.
</details>
<details>
<summary>摘要</summary>
We first formulate specialized prompts to enhance the ability of LLM to comprehend recommendation tasks. Then, we use these prompts to fine-tune the LLaMA backbone LLM on a dataset of user-item interactions, represented by textual data, to capture user preferences and item characteristics. Our research highlights the potential of LLM-based generative recommendation in revolutionizing the domain of recommendation systems and provides a foundational framework for future explorations in this field. We conduct extensive experiments on benchmark datasets, and the results show that our GenRec method has significantly better performance on large datasets.
</details></li>
</ul>
<hr>
<h2 id="3D-IDS-Doubly-Disentangled-Dynamic-Intrusion-Detection"><a href="#3D-IDS-Doubly-Disentangled-Dynamic-Intrusion-Detection" class="headerlink" title="3D-IDS: Doubly Disentangled Dynamic Intrusion Detection"></a>3D-IDS: Doubly Disentangled Dynamic Intrusion Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11079">http://arxiv.org/abs/2307.11079</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenyang Qiu, Yingsheng Geng, Junrui Lu, Kaida Chen, Shitong Zhu, Ya Su, Guoshun Nan, Can Zhang, Junsong Fu, Qimei Cui, Xiaofeng Tao</li>
<li>for: 提高网络入侵检测系统（NIDS）的检测精度和可解释性，帮助旁减不良攻击对信息基础设施的威胁。</li>
<li>methods: 提出了一种基于两步特征分解和动态图diffusion算法的新方法（3D-IDS），通过自动对复杂的攻击特征进行非参数化优化，生成攻击特征表示，并使用动态图diffusion方法进行空间时间聚合，有效地识别多种攻击，包括未知威胁和已知威胁。</li>
<li>results: 实验表明，3D-IDS可以有效地识别多种攻击，包括未知威胁和已知威胁，并且比现有方法更高的检测精度和可解释性。<details>
<summary>Abstract</summary>
Network-based intrusion detection system (NIDS) monitors network traffic for malicious activities, forming the frontline defense against increasing attacks over information infrastructures. Although promising, our quantitative analysis shows that existing methods perform inconsistently in declaring various unknown attacks (e.g., 9% and 35% F1 respectively for two distinct unknown threats for an SVM-based method) or detecting diverse known attacks (e.g., 31% F1 for the Backdoor and 93% F1 for DDoS by a GCN-based state-of-the-art method), and reveals that the underlying cause is entangled distributions of flow features. This motivates us to propose 3D-IDS, a novel method that aims to tackle the above issues through two-step feature disentanglements and a dynamic graph diffusion scheme. Specifically, we first disentangle traffic features by a non-parameterized optimization based on mutual information, automatically differentiating tens and hundreds of complex features of various attacks. Such differentiated features will be fed into a memory model to generate representations, which are further disentangled to highlight the attack-specific features. Finally, we use a novel graph diffusion method that dynamically fuses the network topology for spatial-temporal aggregation in evolving data streams. By doing so, we can effectively identify various attacks in encrypted traffics, including unknown threats and known ones that are not easily detected. Experiments show the superiority of our 3D-IDS. We also demonstrate that our two-step feature disentanglements benefit the explainability of NIDS.
</details>
<details>
<summary>摘要</summary>
To address these issues, we propose 3D-IDS, a novel method that utilizes two-step feature disentanglement and a dynamic graph diffusion scheme. First, we disentangle traffic features using a non-parameterized optimization based on mutual information, automatically differentiating tens and hundreds of complex features of various attacks. These differentiated features are then fed into a memory model to generate representations, which are further disentangled to highlight attack-specific features. Finally, we use a novel graph diffusion method that dynamically fuses the network topology for spatial-temporal aggregation in evolving data streams. This enables effective identification of various attacks in encrypted traffics, including unknown threats and known ones that are not easily detected.Experiments show the superiority of our 3D-IDS. Additionally, we demonstrate that our two-step feature disentanglements improve the explainability of NIDS.
</details></li>
</ul>
<hr>
<h2 id="An-Adaptive-Optimization-Approach-to-Personalized-Financial-Incentives-in-Mobile-Behavioral-Weight-Loss-Interventions"><a href="#An-Adaptive-Optimization-Approach-to-Personalized-Financial-Incentives-in-Mobile-Behavioral-Weight-Loss-Interventions" class="headerlink" title="An Adaptive Optimization Approach to Personalized Financial Incentives in Mobile Behavioral Weight Loss Interventions"></a>An Adaptive Optimization Approach to Personalized Financial Incentives in Mobile Behavioral Weight Loss Interventions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00444">http://arxiv.org/abs/2307.00444</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiaomei Li, Kara L. Gavin, Corrine I. Voils, Yonatan Mintz</li>
<li>for: 本研究旨在设计个性化的营养干预，使用直接金钱奖励来鼓励身高减轻，同时保持在研究预算内。</li>
<li>methods: 本研究使用机器学习方法预测参与者如何响应不同奖励计划，并在Behavioral 干预中使用这些预测来定制奖励。</li>
<li>results: 研究结果表明，个性化奖励设计可以提高营养干预的效果和经济性。<details>
<summary>Abstract</summary>
Obesity is a critical healthcare issue affecting the United States. The least risky treatments available for obesity are behavioral interventions meant to promote diet and exercise. Often these interventions contain a mobile component that allows interventionists to collect participants level data and provide participants with incentives and goals to promote long term behavioral change. Recently, there has been interest in using direct financial incentives to promote behavior change. However, adherence is challenging in these interventions, as each participant will react differently to different incentive structure and amounts, leading researchers to consider personalized interventions. The key challenge for personalization, is that the clinicians do not know a priori how best to administer incentives to participants, and given finite intervention budgets how to disburse costly resources efficiently. In this paper, we consider this challenge of designing personalized weight loss interventions that use direct financial incentives to motivate weight loss while remaining within a budget. We create a machine learning approach that is able to predict how individuals may react to different incentive schedules within the context of a behavioral intervention. We use this predictive model in an adaptive framework that over the course of the intervention computes what incentives to disburse to participants and remain within the study budget. We provide both theoretical guarantees for our modeling and optimization approaches as well as demonstrate their performance in a simulated weight loss study. Our results highlight the cost efficiency and effectiveness of our personalized intervention design for weight loss.
</details>
<details>
<summary>摘要</summary>
肥胖是美国医疗系统中的一个严重问题。最安全有效的肥胖治疗方法是行为改变方法，包括提倡饮食和运动。这些方法经常包括移动组件，允许 interveners 收集参与者的数据并为参与者提供激励和目标，以促进长期行为变化。在最近，有兴趣使用直接金钱激励来促进行为变化。然而，遵循性困难，因为每个参与者都会不同地对不同的激励结构和金额响应不同。这导致研究人员考虑个性化 intervención。个性化挑战是，临床医生不知道在先知道如何向参与者分配激励，以及如何有效地分配有限的投资资源。在这篇论文中，我们考虑这个个性化肥胖损重优化问题。我们开发了一种机器学习方法，可以预测参与者如何响应不同的激励计划。我们使用这个预测模型，在行为改变方法中进行adaptive框架，在训练期间计算怎样分配激励，以保持在研究预算内。我们提供了理论保证和优化方法的实践表现，并在模拟的肥胖损重研究中证明了我们的个性化 intervención的成本效果。我们的结果表明，我们的个性化 intervención设计可以有效地促进肥胖损重。
</details></li>
</ul>
<hr>
<h2 id="One-Copy-Is-All-You-Need-Resource-Efficient-Streaming-of-Medical-Imaging-Data-at-Scale"><a href="#One-Copy-Is-All-You-Need-Resource-Efficient-Streaming-of-Medical-Imaging-Data-at-Scale" class="headerlink" title="One Copy Is All You Need: Resource-Efficient Streaming of Medical Imaging Data at Scale"></a>One Copy Is All You Need: Resource-Efficient Streaming of Medical Imaging Data at Scale</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00438">http://arxiv.org/abs/2307.00438</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/um2ii/openjphpy">https://github.com/um2ii/openjphpy</a></li>
<li>paper_authors: Pranav Kulkarni, Adway Kanhere, Eliot Siegel, Paul H. Yi, Vishwa S. Parekh</li>
<li>for: 这篇论文是为了解决医疗影像数据集大量化问题，并且提高人工智能工具的开发速度。</li>
<li>methods: 这篇论文使用了一个开源框架called MIST，实现了进度分辨率的运算过程，允许用户在不同的分辨率下载取医疗影像。</li>
<li>results: 这篇论文的结果显示，使用MIST可以将医疗影像集中存储和流式处理的设备不足问题降低&gt;90%，并且维持深度学习应用中的诊断质量。<details>
<summary>Abstract</summary>
Large-scale medical imaging datasets have accelerated development of artificial intelligence tools for clinical decision support. However, the large size of these datasets is a bottleneck for users with limited storage and bandwidth. Many users may not even require such large datasets as AI models are often trained on lower resolution images. If users could directly download at their desired resolution, storage and bandwidth requirements would significantly decrease. However, it is impossible to anticipate every users' requirements and impractical to store the data at multiple resolutions. What if we could store images at a single resolution but send them at different ones? We propose MIST, an open-source framework to operationalize progressive resolution for streaming medical images at multiple resolutions from a single high-resolution copy. We demonstrate that MIST can dramatically reduce imaging infrastructure inefficiencies for hosting and streaming medical images by >90%, while maintaining diagnostic quality for deep learning applications.
</details>
<details>
<summary>摘要</summary>
大规模医疗影像数据集的扩大已经推动了人工智能工具的临床决策支持发展。然而，这些大规模数据集的大小成为了用户储存和带宽限制的瓶颈。许多用户可能不需要这样大的数据集，因为人工智能模型通常是在更低的分辨率图像上训练的。如果用户可以直接下载他们所需的分辨率，储存和带宽需求将会减少很多。然而，预测每个用户的需求是不可能的，并且存储数据在多个分辨率下是不实用的。我们提出了MIST框架，一个开源的框架，用于实现进行式分辨率的流动医疗影像。我们示示了MIST可以减少医疗影像基础设施的不fficient的使用>90%，而且保持深度学习应用的诊断质量。
</details></li>
</ul>
<hr>
<h2 id="Data-Driven-Design-for-Metamaterials-and-Multiscale-Systems-A-Review"><a href="#Data-Driven-Design-for-Metamaterials-and-Multiscale-Systems-A-Review" class="headerlink" title="Data-Driven Design for Metamaterials and Multiscale Systems: A Review"></a>Data-Driven Design for Metamaterials and Multiscale Systems: A Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05506">http://arxiv.org/abs/2307.05506</a></li>
<li>repo_url: None</li>
<li>paper_authors: Doksoo Lee, Wei Wayne Chen, Liwei Wang, Yu-Chin Chan, Wei Chen</li>
<li>for: 这篇论文旨在探讨数据驱动设计方法在Meta材料设计中的潜力。</li>
<li>methods: 该论文使用数据收集、机器学习基于单元细胞设计和数据驱动多尺度优化等方法来实现数据驱动设计。</li>
<li>results: 论文提出了一种束缚数据驱动设计的总体方法，并将现有研究分为数据驱动模块，包括数据收集、机器学习基于单元细胞设计和数据驱动多尺度优化等方法。<details>
<summary>Abstract</summary>
Metamaterials are artificial materials designed to exhibit effective material parameters that go beyond those found in nature. Composed of unit cells with rich designability that are assembled into multiscale systems, they hold great promise for realizing next-generation devices with exceptional, often exotic, functionalities. However, the vast design space and intricate structure-property relationships pose significant challenges in their design. A compelling paradigm that could bring the full potential of metamaterials to fruition is emerging: data-driven design. In this review, we provide a holistic overview of this rapidly evolving field, emphasizing the general methodology instead of specific domains and deployment contexts. We organize existing research into data-driven modules, encompassing data acquisition, machine learning-based unit cell design, and data-driven multiscale optimization. We further categorize the approaches within each module based on shared principles, analyze and compare strengths and applicability, explore connections between different modules, and identify open research questions and opportunities.
</details>
<details>
<summary>摘要</summary>
美特材料是人造材料，旨在实现自然界之外的效果。它们由单元细胞组合而成，单元细胞具有丰富的设计性，可以组成多尺度系统。这些材料具有极高的潜在功能，但是设计困难重大，因为设计空间庞大，结构-性能关系复杂。一种吸引人的思想是数据驱动设计，这种思想在这篇文章中得到了详细的介绍。我们将现有的研究分为三个数据驱动模块：数据收集、机器学习基于单元细胞设计和数据驱动多尺度优化。每个模块都包含不同的方法，我们根据共同原则分类和分析它们。我们还探讨了不同模块之间的连接，并评估了各模块的优劣和适用范围。最后，我们还提出了一些未解决的研究问题和机遇。
</details></li>
</ul>
<hr>
<h2 id="Sparsity-aware-generalization-theory-for-deep-neural-networks"><a href="#Sparsity-aware-generalization-theory-for-deep-neural-networks" class="headerlink" title="Sparsity-aware generalization theory for deep neural networks"></a>Sparsity-aware generalization theory for deep neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00426">http://arxiv.org/abs/2307.00426</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ramchandran Muthukumar, Jeremias Sulam</li>
<li>for: 本研究旨在探讨深度人工神经网络的泛化能力，并提出一种新的分析方法来解释这种泛化能力。</li>
<li>methods: 本研究使用了深度循环神经网络，并开发了一种基于隐藏层活动的度量方法来衡量模型的泛化能力。</li>
<li>results: 研究发现，隐藏层活动的度量可以用于衡量模型的泛化能力，并且可以提供非虚假的下界，即使模型具有较高的参数数量。<details>
<summary>Abstract</summary>
Deep artificial neural networks achieve surprising generalization abilities that remain poorly understood. In this paper, we present a new approach to analyzing generalization for deep feed-forward ReLU networks that takes advantage of the degree of sparsity that is achieved in the hidden layer activations. By developing a framework that accounts for this reduced effective model size for each input sample, we are able to show fundamental trade-offs between sparsity and generalization. Importantly, our results make no strong assumptions about the degree of sparsity achieved by the model, and it improves over recent norm-based approaches. We illustrate our results numerically, demonstrating non-vacuous bounds when coupled with data-dependent priors in specific settings, even in over-parametrized models.
</details>
<details>
<summary>摘要</summary>
深度人工神经网络实现了奇异的泛化能力，这些能力尚未得到充分理解。在这篇论文中，我们提出了一种新的分析泛化方法，利用隐藏层活动的稀畴程度来考虑。我们开发了一个考虑这个减少的有效模型大小的框架，以便为每个输入样本表示基准。我们可以显示泛化和稀畴之间存在基本的负面关系，这些结果不假设模型达到了哪个水平的稀畴程度。我们的结果超过了最近的 нор-based方法。我们通过数值计算示出了非虚无关的下界，即使在过参数化模型中。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Algorithms-for-Relaxed-Pareto-Set-Identification"><a href="#Adaptive-Algorithms-for-Relaxed-Pareto-Set-Identification" class="headerlink" title="Adaptive Algorithms for Relaxed Pareto Set Identification"></a>Adaptive Algorithms for Relaxed Pareto Set Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00424">http://arxiv.org/abs/2307.00424</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cyrille Kone, Emilie Kaufmann, Laura Richert</li>
<li>for: 本文研究了一种多目标多枪支牛津模型中的固定信任性识别Pareto优点集的问题。由于确定精确的Pareto集可能需要很大的样本量，因此研究了一种允许输出一些近似优点枪支的放松。此外，本文还研究了其他放松方法，允许Identify一个相关的Pareto集子集。</li>
<li>methods: 本文提出了一种单一的抽样策略，called Adaptive Pareto Exploration，可以与不同的停止规则结合使用，以满足不同的放松。本文还分析了不同组合的抽样复杂度，特别是在寻找最多$k$ Pareto优点枪支时的减少样本复杂度。</li>
<li>results: 本文在一个实际应用中展示了Adaptive Pareto Exploration的良好实践性，在考虑多个免疫力标准时选择 Covid-19 疫苗策略的问题上。<details>
<summary>Abstract</summary>
In this paper we revisit the fixed-confidence identification of the Pareto optimal set in a multi-objective multi-armed bandit model. As the sample complexity to identify the exact Pareto set can be very large, a relaxation allowing to output some additional near-optimal arms has been studied. In this work we also tackle alternative relaxations that allow instead to identify a relevant subset of the Pareto set. Notably, we propose a single sampling strategy, called Adaptive Pareto Exploration, that can be used in conjunction with different stopping rules to take into account different relaxations of the Pareto Set Identification problem. We analyze the sample complexity of these different combinations, quantifying in particular the reduction in sample complexity that occurs when one seeks to identify at most $k$ Pareto optimal arms. We showcase the good practical performance of Adaptive Pareto Exploration on a real-world scenario, in which we adaptively explore several vaccination strategies against Covid-19 in order to find the optimal ones when multiple immunogenicity criteria are taken into account.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们重新审视了多目标多机枪猎猎人模型中固定信心的标准化集的预测。由于样本复杂度来确定精确的Pareto集可以非常大，因此有关输出一些附加的近似优致的机枪的放弃措施的研究。在这种工作中，我们也研究了不同的放弃措施，允许在Pareto集标准化问题中确定一个相关的子集。特别是，我们提出了一种单一的采样策略，called Adaptive Pareto Exploration，可以与不同的停止规则结合使用，以便在不同的放弃措施中考虑不同的Pareto集标准化问题。我们分析了不同组合的样本复杂度，特别是在寻找最多$k$ Pareto优致机枪时的样本复杂度减少。我们还展示了Adaptive Pareto Exploration在一个实际场景中的良好实践性，在多种immunogenicity标准下对covid-19疫苗的适应性进行了可控的探索。
</details></li>
</ul>
<hr>
<h2 id="JoinBoost-Grow-Trees-Over-Normalized-Data-Using-Only-SQL"><a href="#JoinBoost-Grow-Trees-Over-Normalized-Data-Using-Only-SQL" class="headerlink" title="JoinBoost: Grow Trees Over Normalized Data Using Only SQL"></a>JoinBoost: Grow Trees Over Normalized Data Using Only SQL</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00422">http://arxiv.org/abs/2307.00422</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zezhou Huang, Rathijit Sen, Jiaxiang Liu, Eugene Wu</li>
<li>for: 这种论文的目的是提出一种基于 SQL 的内存中 ML 系统，以避免数据移动和提供数据管理。</li>
<li>methods: 这种系统使用了纯 SQL 语句来训练树型模型，并且可以在任何 DBMS 上运行。</li>
<li>results: 实验表明，JoinBoost 比特有限的 LightGBM 快三倍（1.1倍），并且与现有的内存中 ML 系统相比，速度超过一个数量级。此外，JoinBoost 可以跨越 LightGBM 的特点，包括特性数、数据库大小和Join图复杂度。<details>
<summary>Abstract</summary>
Although dominant for tabular data, ML libraries that train tree models over normalized databases (e.g., LightGBM, XGBoost) require the data to be denormalized as a single table, materialized, and exported. This process is not scalable, slow, and poses security risks. In-DB ML aims to train models within DBMSes to avoid data movement and provide data governance. Rather than modify a DBMS to support In-DB ML, is it possible to offer competitive tree training performance to specialized ML libraries...with only SQL?   We present JoinBoost, a Python library that rewrites tree training algorithms over normalized databases into pure SQL. It is portable to any DBMS, offers performance competitive with specialized ML libraries, and scales with the underlying DBMS capabilities. JoinBoost extends prior work from both algorithmic and systems perspectives. Algorithmically, we support factorized gradient boosting, by updating the $Y$ variable to the residual in the non-materialized join result. Although this view update problem is generally ambiguous, we identify addition-to-multiplication preserving, the key property of variance semi-ring to support rmse, the most widely used criterion. System-wise, we identify residual updates as a performance bottleneck. Such overhead can be natively minimized on columnar DBMSes by creating a new column of residual values and adding it as a projection. We validate this with two implementations on DuckDB, with no or minimal modifications to its internals for portability. Our experiment shows that JoinBoost is 3x (1.1x) faster for random forests (gradient boosting) compared to LightGBM, and over an order magnitude faster than state-of-the-art In-DB ML systems. Further, JoinBoost scales well beyond LightGBM in terms of the # features, DB size (TPC-DS SF=1000), and join graph complexity (galaxy schemas).
</details>
<details>
<summary>摘要</summary>
although dominant for tabular data, machine learning（ML）libraries that train tree models over normalized databases（e.g., LightGBM, XGBoost）require the data to be denormalized as a single table, materialized, and exported. This process is not scalable, slow, and poses security risks. In-DB ML aims to train models within DBMSes to avoid data movement and provide data governance. Rather than modify a DBMS to support In-DB ML, is it possible to offer competitive tree training performance to specialized ML libraries...with only SQL?  We present JoinBoost, a Python library that rewrites tree training algorithms over normalized databases into pure SQL. It is portable to any DBMS, offers performance competitive with specialized ML libraries, and scales with the underlying DBMS capabilities. JoinBoost extends prior work from both algorithmic and systems perspectives. Algorithmically, we support factorized gradient boosting, by updating the $Y$ variable to the residual in the non-materialized join result. Although this view update problem is generally ambiguous, we identify addition-to-multiplication preserving, the key property of variance semi-ring to support rmse, the most widely used criterion. System-wise, we identify residual updates as a performance bottleneck. Such overhead can be natively minimized on columnar DBMSes by creating a new column of residual values and adding it as a projection. We validate this with two implementations on DuckDB, with no or minimal modifications to its internals for portability. Our experiment shows that JoinBoost is 3x (1.1x) faster for random forests (gradient boosting) compared to LightGBM, and over an order magnitude faster than state-of-the-art In-DB ML systems. Further, JoinBoost scales well beyond LightGBM in terms of the # features, DB size (TPC-DS SF=1000), and join graph complexity (galaxy schemas).
</details></li>
</ul>
<hr>
<h2 id="Provably-Efficient-UCB-type-Algorithms-For-Learning-Predictive-State-Representations"><a href="#Provably-Efficient-UCB-type-Algorithms-For-Learning-Predictive-State-Representations" class="headerlink" title="Provably Efficient UCB-type Algorithms For Learning Predictive State Representations"></a>Provably Efficient UCB-type Algorithms For Learning Predictive State Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00405">http://arxiv.org/abs/2307.00405</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruiquan Huang, Yingbin Liang, Jing Yang</li>
<li>for: 本研究旨在提高累积奖励的策略选择问题，包括Markov决策过程（MDPs）和部分可见MDPs（POMDPs）为特殊情况。</li>
<li>methods: 该研究提出了首个已知的UCB类型方法，基于预测状态表示（PSRs），其中包括一个新的奖励项来Upper bound total variation distance between estimated和true模型。</li>
<li>results: 我们计算出了在线和离线PSRs的样本复杂性下界，并证明了我们的设计的UCB类型算法具有计算效率、最后一轮保证近似优策、和模型准确性的优点。<details>
<summary>Abstract</summary>
The general sequential decision-making problem, which includes Markov decision processes (MDPs) and partially observable MDPs (POMDPs) as special cases, aims at maximizing a cumulative reward by making a sequence of decisions based on a history of observations and actions over time. Recent studies have shown that the sequential decision-making problem is statistically learnable if it admits a low-rank structure modeled by predictive state representations (PSRs). Despite these advancements, existing approaches typically involve oracles or steps that are not computationally efficient. On the other hand, the upper confidence bound (UCB) based approaches, which have served successfully as computationally efficient methods in bandits and MDPs, have not been investigated for more general PSRs, due to the difficulty of optimistic bonus design in these more challenging settings. This paper proposes the first known UCB-type approach for PSRs, featuring a novel bonus term that upper bounds the total variation distance between the estimated and true models. We further characterize the sample complexity bounds for our designed UCB-type algorithms for both online and offline PSRs. In contrast to existing approaches for PSRs, our UCB-type algorithms enjoy computational efficiency, last-iterate guaranteed near-optimal policy, and guaranteed model accuracy.
</details>
<details>
<summary>摘要</summary>
通用顺序决策问题（包括Markov决策过程（MDPs）和部分可见MDPs（POMDPs）为特例）的目标是通过时间序列的决策来 maximize 累积奖励。 recent studies have shown that this problem is statistically learnable if it admits a low-rank structure modeled by predictive state representations (PSRs). However, existing approaches typically involve oracles or computationally inefficient steps. On the other hand, the upper confidence bound (UCB) based approaches, which have been successful in bandits and MDPs, have not been investigated for more general PSRs due to the difficulty of optimistic bonus design in these more challenging settings. This paper proposes the first known UCB-type approach for PSRs, featuring a novel bonus term that upper bounds the total variation distance between the estimated and true models. We further characterize the sample complexity bounds for our designed UCB-type algorithms for both online and offline PSRs. Unlike existing approaches for PSRs, our UCB-type algorithms enjoy computational efficiency, last-iterate guaranteed near-optimal policy, and guaranteed model accuracy.Here's the word-for-word translation of the text into Simplified Chinese:通用顺序决策问题（包括Markov决策过程（MDPs）和部分可见MDPs（POMDPs）为特例）的目标是通过时间序列的决策来 maximize 累积奖励。 recent studies have shown that this problem is statistically learnable if it admits a low-rank structure modeled by predictive state representations (PSRs). however, existing approaches typically involve oracles or computationally inefficient steps. On the other hand, the upper confidence bound (UCB) based approaches, which have been successful in bandits and MDPs, have not been investigated for more general PSRs due to the difficulty of optimistic bonus design in these more challenging settings. this paper proposes the first known UCB-type approach for PSRs, featuring a novel bonus term that upper bounds the total variation distance between the estimated and true models. we further characterize the sample complexity bounds for our designed UCB-type algorithms for both online and offline PSRs. unlike existing approaches for PSRs, our UCB-type algorithms enjoy computational efficiency, last-iterate guaranteed near-optimal policy, and guaranteed model accuracy.
</details></li>
</ul>
<hr>
<h2 id="ProbVLM-Probabilistic-Adapter-for-Frozen-Vison-Language-Models"><a href="#ProbVLM-Probabilistic-Adapter-for-Frozen-Vison-Language-Models" class="headerlink" title="ProbVLM: Probabilistic Adapter for Frozen Vison-Language Models"></a>ProbVLM: Probabilistic Adapter for Frozen Vison-Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00398">http://arxiv.org/abs/2307.00398</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ExplainableML/ProbVLM">https://github.com/ExplainableML/ProbVLM</a></li>
<li>paper_authors: Uddeshya Upadhyay, Shyamgopal Karthik, Massimiliano Mancini, Zeynep Akata</li>
<li>for: 本研究旨在提高大规模视觉语言模型（VLM）的表现，以实现更好的协同运算和模型选择。</li>
<li>methods: 本研究提出了一种 probabilistic adapter，可以在posts-hoc方式中对已经预训练的 VLM 进行概率调整，以估计嵌入空间中的概率分布。</li>
<li>results: 在四个挑战性 dataset 上，包括 COCO、Flickr、CUB 和 Oxford-flowers，研究人员可以通过估计嵌入空间中的概率分布，评估 VLM 的嵌入不确定性，并证明 ProbVLM 在回归任务中表现出色。此外，研究人员还提出了两个现实世界下沉浸任务，即活动学习和模型选择，并证明在这些任务中，估计嵌入空间中的概率分布具有很好的帮助作用。最后，研究人员还介绍了一种基于大规模预训练的潜在扩散模型，用于可见化嵌入分布。<details>
<summary>Abstract</summary>
Large-scale vision-language models (VLMs) like CLIP successfully find correspondences between images and text. Through the standard deterministic mapping process, an image or a text sample is mapped to a single vector in the embedding space. This is problematic: as multiple samples (images or text) can abstract the same concept in the physical world, deterministic embeddings do not reflect the inherent ambiguity in the embedding space. We propose ProbVLM, a probabilistic adapter that estimates probability distributions for the embeddings of pre-trained VLMs via inter/intra-modal alignment in a post-hoc manner without needing large-scale datasets or computing. On four challenging datasets, i.e., COCO, Flickr, CUB, and Oxford-flowers, we estimate the multi-modal embedding uncertainties for two VLMs, i.e., CLIP and BLIP, quantify the calibration of embedding uncertainties in retrieval tasks and show that ProbVLM outperforms other methods. Furthermore, we propose active learning and model selection as two real-world downstream tasks for VLMs and show that the estimated uncertainty aids both tasks. Lastly, we present a novel technique for visualizing the embedding distributions using a large-scale pre-trained latent diffusion model.
</details>
<details>
<summary>摘要</summary>
大规模视力语言模型（VLM）如CLIP成功地找到图像和文本之间的对应关系。通过标准排定 mapping 过程，一个图像或文本样本将映射到 embedding 空间中的单个向量上。这是一个问题：多个样本（图像或文本）可以抽象 Physical 世界中的同一个概念，因此排定 embedding 不会反映 embedding 空间中的内在含义。我们提议 ProbVLM，一种 probabilistic adapter，通过对 pre-trained VLM 的嵌入进行概率分布的估计，在后续方式中无需大规模数据或计算。在四个具有挑战性的 datasets 上，我们估算 pre-trained VLM 的嵌入不确定性，衡量嵌入不确定性的准确性在检索任务中，并示出 ProbVLM 超过其他方法。此外，我们提出了基于 VLM 的活动学习和模型选择两个实际应用任务，并证明估计不确定性可以 aid 这两个任务。最后，我们介绍了一种使用大规模预训练的潜在扩散模型来可见 embedding 分布的新技术。
</details></li>
</ul>
<hr>
<h2 id="MobileViG-Graph-Based-Sparse-Attention-for-Mobile-Vision-Applications"><a href="#MobileViG-Graph-Based-Sparse-Attention-for-Mobile-Vision-Applications" class="headerlink" title="MobileViG: Graph-Based Sparse Attention for Mobile Vision Applications"></a>MobileViG: Graph-Based Sparse Attention for Mobile Vision Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00395">http://arxiv.org/abs/2307.00395</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sldgroup/mobilevig">https://github.com/sldgroup/mobilevig</a></li>
<li>paper_authors: Mustafa Munir, William Avery, Radu Marculescu<br>for: This paper proposes a new graph-based sparse attention mechanism (SVGA) and a hybrid CNN-GNN architecture (MobileViG) for vision tasks on mobile devices.methods: The proposed SVGA mechanism is designed to reduce the computational cost of representing images as graph structures, while the MobileViG architecture combines SVGA with a CNN backbone.results: Extensive experiments show that MobileViG outperforms existing ViG models and mobile CNN and ViT architectures in terms of accuracy and&#x2F;or speed on image classification, object detection, and instance segmentation tasks. The fastest model, MobileViG-Ti, achieves 75.7% top-1 accuracy with 0.78 ms inference latency on iPhone 13 Mini NPU, while the largest model, MobileViG-B, obtains 82.6% top-1 accuracy with only 2.30 ms latency.<details>
<summary>Abstract</summary>
Traditionally, convolutional neural networks (CNN) and vision transformers (ViT) have dominated computer vision. However, recently proposed vision graph neural networks (ViG) provide a new avenue for exploration. Unfortunately, for mobile applications, ViGs are computationally expensive due to the overhead of representing images as graph structures. In this work, we propose a new graph-based sparse attention mechanism, Sparse Vision Graph Attention (SVGA), that is designed for ViGs running on mobile devices. Additionally, we propose the first hybrid CNN-GNN architecture for vision tasks on mobile devices, MobileViG, which uses SVGA. Extensive experiments show that MobileViG beats existing ViG models and existing mobile CNN and ViT architectures in terms of accuracy and/or speed on image classification, object detection, and instance segmentation tasks. Our fastest model, MobileViG-Ti, achieves 75.7% top-1 accuracy on ImageNet-1K with 0.78 ms inference latency on iPhone 13 Mini NPU (compiled with CoreML), which is faster than MobileNetV2x1.4 (1.02 ms, 74.7% top-1) and MobileNetV2x1.0 (0.81 ms, 71.8% top-1). Our largest model, MobileViG-B obtains 82.6% top-1 accuracy with only 2.30 ms latency, which is faster and more accurate than the similarly sized EfficientFormer-L3 model (2.77 ms, 82.4%). Our work proves that well designed hybrid CNN-GNN architectures can be a new avenue of exploration for designing models that are extremely fast and accurate on mobile devices. Our code is publicly available at https://github.com/SLDGroup/MobileViG.
</details>
<details>
<summary>摘要</summary>
传统上，卷积神经网络（CNN）和视Transformer（ViT）在计算机视觉领域占据主导地位，但最近提出的视图图神经网络（ViG）提供了一个新的探索方向。然而，由于图像表示为图结构所带来的计算开销，ViG在移动设备上是计算昂贵的。在这种情况下，我们提出了一种新的图像 sparse attention机制——图像 sparse vision graph attention（SVGA），用于适应移动设备上的 ViG 运行。此外，我们还提出了首个在移动设备上使用 CNN-GNN 架构的 Hybrid CNN-GNN 模型——MobileViG，该模型使用 SVGA。我们的实验表明，MobileViG 在图像分类、物体检测和实例 segmentation 任务上比现有的 ViG 模型和现有的移动 CNN 和 ViT 架构更高的准确率和/或运行速度。我们的最快模型，MobileViG-Ti，在 ImageNet-1K 上达到了 75.7% 的顶部 1 准确率，并且在 iPhone 13 Mini NPU 上编译 CoreML 时间为 0.78 ms，比 MobileNetV2x1.4 (1.02 ms, 74.7% top-1) 和 MobileNetV2x1.0 (0.81 ms, 71.8% top-1) 更快。我们的最大模型，MobileViG-B，在 82.6% 的顶部 1 准确率下，只需 2.30 ms 的时间，这比 EfficientFormer-L3 模型 (2.77 ms, 82.4%) 更快和更准确。我们的工作证明了，通过设计合适的 Hybrid CNN-GNN 架构，可以在移动设备上设计出EXTREMELY FAST和EXTREMELY ACCURATE的模型。我们的代码可以在 <https://github.com/SLDGroup/MobileViG> 上获取。
</details></li>
</ul>
<hr>
<h2 id="CasTGAN-Cascaded-Generative-Adversarial-Network-for-Realistic-Tabular-Data-Synthesis"><a href="#CasTGAN-Cascaded-Generative-Adversarial-Network-for-Realistic-Tabular-Data-Synthesis" class="headerlink" title="CasTGAN: Cascaded Generative Adversarial Network for Realistic Tabular Data Synthesis"></a>CasTGAN: Cascaded Generative Adversarial Network for Realistic Tabular Data Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00384">http://arxiv.org/abs/2307.00384</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/abedshantti/castgan">https://github.com/abedshantti/castgan</a></li>
<li>paper_authors: Abdallah Alshantti, Damiano Varagnolo, Adil Rasheed, Aria Rahmati, Frank Westad</li>
<li>for: 本文提出了一种基于生成对抗网络（GAN）的方法，用于生成具有真实性的表格数据，特别是关注Validity问题。</li>
<li>methods: 本文提出了一种级联的表格GAN框架（CasTGAN），通过级联的扩展，使生成的数据更加真实地反映原始数据中的特征相互关系。</li>
<li>results: 实验结果表明，CasTGAN能够很好地捕捉原始数据中特征之间的相互关系和约束，尤其是高维数据集。此外，对模型进行一些扰动处理可以提高模型对特定攻击的抗性。<details>
<summary>Abstract</summary>
Generative adversarial networks (GANs) have drawn considerable attention in recent years for their proven capability in generating synthetic data which can be utilized for multiple purposes. While GANs have demonstrated tremendous successes in producing synthetic data samples that replicate the dynamics of the original datasets, the validity of the synthetic data and the underlying privacy concerns represent major challenges which are not sufficiently addressed. In this work, we design a cascaded tabular GAN framework (CasTGAN) for generating realistic tabular data with a specific focus on the validity of the output. In this context, validity refers to the the dependency between features that can be found in the real data, but is typically misrepresented by traditional generative models. Our key idea entails that employing a cascaded architecture in which a dedicated generator samples each feature, the synthetic output becomes more representative of the real data. Our experimental results demonstrate that our model well captures the constraints and the correlations between the features of the real data, especially the high dimensional datasets. Furthermore, we evaluate the risk of white-box privacy attacks on our model and subsequently show that applying some perturbations to the auxiliary learners in CasTGAN increases the overall robustness of our model against targeted attacks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Residual-based-attention-and-connection-to-information-bottleneck-theory-in-PINNs"><a href="#Residual-based-attention-and-connection-to-information-bottleneck-theory-in-PINNs" class="headerlink" title="Residual-based attention and connection to information bottleneck theory in PINNs"></a>Residual-based attention and connection to information bottleneck theory in PINNs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.00379">http://arxiv.org/abs/2307.00379</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/soanagno/rba-pinns">https://github.com/soanagno/rba-pinns</a></li>
<li>paper_authors: Sokratis J. Anagnostopoulos, Juan Diego Toscano, Nikolaos Stergiopulos, George Em Karniadakis</li>
<li>for: 本研究旨在提高物理学习机制中的数据集成效率和无缝性。</li>
<li>methods: 该研究提出了一种高效、不需要梯度的重量规则，用于加速物理学习机制中的动态或静态系统的收敛。该简单 yet effective 的注意力机制是基于系统的演化准确误差，并且不需要额外的计算成本或反向学习。</li>
<li>results: 该研究表明，该重量规则可以在标准优化器上实现相对 $L^{2}$ 误差在 $10^{-5}$ 水平。此外，通过分析训练过程中的权重演化，研究人员发现了两个不同的学习阶段，与信息瓶颈理论（IB）中的匹配和扩散阶段相似。<details>
<summary>Abstract</summary>
Driven by the need for more efficient and seamless integration of physical models and data, physics-informed neural networks (PINNs) have seen a surge of interest in recent years. However, ensuring the reliability of their convergence and accuracy remains a challenge. In this work, we propose an efficient, gradient-less weighting scheme for PINNs, that accelerates the convergence of dynamic or static systems. This simple yet effective attention mechanism is a function of the evolving cumulative residuals and aims to make the optimizer aware of problematic regions at no extra computational cost or adversarial learning. We illustrate that this general method consistently achieves a relative $L^{2}$ error of the order of $10^{-5}$ using standard optimizers on typical benchmark cases of the literature. Furthermore, by investigating the evolution of weights during training, we identify two distinct learning phases reminiscent of the fitting and diffusion phases proposed by the information bottleneck (IB) theory. Subsequent gradient analysis supports this hypothesis by aligning the transition from high to low signal-to-noise ratio (SNR) with the transition from fitting to diffusion regimes of the adopted weights. This novel correlation between PINNs and IB theory could open future possibilities for understanding the underlying mechanisms behind the training and stability of PINNs and, more broadly, of neural operators.
</details>
<details>
<summary>摘要</summary>
驱动了更高效和无缝的物理模型和数据集成的需求，物理学 informed neural networks（PINNs）在最近几年内得到了广泛的关注。然而，保证其减少和精度的可靠性仍然是一个挑战。在这种工作中，我们提出了一种高效的无梯度权重方案，用于加速动态或静态系统的PINNs的收敛。这种简单 yet effective的注意力机制是函数所处的积累差异，并且在不Extra的计算成本或对抗学习的情况下，使得优化器对问题地带出更多的注意。我们示出，这种通用方法可以在典型的文献中的测试案例中实现相对的L2误差为10^-5水平。此外，通过分析训练过程中权重的发展，我们发现了IB理论中的两个不同学习阶段，即“适应阶段”和“扩散阶段”。这种预测和权重的采用支持了这一假设，并且对PINNs和更广泛的神经运算器的稳定性和训练机制的理解带来了新的可能性。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://example.com/2023/07/02/cs.LG_2023_07_02/" data-id="cllsjvzbj000gf588d5gw1rut" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/15/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/14/">14</a><a class="page-number" href="/page/15/">15</a><span class="page-number current">16</span><a class="page-number" href="/page/17/">17</a><a class="extend next" rel="next" href="/page/17/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CR/">cs.CR</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">43</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">42</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">44</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">53</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">114</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'', root:''}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
