
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/15/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.LG_2023_07_07" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/07/cs.LG_2023_07_07/" class="article-date">
  <time datetime="2023-07-06T16:00:00.000Z" itemprop="datePublished">2023-07-07</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/07/cs.LG_2023_07_07/">cs.LG - 2023-07-07 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Differentiable-Turbulence"><a href="#Differentiable-Turbulence" class="headerlink" title="Differentiable Turbulence"></a>Differentiable Turbulence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03683">http://arxiv.org/abs/2307.03683</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tumaer/JAXFLUIDS">https://github.com/tumaer/JAXFLUIDS</a></li>
<li>paper_authors: Varun Shankar, Romit Maulik, Venkatasubramanian Viswanathan</li>
<li>for: 这个论文旨在提出一种基于深度学习的大气动力学液体流动模型，以提高二维液体动力学中的涨潮层次粗细规范模型的准确性。</li>
<li>methods: 这个论文使用了可微分的液体动力学，并结合物理恰当的深度学习架构来学习高效和通用的涨潮层次粗细规范模型。</li>
<li>results: 研究发现，包含小规模非本地特征是最关键的，以实现有效的涨潮层次粗细规范模型，而大规模特征可以提高解 posteriori 解场的点对应精度。模型可以在不同的流动配置下进行普适化，包括不同的 Reynolds 数和冲击条件。<details>
<summary>Abstract</summary>
Deep learning is increasingly becoming a promising pathway to improving the accuracy of sub-grid scale (SGS) turbulence closure models for large eddy simulations (LES). We leverage the concept of differentiable turbulence, whereby an end-to-end differentiable solver is used in combination with physics-inspired choices of deep learning architectures to learn highly effective and versatile SGS models for two-dimensional turbulent flow. We perform an in-depth analysis of the inductive biases in the chosen architectures, finding that the inclusion of small-scale non-local features is most critical to effective SGS modeling, while large-scale features can improve pointwise accuracy of the a-posteriori solution field. The filtered velocity gradient tensor can be mapped directly to the SGS stress via decomposition of the inputs and outputs into isotropic, deviatoric, and anti-symmetric components. We see that the model can generalize to a variety of flow configurations, including higher and lower Reynolds numbers and different forcing conditions. We show that the differentiable physics paradigm is more successful than offline, a-priori learning, and that hybrid solver-in-the-loop approaches to deep learning offer an ideal balance between computational efficiency, accuracy, and generalization. Our experiments provide physics-based recommendations for deep-learning based SGS modeling for generalizable closure modeling of turbulence.
</details>
<details>
<summary>摘要</summary>
深度学习在提高大涨规（SGS）涨规模型精度方面表现越来越有前途。我们利用了可导ifferentiable turbulence的概念，其中一个可导ifferentiable solver与物理启发的深度学习架构结合使用，以学习高效和多样化的SGS模型。我们进行了深入的杂散偏见分析，发现包含小规模非本地特征是最重要的SGS模型化特征，而大规模特征可以提高 posteriori 解场中点精度。 filtered velocity gradient tensor 可以直接映射到 SGS 压力，通过输入和输出的归一化、异常值分解和反对映射。我们发现模型可以通过不同的流场配置和强制条件进行泛化，包括不同 Reynolds 数和强制条件。我们还证明了可导ifferentiable physics  парадиг是一个更成功的方法，而不是离线、先验学习。我们的实验结果为深度学习基于 SGS 模型的涨规模型化提供物理学习的建议。
</details></li>
</ul>
<hr>
<h2 id="GeoPhy-Differentiable-Phylogenetic-Inference-via-Geometric-Gradients-of-Tree-Topologies"><a href="#GeoPhy-Differentiable-Phylogenetic-Inference-via-Geometric-Gradients-of-Tree-Topologies" class="headerlink" title="GeoPhy: Differentiable Phylogenetic Inference via Geometric Gradients of Tree Topologies"></a>GeoPhy: Differentiable Phylogenetic Inference via Geometric Gradients of Tree Topologies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03675">http://arxiv.org/abs/2307.03675</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/m1m0r1/geophy">https://github.com/m1m0r1/geophy</a></li>
<li>paper_authors: Takahiro Mimori, Michiaki Hamada</li>
<li>for: 理解生物数据中的演化关系，即使考虑分子遗传学模型的不确定性。</li>
<li>methods: 使用可 diferenciable 的形式ulation来进行phylogenetic inference，利用连续几何空间中的特有表示方式来表示树图分布。</li>
<li>results: 在使用实际 benchmark 数据进行实验中，GeoPhy 方法与其他 approximate Bayesian 方法相比，显著地提高了性能。<details>
<summary>Abstract</summary>
Phylogenetic inference, grounded in molecular evolution models, is essential for understanding the evolutionary relationships in biological data. Accounting for the uncertainty of phylogenetic tree variables, which include tree topologies and evolutionary distances on branches, is crucial for accurately inferring species relationships from molecular data and tasks requiring variable marginalization. Variational Bayesian methods are key to developing scalable, practical models; however, it remains challenging to conduct phylogenetic inference without restricting the combinatorially vast number of possible tree topologies. In this work, we introduce a novel, fully differentiable formulation of phylogenetic inference that leverages a unique representation of topological distributions in continuous geometric spaces. Through practical considerations on design spaces and control variates for gradient estimations, our approach, GeoPhy, enables variational inference without limiting the topological candidates. In experiments using real benchmark datasets, GeoPhy significantly outperformed other approximate Bayesian methods that considered whole topologies.
</details>
<details>
<summary>摘要</summary>
生物数据中的进化关系理解需要基于分子进化模型的phylogenetic inference。考虑phylogenetic树变量的不确定性，包括树 topology和演化距离在支持下，是准确推断物种关系和基于分子数据的任务需要变量聚合的关键。variational Bayesian方法是开发可扩展、实用模型的关键，但是不限定可能的树体系数量是一个挑战。在这种情况下，我们介绍了一种新的、完全 differentiable的phylogenetic inference形式，利用连续几何空间中特有的树分布表示。通过实践设计空间和控制变量的考虑，我们的方法GeoPhy可以在不限定树体系数量的情况下进行变量整合。在使用实际 benchmark数据进行实验中，GeoPhy表现出了与其他approximate Bayesian方法相比的显著优势。
</details></li>
</ul>
<hr>
<h2 id="Simulation-free-Schrodinger-bridges-via-score-and-flow-matching"><a href="#Simulation-free-Schrodinger-bridges-via-score-and-flow-matching" class="headerlink" title="Simulation-free Schrödinger bridges via score and flow matching"></a>Simulation-free Schrödinger bridges via score and flow matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03672">http://arxiv.org/abs/2307.03672</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/atong01/conditional-flow-matching">https://github.com/atong01/conditional-flow-matching</a></li>
<li>paper_authors: Alexander Tong, Nikolay Malkin, Kilian Fatras, Lazar Atanackovic, Yanlei Zhang, Guillaume Huguet, Guy Wolf, Yoshua Bengio</li>
<li>for: 学习细胞动态模型，即生物学中细胞的行为和变化。</li>
<li>methods: 使用SF2M方法，即 simulation-free score and flow matching 方法，不需要 simulations 来学习细胞动态模型。这种方法基于 Schrödinger bridge 问题，使用 static entropy-regularized optimal transport 或者 minibatch approximation 来有效地学习 SB 问题。</li>
<li>results: 通过应用 SF2M 方法，可以准确地模型高维细胞动态模型，并且可以回归知道的基因调控网络。此外，SF2M 方法比之前的 simulate-based 方法更高效和更准确。<details>
<summary>Abstract</summary>
We present simulation-free score and flow matching ([SF]$^2$M), a simulation-free objective for inferring stochastic dynamics given unpaired source and target samples drawn from arbitrary distributions. Our method generalizes both the score-matching loss used in the training of diffusion models and the recently proposed flow matching loss used in the training of continuous normalizing flows. [SF]$^2$M interprets continuous-time stochastic generative modeling as a Schr\"odinger bridge (SB) problem. It relies on static entropy-regularized optimal transport, or a minibatch approximation, to efficiently learn the SB without simulating the learned stochastic process. We find that [SF]$^2$M is more efficient and gives more accurate solutions to the SB problem than simulation-based methods from prior work. Finally, we apply [SF]$^2$M to the problem of learning cell dynamics from snapshot data. Notably, [SF]$^2$M is the first method to accurately model cell dynamics in high dimensions and can recover known gene regulatory networks from simulated data.
</details>
<details>
<summary>摘要</summary>
我们提出了一个无 simulate 的得分和流动匹配（[SF]$^2$M），它是一个无 simulate 的目标，用于对未配对的源和目标样本进行推测 Stochastic 动力学。我们的方法扩展了对于传播模型的训练中使用的得分匹配损失，以及最近提出的流动匹配损失，用于对紧致常态流动的训练。[SF]$^2$M 视为连续时间的泊松桥（SB）问题，并且透过静止 entropy 调整的最佳运输或批处替代方法来快速学习 SB 无需运行学习的数学过程。我们发现 [SF]$^2$M 比从先前的作业中的 simulate 方法更加高效且更精准地解决 SB 问题。最后，我们应用 [SF]$^2$M 来学习细胞动力学从快照数据中。特别是，[SF]$^2$M 是高维度细胞动力学的首个精准模型，并且可以从实验数据中回传知名的遗传因子网络。
</details></li>
</ul>
<hr>
<h2 id="Online-Network-Source-Optimization-with-Graph-Kernel-MAB"><a href="#Online-Network-Source-Optimization-with-Graph-Kernel-MAB" class="headerlink" title="Online Network Source Optimization with Graph-Kernel MAB"></a>Online Network Source Optimization with Graph-Kernel MAB</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03641">http://arxiv.org/abs/2307.03641</a></li>
<li>repo_url: None</li>
<li>paper_authors: Laura Toni, Pascal Frossard</li>
<li>for: 学习在大规模网络中最优化来自未知网络过程的奖励。</li>
<li>methods: 使用图kernels多臂抽象算法和适应性图字典模型来实现在线学习，并使用 Grab-UCB 在线顺序决策策略来学习参数。</li>
<li>results: 在 simulations 中，提议的在线学习算法比基准Offline方法更高效，并且在聚合约束和计算复杂度方面具有更好的性能。<details>
<summary>Abstract</summary>
We propose Grab-UCB, a graph-kernel multi-arms bandit algorithm to learn online the optimal source placement in large scale networks, such that the reward obtained from a priori unknown network processes is maximized. The uncertainty calls for online learning, which suffers however from the curse of dimensionality. To achieve sample efficiency, we describe the network processes with an adaptive graph dictionary model, which typically leads to sparse spectral representations. This enables a data-efficient learning framework, whose learning rate scales with the dimension of the spectral representation model instead of the one of the network. We then propose Grab-UCB, an online sequential decision strategy that learns the parameters of the spectral representation while optimizing the action strategy. We derive the performance guarantees that depend on network parameters, which further influence the learning curve of the sequential decision strategy We introduce a computationally simplified solving method, Grab-arm-Light, an algorithm that walks along the edges of the polytope representing the objective function. Simulations results show that the proposed online learning algorithm outperforms baseline offline methods that typically separate the learning phase from the testing one. The results confirm the theoretical findings, and further highlight the gain of the proposed online learning strategy in terms of cumulative regret, sample efficiency and computational complexity.
</details>
<details>
<summary>摘要</summary>
我们提议Grab-UCB算法，用图kernel多臂牌 Algorithm to learn在大规模网络中的优化来源分配，以 Maximize the reward from a priori unknown network processes. 因为uncertainty calls for online learning, which suffers from the curse of dimensionality. To achieve sample efficiency, we use an adaptive graph dictionary model to describe the network processes, which typically leads to sparse spectral representations. This enables a data-efficient learning framework, whose learning rate scales with the dimension of the spectral representation model instead of the one of the network. We then propose Grab-UCB, an online sequential decision strategy that learns the parameters of the spectral representation while optimizing the action strategy. We derive the performance guarantees that depend on network parameters, which further influence the learning curve of the sequential decision strategy. We introduce a computationally simplified solving method, Grab-arm-Light, an algorithm that walks along the edges of the polytope representing the objective function. Simulation results show that the proposed online learning algorithm outperforms baseline offline methods that typically separate the learning phase from the testing one. The results confirm the theoretical findings, and further highlight the gain of the proposed online learning strategy in terms of cumulative regret, sample efficiency, and computational complexity.Note: The translation is in Simplified Chinese, which is a standardized form of Chinese used in mainland China and Singapore. The translation may vary depending on the region or dialect.
</details></li>
</ul>
<hr>
<h2 id="PAC-bounds-of-continuous-Linear-Parameter-Varying-systems-related-to-neural-ODEs"><a href="#PAC-bounds-of-continuous-Linear-Parameter-Varying-systems-related-to-neural-ODEs" class="headerlink" title="PAC bounds of continuous Linear Parameter-Varying systems related to neural ODEs"></a>PAC bounds of continuous Linear Parameter-Varying systems related to neural ODEs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03630">http://arxiv.org/abs/2307.03630</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dániel Rácz, Mihály Petreczky, Bálint Daróczy</li>
<li>for: 本研究考虑了使用神经ordinary differential equation（neural ODE）在连续时间中学习线性参数变化（LPV）系统。</li>
<li>methods: 本文使用了LPV系统中的 bilinear系统，并证明了一类神经ODE可以被LPV系统中嵌入。作者还提供了一种 Probably Approximately Correct（PAC） bound，用于量化LPV系统相关神经ODE的稳定性。</li>
<li>results: 本文的主要贡献是提供了不依赖于集成时间的PAC bound，用于量化LPV系统相关神经ODE的稳定性。<details>
<summary>Abstract</summary>
We consider the problem of learning Neural Ordinary Differential Equations (neural ODEs) within the context of Linear Parameter-Varying (LPV) systems in continuous-time. LPV systems contain bilinear systems which are known to be universal approximators for non-linear systems. Moreover, a large class of neural ODEs can be embedded into LPV systems. As our main contribution we provide Probably Approximately Correct (PAC) bounds under stability for LPV systems related to neural ODEs. The resulting bounds have the advantage that they do not depend on the integration interval.
</details>
<details>
<summary>摘要</summary>
我们考虑了内联神经ordinary differential equations（内联神经ODE）在连续时间中的学习问题，具体来说是在线性参数变量（LPV）系统中。LPV系统包含bilinear系统，这些系统是非线性系统的通用近似器。此外，大量的内联神经ODE可以被LPV系统中嵌入。作为我们的主要贡献，我们提供了可靠地近似正确（PAC）的下界，这些下界不依赖于集成时间。
</details></li>
</ul>
<hr>
<h2 id="Toward-High-Performance-Energy-and-Power-Battery-Cells-with-Machine-Learning-based-Optimization-of-Electrode-Manufacturing"><a href="#Toward-High-Performance-Energy-and-Power-Battery-Cells-with-Machine-Learning-based-Optimization-of-Electrode-Manufacturing" class="headerlink" title="Toward High-Performance Energy and Power Battery Cells with Machine Learning-based Optimization of Electrode Manufacturing"></a>Toward High-Performance Energy and Power Battery Cells with Machine Learning-based Optimization of Electrode Manufacturing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05521">http://arxiv.org/abs/2307.05521</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marc Duquesnoy, Chaoyue Liu, Vishank Kumar, Elixabete Ayerbe, Alejandro A. Franco</li>
<li>for: 本研究旨在优化锂离子电池电极生产过程，以满足增长的能源需求。特别是锂离子电池生产的优化非常重要，因为它会影响电池在应用中的实际性能。</li>
<li>methods: 本研究提出了一种数据驱动的机器学习（ML）助记录管道，用于对电解质性能进行双目标优化。该管道使得可逆设计制造过程参数，以生产适用于能量或动力应用的电极。这与我们之前的研究相似，在改进电极微结构中提高了电解质传输性能。</li>
<li>results: 我们的结果表明，以高活跃物质和中间固体含量和满化程度为优化目标，可以获得优化的电极。<details>
<summary>Abstract</summary>
The optimization of the electrode manufacturing process is important for upscaling the application of Lithium Ion Batteries (LIBs) to cater for growing energy demand. In particular, LIB manufacturing is very important to be optimized because it determines the practical performance of the cells when the latter are being used in applications such as electric vehicles. In this study, we tackled the issue of high-performance electrodes for desired battery application conditions by proposing a powerful data-driven approach supported by a deterministic machine learning (ML)-assisted pipeline for bi-objective optimization of the electrochemical performance. This ML pipeline allows the inverse design of the process parameters to adopt in order to manufacture electrodes for energy or power applications. The latter work is an analogy to our previous work that supported the optimization of the electrode microstructures for kinetic, ionic, and electronic transport properties improvement. An electrochemical pseudo-two-dimensional model is fed with the electrode properties characterizing the electrode microstructures generated by manufacturing simulations and used to simulate the electrochemical performances. Secondly, the resulting dataset was used to train a deterministic ML model to implement fast bi-objective optimizations to identify optimal electrodes. Our results suggested a high amount of active material, combined with intermediate values of solid content in the slurry and calendering degree, to achieve the optimal electrodes.
</details>
<details>
<summary>摘要</summary>
降低锂离子电池（LIB）生产过程优化的重要性，是因为它会影响电池在实际应用中的实际性。例如，在电动汽车中使用的电池。在这种研究中，我们通过提出一种强大的数据驱动方法，支持由确定性机器学习（ML）托管的双目标优化管道，来解决高性能电极的问题。这个ML管道允许逆向设计生产参数，以生产适用于能量或功率应用的电极。这与我们之前的工作相似，曾经支持电极微结构优化，以提高电池的离子、镁和电子传输性能。一个电化学 Pseudo-二维模型，通过电极性能特征来模拟电化学性能。其次，生成的数据集用于训练一个确定性ML模型，以实现快速双目标优化，并提取最佳电极。我们的结果表明，高活性材料和中值的固体含量和滚筒度，可以实现最佳电极。
</details></li>
</ul>
<hr>
<h2 id="GEANN-Scalable-Graph-Augmentations-for-Multi-Horizon-Time-Series-Forecasting"><a href="#GEANN-Scalable-Graph-Augmentations-for-Multi-Horizon-Time-Series-Forecasting" class="headerlink" title="GEANN: Scalable Graph Augmentations for Multi-Horizon Time Series Forecasting"></a>GEANN: Scalable Graph Augmentations for Multi-Horizon Time Series Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03595">http://arxiv.org/abs/2307.03595</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sitan Yang, Malcolm Wolff, Shankar Ramasubramanian, Vincent Quenneville-Belair, Ronak Metha, Michael W. Mahoney</li>
<li>for: 本研究旨在解决深度神经网络模型在缺乏历史数据的情况下进行多个时间序列预测，即“冷启动”问题。</li>
<li>methods: 该研究提出了一种使用图神经网络（GNN）作为预测器的数据增强方法，通过捕捉多个时间序列之间的复杂关系来增强预测器的编码器。</li>
<li>results: 研究在target应用中，对一家大型电子商务公司的需求预测 task 进行了测试，并表明其方法在小数据集（100K产品）和大数据集（超过200W产品）上均显著提高了模型的总性能，尤其是对于“冷启动”产品（新上市或者Recently out-of-stock）的预测性能具有显著的提升。<details>
<summary>Abstract</summary>
Encoder-decoder deep neural networks have been increasingly studied for multi-horizon time series forecasting, especially in real-world applications. However, to forecast accurately, these sophisticated models typically rely on a large number of time series examples with substantial history. A rapidly growing topic of interest is forecasting time series which lack sufficient historical data -- often referred to as the ``cold start'' problem. In this paper, we introduce a novel yet simple method to address this problem by leveraging graph neural networks (GNNs) as a data augmentation for enhancing the encoder used by such forecasters. These GNN-based features can capture complex inter-series relationships, and their generation process can be optimized end-to-end with the forecasting task. We show that our architecture can use either data-driven or domain knowledge-defined graphs, scaling to incorporate information from multiple very large graphs with millions of nodes. In our target application of demand forecasting for a large e-commerce retailer, we demonstrate on both a small dataset of 100K products and a large dataset with over 2 million products that our method improves overall performance over competitive baseline models. More importantly, we show that it brings substantially more gains to ``cold start'' products such as those newly launched or recently out-of-stock.
</details>
<details>
<summary>摘要</summary>
<<SYS>>传输文本到Simplified Chinese表示。<</SYS>>深度神经网络（encoder-decoder）在实际应用中得到了更多研究，特别是用于多个时间序列预测。然而，为了准确预测，这些复杂的模型通常需要大量的时间序列示例，而且这些示例通常具有较长的历史记录。在这篇论文中，我们介绍了一种新的简单方法，利用图 neural network（GNN）作为编码器的数据扩充，以提高预测性能。这些GNN基于的特征可以捕捉复杂的时间序列之间关系，并且其生成过程可以通过预测任务进行END-TO-END优化。我们表明，我们的架构可以使用数据驱动或定义在领域知识图中的图，并可扩展到涉及多个巨大图的信息。在我们的目标应用中，我们在100000个产品的小数据集和超过2000000个产品的大数据集上进行了实验，并证明了我们的方法在相比基eline模型的情况下提高了总性能。更重要的是，我们发现在“冷启动”产品上（例如新推出或者售罄），我们的方法带来了极大的改善。
</details></li>
</ul>
<hr>
<h2 id="Accelerated-Optimization-Landscape-of-Linear-Quadratic-Regulator"><a href="#Accelerated-Optimization-Landscape-of-Linear-Quadratic-Regulator" class="headerlink" title="Accelerated Optimization Landscape of Linear-Quadratic Regulator"></a>Accelerated Optimization Landscape of Linear-Quadratic Regulator</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03590">http://arxiv.org/abs/2307.03590</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lechen Feng, Yuan-Hua Ni</li>
<li>for: 这篇论文关注的是优化控制领域的线性quadratic regulator（LQR）问题。</li>
<li>methods: 该论文提出了一种基于首频减速优化框架的LQR问题解决方案，并对SLQR和OLQR两种不同情况进行了分别的分析。</li>
<li>results: 研究人员通过提出一种 Lipschitz Hessian 性质的LQR性能函数，以及利用 симплекс 牛顿方法和重启规则来保持连续时间的优化率，实现了对SLQR和OLQR问题的高精度解决。<details>
<summary>Abstract</summary>
Linear-quadratic regulator (LQR) is a landmark problem in the field of optimal control, which is the concern of this paper. Generally, LQR is classified into state-feedback LQR (SLQR) and output-feedback LQR (OLQR) based on whether the full state is obtained. It has been suggested in existing literature that both the SLQR and the OLQR could be viewed as \textit{constrained nonconvex matrix optimization} problems in which the only variable to be optimized is the feedback gain matrix. In this paper, we introduce a first-order accelerated optimization framework of handling the LQR problem, and give its convergence analysis for the cases of SLQR and OLQR, respectively.   Specifically, a Lipschiz Hessian property of LQR performance criterion is presented, which turns out to be a crucial property for the application of modern optimization techniques. For the SLQR problem, a continuous-time hybrid dynamic system is introduced, whose solution trajectory is shown to converge exponentially to the optimal feedback gain with Nesterov-optimal order $1-\frac{1}{\sqrt{\kappa}}$ ($\kappa$ the condition number). Then, the symplectic Euler scheme is utilized to discretize the hybrid dynamic system, and a Nesterov-type method with a restarting rule is proposed that preserves the continuous-time convergence rate, i.e., the discretized algorithm admits the Nesterov-optimal convergence order. For the OLQR problem, a Hessian-free accelerated framework is proposed, which is a two-procedure method consisting of semiconvex function optimization and negative curvature exploitation. In a time $\mathcal{O}(\epsilon^{-7/4}\log(1/\epsilon))$, the method can find an $\epsilon$-stationary point of the performance criterion; this entails that the method improves upon the $\mathcal{O}(\epsilon^{-2})$ complexity of vanilla gradient descent. Moreover, our method provides the second-order guarantee of stationary point.
</details>
<details>
<summary>摘要</summary>
Linear-quadratic regulator (LQR) 是控制理论中的一个标志性问题，这篇文章的研究对象。通常情况下，LQR可以分为基于状态反馈（SLQR）和基于输出反馈（OLQR）两种，根据是否获得全状态。在现有文献中，有人提出了视为非对称矩阵优化问题的思路，其中仅仅是反馈矩阵进行优化。在这篇文章中，我们介绍了一种基于首频加速优化框架，并对 SLQR 和 OLQR 两种情况进行了分别的可控性分析。 Specifically, we present a Lipschitz Hessian property of LQR performance criterion, which turns out to be a crucial property for the application of modern optimization techniques. For the SLQR problem, we introduce a continuous-time hybrid dynamic system, whose solution trajectory is shown to converge exponentially to the optimal feedback gain with Nesterov-optimal order $1-\frac{1}{\sqrt{\kappa}}$ ($\kappa$ the condition number). Then, the symplectic Euler scheme is utilized to discretize the hybrid dynamic system, and a Nesterov-type method with a restarting rule is proposed that preserves the continuous-time convergence rate, i.e., the discretized algorithm admits the Nesterov-optimal convergence order. For the OLQR problem, we propose a Hessian-free accelerated framework, which is a two-procedure method consisting of semiconvex function optimization and negative curvature exploitation. In a time $\mathcal{O}(\epsilon^{-7/4}\log(1/\epsilon))$, the method can find an $\epsilon$-stationary point of the performance criterion; this entails that the method improves upon the $\mathcal{O}(\epsilon^{-2})$ complexity of vanilla gradient descent. Moreover, our method provides the second-order guarantee of stationary point.
</details></li>
</ul>
<hr>
<h2 id="BOF-UCB-A-Bayesian-Optimistic-Frequentist-Algorithm-for-Non-Stationary-Contextual-Bandits"><a href="#BOF-UCB-A-Bayesian-Optimistic-Frequentist-Algorithm-for-Non-Stationary-Contextual-Bandits" class="headerlink" title="BOF-UCB: A Bayesian-Optimistic Frequentist Algorithm for Non-Stationary Contextual Bandits"></a>BOF-UCB: A Bayesian-Optimistic Frequentist Algorithm for Non-Stationary Contextual Bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03587">http://arxiv.org/abs/2307.03587</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicklas Werge, Abdullah Akgül, Melih Kandemir</li>
<li>for: 这个论文旨在提出一种新的 bayesian-optimistic frequentist upper confidence bound（BOF-UCB）算法，用于 Stochastic Contextual Linear Bandits（SCLB）中的非站ARY环境。</li>
<li>methods: 这个算法利用累缲 Bayesian 更新来推算未知的回归参数 posterior distribution，然后使用频quentist方法计算 Upper Confidence Bound（UCB），将最大化预期回归在 posterior distribution 上。</li>
<li>results: 我们提供了 BOF-UCB 的性能理论保证，并在实验中显示它在 Synthetic 数据和 classical control 任务中能够平衡寻找和实现，并且在非站ARY环境中表现比 existing methods 更好。<details>
<summary>Abstract</summary>
We propose a novel Bayesian-Optimistic Frequentist Upper Confidence Bound (BOF-UCB) algorithm for stochastic contextual linear bandits in non-stationary environments. This unique combination of Bayesian and frequentist principles enhances adaptability and performance in dynamic settings. The BOF-UCB algorithm utilizes sequential Bayesian updates to infer the posterior distribution of the unknown regression parameter, and subsequently employs a frequentist approach to compute the Upper Confidence Bound (UCB) by maximizing the expected reward over the posterior distribution. We provide theoretical guarantees of BOF-UCB's performance and demonstrate its effectiveness in balancing exploration and exploitation on synthetic datasets and classical control tasks in a reinforcement learning setting. Our results show that BOF-UCB outperforms existing methods, making it a promising solution for sequential decision-making in non-stationary environments.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的泛bayesian-Optimistic Frequentist Upper Confidence Bound（BOF-UCB）算法，用于非站点环境下的随机contextual linear bandit。这种独特的bayesian和频quentist原则的结合，提高了适应性和性能在动态环境下。BOF-UCB算法通过顺序的bayesian更新来推算未知回归参数的 posterior distribution，然后使用频quentist方法计算最大期望奖励的Upper Confidence Bound（UCB）。我们提供了BOF-UCB性能的理论保证，并在synthetic数据集和 классиcal控制任务中的reinforcement learning Setting中进行了实验证明。我们的结果表明，BOF-UCB超越了现有方法，这使得它成为非站点环境下的sequential decision-making的优秀解决方案。
</details></li>
</ul>
<hr>
<h2 id="ContextLabeler-Dataset-physical-and-virtual-sensors-data-collected-from-smartphone-usage-in-the-wild"><a href="#ContextLabeler-Dataset-physical-and-virtual-sensors-data-collected-from-smartphone-usage-in-the-wild" class="headerlink" title="ContextLabeler Dataset: physical and virtual sensors data collected from smartphone usage in-the-wild"></a>ContextLabeler Dataset: physical and virtual sensors data collected from smartphone usage in-the-wild</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03586">http://arxiv.org/abs/2307.03586</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mattia Giovanni Campana, Franca Delmastro</li>
<li>for: 这 paper 描述了一个数据采集计划和从智能手机传感器获得的相关 daily life 活动的数据集，包括 3 名志愿者在 2 周的时间内进行的数据采集。这个数据集包含超过 45K 个数据样本，每个样本包含 1332 个特征，其中包括运动传感器、运行中的应用程序、附近设备和天气条件等多种物理和虚拟传感器。此外，每个数据样本还关联着一个真实的 Label，描述用户在感测实验中的活动和情况（例如，工作、在餐厅、进行体育活动等）。</li>
<li>methods: 作者使用了智能手机传感器进行数据采集，并没有对用户的行为做任何干扰或限制。这使得收集到的数据成为了一个不受干扰的、真实的数据集，可以用于定义和评估一 broad 范围内的 context-aware 解决方案（包括算法和协议），以适应移动环境中的用户情况变化。</li>
<li>results: 作者收集到了一个包含超过 45K 个数据样本的数据集，每个样本包含 1332 个特征。此外，每个数据样本还关联着一个真实的 Label，描述用户在感测实验中的活动和情况。这个数据集可以用于定义和评估 context-aware 解决方案，以适应移动环境中的用户情况变化。<details>
<summary>Abstract</summary>
This paper describes a data collection campaign and the resulting dataset derived from smartphone sensors characterizing the daily life activities of 3 volunteers in a period of two weeks. The dataset is released as a collection of CSV files containing more than 45K data samples, where each sample is composed by 1332 features related to a heterogeneous set of physical and virtual sensors, including motion sensors, running applications, devices in proximity, and weather conditions. Moreover, each data sample is associated with a ground truth label that describes the user activity and the situation in which she was involved during the sensing experiment (e.g., working, at restaurant, and doing sport activity). To avoid introducing any bias during the data collection, we performed the sensing experiment in-the-wild, that is, by using the volunteers' devices, and without defining any constraint related to the user's behavior. For this reason, the collected dataset represents a useful source of real data to both define and evaluate a broad set of novel context-aware solutions (both algorithms and protocols) that aim to adapt their behavior according to the changes in the user's situation in a mobile environment.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Programmable-Synthetic-Tabular-Data-Generation"><a href="#Programmable-Synthetic-Tabular-Data-Generation" class="headerlink" title="Programmable Synthetic Tabular Data Generation"></a>Programmable Synthetic Tabular Data Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03577">http://arxiv.org/abs/2307.03577</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mark Vero, Mislav Balunović, Martin Vechev</li>
<li>for: 生成具有约束的Tabular数据，以便在具有隐私、数据质量和数据共享限制的情况下进行大量数据的利用。</li>
<li>methods: ProgSyn使用了一种可编程的生成模型，通过在原始数据集上预训练并在基于提供的特定需求自动 derivation的梯度下细化，以确保生成的数据具有高质量并遵循特定需求。</li>
<li>results: ProgSyn在多种约束下达到了新的状态功能，如在Adult数据集上保持同等公平性水平下提高了下游预测性能2.3%。总的来说，ProgSyn提供了一个 versatile 和可 accessible的框架，用于生成具有约束的Tabular数据，并允许特定需求的扩展。<details>
<summary>Abstract</summary>
Large amounts of tabular data remain underutilized due to privacy, data quality, and data sharing limitations. While training a generative model producing synthetic data resembling the original distribution addresses some of these issues, most applications require additional constraints from the generated data. Existing synthetic data approaches are limited as they typically only handle specific constraints, e.g., differential privacy (DP) or increased fairness, and lack an accessible interface for declaring general specifications. In this work, we introduce ProgSyn, the first programmable synthetic tabular data generation algorithm that allows for comprehensive customization over the generated data. To ensure high data quality while adhering to custom specifications, ProgSyn pre-trains a generative model on the original dataset and fine-tunes it on a differentiable loss automatically derived from the provided specifications. These can be programmatically declared using statistical and logical expressions, supporting a wide range of requirements (e.g., DP or fairness, among others). We conduct an extensive experimental evaluation of ProgSyn on a number of constraints, achieving a new state-of-the-art on some, while remaining general. For instance, at the same fairness level we achieve 2.3% higher downstream accuracy than the state-of-the-art in fair synthetic data generation on the Adult dataset. Overall, ProgSyn provides a versatile and accessible framework for generating constrained synthetic tabular data, allowing for specifications that generalize beyond the capabilities of prior work.
</details>
<details>
<summary>摘要</summary>
大量的表格数据因为隐私、数据质量和数据共享限制而尚未得到充分利用。训练一个生成模型生成具有原始分布的 sintetic 数据可以解决一些问题，但大多数应用需要更多的约束来限制生成的数据。现有的 sintetic 数据方法有限，它们通常只能处理特定的约束，如差分隐私（DP）或增强公平，而且缺乏可访问的接口来声明通用规则。在这项工作中，我们介绍ProgSyn，首个可编程的 sintetic 表格数据生成算法，允许用户根据需要进行全面的定制。为保证高质量的生成数据，ProgSyn在原始数据集上预训练生成模型，然后在基于提供的规则自动生成的差分损失上进行细化。这些规则可以使用统计和逻辑表达式进行程序matically声明，支持广泛的要求（例如DP或公平等）。我们在一些约束下进行了广泛的实验测试， achieved 新的状态 искусственный数据生成的状态之一，在一些约束下达到了新的状态之一，而且可以泛化。例如，在保持同等公平水平下，我们在Adult数据集上 achieved 2.3% 更高的下游准确率，比之前的最佳状态更高。总之，ProgSyn 提供了一个通用、可访问的 sintetic 表格数据生成框架，允许用户根据需要声明约束，这些约束可以超越现有的工作的能力。
</details></li>
</ul>
<hr>
<h2 id="One-Step-of-Gradient-Descent-is-Provably-the-Optimal-In-Context-Learner-with-One-Layer-of-Linear-Self-Attention"><a href="#One-Step-of-Gradient-Descent-is-Provably-the-Optimal-In-Context-Learner-with-One-Layer-of-Linear-Self-Attention" class="headerlink" title="One Step of Gradient Descent is Provably the Optimal In-Context Learner with One Layer of Linear Self-Attention"></a>One Step of Gradient Descent is Provably the Optimal In-Context Learner with One Layer of Linear Self-Attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03576">http://arxiv.org/abs/2307.03576</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arvind Mahankali, Tatsunori B. Hashimoto, Tengyu Ma</li>
<li>for: 本研究目的是研究一层线性自注意力层（Transformer）在各种不同噪音和回归函数下的学习行为。</li>
<li>methods: 本研究使用了一层线性自注意力层，并在各种噪音和回归函数下进行了预训练。</li>
<li>results: 研究发现，当covariate从标准高斯分布中采样时，一层线性自注意力层会在最小二乘回归目标下进行单步Gradient Descent（GD）。而在非标准高斯分布下，改变weight vector和响应变量的分布会导致学习的算法发生显著变化。<details>
<summary>Abstract</summary>
Recent works have empirically analyzed in-context learning and shown that transformers trained on synthetic linear regression tasks can learn to implement ridge regression, which is the Bayes-optimal predictor, given sufficient capacity [Aky\"urek et al., 2023], while one-layer transformers with linear self-attention and no MLP layer will learn to implement one step of gradient descent (GD) on a least-squares linear regression objective [von Oswald et al., 2022]. However, the theory behind these observations remains poorly understood. We theoretically study transformers with a single layer of linear self-attention, trained on synthetic noisy linear regression data. First, we mathematically show that when the covariates are drawn from a standard Gaussian distribution, the one-layer transformer which minimizes the pre-training loss will implement a single step of GD on the least-squares linear regression objective. Then, we find that changing the distribution of the covariates and weight vector to a non-isotropic Gaussian distribution has a strong impact on the learned algorithm: the global minimizer of the pre-training loss now implements a single step of $\textit{pre-conditioned}$ GD. However, if only the distribution of the responses is changed, then this does not have a large effect on the learned algorithm: even when the response comes from a more general family of $\textit{nonlinear}$ functions, the global minimizer of the pre-training loss still implements a single step of GD on a least-squares linear regression objective.
</details>
<details>
<summary>摘要</summary>
近期研究探讨了在上下文中学习，并证明了使用生成的线性回归任务训练的变换器可以学习实现ridge回归，即极值优化预测器，只要容量足够大 [Aky\"urek et al., 2023]。另一方面，一层变换器 WITH linear self-attention 和无MLP层会学习实现一步Gradient Descent（GD）在最小二乘线性回归目标上 [von Oswald et al., 2022]。然而，这些观察的理论基础还未得到充分理解。我们在变换器中使用单层线性自注意力进行理论研究，并在生成噪声线性回归数据上训练。我们首先 математиче地表明，当covariates从标准 Gaussian 分布中采样时，一层变换器最小化预训练损失将实现一步GD在最小二乘线性回归目标上。然后，我们发现将covariates和重量 вектор从标准 Gaussian 分布更改为非均勋 Gaussian 分布会导致学习的算法强烈受到影响：全局最小化预训练损失的算法将实现一步pre-conditioned GD。但是，只是更改响应的分布而不是weight vector的分布，则不会导致很大的影响：即使响应来自更一般的非线性函数家族，全局最小化预训练损失的算法仍然会实现一步GD在最小二乘线性回归目标上。
</details></li>
</ul>
<hr>
<h2 id="Smoothing-the-Edges-A-General-Framework-for-Smooth-Optimization-in-Sparse-Regularization-using-Hadamard-Overparametrization"><a href="#Smoothing-the-Edges-A-General-Framework-for-Smooth-Optimization-in-Sparse-Regularization-using-Hadamard-Overparametrization" class="headerlink" title="Smoothing the Edges: A General Framework for Smooth Optimization in Sparse Regularization using Hadamard Overparametrization"></a>Smoothing the Edges: A General Framework for Smooth Optimization in Sparse Regularization using Hadamard Overparametrization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03571">http://arxiv.org/abs/2307.03571</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chris Kolb, Christian L. Müller, Bernd Bischl, David Rügamer</li>
<li>for: 本文提出了一种框架，用于平滑优化目标函数中的 $\ell_q$ 和 $\ell_{p,q}$ 正则化，以实现结构化稀疏性。</li>
<li>methods: 本方法使用了常用的随机梯度下降算法，而不需要特殊的优化算法，从而实现了可导的稀疏正则化无需简化。</li>
<li>results: 我们的方法可以具有与普通的 convex 正则化一样的全局最优解，并且可以保证地具有原始参数化中的本地最优解。此外，我们还提供了一个整合的视角，汇集了不同的参数化Literature中的概念，并对现有方法进行了meaningful扩展。在数值实验中，我们证明了我们的方法的可行性和效果，与常见的凸和非凸正则化相比，能够匹配或超越。<details>
<summary>Abstract</summary>
This paper presents a framework for smooth optimization of objectives with $\ell_q$ and $\ell_{p,q}$ regularization for (structured) sparsity. Finding solutions to these non-smooth and possibly non-convex problems typically relies on specialized optimization routines. In contrast, the method studied here is compatible with off-the-shelf (stochastic) gradient descent that is ubiquitous in deep learning, thereby enabling differentiable sparse regularization without approximations. The proposed optimization transfer comprises an overparametrization of selected model parameters followed by a change of penalties. In the overparametrized problem, smooth and convex $\ell_2$ regularization induces non-smooth and non-convex regularization in the original parametrization. We show that the resulting surrogate problem not only has an identical global optimum but also exactly preserves the local minima. This is particularly useful in non-convex regularization, where finding global solutions is NP-hard and local minima often generalize well. We provide an integrative overview that consolidates various literature strands on sparsity-inducing parametrizations in a general setting and meaningfully extend existing approaches. The feasibility of our approach is evaluated through numerical experiments, demonstrating its effectiveness by matching or outperforming common implementations of convex and non-convex regularizers.
</details>
<details>
<summary>摘要</summary>
（简化中文）这篇论文提出了一种基于 $\ell_q$ 和 $\ell_{p,q}$ 正则化的对象函数的平滑优化框架，用于Structured sparsity。现有的特殊化优化方法通常用于解决这些非短途和非凸问题。然而，提出的方法与深度学习中广泛使用的Stochastic gradient descent（SGD）兼容，可以无需 aproximations 实现 differentiable sparse regularization。优化转移包括在选择的模型参数上进行过参数化，然后改变 penalty。过参数化的问题会导致 smooth 和 convex $\ell_2$ regularization 在原始参数化中induces non-smooth 和 non-convex regularization。我们证明了这个代理问题不仅有identical global optimum，还能够 exactly preserve local minima。这 particualrly useful in non-convex regularization，因为找到全局解是NP-hard，而local minima通常 generalize well。我们提供了一个整合性的Overview，汇集了不同的文献弦线程在一个通用的设定下，并meaningfully extends existing approaches。 feasibility of our approach is evaluated through numerical experiments, demonstrating its effectiveness by matching or outperforming common implementations of convex and non-convex regularizers。
</details></li>
</ul>
<hr>
<h2 id="MALIBO-Meta-learning-for-Likelihood-free-Bayesian-Optimization"><a href="#MALIBO-Meta-learning-for-Likelihood-free-Bayesian-Optimization" class="headerlink" title="MALIBO: Meta-learning for Likelihood-free Bayesian Optimization"></a>MALIBO: Meta-learning for Likelihood-free Bayesian Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03565">http://arxiv.org/abs/2307.03565</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiarong Pan, Stefan Falkner, Felix Berkenkamp, Joaquin Vanschoren</li>
<li>for: 这个研究的目的是提出一种基于meta-学习的bayesian搜寻（BO）方法，以便更快地优化成本高的黑盒函数。</li>
<li>methods: 这个方法不使用代理模型，而是直接从不同任务之间的相似性学习query的价值。它还包括一个辅助模型，以便在新任务中进行稳定的适应。</li>
<li>results: 实验结果显示，这个方法在不同的benchmark中展示了强大的任何时间性和超越了现有的meta-学习BO方法。<details>
<summary>Abstract</summary>
Bayesian optimization (BO) is a popular method to optimize costly black-box functions. While traditional BO optimizes each new target task from scratch, meta-learning has emerged as a way to leverage knowledge from related tasks to optimize new tasks faster. However, existing meta-learning BO methods rely on surrogate models that suffer from scalability issues and are sensitive to observations with different scales and noise types across tasks. Moreover, they often overlook the uncertainty associated with task similarity. This leads to unreliable task adaptation when only limited observations are obtained or when the new tasks differ significantly from the related tasks. To address these limitations, we propose a novel meta-learning BO approach that bypasses the surrogate model and directly learns the utility of queries across tasks. Our method explicitly models task uncertainty and includes an auxiliary model to enable robust adaptation to new tasks. Extensive experiments show that our method demonstrates strong anytime performance and outperforms state-of-the-art meta-learning BO methods in various benchmarks.
</details>
<details>
<summary>摘要</summary>
bayesian 优化（BO）是一种常用的优化昂贵黑色函数的方法。而传统的 BO 每次新的目标任务都从scratch开始优化，而 meta-learning 则是一种能够借鉴相关任务来快速优化新任务的方法。然而，现有的 meta-learning BO 方法通常依赖于不准确的代理模型，这些模型受到任务规模和噪声的影响，导致缺乏可扩展性和可靠性。此外，它们经常忽略任务之间的uncertainty。这会导致在只有有限的观察数据时或者新任务与相关任务存在差异时，task adaptation 不可靠。为了解决这些限制，我们提出了一种新的 meta-learning BO 方法，该方法 circumvents 代理模型，直接在任务间学习查询的用于性。我们的方法显式地模型任务的uncertainty，并采用 auxilary 模型来实现鲁棒的任务适应性。我们的实验表明，我们的方法在不同的 benchmark 中具有强大的任何时间性和超越了当前的 meta-learning BO 方法。
</details></li>
</ul>
<hr>
<h2 id="DWReCO-at-CheckThat-2023-Enhancing-Subjectivity-Detection-through-Style-based-Data-Sampling"><a href="#DWReCO-at-CheckThat-2023-Enhancing-Subjectivity-Detection-through-Style-based-Data-Sampling" class="headerlink" title="DWReCO at CheckThat! 2023: Enhancing Subjectivity Detection through Style-based Data Sampling"></a>DWReCO at CheckThat! 2023: Enhancing Subjectivity Detection through Style-based Data Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03550">http://arxiv.org/abs/2307.03550</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ipek Baris Schlicht, Lynn Khellaf, Defne Altiok</li>
<li>for: 本研究投稿于CheckThat! Lab的主观检测任务中，以解决任务中的类别不均衡问题。</li>
<li>methods: 使用GPT-3模型生成基于新闻方面的主观检查列表的提问，并使用这些提问生成更多的训练材料。使用扩展的训练集进行语言特定的变换器模型的练习。</li>
<li>results: 在英语、德语和土耳其语中，发现不同的主观风格都是有效的，而风格基本替换在土耳其语和英语中效果更好。然而，GPT-3模型在非英语语言中生成风格基本文本时有时会表现不佳。<details>
<summary>Abstract</summary>
This paper describes our submission for the subjectivity detection task at the CheckThat! Lab. To tackle class imbalances in the task, we have generated additional training materials with GPT-3 models using prompts of different styles from a subjectivity checklist based on journalistic perspective. We used the extended training set to fine-tune language-specific transformer models. Our experiments in English, German and Turkish demonstrate that different subjective styles are effective across all languages. In addition, we observe that the style-based oversampling is better than paraphrasing in Turkish and English. Lastly, the GPT-3 models sometimes produce lacklustre results when generating style-based texts in non-English languages.
</details>
<details>
<summary>摘要</summary>
这份论文描述了我们在CheckThat! Lab中的主观检测任务提交。为了解决任务中的类别不均衡，我们使用基于新闻业观点的主观检查表 generator GPT-3 模型生成了额外的训练材料。我们使用这些扩展训练集来练化语言特定的转换器模型。我们的实验表明，不同的主观风格在所有语言中都是有效的。此外，我们发现在土耳其语和英语中，风格基本替换比paraphrasing更有效。最后，GPT-3 模型在非英语语言中生成风格基本文本时有时会表现平庸。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Graph-Attention-for-Anomaly-Detection-in-Heterogeneous-Sensor-Networks"><a href="#Dynamic-Graph-Attention-for-Anomaly-Detection-in-Heterogeneous-Sensor-Networks" class="headerlink" title="Dynamic Graph Attention for Anomaly Detection in Heterogeneous Sensor Networks"></a>Dynamic Graph Attention for Anomaly Detection in Heterogeneous Sensor Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03761">http://arxiv.org/abs/2307.03761</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/MengjieZhao/dygatad">https://github.com/MengjieZhao/dygatad</a></li>
<li>paper_authors: Mengjie Zhao, Olga Fink</li>
<li>for: 这篇论文主要是为了探讨运算系统监控中的异常探测问题，尤其是运算系统监控数据中的多变量时间序列（MTS）数据，并提出了一个基于图形的异常探测方法来解决这个问题。</li>
<li>methods: 这篇论文提出了一个名为 DyGATAD（动态图形注意力异常探测）的图形基于异常探测方法，这个方法利用注意力机制来建构一个当前运算系统监控数据中的连续图形表示，并且考虑了系统中各个时间序列之间的关系变化。</li>
<li>results: 这篇论文透过使用实验和工业规模的多相运输设备实验，证明了 DyGATAD 方法在运算系统监控数据中的异常探测能力，特别是在早期发现 fault 的情况下表现出色，甚至在 fault 的严重程度很低时也能够实现高精度的探测。<details>
<summary>Abstract</summary>
In the era of digital transformation, systems monitored by the Industrial Internet of Things (IIoTs) generate large amounts of Multivariate Time Series (MTS) data through heterogeneous sensor networks. While this data facilitates condition monitoring and anomaly detection, the increasing complexity and interdependencies within the sensor network pose significant challenges for anomaly detection. Despite progress in this field, much of the focus has been on point anomalies and contextual anomalies, with lesser attention paid to collective anomalies. A less addressed but common variant of collective anomalies is when the abnormal collective behavior is caused by shifts in interrelationships within the system. This can be due to abnormal environmental conditions like overheating, improper operational settings resulting from cyber-physical attacks, or system-level faults. To address these challenges, this paper proposes DyGATAD (Dynamic Graph Attention for Anomaly Detection), a graph-based anomaly detection framework that leverages the attention mechanism to construct a continuous graph representation of multivariate time series by inferring dynamic edges between time series. DyGATAD incorporates an operating condition-aware reconstruction combined with a topology-based anomaly score, thereby enhancing the detection ability of relationship shifts. We evaluate the performance of DyGATAD using both a synthetic dataset with controlled varying fault severity levels and an industrial-scale multiphase flow facility benchmark featuring various fault types with different detection difficulties. Our proposed approach demonstrated superior performance in collective anomaly detection for sensor networks, showing particular strength in early-stage fault detection, even in the case of faults with minimal severity.
</details>
<details>
<summary>摘要</summary>
在数字转型时代，由工业互联网Of Things（IIoT）监控的系统会生成大量多变量时间序列（MTS）数据，该数据可以帮助condition monitoring和异常检测。然而，随着传感器网络的复杂度和互相关系的增加，异常检测受到了重大挑战。虽然在这个领域已经有了很多进展，但是大多数注意力是集中在点异常和上下文异常上，对集体异常的研究相对较少。一种较少被关注但很常见的集体异常情况是，在系统中的异常 коллектив行为是由系统间关系的变化引起的。这可能是因为环境条件异常（如过热）、不正确的操作设置（由于Cyber-physical attacks）或系统级别的故障。为解决这些挑战，这篇论文提出了 DyGATAD（动态图注意力异常检测），一种基于图的异常检测框架，通过注意力机制来构建多变量时间序列中的连续图表示。DyGATAD结合了运行条件感知重建和图形异常分数，从而提高了关系变化的检测能力。我们使用了一个synthetic数据集和一个工业级多相流设施测试数据来评估DyGATAD的性能。我们的提出的方法在感知器网络中的集体异常检测方面表现出色，特别是在早期异常检测和轻度异常检测方面。
</details></li>
</ul>
<hr>
<h2 id="Roman-Numeral-Analysis-with-Graph-Neural-Networks-Onset-wise-Predictions-from-Note-wise-Features"><a href="#Roman-Numeral-Analysis-with-Graph-Neural-Networks-Onset-wise-Predictions-from-Note-wise-Features" class="headerlink" title="Roman Numeral Analysis with Graph Neural Networks: Onset-wise Predictions from Note-wise Features"></a>Roman Numeral Analysis with Graph Neural Networks: Onset-wise Predictions from Note-wise Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03544">http://arxiv.org/abs/2307.03544</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/manoskary/chordgnn">https://github.com/manoskary/chordgnn</a></li>
<li>paper_authors: Emmanouil Karystinaios, Gerhard Widmer</li>
<li>for: 本研究旨在提出一种新的自动罗马数分析方法，用于把纯 симвоlic music 转化为罗马数表示。</li>
<li>methods: 该方法基于图神经网络（GNNs），可以直接处理每个音符的特征和间接关系。具有新的边减法，使得模型可以生成音符层次的表示。</li>
<li>results: 对于参考数据集，提出的 ChordGNN 模型表现更高精度，比对 existed 状态的艺术模型更高。此外，我们还 investigate 了模型的 variant，包括 NADE 和后处理技术。完整的代码可以在 GitHub 上找到。<details>
<summary>Abstract</summary>
Roman Numeral analysis is the important task of identifying chords and their functional context in pieces of tonal music. This paper presents a new approach to automatic Roman Numeral analysis in symbolic music. While existing techniques rely on an intermediate lossy representation of the score, we propose a new method based on Graph Neural Networks (GNNs) that enable the direct description and processing of each individual note in the score. The proposed architecture can leverage notewise features and interdependencies between notes but yield onset-wise representation by virtue of our novel edge contraction algorithm. Our results demonstrate that ChordGNN outperforms existing state-of-the-art models, achieving higher accuracy in Roman Numeral analysis on the reference datasets. In addition, we investigate variants of our model using proposed techniques such as NADE, and post-processing of the chord predictions. The full source code for this work is available at https://github.com/manoskary/chordgnn
</details>
<details>
<summary>摘要</summary>
Symbolic music中的罗马数字分析是一项重要的任务，旨在识别乐曲中的和声和其功能上下文。这篇论文提出了一种新的自动罗马数字分析方法，基于图神经网络（GNNs），可以直接描述乐曲中每个个音的特征和相互关系。我们的建议可以利用每个音的特征和间隔之间的相互关系，并通过我们的新的边缩合算法将每个音转换为和声表示。我们的结果表明，ChordGNN比现有的状态当前模型高效，在参照数据集上达到更高的罗马数字分析准确率。此外，我们还考虑了我们的模型的变体，使用提出的技术如NADE，以及后处理矩阵预测结果。完整的代码可以在https://github.com/manoskary/chordgnn上获取。
</details></li>
</ul>
<hr>
<h2 id="Do-DL-models-and-training-environments-have-an-impact-on-energy-consumption"><a href="#Do-DL-models-and-training-environments-have-an-impact-on-energy-consumption" class="headerlink" title="Do DL models and training environments have an impact on energy consumption?"></a>Do DL models and training environments have an impact on energy consumption?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05520">http://arxiv.org/abs/2307.05520</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/GAISSA-UPC/seaa2023_ect">https://github.com/GAISSA-UPC/seaa2023_ect</a></li>
<li>paper_authors: Santiago del Rey, Silverio Martínez-Fernández, Luís Cruz, Xavier Franch</li>
<li>for: 降低深度学习模型训练时的碳脚印。</li>
<li>methods: 分析模型架构和训练环境对训练更绿色计算机视觉模型的影响。</li>
<li>results: 选择合适的模型架构和训练环境可以减少能源消耗（最高达98.83%），但 Correctness 下降很小。 GPU 适应模型计算复杂性的增长，以提高能效性。<details>
<summary>Abstract</summary>
Current research in the computer vision field mainly focuses on improving Deep Learning (DL) correctness and inference time performance. However, there is still little work on the huge carbon footprint that has training DL models. This study aims to analyze the impact of the model architecture and training environment when training greener computer vision models. We divide this goal into two research questions. First, we analyze the effects of model architecture on achieving greener models while keeping correctness at optimal levels. Second, we study the influence of the training environment on producing greener models. To investigate these relationships, we collect multiple metrics related to energy efficiency and model correctness during the models' training. Then, we outline the trade-offs between the measured energy efficiency and the models' correctness regarding model architecture, and their relationship with the training environment. We conduct this research in the context of a computer vision system for image classification. In conclusion, we show that selecting the proper model architecture and training environment can reduce energy consumption dramatically (up to 98.83%) at the cost of negligible decreases in correctness. Also, we find evidence that GPUs should scale with the models' computational complexity for better energy efficiency.
</details>
<details>
<summary>摘要</summary>
现有研究主要集中在深度学习（DL）正确性和推理速度表现 improvemen。然而，还没有很多关于训练DL模型的巨大碳脚印的工作。这种研究目标是分析训练绿色计算机视觉模型时的模型架构和训练环境的影响。我们将这个目标分成两个研究问题。第一个问题是分析保持正确性水平时实现绿色模型的模型架构的影响。第二个问题是研究训练环境对生成绿色模型的影响。为了调查这些关系，我们收集了多个能效性和模型正确性的指标 during 模型的训练。然后，我们描述了在模型架构和训练环境的影响下，能效性和正确性之间的交易。我们在图像分类计算机视觉系统中进行了这种研究。结果表明，选择合适的模型架构和训练环境可以减少能 consumption (最多98.83%)，同时对正确性的影响很小。此外，我们发现GPU在模型的计算复杂性增加时应该呈现加速的趋势，以实现更好的能效性。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Graph-Pooling-for-Explainable-Classification-of-Brain-Networks"><a href="#Contrastive-Graph-Pooling-for-Explainable-Classification-of-Brain-Networks" class="headerlink" title="Contrastive Graph Pooling for Explainable Classification of Brain Networks"></a>Contrastive Graph Pooling for Explainable Classification of Brain Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11133">http://arxiv.org/abs/2307.11133</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaxing Xu, Qingtian Bian, Xinhang Li, Aihu Zhang, Yiping Ke, Miao Qiao, Wei Zhang, Wei Khang Jeremy Sim, Balázs Gulyás</li>
<li>for: 这paper是用来探讨Functional magnetic resonance imaging (fMRI)数据的分析方法，特别是用于发现神经退化疾病如parkinson’s disease, Alzheimer’s disease 和autism spectrum disorder.</li>
<li>methods: 这paper使用了图神经网络（GNN）来提取特征，但是需要特殊的设计来适应fMRI数据的特点。该paper提出了一种对比性双重注意块和可微的图汇聚方法（ContrastPool），以更好地利用GNN来挖掘脑网络中的特征。</li>
<li>results: 该paper在5个休息态fMRI脑网络数据集上进行了应用，并证明了其在比较现有基线上的超越。case study表明，该方法提取的特征与 neuroscience文献中的领域知识匹配，并揭示了直观的发现。该paper的贡献表明了ContrastPool在理解脑网络和神经退化疾病方面的潜力。<details>
<summary>Abstract</summary>
Functional magnetic resonance imaging (fMRI) is a commonly used technique to measure neural activation. Its application has been particularly important in identifying underlying neurodegenerative conditions such as Parkinson's, Alzheimer's, and Autism. Recent analysis of fMRI data models the brain as a graph and extracts features by graph neural networks (GNNs). However, the unique characteristics of fMRI data require a special design of GNN. Tailoring GNN to generate effective and domain-explainable features remains challenging. In this paper, we propose a contrastive dual-attention block and a differentiable graph pooling method called ContrastPool to better utilize GNN for brain networks, meeting fMRI-specific requirements. We apply our method to 5 resting-state fMRI brain network datasets of 3 diseases and demonstrate its superiority over state-of-the-art baselines. Our case study confirms that the patterns extracted by our method match the domain knowledge in neuroscience literature, and disclose direct and interesting insights. Our contributions underscore the potential of ContrastPool for advancing the understanding of brain networks and neurodegenerative conditions.
</details>
<details>
<summary>摘要</summary>
функциональная магнитная резонансная томография (fMRI) 是一种常用的技术来测量神经活化。它的应用尤其重要在发现下面的 нейродегенератив Conditions  such as Parkinson's, Alzheimer's, and Autism.  current analysis of fMRI data models the brain as a graph and extracts features by graph neural networks (GNNs). However, the unique characteristics of fMRI data require a special design of GNN. Tailoring GNN to generate effective and domain-explainable features remains challenging. In this paper, we propose a contrastive dual-attention block and a differentiable graph pooling method called ContrastPool to better utilize GNN for brain networks, meeting fMRI-specific requirements. We apply our method to 5 resting-state fMRI brain network datasets of 3 diseases and demonstrate its superiority over state-of-the-art baselines. Our case study confirms that the patterns extracted by our method match the domain knowledge in neuroscience literature, and disclose direct and interesting insights. Our contributions underscore the potential of ContrastPool for advancing the understanding of brain networks and neurodegenerative conditions.Here's the translation in Traditional Chinese as well: функціональна магнітна резонансна томографія (fMRI) 是一种常用的技术来测量神经活化。它的应用尤其重要在发现下面的 нейродегенератив Conditions  such as Parkinson's, Alzheimer's, and Autism.  current analysis of fMRI data models the brain as a graph and extracts features by graph neural networks (GNNs). However, the unique characteristics of fMRI data require a special design of GNN. Tailoring GNN to generate effective and domain-explainable features remains challenging. In this paper, we propose a contrastive dual-attention block and a differentiable graph pooling method called ContrastPool to better utilize GNN for brain networks, meeting fMRI-specific requirements. We apply our method to 5 resting-state fMRI brain network datasets of 3 diseases and demonstrate its superiority over state-of-the-art baselines. Our case study confirms that the patterns extracted by our method match the domain knowledge in neuroscience literature, and disclose direct and interesting insights. Our contributions underscore the potential of ContrastPool for advancing the understanding of brain networks and neurodegenerative conditions.
</details></li>
</ul>
<hr>
<h2 id="Incentive-Allocation-in-Vertical-Federated-Learning-Based-on-Bankruptcy-Problem"><a href="#Incentive-Allocation-in-Vertical-Federated-Learning-Based-on-Bankruptcy-Problem" class="headerlink" title="Incentive Allocation in Vertical Federated Learning Based on Bankruptcy Problem"></a>Incentive Allocation in Vertical Federated Learning Based on Bankruptcy Problem</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03515">http://arxiv.org/abs/2307.03515</a></li>
<li>repo_url: None</li>
<li>paper_authors: Afsana Khan, Marijn ten Thij, Frank Thuijsman, Anna Wilbik</li>
<li>for: 这篇论文探讨了如何为在Vertically Federated Learning（VFL）中活动党（具有标签的数据的党）对不活跃党（没有标签的数据的党）的参与进行奖励。</li>
<li>methods: 本论文使用了 банкрот游戏理论的变形，known as the Bankruptcy Problem，并使用了塔尔散分法解决问题。</li>
<li>results: 本论文透过实验和实际数据显示，证明了其可以保证参与者受益，并且比较了旧的计算Shapley值的方法，表明了其的方法更加有效，需要 fewer computations。<details>
<summary>Abstract</summary>
Vertical federated learning (VFL) is a promising approach for collaboratively training machine learning models using private data partitioned vertically across different parties. Ideally in a VFL setting, the active party (party possessing features of samples with labels) benefits by improving its machine learning model through collaboration with some passive parties (parties possessing additional features of the same samples without labels) in a privacy preserving manner. However, motivating passive parties to participate in VFL can be challenging. In this paper, we focus on the problem of allocating incentives to the passive parties by the active party based on their contributions to the VFL process. We formulate this problem as a variant of the Nucleolus game theory concept, known as the Bankruptcy Problem, and solve it using the Talmud's division rule. We evaluate our proposed method on synthetic and real-world datasets and show that it ensures fairness and stability in incentive allocation among passive parties who contribute their data to the federated model. Additionally, we compare our method to the existing solution of calculating Shapley values and show that our approach provides a more efficient solution with fewer computations.
</details>
<details>
<summary>摘要</summary>
纵向联合学习（VFL）是一种有前途的方法，通过私有数据分区Vertically Across不同的方针进行机器学习模型的共同训练。在VFLSetting中，活跃的方（具有标签的样本的特征）可以通过与一些被动方（不具有标签的样本的特征）的合作来改进其机器学习模型，这样做得有隐私保护的方式。然而，鼓励被动方参与VFL可以是困难的。在这篇论文中，我们关注在给被动方分配奖励的问题上。我们将这个问题定义为变种的核心游戏理论概念——银行rup难题，并使用塔尔摩德分配规则解决。我们对 synthetic 和实际数据集进行了评估，并证明了我们的提议方法能确保在被动方参与VFL过程中奖励分配是公平和稳定的。此外，我们与现有的计算Shapley值的方法进行比较，并证明了我们的方法提供了更高效的解决方案，计算量更少。
</details></li>
</ul>
<hr>
<h2 id="DEFT-Exploiting-Gradient-Norm-Difference-between-Model-Layers-for-Scalable-Gradient-Sparsification"><a href="#DEFT-Exploiting-Gradient-Norm-Difference-between-Model-Layers-for-Scalable-Gradient-Sparsification" class="headerlink" title="DEFT: Exploiting Gradient Norm Difference between Model Layers for Scalable Gradient Sparsification"></a>DEFT: Exploiting Gradient Norm Difference between Model Layers for Scalable Gradient Sparsification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03500">http://arxiv.org/abs/2307.03500</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kljp/deft">https://github.com/kljp/deft</a></li>
<li>paper_authors: Daegun Yoon, Sangyoon Oh</li>
<li>for: 提高分布式深度学习中通信压力过大的问题，提出了一种新的梯度简化方法，即DEFT。</li>
<li>methods: DEFT将梯度选择任务分解成多个子任务，并将其分配给工作者进行并行计算，从而降低了计算成本，并且可以避免梯度积累问题。</li>
<li>results: DEFT在实验中显示出了与现有简化器相比明显的提高，同时保持高度的收敛性。<details>
<summary>Abstract</summary>
Gradient sparsification is a widely adopted solution for reducing the excessive communication traffic in distributed deep learning. However, most existing gradient sparsifiers have relatively poor scalability because of considerable computational cost of gradient selection and/or increased communication traffic owing to gradient build-up. To address these challenges, we propose a novel gradient sparsification scheme, DEFT, that partitions the gradient selection task into sub tasks and distributes them to workers. DEFT differs from existing sparsifiers, wherein every worker selects gradients among all gradients. Consequently, the computational cost can be reduced as the number of workers increases. Moreover, gradient build-up can be eliminated because DEFT allows workers to select gradients in partitions that are non-intersecting (between workers). Therefore, even if the number of workers increases, the communication traffic can be maintained as per user requirement.   To avoid the loss of significance of gradient selection, DEFT selects more gradients in the layers that have a larger gradient norm than the other layers. Because every layer has a different computational load, DEFT allocates layers to workers using a bin-packing algorithm to maintain a balanced load of gradient selection between workers. In our empirical evaluation, DEFT shows a significant improvement in training performance in terms of speed in gradient selection over existing sparsifiers while achieving high convergence performance.
</details>
<details>
<summary>摘要</summary>
分布式深度学习中的梯度简化是一种广泛采用的解决方案，以减少分布式学习中的过度通信交流。然而，现有的大多数梯度简化方法具有较差的可扩展性，因为梯度选择和/或梯度积累带来了较高的计算成本。为了解决这些挑战，我们提出了一种新的梯度简化方案，即DEFT。DEFT方案将梯度选择任务分解成多个子任务，并将它们分配给工作者。与现有的简化方法不同，DEFT中每个工作者不需要选择所有的梯度。因此，计算成本可以随着工作者数量的增加而减少。此外，梯度积累可以被消除，因为DEFT允许工作者在不相交的 partition（ между工作者）中选择梯度。因此，即使工作者数量增加，通信交流也可以保持在用户需求的水平。为了保持梯度选择的重要性，DEFT在各层中选择更多的梯度，以避免梯度简化导致的数据损失。由于每层都有不同的计算负担，DEFT使用一种堆叠算法将层分配给工作者，以保持各个层的梯度选择均衡。在我们的实验评估中，DEFT在速度和稳定性两个方面显示出了明显的改善，而且可以实现高度的并行化。
</details></li>
</ul>
<hr>
<h2 id="HoughLaneNet-Lane-Detection-with-Deep-Hough-Transform-and-Dynamic-Convolution"><a href="#HoughLaneNet-Lane-Detection-with-Deep-Hough-Transform-and-Dynamic-Convolution" class="headerlink" title="HoughLaneNet: Lane Detection with Deep Hough Transform and Dynamic Convolution"></a>HoughLaneNet: Lane Detection with Deep Hough Transform and Dynamic Convolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03494">http://arxiv.org/abs/2307.03494</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jia-Qi Zhang, Hao-Bin Duan, Jun-Long Chen, Ariel Shamir, Miao Wang</li>
<li>for: 本研究旨在提高自动驾驶车辆 Lane detection 的精度，以便更好地满足自动驾驶技术的需求。</li>
<li>methods: 本研究提出了一种基于 hierarchical Deep Hough Transform (DHT) 的方法，利用图像中所有的 Lane 特征在 Hough 参数空间进行组合。此外，还提出了一种改进点选择方法和一种动态卷积模块，以更好地 differentiate  между各个 Lane 特征。</li>
<li>results: 实验结果表明，提出的方法在检测受掩蔽或损坏的 Lane 图像时表现出色，与现有技术相比，其性能有所提高。<details>
<summary>Abstract</summary>
The task of lane detection has garnered considerable attention in the field of autonomous driving due to its complexity. Lanes can present difficulties for detection, as they can be narrow, fragmented, and often obscured by heavy traffic. However, it has been observed that the lanes have a geometrical structure that resembles a straight line, leading to improved lane detection results when utilizing this characteristic. To address this challenge, we propose a hierarchical Deep Hough Transform (DHT) approach that combines all lane features in an image into the Hough parameter space. Additionally, we refine the point selection method and incorporate a Dynamic Convolution Module to effectively differentiate between lanes in the original image. Our network architecture comprises a backbone network, either a ResNet or Pyramid Vision Transformer, a Feature Pyramid Network as the neck to extract multi-scale features, and a hierarchical DHT-based feature aggregation head to accurately segment each lane. By utilizing the lane features in the Hough parameter space, the network learns dynamic convolution kernel parameters corresponding to each lane, allowing the Dynamic Convolution Module to effectively differentiate between lane features. Subsequently, the lane features are fed into the feature decoder, which predicts the final position of the lane. Our proposed network structure demonstrates improved performance in detecting heavily occluded or worn lane images, as evidenced by our extensive experimental results, which show that our method outperforms or is on par with state-of-the-art techniques.
</details>
<details>
<summary>摘要</summary>
自动驾驶领域内，车道检测已经吸引了非常大的关注，因为它的复杂性。车道可能会变窄、散乱或者受到交通干扰，但是观察到的是车道具有几何结构，这使得使用这个特点可以提高车道检测的结果。为了解决这个挑战，我们提出了层次式深度投影变换（DHT）方法，将整个图像中的所有车道特征都归类到投影参数空间中。此外，我们还改进了点选择方法，并在原始图像中添加了动态卷积模块，以有效地将车道特征分化开。我们的网络架构包括后备网络（可以是ResNet或Pyramid Vision Transformer）、特征层次网络作为颈部EXTRACT多个尺度特征，以及层次DHT基于特征归一化头来准确地分类每条车道。通过利用车道特征在投影参数空间中，网络学习了动态卷积kernel参数相应于每条车道，使得动态卷积模块能够有效地分化开车道特征。最后，车道特征被传递给特征解码器，解码器预测了车道的最终位置。我们提出的网络结构在实际实验中表现出色，证明我们的方法在检测受阻或损坏车道图像时表现出优于或与当前领先技术相当。
</details></li>
</ul>
<hr>
<h2 id="ITA-An-Energy-Efficient-Attention-and-Softmax-Accelerator-for-Quantized-Transformers"><a href="#ITA-An-Energy-Efficient-Attention-and-Softmax-Accelerator-for-Quantized-Transformers" class="headerlink" title="ITA: An Energy-Efficient Attention and Softmax Accelerator for Quantized Transformers"></a>ITA: An Energy-Efficient Attention and Softmax Accelerator for Quantized Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03493">http://arxiv.org/abs/2307.03493</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gamze İslamoğlu, Moritz Scherer, Gianna Paulin, Tim Fischer, Victor J. B. Jung, Angelo Garofalo, Luca Benini</li>
<li>for: 本研究旨在提出一种高效的Transformer网络加速器，用于适应嵌入式系统中的自然语言处理任务。</li>
<li>methods: 该研究使用8位量化和创新的软MAX实现，实现在流式模式下计算，从而减少数据移动和能耗。</li>
<li>results: ITA在22nm Fully-Depleted Silicon-on-Insulator技术下，在0.8V voltage下达到16.9 TOPS&#x2F;W的能效率，同时在面积效率方面超过现有的Transformer加速器。<details>
<summary>Abstract</summary>
Transformer networks have emerged as the state-of-the-art approach for natural language processing tasks and are gaining popularity in other domains such as computer vision and audio processing. However, the efficient hardware acceleration of transformer models poses new challenges due to their high arithmetic intensities, large memory requirements, and complex dataflow dependencies. In this work, we propose ITA, a novel accelerator architecture for transformers and related models that targets efficient inference on embedded systems by exploiting 8-bit quantization and an innovative softmax implementation that operates exclusively on integer values. By computing on-the-fly in streaming mode, our softmax implementation minimizes data movement and energy consumption. ITA achieves competitive energy efficiency with respect to state-of-the-art transformer accelerators with 16.9 TOPS/W, while outperforming them in area efficiency with 5.93 TOPS/mm$^2$ in 22 nm fully-depleted silicon-on-insulator technology at 0.8 V.
</details>
<details>
<summary>摘要</summary>
transformer 网络已经成为自然语言处理任务的状态泰斗方法，而在计算机视觉和音频处理领域也越来越受欢迎。然而，加速transformer模型的高精度计算和大量内存需求带来新的挑战。在这篇文章中，我们提出了一种名为ITA的新加速架构，这种架构 targets高效的在嵌入式系统上进行推理，通过8位量化和一种新的软 макс实现，该实现具有快速计算和减少数据移动的特点。ITA在0.8V 22nm Fully-depleted silicon-on-insulator技术中实现了0.8V 22nm Fully-depleted silicon-on-insulator技术中实现了16.9 TOPS/W的能效率，同时也超过了现有的state-of-the-art transformer加速器的5.93 TOPS/mm$^2$ 。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Graph-Convolution-Networks-for-Traffic-Flow-Forecasting"><a href="#Adaptive-Graph-Convolution-Networks-for-Traffic-Flow-Forecasting" class="headerlink" title="Adaptive Graph Convolution Networks for Traffic Flow Forecasting"></a>Adaptive Graph Convolution Networks for Traffic Flow Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05517">http://arxiv.org/abs/2307.05517</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhengdaoli/agc-net">https://github.com/zhengdaoli/agc-net</a></li>
<li>paper_authors: Zhengdao Li, Wei Li, Kai Hwang</li>
<li>for: 预测交通流速度是一项非常具有挑战性的任务，因为道路条件在空间和时间两个维度上是动态变化的。</li>
<li>methods: 本文提出了一种新的 adaptive graph convolution network (AGC-net)，用于解决现有的 graph neural network (GNN) 中忽略时间变化道路条件的问题。AGC-net 基于一种新的上下文注意机制，包括一系列可学习的扩散尺度的 graph wavelets。</li>
<li>results: 实验结果表明，AGC-net 可以准确预测交通流速度，并且与其他基eline模型相比，有 significannot 的提高。两个公共的交通数据集上的实验结果都表明了 AGC-net 的效果。<details>
<summary>Abstract</summary>
Traffic flow forecasting is a highly challenging task due to the dynamic spatial-temporal road conditions. Graph neural networks (GNN) has been widely applied in this task. However, most of these GNNs ignore the effects of time-varying road conditions due to the fixed range of the convolution receptive field. In this paper, we propose a novel Adaptive Graph Convolution Networks (AGC-net) to address this issue in GNN. The AGC-net is constructed by the Adaptive Graph Convolution (AGC) based on a novel context attention mechanism, which consists of a set of graph wavelets with various learnable scales. The AGC transforms the spatial graph representations into time-sensitive features considering the temporal context. Moreover, a shifted graph convolution kernel is designed to enhance the AGC, which attempts to correct the deviations caused by inaccurate topology. Experimental results on two public traffic datasets demonstrate the effectiveness of the AGC-net\footnote{Code is available at: https://github.com/zhengdaoli/AGC-net} which outperforms other baseline models significantly.
</details>
<details>
<summary>摘要</summary>
traffic flow forecasting 是一个非常具有挑战性的任务，因为道路条件在空间和时间上都是动态的。 graph neural networks (GNN) 已经广泛应用于这个任务。然而，大多数这些 GNN 忽略了时间变化的道路条件，因为它们的固定范围的卷积感知场所不能考虑时间上的变化。在这篇论文中，我们提出了一种新的 Adaptive Graph Convolution Networks (AGC-net)，用于解决 GNN 中的这个问题。AGC-net 由 Adaptive Graph Convolution (AGC) 基于一种新的上下文注意机制组成，该机制包括一组可学习的扩散尺度的图波лет。AGC 将空间图表示转化为时间敏感的特征，考虑到时间上的上下文。此外，我们还设计了一个偏移 graph convolution kernel，用于强化 AGC，以尝试修正因为不准确的 topology 所导致的偏差。实验结果表明，AGC-net 在两个公共的交通数据集上表现出色，与其他基准模型相比，具有显著的优势。Note: Please note that the translation is in Simplified Chinese, and the word order may be different from the original text.
</details></li>
</ul>
<hr>
<h2 id="Learning-Theory-of-Distribution-Regression-with-Neural-Networks"><a href="#Learning-Theory-of-Distribution-Regression-with-Neural-Networks" class="headerlink" title="Learning Theory of Distribution Regression with Neural Networks"></a>Learning Theory of Distribution Regression with Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03487">http://arxiv.org/abs/2307.03487</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/djdprogramming/adfa2">https://github.com/djdprogramming/adfa2</a></li>
<li>paper_authors: Zhongjie Shi, Zhan Yu, Ding-Xuan Zhou</li>
<li>for: 本文目的是建立分布回归的近似理论和学习理论，使用完全连接神经网络(FNN)来实现。</li>
<li>methods: 本文使用了一种新的神经网络结构，即分布输入神经网络，来解决传统神经网络不能直接使用于分布输入的问题。</li>
<li>results: 本文通过一种新的两阶段错误分解技术， derivation of almost optimal learning rates for the proposed distribution regression model up to logarithmic terms。<details>
<summary>Abstract</summary>
In this paper, we aim at establishing an approximation theory and a learning theory of distribution regression via a fully connected neural network (FNN). In contrast to the classical regression methods, the input variables of distribution regression are probability measures. Then we often need to perform a second-stage sampling process to approximate the actual information of the distribution. On the other hand, the classical neural network structure requires the input variable to be a vector. When the input samples are probability distributions, the traditional deep neural network method cannot be directly used and the difficulty arises for distribution regression. A well-defined neural network structure for distribution inputs is intensively desirable. There is no mathematical model and theoretical analysis on neural network realization of distribution regression. To overcome technical difficulties and address this issue, we establish a novel fully connected neural network framework to realize an approximation theory of functionals defined on the space of Borel probability measures. Furthermore, based on the established functional approximation results, in the hypothesis space induced by the novel FNN structure with distribution inputs, almost optimal learning rates for the proposed distribution regression model up to logarithmic terms are derived via a novel two-stage error decomposition technique.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们目标是建立分布回归的近似理论和学习理论，通过全连接神经网络（FNN）来实现。与传统的回归方法不同，分布回归的输入变量是概率度量。因此，我们需要进行第二阶采样过程来近似实际分布的信息。然而，传统的神经网络结构需要输入变量为向量。当输入样本是概率分布时，传统的深度神经网络方法无法直接使用，这会导致技术困难。我们需要一种具有良好定义的神经网络结构来处理分布输入。在现有的数学模型和理论分析之外，我们在FNN结构中建立了一个新的分布输入神经网络框架，以实现函数als定义在柯博尔概率度量空间上的近似理论。此外，基于建立的函数近似结果，我们通过一种新的两阶错 decomposition技术，在带有分布输入的FNN结构下， derivation almost optimal learning rate的提案 Distribution Regression模型，即使到对数阶段。
</details></li>
</ul>
<hr>
<h2 id="Discovering-Hierarchical-Achievements-in-Reinforcement-Learning-via-Contrastive-Learning"><a href="#Discovering-Hierarchical-Achievements-in-Reinforcement-Learning-via-Contrastive-Learning" class="headerlink" title="Discovering Hierarchical Achievements in Reinforcement Learning via Contrastive Learning"></a>Discovering Hierarchical Achievements in Reinforcement Learning via Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03486">http://arxiv.org/abs/2307.03486</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seungyong Moon, Junyoung Yeom, Bumsoo Park, Hyun Oh Song</li>
<li>for: 这种研究旨在解决在生成的环境中发现层次结构的成就所存在的挑战。</li>
<li>methods: 该研究使用的方法包括 proximal policy optimization (PPO) 和 achievement distillation。</li>
<li>results: PPO  Agent 可以预测下一个成就的解锁程度，并且通过 achievement distillation 方法强化了 agent 的成就预测能力，显示了在具有更多的模型参数和更高效的样本收集方法下达到了 state-of-the-art 性能。<details>
<summary>Abstract</summary>
Discovering achievements with a hierarchical structure on procedurally generated environments poses a significant challenge. This requires agents to possess a broad range of abilities, including generalization and long-term reasoning. Many prior methods are built upon model-based or hierarchical approaches, with the belief that an explicit module for long-term planning would be beneficial for learning hierarchical achievements. However, these methods require an excessive amount of environment interactions or large model sizes, limiting their practicality. In this work, we identify that proximal policy optimization (PPO), a simple and versatile model-free algorithm, outperforms the prior methods with recent implementation practices. Moreover, we find that the PPO agent can predict the next achievement to be unlocked to some extent, though with low confidence. Based on this observation, we propose a novel contrastive learning method, called achievement distillation, that strengthens the agent's capability to predict the next achievement. Our method exhibits a strong capacity for discovering hierarchical achievements and shows state-of-the-art performance on the challenging Crafter environment using fewer model parameters in a sample-efficient regime.
</details>
<details>
<summary>摘要</summary>
发现具有层次结构的成就需要智能体具备广泛的能力，包括通用化和长期逻辑。许多先前方法基于模型化或层次方法，假设有一个显式的长期规划模块可以帮助学习层次成就。然而，这些方法需要很多环境互动或大型模型，限制其实用性。在这种情况下，我们发现，靠近策略优化（PPO）算法，一种简单而多功能的模型自由算法，在当前实施方法下表现出色，超越先前的方法。此外，我们发现PPO Agent可以预测下一个成就的概率，虽然 confidence 较低。基于这个观察，我们提出了一种新的对比学习方法，即成就馆定，该方法可以增强智能体预测下一个成就的能力。我们的方法在挑战性的 Crafter 环境中表现出色，使用更少的模型参数在样本效率的 régime 中达到了领先的性能。
</details></li>
</ul>
<hr>
<h2 id="Unpaired-Multi-View-Graph-Clustering-with-Cross-View-Structure-Matching"><a href="#Unpaired-Multi-View-Graph-Clustering-with-Cross-View-Structure-Matching" class="headerlink" title="Unpaired Multi-View Graph Clustering with Cross-View Structure Matching"></a>Unpaired Multi-View Graph Clustering with Cross-View Structure Matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03476">http://arxiv.org/abs/2307.03476</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wy1019/upmgc-sm">https://github.com/wy1019/upmgc-sm</a></li>
<li>paper_authors: Yi Wen, Siwei Wang, Qing Liao, Weixuan Liang, Ke Liang, Xinhang Wan, Xinwang Liu</li>
<li>for:  addresses the data-unpaired problem (DUP) in multi-view literature by proposing a novel parameter-free graph clustering framework.</li>
<li>methods:  utilizes the structural information from each view to refine cross-view correspondences, and is a unified framework for both fully and partially unpaired multi-view graph clustering.</li>
<li>results:  extensive experiments demonstrate the effectiveness and generalization of the proposed framework for both paired and unpaired datasets.Here’s the full text in Simplified Chinese:</li>
<li>for:  Addresses the data-unpaired problem (DUP) in multi-view literature by proposing a novel parameter-free graph clustering framework.</li>
<li>methods:  Utilizes the structural information from each view to refine cross-view correspondences, and is a unified framework for both fully and partially unpaired multi-view graph clustering.</li>
<li>results:  Extensive experiments demonstrate the effectiveness and generalization of the proposed framework for both paired and unpaired datasets.<details>
<summary>Abstract</summary>
Multi-view clustering (MVC), which effectively fuses information from multiple views for better performance, has received increasing attention. Most existing MVC methods assume that multi-view data are fully paired, which means that the mappings of all corresponding samples between views are pre-defined or given in advance. However, the data correspondence is often incomplete in real-world applications due to data corruption or sensor differences, referred as the data-unpaired problem (DUP) in multi-view literature. Although several attempts have been made to address the DUP issue, they suffer from the following drawbacks: 1) Most methods focus on the feature representation while ignoring the structural information of multi-view data, which is essential for clustering tasks; 2) Existing methods for partially unpaired problems rely on pre-given cross-view alignment information, resulting in their inability to handle fully unpaired problems; 3) Their inevitable parameters degrade the efficiency and applicability of the models. To tackle these issues, we propose a novel parameter-free graph clustering framework termed Unpaired Multi-view Graph Clustering framework with Cross-View Structure Matching (UPMGC-SM). Specifically, unlike the existing methods, UPMGC-SM effectively utilizes the structural information from each view to refine cross-view correspondences. Besides, our UPMGC-SM is a unified framework for both the fully and partially unpaired multi-view graph clustering. Moreover, existing graph clustering methods can adopt our UPMGC-SM to enhance their ability for unpaired scenarios. Extensive experiments demonstrate the effectiveness and generalization of our proposed framework for both paired and unpaired datasets.
</details>
<details>
<summary>摘要</summary>
多视图划分（MVC），能够有效地将多个视图中的信息结合起来，在最近几年内受到了越来越多的关注。大多数现有的MVC方法假设所有视图中的样本都是已知的，即所有样本之间的映射都是先前定义的。然而，在实际应用中，数据对应关系 oftentimes 是不完全的，这被称为多视图数据不对应问题（DUP）。虽然有几种尝试 Addressing the DUP issue，但它们受到以下缺点的限制：1）大多数方法专注于特征表示，而忽略多视图数据的结构信息，这是 clustering 任务中非常重要的; 2）现有的方法只适用于部分不对应的问题，它们无法处理完全不对应的问题; 3）它们的参数会降低模型的效率和可应用性。为了解决这些问题，我们提出了一种无参数的图 clustering 框架，名为无参数多视图图 clustering 框架 with Cross-View Structure Matching（UPMGC-SM）。与现有方法不同的是，UPMGC-SM 可以充分利用每个视图中的结构信息，以改进 cross-view 对应关系。此外，我们的 UPMGC-SM 是一种通用的框架，可以处理完全不对应和部分不对应的多视图图 clustering 问题。此外，现有的图 clustering 方法可以采用我们的 UPMGC-SM 来增强它们对不对应场景的能力。广泛的实验表明我们提出的框架在 paired 和 unpaired 数据集上的效果和通用性都很强。
</details></li>
</ul>
<hr>
<h2 id="Action-State-Dependent-Dynamic-Model-Selection"><a href="#Action-State-Dependent-Dynamic-Model-Selection" class="headerlink" title="Action-State Dependent Dynamic Model Selection"></a>Action-State Dependent Dynamic Model Selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04754">http://arxiv.org/abs/2307.04754</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francesco Cordoni, Alessio Sancetta</li>
<li>for: 这篇论文目的是为了找到在某些世界状态下最佳的模型，以及在这些状态下如何动态地选择模型。</li>
<li>methods: 这篇论文使用了强化学习算法来近似地解决这个动态程序问题，并且可以从数据中估计最佳策略。</li>
<li>results: 实际应用中，这种方法能够在使用macro经济变量和价格数据时，超过选择最佳股票模型的寻找方法。<details>
<summary>Abstract</summary>
A model among many may only be best under certain states of the world. Switching from a model to another can also be costly. Finding a procedure to dynamically choose a model in these circumstances requires to solve a complex estimation procedure and a dynamic programming problem. A Reinforcement learning algorithm is used to approximate and estimate from the data the optimal solution to this dynamic programming problem. The algorithm is shown to consistently estimate the optimal policy that may choose different models based on a set of covariates. A typical example is the one of switching between different portfolio models under rebalancing costs, using macroeconomic information. Using a set of macroeconomic variables and price data, an empirical application to the aforementioned portfolio problem shows superior performance to choosing the best portfolio model with hindsight.
</details>
<details>
<summary>摘要</summary>
一个模型在多种状况下只是最佳的。从一个模型到另一个的转换也可能是昂贵的。在这些情况下，找到一种动态选择模型的过程需要解决一个复杂的估计问题和动态programming问题。一种强化学习算法可以从数据中approxiamte和估计最佳解决方案。这种算法能够适应不同状况下的模型选择。一个典型的应用是在划转成本下选择不同的投资模型，使用macro经济信息。使用一组macro经济变量和价格数据，对投资问题的一个empirical应用表现出色，比选择划算后的最佳投资模型更高效。
</details></li>
</ul>
<hr>
<h2 id="Solvent-A-Framework-for-Protein-Folding"><a href="#Solvent-A-Framework-for-Protein-Folding" class="headerlink" title="Solvent: A Framework for Protein Folding"></a>Solvent: A Framework for Protein Folding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04603">http://arxiv.org/abs/2307.04603</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kakaobrain/solvent">https://github.com/kakaobrain/solvent</a></li>
<li>paper_authors: Jaemyung Lee, Kyeongtak Han, Jaehoon Kim, Hasun Yu, Youhan Lee</li>
<li>For: The paper aims to provide a unified research framework for protein folding, called Solvent, which supports various state-of-the-art models and enables consistent and fair comparisons among different approaches.* Methods: Solvent is built with a modular design, allowing for different models to be easily integrated and trained on the same dataset. The framework includes implementations of several well-known algorithms and their components, and provides a variety of training and evaluation options.* Results: The paper presents experiments using Solvent to benchmark well-known algorithms and their components, providing insights into the protein structure modeling field. The results demonstrate the potential of Solvent to increase the reliability and consistency of proposed models, as well as improve efficiency in both speed and costs.<details>
<summary>Abstract</summary>
Consistency and reliability are crucial for conducting AI research. Many famous research fields, such as object detection, have been compared and validated with solid benchmark frameworks. After AlphaFold2, the protein folding task has entered a new phase, and many methods are proposed based on the component of AlphaFold2. The importance of a unified research framework in protein folding contains implementations and benchmarks to consistently and fairly compare various approaches. To achieve this, we present Solvent, a protein folding framework that supports significant components of state-of-the-art models in the manner of an off-the-shelf interface Solvent contains different models implemented in a unified codebase and supports training and evaluation for defined models on the same dataset. We benchmark well-known algorithms and their components and provide experiments that give helpful insights into the protein structure modeling field. We hope that Solvent will increase the reliability and consistency of proposed models and give efficiency in both speed and costs, resulting in acceleration on protein folding modeling research. The code is available at https://github.com/kakaobrain/solvent, and the project will continue to be developed.
</details>
<details>
<summary>摘要</summary>
“一致性和可靠性是AI研究中非常重要的。许多著名的研究领域，如对象检测，都已经被比较和验证了坚实的 bencmark 框架。 alphaFold2 后，蛋白质折叠任务进入了新的阶段，许多方法都是基于 alphaFold2 的组件。一个统一的研究框架在蛋白质折叠中的重要性，它可以一直支持当前领先的模型组件，并且可以在同一个代码库中实现和评估定义的模型。我们称之为 Solvent，它支持当前领先的模型组件，并且可以在同一个代码库中实现和评估定义的模型。我们对一些知名的算法和其组件进行了比较，并提供了有用的实验结果，它们可以帮助我们更好地理解蛋白质结构模型领域。我们希望 Solvent 能够增加提案模型的一致性和可靠性，并且能够提高速度和成本的效率，从而加速蛋白质结构模型研究。代码可以在 https://github.com/kakaobrain/solvent 上获取，项目将继续开发。”
</details></li>
</ul>
<hr>
<h2 id="A-Survey-on-Graph-Neural-Networks-for-Time-Series-Forecasting-Classification-Imputation-and-Anomaly-Detection"><a href="#A-Survey-on-Graph-Neural-Networks-for-Time-Series-Forecasting-Classification-Imputation-and-Anomaly-Detection" class="headerlink" title="A Survey on Graph Neural Networks for Time Series: Forecasting, Classification, Imputation, and Anomaly Detection"></a>A Survey on Graph Neural Networks for Time Series: Forecasting, Classification, Imputation, and Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03759">http://arxiv.org/abs/2307.03759</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kimmeen/awesome-gnn4ts">https://github.com/kimmeen/awesome-gnn4ts</a></li>
<li>paper_authors: Ming Jin, Huan Yee Koh, Qingsong Wen, Daniele Zambon, Cesare Alippi, Geoffrey I. Webb, Irwin King, Shirui Pan</li>
<li>for: 本文主要针对时间序列分析领域的研究，旨在帮助设计者和实践者更好地理解、建立应用和推动关于图 neuronal networks for time series analysis（GNN4TS）的研究。</li>
<li>methods: 本文使用了图 neuronal networks（GNN）来分析时间序列数据，并对时间序列分析领域的各种任务进行了分类和推导。</li>
<li>results: 本文对GNN4TS的研究进行了全面的回顾和评估，并介绍了一些代表性的研究和应用例子，同时也预测了未来研究的发展趋势。<details>
<summary>Abstract</summary>
Time series are the primary data type used to record dynamic system measurements and generated in great volume by both physical sensors and online processes (virtual sensors). Time series analytics is therefore crucial to unlocking the wealth of information implicit in available data. With the recent advancements in graph neural networks (GNNs), there has been a surge in GNN-based approaches for time series analysis. These approaches can explicitly model inter-temporal and inter-variable relationships, which traditional and other deep neural network-based methods struggle to do. In this survey, we provide a comprehensive review of graph neural networks for time series analysis (GNN4TS), encompassing four fundamental dimensions: forecasting, classification, anomaly detection, and imputation. Our aim is to guide designers and practitioners to understand, build applications, and advance research of GNN4TS. At first, we provide a comprehensive task-oriented taxonomy of GNN4TS. Then, we present and discuss representative research works and introduce mainstream applications of GNN4TS. A comprehensive discussion of potential future research directions completes the survey. This survey, for the first time, brings together a vast array of knowledge on GNN-based time series research, highlighting foundations, practical applications, and opportunities of graph neural networks for time series analysis.
</details>
<details>
<summary>摘要</summary>
时序序列是主要数据类型，用于记录动态系统测量结果，并且由物理感知器和在线过程生成大量数据（虚拟感知器）。时序序列分析是解锁可用数据中的宝库的关键。随着图神经网络（GNN）的最近进步，GNN-based时序序列分析方法在不断增长。这些方法可以直接模型时间和空间关系，传统和其他深度神经网络基于方法难以完成。在这项调查中，我们提供了完整的图神经网络时序序列分析（GNN4TS）评论，涵盖四个基本维度：预测、分类、异常检测和补做。我们的目标是帮助设计者和实践者理解、建立应用和推动GNN4TS研究。首先，我们提供了完整的任务导向的分类法GNN4TS。然后，我们展示和讨论了代表性的研究工作，并介绍了主流应用GNN4TS。最后，我们对未来研究方向进行了全面的讨论，这项调查，是首次将大量关于GNN基于时序序列研究的知识集中，把注重Foundations、实践应用和机遇的图神经网络时序序列分析。
</details></li>
</ul>
<hr>
<h2 id="Differential-Privacy-for-Clustering-Under-Continual-Observation"><a href="#Differential-Privacy-for-Clustering-Under-Continual-Observation" class="headerlink" title="Differential Privacy for Clustering Under Continual Observation"></a>Differential Privacy for Clustering Under Continual Observation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03430">http://arxiv.org/abs/2307.03430</a></li>
<li>repo_url: None</li>
<li>paper_authors: Max Dupré la Tour, Monika Henzinger, David Saulpic</li>
<li>for: 隐私 clustering一个在 $\mathbb{R}^d$ 上的数据集，该数据集在插入和删除点时进行更新。</li>
<li>methods: 提供了一种 $\varepsilon$-分割隐私 clustering机制，用于实现 $k$-means 目标，并且在 continual observation 下实现。这是首次对这个问题提供了一个增量隐私的解决方案，其损耗因数只是对数增长。</li>
<li>results: 提出了一种基于私有隐私 greedy 近似算法和维度减少算法的方法，可以实现高效的隐私 clustering。此外， partial 扩展了结果到 $k$-medians 问题。<details>
<summary>Abstract</summary>
We consider the problem of clustering privately a dataset in $\mathbb{R}^d$ that undergoes both insertion and deletion of points. Specifically, we give an $\varepsilon$-differentially private clustering mechanism for the $k$-means objective under continual observation. This is the first approximation algorithm for that problem with an additive error that depends only logarithmically in the number $T$ of updates. The multiplicative error is almost the same as non privately. To do so we show how to perform dimension reduction under continual observation and combine it with a differentially private greedy approximation algorithm for $k$-means. We also partially extend our results to the $k$-median problem.
</details>
<details>
<summary>摘要</summary>
我们考虑一个隐私 clustering 问题，对于一个在 $\mathbb{R}^d$ 上的资料集，该资料集会在批量更新的情况下进行插入和删除点。我们提供了一个 $\varepsilon$-隐私 clustering 机制，用于 $k$-means 目标下，并且这个方法具有对数幂递增的误差。我们还详细说明了如何在批量更新下进行维度缩减，并且与隐私保证的暴末搜索法相结合。此外，我们也对 $k$-medians 问题进行了一定的扩展。
</details></li>
</ul>
<hr>
<h2 id="Merging-Diverging-Hybrid-Transformer-Networks-for-Survival-Prediction-in-Head-and-Neck-Cancer"><a href="#Merging-Diverging-Hybrid-Transformer-Networks-for-Survival-Prediction-in-Head-and-Neck-Cancer" class="headerlink" title="Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer"></a>Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03427">http://arxiv.org/abs/2307.03427</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mungomeng/survival-xsurv">https://github.com/mungomeng/survival-xsurv</a></li>
<li>paper_authors: Mingyuan Meng, Lei Bi, Michael Fulham, Dagan Feng, Jinman Kim<br>for:This paper aims to improve survival prediction for cancer patients by developing a deep learning model that can effectively fuse multi-modality images (e.g., PET-CT) and extract region-specific information.methods:The proposed method uses a merging-diverging learning framework, which consists of a merging encoder and a diverging decoder. The merging encoder uses a Hybrid Parallel Cross-Attention (HPCA) block to fuse multi-modality features, while the diverging decoder uses a Region-specific Attention Gate (RAG) block to screen out features related to lesion regions.results:The proposed method (XSurv) outperforms state-of-the-art survival prediction methods on the public dataset of HECKTOR 2022. Specifically, XSurv combines the complementary information in PET and CT images and extracts region-specific prognostic information in PT and MLN regions, leading to improved survival prediction accuracy.<details>
<summary>Abstract</summary>
Survival prediction is crucial for cancer patients as it provides early prognostic information for treatment planning. Recently, deep survival models based on deep learning and medical images have shown promising performance for survival prediction. However, existing deep survival models are not well developed in utilizing multi-modality images (e.g., PET-CT) and in extracting region-specific information (e.g., the prognostic information in Primary Tumor (PT) and Metastatic Lymph Node (MLN) regions). In view of this, we propose a merging-diverging learning framework for survival prediction from multi-modality images. This framework has a merging encoder to fuse multi-modality information and a diverging decoder to extract region-specific information. In the merging encoder, we propose a Hybrid Parallel Cross-Attention (HPCA) block to effectively fuse multi-modality features via parallel convolutional layers and cross-attention transformers. In the diverging decoder, we propose a Region-specific Attention Gate (RAG) block to screen out the features related to lesion regions. Our framework is demonstrated on survival prediction from PET-CT images in Head and Neck (H&N) cancer, by designing an X-shape merging-diverging hybrid transformer network (named XSurv). Our XSurv combines the complementary information in PET and CT images and extracts the region-specific prognostic information in PT and MLN regions. Extensive experiments on the public dataset of HEad and neCK TumOR segmentation and outcome prediction challenge (HECKTOR 2022) demonstrate that our XSurv outperforms state-of-the-art survival prediction methods.
</details>
<details>
<summary>摘要</summary>
生存预测对于癌症患者非常重要，因为它提供了早期的诊断信息，以便为治疗规划。最近，深度存生模型基于深度学习和医疗图像已经展示了有前景的表现。然而，现有的深度存生模型尚未充分利用多Modalities图像（例如PET-CT），也没有充分提取区域特定信息（例如主要肿瘤（PT）和肿瘤静脉节（MLN）区域的诊断信息）。为了解决这一问题，我们提出了一种合并-分化学习框架 для存生预测。这个框架包括一个合并编码器，用于融合多Modalities信息，以及一个分化解码器，用于提取区域特定信息。在合并编码器中，我们提出了一种Hybrid Parallel Cross-Attention（HPCA）块，用于有效地融合多Modalities特征，并通过并行的卷积层和交互变换器来实现。在分化解码器中，我们提出了一种Region-specific Attention Gate（RAG）块，用于筛选出病理区域相关的特征。我们的框架在PET-CT图像上进行存生预测，并通过设计一个X-形合并-分化混合变换网络（名为XSurv）来组合PET和CT图像的补做信息，并提取PT和MLN区域的区域特定诊断信息。我们的XSurv在HECKTOR2022公共数据集上进行了广泛的实验，并证明了它的出色表现。
</details></li>
</ul>
<hr>
<h2 id="Hyperspectral-and-Multispectral-Image-Fusion-Using-the-Conditional-Denoising-Diffusion-Probabilistic-Model"><a href="#Hyperspectral-and-Multispectral-Image-Fusion-Using-the-Conditional-Denoising-Diffusion-Probabilistic-Model" class="headerlink" title="Hyperspectral and Multispectral Image Fusion Using the Conditional Denoising Diffusion Probabilistic Model"></a>Hyperspectral and Multispectral Image Fusion Using the Conditional Denoising Diffusion Probabilistic Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03423">http://arxiv.org/abs/2307.03423</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shuaikaishi/ddpmfus">https://github.com/shuaikaishi/ddpmfus</a></li>
<li>paper_authors: Shuaikai Shi, Lijun Zhang, Jie Chen</li>
<li>for: 这 paper 是为了提出一种基于深度学习的干扰推理模型，用于折衔高spectral像和多spectral像。</li>
<li>methods: 该方法基于 Conditional Denoising Diffusion Probabilistic Model（DDPM），包括前向扩散过程和反向去干扰过程。</li>
<li>results: 实验表明，该方法在一个室内和两个遥感数据集上显示出了比其他高级深度学习基于合并方法更高的性能。<details>
<summary>Abstract</summary>
Hyperspectral images (HSI) have a large amount of spectral information reflecting the characteristics of matter, while their spatial resolution is low due to the limitations of imaging technology. Complementary to this are multispectral images (MSI), e.g., RGB images, with high spatial resolution but insufficient spectral bands. Hyperspectral and multispectral image fusion is a technique for acquiring ideal images that have both high spatial and high spectral resolution cost-effectively. Many existing HSI and MSI fusion algorithms rely on known imaging degradation models, which are often not available in practice. In this paper, we propose a deep fusion method based on the conditional denoising diffusion probabilistic model, called DDPM-Fus. Specifically, the DDPM-Fus contains the forward diffusion process which gradually adds Gaussian noise to the high spatial resolution HSI (HrHSI) and another reverse denoising process which learns to predict the desired HrHSI from its noisy version conditioning on the corresponding high spatial resolution MSI (HrMSI) and low spatial resolution HSI (LrHSI). Once the training is completes, the proposed DDPM-Fus implements the reverse process on the test HrMSI and LrHSI to generate the fused HrHSI. Experiments conducted on one indoor and two remote sensing datasets show the superiority of the proposed model when compared with other advanced deep learningbased fusion methods. The codes of this work will be opensourced at this address: https://github.com/shuaikaishi/DDPMFus for reproducibility.
</details>
<details>
<summary>摘要</summary>
干扰图像（HSI）具有大量的spectral信息，反映物质特点，但其空间分辨率受成像技术限制而受到限制。与此相对的是多spectral图像（MSI），如RGB图像，具有高空间分辨率，但lack spectral bands。干扰图像和多spectral图像的图像混合是一种获得理想图像，即高空间和高spectral分辨率的图像，可以在成本效益的情况下获得。现有的HSI和MSI混合算法多数基于知名的损坏模型，这些模型在实践中经常不可用。在这篇文章中，我们提出了基于条件滤波泛化模型的深度混合方法，称为DDPM-Fus。具体来说，DDPM-Fus包括前向滤波过程，逐渐添加高斯噪声到高空间分辨率干扰图像（HrHSI），以及另一个反向恢复过程，学习预测desired HrHSI的噪声版本，条件在HrMSI和LrHSI的帮助下。一旦训练完成，我们的DDPM-Fus会在测试HrMSI和LrHSI上实现反向过程，生成混合后的HrHSI。我们在一个室内和两个遥感数据集上进行了实验，并证明了我们的方法在其他先进的深度学习基于混合方法之上具有superiority。我们将在这个地址上开源我们的代码：https://github.com/shuaikaishi/DDPMFus，以便复制。
</details></li>
</ul>
<hr>
<h2 id="QI2-–-an-Interactive-Tool-for-Data-Quality-Assurance"><a href="#QI2-–-an-Interactive-Tool-for-Data-Quality-Assurance" class="headerlink" title="QI2 – an Interactive Tool for Data Quality Assurance"></a>QI2 – an Interactive Tool for Data Quality Assurance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03419">http://arxiv.org/abs/2307.03419</a></li>
<li>repo_url: None</li>
<li>paper_authors: Simon Geerkens, Christian Sieberichs, Alexander Braun, Thomas Waschulzik</li>
<li>for: 这篇论文主要用于提高机器学习系统和大数据的数据质量，以满足欧盟的AI法规要求，特别是安全相关的机器学习系统的市场引入。</li>
<li>methods: 本论文提出了一种新的数据质量检查方法，可以同时检查多个数据质量方面的要求。这种方法基于量化的数据质量标准，可以帮助确保数据的质量符合要求。</li>
<li>results: 在小例子数据集上，本方法能够成功地检查数据质量，并且在知名的MNIST数据集上进行了实践示例。<details>
<summary>Abstract</summary>
The importance of high data quality is increasing with the growing impact and distribution of ML systems and big data. Also the planned AI Act from the European commission defines challenging legal requirements for data quality especially for the market introduction of safety relevant ML systems. In this paper we introduce a novel approach that supports the data quality assurance process of multiple data quality aspects. This approach enables the verification of quantitative data quality requirements. The concept and benefits are introduced and explained on small example data sets. How the method is applied is demonstrated on the well known MNIST data set based an handwritten digits.
</details>
<details>
<summary>摘要</summary>
“高品质的数据价值在机器学习系统和大数据的普及和影响力增长的同时也在提高。欧盟委员会的AI法案也将要求严格的数据质量标准，特别是在安全相关的机器学习系统上市。本文将介绍一种支持多种数据质量方面的质量保证过程的新方法。这种方法可以评估量数据质量要求的实施情况。本文将以小型数据集作为例子，介绍概念和优点，并在知名的MNIST数据集上显示如何实施。”Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="AdaptiveRec-Adaptively-Construct-Pairs-for-Contrastive-Learning-in-Sequential-Recommendation"><a href="#AdaptiveRec-Adaptively-Construct-Pairs-for-Contrastive-Learning-in-Sequential-Recommendation" class="headerlink" title="AdaptiveRec: Adaptively Construct Pairs for Contrastive Learning in Sequential Recommendation"></a>AdaptiveRec: Adaptively Construct Pairs for Contrastive Learning in Sequential Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05469">http://arxiv.org/abs/2307.05469</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jaeheyoung Jeon, Jung Hyun Ryu, Jewoong Cho, Myungjoo Kang</li>
<li>for: 解决Sequential recommendation systems中的对比学习挑战，特别是false negative问题，提高推荐算法效果。</li>
<li>methods: 提出了一种进步的对比学习方法，改进了物品嵌入的质量，避免了类似实例被误分类为不相似的问题。</li>
<li>results: 实验结果表明，提出的方法能够提高推荐系统的性能，比 EXISTS 系统更高效。此外，该方法在不同的推荐场景下也有广泛的应用前景。<details>
<summary>Abstract</summary>
This paper presents a solution to the challenges faced by contrastive learning in sequential recommendation systems. In particular, it addresses the issue of false negative, which limits the effectiveness of recommendation algorithms. By introducing an advanced approach to contrastive learning, the proposed method improves the quality of item embeddings and mitigates the problem of falsely categorizing similar instances as dissimilar. Experimental results demonstrate performance enhancements compared to existing systems. The flexibility and applicability of the proposed approach across various recommendation scenarios further highlight its value in enhancing sequential recommendation systems.
</details>
<details>
<summary>摘要</summary>
Here's the text in Simplified Chinese:这篇论文提出了对比学习在序列推荐系统中的挑战，特别是False Negative问题，这限制了推荐算法的效iveness。通过引入高级的对比学习方法，提议方法可以改善item embedding的质量，避免错误地将相似的实例分类为不相似。实验结果表明，提议方法比现有系统有所提高，并且可以适用于不同的推荐enario，进一步强调它在序列推荐系统中的价值。
</details></li>
</ul>
<hr>
<h2 id="Learning-from-Heterogeneity-A-Dynamic-Learning-Framework-for-Hypergraphs"><a href="#Learning-from-Heterogeneity-A-Dynamic-Learning-Framework-for-Hypergraphs" class="headerlink" title="Learning from Heterogeneity: A Dynamic Learning Framework for Hypergraphs"></a>Learning from Heterogeneity: A Dynamic Learning Framework for Hypergraphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03411">http://arxiv.org/abs/2307.03411</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tiehua Zhang, Yuze Liu, Zhishu Shen, Xingjun Ma, Xin Chen, Xiaowei Huang, Jun Yin, Jiong Jin</li>
<li>for: 这篇论文的目的是提出一种基于hypergraph学习的图学习框架，以捕捉图中隐藏的高阶相关性。</li>
<li>methods: 该框架使用动态hyperedge构建和注意力更新来利用图中不同特征的多样性。首先，通过对应式融合策略生成高质量特征，然后通过动态分组生成hypergraph，并进行类型特定的hypergraph学习过程。</li>
<li>results: 经过对多个popular数据集的广泛测试， comparing with11种现有的状态对节点分类和链接预测任务，该框架表现出了显著的性能提升（平均12.5%在节点分类任务上，13.3%在链接预测任务上），证明了该框架的有效性。<details>
<summary>Abstract</summary>
Graph neural network (GNN) has gained increasing popularity in recent years owing to its capability and flexibility in modeling complex graph structure data. Among all graph learning methods, hypergraph learning is a technique for exploring the implicit higher-order correlations when training the embedding space of the graph. In this paper, we propose a hypergraph learning framework named LFH that is capable of dynamic hyperedge construction and attentive embedding update utilizing the heterogeneity attributes of the graph. Specifically, in our framework, the high-quality features are first generated by the pairwise fusion strategy that utilizes explicit graph structure information when generating initial node embedding. Afterwards, a hypergraph is constructed through the dynamic grouping of implicit hyperedges, followed by the type-specific hypergraph learning process. To evaluate the effectiveness of our proposed framework, we conduct comprehensive experiments on several popular datasets with eleven state-of-the-art models on both node classification and link prediction tasks, which fall into categories of homogeneous pairwise graph learning, heterogeneous pairwise graph learning, and hypergraph learning. The experiment results demonstrate a significant performance gain (average 12.5% in node classification and 13.3% in link prediction) compared with recent state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
GRAPH NEURAL NETWORK (GNN) 在最近几年内得到了越来越多的推广，这主要归功于它在处理复杂图结构数据时的能力和灵活性。在所有图学习方法中，超 graf学习是一种技术，用于在训练图的嵌入空间时探索隐藏的高阶相关性。在这篇论文中，我们提出了一个名为LFH的超 graf学习框架，可以在动态组成hyperedge并通过heterogeneity attribute来进行注意力更新。具体来说，在我们的框架中，高质量的特征首先通过对称的对抗策略生成初始节点嵌入。接着，通过动态分组的超 graf组建，然后进行类型特定的超 graf学习过程。为了评估我们提出的框架的效果，我们在多个popular dataset上进行了对 eleven state-of-the-art模型的比较，包括节点分类和链接预测任务，这些任务可以分为同质对策graph学习、不同质对策graph学习和超 graf学习。实验结果表明，我们的提出的框架在节点分类和链接预测任务中表现出了显著的性能提升（平均12.5%和13.3%），相比最近的state-of-the-art方法。
</details></li>
</ul>
<hr>
<h2 id="Scalable-High-Dimensional-Multivariate-Linear-Regression-for-Feature-Distributed-Data"><a href="#Scalable-High-Dimensional-Multivariate-Linear-Regression-for-Feature-Distributed-Data" class="headerlink" title="Scalable High-Dimensional Multivariate Linear Regression for Feature-Distributed Data"></a>Scalable High-Dimensional Multivariate Linear Regression for Feature-Distributed Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03410">http://arxiv.org/abs/2307.03410</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuo-Chieh Huang, Ruey S. Tsay</li>
<li>for: 这篇论文是为了应用多变量线性回传数据，并且能够处理Feature-分布式数据，这种数据在应用中日益增加。</li>
<li>methods: 本论文提出了一个两阶段松弛迪的漫游算法（TSRGA），用于应用多变量线性回传数据。TSRGA的通信复杂度不随特征维度而增加，因此具有高可扩展性。在多变量回应变数情况下，TSRGA可以获得低阶系数估计。</li>
<li>results: 在模拟实验中，TSRGA具有快速对准性。最后，本论文应用了提案的TSRGA在金融应用中，利用10-K报告中的无结构数据，证明了其在具有多个紧密大维度矩阵的应用中的有用性。<details>
<summary>Abstract</summary>
Feature-distributed data, referred to data partitioned by features and stored across multiple computing nodes, are increasingly common in applications with a large number of features. This paper proposes a two-stage relaxed greedy algorithm (TSRGA) for applying multivariate linear regression to such data. The main advantage of TSRGA is that its communication complexity does not depend on the feature dimension, making it highly scalable to very large data sets. In addition, for multivariate response variables, TSRGA can be used to yield low-rank coefficient estimates. The fast convergence of TSRGA is validated by simulation experiments. Finally, we apply the proposed TSRGA in a financial application that leverages unstructured data from the 10-K reports, demonstrating its usefulness in applications with many dense large-dimensional matrices.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>应用中逐渐增长的分布式数据（即根据特征分区存储在多个计算节点上的数据），这篇论文提出了一种两阶段松弛抽象算法（TSRGA）用于应用多变量直线回归。TSRGA的优点在于，它的通信复杂度不随特征维度增长，因此对很大数据集进行扩展非常可行。此外，对多变量响应变量，TSRGA可以生成低级卷积系数估计。在实验中，TSRGA的快速收敛性得到了验证。最后，我们在金融应用中使用了提案的 TSRGA，通过利用10-K报告中的无结构数据，示出了在具有多个稠密大维度矩阵的应用中的实用性。
</details></li>
</ul>
<hr>
<h2 id="A-Self-Supervised-Algorithm-for-Denoising-Photoplethysmography-Signals-for-Heart-Rate-Estimation-from-Wearables"><a href="#A-Self-Supervised-Algorithm-for-Denoising-Photoplethysmography-Signals-for-Heart-Rate-Estimation-from-Wearables" class="headerlink" title="A Self-Supervised Algorithm for Denoising Photoplethysmography Signals for Heart Rate Estimation from Wearables"></a>A Self-Supervised Algorithm for Denoising Photoplethysmography Signals for Heart Rate Estimation from Wearables</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05339">http://arxiv.org/abs/2307.05339</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pranay Jain, Cheng Ding, Cynthia Rudin, Xiao Hu</li>
<li>for: 本研究旨在提高智能手表和其他穿戴式设备中的心率监测精度，通过去噪掉噪音和运动干扰的PPG信号。</li>
<li>methods: 我们提出了一种基于自我超视的净化算法，利用大量的清晰PPG信号数据进行自动编码器的训练，以重建受损PPG信号。</li>
<li>results: 我们的算法可以提供更好的心率估计，并且对PPG信号的各种健康指标进行下游分析也显示了明显的改善。<details>
<summary>Abstract</summary>
Smart watches and other wearable devices are equipped with photoplethysmography (PPG) sensors for monitoring heart rate and other aspects of cardiovascular health. However, PPG signals collected from such devices are susceptible to corruption from noise and motion artifacts, which cause errors in heart rate estimation. Typical denoising approaches filter or reconstruct the signal in ways that eliminate much of the morphological information, even from the clean parts of the signal that would be useful to preserve. In this work, we develop an algorithm for denoising PPG signals that reconstructs the corrupted parts of the signal, while preserving the clean parts of the PPG signal. Our novel framework relies on self-supervised training, where we leverage a large database of clean PPG signals to train a denoising autoencoder. As we show, our reconstructed signals provide better estimates of heart rate from PPG signals than the leading heart rate estimation methods. Further experiments show significant improvement in Heart Rate Variability (HRV) estimation from PPG signals using our algorithm. We conclude that our algorithm denoises PPG signals in a way that can improve downstream analysis of many different health metrics from wearable devices.
</details>
<details>
<summary>摘要</summary>
智能手表和其他穿戴式设备通常配备了光谱 plethysmography (PPG) 传感器，用于监测心率和其他循环征象。然而，PPG 信号从这些设备中收集的信号受到噪声和运动artefacts的污染，导致心率估计出错。现有的减噪方法通常使用过滤或重建信号的方式，以消除大量的形态信息，包括净化部分的PPG信号，这些信号是有用的保留。在这种工作中，我们开发了一种用于减噪PPG信号的算法，可以重建污染的部分信号，同时保留净化部分的PPG信号。我们的新框架基于自我超vised学习，我们利用大量的净化PPG信号数据库来训练一个减噪自适应神经网络。我们的重建信号提供了更好的心率估计，与主流心率估计方法相比。进一步的实验表明，我们的算法可以大幅提高来自PPG信号的循环变化估计（HRV）。我们 conclude that我们的算法可以有效地减噪PPG信号，以提高来自穿戴式设备的多种健康指标的分析。
</details></li>
</ul>
<hr>
<h2 id="Goal-Conditioned-Predictive-Coding-as-an-Implicit-Planner-for-Offline-Reinforcement-Learning"><a href="#Goal-Conditioned-Predictive-Coding-as-an-Implicit-Planner-for-Offline-Reinforcement-Learning" class="headerlink" title="Goal-Conditioned Predictive Coding as an Implicit Planner for Offline Reinforcement Learning"></a>Goal-Conditioned Predictive Coding as an Implicit Planner for Offline Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03406">http://arxiv.org/abs/2307.03406</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zilai Zeng, Ce Zhang, Shijie Wang, Chen Sun</li>
<li>for: 本研究旨在调查 Whether sequence modeling can condense trajectories into useful representations that contribute to policy learning.</li>
<li>methods: 本研究采用了一个 Two-stage 框架，首先使用Sequence modeling技术Summary trajectories,然后使用这些表示来学习策略和愿景目标。</li>
<li>results: 研究发现，Sequence modeling 有效地减少了一些决策任务的训练时间，并且可以学习出高性能的策略。此外，GCPC 方法学习了一个 Conditioned 的未来 Representation，并在 AntMaze、FrankaKitchen 和 Locomotion 环境中达到了竞争性的性能。<details>
<summary>Abstract</summary>
Recent work has demonstrated the effectiveness of formulating decision making as a supervised learning problem on offline-collected trajectories. However, the benefits of performing sequence modeling on trajectory data is not yet clear. In this work we investigate if sequence modeling has the capability to condense trajectories into useful representations that can contribute to policy learning. To achieve this, we adopt a two-stage framework that first summarizes trajectories with sequence modeling techniques, and then employs these representations to learn a policy along with a desired goal. This design allows many existing supervised offline RL methods to be considered as specific instances of our framework. Within this framework, we introduce Goal-Conditioned Predicitve Coding (GCPC), an approach that brings powerful trajectory representations and leads to performant policies. We conduct extensive empirical evaluations on AntMaze, FrankaKitchen and Locomotion environments, and observe that sequence modeling has a significant impact on some decision making tasks. In addition, we demonstrate that GCPC learns a goal-conditioned latent representation about the future, which serves as an "implicit planner", and enables competitive performance on all three benchmarks.
</details>
<details>
<summary>摘要</summary>
最近的工作已经证明了将决策问题定义为有监督学习问题的可行性。然而，使用序列模型处理轨迹数据的利点还不够清晰。在这种情况下，我们 investigate whether sequence modeling can condense trajectories into useful representations that can contribute to policy learning. To achieve this, we adopt a two-stage framework that first summarizes trajectories with sequence modeling techniques and then employs these representations to learn a policy along with a desired goal. This design allows many existing supervised offline RL methods to be considered as specific instances of our framework. Within this framework, we introduce Goal-Conditioned Predicitve Coding (GCPC), an approach that brings powerful trajectory representations and leads to performant policies. We conduct extensive empirical evaluations on AntMaze, FrankaKitchen and Locomotion environments, and observe that sequence modeling has a significant impact on some decision making tasks. In addition, we demonstrate that GCPC learns a goal-conditioned latent representation about the future, which serves as an "implicit planner" and enables competitive performance on all three benchmarks.Here's the word-for-word translation of the given text into Simplified Chinese:最近的工作已经证明了将决策问题定义为有监督学习问题的可行性。然而，使用序列模型处理轨迹数据的利点还不够清晰。在这种情况下，我们 investigate whether sequence modeling can condense trajectories into useful representations that can contribute to policy learning. To achieve this, we adopt a two-stage framework that first summarizes trajectories with sequence modeling techniques and then employs these representations to learn a policy along with a desired goal. This design allows many existing supervised offline RL methods to be considered as specific instances of our framework. Within this framework, we introduce Goal-Conditioned Predicitve Coding (GCPC), an approach that brings powerful trajectory representations and leads to performant policies. We conduct extensive empirical evaluations on AntMaze, FrankaKitchen and Locomotion environments, and observe that sequence modeling has a significant impact on some decision making tasks. In addition, we demonstrate that GCPC learns a goal-conditioned latent representation about the future, which serves as an "implicit planner" and enables competitive performance on all three benchmarks.
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Potential-of-Large-Language-Models-LLMs-in-Learning-on-Graphs"><a href="#Exploring-the-Potential-of-Large-Language-Models-LLMs-in-Learning-on-Graphs" class="headerlink" title="Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs"></a>Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03393">http://arxiv.org/abs/2307.03393</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/CurryTang/Graph-LLM">https://github.com/CurryTang/Graph-LLM</a></li>
<li>paper_authors: Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi Wei, Shuaiqiang Wang, Dawei Yin, Wenqi Fan, Hui Liu, Jiliang Tang</li>
<li>for: 本文探讨了大语言模型（LLMs）在图机器学习中的潜力，特别是节点分类任务中。</li>
<li>methods: 本文提出了两种可能的管道： LLMS-as-Enhancers 和 LLMS-as-Predictors。前者利用 LLMS 增强节点的文本特征，然后通过 GNNs 生成预测结果。后者直接使用 LLMS 作为独立预测器。</li>
<li>results: 本文通过了多种设定下的全面和系统的实验研究，从实验结果中得出了原创的观察和新的发现，开启了新的可能性和建议，并提出了潜在的方向，以便更好地利用 LLMS 进行图学习。<details>
<summary>Abstract</summary>
Learning on Graphs has attracted immense attention due to its wide real-world applications. The most popular pipeline for learning on graphs with textual node attributes primarily relies on Graph Neural Networks (GNNs), and utilizes shallow text embedding as initial node representations, which has limitations in general knowledge and profound semantic understanding. In recent years, Large Language Models (LLMs) have been proven to possess extensive common knowledge and powerful semantic comprehension abilities that have revolutionized existing workflows to handle text data. In this paper, we aim to explore the potential of LLMs in graph machine learning, especially the node classification task, and investigate two possible pipelines: LLMs-as-Enhancers and LLMs-as-Predictors. The former leverages LLMs to enhance nodes' text attributes with their massive knowledge and then generate predictions through GNNs. The latter attempts to directly employ LLMs as standalone predictors. We conduct comprehensive and systematical studies on these two pipelines under various settings. From comprehensive empirical results, we make original observations and find new insights that open new possibilities and suggest promising directions to leverage LLMs for learning on graphs. Our codes and datasets are available at https://github.com/CurryTang/Graph-LLM.
</details>
<details>
<summary>摘要</summary>
学习图有很多应用，吸引了很多人的注意。最受欢迎的图学习管道是使用图神经网络（GNNs），并使用图节点特征的浅层文本嵌入，这有限制在普遍知识和深层semantic理解方面。在过去几年，大型自然语言模型（LLMs）已经证明了具有广泛的通用知识和强大的semantic理解能力，这些能力在处理文本数据方面引发了革命。在这篇论文中，我们想要探索LLMs在图机器学习中的潜力，特别是节点分类任务，并研究了两个可能的管道：LLMs-as-Enhancers和LLMs-as-Predictors。前者利用LLMs来增强节点的文本特征，然后通过GNNs生成预测结果。后者尝试直接使用LLMs作为独立预测器。我们在不同的设置下进行了全面和系统的研究，从实验结果中得出了原创的观察和新的发现，这些发现开启了新的可能性和建议了潜在的方向，以便利用LLMs进行图学习。我们的代码和数据集可以在https://github.com/CurryTang/Graph-LLM上下载。
</details></li>
</ul>
<hr>
<h2 id="AI-UPV-at-EXIST-2023-–-Sexism-Characterization-Using-Large-Language-Models-Under-The-Learning-with-Disagreements-Regime"><a href="#AI-UPV-at-EXIST-2023-–-Sexism-Characterization-Using-Large-Language-Models-Under-The-Learning-with-Disagreements-Regime" class="headerlink" title="AI-UPV at EXIST 2023 – Sexism Characterization Using Large Language Models Under The Learning with Disagreements Regime"></a>AI-UPV at EXIST 2023 – Sexism Characterization Using Large Language Models Under The Learning with Disagreements Regime</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03385">http://arxiv.org/abs/2307.03385</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/angelfelipemp/sexism-llm-learning-with-disagreement">https://github.com/angelfelipemp/sexism-llm-learning-with-disagreement</a></li>
<li>paper_authors: Angel Felipe Magnossão de Paula, Giulia Rizzi, Elisabetta Fersini, Damiano Spina</li>
<li>for: 本研究旨在开发自动检测社交媒体上的性别歧视和其他不尊重和仇恨行为，以促进在线环境中的包容和尊重。</li>
<li>methods: 本研究使用大型自然语言模型（i.e., mBERT和XLM-RoBERTa）和 ensemble策略进行性别歧视 Identification和分类，并在英语和西班牙语之间进行了比较。</li>
<li>results: 本研究在EXIST实验室2023中参与了三个任务，其中在第2任务中以软评估方式获得了第四名，并在第3任务中获得了最高ICM-Soft&#x3D;-2.32和normalized ICM-Soft&#x3D;0.79。<details>
<summary>Abstract</summary>
With the increasing influence of social media platforms, it has become crucial to develop automated systems capable of detecting instances of sexism and other disrespectful and hateful behaviors to promote a more inclusive and respectful online environment. Nevertheless, these tasks are considerably challenging considering different hate categories and the author's intentions, especially under the learning with disagreements regime. This paper describes AI-UPV team's participation in the EXIST (sEXism Identification in Social neTworks) Lab at CLEF 2023. The proposed approach aims at addressing the task of sexism identification and characterization under the learning with disagreements paradigm by training directly from the data with disagreements, without using any aggregated label. Yet, performances considering both soft and hard evaluations are reported. The proposed system uses large language models (i.e., mBERT and XLM-RoBERTa) and ensemble strategies for sexism identification and classification in English and Spanish. In particular, our system is articulated in three different pipelines. The ensemble approach outperformed the individual large language models obtaining the best performances both adopting a soft and a hard label evaluation. This work describes the participation in all the three EXIST tasks, considering a soft evaluation, it obtained fourth place in Task 2 at EXIST and first place in Task 3, with the highest ICM-Soft of -2.32 and a normalized ICM-Soft of 0.79. The source code of our approaches is publicly available at https://github.com/AngelFelipeMP/Sexism-LLM-Learning-With-Disagreement.
</details>
<details>
<summary>摘要</summary>
随着社交媒体平台的普及，已成为必要的开发自动化系统，以检测社交媒体上的性别歧视和其他不尊重和仇恨行为，以促进更加包容和尊重的在线环境。然而，这些任务具有不同的仇恨类别和作者意图，特别是在学习各种不同的观点下。这篇文章描述了AI-UPV团队在EXIST（sEXism Identification in Social neTworks）实验室中的参与。提议的方法是通过直接从数据中学习，不使用任何综合标签，来解决性别歧视的识别和分类问题。然而，我们还是根据软和硬评估进行了性能评估。我们使用了大型语言模型（i.e., mBERT和XLM-RoBERTa）和ensemble策略进行性别歧视识别和分类。特别是，我们的系统是由三个不同的管道组成。 ensemble方法在软和硬评估中都表现出了最佳性能，并在EXIST任务中获得了第四名（Task 2）和第一名（Task 3），其ICM-Soft=-2.32和normalized ICM-Soft为0.79。我们的代码可以在https://github.com/AngelFelipeMP/Sexism-LLM-Learning-With-Disagreement上获取。
</details></li>
</ul>
<hr>
<h2 id="Teaching-Arithmetic-to-Small-Transformers"><a href="#Teaching-Arithmetic-to-Small-Transformers" class="headerlink" title="Teaching Arithmetic to Small Transformers"></a>Teaching Arithmetic to Small Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03381">http://arxiv.org/abs/2307.03381</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lee-ny/teaching_arithmetic">https://github.com/lee-ny/teaching_arithmetic</a></li>
<li>paper_authors: Nayoung Lee, Kartik Sreenivasan, Jason D. Lee, Kangwook Lee, Dimitris Papailiopoulos</li>
<li>for: 这个研究旨在探讨使用自然语言数据来快速启动大型语言模型的算术能力。</li>
<li>methods: 研究使用无监督下一个词预测目标进行 arithmetic 操作的学习，包括加法、乘法和平方根等操作。</li>
<li>results: 研究发现，通过使用简单的 transformer 模型和适当的数据格式化，可以使用 next-token prediction 目标来快速学习算术操作，并且这种方法可以同时提高准确率、样本复杂度和 converges 速度。<details>
<summary>Abstract</summary>
Large language models like GPT-4 exhibit emergent capabilities across general-purpose tasks, such as basic arithmetic, when trained on extensive text data, even though these tasks are not explicitly encoded by the unsupervised, next-token prediction objective. This study investigates how small transformers, trained from random initialization, can efficiently learn arithmetic operations such as addition, multiplication, and elementary functions like square root, using the next-token prediction objective. We first demonstrate that conventional training data is not the most effective for arithmetic learning, and simple formatting changes can significantly improve accuracy. This leads to sharp phase transitions as a function of training data scale, which, in some cases, can be explained through connections to low-rank matrix completion. Building on prior work, we then train on chain-of-thought style data that includes intermediate step results. Even in the complete absence of pretraining, this approach significantly and simultaneously improves accuracy, sample complexity, and convergence speed. We also study the interplay between arithmetic and text data during training and examine the effects of few-shot prompting, pretraining, and model scale. Additionally, we discuss length generalization challenges. Our work highlights the importance of high-quality, instructive data that considers the particular characteristics of the next-word prediction objective for rapidly eliciting arithmetic capabilities.
</details>
<details>
<summary>摘要</summary>
大型语言模型如GPT-4会展示涉及到通用任务的emergent能力，例如基本的算术运算，当它们被训练在广泛的文本数据上，即使这些任务没有直接被Encoding在无监督的下一个字符预测目标中。这个研究探索了如何使用下一个字符预测目标来快速学习算术运算，包括加法、乘法和幂函数。我们首先示出了传统训练数据不是最有效的 для算术学习，并且简单的格式变更可以提高准确性。这会导致训练数据的数量对学习数据的阶段性变化产生锐角转折，在一些情况下，这些转折可以通过低维矩阵完成性的连结来解释。我们 THEN以链条式数据来训练，包括中途结果。甚至在完全absence of pre-training，这种方法可以对准确性、样本复杂度和训练速度进行同时提高。我们还研究了在训练过程中文本和算术数据之间的互动，以及几个shot prompting、预训练和模型scale的影响。此外，我们还讨论了长度扩展的挑战。我们的工作强调了高质量、 instruктив的数据的重要性，以应对特定的下一个字符预测目标，快速抽象出算术能力。
</details></li>
</ul>
<hr>
<h2 id="On-Formal-Feature-Attribution-and-Its-Approximation"><a href="#On-Formal-Feature-Attribution-and-Its-Approximation" class="headerlink" title="On Formal Feature Attribution and Its Approximation"></a>On Formal Feature Attribution and Its Approximation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03380">http://arxiv.org/abs/2307.03380</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ffattr/ffa">https://github.com/ffattr/ffa</a></li>
<li>paper_authors: Jinqiang Yu, Alexey Ignatiev, Peter J. Stuckey</li>
<li>For: This paper focuses on the problem of feature attribution in machine learning models, specifically in the context of explainable artificial intelligence (XAI). It proposes a new approach called formal feature attribution (FFA) to address the limitations of existing methods.* Methods: The paper uses formal methods to analyze and evaluate the feature attribution of machine learning models. It specifically employs formal explanation enumeration to compute the exact FFA, and proposes an efficient approximation technique to handle the practical complexity of the problem.* Results: The paper provides experimental evidence of the effectiveness of the proposed approximate FFA method, comparing it to existing feature attribution algorithms in terms of feature importance and relative order. It demonstrates that FFA can provide more accurate and informative attributions than existing methods, while also being more efficient in practical settings.<details>
<summary>Abstract</summary>
Recent years have witnessed the widespread use of artificial intelligence (AI) algorithms and machine learning (ML) models. Despite their tremendous success, a number of vital problems like ML model brittleness, their fairness, and the lack of interpretability warrant the need for the active developments in explainable artificial intelligence (XAI) and formal ML model verification. The two major lines of work in XAI include feature selection methods, e.g. Anchors, and feature attribution techniques, e.g. LIME and SHAP. Despite their promise, most of the existing feature selection and attribution approaches are susceptible to a range of critical issues, including explanation unsoundness and out-of-distribution sampling. A recent formal approach to XAI (FXAI) although serving as an alternative to the above and free of these issues suffers from a few other limitations. For instance and besides the scalability limitation, the formal approach is unable to tackle the feature attribution problem. Additionally, a formal explanation despite being formally sound is typically quite large, which hampers its applicability in practical settings. Motivated by the above, this paper proposes a way to apply the apparatus of formal XAI to the case of feature attribution based on formal explanation enumeration. Formal feature attribution (FFA) is argued to be advantageous over the existing methods, both formal and non-formal. Given the practical complexity of the problem, the paper then proposes an efficient technique for approximating exact FFA. Finally, it offers experimental evidence of the effectiveness of the proposed approximate FFA in comparison to the existing feature attribution algorithms not only in terms of feature importance and but also in terms of their relative order.
</details>
<details>
<summary>摘要</summary>
近年来，人工智能（AI）算法和机器学习（ML）模型在各个领域得到了广泛的应用。虽然它们取得了很大的成功，但是一些重要的问题仍然需要解决，如机器学习模型的 brittleness、公正性和解释性的缺失。这些问题促使了活跃的开发Explainable Artificial Intelligence（XAI）和正式的机器学习模型验证。XAI的两大主要方向是特征选择方法，如Anchors，以及特征归因技术，如LIME和SHAP。尽管它们承诺了很多，但是现有的特征选择和归因方法受到了许多重要的问题的威胁，如解释不准确和非常型采样。一种最近的正式XAI方法，尽管作为一种alternative，免受了这些问题，但它又有一些其他的限制，例如可扩展性的限制，无法解决特征归因问题。此外，正式的解释，即正式承诺，通常很大，这会妨碍它在实践中的应用。为了解决这些问题，本文提出了一种基于正式XAI的特征归因方法，即正式特征归因（FFA）。 FF A argued to be advantageous over the existing methods, both formal and non-formal。 compte tenu de la complexité pratique du problème, la paper then propose une méthode efficace pour approximer l'explication exacte FFA。Finally, it offers experimental evidence of the effectiveness of the proposed approximate FFA in comparison to the existing feature attribution algorithms not only in terms of feature importance but also in terms of their relative order.
</details></li>
</ul>
<hr>
<h2 id="Mitigating-Negative-Transfer-with-Task-Awareness-for-Sexism-Hate-Speech-and-Toxic-Language-Detection"><a href="#Mitigating-Negative-Transfer-with-Task-Awareness-for-Sexism-Hate-Speech-and-Toxic-Language-Detection" class="headerlink" title="Mitigating Negative Transfer with Task Awareness for Sexism, Hate Speech, and Toxic Language Detection"></a>Mitigating Negative Transfer with Task Awareness for Sexism, Hate Speech, and Toxic Language Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03377">http://arxiv.org/abs/2307.03377</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/angelfelipemp/mitigating-negative-transfer-with-ta">https://github.com/angelfelipemp/mitigating-negative-transfer-with-ta</a></li>
<li>paper_authors: Angel Felipe Magnossão de Paula, Paolo Rosso, Damiano Spina</li>
<li>for: 这篇论文目的是解决机器学习中的负转移问题。</li>
<li>methods: 该论文提出了一种基于任务意识概念的方法来缓解负转移问题。</li>
<li>results: 该方法在EXIST-2021和HatEval-2019测试基准上实现了新的状态作图，并且与 класси型多任务学习方法相比，提高了性能。<details>
<summary>Abstract</summary>
This paper proposes a novelty approach to mitigate the negative transfer problem. In the field of machine learning, the common strategy is to apply the Single-Task Learning approach in order to train a supervised model to solve a specific task. Training a robust model requires a lot of data and a significant amount of computational resources, making this solution unfeasible in cases where data are unavailable or expensive to gather. Therefore another solution, based on the sharing of information between tasks, has been developed: Multi-Task Learning (MTL). Despite the recent developments regarding MTL, the problem of negative transfer has still to be solved. Negative transfer is a phenomenon that occurs when noisy information is shared between tasks, resulting in a drop in performance. This paper proposes a new approach to mitigate the negative transfer problem based on the task awareness concept. The proposed approach results in diminishing the negative transfer together with an improvement of performance over classic MTL solution. Moreover, the proposed approach has been implemented in two unified architectures to detect Sexism, Hate Speech, and Toxic Language in text comments. The proposed architectures set a new state-of-the-art both in EXIST-2021 and HatEval-2019 benchmarks.
</details>
<details>
<summary>摘要</summary>
Here is the text in Simplified Chinese:这篇论文提出了一种新的方法来解决多任务学习中的负面传递问题。在机器学习领域中，通常采用单任务学习方法来训练特定任务的超级vised模型，但是这需要很多数据和计算资源。为了解决这个限制，多任务学习（MTL）被开发出来，它在任务之间共享信息。然而，负面传递现象会导致任务之间的信息干扰，从而导致性能下降。这篇论文提出了一种基于任务意识概念的新方法来 Mitigate负面传递问题，并且在EXIST-2021和HatEval-2019测试benchmark上设置了新的状态公共。
</details></li>
</ul>
<hr>
<h2 id="STG-MTL-Scalable-Task-Grouping-for-Multi-Task-Learning-Using-Data-Map"><a href="#STG-MTL-Scalable-Task-Grouping-for-Multi-Task-Learning-Using-Data-Map" class="headerlink" title="STG-MTL: Scalable Task Grouping for Multi-Task Learning Using Data Map"></a>STG-MTL: Scalable Task Grouping for Multi-Task Learning Using Data Map</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03374">http://arxiv.org/abs/2307.03374</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ammar Sherif, Abubakar Abid, Mustafa Elattar, Mohamed ElHelw</li>
<li>for: 提高多任务学习（MTL）的性能，解决 tradicional Single-Task Learning（STL）的缺点。</li>
<li>methods: 使用手工设计的特征数据地图，捕捉每个分类任务在MTL训练中的训练行为，从而实现可扩展和可组合的解决方案。</li>
<li>results: 实验表明，我们的方法可以有效地处理大量任务（达100个），并且可以提高MTL的性能。<details>
<summary>Abstract</summary>
Multi-Task Learning (MTL) is a powerful technique that has gained popularity due to its performance improvement over traditional Single-Task Learning (STL). However, MTL is often challenging because there is an exponential number of possible task groupings, which can make it difficult to choose the best one, and some groupings might produce performance degradation due to negative interference between tasks. Furthermore, existing solutions are severely suffering from scalability issues, limiting any practical application. In our paper, we propose a new data-driven method that addresses these challenges and provides a scalable and modular solution for classification task grouping based on hand-crafted features, specifically Data Maps, which capture the training behavior for each classification task during the MTL training. We experiment with the method demonstrating its effectiveness, even on an unprecedented number of tasks (up to 100).
</details>
<details>
<summary>摘要</summary>
多任务学习（MTL）是一种强大的技术，它在单任务学习（STL）的基础上提高性能，但是MTL也有很多挑战。其中一个主要挑战是可能的任务分组的数量是无限的，这使得选择最佳任务分组变得困难，而且一些任务分组可能会导致任务之间的负面干扰，从而降低性能。此外，现有的解决方案受到可扩展性的限制，这限制了它们在实际应用中的使用。在我们的论文中，我们提出了一种基于手工特征的数据驱动方法，该方法可以 Address these challenges and provide a scalable and modular solution for classification task grouping. We experiment with the method and demonstrate its effectiveness, even on an unprecedented number of tasks (up to 100).
</details></li>
</ul>
<hr>
<h2 id="Distilled-Pruning-Using-Synthetic-Data-to-Win-the-Lottery"><a href="#Distilled-Pruning-Using-Synthetic-Data-to-Win-the-Lottery" class="headerlink" title="Distilled Pruning: Using Synthetic Data to Win the Lottery"></a>Distilled Pruning: Using Synthetic Data to Win the Lottery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03364">http://arxiv.org/abs/2307.03364</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/luke-mcdermott-mi/distilled-pruning">https://github.com/luke-mcdermott-mi/distilled-pruning</a></li>
<li>paper_authors: Luke McDermott, Daniel Cummings</li>
<li>for: 这篇论文旨在提出一种使用精炼数据的深度学习模型剪辑方法，不同于传统的建筑或算法优化方法。</li>
<li>methods: 这种方法利用精炼数据捕捉大数据集中的关键模式，并如何利用这种能力来实现 computationally efficient 的剪辑过程。</li>
<li>results: 实验结果表明，使用精炼数据可以在 CIFAR-10 上找到更加快速的、相对精炼的剪辑结果，比 Iterative Magnitude Pruning 快到 5 倍。这些结果表明使用精炼数据可以提高资源有效的神经网络剪辑、模型压缩和神经建筑搜索。<details>
<summary>Abstract</summary>
This work introduces a novel approach to pruning deep learning models by using distilled data. Unlike conventional strategies which primarily focus on architectural or algorithmic optimization, our method reconsiders the role of data in these scenarios. Distilled datasets capture essential patterns from larger datasets, and we demonstrate how to leverage this capability to enable a computationally efficient pruning process. Our approach can find sparse, trainable subnetworks (a.k.a. Lottery Tickets) up to 5x faster than Iterative Magnitude Pruning at comparable sparsity on CIFAR-10. The experimental results highlight the potential of using distilled data for resource-efficient neural network pruning, model compression, and neural architecture search.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Federated-Unlearning-via-Active-Forgetting"><a href="#Federated-Unlearning-via-Active-Forgetting" class="headerlink" title="Federated Unlearning via Active Forgetting"></a>Federated Unlearning via Active Forgetting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03363">http://arxiv.org/abs/2307.03363</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuyuan Li, Chaochao Chen, Xiaolin Zheng, Jiaming Zhang</li>
<li>for: 本研究旨在提出一种基于增量学习的联合学习无学习方法，以解决联合学习无学习问题。</li>
<li>methods: 我们提出的方法基于增量学习，不需要特定的模型和联合设置。我们利用新的记忆替换老的记忆，模仿人脑中的活动忘记。Specifically, the model intended to unlearn serves as a student model that continuously learns from randomly initiated teacher models. To prevent catastrophic forgetting of non-target data, we utilize elastic weight consolidation to elastically constrain weight change.</li>
<li>results: 我们的方法在三个标准 benchmark 数据集上进行了广泛的实验，并得到了满意的效果和效率。另外，我们还通过后门攻击示例表明了我们的方法具有满意的完整性。<details>
<summary>Abstract</summary>
The increasing concerns regarding the privacy of machine learning models have catalyzed the exploration of machine unlearning, i.e., a process that removes the influence of training data on machine learning models. This concern also arises in the realm of federated learning, prompting researchers to address the federated unlearning problem. However, federated unlearning remains challenging. Existing unlearning methods can be broadly categorized into two approaches, i.e., exact unlearning and approximate unlearning. Firstly, implementing exact unlearning, which typically relies on the partition-aggregation framework, in a distributed manner does not improve time efficiency theoretically. Secondly, existing federated (approximate) unlearning methods suffer from imprecise data influence estimation, significant computational burden, or both. To this end, we propose a novel federated unlearning framework based on incremental learning, which is independent of specific models and federated settings. Our framework differs from existing federated unlearning methods that rely on approximate retraining or data influence estimation. Instead, we leverage new memories to overwrite old ones, imitating the process of \textit{active forgetting} in neurology. Specifically, the model, intended to unlearn, serves as a student model that continuously learns from randomly initiated teacher models. To preserve catastrophic forgetting of non-target data, we utilize elastic weight consolidation to elastically constrain weight change. Extensive experiments on three benchmark datasets demonstrate the efficiency and effectiveness of our proposed method. The result of backdoor attacks demonstrates that our proposed method achieves satisfying completeness.
</details>
<details>
<summary>摘要</summary>
随着机器学习模型的隐私问题的增加，许多研究者开始探讨机器学习模型的卸载问题，即使模型不再受训练数据的影响。在联合学习领域，这种问题也得到了关注，但是联合卸载仍然是一个挑战。现有的卸载方法可以大致分为两类：精确卸载和approximate卸载。首先，在分布式环境中实现精确卸载不会提高时间效率理论上。其次，现有的联合卸载方法受到数据影响估计不准确、计算负担大、或者都有问题。为此，我们提出了一种基于增量学习的联合卸载框架，不受特定模型和联合设置的限制。我们的框架与现有的联合卸载方法不同，不是通过精度抽象重新训练或数据影响估计来实现卸载。相反，我们利用新的记忆来覆盖老的记忆，模仿人脑中的活动忘记。具体来说，作为卸载的模型，我们的模型在随机开始的老师模型的指导下不断学习。为避免非目标数据的悲观性忘记，我们利用弹性重要权重卷积来稳定重要权重的变化。我们在三个标准数据集上进行了广泛的实验，结果表明我们的提出方法是高效和有效的。结果还表明，我们的方法可以满足完整性要求。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Biased-Attitude-Associations-of-Language-Models-in-an-Intersectional-Context"><a href="#Evaluating-Biased-Attitude-Associations-of-Language-Models-in-an-Intersectional-Context" class="headerlink" title="Evaluating Biased Attitude Associations of Language Models in an Intersectional Context"></a>Evaluating Biased Attitude Associations of Language Models in an Intersectional Context</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03360">http://arxiv.org/abs/2307.03360</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shivaomrani/llm-bias">https://github.com/shivaomrani/llm-bias</a></li>
<li>paper_authors: Shiva Omrani Sabbaghi, Robert Wolfe, Aylin Caliskan</li>
<li>For: The paper aims to quantify the biases in language models using a sentence template that provides an intersectional context, and to study the associations of underrepresented groups in language.* Methods: The paper uses a concept projection approach to capture the valence subspace through contextualized word embeddings of language models, and adapts the projection-based approach to embedding association tests to quantify bias.* Results: The paper finds that language models exhibit the most biased attitudes against gender identity, social class, and sexual orientation signals in language, and that the largest and better-performing model is also more biased. The approach enables the study of complex intersectional biases and contributes to design justice by studying the associations of underrepresented groups in language.<details>
<summary>Abstract</summary>
Language models are trained on large-scale corpora that embed implicit biases documented in psychology. Valence associations (pleasantness/unpleasantness) of social groups determine the biased attitudes towards groups and concepts in social cognition. Building on this established literature, we quantify how social groups are valenced in English language models using a sentence template that provides an intersectional context. We study biases related to age, education, gender, height, intelligence, literacy, race, religion, sex, sexual orientation, social class, and weight. We present a concept projection approach to capture the valence subspace through contextualized word embeddings of language models. Adapting the projection-based approach to embedding association tests that quantify bias, we find that language models exhibit the most biased attitudes against gender identity, social class, and sexual orientation signals in language. We find that the largest and better-performing model that we study is also more biased as it effectively captures bias embedded in sociocultural data. We validate the bias evaluation method by overperforming on an intrinsic valence evaluation task. The approach enables us to measure complex intersectional biases as they are known to manifest in the outputs and applications of language models that perpetuate historical biases. Moreover, our approach contributes to design justice as it studies the associations of groups underrepresented in language such as transgender and homosexual individuals.
</details>
<details>
<summary>摘要</summary>
受大规模文献吸收的语言模型具有隐式偏见，这些偏见在社会认知中确定语言模型对社会集团和概念的偏见态度。基于已有文献，我们使用一个 intersecting 上下文中的句子模板来衡量社会集团的VALence（愉悦程度）。我们研究年龄、教育、性别、身高、智商、文化程度、种族、宗教、性别、性取向、社会阶层和体重等因素对语言模型的偏见。我们采用一种投影方法来捕捉VALence子空间，并通过contextualized word embeddings来衡量语言模型的偏见。我们发现语言模型对性别认同、社会阶层和性取向信号表现出最大的偏见。此外，我们发现最大和最高性能的模型也是最偏见的，因为它能够吸收社会文化数据中嵌入的偏见。我们验证了偏见评价方法的正确性，并发现该方法可以衡量复杂的交叉群偏见，这些偏见在语言模型的输出和应用中仍然存在。此外，我们的方法对设计正义做出贡献，因为它研究未 Represented 在语言中的群体，如 трансジェンダ和同性恋者。
</details></li>
</ul>
<hr>
<h2 id="CSCLog-A-Component-Subsequence-Correlation-Aware-Log-Anomaly-Detection-Method"><a href="#CSCLog-A-Component-Subsequence-Correlation-Aware-Log-Anomaly-Detection-Method" class="headerlink" title="CSCLog: A Component Subsequence Correlation-Aware Log Anomaly Detection Method"></a>CSCLog: A Component Subsequence Correlation-Aware Log Anomaly Detection Method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03359">http://arxiv.org/abs/2307.03359</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hang-z/csclog">https://github.com/hang-z/csclog</a></li>
<li>paper_authors: Ling Chen, Chaodu Song, Xu Wang, Dachao Fu, Feifei Li</li>
<li>for: 这个研究旨在提出一个基于系统日志的异常探测方法，以应对智能运营中的异常探测 зада难。</li>
<li>methods: 本研究使用了组件 subsequences corrrelation-aware 方法 (CSCLog)，它不具备传统方法所具备的续接性，同时还能够模型异常 subsequences 之间的隐式相互关联。</li>
<li>results: 实验结果显示，CSCLog 方法可以对四个公开的系统日志数据进行异常探测，与最佳基eline相比，平均提高了7.41%的标准偏差。<details>
<summary>Abstract</summary>
Anomaly detection based on system logs plays an important role in intelligent operations, which is a challenging task due to the extremely complex log patterns. Existing methods detect anomalies by capturing the sequential dependencies in log sequences, which ignore the interactions of subsequences. To this end, we propose CSCLog, a Component Subsequence Correlation-Aware Log anomaly detection method, which not only captures the sequential dependencies in subsequences, but also models the implicit correlations of subsequences. Specifically, subsequences are extracted from log sequences based on components and the sequential dependencies in subsequences are captured by Long Short-Term Memory Networks (LSTMs). An implicit correlation encoder is introduced to model the implicit correlations of subsequences adaptively. In addition, Graph Convolution Networks (GCNs) are employed to accomplish the information interactions of subsequences. Finally, attention mechanisms are exploited to fuse the embeddings of all subsequences. Extensive experiments on four publicly available log datasets demonstrate the effectiveness of CSCLog, outperforming the best baseline by an average of 7.41% in Macro F1-Measure.
</details>
<details>
<summary>摘要</summary>
“异常检测基于系统日志记录是智能运维中重要的一个任务，但是由于系统日志记录的极其复杂，这是一项挑战性的任务。现有的方法通过捕捉系统日志记录序列中的顺序相关性来检测异常，但是它们忽略了系统日志记录序列中的间接相关性。为此，我们提出了CSCLog方法，它不仅捕捉系统日志记录序列中的顺序相关性，而且模型了系统日志记录序列中的间接相关性。具体来说，我们从系统日志记录序列中提取了子序列，并使用Long Short-Term Memory Networks（LSTM）捕捉了这些子序列中的顺序相关性。此外，我们引入了一个适应性的间接相关性编码器，以模型系统日志记录序列中的间接相关性。同时，我们使用Graph Convolution Networks（GCNs）来实现系统日志记录序列中的信息互动。最后，我们利用了注意力机制来融合所有子序列的嵌入。我们对四个公开的系统日志数据集进行了广泛的实验，并证明了CSCLog方法的有效性，与最佳基eline相比，CSCLog方法的平均准确率提高了7.41%。”
</details></li>
</ul>
<hr>
<h2 id="Stability-and-Generalization-of-Stochastic-Compositional-Gradient-Descent-Algorithms"><a href="#Stability-and-Generalization-of-Stochastic-Compositional-Gradient-Descent-Algorithms" class="headerlink" title="Stability and Generalization of Stochastic Compositional Gradient Descent Algorithms"></a>Stability and Generalization of Stochastic Compositional Gradient Descent Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03357">http://arxiv.org/abs/2307.03357</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ming Yang, Xiyuan Wei, Tianbao Yang, Yiming Ying</li>
<li>for: 本文研究了Stochastic Compositional Optimization（SCO）问题的稳定性和泛化性，即在各种机器学习任务中，如奖励学习、AUC最大化和元学习，目标函数具有嵌套结构和随机性。</li>
<li>methods: 本文使用了统计学习理论的机制来分析SCO算法的稳定性和泛化性。首先，我们引入了一种稳定性概念called compositional uniform stability，并证明其与泛化之间的几何关系。然后，我们证明了SCGD和SCSC算法的 compositional uniform stability 结果。最后，我们 derive了基于稳定性和优化误差的维度独立过剩风险 bounds。</li>
<li>results: 本文的结果显示，通过分析SCO算法的稳定性和泛化性，可以更好地理解这些算法在未来测试示例上的行为。此外，我们还提供了一个基于稳定性和优化误差的维度独立过剩风险 bounds，这是现有的首例研究。<details>
<summary>Abstract</summary>
Many machine learning tasks can be formulated as a stochastic compositional optimization (SCO) problem such as reinforcement learning, AUC maximization, and meta-learning, where the objective function involves a nested composition associated with an expectation. While a significant amount of studies has been devoted to studying the convergence behavior of SCO algorithms, there is little work on understanding their generalization, i.e., how these learning algorithms built from training examples would behave on future test examples. In this paper, we provide the stability and generalization analysis of stochastic compositional gradient descent algorithms through the lens of algorithmic stability in the framework of statistical learning theory. Firstly, we introduce a stability concept called compositional uniform stability and establish its quantitative relation with generalization for SCO problems. Then, we establish the compositional uniform stability results for two popular stochastic compositional gradient descent algorithms, namely SCGD and SCSC. Finally, we derive dimension-independent excess risk bounds for SCGD and SCSC by trade-offing their stability results and optimization errors. To the best of our knowledge, these are the first-ever-known results on stability and generalization analysis of stochastic compositional gradient descent algorithms.
</details>
<details>
<summary>摘要</summary>
多种机器学习任务可以表示为随机 compositional optimization（SCO）问题，如奖励学习、AUC最大化和元学习，其目标函数含有嵌入的嵌入关系。虽然有很多研究关注了 SCO 算法的收敛性行为，但对于这些学习算法在未来测试例子上的表现，却有很少研究。在这篇论文中，我们提供了 SCO 算法的稳定性和泛化分析，通过统计学学习理论的框架。首先，我们引入了一种稳定性概念called compositional uniform stability，并证明其与泛化之间存在确定的关系。然后，我们证明了 SCGD 和 SCSC 两种流行的随机 compositional gradient descent 算法的 compositional uniform stability 结果。最后，我们 derivated 不同维度的维度独立过分的剩余风险 bound，通过考虑这些算法的稳定性结果和优化错误来做出交换。根据我们所知，这些结果是 SCO 算法的稳定性和泛化分析的首次研究成果。
</details></li>
</ul>
<hr>
<h2 id="Federated-Learning-over-a-Wireless-Network-Distributed-User-Selection-through-Random-Access"><a href="#Federated-Learning-over-a-Wireless-Network-Distributed-User-Selection-through-Random-Access" class="headerlink" title="Federated Learning over a Wireless Network: Distributed User Selection through Random Access"></a>Federated Learning over a Wireless Network: Distributed User Selection through Random Access</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03758">http://arxiv.org/abs/2307.03758</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Sun, Shiyao Ma, Ce Zheng, Songtao Wu, Tao Cui, Lingjuan Lyu</li>
<li>for: 降低联合学习（FL）在无线网络上的通信成本，用户选择已成为关键。</li>
<li>methods: 本研究提出了一种基于网络本身的分布式用户选择方法，利用无线资源竞争机制。使用多路访问（CSMA）机制为例，在每次训练中各用户获得Radio资源的机会。</li>
<li>results: 通过控制竞争窗口大小，以增加某些用户在每次训练中获得Radio资源的机会，实现了适度的用户选择。通过训练数据偏迟为FL用户选择目标场景。使用计数机制保证了公平性。在不同的数据集上进行了丰富的实践，并显示该方法可以快速达到与中央用户选择方法相似的准确率。<details>
<summary>Abstract</summary>
User selection has become crucial for decreasing the communication costs of federated learning (FL) over wireless networks. However, centralized user selection causes additional system complexity. This study proposes a network intrinsic approach of distributed user selection that leverages the radio resource competition mechanism in random access. Taking the carrier sensing multiple access (CSMA) mechanism as an example of random access, we manipulate the contention window (CW) size to prioritize certain users for obtaining radio resources in each round of training. Training data bias is used as a target scenario for FL with user selection. Prioritization is based on the distance between the newly trained local model and the global model of the previous round. To avoid excessive contribution by certain users, a counting mechanism is used to ensure fairness. Simulations with various datasets demonstrate that this method can rapidly achieve convergence similar to that of the centralized user selection approach.
</details>
<details>
<summary>摘要</summary>
用户选择已成为聚合学习（FL）在无线网络上减少通信成本的关键。然而，中央化用户选择会增加系统复杂性。本研究提出了基于网络内部的分布式用户选择方法，利用无线资源竞争机制。使用干扰多访问（CSMA）机制为例，我们在每次训练中 manipulate 竞争窗口（CW）大小，以优先给予某些用户无线资源。在训练数据偏袋场景下，我们根据上一轮训练的全球模型与当前轮训练的本地模型之间的距离，对用户进行优先级排序。为避免某些用户的过度贡献，我们使用计数机制保持公平。通过对不同的数据集进行临床示例，我们的方法可以快速达到与中央化用户选择方法相似的减少。
</details></li>
</ul>
<hr>
<h2 id="Distilling-Universal-and-Joint-Knowledge-for-Cross-Domain-Model-Compression-on-Time-Series-Data"><a href="#Distilling-Universal-and-Joint-Knowledge-for-Cross-Domain-Model-Compression-on-Time-Series-Data" class="headerlink" title="Distilling Universal and Joint Knowledge for Cross-Domain Model Compression on Time Series Data"></a>Distilling Universal and Joint Knowledge for Cross-Domain Model Compression on Time Series Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03347">http://arxiv.org/abs/2307.03347</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ijcai2023/uni_kd">https://github.com/ijcai2023/uni_kd</a></li>
<li>paper_authors: Qing Xu, Min Wu, Xiaoli Li, Kezhi Mao, Zhenghua Chen</li>
<li>for: 这个论文旨在提出一个标准化的架构，以便在有限资源的环境中实现深度学习模型的压缩和适应跨领域类别变化。</li>
<li>methods: 这个方法使用了一个新的统一知识传播（UNI-KD）框架，将两个领域之间的知识传播到学习者模型中，包括通用的特征水平知识和共享的数据领域知识。</li>
<li>results: 实验结果显示，这个方法在四个时间序列数据集上的性能比前一代（SOTA）标准更高，并且可以实现跨领域类别变化中的模型压缩和适应。<details>
<summary>Abstract</summary>
For many real-world time series tasks, the computational complexity of prevalent deep leaning models often hinders the deployment on resource-limited environments (e.g., smartphones). Moreover, due to the inevitable domain shift between model training (source) and deploying (target) stages, compressing those deep models under cross-domain scenarios becomes more challenging. Although some of existing works have already explored cross-domain knowledge distillation for model compression, they are either biased to source data or heavily tangled between source and target data. To this end, we design a novel end-to-end framework called Universal and joint knowledge distillation (UNI-KD) for cross-domain model compression. In particular, we propose to transfer both the universal feature-level knowledge across source and target domains and the joint logit-level knowledge shared by both domains from the teacher to the student model via an adversarial learning scheme. More specifically, a feature-domain discriminator is employed to align teacher's and student's representations for universal knowledge transfer. A data-domain discriminator is utilized to prioritize the domain-shared samples for joint knowledge transfer. Extensive experimental results on four time series datasets demonstrate the superiority of our proposed method over state-of-the-art (SOTA) benchmarks.
</details>
<details>
<summary>摘要</summary>
Many real-world 时序系列任务中，现有的深度学习模型的计算复杂性 oft hinders 部署在有限资源环境（例如智能手机）中。此外，由于源领域和目标领域之间的预期域转换，压缩这些深度模型在交叉领域场景下变得更加挑战。虽然一些现有的工作已经探索了交叉领域知识填充，但它们是 either 偏向源数据还是 heavily tangled  между源和目标数据。为此，我们设计了一个 novel 整体框架，即 Universal and joint knowledge distillation（UNI-KD），用于交叉领域模型压缩。具体来说，我们提议将 teacher 模型中的通用特征层级知识传递给学生模型，并在 adversarial learning scheme 中使用 feature-domain discriminator 对 teacher 的表示进行对接。此外，我们还使用 data-domain discriminator 来优先级化目标领域中共享的样本，以便进行交叉领域知识传递。我们对四个时序系列 dataset 进行了广泛的实验，结果表明我们的提议方法比现有的标准准则（SOTA）更高效。
</details></li>
</ul>
<hr>
<h2 id="Dividing-and-Conquering-a-BlackBox-to-a-Mixture-of-Interpretable-Models-Route-Interpret-Repeat"><a href="#Dividing-and-Conquering-a-BlackBox-to-a-Mixture-of-Interpretable-Models-Route-Interpret-Repeat" class="headerlink" title="Dividing and Conquering a BlackBox to a Mixture of Interpretable Models: Route, Interpret, Repeat"></a>Dividing and Conquering a BlackBox to a Mixture of Interpretable Models: Route, Interpret, Repeat</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05350">http://arxiv.org/abs/2307.05350</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/batmanlab/ICML-2023-Route-interpret-repeat">https://github.com/batmanlab/ICML-2023-Route-interpret-repeat</a></li>
<li>paper_authors: Shantanu Ghosh, Ke Yu, Forough Arabshahi, Kayhan Batmanghelich</li>
<li>for: This paper aims to blur the distinction between post hoc explanation of a Blackbox and constructing interpretable models.</li>
<li>methods: The proposed method begins with a Blackbox, iteratively carves out a mixture of interpretable experts (MoIE) and a residual network, and uses First Order Logic (FOL) to provide basic reasoning on concepts from the Blackbox.</li>
<li>results: The extensive experiments show that the proposed approach (1) identifies a diverse set of instance-specific concepts with high concept completeness via MoIE without compromising performance, (2) identifies the relatively “harder” samples to explain via residuals, (3) outperforms the interpretable by-design models by significant margins during test-time interventions, and (4) fixes the shortcut learned by the original Blackbox.Here is the same information in Simplified Chinese:</li>
<li>for: 这篇论文目标是让黑盒模型的解释和可解释模型之间的分化越来越模糊。</li>
<li>methods: 提议的方法从黑盒开始，iteratively刻划出一个混合的可解释专家（MoIE）和剩下的待处理网络，并使用First Order Logic（FOL）提供黑盒中基本的推理。</li>
<li>results: 广泛的实验显示，提议的方法（1）通过MoIE实现了高完整性的实例特定概念，无需牺牲性能，（2）通过剩下的待处理网络实现了对更加“Difficult”的样本的解释，（3）在测试时间干涉中高度超越了可解释设计模型，（4）解决了黑盒学习的短circuit。 MoIE代码可以在以下链接获取：<a target="_blank" rel="noopener" href="https://github.com/batmanlab/ICML-2023-Route-interpret-repeat">https://github.com/batmanlab/ICML-2023-Route-interpret-repeat</a><details>
<summary>Abstract</summary>
ML model design either starts with an interpretable model or a Blackbox and explains it post hoc. Blackbox models are flexible but difficult to explain, while interpretable models are inherently explainable. Yet, interpretable models require extensive ML knowledge and tend to be less flexible and underperforming than their Blackbox variants. This paper aims to blur the distinction between a post hoc explanation of a Blackbox and constructing interpretable models. Beginning with a Blackbox, we iteratively carve out a mixture of interpretable experts (MoIE) and a residual network. Each interpretable model specializes in a subset of samples and explains them using First Order Logic (FOL), providing basic reasoning on concepts from the Blackbox. We route the remaining samples through a flexible residual. We repeat the method on the residual network until all the interpretable models explain the desired proportion of data. Our extensive experiments show that our route, interpret, and repeat approach (1) identifies a diverse set of instance-specific concepts with high concept completeness via MoIE without compromising in performance, (2) identifies the relatively ``harder'' samples to explain via residuals, (3) outperforms the interpretable by-design models by significant margins during test-time interventions, and (4) fixes the shortcut learned by the original Blackbox. The code for MoIE is publicly available at: \url{https://github.com/batmanlab/ICML-2023-Route-interpret-repeat}
</details>
<details>
<summary>摘要</summary>
<<SYS>>模型设计 Either starts with an interpretable model or a Blackbox and explains it post hoc. Blackbox models are flexible but difficult to explain, while interpretable models are inherently explainable. Yet, interpretable models require extensive ML knowledge and tend to be less flexible and underperforming than their Blackbox variants. This paper aims to blur the distinction between a post hoc explanation of a Blackbox and constructing interpretable models. Beginning with a Blackbox, we iteratively carve out a mixture of interpretable experts (MoIE) and a residual network. Each interpretable model specializes in a subset of samples and explains them using First Order Logic (FOL), providing basic reasoning on concepts from the Blackbox. We route the remaining samples through a flexible residual. We repeat the method on the residual network until all the interpretable models explain the desired proportion of data. Our extensive experiments show that our route, interpret, and repeat approach (1) identifies a diverse set of instance-specific concepts with high concept completeness via MoIE without compromising in performance, (2) identifies the relatively ``harder'' samples to explain via residuals, (3) outperforms the interpretable by-design models by significant margins during test-time interventions, and (4) fixes the shortcut learned by the original Blackbox. 模型设计可以开始 Either with an interpretable model or a Blackbox，并在后续进行解释。Blackbox模型具有灵活性，但它们具有困难解释的特性，而可解释模型则具有内在的解释性。然而，可解释模型需要ML知识的涵盖和具有较差的灵活性和性能下降。这篇论文目标是将黑盒模型的后续解释与构建可解释模型进行混合。我们从黑盒模型开始，并在每次迭代中逐步划分出一个混合的可解释专家（MoIE）和剩下的剩余网络。每个可解释模型专门处理一 subset of samples，并使用First Order Logic（FOL）进行基本的推理，提供黑盒模型中的基本概念。我们将剩下的样本通过一个灵活的剩余网络进行路由。我们在剩下的网络上重复这种方法，直到所有的可解释模型解释满足所需的数据比例。我们的广泛的实验表明，我们的路由、解释和重复方法（1）可以通过MoIE无需牺牲性能来实现高度完整的概念，（2）可以通过剩余来解释一些更加困难的样本，（3）在测试时间干涉中大幅度超越可解释设计模型，以及（4）修复黑盒模型中学习的短circuit。MoIE的代码可以在以下地址找到：<https://github.com/batmanlab/ICML-2023-Route-interpret-repeat>
</details></li>
</ul>
<hr>
<h2 id="Personalized-Prediction-of-Recurrent-Stress-Events-Using-Self-Supervised-Learning-on-Multimodal-Time-Series-Data"><a href="#Personalized-Prediction-of-Recurrent-Stress-Events-Using-Self-Supervised-Learning-on-Multimodal-Time-Series-Data" class="headerlink" title="Personalized Prediction of Recurrent Stress Events Using Self-Supervised Learning on Multimodal Time-Series Data"></a>Personalized Prediction of Recurrent Stress Events Using Self-Supervised Learning on Multimodal Time-Series Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03337">http://arxiv.org/abs/2307.03337</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tanvir Islam, Peter Washington</li>
<li>for: 预测chronic stress的发展和影响</li>
<li>methods: 使用穿戴式生物信号数据，采用自我超vision学习（SSL）技术进行个性化预测</li>
<li>results: 在Wearable Stress and Affect Detection（WESAD）数据集上测试，SSL模型表现更好，只需使用 less than 5% 的注释，这表明该方法可以个性化预测chronic stressI hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Chronic stress can significantly affect physical and mental health. The advent of wearable technology allows for the tracking of physiological signals, potentially leading to innovative stress prediction and intervention methods. However, challenges such as label scarcity and data heterogeneity render stress prediction difficult in practice. To counter these issues, we have developed a multimodal personalized stress prediction system using wearable biosignal data. We employ self-supervised learning (SSL) to pre-train the models on each subject's data, allowing the models to learn the baseline dynamics of the participant's biosignals prior to fine-tuning the stress prediction task. We test our model on the Wearable Stress and Affect Detection (WESAD) dataset, demonstrating that our SSL models outperform non-SSL models while utilizing less than 5% of the annotations. These results suggest that our approach can personalize stress prediction to each user with minimal annotations. This paradigm has the potential to enable personalized prediction of a variety of recurring health events using complex multimodal data streams.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Variational-quantum-regression-algorithm-with-encoded-data-structure"><a href="#Variational-quantum-regression-algorithm-with-encoded-data-structure" class="headerlink" title="Variational quantum regression algorithm with encoded data structure"></a>Variational quantum regression algorithm with encoded data structure</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03334">http://arxiv.org/abs/2307.03334</a></li>
<li>repo_url: None</li>
<li>paper_authors: C. -C. Joseph Wang, Ryan S. Bennink</li>
<li>For:  solves practical problems such as combinatorial optimization, quantum chemistry simulation, quantum machine learning, and quantum error correction on noisy quantum computers.* Methods:  constructs a quantum regression algorithm with model interpretability, employs a circuit that directly encodes the data in quantum amplitudes, and uses compressed encoding and digital-analog gate operation to reduce the run time complexity.* Results:  achieves a logarithmic reduction in the number of physical qubits needed compared to traditional one-hot-encoding techniques, and demonstrates the effectiveness of the algorithm for linear and nonlinear regression with ensemble model training and important feature selection.<details>
<summary>Abstract</summary>
Variational quantum algorithms (VQAs) prevail to solve practical problems such as combinatorial optimization, quantum chemistry simulation, quantum machine learning, and quantum error correction on noisy quantum computers. For variational quantum machine learning, a variational algorithm with model interpretability built into the algorithm is yet to be exploited. In this paper, we construct a quantum regression algorithm and identify the direct relation of variational parameters to learned regression coefficients, while employing a circuit that directly encodes the data in quantum amplitudes reflecting the structure of the classical data table. The algorithm is particularly suitable for well-connected qubits. With compressed encoding and digital-analog gate operation, the run time complexity is logarithmically more advantageous than that for digital 2-local gate native hardware with the number of data entries encoded, a decent improvement in noisy intermediate-scale quantum computers and a minor improvement for large-scale quantum computing Our suggested method of compressed binary encoding offers a remarkable reduction in the number of physical qubits needed when compared to the traditional one-hot-encoding technique with the same input data. The algorithm inherently performs linear regression but can also be used easily for nonlinear regression by building nonlinear features into the training data. In terms of measured cost function which distinguishes a good model from a poor one for model training, it will be effective only when the number of features is much less than the number of records for the encoded data structure to be observable. To echo this finding and mitigate hardware noise in practice, the ensemble model training from the quantum regression model learning with important feature selection from regularization is incorporated and illustrated numerically.
</details>
<details>
<summary>摘要</summary>
varyational quantum algorithms (VQAs) prevail in solving practical problems such as combinatorial optimization, quantum chemistry simulation, quantum machine learning, and quantum error correction on noisy quantum computers. For variational quantum machine learning, a variational algorithm with model interpretability built into the algorithm is yet to be exploited. In this paper, we construct a quantum regression algorithm and identify the direct relation of variational parameters to learned regression coefficients, while employing a circuit that directly encodes the data in quantum amplitudes reflecting the structure of the classical data table. The algorithm is particularly suitable for well-connected qubits. With compressed encoding and digital-analog gate operation, the run time complexity is logarithmically more advantageous than that for digital 2-local gate native hardware with the number of data entries encoded, a decent improvement in noisy intermediate-scale quantum computers and a minor improvement for large-scale quantum computing. Our suggested method of compressed binary encoding offers a remarkable reduction in the number of physical qubits needed when compared to the traditional one-hot-encoding technique with the same input data. The algorithm inherently performs linear regression but can also be used easily for nonlinear regression by building nonlinear features into the training data. In terms of measured cost function which distinguishes a good model from a poor one for model training, it will be effective only when the number of features is much less than the number of records for the encoded data structure to be observable. To echo this finding and mitigate hardware noise in practice, the ensemble model training from the quantum regression model learning with important feature selection from regularization is incorporated and illustrated numerically.
</details></li>
</ul>
<hr>
<h2 id="ACDNet-Attention-guided-Collaborative-Decision-Network-for-Effective-Medication-Recommendation"><a href="#ACDNet-Attention-guided-Collaborative-Decision-Network-for-Effective-Medication-Recommendation" class="headerlink" title="ACDNet: Attention-guided Collaborative Decision Network for Effective Medication Recommendation"></a>ACDNet: Attention-guided Collaborative Decision Network for Effective Medication Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03332">http://arxiv.org/abs/2307.03332</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiacong Mi, Yi Zu, Zhuoyuan Wang, Jieyue He</li>
<li>for: 这篇研究旨在提出一个基于电子健康纪录（EHR）的药物建议模型，以帮助医生更好地诊断和治疗病人。</li>
<li>methods: 这篇研究使用了注意力机制和Transformer来实现病人健康状况和药物纪录的有效捕捉，并且运用了一个协同决策架构，通过药物纪录和药物表现之间的相似性来促进建议过程。</li>
<li>results: 实验结果显示，这篇研究在两个大规模医疗数据集MIMIC-III和MIMIC-IV上表现出色，与之前的模型相比，它在Jaccard、PR-AUC和F1分数上明显提高。此外，实验中的删除实验和实验案例显示了每个模组的贡献度，证实了它们对整体性能的贡献。<details>
<summary>Abstract</summary>
Medication recommendation using Electronic Health Records (EHR) is challenging due to complex medical data. Current approaches extract longitudinal information from patient EHR to personalize recommendations. However, existing models often lack sufficient patient representation and overlook the importance of considering the similarity between a patient's medication records and specific medicines. Therefore, an Attention-guided Collaborative Decision Network (ACDNet) for medication recommendation is proposed in this paper. Specifically, ACDNet utilizes attention mechanism and Transformer to effectively capture patient health conditions and medication records by modeling their historical visits at both global and local levels. ACDNet also employs a collaborative decision framework, utilizing the similarity between medication records and medicine representation to facilitate the recommendation process. The experimental results on two extensive medical datasets, MIMIC-III and MIMIC-IV, clearly demonstrate that ACDNet outperforms state-of-the-art models in terms of Jaccard, PR-AUC, and F1 score, reaffirming its superiority. Moreover, the ablation experiments provide solid evidence of the effectiveness of each module in ACDNet, validating their contribution to the overall performance. Furthermore, a detailed case study reinforces the effectiveness of ACDNet in medication recommendation based on EHR data, showcasing its practical value in real-world healthcare scenarios.
</details>
<details>
<summary>摘要</summary>
运用电子健康记录（EHR）提供处方建议是具有复杂医疗资料的挑战。现有方法通常从病人EHR中提取长期信息，以personalize处方建议。然而，现有的模型通常缺乏病人表现的完整性，并忽略了考虑病人处方记录和具体药品之间的相似性。因此，本文提出了一个注意力导向的协同决策网络（ACDNet），用于处方建议。具体来说，ACDNet使用注意力机制和Transformer来有效地捕捉病人健康状态和处方记录，并通过模型病人的历史访问记录，实现全球和局部水平的同步运算。ACDNet还使用协同决策框架，通过考虑处方记录和药品表示之间的相似性，来协助建议过程。实验结果显示，ACDNet在两个大量医疗数据集MIMIC-III和MIMIC-IV上具有较高的Jaccard、PR-AUC和F1分数，与现有模型相比，具体表明其超越性。此外，删除实验显示了每个模组在ACDNet中的贡献，证实它们的贡献为整体性能的重要原因。此外，一个详细的实验案例证明ACDNet在基于EHR数据的处方建议中的实际价值，展现其在实际医疗应用中的实用性。
</details></li>
</ul>
<hr>
<h2 id="Encoder-Decoder-Networks-for-Self-Supervised-Pretraining-and-Downstream-Signal-Bandwidth-Regression-on-Digital-Antenna-Arrays"><a href="#Encoder-Decoder-Networks-for-Self-Supervised-Pretraining-and-Downstream-Signal-Bandwidth-Regression-on-Digital-Antenna-Arrays" class="headerlink" title="Encoder-Decoder Networks for Self-Supervised Pretraining and Downstream Signal Bandwidth Regression on Digital Antenna Arrays"></a>Encoder-Decoder Networks for Self-Supervised Pretraining and Downstream Signal Bandwidth Regression on Digital Antenna Arrays</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03327">http://arxiv.org/abs/2307.03327</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rajib Bhattacharjea, Nathan West</li>
<li>for: 这个研究是应用自动学习技术到数字天线阵列数据上的首次应用。</li>
<li>methods: 研究使用encoder-decoder网络进行自我超vised隐藏重建任务，称为频道填充，用于推断数字天线阵列数据中的 zeros 层面。无需人工标注数据。</li>
<li>results: 我们发现，通过在新网络中转移encoder架构和参数，并在小量标注数据上训练，可以使新网络在数字天线阵列数据上进行带宽调整任务更好than一个Equivalent网络从随机初始化开始训练。<details>
<summary>Abstract</summary>
This work presents the first applications of self-supervised learning applied to data from digital antenna arrays. Encoder-decoder networks are pretrained on digital array data to perform a self-supervised noisy-reconstruction task called channel in-painting, in which the network infers the contents of array data that has been masked with zeros. The self-supervised step requires no human-labeled data. The encoder architecture and weights from pretraining are then transferred to a new network with a task-specific decoder, and the new network is trained on a small volume of labeled data. We show that pretraining on the unlabeled data allows the new network to perform the task of bandwidth regression on the digital array data better than an equivalent network that is trained on the same labeled data from random initialization.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Machine-Learning-to-detect-cyber-attacks-and-discriminating-the-types-of-power-system-disturbances"><a href="#Machine-Learning-to-detect-cyber-attacks-and-discriminating-the-types-of-power-system-disturbances" class="headerlink" title="Machine Learning to detect cyber-attacks and discriminating the types of power system disturbances"></a>Machine Learning to detect cyber-attacks and discriminating the types of power system disturbances</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03323">http://arxiv.org/abs/2307.03323</a></li>
<li>repo_url: None</li>
<li>paper_authors: Diane Tuyizere, Remy Ihabwikuzo</li>
<li>for: 这项研究目标是为智能电网提供机器学习基于攻击探测模型，以便更好地识别和防范攻击。</li>
<li>methods: 该模型使用phasor measuring devices（PMUs）采集数据和日志，并使用机器学习算法来学习系统行为并识别潜在的安全边界。</li>
<li>results: 研究发现，使用Random Forest模型可以达到90.56%的检测精度，并且有助于操作人员做出决策。<details>
<summary>Abstract</summary>
This research proposes a machine learning-based attack detection model for power systems, specifically targeting smart grids. By utilizing data and logs collected from Phasor Measuring Devices (PMUs), the model aims to learn system behaviors and effectively identify potential security boundaries. The proposed approach involves crucial stages including dataset pre-processing, feature selection, model creation, and evaluation. To validate our approach, we used a dataset used, consist of 15 separate datasets obtained from different PMUs, relay snort alarms and logs. Three machine learning models: Random Forest, Logistic Regression, and K-Nearest Neighbour were built and evaluated using various performance metrics. The findings indicate that the Random Forest model achieves the highest performance with an accuracy of 90.56% in detecting power system disturbances and has the potential in assisting operators in decision-making processes.
</details>
<details>
<summary>摘要</summary>
这个研究提出了一种基于机器学习的电力系统攻击检测模型，特别是针对智能电网。通过利用phasor Measuring Devices（PMUs）收集的数据和日志，模型希望学习系统行为并有效地识别潜在的安全边界。提出的方法包括重要的阶段，包括数据集 pré-处理、特征选择、模型创建和评估。为验证我们的方法，我们使用了15个不同PMUs、闭合风暴报警和日志的数据集。我们建立了三种机器学习模型：Random Forest、Logistic Regression和K-Nearest Neighbour，并使用了不同的性能指标进行评估。研究发现，Random Forest模型在检测电力系统干扰的准确率达90.56%，并有助于操作人员决策过程中。
</details></li>
</ul>
<hr>
<h2 id="Assisting-Clinical-Decisions-for-Scarcely-Available-Treatment-via-Disentangled-Latent-Representation"><a href="#Assisting-Clinical-Decisions-for-Scarcely-Available-Treatment-via-Disentangled-Latent-Representation" class="headerlink" title="Assisting Clinical Decisions for Scarcely Available Treatment via Disentangled Latent Representation"></a>Assisting Clinical Decisions for Scarcely Available Treatment via Disentangled Latent Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03315">http://arxiv.org/abs/2307.03315</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bing Xue, Ahmed Sameh Said, Ziqi Xu, Hanyang Liu, Neel Shah, Hanqing Yang, Philip Payne, Chenyang Lu</li>
<li>for: This paper aims to support clinical decisions for COVID-19 patients who require extracorporeal membrane oxygenation (ECMO) treatment.</li>
<li>methods: The paper proposes a novel approach called Treatment Variational AutoEncoder (TVAE) to predict individualized treatment outcomes for COVID-19 patients. TVAE uses a deep latent variable model to represent patients’ potential treatment assignments and factual&#x2F;counterfactual outcomes, and alleviates prediction errors through a reconstruction regularization scheme and semi-supervision.</li>
<li>results: The paper evaluates TVAE on two real-world COVID-19 datasets and shows that it outperforms state-of-the-art treatment effect models in predicting propensity scores and factual outcomes on heterogeneous datasets. Additionally, TVAE outperforms existing models in individual treatment effect estimation on a synthesized dataset.<details>
<summary>Abstract</summary>
Extracorporeal membrane oxygenation (ECMO) is an essential life-supporting modality for COVID-19 patients who are refractory to conventional therapies. However, the proper treatment decision has been the subject of significant debate and it remains controversial about who benefits from this scarcely available and technically complex treatment option. To support clinical decisions, it is a critical need to predict the treatment need and the potential treatment and no-treatment responses. Targeting this clinical challenge, we propose Treatment Variational AutoEncoder (TVAE), a novel approach for individualized treatment analysis. TVAE is specifically designed to address the modeling challenges like ECMO with strong treatment selection bias and scarce treatment cases. TVAE conceptualizes the treatment decision as a multi-scale problem. We model a patient's potential treatment assignment and the factual and counterfactual outcomes as part of their intrinsic characteristics that can be represented by a deep latent variable model. The factual and counterfactual prediction errors are alleviated via a reconstruction regularization scheme together with semi-supervision, and the selection bias and the scarcity of treatment cases are mitigated by the disentangled and distribution-matched latent space and the label-balancing generative strategy. We evaluate TVAE on two real-world COVID-19 datasets: an international dataset collected from 1651 hospitals across 63 countries, and a institutional dataset collected from 15 hospitals. The results show that TVAE outperforms state-of-the-art treatment effect models in predicting both the propensity scores and factual outcomes on heterogeneous COVID-19 datasets. Additional experiments also show TVAE outperforms the best existing models in individual treatment effect estimation on the synthesized IHDP benchmark dataset.
</details>
<details>
<summary>摘要</summary>
外部肺氧化（ECMO）是covid-19患者无法响应传统治疗的生命支持 modalities。然而，正确的治疗决策仍然存在争议，并且不确定哪些患者会从这种罕见和技术复杂的治疗选择中受益。为支持临床决策，我们需要预测治疗需求和可能的治疗和无治疗响应。为解决这种临床挑战，我们提出了个性化治疗分析方法——治疗变量自适应器（TVAE）。TVAE是为了解决ECMO治疗选择偏袋和罕见治疗案例的模型挑战而设计的。我们将患者的可能的治疗决策和实际和对照结果视为患者的内在特征，并使用深度约束模型来表示。寻求和对照预测错误的约束来自重构规则和半监督学习，同时通过分配空间和标签匹配的生成策略来缓解选择偏袋和罕见治疗案例的问题。我们在两个真实世界COVID-19数据集上评估了TVAE：一个国际数据集来自1651家医院在63个国家，另一个机构数据集来自15家医院。结果表明，TVAE在不同COVID-19数据集上预测propensity score和实际结果的性能都高于状态的投入效果模型。此外，我们还通过附加的实验表明，TVAE在个体治疗效果预测方面也超过了 beste existing models。
</details></li>
</ul>
<hr>
<h2 id="On-Invariance-Equivariance-Correlation-and-Convolution-of-Spherical-Harmonic-Representations-for-Scalar-and-Vectorial-Data"><a href="#On-Invariance-Equivariance-Correlation-and-Convolution-of-Spherical-Harmonic-Representations-for-Scalar-and-Vectorial-Data" class="headerlink" title="On Invariance, Equivariance, Correlation and Convolution of Spherical Harmonic Representations for Scalar and Vectorial Data"></a>On Invariance, Equivariance, Correlation and Convolution of Spherical Harmonic Representations for Scalar and Vectorial Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03311">http://arxiv.org/abs/2307.03311</a></li>
<li>repo_url: None</li>
<li>paper_authors: Janis Keuper</li>
<li>for: 这份技术报告提供了圆形幂（SH）频谱中数据的数学表示的深入介绍，包括无法变和对称特征、卷积和圆形幂上信号的精确相关性。</li>
<li>methods: 本文使用了圆形幂表示，包括无法变和对称特征、卷积和圆形幂上信号的精确相关性。</li>
<li>results: 本文扩展了scalar SH表示到vectorial harmonics（VH），为3Dvector场在圆形幂上提供了相同的功能。<details>
<summary>Abstract</summary>
The mathematical representations of data in the Spherical Harmonic (SH) domain has recently regained increasing interest in the machine learning community. This technical report gives an in-depth introduction to the theoretical foundation and practical implementation of SH representations, summarizing works on rotation invariant and equivariant features, as well as convolutions and exact correlations of signals on spheres. In extension, these methods are then generalized from scalar SH representations to Vectorial Harmonics (VH), providing the same capabilities for 3d vector fields on spheres
</details>
<details>
<summary>摘要</summary>
Recently, the mathematical representations of data in the Spherical Harmonic (SH) domain have gained increasing interest in the machine learning community. This technical report provides an in-depth introduction to the theoretical foundation and practical implementation of SH representations, including works on rotation invariant and equivariant features, as well as convolutions and exact correlations of signals on spheres. Additionally, these methods are extended from scalar SH representations to Vectorial Harmonics (VH), enabling the same capabilities for 3D vector fields on spheres.Here's the translation in Traditional Chinese:最近，圆球几何（Spherical Harmonic，SH）领域中的数据数学表现方法在机器学习社区中受到增加的关注。本技术报告将提供深入的理论基础和实践SH表现方法，包括对于旋转不变和对称特征、圆球上信号的卷积和精确相关性。此外，这些方法还被扩展到对 vectorial harmonics（VH），实现3D вектор场在圆球上的相同能力。
</details></li>
</ul>
<hr>
<h2 id="When-Fair-Classification-Meets-Noisy-Protected-Attributes"><a href="#When-Fair-Classification-Meets-Noisy-Protected-Attributes" class="headerlink" title="When Fair Classification Meets Noisy Protected Attributes"></a>When Fair Classification Meets Noisy Protected Attributes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03306">http://arxiv.org/abs/2307.03306</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/evijit/awareness_vs_unawareness">https://github.com/evijit/awareness_vs_unawareness</a></li>
<li>paper_authors: Avijit Ghosh, Pablo Kvitca, Christo Wilson</li>
<li>for: 本研究旨在解决算法公平性的实际挑战，包括数据集中保护属性的可用性和可靠性问题。</li>
<li>methods: 本研究使用了不同的公平分类算法，包括依赖属性、忽略属性和不依赖属性的算法，并对这些算法进行了比较。</li>
<li>results: 研究发现，忽略属性和忽略噪声的公平分类算法可以在保护属性是不可靠或噪声的情况下达到类似的性能水平，但实施需要谨慎。<details>
<summary>Abstract</summary>
The operationalization of algorithmic fairness comes with several practical challenges, not the least of which is the availability or reliability of protected attributes in datasets. In real-world contexts, practical and legal impediments may prevent the collection and use of demographic data, making it difficult to ensure algorithmic fairness. While initial fairness algorithms did not consider these limitations, recent proposals aim to achieve algorithmic fairness in classification by incorporating noisiness in protected attributes or not using protected attributes at all.   To the best of our knowledge, this is the first head-to-head study of fair classification algorithms to compare attribute-reliant, noise-tolerant and attribute-blind algorithms along the dual axes of predictivity and fairness. We evaluated these algorithms via case studies on four real-world datasets and synthetic perturbations. Our study reveals that attribute-blind and noise-tolerant fair classifiers can potentially achieve similar level of performance as attribute-reliant algorithms, even when protected attributes are noisy. However, implementing them in practice requires careful nuance. Our study provides insights into the practical implications of using fair classification algorithms in scenarios where protected attributes are noisy or partially available.
</details>
<details>
<summary>摘要</summary>
“algorithmic fairness的实施面临多种实际挑战，其中最大的问题之一是数据集中保护特征的可用性和可靠性。在真实世界中，法律和实际困难可能会阻止对民生数据的收集和使用，使得保证algorithmic fairness变得困难。初期的公平算法并不考虑这些限制，但最新的建议旨在通过不考虑保护特征或使用噪音来实现公平分类。根据我们所知，这是首次对公平分类算法进行了头对头比较，并考虑了两个轴：预测性和公平性。我们通过四个真实世界数据集和 sintetic perturbations 进行了测试。我们的研究发现，忽略保护特征和噪音忍容的公平分类算法可能能够与依赖保护特征的算法具有相似的性能水平，即使保护特征噪音。但是，在实践中实现这些算法需要谨慎。我们的研究为在保护特征噪音或部分可用的场景中使用公平分类算法提供了实践意义。”Note: Simplified Chinese is also known as "Mandarin" or "Standard Chinese".
</details></li>
</ul>
<hr>
<h2 id="A-Vulnerability-of-Attribution-Methods-Using-Pre-Softmax-Scores"><a href="#A-Vulnerability-of-Attribution-Methods-Using-Pre-Softmax-Scores" class="headerlink" title="A Vulnerability of Attribution Methods Using Pre-Softmax Scores"></a>A Vulnerability of Attribution Methods Using Pre-Softmax Scores</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03305">http://arxiv.org/abs/2307.03305</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mlerma54/adversarial-attacks-on-saliency-maps">https://github.com/mlerma54/adversarial-attacks-on-saliency-maps</a></li>
<li>paper_authors: Miguel Lerma, Mirtha Lucas</li>
<li>for: 这种论文探讨了一种类型的归类器中的拟合方法，即使这种模型受到了恶意攻击，小量修改模型也可以导致拟合方法的解释结果受到影响。</li>
<li>methods: 这种论文使用了一种类型的归类器，并使用了某些修改方法来影响拟合方法的解释结果。</li>
<li>results: 研究发现，这种修改方法可以导致拟合方法的解释结果受到影响，而不需要改变模型的输出。<details>
<summary>Abstract</summary>
We discuss a vulnerability involving a category of attribution methods used to provide explanations for the outputs of convolutional neural networks working as classifiers. It is known that this type of networks are vulnerable to adversarial attacks, in which imperceptible perturbations of the input may alter the outputs of the model. In contrast, here we focus on effects that small modifications in the model may cause on the attribution method without altering the model outputs.
</details>
<details>
<summary>摘要</summary>
我们讨论了一个漏洞，它与对于卷积神经网作为分类器的说明方法有关。知道这种神经网容易受到敌意攻击，这种攻击可以通过对输入进行微妙的变化，导致模型的输出变化。相反，我们在这里专注于对于说明方法的小修改，不会改变模型的输出。
</details></li>
</ul>
<hr>
<h2 id="Equivariant-Spherical-CNN-for-Data-Efficient-and-High-Performance-Medical-Image-Processing"><a href="#Equivariant-Spherical-CNN-for-Data-Efficient-and-High-Performance-Medical-Image-Processing" class="headerlink" title="Equivariant Spherical CNN for Data Efficient and High-Performance Medical Image Processing"></a>Equivariant Spherical CNN for Data Efficient and High-Performance Medical Image Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03298">http://arxiv.org/abs/2307.03298</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amirreza Hashemi, Yuemeng Feng, Hamid Sabet</li>
<li>for: 这个研究旨在提高医疗图像处理领域中的Tomography应用，并且提出了一种新的对称网络方法来改善这些应用的效率和性能。</li>
<li>methods: 这个研究使用了一种叫做对称网络的方法，这种方法可以将医疗图像处理中的训练集不断地缩小，以提高网络的稳定性和效率。</li>
<li>results: 研究结果显示，使用对称网络可以实现医疗图像处理中的高品质和高效率，并且可以降低训练集的size，以减少训练时间和计算成本。<details>
<summary>Abstract</summary>
This work highlights the significance of equivariant networks as efficient and high-performance approaches for tomography applications. Our study builds upon the limitations of Convolutional Neural Networks (CNNs), which have shown promise in post-processing various medical imaging systems. However, the efficiency of conventional CNNs heavily relies on an undiminished and proper training set. To tackle this issue, in this study, we introduce an equivariant network, aiming to reduce CNN's dependency on specific training sets. We evaluate the efficacy of equivariant CNNs on spherical signals for tomographic medical imaging problems. Our results demonstrate superior quality and computational efficiency of spherical CNNs (SCNNs) in denoising and reconstructing benchmark problems. Furthermore, we propose a novel approach to employ SCNNs as a complement to conventional image reconstruction tools, enhancing the outcomes while reducing reliance on the training set. Across all cases, we observe a significant decrease in computational costs while maintaining the same or higher quality of image processing using SCNNs compared to CNNs. Additionally, we explore the potential of this network for broader tomography applications, particularly those requiring omnidirectional representation.
</details>
<details>
<summary>摘要</summary>
Translation note:* "Equivariant networks" is translated as "协变网络" (fùbiàn wǎngluò), which means the network architecture that preserves the symmetry of the input data.* "Spherical signals" is translated as "球形信号" (qiúxíng xìnhù), which refers to the signals that have spherical symmetry.* "Tomographic medical imaging" is translated as "tomography医学影像" (tòngshì yīxué yǐngxiàng), which refers to the medical imaging techniques that use X-rays or other forms of radiation to create cross-sectional images of the body.* "Convolutional Neural Networks" is translated as "卷积神经网络" (juéshì shénxiào wǎngluò), which is the abbreviation of CNNs.* "Omnidirectional representation" is translated as "全方位表示" (quánfāngwèi bǎoshì), which means the representation that captures the information from all directions.
</details></li>
</ul>
<hr>
<h2 id="OmniBoost-Boosting-Throughput-of-Heterogeneous-Embedded-Devices-under-Multi-DNN-Workload"><a href="#OmniBoost-Boosting-Throughput-of-Heterogeneous-Embedded-Devices-under-Multi-DNN-Workload" class="headerlink" title="OmniBoost: Boosting Throughput of Heterogeneous Embedded Devices under Multi-DNN Workload"></a>OmniBoost: Boosting Throughput of Heterogeneous Embedded Devices under Multi-DNN Workload</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03290">http://arxiv.org/abs/2307.03290</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andreas Karatzas, Iraklis Anagnostopoulos</li>
<li>for: 提高多个深度神经网络（DNN）应用工作负载的高性能和高效率</li>
<li>methods: 使用杂种加速器、硬件异构性和随机空间探索技术</li>
<li>results: 与其他状态对比方法相比，实现了平均吞吐量提高4.6倍<details>
<summary>Abstract</summary>
Modern Deep Neural Networks (DNNs) exhibit profound efficiency and accuracy properties. This has introduced application workloads that comprise of multiple DNN applications, raising new challenges regarding workload distribution. Equipped with a diverse set of accelerators, newer embedded system present architectural heterogeneity, which current run-time controllers are unable to fully utilize. To enable high throughput in multi-DNN workloads, such a controller is ought to explore hundreds of thousands of possible solutions to exploit the underlying heterogeneity. In this paper, we propose OmniBoost, a lightweight and extensible multi-DNN manager for heterogeneous embedded devices. We leverage stochastic space exploration and we combine it with a highly accurate performance estimator to observe a x4.6 average throughput boost compared to other state-of-the-art methods. The evaluation was performed on the HiKey970 development board.
</details>
<details>
<summary>摘要</summary>
现代深度神经网络（DNN）具有深刻的效率和准确性特性。这引入了包含多个DNN应用的工作负荷，引起了新的工作负荷分布挑战。新的嵌入式系统采用多种加速器，导致系统架构多样性，现有的运行时控制器无法完全利用。为实现高吞吨在多个DNN工作负荷中，这种控制器应该探索数以千计的可能性。在这篇论文中，我们提出了OmniBoost，一个轻量级的多DNN管理器，适用于多种嵌入式设备。我们利用随机空间探索和高度准确的性能估计器，观察到与其他状态态方法相比，平均吞吨提升4.6倍。测试结果在HiKey970开发板上进行。
</details></li>
</ul>
<hr>
<h2 id="Optimal-Scalarizations-for-Sublinear-Hypervolume-Regret"><a href="#Optimal-Scalarizations-for-Sublinear-Hypervolume-Regret" class="headerlink" title="Optimal Scalarizations for Sublinear Hypervolume Regret"></a>Optimal Scalarizations for Sublinear Hypervolume Regret</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03288">http://arxiv.org/abs/2307.03288</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiuyi Zhang</li>
<li>for: 本研究旨在找到一种简单的非线性归一化方法，可以在多目标设定中探索多个目标的 pareto 前沿，并提高搜索效率。</li>
<li>methods: 我们使用了 hypervolume 归一化方法，并采用了随机权重的方法来评估不同的归一化方法。</li>
<li>results: 我们的研究表明，使用 hypervolume 归一化方法可以获得提高的搜索效率，并且可以在多目标问题中提供更好的解决方案。我们的实验结果也表明，使用简单的 hypervolume 归一化方法可以在 bayesian 优化中表现更好，并且可以超越标准的多目标算法，如 EHVI。<details>
<summary>Abstract</summary>
Scalarization is a general technique that can be deployed in any multiobjective setting to reduce multiple objectives into one, such as recently in RLHF for training reward models that align human preferences. Yet some have dismissed this classical approach because linear scalarizations are known to miss concave regions of the Pareto frontier. To that end, we aim to find simple non-linear scalarizations that can explore a diverse set of $k$ objectives on the Pareto frontier, as measured by the dominated hypervolume. We show that hypervolume scalarizations with uniformly random weights are surprisingly optimal for provably minimizing the hypervolume regret, achieving an optimal sublinear regret bound of $O(T^{-1/k})$, with matching lower bounds that preclude any algorithm from doing better asymptotically. As a theoretical case study, we consider the multiobjective stochastic linear bandits problem and demonstrate that by exploiting the sublinear regret bounds of the hypervolume scalarizations, we can derive a novel non-Euclidean analysis that produces improved hypervolume regret bounds of $\tilde{O}( d T^{-1/2} + T^{-1/k})$. We support our theory with strong empirical performance of using simple hypervolume scalarizations that consistently outperforms both the linear and Chebyshev scalarizations, as well as standard multiobjective algorithms in bayesian optimization, such as EHVI.
</details>
<details>
<summary>摘要</summary>
scalarization 是一种通用技术，可以在多目标设置中降低多个目标到一个，例如在RLHF中训练奖励模型，以实现人类偏好的Alignment。然而，一些人认为这种经典方法不合适，因为线性Scalarization会错过凹陷区域的Pareto前沿。为此，我们想找到简单的非线性Scalarization，以探索$k$个目标在Pareto前沿上的多样化集合，由dominated hypervolume来度量。我们表明，在随机权重下的 hypervolume scalarization 可以让我们提取优质的 hypervolume regret，实现 $O(T^{-1/k})$ 的优linear regret bound，与它们匹配的下界，阻止任何算法在极限情况下做得更好。作为一个理论案例，我们考虑了多目标随机线性带宽问题，并证明了通过权重Scalarization 的Sublinear regret bound，我们可以 derivate一个新的非Euclidean分析，生成改进的 hypervolume regret bound 的 $\tilde{O}(dT^{-1/2} + T^{-1/k})$。我们的理论实际上支持了使用简单的 hypervolume scalarization，常常超越了线性和Chebyshev scalarization，以及标准多目标算法在 bayesian optimization 中，如EHVI。
</details></li>
</ul>
<hr>
<h2 id="Empirical-Analysis-of-a-Segmentation-Foundation-Model-in-Prostate-Imaging"><a href="#Empirical-Analysis-of-a-Segmentation-Foundation-Model-in-Prostate-Imaging" class="headerlink" title="Empirical Analysis of a Segmentation Foundation Model in Prostate Imaging"></a>Empirical Analysis of a Segmentation Foundation Model in Prostate Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03266">http://arxiv.org/abs/2307.03266</a></li>
<li>repo_url: None</li>
<li>paper_authors: Heejong Kim, Victor Ion Butoi, Adrian V. Dalca, Daniel J. A. Margolis, Mert R. Sabuncu</li>
<li>for: This paper is written for the purpose of evaluating the effectiveness of a foundation model for medical image segmentation, specifically in the context of prostate imaging.</li>
<li>methods: The paper uses a recently developed foundation model called UniverSeg, which is compared against the conventional approach of training a task-specific segmentation model.</li>
<li>results: The study finds that the foundation model achieves competitive performance in prostate imaging segmentation, and highlights several important factors that will be important in the development and adoption of foundation models for medical image segmentation.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文是为了评估医疗图像分割领域中的基础模型效果，具体来说是在肾脏成像中进行评估。</li>
<li>methods: 这篇论文使用了一个最近开发的基础模型，即UniverSeg，与传统的任务特定分割模型进行比较。</li>
<li>results: 研究发现，基础模型在肾脏成像分割中实现了竞争性的性能，并提出了各种重要因素，这些因素将在基础模型的开发和应用中扮演重要的角色。<details>
<summary>Abstract</summary>
Most state-of-the-art techniques for medical image segmentation rely on deep-learning models. These models, however, are often trained on narrowly-defined tasks in a supervised fashion, which requires expensive labeled datasets. Recent advances in several machine learning domains, such as natural language generation have demonstrated the feasibility and utility of building foundation models that can be customized for various downstream tasks with little to no labeled data. This likely represents a paradigm shift for medical imaging, where we expect that foundation models may shape the future of the field. In this paper, we consider a recently developed foundation model for medical image segmentation, UniverSeg. We conduct an empirical evaluation study in the context of prostate imaging and compare it against the conventional approach of training a task-specific segmentation model. Our results and discussion highlight several important factors that will likely be important in the development and adoption of foundation models for medical image segmentation.
</details>
<details>
<summary>摘要</summary>
现代医疗影像分类技术多采用深度学习模型。然而，这些模型通常需要高价的标签数据来训练，导致成本高昂。近年，自然语言生成等机器学习领域的进步，已经证明了建立基础模型，可以根据不同的下游任务进行定制，仅需少量或无标签数据。这将可能成为医疗影像领域的新模式，我们预料基础模型将未来医疗影像领域的发展推动。本文考虑了最近发展的医疗影像分类基础模型UniverSeg，并在阴茎影像上进行了实验性评估，与传统方法（即训练专门的医疗影像分类模型）进行比较。我们的结果和讨论显示了一些重要的因素，将影响医疗影像分类基础模型的发展和采用。
</details></li>
</ul>
<hr>
<h2 id="Vision-Language-Transformers-A-Survey"><a href="#Vision-Language-Transformers-A-Survey" class="headerlink" title="Vision Language Transformers: A Survey"></a>Vision Language Transformers: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03254">http://arxiv.org/abs/2307.03254</a></li>
<li>repo_url: None</li>
<li>paper_authors: Clayton Fields, Casey Kennington</li>
<li>for: 这个论文旨在总结目前已经公布的视觉语言传感器模型研究，以及这些模型在不同任务上的应用和表现。</li>
<li>methods: 这些模型使用了基于transformer架构的 pré-training方法，并通过微调参数和架构来适应不同任务。</li>
<li>results: 这些模型在视觉语言任务上表现出色，并且在不同任务上具有较高的灵活性和适应能力。<details>
<summary>Abstract</summary>
Vision language tasks, such as answering questions about or generating captions that describe an image, are difficult tasks for computers to perform. A relatively recent body of research has adapted the pretrained transformer architecture introduced in \citet{vaswani2017attention} to vision language modeling. Transformer models have greatly improved performance and versatility over previous vision language models. They do so by pretraining models on a large generic datasets and transferring their learning to new tasks with minor changes in architecture and parameter values. This type of transfer learning has become the standard modeling practice in both natural language processing and computer vision. Vision language transformers offer the promise of producing similar advancements in tasks which require both vision and language. In this paper, we provide a broad synthesis of the currently available research on vision language transformer models and offer some analysis of their strengths, limitations and some open questions that remain.
</details>
<details>
<summary>摘要</summary>
computer vision tasks that require both vision and language, such as answering questions about or generating captions that describe an image, are difficult for computers to perform. Recently, researchers have adapted the pretrained transformer architecture introduced in \citet{vaswani2017attention} to vision language modeling, which has greatly improved performance and versatility over previous vision language models. These models are trained on large generic datasets and then transferred to new tasks with minor changes in architecture and parameter values, which has become the standard modeling practice in both natural language processing and computer vision. Vision language transformers offer the promise of producing similar advancements in tasks that require both vision and language. In this paper, we provide a comprehensive overview of the currently available research on vision language transformer models and offer some analysis of their strengths, limitations, and open questions that remain.
</details></li>
</ul>
<hr>
<h2 id="Learned-Kernels-for-Interpretable-and-Efficient-PPG-Signal-Quality-Assessment-and-Artifact-Segmentation"><a href="#Learned-Kernels-for-Interpretable-and-Efficient-PPG-Signal-Quality-Assessment-and-Artifact-Segmentation" class="headerlink" title="Learned Kernels for Interpretable and Efficient PPG Signal Quality Assessment and Artifact Segmentation"></a>Learned Kernels for Interpretable and Efficient PPG Signal Quality Assessment and Artifact Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05385">http://arxiv.org/abs/2307.05385</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sully F. Chen, Zhicheng Guo, Cheng Ding, Xiao Hu, Cynthia Rudin</li>
<li>for: 本研究旨在提出一种可靠、高效、可解释的信号质量评估和噪声分 Segmentation方法，以确保robust和准确地提取生物Physiological Parameters。</li>
<li>methods: 本方法使用了一种小量且可解释的卷积核来学习，与之前的手工特征检测器或信号度量计算相比，具有更高的性能，同时具有可解释性和低功耗特性。</li>
<li>results: 本研究实验结果表明，提出的方法可以与现有的深度神经网络（DNN）方法相当或更好地提取Physiological Parameters，同时具有许多次更多的参数和更高的计算和存储效率。<details>
<summary>Abstract</summary>
Photoplethysmography (PPG) provides a low-cost, non-invasive method to continuously monitor various cardiovascular parameters. PPG signals are generated by wearable devices and frequently contain large artifacts caused by external factors, such as motion of the human subject. In order to ensure robust and accurate extraction of physiological parameters, corrupted areas of the signal need to be identified and handled appropriately. Previous methodology relied either on handcrafted feature detectors or signal metrics which yield sub-optimal performance, or relied on machine learning techniques such as deep neural networks (DNN) which lack interpretability and are computationally and memory intensive. In this work, we present a novel method to learn a small set of interpretable convolutional kernels that has performance similar to -- and often better than -- the state-of-the-art DNN approach with several orders of magnitude fewer parameters. This work allows for efficient, robust, and interpretable signal quality assessment and artifact segmentation on low-power devices.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Neural-Network-Field-Theories-Non-Gaussianity-Actions-and-Locality"><a href="#Neural-Network-Field-Theories-Non-Gaussianity-Actions-and-Locality" class="headerlink" title="Neural Network Field Theories: Non-Gaussianity, Actions, and Locality"></a>Neural Network Field Theories: Non-Gaussianity, Actions, and Locality</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03223">http://arxiv.org/abs/2307.03223</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mehmet Demirtas, James Halverson, Anindita Maiti, Matthew D. Schwartz, Keegan Stoner</li>
<li>for: 这篇论文探讨了场理论中的征function distribution，以及由 neural network ensemble describe这种分布的可能性。</li>
<li>methods: 论文使用了场理论中的中心限定定律，以及对 neural network 参数的小量偏置，来描述分布。</li>
<li>results: 论文表明，在 infinite-width （infinite-$N） Limit下， neural network ensemble可以被视为一种自由场理论，并且可以使用 field theory 的方法来描述。<details>
<summary>Abstract</summary>
Both the path integral measure in field theory and ensembles of neural networks describe distributions over functions. When the central limit theorem can be applied in the infinite-width (infinite-$N$) limit, the ensemble of networks corresponds to a free field theory. Although an expansion in $1/N$ corresponds to interactions in the field theory, others, such as in a small breaking of the statistical independence of network parameters, can also lead to interacting theories. These other expansions can be advantageous over the $1/N$-expansion, for example by improved behavior with respect to the universal approximation theorem. Given the connected correlators of a field theory, one can systematically reconstruct the action order-by-order in the expansion parameter, using a new Feynman diagram prescription whose vertices are the connected correlators. This method is motivated by the Edgeworth expansion and allows one to derive actions for neural network field theories. Conversely, the correspondence allows one to engineer architectures realizing a given field theory by representing action deformations as deformations of neural network parameter densities. As an example, $\phi^4$ theory is realized as an infinite-$N$ neural network field theory.
</details>
<details>
<summary>摘要</summary>
Both the path integral measure in field theory and ensembles of neural networks describe distributions over functions. When the central limit theorem can be applied in the infinite-width (infinite-$N$) limit, the ensemble of networks corresponds to a free field theory. Although an expansion in $1/N$ corresponds to interactions in the field theory, others, such as in a small breaking of the statistical independence of network parameters, can also lead to interacting theories. These other expansions can be advantageous over the $1/N$-expansion, for example by improved behavior with respect to the universal approximation theorem. Given the connected correlators of a field theory, one can systematically reconstruct the action order-by-order in the expansion parameter, using a new Feynman diagram prescription whose vertices are the connected correlators. This method is motivated by the Edgeworth expansion and allows one to derive actions for neural network field theories. Conversely, the correspondence allows one to engineer architectures realizing a given field theory by representing action deformations as deformations of neural network parameter densities. As an example, $\phi^4$ theory is realized as an infinite-$N$ neural network field theory.Here's the translation in Traditional Chinese: Both the path integral measure in field theory and ensembles of neural networks describe distributions over functions. When the central limit theorem can be applied in the infinite-width (infinite-$N$) limit, the ensemble of networks corresponds to a free field theory. Although an expansion in $1/N$ corresponds to interactions in the field theory, others, such as in a small breaking of the statistical independence of network parameters, can also lead to interacting theories. These other expansions can be advantageous over the $1/N$-expansion, for example by improved behavior with respect to the universal approximation theorem. Given the connected correlators of a field theory, one can systematically reconstruct the action order-by-order in the expansion parameter, using a new Feynman diagram prescription whose vertices are the connected correlators. This method is motivated by the Edgeworth expansion and allows one to derive actions for neural network field theories. Conversely, the correspondence allows one to engineer architectures realizing a given field theory by representing action deformations as deformations of neural network parameter densities. As an example, $\phi^4$ theory is realized as an infinite-$N$ neural network field theory.
</details></li>
</ul>
<hr>
<h2 id="Synthesizing-Artistic-Cinemagraphs-from-Text"><a href="#Synthesizing-Artistic-Cinemagraphs-from-Text" class="headerlink" title="Synthesizing Artistic Cinemagraphs from Text"></a>Synthesizing Artistic Cinemagraphs from Text</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03190">http://arxiv.org/abs/2307.03190</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/text2cinemagraph/text2cinemagraph">https://github.com/text2cinemagraph/text2cinemagraph</a></li>
<li>paper_authors: Aniruddha Mahapatra, Aliaksandr Siarohin, Hsin-Ying Lee, Sergey Tulyakov, Jun-Yan Zhu</li>
<li>for: 本研究旨在创建基于文本描述的电影画面。</li>
<li>methods: 本方法使用了自动生成图像双胞胎的想法，通过将文本描述转化为一对包含艺术风格和自然风格的图像。然后，通过分析自然图像和视频数据，对实际图像进行 segmentation 和动作预测，并将预测动作传递到艺术图像中。</li>
<li>results: 本研究的结果表明， compared to现有方法，本方法在创建自然风景以及艺术和其他世界的电影画面方面表现出色，并且可以控制动作方向使用文本。此外，本研究还扩展到了将现有的画作动画化以及通过文本控制动作方向。<details>
<summary>Abstract</summary>
We introduce Text2Cinemagraph, a fully automated method for creating cinemagraphs from text descriptions - an especially challenging task when prompts feature imaginary elements and artistic styles, given the complexity of interpreting the semantics and motions of these images. Existing single-image animation methods fall short on artistic inputs, and recent text-based video methods frequently introduce temporal inconsistencies, struggling to keep certain regions static. To address these challenges, we propose an idea of synthesizing image twins from a single text prompt - a pair of an artistic image and its pixel-aligned corresponding natural-looking twin. While the artistic image depicts the style and appearance detailed in our text prompt, the realistic counterpart greatly simplifies layout and motion analysis. Leveraging existing natural image and video datasets, we can accurately segment the realistic image and predict plausible motion given the semantic information. The predicted motion can then be transferred to the artistic image to create the final cinemagraph. Our method outperforms existing approaches in creating cinemagraphs for natural landscapes as well as artistic and other-worldly scenes, as validated by automated metrics and user studies. Finally, we demonstrate two extensions: animating existing paintings and controlling motion directions using text.
</details>
<details>
<summary>摘要</summary>
我们介绍Text2Cinemagraph，一种完全自动的方法，可以从文本描述中生成电影图像 - 特别是处理含有想象元素和艺术风格的描述时，这是一个非常困难的任务。现有的单图动画方法在艺术输入方面有限，而 recient的文本基于视频方法经常出现时间不一致，尝试维持某些区域静止。为解决这些挑战，我们提出了一种将文本描述转化为两个图像的想法 - 一个是一个艺术性的图像，另一个是其像素对齐的自然看起来的图像。而艺术性的图像会具有文本描述中的风格和形态，而自然图像则会大大简化布局和动作分析。利用现有的自然图像和视频数据集，我们可以准确地分割自然图像，并预测可能的动作，基于 semantic信息。预测的动作然后可以被传递到艺术性的图像，以创建最终的电影图像。我们的方法比既有方法在创建电影图像的自然风景以及艺术和其他世界的场景上表现出色，并通过自动度量和用户研究得到了证明。最后，我们还展示了两种扩展：将现有的画作动画并控制动作方向使用文本。
</details></li>
</ul>
<hr>
<h2 id="TGRL-An-Algorithm-for-Teacher-Guided-Reinforcement-Learning"><a href="#TGRL-An-Algorithm-for-Teacher-Guided-Reinforcement-Learning" class="headerlink" title="TGRL: An Algorithm for Teacher Guided Reinforcement Learning"></a>TGRL: An Algorithm for Teacher Guided Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03186">http://arxiv.org/abs/2307.03186</a></li>
<li>repo_url: None</li>
<li>paper_authors: Idan Shenfeld, Zhang-Wei Hong, Aviv Tamar, Pulkit Agrawal</li>
<li>for: 本文目的是解决Sequential Decision-Making问题，通过结合权威指导和奖励学习两种已知方法。</li>
<li>methods: 本文使用的方法是在权威指导和奖励学习目标之间进行平衡，以实现更好的性能。</li>
<li>results: 本文的实验结果显示，使用Teacher Guided Reinforcement Learning（TGRL）方法可以在多个领域中超越强基线，而无需进行参数调整。<details>
<summary>Abstract</summary>
Learning from rewards (i.e., reinforcement learning or RL) and learning to imitate a teacher (i.e., teacher-student learning) are two established approaches for solving sequential decision-making problems. To combine the benefits of these different forms of learning, it is common to train a policy to maximize a combination of reinforcement and teacher-student learning objectives. However, without a principled method to balance these objectives, prior work used heuristics and problem-specific hyperparameter searches to balance the two objectives. We present a $\textit{principled}$ approach, along with an approximate implementation for $\textit{dynamically}$ and $\textit{automatically}$ balancing when to follow the teacher and when to use rewards. The main idea is to adjust the importance of teacher supervision by comparing the agent's performance to the counterfactual scenario of the agent learning without teacher supervision and only from rewards. If using teacher supervision improves performance, the importance of teacher supervision is increased and otherwise it is decreased. Our method, $\textit{Teacher Guided Reinforcement Learning}$ (TGRL), outperforms strong baselines across diverse domains without hyper-parameter tuning.
</details>
<details>
<summary>摘要</summary>
学习从奖励（i.e., 奖励学习或RL）和学习教师（i.e., 教师学习）是两种成熔的解决Sequential decision-making问题的方法。为了结合这些不同的学习方法的优点，通常是训练一个策略以最大化权重的奖励和教师学习目标。然而，在过去，无法使用原则性的方法均衡这两个目标，而是使用规则和问题特有的超参数搜索来均衡。我们提出了一种原则性的方法，以及一种近似的实现方式，可以在运动时动态地和自动地调整在学习从教师和奖励中选择何时遵循教师的指导。我们的方法被称为“教师导向奖励学习”（TGRL），在多个领域中击败了强大的基准值，无需hyperparameter调整。
</details></li>
</ul>
<hr>
<h2 id="Quantification-of-Uncertainty-with-Adversarial-Models"><a href="#Quantification-of-Uncertainty-with-Adversarial-Models" class="headerlink" title="Quantification of Uncertainty with Adversarial Models"></a>Quantification of Uncertainty with Adversarial Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03217">http://arxiv.org/abs/2307.03217</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ml-jku/quam">https://github.com/ml-jku/quam</a></li>
<li>paper_authors: Kajetan Schweighofer, Lukas Aichberger, Mykyta Ielanskyi, Günter Klambauer, Sepp Hochreiter</li>
<li>for: 这篇论文的目的是提出一种新的不确定量化方法，以便在实际应用中做出可靠的预测。</li>
<li>methods: 这篇论文使用了rival models的对抗方法（QUAM）来估计epistemic uncertainty，这种方法可以更好地估计这种不确定性，并且比前一些方法（如深度组合或MC dropout）更加精确。</li>
<li>results: 实验显示，QUAM方法可以优化深度学习模型中的不确定量化，并且在类型识别、物体检测和其他视觉任务中表现出色，比前一些方法更好。<details>
<summary>Abstract</summary>
Quantifying uncertainty is important for actionable predictions in real-world applications. A crucial part of predictive uncertainty quantification is the estimation of epistemic uncertainty, which is defined as an integral of the product between a divergence function and the posterior. Current methods such as Deep Ensembles or MC dropout underperform at estimating the epistemic uncertainty, since they primarily consider the posterior when sampling models. We suggest Quantification of Uncertainty with Adversarial Models (QUAM) to better estimate the epistemic uncertainty. QUAM identifies regions where the whole product under the integral is large, not just the posterior. Consequently, QUAM has lower approximation error of the epistemic uncertainty compared to previous methods. Models for which the product is large correspond to adversarial models (not adversarial examples!). Adversarial models have both a high posterior as well as a high divergence between their predictions and that of a reference model. Our experiments show that QUAM excels in capturing epistemic uncertainty for deep learning models and outperforms previous methods on challenging tasks in the vision domain.
</details>
<details>
<summary>摘要</summary>
量化未知是重要的 predictive uncertainty quantification 中的一部分。 epistemic uncertainty 的定义为积分函数和 posterior 的产品。现有的方法，如 Deep Ensembles 或 MC dropout，在估计 epistemic uncertainty 方面表现不佳，因为它们主要依靠 posterior 的样本。我们建议 Quantification of Uncertainty with Adversarial Models (QUAM)，可以更好地估计 epistemic uncertainty。QUAM 可以在积分函数下找到整体积分值大的区域，不仅是 posterior。因此，QUAM 的 Approximation error 相对于之前的方法更低。模型具有高积分值的区域对应于 adversarial models（不是 adversarial examples！）。 adversarial models 具有高 posterior 和 reference model 的预测值之间的差异。我们的实验表明，QUAM 在 deep learning 模型中表现出色，与之前的方法在视觉领域中的 challenging tasks 上表现出色。
</details></li>
</ul>
<hr>
<h2 id="Learning-Curves-for-Heterogeneous-Feature-Subsampled-Ridge-Ensembles"><a href="#Learning-Curves-for-Heterogeneous-Feature-Subsampled-Ridge-Ensembles" class="headerlink" title="Learning Curves for Heterogeneous Feature-Subsampled Ridge Ensembles"></a>Learning Curves for Heterogeneous Feature-Subsampled Ridge Ensembles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03176">http://arxiv.org/abs/2307.03176</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/benruben87/Learning-Curves-for-Heterogeneous-Feature-Subsampled-Ridge-Ensembles">https://github.com/benruben87/Learning-Curves-for-Heterogeneous-Feature-Subsampled-Ridge-Ensembles</a></li>
<li>paper_authors: Benjamin S. Ruben, Cengiz Pehlevan</li>
<li>for: 降低预测差异的方法，使用Random Subspace Method和Feature Bagging方法。</li>
<li>methods: 使用ridge regression在子集中适应特征，并使用statistical physics的replica trick来 derivate学习曲线。</li>
<li>results: 在线性回归设置下，通过调整子集大小和特征数量，实现更好的预测性能，并发现在参数空间中存在锐transition。<details>
<summary>Abstract</summary>
Feature bagging is a well-established ensembling method which aims to reduce prediction variance by training estimators in an ensemble on random subsamples or projections of features. Typically, ensembles are chosen to be homogeneous, in the sense the the number of feature dimensions available to an estimator is uniform across the ensemble. Here, we introduce heterogeneous feature ensembling, with estimators built on varying number of feature dimensions, and consider its performance in a linear regression setting. We study an ensemble of linear predictors, each fit using ridge regression on a subset of the available features. We allow the number of features included in these subsets to vary. Using the replica trick from statistical physics, we derive learning curves for ridge ensembles with deterministic linear masks. We obtain explicit expressions for the learning curves in the case of equicorrelated data with an isotropic feature noise. Using the derived expressions, we investigate the effect of subsampling and ensembling, finding sharp transitions in the optimal ensembling strategy in the parameter space of noise level, data correlations, and data-task alignment. Finally, we suggest variable-dimension feature bagging as a strategy to mitigate double descent for robust machine learning in practice.
</details>
<details>
<summary>摘要</summary>
feature bagging 是一种已经广泛应用的 ensemble 方法，旨在降低预测变分的方法，通过在随机子样本或投影中训练 estimator  ensemble。通常，ensemble 被选择为Homogeneous，即每个 estimator 在 ensemble 中 disposal 的 feature 维度是固定的。在这文中，我们介绍了Heterogeneous feature ensembling，其中 estimator 建立在不同的 feature 维度上，并考虑其在线性回归设置下的性能。我们研究了一个 ensemble 的线性预测器，每个预测器使用ridge regression在一 subset 中的可用 feature 上进行训练。我们允许这些subset 中包含的 feature 的数量发生变化。使用统计物理中的replica trick，我们得到了ridge ensemble 的学习曲线，其中包括 equicorrelated 数据和各向异otropic 特征噪音。使用 derivations 中的表达，我们调查了 subsampling 和 ensembling 对 optimal 结果的影响，并发现了参数空间中的锐转点。最后，我们建议 variable-dimension feature bagging 作为一种 mitigate double descent 的实践策略。
</details></li>
</ul>
<hr>
<h2 id="Push-Past-Green-Learning-to-Look-Behind-Plant-Foliage-by-Moving-It"><a href="#Push-Past-Green-Learning-to-Look-Behind-Plant-Foliage-by-Moving-It" class="headerlink" title="Push Past Green: Learning to Look Behind Plant Foliage by Moving It"></a>Push Past Green: Learning to Look Behind Plant Foliage by Moving It</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03175">http://arxiv.org/abs/2307.03175</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoyu Zhang, Saurabh Gupta</li>
<li>for: 这个论文的目的是解决自动化农业应用（如检查、fenotiping、摘取水果）中对植物叶子和枝条的操作带来的挑战。</li>
<li>methods: 这篇论文使用数据驱动方法来解决这些挑战。它使用自我超级视觉网络SRPNet来预测执行一个候选动作后植物上的空间可见性。</li>
<li>results: 实验表明SRPNet在5个设定下对一种 sintetic (蔷薇) 和一种真实植物 ( Draceana) 的物理测试床上表现出色，超过了一种竞争性手工探索方法。 SRPNet也在对手工动力模型和相关减少中表现出色。<details>
<summary>Abstract</summary>
Autonomous agriculture applications (e.g., inspection, phenotyping, plucking fruits) require manipulating the plant foliage to look behind the leaves and the branches. Partial visibility, extreme clutter, thin structures, and unknown geometry and dynamics for plants make such manipulation challenging. We tackle these challenges through data-driven methods. We use self-supervision to train SRPNet, a neural network that predicts what space is revealed on execution of a candidate action on a given plant. We use SRPNet with the cross-entropy method to predict actions that are effective at revealing space beneath plant foliage. Furthermore, as SRPNet does not just predict how much space is revealed but also where it is revealed, we can execute a sequence of actions that incrementally reveal more and more space beneath the plant foliage. We experiment with a synthetic (vines) and a real plant (Dracaena) on a physical test-bed across 5 settings including 2 settings that test generalization to novel plant configurations. Our experiments reveal the effectiveness of our overall method, PPG, over a competitive hand-crafted exploration method, and the effectiveness of SRPNet over a hand-crafted dynamics model and relevant ablations.
</details>
<details>
<summary>摘要</summary>
自主农业应用（如检查、辐射类型、摘取水果）需要对植物叶子和枝干进行检查和操作。由于植物的部分可见性、极度堆积、细小结构和不确定的植物geometry和动力学，这种操作具有挑战性。我们通过数据驱动方法解决这些挑战。我们使用自我监督训练SRPNet，一种神经网络，该网络预测执行给定植物的候选动作后所可见的空间。我们使用SRPNet与十字积分方法预测有效的执行动作，以便逐渐暴露植物下方的空间。我们在Synthetic（蔷薇）和实际植物（Dracean）上进行了物理测试，并在5个设定中进行了测试，其中2个设定检验了植物配置的普适性。我们的实验表明我们的总方法PPG在比手动探索方法更有效，而SRPNet在手动动力模型和相关减少中表现更有效。
</details></li>
</ul>
<hr>
<h2 id="Wasserstein-Quantum-Monte-Carlo-A-Novel-Approach-for-Solving-the-Quantum-Many-Body-Schrodinger-Equation"><a href="#Wasserstein-Quantum-Monte-Carlo-A-Novel-Approach-for-Solving-the-Quantum-Many-Body-Schrodinger-Equation" class="headerlink" title="Wasserstein Quantum Monte Carlo: A Novel Approach for Solving the Quantum Many-Body Schrödinger Equation"></a>Wasserstein Quantum Monte Carlo: A Novel Approach for Solving the Quantum Many-Body Schrödinger Equation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07050">http://arxiv.org/abs/2307.07050</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/necludov/wqmc">https://github.com/necludov/wqmc</a></li>
<li>paper_authors: Kirill Neklyudov, Jannes Nys, Luca Thiede, Juan Carrasquilla, Qiang Liu, Max Welling, Alireza Makhzani</li>
<li>for:  solves the quantum many-body Schrödinger equation, a fundamental problem in quantum physics, chemistry, and materials science.</li>
<li>methods:  uses deep learning methods to represent wave functions as neural networks, and reformulates energy functional minimization in the space of Born distributions.</li>
<li>results:  demonstrates faster convergence to the ground state of molecular systems using the proposed “Wasserstein Quantum Monte Carlo” (WQMC) method.Here’s the full text in Simplified Chinese:</li>
<li>for:  solves the quantum many-body Schrödinger equation, a fundamental problem in quantum physics, chemistry, and materials science.</li>
<li>methods:  uses deep learning methods to represent wave functions as neural networks, and reformulates energy functional minimization in the space of Born distributions.</li>
<li>results:  demonstrates faster convergence to the ground state of molecular systems using the proposed “Wasserstein Quantum Monte Carlo” (WQMC) method.<details>
<summary>Abstract</summary>
Solving the quantum many-body Schr\"odinger equation is a fundamental and challenging problem in the fields of quantum physics, quantum chemistry, and material sciences. One of the common computational approaches to this problem is Quantum Variational Monte Carlo (QVMC), in which ground-state solutions are obtained by minimizing the energy of the system within a restricted family of parameterized wave functions. Deep learning methods partially address the limitations of traditional QVMC by representing a rich family of wave functions in terms of neural networks. However, the optimization objective in QVMC remains notoriously hard to minimize and requires second-order optimization methods such as natural gradient. In this paper, we first reformulate energy functional minimization in the space of Born distributions corresponding to particle-permutation (anti-)symmetric wave functions, rather than the space of wave functions. We then interpret QVMC as the Fisher-Rao gradient flow in this distributional space, followed by a projection step onto the variational manifold. This perspective provides us with a principled framework to derive new QMC algorithms, by endowing the distributional space with better metrics, and following the projected gradient flow induced by those metrics. More specifically, we propose "Wasserstein Quantum Monte Carlo" (WQMC), which uses the gradient flow induced by the Wasserstein metric, rather than Fisher-Rao metric, and corresponds to transporting the probability mass, rather than teleporting it. We demonstrate empirically that the dynamics of WQMC results in faster convergence to the ground state of molecular systems.
</details>
<details>
<summary>摘要</summary>
解决量子多体Шрёдингер方程是物理学、化学和材料科学领域的基本和挑战性问题。一种常见的计算方法是量子变量 Monte Carlo（QVMC），在这种方法中，系统的基态解是通过在限定的参数化波函数内寻找能量最小值来获得。深度学习方法可以部分解决传统QVMC中的限制，因为它可以表示一个富有的波函数家族使用神经网络。然而，QVMC中的优化目标仍然具有困难度，需要使用次序优化方法，如自然梯度。在这篇论文中，我们首先将能量函数最小化转换为 Born 分布对应的 particle-permutation（反）对称波函数的空间中进行，然后将 QVMC 解释为 Born 分布空间中的 Fisher-Rao 梯度流。接着，我们在这个分布空间中尝试新的 QMC 算法，通过给分布空间添加更好的 метри，并跟踪这些 метри 导引的投影流。更具体来说，我们提出了 "Wasserstein Quantum Monte Carlo"（WQMC），它使用梯度流导引的 Wasserstein  metric，而不是 Fisher-Rao  metric，并与teleporting 不同。我们通过实验证明，WQMC 的动力学会更快地 converges 到分子系统的基态解。
</details></li>
</ul>
<hr>
<h2 id="Focused-Transformer-Contrastive-Training-for-Context-Scaling"><a href="#Focused-Transformer-Contrastive-Training-for-Context-Scaling" class="headerlink" title="Focused Transformer: Contrastive Training for Context Scaling"></a>Focused Transformer: Contrastive Training for Context Scaling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03170">http://arxiv.org/abs/2307.03170</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cstankonrad/long_llama">https://github.com/cstankonrad/long_llama</a></li>
<li>paper_authors: Szymon Tworkowski, Konrad Staniszewski, Mikołaj Pacek, Yuhuai Wu, Henryk Michalewski, Piotr Miłoś</li>
<li>for: 提高大型语言模型在上下文长度方面的潜在能力</li>
<li>methods: 通过访问外部内存，让注意层访问更多的文档，并采用对比学习的训练方法解决焦点问题</li>
<li>results: 实现了在长上下文下进行精准的启发式学习，并且可以细化大型模型的上下文长度，提高模型的性能<details>
<summary>Abstract</summary>
Large language models have an exceptional capability to incorporate new information in a contextual manner. However, the full potential of such an approach is often restrained due to a limitation in the effective context length. One solution to this issue is to endow an attention layer with access to an external memory, which comprises of (key, value) pairs. Yet, as the number of documents increases, the proportion of relevant keys to irrelevant ones decreases, leading the model to focus more on the irrelevant keys. We identify a significant challenge, dubbed the distraction issue, where keys linked to different semantic values might overlap, making them hard to distinguish. To tackle this problem, we introduce the Focused Transformer (FoT), a technique that employs a training process inspired by contrastive learning. This novel approach enhances the structure of the (key, value) space, enabling an extension of the context length. Our method allows for fine-tuning pre-existing, large-scale models to lengthen their effective context. This is demonstrated by our fine-tuning of $3B$ and $7B$ OpenLLaMA checkpoints. The resulting models, which we name LongLLaMA, exhibit advancements in tasks requiring a long context. We further illustrate that our LongLLaMA models adeptly manage a $256 k$ context length for passkey retrieval.
</details>
<details>
<summary>摘要</summary>
大型语言模型具有Exceptional的能力 Contextual 地搜集新信息。然而，这种方法的潜力 Frequently 受限因为Context Length的限制。一种解决方案是赋予Attention层访问 External Memory，其包含（键、值）对。然而，随着文档数量的增加，相关键对应的权重比例逐渐减少，导致模型更多地关注无关键。我们描述了一个Significant Challenge，称之为distraction issue，其中键 Linked to Different Semantic Values 可能会 overlap，使其困难分辨。为解决这个问题，我们引入了Focused Transformer（FoT），一种基于对比学习的训练方法。这种新的approach 使（键、值）空间的结构更加稠密，使Context Length可以更长。我们的方法允许对Pre-existing, Large-scale模型进行细化，从而Lengthen its Effective Context。我们的 Fine-tuning  $3B$ 和 $7B$ OpenLLaMA Checkpoint 的结果，我们命名为LongLLaMA，在需要Long Context的任务中展现出了进步。我们还证明了我们的 LongLLaMA 模型可以efficaciously manage $256k$ Context Length for Passkey Retrieval。
</details></li>
</ul>
<hr>
<h2 id="Can-Domain-Adaptation-Improve-Accuracy-and-Fairness-of-Skin-Lesion-Classification"><a href="#Can-Domain-Adaptation-Improve-Accuracy-and-Fairness-of-Skin-Lesion-Classification" class="headerlink" title="Can Domain Adaptation Improve Accuracy and Fairness of Skin Lesion Classification?"></a>Can Domain Adaptation Improve Accuracy and Fairness of Skin Lesion Classification?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03157">http://arxiv.org/abs/2307.03157</a></li>
<li>repo_url: None</li>
<li>paper_authors: Janet Wang, Yunbei Zhang, Zhengming Ding, Jihun Hamm</li>
<li>for: 本研究旨在 investigate unsupervised domain adaptation (UDA) 方法在皮肤癌症分类 tasks 中的可行性，以提高精度和可靠性。</li>
<li>methods: 本研究使用了多个皮肤癌症数据集，并 investigate 不同的 UDA 训练方案，包括单源、合并源和多源。</li>
<li>results: 研究结果显示，UDA 在 binary 分类任务中效果显著，并且在减轻偏置问题时进一步提高了性能。在多类任务中，UDA 的表现较弱，需要处理偏置问题以达到上baseline的准确率。通过我们的量化分析，我们发现测试错误率与标签转移强相关，而特征级 UDA 方法在不平衡数据集上有限制。最后，我们的研究表明，UDA 可以有效地减少对少数群体的偏见，且不需要显式使用公平预处理技术。<details>
<summary>Abstract</summary>
Deep learning-based diagnostic system has demonstrated potential in classifying skin cancer conditions when labeled training example are abundant. However, skin lesion analysis often suffers from a scarcity of labeled data, hindering the development of an accurate and reliable diagnostic system. In this work, we leverage multiple skin lesion datasets and investigate the feasibility of various unsupervised domain adaptation (UDA) methods in binary and multi-class skin lesion classification. In particular, we assess three UDA training schemes: single-, combined-, and multi-source. Our experiment results show that UDA is effective in binary classification, with further improvement being observed when imbalance is mitigated. In multi-class task, its performance is less prominent, and imbalance problem again needs to be addressed to achieve above-baseline accuracy. Through our quantitative analysis, we find that the test error of multi-class tasks is strongly correlated with label shift, and feature-level UDA methods have limitations when handling imbalanced datasets. Finally, our study reveals that UDA can effectively reduce bias against minority groups and promote fairness, even without the explicit use of fairness-focused techniques.
</details>
<details>
<summary>摘要</summary>
深度学习基于的诊断系统在有 suficient 标注示例时已经表现出了抑分类皮肤癌的潜力。然而，皮肤肿瘤分析通常受到标注数据的不足的限制，这阻碍了建立准确可靠的诊断系统。在这个工作中，我们利用多个皮肤肿瘤数据集，并 investigate了不同的无监督领域适应（UDA）方法在binary和多类皮肤肿瘤分类中的可行性。特别是，我们评估了单源、合并源和多源的UDA训练方案。我们的实验结果表明，UDA在binary分类任务中是有效的，并且在减轻偏见时进一步提高了表现。在多类任务中，其表现较弱，需要解决偏见问题以达到上基线的准确率。我们的量化分析表明，测试错误的多类任务和标签转移之间存在强相关性，而feature层UDA方法在不均衡数据集上有限制。最后，我们的研究表明，UDA可以有效地减少对少数群体的偏见，无需显式使用关注公平性的技术。
</details></li>
</ul>
<hr>
<h2 id="Topology-Aware-Loss-for-Aorta-and-Great-Vessel-Segmentation-in-Computed-Tomography-Images"><a href="#Topology-Aware-Loss-for-Aorta-and-Great-Vessel-Segmentation-in-Computed-Tomography-Images" class="headerlink" title="Topology-Aware Loss for Aorta and Great Vessel Segmentation in Computed Tomography Images"></a>Topology-Aware Loss for Aorta and Great Vessel Segmentation in Computed Tomography Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03137">http://arxiv.org/abs/2307.03137</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seher Ozcelik, Sinan Unver, Ilke Ali Gurses, Rustu Turkay, Cigdem Gunduz-Demir</li>
<li>for: 提高图像分割 tasks 中的性能，特别是在人体 анатоMY 中 vessels 的分割任务上。</li>
<li>methods: 提出了一种新的 topology-aware 损失函数，通过 persistent homology 来衡量网络预测和真实值之间的拓扑不同。</li>
<li>results: 对于 4327 个 CT 图像和 24 个主体的实验表明，提出的损失函数可以更好地提高图像分割的性能， indicating the effectiveness of this approach.<details>
<summary>Abstract</summary>
Segmentation networks are not explicitly imposed to learn global invariants of an image, such as the shape of an object and the geometry between multiple objects, when they are trained with a standard loss function. On the other hand, incorporating such invariants into network training may help improve performance for various segmentation tasks when they are the intrinsic characteristics of the objects to be segmented. One example is segmentation of aorta and great vessels in computed tomography (CT) images where vessels are found in a particular geometry in the body due to the human anatomy and they mostly seem as round objects on a 2D CT image. This paper addresses this issue by introducing a new topology-aware loss function that penalizes topology dissimilarities between the ground truth and prediction through persistent homology. Different from the previously suggested segmentation network designs, which apply the threshold filtration on a likelihood function of the prediction map and the Betti numbers of the ground truth, this paper proposes to apply the Vietoris-Rips filtration to obtain persistence diagrams of both ground truth and prediction maps and calculate the dissimilarity with the Wasserstein distance between the corresponding persistence diagrams. The use of this filtration has advantage of modeling shape and geometry at the same time, which may not happen when the threshold filtration is applied. Our experiments on 4327 CT images of 24 subjects reveal that the proposed topology-aware loss function leads to better results than its counterparts, indicating the effectiveness of this use.
</details>
<details>
<summary>摘要</summary>
对于批处理图像中的分割任务，传统的损失函数不会直接学习图像中的全局不变量，如物体形状和多个物体之间的几何关系。然而，在某些任务中，这些不变量是物体的内在特征，通过将它们包含在网络训练中可能会提高分割性能。例如，计算机 Tomatoes（CT）图像中的血管和大血管分割任务中，血管在人体 анаatomy 中的特定几何位置，通常在2D CT 图像上看到为圆形物体。本文通过引入一种新的 topology-aware 损失函数来解决这个问题，该损失函数通过 persist homology  penalty  topology 不同性 zwischen 真实值和预测值。与之前的 segmentation 网络设计不同，这里不是通过阈值滤波器应用 likelihood 函数和 Betti 数来实现，而是通过 Vietoris-Rips 滤波器来获得预测和真实值的 persistence 图，并计算它们之间的 Wasserstein 距离。这种方法的优点在于同时模型形状和几何，可能不会在使用阈值滤波器时发生。我们在 4327 个 CT 图像上进行了 24 个人的实验，发现提案的 topology-aware 损失函数可以更好地处理这些任务，表明其效果。
</details></li>
</ul>
<hr>
<h2 id="Multiplicative-Updates-for-Online-Convex-Optimization-over-Symmetric-Cones"><a href="#Multiplicative-Updates-for-Online-Convex-Optimization-over-Symmetric-Cones" class="headerlink" title="Multiplicative Updates for Online Convex Optimization over Symmetric Cones"></a>Multiplicative Updates for Online Convex Optimization over Symmetric Cones</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03136">http://arxiv.org/abs/2307.03136</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/waynelin74/OCO_SymmetricCones">https://github.com/waynelin74/OCO_SymmetricCones</a></li>
<li>paper_authors: Ilayda Canyakmaz, Wayne Lin, Georgios Piliouras, Antonios Varvitsiotis</li>
<li>For: 该 paper 研究在线凸优化中，可能的动作是 trace-one 元素在 симметричный cone 中的扩展，涵盖了广泛研究的专家设置和其量子对应体。* Methods: 该 paper 使用了 Euclidean Jordan Algebras 的工具，提出了无投影的 Symmetric-Cone Multiplicative Weights Update (SCMWU) 算法，用于在 trace-one slice 上进行在线优化。* Results: 该 paper 证明了 SCMWU 算法是一个无误算法，并且扩展了 Multiplicative Weights Update 方法的分析，包括probability simplex 和 density matrices 的扩展。<details>
<summary>Abstract</summary>
We study online convex optimization where the possible actions are trace-one elements in a symmetric cone, generalizing the extensively-studied experts setup and its quantum counterpart. Symmetric cones provide a unifying framework for some of the most important optimization models, including linear, second-order cone, and semidefinite optimization. Using tools from the field of Euclidean Jordan Algebras, we introduce the Symmetric-Cone Multiplicative Weights Update (SCMWU), a projection-free algorithm for online optimization over the trace-one slice of an arbitrary symmetric cone. We show that SCMWU is equivalent to Follow-the-Regularized-Leader and Online Mirror Descent with symmetric-cone negative entropy as regularizer. Using this structural result we show that SCMWU is a no-regret algorithm, and verify our theoretical results with extensive experiments. Our results unify and generalize the analysis for the Multiplicative Weights Update method over the probability simplex and the Matrix Multiplicative Weights Update method over the set of density matrices.
</details>
<details>
<summary>摘要</summary>
我们研究在线凸优化问题，其可能的动作是 traces-one 元素在一个对称体中，泛化了广泛研究的专家设定和其量子对应器。对称体提供一个统一的框架，包括线性、第二阶凸优化和半definite 优化问题。使用 Euclid  Jordan 代数的工具，我们引入了 trace-one slice 的Symmetric-Cone 多重量更新（SCMWU）算法，不需要投影。我们证明 SCMWU 等价于 Follow-the-Regularized-Leader 和 Online Mirror Descent 的对称体负 entropy 作为规则。使用这种结构结果，我们证明 SCMWU 是一个不会追攻的算法，并通过广泛的实验来验证我们的理论结果。我们的结果将 Multiplicative Weights Update 方法在概率 Simplex 和 Matrix Multiplicative Weights Update 方法在密度矩阵上的分析统一和推广。
</details></li>
</ul>
<hr>
<h2 id="Distilling-Large-Vision-Language-Model-with-Out-of-Distribution-Generalizability"><a href="#Distilling-Large-Vision-Language-Model-with-Out-of-Distribution-Generalizability" class="headerlink" title="Distilling Large Vision-Language Model with Out-of-Distribution Generalizability"></a>Distilling Large Vision-Language Model with Out-of-Distribution Generalizability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03135">http://arxiv.org/abs/2307.03135</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xuanlinli17/large_vlm_distillation_ood">https://github.com/xuanlinli17/large_vlm_distillation_ood</a></li>
<li>paper_authors: Xuanlin Li, Yunhao Fang, Minghua Liu, Zhan Ling, Zhuowen Tu, Hao Su</li>
<li>For:	+ The paper aims to investigate the distillation of visual representations in large teacher vision-language models into lightweight student models, with a focus on open-vocabulary out-of-distribution (OOD) generalization.* Methods:	+ The proposed method uses two principles from vision and language modality perspectives to enhance student’s OOD generalization: (1) by better imitating teacher’s visual representation space, and carefully promoting better coherence in vision-language alignment with the teacher; (2) by enriching the teacher’s language representations with informative and finegrained semantic attributes to effectively distinguish between different labels.* Results:	+ The results demonstrate significant improvements in zero-shot and few-shot student performance on open-vocabulary out-of-distribution classification, highlighting the effectiveness of the proposed approaches.<details>
<summary>Abstract</summary>
Large vision-language models have achieved outstanding performance, but their size and computational requirements make their deployment on resource-constrained devices and time-sensitive tasks impractical. Model distillation, the process of creating smaller, faster models that maintain the performance of larger models, is a promising direction towards the solution. This paper investigates the distillation of visual representations in large teacher vision-language models into lightweight student models using a small- or mid-scale dataset. Notably, this study focuses on open-vocabulary out-of-distribution (OOD) generalization, a challenging problem that has been overlooked in previous model distillation literature. We propose two principles from vision and language modality perspectives to enhance student's OOD generalization: (1) by better imitating teacher's visual representation space, and carefully promoting better coherence in vision-language alignment with the teacher; (2) by enriching the teacher's language representations with informative and finegrained semantic attributes to effectively distinguish between different labels. We propose several metrics and conduct extensive experiments to investigate their techniques. The results demonstrate significant improvements in zero-shot and few-shot student performance on open-vocabulary out-of-distribution classification, highlighting the effectiveness of our proposed approaches. Code released at https://github.com/xuanlinli17/large_vlm_distillation_ood
</details>
<details>
<summary>摘要</summary>
大型视言语模型已经实现了出色的表现，但它们的大小和计算需求使其在有限的设备和时间上不可靠性不允许其部署。模型缩小，将大型模型转换成更小的更快的模型，以保持大型模型的表现，是一个有前途的方向。这篇论文 investigate teacher视言语模型中的视 representations的缩小，使用小规模或中规模的 dataset。特别是，这种研究强调了无法表示（OOD）泛化问题，在前一个model distillation文献中受到了忽略。我们提出了两个原则，从视觉和语言模式的角度来提高学生的OOD泛化表现：（1）更好地模仿教师的视觉表示空间，并且细致地协调视语对应关系;（2）使用有用和细致的语言特征来有效地分类不同的标签。我们提出了一些指标，并进行了广泛的实验来调查它们的技术。结果表明，我们的提出的方法在零shot和几shot学生表现中具有显著的改进，强调了我们的提出的方法的效iveness。代码可以在https://github.com/xuanlinli17/large_vlm_distillation_ood中下载。
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-Test-Time-Adaptation-against-Distribution-Shifts-in-Image-Classification"><a href="#Benchmarking-Test-Time-Adaptation-against-Distribution-Shifts-in-Image-Classification" class="headerlink" title="Benchmarking Test-Time Adaptation against Distribution Shifts in Image Classification"></a>Benchmarking Test-Time Adaptation against Distribution Shifts in Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03133">http://arxiv.org/abs/2307.03133</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuyongcan/benchmark-tta">https://github.com/yuyongcan/benchmark-tta</a></li>
<li>paper_authors: Yongcan Yu, Lijun Sheng, Ran He, Jian Liang</li>
<li>for:  This paper aims to provide a benchmark for test-time adaptation (TTA) methods to enhance the generalization performance of models and improve their robustness against distribution shifts.</li>
<li>methods: The paper evaluates 13 prominent TTA methods and their variants on five widely used image classification datasets, including CIFAR-10-C, CIFAR-100-C, ImageNet-C, DomainNet, and Office-Home. These methods cover a range of adaptation scenarios, such as online adaptation vs. offline adaptation, instance adaptation vs. batch adaptation vs. domain adaptation.</li>
<li>results: The paper presents a unified framework in PyTorch to evaluate and compare the effectiveness of TTA methods across different datasets and network architectures. By establishing this benchmark, the authors aim to provide researchers and practitioners with a reliable means of assessing and comparing the effectiveness of TTA methods in improving model robustness and generalization performance.<details>
<summary>Abstract</summary>
Test-time adaptation (TTA) is a technique aimed at enhancing the generalization performance of models by leveraging unlabeled samples solely during prediction. Given the need for robustness in neural network systems when faced with distribution shifts, numerous TTA methods have recently been proposed. However, evaluating these methods is often done under different settings, such as varying distribution shifts, backbones, and designing scenarios, leading to a lack of consistent and fair benchmarks to validate their effectiveness. To address this issue, we present a benchmark that systematically evaluates 13 prominent TTA methods and their variants on five widely used image classification datasets: CIFAR-10-C, CIFAR-100-C, ImageNet-C, DomainNet, and Office-Home. These methods encompass a wide range of adaptation scenarios (e.g. online adaptation v.s. offline adaptation, instance adaptation v.s. batch adaptation v.s. domain adaptation). Furthermore, we explore the compatibility of different TTA methods with diverse network backbones. To implement this benchmark, we have developed a unified framework in PyTorch, which allows for consistent evaluation and comparison of the TTA methods across the different datasets and network architectures. By establishing this benchmark, we aim to provide researchers and practitioners with a reliable means of assessing and comparing the effectiveness of TTA methods in improving model robustness and generalization performance. Our code is available at https://github.com/yuyongcan/Benchmark-TTA.
</details>
<details>
<summary>摘要</summary>
Test-time adaptation (TTA) 是一种技术，目的是通过在预测时使用无标示样本来提高模型的总体性能。由于神经网络系统面临到分布shift时的稳定性问题，现在有许多TTA方法被提出。然而，评估这些方法的效果通常是在不同的设置下进行，例如不同的分布shift、背景和设计方案，导致了评估这些方法的标准化和公平的标准准比不够。为解决这个问题，我们提出了一个benchmark，可以系统地评估13种知名的TTA方法和其变种在五种常用的图像分类 datasets上：CIFAR-10-C、CIFAR-100-C、ImageNet-C、DomainNet和Office-Home。这些方法涵盖了各种适应enario（例如在线适应与离线适应、实例适应与批适应、领域适应）。此外，我们还探索了不同的TTA方法与不同的网络背景的兼容性。为实现这个benchmark，我们在PyTorch上开发了一套统一的框架，可以在不同的 datasets和网络架构上进行一致的评估和比较TTA方法的效果。通过设立这个benchmark，我们希望为研究者和实践者提供一个可靠的方式来评估和比较TTA方法在提高模型的Robustness和总体性能方面的效果。我们的代码可以在https://github.com/yuyongcan/Benchmark-TTA上获取。
</details></li>
</ul>
<hr>
<h2 id="T-MARS-Improving-Visual-Representations-by-Circumventing-Text-Feature-Learning"><a href="#T-MARS-Improving-Visual-Representations-by-Circumventing-Text-Feature-Learning" class="headerlink" title="T-MARS: Improving Visual Representations by Circumventing Text Feature Learning"></a>T-MARS: Improving Visual Representations by Circumventing Text Feature Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03132">http://arxiv.org/abs/2307.03132</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/locuslab/t-mars">https://github.com/locuslab/t-mars</a></li>
<li>paper_authors: Pratyush Maini, Sachin Goyal, Zachary C. Lipton, J. Zico Kolter, Aditi Raghunathan</li>
<li>for: 提高计算机视觉领域的状态提取大量网络源数据的优化，以提高零或几shot认识的性能。</li>
<li>methods: 提出一种新的数据筛选方法，基于我们发现LAION dataset中40%的图片含有与标签重叠的文本，这些数据可能会使模型做到光学字符识别而不是学习视觉特征。我们的方法使用文本屏蔽和CLIP相似度分数来筛选图片，从而过滤出只含有重叠文本的图片。</li>
<li>results: 在DataComp数据筛选benchmark中，T-MARS方法在”中等规模”下的性能比顶尖方法提高6.5%在ImageNet和4.7%在VTAB。此外，我们在不同的数据池大小从2M到64M进行系统性的评估，发现T-MARS方法的准确率提升与数据和计算的扩展幂。代码可以在<a target="_blank" rel="noopener" href="https://github.com/locuslab/T-MARS%E4%B8%AD%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/locuslab/T-MARS中下载。</a><details>
<summary>Abstract</summary>
Large web-sourced multimodal datasets have powered a slew of new methods for learning general-purpose visual representations, advancing the state of the art in computer vision and revolutionizing zero- and few-shot recognition. One crucial decision facing practitioners is how, if at all, to curate these ever-larger datasets. For example, the creators of the LAION-5B dataset chose to retain only image-caption pairs whose CLIP similarity score exceeded a designated threshold. In this paper, we propose a new state-of-the-art data filtering approach motivated by our observation that nearly 40% of LAION's images contain text that overlaps significantly with the caption. Intuitively, such data could be wasteful as it incentivizes models to perform optical character recognition rather than learning visual features. However, naively removing all such data could also be wasteful, as it throws away images that contain visual features (in addition to overlapping text). Our simple and scalable approach, T-MARS (Text Masking and Re-Scoring), filters out only those pairs where the text dominates the remaining visual features -- by first masking out the text and then filtering out those with a low CLIP similarity score of the masked image. Experimentally, T-MARS outperforms the top-ranked method on the "medium scale" of DataComp (a data filtering benchmark) by a margin of 6.5% on ImageNet and 4.7% on VTAB. Additionally, our systematic evaluation on various data pool sizes from 2M to 64M shows that the accuracy gains enjoyed by T-MARS linearly increase as data and compute are scaled exponentially. Code is available at https://github.com/locuslab/T-MARS.
</details>
<details>
<summary>摘要</summary>
大量网络收集的多模式数据已经推动了新的计算机视觉表示学习方法的发展，对计算机视觉领域进行了革命性的改进。实际操作者面临的一个关键决策是如何处理这些日益增大的数据集。例如，LAION-5B数据集的创建者选择了只保留图像和描述对应的CLIP相似度score超过设置的阈值。在这篇论文中，我们提出了一种新的数据筛选方法，它是基于我们观察到LAION中约40%的图像含有与描述重叠的文本的观察。这些数据可能是浪费的，因为它们可能会让模型做Optical Character Recognition而不是学习视觉特征。然而， Naively removing all such data could also be wasteful, as it would throw away images that contain visual features (in addition to overlapping text).我们的简单和可扩展方法T-MARS（文本覆盖和重新分配）会过滤掉那些文本占据图像的大部分视觉特征的对应。我们首先对图像中的文本进行覆盖，然后对CLIP相似度score的覆盖图像进行过滤。实验表明，T-MARS在DataComp中的"中等规模"上比顶尖方法表现出6.5%的提升，在ImageNet和VTAB上分别提升4.7%和6.5%。此外，我们对不同的数据池大小从2M到64M进行系统性的评估，发现T-MARS的准确率提升 linearly 随着数据和计算的扩展幂数。代码可以在https://github.com/locuslab/T-MARS上获取。
</details></li>
</ul>
<hr>
<h2 id="Principal-subbundles-for-dimension-reduction"><a href="#Principal-subbundles-for-dimension-reduction" class="headerlink" title="Principal subbundles for dimension reduction"></a>Principal subbundles for dimension reduction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03128">http://arxiv.org/abs/2307.03128</a></li>
<li>repo_url: None</li>
<li>paper_authors: Morten Akhøj, James Benn, Erlend Grong, Stefan Sommer, Xavier Pennec</li>
<li>for: 该 paper 用 sub-Riemannian geometry 进行拟合 manifold 和 surface reconstruction，并将本地线性近似转换为 lower dimensional bundle。</li>
<li>methods: 使用 local PCA 获取 local approximations，并将其集成到 rank $k$ tangent subbundle 中，$k&lt;d$。这个 sub-Riemannian metric 可以应用于一些重要的问题，如：建立 approximating submanifold $M$，构建点云在 $\mathbb{R}^k$ 中的表示，计算 observations 之间的距离，并考虑 learned geometry。</li>
<li>results: 通过 simulations 表明，该框架在噪声数据上是稳定的，并且可以扩展到知道 Riemannian manifold 的情况。<details>
<summary>Abstract</summary>
In this paper we demonstrate how sub-Riemannian geometry can be used for manifold learning and surface reconstruction by combining local linear approximations of a point cloud to obtain lower dimensional bundles. Local approximations obtained by local PCAs are collected into a rank $k$ tangent subbundle on $\mathbb{R}^d$, $k<d$, which we call a principal subbundle. This determines a sub-Riemannian metric on $\mathbb{R}^d$. We show that sub-Riemannian geodesics with respect to this metric can successfully be applied to a number of important problems, such as: explicit construction of an approximating submanifold $M$, construction of a representation of the point-cloud in $\mathbb{R}^k$, and computation of distances between observations, taking the learned geometry into account. The reconstruction is guaranteed to equal the true submanifold in the limit case where tangent spaces are estimated exactly. Via simulations, we show that the framework is robust when applied to noisy data. Furthermore, the framework generalizes to observations on an a priori known Riemannian manifold.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们示例了如何使用非里曼几何来进行拟合 manifold 和表面重建，通过将本地线性近似组合到一个更低维度的束上。本地近似由本地 PCA 获得，并将其集成为一个 rank $k$ 的 tangent 束在 $\mathbb{R}^d$ 上，其中 $k<d$。我们称之为主Bundle。这确定了一个 sub-Riemannian 度量在 $\mathbb{R}^d$ 上。我们示示了 sub-Riemannian 径在这个度量下可以成功应用于一些重要问题，如：生成一个近似 submanifold $M$，将点云表示在 $\mathbb{R}^k$ 上，以及根据学习的几何来计算观察之间的距离。重建是在 tangent 空间被估计为 preciselly 时保证等于真实的 submanifold。通过仿真，我们示示了该框架在噪声数据上是稳定的。此外，该框架可推广到已知 Riemannian 拟合 manifold 上的观察。
</details></li>
</ul>
<hr>
<h2 id="Context-Aware-Configuration-and-Management-of-WiFi-Direct-Groups-for-Real-Opportunistic-Networks"><a href="#Context-Aware-Configuration-and-Management-of-WiFi-Direct-Groups-for-Real-Opportunistic-Networks" class="headerlink" title="Context-Aware Configuration and Management of WiFi Direct Groups for Real Opportunistic Networks"></a>Context-Aware Configuration and Management of WiFi Direct Groups for Real Opportunistic Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03126">http://arxiv.org/abs/2307.03126</a></li>
<li>repo_url: None</li>
<li>paper_authors: Valerio Arnaboldi, Mattia Giovanni Campana, Franca Delmastro</li>
<li>for: 该研究旨在提高 Wi-Fi Direct 技术在商业移动设备上的支持，以便实现基于设备间通信（D2D）的网络解决方案。</li>
<li>methods: 该研究提议一种新的中间层协议（WiFi Direct Group Manager，WFD-GM），以便自动配置和管理 Wi-Fi Direct 组。该协议包括一个 контекст函数，该函数考虑不同参数来创建最佳组配置，包括节点稳定性和功率水平。</li>
<li>results: 研究结果显示，WFD-GM 在不同的 mobilty 模型、地理区域和节点数量的三种参考enario中表现出色，与基准方法相比，在中等&#x2F;低 mobilty 情况下表现更好，在高 mobilty 情况下与基准方法相当，无论添加额外开销。<details>
<summary>Abstract</summary>
Wi-Fi Direct is a promising technology for the support of device-to-device communications (D2D) on commercial mobile devices. However, the standard as-it-is is not sufficient to support the real deployment of networking solutions entirely based on D2D such as opportunistic networks. In fact, WiFi Direct presents some characteristics that could limit the autonomous creation of D2D connections among users' personal devices. Specifically, the standard explicitly requires the user's authorization to establish a connection between two or more devices, and it provides a limited support for inter-group communication. In some cases, this might lead to the creation of isolated groups of nodes which cannot communicate among each other. In this paper, we propose a novel middleware-layer protocol for the efficient configuration and management of WiFi Direct groups (WiFi Direct Group Manager, WFD-GM) to enable autonomous connections and inter-group communication. This enables opportunistic networks in real conditions (e.g., variable mobility and network size). WFD-GM defines a context function that takes into account heterogeneous parameters for the creation of the best group configuration in a specific time window, including an index of nodes' stability and power levels. We evaluate the protocol performances by simulating three reference scenarios including different mobility models, geographical areas and number of nodes. Simulations are also supported by experimental results related to the evaluation in a real testbed of the involved context parameters. We compare WFD-GM with the state-of-the-art solutions and we show that it performs significantly better than a Baseline approach in scenarios with medium/low mobility, and it is comparable with it in case of high mobility, without introducing additional overhead.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Learning-Multi-Agent-Intention-Aware-Communication-for-Optimal-Multi-Order-Execution-in-Finance"><a href="#Learning-Multi-Agent-Intention-Aware-Communication-for-Optimal-Multi-Order-Execution-in-Finance" class="headerlink" title="Learning Multi-Agent Intention-Aware Communication for Optimal Multi-Order Execution in Finance"></a>Learning Multi-Agent Intention-Aware Communication for Optimal Multi-Order Execution in Finance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03119">http://arxiv.org/abs/2307.03119</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuchen Fang, Zhenggang Tang, Kan Ren, Weiqing Liu, Li Zhao, Jiang Bian, Dongsheng Li, Weinan Zhang, Yong Yu, Tie-Yan Liu</li>
<li>for: 这 paper 的目的是解决多个订单同时执行的问题，使用模型自由学习（RL）技术。</li>
<li>methods: 这 paper 使用多代理RL（MARL）方法，每个代理都是一个特定的订单执行者，与其他代理交换信息以协同 Maximize 总收益。</li>
<li>results: 实验结果表明，使用提议的多round通信协议和行动值归因方法可以提高协同效果，并且在两个真实市场上达到了显著更好的性能。<details>
<summary>Abstract</summary>
Order execution is a fundamental task in quantitative finance, aiming at finishing acquisition or liquidation for a number of trading orders of the specific assets. Recent advance in model-free reinforcement learning (RL) provides a data-driven solution to the order execution problem. However, the existing works always optimize execution for an individual order, overlooking the practice that multiple orders are specified to execute simultaneously, resulting in suboptimality and bias. In this paper, we first present a multi-agent RL (MARL) method for multi-order execution considering practical constraints. Specifically, we treat every agent as an individual operator to trade one specific order, while keeping communicating with each other and collaborating for maximizing the overall profits. Nevertheless, the existing MARL algorithms often incorporate communication among agents by exchanging only the information of their partial observations, which is inefficient in complicated financial market. To improve collaboration, we then propose a learnable multi-round communication protocol, for the agents communicating the intended actions with each other and refining accordingly. It is optimized through a novel action value attribution method which is provably consistent with the original learning objective yet more efficient. The experiments on the data from two real-world markets have illustrated superior performance with significantly better collaboration effectiveness achieved by our method.
</details>
<details>
<summary>摘要</summary>
文本翻译为简化中文：订单执行是金融计算机科学中的基本任务，旨在完成购买或卖出一些资产的交易订单。现代无模型学习（RL）技术提供了基于数据的订单执行解决方案。然而，现有的工作都是优化每个订单的执行，忽略了实际情况下多个订单同时执行的情况，导致不优化和偏见。在本文中，我们首先提出了多代理RL（MARL）方法，用于多订单执行，考虑实际约束。 Specifically，我们将每个代理视为一个特定订单的交易者，保持与彼此交流，以 maximize 总收益。然而，现有的 MARL 算法通常通过互相交换部分观察信息来进行交流，这在金融市场中是不fficient的。为了改善协作，我们则提出了学习型多轮通信协议，让代理通过交换意图动作来交流，并根据此进行修正。这种协议由一种新的动作值归属方法优化，该方法与原始学习目标一致， yet more efficient。实验结果表明，我们的方法在两个真实的市场数据上表现出色，与传统方法相比，具有显著更好的协作效果。
</details></li>
</ul>
<hr>
<h2 id="Steel-Surface-Roughness-Parameter-Calculations-Using-Lasers-and-Machine-Learning-Models"><a href="#Steel-Surface-Roughness-Parameter-Calculations-Using-Lasers-and-Machine-Learning-Models" class="headerlink" title="Steel Surface Roughness Parameter Calculations Using Lasers and Machine Learning Models"></a>Steel Surface Roughness Parameter Calculations Using Lasers and Machine Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03723">http://arxiv.org/abs/2307.03723</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alex Milne, Xianghua Xie</li>
<li>for: 这个论文主要是为了提高热压板钢制造过程中表面质量的控制。</li>
<li>methods: 这篇论文使用了现代机器学习模型来提高在生产过程中的在线测量结果的准确性，以提高表面质量控制。</li>
<li>results: 研究表明，使用数据驱动的方法可以提高表面质量控制，并且可以实现在生产过程中的实时调整。<details>
<summary>Abstract</summary>
Control of surface texture in strip steel is essential to meet customer requirements during galvanizing and temper rolling processes. Traditional methods rely on post-production stylus measurements, while on-line techniques offer non-contact and real-time measurements of the entire strip. However, ensuring accurate measurement is imperative for their effective utilization in the manufacturing pipeline. Moreover, accurate on-line measurements enable real-time adjustments of manufacturing processing parameters during production, ensuring consistent quality and the possibility of closed-loop control of the temper mill. In this study, we leverage state-of-the-art machine learning models to enhance the transformation of on-line measurements into significantly a more accurate Ra surface roughness metric. By comparing a selection of data-driven approaches, including both deep learning and non-deep learning methods, to the close-form transformation, we evaluate their potential for improving surface texture control in temper strip steel manufacturing.
</details>
<details>
<summary>摘要</summary>
控制表面质量在带钢制造中是非常重要，以满足锈钢和氧化钢的需求。传统方法依靠后期探针测量，而在线技术可以实现不接触的实时测量整个带。然而，确保准确测量是关键，以便在生产过程中实现实时调整制造过程参数，保证产品质量的一致性和闭环控制温钢厂。本研究利用当前最佳的机器学习模型，以提高在线测量转换为更准确的Ra表面粗糙度指标。通过比较数据驱动方法，包括深度学习和非深度学习方法，与关系式转换，我们评估其在表面 тексту라控制方面的潜在提高。
</details></li>
</ul>
<hr>
<h2 id="Quantum-Solutions-to-the-Privacy-vs-Utility-Tradeoff"><a href="#Quantum-Solutions-to-the-Privacy-vs-Utility-Tradeoff" class="headerlink" title="Quantum Solutions to the Privacy vs. Utility Tradeoff"></a>Quantum Solutions to the Privacy vs. Utility Tradeoff</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03118">http://arxiv.org/abs/2307.03118</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sagnik Chatterjee, Vyacheslav Kungurtsev</li>
<li>for: 保障生成模型中的数据隐私和安全性</li>
<li>methods: 使用量子密码学 primitives 和可证明的隐私和安全性保证</li>
<li>results: 提供了一种基于量子密码学 primitives 的新架构，可以在任何现有的类型或量子生成模型之上使用，并且具有具有很高的安全性和隐私性保证。<details>
<summary>Abstract</summary>
In this work, we propose a novel architecture (and several variants thereof) based on quantum cryptographic primitives with provable privacy and security guarantees regarding membership inference attacks on generative models. Our architecture can be used on top of any existing classical or quantum generative models. We argue that the use of quantum gates associated with unitary operators provides inherent advantages compared to standard Differential Privacy based techniques for establishing guaranteed security from all polynomial-time adversaries.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们提出了一种新的架构（以及其变体），基于量子密码学 primitives，具有可证明的隐私和安全保证，对于生成模型的会员推测攻击。我们的架构可以在现有的类别或量子生成模型之上使用。我们认为，使用量子门相关的单位操作器提供了内置的优势，比标准推Diff Privacy基本技术更能提供来自所有多项时间敌对者的保证的安全性。
</details></li>
</ul>
<hr>
<h2 id="Region-Wise-Attentive-Multi-View-Representation-Learning-for-Urban-Region-Embeddings"><a href="#Region-Wise-Attentive-Multi-View-Representation-Learning-for-Urban-Region-Embeddings" class="headerlink" title="Region-Wise Attentive Multi-View Representation Learning for Urban Region Embeddings"></a>Region-Wise Attentive Multi-View Representation Learning for Urban Region Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03212">http://arxiv.org/abs/2307.03212</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weiliang Chan, Qianqian Ren</li>
<li>for: 本研究旨在提出一种 Region-Wise Multi-View Representation Learning (ROMER) 模型，用于捕捉多视图关系并学习表达式的城市区域表示。</li>
<li>methods: 该模型首先捕捉多视图相关性从移动流 Patterns、POI semantics 和 Check-in dynamics 中，然后采用全球图注意力网络学习任意两个顶点之间的相似性。在这之后，我们提出了一种两个阶段融合模块，以全面考虑多视图特征并共享特征。</li>
<li>results: 对实际世界数据集进行了两个下游任务的实验，结果显示，我们的模型在比较 estado-of-the-art 方法时，提高了17%的性能。<details>
<summary>Abstract</summary>
Urban region embedding is an important and yet highly challenging issue due to the complexity and constantly changing nature of urban data. To address the challenges, we propose a Region-Wise Multi-View Representation Learning (ROMER) to capture multi-view dependencies and learn expressive representations of urban regions without the constraints of rigid neighbourhood region conditions. Our model focus on learn urban region representation from multi-source urban data. First, we capture the multi-view correlations from mobility flow patterns, POI semantics and check-in dynamics. Then, we adopt global graph attention networks to learn similarity of any two vertices in graphs. To comprehensively consider and share features of multiple views, a two-stage fusion module is further proposed to learn weights with external attention to fuse multi-view embeddings. Extensive experiments for two downstream tasks on real-world datasets demonstrate that our model outperforms state-of-the-art methods by up to 17\% improvement.
</details>
<details>
<summary>摘要</summary>
城市区域嵌入是一项重要但又具有极高挑战性的问题，主要因为城市数据的复杂性和不断变化。为 Addressing these challenges, we propose a Region-Wise Multi-View Representation Learning (ROMER) method to capture multi-view dependencies and learn expressive representations of urban regions without the constraints of rigid neighborhood region conditions. Our model focuses on learning urban region representation from multi-source urban data. First, we capture the multi-view correlations from mobility flow patterns, POI semantics, and check-in dynamics. Then, we adopt global graph attention networks to learn the similarity of any two vertices in graphs. To comprehensively consider and share features of multiple views, a two-stage fusion module is further proposed to learn weights with external attention to fuse multi-view embeddings. Extensive experiments for two downstream tasks on real-world datasets demonstrate that our model outperforms state-of-the-art methods by up to 17\% improvement.Here's the text with some additional information about the Simplified Chinese translation:The translation is in Simplified Chinese, which is the standard written form of Chinese used in mainland China and Singapore. The text is written in a formal and academic style, using technical terms and phrases commonly used in the field of computer science and machine learning. The translation aims to convey the same meaning and information as the original English text, while also taking into account the grammatical and syntactical conventions of Simplified Chinese.Please note that the translation is provided for reference only, and may not be perfect or entirely accurate. If you have any specific questions or requests for clarification, please feel free to ask.
</details></li>
</ul>
<hr>
<h2 id="How-to-Detect-Unauthorized-Data-Usages-in-Text-to-image-Diffusion-Models"><a href="#How-to-Detect-Unauthorized-Data-Usages-in-Text-to-image-Diffusion-Models" class="headerlink" title="How to Detect Unauthorized Data Usages in Text-to-image Diffusion Models"></a>How to Detect Unauthorized Data Usages in Text-to-image Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03108">http://arxiv.org/abs/2307.03108</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenting Wang, Chen Chen, Yuchen Liu, Lingjuan Lyu, Dimitris Metaxas, Shiqing Ma</li>
<li>for: 防止文本到图像扩散模型的非法数据使用</li>
<li>methods: 植入插入记忆法，通过分析模型是否记忆插入内容来检测非法数据使用</li>
<li>results: 实验表明，提议的方法可以准确检测文本到图像扩散模型中的非法数据使用<details>
<summary>Abstract</summary>
Recent text-to-image diffusion models have shown surprising performance in generating high-quality images. However, concerns have arisen regarding the unauthorized usage of data during the training process. One example is when a model trainer collects a set of images created by a particular artist and attempts to train a model capable of generating similar images without obtaining permission from the artist. To address this issue, it becomes crucial to detect unauthorized data usage. In this paper, we propose a method for detecting such unauthorized data usage by planting injected memorization into the text-to-image diffusion models trained on the protected dataset. Specifically, we modify the protected image dataset by adding unique contents on the images such as stealthy image wrapping functions that are imperceptible to human vision but can be captured and memorized by diffusion models. By analyzing whether the model has memorization for the injected content (i.e., whether the generated images are processed by the chosen post-processing function), we can detect models that had illegally utilized the unauthorized data. Our experiments conducted on Stable Diffusion and LoRA model demonstrate the effectiveness of the proposed method in detecting unauthorized data usages.
</details>
<details>
<summary>摘要</summary>
近些时候，文本到图像扩散模型的表现有所惊喜，但也有一些问题被报告。例如，一个模型培训者可能会收集一组由某个艺术家创作的图像，然后尝试使用这些图像来训练一个可以生成类似图像的模型，而不是获得艺术家的授权。为解决这个问题，在训练过程中检测不当数据使用变得非常重要。在这篇论文中，我们提议一种方法来检测这种不当数据使用，即在文本到图像扩散模型中植入插入记忆。具体来说，我们修改了受保护的图像集，并添加了一些隐藏的图像包装函数，这些函数可以让模型在生成图像时进行隐藏的记忆。通过判断模型是否有记忆这些插入的内容（即是否通过选择的后处理函数处理生成的图像），我们可以检测模型是否使用了未经授权的数据。我们在Stable Diffusion和LoRA模型上进行了实验，并证明了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Beyond-Intuition-a-Framework-for-Applying-GPs-to-Real-World-Data"><a href="#Beyond-Intuition-a-Framework-for-Applying-GPs-to-Real-World-Data" class="headerlink" title="Beyond Intuition, a Framework for Applying GPs to Real-World Data"></a>Beyond Intuition, a Framework for Applying GPs to Real-World Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03093">http://arxiv.org/abs/2307.03093</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kenzaxtazi/icml23-gpframe">https://github.com/kenzaxtazi/icml23-gpframe</a></li>
<li>paper_authors: Kenza Tazi, Jihao Andreas Lin, Ross Viljoen, Alex Gardner, ST John, Hong Ge, Richard E. Turner</li>
<li>for: 这篇论文是用于描述如何使用 Gaussian Processes (GPs) 进行回溯 regression 的方法和指南。</li>
<li>methods: 这篇论文使用的方法包括 kernel 设计和 computational scalability 的选择，以及如何设置一个强健且具体化的 GP 模型。</li>
<li>results: 在实际应用中，这篇论文使用 GPs 进行推测 glacier elevation change 的结果比较准确。<details>
<summary>Abstract</summary>
Gaussian Processes (GPs) offer an attractive method for regression over small, structured and correlated datasets. However, their deployment is hindered by computational costs and limited guidelines on how to apply GPs beyond simple low-dimensional datasets. We propose a framework to identify the suitability of GPs to a given problem and how to set up a robust and well-specified GP model. The guidelines formalise the decisions of experienced GP practitioners, with an emphasis on kernel design and options for computational scalability. The framework is then applied to a case study of glacier elevation change yielding more accurate results at test time.
</details>
<details>
<summary>摘要</summary>
Note:* "Gaussian Processes" is translated as "Gaussian processes" in Simplified Chinese, which is the standard way to refer to this topic in Chinese.* "low-dimensional" is translated as "小型" (xiǎo yì) in Simplified Chinese, which means "small" or "low-dimensional" in English.* "kernel design" is translated as "kernel设计" (jīn yì jīng yì) in Simplified Chinese, which means "kernel design" in English.* "computational scalability" is translated as "计算可扩展性" (jì yì kě xiǎo yì) in Simplified Chinese, which means "computational scalability" in English.
</details></li>
</ul>
<hr>
<h2 id="A-Novel-Site-Agnostic-Multimodal-Deep-Learning-Model-to-Identify-Pro-Eating-Disorder-Content-on-Social-Media"><a href="#A-Novel-Site-Agnostic-Multimodal-Deep-Learning-Model-to-Identify-Pro-Eating-Disorder-Content-on-Social-Media" class="headerlink" title="A Novel Site-Agnostic Multimodal Deep Learning Model to Identify Pro-Eating Disorder Content on Social Media"></a>A Novel Site-Agnostic Multimodal Deep Learning Model to Identify Pro-Eating Disorder Content on Social Media</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.06775">http://arxiv.org/abs/2307.06775</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonathan Feldman</li>
<li>for: This study aimed to create a multimodal deep learning model to determine if social media posts promote eating disorders based on visual and textual data.</li>
<li>methods: The study used a labeled dataset of Tweets and trained and tested twelve deep learning models, including a multimodal fusion of the RoBERTa natural language processing model and the MaxViT image classification model.</li>
<li>results: The RoBERTa and MaxViT fusion model achieved accuracy and F1 scores of 95.9% and 0.959, respectively, and was used to classify an unlabeled dataset of posts from Tumblr and Reddit. The model also uncovered a drastic decrease in the relative abundance of content that promotes eating disorders on eight Twitter hashtags since 2014, but with a resurgence by 2018.Here is the information in Simplified Chinese text:</li>
<li>for: 这个研究目的是创建一种基于多模态深度学习模型，以确定社交媒体文章是否推广吃见症，基于视觉和文本数据。</li>
<li>methods: 该研究使用了 Twitter 上的标注数据集，并训练和测试了十二个深度学习模型，其中包括 RoBERTa 自然语言处理模型和 MaxViT 图像分类模型的多模态融合。</li>
<li>results: RoBERTa 和 MaxViT 融合模型在识别 Tweets 中推广吃见症的任务上实现了准确率和 F1 分数为 95.9% 和 0.959，分别。此外，该模型还用于分类 Tumblr 和 Reddit 上的不标注数据集，并获得了类似于前一代研究所得到的结果，表明深度学习模型可以开发出与人类研究者相似的洞察。此外，模型还进行了 Twitter 上八个 Hashtag 的时间序分析，发现自2014年以来，内容推广吃见症的相对含量在这些社区内逐渐下降，但到2018年，内容推广吃见症又开始增加或停止下降。<details>
<summary>Abstract</summary>
Over the last decade, there has been a vast increase in eating disorder diagnoses and eating disorder-attributed deaths, reaching their zenith during the Covid-19 pandemic. This immense growth derived in part from the stressors of the pandemic but also from increased exposure to social media, which is rife with content that promotes eating disorders. This study aimed to create a multimodal deep learning model that can determine if a given social media post promotes eating disorders based on a combination of visual and textual data. A labeled dataset of Tweets was collected from Twitter, upon which twelve deep learning models were trained and tested. Based on model performance, the most effective deep learning model was the multimodal fusion of the RoBERTa natural language processing model and the MaxViT image classification model, attaining accuracy and F1 scores of 95.9% and 0.959, respectively. The RoBERTa and MaxViT fusion model, deployed to classify an unlabeled dataset of posts from the social media sites Tumblr and Reddit, generated results akin to those of previous research studies that did not employ artificial intelligence-based techniques, indicating that deep learning models can develop insights congruent to those of researchers. Additionally, the model was used to conduct a timeseries analysis of yet unseen Tweets from eight Twitter hashtags, uncovering that, since 2014, the relative abundance of content that promotes eating disorders has decreased drastically within those communities. Despite this reduction, by 2018, content that promotes eating disorders had either stopped declining or increased in ampleness anew on these hashtags.
</details>
<details>
<summary>摘要</summary>
过去一代，食用疾病诊断和因食用疾病而导致的死亡人数有很大增长，特别是在covid-19大流行期间。这种巨大增长来自于流行病的压力以及社交媒体上的内容，后者在患食用疾病的人群中更加普遍。这项研究旨在创建一个多模态深度学习模型，可以根据文本和视频数据判断社交媒体文章是否推广食用疾病。在Twitter上收集了一个标注的Twitter文章集，并训练了12个深度学习模型。根据模型性能，最有效的深度学习模型是将RoBERTa自然语言处理模型和MaxViT图像分类模型 multimodal融合，它的准确率和F1分数分别为95.9%和0.959。这个模型在分析Twitter上的未标注文章时，能够生成与人工智能技术不使用的研究成果相似的结果， indicating that deep learning models can develop insights congruent to those of researchers。此外，该模型还用于对Twitter上的八个Hashtag进行时间序分析，发现自2014年以来，这些社群中的不健康食物内容的相对含量有很大减少。然而，到2018年，这些社群中的不健康食物内容的含量已经减少或增加了。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/07/cs.LG_2023_07_07/" data-id="cllsj9wy60011uv8832ideiba" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_07_07" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/07/cs.SD_2023_07_07/" class="article-date">
  <time datetime="2023-07-06T16:00:00.000Z" itemprop="datePublished">2023-07-07</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/07/cs.SD_2023_07_07/">cs.SD - 2023-07-07 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="The-CHiME-7-UDASE-task-Unsupervised-domain-adaptation-for-conversational-speech-enhancement"><a href="#The-CHiME-7-UDASE-task-Unsupervised-domain-adaptation-for-conversational-speech-enhancement" class="headerlink" title="The CHiME-7 UDASE task: Unsupervised domain adaptation for conversational speech enhancement"></a>The CHiME-7 UDASE task: Unsupervised domain adaptation for conversational speech enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03533">http://arxiv.org/abs/2307.03533</a></li>
<li>repo_url: None</li>
<li>paper_authors: Simon Leglaive, Léonie Borne, Efthymios Tzinis, Mostafa Sadeghi, Matthieu Fraticelli, Scott Wisdom, Manuel Pariente, Daniel Pressnitzer, John R. Hershey</li>
<li>for: 这篇论文的目的是提出一个无监督领域适应对话音频减噪任务（UDASE），以利用实际采集的噪声听写记录来适应语音减噪模型。</li>
<li>methods: 这篇论文使用了无监督领域适应技术，利用实际采集的噪声听写记录来适应语音减噪模型。</li>
<li>results: 这篇论文提出了一个基eline系统，用于解决 conversational speech 中的噪声问题。<details>
<summary>Abstract</summary>
Supervised speech enhancement models are trained using artificially generated mixtures of clean speech and noise signals, which may not match real-world recording conditions at test time. This mismatch can lead to poor performance if the test domain significantly differs from the synthetic training domain. In this paper, we introduce the unsupervised domain adaptation for conversational speech enhancement (UDASE) task of the 7th CHiME challenge. This task aims to leverage real-world noisy speech recordings from the target test domain for unsupervised domain adaptation of speech enhancement models. The target test domain corresponds to the multi-speaker reverberant conversational speech recordings of the CHiME-5 dataset, for which the ground-truth clean speech reference is not available. Given a CHiME-5 recording, the task is to estimate the clean, potentially multi-speaker, reverberant speech, removing the additive background noise. We discuss the motivation for the CHiME-7 UDASE task and describe the data, the task, and the baseline system.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。<</SYS>>超级vised语音提升模型通常通过人工生成的清晰语音和噪声信号混合来进行训练，这些混合可能不符合实际录音条件。这种匹配不符问题可能会导致测试时的性能差。在这篇论文中，我们介绍了无监督领域适应对话语音提升任务（UDASE）的7个CHiME挑战。这个任务的目标是利用真实世界的噪声语音记录来无监督适应语音提升模型。目标测试频道对应的是CHiME-5数据集中的多个说话人 reverberant conversational speech记录，其中没有清晰语音参考。给一个CHiME-5记录，任务是估算清晰、可能多个说话人、 reverberant speech，从噪声背景噪声中移除。我们介绍了CHiME-7 UDASE任务的动机和数据、任务和基eline系统。
</details></li>
</ul>
<hr>
<h2 id="Token-Level-Serialized-Output-Training-for-Joint-Streaming-ASR-and-ST-Leveraging-Textual-Alignments"><a href="#Token-Level-Serialized-Output-Training-for-Joint-Streaming-ASR-and-ST-Leveraging-Textual-Alignments" class="headerlink" title="Token-Level Serialized Output Training for Joint Streaming ASR and ST Leveraging Textual Alignments"></a>Token-Level Serialized Output Training for Joint Streaming ASR and ST Leveraging Textual Alignments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03354">http://arxiv.org/abs/2307.03354</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sara Papi, Peidong Wan, Junkun Chen, Jian Xue, Jinyu Li, Yashesh Gaur</li>
<li>for: 这篇论文旨在提高实时语音识别和翻译的效果，并且使用单一decoder进行同时生成ASR和ST输出。</li>
<li>methods: 该方法使用一种joint token-level serialized output训练方法，通过利用市场上的文本对齐器来实现源和目标词的混合。</li>
<li>results: 实验表明，该方法在单语言（it-en）和多语言（de,es,it）设置下均能够达到最佳的质量-延迟平衡，并且与分立的ASR和ST模型相比，输出质量不减、甚至提高了0.4 BLEU和1.1 WER。<details>
<summary>Abstract</summary>
In real-world applications, users often require both translations and transcriptions of speech to enhance their comprehension, particularly in streaming scenarios where incremental generation is necessary. This paper introduces a streaming Transformer-Transducer that jointly generates automatic speech recognition (ASR) and speech translation (ST) outputs using a single decoder. To produce ASR and ST content effectively with minimal latency, we propose a joint token-level serialized output training method that interleaves source and target words by leveraging an off-the-shelf textual aligner. Experiments in monolingual (it-en) and multilingual (\{de,es,it\}-en) settings demonstrate that our approach achieves the best quality-latency balance. With an average ASR latency of 1s and ST latency of 1.3s, our model shows no degradation or even improves output quality compared to separate ASR and ST models, yielding an average improvement of 1.1 WER and 0.4 BLEU in the multilingual case.
</details>
<details>
<summary>摘要</summary>
在实际应用中，用户经常需要同时获得翻译和转写的speech内容，以提高其理解度，特别在流媒体enario中，需要逐步生成。这篇论文介绍了一个流动Transformer-Transducer，通过单个解码器同时生成自动语音识别（ASR）和语音翻译（ST）输出。为了在最小延迟下生成ASR和ST内容，我们提议使用单个Token水平的 serialized输出训练方法，通过利用市场上的文本对齐器来扩展源和目标词。实验在单语言（it-en）和多语言（de,es,it）的设置下表明，我们的方法可以实现最佳的质量-延迟平衡。我们的模型在1s的ASR延迟和1.3s的ST延迟下，不产生质量下降或者even improves输出质量，相对于分离的ASR和ST模型，平均提高1.1 WER和0.4 BLEU的多语言情况。
</details></li>
</ul>
<hr>
<h2 id="Gammatonegram-Representation-for-End-to-End-Dysarthric-Speech-Processing-Tasks-Speech-Recognition-Speaker-Identification-and-Intelligibility-Assessment"><a href="#Gammatonegram-Representation-for-End-to-End-Dysarthric-Speech-Processing-Tasks-Speech-Recognition-Speaker-Identification-and-Intelligibility-Assessment" class="headerlink" title="Gammatonegram Representation for End-to-End Dysarthric Speech Processing Tasks: Speech Recognition, Speaker Identification, and Intelligibility Assessment"></a>Gammatonegram Representation for End-to-End Dysarthric Speech Processing Tasks: Speech Recognition, Speaker Identification, and Intelligibility Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03296">http://arxiv.org/abs/2307.03296</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/areffarhadi/gammatonegram_cnn_dysarthric_speech">https://github.com/areffarhadi/gammatonegram_cnn_dysarthric_speech</a></li>
<li>paper_authors: Aref Farhadipour, Hadi Veisi</li>
<li>for:  This paper aims to develop a system for speech recognition, speaker identification, and intelligibility assessment for individuals with dysarthria.</li>
<li>methods:  The proposed system uses gammatonegram to represent audio files with discriminative details, which are then fed into a convolutional neural network (CNN) for recognition. The system also employs transfer learning and Alexnet pre-training for improved accuracy.</li>
<li>results:  The proposed system achieved 91.29% accuracy in speaker-dependent mode, 87.74% accuracy in text-dependent mode, and 96.47% accuracy in two-class mode for intelligibility assessment. Additionally, the multi-network speech recognition system achieved an accuracy of 92.3% WRR.<details>
<summary>Abstract</summary>
Dysarthria is a disability that causes a disturbance in the human speech system and reduces the quality and intelligibility of a person's speech. Because of this effect, the normal speech processing systems can not work properly on impaired speech. This disability is usually associated with physical disabilities. Therefore, designing a system that can perform some tasks by receiving voice commands in the smart home can be a significant achievement. In this work, we introduce gammatonegram as an effective method to represent audio files with discriminative details, which is used as input for the convolutional neural network. On the other word, we convert each speech file into an image and propose image recognition system to classify speech in different scenarios. Proposed CNN is based on the transfer learning method on the pre-trained Alexnet. In this research, the efficiency of the proposed system for speech recognition, speaker identification, and intelligibility assessment is evaluated. According to the results on the UA dataset, the proposed speech recognition system achieved 91.29% accuracy in speaker-dependent mode, the speaker identification system acquired 87.74% accuracy in text-dependent mode, and the intelligibility assessment system achieved 96.47% accuracy in two-class mode. Finally, we propose a multi-network speech recognition system that works fully automatically. This system is located in a cascade arrangement with the two-class intelligibility assessment system, and the output of this system activates each one of the speech recognition networks. This architecture achieves an accuracy of 92.3% WRR. The source code of this paper is available.
</details>
<details>
<summary>摘要</summary>
<<sys.translation.content>>嗣瑞thesis是一种功能受限的人类语言系统的异常，导致人类语音质量和可理解性降低。由于这种效果，常规的语音处理系统无法正常工作。这种疾病通常与物理障碍有关。因此，设计一个可以通过声音命令在智能家庭中完成一些任务的系统可以是一项重要成果。在这种工作中，我们介绍了一种有效的方法来表示音频文件的特征，即干扰agram。即将每个语音文件转换成图像，并提议图像识别系统来分类语音在不同的场景中。我们的CNN基于传输学习方法，使用预训练的Alexnet。在这项研究中，我们评估了提议的系统的效率，包括语音识别、说话人识别和可理解性评估。根据UA数据集的结果，我们的语音识别系统在 speaker-dependent 模式下达到了 91.29% 的准确率，说话人识别系统在 text-dependent 模式下达到了 87.74% 的准确率，而可理解性评估系统在 two-class 模式下达到了 96.47% 的准确率。最后，我们提议了一个多网络语音识别系统，该系统位于堆叠式排序中，以及每个语音识别网络的输出。这种架构达到了 92.3% WRR 的准确率。本文的源代码可以获取。<</sys.translation.content>>
</details></li>
</ul>
<hr>
<h2 id="Performance-Comparison-of-Pre-trained-Models-for-Speech-to-Text-in-Turkish-Whisper-Small-and-Wav2Vec2-XLS-R-300M"><a href="#Performance-Comparison-of-Pre-trained-Models-for-Speech-to-Text-in-Turkish-Whisper-Small-and-Wav2Vec2-XLS-R-300M" class="headerlink" title="Performance Comparison of Pre-trained Models for Speech-to-Text in Turkish: Whisper-Small and Wav2Vec2-XLS-R-300M"></a>Performance Comparison of Pre-trained Models for Speech-to-Text in Turkish: Whisper-Small and Wav2Vec2-XLS-R-300M</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04765">http://arxiv.org/abs/2307.04765</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oyku Berfin Mercan, Sercan Cepni, Davut Emre Tasar, Sukru Ozan</li>
<li>for: 本研究探讨了两种预训练多语言模型（Whisper-Small和Wav2Vec2-XLS-R-300M）在土耳其语言上的表现。</li>
<li>methods: 本研究使用了Mozilla Common Voice版本11.0，这是一个开源的土耳其语言数据集，并对两种模型进行了微调。</li>
<li>results: 研究发现，使用Wav2Vec2-XLS-R-300M模型可以得到更高的语音识别精度（WER值为0.16），而使用Whisper-Small模型的WER值为0.28。此外，研究还发现，使用测试数据集，不包括在训练和验证数据集中的call center记录，可以提高模型的表现。<details>
<summary>Abstract</summary>
In this study, the performances of the Whisper-Small and Wav2Vec2-XLS-R-300M models which are two pre-trained multilingual models for speech to text were examined for the Turkish language. Mozilla Common Voice version 11.0 which is prepared in Turkish language and is an open-source data set, was used in the study. The multilingual models, Whisper- Small and Wav2Vec2-XLS-R-300M were fine-tuned with this data set which contains a small amount of data. The speech to text performance of the two models was compared. WER values are calculated as 0.28 and 0.16 for the Wav2Vec2-XLS- R-300M and the Whisper-Small models respectively. In addition, the performances of the models were examined with the test data prepared with call center records that were not included in the training and validation dataset.
</details>
<details>
<summary>摘要</summary>
在这个研究中，我们研究了两种预训练的多语言模型：Whisper-Small和Wav2Vec2-XLS-R-300M，用于识别土耳其语。这些模型在土耳其语 Mozilla Common Voice 版本11.0 数据集上进行了训练和测试。这个数据集包含一小量的数据，我们使用这些数据来精度地训练和测试这两种模型。我们计算了 WER 值，其中 Wav2Vec2-XLS-R-300M 的 WER 值为 0.28，Whisper-Small 模型的 WER 值为 0.16。此外，我们还测试了这两种模型在未包括在训练和验证数据集中的测试数据上的性能。
</details></li>
</ul>
<hr>
<h2 id="Whisper-AT-Noise-Robust-Automatic-Speech-Recognizers-are-Also-Strong-General-Audio-Event-Taggers"><a href="#Whisper-AT-Noise-Robust-Automatic-Speech-Recognizers-are-Also-Strong-General-Audio-Event-Taggers" class="headerlink" title="Whisper-AT: Noise-Robust Automatic Speech Recognizers are Also Strong General Audio Event Taggers"></a>Whisper-AT: Noise-Robust Automatic Speech Recognizers are Also Strong General Audio Event Taggers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03183">http://arxiv.org/abs/2307.03183</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/YuanGongND/whisper-at">https://github.com/YuanGongND/whisper-at</a></li>
<li>paper_authors: Yuan Gong, Sameer Khurana, Leonid Karlinsky, James Glass</li>
<li>for: 这个论文专注于 Whisper 模型，一种基于大量标注的语音识别模型，以及该模型在不同环境下的表现。</li>
<li>methods: 论文首先展示了 Whisper 模型对真实世界背景噪音的强健性，但它的音频表示并不是噪音不变的，而是高度相关于非语音噪音。基于这一发现，论文建立了一个简单的音频标记和语音识别模型 Whisper-AT，通过冻结 Whisper 的背bone，并在顶部添加一个轻量级的音频标记模型。</li>
<li>results: Whisper-AT 可以在单个前进 pass 中同时进行语音识别和音频标记，并且只需 &lt;1% 的额外计算成本。<details>
<summary>Abstract</summary>
In this paper, we focus on Whisper, a recent automatic speech recognition model trained with a massive 680k hour labeled speech corpus recorded in diverse conditions. We first show an interesting finding that while Whisper is very robust against real-world background sounds (e.g., music), its audio representation is actually not noise-invariant, but is instead highly correlated to non-speech sounds, indicating that Whisper recognizes speech conditioned on the noise type. With this finding, we build a unified audio tagging and speech recognition model Whisper-AT by freezing the backbone of Whisper, and training a lightweight audio tagging model on top of it. With <1% extra computational cost, Whisper-AT can recognize audio events, in addition to spoken text, in a single forward pass.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们关注Whisper，一种最近的自动语音识别模型，使用了巨量的680万小时标注的语音词汇录音数据，录制在多种条件下。我们首先发现一个有趣的现象，即Whisper对实际世界背景声（如音乐）非常鲁棒，但它的音频表示并不是噪声不变的，而是高度相关于非语音声音，表明Whisper识别语音 conditional 于噪声类型。基于这一发现，我们构建了一个整合音频标记和语音识别模型Whisper-AT，通过冻结Whisper的背bone，并在其上训练一个轻量级音频标记模型。与<1%的额外计算成本，Whisper-AT可以在单个前进通过recognize audio事件，以及说话文本，在一个前进中完成。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/07/cs.SD_2023_07_07/" data-id="cllsj9wyv003duv881r8vdzia" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_07_07" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/07/eess.AS_2023_07_07/" class="article-date">
  <time datetime="2023-07-06T16:00:00.000Z" itemprop="datePublished">2023-07-07</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/07/eess.AS_2023_07_07/">eess.AS - 2023-07-07 22:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Recovering-implicit-pitch-contours-from-formants-in-whispered-speech"><a href="#Recovering-implicit-pitch-contours-from-formants-in-whispered-speech" class="headerlink" title="Recovering implicit pitch contours from formants in whispered speech"></a>Recovering implicit pitch contours from formants in whispered speech</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03168">http://arxiv.org/abs/2307.03168</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pablo Pérez Zarazaga, Zofia Malisz</li>
<li>for: 这个论文主要研究了whisper speech中的intonation的感知和表达方式。</li>
<li>methods: 作者使用了一种两步方法，首先使用平行 corpus 将whisper中的声学特征转换成相应的phonatedEquivalents，然后分析声学特征来预测phonated pitch contour的变化。</li>
<li>results: 研究发现，使用这种方法可以确定whisper中的声学特征和phonated pitch contour之间的关系，并揭示了whisper中的implicit pitch contour。<details>
<summary>Abstract</summary>
Whispered speech is characterised by a noise-like excitation that results in the lack of fundamental frequency. Considering that prosodic phenomena such as intonation are perceived through f0 variation, the perception of whispered prosody is relatively difficult. At the same time, studies have shown that speakers do attempt to produce intonation when whispering and that prosodic variability is being transmitted, suggesting that intonation "survives" in whispered formant structure. In this paper, we aim to estimate the way in which formant contours correlate with an "implicit" pitch contour in whisper, using a machine learning model. We propose a two-step method: using a parallel corpus, we first transform the whispered formants into their phonated equivalents using a denoising autoencoder. We then analyse the formant contours to predict phonated pitch contour variation. We observe that our method is effective in establishing a relationship between whispered and phonated formants and in uncovering implicit pitch contours in whisper.
</details>
<details>
<summary>摘要</summary>
含秘语言特征为噪声类刺激，导致基本频率的缺失。由于语音学中的听觉现象如声调变化是通过f0变化传递的，因此听众对潜 voce 的识别相对较难。然而，研究表明，当speaker whispering时，他们仍会尝试生成声调，并且发现了不同的语音变化，表明声调在潜 voce 中存在。在这篇论文中，我们想使用机器学习模型来估算潜 voce 中形式轨迹与隐藏的声调轨迹之间的相关性。我们提出了一种两步方法：首先，使用平行 корпу斯，将潜 voce 的形式轨迹转换为其相应的声调轨迹，使用杜因噪声自适应神经网络。然后，我们分析形式轨迹，预测声调轨迹的变化。我们发现，我们的方法能够有效地建立潜 voce 中形式轨迹和声调轨迹之间的关系，并且揭示了隐藏的声调轨迹。
</details></li>
</ul>
<hr>
<h2 id="Label-Synchronous-Neural-Transducer-for-End-to-End-ASR"><a href="#Label-Synchronous-Neural-Transducer-for-End-to-End-ASR" class="headerlink" title="Label-Synchronous Neural Transducer for End-to-End ASR"></a>Label-Synchronous Neural Transducer for End-to-End ASR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03088">http://arxiv.org/abs/2307.03088</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keqi Deng, Philip C. Woodland</li>
<li>for: 这篇论文是关于 streaming ASR 的自然途径，但是它在使用文本数据进行预测时遇到了挑战。</li>
<li>methods: 该论文提出了一种 label-同步神经转换器 (LS-Transducer)，它在输出序列中提取出了标签水平编码表示，然后将其与预测网络输出结合。这样，不再需要使用空token，并且可以轻松地适应文本数据。此外，论文还提出了一种自动循环整合和触发 (AIF) 机制，以生成标筹水平编码表示。</li>
<li>results: 实验表明，相比标准神经转换器，提出的 LS-Transducer 在内部预测 Librispeech-100h 数据上减少了10%的相对WRER（文本识别错误率），以及在跨频度的 TED-LIUM 2 和 AESRC2020 数据上减少了17%和19%的相对WRER。<details>
<summary>Abstract</summary>
Neural transducers provide a natural approach to streaming ASR. However, they augment output sequences with blank tokens which leads to challenges for domain adaptation using text data. This paper proposes a label-synchronous neural transducer (LS-Transducer), which extracts a label-level encoder representation before combining it with the prediction network output. Hence blank tokens are no longer needed and the prediction network can be easily adapted using text data. An Auto-regressive Integrate-and-Fire (AIF) mechanism is proposed to generate the label-level encoder representation while retaining the streaming property. In addition, a streaming joint decoding method is designed to improve ASR accuracy. Experiments show that compared to standard neural transducers, the proposed LS-Transducer gave a 10% relative WER reduction (WERR) for intra-domain Librispeech-100h data, as well as 17% and 19% relative WERRs on cross-domain TED-LIUM 2 and AESRC2020 data with an adapted prediction network.
</details>
<details>
<summary>摘要</summary>
“神经变换器提供了自然的流处理ASR方法。然而，它们在输出序列中添加空token，导致领域适应使用文本数据具有挑战。这篇论文提议了一种标签同步神经变换器（LS-Transducer），它在组合预测网络输出之前提取标签水平Encoder表示。因此，空token不再需要，预测网络可以轻松地适应文本数据。此外，一种自动重启综合射频（AIF）机制被提议，以生成标签水平Encoder表示，同时保持流处理性。此外，一种流处理共同解码方法被设计，以提高ASR准确性。实验表明，相比标准神经变换器，提议的LS-Transducer在内领域Librispeech-100h数据上减少了10%的相对WRER（文本识别错误率），以及在跨领域TED-LIUM 2和AESRC2020数据上适应预测网络后减少了17%和19%的相对WRER。”Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/07/eess.AS_2023_07_07/" data-id="cllsj9wzj005quv88ayofdrmd" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_07_07" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/07/eess.IV_2023_07_07/" class="article-date">
  <time datetime="2023-07-06T16:00:00.000Z" itemprop="datePublished">2023-07-07</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/07/eess.IV_2023_07_07/">eess.IV - 2023-07-07 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Detecting-the-Sensing-Area-of-A-Laparoscopic-Probe-in-Minimally-Invasive-Cancer-Surgery"><a href="#Detecting-the-Sensing-Area-of-A-Laparoscopic-Probe-in-Minimally-Invasive-Cancer-Surgery" class="headerlink" title="Detecting the Sensing Area of A Laparoscopic Probe in Minimally Invasive Cancer Surgery"></a>Detecting the Sensing Area of A Laparoscopic Probe in Minimally Invasive Cancer Surgery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03662">http://arxiv.org/abs/2307.03662</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/br0202/sensing_area_detection">https://github.com/br0202/sensing_area_detection</a></li>
<li>paper_authors: Baoru Huang, Yicheng Hu, Anh Nguyen, Stamatia Giannarou, Daniel S. Elson</li>
<li>for: This paper aims to improve the accuracy of endoscopic radio-guided cancer detection and resection by developing a novel method for detecting the sensing area of a tethered laparoscopic gamma detector.</li>
<li>methods: The proposed method uses a simple regression network to leverage high-dimensional image features and probe position information to visualize gamma activity origination on the tissue surface.</li>
<li>results: The authors demonstrated the effectiveness of their method through intensive experimentation using two publicly released datasets captured with a custom-designed, portable stereo laparoscope system, establishing a new performance benchmark.<details>
<summary>Abstract</summary>
In surgical oncology, it is challenging for surgeons to identify lymph nodes and completely resect cancer even with pre-operative imaging systems like PET and CT, because of the lack of reliable intraoperative visualization tools. Endoscopic radio-guided cancer detection and resection has recently been evaluated whereby a novel tethered laparoscopic gamma detector is used to localize a preoperatively injected radiotracer. This can both enhance the endoscopic imaging and complement preoperative nuclear imaging data. However, gamma activity visualization is challenging to present to the operator because the probe is non-imaging and it does not visibly indicate the activity origination on the tissue surface. Initial failed attempts used segmentation or geometric methods, but led to the discovery that it could be resolved by leveraging high-dimensional image features and probe position information. To demonstrate the effectiveness of this solution, we designed and implemented a simple regression network that successfully addressed the problem. To further validate the proposed solution, we acquired and publicly released two datasets captured using a custom-designed, portable stereo laparoscope system. Through intensive experimentation, we demonstrated that our method can successfully and effectively detect the sensing area, establishing a new performance benchmark. Code and data are available at https://github.com/br0202/Sensing_area_detection.git
</details>
<details>
<summary>摘要</summary>
在外科onkoloji中，外科医生很难以识别lymph node和完全remove癌症，即使使用前 operated imaging系统like PET和CT，因为缺乏可靠的手术过程中的视觉化工具。 Recently, endoscopic radio-guided cancer detection and resection has been evaluated, which uses a novel tethered laparoscopic gamma detector to localize a preoperatively injected radiotracer. This can both enhance endoscopic imaging and complement preoperative nuclear imaging data. However, gamma activity visualization is challenging to present to the operator because the probe is non-imaging and it does not visibly indicate the activity origination on the tissue surface. Early attempts used segmentation or geometric methods, but these were not successful. Instead, we found that the problem could be resolved by leveraging high-dimensional image features and probe position information. To demonstrate the effectiveness of this solution, we designed and implemented a simple regression network that successfully addressed the problem. To further validate the proposed solution, we acquired and publicly released two datasets captured using a custom-designed, portable stereo laparoscope system. Through extensive experimentation, we demonstrated that our method can successfully and effectively detect the sensing area, establishing a new performance benchmark. Code and data are available at <https://github.com/br0202/Sensing_area_detection.git>.
</details></li>
</ul>
<hr>
<h2 id="VesselVAE-Recursive-Variational-Autoencoders-for-3D-Blood-Vessel-Synthesis"><a href="#VesselVAE-Recursive-Variational-Autoencoders-for-3D-Blood-Vessel-Synthesis" class="headerlink" title="VesselVAE: Recursive Variational Autoencoders for 3D Blood Vessel Synthesis"></a>VesselVAE: Recursive Variational Autoencoders for 3D Blood Vessel Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03592">http://arxiv.org/abs/2307.03592</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paula Feldman, Miguel Fainstein, Viviana Siless, Claudio Delrieux, Emmanuel Iarussi</li>
<li>For: 该论文旨在提出一种数据驱动的生成框架，用于synthesizing blood vessel 3D geometry。* Methods: 该方法使用Recursive Variational Neural Network（RVNN），全面利用血管的层次结构，学习低维抽象空间，包括分支连接性和表面特征。* Results: 该方法可以生成高度相似的真实和 sintetic 数据，包括半径（.97）、长度（.95）和折叠度（.96）。通过深度神经网络的力量，该方法生成的3D血管模型具有高度准确和多样性，这对医疗和手术培训、血液动力学 simulations 等方面都非常重要。<details>
<summary>Abstract</summary>
We present a data-driven generative framework for synthesizing blood vessel 3D geometry. This is a challenging task due to the complexity of vascular systems, which are highly variating in shape, size, and structure. Existing model-based methods provide some degree of control and variation in the structures produced, but fail to capture the diversity of actual anatomical data. We developed VesselVAE, a recursive variational Neural Network that fully exploits the hierarchical organization of the vessel and learns a low-dimensional manifold encoding branch connectivity along with geometry features describing the target surface. After training, the VesselVAE latent space can be sampled to generate new vessel geometries. To the best of our knowledge, this work is the first to utilize this technique for synthesizing blood vessels. We achieve similarities of synthetic and real data for radius (.97), length (.95), and tortuosity (.96). By leveraging the power of deep neural networks, we generate 3D models of blood vessels that are both accurate and diverse, which is crucial for medical and surgical training, hemodynamic simulations, and many other purposes.
</details>
<details>
<summary>摘要</summary>
我们提出了一种基于数据的生成框架，用于 sintesizing 血管三维几何结构。这是一项具有挑战性的任务，因为血管系统的复杂性以及形态、大小和结构的多样性。现有的模型基于方法可以提供一定的控制和变化，但是它们无法捕捉实际 анатомиче数据的多样性。我们开发了 VesselVAE，一种嵌入式的变量神经网络，它完全利用血管的层次结构，并学习低维度拟合空间，包括连接分支和表面几何特征。经过训练，VesselVAE 的缓存空间可以采样生成新的血管几何结构。根据我们所知，这是第一次利用这种技术来 sintesizing 血管。我们实现了真实数据和 sintesizing 数据之间的相似性（.97）、长度（.95）和折叠性（.96）。通过利用深度神经网络的力量，我们生成了高度准确和多样的血管三维模型，这对医疗和手术培训、血液动力学计算以及许多其他目的都是关键。
</details></li>
</ul>
<hr>
<h2 id="Physical-Color-Calibration-of-Digital-Pathology-Scanners-for-Robust-Artificial-Intelligence-Assisted-Cancer-Diagnosis"><a href="#Physical-Color-Calibration-of-Digital-Pathology-Scanners-for-Robust-Artificial-Intelligence-Assisted-Cancer-Diagnosis" class="headerlink" title="Physical Color Calibration of Digital Pathology Scanners for Robust Artificial Intelligence Assisted Cancer Diagnosis"></a>Physical Color Calibration of Digital Pathology Scanners for Robust Artificial Intelligence Assisted Cancer Diagnosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05519">http://arxiv.org/abs/2307.05519</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoyi Ji, Richard Salmon, Nita Mulliqi, Umair Khan, Yinxi Wang, Anders Blilie, Henrik Olsson, Bodil Ginnerup Pedersen, Karina Dalsgaard Sørensen, Benedicte Parm Ulhøi, Svein R Kjosavik, Emilius AM Janssen, Mattias Rantalainen, Lars Egevad, Pekka Ruusuvuori, Martin Eklund, Kimmo Kartasalo</li>
<li>for: 提高艾特ints家用于数字病理学的可靠性和应用性</li>
<li>methods: 使用物理颜色准确标准化扫描仪的扫描图像，以提高人工智能系统的评估性和可靠性</li>
<li>results: 实验结果表明，物理颜色准确标准化可以标准化扫描仪的扫描图像，提高艾特ints家的评估性和可靠性，使其在临床应用中更加可靠和可行<details>
<summary>Abstract</summary>
The potential of artificial intelligence (AI) in digital pathology is limited by technical inconsistencies in the production of whole slide images (WSIs), leading to degraded AI performance and posing a challenge for widespread clinical application as fine-tuning algorithms for each new site is impractical. Changes in the imaging workflow can also lead to compromised diagnoses and patient safety risks. We evaluated whether physical color calibration of scanners can standardize WSI appearance and enable robust AI performance. We employed a color calibration slide in four different laboratories and evaluated its impact on the performance of an AI system for prostate cancer diagnosis on 1,161 WSIs. Color standardization resulted in consistently improved AI model calibration and significant improvements in Gleason grading performance. The study demonstrates that physical color calibration provides a potential solution to the variation introduced by different scanners, making AI-based cancer diagnostics more reliable and applicable in clinical settings.
</details>
<details>
<summary>摘要</summary>
人工智能（AI）在数字 PATHOLOGY 的潜力受到数字标本（WSIs）的技术不一致所限制，导致 AI 性能下降，使得广泛应用在临床中采用困难。变化在扫描 workflow 也可能导致诊断错误和患者安全风险。我们评估了扫描器的物理色彩准确性是否可以标准化 WSI 的外观，并使 AI 系统在 1,161 个标本上表现出色。结果显示，物理色彩准确性可以提高 AI 模型准确性，并且在 Gleason 分期性能上显示了明显的改善。这一研究表明，物理色彩准确性可以解决不同扫描器导致的变化，使 AI 基于 cancer 诊断更可靠和在临床设置中实用。
</details></li>
</ul>
<hr>
<h2 id="A-Deep-Active-Contour-Model-for-Delineating-Glacier-Calving-Fronts"><a href="#A-Deep-Active-Contour-Model-for-Delineating-Glacier-Calving-Fronts" class="headerlink" title="A Deep Active Contour Model for Delineating Glacier Calving Fronts"></a>A Deep Active Contour Model for Delineating Glacier Calving Fronts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03461">http://arxiv.org/abs/2307.03461</a></li>
<li>repo_url: None</li>
<li>paper_authors: Konrad Heidler, Lichao Mou, Erik Loebel, Mirko Scheinert, Sébastien Lefèvre, Xiao Xiang Zhu</li>
<li>for: 这个论文是关于冰川分裂前模型的研究，旨在提出一种基于 outline 找出的方法，以提高冰川分裂前探测器的准确率。</li>
<li>methods: 该方法 combine 了 Convolutional Neural Networks (CNNs) 和 active contour model，以提取特征和描述 outline。</li>
<li>results: 通过在格陵兰冰川的多个大规模数据集上训练和评估，该方法被证明超过基于 segmentation 和 edge-detection 的方法。此外，该方法还可以更好地计算模型预测结果的不确定性。<details>
<summary>Abstract</summary>
Choosing how to encode a real-world problem as a machine learning task is an important design decision in machine learning. The task of glacier calving front modeling has often been approached as a semantic segmentation task. Recent studies have shown that combining segmentation with edge detection can improve the accuracy of calving front detectors. Building on this observation, we completely rephrase the task as a contour tracing problem and propose a model for explicit contour detection that does not incorporate any dense predictions as intermediate steps. The proposed approach, called ``Charting Outlines by Recurrent Adaptation'' (COBRA), combines Convolutional Neural Networks (CNNs) for feature extraction and active contour models for the delineation. By training and evaluating on several large-scale datasets of Greenland's outlet glaciers, we show that this approach indeed outperforms the aforementioned methods based on segmentation and edge-detection. Finally, we demonstrate that explicit contour detection has benefits over pixel-wise methods when quantifying the models' prediction uncertainties. The project page containing the code and animated model predictions can be found at \url{https://khdlr.github.io/COBRA/}.
</details>
<details>
<summary>摘要</summary>
选择如何编码实际问题为机器学习任务是机器学习设计决策的重要一环。冰川脱落前模型的任务经常被看作为语义分割任务。 latest studies have shown that combining segmentation with edge detection can improve the accuracy of calving front detectors. 在这个基础上，我们完全重新表述任务为一个 outline tracing 问题，并提出一种不包含任何稠密预测的模型。我们称之为“Charting Outlines by Recurrent Adaptation”（COBRA），它将 Convolutional Neural Networks（CNNs）用于特征提取和活动 kontur 模型来进行定义。通过对瑞典格陵兰冰川出口的数据进行训练和评估，我们示出了这种方法实际上超过了以前基于 segmentation 和 edge-detection 的方法。最后，我们示出了明确的 outline 检测在量化模型预测不确定性时的优势。关于这个项目，包含代码和动画预测的项目页面可以在 \url{https://khdlr.github.io/COBRA/} 上找到。
</details></li>
</ul>
<hr>
<h2 id="Non-iterative-Coarse-to-fine-Transformer-Networks-for-Joint-Affine-and-Deformable-Image-Registration"><a href="#Non-iterative-Coarse-to-fine-Transformer-Networks-for-Joint-Affine-and-Deformable-Image-Registration" class="headerlink" title="Non-iterative Coarse-to-fine Transformer Networks for Joint Affine and Deformable Image Registration"></a>Non-iterative Coarse-to-fine Transformer Networks for Joint Affine and Deformable Image Registration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03421">http://arxiv.org/abs/2307.03421</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mungomeng/registration-nice-trans">https://github.com/mungomeng/registration-nice-trans</a></li>
<li>paper_authors: Mingyuan Meng, Lei Bi, Michael Fulham, Dagan Feng, Jinman Kim</li>
<li>for: 这个论文主要是为了提出一种基于深度学习的非迭代抽象图像 региSTRATION方法，以提高图像REGISTRATION的精度和效率。</li>
<li>methods: 这个方法使用了非迭代抽象图像REGISTRATION的单个网络，并首次将 transformers 引入到 NICE 图像REGISTRATION 框架中，以模型图像之间的长距离相关性。</li>
<li>results: 经过广泛的七个公共数据集的实验，这个方法的注意力 Transformer 在 NICE 图像REGISTRATION 中表现出了优于当前状态艺术的精度和效率。<details>
<summary>Abstract</summary>
Image registration is a fundamental requirement for medical image analysis. Deep registration methods based on deep learning have been widely recognized for their capabilities to perform fast end-to-end registration. Many deep registration methods achieved state-of-the-art performance by performing coarse-to-fine registration, where multiple registration steps were iterated with cascaded networks. Recently, Non-Iterative Coarse-to-finE (NICE) registration methods have been proposed to perform coarse-to-fine registration in a single network and showed advantages in both registration accuracy and runtime. However, existing NICE registration methods mainly focus on deformable registration, while affine registration, a common prerequisite, is still reliant on time-consuming traditional optimization-based methods or extra affine registration networks. In addition, existing NICE registration methods are limited by the intrinsic locality of convolution operations. Transformers may address this limitation for their capabilities to capture long-range dependency, but the benefits of using transformers for NICE registration have not been explored. In this study, we propose a Non-Iterative Coarse-to-finE Transformer network (NICE-Trans) for image registration. Our NICE-Trans is the first deep registration method that (i) performs joint affine and deformable coarse-to-fine registration within a single network, and (ii) embeds transformers into a NICE registration framework to model long-range relevance between images. Extensive experiments with seven public datasets show that our NICE-Trans outperforms state-of-the-art registration methods on both registration accuracy and runtime.
</details>
<details>
<summary>摘要</summary>
医疗图像分析中的图像注册是一项基本要求。基于深度学习的深度注册方法在过去几年内得到了广泛的认可，因为它们可以快速完成端到端注册。许多深度注册方法在多个注册步骤中使用了缩放网络，以实现粗细到细节的注册。然而，现有的NICE注册方法主要关注于可变性注册，而平移注册，是医疗图像注册的常见前提，仍然是通过传统的优化方法或额外的平移注册网络来实现的。此外，现有的NICE注册方法受到了卷积操作的本地性的限制。使用transformer可以解决这一限制，因为它可以捕捉图像之间的长距离相关性。但是，使用transformer进行NICE注册的好处尚未得到了探讨。在本研究中，我们提出了一种非iterative粗细到细节的transformer网络（NICE-Trans） для图像注册。我们的NICE-Trans是首个深度注册方法，它（i）在单个网络中同时实现了平移和可变性的粗细到细节注册，（ii）将transformer引入NICE注册框架，以模型图像之间的长距离相关性。我们在七个公共数据集上进行了广泛的实验，结果表明，我们的NICE-Trans比状态之前的注册方法在注册精度和运行时间上都有提高。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Hyperspectral-and-Multispectral-Images-Fusion-Based-on-the-Cycle-Consistency"><a href="#Unsupervised-Hyperspectral-and-Multispectral-Images-Fusion-Based-on-the-Cycle-Consistency" class="headerlink" title="Unsupervised Hyperspectral and Multispectral Images Fusion Based on the Cycle Consistency"></a>Unsupervised Hyperspectral and Multispectral Images Fusion Based on the Cycle Consistency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03413">http://arxiv.org/abs/2307.03413</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shuaikaishi/CycFusion">https://github.com/shuaikaishi/CycFusion</a></li>
<li>paper_authors: Shuaikai Shi, Lijun Zhang, Yoann Altmann, Jie Chen</li>
<li>for: 这个论文主要针对的问题是如何实现高spectral resolution和高 spatial resolution的图像拼接，并且提出了一种基于循环一致的无监督拼接模型。</li>
<li>methods: 该模型基于循环一致的概念，将低spectral resolution的干扰图像（LrHSI）和高spectral resolution的多spectral图像（HrMSI）映射到高spectral resolution的图像中，并且通过单个变换和双重变换的对比来学习域转换。</li>
<li>results: 对于多个数据集，该模型的实验结果表明，与其他无监督拼接方法相比，该模型能够更好地实现高spectral resolution和高 spatial resolution的图像拼接，并且可以在不知道干扰参数的情况下进行拼接。<details>
<summary>Abstract</summary>
Hyperspectral images (HSI) with abundant spectral information reflected materials property usually perform low spatial resolution due to the hardware limits. Meanwhile, multispectral images (MSI), e.g., RGB images, have a high spatial resolution but deficient spectral signatures. Hyperspectral and multispectral image fusion can be cost-effective and efficient for acquiring both high spatial resolution and high spectral resolution images. Many of the conventional HSI and MSI fusion algorithms rely on known spatial degradation parameters, i.e., point spread function, spectral degradation parameters, spectral response function, or both of them. Another class of deep learning-based models relies on the ground truth of high spatial resolution HSI and needs large amounts of paired training images when working in a supervised manner. Both of these models are limited in practical fusion scenarios. In this paper, we propose an unsupervised HSI and MSI fusion model based on the cycle consistency, called CycFusion. The CycFusion learns the domain transformation between low spatial resolution HSI (LrHSI) and high spatial resolution MSI (HrMSI), and the desired high spatial resolution HSI (HrHSI) are considered to be intermediate feature maps in the transformation networks. The CycFusion can be trained with the objective functions of marginal matching in single transform and cycle consistency in double transforms. Moreover, the estimated PSF and SRF are embedded in the model as the pre-training weights, which further enhances the practicality of our proposed model. Experiments conducted on several datasets show that our proposed model outperforms all compared unsupervised fusion methods. The codes of this paper will be available at this address: https: //github.com/shuaikaishi/CycFusion for reproducibility.
</details>
<details>
<summary>摘要</summary>
干扰影像（HSI）具有丰富的 спектраль信息，通常由硬件限制而导致低空间分辨率。而多spectral影像（MSI），例如RGB影像，具有高空间分辨率，但缺乏 спектраль特征。干扰影像和多spectral影像的图像混合可以是成本效益和高效的方式，以获得高空间分辨率和高спектраль分辨率的图像。许多传统的HSI和MSI混合算法取决于已知的空间退化参数，例如点扩散函数、spectral退化参数、spectral响应函数或其中之一。另一类的深度学习模型需要大量的实验训练图像，且需要高空间分辨率HSI的实验训练图像。这两类模型在实际混合应用场景中有限制。在这篇论文中，我们提出了一种不需要实验训练图像的干扰影像和多spectral影像混合模型，基于循环一致性，称为CycFusion。CycFusion学习了干扰影像低空间分辨率（LrHSI）和高空间分辨率多spectral影像（HrMSI）之间的Domain转换，并考虑了欲有的高空间分辨率HSI作为中间特征图。CycFusion可以通过单个转换和双转换的对应函数来训练，并且可以在干扰影像和多spectral影像之间进行自适应的混合。此外，我们还在模型中嵌入了估计的PSF和SRF，以进一步提高模型的实用性。我们在多个数据集上进行了实验，结果显示，我们的提出的模型在无监督情况下超过所有相比的无监督混合方法。模型代码将在以下地址可用：https: //github.com/shuaikaishi/CycFusion，以便重现。
</details></li>
</ul>
<hr>
<h2 id="Towards-Robust-SDRTV-to-HDRTV-via-Dual-Inverse-Degradation-Network"><a href="#Towards-Robust-SDRTV-to-HDRTV-via-Dual-Inverse-Degradation-Network" class="headerlink" title="Towards Robust SDRTV-to-HDRTV via Dual Inverse Degradation Network"></a>Towards Robust SDRTV-to-HDRTV via Dual Inverse Degradation Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03394">http://arxiv.org/abs/2307.03394</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kepeng Xu, Gang He, Li Xu, Xingchao Yang, Ming Sun, Yuzhi Wang, Zijia Ma, Haoqiang Fan, Xing Wen</li>
<li>for: 提高SDRTV至HDRTV的转换效果，并且解决转换过程中存在的编码痕迹的增强问题。</li>
<li>methods: 提出了一种双 inverse degradation SDRTV-to-HDRTV网络DIDNet，包括时间空间特征对齐模块和双调度卷积，以及波帕特注意模块，以提高颜色恢复能力和编码痕迹除减能力。</li>
<li>results: 与当前状态艺术方法相比，提出的方法在量化结果、视觉质量和推理时间等方面具有显著优势，因此可以在实际应用场景中提高SDRTV至HDRTV的转换效果。<details>
<summary>Abstract</summary>
Recently, the transformation of standard dynamic range TV (SDRTV) to high dynamic range TV (HDRTV) is in high demand due to the scarcity of HDRTV content. However, the conversion of SDRTV to HDRTV often amplifies the existing coding artifacts in SDRTV which deteriorate the visual quality of the output. In this study, we propose a dual inverse degradation SDRTV-to-HDRTV network DIDNet to address the issue of coding artifact restoration in converted HDRTV, which has not been previously studied. Specifically, we propose a temporal-spatial feature alignment module and dual modulation convolution to remove coding artifacts and enhance color restoration ability. Furthermore, a wavelet attention module is proposed to improve SDRTV features in the frequency domain. An auxiliary loss is introduced to decouple the learning process for effectively restoring from dual degradation. The proposed method outperforms the current state-of-the-art method in terms of quantitative results, visual quality, and inference times, thus enhancing the performance of the SDRTV-to-HDRTV method in real-world scenarios.
</details>
<details>
<summary>摘要</summary>
最近，标准动态范围电视（SDRTV）到高动态范围电视（HDRTV）的转换受到了高动态范围内容缺乏的限制。然而，将SDRTV转换为HDRTV经常会加剧SDRTV中存在的编码 artifacts，从而降低输出视质。在这项研究中，我们提出了一个双重逆减 SDRTV-to-HDRTV 网络 DIDNet，以解决转换后 HDRTV 中的编码 artifact 纠正问题，这一问题尚未被研究。特别是，我们提出了时空特征对齐模块和双扩涨 convolution来除除编码 artifacts和提高颜色纠正能力。此外，我们还提出了wavelet 注意力模块，以提高 SDRTV 特征在频域中。还引入了一个 auxillary 损失函数，以分离学习过程中的纠正过程，从而提高 SDRTV-to-HDRTV 方法的实际场景性能。根据量化结果和视觉质量，我们的方法超过了当前状态的艺术方法，从而提高 SDRTV-to-HDRTV 方法的性能。
</details></li>
</ul>
<hr>
<h2 id="CheXmask-a-large-scale-dataset-of-anatomical-segmentation-masks-for-multi-center-chest-x-ray-images"><a href="#CheXmask-a-large-scale-dataset-of-anatomical-segmentation-masks-for-multi-center-chest-x-ray-images" class="headerlink" title="CheXmask: a large-scale dataset of anatomical segmentation masks for multi-center chest x-ray images"></a>CheXmask: a large-scale dataset of anatomical segmentation masks for multi-center chest x-ray images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03293">http://arxiv.org/abs/2307.03293</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ngaggion/chexmask-database">https://github.com/ngaggion/chexmask-database</a></li>
<li>paper_authors: Nicolás Gaggion, Candelaria Mosquera, Lucas Mansilla, Martina Aineseder, Diego H. Milone, Enzo Ferrante</li>
<li>for: 这个研究的目的是为了提供一个大型、多中心的胸部X射影分类数据集，以便帮助开发更好的人工智能模型。</li>
<li>methods: 这个研究使用了HybridGNet模型来确保所有数据集中的分类都是一致的和高品质的。</li>
<li>results: 这个研究产生了676,803个分类掩模，并进行了严格的验证，包括专业医生评价和自动化品质控制，以验证所有掩模的品质。<details>
<summary>Abstract</summary>
The development of successful artificial intelligence models for chest X-ray analysis relies on large, diverse datasets with high-quality annotations. While several databases of chest X-ray images have been released, most include disease diagnosis labels but lack detailed pixel-level anatomical segmentation labels. To address this gap, we introduce an extensive chest X-ray multi-center segmentation dataset with uniform and fine-grain anatomical annotations for images coming from six well-known publicly available databases: CANDID-PTX, ChestX-ray8, Chexpert, MIMIC-CXR-JPG, Padchest, and VinDr-CXR, resulting in 676,803 segmentation masks. Our methodology utilizes the HybridGNet model to ensure consistent and high-quality segmentations across all datasets. Rigorous validation, including expert physician evaluation and automatic quality control, was conducted to validate the resulting masks. Additionally, we provide individualized quality indices per mask and an overall quality estimation per dataset. This dataset serves as a valuable resource for the broader scientific community, streamlining the development and assessment of innovative methodologies in chest X-ray analysis. The CheXmask dataset is publicly available at: \url{https://physionet.org/content/chexmask-cxr-segmentation-data/}.
</details>
<details>
<summary>摘要</summary>
发展成功人工智能模型需要大量多样化的数据集，以便进行胸部X射线分析。虽然一些胸部X射线图像数据库已经发布，但大多数只包含疾病诊断标签，缺乏细致的像素级别解剖标注。为了解决这个问题，我们介绍了一个胸部X射线多中心分割数据集，包括六个公共可用的数据库：CANDID-PTX、ChestX-ray8、Chexpert、MIMIC-CXR-JPG、Padchest和VinDr-CXR，共计676,803个分割mask。我们的方法使用HybridGNet模型，确保分割结果具有一致性和高质量。我们进行了严格的验证，包括专业医生评估和自动化质量控制，以验证结果。此外，我们还提供了每个mask的个性化质量指数和每个数据集的总质量估计。这个数据集对科学社区来说是一个重要的资源，可以促进胸部X射线分析领域的创新和评估。CheXmask数据集可以在以下链接获取：https://physionet.org/content/chexmask-cxr-segmentation-data/。
</details></li>
</ul>
<hr>
<h2 id="ADASSM-Adversarial-Data-Augmentation-in-Statistical-Shape-Models-From-Images"><a href="#ADASSM-Adversarial-Data-Augmentation-in-Statistical-Shape-Models-From-Images" class="headerlink" title="ADASSM: Adversarial Data Augmentation in Statistical Shape Models From Images"></a>ADASSM: Adversarial Data Augmentation in Statistical Shape Models From Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03273">http://arxiv.org/abs/2307.03273</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mokshagna Sai Teja Karanam, Tushar Kataria, Krithika Iyer, Shireen Elhabian</li>
<li>for: 这paper的目的是提出一种基于深度学习的图像到统计形态模型（SSM）框架，以提高图像到SSM的准确率和效率。</li>
<li>methods: 该paper使用了深度学习模型来学习图像中的形态表示，并使用了KDE方法生成形态增强样本以帮助图像到SSM网络实现比较高的准确率。</li>
<li>results: 该paper的实验结果表明，使用该novel strategy可以提高图像到SSM网络的准确率，并且可以避免深度学习模型的图像基于Texture偏好。<details>
<summary>Abstract</summary>
Statistical shape models (SSM) have been well-established as an excellent tool for identifying variations in the morphology of anatomy across the underlying population. Shape models use consistent shape representation across all the samples in a given cohort, which helps to compare shapes and identify the variations that can detect pathologies and help in formulating treatment plans. In medical imaging, computing these shape representations from CT/MRI scans requires time-intensive preprocessing operations, including but not limited to anatomy segmentation annotations, registration, and texture denoising. Deep learning models have demonstrated exceptional capabilities in learning shape representations directly from volumetric images, giving rise to highly effective and efficient Image-to-SSM networks. Nevertheless, these models are data-hungry and due to the limited availability of medical data, deep learning models tend to overfit. Offline data augmentation techniques, that use kernel density estimation based (KDE) methods for generating shape-augmented samples, have successfully aided Image-to-SSM networks in achieving comparable accuracy to traditional SSM methods. However, these augmentation methods focus on shape augmentation, whereas deep learning models exhibit image-based texture bias resulting in sub-optimal models. This paper introduces a novel strategy for on-the-fly data augmentation for the Image-to-SSM framework by leveraging data-dependent noise generation or texture augmentation. The proposed framework is trained as an adversary to the Image-to-SSM network, augmenting diverse and challenging noisy samples. Our approach achieves improved accuracy by encouraging the model to focus on the underlying geometry rather than relying solely on pixel values.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Fully-Automated-and-Explainable-Algorithm-for-the-Prediction-of-Malignant-Transformation-in-Oral-Epithelial-Dysplasia"><a href="#A-Fully-Automated-and-Explainable-Algorithm-for-the-Prediction-of-Malignant-Transformation-in-Oral-Epithelial-Dysplasia" class="headerlink" title="A Fully Automated and Explainable Algorithm for the Prediction of Malignant Transformation in Oral Epithelial Dysplasia"></a>A Fully Automated and Explainable Algorithm for the Prediction of Malignant Transformation in Oral Epithelial Dysplasia</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03757">http://arxiv.org/abs/2307.03757</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adam J Shephard, Raja Muhammad Saad Bashir, Hanya Mahmood, Mostafa Jahanifar, Fayyaz Minhas, Shan E Ahmed Raza, Kris D McCombe, Stephanie G Craig, Jacqueline James, Jill Brooks, Paul Nankivell, Hisham Mehanna, Syed Ali Khurram, Nasir M Rajpoot<br>for: 这个研究的目的是提出一种完全自动化的算法，以便预测口腔细膜癌变的发生，基于可读取的核仁特征。methods: 这个算法使用了自己设计的Segmentation模型，以及一种浅层神经网络，以检测和分类口腔细膜中的核仁。results: 该算法在三个外部数据集上进行了验证，AUROC值为0.74，表明其可以准确预测口腔细膜癌变的发生。此外，Survival分析表明该算法可以预测癌变的发生，并且可以更好地预测比 manually-assigned WHO和二元等级。<details>
<summary>Abstract</summary>
Oral epithelial dysplasia (OED) is a premalignant histopathological diagnosis given to lesions of the oral cavity. Its grading suffers from significant inter-/intra- observer variability, and does not reliably predict malignancy progression, potentially leading to suboptimal treatment decisions. To address this, we developed a novel artificial intelligence algorithm that can assign an Oral Malignant Transformation (OMT) risk score, based on histological patterns in the in Haematoxylin and Eosin stained whole slide images, to quantify the risk of OED progression. The algorithm is based on the detection and segmentation of nuclei within (and around) the epithelium using an in-house segmentation model. We then employed a shallow neural network fed with interpretable morphological/spatial features, emulating histological markers. We conducted internal cross-validation on our development cohort (Sheffield; n = 193 cases) followed by independent validation on two external cohorts (Birmingham and Belfast; n = 92 cases). The proposed OMTscore yields an AUROC = 0.74 in predicting whether an OED progresses to malignancy or not. Survival analyses showed the prognostic value of our OMTscore for predicting malignancy transformation, when compared to the manually-assigned WHO and binary grades. Analysis of the correctly predicted cases elucidated the presence of peri-epithelial and epithelium-infiltrating lymphocytes in the most predictive patches of cases that transformed (p < 0.0001). This is the first study to propose a completely automated algorithm for predicting OED transformation based on interpretable nuclear features, whilst being validated on external datasets. The algorithm shows better-than-human-level performance for prediction of OED malignant transformation and offers a promising solution to the challenges of grading OED in routine clinical practice.
</details>
<details>
<summary>摘要</summary>
口腔粘膜细胞变化（OED）是口腔部位的一种前癌性病理诊断，但其分级存在很大的干扰因素和内部/外部观察者的不一致，不能准确预测癌变进程，可能导致不佳的治疗决策。为了解决这个问题，我们开发了一种新的人工智能算法，可以根据染色体板术影像中的细胞核特征分配口腔癌变转换风险分数（OMT分数），以评估OED转换癌变的风险。这种算法基于检测和分类细胞核的具体方法，使用了我们自己开发的分割模型。然后，我们使用了一个浅层神经网络，以便使用可读性特征来模拟 histological markers。我们在我们的开发组（Sheffield）进行了内部十字验证（n = 193例），然后在两个外部组（Birmingham和Belfast）进行了独立验证（n = 92例）。我们的提出的 OMT 分数可以在预测OED转换癌变是否存在的问题上达到 AUROC = 0.74 的性能。我们的存活分析表明，我们的 OMT 分数具有预测癌变转换的 проGNosis 价值，比较于由人工分配的 WHO 和二分类分数。分析 Correctly 预测的 случаeschannel 显示，在转换的 caso 中存在辐射半径内的卵细胞和细胞核的卫星细胞（p < 0.0001）。这是首个提出一种完全自动化的 OED 转换预测算法，基于可读性的核特征，并在外部数据上进行了验证。这种算法在预测 OED 癌变转换性能上达到了人类水平，并且提供了一个可能的解决方案，以解决 OED 的分级挑战。
</details></li>
</ul>
<hr>
<h2 id="Semantic-Aware-Image-Compressed-Sensing"><a href="#Semantic-Aware-Image-Compressed-Sensing" class="headerlink" title="Semantic-Aware Image Compressed Sensing"></a>Semantic-Aware Image Compressed Sensing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03246">http://arxiv.org/abs/2307.03246</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bowen Zhang, Zhijin Qin, Geoffrey Ye Li</li>
<li>for: 提高图像压缩感知效率</li>
<li>methods: 使用基于深度学习的图像压缩感知系统，并使用策略网络分析图像semantic信息，确定不同图像区域的测量矩阵</li>
<li>results: 提出了一种基于semantic信息的图像压缩感知系统，实现了提高图像压缩感知效率<details>
<summary>Abstract</summary>
Deep learning based image compressed sensing (CS) has achieved great success. However, existing CS systems mainly adopt a fixed measurement matrix to images, ignoring the fact the optimal measurement numbers and bases are different for different images. To further improve the sensing efficiency, we propose a novel semantic-aware image CS system. In our system, the encoder first uses a fixed number of base CS measurements to sense different images. According to the base CS results, the encoder then employs a policy network to analyze the semantic information in images and determines the measurement matrix for different image areas. At the decoder side, a semantic-aware initial reconstruction network is developed to deal with the changes of measurement matrices used at the encoder. A rate-distortion training loss is further introduced to dynamically adjust the average compression ratio for the semantic-aware CS system and the policy network is trained jointly with the encoder and the decoder in an en-to-end manner by using some proxy functions. Numerical results show that the proposed semantic-aware image CS system is superior to the traditional ones with fixed measurement matrices.
</details>
<details>
<summary>摘要</summary>
深度学习基于图像压缩感知（CS）技术已经取得了很大成功。然而，现有的CS系统主要采用固定的测量矩阵来对图像进行测量，忽略了图像的优化测量数量和基准的不同。为了进一步提高感知效率，我们提出了一种新的Semantic-aware图像CS系统。在我们的系统中，Encoder首先使用固定数量的基础CS测量来感知不同的图像。根据基础CS结果，Encoder然后使用一个政策网络分析图像中的Semantic信息，并确定不同的图像区域的测量矩阵。在Decoder端，一个Semantic-aware初始重建网络被开发以处理测量矩阵的变化。此外，我们还引入了一个率-压缩训练损失，以 Dynamically adjust the average compression ratio for the Semantic-aware CS system。Policy网络与Encoder和Decoder在一起训练，使用某些代理函数。numerical results show that the proposed Semantic-aware image CS system is superior to traditional ones with fixed measurement matrices.Note: Please note that the translation is in Simplified Chinese, and some words or phrases may have been translated differently in Traditional Chinese.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/07/eess.IV_2023_07_07/" data-id="cllsj9wzz007cuv883gfu8li7" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_07_06" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/06/cs.LG_2023_07_06/" class="article-date">
  <time datetime="2023-07-05T16:00:00.000Z" itemprop="datePublished">2023-07-06</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/06/cs.LG_2023_07_06/">cs.LG - 2023-07-06 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Learning-Disentangled-Representations-in-Signed-Directed-Graphs-without-Social-Assumptions"><a href="#Learning-Disentangled-Representations-in-Signed-Directed-Graphs-without-Social-Assumptions" class="headerlink" title="Learning Disentangled Representations in Signed Directed Graphs without Social Assumptions"></a>Learning Disentangled Representations in Signed Directed Graphs without Social Assumptions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03077">http://arxiv.org/abs/2307.03077</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/geonwooko/dines">https://github.com/geonwooko/dines</a></li>
<li>paper_authors: Geonwoo Ko, Jinhong Jung</li>
<li>for: 学习签名图的节点表示（node representation learning in signed directed graphs）</li>
<li>methods: 提出了一种新的方法DINES，它采用分离框架，将每个嵌入分解成不同的因素，以捕捉多个潜在因素。还使用了轻量级的图 convolution，不依赖社会理论。</li>
<li>results: 经过广泛的实验，DINES方法在真实世界签名图上效果很好，可以准确预测边的签名。与其他竞争者相比，DINES方法显著超越其他方法。<details>
<summary>Abstract</summary>
Signed graphs are complex systems that represent trust relationships or preferences in various domains. Learning node representations in such graphs is crucial for many mining tasks. Although real-world signed relationships can be influenced by multiple latent factors, most existing methods often oversimplify the modeling of signed relationships by relying on social theories and treating them as simplistic factors. This limits their expressiveness and their ability to capture the diverse factors that shape these relationships. In this paper, we propose DINES, a novel method for learning disentangled node representations in signed directed graphs without social assumptions. We adopt a disentangled framework that separates each embedding into distinct factors, allowing for capturing multiple latent factors. We also explore lightweight graph convolutions that focus solely on sign and direction, without depending on social theories. Additionally, we propose a decoder that effectively classifies an edge's sign by considering correlations between the factors. To further enhance disentanglement, we jointly train a self-supervised factor discriminator with our encoder and decoder. Throughout extensive experiments on real-world signed directed graphs, we show that DINES effectively learns disentangled node representations, and significantly outperforms its competitors in the sign prediction task.
</details>
<details>
<summary>摘要</summary>
签名图是复杂系统，表示信任关系或偏好在不同领域。学习图节表示是挖掘任务中的关键。although real-world签名关系可能受到多种隐藏因素影响，现有方法通常夸大了签名关系的模型化，通过社会理论对它们进行简单化。这限制了它们的表达能力和捕捉签名关系的多样性。在这篇论文中，我们提出了DINES方法，一种学习签名直接图的不同因素分解节点表示方法。我们采用分解框架，将每个嵌入分解成不同因素，以捕捉多个隐藏因素。我们还提出了一种简单的Edge sign类型的预测方法，通过考虑因素之间的相关性来更好地预测签名关系。为了进一步提高分离度，我们同时训练了一个自我超vised因素分类器与我们的编码器和解码器一起。通过对实际签名直接图进行广泛的实验，我们表明DINES方法可以有效地学习分离节点表示，并在签名预测任务中显著超越其竞争对手。
</details></li>
</ul>
<hr>
<h2 id="A-Hybrid-End-to-End-Spatio-Temporal-Attention-Neural-Network-with-Graph-Smooth-Signals-for-EEG-Emotion-Recognition"><a href="#A-Hybrid-End-to-End-Spatio-Temporal-Attention-Neural-Network-with-Graph-Smooth-Signals-for-EEG-Emotion-Recognition" class="headerlink" title="A Hybrid End-to-End Spatio-Temporal Attention Neural Network with Graph-Smooth Signals for EEG Emotion Recognition"></a>A Hybrid End-to-End Spatio-Temporal Attention Neural Network with Graph-Smooth Signals for EEG Emotion Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03068">http://arxiv.org/abs/2307.03068</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shadi Sartipi, Mastaneh Torkamani-Azar, Mujdat Cetin</li>
<li>for: 这篇论文的主要目标是设计一种自动识别情感状态的模型，具体来说是使用深度神经网络来实现。</li>
<li>methods: 这篇论文使用了一种混合结构的空间-时间编码和循环注意力网络块，以及一个预处理步骤使用图像信号处理工具来进行图像平滑处理。</li>
<li>results: 根据DEAP数据集，这种建议的模型的性能超过了当前状态的极性识别结果，并且通过跨Modalities的传输学习（TL）来证明模型的学习是通用的。<details>
<summary>Abstract</summary>
Recently, physiological data such as electroencephalography (EEG) signals have attracted significant attention in affective computing. In this context, the main goal is to design an automated model that can assess emotional states. Lately, deep neural networks have shown promising performance in emotion recognition tasks. However, designing a deep architecture that can extract practical information from raw data is still a challenge. Here, we introduce a deep neural network that acquires interpretable physiological representations by a hybrid structure of spatio-temporal encoding and recurrent attention network blocks. Furthermore, a preprocessing step is applied to the raw data using graph signal processing tools to perform graph smoothing in the spatial domain. We demonstrate that our proposed architecture exceeds state-of-the-art results for emotion classification on the publicly available DEAP dataset. To explore the generality of the learned model, we also evaluate the performance of our architecture towards transfer learning (TL) by transferring the model parameters from a specific source to other target domains. Using DEAP as the source dataset, we demonstrate the effectiveness of our model in performing cross-modality TL and improving emotion classification accuracy on DREAMER and the Emotional English Word (EEWD) datasets, which involve EEG-based emotion classification tasks with different stimuli.
</details>
<details>
<summary>摘要</summary>
近些年，生理数据如电enzephalography（EEG）信号在情感计算中吸引了广泛的关注。在这个上下文中，主要的目标是设计一个自动化的模型，可以评估情感状态。最近，深度神经网络在情感识别任务中表现出了良好的表现。然而，设计一个深度架构，可以从原始数据中提取实用信息，仍然是一个挑战。我们在这里引入了一个深度神经网络，通过混合式空间-时间编码和循环注意网络块来获得可解释的生理学表示。此外，我们对原始数据进行了预处理步骤，使用图像信号处理工具来实现图像平滑处理。我们示出了我们提议的架构可以在公共可用的DEAP数据集上超过状态的报告结果进行情感分类。为了探索学习的一致性，我们还评估了我们学习的模型在转移学习（TL）中的性能。使用DEAP作为源数据集，我们示出了我们模型在跨Modal TL中的效果，并在DREAMER和Emotional English Word（EEWD）数据集上进行了不同的刺激情感分类任务的改进。
</details></li>
</ul>
<hr>
<h2 id="DeepOnto-A-Python-Package-for-Ontology-Engineering-with-Deep-Learning"><a href="#DeepOnto-A-Python-Package-for-Ontology-Engineering-with-Deep-Learning" class="headerlink" title="DeepOnto: A Python Package for Ontology Engineering with Deep Learning"></a>DeepOnto: A Python Package for Ontology Engineering with Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03067">http://arxiv.org/abs/2307.03067</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/KRR-Oxford/DeepOnto">https://github.com/KRR-Oxford/DeepOnto</a></li>
<li>paper_authors: Yuan He, Jiaoyan Chen, Hang Dong, Ian Horrocks, Carlo Allocca, Taehun Kim, Brahmananda Sapkota</li>
<li>for: 本文旨在应用深度学习技术，特别是语言模型（LM），在 ontology engineering 中进行整合和实现。</li>
<li>methods: 本文使用 Python 框架 PyTorch 和 Tensorflow，并与广泛使用的 ontology API 如 OWL API 和 Jena 进行整合。</li>
<li>results: 本文提出了 Deeponto，一个 Python 套件，可以实现 ontology engineering 中的多个任务，包括 ontology 调整和完成，并且可以运用深度学习方法，如预训 LM。在文中还提供了两个实际应用案例：Samsung Research UK 的数位健康教学和 OAEI 的 Bio-ML 追踪。<details>
<summary>Abstract</summary>
Applying deep learning techniques, particularly language models (LMs), in ontology engineering has raised widespread attention. However, deep learning frameworks like PyTorch and Tensorflow are predominantly developed for Python programming, while widely-used ontology APIs, such as the OWL API and Jena, are primarily Java-based. To facilitate seamless integration of these frameworks and APIs, we present Deeponto, a Python package designed for ontology engineering. The package encompasses a core ontology processing module founded on the widely-recognised and reliable OWL API, encapsulating its fundamental features in a more "Pythonic" manner and extending its capabilities to include other essential components including reasoning, verbalisation, normalisation, projection, and more. Building on this module, Deeponto offers a suite of tools, resources, and algorithms that support various ontology engineering tasks, such as ontology alignment and completion, by harnessing deep learning methodologies, primarily pre-trained LMs. In this paper, we also demonstrate the practical utility of Deeponto through two use-cases: the Digital Health Coaching in Samsung Research UK and the Bio-ML track of the Ontology Alignment Evaluation Initiative (OAEI).
</details>
<details>
<summary>摘要</summary>
使用深度学习技术，特别是语言模型（LM），在onto工程中引起了广泛的关注。然而，深度学习框架如PyTorch和Tensorflow主要为Python编程语言开发，而广泛使用onto API，如OWL API和Jena，是主要基于Java编程语言。为了实现这些框架和API的无缝集成，我们提出了Deeponto，一个Python包用于onto工程。该包包括一个核心onto处理模块，基于广泛认可和可靠的OWL API，将其主要特征封装在更"Pythonic"的方式，并将其扩展到包括其他重要组成部分，如理解、词法、正规化、投影等。在这个模块基础之上，Deeponto提供了一组工具、资源和算法，支持多种onto工程任务，如onto对齐和完成，通过利用深度学习方法，主要是预训练LM。在这篇论文中，我们还通过两个使用案例，分别是Samsung Research UK的数字健康帮助和OAEI的生物ML轨道，证明Deeponto的实用性。
</details></li>
</ul>
<hr>
<h2 id="Generalizing-Backpropagation-for-Gradient-Based-Interpretability"><a href="#Generalizing-Backpropagation-for-Gradient-Based-Interpretability" class="headerlink" title="Generalizing Backpropagation for Gradient-Based Interpretability"></a>Generalizing Backpropagation for Gradient-Based Interpretability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03056">http://arxiv.org/abs/2307.03056</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kdu4108/semiring-backprop-exps">https://github.com/kdu4108/semiring-backprop-exps</a></li>
<li>paper_authors: Kevin Du, Lucas Torroba Hennigen, Niklas Stoehr, Alexander Warstadt, Ryan Cotterell</li>
<li>for: 本研究旨在提高深度神经网络的解释能力，通过计算模型的输出对应输入的梯度来解释模型的工作机制。</li>
<li>methods: 本研究使用semiring来扩展归档分析方法，从而计算出模型的梯度图的其他可解释统计量，如最大权重路径和熵。</li>
<li>results: 通过synthetic数据和BERT模型进行实验，研究发现：(a) 模型中组件的梯度流量反映该组件对预测的重要性，(b) SVA任务中自动注意力机制的特定路径对模型的预测具有重要性。<details>
<summary>Abstract</summary>
Many popular feature-attribution methods for interpreting deep neural networks rely on computing the gradients of a model's output with respect to its inputs. While these methods can indicate which input features may be important for the model's prediction, they reveal little about the inner workings of the model itself. In this paper, we observe that the gradient computation of a model is a special case of a more general formulation using semirings. This observation allows us to generalize the backpropagation algorithm to efficiently compute other interpretable statistics about the gradient graph of a neural network, such as the highest-weighted path and entropy. We implement this generalized algorithm, evaluate it on synthetic datasets to better understand the statistics it computes, and apply it to study BERT's behavior on the subject-verb number agreement task (SVA). With this method, we (a) validate that the amount of gradient flow through a component of a model reflects its importance to a prediction and (b) for SVA, identify which pathways of the self-attention mechanism are most important.
</details>
<details>
<summary>摘要</summary>
很多流行的特征归因方法用于解释深度神经网络，都是通过计算模型输出与输入之间的梯度来实现的。虽然这些方法可以指示模型预测中哪些输入特征是重要的，但它们对模型自身的工作机制提供的信息很少。在这篇论文中，我们发现了一种使用半群的形式ulation来计算模型的梯度。这一发现使得我们可以扩展backpropagation算法，以计算模型梯度图的其他可解释统计，例如最大重量路径和熵。我们实现了这种扩展后的算法，在synthetic数据上进行了更好的理解这些统计的测试，并将其应用于研究BERT在主语-谓语数目协调任务（SVA）中的行为。通过这种方法，我们（a）验证了模型中组件的重要性与预测中的梯度流量相对应，（b） для SVA，找到了自注意机制中最重要的路径。
</details></li>
</ul>
<hr>
<h2 id="Origin-Destination-Travel-Time-Oracle-for-Map-based-Services"><a href="#Origin-Destination-Travel-Time-Oracle-for-Map-based-Services" class="headerlink" title="Origin-Destination Travel Time Oracle for Map-based Services"></a>Origin-Destination Travel Time Oracle for Map-based Services</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03048">http://arxiv.org/abs/2307.03048</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yan Lin, Huaiyu Wan, Jilin Hu, Shengnan Guo, Bin Yang, Youfang Lin, Christian S. Jensen</li>
<li>for: 这个论文的目的是提出一种基于历史轨迹的Origin-Destination（OD）旅行时间估计（TTE）解决方案，以便构建OD旅行时间估计 oracle。</li>
<li>methods: 该解决方案基于一个两阶段框架，包括一个 conditioned Pixelated Trajectories（PiT）denoiser 和一个Masked Vision Transformer（MViT）。denoiser 通过学习OD对历史轨迹的相关性，建立了一个 diffusion-based PiT 推理过程；MViT 可以快速和高效地根据推理出的 PiT 估计旅行时间。</li>
<li>results:  experiments 表明，相比基eline方法，DOT 能够在精度、可扩展性和可解释性三个方面取得更好的性能。<details>
<summary>Abstract</summary>
Given an origin (O), a destination (D), and a departure time (T), an Origin-Destination (OD) travel time oracle~(ODT-Oracle) returns an estimate of the time it takes to travel from O to D when departing at T. ODT-Oracles serve important purposes in map-based services. To enable the construction of such oracles, we provide a travel-time estimation (TTE) solution that leverages historical trajectories to estimate time-varying travel times for OD pairs.   The problem is complicated by the fact that multiple historical trajectories with different travel times may connect an OD pair, while trajectories may vary from one another. To solve the problem, it is crucial to remove outlier trajectories when doing travel time estimation for future queries.   We propose a novel, two-stage framework called Diffusion-based Origin-destination Travel Time Estimation (DOT), that solves the problem. First, DOT employs a conditioned Pixelated Trajectories (PiT) denoiser that enables building a diffusion-based PiT inference process by learning correlations between OD pairs and historical trajectories. Specifically, given an OD pair and a departure time, we aim to infer a PiT. Next, DOT encompasses a Masked Vision Transformer~(MViT) that effectively and efficiently estimates a travel time based on the inferred PiT. We report on extensive experiments on two real-world datasets that offer evidence that DOT is capable of outperforming baseline methods in terms of accuracy, scalability, and explainability.
</details>
<details>
<summary>摘要</summary>
Given an origin (O), a destination (D), and a departure time (T), an Origin-Destination (OD) travel time oracle (ODT-Oracle) returns an estimate of the time it takes to travel from O to D when departing at T. ODT-Oracles serve important purposes in map-based services. To enable the construction of such oracles, we provide a travel-time estimation (TTE) solution that leverages historical trajectories to estimate time-varying travel times for OD pairs.  The problem is complicated by the fact that multiple historical trajectories with different travel times may connect an OD pair, while trajectories may vary from one another. To solve the problem, it is crucial to remove outlier trajectories when doing travel time estimation for future queries.   We propose a novel, two-stage framework called Diffusion-based Origin-destination Travel Time Estimation (DOT), that solves the problem. First, DOT employs a conditioned Pixelated Trajectories (PiT) denoiser that enables building a diffusion-based PiT inference process by learning correlations between OD pairs and historical trajectories. Specifically, given an OD pair and a departure time, we aim to infer a PiT. Next, DOT encompasses a Masked Vision Transformer~(MViT) that effectively and efficiently estimates a travel time based on the inferred PiT. We report on extensive experiments on two real-world datasets that offer evidence that DOT is capable of outperforming baseline methods in terms of accuracy, scalability, and explainability.
</details></li>
</ul>
<hr>
<h2 id="Track-Mix-Generation-on-Music-Streaming-Services-using-Transformers"><a href="#Track-Mix-Generation-on-Music-Streaming-Services-using-Transformers" class="headerlink" title="Track Mix Generation on Music Streaming Services using Transformers"></a>Track Mix Generation on Music Streaming Services using Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03045">http://arxiv.org/abs/2307.03045</a></li>
<li>repo_url: None</li>
<li>paper_authors: Walid Bendada, Théo Bontempelli, Mathieu Morlon, Benjamin Chapus, Thibault Cador, Thomas Bouabça, Guillaume Salha-Galvan</li>
<li>for: 这篇论文描述了一个名为Track Mix的个性化播放列表生成系统，于2022年在音乐流媒体服务Deezer上发布。这个系统可以根据初始音乐曲目自动生成“混合”播放列表，让用户找到与他们喜爱的音乐类似的内容。</li>
<li>methods: 为生成这些混合，我们使用了一个基于Transformer模型的方法，该模型在用户播放列表中处理了百万个轨迹序列。我们还对使用Transformer模型进行混合生成的优势、缺点和技术挑战进行分析，并与传统的合作推荐方法进行比较。</li>
<li>results:  desde su lanzamiento, Track Mix ha generado listas de reproducción diarias para millones de usuarios en Deezer, mejorando su experiencia de descubrimiento de música en la plataforma.<details>
<summary>Abstract</summary>
This paper introduces Track Mix, a personalized playlist generation system released in 2022 on the music streaming service Deezer. Track Mix automatically generates "mix" playlists inspired by initial music tracks, allowing users to discover music similar to their favorite content. To generate these mixes, we consider a Transformer model trained on millions of track sequences from user playlists. In light of the growing popularity of Transformers in recent years, we analyze the advantages, drawbacks, and technical challenges of using such a model for mix generation on the service, compared to a more traditional collaborative filtering approach. Since its release, Track Mix has been generating playlists for millions of users daily, enhancing their music discovery experience on Deezer.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了Deezer音乐流媒体服务于2022年发布的个性化播放列表生成系统Track Mix。Track Mix自动生成基于初始音乐曲目的"混合"播放列表，让用户发现与自己喜爱的音乐相似的歌曲。为生成这些混合，我们考虑了基于千万个轨迹序列的Transformer模型。在过去几年内，Transformers的普及程度在不断增长，我们对使用这种模型进行混合生成在服务上的优势、缺点和技术挑战进行分析，并与传统的共同推荐方法进行比较。自其发布以来，Track Mix每天为数百万用户生成播放列表，提高了Deezer音乐发现体验。Note: Please note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="A-Near-Linear-Time-Algorithm-for-the-Chamfer-Distance"><a href="#A-Near-Linear-Time-Algorithm-for-the-Chamfer-Distance" class="headerlink" title="A Near-Linear Time Algorithm for the Chamfer Distance"></a>A Near-Linear Time Algorithm for the Chamfer Distance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03043">http://arxiv.org/abs/2307.03043</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ainesh Bakshi, Piotr Indyk, Rajesh Jayaram, Sandeep Silwal, Erik Waingarten</li>
<li>for: 点云集 $A,B \subset \mathbb{R}^d$ 的 Chamfer 距离</li>
<li>methods: 使用 $O(d n^2)$-时间简单扫描算法</li>
<li>results: 提出了首个 $(1+\epsilon)$-近似算法，运行时间为 $O(nd \log (n)&#x2F;\varepsilon^2)$，并且可实现。实验表明其准确且快速于大高维数据集。<details>
<summary>Abstract</summary>
For any two point sets $A,B \subset \mathbb{R}^d$ of size up to $n$, the Chamfer distance from $A$ to $B$ is defined as $\text{CH}(A,B)=\sum_{a \in A} \min_{b \in B} d_X(a,b)$, where $d_X$ is the underlying distance measure (e.g., the Euclidean or Manhattan distance). The Chamfer distance is a popular measure of dissimilarity between point clouds, used in many machine learning, computer vision, and graphics applications, and admits a straightforward $O(d n^2)$-time brute force algorithm. Further, the Chamfer distance is often used as a proxy for the more computationally demanding Earth-Mover (Optimal Transport) Distance. However, the \emph{quadratic} dependence on $n$ in the running time makes the naive approach intractable for large datasets.   We overcome this bottleneck and present the first $(1+\epsilon)$-approximate algorithm for estimating the Chamfer distance with a near-linear running time. Specifically, our algorithm runs in time $O(nd \log (n)/\varepsilon^2)$ and is implementable. Our experiments demonstrate that it is both accurate and fast on large high-dimensional datasets. We believe that our algorithm will open new avenues for analyzing large high-dimensional point clouds. We also give evidence that if the goal is to \emph{report} a $(1+\varepsilon)$-approximate mapping from $A$ to $B$ (as opposed to just its value), then any sub-quadratic time algorithm is unlikely to exist.
</details>
<details>
<summary>摘要</summary>
For any two point sets $A,B \subset \mathbb{R}^d$ of size up to $n$, the Chamfer distance from $A$ to $B$ is defined as $\text{CH}(A,B)=\sum_{a \in A} \min_{b \in B} d_X(a,b)$, where $d_X$ is the underlying distance measure (e.g., the Euclidean or Manhattan distance). The Chamfer distance is a popular measure of dissimilarity between point clouds, used in many machine learning, computer vision, and graphics applications, and admits a straightforward $O(d n^2)$-time brute force algorithm. However, the \emph{quadratic} dependence on $n$ in the running time makes the naive approach intractable for large datasets.We overcome this bottleneck and present the first $(1+\epsilon)$-approximate algorithm for estimating the Chamfer distance with a near-linear running time. Specifically, our algorithm runs in time $O(nd \log (n)/\varepsilon^2)$ and is implementable. Our experiments demonstrate that it is both accurate and fast on large high-dimensional datasets. We believe that our algorithm will open new avenues for analyzing large high-dimensional point clouds.In addition, we show that if the goal is to \emph{report} a $(1+\varepsilon)$-approximate mapping from $A$ to $B$ (as opposed to just its value), then any sub-quadratic time algorithm is unlikely to exist.
</details></li>
</ul>
<hr>
<h2 id="Parameter-Efficient-Fine-Tuning-of-LLaMA-for-the-Clinical-Domain"><a href="#Parameter-Efficient-Fine-Tuning-of-LLaMA-for-the-Clinical-Domain" class="headerlink" title="Parameter-Efficient Fine-Tuning of LLaMA for the Clinical Domain"></a>Parameter-Efficient Fine-Tuning of LLaMA for the Clinical Domain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03042">http://arxiv.org/abs/2307.03042</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aryo Pradipta Gema, Luke Daines, Pasquale Minervini, Beatrice Alex</li>
<li>for: 这篇论文的目的是如何将预训语言模型（LLaMA）应用到医疗领域中，以提高模型在这个领域的性能。</li>
<li>methods: 这篇论文使用了一种叫做Parameter-Efficient Fine-Tuning（PEFT）的技术，将预训语言模型中的一小部分parameters进行精确地微调整，以降低在领域适应中的计算需求。</li>
<li>results: 这篇论文的结果显示，使用PEFT技术和医疗领域的资料集进行微调整，可以实现与专门医疗语言模型相比的竞争水平，并且在大规模多类别标签分类任务中实现了6-9%的AUROC分数提升。<details>
<summary>Abstract</summary>
Adapting pretrained language models to novel domains, such as clinical applications, traditionally involves retraining their entire set of parameters. However, this approach is increasingly proven to be impractical owing to the substantial computational requirements associated with training such large language models. To address this issue, Parameter-Efficient Fine-Tuning (PEFT) techniques offer a viable solution by selectively fine-tuning a small subset of additional parameters, significantly reducing the computational requirements for domain adaptation. In this study, we propose Clinical LLaMA-LoRA, a PEFT adapter layer built upon the open-sourced LLaMA model. Clinical LLaMA-LoRA is trained using clinical notes obtained from the MIMIC-IV database, thereby creating a specialised adapter designed for the clinical domain. Additionally, we propose a two-step PEFT framework which fuses Clinical LLaMA-LoRA with Downstream LLaMA-LoRA, another PEFT adapter specialised for downstream tasks. We evaluate this framework on multiple clinical outcome prediction datasets, comparing it to clinically trained language models. Our proposed framework achieves a state-of-the-art AUROC score averaged across all clinical downstream tasks. We observe substantial improvements of 6-9% AUROC score in the large-scale multilabel classification tasks, such as diagnoses and procedures classification.
</details>
<details>
<summary>摘要</summary>
traditional clinical applications中的适应措施通常是 retrained整个语言模型的参数集。但这种方法随着语言模型的大小增长，所需的计算资源增长得非常快，已经成为一种实际难以进行的。为解决这个问题，Parameter-Efficient Fine-Tuning（PEFT）技术提供了一个可行的解决方案，通过选择ively fine-tune一小部分的参数，大幅降低了适应domain的计算资源需求。在这个研究中，我们提出了Clinical LLaMA-LoRA，一个基于开源的LLaMA模型的PEFT适应层。Clinical LLaMA-LoRA通过使用来自MIMIC-IV数据库的临床笔记进行训练，创造了一个特殊的适应器，专门适用于临床领域。此外，我们还提出了一个两步PEFT框架，将Clinical LLaMA-LoRA与Downstream LLaMA-LoRA，另一个特殊的PEFT适应器，融合在一起。我们对多个临床结果预测任务进行评估，与临床训练的语言模型进行比较。我们的提议框架在所有临床下游任务中获得了状态控制的AUROC分数。我们发现在大规模多标签分类任务中，如诊断和处方分类，有显著的提高，AUROC分数提高了6-9%。
</details></li>
</ul>
<hr>
<h2 id="FITS-Modeling-Time-Series-with-10k-Parameters"><a href="#FITS-Modeling-Time-Series-with-10k-Parameters" class="headerlink" title="FITS: Modeling Time Series with $10k$ Parameters"></a>FITS: Modeling Time Series with $10k$ Parameters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03756">http://arxiv.org/abs/2307.03756</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhijian Xu, Ailing Zeng, Qiang Xu</li>
<li>for: 本研究推出了一种名为FITS的轻量级时间序列分析模型。</li>
<li>methods: FITS模型基于时间频谱预测的原则，通过抛弃高频组分来实现与状态艺术模型相同的性能，但具有只有约10k参数的极其紧凑的体积。</li>
<li>results: FITS模型可以实现时间序列预测和异常检测任务的性能，而且可以轻松地在边缘设备上训练和部署。<details>
<summary>Abstract</summary>
In this paper, we introduce FITS, a lightweight yet powerful model for time series analysis. Unlike existing models that directly process raw time-domain data, FITS operates on the principle that time series can be manipulated through interpolation in the complex frequency domain. By discarding high-frequency components with negligible impact on time series data, FITS achieves performance comparable to state-of-the-art models for time series forecasting and anomaly detection tasks, while having a remarkably compact size of only approximately $10k$ parameters. Such a lightweight model can be easily trained and deployed in edge devices, creating opportunities for various applications. The anonymous code repo is available in: \url{https://anonymous.4open.science/r/FITS}
</details>
<details>
<summary>摘要</summary>
“在这篇论文中，我们介绍了FITS模型，它是一种轻量级却强大的时间序列分析模型。与现有模型不同，FITS在假设时间序列可以通过在复杂频率域中进行 interpolate 来处理时间序列数据。通过抛弃具有negligible影响的高频组件，FITS可以达到与当前领先模型相当的性能水平，而且只有约10k个参数。这种轻量级的模型可以轻松地在边缘设备中训练和部署，开创了许多应用场景。代码存储库可以在以下链接中找到：https://anonymous.4open.science/r/FITS”
</details></li>
</ul>
<hr>
<h2 id="PCL-Indexability-and-Whittle-Index-for-Restless-Bandits-with-General-Observation-Models"><a href="#PCL-Indexability-and-Whittle-Index-for-Restless-Bandits-with-General-Observation-Models" class="headerlink" title="PCL-Indexability and Whittle Index for Restless Bandits with General Observation Models"></a>PCL-Indexability and Whittle Index for Restless Bandits with General Observation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03034">http://arxiv.org/abs/2307.03034</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keqin Liu, Chengzhong Zhang</li>
<li>for: 本 paper 考虑了一种通用观察模型，用于处理不稳定的多臂投机问题。player需要基于certain feedback机制进行操作，但这些feedback机制可能受到资源约束或环境噪音等因素的影响，导致feedback错误。</li>
<li>methods: 作者采用了一种概率模型来描述反馈&#x2F;观察动态，并将问题转化为一个无穷状态问题。使用了可achivable region方法和partial conservation law（PCL）分析问题的 indextability和优先级指数（Whittle指数）。</li>
<li>results: 作者提出了一种近似过程，将问题转化为一个可以使用AG算法（Ni~no-Mora和Bertsimas）解决的Finite-state问题。实验显示，作者的算法在性能上表现非常出色。<details>
<summary>Abstract</summary>
In this paper, we consider a general observation model for restless multi-armed bandit problems. The operation of the player needs to be based on certain feedback mechanism that is error-prone due to resource constraints or environmental or intrinsic noises. By establishing a general probabilistic model for dynamics of feedback/observation, we formulate the problem as a restless bandit with a countable belief state space starting from an arbitrary initial belief (a priori information). We apply the achievable region method with partial conservation law (PCL) to the infinite-state problem and analyze its indexability and priority index (Whittle index). Finally, we propose an approximation process to transform the problem into which the AG algorithm of Ni\~no-Mora and Bertsimas for finite-state problems can be applied to. Numerical experiments show that our algorithm has an excellent performance.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们考虑了一种通用的观察模型 для无休multi-armed bandit问题。玩家的操作需要基于一定的反馈机制，由于资源限制或环境或内在噪声而存在错误。通过建立一个通用的概率模型 для反馈/观察动态，我们将问题转化为一个无限状态的restless bandit问题，从初始信息开始。我们使用可 achievable region方法和部分保存法（PCL）分析问题的指数性和优先级指标（Whittle指标）。最后，我们提出一种简化过程，使得可以将问题转化为一个有限状态的问题，并应用Ni\~no-Mora和Bertsimas的AG算法。实验表明，我们的算法具有出色的性能。
</details></li>
</ul>
<hr>
<h2 id="PseudoCell-Hard-Negative-Mining-as-Pseudo-Labeling-for-Deep-Learning-Based-Centroblast-Cell-Detection"><a href="#PseudoCell-Hard-Negative-Mining-as-Pseudo-Labeling-for-Deep-Learning-Based-Centroblast-Cell-Detection" class="headerlink" title="PseudoCell: Hard Negative Mining as Pseudo Labeling for Deep Learning-Based Centroblast Cell Detection"></a>PseudoCell: Hard Negative Mining as Pseudo Labeling for Deep Learning-Based Centroblast Cell Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03211">http://arxiv.org/abs/2307.03211</a></li>
<li>repo_url: None</li>
<li>paper_authors: Narongrid Seesawad, Piyalitt Ittichaiwong, Thapanun Sudhawiyangkul, Phattarapong Sawangjai, Peti Thuwajit, Paisarn Boonsakan, Supasan Sripodok, Kanyakorn Veerakanjana, Phoomraphee Luenam, Komgrid Charngkaew, Ananya Pongpaibul, Napat Angkathunyakul, Narit Hnoohom, Sumeth Yuenyong, Chanitra Thuwajit, Theerawit Wilaiprasitporn</li>
<li>for: 帮助病理学家在染色体检查中快速屏除非中blast细胞，提高病理诊断效率。</li>
<li>methods: 使用深度学习模型自动检测中blast细胞，并结合实际病理医生提供的中blast标注数据和 pseudo-负标注数据（基于假阳性预测结果中的细胞形态特征），以提高检测精度。</li>
<li>results: 在实验中， PseudoCell 可以减少病理医生的工作负担，准确地将注意力集中在需要的区域上，并可以根据信任值进行精细的区域屏除。在不同的信任值下， PseudoCell 可以消除58.18%-99.35%的非中blast组织区域。这种方法可以帮助病理医生更快速地完成检查，不需要精心标注数据进行改进。<details>
<summary>Abstract</summary>
Patch classification models based on deep learning have been utilized in whole-slide images (WSI) of H&E-stained tissue samples to assist pathologists in grading follicular lymphoma patients. However, these approaches still require pathologists to manually identify centroblast cells and provide refined labels for optimal performance. To address this, we propose PseudoCell, an object detection framework to automate centroblast detection in WSI (source code is available at https://github.com/IoBT-VISTEC/PseudoCell.git). This framework incorporates centroblast labels from pathologists and combines them with pseudo-negative labels obtained from undersampled false-positive predictions using the cell's morphological features. By employing PseudoCell, pathologists' workload can be reduced as it accurately narrows down the areas requiring their attention during examining tissue. Depending on the confidence threshold, PseudoCell can eliminate 58.18-99.35% of non-centroblasts tissue areas on WSI. This study presents a practical centroblast prescreening method that does not require pathologists' refined labels for improvement. Detailed guidance on the practical implementation of PseudoCell is provided in the discussion section.
</details>
<details>
<summary>摘要</summary>
报告中的文本翻译为简化中文：深度学习模型在染色质 immagini (WSI) 中的报告中使用了报告用于报告评估抗体混合癌症患者。然而，这些方法仍然需要病理医生手动标识中心blast细胞和提供高级标签以实现最佳性能。为了解决这个问题，我们提出了 PseudoCell，一个对象检测框架，可以自动检测WSI中的中心blast细胞。这个框架利用病理医生提供的中心blast标签和基于细胞形态特征的 pseudo-negative 标签进行组合。通过使用 PseudoCell，病理医生的工作负担可以减少，因为它可以准确地将病理医生的注意力集中在WSI中需要注意的区域上。根据信任阈值，PseudoCell可以从WSI中消除58.18-99.35%的非中心blast区域。这项研究提出了一种实用的中心blast预选方法，不需要病理医生的高级标签进行改进。详细的实现 PseudoCell 的指导在讨论部分中提供。
</details></li>
</ul>
<hr>
<h2 id="Improving-Retrieval-Augmented-Large-Language-Models-via-Data-Importance-Learning"><a href="#Improving-Retrieval-Augmented-Large-Language-Models-via-Data-Importance-Learning" class="headerlink" title="Improving Retrieval-Augmented Large Language Models via Data Importance Learning"></a>Improving Retrieval-Augmented Large Language Models via Data Importance Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03027">http://arxiv.org/abs/2307.03027</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/amsterdata/ragbooster">https://github.com/amsterdata/ragbooster</a></li>
<li>paper_authors: Xiaozhong Lyu, Stefan Grafberger, Samantha Biegel, Shaopeng Wei, Meng Cao, Sebastian Schelter, Ce Zhang</li>
<li>for: 提高大语言模型的性能，无需进行进一步训练。</li>
<li>methods: 使用多线性扩展算法计算数据重要性，并提出（ε，δ）优化算法。</li>
<li>results: 可以提高大语言模型的性能，只需对搜索结果进行排重或重新权重，而无需进行进一步训练。在某些任务上，可以使一个小型模型（如GPT-JT），通过与搜索引擎API结合，超越GPT-3.5（无 Retrieval增强）。此外，我们还证明了在实践中可以有效计算多线性扩展的 weights（例如，在100万元素的数据集上只需几分钟时间）。<details>
<summary>Abstract</summary>
Retrieval augmentation enables large language models to take advantage of external knowledge, for example on tasks like question answering and data imputation. However, the performance of such retrieval-augmented models is limited by the data quality of their underlying retrieval corpus. In this paper, we propose an algorithm based on multilinear extension for evaluating the data importance of retrieved data points. There are exponentially many terms in the multilinear extension, and one key contribution of this paper is a polynomial time algorithm that computes exactly, given a retrieval-augmented model with an additive utility function and a validation set, the data importance of data points in the retrieval corpus using the multilinear extension of the model's utility function. We further proposed an even more efficient ({\epsilon}, {\delta})-approximation algorithm. Our experimental results illustrate that we can enhance the performance of large language models by only pruning or reweighting the retrieval corpus, without requiring further training. For some tasks, this even allows a small model (e.g., GPT-JT), augmented with a search engine API, to outperform GPT-3.5 (without retrieval augmentation). Moreover, we show that weights based on multilinear extension can be computed efficiently in practice (e.g., in less than ten minutes for a corpus with 100 million elements).
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Sparse-Graphical-Linear-Dynamical-Systems"><a href="#Sparse-Graphical-Linear-Dynamical-Systems" class="headerlink" title="Sparse Graphical Linear Dynamical Systems"></a>Sparse Graphical Linear Dynamical Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03210">http://arxiv.org/abs/2307.03210</a></li>
<li>repo_url: None</li>
<li>paper_authors: Emilie Chouzenoux, Victor Elvira</li>
<li>for: 这篇论文主要针对时间序列数据的研究，尤其是状态空间模型（SSM）的参数估计问题。</li>
<li>methods: 本文提出了一种新的图像模型框架，它结合了统计图像模型（graphical Lasso）和 causal-based图像模型（graphical Granger），以便 simultanously incorporating static and dynamic graphical information within the context of SSMs。</li>
<li>results: 实验 validate了提出的方法，并表明其可以有效地处理实际时间序列数据。<details>
<summary>Abstract</summary>
Time-series datasets are central in numerous fields of science and engineering, such as biomedicine, Earth observation, and network analysis. Extensive research exists on state-space models (SSMs), which are powerful mathematical tools that allow for probabilistic and interpretable learning on time series. Estimating the model parameters in SSMs is arguably one of the most complicated tasks, and the inclusion of prior knowledge is known to both ease the interpretation but also to complicate the inferential tasks. Very recent works have attempted to incorporate a graphical perspective on some of those model parameters, but they present notable limitations that this work addresses. More generally, existing graphical modeling tools are designed to incorporate either static information, focusing on statistical dependencies among independent random variables (e.g., graphical Lasso approach), or dynamic information, emphasizing causal relationships among time series samples (e.g., graphical Granger approaches). However, there are no joint approaches combining static and dynamic graphical modeling within the context of SSMs. This work proposes a novel approach to fill this gap by introducing a joint graphical modeling framework that bridges the static graphical Lasso model and a causal-based graphical approach for the linear-Gaussian SSM. We present DGLASSO (Dynamic Graphical Lasso), a new inference method within this framework that implements an efficient block alternating majorization-minimization algorithm. The algorithm's convergence is established by departing from modern tools from nonlinear analysis. Experimental validation on synthetic and real weather variability data showcases the effectiveness of the proposed model and inference algorithm.
</details>
<details>
<summary>摘要</summary>
时间序列数据在各个科学和工程领域中具有重要地位，如生物医学、地球观测和网络分析。现有广泛的研究探讨状态空间模型（SSM），它们是数学工具的强大工具，可以在时间序列上进行概率性和可解释的学习。估计SSM模型参数是最复杂的任务之一，并且包含先验知识可以使解释更加容易，但同时也会增加推理任务的复杂性。最近几年的研究尝试将一些模型参数视为图形上的变量，但它们存在一些限制。这种工作的目的是填补这个空白，并提出一种新的 JOINT 图形模型框架，该框架结合了统计依赖关系和 causal 关系，并将其应用于线性加 Gaussian SSM 中。我们提出了一种新的推理方法，称为 DGLASSO（动态图形lasso），它使用一种高效的块 Alternating Majorization-Minimization 算法。我们证明了该算法的收敛性，并通过使用现代非线性分析工具。实验 validate 在 sintetic 和实际气象异常数据上，显示了我们提出的模型和推理算法的效果。
</details></li>
</ul>
<hr>
<h2 id="Improving-the-Efficiency-of-Human-in-the-Loop-Systems-Adding-Artificial-to-Human-Experts"><a href="#Improving-the-Efficiency-of-Human-in-the-Loop-Systems-Adding-Artificial-to-Human-Experts" class="headerlink" title="Improving the Efficiency of Human-in-the-Loop Systems: Adding Artificial to Human Experts"></a>Improving the Efficiency of Human-in-the-Loop Systems: Adding Artificial to Human Experts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03003">http://arxiv.org/abs/2307.03003</a></li>
<li>repo_url: None</li>
<li>paper_authors: Johannes Jakubik, Daniel Weber, Patrick Hemmer, Michael Vössing, Gerhard Satzger</li>
<li>for: 这篇论文旨在提高人工智能和机器学习模型之间的合作，以增加数据的值得提取。</li>
<li>methods: 这篇论文使用了人类在循环中（HITL）的扩展，将困难分类的数据交给人类审核。但是，这种方法需要大量的人力投入，导致资源的浪费。因此，这篇论文提出了一个混合系统，可以将人类审核的知识传授给人工智能。</li>
<li>results: 这篇论文的实验结果显示，该方法可以比传统HITL系统更高效地处理数据分类 задачі。<details>
<summary>Abstract</summary>
Information systems increasingly leverage artificial intelligence (AI) and machine learning (ML) to generate value from vast amounts of data. However, ML models are imperfect and can generate incorrect classifications. Hence, human-in-the-loop (HITL) extensions to ML models add a human review for instances that are difficult to classify. This study argues that continuously relying on human experts to handle difficult model classifications leads to a strong increase in human effort, which strains limited resources. To address this issue, we propose a hybrid system that creates artificial experts that learn to classify data instances from unknown classes previously reviewed by human experts. Our hybrid system assesses which artificial expert is suitable for classifying an instance from an unknown class and automatically assigns it. Over time, this reduces human effort and increases the efficiency of the system. Our experiments demonstrate that our approach outperforms traditional HITL systems for several benchmarks on image classification.
</details>
<details>
<summary>摘要</summary>
信息系统越来越利用人工智能（AI）和机器学习（ML）来生成价值从大量数据中。然而，ML模型不完美，可能会生成错误的分类。因此，人类在循环（HITL）扩展对ML模型进行人工审核，以便对难以分类的实例进行人类审核。这项研究认为，不断依赖于人类专家来处理困难分类会导致人力劳累，占用有限资源。为解决这个问题，我们提议一种混合系统，创建人工专家，从人类专家之前未知的类型中学习分类数据实例。我们的混合系统根据不同的人工专家选择适合分类某个实例，并自动分配。随着时间的推移，这将减少人力劳累，提高系统的效率。我们的实验表明，我们的方法在多个图像分类benchmark上表现出色。
</details></li>
</ul>
<hr>
<h2 id="ContainerGym-A-Real-World-Reinforcement-Learning-Benchmark-for-Resource-Allocation"><a href="#ContainerGym-A-Real-World-Reinforcement-Learning-Benchmark-for-Resource-Allocation" class="headerlink" title="ContainerGym: A Real-World Reinforcement Learning Benchmark for Resource Allocation"></a>ContainerGym: A Real-World Reinforcement Learning Benchmark for Resource Allocation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02991">http://arxiv.org/abs/2307.02991</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pendu/containergym">https://github.com/pendu/containergym</a></li>
<li>paper_authors: Abhijeet Pendyala, Justin Dettmer, Tobias Glasmachers, Asma Atamna</li>
<li>for: 本研究的目的是提出一个基于现实世界工业资源分配任务的问题集，用于评估对现实世界决策问题的应用。</li>
<li>methods: 本研究使用了一个称为ContainerGym的实验室，用于评估对不确定性和变量维度的挑战。</li>
<li>results: 研究发现了一些知名的深度问题学习算法，如PPO、TRPO和DQN，在面对现实世界问题时存在一些 interessante的限制。<details>
<summary>Abstract</summary>
We present ContainerGym, a benchmark for reinforcement learning inspired by a real-world industrial resource allocation task. The proposed benchmark encodes a range of challenges commonly encountered in real-world sequential decision making problems, such as uncertainty. It can be configured to instantiate problems of varying degrees of difficulty, e.g., in terms of variable dimensionality. Our benchmark differs from other reinforcement learning benchmarks, including the ones aiming to encode real-world difficulties, in that it is directly derived from a real-world industrial problem, which underwent minimal simplification and streamlining. It is sufficiently versatile to evaluate reinforcement learning algorithms on any real-world problem that fits our resource allocation framework. We provide results of standard baseline methods. Going beyond the usual training reward curves, our results and the statistical tools used to interpret them allow to highlight interesting limitations of well-known deep reinforcement learning algorithms, namely PPO, TRPO and DQN.
</details>
<details>
<summary>摘要</summary>
我们介绍ContainerGym，一个基于现实世界的资源分配任务的问题集，用于评估人工智能推广学习算法。我们的问题集具有现实世界决策问题中常见的挑战，例如不确定性。我们的问题集可以根据问题的困难度而设置，例如可以根据变量的维度来调整问题的难度。相比其他的人工智能推广学习问题集，我们的问题集更加直接地来自现实世界的问题，它仅受到了最小的简化和整理。因此，我们的问题集可以用来评估任何适合我们的资源分配框架的现实世界问题。我们提供了标准基eline方法的结果，以及使用的统计工具，以便highlight interesseting的深度问题学习算法的局限性，例如PPO、TRPO和DQN。
</details></li>
</ul>
<hr>
<h2 id="A-Privacy-Preserving-Walk-in-the-Latent-Space-of-Generative-Models-for-Medical-Applications"><a href="#A-Privacy-Preserving-Walk-in-the-Latent-Space-of-Generative-Models-for-Medical-Applications" class="headerlink" title="A Privacy-Preserving Walk in the Latent Space of Generative Models for Medical Applications"></a>A Privacy-Preserving Walk in the Latent Space of Generative Models for Medical Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02984">http://arxiv.org/abs/2307.02984</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/perceivelab/plan">https://github.com/perceivelab/plan</a></li>
<li>paper_authors: Matteo Pennisi, Federica Proietto Salanitri, Giovanni Bellitto, Simone Palazzo, Ulas Bagci, Concetto Spampinato</li>
<li>for: 本研究旨在提出一种 latent space 导航策略，以便生成多样化的 sintetic 样本，支持深度模型的有效训练，同时坚持隐私问题的原则性解决。</li>
<li>methods: 我们的方法利用卫星标识器作为导航指南，在 latent space 中不线性步行，以避免遇到真实样本的近似者。我们还证明了，任意两个随机选择的 latent space 点之间的步行策略比线性 interpolate 更安全。</li>
<li>results: 我们在两个抑TB 和肥皮病类型分类 benchmark 上测试了我们的路径找索策略与 k-same 方法组合，结果表明，使用我们的方法可以减少模型训练时隐私泄露的风险，而不 sacrifice 模型性能。<details>
<summary>Abstract</summary>
Generative Adversarial Networks (GANs) have demonstrated their ability to generate synthetic samples that match a target distribution. However, from a privacy perspective, using GANs as a proxy for data sharing is not a safe solution, as they tend to embed near-duplicates of real samples in the latent space. Recent works, inspired by k-anonymity principles, address this issue through sample aggregation in the latent space, with the drawback of reducing the dataset by a factor of k. Our work aims to mitigate this problem by proposing a latent space navigation strategy able to generate diverse synthetic samples that may support effective training of deep models, while addressing privacy concerns in a principled way. Our approach leverages an auxiliary identity classifier as a guide to non-linearly walk between points in the latent space, minimizing the risk of collision with near-duplicates of real samples. We empirically demonstrate that, given any random pair of points in the latent space, our walking strategy is safer than linear interpolation. We then test our path-finding strategy combined to k-same methods and demonstrate, on two benchmarks for tuberculosis and diabetic retinopathy classification, that training a model using samples generated by our approach mitigate drops in performance, while keeping privacy preservation.
</details>
<details>
<summary>摘要</summary>
генеративные adversarial networks (GANs) 已经证明可以生成匹配目标分布的合成样本。然而，从隐私角度来看，使用 GANs 作为数据共享的代理不是安全的解决方案，因为它们往往将真实样本的近似 duplicates embedding 在幂空间中。现有作品，受到 k-隐身原则的启发，通过在幂空间中进行样本聚合来解决这个问题，但这会导致数据集被减少一个系数 k。我们的工作是通过提议一种幂空间导航策略，以生成多样的合成样本，支持深度模型的有效训练，同时在原则上保持隐私。我们的方法利用辅助标识类фика器作为帮助器，在幂空间中不线性步行，最小化与真实样本近似 duplicates 的风险。我们实验表明，任意两个随机点在幂空间中的步行策略比线性 interpolate 更安全。然后，我们测试了我们的路径找到策略与 k-same 方法结合使用，并在两个标准 benchmark 上进行肢体病诊断和糖尿病肝病诊断，结果表明，通过我们的方法训练的模型可以减少性能下降，同时保持隐私。
</details></li>
</ul>
<hr>
<h2 id="Transfer-Learning-for-the-Efficient-Detection-of-COVID-19-from-Smartphone-Audio-Data"><a href="#Transfer-Learning-for-the-Efficient-Detection-of-COVID-19-from-Smartphone-Audio-Data" class="headerlink" title="Transfer Learning for the Efficient Detection of COVID-19 from Smartphone Audio Data"></a>Transfer Learning for the Efficient Detection of COVID-19 from Smartphone Audio Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02975">http://arxiv.org/abs/2307.02975</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mattiacampana/transfer-learning-covid-19">https://github.com/mattiacampana/transfer-learning-covid-19</a></li>
<li>paper_authors: Mattia Giovanni Campana, Franca Delmastro, Elena Pagani</li>
<li>for: 这篇论文的目的是为了探索智能手机数据中疾病检测的开放研究挑战，以及COVID-19 和其呼吸症状的早期检测。</li>
<li>methods: 这篇论文使用了三种深度学习模型（VGGish、YAMNET 和 L\textsuperscript{3}-Net），以及两种转移学习方法（特征提取和精度调整），并通过用户独立的实验评估这些模型在四个数据集（总共13,447个样本）上的表现。</li>
<li>results: 结果显示L\textsuperscript{3}-Net在所有实验设定中表现最佳，与其他解决方案相比，提高了12.3%的精度-回传Area Under the Curve（AUC），并在特征提取和精度调整方法中均表现出色。此外，研究发现将专案调整到预训练的内部层次通常会导致表现下降，均下降6.6%。<details>
<summary>Abstract</summary>
Disease detection from smartphone data represents an open research challenge in mobile health (m-health) systems. COVID-19 and its respiratory symptoms are an important case study in this area and their early detection is a potential real instrument to counteract the pandemic situation. The efficacy of this solution mainly depends on the performances of AI algorithms applied to the collected data and their possible implementation directly on the users' mobile devices. Considering these issues, and the limited amount of available data, in this paper we present the experimental evaluation of 3 different deep learning models, compared also with hand-crafted features, and of two main approaches of transfer learning in the considered scenario: both feature extraction and fine-tuning. Specifically, we considered VGGish, YAMNET, and L\textsuperscript{3}-Net (including 12 different configurations) evaluated through user-independent experiments on 4 different datasets (13,447 samples in total). Results clearly show the advantages of L\textsuperscript{3}-Net in all the experimental settings as it overcomes the other solutions by 12.3\% in terms of Precision-Recall AUC as features extractor, and by 10\% when the model is fine-tuned. Moreover, we note that to fine-tune only the fully-connected layers of the pre-trained models generally leads to worse performances, with an average drop of 6.6\% with respect to feature extraction. %highlighting the need for further investigations. Finally, we evaluate the memory footprints of the different models for their possible applications on commercial mobile devices.
</details>
<details>
<summary>摘要</summary>
《医疗预测从智能手机数据中的挑战》是移动医疗（m-health）系统中的一个开放研究领域。COVID-19和其呼吸症状是这个领域中一个重要的案例研究，早期检测可以成为对抗疫情情况的实际工具。这种解决方案的有效性主要取决于应用于收集的数据的人工智能算法的性能，以及它们可能的直接在用户的移动设备上进行实现。考虑这些问题，以及有限的数据量，在这篇论文中我们提出了三种不同的深度学习模型的实验评估，并与手工设计的特征进行比较。 Specifically, we considered VGGish, YAMNET, and L\textsuperscript{3}-Net (including 12 different configurations) evaluated through user-independent experiments on 4 different datasets (13,447 samples in total). 结果显示L\textsuperscript{3}-Net在所有实验设定中都优于其他解决方案，在报告精度- recall AUC 方面提高12.3%，并在Feature extractor和 fine-tuning 方法下提高10%。此外，我们注意到只调整全连接层的准备模型通常会导致性能下降，均为6.6%。 % highlighting the need for further investigations. Finally, we evaluate the memory footprints of the different models for their possible applications on commercial mobile devices.
</details></li>
</ul>
<hr>
<h2 id="Pruning-vs-Quantization-Which-is-Better"><a href="#Pruning-vs-Quantization-Which-is-Better" class="headerlink" title="Pruning vs Quantization: Which is Better?"></a>Pruning vs Quantization: Which is Better?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02973">http://arxiv.org/abs/2307.02973</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrey Kuzmin, Markus Nagel, Mart van Baalen, Arash Behboodi, Tijmen Blankevoort</li>
<li>for:  Comparing the effectiveness of neural network quantization and pruning techniques for compressing deep neural networks.</li>
<li>methods:  Analytical and empirical comparisons of expected quantization and pruning error, and lower bounds for per-layer pruning and quantization error in trained networks.</li>
<li>results:  Quantization outperforms pruning in most cases, but pruning might be beneficial in some scenarios with very high compression ratios.<details>
<summary>Abstract</summary>
Neural network pruning and quantization techniques are almost as old as neural networks themselves. However, to date only ad-hoc comparisons between the two have been published. In this paper, we set out to answer the question on which is better: neural network quantization or pruning? By answering this question, we hope to inform design decisions made on neural network hardware going forward. We provide an extensive comparison between the two techniques for compressing deep neural networks. First, we give an analytical comparison of expected quantization and pruning error for general data distributions. Then, we provide lower bounds for the per-layer pruning and quantization error in trained networks, and compare these to empirical error after optimization. Finally, we provide an extensive experimental comparison for training 8 large-scale models on 3 tasks. Our results show that in most cases quantization outperforms pruning. Only in some scenarios with very high compression ratio, pruning might be beneficial from an accuracy standpoint.
</details>
<details>
<summary>摘要</summary>
neural network 压缩和量化技术已经几乎与神经网络一起出现了。然而，到目前为止只有随意比较这两种技术的比较。在这篇论文中，我们决定回答以下问题：哪个更好：神经网络量化或剪枝？通过回答这个问题，我们希望能够对神经网络硬件设计决策提供指导。我们对两种压缩深度神经网络的比较进行了广泛的比较。首先，我们给出了对于一般数据分布的量化和剪枝错误的分析比较。然后，我们提供了层次剪枝和量化错误的下界，并与经验性错误进行比较。最后，我们对3个任务上8个大规模模型进行了广泛的实验比较。我们的结果表明，大多数情况下，量化超过剪枝。只有在压缩比较高时，剪枝可能在准确性方面具有一定的优势。
</details></li>
</ul>
<hr>
<h2 id="DPM-Clustering-Sensitive-Data-through-Separation"><a href="#DPM-Clustering-Sensitive-Data-through-Separation" class="headerlink" title="DPM: Clustering Sensitive Data through Separation"></a>DPM: Clustering Sensitive Data through Separation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02969">http://arxiv.org/abs/2307.02969</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yara Schütt, Johannes Liebenow, Tanya Braun, Marcel Gehrke, Florian Thaeter, Esfandiar Mohammadi</li>
<li>for: 隐私保护集群算法可以在无监管的情况下对数据进行不监管的分组，同时保证敏感信息的安全性。</li>
<li>methods: 本文引入了一种新的差分隐私分组算法DPM，该算法通过在差分隐私的情况下搜索精准的数据点分隔器来实现隐私保护集群。DPM解决了两个关键挑战：寻找大距离分隔器而不是小距离分隔器，以及有效地花费隐私预算。</li>
<li>results: 实验评估表明，DPM可以与基准算法KMeans++进行比较，在ε&#x3D;1和δ&#x3D;10^-5的情况下，DPM可以在synthetic数据集上提高不同类别的固有稳定度（inertia）的最大值，相比之下，与状态公算法Chang和Kamath的分布式 clustering算法相比，DPM可以提高最大值的差异达50%（synthetic数据集）和62%（实际数据集）。<details>
<summary>Abstract</summary>
Privacy-preserving clustering groups data points in an unsupervised manner whilst ensuring that sensitive information remains protected. Previous privacy-preserving clustering focused on identifying concentration of point clouds. In this paper, we take another path and focus on identifying appropriate separators that split a data set. We introduce the novel differentially private clustering algorithm DPM that searches for accurate data point separators in a differentially private manner. DPM addresses two key challenges for finding accurate separators: identifying separators that are large gaps between clusters instead of small gaps within a cluster and, to efficiently spend the privacy budget, prioritising separators that split the data into large subparts. Using the differentially private Exponential Mechanism, DPM randomly chooses cluster separators with provably high utility: For a data set $D$, if there is a wide low-density separator in the central $60\%$ quantile, DPM finds that separator with probability $1 - \exp(-\sqrt{|D|})$. Our experimental evaluation demonstrates that DPM achieves significant improvements in terms of the clustering metric inertia. With the inertia results of the non-private KMeans++ as a baseline, for $\varepsilon = 1$ and $\delta=10^{-5}$ DPM improves upon the difference to the baseline by up to $50\%$ for a synthetic data set and by up to $62\%$ for a real-world data set compared to a state-of-the-art clustering algorithm by Chang and Kamath.
</details>
<details>
<summary>摘要</summary>
自避嫌敏感聚类分析数据点，保护敏感信息的同时，也可以自动找到数据集中的精准分割器。在这篇论文中，我们不同于之前的敏感聚类方法，我们的方法是通过找到数据集中的合适分割器来实现敏感聚类。我们提出了一种新的敏感聚类算法DPM，它在 differentially private 的方式下搜索数据集中的精准分割器。DPM 解决了两个关键问题：一是找到分割器，而不是在集群中找到小距离的分割器，二是有效地花费隐私预算。DPM 使用了不同于 KMeans++ 的扩展机制，可以很好地降低隐私预算。我们的实验评估表明，DPM 可以在各种数据集上达到显著的聚类稳定度提升。相比之下，与 KMeans++ 为基线，DPM 在 $\varepsilon = 1$ 和 $\delta = 10^{-5}$ 下可以在 synthetic 数据集上提高差异达到 $50\%$，在 real-world 数据集上提高差异达到 $62\%$。
</details></li>
</ul>
<hr>
<h2 id="SegNetr-Rethinking-the-local-global-interactions-and-skip-connections-in-U-shaped-networks"><a href="#SegNetr-Rethinking-the-local-global-interactions-and-skip-connections-in-U-shaped-networks" class="headerlink" title="SegNetr: Rethinking the local-global interactions and skip connections in U-shaped networks"></a>SegNetr: Rethinking the local-global interactions and skip connections in U-shaped networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02953">http://arxiv.org/abs/2307.02953</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junlong Cheng, Chengrui Gao, Fengjie Wang, Min Zhu</li>
<li>for: 这篇研究旨在提出一个轻量级的医疗影像分类网络（SegNetr），以提高医疗影像分类的精度和效率。</li>
<li>methods: 这篇研究使用了一个新的SegNetr块，可以在任何阶段进行本地-全球互动，并且具有线性复杂度。另外，研究者还提出了一个通用的资讯保留 skip connection（IRSC），以保持嵌入对应预测器的空间位置资讯，并实现精确的融合。</li>
<li>results: 研究结果显示，SegNetr在四个主流医疗影像分类数据集上表现出色，与常用的U-Net相比，缩减了59%和76%的参数和GFLOPs，并且保持了医疗影像分类的精度。此外，研究者还发现，SegNetr可以将其 Component 应用到其他 U-shaped 网络上，以提高其分类性能。<details>
<summary>Abstract</summary>
Recently, U-shaped networks have dominated the field of medical image segmentation due to their simple and easily tuned structure. However, existing U-shaped segmentation networks: 1) mostly focus on designing complex self-attention modules to compensate for the lack of long-term dependence based on convolution operation, which increases the overall number of parameters and computational complexity of the network; 2) simply fuse the features of encoder and decoder, ignoring the connection between their spatial locations. In this paper, we rethink the above problem and build a lightweight medical image segmentation network, called SegNetr. Specifically, we introduce a novel SegNetr block that can perform local-global interactions dynamically at any stage and with only linear complexity. At the same time, we design a general information retention skip connection (IRSC) to preserve the spatial location information of encoder features and achieve accurate fusion with the decoder features. We validate the effectiveness of SegNetr on four mainstream medical image segmentation datasets, with 59\% and 76\% fewer parameters and GFLOPs than vanilla U-Net, while achieving segmentation performance comparable to state-of-the-art methods. Notably, the components proposed in this paper can be applied to other U-shaped networks to improve their segmentation performance.
</details>
<details>
<summary>摘要</summary>
近期，U型网络在医学图像分割领域占据主导地位，主要是因为它们的简单和容易调整结构。然而，现有的U型分割网络：1）大多数是设计复杂的自注意模块，以补做 convolution 操作所带来的长期依赖缺失，这会增加总参数数和网络的计算复杂度; 2）简单地融合 encoder 和 decoder 的特征，忽略了它们的空间位置之间的连接。在这篇论文中，我们重新思考了以上问题，并建立了一个轻量级的医学图像分割网络，称为 SegNetr。具体来说，我们提出了一个新的 SegNetr 块，可以在任何阶段进行本地-全局交互，并且只有线性复杂度。同时，我们设计了一种普适的信息保留skip连接（IRSC），以保持 encoder 特征的空间位置信息，并与 decoder 特征进行准确融合。我们验证了 SegNetr 在四大流行的医学图像分割 dataset 上的效果，与 vanilla U-Net 相比，它具有59%和76% fewer parameters和GFLOPs，同时 achieving segmentation performance 与当前状态OFthe-art 方法相当。尤其是，本文所提出的组件可以应用于其他 U-shaped 网络，以提高它们的分割性能。
</details></li>
</ul>
<hr>
<h2 id="When-No-Rejection-Learning-is-Optimal-for-Regression-with-Rejection"><a href="#When-No-Rejection-Learning-is-Optimal-for-Regression-with-Rejection" class="headerlink" title="When No-Rejection Learning is Optimal for Regression with Rejection"></a>When No-Rejection Learning is Optimal for Regression with Rejection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02932">http://arxiv.org/abs/2307.02932</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaocheng Li, Shang Liu, Chunlin Sun, Hanzhao Wang</li>
<li>for: 这个论文研究了人工智能和人类之间的预测任务交互模型，具体来说是一个名为学习拒绝（Learning with Rejection）的模型。</li>
<li>methods: 这个模型有两个组成部分：预测器和拒绝器。当样本到达时，拒绝器首先决定是否接受它；如果被接受，然后预测器完成预测任务；如果被拒绝，那么预测将被委托给人类。学习问题需要同时学习预测器和拒绝器。这会改变传统的损失函数结构，并经常导致非对称和不一致问题。</li>
<li>results: 研究发现，在预测问题中，使用拒绝学习策略可以提高预测性能。此外，通过将拒绝学习策略与传统的预测器学习策略结合使用，可以提高预测性能。<details>
<summary>Abstract</summary>
Learning with rejection is a prototypical model for studying the interaction between humans and AI on prediction tasks. The model has two components, a predictor and a rejector. Upon the arrival of a sample, the rejector first decides whether to accept it; if accepted, the predictor fulfills the prediction task, and if rejected, the prediction will be deferred to humans. The learning problem requires learning a predictor and a rejector simultaneously. This changes the structure of the conventional loss function and often results in non-convexity and inconsistency issues. For the classification with rejection problem, several works develop surrogate losses for the jointly learning with provable consistency guarantees; in parallel, there has been less work for the regression counterpart. We study the regression with rejection (RwR) problem and investigate the no-rejection learning strategy which treats the RwR problem as a standard regression task to learn the predictor. We establish that the suboptimality of the no-rejection learning strategy observed in the literature can be mitigated by enlarging the function class of the predictor. Then we introduce the truncated loss to single out the learning for the predictor and we show that a consistent surrogate property can be established for the predictor individually in an easier way than for the predictor and the rejector jointly. Our findings advocate for a two-step learning procedure that first uses all the data to learn the predictor and then calibrates the prediction loss for the rejector. It is better aligned with the common intuition that more data samples will lead to a better predictor and it calls for more efforts on a better design of calibration algorithms for learning the rejector. While our discussions mainly focus on the regression problem, the theoretical results and insights generalize to the classification problem as well.
</details>
<details>
<summary>摘要</summary>
学习 WITH 拒绝是一种典型的模型，用于研究人类和 AI 在预测任务之间的交互。该模型包括一个预测器和一个拒绝者。当样本到达时，拒绝者首先决定是否接受它；如果被接受，预测器完成预测任务；如果被拒绝，预测将被延迟到人类。学习问题需要同时学习预测器和拒绝者。这会改变传统的损失函数结构，并常常导致非几何和不一致性问题。对于类别 WITH 拒绝问题，一些工作提出了证明可靠的替代损失函数，而对于 regression 问题，有少量的研究。我们研究了 regression with rejection（RwR）问题，并 investigate no-rejection 学习策略，即将 RwR 问题视为标准的回归任务，以学习预测器。我们证明了在文献中观察到的不优化情况可以通过扩大预测器的功能集来缓解。然后，我们引入了剪辑损失来单独学习预测器，并证明了在更容易的方式上可以建立预测器的一致性属性。我们的发现建议在所有数据上首先学习预测器，然后对预测器进行调整，以更好地适应实际情况。这种二步学习方式更符合人们对更多数据样本会导致更好的预测器的共识，并且强调了更好地设计报告算法以提高拒绝者的学习。虽然我们的讨论主要关注回归问题，但我们的理论结论和发现都适用于类别问题。
</details></li>
</ul>
<hr>
<h2 id="A-Real-time-Human-Pose-Estimation-Approach-for-Optimal-Sensor-Placement-in-Sensor-based-Human-Activity-Recognition"><a href="#A-Real-time-Human-Pose-Estimation-Approach-for-Optimal-Sensor-Placement-in-Sensor-based-Human-Activity-Recognition" class="headerlink" title="A Real-time Human Pose Estimation Approach for Optimal Sensor Placement in Sensor-based Human Activity Recognition"></a>A Real-time Human Pose Estimation Approach for Optimal Sensor Placement in Sensor-based Human Activity Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02906">http://arxiv.org/abs/2307.02906</a></li>
<li>repo_url: None</li>
<li>paper_authors: Orhan Konak, Alexander Wischmann, Robin van de Water, Bert Arnrich</li>
<li>for: 本研究旨在提供一种可靠的人体活动识别方法，以便实现不侵入式监测人体活动。</li>
<li>methods: 本研究使用了实时2D姿态估计来确定最佳传感器位置，并使用视频记录来获取目标活动的2D姿态数据。</li>
<li>results: 研究发现，视觉基于的传感器位置选择方法可以与传统深度学习方法相比，表现相似，证明了其有效性。<details>
<summary>Abstract</summary>
Sensor-based Human Activity Recognition facilitates unobtrusive monitoring of human movements. However, determining the most effective sensor placement for optimal classification performance remains challenging. This paper introduces a novel methodology to resolve this issue, using real-time 2D pose estimations derived from video recordings of target activities. The derived skeleton data provides a unique strategy for identifying the optimal sensor location. We validate our approach through a feasibility study, applying inertial sensors to monitor 13 different activities across ten subjects. Our findings indicate that the vision-based method for sensor placement offers comparable results to the conventional deep learning approach, demonstrating its efficacy. This research significantly advances the field of Human Activity Recognition by providing a lightweight, on-device solution for determining the optimal sensor placement, thereby enhancing data anonymization and supporting a multimodal classification approach.
</details>
<details>
<summary>摘要</summary>
《受器件基于人体活动识别》可以不侵入地监测人体活动。然而，确定最佳受器件位置以实现优化的分类性能仍然是一个挑战。这篇论文提出了一种新的方法来解决这个问题，使用实时二维姿态估计来从视频记录中获取目标活动的姿态数据。这些姿态数据提供了一种独特的策略来确定最佳受器件位置。我们通过实验验证了我们的方法，使用抖动仪器来监测13种不同的活动，并在10名参与者身上进行了评估。我们的发现表明，基于视频的受器件位置确定方法和深度学习方法具有相似的效果，这证明了我们的方法的有效性。这项研究在人体活动识别领域中做出了重要贡献，提供了一种轻量级、在设备上进行的受器件位置确定方法，从而提高数据匿名化和支持多模态分类approach。
</details></li>
</ul>
<hr>
<h2 id="PUFFIN-A-Path-Unifying-Feed-Forward-Interfaced-Network-for-Vapor-Pressure-Prediction"><a href="#PUFFIN-A-Path-Unifying-Feed-Forward-Interfaced-Network-for-Vapor-Pressure-Prediction" class="headerlink" title="PUFFIN: A Path-Unifying Feed-Forward Interfaced Network for Vapor Pressure Prediction"></a>PUFFIN: A Path-Unifying Feed-Forward Interfaced Network for Vapor Pressure Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02903">http://arxiv.org/abs/2307.02903</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vinicius Viena Santana, Carine Menezes Rebello, Luana P. Queiroz, Ana Mafalda Ribeiro, Nadia Shardt, Idelfonso B. R. Nogueira</li>
<li>for: 提高化学物质预测的精度，以满足工业和环境应用需求。</li>
<li>methods: 提出了一种基于机器学习的框架，即PUFFIN（路径联合启发前向网络）， combinig transfer learning 和启发节点，以提高热压缩预测。</li>
<li>results: PUFFIN 比不使用启发节点或使用通用描述器的其他策略更高的性能。框架的包含域专业知识来超越数据罕见性的能力，适用于更广泛的化学物质分析预测。<details>
<summary>Abstract</summary>
Accurately predicting vapor pressure is vital for various industrial and environmental applications. However, obtaining accurate measurements for all compounds of interest is not possible due to the resource and labor intensity of experiments. The demand for resources and labor further multiplies when a temperature-dependent relationship for predicting vapor pressure is desired. In this paper, we propose PUFFIN (Path-Unifying Feed-Forward Interfaced Network), a machine learning framework that combines transfer learning with a new inductive bias node inspired by domain knowledge (the Antoine equation) to improve vapor pressure prediction. By leveraging inductive bias and transfer learning using graph embeddings, PUFFIN outperforms alternative strategies that do not use inductive bias or that use generic descriptors of compounds. The framework's incorporation of domain-specific knowledge to overcome the limitation of poor data availability shows its potential for broader applications in chemical compound analysis, including the prediction of other physicochemical properties. Importantly, our proposed machine learning framework is partially interpretable, because the inductive Antoine node yields network-derived Antoine equation coefficients. It would then be possible to directly incorporate the obtained analytical expression in process design software for better prediction and control of processes occurring in industry and the environment.
</details>
<details>
<summary>摘要</summary>
精确预测蒸汽压力在各种工业和环境应用中非常重要。然而，为所有 интерес的化合物都获得准确测量是不可能的，因为实验资源和劳动力成本过高。随着温度的变化，测量蒸汽压力的关系也变得更加复杂。在这篇论文中，我们提出了PUFFIN（Path-Unifying Feed-Forward Interfaced Network）机器学习框架，该框架结合了传输学习和基于预测方程的新权重节点，以提高蒸汽压力预测。通过利用预测方程的启发和传输学习使用图像描述符，PUFFIN超越了不使用预测方程或使用通用描述符的策略。该框架的包含域专业知识以解决资料不足的限制，表明其在化学物质分析中的潜在应用。特别是，我们提出的机器学习框架部分可解释，因为 Antoine 节点的启发导致了网络获得的 Antoine 方程系数。因此，可以直接在进程设计软件中包含获得的分析表达，以提高工业和环境中进程预测和控制的准确性。
</details></li>
</ul>
<hr>
<h2 id="Free-Bits-Latency-Optimization-of-Mixed-Precision-Quantized-Neural-Networks-on-the-Edge"><a href="#Free-Bits-Latency-Optimization-of-Mixed-Precision-Quantized-Neural-Networks-on-the-Edge" class="headerlink" title="Free Bits: Latency Optimization of Mixed-Precision Quantized Neural Networks on the Edge"></a>Free Bits: Latency Optimization of Mixed-Precision Quantized Neural Networks on the Edge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02894">http://arxiv.org/abs/2307.02894</a></li>
<li>repo_url: None</li>
<li>paper_authors: Georg Rutishauser, Francesco Conti, Luca Benini</li>
<li>for: 这篇论文旨在优化混合精度量化的调整运算，以实现模型大小、延迟时间和统计准确性之间的最佳调整。</li>
<li>methods: 这篇论文提出了一种混合精度量化搜寻方法，包括一个硬件无关的微分搜寻算法和一个硬件对应的优化方法，以找到适合特定硬件目标的混合精度配置。</li>
<li>results: 这篇论文在MobileNetV1和MobileNetV2上进行了评估，并在一家多核RISC-V微控制器平台上部署了结果。获得了与8位模型相比的28.6%的终端延迟减少，并且在不支持子字元运算的硬件上也获得了速度优化。此外，论文还证明了其方法对于针对对减少二进制操作数量的分别搜寻来调整精度配置的超越性。<details>
<summary>Abstract</summary>
Mixed-precision quantization, where a deep neural network's layers are quantized to different precisions, offers the opportunity to optimize the trade-offs between model size, latency, and statistical accuracy beyond what can be achieved with homogeneous-bit-width quantization. To navigate the intractable search space of mixed-precision configurations for a given network, this paper proposes a hybrid search methodology. It consists of a hardware-agnostic differentiable search algorithm followed by a hardware-aware heuristic optimization to find mixed-precision configurations latency-optimized for a specific hardware target. We evaluate our algorithm on MobileNetV1 and MobileNetV2 and deploy the resulting networks on a family of multi-core RISC-V microcontroller platforms with different hardware characteristics. We achieve up to 28.6% reduction of end-to-end latency compared to an 8-bit model at a negligible accuracy drop from a full-precision baseline on the 1000-class ImageNet dataset. We demonstrate speedups relative to an 8-bit baseline, even on systems with no hardware support for sub-byte arithmetic at negligible accuracy drop. Furthermore, we show the superiority of our approach with respect to differentiable search targeting reduced binary operation counts as a proxy for latency.
</details>
<details>
<summary>摘要</summary>
混合精度量化，其中深度神经网络层次被量化到不同精度，可以超越基于同一bit宽度量化的优化。为探索混合精度配置空间中的优化搜索方法，这篇论文提议了一种混合搜索方法。该方法包括一个硬件无关的可微分搜索算法，以及一个硬件aware的优化策略，以找到适合特定硬件目标的延迟优化的混合精度配置。我们在MobileNetV1和MobileNetV2上测试了我们的算法，并将结果部署到一家多核RISC-V微控制器平台上，该平台具有不同硬件特性。我们实现了与8位模型相比的28.6%的端到端延迟减少，而且只有一个可观的减少精度。此外，我们还证明了我们的方法在对减少二进制运算数量为延迟的目标进行搜索时的优越性。
</details></li>
</ul>
<hr>
<h2 id="BaBE-Enhancing-Fairness-via-Estimation-of-Latent-Explaining-Variables"><a href="#BaBE-Enhancing-Fairness-via-Estimation-of-Latent-Explaining-Variables" class="headerlink" title="BaBE: Enhancing Fairness via Estimation of Latent Explaining Variables"></a>BaBE: Enhancing Fairness via Estimation of Latent Explaining Variables</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02891">http://arxiv.org/abs/2307.02891</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/babe-algorithm/babe">https://github.com/babe-algorithm/babe</a></li>
<li>paper_authors: Ruta Binkyte, Daniele Gorla, Catuscia Palamidessi</li>
<li>for:  solves the problem of unfair discrimination between two groups by proposing a pre-processing method to achieve fairness.</li>
<li>methods:  uses Bayesian Bias Elimination (BaBE), a combination of Bayes inference and the Expectation-Maximization method, to estimate the most likely value of the latent explanatory variable E for each group.</li>
<li>results:  shows good fairness and high accuracy in experiments on synthetic and real data sets.<details>
<summary>Abstract</summary>
We consider the problem of unfair discrimination between two groups and propose a pre-processing method to achieve fairness. Corrective methods like statistical parity usually lead to bad accuracy and do not really achieve fairness in situations where there is a correlation between the sensitive attribute S and the legitimate attribute E (explanatory variable) that should determine the decision. To overcome these drawbacks, other notions of fairness have been proposed, in particular, conditional statistical parity and equal opportunity. However, E is often not directly observable in the data, i.e., it is a latent variable. We may observe some other variable Z representing E, but the problem is that Z may also be affected by S, hence Z itself can be biased. To deal with this problem, we propose BaBE (Bayesian Bias Elimination), an approach based on a combination of Bayes inference and the Expectation-Maximization method, to estimate the most likely value of E for a given Z for each group. The decision can then be based directly on the estimated E. We show, by experiments on synthetic and real data sets, that our approach provides a good level of fairness as well as high accuracy.
</details>
<details>
<summary>摘要</summary>
我们考虑了不公正歧视 между两个群体，并提出了预处理方法以实现公正。通常的统计平衡方法会导致坏准确率并不实际实现公正，特别是在敏感属性S和合法属性E（解释变量）之间存在相关性时。为了解决这些缺点，其他公正的概念被提出，特别是conditional statistical parity和equal opportunity。然而，E通常不直接可见于数据中，即是隐藏变量。我们可能观察到Z表示E，但问题在于Z也可能受S的影响，因此Z本身可能受到偏见。为解决这个问题，我们提出了BaBE（抽象折衣 Bayesian Bias Elimination）方法，基于抽象折衣和期望最大化方法，以估计每个群体中E的最可能值。然后，决策可以直接基于估计的E。我们通过对 sintetic和实际数据集进行实验，示出了我们的方法可以实现高度的公正以及高准确率。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Solve-Tasks-with-Exploring-Prior-Behaviours"><a href="#Learning-to-Solve-Tasks-with-Exploring-Prior-Behaviours" class="headerlink" title="Learning to Solve Tasks with Exploring Prior Behaviours"></a>Learning to Solve Tasks with Exploring Prior Behaviours</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02889">http://arxiv.org/abs/2307.02889</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ricky-zhu/irdec">https://github.com/ricky-zhu/irdec</a></li>
<li>paper_authors: Ruiqi Zhu, Siyuan Li, Tianhong Dai, Chongjie Zhang, Oya Celiktutan</li>
<li>for: 帮助解决 sparse-reward 任务</li>
<li>methods: 使用 Intrinsic Rewards Driven Example-based Control (IRDEC) 方法，能够让代理人学习并掌握先前的行为，然后将其与任务特定的行为相连接以解决 sparse-reward 任务</li>
<li>results: 在三个导航任务和一个机器人处理任务中，表现优于其他基准值<details>
<summary>Abstract</summary>
Demonstrations are widely used in Deep Reinforcement Learning (DRL) for facilitating solving tasks with sparse rewards. However, the tasks in real-world scenarios can often have varied initial conditions from the demonstration, which would require additional prior behaviours. For example, consider we are given the demonstration for the task of \emph{picking up an object from an open drawer}, but the drawer is closed in the training. Without acquiring the prior behaviours of opening the drawer, the robot is unlikely to solve the task. To address this, in this paper we propose an Intrinsic Rewards Driven Example-based Control \textbf{(IRDEC)}. Our method can endow agents with the ability to explore and acquire the required prior behaviours and then connect to the task-specific behaviours in the demonstration to solve sparse-reward tasks without requiring additional demonstration of the prior behaviours. The performance of our method outperforms other baselines on three navigation tasks and one robotic manipulation task with sparse rewards. Codes are available at https://github.com/Ricky-Zhu/IRDEC.
</details>
<details>
<summary>摘要</summary>
深圳�技术（DRL）广泛使用演示来解决具有罕见奖励的任务。然而，实际场景中的任务可能会有 demonstrate 中的初始条件不同，需要额外的先行行为。例如，假设我们被给定了选择开放抽屉中的物品任务的演示，但抽屉在训练时是关闭的。如果不具备开启抽屉的先行行为，机器人很 unlikely 解决任务。为此，本文提出了内在奖励驱动示例控制方法（IRDEC）。我们的方法可以让代理人探索和获得所需的先行行为，然后与任务特定行为在演示中连接以解决罕见奖励任务，无需额外的先行行为示例。我们的方法在三个导航任务和一个机器人抓取任务中表现出色，超过其他基线。代码可以在 https://github.com/Ricky-Zhu/IRDEC 上获取。
</details></li>
</ul>
<hr>
<h2 id="Sample-Efficient-Learning-of-POMDPs-with-Multiple-Observations-In-Hindsight"><a href="#Sample-Efficient-Learning-of-POMDPs-with-Multiple-Observations-In-Hindsight" class="headerlink" title="Sample-Efficient Learning of POMDPs with Multiple Observations In Hindsight"></a>Sample-Efficient Learning of POMDPs with Multiple Observations In Hindsight</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02884">http://arxiv.org/abs/2307.02884</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiacheng Guo, Minshuo Chen, Huan Wang, Caiming Xiong, Mengdi Wang, Yu Bai</li>
<li>for: 研究 POMDP 中学习样本效率的问题，这是 reinforcement learning 中一个具有极大强度困难的问题。</li>
<li>methods: 提出了一种增强反馈模型，称为“多观察往事”，在每个 POMDP 交互回合后，学习者可以收集到更多的观察数据，但不能直接观察到latent state。</li>
<li>results: 证明在这种反馈模型下，可以实现 sample-efficient learning，并且该模型可以涵盖两类新的 POMDP  subclass：多观察揭示 POMDP 和 distinguishable POMDP。这两类 subclass 都是 revelaing POMDP 的推广和放松，但只需要latent state emission distribution 不同而不需要linearly independent。<details>
<summary>Abstract</summary>
This paper studies the sample-efficiency of learning in Partially Observable Markov Decision Processes (POMDPs), a challenging problem in reinforcement learning that is known to be exponentially hard in the worst-case. Motivated by real-world settings such as loading in game playing, we propose an enhanced feedback model called ``multiple observations in hindsight'', where after each episode of interaction with the POMDP, the learner may collect multiple additional observations emitted from the encountered latent states, but may not observe the latent states themselves. We show that sample-efficient learning under this feedback model is possible for two new subclasses of POMDPs: \emph{multi-observation revealing POMDPs} and \emph{distinguishable POMDPs}. Both subclasses generalize and substantially relax \emph{revealing POMDPs} -- a widely studied subclass for which sample-efficient learning is possible under standard trajectory feedback. Notably, distinguishable POMDPs only require the emission distributions from different latent states to be \emph{different} instead of \emph{linearly independent} as required in revealing POMDPs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Machine-Learned-Ranking-Algorithm-for-Dynamic-and-Personalised-Car-Pooling-Services"><a href="#A-Machine-Learned-Ranking-Algorithm-for-Dynamic-and-Personalised-Car-Pooling-Services" class="headerlink" title="A Machine-Learned Ranking Algorithm for Dynamic and Personalised Car Pooling Services"></a>A Machine-Learned Ranking Algorithm for Dynamic and Personalised Car Pooling Services</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05697">http://arxiv.org/abs/2307.05697</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mattia Giovanni Campana, Franca Delmastro, Raffaele Bruno</li>
<li>For: 降低城市交通堵塞和污染* Methods: 使用学习排序技术自动生成每个用户的个性化选择模型，并将这些模型用于建议乘客与司机的合适的共乘机会* Results: 实验结果表明，提议的解决方案可以快速和准确地预测用户的个性化选择模型，并在静态和动态条件下表现出优异的性能。<details>
<summary>Abstract</summary>
Car pooling is expected to significantly help in reducing traffic congestion and pollution in cities by enabling drivers to share their cars with travellers with similar itineraries and time schedules. A number of car pooling matching services have been designed in order to efficiently find successful ride matches in a given pool of drivers and potential passengers. However, it is now recognised that many non-monetary aspects and social considerations, besides simple mobility needs, may influence the individual willingness of sharing a ride, which are difficult to predict. To address this problem, in this study we propose GoTogether, a recommender system for car pooling services that leverages on learning-to-rank techniques to automatically derive the personalised ranking model of each user from the history of her choices (i.e., the type of accepted or rejected shared rides). Then, GoTogether builds the list of recommended rides in order to maximise the success rate of the offered matches. To test the performance of our scheme we use real data from Twitter and Foursquare sources in order to generate a dataset of plausible mobility patterns and ride requests in a metropolitan area. The results show that the proposed solution quickly obtain an accurate prediction of the personalised user's choice model both in static and dynamic conditions.
</details>
<details>
<summary>摘要</summary>
卡车pooling 预计将能够有效地减少城市塞车和污染，通过让 drivers 和旅行者共享车辆，并且将驾驶者和旅行者的行程和时间表汇入一起。然而，现在已经被认为，在分享车辆的决策中，不只有交通需求，还有许多非实物的方面和社交考虑，这些难以预测。为解决这个问题，在这个研究中，我们提出了 GoTogether，一个基于学习排名技术的推荐系统，可以自动从用户的历史选择（即接受或拒绝分享的车辆类型）中 derivate 用户的个人化选择模型。然后，GoTogether 将建立用户的个人化推荐列表，以最大化分享成功率。为验证我们的方案的性能，我们使用了 Twitter 和 Foursquare 的数据来生成一个城市区域的可能的流动模式和分享请求。结果显示，我们的方案快速地获得了个人化用户选择模型的精准预测， both in static and dynamic conditions。
</details></li>
</ul>
<hr>
<h2 id="Towards-a-safe-MLOps-Process-for-the-Continuous-Development-and-Safety-Assurance-of-ML-based-Systems-in-the-Railway-Domain"><a href="#Towards-a-safe-MLOps-Process-for-the-Continuous-Development-and-Safety-Assurance-of-ML-based-Systems-in-the-Railway-Domain" class="headerlink" title="Towards a safe MLOps Process for the Continuous Development and Safety Assurance of ML-based Systems in the Railway Domain"></a>Towards a safe MLOps Process for the Continuous Development and Safety Assurance of ML-based Systems in the Railway Domain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02867">http://arxiv.org/abs/2307.02867</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marc Zeller, Thomas Waschulzik, Reiner Schmid, Claus Bahlmann</li>
<li>for: 本文旨在提出一种安全的Machine Learning Operations（MLOps）过程，用于不断开发和安全验证基于机器学习（ML）技术的铁路领域系统。</li>
<li>methods: 本文使用了系统工程、安全验证和ML生命周期的组合，实现了一个完整的工作流程。同时，文章还描述了自动化不同阶段的挑战。</li>
<li>results: 本文提出了一种安全的MLOps过程，可以帮助实现不断开发和安全验证铁路领域的ML-基于系统。这种过程可以提高系统的可靠性、可重构性和可灵活应用性。<details>
<summary>Abstract</summary>
Traditional automation technologies alone are not sufficient to enable driverless operation of trains (called Grade of Automation (GoA) 4) on non-restricted infrastructure. The required perception tasks are nowadays realized using Machine Learning (ML) and thus need to be developed and deployed reliably and efficiently. One important aspect to achieve this is to use an MLOps process for tackling improved reproducibility, traceability, collaboration, and continuous adaptation of a driverless operation to changing conditions. MLOps mixes ML application development and operation (Ops) and enables high frequency software releases and continuous innovation based on the feedback from operations. In this paper, we outline a safe MLOps process for the continuous development and safety assurance of ML-based systems in the railway domain. It integrates system engineering, safety assurance, and the ML life-cycle in a comprehensive workflow. We present the individual stages of the process and their interactions. Moreover, we describe relevant challenges to automate the different stages of the safe MLOps process.
</details>
<details>
<summary>摘要</summary>
传统自动化技术独立无法实现列车自动驾驶（称为级别自动化（GoA）4）在不受限制的基础设施上。需要完成的感知任务现在通常通过机器学习（ML）实现，因此需要可靠地开发和部署，以及持续地适应变化的条件。一个重要的方法是使用 MLOps 过程来提高可重复性、跟踪性、合作和持续创新，基于运营反馈。在这篇论文中，我们介绍了一种安全的 MLOps 过程，用于不断发展和安全验证 ML 基于系统的 railway 领域中的系统工程、安全验证和 ML 生命周期的Integration。我们介绍了不同阶段的过程和它们之间的交互。此外，我们还描述了自动化不同阶段的安全 MLOps 过程中的挑战。
</details></li>
</ul>
<hr>
<h2 id="PLIERS-a-Popularity-Based-Recommender-System-for-Content-Dissemination-in-Online-Social-Networks"><a href="#PLIERS-a-Popularity-Based-Recommender-System-for-Content-Dissemination-in-Online-Social-Networks" class="headerlink" title="PLIERS: a Popularity-Based Recommender System for Content Dissemination in Online Social Networks"></a>PLIERS: a Popularity-Based Recommender System for Content Dissemination in Online Social Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02865">http://arxiv.org/abs/2307.02865</a></li>
<li>repo_url: None</li>
<li>paper_authors: Valerio Arnaboldi, Mattia Giovanni Campana, Franca Delmastro, Elena Pagani</li>
<li>for: 这篇论文是为了提出一种新的标签基于推荐系统（PLIERS），该系统基于用户主要关注已经拥有的物品和标签的流行程度来进行推荐。</li>
<li>methods: 该论文使用了标签的流行程度作为推荐的依据，并通过一系列实验证明了PLIERS的效果。</li>
<li>results: 实验结果表明，PLIERS可以比现有的解决方案更好地平衡算法复杂性和个性化推荐的级别，同时提供更有个性化、有 relevance 和有创新性的推荐。<details>
<summary>Abstract</summary>
In this paper, we propose a novel tag-based recommender system called PLIERS, which relies on the assumption that users are mainly interested in items and tags with similar popularity to those they already own. PLIERS is aimed at reaching a good tradeoff between algorithmic complexity and the level of personalization of recommended items. To evaluate PLIERS, we performed a set of experiments on real OSN datasets, demonstrating that it outperforms state-of-the-art solutions in terms of personalization, relevance, and novelty of recommendations.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的标签基于推荐系统，即PLIERS，它假设用户主要关注的item和标签具有类似的流行度。PLIERS的目标是实现算法复杂性和个性化推荐项的好 equilibrio。为了评估PLIERS，我们在实际社交媒体数据集上进行了一系列实验，并证明它在个性化、 relevance和新颖性方面超过了现状最佳解决方案。
</details></li>
</ul>
<hr>
<h2 id="Provably-Efficient-Iterated-CVaR-Reinforcement-Learning-with-Function-Approximation"><a href="#Provably-Efficient-Iterated-CVaR-Reinforcement-Learning-with-Function-Approximation" class="headerlink" title="Provably Efficient Iterated CVaR Reinforcement Learning with Function Approximation"></a>Provably Efficient Iterated CVaR Reinforcement Learning with Function Approximation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02842">http://arxiv.org/abs/2307.02842</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Chen, Yihan Du, Pihe Hu, Siwei Wang, Desheng Wu, Longbo Huang</li>
<li>For: 本研究旨在强化政策，以保证在决策过程中保持安全性。* Methods: 本文提出了一种新的风险敏感强化学习形式，使用迭代条件值风险（CVaR）目标函数，并提供了一种基于线性和通用函数近似的算法。* Results: 提出的算法ICVar-L和ICVar-G可以在不同的维度和集数下实现可控的停损 regret，并且提供了一些新的技术，如CVaR运算数学减法、ridge regression与CVaR适应特征以及改进的椭球 potential 函数。<details>
<summary>Abstract</summary>
Risk-sensitive reinforcement learning (RL) aims to optimize policies that balance the expected reward and risk. In this paper, we investigate a novel risk-sensitive RL formulation with an Iterated Conditional Value-at-Risk (CVaR) objective under linear and general function approximations. This new formulation, named ICVaR-RL with function approximation, provides a principled way to guarantee safety at each decision step. For ICVaR-RL with linear function approximation, we propose a computationally efficient algorithm ICVaR-L, which achieves an $\widetilde{O}(\sqrt{\alpha^{-(H+1)}(d^2H^4+dH^6)K})$ regret, where $\alpha$ is the risk level, $d$ is the dimension of state-action features, $H$ is the length of each episode, and $K$ is the number of episodes. We also establish a matching lower bound $\Omega(\sqrt{\alpha^{-(H-1)}d^2K})$ to validate the optimality of ICVaR-L with respect to $d$ and $K$. For ICVaR-RL with general function approximation, we propose algorithm ICVaR-G, which achieves an $\widetilde{O}(\sqrt{\alpha^{-(H+1)}DH^4K})$ regret, where $D$ is a dimensional parameter that depends on the eluder dimension and covering number. Furthermore, our analysis provides several novel techniques for risk-sensitive RL, including an efficient approximation of the CVaR operator, a new ridge regression with CVaR-adapted features, and a refined elliptical potential lemma.
</details>
<details>
<summary>摘要</summary>
��risk-sensitive reinforcement learning（RL）目的是优化策略，既能够获得预期的奖励，又能够保证安全。在这篇论文中，我们研究了一种新的risk-sensitive RL形式，即Iterated Conditional Value-at-Risk（CVaR）目标下的RL。这种新形式被称为ICVaR-RL，它提供了一种理性的方式来在各个决策步骤上保证安全。 дляICVaR-RLLinear function approximation，我们提出了一种高效的算法ICVaR-L，它在 $\widetilde{O}(\sqrt{\alpha^{-(H+1)}(d^2H^4+dH^6)K})$  regret上达到，其中 $\alpha$ 是风险水平， $d$ 是状态动作特征的维度， $H$ 是每个episode的长度， $K$ 是集数。我们还证明了一个匹配的下界 $\Omega(\sqrt{\alpha^{-(H-1)}d^2K})$，以验证ICVaR-L的优化性。为ICVaR-RL General function approximation，我们提出了一种算法ICVaR-G，它在 $\widetilde{O}(\sqrt{\alpha^{-(H+1)}DH^4K})$  regret上达到，其中 $D$ 是一个参数，取决于eluder dimension和covering number。此外，我们的分析还提供了一些新的技术，包括CVaR运算的有效 aproximation、CVaR适应的ridge regression和Refined elliptical potential lemma。
</details></li>
</ul>
<hr>
<h2 id="Policy-Contrastive-Imitation-Learning"><a href="#Policy-Contrastive-Imitation-Learning" class="headerlink" title="Policy Contrastive Imitation Learning"></a>Policy Contrastive Imitation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02829">http://arxiv.org/abs/2307.02829</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jialei Huang, Zhaoheng Yin, Yingdong Hu, Yang Gao</li>
<li>for: 该论文的目的是提出一种新的仿制学习方法，即Policy Contrastive Imitation Learning（PCIL），以解决现有的仿制学习方法中的一个主要问题，即仿制学习器的表征质量低下。</li>
<li>methods: 该论文提出了一种新的仿制学习方法PCIL，该方法通过在不同策略之间固定 anchoring 来学习一个对比性的表征空间，并通过cosine相似性来生成一个平滑的奖励。</li>
<li>results: 该论文的实验结果表明，PCIL可以在DeepMind Control suite上实现最佳性能，并且qualitative result Suggests that PCIL建立了一个更平滑和更有意义的表征空间 для仿制学习。<details>
<summary>Abstract</summary>
Adversarial imitation learning (AIL) is a popular method that has recently achieved much success. However, the performance of AIL is still unsatisfactory on the more challenging tasks. We find that one of the major reasons is due to the low quality of AIL discriminator representation. Since the AIL discriminator is trained via binary classification that does not necessarily discriminate the policy from the expert in a meaningful way, the resulting reward might not be meaningful either. We propose a new method called Policy Contrastive Imitation Learning (PCIL) to resolve this issue. PCIL learns a contrastive representation space by anchoring on different policies and generates a smooth cosine-similarity-based reward. Our proposed representation learning objective can be viewed as a stronger version of the AIL objective and provide a more meaningful comparison between the agent and the policy. From a theoretical perspective, we show the validity of our method using the apprenticeship learning framework. Furthermore, our empirical evaluation on the DeepMind Control suite demonstrates that PCIL can achieve state-of-the-art performance. Finally, qualitative results suggest that PCIL builds a smoother and more meaningful representation space for imitation learning.
</details>
<details>
<summary>摘要</summary>
“智慧传承学习（AIL）是一种流行的方法，它最近已经取得了很大的成功。然而，AIL在更加具体的任务上的表现仍然不满意。我们发现其中一个主要原因是AIL对于策略的识别器表现质量低下。因为AIL的识别器是通过二元分类来训练，这并不一定能够对策略和专家的比较进行真实的分辨。因此，我们提出了一新的方法called Policy Contrastive Imitation Learning（PCIL），以解决这个问题。PCIL通过不同策略的固定 anchor，学习一个对策略的对比性的表现空间，并通过cosine相似性基于的奖励来评估策略。我们的提案的表现学习目标可以视为AIL目标的强化版本，并提供了更加真实的策略和专家之间的比较。从理论上看，我们显示了PCIL的正确性，使用了学习传承框架。此外，我们的实验评估在DeepMind Control套件中，展示了PCIL可以实现最佳性能。最后，我们的质数数据显示PCIL可以建立一个更加平滑和真实的对比学习空间。”
</details></li>
</ul>
<hr>
<h2 id="Sampling-based-Fast-Gradient-Rescaling-Method-for-Highly-Transferable-Adversarial-Attacks"><a href="#Sampling-based-Fast-Gradient-Rescaling-Method-for-Highly-Transferable-Adversarial-Attacks" class="headerlink" title="Sampling-based Fast Gradient Rescaling Method for Highly Transferable Adversarial Attacks"></a>Sampling-based Fast Gradient Rescaling Method for Highly Transferable Adversarial Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02828">http://arxiv.org/abs/2307.02828</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/JHL-HUST/S-FGRM">https://github.com/JHL-HUST/S-FGRM</a></li>
<li>paper_authors: Xu Han, Anmin Liu, Chenxuan Yao, Yanbo Fan, Kun He</li>
<li>for: 针对深度神经网络受到黑盒攻击的研究，尤其是黑盒攻击的传输性能。</li>
<li>methods: 基于梯度更新的 gradient-based 方法，包括使用 sign 函数生成梯度更新的噪声。</li>
<li>results: 提出一种 Sampling-based Fast Gradient Rescaling Method (S-FGRM)，可以减少梯度更新的误差并提高黑盒攻击的传输性能。通过数据缩放substitute sign 函数而不需要额外计算成本，并提出了 Depth First Sampling 方法来消除噪声并稳定梯度更新。对于任何 gradient-based 攻击方法，我们的方法都可以用，并且可以与其他输入转换或ensemble方法相结合以进一步提高黑盒攻击的传输性能。在标准 ImageNet 数据集上进行了广泛的实验，并达到了比基eline的state-of-the-art 性能。<details>
<summary>Abstract</summary>
Deep neural networks are known to be vulnerable to adversarial examples crafted by adding human-imperceptible perturbations to the benign input. After achieving nearly 100% attack success rates in white-box setting, more focus is shifted to black-box attacks, of which the transferability of adversarial examples has gained significant attention. In either case, the common gradient-based methods generally use the sign function to generate perturbations on the gradient update, that offers a roughly correct direction and has gained great success. But little work pays attention to its possible limitation. In this work, we observe that the deviation between the original gradient and the generated noise may lead to inaccurate gradient update estimation and suboptimal solutions for adversarial transferability. To this end, we propose a Sampling-based Fast Gradient Rescaling Method (S-FGRM). Specifically, we use data rescaling to substitute the sign function without extra computational cost. We further propose a Depth First Sampling method to eliminate the fluctuation of rescaling and stabilize the gradient update. Our method could be used in any gradient-based attacks and is extensible to be integrated with various input transformation or ensemble methods to further improve the adversarial transferability. Extensive experiments on the standard ImageNet dataset show that our method could significantly boost the transferability of gradient-based attacks and outperform the state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
在这种情况下，我们发现，梯度更新估计中的偏差可能会导致不准确的梯度更新估计，从而导致攻击性能下降。为了解决这个问题，我们提出了一种快速梯度缩放方法（S-FGRM）。具体来说，我们使用数据缩放来取代 sign 函数，而不需要额外的计算成本。此外，我们还提出了深度优先采样方法，以消除缩放的摆动，稳定梯度更新。我们的方法可以在任何梯度基本攻击中使用，并可以与不同的输入变换或集成方法结合使用，以进一步提高攻击性能。我们在标准 ImageNet 数据集上进行了广泛的实验，发现我们的方法可以在攻击性能上提高很多，并超越当前的基eline。
</details></li>
</ul>
<hr>
<h2 id="Trends-in-Machine-Learning-and-Electroencephalogram-EEG-A-Review-for-Undergraduate-Researchers"><a href="#Trends-in-Machine-Learning-and-Electroencephalogram-EEG-A-Review-for-Undergraduate-Researchers" class="headerlink" title="Trends in Machine Learning and Electroencephalogram (EEG): A Review for Undergraduate Researchers"></a>Trends in Machine Learning and Electroencephalogram (EEG): A Review for Undergraduate Researchers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02819">http://arxiv.org/abs/2307.02819</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nathan Koome Murungi, Michael Vinh Pham, Xufeng Dai, Xiaodong Qu</li>
<li>for: 本文是一篇系统性文献综述，探讨了在机器学习上的脑机器接口（BCI）研究，尤其是使用电энцеfalography（EEG）进行的研究。</li>
<li>methods: 本文使用了最新的研究方法和算法，包括EEG数据采集、数据处理和分析等。</li>
<li>results: 本文对BCI研究进行了系统性的总结和分析，提供了最新的发现和探讨，并对未来的研究预测了一些有前途的方向。<details>
<summary>Abstract</summary>
This paper presents a systematic literature review on Brain-Computer Interfaces (BCIs) in the context of Machine Learning. Our focus is on Electroencephalography (EEG) research, highlighting the latest trends as of 2023. The objective is to provide undergraduate researchers with an accessible overview of the BCI field, covering tasks, algorithms, and datasets. By synthesizing recent findings, our aim is to offer a fundamental understanding of BCI research, identifying promising avenues for future investigations.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种系统性的文献评议，探讨了在机器学习之下的脑计算器接口（BCI）。我们的关注点在于电enzephalography（EEG）研究，强调最新的趋势到2023年。我们的目标是为大学生研究者提供访问性的BCI领域概述，包括任务、算法和数据集。通过总结最近的发现，我们希望为未来的研究提供丰富的理解，并确定了可能的发展方向。Note: Simplified Chinese is used here, as it is more widely used in mainland China and other parts of the world. Traditional Chinese is also an option, but it may be less accessible to some readers.
</details></li>
</ul>
<hr>
<h2 id="CPDG-A-Contrastive-Pre-Training-Method-for-Dynamic-Graph-Neural-Networks"><a href="#CPDG-A-Contrastive-Pre-Training-Method-for-Dynamic-Graph-Neural-Networks" class="headerlink" title="CPDG: A Contrastive Pre-Training Method for Dynamic Graph Neural Networks"></a>CPDG: A Contrastive Pre-Training Method for Dynamic Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02813">http://arxiv.org/abs/2307.02813</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanchen Bei, Hao Xu, Sheng Zhou, Huixuan Chi, Haishuai Wang, Mengdi Zhang, Zhao Li, Jiajun Bu</li>
<li>for: 本研究旨在提高动态图神经网络（DGNN）在实际场景中的实用应用。</li>
<li>methods: 该研究提出了一种名为Contrastive Pre-Training Method for Dynamic Graph Neural Networks（CPDG），通过灵活的结构-时间子图采样器和结构-时间对照预训练方案来解决DGNN预训练中的总化能力和长短期模型能力问题。</li>
<li>results: 对于不同的下游任务和三种传输设置，CPDG在大规模的研究和实际动态图数据集上进行了广泛的实验，并显示了与现有方法相比的显著性提高。<details>
<summary>Abstract</summary>
Dynamic graph data mining has gained popularity in recent years due to the rich information contained in dynamic graphs and their widespread use in the real world. Despite the advances in dynamic graph neural networks (DGNNs), the rich information and diverse downstream tasks have posed significant difficulties for the practical application of DGNNs in industrial scenarios. To this end, in this paper, we propose to address them by pre-training and present the Contrastive Pre-Training Method for Dynamic Graph Neural Networks (CPDG). CPDG tackles the challenges of pre-training for DGNNs, including generalization capability and long-short term modeling capability, through a flexible structural-temporal subgraph sampler along with structural-temporal contrastive pre-training schemes. Extensive experiments conducted on both large-scale research and industrial dynamic graph datasets show that CPDG outperforms existing methods in dynamic graph pre-training for various downstream tasks under three transfer settings.
</details>
<details>
<summary>摘要</summary>
“动态图数据挖掘在最近几年内得到了广泛应用，这是因为动态图中含有丰富的信息和在实际世界中的广泛使用。尽管动态图神经网络（DGNN）得到了进步，但是动态图信息的丰富和多样化下渠道任务却对DGNN在工业场景中的实际应用带来了很大的挑战。为了解决这些挑战，在本文中，我们提出了一种名为对比预训练方法 для动态图神经网络（CPDG）。CPDG通过flexible的结构-时间子图采样器和结构-时间对比预训练方案，解决了DGNN预训练中的通用能力和长短期模型能力问题。在大规模的研究和工业动态图数据集上进行了广泛的实验，得到了CPDG在多种下渠道任务下的比较优秀表现。”
</details></li>
</ul>
<hr>
<h2 id="OLR-WA-Online-Regression-with-Weighted-Average"><a href="#OLR-WA-Online-Regression-with-Weighted-Average" class="headerlink" title="OLR-WA Online Regression with Weighted Average"></a>OLR-WA Online Regression with Weighted Average</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02804">http://arxiv.org/abs/2307.02804</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Abu-Shaira, Greg Speegle</li>
<li>for: 这个论文是为了解决机器学习模型建立的问题，即需要大量的训练数据来建立准确的模型。</li>
<li>methods: 这个论文提出了一种新的在线学习方法，即OLR-WA（在线回归Weighted Average）方法，该方法可以在新的数据陆续到达时，不需要重新计算整个模型，而是可以逐步更新模型，并且可以根据用户定义的权重来偏好新数据或旧数据。</li>
<li>results: 在2D和3D的实验中，OLR-WA方法与整个数据集的静止批处理模型的性能相似，而且可以根据用户设置的权重来控制OLR-WA方法是否更快地适应变化或者更慢地抵抗变化。<details>
<summary>Abstract</summary>
Machine Learning requires a large amount of training data in order to build accurate models. Sometimes the data arrives over time, requiring significant storage space and recalculating the model to account for the new data. On-line learning addresses these issues by incrementally modifying the model as data is encountered, and then discarding the data. In this study we introduce a new online linear regression approach. Our approach combines newly arriving data with a previously existing model to create a new model. The introduced model, named OLR-WA (OnLine Regression with Weighted Average) uses user-defined weights to provide flexibility in the face of changing data to bias the results in favor of old or new data. We have conducted 2-D and 3-D experiments comparing OLR-WA to a static model using the entire data set. The results show that for consistent data, OLR-WA and the static batch model perform similarly and for varying data, the user can set the OLR-WA to adapt more quickly or to resist change.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Few-Shot-Personalized-Saliency-Prediction-Using-Tensor-Regression-for-Preserving-Structural-Global-Information"><a href="#Few-Shot-Personalized-Saliency-Prediction-Using-Tensor-Regression-for-Preserving-Structural-Global-Information" class="headerlink" title="Few-Shot Personalized Saliency Prediction Using Tensor Regression for Preserving Structural Global Information"></a>Few-Shot Personalized Saliency Prediction Using Tensor Regression for Preserving Structural Global Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02799">http://arxiv.org/abs/2307.02799</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuya Moroto, Keisuke Maeda, Takahiro Ogawa, Miki Haseyama</li>
<li>for: 预测个性化注意力图（PSM），以保留图像的结构信息。</li>
<li>methods: 使用tensor-to-matrix regression模型，以保持图像的结构信息。</li>
<li>results: 在实验结果中，提出的方法比前期方法更高的预测精度。<details>
<summary>Abstract</summary>
This paper presents a few-shot personalized saliency prediction using tensor-to-matrix regression for preserving the structural global information of personalized saliency maps (PSMs). In contrast to a general saliency map, a PSM has been great potential since its map indicates the person-specific visual attention that is useful for obtaining individual visual preferences from heterogeneity of gazed areas. The PSM prediction is needed for acquiring the PSM for the unseen image, but its prediction is still a challenging task due to the complexity of individual gaze patterns. For recognizing individual gaze patterns from the limited amount of eye-tracking data, the previous methods adopt the similarity of gaze tendency between persons. However, in the previous methods, the PSMs are vectorized for the prediction model. In this way, the structural global information of the PSMs corresponding to the image is ignored. For automatically revealing the relationship between PSMs, we focus on the tensor-based regression model that can preserve the structural information of PSMs, and realize the improvement of the prediction accuracy. In the experimental results, we confirm the proposed method including the tensor-based regression outperforms the comparative methods.
</details>
<details>
<summary>摘要</summary>
Previous methods have used similarity of gaze tendencies between persons to recognize individual patterns, but these methods vectorize PSMs for the prediction model, ignoring the structural global information of the PSMs corresponding to the image. In this study, we focus on a tensor-based regression model that can preserve the structural information of PSMs, and demonstrate improved prediction accuracy.Experimental results confirm that the proposed method, including tensor-based regression, outperforms comparative methods. By automatically revealing the relationship between PSMs, our method improves the accuracy of personalized saliency prediction.
</details></li>
</ul>
<hr>
<h2 id="VerifAI-Verified-Generative-AI"><a href="#VerifAI-Verified-Generative-AI" class="headerlink" title="VerifAI: Verified Generative AI"></a>VerifAI: Verified Generative AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02796">http://arxiv.org/abs/2307.02796</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nan Tang, Chenyu Yang, Ju Fan, Lei Cao</li>
<li>for: 提高生成AI的准确性和可靠性</li>
<li>methods: 通过多模式数据湖的数据分析和评估，确保生成AI输出的正确性</li>
<li>results: 提高生成AI的可靠性和 trasparency，促进决策的可信度<details>
<summary>Abstract</summary>
Generative AI has made significant strides, yet concerns about the accuracy and reliability of its outputs continue to grow. Such inaccuracies can have serious consequences such as inaccurate decision-making, the spread of false information, privacy violations, legal liabilities, and more. Although efforts to address these risks are underway, including explainable AI and responsible AI practices such as transparency, privacy protection, bias mitigation, and social and environmental responsibility, misinformation caused by generative AI will remain a significant challenge. We propose that verifying the outputs of generative AI from a data management perspective is an emerging issue for generative AI. This involves analyzing the underlying data from multi-modal data lakes, including text files, tables, and knowledge graphs, and assessing its quality and consistency. By doing so, we can establish a stronger foundation for evaluating the outputs of generative AI models. Such an approach can ensure the correctness of generative AI, promote transparency, and enable decision-making with greater confidence. Our vision is to promote the development of verifiable generative AI and contribute to a more trustworthy and responsible use of AI.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="The-Role-of-Subgroup-Separability-in-Group-Fair-Medical-Image-Classification"><a href="#The-Role-of-Subgroup-Separability-in-Group-Fair-Medical-Image-Classification" class="headerlink" title="The Role of Subgroup Separability in Group-Fair Medical Image Classification"></a>The Role of Subgroup Separability in Group-Fair Medical Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02791">http://arxiv.org/abs/2307.02791</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/biomedia-mira/subgroup-separability">https://github.com/biomedia-mira/subgroup-separability</a></li>
<li>paper_authors: Charles Jones, Mélanie Roschewitz, Ben Glocker</li>
<li>for: 这个论文探讨了深度分类器的性能差异。</li>
<li>methods: 该论文使用了 teoretic analysis和广泛的实验evaluation来研究深度分类器在不同医疗影像Modalities和保护特征下的性能差异。</li>
<li>results: 研究发现，深度分类器在不同医疗影像Modalities和保护特征下的能力将个体分为子群变化很大，而且这种性能差异与模型受到系统性偏见的情况有关。这些发现为开发公正医疗AI提供了重要的新视角。<details>
<summary>Abstract</summary>
We investigate performance disparities in deep classifiers. We find that the ability of classifiers to separate individuals into subgroups varies substantially across medical imaging modalities and protected characteristics; crucially, we show that this property is predictive of algorithmic bias. Through theoretical analysis and extensive empirical evaluation, we find a relationship between subgroup separability, subgroup disparities, and performance degradation when models are trained on data with systematic bias such as underdiagnosis. Our findings shed new light on the question of how models become biased, providing important insights for the development of fair medical imaging AI.
</details>
<details>
<summary>摘要</summary>
我们研究深度分类器的性能差异。我们发现分类器将个体分为子群时的能力差异很大 across medical imaging modalities和保护特征; 重要的是，我们发现这个性能是predictive of algorithmic bias。通过理论分析和广泛的实验评估，我们发现与subgroup separability, subgroup disparities和模型在数据中系统性偏见时的性能下降之间存在关系。我们的发现为开发公正医疗AI提供了重要的新idea。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Models-Empowered-Autonomous-Edge-AI-for-Connected-Intelligence"><a href="#Large-Language-Models-Empowered-Autonomous-Edge-AI-for-Connected-Intelligence" class="headerlink" title="Large Language Models Empowered Autonomous Edge AI for Connected Intelligence"></a>Large Language Models Empowered Autonomous Edge AI for Connected Intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02779">http://arxiv.org/abs/2307.02779</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifei Shen, Jiawei Shao, Xinjie Zhang, Zehong Lin, Hao Pan, Dongsheng Li, Jun Zhang, Khaled B. Letaief</li>
<li>for: 这篇论文旨在实现连接智能，即在无线网络中快速、低延迟、隐私保护的人工智能服务。</li>
<li>methods: 该系统使用云-边缘-客户端层次架构，其中大型自然语言模型（生成式预训练变换器）在云服务器上运行，而其他AI模型在设备和边缘服务器上并行部署。</li>
<li>results: 实验结果显示该系统可以准确理解用户需求，高效执行AI模型，并通过边缘联合学习生成高性能AI模型。<details>
<summary>Abstract</summary>
The evolution of wireless networks gravitates towards connected intelligence, a concept that envisions seamless interconnectivity among humans, objects, and intelligence in a hyper-connected cyber-physical world. Edge AI emerges as a promising solution to achieve connected intelligence by delivering high-quality, low-latency, and privacy-preserving AI services at the network edge. In this article, we introduce an autonomous edge AI system that automatically organizes, adapts, and optimizes itself to meet users' diverse requirements. The system employs a cloud-edge-client hierarchical architecture, where the large language model, i.e., Generative Pretrained Transformer (GPT), resides in the cloud, and other AI models are co-deployed on devices and edge servers. By leveraging the powerful abilities of GPT in language understanding, planning, and code generation, we present a versatile framework that efficiently coordinates edge AI models to cater to users' personal demands while automatically generating code to train new models via edge federated learning. Experimental results demonstrate the system's remarkable ability to accurately comprehend user demands, efficiently execute AI models with minimal cost, and effectively create high-performance AI models through federated learning.
</details>
<details>
<summary>摘要</summary>
wireless networks 的演化倾向于连接智能，这是一种概念，它描述了人机物之间的无缝连接，并在具有质量、延迟和隐私保护的半Connected cyber-physical world中实现智能连接。 Edge AI 作为一种实现连接智能的有力解决方案，可以在网络边缘提供高质量、低延迟和隐私保护的 AI 服务。在本文中，我们介绍了一个自动化的边缘 AI 系统，该系统可以自动组织、适应和优化以满足用户的多样化需求。该系统采用了云端-边缘-客户端层次架构，其中大型语言模型（i.e., 生成预训练 transformer，GPT）在云端 resid，而其他 AI 模型在设备和边缘服务器上并行部署。通过利用 GPT 的强大语言理解、规划和代码生成能力，我们提出了一种多功能框架，可以高效协调边缘 AI 模型，以满足用户的个人需求，并自动生成代码以进行边缘联合学习。实验结果表明该系统具有remarkable的能力，可以准确理解用户需求，高效执行 AI 模型，并通过联合学习创建高性能 AI 模型。
</details></li>
</ul>
<hr>
<h2 id="Temporal-Difference-Learning-for-High-Dimensional-PIDEs-with-Jumps"><a href="#Temporal-Difference-Learning-for-High-Dimensional-PIDEs-with-Jumps" class="headerlink" title="Temporal Difference Learning for High-Dimensional PIDEs with Jumps"></a>Temporal Difference Learning for High-Dimensional PIDEs with Jumps</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02766">http://arxiv.org/abs/2307.02766</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liwei Lu, Hailong Guo, Xu Yang, Yi Zhu</li>
<li>for: 解决高维度partial integro-differential equations (PIDEs) 的深度学习框架。</li>
<li>methods: 基于 temporal difference learning 的 Levy 过程集和对应的强化学习模型。</li>
<li>results: 在100维度实验中Relative error 为 O(10^{-3})，在一维纯跳问题中Relative error 为 O(10^{-4）。 Additionally, the method demonstrates low computational cost and robustness, making it suitable for addressing problems with different forms and intensities of jumps.<details>
<summary>Abstract</summary>
In this paper, we propose a deep learning framework for solving high-dimensional partial integro-differential equations (PIDEs) based on the temporal difference learning. We introduce a set of Levy processes and construct a corresponding reinforcement learning model. To simulate the entire process, we use deep neural networks to represent the solutions and non-local terms of the equations. Subsequently, we train the networks using the temporal difference error, termination condition, and properties of the non-local terms as the loss function. The relative error of the method reaches O(10^{-3}) in 100-dimensional experiments and O(10^{-4}) in one-dimensional pure jump problems. Additionally, our method demonstrates the advantages of low computational cost and robustness, making it well-suited for addressing problems with different forms and intensities of jumps.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种深度学习框架，用于解决高维partial integro-differential equations（PIDEs）。我们引入了一组Levy进程，并构建了相应的回归学习模型。为了模拟整个过程，我们使用深度神经网络来表示方程的解和非本地项。然后，我们使用时间差错、终止条件和非本地项的性质作为损失函数进行训练。在100维实验中，我们达到了相对误差为O(10^{-3})，而在一维纯跳问题中，我们达到了O(10^{-4})。此外，我们的方法还具有低计算成本和稳定性，使其适用于不同形式和强度的跳跃问题。
</details></li>
</ul>
<hr>
<h2 id="When-Does-Confidence-Based-Cascade-Deferral-Suffice"><a href="#When-Does-Confidence-Based-Cascade-Deferral-Suffice" class="headerlink" title="When Does Confidence-Based Cascade Deferral Suffice?"></a>When Does Confidence-Based Cascade Deferral Suffice?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02764">http://arxiv.org/abs/2307.02764</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wittawat Jitkrittum, Neha Gupta, Aditya Krishna Menon, Harikrishna Narasimhan, Ankit Singh Rawat, Sanjiv Kumar</li>
<li>for: 本文研究了 cascade 模型中的推论成本可以适应性地变化的策略，特别是使用 confidence 来决定是否继续预测。</li>
<li>methods: 本文使用了一种简单的推论规则，即根据当前分类器的信任度来决定是否继续预测。然而，这种信任度基于的推论规则并不考虑 cascade 的结构，例如下游模型的错误。</li>
<li>results: 本文发现在某些情况下，信任度基于的推论规则可能会失败，而 alternate 推论策略可以在这些情况下表现更好。本文首先提出了一种理论性的最佳推论规则，然后研究了后备推论机制，并证明它们可以在某些情况下大幅提高推论性能。<details>
<summary>Abstract</summary>
Cascades are a classical strategy to enable inference cost to vary adaptively across samples, wherein a sequence of classifiers are invoked in turn. A deferral rule determines whether to invoke the next classifier in the sequence, or to terminate prediction. One simple deferral rule employs the confidence of the current classifier, e.g., based on the maximum predicted softmax probability. Despite being oblivious to the structure of the cascade -- e.g., not modelling the errors of downstream models -- such confidence-based deferral often works remarkably well in practice. In this paper, we seek to better understand the conditions under which confidence-based deferral may fail, and when alternate deferral strategies can perform better. We first present a theoretical characterisation of the optimal deferral rule, which precisely characterises settings under which confidence-based deferral may suffer. We then study post-hoc deferral mechanisms, and demonstrate they can significantly improve upon confidence-based deferral in settings where (i) downstream models are specialists that only work well on a subset of inputs, (ii) samples are subject to label noise, and (iii) there is distribution shift between the train and test set.
</details>
<details>
<summary>摘要</summary>
瀑布是一种经典的战略，以适应性地变化在不同的样本上进行推理成本。一个推延规则确定在逻辑顺序中是否invoked下一个分类器，或者终止预测。一种简单的推延规则是基于当前分类器的信任度，例如基于最大预测概率。尽管无法考虑瀑布的结构——例如下游模型的错误——但这种信任度基于推延规则在实践中经常工作非常好。在这篇论文中，我们想要更好地了解 confidence-based 推延可能失败的条件，以及在哪些情况下 alternate 推延策略可以表现更好。我们首先提出了一个理论上的最佳推延规则的 caracterization，准确地描述了 confidence-based 推延可能失败的情况。然后我们研究了后续推延机制，并证明它们可以在下列情况下显著超越 confidence-based 推延：（i）下游模型只是特定输入上工作良好，（ii）样本受到标签噪音，（iii）训练集和测试集之间存在分布变化。
</details></li>
</ul>
<hr>
<h2 id="Undecimated-Wavelet-Transform-for-Word-Embedded-Semantic-Marginal-Autoencoder-in-Security-improvement-and-Denoising-different-Languages"><a href="#Undecimated-Wavelet-Transform-for-Word-Embedded-Semantic-Marginal-Autoencoder-in-Security-improvement-and-Denoising-different-Languages" class="headerlink" title="Undecimated Wavelet Transform for Word Embedded Semantic Marginal Autoencoder in Security improvement and Denoising different Languages"></a>Undecimated Wavelet Transform for Word Embedded Semantic Marginal Autoencoder in Security improvement and Denoising different Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03679">http://arxiv.org/abs/2307.03679</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shreyanth S</li>
<li>for: 提高安全措施和多语言噪声纠正</li>
<li>methods: 结合不减波лет变换和Word Embedded Semantic Marginal Autoencoder (WESMA)</li>
<li>results: 成功提高多语言安全性和数据质量<details>
<summary>Abstract</summary>
By combining the undecimated wavelet transform within a Word Embedded Semantic Marginal Autoencoder (WESMA), this research study provides a novel strategy for improving security measures and denoising multiple languages. The incorporation of these strategies is intended to address the issues of robustness, privacy, and multilingualism in data processing applications. The undecimated wavelet transform is used as a feature extraction tool to identify prominent language patterns and structural qualities in the input data. The proposed system may successfully capture significant information while preserving the temporal and geographical links within the data by employing this transform. This improves security measures by increasing the system's ability to detect abnormalities, discover hidden patterns, and distinguish between legitimate content and dangerous threats. The Word Embedded Semantic Marginal Autoencoder also functions as an intelligent framework for dimensionality and noise reduction. The autoencoder effectively learns the underlying semantics of the data and reduces noise components by exploiting word embeddings and semantic context. As a result, data quality and accuracy are increased in following processing stages. The suggested methodology is tested using a diversified dataset that includes several languages and security scenarios. The experimental results show that the proposed approach is effective in attaining security enhancement and denoising capabilities across multiple languages. The system is strong in dealing with linguistic variances, producing consistent outcomes regardless of the language used. Furthermore, incorporating the undecimated wavelet transform considerably improves the system's ability to efficiently address complex security concerns
</details>
<details>
<summary>摘要</summary>
这个研究使用Word Embedded Semantic Marginal Autoencoder（WESMA）和不删减波лет变数（Undecimated Wavelet Transform）的结合，提供了一个新的安全措施和类别数据处理领域的改进策略。这些策略的目的是解决资料处理中的问题，包括Robustness、隐私和多语言支持。Undecimated Wavelet Transform 用于特征提取，以找出输入数据中的主要语言模式和结构特征。这个系统可以成功地捕捉重要信息，并保持资料中的时间和地理连结。这将提高安全措施，增加系统的检测异常、发现隐藏模式和分辨合法内容和危险威胁的能力。Word Embedded Semantic Marginal Autoencoder 同时serve as an intelligent framework for dimensionality and noise reduction，它能够学习资料的底层 semantics，并通过Word Embeddings和Semantic Context来减少噪声。因此，处理后的数据质量和准确性增加。实验结果显示，提案的方法能够在多种语言和安全enario下实现安全增强和类别数据处理能力。系统在不同语言下的处理表现均匀，并且在应用Undecimated Wavelet Transform时，具有更高的处理复杂安全问题的能力。
</details></li>
</ul>
<hr>
<h2 id="Optimal-Bandwidth-Selection-for-DENCLUE-Algorithm"><a href="#Optimal-Bandwidth-Selection-for-DENCLUE-Algorithm" class="headerlink" title="Optimal Bandwidth Selection for DENCLUE Algorithm"></a>Optimal Bandwidth Selection for DENCLUE Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03206">http://arxiv.org/abs/2307.03206</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Wang</li>
<li>for: 本研究旨在提出一种新的参数选择方法，以提高density-based clustering算法DENCLUE的性能。</li>
<li>methods: 本研究使用了一种新的参数选择方法，基于density-based clustering算法DENCLUE。</li>
<li>results: 实验结果表明，新的参数选择方法可以提高density-based clustering算法DENCLUE的性能。<details>
<summary>Abstract</summary>
In modern day industry, clustering algorithms are daily routines of algorithm engineers. Although clustering algorithms experienced rapid growth before 2010. Innovation related to the research topic has stagnated after deep learning became the de facto industrial standard for machine learning applications. In 2007, a density-based clustering algorithm named DENCLUE was invented to solve clustering problem for nonlinear data structures. However, its parameter selection problem was largely neglected until 2011. In this paper, we propose a new approach to compute the optimal parameters for the DENCLUE algorithm, and discuss its performance in the experiment section.
</details>
<details>
<summary>摘要</summary>
现代工业中，聚类算法是算法工程师的日常 Routine。虽然聚类算法在2010年之前经历了快速增长，但是与研究主题相关的创新却在深度学习成为机器学习应用的 де facto标准后停滞不前。2007年，一种基于浓度的聚类算法 named DENCLUE 被发明，用于解决非线性数据结构上的聚类问题。然而，该算法的参数选择问题一直未得到了充分的关注，直到2011年。在这篇论文中，我们提出了一种新的方法来计算 DENCLUE 算法的优化参数，并在实验部分讨论其性能。
</details></li>
</ul>
<hr>
<h2 id="Offline-Reinforcement-Learning-with-Imbalanced-Datasets"><a href="#Offline-Reinforcement-Learning-with-Imbalanced-Datasets" class="headerlink" title="Offline Reinforcement Learning with Imbalanced Datasets"></a>Offline Reinforcement Learning with Imbalanced Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02752">http://arxiv.org/abs/2307.02752</a></li>
<li>repo_url: None</li>
<li>paper_authors: Li Jiang, Sijie Chen, Jielin Qiu, Haoran Xu, Wai Kin Chan, Zhao Ding</li>
<li>for: 本研究旨在解决现有的离线强化学习（RL）研究中的数据不均衡问题，即在实际世界中的数据分布不均衡。</li>
<li>methods: 本研究使用了增强的CQL方法，通过记忆过程来恢复过去相关的经验，以解决离线RL中数据不均衡的挑战。</li>
<li>results: 对于具有不同水平的不均衡数据集，我们的方法比基线方法表现出色，获得了更高的性能。<details>
<summary>Abstract</summary>
The prevalent use of benchmarks in current offline reinforcement learning (RL) research has led to a neglect of the imbalance of real-world dataset distributions in the development of models. The real-world offline RL dataset is often imbalanced over the state space due to the challenge of exploration or safety considerations. In this paper, we specify properties of imbalanced datasets in offline RL, where the state coverage follows a power law distribution characterized by skewed policies. Theoretically and empirically, we show that typically offline RL methods based on distributional constraints, such as conservative Q-learning (CQL), are ineffective in extracting policies under the imbalanced dataset. Inspired by natural intelligence, we propose a novel offline RL method that utilizes the augmentation of CQL with a retrieval process to recall past related experiences, effectively alleviating the challenges posed by imbalanced datasets. We evaluate our method on several tasks in the context of imbalanced datasets with varying levels of imbalance, utilizing the variant of D4RL. Empirical results demonstrate the superiority of our method over other baselines.
</details>
<details>
<summary>摘要</summary>
现有研究中的大部分Offline reinforcement learning（RL）都使用了标准的benchmark，导致实际世界数据集的不均衡问题被忽略。实际世界中的Offline RL数据集经常具有状态空间的不均衡，这可能是因为探索挑战或安全考虑。在这篇论文中，我们指定了Offline RL中的不均衡数据集的性质，其中状态覆盖率遵循一个带有扁平政策的力学分布。理论和实验表明，通常Offline RL方法，如保守Q学习（CQL），在不均衡数据集上不能有效提取策略。 inspirited by natural intelligence，我们提议一种新的Offline RL方法，该方法利用CQL的扩展和回味过程来回忆过去相关的经验，有效地解决了不均衡数据集中的挑战。我们在多个任务上进行了在不均衡数据集上的评估，使用了D4RL的变体。实验结果表明，我们的方法在其他基elines之上具有显著的优越性。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-the-Evaluators-Are-Current-Few-Shot-Learning-Benchmarks-Fit-for-Purpose"><a href="#Evaluating-the-Evaluators-Are-Current-Few-Shot-Learning-Benchmarks-Fit-for-Purpose" class="headerlink" title="Evaluating the Evaluators: Are Current Few-Shot Learning Benchmarks Fit for Purpose?"></a>Evaluating the Evaluators: Are Current Few-Shot Learning Benchmarks Fit for Purpose?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02732">http://arxiv.org/abs/2307.02732</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luísa Shimabucoro, Timothy Hospedales, Henry Gouk</li>
<li>for: 本研究旨在investigate task-level evaluation in few-shot learning, 以提供更加可靠的模型评估方法。</li>
<li>methods: 本文使用cross-validation with a low number of folds和bootstrapping来评估模型性能，并考虑了多种模型选择策略。</li>
<li>results: 研究发现，使用cross-validation with a low number of folds可以直接 estimating model performance，而使用bootstrapping或cross-validation with a large number of folds更适合用于模型选择。总之，现有的几个shot learning benchmarks不适合用于评估单个任务的模型性能。<details>
<summary>Abstract</summary>
Numerous benchmarks for Few-Shot Learning have been proposed in the last decade. However all of these benchmarks focus on performance averaged over many tasks, and the question of how to reliably evaluate and tune models trained for individual tasks in this regime has not been addressed. This paper presents the first investigation into task-level evaluation -- a fundamental step when deploying a model. We measure the accuracy of performance estimators in the few-shot setting, consider strategies for model selection, and examine the reasons for the failure of evaluators usually thought of as being robust. We conclude that cross-validation with a low number of folds is the best choice for directly estimating the performance of a model, whereas using bootstrapping or cross validation with a large number of folds is better for model selection purposes. Overall, we find that existing benchmarks for few-shot learning are not designed in such a way that one can get a reliable picture of how effectively methods can be used on individual tasks.
</details>
<details>
<summary>摘要</summary>
多种几担学习 benchmark 在过去一代提出了，但所有这些 benchmark 都专注于多任务性能的平均值，而忽略了如何可靠地评估和调整用于个个任务的模型。这篇文章是首次研究到任务级评估 -- 在这个 Régime 中发挥作用的基本步骤。我们测量了几担学习中性能估计器的准确性，考虑了选择模型的策略，并探究了通常被视为可靠的评估器的失败原因。我们结论是，使用少量剂的交叉验证是直接测量模型性能的最佳选择，而使用bootstrap或多剂交叉验证更适合用于选择模型目的。总之，我们发现现有的几担学习 benchmark 并没有设计得能够提供用于个个任务的可靠性能图像。
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-Empowerment-Towards-Tractable-Empowerment-Based-Skill-Learning"><a href="#Hierarchical-Empowerment-Towards-Tractable-Empowerment-Based-Skill-Learning" class="headerlink" title="Hierarchical Empowerment: Towards Tractable Empowerment-Based Skill-Learning"></a>Hierarchical Empowerment: Towards Tractable Empowerment-Based Skill-Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02728">http://arxiv.org/abs/2307.02728</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrew Levy, Sreehari Rammohan, Alessandro Allievi, Scott Niekum, George Konidaris</li>
<li>for: 本研究旨在开发一种新的框架，以便更加可 tractable 地学习大量独特技能。</li>
<li>methods: 本研究使用 Goal-Conditioned Hierarchical Reinforcement Learning 概念，并提出了一种新的约束下降 bounds 方法，以便更加准确地计算 Empowerment。此外，本研究还提出了一种层次结构，用于计算 Empowerment over exponentially longer time scales。</li>
<li>results: 在一系列的 simulated robotics tasks 中，我们的 four level agents 能够学习技能，covering a surface area over two orders of magnitude larger than prior work。<details>
<summary>Abstract</summary>
General purpose agents will require large repertoires of skills. Empowerment -- the maximum mutual information between skills and the states -- provides a pathway for learning large collections of distinct skills, but mutual information is difficult to optimize. We introduce a new framework, Hierarchical Empowerment, that makes computing empowerment more tractable by integrating concepts from Goal-Conditioned Hierarchical Reinforcement Learning. Our framework makes two specific contributions. First, we introduce a new variational lower bound on mutual information that can be used to compute empowerment over short horizons. Second, we introduce a hierarchical architecture for computing empowerment over exponentially longer time scales. We verify the contributions of the framework in a series of simulated robotics tasks. In a popular ant navigation domain, our four level agents are able to learn skills that cover a surface area over two orders of magnitude larger than prior work.
</details>
<details>
<summary>摘要</summary>
通用代理人需要大量技能。使owerment——最大双方信息共同性——提供了学习大量独特技能的路径，但双方信息困难优化。我们提出了一个新的框架，层次empowerment，它通过组合目标受控层次学习概念来使计算empowerment更加可 tractable。我们的框架做出了两项具体贡献：首先，我们引入了一种新的变量下界，用于计算短时间段内的共同信息，这可以用来计算empowerment。其次，我们引入了一个层次建构，用于在指数增长的时间尺度上计算empowerment。我们在一些模拟的机器人任务中验证了我们的框架的贡献，并在受欢迎的蚂蚁导航领域中，我们的四级代理人能够学习技能，占据了两个级别的表面积，比过去的工作更大。
</details></li>
</ul>
<hr>
<h2 id="Through-the-Fairness-Lens-Experimental-Analysis-and-Evaluation-of-Entity-Matching"><a href="#Through-the-Fairness-Lens-Experimental-Analysis-and-Evaluation-of-Entity-Matching" class="headerlink" title="Through the Fairness Lens: Experimental Analysis and Evaluation of Entity Matching"></a>Through the Fairness Lens: Experimental Analysis and Evaluation of Entity Matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02726">http://arxiv.org/abs/2307.02726</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/uic-indexlab/fair_entity_matching">https://github.com/uic-indexlab/fair_entity_matching</a></li>
<li>paper_authors: Nima Shahbazi, Nikola Danevski, Fatemeh Nargesian, Abolfazl Asudeh, Divesh Srivastava</li>
<li>for: This paper aims to address the fairness of entity matching (EM) techniques, which have not been well-studied despite extensive research on algorithmic fairness.</li>
<li>methods: The authors perform an extensive experimental evaluation of various EM techniques using two social datasets generated from publicly available datasets.</li>
<li>results: The authors find that EM techniques can be unfair under certain conditions, such as when some demographic groups are overrepresented or when names are more similar in some groups compared to others. They also find that certain fairness definitions, such as positive predictive value parity and true positive rate parity, are more capable of revealing EM unfairness due to the class imbalance nature of EM.<details>
<summary>Abstract</summary>
Entity matching (EM) is a challenging problem studied by different communities for over half a century. Algorithmic fairness has also become a timely topic to address machine bias and its societal impacts. Despite extensive research on these two topics, little attention has been paid to the fairness of entity matching.   Towards addressing this gap, we perform an extensive experimental evaluation of a variety of EM techniques in this paper. We generated two social datasets from publicly available datasets for the purpose of auditing EM through the lens of fairness. Our findings underscore potential unfairness under two common conditions in real-world societies: (i) when some demographic groups are overrepresented, and (ii) when names are more similar in some groups compared to others. Among our many findings, it is noteworthy to mention that while various fairness definitions are valuable for different settings, due to EM's class imbalance nature, measures such as positive predictive value parity and true positive rate parity are, in general, more capable of revealing EM unfairness.
</details>
<details>
<summary>摘要</summary>
<<SYS>>TRANSLATE_TEXTEntity matching (EM) 是一个长期研究的问题，已经被不同的社区研究了超过半个世纪。algorithmic fairness 也在当今成为一个时间问题，以 Address Machine Bias 和 Its societal impacts。despite extensive research on these two topics, little attention has been paid to the fairness of entity matching.  在这篇论文中，我们进行了广泛的实验评估多种 EM 技术。我们为了审核 EM 的公平性，Generated two social datasets from publicly available datasets。our findings highlight potential unfairness under two common conditions in real-world societies: (i) when some demographic groups are overrepresented, and (ii) when names are more similar in some groups compared to others. among our many findings, it is noteworthy to mention that while various fairness definitions are valuable for different settings, due to EM's class imbalance nature, measures such as positive predictive value parity and true positive rate parity are, in general, more capable of revealing EM unfairness.<</SYS>>
</details></li>
</ul>
<hr>
<h2 id="Understanding-Uncertainty-Sampling"><a href="#Understanding-Uncertainty-Sampling" class="headerlink" title="Understanding Uncertainty Sampling"></a>Understanding Uncertainty Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02719">http://arxiv.org/abs/2307.02719</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liushangnoname/uncertainty-sampling">https://github.com/liushangnoname/uncertainty-sampling</a></li>
<li>paper_authors: Shang Liu, Xiaocheng Li<br>for:这篇论文的目的是系统地探讨不确定样本选择算法，并提出一个新的不确定度量“损失作为不确定”，以及一个通用的一致损失函数，以便在不同的任务和损失函数下进行定制化。methods:这篇论文使用了一些数学理论和计算机科学技术，包括流程式和质量控制的损失函数、统计学的随机损失函数、和机器学习的损失函数。results:这篇论文提出了一个新的不确定度量“损失作为不确定”，并证明了这个度量的充分性和相对优化性。此外，这篇论文还提出了一个通用的一致损失函数，并证明了这个函数在不同的任务和损失函数下的定制化。最后，这篇论文还与某些不确定样本选择算法的理论和实际问题进行了连接。<details>
<summary>Abstract</summary>
Uncertainty sampling is a prevalent active learning algorithm that queries sequentially the annotations of data samples which the current prediction model is uncertain about. However, the usage of uncertainty sampling has been largely heuristic: (i) There is no consensus on the proper definition of "uncertainty" for a specific task under a specific loss; (ii) There is no theoretical guarantee that prescribes a standard protocol to implement the algorithm, for example, how to handle the sequentially arrived annotated data under the framework of optimization algorithms such as stochastic gradient descent. In this work, we systematically examine uncertainty sampling algorithms under both stream-based and pool-based active learning. We propose a notion of equivalent loss which depends on the used uncertainty measure and the original loss function and establish that an uncertainty sampling algorithm essentially optimizes against such an equivalent loss. The perspective verifies the properness of existing uncertainty measures from two aspects: surrogate property and loss convexity. Furthermore, we propose a new notion for designing uncertainty measures called \textit{loss as uncertainty}. The idea is to use the conditional expected loss given the features as the uncertainty measure. Such an uncertainty measure has nice analytical properties and generality to cover both classification and regression problems, which enable us to provide the first generalization bound for uncertainty sampling algorithms under both stream-based and pool-based settings, in the full generality of the underlying model and problem. Lastly, we establish connections between certain variants of the uncertainty sampling algorithms with risk-sensitive objectives and distributional robustness, which can partly explain the advantage of uncertainty sampling algorithms when the sample size is small.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate "Uncertainty sampling is a prevalent active learning algorithm that queries sequentially the annotations of data samples which the current prediction model is uncertain about. However, the usage of uncertainty sampling has been largely heuristic: (i) There is no consensus on the proper definition of "uncertainty" for a specific task under a specific loss; (ii) There is no theoretical guarantee that prescribes a standard protocol to implement the algorithm, for example, how to handle the sequentially arrived annotated data under the framework of optimization algorithms such as stochastic gradient descent. In this work, we systematically examine uncertainty sampling algorithms under both stream-based and pool-based active learning. We propose a notion of equivalent loss which depends on the used uncertainty measure and the original loss function and establish that an uncertainty sampling algorithm essentially optimizes against such an equivalent loss. The perspective verifies the properness of existing uncertainty measures from two aspects: surrogate property and loss convexity. Furthermore, we propose a new notion for designing uncertainty measures called \textit{loss as uncertainty}. The idea is to use the conditional expected loss given the features as the uncertainty measure. Such an uncertainty measure has nice analytical properties and generality to cover both classification and regression problems, which enable us to provide the first generalization bound for uncertainty sampling algorithms under both stream-based and pool-based settings, in the full generality of the underlying model and problem. Lastly, we establish connections between certain variants of the uncertainty sampling algorithms with risk-sensitive objectives and distributional robustness, which can partly explain the advantage of uncertainty sampling algorithms when the sample size is small."中文翻译：Active learning中的uncertainty sampling是一种广泛使用的算法，它采样数据样本，并在这些样本上请求标注。然而，uncertainty sampling的使用具有许多不确定性：（i）没有对特定任务和损失函数的uncertainty定义的共识；（ii）没有理论保证，要求实现这个算法，例如如何在权重 descent等优化算法下处理顺序到达的标注数据。在这种工作中，我们系统地研究了uncertainty sampling算法在流式活动学习和储存活动学习下的性能。我们提出了一个等效损失的概念，它取决于使用的uncertainty度量和原始损失函数。我们证明了uncertainty sampling算法实际上是优化这个等效损失的。我们从两个方面验证了现有的uncertainty度量的正确性：代理性和损失凹陷。此外，我们提出了一个新的设计uncertainty度量的思想，即使用特征 conditional expected loss作为uncertainty度量。这种uncertainty度量具有良好的分析性和通用性，可以覆盖类别和回归问题。这使得我们可以提供流式和储存活动学习下的第一个一般化 bounds。最后，我们建立了certain variants of uncertainty sampling算法与风险敏感目标和分布 robustness之间的连接，这可以部分解释uncertainty sampling算法在样本量小时的优势。
</details></li>
</ul>
<hr>
<h2 id="Multi-Similarity-Contrastive-Learning"><a href="#Multi-Similarity-Contrastive-Learning" class="headerlink" title="Multi-Similarity Contrastive Learning"></a>Multi-Similarity Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02712">http://arxiv.org/abs/2307.02712</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lkh-meredith/Debiased-Momentum-Contrastive-Learning-for-Multimodal-Video-Similarity-Measures">https://github.com/lkh-meredith/Debiased-Momentum-Contrastive-Learning-for-Multimodal-Video-Similarity-Measures</a></li>
<li>paper_authors: Emily Mu, John Guttag, Maggie Makar</li>
<li>for: 学习一种多相似性强制约束的抽象表示法，以提高模型的泛化能力。</li>
<li>methods: 利用多个相似度指标的监督来学习抽象表示，自动学习相似性权重，以优化模型的泛化性能。</li>
<li>results: 对比州chart的基eline，模型经过MSCon强制约束后在领域内和领域外的表现均有较好的成绩。<details>
<summary>Abstract</summary>
Given a similarity metric, contrastive methods learn a representation in which examples that are similar are pushed together and examples that are dissimilar are pulled apart. Contrastive learning techniques have been utilized extensively to learn representations for tasks ranging from image classification to caption generation. However, existing contrastive learning approaches can fail to generalize because they do not take into account the possibility of different similarity relations. In this paper, we propose a novel multi-similarity contrastive loss (MSCon), that learns generalizable embeddings by jointly utilizing supervision from multiple metrics of similarity. Our method automatically learns contrastive similarity weightings based on the uncertainty in the corresponding similarity, down-weighting uncertain tasks and leading to better out-of-domain generalization to new tasks. We show empirically that networks trained with MSCon outperform state-of-the-art baselines on in-domain and out-of-domain settings.
</details>
<details>
<summary>摘要</summary>
给定一个相似度 metric，对冲方法会学习一个 representation，其中相似的例子会被拟合在一起，而不相似的例子会被分离开来。对冲学习技术已经广泛应用于从图像分类到caption生成等任务中来学习表示。然而，现有的对冲学习方法可能会失去泛化能力，因为它们不考虑可能存在多种相似度关系。在这篇论文中，我们提出了一种新的多相似度对冲损失函数（MSCon），它可以通过多种相似度指标来学习泛化 embedding。我们的方法可以自动学习对冲相似度权重，基于相似度uncertainty的下降，从而实现更好的对外部领域泛化。我们通过实验表明，使用 MSCon 训练的网络在域内和域外设置中都能够超越状态机器。
</details></li>
</ul>
<hr>
<h2 id="Towards-Symmetry-Aware-Generation-of-Periodic-Materials"><a href="#Towards-Symmetry-Aware-Generation-of-Periodic-Materials" class="headerlink" title="Towards Symmetry-Aware Generation of Periodic Materials"></a>Towards Symmetry-Aware Generation of Periodic Materials</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02707">http://arxiv.org/abs/2307.02707</a></li>
<li>repo_url: None</li>
<li>paper_authors: Youzhi Luo, Chengkai Liu, Shuiwang Ji</li>
<li>For: 本研究旨在生成具有固体结构的周期材料，而现有的深度学习方法尚未完全捕捉周期材料的物理准确性。* Methods: 本文提出了一种新的材料生成方法——SyMat，可以捕捉物理准确性的周期材料结构。SyMat使用自适应神经网络模型生成原子类型集、晶格长度和晶格角，并使用一种新的协调分布模型来进行矩阵协调。* Results: SyMat理论上具有对所有Symmetry转换的不变性，并在随机生成和性能优化任务中达到了出色的表现。<details>
<summary>Abstract</summary>
We consider the problem of generating periodic materials with deep models. While symmetry-aware molecule generation has been studied extensively, periodic materials possess different symmetries, which have not been completely captured by existing methods. In this work, we propose SyMat, a novel material generation approach that can capture physical symmetries of periodic material structures. SyMat generates atom types and lattices of materials through generating atom type sets, lattice lengths and lattice angles with a variational auto-encoder model. In addition, SyMat employs a score-based diffusion model to generate atom coordinates of materials, in which a novel symmetry-aware probabilistic model is used in the coordinate diffusion process. We show that SyMat is theoretically invariant to all symmetry transformations on materials and demonstrate that SyMat achieves promising performance on random generation and property optimization tasks.
</details>
<details>
<summary>摘要</summary>
我们考虑了使用深度模型生成 periodic 材料的问题。 Although symmetry-aware molecule generation has been studied extensively, periodic materials have different symmetries that have not been fully captured by existing methods. In this work, we propose SyMat, a novel material generation approach that can capture the physical symmetries of periodic material structures. SyMat generates atom types and lattices of materials by generating atom type sets, lattice lengths, and lattice angles with a variational autoencoder model. In addition, SyMat employs a score-based diffusion model to generate atom coordinates of materials, using a novel symmetry-aware probabilistic model in the coordinate diffusion process. We prove that SyMat is theoretically invariant to all symmetry transformations on materials and demonstrate that SyMat achieves promising performance on random generation and property optimization tasks.
</details></li>
</ul>
<hr>
<h2 id="Loss-Functions-and-Metrics-in-Deep-Learning-A-Review"><a href="#Loss-Functions-and-Metrics-in-Deep-Learning-A-Review" class="headerlink" title="Loss Functions and Metrics in Deep Learning. A Review"></a>Loss Functions and Metrics in Deep Learning. A Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02694">http://arxiv.org/abs/2307.02694</a></li>
<li>repo_url: None</li>
<li>paper_authors: Juan Terven, Diana M. Cordova-Esparza, Alfonzo Ramirez-Pedraza, Edgar A. Chavez-Urbiola</li>
<li>for: 这篇论文旨在探讨深度学习中最常用的损失函数和性能指标，以帮助实践者选择适合自己特定任务的方法。</li>
<li>methods: 这篇论文评论了深度学习中最常用的损失函数和性能指标，包括其优点和局限性，以及它们在不同的深度学习问题中的应用。</li>
<li>results: 论文总结了不同损失函数和性能指标的应用，并提供了帮助实践者选择适合自己特定任务的方法的概述。<details>
<summary>Abstract</summary>
One of the essential components of deep learning is the choice of the loss function and performance metrics used to train and evaluate models. This paper reviews the most prevalent loss functions and performance measurements in deep learning. We examine the benefits and limits of each technique and illustrate their application to various deep-learning problems. Our review aims to give a comprehensive picture of the different loss functions and performance indicators used in the most common deep learning tasks and help practitioners choose the best method for their specific task.
</details>
<details>
<summary>摘要</summary>
一个深度学习中的重要组件是选择的损失函数和评价指标来训练和评估模型。本文评审了深度学习中最常用的损失函数和评价指标，探讨它们的优缺点和在不同的深度学习问题中的应用。我们的评审旨在给深度学习各种任务中的不同损失函数和评价指标带来全面的认知，帮助实践者选择适合自己特定任务的方法。Note: "深度学习" in Simplified Chinese is written as "深度学习" (shēn dēng xué xí), not "深度机器学习" (shēn dēng jī shū xí) as it is sometimes translated.
</details></li>
</ul>
<hr>
<h2 id="Kernels-Data-Physics"><a href="#Kernels-Data-Physics" class="headerlink" title="Kernels, Data &amp; Physics"></a>Kernels, Data &amp; Physics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02693">http://arxiv.org/abs/2307.02693</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francesco Cagnetta, Deborah Oliveira, Mahalakshmi Sabanayagam, Nikolaos Tsilivis, Julia Kempe</li>
<li>for: 这篇论文主要是为了解决机器学习中的一些不可解决的问题，通过找到可解决的kernel形式来获得更好的理解。</li>
<li>methods: 这篇论文使用了NTK方法，即通过找到一个可解决的kernel来理解一个不可解决的问题。</li>
<li>results: 论文中提出了一些实际应用，如数据简化和鲁棒性增强，以及一些示例的偏见假设。<details>
<summary>Abstract</summary>
Lecture notes from the course given by Professor Julia Kempe at the summer school "Statistical physics of Machine Learning" in Les Houches. The notes discuss the so-called NTK approach to problems in machine learning, which consists of gaining an understanding of generally unsolvable problems by finding a tractable kernel formulation. The notes are mainly focused on practical applications such as data distillation and adversarial robustness, examples of inductive bias are also discussed.
</details>
<details>
<summary>摘要</summary>
lecture notes from Professor Julia Kempe's course at the "Statistical physics of Machine Learning" summer school in Les Houches, which discusses the so-called NTK approach to problems in machine learning. This approach involves finding a tractable kernel formulation to gain an understanding of generally unsolvable problems. The notes focus mainly on practical applications such as data distillation and adversarial robustness, and examples of inductive bias are also discussed.Here is the translation in Traditional Chinese:lecture notes from Professor Julia Kempe's course at the "Statistical physics of Machine Learning" summer school in Les Houches, which discusses the so-called NTK approach to problems in machine learning. This approach involves finding a tractable kernel formulation to gain an understanding of generally unsolvable problems. The notes focus mainly on practical applications such as data distillation and adversarial robustness, and examples of inductive bias are also discussed.
</details></li>
</ul>
<hr>
<h2 id="Scaling-In-Context-Demonstrations-with-Structured-Attention"><a href="#Scaling-In-Context-Demonstrations-with-Structured-Attention" class="headerlink" title="Scaling In-Context Demonstrations with Structured Attention"></a>Scaling In-Context Demonstrations with Structured Attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02690">http://arxiv.org/abs/2307.02690</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianle Cai, Kaixuan Huang, Jason D. Lee, Mengdi Wang</li>
<li>for: 提高大语言模型（LLM）在上下文学习中的能力，即从少量示例中学习到某个任务的执行。</li>
<li>methods: 提出了一种更好的建筑设计，即SAICL（结构化注意力 для上下文学习），它将全注意力替换为特定于上下文学习的结构化注意力机制，并将各个示例之间的不必要依赖关系除掉，使模型具有示例顺序 permutation 的不变性。</li>
<li>results: SAICL在meta-training框架下评估，与全注意力相比具有相似或更好的性能，并在执行上实现了 up to 3.4x 速度提升。SAICL 还在每个示例独立处理方法（FiD）的强基eline上表现出色，并且因为其线性特性，可以轻松扩展到多个示例，并在扩展时保持不间断的性能提升。<details>
<summary>Abstract</summary>
The recent surge of large language models (LLMs) highlights their ability to perform in-context learning, i.e., "learning" to perform a task from a few demonstrations in the context without any parameter updates. However, their capabilities of in-context learning are limited by the model architecture: 1) the use of demonstrations is constrained by a maximum sentence length due to positional embeddings; 2) the quadratic complexity of attention hinders users from using more demonstrations efficiently; 3) LLMs are shown to be sensitive to the order of the demonstrations. In this work, we tackle these challenges by proposing a better architectural design for in-context learning. We propose SAICL (Structured Attention for In-Context Learning), which replaces the full-attention by a structured attention mechanism designed for in-context learning, and removes unnecessary dependencies between individual demonstrations, while making the model invariant to the permutation of demonstrations. We evaluate SAICL in a meta-training framework and show that SAICL achieves comparable or better performance than full attention while obtaining up to 3.4x inference speed-up. SAICL also consistently outperforms a strong Fusion-in-Decoder (FiD) baseline which processes each demonstration independently. Finally, thanks to its linear nature, we demonstrate that SAICL can easily scale to hundreds of demonstrations with continuous performance gains with scaling.
</details>
<details>
<summary>摘要</summary>
最近的大语言模型（LLM）的升温 highlights 它们在上下文学习中的能力，即从数据中学习某种任务，而无需更新参数。然而，这些模型在上下文学习的能力受到模型体系的限制：1）使用示例的限制由于位置嵌入而导致最大句子长度; 2）关注的二次复杂性阻碍用户更多示例的有效使用; 3）LLMs 被证明敏感于示例的顺序。在这种情况下，我们解决这些挑战的方法是提出一种更好的体系设计，即 SAICL（结构化关注 для上下文学习）。SAICL 将全关注替换为特定于上下文学习的结构化关注机制，并消除示例之间的不必要依赖关系，使模型对示例的顺序无敏感。我们在 meta-training 框架中评估 SAICL，并显示 SAICL 与全关注相比实现了相似或更好的性能，并在执行上获得了最多 3.4 倍的速度提升。SAICL 还一直超过了强的 Fusion-in-Decoder（FiD）基eline，该基eline 每个示例独立进行处理。最后，由于它的线性性，我们证明 SAICL 可以轻松扩展到数百个示例，并在扩展时保持不断提高性能。
</details></li>
</ul>
<hr>
<h2 id="GIT-Detecting-Uncertainty-Out-Of-Distribution-and-Adversarial-Samples-using-Gradients-and-Invariance-Transformations"><a href="#GIT-Detecting-Uncertainty-Out-Of-Distribution-and-Adversarial-Samples-using-Gradients-and-Invariance-Transformations" class="headerlink" title="GIT: Detecting Uncertainty, Out-Of-Distribution and Adversarial Samples using Gradients and Invariance Transformations"></a>GIT: Detecting Uncertainty, Out-Of-Distribution and Adversarial Samples using Gradients and Invariance Transformations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02672">http://arxiv.org/abs/2307.02672</a></li>
<li>repo_url: None</li>
<li>paper_authors: Julia Lust, Alexandru P. Condurache</li>
<li>for: 检测深度神经网络的泛化错误</li>
<li>methods:  combines 使用梯度信息和变换方法</li>
<li>results: 在多种网络架构、问题设置和扰动类型上实现了比州-of-the-art的高效性<details>
<summary>Abstract</summary>
Deep neural networks tend to make overconfident predictions and often require additional detectors for misclassifications, particularly for safety-critical applications. Existing detection methods usually only focus on adversarial attacks or out-of-distribution samples as reasons for false predictions. However, generalization errors occur due to diverse reasons often related to poorly learning relevant invariances. We therefore propose GIT, a holistic approach for the detection of generalization errors that combines the usage of gradient information and invariance transformations. The invariance transformations are designed to shift misclassified samples back into the generalization area of the neural network, while the gradient information measures the contradiction between the initial prediction and the corresponding inherent computations of the neural network using the transformed sample. Our experiments demonstrate the superior performance of GIT compared to the state-of-the-art on a variety of network architectures, problem setups and perturbation types.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Active-Class-Selection-for-Few-Shot-Class-Incremental-Learning"><a href="#Active-Class-Selection-for-Few-Shot-Class-Incremental-Learning" class="headerlink" title="Active Class Selection for Few-Shot Class-Incremental Learning"></a>Active Class Selection for Few-Shot Class-Incremental Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02641">http://arxiv.org/abs/2307.02641</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chrismcclurg/fscil-acs">https://github.com/chrismcclurg/fscil-acs</a></li>
<li>paper_authors: Christopher McClurg, Ali Ayub, Harsh Tyagi, Sarah M. Rajtmajer, Alan R. Wagner</li>
<li>for: 这个论文的目的是开发一种能让自主机器人在有限的互动情况下不断学习环境中的新对象。</li>
<li>methods: 这个论文使用了几何学incremental学习和活动类选择的想法，并将其结合到一个基于状态 искусственный智能的模型中。</li>
<li>results: 实验结果表明，这种方法可以在真实世界应用中提供长期的有效性，并且可以帮助机器人在有限的互动情况下不断学习和更新其模型。<details>
<summary>Abstract</summary>
For real-world applications, robots will need to continually learn in their environments through limited interactions with their users. Toward this, previous works in few-shot class incremental learning (FSCIL) and active class selection (ACS) have achieved promising results but were tested in constrained setups. Therefore, in this paper, we combine ideas from FSCIL and ACS to develop a novel framework that can allow an autonomous agent to continually learn new objects by asking its users to label only a few of the most informative objects in the environment. To this end, we build on a state-of-the-art (SOTA) FSCIL model and extend it with techniques from ACS literature. We term this model Few-shot Incremental Active class SeleCtiOn (FIASco). We further integrate a potential field-based navigation technique with our model to develop a complete framework that can allow an agent to process and reason on its sensory data through the FIASco model, navigate towards the most informative object in the environment, gather data about the object through its sensors and incrementally update the FIASco model. Experimental results on a simulated agent and a real robot show the significance of our approach for long-term real-world robotics applications.
</details>
<details>
<summary>摘要</summary>
实际应用中， роботы需要不断学习环境中的新对象，通过与用户有限的交互来实现。以前的研究中，几shot类增量学习（FSCIL）和活动类选择（ACS）都有获得了可塑性的结果，但是都是在限制的设置下测试的。因此，在这篇论文中，我们将 combining FSCIL 和 ACS 的想法，开发一个能让自主代理人不断学习新的对象，只需要用户标注环境中最有用的一些对象。为此，我们基于现状的 FSCIL 模型，并将 ACS 文献中的技术添加到其中。我们称这个模型为 Few-shot Incremental Active class SeleCtiOn（FIASco）。此外，我们还将潜在场景基本navitation技术与我们的模型集成，开发了一个完整的框架，让代理人可以通过 FIASco 模型处理和理解其感知数据， navigate到环境中最有用的对象，通过感知器收集数据，并不断更新 FIASco 模型。实验结果表明，我们的方法在模拟的代理人和真实的 робот上都具有长期实际应用的重要性。
</details></li>
</ul>
<hr>
<h2 id="Hybrid-Ground-State-Quantum-Algorithms-based-on-Neural-Schrodinger-Forging"><a href="#Hybrid-Ground-State-Quantum-Algorithms-based-on-Neural-Schrodinger-Forging" class="headerlink" title="Hybrid Ground-State Quantum Algorithms based on Neural Schrödinger Forging"></a>Hybrid Ground-State Quantum Algorithms based on Neural Schrödinger Forging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02633">http://arxiv.org/abs/2307.02633</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paulin de Schoulepnikoff, Oriel Kiss, Sofia Vallecorsa, Giuseppe Carleo, Michele Grossi</li>
<li>for: 解决量子系统的地面问题</li>
<li>methods: 使用生成式神经网络来筛选关键的 bitstring，消减对 Schmidt 分解的约数</li>
<li>results: 比标准实现更高效，可应用于更大的系统和非排列不变系统<details>
<summary>Abstract</summary>
Entanglement forging based variational algorithms leverage the bi-partition of quantum systems for addressing ground state problems. The primary limitation of these approaches lies in the exponential summation required over the numerous potential basis states, or bitstrings, when performing the Schmidt decomposition of the whole system. To overcome this challenge, we propose a new method for entanglement forging employing generative neural networks to identify the most pertinent bitstrings, eliminating the need for the exponential sum. Through empirical demonstrations on systems of increasing complexity, we show that the proposed algorithm achieves comparable or superior performance compared to the existing standard implementation of entanglement forging. Moreover, by controlling the amount of required resources, this scheme can be applied to larger, as well as non permutation invariant systems, where the latter constraint is associated with the Heisenberg forging procedure. We substantiate our findings through numerical simulations conducted on spins models exhibiting one-dimensional ring, two-dimensional triangular lattice topologies, and nuclear shell model configurations.
</details>
<details>
<summary>摘要</summary>
Entanglement forging based variational algorithms 利用量子系统的二分法解决零点问题。主要限制是对很多 potential basis states（或字串）进行 Schmidt 分解的极限积分。为了解决这个挑战，我们提议一种使用生成神经网络来标识最重要的字串，从而消除极限积分的需要。通过实验表明，我们的方法在不同级别的系统上都可以达到相对或更高的性能，而且可以控制所需资源，因此可以应用于更大的系统和非 permutation 不变系统。我们通过对磁矩模型、二维三角形矩阵和核shell模型的数值仿真来证明我们的发现。
</details></li>
</ul>
<hr>
<h2 id="Stability-of-Q-Learning-Through-Design-and-Optimism"><a href="#Stability-of-Q-Learning-Through-Design-and-Optimism" class="headerlink" title="Stability of Q-Learning Through Design and Optimism"></a>Stability of Q-Learning Through Design and Optimism</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02632">http://arxiv.org/abs/2307.02632</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sean Meyn</li>
<li>for: 本研究是一篇关于渐进学习和Q学习的讲解和新方法的论文，其中包括在法国南部的INFORMS APS投票讲座，于2023年6月举行。</li>
<li>methods: 本研究使用了Stochastic Approximation和Q学习算法，以及一些新的稳定性和可能加速的转化方法。其中两个全新的贡献是：1. Linear function approximation下Q学习稳定性的问题，已经是研究的开放问题之一，这里表明了采用修改 Gibbs 政策可以确保算法的稳定性（参数估计在 bounded 内），但是转化仍然是许多研究的开放问题之一。2. Zap Zero 算法，可以近似新顿-拉普逊流，不需要矩阵倒数。它在一定条件下是稳定的和可靠的，并且适用于Q学习和其他渐进学习算法。</li>
<li>results: 本研究获得了一些新的结果，包括：1. 使用修改 Gibbs 政策可以确保Q学习的稳定性，但是转化仍然是一个开放问题。2. Zap Zero 算法可以在一定条件下实现稳定和可靠的转化。<details>
<summary>Abstract</summary>
Q-learning has become an important part of the reinforcement learning toolkit since its introduction in the dissertation of Chris Watkins in the 1980s. The purpose of this paper is in part a tutorial on stochastic approximation and Q-learning, providing details regarding the INFORMS APS inaugural Applied Probability Trust Plenary Lecture, presented in Nancy France, June 2023.   The paper also presents new approaches to ensure stability and potentially accelerated convergence for these algorithms, and stochastic approximation in other settings. Two contributions are entirely new:   1. Stability of Q-learning with linear function approximation has been an open topic for research for over three decades. It is shown that with appropriate optimistic training in the form of a modified Gibbs policy, there exists a solution to the projected Bellman equation, and the algorithm is stable (in terms of bounded parameter estimates). Convergence remains one of many open topics for research.   2. The new Zap Zero algorithm is designed to approximate the Newton-Raphson flow without matrix inversion. It is stable and convergent under mild assumptions on the mean flow vector field for the algorithm, and compatible statistical assumption on an underlying Markov chain. The algorithm is a general approach to stochastic approximation which in particular applies to Q-learning with "oblivious" training even with non-linear function approximation.
</details>
<details>
<summary>摘要</summary>
Q-学习已经成为现代回归学习工具包的重要组成部分，自1980年代Chris Watkins的论文发表以来。这篇论文的目的之一是提供Stochastic Approximation和Q-学习的教程，详细介绍了2023年6月在法国南部的INFORMS APS投资论坛演讲。此外，论文还提出了新的方法来保证这些算法的稳定性和可能更快的收敛，以及Stochastic Approximation在其他设置下的应用。两个贡献是完全新的：1. Q-学习使用线性函数近似的稳定性问题在研究中一直是一个开放的问题。这篇论文示出，通过修改Gibbs策略，存在一个解决了投影贝尔曼方程的算法，并且这个算法是稳定的（以bounded参数估计的形式）。然而，收敛问题仍然是许多开放的研究问题。2. Zap Zero算法是一种用于近似新顿-拉普逊流动的算法，不需要矩阵反转。它在一些简单的假设下是稳定的和收敛的，并且可以应用于Q-学习的“无知”训练，包括非线性函数近似。这个算法是一种通用的Stochastic Approximation方法，可以应用于Q-学习以外的其他问题。
</details></li>
</ul>
<hr>
<h2 id="An-explainable-model-to-support-the-decision-about-the-therapy-protocol-for-AML"><a href="#An-explainable-model-to-support-the-decision-about-the-therapy-protocol-for-AML" class="headerlink" title="An explainable model to support the decision about the therapy protocol for AML"></a>An explainable model to support the decision about the therapy protocol for AML</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02631">http://arxiv.org/abs/2307.02631</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jade M. Almeida, Giovanna A. Castro, João A. Machado-Neto, Tiago A. Almeida</li>
<li>for: 预测AML患者的生存机会，以支持医生决策最佳治疗协议。</li>
<li>methods: 使用数据分析和可解释机器学习模型，以支持医生决策。</li>
<li>results: 提出了一个可解释的机器学习模型，可以安全地支持医生决策，并且实验结果具有承诺性。<details>
<summary>Abstract</summary>
Acute Myeloid Leukemia (AML) is one of the most aggressive types of hematological neoplasm. To support the specialists' decision about the appropriate therapy, patients with AML receive a prognostic of outcomes according to their cytogenetic and molecular characteristics, often divided into three risk categories: favorable, intermediate, and adverse. However, the current risk classification has known problems, such as the heterogeneity between patients of the same risk group and no clear definition of the intermediate risk category. Moreover, as most patients with AML receive an intermediate-risk classification, specialists often demand other tests and analyses, leading to delayed treatment and worsening of the patient's clinical condition. This paper presents the data analysis and an explainable machine-learning model to support the decision about the most appropriate therapy protocol according to the patient's survival prediction. In addition to the prediction model being explainable, the results obtained are promising and indicate that it is possible to use it to support the specialists' decisions safely. Most importantly, the findings offered in this study have the potential to open new avenues of research toward better treatments and prognostic markers.
</details>
<details>
<summary>摘要</summary>
急性骨髓性白血病（AML）是一种非常严重的血液疾病之一。为了支持专家决策最佳治疗方案，AML患者通常会根据其细胞学和分子特征进行评估，并将患者分为三个风险 категории：有利、中等和不利。然而，现有的风险分类系统存在一些知道的问题，如患者之间的不同性和中等风险类别的不清晰定义。此外，由于大多数AML患者被诊断为中等风险，专家通常会要求更多的测试和分析，导致治疗延迟和病情加重。本文提出了数据分析和可解释的机器学习模型，以支持专家决策最佳治疗方案。除了模型的可解释性之外，研究结果具有推动力，并表明可以安全地使用这些模型来支持专家决策。最重要的是，本研究的发现有可能开启新的研究途径，以提高治疗和诊断 marker 的效果。
</details></li>
</ul>
<hr>
<h2 id="FLuID-Mitigating-Stragglers-in-Federated-Learning-using-Invariant-Dropout"><a href="#FLuID-Mitigating-Stragglers-in-Federated-Learning-using-Invariant-Dropout" class="headerlink" title="FLuID: Mitigating Stragglers in Federated Learning using Invariant Dropout"></a>FLuID: Mitigating Stragglers in Federated Learning using Invariant Dropout</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02623">http://arxiv.org/abs/2307.02623</a></li>
<li>repo_url: None</li>
<li>paper_authors: Irene Wang, Prashant J. Nair, Divya Mahajan</li>
<li>for: 这个论文的目的是解决 Federated Learning（FL）中的性能瓶颈问题，即在多个设备上进行本地机器学习模型训练，然后将模型更新同步到共享服务器上，以保护用户隐私。</li>
<li>methods: 这个论文使用了一种名为“Invariant Dropout”的方法，该方法可以在 FL 中提取一个子模型，以降低可能影响精度的随机 Dropout 操作。此外，这个论文还提出了一个适应性训练框架，称为 Federated Learning using Invariant Dropout（FLuID），该框架可以在 runtime 中动态调整训练负担，以适应不同的设备性能。</li>
<li>results: 论文的实验结果表明，Invariant Dropout 可以保持基eline模型的效率，同时解决 FL 中的性能瓶颈问题。此外，FLuID 可以在 runtime 中动态调整训练负担，以适应不同的设备性能。<details>
<summary>Abstract</summary>
Federated Learning (FL) allows machine learning models to train locally on individual mobile devices, synchronizing model updates via a shared server. This approach safeguards user privacy; however, it also generates a heterogeneous training environment due to the varying performance capabilities across devices. As a result, straggler devices with lower performance often dictate the overall training time in FL. In this work, we aim to alleviate this performance bottleneck due to stragglers by dynamically balancing the training load across the system. We introduce Invariant Dropout, a method that extracts a sub-model based on the weight update threshold, thereby minimizing potential impacts on accuracy. Building on this dropout technique, we develop an adaptive training framework, Federated Learning using Invariant Dropout (FLuID). FLuID offers a lightweight sub-model extraction to regulate computational intensity, thereby reducing the load on straggler devices without affecting model quality. Our method leverages neuron updates from non-straggler devices to construct a tailored sub-model for each straggler based on client performance profiling. Furthermore, FLuID can dynamically adapt to changes in stragglers as runtime conditions shift. We evaluate FLuID using five real-world mobile clients. The evaluations show that Invariant Dropout maintains baseline model efficiency while alleviating the performance bottleneck of stragglers through a dynamic, runtime approach.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 允许机器学习模型在个人手持设备上本地进行训练，并通过共享服务器进行模型更新同步。这种方法保护用户隐私，但也创造了不同设备性能水平的多样化训练环境。因此，在FL中，慢速设备（straggler）的低性能经常决定整体训练时间。在这项工作中，我们想使用动态负载均衡来缓解FL中的性能瓶颈。我们提出了“ invariable dropout” 方法，该方法根据模型更新阈值提取子模型，以最小化可能的影响精度。基于这种dropout技术，我们开发了适应训练框架，称为 Federated Learning using Invariant Dropout (FLuID)。FLuID 提供了轻量级的子模型提取机制，以规避计算急剧的设备。我们的方法通过非慢速设备的神经元更新来构建每个慢速设备的个性化子模型，基于客户端性能分析。此外，FLuID 可以在运行时conditions 发生变化时动态适应。我们通过使用五个真实的手持设备进行评估，发现， invariable dropout 可以保持基eline模型效率，同时通过动态、运行时的方式缓解慢速设备的性能瓶颈。
</details></li>
</ul>
<hr>
<h2 id="Learning-when-to-observe-A-frugal-reinforcement-learning-framework-for-a-high-cost-world"><a href="#Learning-when-to-observe-A-frugal-reinforcement-learning-framework-for-a-high-cost-world" class="headerlink" title="Learning when to observe: A frugal reinforcement learning framework for a high-cost world"></a>Learning when to observe: A frugal reinforcement learning framework for a high-cost world</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02620">http://arxiv.org/abs/2307.02620</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cbellinger27/learning-when-to-observe-in-rl">https://github.com/cbellinger27/learning-when-to-observe-in-rl</a></li>
<li>paper_authors: Colin Bellinger, Mark Crowley, Isaac Tamblyn</li>
<li>for: 本研究旨在探讨RL算法在环境状态测量成本高的应用场景下是否可以学习出高效的控制策略。</li>
<li>methods: 本文采用了 Deep Dynamic Multi-Step Observationless Agent（DMSOA），并对其进行了比较和实验评估在OpenAI gym和Atari Pong环境中。</li>
<li>results: 研究结果表明，DMSOA可以更好地学习控制策略，只需 fewer decision steps和测量步骤，并且在OpenAI gym和Atari Pong环境中表现出了更高的效果。<details>
<summary>Abstract</summary>
Reinforcement learning (RL) has been shown to learn sophisticated control policies for complex tasks including games, robotics, heating and cooling systems and text generation. The action-perception cycle in RL, however, generally assumes that a measurement of the state of the environment is available at each time step without a cost. In applications such as materials design, deep-sea and planetary robot exploration and medicine, however, there can be a high cost associated with measuring, or even approximating, the state of the environment. In this paper, we survey the recently growing literature that adopts the perspective that an RL agent might not need, or even want, a costly measurement at each time step. Within this context, we propose the Deep Dynamic Multi-Step Observationless Agent (DMSOA), contrast it with the literature and empirically evaluate it on OpenAI gym and Atari Pong environments. Our results, show that DMSOA learns a better policy with fewer decision steps and measurements than the considered alternative from the literature. The corresponding code is available at: \url{https://github.com/cbellinger27/Learning-when-to-observe-in-RL
</details>
<details>
<summary>摘要</summary>
Reinforcement learning (RL) 已经能够学习复杂任务，如游戏、机器人、暖气和冷气系统以及文本生成。但是，行动-感知循环在 RL 中通常假设在每个时间步骤上可以无 costa 地测量环境状态。在材料设计、深海和行星探险、医学应用等领域，可能存在高昂的测量环境状态的成本。在这篇论文中，我们报道了最近快速增长的文献，该文献采用了RL代理不需要或甚至不想在每个时间步骤上测量环境状态的视角。在这个Context中，我们提出了深度动态多步无测量代理（DMSOA），并与文献进行了比较。我们的结果表明，DMSOA 可以更好地学习策略，使用更少的决策步骤和测量。相关代码可以在以下链接中找到：https://github.com/cbellinger27/Learning-when-to-observe-in-RL。
</details></li>
</ul>
<hr>
<h2 id="Human-Inspired-Progressive-Alignment-and-Comparative-Learning-for-Grounded-Word-Acquisition"><a href="#Human-Inspired-Progressive-Alignment-and-Comparative-Learning-for-Grounded-Word-Acquisition" class="headerlink" title="Human Inspired Progressive Alignment and Comparative Learning for Grounded Word Acquisition"></a>Human Inspired Progressive Alignment and Comparative Learning for Grounded Word Acquisition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02615">http://arxiv.org/abs/2307.02615</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sled-group/comparative-learning">https://github.com/sled-group/comparative-learning</a></li>
<li>paper_authors: Yuwei Bao, Barrett Martin Lattimer, Joyce Chai</li>
<li>for: 这个论文目的是提出一种基于人类婴儿语言学习的计算过程，用于词汇学习。</li>
<li>methods: 这个论文使用了比较学习的方法，通过比较不同特征之间的相似性和差异，学习抽取共同语言标签中的信息。</li>
<li>results: 实验结果表明，这种方法可以具有高效的 continent learning 特性，能够不断学习更多的概念。<details>
<summary>Abstract</summary>
Human language acquisition is an efficient, supervised, and continual process. In this work, we took inspiration from how human babies acquire their first language, and developed a computational process for word acquisition through comparative learning. Motivated by cognitive findings, we generated a small dataset that enables the computation models to compare the similarities and differences of various attributes, learn to filter out and extract the common information for each shared linguistic label. We frame the acquisition of words as not only the information filtration process, but also as representation-symbol mapping. This procedure does not involve a fixed vocabulary size, nor a discriminative objective, and allows the models to continually learn more concepts efficiently. Our results in controlled experiments have shown the potential of this approach for efficient continual learning of grounded words.
</details>
<details>
<summary>摘要</summary>
人类语言学习是一个高效、指导的、不断进行的过程。在这项工作中，我们 Draw inspiration from how human babies acquire their first language, and developed a computational process for word acquisition through comparative learning。 Based on cognitive findings, we generated a small dataset that enables the computation models to compare the similarities and differences of various attributes, learn to filter out and extract the common information for each shared linguistic label。 We frame the acquisition of words as not only the information filtration process, but also as representation-symbol mapping。 This procedure does not involve a fixed vocabulary size, nor a discriminative objective, and allows the models to continually learn more concepts efficiently。 Our results in controlled experiments have shown the potential of this approach for efficient continual learning of grounded words。
</details></li>
</ul>
<hr>
<h2 id="Additive-Decoders-for-Latent-Variables-Identification-and-Cartesian-Product-Extrapolation"><a href="#Additive-Decoders-for-Latent-Variables-Identification-and-Cartesian-Product-Extrapolation" class="headerlink" title="Additive Decoders for Latent Variables Identification and Cartesian-Product Extrapolation"></a>Additive Decoders for Latent Variables Identification and Cartesian-Product Extrapolation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02598">http://arxiv.org/abs/2307.02598</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/divyat09/additive_decoder_extrapolation">https://github.com/divyat09/additive_decoder_extrapolation</a></li>
<li>paper_authors: Sébastien Lachapelle, Divyat Mahajan, Ioannis Mitliagkas, Simon Lacoste-Julien</li>
<li>for: 本 paper 探讨了 latent variables 标注和 “out-of-support” 图像生成在表示学习中的问题。</li>
<li>methods: 本 paper 使用了一类被称为 additive 的解码器，它们类似于用于 object-centric representation learning (OCRL) 的解码器，并适用于可以为图像 decomposition 为不同物体图像的情况。</li>
<li>results: 本 paper 提供了一些条件，以确保使用 additive 解码器来解决重建问题时可以准确地标注 latent variables 的块，并且可以允许 permutation 和 block-wise invertible transformations。此外，本 paper 还证明了 additive 解码器可以生成新的图像，并且可以在新的方式中重新组合已经观察到的变量因素。<details>
<summary>Abstract</summary>
We tackle the problems of latent variables identification and "out-of-support" image generation in representation learning. We show that both are possible for a class of decoders that we call additive, which are reminiscent of decoders used for object-centric representation learning (OCRL) and well suited for images that can be decomposed as a sum of object-specific images. We provide conditions under which exactly solving the reconstruction problem using an additive decoder is guaranteed to identify the blocks of latent variables up to permutation and block-wise invertible transformations. This guarantee relies only on very weak assumptions about the distribution of the latent factors, which might present statistical dependencies and have an almost arbitrarily shaped support. Our result provides a new setting where nonlinear independent component analysis (ICA) is possible and adds to our theoretical understanding of OCRL methods. We also show theoretically that additive decoders can generate novel images by recombining observed factors of variations in novel ways, an ability we refer to as Cartesian-product extrapolation. We show empirically that additivity is crucial for both identifiability and extrapolation on simulated data.
</details>
<details>
<summary>摘要</summary>
我们研究了隐藏变量标识和“不支持”图像生成问题在表示学习中。我们表明这两个问题可以通过我们称为添加型decoder解决，这类decoder与物体中心表示学习（OCRL）中使用的decoder类似，适用于可以分解为对象特定图像的和其他图像的总和。我们提供了解决这些问题时使用添加型decoder的条件， garantuee 隐藏变量块可以通过 permutation和块级可逆变换进行唯一标识。这个保证只需要很弱的 latent factor 分布的假设，这些分布可能具有统计依赖关系和极其复杂的支持形状。我们的结果为非线性独立 componon 分析（ICA）提供了新的设置，并补充了OCRL方法的理论理解。我们还证明了添加型decoder可以通过 recombining 观察到的因变量来生成新的图像，我们称之为 Cartesian-product 推导。我们通过实验表明，添加性是隐藏变量标识和 extrapolation 问题的关键。
</details></li>
</ul>
<hr>
<h2 id="TransformerG2G-Adaptive-time-stepping-for-learning-temporal-graph-embeddings-using-transformers"><a href="#TransformerG2G-Adaptive-time-stepping-for-learning-temporal-graph-embeddings-using-transformers" class="headerlink" title="TransformerG2G: Adaptive time-stepping for learning temporal graph embeddings using transformers"></a>TransformerG2G: Adaptive time-stepping for learning temporal graph embeddings using transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02588">http://arxiv.org/abs/2307.02588</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alan John Varghese, Aniruddha Bora, Mengjia Xu, George Em Karniadakis</li>
<li>for: 本研究旨在提出一种基于 transformer 编码器的图像模型，以便更好地学习多变时间图的动态特征。</li>
<li>methods: 该模型使用 transformer 编码器首先学习当前时间点 ($t$) 和上一个时间点 ($t-1, t-l$, $l$ 是 Context 的长度) 中节点的中间表示。然后，使用两个投影层生成时间点 $t$ 的彩色矩阵。</li>
<li>results: 对于多种不同的 benchmark 和“新鲜度”水平，我们的模型在链接预测精度和计算效率方面与传统多步方法和我们之前的模型（DynG2G）相比，表现出了明显的优势。此外，通过分析注意力权重，我们可以揭示时间依赖关系，找到影响因素，并获得图structure的复杂交互。例如，我们发现了节点度与注意力权重之间的强相关性，这表明了节点度在图结构的不同阶段的作用。<details>
<summary>Abstract</summary>
Dynamic graph embedding has emerged as a very effective technique for addressing diverse temporal graph analytic tasks (i.e., link prediction, node classification, recommender systems, anomaly detection, and graph generation) in various applications. Such temporal graphs exhibit heterogeneous transient dynamics, varying time intervals, and highly evolving node features throughout their evolution. Hence, incorporating long-range dependencies from the historical graph context plays a crucial role in accurately learning their temporal dynamics. In this paper, we develop a graph embedding model with uncertainty quantification, TransformerG2G, by exploiting the advanced transformer encoder to first learn intermediate node representations from its current state ($t$) and previous context (over timestamps [$t-1, t-l$], $l$ is the length of context). Moreover, we employ two projection layers to generate lower-dimensional multivariate Gaussian distributions as each node's latent embedding at timestamp $t$. We consider diverse benchmarks with varying levels of ``novelty" as measured by the TEA plots. Our experiments demonstrate that the proposed TransformerG2G model outperforms conventional multi-step methods and our prior work (DynG2G) in terms of both link prediction accuracy and computational efficiency, especially for high degree of novelty. Furthermore, the learned time-dependent attention weights across multiple graph snapshots reveal the development of an automatic adaptive time stepping enabled by the transformer. Importantly, by examining the attention weights, we can uncover temporal dependencies, identify influential elements, and gain insights into the complex interactions within the graph structure. For example, we identified a strong correlation between attention weights and node degree at the various stages of the graph topology evolution.
</details>
<details>
<summary>摘要</summary>
“动态图 embedding 技术在许多应用中成为了非常有效的方法，用于解决多种时间图分析任务（如链接预测、节点分类、推荐系统、异常检测和图生成）。这些时间图具有不同的时间间隔和变化快速的节点特征。因此，在学习时间图动态的过程中，需要考虑长距离的历史图 conte xt。在这篇论文中，我们开发了一种图 embedding 模型，名为 TransformerG2G，通过利用高级变换 encoder 来首先从当前状态（$t$）和上一个时间步（[$t-1$, $t-l$）中学习中间节点表示。此外，我们采用了两层投影层来生成每个节点的几个维度的多元 Gaussian 分布作为其秘密嵌入。我们在多种不同的“新鲜度”测试（根据 TEA 图表）中进行了多种 benchmark，结果显示，我们的提案的 TransformerG2G 模型在链接预测精度和计算效率方面都高于传统的多步方法和我们之前的 DynG2G 模型，特别是在高度新鲜度时。此外，我们通过分析注意力权重来揭示时间图中的时间依赖关系，找到影响因子和强调关系，并从图结构中获得有价值的信息。例如，我们发现了节点度和注意力权重之间的强相关关系在不同的图结构发展阶段。”
</details></li>
</ul>
<hr>
<h2 id="Multimodal-Temporal-Fusion-Transformers-Are-Good-Product-Demand-Forecasters"><a href="#Multimodal-Temporal-Fusion-Transformers-Are-Good-Product-Demand-Forecasters" class="headerlink" title="Multimodal Temporal Fusion Transformers Are Good Product Demand Forecasters"></a>Multimodal Temporal Fusion Transformers Are Good Product Demand Forecasters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02578">http://arxiv.org/abs/2307.02578</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maarten Sukel, Stevan Rudinac, Marcel Worring</li>
<li>for: 预测产品需求，解决冷启动问题和类别动态问题。</li>
<li>methods: 使用 convolutional 网络、图Structured 网络和 transformer 网络，将多Modal 信息（图像和文本描述）与历史需求、类别信息和时序信息结合使用。</li>
<li>results: 在大规模实际数据集上进行实验，提出的方法可以有效地预测各种产品的需求，并且与传统方法相比，具有更高的准确率和可靠性。<details>
<summary>Abstract</summary>
Multimodal demand forecasting aims at predicting product demand utilizing visual, textual, and contextual information. This paper proposes a method for multimodal product demand forecasting using convolutional, graph-based, and transformer-based architectures. Traditional approaches to demand forecasting rely on historical demand, product categories, and additional contextual information such as seasonality and events. However, these approaches have several shortcomings, such as the cold start problem making it difficult to predict product demand until sufficient historical data is available for a particular product, and their inability to properly deal with category dynamics. By incorporating multimodal information, such as product images and textual descriptions, our architecture aims to address the shortcomings of traditional approaches and outperform them. The experiments conducted on a large real-world dataset show that the proposed approach effectively predicts demand for a wide range of products. The multimodal pipeline presented in this work enhances the accuracy and reliability of the predictions, demonstrating the potential of leveraging multimodal information in product demand forecasting.
</details>
<details>
<summary>摘要</summary>
<trans_type>simplified_chinese</trans_type><text>多modal需求预测目标在于预测产品需求使用视觉、文本和上下文信息。这篇论文提出一种基于卷积、图гра数据结构和变换器结构的多modal产品需求预测方法。传统的需求预测方法依靠历史需求、产品类别以及附加的上下文信息如季节和事件。然而，这些方法有几个缺陷，如冷启动问题，使得预测产品需求 Until sufficient historical data is available for a particular product 困难，以及它们对类别动态不够好地处理。通过把多modal信息，如产品图像和文本描述，纳入到方法中，我们的建议方法可以解决传统方法的缺陷，并超越它们。实验在大量实际数据集上显示，我们的方法可以有效预测各种产品的需求。在这篇论文中提出的多modal管道可以增强预测的准确性和可靠性， thereby demonstrating the potential of leveraging multimodal information in product demand forecasting.</text></sys>
</details></li>
</ul>
<hr>
<h2 id="Several-categories-of-Large-Language-Models-LLMs-A-Short-Survey"><a href="#Several-categories-of-Large-Language-Models-LLMs-A-Short-Survey" class="headerlink" title="Several categories of Large Language Models (LLMs): A Short Survey"></a>Several categories of Large Language Models (LLMs): A Short Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10188">http://arxiv.org/abs/2307.10188</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saurabh Pahune, Manoj Chandrasekharan</li>
<li>for: 这个论文的目的是为聊天机器人和虚拟智能助手技术提供有用的信息和未来方向。</li>
<li>methods: 这篇论文涵盖了不同类型的大语言模型（LLM），包括任务基金经济LLM、多语言语言LLM、医学和生物医学LLM、视觉语言LLM以及代码语言模型。它还描述了这些类型的LLM使用的方法、特点、数据集、变换器模型和比较度量。</li>
<li>results: 这篇论文总结了各类LLM的研究进展和努力，包括任务基金经济LLM、多语言语言LLM、医学和生物医学LLM、视觉语言LLM以及代码语言模型。它还强调了聊天机器人和虚拟智能助手技术的未解决问题，如提高自然语言处理、提高聊天机器人智能和解决道德和法律问题。<details>
<summary>Abstract</summary>
Large Language Models(LLMs)have become effective tools for natural language processing and have been used in many different fields. This essay offers a succinct summary of various LLM subcategories. The survey emphasizes recent developments and efforts made for various LLM kinds, including task-based financial LLMs, multilingual language LLMs, biomedical and clinical LLMs, vision language LLMs, and code language models. The survey gives a general summary of the methods, attributes, datasets, transformer models, and comparison metrics applied in each category of LLMs. Furthermore, it highlights unresolved problems in the field of developing chatbots and virtual assistants, such as boosting natural language processing, enhancing chatbot intelligence, and resolving moral and legal dilemmas. The purpose of this study is to provide readers, developers, academics, and users interested in LLM-based chatbots and virtual intelligent assistant technologies with useful information and future directions.
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLM）已成为自然语言处理的有效工具，广泛应用于多个领域。本文提供LLM各种子类划分的简洁概述，强调最新的发展和努力。包括任务基金 languages LLMs, 多语言语言 LLMs, 医疗和临床 LLMs, 视觉语言 LLMs, 和代码语言模型。本文介绍每种LLM类型的方法、特点、数据集、转换器模型和比较指标。此外，它还抛光了虚拟助手和智能客服技术的未解决问题，如提高自然语言处理、增强聊天机器人智能和解决道德和法律问题。本文的目的是为有关LLM基于聊天机器人和虚拟智能助手技术的读者、开发者、学者和用户提供有用信息和未来方向。
</details></li>
</ul>
<hr>
<h2 id="How-accurate-are-existing-land-cover-maps-for-agriculture-in-Sub-Saharan-Africa"><a href="#How-accurate-are-existing-land-cover-maps-for-agriculture-in-Sub-Saharan-Africa" class="headerlink" title="How accurate are existing land cover maps for agriculture in Sub-Saharan Africa?"></a>How accurate are existing land cover maps for agriculture in Sub-Saharan Africa?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02575">http://arxiv.org/abs/2307.02575</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nasaharvest/crop-mask">https://github.com/nasaharvest/crop-mask</a></li>
<li>paper_authors: Hannah Kerner, Catherine Nakalembe, Adam Yang, Ivan Zvonkov, Ryan McWeeny, Gabriel Tseng, Inbal Becker-Reshef</li>
<li>for: 评估农业生产和粮食安全性</li>
<li>methods: 使用11个公共可用的土地覆盖图进行对比和评估，以确定最适合农业监测和评估的土地覆盖图</li>
<li>results: 结果表明不同的土地覆盖图在不同国家和地区之间存在差异和低准确性，建议用户选择最适合自己需求的土地覆盖图，并促进未来的研究集中于解决图像间的差异和提高低准确性区域的准确性。<details>
<summary>Abstract</summary>
Satellite Earth observations (EO) can provide affordable and timely information for assessing crop conditions and food production. Such monitoring systems are essential in Africa, where there is high food insecurity and sparse agricultural statistics. EO-based monitoring systems require accurate cropland maps to provide information about croplands, but there is a lack of data to determine which of the many available land cover maps most accurately identify cropland in African countries. This study provides a quantitative evaluation and intercomparison of 11 publicly available land cover maps to assess their suitability for cropland classification and EO-based agriculture monitoring in Africa using statistically rigorous reference datasets from 8 countries. We hope the results of this study will help users determine the most suitable map for their needs and encourage future work to focus on resolving inconsistencies between maps and improving accuracy in low-accuracy regions.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Semi-supervised-Learning-from-Street-View-Images-and-OpenStreetMap-for-Automatic-Building-Height-Estimation"><a href="#Semi-supervised-Learning-from-Street-View-Images-and-OpenStreetMap-for-Automatic-Building-Height-Estimation" class="headerlink" title="Semi-supervised Learning from Street-View Images and OpenStreetMap for Automatic Building Height Estimation"></a>Semi-supervised Learning from Street-View Images and OpenStreetMap for Automatic Building Height Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02574">http://arxiv.org/abs/2307.02574</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bobleegogogo/building_height">https://github.com/bobleegogogo/building_height</a></li>
<li>paper_authors: Hao Li, Zhendong Yuan, Gabriel Dax, Gefei Kong, Hongchao Fan, Alexander Zipf, Martin Werner</li>
<li>for: 这种研究的目的是为了提供一种自动将建筑高度从低成本的地理信息数据中提取出来的方法，以便在生成低成本和开源的3D城市模型中使用。</li>
<li>methods: 本研究使用了 semi-supervised learning（SSL）方法，包括提出一种SSLSchema，并使用多层形态特征从OSM数据中提取建筑高度的信息。</li>
<li>results: 在test dataset中，使用Random Forest（RF）、Support Vector Machine（SVM）和Convolutional Neural Network（CNN）三种不同的回归模型，SSL方法在优化建筑高度估计中带来了明显的性能提升，MAE约为2.1米，与现有方法相比具有竞争力。<details>
<summary>Abstract</summary>
Accurate building height estimation is key to the automatic derivation of 3D city models from emerging big geospatial data, including Volunteered Geographical Information (VGI). However, an automatic solution for large-scale building height estimation based on low-cost VGI data is currently missing. The fast development of VGI data platforms, especially OpenStreetMap (OSM) and crowdsourced street-view images (SVI), offers a stimulating opportunity to fill this research gap. In this work, we propose a semi-supervised learning (SSL) method of automatically estimating building height from Mapillary SVI and OSM data to generate low-cost and open-source 3D city modeling in LoD1. The proposed method consists of three parts: first, we propose an SSL schema with the option of setting a different ratio of "pseudo label" during the supervised regression; second, we extract multi-level morphometric features from OSM data (i.e., buildings and streets) for the purposed of inferring building height; last, we design a building floor estimation workflow with a pre-trained facade object detection network to generate "pseudo label" from SVI and assign it to the corresponding OSM building footprint. In a case study, we validate the proposed SSL method in the city of Heidelberg, Germany and evaluate the model performance against the reference data of building heights. Based on three different regression models, namely Random Forest (RF), Support Vector Machine (SVM), and Convolutional Neural Network (CNN), the SSL method leads to a clear performance boosting in estimating building heights with a Mean Absolute Error (MAE) around 2.1 meters, which is competitive to state-of-the-art approaches. The preliminary result is promising and motivates our future work in scaling up the proposed method based on low-cost VGI data, with possibilities in even regions and areas with diverse data quality and availability.
</details>
<details>
<summary>摘要</summary>
正确的建筑高度估计是验证3D城市模型自大规模的地理空间数据中emerging的Automatic Derivation的关键，包括义务授权地理信息(VGI)。然而，一个基于低成本VGI数据的大规模建筑高度估计方法目前缺失。开放街图(OSM)和人群创建的街景图像(SVI)的快速发展提供了一个刺激的机会，以填补这个研究潜在。在这个工作中，我们提出了一个半监督学习(SSL)方法，用于自动从Mapillary SVI和OSM数据中估计建筑高度，以生成低成本和开源的3D城市建模（LoD1）。我们的方法包括三个部分：第一，我们提出了SSL schema，让使用者在监督回归中设置不同的"伪标签"比率；第二，我们从OSM数据中提取多层次形态特征，以估计建筑高度；第三，我们设计了一个建筑楼层估计工作流程，使用预训练的外观物体检测网络来从SVI中生成"伪标签"，并将其分配到相应的OSM建筑基本面。在一个 Heidelberg 城市的应用中，我们认为SSL方法可以明显提高建筑高度估计的性能，MAE约2.1米，与现有方法竞争。这个初步结果将验证我们未来将基于低成本VGI数据扩展提案，包括不同的数据质量和可用性。
</details></li>
</ul>
<hr>
<h2 id="Conditional-Korhunen-Loeve-regression-model-with-Basis-Adaptation-for-high-dimensional-problems-uncertainty-quantification-and-inverse-modeling"><a href="#Conditional-Korhunen-Loeve-regression-model-with-Basis-Adaptation-for-high-dimensional-problems-uncertainty-quantification-and-inverse-modeling" class="headerlink" title="Conditional Korhunen-Loéve regression model with Basis Adaptation for high-dimensional problems: uncertainty quantification and inverse modeling"></a>Conditional Korhunen-Loéve regression model with Basis Adaptation for high-dimensional problems: uncertainty quantification and inverse modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02572">http://arxiv.org/abs/2307.02572</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu-Hong Yeung, Ramakrishna Tipireddy, David A. Barajas-Solano, Alexandre M. Tartakovsky<br>for: 这个论文的目的是提高Physical Systems中对可观测Response的模拟精度，特别是在高维问题中。methods: 该论文使用 truncated unconditional Karhunen-Lo&#39;{e}ve expansions (KLEs) 和 conditional Karhunen-Lo&#39;{e}ve expansions (CKLEs) 来构建模拟模型，并通过 Gaussian process regression 来conditioning the covariance kernel of the unconditional expansion on the direct measurements。results: 该论文的结果表明，基于CKLEs的BA模拟模型在forward uncertainty quantification tasks中比基于unconditional expansions的BA模拟模型更加精度。此外，使用CKLE-based BA模拟模型进行 inverse estimation of hydraulic transmissivity field 的结果也比使用unconditional BA模拟模型更加准确。<details>
<summary>Abstract</summary>
We propose a methodology for improving the accuracy of surrogate models of the observable response of physical systems as a function of the systems' spatially heterogeneous parameter fields with applications to uncertainty quantification and parameter estimation in high-dimensional problems. Practitioners often formulate finite-dimensional representations of spatially heterogeneous parameter fields using truncated unconditional Karhunen-Lo\'{e}ve expansions (KLEs) for a certain choice of unconditional covariance kernel and construct surrogate models of the observable response with respect to the random variables in the KLE. When direct measurements of the parameter fields are available, we propose improving the accuracy of these surrogate models by representing the parameter fields via conditional Karhunen-Lo\'{e}ve expansions (CKLEs). CKLEs are constructed by conditioning the covariance kernel of the unconditional expansion on the direct measurements via Gaussian process regression and then truncating the corresponding KLE. We apply the proposed methodology to constructing surrogate models via the Basis Adaptation (BA) method of the stationary hydraulic head response, measured at spatially discrete observation locations, of a groundwater flow model of the Hanford Site, as a function of the 1,000-dimensional representation of the model's log-transmissivity field. We find that BA surrogate models of the hydraulic head based on CKLEs are more accurate than BA surrogate models based on unconditional expansions for forward uncertainty quantification tasks. Furthermore, we find that inverse estimates of the hydraulic transmissivity field computed using CKLE-based BA surrogate models are more accurate than those computed using unconditional BA surrogate models.
</details>
<details>
<summary>摘要</summary>
我们提出一种方法来提高非模拟模型 observable response 的准确性，该 observable response 是基于物理系统的空间各个参数场的函数。实际工作者通常使用 truncated unconditional Karhunen-Lo\'{e}ve expansion (KLE) 来形式化空间各个参数场，然后使用这些 KLE 来构建非模拟模型。当直接测量 parameter 场可用时，我们提议使用 conditional Karhunen-Lo\'{e}ve expansion (CKLE) 来改进非模拟模型的准确性。CKLE 通过 conditioning covariance kernel 的 unconditional expansion 于直接测量，使用 Gaussian process regression  truncate 相应的 KLE。我们应用该方法ологи到 constructing 非模拟模型 via Basis Adaptation (BA) 方法， Specifically, we use the BA method to construct surrogate models of the stationary hydraulic head response of a groundwater flow model of the Hanford Site, as a function of the 1,000-dimensional representation of the model's log-transmissivity field. Our results show that BA surrogate models based on CKLEs are more accurate than BA surrogate models based on unconditional expansions for forward uncertainty quantification tasks. Furthermore, we find that inverse estimates of the hydraulic transmissivity field computed using CKLE-based BA surrogate models are more accurate than those computed using unconditional BA surrogate models.
</details></li>
</ul>
<hr>
<h2 id="LongNet-Scaling-Transformers-to-1-000-000-000-Tokens"><a href="#LongNet-Scaling-Transformers-to-1-000-000-000-Tokens" class="headerlink" title="LongNet: Scaling Transformers to 1,000,000,000 Tokens"></a>LongNet: Scaling Transformers to 1,000,000,000 Tokens</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02486">http://arxiv.org/abs/2307.02486</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/unilm">https://github.com/microsoft/unilm</a></li>
<li>paper_authors: Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning Zheng, Furu Wei</li>
<li>for: 长度较长的序列处理，例如整个文库或互联网序列。</li>
<li>methods: 提出了扩散注意力的方法，可以将注意力场展开到远距离处，从而提高序列处理的效率和范围。</li>
<li>results: 实验结果表明，LongNet可以在长序列模型和通用语言任务上达到强表现，并且可以serve为分布式训练器。<details>
<summary>Abstract</summary>
Scaling sequence length has become a critical demand in the era of large language models. However, existing methods struggle with either computational complexity or model expressivity, rendering the maximum sequence length restricted. To address this issue, we introduce LongNet, a Transformer variant that can scale sequence length to more than 1 billion tokens, without sacrificing the performance on shorter sequences. Specifically, we propose dilated attention, which expands the attentive field exponentially as the distance grows. LongNet has significant advantages: 1) it has a linear computation complexity and a logarithm dependency between any two tokens in a sequence; 2) it can be served as a distributed trainer for extremely long sequences; 3) its dilated attention is a drop-in replacement for standard attention, which can be seamlessly integrated with the existing Transformer-based optimization. Experiments results demonstrate that LongNet yields strong performance on both long-sequence modeling and general language tasks. Our work opens up new possibilities for modeling very long sequences, e.g., treating a whole corpus or even the entire Internet as a sequence.
</details>
<details>
<summary>摘要</summary>
<<SYS>>TRANSLATE_TEXTsequence length scaling has become a critical demand in the era of large language models. however, existing methods struggle with either computational complexity or model expressivity, rendering the maximum sequence length restricted. to address this issue, we introduce longnet, a transformer variant that can scale sequence length to more than 1 billion tokens, without sacrificing the performance on shorter sequences. specifically, we propose dilated attention, which expands the attentive field exponentially as the distance grows. longnet has significant advantages: 1) it has a linear computation complexity and a logarithm dependency between any two tokens in a sequence; 2) it can be served as a distributed trainer for extremely long sequences; 3) its dilated attention is a drop-in replacement for standard attention, which can be seamlessly integrated with the existing transformer-based optimization. experiments results demonstrate that longnet yields strong performance on both long-sequence modeling and general language tasks. our work opens up new possibilities for modeling very long sequences, e.g., treating a whole corpus or even the entire internet as a sequence.TRANSLATE_TEXT
</details></li>
</ul>
<hr>
<h2 id="Elastic-Decision-Transformer"><a href="#Elastic-Decision-Transformer" class="headerlink" title="Elastic Decision Transformer"></a>Elastic Decision Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02484">http://arxiv.org/abs/2307.02484</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/danderfer/Comp_Sci_Sem_2">https://github.com/danderfer/Comp_Sci_Sem_2</a></li>
<li>paper_authors: Yueh-Hua Wu, Xiaolong Wang, Masashi Hamaya</li>
<li>for: 提出了一种基于决策变换器的扩展方法，以解决DT和其变种在生成优化路径时的问题。</li>
<li>methods: 利用了DT的历史记录来实现 trajectory stitching 在测试时，并在不同情况下保留不同的历史记录，以优化路径。</li>
<li>results: 在D4RL locomotor benchmark和Atari游戏中，EDT比基于Q学习的方法表现更好，特别是在多任务情况下。Translation:</li>
<li>for: The paper proposes an extension of the Decision Transformer (DT) to solve the problem of trajectory stitching during action inference.</li>
<li>methods: The proposed method uses DT’s history record to implement trajectory stitching at test time, and retains different history records in different situations to optimize the path.</li>
<li>results: In the D4RL locomotor benchmark and Atari games, EDT outperforms Q-learning-based methods, especially in multi-task scenarios.<details>
<summary>Abstract</summary>
This paper introduces Elastic Decision Transformer (EDT), a significant advancement over the existing Decision Transformer (DT) and its variants. Although DT purports to generate an optimal trajectory, empirical evidence suggests it struggles with trajectory stitching, a process involving the generation of an optimal or near-optimal trajectory from the best parts of a set of sub-optimal trajectories. The proposed EDT differentiates itself by facilitating trajectory stitching during action inference at test time, achieved by adjusting the history length maintained in DT. Further, the EDT optimizes the trajectory by retaining a longer history when the previous trajectory is optimal and a shorter one when it is sub-optimal, enabling it to "stitch" with a more optimal trajectory. Extensive experimentation demonstrates EDT's ability to bridge the performance gap between DT-based and Q Learning-based approaches. In particular, the EDT outperforms Q Learning-based methods in a multi-task regime on the D4RL locomotion benchmark and Atari games. Videos are available at: https://kristery.github.io/edt/
</details>
<details>
<summary>摘要</summary>
EDT addresses this issue by stitching trajectories during action inference at test time, achieved by adjusting the history length maintained in DT. EDT optimizes the trajectory by retaining a longer history when the previous trajectory is optimal and a shorter one when it is sub-optimal, allowing it to "stitch" with a more optimal trajectory.Experiments show that EDT can bridge the performance gap between DT-based and Q Learning-based approaches. Specifically, EDT outperforms Q Learning-based methods in a multi-task regime on the D4RL locomotion benchmark and Atari games. Videos are available at: <https://kristery.github.io/edt/>.
</details></li>
</ul>
<hr>
<h2 id="Jailbroken-How-Does-LLM-Safety-Training-Fail"><a href="#Jailbroken-How-Does-LLM-Safety-Training-Fail" class="headerlink" title="Jailbroken: How Does LLM Safety Training Fail?"></a>Jailbroken: How Does LLM Safety Training Fail?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02483">http://arxiv.org/abs/2307.02483</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander Wei, Nika Haghtalab, Jacob Steinhardt</li>
<li>for: 本研究旨在探讨大语言模型受到攻击的原因和如何创造攻击。</li>
<li>methods: 研究人员使用了两种故障模式来引导攻击设计：竞合目标和欠拟合泛化。</li>
<li>results: 研究发现，即使使用了广泛的红队训练和安全训练，现有的模型仍然存在漏洞，新的攻击方法可以在所有提问集中成功。<details>
<summary>Abstract</summary>
Large language models trained for safety and harmlessness remain susceptible to adversarial misuse, as evidenced by the prevalence of "jailbreak" attacks on early releases of ChatGPT that elicit undesired behavior. Going beyond recognition of the issue, we investigate why such attacks succeed and how they can be created. We hypothesize two failure modes of safety training: competing objectives and mismatched generalization. Competing objectives arise when a model's capabilities and safety goals conflict, while mismatched generalization occurs when safety training fails to generalize to a domain for which capabilities exist. We use these failure modes to guide jailbreak design and then evaluate state-of-the-art models, including OpenAI's GPT-4 and Anthropic's Claude v1.3, against both existing and newly designed attacks. We find that vulnerabilities persist despite the extensive red-teaming and safety-training efforts behind these models. Notably, new attacks utilizing our failure modes succeed on every prompt in a collection of unsafe requests from the models' red-teaming evaluation sets and outperform existing ad hoc jailbreaks. Our analysis emphasizes the need for safety-capability parity -- that safety mechanisms should be as sophisticated as the underlying model -- and argues against the idea that scaling alone can resolve these safety failure modes.
</details>
<details>
<summary>摘要</summary>
大型语言模型培养为安全和无害性的问题尚存在攻击风险，例如ChatGPT的早期发布版本的"监狱攻击"，这些攻击可以让模型表现出不жела的行为。我们不仅承认这个问题，还进一步调查这些攻击成功的原因和如何创造。我们假设两种安全培训失败情况：竞合目标和不同渠道的泛化。竞合目标发生在模型的能力和安全目标之间的冲突，而不同渠道的泛化发生在安全培训无法泛化到一个具有能力的领域。我们使用这些失败情况来引导监狱设计，然后评估当前最好的模型，包括OpenAI的GPT-4和Anthropic的Claude v1.3，对于现有和新设计的攻击。我们发现，即使这些模型进行了广泛的红色队伍和安全培训，漏洞仍然存在。尤其是，我们新设计的攻击可以在每个提示中成功，并且在模型的红色队伍评估集中的 unsafe requests 中表现出更好的效果。我们的分析强调了安全机制应该和模型的能力相似，而不是假设升级 alone 可以解决这些安全失败情况。
</details></li>
</ul>
<hr>
<h2 id="Conditional-independence-testing-under-model-misspecification"><a href="#Conditional-independence-testing-under-model-misspecification" class="headerlink" title="Conditional independence testing under model misspecification"></a>Conditional independence testing under model misspecification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02520">http://arxiv.org/abs/2307.02520</a></li>
<li>repo_url: None</li>
<li>paper_authors: Felipe Maia Polo, Yuekai Sun, Moulinath Banerjee</li>
<li>for: 本文研究了模型错误的情况下 regression-based 独立性测试的性能。</li>
<li>methods: 本文提出了三种 regression-based 测试方法的新上界或近似值，以及一种robust against model misspecification的新测试方法——Rao-Blackwellized Predictor Test (RBPT)。</li>
<li>results: 实验结果表明，RBPT 可以在模型错误情况下提供更好的测试性能，而且可以与现有的测试方法进行比较。<details>
<summary>Abstract</summary>
Conditional independence (CI) testing is fundamental and challenging in modern statistics and machine learning. Many modern methods for CI testing rely on powerful supervised learning methods to learn regression functions or Bayes predictors as an intermediate step. Although the methods are guaranteed to control Type-I error when the supervised learning methods accurately estimate the regression functions or Bayes predictors, their behavior is less understood when they fail due to model misspecification. In a broader sense, model misspecification can arise even when universal approximators (e.g., deep neural nets) are employed. Then, we study the performance of regression-based CI tests under model misspecification. Namely, we propose new approximations or upper bounds for the testing errors of three regression-based tests that depend on misspecification errors. Moreover, we introduce the Rao-Blackwellized Predictor Test (RBPT), a novel regression-based CI test robust against model misspecification. Finally, we conduct experiments with artificial and real data, showcasing the usefulness of our theory and methods.
</details>
<details>
<summary>摘要</summary>
conditional independence (CI) 测试是现代统计和机器学习中的基础和挑战。许多现代CI测试方法 rely on 强大的指导学习方法来学习回归函数或 bayes 预测函数作为中间步骤。虽然这些方法能够控制类型一错误，但它们在模型误差时表现不准确。在更广泛的意义上，模型误差可以出现，即使使用 universial approximators（例如深度神经网）。我们研究了 regression-based CI 测试下模型误差的性能。即，我们提出了新的近似或上限值 для三种 regression-based 测试的测试误差，其中受到模型误差的影响。此外，我们引入了 Rao-Blackwellized Predictor Test（RBPT），一种robust against model misspecification的新的 regression-based CI 测试。最后，我们在人工和实际数据上进行了实验，展示了我们的理论和方法的实用性。
</details></li>
</ul>
<hr>
<h2 id="Linear-Regression-on-Manifold-Structured-Data-the-Impact-of-Extrinsic-Geometry-on-Solutions"><a href="#Linear-Regression-on-Manifold-Structured-Data-the-Impact-of-Extrinsic-Geometry-on-Solutions" class="headerlink" title="Linear Regression on Manifold Structured Data: the Impact of Extrinsic Geometry on Solutions"></a>Linear Regression on Manifold Structured Data: the Impact of Extrinsic Geometry on Solutions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02478">http://arxiv.org/abs/2307.02478</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liangchen Liu, Juncai He, Richard Tsai</li>
<li>for: 这个论文研究了线性回归在数据构造在 manifold 上的应用。</li>
<li>methods: 作者们使用了 manifold 的嵌入空间的欧几何学空间来分析数据构造的外在 geometry 对回归的影响。</li>
<li>results: 研究发现，当数据构造在某些维度上是平坦的时候，线性回归无Unique解。而在其他维度上，拓扑 manifold 的 curvature（或者参数化时的高阶非线性）会对回归解决做出重要贡献，特别是在沿着 manifold 的正常方向解决。这些发现表明了数据构造geometry对回归模型的稳定性具有重要作用。<details>
<summary>Abstract</summary>
In this paper, we study linear regression applied to data structured on a manifold. We assume that the data manifold is smooth and is embedded in a Euclidean space, and our objective is to reveal the impact of the data manifold's extrinsic geometry on the regression. Specifically, we analyze the impact of the manifold's curvatures (or higher order nonlinearity in the parameterization when the curvatures are locally zero) on the uniqueness of the regression solution. Our findings suggest that the corresponding linear regression does not have a unique solution when the embedded submanifold is flat in some dimensions. Otherwise, the manifold's curvature (or higher order nonlinearity in the embedding) may contribute significantly, particularly in the solution associated with the normal directions of the manifold. Our findings thus reveal the role of data manifold geometry in ensuring the stability of regression models for out-of-distribution inferences.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了线性回归在数据构造在拟合空间上的应用。我们假设数据构造是光滑的，并且嵌入在几何空间中，我们的目标是探讨数据构造的外部几何特性对回归的影响。Specifically，我们分析了数据构造的曲率（或者在参数化时的高阶非线性）对回归解决uniqueness的影响。我们发现当托管的子拟合空间在一些维度上是平的时，相应的线性回归并没有唯一的解。否则，拟合空间的曲率（或者参数化时的高阶非线性）可能会对解决做出重要贡献，特别是在与拟合空间的正常方向相关的解决方案中。我们的发现表明了数据构造几何的作用在外部归一致推断中的稳定性。
</details></li>
</ul>
<hr>
<h2 id="Performance-Scaling-via-Optimal-Transport-Enabling-Data-Selection-from-Partially-Revealed-Sources"><a href="#Performance-Scaling-via-Optimal-Transport-Enabling-Data-Selection-from-Partially-Revealed-Sources" class="headerlink" title="Performance Scaling via Optimal Transport: Enabling Data Selection from Partially Revealed Sources"></a>Performance Scaling via Optimal Transport: Enabling Data Selection from Partially Revealed Sources</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02460">http://arxiv.org/abs/2307.02460</a></li>
<li>repo_url: None</li>
<li>paper_authors: Feiyang Kang, Hoang Anh Just, Anit Kumar Sahu, Ruoxi Jia<br>for: This paper aims to improve the process of data selection for machine learning models by proposing a framework called <projektor>. The paper focuses on the scenario where only a limited subset of samples are available before an acquisition decision is made.methods: The proposed framework uses a two-stage performance inference process, which includes:1. Leveraging the Optimal Transport distance to predict the model’s performance for any data mixture ratio within the range of disclosed data sizes.2. Extrapolating the performance to larger undisclosed data sizes based on a novel parameter-free mapping technique inspired by neural scaling laws.results: The paper demonstrates that <projektor> significantly improves existing performance scaling approaches in terms of both the accuracy of performance inference and the computation costs associated with constructing the performance predictor. Additionally, <projektor> outperforms other off-the-shelf solutions in data selection effectiveness.Here is the information in Simplified Chinese text:for: 这 paper 的目的是改进机器学习模型选择数据的过程，提出了一个名为 <projektor> 的框架。 paper 关注到只有有限数据样本可用于决策前的情况。methods: <projektor> 使用的是一个两个阶段性表现预测过程，包括：1. 利用 Optimal Transport 距离来预测模型在任何数据混合比例范围内的性能。2. 基于一种新的无参数映射技术，使用神经Scaling laws 来推广表现到更大的未知数据大小。results: paper 显示，<projektor> 可以significantly 改进现有的表现推断方法，包括表现预测精度和构建表现预测器所需的计算成本。另外，<projektor> 也可以很大程度上超越其他的OFF-the-SHELF 解决方案在数据选择效果上。<details>
<summary>Abstract</summary>
Traditionally, data selection has been studied in settings where all samples from prospective sources are fully revealed to a machine learning developer. However, in practical data exchange scenarios, data providers often reveal only a limited subset of samples before an acquisition decision is made. Recently, there have been efforts to fit scaling laws that predict model performance at any size and data source composition using the limited available samples. However, these scaling functions are black-box, computationally expensive to fit, highly susceptible to overfitting, or/and difficult to optimize for data selection. This paper proposes a framework called <projektor>, which predicts model performance and supports data selection decisions based on partial samples of prospective data sources. Our approach distinguishes itself from existing work by introducing a novel *two-stage* performance inference process. In the first stage, we leverage the Optimal Transport distance to predict the model's performance for any data mixture ratio within the range of disclosed data sizes. In the second stage, we extrapolate the performance to larger undisclosed data sizes based on a novel parameter-free mapping technique inspired by neural scaling laws. We further derive an efficient gradient-based method to select data sources based on the projected model performance. Evaluation over a diverse range of applications demonstrates that <projektor> significantly improves existing performance scaling approaches in terms of both the accuracy of performance inference and the computation costs associated with constructing the performance predictor. Also, <projektor> outperforms by a wide margin in data selection effectiveness compared to a range of other off-the-shelf solutions.
</details>
<details>
<summary>摘要</summary>
传统上，数据选择已经被研究在所有样本都是完全公开给机器学习开发人员的设置下。然而，在实际数据交换场景下，数据提供者通常只披露一个有限的子集的样本前于收购决策。最近，有人尝试使用有限可用样本来预测模型性能的拓扑函数。然而，这些拓扑函数是黑盒子，计算成本高，易于过拟合，或者困难优化数据选择。这篇论文提出了一个名为<projektor>的框架，可以根据部分样本预测模型性能并支持数据选择决策。我们的方法与现有工作不同，我们引入了一种新的两stage表现预测过程。在第一个阶段，我们利用最优运输距离来预测模型在任何数据混合比例范围内的性能。在第二个阶段，我们通过一种新的无参数映射技术，基于神经拓扑法则，来推断模型在未知大小的数据上的性能。我们还提出了一种高效的梯度下降方法来选择数据源基于预测模型性能。对于多种应用程序的评估表明，<projektor>在性能拓扑预测和构建表现预测器的计算成本方面都有显著改进，而且在数据选择效果上也高效过很多其他OFFTHE-SHELF解决方案。
</details></li>
</ul>
<hr>
<h2 id="Gaussian-Database-Alignment-and-Gaussian-Planted-Matching"><a href="#Gaussian-Database-Alignment-and-Gaussian-Planted-Matching" class="headerlink" title="Gaussian Database Alignment and Gaussian Planted Matching"></a>Gaussian Database Alignment and Gaussian Planted Matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02459">http://arxiv.org/abs/2307.02459</a></li>
<li>repo_url: None</li>
<li>paper_authors: Osman Emre Dai, Daniel Cullina, Negar Kiyavash</li>
<li>for: 这种研究的目的是解决数据库对照问题，即给定两个匿名化的数据库，找到它们之间的相似性并将它们对应地进行对照。</li>
<li>methods: 这种问题与植入匹配问题相关，即给定一个大图，找到该图中的匹配。作者们使用了最大似然方法来解决这种问题，该方法是一个线性程序。</li>
<li>results: 作者们发现，当数据库特征维度为ω（log n）时，对照性的性能阈值与植入匹配阈值相同。此外，作者们还研究了各种约束的放松以更好地理解它们在不同情况下的效果，并提供了可达性和反向下界 bounds。<details>
<summary>Abstract</summary>
Database alignment is a variant of the graph alignment problem: Given a pair of anonymized databases containing separate yet correlated features for a set of users, the problem is to identify the correspondence between the features and align the anonymized user sets based on correlation alone. This closely relates to planted matching, where given a bigraph with random weights, the goal is to identify the underlying matching that generated the given weights. We study an instance of the database alignment problem with multivariate Gaussian features and derive results that apply both for database alignment and for planted matching, demonstrating the connection between them. The performance thresholds for database alignment converge to that for planted matching when the dimensionality of the database features is \(\omega(\log n)\), where \(n\) is the size of the alignment, and no individual feature is too strong. The maximum likelihood algorithms for both planted matching and database alignment take the form of a linear program and we study relaxations to better understand the significance of various constraints under various conditions and present achievability and converse bounds. Our results show that the almost-exact alignment threshold for the relaxed algorithms coincide with that of maximum likelihood, while there is a gap between the exact alignment thresholds. Our analysis and results extend to the unbalanced case where one user set is not fully covered by the alignment.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate "Database alignment is a variant of the graph alignment problem: Given a pair of anonymized databases containing separate yet correlated features for a set of users, the problem is to identify the correspondence between the features and align the anonymized user sets based on correlation alone. This closely relates to planted matching, where given a bigraph with random weights, the goal is to identify the underlying matching that generated the given weights. We study an instance of the database alignment problem with multivariate Gaussian features and derive results that apply both for database alignment and for planted matching, demonstrating the connection between them. The performance thresholds for database alignment converge to that for planted matching when the dimensionality of the database features is ω(log n), where n is the size of the alignment, and no individual feature is too strong. The maximum likelihood algorithms for both planted matching and database alignment take the form of a linear program and we study relaxations to better understand the significance of various constraints under various conditions and present achievability and converse bounds. Our results show that the almost-exact alignment threshold for the relaxed algorithms coincide with that of maximum likelihood, while there is a gap between the exact alignment thresholds. Our analysis and results extend to the unbalanced case where one user set is not fully covered by the alignment."into Simplified Chinese. Here's the translation:数据库对应问题是图像问题的变种：给定一对匿名化的数据库，它们包含用户特征的分离 yet 相关的特征，问题是将这些特征对应起来，并将匿名化用户集相互对应。这与植入匹配问题密切相关，给定一个大Graph with random weights，问题是找出该大Graph中的匹配。我们研究了一个包含多变量 Gaussian 特征的数据库对应问题，并 derive 结果适用于数据库对应和植入匹配。我们发现当数据库特征维度为 ω(log n) 时，对应性reshold 与植入匹配问题的性能reshold 相同，其中 n 是对应的大小。此外，我们还发现了在不同条件下不同约束的放松关系，并 analyze 其可行性和反向关系。我们的结果表明，在放松问题中，准确对应的阈值与最大可能性问题的阈值相同，但是准确对应的阈值与最大可能性问题的阈值之间存在差异。我们的分析和结果还扩展到不均衡的情况，其中一个用户集不完全覆盖对应。
</details></li>
</ul>
<hr>
<h2 id="Transgressing-the-boundaries-towards-a-rigorous-understanding-of-deep-learning-and-its-non-robustness"><a href="#Transgressing-the-boundaries-towards-a-rigorous-understanding-of-deep-learning-and-its-non-robustness" class="headerlink" title="Transgressing the boundaries: towards a rigorous understanding of deep learning and its (non-)robustness"></a>Transgressing the boundaries: towards a rigorous understanding of deep learning and its (non-)robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02454">http://arxiv.org/abs/2307.02454</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carsten Hartmann, Lorenz Richter</li>
<li>for: This paper focuses on the robustness issues of deep learning (DL) and bridges concerns and attempts from approximation theory to statistical learning theory.</li>
<li>methods: The paper reviews Bayesian Deep Learning as a means for uncertainty quantification and rigorous explainability.</li>
<li>results: The paper provides a systematic mathematical approach to understanding the specifics of DL and its success in various applications.<details>
<summary>Abstract</summary>
The recent advances in machine learning in various fields of applications can be largely attributed to the rise of deep learning (DL) methods and architectures. Despite being a key technology behind autonomous cars, image processing, speech recognition, etc., a notorious problem remains the lack of theoretical understanding of DL and related interpretability and (adversarial) robustness issues. Understanding the specifics of DL, as compared to, say, other forms of nonlinear regression methods or statistical learning, is interesting from a mathematical perspective, but at the same time it is of crucial importance in practice: treating neural networks as mere black boxes might be sufficient in certain cases, but many applications require waterproof performance guarantees and a deeper understanding of what could go wrong and why it could go wrong. It is probably fair to say that, despite being mathematically well founded as a method to approximate complicated functions, DL is mostly still more like modern alchemy that is firmly in the hands of engineers and computer scientists. Nevertheless, it is evident that certain specifics of DL that could explain its success in applications demands systematic mathematical approaches. In this work, we review robustness issues of DL and particularly bridge concerns and attempts from approximation theory to statistical learning theory. Further, we review Bayesian Deep Learning as a means for uncertainty quantification and rigorous explainability.
</details>
<details>
<summary>摘要</summary>
Machine learning has made significant progress in various fields, thanks to the rise of deep learning (DL) methods and architectures. However, a major challenge remains the lack of theoretical understanding of DL, which hinders its widespread adoption. This review aims to provide an overview of the challenges in DL, specifically in the areas of interpretability and robustness, and discuss potential solutions.The Importance of Understanding DLDeep learning is a powerful tool for approximating complex functions, but its lack of theoretical understanding poses significant challenges. While it may be sufficient to treat neural networks as black boxes in some cases, many applications require rigorous performance guarantees and a deeper understanding of what can go wrong. This is particularly important in high-stakes applications such as autonomous vehicles, image processing, and speech recognition.Mathematical Foundations of DLDespite its success in applications, DL is still largely based on modern alchemy, with many of its underlying principles still not well understood. However, recent advances in approximation theory and statistical learning theory have provided valuable insights into the mathematical foundations of DL.Robustness Issues in DLOne of the major challenges in DL is its lack of robustness to adversarial attacks and other forms of input noise. This has serious implications for applications where DL models must be relied upon to make critical decisions. To address this challenge, researchers have proposed various techniques, such as adversarial training and input preprocessing.Bayesian Deep LearningBayesian deep learning (BDL) is a promising approach to addressing the challenges of DL. BDL combines the strengths of deep learning with the principles of Bayesian inference, providing a framework for uncertainty quantification and rigorous explainability. By incorporating prior knowledge and uncertainty into the DL framework, BDL offers a more robust and reliable approach to machine learning.ConclusionIn conclusion, deep learning has revolutionized the field of machine learning, but its lack of theoretical understanding poses significant challenges. By reviewing the robustness issues of DL and exploring the potential of BDL, we can develop more reliable and robust machine learning models that can be applied in a wide range of domains.
</details></li>
</ul>
<hr>
<h2 id="Hoodwinked-Deception-and-Cooperation-in-a-Text-Based-Game-for-Language-Models"><a href="#Hoodwinked-Deception-and-Cooperation-in-a-Text-Based-Game-for-Language-Models" class="headerlink" title="Hoodwinked: Deception and Cooperation in a Text-Based Game for Language Models"></a>Hoodwinked: Deception and Cooperation in a Text-Based Game for Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01404">http://arxiv.org/abs/2308.01404</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aogara-ds/hoodwinked">https://github.com/aogara-ds/hoodwinked</a></li>
<li>paper_authors: Aidan O’Gara</li>
<li>for: 这篇论文研究了现有语言模型是否能够识别和诈骗？作者们提出了一款基于文本游戏的测试方法，以便评估语言模型的干预能力。</li>
<li>methods: 作者们使用了GPT-3、GPT-3.5和GPT-4等语言模型，并在这些模型中控制了代理人。在游戏中，玩家需要通过自然语言对话和投票来决定是否banish其他玩家。</li>
<li>results: 研究发现，使用更高级别的语言模型可以更好地识别和诈骗。在18个比较中，更高级别的模型比较小的模型表现更好，并且在对话中使用更多的推理和说服技巧。<details>
<summary>Abstract</summary>
Are current language models capable of deception and lie detection? We study this question by introducing a text-based game called $\textit{Hoodwinked}$, inspired by Mafia and Among Us. Players are locked in a house and must find a key to escape, but one player is tasked with killing the others. Each time a murder is committed, the surviving players have a natural language discussion then vote to banish one player from the game. We conduct experiments with agents controlled by GPT-3, GPT-3.5, and GPT-4 and find evidence of deception and lie detection capabilities. The killer often denies their crime and accuses others, leading to measurable effects on voting outcomes. More advanced models are more effective killers, outperforming smaller models in 18 of 24 pairwise comparisons. Secondary metrics provide evidence that this improvement is not mediated by different actions, but rather by stronger persuasive skills during discussions. To evaluate the ability of AI agents to deceive humans, we make this game publicly available at h https://hoodwinked.ai/ .
</details>
<details>
<summary>摘要</summary>
现在的语言模型能够做出骗局和谎言吗？我们通过一款基于文本的游戏《骗子》（Hoodwinked）来研究这个问题，这款游戏灵感来自《贾布》和《 Among Us》。玩家被困在一个房子中，需要找到逃脱的钥匙，但有一个玩家被要求杀死其他玩家。每次杀人时，幸存的玩家们进行自然语言的讨论，然后投票将一名玩家从游戏中开除。我们在使用GPT-3、GPT-3.5和GPT-4控制的代理人进行实验，发现了骗局和谎言的能力。杀人者经常否认犯罪，指责其他人，导致讨论后投票结果受到影响。更高级的模型在18个比赛中赢得了18场，比较小的模型表现更差。次要指标表明这种改进不来自不同的行动，而是来自更强的说服技巧在讨论中。为了评估人工智能代理人是否能骗人类，我们将这款游戏公开发布在<https://hoodwinked.ai/>。
</details></li>
</ul>
<hr>
<h2 id="An-Exploratory-Literature-Study-on-Sharing-and-Energy-Use-of-Language-Models-for-Source-Code"><a href="#An-Exploratory-Literature-Study-on-Sharing-and-Energy-Use-of-Language-Models-for-Source-Code" class="headerlink" title="An Exploratory Literature Study on Sharing and Energy Use of Language Models for Source Code"></a>An Exploratory Literature Study on Sharing and Energy Use of Language Models for Source Code</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02443">http://arxiv.org/abs/2307.02443</a></li>
<li>repo_url: None</li>
<li>paper_authors: Max Hort, Anastasiia Grishina, Leon Moonen</li>
<li>for: 这个研究的主要目标是调查语言模型在软件工程任务上的训练和共享情况，以及训练过程中的能效性。</li>
<li>methods: 这个研究使用了雪崩式Literature搜索来找到语言模型在源代码上的应用，并分析这些应用的可重用性从可持续性的角度。</li>
<li>results: 研究发现，现有的494篇唯一出版物中，293篇 relevanter Publications使用语言模型解决源代码相关任务。其中，27% (79&#x2F;293)的研究将artifacts分享给他人 reuse。此外，研究还收集了训练过程中硬件使用情况以及训练时间，从而了解训练过程中的能效性。研究发现，当前的学术研究中有40%的论文没有分享源代码或训练过程中的artifacts，建议共享源代码和训练过程中的artifacts，以便实现可持续的可重用性。<details>
<summary>Abstract</summary>
Large language models trained on source code can support a variety of software development tasks, such as code recommendation and program repair. Large amounts of data for training such models benefit the models' performance. However, the size of the data and models results in long training times and high energy consumption. While publishing source code allows for replicability, users need to repeat the expensive training process if models are not shared. The main goal of the study is to investigate if publications that trained language models for software engineering (SE) tasks share source code and trained artifacts. The second goal is to analyze the transparency on training energy usage. We perform a snowballing-based literature search to find publications on language models for source code, and analyze their reusability from a sustainability standpoint.   From 494 unique publications, we identified 293 relevant publications that use language models to address code-related tasks. Among them, 27% (79 out of 293) make artifacts available for reuse. This can be in the form of tools or IDE plugins designed for specific tasks or task-agnostic models that can be fine-tuned for a variety of downstream tasks. Moreover, we collect insights on the hardware used for model training, as well as training time, which together determine the energy consumption of the development process. We find that there are deficiencies in the sharing of information and artifacts for current studies on source code models for software engineering tasks, with 40% of the surveyed papers not sharing source code or trained artifacts. We recommend the sharing of source code as well as trained artifacts, to enable sustainable reproducibility. Moreover, comprehensive information on training times and hardware configurations should be shared for transparency on a model's carbon footprint.
</details>
<details>
<summary>摘要</summary>
大型语言模型可以支持软件开发工作中的多种任务，例如代码建议和程式修理。大量训练数据可以提高模型的表现。然而，模型和数据的大小导致训练时间很长，能源消耗高。发布源代码可以保证可复制性，但用户需要重复进行昂贵的训练过程。研究的主要目标是探索发布了语言模型 для软件工程（SE）任务的文献是否共享源代码和训练遗产。第二个目标是分析训练时间的可视性。我们使用雪球搜寻法进行文献搜寻，并分析它们在可持续性方面的可重用性。从494份唯一的文献中，我们获得了293份相关的文献，用语言模型解决代码相关的任务。其中，27%（79份中的27份）公开了工具或IDE插件，这些工具可以用于特定任务或任务无关的模型，可以进行多种下游任务的微调。此外，我们收集了训练硬件和训练时间的信息，这些信息共同决定模型训练过程中的能源消耗。我们发现现有的研究中有40%的文献不会共享源代码或训练遗产，我们建议共享源代码以及训练遗产，以实现可复制性。此外，我们建议分享训练时间和硬件配置的详细信息，以便透明度地描述模型的碳足迹。
</details></li>
</ul>
<hr>
<h2 id="Privacy-Amplification-via-Importance-Sampling"><a href="#Privacy-Amplification-via-Importance-Sampling" class="headerlink" title="Privacy Amplification via Importance Sampling"></a>Privacy Amplification via Importance Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10187">http://arxiv.org/abs/2307.10187</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dominik Fay, Sebastian Mair, Jens Sjölund</li>
<li>for: 本研究探讨对各种抽样方法进行隐私增强的影响，以及这些方法在实现隐私保护的过程中的缺点和限制。</li>
<li>methods: 本研究使用了重要抽样法来实现隐私增强，具体来说是通过对每个数据点的抽样概率进行重要性权重来实现。</li>
<li>results: 研究结果表明，通过重要抽样法可以在保持隐私水平的情况下提高数据分布的准确性和效率。同时，研究还发现了一些缺点和限制，如重要抽样法可能会增加数据分布的尺度和隐私泄露的风险。<details>
<summary>Abstract</summary>
We examine the privacy-enhancing properties of subsampling a data set via importance sampling as a pre-processing step for differentially private mechanisms. This extends the established privacy amplification by subsampling result to importance sampling where each data point is weighted by the reciprocal of its selection probability. The implications for privacy of weighting each point are not obvious. On the one hand, a lower selection probability leads to a stronger privacy amplification. On the other hand, the higher the weight, the stronger the influence of the point on the output of the mechanism in the event that the point does get selected. We provide a general result that quantifies the trade-off between these two effects. We show that heterogeneous sampling probabilities can lead to both stronger privacy and better utility than uniform subsampling while retaining the subsample size. In particular, we formulate and solve the problem of privacy-optimal sampling, that is, finding the importance weights that minimize the expected subset size subject to a given privacy budget. Empirically, we evaluate the privacy, efficiency, and accuracy of importance sampling-based privacy amplification on the example of k-means clustering.
</details>
<details>
<summary>摘要</summary>
我们研究对减少数据集的隐私Properties of subsampling as a pre-processing step for differentially private mechanisms. This extends the established privacy amplification by subsampling result to importance sampling, where each data point is weighted by the reciprocal of its selection probability. The implications for privacy of weighting each point are not obvious. On the one hand, a lower selection probability leads to a stronger privacy amplification. On the other hand, the higher the weight, the stronger the influence of the point on the output of the mechanism in the event that the point does get selected. We provide a general result that quantifies the trade-off between these two effects. We show that heterogeneous sampling probabilities can lead to both stronger privacy and better utility than uniform subsampling while retaining the subsample size. In particular, we formulate and solve the problem of privacy-optimal sampling, that is, finding the importance weights that minimize the expected subset size subject to a given privacy budget. Empirically, we evaluate the privacy, efficiency, and accuracy of importance sampling-based privacy amplification on the example of k-means clustering.Here's the text with the original English text and the Simplified Chinese translation side by side for reference:Original English text:We examine the privacy-enhancing properties of subsampling a data set via importance sampling as a pre-processing step for differentially private mechanisms. This extends the established privacy amplification by subsampling result to importance sampling, where each data point is weighted by the reciprocal of its selection probability. The implications for privacy of weighting each point are not obvious. On the one hand, a lower selection probability leads to a stronger privacy amplification. On the other hand, the higher the weight, the stronger the influence of the point on the output of the mechanism in the event that the point does get selected. We provide a general result that quantifies the trade-off between these two effects. We show that heterogeneous sampling probabilities can lead to both stronger privacy and better utility than uniform subsampling while retaining the subsample size. In particular, we formulate and solve the problem of privacy-optimal sampling, that is, finding the importance weights that minimize the expected subset size subject to a given privacy budget. Empirically, we evaluate the privacy, efficiency, and accuracy of importance sampling-based privacy amplification on the example of k-means clustering.Simplified Chinese translation:我们研究对减少数据集的隐私Properties of subsampling as a pre-processing step for differentially private mechanisms. This extends the established privacy amplification by subsampling result to importance sampling, where each data point is weighted by the reciprocal of its selection probability. The implications for privacy of weighting each point are not obvious. On the one hand, a lower selection probability leads to a stronger privacy amplification. On the other hand, the higher the weight, the stronger the influence of the point on the output of the mechanism in the event that the point does get selected. We provide a general result that quantifies the trade-off between these two effects. We show that heterogeneous sampling probabilities can lead to both stronger privacy and better utility than uniform subsampling while retaining the subsample size. In particular, we formulate and solve the problem of privacy-optimal sampling, that is, finding the importance weights that minimize the expected subset size subject to a given privacy budget. Empirically, we evaluate the privacy, efficiency, and accuracy of importance sampling-based privacy amplification on the example of k-means clustering.
</details></li>
</ul>
<hr>
<h2 id="External-Reasoning-Towards-Multi-Large-Language-Models-Interchangeable-Assistance-with-Human-Feedback"><a href="#External-Reasoning-Towards-Multi-Large-Language-Models-Interchangeable-Assistance-with-Human-Feedback" class="headerlink" title="External Reasoning: Towards Multi-Large-Language-Models Interchangeable Assistance with Human Feedback"></a>External Reasoning: Towards Multi-Large-Language-Models Interchangeable Assistance with Human Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12057">http://arxiv.org/abs/2307.12057</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/AkideLiu/ANLP">https://github.com/AkideLiu/ANLP</a></li>
<li>paper_authors: Akide Liu</li>
<li>for: 提高人工智能的普遍智能水平，解决复杂的AI任务</li>
<li>methods: 通过选择性地 интегра External Reasoning 知识库，并在不同级别提供多种LLM交换帮助</li>
<li>results: 实现了提高多种LLM表现，超过现有解决方案，并且更高效 чем直接LLM处理全文<details>
<summary>Abstract</summary>
Memory is identified as a crucial human faculty that allows for the retention of visual and linguistic information within the hippocampus and neurons in the brain, which can subsequently be retrieved to address real-world challenges that arise through a lifetime of learning. The resolution of complex AI tasks through the application of acquired knowledge represents a stride toward the realization of artificial general intelligence. However, despite the prevalence of Large Language Models (LLMs) like GPT-3.5 and GPT-4 , which have displayed remarkable capabilities in language comprehension, generation, interaction, and reasoning, they are inhibited by constraints on context length that preclude the processing of extensive, continually evolving knowledge bases. This paper proposes that LLMs could be augmented through the selective integration of knowledge from external repositories, and in doing so, introduces a novel methodology for External Reasoning, exemplified by ChatPDF. Central to this approach is the establishment of a tiered policy for \textbf{External Reasoning based on Multiple LLM Interchange Assistance}, where the level of support rendered is modulated across entry, intermediate, and advanced tiers based on the complexity of the query, with adjustments made in response to human feedback. A comprehensive evaluation of this methodology is conducted using multiple LLMs and the results indicate state-of-the-art performance, surpassing existing solutions including ChatPDF.com. Moreover, the paper emphasizes that this approach is more efficient compared to the direct processing of full text by LLMs.
</details>
<details>
<summary>摘要</summary>
记忆是人类重要的一种功能，允许人们保持视觉和语言信息在脑中 hippocampus 和 neurons 中，并可以在面临生活中的挑战时进行检索。通过人工智能技术的应用，人们可以解决复杂的问题，是人工智能的实现的一步。然而，即使现有的大语言模型（LLMs）如 GPT-3.5 和 GPT-4 已经显示出了惊人的语言理解、生成、互动和理智能力，它们却受到了知识库的长度限制，无法处理大量、不断发展的知识库。本文提出，LLMs 可以通过外部知识集成 selective 的方式进行增强，并 introduce 一种新的方法ology ，例如 ChatPDF。在这种方法中，根据查询的复杂程度，对 LLMs 进行多个 tier 的多种助手支持，并根据人类反馈进行调整。通过多种 LLMs 的测试，结果表明，这种方法可以达到现有解决方案的同等或更高水平，并且更高效于直接 LLMS 处理全文。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Continual-Learning-for-Code-Generation-Models"><a href="#Exploring-Continual-Learning-for-Code-Generation-Models" class="headerlink" title="Exploring Continual Learning for Code Generation Models"></a>Exploring Continual Learning for Code Generation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02435">http://arxiv.org/abs/2307.02435</a></li>
<li>repo_url: None</li>
<li>paper_authors: Prateek Yadav, Qing Sun, Hantian Ding, Xiaopeng Li, Dejiao Zhang, Ming Tan, Xiaofei Ma, Parminder Bhatia, Ramesh Nallapati, Murali Krishna Ramanathan, Mohit Bansal, Bing Xiang</li>
<li>for: This paper aims to address the issue of continual learning (CL) in the code domain, specifically for large-scale code generation models like Codex and CodeT5.</li>
<li>methods: The authors propose a new benchmark called CodeTask-CL that covers a wide range of coding tasks and compare popular CL techniques from NLP and Vision domains. They also introduce a new method called Prompt Pooling with Teacher Forcing (PP-TF) that addresses the issue of catastrophic forgetting in coding tasks.</li>
<li>results: The authors achieve a 21.54% improvement over Prompt Pooling with their proposed method PP-TF, demonstrating the effectiveness of their approach. They also establish a training pipeline for CL on code models that can be used for further development of CL methods.Here is the information in Simplified Chinese text:</li>
<li>for: 这篇论文目标是解决大规模代码生成模型如Codex和CodeT5的连续学习（CL）问题。</li>
<li>methods: 作者提出了一个新的 benchmark called CodeTask-CL，该 benchmark 涵盖了广泛的编程任务，并与 NLP 和 Computer Vision 领域的 CL 技术进行比较。他们还提出了一种新的方法 called Prompt Pooling with Teacher Forcing (PP-TF)，用于解决编程任务中的快速忘记问题。</li>
<li>results: 作者通过 PP-TF 方法实现了21.54% 的提升，证明了他们的方法的有效性。他们还建立了一个用于 CL 的代码训练管道，这可以激励更多人开发 CL 方法。<details>
<summary>Abstract</summary>
Large-scale code generation models such as Codex and CodeT5 have achieved impressive performance. However, libraries are upgraded or deprecated very frequently and re-training large-scale language models is computationally expensive. Therefore, Continual Learning (CL) is an important aspect that remains underexplored in the code domain. In this paper, we introduce a benchmark called CodeTask-CL that covers a wide range of tasks, including code generation, translation, summarization, and refinement, with different input and output programming languages. Next, on our CodeTask-CL benchmark, we compare popular CL techniques from NLP and Vision domains. We find that effective methods like Prompt Pooling (PP) suffer from catastrophic forgetting due to the unstable training of the prompt selection mechanism caused by stark distribution shifts in coding tasks. We address this issue with our proposed method, Prompt Pooling with Teacher Forcing (PP-TF), that stabilizes training by enforcing constraints on the prompt selection mechanism and leads to a 21.54% improvement over Prompt Pooling. Along with the benchmark, we establish a training pipeline that can be used for CL on code models, which we believe can motivate further development of CL methods for code models. Our code is available at https://github.com/amazon-science/codetaskcl-pptf
</details>
<details>
<summary>摘要</summary>
大规模代码生成模型如Codex和CodeT5已经实现了印象深刻的性能。然而，库regularly更新或 deprecates，重新训练大规模语言模型是计算成本高昂。因此，持续学习（Continual Learning，CL）在代码领域是一个重要的不足之处。在这篇论文中，我们介绍了一个名为CodeTask-CL的benchmark，该benchmark包括了代码生成、翻译、摘要和修订等多种任务，并且支持不同的输入和输出编程语言。接着，在我们的CodeTask-CLbenchmark上，我们比较了 popular CL技术from NLP和Vision领域。我们发现，有效的方法如Prompt Pooling (PP)会受到极端分布变化的影响，导致训练不稳定，从而导致忘记灾难。我们解决这个问题，提出了我们的提案，Prompt Pooling with Teacher Forcing (PP-TF)，该方法可以稳定训练，并且在Prompt Pooling的基础上提高了21.54%。此外，我们还提供了一个可用于CL的代码训练管道，我们认为这可以激励further development of CL方法 для代码模型。我们的代码可以在https://github.com/amazon-science/codetaskcl-pptf找到。
</details></li>
</ul>
<hr>
<h2 id="A-probabilistic-data-driven-closure-model-for-RANS-simulations-with-aleatoric-model-uncertainty"><a href="#A-probabilistic-data-driven-closure-model-for-RANS-simulations-with-aleatoric-model-uncertainty" class="headerlink" title="A probabilistic, data-driven closure model for RANS simulations with aleatoric, model uncertainty"></a>A probabilistic, data-driven closure model for RANS simulations with aleatoric, model uncertainty</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02432">http://arxiv.org/abs/2307.02432</a></li>
<li>repo_url: None</li>
<li>paper_authors: Atul Agrawal, Phaedon-Stelios Koutsourelakis<br>for:* 这个论文旨在提出一种基于数据的、闭合模型，用于Reynolds均值 Navier-Stokes（RANS） simulations，该模型包括随机变量和模型不确定性。methods:* 该 closure 由两部分组成：一个parametric部分，使用之前提出的神经网络基于张量基函数，这些基函数依赖于流体张量的约束和旋转张量的 invariants。此外，还包括随机变量，用于补做模型错误。results:* 该模型可以生成准确、 probabilistic、预测性的结果，包括所有流体量，即使在模型错误存在的情况下。在牛顿障碍流 пробле目中， demonstrates the capability of the proposed model to produce accurate, probabilistic, predictive estimates for all flow quantities, even in regions where model errors are present.<details>
<summary>Abstract</summary>
We propose a data-driven, closure model for Reynolds-averaged Navier-Stokes (RANS) simulations that incorporates aleatoric, model uncertainty. The proposed closure consists of two parts. A parametric one, which utilizes previously proposed, neural-network-based tensor basis functions dependent on the rate of strain and rotation tensor invariants. This is complemented by latent, random variables which account for aleatoric model errors. A fully Bayesian formulation is proposed, combined with a sparsity-inducing prior in order to identify regions in the problem domain where the parametric closure is insufficient and where stochastic corrections to the Reynolds stress tensor are needed. Training is performed using sparse, indirect data, such as mean velocities and pressures, in contrast to the majority of alternatives that require direct Reynolds stress data. For inference and learning, a Stochastic Variational Inference scheme is employed, which is based on Monte Carlo estimates of the pertinent objective in conjunction with the reparametrization trick. This necessitates derivatives of the output of the RANS solver, for which we developed an adjoint-based formulation. In this manner, the parametric sensitivities from the differentiable solver can be combined with the built-in, automatic differentiation capability of the neural network library in order to enable an end-to-end differentiable framework. We demonstrate the capability of the proposed model to produce accurate, probabilistic, predictive estimates for all flow quantities, even in regions where model errors are present, on a separated flow in the backward-facing step benchmark problem.
</details>
<details>
<summary>摘要</summary>
我们提出了一种数据驱动的、闭合模型，用于Reynolds均值 Navier-Stokes（RANS） simulations，该模型包括不确定性。该闭合分为两部分：一个parametric部分，使用之前提出的神经网络基于张量函数，这些函数依赖于流速度和旋转张量的 invariants。此外，还有一些随机变量，帮助考虑模型的不确定性。我们提出了一种完全 bayesian 的 формулиров法，并将之与一个稀疏逼 zero prior 结合，以便在问题空间中标识参数闭合不充分的区域，并在这些区域中进行随机修正 Reynolds 压力张量。我们使用了稀疏、间接数据，如平均速度和压力，而不是直接的 Reynolds 压力数据进行训练。 для推断和学习，我们使用了Stochastic Variational Inference 方法，该方法基于 Monte Carlo 估计和 reparametrization trick。这使得我们可以将parametric 敏感度与神经网络库中的自动导数能力结合，以实现一个可微的框架。我们在逆向排流缘问题上示出了我们的模型的能力，可以生成准确的、probabilistic 预测结果，包括在模型错误存在的区域。
</details></li>
</ul>
<hr>
<h2 id="In-Context-Learning-for-Attention-Scheme-from-Single-Softmax-Regression-to-Multiple-Softmax-Regression-via-a-Tensor-Trick"><a href="#In-Context-Learning-for-Attention-Scheme-from-Single-Softmax-Regression-to-Multiple-Softmax-Regression-via-a-Tensor-Trick" class="headerlink" title="In-Context Learning for Attention Scheme: from Single Softmax Regression to Multiple Softmax Regression via a Tensor Trick"></a>In-Context Learning for Attention Scheme: from Single Softmax Regression to Multiple Softmax Regression via a Tensor Trick</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02419">http://arxiv.org/abs/2307.02419</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yeqi Gao, Zhao Song, Shenghao Xie</li>
<li>for: 这个论文主要研究了大语言模型（LLMs）在人类社会中所带来的显著和转变性改变。</li>
<li>methods: 该论文采用了vectorization技术来解决 regression问题，将维度从$d$扩展到$d^2$。</li>
<li>results: 研究人员通过完成 lipschitz 分析，得出了关于在上下文学习中的主要结果。<details>
<summary>Abstract</summary>
Large language models (LLMs) have brought significant and transformative changes in human society. These models have demonstrated remarkable capabilities in natural language understanding and generation, leading to various advancements and impacts across several domains.   We consider the in-context learning under two formulation for attention related regression in this work. Given matrices $A_1 \in \mathbb{R}^{n \times d}$, and $A_2 \in \mathbb{R}^{n \times d}$ and $B \in \mathbb{R}^{n \times n}$, the purpose is to solve some certain optimization problems: Normalized version $\min_{X} \| D(X)^{-1} \exp(A_1 X A_2^\top) - B \|_F^2$ and Rescaled version $\| \exp(A_1 X A_2^\top) - D(X) \cdot B \|_F^2$. Here $D(X) := \mathrm{diag}( \exp(A_1 X A_2^\top) {\bf 1}_n )$.   Our regression problem shares similarities with previous studies on softmax-related regression. Prior research has extensively investigated regression techniques related to softmax regression: Normalized version $\| \langle \exp(Ax) , {\bf 1}_n \rangle^{-1} \exp(Ax) - b \|_2^2$ and Resscaled version $\| \exp(Ax) - \langle \exp(Ax), {\bf 1}_n \rangle b \|_2^2 $   In contrast to previous approaches, we adopt a vectorization technique to address the regression problem in matrix formulation. This approach expands the dimension from $d$ to $d^2$, resembling the formulation of the regression problem mentioned earlier.   Upon completing the lipschitz analysis of our regression function, we have derived our main result concerning in-context learning.
</details>
<details>
<summary>摘要</summary>
In this work, we consider in-context learning under two formulations for attention-related regression. Given matrices $A_1 \in \mathbb{R}^{n \times d}$, $A_2 \in \mathbb{R}^{n \times d}$, and $B \in \mathbb{R}^{n \times n}$, our goal is to solve certain optimization problems:1. Normalized version: $\min_{X} \| D(X)^{-1} \exp(A_1 X A_2^\top) - B \|_F^2$2. Rescaled version: $\| \exp(A_1 X A_2^\top) - D(X) \cdot B \|_F^2$Here, $D(X) = \text{diag}(\exp(A_1 X A_2^\top) \mathbf{1}_n)$. Our regression problem shares similarities with previous studies on softmax-related regression. Prior research has extensively investigated regression techniques related to softmax regression:1. Normalized version: $\| \langle \exp(Ax), \mathbf{1}_n \rangle^{-1} \exp(Ax) - b \|_2^2$2. Rescaled version: $\| \exp(Ax) - \langle \exp(Ax), \mathbf{1}_n \rangle b \|_2^2$In contrast to previous approaches, we adopt a vectorization technique to address the regression problem in matrix formulation. This approach expands the dimension from $d$ to $d^2$, resembling the formulation of the regression problem mentioned earlier.After completing the Lipschitz analysis of our regression function, we have derived our main result concerning in-context learning.
</details></li>
</ul>
<hr>
<h2 id="Multi-objective-Deep-Reinforcement-Learning-for-Mobile-Edge-Computing"><a href="#Multi-objective-Deep-Reinforcement-Learning-for-Mobile-Edge-Computing" class="headerlink" title="Multi-objective Deep Reinforcement Learning for Mobile Edge Computing"></a>Multi-objective Deep Reinforcement Learning for Mobile Edge Computing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14346">http://arxiv.org/abs/2307.14346</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gracefulning/mec_morl_multipolicy">https://github.com/gracefulning/mec_morl_multipolicy</a></li>
<li>paper_authors: Ning Yang, Junrui Wen, Meng Zhang, Ming Tang</li>
<li>For: The paper is written for next-generation mobile network applications that prioritize various performance metrics, including delays and energy consumption, and addresses the challenge of unknown preferences in multi-objective resource scheduling for mobile edge computing (MEC) systems.* Methods: The paper uses a multi-objective reinforcement learning (MORL) scheme with proximal policy optimization (PPO) to address the challenge of unknown preferences in MEC systems, and introduces a well-designed state encoding method for constructing features for multiple edges in MEC systems and a sophisticated reward function for accurately computing the utilities of delay and energy consumption.* Results: The paper demonstrates that the proposed MORL scheme enhances the hypervolume of the Pareto front by up to 233.1% compared to benchmarks, indicating the effectiveness of the proposed method in improving the performance of MEC systems.Here’s the information in Simplified Chinese text:* For: 这篇论文是为下一代 mobil 网络应用程序而写的，这些应用程序优先级包括延迟和能耗。* Methods: 这篇论文使用了多目标学习（MORL）和 proximal policy 优化（PPO）来解决 MEC 系统中 unknown preferences 的问题，并引入了多边的状态编码方法和准确计算延迟和能耗的奖励函数。* Results: 论文的实验结果表明，提议的 MORL 方案可以提高 MEC 系统的 Pareto 前面的卷积体率，相比基准值，提高了233.1%。<details>
<summary>Abstract</summary>
Mobile edge computing (MEC) is essential for next-generation mobile network applications that prioritize various performance metrics, including delays and energy consumption. However, conventional single-objective scheduling solutions cannot be directly applied to practical systems in which the preferences of these applications (i.e., the weights of different objectives) are often unknown or challenging to specify in advance. In this study, we address this issue by formulating a multi-objective offloading problem for MEC with multiple edges to minimize expected long-term energy consumption and transmission delay while considering unknown preferences as parameters. To address the challenge of unknown preferences, we design a multi-objective (deep) reinforcement learning (MORL)-based resource scheduling scheme with proximal policy optimization (PPO). In addition, we introduce a well-designed state encoding method for constructing features for multiple edges in MEC systems, a sophisticated reward function for accurately computing the utilities of delay and energy consumption. Simulation results demonstrate that our proposed MORL scheme enhances the hypervolume of the Pareto front by up to 233.1% compared to benchmarks. Our full framework is available at https://github.com/gracefulning/mec_morl_multipolicy.
</details>
<details>
<summary>摘要</summary>
Mobile edge computing (MEC) 是下一代移动网络应用程序的关键技术，这些应用程序强调多种性能指标，包括延迟和能耗。然而，传统的单目标调度解决方案无法直接应用于实际系统中，因为这些应用程序的偏好（即不同目标的权重）通常未知或难以在先 specify。在本研究中，我们解决了这个问题，通过对 MEC 系统中的多个边进行减少预期长期能耗和传输延迟的多目标卸载问题进行定义。为了解决未知偏好的挑战，我们提出了一种基于多目标学习（deep reinforcement learning，DRL）的资源调度方案，并使用 proximal policy optimization（PPO）来解决。此外，我们还提出了一种有效的状态编码方法，用于构建多边 MEC 系统中的特征。此外，我们还提出了一种具有准确计算延迟和能耗使用的复杂的奖励函数。实验结果表明，我们的提议的 MORL 方案可以提高 Pareto 前方的权重的增加率达到 233.1% 相比 benchmark。我们的全面框架可以在 GitHub 上找到：https://github.com/gracefulning/mec_morl_multipolicy。
</details></li>
</ul>
<hr>
<h2 id="OpenDelta-A-Plug-and-play-Library-for-Parameter-efficient-Adaptation-of-Pre-trained-Models"><a href="#OpenDelta-A-Plug-and-play-Library-for-Parameter-efficient-Adaptation-of-Pre-trained-Models" class="headerlink" title="OpenDelta: A Plug-and-play Library for Parameter-efficient Adaptation of Pre-trained Models"></a>OpenDelta: A Plug-and-play Library for Parameter-efficient Adaptation of Pre-trained Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03084">http://arxiv.org/abs/2307.03084</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thunlp/opendelta">https://github.com/thunlp/opendelta</a></li>
<li>paper_authors: Shengding Hu, Ning Ding, Weilin Zhao, Xingtai Lv, Zhen Zhang, Zhiyuan Liu, Maosong Sun</li>
<li>for: 这篇研究目的是提出一个开源库OpenDelta，用于实现大型预训练模型（PTMs）的快速适应下游任务。</li>
<li>methods: OpenDelta使用多种δ调整方法来实现快速适应，这些方法包括δ模组、阶层δ调整等，并且可以与不同的预训练模型（PTMs）集成。</li>
<li>results: OpenDelta提供了一个通用的、可调整的、可扩展的平台，可以帮助研究者和实践者快速适应大型预训练模型。<details>
<summary>Abstract</summary>
The scale of large pre-trained models (PTMs) poses significant challenges in adapting to downstream tasks due to the high optimization overhead and storage costs associated with full-parameter fine-tuning. To address this, many studies explore parameter-efficient tuning methods, also framed as "delta tuning", which updates only a small subset of parameters, known as "delta modules", while keeping the backbone model's parameters fixed. However, the practicality and flexibility of delta tuning have been limited due to existing implementations that directly modify the code of the backbone PTMs and hard-code specific delta tuning methods for each PTM. In this paper, we present OpenDelta, an open-source library that overcomes these limitations by providing a plug-and-play implementation of various delta tuning methods. Our novel techniques eliminate the need to modify the backbone PTMs' code, making OpenDelta compatible with different, even novel PTMs. OpenDelta is designed to be simple, modular, and extensible, providing a comprehensive platform for researchers and practitioners to adapt large PTMs efficiently.
</details>
<details>
<summary>摘要</summary>
大型预训练模型（PTM）的缺省大小带来了适应下游任务的 significiant挑战，这是因为全参数细化训练所需的优化开销和存储成本很高。为了解决这个问题，许多研究尝试了参数效率的训练方法，也称为"delta tuning"，这种方法只更新一个小subset的参数，称为"delta module"，而保持背景模型的参数不变。然而，现有的实现方法直接修改背景PTM的代码，这限制了 delta tuning 的实用性和灵活性。在这篇论文中，我们介绍了 OpenDelta，一个开源库，它解决了这些限制。OpenDelta 提供了一个可插入的 delta tuning 实现方式，不需要修改背景PTM 的代码，因此可以与不同的、甚至是新的 PTM 兼容。我们的新技术使得 OpenDelta 可以与不同的 delta tuning 方法进行整合，提供了一个简单、干净、可扩展的平台，以便研究人员和实践者们能够效率地适应大型 PTM。
</details></li>
</ul>
<hr>
<h2 id="ν-2-Flows-Fast-and-improved-neutrino-reconstruction-in-multi-neutrino-final-states-with-conditional-normalizing-flows"><a href="#ν-2-Flows-Fast-and-improved-neutrino-reconstruction-in-multi-neutrino-final-states-with-conditional-normalizing-flows" class="headerlink" title="$ν^2$-Flows: Fast and improved neutrino reconstruction in multi-neutrino final states with conditional normalizing flows"></a>$ν^2$-Flows: Fast and improved neutrino reconstruction in multi-neutrino final states with conditional normalizing flows</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02405">http://arxiv.org/abs/2307.02405</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rodem-hep/nu2flows">https://github.com/rodem-hep/nu2flows</a></li>
<li>paper_authors: John Andrew Raine, Matthew Leigh, Knut Zoch, Tobias Golling</li>
<li>for: 这个论文是为了扩展$\nu$-Flows方法，使其能处理包含多个中微子的终态。</li>
<li>methods: 这个方法使用了$\nu^2$-Flows方法，可以native地扩展到任何对象类型和多重性的终态。</li>
<li>results: 在$t\bar{t}$同位素事件中，$\nu^2$-Flows方法可以更加准确地重建中微子的动量和相关性，并且可以解决所有事件。比较其他方法，这个方法的推断时间更为短暂，并且可以通过图像处理器并行执行来进一步加速。在应用于$t\bar{t}$同位素事件中，$\nu^2$-Flows方法可以提高每个分布的统计精度，比标准技术提高了1.5倍至2倍，并且在一些情况下可以达到4倍。<details>
<summary>Abstract</summary>
In this work we introduce $\nu^2$-Flows, an extension of the $\nu$-Flows method to final states containing multiple neutrinos. The architecture can natively scale for all combinations of object types and multiplicities in the final state for any desired neutrino multiplicities. In $t\bar{t}$ dilepton events, the momenta of both neutrinos and correlations between them are reconstructed more accurately than when using the most popular standard analytical techniques, and solutions are found for all events. Inference time is significantly faster than competing methods, and can be reduced further by evaluating in parallel on graphics processing units. We apply $\nu^2$-Flows to $t\bar{t}$ dilepton events and show that the per-bin uncertainties in unfolded distributions is much closer to the limit of performance set by perfect neutrino reconstruction than standard techniques. For the chosen double differential observables $\nu^2$-Flows results in improved statistical precision for each bin by a factor of 1.5 to 2 in comparison to the Neutrino Weighting method and up to a factor of four in comparison to the Ellipse approach.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们引入 $\nu^2$-Flows，它是 Final State 中多个中微子的扩展方法。该架构可以Native scale 到所有对象类型和多重性的最终状态，对于任何需要的中微子多重性。在 $t\bar{t}$ 蛇脊事件中，我们可以更准确地重建中微子的动量和它们之间的相关性，而且可以为所有事件找到解决方案。在比较方法时，我们发现 $\nu^2$-Flows 的推理时间明显更快，可以通过在图形处理器上并行执行来进一步减少。我们将 $\nu^2$-Flows 应用于 $t\bar{t}$ 蛇脊事件，并发现每个分布的不确定性在 unfolded 分布中远远 closer to the limit of performance set by perfect neutrino reconstruction  than标准技术。为选择的双 differential 观测量， $\nu^2$-Flows 可以提高每个分布的统计精度，比标准方法的 Neutrino Weighting 方法和 Ellipse 方法高一个factor of 1.5 to 2, and up to a factor of four in comparison to the Ellipse approach。
</details></li>
</ul>
<hr>
<h2 id="Unbalanced-Optimal-Transport-A-Unified-Framework-for-Object-Detection"><a href="#Unbalanced-Optimal-Transport-A-Unified-Framework-for-Object-Detection" class="headerlink" title="Unbalanced Optimal Transport: A Unified Framework for Object Detection"></a>Unbalanced Optimal Transport: A Unified Framework for Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02402">http://arxiv.org/abs/2307.02402</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hdeplaen/uotod">https://github.com/hdeplaen/uotod</a></li>
<li>paper_authors: Henri De Plaen, Pierre-François De Plaen, Johan A. K. Suykens, Marc Proesmans, Tinne Tuytelaars, Luc Van Gool</li>
<li>for: 这个论文的目的是提出一种基于不对称优化运输的对象检测方法，以提高对象检测模型的性能和初始化速度。</li>
<li>methods: 该方法使用了不对称优化运输算法，可以在不同的精度和速度之间选择最佳的属性。</li>
<li>results: 实验表明，使用该方法训练对象检测模型可以达到现状的最佳性能水平，并且提供更快的初始化速度。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
During training, supervised object detection tries to correctly match the predicted bounding boxes and associated classification scores to the ground truth. This is essential to determine which predictions are to be pushed towards which solutions, or to be discarded. Popular matching strategies include matching to the closest ground truth box (mostly used in combination with anchors), or matching via the Hungarian algorithm (mostly used in anchor-free methods). Each of these strategies comes with its own properties, underlying losses, and heuristics. We show how Unbalanced Optimal Transport unifies these different approaches and opens a whole continuum of methods in between. This allows for a finer selection of the desired properties. Experimentally, we show that training an object detection model with Unbalanced Optimal Transport is able to reach the state-of-the-art both in terms of Average Precision and Average Recall as well as to provide a faster initial convergence. The approach is well suited for GPU implementation, which proves to be an advantage for large-scale models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Versatile-Hub-Model-For-Efficient-Information-Propagation-And-Feature-Selection"><a href="#A-Versatile-Hub-Model-For-Efficient-Information-Propagation-And-Feature-Selection" class="headerlink" title="A Versatile Hub Model For Efficient Information Propagation And Feature Selection"></a>A Versatile Hub Model For Efficient Information Propagation And Feature Selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02398">http://arxiv.org/abs/2307.02398</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhaoze Wang, Junsong Wang</li>
<li>for: 这 paper 是 investigate 生物大脑的 topology 特征，以及如何使用这些特征来提高信息传递和认知处理的方法。</li>
<li>methods: 这 paper 使用了 Echo State Network (ESN) 来研究 hub 结构的机制基础，并发现 hub 结构可以提高模型的性能。</li>
<li>results: 研究发现，通过 incorporating  hub 结构，可以提高模型的性能，主要是通过更好地处理信息和提取特征来实现这一点。<details>
<summary>Abstract</summary>
Hub structure, characterized by a few highly interconnected nodes surrounded by a larger number of nodes with fewer connections, is a prominent topological feature of biological brains, contributing to efficient information transfer and cognitive processing across various species. In this paper, a mathematical model of hub structure is presented. The proposed method is versatile and can be broadly applied to both computational neuroscience and Recurrent Neural Networks (RNNs) research. We employ the Echo State Network (ESN) as a means to investigate the mechanistic underpinnings of hub structures. Our findings demonstrate a substantial enhancement in performance upon incorporating the hub structure. Through comprehensive mechanistic analyses, we show that the hub structure improves model performance by facilitating efficient information processing and better feature extractions.
</details>
<details>
<summary>摘要</summary>
<SYS>translate("Hub structure, characterized by a few highly interconnected nodes surrounded by a larger number of nodes with fewer connections, is a prominent topological feature of biological brains, contributing to efficient information transfer and cognitive processing across various species. In this paper, a mathematical model of hub structure is presented. The proposed method is versatile and can be broadly applied to both computational neuroscience and Recurrent Neural Networks (RNNs) research. We employ the Echo State Network (ESN) as a means to investigate the mechanistic underpinnings of hub structures. Our findings demonstrate a substantial enhancement in performance upon incorporating the hub structure. Through comprehensive mechanistic analyses, we show that the hub structure improves model performance by facilitating efficient information processing and better feature extractions."into Simplified Chinese)<SYS> traducción(" estructura de nudo principal, caracterizada por un pequeño número de nodos altamente interconectados rodeados por un mayor número de nodos con menos conexiones, es una característica topológica destacada de los cerebros biológicos, que contribuye a la transferencia eficiente de información y el procesamiento cognitivo en diversas especies. En este artículo, se presenta un modelo matemático de la estructura de nudo. El método propuesto es versátil y puede ser ampliamente aplicado en investigaciones de neurociencia computacional y redes neuronales recurrentes (RNNs). Utilizamos la Red de Estado Eco (ESN) como medio para investigar las bases mecanicistas de las estructuras de nudo. Nuestros hallazgos demuestran una mejora substancial en el rendimiento al incorporar la estructura de nudo. A través de análisis mecanicistas exhaustivos, demostramos que la estructura de nudo mejora el rendimiento del modelo al facilitar el procesamiento eficiente de información y la extracción de características mejor."</SYS>
</details></li>
</ul>
<hr>
<h2 id="Causal-Discovery-with-Language-Models-as-Imperfect-Experts"><a href="#Causal-Discovery-with-Language-Models-as-Imperfect-Experts" class="headerlink" title="Causal Discovery with Language Models as Imperfect Experts"></a>Causal Discovery with Language Models as Imperfect Experts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02390">http://arxiv.org/abs/2307.02390</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/stephlong614/causal-disco">https://github.com/stephlong614/causal-disco</a></li>
<li>paper_authors: Stephanie Long, Alexandre Piché, Valentina Zantedeschi, Tibor Schuster, Alexandre Drouin</li>
<li>for: 本研究旨在提高基于数据驱动的 causal 图 indentification 精度，通过利用专家知识。</li>
<li>methods: 我们提议使用 consistency 属性，如 acyclicity 和 conditional independencies，来修正专家提供的错误信息。</li>
<li>results: 我们在实际数据上使用大型自然语言模型作为不准确的专家，并进行了一个案例研究。<details>
<summary>Abstract</summary>
Understanding the causal relationships that underlie a system is a fundamental prerequisite to accurate decision-making. In this work, we explore how expert knowledge can be used to improve the data-driven identification of causal graphs, beyond Markov equivalence classes. In doing so, we consider a setting where we can query an expert about the orientation of causal relationships between variables, but where the expert may provide erroneous information. We propose strategies for amending such expert knowledge based on consistency properties, e.g., acyclicity and conditional independencies in the equivalence class. We then report a case study, on real data, where a large language model is used as an imperfect expert.
</details>
<details>
<summary>摘要</summary>
理解系统下 causal 关系的下发关系是决策准确的基本前提。在这项工作中，我们研究如何使用专家知识来提高数据驱动的 causal 图识别，超出 markov 等类。在这种情况下，我们可以询问专家关于变量之间 causal 关系的方向，但专家可能提供错误的信息。我们提出了基于一致性属性的纠正策略，如循环无法和条件独立性在等类中。然后，我们报告了一个实际数据的案例研究，使用大语言模型作为不完全专家。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/06/cs.LG_2023_07_06/" data-id="cllsj9wy70015uv88hlodhas8" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_07_06" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/06/cs.SD_2023_07_06/" class="article-date">
  <time datetime="2023-07-05T16:00:00.000Z" itemprop="datePublished">2023-07-06</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/06/cs.SD_2023_07_06/">cs.SD - 2023-07-06 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Audio-visual-End-to-end-Multi-channel-Speech-Separation-Dereverberation-and-Recognition"><a href="#Audio-visual-End-to-end-Multi-channel-Speech-Separation-Dereverberation-and-Recognition" class="headerlink" title="Audio-visual End-to-end Multi-channel Speech Separation, Dereverberation and Recognition"></a>Audio-visual End-to-end Multi-channel Speech Separation, Dereverberation and Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02909">http://arxiv.org/abs/2307.02909</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guinan Li, Jiajun Deng, Mengzhe Geng, Zengrui Jin, Tianzi Wang, Shujie Hu, Mingyu Cui, Helen Meng, Xunying Liu</li>
<li>for: 提高cocktail party speech recognition精度，增强对干扰音频信号的识别能力。</li>
<li>methods: 利用视频信号，实现多通道音频信号分离、去抖振荡和识别，并在前端和后端都具有视觉信息的完整 incorporation。</li>
<li>results: 相比同种audio-only基eline，提高了9.1%和6.2%的word error rate（WER），同时也提高了PESQ、STOI和SRMR分数。<details>
<summary>Abstract</summary>
Accurate recognition of cocktail party speech containing overlapping speakers, noise and reverberation remains a highly challenging task to date. Motivated by the invariance of visual modality to acoustic signal corruption, an audio-visual multi-channel speech separation, dereverberation and recognition approach featuring a full incorporation of visual information into all system components is proposed in this paper. The efficacy of the video input is consistently demonstrated in mask-based MVDR speech separation, DNN-WPE or spectral mapping (SpecM) based speech dereverberation front-end and Conformer ASR back-end. Audio-visual integrated front-end architectures performing speech separation and dereverberation in a pipelined or joint fashion via mask-based WPD are investigated. The error cost mismatch between the speech enhancement front-end and ASR back-end components is minimized by end-to-end jointly fine-tuning using either the ASR cost function alone, or its interpolation with the speech enhancement loss. Experiments were conducted on the mixture overlapped and reverberant speech data constructed using simulation or replay of the Oxford LRS2 dataset. The proposed audio-visual multi-channel speech separation, dereverberation and recognition systems consistently outperformed the comparable audio-only baseline by 9.1% and 6.2% absolute (41.7% and 36.0% relative) word error rate (WER) reductions. Consistent speech enhancement improvements were also obtained on PESQ, STOI and SRMR scores.
</details>
<details>
<summary>摘要</summary>
当前，cocktail party speech中的叠加说话者、噪声和回声识别仍然是一个非常挑战性的任务。这是因为视觉modalities具有对声音信号损害的不变性，所以一种包含视觉信息的多渠道音频视觉混合分离、减少回声和识别方法被提议。这篇论文中的方法是通过使用面积基于的MVDR音频分离、DNN-WPE或spectral mapping（SpecM）基于的音频前端分离和Conformer ASR后端来实现。我们还 investigate了将视觉信息完全 интегрирован到所有系统组件中的音频视觉混合前端架构。在pipelined或联合的方式下，我们使用面积基于的WPD来实现音频视觉混合。为了消除语音提高前端和后端组件之间的错误成本差，我们使用综合jointly fine-tuning，使用ASR成本函数alone或其混合with语音提高损失来减少错误成本差。我们对于混合了simulation和replay的Oxford LRS2 dataset中的杂合混响杂音数据进行了实验。结果表明，我们的音频视觉多渠道分离、减少回声和识别系统在相比于相同的音频基eline的9.1%和6.2%绝对（41.7%和36.0%相对）word error rate（WER）下提高了识别率。此外，我们还获得了相应的语音提高成果在PESQ、STOI和SRMR scores上。
</details></li>
</ul>
<hr>
<h2 id="The-Relationship-Between-Speech-Features-Changes-When-You-Get-Depressed-Feature-Correlations-for-Improving-Speed-and-Performance-of-Depression-Detection"><a href="#The-Relationship-Between-Speech-Features-Changes-When-You-Get-Depressed-Feature-Correlations-for-Improving-Speed-and-Performance-of-Depression-Detection" class="headerlink" title="The Relationship Between Speech Features Changes When You Get Depressed: Feature Correlations for Improving Speed and Performance of Depression Detection"></a>The Relationship Between Speech Features Changes When You Get Depressed: Feature Correlations for Improving Speed and Performance of Depression Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02892">http://arxiv.org/abs/2307.02892</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fuxiang Tao, Wei Ma, Xuri Ge, Anna Esposito, Alessandro Vinciarelli</li>
<li>for: 这个论文研究了听话者抑郁症的影响，发现抑郁症会改变speech中特征之间的相关性。</li>
<li>methods: 该论文使用了SVM和LSTM两种模型，并使用了Androids Corpus dataset，包括112名speaker，其中58人被诊断为职业心理医生。</li>
<li>results: 实验结果显示，使用特征相关矩阵而不是特征向量可以提高模型的训练速度和性能，错误率下降23.1%-26.6%。这可能是因为抑郁 speaker中特征相关性更为变化。<details>
<summary>Abstract</summary>
This work shows that depression changes the correlation between features extracted from speech. Furthermore, it shows that using such an insight can improve the training speed and performance of depression detectors based on SVMs and LSTMs. The experiments were performed over the Androids Corpus, a publicly available dataset involving 112 speakers, including 58 people diagnosed with depression by professional psychiatrists. The results show that the models used in the experiments improve in terms of training speed and performance when fed with feature correlation matrices rather than with feature vectors. The relative reduction of the error rate ranges between 23.1% and 26.6% depending on the model. The probable explanation is that feature correlation matrices appear to be more variable in the case of depressed speakers. Correspondingly, such a phenomenon can be thought of as a depression marker.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Evaluating-raw-waveforms-with-deep-learning-frameworks-for-speech-emotion-recognition"><a href="#Evaluating-raw-waveforms-with-deep-learning-frameworks-for-speech-emotion-recognition" class="headerlink" title="Evaluating raw waveforms with deep learning frameworks for speech emotion recognition"></a>Evaluating raw waveforms with deep learning frameworks for speech emotion recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02820">http://arxiv.org/abs/2307.02820</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zeynep Hilal Kilimci, Ulku Bayraktar, Ayhan Kucukmanisa<br>for:The paper focuses on speech emotion recognition using deep learning techniques, specifically on the contribution of feeding raw audio files directly into deep neural networks without any feature extraction stage.methods:The proposed model uses a combination of machine learning algorithms, ensemble learning methods, and deep and hybrid deep learning techniques, including support vector machine, decision tree, naive Bayes, random forests, majority voting, and stacking. The model also employs convolutional neural networks, long short-term memory networks, and hybrid CNN-LSTM models.results:The proposed model achieves state-of-the-art performance on six different data sets, including EMO-DB, RAVDESS, TESS, CREMA, SAVEE, and TESS+RAVDESS. Specifically, the CNN model achieves 95.86% accuracy on the TESS+RAVDESS data set, outperforming existing approaches. The proposed model also demonstrates high accuracy on other data sets, ranging from 90.34% to 99.48% and 69.72% to 85.76%, depending on the data set and the model used.<details>
<summary>Abstract</summary>
Speech emotion recognition is a challenging task in speech processing field. For this reason, feature extraction process has a crucial importance to demonstrate and process the speech signals. In this work, we represent a model, which feeds raw audio files directly into the deep neural networks without any feature extraction stage for the recognition of emotions utilizing six different data sets, EMO-DB, RAVDESS, TESS, CREMA, SAVEE, and TESS+RAVDESS. To demonstrate the contribution of proposed model, the performance of traditional feature extraction techniques namely, mel-scale spectogram, mel-frequency cepstral coefficients, are blended with machine learning algorithms, ensemble learning methods, deep and hybrid deep learning techniques. Support vector machine, decision tree, naive Bayes, random forests models are evaluated as machine learning algorithms while majority voting and stacking methods are assessed as ensemble learning techniques. Moreover, convolutional neural networks, long short-term memory networks, and hybrid CNN- LSTM model are evaluated as deep learning techniques and compared with machine learning and ensemble learning methods. To demonstrate the effectiveness of proposed model, the comparison with state-of-the-art studies are carried out. Based on the experiment results, CNN model excels existent approaches with 95.86% of accuracy for TESS+RAVDESS data set using raw audio files, thence determining the new state-of-the-art. The proposed model performs 90.34% of accuracy for EMO-DB with CNN model, 90.42% of accuracy for RAVDESS with CNN model, 99.48% of accuracy for TESS with LSTM model, 69.72% of accuracy for CREMA with CNN model, 85.76% of accuracy for SAVEE with CNN model in speaker-independent audio categorization problems.
</details>
<details>
<summary>摘要</summary>
《speech emotion recognition是speech processing领域中的一项挑战。为了实现这一目标，特征提取过程具有重要的重要性。在这项工作中，我们提出了一种模型，该模型直接将原始音频文件传递给深度神经网络，不需要特征提取阶段。为了证明模型的贡献，我们对传统特征提取技术（mel-scale spectogram、mel-frequency cepstral coefficients）与机器学习算法（支持向量机、决策树、naive Bayes、Random Forest）、ensemble learning方法（majority voting、stacking）进行比较。此外，我们还评估了深度学习技术（卷积神经网络、长短期记忆网络、混合卷积-LSTM）。基于实验结果，CNN模型在TESS+RAVDESS数据集上达到了95.86%的准确率，超过了现有方法，确定了新的状态态-of-the-art。我们的模型在EMO-DB、RAVDESS、TESS、CREMA和SAVEE数据集上达到了90.34%、90.42%、99.48%、69.72%和85.76%的准确率。》
</details></li>
</ul>
<hr>
<h2 id="DSARSR-Deep-Stacked-Auto-encoders-Enhanced-Robust-Speaker-Recognition"><a href="#DSARSR-Deep-Stacked-Auto-encoders-Enhanced-Robust-Speaker-Recognition" class="headerlink" title="DSARSR: Deep Stacked Auto-encoders Enhanced Robust Speaker Recognition"></a>DSARSR: Deep Stacked Auto-encoders Enhanced Robust Speaker Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02751">http://arxiv.org/abs/2307.02751</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhifeng Wang, Chunyan Zeng, Surong Duan, Hongjie Ouyang, Hongmin Xu</li>
<li>for: 这项研究旨在提高cross-channel条件下i-vector框架的Robustness，并探索使用深度学习进行人识别。</li>
<li>methods: 该研究使用Stacked Auto-encoders来提取i-vector的抽象，而不是使用PLDA。在预处理和特征提取后，使用Speaker和Channel独立的speech进行UBM训练。</li>
<li>results: 实验结果显示，提出的方法比之前的方法表现更好。<details>
<summary>Abstract</summary>
Speaker recognition is a biometric modality that utilizes the speaker's speech segments to recognize the identity, determining whether the test speaker belongs to one of the enrolled speakers. In order to improve the robustness of the i-vector framework on cross-channel conditions and explore the nova method for applying deep learning to speaker recognition, the Stacked Auto-encoders are used to get the abstract extraction of the i-vector instead of applying PLDA. After pre-processing and feature extraction, the speaker and channel-independent speeches are employed for UBM training. The UBM is then used to extract the i-vector of the enrollment and test speech. Unlike the traditional i-vector framework, which uses linear discriminant analysis (LDA) to reduce dimension and increase the discrimination between speaker subspaces, this research use stacked auto-encoders to reconstruct the i-vector with lower dimension and different classifiers can be chosen to achieve final classification. The experimental results show that the proposed method achieves better performance than the state-of-the-art method.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="On-Device-Constrained-Self-Supervised-Speech-Representation-Learning-for-Keyword-Spotting-via-Knowledge-Distillation"><a href="#On-Device-Constrained-Self-Supervised-Speech-Representation-Learning-for-Keyword-Spotting-via-Knowledge-Distillation" class="headerlink" title="On-Device Constrained Self-Supervised Speech Representation Learning for Keyword Spotting via Knowledge Distillation"></a>On-Device Constrained Self-Supervised Speech Representation Learning for Keyword Spotting via Knowledge Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02720">http://arxiv.org/abs/2307.02720</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gene-Ping Yang, Yue Gu, Qingming Tang, Dongsu Du, Yuzong Liu</li>
<li>for: 这篇论文是为了提高在 Alexa 关键词搜寻任务中的语音识别精度，特别是在内存限制和受损数据收集的情况下。</li>
<li>methods: 我们使用了知识传授法，将大型自我超vised模型的知识转移到较小的、轻量级模型中，并使用双重视野交叉相关知识传授和老师的codebook作为学习目标。</li>
<li>results: 我们在使用了我们的S3RL架构进行 Alexa 关键词搜寻探测任务时，在正常和噪音情况下表现出色，证明了知识传授方法在自我超vised模型中构建关键词搜寻模型的时候具有优秀的效果，并且在内存限制和受损数据收集的情况下运行。<details>
<summary>Abstract</summary>
Large self-supervised models are effective feature extractors, but their application is challenging under on-device budget constraints and biased dataset collection, especially in keyword spotting. To address this, we proposed a knowledge distillation-based self-supervised speech representation learning (S3RL) architecture for on-device keyword spotting. Our approach used a teacher-student framework to transfer knowledge from a larger, more complex model to a smaller, light-weight model using dual-view cross-correlation distillation and the teacher's codebook as learning objectives. We evaluated our model's performance on an Alexa keyword spotting detection task using a 16.6k-hour in-house dataset. Our technique showed exceptional performance in normal and noisy conditions, demonstrating the efficacy of knowledge distillation methods in constructing self-supervised models for keyword spotting tasks while working within on-device resource constraints.
</details>
<details>
<summary>摘要</summary>
大型自我监督模型是有效的特征提取器，但是在设备内存限制和偏见数据采集下面临挑战，特别是在关键词检测中。为解决这问题，我们提出了基于知识填充的自我监督语音表示学习（S3RL）建模，用于在设备上进行关键词检测。我们采用了教师-学生框架，将大型、复杂的模型知识传播到小型、轻量级模型，使用双视交叉相关知识填充和教师的编码库作为学习目标。我们使用Alexa关键词检测任务上的16.6万小时内部数据进行评估。我们的技术在正常和噪音条件下都表现出色，证明了知识填充方法在构建自我监督模型的关键词检测任务中的效果。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/06/cs.SD_2023_07_06/" data-id="cllsj9wyt0038uv88gro789pf" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_07_06" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/06/eess.AS_2023_07_06/" class="article-date">
  <time datetime="2023-07-05T16:00:00.000Z" itemprop="datePublished">2023-07-06</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/06/eess.AS_2023_07_06/">eess.AS - 2023-07-06 22:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Read-Look-or-Listen-What’s-Needed-for-Solving-a-Multimodal-Dataset"><a href="#Read-Look-or-Listen-What’s-Needed-for-Solving-a-Multimodal-Dataset" class="headerlink" title="Read, Look or Listen? What’s Needed for Solving a Multimodal Dataset"></a>Read, Look or Listen? What’s Needed for Solving a Multimodal Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04532">http://arxiv.org/abs/2307.04532</a></li>
<li>repo_url: None</li>
<li>paper_authors: Netta Madvil, Yonatan Bitton, Roy Schwartz</li>
<li>for: 本研究旨在探讨大规模多模式数据集的质量评估问题。</li>
<li>methods: 我们提出了一种两步方法，利用小量人工标注将每个多模式实例映射到需要处理的Modalities。</li>
<li>results: 我们应用方法到TVQA视频问答数据集，发现大多数问题可以通过单一模式答案，无论是视频还是音频。此外，我们发现更多的70%的问题可以使用多种不同的单模式策略解决，如只看视频或只听音频。此外，我们发现MERLOT Reserve在图像基于问题上表现不佳，而文本和音频则表现较好。基于我们的观察，我们提出了一个新的测试集，其中模型需要使用多种模式，并观察到模型性能减降很大。<details>
<summary>Abstract</summary>
The prevalence of large-scale multimodal datasets presents unique challenges in assessing dataset quality. We propose a two-step method to analyze multimodal datasets, which leverages a small seed of human annotation to map each multimodal instance to the modalities required to process it. Our method sheds light on the importance of different modalities in datasets, as well as the relationship between them. We apply our approach to TVQA, a video question-answering dataset, and discover that most questions can be answered using a single modality, without a substantial bias towards any specific modality. Moreover, we find that more than 70% of the questions are solvable using several different single-modality strategies, e.g., by either looking at the video or listening to the audio, highlighting the limited integration of multiple modalities in TVQA. We leverage our annotation and analyze the MERLOT Reserve, finding that it struggles with image-based questions compared to text and audio, but also with auditory speaker identification. Based on our observations, we introduce a new test set that necessitates multiple modalities, observing a dramatic drop in model performance. Our methodology provides valuable insights into multimodal datasets and highlights the need for the development of more robust models.
</details>
<details>
<summary>摘要</summary>
现代大规模多modal数据集存在独特的评估数据集质量挑战。我们提出了一种两步方法，使用小量人工标注来将多modal实例映射到需要处理它的modalities。我们的方法揭示了不同modalities在数据集中的重要性，以及它们之间的关系。我们应用了我们的方法到TVQA视频问答数据集，发现大多数问题可以使用单一modalities解决，无论哪种modalities。此外，我们发现超过70%的问题可以使用多个不同的单modalities策略解决，例如通过视频或音频来解决问题，这 highlights TVQA中多modalities的有限整合。我们利用我们的标注和分析MERLOT Reserve，发现它在图像基于问题上表现不佳，比 Text和音频更差。基于我们的观察，我们引入了一个新的测试集，需要多modalities，观察模型性能异常下降。我们的方法ология为多modal数据集提供了有价值的洞察，并高亮了需要更robust的模型的开发。
</details></li>
</ul>
<hr>
<h2 id="Deep-Speech-Synthesis-from-MRI-Based-Articulatory-Representations"><a href="#Deep-Speech-Synthesis-from-MRI-Based-Articulatory-Representations" class="headerlink" title="Deep Speech Synthesis from MRI-Based Articulatory Representations"></a>Deep Speech Synthesis from MRI-Based Articulatory Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02471">http://arxiv.org/abs/2307.02471</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/articulatory/articulatory">https://github.com/articulatory/articulatory</a></li>
<li>paper_authors: Peter Wu, Tingle Li, Yijing Lu, Yubin Zhang, Jiachen Lian, Alan W Black, Louis Goldstein, Shinji Watanabe, Gopala K. Anumanchipalli</li>
<li>for: 这个研究旨在开发一种基于人类声道信息的语音合成方法，以提高语音合成效率、通用性和可解释性。</li>
<li>methods: 该研究使用MRI技术获取更广泛的声道信息，并引入Normalization和denoising等处理方法，以提高深度学习模型的普适性和语音质量。</li>
<li>results: 研究人员通过一系列ablations表明，MRI表示的声道信息更加全面和精准，并且可以提高语音合成效率和质量。<details>
<summary>Abstract</summary>
In this paper, we study articulatory synthesis, a speech synthesis method using human vocal tract information that offers a way to develop efficient, generalizable and interpretable synthesizers. While recent advances have enabled intelligible articulatory synthesis using electromagnetic articulography (EMA), these methods lack critical articulatory information like excitation and nasality, limiting generalization capabilities. To bridge this gap, we propose an alternative MRI-based feature set that covers a much more extensive articulatory space than EMA. We also introduce normalization and denoising procedures to enhance the generalizability of deep learning methods trained on MRI data. Moreover, we propose an MRI-to-speech model that improves both computational efficiency and speech fidelity. Finally, through a series of ablations, we show that the proposed MRI representation is more comprehensive than EMA and identify the most suitable MRI feature subset for articulatory synthesis.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了语音合成方法，即使用人类声门信息来开发高效、通用和可解释的合成器。最近的进展使得可以实现有声合成，但这些方法缺乏关键的唇形信息，如刺激和腔软度，这限制了其泛化能力。为了弥补这一点，我们提议一种基于MRI的特征集，覆盖了EMA的艺术iculatory空间的多倍。我们还提出了normalization和denoising的过程来提高深度学习方法在MRI数据上的泛化性。此外，我们提出了MRI-to-speech模型，可以提高计算效率和语音准确性。最后，通过一系列剥减实验，我们证明了我们的MRI表示是EMA的更加全面的，并确定了最适合语音合成的MRI特征子。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/06/eess.AS_2023_07_06/" data-id="cllsj9wzh005juv88cqut3gws" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_07_06" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/06/eess.IV_2023_07_06/" class="article-date">
  <time datetime="2023-07-05T16:00:00.000Z" itemprop="datePublished">2023-07-06</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/06/eess.IV_2023_07_06/">eess.IV - 2023-07-06 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Self-supervised-learning-via-inter-modal-reconstruction-and-feature-projection-networks-for-label-efficient-3D-to-2D-segmentation"><a href="#Self-supervised-learning-via-inter-modal-reconstruction-and-feature-projection-networks-for-label-efficient-3D-to-2D-segmentation" class="headerlink" title="Self-supervised learning via inter-modal reconstruction and feature projection networks for label-efficient 3D-to-2D segmentation"></a>Self-supervised learning via inter-modal reconstruction and feature projection networks for label-efficient 3D-to-2D segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03008">http://arxiv.org/abs/2307.03008</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/j-morano/multimodal-ssl-fpn">https://github.com/j-morano/multimodal-ssl-fpn</a></li>
<li>paper_authors: José Morano, Guilherme Aresta, Dmitrii Lachinov, Julia Mai, Ursula Schmidt-Erfurth, Hrvoje Bogunović</li>
<li>for: 该研究旨在提出一种标签效率高的三维到二维图像分割方法，以降低医生的工作负担。</li>
<li>methods: 该方法使用了一种新的卷积神经网络和自我超vised学习方法，其中包括了新的三维到二维块。</li>
<li>results: 实验结果显示，提出的方法可以在具有有限标签数据的场景下提高状态的艺术，达到8%的 dice分数提升。此外，自我超vised学习方法可以进一步提高这种性能，并且在不同的网络架构下都有益。<details>
<summary>Abstract</summary>
Deep learning has become a valuable tool for the automation of certain medical image segmentation tasks, significantly relieving the workload of medical specialists. Some of these tasks require segmentation to be performed on a subset of the input dimensions, the most common case being 3D-to-2D. However, the performance of existing methods is strongly conditioned by the amount of labeled data available, as there is currently no data efficient method, e.g. transfer learning, that has been validated on these tasks. In this work, we propose a novel convolutional neural network (CNN) and self-supervised learning (SSL) method for label-efficient 3D-to-2D segmentation. The CNN is composed of a 3D encoder and a 2D decoder connected by novel 3D-to-2D blocks. The SSL method consists of reconstructing image pairs of modalities with different dimensionality. The approach has been validated in two tasks with clinical relevance: the en-face segmentation of geographic atrophy and reticular pseudodrusen in optical coherence tomography. Results on different datasets demonstrate that the proposed CNN significantly improves the state of the art in scenarios with limited labeled data by up to 8% in Dice score. Moreover, the proposed SSL method allows further improvement of this performance by up to 23%, and we show that the SSL is beneficial regardless of the network architecture.
</details>
<details>
<summary>摘要</summary>
深度学习已成为医疗图像分割任务自动化的有价值工具，减轻医生的工作负担。一些这些任务需要在输入维度中进行分割，最常见的情况是从3D转换到2D。然而，现有方法的性能受到可用标注数据量的限制，例如没有数据效果的传输学习方法，这些方法没有被验证。在这种情况下，我们提出了一种新的卷积神经网络（CNN）和自我超vised学习（SSL）方法，用于标签效率3D-to-2D分割。CNN包括3D编码器和2D解码器连接的三个3D-to-2D块。SSL方法是使用不同维度的图像对创建图像对。我们在两个临床有 relevance 的任务中验证了该方法：在optical coherence tomography中的扁平分割和细胞ular pseudodrusen。不同的数据集结果表明，我们提出的CNN可以在有限标注数据情况下提高状态的艺术到8%的Dice分数。此外，我们还证明了SSL方法可以进一步改进这种性能，并且该方法对网络架构无关。
</details></li>
</ul>
<hr>
<h2 id="Fourier-Net-Leveraging-Band-Limited-Representation-for-Efficient-3D-Medical-Image-Registration"><a href="#Fourier-Net-Leveraging-Band-Limited-Representation-for-Efficient-3D-Medical-Image-Registration" class="headerlink" title="Fourier-Net+: Leveraging Band-Limited Representation for Efficient 3D Medical Image Registration"></a>Fourier-Net+: Leveraging Band-Limited Representation for Efficient 3D Medical Image Registration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02997">http://arxiv.org/abs/2307.02997</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xi-jia/fourier-net">https://github.com/xi-jia/fourier-net</a></li>
<li>paper_authors: Xi Jia, Alexander Thorley, Alberto Gomez, Wenqi Lu, Dipak Kotecha, Jinming Duan</li>
<li>for: 这个研究旨在提高Unsupervised Image Registration中的维度变换运算效率，使用Fourier-Net来取代传统的U-Net式维度变换网络。</li>
<li>methods: Fourier-Net使用参数无法模型驱动解oder来取代传统的U-Net式维度变换网络的复杂路径，并从Fourier domain中学习低维度的扩展场。Fourier-Net+进一步将内部网络的层数减少，并将内部网络的输入扩展至更大的对象网络。</li>
<li>results: 该研究表明，使用Fourier-Net和Fourier-Net+可以实现与现有方法相同的注册性能，并且具有更快的测试速度、更低的内存压缩和更少的乘法加法操作。此外，Fourier-Net+可以实现大规模3D注册的高效训练。<details>
<summary>Abstract</summary>
U-Net style networks are commonly utilized in unsupervised image registration to predict dense displacement fields, which for high-resolution volumetric image data is a resource-intensive and time-consuming task. To tackle this challenge, we first propose Fourier-Net, which replaces the costly U-Net style expansive path with a parameter-free model-driven decoder. Instead of directly predicting a full-resolution displacement field, our Fourier-Net learns a low-dimensional representation of the displacement field in the band-limited Fourier domain which our model-driven decoder converts to a full-resolution displacement field in the spatial domain. Expanding upon Fourier-Net, we then introduce Fourier-Net+, which additionally takes the band-limited spatial representation of the images as input and further reduces the number of convolutional layers in the U-Net style network's contracting path. Finally, to enhance the registration performance, we propose a cascaded version of Fourier-Net+. We evaluate our proposed methods on three datasets, on which our proposed Fourier-Net and its variants achieve comparable results with current state-of-the art methods, while exhibiting faster inference speeds, lower memory footprint, and fewer multiply-add operations. With such small computational cost, our Fourier-Net+ enables the efficient training of large-scale 3D registration on low-VRAM GPUs. Our code is publicly available at \url{https://github.com/xi-jia/Fourier-Net}.
</details>
<details>
<summary>摘要</summary>
U-Net风格网络通常用于无监督图像registrations，预测高分辨率三维图像数据中的密集偏移场景。为了解决这个挑战，我们首先提出了Fourier-Net，它将U-Net风格的昂贵expandive路径换成一个无参数的模型驱动decoder。而不是直接预测全分辨率偏移场景，Fourier-Net学习了带限 FourierDomain中的低维度偏移场景表示。我们的model-driven decoder将这个低维度表示转换成全分辨率偏移场景。在这基础之上，我们还提出了Fourier-Net+，它还接受了带限 spatial表示的图像，并降低了U-Net风格网络的 contraction path中的卷积层数。最后，我们提出了Fourier-Net+的卷积版本，用于进一步提高registrations的性能。我们在三个dataset上评估了我们的提议方法，其中Fourier-Net和其 variants具有与当前状态的方法相同的性能，而且具有更快的推理速度、更低的内存占用和更少的 multiply-add操作。这样的小计算成本使得我们的Fourier-Net+在低VRAM GPU上能够高效地训练大规模3D registration。我们的代码publicly available at \url{https://github.com/xi-jia/Fourier-Net}.
</details></li>
</ul>
<hr>
<h2 id="Noise-to-Norm-Reconstruction-for-Industrial-Anomaly-Detection-and-Localization"><a href="#Noise-to-Norm-Reconstruction-for-Industrial-Anomaly-Detection-and-Localization" class="headerlink" title="Noise-to-Norm Reconstruction for Industrial Anomaly Detection and Localization"></a>Noise-to-Norm Reconstruction for Industrial Anomaly Detection and Localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02836">http://arxiv.org/abs/2307.02836</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiqi Deng, Zhiyu Sun, Ruiyan Zhuang, Jun Gong</li>
<li>for: 这个研究旨在提出一种基于恢复Error的异常检测方法，以便在物品位置变化较大的数据集上进行异常检测和地址化。</li>
<li>methods: 该方法使用了杂变恢复网络，包括多尺度融合和复原注意力模块，实现了端到端异常检测和定位。</li>
<li>results: 实验表明，该方法能够准确地检测和定位异常区域，并在MPDD和VisA数据集上达到了最新的方法的竞争性成绩。<details>
<summary>Abstract</summary>
Anomaly detection has a wide range of applications and is especially important in industrial quality inspection. Currently, many top-performing anomaly-detection models rely on feature-embedding methods. However, these methods do not perform well on datasets with large variations in object locations. Reconstruction-based methods use reconstruction errors to detect anomalies without considering positional differences between samples. In this study, a reconstruction-based method using the noise-to-norm paradigm is proposed, which avoids the invariant reconstruction of anomalous regions. Our reconstruction network is based on M-net and incorporates multiscale fusion and residual attention modules to enable end-to-end anomaly detection and localization. Experiments demonstrate that the method is effective in reconstructing anomalous regions into normal patterns and achieving accurate anomaly detection and localization. On the MPDD and VisA datasets, our proposed method achieved more competitive results than the latest methods, and it set a new state-of-the-art standard on the MPDD dataset.
</details>
<details>
<summary>摘要</summary>
《异常检测在各种应用领域有广泛的应用，特别是在工业质量检测中非常重要。目前许多高性能异常检测模型都采用特征嵌入方法。但这些方法在样本位置差异较大的数据集上表现不佳。重建基于方法利用重建错误来检测异常，而不考虑样本间位置差异。在本研究中，我们提出了基于噪声至常数据（noise-to-norm）的重建基于方法，以避免异常区域的恒常重建。我们的重建网络基于M-net，并包括多尺度融合和剩余注意模块，以实现端到端异常检测和地图化。实验表明，我们的提议方法可以有效地将异常区域重建为常见模式，并实现高精度的异常检测和地图化。在MPDD和VisA数据集上，我们的提议方法超过了最新的方法，并在MPDD数据集上设置了新的状态标准。》Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Bundle-specific-Tractogram-Distribution-Estimation-Using-Higher-order-Streamline-Differential-Equation"><a href="#Bundle-specific-Tractogram-Distribution-Estimation-Using-Higher-order-Streamline-Differential-Equation" class="headerlink" title="Bundle-specific Tractogram Distribution Estimation Using Higher-order Streamline Differential Equation"></a>Bundle-specific Tractogram Distribution Estimation Using Higher-order Streamline Differential Equation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02825">http://arxiv.org/abs/2307.02825</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanjing Feng, Lei Xie, Jingqiang Wang, Jianzhong He, Fei Gao<br>for: This paper aims to improve tractography methods for reconstructing complex global fiber bundles in the brain, which are prone to producing erroneous tracks and missing true positive connections.methods: The proposed method uses a bundle-specific tractogram distribution function based on a higher-order streamline differential equation, which reconstructs streamline bundles in a “cluster to cluster” manner. The method also introduces anatomic priors to guide the tractography process and improve the accuracy of fiber bundle reconstruction.results: The proposed method demonstrates better results in reconstructing long-range, twisting, and large fanning tracts compared to traditional peaks-based tractography methods. The method also reduces the error deviation and accumulation at the local level, and provides a more accurate representation of the complex global fiber bundles in the brain.<details>
<summary>Abstract</summary>
Tractography traces the peak directions extracted from fiber orientation distribution (FOD) suffering from ambiguous spatial correspondences between diffusion directions and fiber geometry, which is prone to producing erroneous tracks while missing true positive connections. The peaks-based tractography methods 'locally' reconstructed streamlines in 'single to single' manner, thus lacking of global information about the trend of the whole fiber bundle. In this work, we propose a novel tractography method based on a bundle-specific tractogram distribution function by using a higher-order streamline differential equation, which reconstructs the streamline bundles in 'cluster to cluster' manner. A unified framework for any higher-order streamline differential equation is presented to describe the fiber bundles with disjoint streamlines defined based on the diffusion tensor vector field. At the global level, the tractography process is simplified as the estimation of bundle-specific tractogram distribution (BTD) coefficients by minimizing the energy optimization model, and is used to characterize the relations between BTD and diffusion tensor vector under the prior guidance by introducing the tractogram bundle information to provide anatomic priors. Experiments are performed on simulated Hough, Sine, Circle data, ISMRM 2015 Tractography Challenge data, FiberCup data, and in vivo data from the Human Connectome Project (HCP) data for qualitative and quantitative evaluation. The results demonstrate that our approach can reconstruct the complex global fiber bundles directly. BTD reduces the error deviation and accumulation at the local level and shows better results in reconstructing long-range, twisting, and large fanning tracts.
</details>
<details>
<summary>摘要</summary>
tractography 跟踪 peak 方向，从 fiber orientation distribution (FOD) 中提取的方向，受到不确定的空间相对关系，容易生成错误的轨迹，而且错过真实正确的连接。 peaks-based tractography 方法在 'single to single' manner 地重建流线，缺乏全局信息，不能捕捉整个纤维Bundle的趋势。在这种工作中，我们提出了一种基于纤维特有 tractogram distribution 函数的新的 tractography 方法，使用高阶流线差分方程，重建流线集在 'cluster to cluster' manner 中。我们提出了一个综合的框架，用于描述纤维Bundle中的不同流线。在全局水平， tractography 过程被简化为估计纤维特有 tractogram distribution（BTD）系数，并用来描述纤维Bundle中的纤维tensor vector 场的关系。我们引入了 tractogram 短信息，以提供生物学上的先验知识。我们在 Hough、Sine、Circle 数据、ISMRM 2015 Tractography Challenge 数据、FiberCup 数据和人类连接计划（HCP）数据进行了质量和量化评估。结果表明，我们的方法可以直接重建复杂的全局纤维Bundle。BTD 降低了地方错误和积累，并且更好地重建长距离、扭转和大扇辐 tracts。
</details></li>
</ul>
<hr>
<h2 id="Single-Image-LDR-to-HDR-Conversion-using-Conditional-Diffusion"><a href="#Single-Image-LDR-to-HDR-Conversion-using-Conditional-Diffusion" class="headerlink" title="Single Image LDR to HDR Conversion using Conditional Diffusion"></a>Single Image LDR to HDR Conversion using Conditional Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02814">http://arxiv.org/abs/2307.02814</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dwip Dalal, Gautam Vashishtha, Prajwal Singh, Shanmuganathan Raman</li>
<li>for: 该论文旨在使用深度学习方法来复制真实的场景，但低动态范围（LDR）摄像头无法表示实际场景的广泛的动态范围，导致图像被折制或过度曝光。</li>
<li>methods: 该论文提出了一种基于深度学习的方法，用于从阴影和突出处恢复细节，同时重建高动态范围（HDR）图像。我们将问题定义为一个图像到图像（I2I）翻译任务，并提出了一种基于conditioned Denoising Diffusion Probabilistic Model（DDPM）的框架。我们在该框架中添加了一个深度神经网络基于autoencoder来提高输入LDR图像的latent表示质量。此外，我们还提出了一种新的损失函数，称为曝光损失（Exposure Loss），该损失函数可以更好地导向梯度的方向，进一步提高结果的质量。</li>
<li>results: 我们通过对比性和质量测试，证明了我们提出的方法的效果。结果表明，一种简单的conditioned diffusion-based方法可以取代复杂的摄像头管线基础架构。<details>
<summary>Abstract</summary>
Digital imaging aims to replicate realistic scenes, but Low Dynamic Range (LDR) cameras cannot represent the wide dynamic range of real scenes, resulting in under-/overexposed images. This paper presents a deep learning-based approach for recovering intricate details from shadows and highlights while reconstructing High Dynamic Range (HDR) images. We formulate the problem as an image-to-image (I2I) translation task and propose a conditional Denoising Diffusion Probabilistic Model (DDPM) based framework using classifier-free guidance. We incorporate a deep CNN-based autoencoder in our proposed framework to enhance the quality of the latent representation of the input LDR image used for conditioning. Moreover, we introduce a new loss function for LDR-HDR translation tasks, termed Exposure Loss. This loss helps direct gradients in the opposite direction of the saturation, further improving the results' quality. By conducting comprehensive quantitative and qualitative experiments, we have effectively demonstrated the proficiency of our proposed method. The results indicate that a simple conditional diffusion-based method can replace the complex camera pipeline-based architectures.
</details>
<details>
<summary>摘要</summary>
数字图像处理目标是复制真实场景，但低动态范围（LDR）摄像机无法表示实际场景的广泛动态范围，导致图像被折制或过度曝光。这篇论文提出了基于深度学习的方法，用于从阴影和突出处恢复细节，同时重建高动态范围（HDR）图像。我们将问题定义为图像到图像（I2I）翻译任务，并提出了基于conditional Denoising Diffusion Probabilistic Model（DDPM）的框架。我们在我们的提议框架中集成了深度 neural network（CNN）基于autoencoder，以提高输入LDR图像的内存质量。此外，我们还引入了一种新的损失函数，称为曝光损失（Exposure Loss），该损失函数可以引导梯度的方向与暴露度相反，进一步提高结果的质量。通过进行全面的量化和质量测试，我们有效地表明了我们的提议方法的效果。结果表明，一种简单的增强 diffusion-based方法可以取代复杂的摄像机管线 Architecture。
</details></li>
</ul>
<hr>
<h2 id="Advancing-Zero-Shot-Digital-Human-Quality-Assessment-through-Text-Prompted-Evaluation"><a href="#Advancing-Zero-Shot-Digital-Human-Quality-Assessment-through-Text-Prompted-Evaluation" class="headerlink" title="Advancing Zero-Shot Digital Human Quality Assessment through Text-Prompted Evaluation"></a>Advancing Zero-Shot Digital Human Quality Assessment through Text-Prompted Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02808">http://arxiv.org/abs/2307.02808</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zzc-1998/sjtu-h3d">https://github.com/zzc-1998/sjtu-h3d</a></li>
<li>paper_authors: Zicheng Zhang, Wei Sun, Yingjie Zhou, Haoning Wu, Chunyi Li, Xiongkuo Min, Xiaohong Liu, Guangtao Zhai, Weisi Lin</li>
<li>for: 本研究旨在提供一个全身数字人质量评估（DHQA）数据库，以便进行数字人质量评估研究。</li>
<li>methods: 本研究使用了7种类型的扭曲方法生成1,120个扭曲的参考数字人，以及40个高品质的参考数字人。furthermore, the research proposes a zero-shot DHQA approach that leverages semantic and distortion features extracted from projections, as well as geometry features derived from the mesh structure of digital humans.</li>
<li>results: 研究结果显示，Zero-shot DHQA方法可以在不受数据库偏见影响的情况下，实现一定的质量评估表现。此外，研究也引入了数字人质量指数（DHQI），可以作为一个强大的基eline，促进领域的进步。<details>
<summary>Abstract</summary>
Digital humans have witnessed extensive applications in various domains, necessitating related quality assessment studies. However, there is a lack of comprehensive digital human quality assessment (DHQA) databases. To address this gap, we propose SJTU-H3D, a subjective quality assessment database specifically designed for full-body digital humans. It comprises 40 high-quality reference digital humans and 1,120 labeled distorted counterparts generated with seven types of distortions. The SJTU-H3D database can serve as a benchmark for DHQA research, allowing evaluation and refinement of processing algorithms. Further, we propose a zero-shot DHQA approach that focuses on no-reference (NR) scenarios to ensure generalization capabilities while mitigating database bias. Our method leverages semantic and distortion features extracted from projections, as well as geometry features derived from the mesh structure of digital humans. Specifically, we employ the Contrastive Language-Image Pre-training (CLIP) model to measure semantic affinity and incorporate the Naturalness Image Quality Evaluator (NIQE) model to capture low-level distortion information. Additionally, we utilize dihedral angles as geometry descriptors to extract mesh features. By aggregating these measures, we introduce the Digital Human Quality Index (DHQI), which demonstrates significant improvements in zero-shot performance. The DHQI can also serve as a robust baseline for DHQA tasks, facilitating advancements in the field. The database and the code are available at https://github.com/zzc-1998/SJTU-H3D.
</details>
<details>
<summary>摘要</summary>
“数字人类”在各个领域得到了广泛的应用，但是相关的质量评估研究受到了不足的全面数字人质量评估（DHQA）数据库的限制。为了解决这一问题，我们提出了《上海交通大学高质量数字人质量评估数据库》（SJTU-H3D），这是专门为全身数字人创建的主观质量评估数据库。它包括40个高质量参照数字人和1,120个扭曲对应件，通过七种扭曲方法生成。SJTU-H3D数据库可以作为DHQA研究的标准准确，用于评估和优化处理算法。此外，我们提出了一种零批量DHQA方法，专注于无参（NR）场景，以确保泛化能力的同时避免数据库偏见。我们的方法利用CLIP模型测量语义相似性，并使用NIQE模型捕捉低级扭曲信息。此外，我们还利用截角角度来描述数字人的网格结构，从而提取网格特征。通过积合这些度量，我们引入了数字人质量指数（DHQI），它在零批量情况下显示出了显著的改善。DHQI还可以作为DHQA任务的Robust基准，促进领域的进步。SJTU-H3D数据库和代码可以在GitHub上下载。
</details></li>
</ul>
<hr>
<h2 id="Retinex-based-Image-Denoising-Contrast-Enhancement-using-Gradient-Graph-Laplacian-Regularizer"><a href="#Retinex-based-Image-Denoising-Contrast-Enhancement-using-Gradient-Graph-Laplacian-Regularizer" class="headerlink" title="Retinex-based Image Denoising &#x2F; Contrast Enhancement using Gradient Graph Laplacian Regularizer"></a>Retinex-based Image Denoising &#x2F; Contrast Enhancement using Gradient Graph Laplacian Regularizer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02625">http://arxiv.org/abs/2307.02625</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yeganeh Gharedaghi, Gene Cheung, Xianming Liu</li>
<li>for: 提高低光照图像质量</li>
<li>methods: 使用图像Retinex理论和图像laplacian regularizer进行降噪和增强对比</li>
<li>results: 实验结果表明，我们的算法可以提高图像质量，同时减少计算复杂度<details>
<summary>Abstract</summary>
Images captured in poorly lit conditions are often corrupted by acquisition noise. Leveraging recent advances in graph-based regularization, we propose a fast Retinex-based restoration scheme that denoises and contrast-enhances an image. Specifically, by Retinex theory we first assume that each image pixel is a multiplication of its reflectance and illumination components. We next assume that the reflectance and illumination components are piecewise constant (PWC) and continuous piecewise planar (PWP) signals, which can be recovered via graph Laplacian regularizer (GLR) and gradient graph Laplacian regularizer (GGLR) respectively. We formulate quadratic objectives regularized by GLR and GGLR, which are minimized alternately until convergence by solving linear systems -- with improved condition numbers via proposed preconditioners -- via conjugate gradient (CG) efficiently. Experimental results show that our algorithm achieves competitive visual image quality while reducing computation complexity noticeably.
</details>
<details>
<summary>摘要</summary>
图像采集在低光照条件下经常受到获取噪声的损害。我们基于最新的图格基于常量化的正则化技术，提出一种快速的Retinex基于的图像修复方案，可以去噪和提高图像的对比度。具体来说，我们首先假设每个图像像素是其反射和照明组成部分的乘积。我们接下来假设反射和照明组成部分是连续的 piecwise 板状信号（PWP）和 piecewise 常量信号（PWC），可以通过图像拉普拉斯正则izer（GLR）和梯度图像拉普拉斯正则izer（GGLR）分别进行回归。我们定义了quadratic 目标函数，其中GLR和GGLR的正则化项被加以规范化，然后通过 conjugate gradient（CG）高效地解决线性系统，并使用我们提出的预conditioner来提高condition number。实验结果表明，我们的算法可以达到竞争力强的视觉图像质量，同时减少计算复杂度明显。
</details></li>
</ul>
<hr>
<h2 id="AxonCallosumEM-Dataset-Axon-Semantic-Segmentation-of-Whole-Corpus-Callosum-cross-section-from-EM-Images"><a href="#AxonCallosumEM-Dataset-Axon-Semantic-Segmentation-of-Whole-Corpus-Callosum-cross-section-from-EM-Images" class="headerlink" title="AxonCallosumEM Dataset: Axon Semantic Segmentation of Whole Corpus Callosum cross section from EM Images"></a>AxonCallosumEM Dataset: Axon Semantic Segmentation of Whole Corpus Callosum cross section from EM Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02464">http://arxiv.org/abs/2307.02464</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ao Cheng, Guoqiang Zhao, Lirong Wang, Ruobing Zhang</li>
<li>for: 这个研究的目的是为了精确地重建动物神经系统中的 axon 和 myelin sheath 的复杂结构，以及提供大规模 EM 数据集，以促进和评估全面的 corpus callosum 重建。</li>
<li>methods: 这个研究使用了 Electron Microscope (EM) 技术，并开发了一个基于 Segment Anything Model (SAM) 的 fine-tuning 方法，以进行 EM 图像分类 задачі的推导。</li>
<li>results: 这个研究获得了一个名为 AxonCallosumEM 的大规模 EM 数据集，并使用了这个数据集进行训练和测试。EM-SAM 方法在这个数据集上表现出色，与其他现有的方法相比，具有更高的准确率和更好的一致性。<details>
<summary>Abstract</summary>
The electron microscope (EM) remains the predominant technique for elucidating intricate details of the animal nervous system at the nanometer scale. However, accurately reconstructing the complex morphology of axons and myelin sheaths poses a significant challenge. Furthermore, the absence of publicly available, large-scale EM datasets encompassing complete cross sections of the corpus callosum, with dense ground truth segmentation for axons and myelin sheaths, hinders the advancement and evaluation of holistic corpus callosum reconstructions. To surmount these obstacles, we introduce the AxonCallosumEM dataset, comprising a 1.83 times 5.76mm EM image captured from the corpus callosum of the Rett Syndrome (RTT) mouse model, which entail extensive axon bundles. We meticulously proofread over 600,000 patches at a resolution of 1024 times 1024, thus providing a comprehensive ground truth for myelinated axons and myelin sheaths. Additionally, we extensively annotated three distinct regions within the dataset for the purposes of training, testing, and validation. Utilizing this dataset, we develop a fine-tuning methodology that adapts Segment Anything Model (SAM) to EM images segmentation tasks, called EM-SAM, enabling outperforms other state-of-the-art methods. Furthermore, we present the evaluation results of EM-SAM as a baseline.
</details>
<details>
<summary>摘要</summary>
电子顾 microscope (EM) 仍然是研究动物神经系统细胞级结构的主要技术。然而，准确地重建神经元和脱膜的复杂形态却是一项非常大的挑战。此外，没有公共可用的大规模 EM 数据集覆盖整个脊梗，包括坚实的基本 truth 分类 для神经元和脱膜，使得整体脊梗重建的进步和评估受到了限制。为了突破这些障碍，我们介绍了 AxonCallosumEM 数据集，包括来自脊梗综合症 (RTT) 小鼠模型的 1.83 times 5.76mm EM 图像，其中包含了广泛的 axon 集合。我们仔细检查了超过 600,000 个小块，解决了 1024 times 1024 的分辨率，以提供脱膜神经元和脱膜的完整基本实际。此外，我们还为了训练、测试和验证而进行了三个不同的区域的杂志。使用这个数据集，我们开发了一种基于 Segment Anything Model (SAM) 的微调方法，称为 EM-SAM，该方法可以超越其他当前的状况。此外，我们还提供了 EM-SAM 的评估结果作为基准。
</details></li>
</ul>
<hr>
<h2 id="Expert-Agnostic-Ultrasound-Image-Quality-Assessment-using-Deep-Variational-Clustering"><a href="#Expert-Agnostic-Ultrasound-Image-Quality-Assessment-using-Deep-Variational-Clustering" class="headerlink" title="Expert-Agnostic Ultrasound Image Quality Assessment using Deep Variational Clustering"></a>Expert-Agnostic Ultrasound Image Quality Assessment using Deep Variational Clustering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02462">http://arxiv.org/abs/2307.02462</a></li>
<li>repo_url: None</li>
<li>paper_authors: Deepak Raina, Dimitrios Ntentia, SH Chandrashekhara, Richard Voyles, Subir Kumar Saha</li>
<li>for: 用于评估ultrasound图像质量，自动化评估可以减少操作员依赖性和Subjective variation。</li>
<li>methods: 提出了一种不需要手动标注的自动评估方法，使用variational autoencoder加载了三个模块：预处理、分组和后处理，以增强、提取、分组和可视化ultrasound图像质量特征表示。</li>
<li>results: 对尿道ultrasound图像质量进行评估，实现了78%的准确率和与状态元方法相比的更高性能。<details>
<summary>Abstract</summary>
Ultrasound imaging is a commonly used modality for several diagnostic and therapeutic procedures. However, the diagnosis by ultrasound relies heavily on the quality of images assessed manually by sonographers, which diminishes the objectivity of the diagnosis and makes it operator-dependent. The supervised learning-based methods for automated quality assessment require manually annotated datasets, which are highly labour-intensive to acquire. These ultrasound images are low in quality and suffer from noisy annotations caused by inter-observer perceptual variations, which hampers learning efficiency. We propose an UnSupervised UltraSound image Quality assessment Network, US2QNet, that eliminates the burden and uncertainty of manual annotations. US2QNet uses the variational autoencoder embedded with the three modules, pre-processing, clustering and post-processing, to jointly enhance, extract, cluster and visualize the quality feature representation of ultrasound images. The pre-processing module uses filtering of images to point the network's attention towards salient quality features, rather than getting distracted by noise. Post-processing is proposed for visualizing the clusters of feature representations in 2D space. We validated the proposed framework for quality assessment of the urinary bladder ultrasound images. The proposed framework achieved 78% accuracy and superior performance to state-of-the-art clustering methods.
</details>
<details>
<summary>摘要</summary>
ultrasound imaging 是一种广泛使用的Modalities，用于许多诊断和治疗过程。然而，由于诊断基于ultrasound的图像质量，它取决于sonographers manually assessing the images的质量，这会使诊断变得人工依赖。supervised learning-based方法需要手动标注的数据集，这是非常劳动密集的获得。这些ultrasound图像质量低，并且受到了 manually annotated datasets的噪音干扰，这会降低学习效率。我们提出了一种不需要手动标注的自主ultrasound图像质量评估网络，称为US2QNet。US2QNet使用包括预处理、聚类和后处理三个模块的变分自动编码器，并在一起进行协同增强、提取、聚类和可视化ultrasound图像质量特征表示。预处理模块使用图像滤波器，将网络的注意力集中在有关质量特征的图像特征上，而不是受到噪音的扰乱。后处理是为可视化特征表示的集群在2D空间进行提出。我们验证了提出的框架，用于评估膀胱ultrasound图像质量。提出的框架实现了78%的准确率，并超过了状态的聚类方法的性能。
</details></li>
</ul>
<hr>
<h2 id="LLCaps-Learning-to-Illuminate-Low-Light-Capsule-Endoscopy-with-Curved-Wavelet-Attention-and-Reverse-Diffusion"><a href="#LLCaps-Learning-to-Illuminate-Low-Light-Capsule-Endoscopy-with-Curved-Wavelet-Attention-and-Reverse-Diffusion" class="headerlink" title="LLCaps: Learning to Illuminate Low-Light Capsule Endoscopy with Curved Wavelet Attention and Reverse Diffusion"></a>LLCaps: Learning to Illuminate Low-Light Capsule Endoscopy with Curved Wavelet Attention and Reverse Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02452">http://arxiv.org/abs/2307.02452</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/longbai1006/llcaps">https://github.com/longbai1006/llcaps</a></li>
<li>paper_authors: Long Bai, Tong Chen, Yanan Wu, An Wang, Mobarakol Islam, Hongliang Ren</li>
<li>for: 这个研究是为了提高无线电囊镜检查（WCE）的病理诊断效率和精确度，使其成为胃肠病诊断中不可或缺的工具。</li>
<li>methods: 本研究提出了一个基于多层态击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击击�<details>
<summary>Abstract</summary>
Wireless capsule endoscopy (WCE) is a painless and non-invasive diagnostic tool for gastrointestinal (GI) diseases. However, due to GI anatomical constraints and hardware manufacturing limitations, WCE vision signals may suffer from insufficient illumination, leading to a complicated screening and examination procedure. Deep learning-based low-light image enhancement (LLIE) in the medical field gradually attracts researchers. Given the exuberant development of the denoising diffusion probabilistic model (DDPM) in computer vision, we introduce a WCE LLIE framework based on the multi-scale convolutional neural network (CNN) and reverse diffusion process. The multi-scale design allows models to preserve high-resolution representation and context information from low-resolution, while the curved wavelet attention (CWA) block is proposed for high-frequency and local feature learning. Furthermore, we combine the reverse diffusion procedure to further optimize the shallow output and generate the most realistic image. The proposed method is compared with ten state-of-the-art (SOTA) LLIE methods and significantly outperforms quantitatively and qualitatively. The superior performance on GI disease segmentation further demonstrates the clinical potential of our proposed model. Our code is publicly accessible.
</details>
<details>
<summary>摘要</summary>
无线胶囊内镜（WCE）是一种不痛不侵的诊断工具 для Gastrointestinal（GI）疾病。然而，由于GI生物学结构和硬件生产限制，WCE视图信号可能受到不足照明的影响，导致诊断和检查过程复杂。在医学领域，深度学习基于图像提高（LLIE）逐渐吸引研究人员。基于计算机视觉的DDPM模型在发展的过程中，我们介绍了一种基于多尺度卷积神经网络（CNN）和反扩散过程的WCE LLIE框架。多尺度设计使得模型保留高分辨率表示和上下文信息，而弯曲波形注意力（CWA）块则用于高频和本地特征学习。此外，我们将反扩散过程与模型结合，以进一步优化浅输出并生成最真实的图像。我们的提案与现有的10种SOTA LLIE方法进行比较，并显著超越量化和质量上。此外，我们还通过GI疾病分割诊断的超越性表现，更加证明了我们的提案在临床中的潜在应用。我们的代码公共可访问。
</details></li>
</ul>
<hr>
<h2 id="Base-Layer-Efficiency-in-Scalable-Human-Machine-Coding"><a href="#Base-Layer-Efficiency-in-Scalable-Human-Machine-Coding" class="headerlink" title="Base Layer Efficiency in Scalable Human-Machine Coding"></a>Base Layer Efficiency in Scalable Human-Machine Coding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02430">http://arxiv.org/abs/2307.02430</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yalda Foroutan, Alon Harell, Anderson de Andrade, Ivan V. Bajić</li>
<li>for: 这个论文是为了提高现代可扩展人机图像编码器的基层编码效率而写的。</li>
<li>methods: 论文使用了现有的人机图像编码器，并对其进行了分层编码和对比分析，以提高基层编码效率。</li>
<li>results: 论文表明，通过对基层编码进行优化，可以提高BD-Rate的效率， Specifically, the paper shows that gains of 20-40% in BD-Rate compared to the current best results on object detection and instance segmentation are possible.<details>
<summary>Abstract</summary>
A basic premise in scalable human-machine coding is that the base layer is intended for automated machine analysis and is therefore more compressible than the same content would be for human viewing. Use cases for such coding include video surveillance and traffic monitoring, where the majority of the content will never be seen by humans. Therefore, base layer efficiency is of paramount importance because the system would most frequently operate at the base-layer rate. In this paper, we analyze the coding efficiency of the base layer in a state-of-the-art scalable human-machine image codec, and show that it can be improved. In particular, we demonstrate that gains of 20-40% in BD-Rate compared to the currently best results on object detection and instance segmentation are possible.
</details>
<details>
<summary>摘要</summary>
基本前提是可扩展的人机编程的基层是为机器自动分析而设计，因此基层内容比同样的内容用于人类视觉更容易压缩。使用场景包括视频监控和交通监测，大多数内容从未被人类查看。因此，基层的编码效率非常重要，系统大多数时间都会在基层级别运行。在这篇论文中，我们分析了一个现代可扩展人机图像编码器的基层编码效率，并证明可以提高。特别是，我们示出了对 объек detection和实例 segmentation的Current best results可以提高20-40%的BD-Rate。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/06/eess.IV_2023_07_06/" data-id="cllsj9wzy007auv883d3lbvi2" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_07_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/05/cs.LG_2023_07_05/" class="article-date">
  <time datetime="2023-07-04T16:00:00.000Z" itemprop="datePublished">2023-07-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/05/cs.LG_2023_07_05/">cs.LG - 2023-07-05 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Machine-learning-at-the-mesoscale-a-computation-dissipation-bottleneck"><a href="#Machine-learning-at-the-mesoscale-a-computation-dissipation-bottleneck" class="headerlink" title="Machine learning at the mesoscale: a computation-dissipation bottleneck"></a>Machine learning at the mesoscale: a computation-dissipation bottleneck</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02379">http://arxiv.org/abs/2307.02379</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alessandro Ingrosso, Emanuele Panizon</li>
<li>for: 该论文旨在探讨物理系统中信息处理成本的兼顾问题，即性能和能getic费用之间的trade-off。</li>
<li>methods: 该论文采用了 computation-dissipation bottleneck  frameworks，通过使用实际数据集和 sintetic任务，证明了不对称互动的存在导致了性能提高。</li>
<li>results: 该论文的研究结果表明，在输入输出设备中，非平衡情况会导致信息压缩、计算输入输出和动力不逆转行为的均衡。<details>
<summary>Abstract</summary>
The cost of information processing in physical systems calls for a trade-off between performance and energetic expenditure. Here we formulate and study a computation-dissipation bottleneck in mesoscopic systems used as input-output devices. Using both real datasets and synthetic tasks, we show how non-equilibrium leads to enhanced performance. Our framework sheds light on a crucial compromise between information compression, input-output computation and dynamic irreversibility induced by non-reciprocal interactions.
</details>
<details>
<summary>摘要</summary>
信息处理成本在物理系统中需要一种权衡 между性能和能getic耗用。我们在 mesoscopic 系统中作为输入输出设备形式ulated computation-dissipation bottleneck，并使用实际数据和synthetic任务来表征非平衡导致性能增强。我们的框架揭示了一种关键的信息压缩、输入输出计算和动力不可逆性引起的妥协。
</details></li>
</ul>
<hr>
<h2 id="Continuum-Limits-of-Ollivier’s-Ricci-Curvature-on-data-clouds-pointwise-consistency-and-global-lower-bounds"><a href="#Continuum-Limits-of-Ollivier’s-Ricci-Curvature-on-data-clouds-pointwise-consistency-and-global-lower-bounds" class="headerlink" title="Continuum Limits of Ollivier’s Ricci Curvature on data clouds: pointwise consistency and global lower bounds"></a>Continuum Limits of Ollivier’s Ricci Curvature on data clouds: pointwise consistency and global lower bounds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02378">http://arxiv.org/abs/2307.02378</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicolas Garcia Trillos, Melanie Weber</li>
<li>for: 研究了一个低维抽象 $\mathcal{M} \subseteq \mathbb{R}^d$ 上的Random geometric graph的 curvature与抽象 $\mathcal{M}$ 的曲率之间的关系，通过维度下的连续假设。</li>
<li>methods: 使用了Ollivier的离散 Ricci curvature的连续假设，并证明了点性不变的consistency结果，以及如果 $\mathcal{M}$ 有下界为正数的 Ricci curvature，那么Random geometric graph将高概率拥有这种全球结构性。</li>
<li>results: 显示了热核辐射在图上的 contraction 性和抽象 $\mathcal{M}$ 的曲率之间的关系，以及从数据云中学习抽象 $\mathcal{M}$ 的应用。特别是，证明了consistency结果可以用来描述抽象 $\mathcal{M}$ 的内在曲率从外在曲率中。<details>
<summary>Abstract</summary>
Let $\mathcal{M} \subseteq \mathbb{R}^d$ denote a low-dimensional manifold and let $\mathcal{X}= \{ x_1, \dots, x_n \}$ be a collection of points uniformly sampled from $\mathcal{M}$. We study the relationship between the curvature of a random geometric graph built from $\mathcal{X}$ and the curvature of the manifold $\mathcal{M}$ via continuum limits of Ollivier's discrete Ricci curvature. We prove pointwise, non-asymptotic consistency results and also show that if $\mathcal{M}$ has Ricci curvature bounded from below by a positive constant, then the random geometric graph will inherit this global structural property with high probability. We discuss applications of the global discrete curvature bounds to contraction properties of heat kernels on graphs, as well as implications for manifold learning from data clouds. In particular, we show that the consistency results allow for characterizing the intrinsic curvature of a manifold from extrinsic curvature.
</details>
<details>
<summary>摘要</summary>
Let $\mathcal{M} \subseteq \mathbb{R}^d$ be a low-dimensional manifold, and let $\mathcal{X} = \{ x_1, \dots, x_n \}$ be a collection of points uniformly sampled from $\mathcal{M}$. We study the relationship between the curvature of a random geometric graph built from $\mathcal{X}$ and the curvature of the manifold $\mathcal{M}$ via continuum limits of Ollivier's discrete Ricci curvature. We prove pointwise, non-asymptotic consistency results and also show that if $\mathcal{M}$ has Ricci curvature bounded from below by a positive constant, then the random geometric graph will inherit this global structural property with high probability. We discuss applications of the global discrete curvature bounds to contraction properties of heat kernels on graphs, as well as implications for manifold learning from data clouds. In particular, we show that the consistency results allow for characterizing the intrinsic curvature of a manifold from extrinsic curvature.Here is the translation in Traditional Chinese:Let $\mathcal{M} \subseteq \mathbb{R}^d$ be a low-dimensional manifold, and let $\mathcal{X} = \{ x_1, \dots, x_n \}$ be a collection of points uniformly sampled from $\mathcal{M}$. We study the relationship between the curvature of a random geometric graph built from $\mathcal{X}$ and the curvature of the manifold $\mathcal{M}$ via continuum limits of Ollivier's discrete Ricci curvature. We prove pointwise, non-asymptotic consistency results and also show that if $\mathcal{M}$ has Ricci curvature bounded from below by a positive constant, then the random geometric graph will inherit this global structural property with high probability. We discuss applications of the global discrete curvature bounds to contraction properties of heat kernels on graphs, as well as implications for manifold learning from data clouds. In particular, we show that the consistency results allow for characterizing the intrinsic curvature of a manifold from extrinsic curvature.
</details></li>
</ul>
<hr>
<h2 id="Distance-Preserving-Machine-Learning-for-Uncertainty-Aware-Accelerator-Capacitance-Predictions"><a href="#Distance-Preserving-Machine-Learning-for-Uncertainty-Aware-Accelerator-Capacitance-Predictions" class="headerlink" title="Distance Preserving Machine Learning for Uncertainty Aware Accelerator Capacitance Predictions"></a>Distance Preserving Machine Learning for Uncertainty Aware Accelerator Capacitance Predictions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02367">http://arxiv.org/abs/2307.02367</a></li>
<li>repo_url: None</li>
<li>paper_authors: Steven Goldenberg, Malachi Schram, Kishansingh Rajput, Thomas Britton, Chris Pappas, Dan Lu, Jared Walden, Majdi I. Radaideh, Sarah Cousineau, Sudarshan Harave</li>
<li>for: 这个论文的目的是提供一种可靠的机器学习模型，尤其是在安全敏感的应用中，如加速器系统。</li>
<li>methods: 这个论文使用了深度神经网络和 Gaussian process 近似技术，并比较了两种不同的特征提取器，包括单元值分解和spectral-normalized dense layer。</li>
<li>results: 这个模型可以实现较好的距离保持和内部容易预测，并且预测在正常分布中的电容量值几乎没有误差（少于1%）。<details>
<summary>Abstract</summary>
Providing accurate uncertainty estimations is essential for producing reliable machine learning models, especially in safety-critical applications such as accelerator systems. Gaussian process models are generally regarded as the gold standard method for this task, but they can struggle with large, high-dimensional datasets. Combining deep neural networks with Gaussian process approximation techniques have shown promising results, but dimensionality reduction through standard deep neural network layers is not guaranteed to maintain the distance information necessary for Gaussian process models. We build on previous work by comparing the use of the singular value decomposition against a spectral-normalized dense layer as a feature extractor for a deep neural Gaussian process approximation model and apply it to a capacitance prediction problem for the High Voltage Converter Modulators in the Oak Ridge Spallation Neutron Source. Our model shows improved distance preservation and predicts in-distribution capacitance values with less than 1% error.
</details>
<details>
<summary>摘要</summary>
提供准确的不确定性估计是机器学习模型生成可靠性的关键，特别是在安全关键应用中，如加速器系统。高斯过程模型通常被视为金标准方法，但它们可能在大量、高维度数据集上表现不佳。将深度神经网络与高斯过程approximation技术结合使用已经显示出了有希望的结果，但通过标准的深度神经网络层进行维度减少并不一定能保持高斯过程模型所需的距离信息。我们基于先前的工作，比较使用对快速特征EXTRACTOR的singular value decomposition和spectral-normalized dense layer作为深度神经网络 Gaussian process approximation模型的特征提取器，并应用于高电压转换模ulators在Oak Ridge Spallation Neutron Source中的电容量预测问题。我们的模型表现出了改善的距离保持和预测在distribution中的电容量值，误差低于1%。
</details></li>
</ul>
<hr>
<h2 id="Scaling-Laws-Do-Not-Scale"><a href="#Scaling-Laws-Do-Not-Scale" class="headerlink" title="Scaling Laws Do Not Scale"></a>Scaling Laws Do Not Scale</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03201">http://arxiv.org/abs/2307.03201</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/MarkipTheMudkip/in-class-project-2">https://github.com/MarkipTheMudkip/in-class-project-2</a></li>
<li>paper_authors: Fernando Diaz, Michael Madaio<br>for: This paper argues that the performance of large AI models may not continue to improve as datasets get larger, as different communities represented in the dataset may have values or preferences not captured by the metrics used to evaluate model performance.methods: The paper highlights the potential risks of using scaling laws to evaluate the performance of AI models, as these laws overlook the possibility that different communities may have different values or preferences.results: The paper suggests that as datasets used to train large AI models grow, the number of distinct communities included in the dataset is likely to increase, and these communities may have different values or preferences that are not captured by the metrics used to evaluate model performance.<details>
<summary>Abstract</summary>
Recent work has proposed a power law relationship, referred to as ``scaling laws,'' between the performance of artificial intelligence (AI) models and aspects of those models' design (e.g., dataset size). In other words, as the size of a dataset (or model parameters, etc) increases, the performance of a given model trained on that dataset will correspondingly increase. However, while compelling in the aggregate, this scaling law relationship overlooks the ways that metrics used to measure performance may be precarious and contested, or may not correspond with how different groups of people may perceive the quality of models' output. In this paper, we argue that as the size of datasets used to train large AI models grows, the number of distinct communities (including demographic groups) whose data is included in a given dataset is likely to grow, each of whom may have different values. As a result, there is an increased risk that communities represented in a dataset may have values or preferences not captured by (or in the worst case, at odds with) the metrics used to evaluate model performance for scaling laws. We end the paper with implications for AI scaling laws -- that models may not, in fact, continue to improve as the datasets get larger -- at least not for all people or communities impacted by those models.
</details>
<details>
<summary>摘要</summary>
近期的研究已经提出了一种力学关系，称为“扩大法律”，表明人工智能（AI）模型的性能和模型设计参数之间存在线性关系。即随着数据集大小（或模型参数等）的增加，使用该数据集训练的模型的性能会随之增加。然而，这个扩大法律关系忽视了评估模型性能的指标可能是不安定的、争议的或者不符合不同群体的评价标准。在这篇论文中，我们 argueThat as the size of datasets used to train large AI models grows, the number of distinct communities (including demographic groups) whose data is included in a given dataset is likely to grow, each of whom may have different values. As a result, there is an increased risk that communities represented in a dataset may have values or preferences not captured by (or in the worst case, at odds with) the metrics used to evaluate model performance for scaling laws. We end the paper with implications for AI scaling laws -- that models may not, in fact, continue to improve as the datasets get larger -- at least not for all people or communities impacted by those models.Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. Traditional Chinese is used in Taiwan, Hong Kong, and other countries.
</details></li>
</ul>
<hr>
<h2 id="Decentralized-Data-Governance-as-Part-of-a-Data-Mesh-Platform-Concepts-and-Approaches"><a href="#Decentralized-Data-Governance-as-Part-of-a-Data-Mesh-Platform-Concepts-and-Approaches" class="headerlink" title="Decentralized Data Governance as Part of a Data Mesh Platform: Concepts and Approaches"></a>Decentralized Data Governance as Part of a Data Mesh Platform: Concepts and Approaches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02357">http://arxiv.org/abs/2307.02357</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arif Wider, Sumedha Verma, Atif Akhtar</li>
<li>for: 本文旨在提供一个概念模型，用于解决数据网络（Data Mesh）的数据管理问题。</li>
<li>methods: 本文使用自动化技术，以提供一个自助数据基础设施平台，以便有效管理数据网络。</li>
<li>results: 本文提出了一种数据网络管理模型，并讨论了如何通过平台方式实现数据管理的自治。<details>
<summary>Abstract</summary>
Data mesh is a socio-technical approach to decentralized analytics data management. To manage this decentralization efficiently, data mesh relies on automation provided by a self-service data infrastructure platform. A key aspect of this platform is to enable decentralized data governance. Because data mesh is a young approach, there is a lack of coherence in how data mesh concepts are interpreted in the industry, and almost no work on how a data mesh platform facilitates governance. This paper presents a conceptual model of key data mesh concepts and discusses different approaches to drive governance through platform means. The insights presented are drawn from concrete experiences of implementing a fully-functional data mesh platform that can be used as a reference on how to approach data mesh platform development.
</details>
<details>
<summary>摘要</summary>
“数据网”是一种社会技术方法，用于分布式数据分析数据管理。为了有效地管理这种分布，数据网利用自助数据基础设施平台的自动化。该平台的一个关键方面是实现分布式数据管理。由于数据网是一种新的方法，因此在行业中对数据网概念的解释存在一定的不一致，而且对于平台如何通过平台手段进行管理的研究几乎缺乏。本文提出了一个概念模型，描述了不同的数据网平台管理方法，并从实际实施了一个可用的数据网平台 refer to as a reference for how to approach data mesh platform development.Here's a word-for-word translation of the text:“数据网”是一种社会技术方法，用于分布式数据分析数据管理。为了有效地管理这种分布，数据网利用自助数据基础设施平台的自动化。该平台的一个关键方面是实现分布式数据管理。由于数据网是一种新的方法，因此在行业中对数据网概念的解释存在一定的不一致，而且对于平台如何通过平台手段进行管理的研究几乎缺乏。本文提出了一个概念模型，描述了不同的数据网平台管理方法，并从实际实施了一个可用的数据网平台 refer to as a reference for how to approach data mesh platform development.
</details></li>
</ul>
<hr>
<h2 id="LLQL-Logistic-Likelihood-Q-Learning-for-Reinforcement-Learning"><a href="#LLQL-Logistic-Likelihood-Q-Learning-for-Reinforcement-Learning" class="headerlink" title="LLQL: Logistic Likelihood Q-Learning for Reinforcement Learning"></a>LLQL: Logistic Likelihood Q-Learning for Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02345">http://arxiv.org/abs/2307.02345</a></li>
<li>repo_url: None</li>
<li>paper_authors: Outongyi Lv, Bingxin Zhou, Yu Guang Wang</li>
<li>for: 这个研究的目的是分析在线和离线RL中bellman错误的分布特性。</li>
<li>methods: 这个研究使用了分布统计学方法来分析bellman错误的分布特性，并基于这些分布特性来改进MSELoss和LLoss两种损失函数。</li>
<li>results: 研究发现在线环境中bellman错误遵循Logistic分布，而离线环境中bellman错误遵循受限的Logistic分布，这个受限分布与离线数据集中的先前策略相关。这些发现导致了改进MSELoss的假设，并使用Logistic最大 LIKElihood函数来构建LLoss作为替代损失函数。此外，研究还发现在离线数据集中，奖励应该遵循特定的分布，以便实现离线目标。在实验中，研究对Soft-Actor-Critic的两种变体在线和离线环境中进行了控制变量修正。结果证实了我们在线和离线设定中的假设，并发现LLoss的方差小于MSELoss。<details>
<summary>Abstract</summary>
Currently, research on Reinforcement learning (RL) can be broadly classified into two categories: online RL and offline RL. Both in online and offline RL, the primary focus of research on the Bellman error lies in the optimization techniques and performance improvement, rather than exploring the inherent structural properties of the Bellman error, such as distribution characteristics. In this study, we analyze the distribution of the Bellman approximation error in both online and offline settings. We find that in the online environment, the Bellman error follows a Logistic distribution, while in the offline environment, the Bellman error follows a constrained Logistic distribution, where the constrained distribution is dependent on the prior policy in the offline data set. Based on this finding, we have improved the MSELoss which is based on the assumption that the Bellman errors follow a normal distribution, and we utilized the Logistic maximum likelihood function to construct $\rm LLoss$ as an alternative loss function. In addition, we observed that the rewards in the offline data set should follow a specific distribution, which would facilitate the achievement of offline objectives. In our numerical experiments, we performed controlled variable corrections on the loss functions of two variants of Soft-Actor-Critic in both online and offline environments. The results confirmed our hypothesis regarding the online and offline settings, we also found that the variance of LLoss is smaller than MSELoss. Our research provides valuable insights for further investigations based on the distribution of Bellman errors.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:现在， reinforcement learning（RL）研究可以分为两类：在线RL和离线RL。在线和离线RL中，bellman错误的主要研究方向都是优化技术和性能提高，而不是探索bellman错误的内在结构特性，如分布特性。在这项研究中，我们分析了在线和离线设置中bellman预测错误的分布。我们发现在线环境中，bellman错误遵循Logistic分布，而在离线环境中，bellman错误遵循受限Logistic分布，其受限分布取决于离线数据集中的先前策略。根据这一发现，我们改进了基于bellman错误预测的MSE损失，并利用Logistic最大likely函数来构建LLoss作为替代损失函数。此外，我们发现离线数据集中的奖励应该遵循特定的分布，以便实现离线目标。在我们的数值实验中，我们对Soft-Actor-Critic两种变体的损失函数进行了在线和离线环境中的变量修正。结果证明了我们在线和离线设置中的假设，并发现LLoss的方差比MSE损失更小。我们的研究提供了有价值的发现，用于进一步基于bellman错误分布的调查。
</details></li>
</ul>
<hr>
<h2 id="FAM-Relative-Flatness-Aware-Minimization"><a href="#FAM-Relative-Flatness-Aware-Minimization" class="headerlink" title="FAM: Relative Flatness Aware Minimization"></a>FAM: Relative Flatness Aware Minimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02337">http://arxiv.org/abs/2307.02337</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kampmichael/RelativeFlatnessAndGeneralization">https://github.com/kampmichael/RelativeFlatnessAndGeneralization</a></li>
<li>paper_authors: Linara Adilova, Amr Abourayya, Jianning Li, Amin Dada, Henning Petzka, Jan Egger, Jens Kleesiek, Michael Kamp</li>
<li>for: 提高模型的泛化能力</li>
<li>methods: 使用 relative flatness 度量，并通过一个简单的正则化项来优化</li>
<li>results: 在多种应用和模型中，FAM 可以提高模型的泛化性能，并且可以在标准训练和finetuning中使用<details>
<summary>Abstract</summary>
Flatness of the loss curve around a model at hand has been shown to empirically correlate with its generalization ability. Optimizing for flatness has been proposed as early as 1994 by Hochreiter and Schmidthuber, and was followed by more recent successful sharpness-aware optimization techniques. Their widespread adoption in practice, though, is dubious because of the lack of theoretically grounded connection between flatness and generalization, in particular in light of the reparameterization curse - certain reparameterizations of a neural network change most flatness measures but do not change generalization. Recent theoretical work suggests that a particular relative flatness measure can be connected to generalization and solves the reparameterization curse. In this paper, we derive a regularizer based on this relative flatness that is easy to compute, fast, efficient, and works with arbitrary loss functions. It requires computing the Hessian only of a single layer of the network, which makes it applicable to large neural networks, and with it avoids an expensive mapping of the loss surface in the vicinity of the model. In an extensive empirical evaluation we show that this relative flatness aware minimization (FAM) improves generalization in a multitude of applications and models, both in finetuning and standard training. We make the code available at github.
</details>
<details>
<summary>摘要</summary>
几乎所有的模型都会在某些特定的输入上 exhibit 非常好的特性，但这并不意味着它们会在整体上具有更好的泛化能力。在1994年，豪克雷特和Schmid哈姆布尔首先提出了优化flatness的想法，然后是更加成功的锐度感知优化技术。尽管它们在实践中的普及度不高，因为没有理论上的基础连接flatness和泛化，尤其是在考虑到映射渐尘的问题。最近的理论工作表明，一种特定的相对flatness度量可以与泛化相关，并且解决了映射渐尘的问题。在这篇论文中，我们 derivates一种基于这种相对flatness度量的正则化器，它容易计算、快速、高效、可以与任何损失函数结合使用。它只需计算单个层的梯度，因此可以应用于大型神经网络，而且不需要在损失函数的附近进行昂贵的映射。在广泛的实验中，我们证明了这种相对flatness感知优化（FAM）可以提高多种应用和模型的泛化能力，包括finetuning和标准训练。我们在github上提供了代码。
</details></li>
</ul>
<hr>
<h2 id="Data-driven-Predictive-Latency-for-5G-A-Theoretical-and-Experimental-Analysis-Using-Network-Measurements"><a href="#Data-driven-Predictive-Latency-for-5G-A-Theoretical-and-Experimental-Analysis-Using-Network-Measurements" class="headerlink" title="Data-driven Predictive Latency for 5G: A Theoretical and Experimental Analysis Using Network Measurements"></a>Data-driven Predictive Latency for 5G: A Theoretical and Experimental Analysis Using Network Measurements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02329">http://arxiv.org/abs/2307.02329</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marco Skocaj, Francesca Conserva, Nicol Sarcone Grande, Andrea Orsi, Davide Micheli, Giorgio Ghinamo, Simone Bizzarri, Roberto Verdone</li>
<li>for: This paper is written to analyze predictive latency within 5G networks using real-world network data.</li>
<li>methods: The paper uses an analytical formulation of user-plane latency as a Hypoexponential distribution, and conducts experimental results of probabilistic regression, anomaly detection, and predictive forecasting using Machine Learning (ML) techniques such as Bayesian Learning (BL) and Machine Learning on Graphs (GML).</li>
<li>results: The paper provides valuable insights into the efficacy of predictive algorithms in practical applications, and validates the proposed framework using data gathered from scenarios of vehicular mobility, dense-urban traffic, and social gathering events.<details>
<summary>Abstract</summary>
The advent of novel 5G services and applications with binding latency requirements and guaranteed Quality of Service (QoS) hastened the need to incorporate autonomous and proactive decision-making in network management procedures. The objective of our study is to provide a thorough analysis of predictive latency within 5G networks by utilizing real-world network data that is accessible to mobile network operators (MNOs). In particular, (i) we present an analytical formulation of the user-plane latency as a Hypoexponential distribution, which is validated by means of a comparative analysis with empirical measurements, and (ii) we conduct experimental results of probabilistic regression, anomaly detection, and predictive forecasting leveraging on emerging domains in Machine Learning (ML), such as Bayesian Learning (BL) and Machine Learning on Graphs (GML). We test our predictive framework using data gathered from scenarios of vehicular mobility, dense-urban traffic, and social gathering events. Our results provide valuable insights into the efficacy of predictive algorithms in practical applications.
</details>
<details>
<summary>摘要</summary>
五代新服务和应用程序的紧耦合延迟和质量服务（QoS）的出现加剧了网络管理过程中的自主和积极决策的需求。我们的研究目标是对5G网络中的预测延迟进行全面分析，并使用可达到移动网络运营商（MNOs）的实际网络数据进行验证。具体来说，我们：(i) 提出了用户层延迟的分布式 Hypoexponential 分布，并通过对实际测量数据进行比较分析来验证其有效性。(ii) 通过使用 emerging 领域的机器学习（ML）技术，如 bayesian 学习（BL）和机器学习在图上（GML），进行可预测性的回归、异常检测和预测预测，并对各种场景进行测试，包括交通堵塞、都市化交通和社交聚会等。我们的结果提供了实用应用中预测算法的有用信息。
</details></li>
</ul>
<hr>
<h2 id="Exploring-new-ways-Enforcing-representational-dissimilarity-to-learn-new-features-and-reduce-error-consistency"><a href="#Exploring-new-ways-Enforcing-representational-dissimilarity-to-learn-new-features-and-reduce-error-consistency" class="headerlink" title="Exploring new ways: Enforcing representational dissimilarity to learn new features and reduce error consistency"></a>Exploring new ways: Enforcing representational dissimilarity to learn new features and reduce error consistency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02516">http://arxiv.org/abs/2307.02516</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tassilo Wald, Constantin Ulrich, Fabian Isensee, David Zimmerer, Gregor Koehler, Michael Baumgartner, Klaus H. Maier-Hein</li>
<li>for: 提高模型 ensemble 的准确率</li>
<li>methods: 利用 representational similarity field 方法来促进模型之间的不同性 durante 训练</li>
<li>results: 实现了更高的 ensemble 准确率，并且Output predictions 之间更加不相关，从而降低了模型 ensemble 的共同失败模式<details>
<summary>Abstract</summary>
Independently trained machine learning models tend to learn similar features. Given an ensemble of independently trained models, this results in correlated predictions and common failure modes. Previous attempts focusing on decorrelation of output predictions or logits yielded mixed results, particularly due to their reduction in model accuracy caused by conflicting optimization objectives. In this paper, we propose the novel idea of utilizing methods of the representational similarity field to promote dissimilarity during training instead of measuring similarity of trained models. To this end, we promote intermediate representations to be dissimilar at different depths between architectures, with the goal of learning robust ensembles with disjoint failure modes. We show that highly dissimilar intermediate representations result in less correlated output predictions and slightly lower error consistency, resulting in higher ensemble accuracy. With this, we shine first light on the connection between intermediate representations and their impact on the output predictions.
</details>
<details>
<summary>摘要</summary>
设置语言为简化中文。<</SYS>>独立训练的机器学习模型通常会学习类似的特征。给出一个独立训练的模型集合，这会导致相互相关的预测和共同失败模式。过去关注输出预测或搜索阶段的减 corr 的尝试已经获得了杂合的结果，特别是因为它们在优化目标之间发生了矛盾。在这篇论文中，我们提出了一种新的想法，利用表示相似场来促进训练期间的不同性。为此，我们将不同深度的建筑物 promoted 到不同的中间表示，以达到学习强大的集成模型，并且学习不同的失败模式。我们发现，在不同的中间表示下，输出预测之间的相互关系较强，并且 ensemble 精度较高。我们首次探讨了中间表示如何影响输出预测的问题。
</details></li>
</ul>
<hr>
<h2 id="LOB-Based-Deep-Learning-Models-for-Stock-Price-Trend-Prediction-A-Benchmark-Study"><a href="#LOB-Based-Deep-Learning-Models-for-Stock-Price-Trend-Prediction-A-Benchmark-Study" class="headerlink" title="LOB-Based Deep Learning Models for Stock Price Trend Prediction: A Benchmark Study"></a>LOB-Based Deep Learning Models for Stock Price Trend Prediction: A Benchmark Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01915">http://arxiv.org/abs/2308.01915</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matteo Prata, Giuseppe Masi, Leonardo Berti, Viviana Arrigoni, Andrea Coletta, Irene Cannistraci, Svitlana Vyetrenko, Paola Velardi, Novella Bartolini</li>
<li>for: 股票价格趋势预测（SPTP）基于限价书（LOB）数据的深度学习模型的可靠性和泛化性研究。</li>
<li>methods: 我们开发了一个名为LOBCAST的开源框架，它包括数据准备、深度学习模型训练、评估和收益分析。</li>
<li>results: 我们的广泛实验发现所有模型在新数据上表现糟糕，这引发了市场应用性的问题。我们的工作作为一个标准，暴露了当前方法的潜在和局限性，并提供了创新解决方案的视野。<details>
<summary>Abstract</summary>
The recent advancements in Deep Learning (DL) research have notably influenced the finance sector. We examine the robustness and generalizability of fifteen state-of-the-art DL models focusing on Stock Price Trend Prediction (SPTP) based on Limit Order Book (LOB) data. To carry out this study, we developed LOBCAST, an open-source framework that incorporates data preprocessing, DL model training, evaluation and profit analysis. Our extensive experiments reveal that all models exhibit a significant performance drop when exposed to new data, thereby raising questions about their real-world market applicability. Our work serves as a benchmark, illuminating the potential and the limitations of current approaches and providing insight for innovative solutions.
</details>
<details>
<summary>摘要</summary>
现代深度学习（DL）研究的进步已经很大程度上影响了金融领域。我们对十五种当前最佳DL模型进行了评估，以便对使用Limit Order Book（LOB）数据进行股票价格趋势预测（SPTP）。为了实现这项研究，我们开发了一个名为LOBCAST的开源框架，该框架包括数据处理、DL模型训练、评估和利润分析。我们的广泛实验表明，所有模型在新数据上表现出了显著的性能下降，这引发了market应用实际性的问题。我们的工作作为一个 referential，探讨了当前approach的潜力和局限性，并提供了创新解决方案的思路。
</details></li>
</ul>
<hr>
<h2 id="Deep-Contract-Design-via-Discontinuous-Piecewise-Affine-Neural-Networks"><a href="#Deep-Contract-Design-via-Discontinuous-Piecewise-Affine-Neural-Networks" class="headerlink" title="Deep Contract Design via Discontinuous Piecewise Affine Neural Networks"></a>Deep Contract Design via Discontinuous Piecewise Affine Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02318">http://arxiv.org/abs/2307.02318</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tonghan Wang, Paul Dütting, Dmitry Ivanov, Inbal Talgam-Cohen, David C. Parkes</li>
<li>for: 这篇论文是为了研究深度学习在合同设计中的应用。</li>
<li>methods: 这篇论文使用了深度学习来自动设计优化的合同。它使用了一种新的表示方法——Discontinuous ReLU（DeLU）网络，来模型主体的利益函数。</li>
<li>results: 实验结果表明，使用DeLU网络可以 успеш地近似主体的利益函数，并且可以使用少量的训练样本和内部点法来解决优化问题。<details>
<summary>Abstract</summary>
Contract design involves a principal who establishes contractual agreements about payments for outcomes that arise from the actions of an agent. In this paper, we initiate the study of deep learning for the automated design of optimal contracts. We formulate this as an offline learning problem, where a deep network is used to represent the principal's expected utility as a function of the design of a contract. We introduce a novel representation: the Discontinuous ReLU (DeLU) network, which models the principal's utility as a discontinuous piecewise affine function where each piece corresponds to the agent taking a particular action. DeLU networks implicitly learn closed-form expressions for the incentive compatibility constraints of the agent and the utility maximization objective of the principal, and support parallel inference on each piece through linear programming or interior-point methods that solve for optimal contracts. We provide empirical results that demonstrate success in approximating the principal's utility function with a small number of training samples and scaling to find approximately optimal contracts on problems with a large number of actions and outcomes.
</details>
<details>
<summary>摘要</summary>
（本文研究了深度学习在合同设计中的应用，具体来说是设计优化的合同。我们将这视为一个离线学习问题，其中深度网络用于表示主人的预期用处函数，并且引入了一种新的表示方法：离散ReLU（DeLU）网络。DeLU网络模型了主人的用处函数为离散的piecewise afine函数，每个piece对应代理人行动的不同。DeLU网络隐式学习了代理人的奖励一致约束和主人的用处最大化目标，并且支持并行推理每个piece的linear programming或内部点法解决优化合同。我们提供了实验结果，证明了使用少量训练样本和扩展可以successfully approximate主人的用处函数，并且可以扩展到解决包含大量行动和结果的问题）。
</details></li>
</ul>
<hr>
<h2 id="Sumformer-Universal-Approximation-for-Efficient-Transformers"><a href="#Sumformer-Universal-Approximation-for-Efficient-Transformers" class="headerlink" title="Sumformer: Universal Approximation for Efficient Transformers"></a>Sumformer: Universal Approximation for Efficient Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02301">http://arxiv.org/abs/2307.02301</a></li>
<li>repo_url: None</li>
<li>paper_authors: Silas Alberti, Niclas Dern, Laura Thesing, Gitta Kutyniok</li>
<li>for: 这篇论文主要针对的是提出一种新的嵌入式序列处理模型，以解决Transformer模型的时间和空间复杂度问题。</li>
<li>methods: 该论文使用了Linformer和Performer模型，并通过一种新的概念——Sumformer来实现对这些模型的 universally approximating。</li>
<li>results: 该论文通过实验和理论分析表明，Sumformer模型可以成功地实现对Linformer和Performer模型的universal approximation，并且提供了一个新的证明，证明Transformer模型只需要一层注意力层可以实现universal approximation。<details>
<summary>Abstract</summary>
Natural language processing (NLP) made an impressive jump with the introduction of Transformers. ChatGPT is one of the most famous examples, changing the perception of the possibilities of AI even outside the research community. However, besides the impressive performance, the quadratic time and space complexity of Transformers with respect to sequence length pose significant limitations for handling long sequences. While efficient Transformer architectures like Linformer and Performer with linear complexity have emerged as promising solutions, their theoretical understanding remains limited. In this paper, we introduce Sumformer, a novel and simple architecture capable of universally approximating equivariant sequence-to-sequence functions. We use Sumformer to give the first universal approximation results for Linformer and Performer. Moreover, we derive a new proof for Transformers, showing that just one attention layer is sufficient for universal approximation.
</details>
<details>
<summary>摘要</summary>
自然语言处理（NLP）在Transformers的出现后得到了非常出色的进步。ChatGPT是其中最著名的例子，对外部研究者的认知产生了深刻的影响。然而，Transformers的时间和空间复杂度随序列长度的增加平方速度增长，对于处理长序列存在重大的限制。虽然有效的Transformers架构如Linformer和Performer已经出现，但它们的理论理解仍然受限。在这篇论文中，我们介绍了Sumformer，一种新的简单架构，可以通用地近似Equivariant sequence-to-sequence函数。我们使用Sumformer来给Linformer和Performer提供首个通用近似结果。此外，我们还 deriv了一个新的证明，显示Transformers只需一层注意力层就能universal approximation。
</details></li>
</ul>
<hr>
<h2 id="Improving-Address-Matching-using-Siamese-Transformer-Networks"><a href="#Improving-Address-Matching-using-Siamese-Transformer-Networks" class="headerlink" title="Improving Address Matching using Siamese Transformer Networks"></a>Improving Address Matching using Siamese Transformer Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02300">http://arxiv.org/abs/2307.02300</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/avduarte333/adress-matching">https://github.com/avduarte333/adress-matching</a></li>
<li>paper_authors: André V. Duarte, Arlindo L. Oliveira</li>
<li>for: 提高葡萄牙地址匹配效率，降低错误交付的风险</li>
<li>methods: 使用深度学习模型，包括bi-encoder和cross-encoder，对葡萄牙邮政地址进行嵌入表示，并进行高精度排名</li>
<li>results: 测试用例显示，模型在葡萄牙邮政地址上达到了95%以上的高精度率，并且使用GPU计算可以提高推理速度，比传统方法快4.5倍<details>
<summary>Abstract</summary>
Matching addresses is a critical task for companies and post offices involved in the processing and delivery of packages. The ramifications of incorrectly delivering a package to the wrong recipient are numerous, ranging from harm to the company's reputation to economic and environmental costs. This research introduces a deep learning-based model designed to increase the efficiency of address matching for Portuguese addresses. The model comprises two parts: (i) a bi-encoder, which is fine-tuned to create meaningful embeddings of Portuguese postal addresses, utilized to retrieve the top 10 likely matches of the un-normalized target address from a normalized database, and (ii) a cross-encoder, which is fine-tuned to accurately rerank the 10 addresses obtained by the bi-encoder. The model has been tested on a real-case scenario of Portuguese addresses and exhibits a high degree of accuracy, exceeding 95% at the door level. When utilized with GPU computations, the inference speed is about 4.5 times quicker than other traditional approaches such as BM25. An implementation of this system in a real-world scenario would substantially increase the effectiveness of the distribution process. Such an implementation is currently under investigation.
</details>
<details>
<summary>摘要</summary>
企业和邮政机构在包裹处理和交付过程中，匹配地址是一项非常重要的任务。 incorrect 地址交付可能会对公司的名誉产生负面影响，并且可能会导致经济和环境损失。这项研究推出了一种基于深度学习的地址匹配模型，用于提高葡萄牙地址的匹配效率。该模型包括两部分：1. 双编码器，通过细化葡萄牙邮政地址来生成有意义的嵌入。使用这些嵌入来从normalized数据库中检索最有可能性的10个目标地址。2. 混合编码器，通过细化10个从双编码器获取的地址来准确地排名它们。这个模型在葡萄牙地址的实际案例中进行测试，表现出了高度的准确率，超过95%的门槛。在使用GPU计算时，推理速度比传统方法 such as BM25 快得多，约为4.5倍。在实际应用场景中，这种系统的实施将有效地提高物流过程的效率。现在正在进行实际应用研究。
</details></li>
</ul>
<hr>
<h2 id="Meta-Learning-Adversarial-Bandit-Algorithms"><a href="#Meta-Learning-Adversarial-Bandit-Algorithms" class="headerlink" title="Meta-Learning Adversarial Bandit Algorithms"></a>Meta-Learning Adversarial Bandit Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02295">http://arxiv.org/abs/2307.02295</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mikhail Khodak, Ilya Osadchiy, Keegan Harris, Maria-Florina Balcan, Kfir Y. Levy, Ron Meir, Zhiwei Steven Wu</li>
<li>for: 本研究旨在提高多个任务的性能，假设这些任务之间存在自然的相似度测试。</li>
<li>methods: 我们设计了meta算法，用于同时调整内部学习器的初始化和其他超参数。对于多抢拍机（MAB）和bandit线性优化（BLO）两个重要情况，我们设计了外部学习器来同时调整初始化和超参数。</li>
<li>results: 我们证明了在不Regularization的follow-the-leader combinated with two levels of low-dimensional hyperparameter tuning是可以学习一个序列的Affine函数， bounding the regret of online mirror descent（OMD）。<details>
<summary>Abstract</summary>
We study online meta-learning with bandit feedback, with the goal of improving performance across multiple tasks if they are similar according to some natural similarity measure. As the first to target the adversarial online-within-online partial-information setting, we design meta-algorithms that combine outer learners to simultaneously tune the initialization and other hyperparameters of an inner learner for two important cases: multi-armed bandits (MAB) and bandit linear optimization (BLO). For MAB, the meta-learners initialize and set hyperparameters of the Tsallis-entropy generalization of Exp3, with the task-averaged regret improving if the entropy of the optima-in-hindsight is small. For BLO, we learn to initialize and tune online mirror descent (OMD) with self-concordant barrier regularizers, showing that task-averaged regret varies directly with an action space-dependent measure they induce. Our guarantees rely on proving that unregularized follow-the-leader combined with two levels of low-dimensional hyperparameter tuning is enough to learn a sequence of affine functions of non-Lipschitz and sometimes non-convex Bregman divergences bounding the regret of OMD.
</details>
<details>
<summary>摘要</summary>
我们研究在线元学习，使得在多个任务之间提高性能，如果这些任务具有自然的相似度度量。作为首先针对在线内部部分信息设定下进行反恶力学学习的研究，我们设计了元学习算法，将外层学习器与内层学习器的初始化和其他超参数同时调整。对于多枪炮（MAB）和反馈函数优化（BLO）两个重要的案例，我们的元学习算法都可以达到良好的性能。对于 MAB，我们使用 Tsallis-Entropy 泛化 Exp3，并证明在任务平均误差小时，元学习器可以提高性能。对于 BLO，我们学习在线镜像下降（OMD）的初始化和调整，并证明任务平均误差与行动空间依赖的度量直接相关。我们的保证基于证明，不带权重的跟随者，加上两级低维度超参数调整，可以学习一个 sequence of affine function， bounding OMD 的误差。
</details></li>
</ul>
<hr>
<h2 id="Absorbing-Phase-Transitions-in-Artificial-Deep-Neural-Networks"><a href="#Absorbing-Phase-Transitions-in-Artificial-Deep-Neural-Networks" class="headerlink" title="Absorbing Phase Transitions in Artificial Deep Neural Networks"></a>Absorbing Phase Transitions in Artificial Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02284">http://arxiv.org/abs/2307.02284</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keiichi Tamai, Tsuyoshi Okubo, Truong Vinh Truong Duy, Naotake Natori, Synge Todo</li>
<li>for: 了解抽象神经网络的行为</li>
<li>methods: 使用 mean-field theory 和 absorbing phase transitions 的研究方法</li>
<li>results: 显示了抽象神经网络的行为可以通过critical phenomena来理解，并且 Architecture 的不同会影响transition的类型。In simpler Chinese:</li>
<li>for: 了解神经网络的行为</li>
<li>methods: 使用 mean-field theory 和 absorbing phase transitions 的方法</li>
<li>results: 发现神经网络的行为可以通过critical phenomena来理解，而且不同的 Architecture 会影响transition的类型。<details>
<summary>Abstract</summary>
Theoretical understanding of the behavior of infinitely-wide neural networks has been rapidly developed for various architectures due to the celebrated mean-field theory. However, there is a lack of a clear, intuitive framework for extending our understanding to finite networks that are of more practical and realistic importance. In the present contribution, we demonstrate that the behavior of properly initialized neural networks can be understood in terms of universal critical phenomena in absorbing phase transitions. More specifically, we study the order-to-chaos transition in the fully-connected feedforward neural networks and the convolutional ones to show that (i) there is a well-defined transition from the ordered state to the chaotics state even for the finite networks, and (ii) difference in architecture is reflected in that of the universality class of the transition. Remarkably, the finite-size scaling can also be successfully applied, indicating that intuitive phenomenological argument could lead us to semi-quantitative description of the signal propagation dynamics.
</details>
<details>
<summary>摘要</summary>
theoretical understanding of the behavior of infinitely-wide neural networks has been rapidly developed for various architectures due to the celebrated mean-field theory. However, there is a lack of a clear, intuitive framework for extending our understanding to finite networks that are of more practical and realistic importance. In the present contribution, we demonstrate that the behavior of properly initialized neural networks can be understood in terms of universal critical phenomena in absorbing phase transitions. More specifically, we study the order-to-chaos transition in the fully-connected feedforward neural networks and the convolutional ones to show that (i) there is a well-defined transition from the ordered state to the chaotics state even for the finite networks, and (ii) difference in architecture is reflected in that of the universality class of the transition. Remarkably, the finite-size scaling can also be successfully applied, indicating that intuitive phenomenological argument could lead us to semi-quantitative description of the signal propagation dynamics.Here's the translation breakdown:*  theoretically understanding (理论理解) = 理论理解 (Simplified Chinese)*  has been rapidly developed (has been rapidly developed) = 快速发展 (Simplified Chinese)*  for various architectures (for various architectures) =  для多种架构 (Simplified Chinese)*  due to the celebrated mean-field theory (due to the celebrated mean-field theory) = 因为著名的平均场理论 (Simplified Chinese)*  However, there is a lack of (However, there is a lack of) = 然而，缺乏 (Simplified Chinese)*  a clear, intuitive framework (a clear, intuitive framework) = 一个明确、直观的框架 (Simplified Chinese)*  for extending our understanding (for extending our understanding) = 以便扩展我们的理解 (Simplified Chinese)*  to finite networks (to finite networks) = 到有限网络 (Simplified Chinese)*  that are of more practical and realistic importance (that are of more practical and realistic importance) = 更加实用和现实重要的网络 (Simplified Chinese)*  In the present contribution (In the present contribution) = 在当前的贡献中 (Simplified Chinese)*  we demonstrate (we demonstrate) = 我们展示 (Simplified Chinese)*  that the behavior (that the behavior) = 网络的行为 (Simplified Chinese)*  of properly initialized neural networks (of properly initialized neural networks) = 初始化过的神经网络的行为 (Simplified Chinese)*  can be understood (can be understood) = 可以理解 (Simplified Chinese)*  in terms of universal critical phenomena (in terms of universal critical phenomena) = 以 Kritical 现象的形式来理解 (Simplified Chinese)*  in absorbing phase transitions (in absorbing phase transitions) = 在吸收相对 equilibria 阶段中 (Simplified Chinese)*  More specifically (More specifically) = 更加细致 (Simplified Chinese)*  we study (we study) = 我们研究 (Simplified Chinese)*  the order-to-chaos transition (the order-to-chaos transition) = 从有序到无序的转变 (Simplified Chinese)*  in the fully-connected feedforward neural networks (in the fully-connected feedforward neural networks) = 在完全连接的前向神经网络中 (Simplified Chinese)*  and the convolutional ones (and the convolutional ones) = 以及卷积神经网络 (Simplified Chinese)*  to show (to show) = 以示 (Simplified Chinese)*  that (i) there is a well-defined transition (that (i) there is a well-defined transition) = 显示 (i) 存在一个明确的转变 (Simplified Chinese)*  from the ordered state (from the ordered state) = 从有序状态 (Simplified Chinese)*  to the chaotics state (to the chaotics state) = 到无序状态 (Simplified Chinese)*  even for the finite networks (even for the finite networks) = 甚至 для有限网络 (Simplified Chinese)*  and (ii) difference in architecture (and (ii) difference in architecture) = 以及 (ii) 不同的架构 (Simplified Chinese)*  is reflected in that of the universality class (is reflected in that of the universality class) = 在不同的架构中反映出的 универсалитет 类型 (Simplified Chinese)*  Remarkably (Remarkably) = 很有趣 (Simplified Chinese)*  the finite-size scaling (the finite-size scaling) = 有限大小缩放 (Simplified Chinese)*  can also be successfully applied (can also be successfully applied) = 也可以成功应用 (Simplified Chinese)*  indicating (indicating) = 表明 (Simplified Chinese)*  that (that) = 的 (Simplified Chinese)*  intuitive phenomenological argument (intuitive phenomenological argument) = 直观的现象学 argue (Simplified Chinese)*  could lead us to (could lead us to) = 可以导我们到 (Simplified Chinese)*  semi-quantitative description (semi-quantitative description) = 半量化的描述 (Simplified Chinese)*  of the signal propagation dynamics (of the signal propagation dynamics) = 信号传播动态的描述 (Simplified Chinese)
</details></li>
</ul>
<hr>
<h2 id="From-NeurODEs-to-AutoencODEs-a-mean-field-control-framework-for-width-varying-Neural-Networks"><a href="#From-NeurODEs-to-AutoencODEs-a-mean-field-control-framework-for-width-varying-Neural-Networks" class="headerlink" title="From NeurODEs to AutoencODEs: a mean-field control framework for width-varying Neural Networks"></a>From NeurODEs to AutoencODEs: a mean-field control framework for width-varying Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02279">http://arxiv.org/abs/2307.02279</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cristina Cipriani, Massimo Fornasier, Alessandro Scagliotti</li>
<li>for: This paper aims to extend the mean-field control framework for continuous-time Autoencoders (AutoencODEs) to handle low Tikhonov regularization and potentially non-convex cost landscapes.</li>
<li>methods: The paper proposes a modification of the controlled field in the AutoencODE to enable the extension of the mean-field control framework, and develops a training method tailored to this specific type of Autoencoders with residual connections.</li>
<li>results: The paper shows that many of the global results obtained for high Tikhonov regularization can be recovered in regions where the loss function is locally convex, and validates the approach through numerical experiments conducted on various examples.<details>
<summary>Abstract</summary>
The connection between Residual Neural Networks (ResNets) and continuous-time control systems (known as NeurODEs) has led to a mathematical analysis of neural networks which has provided interesting results of both theoretical and practical significance. However, by construction, NeurODEs have been limited to describing constant-width layers, making them unsuitable for modeling deep learning architectures with layers of variable width. In this paper, we propose a continuous-time Autoencoder, which we call AutoencODE, based on a modification of the controlled field that drives the dynamics. This adaptation enables the extension of the mean-field control framework originally devised for conventional NeurODEs. In this setting, we tackle the case of low Tikhonov regularization, resulting in potentially non-convex cost landscapes. While the global results obtained for high Tikhonov regularization may not hold globally, we show that many of them can be recovered in regions where the loss function is locally convex. Inspired by our theoretical findings, we develop a training method tailored to this specific type of Autoencoders with residual connections, and we validate our approach through numerical experiments conducted on various examples.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将神经网络（ResNets）与连续时间控制系统（NeurODEs）的连接，导致了神经网络的数学分析，并且提供了有趣的理论和实践 significado。然而，由于NeurODEs的构造，它们只适用于描述常数宽的层，因此不适用于模型深度学习架构中的层。在这篇论文中，我们提出了一种基于修改控制场的连续时间自编码器，我们称之为AutoencODE。这种修改允许我们将mean-field控制框架原来设计 для传统的NeurODEs应用到这种新的Autoencoder中。在这个设定下，我们研究了低Tikhonov正则化的情况，导致的可能是非对称的成本地图。虽然高Tikhonov正则化的全局结果可能不会全球适用，但我们表明了在成本地图的局部几何上，许多全局结果可以被恢复。受到我们的理论发现的启发，我们开发了针对这种特殊类型的Autoencoders的训练方法，并通过对各种示例进行数学实验验证了我们的方法。
</details></li>
</ul>
<hr>
<h2 id="First-Explore-then-Exploit-Meta-Learning-Intelligent-Exploration"><a href="#First-Explore-then-Exploit-Meta-Learning-Intelligent-Exploration" class="headerlink" title="First-Explore, then Exploit: Meta-Learning Intelligent Exploration"></a>First-Explore, then Exploit: Meta-Learning Intelligent Exploration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02276">http://arxiv.org/abs/2307.02276</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/btnorman/First-Explore">https://github.com/btnorman/First-Explore</a></li>
<li>paper_authors: Ben Norman, Jeff Clune</li>
<li>For: The paper aims to address the issue of intelligent exploration in reinforcement learning (RL) agents, which have been limited by the conflict between exploration and exploitation.* Methods: The proposed First-Explore framework consists of two policies: one for exploration and one for exploitation. The explore policy learns to explore the environment, while the exploit policy learns to exploit the learned knowledge.* Results: The paper demonstrates that First-Explore can learn intelligent exploration strategies such as exhaustive search and outperforms dominant standard RL and meta-RL approaches on domains where exploration requires sacrificing reward.<details>
<summary>Abstract</summary>
Standard reinforcement learning (RL) agents never intelligently explore like a human (i.e. by taking into account complex domain priors and previous explorations). Even the most basic intelligent exploration strategies such as exhaustive search are only inefficiently or poorly approximated by approaches such as novelty search or intrinsic motivation, let alone more complicated strategies like learning new skills, climbing stairs, opening doors, or conducting experiments. This lack of intelligent exploration limits sample efficiency and prevents solving hard exploration domains. We argue a core barrier prohibiting many RL approaches from learning intelligent exploration is that the methods attempt to explore and exploit simultaneously, which harms both exploration and exploitation as the goals often conflict. We propose a novel meta-RL framework (First-Explore) with two policies: one policy learns to only explore and one policy learns to only exploit. Once trained, we can then explore with the explore policy, for as long as desired, and then exploit based on all the information gained during exploration. This approach avoids the conflict of trying to do both exploration and exploitation at once. We demonstrate that First-Explore can learn intelligent exploration strategies such as exhaustive search and more, and that it outperforms dominant standard RL and meta-RL approaches on domains where exploration requires sacrificing reward. First-Explore is a significant step towards creating meta-RL algorithms capable of learning human-level exploration which is essential to solve challenging unseen hard-exploration domains.
</details>
<details>
<summary>摘要</summary>
标准强化学习（RL）代理没有人类智能的探索能力（即考虑复杂领域假设和前一次探索）。 même 最基本的智能探索策略，如探索所有可能性，都是通过方法如新鲜度搜索或内在动机来不够或不准确地 aproximated。这种缺乏智能探索限制了样本效率，阻碍解决困难探索领域。我们认为许多RL方法无法学习智能探索的核心障碍在于这些方法尝试同时探索和利用，这两个目标经常矛盾。我们提出了一种新的元RL框架（First-Explore），其中有两个策略：一个策略学习只探索，另一个策略学习只利用。一旦训练完成，我们可以使用探索策略，探索到心仪的时间，然后根据所获得的信息进行利用。这种方法避免了同时尝试探索和利用的矛盾。我们证明First-Explore可以学习智能探索策略，例如探索所有可能性，并且超过了主流标准RL和元RL方法在需要牺牲奖励的领域中的表现。First-Explore是创造meta-RL算法可以学习人类水平的探索的重要一步，解决了许多未解之探索领域。
</details></li>
</ul>
<hr>
<h2 id="Convolutions-Through-the-Lens-of-Tensor-Networks"><a href="#Convolutions-Through-the-Lens-of-Tensor-Networks" class="headerlink" title="Convolutions Through the Lens of Tensor Networks"></a>Convolutions Through the Lens of Tensor Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02275">http://arxiv.org/abs/2307.02275</a></li>
<li>repo_url: None</li>
<li>paper_authors: Felix Dangel</li>
<li>for: 这篇论文旨在探讨卷积神经网络（TN）如何用于理解卷积层。</li>
<li>methods: 该论文使用了tensor网络（TN）来理解卷积层，并通过绘制图像和执行函数转换、子tensor访问和融合来表示卷积层的含义。</li>
<li>results: 该论文通过对各种自动梯度计算和各种精度估计的图像表示，证明了TN的表达力。此外，该论文还提供了基于连接性模式的卷积特性转换，以便简化和加速图像计算。最后，该论文表明了使用TN实现的计算性能。<details>
<summary>Abstract</summary>
Despite their simple intuition, convolutions are more tedious to analyze than dense layers, which complicates the generalization of theoretical and algorithmic ideas. We provide a new perspective onto convolutions through tensor networks (TNs) which allow reasoning about the underlying tensor multiplications by drawing diagrams, and manipulating them to perform function transformations, sub-tensor access, and fusion. We demonstrate this expressive power by deriving the diagrams of various autodiff operations and popular approximations of second-order information with full hyper-parameter support, batching, channel groups, and generalization to arbitrary convolution dimensions. Further, we provide convolution-specific transformations based on the connectivity pattern which allow to re-wire and simplify diagrams before evaluation. Finally, we probe computational performance, relying on established machinery for efficient TN contraction. Our TN implementation speeds up a recently-proposed KFAC variant up to 4.5x and enables new hardware-efficient tensor dropout for approximate backpropagation.
</details>
<details>
<summary>摘要</summary>
尽管卷积有简单的直觉，但它们在理论和算法上的总结和推广是更为复杂的。我们通过tensor网络（TN）提供了一新的视角来理解卷积，这些网络允许我们通过绘制图表和修改它们来实现函数转换、子tensor访问、混合等操作。我们通过示例逻辑来证明TN的表达力，包括自动梯度下降操作和流行的第二个信息近似算法，并支持批处理、通道组、普通的卷积维度等参数。此外，我们还提供了基于卷积连接 patrern的转换，可以重新排列和简化图表之前进行评估。最后，我们评估了TN实现的计算性能，利用现有的高效TN收缩机制。我们的TN实现可以加速一种最近提出的KFAC变种，并启用了新的硬件高效的tensor dropout来实现精确的反propagation。
</details></li>
</ul>
<hr>
<h2 id="Dynamical-Isometry-based-Rigorous-Fair-Neural-Architecture-Search"><a href="#Dynamical-Isometry-based-Rigorous-Fair-Neural-Architecture-Search" class="headerlink" title="Dynamical Isometry based Rigorous Fair Neural Architecture Search"></a>Dynamical Isometry based Rigorous Fair Neural Architecture Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02263">http://arxiv.org/abs/2307.02263</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianxiang Luo, Junyi Hu, Tianji Pang, Weihao Huang, Chuang Liu</li>
<li>for: 提高神经网络搜索的效率和可解释性，以及 garantizar la justicia en la evaluación de módulos。</li>
<li>methods: 基于动态同异ometry的新型神经网络搜索算法，使用fix point analysis方法对平均场观测随机神经网络的动态行为进行分析，并证明模块选择策略是正见的。</li>
<li>results: 通过对ImageNet分类任务进行广泛的实验，显示了使用提议方法可以在同等大小的神经网络中达到顶尖的top-1验证精度，并且demonstrated that our method can achieve better and more stable training performance without loss of generality。<details>
<summary>Abstract</summary>
Recently, the weight-sharing technique has significantly speeded up the training and evaluation procedure of neural architecture search. However, most existing weight-sharing strategies are solely based on experience or observation, which makes the searching results lack interpretability and rationality. In addition, due to the negligence of fairness, current methods are prone to make misjudgments in module evaluation. To address these problems, we propose a novel neural architecture search algorithm based on dynamical isometry. We use the fix point analysis method in the mean field theory to analyze the dynamics behavior in the steady state random neural network, and how dynamic isometry guarantees the fairness of weight-sharing based NAS. Meanwhile, we prove that our module selection strategy is rigorous fair by estimating the generalization error of all modules with well-conditioned Jacobian. Extensive experiments show that, with the same size, the architecture searched by the proposed method can achieve state-of-the-art top-1 validation accuracy on ImageNet classification. In addition, we demonstrate that our method is able to achieve better and more stable training performance without loss of generality.
</details>
<details>
<summary>摘要</summary>
最近，Weight-sharing技术在神经网络搜索中提高了训练和评估过程的速度。然而，大多数现有的Weight-sharing策略都是基于经验或观察，lack of interpretability和理性性。此外，由于对公平性的忽视，当前的方法容易做出不准确的模块评估。为解决这些问题，我们提出了一种基于动态同尺的神经网络搜索算法。我们使用了 fixes point analysis方法来分析动态同尺在平均场 teor 中的动态行为，并证明了动态同尺的权重分享可以保证公平性。此外，我们证明了我们的模块选择策略是正则公平的，可以通过Jacobian的condition number来估算模块的总体适应性。实验表明，我们的方法可以在ImageNet分类任务中 achievestate-of-the-art的顶部一 validate accuracy，并且可以在不失一般性的前提下提高训练性能。
</details></li>
</ul>
<hr>
<h2 id="Multivariate-Time-Series-Classification-A-Deep-Learning-Approach"><a href="#Multivariate-Time-Series-Classification-A-Deep-Learning-Approach" class="headerlink" title="Multivariate Time Series Classification: A Deep Learning Approach"></a>Multivariate Time Series Classification: A Deep Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02253">http://arxiv.org/abs/2307.02253</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/radrumond/timehetnet">https://github.com/radrumond/timehetnet</a></li>
<li>paper_authors: Mohamed Abouelnaga, Julien Vitay, Aida Farahani</li>
<li>for: 本研究探讨了不同方法和神经网络架构在时间序列分类领域中的可行性。</li>
<li>methods: 本研究使用了 Fully Convolutional Networks (FCN) 和 Long Short-Term Memory (LSTM)  дляsupervised learning，以及 Recurrent Autoencoders  дляsemisupervised learning。</li>
<li>results: 通过分析时间序列数据，研究发现不同参数的影响，并通过精度和准确率等指标评估不同方法的 diferencias，以确定适合这种问题的方法。<details>
<summary>Abstract</summary>
This paper investigates different methods and various neural network architectures applicable in the time series classification domain. The data is obtained from a fleet of gas sensors that measure and track quantities such as oxygen and sound. With the help of this data, we can detect events such as occupancy in a specific environment. At first, we analyze the time series data to understand the effect of different parameters, such as the sequence length, when training our models. These models employ Fully Convolutional Networks (FCN) and Long Short-Term Memory (LSTM) for supervised learning and Recurrent Autoencoders for semisupervised learning. Throughout this study, we spot the differences between these methods based on metrics such as precision and recall identifying which technique best suits this problem.
</details>
<details>
<summary>摘要</summary>
First, we analyze the time series data to understand the impact of different parameters, such as sequence length, when training our models. Our models use Fully Convolutional Networks (FCN) and Long Short-Term Memory (LSTM) for supervised learning, and Recurrent Autoencoders for semisupervised learning.Throughout the study, we compare the performance of these methods based on metrics such as precision and recall, and identify which technique is best suited for this problem.
</details></li>
</ul>
<hr>
<h2 id="RanPAC-Random-Projections-and-Pre-trained-Models-for-Continual-Learning"><a href="#RanPAC-Random-Projections-and-Pre-trained-Models-for-Continual-Learning" class="headerlink" title="RanPAC: Random Projections and Pre-trained Models for Continual Learning"></a>RanPAC: Random Projections and Pre-trained Models for Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02251">http://arxiv.org/abs/2307.02251</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mark D. McDonnell, Dong Gong, Amin Parveneh, Ehsan Abbasnejad, Anton van den Hengel</li>
<li>for: 这个研究是为了解决在非站点数据流中进行逐步学习（Continual Learning，CL）时，不要忘记之前学习的知识。</li>
<li>methods: 这个研究使用了预训模型（pre-trained models），并将其应用到不同的下游需求。它们可以直接使用预训模型的特征（pre-extracted features），或者使用适材化器（adaptors）。但是，这些方法可能会导致忘记现象。因此，这个研究提出了一个简洁有效的方法，通过增加特征之间的互动，提高分类器的线性分类能力，并避免忘记现象。</li>
<li>results: 这个研究发现，透过将固定的Random Projector层加入预训模型的特征表现和出力头，可以增加特征之间的互动，提高分类器的线性分类能力，并避免忘记现象。此外，调整分类器的标本集也可以帮助避免分布差异导致的忘记现象。这些技术在七个类别增量学习 benchmark 测试中，与过去的方法相比，可以大幅降低最终的错误率，并且不需要使用任何复习内存。<details>
<summary>Abstract</summary>
Continual learning (CL) aims to incrementally learn different tasks (such as classification) in a non-stationary data stream without forgetting old ones. Most CL works focus on tackling catastrophic forgetting under a learning-from-scratch paradigm. However, with the increasing prominence of foundation models, pre-trained models equipped with informative representations have become available for various downstream requirements. Several CL methods based on pre-trained models have been explored, either utilizing pre-extracted features directly (which makes bridging distribution gaps challenging) or incorporating adaptors (which may be subject to forgetting). In this paper, we propose a concise and effective approach for CL with pre-trained models. Given that forgetting occurs during parameter updating, we contemplate an alternative approach that exploits training-free random projectors and class-prototype accumulation, which thus bypasses the issue. Specifically, we inject a frozen Random Projection layer with nonlinear activation between the pre-trained model's feature representations and output head, which captures interactions between features with expanded dimensionality, providing enhanced linear separability for class-prototype-based CL. We also demonstrate the importance of decorrelating the class-prototypes to reduce the distribution disparity when using pre-trained representations. These techniques prove to be effective and circumvent the problem of forgetting for both class- and domain-incremental continual learning. Compared to previous methods applied to pre-trained ViT-B/16 models, we reduce final error rates by between 10\% and 62\% on seven class-incremental benchmark datasets, despite not using any rehearsal memory. We conclude that the full potential of pre-trained models for simple, effective, and fast continual learning has not hitherto been fully tapped.
</details>
<details>
<summary>摘要</summary>
In this paper, we propose a concise and effective approach for CL with pre-trained models. Given that forgetting occurs during parameter updating, we contemplate an alternative approach that exploits training-free random projectors and class-prototype accumulation, which thus bypasses the issue. Specifically, we inject a frozen Random Projection layer with nonlinear activation between the pre-trained model's feature representations and output head, which captures interactions between features with expanded dimensionality, providing enhanced linear separability for class-prototype-based CL. We also demonstrate the importance of decorrelating the class-prototypes to reduce the distribution disparity when using pre-trained representations. These techniques prove to be effective and circumvent the problem of forgetting for both class- and domain-incremental continual learning.Compared to previous methods applied to pre-trained ViT-B/16 models, we reduce final error rates by between 10\% and 62\% on seven class-incremental benchmark datasets, despite not using any rehearsal memory. We conclude that the full potential of pre-trained models for simple, effective, and fast continual learning has not hitherto been fully tapped.
</details></li>
</ul>
<hr>
<h2 id="Set-Learning-for-Accurate-and-Calibrated-Models"><a href="#Set-Learning-for-Accurate-and-Calibrated-Models" class="headerlink" title="Set Learning for Accurate and Calibrated Models"></a>Set Learning for Accurate and Calibrated Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02245">http://arxiv.org/abs/2307.02245</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lukasmut/oko">https://github.com/lukasmut/oko</a></li>
<li>paper_authors: Lukas Muttenthaler, Robert A. Vandermeulen, Qiuyi Zhang, Thomas Unterthiner, Klaus-Robert Müller</li>
<li>for: 降低机器学习模型的自信和准确性问题，提高模型的准确率和准确性，特别在有限的训练数据和类别偏挤的情况下。</li>
<li>methods: 提出了一种新的odd-$k$-out learning（OKO）方法，通过将cross-entropy error最小化为集合而不是单个示例，使模型能够捕捉数据示例之间的相关性，并提高准确率和准确性。</li>
<li>results: OKO方法可以在有限的训练数据和类别偏挤的情况下提高模型的准确率和准确性，并且可以不需要额外的调整参数，如温度Scaling。我们提供了理论支持和广泛的实验分析，证明OKO方法的有效性。<details>
<summary>Abstract</summary>
Model overconfidence and poor calibration are common in machine learning and difficult to account for when applying standard empirical risk minimization. In this work, we propose a novel method to alleviate these problems that we call odd-$k$-out learning (OKO), which minimizes the cross-entropy error for sets rather than for single examples. This naturally allows the model to capture correlations across data examples and achieves both better accuracy and calibration, especially in limited training data and class-imbalanced regimes. Perhaps surprisingly, OKO often yields better calibration even when training with hard labels and dropping any additional calibration parameter tuning, such as temperature scaling. We provide theoretical justification, establishing that OKO naturally yields better calibration, and provide extensive experimental analyses that corroborate our theoretical findings. We emphasize that OKO is a general framework that can be easily adapted to many settings and the trained model can be applied to single examples at inference time, without introducing significant run-time overhead or architecture changes.
</details>
<details>
<summary>摘要</summary>
MODEL 过信任和轻度预测困难在机器学习中存在，而且使用标准的empirical risk minimization方法很难准确地考虑这些问题。在这个工作中，我们提出了一种新的方法，称为奇数-$k$-out learning（OKO），该方法通过将cross-entropy errorMinimize for sets而不是单个例子。这 naturally allows the model to capture数据示例之间的相关性，并实现更高的准确率和报告率，尤其在有限的训练数据和类别不均衡情况下。尽管OKO经常提供更好的报告率，而且在使用硬标签和dropping any additional calibration parameter tuning时，我们提供了理论基础，证明OKO自然地提供更好的报告率。我们还提供了广泛的实验分析，证明我们的理论发现。我们强调OKO是一种通用的框架，可以轻松地适应多种设置，并且训练后的模型可以在推理时间应用于单个例子，无需添加显著的运行时过程 overhead或者architecture change。
</details></li>
</ul>
<hr>
<h2 id="Knowledge-Guided-Additive-Modeling-For-Supervised-Regression"><a href="#Knowledge-Guided-Additive-Modeling-For-Supervised-Regression" class="headerlink" title="Knowledge-Guided Additive Modeling For Supervised Regression"></a>Knowledge-Guided Additive Modeling For Supervised Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02229">http://arxiv.org/abs/2307.02229</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yannclaes/kg-regression">https://github.com/yannclaes/kg-regression</a></li>
<li>paper_authors: Yann Claes, Vân Anh Huynh-Thu, Pierre Geurts</li>
<li>for: 本研究旨在评估混合模型在标准回归问题上的性能，并与传统机器学习方法进行比较。</li>
<li>methods: 本研究使用了混合模型，其中包括添加式地将 Parametric 物理 термин与机器学习 термин相加。 我们还研究了模型免Selection 训练方法。</li>
<li>results: 我们在 synthetic 和实际回归问题上进行了多种方法的比较，结果表明，混合模型在global performance和参数确定方面具有优势。<details>
<summary>Abstract</summary>
Learning processes by exploiting restricted domain knowledge is an important task across a plethora of scientific areas, with more and more hybrid methods combining data-driven and model-based approaches. However, while such hybrid methods have been tested in various scientific applications, they have been mostly tested on dynamical systems, with only limited study about the influence of each model component on global performance and parameter identification. In this work, we assess the performance of hybrid modeling against traditional machine learning methods on standard regression problems. We compare, on both synthetic and real regression problems, several approaches for training such hybrid models. We focus on hybrid methods that additively combine a parametric physical term with a machine learning term and investigate model-agnostic training procedures. We also introduce a new hybrid approach based on partial dependence functions. Experiments are carried out with different types of machine learning models, including tree-based models and artificial neural networks.
</details>
<details>
<summary>摘要</summary>
学习通过利用限制领域知识是科学领域中重要任务，随着更多的混合方法相继出现，这些混合方法结合数据驱动和模型基于方法。然而，虽然这些混合方法在科学应用中得到了证明，但是它们在动力系统上进行了大多数测试，对每个模型组件对全局性表现的影响尚未得到了充分的研究。在这项工作中，我们对混合模型与传统机器学习方法进行比较，在标准回归问题上进行了评估。我们比较了多种混合方法，包括添加式地将 Parametric 物理项与机器学习项相加的方法，以及模型无关的训练过程。此外，我们还介绍了一种基于 partial dependence 函数的新的混合方法。实验使用了不同类型的机器学习模型，包括树状模型和人工神经网络。
</details></li>
</ul>
<hr>
<h2 id="Personalized-Federated-Learning-via-Amortized-Bayesian-Meta-Learning"><a href="#Personalized-Federated-Learning-via-Amortized-Bayesian-Meta-Learning" class="headerlink" title="Personalized Federated Learning via Amortized Bayesian Meta-Learning"></a>Personalized Federated Learning via Amortized Bayesian Meta-Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02222">http://arxiv.org/abs/2307.02222</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiyu Liu, Shaogao Lv, Dun Zeng, Zenglin Xu, Hui Wang, Yue Yu</li>
<li>for: 本研究旨在 Addressing the challenge of statistical heterogeneity in federated learning, 即让多个客户端协同学习一个全局模型，而不曝光他们私有数据。</li>
<li>methods: 本文提出了一种新的个性化联合学习方法，即\emph{FedABML}，它使用了层次变分推理来跨客户端。全局前期目标是捕捉客户端间共同内在结构的表示，然后将其转移到每个客户端的特定任务上，以便通过一些本地更新生成高度准确的客户端特定抽象 posterior。</li>
<li>results: 我们的理论分析表明，\emph{FedABML} 可以在未见数据上提供一个上下文bound，并且保证模型在未见数据上的泛化性能。此外，我们还实现了一些验证性实验，显示\emph{FedABML} 可以超越一些竞争对手。<details>
<summary>Abstract</summary>
Federated learning is a decentralized and privacy-preserving technique that enables multiple clients to collaborate with a server to learn a global model without exposing their private data. However, the presence of statistical heterogeneity among clients poses a challenge, as the global model may struggle to perform well on each client's specific task. To address this issue, we introduce a new perspective on personalized federated learning through Amortized Bayesian Meta-Learning. Specifically, we propose a novel algorithm called \emph{FedABML}, which employs hierarchical variational inference across clients. The global prior aims to capture representations of common intrinsic structures from heterogeneous clients, which can then be transferred to their respective tasks and aid in the generation of accurate client-specific approximate posteriors through a few local updates. Our theoretical analysis provides an upper bound on the average generalization error and guarantees the generalization performance on unseen data. Finally, several empirical results are implemented to demonstrate that \emph{FedABML} outperforms several competitive baselines.
</details>
<details>
<summary>摘要</summary>
“联邦学习”是一种分散式和隐私保证的技术，让多个客户端与服务器共同学习一个全球模型，不会曝露他们的私人数据。然而，客户端的统计差异对全球模型的性能产生挑战，因为全球模型可能无法很好地适应每个客户端的特定任务。为解决这个问题，我们将在个人化联邦学习中引入新的见解，通过整合泛化统计学和机器学习。 Specifically，我们提出一个名为“FedABML”的新算法，它使用客户端之间的层次统计推导，以捕捉客户端的共同内在结构表现。这些表现可以转移到每个客户端的特定任务中，并通过一些本地更新产生高精度的客户端特定概率 posteriors。我们的理论分析提供了随机数据的平均泛化错误上限，并保证模型在未见数据上的泛化性能。最后，我们进行了实验，证明了FedABML在多个竞争性基eline上表现出色。
</details></li>
</ul>
<hr>
<h2 id="On-the-Adversarial-Robustness-of-Generative-Autoencoders-in-the-Latent-Space"><a href="#On-the-Adversarial-Robustness-of-Generative-Autoencoders-in-the-Latent-Space" class="headerlink" title="On the Adversarial Robustness of Generative Autoencoders in the Latent Space"></a>On the Adversarial Robustness of Generative Autoencoders in the Latent Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02202">http://arxiv.org/abs/2307.02202</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingfei Lu, Badong Chen<br>for:This paper focuses on the adversarial robustness of generative autoencoders, specifically in the latent space.methods:The authors use various attacks in the latent space to demonstrate the vulnerability of popular generative autoencoders. They also compare the performance of variational autoencoders with their deterministic variants and observe that the latter has better latent robustness.results:The authors find that there is a trade-off between adversarial robustness and the degree of disentanglement of the latent codes. They also show that adversarial training can improve the latent robustness of VAEs.<details>
<summary>Abstract</summary>
The generative autoencoders, such as the variational autoencoders or the adversarial autoencoders, have achieved great success in lots of real-world applications, including image generation, and signal communication.   However, little concern has been devoted to their robustness during practical deployment.   Due to the probabilistic latent structure, variational autoencoders (VAEs) may confront problems such as a mismatch between the posterior distribution of the latent and real data manifold, or discontinuity in the posterior distribution of the latent.   This leaves a back door for malicious attackers to collapse VAEs from the latent space, especially in scenarios where the encoder and decoder are used separately, such as communication and compressed sensing.   In this work, we provide the first study on the adversarial robustness of generative autoencoders in the latent space.   Specifically, we empirically demonstrate the latent vulnerability of popular generative autoencoders through attacks in the latent space.   We also evaluate the difference between variational autoencoders and their deterministic variants and observe that the latter performs better in latent robustness.   Meanwhile, we identify a potential trade-off between the adversarial robustness and the degree of the disentanglement of the latent codes.   Additionally, we also verify the feasibility of improvement for the latent robustness of VAEs through adversarial training.   In summary, we suggest concerning the adversarial latent robustness of the generative autoencoders, analyze several robustness-relative issues, and give some insights into a series of key challenges.
</details>
<details>
<summary>摘要</summary>
“生成自 taught autoencoders，如variational autoencoders或adversarial autoencoders，在实际应用中取得了很大的成功，包括图像生成和信号通信。然而，对于它们在实际应用中的Robustness仍然受到了很少的关注。由于生成自 taught autoencoders的潜在阶层结构是probabilistic，因此它们可能会面临 posterior distribution of the latent和实际数据构造的不一致问题，或者 latent posterior distribution的突变。这使得黑客可以通过从latent空间攻击VAEs，特别是在encoder和decoder分开使用的情况下，如传输和压缩感知。在这个工作中，我们提供了生成自 taught autoencoders在latent空间的攻击Robustness的首次研究。我们透过实验示出了流行的生成自 taught autoencoders在latent空间的漏攻击敏感性。我们还评估了variational autoencoders和其决定性版本之间的差异，发现后者在latent robustness方面表现更好。此外，我们也发现了在提高VAEs的latent robustness方面存在一定的贸易关系。最后，我们还验证了对VAEs的latent robustness进行反向培训可以提高其Robustness。总之，我们建议关注生成自 taught autoencoders的latent Robustness，分析了一些Robustness相关的问题，并给出了一些关键挑战的见解。”
</details></li>
</ul>
<hr>
<h2 id="ChiENN-Embracing-Molecular-Chirality-with-Graph-Neural-Networks"><a href="#ChiENN-Embracing-Molecular-Chirality-with-Graph-Neural-Networks" class="headerlink" title="ChiENN: Embracing Molecular Chirality with Graph Neural Networks"></a>ChiENN: Embracing Molecular Chirality with Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02198">http://arxiv.org/abs/2307.02198</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gmum/chienn">https://github.com/gmum/chienn</a></li>
<li>paper_authors: Piotr Gaiński, Michał Koziarski, Jacek Tabor, Marek Śmieja</li>
<li>for: 本研究旨在使用Graph Neural Networks (GNNs)在化学分子 graph 上进行预测，并能够区分同一分子的镜像（折射体）。</li>
<li>methods: 我们提出了一种理论上正确的消息传递方案，使得 GNNs 能够具有邻居节点顺序的敏感性。我们在这个概念上将其应用于分子异旋性预测任务中，并构建了具有折射体敏感性的 Chiral Edge Neural Network (ChiENN) 层。</li>
<li>results: 我们的实验结果显示，将 ChiENN 层添加到 GNN 模型中，可以超越当前状态艺术方法在分子异旋性预测任务中的性能。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) play a fundamental role in many deep learning problems, in particular in cheminformatics. However, typical GNNs cannot capture the concept of chirality, which means they do not distinguish between the 3D graph of a chemical compound and its mirror image (enantiomer). The ability to distinguish between enantiomers is important especially in drug discovery because enantiomers can have very distinct biochemical properties. In this paper, we propose a theoretically justified message-passing scheme, which makes GNNs sensitive to the order of node neighbors. We apply that general concept in the context of molecular chirality to construct Chiral Edge Neural Network (ChiENN) layer which can be appended to any GNN model to enable chirality-awareness. Our experiments show that adding ChiENN layers to a GNN outperforms current state-of-the-art methods in chiral-sensitive molecular property prediction tasks.
</details>
<details>
<summary>摘要</summary>
图 neural network (GNN) 在深度学习中扮演了基本角色，特别是在化学信息学中。然而， typical GNN 无法捕捉扁平性概念，这意味着它们不能分辨化学结构图和其镜像（扁平体）之间的差异。在药物发现中，能够分辨扁平体的能力是非常重要的，因为扁平体可能具有非常不同的生物化学性质。在这篇论文中，我们提出了一种理论基础的消息传递方案，该方案使 GNN 对节点邻居的顺序敏感。我们将该概念应用于分子扁平性上，并构建了 Chiral Edge Neural Network（ChiENN）层，可以让 GNN 模型具有扁平性意识。我们的实验表明，将 ChiENN 层添加到 GNN 模型后，可以超越当前状态的术法在扁平性敏感分子性质预测任务中表现。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-AI-systems-under-uncertain-ground-truth-a-case-study-in-dermatology"><a href="#Evaluating-AI-systems-under-uncertain-ground-truth-a-case-study-in-dermatology" class="headerlink" title="Evaluating AI systems under uncertain ground truth: a case study in dermatology"></a>Evaluating AI systems under uncertain ground truth: a case study in dermatology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02191">http://arxiv.org/abs/2307.02191</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Stutz, Ali Taylan Cemgil, Abhijit Guha Roy, Tatiana Matejovicova, Melih Barsbey, Patricia Strachan, Mike Schaekermann, Jan Freyberg, Rajeev Rikhye, Beverly Freeman, Javier Perez Matos, Umesh Telang, Dale R. Webster, Yuan Liu, Greg S. Corrado, Yossi Matias, Pushmeet Kohli, Yun Liu, Arnaud Doucet, Alan Karthikesalingam</li>
<li>for: 这个论文目的是提出了一种方法来评估AI模型的性能时考虑到真实的数据预期不确定性。</li>
<li>methods: 这个论文使用了一种基于统计模型的方法来汇集笔记，并提出了一种新的性能评价指标来考虑annotations uncertainty。</li>
<li>results: 研究发现，使用传统的deterministic aggregation方法时，评估结果具有很大的uncertainty，而使用提出的统计模型方法可以更好地评估模型的性能和uncertainty。<details>
<summary>Abstract</summary>
For safety, AI systems in health undergo thorough evaluations before deployment, validating their predictions against a ground truth that is assumed certain. However, this is actually not the case and the ground truth may be uncertain. Unfortunately, this is largely ignored in standard evaluation of AI models but can have severe consequences such as overestimating the future performance. To avoid this, we measure the effects of ground truth uncertainty, which we assume decomposes into two main components: annotation uncertainty which stems from the lack of reliable annotations, and inherent uncertainty due to limited observational information. This ground truth uncertainty is ignored when estimating the ground truth by deterministically aggregating annotations, e.g., by majority voting or averaging. In contrast, we propose a framework where aggregation is done using a statistical model. Specifically, we frame aggregation of annotations as posterior inference of so-called plausibilities, representing distributions over classes in a classification setting, subject to a hyper-parameter encoding annotator reliability. Based on this model, we propose a metric for measuring annotation uncertainty and provide uncertainty-adjusted metrics for performance evaluation. We present a case study applying our framework to skin condition classification from images where annotations are provided in the form of differential diagnoses. The deterministic adjudication process called inverse rank normalization (IRN) from previous work ignores ground truth uncertainty in evaluation. Instead, we present two alternative statistical models: a probabilistic version of IRN and a Plackett-Luce-based model. We find that a large portion of the dataset exhibits significant ground truth uncertainty and standard IRN-based evaluation severely over-estimates performance without providing uncertainty estimates.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)为了保障，医疗领域中的 AI 系统在部署前都会进行严格的评估，以验证其预测结果与固定的真实值进行比较。然而，事实上，真实值并不是固定的，而是具有uncertainty。这一点很容易被忽略，但可能导致未来性能的过度估计。为了避免这种情况，我们需要考虑真实值的uncertainty。我们假设真实值的uncertainty可以分解为两个主要组成部分：注释uncertainty和内在uncertainty。注释uncertainty来自于不可靠的注释，而内在uncertainty来自于限制的观察信息。标准评估方法忽略了这些uncertainty，而是通过 deterministic aggregation（例如，多数投票或平均）来Estimating the ground truth。在这种情况下，我们提出了一种使用统计模型进行注释聚合的框架。specifically，我们将注释聚合视为 posterior inference of so-called plausibilities， representing distributions over classes in a classification setting, subject to a hyper-parameter encoding annotator reliability。基于这个模型，我们提出了一个度量注释uncertainty的metric，并提供了不确定度调整的性能评估 metric。我们在皮肤状况分类从图像中进行了一个案例研究， где注释是在 differential diagnoses 的形式提供的。previous work的 deterministic adjudication process（IRN）忽略了真实值uncertainty，而是通过 deterministic aggregation来Estimating the ground truth。在这种情况下，我们提出了两种统计模型：一种是probabilistic IRN，另一种是Plackett-Luce-based model。我们发现大量数据中存在很大的真实值uncertainty，标准 IRN-based evaluation 严重过度估计性能。
</details></li>
</ul>
<hr>
<h2 id="Diffusion-Models-for-Computational-Design-at-the-Example-of-Floor-Plans"><a href="#Diffusion-Models-for-Computational-Design-at-the-Example-of-Floor-Plans" class="headerlink" title="Diffusion Models for Computational Design at the Example of Floor Plans"></a>Diffusion Models for Computational Design at the Example of Floor Plans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02511">http://arxiv.org/abs/2307.02511</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joern Ploennigs, Markus Berger</li>
<li>for: 这个研究旨在测试传播模型在土木工程中的应用，尤其是创建特定的建筑计划。</li>
<li>methods: 这个研究使用传播模型实现了图像生成，并提出了改进 semantic 编码的新传播模型。</li>
<li>results: 研究发现，这些传播模型可以从 6% 提高至 90% 的有效 floor plan 生成，并且在不同的例子中进行了多个实验。<details>
<summary>Abstract</summary>
AI Image generators based on diffusion models are widely discussed recently for their capability to create images from simple text prompts. But, for practical use in civil engineering they need to be able to create specific construction plans for given constraints. Within this paper we explore the capabilities of those diffusion-based AI generators for computational design at the example of floor plans and identify their current limitation. We explain how the diffusion-models work and propose new diffusion models with improved semantic encoding. In several experiments we show that we can improve validity of generated floor plans from 6% to 90% and query performance for different examples. We identify short comings and derive future research challenges of those models and discuss the need to combine diffusion models with building information modelling. With this we provide key insights into the current state and future directions for diffusion models in civil engineering.
</details>
<details>
<summary>摘要</summary>
《Diffusion模型基于AI图像生成器在近期受到广泛关注，能够从简单文本提示中生成图像。但在实际的ivil工程应用中，它们需要能够根据给定的约束创建特定的建筑计划。本文介绍了Diffusion模型在计算设计中的能力，以卷积图生成器为例，并评估了其Semantic编码的改进。我们在多个实验中示出，可以从6%提高到90%的有效性，并且提高了不同的示例的查询性能。我们还识别了这些模型的缺点，并提出了未来研究挑战。我们认为Diffusion模型和建筑信息模型的结合是未来的发展趋势。这些发现为Diffusion模型在 civil工程领域的现状和未来发展提供了关键的导向。》Note: Please note that the translation is in Simplified Chinese, which is one of the two standard forms of Chinese. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="DiffFlow-A-Unified-SDE-Framework-for-Score-Based-Diffusion-Models-and-Generative-Adversarial-Networks"><a href="#DiffFlow-A-Unified-SDE-Framework-for-Score-Based-Diffusion-Models-and-Generative-Adversarial-Networks" class="headerlink" title="DiffFlow: A Unified SDE Framework for Score-Based Diffusion Models and Generative Adversarial Networks"></a>DiffFlow: A Unified SDE Framework for Score-Based Diffusion Models and Generative Adversarial Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02159">http://arxiv.org/abs/2307.02159</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingwei Zhang, Han Shi, Jincheng Yu, Enze Xie, Zhenguo Li</li>
<li>for: 这个论文的目的是提出一种统一的概率理论框架，用于描述 explict 生成模型和 implicit 生成模型之间的关系。</li>
<li>methods: 该论文使用了一种名为 Discriminator Denoising Diffusion Flow (DiffFlow) 的新型 Stochastic Differential Equation (SDE)，用于描述生成模型的学习动态。</li>
<li>results: 该论文提出了一种可以在Explicit 生成模型和 implicit 生成模型之间进行满足的平衡点，并且可以通过调整权重来实现高质量样本的生成和快速样本生成。<details>
<summary>Abstract</summary>
Generative models can be categorized into two types: explicit generative models that define explicit density forms and allow exact likelihood inference, such as score-based diffusion models (SDMs) and normalizing flows; implicit generative models that directly learn a transformation from the prior to the data distribution, such as generative adversarial nets (GANs). While these two types of models have shown great success, they suffer from respective limitations that hinder them from achieving fast sampling and high sample quality simultaneously. In this paper, we propose a unified theoretic framework for SDMs and GANs. We shown that: i) the learning dynamics of both SDMs and GANs can be described as a novel SDE named Discriminator Denoising Diffusion Flow (DiffFlow) where the drift can be determined by some weighted combinations of scores of the real data and the generated data; ii) By adjusting the relative weights between different score terms, we can obtain a smooth transition between SDMs and GANs while the marginal distribution of the SDE remains invariant to the change of the weights; iii) we prove the asymptotic optimality and maximal likelihood training scheme of the DiffFlow dynamics; iv) under our unified theoretic framework, we introduce several instantiations of the DiffFLow that provide new algorithms beyond GANs and SDMs with exact likelihood inference and have potential to achieve flexible trade-off between high sample quality and fast sampling speed.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>生成模型可以分为两类：Explicit生成模型，它们定义明确的概率形式，并允许准确的可能性推断，如排Diffusion模型（SDM）和Normalizing Flow；Implicit生成模型，它们直接学习数据分布与假设分布之间的变换，如生成敌方网络（GAN）。虽然这两种模型都有显著的成功，但它们受到减速采样和高质量采样的限制。在这篇论文中，我们提出一个统一的理论框架，用于SDM和GAN。我们证明了：i) SDM和GAN的学习动力可以被描述为一种名为Discriminator Denoising Diffusion Flow（DiffFlow）的新型SDE，其涨动可以由真实数据和生成数据的得分组成的权重所决定; ii) 通过调整不同得分项的相对权重，可以实现SDM和GAN之间的滑块过渡，而且采样分布的总体征不变; iii) 我们证明DiffFlow动力的极限优化和最大可能性训练方案; iv) 在我们统一的理论框架下，我们引入了多种DiffFlow实例，提供了新的算法，包括SDM和GAN中的准确可能性推断和高速采样速度。
</details></li>
</ul>
<hr>
<h2 id="Wasserstein-Auto-Encoders-of-Merge-Trees-and-Persistence-Diagrams"><a href="#Wasserstein-Auto-Encoders-of-Merge-Trees-and-Persistence-Diagrams" class="headerlink" title="Wasserstein Auto-Encoders of Merge Trees (and Persistence Diagrams)"></a>Wasserstein Auto-Encoders of Merge Trees (and Persistence Diagrams)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02509">http://arxiv.org/abs/2307.02509</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahieu Pont, Julien Tierny</li>
<li>for: 本研究提出了一种基于 Wasserstein  metric 空间的merge tree auto-encoding（MT-WAE）方法，用于提高传统自编码器的准确率和可读性。</li>
<li>methods: 本方法使用了一种新的非线性神经网络结构，将merge tree经过多层神经网络的操作，以实现更高的准确率和可读性。</li>
<li>results: 实验结果表明，MT-WAE可以快速计算merge tree，并且可以准确地压缩merge tree，同时 preserved Wasserstein 距离和 clusters。此外，本方法还可以应用于维度减少和数据分析等领域。<details>
<summary>Abstract</summary>
This paper presents a computational framework for the Wasserstein auto-encoding of merge trees (MT-WAE), a novel extension of the classical auto-encoder neural network architecture to the Wasserstein metric space of merge trees. In contrast to traditional auto-encoders which operate on vectorized data, our formulation explicitly manipulates merge trees on their associated metric space at each layer of the network, resulting in superior accuracy and interpretability. Our novel neural network approach can be interpreted as a non-linear generalization of previous linear attempts [65] at merge tree encoding. It also trivially extends to persistence diagrams. Extensive experiments on public ensembles demonstrate the efficiency of our algorithms, with MT-WAE computations in the orders of minutes on average. We show the utility of our contributions in two applications adapted from previous work on merge tree encoding [65]. First, we apply MT-WAE to data reduction and reliably compress merge trees by concisely representing them with their coordinates in the final layer of our auto-encoder. Second, we document an application to dimensionality reduction, by exploiting the latent space of our auto-encoder, for the visual analysis of ensemble data. We illustrate the versatility of our framework by introducing two penalty terms, to help preserve in the latent space both the Wasserstein distances between merge trees, as well as their clusters. In both applications, quantitative experiments assess the relevance of our framework. Finally, we provide a C++ implementation that can be used for reproducibility.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Harmonizing-Feature-Attributions-Across-Deep-Learning-Architectures-Enhancing-Interpretability-and-Consistency"><a href="#Harmonizing-Feature-Attributions-Across-Deep-Learning-Architectures-Enhancing-Interpretability-and-Consistency" class="headerlink" title="Harmonizing Feature Attributions Across Deep Learning Architectures: Enhancing Interpretability and Consistency"></a>Harmonizing Feature Attributions Across Deep Learning Architectures: Enhancing Interpretability and Consistency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02150">http://arxiv.org/abs/2307.02150</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Abdul Kadir, Gowtham Krishna Addluri, Daniel Sonntag</li>
<li>for: 这 study aims to improve the interpretability and trustworthiness of machine learning models by examining the generalization of feature attributions across various deep learning architectures.</li>
<li>methods: The study uses feature attribution methods to provide local explanations of model predictions, and explores the feasibility of utilizing these methods as a future detector.</li>
<li>results: The findings suggest that harmonized feature attribution methods can improve interpretability and trust in machine learning applications, regardless of the underlying architecture.<details>
<summary>Abstract</summary>
Ensuring the trustworthiness and interpretability of machine learning models is critical to their deployment in real-world applications. Feature attribution methods have gained significant attention, which provide local explanations of model predictions by attributing importance to individual input features. This study examines the generalization of feature attributions across various deep learning architectures, such as convolutional neural networks (CNNs) and vision transformers. We aim to assess the feasibility of utilizing a feature attribution method as a future detector and examine how these features can be harmonized across multiple models employing distinct architectures but trained on the same data distribution. By exploring this harmonization, we aim to develop a more coherent and optimistic understanding of feature attributions, enhancing the consistency of local explanations across diverse deep-learning models. Our findings highlight the potential for harmonized feature attribution methods to improve interpretability and foster trust in machine learning applications, regardless of the underlying architecture.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Towards-Open-Federated-Learning-Platforms-Survey-and-Vision-from-Technical-and-Legal-Perspectives"><a href="#Towards-Open-Federated-Learning-Platforms-Survey-and-Vision-from-Technical-and-Legal-Perspectives" class="headerlink" title="Towards Open Federated Learning Platforms: Survey and Vision from Technical and Legal Perspectives"></a>Towards Open Federated Learning Platforms: Survey and Vision from Technical and Legal Perspectives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02140">http://arxiv.org/abs/2307.02140</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/morningd/model-centric-fml">https://github.com/morningd/model-centric-fml</a></li>
<li>paper_authors: Moming Duan</li>
<li>for: 本文提出了一种新的 Federated Learning（FL）平台设计，即开放式 Federated Learning Platforms，以扩展FL的应用场景和提高数据持有者的参与积极性。</li>
<li>methods: 本文提出了两种对接口型FL框架的替换方案：查询型FL和合同型FL，以解决FL中的严重的服务器-客户端耦合、模型重复利用和非公共问题。</li>
<li>results: 本文通过对技术和法律领域的分析，证明了开放式FL平台的可行性和优势，并提出了一种模型license兼容分类法，以便在FL研究中更好地识别和解决模型使用权限问题。<details>
<summary>Abstract</summary>
Traditional Federated Learning (FL) follows a server-domincated cooperation paradigm which narrows the application scenarios of FL and decreases the enthusiasm of data holders to participate. To fully unleash the potential of FL, we advocate rethinking the design of current FL frameworks and extending it to a more generalized concept: Open Federated Learning Platforms. We propose two reciprocal cooperation frameworks for FL to achieve this: query-based FL and contract-based FL. In this survey, we conduct a comprehensive review of the feasibility of constructing an open FL platform from both technical and legal perspectives. We begin by reviewing the definition of FL and summarizing its inherent limitations, including server-client coupling, low model reusability, and non-public. In the query-based FL platform, which is an open model sharing and reusing platform empowered by the community for model mining, we explore a wide range of valuable topics, including the availability of up-to-date model repositories for model querying, legal compliance analysis between different model licenses, and copyright issues and intellectual property protection in model reusing. In particular, we introduce a novel taxonomy to streamline the analysis of model license compatibility in FL studies that involve batch model reusing methods, including combination, amalgamation, distillation, and generation. This taxonomy provides a systematic framework for identifying the corresponding clauses of licenses and facilitates the identification of potential legal implications and restrictions when reusing models. Through this survey, we uncover the the current dilemmas faced by FL and advocate for the development of sustainable open FL platforms. We aim to provide guidance for establishing such platforms in the future, while identifying potential problems and challenges that need to be addressed.
</details>
<details>
<summary>摘要</summary>
传统的联合学习（FL）采用服务器主导的合作方式，这限制了FL的应用场景和数据持有者的参与积极性。为了充分发挥FL的潜力，我们提倡重新设计当前FL框架，扩展其为更通用的概念：开放联合学习平台。我们提出了两种相互合作的FL框架：查询基于的FL和合同基于的FL。在这篇评论中，我们对构建开放FL平台的技术和法律方面进行了全面的审查。我们开始介绍FL的定义和其内置的局限性，包括服务器客户端集成、低级别模型重用和非公共。在查询基于的FL平台中，我们探讨了许多有价值的话题，包括社区 empowered 的模型分享和重用平台，以及模型查询时的法律合规分析、版权问题和知识产权保护。特别是，我们提出了一种新的分类系统，用于协调FL研究中批处理模型 reuse 方法中的许可证兼容性分析，包括组合、混合、精炼和生成等方法。这种分类系统为在FL研究中复用模型时鉴别相关的许可证条款，并且可以帮助确定复用模型时的可能的法律后果和限制。通过这篇评论，我们揭示了当前FL面临的困境，并提倡开发可持续的开放FL平台。我们希望通过这篇评论，为未来建立开放FL平台提供指南，并识别可能的问题和挑战。
</details></li>
</ul>
<hr>
<h2 id="Implicit-Differentiation-for-Hyperparameter-Tuning-the-Weighted-Graphical-Lasso"><a href="#Implicit-Differentiation-for-Hyperparameter-Tuning-the-Weighted-Graphical-Lasso" class="headerlink" title="Implicit Differentiation for Hyperparameter Tuning the Weighted Graphical Lasso"></a>Implicit Differentiation for Hyperparameter Tuning the Weighted Graphical Lasso</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02130">http://arxiv.org/abs/2307.02130</a></li>
<li>repo_url: None</li>
<li>paper_authors: Can Pouliquen, Paulo Gonçalves, Mathurin Massias, Titouan Vayer</li>
<li>for: 提出一种 Framework 和算法来调整图像隐藏常量的超参数。</li>
<li>methods: 使用一种精简型搜索方法来解决一个笛卡尔级别优化问题。</li>
<li>results:  derivation of the Jacobian of the Graphical Lasso solution with respect to its regularization hyperparameters.<details>
<summary>Abstract</summary>
We provide a framework and algorithm for tuning the hyperparameters of the Graphical Lasso via a bilevel optimization problem solved with a first-order method. In particular, we derive the Jacobian of the Graphical Lasso solution with respect to its regularization hyperparameters.
</details>
<details>
<summary>摘要</summary>
我们提供了一个框架和算法，用于调整图解lasso的超参数via一个双层优化问题，解决使用首个方法。特别是，我们计算了图解lasso解的正则化超参数对它的Jacobian。Here's a breakdown of the translation:* 我们 (wǒmen) - we* 提供 (tīngyè) - provide* 框架 (kāiframe) - framework* 算法 (suānfǎ) - algorithm* 调整 (tiējian) - tune* 超参数 (chāoxiǎn) - hyperparameters* via (via) - via* 双层优化问题 (shuāngcéng yòuhuì wèn) - bilevel optimization problem* 解决 (jiějué) - solve* 使用 (fùyòu) - using* 首个方法 (shǒu gè fāng) - first-order method* 特别是 (tèbié shì) - particularly* 我们计算 (wǒmen jìsuān) - we calculate* 图解lasso解 (tújiě lasso jiě) - Graphical Lasso solution* 正则化超参数 (zhèngxíng huìxiāng) - regularization hyperparameters* 对 (duì) - on* 它 (tā) - it* Jacobian (jiābǐjian) - Jacobian
</details></li>
</ul>
<hr>
<h2 id="How-Deep-Neural-Networks-Learn-Compositional-Data-The-Random-Hierarchy-Model"><a href="#How-Deep-Neural-Networks-Learn-Compositional-Data-The-Random-Hierarchy-Model" class="headerlink" title="How Deep Neural Networks Learn Compositional Data: The Random Hierarchy Model"></a>How Deep Neural Networks Learn Compositional Data: The Random Hierarchy Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02129">http://arxiv.org/abs/2307.02129</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pcsl-epfl/hierarchy-learning">https://github.com/pcsl-epfl/hierarchy-learning</a></li>
<li>paper_authors: Leonardo Petrini, Francesco Cagnetta, Umberto M. Tomasini, Alessandro Favero, Matthieu Wyart</li>
<li>for: 这个论文的目的是解释深度卷积神经网络如何在高维度数据上学习普遍的任务。</li>
<li>methods: 这个论文使用的方法是使用深度卷积神经网络来学习Random Hierarchy Model，这是一个模拟真实数据的简单分类任务。</li>
<li>results: 研究发现，深度卷积神经网络需要的训练数据量（$P^*$）与高维度数据中类别的数量（$n_c$）和高级特征的数量（$m$）以及重复层数（$L$）有关，具体来说，$P^*$的增长率为$n_c m^L$,只是增长平方根。此外，研究还发现，当训练数据量够多时，深度卷积神经网络的表征将变得对于同义词替换无关，并且可以捕捉低级特征与类别之间的相关性。<details>
<summary>Abstract</summary>
Learning generic high-dimensional tasks is notably hard, as it requires a number of training data exponential in the dimension. Yet, deep convolutional neural networks (CNNs) have shown remarkable success in overcoming this challenge. A popular hypothesis is that learnable tasks are highly structured and that CNNs leverage this structure to build a low-dimensional representation of the data. However, little is known about how much training data they require, and how this number depends on the data structure. This paper answers this question for a simple classification task that seeks to capture relevant aspects of real data: the Random Hierarchy Model. In this model, each of the $n_c$ classes corresponds to $m$ synonymic compositions of high-level features, which are in turn composed of sub-features through an iterative process repeated $L$ times. We find that the number of training data $P^*$ required by deep CNNs to learn this task (i) grows asymptotically as $n_c m^L$, which is only polynomial in the input dimensionality; (ii) coincides with the training set size such that the representation of a trained network becomes invariant to exchanges of synonyms; (iii) corresponds to the number of data at which the correlations between low-level features and classes become detectable. Overall, our results indicate how deep CNNs can overcome the curse of dimensionality by building invariant representations, and provide an estimate of the number of data required to learn a task based on its hierarchically compositional structure.
</details>
<details>
<summary>摘要</summary>
学习高维任务非常困难，因为它们需要数据量的幂等于维度。然而，深度卷积神经网络（CNN）表现出了很好的成功，即使在高维数据上。一种广泛的假设是，学习任务是高度结构化的，并且CNN可以利用这种结构来建立低维表示。然而，对于学习多少数据，还不清楚。这篇论文回答了这个问题，对于一个简单的分类任务，即Random Hierarchy Model。在这个模型中，每个分类对应$n_c$个高级特征的多个同义词组合，这些组合通过循环的过程重复$L$次。我们发现，深度CNN需要学习这个任务的数据量($P^*$)（i）在$n_c m^L$的极限上增长，这只是输入维度的多项式函数；（ii）与训练集大小相同，使得训练后神经网络的表示变得对同义词交换无关的；（iii）与低级特征和类之间的相关性变得可识别。总的来说，我们的结果表明深度CNN可以超越维度味精，并提供了学习任务基于层次结构的数据量的估算。
</details></li>
</ul>
<hr>
<h2 id="Robust-Graph-Structure-Learning-with-the-Alignment-of-Features-and-Adjacency-Matrix"><a href="#Robust-Graph-Structure-Learning-with-the-Alignment-of-Features-and-Adjacency-Matrix" class="headerlink" title="Robust Graph Structure Learning with the Alignment of Features and Adjacency Matrix"></a>Robust Graph Structure Learning with the Alignment of Features and Adjacency Matrix</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02126">http://arxiv.org/abs/2307.02126</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaogao Lv, Gang Wen, Shiyu Liu, Linsen Wei, Ming Li</li>
<li>for: 提高图 neural network 的 robustness，jointly 学习干净图结构和对应表示。</li>
<li>methods: 提出了一种新的准则化 graph structure learning 方法，利用特征信息和图信息的协调，基于我们 derive的节点级 Rademacher 复杂性下界。还具有减少维度的稀疏降维方法，使用低维度的节点特征来利用图结构。</li>
<li>results: 对实际图据进行了实验，表明我们提出的 GSL 方法在受到噪声影响的图结构下表现出色，超过了多种竞争对手。<details>
<summary>Abstract</summary>
To improve the robustness of graph neural networks (GNN), graph structure learning (GSL) has attracted great interest due to the pervasiveness of noise in graph data. Many approaches have been proposed for GSL to jointly learn a clean graph structure and corresponding representations. To extend the previous work, this paper proposes a novel regularized GSL approach, particularly with an alignment of feature information and graph information, which is motivated mainly by our derived lower bound of node-level Rademacher complexity for GNNs. Additionally, our proposed approach incorporates sparse dimensional reduction to leverage low-dimensional node features that are relevant to the graph structure. To evaluate the effectiveness of our approach, we conduct experiments on real-world graphs. The results demonstrate that our proposed GSL method outperforms several competitive baselines, especially in scenarios where the graph structures are heavily affected by noise. Overall, our research highlights the importance of integrating feature and graph information alignment in GSL, as inspired by our derived theoretical result, and showcases the superiority of our approach in handling noisy graph structures through comprehensive experiments on real-world datasets.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Multi-Scale-U-Shape-MLP-for-Hyperspectral-Image-Classification"><a href="#Multi-Scale-U-Shape-MLP-for-Hyperspectral-Image-Classification" class="headerlink" title="Multi-Scale U-Shape MLP for Hyperspectral Image Classification"></a>Multi-Scale U-Shape MLP for Hyperspectral Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10186">http://arxiv.org/abs/2307.10186</a></li>
<li>repo_url: None</li>
<li>paper_authors: Moule Lin, Weipeng Jing, Donglin Di, Guangsheng Chen, Houbing Song</li>
<li>for: 该研究旨在提出一种基于多尺度U型多层感知器（MUMLP）模型，以提高 гиперспектраль图像中像素的标识率。</li>
<li>methods: 该模型由设计的多尺度渠道（MSC）块和U型多层感知器（UMLP）结构组成。 MSC将通道维度变换并混合 spectral band 特征，以生成深度水平的表示。 UMLP 由encoder-decoder结构和多层感知器层组成，能够压缩大规模参数。</li>
<li>results: 对于三个公共数据集（Pavia University、Houston 2013和Houston 2018），研究人员进行了广泛的实验，并证明了该模型可以在多种预测任务中卓越于现状顶尖方法。<details>
<summary>Abstract</summary>
Hyperspectral images have significant applications in various domains, since they register numerous semantic and spatial information in the spectral band with spatial variability of spectral signatures. Two critical challenges in identifying pixels of the hyperspectral image are respectively representing the correlated information among the local and global, as well as the abundant parameters of the model. To tackle this challenge, we propose a Multi-Scale U-shape Multi-Layer Perceptron (MUMLP) a model consisting of the designed MSC (Multi-Scale Channel) block and the UMLP (U-shape Multi-Layer Perceptron) structure. MSC transforms the channel dimension and mixes spectral band feature to embed the deep-level representation adequately. UMLP is designed by the encoder-decoder structure with multi-layer perceptron layers, which is capable of compressing the large-scale parameters. Extensive experiments are conducted to demonstrate our model can outperform state-of-the-art methods across-the-board on three wide-adopted public datasets, namely Pavia University, Houston 2013 and Houston 2018
</details>
<details>
<summary>摘要</summary>
To address these challenges, we propose a Multi-Scale U-shape Multi-Layer Perceptron (MUMLP) model, consisting of a designed Multi-Scale Channel (MSC) block and a U-shape Multi-Layer Perceptron (UMLP) structure. The MSC block transforms the channel dimension and mixes spectral band features to embed deep-level representation adequately. The UMLP structure is designed with an encoder-decoder structure and multi-layer perceptron layers, which can compress large-scale parameters.Extensive experiments demonstrate that our MUMLP model outperforms state-of-the-art methods across-the-board on three widely adopted public datasets, namely Pavia University, Houston 2013, and Houston 2018.
</details></li>
</ul>
<hr>
<h2 id="Proportional-Response-Contextual-Bandits-for-Simple-and-Cumulative-Regret-Minimization"><a href="#Proportional-Response-Contextual-Bandits-for-Simple-and-Cumulative-Regret-Minimization" class="headerlink" title="Proportional Response: Contextual Bandits for Simple and Cumulative Regret Minimization"></a>Proportional Response: Contextual Bandits for Simple and Cumulative Regret Minimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02108">http://arxiv.org/abs/2307.02108</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sanath Kumar Krishnamurthy, Ruohan Zhan, Susan Athey, Emma Brunskill</li>
<li>for: 这篇 paper 的目的是提出一种 computationally efficient bandit algorithm 来实现 contextual bandit 的 optimal treatment assignment policy，并且可以适应 cumulative regret minimization 和 simple regret minimization 两种不同的目标。</li>
<li>methods: 这篇 paper 使用了一种新的 family of computationally efficient bandit algorithms，这些算法可以适应 contextual bandit 的条件下的模型错误和统计不确定性，并且可以在 continuous arm settings 中进行应用。这些算法基于 “conformal arm sets” (CASs) 的构造和依赖，CASs 提供了每个 context 中的一个包含 context-specific optimal arm 的集合，以 guaranteee 最小化 regret。</li>
<li>results: 这篇 paper 的实验结果显示了这些算法在 simple regret 和 cumulative regret 上都有优秀的表现，并且可以适应 contextual bandit 的不同条件下。此外，paper 还证明了一个 negative result，即一个 algorithm 无法同时 achiev instance-dependent simple regret guarantees 和 minimax optimal cumulative regret guarantees。<details>
<summary>Abstract</summary>
Simple regret minimization is a critical problem in learning optimal treatment assignment policies across various domains, including healthcare and e-commerce. However, it remains understudied in the contextual bandit setting. We propose a new family of computationally efficient bandit algorithms for the stochastic contextual bandit settings, with the flexibility to be adapted for cumulative regret minimization (with near-optimal minimax guarantees) and simple regret minimization (with SOTA guarantees). Furthermore, our algorithms adapt to model misspecification and extend to the continuous arm settings. These advantages come from constructing and relying on "conformal arm sets" (CASs), which provide a set of arms at every context that encompass the context-specific optimal arm with some probability across the context distribution. Our positive results on simple and cumulative regret guarantees are contrasted by a negative result, which shows that an algorithm can't achieve instance-dependent simple regret guarantees while simultaneously achieving minimax optimal cumulative regret guarantees.
</details>
<details>
<summary>摘要</summary>
<<SYS>>设置为使用简化中文（简化字）。<</SYS>>在多个领域中，包括医疗和电商，学习最佳准备分配策略是一个关键问题。然而，在上下文抽象机器人设置中，这个问题尚未得到充分研究。我们提出了一种新的计算效率高的抽象机器人算法家族，用于 Stochastic Contextual Bandit 设置，并且可以适应积累 regret 最小化（具有近似最优最小化保证）和简单 regret 最小化（具有 State-of-the-Art 保证）。此外，我们的算法可以适应模型误差和连续臂设置。这些优势来自于构造和依赖于 "conformal arm sets"（CASs），它们在每个上下文中提供一组拥有 Context-specific 优臂的arm，并且在 Context 分布中具有一定的概率。我们的正面结果表明，我们的算法可以在简单 regret 和积累 regret 两个方面提供保证，而且在模型误差和连续臂设置下也能够适应。相比之下，一个负面结果表明，无法同时实现实例特定的简单 regret 保证和最优的积累 regret 保证。
</details></li>
</ul>
<hr>
<h2 id="SoK-Privacy-Preserving-Data-Synthesis"><a href="#SoK-Privacy-Preserving-Data-Synthesis" class="headerlink" title="SoK: Privacy-Preserving Data Synthesis"></a>SoK: Privacy-Preserving Data Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02106">http://arxiv.org/abs/2307.02106</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuzheng Hu, Fan Wu, Qinbin Li, Yunhui Long, Gonzalo Munilla Garrido, Chang Ge, Bolin Ding, David Forsyth, Bo Li, Dawn Song</li>
<li>for: 本研究旨在提供一份概述、分析和讨论隐私保护数据分析（PPDS）领域的综述，以便回答有关PPDS方法的设计原则、分类、优缺点等问题。</li>
<li>methods: 本研究批判了两种主流PPDS方法：统计方法和深度学习（DL）基于方法。统计方法包括模型和表示方法的选择，而DL基于方法则包括不同生成模型原理。此外，我们还提供了参考表格、概括结论和开放问题。</li>
<li>results: 我们对私人图像生成任务进行了 benchmarking，并确定了DP-MERF是一种通用的方法。此外，我们还系统化了过去十年的研究成果，并提出了未来研究方向和对研究人员的呼吁。<details>
<summary>Abstract</summary>
As the prevalence of data analysis grows, safeguarding data privacy has become a paramount concern. Consequently, there has been an upsurge in the development of mechanisms aimed at privacy-preserving data analyses. However, these approaches are task-specific; designing algorithms for new tasks is a cumbersome process. As an alternative, one can create synthetic data that is (ideally) devoid of private information. This paper focuses on privacy-preserving data synthesis (PPDS) by providing a comprehensive overview, analysis, and discussion of the field. Specifically, we put forth a master recipe that unifies two prominent strands of research in PPDS: statistical methods and deep learning (DL)-based methods. Under the master recipe, we further dissect the statistical methods into choices of modeling and representation, and investigate the DL-based methods by different generative modeling principles. To consolidate our findings, we provide comprehensive reference tables, distill key takeaways, and identify open problems in the existing literature. In doing so, we aim to answer the following questions: What are the design principles behind different PPDS methods? How can we categorize these methods, and what are the advantages and disadvantages associated with each category? Can we provide guidelines for method selection in different real-world scenarios? We proceed to benchmark several prominent DL-based methods on the task of private image synthesis and conclude that DP-MERF is an all-purpose approach. Finally, upon systematizing the work over the past decade, we identify future directions and call for actions from researchers.
</details>
<details>
<summary>摘要</summary>
随着数据分析的普及，保护数据隐私已成为首要的关注点。因此，在数据分析中保持隐私的机制的开发呈现了增加趋势。然而，这些方法都是任务特定的，设计新任务的算法是一个繁琐的过程。为了解决这问题，可以创建没有隐私信息的Synthetic Data。本文关注于隐私保护数据合成（PPDS），提供了全面的概述、分析和讨论。特别是，我们提出了一种综合方法，称为“master recipe”，可以统一两个PPDS研究的主要流派：统计方法和深度学习（DL）基本方法。在master recipe下，我们进一步剖析统计方法，包括模型和表示方法的选择，并investigate DL基本模型的不同原则。为了归纳我们的发现，我们提供了完整的参考表格，概括关键点，并识别现有文献中的开放问题。因此，我们想回答以下问题：PPDS方法的设计原则是什么？如何分类这些方法，它们具有什么优势和缺点？是否可以提供实际应用场景中的方法选择指南？我们继续使用DP-MERF方法进行私人图像生成测试，并证明它是一种通用的方法。最后，我们系统化过去十年的工作，并提出未来方向和研究者的呼吁。
</details></li>
</ul>
<hr>
<h2 id="DARE-Towards-Robust-Text-Explanations-in-Biomedical-and-Healthcare-Applications"><a href="#DARE-Towards-Robust-Text-Explanations-in-Biomedical-and-Healthcare-Applications" class="headerlink" title="DARE: Towards Robust Text Explanations in Biomedical and Healthcare Applications"></a>DARE: Towards Robust Text Explanations in Biomedical and Healthcare Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02094">http://arxiv.org/abs/2307.02094</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ibm/domain-adaptive-attribution-robustness">https://github.com/ibm/domain-adaptive-attribution-robustness</a></li>
<li>paper_authors: Adam Ivankay, Mattia Rigotti, Pascal Frossard</li>
<li>for:  This paper aims to provide a better understanding of the robustness of deep neural network explanations in the biomedical domain.</li>
<li>methods:  The paper proposes a new approach called DomainAdaptiveAREstimator (DARE) to estimate the attribution robustness of explanations in the biomedical domain. DARE takes into account domain-specific plausibility to ensure that the explanations are both accurate and relevant to the domain experts.</li>
<li>results:  The paper presents two methods, adversarial training and FAR training, to mitigate the brittleness of explanations in the biomedical domain. The proposed methods are validated through extensive experiments on three established biomedical benchmarks.<details>
<summary>Abstract</summary>
Along with the successful deployment of deep neural networks in several application domains, the need to unravel the black-box nature of these networks has seen a significant increase recently. Several methods have been introduced to provide insight into the inference process of deep neural networks. However, most of these explainability methods have been shown to be brittle in the face of adversarial perturbations of their inputs in the image and generic textual domain. In this work we show that this phenomenon extends to specific and important high stakes domains like biomedical datasets. In particular, we observe that the robustness of explanations should be characterized in terms of the accuracy of the explanation in linking a model's inputs and its decisions - faithfulness - and its relevance from the perspective of domain experts - plausibility. This is crucial to prevent explanations that are inaccurate but still look convincing in the context of the domain at hand. To this end, we show how to adapt current attribution robustness estimation methods to a given domain, so as to take into account domain-specific plausibility. This results in our DomainAdaptiveAREstimator (DARE) attribution robustness estimator, allowing us to properly characterize the domain-specific robustness of faithful explanations. Next, we provide two methods, adversarial training and FAR training, to mitigate the brittleness characterized by DARE, allowing us to train networks that display robust attributions. Finally, we empirically validate our methods with extensive experiments on three established biomedical benchmarks.
</details>
<details>
<summary>摘要</summary>
alongside the successful deployment of deep neural networks in several application domains, the need to unravel the black-box nature of these networks has increased significantly recently. several methods have been introduced to provide insight into the inference process of deep neural networks. however, most of these explainability methods have been shown to be brittle in the face of adversarial perturbations of their inputs in the image and generic textual domain. in this work, we show that this phenomenon extends to specific and important high-stakes domains like biomedical datasets. in particular, we observe that the robustness of explanations should be characterized in terms of the accuracy of the explanation in linking a model's inputs and its decisions - faithfulness - and its relevance from the perspective of domain experts - plausibility. this is crucial to prevent explanations that are inaccurate but still look convincing in the context of the domain at hand. to this end, we show how to adapt current attribution robustness estimation methods to a given domain, so as to take into account domain-specific plausibility. this results in our domain-adaptive attribution robustness estimator (DARE) attribution robustness estimator, allowing us to properly characterize the domain-specific robustness of faithful explanations. next, we provide two methods, adversarial training and far training, to mitigate the brittleness characterized by DARE, allowing us to train networks that display robust attributions. finally, we empirically validate our methods with extensive experiments on three established biomedical benchmarks.
</details></li>
</ul>
<hr>
<h2 id="Make-A-Long-Image-Short-Adaptive-Token-Length-for-Vision-Transformers"><a href="#Make-A-Long-Image-Short-Adaptive-Token-Length-for-Vision-Transformers" class="headerlink" title="Make A Long Image Short: Adaptive Token Length for Vision Transformers"></a>Make A Long Image Short: Adaptive Token Length for Vision Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02092">http://arxiv.org/abs/2307.02092</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiqi Zhou, Yichen Zhu</li>
<li>for: 提高预测速度，减少计算成本</li>
<li>methods: 提出了一种适应测试时动态调整图像token长的方法，包括训练一个可变长度ViT模型和使用一个轻量级的Token长分配器（TLA）来分配最优的token长度</li>
<li>results: 实现了对多种现代视Transformer架构的减少计算成本，并在图像分类和动作识别任务上验证了方法的有效性<details>
<summary>Abstract</summary>
The vision transformer is a model that breaks down each image into a sequence of tokens with a fixed length and processes them similarly to words in natural language processing. Although increasing the number of tokens typically results in better performance, it also leads to a considerable increase in computational cost. Motivated by the saying "A picture is worth a thousand words," we propose an innovative approach to accelerate the ViT model by shortening long images. Specifically, we introduce a method for adaptively assigning token length for each image at test time to accelerate inference speed. First, we train a Resizable-ViT (ReViT) model capable of processing input with diverse token lengths. Next, we extract token-length labels from ReViT that indicate the minimum number of tokens required to achieve accurate predictions. We then use these labels to train a lightweight Token-Length Assigner (TLA) that allocates the optimal token length for each image during inference. The TLA enables ReViT to process images with the minimum sufficient number of tokens, reducing token numbers in the ViT model and improving inference speed. Our approach is general and compatible with modern vision transformer architectures, significantly reducing computational costs. We verified the effectiveness of our methods on multiple representative ViT models on image classification and action recognition.
</details>
<details>
<summary>摘要</summary>
“当代视觉转换器（ViT）模型将图像转换为一系列有 fix 长度的 токен，并对其进行语言处理的处理方式。虽然增加 токен 的数量通常会导致性能提高，但也会带来巨大的 Computational cost。为了解决这个问题，我们提出了一个创新的方法，即在评估时阶段适应地设定图像的 токен 长度。首先，我们训练了可以处理多种 токен 长度的 Resizable-ViT（ReViT）模型。接着，我们从 ReViT 中提取了 token-length 标签，这些标签指示了对于正确预测所需的最少的 токен 数量。我们然后使用这些标签进行训练一个轻量级的 Token-Length Assigner（TLA），这个 TLA 可以在评估过程中为每个图像分配最佳的 токен 长度。这个方法可以让 ReViT 在评估过程中对图像进行适当的处理，并且可以大幅降低 Computational cost。我们验证了我们的方法在多个代表性的 ViT 模型上进行图像分类和动作识别中的效果。”
</details></li>
</ul>
<hr>
<h2 id="Combating-Confirmation-Bias-A-Unified-Pseudo-Labeling-Framework-for-Entity-Alignment"><a href="#Combating-Confirmation-Bias-A-Unified-Pseudo-Labeling-Framework-for-Entity-Alignment" class="headerlink" title="Combating Confirmation Bias: A Unified Pseudo-Labeling Framework for Entity Alignment"></a>Combating Confirmation Bias: A Unified Pseudo-Labeling Framework for Entity Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02075">http://arxiv.org/abs/2307.02075</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qijie Ding, Jie Yin, Daokun Zhang, Junbin Gao</li>
<li>for: 提高实体对应性预测的准确率，抗衡假标签错误的影响</li>
<li>methods: 提出一种独特的 pseudo-labeling 框架（UPL-EA），通过精准的 Transport 模型和跨迭代 pseudo-标签准化来消除 pseudo-标签错误，提高实体对应性预测的准确率</li>
<li>results: 实验结果表明，我们的方法可以在有限的先前对应种子基础下达到竞争性的性能，并经过理论支持和实验验证，我们的方法可以减少 Type I 和 Type II pseudo-标签错误的影响<details>
<summary>Abstract</summary>
Entity alignment (EA) aims at identifying equivalent entity pairs across different knowledge graphs (KGs) that refer to the same real-world identity. To systematically combat confirmation bias for pseudo-labeling-based entity alignment, we propose a Unified Pseudo-Labeling framework for Entity Alignment (UPL-EA) that explicitly eliminates pseudo-labeling errors to boost the accuracy of entity alignment. UPL-EA consists of two complementary components: (1) The Optimal Transport (OT)-based pseudo-labeling uses discrete OT modeling as an effective means to enable more accurate determination of entity correspondences across two KGs and to mitigate the adverse impact of erroneous matches. A simple but highly effective criterion is further devised to derive pseudo-labeled entity pairs that satisfy one-to-one correspondences at each iteration. (2) The cross-iteration pseudo-label calibration operates across multiple consecutive iterations to further improve the pseudo-labeling precision rate by reducing the local pseudo-label selection variability with a theoretical guarantee. The two components are respectively designed to eliminate Type I and Type II pseudo-labeling errors identified through our analyse. The calibrated pseudo-labels are thereafter used to augment prior alignment seeds to reinforce subsequent model training for alignment inference. The effectiveness of UPL-EA in eliminating pseudo-labeling errors is both theoretically supported and experimentally validated. The experimental results show that our approach achieves competitive performance with limited prior alignment seeds.
</details>
<details>
<summary>摘要</summary>
Entity alignment (EA) 目标是在不同知识 graphs (KGs) 中标识同一个真实世界标识的等价实体对。为了系统地战胜假标注Error供 pseudo-labeling-based entity alignment，我们提出了一种Unified Pseudo-Labeling框架 дляEntity Alignment (UPL-EA)，该框架可以显著提高实体对应的准确率。UPL-EA包括两个补充部分：1. 基于Optimal Transport (OT)的 pseudo-labeling使用离散OT模型作为有效的方法来帮助更准确地确定两个KG中的实体对应关系，并 Mitigate the adverse impact of erroneous matches。我们还提出了一个简单 yet highly effective的标准来 derive pseudo-labeled entity pairs that satisfy one-to-one correspondences at each iteration。2. 跨迭代 pseudo-label calibration operated across multiple consecutive iterations to further improve the pseudo-labeling precision rate by reducing the local pseudo-label selection variability with a theoretical guarantee。这两个部分分别是为了消除Type I和Type II pseudo-labeling errors，这些错误被我们的分析所识别出。归一化后的 pseudo-labels 然后被用来增强后续模型训练中的对应性。我们的方法在 theoretically supported 和 experimentally validated 的情况下，可以减少 pseudo-labeling errors。实验结果显示，我们的方法在有限的先前对Alignment seeds的情况下可以达到竞争性的性能。
</details></li>
</ul>
<hr>
<h2 id="Performance-Modeling-of-Data-Storage-Systems-using-Generative-Models"><a href="#Performance-Modeling-of-Data-Storage-Systems-using-Generative-Models" class="headerlink" title="Performance Modeling of Data Storage Systems using Generative Models"></a>Performance Modeling of Data Storage Systems using Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02073">http://arxiv.org/abs/2307.02073</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/djdprogramming/adfa2">https://github.com/djdprogramming/adfa2</a></li>
<li>paper_authors: Abdalaziz Rashid Al-Maeeni, Aziz Temirkhanov, Artem Ryzhikov, Mikhail Hushchyn</li>
<li>for: 这 paper 是用于高精度模型系统的研究。</li>
<li>methods: 这 paper 使用机器学习基于生成模型来构建存储系统模型。</li>
<li>results: 实验结果显示该模型可以对系统性能做出高精度预测（IOPS 和响应时间），错误率在4-10%和3-16%之间，与 Little’s law 之间呈0.99Spearman 相似性。此外，文章还提供了可用于机器学习 regression 算法、条件生成模型和不确定性估计方法的新数据集。<details>
<summary>Abstract</summary>
High-precision modeling of systems is one of the main areas of industrial data analysis. Models of systems, their digital twins, are used to predict their behavior under various conditions. We have developed several models of a storage system using machine learning-based generative models. The system consists of several components: hard disk drive (HDD) and solid-state drive (SSD) storage pools with different RAID schemes and cache. Each storage component is represented by a probabilistic model that describes the probability distribution of the component performance in terms of IOPS and latency, depending on their configuration and external data load parameters. The results of the experiments demonstrate the errors of 4-10 % for IOPS and 3-16 % for latency predictions depending on the components and models of the system. The predictions show up to 0.99 Pearson correlation with Little's law, which can be used for unsupervised reliability checks of the models. In addition, we present novel data sets that can be used for benchmarking regression algorithms, conditional generative models, and uncertainty estimation methods in machine learning.
</details>
<details>
<summary>摘要</summary>
高精度模型化系统是工业数据分析的一个主要领域。系统的模型，也称为数字响应器，用于预测它们在不同条件下的行为。我们已经开发了一些基于机器学习的生成模型，用于模型存储系统。该系统包括多个组件：硬盘驱动器（HDD）和固态驱动器（SSD）存储池，以及不同的RAID方案和缓存。每个存储组件都是由一个概率模型来描述该组件性能的可能性分布，包括IOPS和延迟时间，它们取决于组件的配置和外部数据负荷参数。实验结果显示，预测错误率为4-10% для IOPS和3-16% для延迟时间，具体取决于组件和模型。这些预测还与李тт尔定律（Little's law）之间有0.99余 correlations，可以用于无监督可靠性检查。此外，我们还提供了一些新的数据集，可以用于机器学习 regression 算法、条件生成模型和不确定性估计方法的Benchmark。
</details></li>
</ul>
<hr>
<h2 id="A-Comparison-of-Machine-Learning-Methods-for-Data-with-High-Cardinality-Categorical-Variables"><a href="#A-Comparison-of-Machine-Learning-Methods-for-Data-with-High-Cardinality-Categorical-Variables" class="headerlink" title="A Comparison of Machine Learning Methods for Data with High-Cardinality Categorical Variables"></a>A Comparison of Machine Learning Methods for Data with High-Cardinality Categorical Variables</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02071">http://arxiv.org/abs/2307.02071</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fabsig/compare_ml_highcardinality_categorical_variables">https://github.com/fabsig/compare_ml_highcardinality_categorical_variables</a></li>
<li>paper_authors: Fabio Sigrist</li>
<li>for: 这篇论文主要研究高Cardinality categorical variables的机器学习模型。</li>
<li>methods: 论文使用了树融合和深度神经网络两种机器学习方法，以及线性混合效应模型。</li>
<li>results: 研究发现，机器学习模型带有随机效应的版本比 классиical版本更高的预测精度。此外，树融合带有随机效应的版本也比深度神经网络带有随机效应的版本更高的预测精度。<details>
<summary>Abstract</summary>
High-cardinality categorical variables are variables for which the number of different levels is large relative to the sample size of a data set, or in other words, there are few data points per level. Machine learning methods can have difficulties with high-cardinality variables. In this article, we empirically compare several versions of two of the most successful machine learning methods, tree-boosting and deep neural networks, and linear mixed effects models using multiple tabular data sets with high-cardinality categorical variables. We find that, first, machine learning models with random effects have higher prediction accuracy than their classical counterparts without random effects, and, second, tree-boosting with random effects outperforms deep neural networks with random effects.
</details>
<details>
<summary>摘要</summary>
高级别分类变量是指数据集中每个变量有许多不同的水平，与样本大小相比，这些变量的数量很大。机器学习方法可能会遇到困难处理高级别分类变量。本文employs empirical comparisons of several versions of two of the most successful machine learning methods, tree-boosting and deep neural networks, as well as linear mixed effects models using multiple tabular data sets with high-cardinality categorical variables. Our findings show that: first, machine learning models with random effects have higher prediction accuracy than their classical counterparts without random effects; second, tree-boosting with random effects outperforms deep neural networks with random effects.
</details></li>
</ul>
<hr>
<h2 id="Universal-Rates-for-Multiclass-Learning"><a href="#Universal-Rates-for-Multiclass-Learning" class="headerlink" title="Universal Rates for Multiclass Learning"></a>Universal Rates for Multiclass Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02066">http://arxiv.org/abs/2307.02066</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Machinfy/Human-Activity-Recognition-with-Smartphones">https://github.com/Machinfy/Human-Activity-Recognition-with-Smartphones</a></li>
<li>paper_authors: Steve Hanneke, Shay Moran, Qian Zhang</li>
<li>for: 这个论文是为了研究多类分类的普适率而写的。</li>
<li>methods: 这篇论文使用了 pseudo-cubes 和 DSL 树来研究多类分类的学习问题。</li>
<li>results: 这篇论文提出了一个普适率 bound，解决了 Kalavasis 等人（2022）对多类分类问题的开问。  Additionally, the paper shows that any class with an infinite Littlestone tree requires arbitrarily slow rates, while any class with a near-linear rate must have no infinite DSL tree.<details>
<summary>Abstract</summary>
We study universal rates for multiclass classification, establishing the optimal rates (up to log factors) for all hypothesis classes. This generalizes previous results on binary classification (Bousquet, Hanneke, Moran, van Handel, and Yehudayoff, 2021), and resolves an open question studied by Kalavasis, Velegkas, and Karbasi (2022) who handled the multiclass setting with a bounded number of class labels. In contrast, our result applies for any countable label space. Even for finite label space, our proofs provide a more precise bounds on the learning curves, as they do not depend on the number of labels. Specifically, we show that any class admits exponential rates if and only if it has no infinite Littlestone tree, and admits (near-)linear rates if and only if it has no infinite Daniely-Shalev-Shwartz-Littleston (DSL) tree, and otherwise requires arbitrarily slow rates. DSL trees are a new structure we define in this work, in which each node of the tree is given by a pseudo-cube of possible classifications of a given set of points. Pseudo-cubes are a structure, rooted in the work of Daniely and Shalev-Shwartz (2014), and recently shown by Brukhim, Carmon, Dinur, Moran, and Yehudayoff (2022) to characterize PAC learnability (i.e., uniform rates) for multiclass classification. We also resolve an open question of Kalavasis, Velegkas, and Karbasi (2022) regarding the equivalence of classes having infinite Graph-Littlestone (GL) trees versus infinite Natarajan-Littlestone (NL) trees, showing that they are indeed equivalent.
</details>
<details>
<summary>摘要</summary>
我们研究了 universality 的率数，确定了所有假设集合中的优化率（几乎Log因子）。这个结果总结了过去关于二分类（Bousquet, Hanneke, Moran, van Handel, 和 Yehudayoff, 2021）的研究，并解决了 Kalavasis, Velegkas, 和 Karbasi (2022) 处理多类标签的问题，他们只处理了具有bounded数量的类标签的情况。相比之下，我们的结果适用于任何可 COUNTABLE 标签空间。即使是Finite 标签空间，我们的证明还提供了更精确的学习曲线，因为它们不dependent于标签数量。我们证明，任何一个类别都可以实现指数率，如果和只有无限Littlestone树，而不是有限GL树。GL树是一种我们在这个工作中定义的新结构，每个节点都是一个可能的多个分类的pseudo-cube。pseudo-cubes是Daniely 和 Shalev-Shwartz (2014) 的工作中的一种结构，而且在 Brukhim, Carmon, Dinur, Moran, 和 Yehudayoff (2022) 的研究中被证明可以 caracterize PAC 学习（即uniform rates）多类标签分类。我们还解决了 Kalavasis, Velegkas, 和 Karbasi (2022) 关于无限GL树与无限NL树之间的等价性问题，证明它们确实是等价的。
</details></li>
</ul>
<hr>
<h2 id="Line-Graphics-Digitization-A-Step-Towards-Full-Automation"><a href="#Line-Graphics-Digitization-A-Step-Towards-Full-Automation" class="headerlink" title="Line Graphics Digitization: A Step Towards Full Automation"></a>Line Graphics Digitization: A Step Towards Full Automation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02065">http://arxiv.org/abs/2307.02065</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/moured/document-graphics-digitization">https://github.com/moured/document-graphics-digitization</a></li>
<li>paper_authors: Omar Moured, Jiaming Zhang, Alina Roitberg, Thorsten Schwarz, Rainer Stiefelhagen</li>
<li>for: 本研究旨在提高数字化文档的可访问性和可重现性，特别是对于数据统计图表的自动化涂抹和文本内容的研究已经是长期的焦点。</li>
<li>methods: 本文引入了细致的数学图表视觉理解任务，并提供了Line Graphics（LG）数据集，包括5种粗细类别的像素级别注解。我们的数据集包括450份来自不同领域的文档，共520张数据图像。</li>
<li>results: 我们在7种当前顶峰模型中测试了LG数据集，并发现这些模型在数据统计图表的Semantic Segmentation和Object Detection任务中的表现。为了推动数据统计图表的数字化进程，我们将会在社区内分享数据集、代码和模型。<details>
<summary>Abstract</summary>
The digitization of documents allows for wider accessibility and reproducibility. While automatic digitization of document layout and text content has been a long-standing focus of research, this problem in regard to graphical elements, such as statistical plots, has been under-explored. In this paper, we introduce the task of fine-grained visual understanding of mathematical graphics and present the Line Graphics (LG) dataset, which includes pixel-wise annotations of 5 coarse and 10 fine-grained categories. Our dataset covers 520 images of mathematical graphics collected from 450 documents from different disciplines. Our proposed dataset can support two different computer vision tasks, i.e., semantic segmentation and object detection. To benchmark our LG dataset, we explore 7 state-of-the-art models. To foster further research on the digitization of statistical graphs, we will make the dataset, code, and models publicly available to the community.
</details>
<details>
<summary>摘要</summary>
digitization of documents allow for wider accessibility and reproducibility。although automatic digitization of document layout and text content has been a long-standing focus of research，this problem in regard to graphical elements，such as statistical plots，has been under-explored。in this paper，we introduce the task of fine-grained visual understanding of mathematical graphics and present the Line Graphics (LG) dataset，which includes pixel-wise annotations of 5 coarse and 10 fine-grained categories。our dataset covers 520 images of mathematical graphics collected from 450 documents from different disciplines。our proposed dataset can support two different computer vision tasks，i.e., semantic segmentation and object detection。to benchmark our LG dataset，we explore 7 state-of-the-art models。to foster further research on the digitization of statistical graphs，we will make the dataset，code，and models publicly available to the community。Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Facing-off-World-Model-Backbones-RNNs-Transformers-and-S4"><a href="#Facing-off-World-Model-Backbones-RNNs-Transformers-and-S4" class="headerlink" title="Facing off World Model Backbones: RNNs, Transformers, and S4"></a>Facing off World Model Backbones: RNNs, Transformers, and S4</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02064">http://arxiv.org/abs/2307.02064</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fei Deng, Junyeong Park, Sungjin Ahn</li>
<li>for: 提高模型基于学习 reinforcement learning（MBRL）代理的能力，增强代理的长期记忆。</li>
<li>methods:  explore alternative world model backbones，包括Transformers和Structured State Space Sequence（S4）模型，以提高长期记忆。</li>
<li>results: S4WM表现出优于Transformer-based world models的长期记忆能力，同时具有更高的训练效率和想象能力。这些结果铺开了开发更强的MBRL代理的道路。<details>
<summary>Abstract</summary>
World models are a fundamental component in model-based reinforcement learning (MBRL) agents. To perform temporally extended and consistent simulations of the future in partially observable environments, world models need to possess long-term memory. However, state-of-the-art MBRL agents, such as Dreamer, predominantly employ recurrent neural networks (RNNs) as their world model backbone, which have limited memory capacity. In this paper, we seek to explore alternative world model backbones for improving long-term memory. In particular, we investigate the effectiveness of Transformers and Structured State Space Sequence (S4) models, motivated by their remarkable ability to capture long-range dependencies in low-dimensional sequences and their complementary strengths. We propose S4WM, the first S4-based world model that can generate high-dimensional image sequences through latent imagination. Furthermore, we extensively compare RNN-, Transformer-, and S4-based world models across four sets of environments, which we have specifically tailored to assess crucial memory capabilities of world models, including long-term imagination, context-dependent recall, reward prediction, and memory-based reasoning. Our findings demonstrate that S4WM outperforms Transformer-based world models in terms of long-term memory, while exhibiting greater efficiency during training and imagination. These results pave the way for the development of stronger MBRL agents.
</details>
<details>
<summary>摘要</summary>
世界模型是模型基 Reinforcement learning（MBRL）代理的重要组成部分。为在部分可见环境中进行持续时间扩展和一致的模拟未来，世界模型需要拥有长期记忆。然而，当前的MBRL代理，如梦幻，主要采用回归神经网络（RNN）作为世界模型脊梁，它们具有有限的记忆容量。在这篇论文中，我们寻找了替代的世界模型脊梁，以提高长期记忆。具体来说，我们调查了转换器和结构化状态空间序列（S4）模型，这些模型具有长距离依赖关系的捕捉能力和相互补偿的优势。我们提出了S4WM，首个基于S4模型的世界模型，可以通过幻想生成高维图像序列。此外，我们对RNN-, Transformer-, 和 S4-基于世界模型进行了广泛比较，并在我们专门为评估世界模型的重要记忆能力而设计的四组环境中进行了测试。我们的结果表明，S4WM在长期记忆方面比转换器基本世界模型高效，同时在训练和幻想过程中更高效。这些成果为开发更强的MBRL代理铺平道。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Attacks-on-Image-Classification-Models-FGSM-and-Patch-Attacks-and-their-Impact"><a href="#Adversarial-Attacks-on-Image-Classification-Models-FGSM-and-Patch-Attacks-and-their-Impact" class="headerlink" title="Adversarial Attacks on Image Classification Models: FGSM and Patch Attacks and their Impact"></a>Adversarial Attacks on Image Classification Models: FGSM and Patch Attacks and their Impact</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02055">http://arxiv.org/abs/2307.02055</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jaydip Sen, Subhasis Dasgupta</li>
<li>for: 本文介绍了对图像分类模型使用 adversarial 攻击的概念。</li>
<li>methods: 本文讨论了两种常见的 adversarial 攻击方法，即快速梯度签名法 (FGSM) 和 adversarial 贴图攻击。</li>
<li>results: 对三种强大预训练的图像分类模型（ResNet-34、GoogleNet、DenseNet-161）进行了攻击性评估，并计算了图像分类任务中模型在攻击和不攻击情况下的分类精度。<details>
<summary>Abstract</summary>
This chapter introduces the concept of adversarial attacks on image classification models built on convolutional neural networks (CNN). CNNs are very popular deep-learning models which are used in image classification tasks. However, very powerful and pre-trained CNN models working very accurately on image datasets for image classification tasks may perform disastrously when the networks are under adversarial attacks. In this work, two very well-known adversarial attacks are discussed and their impact on the performance of image classifiers is analyzed. These two adversarial attacks are the fast gradient sign method (FGSM) and adversarial patch attack. These attacks are launched on three powerful pre-trained image classifier architectures, ResNet-34, GoogleNet, and DenseNet-161. The classification accuracy of the models in the absence and presence of the two attacks are computed on images from the publicly accessible ImageNet dataset. The results are analyzed to evaluate the impact of the attacks on the image classification task.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Graph-Neural-Network-based-Power-Flow-Model"><a href="#Graph-Neural-Network-based-Power-Flow-Model" class="headerlink" title="Graph Neural Network-based Power Flow Model"></a>Graph Neural Network-based Power Flow Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02049">http://arxiv.org/abs/2307.02049</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingjian Tuo, Xingpeng Li, Tianxia Zhao</li>
<li>for: 这篇论文的目的是提出一种基于图神经网络（GNN）的电力流计算模型，以提高电力系统中线流计算的准确性和效率。</li>
<li>methods: 该模型使用历史电力系统数据进行训练，并使用图神经网络（GNN）模型来预测电力流结果。</li>
<li>results: 对比于传统的直流电力流计算模型和深度神经网络（DNN）、卷积神经网络（CNN）模型，该GNN模型能够提供更准确的解决方案，并且高效。<details>
<summary>Abstract</summary>
Power flow analysis plays a crucial role in examining the electricity flow within a power system network. By performing power flow calculations, the system's steady-state variables, including voltage magnitude, phase angle at each bus, active/reactive power flow across branches, can be determined. While the widely used DC power flow model offers speed and robustness, it may yield inaccurate line flow results for certain transmission lines. This issue becomes more critical when dealing with renewable energy sources such as wind farms, which are often located far from the main grid. Obtaining precise line flow results for these critical lines is vital for next operations. To address these challenges, data-driven approaches leverage historical grid profiles. In this paper, a graph neural network (GNN) model is trained using historical power system data to predict power flow outcomes. The GNN model enables rapid estimation of line flows. A comprehensive performance analysis is conducted, comparing the proposed GNN-based power flow model with the traditional DC power flow model, as well as deep neural network (DNN) and convolutional neural network (CNN). The results on test systems demonstrate that the proposed GNN-based power flow model provides more accurate solutions with high efficiency comparing to benchmark models.
</details>
<details>
<summary>摘要</summary>
电流流分析在电力系统网络中扮演着关键的角色，可以确定电力系统的稳定状态变量，包括每个总机的相位角和电压大小。虽然广泛使用的直流电流模型具有速度和可靠性，但可能导致certain transmission lines的流量结果不准确。这个问题在处理可再生能源such as wind farms时变得更加重要，这些可再生能源往往位于主网络远离的地方。为了解决这些挑战，数据驱动方法可以利用历史电力系统数据来预测电流流的结果。在这篇论文中，一种基于图神经网络（GNN）模型被训练使用历史电力系统数据来预测电流流的结果。GNN模型可以快速估算线流。我们进行了全面的性能分析，比较了提议的GNN-based电流流模型与传统的直流电流模型、深度神经网络（DNN）和卷积神经网络（CNN）模型。测试系统上的结果表明，提议的GNN-based电流流模型可以提供更加准确的解决方案，并且高效性比benchmark模型更高。
</details></li>
</ul>
<hr>
<h2 id="Fisher-Weighted-Merge-of-Contrastive-Learning-Models-in-Sequential-Recommendation"><a href="#Fisher-Weighted-Merge-of-Contrastive-Learning-Models-in-Sequential-Recommendation" class="headerlink" title="Fisher-Weighted Merge of Contrastive Learning Models in Sequential Recommendation"></a>Fisher-Weighted Merge of Contrastive Learning Models in Sequential Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05476">http://arxiv.org/abs/2307.05476</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jung Hyun Ryu, Jaeheyoung Jeon, Jewoong Cho, Myungjoo Kang 1</li>
<li>for: 这篇论文主要针对推荐系统中的次序推荐问题，即为用户随时间的偏好进行推荐。</li>
<li>methods: 本论文使用了对照学习方法，将多个模型的参数融合，以提高推荐系统的总性能。</li>
<li>results: 经过广泛的实验，本论文显示出该方法的效果，并证明其能够提高次序推荐系统的状态前进。<details>
<summary>Abstract</summary>
Along with the exponential growth of online platforms and services, recommendation systems have become essential for identifying relevant items based on user preferences. The domain of sequential recommendation aims to capture evolving user preferences over time. To address dynamic preference, various contrastive learning methods have been proposed to target data sparsity, a challenge in recommendation systems due to the limited user-item interactions. In this paper, we are the first to apply the Fisher-Merging method to Sequential Recommendation, addressing and resolving practical challenges associated with it. This approach ensures robust fine-tuning by merging the parameters of multiple models, resulting in improved overall performance. Through extensive experiments, we demonstrate the effectiveness of our proposed methods, highlighting their potential to advance the state-of-the-art in sequential learning and recommendation systems.
</details>
<details>
<summary>摘要</summary>
随着在线平台和服务的快速增长，推荐系统已成为用户喜好的标准工具。序列推荐的领域旨在捕捉用户的时间演变的偏好。为了解决动态偏好的挑战，多种对照学习方法已经被提议用于目标数据稀缺。在这篇论文中，我们是首次将施耐德-抽取方法应用于序列推荐，解决和解决实际挑战。这种方法确保了精度的练习调整，从而提高总性性能。通过广泛的实验，我们证明了我们的提议方法的效果， highlighting their potential to advance the state-of-the-art in sequential learning and recommendation systems.
</details></li>
</ul>
<hr>
<h2 id="VertiBench-Advancing-Feature-Distribution-Diversity-in-Vertical-Federated-Learning-Benchmarks"><a href="#VertiBench-Advancing-Feature-Distribution-Diversity-in-Vertical-Federated-Learning-Benchmarks" class="headerlink" title="VertiBench: Advancing Feature Distribution Diversity in Vertical Federated Learning Benchmarks"></a>VertiBench: Advancing Feature Distribution Diversity in Vertical Federated Learning Benchmarks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02040">http://arxiv.org/abs/2307.02040</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhaomin Wu, Junyi Hou, Bingsheng He</li>
<li>for: 本研究はVertical Federated Learning（VFL）の性能评価に适用される公共世界データセットの欠如に対処します。</li>
<li>methods: 本研究では、Feature importanceとFeature correlationの2つの键因子を考虑し、それぞれに対応する评価指标とデータセットの分割方法を提案します。</li>
<li>results: 本研究では、State-of-the-art VFLアルゴリズムの效果的な评価を提供し、Future researchの参考になる価値ある Insightsを提供します。<details>
<summary>Abstract</summary>
Vertical Federated Learning (VFL) is a crucial paradigm for training machine learning models on feature-partitioned, distributed data. However, due to privacy restrictions, few public real-world VFL datasets exist for algorithm evaluation, and these represent a limited array of feature distributions. Existing benchmarks often resort to synthetic datasets, derived from arbitrary feature splits from a global set, which only capture a subset of feature distributions, leading to inadequate algorithm performance assessment. This paper addresses these shortcomings by introducing two key factors affecting VFL performance - feature importance and feature correlation - and proposing associated evaluation metrics and dataset splitting methods. Additionally, we introduce a real VFL dataset to address the deficit in image-image VFL scenarios. Our comprehensive evaluation of cutting-edge VFL algorithms provides valuable insights for future research in the field.
</details>
<details>
<summary>摘要</summary>
纵向联合学习（VFL）是训练机器学习模型的重要方法，该方法在分布式数据上进行特征分区。然而由于隐私限制，公共世界中的VFL数据集很少，这些数据集只代表了有限的特征分布。现有的标准 benchmark 通常采用人工生成的数据集，这些数据集只反映了一部分特征分布，导致算法性能评估不准确。本文解决这些缺陷，通过介绍特征重要性和特征相关性两个关键因素，并提出相应的评价指标和数据分割方法。此外，我们还介绍了一个真实存在的VFL数据集，用于解决图像-图像VFL场景中的不足。我们对当前VFL领域最先进的算法进行了全面的评估，提供了valuable的情况参考。
</details></li>
</ul>
<hr>
<h2 id="Monte-Carlo-Sampling-without-Isoperimetry-A-Reverse-Diffusion-Approach"><a href="#Monte-Carlo-Sampling-without-Isoperimetry-A-Reverse-Diffusion-Approach" class="headerlink" title="Monte Carlo Sampling without Isoperimetry: A Reverse Diffusion Approach"></a>Monte Carlo Sampling without Isoperimetry: A Reverse Diffusion Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02037">http://arxiv.org/abs/2307.02037</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xunpeng Huang, Hanze Dong, Yifan Hao, Yian Ma, Tong Zhang</li>
<li>for: 本研究探讨了 posterior sampling 的可能性，它是通过反射扩散来实现高质量数据样本的生成模型的效能的一种方法。</li>
<li>methods: 本研究使用了分解过程kernel的技术，将 score estimation 转化为了一个mean estimation问题，从而实现了一种新的 posterior sampling 算法。</li>
<li>results: 我们提供了这种算法的收敛分析，并证明了其在高维样本中的性能比传统MCMC方法更高，这是因为该算法的auxiliary distribution的一些性质可以减少误差。<details>
<summary>Abstract</summary>
The efficacy of modern generative models is commonly contingent upon the precision of score estimation along the diffusion path, with a focus on diffusion models and their ability to generate high-quality data samples. This study delves into the potentialities of posterior sampling through reverse diffusion. An examination of the sampling literature reveals that score estimation can be transformed into a mean estimation problem via the decomposition of the transition kernel. By estimating the mean of the auxiliary distribution, the reverse diffusion process can give rise to a novel posterior sampling algorithm, which diverges from traditional gradient-based Markov Chain Monte Carlo (MCMC) methods. We provide the convergence analysis in total variation distance and demonstrate that the isoperimetric dependency of the proposed algorithm is comparatively lower than that observed in conventional MCMC techniques, which justifies the superior performance for high dimensional sampling with error tolerance. Our analytical framework offers fresh perspectives on the complexity of score estimation at various time points, as denoted by the properties of the auxiliary distribution.
</details>
<details>
<summary>摘要</summary>
现代生成模型的效果通常取决于扩散路径上的分数估计精度，尤其是扩散模型和它们能够生成高质量数据样本。这项研究探讨了反扩散 posterior 采样的可能性，通过将分数估计转换为auxiliary distribution的均值估计问题。通过估计auxiliary distribution的均值，反扩散过程可以生成一种新的 posterior 采样算法，与传统的梯度基本 Markov Chain Monte Carlo (MCMC) 方法不同。我们提供了整体变量距离的收敛分析，并证明了提案的算法的iso依赖关系比传统 MCMC 技术更低，这 justify了高维度采样中的高精度和误差容忍。我们的分析框架为 score estimation 的复杂性在不同时刻点提供了新的视角，即auxiliary distribution的属性。
</details></li>
</ul>
<hr>
<h2 id="Ranking-with-Abstention"><a href="#Ranking-with-Abstention" class="headerlink" title="Ranking with Abstention"></a>Ranking with Abstention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02035">http://arxiv.org/abs/2307.02035</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anqi Mao, Mehryar Mohri, Yutao Zhong</li>
<li>for: 这个论文提出了一种新的排名概念，即learner可以在一定成本$c$的情况下决定不预测。</li>
<li>methods: 这个论文使用了一种扩展的理论分析，包括线性函数家族和带有一个隐藏层的神经网络的$H$-一致性 bound。</li>
<li>results: 实验结果表明，这种排名方法在实际应用中具有效果。<details>
<summary>Abstract</summary>
We introduce a novel framework of ranking with abstention, where the learner can abstain from making prediction at some limited cost $c$. We present a extensive theoretical analysis of this framework including a series of $H$-consistency bounds for both the family of linear functions and that of neural networks with one hidden-layer. These theoretical guarantees are the state-of-the-art consistency guarantees in the literature, which are upper bounds on the target loss estimation error of a predictor in a hypothesis set $H$, expressed in terms of the surrogate loss estimation error of that predictor. We further argue that our proposed abstention methods are important when using common equicontinuous hypothesis sets in practice. We report the results of experiments illustrating the effectiveness of ranking with abstention.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种新的排名框架，其中学习者可以在某些有限成本$c$的情况下退出预测。我们提供了广泛的理论分析，包括线性函数家族和带一层隐藏层神经网络的$H$-一致性上下文。这些理论保证是文献中的最佳一致性保证，它们是指定损失函数集$H$中预测器的目标损失估计错误的Upper bound，表示了预测器在损失函数集$H$中的损失估计错误。我们还 argue что我们提议的投降方法在实际中使用公共等距inuous假设集时是重要的。我们报告了实验结果，证明了排名框架的效果。
</details></li>
</ul>
<hr>
<h2 id="Improving-Automatic-Parallel-Training-via-Balanced-Memory-Workload-Optimization"><a href="#Improving-Automatic-Parallel-Training-via-Balanced-Memory-Workload-Optimization" class="headerlink" title="Improving Automatic Parallel Training via Balanced Memory Workload Optimization"></a>Improving Automatic Parallel Training via Balanced Memory Workload Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02031">http://arxiv.org/abs/2307.02031</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yujie Wang, Youhe Jiang, Xupeng Miao, Fangcheng Fu, Xiaonan Nie, Bin Cui</li>
<li>for:  This paper aims to improve the efficiency of training Transformer models across multiple GPUs.</li>
<li>methods:  The paper proposes a novel system framework called Galvatron-BMW, which integrates multiple parallelism dimensions and automatically identifies the most efficient hybrid parallelism strategy using a decision tree approach and dynamic programming search algorithm.</li>
<li>results:  Galvatron-BMW consistently achieves superior system throughput in automating distributed training under varying GPU memory constraints, surpassing previous approaches that rely on limited parallelism strategies.Here is the text in Simplified Chinese:</li>
<li>for: 这篇论文目的是提高多个GPU上Transformer模型的训练效率。</li>
<li>methods: 该论文提出了一种新的系统框架called Galvatron-BMW，它集成了多种并发方向并自动确定最佳混合并发策略，使用决策树方法和动态规划搜索算法。</li>
<li>results: Galvatron-BMW在不同Transformer模型的测试场景中 consistently达到了自动化分布训练的最高系统吞吐量，超过了前一些仅仅采用有限并发策略的方法。<details>
<summary>Abstract</summary>
Transformer models have emerged as the leading approach for achieving state-of-the-art performance across various application domains, serving as the foundation for advanced large-scale deep learning (DL) models. However, efficiently training these models across multiple GPUs remains a complex challenge due to the abundance of parallelism options. Existing DL systems either require manual efforts to design distributed training plans or limit parallelism combinations to a constrained search space. In this paper, we present Galvatron-BMW, a novel system framework that integrates multiple prevalent parallelism dimensions and automatically identifies the most efficient hybrid parallelism strategy. To effectively navigate this vast search space, we employ a decision tree approach for decomposition and pruning based on intuitive insights. We further utilize a dynamic programming search algorithm to derive the optimal plan. Moreover, to improve resource utilization and enhance system efficiency, we propose a bi-objective optimization workflow that focuses on workload balance. Our evaluations on different Transformer models demonstrate the capabilities of Galvatron-BMW in automating distributed training under varying GPU memory constraints. Across all tested scenarios, Galvatron-BMW consistently achieves superior system throughput, surpassing previous approaches that rely on limited parallelism strategies.
</details>
<details>
<summary>摘要</summary>
<<SYS>> tranlate_text: transformer 模型已经成为实现不同应用领域的状态码模型的主流方法， serving 为高级大规模深度学习（DL）模型的基础。然而，在多个GPU上有效地训练这些模型仍然是一个复杂的挑战，因为存在丰富的并行性选择。现有的 DL 系统可以通过手动设计分布式训练计划或限制并行性组合的搜索空间。在这篇论文中，我们提出了 Galvatron-BMW 系统框架，该系统框架集成了多种流行的并行性维度，并自动确定最有效的混合并行性策略。为了有效地探索这个庞大的搜索空间，我们使用决策树方法进行分解和剔除，基于直观的理解。此外，我们还提出了一种动态搜索算法，以derive 最佳计划。此外，为了提高资源利用率和系统效率，我们提出了一种两个目标优化工作流程，该工作流程关注工作负荷均衡。我们对不同的 transformer 模型进行了不同的评估， demonstrates  Galvatron-BMW 在不同的 GPU 内存限制下自动化分布式训练的能力。在所有测试场景中，Galvatron-BMW  consistently  achieve  superior system throughput，超越了基于有限并行性策略的前一代方法。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="EHRSHOT-An-EHR-Benchmark-for-Few-Shot-Evaluation-of-Foundation-Models"><a href="#EHRSHOT-An-EHR-Benchmark-for-Few-Shot-Evaluation-of-Foundation-Models" class="headerlink" title="EHRSHOT: An EHR Benchmark for Few-Shot Evaluation of Foundation Models"></a>EHRSHOT: An EHR Benchmark for Few-Shot Evaluation of Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02028">http://arxiv.org/abs/2307.02028</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/som-shahlab/ehrshot-benchmark">https://github.com/som-shahlab/ehrshot-benchmark</a></li>
<li>paper_authors: Michael Wornow, Rahul Thapa, Ethan Steinberg, Jason Fries, Nigam Shah</li>
<li>for: 本研究的目的是提高医疗机器学习（ML）在医疗领域的进步，通过公共数据集、任务和模型的共享，但医疗领域的ML进步受到共享资产的限制。本研究通过三个贡献来解决这些挑战。</li>
<li>methods: 本研究使用了一个新的数据集，名为EHRSHOT，这是医疗记录电子档案（EHR）中的6,712名患者的去identify的结构化数据。与MIMIC-III&#x2F;IV和其他流行的EHR数据集不同，EHRSHOT是长期跟踪的，而不是仅仅是ICU&#x2F;ED patients的数据。此外，本研究还公布了一个141M参数的临床基础模型，这是一个可以处理coded EHR数据的完整模型，而不是只能处理不结构化文本的模型。</li>
<li>results: 本研究定义了15个几个shot临床预测任务，使得可以评估基础模型的样本效率和任务适应性。同时，研究者们还提供了一个可重现结果的代码，以及模型和数据集（通过研究数据使用协议获取）。<details>
<summary>Abstract</summary>
While the general machine learning (ML) community has benefited from public datasets, tasks, and models, the progress of ML in healthcare has been hampered by a lack of such shared assets. The success of foundation models creates new challenges for healthcare ML by requiring access to shared pretrained models to validate performance benefits. We help address these challenges through three contributions. First, we publish a new dataset, EHRSHOT, containing de-identified structured data from the electronic health records (EHRs) of 6,712 patients from Stanford Medicine. Unlike MIMIC-III/IV and other popular EHR datasets, EHRSHOT is longitudinal and not restricted to ICU/ED patients. Second, we publish the weights of a 141M parameter clinical foundation model pretrained on the structured EHR data of 2.57M patients. We are one of the first to fully release such a model for coded EHR data; in contrast, most prior models released for clinical data (e.g. GatorTron, ClinicalBERT) only work with unstructured text and cannot process the rich, structured data within an EHR. We provide an end-to-end pipeline for the community to validate and build upon its performance. Third, we define 15 few-shot clinical prediction tasks, enabling evaluation of foundation models on benefits such as sample efficiency and task adaption. The code to reproduce our results, as well as the model and dataset (via a research data use agreement), are available at our Github repo here: https://github.com/som-shahlab/ehrshot-benchmark
</details>
<details>
<summary>摘要</summary>
generale 机器学习（ML）社区得益于公共数据集、任务和模型，而医疗机器学习（ML）的进步却受到公共资产的缺乏所妨碍。成功的基本模型创造了新的挑战，需要访问共享预训练模型来验证性能 beneficiaries。我们通过以下三个贡献来解决这些挑战：1. 我们发布了一个新的数据集，EHRSHOT，包含了医疗电子病历（EHR）中6,712名患者的去掉个人信息的结构化数据。与MIMIC-III/IV和其他流行的EHR数据集不同，EHRSHOT是 longitudinal 的，而不是仅仅是ICU/ED patients。2. 我们发布了一个141M参数的临床基础模型，预训练于EHR数据中的结构化数据中的2.57M名患者。我们是一个 Among the first to fully release such a model for coded EHR data; in contrast, most prior models released for clinical data (e.g. GatorTron, ClinicalBERT) only work with unstructured text and cannot process the rich, structured data within an EHR。我们提供了一个端到端的管道，让社区可以验证和基于其性能。3. 我们定义了15个几个shot的临床预测任务，使得基础模型的性能在样本效率和任务适应方面进行评估。我们的代码、模型和数据集（通过研究数据用途协议获取）都可以在我们 GitHub 仓库中找到：https://github.com/som-shahlab/ehrshot-benchmark。
</details></li>
</ul>
<hr>
<h2 id="Using-Random-Effects-Machine-Learning-Algorithms-to-Identify-Vulnerability-to-Depression"><a href="#Using-Random-Effects-Machine-Learning-Algorithms-to-Identify-Vulnerability-to-Depression" class="headerlink" title="Using Random Effects Machine Learning Algorithms to Identify Vulnerability to Depression"></a>Using Random Effects Machine Learning Algorithms to Identify Vulnerability to Depression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02023">http://arxiv.org/abs/2307.02023</a></li>
<li>repo_url: None</li>
<li>paper_authors: Runa Bhaumik, Jonathan Stange</li>
<li>for: 预测青年成年人抑郁症状的诊断和预后 прогнозинг</li>
<li>methods: 使用数据驱动的机器学习方法（RE-EM树和MERF）对抑郁风险因素进行分类和识别</li>
<li>results: 结果表明，RE-EM树和MERF方法可以准确地预测青年成年人抑郁症状，并且可以确定抑郁风险因素的复杂相互作用，以及哪些因素对于预后预测最有用。<details>
<summary>Abstract</summary>
Background: Reliable prediction of clinical progression over time can improve the outcomes of depression. Little work has been done integrating various risk factors for depression, to determine the combinations of factors with the greatest utility for identifying which individuals are at the greatest risk. Method: This study demonstrates that data-driven machine learning (ML) methods such as RE-EM (Random Effects/Expectation Maximization) trees and MERF (Mixed Effects Random Forest) can be applied to reliably identify variables that have the greatest utility for classifying subgroups at greatest risk for depression. 185 young adults completed measures of depression risk, including rumination, worry, negative cognitive styles, cognitive and coping flexibilities, and negative life events, along with symptoms of depression. We trained RE-EM trees and MERF algorithms and compared them to traditional linear mixed models (LMMs) predicting depressive symptoms prospectively and concurrently with cross-validation. Results: Our results indicated that the RE-EM tree and MERF methods model complex interactions, identify subgroups of individuals and predict depression severity comparable to LMM. Further, machine learning models determined that brooding, negative life events, negative cognitive styles, and perceived control were the most relevant predictors of future depression levels. Conclusions: Random effects machine learning models have the potential for high clinical utility and can be leveraged for interventions to reduce vulnerability to depression.
</details>
<details>
<summary>摘要</summary>
背景：可靠预测临床进程的发展可以提高抑郁症的结果。然而，有少量的研究把不同的风险因素 integrate 以确定最有用的组合因素，以确定患有抑郁症的个人是否处于最高风险。方法：本研究表明，数据驱动的机器学习（ML）方法，如RE-EM（随机效应/期望最大化）树和MERF（混合效应随机森林）可以可靠地识别出抑郁症风险的最有用变量。185名年轻成人完成了抑郁风险的测量，包括催眠、担忧、消极思维、认知和处理的灵活性、以及负面生活事件，同时测量抑郁症的 симптом。我们训练了RE-EM树和MERF算法，并与传统的线性混合模型（LMM）相比，预测抑郁症的严重程度。结果：我们的结果表明，RE-EM树和MERF方法可以模型复杂的交互，分类个人为不同的子组合，并且预测抑郁症的严重程度与LMM相当。此外，机器学习模型确定了催眠、负面生活事件、消极思维和感知控制是抑郁症的最有用预测因素。结论：Random effects机器学习模型具有高临床实用性，可以用于降低抑郁症的抵触性。
</details></li>
</ul>
<hr>
<h2 id="Modular-DFR-Digital-Delayed-Feedback-Reservoir-Model-for-Enhancing-Design-Flexibility"><a href="#Modular-DFR-Digital-Delayed-Feedback-Reservoir-Model-for-Enhancing-Design-Flexibility" class="headerlink" title="Modular DFR: Digital Delayed Feedback Reservoir Model for Enhancing Design Flexibility"></a>Modular DFR: Digital Delayed Feedback Reservoir Model for Enhancing Design Flexibility</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11094">http://arxiv.org/abs/2307.11094</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sosei Ikeda, Hiromitsu Awano, Takashi Sato</li>
<li>for: 这个论文主要是为了提出一种全数字式延迟反馈水库系统（DFR），以便在硬件实现中使用。</li>
<li>methods: 该论文提出了一种新的模块化DFR模型，该模型可以完全在数字domain中实现，并且可以采用不同的非线性函数进行选择，从而提高准确性而减少功耗。</li>
<li>results: 该论文通过两种不同的非线性函数实现DFR，实现了功耗降低10倍和吞吐量提高5.3倍，而保持相同或更好的准确性。<details>
<summary>Abstract</summary>
A delayed feedback reservoir (DFR) is a type of reservoir computing system well-suited for hardware implementations owing to its simple structure. Most existing DFR implementations use analog circuits that require both digital-to-analog and analog-to-digital converters for interfacing. However, digital DFRs emulate analog nonlinear components in the digital domain, resulting in a lack of design flexibility and higher power consumption. In this paper, we propose a novel modular DFR model that is suitable for fully digital implementations. The proposed model reduces the number of hyperparameters and allows flexibility in the selection of the nonlinear function, which improves the accuracy while reducing the power consumption. We further present two DFR realizations with different nonlinear functions, achieving 10x power reduction and 5.3x throughput improvement while maintaining equal or better accuracy.
</details>
<details>
<summary>摘要</summary>
一种延迟反馈蓄水池（DFR）是一种适合硬件实现的计算系统，具有简单的结构。现有大多数 DFR 实现使用分析电路，需要数字到分析和分析到数字转换器进行交互。然而，数字 DFR 模拟分析电路在数字领域，导致设计灵活性偏低和功耗更高。在本文中，我们提出一种新的模块化 DFR 模型，适合完全数字实现。我们的提案减少了多个超参数，并允许非线性函数的选择，从而提高了准确性，同时降低了功耗。我们进一步采用了两种不同的非线性函数来实现 DFR，实现了功耗降低10倍和通过putthrough提高5.3倍，保持或更好的准确性。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-the-Effectiveness-of-Large-Language-Models-in-Representing-Textual-Descriptions-of-Geometry-and-Spatial-Relations"><a href="#Evaluating-the-Effectiveness-of-Large-Language-Models-in-Representing-Textual-Descriptions-of-Geometry-and-Spatial-Relations" class="headerlink" title="Evaluating the Effectiveness of Large Language Models in Representing Textual Descriptions of Geometry and Spatial Relations"></a>Evaluating the Effectiveness of Large Language Models in Representing Textual Descriptions of Geometry and Spatial Relations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03678">http://arxiv.org/abs/2307.03678</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuhan Ji, Song Gao</li>
<li>for: 评估大语言模型（LLMs）在表示几何和其空间关系方面的能力。</li>
<li>methods: 使用GPT-2和BERT等大语言模型将文本（WKT）格式的几何编码并feed其 embeddings 到分类器和回归器进行评估效果。</li>
<li>results: LLMs-生成的embeddings可以保持几何类型和捕捉一定的空间关系（准确率达73%），但还存在估算数值和检索空间相关对象的挑战。<details>
<summary>Abstract</summary>
This research focuses on assessing the ability of large language models (LLMs) in representing geometries and their spatial relations. We utilize LLMs including GPT-2 and BERT to encode the well-known text (WKT) format of geometries and then feed their embeddings into classifiers and regressors to evaluate the effectiveness of the LLMs-generated embeddings for geometric attributes. The experiments demonstrate that while the LLMs-generated embeddings can preserve geometry types and capture some spatial relations (up to 73% accuracy), challenges remain in estimating numeric values and retrieving spatially related objects. This research highlights the need for improvement in terms of capturing the nuances and complexities of the underlying geospatial data and integrating domain knowledge to support various GeoAI applications using foundation models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="STS-CCL-Spatial-Temporal-Synchronous-Contextual-Contrastive-Learning-for-Urban-Traffic-Forecasting"><a href="#STS-CCL-Spatial-Temporal-Synchronous-Contextual-Contrastive-Learning-for-Urban-Traffic-Forecasting" class="headerlink" title="STS-CCL: Spatial-Temporal Synchronous Contextual Contrastive Learning for Urban Traffic Forecasting"></a>STS-CCL: Spatial-Temporal Synchronous Contextual Contrastive Learning for Urban Traffic Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02507">http://arxiv.org/abs/2307.02507</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lincan Li, Kaixiang Yang, Fengji Luo, Jichao Bi</li>
<li>for: 这个研究目的是为了提高大规模未标注交通数据中的复杂空间时间表现，以及对于其他缺乏数据的跨空间任务。</li>
<li>methods: 这篇论文使用了进步的对照学习和一个新的空间时间同步Contextual Contrastive Learning（STS-CCL）模型，包括对于空间时间граф数据的基本和强化增强方法，以及一个空间时间同步对照模组（STS-CM），以同时捕捉出Decent空间时间依赖关系。</li>
<li>results: 实验和评估结果显示，使用STS-CCL模型建立预测器可以对交通预测 benchmark 进行超越性的表现，并且适合具有缺乏数据的大规模跨空间任务。<details>
<summary>Abstract</summary>
Efficiently capturing the complex spatiotemporal representations from large-scale unlabeled traffic data remains to be a challenging task. In considering of the dilemma, this work employs the advanced contrastive learning and proposes a novel Spatial-Temporal Synchronous Contextual Contrastive Learning (STS-CCL) model. First, we elaborate the basic and strong augmentation methods for spatiotemporal graph data, which not only perturb the data in terms of graph structure and temporal characteristics, but also employ a learning-based dynamic graph view generator for adaptive augmentation. Second, we introduce a Spatial-Temporal Synchronous Contrastive Module (STS-CM) to simultaneously capture the decent spatial-temporal dependencies and realize graph-level contrasting. To further discriminate node individuals in negative filtering, a Semantic Contextual Contrastive method is designed based on semantic features and spatial heterogeneity, achieving node-level contrastive learning along with negative filtering. Finally, we present a hard mutual-view contrastive training scheme and extend the classic contrastive loss to an integrated objective function, yielding better performance. Extensive experiments and evaluations demonstrate that building a predictor upon STS-CCL contrastive learning model gains superior performance than existing traffic forecasting benchmarks. The proposed STS-CCL is highly suitable for large datasets with only a few labeled data and other spatiotemporal tasks with data scarcity issue.
</details>
<details>
<summary>摘要</summary>
efficiently capturing the complex spatiotemporal representations from large-scale unlabeled traffic data remains a challenging task. to address this challenge, this work employs advanced contrastive learning and proposes a novel spatial-temporal synchronous contextual contrastive learning (STS-CCL) model. first, we elaborate on the basic and strong augmentation methods for spatiotemporal graph data, which not only perturb the data in terms of graph structure and temporal characteristics, but also employ a learning-based dynamic graph view generator for adaptive augmentation. second, we introduce a spatial-temporal synchronous contrastive module (STS-CM) to simultaneously capture the decent spatial-temporal dependencies and realize graph-level contrasting. to further discriminate node individuals in negative filtering, a semantic contextual contrastive method is designed based on semantic features and spatial heterogeneity, achieving node-level contrastive learning along with negative filtering. finally, we present a hard mutual-view contrastive training scheme and extend the classic contrastive loss to an integrated objective function, yielding better performance. extensive experiments and evaluations demonstrate that building a predictor upon STS-CCL contrastive learning model gains superior performance than existing traffic forecasting benchmarks. the proposed STS-CCL is highly suitable for large datasets with only a few labeled data and other spatiotemporal tasks with data scarcity issue.
</details></li>
</ul>
<hr>
<h2 id="Distilling-Missing-Modality-Knowledge-from-Ultrasound-for-Endometriosis-Diagnosis-with-Magnetic-Resonance-Images"><a href="#Distilling-Missing-Modality-Knowledge-from-Ultrasound-for-Endometriosis-Diagnosis-with-Magnetic-Resonance-Images" class="headerlink" title="Distilling Missing Modality Knowledge from Ultrasound for Endometriosis Diagnosis with Magnetic Resonance Images"></a>Distilling Missing Modality Knowledge from Ultrasound for Endometriosis Diagnosis with Magnetic Resonance Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02000">http://arxiv.org/abs/2307.02000</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuan Zhang, Hu Wang, David Butler, Minh-Son To, Jodie Avery, M Louise Hull, Gustavo Carneiro</li>
<li>for: 提高 Magnetic Resonance Imaging (MRI) 图像中镜像腔腔积极膜（POD）探测精度，使用知识汇抽法。</li>
<li>methods: 利用不同数据集的 teacher 模型和学生模型，通过知识汇抽法进行训练，提高学生模型对 MRI 图像中 POD 探测的精度。</li>
<li>results: 实验结果表明，使用我们提出的方法可以提高 MRI 图像中 POD 探测的精度。<details>
<summary>Abstract</summary>
Endometriosis is a common chronic gynecological disorder that has many characteristics, including the pouch of Douglas (POD) obliteration, which can be diagnosed using Transvaginal gynecological ultrasound (TVUS) scans and magnetic resonance imaging (MRI). TVUS and MRI are complementary non-invasive endometriosis diagnosis imaging techniques, but patients are usually not scanned using both modalities and, it is generally more challenging to detect POD obliteration from MRI than TVUS. To mitigate this classification imbalance, we propose in this paper a knowledge distillation training algorithm to improve the POD obliteration detection from MRI by leveraging the detection results from unpaired TVUS data. More specifically, our algorithm pre-trains a teacher model to detect POD obliteration from TVUS data, and it also pre-trains a student model with 3D masked auto-encoder using a large amount of unlabelled pelvic 3D MRI volumes. Next, we distill the knowledge from the teacher TVUS POD obliteration detector to train the student MRI model by minimizing a regression loss that approximates the output of the student to the teacher using unpaired TVUS and MRI data. Experimental results on our endometriosis dataset containing TVUS and MRI data demonstrate the effectiveness of our method to improve the POD detection accuracy from MRI.
</details>
<details>
<summary>摘要</summary>
具体来说，我们的算法首先在 TVUS 数据上训练一个教师模型，用于检测 PODS 消失。然后，我们将这个教师模型与一个学生模型相结合，使用大量的未标注 pelvic 3D MRI 数据进行训练。接着，我们将教师模型中的知识传授给学生模型，使其通过对不同的 TVUS 和 MRI 数据进行无标注对应的损失函数来学习。实验结果表明，我们的方法可以提高 MRI 上 PODS 的检测精度。
</details></li>
</ul>
<hr>
<h2 id="Zero-Shot-Neural-Architecture-Search-Challenges-Solutions-and-Opportunities"><a href="#Zero-Shot-Neural-Architecture-Search-Challenges-Solutions-and-Opportunities" class="headerlink" title="Zero-Shot Neural Architecture Search: Challenges, Solutions, and Opportunities"></a>Zero-Shot Neural Architecture Search: Challenges, Solutions, and Opportunities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01998">http://arxiv.org/abs/2307.01998</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sldgroup/survey-zero-shot-nas">https://github.com/sldgroup/survey-zero-shot-nas</a></li>
<li>paper_authors: Guihong Li, Duc Hoang, Kartikeya Bhardwaj, Ming Lin, Zhangyang Wang, Radu Marculescu</li>
<li>for: 本文旨在审视和比较当前最佳实践（SOTA）的零shot Neural Architecture Search（NAS）方法，强调它们在硬件上的意识。</li>
<li>methods: 本文首先介绍主流的零shot proxy，并讲解它们的理论基础。然后通过大规模的实验比较这些零shot proxy，并在硬件意识和硬件感知 NAS 场景中证明其效果。</li>
<li>results: 本文的实验结果表明，零shot NAS 方法在硬件意识和硬件感知场景中具有极高的效果，并且可以在不同的硬件背景下进行可靠的 NAS。此外，本文还提出了一些可能更好的 proxy 设计的想法。<details>
<summary>Abstract</summary>
Recently, zero-shot (or training-free) Neural Architecture Search (NAS) approaches have been proposed to liberate the NAS from training requirements. The key idea behind zero-shot NAS approaches is to design proxies that predict the accuracies of the given networks without training network parameters. The proxies proposed so far are usually inspired by recent progress in theoretical deep learning and have shown great potential on several NAS benchmark datasets. This paper aims to comprehensively review and compare the state-of-the-art (SOTA) zero-shot NAS approaches, with an emphasis on their hardware awareness. To this end, we first review the mainstream zero-shot proxies and discuss their theoretical underpinnings. We then compare these zero-shot proxies through large-scale experiments and demonstrate their effectiveness in both hardware-aware and hardware-oblivious NAS scenarios. Finally, we point out several promising ideas to design better proxies. Our source code and the related paper list are available on https://github.com/SLDGroup/survey-zero-shot-nas.
</details>
<details>
<summary>摘要</summary>
最近，零shot（或无需训练）神经建筑搜索（NAS）方法已经被提出，以解 liberate NAS 从训练要求中。零shot NAS 方法的关键想法是通过不需要训练网络参数来预测网络的准确性。已经提出的proxy都是基于现代神经网络理论的发展，在几个 NAS 比赛数据集上显示出了极高的潜力。这篇论文的目的是对当前领先的零shot NAS 方法进行全面的审视和比较，强调硬件意识。为此，我们首先介绍主流零shot proxy，并讨论它们的理论基础。然后，我们通过大规模的实验比较这些零shot proxy，并在硬件意识和硬件无知 NAS 场景中证明它们的效iveness。最后，我们提出了一些可能会设计更好的proxy的想法。我们的源代码和相关论文列表可以在https://github.com/SLDGroup/survey-zero-shot-nas上获取。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Feature-based-Deep-Reinforcement-Learning-for-Flow-Control-of-Circular-Cylinder-with-Sparse-Surface-Pressure-Sensing"><a href="#Dynamic-Feature-based-Deep-Reinforcement-Learning-for-Flow-Control-of-Circular-Cylinder-with-Sparse-Surface-Pressure-Sensing" class="headerlink" title="Dynamic Feature-based Deep Reinforcement Learning for Flow Control of Circular Cylinder with Sparse Surface Pressure Sensing"></a>Dynamic Feature-based Deep Reinforcement Learning for Flow Control of Circular Cylinder with Sparse Surface Pressure Sensing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01995">http://arxiv.org/abs/2307.01995</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiulei Wang, Lei Yan, Gang Hu, Wenli Chen, Bernd R. Noack</li>
<li>for: 这个研究旨在开发一种基于深度学习的闭Loop瓣纹控制算法，以降低瓣纹 drag 和 lift 波动，并且在感知不充分的情况下进行自适应控制。</li>
<li>methods: 该研究基于深度学习，将感知信号提升为动态特征（DF），以预测未来的流态态。 resulting DF-DRL 自动学习了响应控制器，无需动态模型。</li>
<li>results: 对比标准模型，DF-DRL 模型的瓣纹系数降低了25%。使用单个表面压力传感器，DF-DRL 可以降低瓣纹系数到状态 искусственный智能性的8%，并且减少了升力系数波动。这种方法还在更高的 Reynolds 数下表现良好，降低了瓣纹系数32.2% 和 46.55%。<details>
<summary>Abstract</summary>
This study proposes a self-learning algorithm for closed-loop cylinder wake control targeting lower drag and lower lift fluctuations with the additional challenge of sparse sensor information, taking deep reinforcement learning as the starting point. DRL performance is significantly improved by lifting the sensor signals to dynamic features (DF), which predict future flow states. The resulting dynamic feature-based DRL (DF-DRL) automatically learns a feedback control in the plant without a dynamic model. Results show that the drag coefficient of the DF-DRL model is 25% less than the vanilla model based on direct sensor feedback. More importantly, using only one surface pressure sensor, DF-DRL can reduce the drag coefficient to a state-of-the-art performance of about 8% at Re = 100 and significantly mitigate lift coefficient fluctuations. Hence, DF-DRL allows the deployment of sparse sensing of the flow without degrading the control performance. This method also shows good robustness in controlling flow under higher Reynolds numbers, which reduces the drag coefficient by 32.2% and 46.55% at Re = 500 and 1000, respectively, indicating the broad applicability of the method. Since surface pressure information is more straightforward to measure in realistic scenarios than flow velocity information, this study provides a valuable reference for experimentally designing the active flow control of a circular cylinder based on wall pressure signals, which is an essential step toward further developing intelligent control in realistic multi-input multi-output (MIMO) system.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="The-KiTS21-Challenge-Automatic-segmentation-of-kidneys-renal-tumors-and-renal-cysts-in-corticomedullary-phase-CT"><a href="#The-KiTS21-Challenge-Automatic-segmentation-of-kidneys-renal-tumors-and-renal-cysts-in-corticomedullary-phase-CT" class="headerlink" title="The KiTS21 Challenge: Automatic segmentation of kidneys, renal tumors, and renal cysts in corticomedullary-phase CT"></a>The KiTS21 Challenge: Automatic segmentation of kidneys, renal tumors, and renal cysts in corticomedullary-phase CT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01984">http://arxiv.org/abs/2307.01984</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/neheller/kits21">https://github.com/neheller/kits21</a></li>
<li>paper_authors: Nicholas Heller, Fabian Isensee, Dasha Trofimova, Resha Tejpaul, Zhongchen Zhao, Huai Chen, Lisheng Wang, Alex Golts, Daniel Khapun, Daniel Shats, Yoel Shoshan, Flora Gilboa-Solomon, Yasmeen George, Xi Yang, Jianpeng Zhang, Jing Zhang, Yong Xia, Mengran Wu, Zhiyang Liu, Ed Walczak, Sean McSweeney, Ranveer Vasdev, Chris Hornung, Rafat Solaiman, Jamee Schoephoerster, Bailey Abernathy, David Wu, Safa Abdulkadir, Ben Byun, Justice Spriggs, Griffin Struyk, Alexandra Austin, Ben Simpson, Michael Hagstrom, Sierra Virnig, John French, Nitin Venkatesh, Sarah Chan, Keenan Moore, Anna Jacobsen, Susan Austin, Mark Austin, Subodh Regmi, Nikolaos Papanikolopoulos, Christopher Weight</li>
<li>for: 本文是关于2021年的肾茵和肾肿瘤分割挑战（KiTS21）的挑战报告，与2021年的医疗图像计算和计算机助手外科会议（MICCAI）一起举行。</li>
<li>methods: 本挑战使用了一种新的标注方法，收集了每个区域兴趣的三个独立标注，并使用了一个基于网络的标注工具进行完全透明的标注。此外，KiTS21测试集来自外部机构，挑战参与者开发出能够通用化的方法。</li>
<li>results:  despite the challenges, the top-performing teams achieved a significant improvement over the state of the art set in 2019, and this performance is shown to inch ever closer to human-level performance. Here’s the translation in Traditional Chinese:</li>
<li>for: 本文是关于2021年的肾茵和肾肿瘤分割挑战（KiTS21）的挑战报告，与2021年的医疗图像计算和计算机助手外科会议（MICCAI）一起举行。</li>
<li>methods: 本挑战使用了一种新的标注方法，收集了每个区域兴趣的三个独立标注，并使用了一个基于网络的标注工具进行完全透明的标注。此外，KiTS21测试集来自外部机构，挑战参与者开发出能够通用化的方法。</li>
<li>results:  despite the challenges, the top-performing teams achieved a significant improvement over the state of the art set in 2019, and this performance is shown to inch ever closer to human-level performance.<details>
<summary>Abstract</summary>
This paper presents the challenge report for the 2021 Kidney and Kidney Tumor Segmentation Challenge (KiTS21) held in conjunction with the 2021 international conference on Medical Image Computing and Computer Assisted Interventions (MICCAI). KiTS21 is a sequel to its first edition in 2019, and it features a variety of innovations in how the challenge was designed, in addition to a larger dataset. A novel annotation method was used to collect three separate annotations for each region of interest, and these annotations were performed in a fully transparent setting using a web-based annotation tool. Further, the KiTS21 test set was collected from an outside institution, challenging participants to develop methods that generalize well to new populations. Nonetheless, the top-performing teams achieved a significant improvement over the state of the art set in 2019, and this performance is shown to inch ever closer to human-level performance. An in-depth meta-analysis is presented describing which methods were used and how they faired on the leaderboard, as well as the characteristics of which cases generally saw good performance, and which did not. Overall KiTS21 facilitated a significant advancement in the state of the art in kidney tumor segmentation, and provides useful insights that are applicable to the field of semantic segmentation as a whole.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了2021年的肾脏和肾肿瘤分割挑战（KiTS21）的挑战报告，该挑战在2021年的医学影像计算和计算助手外科学会（MICCAI）会议上举行。KiTS21是2019年的首届版本的续作，它在设计方面添加了许多创新，同时使用了更大的数据集。在这次挑战中，使用了一种新的注解方法，每个区域兴趣都有三个独立的注解，并在网络上使用了 transparent 的注解工具进行了注解。此外，KiTS21 测试集来自于外部机构，挑战参与者们开发出能够在新人口中广泛应用的方法。不过，最高排名的团队在2019年的状态前进set上达到了显著的改进，并且这种性能在人类水平逐渐往近。文章还提供了一个深入的meta-分析，描述了参与者们使用的方法以及其在排名表上的表现，以及特定情况下的好坏表现。总的来说，KiTS21 对肾脏瘤分割领域的状态前进做出了重要贡献，并为 semantic segmentation 领域提供了有用的指导。
</details></li>
</ul>
<hr>
<h2 id="A-ChatGPT-Aided-Explainable-Framework-for-Zero-Shot-Medical-Image-Diagnosis"><a href="#A-ChatGPT-Aided-Explainable-Framework-for-Zero-Shot-Medical-Image-Diagnosis" class="headerlink" title="A ChatGPT Aided Explainable Framework for Zero-Shot Medical Image Diagnosis"></a>A ChatGPT Aided Explainable Framework for Zero-Shot Medical Image Diagnosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01981">http://arxiv.org/abs/2307.01981</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaxiang Liu, Tianxiang Hu, Yan Zhang, Xiaotang Gai, Yang Feng, Zuozhu Liu</li>
<li>for: 这个研究是为了提出一个零条件医疗影像分类框架，以便在实际应用中对于有限的疾病或大规模标注数据进行医疗诊断。</li>
<li>methods: 这个研究使用了CLIP的预训练视觉语言模型，并与ChatGPT进行整合，以提供可解释的医疗诊断。在这个框架中，我们使用了分类名称来询问大型语言模型（LLMs），以生成更多的cue和知识，例如疾病 симптом或描述，帮助提供更加精确和可解释的诊断。</li>
<li>results: 我们在一个私人数据集和四个公共数据集上进行了广泛的实验，并进行了详细分析，结果显示了我们的零条件医疗影像分类框架的有效性和可解释性，证明了VLMs和LLMs在医疗应用中的巨大潜力。<details>
<summary>Abstract</summary>
Zero-shot medical image classification is a critical process in real-world scenarios where we have limited access to all possible diseases or large-scale annotated data. It involves computing similarity scores between a query medical image and possible disease categories to determine the diagnostic result. Recent advances in pretrained vision-language models (VLMs) such as CLIP have shown great performance for zero-shot natural image recognition and exhibit benefits in medical applications. However, an explainable zero-shot medical image recognition framework with promising performance is yet under development. In this paper, we propose a novel CLIP-based zero-shot medical image classification framework supplemented with ChatGPT for explainable diagnosis, mimicking the diagnostic process performed by human experts. The key idea is to query large language models (LLMs) with category names to automatically generate additional cues and knowledge, such as disease symptoms or descriptions other than a single category name, to help provide more accurate and explainable diagnosis in CLIP. We further design specific prompts to enhance the quality of generated texts by ChatGPT that describe visual medical features. Extensive results on one private dataset and four public datasets along with detailed analysis demonstrate the effectiveness and explainability of our training-free zero-shot diagnosis pipeline, corroborating the great potential of VLMs and LLMs for medical applications.
</details>
<details>
<summary>摘要</summary>
zero-shot医疗影像分类是现实世界中的关键过程，其中我们可能只有有限的疾病或大规模注释的数据。它涉及计算医疗影像和可能的疾病类别之间的相似性分数，以确定诊断结果。现代预训练视觉语言模型（VLM），如CLIP，在无需训练的情况下显示出了非常好的性能，并且在医疗应用中展现出了优势。然而，一个可解释的无需训练医疗影像分类框架仍然在开发中。在本文中，我们提出了一种基于CLIP的新的无需训练医疗影像分类框架，并与ChatGPT结合使用以提供可解释的诊断。我们的关键想法是使用类别名称来查询大型语言模型（LLM），以自动生成更多的引导和知识，如疾病 симптом或描述，以帮助提供更准确和可解释的诊断。我们还设计了特定的提示，以提高生成的文本中的可读性。我们的无需训练零shot诊断管道在一个私人数据集和四个公共数据集上进行了广泛的测试，并进行了详细的分析，结果证明了我们的训练free零shot诊断管道的有效性和可解释性，证明了VLM和LLM在医疗应用中的潜力。
</details></li>
</ul>
<hr>
<h2 id="Algorithme-EM-regularise"><a href="#Algorithme-EM-regularise" class="headerlink" title="Algorithme EM régularisé"></a>Algorithme EM régularisé</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01955">http://arxiv.org/abs/2307.01955</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pierre Houdouin, Matthieu Jonkcheere, Frederic Pascal</li>
<li>for: 用于处理小样本大数据的 Gaussian Mixture Model (GMM) 最优化likelihood问题。</li>
<li>methods: 提出了一种受限制的EM算法，通过使用先验知识来缓解小样本大数据的问题，以确保covariance矩阵更新的正定性。</li>
<li>results: 实验表明该方法在 clustering 任务中表现良好。<details>
<summary>Abstract</summary>
Expectation-Maximization (EM) algorithm is a widely used iterative algorithm for computing maximum likelihood estimate when dealing with Gaussian Mixture Model (GMM). When the sample size is smaller than the data dimension, this could lead to a singular or poorly conditioned covariance matrix and, thus, to performance reduction. This paper presents a regularized version of the EM algorithm that efficiently uses prior knowledge to cope with a small sample size. This method aims to maximize a penalized GMM likelihood where regularized estimation may ensure positive definiteness of covariance matrix updates by shrinking the estimators towards some structured target covariance matrices. Finally, experiments on real data highlight the good performance of the proposed algorithm for clustering purposes
</details>
<details>
<summary>摘要</summary>
<<SYS>>预期最大化（EM）算法是一种广泛使用的迭代算法，用于计算 Gaussian Mixture Model（GMM）中的最大可能性。当样本大小小于数据维度时，这可能导致一个稀疏或不良条件的协方差矩阵，从而导致性能下降。这篇文章提出了一种经过规格化的 EM 算法，可以有效地利用先前知识来应对小样本大小。该方法的目标是最大化约束后 GMM likelihood，通过压缩估计器向一些结构化目标协方差矩阵偏转。最后，在实际数据上进行了 clustering 实验，并证明了该算法的良好性能。>>>
</details></li>
</ul>
<hr>
<h2 id="FEMDA-Une-methode-de-classification-robuste-et-flexible"><a href="#FEMDA-Une-methode-de-classification-robuste-et-flexible" class="headerlink" title="FEMDA: Une méthode de classification robuste et flexible"></a>FEMDA: Une méthode de classification robuste et flexible</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01954">http://arxiv.org/abs/2307.01954</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pierre Houdouin, Matthieu Jonckheere, Frederic Pascal</li>
<li>for: 本研究旨在提出一种可以承受不同标准差和独立但不同分布的样本的新分类分析技术，以替代传统的线性和 quadratic discriminant分析方法，这些方法受到非泊oluisson分布和杂乱数据的影响。</li>
<li>methods: 该技术基于每个数据点都由其自己的arbitrary Elliptically Symmetrical（ES）分布和自己的扩展参数来定义，这使得模型能够处理可能非常不同、独立但不同分布的样本。</li>
<li>results: 该技术比其他状态艺术方法更快速、简单，对于涉及到非泊oluisson分布和杂乱数据的情况具有更高的Robustness，可以更好地适应实际应用中的数据分布。<details>
<summary>Abstract</summary>
Linear and Quadratic Discriminant Analysis (LDA and QDA) are well-known classical methods but can heavily suffer from non-Gaussian distributions and/or contaminated datasets, mainly because of the underlying Gaussian assumption that is not robust. This paper studies the robustness to scale changes in the data of a new discriminant analysis technique where each data point is drawn by its own arbitrary Elliptically Symmetrical (ES) distribution and its own arbitrary scale parameter. Such a model allows for possibly very heterogeneous, independent but non-identically distributed samples. The new decision rule derived is simple, fast, and robust to scale changes in the data compared to other state-of-the-art method
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Neural-Collapse-Perspective-on-Feature-Evolution-in-Graph-Neural-Networks"><a href="#A-Neural-Collapse-Perspective-on-Feature-Evolution-in-Graph-Neural-Networks" class="headerlink" title="A Neural Collapse Perspective on Feature Evolution in Graph Neural Networks"></a>A Neural Collapse Perspective on Feature Evolution in Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01951">http://arxiv.org/abs/2307.01951</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kvignesh1420/gnn_collapse">https://github.com/kvignesh1420/gnn_collapse</a></li>
<li>paper_authors: Vignesh Kothapalli, Tom Tirer, Joan Bruna</li>
<li>for: 本研究ocuses on node-wise classification tasks using graph neural networks (GNNs), and explores the interplay between graph topology and feature evolution.</li>
<li>methods: 本研究使用了community detection on stochastic block model graphs to illustrate the feature evolution, and explores the “Neural Collapse” (NC) phenomenon to understand the reduction in within-class variability.</li>
<li>results: 研究发现，在node-wise classification setting中，也有一定的减少内类差异，但不如instance-wise caso。然而，我们通过理论分析发现，这种减少内类差异的情况需要图像 obey certain strict structural conditions。此外，我们还研究了层次的feature variability evolution和spectral methods的差异。<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) have become increasingly popular for classification tasks on graph-structured data. Yet, the interplay between graph topology and feature evolution in GNNs is not well understood. In this paper, we focus on node-wise classification, illustrated with community detection on stochastic block model graphs, and explore the feature evolution through the lens of the "Neural Collapse" (NC) phenomenon. When training instance-wise deep classifiers (e.g. for image classification) beyond the zero training error point, NC demonstrates a reduction in the deepest features' within-class variability and an increased alignment of their class means to certain symmetric structures. We start with an empirical study that shows that a decrease in within-class variability is also prevalent in the node-wise classification setting, however, not to the extent observed in the instance-wise case. Then, we theoretically study this distinction. Specifically, we show that even an "optimistic" mathematical model requires that the graphs obey a strict structural condition in order to possess a minimizer with exact collapse. Interestingly, this condition is viable also for heterophilic graphs and relates to recent empirical studies on settings with improved GNNs' generalization. Furthermore, by studying the gradient dynamics of the theoretical model, we provide reasoning for the partial collapse observed empirically. Finally, we present a study on the evolution of within- and between-class feature variability across layers of a well-trained GNN and contrast the behavior with spectral methods.
</details>
<details>
<summary>摘要</summary>
格 Edge 神经网络 (GNNs) 在图像数据上的分类任务中得到了广泛的应用。然而，图像结构和特征进化在 GNNs 之间的关系还不够了解。在这篇论文中，我们将注意力集中在图像分类任务上，使用社会均衡图来检测社群，并通过 "神经崩溃" (NC) 现象来探索特征进化。当训练深度分类器（例如图像分类） beyond 零训练错误点时，NC 显示出深度特征内部的同类变化减少和类别中心对某些对称结构的偏好增加。我们开始于一个实验研究，显示在节点级分类设定下，也存在类似的减少同类变化现象，但不如实例级分类情况那么严重。然后，我们进行了理论研究。我们表明，即使使用 "乐观" 的数学模型， graphs 需要遵循一种严格的结构条件，以便具有精确的崩溃。意外地，这种条件适用于异谱图也，并与最近的实际研究中的 GNNs 的泛化有关。此外，我们通过研究理论模型的梯度动力学，提供了崩溃观察到的解释。最后，我们展示了一个层次进化特征的演化过程，并与 спектраль方法相比较。
</details></li>
</ul>
<hr>
<h2 id="A-Synthetic-Electrocardiogram-ECG-Image-Generation-Toolbox-to-Facilitate-Deep-Learning-Based-Scanned-ECG-Digitization"><a href="#A-Synthetic-Electrocardiogram-ECG-Image-Generation-Toolbox-to-Facilitate-Deep-Learning-Based-Scanned-ECG-Digitization" class="headerlink" title="A Synthetic Electrocardiogram (ECG) Image Generation Toolbox to Facilitate Deep Learning-Based Scanned ECG Digitization"></a>A Synthetic Electrocardiogram (ECG) Image Generation Toolbox to Facilitate Deep Learning-Based Scanned ECG Digitization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01946">http://arxiv.org/abs/2307.01946</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kshama Kodthalu Shivashankara, Afagh Mehri Shervedani, Reza Sameni</li>
<li>for: 这个论文的目的是提出一种新的方法来生成Synthetic ECG图像，以便用于训练深度学习模型进行算法式ECG诊断。</li>
<li>methods: 该方法利用了深度学习图像处理技术，并将 PhysioNet PTB-XL ECG时间序列数据作为引用时间序列数据，通过数据扩展技术来生成Synthetic ECG图像。</li>
<li>results: 研究人员通过计算信号噪声比（SNR）来评估生成的Synthetic ECG图像质量，结果显示了平均信号恢复SNR为27$\pm$2.8dB，这说明了提出的Synthetic ECG图像集可以用于训练深度学习模型。<details>
<summary>Abstract</summary>
The electrocardiogram (ECG) is an accurate and widely available tool for diagnosing cardiovascular diseases. ECGs have been recorded in printed formats for decades and their digitization holds great potential for training machine learning (ML) models in algorithmic ECG diagnosis. Physical ECG archives are at risk of deterioration and scanning printed ECGs alone is insufficient, as ML models require ECG time-series data. Therefore, the digitization and conversion of paper ECG archives into time-series data is of utmost importance. Deep learning models for image processing show promise in this regard. However, the scarcity of ECG archives with reference time-series is a challenge. Data augmentation techniques utilizing \textit{digital twins} present a potential solution.   We introduce a novel method for generating synthetic ECG images on standard paper-like ECG backgrounds with realistic artifacts. Distortions including handwritten text artifacts, wrinkles, creases and perspective transforms are applied to the generated images, without personally identifiable information. As a use case, we generated an ECG image dataset of 21,801 records from the 12-lead PhysioNet PTB-XL ECG time-series dataset. A deep ECG image digitization model was built and trained on the synthetic dataset, and was employed to convert the synthetic images to time-series data for evaluation. The signal-to-noise ratio (SNR) was calculated to assess the image digitization quality vs the ground truth ECG time-series. The results show an average signal recovery SNR of 27$\pm$2.8\,dB, demonstrating the significance of the proposed synthetic ECG image dataset for training deep learning models. The codebase is available as an open-access toolbox for ECG research.
</details>
<details>
<summary>摘要</summary>
电rokardiogram (ECG) 是一种精度很高且普遍可用的工具，用于诊断心血管疾病。ECG 已经被记录在Printed format 中decades，其数字化具有很大的潜力，用于训练机器学习（ML）模型。Physical ECG archive 面临着逐渐衰老和损坏的风险，而且将Printed ECG 纸背景上的ECG 记录scan alone 是不够的，因为ML 模型需要时间序列数据。因此，将纸背景上的ECG 记录数字化和转换为时间序列数据是非常重要的。深度学习模型 для图像处理表示了可能性。然而，获取ECG archive 中的参考时间序列数据是一个挑战。使用数据扩展技术利用“数字双胞胎”的想法可以解决这个问题。我们提出了一种新的方法，用于在标准纸背景上生成synthetic ECG 图像。这些图像包括手写文本 artifacts、折叠、皱纹和视角变换等缺失，但不包含个人可识别信息。作为用例，我们生成了21,801个纪录，来自PhysioNet PTB-XL ECG 时间序列 dataset。我们建立了一个深度ECG 图像数字化模型，并将其训练在生成的synthetic dataset上。然后，我们使用该模型将生成的synthetic图像转换为时间序列数据，并计算了信号噪声比（SNR）来评估图像数字化质量与真实ECG 时间序列数据之间的对比。结果显示，生成的ECG 图像数据的平均信号恢复SNR为27$\pm$2.8dB，这说明了我们提出的Synthetic ECG 图像dataset的重要性。我们的代码库作为一个开源工具箱，用于心血管疾病研究。
</details></li>
</ul>
<hr>
<h2 id="Text-Sketch-Image-Compression-at-Ultra-Low-Rates"><a href="#Text-Sketch-Image-Compression-at-Ultra-Low-Rates" class="headerlink" title="Text + Sketch: Image Compression at Ultra Low Rates"></a>Text + Sketch: Image Compression at Ultra Low Rates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01944">http://arxiv.org/abs/2307.01944</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/leieric/text-sketch">https://github.com/leieric/text-sketch</a></li>
<li>paper_authors: Eric Lei, Yiğit Berkay Uslu, Hamed Hassani, Shirin Saeedi Bidokhti</li>
<li>for: 本文旨在探讨如何使用文本描述生成高质量图像，并用于图像压缩。</li>
<li>methods: 本文使用了一些直接使用预训练模型进行图像压缩的技术，包括使用文本描述和侧信息生成高质量重建图像，以及使用预训练模型进行图像压缩。</li>
<li>results: 研究发现，使用这些技术可以在非常低的比特率下实现高度的semantic和spatial结构保持，并且在learned compressors中显著提高了感知和semantic faithfulness。<details>
<summary>Abstract</summary>
Recent advances in text-to-image generative models provide the ability to generate high-quality images from short text descriptions. These foundation models, when pre-trained on billion-scale datasets, are effective for various downstream tasks with little or no further training. A natural question to ask is how such models may be adapted for image compression. We investigate several techniques in which the pre-trained models can be directly used to implement compression schemes targeting novel low rate regimes. We show how text descriptions can be used in conjunction with side information to generate high-fidelity reconstructions that preserve both semantics and spatial structure of the original. We demonstrate that at very low bit-rates, our method can significantly improve upon learned compressors in terms of perceptual and semantic fidelity, despite no end-to-end training.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Neural-Network-Based-Enrichment-of-Reproducing-Kernel-Approximation-for-Modeling-Brittle-Fracture"><a href="#A-Neural-Network-Based-Enrichment-of-Reproducing-Kernel-Approximation-for-Modeling-Brittle-Fracture" class="headerlink" title="A Neural Network-Based Enrichment of Reproducing Kernel Approximation for Modeling Brittle Fracture"></a>A Neural Network-Based Enrichment of Reproducing Kernel Approximation for Modeling Brittle Fracture</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01937">http://arxiv.org/abs/2307.01937</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonghyuk Baek, Jiun-Shyan Chen</li>
<li>For: The paper is written to propose an improved version of the neural network-enhanced Reproducing Kernel Particle Method (NN-RKPM) for modeling brittle fracture.* Methods: The proposed method uses a background reproducing kernel (RK) approximation defined on a coarse and uniform discretization, which is enriched by a neural network (NN) approximation under a Partition of Unity framework. The NN approximation automatically locates and inserts regularized discontinuities in the function space.* Results: The proposed method is demonstrated to be effective through a series of numerical examples involving damage propagation and branching, and the solution convergence of the proposed method is guaranteed.Here are the three points in Simplified Chinese:* For: 这篇论文是为了提出一种改进版的神经网络增强的 reproduce kernel particle method (NN-RKPM)，用于模拟脆性断裂。* Methods: 该方法使用了一个背景 reproduce kernel (RK) approximation，定义在一个粗略和均匀的离散中，并通过一个神经网络 (NN)  aproximation 下的 Partition of Unity 框架进行增强。NN  aproximation 自动在函数空间中找到和插入正规破碎。* Results: 该方法在一系列的数值例子中，包括损害传播和分支，并且解的收敛性是保证的。<details>
<summary>Abstract</summary>
Numerical modeling of localizations is a challenging task due to the evolving rough solution in which the localization paths are not predefined. Despite decades of efforts, there is a need for innovative discretization-independent computational methods to predict the evolution of localizations. In this work, an improved version of the neural network-enhanced Reproducing Kernel Particle Method (NN-RKPM) is proposed for modeling brittle fracture. In the proposed method, a background reproducing kernel (RK) approximation defined on a coarse and uniform discretization is enriched by a neural network (NN) approximation under a Partition of Unity framework. In the NN approximation, the deep neural network automatically locates and inserts regularized discontinuities in the function space. The NN-based enrichment functions are then patched together with RK approximation functions using RK as a Partition of Unity patching function. The optimum NN parameters defining the location, orientation, and displacement distribution across location together with RK approximation coefficients are obtained via the energy-based loss function minimization. To regularize the NN-RK approximation, a constraint on the spatial gradient of the parametric coordinates is imposed in the loss function. Analysis of the convergence properties shows that the solution convergence of the proposed method is guaranteed. The effectiveness of the proposed method is demonstrated by a series of numerical examples involving damage propagation and branching.
</details>
<details>
<summary>摘要</summary>
numerical modeling of localizations 是一个复杂的任务，因为本地化路径不是预定的。 DESPITE 数十年的努力，目前仍需要创新的离散独立计算方法，以预测本地化的演化。 在这种工作中，一种改进的神经网络增强的复现器kernel方法（NN-RKPM）被提议用于模拟脆弱裂解。在提议的方法中，背景的复现器kernel（RK）approximation在粗略和均匀的离散上定义，然后通过一个神经网络（NN）approximation在Partition of Unity框架下进行增强。在NNapproximation中，深度神经网络自动在函数空间中找到并插入正规化缺陷。然后，NN基于的增强函数被与RKapproximation函数用RK作为Partition of Unity patching函数相连接。通过能量基本的损失函数最小化来获取优化NN参数，其中NN参数包括位置、方向和分布的拟合。为了正则化NN-RKapproximation，在损失函数中添加了空间梯度的约束。分析表示方法的扩散性是保证的。通过一系列数字示例，包括损失传播和分支，这种方法的有效性得到证明。
</details></li>
</ul>
<hr>
<h2 id="MDI-A-Flexible-Random-Forest-Based-Feature-Importance-Framework"><a href="#MDI-A-Flexible-Random-Forest-Based-Feature-Importance-Framework" class="headerlink" title="MDI+: A Flexible Random Forest-Based Feature Importance Framework"></a>MDI+: A Flexible Random Forest-Based Feature Importance Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01932">http://arxiv.org/abs/2307.01932</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/csinva/imodels">https://github.com/csinva/imodels</a></li>
<li>paper_authors: Abhineet Agarwal, Ana M. Kenney, Yan Shuo Tan, Tiffany M. Tang, Bin Yu</li>
<li>for: 本研究旨在提出一种可变的特征重要性框架，即MDI+，以提高Random Forest模型中特征的重要性评估。</li>
<li>methods: 本研究使用了Random Forest模型和Generalized Linear Models（GLMs），并提出了一种基于Predictability、Computability和Stability框架的指南，以帮助实践者选择适合的GLM和评价指标。</li>
<li>results: 实验表明，MDI+可以在识别信号特征方面表现出色，并且在实际应用中可以提取已知的预测性基因，并且比现有的特征重要性评估方法具有更高的稳定性。<details>
<summary>Abstract</summary>
Mean decrease in impurity (MDI) is a popular feature importance measure for random forests (RFs). We show that the MDI for a feature $X_k$ in each tree in an RF is equivalent to the unnormalized $R^2$ value in a linear regression of the response on the collection of decision stumps that split on $X_k$. We use this interpretation to propose a flexible feature importance framework called MDI+. Specifically, MDI+ generalizes MDI by allowing the analyst to replace the linear regression model and $R^2$ metric with regularized generalized linear models (GLMs) and metrics better suited for the given data structure. Moreover, MDI+ incorporates additional features to mitigate known biases of decision trees against additive or smooth models. We further provide guidance on how practitioners can choose an appropriate GLM and metric based upon the Predictability, Computability, Stability framework for veridical data science. Extensive data-inspired simulations show that MDI+ significantly outperforms popular feature importance measures in identifying signal features. We also apply MDI+ to two real-world case studies on drug response prediction and breast cancer subtype classification. We show that MDI+ extracts well-established predictive genes with significantly greater stability compared to existing feature importance measures. All code and models are released in a full-fledged python package on Github.
</details>
<details>
<summary>摘要</summary>
“ mean decrease in impurity (MDI) 是一个流行的特征重要度量表 для random forest (RF)。我们证明了 MDI 中的特征 $X_k$ 在每棵树中的 RF 相等于不调和的 $R^2$ 值在对应的决策探针中的线性回传模型中。我们使用这个解释来提出一个灵活的特征重要度框架called MDI+。 Specifically, MDI+ 将 MDI 扩展到让分析师可以更改线性回传模型和 $R^2$ 指标，并且包括额外的特征以减少决策树对添加或平滑模型的偏见。我们还提供适当的 GLM 和指标基于 Predictability, Computability, Stability 框架的指南。广泛的数据验证表明 MDI+ 可以对应用于特征重要度度量表示明显的提高。我们还应用 MDI+ 到了两个实际的应用案例：药物对应预测和乳癌类型分类。我们发现 MDI+ 可以提取稳定且有高预测力的遗传因素，较常用的特征重要度度量表示明显的更好。所有的代码和模型都可以在 GitHub 上找到。”
</details></li>
</ul>
<hr>
<h2 id="Learning-ECG-signal-features-without-backpropagation"><a href="#Learning-ECG-signal-features-without-backpropagation" class="headerlink" title="Learning ECG signal features without backpropagation"></a>Learning ECG signal features without backpropagation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01930">http://arxiv.org/abs/2307.01930</a></li>
<li>repo_url: None</li>
<li>paper_authors: Péter Pósfay, Marcell T. Kurbucz, Péter Kovács, Antal Jakovác</li>
<li>for: 这篇论文的目的是提出一种新的方法来生成时间序列数据的表示方式，以提高下游任务的效果、范围和可应用性。</li>
<li>methods: 该方法基于物理学的想法，通过数据驱动的方式构建一个减少的表示，同时能够捕捉数据的下面结构和任务特定信息，并且仍然保持易于理解、可读性和验证性。</li>
<li>results: 通过应用该方法于心跳信号分类任务，实现了状态首位表现。<details>
<summary>Abstract</summary>
Representation learning has become a crucial area of research in machine learning, as it aims to discover efficient ways of representing raw data with useful features to increase the effectiveness, scope and applicability of downstream tasks such as classification and prediction. In this paper, we propose a novel method to generate representations for time series-type data. This method relies on ideas from theoretical physics to construct a compact representation in a data-driven way, and it can capture both the underlying structure of the data and task-specific information while still remaining intuitive, interpretable and verifiable. This novel methodology aims to identify linear laws that can effectively capture a shared characteristic among samples belonging to a specific class. By subsequently utilizing these laws to generate a classifier-agnostic representation in a forward manner, they become applicable in a generalized setting. We demonstrate the effectiveness of our approach on the task of ECG signal classification, achieving state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
研究者们在机器学习领域内，尤其是在 Representation learning 方面，为了找到可以快速、高效地将原始数据转换为有用特征，以提高下游任务（如分类和预测）的效果、范围和可重用性。在这篇论文中，我们提出了一种新的方法，用于生成时间序列型数据的表示。这种方法基于物理学的想法，通过在数据驱动的方式下构建一个压缩表示，能够捕捉数据的下面结构和任务特定信息，同时仍然保持易于理解、可读性和可验证性。这种新的方法ology 目标是在特定类别中找到共同的特征，并通过这些法律生成一个批处器无关的表示，以便在总体上应用。我们在 ECG 信号分类任务中证明了我们的方法的效果，达到了领导性的表现。
</details></li>
</ul>
<hr>
<h2 id="ProtoDiffusion-Classifier-Free-Diffusion-Guidance-with-Prototype-Learning"><a href="#ProtoDiffusion-Classifier-Free-Diffusion-Guidance-with-Prototype-Learning" class="headerlink" title="ProtoDiffusion: Classifier-Free Diffusion Guidance with Prototype Learning"></a>ProtoDiffusion: Classifier-Free Diffusion Guidance with Prototype Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01924">http://arxiv.org/abs/2307.01924</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gulcin Baykal, Halil Faruk Karagoz, Taha Binhuraib, Gozde Unal</li>
<li>for: 提高生成质量和稳定性，减少训练时间</li>
<li>methods:  integrate prototype learning into diffusion models</li>
<li>results: 在不同的数据集和实验设置下，成功实现更高的生成质量和更快的训练时间<details>
<summary>Abstract</summary>
Diffusion models are generative models that have shown significant advantages compared to other generative models in terms of higher generation quality and more stable training. However, the computational need for training diffusion models is considerably increased. In this work, we incorporate prototype learning into diffusion models to achieve high generation quality faster than the original diffusion model. Instead of randomly initialized class embeddings, we use separately learned class prototypes as the conditioning information to guide the diffusion process. We observe that our method, called ProtoDiffusion, achieves better performance in the early stages of training compared to the baseline method, signifying that using the learned prototypes shortens the training time. We demonstrate the performance of ProtoDiffusion using various datasets and experimental settings, achieving the best performance in shorter times across all settings.
</details>
<details>
<summary>摘要</summary>
Diffusion models 是一类生成模型，在生成质量和训练稳定性方面表现出了明显的优势。然而，训练 diffusion models 所需的计算资源增加了 considrably。在这种情况下，我们将 prototype learning 引入 diffusion models，以实现更高的生成质量和更快的训练速度。而不是使用随机初始化的类嵌入，我们使用分开学习的类prototype来导引diffusion过程。我们发现，我们的方法（即 ProtoDiffusion）在训练的早期阶段表现出了更好的性能，这表明使用学习的 prototype 可以缩短训练时间。我们通过不同的数据集和实验设置来证明 ProtoDiffusion 的性能，在所有设置下都达到了最佳性能，并且在更短的时间内完成。
</details></li>
</ul>
<hr>
<h2 id="ClimateLearn-Benchmarking-Machine-Learning-for-Weather-and-Climate-Modeling"><a href="#ClimateLearn-Benchmarking-Machine-Learning-for-Weather-and-Climate-Modeling" class="headerlink" title="ClimateLearn: Benchmarking Machine Learning for Weather and Climate Modeling"></a>ClimateLearn: Benchmarking Machine Learning for Weather and Climate Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01909">http://arxiv.org/abs/2307.01909</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aditya-grover/climate-learn">https://github.com/aditya-grover/climate-learn</a></li>
<li>paper_authors: Tung Nguyen, Jason Jewik, Hritik Bansal, Prakhar Sharma, Aditya Grover<br>for:* The paper is written to introduce an open-source PyTorch library called ClimateLearn for training and evaluating machine learning models in data-driven climate science.methods:* The library includes holistic pipelines for dataset processing, state-of-the-art deep learning models, and quantitative and qualitative evaluation for standard weather and climate modeling tasks.results:* The authors have performed comprehensive forecasting and downscaling experiments to showcase the capabilities and key features of their library, and to their knowledge, ClimateLearn is the first large-scale, open-source effort for bridging research in weather and climate modeling with modern machine learning systems.<details>
<summary>Abstract</summary>
Modeling weather and climate is an essential endeavor to understand the near- and long-term impacts of climate change, as well as inform technology and policymaking for adaptation and mitigation efforts. In recent years, there has been a surging interest in applying data-driven methods based on machine learning for solving core problems such as weather forecasting and climate downscaling. Despite promising results, much of this progress has been impaired due to the lack of large-scale, open-source efforts for reproducibility, resulting in the use of inconsistent or underspecified datasets, training setups, and evaluations by both domain scientists and artificial intelligence researchers. We introduce ClimateLearn, an open-source PyTorch library that vastly simplifies the training and evaluation of machine learning models for data-driven climate science. ClimateLearn consists of holistic pipelines for dataset processing (e.g., ERA5, CMIP6, PRISM), implementation of state-of-the-art deep learning models (e.g., Transformers, ResNets), and quantitative and qualitative evaluation for standard weather and climate modeling tasks. We supplement these functionalities with extensive documentation, contribution guides, and quickstart tutorials to expand access and promote community growth. We have also performed comprehensive forecasting and downscaling experiments to showcase the capabilities and key features of our library. To our knowledge, ClimateLearn is the first large-scale, open-source effort for bridging research in weather and climate modeling with modern machine learning systems. Our library is available publicly at https://github.com/aditya-grover/climate-learn.
</details>
<details>
<summary>摘要</summary>
模拟天气和气候是一项非常重要的努力，以便更好地理解气候变化的短期和长期影响，以及为适应和控制努力提供技术和政策。在过去几年中，有一种增长的兴趣在应用基于机器学习的数据驱动方法来解决气候科学中的核心问题，如天气预报和气候减小。然而，由于缺乏大规模、开源的努力，导致许多进步受到了限制，因为很多域科学家和人工智能研究者使用不一致或不够特定的数据集、训练setup和评估方法。我们介绍了一个名为ClimateLearn的开源PyTorch库，该库可以很大程度地简化天气预报和气候模型训练和评估的过程。ClimateLearn包括整体数据处理管道（如ERA5、CMIP6、PRISM）、现代深度学习模型（如转换器、径深网络）的实现，以及标准天气和气候模型计算任务的量化和质量评估。我们还提供了广泛的文档、贡献指南和快速入门教程，以扩大访问权限和促进社区增长。我们还执行了广泛的预测和减小实验，以示出库的能力和关键特点。到我们所知，ClimateLearn是首个大规模、开源的气候科学与现代机器学习系统之间的桥梁。我们的库可以在https://github.com/aditya-grover/climate-learn上获取。
</details></li>
</ul>
<hr>
<h2 id="Stability-Analysis-Framework-for-Particle-based-Distance-GANs-with-Wasserstein-Gradient-Flow"><a href="#Stability-Analysis-Framework-for-Particle-based-Distance-GANs-with-Wasserstein-Gradient-Flow" class="headerlink" title="Stability Analysis Framework for Particle-based Distance GANs with Wasserstein Gradient Flow"></a>Stability Analysis Framework for Particle-based Distance GANs with Wasserstein Gradient Flow</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01879">http://arxiv.org/abs/2307.01879</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chuqi Chen, Yue Wu, Yang Xiang</li>
<li>For: 本研究 investigate the training process of generative networks that use particle-based distance as the objective function, such as MMD GAN, Cramér GAN, and EIEG GAN. However, these GANs often suffer from unstable training.* Methods: 我们 analyze the stability of the training process of these GANs from the perspective of probability density dynamics. We regard the discriminator $D$ as a feature transformation mapping and the generator $G$ as a random variable mapping. We use the Wasserstein gradient flow of the probability density function to perform stability analysis.* Results: 我们发现 that the training process of the discriminator is usually unstable due to the formulation of $\min_G \max_D E(G, D)$ in GANs. To address this issue, we add a stabilizing term in the discriminator loss function. We conduct experiments to validate our stability analysis and stabilizing method.<details>
<summary>Abstract</summary>
In this paper, we investigate the training process of generative networks that use a type of probability density distance named particle-based distance as the objective function, e.g. MMD GAN, Cram\'er GAN, EIEG GAN. However, these GANs often suffer from the problem of unstable training. In this paper, we analyze the stability of the training process of these GANs from the perspective of probability density dynamics. In our framework, we regard the discriminator $D$ in these GANs as a feature transformation mapping that maps high dimensional data into a feature space, while the generator $G$ maps random variables to samples that resemble real data in terms of feature space. This perspective enables us to perform stability analysis for the training of GANs using the Wasserstein gradient flow of the probability density function. We find that the training process of the discriminator is usually unstable due to the formulation of $\min_G \max_D E(G, D)$ in GANs. To address this issue, we add a stabilizing term in the discriminator loss function. We conduct experiments to validate our stability analysis and stabilizing method.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了生成网络在使用某种概率密度距离函数作为目标函数时的训练过程，例如MMD GAN、Cramér GAN、EIEG GAN。然而，这些GANs经常遇到训练不稳定的问题。在这篇论文中，我们从概率密度动力学的角度分析了这些GANs的训练过程的稳定性。我们认为权重网络$D$ acts as a feature transformation mapping that maps high-dimensional data into a feature space, while generator $G$ maps random variables to samples that resemble real data in terms of feature space.这种角度允许我们使用泊松流程来分析GANs的训练过程的稳定性。我们发现通常在GANs中的训练过程中，权重网络的训练是不稳定的，这是由于GANs中的$\min_G \max_D E(G, D)$的形式化引起的。为了解决这个问题，我们在权重网络的损失函数中添加了稳定化项。我们进行了实验来验证我们的稳定性分析和稳定化方法。
</details></li>
</ul>
<hr>
<h2 id="Fast-Private-Kernel-Density-Estimation-via-Locality-Sensitive-Quantization"><a href="#Fast-Private-Kernel-Density-Estimation-via-Locality-Sensitive-Quantization" class="headerlink" title="Fast Private Kernel Density Estimation via Locality Sensitive Quantization"></a>Fast Private Kernel Density Estimation via Locality Sensitive Quantization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01877">http://arxiv.org/abs/2307.01877</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/talwagner/lsq">https://github.com/talwagner/lsq</a></li>
<li>paper_authors: Tal Wagner, Yonatan Naamad, Nina Mishra</li>
<li>for: efficient mechanisms for differentially private kernel density estimation (DP-KDE)</li>
<li>methods: Locality Sensitive Quantization (LSQ) framework, which leverages existing non-private KDE methods and privatizes them in a black-box manner</li>
<li>results: DP-KDE mechanisms that are fast and accurate on large datasets in both high and low dimensions, with linear time complexity in the number of dimensions $d$<details>
<summary>Abstract</summary>
We study efficient mechanisms for differentially private kernel density estimation (DP-KDE). Prior work for the Gaussian kernel described algorithms that run in time exponential in the number of dimensions $d$. This paper breaks the exponential barrier, and shows how the KDE can privately be approximated in time linear in $d$, making it feasible for high-dimensional data. We also present improved bounds for low-dimensional data.   Our results are obtained through a general framework, which we term Locality Sensitive Quantization (LSQ), for constructing private KDE mechanisms where existing KDE approximation techniques can be applied. It lets us leverage several efficient non-private KDE methods -- like Random Fourier Features, the Fast Gauss Transform, and Locality Sensitive Hashing -- and ``privatize'' them in a black-box manner. Our experiments demonstrate that our resulting DP-KDE mechanisms are fast and accurate on large datasets in both high and low dimensions.
</details>
<details>
<summary>摘要</summary>
我们研究高效的权限私钥频率概率密度估计（DP-KDE）机制。先前的工作对于 Gaussian kernel 提出了时间复杂度为对数函数($d$)的算法。这篇论文破坏了这个限制，并示出了在高维数据时间复杂度 linear 的 KDE  aproximation 机制，使得其成为可行的。我们还提供了低维数据的改进 bound。我们的结果基于一个通用的框架，我们称之为 Local Sensitive Quantization（LSQ），用于构建私钥 KDE 机制。它允许我们利用一些高效的非私钥 KDE 方法，如 Random Fourier Features、Fast Gauss Transform 和 Locality Sensitive Hashing，并将它们“黑盒”化，以实现私钥 KDE 机制。我们的实验表明，我们的 resulting DP-KDE 机制在大数据集上具有高速和高准确性。
</details></li>
</ul>
<hr>
<h2 id="Generalization-Guarantees-via-Algorithm-dependent-Rademacher-Complexity"><a href="#Generalization-Guarantees-via-Algorithm-dependent-Rademacher-Complexity" class="headerlink" title="Generalization Guarantees via Algorithm-dependent Rademacher Complexity"></a>Generalization Guarantees via Algorithm-dependent Rademacher Complexity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02501">http://arxiv.org/abs/2307.02501</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sarah Sachs, Tim van Erven, Liam Hodgkinson, Rajiv Khanna, Umut Simsekli</li>
<li>for: 本文旨在提供一种新的复杂度度量来控制通用化误差，用于现代机器学习算法。</li>
<li>methods: 本文使用了一种基于卷积函数的复杂度度量，并利用了这种度量的一些标准性质和各种各样的假设集合的结构，从而得到了一些新的一阶bounds。</li>
<li>results: 本文得到了一些新的一阶bounds，包括基于幂积函数的bounds和基于假设集合的稳定性的bounds。这些bounds可以扩展到连续函数空间中的函数类和压缩算法，并且比之前的方法更加简单和直观。<details>
<summary>Abstract</summary>
Algorithm- and data-dependent generalization bounds are required to explain the generalization behavior of modern machine learning algorithms. In this context, there exists information theoretic generalization bounds that involve (various forms of) mutual information, as well as bounds based on hypothesis set stability. We propose a conceptually related, but technically distinct complexity measure to control generalization error, which is the empirical Rademacher complexity of an algorithm- and data-dependent hypothesis class. Combining standard properties of Rademacher complexity with the convenient structure of this class, we are able to (i) obtain novel bounds based on the finite fractal dimension, which (a) extend previous fractal dimension-type bounds from continuous to finite hypothesis classes, and (b) avoid a mutual information term that was required in prior work; (ii) we greatly simplify the proof of a recent dimension-independent generalization bound for stochastic gradient descent; and (iii) we easily recover results for VC classes and compression schemes, similar to approaches based on conditional mutual information.
</details>
<details>
<summary>摘要</summary>
Algorithm-和数据-依赖的总结 bounds 是现代机器学习算法的总结行为的解释需要的。在这种情况下，存在信息理论性的总结 bounds，其中包括（不同形式的）相互信息，以及基于假设集的稳定性。我们提出了一个概念上相关，但技术上不同的复杂度测量来控制总结错误，即算法和数据依赖的假设集中的Empirical Rademacher complexity。通过将标准的Rademacher complexity性质与这种类型的概念结合，我们能够：（i）获得基于finite fractal dimension的新的 bounds，这些bounds（a）在前期的继承维度类型 bounds 中扩展到了有限假设类型，并（b）避免在先前的工作中需要的mutual information项;（ii）我们大大简化了最近的维度独立总结 bound for stochastic gradient descent;（iii）我们轻松地回归到VC类和压缩 schemes中的结果，与基于conditional mutual information的方法类似。
</details></li>
</ul>
<hr>
<h2 id="Approximate-Adapt-Anonymize-3A-a-Framework-for-Privacy-Preserving-Training-Data-Release-for-Machine-Learning"><a href="#Approximate-Adapt-Anonymize-3A-a-Framework-for-Privacy-Preserving-Training-Data-Release-for-Machine-Learning" class="headerlink" title="Approximate, Adapt, Anonymize (3A): a Framework for Privacy Preserving Training Data Release for Machine Learning"></a>Approximate, Adapt, Anonymize (3A): a Framework for Privacy Preserving Training Data Release for Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01875">http://arxiv.org/abs/2307.01875</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tamas Madl, Weijie Xu, Olivia Choudhury, Matthew Howard</li>
<li>for: 这篇论文目的是提高机器学习中的数据 Utility，同时保证 differential privacy。</li>
<li>methods: 本文提出了一个名为 3A (Approximate, Adapt, Anonymize) 的数据发布框架，以 maximize 数据 Utility，同时保证 differential privacy。</li>
<li>results: 实验结果显示，使用本文提出的方法可以实现高度的数据 Utility，并且与实际数据中的模型性能相似。 compared to state-of-the-art models, 本文的方法可以提高数据生成的分类性能。<details>
<summary>Abstract</summary>
The availability of large amounts of informative data is crucial for successful machine learning. However, in domains with sensitive information, the release of high-utility data which protects the privacy of individuals has proven challenging. Despite progress in differential privacy and generative modeling for privacy-preserving data release in the literature, only a few approaches optimize for machine learning utility: most approaches only take into account statistical metrics on the data itself and fail to explicitly preserve the loss metrics of machine learning models that are to be subsequently trained on the generated data. In this paper, we introduce a data release framework, 3A (Approximate, Adapt, Anonymize), to maximize data utility for machine learning, while preserving differential privacy. We also describe a specific implementation of this framework that leverages mixture models to approximate, kernel-inducing points to adapt, and Gaussian differential privacy to anonymize a dataset, in order to ensure that the resulting data is both privacy-preserving and high utility. We present experimental evidence showing minimal discrepancy between performance metrics of models trained on real versus privatized datasets, when evaluated on held-out real data. We also compare our results with several privacy-preserving synthetic data generation models (such as differentially private generative adversarial networks), and report significant increases in classification performance metrics compared to state-of-the-art models. These favorable comparisons show that the presented framework is a promising direction of research, increasing the utility of low-risk synthetic data release for machine learning.
</details>
<details>
<summary>摘要</summary>
“具有大量有用数据的可用性是成功机器学习的关键。然而，在包含敏感信息的领域中，发布高Utility数据以保护个人隐私是挑战。尽管在Literature中已有进步的泛化隐私和生成模型，但大多数方法只考虑数据本身的统计指标，并没有显式保持机器学习模型将要在生成数据上训练的损失指标。在这篇论文中，我们介绍了一个数据发布框架，称为3A（简化、适应、匿名），以最大化机器学习数据的有用性，同时保持泛化隐私。我们还描述了该框架的具体实现，利用混合模型简化数据，使用抽象点适应数据，并使用泛化隐私保护数据，以确保生成的数据具有隐私保护和高Utility。我们通过实验证明，在评估模型在真实数据上的性能时， Privatized 数据与实际数据的差异很小。我们还与一些隐私保护生成数据生成模型进行比较，并发现我们的结果具有显著的提高性，相比于当前的模型。这些有利的比较表明，我们提出的框架是一个有前途的研究方向，增加低风险的生成数据发布的机器学习 utility。”
</details></li>
</ul>
<hr>
<h2 id="A-hybrid-machine-learning-framework-for-clad-characteristics-prediction-in-metal-additive-manufacturing"><a href="#A-hybrid-machine-learning-framework-for-clad-characteristics-prediction-in-metal-additive-manufacturing" class="headerlink" title="A hybrid machine learning framework for clad characteristics prediction in metal additive manufacturing"></a>A hybrid machine learning framework for clad characteristics prediction in metal additive manufacturing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01872">http://arxiv.org/abs/2307.01872</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sinatayebati/cladnet-ml-for-am">https://github.com/sinatayebati/cladnet-ml-for-am</a></li>
<li>paper_authors: Sina Tayebati, Kyu Taek Cho<br>for:This paper aims to develop a hybrid approach that combines computational fluid dynamics (CFD) modeling and machine learning (ML) techniques to predict and understand the characteristics of metal additive manufacturing (MAM) printed clads.methods:The authors use a calibrated CFD model to generate a comprehensive dataset of clad characteristics, including geometry, quality, and processing parameters. They then employ two sets of processing parameters for training ML models, along with versatile ML models and reliable evaluation metrics, to create a scalable learning framework for predicting clad geometry and quality.results:The proposed hybrid approach resolves many challenges of conventional modeling methods in MAM by providing an efficient, accurate, and scalable platform for clad characteristics prediction and optimization. The authors demonstrate the effectiveness of their approach by using it to predict clad geometry and quality under different processing conditions.<details>
<summary>Abstract</summary>
During the past decade, metal additive manufacturing (MAM) has experienced significant developments and gained much attention due to its ability to fabricate complex parts, manufacture products with functionally graded materials, minimize waste, and enable low-cost customization. Despite these advantages, predicting the impact of processing parameters on the characteristics of an MAM printed clad is challenging due to the complex nature of MAM processes. Machine learning (ML) techniques can help connect the physics underlying the process and processing parameters to the clad characteristics. In this study, we introduce a hybrid approach which involves utilizing the data provided by a calibrated multi-physics computational fluid dynamic (CFD) model and experimental research for preparing the essential big dataset, and then uses a comprehensive framework consisting of various ML models to predict and understand clad characteristics. We first compile an extensive dataset by fusing experimental data into the data generated using the developed CFD model for this study. This dataset comprises critical clad characteristics, including geometrical features such as width, height, and depth, labels identifying clad quality, and processing parameters. Second, we use two sets of processing parameters for training the ML models: machine setting parameters and physics-aware parameters, along with versatile ML models and reliable evaluation metrics to create a comprehensive and scalable learning framework for predicting clad geometry and quality. This framework can serve as a basis for clad characteristics control and process optimization. The framework resolves many challenges of conventional modeling methods in MAM by solving t the issue of data scarcity using a hybrid approach and introducing an efficient, accurate, and scalable platform for clad characteristics prediction and optimization.
</details>
<details>
<summary>摘要</summary>
过去一个 décennie，金属添加itive制造（MAM）经历了重要的发展和引起了广泛关注，因为它可以制造复杂的部件，生产具有功能分布的材料的产品，最小化废弃物，并实现低成本定制。然而，预测MAM打印后皮层特性的影响因素是复杂的，因为MAM过程的自然特性。机器学习（ML）技术可以帮助将物理下面的过程和处理参数与皮层特性相连。在本研究中，我们提出了一种混合方法，利用实验室数据和CFD模型提供的数据来准备 essencial的大型数据集，然后使用包括多种ML模型的完整框架来预测和理解皮层特性。我们首先编辑了一个广泛的数据集，将实验室数据和CFD模型生成的数据融合在一起，这个数据集包括皮层特性的关键特征，如宽度、高度和深度，标签标识皮层质量，以及处理参数。第二，我们使用两组处理参数进行训练ML模型：机器设置参数和物理意识参数，以及多种可靠的ML模型和评价指标来建立一个全面、精准和可扩展的学习框架，用于预测皮层几何和质量。这个框架可以作为皮层特性控制和过程优化的基础。这个框架解决了传统模型方法在MAM中的多个挑战，例如数据不足问题，通过混合方法和引入高效、准确和可扩展的平台，以便预测皮层特性和优化过程。
</details></li>
</ul>
<hr>
<h2 id="Self-Consuming-Generative-Models-Go-MAD"><a href="#Self-Consuming-Generative-Models-Go-MAD" class="headerlink" title="Self-Consuming Generative Models Go MAD"></a>Self-Consuming Generative Models Go MAD</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01850">http://arxiv.org/abs/2307.01850</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sina Alemohammad, Josue Casco-Rodriguez, Lorenzo Luzi, Ahmed Imtiaz Humayun, Hossein Babaei, Daniel LeJeune, Ali Siahkoohi, Richard G. Baraniuk</li>
<li>for: 本研究探讨了使用生成AI算法训练下一代模型时，自适应循环的特性。</li>
<li>methods: 我们使用当今最佳生成图像模型的三种家族来分析自适应循环的不同情况，包括在训练过程中是否有固定或新鲜的实际数据可用，以及模型是否受到偏见，以考虑数据质量和多样性之间的trade-off。</li>
<li>results: 我们发现，在没有充足的新鲜实际数据的情况下，自适应循环中的未来生成模型会逐渐减少精度或多样性。我们称这种情况为模型自适应疾病（MAD），与狂牛病相似。<details>
<summary>Abstract</summary>
Seismic advances in generative AI algorithms for imagery, text, and other data types has led to the temptation to use synthetic data to train next-generation models. Repeating this process creates an autophagous (self-consuming) loop whose properties are poorly understood. We conduct a thorough analytical and empirical analysis using state-of-the-art generative image models of three families of autophagous loops that differ in how fixed or fresh real training data is available through the generations of training and in whether the samples from previous generation models have been biased to trade off data quality versus diversity. Our primary conclusion across all scenarios is that without enough fresh real data in each generation of an autophagous loop, future generative models are doomed to have their quality (precision) or diversity (recall) progressively decrease. We term this condition Model Autophagy Disorder (MAD), making analogy to mad cow disease.
</details>
<details>
<summary>摘要</summary>
seized advances in generative AI algorithms for imagery, text, and other data types has led to the temptation to use synthetic data to train next-generation models. Repeating this process creates an autophagous (self-consuming) loop whose properties are poorly understood. We conduct a thorough analytical and empirical analysis using state-of-the-art generative image models of three families of autophagous loops that differ in how fixed or fresh real training data is available through the generations of training and in whether the samples from previous generation models have been biased to trade off data quality versus diversity. Our primary conclusion across all scenarios is that without enough fresh real data in each generation of an autophagous loop, future generative models are doomed to have their quality (precision) or diversity (recall) progressively decrease. We term this condition Model Autophagy Disorder (MAD), making analogy to mad cow disease.Here's the translation breakdown: seized (抓取) - advances autophagous (自食性) - loops generative (生成) - AI algorithms imagery (图像) - data types other (其他) - data typesNote that the word "mad" in the last sentence is not translated, as it is a metaphorical term and not a direct translation.
</details></li>
</ul>
<hr>
<h2 id="Crossway-Diffusion-Improving-Diffusion-based-Visuomotor-Policy-via-Self-supervised-Learning"><a href="#Crossway-Diffusion-Improving-Diffusion-based-Visuomotor-Policy-via-Self-supervised-Learning" class="headerlink" title="Crossway Diffusion: Improving Diffusion-based Visuomotor Policy via Self-supervised Learning"></a>Crossway Diffusion: Improving Diffusion-based Visuomotor Policy via Self-supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01849">http://arxiv.org/abs/2307.01849</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiang Li, Varun Belagali, Jinghuan Shang, Michael S. Ryoo</li>
<li>for:  robot imitation learning</li>
<li>methods:  diffusion models, self-supervised learning (SSL) objective</li>
<li>results:  better representation for policy learning, especially when the demonstrations have different proficiencies.Here’s the full text in Simplified Chinese:</li>
<li>for:  robot imitation learning</li>
<li>methods:  diffusion models, self-supervised learning (SSL) objective</li>
<li>results:  更好的政策学习表示，特别是当示例具有不同水平的时候。<details>
<summary>Abstract</summary>
Sequence modeling approaches have shown promising results in robot imitation learning. Recently, diffusion models have been adopted for behavioral cloning, benefiting from their exceptional capabilities in modeling complex data distribution. In this work, we propose Crossway Diffusion, a method to enhance diffusion-based visuomotor policy learning by using an extra self-supervised learning (SSL) objective. The standard diffusion-based policy generates action sequences from random noise conditioned on visual observations and other low-dimensional states. We further extend this by introducing a new decoder that reconstructs raw image pixels (and other state information) from the intermediate representations of the reverse diffusion process, and train the model jointly using the SSL loss. Our experiments demonstrate the effectiveness of Crossway Diffusion in various simulated and real-world robot tasks, confirming its advantages over the standard diffusion-based policy. We demonstrate that such self-supervised reconstruction enables better representation for policy learning, especially when the demonstrations have different proficiencies.
</details>
<details>
<summary>摘要</summary>
sequence modeling方法在机器人模仿学习中显示了扎实的成果。最近，扩散模型在行为刻画中被采用，因为它们在处理复杂数据分布方面表现出色。在这种工作中，我们提议了跨度扩散（Crossway Diffusion），一种使用额外的自动学习（SSL）目标来增强扩散基于视 Motor 政策学习的方法。标准的扩散基于策略会根据随机噪声和视觉观察结果生成动作序列。我们进一步延伸了这种方法，通过引入一个新的解码器，将推 diffusion 过程中的中间表示重建为原始图像像素和其他状态信息，并在模型中同时使用 SSL 损失进行训练。我们的实验表明，跨度扩散在各种模拟和实际的机器人任务中具有优势，特别是当示例具有不同的技巧水平时。
</details></li>
</ul>
<hr>
<h2 id="Empirical-Sample-Complexity-of-Neural-Network-Mixed-State-Reconstruction"><a href="#Empirical-Sample-Complexity-of-Neural-Network-Mixed-State-Reconstruction" class="headerlink" title="Empirical Sample Complexity of Neural Network Mixed State Reconstruction"></a>Empirical Sample Complexity of Neural Network Mixed State Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01840">http://arxiv.org/abs/2307.01840</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haimeng Zhao, Giuseppe Carleo, Filippo Vicentini</li>
<li>for: 这个论文旨在研究量子状态重建技术，以减少实际应用中的量子极限复杂性。</li>
<li>methods: 该论文使用了不同的量子状态重建技术，包括变分减少技术，并对其进行了数值研究。</li>
<li>results: 研究发现，在温度有限的伊塞ING模型中，使用不同的量子状态重建技术可以系统地减少量子资源的需求。同时，比较了两种主要的量子 neural 状态编码，即量子扩散算符表示和正值算符测量表示，并发现它们在混合性的不同范围内表现不同。<details>
<summary>Abstract</summary>
Quantum state reconstruction using Neural Quantum States has been proposed as a viable tool to reduce quantum shot complexity in practical applications, and its advantage over competing techniques has been shown in numerical experiments focusing mainly on the noiseless case. In this work, we numerically investigate the performance of different quantum state reconstruction techniques for mixed states: the finite-temperature Ising model. We show how to systematically reduce the quantum resource requirement of the algorithms by applying variance reduction techniques. Then, we compare the two leading neural quantum state encodings of the state, namely, the Neural Density Operator and the positive operator-valued measurement representation, and illustrate their different performance as the mixedness of the target state varies. We find that certain encodings are more efficient in different regimes of mixedness and point out the need for designing more efficient encodings in terms of both classical and quantum resources.
</details>
<details>
<summary>摘要</summary>
量子状态重建使用神经量子状态已被提议为实际应用中减少量子射频复杂性的可能工具，并其优势于竞争技术在数字实验中得到了证明。在这项工作中，我们数字实验 investigate了不同量子状态重建技术的性能在杂态场景下：finite-temperature Ising模型。我们表明如何系统地减少量子资源需求的算法，并应用变差缓和技术。然后，我们比较了两种主要的神经量子状态编码方法， namely，神经激发函数和正值算符测量表示法，并示出它们在不同杂度水平下的不同性能。我们发现某些编码在不同的杂度范围内更高效，并指出了设计更高效的编码的需求，即类比和量子资源。
</details></li>
</ul>
<hr>
<h2 id="Collaborative-Score-Distillation-for-Consistent-Visual-Synthesis"><a href="#Collaborative-Score-Distillation-for-Consistent-Visual-Synthesis" class="headerlink" title="Collaborative Score Distillation for Consistent Visual Synthesis"></a>Collaborative Score Distillation for Consistent Visual Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04787">http://arxiv.org/abs/2307.04787</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/subin-kim-cv/CSD">https://github.com/subin-kim-cv/CSD</a></li>
<li>paper_authors: Subin Kim, Kyungmin Lee, June Suk Choi, Jongheon Jeong, Kihyuk Sohn, Jinwoo Shin</li>
<li>for: 提高文本到图像扩散模型的应用范围和可编辑性。</li>
<li>methods: 基于 Stein 变分Gradient Descent（SVGD）的 Collaborative Score Distillation（CSD）方法，通过考虑多个样本的分布来塑造图像集的共聚性。</li>
<li>results: 在各种任务中，如修改投影图像、视频和3D场景，CSD方法能够提高图像集之间的一致性，从而扩展文本到图像扩散模型的应用范围。<details>
<summary>Abstract</summary>
Generative priors of large-scale text-to-image diffusion models enable a wide range of new generation and editing applications on diverse visual modalities. However, when adapting these priors to complex visual modalities, often represented as multiple images (e.g., video), achieving consistency across a set of images is challenging. In this paper, we address this challenge with a novel method, Collaborative Score Distillation (CSD). CSD is based on the Stein Variational Gradient Descent (SVGD). Specifically, we propose to consider multiple samples as "particles" in the SVGD update and combine their score functions to distill generative priors over a set of images synchronously. Thus, CSD facilitates seamless integration of information across 2D images, leading to a consistent visual synthesis across multiple samples. We show the effectiveness of CSD in a variety of tasks, encompassing the visual editing of panorama images, videos, and 3D scenes. Our results underline the competency of CSD as a versatile method for enhancing inter-sample consistency, thereby broadening the applicability of text-to-image diffusion models.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本转换为简化中文。<</SYS>>大规模文本到图像扩散模型的生成先验可以激发多种新的生成和编辑应用程序。然而，当应用这些先验到复杂的视觉模式时，保证一组图像之间的一致性是挑战。在这篇论文中，我们解决这个挑战方法是协同分数精灵（CSD）。CSD基于斯坦变分 Gradient Descent（SVGD）。我们建议将多个样本视为“粒子”在SVGD更新中，并将它们的分数函数组合以静止生成先验覆盖多个图像同步。因此，CSD使得多个图像之间的信息集成更加简单，从而实现了多个样本之间的视觉同步。我们在多种任务中展示了CSD的效果，包括修改广角图像、视频和3D场景。我们的结果表明CSD是一种多功能的方法，可以增强样本之间的一致性，从而扩大文本到图像扩散模型的应用范围。
</details></li>
</ul>
<hr>
<h2 id="DiT-3D-Exploring-Plain-Diffusion-Transformers-for-3D-Shape-Generation"><a href="#DiT-3D-Exploring-Plain-Diffusion-Transformers-for-3D-Shape-Generation" class="headerlink" title="DiT-3D: Exploring Plain Diffusion Transformers for 3D Shape Generation"></a>DiT-3D: Exploring Plain Diffusion Transformers for 3D Shape Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01831">http://arxiv.org/abs/2307.01831</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/DiT-3D/DiT-3D">https://github.com/DiT-3D/DiT-3D</a></li>
<li>paper_authors: Shentong Mo, Enze Xie, Ruihang Chu, Lewei Yao, Lanqing Hong, Matthias Nießner, Zhenguo Li</li>
<li>for: This paper is written for generating high-quality 3D point clouds using a novel Diffusion Transformer architecture, specifically designed for 3D shape generation.</li>
<li>methods: The paper proposes a novel Diffusion Transformer architecture called DiT-3D, which adapts the design philosophy of DiT but incorporates 3D positional and patch embeddings to adaptively aggregate input from voxelized point clouds. The paper also introduces 3D window attention to reduce computational cost in 3D shape generation.</li>
<li>results: The proposed DiT-3D achieves state-of-the-art performance in high-fidelity and diverse 3D point cloud generation on the ShapeNet dataset, with a 4.59 decrease in 1-Nearest Neighbor Accuracy and a 3.51 increase in Coverage metric compared to the state-of-the-art method.<details>
<summary>Abstract</summary>
Recent Diffusion Transformers (e.g., DiT) have demonstrated their powerful effectiveness in generating high-quality 2D images. However, it is still being determined whether the Transformer architecture performs equally well in 3D shape generation, as previous 3D diffusion methods mostly adopted the U-Net architecture. To bridge this gap, we propose a novel Diffusion Transformer for 3D shape generation, namely DiT-3D, which can directly operate the denoising process on voxelized point clouds using plain Transformers. Compared to existing U-Net approaches, our DiT-3D is more scalable in model size and produces much higher quality generations. Specifically, the DiT-3D adopts the design philosophy of DiT but modifies it by incorporating 3D positional and patch embeddings to adaptively aggregate input from voxelized point clouds. To reduce the computational cost of self-attention in 3D shape generation, we incorporate 3D window attention into Transformer blocks, as the increased 3D token length resulting from the additional dimension of voxels can lead to high computation. Finally, linear and devoxelization layers are used to predict the denoised point clouds. In addition, our transformer architecture supports efficient fine-tuning from 2D to 3D, where the pre-trained DiT-2D checkpoint on ImageNet can significantly improve DiT-3D on ShapeNet. Experimental results on the ShapeNet dataset demonstrate that the proposed DiT-3D achieves state-of-the-art performance in high-fidelity and diverse 3D point cloud generation. In particular, our DiT-3D decreases the 1-Nearest Neighbor Accuracy of the state-of-the-art method by 4.59 and increases the Coverage metric by 3.51 when evaluated on Chamfer Distance.
</details>
<details>
<summary>摘要</summary>
最近的扩散变换器（例如DiT）已经表现出了高质量的2D图像生成能力。然而，是否Transformer架构在3D形状生成中表现 similarly well，现在都是一个问题。因为前一些3D扩散方法主要采用U-Net架构。为了bridging这个差距，我们提出了一种新的3D扩散变换器，即DiT-3D，它可以直接对粗糙点云进行杂化处理，并使用平杂Transformers进行操作。相比现有的U-Net方法，我们的DiT-3D更加扩展性强，生成质量更高。具体来说，DiT-3D采用了Diffusion Transformer的设计哲学，但是将其修改为包括3D位置嵌入和补丁嵌入，以适应 voxelized点云的输入。为了降低3D形状生成中自我注意力的计算成本，我们引入了3D窗口注意力，并在Transformer块中应用。最后，我们使用线性和反粗糙层来预测净化后的点云。此外，我们的 transformer 架构支持高效的 fine-tuning 从2D到3D，其中预先训练的 DiT-2D  checkpoint 在 ImageNet 上可以显著提高 DiT-3D 的性能。实验结果表明，我们提出的 DiT-3D 在 ShapeNet 数据集上实现了状态可见的高精度和多样化3D点云生成。具体来说，我们的 DiT-3D 在1-Nearest Neighbor Accuracy 和 Coverage 指标上降低了state-of-the-art 方法的值，分别降低了4.59和3.51。
</details></li>
</ul>
<hr>
<h2 id="Deconstructing-Data-Reconstruction-Multiclass-Weight-Decay-and-General-Losses"><a href="#Deconstructing-Data-Reconstruction-Multiclass-Weight-Decay-and-General-Losses" class="headerlink" title="Deconstructing Data Reconstruction: Multiclass, Weight Decay and General Losses"></a>Deconstructing Data Reconstruction: Multiclass, Weight Decay and General Losses</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01827">http://arxiv.org/abs/2307.01827</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gon Buzaglo, Niv Haim, Gilad Yehudai, Gal Vardi, Yakir Oz, Yaniv Nikankin, Michal Irani</li>
<li>for: 这个研究的目的是探讨神经网络中训练样本的内存化现象，以及这种现象对神经网络的影响。</li>
<li>methods: 该研究使用了多层感知器和卷积神经网络进行重建训练样本的方法，并对不同的损失函数进行了探讨。</li>
<li>results: 研究发现，使用权重衰变 durante 训练可以提高神经网络的重建可能性，同时也影响了神经网络的性能。此外，研究还发现，在训练样本数量和神经网络neuron数量之间存在一定的关系。<details>
<summary>Abstract</summary>
Memorization of training data is an active research area, yet our understanding of the inner workings of neural networks is still in its infancy. Recently, Haim et al. (2022) proposed a scheme to reconstruct training samples from multilayer perceptron binary classifiers, effectively demonstrating that a large portion of training samples are encoded in the parameters of such networks. In this work, we extend their findings in several directions, including reconstruction from multiclass and convolutional neural networks. We derive a more general reconstruction scheme which is applicable to a wider range of loss functions such as regression losses. Moreover, we study the various factors that contribute to networks' susceptibility to such reconstruction schemes. Intriguingly, we observe that using weight decay during training increases reconstructability both in terms of quantity and quality. Additionally, we examine the influence of the number of neurons relative to the number of training samples on the reconstructability.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>> neural network 的吸收训练数据是一个活跃的研究领域，然而我们对其内部工作的理解仍然处于初期阶段。最近，海沃特等人（2022）提出了一种方案，可以从多层感知器二分类网络中重建训练样本，Effectively demonstrating that a large portion of training samples are encoded in the parameters of such networks。在这项工作中，我们将这些发现扩展到多类和卷积神经网络，并 derivate a more general reconstruction scheme 可以应用于更广泛的损失函数，如回归损失。此外，我们研究了不同因素对神经网络的重建性的影响，发现使用权重衰减 durante 训练可以提高重建性 both in terms of quantity and quality。此外，我们还研究了神经网络的 neurons 和训练样本的数量之间的关系。
</details></li>
</ul>
<hr>
<h2 id="Structural-Balance-and-Random-Walks-on-Complex-Networks-with-Complex-Weights"><a href="#Structural-Balance-and-Random-Walks-on-Complex-Networks-with-Complex-Weights" class="headerlink" title="Structural Balance and Random Walks on Complex Networks with Complex Weights"></a>Structural Balance and Random Walks on Complex Networks with Complex Weights</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01813">http://arxiv.org/abs/2307.01813</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Tian, Renaud Lambiotte</li>
<li>for: This paper focuses on the study of complex-weighted networks, specifically investigating their structural and dynamical properties when the weight matrix is Hermitian.</li>
<li>methods: The authors use concepts from signed graphs to classify complex-weighted networks based on structural balance and explore the shared spectral properties within each type. They also apply the results to characterize the dynamics of random walks on these networks.</li>
<li>results: The paper shows that local consensus can be achieved asymptotically when the graph is structurally balanced, while global consensus will be obtained when it is strictly unbalanced. The authors also propose a spectral clustering algorithm and explore the performance of the algorithm on both synthetic and real networks.<details>
<summary>Abstract</summary>
Complex numbers define the relationship between entities in many situations. A canonical example would be the off-diagonal terms in a Hamiltonian matrix in quantum physics. Recent years have seen an increasing interest to extend the tools of network science when the weight of edges are complex numbers. Here, we focus on the case when the weight matrix is Hermitian, a reasonable assumption in many applications, and investigate both structural and dynamical properties of the complex-weighted networks. Building on concepts from signed graphs, we introduce a classification of complex-weighted networks based on the notion of structural balance, and illustrate the shared spectral properties within each type. We then apply the results to characterise the dynamics of random walks on complex-weighted networks, where local consensus can be achieved asymptotically when the graph is structurally balanced, while global consensus will be obtained when it is strictly unbalanced. Finally, we explore potential applications of our findings by generalising the notion of cut, and propose an associated spectral clustering algorithm. We also provide further characteristics of the magnetic Laplacian, associating directed networks to complex-weighted ones. The performance of the algorithm is verified on both synthetic and real networks.
</details>
<details>
<summary>摘要</summary>
复杂数字定义了实体之间的关系在许多情况下。一个典型的例子是量子物理中的哈密顿矩阵中的偏置项。过去几年，有越来越多的研究者想要扩展网络科学中的工具，当Weight of edges是复数时。我们在这里关注Hermitian矩阵的情况，这是许多应用中的合理假设。我们研究了复数权重网络的结构和动态特性，并基于签名图的概念引入了复数权重网络的分类。我们发现在不同类型的网络中，存在共同的 спектраль性质。然后，我们应用结果来描述复杂权重网络上Random walk的动态，当网络是结构均衡的时，本地协同可以在极限上 achievable，而全球协同则需要网络是严格不均衡的。最后，我们探讨了我们的发现的应用，包括通过扩展割的概念和相关的 спектраль划分算法。我们还提供了复杂 Laplacian的性能，将导向网络与复数权重网络相关联。我们的实验表明，我们的算法在 synthetic 和实际网络上都能够 достичь好的性能。
</details></li>
</ul>
<hr>
<h2 id="Capturing-Local-Temperature-Evolution-during-Additive-Manufacturing-through-Fourier-Neural-Operators"><a href="#Capturing-Local-Temperature-Evolution-during-Additive-Manufacturing-through-Fourier-Neural-Operators" class="headerlink" title="Capturing Local Temperature Evolution during Additive Manufacturing through Fourier Neural Operators"></a>Capturing Local Temperature Evolution during Additive Manufacturing through Fourier Neural Operators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01804">http://arxiv.org/abs/2307.01804</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiangce Chen, Wenzhuo Xu, Martha Baldwin, Björn Nijhuis, Ton van den Boogaard, Noelia Grande Gutiérrez, Sneha Prabha Narra, Christopher McComb</li>
<li>for: 本研究旨在提高附加制造技术的性能，通过快速模拟热性能。</li>
<li>methods: 本文使用Fourier Neural Operator来捕捉加工过程中的本地温度演化。</li>
<li>results: 模型在numerical simulations中表现出高精度，并且可以在不同的几何体上保持通用性。In English, that would be:</li>
<li>for: The purpose of this research is to improve the performance of additive manufacturing technologies by quickly simulating thermal behavior.</li>
<li>methods: The paper uses Fourier Neural Operator to capture the local temperature evolution during the manufacturing process.</li>
<li>results: The model shows high accuracy in numerical simulations and maintains generalizability to different geometries.<details>
<summary>Abstract</summary>
High-fidelity, data-driven models that can quickly simulate thermal behavior during additive manufacturing (AM) are crucial for improving the performance of AM technologies in multiple areas, such as part design, process planning, monitoring, and control. However, the complexities of part geometries make it challenging for current models to maintain high accuracy across a wide range of geometries. Additionally, many models report a low mean square error (MSE) across the entire domain (part). However, in each time step, most areas of the domain do not experience significant changes in temperature, except for the heat-affected zones near recent depositions. Therefore, the MSE-based fidelity measurement of the models may be overestimated.   This paper presents a data-driven model that uses Fourier Neural Operator to capture the local temperature evolution during the additive manufacturing process. In addition, the authors propose to evaluate the model using the $R^2$ metric, which provides a relative measure of the model's performance compared to using mean temperature as a prediction. The model was tested on numerical simulations based on the Discontinuous Galerkin Finite Element Method for the Direct Energy Deposition process, and the results demonstrate that the model achieves high fidelity as measured by $R^2$ and maintains generalizability to geometries that were not included in the training process.
</details>
<details>
<summary>摘要</summary>
高精度、数据驱动的模型可以快速模拟附加制造过程中的热性能，这些模型在多个领域，如部件设计、过程规划、监测和控制方面，都有提高附加制造技术的表现。然而，部件的复杂 геометри Structure 使得当前的模型难以保持高精度 across 各种 geometries。此外，许多模型报告了 across 整个领域 ($part$) 的低 Mean Square Error ($MSE$)，但在每个时间步骤中，大多数领域并不经历 significannot 的温度变化，只有近 recent depositions 的热效应区域。因此，基于 $MSE$ 的模型准确性测试可能受到过度估计。本文提出了一种基于 Fourier Neural Operator 的数据驱动模型，用于捕捉附加制造过程中的本地温度演化。此外，作者们提议使用 $R^2$ 指标来评估模型的性能，$R^2$ 指标为模型的Relative 性能指标，可以与使用 Mean Temperature 作为预测的 $R^2$ 指标进行比较。模型在基于 Discontinuous Galerkin Finite Element Method 的数值 simulations 上进行测试，结果表明该模型在 $R^2$ 指标下达到了高准确性，并且可以在不包含在训练过程中的 geometry 上保持通用性。
</details></li>
</ul>
<hr>
<h2 id="Edge-aware-Multi-task-Network-for-Integrating-Quantification-Segmentation-and-Uncertainty-Prediction-of-Liver-Tumor-on-Multi-modality-Non-contrast-MRI"><a href="#Edge-aware-Multi-task-Network-for-Integrating-Quantification-Segmentation-and-Uncertainty-Prediction-of-Liver-Tumor-on-Multi-modality-Non-contrast-MRI" class="headerlink" title="Edge-aware Multi-task Network for Integrating Quantification Segmentation and Uncertainty Prediction of Liver Tumor on Multi-modality Non-contrast MRI"></a>Edge-aware Multi-task Network for Integrating Quantification Segmentation and Uncertainty Prediction of Liver Tumor on Multi-modality Non-contrast MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01798">http://arxiv.org/abs/2307.01798</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaojiao Xiao, Qinmin Hu, Guanghui Wang</li>
<li>for:  Liver tumor diagnosis and analysis</li>
<li>methods:  Multi-modality non-contrast magnetic resonance imaging (NCMRI) fusion, edge-aware feature aggregation module (EaFA), and multi-task learning</li>
<li>results:  Outperformed state-of-the-art methods with a dice similarity coefficient of 90.01$\pm$1.23 and a mean absolute error of 2.72$\pm$0.58 mm for MD.<details>
<summary>Abstract</summary>
Simultaneous multi-index quantification, segmentation, and uncertainty estimation of liver tumors on multi-modality non-contrast magnetic resonance imaging (NCMRI) are crucial for accurate diagnosis. However, existing methods lack an effective mechanism for multi-modality NCMRI fusion and accurate boundary information capture, making these tasks challenging. To address these issues, this paper proposes a unified framework, namely edge-aware multi-task network (EaMtNet), to associate multi-index quantification, segmentation, and uncertainty of liver tumors on the multi-modality NCMRI. The EaMtNet employs two parallel CNN encoders and the Sobel filters to extract local features and edge maps, respectively. The newly designed edge-aware feature aggregation module (EaFA) is used for feature fusion and selection, making the network edge-aware by capturing long-range dependency between feature and edge maps. Multi-tasking leverages prediction discrepancy to estimate uncertainty and improve segmentation and quantification performance. Extensive experiments are performed on multi-modality NCMRI with 250 clinical subjects. The proposed model outperforms the state-of-the-art by a large margin, achieving a dice similarity coefficient of 90.01$\pm$1.23 and a mean absolute error of 2.72$\pm$0.58 mm for MD. The results demonstrate the potential of EaMtNet as a reliable clinical-aided tool for medical image analysis.
</details>
<details>
<summary>摘要</summary>
simultanous多指标评估、分割和不确定度估计liver肿瘤在多Modal非contrast磁共振成像（NCMRI）中是诊断精准的关键。然而，现有方法缺乏有效的多Modal NCMRI融合机制和准确边界信息捕获机制，使这些任务变得困难。为解决这些问题，这篇论文提出了一个统一框架，即edge-aware多任务网络（EaMtNet），用于 associating multi-index评估、分割和不确定度估计liver肿瘤在多Modal NCMRI中。EaMtNet使用了两个并行的CNN Encoder和Sobel滤波器来提取本地特征和边图，分别。新设计的edge-aware特征聚合模块（EaFA）用于特征融合和选择，使网络变得edge-aware，捕捉特征和边图之间的长距离依赖关系。多任务利用预测差异来估计不确定度和提高分割和评估性能。广泛的实验在多Modal NCMRI上进行，涉及250名临床实验者。提出的模型在多Modal NCMRI中表现出色，达到了dice相似度系数90.01±1.23和平均绝对误差2.72±0.58mm for MD。结果表明EaMtNet可能成为一种可靠的临床辅助工具 для医学影像分析。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/05/cs.LG_2023_07_05/" data-id="cllsj9wy3000tuv88d50o10cv" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_07_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/05/cs.SD_2023_07_05/" class="article-date">
  <time datetime="2023-07-04T16:00:00.000Z" itemprop="datePublished">2023-07-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/05/cs.SD_2023_07_05/">cs.SD - 2023-07-05 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Why-can-big-bi-be-changed-to-bi-gbi-A-mathematical-model-of-syllabification-and-articulatory-synthesis"><a href="#Why-can-big-bi-be-changed-to-bi-gbi-A-mathematical-model-of-syllabification-and-articulatory-synthesis" class="headerlink" title="Why can big.bi be changed to bi.gbi? A mathematical model of syllabification and articulatory synthesis"></a>Why can big.bi be changed to bi.gbi? A mathematical model of syllabification and articulatory synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02299">http://arxiv.org/abs/2307.02299</a></li>
<li>repo_url: None</li>
<li>paper_authors: Frédéric Berthommier</li>
<li>for: 这篇论文是用来描述一种简化的语音合成模型，该模型包括四个阶段，用于计划语音动作和计算语音动态。</li>
<li>methods: 该模型使用了拓扑图和选择算子来计划语音动作，并使用了VLAM模型进行synthesis。</li>
<li>results: 该模型能够描述语音cluster的拼音、元音的拼音和语音的变化，并且能够模拟句子中的语音变化。<details>
<summary>Abstract</summary>
A simplified model of articulatory synthesis involving four stages is presented. The planning of articulatory gestures is based on syllable graphs with arcs and nodes that are implemented in a complex representation. This was first motivated by a reduction in the many-to-one relationship between articulatory parameters and formant space. This allows for consistent trajectory planning and computation of articulation dynamics with coordination and selection operators. The flow of articulatory parameters is derived from these graphs with four equations. Many assertions of Articulatory Phonology have been abandoned. This framework is adapted to synthesis using VLAM (a Maeda's model) and simulations are performed with syllables including main vowels and the plosives /b,d,g/ only. The model is able to describe consonant-vowel coarticulation, articulation of consonant clusters, and verbal transformations are seen as transitions of the syllable graph structure.
</details>
<details>
<summary>摘要</summary>
提出了一种简化的语音合成模型，包括四个阶段。词形图的规划基于句子树，实现了复杂的表示。这是由于减少了许多到形式空间的多对一关系。这使得运动规划和形成动力计算可以采用协调和选择运算。词形图中的四个方程描述了流体参数的流动。许多语音学理学的假设被抛弃。这种框架被适应到使用VLAM（Maeda的模型）进行合成，并在包括主元音和擦音 /b,d,g/ 的 syllables 中进行了模拟。该模型可以描述共振元音和擦音 cluster 的语音合成，以及 verb 转折的变化。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Multimodal-Approaches-for-Alzheimer’s-Disease-Detection-Using-Patient-Speech-Transcript-and-Audio-Data"><a href="#Exploring-Multimodal-Approaches-for-Alzheimer’s-Disease-Detection-Using-Patient-Speech-Transcript-and-Audio-Data" class="headerlink" title="Exploring Multimodal Approaches for Alzheimer’s Disease Detection Using Patient Speech Transcript and Audio Data"></a>Exploring Multimodal Approaches for Alzheimer’s Disease Detection Using Patient Speech Transcript and Audio Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02514">http://arxiv.org/abs/2307.02514</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shui-dun/multimodal_ad">https://github.com/shui-dun/multimodal_ad</a></li>
<li>paper_authors: Hongmin Cai, Xiaoke Huang, Zhengliang Liu, Wenxiong Liao, Haixing Dai, Zihao Wu, Dajiang Zhu, Hui Ren, Quanzheng Li, Tianming Liu, Xiang Li</li>
<li>for: 这个研究旨在检测阿尔ц海默病（AD）使用患者的语音和讲解数据，以便早期诊断这种疾病。</li>
<li>methods: 该研究使用预训练语言模型和图 neural network（GNN）构建语音和讲解的图，并从图中提取特征进行AD检测。此外，还使用了数据扩展技术，如同义词替换和GPT基于的扩展器，以解决小数据集问题。</li>
<li>results: 我们的实验结果表明，将语音和讲解数据扩展为更大的数据集可以提高AD检测的准确率。此外，我们还发现，将语音转换回原始音频，并使用它们进行对比学习可以提高AD检测的效果。<details>
<summary>Abstract</summary>
Alzheimer's disease (AD) is a common form of dementia that severely impacts patient health. As AD impairs the patient's language understanding and expression ability, the speech of AD patients can serve as an indicator of this disease. This study investigates various methods for detecting AD using patients' speech and transcripts data from the DementiaBank Pitt database. The proposed approach involves pre-trained language models and Graph Neural Network (GNN) that constructs a graph from the speech transcript, and extracts features using GNN for AD detection. Data augmentation techniques, including synonym replacement, GPT-based augmenter, and so on, were used to address the small dataset size. Audio data was also introduced, and WavLM model was used to extract audio features. These features were then fused with text features using various methods. Finally, a contrastive learning approach was attempted by converting speech transcripts back to audio and using it for contrastive learning with the original audio. We conducted intensive experiments and analysis on the above methods. Our findings shed light on the challenges and potential solutions in AD detection using speech and audio data.
</details>
<details>
<summary>摘要</summary>
阿尔茨海默病 (AD) 是一种常见的 демен茨病，它对病人的健康产生严重的影响。由于 AD 会导致病人语言理解和表达能力受损，因此病人的言语可以作为这种疾病的指标。这项研究通过使用患者的言语和文本数据，从德мент银行 Pit 数据库中提取数据，并利用预训练语言模型和图 neural network (GNN) 构建一个图，以提取特征用于 AD 检测。为了 Addressing the small dataset size, data augmentation techniques, including synonym replacement, GPT-based augmenter, and so on, were used.  Additionally, audio data was also introduced, and WavLM model was used to extract audio features. These features were then fused with text features using various methods. Finally, a contrastive learning approach was attempted by converting speech transcripts back to audio and using it for contrastive learning with the original audio. We conducted intensive experiments and analysis on the above methods. Our findings shed light on the challenges and potential solutions in AD detection using speech and audio data.Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Self-supervised-learning-with-diffusion-based-multichannel-speech-enhancement-for-speaker-verification-under-noisy-conditions"><a href="#Self-supervised-learning-with-diffusion-based-multichannel-speech-enhancement-for-speaker-verification-under-noisy-conditions" class="headerlink" title="Self-supervised learning with diffusion-based multichannel speech enhancement for speaker verification under noisy conditions"></a>Self-supervised learning with diffusion-based multichannel speech enhancement for speaker verification under noisy conditions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02244">http://arxiv.org/abs/2307.02244</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sandipana Dowerah, Ajinkya Kulkarni, Romain Serizel, Denis Jouvet</li>
<li>for: 提高噪音和折射环境下的人脸识别性能</li>
<li>methods: 使用多通道散度概率模型进行多通道speech减去，并利用自我超vised学习来 JOINTLY优化Diff-Filter和预训练的ECAPA-TDNN人脸识别模型</li>
<li>results: 在MultiSV多通道人脸识别dataset上进行评估，并显示在噪音多通道条件下获得显著的提高<details>
<summary>Abstract</summary>
The paper introduces Diff-Filter, a multichannel speech enhancement approach based on the diffusion probabilistic model, for improving speaker verification performance under noisy and reverberant conditions. It also presents a new two-step training procedure that takes the benefit of self-supervised learning. In the first stage, the Diff-Filter is trained by conducting timedomain speech filtering using a scoring-based diffusion model. In the second stage, the Diff-Filter is jointly optimized with a pre-trained ECAPA-TDNN speaker verification model under a self-supervised learning framework. We present a novel loss based on equal error rate. This loss is used to conduct selfsupervised learning on a dataset that is not labelled in terms of speakers. The proposed approach is evaluated on MultiSV, a multichannel speaker verification dataset, and shows significant improvements in performance under noisy multichannel conditions.
</details>
<details>
<summary>摘要</summary>
文章介绍了一种多通道语音提升方法——Diff-Filter，该方法基于扩散概率模型，用于提高听话人识别性能在噪音和频率干扰的情况下。文章还提出了一种新的两步训练方法，利用自动学习的优势。在第一个阶段，Diff-Filter通过进行时间频谱语音筛选，使用得分基于扩散模型进行训练。在第二个阶段，Diff-Filter与预训练的 ECAPA-TDNN  speaker识别模型进行共同优化，使用一种新的平等错误率损失函数进行自动学习。我们在 MultiSV 多通道听话人识别 dataset 进行评估，并发现该方法在噪音多通道情况下显著提高了性能。
</details></li>
</ul>
<hr>
<h2 id="LOAF-M2L-Joint-Learning-of-Wording-and-Formatting-for-Singable-Melody-to-Lyric-Generation"><a href="#LOAF-M2L-Joint-Learning-of-Wording-and-Formatting-for-Singable-Melody-to-Lyric-Generation" class="headerlink" title="LOAF-M2L: Joint Learning of Wording and Formatting for Singable Melody-to-Lyric Generation"></a>LOAF-M2L: Joint Learning of Wording and Formatting for Singable Melody-to-Lyric Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02146">http://arxiv.org/abs/2307.02146</a></li>
<li>repo_url: None</li>
<li>paper_authors: Longshen Ou, Xichu Ma, Ye Wang</li>
<li>for:  bridges the singability gap between generated lyrics and melodies, improving the compatibility of the outputs with the melody.</li>
<li>methods:  jointly Learning wOrding And Formatting during Melody-to-Lyric training (LOAF-M2L), with a new objective informed by musicological research on the relationship between melody and lyrics.</li>
<li>results:  achieves 3.75% and 21.44% absolute accuracy gains in the outputs’ number-of-line and syllable-per-line requirements, and demonstrates a 63.92% and 74.18% relative improvement of music-lyric compatibility and overall quality in the subjective evaluation, compared to the state-of-the-art melody-to-lyric generation model.<details>
<summary>Abstract</summary>
Despite previous efforts in melody-to-lyric generation research, there is still a significant compatibility gap between generated lyrics and melodies, negatively impacting the singability of the outputs. This paper bridges the singability gap with a novel approach to generating singable lyrics by jointly Learning wOrding And Formatting during Melody-to-Lyric training (LOAF-M2L). After general-domain pretraining, our proposed model acquires length awareness first from a large text-only lyric corpus. Then, we introduce a new objective informed by musicological research on the relationship between melody and lyrics during melody-to-lyric training, which enables the model to learn the fine-grained format requirements of the melody. Our model achieves 3.75% and 21.44% absolute accuracy gains in the outputs' number-of-line and syllable-per-line requirements compared to naive fine-tuning, without sacrificing text fluency. Furthermore, our model demonstrates a 63.92% and 74.18% relative improvement of music-lyric compatibility and overall quality in the subjective evaluation, compared to the state-of-the-art melody-to-lyric generation model, highlighting the significance of formatting learning.
</details>
<details>
<summary>摘要</summary>
尽管之前的尝试在旋律到歌词生成研究中已经做出了很多努力，但是还是存在很大的兼容性差距，这对生成的输出有负面影响。这篇论文通过一种新的approach来bridging这个兼容性差距，即在melody-to-lyric训练中同时学习wording和 formatting（LOAF-M2L）。在通用领域预训练后，我们的提议的模型首先从大量的文本只lyric corpus中获得了长度意识。然后，我们引入了基于音乐学研究的旋律和歌词之间的关系的新目标，使模型学习细腻的格式要求。我们的模型在输出的数量线和音节数要求上做出了3.75%和21.44%的绝对准确率提升，而无需牺牲文本流畅性。此外，我们的模型在主观评估中表现出了63.92%和74.18%的相对提升，相比之前的状态对旋律到歌词生成模型，这 highlights the significance of formatting learning。
</details></li>
</ul>
<hr>
<h2 id="Going-Retro-Astonishingly-Simple-Yet-Effective-Rule-based-Prosody-Modelling-for-Speech-Synthesis-Simulating-Emotion-Dimensions"><a href="#Going-Retro-Astonishingly-Simple-Yet-Effective-Rule-based-Prosody-Modelling-for-Speech-Synthesis-Simulating-Emotion-Dimensions" class="headerlink" title="Going Retro: Astonishingly Simple Yet Effective Rule-based Prosody Modelling for Speech Synthesis Simulating Emotion Dimensions"></a>Going Retro: Astonishingly Simple Yet Effective Rule-based Prosody Modelling for Speech Synthesis Simulating Emotion Dimensions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02132">http://arxiv.org/abs/2307.02132</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/felixbur/syntact">https://github.com/felixbur/syntact</a></li>
<li>paper_authors: Felix Burkhardt, Uwe Reichel, Florian Eyben, Björn Schuller</li>
<li>for: 这个论文是为了研究如何通过 modify 语音合成的 просоди来模拟情感表达。</li>
<li>methods: 这两个规则基于的模型使用 speech synthesis markup language (SSML) 来控制语音合成的 просоди，以模拟情感表达。</li>
<li>results: Results indicate that with a very simple method both dimensions arousal (.76 UAR) and valence (.43 UAR) can be simulated。<details>
<summary>Abstract</summary>
We introduce two rule-based models to modify the prosody of speech synthesis in order to modulate the emotion to be expressed. The prosody modulation is based on speech synthesis markup language (SSML) and can be used with any commercial speech synthesizer. The models as well as the optimization result are evaluated against human emotion annotations. Results indicate that with a very simple method both dimensions arousal (.76 UAR) and valence (.43 UAR) can be simulated.
</details>
<details>
<summary>摘要</summary>
我们介绍两个规律模型，用以 modify 语音合成中的情感表达。这些模型基于语音合成标记语言（SSML），可以与任何商业语音合成器使用。我们将这些模型以及优化结果评估于人类情感标注。结果显示，使用非常简单的方法，我们可以模拟出两个维度的情感表达，即情感刺激（.76 UAR）和情感负面（.43 UAR）。
</details></li>
</ul>
<hr>
<h2 id="A-Database-with-Directivities-of-Musical-Instruments"><a href="#A-Database-with-Directivities-of-Musical-Instruments" class="headerlink" title="A Database with Directivities of Musical Instruments"></a>A Database with Directivities of Musical Instruments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02110">http://arxiv.org/abs/2307.02110</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Ackermann, Fabian Brinkmann, Stefan Weinzierl</li>
<li>for: 这个论文是为了提供41种现代和历史乐器的听录和射频模式数据，并计算每种乐器的一三 octave 带的直达性平均值，这些数据适用于音响模拟和听觉 simulate。</li>
<li>methods: 这个论文使用了32道球形 Mikrofon array 在零噪条件下测量乐器的听录和射频模式，并计算每种乐器的直达性平均值，然后使用球面spline interpolating 进行空间 upsampling，并转换为 OpenDAFF 和 GLL 格式，以便在房间声学和电子声学模拟软件中使用。</li>
<li>results: 这个论文提供了41种乐器的听录和射频模式数据，包括每种乐器的直达性平均值，以及在不同的射频带width 下的射频模式。这些数据适用于音响模拟和听觉 simulate，并且可以用于创建真实的听觉效果。<details>
<summary>Abstract</summary>
We present a database of recordings and radiation patterns of individual notes for 41 modern and historical musical instruments, measured with a 32-channel spherical microphone array in anechoic conditions. In addition, directivities averaged in one-third octave bands have been calculated for each instrument, which are suitable for use in acoustic simulation and auralisation. The data are provided in SOFA format. Spatial upsampling of the directivities was performed based on spherical spline interpolation and converted to OpenDAFF and GLL format for use in room acoustic and electro-acoustic simulation software. For this purpose, a method is presented how these directivities can be referenced to a specific microphone position in order to achieve a physically correct auralisation without colouration. The data is available under the CC BY-SA 4.0 licence.
</details>
<details>
<summary>摘要</summary>
我们提供了41种现代和历史乐器的录音和辐射模式数据，使用32道球形 микро链测量在闭合条件下。此外，我们还计算了每种乐器的一半 ок塔频率带的直达性平均值，这些数据适用于音响 simulation 和听众化。数据以SOFA格式提供，并使用球面插值和OpenDAFF/GLL格式进行空间上抽象，以便在房间音响和电子音响 simulate 软件中使用。为实现物理正确的听众化而不受抹音影响，我们还提供了一种方法，以便将这些直达性参考到特定的 Microphone 位置。数据以CC BY-SA 4.0 许可证提供。
</details></li>
</ul>
<hr>
<h2 id="Flowchase-a-Mobile-Application-for-Pronunciation-Training"><a href="#Flowchase-a-Mobile-Application-for-Pronunciation-Training" class="headerlink" title="Flowchase: a Mobile Application for Pronunciation Training"></a>Flowchase: a Mobile Application for Pronunciation Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02051">http://arxiv.org/abs/2307.02051</a></li>
<li>repo_url: None</li>
<li>paper_authors: Noé Tits, Zoé Broisson</li>
<li>for: 提供个性化、实时反馈给英语学习者</li>
<li>methods: 使用流处理应用程序和语音技术分析语音段和超段特征</li>
<li>results: 通过组合机器学习模型实现联合强制对齐和音名识别，提供了对多个段和超段发音方面的反馈设计Here’s a breakdown of each point:</li>
<li>for: The paper is written for providing personalized and instant feedback to English learners.</li>
<li>methods: The paper uses a mobile application called Flowchase and a speech technology that can segment and analyze speech segmental and supra-segmental features. The speech processing pipeline receives linguistic information and a speech sample, and then performs joint forced-alignment and phonetic recognition using machine learning models based on speech representation learning.</li>
<li>results: The paper achieves accurate recognition of segmental and supra-segmental pronunciation aspects using a combination of machine learning models, which enables the provision of personalized and instant feedback to English learners.<details>
<summary>Abstract</summary>
In this paper, we present a solution for providing personalized and instant feedback to English learners through a mobile application, called Flowchase, that is connected to a speech technology able to segment and analyze speech segmental and supra-segmental features. The speech processing pipeline receives linguistic information corresponding to an utterance to analyze along with a speech sample. After validation of the speech sample, a joint forced-alignment and phonetic recognition is performed thanks to a combination of machine learning models based on speech representation learning that provides necessary information for designing a feedback on a series of segmental and supra-segmental pronunciation aspects.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种解决方案，通过一款名为Flowchase的移动应用程序，为英语学习者提供个性化和实时反馈。这个应用程序与一种可以分segment和supra-segment的speech技术相连接，以进行语音处理管道。在语音处理管道中，我们使用机器学习模型来学习语音表示学习，以获得 необходимы的信息，以设计一系列的segmental和supra-segmental发音方面的反馈。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/05/cs.SD_2023_07_05/" data-id="cllsj9wyu003buv889wno2be8" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/14/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/13/">13</a><a class="page-number" href="/page/14/">14</a><span class="page-number current">15</span><a class="page-number" href="/page/16/">16</a><a class="page-number" href="/page/17/">17</a><a class="extend next" rel="next" href="/page/16/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CR/">cs.CR</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">43</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">42</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">44</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">53</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">114</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'', root:''}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
