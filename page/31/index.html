
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/31/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.CL_2023_10_10" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/10/cs.CL_2023_10_10/" class="article-date">
  <time datetime="2023-10-10T11:00:00.000Z" itemprop="datePublished">2023-10-10</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/10/cs.CL_2023_10_10/">cs.CL - 2023-10-10</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Crossing-the-Threshold-Idiomatic-Machine-Translation-through-Retrieval-Augmentation-and-Loss-Weighting"><a href="#Crossing-the-Threshold-Idiomatic-Machine-Translation-through-Retrieval-Augmentation-and-Loss-Weighting" class="headerlink" title="Crossing the Threshold: Idiomatic Machine Translation through Retrieval Augmentation and Loss Weighting"></a>Crossing the Threshold: Idiomatic Machine Translation through Retrieval Augmentation and Loss Weighting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07081">http://arxiv.org/abs/2310.07081</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nightingal3/idiom-translation">https://github.com/nightingal3/idiom-translation</a></li>
<li>paper_authors: Emmy Liu, Aditi Chaudhary, Graham Neubig</li>
<li>for: 本研究旨在提高机器翻译系统对idiomatic表达的翻译能力。</li>
<li>methods: 本研究使用transformer型机器翻译模型，并提出了两种简单 yet有效的技巧来提高翻译效果：一是策略性地增加训练损失的权重，二是使用检索支持模型。</li>
<li>results: 研究发现，使用这两种技巧可以提高一个强有力的预训练机器翻译模型对idiomatic sentences的翻译精度，最高提高13%。此外，这些技巧还可以对非idiomatic sentences进行改进。<details>
<summary>Abstract</summary>
Idioms are common in everyday language, but often pose a challenge to translators because their meanings do not follow from the meanings of their parts. Despite significant advances, machine translation systems still struggle to translate idiomatic expressions. We provide a simple characterization of idiomatic translation and related issues. This allows us to conduct a synthetic experiment revealing a tipping point at which transformer-based machine translation models correctly default to idiomatic translations. To expand multilingual resources, we compile a dataset of ~4k natural sentences containing idiomatic expressions in French, Finnish, and Japanese. To improve translation of natural idioms, we introduce two straightforward yet effective techniques: the strategic upweighting of training loss on potentially idiomatic sentences, and using retrieval-augmented models. This not only improves the accuracy of a strong pretrained MT model on idiomatic sentences by up to 13% in absolute accuracy, but also holds potential benefits for non-idiomatic sentences.
</details>
<details>
<summary>摘要</summary>
idioms 是日常语言中很常见的表达方式，但是它们的意思并不是由其部件的意思所推导出来。虽然有了 significiant advances，机器翻译系统仍然难以翻译idiomatic表达。我们提供了一个简单的idiomatic翻译特征化和相关问题。这允许我们进行一个 sintethic experiment，揭示了使用 transformer-based 机器翻译模型时，正确地采用idiomatic翻译的tipping point。为扩展多语言资源，我们编译了 ~4k 自然句子中包含idiomatic表达的 French、Finland 和 Japanese 语言数据集。为了改进自然idiomatic翻译，我们介绍了两种简单 yet effective 技术：一是对潜在idiomatic句子的训练损失进行战略性增加，二是使用retrieval-augmented模型。这不仅提高了一个强制trained MT模型在idiomatic句子上的准确率，还有可能对非idiomatic句子产生正面的影响。
</details></li>
</ul>
<hr>
<h2 id="Automatic-Macro-Mining-from-Interaction-Traces-at-Scale"><a href="#Automatic-Macro-Mining-from-Interaction-Traces-at-Scale" class="headerlink" title="Automatic Macro Mining from Interaction Traces at Scale"></a>Automatic Macro Mining from Interaction Traces at Scale</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07023">http://arxiv.org/abs/2310.07023</a></li>
<li>repo_url: None</li>
<li>paper_authors: Forrest Huang, Gang Li, Tao Li, Yang Li</li>
<li>for: 本研究旨在自动从移动应用程序中提取含义强大的 macro，以便更好地理解移动交互和实现任务自动化。</li>
<li>methods: 本研究提出了一种基于大型自然语言模型（LLM）的方法，可以自动从随机和用户自定义的移动交互轨迹中提取含义强大的 macro。这些 macro 被自动标记为自然语言描述，并且可以完全执行。</li>
<li>results: 研究人员通过多种研究，包括用户评估、比较分析和自动执行这些 macro，证明了本approach的有效性和提取的 macro 在下游应用中的有用性。<details>
<summary>Abstract</summary>
Macros are building block tasks of our everyday smartphone activity (e.g., "login", or "booking a flight"). Effectively extracting macros is important for understanding mobile interaction and enabling task automation. These macros are however difficult to extract at scale as they can be comprised of multiple steps yet hidden within programmatic components of the app. In this paper, we introduce a novel approach based on Large Language Models (LLMs) to automatically extract semantically meaningful macros from both random and user-curated mobile interaction traces. The macros produced by our approach are automatically tagged with natural language descriptions and are fully executable. To examine the quality of extraction, we conduct multiple studies, including user evaluation, comparative analysis against human-curated tasks, and automatic execution of these macros. These experiments and analyses show the effectiveness of our approach and the usefulness of extracted macros in various downstream applications.
</details>
<details>
<summary>摘要</summary>
macro 是我们每天手机活动的基本构建块（例如，"登录" 或 "预订航班")。抽取macro有助于理解移动交互和实现任务自动化。但是，由于这些macro可能由多个步骤组成，并且隐藏在应用程序的编程组件中，因此EXTRACTING MACROS AT SCALE 是一项重要的挑战。在这篇论文中，我们提出了一种基于大语言模型（LLMs）的新方法，可以自动抽取手机交互轨迹中的semantically meaningful macro。这些macro被自动标记为自然语言描述，并且可以自动执行。为了评估EXTRACTING MACROS的质量，我们进行了多个研究，包括用户评估、对人工Curate任务进行比较分析，以及自动执行这些macro。这些实验和分析表明了我们的方法的有效性和抽取的macro的多种下游应用。
</details></li>
</ul>
<hr>
<h2 id="LLMs-as-Potential-Brainstorming-Partners-for-Math-and-Science-Problems"><a href="#LLMs-as-Potential-Brainstorming-Partners-for-Math-and-Science-Problems" class="headerlink" title="LLMs as Potential Brainstorming Partners for Math and Science Problems"></a>LLMs as Potential Brainstorming Partners for Math and Science Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10677">http://arxiv.org/abs/2310.10677</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sophia Gu</li>
<li>for: 这种研究的目的是探索现代深度学习模型在与人类合作解决复杂数学和科学问题时的能力。</li>
<li>methods: 这项研究使用了大量语言模型（LLMs）的最新进展，特别是GPT-4模型，进行了详细的案例研究，以探索这些模型在人类合作brainstorming中的能力和局限性。</li>
<li>results: 研究发现，当前的state-of-the-art LLMs在collective brainstorming中表现出了扎实的能力，并且可以帮助人类解决一些复杂的数学和科学问题。但是，这些模型也存在一些局限性和缺陷，需要进一步的改进和调整。<details>
<summary>Abstract</summary>
With the recent rise of widely successful deep learning models, there is emerging interest among professionals in various math and science communities to see and evaluate the state-of-the-art models' abilities to collaborate on finding or solving problems that often require creativity and thus brainstorming. While a significant chasm still exists between current human-machine intellectual collaborations and the resolution of complex math and science problems, such as the six unsolved Millennium Prize Problems, our initial investigation into this matter reveals a promising step towards bridging the divide. This is due to the recent advancements in Large Language Models (LLMs). More specifically, we conduct comprehensive case studies to explore both the capabilities and limitations of the current state-of-the-art LLM, notably GPT-4, in collective brainstorming with humans.
</details>
<details>
<summary>摘要</summary>
Recently, with the rise of widely successful deep learning models, there is growing interest among professionals in various math and science communities to see and evaluate the state-of-the-art models' abilities to collaborate on finding or solving problems that often require creativity and thus brainstorming. Although a significant gap still exists between current human-machine intellectual collaborations and the resolution of complex math and science problems, such as the six unsolved Millennium Prize Problems, our preliminary investigation into this matter reveals a promising step towards bridging the divide. This is due to the recent advancements in Large Language Models (LLMs). Specifically, we conduct comprehensive case studies to explore both the capabilities and limitations of the current state-of-the-art LLM, notably GPT-4, in collective brainstorming with humans.
</details></li>
</ul>
<hr>
<h2 id="Violation-of-Expectation-via-Metacognitive-Prompting-Reduces-Theory-of-Mind-Prediction-Error-in-Large-Language-Models"><a href="#Violation-of-Expectation-via-Metacognitive-Prompting-Reduces-Theory-of-Mind-Prediction-Error-in-Large-Language-Models" class="headerlink" title="Violation of Expectation via Metacognitive Prompting Reduces Theory of Mind Prediction Error in Large Language Models"></a>Violation of Expectation via Metacognitive Prompting Reduces Theory of Mind Prediction Error in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06983">http://arxiv.org/abs/2310.06983</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/plastic-labs/voe-paper-eval">https://github.com/plastic-labs/voe-paper-eval</a></li>
<li>paper_authors: Courtland Leer, Vincent Trost, Vineeth Voruganti</li>
<li>for: 本研究旨在探讨 Large Language Models (LLMs) 在理解人类心理的能力是如何提高的。</li>
<li>methods: 本研究使用了一种 Developmental psychology 中的机制 known as Violation of Expectation (VoE)，以减少 LLM 预测用户的错误。并提出了一个 \textit{metacognitive prompting} 框架来应用 VoE 在 AI 教育中。</li>
<li>results: 研究发现，通过存储和检索在 LLM 对用户预期的情况下出现的事实，LLMs 能够学习关于用户的知识。最后，研究探讨了模型用户心理的潜在危险和可能的未来研究方向。<details>
<summary>Abstract</summary>
Recent research shows that Large Language Models (LLMs) exhibit a compelling level of proficiency in Theory of Mind (ToM) tasks. This ability to impute unobservable mental states to others is vital to human social cognition and may prove equally important in principal-agent relations between individual humans and Artificial Intelligences (AIs). In this paper, we explore how a mechanism studied in developmental psychology known as Violation of Expectation (VoE) can be implemented to reduce errors in LLM prediction about users by leveraging emergent ToM affordances. And we introduce a \textit{metacognitive prompting} framework to apply VoE in the context of an AI tutor. By storing and retrieving facts derived in cases where LLM expectation about the user was violated, we find that LLMs are able to learn about users in ways that echo theories of human learning. Finally, we discuss latent hazards and augmentative opportunities associated with modeling user psychology and propose ways to mitigate risk along with possible directions for future inquiry.
</details>
<details>
<summary>摘要</summary>
现代研究显示大语言模型（LLM）在理论心理（ToM）任务中表现出吸引人的水平。这种能够推理他人隐藏的心理状态的能力是人类社交认知的核心，可能对人工智能（AI）和人之间的主体-代理关系也非常重要。在这篇论文中，我们探讨了在发展心理学中研究的违反预期（VoE）机制，以减少LLM预测用户时的错误。我们还提出了一种“认知推导”框架，用于在AI教育者中应用VoE。通过存储和重新获取在LLM预测用户时出现的情况中的事实，我们发现LLM能够通过对用户学习方式的模拟来学习用户。最后，我们讨论了模型用户心理的潜在危险和可能的发展方向，并提出了降低风险的方法。
</details></li>
</ul>
<hr>
<h2 id="Why-bother-with-geometry-On-the-relevance-of-linear-decompositions-of-Transformer-embeddings"><a href="#Why-bother-with-geometry-On-the-relevance-of-linear-decompositions-of-Transformer-embeddings" class="headerlink" title="Why bother with geometry? On the relevance of linear decompositions of Transformer embeddings"></a>Why bother with geometry? On the relevance of linear decompositions of Transformer embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06977">http://arxiv.org/abs/2310.06977</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/timotheemickus/seq2seq-splat">https://github.com/timotheemickus/seq2seq-splat</a></li>
<li>paper_authors: Timothee Mickus, Raúl Vázquez</li>
<li>for: 这个研究旨在研究Transformer嵌入的线性分解是否有实际意义。</li>
<li>methods: 这个研究使用了两种嵌入分解方法来研究机器翻译decoder的表示。</li>
<li>results: 研究结果表明，嵌入分解指标与模型性能显示正相关，但是在不同的运行中存在很大的变化，表明geometry更反映模型特有的特征而不是句子特定的计算。<details>
<summary>Abstract</summary>
A recent body of work has demonstrated that Transformer embeddings can be linearly decomposed into well-defined sums of factors, that can in turn be related to specific network inputs or components. There is however still a dearth of work studying whether these mathematical reformulations are empirically meaningful. In the present work, we study representations from machine-translation decoders using two of such embedding decomposition methods. Our results indicate that, while decomposition-derived indicators effectively correlate with model performance, variation across different runs suggests a more nuanced take on this question. The high variability of our measurements indicate that geometry reflects model-specific characteristics more than it does sentence-specific computations, and that similar training conditions do not guarantee similar vector spaces.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Jaynes-Machine-The-universal-microstructure-of-deep-neural-networks"><a href="#Jaynes-Machine-The-universal-microstructure-of-deep-neural-networks" class="headerlink" title="Jaynes Machine: The universal microstructure of deep neural networks"></a>Jaynes Machine: The universal microstructure of deep neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06960">http://arxiv.org/abs/2310.06960</a></li>
<li>repo_url: None</li>
<li>paper_authors: Venkat Venkatasubramanian, N. Sanjeevrajan, Manasi Khandekar</li>
<li>for: 这 paper 的目的是提出一种新的深度神经网络的微结构理论。</li>
<li>methods: 这 paper 使用了一种名为统计电动力学的概念总结，它是统计 термо动力学和潜在游戏理论的概念合并。这种理论预测了深度神经网络中所有高度连接层的连接强度分布为 Lognormal（$LN(\mu, \sigma)$），并且在理想条件下，$\mu$ 和 $\sigma$ 在所有层次和所有网络中都相同。这是因为所有连接在竞争和贡献效用方面达到了平衡，从而实现了总损失函数的最小化。</li>
<li>results: 这 paper 通过对六个大规模的深度神经网络实际数据进行验证，证明了这些预测的正确性。此外，这 paper 还讨论了如何利用这些结果来降低训练大深度神经网络所需的数据、时间和计算资源。<details>
<summary>Abstract</summary>
We present a novel theory of the microstructure of deep neural networks. Using a theoretical framework called statistical teleodynamics, which is a conceptual synthesis of statistical thermodynamics and potential game theory, we predict that all highly connected layers of deep neural networks have a universal microstructure of connection strengths that is distributed lognormally ($LN({\mu}, {\sigma})$). Furthermore, under ideal conditions, the theory predicts that ${\mu}$ and ${\sigma}$ are the same for all layers in all networks. This is shown to be the result of an arbitrage equilibrium where all connections compete and contribute the same effective utility towards the minimization of the overall loss function. These surprising predictions are shown to be supported by empirical data from six large-scale deep neural networks in real life. We also discuss how these results can be exploited to reduce the amount of data, time, and computational resources needed to train large deep neural networks.
</details>
<details>
<summary>摘要</summary>
我团队提出了一种新的深度神经网络微结构理论。使用一种名为统计电动力学的理论框架，这是统计 термодинами学和潜在游戏理论的概念合成。我们预测了所有深度神经网络中高度连接层的微结构强度分布为Lognormal（$LN(\mu, \sigma)$）。此外，在理想情况下，我们预测$\mu$和$\sigma$在所有层次和所有网络中都相同。这是因为所有连接都在竞争和贡献同样的有效利用于最小化总损失函数。这些意外预测得到了实际数据中6个大规模深度神经网络的支持。我们还讨论了如何利用这些结果减少训练大深度神经网络所需的数据、时间和计算资源。
</details></li>
</ul>
<hr>
<h2 id="Creation-Of-A-ChatBot-Based-On-Natural-Language-Proccesing-For-Whatsapp"><a href="#Creation-Of-A-ChatBot-Based-On-Natural-Language-Proccesing-For-Whatsapp" class="headerlink" title="Creation Of A ChatBot Based On Natural Language Proccesing For Whatsapp"></a>Creation Of A ChatBot Based On Natural Language Proccesing For Whatsapp</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10675">http://arxiv.org/abs/2310.10675</a></li>
<li>repo_url: None</li>
<li>paper_authors: Valderrama Jonatan, Aguilar-Alonso Igor</li>
<li>for: 提高客户满意度和公司服务质量 through WhatsApp chatbot</li>
<li>methods: 基于自然语言处理的 chatbot 开发</li>
<li>results: 实现快速和准确的回答，提高客户服务效率和客户满意度Here’s the simplified Chinese text for each point:</li>
<li>for: 通过 WhatsApp chatbot 提高客户满意度和公司服务质量</li>
<li>methods: 基于自然语言处理的 chatbot 开发</li>
<li>results: 实现快速和准确的回答，提高客户服务效率和客户满意度<details>
<summary>Abstract</summary>
In the era of digital transformation, customer service is of paramount importance to the success of organizations, and to meet the growing demand for immediate responses and personalized assistance 24 hours a day, chatbots have become a promising tool to solve these problems. Currently, there are many companies that need to provide these solutions to their customers, which motivates us to study this problem and offer a suitable solution. The objective of this study is to develop a chatbot based on natural language processing to improve customer satisfaction and improve the quality of service provided by the company through WhatsApp. The solution focuses on creating a chatbot that efficiently and effectively handles user queries. A literature review related to existing chatbots has been conducted, analyzing methodological approaches, artificial intelligence techniques and quality attributes used in the implementation of chatbots. The results found highlight that chatbots based on natural language processing enable fast and accurate responses, which improves the efficiency of customer service, as chatbots contribute to customer satisfaction by providing accurate answers and quick solutions to their queries at any time. Some authors point out that artificial intelligence techniques, such as machine learning, improve the learning and adaptability of chatbots as user interactions occur, so a good choice of appropriate natural language understanding technologies is essential for optimal chatbot performance. The results of this study will provide a solid foundation for the design and development of effective chatbots for customer service, ensuring a satisfactory user experience and thus meeting the needs of the organization.
</details>
<details>
<summary>摘要</summary>
在数字化转型时代，客户服务对组织的成功非常重要，为了应对增长的快速响应和个性化帮助需求，聊天机器人已成为一种有前途的解决方案。目前有很多公司需要为客户提供这些解决方案，这使我们感到需要研究这个问题并提供适合的解决方案。本研究的目标是开发基于自然语言处理的聊天机器人，以提高客户满意度和公司向客户提供的服务质量。解决方案关注于创建高效高质量的聊天机器人，以快速和准确地处理用户查询。在现有聊天机器人的研究中，我们进行了文献综述，分析了方法ológicas approached,人工智能技术和质量特征在聊天机器人的实施中使用。结果显示，基于自然语言处理的聊天机器人可以快速和准确地回答用户查询，从而提高客户服务的效率，因为聊天机器人可以为客户提供快速和准确的答案，使用户满意度提高。一些作者指出，人工智能技术，如机器学习，可以使聊天机器人在用户互动时进行学习和适应，因此选择合适的自然语言理解技术是聊天机器人性能优化的关键。本研究的结果将为聊天机器人的设计和开发提供坚实的基础，确保用户体验满意，从而满足组织的需求。
</details></li>
</ul>
<hr>
<h2 id="Document-Level-Supervision-for-Multi-Aspect-Sentiment-Analysis-Without-Fine-grained-Labels"><a href="#Document-Level-Supervision-for-Multi-Aspect-Sentiment-Analysis-Without-Fine-grained-Labels" class="headerlink" title="Document-Level Supervision for Multi-Aspect Sentiment Analysis Without Fine-grained Labels"></a>Document-Level Supervision for Multi-Aspect Sentiment Analysis Without Fine-grained Labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06940">http://arxiv.org/abs/2310.06940</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kasturi Bhattacharjee, Rashmi Gangadharaiah</li>
<li>for: This paper proposes a VAE-based topic modeling approach for aspect-based sentiment analysis (ABSA) that does not require fine-grained labels for aspects or sentiments.</li>
<li>methods: The proposed approach uses document-level supervision and leverages user-generated text with overall sentiment to detect multiple aspects in a document and reason about their contributions to the overall sentiment.</li>
<li>results: The approach significantly outperforms a state-of-the-art baseline on two benchmark datasets from different domains.Here’s the text in Simplified Chinese:</li>
<li>for: 这篇论文提出了一种基于VAE的话题模型方法，用于无监督的方面情感分析（ABSA），不需要细化的标签 для方面或情感。</li>
<li>methods: 该方法使用文档级别的监督，利用用户生成的文本中的总情感来探测文档中的多个方面，并将这些方面的情感相互综合来理解整个文档的情感。</li>
<li>results: 该方法在两个不同领域的两个标准 benchmark 数据集上显著超越了一个状态监督的基准。<details>
<summary>Abstract</summary>
Aspect-based sentiment analysis (ABSA) is a widely studied topic, most often trained through supervision from human annotations of opinionated texts. These fine-grained annotations include identifying aspects towards which a user expresses their sentiment, and their associated polarities (aspect-based sentiments). Such fine-grained annotations can be expensive and often infeasible to obtain in real-world settings. There is, however, an abundance of scenarios where user-generated text contains an overall sentiment, such as a rating of 1-5 in user reviews or user-generated feedback, which may be leveraged for this task. In this paper, we propose a VAE-based topic modeling approach that performs ABSA using document-level supervision and without requiring fine-grained labels for either aspects or sentiments. Our approach allows for the detection of multiple aspects in a document, thereby allowing for the possibility of reasoning about how sentiment expressed through multiple aspects comes together to form an observable overall document-level sentiment. We demonstrate results on two benchmark datasets from two different domains, significantly outperforming a state-of-the-art baseline.
</details>
<details>
<summary>摘要</summary>
《方面基于情感分析（ABSA）是一个广泛研究的话题，通常通过人类注释的意见文本进行培育。这些细化的注释包括确定用户表达情感的方面以及其相关的负面性（方面基于情感）。然而，在实际场景中获得这些细化注释可能是昂贵的和不可能完成的。在这篇论文中，我们提出了基于VAE的话题模型方法，用于实现ABSA，不需要文本级别的细化标注，也不需要方面或情感的细化标注。我们的方法允许文档中检测多个方面，从而允许理解多个方面的情感如何共同形成可见的总文档级别的情感。我们在两个不同领域的两个标准 benchmark 数据集上进行了实验，并在比较一个基eline之下显著地提高了性能。》Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format instead.
</details></li>
</ul>
<hr>
<h2 id="Improving-Contrastive-Learning-of-Sentence-Embeddings-with-Focal-InfoNCE"><a href="#Improving-Contrastive-Learning-of-Sentence-Embeddings-with-Focal-InfoNCE" class="headerlink" title="Improving Contrastive Learning of Sentence Embeddings with Focal-InfoNCE"></a>Improving Contrastive Learning of Sentence Embeddings with Focal-InfoNCE</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06918">http://arxiv.org/abs/2310.06918</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/puerrrr/focal-infonce">https://github.com/puerrrr/focal-infonce</a></li>
<li>paper_authors: Pengyue Hou, Xingyu Li</li>
<li>for: 提高句子表示的质量</li>
<li>methods:  combinest SimCSE 和 hard negative mining， introduce self-paced modulation terms in the contrastive objective</li>
<li>results: 改进句子表示的Spearman correlation和Alignment和Uniformity<details>
<summary>Abstract</summary>
The recent success of SimCSE has greatly advanced state-of-the-art sentence representations. However, the original formulation of SimCSE does not fully exploit the potential of hard negative samples in contrastive learning. This study introduces an unsupervised contrastive learning framework that combines SimCSE with hard negative mining, aiming to enhance the quality of sentence embeddings. The proposed focal-InfoNCE function introduces self-paced modulation terms in the contrastive objective, downweighting the loss associated with easy negatives and encouraging the model focusing on hard negatives. Experimentation on various STS benchmarks shows that our method improves sentence embeddings in terms of Spearman's correlation and representation alignment and uniformity.
</details>
<details>
<summary>摘要</summary>
最近，SimCSE的成功有效地提高了现代句子表示的状态艺。然而，原始的SimCSE формулировция并没有充分利用强有力的负样本在对比学习中的潜力。本研究提出了一种无监督对比学习框架，将SimCSE与强负样本挖掘结合起来，以提高句子嵌入的质量。我们提出的自适应InfoNCE函数在对比目标中添加了自适应调整项，将易于获得的负样本下Weight，让模型更加注重困难的负样本。经过实验表明，我们的方法可以提高句子嵌入的斯宾森相关度和表示对应性和一致性。
</details></li>
</ul>
<hr>
<h2 id="A-Comparative-Study-of-Transformer-based-Neural-Text-Representation-Techniques-on-Bug-Triaging"><a href="#A-Comparative-Study-of-Transformer-based-Neural-Text-Representation-Techniques-on-Bug-Triaging" class="headerlink" title="A Comparative Study of Transformer-based Neural Text Representation Techniques on Bug Triaging"></a>A Comparative Study of Transformer-based Neural Text Representation Techniques on Bug Triaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06913">http://arxiv.org/abs/2310.06913</a></li>
<li>repo_url: None</li>
<li>paper_authors: Atish Kumar Dipongkor, Kevin Moran</li>
<li>for: 本研究旨在自动化漏洞报告的三个步骤：识别开发者和组件，地方化漏洞，和修复漏洞。</li>
<li>methods: 本研究使用 transformer-based 语言模型进行自动化漏洞报告的任务，包括 DeBERTa 等多种方法。</li>
<li>results: 研究发现，DeBERTa 是最有效的方法，在开发者和组件归属和漏洞地方化等三个任务中具有 statistically significant 的表现优势。但是，每种方法都有其特点和优势，适用于不同类型的漏洞报告。<details>
<summary>Abstract</summary>
Often, the first step in managing bug reports is related to triaging a bug to the appropriate developer who is best suited to understand, localize, and fix the target bug. Additionally, assigning a given bug to a particular part of a software project can help to expedite the fixing process. However, despite the importance of these activities, they are quite challenging, where days can be spent on the manual triaging process. Past studies have attempted to leverage the limited textual data of bug reports to train text classification models that automate this process -- to varying degrees of success. However, the textual representations and machine learning models used in prior work are limited by their expressiveness, often failing to capture nuanced textual patterns that might otherwise aid in the triaging process. Recently, large, transformer-based, pre-trained neural text representation techniques such as BERT have achieved greater performance in several natural language processing tasks. However, the potential for using these techniques to improve upon prior approaches for automated bug triaging is not well studied or understood.   Therefore, in this paper we offer one of the first investigations that fine-tunes transformer-based language models for the task of bug triaging on four open source datasets, spanning a collective 53 years of development history with over 400 developers and over 150 software project components. Our study includes both a quantitative and qualitative analysis of effectiveness. Our findings illustrate that DeBERTa is the most effective technique across the triaging tasks of developer and component assignment, and the measured performance delta is statistically significant compared to other techniques. However, through our qualitative analysis, we also observe that each technique possesses unique abilities best suited to certain types of bug reports.
</details>
<details>
<summary>摘要</summary>
通常，处理bug报告的第一步是将bug分配到适合的开发者，以便他们能够更好地理解、本地化和修复目标bug。此外，将bug分配到特定的软件项目部分也可以帮助加速修复过程。然而，这些活动具有挑战性，可能需要数天的手动分配过程。过去的研究已经尝试使用bug报告的有限文本数据来训练文本分类模型，以便自动进行这些活动——尽管效果不一。然而，这些表达和机器学习模型在先前的工作中有限，常常无法捕捉bug报告中细腻的文本模式，这可能会帮助分配过程。最近，大型的transformer-based大型预训练神经网络模型，如BERT，在自然语言处理任务中已经达到了更高的性能。然而，使用这些技术来改进先前的自动分配策略的可能性并不很了解或研究。因此，在这篇论文中，我们提供了一个由BERT等大型神经网络模型进行微调的首次研究，用于在四个开源数据集上进行自动分配。我们的研究包括量化和质量分析的效果分析。我们的发现表明，DeBERTa是分配任务中最有效的技术，并且与其他技术的性能差异是统计学上有意义的。然而，我们的质量分析也表明，每种技术都具有特定的优势，适合某些类型的bug报告。
</details></li>
</ul>
<hr>
<h2 id="LongLLMLingua-Accelerating-and-Enhancing-LLMs-in-Long-Context-Scenarios-via-Prompt-Compression"><a href="#LongLLMLingua-Accelerating-and-Enhancing-LLMs-in-Long-Context-Scenarios-via-Prompt-Compression" class="headerlink" title="LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression"></a>LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06839">http://arxiv.org/abs/2310.06839</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/LLMLingua">https://github.com/microsoft/LLMLingua</a></li>
<li>paper_authors: Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, Lili Qiu</li>
<li>for: 提高大语言模型（LLM）的计算成本、财务成本和响应时间，以及提高LLM在长文本场景下的性能。</li>
<li>methods: 提出了一种名为LongLLMLingua的提示压缩方法，通过提高LLM对关键信息的感知，同时解决了上述三个挑战。</li>
<li>results: 在单文检索、多文检索、简要摘要、 sintetic 任务和代码完成任务等长文本场景中，LongLLMLingua 可以 deriv 更高的性能，并降低了终端系统的响应时间。例如，在 NaturalQuestions  bencmark 上，LongLLMLingua 可以提高 GPT-3.5-Turbo 的性能 by 17.1%，并且只需要输入 ~4x  fewer tokens。此外，LongLLMLingua 可以在压缩提示的情况下，提高终端系统的响应速度。<details>
<summary>Abstract</summary>
In long context scenarios, large language models (LLMs) face three main challenges: higher computational/financial cost, longer latency, and inferior performance. Some studies reveal that the performance of LLMs depends on both the density and the position of the key information (question relevant) in the input prompt. Inspired by these findings, we propose LongLLMLingua for prompt compression towards improving LLMs' perception of the key information to simultaneously address the three challenges. We conduct evaluation on a wide range of long context scenarios including single-/multi-document QA, few-shot learning, summarization, synthetic tasks, and code completion. The experimental results show that LongLLMLingua compressed prompt can derive higher performance with much less cost. The latency of the end-to-end system is also reduced. For example, on NaturalQuestions benchmark, LongLLMLingua gains a performance boost of up to 17.1% over the original prompt with ~4x fewer tokens as input to GPT-3.5-Turbo. It can derive cost savings of \$28.5 and \$27.4 per 1,000 samples from the LongBench and ZeroScrolls benchmark, respectively. Additionally, when compressing prompts of ~10k tokens at a compression rate of 2x-10x, LongLLMLingua can speed up the end-to-end latency by 1.4x-3.8x. Our code is available at https://aka.ms/LLMLingua.
</details>
<details>
<summary>摘要</summary>
受长文本场景限制，大语言模型（LLM）面临三大挑战：更高的计算/金融成本、更长的延迟时间和较差的性能。一些研究表明，LLM的性能与输入提示中关键信息的密度和位置有关。以这些发现为灵感，我们提出了LongLLMLingua，用于提取提示中关键信息，以同时解决这三个挑战。我们在单/多文档问答、几拍学习、概要、人工任务和代码完成等多种长文本场景进行评估。实验结果表明，LongLLMLingua压缩后的提示可以提高性能，并且减少了终端系统的延迟时间。例如，在NaturalQuestionsBenchmark上，LongLLMLingua可以在GPT-3.5-Turbo上提高性能，并且只需输入4x少于原始提示的token数量。此外，当压缩提示长度为10k字时，LongLLMLingua可以将终端系统的延迟时间加速1.4x-3.8x。我们的代码可以在https://aka.ms/LLMLingua上下载。
</details></li>
</ul>
<hr>
<h2 id="Generating-and-Evaluating-Tests-for-K-12-Students-with-Language-Model-Simulations-A-Case-Study-on-Sentence-Reading-Efficiency"><a href="#Generating-and-Evaluating-Tests-for-K-12-Students-with-Language-Model-Simulations-A-Case-Study-on-Sentence-Reading-Efficiency" class="headerlink" title="Generating and Evaluating Tests for K-12 Students with Language Model Simulations: A Case Study on Sentence Reading Efficiency"></a>Generating and Evaluating Tests for K-12 Students with Language Model Simulations: A Case Study on Sentence Reading Efficiency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06837">http://arxiv.org/abs/2310.06837</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eric Zelikman, Wanjing Anya Ma, Jasmine E. Tran, Diyi Yang, Jason D. Yeatman, Nick Haber</li>
<li>for: 这个论文的目的是为了开发一种高质量的同时测试，以便更好地评估学生的阅读能力。</li>
<li>methods: 这个论文使用了大型自然语言模型（LLM）来模拟之前学生对未看过的题目的回答，以估计每个题目的难度和抽象程度。</li>
<li>results: 该论文使用GPT-4生成新的测试项，并使用精度调整后的LLM来筛选符合心理测量标准的题目。 results show that the generated test scores are highly correlated (r&#x3D;0.93) with those of a standard test form written by human experts, and the generated tests closely correspond to the original test’s difficulty and reliability based on crowdworker responses.<details>
<summary>Abstract</summary>
Developing an educational test can be expensive and time-consuming, as each item must be written by experts and then evaluated by collecting hundreds of student responses. Moreover, many tests require multiple distinct sets of questions administered throughout the school year to closely monitor students' progress, known as parallel tests. In this study, we focus on tests of silent sentence reading efficiency, used to assess students' reading ability over time. To generate high-quality parallel tests, we propose to fine-tune large language models (LLMs) to simulate how previous students would have responded to unseen items. With these simulated responses, we can estimate each item's difficulty and ambiguity. We first use GPT-4 to generate new test items following a list of expert-developed rules and then apply a fine-tuned LLM to filter the items based on criteria from psychological measurements. We also propose an optimal-transport-inspired technique for generating parallel tests and show the generated tests closely correspond to the original test's difficulty and reliability based on crowdworker responses. Our evaluation of a generated test with 234 students from grades 2 to 8 produces test scores highly correlated (r=0.93) to those of a standard test form written by human experts and evaluated across thousands of K-12 students.
</details>
<details>
<summary>摘要</summary>
开发教育测试可能会很昂贵和时间consuming，因为每个项目都需要由专家写作并由数百名学生回答。此外，许多测试需要在学年中多次进行测试，以便密切监测学生的进步，这种测试被称为平行测试。在这项研究中，我们关注 silent sentence reading efficiency 测试，用于评估学生的阅读能力。为生成高质量平行测试，我们提议使用大型自然语言模型（LLM）来模拟以前学生对未看过的问题的回答。通过这些模拟回答，我们可以估算每个问题的难度和抽象性。我们首先使用 GPT-4 生成新的测试项目，并应用一个精度调整的 LLM 来过滤测试项目基于心理测量的标准。我们还提出一种基于最优运输的技术来生成平行测试，并证明生成的测试与原始测试的难度和可靠性具有高度相似性。我们对234名二至八年级学生进行评估，得到的测试分数与由人类专家编写的标准测试形式相高度相关（r=0.93）。
</details></li>
</ul>
<hr>
<h2 id="Lemur-Harmonizing-Natural-Language-and-Code-for-Language-Agents"><a href="#Lemur-Harmonizing-Natural-Language-and-Code-for-Language-Agents" class="headerlink" title="Lemur: Harmonizing Natural Language and Code for Language Agents"></a>Lemur: Harmonizing Natural Language and Code for Language Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06830">http://arxiv.org/abs/2310.06830</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/openlemur/lemur">https://github.com/openlemur/lemur</a></li>
<li>paper_authors: Yiheng Xu, Hongjin Su, Chen Xing, Boyu Mi, Qian Liu, Weijia Shi, Binyuan Hui, Fan Zhou, Yitao Liu, Tianbao Xie, Zhoujun Cheng, Siheng Zhao, Lingpeng Kong, Bailin Wang, Caiming Xiong, Tao Yu</li>
<li>For: 本研究开发了一个名为Lemur和Lemur-Chat的开源语言模型，用于实现多元语言代理人。* Methods: 研究人员使用了一个代码数据集进行谨慎预训，并对文本和程式码数据进行微调。* Results: 研究人员通过实验发现，Lemur和Lemur-Chat可以在多种环境中实现高水平的表现，并且与商业化模型相比，它们在代理人能力方面表现更为出色。<details>
<summary>Abstract</summary>
We introduce Lemur and Lemur-Chat, openly accessible language models optimized for both natural language and coding capabilities to serve as the backbone of versatile language agents. The evolution from language chat models to functional language agents demands that models not only master human interaction, reasoning, and planning but also ensure grounding in the relevant environments. This calls for a harmonious blend of language and coding capabilities in the models. Lemur and Lemur-Chat are proposed to address this necessity, demonstrating balanced proficiencies in both domains, unlike existing open-source models that tend to specialize in either. Through meticulous pre-training using a code-intensive corpus and instruction fine-tuning on text and code data, our models achieve state-of-the-art averaged performance across diverse text and coding benchmarks among open-source models. Comprehensive experiments demonstrate Lemur's superiority over existing open-source models and its proficiency across various agent tasks involving human communication, tool usage, and interaction under fully- and partially- observable environments. The harmonization between natural and programming languages enables Lemur-Chat to significantly narrow the gap with proprietary models on agent abilities, providing key insights into developing advanced open-source agents adept at reasoning, planning, and operating seamlessly across environments. https://github.com/OpenLemur/Lemur
</details>
<details>
<summary>摘要</summary>
我们介绍Lemur和Lemur-Chat，是一 pair of开源语言模型，旨在扩展语言和程式码之间的共同能力，以便建立多元化的语言代理。从语言交流模型演化为功能性语言代理需要模型不仅掌握人类互动、推理和观念，而且还需要与环境相互融合。这需要模型同时具备语言和程式码的能力。Lemur和Lemur-Chat被提议以应对这个需求，并在多个语言和程式码benchmark测试中表现出色。我们通过精心预训使用一个具有程式码的资料集，以及对文本和程式码数据进行精确调整，使我们的模型在开源模型中表现出积极的平均性能。实验结果显示Lemur在开源模型中表现出色，并且在不同的代理任务中具备广泛的能力，包括人类交流、工具使用和受完全和受限 Observable 环境中的互动。通过自然语言和程式码之间的融合，Lemur-Chat可以对Proprietary模型的代理能力进行明显的缩小，提供关键的意见，以帮助开发高水准的开源代理，能够快速推理、规划和在不同环境中顺畅运行。更多资讯可以在GitHub上找到：https://github.com/OpenLemur/Lemur
</details></li>
</ul>
<hr>
<h2 id="Teaching-Language-Models-to-Hallucinate-Less-with-Synthetic-Tasks"><a href="#Teaching-Language-Models-to-Hallucinate-Less-with-Synthetic-Tasks" class="headerlink" title="Teaching Language Models to Hallucinate Less with Synthetic Tasks"></a>Teaching Language Models to Hallucinate Less with Synthetic Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06827">http://arxiv.org/abs/2310.06827</a></li>
<li>repo_url: None</li>
<li>paper_authors: Erik Jones, Hamid Palangi, Clarisse Simões, Varun Chandrasekaran, Subhabrata Mukherjee, Arindam Mitra, Ahmed Awadallah, Ece Kamar</li>
<li>for: 本研究旨在提高大型自然语言模型（LLM）在抽象摘要任务上减少幻觉，以提高模型在实际任务上的表现。</li>
<li>methods: 本研究使用一种名为SynTra的方法，首先在一个 sintetic task 上设计了一个易于诱发和测量幻觉的任务，然后使用这个任务进行预 fixing  LLM 的系统消息，最后将系统消息应用到实际的摘要任务上。</li>
<li>results: 在三个实际的摘要任务上，SynTra 能够减少两个 13B 参数的 LLM 的幻觉。此外，研究还发现，在 synthetic task 上优化系统消息比优化模型参数更加重要，而 fine-tuning 整个模型在 synthetic task 上可能会增加幻觉。<details>
<summary>Abstract</summary>
Large language models (LLMs) frequently hallucinate on abstractive summarization tasks such as document-based question-answering, meeting summarization, and clinical report generation, even though all necessary information is included in context. However, optimizing LLMs to hallucinate less on these tasks is challenging, as hallucination is hard to efficiently evaluate at each optimization step. In this work, we show that reducing hallucination on a synthetic task can also reduce hallucination on real-world downstream tasks. Our method, SynTra, first designs a synthetic task where hallucinations are easy to elicit and measure. It next optimizes the LLM's system message via prefix-tuning on the synthetic task, and finally transfers the system message to realistic, hard-to-optimize tasks. Across three realistic abstractive summarization tasks, SynTra reduces hallucination for two 13B-parameter LLMs using only a synthetic retrieval task for supervision. We also find that optimizing the system message rather than the model weights can be critical; fine-tuning the entire model on the synthetic task can counterintuitively increase hallucination. Overall, SynTra demonstrates that the extra flexibility of working with synthetic data can help mitigate undesired behaviors in practice.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在抽象摘要化任务中常常会出现幻想，例如文档问答、会议摘要和医疗报告生成，即使所有必要的信息都包含在 контек斯中。但是，对 LLM 进行幻想调整是困难的，因为幻想难以在每个优化步骤中有效评估。在这个工作中，我们显示了将幻想降低在 sintetic 任务上可以降低实际世界下渠道任务中的幻想。我们的方法 SynTra 首先设计了 sintetic 任务，可以轻松诱发和评估幻想。然后，SynTra 透过 prefix-tuning 优化 LLM 的系统讯息，最后将系统讯息转换到实际、difficult-to-optimize 任务上。在三个实际抽象摘要化任务中，SynTra 可以降低两个 13B 参数 LLM 的幻想。我们还发现，对系统讯息进行优化可以是关键的；精确地调整整个模型的参数可能会增加幻想。总的来说，SynTra 显示了使用 sintetic 数据可以帮助解决实际中的问题。
</details></li>
</ul>
<hr>
<h2 id="Text-Embeddings-Reveal-Almost-As-Much-As-Text"><a href="#Text-Embeddings-Reveal-Almost-As-Much-As-Text" class="headerlink" title="Text Embeddings Reveal (Almost) As Much As Text"></a>Text Embeddings Reveal (Almost) As Much As Text</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06816">http://arxiv.org/abs/2310.06816</a></li>
<li>repo_url: None</li>
<li>paper_authors: John X. Morris, Volodymyr Kuleshov, Vitaly Shmatikov, Alexander M. Rush</li>
<li>for:  investigate the problem of embedding inversion, reconstructing the full text represented in dense text embeddings.</li>
<li>methods:  frame the problem as controlled generation: generating text that, when reembedded, is close to a fixed point in latent space.</li>
<li>results:  recover $92%$ of $32\text{-token}$ text inputs exactly using a multi-step method that iteratively corrects and re-embeds text.<details>
<summary>Abstract</summary>
How much private information do text embeddings reveal about the original text? We investigate the problem of embedding \textit{inversion}, reconstructing the full text represented in dense text embeddings. We frame the problem as controlled generation: generating text that, when reembedded, is close to a fixed point in latent space. We find that although a na\"ive model conditioned on the embedding performs poorly, a multi-step method that iteratively corrects and re-embeds text is able to recover $92\%$ of $32\text{-token}$ text inputs exactly. We train our model to decode text embeddings from two state-of-the-art embedding models, and also show that our model can recover important personal information (full names) from a dataset of clinical notes. Our code is available on Github: \href{https://github.com/jxmorris12/vec2text}{github.com/jxmorris12/vec2text}.
</details>
<details>
<summary>摘要</summary>
TEXT我们研究了文本嵌入的私人信息泄露问题，具体来说是文本嵌入的反推问题，即通过 dense text embeddings 中的点来恢复原始文本。我们将问题定义为控制生成问题，即生成文本，其重新嵌入后与给定点在嵌入空间很近。我们发现，直接使用嵌入模型conditioned的模型表现不佳，但是通过Iteratively Correct and Re-Embed Text（ICRT）方法，可以准确地恢复 $92\%$ 的 $32$-token 文本输入。我们使用两种现状顶尖嵌入模型来训练我们的模型，并示出我们的模型可以从医疗笔记中提取重要的个人信息（全名）。我们的代码可以在 Github 上找到：https://github.com/jxmorris12/vec2text。Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Uni3D-Exploring-Unified-3D-Representation-at-Scale"><a href="#Uni3D-Exploring-Unified-3D-Representation-at-Scale" class="headerlink" title="Uni3D: Exploring Unified 3D Representation at Scale"></a>Uni3D: Exploring Unified 3D Representation at Scale</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06773">http://arxiv.org/abs/2310.06773</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/baaivision/uni3d">https://github.com/baaivision/uni3d</a></li>
<li>paper_authors: Junsheng Zhou, Jinsheng Wang, Baorui Ma, Yu-Shen Liu, Tiejun Huang, Xinlong Wang</li>
<li>For: 本研究旨在探讨3D对象和场景的扩大表示，以探索3D世界中的一元表示。* Methods: 本研究使用2D初始化的ViT端到终推理，将3D点云特征与图像文本对齐。通过简单的architecture和预测任务，Uni3D可以利用丰富的2D预测模型和图像文本对齐模型作为初始化，从而解锁2D模型和扩大策略在3D世界中的潜力。* Results: 我们效率地扩大Uni3D到一亿个参数，并在广泛的3D任务中设置新的纪录，如零shot分类、少shot分类、开放世界理解和部分 segmentation。我们还示出Uni3D的强大表示能够应用于3D绘制和 Retrieval in the wild。我们认为Uni3D提供了一个新的方向，用于探索3D表示的扩大和效率。<details>
<summary>Abstract</summary>
Scaling up representations for images or text has been extensively investigated in the past few years and has led to revolutions in learning vision and language. However, scalable representation for 3D objects and scenes is relatively unexplored. In this work, we present Uni3D, a 3D foundation model to explore the unified 3D representation at scale. Uni3D uses a 2D initialized ViT end-to-end pretrained to align the 3D point cloud features with the image-text aligned features. Via the simple architecture and pretext task, Uni3D can leverage abundant 2D pretrained models as initialization and image-text aligned models as the target, unlocking the great potential of 2D models and scaling-up strategies to the 3D world. We efficiently scale up Uni3D to one billion parameters, and set new records on a broad range of 3D tasks, such as zero-shot classification, few-shot classification, open-world understanding and part segmentation. We show that the strong Uni3D representation also enables applications such as 3D painting and retrieval in the wild. We believe that Uni3D provides a new direction for exploring both scaling up and efficiency of the representation in 3D domain.
</details>
<details>
<summary>摘要</summary>
压缩表示法在图像或文本领域已经得到了广泛的研究，并导致了视觉和语言学习领域的革命。然而，对于3D对象和场景的可扩展表示仍然相对未经探索。在这个工作中，我们提出了Uni3D，一个用于探索可扩展3D表示的基础模型。Uni3D使用一个初始化为2D的ViT结构，通过对3D点云特征与图像和文本对齐的方式进行预training，以获得一个协调的3D表示。通过简单的建筑和预text任务，Uni3D可以利用丰富的2D预训练模型和图像和文本对齐的模型作为目标，从而解锁2D模型和扩展策略在3D世界的潜力。我们效率地扩展Uni3D到一亿个参数，并在广泛的3D任务上设置新的纪录，如零shot分类、几shot分类、开放世界理解和部分 segmentation。我们显示Uni3D表示也可以应用于3D涂鸦和野外检索。我们认为Uni3D提供了一个新的方向，用于探索3D领域中的表示扩展和效率。
</details></li>
</ul>
<hr>
<h2 id="OmniLingo-Listening-and-speaking-based-language-learning"><a href="#OmniLingo-Listening-and-speaking-based-language-learning" class="headerlink" title="OmniLingo: Listening- and speaking-based language learning"></a>OmniLingo: Listening- and speaking-based language learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06764">http://arxiv.org/abs/2310.06764</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francis M. Tyers, Nicholas Howell</li>
<li>for: 这篇论文旨在提供一种分布数据架构和应用示例，用于语言学习应用程序中的听说学习。</li>
<li>methods: 该架构基于Interplanetary Filesystem（IPFS），强调用户主权 над数据。</li>
<li>results: 论文提供了一个基于IPFS的分布数据架构和一个示例客户端，用于支持语言学习应用程序的听说学习。<details>
<summary>Abstract</summary>
In this demo paper we present OmniLingo, an architecture for distributing data for listening- and speaking-based language learning applications and a demonstration client built using the architecture. The architecture is based on the Interplanetary Filesystem (IPFS) and puts at the forefront user sovereignty over data.
</details>
<details>
<summary>摘要</summary>
在这份 demo 纸上，我们介绍 OmniLingo，一种分布式数据架构，用于语音和语言学习应用程序，以及一个基于 Interplanetary Filesystem (IPFS) 的示例客户端。这种架构强调用户主权 над数据。Here's a breakdown of the text:* "在这份 demo 纸上" (在这份 demo 纸上) - This phrase is used to indicate that the topic being discussed is a demo or a sample.* "我们介绍 OmniLingo" (我们介绍 OmniLingo) - This phrase introduces the topic of the discussion, which is OmniLingo.* "一种分布式数据架构" (一种分布式数据架构) - This phrase describes OmniLingo as a distributed data architecture.* "用于语音和语言学习应用程序" (用于语音和语言学习应用程序) - This phrase explains the purpose of OmniLingo, which is to support listening- and speaking-based language learning applications.* "以及一个基于 Interplanetary Filesystem (IPFS) 的示例客户端" (以及一个基于 Interplanetary Filesystem (IPFS) 的示例客户端) - This phrase provides more information about OmniLingo, specifically that it is based on the Interplanetary Filesystem (IPFS) and includes a demonstration client.* "这种架构强调用户主权 над数据" (这种架构强调用户主权 над数据) - This phrase emphasizes the importance of user sovereignty over data in the OmniLingo architecture.
</details></li>
</ul>
<hr>
<h2 id="TRACE-A-Comprehensive-Benchmark-for-Continual-Learning-in-Large-Language-Models"><a href="#TRACE-A-Comprehensive-Benchmark-for-Continual-Learning-in-Large-Language-Models" class="headerlink" title="TRACE: A Comprehensive Benchmark for Continual Learning in Large Language Models"></a>TRACE: A Comprehensive Benchmark for Continual Learning in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06762">http://arxiv.org/abs/2310.06762</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/beyonderxx/trace">https://github.com/beyonderxx/trace</a></li>
<li>paper_authors: Xiao Wang, Yuansen Zhang, Tianze Chen, Songyang Gao, Senjie Jin, Xianjun Yang, Zhiheng Xi, Rui Zheng, Yicheng Zou, Tao Gui, Qi Zhang, Xuanjing Huang</li>
<li>for: 本研究旨在评估已经aligned的大型语言模型（LLMs）在连续学习中的能力。</li>
<li>methods: 本研究使用了一个新的benchmark方法 named TRACE，包括8个不同的数据集，涵盖域专业任务、多语言能力、代码生成和数学逻辑等多种挑战任务。</li>
<li>results: 实验结果表明，在TRACE数据集上训练后，已经aligned的LLMs呈现了显著的普通能力和指令遵循能力下降。例如，llama2-chat 13B在gsm8k数据集上的准确率从28.8%降至2%。这表明需要找到一个适合的权衡，以确保实现特定任务的表现，而不会导致LLMs的原始能力减退。<details>
<summary>Abstract</summary>
Aligned large language models (LLMs) demonstrate exceptional capabilities in task-solving, following instructions, and ensuring safety. However, the continual learning aspect of these aligned LLMs has been largely overlooked. Existing continual learning benchmarks lack sufficient challenge for leading aligned LLMs, owing to both their simplicity and the models' potential exposure during instruction tuning. In this paper, we introduce TRACE, a novel benchmark designed to evaluate continual learning in LLMs. TRACE consists of 8 distinct datasets spanning challenging tasks including domain-specific tasks, multilingual capabilities, code generation, and mathematical reasoning. All datasets are standardized into a unified format, allowing for effortless automatic evaluation of LLMs. Our experiments show that after training on TRACE, aligned LLMs exhibit significant declines in both general ability and instruction-following capabilities. For example, the accuracy of llama2-chat 13B on gsm8k dataset declined precipitously from 28.8\% to 2\% after training on our datasets. This highlights the challenge of finding a suitable tradeoff between achieving performance on specific tasks while preserving the original prowess of LLMs. Empirical findings suggest that tasks inherently equipped with reasoning paths contribute significantly to preserving certain capabilities of LLMs against potential declines. Motivated by this, we introduce the Reasoning-augmented Continual Learning (RCL) approach. RCL integrates task-specific cues with meta-rationales, effectively reducing catastrophic forgetting in LLMs while expediting convergence on novel tasks.
</details>
<details>
<summary>摘要</summary>
aligned large language models (LLMs) 表现出色地解决任务、遵循指令和保持安全。然而，这些aligned LLMs的持续学习方面尚未得到充分的注意。现有的持续学习标准benchmarklacks sufficient challenge for leading aligned LLMs, owing to both their simplicity and the models' potential exposure during instruction tuning.在这篇论文中，我们介绍TRACE，一个新的benchmark，用于评估LLMs的持续学习能力。TRACE包括8个不同的数据集，涵盖域специфи任务、多语言能力、代码生成和数学逻辑推理。所有数据集都是 стандар化了，以便自动评估LLMs。我们的实验表明，在TRACE上训练后，aligned LLMs的总能力和遵循指令能力都会显著下降。例如，llama2-chat 13B在gsm8k数据集上的准确率从28.8%下降到2%。这说明了在寻找适当的任务和原始模型能力之间的权衡是一个挑战。我们的实验结果表明，具有逻辑路径的任务可以帮助保持LLMs的一些能力。基于这一点，我们提出了Reasoning-augmented Continual Learning（RCL）方法。RCL通过将任务特有的cue与元理性相结合，以降低LLMs中的恶化学习，同时加速在新任务上的 converges。
</details></li>
</ul>
<hr>
<h2 id="Temporally-Aligning-Long-Audio-Interviews-with-Questions-A-Case-Study-in-Multimodal-Data-Integration"><a href="#Temporally-Aligning-Long-Audio-Interviews-with-Questions-A-Case-Study-in-Multimodal-Data-Integration" class="headerlink" title="Temporally Aligning Long Audio Interviews with Questions: A Case Study in Multimodal Data Integration"></a>Temporally Aligning Long Audio Interviews with Questions: A Case Study in Multimodal Data Integration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06702">http://arxiv.org/abs/2310.06702</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/piyushsinghpasi/INDENT">https://github.com/piyushsinghpasi/INDENT</a></li>
<li>paper_authors: Piyush Singh Pasi, Karthikeya Battepati, Preethi Jyothi, Ganesh Ramakrishnan, Tanmay Mahapatra, Manoj Singh</li>
<li>for: 这个研究是为了解决长 Audio-to-text 的对齐问题，通常在训练时使用完整的监督。但是，这些研究通常不是在长 Audio 文件中，其中文本 queries 不会直接出现在 Audio 文件中。这个研究是与印度 CARE 组织合作，收集了来自印度比хар邦的农村区域的长 Audio 健康调查。</li>
<li>methods: 我们提出了一个名为 INDENT 的框架，使用 crossed attention 模型和文本 вопро卷中的 temporal ordering 信息来学习 speech 嵌入。这些学习的嵌入被用于在搜寻时根据文本 queries 找到相应的 Audio 段。</li>
<li>results: 我们对比了 INDENT 模型和文本基于的 heuristics 模型，并证明了 INDENT 模型在 R-avg 方面提高了约 3%。我们还表明了使用 state-of-the-art ASR 模型生成的噪音 ASR 可以在搜寻时提供更好的结果。 finally，我们证明了 INDENT 只需要在印地语料上训练，就可以在 11 种指定语言上进行搜寻。<details>
<summary>Abstract</summary>
The problem of audio-to-text alignment has seen significant amount of research using complete supervision during training. However, this is typically not in the context of long audio recordings wherein the text being queried does not appear verbatim within the audio file. This work is a collaboration with a non-governmental organization called CARE India that collects long audio health surveys from young mothers residing in rural parts of Bihar, India. Given a question drawn from a questionnaire that is used to guide these surveys, we aim to locate where the question is asked within a long audio recording. This is of great value to African and Asian organizations that would otherwise have to painstakingly go through long and noisy audio recordings to locate questions (and answers) of interest. Our proposed framework, INDENT, uses a cross-attention-based model and prior information on the temporal ordering of sentences to learn speech embeddings that capture the semantics of the underlying spoken text. These learnt embeddings are used to retrieve the corresponding audio segment based on text queries at inference time. We empirically demonstrate the significant effectiveness (improvement in R-avg of about 3%) of our model over those obtained using text-based heuristics. We also show how noisy ASR, generated using state-of-the-art ASR models for Indian languages, yields better results when used in place of speech. INDENT, trained only on Hindi data is able to cater to all languages supported by the (semantically) shared text space. We illustrate this empirically on 11 Indic languages.
</details>
<details>
<summary>摘要</summary>
audio-to-文本对齐问题在训练中得到了大量研究，通常是使用完全监督。但是，这并不是在长度较长的音频文件中，文本 queries 中的内容不是直接出现在音频文件中的情况。这是一项与非政府组织CARE印度合作的工作，收集了印度北部锡库的年轻母亲的长 Audio 健康调查。给定一个问卷中的问题，我们的目标是在长 Audio 录音中找到这个问题的位置。这对于非洲和亚洲组织来说是非常有价值的，否则他们需要慢慢地从长度较长的 Audio 录音中找到问题（以及答案）。我们提出了一个名为 INDENT 的框架，使用 cross-attention 模型和前期知识来学习 speech 嵌入，这些嵌入 capture 了下面的含义。在推理时，我们使用这些学习的嵌入来根据文本查询 retrieve 相应的音频段。我们实际示出了我们模型比使用文本基于的优化法得到的效果更好（提高 R-avg 约 3%）。我们还显示了使用 state-of-the-art ASR 模型生成的噪音 ASR 可以在某些情况下提供更好的结果。INDENT，只在印地语料上训练，能够涵盖所有支持 Semantic 共享文本空间中的语言。我们在 11 种指定语言上进行了实质性的示例。
</details></li>
</ul>
<hr>
<h2 id="Learning-Multiplex-Embeddings-on-Text-rich-Networks-with-One-Text-Encoder"><a href="#Learning-Multiplex-Embeddings-on-Text-rich-Networks-with-One-Text-Encoder" class="headerlink" title="Learning Multiplex Embeddings on Text-rich Networks with One Text Encoder"></a>Learning Multiplex Embeddings on Text-rich Networks with One Text Encoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06684">http://arxiv.org/abs/2310.06684</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bowen Jin, Wentao Zhang, Yu Zhang, Yu Meng, Han Zhao, Jiawei Han</li>
<li>for: 学习多重文本网络中的多种关系</li>
<li>methods: 使用一个文本编码器来模型关系之间的共享知识，并使用少量参数来 derivation 关系特定的表示</li>
<li>results: 在五个网络中的九个下游任务上，METERN significantly 和 consistently 超过基线方法，并且 Parameters 的效率高。In English, this means:</li>
<li>for: Learning multiple types of relationships in text-rich networks</li>
<li>methods: Using one text encoder to model shared knowledge across relations, and deriving relation-specific representations with a small number of parameters</li>
<li>results: Significantly and consistently outperforming baselines on nine downstream tasks in five networks, with high parameter efficiency.<details>
<summary>Abstract</summary>
In real-world scenarios, texts in a network are often linked by multiple semantic relations (e.g., papers in an academic network are referenced by other publications, written by the same author, or published in the same venue), where text documents and their relations form a multiplex text-rich network. Mainstream text representation learning methods use pretrained language models (PLMs) to generate one embedding for each text unit, expecting that all types of relations between texts can be captured by these single-view embeddings. However, this presumption does not hold particularly in multiplex text-rich networks. Along another line of work, multiplex graph neural networks (GNNs) directly initialize node attributes as a feature vector for node representation learning, but they cannot fully capture the semantics of the nodes' associated texts. To bridge these gaps, we propose METERN, a new framework for learning Multiplex Embeddings on TExt-Rich Networks. In contrast to existing methods, METERN uses one text encoder to model the shared knowledge across relations and leverages a small number of parameters per relation to derive relation-specific representations. This allows the encoder to effectively capture the multiplex structures in the network while also preserving parameter efficiency. We conduct experiments on nine downstream tasks in five networks from both academic and e-commerce domains, where METERN outperforms baselines significantly and consistently. The code is available at https://github.com/PeterGriffinJin/METERN-submit.
</details>
<details>
<summary>摘要</summary>
在实际场景中，网络中的文本经常被多种Semantic relation连接（例如，学术文献之间的引用、作者之间的共同写作或者发表在同一个会议上），这些文本文档和其关系组成了一个多重文本rich网络。主流文本表示学习方法使用预训练语言模型（PLM）生成每个文本单元的一个嵌入，期望所有类型的文本关系都可以通过这些单一视图嵌入被捕捉。然而，这个假设不符合特别在多重文本rich网络中。另一条工作线索是多种文本 graphs neural networks（GNNs）直接初始化节点属性为节点表示学习的特征向量，但它们无法完全捕捉节点相关文本的 semantics。为了覆盖这些差距，我们提出了METERN框架，一种新的文本多重嵌入学习框架。METERN使用一个文本编码器来模型关系间共享知识，并使用每个关系只需一些参数来生成特定关系表示。这使得编码器能够有效地捕捉多重结构，同时也能够保持参数效率。我们在五个网络和九个下渠任务上进行了实验，METERN与基线相比显著地提高了表现，并在多个网络和任务上保持稳定的高效性。代码可以在https://github.com/PeterGriffinJin/METERN-submit中找到。
</details></li>
</ul>
<hr>
<h2 id="SEER-A-Knapsack-approach-to-Exemplar-Selection-for-In-Context-HybridQA"><a href="#SEER-A-Knapsack-approach-to-Exemplar-Selection-for-In-Context-HybridQA" class="headerlink" title="SEER : A Knapsack approach to Exemplar Selection for In-Context HybridQA"></a>SEER : A Knapsack approach to Exemplar Selection for In-Context HybridQA</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06675">http://arxiv.org/abs/2310.06675</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jtonglet/seer">https://github.com/jtonglet/seer</a></li>
<li>paper_authors: Jonathan Tonglet, Manon Reusens, Philipp Borchert, Bart Baesens</li>
<li>for: 本研究旨在提高 HybridQA  tasks 的表达能力，通过选择 Representative 和多样化的 exemplars 来提高 reasoning 性能。</li>
<li>methods: 本文提出 Selection of ExEmplars for hybrid Reasoning (SEER) 方法，该方法将 exemplar 选择问题转化为 Knapsack 整数线性编程，以便满足多样化约束和容量约束。</li>
<li>results: 在 FinQA 和 TAT-QA 两个实际 benchmark 上，SEER 方法比前一代 exemplar 选择方法表现更高效。<details>
<summary>Abstract</summary>
Question answering over hybrid contexts is a complex task, which requires the combination of information extracted from unstructured texts and structured tables in various ways. Recently, In-Context Learning demonstrated significant performance advances for reasoning tasks. In this paradigm, a large language model performs predictions based on a small set of supporting exemplars. The performance of In-Context Learning depends heavily on the selection procedure of the supporting exemplars, particularly in the case of HybridQA, where considering the diversity of reasoning chains and the large size of the hybrid contexts becomes crucial. In this work, we present Selection of ExEmplars for hybrid Reasoning (SEER), a novel method for selecting a set of exemplars that is both representative and diverse. The key novelty of SEER is that it formulates exemplar selection as a Knapsack Integer Linear Program. The Knapsack framework provides the flexibility to incorporate diversity constraints that prioritize exemplars with desirable attributes, and capacity constraints that ensure that the prompt size respects the provided capacity budgets. The effectiveness of SEER is demonstrated on FinQA and TAT-QA, two real-world benchmarks for HybridQA, where it outperforms previous exemplar selection methods.
</details>
<details>
<summary>摘要</summary>
In this work, we propose Selection of ExEmplars for hybrid Reasoning (SEER), a novel method for selecting a set of exemplars that is both representative and diverse. The key innovation of SEER is that it formulates exemplar selection as a Knapsack Integer Linear Program. The Knapsack framework provides the flexibility to incorporate diversity constraints that prioritize exemplars with desirable attributes and capacity constraints that ensure that the prompt size respects the provided capacity budgets.We demonstrate the effectiveness of SEER on FinQA and TAT-QA, two real-world benchmarks for HybridQA, where it outperforms previous exemplar selection methods.
</details></li>
</ul>
<hr>
<h2 id="Making-Large-Language-Models-Perform-Better-in-Knowledge-Graph-Completion"><a href="#Making-Large-Language-Models-Perform-Better-in-Knowledge-Graph-Completion" class="headerlink" title="Making Large Language Models Perform Better in Knowledge Graph Completion"></a>Making Large Language Models Perform Better in Knowledge Graph Completion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06671">http://arxiv.org/abs/2310.06671</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zjukg/kopa">https://github.com/zjukg/kopa</a></li>
<li>paper_authors: Yichi Zhang, Zhuo Chen, Wen Zhang, Huajun Chen</li>
<li>for: 这个论文主要 targets 是如何使用语言模型（LLM）来完善知识 graphs（KGs），以提高 web 上自动服务的效能。</li>
<li>methods: 该论文提出了一种基于 LLM 的知识Graph completion（KGC）方法，通过将现有 LLM 模型转移到 структура感知Setting中，并提出了一种名为知识前缀适配器（KoPA）来使 LLM 能够更好地理解知识结构。KoPA 使用结构嵌入预训练来捕捉 KG 中实体和关系的结构信息，然后将这些结构嵌入 проек到文本空间，从而获得虚拟知识token作为输入提示。</li>
<li>results: 作者通过对这些结构意识LLM-based KGC方法进行了广泛的实验和深入分析，并证明了在引入结构信息后，LLM 的知识理解能力得到了改善。<details>
<summary>Abstract</summary>
Large language model (LLM) based knowledge graph completion (KGC) aims to predict the missing triples in the KGs with LLMs and enrich the KGs to become better web infrastructure, which can benefit a lot of web-based automatic services. However, research about LLM-based KGC is limited and lacks effective utilization of LLM's inference capabilities, which ignores the important structural information in KGs and prevents LLMs from acquiring accurate factual knowledge. In this paper, we discuss how to incorporate the helpful KG structural information into the LLMs, aiming to achieve structrual-aware reasoning in the LLMs. We first transfer the existing LLM paradigms to structural-aware settings and further propose a knowledge prefix adapter (KoPA) to fulfill this stated goal. KoPA employs structural embedding pre-training to capture the structural information of entities and relations in the KG. Then KoPA informs the LLMs of the knowledge prefix adapter which projects the structural embeddings into the textual space and obtains virtual knowledge tokens as a prefix of the input prompt. We conduct comprehensive experiments on these structural-aware LLM-based KGC methods and provide an in-depth analysis comparing how the introduction of structural information would be better for LLM's knowledge reasoning ability. Our code is released at https://github.com/zjukg/KoPA.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Self-Supervised-Representation-Learning-for-Online-Handwriting-Text-Classification"><a href="#Self-Supervised-Representation-Learning-for-Online-Handwriting-Text-Classification" class="headerlink" title="Self-Supervised Representation Learning for Online Handwriting Text Classification"></a>Self-Supervised Representation Learning for Online Handwriting Text Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06645">http://arxiv.org/abs/2310.06645</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pouya Mehralian, Bagher BabaAli, Ashena Gorgan Mohammadi</li>
<li>for: 这项研究旨在提出一种新的自助学习任务，以提取在线手写文本中人员的英文和中文语言writing的有用表示。</li>
<li>methods: 该研究使用了Part of Stroke Masking（POSM）作为预处理模型的预测任务，并提出了两种精度预处理模型的精度。</li>
<li>results: 该研究通过对预处理模型进行内在和外在评估方法，发现预处理模型可以达到写作人员认知、性别识别和手性识别等任务的最新状态。<details>
<summary>Abstract</summary>
Self-supervised learning offers an efficient way of extracting rich representations from various types of unlabeled data while avoiding the cost of annotating large-scale datasets. This is achievable by designing a pretext task to form pseudo labels with respect to the modality and domain of the data. Given the evolving applications of online handwritten texts, in this study, we propose the novel Part of Stroke Masking (POSM) as a pretext task for pretraining models to extract informative representations from the online handwriting of individuals in English and Chinese languages, along with two suggested pipelines for fine-tuning the pretrained models. To evaluate the quality of the extracted representations, we use both intrinsic and extrinsic evaluation methods. The pretrained models are fine-tuned to achieve state-of-the-art results in tasks such as writer identification, gender classification, and handedness classification, also highlighting the superiority of utilizing the pretrained models over the models trained from scratch.
</details>
<details>
<summary>摘要</summary>
自我指导学习提供了一种高效的方法，可以从不同类型的无标记数据中提取丰富的表示，而不需要投入大规模数据集的标注成本。这可以通过设计一个预tex任务，以模式和领域为据，生成 pseudo标签。在在线手写文本的应用场景中，在这项研究中，我们提出了一种新的部分roke掩蔽（POSM）作为预training模型的预tex任务，以提取英语和中文语言的在线手写人员的信息有价值表示。同时，我们还提出了两种可行的精度调整管道。为了评估提取的表示质量，我们使用了内在和外在评估方法。经过精度调整，预training模型可以达到当今最佳的写作人员认可、性别分类和手征分类等任务的结果，同时还 highlighted 预training模型的优势，比投入从头开始训练的模型更高效。
</details></li>
</ul>
<hr>
<h2 id="What-If-the-TV-Was-Off-Examining-Counterfactual-Reasoning-Abilities-of-Multi-modal-Language-Models"><a href="#What-If-the-TV-Was-Off-Examining-Counterfactual-Reasoning-Abilities-of-Multi-modal-Language-Models" class="headerlink" title="What If the TV Was Off? Examining Counterfactual Reasoning Abilities of Multi-modal Language Models"></a>What If the TV Was Off? Examining Counterfactual Reasoning Abilities of Multi-modal Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06627">http://arxiv.org/abs/2310.06627</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/letian2003/c-vqa">https://github.com/letian2003/c-vqa</a></li>
<li>paper_authors: Letian Zhang, Xiaotong Zhai, Zhongkai Zhao, Xin Wen, Yongshuo Zong, Bingchen Zhao</li>
<li>For: This paper aims to benchmark the counterfactual reasoning ability of multi-modal large language models.* Methods: The authors use the VQAv2 dataset and add a counterfactual presupposition to the questions, then generate counterfactual questions and answers using ChatGPT. They manually examine all generated questions and answers to ensure correctness.* Results: The authors evaluate recent vision language models on their newly collected test dataset and find that all models exhibit a large performance drop compared to the results tested on questions without the counterfactual presupposition, indicating that there is still room for improving vision language models. Additionally, the authors find a large gap between GPT-4 and current open-source models.Here are the three points in Simplified Chinese text:* For: 这篇论文目的是为了评估多模态大语言模型的反事实理解能力。* Methods: 作者使用VQAv2集成 dataset，并将问题中添加反事实前提，然后使用ChatGPT生成反事实问题和答案。他们手动检查所有生成的问题和答案，以确保正确性。* Results: 作者在新收集的测试集上评估了最近的视觉语言模型，发现所有模型在反事实前提下的表现均下降了较大，这表明还有很大的空间用于发展视觉语言模型。此外，作者发现GPT-4和当前开源模型之间存在很大的差距。<details>
<summary>Abstract</summary>
Counterfactual reasoning ability is one of the core abilities of human intelligence. This reasoning process involves the processing of alternatives to observed states or past events, and this process can improve our ability for planning and decision-making. In this work, we focus on benchmarking the counterfactual reasoning ability of multi-modal large language models. We take the question and answer pairs from the VQAv2 dataset and add one counterfactual presupposition to the questions, with the answer being modified accordingly. After generating counterfactual questions and answers using ChatGPT, we manually examine all generated questions and answers to ensure correctness. Over 2k counterfactual question and answer pairs are collected this way. We evaluate recent vision language models on our newly collected test dataset and found that all models exhibit a large performance drop compared to the results tested on questions without the counterfactual presupposition. This result indicates that there still exists space for developing vision language models. Apart from the vision language models, our proposed dataset can also serves as a benchmark for evaluating the ability of code generation LLMs, results demonstrate a large gap between GPT-4 and current open-source models. Our code and dataset are available at \url{https://github.com/Letian2003/C-VQA}.
</details>
<details>
<summary>摘要</summary>
《 counterfactual 理解能力是人类智能核心能力之一。这种理解过程包括评估观察到的状态或过去事件的 alternativas，可以提高我们的规划和决策能力。在这项工作中，我们将关注多模态大语言模型的 counterfactual 理解能力。我们从 VQAv2 数据集中提取了问题和答案对，并在其中添加了 counterfactual 前提，答案相应地被修改。通过使用 ChatGPT 生成 counterfactual 问题和答案，我们手动检查所有生成的问题和答案，以确保正确性。共收集了超过 2k 个 counterfactual 问题和答案对。我们对最新的视觉语言模型进行评估，发现所有模型在我们新收集的测试数据集上表现出大量的性能下降，这表明还存在开发视觉语言模型的空间。此外，我们的提出的数据集也可以用于评估代码生成 LLMS，结果显示 GPT-4 与当前开源模型存在很大差距。我们的代码和数据集可以在 <https://github.com/Letian2003/C-VQA> 上获取。》Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="No-Pitch-Left-Behind-Addressing-Gender-Unbalance-in-Automatic-Speech-Recognition-through-Pitch-Manipulation"><a href="#No-Pitch-Left-Behind-Addressing-Gender-Unbalance-in-Automatic-Speech-Recognition-through-Pitch-Manipulation" class="headerlink" title="No Pitch Left Behind: Addressing Gender Unbalance in Automatic Speech Recognition through Pitch Manipulation"></a>No Pitch Left Behind: Addressing Gender Unbalance in Automatic Speech Recognition through Pitch Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06590">http://arxiv.org/abs/2310.06590</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hlt-mt/fbk-fairseq">https://github.com/hlt-mt/fbk-fairseq</a></li>
<li>paper_authors: Dennis Fucci, Marco Gaido, Matteo Negri, Mauro Cettolo, Luisa Bentivogli</li>
<li>for: 提高女性发音识别精度（End-to-end neural architectures）</li>
<li>methods: 使用基频和形板数据的数据增强技术（Data augmentation technique）</li>
<li>results: 对女性发音的识别精度提高9.87%，特别是对最少表示的基频范围内的发音进行了更大的改进。<details>
<summary>Abstract</summary>
Automatic speech recognition (ASR) systems are known to be sensitive to the sociolinguistic variability of speech data, in which gender plays a crucial role. This can result in disparities in recognition accuracy between male and female speakers, primarily due to the under-representation of the latter group in the training data. While in the context of hybrid ASR models several solutions have been proposed, the gender bias issue has not been explicitly addressed in end-to-end neural architectures. To fill this gap, we propose a data augmentation technique that manipulates the fundamental frequency (f0) and formants. This technique reduces the data unbalance among genders by simulating voices of the under-represented female speakers and increases the variability within each gender group. Experiments on spontaneous English speech show that our technique yields a relative WER improvement up to 9.87% for utterances by female speakers, with larger gains for the least-represented f0 ranges.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="FTFT-efficient-and-robust-Fine-Tuning-by-transFerring-Training-dynamics"><a href="#FTFT-efficient-and-robust-Fine-Tuning-by-transFerring-Training-dynamics" class="headerlink" title="FTFT: efficient and robust Fine-Tuning by transFerring Training dynamics"></a>FTFT: efficient and robust Fine-Tuning by transFerring Training dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06588">http://arxiv.org/abs/2310.06588</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yupei Du, Albert Gatt, Dong Nguyen</li>
<li>for: 提高大型预训练语言模型（PLM）的 Robustness 性能</li>
<li>methods: 使用 Data Map 方法，包括在参考模型上进行 fine-tuning，然后选择一部分重要的训练示例，并在这些选择的示例上进行 fine-tuning</li>
<li>results: 比起 conventional Empirical Risk Minimization (ERM)，使用 Fine-Tuning by transFerring Training dynamics (FTFT) 方法可以更快速地达到更好的泛化 robustness 性能，同时占用训练成本的一半。<details>
<summary>Abstract</summary>
Despite the massive success of fine-tuning large Pre-trained Language Models (PLMs) on a wide range of Natural Language Processing (NLP) tasks, they remain susceptible to out-of-distribution (OOD) and adversarial inputs. Data map (DM) is a simple yet effective dual-model approach that enhances the robustness of fine-tuned PLMs, which involves fine-tuning a model on the original training set (i.e. reference model), selecting a specified fraction of important training examples according to the training dynamics of the reference model, and fine-tuning the same model on these selected examples (i.e. main model). However, it suffers from the drawback of requiring fine-tuning the same model twice, which is computationally expensive for large models. In this paper, we first show that 1) training dynamics are highly transferable across different model sizes and different pre-training methods, and that 2) main models fine-tuned using DM learn faster than when using conventional Empirical Risk Minimization (ERM). Building on these observations, we propose a novel fine-tuning approach based on the DM method: Fine-Tuning by transFerring Training dynamics (FTFT). Compared with DM, FTFT uses more efficient reference models and then fine-tunes more capable main models for fewer steps. Our experiments show that FTFT achieves better generalization robustness than ERM while spending less than half of the training cost.
</details>
<details>
<summary>摘要</summary>
尽管大型预训言语模型（PLM）的精细调整在各种自然语言处理（NLP）任务上取得了巨大成功，但它们仍然容易受到生成外部输入（OOD）和恶意输入的影响。数据映射（DM）是一种简单 yet有效的双模型方法，可以提高精细调整后PLM的Robustness，该方法包括将引用模型（i.e. reference model）在原始训练集上进行精细调整，然后选择该模型在训练动态中的一定比率的重要训练示例，并将该示例精细调整到同一模型上（i.e. main model）。然而，它的缺点在于需要两次精细调整同一模型，这会对大型模型来说很 computationally expensive。在这篇论文中，我们首先表明了以下两点：1）训练动态在不同的模型大小和预训练方法之间具有很高的传递性，2）使用DM方法精细调整的主模型在训练过程中更快速地 converges。基于这些观察，我们提出了一种基于DM方法的新的精细调整方法：FTFT（Fine-Tuning by transFerring Training dynamics）。相比DM，FTFT使用更有效的引用模型，然后精细调整更有能力的主模型，需要更少的训练步骤。我们的实验表明，FTFT在比ERM更好的泛化 Robustness 的同时，训练成本也比ERM低于一半。
</details></li>
</ul>
<hr>
<h2 id="AutoCycle-VC-Towards-Bottleneck-Independent-Zero-Shot-Cross-Lingual-Voice-Conversion"><a href="#AutoCycle-VC-Towards-Bottleneck-Independent-Zero-Shot-Cross-Lingual-Voice-Conversion" class="headerlink" title="AutoCycle-VC: Towards Bottleneck-Independent Zero-Shot Cross-Lingual Voice Conversion"></a>AutoCycle-VC: Towards Bottleneck-Independent Zero-Shot Cross-Lingual Voice Conversion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06546">http://arxiv.org/abs/2310.06546</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haeyun Choi, Jio Gim, Yuho Lee, Youngin Kim, Young-Joo Suh</li>
<li>for: 这个论文提出了一种简单而强大的零shot语音转换系统，该系统使用一个循环结构和MEL-spectrogram预处理。之前的works因过度依赖瓶颈结构而导致信息损失和差异化synthesis质量。此外，仅仅通过自我重建损失来重建不同的speaker的语音也是一个问题。</li>
<li>methods: 我们提出了一种循环一致损失，该损失考虑了转换回和转换过的target和source speaker之间的对应关系。此外，我们还使用了堆栈随机洗涤的MEL-spectrogram和标签平滑方法来在speaker encoder训练中提取时间独立的全局speaker表示。</li>
<li>results: 我们的模型在对比之前的state-of-the-art结果时表现出色，并在主观和客观评估中都达到了更高的评价标准。此外，我们的模型还可以实现 cross-lingual语音转换和提高synthesized语音的质量。<details>
<summary>Abstract</summary>
This paper proposes a simple and robust zero-shot voice conversion system with a cycle structure and mel-spectrogram pre-processing. Previous works suffer from information loss and poor synthesis quality due to their reliance on a carefully designed bottleneck structure. Moreover, models relying solely on self-reconstruction loss struggled with reproducing different speakers' voices. To address these issues, we suggested a cycle-consistency loss that considers conversion back and forth between target and source speakers. Additionally, stacked random-shuffled mel-spectrograms and a label smoothing method are utilized during speaker encoder training to extract a time-independent global speaker representation from speech, which is the key to a zero-shot conversion. Our model outperforms existing state-of-the-art results in both subjective and objective evaluations. Furthermore, it facilitates cross-lingual voice conversions and enhances the quality of synthesized speech.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种简单而可靠的零shot语音转换系统，具有一个循环结构和mel-spectrogram预处理。前一些工作受到瓶颈结构的限制，导致信息损失和Synthesis质量不佳。而且，仅仅依靠自我重建损失的模型很难复制不同的发音者的voice。为了解决这些问题，我们建议了一种循环一致损失，考虑 conversions between 目标和源发音者。此外，我们在Speaker encoder训练时使用了Random-shuffled mel-spectrograms和标签平滑方法，以提取speech中的时间独立的全局发音者表示。这是零shot转换的关键。我们的模型在主观和客观评估中都超过了现有的状态场的结果，并且允许跨语言的语音转换和提高合成语音质量。
</details></li>
</ul>
<hr>
<h2 id="EmoTwiCS-A-Corpus-for-Modelling-Emotion-Trajectories-in-Dutch-Customer-Service-Dialogues-on-Twitter"><a href="#EmoTwiCS-A-Corpus-for-Modelling-Emotion-Trajectories-in-Dutch-Customer-Service-Dialogues-on-Twitter" class="headerlink" title="EmoTwiCS: A Corpus for Modelling Emotion Trajectories in Dutch Customer Service Dialogues on Twitter"></a>EmoTwiCS: A Corpus for Modelling Emotion Trajectories in Dutch Customer Service Dialogues on Twitter</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06536">http://arxiv.org/abs/2310.06536</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sofie Labat, Thomas Demeester, Véronique Hoste<br>for:这篇论文的目的是为了提供一个有用的满足客户需求的社交媒体上的客户服务对话集，以便在这些平台上自动检测情绪。methods:这篇论文使用的方法包括Twitter上的客户服务对话集的收集和标注，并对这些对话中的情绪进行了分类和评价。results:这篇论文的结果包括一个高质量的情绪演变轨迹数据集，以及对这些数据集的多种分析和应用。<details>
<summary>Abstract</summary>
Due to the rise of user-generated content, social media is increasingly adopted as a channel to deliver customer service. Given the public character of these online platforms, the automatic detection of emotions forms an important application in monitoring customer satisfaction and preventing negative word-of-mouth. This paper introduces EmoTwiCS, a corpus of 9,489 Dutch customer service dialogues on Twitter that are annotated for emotion trajectories. In our business-oriented corpus, we view emotions as dynamic attributes of the customer that can change at each utterance of the conversation. The term `emotion trajectory' refers therefore not only to the fine-grained emotions experienced by customers (annotated with 28 labels and valence-arousal-dominance scores), but also to the event happening prior to the conversation and the responses made by the human operator (both annotated with 8 categories). Inter-annotator agreement (IAA) scores on the resulting dataset are substantial and comparable with related research, underscoring its high quality. Given the interplay between the different layers of annotated information, we perform several in-depth analyses to investigate (i) static emotions in isolated tweets, (ii) dynamic emotions and their shifts in trajectory, and (iii) the role of causes and response strategies in emotion trajectories. We conclude by listing the advantages and limitations of our dataset, after which we give some suggestions on the different types of predictive modelling tasks and open research questions to which EmoTwiCS can be applied. The dataset is available upon request and will be made publicly available upon acceptance of the paper.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:由于用户生成内容的升起，社交媒体越来越被用作客服渠道。由于这些在线平台的公共性，自动检测情感的应用变得非常重要，以监测客户满意度并避免负面Word of mouth。本文介绍了 EmoTwiCS，一个包含9489个荷兰客服对话的推特数据集，每个对话都被注释为情感轨迹。在我们的商业化数据集中，我们视情感为客户的动态特性，可以在每个对话中改变。“情感轨迹”这个术语不仅包括客户经验的细腻情感（通过28个标签和挥腾评分得分），还包括对话之前的事件和人工操作员的回应（两者各被注释为8个类别）。结果的交互注释者一致性（IAA）分数很高，与相关研究相当，这证明数据的高质量。由于不同层次的注释信息之间的互动，我们进行了多种深入分析， investigate (i) 隔离 tweet 中的静态情感， (ii) 情感的变化和轨迹的转折，以及 (iii) 事件和回应策略在情感轨迹中的作用。我们 conclude 后列出了数据集的优点和限制，然后给出了针对不同预测模型任务和开放研究 вопро题的建议。数据集可以在请求时获得，并在文章接受后公开发布。
</details></li>
</ul>
<hr>
<h2 id="Toward-Semantic-Publishing-in-Non-Invasive-Brain-Stimulation-A-Comprehensive-Analysis-of-rTMS-Studies"><a href="#Toward-Semantic-Publishing-in-Non-Invasive-Brain-Stimulation-A-Comprehensive-Analysis-of-rTMS-Studies" class="headerlink" title="Toward Semantic Publishing in Non-Invasive Brain Stimulation: A Comprehensive Analysis of rTMS Studies"></a>Toward Semantic Publishing in Non-Invasive Brain Stimulation: A Comprehensive Analysis of rTMS Studies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06517">http://arxiv.org/abs/2310.06517</a></li>
<li>repo_url: None</li>
<li>paper_authors: Swathi Anil, Jennifer D’Souza</li>
<li>for: 这篇论文目的是推动不侵入性脑刺激（NIBS）领域的交叉学科合作，以普遍采用计算机科学 semantics 报道方法来标准化 Neuroscience NIBS 研究的描述，使其能够被复制、访问、共享和重用（FAIR）。</li>
<li>methods: 本论文使用了大规模系统性审查，对 600 篇复合性Transcranial Magnetic Stimulation（rTMS）研究进行了描述，并描述了这些研究的关键特征，以便在结构化的描述和比较中使用。</li>
<li>results: 本论文通过实施 FAIR Semantic Web 资源（s）基本publishing 方案，对 600 篇审查的 rTMS 研究进行了 semantic publishing 在知识图库中。<details>
<summary>Abstract</summary>
Noninvasive brain stimulation (NIBS) encompasses transcranial stimulation techniques that can influence brain excitability. These techniques have the potential to treat conditions like depression, anxiety, and chronic pain, and to provide insights into brain function. However, a lack of standardized reporting practices limits its reproducibility and full clinical potential. This paper aims to foster interinterdisciplinarity toward adopting Computer Science Semantic reporting methods for the standardized documentation of Neuroscience NIBS studies making them explicitly Findable, Accessible, Interoperable, and Reusable (FAIR).   In a large-scale systematic review of 600 repetitive transcranial magnetic stimulation (rTMS), a subarea of NIBS, dosages, we describe key properties that allow for structured descriptions and comparisons of the studies. This paper showcases the semantic publishing of NIBS in the ecosphere of knowledge-graph-based next-generation scholarly digital libraries. Specifically, the FAIR Semantic Web resource(s)-based publishing paradigm is implemented for the 600 reviewed rTMS studies in the Open Research Knowledge Graph.
</details>
<details>
<summary>摘要</summary>
非侵入性脑刺激（NIBS）涵盖了跨脑刺激技术，可以影响脑部活动。这些技术有可能用于治疗厌食症、抑郁症和慢性疼痛等疾病，并提供脑功能的知识。然而，由于报告方法不够标准化，NIBS的复制性和临床潜力受到限制。这篇文章的目的是推动不同领域的学者共同努力，以采用计算机科学 semantic 报告方法，为脑科学 NIBS 研究提供标准化的描述和比较。在600例重复脑刺激（rTMS）系统性回顾中，我们描述了允许结构化描述和比较研究的关键性质。这篇文章展示了 NIBS 在知识图像基础的下一代学术数字图书馆中的semantic publishing paradigm。具体来说，本文使用 FAIR Semantic Web 资源（s）基于的发布方式，对600例回顾的 rTMS 研究进行开放式研究知识图像中的发布。
</details></li>
</ul>
<hr>
<h2 id="The-Limits-of-ChatGPT-in-Extracting-Aspect-Category-Opinion-Sentiment-Quadruples-A-Comparative-Analysis"><a href="#The-Limits-of-ChatGPT-in-Extracting-Aspect-Category-Opinion-Sentiment-Quadruples-A-Comparative-Analysis" class="headerlink" title="The Limits of ChatGPT in Extracting Aspect-Category-Opinion-Sentiment Quadruples: A Comparative Analysis"></a>The Limits of ChatGPT in Extracting Aspect-Category-Opinion-Sentiment Quadruples: A Comparative Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06502">http://arxiv.org/abs/2310.06502</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiancai Xu, Jia-Dong Zhang, Rongchang Xiao, Lei Xiong</li>
<li>for: 本研究是为了检验ChatGPT是否可以在文本中提取复杂的四元组（即属性-类别-意见-情感）。</li>
<li>methods: 本研究使用了特制的提示模板，以便ChatGPT可以有效地处理这个复杂的四元组提取任务。 此外，我们还提出了一种基于少量示例的选择方法，以完全利用ChatGPT的内在学习能力并提高其效iveness在这个任务上。</li>
<li>results: 我们对ChatGPT与现有状态的四元组提取模型进行了比较，并在四个公共数据集上进行了评估。我们发现ChatGPT在这个任务上的表现不佳，但是它在某些情况下表现出了良好的能力。<details>
<summary>Abstract</summary>
Recently, ChatGPT has attracted great attention from both industry and academia due to its surprising abilities in natural language understanding and generation. We are particularly curious about whether it can achieve promising performance on one of the most complex tasks in aspect-based sentiment analysis, i.e., extracting aspect-category-opinion-sentiment quadruples from texts. To this end, in this paper we develop a specialized prompt template that enables ChatGPT to effectively tackle this complex quadruple extraction task. Further, we propose a selection method on few-shot examples to fully exploit the in-context learning ability of ChatGPT and uplift its effectiveness on this complex task. Finally, we provide a comparative evaluation on ChatGPT against existing state-of-the-art quadruple extraction models based on four public datasets and highlight some important findings regarding the capability boundaries of ChatGPT in the quadruple extraction.
</details>
<details>
<summary>摘要</summary>
近期，ChatGPT已经吸引了行业和学术界的广泛关注，因为它在自然语言理解和生成方面表现出了惊人的能力。我们尤其关注ChatGPT是否可以在一个最复杂的任务中表现出色，即从文本中提取方面-类别-意见-情感四元组。为此，在这篇论文中，我们开发了特有的提示模板，使得ChatGPT能够有效地解决这个复杂的四元组提取任务。此外，我们提出了基于少量示例选择的方法，以充分利用ChatGPT在上下文学习中的能力，提高它在这个任务上的效iveness。最后，我们对ChatGPT与现有状态的四元组提取模型进行了比较评估，并发现了一些关于ChatGPT在四元组提取任务上的能力边界的重要发现。
</details></li>
</ul>
<hr>
<h2 id="A-New-Benchmark-and-Reverse-Validation-Method-for-Passage-level-Hallucination-Detection"><a href="#A-New-Benchmark-and-Reverse-Validation-Method-for-Passage-level-Hallucination-Detection" class="headerlink" title="A New Benchmark and Reverse Validation Method for Passage-level Hallucination Detection"></a>A New Benchmark and Reverse Validation Method for Passage-level Hallucination Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06498">http://arxiv.org/abs/2310.06498</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/maybenotime/phd">https://github.com/maybenotime/phd</a></li>
<li>paper_authors: Shiping Yang, Renliang Sun, Xiaojun Wan</li>
<li>for: 本研究旨在提出一种自我检查方法，以检测 LLM 生成的幻见（false information）。</li>
<li>methods: 本方法基于反验证，可以在零资源条件下自动检测幻见。而我们还构建了一个幻见检测 benchmark，名为 PHD，用于评估不同方法的性能。</li>
<li>results: 我们的方法在两个数据集上对比baseline方法表现出色，具有更高的准确率和更低的质量成本。此外，我们还手动分析了 LLM 失败检测的一些例子，发现零资源方法具有共同的限制。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have shown their ability to collaborate effectively with humans in real-world scenarios. However, LLMs are apt to generate hallucinations, i.e., makeup incorrect text and unverified information, which can cause significant damage when deployed for mission-critical tasks. In this paper, we propose a self-check approach based on reverse validation to detect factual errors automatically in a zero-resource fashion. To facilitate future studies and assess different methods, we construct a hallucination detection benchmark named PHD, which is generated by ChatGPT and annotated by human annotators. Contrasting previous studies of zero-resource hallucination detection, our method and benchmark concentrate on passage-level detection instead of sentence-level. We empirically evaluate our method and existing zero-resource detection methods on two datasets. The experimental results demonstrate that the proposed method considerably outperforms the baselines while costing fewer tokens and less time. Furthermore, we manually analyze some hallucination cases that LLM failed to capture, revealing the shared limitation of zero-resource methods.
</details>
<details>
<summary>摘要</summary>
Note: "Simplified Chinese" is a translation of the text into Chinese, using simpler grammar and vocabulary to make it easier to understand for native Chinese speakers. However, please note that the translation may not be perfect and may not capture all the nuances of the original text.
</details></li>
</ul>
<hr>
<h2 id="SpikeCLIP-A-Contrastive-Language-Image-Pretrained-Spiking-Neural-Network"><a href="#SpikeCLIP-A-Contrastive-Language-Image-Pretrained-Spiking-Neural-Network" class="headerlink" title="SpikeCLIP: A Contrastive Language-Image Pretrained Spiking Neural Network"></a>SpikeCLIP: A Contrastive Language-Image Pretrained Spiking Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06488">http://arxiv.org/abs/2310.06488</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianlong Li, Wenhao Liu, Changze Lv, Jianhan Xu, Cenyuan Zhang, Muling Wu, Xiaoqing Zheng, Xuanjing Huang</li>
<li>For: 本文旨在探讨使用刺激神经网络（SNN）在多Modal场景中的扩展，并采用了一种新的框架 named SpikeCLIP，以提高在多Modal场景中的刺激计算的效能。* Methods: 本文使用了一种两步方法，包括“Alignment Pre-training”和“双损失精度调整”，以将刺激计算与深度神经网络（DNN）相结合，从而实现在多Modal场景中的刺激计算。* Results: 实验结果表明，使用SpikeCLIP框架可以在多Modal场景中实现刺激计算的相对比较好的性能，同时减少了能耗量。此外，SpikeCLIP还可以保持在图像分类任务中的稳定性，即使涉及到不在特定类别中的类别标签。<details>
<summary>Abstract</summary>
Spiking neural networks (SNNs) have demonstrated the capability to achieve comparable performance to deep neural networks (DNNs) in both visual and linguistic domains while offering the advantages of improved energy efficiency and adherence to biological plausibility. However, the extension of such single-modality SNNs into the realm of multimodal scenarios remains an unexplored territory. Drawing inspiration from the concept of contrastive language-image pre-training (CLIP), we introduce a novel framework, named SpikeCLIP, to address the gap between two modalities within the context of spike-based computing through a two-step recipe involving ``Alignment Pre-training + Dual-Loss Fine-tuning". Extensive experiments demonstrate that SNNs achieve comparable results to their DNN counterparts while significantly reducing energy consumption across a variety of datasets commonly used for multimodal model evaluation. Furthermore, SpikeCLIP maintains robust performance in image classification tasks that involve class labels not predefined within specific categories.
</details>
<details>
<summary>摘要</summary>
聚合神经网络（SNN）已经表现出与深度神经网络（DNN）相当的性能在视觉和语言领域，而且具有更好的能效性和生物启发性。然而，将单模态SNN扩展到多模态场景仍然是一个未探索的领域。 Drawing inspiration from语言-图像准备（CLIP）的概念，我们提出了一种新的框架，名为SpikeCLIP，以解决在毫 COUNTING  computing中两个模态之间的差异。我们采用了两步方法：“对齐预训练 + 双损失细化”。广泛的实验表明，SNN可以与其DNN对应类型相当，同时具有显著降低能耗的优势。此外，SpikeCLIP在图像分类任务中保持了不受限定类别的稳定性。
</details></li>
</ul>
<hr>
<h2 id="Multilingual-Jailbreak-Challenges-in-Large-Language-Models"><a href="#Multilingual-Jailbreak-Challenges-in-Large-Language-Models" class="headerlink" title="Multilingual Jailbreak Challenges in Large Language Models"></a>Multilingual Jailbreak Challenges in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06474">http://arxiv.org/abs/2310.06474</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/DAMO-NLP-SG/multilingual-safety-for-LLMs">https://github.com/DAMO-NLP-SG/multilingual-safety-for-LLMs</a></li>
<li>paper_authors: Yue Deng, Wenxuan Zhang, Sinno Jialin Pan, Lidong Bing</li>
<li>for: This paper aims to address the safety concerns associated with large language models (LLMs) in the multilingual context, specifically the “jailbreak” problem where malicious instructions can manipulate LLMs to exhibit undesirable behavior.</li>
<li>methods: The paper reveals the presence of multilingual jailbreak challenges within LLMs and considers two potential risk scenarios: unintentional and intentional. The authors experimentally demonstrate that low-resource languages are more susceptible to unsafe content generation, and propose a novel \textsc{Self-Defense} framework for safety fine-tuning.</li>
<li>results: The paper shows that the proposed \textsc{Self-Defense} framework can achieve a substantial reduction in unsafe content generation for ChatGPT, with an 80.92% reduction in unsafe output for the intentional scenario and a three times increase in unsafe content for the unintentional scenario compared to high-resource languages.Here’s the Chinese translation of the three points:</li>
<li>for: 这篇论文目标是解决大语言模型（LLMs）在多语言场景下的安全问题，特别是“监狱”问题，其中恶意指令可以 manipulate LLMs 以产生不жела的行为。</li>
<li>methods: 论文揭示了 LLMs 中的多语言监狱挑战，并考虑了两种风险enario：不计划的和计划的。试验表明，低资源语言存在更高的危险内容生成率，并提出了一种名为 \textsc{Self-Defense} 的新框架，用于安全 fine-tuning。</li>
<li>results: 论文显示，\textsc{Self-Defense} 框架可以减少 ChatGPT 的危险输出，具体来说，对于意外情况，低资源语言的危险内容生成率高三倍于高资源语言，而对于意图情况， \textsc{Self-Defense} 框架可以减少 unsafe 输出的比例为 80.92%。<details>
<summary>Abstract</summary>
While large language models (LLMs) exhibit remarkable capabilities across a wide range of tasks, they pose potential safety concerns, such as the ``jailbreak'' problem, wherein malicious instructions can manipulate LLMs to exhibit undesirable behavior. Although several preventive measures have been developed to mitigate the potential risks associated with LLMs, they have primarily focused on English data. In this study, we reveal the presence of multilingual jailbreak challenges within LLMs and consider two potential risk scenarios: unintentional and intentional. The unintentional scenario involves users querying LLMs using non-English prompts and inadvertently bypassing the safety mechanisms, while the intentional scenario concerns malicious users combining malicious instructions with multilingual prompts to deliberately attack LLMs. The experimental results reveal that in the unintentional scenario, the rate of unsafe content increases as the availability of languages decreases. Specifically, low-resource languages exhibit three times the likelihood of encountering harmful content compared to high-resource languages, with both ChatGPT and GPT-4. In the intentional scenario, multilingual prompts can exacerbate the negative impact of malicious instructions, with astonishingly high rates of unsafe output: 80.92\% for ChatGPT and 40.71\% for GPT-4. To handle such a challenge in the multilingual context, we propose a novel \textsc{Self-Defense} framework that automatically generates multilingual training data for safety fine-tuning. Experimental results show that ChatGPT fine-tuned with such data can achieve a substantial reduction in unsafe content generation. Data is available at https://github.com/DAMO-NLP-SG/multilingual-safety-for-LLMs. Warning: This paper contains examples with potentially harmful content.
</details>
<details>
<summary>摘要</summary>
large language models (LLMs)  display remarkable capabilities across a wide range of tasks, but they also pose potential safety concerns, such as the "jailbreak" problem, where malicious instructions can manipulate LLMs to exhibit undesirable behavior. Although several preventive measures have been developed to mitigate the potential risks associated with LLMs, they have primarily focused on English data. In this study, we reveal the presence of multilingual jailbreak challenges within LLMs and consider two potential risk scenarios: unintentional and intentional. The unintentional scenario involves users querying LLMs using non-English prompts and inadvertently bypassing the safety mechanisms, while the intentional scenario concerns malicious users combining malicious instructions with multilingual prompts to deliberately attack LLMs.our experimental results show that in the unintentional scenario, the rate of unsafe content increases as the availability of languages decreases. Specifically, low-resource languages exhibit three times the likelihood of encountering harmful content compared to high-resource languages, with both ChatGPT and GPT-4. In the intentional scenario, multilingual prompts can exacerbate the negative impact of malicious instructions, with astonishingly high rates of unsafe output: 80.92% for ChatGPT and 40.71% for GPT-4.to handle such a challenge in the multilingual context, we propose a novel \textsc{Self-Defense} framework that automatically generates multilingual training data for safety fine-tuning. Experimental results show that ChatGPT fine-tuned with such data can achieve a substantial reduction in unsafe content generation. Data is available at https://github.com/DAMO-NLP-SG/multilingual-safety-for-LLMs. Warning: This paper contains examples with potentially harmful content.
</details></li>
</ul>
<hr>
<h2 id="Cultural-Compass-Predicting-Transfer-Learning-Success-in-Offensive-Language-Detection-with-Cultural-Features"><a href="#Cultural-Compass-Predicting-Transfer-Learning-Success-in-Offensive-Language-Detection-with-Cultural-Features" class="headerlink" title="Cultural Compass: Predicting Transfer Learning Success in Offensive Language Detection with Cultural Features"></a>Cultural Compass: Predicting Transfer Learning Success in Offensive Language Detection with Cultural Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06458">http://arxiv.org/abs/2310.06458</a></li>
<li>repo_url: None</li>
<li>paper_authors: Li Zhou, Antonia Karamolegkou, Wenyu Chen, Daniel Hershcovich</li>
<li>for: 这个研究旨在探讨文化特征是否能准确预测跨文化传输学习效果，以提高语言技术的包容性和文化敏感性。</li>
<li>methods: 研究者使用了文化价值调查来评估跨文化传输学习的效果，并发现文化价值调查可以预测跨文化传输学习的成功。此外，研究者还发现使用了粗鄙词距可以进一步提高跨文化传输学习的效果。</li>
<li>results: 研究发现文化价值调查indeed possess a predictive power for cross-cultural transfer learning success in OLD tasks, and that it can be further improved using offensive word distance.<details>
<summary>Abstract</summary>
The increasing ubiquity of language technology necessitates a shift towards considering cultural diversity in the machine learning realm, particularly for subjective tasks that rely heavily on cultural nuances, such as Offensive Language Detection (OLD). Current understanding underscores that these tasks are substantially influenced by cultural values, however, a notable gap exists in determining if cultural features can accurately predict the success of cross-cultural transfer learning for such subjective tasks. Addressing this, our study delves into the intersection of cultural features and transfer learning effectiveness. The findings reveal that cultural value surveys indeed possess a predictive power for cross-cultural transfer learning success in OLD tasks and that it can be further improved using offensive word distance. Based on these results, we advocate for the integration of cultural information into datasets. Additionally, we recommend leveraging data sources rich in cultural information, such as surveys, to enhance cultural adaptability. Our research signifies a step forward in the quest for more inclusive, culturally sensitive language technologies.
</details>
<details>
<summary>摘要</summary>
随着语言技术的普及，需要对文化多样性在机器学习领域进行考虑，特别是对于基于文化特点的主观任务，如涉礼语言检测（OLD）。现有研究表明，这类任务受到文化价值的影响，但是存在一定的掌握问题，即可以否准确预测跨文化传输学习的成功。我们的研究团队对此进行了调查，发现文化价值调查确实可以预测跨文化传输学习成功，并且可以通过涉礼词语距离进一步改进。根据这些结果，我们建议将文化信息纳入数据集中，并且建议使用具有文化信息的数据源，如调查，来提高文化适应性。我们的研究表明，针对更包容、文化敏感的语言技术的开发是一步前进。
</details></li>
</ul>
<hr>
<h2 id="MemSum-DQA-Adapting-An-Efficient-Long-Document-Extractive-Summarizer-for-Document-Question-Answering"><a href="#MemSum-DQA-Adapting-An-Efficient-Long-Document-Extractive-Summarizer-for-Document-Question-Answering" class="headerlink" title="MemSum-DQA: Adapting An Efficient Long Document Extractive Summarizer for Document Question Answering"></a>MemSum-DQA: Adapting An Efficient Long Document Extractive Summarizer for Document Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06436">http://arxiv.org/abs/2310.06436</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nianlonggu/memsum-dqa">https://github.com/nianlonggu/memsum-dqa</a></li>
<li>paper_authors: Nianlong Gu, Yingqiang Gao, Richard H. R. Hahnloser</li>
<li>for: 文章主要针对的是文档问答（DQA）任务，旨在提高文档抽取概要的能力。</li>
<li>methods: 该系统使用了 MemSum，一种长文档抽取概要器，进行文档问答。文档被解析为多个块，每个块都附加了提供的问题和问题类型，然后 selectively 提取块作为答案。</li>
<li>results: 与先前的基eline相比，MemSum-DQA在全文 answering 任务上提高了9%的精确匹配率。此外，MemSum-DQA在儿童关系理解方面表现出色，这指示了抽取概要技术在 DQA 任务中的潜在优势。<details>
<summary>Abstract</summary>
We introduce MemSum-DQA, an efficient system for document question answering (DQA) that leverages MemSum, a long document extractive summarizer. By prefixing each text block in the parsed document with the provided question and question type, MemSum-DQA selectively extracts text blocks as answers from documents. On full-document answering tasks, this approach yields a 9% improvement in exact match accuracy over prior state-of-the-art baselines. Notably, MemSum-DQA excels in addressing questions related to child-relationship understanding, underscoring the potential of extractive summarization techniques for DQA tasks.
</details>
<details>
<summary>摘要</summary>
我们介绍MemSum-DQA，一种高效的文档问答系统（DQA），利用MemSum，一种长文档抽取式概要系统。通过在文档中每个文本块前置提供的问题和问题类型，MemSum-DQA选择性地从文档中提取答案。在全文 answering 任务上，这种方法比之前的基线性能提高9%。尤其是在儿童关系理解方面，MemSum-DQA表现出色，这 highlights the potential of 抽取式概要技术在 DQA 任务中。
</details></li>
</ul>
<hr>
<h2 id="Humans-and-language-models-diverge-when-predicting-repeating-text"><a href="#Humans-and-language-models-diverge-when-predicting-repeating-text" class="headerlink" title="Humans and language models diverge when predicting repeating text"></a>Humans and language models diverge when predicting repeating text</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06408">http://arxiv.org/abs/2310.06408</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/HuthLab/lm-repeating-text">https://github.com/HuthLab/lm-repeating-text</a></li>
<li>paper_authors: Aditya R. Vaidya, Javier Turek, Alexander G. Huth</li>
<li>for: 这个研究是为了检验语言模型在下一个单词预测任务中是否能够准确模拟人类行为。</li>
<li>methods: 这个研究使用了GPT-2语言模型和人类参与者的下一个单词预测数据集，并对这些数据进行分析和比较。</li>
<li>results: 研究发现，在第一次显示文本扩展时，人类和语言模型的性能很高相关，但是当memory（或在场景学习）开始发挥作用时，人类和语言模型的性能快速分化。研究发现了这种分化的原因，并通过添加带有力学律回归的注意头来解决这个问题，使模型更像人类。<details>
<summary>Abstract</summary>
Language models that are trained on the next-word prediction task have been shown to accurately model human behavior in word prediction and reading speed. In contrast with these findings, we present a scenario in which the performance of humans and LMs diverges. We collected a dataset of human next-word predictions for five stimuli that are formed by repeating spans of text. Human and GPT-2 LM predictions are strongly aligned in the first presentation of a text span, but their performance quickly diverges when memory (or in-context learning) begins to play a role. We traced the cause of this divergence to specific attention heads in a middle layer. Adding a power-law recency bias to these attention heads yielded a model that performs much more similarly to humans. We hope that this scenario will spur future work in bringing LMs closer to human behavior.
</details>
<details>
<summary>摘要</summary>
语言模型，它们在下一个词预测任务上训练，已经能够准确地模拟人类行为。然而，我们提出了一种情况，在这种情况下，人类和语言模型（LM）的性能开始分化。我们收集了五个句子的人类下一个词预测数据集。人类和GPT-2语言模型在第一次文本段的预测 task 上强相关，但是他们的性能很快地分化，当内存（或在场景学习）开始发挥作用时。我们追踪了这种分化的原因，发现了特定的注意头在中间层。将power-law recency bias添加到这些注意头可以创建一个与人类更相似的模型。我们希望这种情况能够促进未来的研究，使语言模型更接近人类行为。
</details></li>
</ul>
<hr>
<h2 id="Improved-prompting-and-process-for-writing-user-personas-with-LLMs-using-qualitative-interviews-Capturing-behaviour-and-personality-traits-of-users"><a href="#Improved-prompting-and-process-for-writing-user-personas-with-LLMs-using-qualitative-interviews-Capturing-behaviour-and-personality-traits-of-users" class="headerlink" title="Improved prompting and process for writing user personas with LLMs, using qualitative interviews: Capturing behaviour and personality traits of users"></a>Improved prompting and process for writing user personas with LLMs, using qualitative interviews: Capturing behaviour and personality traits of users</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06391">http://arxiv.org/abs/2310.06391</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefano De Paoli<br>for:The paper aims to present a workflow for creating user personas using large language models, specifically through the results of thematic analysis of qualitative interviews.methods:The proposed workflow utilizes improved prompting and a larger pool of themes compared to previous work by the author, made possible by the capabilities of a recently released large language model (GPT3.5-Turbo-16k) and refined prompting for creating personas.results:The paper discusses the improved workflow for creating personas and offers reflections on the relationship between the proposed process and existing approaches to personas, as well as the capacity of LLMs to capture user behaviors and personality traits from the underlying dataset of qualitative interviews used for analysis.<details>
<summary>Abstract</summary>
This draft paper presents a workflow for creating User Personas with Large Language Models, using the results of a Thematic Analysis of qualitative interviews. The proposed workflow uses improved prompting and a larger pool of Themes, compared to previous work conducted by the author for the same task. This is possible due to the capabilities of a recently released LLM which allows the processing of 16 thousand tokens (GPT3.5-Turbo-16k) and also due to the possibility to offer a refined prompting for the creation of Personas. The paper offers details of performing Phase 2 and 3 of Thematic Analysis, and then discusses the improved workflow for creating Personas. The paper also offers some reflections on the relationship between the proposed process and existing approaches to Personas such as the data-driven and qualitative Personas. Moreover, the paper offers reflections on the capacity of LLMs to capture user behaviours and personality traits, from the underlying dataset of qualitative interviews used for the analysis.
</details>
<details>
<summary>摘要</summary>
这份草稿文章介绍了使用大语言模型创建用户人物的工作流程，基于论题分析的访谈结果。提议的工作流程使用改进的提示和更大的主题池，比前一作者为同任务所做的工作更好。这几乎可以归功于最近发布的LLM，它可以处理16千个字符（GPT3.5-Turbo-16k），以及可以提供更精细的提示 для创建人物。文章详细介绍了执行阶段2和3的论题分析，然后讨论了改进的工作流程。文章还提供了关于提案过程和现有方法人物之间的关系的反思，以及LLM对用户行为和人格特征的捕捉能力。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Model-Selection-and-Decoding-for-Keyphrase-Generation-with-Pre-trained-Sequence-to-Sequence-Models"><a href="#Rethinking-Model-Selection-and-Decoding-for-Keyphrase-Generation-with-Pre-trained-Sequence-to-Sequence-Models" class="headerlink" title="Rethinking Model Selection and Decoding for Keyphrase Generation with Pre-trained Sequence-to-Sequence Models"></a>Rethinking Model Selection and Decoding for Keyphrase Generation with Pre-trained Sequence-to-Sequence Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06374">http://arxiv.org/abs/2310.06374</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/uclanlp/deepkpg">https://github.com/uclanlp/deepkpg</a></li>
<li>paper_authors: Di Wu, Wasi Uddin Ahmad, Kai-Wei Chang</li>
<li>for: 本研究旨在系统地研究基于语言模型（PLM）的关键短语生成（KPG）任务中，不同的模型选择和解码策略的影响。</li>
<li>methods: 本研究使用了seq2seq预训练语言模型（PLM）来进行KPG任务，并系统地分析了不同的模型选择和解码策略对KPG任务的影响。</li>
<li>results: 研究发现，在选择PLM模型时，仅增加模型大小或进行任务特定适应并不是parameterfficient的; 在解码方面，使用抽样查找方法可以提高F1分数，但是它在回味方面落后于简单搜索方法。基于这些发现，本研究提出了一种基于概率的decode-select算法，可以改进greedy搜索。<details>
<summary>Abstract</summary>
Keyphrase Generation (KPG) is a longstanding task in NLP with widespread applications. The advent of sequence-to-sequence (seq2seq) pre-trained language models (PLMs) has ushered in a transformative era for KPG, yielding promising performance improvements. However, many design decisions remain unexplored and are often made arbitrarily. This paper undertakes a systematic analysis of the influence of model selection and decoding strategies on PLM-based KPG. We begin by elucidating why seq2seq PLMs are apt for KPG, anchored by an attention-driven hypothesis. We then establish that conventional wisdom for selecting seq2seq PLMs lacks depth: (1) merely increasing model size or performing task-specific adaptation is not parameter-efficient; (2) although combining in-domain pre-training with task adaptation benefits KPG, it does partially hinder generalization. Regarding decoding, we demonstrate that while greedy search achieves strong F1 scores, it lags in recall compared with sampling-based methods. Based on these insights, we propose DeSel, a likelihood-based decode-select algorithm for seq2seq PLMs. DeSel improves greedy search by an average of 4.7% semantic F1 across five datasets. Our collective findings pave the way for deeper future investigations into PLM-based KPG.
</details>
<details>
<summary>摘要</summary>
《键签生成（KPG）是NLPT中长期任务，广泛应用。 seq2seq预训练语言模型（PLM）的出现，为KPG带来了转变性的时代，提高性能。然而，许多设计决策仍然未经探索，经常采取优化的方式。本文进行了系统性的分析，探讨PLM基于KPG的模型选择和解码策略对Seq2Seq PLM的影响。我们开始由Seq2Seq PLM适用于KPG的原因，基于注意力驱动的假设。然后，我们发现了现有的Seq2Seq PLM选择方法的缺陷：（1）仅通过增加模型大小或进行任务特定的适应，不能减少参数的效率；（2）虽然结合域内预训练和任务适应可以提高KPG，但也会部分削弱泛化性。对于解码，我们表明了批量搜索可以 дости得强大的F1分数，但在回归方面落后于抽样方法。基于这些发现，我们提出了DeSel算法，它是基于概率的解码-选择算法，可以改进批量搜索。DeSel在五个数据集上提高了4.7%的语义F1分数。我们的总体发现可以为PLM基于KPG的未来研究开辟道路。》
</details></li>
</ul>
<hr>
<h2 id="Multi-Modal-Knowledge-Graph-Transformer-Framework-for-Multi-Modal-Entity-Alignment"><a href="#Multi-Modal-Knowledge-Graph-Transformer-Framework-for-Multi-Modal-Entity-Alignment" class="headerlink" title="Multi-Modal Knowledge Graph Transformer Framework for Multi-Modal Entity Alignment"></a>Multi-Modal Knowledge Graph Transformer Framework for Multi-Modal Entity Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06365">http://arxiv.org/abs/2310.06365</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xiaoqian19940510/moalign">https://github.com/xiaoqian19940510/moalign</a></li>
<li>paper_authors: Qian Li, Cheng Ji, Shu Guo, Zhaoji Liang, Lihong Wang, Jianxin Li</li>
<li>for: 提高多ModalEntityAlignment（MMEA）任务的性能，解决多Modal知识图（MMKG）中Equivalent entity pair的匹配问题。</li>
<li>methods: 提出了一种新的MMEA transformer，即MoAlign，通过针对不同类型信息（邻近实体、多Modal特征、实体类型）的层次引入，提高匹配任务的准确率。</li>
<li>results: 对多个benchmark dataset进行了广泛的实验，得到了优秀的实体匹配性能，比STRONG竞争对手更高。<details>
<summary>Abstract</summary>
Multi-Modal Entity Alignment (MMEA) is a critical task that aims to identify equivalent entity pairs across multi-modal knowledge graphs (MMKGs). However, this task faces challenges due to the presence of different types of information, including neighboring entities, multi-modal attributes, and entity types. Directly incorporating the above information (e.g., concatenation or attention) can lead to an unaligned information space. To address these challenges, we propose a novel MMEA transformer, called MoAlign, that hierarchically introduces neighbor features, multi-modal attributes, and entity types to enhance the alignment task. Taking advantage of the transformer's ability to better integrate multiple information, we design a hierarchical modifiable self-attention block in a transformer encoder to preserve the unique semantics of different information. Furthermore, we design two entity-type prefix injection methods to integrate entity-type information using type prefixes, which help to restrict the global information of entities not present in the MMKGs. Our extensive experiments on benchmark datasets demonstrate that our approach outperforms strong competitors and achieves excellent entity alignment performance.
</details>
<details>
<summary>摘要</summary>
多modalEntityAlignment（MMEA）是一个关键任务，旨在在多modal知识图（MMKG）中寻找等价实体对。然而，这个任务面临着不同类型的信息的存在，包括邻居实体、多modal特征和实体类型。直接包含这些信息（例如， concatenation 或 attention）可能会导致不一致的信息空间。为了解决这些挑战，我们提出了一种新的MMEA transformer，called MoAlign，它在多modal知识图中层次引入邻居特征、多modal特征和实体类型，以提高对齐任务。利用trasnformer的能力更好地集成多种信息，我们设计了一个层次可变自注意力块，以保持不同信息的唯一 semantics。此外，我们设计了两种实体类型前缀注入方法，以integrate实体类型信息使用类型前缀，帮助限制global信息的实体不在MMKG中。我们对标准数据集进行了广泛的实验，demonstrate that our approach outperforms strong competitors and achieves excellent entity alignment performance.
</details></li>
</ul>
<hr>
<h2 id="InfoCL-Alleviating-Catastrophic-Forgetting-in-Continual-Text-Classification-from-An-Information-Theoretic-Perspective"><a href="#InfoCL-Alleviating-Catastrophic-Forgetting-in-Continual-Text-Classification-from-An-Information-Theoretic-Perspective" class="headerlink" title="InfoCL: Alleviating Catastrophic Forgetting in Continual Text Classification from An Information Theoretic Perspective"></a>InfoCL: Alleviating Catastrophic Forgetting in Continual Text Classification from An Information Theoretic Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06362">http://arxiv.org/abs/2310.06362</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yifan-song793/infocl">https://github.com/yifan-song793/infocl</a></li>
<li>paper_authors: Yifan Song, Peiyi Wang, Weimin Xiong, Dawei Zhu, Tianyu Liu, Zhifang Sui, Sujian Li</li>
<li>for: 本研究旨在提出一种新的 continual learning 方法，以解决在类增cremental 设定下的 forgetting 问题。</li>
<li>methods: 我们提出了一种基于信息瓶颈的 representation learning 方法，并使用了 fast-slow 和 current-past 对比学习以提高表征学习过程。 </li>
<li>results: 我们的方法可以有效地避免 forgetting 问题，并在三个文本分类任务上实现了 state-of-the-art 的性能。<details>
<summary>Abstract</summary>
Continual learning (CL) aims to constantly learn new knowledge over time while avoiding catastrophic forgetting on old tasks. We focus on continual text classification under the class-incremental setting. Recent CL studies have identified the severe performance decrease on analogous classes as a key factor for catastrophic forgetting. In this paper, through an in-depth exploration of the representation learning process in CL, we discover that the compression effect of the information bottleneck leads to confusion on analogous classes. To enable the model learn more sufficient representations, we propose a novel replay-based continual text classification method, InfoCL. Our approach utilizes fast-slow and current-past contrastive learning to perform mutual information maximization and better recover the previously learned representations. In addition, InfoCL incorporates an adversarial memory augmentation strategy to alleviate the overfitting problem of replay. Experimental results demonstrate that InfoCL effectively mitigates forgetting and achieves state-of-the-art performance on three text classification tasks. The code is publicly available at https://github.com/Yifan-Song793/InfoCL.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese: kontinual learning (CL) 目标是不断学习新知识，而避免在老任务上出现致命忘记。我们在类增量设定下进行文本分类 continual learning。 current CL 研究表明，在相似类上的性能下降是致命忘记的关键因素。在这篇文章中，我们通过 Continual learning 的表征学习过程的深入探索，发现信息瓶颈压缩的效果导致了相似类的混淆。为了让模型学习更加充分的表示，我们提议一种基于 InfoCL 的循环学习方法。我们的方法通过快慢学习和当前过去的对比学习来实现对信息的最大化。此外，InfoCL 还包括一种对抗记忆增强策略，以解决回放中的过拟合问题。实验结果表明，InfoCL 有效地避免了致命忘记，并在三个文本分类任务上达到了状态的最佳性能。代码可以在 https://github.com/Yifan-Song793/InfoCL 上获取。
</details></li>
</ul>
<hr>
<h2 id="A-Semantic-Invariant-Robust-Watermark-for-Large-Language-Models"><a href="#A-Semantic-Invariant-Robust-Watermark-for-Large-Language-Models" class="headerlink" title="A Semantic Invariant Robust Watermark for Large Language Models"></a>A Semantic Invariant Robust Watermark for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06356">http://arxiv.org/abs/2310.06356</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aiwei Liu, Leyi Pan, Xuming Hu, Shiao Meng, Lijie Wen</li>
<li>for: 本研究旨在提出一种semantic invariant watermarking方法，以提高LLMs中文生成器的攻击Robustness和安全Robustness。</li>
<li>methods: 本方法使用另一个嵌入LM生成所有前导token的semantic embedding，然后将这些semantic embedding转化为 watermark logits through our trained watermark model。</li>
<li>results: 研究表明，我们的方法在semantically invariant setting中具有高度的攻击Robustness和安全Robustness。此外，我们的 watermark还具有足够的安全Robustness。<details>
<summary>Abstract</summary>
Watermark algorithms for large language models (LLMs) have achieved extremely high accuracy in detecting text generated by LLMs. Such algorithms typically involve adding extra watermark logits to the LLM's logits at each generation step. However, prior algorithms face a trade-off between attack robustness and security robustness. This is because the watermark logits for a token are determined by a certain number of preceding tokens; a small number leads to low security robustness, while a large number results in insufficient attack robustness. In this work, we propose a semantic invariant watermarking method for LLMs that provides both attack robustness and security robustness. The watermark logits in our work are determined by the semantics of all preceding tokens. Specifically, we utilize another embedding LLM to generate semantic embeddings for all preceding tokens, and then these semantic embeddings are transformed into the watermark logits through our trained watermark model. Subsequent analyses and experiments demonstrated the attack robustness of our method in semantically invariant settings: synonym substitution and text paraphrasing settings. Finally, we also show that our watermark possesses adequate security robustness. Our code and data are available at https://github.com/THU-BPM/Robust_Watermark.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）的水印算法已经实现了极高的准确率，用于检测由LLM生成的文本。通常，这些算法都是通过在生成步骤中添加额外的水印噢来实现的。然而，先前的算法面临着一种负面的贸易OFF和安全性之间的贸易OFF。这是因为水印噢的某个token是由前一些token决定的，一个小数会导致安全性不足，而一个大数则会导致攻击鲁棒性不足。在这项工作中，我们提出了一种基于 semantics的强水印方法，该方法可以同时提供攻击鲁棒性和安全性。我们的水印噢是由所有前一些token的 semantics 决定的。具体来说，我们使用另一个嵌入式语言模型来生成所有前一些token的 semantics 嵌入，然后将这些 semantics 嵌入转换成水印噢 через我们训练的水印模型。后续的分析和实验表明了我们的方法在semantically invariant的 Setting 中具有攻击鲁棒性。此外，我们还证明了我们的水印具有足够的安全性。我们的代码和数据可以在https://github.com/THU-BPM/Robust_Watermark上获取。
</details></li>
</ul>
<hr>
<h2 id="Selective-Demonstrations-for-Cross-domain-Text-to-SQL"><a href="#Selective-Demonstrations-for-Cross-domain-Text-to-SQL" class="headerlink" title="Selective Demonstrations for Cross-domain Text-to-SQL"></a>Selective Demonstrations for Cross-domain Text-to-SQL</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06302">http://arxiv.org/abs/2310.06302</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuaichen Chang, Eric Fosler-Lussier</li>
<li>for: 本研究旨在探讨大语言模型（LLMs）在cross-domain文本到SQL任务中的泛化能力，以及如何使用域内示例来提高其性能。</li>
<li>methods: 本研究使用了域外示例和生成的域内示例来构建示例集，并提出了一种选择示例框架ODIS。ODIS利用了域外示例和域内示例的优点，并且可以在不含域内标注的情况下进行选择。</li>
<li>results: 对两个cross-domain文本到SQL数据集进行了实验，ODIS比基eline方法提高了1.1和11.8个执行精度点。<details>
<summary>Abstract</summary>
Large language models (LLMs) with in-context learning have demonstrated impressive generalization capabilities in the cross-domain text-to-SQL task, without the use of in-domain annotations. However, incorporating in-domain demonstration examples has been found to greatly enhance LLMs' performance. In this paper, we delve into the key factors within in-domain examples that contribute to the improvement and explore whether we can harness these benefits without relying on in-domain annotations. Based on our findings, we propose a demonstration selection framework ODIS which utilizes both out-of-domain examples and synthetically generated in-domain examples to construct demonstrations. By retrieving demonstrations from hybrid sources, ODIS leverages the advantages of both, showcasing its effectiveness compared to baseline methods that rely on a single data source. Furthermore, ODIS outperforms state-of-the-art approaches on two cross-domain text-to-SQL datasets, with improvements of 1.1 and 11.8 points in execution accuracy, respectively.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在跨领域文本到SQL任务中展示了印象深刻的普遍化能力，不需要使用领域标注。但是，包含领域示例可以大幅提高LLM的表现。在这篇论文中，我们探讨了领域示例中关键因素对提升的贡献，并查探我们是否可以利用这些优点而不需要领域标注。基于我们的发现，我们提出了一个示例选择框架ODIS，这个框架使用了外部示例和人工生成的领域示例来建立示例。通过从混合来源获取示例，ODIS可以利用这两种来源的优点，并且在两个跨领域文本到SQL数据集上显示出比基准方法更高的效果。此外，ODIS比前一代方法在两个数据集上表现更好，具体的提升为1.1和11.8个执行精度分别。
</details></li>
</ul>
<hr>
<h2 id="An-experiment-on-an-automated-literature-survey-of-data-driven-speech-enhancement-methods"><a href="#An-experiment-on-an-automated-literature-survey-of-data-driven-speech-enhancement-methods" class="headerlink" title="An experiment on an automated literature survey of data-driven speech enhancement methods"></a>An experiment on an automated literature survey of data-driven speech enhancement methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06260">http://arxiv.org/abs/2310.06260</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arthur dos Santos, Jayr Pereira, Rodrigo Nogueira, Bruno Masiero, Shiva Sander-Tavallaey, Elias Zea</li>
<li>for:  automatizieren einer Literatur-Überblick über 116 Artikel zu data-getriebenen Sprechverbesserungsverfahren</li>
<li>methods: 使用一个生成的预训练转换器（GPT）模型自动进行文献综述</li>
<li>results: 评估GPT模型在提供准确回答特定问题关于选择的人工参考文献中的能力和局限性<details>
<summary>Abstract</summary>
The increasing number of scientific publications in acoustics, in general, presents difficulties in conducting traditional literature surveys. This work explores the use of a generative pre-trained transformer (GPT) model to automate a literature survey of 116 articles on data-driven speech enhancement methods. The main objective is to evaluate the capabilities and limitations of the model in providing accurate responses to specific queries about the papers selected from a reference human-based survey. While we see great potential to automate literature surveys in acoustics, improvements are needed to address technical questions more clearly and accurately.
</details>
<details>
<summary>摘要</summary>
“随着科学期刊中有限的增加，传统的文献综述became increasingly difficult。本研究探讨使用生成器预训transformer（GPT）模型自动进行116篇资料驱动 speech 增强方法的文献综述。主要目的是评估模型对 especific queries 的答案是否具有准确性。 Although we see great potential in automating literature surveys in acoustics, further improvements are needed to address technical questions more clearly and accurately.”Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="GeoLLM-Extracting-Geospatial-Knowledge-from-Large-Language-Models"><a href="#GeoLLM-Extracting-Geospatial-Knowledge-from-Large-Language-Models" class="headerlink" title="GeoLLM: Extracting Geospatial Knowledge from Large Language Models"></a>GeoLLM: Extracting Geospatial Knowledge from Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06213">http://arxiv.org/abs/2310.06213</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rohin Manvi, Samar Khanna, Gengchen Mai, Marshall Burke, David Lobell, Stefano Ermon</li>
<li>for: 本研究是使用自然语言处理技术（NLP）和机器学习（ML）来解决地ospatial Tasks的应用问题，特别是使用互联网语言资料库（LLMs）来提取地ospatial知识。</li>
<li>methods: 本研究提出了一种新的方法 called GeoLLM，该方法可以有效地提取地ospatial知识从LLMs中，并且可以与OpenStreetMap地图数据结合使用。</li>
<li>results: 根据实验结果，GeoLLM方法可以与基elines相比提高70%的性能（用Pearson的$r^2$进行衡量），并且与现有的卫星数据 benchmark相当或更高。此外，研究还发现LLMs具有remarkable的空间信息和 sample-efficient特点。<details>
<summary>Abstract</summary>
The application of machine learning (ML) in a range of geospatial tasks is increasingly common but often relies on globally available covariates such as satellite imagery that can either be expensive or lack predictive power. Here we explore the question of whether the vast amounts of knowledge found in Internet language corpora, now compressed within large language models (LLMs), can be leveraged for geospatial prediction tasks. We first demonstrate that LLMs embed remarkable spatial information about locations, but naively querying LLMs using geographic coordinates alone is ineffective in predicting key indicators like population density. We then present GeoLLM, a novel method that can effectively extract geospatial knowledge from LLMs with auxiliary map data from OpenStreetMap. We demonstrate the utility of our approach across multiple tasks of central interest to the international community, including the measurement of population density and economic livelihoods. Across these tasks, our method demonstrates a 70% improvement in performance (measured using Pearson's $r^2$) relative to baselines that use nearest neighbors or use information directly from the prompt, and performance equal to or exceeding satellite-based benchmarks in the literature. With GeoLLM, we observe that GPT-3.5 outperforms Llama 2 and RoBERTa by 19% and 51% respectively, suggesting that the performance of our method scales well with the size of the model and its pretraining dataset. Our experiments reveal that LLMs are remarkably sample-efficient, rich in geospatial information, and robust across the globe. Crucially, GeoLLM shows promise in mitigating the limitations of existing geospatial covariates and complementing them well.
</details>
<details>
<summary>摘要</summary>
machine learning（ml）在各种地ospatial任务中越来越普遍，但常常基于全球可用的 covariates，如卫星影像，这些 covariates 可能是昂贵的或者预测力不强。在这里，我们考虑了 Whether the vast amounts of knowledge found in Internet language corpora, now compressed within large language models（LLMs），可以为 geospatial prediction tasks 提供支持。我们首先表明了 LLMs 嵌入了很多地理信息，但是直接使用地理坐标查询 LLMs 是不能有效地预测重要指标，如人口密度。然后，我们提出了 GeoLLM，一种新的方法，可以有效地从 LLMs 提取地ospatial 知识，并且可以与 OpenStreetMap 中的 auxiliary map 数据结合使用。我们在多个国际社区中的重要任务上进行了多个任务，包括人口密度的测量和经济生活水平的评估。在这些任务中，我们的方法比基eline 使用 nearest neighbors 或者直接从提示中获取信息的方法提高了70%（ measured using Pearson's $r^2$）。此外，我们发现 GPT-3.5 在 GeoLLM 中表现比 Llama 2 和 RoBERTa 好19%和51% respectively，这表明我们的方法可以很好地扩展到不同的模型和预训练集。我们的实验表明 LLMs 在各个地方都具有很好的sample efficiency，rich in geospatial information，和 robustness。此外，GeoLLM 可以有效地缓解现有的地ospatial covariates 的限制，并且可以补充它们良好。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/10/cs.CL_2023_10_10/" data-id="clp89dob900d3i7880fwxcy46" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_10_10" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/10/cs.LG_2023_10_10/" class="article-date">
  <time datetime="2023-10-10T10:00:00.000Z" itemprop="datePublished">2023-10-10</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/10/cs.LG_2023_10_10/">cs.LG - 2023-10-10</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Investigating-the-Adversarial-Robustness-of-Density-Estimation-Using-the-Probability-Flow-ODE"><a href="#Investigating-the-Adversarial-Robustness-of-Density-Estimation-Using-the-Probability-Flow-ODE" class="headerlink" title="Investigating the Adversarial Robustness of Density Estimation Using the Probability Flow ODE"></a>Investigating the Adversarial Robustness of Density Estimation Using the Probability Flow ODE</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07084">http://arxiv.org/abs/2310.07084</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marius Arvinte, Cory Cornelius, Jason Martin, Nageen Himayat</li>
<li>for: 这个论文旨在检验抽样模型在面对梯度基于概率最大化攻击时的Robustness，以及与样本复杂度之间的关系。</li>
<li>methods: 这个论文使用了概率流（PF）神经网络ordinary differential equation（ODE）模型进行不偏概率估计，并对这种方法进行了六种梯度基于概率最大化攻击的评估。</li>
<li>results: 实验结果表明，使用PF ODE模型进行概率估计是对高复杂度、高概率攻击的Robustness的。此外，在CIFAR-10 dataset上，一些黑客攻击样本具有semantic meaning,如预期的Robust estimator中的。<details>
<summary>Abstract</summary>
Beyond their impressive sampling capabilities, score-based diffusion models offer a powerful analysis tool in the form of unbiased density estimation of a query sample under the training data distribution. In this work, we investigate the robustness of density estimation using the probability flow (PF) neural ordinary differential equation (ODE) model against gradient-based likelihood maximization attacks and the relation to sample complexity, where the compressed size of a sample is used as a measure of its complexity. We introduce and evaluate six gradient-based log-likelihood maximization attacks, including a novel reverse integration attack. Our experimental evaluations on CIFAR-10 show that density estimation using the PF ODE is robust against high-complexity, high-likelihood attacks, and that in some cases adversarial samples are semantically meaningful, as expected from a robust estimator.
</details>
<details>
<summary>摘要</summary>
除了其吸引人的采样能力之外，分数基于扩散模型还提供了一种强大的分析工具，即对训练数据分布下的查询样本进行不偏的density估计。在这种工作中，我们研究了PF neural differential equation（ODE）模型对梯度基于可能性最大化攻击的Robustness，以及与样本复杂度之间的关系。我们介绍并评估了6种梯度基于Log-likelihood最大化攻击，其中包括一种新的反整合攻击。我们的实验评估表明，使用PF ODE进行density估计对于高复杂性、高可能性攻击是Robust，而且在某些情况下，黑客样本具有semantically meaningful的意义，与一个Robust估计器相符。
</details></li>
</ul>
<hr>
<h2 id="Taking-the-human-out-of-decomposition-based-optimization-via-artificial-intelligence-Part-II-Learning-to-initialize"><a href="#Taking-the-human-out-of-decomposition-based-optimization-via-artificial-intelligence-Part-II-Learning-to-initialize" class="headerlink" title="Taking the human out of decomposition-based optimization via artificial intelligence: Part II. Learning to initialize"></a>Taking the human out of decomposition-based optimization via artificial intelligence: Part II. Learning to initialize</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07082">http://arxiv.org/abs/2310.07082</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ilias Mitrai, Prodromos Daoutidis</li>
<li>for: 解决大规模优化问题，frequently encountered in process systems engineering tasks.</li>
<li>methods: 使用机器学习方法学习优化算法的最佳初始化，以减少计算时间。</li>
<li>results: 提出的方法可以带来显著减少解决时间，并且活动学习可以减少学习数据量。Here’s a breakdown of each point:1. for: The paper is written for solving large-scale optimization problems in process systems engineering tasks.2. methods: The paper proposes using machine learning to learn the optimal initialization of decomposition-based solution methods, which can reduce the computational time.3. results: The proposed method can significantly reduce the solution time, and active learning can reduce the amount of data required for learning.<details>
<summary>Abstract</summary>
The repeated solution of large-scale optimization problems arises frequently in process systems engineering tasks. Decomposition-based solution methods have been widely used to reduce the corresponding computational time, yet their implementation has multiple steps that are difficult to configure. We propose a machine learning approach to learn the optimal initialization of such algorithms which minimizes the computational time. Active and supervised learning is used to learn a surrogate model that predicts the computational performance for a given initialization. We apply this approach to the initialization of Generalized Benders Decomposition for the solution of mixed integer model predictive control problems. The surrogate models are used to find the optimal number of initial cuts that should be added in the master problem. The results show that the proposed approach can lead to a significant reduction in solution time, and active learning can reduce the data required for learning.
</details>
<details>
<summary>摘要</summary>
大规模优化问题的重复解决问题经常出现在进程系统工程中的任务中。基于分解的解决方法广泛使用，但它们的实施具有多个步骤，这些步骤困难配置。我们提议使用机器学习方法来学习优化算法的初始化，以降低计算时间。我们使用活动学习和监督学习来学习一个预测算法的计算性能，这个预测算法用于确定给定初始化的计算时间。我们应用这种方法到 generalized Benders decomposition 的初始化中，用于解决混合整数预测控制问题。 surrogate 模型用于找到最佳的初始剖分数，以降低解决时间。结果表明，我们的方法可以带来显著的解决时间减少，并且活动学习可以减少学习数据量。
</details></li>
</ul>
<hr>
<h2 id="Secure-Decentralized-Learning-with-Blockchain"><a href="#Secure-Decentralized-Learning-with-Blockchain" class="headerlink" title="Secure Decentralized Learning with Blockchain"></a>Secure Decentralized Learning with Blockchain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07079">http://arxiv.org/abs/2310.07079</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Mdshobu/Liberty-House-Club-Whitepaper">https://github.com/Mdshobu/Liberty-House-Club-Whitepaper</a></li>
<li>paper_authors: Xiaoxue Zhang, Yifan Hua, Chen Qian</li>
<li>for: 防止单点失败问题，提高机器学习任务的数据隐私和通信效率。</li>
<li>methods: 使用块链技术进行分布式机器学习，实现模型验证和审核。</li>
<li>results: 在30% 恶意客户端情况下，通过信誉机制实现快速模型融合和高精度。<details>
<summary>Abstract</summary>
Federated Learning (FL) is a well-known paradigm of distributed machine learning on mobile and IoT devices, which preserves data privacy and optimizes communication efficiency. To avoid the single point of failure problem in FL, decentralized federated learning (DFL) has been proposed to use peer-to-peer communication for model aggregation, which has been considered an attractive solution for machine learning tasks on distributed personal devices. However, this process is vulnerable to attackers who share false models and data. If there exists a group of malicious clients, they might harm the performance of the model by carrying out a poisoning attack. In addition, in DFL, clients often lack the incentives to contribute their computing powers to do model training. In this paper, we proposed Blockchain-based Decentralized Federated Learning (BDFL), which leverages a blockchain for decentralized model verification and auditing. BDFL includes an auditor committee for model verification, an incentive mechanism to encourage the participation of clients, a reputation model to evaluate the trustworthiness of clients, and a protocol suite for dynamic network updates. Evaluation results show that, with the reputation mechanism, BDFL achieves fast model convergence and high accuracy on real datasets even if there exist 30\% malicious clients in the system.
</details>
<details>
<summary>摘要</summary>
federated learning（FL）是一种已知的分布式机器学习模式，适用于移动设备和物联网设备，保持数据隐私和通信效率。为了解决FL中的单点失败问题，分布式 federated learning（DFL）已经提议使用对等通信进行模型集成，这被视为对于分布在个人设备上的机器学习任务的有appealing解决方案。然而，这个过程受到攻击者们发送false模型和数据的威胁。如果存在一群恶意客户端，他们可能会通过毒品攻击伤害模型的性能。此外，在DFL中，客户端经常缺乏参与到模型训练中的动机。在这篇论文中，我们提出了基于区块链的分布式 federated learning（BDFL），该技术利用区块链进行分布式模型验证和审核。BDFL包括一个审计委员会 для模型验证、一种激励客户端参与的机制、一个客户端信任度评估模型以及一套协议集 для动态网络更新。评估结果表明，在各种情况下，包括30%的恶意客户端，BDFL仍能够快速启 converges和高精度地完成实际数据集的模型训练。
</details></li>
</ul>
<hr>
<h2 id="Taking-the-human-out-of-decomposition-based-optimization-via-artificial-intelligence-Part-I-Learning-when-to-decompose"><a href="#Taking-the-human-out-of-decomposition-based-optimization-via-artificial-intelligence-Part-I-Learning-when-to-decompose" class="headerlink" title="Taking the human out of decomposition-based optimization via artificial intelligence: Part I. Learning when to decompose"></a>Taking the human out of decomposition-based optimization via artificial intelligence: Part I. Learning when to decompose</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07068">http://arxiv.org/abs/2310.07068</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ilias Mitrai, Prodromos Daoutidis</li>
<li>for: 本文提出了一种图 классификация方法，用于自动决定使用固定或分解方法解决优化问题。</li>
<li>methods: 该方法使用图表示优化问题中变量和约束之间的结构和功能联系，然后建立图分类器来决定问题的最佳解决方法。</li>
<li>results: 该方法可以开发一个可以判断凸混合整数非线性 програм的最佳解决方法是使用分支和约束算法还是外接算法。此外，可以将学习的分类器 integrate到现有混合整数优化解决方案中。<details>
<summary>Abstract</summary>
In this paper, we propose a graph classification approach for automatically determining whether to use a monolithic or a decomposition-based solution method. In this approach, an optimization problem is represented as a graph that captures the structural and functional coupling among the variables and constraints of the problem via an appropriate set of features. Given this representation, a graph classifier is built to determine the best solution method for a given problem. The proposed approach is used to develop a classifier that determines whether a convex Mixed Integer Nonlinear Programming problem should be solved using branch and bound or the outer approximation algorithm. Finally, it is shown how the learned classifier can be incorporated into existing mixed integer optimization solvers.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种图 классификация方法，用于自动确定是否使用简单或含 decomposition 的解决方法。在这种方法中，一个优化问题被表示为一个图， capture 变量和约束之间的结构和功能相互关系via 合适的特征集。给出这种表示，一个图分类器被建立，以确定给定问题的最佳解决方法。我们所提出的方法用于开发一个可以判断 convex 混合整数非线性程序问题是否使用分支和约束算法或外接算法解决。最后，我们示出了如何将学习的分类器集成到现有的混合整数优化解决方案中。
</details></li>
</ul>
<hr>
<h2 id="Acoustic-Model-Fusion-for-End-to-end-Speech-Recognition"><a href="#Acoustic-Model-Fusion-for-End-to-end-Speech-Recognition" class="headerlink" title="Acoustic Model Fusion for End-to-end Speech Recognition"></a>Acoustic Model Fusion for End-to-end Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07062">http://arxiv.org/abs/2310.07062</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhihong Lei, Mingbin Xu, Shiyi Han, Leo Liu, Zhen Huang, Tim Ng, Yuanyuan Zhang, Ernest Pusateri, Mirko Hannemann, Yaqiao Deng, Man-Hung Siu</li>
<li>for: 提高 ASR 系统的准确率和 named entity recognition 性能</li>
<li>methods: 提出一种将 external acoustic model integrated into end-to-end ASR 系统的方法，以更好地解决频率域匹配问题</li>
<li>results: 实现了在不同测试集上的词错率下降，最高达14.3%，同时Named entity recognition的性能也得到了明显提高<details>
<summary>Abstract</summary>
Recent advances in deep learning and automatic speech recognition (ASR) have enabled the end-to-end (E2E) ASR system and boosted the accuracy to a new level. The E2E systems implicitly model all conventional ASR components, such as the acoustic model (AM) and the language model (LM), in a single network trained on audio-text pairs. Despite this simpler system architecture, fusing a separate LM, trained exclusively on text corpora, into the E2E system has proven to be beneficial. However, the application of LM fusion presents certain drawbacks, such as its inability to address the domain mismatch issue inherent to the internal AM. Drawing inspiration from the concept of LM fusion, we propose the integration of an external AM into the E2E system to better address the domain mismatch. By implementing this novel approach, we have achieved a significant reduction in the word error rate, with an impressive drop of up to 14.3% across varied test sets. We also discovered that this AM fusion approach is particularly beneficial in enhancing named entity recognition.
</details>
<details>
<summary>摘要</summary>
Traditional ASR systems consist of separate AM and language model (LM) components, but E2E systems combine these components into a single network trained on audio-text pairs. Despite this simpler architecture, fusing a separate LM trained exclusively on text corpora into the E2E system has been shown to be beneficial. However, this approach is limited by its inability to address the domain mismatch issue inherent to the internal AM. Our proposed approach of integrating an external AM into the E2E system addresses this issue by providing a more diverse set of acoustic features to the network. This allows the network to better handle variations in speech and improve overall accuracy. We have tested our approach on a variety of datasets and have achieved significant improvements in word error rates, with an impressive drop of up to 14.3% across all test sets. Additionally, we have found that this approach is particularly effective in enhancing named entity recognition.
</details></li>
</ul>
<hr>
<h2 id="Spiral-Elliptical-automated-galaxy-morphology-classification-from-telescope-images"><a href="#Spiral-Elliptical-automated-galaxy-morphology-classification-from-telescope-images" class="headerlink" title="Spiral-Elliptical automated galaxy morphology classification from telescope images"></a>Spiral-Elliptical automated galaxy morphology classification from telescope images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07740">http://arxiv.org/abs/2310.07740</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matthew J. Baumstark, Giuseppe Vinci</li>
<li>for: 本研究旨在提出 нов的星系形态统计方法，以提高现有的星系形态分类方法的效率和准确性。</li>
<li>methods: 本研究使用了两种新的星系形态统计方法：descent average和descent variance，以及简化了现有的图像统计指标（concentration、asymmetry和clumpiness）。</li>
<li>results: 使用 Sloan Digital Sky Survey 的星系图像数据，我们证明了我们提出的图像统计方法可以高效地检测扁旋和螺旋星系，并且可以作为Random Forest 分类器的特征来使用。<details>
<summary>Abstract</summary>
The classification of galaxy morphologies is an important step in the investigation of theories of hierarchical structure formation. While human expert visual classification remains quite effective and accurate, it cannot keep up with the massive influx of data from emerging sky surveys. A variety of approaches have been proposed to classify large numbers of galaxies; these approaches include crowdsourced visual classification, and automated and computational methods, such as machine learning methods based on designed morphology statistics and deep learning. In this work, we develop two novel galaxy morphology statistics, descent average and descent variance, which can be efficiently extracted from telescope galaxy images. We further propose simplified versions of the existing image statistics concentration, asymmetry, and clumpiness, which have been widely used in the literature of galaxy morphologies. We utilize the galaxy image data from the Sloan Digital Sky Survey to demonstrate the effective performance of our proposed image statistics at accurately detecting spiral and elliptical galaxies when used as features of a random forest classifier.
</details>
<details>
<summary>摘要</summary>
《星系形态分类是astrophysical structure formation理论研究中一个重要步骤。虽然人类专家视觉分类仍然非常有效和准确，但由于天文大观测数据的涌入，人类分类无法满足数据的需求。各种方法被提出来分类大量的星系，包括人工智能分类和计算机方法，如基于设计的形态统计和深度学习。在这项工作中，我们开发了两种新的星系形态统计，即下降平均值和下降方差，可以快速从望远镜星系图像中提取。我们还提出了现有图像统计的简化版本，包括吸引度、非均匀性和块性，这些统计在Literature中广泛使用。我们使用 Sloan Digital Sky Survey 的星系图像数据来证明我们提出的图像统计能够准确地检测扁旋和椭圆星系，当作Random Forest 分类器的特征。》Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="FedMFS-Federated-Multimodal-Fusion-Learning-with-Selective-Modality-Communication"><a href="#FedMFS-Federated-Multimodal-Fusion-Learning-with-Selective-Modality-Communication" class="headerlink" title="FedMFS: Federated Multimodal Fusion Learning with Selective Modality Communication"></a>FedMFS: Federated Multimodal Fusion Learning with Selective Modality Communication</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07048">http://arxiv.org/abs/2310.07048</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liangqi Yuan, Dong-Jun Han, Vishnu Pandi Chellapandi, Stanislaw H. Żak, Christopher G. Brinton</li>
<li>for: 本研究旨在提出一种新的分布式机器学习（Federated Learning，FL）方法，用于在互联网物联网（IoT）环境中进行多Modal数据融合学习。</li>
<li>methods: 本研究提出了一种名为Federated Multimodal Fusion learning with Selective modality communication（FedMFS）的新方法，该方法利用Shapley值来衡量每个模式的贡献，并根据模式模型大小来衡量通信开销，以便每个客户端可以选择上传模式模型到服务器进行集成。</li>
<li>results: 实验结果表明，FedMFS方法可以减少一很大的通信开销，同时保持与基准值相对的准确性。实际上，FedMFS方法可以在真实的多Modal数据集上实现相对于基准值的20%的通信开销减少。<details>
<summary>Abstract</summary>
Federated learning (FL) is a distributed machine learning (ML) paradigm that enables clients to collaborate without accessing, infringing upon, or leaking original user data by sharing only model parameters. In the Internet of Things (IoT), edge devices are increasingly leveraging multimodal data compositions and fusion paradigms to enhance model performance. However, in FL applications, two main challenges remain open: (i) addressing the issues caused by heterogeneous clients lacking specific modalities and (ii) devising an optimal modality upload strategy to minimize communication overhead while maximizing learning performance. In this paper, we propose Federated Multimodal Fusion learning with Selective modality communication (FedMFS), a new multimodal fusion FL methodology that can tackle the above mentioned challenges. The key idea is to utilize Shapley values to quantify each modality's contribution and modality model size to gauge communication overhead, so that each client can selectively upload the modality models to the server for aggregation. This enables FedMFS to flexibly balance performance against communication costs, depending on resource constraints and applications. Experiments on real-world multimodal datasets demonstrate the effectiveness of FedMFS, achieving comparable accuracy while reducing communication overhead by one twentieth compared to baselines.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 是一种分布式机器学习 (ML)  paradigma， enables clients to collaborate without accessing, infringing upon, or leaking original user data by sharing only model parameters. In the Internet of Things (IoT), edge devices are increasingly leveraging multimodal data compositions and fusion paradigms to enhance model performance. However, in FL applications, two main challenges remain open: (i) addressing the issues caused by heterogeneous clients lacking specific modalities and (ii) devising an optimal modality upload strategy to minimize communication overhead while maximizing learning performance. In this paper, we propose Federated Multimodal Fusion learning with Selective modality communication (FedMFS), a new multimodal fusion FL methodology that can tackle the above mentioned challenges. The key idea is to utilize Shapley values to quantify each modality's contribution and modality model size to gauge communication overhead, so that each client can selectively upload the modality models to the server for aggregation. This enables FedMFS to flexibly balance performance against communication costs, depending on resource constraints and applications. Experiments on real-world multimodal datasets demonstrate the effectiveness of FedMFS, achieving comparable accuracy while reducing communication overhead by one twentieth compared to baselines.
</details></li>
</ul>
<hr>
<h2 id="A-predict-and-optimize-approach-to-profit-driven-churn-prevention"><a href="#A-predict-and-optimize-approach-to-profit-driven-churn-prevention" class="headerlink" title="A predict-and-optimize approach to profit-driven churn prevention"></a>A predict-and-optimize approach to profit-driven churn prevention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07047">http://arxiv.org/abs/2310.07047</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nuria Gómez-Vargas, Sebastián Maldonado, Carla Vairetti</li>
<li>for: 预防营业潜在客户流失，以提高公司的营业额。</li>
<li>methods: 提出了一种基于预测和优化的客户挑选策略，将客户生命周期价值（CLV）作为主要评估标准，以避免营业损失。</li>
<li>results: 在12个客户流失预测数据集上，该策略达到了最佳平均收益水平，比其他常见策略更高。<details>
<summary>Abstract</summary>
In this paper, we introduce a novel predict-and-optimize method for profit-driven churn prevention. We frame the task of targeting customers for a retention campaign as a regret minimization problem. The main objective is to leverage individual customer lifetime values (CLVs) to ensure that only the most valuable customers are targeted. In contrast, many profit-driven strategies focus on churn probabilities while considering average CLVs. This often results in significant information loss due to data aggregation. Our proposed model aligns with the guidelines of Predict-and-Optimize (PnO) frameworks and can be efficiently solved using stochastic gradient descent methods. Results from 12 churn prediction datasets underscore the effectiveness of our approach, which achieves the best average performance compared to other well-established strategies in terms of average profit.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了一种新的预测和优化方法，用于防止利润驱动的客户流失。我们将客户退货活动的目标客户群作为 regret 最小化问题来定义。我们的主要目标是通过个体客户生命周期价值（CLV）来确保只有最有价值的客户被targeting。与此相比，许多利润驱动策略往往强调退货概率，而不考虑CLV的含义。这经常导致数据汇总所产生的信息损失。我们提出的模型遵循Predict-and-Optimize（PnO）框架的指南，可以使用Stochastic Gradient Descent（SGD）方法高效地解决。Results from 12 churn prediction datasets confirm the effectiveness of our approach, which achieves the best average performance compared to other well-established strategies in terms of average profit.Here's a word-for-word translation of the text into Simplified Chinese:在这篇论文中，我们介绍了一种新的预测和优化方法，用于防止利润驱动的客户流失。我们将客户退货活动的目标客户群作为 regret 最小化问题来定义。我们的主要目标是通过个体客户生命周期价值（CLV）来确保只有最有价值的客户被targeting。与此相比，许多利润驱动策略往往强调退货概率，而不考虑CLV的含义。这经常导致数据汇总所产生的信息损失。我们提出的模型遵循Predict-and-Optimize（PnO）框架的指南，可以使用Stochastic Gradient Descent（SGD）方法高效地解决。Results from 12 churn prediction datasets confirm the effectiveness of our approach, which achieves the best average performance compared to other well-established strategies in terms of average profit.
</details></li>
</ul>
<hr>
<h2 id="Neural-Harmonium-An-Interpretable-Deep-Structure-for-Nonlinear-Dynamic-System-Identification-with-Application-to-Audio-Processing"><a href="#Neural-Harmonium-An-Interpretable-Deep-Structure-for-Nonlinear-Dynamic-System-Identification-with-Application-to-Audio-Processing" class="headerlink" title="Neural Harmonium: An Interpretable Deep Structure for Nonlinear Dynamic System Identification with Application to Audio Processing"></a>Neural Harmonium: An Interpretable Deep Structure for Nonlinear Dynamic System Identification with Application to Audio Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07032">http://arxiv.org/abs/2310.07032</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karim Helwani, Erfan Soltanmohammadi, Michael M. Goodwin</li>
<li>for: 本研究目的是提高深度神经网络的解释性，尤其在应用于物理问题上。解释性可以帮助我们理解模型的泛化能力和其局限性。</li>
<li>methods: 本文提出了一种可解释性深度结构，用于模型动态系统。该模型使用干扰分析，在时域-频域中模型系统，同时保持高时间和频率分辨率。此外，模型采用递归结构，可以快速、稳定、且准确地进行第二阶导数计算，不需要显式计算权差矩阵。为了缓解建立模型块的高维度问题，使用神经网络来描述频率相互关系。</li>
<li>results: 在非线性系统识别问题上，提出的方法得到了证明。在音频干扰抑制问题中，通过对比与其他现有解决方案的实验，表明了我们的方法在实际应用中的效果。<details>
<summary>Abstract</summary>
Improving the interpretability of deep neural networks has recently gained increased attention, especially when the power of deep learning is leveraged to solve problems in physics. Interpretability helps us understand a model's ability to generalize and reveal its limitations. In this paper, we introduce a causal interpretable deep structure for modeling dynamic systems. Our proposed model makes use of the harmonic analysis by modeling the system in a time-frequency domain while maintaining high temporal and spectral resolution. Moreover, the model is built in an order recursive manner which allows for fast, robust, and exact second order optimization without the need for an explicit Hessian calculation. To circumvent the resulting high dimensionality of the building blocks of our system, a neural network is designed to identify the frequency interdependencies. The proposed model is illustrated and validated on nonlinear system identification problems as required for audio signal processing tasks. Crowd-sourced experimentation contrasting the performance of the proposed approach to other state-of-the-art solutions on an acoustic echo cancellation scenario confirms the effectiveness of our method for real-life applications.
</details>
<details>
<summary>摘要</summary>
深度学习在物理问题中的应用已经受到了提高解释性的关注，特别是当深度学习的力量被应用于解决物理问题时。解释性能我们理解模型的泛化能力和其局限性。在这篇论文中，我们介绍了一种可 causal 解释深度结构，用于模型动态系统。我们的提议的模型利用干扰分析，将系统模型在时间频域中进行了时间频谱分析，同时保持高度的时间和频率分辨率。此外，模型采用递归的构建方式，可以快速、稳定、准确地进行第二阶导数计算，不需要显式表达Hessian。为了避免建模块的高维度，我们设计了一个神经网络来识别频率相互关系。我们的提议模型在非线性系统识别问题中得到了验证，特别是在音频信号处理任务中。通过人工 эксперимент，我们比较了我们的方法与其他现有解决方案在音频适应噪抑问题中的性能，并证明了我们的方法在实际应用中的有效性。
</details></li>
</ul>
<hr>
<h2 id="Neural-Relational-Inference-with-Fast-Modular-Meta-learning"><a href="#Neural-Relational-Inference-with-Fast-Modular-Meta-learning" class="headerlink" title="Neural Relational Inference with Fast Modular Meta-learning"></a>Neural Relational Inference with Fast Modular Meta-learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07015">http://arxiv.org/abs/2310.07015</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/FerranAlet/modular-metalearning">https://github.com/FerranAlet/modular-metalearning</a></li>
<li>paper_authors: Ferran Alet, Erica Weng, Tomás Lozano Pérez, Leslie Pack Kaelbling</li>
<li>for: 这 paper 用于解决多种互动的关系推理问题，以便从观察数据中学习系统的动态。</li>
<li>methods: 这 paper 使用模块化元学习法，通过不同组合方式训练神经模块，以解决多种任务。</li>
<li>results: 这 paper 使用模块化元学习法提高了推理能力，可以更有效地利用观察数据，并且可以估计未直接观察到的实体状态。<details>
<summary>Abstract</summary>
\textit{Graph neural networks} (GNNs) are effective models for many dynamical systems consisting of entities and relations. Although most GNN applications assume a single type of entity and relation, many situations involve multiple types of interactions. \textit{Relational inference} is the problem of inferring these interactions and learning the dynamics from observational data. We frame relational inference as a \textit{modular meta-learning} problem, where neural modules are trained to be composed in different ways to solve many tasks. This meta-learning framework allows us to implicitly encode time invariance and infer relations in context of one another rather than independently, which increases inference capacity. Framing inference as the inner-loop optimization of meta-learning leads to a model-based approach that is more data-efficient and capable of estimating the state of entities that we do not observe directly, but whose existence can be inferred from their effect on observed entities. To address the large search space of graph neural network compositions, we meta-learn a \textit{proposal function} that speeds up the inner-loop simulated annealing search within the modular meta-learning algorithm, providing two orders of magnitude increase in the size of problems that can be addressed.
</details>
<details>
<summary>摘要</summary>
\begin{itemize}\item 图 neural networks（GNNs）是适用于许多动态系统中的有效模型，该系统包括实体和关系。 although most GNN applications assume a single type of entity and relation, many situations involve multiple types of interactions.\item 关系推理（relational inference）是从观察数据中推理这些交互的问题，学习这些交互的动态。 we frame relational inference as a modular meta-learning problem, where neural modules are trained to be composed in different ways to solve many tasks.\item 这个meta-learning框架允许我们通过不同的模块组合来解决多种任务，从而隐式地编码了时间不变性，并在彼此之间学习关系，这使得推理能力更高。\item 将推理视为meta-learning的内部循环优化问题，导致一种基于模型的方法，更有效率地使用数据，并能够估计不 direktly observable的实体状态，而是通过其影响已知实体来推理其存在。\item 为了解决图 neural network的模块组合搜索的大搜索空间，我们meta-learn a proposal function，这将在模块meta-learning算法中加速内部逻辑搜索，提供了两个数量级的提高，使得可以处理的问题规模提高了两个数量级。\end{itemize}Note: Please note that the translation is in Simplified Chinese, and some words or phrases may have different translations in Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Sound-skwatter-Did-You-Mean-Sound-squatter-AI-powered-Generator-for-Phishing-Prevention"><a href="#Sound-skwatter-Did-You-Mean-Sound-squatter-AI-powered-Generator-for-Phishing-Prevention" class="headerlink" title="Sound-skwatter (Did You Mean: Sound-squatter?) AI-powered Generator for Phishing Prevention"></a>Sound-skwatter (Did You Mean: Sound-squatter?) AI-powered Generator for Phishing Prevention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07005">http://arxiv.org/abs/2310.07005</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rodolfo Valentim, Idilio Drago, Marco Mellia, Federico Cerutti</li>
<li>for: 防御声钩攻击（Sound-squatting），使用人工智能生成声钩候选者。</li>
<li>methods: 使用 transformers 网络和声学模型组合，学习声音相似性。</li>
<li>results: 可以自动找到已知同音词和数千个高质量候选者，同时支持交互语言的声钩攻击。<details>
<summary>Abstract</summary>
Sound-squatting is a phishing attack that tricks users into malicious resources by exploiting similarities in the pronunciation of words. Proactive defense against sound-squatting candidates is complex, and existing solutions rely on manually curated lists of homophones. We here introduce Sound-skwatter, a multi-language AI-based system that generates sound-squatting candidates for proactive defense. Sound-skwatter relies on an innovative multi-modal combination of Transformers Networks and acoustic models to learn sound similarities. We show that Sound-skwatter can automatically list known homophones and thousands of high-quality candidates. In addition, it covers cross-language sound-squatting, i.e., when the reader and the listener speak different languages, supporting any combination of languages. We apply Sound-skwatter to network-centric phishing via squatted domain names. We find ~ 10% of the generated domains exist in the wild, the vast majority unknown to protection solutions. Next, we show attacks on the PyPI package manager, where ~ 17% of the popular packages have at least one existing candidate. We believe Sound-skwatter is a crucial asset to mitigate the sound-squatting phenomenon proactively on the Internet. To increase its impact, we publish an online demo and release our models and code as open source.
</details>
<details>
<summary>摘要</summary>
声音骗鱼是一种钓鱼攻击，通过利用声音相似性来骗用户访问恶意资源。现有的防御方法复杂，并且 existing solutions 依赖于手动维护的同音词列表。我们在这里介绍 Sound-skwatter，一个多语言基于 AI 系统，用于生成声音骗鱼候选者。Sound-skwatter 利用了一种创新的多模式 комбиinación，包括 transformers 网络和声音模型，以学习声音相似性。我们表明，Sound-skwatter 可以自动列出已知同音词和数千个高质量候选者。此外，它还支持跨语言声音骗鱼，即当读者和听众说不同语言时。我们应用 Sound-skwatter 于网络中心式骗鱼 via 骗取的域名。我们发现 ~ 10% 的生成域名在野，大多数都是未知的保护解决方案。接着，我们表明 ~ 17% 的流行包在 PyPI 包管理器中有至少一个现有的候选者。我们认为 Sound-skwatter 是在互联网上防止声音骗鱼的关键资产，以提高其影响力，我们在线发布了 demo 和发布我们的模型和代码为开源。
</details></li>
</ul>
<hr>
<h2 id="CarDS-Plus-ECG-Platform-Development-and-Feasibility-Evaluation-of-a-Multiplatform-Artificial-Intelligence-Toolkit-for-Portable-and-Wearable-Device-Electrocardiograms"><a href="#CarDS-Plus-ECG-Platform-Development-and-Feasibility-Evaluation-of-a-Multiplatform-Artificial-Intelligence-Toolkit-for-Portable-and-Wearable-Device-Electrocardiograms" class="headerlink" title="CarDS-Plus ECG Platform: Development and Feasibility Evaluation of a Multiplatform Artificial Intelligence Toolkit for Portable and Wearable Device Electrocardiograms"></a>CarDS-Plus ECG Platform: Development and Feasibility Evaluation of a Multiplatform Artificial Intelligence Toolkit for Portable and Wearable Device Electrocardiograms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07000">http://arxiv.org/abs/2310.07000</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sumukh Vasisht Shankar, Evangelos K Oikonomou, Rohan Khera<br>for: 这个研究旨在开发一个多平台系统，以快速部署基于人工智能的单导电喷（ECG）解决方案，用于临床调查和诊断。methods: 这个研究使用了多种设计考虑因素，包括具体应用场景、数据流程优化和实时推断等方面，以实现将多种来源的单导电喷数据传输到中央数据湖，并通过人工智能模型进行ECG解译。results: 研究表明，这个平台可以快速地从获取到报告结果，平均需时为33.0-35.7秒，无论使用哪种商业化的设备（Apple Watch和KardiaMobile）。这些结果表明了将设计原则翻译到快速部署的策略是可行的，并且可以在临床医疗中实现影响。<details>
<summary>Abstract</summary>
In the rapidly evolving landscape of modern healthcare, the integration of wearable & portable technology provides a unique opportunity for personalized health monitoring in the community. Devices like the Apple Watch, FitBit, and AliveCor KardiaMobile have revolutionized the acquisition and processing of intricate health data streams. Amidst the variety of data collected by these gadgets, single-lead electrocardiogram (ECG) recordings have emerged as a crucial source of information for monitoring cardiovascular health. There has been significant advances in artificial intelligence capable of interpreting these 1-lead ECGs, facilitating clinical diagnosis as well as the detection of rare cardiac disorders. This design study describes the development of an innovative multiplatform system aimed at the rapid deployment of AI-based ECG solutions for clinical investigation & care delivery. The study examines design considerations, aligning them with specific applications, develops data flows to maximize efficiency for research & clinical use. This process encompasses the reception of single-lead ECGs from diverse wearable devices, channeling this data into a centralized data lake & facilitating real-time inference through AI models for ECG interpretation. An evaluation of the platform demonstrates a mean duration from acquisition to reporting of results of 33.0 to 35.7 seconds, after a standard 30 second acquisition. There were no substantial differences in acquisition to reporting across two commercially available devices (Apple Watch and KardiaMobile). These results demonstrate the succcessful translation of design principles into a fully integrated & efficient strategy for leveraging 1-lead ECGs across platforms & interpretation by AI-ECG algorithms. Such a platform is critical to translating AI discoveries for wearable and portable ECG devices to clinical impact through rapid deployment.
</details>
<details>
<summary>摘要</summary>
在现代医疗面前的急速发展 landscape中，穿戴式和可携式技术的集成提供了个人化健康监测在社区的唯一机会。例如Apple Watch、FitBit和AliveCor KardiaMobile等设备已经革命化了健康数据流的收集和处理。在这些设备收集的数据中，单Channel electrocardiogram（ECG）记录已经成为监测心血管健康的关键来源。人工智能（AI）技术的进步使得可以解释这些1-Channel ECG，从而促进诊断和检测罕见心血管疾病。这个设计研究描述了一种创新的多平台系统，旨在快速部署AI-基于ECG解决方案 для临床调查和诊疗。研究考虑了设计因素，与特定应用相对应，并开发了数据流程，以最大化研究和临床使用的效率。这个过程包括从多种穿戴式设备接收单Channel ECG，将数据传输到中央数据湖，并通过AI模型对ECG进行实时解释。研究表明，平台的实现可以在收集到报告结果的时间内减少了33.0到35.7秒，并且没有显著差异在不同的商业设备（Apple Watch和KardiaMobile）上。这些结果证明了设计原则的成功翻译为一个高效集成的策略，可以在多个平台上使用AI-ECG算法进行单Channel ECG的解释。这种平台是评估AI发现的穿戴式和可携式ECG设备的临床影响的关键。
</details></li>
</ul>
<hr>
<h2 id="Federated-Quantum-Machine-Learning-with-Differential-Privacy"><a href="#Federated-Quantum-Machine-Learning-with-Differential-Privacy" class="headerlink" title="Federated Quantum Machine Learning with Differential Privacy"></a>Federated Quantum Machine Learning with Differential Privacy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06973">http://arxiv.org/abs/2310.06973</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rod Rofougaran, Shinjae Yoo, Huan-Hsin Tseng, Samuel Yen-Chi Chen</li>
<li>for: 保护敏感训练数据的隐私，提供更安全和效率的人工智能实现。</li>
<li>methods: 结合量子联合学习（QFL）和量子差异隐私（QDP）两种隐私保护方法，实现量子平台上的数据隐私保护。</li>
<li>results: 使用量子-классиical机器学习模型对猫vs狗数据集进行二分类，实现了测试准确率超过0.98，同时保持ε值小于1.3。验证了 federated differentially private training 是一种可行的隐私保护方法 для量子机器学习 на Noisy Intermediate-Scale Quantum（NISQ）设备。<details>
<summary>Abstract</summary>
The preservation of privacy is a critical concern in the implementation of artificial intelligence on sensitive training data. There are several techniques to preserve data privacy but quantum computations are inherently more secure due to the no-cloning theorem, resulting in a most desirable computational platform on top of the potential quantum advantages. There have been prior works in protecting data privacy by Quantum Federated Learning (QFL) and Quantum Differential Privacy (QDP) studied independently. However, to the best of our knowledge, no prior work has addressed both QFL and QDP together yet. Here, we propose to combine these privacy-preserving methods and implement them on the quantum platform, so that we can achieve comprehensive protection against data leakage (QFL) and model inversion attacks (QDP). This implementation promises more efficient and secure artificial intelligence. In this paper, we present a successful implementation of these privacy-preservation methods by performing the binary classification of the Cats vs Dogs dataset. Using our quantum-classical machine learning model, we obtained a test accuracy of over 0.98, while maintaining epsilon values less than 1.3. We show that federated differentially private training is a viable privacy preservation method for quantum machine learning on Noisy Intermediate-Scale Quantum (NISQ) devices.
</details>
<details>
<summary>摘要</summary>
保护隐私是人工智能在敏感训练数据实施中的关键问题。有几种技术来保护数据隐私，但量子计算机是因为无论护法 theorem，因此在计算平台上具有最好的安全性。先前有关保护数据隐私的研究，包括量子联合学习（QFL）和量子差分隐私（QDP），但是到目前为止没有任何研究既 combinates these two privacy-preserving methods。在这篇文章中，我们提议将这两种隐私保护方法结合在一起，并在量子平台上实现，以实现全面的数据泄露防止（QFL）和模型反向攻击防止（QDP）。这种实现承诺更高效和安全的人工智能。在这篇文章中，我们成功地实现了这些隐私保护方法，通过对猫vs狗数据集进行二分类。使用我们的量子-классиical机器学习模型，我们在测试精度达0.98，而且psilon值低于1.3。我们显示，联邦差分隐私训练是量子机器学习在Noisy Intermediate-Scale Quantum（NISQ）设备上可行的隐私保护方法。
</details></li>
</ul>
<hr>
<h2 id="Flood-and-Echo-Algorithmic-Alignment-of-GNNs-with-Distributed-Computing"><a href="#Flood-and-Echo-Algorithmic-Alignment-of-GNNs-with-Distributed-Computing" class="headerlink" title="Flood and Echo: Algorithmic Alignment of GNNs with Distributed Computing"></a>Flood and Echo: Algorithmic Alignment of GNNs with Distributed Computing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06970">http://arxiv.org/abs/2310.06970</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joël Mathys, Florian Grötschla, Kalyan Varma Nadimpalli, Roger Wattenhofer</li>
<li>for: 本研究旨在开发一种基于分布式算法设计原理的扩展推理框架，以便在大图上进行信息交换和推理扩展。</li>
<li>methods: 该框架基于洪水和回声网络的设计原理，通过在整个图上传递消息，以达到自适应的扩展和信息交换。</li>
<li>results: 研究表明，该框架在许多情况下比传统的推理框架更有效率，并且能够有效地进行信息交换和推理扩展。<details>
<summary>Abstract</summary>
Graph Neural Networks are a natural fit for learning algorithms. They can directly represent tasks through an abstract but versatile graph structure and handle inputs of different sizes. This opens up the possibility for scaling and extrapolation to larger graphs, one of the most important advantages of an algorithm. However, this raises two core questions i) How can we enable nodes to gather the required information in a given graph ($\textit{information exchange}$), even if is far away and ii) How can we design an execution framework which enables this information exchange for extrapolation to larger graph sizes ($\textit{algorithmic alignment for extrapolation}$). We propose a new execution framework that is inspired by the design principles of distributed algorithms: Flood and Echo Net. It propagates messages through the entire graph in a wave like activation pattern, which naturally generalizes to larger instances. Through its sparse but parallel activations it is provably more efficient in terms of message complexity. We study the proposed model and provide both empirical evidence and theoretical insights in terms of its expressiveness, efficiency, information exchange and ability to extrapolate.
</details>
<details>
<summary>摘要</summary>
GRAPH Neural Networks 是一种自然的适应算法。它们可以直接通过抽象但强大的图结构表示任务，并处理不同大小的输入。这打开了扩大和推断到更大图的可能性，是算法中最重要的优势。然而，这引出了两个核心问题：（i）如何使节点获得图中需要的信息（信息交换），即使它们在远方的 ;（ii）如何设计一个执行框架，使得这些信息交换在更大的图像上进行推断（算法对适应推断）。我们提出了一种新的执行框架，它是基于分布式算法的设计原则：洪涝网络和回声网络。它在整个图上传递消息，使得它自然泛化到更大的实例。通过它的稀疏但平行的活动，可以证明它比消息复杂度更高效。我们研究了提议的模型，并提供了both empirical evidence和理论听见，包括表达能力、效率、信息交换和推断能力。
</details></li>
</ul>
<hr>
<h2 id="Positivity-free-Policy-Learning-with-Observational-Data"><a href="#Positivity-free-Policy-Learning-with-Observational-Data" class="headerlink" title="Positivity-free Policy Learning with Observational Data"></a>Positivity-free Policy Learning with Observational Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06969">http://arxiv.org/abs/2310.06969</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/panzhaooo/positivity-free-policy-learning">https://github.com/panzhaooo/positivity-free-policy-learning</a></li>
<li>paper_authors: Pan Zhao, Antoine Chambaz, Julie Josse, Shu Yang</li>
<li>for: 本研究的目的是用观察数据学习政策，以便在具有特定约束的情况下获得最佳减法分配策略。</li>
<li>methods: 本研究提出了一种新的不假设 positivity 的政策学习框架，以解决实际场景中 positivity 假设的困难。该框架利用增量概率分配策略来调整减法分配策略的概率值，而不是直接将减法分配策略分配给减法。我们描述了这种增量概率分配策略，并提出了鉴定条件，使用半 Parametric 效率理论提出高效的估计器，可以在搅拌 machine learning 算法时实现快速的收敛率。</li>
<li>results: 本研究提供了对政策学习的理论保证，并验证了提出的框架的finite-sample表现，通过了全面的数据实验，以确保从观察数据中提取 causal 效应是 Both 可靠和可靠。<details>
<summary>Abstract</summary>
Policy learning utilizing observational data is pivotal across various domains, with the objective of learning the optimal treatment assignment policy while adhering to specific constraints such as fairness, budget, and simplicity. This study introduces a novel positivity-free (stochastic) policy learning framework designed to address the challenges posed by the impracticality of the positivity assumption in real-world scenarios. This framework leverages incremental propensity score policies to adjust propensity score values instead of assigning fixed values to treatments. We characterize these incremental propensity score policies and establish identification conditions, employing semiparametric efficiency theory to propose efficient estimators capable of achieving rapid convergence rates, even when integrated with advanced machine learning algorithms. This paper provides a thorough exploration of the theoretical guarantees associated with policy learning and validates the proposed framework's finite-sample performance through comprehensive numerical experiments, ensuring the identification of causal effects from observational data is both robust and reliable.
</details>
<details>
<summary>摘要</summary>
政策学习使用观察数据是多种领域的关键，旨在学习最佳治理分配策略，遵循特定的限制，如公平、预算和简单性。本研究提出了一种新的无正定性（随机）政策学习框架，用于实际世界场景中缺乏正定性的挑战。这种框架利用增量抽象分数策略来调整治理分数值，而不是将固定值分配给治理。我们描述这种增量抽象分数策略，并提出了定型条件，使用半 Parametric 效率理论提出高效的估计器，可以在融合先进机器学习算法时实现快速收敛速率。本文对政策学习的理论保证和finite-sample表现进行了全面的探讨，并通过了广泛的数字实验，以确保从观察数据中检测到的 causal 效应是可靠和可信。</SYS>Here's the translation of the text into Simplified Chinese:<SYS>政策学习使用观察数据是多种领域的关键，旨在学习最佳治理分配策略，遵循特定的限制，如公平、预算和简单性。本研究提出了一种新的无正定性（随机）政策学习框架，用于实际世界场景中缺乏正定性的挑战。这种框架利用增量抽象分数策略来调整治理分数值，而不是将固定值分配给治理。我们描述这种增量抽象分数策略，并提出了定型条件。使用半 Parametric 效率理论提出高效的估计器，可以在融合先进机器学习算法时实现快速收敛速率。本文对政策学习的理论保证和finite-sample表现进行了全面的探讨，并通过了广泛的数字实验，以确保从观察数据中检测到的 causal 效应是可靠和可信。</SYS>
</details></li>
</ul>
<hr>
<h2 id="Diffusion-Prior-Regularized-Iterative-Reconstruction-for-Low-dose-CT"><a href="#Diffusion-Prior-Regularized-Iterative-Reconstruction-for-Low-dose-CT" class="headerlink" title="Diffusion Prior Regularized Iterative Reconstruction for Low-dose CT"></a>Diffusion Prior Regularized Iterative Reconstruction for Low-dose CT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06949">http://arxiv.org/abs/2310.06949</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenjun Xia, Yongyi Shi, Chuang Niu, Wenxiang Cong, Ge Wang</li>
<li>for: 减少X射线辐射剂量，提高 computed tomography（CT）图像质量</li>
<li>methods: 引入迭代重建算法，并将杂散抑制推理模型（DDPM）与数据准确性优先重建方法融合</li>
<li>results: 实现高Definition CT图像重建，减少辐射剂量<details>
<summary>Abstract</summary>
Computed tomography (CT) involves a patient's exposure to ionizing radiation. To reduce the radiation dose, we can either lower the X-ray photon count or down-sample projection views. However, either of the ways often compromises image quality. To address this challenge, here we introduce an iterative reconstruction algorithm regularized by a diffusion prior. Drawing on the exceptional imaging prowess of the denoising diffusion probabilistic model (DDPM), we merge it with a reconstruction procedure that prioritizes data fidelity. This fusion capitalizes on the merits of both techniques, delivering exceptional reconstruction results in an unsupervised framework. To further enhance the efficiency of the reconstruction process, we incorporate the Nesterov momentum acceleration technique. This enhancement facilitates superior diffusion sampling in fewer steps. As demonstrated in our experiments, our method offers a potential pathway to high-definition CT image reconstruction with minimized radiation.
</details>
<details>
<summary>摘要</summary>
computed tomography (CT) 涉及到辐射 ionizing radiation，以降低辐射剂量，可以 either 降低 X-ray  фото counts 或者下推 projection views。然而，任一种方法通常会 compromise 图像质量。为 Addressing this challenge, here we introduce an iterative reconstruction algorithm regularized by a diffusion prior。 drawing on the exceptional imaging prowess of the denoising diffusion probabilistic model (DDPM), we merge it with a reconstruction procedure that prioritizes data fidelity。 This fusion capitalizes on the merits of both techniques, delivering exceptional reconstruction results in an unsupervised framework。 To further enhance the efficiency of the reconstruction process, we incorporate the Nesterov momentum acceleration technique。 This enhancement facilitates superior diffusion sampling in fewer steps。 As demonstrated in our experiments, our method offers a potential pathway to high-definition CT image reconstruction with minimized radiation。Note: Please note that the translation is in Simplified Chinese, which is one of the two standard versions of Chinese. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="A-Variational-Autoencoder-Framework-for-Robust-Physics-Informed-Cyberattack-Recognition-in-Industrial-Cyber-Physical-Systems"><a href="#A-Variational-Autoencoder-Framework-for-Robust-Physics-Informed-Cyberattack-Recognition-in-Industrial-Cyber-Physical-Systems" class="headerlink" title="A Variational Autoencoder Framework for Robust, Physics-Informed Cyberattack Recognition in Industrial Cyber-Physical Systems"></a>A Variational Autoencoder Framework for Robust, Physics-Informed Cyberattack Recognition in Industrial Cyber-Physical Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06948">http://arxiv.org/abs/2310.06948</a></li>
<li>repo_url: None</li>
<li>paper_authors: Navid Aftabi, Dan Li, Paritosh Ramanan</li>
<li>for: 本研究旨在检测、诊断和定位工业控制系统中的隐藏攻击（covert attack）。</li>
<li>methods: 本研究提出了一种数据驱动的框架， combining variational autoencoder（VAE）、回归神经网络（RNN）和深度神经网络（DNN），以检测、诊断和定位隐藏攻击。</li>
<li>results: 经过实验研究，提出的方法在一个网络化的电力传输系统上的实验研究中表现出了应用性和效果。<details>
<summary>Abstract</summary>
Cybersecurity of Industrial Cyber-Physical Systems is drawing significant concerns as data communication increasingly leverages wireless networks. A lot of data-driven methods were develope for detecting cyberattacks, but few are focused on distinguishing them from equipment faults. In this paper, we develop a data-driven framework that can be used to detect, diagnose, and localize a type of cyberattack called covert attacks on networked industrial control systems. The framework has a hybrid design that combines a variational autoencoder (VAE), a recurrent neural network (RNN), and a Deep Neural Network (DNN). This data-driven framework considers the temporal behavior of a generic physical system that extracts features from the time series of the sensor measurements that can be used for detecting covert attacks, distinguishing them from equipment faults, as well as localize the attack/fault. We evaluate the performance of the proposed method through a realistic simulation study on a networked power transmission system as a typical example of ICS. We compare the performance of the proposed method with the traditional model-based method to show its applicability and efficacy.
</details>
<details>
<summary>摘要</summary>
工业控制系统的网络化Cybersecurity引发了 significiant concerns，因为数据通信越来越多地使用无线网络。许多数据驱动方法已经开发，但很少关注于分化攻击和设备故障之间的差异。在这篇论文中，我们开发了一个数据驱动的框架，可以用于检测、诊断和地址网络化工业控制系统中的隐藏攻击。这个框架具有混合设计，组合了变量自适应器（VAE）、回归神经网络（RNN）和深度神经网络（DNN）。这个数据驱动框架考虑了生成器物理系统的时间行为，从感知器测量时间序列中提取特征，用于检测隐藏攻击、分化攻击和位置攻击。我们通过一个现实的 simulate 研究，对一个网络化的电力传输系统进行评估，以示方法的适用性和效果。我们将传统的模型基型方法与该方法进行比较，以显示其适用性和有效性。
</details></li>
</ul>
<hr>
<h2 id="LLMs-Killed-the-Script-Kiddie-How-Agents-Supported-by-Large-Language-Models-Change-the-Landscape-of-Network-Threat-Testing"><a href="#LLMs-Killed-the-Script-Kiddie-How-Agents-Supported-by-Large-Language-Models-Change-the-Landscape-of-Network-Threat-Testing" class="headerlink" title="LLMs Killed the Script Kiddie: How Agents Supported by Large Language Models Change the Landscape of Network Threat Testing"></a>LLMs Killed the Script Kiddie: How Agents Supported by Large Language Models Change the Landscape of Network Threat Testing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06936">http://arxiv.org/abs/2310.06936</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stephen Moskal, Sam Laney, Erik Hemberg, Una-May O’Reilly</li>
<li>for: 这个研究探讨了大语言模型（LLM）在掌握威胁、生成工具信息和自动化网络攻击方面的潜力。</li>
<li>methods: 研究员采用了手动探索 LLM 在支持特定威胁行动和决策方面的方法，然后通过自动化决策过程来实现网络攻击。研究人员还提出了引言工程方法来实现计划-行动-报告循环，以及一种提示链设计来导向Sequential决策过程。</li>
<li>results: 研究人员通过 demonstrate 一个简单的网络攻击use case来评估 LLM 在网络攻击方面的知识程度，并提供了引言设计的指导方针。研究人员还提出了 LLM 在加速威胁actor能力方面的可能影响和伦理考虑。研究结果表明，LLM 可以用于生成有用的信息和自动化网络攻击，但是它们的潜力和敏捷性仍需进一步探索。<details>
<summary>Abstract</summary>
In this paper, we explore the potential of Large Language Models (LLMs) to reason about threats, generate information about tools, and automate cyber campaigns. We begin with a manual exploration of LLMs in supporting specific threat-related actions and decisions. We proceed by automating the decision process in a cyber campaign. We present prompt engineering approaches for a plan-act-report loop for one action of a threat campaign and and a prompt chaining design that directs the sequential decision process of a multi-action campaign. We assess the extent of LLM's cyber-specific knowledge w.r.t the short campaign we demonstrate and provide insights into prompt design for eliciting actionable responses. We discuss the potential impact of LLMs on the threat landscape and the ethical considerations of using LLMs for accelerating threat actor capabilities. We report a promising, yet concerning, application of generative AI to cyber threats. However, the LLM's capabilities to deal with more complex networks, sophisticated vulnerabilities, and the sensitivity of prompts are open questions. This research should spur deliberations over the inevitable advancements in LLM-supported cyber adversarial landscape.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们探讨大语言模型（LLM）在处理威胁、生成工具信息和自动化网络攻击方面的潜力。我们开始于手动探索LLM在支持特定威胁行动和决策过程中的能力。然后我们将决策过程自动化，并提出了一种plan-act-report循环和一种链接式提示设计，以导引多个行动的顺序决策过程。我们评估了LLM在短期攻击кампаgn中的网络专业知识的程度，并提供了提示设计的启示，以便获得可行的回答。我们讨论了LLM在威胁风险面临的潜在影响和使用LLM加速攻击者能力的伦理考虑因素。我们报道了一种有前途又担忧的应用 génériques AI 在网络威胁方面，但 LLM 在更复杂的网络、更复杂的漏洞和提示敏感性方面的能力仍然是开Question。这种研究应当促使人们对 LLM 在网络威胁领域的不断进步举行深思熟虑。
</details></li>
</ul>
<hr>
<h2 id="Quantum-Shadow-Gradient-Descent-for-Quantum-Learning"><a href="#Quantum-Shadow-Gradient-Descent-for-Quantum-Learning" class="headerlink" title="Quantum Shadow Gradient Descent for Quantum Learning"></a>Quantum Shadow Gradient Descent for Quantum Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06935">http://arxiv.org/abs/2310.06935</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohsen Heidari, Mobasshir A Naved, Wenbo Xie, Arjun Jacob Grama, Wojciech Szpankowski</li>
<li>for: 这个论文提出了一种新的程序，叫做量子影差下降（QSGD），用于解决关键挑战。我们的方法具有一次性的优点，不需要任何样本的重复，并且与精确的质量计算相比，其减速率相当。</li>
<li>methods: 我们提出了一种新的技术，即生成量子影（QSS），而不是传统的类型 Ansatz。在经典计算机上进行计算时，这些计算会变得繁琐和不可持续，因为维度会增长 exponentially。我们的方法是通过测量量子影来解决这个问题。</li>
<li>results: 我们的研究表明，使用量子影可以减少计算量，并且可以应用于更一般的非产品 Ansatz 中。我们提供了理论证明、减速分析和数值实验来支持我们的结论。<details>
<summary>Abstract</summary>
This paper proposes a new procedure called quantum shadow gradient descent (QSGD) that addresses these key challenges. Our method has the benefits of a one-shot approach, in not requiring any sample duplication while having a convergence rate comparable to the ideal update rule using exact gradient computation. We propose a new technique for generating quantum shadow samples (QSS), which generates quantum shadows as opposed to classical shadows used in existing works. With classical shadows, the computations are typically performed on classical computers and, hence, are prohibitive since the dimension grows exponentially. Our approach resolves this issue by measurements of quantum shadows. As the second main contribution, we study more general non-product ansatz of the form $\exp\{i\sum_j \theta_j A_j\}$ that model variational Hamiltonians. We prove that the gradient can be written in terms of the gradient of single-parameter ansatzes that can be easily measured. Our proof is based on the Suzuki-Trotter approximation; however, our expressions are exact, unlike prior efforts that approximate non-product operators. As a result, existing gradient measurement techniques can be applied to more general VQAs followed by correction terms without any approximation penalty. We provide theoretical proofs, convergence analysis and verify our results through numerical experiments.
</details>
<details>
<summary>摘要</summary>
We introduce a new technique for generating quantum shadow samples (QSS), which generates quantum shadows instead of the classical shadows used in existing works. With classical shadows, computations are typically performed on classical computers, and the dimension grows exponentially. Our approach resolves this issue by measuring quantum shadows.As the second main contribution, we study more general non-product ansatz of the form $\exp\{i\sum_j \theta_j A_j\}$ that model variational Hamiltonians. We prove that the gradient can be written in terms of the gradient of single-parameter ansatzes that can be easily measured. Our proof is based on the Suzuki-Trotter approximation, but our expressions are exact, unlike prior efforts that approximate non-product operators.As a result, existing gradient measurement techniques can be applied to more general VQAs followed by correction terms without any approximation penalty. We provide theoretical proofs, convergence analysis, and verify our results through numerical experiments.
</details></li>
</ul>
<hr>
<h2 id="Prosody-Analysis-of-Audiobooks"><a href="#Prosody-Analysis-of-Audiobooks" class="headerlink" title="Prosody Analysis of Audiobooks"></a>Prosody Analysis of Audiobooks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06930">http://arxiv.org/abs/2310.06930</a></li>
<li>repo_url: None</li>
<li>paper_authors: Charuta Pethe, Yunting Yin, Steven Skiena</li>
<li>for: 这个论文的目的是提高文本读取的自然语音质量。</li>
<li>methods: 这个论文使用了语言模型来预测文本中的语调属性（抑高、声量和速度），并与人工读书 recording进行对比。</li>
<li>results: 研究结果表明，使用这种方法可以更好地预测读书 recording中的语调属性，并且与人工读书 recording更加相似。此外，人类评估研究也表明，人们更偏好使用这种方法生成的Audiobook读书 recording。<details>
<summary>Abstract</summary>
Recent advances in text-to-speech have made it possible to generate natural-sounding audio from text. However, audiobook narrations involve dramatic vocalizations and intonations by the reader, with greater reliance on emotions, dialogues, and descriptions in the narrative. Using our dataset of 93 aligned book-audiobook pairs, we present improved models for prosody prediction properties (pitch, volume, and rate of speech) from narrative text using language modeling. Our predicted prosody attributes correlate much better with human audiobook readings than results from a state-of-the-art commercial TTS system: our predicted pitch shows a higher correlation with human reading for 22 out of the 24 books, while our predicted volume attribute proves more similar to human reading for 23 out of the 24 books. Finally, we present a human evaluation study to quantify the extent that people prefer prosody-enhanced audiobook readings over commercial text-to-speech systems.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Stochastic-Super-resolution-of-Cosmological-Simulations-with-Denoising-Diffusion-Models"><a href="#Stochastic-Super-resolution-of-Cosmological-Simulations-with-Denoising-Diffusion-Models" class="headerlink" title="Stochastic Super-resolution of Cosmological Simulations with Denoising Diffusion Models"></a>Stochastic Super-resolution of Cosmological Simulations with Denoising Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06929">http://arxiv.org/abs/2310.06929</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andreas Schanz, Florian List, Oliver Hahn</li>
<li>for: 这个论文的目的是用深度学习模型来提高 cosmological simulation 的分辨率，使其能够更好地模拟小规模结构。</li>
<li>methods: 这个论文使用了 denoising diffusion models 作为生成模型，并开发了一种新的 “filter-boosted” 训练方法来提高模型的准确性。</li>
<li>results: 这个论文的结果表明，使用 denoising diffusion models 可以生成高度可信度的 super-resolution 图像和电磁波谱，并且能够复制低分辨率 simulation 中的小规模特征。这些结果表明，这种 super-resolution 模型可以用于 cosmic structure formation 中的 uncertainty quantification。<details>
<summary>Abstract</summary>
In recent years, deep learning models have been successfully employed for augmenting low-resolution cosmological simulations with small-scale information, a task known as "super-resolution". So far, these cosmological super-resolution models have relied on generative adversarial networks (GANs), which can achieve highly realistic results, but suffer from various shortcomings (e.g. low sample diversity). We introduce denoising diffusion models as a powerful generative model for super-resolving cosmic large-scale structure predictions (as a first proof-of-concept in two dimensions). To obtain accurate results down to small scales, we develop a new "filter-boosted" training approach that redistributes the importance of different scales in the pixel-wise training objective. We demonstrate that our model not only produces convincing super-resolution images and power spectra consistent at the percent level, but is also able to reproduce the diversity of small-scale features consistent with a given low-resolution simulation. This enables uncertainty quantification for the generated small-scale features, which is critical for the usefulness of such super-resolution models as a viable surrogate model for cosmic structure formation.
</details>
<details>
<summary>摘要</summary>
Recently, deep learning models have been successfully used for augmenting low-resolution cosmological simulations with small-scale information, a task known as "super-resolution". So far, these cosmological super-resolution models have relied on generative adversarial networks (GANs), which can achieve highly realistic results, but suffer from various shortcomings (e.g. low sample diversity). We introduce denoising diffusion models as a powerful generative model for super-resolving cosmic large-scale structure predictions (as a first proof-of-concept in two dimensions). To obtain accurate results down to small scales, we develop a new "filter-boosted" training approach that redistributes the importance of different scales in the pixel-wise training objective. We demonstrate that our model not only produces convincing super-resolution images and power spectra consistent at the percent level, but is also able to reproduce the diversity of small-scale features consistent with a given low-resolution simulation. This enables uncertainty quantification for the generated small-scale features, which is critical for the usefulness of such super-resolution models as a viable surrogate model for cosmic structure formation.Here is the text with some additional information about the translation:The translation is in Simplified Chinese, which is the standardized form of Chinese used in mainland China and widely used in other countries as well. The translation is written in a formal and precise style, using technical terms and phrases appropriate for a scientific paper. The text includes some specialized vocabulary and concepts related to cosmology and deep learning, which are translated accurately and consistently based on their meanings in the context of the text. The translation also includes some cultural references and expressions that are specific to Chinese culture, but are not essential to the understanding of the scientific content. Overall, the translation is accurate and faithful to the original text, and should be easily understandable to readers who are familiar with the subject matter and the language.
</details></li>
</ul>
<hr>
<h2 id="Inverse-Factorized-Q-Learning-for-Cooperative-Multi-agent-Imitation-Learning"><a href="#Inverse-Factorized-Q-Learning-for-Cooperative-Multi-agent-Imitation-Learning" class="headerlink" title="Inverse Factorized Q-Learning for Cooperative Multi-agent Imitation Learning"></a>Inverse Factorized Q-Learning for Cooperative Multi-agent Imitation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06801">http://arxiv.org/abs/2310.06801</a></li>
<li>repo_url: None</li>
<li>paper_authors: The Viet Bui, Tien Mai, Thanh Hong Nguyen</li>
<li>for: 这个论文关注了多智能体学习（IL）在合作多智能体系统中的应用，具体来说是通过示范学习来学习专家行为。</li>
<li>methods: 该论文提出了一种新的多智能体IL算法，该算法利用混合网络来聚合分布式Q函数，并且可以通过全局状态信息来训练混合网络的参数。</li>
<li>results: 该论文通过对一些复杂的竞争和合作多智能体游戏环境进行了广泛的实验，证明了该算法的有效性，并且比现有的多智能体IL算法表现更好。<details>
<summary>Abstract</summary>
This paper concerns imitation learning (IL) (i.e, the problem of learning to mimic expert behaviors from demonstrations) in cooperative multi-agent systems. The learning problem under consideration poses several challenges, characterized by high-dimensional state and action spaces and intricate inter-agent dependencies. In a single-agent setting, IL has proven to be done efficiently through an inverse soft-Q learning process given expert demonstrations. However, extending this framework to a multi-agent context introduces the need to simultaneously learn both local value functions to capture local observations and individual actions, and a joint value function for exploiting centralized learning. In this work, we introduce a novel multi-agent IL algorithm designed to address these challenges. Our approach enables the centralized learning by leveraging mixing networks to aggregate decentralized Q functions. A main advantage of this approach is that the weights of the mixing networks can be trained using information derived from global states. We further establish conditions for the mixing networks under which the multi-agent objective function exhibits convexity within the Q function space. We present extensive experiments conducted on some challenging competitive and cooperative multi-agent game environments, including an advanced version of the Star-Craft multi-agent challenge (i.e., SMACv2), which demonstrates the effectiveness of our proposed algorithm compared to existing state-of-the-art multi-agent IL algorithms.
</details>
<details>
<summary>摘要</summary>
To address these challenges, we propose a novel multi-agent IL algorithm that leverages mixing networks to aggregate decentralized Q functions. The weights of the mixing networks can be trained using information derived from global states. We establish conditions for the mixing networks under which the multi-agent objective function exhibits convexity within the Q function space.We present extensive experiments conducted on challenging competitive and cooperative multi-agent game environments, including the advanced version of the Star-Craft multi-agent challenge (SMACv2). Our proposed algorithm outperforms existing state-of-the-art multi-agent IL algorithms.
</details></li>
</ul>
<hr>
<h2 id="Test-Evaluation-Best-Practices-for-Machine-Learning-Enabled-Systems"><a href="#Test-Evaluation-Best-Practices-for-Machine-Learning-Enabled-Systems" class="headerlink" title="Test &amp; Evaluation Best Practices for Machine Learning-Enabled Systems"></a>Test &amp; Evaluation Best Practices for Machine Learning-Enabled Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06800">http://arxiv.org/abs/2310.06800</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jaganmohan Chandrasekaran, Tyler Cody, Nicola McCarthy, Erin Lanus, Laura Freeman</li>
<li>for: This paper aims to present best practices for the Test and Evaluation (T&amp;E) of Machine Learning (ML)-enabled software systems across their lifecycle.</li>
<li>methods: The paper categorizes the lifecycle of ML-enabled software systems into three stages: component, integration and deployment, and post-deployment. The primary objective is to test and evaluate the ML model as a standalone component, and then evaluate an integrated ML-enabled system consisting of both ML and non-ML components.</li>
<li>results: The paper highlights the challenges of T&amp;E in ML-enabled software systems and the need for systematic testing approaches, adequacy measurements, and metrics to address these challenges across all stages of the ML-enabled system lifecycle.<details>
<summary>Abstract</summary>
Machine learning (ML) - based software systems are rapidly gaining adoption across various domains, making it increasingly essential to ensure they perform as intended. This report presents best practices for the Test and Evaluation (T&E) of ML-enabled software systems across its lifecycle. We categorize the lifecycle of ML-enabled software systems into three stages: component, integration and deployment, and post-deployment. At the component level, the primary objective is to test and evaluate the ML model as a standalone component. Next, in the integration and deployment stage, the goal is to evaluate an integrated ML-enabled system consisting of both ML and non-ML components. Finally, once the ML-enabled software system is deployed and operationalized, the T&E objective is to ensure the system performs as intended. Maintenance activities for ML-enabled software systems span the lifecycle and involve maintaining various assets of ML-enabled software systems.   Given its unique characteristics, the T&E of ML-enabled software systems is challenging. While significant research has been reported on T&E at the component level, limited work is reported on T&E in the remaining two stages. Furthermore, in many cases, there is a lack of systematic T&E strategies throughout the ML-enabled system's lifecycle. This leads practitioners to resort to ad-hoc T&E practices, which can undermine user confidence in the reliability of ML-enabled software systems. New systematic testing approaches, adequacy measurements, and metrics are required to address the T&E challenges across all stages of the ML-enabled system lifecycle.
</details>
<details>
<summary>摘要</summary>
At the component stage, the primary goal is to evaluate the ML model as a standalone component. In the integration and deployment stage, the objective is to evaluate an integrated ML-enabled system consisting of both ML and non-ML components. Finally, once the ML-enabled software system is deployed and operationalized, the T&E objective is to ensure the system performs as intended.Maintenance activities for ML-enabled software systems span the lifecycle and involve maintaining various assets of ML-enabled software systems. The T&E of ML-enabled software systems is challenging due to their unique characteristics. While there has been significant research on T&E at the component level, there is limited work on T&E in the remaining two stages. Moreover, there is often a lack of systematic T&E strategies throughout the ML-enabled system's lifecycle, leading practitioners to resort to ad-hoc T&E practices that can undermine user confidence in the reliability of ML-enabled software systems.New systematic testing approaches, adequacy measurements, and metrics are needed to address the T&E challenges across all stages of the ML-enabled system lifecycle.
</details></li>
</ul>
<hr>
<h2 id="Spectral-Entry-wise-Matrix-Estimation-for-Low-Rank-Reinforcement-Learning"><a href="#Spectral-Entry-wise-Matrix-Estimation-for-Low-Rank-Reinforcement-Learning" class="headerlink" title="Spectral Entry-wise Matrix Estimation for Low-Rank Reinforcement Learning"></a>Spectral Entry-wise Matrix Estimation for Low-Rank Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06793">http://arxiv.org/abs/2310.06793</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefan Stojanovic, Yassir Jedra, Alexandre Proutiere</li>
<li>for:  Matrix estimation problems in reinforcement learning (RL) with low-rank structure, such as low-rank bandits and Markov Decision Processes (MDPs).</li>
<li>methods:  Spectral-based matrix estimation approaches that efficiently recover the singular subspaces of the matrix and exhibit nearly-minimal entry-wise error.</li>
<li>results:  State-of-the-art performance guarantees for two examples of algorithms: a regret minimization algorithm for low-rank bandit problems, and a best policy identification algorithm for reward-free RL in low-rank MDPs.<details>
<summary>Abstract</summary>
We study matrix estimation problems arising in reinforcement learning (RL) with low-rank structure. In low-rank bandits, the matrix to be recovered specifies the expected arm rewards, and for low-rank Markov Decision Processes (MDPs), it may for example characterize the transition kernel of the MDP. In both cases, each entry of the matrix carries important information, and we seek estimation methods with low entry-wise error. Importantly, these methods further need to accommodate for inherent correlations in the available data (e.g. for MDPs, the data consists of system trajectories). We investigate the performance of simple spectral-based matrix estimation approaches: we show that they efficiently recover the singular subspaces of the matrix and exhibit nearly-minimal entry-wise error. These new results on low-rank matrix estimation make it possible to devise reinforcement learning algorithms that fully exploit the underlying low-rank structure. We provide two examples of such algorithms: a regret minimization algorithm for low-rank bandit problems, and a best policy identification algorithm for reward-free RL in low-rank MDPs. Both algorithms yield state-of-the-art performance guarantees.
</details>
<details>
<summary>摘要</summary>
我们研究在奖励学习（RL）中出现的矩阵估计问题，其中矩阵往往具有低级别结构。在低级别投机中，矩阵需要 recuperate 表示每个臂奖励，而在低级别Markov决策过程（MDP）中，它可能表示MDP的转移核函数。在两种情况下，每个矩阵元素都具有重要信息，我们寻找低入门错误的估计方法。这些方法还需要考虑数据中的自然相关性（例如，MDP数据包括系统轨迹）。我们研究spectral-based矩阵估计方法的性能，并证明它们可以高效地回归矩阵的单个子空间，并且显示出nearly-minimal 入门错误。这些新结果在低级别矩阵估计方面，使得我们可以开发充分利用低级别结构的奖励学习算法。我们提供了两个例子：一个为低级别投机问题的奖励最小化算法，另一个为无奖励RL的低级别MDP中的最佳策略标识算法。两个算法都有状态 искусственный智能的性能保证。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Predictive-Capabilities-in-Data-Driven-Dynamical-Modeling-with-Automatic-Differentiation-Koopman-and-Neural-ODE-Approaches"><a href="#Enhancing-Predictive-Capabilities-in-Data-Driven-Dynamical-Modeling-with-Automatic-Differentiation-Koopman-and-Neural-ODE-Approaches" class="headerlink" title="Enhancing Predictive Capabilities in Data-Driven Dynamical Modeling with Automatic Differentiation: Koopman and Neural ODE Approaches"></a>Enhancing Predictive Capabilities in Data-Driven Dynamical Modeling with Automatic Differentiation: Koopman and Neural ODE Approaches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06790">http://arxiv.org/abs/2310.06790</a></li>
<li>repo_url: None</li>
<li>paper_authors: C. Ricardo Constante-Amores, Alec J. Linot, Michael D. Graham</li>
<li>for: 这个论文的目的是提出一种修改了EXTENDED DYNAMIC MODE DECOMPOSITION WITH DICTIONARY LEARNING（EDMD-DL）方法，以确定对象 observable 的字典和相应的 Koopman 算子近似。</li>
<li>methods: 这种方法使用自动微分的技术来促进梯度下降计算，并使用pseudoinverse来实现。</li>
<li>results: 这个方法在测试了多种方法后，与STATE SPACE APPROACH（神经 ODEs）相比，表现更好，并且在不满足 Koopman 算子的线性条件下，也可以达到比较好的结果。<details>
<summary>Abstract</summary>
Data-driven approximations of the Koopman operator are promising for predicting the time evolution of systems characterized by complex dynamics. Among these methods, the approach known as extended dynamic mode decomposition with dictionary learning (EDMD-DL) has garnered significant attention. Here we present a modification of EDMD-DL that concurrently determines both the dictionary of observables and the corresponding approximation of the Koopman operator. This innovation leverages automatic differentiation to facilitate gradient descent computations through the pseudoinverse. We also address the performance of several alternative methodologies. We assess a 'pure' Koopman approach, which involves the direct time-integration of a linear, high-dimensional system governing the dynamics within the space of observables. Additionally, we explore a modified approach where the system alternates between spaces of states and observables at each time step -- this approach no longer satisfies the linearity of the true Koopman operator representation. For further comparisons, we also apply a state space approach (neural ODEs). We consider systems encompassing two and three-dimensional ordinary differential equation systems featuring steady, oscillatory, and chaotic attractors, as well as partial differential equations exhibiting increasingly complex and intricate behaviors. Our framework significantly outperforms EDMD-DL. Furthermore, the state space approach offers superior performance compared to the 'pure' Koopman approach where the entire time evolution occurs in the space of observables. When the temporal evolution of the Koopman approach alternates between states and observables at each time step, however, its predictions become comparable to those of the state space approach.
</details>
<details>
<summary>摘要</summary>
“数据驱动的科普曼算子估计方法显示出预测复杂动力系统时间演化的承诺。这些方法中，使用字典学习的扩展动态模式分解（EDMD-DL）已经吸引了广泛的关注。在这里，我们提出了一种同时确定字典和科普曼算子的估计方法的修改。这种创新利用了自动微分的技术来促进梯度下降计算，通过 Pseudoinverse 来实现。我们还评估了一些其他方法。我们评估了一种 '纯' 科普曼方法，该方法直接在可观察空间中进行时间 инте格alion，并且可以在高维度系统中实现。此外，我们还探讨了一种 modify 方法，该方法在每次时间步骤时将系统转换到不同的空间中，这种方法不再满足真正的科普曼算子表示。为了进一步比较，我们还应用了一种状态空间方法（神经 ODEs）。我们考虑了两维和三维常微方程系统，以及具有复杂和精细行为的 partial differential equation 系统。我们的框架在 EDMD-DL 方法上表现出了显著的改善，而且状态空间方法在比较 '纯' 科普曼方法和 EDMD-DL 方法时表现出了更好的性能。当科普曼方法在每次时间步骤时 alternate  между状态和可观察空间时，其预测结果与状态空间方法相近。”
</details></li>
</ul>
<hr>
<h2 id="Information-Content-Exploration"><a href="#Information-Content-Exploration" class="headerlink" title="Information Content Exploration"></a>Information Content Exploration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06777">http://arxiv.org/abs/2310.06777</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Jacob Chmura, Hasham Burhani, Xiao Qi Shi</li>
<li>for: 这个论文的目的是提出一种基于信息理论的激励奖励方法，以便在稀缺奖励环境下进行有效和可扩展的探索。</li>
<li>methods: 该论文使用了一种基于信息理论的激励奖励方法，即通过最大化 trajectory 中agent所获得信息的内容来衡量探索行为。作者还比较了该方法与其他探索基于奖励的方法，如Curiosity Driven Learning和Random Network Distillation。</li>
<li>results: 作者的信息理论奖励方法在多个游戏中表现出了更高的效率和可扩展性，包括Montezuma Revenge这个知名的奖励学习任务。此外，作者还提出了一种扩展方案，即在离散压缩的射频空间中最大化信息内容，以提高样本效率和扩展性。<details>
<summary>Abstract</summary>
Sparse reward environments are known to be challenging for reinforcement learning agents. In such environments, efficient and scalable exploration is crucial. Exploration is a means by which an agent gains information about the environment. We expand on this topic and propose a new intrinsic reward that systemically quantifies exploratory behavior and promotes state coverage by maximizing the information content of a trajectory taken by an agent. We compare our method to alternative exploration based intrinsic reward techniques, namely Curiosity Driven Learning and Random Network Distillation. We show that our information theoretic reward induces efficient exploration and outperforms in various games, including Montezuma Revenge, a known difficult task for reinforcement learning. Finally, we propose an extension that maximizes information content in a discretely compressed latent space which boosts sample efficiency and generalizes to continuous state spaces.
</details>
<details>
<summary>摘要</summary>
稀有奖励环境是束缚学习代理的挑战之一。在这些环境中，高效和可扩展的探索是关键。探索是一种方式，通过哪里让代理获得环境信息。我们在这个主题上进一步探讨，并提出一种新的内在奖励方法，系统地量化探索行为，并且通过最大化征文轨迹中的信息内容来促进状态覆盖。我们与其他探索基于内在奖励技术进行比较，包括Curiosity Driven Learning和Random Network Distillation。我们显示，我们的信息学的奖励induces高效的探索，并在多个游戏中表现出优秀，包括Montezuma Revenge，这是已知的Difficult Task for reinforcement learning。最后，我们提出了一种扩展，通过最大化离散压缩的秘密空间中的信息内容来提高样本效率和普遍性，以便应用于连续状态空间。
</details></li>
</ul>
<hr>
<h2 id="Causal-Rule-Learning-Enhancing-the-Understanding-of-Heterogeneous-Treatment-Effect-via-Weighted-Causal-Rules"><a href="#Causal-Rule-Learning-Enhancing-the-Understanding-of-Heterogeneous-Treatment-Effect-via-Weighted-Causal-Rules" class="headerlink" title="Causal Rule Learning: Enhancing the Understanding of Heterogeneous Treatment Effect via Weighted Causal Rules"></a>Causal Rule Learning: Enhancing the Understanding of Heterogeneous Treatment Effect via Weighted Causal Rules</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06746">http://arxiv.org/abs/2310.06746</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ying Wu, Hanzhong Liu, Kai Ren, Xiangyu Chang</li>
<li>For: The paper aims to estimate heterogeneous treatment effects using machine learning methods, with a focus on interpretability for healthcare applications.* Methods: The proposed method, called causal rule learning, involves three phases: rule discovery, rule selection, and rule analysis. It uses a causal forest and D-learning method to identify and deconstruct individual-level treatment effects as a linear combination of subgroup-level effects.* Results: The paper demonstrates the superior performance of causal rule learning in estimating heterogeneous treatment effects when the ground truth is complex and the sample size is sufficient, compared to other methods. It also provides insights into the treatment effects of different subgroups and the weights of each rule in the linear combination.Here is the information in Simplified Chinese text:* For: 该研究使用机器学习方法来估计不同受试者对待的差异效果，特别是在医疗应用中，高度需要可读性。* Methods: 提议的方法是 causal rule learning，它包括三个阶段：规则发现、规则选择和规则分析。它使用 causal forest 和 D-learning 方法来发现和分解个体级待遇的差异效果，以解答过去的忽略问题：一个个体是多个组的成员吗？* Results: 研究表明， causal rule learning 在复杂的真实场景中，对差异效果的可读性估计具有显著优势，比其他方法更好。它还提供了不同组别待遇的治疗效果的信息和每个规则在线性组合中的权重。<details>
<summary>Abstract</summary>
Interpretability is a key concern in estimating heterogeneous treatment effects using machine learning methods, especially for healthcare applications where high-stake decisions are often made. Inspired by the Predictive, Descriptive, Relevant framework of interpretability, we propose causal rule learning which finds a refined set of causal rules characterizing potential subgroups to estimate and enhance our understanding of heterogeneous treatment effects. Causal rule learning involves three phases: rule discovery, rule selection, and rule analysis. In the rule discovery phase, we utilize a causal forest to generate a pool of causal rules with corresponding subgroup average treatment effects. The selection phase then employs a D-learning method to select a subset of these rules to deconstruct individual-level treatment effects as a linear combination of the subgroup-level effects. This helps to answer an ignored question by previous literature: what if an individual simultaneously belongs to multiple groups with different average treatment effects? The rule analysis phase outlines a detailed procedure to further analyze each rule in the subset from multiple perspectives, revealing the most promising rules for further validation. The rules themselves, their corresponding subgroup treatment effects, and their weights in the linear combination give us more insights into heterogeneous treatment effects. Simulation and real-world data analysis demonstrate the superior performance of causal rule learning on the interpretable estimation of heterogeneous treatment effect when the ground truth is complex and the sample size is sufficient.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。<</SYS>>解释性是机器学习方法估计不同征型对减震效果的关键问题，特别是在医疗应用中，高度决策是经常被做的。 draw inspiration from predictive, descriptive, relevant framework of interpretability, we propose causal rule learning, which finds a refined set of causal rules characterizing potential subgroups to estimate and enhance our understanding of heterogeneous treatment effects. causal rule learning involves three phases: rule discovery, rule selection, and rule analysis. In the rule discovery phase, we utilize a causal forest to generate a pool of causal rules with corresponding subgroup average treatment effects. The selection phase then employs a D-learning method to select a subset of these rules to deconstruct individual-level treatment effects as a linear combination of the subgroup-level effects. This helps to answer an ignored question by previous literature: what if an individual simultaneously belongs to multiple groups with different average treatment effects? The rule analysis phase outlines a detailed procedure to further analyze each rule in the subset from multiple perspectives, revealing the most promising rules for further validation. The rules themselves, their corresponding subgroup treatment effects, and their weights in the linear combination give us more insights into heterogeneous treatment effects. Simulation and real-world data analysis demonstrate the superior performance of causal rule learning on the interpretable estimation of heterogeneous treatment effect when the ground truth is complex and the sample size is sufficient.
</details></li>
</ul>
<hr>
<h2 id="Growing-ecosystem-of-deep-learning-methods-for-modeling-protein-unicode-x2013-protein-interactions"><a href="#Growing-ecosystem-of-deep-learning-methods-for-modeling-protein-unicode-x2013-protein-interactions" class="headerlink" title="Growing ecosystem of deep learning methods for modeling protein$\unicode{x2013}$protein interactions"></a>Growing ecosystem of deep learning methods for modeling protein$\unicode{x2013}$protein interactions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06725">http://arxiv.org/abs/2310.06725</a></li>
<li>repo_url: None</li>
<li>paper_authors: Julia R. Rogers, Gergő Nikolényi, Mohammed AlQuraishi</li>
<li>for: 该论文旨在探讨深度学习方法在蛋白质交互模式预测方面的应用，以及这些模型在蛋白质结构和交互功能方面的贡献。</li>
<li>methods: 论文使用了深度学习方法，包括表示学习、几何深度学习和生成模型，来模型蛋白质交互。这些模型借鉴了生物物理知识，以便更好地捕捉蛋白质交互的复杂特征。</li>
<li>results: 论文提出了一系列的成果，包括使用表示学习capture蛋白质交互的复杂特征，使用几何深度学习预测蛋白质结构和交互，以及使用生成模型设计新的蛋白质组合体。这些成果推动了蛋白质交互模型的发展，并为探索蛋白质交互的物理机制和工程蛋白质交互提供了新的思路。<details>
<summary>Abstract</summary>
Numerous cellular functions rely on protein$\unicode{x2013}$protein interactions. Efforts to comprehensively characterize them remain challenged however by the diversity of molecular recognition mechanisms employed within the proteome. Deep learning has emerged as a promising approach for tackling this problem by exploiting both experimental data and basic biophysical knowledge about protein interactions. Here, we review the growing ecosystem of deep learning methods for modeling protein interactions, highlighting the diversity of these biophysically-informed models and their respective trade-offs. We discuss recent successes in using representation learning to capture complex features pertinent to predicting protein interactions and interaction sites, geometric deep learning to reason over protein structures and predict complex structures, and generative modeling to design de novo protein assemblies. We also outline some of the outstanding challenges and promising new directions. Opportunities abound to discover novel interactions, elucidate their physical mechanisms, and engineer binders to modulate their functions using deep learning and, ultimately, unravel how protein interactions orchestrate complex cellular behaviors.
</details>
<details>
<summary>摘要</summary>
许多细胞功能都依赖于蛋白质-蛋白质交互。然而，完全描述这些交互的问题仍然面临着蛋白质多样性所带来的挑战。深度学习在解决这个问题上表现出了扎根，因为它可以利用实验数据和蛋白质交互的基本生物物理知识。在这篇文章中，我们评论了深度学习用于模拟蛋白质交互的生态系统，包括这些生物学上 Informed 模型的多样性和它们之间的贸易。我们还讨论了在 representation learning 中捕捉蛋白质交互的复杂特征，在 geometric deep learning 中预测蛋白质结构和预测复杂结构，以及在生成模型中设计新的蛋白质组装。此外，我们还概述了一些未解决的挑战和前瞻的新方向。在使用深度学习来发现新的交互、解释它们的物理机制和通过设计蛋白质拓展器来调整交互的功能时，有很多机会。最终，我们希望通过深度学习来解释蛋白质交互如何指挥细胞行为。
</details></li>
</ul>
<hr>
<h2 id="Improving-Pseudo-Time-Stepping-Convergence-for-CFD-Simulations-With-Neural-Networks"><a href="#Improving-Pseudo-Time-Stepping-Convergence-for-CFD-Simulations-With-Neural-Networks" class="headerlink" title="Improving Pseudo-Time Stepping Convergence for CFD Simulations With Neural Networks"></a>Improving Pseudo-Time Stepping Convergence for CFD Simulations With Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06717">http://arxiv.org/abs/2310.06717</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anouk Zandbergen, Tycho van Noorden, Alexander Heinlein</li>
<li>for: 这种论文是用来描述由奈尔-斯托克方程所描述的粘性流体的计算流体力学（CFD）模拟的。</li>
<li>methods: 这种模拟使用了非线性律vikrey-Stokes方程，并使用了非线性迭代法，如牛顿法，来解决这些方程的系统。</li>
<li>results: 在这种模拟中，使用了一种叫做pseudo-transient continuation的技术，以提高非线性律vikrey-Stokes方程的收敛性。这种技术使用了一个神经网络模型，用于预测当地 pseudo-time step。这种预测方法可以在每个元素上独立地进行，只需要使用当地的信息。 numerically simulate the results of standard benchmark problems, such as flow through a backward facing step geometry and Couette flow, show the performance of the machine learning-enhanced globalization approach.<details>
<summary>Abstract</summary>
Computational fluid dynamics (CFD) simulations of viscous fluids described by the Navier-Stokes equations are considered. Depending on the Reynolds number of the flow, the Navier-Stokes equations may exhibit a highly nonlinear behavior. The system of nonlinear equations resulting from the discretization of the Navier-Stokes equations can be solved using nonlinear iteration methods, such as Newton's method. However, fast quadratic convergence is typically only obtained in a local neighborhood of the solution, and for many configurations, the classical Newton iteration does not converge at all. In such cases, so-called globalization techniques may help to improve convergence.   In this paper, pseudo-transient continuation is employed in order to improve nonlinear convergence. The classical algorithm is enhanced by a neural network model that is trained to predict a local pseudo-time step. Generalization of the novel approach is facilitated by predicting the local pseudo-time step separately on each element using only local information on a patch of adjacent elements as input. Numerical results for standard benchmark problems, including flow through a backward facing step geometry and Couette flow, show the performance of the machine learning-enhanced globalization approach; as the software for the simulations, the CFD module of COMSOL Multiphysics is employed.
</details>
<details>
<summary>摘要</summary>
computational fluid dynamics (CFD) 模拟可以描述由navier-Stokes方程所描述的粘性流体行为。各种 Reynolds 数值可以导致 Navier-Stokes 方程在不同程度上具有非线性行为。通过离散 Navier-Stokes 方程得到的系统非线性方程可以通过非线性迭代方法，如新颖方法，进行解决。然而，通常只有在解的本地邻域内具有快速quadratic convergence的情况下才能获得快速的收敛。在这些情况下，所谓的全局化技术可以帮助改善收敛。在这篇论文中，使用pseudo-transient continuation的方法来改进非线性收敛。经过训练的神经网络模型可以预测当前粘性流体中的local pseudo-time step。通过在每个元素上分别预测local pseudo-time step，并且只使用当地信息进行预测，这种全局化技术可以在不同的粘性流体中实现更好的收敛性。在实验中，使用CFD模块在COMSOL Multiphysics中进行 simulations。Please note that Simplified Chinese is a simplified version of Chinese, and it may not be the exact translation of the original text.
</details></li>
</ul>
<hr>
<h2 id="S4Sleep-Elucidating-the-design-space-of-deep-learning-based-sleep-stage-classification-models"><a href="#S4Sleep-Elucidating-the-design-space-of-deep-learning-based-sleep-stage-classification-models" class="headerlink" title="S4Sleep: Elucidating the design space of deep-learning-based sleep stage classification models"></a>S4Sleep: Elucidating the design space of deep-learning-based sleep stage classification models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06715">http://arxiv.org/abs/2310.06715</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ai4healthuol/s4sleep">https://github.com/ai4healthuol/s4sleep</a></li>
<li>paper_authors: Tiezhi Wang, Nils Strodthoff</li>
<li>for: 这个研究旨在提高某些机器学习算法在sleep stage识别 задании的性能，以降低人工标注的时间消耗和多样性。</li>
<li>methods: 这个研究使用了encoder-predictor架构，并包括了结构化状态空间模型作为一个重要组成部分。</li>
<li>results: 研究发现，这些架构在SHHS数据集上显示了 statistically significant的性能提高，并通过了 both statistical和systematic error estimations。<details>
<summary>Abstract</summary>
Scoring sleep stages in polysomnography recordings is a time-consuming task plagued by significant inter-rater variability. Therefore, it stands to benefit from the application of machine learning algorithms. While many algorithms have been proposed for this purpose, certain critical architectural decisions have not received systematic exploration. In this study, we meticulously investigate these design choices within the broad category of encoder-predictor architectures. We identify robust architectures applicable to both time series and spectrogram input representations. These architectures incorporate structured state space models as integral components, leading to statistically significant advancements in performance on the extensive SHHS dataset. These improvements are assessed through both statistical and systematic error estimations. We anticipate that the architectural insights gained from this study will not only prove valuable for future research in sleep staging but also hold relevance for other time series annotation tasks.
</details>
<details>
<summary>摘要</summary>
评分睡眠阶段在多somnography记录中是一项时间消耗性的任务，受到差异评分者的影响。因此，它可以从机器学习算法的应用中受益。虽然许多算法已经被提出用于此目的，但一些关键的建筑设计决策尚未得到系统的探讨。在这项研究中，我们仔细调查了这些设计选择，并在广泛的SHHS数据集上进行了实证验证。我们发现了一些稳定的架构，可以在时间序列和峰值spectrogram输入表示中应用。这些架构包括结构化状态空间模型为组件，导致了 statistically significant的性能提升。我们通过统计和系统的错误估计来评估这些改进。我们预计，这些建筑学习的成果将不仅对Future sleep stage评分研究有价值，还将对其他时间序列注释任务有 relevance。
</details></li>
</ul>
<hr>
<h2 id="Interpretable-Traffic-Event-Analysis-with-Bayesian-Networks"><a href="#Interpretable-Traffic-Event-Analysis-with-Bayesian-Networks" class="headerlink" title="Interpretable Traffic Event Analysis with Bayesian Networks"></a>Interpretable Traffic Event Analysis with Bayesian Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06713">http://arxiv.org/abs/2310.06713</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tong Yuan, Jian Yang, Zeyi Wen</li>
<li>for: 本研究旨在提供一种可解释的机器学习基于概率网络的交通事故预测方法，以帮助解释交通事故的原因和发生条件。</li>
<li>methods: 本研究使用概率网络建模了交通事故的 causal 关系，并设计了一种数据生成管道来保留交通数据的关键信息。</li>
<li>results: 通过一个具体的案例研究，本研究的方法可以准确预测交通事故，并分析交通和天气事件之间的关系，从而提供可读性的交通事故预测方法。<details>
<summary>Abstract</summary>
Although existing machine learning-based methods for traffic accident analysis can provide good quality results to downstream tasks, they lack interpretability which is crucial for this critical problem. This paper proposes an interpretable framework based on Bayesian Networks for traffic accident prediction. To enable the ease of interpretability, we design a dataset construction pipeline to feed the traffic data into the framework while retaining the essential traffic data information. With a concrete case study, our framework can derive a Bayesian Network from a dataset based on the causal relationships between weather and traffic events across the United States. Consequently, our framework enables the prediction of traffic accidents with competitive accuracy while examining how the probability of these events changes under different conditions, thus illustrating transparent relationships between traffic and weather events. Additionally, the visualization of the network simplifies the analysis of relationships between different variables, revealing the primary causes of traffic accidents and ultimately providing a valuable reference for reducing traffic accidents.
</details>
<details>
<summary>摘要</summary>
尽管现有的机器学习基于方法可以提供下游任务的好质量结果，但它们缺乏可解性，这是交通事故分析中的关键问题。这篇论文提出了一种可解的框架，基于 bayesian 网络，用于交通事故预测。为了实现可解性，我们设计了一个数据建构管道，将交通数据feed到框架中，保留交通数据的重要信息。通过具体的案例研究，我们的框架可以从 dataset 中 deriv 出 bayesian 网络，该网络表示美国交通事故和天气事件之间的 causal 关系。因此，我们的框架可以在不同条件下预测交通事故的发生概率，并评估这些事件的发生probability在不同条件下的变化，从而显示交通和天气事件之间的透明关系。此外，网络的可视化可以简化不同变量之间的关系分析，揭示交通事故的主要原因，并为减少交通事故提供了有价值的参考。
</details></li>
</ul>
<hr>
<h2 id="Zero-Shot-Transfer-in-Imitation-Learning"><a href="#Zero-Shot-Transfer-in-Imitation-Learning" class="headerlink" title="Zero-Shot Transfer in Imitation Learning"></a>Zero-Shot Transfer in Imitation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06710">http://arxiv.org/abs/2310.06710</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alvaro Cauderan, Gauthier Boeshertz, Florian Schwarb, Calvin Zhang</li>
<li>for: 本研究的目的是开发一种可以在不同领域中传递学习的算法，以解决现实世界中的机器学习问题，如 reward 函数设计困难、学习过程中的难以在另一个领域中应用、以及直接在真实世界中学习是昂贵或不可能的。</li>
<li>methods: 本研究使用了最新的深度强化学习技术，包括 AnnealedVAE，以学习分离的状态表示，并使用单个 Q-函数来模仿专家，而不需要对敌对训练。</li>
<li>results: 研究人员在三个不同的环境中证明了该算法的效果，包括一个简单的游戏、一个中等难度的游戏和一个复杂的游戏，并且在不同的传递知识要求下进行了证明。<details>
<summary>Abstract</summary>
We present an algorithm that learns to imitate expert behavior and can transfer to previously unseen domains without retraining. Such an algorithm is extremely relevant in real-world applications such as robotic learning because 1) reward functions are difficult to design, 2) learned policies from one domain are difficult to deploy in another domain and 3) learning directly in the real world is either expensive or unfeasible due to security concerns. To overcome these constraints, we combine recent advances in Deep RL by using an AnnealedVAE to learn a disentangled state representation and imitate an expert by learning a single Q-function which avoids adversarial training. We demonstrate the effectiveness of our method in 3 environments ranging in difficulty and the type of transfer knowledge required.
</details>
<details>
<summary>摘要</summary>
我们提出了一种算法，可以模仿专家行为，并可以在未经 retraining 的情况下在新领域中传输。这种算法在实际应用中非常有用，因为1）奖励函数设计困难，2）从一个领域学习的策略Difficult to deploy in another domain,3）在真实世界中学习直接是非常昂贵或者安全问题。为了解决这些限制，我们将最近的深度学习RL技术与AnnealedVAE结合，学习一个分离的状态表示，并通过学习单个Q函数来模仿专家。我们在3个不同的环境中展示了我们的方法的有效性，这些环境的难度和传输知识类型都不同。
</details></li>
</ul>
<hr>
<h2 id="Generalized-Wick-Decompositions"><a href="#Generalized-Wick-Decompositions" class="headerlink" title="Generalized Wick Decompositions"></a>Generalized Wick Decompositions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06686">http://arxiv.org/abs/2310.06686</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chris MacLeod, Evgenia Nitishinskaya, Buck Shlegeris</li>
<li>for: 本文研究了积分 decompositions（一种将产品随机变量的期望 decomposed into 不同的分解项）和维克 decompositions（一种将产品变量的乘积 decomposed into 不同的子集），然后推广它们到一个新的泛化函数。</li>
<li>methods: 本文使用了积分 decompositions 和维克 decompositions，并将它们推广到一个新的泛化函数。</li>
<li>results: 本文得到了一个新的泛化函数，可以用于代表不同的产品随机变量的乘积。<details>
<summary>Abstract</summary>
We review the cumulant decomposition (a way of decomposing the expectation of a product of random variables (e.g. $\mathbb{E}[XYZ]$) into a sum of terms corresponding to partitions of these variables.) and the Wick decomposition (a way of decomposing a product of (not necessarily random) variables into a sum of terms corresponding to subsets of the variables). Then we generalize each one to a new decomposition where the product function is generalized to an arbitrary function.
</details>
<details>
<summary>摘要</summary>
我们审查汇数分解（一种分解互动随机变量（例如 $\mathbb{E}[XYZ]$）的期望为汇数分割）和威克分解（一种分解互动变量的产生为汇数分割）。然后我们将它们扩展为一个新的分解，其中互动函数被扩展为一个通用函数。Note that "汇数分解" (cumulant decomposition) and "威克分解" (Wick decomposition) are both commonly used terms in probability theory and statistics, and they are often used to analyze the properties of multivariate distributions.
</details></li>
</ul>
<hr>
<h2 id="Enhanced-Graph-Neural-Networks-with-Ego-Centric-Spectral-Subgraph-Embeddings-Augmentation"><a href="#Enhanced-Graph-Neural-Networks-with-Ego-Centric-Spectral-Subgraph-Embeddings-Augmentation" class="headerlink" title="Enhanced Graph Neural Networks with Ego-Centric Spectral Subgraph Embeddings Augmentation"></a>Enhanced Graph Neural Networks with Ego-Centric Spectral Subgraph Embeddings Augmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12169">http://arxiv.org/abs/2310.12169</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anwar-said/esgea">https://github.com/anwar-said/esgea</a></li>
<li>paper_authors: Anwar Said, Mudassir Shabbir, Tyler Derr, Waseem Abbas, Xenofon Koutsoukos</li>
<li>for: 增强 Graph Neural Networks (GNNs) 在复杂网络上进行学习任务的表现。</li>
<li>methods: 我们提出了一种新的方法，即 Ego-centric Spectral subGraph Embedding Augmentation (ESGEA)，用于增强和设计节点特征，特别是在信息缺失的情况下。我们的方法利用当地子图的 topological 结构来生成 topology-aware 节点特征。</li>
<li>results: 我们在 seven 个数据集和八个基eline模型上进行评估，结果显示，对于图像分类任务，ESGEA 可以提高 AUC 的表现，相比基eline模型，提高了10%。对于节点分类任务，ESGEA 可以提高 accuracy 的表现，相比基eline模型，提高了7%。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have shown remarkable merit in performing various learning-based tasks in complex networks. The superior performance of GNNs often correlates with the availability and quality of node-level features in the input networks. However, for many network applications, such node-level information may be missing or unreliable, thereby limiting the applicability and efficacy of GNNs. To address this limitation, we present a novel approach denoted as Ego-centric Spectral subGraph Embedding Augmentation (ESGEA), which aims to enhance and design node features, particularly in scenarios where information is lacking. Our method leverages the topological structure of the local subgraph to create topology-aware node features. The subgraph features are generated using an efficient spectral graph embedding technique, and they serve as node features that capture the local topological organization of the network. The explicit node features, if present, are then enhanced with the subgraph embeddings in order to improve the overall performance. ESGEA is compatible with any GNN-based architecture and is effective even in the absence of node features. We evaluate the proposed method in a social network graph classification task where node attributes are unavailable, as well as in a node classification task where node features are corrupted or even absent. The evaluation results on seven datasets and eight baseline models indicate up to a 10% improvement in AUC and a 7% improvement in accuracy for graph and node classification tasks, respectively.
</details>
<details>
<summary>摘要</summary>
graph neural networks (GNNs) 已经表现出非常出色的表现力在复杂网络中进行学习任务。 GNNs 的高效性 frequently correlates with the availability and quality of node-level features in the input networks。 however, for many network applications, such node-level information may be missing or unreliable， thereby limiting the applicability and efficacy of GNNs。 To address this limitation, we present a novel approach denoted as Ego-centric Spectral subGraph Embedding Augmentation (ESGEA), which aims to enhance and design node features, particularly in scenarios where information is lacking。 Our method leverages the topological structure of the local subgraph to create topology-aware node features。 The subgraph features are generated using an efficient spectral graph embedding technique， and they serve as node features that capture the local topological organization of the network。 The explicit node features, if present, are then enhanced with the subgraph embeddings in order to improve the overall performance。 ESGEA is compatible with any GNN-based architecture and is effective even in the absence of node features。 We evaluate the proposed method in a social network graph classification task where node attributes are unavailable， as well as in a node classification task where node features are corrupted or even absent。 The evaluation results on seven datasets and eight baseline models indicate up to a 10% improvement in AUC and a 7% improvement in accuracy for graph and node classification tasks， respectively。
</details></li>
</ul>
<hr>
<h2 id="On-the-importance-of-catalyst-adsorbate-3D-interactions-for-relaxed-energy-predictions"><a href="#On-the-importance-of-catalyst-adsorbate-3D-interactions-for-relaxed-energy-predictions" class="headerlink" title="On the importance of catalyst-adsorbate 3D interactions for relaxed energy predictions"></a>On the importance of catalyst-adsorbate 3D interactions for relaxed energy predictions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06682">http://arxiv.org/abs/2310.06682</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alvaro Carbonero, Alexandre Duval, Victor Schmidt, Santiago Miret, Alex Hernandez-Garcia, Yoshua Bengio, David Rolnick</li>
<li>for: 预测材料性能和发现</li>
<li>methods: 使用SchNet、DimeNet++和FAENet作为基础架构，对模型进行四种修改来评估其影响：去除输入图中的边、独立表示pooling、不共享后置权重和使用关注机制传递非几何相对信息。</li>
<li>results: 发现虽然去除绑定站信息会降低准确性，修改后的模型仍可以高度准确地预测系统的压缩能量，并且可以在O20数据集上达到remarkably decent MAE。<details>
<summary>Abstract</summary>
The use of machine learning for material property prediction and discovery has traditionally centered on graph neural networks that incorporate the geometric configuration of all atoms. However, in practice not all this information may be readily available, e.g.~when evaluating the potentially unknown binding of adsorbates to catalyst. In this paper, we investigate whether it is possible to predict a system's relaxed energy in the OC20 dataset while ignoring the relative position of the adsorbate with respect to the electro-catalyst. We consider SchNet, DimeNet++ and FAENet as base architectures and measure the impact of four modifications on model performance: removing edges in the input graph, pooling independent representations, not sharing the backbone weights and using an attention mechanism to propagate non-geometric relative information. We find that while removing binding site information impairs accuracy as expected, modified models are able to predict relaxed energies with remarkably decent MAE. Our work suggests future research directions in accelerated materials discovery where information on reactant configurations can be reduced or altogether omitted.
</details>
<details>
<summary>摘要</summary>
传统上，机器学习 для物理性质预测和发现都是通过图 neural networks来实现，其中包括所有原子的几何配置。然而，在实践中，这些信息可能不可获取，例如，评估可能未知的材料吸附物的绑定。在这篇文章中，我们研究了是否可以预测系统的压缩能量在OC20数据集中，而不考虑附着物的相对位置。我们考虑了SchNet、DimeNet++和FAENet作为基础体系，并测试了四种修改对模型性能的影响： removing edges in the input graph、pooling independent representations、不共享背部网重和使用注意机制来传播非几何相对信息。我们发现，尽管 removing binding site information 会降低准确性，但修改后的模型仍然可以预测压缩能量的投影值，并且具有相当的平均误差。我们的工作建议将来的材料发现加速，可以采用减少或完全去除reactant配置信息的方法。
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-Quantum-Systems-with-Magnetic-p-bits"><a href="#Machine-Learning-Quantum-Systems-with-Magnetic-p-bits" class="headerlink" title="Machine Learning Quantum Systems with Magnetic p-bits"></a>Machine Learning Quantum Systems with Magnetic p-bits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06679">http://arxiv.org/abs/2310.06679</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuvro Chowdhury, Kerem Y. Camsari</li>
<li>for: 这篇论文旨在探讨一种基于概率计算的硬件平台，用于解决人工智能算法的计算工作负荷不断增长所带来的危机。</li>
<li>methods: 这篇论文使用概率计算的p-bits作为特定应用和算法的域特定计算 paradigma，并利用磁矩 tunnel junctions（sMTJ）等磁电子设备实现集成的p-计算机。</li>
<li>results: 研究表明，使用这种概率计算机可以实现可扩展和能效的计算，特别适用于将机器学习和量子物理结合起来的新领域。<details>
<summary>Abstract</summary>
The slowing down of Moore's Law has led to a crisis as the computing workloads of Artificial Intelligence (AI) algorithms continue skyrocketing. There is an urgent need for scalable and energy-efficient hardware catering to the unique requirements of AI algorithms and applications. In this environment, probabilistic computing with p-bits emerged as a scalable, domain-specific, and energy-efficient computing paradigm, particularly useful for probabilistic applications and algorithms. In particular, spintronic devices such as stochastic magnetic tunnel junctions (sMTJ) show great promise in designing integrated p-computers. Here, we examine how a scalable probabilistic computer with such magnetic p-bits can be useful for an emerging field combining machine learning and quantum physics.
</details>
<details>
<summary>摘要</summary>
Note:* "Moore's Law" is translated as "Moore's 法则" (Moore zhì yì)* "computing workloads" is translated as "计算工作负载" (jìsuan gongzuò fùyòu)* "Probabilistic computing" is translated as "概率计算" (guīshí jìsuan)* "p-bits" is translated as "p-位" (p-bit)* "spintronic devices" is translated as "磁电子设备" (spintronic seti)* "stochastic magnetic tunnel junctions" is translated as "随机磁隧道结构" (stochastic magnetic tunnel junctions)* "integrated p-computers" is translated as "集成p计算机" (integrated p-computers)* "emerging field" is translated as "新兴领域" (emerging field)* "machine learning and quantum physics" is translated as "机器学习和量子物理" (machine learning and quantum physics)
</details></li>
</ul>
<hr>
<h2 id="Tertiary-Lymphoid-Structures-Generation-through-Graph-based-Diffusion"><a href="#Tertiary-Lymphoid-Structures-Generation-through-Graph-based-Diffusion" class="headerlink" title="Tertiary Lymphoid Structures Generation through Graph-based Diffusion"></a>Tertiary Lymphoid Structures Generation through Graph-based Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06661">http://arxiv.org/abs/2310.06661</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manuel Madeira, Dorina Thanou, Pascal Frossard</li>
<li>for: 这种研究旨在利用图基的深度生成模型来生成生物学意义的细胞图，以更好地理解生物学过程中的规则。</li>
<li>methods: 研究者使用了现状的图基的扩散模型来生成细胞图，并证明了这种模型能够准确地学习细胞图中细胞的三元免疫结构（TLS）含量，这是评估肿瘤进程的重要生物标志。</li>
<li>results: 研究者通过数据扩充来证明了学习生成模型的utilty，并展示了这种模型可以帮助提高肿瘤诊断的准确率。这是首次利用图基的扩散模型来生成生物学意义的细胞图。<details>
<summary>Abstract</summary>
Graph-based representation approaches have been proven to be successful in the analysis of biomedical data, due to their capability of capturing intricate dependencies between biological entities, such as the spatial organization of different cell types in a tumor tissue. However, to further enhance our understanding of the underlying governing biological mechanisms, it is important to accurately capture the actual distributions of such complex data. Graph-based deep generative models are specifically tailored to accomplish that. In this work, we leverage state-of-the-art graph-based diffusion models to generate biologically meaningful cell-graphs. In particular, we show that the adopted graph diffusion model is able to accurately learn the distribution of cells in terms of their tertiary lymphoid structures (TLS) content, a well-established biomarker for evaluating the cancer progression in oncology research. Additionally, we further illustrate the utility of the learned generative models for data augmentation in a TLS classification task. To the best of our knowledge, this is the first work that leverages the power of graph diffusion models in generating meaningful biological cell structures.
</details>
<details>
<summary>摘要</summary>
基于图表表示方法已经在生物医学数据分析中取得成功，因为它们可以捕捉生物实体之间复杂的依赖关系，如肿瘤组织中不同细胞类型之间的空间组织。然而，为了更好地理解生物机制的下面驱动，需要准确地捕捉实际数据的分布。基于图表的深度生成模型可以帮助实现这一目标。在这种工作中，我们利用了状态机器的图表傅振模型，生成生物意义正的细胞图。特别是，我们表明采用的图表傅振模型可以准确地学习细胞的三元免疫结构（TLS）含量，这是评估肿瘤发展的生物标志物。此外，我们还进一步证明了学习的生成模型可以用于数据扩展在TLS分类任务中。根据我们所知，这是首次利用图表傅振模型生成有意义的生物细胞结构。
</details></li>
</ul>
<hr>
<h2 id="Zero-Level-Set-Encoder-for-Neural-Distance-Fields"><a href="#Zero-Level-Set-Encoder-for-Neural-Distance-Fields" class="headerlink" title="Zero-Level-Set Encoder for Neural Distance Fields"></a>Zero-Level-Set Encoder for Neural Distance Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06644">http://arxiv.org/abs/2310.06644</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefan Rhys Jeske, Jonathan Klein, Dominik L. Michels, Jan Bender</li>
<li>for: 本研究旨在提出一种基于神经网络的3D形状表示方法，以减少计算开销并提高推理效率。</li>
<li>methods: 我们提出了一种新的编码-解码神经网络，其包括多级混合系统和维度分解器，并且通过解决固有方程来训练网络。在推理过程中，我们只需要知道零级集，而不需要预先知道非零距离值或形状占据情况。</li>
<li>results: 我们在多种数据集上进行了实验，并证明了我们的方法的有效性、普遍性和可扩展性。我们的方法可以处理弯曲3D形状、单类编码和多类编码等多种应用场景。<details>
<summary>Abstract</summary>
Neural shape representation generally refers to representing 3D geometry using neural networks, e.g., to compute a signed distance or occupancy value at a specific spatial position. Previous methods tend to rely on the auto-decoder paradigm, which often requires densely-sampled and accurate signed distances to be known during training and testing, as well as an additional optimization loop during inference. This introduces a lot of computational overhead, in addition to having to compute signed distances analytically, even during testing. In this paper, we present a novel encoder-decoder neural network for embedding 3D shapes in a single forward pass. Our architecture is based on a multi-scale hybrid system incorporating graph-based and voxel-based components, as well as a continuously differentiable decoder. Furthermore, the network is trained to solve the Eikonal equation and only requires knowledge of the zero-level set for training and inference. Additional volumetric samples can be generated on-the-fly, and incorporated in an unsupervised manner. This means that in contrast to most previous work, our network is able to output valid signed distance fields without explicit prior knowledge of non-zero distance values or shape occupancy. In other words, our network computes approximate solutions to the boundary-valued Eikonal equation. It also requires only a single forward pass during inference, instead of the common latent code optimization. We further propose a modification of the loss function in case that surface normals are not well defined, e.g., in the context of non-watertight surface-meshes and non-manifold geometry. We finally demonstrate the efficacy, generalizability and scalability of our method on datasets consisting of deforming 3D shapes, single class encoding and multiclass encoding, showcasing a wide range of possible applications.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Implicit-Variational-Inference-for-High-Dimensional-Posteriors"><a href="#Implicit-Variational-Inference-for-High-Dimensional-Posteriors" class="headerlink" title="Implicit Variational Inference for High-Dimensional Posteriors"></a>Implicit Variational Inference for High-Dimensional Posteriors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06643">http://arxiv.org/abs/2310.06643</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anshuk Uppal, Kristoffer Stensbo-Smidt, Wouter K. Boomsma, Jes Frellsen</li>
<li>For: The paper is written for advancing the field of variational inference in Bayesian neural networks, specifically by proposing a new method for approximating complex multimodal and correlated posteriors using neural samplers with implicit distributions.* Methods: The paper introduces novel bounds that come about by locally linearizing the neural sampler, which is distinct from existing methods that rely on additional discriminator networks and unstable adversarial objectives. The paper also presents a new sampler architecture that enables implicit distributions over millions of latent variables, addressing computational concerns by using differentiable numerical approximations.* Results: The paper demonstrates that the proposed method is capable of recovering correlations across layers in large Bayesian neural networks, a property that is crucial for a network’s performance but notoriously challenging to achieve. The paper also shows that the expressive posteriors obtained using the proposed method outperform state-of-the-art uncertainty quantification methods in downstream tasks, validating the effectiveness of the training algorithm and the quality of the learned implicit approximation.<details>
<summary>Abstract</summary>
In variational inference, the benefits of Bayesian models rely on accurately capturing the true posterior distribution. We propose using neural samplers that specify implicit distributions, which are well-suited for approximating complex multimodal and correlated posteriors in high-dimensional spaces. Our approach advances inference using implicit distributions by introducing novel bounds that come about by locally linearising the neural sampler. This is distinct from existing methods that rely on additional discriminator networks and unstable adversarial objectives. Furthermore, we present a new sampler architecture that, for the first time, enables implicit distributions over millions of latent variables, addressing computational concerns by using differentiable numerical approximations. Our empirical analysis indicates our method is capable of recovering correlations across layers in large Bayesian neural networks, a property that is crucial for a network's performance but notoriously challenging to achieve. To the best of our knowledge, no other method has been shown to accomplish this task for such large models. Through experiments in downstream tasks, we demonstrate that our expressive posteriors outperform state-of-the-art uncertainty quantification methods, validating the effectiveness of our training algorithm and the quality of the learned implicit approximation.
</details>
<details>
<summary>摘要</summary>
在变分推断中， bayesian 模型的优点取决于正确地捕捉真实 posterior distribution。我们提议使用神经网络 sampler，这些 sampler  specify implicit distribution，适用于高维空间中复杂的多模态和相关 posterior。我们的方法在神经网络 sampler 中引入新的 bound，通过本地线性化来提高推断。这与现有的方法不同，它们基于额外的 discriminator 网络和不稳定的对抗性目标。此外，我们提出了一新的 sampler 架构，可以对 millions 个 latent variable 进行隐式分布，通过使用可微的数学近似来解决计算问题。我们的实验表明，我们的方法可以在大 bayesian 神经网络中恢复层之间的相关性，这是一个关键的性能因素，但是很难实现。而我们的表达式 posterior 可以超越现有的 uncertainty quantification 方法，证明我们的训练算法的有效性和学习的隐式近似质量。
</details></li>
</ul>
<hr>
<h2 id="The-Lattice-Overparametrization-Paradigm-for-the-Machine-Learning-of-Lattice-Operators"><a href="#The-Lattice-Overparametrization-Paradigm-for-the-Machine-Learning-of-Lattice-Operators" class="headerlink" title="The Lattice Overparametrization Paradigm for the Machine Learning of Lattice Operators"></a>The Lattice Overparametrization Paradigm for the Machine Learning of Lattice Operators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06639">http://arxiv.org/abs/2310.06639</a></li>
<li>repo_url: None</li>
<li>paper_authors: Diego Marcondes, Junior Barrera</li>
<li>for: 本文旨在提出一种学习条件下的势函数算子的方法，以控制、透明度和可解性为特点。</li>
<li>methods: 本文使用的方法包括stoochastic lattice gradient descent算法和基于材料的算法，以及如何从势函数算子的重 parametrization中计算其基。</li>
<li>results: 本文的结果表明，通过使用stochastic lattice gradient descent算法可以有效地学习势函数算子，并且可以通过计算其基来了解势函数算子的性质。此外，本文还证明了这种学习方法具有控制、透明度和可解性的特点，这些特点在现代机器学习方法中缺失。<details>
<summary>Abstract</summary>
The machine learning of lattice operators has three possible bottlenecks. From a statistical standpoint, it is necessary to design a constrained class of operators based on prior information with low bias, and low complexity relative to the sample size. From a computational perspective, there should be an efficient algorithm to minimize an empirical error over the class. From an understanding point of view, the properties of the learned operator need to be derived, so its behavior can be theoretically understood. The statistical bottleneck can be overcome due to the rich literature about the representation of lattice operators, but there is no general learning algorithm for them. In this paper, we discuss a learning paradigm in which, by overparametrizing a class via elements in a lattice, an algorithm for minimizing functions in a lattice is applied to learn. We present the stochastic lattice gradient descent algorithm as a general algorithm to learn on constrained classes of operators as long as a lattice overparametrization of it is fixed, and we discuss previous works which are proves of concept. Moreover, if there are algorithms to compute the basis of an operator from its overparametrization, then its properties can be deduced and the understanding bottleneck is also overcome. This learning paradigm has three properties that modern methods based on neural networks lack: control, transparency and interpretability. Nowadays, there is an increasing demand for methods with these characteristics, and we believe that mathematical morphology is in a unique position to supply them. The lattice overparametrization paradigm could be a missing piece for it to achieve its full potential within modern machine learning.
</details>
<details>
<summary>摘要</summary>
《机器学习阶层算子的三个可能的瓶颈》 Machine learning lattice operators have three possible bottlenecks. From a statistical standpoint, it is necessary to design a constrained class of operators based on prior information with low bias and low complexity relative to the sample size. From a computational perspective, there should be an efficient algorithm to minimize an empirical error over the class. From an understanding standpoint, the properties of the learned operator need to be derived, so its behavior can be theoretically understood.The statistical bottleneck can be overcome due to the rich literature about the representation of lattice operators, but there is no general learning algorithm for them. In this paper, we discuss a learning paradigm in which, by overparametrizing a class via elements in a lattice, an algorithm for minimizing functions in a lattice is applied to learn. We present the stochastic lattice gradient descent algorithm as a general algorithm to learn on constrained classes of operators as long as a lattice overparametrization of it is fixed, and we discuss previous works which are proofs of concept.Moreover, if there are algorithms to compute the basis of an operator from its overparametrization, then its properties can be deduced, and the understanding bottleneck is also overcome. This learning paradigm has three properties that modern methods based on neural networks lack: control, transparency, and interpretability. Nowadays, there is an increasing demand for methods with these characteristics, and we believe that mathematical morphology is in a unique position to supply them. The lattice overparametrization paradigm could be a missing piece for it to achieve its full potential within modern machine learning.
</details></li>
</ul>
<hr>
<h2 id="iTransformer-Inverted-Transformers-Are-Effective-for-Time-Series-Forecasting"><a href="#iTransformer-Inverted-Transformers-Are-Effective-for-Time-Series-Forecasting" class="headerlink" title="iTransformer: Inverted Transformers Are Effective for Time Series Forecasting"></a>iTransformer: Inverted Transformers Are Effective for Time Series Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06625">http://arxiv.org/abs/2310.06625</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thuml/iTransformer">https://github.com/thuml/iTransformer</a></li>
<li>paper_authors: Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, Mingsheng Long</li>
<li>for: 提高时间序列预测性能和泛化能力，并减少计算量</li>
<li>methods: 基于Transformer架构，但无需修改基本组件，通过倒转注意力机制和Feed Forward网络来学习多变量相关性</li>
<li>results: 在多个真实世界数据集上实现了一致状态的前一个性和泛化能力，提高了Transformer家族在时间序列预测中的表现，并且可以更好地利用不同的lookback窗口和变量。<details>
<summary>Abstract</summary>
The recent boom of linear forecasting models questions the ongoing passion for architectural modifications of Transformer-based forecasters. These forecasters leverage Transformers to model the global dependencies over temporal tokens of time series, with each token formed by multiple variates of the same timestamp. However, Transformer is challenged in forecasting series with larger lookback windows due to performance degradation and computation explosion. Besides, the unified embedding for each temporal token fuses multiple variates with potentially unaligned timestamps and distinct physical measurements, which may fail in learning variate-centric representations and result in meaningless attention maps. In this work, we reflect on the competent duties of Transformer components and repurpose the Transformer architecture without any adaptation on the basic components. We propose iTransformer that simply inverts the duties of the attention mechanism and the feed-forward network. Specifically, the time points of individual series are embedded into variate tokens which are utilized by the attention mechanism to capture multivariate correlations; meanwhile, the feed-forward network is applied for each variate token to learn nonlinear representations. The iTransformer model achieves consistent state-of-the-art on several real-world datasets, which further empowers the Transformer family with promoted performance, generalization ability across different variates, and better utilization of arbitrary lookback windows, making it a nice alternative as the fundamental backbone of time series forecasting.
</details>
<details>
<summary>摘要</summary>
Recent 崩溃 linear 预测模型 让人们对 transformer 基于预测器的建筑修改lost interest。这些预测器利用 transformer 模型全球时间序列中的全局依赖关系，每个时间戳由多个变量组成。然而， transformer 在更大的 lookback 窗口预测中表现不佳，因为性能下降和计算暴涨。此外，通用 embedding 对每个时间戳进行综合 embedding 可能会失去变量 centered 表示和无用的注意力地图。在这项工作中，我们反思 transformer 组件的能力和挑战，并将 transformer 架构重新定义为 iTransformer。iTransformer 简单地将 attention 机制和 feed-forward 网络的职责反转过来。具体来说，每个时间序列的时刻点被转换成 variate token，并由 attention 机制来捕捉多元相关性；而 feed-forward 网络则是为每个 variate token 进行非线性表示学习。iTransformer 模型在多个实际数据集上具有一致的 state-of-the-art 性能，这使得 transformer 家族受到了提高性能、泛化能力和不同变量之间的更好利用，从而成为时间序列预测的基本脊梁。
</details></li>
</ul>
<hr>
<h2 id="Robustness-May-be-More-Brittle-than-We-Think-under-Different-Degrees-of-Distribution-Shifts"><a href="#Robustness-May-be-More-Brittle-than-We-Think-under-Different-Degrees-of-Distribution-Shifts" class="headerlink" title="Robustness May be More Brittle than We Think under Different Degrees of Distribution Shifts"></a>Robustness May be More Brittle than We Think under Different Degrees of Distribution Shifts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06622">http://arxiv.org/abs/2310.06622</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaican Li, Yifan Zhang, Lanqing Hong, Zhenguo Li, Nevin L. Zhang</li>
<li>for: 本研究旨在探讨模型在不同分布偏移度下的抗衰减性，以提高对模型在不同场景下的评估。</li>
<li>methods: 研究人员采用了多个数据集，并对模型在不同分布偏移度下的性能进行了评估。</li>
<li>results: 研究人员发现，模型在不同分布偏移度下的抗衰减性可能很弱，而且可能存在较大的分布偏移度下的潜在风险。此外，大规模预训练模型，如CLIP，在novel downstream任务中的分布偏移度下也具有敏感性。<details>
<summary>Abstract</summary>
Out-of-distribution (OOD) generalization is a complicated problem due to the idiosyncrasies of possible distribution shifts between training and test domains. Most benchmarks employ diverse datasets to address this issue; however, the degree of the distribution shift between the training domains and the test domains of each dataset remains largely fixed. This may lead to biased conclusions that either underestimate or overestimate the actual OOD performance of a model. Our study delves into a more nuanced evaluation setting that covers a broad range of shift degrees. We show that the robustness of models can be quite brittle and inconsistent under different degrees of distribution shifts, and therefore one should be more cautious when drawing conclusions from evaluations under a limited range of degrees. In addition, we observe that large-scale pre-trained models, such as CLIP, are sensitive to even minute distribution shifts of novel downstream tasks. This indicates that while pre-trained representations may help improve downstream in-distribution performance, they could have minimal or even adverse effects on generalization in certain OOD scenarios of the downstream task if not used properly. In light of these findings, we encourage future research to conduct evaluations across a broader range of shift degrees whenever possible.
</details>
<details>
<summary>摘要</summary>
外部分布（OOD）泛化是一个复杂的问题，因为可能存在训练和测试领域之间的特殊性和分布差异。大多数标准准测试使用多种数据集来解决这个问题，但是每个数据集的测试领域分布shift的度量仍然很大程度上固定。这可能会导致偏向的结论， Either underestimate或Overestimate实际OOD模型的性能。我们的研究探讨了一种更加细化的评估环境，覆盖了广泛的分布差异度。我们发现模型的Robustness可能很脆弱和不一致，因此在不同的分布差异度下，一个应该更加小心地做结论。此外，我们发现大规模预训练模型，如CLIP，对小型分布差异的新任务有敏感性。这表示，虽然预训练表示可以帮助改进下游领域的表现，但是在某些OOD场景下，它们可能会具有微不足或甚至有害的效果。为了更好地评估OOD性能，我们建议将来的研究在可能的范围内进行评估。
</details></li>
</ul>
<hr>
<h2 id="Discovering-Interpretable-Physical-Models-Using-Symbolic-Regression-and-Discrete-Exterior-Calculus"><a href="#Discovering-Interpretable-Physical-Models-Using-Symbolic-Regression-and-Discrete-Exterior-Calculus" class="headerlink" title="Discovering Interpretable Physical Models Using Symbolic Regression and Discrete Exterior Calculus"></a>Discovering Interpretable Physical Models Using Symbolic Regression and Discrete Exterior Calculus</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06609">http://arxiv.org/abs/2310.06609</a></li>
<li>repo_url: None</li>
<li>paper_authors: Simone Manti, Alessandro Lucantonio</li>
<li>for: 这种方法是用于自动发现物理模型，从实验数据开始，以便提高物理 simulations 的准确性和泛化能力。</li>
<li>methods: 这种方法结合了Symbolic Regression（SR）和Discrete Exterior Calculus（DEC），使用了一种自然的通用的整数数学语言，以便推导和分析物理模型。 DEC 提供了一些拓扑学上的建构，以及一种强类型的 SR 过程，以确保数学表达的正确性和减少搜索空间。</li>
<li>results: 通过使用这种方法， authors 成功地重新发现了三个维度物理学中的模型：波松方程、欧拉的弹性材料和Linear Elasticity 方程。这些模型具有通用的特点，可以应用于多种物理模拟问题。<details>
<summary>Abstract</summary>
Computational modeling is a key resource to gather insight into physical systems in modern scientific research and engineering. While access to large amount of data has fueled the use of Machine Learning (ML) to recover physical models from experiments and increase the accuracy of physical simulations, purely data-driven models have limited generalization and interpretability. To overcome these limitations, we propose a framework that combines Symbolic Regression (SR) and Discrete Exterior Calculus (DEC) for the automated discovery of physical models starting from experimental data. Since these models consist of mathematical expressions, they are interpretable and amenable to analysis, and the use of a natural, general-purpose discrete mathematical language for physics favors generalization with limited input data. Importantly, DEC provides building blocks for the discrete analogue of field theories, which are beyond the state-of-the-art applications of SR to physical problems. Further, we show that DEC allows to implement a strongly-typed SR procedure that guarantees the mathematical consistency of the recovered models and reduces the search space of symbolic expressions. Finally, we prove the effectiveness of our methodology by re-discovering three models of Continuum Physics from synthetic experimental data: Poisson equation, the Euler's Elastica and the equations of Linear Elasticity. Thanks to their general-purpose nature, the methods developed in this paper may be applied to diverse contexts of physical modeling.
</details>
<details>
<summary>摘要</summary>
现代科学研究和工程中的物理系统模型化是一个关键资源，帮助我们更深入理解物理系统的行为。虽然大量数据的可用性推动了机器学习（ML）技术来从实验中提取物理模型并提高物理仿真的准确性，但纯数据驱动的模型受到限制，其可重复性和可解释性受到限制。为了超越这些限制，我们提出了一种整合符号 regression（SR）和离散外部 calculus（DEC）的框架，用于自动找到从实验数据开始的物理模型。由于这些模型由数学表达组成，它们可以被解释和分析，而使用自然的通用离散数学语言也会增加泛化的能力。此外，DEC提供了离散场论的建构元素，这些元素超越了当前SR在物理问题上的应用状况。此外，我们还证明了DEC可以实现强类型的SR过程，以确保数学模型的数学一致性，并减少符号表达的搜索空间。最后，我们证明了我们的方法效果，通过从合成实验数据中重新发现波松方程、欧拉-埃拉斯特拉方程和线性弹性方程。由于这些方程的通用性，我们的方法可以应用于多种物理模型化的 Context。
</details></li>
</ul>
<hr>
<h2 id="XAI-for-Early-Crop-Classification"><a href="#XAI-for-Early-Crop-Classification" class="headerlink" title="XAI for Early Crop Classification"></a>XAI for Early Crop Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06574">http://arxiv.org/abs/2310.06574</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ayshah Chan, Maja Schneider, Marco Körner</li>
<li>for: 预测早期农作物种类通过可解释AI方法</li>
<li>methods: 使用层次 relevance propagation (LRP) 方法确定重要时间步骤，并选择一个精选的时间范围来构成最短的分类时间范围</li>
<li>results: 确定的时间范围为2019年4月21日至2019年8月9日，与全时序相比只失去0.75%的准确率，而且LRP得到的重要时间步骤也揭示了输入值中的小 Details，这些Details可以用来区分不同的类别。<details>
<summary>Abstract</summary>
We propose an approach for early crop classification through identifying important timesteps with eXplainable AI (XAI) methods. Our approach consists of training a baseline crop classification model to carry out layer-wise relevance propagation (LRP) so that the salient time step can be identified. We chose a selected number of such important time indices to create the bounding region of the shortest possible classification timeframe. We identified the period 21st April 2019 to 9th August 2019 as having the best trade-off in terms of accuracy and earliness. This timeframe only suffers a 0.75% loss in accuracy as compared to using the full timeseries. We observed that the LRP-derived important timesteps also highlight small details in input values that differentiates between different classes and
</details>
<details>
<summary>摘要</summary>
我们提出了一种采用可解释AI（XAI）方法进行早期作物分类的方法。我们的方法包括训练一个基eline作物分类模型，并使用层wise relevance propagation（LRP）来确定重要的时间步骤。我们选择了一些重要的时间索引，并将其用于创建最短的可能的分类时间范围。我们确定的时间范围为2019年4月21日至2019年8月9日，这个时间范围只减少了0.75%的准确率，相比使用完整时间序列。我们发现LRP得到的重要时间步骤还高亮了输入值中的小 Details，这些细节可以用于分类不同类型的作物。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-reconstruction-with-uncertainty-estimation-for-γ-photon-interaction-in-fast-scintillator-detectors"><a href="#Deep-Learning-reconstruction-with-uncertainty-estimation-for-γ-photon-interaction-in-fast-scintillator-detectors" class="headerlink" title="Deep Learning reconstruction with uncertainty estimation for $γ$ photon interaction in fast scintillator detectors"></a>Deep Learning reconstruction with uncertainty estimation for $γ$ photon interaction in fast scintillator detectors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06572">http://arxiv.org/abs/2310.06572</a></li>
<li>repo_url: None</li>
<li>paper_authors: Geoffrey Daniel, Mohamed Bahi Yahiaoui, Claude Comtat, Sebastien Jan, Olga Kochebina, Jean-Marc Martinez, Viktoriya Sergeyeva, Viatcheslav Sharyy, Chi-Hsun Sung, Dominique Yvon</li>
<li>for: 这篇论文旨在提出一种基于物理学习的gamma射频辐射量测量方法，用于 Positron Emission Tomography（PET）成像。</li>
<li>methods: 该方法使用Density Neural Networkapproach来估算PbWO4固体探测器中gamma射频辐射的二维坐标。我们定义了自定义损失函数，以估算恢复过程中的内在不确定性和探测器的物理约束。</li>
<li>results: 研究结果表明该方法的有效性和可靠性，并且强调了估算结果的不确定性的重要性。我们还讨论了该方法在PET成像质量提高方面的潜在影响和如何使用结果来改进模型和应用中的表现。此外，我们还指出该方法可以扩展到其他应用场景以外。<details>
<summary>Abstract</summary>
This article presents a physics-informed deep learning method for the quantitative estimation of the spatial coordinates of gamma interactions within a monolithic scintillator, with a focus on Positron Emission Tomography (PET) imaging. A Density Neural Network approach is designed to estimate the 2-dimensional gamma photon interaction coordinates in a fast lead tungstate (PbWO4) monolithic scintillator detector. We introduce a custom loss function to estimate the inherent uncertainties associated with the reconstruction process and to incorporate the physical constraints of the detector.   This unique combination allows for more robust and reliable position estimations and the obtained results demonstrate the effectiveness of the proposed approach and highlights the significant benefits of the uncertainties estimation. We discuss its potential impact on improving PET imaging quality and show how the results can be used to improve the exploitation of the model, to bring benefits to the application and how to evaluate the validity of the given prediction and the associated uncertainties. Importantly, our proposed methodology extends beyond this specific use case, as it can be generalized to other applications beyond PET imaging.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:这篇文章介绍了一种基于物理学的深度学习方法，用于量化gamma交互的空间坐标 within a monolithic scintillator detector，尤其是Positron Emission Tomography（PET）成像。该方法使用了Density Neural Network来估算gamma photon交互的2维坐标在fast lead tungstate（PbWO4）monolithic scintillator detector中。我们引入了一个自定义损失函数，以估算重建过程中的自然不确定性和仪器的物理约束。这种独特的组合使得位置估算更加稳定和可靠，并且实际结果证明了我们的提议的有效性，并强调了估算不确定性的重要性。这种方法有可能改善PET成像质量，并且可以扩展到其他 beyond PET成像的应用。
</details></li>
</ul>
<hr>
<h2 id="Statistical-properties-and-privacy-guarantees-of-an-original-distance-based-fully-synthetic-data-generation-method"><a href="#Statistical-properties-and-privacy-guarantees-of-an-original-distance-based-fully-synthetic-data-generation-method" class="headerlink" title="Statistical properties and privacy guarantees of an original distance-based fully synthetic data generation method"></a>Statistical properties and privacy guarantees of an original distance-based fully synthetic data generation method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06571">http://arxiv.org/abs/2310.06571</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rémy Chapelle, Bruno Falissard</li>
<li>for: 这种研究旨在开发一种基于分类和回归树的数据生成框架，以及一种原始距离基于过滤方法，以保护人类参与者数据隐私。</li>
<li>methods: 这种数据生成框架包括四个步骤，每个步骤都是设计来防止特定的隐私泄露风险。研究人员使用了两个或更多的步骤来应用于一个丰富的生物学调查数据集，并计算了隐私和用用度指标。</li>
<li>results: 计算的指标表明，使用全部框架时，每个假数据集都具有了满意的隐私保护水平，特别是对于特性泄露攻击。成员泄露攻击被正式防止，而无需重大改变数据。机器学习方法显示，对于模拟的单个化和链接攻击，成功率很低。各数据集的分布和推论指标与原始数据相似。<details>
<summary>Abstract</summary>
Introduction: The amount of data generated by original research is growing exponentially. Publicly releasing them is recommended to comply with the Open Science principles. However, data collected from human participants cannot be released as-is without raising privacy concerns. Fully synthetic data represent a promising answer to this challenge. This approach is explored by the French Centre de Recherche en {\'E}pid{\'e}miologie et Sant{\'e} des Populations in the form of a synthetic data generation framework based on Classification and Regression Trees and an original distance-based filtering. The goal of this work was to develop a refined version of this framework and to assess its risk-utility profile with empirical and formal tools, including novel ones developed for the purpose of this evaluation.Materials and Methods: Our synthesis framework consists of four successive steps, each of which is designed to prevent specific risks of disclosure. We assessed its performance by applying two or more of these steps to a rich epidemiological dataset. Privacy and utility metrics were computed for each of the resulting synthetic datasets, which were further assessed using machine learning approaches.Results: Computed metrics showed a satisfactory level of protection against attribute disclosure attacks for each synthetic dataset, especially when the full framework was used. Membership disclosure attacks were formally prevented without significantly altering the data. Machine learning approaches showed a low risk of success for simulated singling out and linkability attacks. Distributional and inferential similarity with the original data were high with all datasets.Discussion: This work showed the technical feasibility of generating publicly releasable synthetic data using a multi-step framework. Formal and empirical tools specifically developed for this demonstration are a valuable contribution to this field. Further research should focus on the extension and validation of these tools, in an effort to specify the intrinsic qualities of alternative data synthesis methods.Conclusion: By successfully assessing the quality of data produced using a novel multi-step synthetic data generation framework, we showed the technical and conceptual soundness of the Open-CESP initiative, which seems ripe for full-scale implementation.
</details>
<details>
<summary>摘要</summary>
引言：原始研究数据的数量正在急剧增长。按照开放科学原则，公共发布这些数据是建议的。然而，从人类参与者收集的数据不能直接发布，否则会引起隐私问题。完全 sintética 数据表示一种有 Promise的解决方案。法国中央研究所在这种 sintética 数据生成框架基于分类和回归树和一种原始的距离基于筛选。该工作的目的是开发一个改进版的这种框架，并通过实验和正式工具评估其风险利用性。材料和方法：我们的 sintesis 框架由四个阶段组成，每个阶段都是为预防特定风险的披露。我们使用两个或更多的这些阶段来处理一个丰富的 epidemiological 数据集。隐私和利用度指标在每个 sintetic 数据集中计算，并使用机器学习方法进行评估。结果：计算的指标表明，使用全部框架时，每个 sintetic 数据集的隐私保护水平很高，特别是对于特征披露攻击。成员披露攻击得到了正式防范，而不是对数据造成重要的变化。机器学习方法表示，在模拟的单个化和链接攻击中， sintetic 数据集的风险很低。 distribución 和推论上的相似性很高，所有的 sintetic 数据集都具有高度的相似性。讨论：这项工作证明了使用多步 sintetic 数据生成框架的技术可行性。为此，我们开发了特有的 formal 和实验工具，这些工具对这一领域做出了重要贡献。未来的研究应该集中在这些工具的扩展和验证上，以确定其他数据生成方法的内在特质。结论：通过成功评估使用多步 sintetic 数据生成框架生成的数据质量，我们证明了开放-CESP INITIATIVE 的技术和概念合理性。这一initiative 似乎准备好进行大规模实施。
</details></li>
</ul>
<hr>
<h2 id="An-Edge-Aware-Graph-Autoencoder-Trained-on-Scale-Imbalanced-Data-for-Travelling-Salesman-Problems"><a href="#An-Edge-Aware-Graph-Autoencoder-Trained-on-Scale-Imbalanced-Data-for-Travelling-Salesman-Problems" class="headerlink" title="An Edge-Aware Graph Autoencoder Trained on Scale-Imbalanced Data for Travelling Salesman Problems"></a>An Edge-Aware Graph Autoencoder Trained on Scale-Imbalanced Data for Travelling Salesman Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06543">http://arxiv.org/abs/2310.06543</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiqing Liu, Xueming Yan, Yaochu Jin</li>
<li>for: 解决各种城市数量的旅行商问题 (TSP)</li>
<li>methods: 使用图像学自动编码器（EdgeGAE）模型，通过学习解决方案数据中的链接预测任务来学习解决TSP问题。</li>
<li>results: 对50,000个TSP实例进行了实验，并证明了该方法可以在不同的规模下达到高度竞争力的性能。<details>
<summary>Abstract</summary>
Recent years have witnessed a surge in research on machine learning for combinatorial optimization since learning-based approaches can outperform traditional heuristics and approximate exact solvers at a lower computation cost. However, most existing work on supervised neural combinatorial optimization focuses on TSP instances with a fixed number of cities and requires large amounts of training samples to achieve a good performance, making them less practical to be applied to realistic optimization scenarios. This work aims to develop a data-driven graph representation learning method for solving travelling salesman problems (TSPs) with various numbers of cities. To this end, we propose an edge-aware graph autoencoder (EdgeGAE) model that can learn to solve TSPs after being trained on solution data of various sizes with an imbalanced distribution. We formulate the TSP as a link prediction task on sparse connected graphs. A residual gated encoder is trained to learn latent edge embeddings, followed by an edge-centered decoder to output link predictions in an end-to-end manner. To improve the model's generalization capability of solving large-scale problems, we introduce an active sampling strategy into the training process. In addition, we generate a benchmark dataset containing 50,000 TSP instances with a size from 50 to 500 cities, following an extremely scale-imbalanced distribution, making it ideal for investigating the model's performance for practical applications. We conduct experiments using different amounts of training data with various scales, and the experimental results demonstrate that the proposed data-driven approach achieves a highly competitive performance among state-of-the-art learning-based methods for solving TSPs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Data-level-hybrid-strategy-selection-for-disk-fault-prediction-model-based-on-multivariate-GAN"><a href="#Data-level-hybrid-strategy-selection-for-disk-fault-prediction-model-based-on-multivariate-GAN" class="headerlink" title="Data-level hybrid strategy selection for disk fault prediction model based on multivariate GAN"></a>Data-level hybrid strategy selection for disk fault prediction model based on multivariate GAN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06537">http://arxiv.org/abs/2310.06537</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuangshuang Yuan, Peng Wu, Yuehui Chen</li>
<li>for: 本研究旨在解决数据类别不均的分类问题，特别是对于硬盘健康状况识别任务中的数据类别不均问题。</li>
<li>methods: 本研究使用了多变量生成 adversarial networks (GAN) 生成数据，并通过混合和融合这些数据来协调硬盘 SMART 数据集，以达到数据层次的均衡。同时，使用了遗传算法来提高硬盘缺陷分类预测精度。</li>
<li>results: 研究表明，通过使用 GAN 生成数据和遗传算法，可以提高硬盘缺陷分类预测精度，并且可以更好地处理数据类别不均问题。<details>
<summary>Abstract</summary>
Data class imbalance is a common problem in classification problems, where minority class samples are often more important and more costly to misclassify in a classification task. Therefore, it is very important to solve the data class imbalance classification problem. The SMART dataset exhibits an evident class imbalance, comprising a substantial quantity of healthy samples and a comparatively limited number of defective samples. This dataset serves as a reliable indicator of the disc's health status. In this paper, we obtain the best balanced disk SMART dataset for a specific classification model by mixing and integrating the data synthesised by multivariate generative adversarial networks (GAN) to balance the disk SMART dataset at the data level; and combine it with genetic algorithms to obtain higher disk fault classification prediction accuracy on a specific classification model.
</details>
<details>
<summary>摘要</summary>
数据类别不匹配是常见的分类问题，其中少数类样本经常更重要和更昂贵的错误分类。因此，解决数据类别不匹配分类问题非常重要。SMART数据集显示了明显的类别不匹配，包括大量的健康样本和相对较少的缺陷样本。这个数据集作为磁盘健康状况的可靠指标。在这篇论文中，我们通过将多变量生成对抗网络（GAN）生成的数据混合和 интегра，在数据层面减少磁盘SMART数据集的类别不匹配;并将生成遗传算法与特定分类模型结合，以提高磁盘缺陷分类预测精度。
</details></li>
</ul>
<hr>
<h2 id="Disk-failure-prediction-based-on-multi-layer-domain-adaptive-learning"><a href="#Disk-failure-prediction-based-on-multi-layer-domain-adaptive-learning" class="headerlink" title="Disk failure prediction based on multi-layer domain adaptive learning"></a>Disk failure prediction based on multi-layer domain adaptive learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06534">http://arxiv.org/abs/2310.06534</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guangfu Gao, Peng Wu, Hussain Dawood</li>
<li>for: 预测磁盘失败</li>
<li>methods: 利用多层适应学技术进行预测</li>
<li>results: 提高预测磁盘失败的能力， especialy for disk data with few failure samples.Here’s a more detailed explanation of each point:</li>
<li>for: The paper is written for predicting disk failures, which is an important task in large-scale data storage systems.</li>
<li>methods: The paper proposes a novel method for predicting disk failures by leveraging multi-layer domain adaptive learning techniques. This method involves selecting disk data with numerous faults as the source domain and disk data with fewer faults as the target domain, and training a feature extraction network with the selected origin and destination domains.</li>
<li>results: The proposed technique is demonstrated to be effective in generating a reliable prediction model and improving the ability to predict failures on disk data with few failure samples.<details>
<summary>Abstract</summary>
Large scale data storage is susceptible to failure. As disks are damaged and replaced, traditional machine learning models, which rely on historical data to make predictions, struggle to accurately predict disk failures. This paper presents a novel method for predicting disk failures by leveraging multi-layer domain adaptive learning techniques. First, disk data with numerous faults is selected as the source domain, and disk data with fewer faults is selected as the target domain. A training of the feature extraction network is performed with the selected origin and destination domains. The contrast between the two domains facilitates the transfer of diagnostic knowledge from the domain of source and target. According to the experimental findings, it has been demonstrated that the proposed technique can generate a reliable prediction model and improve the ability to predict failures on disk data with few failure samples.
</details>
<details>
<summary>摘要</summary>
Note: Simplified Chinese is also known as "简化字" or "简化字".Please note that the translation is done using Google Translate and it may not be perfect. Also, the translation may not be exactly the same as the original text, as some words or phrases may not have direct translations in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="AttributionLab-Faithfulness-of-Feature-Attribution-Under-Controllable-Environments"><a href="#AttributionLab-Faithfulness-of-Feature-Attribution-Under-Controllable-Environments" class="headerlink" title="AttributionLab: Faithfulness of Feature Attribution Under Controllable Environments"></a>AttributionLab: Faithfulness of Feature Attribution Under Controllable Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06514">http://arxiv.org/abs/2310.06514</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yang Zhang, Yawei Li, Hannah Brown, Mina Rezaei, Bernd Bischl, Philip Torr, Ashkan Khakzar, Kenji Kawaguchi</li>
<li>for: 本研究旨在检验feature attribution的可靠性，以确保 Attribute Lab 中的结果是否准确 Reflects the neural network’s learning process.</li>
<li>methods: 本研究使用了手动设置 neural network 的 weights 和数据设计，以确定哪些输入特征对 label 有影响，并用这些设计的ground truth特征来评估 attribute 方法的准确性。</li>
<li>results: 研究发现，在 AttributionLab 中设计的 synthetic environment 中，使用了手动设置的 neural network 和数据可以准确 Reflects the neural network’s learning process，并且可以用这种方法来检验 attribute 方法的准确性。<details>
<summary>Abstract</summary>
Feature attribution explains neural network outputs by identifying relevant input features. How do we know if the identified features are indeed relevant to the network? This notion is referred to as faithfulness, an essential property that reflects the alignment between the identified (attributed) features and the features used by the model. One recent trend to test faithfulness is to design the data such that we know which input features are relevant to the label and then train a model on the designed data. Subsequently, the identified features are evaluated by comparing them with these designed ground truth features. However, this idea has the underlying assumption that the neural network learns to use all and only these designed features, while there is no guarantee that the learning process trains the network in this way. In this paper, we solve this missing link by explicitly designing the neural network by manually setting its weights, along with designing data, so we know precisely which input features in the dataset are relevant to the designed network. Thus, we can test faithfulness in AttributionLab, our designed synthetic environment, which serves as a sanity check and is effective in filtering out attribution methods. If an attribution method is not faithful in a simple controlled environment, it can be unreliable in more complex scenarios. Furthermore, the AttributionLab environment serves as a laboratory for controlled experiments through which we can study feature attribution methods, identify issues, and suggest potential improvements.
</details>
<details>
<summary>摘要</summary>
Feature 归属解释 neural network 输出sBy identifying relevant input features。如何确定这些标识的特征是 neural network 中用到的？这个概念被称为 faithfulness，它是一种重要的性质，它反映了模型中使用的特征与归属特征之间的对应关系。一种最近的趋势是通过设计数据来测试 faithfulness，即在训练模型时，知道哪些输入特征与标签之间存在关系，然后在这些设计的真实特征上训练模型。然而，这个想法假设模型学习所有设计的特征，而这并不一定是真实的。在这篇论文中，我们解决了这个缺失的联系。我们明确地设计了 neural network 的权重，并与数据一起设计，因此我们知道哪些数据集中的输入特征与我们设计的网络中用到的特征之间存在关系。因此，我们可以在 AttributionLab 中测试 faithfulness，这是我们自己设计的人工环境，它作为一种 santity check 有效地筛选出归属方法。如果归属方法不忠实在这种简单控制的环境中，那么它在更复杂的场景中可能不可靠。此外，AttributionLab 环境还可以作为一个 controlled experiments 的实验室，我们可以通过这里进行学习归属方法、发现问题和提出改进建议。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Dataset-Distillation-for-Transfer-Learning"><a href="#Self-Supervised-Dataset-Distillation-for-Transfer-Learning" class="headerlink" title="Self-Supervised Dataset Distillation for Transfer Learning"></a>Self-Supervised Dataset Distillation for Transfer Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06511">http://arxiv.org/abs/2310.06511</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dong Bok Lee, Seanie Lee, Joonho Ko, Kenji Kawaguchi, Juho Lee, Sung Ju Hwang</li>
<li>for: 提出了一种新的问题，即将无标注数据集概要压缩到一小型的自我超vised学习（SSL）数据集中。</li>
<li>methods: 提议使用梯度下降法来优化模型的表示，并在内部目标中使用mean squared error（MSE）来避免随机性。</li>
<li>results: 通过实验 validate了方法的有效性，并且可以降低计算成本和获得关键的kernel ridge regression解。<details>
<summary>Abstract</summary>
Dataset distillation methods have achieved remarkable success in distilling a large dataset into a small set of representative samples. However, they are not designed to produce a distilled dataset that can be effectively used for facilitating self-supervised pre-training. To this end, we propose a novel problem of distilling an unlabeled dataset into a set of small synthetic samples for efficient self-supervised learning (SSL). We first prove that a gradient of synthetic samples with respect to a SSL objective in naive bilevel optimization is \textit{biased} due to the randomness originating from data augmentations or masking. To address this issue, we propose to minimize the mean squared error (MSE) between a model's representations of the synthetic examples and their corresponding learnable target feature representations for the inner objective, which does not introduce any randomness. Our primary motivation is that the model obtained by the proposed inner optimization can mimic the \textit{self-supervised target model}. To achieve this, we also introduce the MSE between representations of the inner model and the self-supervised target model on the original full dataset for outer optimization. Lastly, assuming that a feature extractor is fixed, we only optimize a linear head on top of the feature extractor, which allows us to reduce the computational cost and obtain a closed-form solution of the head with kernel ridge regression. We empirically validate the effectiveness of our method on various applications involving transfer learning.
</details>
<details>
<summary>摘要</summary>
dataset 简化方法已经取得了很大的成功，将大量数据简化成一小集 representative samples。然而，它们并不是为生成可以有效地用于自动学习的简化数据集设计的。为此，我们提出了一个新的问题：简化一个没有标签的数据集成一小组小样本，以便高效地进行自动学习（SSL）。我们首先证明了在随机数据扩充或masking中引入的随机性导致的 SSL 目标函数在 naive bilevel 优化中的梯度是偏移的。为解决这个问题，我们提议将内部目标函数设置为 mean squared error（MSE），这样不会引入随机性。我们的主要动机是希望通过提议的内部优化来模仿自动学习目标模型。为此，我们还引入了 MSE  между representations of the inner model 和 self-supervised target model 在原始全 dataset 上，用于外部优化。最后，我们假设了一个固定的 feature extractor，只有在 feature extractor 上进行 linear head 的优化，这使得我们可以降低计算成本并获得一个关于 kernel ridge regression 的闭合型解。我们实际验证了我们的方法在不同应用中的转移学习中的效果。
</details></li>
</ul>
<hr>
<h2 id="Runway-Sign-Classifier-A-DAL-C-Certifiable-Machine-Learning-System"><a href="#Runway-Sign-Classifier-A-DAL-C-Certifiable-Machine-Learning-System" class="headerlink" title="Runway Sign Classifier: A DAL C Certifiable Machine Learning System"></a>Runway Sign Classifier: A DAL C Certifiable Machine Learning System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06506">http://arxiv.org/abs/2310.06506</a></li>
<li>repo_url: None</li>
<li>paper_authors: Konstantin Dmitriev, Johann Schumann, Islam Bostanov, Mostafa Abdelhamid, Florian Holzapfel</li>
<li>For: This paper aims to address the certification challenges of Machine Learning (ML) based systems for medium criticality airborne applications.* Methods: The authors use a Deep Neural Network (DNN) for airport sign detection and classification, and employ an established architectural mitigation technique involving two redundant and dissimilar DNNs. They also use novel ML-specific data management techniques to enhance this approach.* Results: The authors demonstrate compliance with Design Assurance Level (DAL) C, which is a more stringent requirement than their previous work that achieved DAL D.<details>
<summary>Abstract</summary>
In recent years, the remarkable progress of Machine Learning (ML) technologies within the domain of Artificial Intelligence (AI) systems has presented unprecedented opportunities for the aviation industry, paving the way for further advancements in automation, including the potential for single pilot or fully autonomous operation of large commercial airplanes. However, ML technology faces major incompatibilities with existing airborne certification standards, such as ML model traceability and explainability issues or the inadequacy of traditional coverage metrics. Certification of ML-based airborne systems using current standards is problematic due to these challenges. This paper presents a case study of an airborne system utilizing a Deep Neural Network (DNN) for airport sign detection and classification. Building upon our previous work, which demonstrates compliance with Design Assurance Level (DAL) D, we upgrade the system to meet the more stringent requirements of Design Assurance Level C. To achieve DAL C, we employ an established architectural mitigation technique involving two redundant and dissimilar Deep Neural Networks. The application of novel ML-specific data management techniques further enhances this approach. This work is intended to illustrate how the certification challenges of ML-based systems can be addressed for medium criticality airborne applications.
</details>
<details>
<summary>摘要</summary>
This paper presents a case study of an airborne system that utilizes a Deep Neural Network (DNN) for airport sign detection and classification. Building on our previous work, which demonstrated compliance with Design Assurance Level (DAL) D, we upgraded the system to meet the more stringent requirements of DAL C. To achieve DAL C, we employed an established architectural mitigation technique involving two redundant and dissimilar DNNs. Additionally, we applied novel ML-specific data management techniques to enhance this approach.The purpose of this work is to demonstrate how the certification challenges of ML-based systems can be addressed for medium criticality airborne applications. By upgrading the system to meet DAL C requirements, we were able to demonstrate the feasibility of certifying ML-based airborne systems for use in the aviation industry.
</details></li>
</ul>
<hr>
<h2 id="Variance-Reduced-Online-Gradient-Descent-for-Kernelized-Pairwise-Learning-with-Limited-Memory"><a href="#Variance-Reduced-Online-Gradient-Descent-for-Kernelized-Pairwise-Learning-with-Limited-Memory" class="headerlink" title="Variance Reduced Online Gradient Descent for Kernelized Pairwise Learning with Limited Memory"></a>Variance Reduced Online Gradient Descent for Kernelized Pairwise Learning with Limited Memory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06483">http://arxiv.org/abs/2310.06483</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/halquabeh/acml-2023-fpogd-code">https://github.com/halquabeh/acml-2023-fpogd-code</a></li>
<li>paper_authors: Hilal AlQuabeh, Bhaskar Mukhoty, Bin Gu</li>
<li>for: 这篇论文主要写于对在线对照学习中进行对拼式学习的问题上。</li>
<li>methods: 这篇论文提出了一种基于内存限制的在线对照学习算法，该算法可以扩展到内核在线对照学习，并提高了下降 regret。特别是，我们建立了在线对照学习中 gradient 的方差和回归关系，并使用最近的 stratified 样本buffer 大小为 $s$ 来计算在线对照学习的 gradient，其复杂度为 $O(sT)$。</li>
<li>results: 我们的 теоретиче研究表明，在线对照学习中使用方差减少的 gradient 会导致下降 regret 的改进 bound。实验结果表明，我们的算法在实际数据上比 both kernelized 和 linear 在线对照学习算法更有优势。<details>
<summary>Abstract</summary>
Pairwise learning is essential in machine learning, especially for problems involving loss functions defined on pairs of training examples. Online gradient descent (OGD) algorithms have been proposed to handle online pairwise learning, where data arrives sequentially. However, the pairwise nature of the problem makes scalability challenging, as the gradient computation for a new sample involves all past samples. Recent advancements in OGD algorithms have aimed to reduce the complexity of calculating online gradients, achieving complexities less than $O(T)$ and even as low as $O(1)$. However, these approaches are primarily limited to linear models and have induced variance. In this study, we propose a limited memory OGD algorithm that extends to kernel online pairwise learning while improving the sublinear regret. Specifically, we establish a clear connection between the variance of online gradients and the regret, and construct online gradients using the most recent stratified samples with a limited buffer of size of $s$ representing all past data, which have a complexity of $O(sT)$ and employs $O(\sqrt{T}\log{T})$ random Fourier features for kernel approximation. Importantly, our theoretical results demonstrate that the variance-reduced online gradients lead to an improved sublinear regret bound. The experiments on real-world datasets demonstrate the superiority of our algorithm over both kernelized and linear online pairwise learning algorithms.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换给定文本到简化中文。<</SYS>>在机器学习中，对于基于对例学习的问题，对例学习是非常重要的。在线 gradient descent（OGD）算法已经提出来处理在线对例学习，数据顺序到达时进行学习。然而，对例性问题的特点使得扩展性困难，因为新的样本计算gradient时需要所有过去的样本。 latest advances in OGD algorithms have aimed to reduce the complexity of calculating online gradients, achieving complexities less than $O(T)$ and even as low as $O(1)$. However, these approaches are primarily limited to linear models and have induced variance.在这种研究中，我们提出了有限内存OGD算法，扩展到内核在线对例学习，改进了下界 regret。具体来说，我们确定在线 gradients的方差和 regret之间的关系，并使用最近的降序排序样本buffer的大小为$s$，表示所有过去的数据，其复杂度为$O(sT)$。此外，我们还使用$O(\sqrt{T}\log{T})$个随机傅立叶特征来近似内核。我们的理论结果表明，减少方差的在线 gradients会导致改进的下界 regret bound。实验表明，我们的算法在实际 dataset 上比 both kernelized 和 linear online pairwise learning algorithms 高效。
</details></li>
</ul>
<hr>
<h2 id="An-improved-CTGAN-for-data-processing-method-of-imbalanced-disk-failure"><a href="#An-improved-CTGAN-for-data-processing-method-of-imbalanced-disk-failure" class="headerlink" title="An improved CTGAN for data processing method of imbalanced disk failure"></a>An improved CTGAN for data processing method of imbalanced disk failure</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06481">http://arxiv.org/abs/2310.06481</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingbo Jia, Peng Wu, Hussain Dawood</li>
<li>for:  solves the problem of insufficient failure data and imbalance between normal and failure data in disk failure diagnosis.</li>
<li>methods:  uses an improved Conditional Tabular Generative Adversarial Networks (CTGAN) with a residual network and a classifier for specific category discrimination, as well as a discriminator based on residual network.</li>
<li>results:  the synthesized data can further improve the fault diagnosis accuracy of the classifier, as demonstrated by the experimental results.Here is the text in Simplified Chinese:</li>
<li>for: 解决磁盘故障诊断中缺乏磁盘故障数据和正常数据异常分布的问题。</li>
<li>methods: 利用改进后的Conditional Tabular Generative Adversarial Networks (CTGAN)，加入分类器进行特定类别划分，以及基于差异网络的探测器。</li>
<li>results: 通过实验结果表明，使用RCTGAN生成的数据可以进一步提高磁盘故障诊断精度。<details>
<summary>Abstract</summary>
To address the problem of insufficient failure data generated by disks and the imbalance between the number of normal and failure data. The existing Conditional Tabular Generative Adversarial Networks (CTGAN) deep learning methods have been proven to be effective in solving imbalance disk failure data. But CTGAN cannot learn the internal information of disk failure data very well. In this paper, a fault diagnosis method based on improved CTGAN, a classifier for specific category discrimination is added and a discriminator generate adversarial network based on residual network is proposed. We named it Residual Conditional Tabular Generative Adversarial Networks (RCTGAN). Firstly, to enhance the stability of system a residual network is utilized. RCTGAN uses a small amount of real failure data to synthesize fake fault data; Then, the synthesized data is mixed with the real data to balance the amount of normal and failure data; Finally, four classifier (multilayer perceptron, support vector machine, decision tree, random forest) models are trained using the balanced data set, and the performance of the models is evaluated using G-mean. The experimental results show that the data synthesized by the RCTGAN can further improve the fault diagnosis accuracy of the classifier.
</details>
<details>
<summary>摘要</summary>
Firstly, to enhance the stability of the system, a residual network is utilized. RCTGAN uses a small amount of real failure data to synthesize fake fault data, then the synthesized data is mixed with the real data to balance the amount of normal and failure data. Finally, four classifier (multilayer perceptron, support vector machine, decision tree, random forest) models are trained using the balanced data set, and the performance of the models is evaluated using G-mean. The experimental results show that the data synthesized by the RCTGAN can further improve the fault diagnosis accuracy of the classifier.
</details></li>
</ul>
<hr>
<h2 id="Asynchronous-Federated-Learning-with-Incentive-Mechanism-Based-on-Contract-Theory"><a href="#Asynchronous-Federated-Learning-with-Incentive-Mechanism-Based-on-Contract-Theory" class="headerlink" title="Asynchronous Federated Learning with Incentive Mechanism Based on Contract Theory"></a>Asynchronous Federated Learning with Incentive Mechanism Based on Contract Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06448">http://arxiv.org/abs/2310.06448</a></li>
<li>repo_url: None</li>
<li>paper_authors: Danni Yang, Yun Ji, Zhoubin Kou, Xiaoxiong Zhong, Sheng Zhang</li>
<li>for: 提高 federated learning 中客户端的参与度和质量，并吸引高质量客户端参与。</li>
<li>methods: 基于合同理论的奖励机制，适应性地调整客户端的本地模型训练epoch数，考虑因素如时间延迟和测试准确率。</li>
<li>results: 在 MNIST  dataset 上进行了实验，测试精度与 FedAvg 和 FedProx 无攻击情况下相比，提高了 3.12% 和 5.84%；相比理想的本地 SGD，在攻击情况下提高了 1.35%。此外，在寻求同目标准确率情况下，我们的框架需要较少的计算时间。<details>
<summary>Abstract</summary>
To address the challenges posed by the heterogeneity inherent in federated learning (FL) and to attract high-quality clients, various incentive mechanisms have been employed. However, existing incentive mechanisms are typically utilized in conventional synchronous aggregation, resulting in significant straggler issues. In this study, we propose a novel asynchronous FL framework that integrates an incentive mechanism based on contract theory. Within the incentive mechanism, we strive to maximize the utility of the task publisher by adaptively adjusting clients' local model training epochs, taking into account factors such as time delay and test accuracy. In the asynchronous scheme, considering client quality, we devise aggregation weights and an access control algorithm to facilitate asynchronous aggregation. Through experiments conducted on the MNIST dataset, the simulation results demonstrate that the test accuracy achieved by our framework is 3.12% and 5.84% higher than that achieved by FedAvg and FedProx without any attacks, respectively. The framework exhibits a 1.35% accuracy improvement over the ideal Local SGD under attacks. Furthermore, aiming for the same target accuracy, our framework demands notably less computation time than both FedAvg and FedProx.
</details>
<details>
<summary>摘要</summary>
在聚合学习（FL）中处理多样性的挑战和吸引高质量客户端的吸引力，各种奖励机制已经被应用。然而，现有的奖励机制通常在同步聚合中使用，导致了显著的延迟问题。在这项研究中，我们提出了一种新的异步FL框架，该框架 integrate了基于合同理论的奖励机制。在奖励机制中，我们尝试以最大化任务发布者的利益为目标，通过调整客户端本地模型训练 epoch，考虑因素如时间延迟和测试准确率。在异步方案中，考虑客户端质量，我们设计了聚合权重和访问控制算法，以便异步聚合。经过在MNIST数据集上进行的实验，实验结果表明，我们的框架测试准确率与FedAvg和FedProx无攻击情况下的测试准确率相差3.12%和5.84%，分别高于FedAvg和FedProx无攻击情况下的测试准确率。此外，我们的框架在攻击情况下与理想的本地SGD准确率之间差不多。此外，在寻求同样的目标准确率情况下，我们的框架需要比FedAvg和FedProx更少的计算时间。
</details></li>
</ul>
<hr>
<h2 id="Rule-Mining-for-Correcting-Classification-Models"><a href="#Rule-Mining-for-Correcting-Classification-Models" class="headerlink" title="Rule Mining for Correcting Classification Models"></a>Rule Mining for Correcting Classification Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06446">http://arxiv.org/abs/2310.06446</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hirofumi Suzuki, Hiroaki Iwashita, Takuya Takagi, Yuta Fujishige, Satoshi Hara</li>
<li>for: This paper is written for developers who need to continually update or correct machine learning models to ensure high prediction accuracy, particularly in complex systems or software.</li>
<li>methods: The paper proposes a correction rule mining approach to acquire a comprehensive list of rules that describe inaccurate subpopulations and how to correct them. The proposed algorithm combines frequent itemset mining and a unique pruning technique for correction rules.</li>
<li>results: The paper found that the proposed algorithm discovered various rules that help collect data insufficiently learned, directly correct model outputs, and analyze concept drift.<details>
<summary>Abstract</summary>
Machine learning models need to be continually updated or corrected to ensure that the prediction accuracy remains consistently high. In this study, we consider scenarios where developers should be careful to change the prediction results by the model correction, such as when the model is part of a complex system or software. In such scenarios, the developers want to control the specification of the corrections. To achieve this, the developers need to understand which subpopulations of the inputs get inaccurate predictions by the model. Therefore, we propose correction rule mining to acquire a comprehensive list of rules that describe inaccurate subpopulations and how to correct them. We also develop an efficient correction rule mining algorithm that is a combination of frequent itemset mining and a unique pruning technique for correction rules. We observed that the proposed algorithm found various rules which help to collect data insufficiently learned, directly correct model outputs, and analyze concept drift.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Deep-reinforcement-learning-uncovers-processes-for-separating-azeotropic-mixtures-without-prior-knowledge"><a href="#Deep-reinforcement-learning-uncovers-processes-for-separating-azeotropic-mixtures-without-prior-knowledge" class="headerlink" title="Deep reinforcement learning uncovers processes for separating azeotropic mixtures without prior knowledge"></a>Deep reinforcement learning uncovers processes for separating azeotropic mixtures without prior knowledge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06415">http://arxiv.org/abs/2310.06415</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/grimmlab/drl4procsyn">https://github.com/grimmlab/drl4procsyn</a></li>
<li>paper_authors: Quirin Göttl, Jonathan Pirnay, Jakob Burger, Dominik G. Grimm</li>
<li>for: 这个论文旨在探讨用深度学习探索器解决化学工程中的流程设计问题。</li>
<li>methods: 该论文使用深度学习探索器，不需要先验知识，可以在各种复杂的计划问题中表现出优秀的性能。</li>
<li>results: 论文通过示例化一种可以自动学习并应用于多种化学系统中的流程设计方法，并且可以将大于99%的材料分离成纯组分。这显示出探索器的计划灵活性和可靠性。<details>
<summary>Abstract</summary>
Process synthesis in chemical engineering is a complex planning problem due to vast search spaces, continuous parameters and the need for generalization. Deep reinforcement learning agents, trained without prior knowledge, have shown to outperform humans in various complex planning problems in recent years. Existing work on reinforcement learning for flowsheet synthesis shows promising concepts, but focuses on narrow problems in a single chemical system, limiting its practicality. We present a general deep reinforcement learning approach for flowsheet synthesis. We demonstrate the adaptability of a single agent to the general task of separating binary azeotropic mixtures. Without prior knowledge, it learns to craft near-optimal flowsheets for multiple chemical systems, considering different feed compositions and conceptual approaches. On average, the agent can separate more than 99% of the involved materials into pure components, while autonomously learning fundamental process engineering paradigms. This highlights the agent's planning flexibility, an encouraging step toward true generality.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Adversarial-Robustness-in-Graph-Neural-Networks-A-Hamiltonian-Approach"><a href="#Adversarial-Robustness-in-Graph-Neural-Networks-A-Hamiltonian-Approach" class="headerlink" title="Adversarial Robustness in Graph Neural Networks: A Hamiltonian Approach"></a>Adversarial Robustness in Graph Neural Networks: A Hamiltonian Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06396">http://arxiv.org/abs/2310.06396</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zknus/neurips-2023-hang-robustness">https://github.com/zknus/neurips-2023-hang-robustness</a></li>
<li>paper_authors: Kai Zhao, Qiyu Kang, Yang Song, Rui She, Sijie Wang, Wee Peng Tay</li>
<li>For: 本研究探讨了基于多种神经流的图神经网络（GNNs）的抗震性能，尤其是它们与不同的稳定性观念相关，如BIBO稳定性、 Lyapunov稳定性、结构稳定性和保守稳定性。* Methods: 本文提出了基于物理原理的保守汉密尔顿神经流，用于构建抗震性能强的GNNs。并进行了多种验证 benchmark datasets 上的 adversarial attacks 下的实验，以评估不同神经流GNNs 的抗震性能。* Results: 实验结果表明，基于保守汉密尔顿神经流的GNNs 在 adversarial attacks 下具有显著的抗震性能，而 Lyapunov稳定性并不一定能 garantate  adversarial robustness。<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) are vulnerable to adversarial perturbations, including those that affect both node features and graph topology. This paper investigates GNNs derived from diverse neural flows, concentrating on their connection to various stability notions such as BIBO stability, Lyapunov stability, structural stability, and conservative stability. We argue that Lyapunov stability, despite its common use, does not necessarily ensure adversarial robustness. Inspired by physics principles, we advocate for the use of conservative Hamiltonian neural flows to construct GNNs that are robust to adversarial attacks. The adversarial robustness of different neural flow GNNs is empirically compared on several benchmark datasets under a variety of adversarial attacks. Extensive numerical experiments demonstrate that GNNs leveraging conservative Hamiltonian flows with Lyapunov stability substantially improve robustness against adversarial perturbations. The implementation code of experiments is available at https://github.com/zknus/NeurIPS-2023-HANG-Robustness.
</details>
<details>
<summary>摘要</summary>
“神经网络（GNNs）对于攻击性变化具有漏洞，包括影响节点特征和GraphTopology。这篇论文研究GNNs从多种神经流中派生出来的不同稳定性概念，特别是BIBO稳定性、Lyapunov稳定性、结构稳定性和保守稳定性。我们认为Lyapunov稳定性，即使常用，并不一定能保证攻击适应性。以物理原理为 inspiration，我们倡议使用保守的Hamiltonian神经流创建GNNs，以提高对于攻击性变化的抗性。不同神经流GNNs的攻击适应性在多个Benchmark数据集上进行了实验性的比较。实验结果显示， leveraging conservative Hamiltonian flows with Lyapunov stability can significantly improve the robustness of GNNs against adversarial attacks。相关实验代码可以在https://github.com/zknus/NeurIPS-2023-HANG-Robustness中找到。”Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Harnessing-Administrative-Data-Inventories-to-Create-a-Reliable-Transnational-Reference-Database-for-Crop-Type-Monitoring"><a href="#Harnessing-Administrative-Data-Inventories-to-Create-a-Reliable-Transnational-Reference-Database-for-Crop-Type-Monitoring" class="headerlink" title="Harnessing Administrative Data Inventories to Create a Reliable Transnational Reference Database for Crop Type Monitoring"></a>Harnessing Administrative Data Inventories to Create a Reliable Transnational Reference Database for Crop Type Monitoring</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06393">http://arxiv.org/abs/2310.06393</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/maja601/eurocrops">https://github.com/maja601/eurocrops</a></li>
<li>paper_authors: Maja Schneider, Marco Körner</li>
<li>for: 本研究的目的是开发新的参 Referenced Dataset for 耕地类型分类。</li>
<li>methods: 本研究使用了 Administrative 数据的集成和融合方法，以实现跨国可比的参 Referenced Dataset。</li>
<li>results: 研究实现了一个名为 E URO C ROPS 的参 Referenced Dataset，可以用于耕地类型分类。<details>
<summary>Abstract</summary>
With leaps in machine learning techniques and their applicationon Earth observation challenges has unlocked unprecedented performance across the domain. While the further development of these methods was previously limited by the availability and volume of sensor data and computing resources, the lack of adequate reference data is now constituting new bottlenecks. Since creating such ground-truth information is an expensive and error-prone task, new ways must be devised to source reliable, high-quality reference data on large scales. As an example, we showcase E URO C ROPS, a reference dataset for crop type classification that aggregates and harmonizes administrative data surveyed in different countries with the goal of transnational interoperability.
</details>
<details>
<summary>摘要</summary>
随着机器学习技术的大跃进和其应用于地球观测挑战，已经实现了无 precedent的表现在这个领域。然而，由于感知器数据和计算资源的可用性的限制，这些方法的进一步发展被限制。现在，由于创建这些基准信息是一项昂贵和容易出错的任务，新的方法需要被设计，以获取可靠、高质量的参 refer 数据。作为一个示例，我们展示了E URO C ROPS，一个用于蔬菜类别分类的参 refer 数据集，该数据集在不同国家surveyed的行政数据的基础上，实现了跨国共享和协调。
</details></li>
</ul>
<hr>
<h2 id="CAST-Cluster-Aware-Self-Training-for-Tabular-Data"><a href="#CAST-Cluster-Aware-Self-Training-for-Tabular-Data" class="headerlink" title="CAST: Cluster-Aware Self-Training for Tabular Data"></a>CAST: Cluster-Aware Self-Training for Tabular Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06380">http://arxiv.org/abs/2310.06380</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minwook Kim, Juseong Kim, Kibeom Kim, Donggil Kang, Giltae Song</li>
<li>for: 提高 tabular 数据上的自我训练性能，并适用于各种自我训练方法和模型结构。</li>
<li>methods: 基于 cluster assumption 的 Cluster-Aware Self-Training (CAST) 方法，通过迁移矩阵的方式对权重进行补做，以提高 pseudo-label 的准确性。</li>
<li>results: 在 20 个真实世界数据集上进行了广泛的 empirical 评估，证明 CAST 方法不仅性能更高，还具有在不同的自我训练场景下的稳定性。<details>
<summary>Abstract</summary>
Self-training has gained attraction because of its simplicity and versatility, yet it is vulnerable to noisy pseudo-labels. Several studies have proposed successful approaches to tackle this issue, but they have diminished the advantages of self-training because they require specific modifications in self-training algorithms or model architectures. Furthermore, most of them are incompatible with gradient boosting decision trees, which dominate the tabular domain. To address this, we revisit the cluster assumption, which states that data samples that are close to each other tend to belong to the same class. Inspired by the assumption, we propose Cluster-Aware Self-Training (CAST) for tabular data. CAST is a simple and universally adaptable approach for enhancing existing self-training algorithms without significant modifications. Concretely, our method regularizes the confidence of the classifier, which represents the value of the pseudo-label, forcing the pseudo-labels in low-density regions to have lower confidence by leveraging prior knowledge for each class within the training data. Extensive empirical evaluations on up to 20 real-world datasets confirm not only the superior performance of CAST but also its robustness in various setups in self-training contexts.
</details>
<details>
<summary>摘要</summary>
自适应学习已经吸引了广泛关注，因为它的简单性和灵活性，但它受到噪声 pseudo-label 的威胁。许多研究已经提出了成功的方法来解决这个问题，但这些方法减少了自适应学习的优势，因为它们需要特定的修改在自适应学习算法或模型结构上。此外，大多数方法与梯度拟合树不兼容，梯度拟合树在标量领域占据主导地位。为解决这个问题，我们回到了均匀分布假设，即数据样本在邻近的情况下往往属于同一个类。受到这个假设的激发，我们提出了 Cluster-Aware Self-Training（CAST），这是一种简单而通用的方法，可以增强现有的自适应学习算法，无需重大修改。具体来说，我们的方法规范了分类器的信任值，即 pseudo-label 的值，使低密度区域的 pseudo-labels 的信任值降低，通过利用每个类在训练数据中的先验知识。我们对 Up to 20 个实际数据集进行了广泛的实证评估，并证明了 CAST 不仅在不同的自适应学习设置中表现出色，而且在各种各样的自适应学习上下文中具有强大的稳定性。
</details></li>
</ul>
<hr>
<h2 id="Initialization-Bias-of-Fourier-Neural-Operator-Revisiting-the-Edge-of-Chaos"><a href="#Initialization-Bias-of-Fourier-Neural-Operator-Revisiting-the-Edge-of-Chaos" class="headerlink" title="Initialization Bias of Fourier Neural Operator: Revisiting the Edge of Chaos"></a>Initialization Bias of Fourier Neural Operator: Revisiting the Edge of Chaos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06379">http://arxiv.org/abs/2310.06379</a></li>
<li>repo_url: None</li>
<li>paper_authors: Takeshi Koshizuka, Masahiro Fujisawa, Yusuke Tanaka, Issei Sato</li>
<li>for: 本研究探讨了傅立叶神经网络（FNO）的初始化偏见问题。</li>
<li>methods: 本文基于mean-field理论分析了FNO的行为，从“边缘混乱”角度探讨了前向和后向传播行为的特点，并发现了模式舒缩引起的特殊行为。</li>
<li>results: 建议一种基于He初始化方案的FNO初始化方法，可以缓解FNO的初始化偏见问题，并且实验表明这种方法可以稳定地训练32层FNO，无需额外技术或显著性能下降。<details>
<summary>Abstract</summary>
This paper investigates the initialization bias of the Fourier neural operator (FNO). A mean-field theory for FNO is established, analyzing the behavior of the random FNO from an ``edge of chaos'' perspective. We uncover that the forward and backward propagation behaviors exhibit characteristics unique to FNO, induced by mode truncation, while also showcasing similarities to those of densely connected networks. Building upon this observation, we also propose a FNO version of the He initialization scheme to mitigate the negative initialization bias leading to training instability. Experimental results demonstrate the effectiveness of our initialization scheme, enabling stable training of a 32-layer FNO without the need for additional techniques or significant performance degradation.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:这篇论文研究了傅立叶神经算法（FNO）的初始化偏见。一种mean-field理论被建立，从“边缘化”的角度分析FNO的行为。研究发现，FNO的前向和反向传播行为具有特有的特征，与紧密连接网络类似，但也受到模式舍入的影响。基于这一观察，我们还提出了一种基于FNO的He初始化方案，以缓解初始化偏见，实现了一个32层FNO的稳定训练，无需额外技术或显著性能下降。
</details></li>
</ul>
<hr>
<h2 id="Partition-based-differentially-private-synthetic-data-generation"><a href="#Partition-based-differentially-private-synthetic-data-generation" class="headerlink" title="Partition-based differentially private synthetic data generation"></a>Partition-based differentially private synthetic data generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06371">http://arxiv.org/abs/2310.06371</a></li>
<li>repo_url: None</li>
<li>paper_authors: Meifan Zhang, Dihang Deng, Lihua Yin</li>
<li>for: 这篇论文是为了解决private synthetic data sharing的问题，以实现数据隐私和数据分享之间的 equilibrio。</li>
<li>methods: 这篇论文使用了一个partition-based方法，可以实现对大型资料集的分割和组合，以减少隐私预算的分配问题。</li>
<li>results: 实验结果显示，这篇论文的方法比以前的方法更好，可以生成高质量的实验数据，并且可以实现更好的隐私保证。<details>
<summary>Abstract</summary>
Private synthetic data sharing is preferred as it keeps the distribution and nuances of original data compared to summary statistics. The state-of-the-art methods adopt a select-measure-generate paradigm, but measuring large domain marginals still results in much error and allocating privacy budget iteratively is still difficult. To address these issues, our method employs a partition-based approach that effectively reduces errors and improves the quality of synthetic data, even with a limited privacy budget. Results from our experiments demonstrate the superiority of our method over existing approaches. The synthetic data produced using our approach exhibits improved quality and utility, making it a preferable choice for private synthetic data sharing.
</details>
<details>
<summary>摘要</summary>
<<SYS>>私有的合成数据分享被 preference 为它可以保持原始数据的分布和特点，而不是仅仅是使用摘要统计。现状的方法采用 select-measure-generate 方法，但测量大型领域边缘仍然导致很大的错误，并且分配隐私预算的迭代仍然困难。为解决这些问题，我们的方法使用分区方法，有效地减少错误并提高合成数据的质量，即使具有有限的隐私预算。我们的实验结果表明我们的方法在现有的方法之上具有明显的优势。合成使用我们的方法生成的数据具有更高的质量和用用，使其成为私有合成数据分享的首选。>>>
</details></li>
</ul>
<hr>
<h2 id="DrugCLIP-Contrastive-Protein-Molecule-Representation-Learning-for-Virtual-Screening"><a href="#DrugCLIP-Contrastive-Protein-Molecule-Representation-Learning-for-Virtual-Screening" class="headerlink" title="DrugCLIP: Contrastive Protein-Molecule Representation Learning for Virtual Screening"></a>DrugCLIP: Contrastive Protein-Molecule Representation Learning for Virtual Screening</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06367">http://arxiv.org/abs/2310.06367</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bowen Gao, Bo Qiang, Haichuan Tan, Minsi Ren, Yinjun Jia, Minsi Lu, Jingjing Liu, Weiying Ma, Yanyan Lan</li>
<li>for: 这篇论文旨在提出一个新的药物探测方法，协助找到适合的药物来与特定蛋白质腔填充结构匹配。</li>
<li>methods: 本研究使用了一个新的对称学习框架，称为DrugCLIP，它以对称学习方法来训练保护蛋白质和药物之间的对应关系，不需要运算丰富的资料点。</li>
<li>results: 实验结果显示，DrugCLIP方法可以对多种虚拟探测任务进行高效的预测，特别是在零shot情况下，并且可以与传统的探测方法和超vised学习方法相比较。<details>
<summary>Abstract</summary>
Virtual screening, which identifies potential drugs from vast compound databases to bind with a particular protein pocket, is a critical step in AI-assisted drug discovery. Traditional docking methods are highly time-consuming, and can only work with a restricted search library in real-life applications. Recent supervised learning approaches using scoring functions for binding-affinity prediction, although promising, have not yet surpassed docking methods due to their strong dependency on limited data with reliable binding-affinity labels. In this paper, we propose a novel contrastive learning framework, DrugCLIP, by reformulating virtual screening as a dense retrieval task and employing contrastive learning to align representations of binding protein pockets and molecules from a large quantity of pairwise data without explicit binding-affinity scores. We also introduce a biological-knowledge inspired data augmentation strategy to learn better protein-molecule representations. Extensive experiments show that DrugCLIP significantly outperforms traditional docking and supervised learning methods on diverse virtual screening benchmarks with highly reduced computation time, especially in zero-shot setting.
</details>
<details>
<summary>摘要</summary>
虚拟屏选，可以从庞大的化合物库中标识可能的药物，是人工智能辅助药物发现的关键步骤。传统的停船方法需要很长时间，并且在实际应用中只能使用有限的搜索库。最近的监督学习方法使用紧密度函数预测绑定能力，虽然有承诺，仍然受到有限数据中可靠绑定能力标签的依赖。在这篇论文中，我们提出了一种新的对比学习框架，药物CLIP，通过将虚拟屏选改为密集检索任务，并使用对比学习对绑定蛋白质和分子的表示进行对齐。我们还提出了基于生物知识的数据增强策略，以更好地学习蛋白质-分子表示。广泛的实验表明，药物CLIP在多种虚拟屏选标准准 benchmark上表现出色，特别是在零shot Setting下。
</details></li>
</ul>
<hr>
<h2 id="Core-Intermediate-Peripheral-Index-Factor-Analysis-of-Neighborhood-and-Shortest-Paths-based-Centrality-Metrics"><a href="#Core-Intermediate-Peripheral-Index-Factor-Analysis-of-Neighborhood-and-Shortest-Paths-based-Centrality-Metrics" class="headerlink" title="Core-Intermediate-Peripheral Index: Factor Analysis of Neighborhood and Shortest Paths-based Centrality Metrics"></a>Core-Intermediate-Peripheral Index: Factor Analysis of Neighborhood and Shortest Paths-based Centrality Metrics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06358">http://arxiv.org/abs/2310.06358</a></li>
<li>repo_url: None</li>
<li>paper_authors: Natarajan Meghanathan</li>
<li>for: 本研究使用因子分析对四大邻居和最短路基于中心性指标（度、 eigenvector、betweeness和邻接）的原始数据进行分析，并提出了一种新的量化指标called Core-Intermediate-Peripheral（CIP）指标，用于衡量节点在网络中扮演核心节点（网络中心点的大小值）与边缘节点（网络边缘点的小值）的角色。</li>
<li>methods: 本研究使用变макс基于Eigenvector的因子分析（varimax-based rotation of the Eigenvectors）对中心性指标数据矩阵的转置矩阵进行分析，假设网络中有两个因素（核心和边缘）对节点的中心性指标值产生影响。</li>
<li>results: 本研究在12种复杂的实际世界网络上测试了该方法，并发现CIP指标可以准确地捕捉节点在网络中的中心性和边缘性，并且可以用于评估网络中不同类型节点的中心性和边缘性。<details>
<summary>Abstract</summary>
We perform factor analysis on the raw data of the four major neighborhood and shortest paths-based centrality metrics (Degree, Eigenvector, Betweeenness and Closeness) and propose a novel quantitative measure called the Core-Intermediate-Peripheral (CIP) Index to capture the extent with which a node could play the role of a core node (nodes at the center of a network with larger values for any centrality metric) vis-a-vis a peripheral node (nodes that exist at the periphery of a network with lower values for any centrality metric). We conduct factor analysis (varimax-based rotation of the Eigenvectors) on the transpose matrix of the raw centrality metrics dataset, with the node ids as features, under the hypothesis that there are two factors (core and peripheral) that drive the values incurred by the nodes with respect to the centrality metrics. We test our approach on a diverse suite of 12 complex real-world networks.
</details>
<details>
<summary>摘要</summary>
我们对Raw数据进行因素分析，并提出一种新的量化指标called Core-Intermediate-Peripheral（CIP）指数，用于捕捉节点是核心节点（网络中心部分的节点，具有大于其他中心指标值的任何指标）与边缘节点（网络边缘部分的节点，具有较低的任何指标值）的角色扮演的程度。我们使用变差-基于Eigenvector的因素分析（varimax-based rotation of the Eigenvectors）对转置矩阵中的中心指标数据进行分析，假设网络中有两个因素（核心和边缘）驱动节点的中心指标值。我们对12种不同的实际世界网络进行测试。
</details></li>
</ul>
<hr>
<h2 id="Boosting-Continuous-Control-with-Consistency-Policy"><a href="#Boosting-Continuous-Control-with-Consistency-Policy" class="headerlink" title="Boosting Continuous Control with Consistency Policy"></a>Boosting Continuous Control with Consistency Policy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06343">http://arxiv.org/abs/2310.06343</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuhui Chen, Haoran Li, Dongbin Zhao</li>
<li>for: 提高offline reinforcement learning的效率和精度</li>
<li>methods: 基于强度学习的扩散模型，并用Q学习来更新扩散模型的策略</li>
<li>results: 实验结果显示，CPQL可以快速地改进策略，并且在11个Offline任务和21个Online任务中达到了新的状态纪录，提高了推理速度，相比Diffusion-QL，CPQL的推理速度提高了约45倍。<details>
<summary>Abstract</summary>
Due to its training stability and strong expression, the diffusion model has attracted considerable attention in offline reinforcement learning. However, several challenges have also come with it: 1) The demand for a large number of diffusion steps makes the diffusion-model-based methods time inefficient and limits their applications in real-time control; 2) How to achieve policy improvement with accurate guidance for diffusion model-based policy is still an open problem. Inspired by the consistency model, we propose a novel time-efficiency method named Consistency Policy with Q-Learning (CPQL), which derives action from noise by a single step. By establishing a mapping from the reverse diffusion trajectories to the desired policy, we simultaneously address the issues of time efficiency and inaccurate guidance when updating diffusion model-based policy with the learned Q-function. We demonstrate that CPQL can achieve policy improvement with accurate guidance for offline reinforcement learning, and can be seamlessly extended for online RL tasks. Experimental results indicate that CPQL achieves new state-of-the-art performance on 11 offline and 21 online tasks, significantly improving inference speed by nearly 45 times compared to Diffusion-QL. We will release our code later.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>The demand for a large number of diffusion steps makes the diffusion-model-based methods time-inefficient and limits their applications in real-time control.2. How to achieve policy improvement with accurate guidance for diffusion model-based policy is still an open problem.Inspired by the consistency model, we propose a novel time-efficiency method named Consistency Policy with Q-Learning (CPQL), which derives action from noise by a single step. By establishing a mapping from the reverse diffusion trajectories to the desired policy, we simultaneously address the issues of time efficiency and inaccurate guidance when updating diffusion model-based policy with the learned Q-function.We demonstrate that CPQL can achieve policy improvement with accurate guidance for offline reinforcement learning, and can be seamlessly extended for online RL tasks. Experimental results indicate that CPQL achieves new state-of-the-art performance on 11 offline and 21 online tasks, significantly improving inference speed by nearly 45 times compared to Diffusion-QL. We will release our code later.</details></li>
</ol>
<hr>
<h2 id="Federated-Learning-with-Reduced-Information-Leakage-and-Computation"><a href="#Federated-Learning-with-Reduced-Information-Leakage-and-Computation" class="headerlink" title="Federated Learning with Reduced Information Leakage and Computation"></a>Federated Learning with Reduced Information Leakage and Computation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06341">http://arxiv.org/abs/2310.06341</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tongxin Yin, Xueru Zhang, Mohammad Mahdi Khalili, Mingyan Liu</li>
<li>for: 这 paper 是为了提出一种基于分布式学习的隐私保护机制，以便在多个分布式客户端之间协同学习共同模型，而不需要直接披露本地数据。</li>
<li>methods: 该 paper 使用了一种基于首频采样的方法，其中在每个偶数轮 iteration 中， client 只需要提供一个首频采样，而不需要提供整个数据集。这种方法可以减少了 client 的计算量和隐私泄露。</li>
<li>results: 实验表明，Upcycled-FL 可以在具有不同数据类型的客户端上达到更高的准确率，同时具有更好的隐私保护性和训练时间减少。在 average 的情况下，Upcycled-FL 可以减少 48% 的训练时间。<details>
<summary>Abstract</summary>
Federated learning (FL) is a distributed learning paradigm that allows multiple decentralized clients to collaboratively learn a common model without sharing local data. Although local data is not exposed directly, privacy concerns nonetheless exist as clients' sensitive information can be inferred from intermediate computations. Moreover, such information leakage accumulates substantially over time as the same data is repeatedly used during the iterative learning process. As a result, it can be particularly difficult to balance the privacy-accuracy trade-off when designing privacy-preserving FL algorithms. In this paper, we introduce Upcycled-FL, a novel federated learning framework with first-order approximation applied at every even iteration. Under this framework, half of the FL updates incur no information leakage and require much less computation. We first conduct the theoretical analysis on the convergence (rate) of Upcycled-FL, and then apply perturbation mechanisms to preserve privacy. Experiments on real-world data show that Upcycled-FL consistently outperforms existing methods over heterogeneous data, and significantly improves privacy-accuracy trade-off while reducing 48% of the training time on average.
</details>
<details>
<summary>摘要</summary>
federated learning（FL）是一种分布式学习 paradigma，允许多个分散的客户端共同学习一个共同模型，无需直接分享本地数据。 although local data 不会直接暴露，但是隐私问题仍然存在，因为客户端的敏感信息可以通过中间计算被推断出。此外，这种信息泄露会随着训练过程中的重复使用数据堆积，从而使得在设计隐私保护FL算法时进行平衡隐私精度质量的权衡变得特别困难。在这篇论文中，我们介绍了Upcycled-FL，一种新的联合学习框架，在每次偶数轮中应用首降法。在这个框架下，FL更新中的一半不会导致隐私泄露，同时需要 much less computation。我们首先对Upcycled-FL的抽象分析进行了理论分析，然后通过干扰机制来保护隐私。实验表明，Upcycled-FL在具有多样化数据的实际数据上适用，并在隐私精度质量和训练时间之间进行了显著平衡，而且在平均下降48%的训练时间。
</details></li>
</ul>
<hr>
<h2 id="Automatic-nodule-identification-and-differentiation-in-ultrasound-videos-to-facilitate-per-nodule-examination"><a href="#Automatic-nodule-identification-and-differentiation-in-ultrasound-videos-to-facilitate-per-nodule-examination" class="headerlink" title="Automatic nodule identification and differentiation in ultrasound videos to facilitate per-nodule examination"></a>Automatic nodule identification and differentiation in ultrasound videos to facilitate per-nodule examination</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06339">http://arxiv.org/abs/2310.06339</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siyuan Jiang, Yan Ding, Yuling Wang, Lei Xu, Wenli Dai, Wanru Chang, Jianfeng Zhang, Jie Yu, Jianqiao Zhou, Chunquan Zhang, Ping Liang, Dexing Kong</li>
<li>for:  This paper aims to address the problem of identifying and differentiating nodules in breast ultrasound videos, which is a challenging task due to the heterogeneous appearances of nodules in different cross-sectional views.</li>
<li>methods: The authors collected hundreds of breast ultrasound videos and built a nodule reidentification system that consists of two parts: an extractor based on a deep learning model and a real-time clustering algorithm.</li>
<li>results: The system obtained satisfactory results and was able to differentiate ultrasound videos. This is the first attempt to apply re-identification technique in the ultrasonic field.<details>
<summary>Abstract</summary>
Ultrasound is a vital diagnostic technique in health screening, with the advantages of non-invasive, cost-effective, and radiation free, and therefore is widely applied in the diagnosis of nodules. However, it relies heavily on the expertise and clinical experience of the sonographer. In ultrasound images, a single nodule might present heterogeneous appearances in different cross-sectional views which makes it hard to perform per-nodule examination. Sonographers usually discriminate different nodules by examining the nodule features and the surrounding structures like gland and duct, which is cumbersome and time-consuming. To address this problem, we collected hundreds of breast ultrasound videos and built a nodule reidentification system that consists of two parts: an extractor based on the deep learning model that can extract feature vectors from the input video clips and a real-time clustering algorithm that automatically groups feature vectors by nodules. The system obtains satisfactory results and exhibits the capability to differentiate ultrasound videos. As far as we know, it's the first attempt to apply re-identification technique in the ultrasonic field.
</details>
<details>
<summary>摘要</summary>
乳腺超音波是现代医学检测技术中的一种重要方法，具有不侵入、成本低、无辐射等优点，因此广泛应用于腺体诊断。然而，它受到医生和医疗技术人员的专业知识和临床经验的限制。在超音波图像中，单个腺体可能会显示不同的多样性表现，这使得每个腺体的检测变得困难。医生通常通过评估腺体特征和周围的腺体和腺管来 diferenciation 腺体，这是耗时和耗力的。为解决这个问题，我们收集了数百个乳腺超音波视频，并建立了一个腺体重新标识系统，该系统包括两部分：基于深度学习模型的特征提取器，可以从输入视频剪辑中提取特征向量，以及实时分组算法，可以自动将特征向量分组成不同的腺体。系统取得了满意的结果，并表现出了分辑视频的能力。到目前为止，这是首次应用重新标识技术在超音波领域。
</details></li>
</ul>
<hr>
<h2 id="Learning-bounded-degree-polytrees-with-known-skeleton"><a href="#Learning-bounded-degree-polytrees-with-known-skeleton" class="headerlink" title="Learning bounded-degree polytrees with known skeleton"></a>Learning bounded-degree polytrees with known skeleton</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06333">http://arxiv.org/abs/2310.06333</a></li>
<li>repo_url: None</li>
<li>paper_authors: Davin Choo, Joy Qiping Yang, Arnab Bhattacharyya, Clément L. Canonne</li>
<li>for: efficient proper learning of bounded-degree polytrees</li>
<li>methods: polynomial-time algorithm and information-theoretic sample complexity lower bound</li>
<li>results: finite-sample guarantees for learning $d$-polytrees in polynomial time and sample complexity for any bounded $d$ when the underlying undirected graph (skeleton) is known<details>
<summary>Abstract</summary>
We establish finite-sample guarantees for efficient proper learning of bounded-degree polytrees, a rich class of high-dimensional probability distributions and a subclass of Bayesian networks, a widely-studied type of graphical model. Recently, Bhattacharyya et al. (2021) obtained finite-sample guarantees for recovering tree-structured Bayesian networks, i.e., 1-polytrees. We extend their results by providing an efficient algorithm which learns $d$-polytrees in polynomial time and sample complexity for any bounded $d$ when the underlying undirected graph (skeleton) is known. We complement our algorithm with an information-theoretic sample complexity lower bound, showing that the dependence on the dimension and target accuracy parameters are nearly tight.
</details>
<details>
<summary>摘要</summary>
我们设定有限样本保证的高维概率分布bounded-degree polytrees的有效性学习，这是一种高维概率分布的丰富类型和 bayesian networks的一个子集，这种图形模型广泛研究。 Bhattacharyya et al. (2021) 已经获得了恢复树状 bayesian networks的有限样本保证，我们将其结果扩展，提供了在 полиtrees 中efficient的算法，可以在有 bounded 的 $d$ 下在有限时间内learns 和样本复杂度。我们还补充了信息理论样本复杂度下界，表明我们的样本复杂度和精度参数之间的依赖关系几乎是紧密的。
</details></li>
</ul>
<hr>
<h2 id="Exploit-the-antenna-response-consistency-to-define-the-alignment-criteria-for-CSI-data"><a href="#Exploit-the-antenna-response-consistency-to-define-the-alignment-criteria-for-CSI-data" class="headerlink" title="Exploit the antenna response consistency to define the alignment criteria for CSI data"></a>Exploit the antenna response consistency to define the alignment criteria for CSI data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06328">http://arxiv.org/abs/2310.06328</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ke Xu, Jiangtao Wang, Hongyuan Zhu, Dingchang Zheng</li>
<li>for: 本研究旨在解决WIFI基于人体活动识别（HAR）中自助学习（SSL）的挑战，即缺乏标注数据的问题。</li>
<li>methods: 我们提出了一种解决方案，即ARC（报应响响应一致），用于定义适当的对齐标准。ARC可以保持输入空间中的Semantic信息，并引入了对实际噪声的抗性。</li>
<li>results: 我们通过实验证明了ARC的有效性，它可以提高WIFI基于HAR中自助学习的性能。<details>
<summary>Abstract</summary>
Self-supervised learning (SSL) for WiFi-based human activity recognition (HAR) holds great promise due to its ability to address the challenge of insufficient labeled data. However, directly transplanting SSL algorithms, especially contrastive learning, originally designed for other domains to CSI data, often fails to achieve the expected performance. We attribute this issue to the inappropriate alignment criteria, which disrupt the semantic distance consistency between the feature space and the input space. To address this challenge, we introduce \textbf{A}netenna \textbf{R}esponse \textbf{C}onsistency (ARC) as a solution to define proper alignment criteria. ARC is designed to retain semantic information from the input space while introducing robustness to real-world noise. We analyze ARC from the perspective of CSI data structure, demonstrating that its optimal solution leads to a direct mapping from input CSI data to action vectors in the feature map. Furthermore, we provide extensive experimental evidence to validate the effectiveness of ARC in improving the performance of self-supervised learning for WiFi-based HAR.
</details>
<details>
<summary>摘要</summary>
自我监督学习（SSL） для WiFi-based人体活动识别（HAR）具有很大的推荐力，因为它可以解决因不充分的标注数据而带来的挑战。然而，直接将SSL算法，特别是对比学习，从其他领域直接应用于CSI数据时，经常无法达到预期的性能。我们认为这是因为不适当的对齐标准，导致Feature空间和输入空间之间的semantic distance的一致性被打乱。为解决这个挑战，我们介绍了ARC（自适应响应相关）作为一种解决方案，以定义适当的对齐标准。ARC是一种可以保持输入空间中的semantic信息的算法，同时具有对实际世界噪音的抗针对性。我们从CSI数据结构的角度分析ARC，并证明其最佳解决方案导致输入CSI数据直接映射到功能图中的动作向量。此外，我们还提供了详细的实验证据，以证明ARC在自我监督学习中提高WiFi-based HAR性能的效果。
</details></li>
</ul>
<hr>
<h2 id="Transfer-learning-based-physics-informed-convolutional-neural-network-for-simulating-flow-in-porous-media-with-time-varying-controls"><a href="#Transfer-learning-based-physics-informed-convolutional-neural-network-for-simulating-flow-in-porous-media-with-time-varying-controls" class="headerlink" title="Transfer learning-based physics-informed convolutional neural network for simulating flow in porous media with time-varying controls"></a>Transfer learning-based physics-informed convolutional neural network for simulating flow in porous media with time-varying controls</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06319">http://arxiv.org/abs/2310.06319</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jungang Chen, Eduardo Gildin, John E. Killough</li>
<li>for: 这个论文是用来模拟两相流体在孔隙媒体中的模型，并且使用时间变化的井控制来实现控制-到-状态 regression。</li>
<li>methods: 这个模型使用了finite volume scheme来离散流程方程，并且做了loss函数的定义，以保证流体的质量保守定律。Neumann boundary conditions也被细致地包含在半积分方程中，无需额外的损失函数。模型的架构包括两个平行的U-Net结构，输入为井控制，输出为系统状态。</li>
<li>results: 这个模型可以准确地预测油压和水含量在每个时间步骤中，并且可以快速地训练和转移学习。对于不同的储量格和方向，模型的计算效率和准确性都被证明。在对比 numerical方法的计算效率和准确性方面，模型表现出了优异的性能。<details>
<summary>Abstract</summary>
A physics-informed convolutional neural network is proposed to simulate two phase flow in porous media with time-varying well controls. While most of PICNNs in existing literatures worked on parameter-to-state mapping, our proposed network parameterizes the solution with time-varying controls to establish a control-to-state regression. Firstly, finite volume scheme is adopted to discretize flow equations and formulate loss function that respects mass conservation laws. Neumann boundary conditions are seamlessly incorporated into the semi-discretized equations so no additional loss term is needed. The network architecture comprises two parallel U-Net structures, with network inputs being well controls and outputs being the system states. To capture the time-dependent relationship between inputs and outputs, the network is well designed to mimic discretized state space equations. We train the network progressively for every timestep, enabling it to simultaneously predict oil pressure and water saturation at each timestep. After training the network for one timestep, we leverage transfer learning techniques to expedite the training process for subsequent timestep. The proposed model is used to simulate oil-water porous flow scenarios with varying reservoir gridblocks and aspects including computation efficiency and accuracy are compared against corresponding numerical approaches. The results underscore the potential of PICNN in effectively simulating systems with numerous grid blocks, as computation time does not scale with model dimensionality. We assess the temporal error using 10 different testing controls with variation in magnitude and another 10 with higher alternation frequency with proposed control-to-state architecture. Our observations suggest the need for a more robust and reliable model when dealing with controls that exhibit significant variations in magnitude or frequency.
</details>
<details>
<summary>摘要</summary>
提出了一种基于物理学的卷积神经网络（PICNN），用于模拟具有时间变化的两相流体在孔隙媒体中的行为。大多数现有的PICNN都是基于参数到状态映射，而我们提出的网络则使用时间变化的控制来建立控制到状态重 regression。首先，我们采用了 finite volume 方法来离散流体方程，并将损失函数设计为尊重流体保守定律。Neumann 边界条件可以自然地包含在半离散方程中，因此不需要额外的损失项。网络架构包括两个并行的 U-Net 结构，网络输入为控制，输出为系统状态。为了捕捉时间依赖关系 между输入和输出，网络设计得能够模拟离散状态空间方程。我们在每个时间步进行逐步训练，使网络能够同时预测每个时间步的油压和水含量。在训练一个时间步后，我们利用了传输学习技术来加速后续时间步的训练过程。提出的模型用于模拟各种不同的油水孔隙流场景，并进行了对应的numerical方法的比较。结果表明PICNN可以有效地模拟高维度的系统，计算时间不随模型维度增长。我们使用10个测试控制，其中每个控制都有不同的大小和频率，以及另外10个测试控制，其中每个控制都有更高的振荡频率，来评估模型的时间误差。我们的观察表明，当控制 exhibits 显著的变化 в大小或频率时，需要一个更加可靠和可靠的模型。
</details></li>
</ul>
<hr>
<h2 id="Discovering-Mixtures-of-Structural-Causal-Models-from-Time-Series-Data"><a href="#Discovering-Mixtures-of-Structural-Causal-Models-from-Time-Series-Data" class="headerlink" title="Discovering Mixtures of Structural Causal Models from Time Series Data"></a>Discovering Mixtures of Structural Causal Models from Time Series Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06312">http://arxiv.org/abs/2310.06312</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sumanth Varambally, Yi-An Ma, Rose Yu</li>
<li>for: 这 paper 是为了探讨时间序列数据中 causal 关系的推断问题。</li>
<li>methods: 这 paper 使用了一种基于 end-to-end 训练的方法，通过 maximizing 一个 evidence-lower bound 来推断 causal 模型。</li>
<li>results: 经过对 synthetic 和实际数据进行了广泛的实验，这 paper 的方法在 causal discovery 任务中表现出色，特别是当数据来源于多种不同的 causal 图时。  Additionally, the paper proves the identifiability of the model under some mild assumptions.<details>
<summary>Abstract</summary>
In fields such as finance, climate science, and neuroscience, inferring causal relationships from time series data poses a formidable challenge. While contemporary techniques can handle nonlinear relationships between variables and flexible noise distributions, they rely on the simplifying assumption that data originates from the same underlying causal model. In this work, we relax this assumption and perform causal discovery from time series data originating from mixtures of different causal models. We infer both the underlying structural causal models and the posterior probability for each sample belonging to a specific mixture component. Our approach employs an end-to-end training process that maximizes an evidence-lower bound for data likelihood. Through extensive experimentation on both synthetic and real-world datasets, we demonstrate that our method surpasses state-of-the-art benchmarks in causal discovery tasks, particularly when the data emanates from diverse underlying causal graphs. Theoretically, we prove the identifiability of such a model under some mild assumptions.
</details>
<details>
<summary>摘要</summary>
在金融、气候科学和神经科学等领域，从时间序列数据推断 causal 关系是一项具有挑战性的任务。当今技术可以处理非线性变量之间的关系和 flexible 噪声分布，但它们假设数据来自同一个下游 causal 模型。在这种工作中，我们放弃了这个假设，并从时间序列数据来自多种不同 causal 模型的混合中进行 causal 发现。我们推断出下游结构 causal 模型以及每个样本属于特定混合组件的 posterior 概率。我们的方法使用一个端到端的训练过程，以最大化数据可能性的证据下界。经过了大量的实验，我们发现我们的方法在 causal 发现任务中超过了现状征的标准准则，特别是数据来自多种不同 causal 图的情况。从理论角度，我们证明了这样的模型可以在某些轻微假设下进行可 identificability。
</details></li>
</ul>
<hr>
<h2 id="Ensemble-Active-Learning-by-Contextual-Bandits-for-AI-Incubation-in-Manufacturing"><a href="#Ensemble-Active-Learning-by-Contextual-Bandits-for-AI-Incubation-in-Manufacturing" class="headerlink" title="Ensemble Active Learning by Contextual Bandits for AI Incubation in Manufacturing"></a>Ensemble Active Learning by Contextual Bandits for AI Incubation in Manufacturing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06306">http://arxiv.org/abs/2310.06306</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yingyan Zeng, Xiaoyu Chen, Ran Jin</li>
<li>for: 提高流式数据收集中注释努力的Annotation效率，以保持supervised学习基础学习模型的数据质量。</li>
<li>methods: 提议使用ensemble active learning方法，通过contextual bandits实现活动样本标注，保持exploration-exploitation平衡，提高AI模型表现。</li>
<li>results: 实验结果表明，提议方法可以减少注释努力，同时保持数据质量，从而提高AI模型的表现。<details>
<summary>Abstract</summary>
It is challenging but important to save annotation efforts in streaming data acquisition to maintain data quality for supervised learning base learners. We propose an ensemble active learning method to actively acquire samples for annotation by contextual bandits, which is will enforce the exploration-exploitation balance and leading to improved AI modeling performance.
</details>
<details>
<summary>摘要</summary>
“保持流式数据收集中的注释努力是重要的，以确保超参学习基础模型的数据质量。我们提出了一种 ensemble active learning 方法，通过contextual bandits来活动收集样本，以保持探索与利用的平衡，从而提高 AI 模型表现。”Here's a word-for-word translation:“保持流式数据收集中的注释努力是重要的，以确保超参学习基础模型的数据质量。我们提出了一种 ensemble active learning 方法，通过contextual bandits来活动收集样本，以保持探索与利用的平衡，从而提高 AI 模型表现。”Note that Simplified Chinese is used in mainland China, while Traditional Chinese is used in Taiwan and other regions.
</details></li>
</ul>
<hr>
<h2 id="Gem5Pred-Predictive-Approaches-For-Gem5-Simulation-Time"><a href="#Gem5Pred-Predictive-Approaches-For-Gem5-Simulation-Time" class="headerlink" title="Gem5Pred: Predictive Approaches For Gem5 Simulation Time"></a>Gem5Pred: Predictive Approaches For Gem5 Simulation Time</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06290">http://arxiv.org/abs/2310.06290</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tian Yan, Xueyang Li, Sifat Ut Taki, Saeid Mehrdad</li>
<li>for: 本文目的是提供一个用于预测Gem5模拟时间的数据集和基于CodeBERT的三种模型，以满足现有的时间预测问题。</li>
<li>methods: 本文使用了一个专门为这种任务创建的数据集，并采用了三种基于CodeBERT的模型来实现预测任务。</li>
<li>results: 我们的最佳回归模型的 Mean Absolute Error (MAE) 为0.546，而我们的最高精度分类模型的 Accuracy 为0.696。这些模型可以作为未来研究的基础，并且可以与之后的模型进行比较。<details>
<summary>Abstract</summary>
Gem5, an open-source, flexible, and cost-effective simulator, is widely recognized and utilized in both academic and industry fields for hardware simulation. However, the typically time-consuming nature of simulating programs on Gem5 underscores the need for a predictive model that can estimate simulation time. As of now, no such dataset or model exists. In response to this gap, this paper makes a novel contribution by introducing a unique dataset specifically created for this purpose. We also conducted analysis of the effects of different instruction types on the simulation time in Gem5. After this, we employ three distinct models leveraging CodeBERT to execute the prediction task based on the developed dataset. Our superior regression model achieves a Mean Absolute Error (MAE) of 0.546, while our top-performing classification model records an Accuracy of 0.696. Our models establish a foundation for future investigations on this topic, serving as benchmarks against which subsequent models can be compared. We hope that our contribution can simulate further research in this field. The dataset we used is available at https://github.com/XueyangLiOSU/Gem5Pred.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Better-and-Simpler-Lower-Bounds-for-Differentially-Private-Statistical-Estimation"><a href="#Better-and-Simpler-Lower-Bounds-for-Differentially-Private-Statistical-Estimation" class="headerlink" title="Better and Simpler Lower Bounds for Differentially Private Statistical Estimation"></a>Better and Simpler Lower Bounds for Differentially Private Statistical Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06289">http://arxiv.org/abs/2310.06289</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shyam Narayanan</li>
<li>for: 这两个研究目标是为了实现精度高的private estimationTask，具体来说是covariance estimation和mean estimation。</li>
<li>methods: 这两个研究使用了fingerprinting方法和bayesian方法来实现private estimation。</li>
<li>results: 这两个研究得到了以下结论：	+ 对于covariance estimation，需要有 $\tilde{\Omega}\left(\frac{d^{3&#x2F;2}{\alpha \varepsilon} + \frac{d}{\alpha^2}\right)$ 样本，这是前一个研究的改进版本，且是 simpler than previous work。	+ 对于mean estimation，需要有 $\tilde{\Omega}\left(\frac{d}{\alpha^{k&#x2F;(k-1)} \varepsilon} + \frac{d}{\alpha^2}\right)$ 样本，这与已知的Upper bound相符，并且超过了对于纯 diferencial privacy 的最佳下界。<details>
<summary>Abstract</summary>
We provide improved lower bounds for two well-known high-dimensional private estimation tasks. First, we prove that for estimating the covariance of a Gaussian up to spectral error $\alpha$ with approximate differential privacy, one needs $\tilde{\Omega}\left(\frac{d^{3/2}{\alpha \varepsilon} + \frac{d}{\alpha^2}\right)$ samples for any $\alpha \le O(1)$, which is tight up to logarithmic factors. This improves over previous work which established this for $\alpha \le O\left(\frac{1}{\sqrt{d}\right)$, and is also simpler than previous work. Next, we prove that for estimating the mean of a heavy-tailed distribution with bounded $k$th moments with approximate differential privacy, one needs $\tilde{\Omega}\left(\frac{d}{\alpha^{k/(k-1)} \varepsilon} + \frac{d}{\alpha^2}\right)$ samples. This matches known upper bounds and improves over the best known lower bound for this problem, which only hold for pure differential privacy, or when $k = 2$. Our techniques follow the method of fingerprinting and are generally quite simple. Our lower bound for heavy-tailed estimation is based on a black-box reduction from privately estimating identity-covariance Gaussians. Our lower bound for covariance estimation utilizes a Bayesian approach to show that, under an Inverse Wishart prior distribution for the covariance matrix, no private estimator can be accurate even in expectation, without sufficiently many samples.
</details>
<details>
<summary>摘要</summary>
我们提供了几个改进的下界 для两个高维度私人推导任务。首先，我们证明了为了在 Gaussian 的均值上进行约定 $\alpha$ 的私人推导，需要 $\tilde{\Omega}\left(\frac{d^{3/2}{\alpha \varepsilon} + \frac{d}{\alpha^2}\right)$ 样本，这是对于任何 $\alpha \le O(1)$ 都是严格的下界，并且比前一次的成果更为简单。其次，我们证明了在具有bounded $k$th  moments 的非常粗糙分布上进行均值推导时，需要 $\tilde{\Omega}\left(\frac{d}{\alpha^{k/(k-1)} \varepsilon} + \frac{d}{\alpha^2}\right)$ 样本，这与知名的上界相匹配，并且超过了对于纯粹的推导性能的下界，只有在 $k = 2$ 时才能推导出。我们的技术基于指纹技术，通常很简单。我们的下界 для 均值推导基于黑盒减少，具体来说是从私人推导均值 Gaussian 的方向下减少。我们的下界 для 均值推导使用了 bayesian 方法，证明在对均值矩阵的 inverse wishart 分布下，没有私人推导器可以在预期中准确地推导，不具备充分的样本。
</details></li>
</ul>
<hr>
<h2 id="Bi-Level-Offline-Policy-Optimization-with-Limited-Exploration"><a href="#Bi-Level-Offline-Policy-Optimization-with-Limited-Exploration" class="headerlink" title="Bi-Level Offline Policy Optimization with Limited Exploration"></a>Bi-Level Offline Policy Optimization with Limited Exploration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06268">http://arxiv.org/abs/2310.06268</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenzhuo Zhou</li>
<li>for: 学习无线连续学习（RL）策略，以固定的预先收集的数据集进行学习。</li>
<li>methods: 提出了一种二级结构化策略优化算法，该算法模型了策略（上级）和价值函数（下级）之间的层次结构。下级专注于构建一个可靠的价值估计集，以保持小 enough的加权巴尔曼错误平均值，同时控制数据不一致性引起的uncertainty。而上级则尝试通过下级提供的保守价值估计集来最大化策略。</li>
<li>results: 在使用 synthetic、标准 benchmark 和实际世界数据集进行评估中，我们的模型与现状顶尖方法竞争性表现。<details>
<summary>Abstract</summary>
We study offline reinforcement learning (RL) which seeks to learn a good policy based on a fixed, pre-collected dataset. A fundamental challenge behind this task is the distributional shift due to the dataset lacking sufficient exploration, especially under function approximation. To tackle this issue, we propose a bi-level structured policy optimization algorithm that models a hierarchical interaction between the policy (upper-level) and the value function (lower-level). The lower level focuses on constructing a confidence set of value estimates that maintain sufficiently small weighted average Bellman errors, while controlling uncertainty arising from distribution mismatch. Subsequently, at the upper level, the policy aims to maximize a conservative value estimate from the confidence set formed at the lower level. This novel formulation preserves the maximum flexibility of the implicitly induced exploratory data distribution, enabling the power of model extrapolation. In practice, it can be solved through a computationally efficient, penalized adversarial estimation procedure. Our theoretical regret guarantees do not rely on any data-coverage and completeness-type assumptions, only requiring realizability. These guarantees also demonstrate that the learned policy represents the "best effort" among all policies, as no other policies can outperform it. We evaluate our model using a blend of synthetic, benchmark, and real-world datasets for offline RL, showing that it performs competitively with state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)我们研究无线RL，它的目标是基于预先收集的固定数据集学习一个好策略。然而，这个任务面临着数据分布变化的挑战，尤其是在函数近似下。为解决这个问题，我们提出了一个二级结构化策略优化算法，它模型了策略（上层）和价值函数（下层）之间的层次交互。下层关注于建立一个可靠的价值估计集，使其保持小于一定的均值 Bellman 误差，同时控制由数据分布匹配引起的uncertainty。而上层则是通过最大化一个保守的价值估计来优化策略。这种新的表述保留了隐式引入的探索数据分布的最大灵活性，使得模型渐近。在实践中，它可以通过一种 computationally efficient 的偏好对抗估计过程解决。我们的理论 regret 保证不需要任何数据覆盖和完整性类型的假设，只需要可行性。这些保证还证明了学习的策略是所有策略中的"最佳努力"，因为没有其他策略可以超越它。我们使用了一个混合的synthetic、benchmark和实际数据集来评估我们的模型，并显示它与当前的方法竞争。
</details></li>
</ul>
<hr>
<h2 id="A-Unified-View-on-Solving-Objective-Mismatch-in-Model-Based-Reinforcement-Learning"><a href="#A-Unified-View-on-Solving-Objective-Mismatch-in-Model-Based-Reinforcement-Learning" class="headerlink" title="A Unified View on Solving Objective Mismatch in Model-Based Reinforcement Learning"></a>A Unified View on Solving Objective Mismatch in Model-Based Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06253">http://arxiv.org/abs/2310.06253</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ran Wei, Nathan Lambert, Anthony McDonald, Alfredo Garcia, Roberto Calandra</li>
<li>for: 本研究的目的是实现更有效率、适应性和可解释性的实验学习（Model-based Reinforcement Learning，MBRL）代理人。</li>
<li>methods: 本研究使用了多种解决问题对照的方法，包括：	+ 对照环境模型的学习，以便对环境进行预测和决策。	+ 使用对照环境模型的训练，以便改善对环境的理解和决策。	+ 使用不同的评估标准，以便评估模型和策略的效果。</li>
<li>results: 本研究发现，现有的MBRL方法通常会受到“目标差异”的问题，即模型预测的对环境的准确性与策略优化的对环境的回应有所差异。此外，本研究还发现了一些相关的解决方案，包括：	+ 对环境模型的适应和更新。	+ 使用不同的策略优化方法。	+ 使用不同的评估标准。<details>
<summary>Abstract</summary>
Model-based Reinforcement Learning (MBRL) aims to make agents more sample-efficient, adaptive, and explainable by learning an explicit model of the environment. While the capabilities of MBRL agents have significantly improved in recent years, how to best learn the model is still an unresolved question. The majority of MBRL algorithms aim at training the model to make accurate predictions about the environment and subsequently using the model to determine the most rewarding actions. However, recent research has shown that model predictive accuracy is often not correlated with action quality, tracing the root cause to the \emph{objective mismatch} between accurate dynamics model learning and policy optimization of rewards. A number of interrelated solution categories to the objective mismatch problem have emerged as MBRL continues to mature as a research area. In this work, we provide an in-depth survey of these solution categories and propose a taxonomy to foster future research.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>环境模型学习与奖励政策优化的对应性问题。2. 使用不同的优化目标来解决对应性问题。3. 使用多个模型来学习环境和奖励政策之间的关系。4. 使用环境感知技术来改善模型的准确性。5. 使用不同的学习策略来解决对应性问题。本文提供了 MBRL 这些解决方案的深入概述，并提出了一种分类法，以便未来的研究。</details></li>
</ol>
<hr>
<h2 id="Deep-Learning-A-Tutorial"><a href="#Deep-Learning-A-Tutorial" class="headerlink" title="Deep Learning: A Tutorial"></a>Deep Learning: A Tutorial</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06251">http://arxiv.org/abs/2310.06251</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ChristosChristofidis/awesome-deep-learning">https://github.com/ChristosChristofidis/awesome-deep-learning</a></li>
<li>paper_authors: Nick Polson, Vadim Sokolov</li>
<li>for: 这篇论文的目的是为了提供深度学习方法，以提供结构化高维数据的预测和不确定性评估。</li>
<li>methods: 这篇论文使用层次的半同构输入变换来提供预测规则，而不是使用大多数统计模型中的浅层添加性。</li>
<li>results: 这篇论文通过应用层次变换来生成一组特征（或特征），然后使用概率统计方法进行预测。这种方法可以同时实现缩放预测规则和不确定性评估。<details>
<summary>Abstract</summary>
Our goal is to provide a review of deep learning methods which provide insight into structured high-dimensional data. Rather than using shallow additive architectures common to most statistical models, deep learning uses layers of semi-affine input transformations to provide a predictive rule. Applying these layers of transformations leads to a set of attributes (or, features) to which probabilistic statistical methods can be applied. Thus, the best of both worlds can be achieved: scalable prediction rules fortified with uncertainty quantification, where sparse regularization finds the features.
</details>
<details>
<summary>摘要</summary>
我们的目标是为深度学习方法进行评估，以获得结构化高维数据的深入理解。而不是使用大多数统计模型常用的浅层添加性架构，深度学习使用层次的半 Similarity输入变换来提供预测规则。通过这些层次变换，可以获得一组特征（或者特征），这些特征可以通过 probabilistic 统计方法进行评估。因此，可以实现最好的两个世界：可扩展的预测规则和不确定性评估，而 sparse 正则化可以找到特征。Note: "Simplified Chinese" is a translation of "Traditional Chinese" and "简化字" (Simplified Chinese) is a romanization of "简化字" (Simplified Chinese characters).
</details></li>
</ul>
<hr>
<h2 id="Sample-Efficient-Multi-Agent-RL-An-Optimization-Perspective"><a href="#Sample-Efficient-Multi-Agent-RL-An-Optimization-Perspective" class="headerlink" title="Sample-Efficient Multi-Agent RL: An Optimization Perspective"></a>Sample-Efficient Multi-Agent RL: An Optimization Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06243">http://arxiv.org/abs/2310.06243</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nuoya Xiong, Zhihan Liu, Zhaoran Wang, Zhuoran Yang</li>
<li>for: 该 paper 探讨了多智能体强化学习 (MARL) 在通用函数近似下的多智能体Markov 游戏 (MG) 中的学习问题。</li>
<li>methods: 作者引入了一个新的复杂度度量called Multi-Agent Decoupling Coefficient (MADC)，以确定学习过程中的最小假设。基于这个度量，作者提出了第一个可以保证学习效率的算法框架。</li>
<li>results: 作者的算法可以在学习 Nash 均衡、粗略相关均衡和相关均衡问题中达到相对较低的梯度损失，并且与现有的算法相比具有相似的折衡 regret。此外，作者的算法可以避免在数据依赖的约束中解决各个对象的优化问题，从而更易于实际应用。<details>
<summary>Abstract</summary>
We study multi-agent reinforcement learning (MARL) for the general-sum Markov Games (MGs) under the general function approximation. In order to find the minimum assumption for sample-efficient learning, we introduce a novel complexity measure called the Multi-Agent Decoupling Coefficient (MADC) for general-sum MGs. Using this measure, we propose the first unified algorithmic framework that ensures sample efficiency in learning Nash Equilibrium, Coarse Correlated Equilibrium, and Correlated Equilibrium for both model-based and model-free MARL problems with low MADC. We also show that our algorithm provides comparable sublinear regret to the existing works. Moreover, our algorithm combines an equilibrium-solving oracle with a single objective optimization subprocedure that solves for the regularized payoff of each deterministic joint policy, which avoids solving constrained optimization problems within data-dependent constraints (Jin et al. 2020; Wang et al. 2023) or executing sampling procedures with complex multi-objective optimization problems (Foster et al. 2023), thus being more amenable to empirical implementation.
</details>
<details>
<summary>摘要</summary>
我们研究多体学习（MARL）的总和游戏（MG）下的通用函数近似下的多体减噪系数（MADC），以找到最小的假设，以实现样本效率的学习。我们提出了第一个统一的算法框架，可以保证样本效率的学习 Nash 平衡、粗 corr 平衡和相关平衡，并且可以避免在数据依赖的约束下解决减噪问题（Jin et al. 2020；Wang et al. 2023）或者在复杂多目标优化问题下执行抽象多目标优化问题（Foster et al. 2023）。这使我们的算法更易于实际应用。
</details></li>
</ul>
<hr>
<h2 id="A-Bayesian-framework-for-discovering-interpretable-Lagrangian-of-dynamical-systems-from-data"><a href="#A-Bayesian-framework-for-discovering-interpretable-Lagrangian-of-dynamical-systems-from-data" class="headerlink" title="A Bayesian framework for discovering interpretable Lagrangian of dynamical systems from data"></a>A Bayesian framework for discovering interpretable Lagrangian of dynamical systems from data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06241">http://arxiv.org/abs/2310.06241</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tapas Tripura, Souvik Chakraborty</li>
<li>for: 学习和预测物理系统的动力学行为需要深入了解物理法律的基础。这些工作把描述物理系统的方程写入机器学习模型中，而我们提议一种 alternate 方法，使用稀疏泊描述理论来学习物理系统的几何特征。</li>
<li>methods: 我们的方法包括（a）生成可解释的劳伦茨描述，（b）使用 bayesian 学习来衡量限制数据的epistemic不确定性，（c）通过列朗德转换自动生成 Hamiltonian，并（d）提供 ODE 和 PDE 基于的描述。</li>
<li>results: 我们在六个不同的示例中证明了我们的方法的可行性，这些示例包括 both discrete 和连续系统。<details>
<summary>Abstract</summary>
Learning and predicting the dynamics of physical systems requires a profound understanding of the underlying physical laws. Recent works on learning physical laws involve generalizing the equation discovery frameworks to the discovery of Hamiltonian and Lagrangian of physical systems. While the existing methods parameterize the Lagrangian using neural networks, we propose an alternate framework for learning interpretable Lagrangian descriptions of physical systems from limited data using the sparse Bayesian approach. Unlike existing neural network-based approaches, the proposed approach (a) yields an interpretable description of Lagrangian, (b) exploits Bayesian learning to quantify the epistemic uncertainty due to limited data, (c) automates the distillation of Hamiltonian from the learned Lagrangian using Legendre transformation, and (d) provides ordinary (ODE) and partial differential equation (PDE) based descriptions of the observed systems. Six different examples involving both discrete and continuous system illustrates the efficacy of the proposed approach.
</details>
<details>
<summary>摘要</summary>
学习和预测物理系统的动力学需要深刻的物理知识。现有的工作是把物理法则推广到物理系统的寻找方程的发现。而现有的方法通常使用神经网络参数化拉格朗日函数，我们提议一种 alternate 的方法，使得可以从有限数据获得可读性的拉格朗日描述，并且可以量化有限数据所带来的epistemic不确定性。此外，该方法还可以自动从学习到的拉格朗日函数中提取汉密尔顿函数，并且提供了描述系统的常微分方程和偏微分方程描述。我们在六个不同的示例中验证了该方法的有效性，这些示例包括连续和离散系统。
</details></li>
</ul>
<hr>
<h2 id="Differentially-Private-Multi-Site-Treatment-Effect-Estimation"><a href="#Differentially-Private-Multi-Site-Treatment-Effect-Estimation" class="headerlink" title="Differentially Private Multi-Site Treatment Effect Estimation"></a>Differentially Private Multi-Site Treatment Effect Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06237">http://arxiv.org/abs/2310.06237</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tatsuki Koga, Kamalika Chaudhuri, David Page<br>for: This paper aims to provide a federated analytics approach for estimating the average treatment effect (ATE) in healthcare applications while ensuring differential privacy (DP) guarantees at each site.methods: The proposed method uses a class of per-site estimation algorithms that report the ATE estimate and its variance as a quality measure, and an aggregation algorithm on the server side that minimizes the overall variance of the final ATE estimate.results: The authors’ experiments on real and synthetic data show that their method reliably aggregates private statistics across sites and provides a better privacy-utility tradeoff under site heterogeneity than baselines.<details>
<summary>Abstract</summary>
Patient privacy is a major barrier to healthcare AI. For confidentiality reasons, most patient data remains in silo in separate hospitals, preventing the design of data-driven healthcare AI systems that need large volumes of patient data to make effective decisions. A solution to this is collective learning across multiple sites through federated learning with differential privacy. However, literature in this space typically focuses on differentially private statistical estimation and machine learning, which is different from the causal inference-related problems that arise in healthcare. In this work, we take a fresh look at federated learning with a focus on causal inference; specifically, we look at estimating the average treatment effect (ATE), an important task in causal inference for healthcare applications, and provide a federated analytics approach to enable ATE estimation across multiple sites along with differential privacy (DP) guarantees at each site. The main challenge comes from site heterogeneity -- different sites have different sample sizes and privacy budgets. We address this through a class of per-site estimation algorithms that reports the ATE estimate and its variance as a quality measure, and an aggregation algorithm on the server side that minimizes the overall variance of the final ATE estimate. Our experiments on real and synthetic data show that our method reliably aggregates private statistics across sites and provides better privacy-utility tradeoff under site heterogeneity than baselines.
</details>
<details>
<summary>摘要</summary>
�ynamic privacy is a major barrier to healthcare AI. For confidentiality reasons, most patient data remains in silos in separate hospitals, preventing the design of data-driven healthcare AI systems that need large volumes of patient data to make effective decisions. A solution to this is collective learning across multiple sites through federated learning with differential privacy. However, literature in this space typically focuses on differentially private statistical estimation and machine learning, which is different from the causal inference-related problems that arise in healthcare. In this work, we take a fresh look at federated learning with a focus on causal inference; specifically, we look at estimating the average treatment effect (ATE), an important task in causal inference for healthcare applications, and provide a federated analytics approach to enable ATE estimation across multiple sites along with differential privacy (DP) guarantees at each site. The main challenge comes from site heterogeneity -- different sites have different sample sizes and privacy budgets. We address this through a class of per-site estimation algorithms that reports the ATE estimate and its variance as a quality measure, and an aggregation algorithm on the server side that minimizes the overall variance of the final ATE estimate. Our experiments on real and synthetic data show that our method reliably aggregates private statistics across sites and provides better privacy-utility tradeoff under site heterogeneity than baselines.
</details></li>
</ul>
<hr>
<h2 id="Low-Rank-Tensor-Completion-via-Novel-Sparsity-Inducing-Regularizers"><a href="#Low-Rank-Tensor-Completion-via-Novel-Sparsity-Inducing-Regularizers" class="headerlink" title="Low-Rank Tensor Completion via Novel Sparsity-Inducing Regularizers"></a>Low-Rank Tensor Completion via Novel Sparsity-Inducing Regularizers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06233">http://arxiv.org/abs/2310.06233</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhi-Yong Wang, Hing Cheung So, Abdelhak M. Zoubir</li>
<li>for: 提高low-rank tensor completion问题中的稀疏性能。</li>
<li>methods: 使用非对称Surrogate&#x2F;正则化，并开发了基于替换方法的高效算法。</li>
<li>results: 实验结果表明，提案方法可以在实际数据上比前种方法更高的稀疏性能。<details>
<summary>Abstract</summary>
To alleviate the bias generated by the l1-norm in the low-rank tensor completion problem, nonconvex surrogates/regularizers have been suggested to replace the tensor nuclear norm, although both can achieve sparsity. However, the thresholding functions of these nonconvex regularizers may not have closed-form expressions and thus iterations are needed, which increases the computational loads. To solve this issue, we devise a framework to generate sparsity-inducing regularizers with closed-form thresholding functions. These regularizers are applied to low-tubal-rank tensor completion, and efficient algorithms based on the alternating direction method of multipliers are developed. Furthermore, convergence of our methods is analyzed and it is proved that the generated sequences are bounded and any limit point is a stationary point. Experimental results using synthetic and real-world datasets show that the proposed algorithms outperform the state-of-the-art methods in terms of restoration performance.
</details>
<details>
<summary>摘要</summary>
对于低矩阵完成问题中带来的偏调，非凸代替器/规律被建议来取代矩阵核心 нор，尽管它们都能够产生简洁性。然而，非凸规律的擦除函数可能无关closed-form表达，因此需要迭代运算，这会增加computational负担。为解决这个问题，我们设计了一个架构，可以生成具有关闭式擦除函数的简洁化规律。这些规律被应用到低管阵完成问题上，并开发了基于多重方向积分法的有效算法。此外，我们分析了我们的方法的收敛性，并证明其生成的序列是紧缩的，任何限点都是稳定点。实验结果显示，提出的方法在实验数据上比州前方法有更好的修复性。
</details></li>
</ul>
<hr>
<h2 id="Exploring-adversarial-attacks-in-federated-learning-for-medical-imaging"><a href="#Exploring-adversarial-attacks-in-federated-learning-for-medical-imaging" class="headerlink" title="Exploring adversarial attacks in federated learning for medical imaging"></a>Exploring adversarial attacks in federated learning for medical imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06227">http://arxiv.org/abs/2310.06227</a></li>
<li>repo_url: None</li>
<li>paper_authors: Erfan Darzi, Florian Dubost, N. M. Sijtsema, P. M. A van Ooijen</li>
<li>for: 评估 Federated Learning 网络在医疗影像分析中对抗攻击的漏洞。</li>
<li>methods: 使用域专业 MRI肿瘤和病理影像 Datasets 评估攻击者在 Federated Learning 环境中的攻击效果。</li>
<li>results: 测试发现，域专业配置可以使攻击者成功率明显增加。结论强调需要有效的防御机制，并建议现有安全协议在 Federated Medical Image Analysis 系统中进行重新评估。<details>
<summary>Abstract</summary>
Federated learning offers a privacy-preserving framework for medical image analysis but exposes the system to adversarial attacks. This paper aims to evaluate the vulnerabilities of federated learning networks in medical image analysis against such attacks. Employing domain-specific MRI tumor and pathology imaging datasets, we assess the effectiveness of known threat scenarios in a federated learning environment. Our tests reveal that domain-specific configurations can increase the attacker's success rate significantly. The findings emphasize the urgent need for effective defense mechanisms and suggest a critical re-evaluation of current security protocols in federated medical image analysis systems.
</details>
<details>
<summary>摘要</summary>
translate_chinese(    "Federated learning 提供了一个隐私保护的框架 для医疗影像分析，但暴露了系统于敌意攻击。这篇论文旨在评估 federated learning 网络在医疗影像分析中对这些攻击的抵御能力。使用具有域专属 MRI 肿瘤和病理图像 Datasets，我们评估了已知威胁enario在 Federated learning 环境中的效果。我们的测试发现，域专属配置可以提高攻击者的成功率，这些发现强调了现有安全协议的重要性，并建议进行重新评估。")以下是翻译结果：Federated learning 提供了一个隐私保护的框架 для医疗影像分析，但暴露了系统于敌意攻击。这篇论文旨在评估 federated learning 网络在医疗影像分析中对这些攻击的抵御能力。使用具有域专属 MRI 肿瘤和病理图像 Datasets，我们评估了已知威胁enario在 Federated learning 环境中的效果。我们的测试发现，域专属配置可以提高攻击者的成功率，这些发现强调了现有安全协议的重要性，并建议进行重新评估。
</details></li>
</ul>
<hr>
<h2 id="Detecting-and-Learning-Out-of-Distribution-Data-in-the-Open-world-Algorithm-and-Theory"><a href="#Detecting-and-Learning-Out-of-Distribution-Data-in-the-Open-world-Algorithm-and-Theory" class="headerlink" title="Detecting and Learning Out-of-Distribution Data in the Open world: Algorithm and Theory"></a>Detecting and Learning Out-of-Distribution Data in the Open world: Algorithm and Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06221">http://arxiv.org/abs/2310.06221</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiyou Sun</li>
<li>for: 本论文目的在于解决机器学习在开放世界enario中的应用问题，特别是面对 previously unknown data和contexts的情况。</li>
<li>methods: 本研究采用了两个步骤来解决开放世界机器学习的问题：out-of-distribution检测和开放世界表示学习（ORL）。</li>
<li>results: 本研究提出了一系列算法和理论基础，以便建立能够在开放世界中表现出色并且可靠的机器学习模型。<details>
<summary>Abstract</summary>
This thesis makes considerable contributions to the realm of machine learning, specifically in the context of open-world scenarios where systems face previously unseen data and contexts. Traditional machine learning models are usually trained and tested within a fixed and known set of classes, a condition known as the closed-world setting. While this assumption works in controlled environments, it falls short in real-world applications where new classes or categories of data can emerge dynamically and unexpectedly. To address this, our research investigates two intertwined steps essential for open-world machine learning: Out-of-distribution (OOD) Detection and Open-world Representation Learning (ORL). OOD detection focuses on identifying instances from unknown classes that fall outside the model's training distribution. This process reduces the risk of making overly confident, erroneous predictions about unfamiliar inputs. Moving beyond OOD detection, ORL extends the capabilities of the model to not only detect unknown instances but also learn from and incorporate knowledge about these new classes. By delving into these research problems of open-world learning, this thesis contributes both algorithmic solutions and theoretical foundations, which pave the way for building machine learning models that are not only performant but also reliable in the face of the evolving complexities of the real world.
</details>
<details>
<summary>摘要</summary>
OOD detection involves identifying instances from unknown classes that fall outside the model's training distribution. This process helps reduce the risk of making overly confident, erroneous predictions about unfamiliar inputs. In addition, ORL extends the capabilities of the model to not only detect unknown instances but also learn from and incorporate knowledge about these new classes. By tackling these research problems of open-world learning, this thesis provides both algorithmic solutions and theoretical foundations, laying the groundwork for building machine learning models that are not only high-performing but also reliable in the face of the evolving complexities of the real world.
</details></li>
</ul>
<hr>
<h2 id="Federated-Multi-Level-Optimization-over-Decentralized-Networks"><a href="#Federated-Multi-Level-Optimization-over-Decentralized-Networks" class="headerlink" title="Federated Multi-Level Optimization over Decentralized Networks"></a>Federated Multi-Level Optimization over Decentralized Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06217">http://arxiv.org/abs/2310.06217</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuoguang Yang, Xuezhou Zhang, Mengdi Wang</li>
<li>for: 这篇论文目的是研究分布式多层优化问题，即在网络上的 Agent 只能与当前邻居交流优化问题。</li>
<li>methods: 该论文提出了一种基于谈判的分布式多层优化算法，可以在单个时间尺度内解决多个层次优化问题，并通过网络传播来共享信息。</li>
<li>results: 该算法可以在不同应用场景中实现优秀表现，包括hyperparameter tuning、分布式 reinforcement learning 和风险谨慎优化。同时，该算法的样本复杂度为网络大小的直线性增长。<details>
<summary>Abstract</summary>
Multi-level optimization has gained increasing attention in recent years, as it provides a powerful framework for solving complex optimization problems that arise in many fields, such as meta-learning, multi-player games, reinforcement learning, and nested composition optimization. In this paper, we study the problem of distributed multi-level optimization over a network, where agents can only communicate with their immediate neighbors. This setting is motivated by the need for distributed optimization in large-scale systems, where centralized optimization may not be practical or feasible. To address this problem, we propose a novel gossip-based distributed multi-level optimization algorithm that enables networked agents to solve optimization problems at different levels in a single timescale and share information through network propagation. Our algorithm achieves optimal sample complexity, scaling linearly with the network size, and demonstrates state-of-the-art performance on various applications, including hyper-parameter tuning, decentralized reinforcement learning, and risk-averse optimization.
</details>
<details>
<summary>摘要</summary>
多层优化在最近几年内得到了越来越多的关注，因为它提供了一个强大的框架来解决许多领域中的复杂优化问题，如元学习、多 Player 游戏、回归学习和嵌套组合优化。在这篇论文中，我们研究了分布式多层优化问题，其中代理可以只与当前邻居进行交流。这种设定是由大规模系统中的分布式优化需求所驱动的，因为中央化优化可能不是实际或可行的。为解决这个问题，我们提出了一种基于吹拂的分布式多层优化算法，允许网络代理在不同层次上解决优化问题，并在单个时间尺度内共享信息。我们的算法实现了最佳样本复杂度，线性增长与网络大小相关，并在多个应用程序中达到了状态革命性的性能，包括超参调整、分布式回归学习和风险谨慎优化。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/10/cs.LG_2023_10_10/" data-id="clp89dohm00syi7882pf7amia" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_10_10" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/10/eess.IV_2023_10_10/" class="article-date">
  <time datetime="2023-10-10T09:00:00.000Z" itemprop="datePublished">2023-10-10</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/10/eess.IV_2023_10_10/">eess.IV - 2023-10-10</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Open-source-Pulseq-sequences-on-Philips-MRI-scanners"><a href="#Open-source-Pulseq-sequences-on-Philips-MRI-scanners" class="headerlink" title="Open-source Pulseq sequences on Philips MRI scanners"></a>Open-source Pulseq sequences on Philips MRI scanners</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06962">http://arxiv.org/abs/2310.06962</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas H. M. Roos, Edwin Versteeg, Dennis W. J. Klomp, Jeroen C. W. Siero, Jannie P. Wijnen</li>
<li>for: 这个论文的目的是为研究人员开发和分享新的MRI序列提供一个可用的开源平台。</li>
<li>methods: 这个论文使用了修改一些源代码文件来创建一个Pulseq解释器 для Philips MRI系统。验证实验使用了模拟和在7T Achieva MRI系统上进行的phantom扫描。</li>
<li>results: 通过Pulseq实现得到的重建图像与原始实现的图像相当，并且对MRI仪器的资源利用进行了评估，显示了一定的可扩展性。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Purpose: This work aims to address the limitations faced by researchers in developing and sharing new MRI sequences by implementing an interpreter for the open-source MRI pulse sequence format, Pulseq, on a Philips MRI scanner.   Methods: The implementation involved modifying a few source code files to create a Pulseq interpreter for the Philips MRI system. Validation experiments were conducted using simulations and phantom scans performed on a 7T Achieva MRI system. The observed sequence and waveforms were compared to the intended ones, and the gradient waveforms produced by the scanner were verified using a field camera. Image reconstruction was performed using the raw k-space samples acquired from both the native vendor environment and the Pulseq interpreter.   Results: The reconstructed images obtained through the Pulseq implementation were found to be comparable to those obtained through the native implementation. The performance of the Pulseq interpreter was assessed by profiling the CPU utilization of the MRI spectrometer, showing minimal resource utilization for certain sequences.   Conclusion: The successful implementation of the Pulseq interpreter on the Philips MRI scanner demonstrates the feasibility of utilizing Pulseq sequences on Philips MRI scanners. This provides an open-source platform for MRI sequence development, facilitating collaboration among researchers and accelerating scientific progress in the field of MRI.
</details>
<details>
<summary>摘要</summary>
目的：本研究旨在解决开源MRI序列格式（Pulseq）的开发和共享限制，通过在菲利浦MRI系统上实现Pulseq解释器。方法：实现过程包括修改一些源代码文件，以创建一个Pulseq解释器 для菲利浦MRI系统。验证实验使用了模拟和phantom扫描，在7T Achieva MRI系统上进行。观察的序列和波形与意图的相比，并使用场Camera验证了扫描产生的梯度波形。图像重建使用了原始的k空间样本，从Native vendor环境和Pulseq解释器中获取。结果：通过Pulseq实现的重建图像与Native实现的图像相似。解释器的性能评估通过CPU资源的使用量进行，表明某些序列的资源使用率很低。结论：成功实现Pulseq解释器在菲利浦MRI系统上，证明了使用Pulseq序列在菲利浦MRI系统上的可能性。这提供了一个开源平台 дляMRI序列开发，促进研究人员之间的合作，加速MRI领域科学进步。
</details></li>
</ul>
<hr>
<h2 id="Compression-Ratio-Learning-and-Semantic-Communications-for-Video-Imaging"><a href="#Compression-Ratio-Learning-and-Semantic-Communications-for-Video-Imaging" class="headerlink" title="Compression Ratio Learning and Semantic Communications for Video Imaging"></a>Compression Ratio Learning and Semantic Communications for Video Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06246">http://arxiv.org/abs/2310.06246</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bowen Zhang, Zhijin Qin, Geoffrey Ye Li</li>
<li>for: 提高智能 роботи系统中摄像头仪器的感应效率，以减少能源、内存等相关资源。</li>
<li>methods: 使用空间不断比对式压缩比率的影像压缩感应系统，实现高分辨率影像摄取。另外，运用数据传输方法来评估通信系统的性能，并将不同构成算法设计 для应用高动态范围影像摄取、影像压缩感应或动作降噪。</li>
<li>results: 通过使用政策Gradient方法来实现明确的压缩率与影像歪斜调整 trade-off，提高影像质量。numerical results show the superiority of the proposed methods over existing baselines.<details>
<summary>Abstract</summary>
Camera sensors have been widely used in intelligent robotic systems. Developing camera sensors with high sensing efficiency has always been important to reduce the power, memory, and other related resources. Inspired by recent success on programmable sensors and deep optic methods, we design a novel video compressed sensing system with spatially-variant compression ratios, which achieves higher imaging quality than the existing snapshot compressed imaging methods with the same sensing costs. In this article, we also investigate the data transmission methods for programmable sensors, where the performance of communication systems is evaluated by the reconstructed images or videos rather than the transmission of sensor data itself. Usually, different reconstruction algorithms are designed for applications in high dynamic range imaging, video compressive sensing, or motion debluring. This task-aware property inspires a semantic communication framework for programmable sensors. In this work, a policy-gradient based reinforcement learning method is introduced to achieve the explicit trade-off between the compression (or transmission) rate and the image distortion. Numerical results show the superiority of the proposed methods over existing baselines.
</details>
<details>
<summary>摘要</summary>
Camera 感测器在智能 роботи系统中广泛应用。开发高感知效率的 camera 感测器总是非常重要，以降低功能、存储和相关资源。受最近的可编程感测器和深度光学方法的成功启发，我们设计了一种新的视频压缩感测系统，其中具有空间变化的压缩比率，可以实现比现有的单张图像压缩成像方法更高的成像质量。在这篇文章中，我们还研究了用于可编程感测器的数据传输方法，并评估了通信系统的性能基于传输的感测器数据而不是直接传输感测器数据本身。通常，不同的重建算法采用了应用于高动态范围成像、压缩成像或运动锐化等应用场景。这种任务意识的性能 inspirits 一种Semantic Communication Framework for 可编程感测器。在这种工作中，我们引入了一种基于Policy Gradient的强化学习方法，以实现显式地考虑压缩率和图像扭曲之间的平衡。numerical 结果表明我们的方法在现有基线之上具有超越性。
</details></li>
</ul>
<hr>
<h2 id="Domain-Expansion-via-Network-Adaptation-for-Solving-Inverse-Problems"><a href="#Domain-Expansion-via-Network-Adaptation-for-Solving-Inverse-Problems" class="headerlink" title="Domain Expansion via Network Adaptation for Solving Inverse Problems"></a>Domain Expansion via Network Adaptation for Solving Inverse Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06235">http://arxiv.org/abs/2310.06235</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nebiyou Yismaw, Ulugbek S. Kamilov, M. Salman Asif<br>for:* 这篇论文主要针对的是解决计算成像中的反向问题，提出了一种基于深度学习的方法。methods:* 这种方法可以分为两个类别：一是学习网络将测量转换为信号估计，这种方法容易受到数据分布的变化的影响；二是学习信号的假设，然后使用优化方法来回收信号。results:* 这篇论文通过研究不同类型的域转换的影响，并提出了一种可以灵活地适应不同域的框架，使得已经训练过的网络可以更好地适应不同的数据分布。这种方法在自然图像、MRI和CT重建任务中获得了显著更好的性能和参数效率。<details>
<summary>Abstract</summary>
Deep learning-based methods deliver state-of-the-art performance for solving inverse problems that arise in computational imaging. These methods can be broadly divided into two groups: (1) learn a network to map measurements to the signal estimate, which is known to be fragile; (2) learn a prior for the signal to use in an optimization-based recovery. Despite the impressive results from the latter approach, many of these methods also lack robustness to shifts in data distribution, measurements, and noise levels. Such domain shifts result in a performance gap and in some cases introduce undesired artifacts in the estimated signal. In this paper, we explore the qualitative and quantitative effects of various domain shifts and propose a flexible and parameter efficient framework that adapt pretrained networks to such shifts. We demonstrate the effectiveness of our method for a number of natural image, MRI, and CT reconstructions tasks under domain, measurement model, and noise-level shifts. Our experiments demonstrate that our method provides significantly better performance and parameter efficiency compared to existing domain adaptation techniques.
</details>
<details>
<summary>摘要</summary>
深度学习基本方法可以解决计算成像中的逆问题，这些方法可以分为两个组：（1）学习网络来将测量转换为信号估计，这是知道脆弱的；（2）学习信号的先验来在优化基础上进行恢复。尽管后者的方法也具有很好的成果，但是许多这些方法缺乏对数据分布、测量、和噪声水平的鲁棒性，这会导致性能差异和在某些情况下添加不必要的artefacts到估计的信号中。在这篇论文中，我们研究了不同类型的领域变化的影响，并提出了一种灵活和参数高效的框架，可以适应这些变化。我们通过多个自然图像、MRI和CT重建任务的实验，证明了我们的方法可以在不同的领域、测量模型和噪声水平下提供了显著更好的性能和参数高效性，相比之下现有的领域适应技术。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/10/eess.IV_2023_10_10/" data-id="clp89dooh01bei788clt628ty" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_10_10" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/10/eess.SP_2023_10_10/" class="article-date">
  <time datetime="2023-10-10T08:00:00.000Z" itemprop="datePublished">2023-10-10</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/10/eess.SP_2023_10_10/">eess.SP - 2023-10-10</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Reconfigurable-Intelligent-Surfaces-Enabled-Intra-Cell-Pilot-Reuse-in-Massive-MIMO-Systems"><a href="#Reconfigurable-Intelligent-Surfaces-Enabled-Intra-Cell-Pilot-Reuse-in-Massive-MIMO-Systems" class="headerlink" title="Reconfigurable Intelligent Surfaces-Enabled Intra-Cell Pilot Reuse in Massive MIMO Systems"></a>Reconfigurable Intelligent Surfaces-Enabled Intra-Cell Pilot Reuse in Massive MIMO Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06975">http://arxiv.org/abs/2310.06975</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/josecarlos-marinello/ris-pilot-reuse">https://github.com/josecarlos-marinello/ris-pilot-reuse</a></li>
<li>paper_authors: Jose Carlos Marinello Filho, Taufik Abrao, Ekram Hossain, Amine Mezghani</li>
<li>for: 提高现代大规模多输入多出力（mMIMO）网络的设计中，渠道状态信息（CSI）估算是一个关键的问题。</li>
<li>methods: 我们的主要贡献是一种基于智能控制的扩展智能表面（RIS）的approach，用于内部单元的试验干扰。我们使用统计CSI的知识优化RIS相位Shift基于替换函数Optimization框架，并基于决定性方法来 позициониing RIS。</li>
<li>results: 我们的数值结果表明，提案的方案在 both uplink和downlink传输中可以获得显著的性能改进（相比于其他方案）。<details>
<summary>Abstract</summary>
Channel state information (CSI) estimation is a critical issue in the design of modern massive multiple-input multiple-output (mMIMO) networks. With the increasing number of users, assigning orthogonal pilots to everyone incurs a large overhead that strongly penalizes the system's spectral efficiency (SE). It becomes thus necessary to reuse pilots, giving rise to pilot contamination, a vital performance bottleneck of mMIMO networks. Reusing pilots among the users of the same cell is a desirable operation condition from the perspective of reducing training overheads; however, the intra-cell pilot contamination might worsen due to the users' proximity. Reconfigurable intelligent surfaces (RISs), capable of smartly controlling the wireless channel, can be leveraged for intra-cell pilot reuse. In this paper, our main contribution is a RIS-aided approach for intra-cell pilot reuse and the corresponding channel estimation method. Relying upon the knowledge of only statistical CSI, we optimize the RIS phase shifts based on a manifold optimization framework and the RIS positioning based on a deterministic approach. The extensive numerical results highlight the remarkable performance improvements the proposed scheme achieves (for both uplink and downlink transmissions) compared to other alternatives.
</details>
<details>
<summary>摘要</summary>
To mitigate the impact of pilot contamination, this paper proposes a RIS-aided approach for intra-cell pilot reuse and a corresponding channel estimation method. The approach relies on statistical CSI and optimizes the RIS phase shifts using a manifold optimization framework. Additionally, the RIS positioning is determined using a deterministic approach.The proposed scheme achieves significant performance improvements compared to other alternatives, as demonstrated through extensive numerical results. The improvements are evident in both uplink and downlink transmissions. The use of RISs enables the efficient reuse of pilots, reducing the training overhead and improving the system's overall performance.
</details></li>
</ul>
<hr>
<h2 id="Longitudinal-gOSNR-Monitoring-by-Receiver-side-Digital-Signal-Processing-in-Multi-Span-Optical-Transmission-System"><a href="#Longitudinal-gOSNR-Monitoring-by-Receiver-side-Digital-Signal-Processing-in-Multi-Span-Optical-Transmission-System" class="headerlink" title="Longitudinal gOSNR Monitoring by Receiver-side Digital Signal Processing in Multi-Span Optical Transmission System"></a>Longitudinal gOSNR Monitoring by Receiver-side Digital Signal Processing in Multi-Span Optical Transmission System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06807">http://arxiv.org/abs/2310.06807</a></li>
<li>repo_url: None</li>
<li>paper_authors: Choloong Hahn, Junho Chang, Zhiping Jiang</li>
<li>for: 这个论文是用于估计通信链路上的Global Optical Signal-to-Noise Ratio（gOSNR）的方法。</li>
<li>methods: 该方法使用相关模板方法在Rx端进行了长itudinal的gOSNR估计，不需要在链路中间添加监测设备。</li>
<li>results: 实验表明，该方法可以准确地估计链路上的gOSNR，并且可以在12个Span链路中进行实验验证。<details>
<summary>Abstract</summary>
We propose the world first longitudinal gOSNR estimation by using correlation template method at Rx, without any monitoring devices located in the middle of the link. The proposed method is experimentally demonstrated in a 12-span link with commercial transceiver.
</details>
<details>
<summary>摘要</summary>
我们提出了全球首个长itudinal gOSNR估计方法，使用相关模板方法在Rx中进行估计，无需在链接中间设置监测设备。我们的方法在12 span链接中进行实验，并使用商业转发器。Here's a breakdown of the translation:* 全球首个 (gOSNR) - 全球首个 refers to the fact that this is the first method in the world to estimate gOSNR.* 长itudinal (长itudinal) - 长itudinal refers to the fact that the method is used to estimate the gOSNR in the longitudinal direction.* 估计方法 (估计方法) - 估计方法 refers to the method used to estimate the gOSNR.* 使用相关模板方法 (使用相关模板方法) - 使用相关模板方法 refers to the fact that the method uses a correlation template to estimate the gOSNR.* 在Rx中进行估计 (在Rx中进行估计) - 在Rx中进行估计 refers to the fact that the method estimates the gOSNR at the receiver (Rx) side.* 无需在链接中间设置监测设备 (无需在链接中间设置监测设备) - 无需在链接中间设置监测设备 refers to the fact that the method does not require any monitoring devices to be installed in the middle of the link.* 我们的方法 (我们的方法) - 我们的方法 refers to the fact that this is a method proposed by the speaker.* 在12 span链接中进行实验 (在12 span链接中进行实验) - 在12 span链接中进行实验 refers to the fact that the method was experimentally demonstrated in a 12-span link.* 并使用商业转发器 (并使用商业转发器) - 并使用商业转发器 refers to the fact that the method used commercial transceivers in the experiment.
</details></li>
</ul>
<hr>
<h2 id="Joint-Coding-Modulation-for-Digital-Semantic-Communications-via-Variational-Autoencoder"><a href="#Joint-Coding-Modulation-for-Digital-Semantic-Communications-via-Variational-Autoencoder" class="headerlink" title="Joint Coding-Modulation for Digital Semantic Communications via Variational Autoencoder"></a>Joint Coding-Modulation for Digital Semantic Communications via Variational Autoencoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06690">http://arxiv.org/abs/2310.06690</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yufei Bo, Yiheng Duan, Shuo Shao, Meixia Tao</li>
<li>for: 该论文旨在提高通信效率，通过在接收端传输源消息中最重要的Semantic信息。</li>
<li>methods: 该论文提出了一种基于Variational autoencoder (VAE)的数字Semantic通信框架，该框架学习源数据到离散符号的转换概率，从而避免了数字模ulation的非导数问题。</li>
<li>results: 实验结果表明，该提议的联合编码-模ulation框架在不同的通道条件、传输率和模ulation顺序下，比 sepate设计Semantic编码和模ulation的方法表现更好，并且与analog Semantic通信的性能差距随模ulation顺序的增加而减少。<details>
<summary>Abstract</summary>
Semantic communications have emerged as a new paradigm for improving communication efficiency by transmitting the semantic information of a source message that is most relevant to a desired task at the receiver. Most existing approaches typically utilize neural networks (NNs) to design end-to-end semantic communication systems, where NN-based semantic encoders output continuously distributed signals to be sent directly to the channel in an analog communication fashion. In this work, we propose a joint coding-modulation framework for digital semantic communications by using variational autoencoder (VAE). Our approach learns the transition probability from source data to discrete constellation symbols, thereby avoiding the non-differentiability problem of digital modulation. Meanwhile, by jointly designing the coding and modulation process together, we can match the obtained modulation strategy with the operating channel condition. We also derive a matching loss function with information-theoretic meaning for end-to-end training. Experiments conducted on image semantic communication validate that our proposed joint coding-modulation framework outperforms separate design of semantic coding and modulation under various channel conditions, transmission rates, and modulation orders. Furthermore, its performance gap to analog semantic communication reduces as the modulation order increases while enjoying the hardware implementation convenience.
</details>
<details>
<summary>摘要</summary>
semantic communication 已经emerged as a new paradigm to improve communication efficiency by transmitting the most relevant semantic information of a source message to a desired task at the receiver. Most existing approaches typically use neural networks (NNs) to design end-to-end semantic communication systems, where NN-based semantic encoders output continuously distributed signals to be sent directly to the channel in an analog communication fashion.在这项工作中，我们提出了一个joint coding-modulation框架 для数字semantic communication，使用variational autoencoder (VAE)。我们的方法学习源数据到离散符号的过渡概率，从而避免了数字模调不 diferenciability问题。同时，我们通过结合编码和模调过程的共同设计，可以匹配获得的模调策略与运行的通道条件。我们还 derivate一个匹配损失函数with information-theoretic meaning for end-to-end培训。实验在图像semantic communication中 validate that our proposed joint coding-modulation framework outperforms separate design of semantic coding and modulation under various channel conditions, transmission rates, and modulation orders. 此外，我们发现，当modulation order增加时，我们的性能与analog semantic communication的差距逐渐减小，而且享有硬件实现的便利。
</details></li>
</ul>
<hr>
<h2 id="Near-and-Far-Field-Model-Mismatch-Implications-on-6G-Communications-Localization-and-Sensing"><a href="#Near-and-Far-Field-Model-Mismatch-Implications-on-6G-Communications-Localization-and-Sensing" class="headerlink" title="Near and Far Field Model Mismatch: Implications on 6G Communications, Localization, and Sensing"></a>Near and Far Field Model Mismatch: Implications on 6G Communications, Localization, and Sensing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06604">http://arxiv.org/abs/2310.06604</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmed Elzanaty, Jiuyu Liu, Anna Guerra, Francesco Guidi, Yi Ma, Rahim Tafazolli</li>
<li>for: 本研究旨在探讨6G技术在近场（NF）发射条件下运行的可能性，以及NF模型与远场（FF）模型之间的差异对于通信、地位确定和感知系统的影响。</li>
<li>methods: 本研究使用了一系列的数学模型和数据分析技术，以探讨NF模型与FF模型之间的差异对于系统性能的影响。</li>
<li>results: 研究结果表明，NF模型与FF模型之间的差异可能导致系统性能指标如地位精度、感知可靠性和通信效率的下降。<details>
<summary>Abstract</summary>
The upcoming 6G technology is expected to operate in near-field (NF) radiating conditions thanks to high-frequency and electrically large antenna arrays. While several studies have already addressed this possibility, it is worth noting that NF models introduce heightened complexity, the justification for which is not always evident in terms of performance improvements. Therefore, this paper delves into the implications of the disparity between NF and far-field (FF) models concerning communication, localization, and sensing systems. Such disparity might lead to a degradation of performance metrics like localization accuracy, sensing reliability, and communication efficiency. Through an exploration of the effects arising from the mismatches between NF and FF models, this study seeks to illuminate the challenges confronting system designers and offer valuable insights into the balance between model accuracy, which typically requires a high complexity and achievable performance. To substantiate our perspective, we also incorporate a numerical performance assessment confirming the repercussions of the mismatch between NF and FF models.
</details>
<details>
<summary>摘要</summary>
预计6G技术即将在近场（NF）发射条件下运行，由于高频和电动巨大的天线阵列。虽然已有一些研究对这一可能性进行了评估，但值得注意的是，NF模型会带来更高的复杂性，其性能改善的依据并不总是明显。因此，本文探讨NF和FF模型之间的差异对通信、地位确定和探测系统的影响。这种差异可能会导致本地化精度、探测可靠性和通信效率的下降。通过研究NF和FF模型之间的差异的影响，本文旨在披露系统设计师面临的挑战并提供有价值的思路，以寻求在精度和实现性之间寻找平衡。为证明我们的观点，我们还包括一个数字性能评估，证明NF和FF模型之间的差异带来的后果。
</details></li>
</ul>
<hr>
<h2 id="3D-Non-Stationary-Channel-Measurement-and-Analysis-for-MaMIMO-UAV-Communications"><a href="#3D-Non-Stationary-Channel-Measurement-and-Analysis-for-MaMIMO-UAV-Communications" class="headerlink" title="3D Non-Stationary Channel Measurement and Analysis for MaMIMO-UAV Communications"></a>3D Non-Stationary Channel Measurement and Analysis for MaMIMO-UAV Communications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06579">http://arxiv.org/abs/2310.06579</a></li>
<li>repo_url: None</li>
<li>paper_authors: Achiel Colpaert, Zhuangzhuang Cui, Evgenii Vinogradov, Sofie Pollin</li>
<li>for: 该文章是为了研究无人机（UAV）上的大量多输入多出口（MaMIMO）通信系统的通信物理层设计。</li>
<li>methods: 文章首先设计了一个UAV MaMIMO通信平台，然后使用测试台测量了直升机和64个MaMIMO基站之间的上升链路渠道状态信息（CSI）。</li>
<li>results: 文章通过测量和分析了频率域、时间域和空间域的渠道统计特性，包括能量延迟观测图（PDP）、常规干扰和时域频率域的干扰率。此外，文章还提出了定向角（SA）作为时域站点距离的补充指标，并分析了频率域的干扰宽渠和RMS延迟扩散。最后，文章分析了MaMIMO数组元素之间的空间相关性，以证明MaMIMO-UAV通信系统的空间站点准确性。<details>
<summary>Abstract</summary>
Unmanned aerial vehicles (UAVs) have gained popularity in the communications research community because of their versatility in placement and potential to extend the functions of communication networks. However, there remains still a gap in existing works regarding detailed and measurement-verified air-to-ground (A2G) Massive Multi-Input Multi-Output (MaMIMO) channel characteristics which play an important role in realistic deployment. In this paper, we first design a UAV MaMIMO communication platform for channel acquisition. We then use the testbed to measure uplink Channel State Information (CSI) between a rotary-wing drone and a 64-element MaMIMO base station (BS). For characterization, we focus on multidimensional channel stationarity which is a fundamental metric in communication systems. Afterward, we present measurement results and analyze the channel statistics based on power delay profiles (PDPs) considering space, time, and frequency domains. We propose the stationary angle (SA) as a supplementary metric of stationary distance (SD) in the time domain. We analyze the coherence bandwidth and RMS delay spread for frequency stationarity. Finally, spatial correlations between elements are analyzed to indicate the spatial stationarity of the array. The space-time-frequency channel stationary characterization will benefit the physical layer design of MaMIMO-UAV communications.
</details>
<details>
<summary>摘要</summary>
无人飞行器（UAV）在通信研究领域得到了广泛的应用，主要是因为它们的位置灵活性和通信网络功能扩展的潜力。然而，现有的研究还存在一个空白，即详细和测量确认的空中到地面（A2G）大量多输入多输出（MaMIMO）通道特性的研究。在这篇论文中，我们首先设计了一个无人飞行器MaMIMO通信平台，然后使用测试床测量了旋翼飞机和64个MaMIMO基站（BS）之间的上行频率响应（CSI）。对于Characterization，我们将注重多维度通道Stationarity，这是通信系统中的基本指标之一。接着，我们提供了测量结果和分析频率响应的Channel Statistics，考虑了空间、时间和频率频率域。此外，我们还分析了天线元素之间的空间相关性，以评估MaMIMO通信系统的空间站点性。最后，我们提出了“站点角度”（SA）作为时域站点距离（SD）的补充指标，并分析了宽bandwidth和RMS延迟扩散。空间时间频率通道Stationarity的Characterization将有助于MaMIMO-UAV通信physical层设计。
</details></li>
</ul>
<hr>
<h2 id="ChannelComp-A-General-Method-for-Computation-by-Communications"><a href="#ChannelComp-A-General-Method-for-Computation-by-Communications" class="headerlink" title="ChannelComp: A General Method for Computation by Communications"></a>ChannelComp: A General Method for Computation by Communications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06532">http://arxiv.org/abs/2310.06532</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saeed Razavikia, José Mairton Barros Da Silva Júnior, Carlo Fischione</li>
<li>For: The paper proposes a new digital channel computing method named ChannelComp, which can use digital as well as analog modulations, and can achieve arbitrary function computation over-the-air.* Methods: The proposed method uses a feasibility optimization problem to ascertain the optimal modulation for computing arbitrary functions over-the-air, and proposes pre-coders to adapt existing digital modulation schemes for computing the function over the multiple access channel.* Results: The simulation results show that ChannelComp outperforms AirComp, particularly for product functions, with more than 10 dB improvement of the computation error.Here is the text in Simplified Chinese:* For: 本文提出了一种新的数字通道计算方法 named ChannelComp，可以使用数字和分布式模ulation，实现空中进行任意函数计算。* Methods: 提议的方法使用一个可行优化问题确定空中计算任意函数的最佳模ulation，并提出适应现有数字模ulation schemes来计算函数 sobre 多重存取通道。* Results: 实验结果表明，ChannelComp 比 AirComp 更高效，特别是对乘积函数，计算错误下降了More than 10 dB。<details>
<summary>Abstract</summary>
Over-the-air computation (AirComp) is a well-known technique by which several wireless devices transmit by analog amplitude modulation to achieve a sum of their transmit signals at a common receiver. The underlying physical principle is the superposition property of the radio waves. Since such superposition is analog and in amplitude, it is natural that AirComp uses analog amplitude modulations. Unfortunately, this is impractical because most wireless devices today use digital modulations. It would be highly desirable to use digital communications because of their numerous benefits, such as error correction, synchronization, acquisition of channel state information, and widespread use. However, when we use digital modulations for AirComp, a general belief is that the superposition property of the radio waves returns a meaningless overlapping of the digital signals. In this paper, we break through such beliefs and propose an entirely new digital channel computing method named ChannelComp, which can use digital as well as analog modulations. We propose a feasibility optimization problem that ascertains the optimal modulation for computing arbitrary functions over-the-air. Additionally, we propose pre-coders to adapt existing digital modulation schemes for computing the function over the multiple access channel. The simulation results verify the superior performance of ChannelComp compared to AirComp, particularly for the product functions, with more than 10 dB improvement of the computation error.
</details>
<details>
<summary>摘要</summary>
频段计算（AirComp）是一种已知的技术，多个无线设备通过模拟幅度模ulation来实现一个共同接收器上的 signals的总和。物理原理是无线波的积加性质。由于这种积加是Analog的，因此AirComp通常使用Analog幅度模ulation。然而，这是不实用的，因为今天大多数无线设备使用数字模ulation。使用数字模ulation可以获得许多优点，如错误恢复、同步、通道状态信息获取和广泛使用。但是，当用数字模ulation进行频段计算时，通常认为无线波的积加性质返回无意义的数字信号的重叠。在这篇论文中，我们突破这些信念，并提出一种全新的数字通道计算方法，名为ChannelComp，可以使用数字和Analog模ulation。我们提出一个可行优化问题，以确定在空中计算任意函数的最佳模ulation。此外，我们还提出适应器，以适应现有的数字模ulation方案来计算函数在多接收器通道上。实验结果表明ChannelComp比AirComp表现更优，特别是对于乘法函数，错误率下降了10dB以上。
</details></li>
</ul>
<hr>
<h2 id="Plane-Constraints-Aided-Multi-Vehicle-Cooperative-Positioning-Using-Factor-Graph-Optimization"><a href="#Plane-Constraints-Aided-Multi-Vehicle-Cooperative-Positioning-Using-Factor-Graph-Optimization" class="headerlink" title="Plane Constraints Aided Multi-Vehicle Cooperative Positioning Using Factor Graph Optimization"></a>Plane Constraints Aided Multi-Vehicle Cooperative Positioning Using Factor Graph Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06414">http://arxiv.org/abs/2310.06414</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Zhuang, Hongbo Zhao</li>
<li>for: 提高 vehicular 应用中的定位可靠性和精度，通过车辆间距测量和数据交换来实现协同定位技术。</li>
<li>methods: 利用协同定位技术，使用协同获得的位置数据构建道路平面，并将平面参数引入CP算法以提供约束。使用现有的FGO算法结合GNSS raw数据和车辆间距测量数据进行优化。</li>
<li>results: 提高了定位性能，特别是在车辆间距测量受到中断时表现出色。<details>
<summary>Abstract</summary>
The development of vehicle-to-vehicle (V2V) communication facil-itates the study of cooperative positioning (CP) techniques for vehicular applications. The CP methods can improve the posi-tioning availability and accuracy by inter-vehicle ranging and data exchange between vehicles. However, the inter-vehicle rang-ing can be easily interrupted due to many factors such as obsta-cles in-between two cars. Without inter-vehicle ranging, the other cooperative data such as vehicle positions will be wasted, leading to performance degradation of range-based CP methods. To fully utilize the cooperative data and mitigate the impact of inter-vehicle ranging loss, a novel cooperative positioning method aided by plane constraints is proposed in this paper. The positioning results received from cooperative vehicles are used to construct the road plane for each vehicle. The plane parameters are then introduced into CP scheme to impose constraints on positioning solutions. The state-of-art factor graph optimization (FGO) algo-rithm is employed to integrate the plane constraints with raw data of Global Navigation Satellite Systems (GNSS) as well as inter-vehicle ranging measurements. The proposed CP method has the ability to resist the interruptions of inter-vehicle ranging since the plane constraints are computed by just using position-related data. A vehicle can still benefit from the position data of cooperative vehicles even if the inter-vehicle ranging is unavaila-ble. The experimental results indicate the superiority of the pro-posed CP method in positioning performance over the existing methods, especially when the inter-ranging interruptions occur.
</details>
<details>
<summary>摘要</summary>
发展交通自动化技术的车辆到车辆通信（V2V）技术为汽车应用增加了可靠性和精度。但是，在两辆车之间的距离测量中，可能会有各种障碍物，导致距离测量中断。在没有距离测量的情况下，其他合作数据，如车辆位置，将被浪费，从而导致距离基于CP方法的性能下降。为了充分利用合作数据和减少距离测量中断的影响，本文提出了一种基于平面约束的新型合作定位方法。received from cooperative vehicles are used to construct the road plane for each vehicle. The plane parameters are then introduced into CP scheme to impose constraints on positioning solutions. The state-of-art factor graph optimization (FGO) algorithm is employed to integrate the plane constraints with raw data of Global Navigation Satellite Systems (GNSS) as well as inter-vehicle ranging measurements. The proposed CP method has the ability to resist the interruptions of inter-vehicle ranging since the plane constraints are computed by just using position-related data. A vehicle can still benefit from the position data of cooperative vehicles even if the inter-vehicle ranging is unavailable. The experimental results indicate the superiority of the proposed CP method in positioning performance over the existing methods, especially when the inter-ranging interruptions occur.Here's the translation in Traditional Chinese:随着交通自动化技术的发展，车辆通信（V2V）技术对汽车应用增加了可靠性和精度。但是，在两辆车之间的距离测量中，可能会有各种障碍物，导致距离测量中断。在没有距离测量的情况下，其他合作数据，如车辆位置，将被浪费，从而导致距离基于CP方法的性能下降。为了充分利用合作数据和减少距离测量中断的影响，本文提出了一种基于平面约束的新型合作定位方法。received from cooperative vehicles are used to construct the road plane for each vehicle. The plane parameters are then introduced into CP scheme to impose constraints on positioning solutions. The state-of-art factor graph optimization (FGO) algorithm is employed to integrate the plane constraints with raw data of Global Navigation Satellite Systems (GNSS) as well as inter-vehicle ranging measurements. The proposed CP method has the ability to resist the interruptions of inter-vehicle ranging since the plane constraints are computed by just using position-related data. A vehicle can still benefit from the position data of cooperative vehicles even if the inter-vehicle ranging is unavailable. The experimental results indicate the superiority of the proposed CP method in positioning performance over the existing methods, especially when the inter-ranging interruptions occur.
</details></li>
</ul>
<hr>
<h2 id="ISAC-4D-Imaging-System-Based-on-5G-Downlink-Millimeter-Wave-Signal"><a href="#ISAC-4D-Imaging-System-Based-on-5G-Downlink-Millimeter-Wave-Signal" class="headerlink" title="ISAC 4D Imaging System Based on 5G Downlink Millimeter Wave Signal"></a>ISAC 4D Imaging System Based on 5G Downlink Millimeter Wave Signal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06401">http://arxiv.org/abs/2310.06401</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/MrHaobolu/ISAC_4D_IMaging">https://github.com/MrHaobolu/ISAC_4D_IMaging</a></li>
<li>paper_authors: Bohao Lu, Zhiqing Wei, Lin Wang, Ruiyun Zhang, Zhingyong Feng</li>
<li>for: 本研究旨在提出一种基于5G下链路干扰的4D（3D坐标、速度）成像方法，以提高ISAC场景中的感知精度。</li>
<li>methods: 本方法基于2D-FFT与2D-MUSIC技术，使用标准5G下链路 millimeter波信号进行感知。为了提高感知精度，我们还提出了一种基于MIMO虚 aperature技术的发射天线元件安排方案。</li>
<li>results: 我们的 simulations 表明，我们的提议的方法可以提供更好的成像结果。代码可以在<a target="_blank" rel="noopener" href="https://github.com/MrHaobolu/ISAC_4D_Imaging.git%E4%B8%AD%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/MrHaobolu/ISAC_4D_Imaging.git中获取。</a><details>
<summary>Abstract</summary>
Integrated Sensing and Communication(ISAC) has become a key technology for the 5th generation (5G) and 6th generation (6G) wireless communications due to its high spectrum utilization efficiency. Utilizing infrastructure such as 5G Base Stations (BS) to realize environmental imaging and reconstruction is important for promoting the construction of smart cities. Current 4D imaging methods utilizing Frequency Modulated Continuous Wave (FMCW) based Fast Fourier Transform (FFT) are not suitable for ISAC scenarios due to the higher bandwidth occupation and lower resolution. We propose a 4D (3D-Coordinates, Velocity) imaging method with higher sensing accuracy based on 2D-FFT with 2D-MUSIC utilizing standard 5G Downlink (DL) millimeter wave (mmWave) signals. To improve the sensing precision we also design a transceiver antenna array element arrangement scheme based on MIMO virtual aperture technique. We further propose a target detection algorithm based on multi-dimensional Constant False Alarm (CFAR) detection, which optimizes the ISAC imaging signal processing flow and reduces the computational pressure of signal processing. Simulation results show that our proposed method has better imaging results. The code is publicly available at https://github.com/MrHaobolu/ISAC\_4D\_IMaging.git.
</details>
<details>
<summary>摘要</summary>
Integrated Sensing and Communication(ISAC) 已成为 fifth generation (5G) 和 sixth generation (6G) 无线通信技术的关键因素，具有高频率使用效率。通过使用 5G 基站 (BS) 实现环境成像和重建是推动智能城市建设的关键。现有的 4D 成像方法使用 Frequency Modulated Continuous Wave (FMCW) 基于 Fast Fourier Transform (FFT) 不适合 ISAC 场景，因为它们占用更高频率和分辨率较低。我们提议一种基于 2D-FFT 和 2D-MUSIC 的 4D (3D-坐标、速度) 成像方法，可以提高成像精度。我们还设计了一种基于 MIMO 虚拟天线技术的天线元件顺序安排方案，以提高探测精度。此外，我们还提出了一种基于多维度 Constant False Alarm (CFAR) 检测算法的目标检测算法，可以优化 ISAC 成像信号处理流程，并减少信号处理的计算压力。实验结果表明，我们的提议方法可以获得更好的成像效果。代码可以在 <https://github.com/MrHaobolu/ISAC_4D_Imaging.git> 上获取。
</details></li>
</ul>
<hr>
<h2 id="Mutual-Information-Metrics-for-Uplink-MIMO-OFDM-Integrated-Sensing-and-Communication-System"><a href="#Mutual-Information-Metrics-for-Uplink-MIMO-OFDM-Integrated-Sensing-and-Communication-System" class="headerlink" title="Mutual Information Metrics for Uplink MIMO-OFDM Integrated Sensing and Communication System"></a>Mutual Information Metrics for Uplink MIMO-OFDM Integrated Sensing and Communication System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06382">http://arxiv.org/abs/2310.06382</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinghui Piao, Zhiqing Wei, Xin Yuan, Xiaoyu Yang, Huici Wu, Zhiyong Feng</li>
<li>for: 这篇论文主要为了探讨一种基于多输入多输出分复数字化多址（MIMO-OFDM）技术的上传整合感知通信（ISAC）系统。</li>
<li>methods: 本论文使用了诱导函数（MI）作为整合通信和感知性能的统一指标，并在交互性方面 derivation 上下限。然后，通过最大化权重和感知通信MI的加权和积分来优化ISAC波形。</li>
<li>results:  Monte Carlo 仿真结果显示，与其他波形优化方案相比，提议的ISAC方案在总性性能方面表现最佳。<details>
<summary>Abstract</summary>
As the uplink sensing has the advantage of easy implementation, it attracts great attention in integrated sensing and communication (ISAC) system. This paper presents an uplink ISAC system based on multi-input multi-output orthogonal frequency division multiplexing (MIMO-OFDM) technology. The mutual information (MI) is introduced as a unified metric to evaluate the performance of communication and sensing. In this paper, firstly, the upper and lower bounds of communication and sensing MI are derived in details based on the interaction between communication and sensing. And the ISAC waveform is optimized by maximizing the weighted sum of sensing and communication MI. The Monte Carlo simulation results show that, compared with other waveform optimization schemes, the proposed ISAC scheme has the best overall performance.
</details>
<details>
<summary>摘要</summary>
《这个发送感知系统（ISAC）基于多入多出多频分配（MIMO-OFDM）技术，具有易于实现的优点。这篇论文首先 derive了通信和感知之间的互动的上下限，然后将ISAC波形优化为最大化混合通信和感知的干扰情况。实验结果显示，与其他波形优化方案相比，提案的ISAC方案在总性能方面表现最佳。》Here's the translation breakdown:* 这个 (tī gò) - this* 发送感知系统 (fā sòng gǎn jí xìng zhì) - uplink sensing and communication system* 基于 (jī yú) - based on* 多入多出多频分配 (duō rù duō chū duō fēn fāng) - multi-input multi-output orthogonal frequency division multiplexing* 技术 (jì shù) - technology* 具有 (gù yǒu) - has* 易于 (qǐ yú) - easy* 实现 (shí jì) - implementation* 这篇 (zhè běn) - this paper* 论文 (lùn wén) - paper* 首先 (chū xiān) - firstly* derive (dì zhì) - derive* 通信 (tōng xìn) - communication* 感知 (gǎn jí) - sensing* 之间 (zhī jiān) - between* 上下限 (shàng xià jiàn) - upper and lower bounds* 然后 (rán hái) - then* 将ISAC波形 (jiāng ISAC bā xíng) - optimize the ISAC waveform* 优化 (yōu zuò) - optimize* 为 (wèi) - for* 最大化 (zuò dà jiā) - maximize* 混合 (hù hé) - mixed* 通信 (tōng xìn) - communication* 感知 (gǎn jí) - sensing* 干扰 (gān jí) - interference* 情况 (qíng jì) - situation* 实验 (shí yan) - experiment* 结果 (jié guō) - results* 显示 (xiǎn shi) - show* 与 (yǔ) - with* 其他 (qí tè) - other* 波形 (bā xíng) - waveform* 优化方案 (yōu zuò fāng àn) - optimization schemes* 相比 (xiāng bǐ) - compared with* 表现 (biǎo xiǎng) - performance* 最佳 (zuò jiā) - best* 总性能 (zǒng xìng néng) - overall performance
</details></li>
</ul>
<hr>
<h2 id="HoloFed-Environment-Adaptive-Positioning-via-Multi-band-Reconfigurable-Holographic-Surfaces-and-Federated-Learning"><a href="#HoloFed-Environment-Adaptive-Positioning-via-Multi-band-Reconfigurable-Holographic-Surfaces-and-Federated-Learning" class="headerlink" title="HoloFed: Environment-Adaptive Positioning via Multi-band Reconfigurable Holographic Surfaces and Federated Learning"></a>HoloFed: Environment-Adaptive Positioning via Multi-band Reconfigurable Holographic Surfaces and Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06336">http://arxiv.org/abs/2310.06336</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingzhi Hu, Zhe Chen, Tianyue Zheng, Robert Schober, Jun Luo</li>
<li>for: 高精度环境适应用户位置服务（High-Precision Environment-Adaptive User Positioning Service）</li>
<li>methods: 多频环境适应 MB-RHS 和 federated learning（Multi-Band Reconfigurable Holographic Surfaces and Federated Learning）</li>
<li>results: 57% 低于基准值的位置误差变量（57% lower positioning error variance compared to a beam-scanning baseline）<details>
<summary>Abstract</summary>
Positioning is an essential service for various applications and is expected to be integrated with existing communication infrastructures in 5G and 6G. Though current Wi-Fi and cellular base stations (BSs) can be used to support this integration, the resulting precision is unsatisfactory due to the lack of precise control of the wireless signals. Recently, BSs adopting reconfigurable holographic surfaces (RHSs) have been advocated for positioning as RHSs' large number of antenna elements enable generation of arbitrary and highly-focused signal beam patterns. However, existing designs face two major challenges: i) RHSs only have limited operating bandwidth, and ii) the positioning methods cannot adapt to the diverse environments encountered in practice. To overcome these challenges, we present HoloFed, a system providing high-precision environment-adaptive user positioning services by exploiting multi-band(MB)-RHS and federated learning (FL). For improving the positioning performance, a lower bound on the error variance is obtained and utilized for guiding MB-RHS's digital and analog beamforming design. For better adaptability while preserving privacy, an FL framework is proposed for users to collaboratively train a position estimator, where we exploit the transfer learning technique to handle the lack of position labels of the users. Moreover, a scheduling algorithm for the BS to select which users train the position estimator is designed, jointly considering the convergence and efficiency of FL. Our simulation results confirm that HoloFed achieves a 57% lower positioning error variance compared to a beam-scanning baseline and can effectively adapt to diverse environments.
</details>
<details>
<summary>摘要</summary>
positioning是一种重要的服务，用于各种应用程序，预计在5G和6G中与现有的通信基础设施集成。尽管当前的Wi-Fi和mobile基站可以用来支持这种集成，但由于无线信号的精度控制的缺失，所得到的精度不够高。随后，使用可重新配置的干扰表面(RHS)的基站被提议用于位置服务，因为RHS的大量天线元素可以生成自由的信号扫描方式。然而，现有的设计遇到了两个主要挑战：一是RHS只有有限的运作频率，二是位置方法无法适应实际中遇到的多样化环境。为了解决这些挑战，我们提出了HoloFed系统，该系统通过多频（MB）-RHS和联合学习（FL）技术提供高精度环境适应用户位置服务。为了提高位置性能，我们 obtener un lower bound on the error variance and utilizarlo para guiar el diseño de la formación digital y analógica de MB-RHS。另外，我们提出了一个用户协作 trains a position estimator的FL框架，其中我们利用了传输学习技术来处理用户没有位置标签的问题。此外，我们设计了一种BS选择用户训练位置估计器的分配算法，同时考虑了FL的 converges和效率。our simulation results confirm that HoloFed achieves a 57% lower positioning error variance compared to a beam-scanning baseline and can effectively adapt to diverse environments.
</details></li>
</ul>
<hr>
<h2 id="Streaming-Probabilistic-PCA-for-Missing-Data-with-Heteroscedastic-Noise"><a href="#Streaming-Probabilistic-PCA-for-Missing-Data-with-Heteroscedastic-Noise" class="headerlink" title="Streaming Probabilistic PCA for Missing Data with Heteroscedastic Noise"></a>Streaming Probabilistic PCA for Missing Data with Heteroscedastic Noise</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06277">http://arxiv.org/abs/2310.06277</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kyle Gilman, David Hong, Jeffrey A. Fessler, Laura Balzano</li>
<li>for: This paper aims to develop a novel algorithm for principal component analysis (PCA) in streaming data with missing entries and heteroscedastic noise.</li>
<li>methods: The proposed algorithm, called Streaming Heteroscedastic Algorithm for PCA (SHASTA-PCA), uses a stochastic alternating expectation maximization approach to jointly learn the low-rank latent factors and the unknown noise variances from streaming data.</li>
<li>results: Numerical experiments show that SHASTA-PCA outperforms state-of-the-art streaming PCA algorithms in the heteroscedastic setting, and it is applied to highly-heterogeneous real data from astronomy.Here is the summary in Traditional Chinese:</li>
<li>for: 本文目的是发展一个流动数据中存在遗传和不均势噪声的快速几何主成分分析（PCA）算法。</li>
<li>methods: 提案的算法为流动几何算法（SHASTA-PCA），使用随机交互预测最大化方法，同时学习流动数据中的低维特征和未知噪声方差。</li>
<li>results: 数据实验显示，SHASTA-PCA在不均势设定下超过现有的流动PCA算法，并应用于天文学中高度不均势的实际数据。<details>
<summary>Abstract</summary>
Streaming principal component analysis (PCA) is an integral tool in large-scale machine learning for rapidly estimating low-dimensional subspaces of very high dimensional and high arrival-rate data with missing entries and corrupting noise. However, modern trends increasingly combine data from a variety of sources, meaning they may exhibit heterogeneous quality across samples. Since standard streaming PCA algorithms do not account for non-uniform noise, their subspace estimates can quickly degrade. On the other hand, the recently proposed Heteroscedastic Probabilistic PCA Technique (HePPCAT) addresses this heterogeneity, but it was not designed to handle missing entries and streaming data, nor does it adapt to non-stationary behavior in time series data. This paper proposes the Streaming HeteroscedASTic Algorithm for PCA (SHASTA-PCA) to bridge this divide. SHASTA-PCA employs a stochastic alternating expectation maximization approach that jointly learns the low-rank latent factors and the unknown noise variances from streaming data that may have missing entries and heteroscedastic noise, all while maintaining a low memory and computational footprint. Numerical experiments validate the superior subspace estimation of our method compared to state-of-the-art streaming PCA algorithms in the heteroscedastic setting. Finally, we illustrate SHASTA-PCA applied to highly-heterogeneous real data from astronomy.
</details>
<details>
<summary>摘要</summary>
流动主成分分析（PCA）是大规模机器学习中不可或缺的工具，用于快速估计高维数据中的低维子空间，该数据可能含有缺失项和噪声。然而，当现代趋势尝试将不同来源的数据组合起来时，这些数据可能会具有不同的质量水平。标准的流动PCA算法不会考虑非均匀噪声，因此其子空间估计可能很快地下降。相反，最近提出的随机概率PCA技术（HePPCAT）可以处理这种不同质量的数据，但它没有考虑流动数据和缺失项。这篇论文提出了流动随机预期最大化算法（SHASTA-PCA），用于融合这些因素。 SHASTA-PCA使用了随机 alternate expectation maximization方法，同时学习流动数据中缺失项和不同质量噪声的低维 latent factor和未知噪声方差，并保持低的内存和计算负担。数学实验表明，我们的方法在不同质量的噪声情况下比state-of-the-art streaming PCA算法更好地估计子空间。最后，我们示例了 SHASTA-PCA 应用于天文学中的高度不同质量数据。
</details></li>
</ul>
<hr>
<h2 id="Multiscale-information-fusion-for-fault-detection-and-localization-of-battery-energy-storage-systems"><a href="#Multiscale-information-fusion-for-fault-detection-and-localization-of-battery-energy-storage-systems" class="headerlink" title="Multiscale information fusion for fault detection and localization of battery energy storage systems"></a>Multiscale information fusion for fault detection and localization of battery energy storage systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08606">http://arxiv.org/abs/2310.08606</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peng Wei, Han-Xiong Li</li>
<li>for: 本研究旨在提出一种多尺度信息融合方法，用于检测和定位锂离子电池系统（BESS）中的热异常。</li>
<li>methods: 本研究提出了一种基于离差 entropy 的异常检测方法，用于检测锂离子电池系统中的热异常。异常检测方法包括用离差 entropy 测定分布变量中的异常，以及用空间和时间 entropy 测定分布变量中的异常。</li>
<li>results: 实验结果表明，提出的多尺度检测指标可以快速和准确地检测锂离子电池系统中的短路异常，并且可以准确地定位异常的电池单元。<details>
<summary>Abstract</summary>
Battery energy storage system (BESS) has great potential to combat global warming. However, internal abnormalities in the BESS may develop into thermal runaway, causing serious safety incidents. In this study, the multiscale information fusion is proposed for thermal abnormality detection and localization in BESSs. We introduce the concept of dissimilarity entropy as a means to identify anomalies for lumped variables, whereas spatial and temporal entropy measures are presented for the detection of anomalies for distributed variables. Through appropriate parameter optimization, these three entropy functions are integrated into the comprehensive multiscale detection index, which outperforms traditional single-scale detection methods. The proposed multiscale statistic has good interpretability in terms of system energy concentration. Battery system internal short circuit (ISC) experiments have demonstrated that our proposed method can swiftly identify ISC abnormalities and accurately pinpoint the problematic battery cells.
</details>
<details>
<summary>摘要</summary>
锂离子电池能量存储系统（BESS）具有潜在的气候变化防控潜力。然而，BESS中的内部异常可能会导致热跑away，引起严重的安全事件。本研究提议了多级信息融合，用于探测和定位BESS中的热异常。我们引入了异常指标基于杂合变量的异同熵概念，而空间和时间异同熵度量则用于探测分布变量上的热异常。通过合适的参数优化，这三种异同熵函数被集成为了全面的多级检测指标，超越了传统单级检测方法。提议的多级统计具有系统能量集中的良好解释性。锂离子电池内部短路（ISC）实验表明，我们提议的方法可快速检测到ISC异常情况，并准确地确定问题的电池单元。
</details></li>
</ul>
<hr>
<h2 id="Rate-Compatible-LDPC-Neural-Decoding-Network-A-Multi-Task-Learning-Approach"><a href="#Rate-Compatible-LDPC-Neural-Decoding-Network-A-Multi-Task-Learning-Approach" class="headerlink" title="Rate Compatible LDPC Neural Decoding Network: A Multi-Task Learning Approach"></a>Rate Compatible LDPC Neural Decoding Network: A Multi-Task Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06256">http://arxiv.org/abs/2310.06256</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yukun Cheng, Wei Chen, Lun Li, Bo Ai</li>
<li>for: 提高LDPC编码器的decoding性能</li>
<li>methods: 使用多任务学习和Structure of raptor-like LDPC codes</li>
<li>results: 可以处理多个代码速率，无需牺牲帧错误率性能<details>
<summary>Abstract</summary>
Deep learning based decoding networks have shown significant improvement in decoding LDPC codes, but the neural decoders are limited by rate-matching operations such as puncturing or extending, thus needing to train multiple decoders with different code rates for a variety of channel conditions. In this correspondence, we propose a Multi-Task Learning based rate-compatible LDPC ecoding network, which utilizes the structure of raptor-like LDPC codes and can deal with multiple code rates. In the proposed network, different portions of parameters are activated to deal with distinct code rates, which leads to parameter sharing among tasks. Numerical experiments demonstrate the effectiveness of the proposed method. Training the specially designed network under multiple code rates makes the decoder compatible with multiple code rates without sacrificing frame error rate performance.
</details>
<details>
<summary>摘要</summary>
深度学习基于解码网络已经显著提高了LDPC码解码性能，但神经解码器受限于比率匹配操作如抽割或扩展，因此需要训练多个解码器以适应不同的通道条件。在这封通信中，我们提出了基于多任务学习的Rate-Compatible LDPC编码网络，该网络利用了飞行鸟式LDPC码的结构，可以处理多个代码速率。在我们的提案中，不同的参数部分会在不同的代码速率下被激活，从而实现参数共享。数字实验证明我们的方法的有效性。通过特地设计的网络在多个代码速率下进行训练，使解码器与多个代码速率兼容无需牺牲帧错误率性能。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/10/eess.SP_2023_10_10/" data-id="clp89doqc01fri7884xgn408d" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_10_09" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/09/cs.SD_2023_10_09/" class="article-date">
  <time datetime="2023-10-09T15:00:00.000Z" itemprop="datePublished">2023-10-09</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/09/cs.SD_2023_10_09/">cs.SD - 2023-10-09</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="JVNV-A-Corpus-of-Japanese-Emotional-Speech-with-Verbal-Content-and-Nonverbal-Expressions"><a href="#JVNV-A-Corpus-of-Japanese-Emotional-Speech-with-Verbal-Content-and-Nonverbal-Expressions" class="headerlink" title="JVNV: A Corpus of Japanese Emotional Speech with Verbal Content and Nonverbal Expressions"></a>JVNV: A Corpus of Japanese Emotional Speech with Verbal Content and Nonverbal Expressions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06072">http://arxiv.org/abs/2310.06072</a></li>
<li>repo_url: None</li>
<li>paper_authors: Detai Xin, Junfeng Jiang, Shinnosuke Takamichi, Yuki Saito, Akiko Aizawa, Hiroshi Saruwatari</li>
<li>for: The paper is written for researchers and developers working on emotional speech synthesis and related areas, as well as those interested in exploring the use of large language models for script generation.</li>
<li>methods: The paper proposes an automatic script generation method that uses a large language model (ChatGPT) and prompt engineering to produce emotional scripts with nonverbal vocalizations (NVs).</li>
<li>results: The paper demonstrates the effectiveness of the proposed method by showing that the generated scripts have better phoneme coverage and emotion recognizability than previous Japanese emotional speech corpora, and also highlights the challenges of synthesizing emotional speech with NVs.<details>
<summary>Abstract</summary>
We present the JVNV, a Japanese emotional speech corpus with verbal content and nonverbal vocalizations whose scripts are generated by a large-scale language model. Existing emotional speech corpora lack not only proper emotional scripts but also nonverbal vocalizations (NVs) that are essential expressions in spoken language to express emotions. We propose an automatic script generation method to produce emotional scripts by providing seed words with sentiment polarity and phrases of nonverbal vocalizations to ChatGPT using prompt engineering. We select 514 scripts with balanced phoneme coverage from the generated candidate scripts with the assistance of emotion confidence scores and language fluency scores. We demonstrate the effectiveness of JVNV by showing that JVNV has better phoneme coverage and emotion recognizability than previous Japanese emotional speech corpora. We then benchmark JVNV on emotional text-to-speech synthesis using discrete codes to represent NVs. We show that there still exists a gap between the performance of synthesizing read-aloud speech and emotional speech, and adding NVs in the speech makes the task even harder, which brings new challenges for this task and makes JVNV a valuable resource for relevant works in the future. To our best knowledge, JVNV is the first speech corpus that generates scripts automatically using large language models.
</details>
<details>
<summary>摘要</summary>
我们介绍JVNV，一个日本语言情感演讲集合，包含语言内容和非语言声音表达的脚本，这些脚本由大规模语言模型生成。现有的情感演讲集合缺乏不仅有正确的情感脚本，还缺乏非语言声音表达（NV），这些表达是 spoken language 中表达情感的重要组成部分。我们提出一种自动脚本生成方法，通过提供带有情感方向和非语言声音表达的缓解词汇，使用 ChatGPT 的提问工程来生成情感脚本。我们选择了514个脚本，以保证干扰词汇的覆盖率均匀。我们示出JVNV的效果，并证明JVNV在情感演讲Synthesize 中的表达效果更好，而且JVNV 的情感可识别性也更高。然后，我们对JVNV进行了情感文本到语音合成测试，并发现在添加NV后，合成语音的任务变得更加困难，这带来了新的挑战。根据我们所知，JVNV 是第一个使用大型语言模型自动生成的语音演讲集合。
</details></li>
</ul>
<hr>
<h2 id="Audio-compression-assisted-feature-extraction-for-voice-replay-attack-detection"><a href="#Audio-compression-assisted-feature-extraction-for-voice-replay-attack-detection" class="headerlink" title="Audio compression-assisted feature extraction for voice replay attack detection"></a>Audio compression-assisted feature extraction for voice replay attack detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05813">http://arxiv.org/abs/2310.05813</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiangyu Shi, Yuhao Luo, Li Wang, Haorui He, Hao Li, Lei Wang, Zhizheng Wu</li>
<li>for: 本研究旨在提出一种Feature Extraction Approach，用于检测返回攻击。</li>
<li>methods: 该方法使用音频压缩，以提取返回攻击中的频谱信息。</li>
<li>results: 经过大量数据增强和三种分类器的测试，该方法在ASVspoof 2021的物理访问（PA）集上达到了最低的EER值为22.71%。<details>
<summary>Abstract</summary>
Replay attack is one of the most effective and simplest voice spoofing attacks. Detecting replay attacks is challenging, according to the Automatic Speaker Verification Spoofing and Countermeasures Challenge 2021 (ASVspoof 2021), because they involve a loudspeaker, a microphone, and acoustic conditions (e.g., background noise). One obstacle to detecting replay attacks is finding robust feature representations that reflect the channel noise information added to the replayed speech. This study proposes a feature extraction approach that uses audio compression for assistance. Audio compression compresses audio to preserve content and speaker information for transmission. The missed information after decompression is expected to contain content- and speaker-independent information (e.g., channel noise added during the replay process). We conducted a comprehensive experiment with a few data augmentation techniques and 3 classifiers on the ASVspoof 2021 physical access (PA) set and confirmed the effectiveness of the proposed feature extraction approach. To the best of our knowledge, the proposed approach achieves the lowest EER at 22.71% on the ASVspoof 2021 PA evaluation set.
</details>
<details>
<summary>摘要</summary>
<<SYS>>输入文本转换为简化字体中文。<</SYS>>声重播攻击是voice spoofing最有效 simplest的一种，但检测声重播攻击具有挑战性，根据2021年自动说话人识别骗ichi Spoofing和Countermeasures Challenge (ASVspoof 2021)，因为它们需要外壳speaker、 Microphone和听录条件（如背景噪音）。一个检测声重播攻击的障碍是找到Robust的特征表示，以反映在重播过程中添加的频率噪音信息。本研究提议一种特征提取方法，利用音频压缩。音频压缩将音频压缩到保持内容和说话人信息，以便进行传输。压缩后的信息缺失将包含内容和说话人独立的信息（如在重播过程中添加的频率噪音）。我们在一些数据增强技术和3种分类器的帮助下，对ASVspoof 2021physical access（PA）集进行了全面的实验，并证实了提议的特征提取方法的效iveness。根据我们所知，该方法在ASVspoof 2021 PA评估集上的最低EER为22.71%。
</details></li>
</ul>
<hr>
<h2 id="Technocratic-model-of-the-human-auditory-system"><a href="#Technocratic-model-of-the-human-auditory-system" class="headerlink" title="Technocratic model of the human auditory system"></a>Technocratic model of the human auditory system</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05639">http://arxiv.org/abs/2310.05639</a></li>
<li>repo_url: None</li>
<li>paper_authors: M. V. Semotiuk, A. V. Palagin</li>
<li>for: 这项研究探讨了生物体内耳膜中的横滤频和横立波现象。</li>
<li>methods: 研究者使用了一种技术化的方法，通过分析耳膜的形状和表面非均匀性，来模拟生物体内耳系统的physical processes。</li>
<li>results: 研究结果表明，耳膜的径向振荡和横立波是由耳膜的形状和表面非均匀性引起的，并且 Scala media作为信息采集和增强系统，在耳膜旋转轴上具有重要作用。<details>
<summary>Abstract</summary>
In this work, we investigate the phenomenon of transverse resonance and transverse standing waves that occur within the cochlea of living organisms. It is demonstrated that the predisposing factor for their occurrence is the cochlear shape, which resembles a conical acoustic tube coiled into a spiral and exhibits non-uniformities on its internal surface. This cochlear structure facilitates the analysis of constituent sound signals akin to a spectrum analyzer, with a corresponding interpretation of the physical processes occurring in the auditory system. Additionally, we conclude that the cochlear duct's scala media, composed of a system of membranes and the organ of Corti, functions primarily as an information collection and amplification system along the cochlear spiral. Collectively, these findings enable the development of a novel, highly realistic wave model of the auditory system in living organisms based on a technocratic approach within the scientific context.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们研究了生物体内耳膜中的横向振荡和横向站立波。我们示出了耳膜形状是这些现象的导火索，耳膜形状类似于梭形声学管，内部表面存在非均匀性。这种耳膜结构使得对听音信号的分析与听音系统物理过程的解释更加容易。此外，我们还得到结论， scala media在耳膜管中主要作为听音信号采集和增强系统，即耳膜管沿着听音螺旋的方向进行信息采集和增强。总之，这些发现可以基于科技方法，在科学上建立一种高度实际的听音系统模型。
</details></li>
</ul>
<hr>
<h2 id="Super-Denoise-Net-Speech-Super-Resolution-with-Noise-Cancellation-in-Low-Sampling-Rate-Noisy-Environments"><a href="#Super-Denoise-Net-Speech-Super-Resolution-with-Noise-Cancellation-in-Low-Sampling-Rate-Noisy-Environments" class="headerlink" title="Super Denoise Net: Speech Super Resolution with Noise Cancellation in Low Sampling Rate Noisy Environments"></a>Super Denoise Net: Speech Super Resolution with Noise Cancellation in Low Sampling Rate Noisy Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05629">http://arxiv.org/abs/2310.05629</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junkang Yang, Hongqing Liu, Lu Gan, Yi Zhou</li>
<li>for: 提高语音超解析和噪声除去的性能，以适应实际场景中的噪声存在情况。</li>
<li>methods: 提出了一种基于神经网络的Super Denoise Net（SDNet）模型，通过阻止层和格网络层来增强修复能力和在时间频率轴上 capture 信息。</li>
<li>results: 在 DNS 2020 无投射测试集上，SDNet 模型与基eline 语音噪声和超解析模型相比，得到了更高的 объек тив和主观分数。<details>
<summary>Abstract</summary>
Speech super-resolution (SSR) aims to predict a high resolution (HR) speech signal from its low resolution (LR) corresponding part. Most neural SSR models focus on producing the final result in a noise-free environment by recovering the spectrogram of high-frequency part of the signal and concatenating it with the original low-frequency part. Although these methods achieve high accuracy, they become less effective when facing the real-world scenario, where unavoidable noise is present. To address this problem, we propose a Super Denoise Net (SDNet), a neural network for a joint task of super-resolution and noise reduction from a low sampling rate signal. To that end, we design gated convolution and lattice convolution blocks to enhance the repair capability and capture information in the time-frequency axis, respectively. The experiments show our method outperforms baseline speech denoising and SSR models on DNS 2020 no-reverb test set with higher objective and subjective scores.
</details>
<details>
<summary>摘要</summary>
<<SYS>>TRANSLATE_TEXTSpeech super-resolution (SSR) aims to predict a high resolution (HR) speech signal from its low resolution (LR) corresponding part. Most neural SSR models focus on producing the final result in a noise-free environment by recovering the spectrogram of high-frequency part of the signal and concatenating it with the original low-frequency part. Although these methods achieve high accuracy, they become less effective when facing the real-world scenario, where unavoidable noise is present. To address this problem, we propose a Super Denoise Net (SDNet), a neural network for a joint task of super-resolution and noise reduction from a low sampling rate signal. To that end, we design gated convolution and lattice convolution blocks to enhance the repair capability and capture information in the time-frequency axis, respectively. The experiments show our method outperforms baseline speech denoising and SSR models on DNS 2020 no-reverb test set with higher objective and subjective scores.TRANSLATE_TEXT
</details></li>
</ul>
<hr>
<h2 id="Thech-Report-Genuinization-of-Speech-waveform-PMF-for-speaker-detection-spoofing-and-countermeasures"><a href="#Thech-Report-Genuinization-of-Speech-waveform-PMF-for-speaker-detection-spoofing-and-countermeasures" class="headerlink" title="Thech. Report: Genuinization of Speech waveform PMF for speaker detection spoofing and countermeasures"></a>Thech. Report: Genuinization of Speech waveform PMF for speaker detection spoofing and countermeasures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05534">http://arxiv.org/abs/2310.05534</a></li>
<li>repo_url: None</li>
<li>paper_authors: Itshak Lapidot, Jean-Francois Bonastre</li>
<li>for: 防止伪造攻击在语音识别系统中</li>
<li>methods: 提出一个名为“伪造化”的算法，可以降低伪造攻击所导致的语音波形分布差异</li>
<li>results: 实验结果显示，将伪造化算法应用于伪造攻击后，可以大幅提高伪动检测性能，并且在不同的实验情况下均有良好的表现。<details>
<summary>Abstract</summary>
In the context of spoofing attacks in speaker recognition systems, we observed that the waveform probability mass function (PMF) of genuine speech differs significantly from the PMF of speech resulting from the attacks. This is true for synthesized or converted speech as well as replayed speech. We also noticed that this observation seems to have a significant impact on spoofing detection performance. In this article, we propose an algorithm, denoted genuinization, capable of reducing the waveform distribution gap between authentic speech and spoofing speech. Our genuinization algorithm is evaluated on ASVspoof 2019 challenge datasets, using the baseline system provided by the challenge organization. We first assess the influence of genuinization on spoofing performance. Using genuinization for the spoofing attacks degrades spoofing detection performance by up to a factor of 10. Next, we integrate the genuinization algorithm in the spoofing countermeasures and we observe a huge spoofing detection improvement in different cases. The results of our experiments show clearly that waveform distribution plays an important role and must be taken into account by anti-spoofing systems.
</details>
<details>
<summary>摘要</summary>
在声音权限系统中的假声攻击中，我们发现了声波概率质量函数（PMF）的真实声音和假声音之间存在巨大的差异。这种差异适用于合成或转换的声音以及重播声音。我们还注意到，这一观察对假声检测性能产生了重要的影响。在本文中，我们提出了一种算法，称为真实化，可以减少真实声音和假声音之间的声波分布差异。我们的真实化算法在ASVspoof 2019挑战数据集上进行了评估，使用了挑战组织提供的基线系统。我们首先评估了假声攻击后真实化的影响。使用真实化对假声攻击减少了假声检测性能，最多减少了10倍。接着，我们将真实化算法 интеGRATED INTO spoofing countermeasures，并观察到了不同情况下的巨大假声检测改善。我们的实验结果显示，声波分布在反假检测系统中扮演着重要的角色。
</details></li>
</ul>
<hr>
<h2 id="AdvSV-An-Over-the-Air-Adversarial-Attack-Dataset-for-Speaker-Verification"><a href="#AdvSV-An-Over-the-Air-Adversarial-Attack-Dataset-for-Speaker-Verification" class="headerlink" title="AdvSV: An Over-the-Air Adversarial Attack Dataset for Speaker Verification"></a>AdvSV: An Over-the-Air Adversarial Attack Dataset for Speaker Verification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05369">http://arxiv.org/abs/2310.05369</a></li>
<li>repo_url: None</li>
<li>paper_authors: Li Wang, Jiaqi Li, Yuhao Luo, Jiahao Zheng, Lei Wang, Hao Li, Ke Xu, Chengfang Fang, Jie Shi, Zhizheng Wu</li>
<li>for: 本研究旨在提供一个开源的针对语音识别的抗击攻击数据集，以便进一步研究语音识别系统的安全性。</li>
<li>methods: 本研究使用了一种基于振荡器的抗击攻击方法，并在真实的声音环境中进行了测试。</li>
<li>results: 研究发现，使用这种抗击攻击方法可以在语音识别系统中引入攻击，并且可以在不同的声音环境下进行模拟。<details>
<summary>Abstract</summary>
It is known that deep neural networks are vulnerable to adversarial attacks. Although Automatic Speaker Verification (ASV) built on top of deep neural networks exhibits robust performance in controlled scenarios, many studies confirm that ASV is vulnerable to adversarial attacks. The lack of a standard dataset is a bottleneck for further research, especially reproducible research. In this study, we developed an open-source adversarial attack dataset for speaker verification research. As an initial step, we focused on the over-the-air attack. An over-the-air adversarial attack involves a perturbation generation algorithm, a loudspeaker, a microphone, and an acoustic environment. The variations in the recording configurations make it very challenging to reproduce previous research. The AdvSV dataset is constructed using the Voxceleb1 Verification test set as its foundation. This dataset employs representative ASV models subjected to adversarial attacks and records adversarial samples to simulate over-the-air attack settings. The scope of the dataset can be easily extended to include more types of adversarial attacks. The dataset will be released to the public under the CC-BY license. In addition, we also provide a detection baseline for reproducible research.
</details>
<details>
<summary>摘要</summary>
Deep neural networks 是容易受到敌意攻击的。尽管基于深度神经网络的自动说话识别（ASV）在控制场景下表现出了可靠性，但许多研究证明ASV对敌意攻击很敏感。数据集的缺乏标准化是研究的一大障碍，尤其是可重复性研究。在这项研究中，我们开发了一个开源的敌意攻击数据集 для说话识别研究。作为初始步骤，我们专注于无线电攻击。无线电攻击包括一个杂乱生成算法、一个喇叭、一个麦克风和一个声学环境。记录配置的变化使得前期研究的重现非常困难。 AdvSV 数据集基于 Voxceleb1 验证测试集作为基础，这个数据集使用了 Representative ASV 模型在敌意攻击下录制的样本，以模拟无线电攻击场景。数据集的范围可以轻松扩展到更多的敌意攻击类型。数据集将会在 CC-BY license 下公开发布。此外，我们还提供了一个可重复性的检测基线。
</details></li>
</ul>
<hr>
<h2 id="An-Initial-Investigation-of-Neural-Replay-Simulator-for-Over-the-Air-Adversarial-Perturbations-to-Automatic-Speaker-Verification"><a href="#An-Initial-Investigation-of-Neural-Replay-Simulator-for-Over-the-Air-Adversarial-Perturbations-to-Automatic-Speaker-Verification" class="headerlink" title="An Initial Investigation of Neural Replay Simulator for Over-the-Air Adversarial Perturbations to Automatic Speaker Verification"></a>An Initial Investigation of Neural Replay Simulator for Over-the-Air Adversarial Perturbations to Automatic Speaker Verification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05354">http://arxiv.org/abs/2310.05354</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaqi Li, Li Wang, Liumeng Xue, Lei Wang, Zhizheng Wu</li>
<li>for: 防御敏感语音识别系统免受 Physical Access 攻击</li>
<li>methods: 使用神经网络播放模拟器提高 Over-the-air 攻击 robustness</li>
<li>results: 使用神经网络播放模拟器可以大幅提高 Over-the-air 攻击成功率，提高 Physical Access 应用中语音识别系统的安全性问题<details>
<summary>Abstract</summary>
Deep Learning has advanced Automatic Speaker Verification (ASV) in the past few years. Although it is known that deep learning-based ASV systems are vulnerable to adversarial examples in digital access, there are few studies on adversarial attacks in the context of physical access, where a replay process (i.e., over the air) is involved. An over-the-air attack involves a loudspeaker, a microphone, and a replaying environment that impacts the movement of the sound wave. Our initial experiment confirms that the replay process impacts the effectiveness of the over-the-air attack performance. This study performs an initial investigation towards utilizing a neural replay simulator to improve over-the-air adversarial attack robustness. This is achieved by using a neural waveform synthesizer to simulate the replay process when estimating the adversarial perturbations. Experiments conducted on the ASVspoof2019 dataset confirm that the neural replay simulator can considerably increase the success rates of over-the-air adversarial attacks. This raises the concern for adversarial attacks on speaker verification in physical access applications.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/09/cs.SD_2023_10_09/" data-id="clp89dok9010ki788g6lq0ran" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_10_09" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/09/cs.CV_2023_10_09/" class="article-date">
  <time datetime="2023-10-09T13:00:00.000Z" itemprop="datePublished">2023-10-09</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/09/cs.CV_2023_10_09/">cs.CV - 2023-10-09</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="DiPS-Discriminative-Pseudo-Label-Sampling-with-Self-Supervised-Transformers-for-Weakly-Supervised-Object-Localization"><a href="#DiPS-Discriminative-Pseudo-Label-Sampling-with-Self-Supervised-Transformers-for-Weakly-Supervised-Object-Localization" class="headerlink" title="DiPS: Discriminative Pseudo-Label Sampling with Self-Supervised Transformers for Weakly Supervised Object Localization"></a>DiPS: Discriminative Pseudo-Label Sampling with Self-Supervised Transformers for Weakly Supervised Object Localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06196">http://arxiv.org/abs/2310.06196</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shakeebmurtaza/dips">https://github.com/shakeebmurtaza/dips</a></li>
<li>paper_authors: Shakeeb Murtaza, Soufiane Belharbi, Marco Pedersoli, Aydin Sarraf, Eric Granger</li>
<li>for: 本研究想要解决无监督图像本地化问题，使用自适应视Transformers（SSTs）获得高质量的本地化地图，但这些地图仍然是类型不归一的。</li>
<li>methods: 本研究提出了Discriminative Pseudo-label Sampling（DiPS）方法，利用多个注意力地图，通过预训练的类别器确定最有特征的区域，以确保选择的ROI覆盖正确的图像对象而不是背景噪音对象。</li>
<li>results: 实验结果表明，我们的架构、以及我们的transformer基于的提案可以在CUB、ILSVRC、OpenImages和TelDrone等 dataset上获得更高的本地化性能，并且可以在无监督下进行图像本地化和分类任务。<details>
<summary>Abstract</summary>
Self-supervised vision transformers (SSTs) have shown great potential to yield rich localization maps that highlight different objects in an image. However, these maps remain class-agnostic since the model is unsupervised. They often tend to decompose the image into multiple maps containing different objects while being unable to distinguish the object of interest from background noise objects. In this paper, Discriminative Pseudo-label Sampling (DiPS) is introduced to leverage these class-agnostic maps for weakly-supervised object localization (WSOL), where only image-class labels are available. Given multiple attention maps, DiPS relies on a pre-trained classifier to identify the most discriminative regions of each attention map. This ensures that the selected ROIs cover the correct image object while discarding the background ones, and, as such, provides a rich pool of diverse and discriminative proposals to cover different parts of the object. Subsequently, these proposals are used as pseudo-labels to train our new transformer-based WSOL model designed to perform classification and localization tasks. Unlike standard WSOL methods, DiPS optimizes performance in both tasks by using a transformer encoder and a dedicated output head for each task, each trained using dedicated loss functions. To avoid overfitting a single proposal and promote better object coverage, a single proposal is randomly selected among the top ones for a training image at each training step. Experimental results on the challenging CUB, ILSVRC, OpenImages, and TelDrone datasets indicate that our architecture, in combination with our transformer-based proposals, can yield better localization performance than state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
自适应视Transformers（SSTs）已经展示了很大的潜力，可以生成各种不同的对象映射，但这些映射是无监督的，因此无法区分识别对象和背景噪音。在这篇论文中，我们引入了Discriminative Pseudo-label Sampling（DiPS）方法，利用这些无监督映射来实现弱监督对象定位（WSOL），只需要图像类别标签。给出多个注意力映射后，DiPS使用预训练的分类器来确定每个注意力映射中最有挑战性的区域。这确保了选择的ROIs覆盖了正确的图像对象，而不是背景对象，并提供了丰富多样的可靠提案，以便覆盖不同部分的对象。然后，这些提案被用作 Pseudo-标签来训练我们新的 transformer 基于 WSOL 模型，用于进行分类和定位任务。不同于标准 WSOL 方法，DiPS 可以在两个任务之间优化性能，使用 transformer 编码器和专门的输出头，每个任务都使用专门的损失函数进行训练。为了避免单个提案过拟合和促进更好的对象覆盖，在每个训练图像上选择其中一个随机的提案。实验结果表明，我们的架构，与我们的 transformer 基于提案相结合，可以在复杂的 CUB、ILSVRC、OpenImages 和 TelDrone  datasets 上达到更高的定位性能。
</details></li>
</ul>
<hr>
<h2 id="HydraViT-Adaptive-Multi-Branch-Transformer-for-Multi-Label-Disease-Classification-from-Chest-X-ray-Images"><a href="#HydraViT-Adaptive-Multi-Branch-Transformer-for-Multi-Label-Disease-Classification-from-Chest-X-ray-Images" class="headerlink" title="HydraViT: Adaptive Multi-Branch Transformer for Multi-Label Disease Classification from Chest X-ray Images"></a>HydraViT: Adaptive Multi-Branch Transformer for Multi-Label Disease Classification from Chest X-ray Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06143">http://arxiv.org/abs/2310.06143</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/YigitTurali/HydraViT">https://github.com/YigitTurali/HydraViT</a></li>
<li>paper_authors: Şaban Öztürk, M. Yiğit Turalı, Tolga Çukur</li>
<li>for: 这个论文是用于提高胸部X射线图像的多标签分类性能的研究。</li>
<li>methods: 该方法 synergistically combines a transformer backbone with a multi-branch output module，使用自我注意机制来适应任务关键区域，并 dedicates an independent branch to each disease label 以及一个汇总 Branch across labels。</li>
<li>results: 实验表明，Compared to competing attention-guided methods, region-guided methods, and semantic-guided methods, HydraViT 的多标签分类性能平均提高1.2%, 1.4%, 和1.0%。<details>
<summary>Abstract</summary>
Chest X-ray is an essential diagnostic tool in the identification of chest diseases given its high sensitivity to pathological abnormalities in the lungs. However, image-driven diagnosis is still challenging due to heterogeneity in size and location of pathology, as well as visual similarities and co-occurrence of separate pathology. Since disease-related regions often occupy a relatively small portion of diagnostic images, classification models based on traditional convolutional neural networks (CNNs) are adversely affected given their locality bias. While CNNs were previously augmented with attention maps or spatial masks to guide focus on potentially critical regions, learning localization guidance under heterogeneity in the spatial distribution of pathology is challenging. To improve multi-label classification performance, here we propose a novel method, HydraViT, that synergistically combines a transformer backbone with a multi-branch output module with learned weighting. The transformer backbone enhances sensitivity to long-range context in X-ray images, while using the self-attention mechanism to adaptively focus on task-critical regions. The multi-branch output module dedicates an independent branch to each disease label to attain robust learning across separate disease classes, along with an aggregated branch across labels to maintain sensitivity to co-occurrence relationships among pathology. Experiments demonstrate that, on average, HydraViT outperforms competing attention-guided methods by 1.2%, region-guided methods by 1.4%, and semantic-guided methods by 1.0% in multi-label classification performance.
</details>
<details>
<summary>摘要</summary>
骨肉X光是诊断肺病的非常重要工具，可以快速发现肺部疾病的病理变化。然而，基于图像的诊断仍然是一项挑战，因为肺部疾病的形态特征可能具有不同的大小和位置，同时也可能具有相似的视觉特征和共存的疾病。由于疾病相关的区域通常占用图像的相对较小的部分，基于传统的卷积神经网络（CNN）的分类模型受到了地方性偏好的影响。在以前，人们使用了注意力地图或空间mask来引导注意力于可能关键的区域，但在疾病分布的空间分布不均匀的情况下，学习局部化导向是挑战。为了改进多标签分类性能，我们在这里提出了一种新的方法：HydraViT。HydraViT synergistically combines a transformer backbone with a multi-branch output module with learned weighting。 transformer backbone 增强了X光图像的长距离上下文敏感度，并使用自注意机制来自适应任务关键区域。 multi-branch output module 各自设置了独立的分支来每个疾病标签，以实现稳定的学习 Across separate disease classes， along with an aggregated branch across labels to maintain sensitivity to co-occurrence relationships among pathology。实验表明，HydraViT 在多标签分类性能中，平均与竞争对手注意力引导方法比出得1.2%，region-guided方法比出得1.4%，semantic-guided方法比出得1.0%。
</details></li>
</ul>
<hr>
<h2 id="WinSyn-A-High-Resolution-Testbed-for-Synthetic-Data"><a href="#WinSyn-A-High-Resolution-Testbed-for-Synthetic-Data" class="headerlink" title="WinSyn: A High Resolution Testbed for Synthetic Data"></a>WinSyn: A High Resolution Testbed for Synthetic Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08471">http://arxiv.org/abs/2310.08471</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/twak/winsyn_metadata">https://github.com/twak/winsyn_metadata</a></li>
<li>paper_authors: Tom Kelly, John Femiani, Peter Wonka</li>
<li>for: 研究synthetic-to-real学习和人工数据生成</li>
<li>methods: 使用高分辨率照片和渲染技术生成数据集（WinSyn），包括75739张照片和89318个裁剪图像，其中902个是semantic标注</li>
<li>results: 提供了一个域对齐的光idel模型，可以进行多种参数分布和工程方法的实验，并提供了21290个synthetic图像的第二个数据集。这个数据集是用于研究synthetic数据生成领域的一个重要的测试平台。<details>
<summary>Abstract</summary>
We present WinSyn, a dataset consisting of high-resolution photographs and renderings of 3D models as a testbed for synthetic-to-real research. The dataset consists of 75,739 high-resolution photographs of building windows, including traditional and modern designs, captured globally. These include 89,318 cropped subimages of windows, of which 9,002 are semantically labeled. Further, we present our domain-matched photorealistic procedural model which enables experimentation over a variety of parameter distributions and engineering approaches. Our procedural model provides a second corresponding dataset of 21,290 synthetic images. This jointly developed dataset is designed to facilitate research in the field of synthetic-to-real learning and synthetic data generation. WinSyn allows experimentation into the factors that make it challenging for synthetic data to compete with real-world data. We perform ablations using our synthetic model to identify the salient rendering, materials, and geometric factors pertinent to accuracy within the labeling task. We chose windows as a benchmark because they exhibit a large variability of geometry and materials in their design, making them ideal to study synthetic data generation in a constrained setting. We argue that the dataset is a crucial step to enable future research in synthetic data generation for deep learning.
</details>
<details>
<summary>摘要</summary>
我们介绍WinSyn数据集，包含高分辨率照片和3D模型渲染的集合，用于实验式数据生成研究的测试平台。该数据集包含75739张高分辨率照片建筑窗户，包括传统和现代设计，全球摄取。这些数据包括89318张窗户截取图，其中9022个是semantically标注。此外，我们提供域匹配的高品质渲染模型，允许在多种参数分布和工程方法之间进行实验。我们的渲染模型生成了21290张 sintetic图像。这些数据集在实验式数据生成领域的研究中提供了一个jointly开发的测试平台。WinSyn允许我们对于实际数据和 sintetic数据之间的差异进行实验，并通过我们的 sintetic模型进行ablation来确定窗户渲染、材料和几何因素对于标签任务的精度有优先的作用。我们选择窗户作为标准 benchmark，因为它们在设计上具有广泛的几何和材料变化，使其成为研究 sintetic数据生成在限定的设定下的理想选择。我们认为这些数据集是未来研究 sintetic数据生成的深度学习的关键一步。
</details></li>
</ul>
<hr>
<h2 id="Factorized-Tensor-Networks-for-Multi-Task-and-Multi-Domain-Learning"><a href="#Factorized-Tensor-Networks-for-Multi-Task-and-Multi-Domain-Learning" class="headerlink" title="Factorized Tensor Networks for Multi-Task and Multi-Domain Learning"></a>Factorized Tensor Networks for Multi-Task and Multi-Domain Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06124">http://arxiv.org/abs/2310.06124</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yashgarg98/FTN">https://github.com/yashgarg98/FTN</a></li>
<li>paper_authors: Yash Garg, Nebiyou Yismaw, Rakib Hyder, Ashley Prater-Bennette, M. Salman Asif<br>for:FTN 是一种多任务多Domain学习方法，可以通过单一的网络来学习多个任务和多个Domain。methods:FTN 使用一个冻结的背景网络，然后逐渐添加任务&#x2F;Domain特定的低维度tensor因子来共享的冻结网络中。results:FTN 可以在多个目标Domain和任务上达到类似于独立单任务&#x2F;Domain网络的准确率，但需要的额外参数比较少。我们在多个广泛使用的多Domain和多任务数据集上进行了实验，并观察到 FTN 可以在不同的卷积架构和转换架构上达到类似的准确率。<details>
<summary>Abstract</summary>
Multi-task and multi-domain learning methods seek to learn multiple tasks/domains, jointly or one after another, using a single unified network. The key challenge and opportunity is to exploit shared information across tasks and domains to improve the efficiency of the unified network. The efficiency can be in terms of accuracy, storage cost, computation, or sample complexity. In this paper, we propose a factorized tensor network (FTN) that can achieve accuracy comparable to independent single-task/domain networks with a small number of additional parameters. FTN uses a frozen backbone network from a source model and incrementally adds task/domain-specific low-rank tensor factors to the shared frozen network. This approach can adapt to a large number of target domains and tasks without catastrophic forgetting. Furthermore, FTN requires a significantly smaller number of task-specific parameters compared to existing methods. We performed experiments on widely used multi-domain and multi-task datasets. We show the experiments on convolutional-based architecture with different backbones and on transformer-based architecture. We observed that FTN achieves similar accuracy as single-task/domain methods while using only a fraction of additional parameters per task.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="QR-Tag-Angular-Measurement-and-Tracking-with-a-QR-Design-Marker"><a href="#QR-Tag-Angular-Measurement-and-Tracking-with-a-QR-Design-Marker" class="headerlink" title="QR-Tag: Angular Measurement and Tracking with a QR-Design Marker"></a>QR-Tag: Angular Measurement and Tracking with a QR-Design Marker</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06109">http://arxiv.org/abs/2310.06109</a></li>
<li>repo_url: None</li>
<li>paper_authors: Simeng Qiu, Hadi Amata, Wolfgang Heidrich</li>
<li>for: 这个论文主要用于提出一种非接触式对象跟踪方法，用于应用于机器人、虚拟和增强现实以及工业计算机视觉等领域。</li>
<li>methods: 该方法利用误差效应和QR码设计，实时测量和跟踪对象的旋转偏移。</li>
<li>results:  simulations show that the proposed method is computationally efficient and has high accuracy in measuring angular information.<details>
<summary>Abstract</summary>
Directional information measurement has many applications in domains such as robotics, virtual and augmented reality, and industrial computer vision. Conventional methods either require pre-calibration or necessitate controlled environments. The state-of-the-art MoireTag approach exploits the Moire effect and QR-design to continuously track the angular shift precisely. However, it is still not a fully QR code design. To overcome the above challenges, we propose a novel snapshot method for discrete angular measurement and tracking with scannable QR-design patterns that are generated by binary structures printed on both sides of a glass plate. The QR codes, resulting from the parallax effect due to the geometry alignment between two layers, can be readily measured as angular information using a phone camera. The simulation results show that the proposed non-contact object tracking framework is computationally efficient with high accuracy.
</details>
<details>
<summary>摘要</summary>
方向信息测量在 роботиCS、虚拟和增强现实以及工业计算机视觉领域有广泛的应用。传统方法 Either require pre-calibration 或者需要控制环境。 current state-of-the-art MoireTag approach exploits the Moire effect and QR-design to continuously track the angular shift precisely. However, it is still not a fully QR code design. To overcome the above challenges, we propose a novel snapshot method for discrete angular measurement and tracking with scannable QR-design patterns that are generated by binary structures printed on both sides of a glass plate. The QR codes, resulting from the parallax effect due to the geometry alignment between two layers, can be readily measured as angular information using a phone camera. The simulation results show that the proposed non-contact object tracking framework is computationally efficient with high accuracy.Here's the breakdown of the translation:方向信息测量 (directional information measurement) - 方向信息测量 (directional information measurement)领域 (domains) - 领域 (domains)such as robotics, virtual and augmented reality, and industrial computer vision. - 如 robotics, 虚拟和增强现实, 和工业计算机视觉Conventional methods either require pre-calibration or necessitate controlled environments. - 传统方法 Either require pre-calibration 或者需要控制环境。The state-of-the-art MoireTag approach exploits the Moire effect and QR-design to continuously track the angular shift precisely. - current state-of-the-art MoireTag approach exploits the Moire effect and QR-design to continuously track the angular shift precisely.However, it is still not a fully QR code design. - However, it is still not a fully QR code design.To overcome the above challenges, we propose a novel snapshot method for discrete angular measurement and tracking with scannable QR-design patterns that are generated by binary structures printed on both sides of a glass plate. - To overcome the above challenges, we propose a novel snapshot method for discrete angular measurement and tracking with scannable QR-design patterns that are generated by binary structures printed on both sides of a glass plate.The QR codes, resulting from the parallax effect due to the geometry alignment between two layers, can be readily measured as angular information using a phone camera. - The QR codes, resulting from the parallax effect due to the geometry alignment between two layers, can be readily measured as angular information using a phone camera.The simulation results show that the proposed non-contact object tracking framework is computationally efficient with high accuracy. - The simulation results show that the proposed non-contact object tracking framework is computationally efficient with high accuracy.
</details></li>
</ul>
<hr>
<h2 id="Developing-and-Refining-a-Multifunctional-Facial-Recognition-System-for-Older-Adults-with-Cognitive-Impairments-A-Journey-Towards-Enhanced-Quality-of-Life"><a href="#Developing-and-Refining-a-Multifunctional-Facial-Recognition-System-for-Older-Adults-with-Cognitive-Impairments-A-Journey-Towards-Enhanced-Quality-of-Life" class="headerlink" title="Developing and Refining a Multifunctional Facial Recognition System for Older Adults with Cognitive Impairments: A Journey Towards Enhanced Quality of Life"></a>Developing and Refining a Multifunctional Facial Recognition System for Older Adults with Cognitive Impairments: A Journey Towards Enhanced Quality of Life</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06107">http://arxiv.org/abs/2310.06107</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/li-8023/multi-function-face-recognition">https://github.com/li-8023/multi-function-face-recognition</a></li>
<li>paper_authors: Li He</li>
<li>for: 该论文主要用于提供一种适用于老年人智能障碍的多功能面部识别系统（MFRS），以帮助老年人更好地完成日常任务。</li>
<li>methods: 该系统使用了face_recognition库，这是一个免费的开源库，可以提取、识别和处理面部特征。此外，系统还包括拍照和录音功能，以提高系统的用户性和通用性。</li>
<li>results: 该系统的实现和评估表明，它可以帮助老年人更好地完成日常任务，例如识别家人和朋友、记录日常活动和照片，以及帮助老年人找回失去的物品。<details>
<summary>Abstract</summary>
In an era where the global population is aging significantly, cognitive impairments among the elderly have become a major health concern. The need for effective assistive technologies is clear, and facial recognition systems are emerging as promising tools to address this issue. This document discusses the development and evaluation of a new Multifunctional Facial Recognition System (MFRS), designed specifically to assist older adults with cognitive impairments. The MFRS leverages face_recognition [1], a powerful open-source library capable of extracting, identifying, and manipulating facial features. Our system integrates the face recognition and retrieval capabilities of face_recognition, along with additional functionalities to capture images and record voice memos. This combination of features notably enhances the system's usability and versatility, making it a more user-friendly and universally applicable tool for end-users. The source code for this project can be accessed at https://github.com/Li-8023/Multi-function-face-recognition.git.
</details>
<details>
<summary>摘要</summary>
在老龄化的时代，老年人的认知障碍成为了主要的健康问题。需要有效的助助技术，面Recognition系统正在迅速发展。这份文档介绍了一种新的多功能面Recognition系统（MFRS），特意为老年人 WITH cognitive impairments设计。MFRS利用face_recognition[1]，一个强大的开源库，可以提取、识别和修改面部特征。我们的系统结合了面Recognition和检索功能，并添加了捕捉图像和录音笔记功能。这种结合使得系统的使用和通用性得到了显著提高，使得它成为更加用户友好和普遍适用的工具。相关代码可以在https://github.com/Li-8023/Multi-function-face-recognition.git中下载。
</details></li>
</ul>
<hr>
<h2 id="Advancing-Diagnostic-Precision-Leveraging-Machine-Learning-Techniques-for-Accurate-Detection-of-Covid-19-Pneumonia-and-Tuberculosis-in-Chest-X-Ray-Images"><a href="#Advancing-Diagnostic-Precision-Leveraging-Machine-Learning-Techniques-for-Accurate-Detection-of-Covid-19-Pneumonia-and-Tuberculosis-in-Chest-X-Ray-Images" class="headerlink" title="Advancing Diagnostic Precision: Leveraging Machine Learning Techniques for Accurate Detection of Covid-19, Pneumonia, and Tuberculosis in Chest X-Ray Images"></a>Advancing Diagnostic Precision: Leveraging Machine Learning Techniques for Accurate Detection of Covid-19, Pneumonia, and Tuberculosis in Chest X-Ray Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06080">http://arxiv.org/abs/2310.06080</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aditya Kulkarni, Guruprasad Parasnis, Harish Balasubramanian, Vansh Jain, Anmol Chokshi, Reena Sonkusare</li>
<li>for: 这个研究旨在提出一种多类分类方法，用于早期诊断COVID-19、TB和肺炎等肺疾病。</li>
<li>methods: 该方法使用现代深度学习和图像处理技术，包括一种新的卷积神经网络和多种传输学习预训练模型。</li>
<li>results: 研究使用公共可用的多类Kaggle数据集和NIH数据集进行了严格测试，并取得了COVID-19的AUC值为0.95、TB的AUC值为0.99和肺炎的AUC值为0.98，以及Recall和精度等高水平。<details>
<summary>Abstract</summary>
Lung diseases such as COVID-19, tuberculosis (TB), and pneumonia continue to be serious global health concerns that affect millions of people worldwide. In medical practice, chest X-ray examinations have emerged as the norm for diagnosing diseases, particularly chest infections such as COVID-19. Paramedics and scientists are working intensively to create a reliable and precise approach for early-stage COVID-19 diagnosis in order to save lives. But with a variety of symptoms, medical diagnosis of these disorders poses special difficulties. It is essential to address their identification and timely diagnosis in order to successfully treat and prevent these illnesses. In this research, a multiclass classification approach using state-of-the-art methods for deep learning and image processing is proposed. This method takes into account the robustness and efficiency of the system in order to increase diagnostic precision of chest diseases. A comparison between a brand-new convolution neural network (CNN) and several transfer learning pre-trained models including VGG19, ResNet, DenseNet, EfficientNet, and InceptionNet is recommended. Publicly available and widely used research datasets like Shenzen, Montogomery, the multiclass Kaggle dataset and the NIH dataset were used to rigorously test the model. Recall, precision, F1-score, and Area Under Curve (AUC) score are used to evaluate and compare the performance of the proposed model. An AUC value of 0.95 for COVID-19, 0.99 for TB, and 0.98 for pneumonia is obtained using the proposed network. Recall and precision ratings of 0.95, 0.98, and 0.97, respectively, likewise met high standards.
</details>
<details>
<summary>摘要</summary>
肺病如COVID-19、肺 tubercular (TB) 和肺炎病综合症仍然是全球健康问题，影响了数百万人。在医疗实践中，胸部X射线检测成为诊断疾病的标准方法，特别是肺部感染如COVID-19。 paramedics 和科学家在努力创造一种可靠和精准的早期COVID-19诊断方法，以拯救生命。但这些疾病的症状多样，医学诊断呈现特殊困难。因此，有必要解决其识别和早期诊断，以成功地治疗和预防这些疾病。在这项研究中，我们提出了一种多类分类方法，使用现代深度学习和图像处理技术。这种方法考虑了系统的稳定性和效率，以提高胸部疾病诊断的精度。我们建议对多个转移学习预训练模型，包括VGG19、ResNet、DenseNet、EfficientNet和InceptionNet进行比较。使用公共可用的和广泛使用的研究数据集，如深圳、Montgomery、多类Kaggle数据集和NIH数据集，对模型进行严格测试。我们使用Recall、精度、F1分数和报道曲线（AUC）分数来评估和比较提案模型的性能。我们获得了COVID-19的AUC值为0.95，TB的AUC值为0.99，肺炎病的AUC值为0.98，同时Recall和精度分数都达到了高标准。
</details></li>
</ul>
<hr>
<h2 id="FLATTEN-optical-FLow-guided-ATTENtion-for-consistent-text-to-video-editing"><a href="#FLATTEN-optical-FLow-guided-ATTENtion-for-consistent-text-to-video-editing" class="headerlink" title="FLATTEN: optical FLow-guided ATTENtion for consistent text-to-video editing"></a>FLATTEN: optical FLow-guided ATTENtion for consistent text-to-video editing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05922">http://arxiv.org/abs/2310.05922</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuren Cong, Mengmeng Xu, Christian Simon, Shoufa Chen, Jiawei Ren, Yanping Xie, Juan-Manuel Perez-Rua, Bodo Rosenhahn, Tao Xiang, Sen He</li>
<li>for: 文章主要目标是提高文本到视频编辑 task 中的视觉一致性。</li>
<li>methods: 作者首次在 diffusion 模型中的 U-Net 中引入了 optical flow，以解决文本到视频编辑 task 中的一致性问题。</li>
<li>results: 实验结果表明，我们的方法可以减少不一致性，并在现有的文本到视频编辑 benchmark 上达到新的州OF-THE-ART 性能。<details>
<summary>Abstract</summary>
Text-to-video editing aims to edit the visual appearance of a source video conditional on textual prompts. A major challenge in this task is to ensure that all frames in the edited video are visually consistent. Most recent works apply advanced text-to-image diffusion models to this task by inflating 2D spatial attention in the U-Net into spatio-temporal attention. Although temporal context can be added through spatio-temporal attention, it may introduce some irrelevant information for each patch and therefore cause inconsistency in the edited video. In this paper, for the first time, we introduce optical flow into the attention module in the diffusion model's U-Net to address the inconsistency issue for text-to-video editing. Our method, FLATTEN, enforces the patches on the same flow path across different frames to attend to each other in the attention module, thus improving the visual consistency in the edited videos. Additionally, our method is training-free and can be seamlessly integrated into any diffusion-based text-to-video editing methods and improve their visual consistency. Experiment results on existing text-to-video editing benchmarks show that our proposed method achieves the new state-of-the-art performance. In particular, our method excels in maintaining the visual consistency in the edited videos.
</details>
<details>
<summary>摘要</summary>
文本到视频编辑是编辑源视频的视觉外观，根据文本提示进行编辑。该任务的主要挑战是保证所有视频帧的视觉一致性。大多数最新的工作通过增加文本到图像扩散模型来解决这个问题，其中包括在U-Net中增加2D空间注意力。虽然通过空间-时间注意力可以添加时间上下文，但可能会在每个patch中添加无关信息，从而导致编辑视频中的不一致性。在这篇论文中，我们首次在U-Net中引入摩尔变换来解决文本到视频编辑中的不一致性问题。我们的方法，FLATTEN，在注意力模块中使用同一个流动路径的patch来attend于彼此，从而提高编辑视频中的视觉一致性。此外，我们的方法是训练自由的，可以轻松地与任何扩散基于文本到视频编辑方法结合使用，提高其视觉一致性。实验结果表明，我们的提议方法在现有的文本到视频编辑标准准测试集上达到了新的状态纪录性。尤其是，我们的方法在编辑视频中维持视觉一致性的表现出色。
</details></li>
</ul>
<hr>
<h2 id="SimPLR-A-Simple-and-Plain-Transformer-for-Object-Detection-and-Segmentation"><a href="#SimPLR-A-Simple-and-Plain-Transformer-for-Object-Detection-and-Segmentation" class="headerlink" title="SimPLR: A Simple and Plain Transformer for Object Detection and Segmentation"></a>SimPLR: A Simple and Plain Transformer for Object Detection and Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05920">http://arxiv.org/abs/2310.05920</a></li>
<li>repo_url: None</li>
<li>paper_authors: Duy-Kien Nguyen, Martin R. Oswald, Cees G. M. Snoek</li>
<li>for:  This paper aims to improve object detection in images by removing the need for feature pyramids and multi-scale feature maps, which are commonly used in modern object detectors.</li>
<li>methods: The paper proposes a transformer-based detector with scale-aware attention, which allows the detector to operate on single-scale features.</li>
<li>results: The proposed method, called SimPLR, achieves strong performance compared to other object detectors, including end-to-end detectors and plain-backbone detectors, while being faster.<details>
<summary>Abstract</summary>
The ability to detect objects in images at varying scales has played a pivotal role in the design of modern object detectors. Despite considerable progress in removing handcrafted components using transformers, multi-scale feature maps remain a key factor for their empirical success, even with a plain backbone like the Vision Transformer (ViT). In this paper, we show that this reliance on feature pyramids is unnecessary and a transformer-based detector with scale-aware attention enables the plain detector `SimPLR' whose backbone and detection head both operate on single-scale features. The plain architecture allows SimPLR to effectively take advantages of self-supervised learning and scaling approaches with ViTs, yielding strong performance compared to multi-scale counterparts. We demonstrate through our experiments that when scaling to larger backbones, SimPLR indicates better performance than end-to-end detectors (Mask2Former) and plain-backbone detectors (ViTDet), while consistently being faster. The code will be released.
</details>
<details>
<summary>摘要</summary>
现代物体检测器的设计中，检测对象在不同比例的图像中的能力具有重要作用。尽管已经做出了许多进步，使用变换器来消除手工组件，但是多个比例特征图仍然是现有的静态成功的关键因素，即使使用简单的背bone like Vision Transformer (ViT)。在这篇论文中，我们表明这种依赖于特征阶段是不必要的，并使用缩放意识来替代特征阶段，从而实现了简单的检测器 `SimPLR`。该简单的架构允许SimPLR有效地利用自我超vised学习和缩放方法，并在ViTs上表现出优于多比例对手。我们通过实验表明，当扩大到更大的背bone时，SimPLR表现 mejor于端到端检测器（Mask2Former）和平面背bone检测器（ViTDet），而且一直快。代码将被发布。
</details></li>
</ul>
<hr>
<h2 id="Drivable-Avatar-Clothing-Faithful-Full-Body-Telepresence-with-Dynamic-Clothing-Driven-by-Sparse-RGB-D-Input"><a href="#Drivable-Avatar-Clothing-Faithful-Full-Body-Telepresence-with-Dynamic-Clothing-Driven-by-Sparse-RGB-D-Input" class="headerlink" title="Drivable Avatar Clothing: Faithful Full-Body Telepresence with Dynamic Clothing Driven by Sparse RGB-D Input"></a>Drivable Avatar Clothing: Faithful Full-Body Telepresence with Dynamic Clothing Driven by Sparse RGB-D Input</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05917">http://arxiv.org/abs/2310.05917</a></li>
<li>repo_url: None</li>
<li>paper_authors: Donglai Xiang, Fabian Prada, Zhe Cao, Kaiwen Guo, Chenglei Wu, Jessica Hodgins, Timur Bagautdinov</li>
<li>for: 这个论文的目的是为了创建具有协调动作的真实化人物模型，并使用RGB-D输入对裤子和衣服进行精准模拟。</li>
<li>methods: 该论文提出了一种基于神经网络的迭代最近点算法（N-ICP），用于高效地跟踪裤子的大致形状，并使用RGB-D输入对裤子和衣服进行精准模拟。</li>
<li>results: 该论文的实验结果显示，使用N-ICP算法可以高效地生成具有协调动作和真实的裤子和衣服动画，并且可以在新的测试环境中进行普适化。<details>
<summary>Abstract</summary>
Clothing is an important part of human appearance but challenging to model in photorealistic avatars. In this work we present avatars with dynamically moving loose clothing that can be faithfully driven by sparse RGB-D inputs as well as body and face motion. We propose a Neural Iterative Closest Point (N-ICP) algorithm that can efficiently track the coarse garment shape given sparse depth input. Given the coarse tracking results, the input RGB-D images are then remapped to texel-aligned features, which are fed into the drivable avatar models to faithfully reconstruct appearance details. We evaluate our method against recent image-driven synthesis baselines, and conduct a comprehensive analysis of the N-ICP algorithm. We demonstrate that our method can generalize to a novel testing environment, while preserving the ability to produce high-fidelity and faithful clothing dynamics and appearance.
</details>
<details>
<summary>摘要</summary>
<<SYS>> clothings 是人类外表的重要组成部分，但是模拟 photorealistic avatars 中的 clothings 具有挑战性。在这项工作中，我们展示了可以受到 sparse RGB-D 输入的动态摆动粗布衣的 avatars，并且可以通过 body 和 face 运动来详细描述 clothings 的形状。我们提出了一种基于 Neural Iterative Closest Point 算法（N-ICP）的方法，可以高效地跟踪粗布衣的抽象形状，只需要 sparse depth 输入。给出了车 tracking 结果后，输入 RGB-D 图像将被重新映射到 texel-aligned 特征上，然后被 fed 到 drivable avatar 模型中，以实现高质量的形状和外观重建。我们对最近的 image-driven synthesis 基准进行了评估，并进行了 N-ICP 算法的全面分析。我们示出了我们的方法可以在新的测试环境中广泛应用，同时保持高度准确的 clothings 动态和外观重建。
</details></li>
</ul>
<hr>
<h2 id="CoBEVFusion-Cooperative-Perception-with-LiDAR-Camera-Bird’s-Eye-View-Fusion"><a href="#CoBEVFusion-Cooperative-Perception-with-LiDAR-Camera-Bird’s-Eye-View-Fusion" class="headerlink" title="CoBEVFusion: Cooperative Perception with LiDAR-Camera Bird’s-Eye View Fusion"></a>CoBEVFusion: Cooperative Perception with LiDAR-Camera Bird’s-Eye View Fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06008">http://arxiv.org/abs/2310.06008</a></li>
<li>repo_url: None</li>
<li>paper_authors: Donghao Qiao, Farhana Zulkernine</li>
<li>for: 提高自动驾驶车辆的安全性和可靠性，通过共享多感器数据进行共同探测</li>
<li>methods: 利用LiDAR和摄像头数据的多模式融合，通过独立窗口基于交叉注意力（DWCA）模块进行融合，并将融合后的BEV特征图分享到CAV之间，使用3D卷积神经网络进行特征聚合</li>
<li>results: 对OPV2V数据集进行两项探测任务（BEVsemantic segmentation和3D物体检测）的实验结果表明，我们的DWCA LiDAR-camera融合模型在单模态数据和现有BEV融合模型之上具有显著性能优势，而我们的总共同探测架构CoBEVFusion也实现了与其他共同探测模型相当的性能<details>
<summary>Abstract</summary>
Autonomous Vehicles (AVs) use multiple sensors to gather information about their surroundings. By sharing sensor data between Connected Autonomous Vehicles (CAVs), the safety and reliability of these vehicles can be improved through a concept known as cooperative perception. However, recent approaches in cooperative perception only share single sensor information such as cameras or LiDAR. In this research, we explore the fusion of multiple sensor data sources and present a framework, called CoBEVFusion, that fuses LiDAR and camera data to create a Bird's-Eye View (BEV) representation. The CAVs process the multi-modal data locally and utilize a Dual Window-based Cross-Attention (DWCA) module to fuse the LiDAR and camera features into a unified BEV representation. The fused BEV feature maps are shared among the CAVs, and a 3D Convolutional Neural Network is applied to aggregate the features from the CAVs. Our CoBEVFusion framework was evaluated on the cooperative perception dataset OPV2V for two perception tasks: BEV semantic segmentation and 3D object detection. The results show that our DWCA LiDAR-camera fusion model outperforms perception models with single-modal data and state-of-the-art BEV fusion models. Our overall cooperative perception architecture, CoBEVFusion, also achieves comparable performance with other cooperative perception models.
</details>
<details>
<summary>摘要</summary>
自动驾驶车（AV）使用多种感知器来收集它们周围环境的信息。通过Connected Autonomous Vehicles（CAVs）之间分享感知器数据，可以提高自动驾驶车的安全性和可靠性。然而，现有的合作感知方法只是分享单一感知器数据，如摄像头或LiDAR。在这项研究中，我们探索了多感知器数据源的融合，并提出了一个名为CoBEVFusion的框架。CoBEVFusion框架将LiDAR和摄像头数据融合到一起，创建一个鸟瞰视图（BEV）表示。CAVs在本地处理多模态数据，并使用双窗口基于cross-attention（DWCA）模块将LiDAR和摄像头特征融合到一起。融合后的BEV特征地图被CAVs中共享，并将其传输给3D卷积神经网络进行汇聚。我们的CoBEVFusion框架在OPV2V合作感知数据集上进行了两种感知任务的评估：BEV semantic segmentation和3D объек物检测。结果表明，我们的DWCA LiDAR-camera融合模型在单模态数据和现有BEV融合模型之上具有优势。总的来说，我们的CoBEVFusion框架在合作感知领域也实现了相对于其他合作感知模型的比较好的性能。
</details></li>
</ul>
<hr>
<h2 id="Geom-Erasing-Geometry-Driven-Removal-of-Implicit-Concept-in-Diffusion-Models"><a href="#Geom-Erasing-Geometry-Driven-Removal-of-Implicit-Concept-in-Diffusion-Models" class="headerlink" title="Geom-Erasing: Geometry-Driven Removal of Implicit Concept in Diffusion Models"></a>Geom-Erasing: Geometry-Driven Removal of Implicit Concept in Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05873">http://arxiv.org/abs/2310.05873</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhili Liu, Kai Chen, Yifan Zhang, Jianhua Han, Lanqing Hong, Hang Xu, Zhenguo Li, Dit-Yan Yeung, James Kwok</li>
<li>for: 提高Diffusion模型的生成质量，通过个性化数据集进行微调。</li>
<li>methods: 使用额外的可访问分类器或检测器模型，将图像中的隐式概念编码到文本领域。</li>
<li>results: 成功 removaling隐式概念，显示较 существенный改进于现有方法。<details>
<summary>Abstract</summary>
Fine-tuning diffusion models through personalized datasets is an acknowledged method for improving generation quality across downstream tasks, which, however, often inadvertently generates unintended concepts such as watermarks and QR codes, attributed to the limitations in image sources and collecting methods within specific downstream tasks. Existing solutions suffer from eliminating these unintentionally learned implicit concepts, primarily due to the dependency on the model's ability to recognize concepts that it actually cannot discern. In this work, we introduce Geom-Erasing, a novel approach that successfully removes the implicit concepts with either an additional accessible classifier or detector model to encode geometric information of these concepts into text domain. Moreover, we propose Implicit Concept, a novel image-text dataset imbued with three implicit concepts (i.e., watermarks, QR codes, and text) for training and evaluation. Experimental results demonstrate that Geom-Erasing not only identifies but also proficiently eradicates implicit concepts, revealing a significant improvement over the existing methods. The integration of geometric information marks a substantial progression in the precise removal of implicit concepts in diffusion models.
</details>
<details>
<summary>摘要</summary>
fino-tuning Diffusion 模型通过个性化数据集进行改进生成质量的方法是普遍认可的，但它们经常不小心学习不良的概念，如水印和二维码，这是因为图像来源和收集方法的局限性。现有的解决方案受到模型认知概念的能力的限制，因此很难减少这些不良学习的概念。在这种情况下，我们介绍了 Geom-Erasing，一种新的方法，可以成功地从图像中除除不良的概念，通过添加一个可访问的分类器或检测器模型来编码图像中的几何信息到文本领域。此外，我们提出了 Implicit Concept，一个新的图像-文本数据集，含有三种隐式概念（即水印、二维码和文本），用于训练和评估。实验结果表明，Geom-Erasing不仅可以识别，还可以高效地除除隐式概念，显示与现有方法相比有显著的改善。图像几何信息的集成表明了在精准除除隐式概念方面升级了很大的进步。
</details></li>
</ul>
<hr>
<h2 id="Domain-wise-Invariant-Learning-for-Panoptic-Scene-Graph-Generation"><a href="#Domain-wise-Invariant-Learning-for-Panoptic-Scene-Graph-Generation" class="headerlink" title="Domain-wise Invariant Learning for Panoptic Scene Graph Generation"></a>Domain-wise Invariant Learning for Panoptic Scene Graph Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05867">http://arxiv.org/abs/2310.05867</a></li>
<li>repo_url: None</li>
<li>paper_authors: Li Li, You Qin, Wei Ji, Yuxiao Zhou, Roger Zimmermann</li>
<li>for: 提高 PSG 模型在实际应用中的实用性和可靠性，解决 predicate 注释偏见问题。</li>
<li>methods: 提出一种新的框架，通过在每个主体-对象对中测量 predicate 预测风险，并通过学习不变 predicate 表示嵌入来适应ively 转移偏见注释。</li>
<li>results: 实验显示，我们的方法可以显著提高 benchmark 模型的性能，达到新的state-of-the-art性能水平，并在 PSG 数据集上显示出优秀的泛化和效果。<details>
<summary>Abstract</summary>
Panoptic Scene Graph Generation (PSG) involves the detection of objects and the prediction of their corresponding relationships (predicates). However, the presence of biased predicate annotations poses a significant challenge for PSG models, as it hinders their ability to establish a clear decision boundary among different predicates. This issue substantially impedes the practical utility and real-world applicability of PSG models. To address the intrinsic bias above, we propose a novel framework to infer potentially biased annotations by measuring the predicate prediction risks within each subject-object pair (domain), and adaptively transfer the biased annotations to consistent ones by learning invariant predicate representation embeddings. Experiments show that our method significantly improves the performance of benchmark models, achieving a new state-of-the-art performance, and shows great generalization and effectiveness on PSG dataset.
</details>
<details>
<summary>摘要</summary>
泛opepticScene Graph生成（PSG）含有物体探测和相应关系预测（预言）。然而，存在偏见 predicate 注释会对 PSG 模型造成很大障碍，因为它阻碍了模型建立清晰的决策边界。这个问题对 PSG 模型的实际用途和应用程度产生了很大的障碍。为了解决上述内在偏见，我们提出了一种新的框架，通过在每个主体- объек 对（Domain）中测量预测 predicate 的风险，并将偏见的预测转移到一致的 predicate 表示嵌入中。实验表明，我们的方法可以显著提高基准模型的性能，实现新的状态级表现，并在 PSG 数据集上展现了优秀的泛化和效果。
</details></li>
</ul>
<hr>
<h2 id="A-Real-time-Method-for-Inserting-Virtual-Objects-into-Neural-Radiance-Fields"><a href="#A-Real-time-Method-for-Inserting-Virtual-Objects-into-Neural-Radiance-Fields" class="headerlink" title="A Real-time Method for Inserting Virtual Objects into Neural Radiance Fields"></a>A Real-time Method for Inserting Virtual Objects into Neural Radiance Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05837">http://arxiv.org/abs/2310.05837</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keyang Ye, Hongzhi Wu, Xin Tong, Kun Zhou</li>
<li>for: 插入虚拟物体intoNeRF中的场景，以实现真实的照明和阴影效果，并允许交互式地 manipulate虚拟物体。</li>
<li>methods: 利用NeRF中的光线和几何信息，解决了虚拟物体插入 augmented reality 中的多个挑战，包括照明估计、遮挡和阴影。</li>
<li>results: 比革命技术 superior 的照明和阴影效果，可以在实时 inserting 虚拟物体中使用，有很大的应用前途。<details>
<summary>Abstract</summary>
We present the first real-time method for inserting a rigid virtual object into a neural radiance field, which produces realistic lighting and shadowing effects, as well as allows interactive manipulation of the object. By exploiting the rich information about lighting and geometry in a NeRF, our method overcomes several challenges of object insertion in augmented reality. For lighting estimation, we produce accurate, robust and 3D spatially-varying incident lighting that combines the near-field lighting from NeRF and an environment lighting to account for sources not covered by the NeRF. For occlusion, we blend the rendered virtual object with the background scene using an opacity map integrated from the NeRF. For shadows, with a precomputed field of spherical signed distance field, we query the visibility term for any point around the virtual object, and cast soft, detailed shadows onto 3D surfaces. Compared with state-of-the-art techniques, our approach can insert virtual object into scenes with superior fidelity, and has a great potential to be further applied to augmented reality systems.
</details>
<details>
<summary>摘要</summary>
我们提出了首个实时方法，可以在神经辐射场中插入固定形式的虚拟物体，以生成真实的照明和阴影效果，同时允许交互式地修改物体。通过利用神经辐射场中的辐射信息和几何信息，我们的方法超越了虚拟物体插入增强现实系统中的许多挑战。对照明计算，我们生成了准确、可靠和三维空间变化的入射照明，将神经辐射中的近场照明和环境照明结合起来，以 compte для未被神经辐射覆盖的源。对遮挡，我们使用神经辐射中的 opacity map 混合背景场景，以实现虚拟物体与背景的混合。对阴影，我们使用预计算的球面正弦距离场来查询任意点附近虚拟物体的可见性，并投射出软、细节rich的阴影 onto 3D 表面。相比之前的技术，我们的方法可以在场景中插入虚拟物体的精度更高，并具有潜在的应用于增强现实系统。
</details></li>
</ul>
<hr>
<h2 id="Revisiting-the-Temporal-Modeling-in-Spatio-Temporal-Predictive-Learning-under-A-Unified-View"><a href="#Revisiting-the-Temporal-Modeling-in-Spatio-Temporal-Predictive-Learning-under-A-Unified-View" class="headerlink" title="Revisiting the Temporal Modeling in Spatio-Temporal Predictive Learning under A Unified View"></a>Revisiting the Temporal Modeling in Spatio-Temporal Predictive Learning under A Unified View</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05829">http://arxiv.org/abs/2310.05829</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheng Tan, Jue Wang, Zhangyang Gao, Siyuan Li, Lirong Wu, Jun Xia, Stan Z. Li</li>
<li>for: 这篇论文旨在探讨预测学习中的空间时间预测学习，具有广泛应用的应用领域。</li>
<li>methods: 论文提出了两种主流的时间模型方法，分别是回归基本的方法和回归自由的方法。</li>
<li>results: 论文透过实验证明，USTEP（统一空间时间预测学习）方法可以在各种预测学习任务中实现重要的改进，并成为一个可靠的解决方案。<details>
<summary>Abstract</summary>
Spatio-temporal predictive learning plays a crucial role in self-supervised learning, with wide-ranging applications across a diverse range of fields. Previous approaches for temporal modeling fall into two categories: recurrent-based and recurrent-free methods. The former, while meticulously processing frames one by one, neglect short-term spatio-temporal information redundancies, leading to inefficiencies. The latter naively stack frames sequentially, overlooking the inherent temporal dependencies. In this paper, we re-examine the two dominant temporal modeling approaches within the realm of spatio-temporal predictive learning, offering a unified perspective. Building upon this analysis, we introduce USTEP (Unified Spatio-TEmporal Predictive learning), an innovative framework that reconciles the recurrent-based and recurrent-free methods by integrating both micro-temporal and macro-temporal scales. Extensive experiments on a wide range of spatio-temporal predictive learning demonstrate that USTEP achieves significant improvements over existing temporal modeling approaches, thereby establishing it as a robust solution for a wide range of spatio-temporal applications.
</details>
<details>
<summary>摘要</summary>
In this paper, we re-examine the two dominant temporal modeling approaches within the realm of spatio-temporal predictive learning, providing a unified perspective. Building on this analysis, we introduce USTEP (Unified Spatio-TEmporal Predictive learning), an innovative framework that integrates both micro-temporal and macro-temporal scales. Extensive experiments on a wide range of spatio-temporal predictive learning tasks show that USTEP achieves significant improvements over existing temporal modeling approaches, establishing it as a robust solution for a wide range of spatio-temporal applications.
</details></li>
</ul>
<hr>
<h2 id="Provably-Convergent-Data-Driven-Convex-Nonconvex-Regularization"><a href="#Provably-Convergent-Data-Driven-Convex-Nonconvex-Regularization" class="headerlink" title="Provably Convergent Data-Driven Convex-Nonconvex Regularization"></a>Provably Convergent Data-Driven Convex-Nonconvex Regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05812">http://arxiv.org/abs/2310.05812</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zakhar Shumaylov, Jeremy Budd, Subhadip Mukherjee, Carola-Bibiane Schönlieb</li>
<li>for: 解决非对易问题的新方法是通过深度学习学习一个正则化函数。</li>
<li>methods: 我们使用了凸非凸（CNC）框架，并引入了一种新的输入弱凸神经网络（IWCNN）构建，以应用学习反对正则化方法。</li>
<li>results: 我们的方法可以在数据上实现高质量的解决，而不需要证明保证。在实验中，我们发现我们的方法可以超越之前的反对方法的数值问题。<details>
<summary>Abstract</summary>
An emerging new paradigm for solving inverse problems is via the use of deep learning to learn a regularizer from data. This leads to high-quality results, but often at the cost of provable guarantees. In this work, we show how well-posedness and convergent regularization arises within the convex-nonconvex (CNC) framework for inverse problems. We introduce a novel input weakly convex neural network (IWCNN) construction to adapt the method of learned adversarial regularization to the CNC framework. Empirically we show that our method overcomes numerical issues of previous adversarial methods.
</details>
<details>
<summary>摘要</summary>
新的一个思想是通过深度学习学习一个正则化函数从数据中学习，这会导致高质量的解决方案，但通常会额外付出可证明保证。在这个工作中，我们证明了在几何非几何（CNC）框架中的正则化和稳定性。我们介绍了一种新的输入弱有界神经网络（IWCNN）构建方法，以适应学习反对抗散射正则化的CNC框架。我们的方法在数据处理中超越了之前的反对抗散射方法的数学问题。
</details></li>
</ul>
<hr>
<h2 id="Joint-object-detection-and-re-identification-for-3D-obstacle-multi-camera-systems"><a href="#Joint-object-detection-and-re-identification-for-3D-obstacle-multi-camera-systems" class="headerlink" title="Joint object detection and re-identification for 3D obstacle multi-camera systems"></a>Joint object detection and re-identification for 3D obstacle multi-camera systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05785">http://arxiv.org/abs/2310.05785</a></li>
<li>repo_url: None</li>
<li>paper_authors: Irene Cortés, Jorge Beltrán, Arturo de la Escalera, Fernando García</li>
<li>for: 提高自动驾驶系统中对物体检测和识别的精度和效率。</li>
<li>methods: 提出了一种基于摄像头和激光雷达信息的物体检测网络修改方法，包括一个额外的分支用于在同一辆车辆内的相邻摄像头之间重新识别物体。</li>
<li>results: 对比传统非最大值选择（NMS）技术，提出的方法在汽车类划分领域提高了超过5%的性能。<details>
<summary>Abstract</summary>
In recent years, the field of autonomous driving has witnessed remarkable advancements, driven by the integration of a multitude of sensors, including cameras and LiDAR systems, in different prototypes. However, with the proliferation of sensor data comes the pressing need for more sophisticated information processing techniques. This research paper introduces a novel modification to an object detection network that uses camera and lidar information, incorporating an additional branch designed for the task of re-identifying objects across adjacent cameras within the same vehicle while elevating the quality of the baseline 3D object detection outcomes. The proposed methodology employs a two-step detection pipeline: initially, an object detection network is employed, followed by a 3D box estimator that operates on the filtered point cloud generated from the network's detections. Extensive experimental evaluations encompassing both 2D and 3D domains validate the effectiveness of the proposed approach and the results underscore the superiority of this method over traditional Non-Maximum Suppression (NMS) techniques, with an improvement of more than 5\% in the car category in the overlapping areas.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Towards-Lossless-Dataset-Distillation-via-Difficulty-Aligned-Trajectory-Matching"><a href="#Towards-Lossless-Dataset-Distillation-via-Difficulty-Aligned-Trajectory-Matching" class="headerlink" title="Towards Lossless Dataset Distillation via Difficulty-Aligned Trajectory Matching"></a>Towards Lossless Dataset Distillation via Difficulty-Aligned Trajectory Matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05773">http://arxiv.org/abs/2310.05773</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/GzyAftermath/DATM">https://github.com/GzyAftermath/DATM</a></li>
<li>paper_authors: Ziyao Guo, Kai Wang, George Cazenavette, Hui Li, Kaipeng Zhang, Yang You<br>for: 这个论文的目的是怎样的？methods: 这个论文使用了哪些方法？results: 这个论文得到了什么结果？Here are the answers in Simplified Chinese:for: 这个论文的目的是寻找一种能够实现完全无损数据压缩的方法，使得一个基于小样本集的模型可以与基于实际数据集的模型相比。methods: 这个论文使用了一种基于轨迹匹配的方法，即通过优化合成数据集来让模型在训练过程中学习类似的长期趋势。results: 这个论文成功地实现了完全无损数据压缩，即基于小样本集的模型可以与基于实际数据集的模型相比。此外，论文还发现了现有方法在生成大型合成数据集时失效的原因，并提出了一种根据数据集大小调整轨迹匹配的方法。<details>
<summary>Abstract</summary>
The ultimate goal of Dataset Distillation is to synthesize a small synthetic dataset such that a model trained on this synthetic set will perform equally well as a model trained on the full, real dataset. Until now, no method of Dataset Distillation has reached this completely lossless goal, in part due to the fact that previous methods only remain effective when the total number of synthetic samples is extremely small. Since only so much information can be contained in such a small number of samples, it seems that to achieve truly loss dataset distillation, we must develop a distillation method that remains effective as the size of the synthetic dataset grows. In this work, we present such an algorithm and elucidate why existing methods fail to generate larger, high-quality synthetic sets. Current state-of-the-art methods rely on trajectory-matching, or optimizing the synthetic data to induce similar long-term training dynamics as the real data. We empirically find that the training stage of the trajectories we choose to match (i.e., early or late) greatly affects the effectiveness of the distilled dataset. Specifically, early trajectories (where the teacher network learns easy patterns) work well for a low-cardinality synthetic set since there are fewer examples wherein to distribute the necessary information. Conversely, late trajectories (where the teacher network learns hard patterns) provide better signals for larger synthetic sets since there are now enough samples to represent the necessary complex patterns. Based on our findings, we propose to align the difficulty of the generated patterns with the size of the synthetic dataset. In doing so, we successfully scale trajectory matching-based methods to larger synthetic datasets, achieving lossless dataset distillation for the very first time. Code and distilled datasets are available at https://gzyaftermath.github.io/DATM.
</details>
<details>
<summary>摘要</summary>
最终目标 OF dataset distillation 是将一个小型合成数据集 synthesized 到一个模型在这个合成集上训练后表现与实际数据集上训练后表现相同。直到现在，没有任何dataset distillation方法达到了完全无损的目标，其中一个原因是前一些方法只有在非常小的合成样本数量下保持有效。由于只有一定量的信息可以包含在这么少样本中，因此我们认为，为实现真正无损的dataset distillation，我们必须开发一种可以随合成数据集的大小而变的效果的混合方法。在这项工作中，我们提出了一种算法，并解释了现有方法失败的原因。现有的state-of-the-art方法通过轨迹匹配来实现dataset distillation，即通过优化合成数据来让教师网络在实际数据中学习的长期训练动力类似。我们实验发现，在我们选择的轨迹阶段（例如，早期或晚期）对匹配的轨迹有着很大的影响。特别是，早期的轨迹（当教师网络学习易Patterns）适用于小 cardinality的合成数据集，因为这里有 fewer examples 可以分配必要的信息。相反，晚期的轨迹（当教师网络学习困难Patterns）在更大的合成数据集中提供更好的信号，因为现在有 enough samples 来表示必要的复杂Patterns。根据我们的发现，我们提议将合成数据集中的模式难度与合成数据集的大小进行对应。通过这种方式，我们成功地扩展了轨迹匹配基于方法到更大的合成数据集，实现了无损的dataset distillation，这是第一次。可以在 <https://gzyaftermath.github.io/DATM> 获取我们的代码和混合数据集。
</details></li>
</ul>
<hr>
<h2 id="3D-tomatoes’-localisation-with-monocular-cameras-using-histogram-filters"><a href="#3D-tomatoes’-localisation-with-monocular-cameras-using-histogram-filters" class="headerlink" title="3D tomatoes’ localisation with monocular cameras using histogram filters"></a>3D tomatoes’ localisation with monocular cameras using histogram filters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05762">http://arxiv.org/abs/2310.05762</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sandro Costa Magalhães, Filipe Neves dos Santos, António Paulo Moreira, Jorge Dias</li>
<li>for:  Tomatoes position estimation in open-field environments</li>
<li>methods:  Histogram Filters (Bayesian Discrete Filters) with square kernel and Gaussian kernel</li>
<li>results:  Mean absolute error lower than 10mm in simulation and 20mm in laboratory testbed at assessing distance of about 0.5m, viable for real environments but need improvement at closer distances.Here’s the breakdown of each point:1.  Tomatoes position estimation in open-field environments: The paper is focused on developing a method for estimating the position of tomatoes in open-field environments, which is a challenging task due to lighting interferences.2.  Histogram Filters (Bayesian Discrete Filters) with square kernel and Gaussian kernel: The paper proposes using Histogram Filters (Bayesian Discrete Filters) with two different kernel functions (square kernel and Gaussian kernel) to estimate the tomatoes’ positions.3.  Mean absolute error lower than 10mm in simulation and 20mm in laboratory testbed: The proposed method was tested in simulation and in a laboratory testbed, and the results showed a mean absolute error lower than 10mm in simulation and 20mm in the testbed, indicating that the method is effective for estimating tomatoes’ positions in open-field environments. However, the results also suggest that the method needs improvement at closer distances.<details>
<summary>Abstract</summary>
Performing tasks in agriculture, such as fruit monitoring or harvesting, requires perceiving the objects' spatial position. RGB-D cameras are limited under open-field environments due to lightning interferences. Therefore, in this study, we approach the use of Histogram Filters (Bayesian Discrete Filters) to estimate the position of tomatoes in the tomato plant. Two kernel filters were studied: the square kernel and the Gaussian kernel. The implemented algorithm was essayed in simulation, with and without Gaussian noise and random noise, and in a testbed at laboratory conditions. The algorithm reported a mean absolute error lower than 10 mm in simulation and 20 mm in the testbed at laboratory conditions with an assessing distance of about 0.5 m. So, the results are viable for real environments and should be improved at closer distances.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Unleashing-the-power-of-Neural-Collapse-for-Transferability-Estimation"><a href="#Unleashing-the-power-of-Neural-Collapse-for-Transferability-Estimation" class="headerlink" title="Unleashing the power of Neural Collapse for Transferability Estimation"></a>Unleashing the power of Neural Collapse for Transferability Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05754">http://arxiv.org/abs/2310.05754</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuhe Ding, Bo Jiang, Lijun Sheng, Aihua Zheng, Jian Liang</li>
<li>for: 这个论文的目的是提出一种新的传输可行性评估方法，以便无需 Fine-tuning 可以评估模型的适用性。</li>
<li>methods: 该方法基于现有 литературе中广泛使用的神经溃瘤指标，通过全面测量预训练模型的神经溃瘤程度来评估传输可行性。方法包括两个不同的项目：变差溃瘤项目，评估类别分离和类内紧凑程度；和类征公平项目，评估预训练模型对每个类别的公平性。</li>
<li>results: Results 表明 FaCe 在不同的预训练分类模型、不同的网络架构、源数据集和训练损失函数上都有出色的表现，并且在图像分类、 semantic segmentation 和文本分类等多种任务上达到了state-of-the-art的性能，这说明了 FaCe 的效果和普遍性。<details>
<summary>Abstract</summary>
Transferability estimation aims to provide heuristics for quantifying how suitable a pre-trained model is for a specific downstream task, without fine-tuning them all. Prior studies have revealed that well-trained models exhibit the phenomenon of Neural Collapse. Based on a widely used neural collapse metric in existing literature, we observe a strong correlation between the neural collapse of pre-trained models and their corresponding fine-tuned models. Inspired by this observation, we propose a novel method termed Fair Collapse (FaCe) for transferability estimation by comprehensively measuring the degree of neural collapse in the pre-trained model. Typically, FaCe comprises two different terms: the variance collapse term, which assesses the class separation and within-class compactness, and the class fairness term, which quantifies the fairness of the pre-trained model towards each class. We investigate FaCe on a variety of pre-trained classification models across different network architectures, source datasets, and training loss functions. Results show that FaCe yields state-of-the-art performance on different tasks including image classification, semantic segmentation, and text classification, which demonstrate the effectiveness and generalization of our method.
</details>
<details>
<summary>摘要</summary>
<<SYS>>对文本进行简化中文翻译。<</SYS>>转移性估计旨在提供预训练模型下游任务适用性的启发，无需细化所有。先前的研究表明，良好预训练模型会出现神经塌陷现象。根据现有文献中广泛使用的神经塌陷指标，我们发现预训练模型和其相应的细化模型之间存在强相关性。 inspirited by this observation， we propose a novel method termed Fair Collapse (FaCe) for transferability estimation by comprehensively measuring the degree of neural collapse in the pre-trained model. Typically, FaCe comprises two different terms: the variance collapse term, which assesses the class separation and within-class compactness, and the class fairness term, which quantifies the fairness of the pre-trained model towards each class. We investigate FaCe on a variety of pre-trained classification models across different network architectures, source datasets, and training loss functions. Results show that FaCe yields state-of-the-art performance on different tasks including image classification, semantic segmentation, and text classification, which demonstrate the effectiveness and generalization of our method.Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="HyperLips-Hyper-Control-Lips-with-High-Resolution-Decoder-for-Talking-Face-Generation"><a href="#HyperLips-Hyper-Control-Lips-with-High-Resolution-Decoder-for-Talking-Face-Generation" class="headerlink" title="HyperLips: Hyper Control Lips with High Resolution Decoder for Talking Face Generation"></a>HyperLips: Hyper Control Lips with High Resolution Decoder for Talking Face Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05720">http://arxiv.org/abs/2310.05720</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/semchan/HyperLips">https://github.com/semchan/HyperLips</a></li>
<li>paper_authors: Yaosen Chen, Yu Yao, Zhiqiang Li, Wei Wang, Yanru Zhang, Han Yang, Xuming Wen</li>
<li>for: 这个研究旨在提高现有的音频驱动的话头生成方法，以提供更高品质的视觉化面孔生成。</li>
<li>methods: 本研究提出了一个两阶段框架，包括一个干扰网络（HyperNet）来控制 lip 运动，以及一个高分辨率解oder（HRDecoder）来生成高品质的视觉化面孔内容。</li>
<li>results: 实验结果显示，本方法可以与现有的音频驱动的话头生成方法相比，提供更真实、高品质和 lip 同步的视觉化面孔生成。<details>
<summary>Abstract</summary>
Talking face generation has a wide range of potential applications in the field of virtual digital humans. However, rendering high-fidelity facial video while ensuring lip synchronization is still a challenge for existing audio-driven talking face generation approaches. To address this issue, we propose HyperLips, a two-stage framework consisting of a hypernetwork for controlling lips and a high-resolution decoder for rendering high-fidelity faces. In the first stage, we construct a base face generation network that uses the hypernetwork to control the encoding latent code of the visual face information over audio. First, FaceEncoder is used to obtain latent code by extracting features from the visual face information taken from the video source containing the face frame.Then, HyperConv, which weighting parameters are updated by HyperNet with the audio features as input, will modify the latent code to synchronize the lip movement with the audio. Finally, FaceDecoder will decode the modified and synchronized latent code into visual face content. In the second stage, we obtain higher quality face videos through a high-resolution decoder. To further improve the quality of face generation, we trained a high-resolution decoder, HRDecoder, using face images and detected sketches generated from the first stage as input.Extensive quantitative and qualitative experiments show that our method outperforms state-of-the-art work with more realistic, high-fidelity, and lip synchronization. Project page: https://semchan.github.io/HyperLips Project/
</details>
<details>
<summary>摘要</summary>
《带讲话脸生成》有广泛的应用前途在虚拟数字人类领域。然而，基于音频的 talking face生成方法仍然面临高精度脸部视频生成和同步脸部运动的挑战。为解决这个问题，我们提出了 HyperLips，它是一个两个阶段的框架，包括跨域网络（HyperNet）控制脸部运动的 lips 控制器和高分辨率解码器。在第一阶段，我们构建了基础脸部生成网络，使用 HyperNet 控制脸部信息的编码缓存码。首先， FaceEncoder 从视频源中提取脸部特征，然后 HyperConv 使用 HyperNet 更新参数，将编码缓存码与音频特征进行同步。最后， FaceDecoder 将修改和同步的编码缓存码解码为脸部内容。在第二阶段，我们通过使用高分辨率解码器（HRDecoder）来提高脸部生成质量。为了进一步提高脸部生成质量，我们在第一阶段使用 FaceImages 和 DetectedSketches 作为输入训练 HRDecoder。经过广泛的量化和质量测试，我们的方法在质量和同步性方面都能够超越当前的状态 искусственного智能。项目页面：https://semchan.github.io/HyperLips-Project/
</details></li>
</ul>
<hr>
<h2 id="EdVAE-Mitigating-Codebook-Collapse-with-Evidential-Discrete-Variational-Autoencoders"><a href="#EdVAE-Mitigating-Codebook-Collapse-with-Evidential-Discrete-Variational-Autoencoders" class="headerlink" title="EdVAE: Mitigating Codebook Collapse with Evidential Discrete Variational Autoencoders"></a>EdVAE: Mitigating Codebook Collapse with Evidential Discrete Variational Autoencoders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05718">http://arxiv.org/abs/2310.05718</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gulcin Baykal, Melih Kandemir, Gozde Unal</li>
<li>for: 本研究旨在解决深度生成模型在使用逻辑变量自动编码器（dVAE）时出现的抽象缺失问题。</li>
<li>methods: 本研究提出一种新的方法，即使用证据深度学习（EDL）取代softmax函数，以避免dVAE中的抽象缺失问题。</li>
<li>results: 我们的实验表明，使用EdVAE模型可以避免抽象缺失问题，提高重建性能，并提高代码库使用率，相比dVAE和VQ-VAE基于模型。<details>
<summary>Abstract</summary>
Codebook collapse is a common problem in training deep generative models with discrete representation spaces like Vector Quantized Variational Autoencoders (VQ-VAEs). We observe that the same problem arises for the alternatively designed discrete variational autoencoders (dVAEs) whose encoder directly learns a distribution over the codebook embeddings to represent the data. We hypothesize that using the softmax function to obtain a probability distribution causes the codebook collapse by assigning overconfident probabilities to the best matching codebook elements. In this paper, we propose a novel way to incorporate evidential deep learning (EDL) instead of softmax to combat the codebook collapse problem of dVAE. We evidentially monitor the significance of attaining the probability distribution over the codebook embeddings, in contrast to softmax usage. Our experiments using various datasets show that our model, called EdVAE, mitigates codebook collapse while improving the reconstruction performance, and enhances the codebook usage compared to dVAE and VQ-VAE based models.
</details>
<details>
<summary>摘要</summary>
<INST>代码表册塌陷是训练深度生成模型时的一个常见问题，特别是在使用柱状量编码的Vector Quantized Variational Autoencoders (VQ-VAEs) 中。我们发现，同样的问题也出现在 alternatively designed discrete variational autoencoders (dVAEs) 中，其Encoder直接学习一个分布来表示数据。我们认为，使用 softmax 函数获取概率分布会导致代码表册塌陷，因为它会将最佳匹配的代码表册元素授予过于自信的概率。在这篇论文中，我们提议使用 evidential deep learning (EDL) 来代替 softmax，以战胜 dVAE 中的代码表册塌陷问题。我们在不同的数据集上进行了实验，发现我们的模型（EdVAE）可以减轻代码表册塌陷，提高重建性能，并在 VQ-VAE 和 dVAE 基于模型中提高代码表 usage。</INST>Note that Simplified Chinese is a writing system that uses Chinese characters, but omits the traditional Chinese punctuation and grammatical markers. The translation is written in the Simplified Chinese format.
</details></li>
</ul>
<hr>
<h2 id="Uni3DETR-Unified-3D-Detection-Transformer"><a href="#Uni3DETR-Unified-3D-Detection-Transformer" class="headerlink" title="Uni3DETR: Unified 3D Detection Transformer"></a>Uni3DETR: Unified 3D Detection Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05699">http://arxiv.org/abs/2310.05699</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhenyuw16/uni3detr">https://github.com/zhenyuw16/uni3detr</a></li>
<li>paper_authors: Zhenyu Wang, Yali Li, Xi Chen, Hengshuang Zhao, Shengjin Wang</li>
<li>for: 这篇论文的目的是提出一个通用的3D探测器，可以在不同的场景中进行3D探测，包括室内和室外场景。</li>
<li>methods: 这篇论文使用了检测变换器和点矩阵交互来预测对象，并提出了混合查询点的方法，以便在不同的场景中进行最佳化。</li>
<li>results: 实验表明，Uni3DETR在室内和室外场景中都能够显示出优秀的性能，与特定场景的探测器不同，Uni3DETR具有强大的泛化能力。<details>
<summary>Abstract</summary>
Existing point cloud based 3D detectors are designed for the particular scene, either indoor or outdoor ones. Because of the substantial differences in object distribution and point density within point clouds collected from various environments, coupled with the intricate nature of 3D metrics, there is still a lack of a unified network architecture that can accommodate diverse scenes. In this paper, we propose Uni3DETR, a unified 3D detector that addresses indoor and outdoor 3D detection within the same framework. Specifically, we employ the detection transformer with point-voxel interaction for object prediction, which leverages voxel features and points for cross-attention and behaves resistant to the discrepancies from data. We then propose the mixture of query points, which sufficiently exploits global information for dense small-range indoor scenes and local information for large-range sparse outdoor ones. Furthermore, our proposed decoupled IoU provides an easy-to-optimize training target for localization by disentangling the xy and z space. Extensive experiments validate that Uni3DETR exhibits excellent performance consistently on both indoor and outdoor 3D detection. In contrast to previous specialized detectors, which may perform well on some particular datasets but suffer a substantial degradation on different scenes, Uni3DETR demonstrates the strong generalization ability under heterogeneous conditions (Fig. 1).   Codes are available at \href{https://github.com/zhenyuw16/Uni3DETR}{https://github.com/zhenyuw16/Uni3DETR}.
</details>
<details>
<summary>摘要</summary>
现有的点云基于3D检测器是为特定场景设计的，可以是indoor或outdoor场景。由于点云中对象的分布和点密度在不同环境中存在substantial differences，加之3D度量的复杂性，导致现有的网络架构无法涵盖多样化的场景。在这篇论文中，我们提出了Uni3DETR，一种通用的3D检测器，可以同时处理indoor和outdoor场景。我们使用检测转换器和点云交互来预测 объек，这里利用了点云的特征和 voxel 进行跨注意力和抗衰假。我们还提出了对于 dense small-range indoor scenes和large-range sparse outdoor scenes的mixture of query points， sufficient exploits 全球信息。此外，我们提出的分离 IoU 提供了一个容易优化的训练目标 для Localization，通过分离 xy 和 z 空间。我们的Uni3DETR在不同场景下表现出了很好的性能，与之前的特циализирован检测器不同，Uni3DETR 在多样化的环境下具有强大的泛化能力（图1）。代码可以在 <https://github.com/zhenyuw16/Uni3DETR> 上获取。
</details></li>
</ul>
<hr>
<h2 id="Combining-recurrent-and-residual-learning-for-deforestation-monitoring-using-multitemporal-SAR-images"><a href="#Combining-recurrent-and-residual-learning-for-deforestation-monitoring-using-multitemporal-SAR-images" class="headerlink" title="Combining recurrent and residual learning for deforestation monitoring using multitemporal SAR images"></a>Combining recurrent and residual learning for deforestation monitoring using multitemporal SAR images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05697">http://arxiv.org/abs/2310.05697</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carla Nascimento Neves, Raul Queiroz Feitosa, Mabel X. Ortega Adarme, Gilson Antonio Giraldi</li>
<li>for: 本研究旨在提高亚马逊雨林中的森林侵蚀检测精度，利用Synthetic Aperture Radar（SAR）多时序数据。</li>
<li>methods: 本研究提出了三种深度学习模型，包括RRCNN-1、RRCNN-2和RRCNN-3，用于森林侵蚀监测。这些模型都基于卷积神经网络，并且使用多时序SAR数据来提高检测精度。</li>
<li>results: 实验分析表明，使用多时序SAR数据可以更好地检测森林侵蚀。特别是，RRCNN-1模型在所有测试网络中显示出最高精度，同时具有半个处理时间的提高。<details>
<summary>Abstract</summary>
With its vast expanse, exceeding that of Western Europe by twice, the Amazon rainforest stands as the largest forest of the Earth, holding immense importance in global climate regulation. Yet, deforestation detection from remote sensing data in this region poses a critical challenge, often hindered by the persistent cloud cover that obscures optical satellite data for much of the year. Addressing this need, this paper proposes three deep-learning models tailored for deforestation monitoring, utilizing SAR (Synthetic Aperture Radar) multitemporal data moved by its independence on atmospheric conditions. Specifically, the study proposes three novel recurrent fully convolutional network architectures-namely, RRCNN-1, RRCNN-2, and RRCNN-3, crafted to enhance the accuracy of deforestation detection. Additionally, this research explores replacing a bitemporal with multitemporal SAR sequences, motivated by the hypothesis that deforestation signs quickly fade in SAR images over time. A comprehensive assessment of the proposed approaches was conducted using a Sentinel-1 multitemporal sequence from a sample site in the Brazilian rainforest. The experimental analysis confirmed that analyzing a sequence of SAR images over an observation period can reveal deforestation spots undetectable in a pair of images. Notably, experimental results underscored the superiority of the multitemporal approach, yielding approximately a five percent enhancement in F1-Score across all tested network architectures. Particularly the RRCNN-1 achieved the highest accuracy and also boasted half the processing time of its closest counterpart.
</details>
<details>
<summary>摘要</summary>
Amazon雨林是地球上最大的雨林之一，其盛装面积超过西欧的西欧面积之二倍，具有全球气候调节的重要性。然而，在这个区域中的森林开发探测从远程感应数据受到挑战，因为大部分年份受到云层覆盖，使光学卫星数据无法得到有效处理。为了解决这个需求，本研究提出了三个深度学习模型，用于森林开发监控，并利用Synthetic Aperture Radar（SAR）多时间序数据，不受大气情况的限制。具体来说，这些研究提出了三个新的循环型条件网络架构，称为RRCNN-1、RRCNN-2和RRCNN-3，以提高森林开发探测的精度。此外，这些研究还探讨了使用多时间序SAR序列取代双时间序列，这是因为开发痕迹在SAR图像中很快消失。经过了一系列的实验分析，发现可以通过分析多时间序SAR图像序列来探测森林开发痕迹，并且这种方法可以提高精度约五成。特别是RRCNN-1最高精度和处理时间的半倍，与其他架构相比。
</details></li>
</ul>
<hr>
<h2 id="Climate-sensitive-Urban-Planning-through-Optimization-of-Tree-Placements"><a href="#Climate-sensitive-Urban-Planning-through-Optimization-of-Tree-Placements" class="headerlink" title="Climate-sensitive Urban Planning through Optimization of Tree Placements"></a>Climate-sensitive Urban Planning through Optimization of Tree Placements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05691">http://arxiv.org/abs/2310.05691</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lmb-freiburg/tree-planting">https://github.com/lmb-freiburg/tree-planting</a></li>
<li>paper_authors: Simon Schrodi, Ferdinand Briegel, Max Argus, Andreas Christen, Thomas Brox</li>
<li>for:  Mitigating heat stress in urban areas through optimal placement of urban trees.</li>
<li>methods:  Using neural networks to simulate point-wise mean radiant temperatures and an iterated local search framework with tailored adaptations to optimize tree placements.</li>
<li>results:  Empirical efficacy of the approach across a wide spectrum of study areas and time scales, demonstrating the potential of urban trees to mitigate heat stress.<details>
<summary>Abstract</summary>
Climate change is increasing the intensity and frequency of many extreme weather events, including heatwaves, which results in increased thermal discomfort and mortality rates. While global mitigation action is undoubtedly necessary, so is climate adaptation, e.g., through climate-sensitive urban planning. Among the most promising strategies is harnessing the benefits of urban trees in shading and cooling pedestrian-level environments. Our work investigates the challenge of optimal placement of such trees. Physical simulations can estimate the radiative and thermal impact of trees on human thermal comfort but induce high computational costs. This rules out optimization of tree placements over large areas and considering effects over longer time scales. Hence, we employ neural networks to simulate the point-wise mean radiant temperatures--a driving factor of outdoor human thermal comfort--across various time scales, spanning from daily variations to extended time scales of heatwave events and even decades. To optimize tree placements, we harness the innate local effect of trees within the iterated local search framework with tailored adaptations. We show the efficacy of our approach across a wide spectrum of study areas and time scales. We believe that our approach is a step towards empowering decision-makers, urban designers and planners to proactively and effectively assess the potential of urban trees to mitigate heat stress.
</details>
<details>
<summary>摘要</summary>
климат变化会增加多种极端天气事件的Intensity和频率，包括热波，这会导致增加的热舒适度和死亡率。  global的气候适应措施是必要的，例如通过气候敏感的城市规划。  among the most promising strategies is to harness the benefits of urban trees in shading and cooling pedestrian-level environments.  our work investigates the challenge of optimal placement of such trees.  physical simulations can estimate the radiative and thermal impact of trees on human thermal comfort, but induce high computational costs.  This rules out optimization of tree placements over large areas and considering effects over longer time scales.  Hence, we employ neural networks to simulate the point-wise mean radiant temperatures--a driving factor of outdoor human thermal comfort--across various time scales, spanning from daily variations to extended time scales of heatwave events and even decades.  To optimize tree placements, we harness the innate local effect of trees within the iterated local search framework with tailored adaptations.  We show the efficacy of our approach across a wide spectrum of study areas and time scales.  We believe that our approach is a step towards empowering decision-makers, urban designers and planners to proactively and effectively assess the potential of urban trees to mitigate heat stress.
</details></li>
</ul>
<hr>
<h2 id="Analysis-of-Rainfall-Variability-and-Water-Extent-of-Selected-Hydropower-Reservoir-Using-Google-Earth-Engine-GEE-A-Case-Study-from-Two-Tropical-Countries-Sri-Lanka-and-Vietnam"><a href="#Analysis-of-Rainfall-Variability-and-Water-Extent-of-Selected-Hydropower-Reservoir-Using-Google-Earth-Engine-GEE-A-Case-Study-from-Two-Tropical-Countries-Sri-Lanka-and-Vietnam" class="headerlink" title="Analysis of Rainfall Variability and Water Extent of Selected Hydropower Reservoir Using Google Earth Engine (GEE): A Case Study from Two Tropical Countries, Sri Lanka and Vietnam"></a>Analysis of Rainfall Variability and Water Extent of Selected Hydropower Reservoir Using Google Earth Engine (GEE): A Case Study from Two Tropical Countries, Sri Lanka and Vietnam</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05682">http://arxiv.org/abs/2310.05682</a></li>
<li>repo_url: None</li>
<li>paper_authors: Punsisi Rajakaruna, Surajit Ghosh, Bunyod Holmatov<br>for: 这项研究旨在 investigate 瑞典和斯里兰卡两国热带季风地区降水模式和选择的水电库水域面积的关系。methods: 该研究使用高分辨率光学图像和Sentinel-1 Synthetic Aperture Radar（SAR）数据观察和监测不同天气情况下水体的变化，特别是在雨季期间。采用 Climate Hazards Group InfraRed Precipitation with Station（CHIRPS）数据进行降水年际变化的分析，并对选择的水库区域进行水域面积的 derivation。results: 研究结果显示，雨季期间的降水量带来水库水域面积的增加，而非雨季期间的降水量则导致水库水域面积的减少。这些结果表明降水模式对水库水资源的影响，并可以帮助这两个国家决策 relativity 水电、洪水管理和灌溉等方面的政策。<details>
<summary>Abstract</summary>
This study presents a comprehensive remote sensing analysis of rainfall patterns and selected hydropower reservoir water extent in two tropical monsoon countries, Vietnam and Sri Lanka. The aim is to understand the relationship between remotely sensed rainfall data and the dynamic changes (monthly) in reservoir water extent. The analysis utilizes high-resolution optical imagery and Sentinel-1 Synthetic Aperture Radar (SAR) data to observe and monitor water bodies during different weather conditions, especially during the monsoon season. The average annual rainfall for both countries is determined, and spatiotemporal variations in monthly average rainfall are examined at regional and reservoir basin levels using the Climate Hazards Group InfraRed Precipitation with Station (CHIRPS) dataset from 1981 to 2022. Water extents are derived for selected reservoirs using Sentinel-1 SAR Ground Range Detected (GRD) images in Vietnam and Sri Lanka from 2017 to 2022. The images are pre-processed and corrected using terrain correction and refined Lee filter. An automated thresholding algorithm, OTSU, distinguishes water and land, taking advantage of both VV and VH polarization data. The connected pixel count threshold is applied to enhance result accuracy. The results indicate a clear relationship between rainfall patterns and reservoir water extent, with increased precipitation during the monsoon season leading to higher water extents in the later months. This study contributes to understanding how rainfall variability impacts reservoir water resources in tropical monsoon regions. The preliminary findings can inform water resource management strategies and support these countries' decision-making processes related to hydropower generation, flood management, and irrigation.
</details>
<details>
<summary>摘要</summary>
本研究通过远程感知技术分析了越南和斯里兰卡两国热带雨季的降雨模式和选择的水电库水域面积。研究的目的是了解远程感知降雨数据和水库月度变化的关系。研究使用高分辨率光学图像和Sentinel-1Synthetic Aperture Radar（SAR）数据观察和监测不同天气情况下的水体，特别是在雨季期间。研究计算了两国的年平均降雨量，并对不同地区和水库流域的月平均降雨变化进行了空间时间分析使用CHIRPS数据集从1981年到2022年。水域面积 derive 从越南和斯里兰卡2017年到2022年的Sentinel-1 SAR Ground Range Detected（GRD）图像。图像进行了地形 corrections和精细的李 filter 修正。使用OTSU自动分割算法，利用VV和VH极化数据，将水和陆分开。应用连接 pixel 计数阈值以提高结果准确性。结果表明，降雨模式和水库水域面积之间存在明显的关系，雨季期间的降雨量增加会导致 later 月的水域面积增加。这项研究对热带雨季地区水库水资源的变化产生了重要影响，可以为这些国家的水资源管理策略和水电生产、洪水管理和灌溉决策提供参考。
</details></li>
</ul>
<hr>
<h2 id="Anchor-Intermediate-Detector-Decoupling-and-Coupling-Bounding-Boxes-for-Accurate-Object-Detection"><a href="#Anchor-Intermediate-Detector-Decoupling-and-Coupling-Bounding-Boxes-for-Accurate-Object-Detection" class="headerlink" title="Anchor-Intermediate Detector: Decoupling and Coupling Bounding Boxes for Accurate Object Detection"></a>Anchor-Intermediate Detector: Decoupling and Coupling Bounding Boxes for Accurate Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05666">http://arxiv.org/abs/2310.05666</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yilonglv/aid">https://github.com/yilonglv/aid</a></li>
<li>paper_authors: Yilong Lv, Min Li, Yujie He, Shaopeng Li, Zhuzhen He, Aitao Yang</li>
<li>for: 本文提出了一种新的检测器模型，即盒子解couple-couple(BDC)策略，以提高对象检测的准确率。</li>
<li>methods: 该模型使用了多个盒子 вместе工作，而不是单个盒子，以提高边界偏移的预测精度。具体来说，该模型使用了一个简单 yet novel的模型，即盒子间接头(AID)，其包括两个头网络：一个基于盒子的 anchor-based 头和一个基于盒子的 corner-aware 头。</li>
<li>results: 对于 MS COCO 测试 dataset，该模型的实验结果表明，与基eline RetinaNet 和 GFL 方法相比，该模型可以在无任何额外配置下达到 $\sim$2.4 和 $\sim$1.2 AP 的提升。<details>
<summary>Abstract</summary>
Anchor-based detectors have been continuously developed for object detection. However, the individual anchor box makes it difficult to predict the boundary's offset accurately. Instead of taking each bounding box as a closed individual, we consider using multiple boxes together to get prediction boxes. To this end, this paper proposes the \textbf{Box Decouple-Couple(BDC) strategy} in the inference, which no longer discards the overlapping boxes, but decouples the corner points of these boxes. Then, according to each corner's score, we couple the corner points to select the most accurate corner pairs. To meet the BDC strategy, a simple but novel model is designed named the \textbf{Anchor-Intermediate Detector(AID)}, which contains two head networks, i.e., an anchor-based head and an anchor-free \textbf{Corner-aware head}. The corner-aware head is able to score the corners of each bounding box to facilitate the coupling between corner points. Extensive experiments on MS COCO show that the proposed anchor-intermediate detector respectively outperforms their baseline RetinaNet and GFL method by $\sim$2.4 and $\sim$1.2 AP on the MS COCO test-dev dataset without any bells and whistles. Code is available at: https://github.com/YilongLv/AID.
</details>
<details>
<summary>摘要</summary>
锚点基于的检测器一直在不断发展，但是每个锚点盒子的偏移精度预测却受到限制。而不是单独处理每个 bounding box，我们可以考虑使用多个盒子共同预测。为此，本文提出了 \textbf{Box Decouple-Couple（BDC）策略}，不再产生 overlap 的盒子，而是解couple 盒子的角点。然后，根据每个角点的得分，我们couple 角点选择最准确的角点对。为满足 BDC 策略，我们设计了一种简单 yet novel的模型，即 \textbf{锚点-中间检测器（AID）}，它包含两个头网络，即 anchor-based 头和 anchor-free \textbf{角点检测头}。角点检测头可以为每个 bounding box 的角点分配得分，以便 coupling 角点。经验表明，我们的锚点-中间检测器在 MS COCO 测试数据集上分别超过了基eline RetinaNet 和 GFL 方法的 $\sim$2.4 和 $\sim$1.2 AP。代码可以在：https://github.com/YilongLv/AID 找到。
</details></li>
</ul>
<hr>
<h2 id="Exploiting-Manifold-Structured-Data-Priors-for-Improved-MR-Fingerprinting-Reconstruction"><a href="#Exploiting-Manifold-Structured-Data-Priors-for-Improved-MR-Fingerprinting-Reconstruction" class="headerlink" title="Exploiting Manifold Structured Data Priors for Improved MR Fingerprinting Reconstruction"></a>Exploiting Manifold Structured Data Priors for Improved MR Fingerprinting Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05647">http://arxiv.org/abs/2310.05647</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peng Li, Yuping Ji, Yue Hu</li>
<li>for: 高精度高精度MR fingerprinting（MRF）数据的重建问题解决</li>
<li>methods: 基于映射结构数据优先的新型MRF重建框架，利用指print映射到参数 manifold 提高重建性能</li>
<li>results: 实验结果显示，我们的方法可以在非 carteesian 探测场景下减少计算时间，并提高重建性能，比州前方法有显著提升<details>
<summary>Abstract</summary>
Estimating tissue parameter maps with high accuracy and precision from highly undersampled measurements presents one of the major challenges in MR fingerprinting (MRF). Many existing works project the recovered voxel fingerprints onto the Bloch manifold to improve reconstruction performance. However, little research focuses on exploiting the latent manifold structure priors among fingerprints. To fill this gap, we propose a novel MRF reconstruction framework based on manifold structured data priors. Since it is difficult to directly estimate the fingerprint manifold structure, we model the tissue parameters as points on a low-dimensional parameter manifold. We reveal that the fingerprint manifold shares the same intrinsic topology as the parameter manifold, although being embedded in different Euclidean spaces. To exploit the non-linear and non-local redundancies in MRF data, we divide the MRF data into spatial patches, and the similarity measurement among data patches can be accurately obtained using the Euclidean distance between the corresponding patches in the parameter manifold. The measured similarity is then used to construct the graph Laplacian operator, which represents the fingerprint manifold structure. Thus, the fingerprint manifold structure is introduced in the reconstruction framework by using the low-dimensional parameter manifold. Additionally, we incorporate the locally low-rank prior in the reconstruction framework to further utilize the local correlations within each patch for improved reconstruction performance. We also adopt a GPU-accelerated NUFFT library to accelerate reconstruction in non-Cartesian sampling scenarios. Experimental results demonstrate that our method can achieve significantly improved reconstruction performance with reduced computational time over the state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
估计组织参数地图具有高精度和精度从高度受损量测量中进行估计是MR fingerprinting（MRF）的一个主要挑战。许多现有的方法将recovered voxel fingerprints projection onto Bloch manifold以提高重建性能。然而， littleresearch关注在挖掘指纹 manifold的尚未知识之间的 latent manifold structure priors。为了填这个空白，我们提出了一种基于 manifold 结构数据约束的新的MRF重建框架。由于直接估计指纹 manifold 结构困难，我们模型了组织参数为低维度参数 manifold 上的点。我们发现，指纹 manifold 和参数 manifold 具有同一个内在结构，虽然在不同的欧几何空间中嵌入。为了利用MRF数据中的非线性和非本地征 redundancy，我们将MRF数据分成空间块，并通过在参数 manifold 上计算块之间的Euclidean距离来准确地获得数据块之间的相似度。这个测量的相似度后来用于构建图 Laplacian 算子，该算子表示指纹 manifold 结构。因此，指纹 manifold 结构通过使用参数 manifold 被引入到重建框架中。此外，我们还在重建框架中采用了本地低级别 prior，以利用每个块中的本地相关性来提高重建性能。我们还使用了加速非极化 sampling enario的GPU加速的NUFFT库来加速重建。实验结果表明，我们的方法可以在计算时间和重建性能两个方面具有显著改进，与现有方法相比。
</details></li>
</ul>
<hr>
<h2 id="Diagnosing-Catastrophe-Large-parts-of-accuracy-loss-in-continual-learning-can-be-accounted-for-by-readout-misalignment"><a href="#Diagnosing-Catastrophe-Large-parts-of-accuracy-loss-in-continual-learning-can-be-accounted-for-by-readout-misalignment" class="headerlink" title="Diagnosing Catastrophe: Large parts of accuracy loss in continual learning can be accounted for by readout misalignment"></a>Diagnosing Catastrophe: Large parts of accuracy loss in continual learning can be accounted for by readout misalignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05644">http://arxiv.org/abs/2310.05644</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Anthes, Sushrut Thorat, Peter König, Tim C. Kietzmann</li>
<li>for: This paper investigates the representational changes that underlie the phenomenon of catastrophic forgetting in artificial neural networks (ANNs) when trained on changing data distributions.</li>
<li>methods: The paper uses a combination of theoretical analysis and experimental studies to identify the three distinct processes that contribute to catastrophic forgetting.</li>
<li>results: The study finds that the largest component of catastrophic forgetting is a misalignment between hidden representations and readout layers, which causes internal representations to shift. Additionally, the study shows that representational geometry is partially conserved under this misalignment, but a small part of the information is irrecoverably lost. The findings have implications for deep learning applications that need to be continuously updated.Here’s the Chinese version of the information points:</li>
<li>for: 这篇论文研究人工神经网络（ANN）在数据分布变化训练下的表达变化，以解释它们快速忘记旧任务的现象。</li>
<li>methods: 论文使用理论分析和实验研究，确定快速忘记的三种主要过程。</li>
<li>results: 研究发现，快速忘记的主要组成部分是隐藏层和输出层之间的不同，导致内部表示的变化。此外，研究还发现，表示的几何结构在这种不同下仍有一定保留，但有一小部分信息丢失不可回收。这些发现对深度学习应用，需要不断更新，有益。<details>
<summary>Abstract</summary>
Unlike primates, training artificial neural networks on changing data distributions leads to a rapid decrease in performance on old tasks. This phenomenon is commonly referred to as catastrophic forgetting. In this paper, we investigate the representational changes that underlie this performance decrease and identify three distinct processes that together account for the phenomenon. The largest component is a misalignment between hidden representations and readout layers. Misalignment occurs due to learning on additional tasks and causes internal representations to shift. Representational geometry is partially conserved under this misalignment and only a small part of the information is irrecoverably lost. All types of representational changes scale with the dimensionality of hidden representations. These insights have implications for deep learning applications that need to be continuously updated, but may also aid aligning ANN models to the rather robust biological vision.
</details>
<details>
<summary>摘要</summary>
The largest component is a misalignment between hidden representations and readout layers. This misalignment occurs due to learning on additional tasks, causing internal representations to shift. Despite this misalignment, representational geometry is partially conserved, and only a small part of the information is irrecoverably lost.All types of representational changes scale with the dimensionality of hidden representations. These insights have implications for deep learning applications that need to be continuously updated, and may also aid in aligning ANN models with the robust biological vision.
</details></li>
</ul>
<hr>
<h2 id="High-Accuracy-and-Cost-Saving-Active-Learning-3D-WD-UNet-for-Airway-Segmentation"><a href="#High-Accuracy-and-Cost-Saving-Active-Learning-3D-WD-UNet-for-Airway-Segmentation" class="headerlink" title="High Accuracy and Cost-Saving Active Learning 3D WD-UNet for Airway Segmentation"></a>High Accuracy and Cost-Saving Active Learning 3D WD-UNet for Airway Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05638">http://arxiv.org/abs/2310.05638</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiyi Wang, Yang Nan, Simon Walsh, Guang Yang</li>
<li>for: 降低医疗3D计算机断层成像（CT）分割的标注努力。</li>
<li>methods: 提出了一种新的深度活动学习（DeepAL）模型-3D Wasserstein Discriminative UNet（WD-UNet），通过在半supervised的方式学习，加速学习减速，使得模型可以与supervised学习模型匹配或超越其预测结果。</li>
<li>results: 在3D肺空气道CT扫描图像中进行医疗分割，使用不确定度度量（参数化为查询策略的输入），导致更准确的预测结果，比如3DUNet和3D CEUNet等状态当前的深度学习超vised模型。相比之下，WD-UNet不仅节省了诊断员的标注成本，还节省了计算资源。WD-UNet使用有限的标注数据（35%的总数），实现更好的预测 метриク。<details>
<summary>Abstract</summary>
We propose a novel Deep Active Learning (DeepAL) model-3D Wasserstein Discriminative UNet (WD-UNet) for reducing the annotation effort of medical 3D Computed Tomography (CT) segmentation. The proposed WD-UNet learns in a semi-supervised way and accelerates learning convergence to meet or exceed the prediction metrics of supervised learning models. Our method can be embedded with different Active Learning (AL) strategies and different network structures. The model is evaluated on 3D lung airway CT scans for medical segmentation and show that the use of uncertainty metric, which is parametrized as an input of query strategy, leads to more accurate prediction results than some state-of-the-art Deep Learning (DL) supervised models, e.g.,3DUNet and 3D CEUNet. Compared to the above supervised DL methods, our WD-UNet not only saves the cost of annotation for radiologists but also saves computational resources. WD-UNet uses a limited amount of annotated data (35% of the total) to achieve better predictive metrics with a more efficient deep learning model algorithm.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的深度活动学习（DeepAL）模型——3D Wasserstein Discriminative UNet（WD-UNet），用于降低医疗3D计算机Tomography（CT） segmentation的注释努力。我们的WD-UNet在半upervised的方式学习，并加速学习征 Stern 到达或超越supervised学习模型的预测 метри。我们的方法可以与不同的活动学习（AL）策略和不同的网络结构结合使用。我们的模型在3D肺空气道CT扫描图中进行医疗分 segmentation的测试，并显示了使用uncertainty度量（作为查询策略的输入参数）可以获得更准确的预测结果，比如State-of-the-art的深度学习（DL）supervised模型，如3DUNet和3D CEUNet。相比这些supervised DL方法，我们的WD-UNet不仅可以为放射学家节省注释成本，还可以节省计算资源。WD-UNet使用limited amount of annotated data（35% of the total）可以达到更好的预测 метри，同时使用更高效的深度学习算法。
</details></li>
</ul>
<hr>
<h2 id="Locality-Aware-Generalizable-Implicit-Neural-Representation"><a href="#Locality-Aware-Generalizable-Implicit-Neural-Representation" class="headerlink" title="Locality-Aware Generalizable Implicit Neural Representation"></a>Locality-Aware Generalizable Implicit Neural Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05624">http://arxiv.org/abs/2310.05624</a></li>
<li>repo_url: None</li>
<li>paper_authors: Doyup Lee, Chiheon Kim, Minsu Cho, Wook-Shin Han</li>
<li>for: 这 paper 的目的是提出一种新的普适隐藏表示（INR）框架，以便单个连续函数可以表示多个数据实例。</li>
<li>methods: 该框架 combinest 一个 transformer 编码器和一个具有本地化能力的 INR 解码器。 transformer 编码器 预测数据实例中的一系列隐藏 токен，以编码本地信息。 INR 解码器 通过 Cross-Attention 进行选择性聚合隐藏 токен，并逐步解码以获得输出。</li>
<li>results: 该框架在 previous 的普适 INR 上显著提高表达能力，并验证了本地化隐藏的有用性 для 下游任务 such as 图像生成。<details>
<summary>Abstract</summary>
Generalizable implicit neural representation (INR) enables a single continuous function, i.e., a coordinate-based neural network, to represent multiple data instances by modulating its weights or intermediate features using latent codes. However, the expressive power of the state-of-the-art modulation is limited due to its inability to localize and capture fine-grained details of data entities such as specific pixels and rays. To address this issue, we propose a novel framework for generalizable INR that combines a transformer encoder with a locality-aware INR decoder. The transformer encoder predicts a set of latent tokens from a data instance to encode local information into each latent token. The locality-aware INR decoder extracts a modulation vector by selectively aggregating the latent tokens via cross-attention for a coordinate input and then predicts the output by progressively decoding with coarse-to-fine modulation through multiple frequency bandwidths. The selective token aggregation and the multi-band feature modulation enable us to learn locality-aware representation in spatial and spectral aspects, respectively. Our framework significantly outperforms previous generalizable INRs and validates the usefulness of the locality-aware latents for downstream tasks such as image generation.
</details>
<details>
<summary>摘要</summary>
通用隐藏神经表示（INR）可以使一个连续函数，即坐标基本神经网络，表示多个数据实例。通过调整其权重或中间特征使用隐藏代码来模块化其权重。然而，现有的模块化表达能力有限，因为它无法当地化和捕捉数据实体细节，如特定像素和射线。为解决这个问题，我们提出了一种新的框架 для通用INR，该框架将transformer编码器与本地性感知INR解码器结合。transformer编码器预测一个数据实例的latent token集，从而对每个latent token进行本地信息编码。本地性感知INR解码器通过对坐标输入进行交叉注意力选择性聚合latent token，然后逐渐解码为多个频率带宽，进行进一步的坐标解码和特征修饰。这种选择性的token聚合和多频特征修饰使得我们可以学习本地特征表示，并在空间和频率方面进行本地化。我们的框架在前期INR的表达能力方面显著超越了现有的方法，并证明了隐藏代码的有用性 для下游任务，如图像生成。
</details></li>
</ul>
<hr>
<h2 id="ASM-Adaptive-Sample-Mining-for-In-The-Wild-Facial-Expression-Recognition"><a href="#ASM-Adaptive-Sample-Mining-for-In-The-Wild-Facial-Expression-Recognition" class="headerlink" title="ASM: Adaptive Sample Mining for In-The-Wild Facial Expression Recognition"></a>ASM: Adaptive Sample Mining for In-The-Wild Facial Expression Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05618">http://arxiv.org/abs/2310.05618</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyang Zhang, Xiao Sun, Liuwei An, Meng Wang</li>
<li>for: 提高 facial expression recognition (FER) 数据集中表达不确定性和标注错误的问题。</li>
<li>methods: 提出了一种名为 Adaptive Sample Mining (ASM) 的新方法，通过动态地处理每个表达类别中的不确定性和噪声来解决这个问题。</li>
<li>results: 经验证明，该方法可以有效地挖掘不确定性和噪声，并在 synthetic noisy 和原始数据集上超越 State-of-the-Art (SOTA) 方法。<details>
<summary>Abstract</summary>
Given the similarity between facial expression categories, the presence of compound facial expressions, and the subjectivity of annotators, facial expression recognition (FER) datasets often suffer from ambiguity and noisy labels. Ambiguous expressions are challenging to differentiate from expressions with noisy labels, which hurt the robustness of FER models. Furthermore, the difficulty of recognition varies across different expression categories, rendering a uniform approach unfair for all expressions. In this paper, we introduce a novel approach called Adaptive Sample Mining (ASM) to dynamically address ambiguity and noise within each expression category. First, the Adaptive Threshold Learning module generates two thresholds, namely the clean and noisy thresholds, for each category. These thresholds are based on the mean class probabilities at each training epoch. Next, the Sample Mining module partitions the dataset into three subsets: clean, ambiguity, and noise, by comparing the sample confidence with the clean and noisy thresholds. Finally, the Tri-Regularization module employs a mutual learning strategy for the ambiguity subset to enhance discrimination ability, and an unsupervised learning strategy for the noise subset to mitigate the impact of noisy labels. Extensive experiments prove that our method can effectively mine both ambiguity and noise, and outperform SOTA methods on both synthetic noisy and original datasets. The supplement material is available at https://github.com/zzzzzzyang/ASM.
</details>
<details>
<summary>摘要</summary>
由于表情表达category之间的相似性，合成表情和评分器的 Subjectivity， facial expression recognition（FER）数据集经常受到模糊和噪声的影响。模糊的表情和噪声标签降低FER模型的稳定性。此外，不同表情类别的识别难度不同，这使得一种一致的方法不公平对所有表情。在这篇论文中，我们提出了一种新的方法called Adaptive Sample Mining（ASM），用于动态地处理表情中的模糊和噪声。首先，Adaptive Threshold Learning模块生成了每个类别的两个阈值：净和噪声阈值。这些阈值基于每个训练epoch的类别mean probability。接着，Sample Mining模块将数据集分为三个子集：净、模糊和噪声，根据样本信息与净和噪声阈值进行比较。最后，Tri-Regularization模块使用了一种互学习策略来增强模糊子集的推理能力，并使用了一种无监督学习策略来抑制噪声标签的影响。我们的方法能够有效地挖掘模糊和噪声，并在 synthetic noisy和原始数据集上超越了State-of-the-Art（SOTA）方法。详细实验结果可以在https://github.com/zzzzzzyang/ASM中找到。
</details></li>
</ul>
<hr>
<h2 id="Care3D-An-Active-3D-Object-Detection-Dataset-of-Real-Robotic-Care-Environments"><a href="#Care3D-An-Active-3D-Object-Detection-Dataset-of-Real-Robotic-Care-Environments" class="headerlink" title="Care3D: An Active 3D Object Detection Dataset of Real Robotic-Care Environments"></a>Care3D: An Active 3D Object Detection Dataset of Real Robotic-Care Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05600">http://arxiv.org/abs/2310.05600</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/m-g-a/care3d">https://github.com/m-g-a/care3d</a></li>
<li>paper_authors: Michael G. Adam, Sebastian Eger, Martin Piccolrovazzi, Maged Iskandar, Joern Vogel, Alexander Dietrich, Seongjien Bien, Jon Skerlj, Abdeldjallil Naceri, Eckehard Steinbach, Alin Albu-Schaeffer, Sami Haddadin, Wolfram Burgard</li>
<li>for: 为了弥补医疗领域人才匮乏的问题，这篇短文提供了一个帮助Robotics开发的注释数据集。</li>
<li>methods: 本文使用了真实环境中的捕捉数据，以及一个医疗机器人内部的真实环境进行描述。</li>
<li>results: 本文提供了一个可靠的SLAM算法评估数据集，以便在医疗机器人上运行SLAM算法。<details>
<summary>Abstract</summary>
As labor shortage increases in the health sector, the demand for assistive robotics grows. However, the needed test data to develop those robots is scarce, especially for the application of active 3D object detection, where no real data exists at all. This short paper counters this by introducing such an annotated dataset of real environments. The captured environments represent areas which are already in use in the field of robotic health care research. We further provide ground truth data within one room, for assessing SLAM algorithms running directly on a health care robot.
</details>
<details>
<summary>摘要</summary>
随着医疗领域的劳动力短缺增加，需求 для协助 роботикс也在增长。然而，为开发这些机器人所需的测试数据却缺乏，尤其是在活动3D对象检测方面，无现实数据存在。这短篇论文解决了这个问题，通过发布真实环境的注释数据集。这些捕捉的环境代表了现在在医疗机器人研究领域已经使用的区域。我们还提供了一个房间内的真实数据，用于评估运行直接在医疗机器人上的SLAM算法。
</details></li>
</ul>
<hr>
<h2 id="Perceptual-Artifacts-Localization-for-Image-Synthesis-Tasks"><a href="#Perceptual-Artifacts-Localization-for-Image-Synthesis-Tasks" class="headerlink" title="Perceptual Artifacts Localization for Image Synthesis Tasks"></a>Perceptual Artifacts Localization for Image Synthesis Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05590">http://arxiv.org/abs/2310.05590</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/open-mmlab/mmsegmentation">https://github.com/open-mmlab/mmsegmentation</a></li>
<li>paper_authors: Lingzhi Zhang, Zhengjie Xu, Connelly Barnes, Yuqian Zhou, Qing Liu, He Zhang, Sohrab Amirghodsi, Zhe Lin, Eli Shechtman, Jianbo Shi</li>
<li>for: 这个论文主要是为了研究图像生成模型中的感知缺陷，以及如何自动修复这些缺陷。</li>
<li>methods: 该论文使用了一种新的数据集，并提出了一种基于分割模型的感知缺陷地图生成方法。</li>
<li>results: 该论文的实验结果显示，该方法可以有效地检测和修复图像生成中的感知缺陷，并且可以适应不同的图像生成模型。<details>
<summary>Abstract</summary>
Recent advancements in deep generative models have facilitated the creation of photo-realistic images across various tasks. However, these generated images often exhibit perceptual artifacts in specific regions, necessitating manual correction. In this study, we present a comprehensive empirical examination of Perceptual Artifacts Localization (PAL) spanning diverse image synthesis endeavors. We introduce a novel dataset comprising 10,168 generated images, each annotated with per-pixel perceptual artifact labels across ten synthesis tasks. A segmentation model, trained on our proposed dataset, effectively localizes artifacts across a range of tasks. Additionally, we illustrate its proficiency in adapting to previously unseen models using minimal training samples. We further propose an innovative zoom-in inpainting pipeline that seamlessly rectifies perceptual artifacts in the generated images. Through our experimental analyses, we elucidate several practical downstream applications, such as automated artifact rectification, non-referential image quality evaluation, and abnormal region detection in images. The dataset and code are released.
</details>
<details>
<summary>摘要</summary>
中文翻译：现代深度生成模型的发展使得可以生成高品质的图像，但是这些生成图像经常在特定区域出现感知 artifacts，需要手动修正。在这项研究中，我们对感知artifacts的地方进行了广泛的实验性研究，涵盖了多种图像生成任务。我们提出了一个新的数据集，包含10,168个生成图像，每个图像都有每像素的感知artifact标签。我们训练了一个分割模型，并在我们提出的数据集上进行了训练。我们发现这个模型可以在多种任务上有效地 lokalisieren artifacts。此外，我们还展示了它可以使用最小的训练样本适应未经见过的模型。我们还提出了一种innovative的缩放填充框架，可以轻松地修正生成图像中的感知 artifacts。通过我们的实验分析，我们描述了一些实用的下游应用，例如自动修正感知 artifacts、非参照图像质量评价和图像中异常区域检测。我们的数据集和代码都已经发布。
</details></li>
</ul>
<hr>
<h2 id="A-review-of-uncertainty-quantification-in-medical-image-analysis-probabilistic-and-non-probabilistic-methods"><a href="#A-review-of-uncertainty-quantification-in-medical-image-analysis-probabilistic-and-non-probabilistic-methods" class="headerlink" title="A review of uncertainty quantification in medical image analysis: probabilistic and non-probabilistic methods"></a>A review of uncertainty quantification in medical image analysis: probabilistic and non-probabilistic methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06873">http://arxiv.org/abs/2310.06873</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ling Huang, Su Ruan, Yucheng Xing, Mengling Feng</li>
<li>for: 这种评估机器学习模型可靠性的方法，可以帮助医生更好地理解和accept机器学习模型的结果。</li>
<li>methods: 这篇文章介绍了多种评估机器学习模型的不确定性方法，包括概率方法和非概率方法。</li>
<li>results: 文章提供了一个全面的审视，涵盖了各种医学图像任务中机器学习模型的不确定性评估方法。<details>
<summary>Abstract</summary>
The comprehensive integration of machine learning healthcare models within clinical practice remains suboptimal, notwithstanding the proliferation of high-performing solutions reported in the literature. A predominant factor hindering widespread adoption pertains to an insufficiency of evidence affirming the reliability of the aforementioned models. Recently, uncertainty quantification methods have been proposed as a potential solution to quantify the reliability of machine learning models and thus increase the interpretability and acceptability of the result. In this review, we offer a comprehensive overview of prevailing methods proposed to quantify uncertainty inherent in machine learning models developed for various medical image tasks. Contrary to earlier reviews that exclusively focused on probabilistic methods, this review also explores non-probabilistic approaches, thereby furnishing a more holistic survey of research pertaining to uncertainty quantification for machine learning models. Analysis of medical images with the summary and discussion on medical applications and the corresponding uncertainty evaluation protocols are presented, which focus on the specific challenges of uncertainty in medical image analysis. We also highlight some potential future research work at the end. Generally, this review aims to allow researchers from both clinical and technical backgrounds to gain a quick and yet in-depth understanding of the research in uncertainty quantification for medical image analysis machine learning models.
</details>
<details>
<summary>摘要</summary>
文章概要：随着机器学习模型在医疗实践中的普及，其完整性仍然受到限制，尽管文献中报道了高性能解决方案。主要阻碍广泛采用的因素是证据不足，证明机器学习模型的可靠性。最近，不确定性评估方法得到了关注，以量化机器学习模型中的不确定性。本文提供了各种不确定性评估方法的总览，包括非概率方法，以提供更全面的研究报告。分析医疗影像的总结和讨论，以及相应的不确定性评估协议，将注重医疗影像分析中的特殊挑战。此外，我们还提出了未来研究的可能性。本文的目标是为医疗和技术背景的研究人员提供快速但深入了解机器学习模型在医疗影像分析中的不确定性评估研究。
</details></li>
</ul>
<hr>
<h2 id="A-Simple-and-Robust-Framework-for-Cross-Modality-Medical-Image-Segmentation-applied-to-Vision-Transformers"><a href="#A-Simple-and-Robust-Framework-for-Cross-Modality-Medical-Image-Segmentation-applied-to-Vision-Transformers" class="headerlink" title="A Simple and Robust Framework for Cross-Modality Medical Image Segmentation applied to Vision Transformers"></a>A Simple and Robust Framework for Cross-Modality Medical Image Segmentation applied to Vision Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05572">http://arxiv.org/abs/2310.05572</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/matteo-bastico/mi-seg">https://github.com/matteo-bastico/mi-seg</a></li>
<li>paper_authors: Matteo Bastico, David Ryckelynck, Laurent Corté, Yannick Tillier, Etienne Decencière<br>for:This paper aims to address the challenge of cross-modality image segmentation, where a single model can perform well on multiple types of images.methods:The proposed method uses a simple framework that adapts normalization layers based on the input type, trained with non-registered interleaved mixed data.results:The proposed method outperforms other cross-modality segmentation methods on the Multi-Modality Whole Heart Segmentation Challenge, with an improvement of up to 6.87% in Dice accuracy. Additionally, the proposed Conditional Vision Transformer (C-ViT) encoder brings significant improvements to the resulting segmentation.<details>
<summary>Abstract</summary>
When it comes to clinical images, automatic segmentation has a wide variety of applications and a considerable diversity of input domains, such as different types of Magnetic Resonance Images (MRIs) and Computerized Tomography (CT) scans. This heterogeneity is a challenge for cross-modality algorithms that should equally perform independently of the input image type fed to them. Often, segmentation models are trained using a single modality, preventing generalization to other types of input data without resorting to transfer learning techniques. Furthermore, the multi-modal or cross-modality architectures proposed in the literature frequently require registered images, which are not easy to collect in clinical environments, or need additional processing steps, such as synthetic image generation. In this work, we propose a simple framework to achieve fair image segmentation of multiple modalities using a single conditional model that adapts its normalization layers based on the input type, trained with non-registered interleaved mixed data. We show that our framework outperforms other cross-modality segmentation methods, when applied to the same 3D UNet baseline model, on the Multi-Modality Whole Heart Segmentation Challenge. Furthermore, we define the Conditional Vision Transformer (C-ViT) encoder, based on the proposed cross-modality framework, and we show that it brings significant improvements to the resulting segmentation, up to 6.87\% of Dice accuracy, with respect to its baseline reference. The code to reproduce our experiments and the trained model weights are available at https://github.com/matteo-bastico/MI-Seg.
</details>
<details>
<summary>摘要</summary>
当来到临床图像时，自动分割有很多应用场景和各种输入领域的多样性，如不同类型的核磁共振成像（MRI）和计算机成像（CT）扫描。这种多样性是跨模态算法的挑战，这些算法需要独立于输入图像类型来运行。常见的分割模型在训练时使用单一模态，这使得它们无法泛化到其他输入数据类型，需要使用传输学习技术。此外，在文献中提出的多模态或跨模态架构常需要已经注册的图像，这些图像在临床环境中很难获得，或需要额外的处理步骤，如生成synthetic图像。在这种工作中，我们提出了一个简单的框架，可以实现多模态图像分割的公平性，使用单个条件模型，该模型根据输入类型调整正规化层。我们表明，我们的框架在与同一个3D UNet基础模型进行比较时，已经超越了其他跨模态分割方法。此外，我们定义了 Conditional Vision Transformer（C-ViT）Encoder，基于我们的跨模态框架，并证明它对结果的分割带来了显著改进，达到6.87%的Dice准确率，相比其参照基eline。我们在GitHub上提供了实验代码和训练模型参数，请参考https://github.com/matteo-bastico/MI-Seg。
</details></li>
</ul>
<hr>
<h2 id="M3FPolypSegNet-Segmentation-Network-with-Multi-frequency-Feature-Fusion-for-Polyp-Localization-in-Colonoscopy-Images"><a href="#M3FPolypSegNet-Segmentation-Network-with-Multi-frequency-Feature-Fusion-for-Polyp-Localization-in-Colonoscopy-Images" class="headerlink" title="M3FPolypSegNet: Segmentation Network with Multi-frequency Feature Fusion for Polyp Localization in Colonoscopy Images"></a>M3FPolypSegNet: Segmentation Network with Multi-frequency Feature Fusion for Polyp Localization in Colonoscopy Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05538">http://arxiv.org/abs/2310.05538</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ju-Hyeon Nam, Seo-Hyeong Park, Nur Suriza Syazwany, Yerim Jung, Yu-Han Im, Sang-Chul Lee</li>
<li>For: 降低抑制肠癌的风险，通过自动segmentation of polyps* Methods: 使用深度学习， Specifically, a novel frequency-based fully convolutional neural network (M3FPolypSegNet) that decomposes the input image into low&#x2F;high&#x2F;full-frequency components and uses multiple independent multi-frequency encoders to map the input image into a high-dimensional feature space.* Results: 比较多种segmentation模型，取得性能提升6.92%和7.52%的平均提升值， indicating that the proposed model outperformed various segmentation models.<details>
<summary>Abstract</summary>
Polyp segmentation is crucial for preventing colorectal cancer a common type of cancer. Deep learning has been used to segment polyps automatically, which reduces the risk of misdiagnosis. Localizing small polyps in colonoscopy images is challenging because of its complex characteristics, such as color, occlusion, and various shapes of polyps. To address this challenge, a novel frequency-based fully convolutional neural network, Multi-Frequency Feature Fusion Polyp Segmentation Network (M3FPolypSegNet) was proposed to decompose the input image into low/high/full-frequency components to use the characteristics of each component. We used three independent multi-frequency encoders to map multiple input images into a high-dimensional feature space. In the Frequency-ASPP Scalable Attention Module (F-ASPP SAM), ASPP was applied between each frequency component to preserve scale information. Subsequently, scalable attention was applied to emphasize polyp regions in a high-dimensional feature space. Finally, we designed three multi-task learning (i.e., region, edge, and distance) in four decoder blocks to learn the structural characteristics of the region. The proposed model outperformed various segmentation models with performance gains of 6.92% and 7.52% on average for all metrics on CVC-ClinicDB and BKAI-IGH-NeoPolyp, respectively.
</details>
<details>
<summary>摘要</summary>
《多脉冲分割是预防结肠癌的关键，深度学习已经用于自动分割脉冲，从而减少误诊风险。在colonoscopy图像中 lokalisir small脉冲具有复杂的特征，如颜色、 occlusion 和不同形状的脉冲。为解决这个挑战，我们提出了一种基于频率的全 convolutional neural network，即Multi-Frequency Feature Fusion Polyp Segmentation Network (M3FPolypSegNet)。我们将输入图像分解成低/高/全频率组成部分，并使用每个组成部分的特征来进行分割。在Frequency-ASPP Scalable Attention Module (F-ASPP SAM)中，我们应用了ASPP между每个频率组成部分，以保持尺度信息。然后，我们应用了可扩展的注意力 Mechanism来强调脉冲区域在高维特征空间中。最后，我们设计了三种多任务学习（即区域、边缘和距离），并在四个解码块中进行学习区域的结构特征。我们的模型在CVC-ClinicDB和BKAI-IGH-NeoPolyp上的各种评价指标上表现出了6.92%和7.52%的性能提升，相对于其他分割模型。》
</details></li>
</ul>
<hr>
<h2 id="Bi-directional-Deformation-for-Parameterization-of-Neural-Implicit-Surfaces"><a href="#Bi-directional-Deformation-for-Parameterization-of-Neural-Implicit-Surfaces" class="headerlink" title="Bi-directional Deformation for Parameterization of Neural Implicit Surfaces"></a>Bi-directional Deformation for Parameterization of Neural Implicit Surfaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05524">http://arxiv.org/abs/2310.05524</a></li>
<li>repo_url: None</li>
<li>paper_authors: Baixin Xu, Jiangbei Hu, Fei Hou, Kwan-Yee Lin, Wayne Wu, Chen Qian, Ying He</li>
<li>for: 本研究旨在提供一种能够直观地编辑3D对象的新技术，特别是当3D对象表示为神经隐存函数时。</li>
<li>methods: 我们提出了一种基于神经网络的参数化方法，将3D对象映射到简单的参数域中，如球体、立方体或多面体，从而方便视觉化和编辑工作。我们采用了双向偏振变换，以消除任何先前信息的需求。</li>
<li>results: 我们的方法可以快速渲染编辑后的текxture图像，无需重新训练神经网络。此外，我们的方法还支持多对象共参数化和Texture传输。我们在人头和人工对象的图像上进行了实验，并将源代码公开发布。<details>
<summary>Abstract</summary>
The growing capabilities of neural rendering have increased the demand for new techniques that enable the intuitive editing of 3D objects, particularly when they are represented as neural implicit surfaces. In this paper, we present a novel neural algorithm to parameterize neural implicit surfaces to simple parametric domains, such as spheres, cubes or polycubes, where 3D radiance field can be represented as a 2D field, thereby facilitating visualization and various editing tasks. Technically, our method computes a bi-directional deformation between 3D objects and their chosen parametric domains, eliminating the need for any prior information. We adopt a forward mapping of points on the zero level set of the 3D object to a parametric domain, followed by a backward mapping through inverse deformation. To ensure the map is bijective, we employ a cycle loss while optimizing the smoothness of both deformations. Additionally, we leverage a Laplacian regularizer to effectively control angle distortion and offer the flexibility to choose from a range of parametric domains for managing area distortion. Designed for compatibility, our framework integrates seamlessly with existing neural rendering pipelines, taking multi-view images as input to reconstruct 3D geometry and compute the corresponding texture map. We also introduce a simple yet effective technique for intrinsic radiance decomposition, facilitating both view-independent material editing and view-dependent shading editing. Our method allows for the immediate rendering of edited textures through volume rendering, without the need for network re-training. Moreover, our approach supports the co-parameterization of multiple objects and enables texture transfer between them. We demonstrate the effectiveness of our method on images of human heads and man-made objects. We will make the source code publicly available.
</details>
<details>
<summary>摘要</summary>
“对于内部运算的需求增加，特别是对于内部运算表示为对� Neurolayers 的需求。在这篇文章中，我们提出一个新的对� Neurolayers 的对� Parametric 表示方法，使得可以将 3D 颜色场表示为 2D 场，并且方便了视觉化和各种修改任务。技术上，我们的方法通过两个方向的扭曲来将 3D 物体与选择的对� Parametric 类型（例如：球体、立方体或多边形）进行对�映射，从而实现了对� 3D 物体的可视化和修改。我们运用了零层Set 的点映射到对� Parametric 类型，然后透过反对映对映射。为确保映射是对�的，我们运用了一个周期损失函数，并且对映射进行均匀调整。此外，我们还运用了一个 Laplacian 调整器，以控制角度扭曲和面积扭曲。我们的框架可以与现有的内部运算架构集成，并且可以将多视图像作为输入，以实现 3D 几何学的重建和相应的纹理图。我们还提出了一个简单又有效的纹理分解方法，以便进行视� Independent 材质修改和视� Dependent 颜色修改。我们的方法可以立即将修改过的纹理显示出来， без需要网络重训。此外，我们的方法支持多个物体的共同对�映射，并且允许纹理转移 между它们。我们在人类头部和人工物体的图像上进行了试验，并且将源代码公开。”
</details></li>
</ul>
<hr>
<h2 id="Proposal-based-Temporal-Action-Localization-with-Point-level-Supervision"><a href="#Proposal-based-Temporal-Action-Localization-with-Point-level-Supervision" class="headerlink" title="Proposal-based Temporal Action Localization with Point-level Supervision"></a>Proposal-based Temporal Action Localization with Point-level Supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05511">http://arxiv.org/abs/2310.05511</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuan Yin, Yifei Huang, Ryosuke Furuta, Yoichi Sato</li>
<li>for: 本研究旨在提高点水平批处时间动作地标记（PTAL）的性能，PTAL 是在无法提供全程动作标注的情况下，通过单个帧的标注来识别和地标记动作的技术。</li>
<li>methods: 我们提出了一种新的方法，即生成和评估动作提案，以便更好地利用动作的时间信息。此外，我们还引入了一种高效的分 clustering 算法，以生成密集的 Pseudo 标签，并使用细化的对比损失来进一步改进 Pseudo 标签的质量。</li>
<li>results: 我们的方法在四个 benchmark 上实现了比或superior的性能，比如 ActivityNet 1.3、THUMOS 14、GTEA 和 BEOID 数据集。<details>
<summary>Abstract</summary>
Point-level supervised temporal action localization (PTAL) aims at recognizing and localizing actions in untrimmed videos where only a single point (frame) within every action instance is annotated in training data. Without temporal annotations, most previous works adopt the multiple instance learning (MIL) framework, where the input video is segmented into non-overlapped short snippets, and action classification is performed independently on every short snippet. We argue that the MIL framework is suboptimal for PTAL because it operates on separated short snippets that contain limited temporal information. Therefore, the classifier only focuses on several easy-to-distinguish snippets instead of discovering the whole action instance without missing any relevant snippets. To alleviate this problem, we propose a novel method that localizes actions by generating and evaluating action proposals of flexible duration that involve more comprehensive temporal information. Moreover, we introduce an efficient clustering algorithm to efficiently generate dense pseudo labels that provide stronger supervision, and a fine-grained contrastive loss to further refine the quality of pseudo labels. Experiments show that our proposed method achieves competitive or superior performance to the state-of-the-art methods and some fully-supervised methods on four benchmarks: ActivityNet 1.3, THUMOS 14, GTEA, and BEOID datasets.
</details>
<details>
<summary>摘要</summary>
点水平监督时间动作Localization（PTAL）目标是在没有时间标注的视频中识别和地图动作实例。在训练数据中只有每个动作实例中的一帧图像被标注。大多数先前的工作采用多个实例学习（MIL）框架，将输入视频切分成不重叠的短片段，然后独立地对每个短片段进行动作分类。我们认为MIL框架不适合PTAL，因为它只处理每个短片段中含有有限的时间信息，导致分类器只关注一些易于分辨的短片段，而不是找到整个动作实例没有遗弃任何相关的短片段。为解决这个问题，我们提议一种新的方法，通过生成和评估动作提案的灵活时间长度来地图动作。此外，我们引入高效的归一化算法，以生成高密度的假标签，并引入细化的对比损失来进一步改进假标签的质量。实验结果表明，我们的提议方法可以与状态艺术方法和一些完全监督方法在四个标准测试集（ActivityNet 1.3、THUMOS 14、GTEA 和 BEOID 数据集）上达到竞争性或超过性能。
</details></li>
</ul>
<hr>
<h2 id="Colmap-PCD-An-Open-source-Tool-for-Fine-Image-to-point-cloud-Registration"><a href="#Colmap-PCD-An-Open-source-Tool-for-Fine-Image-to-point-cloud-Registration" class="headerlink" title="Colmap-PCD: An Open-source Tool for Fine Image-to-point cloud Registration"></a>Colmap-PCD: An Open-source Tool for Fine Image-to-point cloud Registration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05504">http://arxiv.org/abs/2310.05504</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xiaobaiiiiii/colmap-pcd">https://github.com/xiaobaiiiiii/colmap-pcd</a></li>
<li>paper_authors: Chunge Bai, Ruijie Fu, Xiang Gao<br>for: 本研究旨在解决单目camera重建中存在的缺乏级别信息问题，通过利用预设置的LiDAR地图作为固定约束，实现高精度的重建。methods: 本研究提出了一种新的成本效果重建管道，通过不需要同步拍摄相机和LiDAR数据的注册图像到点云地图，实现了不同区域的重建细节水平的管理。基于Colmap算法，我们开发了一个开源工具Colmap-PCD${^{3}$，以便进一步研究在这个领域中。results: 本研究实现了在不同区域中管理重建细节水平的可能性，并且不需要同步拍摄相机和LiDAR数据。通过与现有的重建方法进行比较，我们发现我们的方法可以提供更高的精度和更多的细节。<details>
<summary>Abstract</summary>
State-of-the-art techniques for monocular camera reconstruction predominantly rely on the Structure from Motion (SfM) pipeline. However, such methods often yield reconstruction outcomes that lack crucial scale information, and over time, accumulation of images leads to inevitable drift issues. In contrast, mapping methods based on LiDAR scans are popular in large-scale urban scene reconstruction due to their precise distance measurements, a capability fundamentally absent in visual-based approaches. Researchers have made attempts to utilize concurrent LiDAR and camera measurements in pursuit of precise scaling and color details within mapping outcomes. However, the outcomes are subject to extrinsic calibration and time synchronization precision. In this paper, we propose a novel cost-effective reconstruction pipeline that utilizes a pre-established LiDAR map as a fixed constraint to effectively address the inherent scale challenges present in monocular camera reconstruction. To our knowledge, our method is the first to register images onto the point cloud map without requiring synchronous capture of camera and LiDAR data, granting us the flexibility to manage reconstruction detail levels across various areas of interest. To facilitate further research in this domain, we have released Colmap-PCD${^{3}$, an open-source tool leveraging the Colmap algorithm, that enables precise fine-scale registration of images to the point cloud map.
</details>
<details>
<summary>摘要</summary>
现代单目镜头重建技术主要基于结构 FROM 运动（SfM）管道。然而，这些方法经常导致重建结果缺乏重要的尺度信息，而且随着图像的增加，时间滤波问题会变得不可避免。相比之下，基于 LiDAR 扫描的映射方法在大规模城市场景重建中广泛应用，因为它们可以提供精确的距离测量，这是视觉基本缺乏的。研究人员已经尝试将同时 captured LiDAR 和镜头数据用于精确的涂抹和颜色细节，但结果受到外部尺度和时间同步精度的限制。在本文中，我们提出了一种新的成本效果重建管道，利用预先建立的 LiDAR 地图作为固定的约束，有效地解决单目镜头重建中的自然尺度挑战。与传统方法不同的是，我们的方法不需要同步捕捉镜头和 LiDAR 数据，这 grant us 更多的灵活性来管理重建细节水平。为了促进这个领域的进一步研究，我们已经发布了 Colmap-PCD${^{3}$，一款开源工具，基于 Colmap 算法，可以精确地将图像注射到点云地图上。
</details></li>
</ul>
<hr>
<h2 id="Semi-Supervised-Object-Detection-with-Uncurated-Unlabeled-Data-for-Remote-Sensing-Images"><a href="#Semi-Supervised-Object-Detection-with-Uncurated-Unlabeled-Data-for-Remote-Sensing-Images" class="headerlink" title="Semi-Supervised Object Detection with Uncurated Unlabeled Data for Remote Sensing Images"></a>Semi-Supervised Object Detection with Uncurated Unlabeled Data for Remote Sensing Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05498">http://arxiv.org/abs/2310.05498</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nanqing Liu, Xun Xu, Yingjie Gao, Heng-Chao Li</li>
<li>for: 这篇 paper 的目的是探讨如何在无监督的资料上进行离散感知图像的标注，以提高标注效率和准确性。</li>
<li>methods: 这篇 paper 使用了 semi-supervised object detection (SSOD) 方法，通过将无监督资料中的所有类别转换为 pseudo-labels，以实现在无监督资料上的标注。 然而，实际情况下可能会出现无法分类的数据（out-of-distribution，OOD）和可分类的数据（in-distribution，ID）混合在一起的情况。这篇 paper 提出了一种直接使用无监督资料进行 OSSOD 的方法，包括使用可靠的对应映射来验证 OOD 数据，以及透过 AdaBoost 等方法来提高 OSSOD 的准确性。</li>
<li>results: 实验结果显示，这篇 paper 的方法在两个 widely 使用的离散感知物体标注 dataset（DIOR 和 DOTA）上具有较高的效率和准确性，并且在面对不同的变化和噪音下也能够保持良好的表现。<details>
<summary>Abstract</summary>
Annotating remote sensing images (RSIs) presents a notable challenge due to its labor-intensive nature. Semi-supervised object detection (SSOD) methods tackle this issue by generating pseudo-labels for the unlabeled data, assuming that all classes found in the unlabeled dataset are also represented in the labeled data. However, real-world situations introduce the possibility of out-of-distribution (OOD) samples being mixed with in-distribution (ID) samples within the unlabeled dataset. In this paper, we delve into techniques for conducting SSOD directly on uncurated unlabeled data, which is termed Open-Set Semi-Supervised Object Detection (OSSOD). Our approach commences by employing labeled in-distribution data to dynamically construct a class-wise feature bank (CFB) that captures features specific to each class. Subsequently, we compare the features of predicted object bounding boxes with the corresponding entries in the CFB to calculate OOD scores. We design an adaptive threshold based on the statistical properties of the CFB, allowing us to filter out OOD samples effectively. The effectiveness of our proposed method is substantiated through extensive experiments on two widely used remote sensing object detection datasets: DIOR and DOTA. These experiments showcase the superior performance and efficacy of our approach for OSSOD on RSIs.
</details>
<details>
<summary>摘要</summary>
annotating remote sensing images (RSIs) 是一项具有很大挑战性的任务，尤其是由于它的劳动 INTENSIVE 性。半supervised object detection (SSOD) 方法可以解决这个问题，通过生成 pseudo-labels  для没有标签数据，假设所有在没有标签数据中出现的类也出现在标签数据中。然而，现实中的情况可能会出现在不符合预期的样本（OOD）被混合到了没有标签数据中。在这篇论文中，我们深入探讨了如何直接在没有检查的数据上进行 SSOD，我们称之为 Open-Set Semi-Supervised Object Detection (OSSOD)。我们的方法开始是使用标注的 Distribution 数据来动态构建一个类别特定的特征银行 (CFB)，以捕捉每个类的特定特征。然后，我们将预测的对象 bounding box 的特征与 CFB 中相应的项目进行比较，以计算 OOD 分数。我们设计了基于 CFB 的适应阈值，以有效地过滤 OOD 样本。我们的提议的方法在 DIOR 和 DOTA 两个广泛使用的 remote sensing 对象检测数据集上进行了广泛的实验，这些实验证明了我们的方法在 OSSOD 中的超越性和效果。
</details></li>
</ul>
<hr>
<h2 id="Geometry-Guided-Ray-Augmentation-for-Neural-Surface-Reconstruction-with-Sparse-Views"><a href="#Geometry-Guided-Ray-Augmentation-for-Neural-Surface-Reconstruction-with-Sparse-Views" class="headerlink" title="Geometry-Guided Ray Augmentation for Neural Surface Reconstruction with Sparse Views"></a>Geometry-Guided Ray Augmentation for Neural Surface Reconstruction with Sparse Views</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05483">http://arxiv.org/abs/2310.05483</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiawei Yao, Chen Wang, Tong Wu, Chuming Li</li>
<li>for: 本 paper 提出了一种基于多视图图像的 3D 场景和物体重建方法。与之前的方法不同的是，这种方法不需要额外信息，如深度或者场景特征，而是利用多视图输入中嵌入的场景特征来创建精确的 pseudo-标签，以便通过优化来获得更高的重建精度。</li>
<li>methods: 我们引入了一种几何学指导的方法，通过利用球面函数预测新的反射环境，以便从缺乏视图中提高表面重建精度。此外，我们的管道还利用代理几何和正确处理干扰，以便生成 pseudo-标签的反射值。</li>
<li>results: 我们的方法，名为 Ray Augmentation (RayAug)，在 DTU 和 Blender 数据集上实现了无需前期训练的超过优result，证明了其在缺乏视图重建问题中的效iveness。我们的管道可以与其他隐式神经重建方法结合使用，以满足不同的应用需求。<details>
<summary>Abstract</summary>
In this paper, we propose a novel method for 3D scene and object reconstruction from sparse multi-view images. Different from previous methods that leverage extra information such as depth or generalizable features across scenes, our approach leverages the scene properties embedded in the multi-view inputs to create precise pseudo-labels for optimization without any prior training. Specifically, we introduce a geometry-guided approach that improves surface reconstruction accuracy from sparse views by leveraging spherical harmonics to predict the novel radiance while holistically considering all color observations for a point in the scene. Also, our pipeline exploits proxy geometry and correctly handles the occlusion in generating the pseudo-labels of radiance, which previous image-warping methods fail to avoid. Our method, dubbed Ray Augmentation (RayAug), achieves superior results on DTU and Blender datasets without requiring prior training, demonstrating its effectiveness in addressing the problem of sparse view reconstruction. Our pipeline is flexible and can be integrated into other implicit neural reconstruction methods for sparse views.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的方法用于从笔粗多视图图像中重建3D场景和物体。与前一些方法不同，我们的方法不使用Extra信息如深度或跨场景通用特征，而是利用多视图输入中嵌入的场景特性来创建精确的pseudo标签，而无需任何前期训练。我们引入了一种几何学导向的方法，通过利用球面幂函数预测新的反射灯度，同时兼容所有场景颜色观察，以提高从笔粗视图中的表面重建精度。此外，我们的管道利用代理几何和正确处理 occlusion，以生成 pseudo标签的灯度。我们的方法，命名为Ray Augmentation（RayAug），在DTU和Blender数据集上实现了无需前期训练的superior结果，证明了其在笔粗视图重建问题中的有效性。我们的管道可以与其他隐式神经重建方法结合使用。
</details></li>
</ul>
<hr>
<h2 id="AdaFuse-Adaptive-Medical-Image-Fusion-Based-on-Spatial-Frequential-Cross-Attention"><a href="#AdaFuse-Adaptive-Medical-Image-Fusion-Based-on-Spatial-Frequential-Cross-Attention" class="headerlink" title="AdaFuse: Adaptive Medical Image Fusion Based on Spatial-Frequential Cross Attention"></a>AdaFuse: Adaptive Medical Image Fusion Based on Spatial-Frequential Cross Attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05462">http://arxiv.org/abs/2310.05462</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xianming-gu/adafuse">https://github.com/xianming-gu/adafuse</a></li>
<li>paper_authors: Xianming Gu, Lihui Wang, Zeyu Deng, Ying Cao, Xingyu Huang, Yue-min Zhu</li>
<li>for: 这篇论文旨在提出一种基于深度学习的多模态医疗图像融合方法，以提高诊断和手术导航的精度。</li>
<li>methods: 该方法基于频谱导向注意力机制，使用 fourier transform 来EXTRACT单模态图像信息，并通过cross-attention块来适应性地融合多模态图像信息。</li>
<li>results: 对多个数据集进行了比较性试验，结果表明，提出的方法可以在视觉质量和量化指标两个方面超过现有的状态 искусственный智能方法。此外，对折扣函数和融合策略进行了可视化检验，证明了提出的方法的有效性。<details>
<summary>Abstract</summary>
Multi-modal medical image fusion is essential for the precise clinical diagnosis and surgical navigation since it can merge the complementary information in multi-modalities into a single image. The quality of the fused image depends on the extracted single modality features as well as the fusion rules for multi-modal information. Existing deep learning-based fusion methods can fully exploit the semantic features of each modality, they cannot distinguish the effective low and high frequency information of each modality and fuse them adaptively. To address this issue, we propose AdaFuse, in which multimodal image information is fused adaptively through frequency-guided attention mechanism based on Fourier transform. Specifically, we propose the cross-attention fusion (CAF) block, which adaptively fuses features of two modalities in the spatial and frequency domains by exchanging key and query values, and then calculates the cross-attention scores between the spatial and frequency features to further guide the spatial-frequential information fusion. The CAF block enhances the high-frequency features of the different modalities so that the details in the fused images can be retained. Moreover, we design a novel loss function composed of structure loss and content loss to preserve both low and high frequency information. Extensive comparison experiments on several datasets demonstrate that the proposed method outperforms state-of-the-art methods in terms of both visual quality and quantitative metrics. The ablation experiments also validate the effectiveness of the proposed loss and fusion strategy.
</details>
<details>
<summary>摘要</summary>
多模式医疗图像融合是致 preciseness 临床诊断和手术导航中不可或缺的，因为它可以将多种多样的信息融合到单一的图像中。图像融合的质量取决于提取出的单一模式特征以及多模式信息融合规则。现有的深度学习基于的融合方法可以充分利用每种模式的语义特征，但是它们无法区分每种模式的有效低频和高频信息，并将其 adaptively 融合。为解决这个问题，我们提出了 AdaFuse，它通过基于傅ри散变换的频谱导向注意力机制来逐渐融合多模式图像信息。具体来说，我们提出了交叉注意力融合（CAF）块，它可以在空间和频谱Domain中逐渐融合两种模式的特征，然后计算交叉注意力分数以更好地导航图像的空间频谱信息融合。CAF块可以增强不同模式的高频特征，以保留融合图像中的细节。此外，我们设计了一种新的损失函数，它包括结构损失和内容损失，以保持两种模式的低频和高频信息。广泛比较实验证明，我们提出的方法在多个数据集上表现出色，超过了当前状态的方法。我们还进行了精细的拆分实验，以证明提出的损失函数和融合策略的效果。
</details></li>
</ul>
<hr>
<h2 id="Memory-Assisted-Sub-Prototype-Mining-for-Universal-Domain-Adaptation"><a href="#Memory-Assisted-Sub-Prototype-Mining-for-Universal-Domain-Adaptation" class="headerlink" title="Memory-Assisted Sub-Prototype Mining for Universal Domain Adaptation"></a>Memory-Assisted Sub-Prototype Mining for Universal Domain Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05453">http://arxiv.org/abs/2310.05453</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxiang Lai, Xinghong Liu, Tao Zhou, Yi Zhou<br>for: 这个论文的目的是提出一种新的记忆驱动的子类别探索方法，以解决当存在类别内的概念变化时，普通的预测模型对类别的适应性不足的问题。methods: 这个方法使用了记忆驱动的子类别探索技术，可以从类别内的内部结构中学习到更加细致的特征空间，从而提高预测模型的适应性和精度。results: 这个方法在多个enario中实现了州际预测模型的提升，包括UniDA、OSDA和PDA等四个benchmark，在大多数情况下都可以 дости得顶尖的性能。<details>
<summary>Abstract</summary>
Universal domain adaptation aims to align the classes and reduce the feature gap between the same category of the source and target domains. The target private category is set as the unknown class during the adaptation process, as it is not included in the source domain. However, most existing methods overlook the intra-class structure within a category, especially in cases where there exists significant concept shift between the samples belonging to the same category. When samples with large concept shift are forced to be pushed together, it may negatively affect the adaptation performance. Moreover, from the interpretability aspect, it is unreasonable to align visual features with significant differences, such as fighter jets and civil aircraft, into the same category. Unfortunately, due to such semantic ambiguity and annotation cost, categories are not always classified in detail, making it difficult for the model to perform precise adaptation. To address these issues, we propose a novel Memory-Assisted Sub-Prototype Mining (MemSPM) method that can learn the differences between samples belonging to the same category and mine sub-classes when there exists significant concept shift between them. By doing so, our model learns a more reasonable feature space that enhances the transferability and reflects the inherent differences among samples annotated as the same category. We evaluate the effectiveness of our MemSPM method over multiple scenarios, including UniDA, OSDA, and PDA. Our method achieves state-of-the-art performance on four benchmarks in most cases.
</details>
<details>
<summary>摘要</summary>
通用领域适应目标是调整源领域和目标领域之间的分类和特征差异。目标私有类别设置为未知类别，因为它不包括源领域中。然而，大多数现有方法忽略了同一类别内的结构，特别是当存在严重概念变化的情况下。当这些样本被迫推 together 时，可能会对适应性产生负面影响。此外，从解释方面来说，将不同的特征，如战斗机和民用飞机，迫使其推入同一类别中，是不合理的。实际上，这种概念混乱和标签成本问题，使得类别不一定严格分类，对模型进行精确适应很困难。为解决这些问题，我们提出了一种新的记忆帮助子型别挖掘（MemSPM）方法。这个方法可以学习同一类别内的差异，并在存在严重概念变化的情况下挖掘子类别。这样，我们的模型可以学习一个更合理的特征空间，增强转移性和反映内部类别之间的差异。我们在多个情况下评估了我们的MemSPM方法，包括UniDA、OSDA和PDA四个标准库。我们的方法在大多数情况下实现了顶尖性能。
</details></li>
</ul>
<hr>
<h2 id="Towards-Fair-and-Comprehensive-Comparisons-for-Image-Based-3D-Object-Detection"><a href="#Towards-Fair-and-Comprehensive-Comparisons-for-Image-Based-3D-Object-Detection" class="headerlink" title="Towards Fair and Comprehensive Comparisons for Image-Based 3D Object Detection"></a>Towards Fair and Comprehensive Comparisons for Image-Based 3D Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05447">http://arxiv.org/abs/2310.05447</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinzhu Ma, Yongtao Wang, Yinmin Zhang, Zhiyi Xia, Yuan Meng, Zhihui Wang, Haojie Li, Wanli Ouyang</li>
<li>for: 提高图像基于3D物体检测领域的研究稳定性和可比性，并提供一套可重复性的训练标准和错误诊断工具箱。</li>
<li>methods: 使用模块化设计的代码库，定制强大的训练秘诀，并提供一套错误诊断工具箱来测量检测模型的细致特征。</li>
<li>results: 通过对当前方法进行深入分析和评估，提供一系列可重复性的结果，并解决一些开放问题，如 KITTI-3D 和 nuScenes 数据集上的差异，以便促进未来的图像基于3D物体检测研究。<details>
<summary>Abstract</summary>
In this work, we build a modular-designed codebase, formulate strong training recipes, design an error diagnosis toolbox, and discuss current methods for image-based 3D object detection. In particular, different from other highly mature tasks, e.g., 2D object detection, the community of image-based 3D object detection is still evolving, where methods often adopt different training recipes and tricks resulting in unfair evaluations and comparisons. What is worse, these tricks may overwhelm their proposed designs in performance, even leading to wrong conclusions. To address this issue, we build a module-designed codebase and formulate unified training standards for the community. Furthermore, we also design an error diagnosis toolbox to measure the detailed characterization of detection models. Using these tools, we analyze current methods in-depth under varying settings and provide discussions for some open questions, e.g., discrepancies in conclusions on KITTI-3D and nuScenes datasets, which have led to different dominant methods for these datasets. We hope that this work will facilitate future research in image-based 3D object detection. Our codes will be released at \url{https://github.com/OpenGVLab/3dodi}
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们建立了模块化设计的代码基础，制定了强大的训练荷重，设计了错误诊断工具箱，并讨论了现有的图像基于3D物体检测方法。特别是，与其他已经成熟的任务，如2D物体检测，社区的图像基于3D物体检测仍在发展中，其方法经常采用不同的训练荷重和技巧，导致不公正的评估和比较。甚至，这些技巧可能会超越其提出的设计，导致错误的结论。为解决这个问题，我们建立了模块化设计的代码基础，并制定了统一的训练标准 для社区。另外，我们还设计了错误诊断工具箱，以测量检测模型的细化特征。使用这些工具，我们在不同的设置下进行了深入的分析，并提供了一些开放的问题的讨论，如基于KITTI-3D和nuScenes dataset的结论不一致问题。我们希望这项工作能够促进图像基于3D物体检测的未来研究。我们的代码将在 GitHub 上发布，请参考 \url{https://github.com/OpenGVLab/3dodi}。
</details></li>
</ul>
<hr>
<h2 id="RetSeg-Retention-based-Colorectal-Polyps-Segmentation-Network"><a href="#RetSeg-Retention-based-Colorectal-Polyps-Segmentation-Network" class="headerlink" title="RetSeg: Retention-based Colorectal Polyps Segmentation Network"></a>RetSeg: Retention-based Colorectal Polyps Segmentation Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05446">http://arxiv.org/abs/2310.05446</a></li>
<li>repo_url: None</li>
<li>paper_authors: Khaled ELKarazle, Valliappan Raman, Caslon Chua, Patrick Then</li>
<li>for: 这个研究的目的是提高医疗影像分析中的肿瘤分类、检测和分 segmentation 的精度和速度，并且应用 Transformer 架构以减少资源消耗和提高training parallelism。</li>
<li>methods: 本研究使用了 Retention Mechanism 来解决 Transformer 在医疗影像分析中的问题，包括资源消耗和training parallelism。RetSeg 是一个以 RetNet 为基础的 encoder-decoder 网络，旨在提高肿瘤分类和资源利用。</li>
<li>results: 本研究在两个公开可用的数据集上训练和验证 RetSeg，并且在多个公开数据集上显示了 RetSeg 的出色表现。尤其是在colonoscopy 影像上，RetSeg 能够提高肿瘤分类的精度和速度。<details>
<summary>Abstract</summary>
Vision Transformers (ViTs) have revolutionized medical imaging analysis, showcasing superior efficacy compared to conventional Convolutional Neural Networks (CNNs) in vital tasks such as polyp classification, detection, and segmentation. Leveraging attention mechanisms to focus on specific image regions, ViTs exhibit contextual awareness in processing visual data, culminating in robust and precise predictions, even for intricate medical images. Moreover, the inherent self-attention mechanism in Transformers accommodates varying input sizes and resolutions, granting an unprecedented flexibility absent in traditional CNNs. However, Transformers grapple with challenges like excessive memory usage and limited training parallelism due to self-attention, rendering them impractical for real-time disease detection on resource-constrained devices. In this study, we address these hurdles by investigating the integration of the recently introduced retention mechanism into polyp segmentation, introducing RetSeg, an encoder-decoder network featuring multi-head retention blocks. Drawing inspiration from Retentive Networks (RetNet), RetSeg is designed to bridge the gap between precise polyp segmentation and resource utilization, particularly tailored for colonoscopy images. We train and validate RetSeg for polyp segmentation employing two publicly available datasets: Kvasir-SEG and CVC-ClinicDB. Additionally, we showcase RetSeg's promising performance across diverse public datasets, including CVC-ColonDB, ETIS-LaribPolypDB, CVC-300, and BKAI-IGH NeoPolyp. While our work represents an early-stage exploration, further in-depth studies are imperative to advance these promising findings.
</details>
<details>
<summary>摘要</summary>
医学影像分析领域中，视野变换器（ViT）已经引领了革命性的改变，在重要的任务中，如肿瘤分类、检测和 segmentation 方面，表现出了Superior efficacy  compared to traditional Convolutional Neural Networks（CNNs）。通过关注特定的图像区域，ViT 展示了Contextual awareness 在处理视觉数据，从而导致了Robust and precise predictions，甚至 для复杂的医学影像。此外，Transformers 内置的自注意机制可以满足不同的输入大小和分辨率，从而提供了前所未有的灵活性，与传统的 CNNs 不同。然而，Transformers 面临着大量内存使用和自注意机制限制了实时疾病检测的应用，特别是在有限的设备资源下。在本研究中，我们解决这些障碍物，通过调查Retention mechanism 的整合进行肿瘤分 segmentation，提出了RetSeg，一种基于 encoder-decoder 网络的多头保留块。 drawing inspiration from Retentive Networks（RetNet），RetSeg 旨在bridging the gap between precise polyp segmentation and resource utilization，特别适用于 colonoscopy 图像。我们使用两个公共可用的数据集进行训练和验证RetSeg：Kvasir-SEG 和 CVC-ClinicDB。此外，我们还展示了RetSeg 在多个公共数据集上的出色表现，包括 CVC-ColonDB、ETIS-LaribPolypDB、CVC-300 和 BKAI-IGH NeoPolyp。虽然我们的工作只是早期的探索，但更深入的研究是必要的，以进一步发展这些有前途的发现。
</details></li>
</ul>
<hr>
<h2 id="AngioMoCo-Learning-based-Motion-Correction-in-Cerebral-Digital-Subtraction-Angiography"><a href="#AngioMoCo-Learning-based-Motion-Correction-in-Cerebral-Digital-Subtraction-Angiography" class="headerlink" title="AngioMoCo: Learning-based Motion Correction in Cerebral Digital Subtraction Angiography"></a>AngioMoCo: Learning-based Motion Correction in Cerebral Digital Subtraction Angiography</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05445">http://arxiv.org/abs/2310.05445</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruisheng Su, Matthijs van der Sluijs, Sandra Cornelissen, Wim van Zwam, Aad van der Lugt, Wiro Niessen, Danny Ruijters, Theo van Walsum, Adrian Dalca</li>
<li>For: The paper aims to address the limitations of cerebral X-ray digital subtraction angiography (DSA) by developing a learning-based framework called AngioMoCo, which generates motion-compensated DSA sequences from X-ray angiography.* Methods: AngioMoCo integrates contrast extraction and motion correction, enabling differentiation between patient motion and intensity changes caused by contrast flow. The framework uses a learning-based approach that is substantially faster than iterative elastix-based methods.* Results: The paper demonstrates the effectiveness of AngioMoCo on a large national multi-center dataset (MR CLEAN Registry) of clinically acquired angiographic images through comprehensive qualitative and quantitative analyses. AngioMoCo produces high-quality motion-compensated DSA, removing motion artifacts while preserving contrast flow.<details>
<summary>Abstract</summary>
Cerebral X-ray digital subtraction angiography (DSA) is the standard imaging technique for visualizing blood flow and guiding endovascular treatments. The quality of DSA is often negatively impacted by body motion during acquisition, leading to decreased diagnostic value. Time-consuming iterative methods address motion correction based on non-rigid registration, and employ sparse key points and non-rigidity penalties to limit vessel distortion. Recent methods alleviate subtraction artifacts by predicting the subtracted frame from the corresponding unsubtracted frame, but do not explicitly compensate for motion-induced misalignment between frames. This hinders the serial evaluation of blood flow, and often causes undesired vasculature and contrast flow alterations, leading to impeded usability in clinical practice. To address these limitations, we present AngioMoCo, a learning-based framework that generates motion-compensated DSA sequences from X-ray angiography. AngioMoCo integrates contrast extraction and motion correction, enabling differentiation between patient motion and intensity changes caused by contrast flow. This strategy improves registration quality while being substantially faster than iterative elastix-based methods. We demonstrate AngioMoCo on a large national multi-center dataset (MR CLEAN Registry) of clinically acquired angiographic images through comprehensive qualitative and quantitative analyses. AngioMoCo produces high-quality motion-compensated DSA, removing motion artifacts while preserving contrast flow. Code is publicly available at https://github.com/RuishengSu/AngioMoCo.
</details>
<details>
<summary>摘要</summary>
脑血管X射线数字抑减成像（DSA）是现代成像技术的标准，用于评估血液流动和导引内镜治疗。然而，DSA的质量经常受到身体运动的影响，导致诊断价值下降。时间consuming的迭代方法 Addresses motion correction based on non-rigid registration, using sparse key points and non-rigidity penalties to limit vessel distortion。Recent methods predict the subtracted frame from the corresponding unsubtracted frame, but do not explicitly compensate for motion-induced misalignment between frames, leading to serial evaluation of blood flow and undesired vasculature and contrast flow alterations, which hinders clinical practice. To overcome these limitations, we present AngioMoCo, a learning-based framework that generates motion-compensated DSA sequences from X-ray angiography. AngioMoCo integrates contrast extraction and motion correction, allowing for the differentiation between patient motion and intensity changes caused by contrast flow. This strategy improves registration quality while being substantially faster than iterative elastix-based methods. We demonstrate AngioMoCo on a large national multi-center dataset (MR CLEAN Registry) of clinically acquired angiographic images through comprehensive qualitative and quantitative analyses. AngioMoCo produces high-quality motion-compensated DSA, removing motion artifacts while preserving contrast flow. Code is publicly available at https://github.com/RuishengSu/AngioMoCo.
</details></li>
</ul>
<hr>
<h2 id="Semantic-aware-Temporal-Channel-wise-Attention-for-Cardiac-Function-Assessment"><a href="#Semantic-aware-Temporal-Channel-wise-Attention-for-Cardiac-Function-Assessment" class="headerlink" title="Semantic-aware Temporal Channel-wise Attention for Cardiac Function Assessment"></a>Semantic-aware Temporal Channel-wise Attention for Cardiac Function Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05428">http://arxiv.org/abs/2310.05428</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guanqi Chen, Guanbin Li</li>
<li>for: 预测左心脏肌功能评估 Left Ventricular Ejection Fraction (LVEF) 基于echocardiogram视频，以提高智能医疗辅助技术的准确性和自动性。</li>
<li>methods: 提议使用 semi-supervised auxilary learning  парадигма，具有左心脏区域分割任务，以帮助左心脏区域的表征学学习。通过引入时间通道wise抽象（TCA）模块，更好地模型左心脏动态信息的重要性。并且通过对 segmentation 图像进行semantic perception，更好地关注左心脏动态模式。</li>
<li>results: 在Standford数据集上达到了状态机器人的性能，与原始模型的改进为0.22 MAE，0.26 RMSE和1.9% $R^2$。<details>
<summary>Abstract</summary>
Cardiac function assessment aims at predicting left ventricular ejection fraction (LVEF) given an echocardiogram video, which requests models to focus on the changes in the left ventricle during the cardiac cycle. How to assess cardiac function accurately and automatically from an echocardiogram video is a valuable topic in intelligent assisted healthcare. Existing video-based methods do not pay much attention to the left ventricular region, nor the left ventricular changes caused by motion. In this work, we propose a semi-supervised auxiliary learning paradigm with a left ventricular segmentation task, which contributes to the representation learning for the left ventricular region. To better model the importance of motion information, we introduce a temporal channel-wise attention (TCA) module to excite those channels used to describe motion. Furthermore, we reform the TCA module with semantic perception by taking the segmentation map of the left ventricle as input to focus on the motion patterns of the left ventricle. Finally, to reduce the difficulty of direct LVEF regression, we utilize an anchor-based classification and regression method to predict LVEF. Our approach achieves state-of-the-art performance on the Stanford dataset with an improvement of 0.22 MAE, 0.26 RMSE, and 1.9% $R^2$.
</details>
<details>
<summary>摘要</summary>
Cardiac function assessment aims to predict left ventricular ejection fraction (LVEF) based on an echocardiogram video, which requires models to focus on changes in the left ventricle during the cardiac cycle. Accurately assessing cardiac function from an echocardiogram video is a valuable topic in intelligent assisted healthcare. Existing video-based methods do not pay much attention to the left ventricular region or the left ventricular changes caused by motion.In this work, we propose a semi-supervised auxiliary learning paradigm with a left ventricular segmentation task, which contributes to the representation learning for the left ventricular region. To better model the importance of motion information, we introduce a temporal channel-wise attention (TCA) module to excite those channels used to describe motion. Furthermore, we reform the TCA module with semantic perception by taking the segmentation map of the left ventricle as input to focus on the motion patterns of the left ventricle. Finally, to reduce the difficulty of direct LVEF regression, we utilize an anchor-based classification and regression method to predict LVEF. Our approach achieves state-of-the-art performance on the Stanford dataset with an improvement of 0.22 MAE, 0.26 RMSE, and 1.9% $R^2$.
</details></li>
</ul>
<hr>
<h2 id="GradientSurf-Gradient-Domain-Neural-Surface-Reconstruction-from-RGB-Video"><a href="#GradientSurf-Gradient-Domain-Neural-Surface-Reconstruction-from-RGB-Video" class="headerlink" title="GradientSurf: Gradient-Domain Neural Surface Reconstruction from RGB Video"></a>GradientSurf: Gradient-Domain Neural Surface Reconstruction from RGB Video</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05406">http://arxiv.org/abs/2310.05406</a></li>
<li>repo_url: None</li>
<li>paper_authors: Crane He Chen, Joerg Liebelt<br>for: 这个论文提出了一种实时表面重建方法，用于从单色RGB视频中重建场景表面。methods: 该方法基于紧密地关联表面、体积和方向云点云，并在梯度域内解决重建问题。不同于传统的Poisson表面重建方法，该方法在线解决问题，使用神经网络在部分扫描过程中逐步更新。results: 对于室内场景重建任务，视觉和量化实验结果显示，提出的方法可以在弯曲区域和小物体上重建表面的细节，并且比前方法有更高的准确率。<details>
<summary>Abstract</summary>
This paper proposes GradientSurf, a novel algorithm for real time surface reconstruction from monocular RGB video. Inspired by Poisson Surface Reconstruction, the proposed method builds on the tight coupling between surface, volume, and oriented point cloud and solves the reconstruction problem in gradient-domain. Unlike Poisson Surface Reconstruction which finds an offline solution to the Poisson equation by solving a linear system after the scanning process is finished, our method finds online solutions from partial scans with a neural network incrementally where the Poisson layer is designed to supervise both local and global reconstruction. The main challenge that existing methods suffer from when reconstructing from RGB signal is a lack of details in the reconstructed surface. We hypothesize this is due to the spectral bias of neural networks towards learning low frequency geometric features. To address this issue, the reconstruction problem is cast onto gradient domain, where zeroth-order and first-order energies are minimized. The zeroth-order term penalizes location of the surface. The first-order term penalizes the difference between the gradient of reconstructed implicit function and the vector field formulated from oriented point clouds sampled at adaptive local densities. For the task of indoor scene reconstruction, visual and quantitative experimental results show that the proposed method reconstructs surfaces with more details in curved regions and higher fidelity for small objects than previous methods.
</details>
<details>
<summary>摘要</summary>
Existing methods suffer from a lack of details in the reconstructed surface, which we attribute to the spectral bias of neural networks towards learning low-frequency geometric features. To address this issue, we cast the reconstruction problem onto the gradient domain, where zeroth-order and first-order energies are minimized. The zeroth-order term penalizes the location of the surface, while the first-order term penalizes the difference between the gradient of the reconstructed implicit function and the vector field formulated from oriented point clouds sampled at adaptive local densities.For the task of indoor scene reconstruction, visual and quantitative experimental results show that the proposed method reconstructs surfaces with more details in curved regions and higher fidelity for small objects than previous methods.
</details></li>
</ul>
<hr>
<h2 id="Efficient-VQGAN-Towards-High-Resolution-Image-Generation-with-Efficient-Vision-Transformers"><a href="#Efficient-VQGAN-Towards-High-Resolution-Image-Generation-with-Efficient-Vision-Transformers" class="headerlink" title="Efficient-VQGAN: Towards High-Resolution Image Generation with Efficient Vision Transformers"></a>Efficient-VQGAN: Towards High-Resolution Image Generation with Efficient Vision Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05400">http://arxiv.org/abs/2310.05400</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiyue Cao, Yueqin Yin, Lianghua Huang, Yu Liu, Xin Zhao, Deli Zhao, Kaiqi Huang</li>
<li>for: 高分辨率图像生成</li>
<li>methods: local attention-based量化模型 + 多层次特征交互 + 自适应生成管道</li>
<li>results: 提高图像生成速度、质量和分辨率<details>
<summary>Abstract</summary>
Vector-quantized image modeling has shown great potential in synthesizing high-quality images. However, generating high-resolution images remains a challenging task due to the quadratic computational overhead of the self-attention process. In this study, we seek to explore a more efficient two-stage framework for high-resolution image generation with improvements in the following three aspects. (1) Based on the observation that the first quantization stage has solid local property, we employ a local attention-based quantization model instead of the global attention mechanism used in previous methods, leading to better efficiency and reconstruction quality. (2) We emphasize the importance of multi-grained feature interaction during image generation and introduce an efficient attention mechanism that combines global attention (long-range semantic consistency within the whole image) and local attention (fined-grained details). This approach results in faster generation speed, higher generation fidelity, and improved resolution. (3) We propose a new generation pipeline incorporating autoencoding training and autoregressive generation strategy, demonstrating a better paradigm for image synthesis. Extensive experiments demonstrate the superiority of our approach in high-quality and high-resolution image reconstruction and generation.
</details>
<details>
<summary>摘要</summary>
vector-quantized 图像模型已经展示出Synthesizing 高质量图像的潜力。然而，生成高分辨率图像仍然是一项具有quadratic computational overhead的挑战，因为自我注意机制的计算复杂度 quadratic。在这项研究中，我们想要探索一种更高效的两 stage 框架，以提高以下三个方面：1. 根据我们所观察到的首次量化阶段具有坚实的地方性质，我们采用了地方注意机制来取代之前的全局注意机制，从而提高效率和重建质量。2. 我们强调图像生成中多层次特征之间的互动对于图像质量的影响，并引入了一种高效的注意机制，该机制结合了全局注意（整个图像的长距离semantic consistency）和地方注意（细腻的特征）。这种方法会提高生成速度、生成质量和分辨率。3. 我们提出了一种新的生成管道，其包括自动编码训练和泛化生成策略。我们的方法在高质量和高分辨率图像重建和生成中具有优势。extensive experiments 表明，我们的方法在高质量和高分辨率图像重建和生成中具有优势。
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-Side-Tuning-for-Vision-Transformers"><a href="#Hierarchical-Side-Tuning-for-Vision-Transformers" class="headerlink" title="Hierarchical Side-Tuning for Vision Transformers"></a>Hierarchical Side-Tuning for Vision Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05393">http://arxiv.org/abs/2310.05393</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/AFeng-x/HST">https://github.com/AFeng-x/HST</a></li>
<li>paper_authors: Weifeng Lin, Ziheng Wu, Jiayu Chen, Wentao Yang, Mingxin Huang, Jun Huang, Lianwen Jin</li>
<li>for: 本文旨在提出一种基于卷积 transformer 的 Hierarchical Side-Tuning (HST) 方法，用于有效地将预训练模型传递到多个下游任务中。</li>
<li>methods: 本文使用了一种基于卷积 transformer 的 Hierarchical Side-Tuning (HST) 方法，其中包括一个轻量级的并行网络 (HSN)，用于生成多级特征并进行预测。</li>
<li>results: 本文在多种视觉任务上进行了广泛的实验，包括分类、物体检测、实例分割和 semantic segmentation。结果显示，HST 方法可以达到最佳性能，其中在 VTAB-1k 测试集上实现了平均 Top-1 准确率为 76.0%，并且只需要 fine-tune 0.78M 参数。在 COCO 测试设置上，HST 方法还超过了全局 fine-tuning，并在 Cascade Mask R-CNN 上获得了更高的权重 AP 值（49.7 和 43.2）。<details>
<summary>Abstract</summary>
Fine-tuning pre-trained Vision Transformers (ViT) has consistently demonstrated promising performance in the realm of visual recognition. However, adapting large pre-trained models to various tasks poses a significant challenge. This challenge arises from the need for each model to undergo an independent and comprehensive fine-tuning process, leading to substantial computational and memory demands. While recent advancements in Parameter-efficient Transfer Learning (PETL) have demonstrated their ability to achieve superior performance compared to full fine-tuning with a smaller subset of parameter updates, they tend to overlook dense prediction tasks such as object detection and segmentation. In this paper, we introduce Hierarchical Side-Tuning (HST), a novel PETL approach that enables ViT transfer to various downstream tasks effectively. Diverging from existing methods that exclusively fine-tune parameters within input spaces or certain modules connected to the backbone, we tune a lightweight and hierarchical side network (HSN) that leverages intermediate activations extracted from the backbone and generates multi-scale features to make predictions. To validate HST, we conducted extensive experiments encompassing diverse visual tasks, including classification, object detection, instance segmentation, and semantic segmentation. Notably, our method achieves state-of-the-art average Top-1 accuracy of 76.0% on VTAB-1k, all while fine-tuning a mere 0.78M parameters. When applied to object detection tasks on COCO testdev benchmark, HST even surpasses full fine-tuning and obtains better performance with 49.7 box AP and 43.2 mask AP using Cascade Mask R-CNN.
</details>
<details>
<summary>摘要</summary>
可以使用已经训练过的视觉转换器（ViT）进行精细调整，以提高视觉识别的性能。然而，将大型预训练模型应用到不同任务时，会带来很大的计算和内存需求。而最近的参数有效转移学习（PETL）技术已经证明可以在小型参数更新的情况下，超越全部 fine-tuning，实现更好的性能。然而，这些方法通常忽略 dense prediction 任务，如对象检测和分割。在这篇论文中，我们介绍了一种新的 PETL 方法，即层次侧调整（HST）。与现有方法不同，我们不是直接在输入空间或特定模块与背景连接的参数进行 fine-tuning，而是在较重的和层次结构的侧网络（HSN）上进行 fine-tuning。HSN 利用了背景网络中的中间活动，生成了多级特征，以进行预测。为 validate HST，我们进行了广泛的实验，包括多种视觉任务，如分类、对象检测、实例分割和semantic segmentation。特别是，我们的方法在 VTAB-1k 上实现了平均 Top-1 准确率为 76.0%，而且只需要 fine-tune 0.78M 个参数。在对象检测任务中，HST 甚至超过了全部 fine-tuning，在 COCO 测试dev benchmark上实现了49.7个box AP和43.2个mask AP，使用 Cascade Mask R-CNN。
</details></li>
</ul>
<hr>
<h2 id="Lightweight-Full-Convolutional-Siamese-Tracker"><a href="#Lightweight-Full-Convolutional-Siamese-Tracker" class="headerlink" title="Lightweight Full-Convolutional Siamese Tracker"></a>Lightweight Full-Convolutional Siamese Tracker</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05392">http://arxiv.org/abs/2310.05392</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liyunfenglyf/lightfc">https://github.com/liyunfenglyf/lightfc</a></li>
<li>paper_authors: Yunfeng Li, Bo Wang, Xueyi Wu, Zhuoyan Liu, Ye Li</li>
<li>for: 提高大规模跟踪器的精度和效率，并在有限资源的平台上应用。</li>
<li>methods: 提出了一种轻量级全 convolutional Siamese tracker，即 LightFC，使用了一种新的有效的协调模块（ECM）和一种新的有效的重心头（ERH）来提高跟踪pipeline的非线性表达能力。</li>
<li>results: 实验表明，LightFC可以实现跟踪器的优化平衡，在性能、参数、Flops和FPS等方面均达到了优秀的水平，并且在 CPU 上运行速度比 MixFormerV2-S 快2倍。<details>
<summary>Abstract</summary>
Although single object trackers have achieved advanced performance, their large-scale models make it difficult to apply them on the platforms with limited resources. Moreover, existing lightweight trackers only achieve balance between 2-3 points in terms of parameters, performance, Flops and FPS. To achieve the optimal balance among these points, this paper propose a lightweight full-convolutional Siamese tracker called LightFC. LightFC employs a novel efficient cross-correlation module (ECM) and a novel efficient rep-center head (ERH) to enhance the nonlinear expressiveness of the convolutional tracking pipeline. The ECM employs an attention-like module design, which conducts spatial and channel linear fusion of fused features and enhances the nonlinearly of the fused features. Additionally, it references successful factors of current lightweight trackers and introduces skip-connections and reuse of search area features. The ERH reparameterizes the feature dimensional stage in the standard center head and introduces channel attention to optimize the bottleneck of key feature flows. Comprehensive experiments show that LightFC achieves the optimal balance between performance, parameters, Flops and FPS. The precision score of LightFC outperforms MixFormerV2-S by 3.7 \% and 6.5 \% on LaSOT and TNL2K, respectively, while using 5x fewer parameters and 4.6x fewer Flops. Besides, LightFC runs 2x faster than MixFormerV2-S on CPUs. Our code and raw results can be found at https://github.com/LiYunfengLYF/LightFC
</details>
<details>
<summary>摘要</summary>
尽管单个对象跟踪器已经实现了高度的表现，但它们的大型模型使得在有限资源的平台上应用困难。此外，现有的轻量级跟踪器只能达到2-3点的平衡，即参数、性能、 Flops 和 FPS 之间的平衡。为了实现这些点之间的优化平衡，本文提出了一种轻量级全 convolutional Siamese 跟踪器，即 LightFC。LightFC 使用了一种新的高效 cross-correlation module (ECM) 和一种新的高效 reuse center head (ERH)，以增强 convolutional 跟踪管道的非线性表达能力。ECM 使用了注意力模块的设计，通过空间和通道线性融合含有融合特征的特征，提高非线性特征的表达能力。此外，它参考了当前轻量级跟踪器的成功因素，并引入了跳过连接和 reuse 搜索区域特征。ERH 重新parameterizes 标准中心头中的特征维度阶段，并引入通道注意力来优化瓶颈特征流。经过全面的实验，LightFC 实现了最佳的性能、参数、 Flops 和 FPS 的平衡。LightFC 的精度分数高于 MixFormerV2-S 的 LaSOT 和 TNL2K 上的精度分数，分别提高了3.7% 和 6.5%，同时使用5x fewer parameters 和 4.6x fewer Flops。此外，LightFC 在 CPU 上运行2x faster than MixFormerV2-S。我们的代码和原始结果可以在 GitHub 上找到：https://github.com/LiYunfengLYF/LightFC
</details></li>
</ul>
<hr>
<h2 id="Neural-Impostor-Editing-Neural-Radiance-Fields-with-Explicit-Shape-Manipulation"><a href="#Neural-Impostor-Editing-Neural-Radiance-Fields-with-Explicit-Shape-Manipulation" class="headerlink" title="Neural Impostor: Editing Neural Radiance Fields with Explicit Shape Manipulation"></a>Neural Impostor: Editing Neural Radiance Fields with Explicit Shape Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05391">http://arxiv.org/abs/2310.05391</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruiyang Liu, Jinxu Xiang, Bowen Zhao, Ran Zhang, Jingyi Yu, Changxi Zheng</li>
<li>for: 提高NeRF的 Editing能力，尤其是几何修改。</li>
<li>methods: 引入explicit tetrahedral mesh和multigrid implicit field，并通过 multigrid barycentric coordinate encoding将explicit shape manipulation和implicit fields的编辑 bridged。</li>
<li>results: 提供了一种实用的方法来 modify NeRF，包括deform、composite和generate neural implicit fields，同时维护复杂的零体积表现。<details>
<summary>Abstract</summary>
Neural Radiance Fields (NeRF) have significantly advanced the generation of highly realistic and expressive 3D scenes. However, the task of editing NeRF, particularly in terms of geometry modification, poses a significant challenge. This issue has obstructed NeRF's wider adoption across various applications. To tackle the problem of efficiently editing neural implicit fields, we introduce Neural Impostor, a hybrid representation incorporating an explicit tetrahedral mesh alongside a multigrid implicit field designated for each tetrahedron within the explicit mesh. Our framework bridges the explicit shape manipulation and the geometric editing of implicit fields by utilizing multigrid barycentric coordinate encoding, thus offering a pragmatic solution to deform, composite, and generate neural implicit fields while maintaining a complex volumetric appearance. Furthermore, we propose a comprehensive pipeline for editing neural implicit fields based on a set of explicit geometric editing operations. We show the robustness and adaptability of our system through diverse examples and experiments, including the editing of both synthetic objects and real captured data. Finally, we demonstrate the authoring process of a hybrid synthetic-captured object utilizing a variety of editing operations, underlining the transformative potential of Neural Impostor in the field of 3D content creation and manipulation.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Three-Stage-Cascade-Framework-for-Blurry-Video-Frame-Interpolation"><a href="#Three-Stage-Cascade-Framework-for-Blurry-Video-Frame-Interpolation" class="headerlink" title="Three-Stage Cascade Framework for Blurry Video Frame Interpolation"></a>Three-Stage Cascade Framework for Blurry Video Frame Interpolation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05383">http://arxiv.org/abs/2310.05383</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pengcheng Lei, Zaoming Yan, Tingting Wang, Faming Fang, Guixu Zhang<br>for: 这个论文的目标是提出一种简单的终端三个阶段框架，以全面利用恶化视频中的有用信息，并提高高速清晰视频生成的性能。methods: 该模型采用了三个阶段：帧 interpolate阶段、时间特征融合阶段和锐化阶段。帧 interpolate阶段使用了时间可变网络直接从恶化输入中提取有用信息并生成目标帧的中间帧。时间特征融合阶段通过双向循环可变网络挖掘每个目标帧的长期时间信息。锐化阶段使用了基于 transformer 的 Taylor 近似网络来逐层恢复高频细节。results: 实验结果表明，我们的模型在四个标准测试集上具有较高的性能，并且在真实恶化视频上也有良好的泛化能力。<details>
<summary>Abstract</summary>
Blurry video frame interpolation (BVFI) aims to generate high-frame-rate clear videos from low-frame-rate blurry videos, is a challenging but important topic in the computer vision community. Blurry videos not only provide spatial and temporal information like clear videos, but also contain additional motion information hidden in each blurry frame. However, existing BVFI methods usually fail to fully leverage all valuable information, which ultimately hinders their performance. In this paper, we propose a simple end-to-end three-stage framework to fully explore useful information from blurry videos. The frame interpolation stage designs a temporal deformable network to directly sample useful information from blurry inputs and synthesize an intermediate frame at an arbitrary time interval. The temporal feature fusion stage explores the long-term temporal information for each target frame through a bi-directional recurrent deformable alignment network. And the deblurring stage applies a transformer-empowered Taylor approximation network to recursively recover the high-frequency details. The proposed three-stage framework has clear task assignment for each module and offers good expandability, the effectiveness of which are demonstrated by various experimental results. We evaluate our model on four benchmarks, including the Adobe240 dataset, GoPro dataset, YouTube240 dataset and Sony dataset. Quantitative and qualitative results indicate that our model outperforms existing SOTA methods. Besides, experiments on real-world blurry videos also indicate the good generalization ability of our model.
</details>
<details>
<summary>摘要</summary>
《不清晰视频帧 interpolate (BVFI)》是计算机视觉领域一项重要但具有挑战性的任务，该任务的目标是将低帧率不清晰视频转换为高帧率清晰视频。不清晰视频除了提供空间信息外，还包含隐藏在每帧不清晰图像中的动态信息。然而，现有的BVFI方法通常不能充分利用所有有价信息，这 ultimately 阻碍其性能。在本文中，我们提出了一个简单的三stage框架来完全探索不清晰视频中的有用信息。帧 interpolate stage 使用时间变换可靠网络来直接从不清晰输入中提取有用信息并生成目标帧中的中间帧。帧特征融合 stage 通过双向径向变换可靠网络来探索每个目标帧的长期时间信息。并且，卷积Transformer 加持 Taylor 近似网络来重复回归高频细节。我们的三stage框架具有明确的任务分配和扩展性，其效果由多种实验证明。我们在 Adobe240 数据集、GoPro数据集、YouTube240 数据集和 Sony 数据集等四个标准准 benchmark 上评估了我们的模型，并取得了较高的比较结果。此外，我们还对真实的不清晰视频进行了实验，并证明了我们的模型具有良好的通用性。
</details></li>
</ul>
<hr>
<h2 id="IPDreamer-Appearance-Controllable-3D-Object-Generation-with-Image-Prompts"><a href="#IPDreamer-Appearance-Controllable-3D-Object-Generation-with-Image-Prompts" class="headerlink" title="IPDreamer: Appearance-Controllable 3D Object Generation with Image Prompts"></a>IPDreamer: Appearance-Controllable 3D Object Generation with Image Prompts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05375">http://arxiv.org/abs/2310.05375</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bohan Zeng, Shanglin Li, Yutang Feng, Hong Li, Sicheng Gao, Jiaming Liu, Huaxia Li, Xu Tang, Jianzhuang Liu, Baochang Zhang</li>
<li>for: 文章目的是提出一种新的文本到3D形态生成方法，以提高3D形态生成的控制性和质量。</li>
<li>methods: 该方法基于大规模文本到图像扩散模型，并采用变量分数精益熬煮法提高3D形态生成的精度和可控性。</li>
<li>results: 实验结果表明，IPDreamer可以有效地生成高质量的3D形态，与文本和图像提示相符，达到了控制性和可预测性的目标。<details>
<summary>Abstract</summary>
Recent advances in text-to-3D generation have been remarkable, with methods such as DreamFusion leveraging large-scale text-to-image diffusion-based models to supervise 3D generation. These methods, including the variational score distillation proposed by ProlificDreamer, enable the synthesis of detailed and photorealistic textured meshes. However, the appearance of 3D objects generated by these methods is often random and uncontrollable, posing a challenge in achieving appearance-controllable 3D objects. To address this challenge, we introduce IPDreamer, a novel approach that incorporates image prompts to provide specific and comprehensive appearance information for 3D object generation. Our results demonstrate that IPDreamer effectively generates high-quality 3D objects that are consistent with both the provided text and image prompts, demonstrating its promising capability in appearance-controllable 3D object generation.
</details>
<details>
<summary>摘要</summary>
近年文本到3D生成技术的发展很remarkable，如DreamFusion等方法利用大规模文本到图像扩散型模型来监督3D生成。这些方法，包括ProlificDreamer提出的变量分数热采样，使得可以生成细节rich和实际图像的纹理体。然而，由这些方法生成的3D对象的外观往往随机和无法控制，这成为实现外观可控3D对象的挑战。为解决这个挑战，我们介绍IPDreamer，一种新的方法，它通过图像提示来提供特定和全面的外观信息，以实现外观可控3D对象的生成。我们的结果表明，IPDreamer能够生成高质量的3D对象，这些对象与提供的文本和图像提示保持一致，这demonstrates its promising capability in appearance-controllable 3D object generation。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Prostate-Cancer-Diagnosis-with-Deep-Learning-A-Study-using-mpMRI-Segmentation-and-Classification"><a href="#Enhancing-Prostate-Cancer-Diagnosis-with-Deep-Learning-A-Study-using-mpMRI-Segmentation-and-Classification" class="headerlink" title="Enhancing Prostate Cancer Diagnosis with Deep Learning: A Study using mpMRI Segmentation and Classification"></a>Enhancing Prostate Cancer Diagnosis with Deep Learning: A Study using mpMRI Segmentation and Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05371">http://arxiv.org/abs/2310.05371</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anil B. Gavade, Neel Kanwal, Priyanka A. Gavade, Rajendra Nerli<br>for: 这个论文主要是为了提高膀肉癌早期诊断和精准诊断，以便提供更好的治疗方案。methods: 这个研究使用了深度学习模型来分类和 segmentation mpMRI 图像，并对不同的抑肿器进行了比较。results: 实验结果表明，结合 U-Net 和 LSTM 模型的ipeline 在分类和 segmentation 任务中表现最佳，其性能超过了所有其他组合。<details>
<summary>Abstract</summary>
Prostate cancer (PCa) is a severe disease among men globally. It is important to identify PCa early and make a precise diagnosis for effective treatment. For PCa diagnosis, Multi-parametric magnetic resonance imaging (mpMRI) emerged as an invaluable imaging modality that offers a precise anatomical view of the prostate gland and its tissue structure. Deep learning (DL) models can enhance existing clinical systems and improve patient care by locating regions of interest for physicians. Recently, DL techniques have been employed to develop a pipeline for segmenting and classifying different cancer types. These studies show that DL can be used to increase diagnostic precision and give objective results without variability. This work uses well-known DL models for the classification and segmentation of mpMRI images to detect PCa. Our implementation involves four pipelines; Semantic DeepSegNet with ResNet50, DeepSegNet with recurrent neural network (RNN), U-Net with RNN, and U-Net with a long short-term memory (LSTM). Each segmentation model is paired with a different classifier to evaluate the performance using different metrics. The results of our experiments show that the pipeline that uses the combination of U-Net and the LSTM model outperforms all other combinations, excelling in both segmentation and classification tasks.
</details>
<details>
<summary>摘要</summary>
乳腺癌（PCa）是男性全球最重要的疾病之一。 Early detection and precise diagnosis are crucial for effective treatment. Multi-parametric magnetic resonance imaging（mpMRI）已成为男性乳腺癌诊断的不可或缺的成像方式，可提供乳腺脏组织的精确 Анатомиче视图。深度学习（DL）模型可以增强现有的临床系统，提高患者的护理质量。在最近的研究中，DL技术已被用来开发分类和 segmentation 不同类型的癌病。这些研究表明，DL可以增强诊断的精度，提供 объектив的结果，无差异。本工作使用了一些常见的 DL 模型，用于分类和 segmentation mpMRI 图像，检测 PCa。我们的实现包括四个管道：Semantic DeepSegNet with ResNet50、DeepSegNet with RNN、U-Net with RNN 和 U-Net with LSTM。每个分 segmentation 模型都与不同的分类器相结合，以评估不同的 metric。实验结果表明，使用 U-Net 和 LSTM 模型的组合，在分 segmentation 和分类任务中具有最高的表现。
</details></li>
</ul>
<hr>
<h2 id="SocialCircle-Learning-the-Angle-based-Social-Interaction-Representation-for-Pedestrian-Trajectory-Prediction"><a href="#SocialCircle-Learning-the-Angle-based-Social-Interaction-Representation-for-Pedestrian-Trajectory-Prediction" class="headerlink" title="SocialCircle: Learning the Angle-based Social Interaction Representation for Pedestrian Trajectory Prediction"></a>SocialCircle: Learning the Angle-based Social Interaction Representation for Pedestrian Trajectory Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05370">http://arxiv.org/abs/2310.05370</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cocoon2wong/socialcircle">https://github.com/cocoon2wong/socialcircle</a></li>
<li>paper_authors: Conghao Wong, Beihao Xia, Xinge You</li>
<li>for: 这篇论文是为了研究和预测在复杂场景中的人工智能和自动驾驶系统中人体和车辆的运动轨迹的方法和技术。</li>
<li>methods: 这篇论文使用了一种新的角度基于的可教学社交表示（SocialCircle），通过在不同的方向角度对目标机器人的位置进行反射，来考虑社交互动的Context。</li>
<li>results: 实验表明，与新发布的轨迹预测模型一起训练SocialCircle后，可以Quantitatively提高预测性能，同时Qualitatively帮助更好地考虑社交互动when预测人行轨迹，与人类直觉相符。<details>
<summary>Abstract</summary>
Analyzing and forecasting trajectories of agents like pedestrians and cars in complex scenes has become more and more significant in many intelligent systems and applications. The diversity and uncertainty in socially interactive behaviors among a rich variety of agents make this task more challenging than other deterministic computer vision tasks. Researchers have made a lot of efforts to quantify the effects of these interactions on future trajectories through different mathematical models and network structures, but this problem has not been well solved. Inspired by marine animals that localize the positions of their companions underwater through echoes, we build a new anglebased trainable social representation, named SocialCircle, for continuously reflecting the context of social interactions at different angular orientations relative to the target agent. We validate the effect of the proposed SocialCircle by training it along with several newly released trajectory prediction models, and experiments show that the SocialCircle not only quantitatively improves the prediction performance, but also qualitatively helps better consider social interactions when forecasting pedestrian trajectories in a way that is consistent with human intuitions.
</details>
<details>
<summary>摘要</summary>
Inspired by marine animals that use echoes to locate their companions underwater, we propose a new angle-based trainable social representation, called SocialCircle, to continuously reflect the context of social interactions at different angular orientations relative to the target agent. We validate the effectiveness of SocialCircle by training it along with several recently released trajectory prediction models, and experiments show that it not only quantitatively improves prediction performance but also qualitatively considers social interactions in a way that is consistent with human intuition when forecasting pedestrian trajectories.
</details></li>
</ul>
<hr>
<h2 id="Rotation-Matters-Generalized-Monocular-3D-Object-Detection-for-Various-Camera-Systems"><a href="#Rotation-Matters-Generalized-Monocular-3D-Object-Detection-for-Various-Camera-Systems" class="headerlink" title="Rotation Matters: Generalized Monocular 3D Object Detection for Various Camera Systems"></a>Rotation Matters: Generalized Monocular 3D Object Detection for Various Camera Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05366">http://arxiv.org/abs/2310.05366</a></li>
<li>repo_url: None</li>
<li>paper_authors: SungHo Moon, JinWoo Bae, SungHoon Im<br>for: 这篇论文的目的是分析单目3D物体探测性能下降的原因，以及提出一种通用的3D物体探测方法，以提高其在不同摄像头系统上的性能。methods: 该论文采用了广泛的实验分析性能下降的原因，并提出了一种修正模块，用于更正估计的3D bounding box位置和方向。results: 该论文的提出的修正模块可以在大多数最新的3D物体探测网络上应用，提高了AP3D score（KITTI中等，IoU &gt; 70%）约6-10倍于基eline，而且对质量和量表现都有显著提高。<details>
<summary>Abstract</summary>
Research on monocular 3D object detection is being actively studied, and as a result, performance has been steadily improving. However, 3D object detection performance is significantly reduced when applied to a camera system different from the system used to capture the training datasets. For example, a 3D detector trained on datasets from a passenger car mostly fails to regress accurate 3D bounding boxes for a camera mounted on a bus. In this paper, we conduct extensive experiments to analyze the factors that cause performance degradation. We find that changing the camera pose, especially camera orientation, relative to the road plane caused performance degradation. In addition, we propose a generalized 3D object detection method that can be universally applied to various camera systems. We newly design a compensation module that corrects the estimated 3D bounding box location and heading direction. The proposed module can be applied to most of the recent 3D object detection networks. It increases AP3D score (KITTI moderate, IoU $> 70\%$) about 6-to-10-times above the baselines without additional training. Both quantitative and qualitative results show the effectiveness of the proposed method.
</details>
<details>
<summary>摘要</summary>
研究监视单目3D对象检测正在活跃进行，因此性能不断提高。然而，当应用于不同的摄像头系统时，3D对象检测性能会受到明显的降低。例如，一个用于汽车摄像头的3D检测器通常无法在公共汽车摄像头上提供准确的3D包围盒。在这篇论文中，我们进行了广泛的实验分析，发现改变摄像头pose，特别是摄像头方向 relative to the road plane，会导致性能下降。此外，我们提议一种通用的3D对象检测方法，可以适用于不同的摄像头系统。我们新设计了一个补偿模块，可以正确地修正估计的3D包围盒位置和方向。该模块可以应用于大多数最近的3D对象检测网络。它可以在不需要再训练的情况下，提高AP3D score（KITTI中等，IoU>70%）约6-10倍。both quantitative and qualitative results表明提案的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="C-2M-DoT-Cross-modal-consistent-multi-view-medical-report-generation-with-domain-transfer-network"><a href="#C-2M-DoT-Cross-modal-consistent-multi-view-medical-report-generation-with-domain-transfer-network" class="headerlink" title="C^2M-DoT: Cross-modal consistent multi-view medical report generation with domain transfer network"></a>C^2M-DoT: Cross-modal consistent multi-view medical report generation with domain transfer network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05355">http://arxiv.org/abs/2310.05355</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruizhi Wang, Xiangtao Wang, Jie Zhou, Thomas Lukasiewicz, Zhenghua Xu</li>
<li>for: 这个研究旨在提高医疗影像报告生成的准确性和可靠性，并且能够在单一影像入力下进行推导。</li>
<li>methods: 本研究提出了一个跨模式一致的多视角医疗报告生成模型（C^2M-DoT），包括对多视角资料进行semantic-based对比学习，并且将这些资料转换为单一模型中使用。</li>
<li>results: 实验结果显示，C^2M-DoT在两个公共 benchmark 数据集上均substantially outperform 州际前进行的基elines 在所有指标中。ablation 研究也证实了每个 ком成分的有效性和必要性。<details>
<summary>Abstract</summary>
In clinical scenarios, multiple medical images with different views are usually generated simultaneously, and these images have high semantic consistency. However, most existing medical report generation methods only consider single-view data. The rich multi-view mutual information of medical images can help generate more accurate reports, however, the dependence of multi-view models on multi-view data in the inference stage severely limits their application in clinical practice. In addition, word-level optimization based on numbers ignores the semantics of reports and medical images, and the generated reports often cannot achieve good performance. Therefore, we propose a cross-modal consistent multi-view medical report generation with a domain transfer network (C^2M-DoT). Specifically, (i) a semantic-based multi-view contrastive learning medical report generation framework is adopted to utilize cross-view information to learn the semantic representation of lesions; (ii) a domain transfer network is further proposed to ensure that the multi-view report generation model can still achieve good inference performance under single-view input; (iii) meanwhile, optimization using a cross-modal consistency loss facilitates the generation of textual reports that are semantically consistent with medical images. Extensive experimental studies on two public benchmark datasets demonstrate that C^2M-DoT substantially outperforms state-of-the-art baselines in all metrics. Ablation studies also confirmed the validity and necessity of each component in C^2M-DoT.
</details>
<details>
<summary>摘要</summary>
在临床场景下，通常同时生成多个具有不同视图的医疗图像，这些图像之间具有高度相关性。然而，大多数现有的医疗报告生成方法只考虑单视图数据。多视图图像之间的丰富多视图相互信息可以帮助生成更准确的报告，但是在推理阶段，多视图模型对多视图数据的依赖妨碍了它们在临床实践中的应用。此外，基于数字的单词优化 ignore 医疗图像和报告的 semantics，并且生成的报告frequently 无法达到好的性能。因此，我们提出了一种跨模态一致多视图医疗报告生成器（C^2M-DoT）。具体来说，我们采用了semantic-based multi-view contrastive learning医疗报告生成框架，以利用cross-view信息来学习lesion的semantic表示; 其次，我们提出了一种域转移网络，以确保多视图报告生成模型在单视图输入下仍可以实现好的推理性能; 最后，我们使用了一种跨模态一致损失来促进生成的文本报告与医疗图像之间的semantic一致。经过广泛的实验研究，我们发现C^2M-DoT在所有指标上都大幅超越了状态对比baselines。我们还进行了ablation研究，并证明了C^2M-DoT中每个组件的有效性和必要性。
</details></li>
</ul>
<hr>
<h2 id="Infrared-Small-Target-Detection-Using-Double-Weighted-Multi-Granularity-Patch-Tensor-Model-With-Tensor-Train-Decomposition"><a href="#Infrared-Small-Target-Detection-Using-Double-Weighted-Multi-Granularity-Patch-Tensor-Model-With-Tensor-Train-Decomposition" class="headerlink" title="Infrared Small Target Detection Using Double-Weighted Multi-Granularity Patch Tensor Model With Tensor-Train Decomposition"></a>Infrared Small Target Detection Using Double-Weighted Multi-Granularity Patch Tensor Model With Tensor-Train Decomposition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05347">http://arxiv.org/abs/2310.05347</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guiyu Zhang, Qunbo Lv, Zui Tao, Baoyu Zhu, Zheng Tan, Yuan Ma</li>
<li>for: 干预热小目标检测在远程感知领域中扮演着重要角色，这paper提出了一种新的double-weighted多粒度infrared patch tensor（DWMGIPT）模型，以解决现有方法中的缺陷。</li>
<li>methods: 该paper使用了多粒度infrared patch tensor（MGIPT）模型，通过不重叠的patches和tensor归一化基于tensor train（TT） decompositions来捕捉不同粒度信息。其次，通过自动权重机制来均衡不同粒度信息的重要性。最后，使用steering kernel（SK）来提取本地结构先验，以抑制干扰。</li>
<li>results: 对于不同的复杂场景，该paper的方法表现了更好的鲁棒性和稳定性，并且在不同评价指标下，与其他八种state-of-the-art方法进行比较，得到了更好的检测性能。<details>
<summary>Abstract</summary>
Infrared small target detection plays an important role in the remote sensing fields. Therefore, many detection algorithms have been proposed, in which the infrared patch-tensor (IPT) model has become a mainstream tool due to its excellent performance. However, most IPT-based methods face great challenges, such as inaccurate measure of the tensor low-rankness and poor robustness to complex scenes, which will leadto poor detection performance. In order to solve these problems, this paper proposes a novel double-weighted multi-granularity infrared patch tensor (DWMGIPT) model. First, to capture different granularity information of tensor from multiple modes, a multi-granularity infrared patch tensor (MGIPT) model is constructed by collecting nonoverlapping patches and tensor augmentation based on the tensor train (TT) decomposition. Second, to explore the latent structure of tensor more efficiently, we utilize the auto-weighted mechanism to balance the importance of information at different granularity. Then, the steering kernel (SK) is employed to extract local structure prior, which suppresses background interference such as strong edges and noise. Finally, an efficient optimization algorithm based on the alternating direction method of multipliers (ADMM) is presented to solve the model. Extensive experiments in various challenging scenes show that the proposed algorithm is robust to noise and different scenes. Compared with the other eight state-of-the-art methods, different evaluation metrics demonstrate that our method achieves better detection performance in various complex scenes.
</details>
<details>
<summary>摘要</summary>
infrared小目标检测在远程感知领域扮演着重要的角色。因此，许多检测算法已经被提出，其中infrared patch-tensor（IPT）模型因其出色的表现而成为主流工具。然而，大多数IPT基于的方法面临着准确度低、鲁棒性差等问题，这将导致检测性能差。为解决这些问题，本文提出了一种新的双重加权多级infrared patch tensor（DWMGIPT）模型。首先，通过多种模式收集非重叠的patches和tensor增强来捕捉不同粒度信息的tensor。其次，通过自动权重机制来均衡不同粒度信息的重要性。然后，使用执政kernel（SK）来提取本地结构优先项，以抑制干扰性的强边和噪声。最后，基于alternating direction method of multipliers（ADMM）的有效优化算法来解决模型。在不同的复杂场景中进行了广泛的实验，结果显示，提出的算法具有较好的鲁棒性和能够抗干扰性。与其他八种当前状态的方法进行比较，不同的评价指标都表明，我们的方法在多种复杂场景中实现了更好的检测性能。
</details></li>
</ul>
<hr>
<h2 id="Anyview-Generalizable-Indoor-3D-Object-Detection-with-Variable-Frames"><a href="#Anyview-Generalizable-Indoor-3D-Object-Detection-with-Variable-Frames" class="headerlink" title="Anyview: Generalizable Indoor 3D Object Detection with Variable Frames"></a>Anyview: Generalizable Indoor 3D Object Detection with Variable Frames</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05346">http://arxiv.org/abs/2310.05346</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xuxw98/AnyView">https://github.com/xuxw98/AnyView</a></li>
<li>paper_authors: Zhenyu Wu, Xiuwei Xu, Ziwei Wang, Chong Xia, Linqing Zhao, Jiwen Lu, Haibin Yan</li>
<li>for: 提出一种新的网络框架，用于实现室内3D对象检测，以适应实际应用场景中的变化输入帧数。</li>
<li>methods: 提出了一种新的3D检测框架，名为AnyView，可以在不同的输入帧数下进行检测，并且可以在不同的RGB-D图像帧中检测3D对象。</li>
<li>results: 在ScanNet dataset上进行了广泛的实验，并达到了高的检测精度和普适性，而且该方法的参数量与基eline相似。<details>
<summary>Abstract</summary>
In this paper, we propose a novel network framework for indoor 3D object detection to handle variable input frame numbers in practical scenarios. Existing methods only consider fixed frames of input data for a single detector, such as monocular RGB-D images or point clouds reconstructed from dense multi-view RGB-D images. While in practical application scenes such as robot navigation and manipulation, the raw input to the 3D detectors is the RGB-D images with variable frame numbers instead of the reconstructed scene point cloud. However, the previous approaches can only handle fixed frame input data and have poor performance with variable frame input. In order to facilitate 3D object detection methods suitable for practical tasks, we present a novel 3D detection framework named AnyView for our practical applications, which generalizes well across different numbers of input frames with a single model. To be specific, we propose a geometric learner to mine the local geometric features of each input RGB-D image frame and implement local-global feature interaction through a designed spatial mixture module. Meanwhile, we further utilize a dynamic token strategy to adaptively adjust the number of extracted features for each frame, which ensures consistent global feature density and further enhances the generalization after fusion. Extensive experiments on the ScanNet dataset show our method achieves both great generalizability and high detection accuracy with a simple and clean architecture containing a similar amount of parameters with the baselines.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的网络框架 для室内3D对象检测，以适应实际应用场景中的变量输入帧数。现有方法只考虑固定帧的输入数据，如独立的RGB-D图像或者 dense多视图RGB-D图像重建的点云。然而，在实际应用场景中，输入到3D检测器的原始数据是RGB-D图像，而不是重建的场景点云。这些前一些方法只能处理固定帧的输入数据，导致其在变量帧输入下表现不佳。为了使3D对象检测方法适用于实际任务，我们提出了一种名为AnyView的新的3D检测框架。我们认为，在不同的输入帧数下，我们可以通过地理学习器来挖掘每帧RGB-D图像的本地几何特征，并通过设计的空间混合模块来实现本地-全局特征互动。此外，我们还使用动态凭据策略来自适应性地调整每帧提取的特征数量，以保证每帧特征的净度和总特征密度的一致，从而进一步提高总体化性。我们在ScanNet数据集进行了广泛的实验，显示我们的方法可以同时具有优秀的一致性和高检测精度，并且具有简单清晰的架构，与基elines相似的参数量。
</details></li>
</ul>
<hr>
<h2 id="What-do-larger-image-classifiers-memorise"><a href="#What-do-larger-image-classifiers-memorise" class="headerlink" title="What do larger image classifiers memorise?"></a>What do larger image classifiers memorise?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05337">http://arxiv.org/abs/2310.05337</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michal Lukasik, Vaishnavh Nagarajan, Ankit Singh Rawat, Aditya Krishna Menon, Sanjiv Kumar</li>
<li>for: 研究现代神经网络的连接 между吸收和总结，以及过参数化模型的总结性能。</li>
<li>methods: 使用Feldman提出的度量来衡量具体训练示例的吸收程度，并对一个ResNet模型在图像分类任务上进行了实验性计算。</li>
<li>results: 发现大型神经网络模型在训练示例上的吸收趋势非常复杂，大多数示例在更大的模型下经受减少吸收，而其余示例则表现出拱形或增加吸收趋势。此外，发现各种 Feldman 吸收度指标不能准确捕捉这些基本趋势。最后，发现知识传递可以减少吸收，同时提高总结性能。<details>
<summary>Abstract</summary>
The success of modern neural networks has prompted study of the connection between memorisation and generalisation: overparameterised models generalise well, despite being able to perfectly fit (memorise) completely random labels. To carefully study this issue, Feldman proposed a metric to quantify the degree of memorisation of individual training examples, and empirically computed the corresponding memorisation profile of a ResNet on image classification bench-marks. While an exciting first glimpse into what real-world models memorise, this leaves open a fundamental question: do larger neural models memorise more? We present a comprehensive empirical analysis of this question on image classification benchmarks. We find that training examples exhibit an unexpectedly diverse set of memorisation trajectories across model sizes: most samples experience decreased memorisation under larger models, while the rest exhibit cap-shaped or increasing memorisation. We show that various proxies for the Feldman memorization score fail to capture these fundamental trends. Lastly, we find that knowledge distillation, an effective and popular model compression technique, tends to inhibit memorisation, while also improving generalisation. Specifically, memorisation is mostly inhibited on examples with increasing memorisation trajectories, thus pointing at how distillation improves generalisation.
</details>
<details>
<summary>摘要</summary>
modern neural network 的成功引起了 memorization 和通用化之间的关系的研究：过参数化模型可以通过完美地适应（memorize）完全随机的标签来泛化 well。为了精心研究这个问题，Feldman 提出了一个度量 Memorialization 的个体训练示例的度量，并对一个 ResNet 在图像分类benchmark 上进行了empirical计算。这是一个对实际模型的 memorization 进行了初步的研究，但还留下了一个基本问题：大型神经网络是否能够更好地记忆？我们进行了一项完整的 empirical 分析，发现训练示例在不同的模型大小下 exhibit 一些不同的记忆轨迹：大多数样本在大型模型下表现出减少的记忆，而剩下的一些样本则表现出顶点形或增加的记忆。我们发现了不同的 Feldman 记忆度量的代理都无法捕捉这些基本趋势。最后，我们发现了知识储存，一种效果和流行的模型压缩技术，会妨碍记忆，同时也会提高通用化。具体来说，记忆在大型模型下增加的样本上受到了妨碍，这指出了如何储存提高通用化。
</details></li>
</ul>
<hr>
<h2 id="GReAT-A-Graph-Regularized-Adversarial-Training-Method"><a href="#GReAT-A-Graph-Regularized-Adversarial-Training-Method" class="headerlink" title="GReAT: A Graph Regularized Adversarial Training Method"></a>GReAT: A Graph Regularized Adversarial Training Method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05336">http://arxiv.org/abs/2310.05336</a></li>
<li>repo_url: None</li>
<li>paper_authors: Samet Bayram, Kenneth Barner</li>
<li>for: 提高深度学习模型的分类性能</li>
<li>methods: 利用数据图 структуры进行对抗训练</li>
<li>results: 比靡�状态方法更有效，提高模型的抗性和通用性<details>
<summary>Abstract</summary>
This paper proposes a regularization method called GReAT, Graph Regularized Adversarial Training, to improve deep learning models' classification performance. Adversarial examples are a well-known challenge in machine learning, where small, purposeful perturbations to input data can mislead models. Adversarial training, a powerful and one of the most effective defense strategies, involves training models with both regular and adversarial examples. However, it often neglects the underlying structure of the data. In response, we propose GReAT, a method that leverages data graph structure to enhance model robustness. GReAT deploys the graph structure of the data into the adversarial training process, resulting in more robust models that better generalize its testing performance and defend against adversarial attacks. Through extensive evaluation on benchmark datasets, we demonstrate GReAT's effectiveness compared to state-of-the-art classification methods, highlighting its potential in improving deep learning models' classification performance.
</details>
<details>
<summary>摘要</summary>
Here's the translation in Simplified Chinese:这篇论文提出了一种名为GReAT（图structure regularized adversarial training）的常规化方法，用于改善深度学习模型的分类性能。对于机器学习来说，针对性例子是一种非常知名的挑战，小量、有目的的输入数据修改可以让模型产生错误。对此，我们提出了GReAT，一种利用数据图结构来增强模型的 Robustness。GReAT在对抗训练过程中部署了数据图结构，从而生成更加Robust的模型，能够更好地抗击针对性例子和对抗攻击。通过对标准 benchmark 数据集进行了广泛的评估，我们展示了GReAT的效果，与现有的分类方法相比，强调了它在改善深度学习模型的分类性能的潜在力量。
</details></li>
</ul>
<hr>
<h2 id="A-Lightweight-Video-Anomaly-Detection-Model-with-Weak-Supervision-and-Adaptive-Instance-Selection"><a href="#A-Lightweight-Video-Anomaly-Detection-Model-with-Weak-Supervision-and-Adaptive-Instance-Selection" class="headerlink" title="A Lightweight Video Anomaly Detection Model with Weak Supervision and Adaptive Instance Selection"></a>A Lightweight Video Anomaly Detection Model with Weak Supervision and Adaptive Instance Selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05330">http://arxiv.org/abs/2310.05330</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yang Wang, Jiaogen Zhou, Jihong Guan</li>
<li>For: This paper focuses on weakly supervised video anomaly detection, which is useful for effective and intelligent public safety management.* Methods: The proposed model uses an adaptive instance selection strategy and a lightweight multi-level temporal correlation attention module, which reduces the number of model parameters to 0.56% of existing methods.* Results: The proposed model achieves comparable or superior AUC scores compared to state-of-the-art methods, with a significantly reduced number of model parameters, making it suitable for resource-limited scenarios such as edge computing.Here is the simplified Chinese version of the three key points:* For: 这篇论文关注弱类视频异常检测，可以实现有效和智能的公共安全管理。* Methods: 提议的模型使用适应实例选择策略和轻量级多层时间相关注意力模块，这将模型参数减少至0.56%以上方法（如RTFM）。* Results: 提议的模型在两个公共数据集（UCF-Crime和上海理工大学）的广泛实验中，可以与状态的方法相比或超过其AUC分数，同时减少了模型参数的数量，适用于限制资源的场景如边缘计算。<details>
<summary>Abstract</summary>
Video anomaly detection is to determine whether there are any abnormal events, behaviors or objects in a given video, which enables effective and intelligent public safety management. As video anomaly labeling is both time-consuming and expensive, most existing works employ unsupervised or weakly supervised learning methods. This paper focuses on weakly supervised video anomaly detection, in which the training videos are labeled whether or not they contain any anomalies, but there is no information about which frames the anomalies are located. However, the uncertainty of weakly labeled data and the large model size prevent existing methods from wide deployment in real scenarios, especially the resource-limit situations such as edge-computing. In this paper, we develop a lightweight video anomaly detection model. On the one hand, we propose an adaptive instance selection strategy, which is based on the model's current status to select confident instances, thereby mitigating the uncertainty of weakly labeled data and subsequently promoting the model's performance. On the other hand, we design a lightweight multi-level temporal correlation attention module and an hourglass-shaped fully connected layer to construct the model, which can reduce the model parameters to only 0.56\% of the existing methods (e.g. RTFM). Our extensive experiments on two public datasets UCF-Crime and ShanghaiTech show that our model can achieve comparable or even superior AUC score compared to the state-of-the-art methods, with a significantly reduced number of model parameters.
</details>
<details>
<summary>摘要</summary>
视频异常检测是确定视频中是否存在任何异常事件、行为或物体，以实现有效和智能公共安全管理。由于视频异常标注是时间费时和昂贵的， większość existing works使用不upervised or weakly supervised learning methods。这篇论文关注弱类视频异常检测，在哪些训练视频被标注为异常或正常，但没有任何Frame异常的信息。然而，弱类标注数据的不确定性和大型模型的尺寸阻碍了现有方法在实际场景中广泛应用，特别是在资源有限的情况下，如边计算。在这篇论文中，我们开发了一个轻量级视频异常检测模型。一方面，我们提出了适应实例选择策略，基于模型当前状态选择信任实例，从而减轻弱类标注数据的不确定性，并使模型性能提高。另一方面，我们设计了轻量级多层时间相关注意力模块和梯形全连接层，构建模型，可以将模型参数减少至0.56%（例如RTFM）。我们对公共数据集UCF-Crime和ShanghaiTech进行了广泛的实验，结果表明，我们的模型可以与状态之artefacts方法相当或超过，同时具有显著减少模型参数的优势。
</details></li>
</ul>
<hr>
<h2 id="Edge-Computing-Enabled-Road-Condition-Monitoring-System-Development-and-Evaluation"><a href="#Edge-Computing-Enabled-Road-Condition-Monitoring-System-Development-and-Evaluation" class="headerlink" title="Edge Computing-Enabled Road Condition Monitoring: System Development and Evaluation"></a>Edge Computing-Enabled Road Condition Monitoring: System Development and Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05321">http://arxiv.org/abs/2310.05321</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdulateef Daud, Mark Amo-Boateng, Neema Jakisa Owor, Armstrong Aboah, Yaw Adu-Gyamfi</li>
<li>For: Real-time pavement condition monitoring for highway agencies to inform pavement maintenance and rehabilitation policies.* Methods: Utilizes affordable MEMS sensors, edge computing, and deployable machine learning models to stream live pavement condition data and reduce latency.* Results: Demonstrates high accuracy in predicting International Roughness Index and classifying pavement segments based on ride quality, with the potential to provide real-time data to State Highway Agencies and Department of Transportation.Here’s the Chinese version:* For: 提供高速公路机构实时路面状况监测，以便制定路面维护和重建策略。* Methods: 利用可获得的便宜MEMS传感器，边缘计算和可部署机器学习模型，实时流传路面状况数据，降低延迟。* Results: 实验表明，该方法可以高度准确地预测国际护净指数，并基于乘用质量分类路面段， achieved an average accuracy of 96.76% on I-70EB and 63.15% on South Providence.<details>
<summary>Abstract</summary>
Real-time pavement condition monitoring provides highway agencies with timely and accurate information that could form the basis of pavement maintenance and rehabilitation policies. Existing technologies rely heavily on manual data processing, are expensive and therefore, difficult to scale for frequent, networklevel pavement condition monitoring. Additionally, these systems require sending large packets of data to the cloud which requires large storage space, are computationally expensive to process, and results in high latency. The current study proposes a solution that capitalizes on the widespread availability of affordable Micro Electro-Mechanical System (MEMS) sensors, edge computing and internet connection capabilities of microcontrollers, and deployable machine learning (ML) models to (a) design an Internet of Things (IoT)-enabled device that can be mounted on axles of vehicles to stream live pavement condition data (b) reduce latency through on-device processing and analytics of pavement condition sensor data before sending to the cloud servers. In this study, three ML models including Random Forest, LightGBM and XGBoost were trained to predict International Roughness Index (IRI) at every 0.1-mile segment. XGBoost had the highest accuracy with an RMSE and MAPE of 16.89in/mi and 20.3%, respectively. In terms of the ability to classify the IRI of pavement segments based on ride quality according to MAP-21 criteria, our proposed device achieved an average accuracy of 96.76% on I-70EB and 63.15% on South Providence. Overall, our proposed device demonstrates significant potential in providing real-time pavement condition data to State Highway Agencies (SHA) and Department of Transportation (DOTs) with a satisfactory level of accuracy.
</details>
<details>
<summary>摘要</summary>
现有技术依赖于手动数据处理，昂贵，难以扩展到频繁的网络级别路面状况监测。此外，这些系统需要将大量数据发送到云服务器，需要大量存储空间，计算昂贵，导致高延迟。本研究提出一种解决方案，利用可得到的便宜MEMS传感器、边缘计算和互联网连接能力，以及可搬移的机器学习（ML）模型，设计一个互联网器（IoT）启用的设备，可以在车辙上附加，实时传输路面状况数据。在本研究中，我们使用Random Forest、LightGBM和XGBoost三种ML模型，对每个0.1英里路段进行预测国际折叠指数（IRI）。XGBoost模型的最小平均差误（RMSE）和平均绝对误差（MAPE）分别为16.89英寸/英里和20.3%。在根据MAP-21标准对路面段的车辙质量进行分类方面，我们的提案设备实现了70.76%的准确率在I-70EB，以及63.15%的准确率在南普罗维登斯。总的来说，我们的提案设备表现出了在提供实时路面状况数据给州公路局（SHA）和交通部（DOTs）的满意水平的可能性。
</details></li>
</ul>
<hr>
<h2 id="Understanding-the-Feature-Norm-for-Out-of-Distribution-Detection"><a href="#Understanding-the-Feature-Norm-for-Out-of-Distribution-Detection" class="headerlink" title="Understanding the Feature Norm for Out-of-Distribution Detection"></a>Understanding the Feature Norm for Out-of-Distribution Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05316">http://arxiv.org/abs/2310.05316</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sayantann11/all-classification-templetes-for-ML">https://github.com/sayantann11/all-classification-templetes-for-ML</a></li>
<li>paper_authors: Jaewoo Park, Jacky Chen Long Chai, Jaeho Yoon, Andrew Beng Jin Teoh</li>
<li>for: 本研究旨在解释 neural network 中批处数据集上的一个惊人现象：即在适用范围内的样本（ID）上，核心层特征的 vector 范数较高，而在未经见过的样本（OOD）上则较低。</li>
<li>methods: 我们通过分析潜在隐藏在网络层次中的分类结构，来解释这一现象。我们发现：1）特征范数是隐藏在网络层次中的类别预测值的信任度，具体来说是最大的 logit。2）特征范数是不同类别模型中的共同特征，可以检测 OOD 样本。3）传统的特征范数不能捕捉隐藏层神经元的启动和停止趋势，可能导致 ID 样本被误认为 OOD 样本。</li>
<li>results: 我们提出了一种新的负意识范数（NAN），可以捕捉隐藏层神经元的启动和停止趋势，并且可以与现有的 OOD 检测器兼容。我们进行了广泛的实验，证明 NAN 的有效性和可靠性，并且可以在无标签环境中使用。<details>
<summary>Abstract</summary>
A neural network trained on a classification dataset often exhibits a higher vector norm of hidden layer features for in-distribution (ID) samples, while producing relatively lower norm values on unseen instances from out-of-distribution (OOD). Despite this intriguing phenomenon being utilized in many applications, the underlying cause has not been thoroughly investigated. In this study, we demystify this very phenomenon by scrutinizing the discriminative structures concealed in the intermediate layers of a neural network. Our analysis leads to the following discoveries: (1) The feature norm is a confidence value of a classifier hidden in the network layer, specifically its maximum logit. Hence, the feature norm distinguishes OOD from ID in the same manner that a classifier confidence does. (2) The feature norm is class-agnostic, thus it can detect OOD samples across diverse discriminative models. (3) The conventional feature norm fails to capture the deactivation tendency of hidden layer neurons, which may lead to misidentification of ID samples as OOD instances. To resolve this drawback, we propose a novel negative-aware norm (NAN) that can capture both the activation and deactivation tendencies of hidden layer neurons. We conduct extensive experiments on NAN, demonstrating its efficacy and compatibility with existing OOD detectors, as well as its capability in label-free environments.
</details>
<details>
<summary>摘要</summary>
一种神经网络训练于分类数据集时，常会表现出高向量范围的隐藏层特征值，而对未看过的外部数据（OOD）则产生相对较低的范围值。尽管这一现象已被广泛应用，但它的根本原因尚未得到全面研究。在这种研究中，我们对神经网络中隐藏的权重结构进行了仔细的检查。我们的分析导致以下发现：1. 特征范围是隐藏在神经网络层次中的类ifier的信息量，具体来说是最大的logs。因此，特征范围可以用来分辨OOD和ID的样本。2. 特征范围是不同类型的权重模型中的共同特征，可以检测OOD样本。3. 传统的特征范围不能捕捉隐藏层神经元的抑制趋势，这可能导致ID样本被误分类为OOD样本。为解决这个缺陷，我们提出了一种新的负意识范围（NAN），可以捕捉隐藏层神经元的活动和抑制趋势。我们进行了广泛的实验，证明了NAN的有效性和与现有OOD检测器兼容性，以及它在无标签环境中的可行性。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/09/cs.CV_2023_10_09/" data-id="clp89dofa00lci788ee5ahift" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_10_09" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/09/cs.AI_2023_10_09/" class="article-date">
  <time datetime="2023-10-09T12:00:00.000Z" itemprop="datePublished">2023-10-09</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/09/cs.AI_2023_10_09/">cs.AI - 2023-10-09</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Estimating-Numbers-without-Regression"><a href="#Estimating-Numbers-without-Regression" class="headerlink" title="Estimating Numbers without Regression"></a>Estimating Numbers without Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06204">http://arxiv.org/abs/2310.06204</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aastha2104/Parkinson-Disease-Prediction">https://github.com/Aastha2104/Parkinson-Disease-Prediction</a></li>
<li>paper_authors: Avijit Thawani, Jay Pujara, Ashwin Kalyan</li>
<li>for: 提高语言模型对数字的表示能力</li>
<li>methods:  modificare 数字表示方法，包括notation、vocabulary和语言模型架构</li>
<li>results:  Tokenization scheme 可以提高 masked number prediction 性能，而无需大规模修改语言模型架构。<details>
<summary>Abstract</summary>
Despite recent successes in language models, their ability to represent numbers is insufficient. Humans conceptualize numbers based on their magnitudes, effectively projecting them on a number line; whereas subword tokenization fails to explicitly capture magnitude by splitting numbers into arbitrary chunks. To alleviate this shortcoming, alternative approaches have been proposed that modify numbers at various stages of the language modeling pipeline. These methods change either the (1) notation in which numbers are written (\eg scientific vs decimal), the (2) vocabulary used to represent numbers or the entire (3) architecture of the underlying language model, to directly regress to a desired number.   Previous work suggests that architectural change helps achieve state-of-the-art on number estimation but we find an insightful ablation: changing the model's vocabulary instead (\eg introduce a new token for numbers in range 10-100) is a far better trade-off. In the context of masked number prediction, a carefully designed tokenization scheme is both the simplest to implement and sufficient, \ie with similar performance to the state-of-the-art approach that requires making significant architectural changes. Finally, we report similar trends on the downstream task of numerical fact estimation (for Fermi Problems) and discuss reasons behind our findings.
</details>
<details>
<summary>摘要</summary>
尽管最近的语言模型具有了一定的成功，它们对数字的表示仍然不够。人类对数字基于其大小来思考，实际将其投射到数字线上，而分词 Tokenization 则不能明确地捕捉大小。为了解决这个缺陷，有些方法提议在语言模型的不同阶段进行修改。这些方法可以改变（1）数字的notation（例如科学 notation vs 十进制），（2）用于表示数字的词汇，或（3）语言模型的基础结构，以直接预测目标数字。根据我们的研究，对语言模型的结构进行修改可以达到领先的性能，但我们发现一个有趣的ablation：改变模型的词汇（例如引入10-100之间的数字新token）是一个更好的交换。在遮盲数字预测 зада务中，一个精心设计的tokenization scheme是最简单的实现方式，并且具有与采用大量结构修改的状态艺术领先性的相似性。最后，我们报告了相似的趋势在下游任务中（例如数学问题），并讨论了我们的发现的原因。
</details></li>
</ul>
<hr>
<h2 id="Look-Up-mAI-GeMM-Increasing-AI-GeMMs-Performance-by-Nearly-2-5x-via-msGeMM"><a href="#Look-Up-mAI-GeMM-Increasing-AI-GeMMs-Performance-by-Nearly-2-5x-via-msGeMM" class="headerlink" title="Look-Up mAI GeMM: Increasing AI GeMMs Performance by Nearly 2.5x via msGeMM"></a>Look-Up mAI GeMM: Increasing AI GeMMs Performance by Nearly 2.5x via msGeMM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06178">http://arxiv.org/abs/2310.06178</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saeed Maleki</li>
<li>for: 本研究旨在提出一种新的低精度数据类型算法，以提高人工智能模型的训练和执行效率。</li>
<li>methods: 本研究使用了一种新的msGeMM算法，该算法可以在低精度数据类型下实现AI模型的训练和执行，并且可以减少约2.5倍的乘法和加法指令数量。</li>
<li>results: 本研究的结果表明，msGeMM算法可以在NVIDIA和AMD的GPU上实现AI模型的训练和执行，并且可以提高模型训练和执行的效率。<details>
<summary>Abstract</summary>
AI models are increasing in size and recent advancement in the community has shown that unlike HPC applications where double precision datatype are required, lower-precision datatypes such as fp8 or int4 are sufficient to bring the same model quality both for training and inference. Following these trends, GPU vendors such as NVIDIA and AMD have added hardware support for fp16, fp8 and int8 GeMM operations with an exceptional performance via Tensor Cores. However, this paper proposes a new algorithm called msGeMM which shows that AI models with low-precision datatypes can run with ~2.5x fewer multiplication and add instructions. Efficient implementation of this algorithm requires special CUDA cores with the ability to add elements from a small look-up table at the rate of Tensor Cores.
</details>
<details>
<summary>摘要</summary>
“人工智能模型不断增大，而最新的社区发展表明，不同于高性能计算应用程序（HPC）中需要双精度数据类型，低精度数据类型如fp8或int4却可以提供同等模型质量 Both for training and inference。随着这些趋势，GPU提供者如NVIDIA和AMD已经添加了硬件支持 дляfp16、fp8和int8 GeMM操作，通过tensor核心实现了非常出色的性能。但本文提出了一新的算法called msGeMM，显示低精度数据类型的AI模型可以透过大约2.5倍的multiplication和add指令数量进行运算。实施此算法需要特殊的CUDA核心，能够快速从小look-up表中添加元素，与tensor核心具有相同的速度。”Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know and I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Factual-and-Personalized-Recommendations-using-Language-Models-and-Reinforcement-Learning"><a href="#Factual-and-Personalized-Recommendations-using-Language-Models-and-Reinforcement-Learning" class="headerlink" title="Factual and Personalized Recommendations using Language Models and Reinforcement Learning"></a>Factual and Personalized Recommendations using Language Models and Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06176">http://arxiv.org/abs/2310.06176</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jihwan Jeong, Yinlam Chow, Guy Tennenholtz, Chih-Wei Hsu, Azamat Tulepbergenov, Mohammad Ghavamzadeh, Craig Boutilier</li>
<li>for: 这个论文主要旨在开发一种能够为用户提供有趣、个性化、有挑战性的电影推荐系统，通过自然语言交互来匹配用户的偏好。</li>
<li>methods: 该论文使用了一种基于语言模型的推荐系统，其中包括一种基于用户偏好的嵌入空间表示，以及一种基于用户反馈的评价函数。</li>
<li>results: 经过实验 validate，该论文的方法可以在MovieLens 25M 数据集上提供有趣、个性化、有挑战性的电影推荐，并且可以准确地捕捉用户的偏好。<details>
<summary>Abstract</summary>
Recommender systems (RSs) play a central role in connecting users to content, products, and services, matching candidate items to users based on their preferences. While traditional RSs rely on implicit user feedback signals, conversational RSs interact with users in natural language. In this work, we develop a comPelling, Precise, Personalized, Preference-relevant language model (P4LM) that recommends items to users while putting emphasis on explaining item characteristics and their relevance. P4LM uses the embedding space representation of a user's preferences to generate compelling responses that are factually-grounded and relevant w.r.t. the user's preferences. Moreover, we develop a joint reward function that measures precision, appeal, and personalization, which we use as AI-based feedback in a reinforcement learning-based language model framework. Using the MovieLens 25M dataset, we demonstrate that P4LM delivers compelling, personalized movie narratives to users.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese translation:推荐系统（RS）在连接用户与内容、产品和服务方面扮演中心角色，根据用户的偏好来匹配候选项。传统的RS通过隐式用户反馈信号来工作，而对话式RS则通过自然语言与用户交互。在这项工作中，我们开发了一个名为P4LM的语言模型，它可以为用户推荐项目，同时强调解释项目特性以及其与用户偏好的相关性。P4LM使用用户偏好的嵌入空间表示来生成有吸引力和个性化的回应，这些回应与用户偏好相关。此外，我们开发了一个共同奖励函数，该函数衡量精度、吸引力和个性化三个方面，并用于在语言模型框架中作为AI-based反馈。使用MovieLens 25M数据集，我们示示了P4LM可以为用户提供有吸引力和个性化的电影情节。
</details></li>
</ul>
<hr>
<h2 id="How-does-prompt-engineering-affect-ChatGPT-performance-on-unsupervised-entity-resolution"><a href="#How-does-prompt-engineering-affect-ChatGPT-performance-on-unsupervised-entity-resolution" class="headerlink" title="How does prompt engineering affect ChatGPT performance on unsupervised entity resolution?"></a>How does prompt engineering affect ChatGPT performance on unsupervised entity resolution?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06174">http://arxiv.org/abs/2310.06174</a></li>
<li>repo_url: None</li>
<li>paper_authors: Khanin Sisaengsuwanchai, Navapat Nananukul, Mayank Kejriwal</li>
<li>for: 这篇论文是关于实体解析（ER）问题的研究，具体来说是研究如何使用大型自然语言模型（LLM）来自动决定两个实体是否指向同一个基础实体。</li>
<li>methods: 本论文使用了大型自然语言模型（LLM），如ChatGPT，来进行实体解析。研究者通过不同的提示方法来评估LLM的性能，并对不同数据集进行比较。</li>
<li>results: 研究结果显示，提示方法可以很大程度上影响LLM的性能，其中一些指标更加敏感于提示方法的变化。此外，结果还表明了数据集的不同性可能会导致提示方法的不同效果。<details>
<summary>Abstract</summary>
Entity Resolution (ER) is the problem of semi-automatically determining when two entities refer to the same underlying entity, with applications ranging from healthcare to e-commerce. Traditional ER solutions required considerable manual expertise, including feature engineering, as well as identification and curation of training data. In many instances, such techniques are highly dependent on the domain. With recent advent in large language models (LLMs), there is an opportunity to make ER much more seamless and domain-independent. However, it is also well known that LLMs can pose risks, and that the quality of their outputs can depend on so-called prompt engineering. Unfortunately, a systematic experimental study on the effects of different prompting methods for addressing ER, using LLMs like ChatGPT, has been lacking thus far. This paper aims to address this gap by conducting such a study. Although preliminary in nature, our results show that prompting can significantly affect the quality of ER, although it affects some metrics more than others, and can also be dataset dependent.
</details>
<details>
<summary>摘要</summary>
entity resolution (er) 是指自动或半自动地确定两个实体是指同一个基础实体，它在医疗、电子商务等领域有广泛的应用。传统的er解决方案需要较大的人工干预，包括特征工程和训练数据的标识和筛选。在许多情况下，这些技术是域特定的。 however， with the recent advent of large language models (LLMs), there is an opportunity to make er much more seamless and domain-independent. unfortunately, it is also well known that LLMs can pose risks, and the quality of their outputs can depend on so-called prompt engineering. to address this gap, this paper aims to conduct a systematic experimental study on the effects of different prompting methods for addressing er, using LLMs like chatgpt. although preliminary in nature, our results show that prompting can significantly affect the quality of er, although it affects some metrics more than others and can also be dataset dependent.
</details></li>
</ul>
<hr>
<h2 id="Memory-Consistent-Neural-Networks-for-Imitation-Learning"><a href="#Memory-Consistent-Neural-Networks-for-Imitation-Learning" class="headerlink" title="Memory-Consistent Neural Networks for Imitation Learning"></a>Memory-Consistent Neural Networks for Imitation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06171">http://arxiv.org/abs/2310.06171</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kaustubhsridhar/MCNN">https://github.com/kaustubhsridhar/MCNN</a></li>
<li>paper_authors: Kaustubh Sridhar, Souradeep Dutta, Dinesh Jayaraman, James Weimer, Insup Lee</li>
<li>For:	+ The paper is written for imitation learning applications, specifically to address the problem of compounding errors in policy synthesis.	+ The authors aim to develop a new method that can learn from expert demonstrations and improve the performance of imitation policies.* Methods:	+ The proposed method is called “memory-consistent neural network” (MCNN), which is a type of deep neural network that is designed to counter the compounding error phenomenon.	+ The MCNN outputs are hard-constrained to stay within clearly specified permissible regions anchored to prototypical “memory” training samples.	+ The authors provide a guaranteed upper bound for the sub-optimality gap induced by MCNN policies.* Results:	+ The authors test the MCNN method on 9 imitation learning tasks, including dexterous robotic manipulation and driving, proprioceptive inputs and visual inputs, and varying sizes and types of demonstration data.	+ They find large and consistent gains in performance using the MCNN method, validating that it is better-suited for imitation learning applications than vanilla deep neural networks.Here is the information in Simplified Chinese text:* For:	+ 这篇论文是为了解决imitition learning应用中的政策合成问题，具体来说是解决错误堆叠问题。	+ 作者们想要开发一种可以从专家示例学习并提高imitition政策的方法。* Methods:	+ 提出的方法是called “memory-consistent neural network” (MCNN)，这是一种特殊的深度神经网络，旨在解决错误堆叠问题。	+ MCNN输出是固定在明确规定的可能区域内的，这些可能区域是基于”memory”训练样本的概念示例。	+ 作者们提供了一个确定的上限 bound дляMCNN政策中的优化性差。* Results:	+ 作者们对9个imitition learning任务进行测试，包括dexterous robotic manipulation和驾驶、 proprioceptive inputs和视觉输入、以及不同的示例数据大小和类型。	+ 他们发现，使用MCNN方法可以获得大量和稳定的性能提升，证明MCNN比vanilla深度神经网络更适合imitition learning应用。<details>
<summary>Abstract</summary>
Imitation learning considerably simplifies policy synthesis compared to alternative approaches by exploiting access to expert demonstrations. For such imitation policies, errors away from the training samples are particularly critical. Even rare slip-ups in the policy action outputs can compound quickly over time, since they lead to unfamiliar future states where the policy is still more likely to err, eventually causing task failures. We revisit simple supervised ``behavior cloning'' for conveniently training the policy from nothing more than pre-recorded demonstrations, but carefully design the model class to counter the compounding error phenomenon. Our ``memory-consistent neural network'' (MCNN) outputs are hard-constrained to stay within clearly specified permissible regions anchored to prototypical ``memory'' training samples. We provide a guaranteed upper bound for the sub-optimality gap induced by MCNN policies. Using MCNNs on 9 imitation learning tasks, with MLP, Transformer, and Diffusion backbones, spanning dexterous robotic manipulation and driving, proprioceptive inputs and visual inputs, and varying sizes and types of demonstration data, we find large and consistent gains in performance, validating that MCNNs are better-suited than vanilla deep neural networks for imitation learning applications. Website: https://sites.google.com/view/mcnn-imitation
</details>
<details>
<summary>摘要</summary>
“模仿学习可以大幅简化政策生成比较于其他方法，通过利用专家示范的访问。对于这些模仿政策，错误离开训练样本是非常重要的。甚至 rare 的政策动作输出错误可以快速堆积，因为它们导致未知的未来状态， где政策仍然更有可能出错，最终导致任务失败。我们重新考虑简单的监督式“行为克隆”，通过只使用预录的示范来训练政策，但是注意地设计模型类来对错误堆积现象进行抗衡。我们的“记忆一致神经网络”（MCNN）输出是固定的约束在 clearly specified 的允许区域内， anchored 在“记忆”训练样本上。我们提供了对 MCNN 政策的至少优化差的保证上限。使用 MCNN 在 9 个模仿学习任务上，包括多种各种类型的示范数据，我们发现了大量和一致的性能提升，证明 MCNN 更适合于模仿学习应用。网站：https://sites.google.com/view/mcnn-imitation”Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Predictable-Artificial-Intelligence"><a href="#Predictable-Artificial-Intelligence" class="headerlink" title="Predictable Artificial Intelligence"></a>Predictable Artificial Intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06167">http://arxiv.org/abs/2310.06167</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/duemig/Stanford-Project-Predicting-stock-prices-using-a-LSTM-Network">https://github.com/duemig/Stanford-Project-Predicting-stock-prices-using-a-LSTM-Network</a></li>
<li>paper_authors: Lexin Zhou, Pablo A. Moreno-Casares, Fernando Martínez-Plumed, John Burden, Ryan Burnell, Lucy Cheke, Cèsar Ferri, Alexandru Marcoci, Behzad Mehrbakhsh, Yael Moros-Daval, Seán Ó hÉigeartaigh, Danaja Rutar, Wout Schellaert, Konstantinos Voudouris, José Hernández-Orallo</li>
<li>for: 这篇论文旨在探讨 Predictable AI 这一新兴研究领域的基本思想和挑战。</li>
<li>methods: 本论文使用的方法是阐述 Predictable AI 领域的问题、假设和挑战，并呼吁开发者关注 AI 预测性的问题。</li>
<li>results: 本论文认为，在 AI 预测性方面取得积极进展可以帮助建立更加可靠、负责任、控制、对齐和安全的 AI 生态系统，因此应该在性能之上优先考虑预测性。<details>
<summary>Abstract</summary>
We introduce the fundamental ideas and challenges of Predictable AI, a nascent research area that explores the ways in which we can anticipate key indicators of present and future AI ecosystems. We argue that achieving predictability is crucial for fostering trust, liability, control, alignment and safety of AI ecosystems, and thus should be prioritised over performance. While distinctive from other areas of technical and non-technical AI research, the questions, hypotheses and challenges relevant to Predictable AI were yet to be clearly described. This paper aims to elucidate them, calls for identifying paths towards AI predictability and outlines the potential impact of this emergent field.
</details>
<details>
<summary>摘要</summary>
我团队介绍了人工智能预测的基本想法和挑战，这是一个新兴的研究领域，探讨了如何预测AI生态系统中的关键指标。我们认为，实现预测性是在AI生态系统中建立信任、责任、控制、对齐和安全的关键，因此应该被优先于性能。尽管与其他技术和非技术AI研究领域不同，Predictable AI中的问题、假设和挑战仍未得到清晰描述。这篇论文的目的是为此领域提供定义，呼吁开发AI预测的道路，并详细说明这个新兴领域的潜在影响。
</details></li>
</ul>
<hr>
<h2 id="CAW-coref-Conjunction-Aware-Word-level-Coreference-Resolution"><a href="#CAW-coref-Conjunction-Aware-Word-level-Coreference-Resolution" class="headerlink" title="CAW-coref: Conjunction-Aware Word-level Coreference Resolution"></a>CAW-coref: Conjunction-Aware Word-level Coreference Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06165">http://arxiv.org/abs/2310.06165</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kareldo/wl-coref">https://github.com/kareldo/wl-coref</a></li>
<li>paper_authors: Karel D’Oosterlinck, Semere Kiros Bitew, Brandon Papineau, Christopher Potts, Thomas Demeester, Chris Develder</li>
<li>for: 这个论文的目的是提高Word-level核心参照解决方法的性能，以便在大量文档中进行信息提取。</li>
<li>methods: 这个论文使用了一种简单 yet effective的解决方法，即在Word-level核心参照模型中处理 conjunction  mentions，以提高 OntoNotes 测试集的 F1 分数。</li>
<li>results: 这个解决方法可以提高 OntoNotes 测试集的 F1 分数 by 0.9%，使得 Word-level 核心参照解决方法与 expensive SOTA 方法之间的差距缩小了 34.6%。<details>
<summary>Abstract</summary>
State-of-the-art coreference resolutions systems depend on multiple LLM calls per document and are thus prohibitively expensive for many use cases (e.g., information extraction with large corpora). The leading word-level coreference system (WL-coref) attains 96.6% of these SOTA systems' performance while being much more efficient. In this work, we identify a routine yet important failure case of WL-coref: dealing with conjoined mentions such as 'Tom and Mary'. We offer a simple yet effective solution that improves the performance on the OntoNotes test set by 0.9% F1, shrinking the gap between efficient word-level coreference resolution and expensive SOTA approaches by 34.6%. Our Conjunction-Aware Word-level coreference model (CAW-coref) and code is available at https://github.com/KarelDO/wl-coref.
</details>
<details>
<summary>摘要</summary>
现代核心投引系统取决于文档中多个LLM调用，因此对许多应用场景（如大量文档提取信息）而言是不可持预算的。领先的单词级投引系统（WL-coref）达到了96.6%的SOTA系统性能，而且非常高效。在这项工作中，我们发现了WL-coref中的一种重要且常见失败情况：处理连接的提及，如“Tom和Mary”。我们提出了一种简单 yet有效的解决方案，在OntoNotes测试集上提高了0.9%的F1分，将高效的单词级投引与昂贵的SOTA方法之间的差距缩小了34.6%。我们的Conjunction-Aware Word-level coreference模型（CAW-coref）和代码可以在GitHub上找到：https://github.com/KarelDO/wl-coref。
</details></li>
</ul>
<hr>
<h2 id="Understanding-Transfer-Learning-and-Gradient-Based-Meta-Learning-Techniques"><a href="#Understanding-Transfer-Learning-and-Gradient-Based-Meta-Learning-Techniques" class="headerlink" title="Understanding Transfer Learning and Gradient-Based Meta-Learning Techniques"></a>Understanding Transfer Learning and Gradient-Based Meta-Learning Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06148">http://arxiv.org/abs/2310.06148</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mikehuisman/transfer-meta-feature-representations">https://github.com/mikehuisman/transfer-meta-feature-representations</a></li>
<li>paper_authors: Mike Huisman, Aske Plaat, Jan N. van Rijn</li>
<li>for: 本研究旨在探讨meta-学习技术在不同数据分布下的表现，并比较了finetuning、MAML和Reptile三种方法的性能。</li>
<li>methods: 本研究使用了三种方法：finetuning、MAML和Reptile。finetuning是一种简单的微调方法，MAML是一种基于学习环境的meta-学习技术，Reptile是一种基于精度评估的meta-学习技术。</li>
<li>results: 研究结果表明，在相同数据分布下，finetuning的性能较高，而MAML和Reptile在不同数据分布下的性能较差。此外，研究还发现MAML和Reptile在严重数据缺乏情况下特化于快适应，而finetuning可以背景学习。最后，研究发现finetuning学习的特征为得到的特征更加多样和特异。<details>
<summary>Abstract</summary>
Deep neural networks can yield good performance on various tasks but often require large amounts of data to train them. Meta-learning received considerable attention as one approach to improve the generalization of these networks from a limited amount of data. Whilst meta-learning techniques have been observed to be successful at this in various scenarios, recent results suggest that when evaluated on tasks from a different data distribution than the one used for training, a baseline that simply finetunes a pre-trained network may be more effective than more complicated meta-learning techniques such as MAML, which is one of the most popular meta-learning techniques. This is surprising as the learning behaviour of MAML mimics that of finetuning: both rely on re-using learned features. We investigate the observed performance differences between finetuning, MAML, and another meta-learning technique called Reptile, and show that MAML and Reptile specialize for fast adaptation in low-data regimes of similar data distribution as the one used for training. Our findings show that both the output layer and the noisy training conditions induced by data scarcity play important roles in facilitating this specialization for MAML. Lastly, we show that the pre-trained features as obtained by the finetuning baseline are more diverse and discriminative than those learned by MAML and Reptile. Due to this lack of diversity and distribution specialization, MAML and Reptile may fail to generalize to out-of-distribution tasks whereas finetuning can fall back on the diversity of the learned features.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Reinforcement-Learning-in-the-Era-of-LLMs-What-is-Essential-What-is-needed-An-RL-Perspective-on-RLHF-Prompting-and-Beyond"><a href="#Reinforcement-Learning-in-the-Era-of-LLMs-What-is-Essential-What-is-needed-An-RL-Perspective-on-RLHF-Prompting-and-Beyond" class="headerlink" title="Reinforcement Learning in the Era of LLMs: What is Essential? What is needed? An RL Perspective on RLHF, Prompting, and Beyond"></a>Reinforcement Learning in the Era of LLMs: What is Essential? What is needed? An RL Perspective on RLHF, Prompting, and Beyond</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06147">http://arxiv.org/abs/2310.06147</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Sun</li>
<li>for: 这 paper 的目的是将传统RL与LLM研究中使用的RL技术相连接，解释RL在LLM中的优势和应用场景。</li>
<li>methods: 这 paper 使用了RLHF技术，具体来说是在线 inverse RL with offline demonstration data，并与 SFT 进行比较。</li>
<li>results: 这 paper 发现RLHF比 SFT 更为有利，因为它可以减少练习数据中的问题折衔。此外，RLHF 可以应用于其他 LLM 任务，如提问评估和优化，即使它们的反馈也是昂贵的。但RLHF 的策略学习更加具有挑战，因为它们的动作维度很高，并且反馈稀缺。<details>
<summary>Abstract</summary>
Recent advancements in Large Language Models (LLMs) have garnered wide attention and led to successful products such as ChatGPT and GPT-4. Their proficiency in adhering to instructions and delivering harmless, helpful, and honest (3H) responses can largely be attributed to the technique of Reinforcement Learning from Human Feedback (RLHF). In this paper, we aim to link the research in conventional RL to RL techniques used in LLM research. Demystify this technique by discussing why, when, and how RL excels. Furthermore, we explore potential future avenues that could either benefit from or contribute to RLHF research.   Highlighted Takeaways:   1. RLHF is Online Inverse RL with Offline Demonstration Data.   2. RLHF $>$ SFT because Imitation Learning (and Inverse RL) $>$ Behavior Cloning (BC) by alleviating the problem of compounding error.   3. The RM step in RLHF generates a proxy of the expensive human feedback, such an insight can be generalized to other LLM tasks such as prompting evaluation and optimization where feedback is also expensive.   4. The policy learning in RLHF is more challenging than conventional problems studied in IRL due to their high action dimensionality and feedback sparsity.   5. The main superiority of PPO over off-policy value-based methods is its stability gained from (almost) on-policy data and conservative policy updates.
</details>
<details>
<summary>摘要</summary>
最近的大语言模型（LLM）的进步引起了广泛的关注，如ChatGPT和GPT-4等产品。它们的遵循指令和提供无害、有益和诚实（3H）回复的能力归功于人类反馈学习（RLHF）的技术。在这篇论文中，我们想要将传统RL研究与LLM研究中的RL技术相连。通过解释RLHF的优势和应用场景，我们希望能够启发更多的研究者关注和投入到这一领域。突出的摘要：1. RLHF是在线反RL与离线示例数据的组合。2. RLHF比SFT更高效，因为假设学习（和反RL）比Behavior Cloning（BC）更高效，因为它可以解决复杂的错误问题。3. RM步骤在RLHF中生成了贵重的人类反馈的代理，这种理解可以推广到其他LLM任务，如提问评估和优化，其中Feedback也是贵重的。4. RLHF中策略学习比传统IRL中的问题更加挑战，因为它们具有高动作维度和反馈稀缺性。5. PPO在比值基方法更稳定，因为它在（近乎）在policy上的数据上学习，并且保守地更新策略。
</details></li>
</ul>
<hr>
<h2 id="Layout-Sequence-Prediction-From-Noisy-Mobile-Modality"><a href="#Layout-Sequence-Prediction-From-Noisy-Mobile-Modality" class="headerlink" title="Layout Sequence Prediction From Noisy Mobile Modality"></a>Layout Sequence Prediction From Noisy Mobile Modality</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06138">http://arxiv.org/abs/2310.06138</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Hai-chao-Zhang/LTrajDiff">https://github.com/Hai-chao-Zhang/LTrajDiff</a></li>
<li>paper_authors: Haichao Zhang, Yi Xu, Hongsheng Lu, Takayuki Shimizu, Yun Fu</li>
<li>for: 本研究旨在解决现实世界中 Layout sequence 和 trajectory prediction 模型所遇到的挑战，使得可以正确地预测行人 bounding box 的 trajectory。</li>
<li>methods: 我们提出了 LTrajDiff，一种新的方法，它将干扰或遮盲的对象视为与完全可见的对象一样重要。LTrajDiff 使用了来自移动设备的感知数据，但是也引入了新的挑战，如模式融合、噪声数据和缺失空间布局和对象大小信息。我们使用了一种杜因采样模型，通过粗细到细的扩散策略，将噪声数据融合到精度 Layout sequence 中。</li>
<li>results: 我们的模型在随机遮盲和非常短输入实验中达到了 SOTA Result，证明了我们的方法可以准确地预测行人 bounding box 的 trajectory，并且可以在实际世界中使用感知数据来预测 pedestrian movement。<details>
<summary>Abstract</summary>
Trajectory prediction plays a vital role in understanding pedestrian movement for applications such as autonomous driving and robotics. Current trajectory prediction models depend on long, complete, and accurately observed sequences from visual modalities. Nevertheless, real-world situations often involve obstructed cameras, missed objects, or objects out of sight due to environmental factors, leading to incomplete or noisy trajectories. To overcome these limitations, we propose LTrajDiff, a novel approach that treats objects obstructed or out of sight as equally important as those with fully visible trajectories. LTrajDiff utilizes sensor data from mobile phones to surmount out-of-sight constraints, albeit introducing new challenges such as modality fusion, noisy data, and the absence of spatial layout and object size information. We employ a denoising diffusion model to predict precise layout sequences from noisy mobile data using a coarse-to-fine diffusion strategy, incorporating the RMS, Siamese Masked Encoding Module, and MFM. Our model predicts layout sequences by implicitly inferring object size and projection status from a single reference timestamp or significantly obstructed sequences. Achieving SOTA results in randomly obstructed experiments and extremely short input experiments, our model illustrates the effectiveness of leveraging noisy mobile data. In summary, our approach offers a promising solution to the challenges faced by layout sequence and trajectory prediction models in real-world settings, paving the way for utilizing sensor data from mobile phones to accurately predict pedestrian bounding box trajectories. To the best of our knowledge, this is the first work that addresses severely obstructed and extremely short layout sequences by combining vision with noisy mobile modality, making it the pioneering work in the field of layout sequence trajectory prediction.
</details>
<details>
<summary>摘要</summary>
atrajectory prediction plays a vital role in understanding pedestrian movement for applications such as autonomous driving and robotics. Current trajectory prediction models depend on long, complete, and accurately observed sequences from visual modalities. Nevertheless, real-world situations often involve obstructed cameras, missed objects, or objects out of sight due to environmental factors, leading to incomplete or noisy trajectories. To overcome these limitations, we propose LTrajDiff, a novel approach that treats objects obstructed or out of sight as equally important as those with fully visible trajectories. LTrajDiff utilizes sensor data from mobile phones to surmount out-of-sight constraints, albeit introducing new challenges such as modality fusion, noisy data, and the absence of spatial layout and object size information. We employ a denoising diffusion model to predict precise layout sequences from noisy mobile data using a coarse-to-fine diffusion strategy, incorporating the RMS, Siamese Masked Encoding Module, and MFM. Our model predicts layout sequences by implicitly inferring object size and projection status from a single reference timestamp or significantly obstructed sequences. Achieving SOTA results in randomly obstructed experiments and extremely short input experiments, our model illustrates the effectiveness of leveraging noisy mobile data. In summary, our approach offers a promising solution to the challenges faced by layout sequence and trajectory prediction models in real-world settings, paving the way for utilizing sensor data from mobile phones to accurately predict pedestrian bounding box trajectories. To the best of our knowledge, this is the first work that addresses severely obstructed and extremely short layout sequences by combining vision with noisy mobile modality, making it the pioneering work in the field of layout sequence trajectory prediction.
</details></li>
</ul>
<hr>
<h2 id="Learning-Layer-wise-Equivariances-Automatically-using-Gradients"><a href="#Learning-Layer-wise-Equivariances-Automatically-using-Gradients" class="headerlink" title="Learning Layer-wise Equivariances Automatically using Gradients"></a>Learning Layer-wise Equivariances Automatically using Gradients</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06131">http://arxiv.org/abs/2310.06131</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tychovdo/ella">https://github.com/tychovdo/ella</a></li>
<li>paper_authors: Tycho F. A. van der Ouderaa, Alexander Immer, Mark van der Wilk</li>
<li>for: 提高神经网络的泛化性能，使其更好地适应不同的输入数据。</li>
<li>methods: 使用权重相互连接结构和梯度下降法自适应地学习层 wise 对称性。</li>
<li>results: 在图像分类任务上，自动学习层 wise 对称性可以达到与固定编码的对称性相同或更好的性能。<details>
<summary>Abstract</summary>
Convolutions encode equivariance symmetries into neural networks leading to better generalisation performance. However, symmetries provide fixed hard constraints on the functions a network can represent, need to be specified in advance, and can not be adapted. Our goal is to allow flexible symmetry constraints that can automatically be learned from data using gradients. Learning symmetry and associated weight connectivity structures from scratch is difficult for two reasons. First, it requires efficient and flexible parameterisations of layer-wise equivariances. Secondly, symmetries act as constraints and are therefore not encouraged by training losses measuring data fit. To overcome these challenges, we improve parameterisations of soft equivariance and learn the amount of equivariance in layers by optimising the marginal likelihood, estimated using differentiable Laplace approximations. The objective balances data fit and model complexity enabling layer-wise symmetry discovery in deep networks. We demonstrate the ability to automatically learn layer-wise equivariances on image classification tasks, achieving equivalent or improved performance over baselines with hard-coded symmetry.
</details>
<details>
<summary>摘要</summary>
将文本翻译成简化中文。</SYS>卷积层可以将等价对称性 encode到神经网络中，导致更好的泛化性表现。然而，对称性提供硬coded的约束，需要在预先指定，并不能适应。我们的目标是让柔性的对称约束，可以通过梯度学习自动从数据中学习。学习对称和相关的权重连接结构从零开始很困难，因为它们需要有效的和灵活的层wise equivariant parameterization。其次，对称性作为约束，因此不会被训练损失奖励。为了解决这些挑战，我们改进了软对称 parameterization，并通过优化 marginal likelihood，使用可微 differentiable Laplace approximations来学习层wise equivariance。该目标平衡数据适应和模型复杂度，使得层wise symmetry discovery在深度网络中自动学习。我们在图像分类任务上展示了自动学习层wise equivariance的能力，与硬编码的对称性具有相同或改进的性能。
</details></li>
</ul>
<hr>
<h2 id="On-Time-Domain-Conformer-Models-for-Monaural-Speech-Separation-in-Noisy-Reverberant-Acoustic-Environments"><a href="#On-Time-Domain-Conformer-Models-for-Monaural-Speech-Separation-in-Noisy-Reverberant-Acoustic-Environments" class="headerlink" title="On Time Domain Conformer Models for Monaural Speech Separation in Noisy Reverberant Acoustic Environments"></a>On Time Domain Conformer Models for Monaural Speech Separation in Noisy Reverberant Acoustic Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06125">http://arxiv.org/abs/2310.06125</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jwr1995/pubsep">https://github.com/jwr1995/pubsep</a></li>
<li>paper_authors: William Ravenscroft, Stefan Goetze, Thomas Hain</li>
<li>for: 这篇论文主要针对多mic麦克风技术领域的speech separation问题进行研究。</li>
<li>methods: 这篇论文使用了卷积扩展器（Conformers），它们在多种speech处理任务中表现良好，但在speech separation领域尚未得到充分研究。最近的state-of-the-art（SOTA）分离模型主要是时域音频分离网络（TasNets）。一些成功的模型使用了双路（DP）网络，它们在本地和全局信息的序列处理中做出了优秀的表现。</li>
<li>results: 在实际的短信号长度下，TD-Conformers在控制特征维度时表现更高效。 authors proposed subsampling layers to further improve computational efficiency. The best TD-Conformer achieves 14.6 dB and 21.2 dB SISDR improvement on the WHAMR and WSJ0-2Mix benchmarks, respectively.<details>
<summary>Abstract</summary>
Speech separation remains an important topic for multi-speaker technology researchers. Convolution augmented transformers (conformers) have performed well for many speech processing tasks but have been under-researched for speech separation. Most recent state-of-the-art (SOTA) separation models have been time-domain audio separation networks (TasNets). A number of successful models have made use of dual-path (DP) networks which sequentially process local and global information. Time domain conformers (TD-Conformers) are an analogue of the DP approach in that they also process local and global context sequentially but have a different time complexity function. It is shown that for realistic shorter signal lengths, conformers are more efficient when controlling for feature dimension. Subsampling layers are proposed to further improve computational efficiency. The best TD-Conformer achieves 14.6 dB and 21.2 dB SISDR improvement on the WHAMR and WSJ0-2Mix benchmarks, respectively.
</details>
<details>
<summary>摘要</summary>
<<SYS>>传送文本到简化中文。</SYS>>研究多говор话技术的人们认为，语音分离仍然是一个重要的话题。卷积加强变换器（conformers）在许多语音处理任务中表现良好，但是它们在语音分离方面尚未得到足够的研究。最近的最佳状态（SOTA）分离模型主要是时域音频分离网络（TasNets）。一些成功的模型具有双路（DP）网络，这些网络先后处理本地和全局信息。时域卷积器（TD-Conformers）是DP方法的同义词，它们也在本地和全局上下文中进行顺序处理，但是它们的时间复杂度函数不同。研究发现，对于更加现实的信号长度，TD-Conformers在控制特征维度时更加高效。抽样层被提议来进一步提高计算效率。最佳TD-Conformer在WHAMR和WSJ0-2Mix测试集上分别提高了14.6dB和21.2dB的SISDR指标。
</details></li>
</ul>
<hr>
<h2 id="Text-driven-Prompt-Generation-for-Vision-Language-Models-in-Federated-Learning"><a href="#Text-driven-Prompt-Generation-for-Vision-Language-Models-in-Federated-Learning" class="headerlink" title="Text-driven Prompt Generation for Vision-Language Models in Federated Learning"></a>Text-driven Prompt Generation for Vision-Language Models in Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06123">http://arxiv.org/abs/2310.06123</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Qiu, Xingyu Li, Chaithanya Kumar Mummadi, Madan Ravi Ganesh, Zhenzhen Li, Lu Peng, Wan-Yi Lin</li>
<li>for: 这个研究旨在提出一种整合多个远端客户的 Federated Text-driven Prompt Generation（FedTPG）方法，以实现视觉语言模型的普遍化。</li>
<li>methods: 这个方法使用了一个内置的文本输入，并通过一个专门的生成网络来学习文本提示。这个生成网络是基于任务相关的文本输入，因此具有内在的内容感知能力，可以对未见过的类别进行普遍化。</li>
<li>results: 我们的实验结果显示，这个方法在九个多标的图像分类任务上比较出色，可以对已知和未知的类别进行更好的普遍化，并且可以应用于新的数据集。<details>
<summary>Abstract</summary>
Prompt learning for vision-language models, e.g., CoOp, has shown great success in adapting CLIP to different downstream tasks, making it a promising solution for federated learning due to computational reasons. Existing prompt learning techniques replace hand-crafted text prompts with learned vectors that offer improvements on seen classes, but struggle to generalize to unseen classes. Our work addresses this challenge by proposing Federated Text-driven Prompt Generation (FedTPG), which learns a unified prompt generation network across multiple remote clients in a scalable manner. The prompt generation network is conditioned on task-related text input, thus is context-aware, making it suitable to generalize for both seen and unseen classes. Our comprehensive empirical evaluations on nine diverse image classification datasets show that our method is superior to existing federated prompt learning methods, that achieve overall better generalization on both seen and unseen classes and is also generalizable to unseen datasets.
</details>
<details>
<summary>摘要</summary>
Prompt learning for vision-language models, such as CoOp, has shown great success in adapting CLIP to different downstream tasks, making it a promising solution for federated learning due to computational reasons. Existing prompt learning techniques replace hand-crafted text prompts with learned vectors that offer improvements on seen classes, but struggle to generalize to unseen classes. Our work addresses this challenge by proposing Federated Text-driven Prompt Generation (FedTPG), which learns a unified prompt generation network across multiple remote clients in a scalable manner. The prompt generation network is conditioned on task-related text input, thus is context-aware, making it suitable to generalize for both seen and unseen classes. Our comprehensive empirical evaluations on nine diverse image classification datasets show that our method is superior to existing federated prompt learning methods, achieving overall better generalization on both seen and unseen classes and is also generalizable to unseen datasets.Here's the translation breakdown:* "Prompt learning" is translated as "提示学习" (tíshì xuéxí)* "Vision-language models" is translated as "视觉语言模型" (wèi jiàn yǔ yán módel)* "CoOp" is translated as "CoOp" (同义词)* "CLIP" is translated as "CLIP" (同义词)* "Federated learning" is translated as "联合学习" (liánhé xuéxí)* "Hand-crafted text prompts" is translated as "手工编写的文本提示" (shǒu gōng biān xī de wén tiě zhǐ)* "Learned vectors" is translated as "学习后的 вектор" (xuéxí hòu de vector)* "Task-related text input" is translated as "任务相关的文本输入" (tâi yè xiāngguān de wén tiě shūrū)* "Context-aware" is translated as "Context-aware" (同义词)* "Federated Text-driven Prompt Generation" is translated as "联合文本驱动提示生成" (liánhé wén tiě qiú xíng chǎng zhǐ jiàn)* "Existing federated prompt learning methods" is translated as "现有的联合提示学习方法" (xiàn yǒu de liánhé zhǐ xuéxí fāngfa)* "Generalize to unseen classes" is translated as "泛化到未见类" (guānghuà dào wèi jiàn lèi)* "Our comprehensive empirical evaluations" is translated as "我们的全面实验评估" (wǒmen de quánxiān shíyìn zhìshì)Note that the translation is based on the standard Simplified Chinese pronunciation and may vary depending on the specific dialect or accent.
</details></li>
</ul>
<hr>
<h2 id="Exploring-Progress-in-Multivariate-Time-Series-Forecasting-Comprehensive-Benchmarking-and-Heterogeneity-Analysis"><a href="#Exploring-Progress-in-Multivariate-Time-Series-Forecasting-Comprehensive-Benchmarking-and-Heterogeneity-Analysis" class="headerlink" title="Exploring Progress in Multivariate Time Series Forecasting: Comprehensive Benchmarking and Heterogeneity Analysis"></a>Exploring Progress in Multivariate Time Series Forecasting: Comprehensive Benchmarking and Heterogeneity Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06119">http://arxiv.org/abs/2310.06119</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zezhishao/basicts">https://github.com/zezhishao/basicts</a></li>
<li>paper_authors: Zezhi Shao, Fei Wang, Yongjun Xu, Wei Wei, Chengqing Yu, Zhao Zhang, Di Yao, Guangyin Jin, Xin Cao, Gao Cong, Christian S. Jensen, Xueqi Cheng</li>
<li>for: 本研究旨在解决现有的评价缺陷和技术方法选择争议，提供关于多变量时间序列预测（MTS）领域进步的深入理解。</li>
<li>methods: 本研究提出了一个名为BasicTS的比较平台，用于公正地评价多变量时间序列预测模型。 BasicTS 设置了一个标准的训练管道和合理的评价标准，使得评估了超过 30 种常见 MTS 预测模型的性能。</li>
<li>results: 研究发现，现有的 MTS 预测模型在不同的时间和空间特征下表现有很大差异。 BasicTS 可以帮助研究人员选择和设计适合的 MTS 预测模型，并提供了多个可重现的性能和效率比较结果。<details>
<summary>Abstract</summary>
Multivariate Time Series (MTS) widely exists in real-word complex systems, such as traffic and energy systems, making their forecasting crucial for understanding and influencing these systems. Recently, deep learning-based approaches have gained much popularity for effectively modeling temporal and spatial dependencies in MTS, specifically in Long-term Time Series Forecasting (LTSF) and Spatial-Temporal Forecasting (STF). However, the fair benchmarking issue and the choice of technical approaches have been hotly debated in related work. Such controversies significantly hinder our understanding of progress in this field. Thus, this paper aims to address these controversies to present insights into advancements achieved. To resolve benchmarking issues, we introduce BasicTS, a benchmark designed for fair comparisons in MTS forecasting. BasicTS establishes a unified training pipeline and reasonable evaluation settings, enabling an unbiased evaluation of over 30 popular MTS forecasting models on more than 18 datasets. Furthermore, we highlight the heterogeneity among MTS datasets and classify them based on temporal and spatial characteristics. We further prove that neglecting heterogeneity is the primary reason for generating controversies in technical approaches. Moreover, based on the proposed BasicTS and rich heterogeneous MTS datasets, we conduct an exhaustive and reproducible performance and efficiency comparison of popular models, providing insights for researchers in selecting and designing MTS forecasting models.
</details>
<details>
<summary>摘要</summary>
多变量时间系列（MTS）广泛存在在实际世界复杂系统中，如交通和能源系统，其预测对这些系统的理解和影响是关键。在最近几年，深度学习基于方法在MTS预测中得到了很多欢迎，特别是在长期时间序列预测（LTSF）和空间-时间预测（STF）中。然而，实际工作中的公平比较问题和技术方法选择问题一直是热点议题。这些争议很大程度上阻碍了我们对这个领域的进步的理解。因此，这篇论文旨在解决这些争议，提供关于领域的进步的新视角。为了解决公平比较问题，我们提出了BasicTS，一个用于公平比较的 benchmark。BasicTS 设计了一个统一的训练管道和合理的评估设置，使得不受偏见的评估了超过30种常见MTS预测模型在18个数据集上。此外，我们还发现了MTS数据集中的多样性，并将其分为了时间和空间特征的两类。我们还证明了忽略多样性是预测技术方法中的主要问题。此外，基于我们提出的BasicTS和丰富的多样化MTS数据集，我们进行了广泛和可重复的性和效率比较，为研究人员提供了选择和设计MTS预测模型的新的指导思想。
</details></li>
</ul>
<hr>
<h2 id="Take-a-Step-Back-Evoking-Reasoning-via-Abstraction-in-Large-Language-Models"><a href="#Take-a-Step-Back-Evoking-Reasoning-via-Abstraction-in-Large-Language-Models" class="headerlink" title="Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models"></a>Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06117">http://arxiv.org/abs/2310.06117</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huaixiu Steven Zheng, Swaroop Mishra, Xinyun Chen, Heng-Tze Cheng, Ed H. Chi, Quoc V Le, Denny Zhou</li>
<li>for: 提高LLM的抽象能力，使其能够从具体情况中提取高级概念和原则，以便更好地进行正确的逻辑推理。</li>
<li>methods: 使用Step-Back Prompting技术，通过提供高级概念和原则来引导LLM的推理步骤，使其能够更好地遵循正确的逻辑推理路径。</li>
<li>results: 在多种复杂的逻辑推理任务中，使用Step-Back Prompting技术可以提高PaLM-2L模型的性能，比如物理和化学知识测验（MMLU Physics和Chemistry）上提高7%和11%，时间问答（TimeQA）上提高27%，以及多步逻辑推理任务（MuSiQue）上提高7%。<details>
<summary>Abstract</summary>
We present Step-Back Prompting, a simple prompting technique that enables LLMs to do abstractions to derive high-level concepts and first principles from instances containing specific details. Using the concepts and principles to guide the reasoning steps, LLMs significantly improve their abilities in following a correct reasoning path towards the solution. We conduct experiments of Step-Back Prompting with PaLM-2L models and observe substantial performance gains on a wide range of challenging reasoning-intensive tasks including STEM, Knowledge QA, and Multi-Hop Reasoning. For instance, Step-Back Prompting improves PaLM-2L performance on MMLU Physics and Chemistry by 7% and 11%, TimeQA by 27%, and MuSiQue by 7%.
</details>
<details>
<summary>摘要</summary>
我们提出Step-Back Prompting，一种简单的提示技术，让机器学习模型（LLMs）从具体的实例中抽象出高水平概念和基本假设，然后使用这些概念和假设来引导逻辑步骤，以提高LLMs的解释正确性。我们在PaLM-2L模型上进行实验，并观察到了广泛的应用数据领域中的表现优化，包括STEM、知识问题答案和多步逻辑等。例如，Step-Back Prompting在物理和化学MMLU中提高PaLM-2L表现的比例为7%和11%，在TimeQA中提高27%，在MuSiQue中提高7%。
</details></li>
</ul>
<hr>
<h2 id="OptiMUS-Optimization-Modeling-Using-MIP-Solvers-and-large-language-models"><a href="#OptiMUS-Optimization-Modeling-Using-MIP-Solvers-and-large-language-models" class="headerlink" title="OptiMUS: Optimization Modeling Using MIP Solvers and large language models"></a>OptiMUS: Optimization Modeling Using MIP Solvers and large language models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06116">http://arxiv.org/abs/2310.06116</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/teshnizi/optimus">https://github.com/teshnizi/optimus</a></li>
<li>paper_authors: Ali AhmadiTeshnizi, Wenzhi Gao, Madeleine Udell</li>
<li>for: 这个论文是为了提供一种基于自然语言描述的优化问题解决方案。</li>
<li>methods: 该论文使用了大语言模型（LLM）来解决优化问题，包括发展数学模型、编写和调试解决方案代码、开发测试、检查生成解决方案的有效性。</li>
<li>results: 试验表明，OptiMUS可以比基本的LLM提示策略多解决优化问题。<details>
<summary>Abstract</summary>
Optimization problems are pervasive across various sectors, from manufacturing and distribution to healthcare. However, most such problems are still solved heuristically by hand rather than optimally by state-of-the-art solvers, as the expertise required to formulate and solve these problems limits the widespread adoption of optimization tools and techniques. We introduce OptiMUS, a Large Language Model (LLM)-based agent designed to formulate and solve MILP problems from their natural language descriptions. OptiMUS is capable of developing mathematical models, writing and debugging solver code, developing tests, and checking the validity of generated solutions. To benchmark our agent, we present NLP4LP, a novel dataset of linear programming (LP) and mixed integer linear programming (MILP) problems. Our experiments demonstrate that OptiMUS solves nearly twice as many problems as a basic LLM prompting strategy. OptiMUS code and NLP4LP dataset are available at \href{https://github.com/teshnizi/OptiMUS}{https://github.com/teshnizi/OptiMUS}
</details>
<details>
<summary>摘要</summary>
优化问题在不同领域广泛存在，从制造和分布到医疗。然而，大多数这些问题仍然通过手动规则来解决而不是使用当前的优化解决方案，因为解决这些问题所需的专业知识限制了优化工具和技术的普及。我们介绍OptiMUS，一个基于大语言模型（LLM）的代理人，可以从自然语言描述中形式化和解决优化问题。OptiMUS可以开发数学模型，编写和调试解决器代码，开发测试，并检查生成的解决方案的有效性。为了评估我们的代理人，我们提出了NLP4LP数据集，一个新的线性 программирова（LP）和混合整数线性程序（MILP）问题的数据集。我们的实验表明，OptiMUS可以比基本的LLM提示策略多 solves一半的问题。OptiMUS代码和NLP4LP数据集可以在<https://github.com/teshnizi/OptiMUS>获取。
</details></li>
</ul>
<hr>
<h2 id="Learning-Interactive-Real-World-Simulators"><a href="#Learning-Interactive-Real-World-Simulators" class="headerlink" title="Learning Interactive Real-World Simulators"></a>Learning Interactive Real-World Simulators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06114">http://arxiv.org/abs/2310.06114</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mengjiao Yang, Yilun Du, Kamyar Ghasemipour, Jonathan Tompson, Dale Schuurmans, Pieter Abbeel</li>
<li>for: 这篇论文的目的是学习一个 универсаль的实际世界模拟器（UniSim），用于模拟人类和机器人之间的互动。</li>
<li>methods: 这篇论文使用生成模型来学习不同类型的数据，包括图像、视频和机器人数据，以实现真实的实际世界 simulate。</li>
<li>results: 这篇论文的实验结果表明，通过在UniSim中训练高级视觉语言规划和低级强化学习策略，可以在真实世界中展示零批量训练的功能。此外，视频描述模型也可以通过与Simulink进行培训，提高其应用范围。<details>
<summary>Abstract</summary>
Generative models trained on internet data have revolutionized how text, image, and video content can be created. Perhaps the next milestone for generative models is to simulate realistic experience in response to actions taken by humans, robots, and other interactive agents. Applications of a real-world simulator range from controllable content creation in games and movies, to training embodied agents purely in simulation that can be directly deployed in the real world. We explore the possibility of learning a universal simulator (UniSim) of real-world interaction through generative modeling. We first make the important observation that natural datasets available for learning a real-world simulator are often rich along different axes (e.g., abundant objects in image data, densely sampled actions in robotics data, and diverse movements in navigation data). With careful orchestration of diverse datasets, each providing a different aspect of the overall experience, UniSim can emulate how humans and agents interact with the world by simulating the visual outcome of both high-level instructions such as "open the drawer" and low-level controls such as "move by x, y" from otherwise static scenes and objects. There are numerous use cases for such a real-world simulator. As an example, we use UniSim to train both high-level vision-language planners and low-level reinforcement learning policies, each of which exhibit zero-shot real-world transfer after training purely in a learned real-world simulator. We also show that other types of intelligence such as video captioning models can benefit from training with simulated experience in UniSim, opening up even wider applications. Video demos can be found at https://universal-simulator.github.io.
</details>
<details>
<summary>摘要</summary>
优化模型在互联网数据上进行训练已经革命化了文本、图像和视频内容的创建方式。可能下一个里程碑 для优化模型是模拟人类、机器人和其他交互代理的真实经验，响应于人类和机器人的行为。我们探讨了通过生成模型学习的UniSim universal simulator，以模拟人类和代理之间的互动。我们发现了自然数据集的重要观察：各种数据集在不同的轴上充满着数据（例如图像数据中的充满物体、机器人数据中的紧密的动作和导航数据中的多种运动）。通过综合考虑这些不同的数据集，UniSim可以模拟人类和代理在世界中交互的方式，包括通过高级指令如“打开抽屉”和低级控制如“移动by x, y”来模拟静止场景和物体的视觉结果。这种真实世界模拟器有很多应用场景。例如，我们使用UniSim训练高级视力语言规划和低级强化学习策略，它们在唯一学习的真实世界模拟器中展现出零基础真实世界传递。此外，我们还发现了训练在UniSim中的视频描述模型可以受益于实际经验，开阔了更广泛的应用领域。视频 demo 可以在 <https://universal-simulator.github.io> 找到。
</details></li>
</ul>
<hr>
<h2 id="When-is-Agnostic-Reinforcement-Learning-Statistically-Tractable"><a href="#When-is-Agnostic-Reinforcement-Learning-Statistically-Tractable" class="headerlink" title="When is Agnostic Reinforcement Learning Statistically Tractable?"></a>When is Agnostic Reinforcement Learning Statistically Tractable?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06113">http://arxiv.org/abs/2310.06113</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zeyu Jia, Gene Li, Alexander Rakhlin, Ayush Sekhari, Nathan Srebro</li>
<li>for: 学习一个 unknown MDP 中的 $\epsilon$-优秀策略， Given a policy class $\Pi$。</li>
<li>methods: 使用一种新的复杂度度量——\emph{spanning capacity}，该度量只取决于集合 $\Pi$ 而不依赖于 MDP 动态。</li>
<li>results: 显示存在一个 policy class $\Pi$ 的 bounded spanning capacity 可以学习，但是需要 superpolynomial 数量的样本。此外，我们还提出了一种新的算法 called POPLER，可以实现 statistically efficient online RL。<details>
<summary>Abstract</summary>
We study the problem of agnostic PAC reinforcement learning (RL): given a policy class $\Pi$, how many rounds of interaction with an unknown MDP (with a potentially large state and action space) are required to learn an $\epsilon$-suboptimal policy with respect to $\Pi$? Towards that end, we introduce a new complexity measure, called the \emph{spanning capacity}, that depends solely on the set $\Pi$ and is independent of the MDP dynamics. With a generative model, we show that for any policy class $\Pi$, bounded spanning capacity characterizes PAC learnability. However, for online RL, the situation is more subtle. We show there exists a policy class $\Pi$ with a bounded spanning capacity that requires a superpolynomial number of samples to learn. This reveals a surprising separation for agnostic learnability between generative access and online access models (as well as between deterministic/stochastic MDPs under online access). On the positive side, we identify an additional \emph{sunflower} structure, which in conjunction with bounded spanning capacity enables statistically efficient online RL via a new algorithm called POPLER, which takes inspiration from classical importance sampling methods as well as techniques for reachable-state identification and policy evaluation in reward-free exploration.
</details>
<details>
<summary>摘要</summary>
我们研究无知RL问题（Policy Optimization with Unknown Dynamics）：给定一个策略集合 $\Pi$，何时需要多少回交互 avec一个未知MDP（可能具有很大的状态和动作空间）以学习一个 $\epsilon$-优化策略？为了解决这个问题，我们引入了一个新的复杂度度量，即 \emph{spanning capacity}，这个度量只取决于集合 $\Pi$，与MDP动力完全无关。使用生成模型，我们证明了任何策略集合 $\Pi$ 的 bounded spanning capacity 是PAC学习可能的。然而，在在线RL中，情况更加复杂。我们证明了存在一个策略集合 $\Pi$ 的 bounded spanning capacity 需要超polynomial数量的样本来学习。这表明了无知学习中的分开性，在生成访问模型和在线访问模型之间，以及在渐进性和随机MDP之间。然而，我们还发现了一种附加的 \emph{sunflower} 结构，它可以在 conjunction  WITH bounded spanning capacity 使得在线RL可以通过一种新的算法called POPLER来实现，这个算法结合了古典的重要性抽象方法以及探索和策略评估技术在奖励free探索中。
</details></li>
</ul>
<hr>
<h2 id="High-Dimensional-Causal-Inference-with-Variational-Backdoor-Adjustment"><a href="#High-Dimensional-Causal-Inference-with-Variational-Backdoor-Adjustment" class="headerlink" title="High Dimensional Causal Inference with Variational Backdoor Adjustment"></a>High Dimensional Causal Inference with Variational Backdoor Adjustment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06100">http://arxiv.org/abs/2310.06100</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/danielmisrael/variational-backdoor-adjustment">https://github.com/danielmisrael/variational-backdoor-adjustment</a></li>
<li>paper_authors: Daniel Israel, Aditya Grover, Guy Van den Broeck</li>
<li>for: 这篇论文旨在应用后门调整方法来估计干扰量，并解决高维度干扰和变量的问题。</li>
<li>methods: 本论文使用生成模型来实现后门调整，并将后门调整视为variational推导中的优化问题。</li>
<li>results: 实验结果显示，本方法能够在高维度设置下估计干扰likelihood，并在各种高维度应用中实现成功。<details>
<summary>Abstract</summary>
Backdoor adjustment is a technique in causal inference for estimating interventional quantities from purely observational data. For example, in medical settings, backdoor adjustment can be used to control for confounding and estimate the effectiveness of a treatment. However, high dimensional treatments and confounders pose a series of potential pitfalls: tractability, identifiability, optimization. In this work, we take a generative modeling approach to backdoor adjustment for high dimensional treatments and confounders. We cast backdoor adjustment as an optimization problem in variational inference without reliance on proxy variables and hidden confounders. Empirically, our method is able to estimate interventional likelihood in a variety of high dimensional settings, including semi-synthetic X-ray medical data. To the best of our knowledge, this is the first application of backdoor adjustment in which all the relevant variables are high dimensional.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用后门调整技术来估计从完全观察数据中的干预量。例如，在医疗设置下，后门调整可以控制干预因素和估计治疗效果。然而，高维度干预和干预因素可能存在一系列潜在的陷阱：可追踪性、可识别性和优化。在这项工作中，我们采用生成模型方法来实现后门调整。我们将后门调整视为变量推断中的优化问题，而不需要使用代理变量和隐藏干预因素。实际上，我们的方法可以在高维度设置下估计干预概率，包括半人工X射数据等多种高维度设置。根据我们知道，这是首次在所有相关变量都是高维度情况下应用后门调整。
</details></li>
</ul>
<hr>
<h2 id="Predictive-auxiliary-objectives-in-deep-RL-mimic-learning-in-the-brain"><a href="#Predictive-auxiliary-objectives-in-deep-RL-mimic-learning-in-the-brain" class="headerlink" title="Predictive auxiliary objectives in deep RL mimic learning in the brain"></a>Predictive auxiliary objectives in deep RL mimic learning in the brain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06089">http://arxiv.org/abs/2310.06089</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ching Fang, Kimberly L Stachenfeld<br>for:This paper explores the use of predictive auxiliary objectives in deep reinforcement learning (RL) to support representation learning and improve task performance.methods:The paper uses a deep RL system with self-supervised auxiliary objectives to study the effects of predictive learning on representation learning across different modules of the system.results:The paper finds that predictive objectives improve and stabilize learning, particularly in resource-limited architectures, and identifies settings where longer predictive horizons better support representational transfer. Additionally, the paper finds that representational changes in the RL system bear a striking resemblance to changes in neural activity observed in the brain.<details>
<summary>Abstract</summary>
The ability to predict upcoming events has been hypothesized to comprise a key aspect of natural and machine cognition. This is supported by trends in deep reinforcement learning (RL), where self-supervised auxiliary objectives such as prediction are widely used to support representation learning and improve task performance. Here, we study the effects predictive auxiliary objectives have on representation learning across different modules of an RL system and how these mimic representational changes observed in the brain. We find that predictive objectives improve and stabilize learning particularly in resource-limited architectures, and we identify settings where longer predictive horizons better support representational transfer. Furthermore, we find that representational changes in this RL system bear a striking resemblance to changes in neural activity observed in the brain across various experiments. Specifically, we draw a connection between the auxiliary predictive model of the RL system and hippocampus, an area thought to learn a predictive model to support memory-guided behavior. We also connect the encoder network and the value learning network of the RL system to visual cortex and striatum in the brain, respectively. This work demonstrates how representation learning in deep RL systems can provide an interpretable framework for modeling multi-region interactions in the brain. The deep RL perspective taken here also suggests an additional role of the hippocampus in the brain -- that of an auxiliary learning system that benefits representation learning in other regions.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Performative-Time-Series-Forecasting"><a href="#Performative-Time-Series-Forecasting" class="headerlink" title="Performative Time-Series Forecasting"></a>Performative Time-Series Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06077">http://arxiv.org/abs/2310.06077</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/adityalab/pets">https://github.com/adityalab/pets</a></li>
<li>paper_authors: Zhiyuan Zhao, Alexander Rodriguez, B. Aditya Prakash</li>
<li>for: 本文旨在解决时间序列预测中的回馈循环问题，即预测结果可能会影响实际结果，从而改变预测目标变量的分布。</li>
<li>methods: 本文提出了一种新的方法Feature Performative-Shifting（FPS），利用延迟响应来预测分布的变化，并根据此预测目标变量。</li>
<li>results: 实验结果表明，FPS方法可以有效地处理回馈循环引起的挑战，并在COVID-19和交通预测任务中表现出优于传统时间序列预测方法。<details>
<summary>Abstract</summary>
Time-series forecasting is a critical challenge in various domains and has witnessed substantial progress in recent years. Many real-life scenarios, such as public health, economics, and social applications, involve feedback loops where predictions can influence the predicted outcome, subsequently altering the target variable's distribution. This phenomenon, known as performativity, introduces the potential for 'self-negating' or 'self-fulfilling' predictions. Despite extensive studies in classification problems across domains, performativity remains largely unexplored in the context of time-series forecasting from a machine-learning perspective.   In this paper, we formalize performative time-series forecasting (PeTS), addressing the challenge of accurate predictions when performativity-induced distribution shifts are possible. We propose a novel approach, Feature Performative-Shifting (FPS), which leverages the concept of delayed response to anticipate distribution shifts and subsequently predicts targets accordingly. We provide theoretical insights suggesting that FPS can potentially lead to reduced generalization error. We conduct comprehensive experiments using multiple time-series models on COVID-19 and traffic forecasting tasks. The results demonstrate that FPS consistently outperforms conventional time-series forecasting methods, highlighting its efficacy in handling performativity-induced challenges.
</details>
<details>
<summary>摘要</summary>
时间序列预测是各个领域中的一项重要挑战，在过去几年中得到了重要进展。许多实际场景，如公共卫生、经济和社会应用，都存在反馈循环，其中预测结果可能会影响预测结果的分布，从而导致“自我实现”或“自我否定”的预测。这种现象被称为“表现力”，它在机器学习角度来看，尚未在时间序列预测中得到了广泛的研究。在这篇论文中，我们正式定义了表现力时间序列预测（PeTS），即在预测过程中考虑表现力引起的分布变化的挑战。我们提出了一种新的方法，即特征表现滚动（FPS），它利用延迟应答来预测分布变化，并根据此预测目标。我们提供了理论分析，表明FPS可能会减少泛化误差。我们在COVID-19和交通预测任务上进行了广泛的实验，结果表明FPS在处理表现力引起的挑战时表现出色，高于传统时间序列预测方法。
</details></li>
</ul>
<hr>
<h2 id="Pain-Forecasting-using-Self-supervised-Learning-and-Patient-Phenotyping-An-attempt-to-prevent-Opioid-Addiction"><a href="#Pain-Forecasting-using-Self-supervised-Learning-and-Patient-Phenotyping-An-attempt-to-prevent-Opioid-Addiction" class="headerlink" title="Pain Forecasting using Self-supervised Learning and Patient Phenotyping: An attempt to prevent Opioid Addiction"></a>Pain Forecasting using Self-supervised Learning and Patient Phenotyping: An attempt to prevent Opioid Addiction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06075">http://arxiv.org/abs/2310.06075</a></li>
<li>repo_url: None</li>
<li>paper_authors: Swati Padhee, Tanvi Banerjee, Daniel M. Abrams, Nirmish Shah</li>
<li>for: 本研究旨在预测患有抗阻塞综合症（SCD）的患者未来疼痛轨迹，以提高他们的生活质量而无需妥协他们的治疗。</li>
<li>methods: 本研究使用自动学习方法来解决疼痛预测问题，并对时间序列数据进行归类，以分类患者的亚群并提供个性化的治疗方案。</li>
<li>results: 实验结果显示，我们的模型在五年的实际数据上表现出色，超过了现有的标准准则，并可以准确地分类患者，提供有价值的临床决策信息。<details>
<summary>Abstract</summary>
Sickle Cell Disease (SCD) is a chronic genetic disorder characterized by recurrent acute painful episodes. Opioids are often used to manage these painful episodes; the extent of their use in managing pain in this disorder is an issue of debate. The risk of addiction and side effects of these opioid treatments can often lead to more pain episodes in the future. Hence, it is crucial to forecast future patient pain trajectories to help patients manage their SCD to improve their quality of life without compromising their treatment. It is challenging to obtain many pain records to design forecasting models since it is mainly recorded by patients' self-report. Therefore, it is expensive and painful (due to the need for patient compliance) to solve pain forecasting problems in a purely supervised manner. In light of this challenge, we propose to solve the pain forecasting problem using self-supervised learning methods. Also, clustering such time-series data is crucial for patient phenotyping, anticipating patients' prognoses by identifying "similar" patients, and designing treatment guidelines tailored to homogeneous patient subgroups. Hence, we propose a self-supervised learning approach for clustering time-series data, where each cluster comprises patients who share similar future pain profiles. Experiments on five years of real-world datasets show that our models achieve superior performance over state-of-the-art benchmarks and identify meaningful clusters that can be translated into actionable information for clinical decision-making.
</details>
<details>
<summary>摘要</summary>
针对患有悉尼细胞病（SCD）的患者，我们提出了一种基于自我监督学习的痛情预测方法。这种方法可以帮助患者更好地管理自己的病情，提高生活质量，而不需要妥协对治疗的影响。由于痛情记录的收集是主要由患者自己报告，因此收集数据的成本很高，而且需要患者的合作性，这使得解决痛情预测问题在完全监督方式下是非常困难的。为了解决这个问题，我们提出了一种基于自我监督学习的时间序列数据划分方法，每个分组包含拥有相似未来痛情轨迹的患者。我们对实际数据进行五年的实验，结果表明我们的模型在比较状态的标准准的基础上表现出色，并能够分配有意义的分组，这些分组可以被翻译成临床决策中的有用信息。
</details></li>
</ul>
<hr>
<h2 id="Augmenting-Vision-Based-Human-Pose-Estimation-with-Rotation-Matrix"><a href="#Augmenting-Vision-Based-Human-Pose-Estimation-with-Rotation-Matrix" class="headerlink" title="Augmenting Vision-Based Human Pose Estimation with Rotation Matrix"></a>Augmenting Vision-Based Human Pose Estimation with Rotation Matrix</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06068">http://arxiv.org/abs/2310.06068</a></li>
<li>repo_url: None</li>
<li>paper_authors: Milad Vazan, Fatemeh Sadat Masoumi, Ruizhi Ou, Reza Rawassizadeh</li>
<li>for: 本研究旨在提高基于姿势估计的活动识别精度，通过结合pose estimation和novel数据增强方法。</li>
<li>methods: 本研究使用pose estimation和一种新的数据增强方法，即旋转矩阵，来增强活动识别的精度。</li>
<li>results: 经过我们的实验，我们发现使用SVM与SGD优化，并结合旋转矩阵数据增强方法，可以达到96%的活动识别精度，而不使用数据增强方法的基准精度只有64%。<details>
<summary>Abstract</summary>
Fitness applications are commonly used to monitor activities within the gym, but they often fail to automatically track indoor activities inside the gym. This study proposes a model that utilizes pose estimation combined with a novel data augmentation method, i.e., rotation matrix. We aim to enhance the classification accuracy of activity recognition based on pose estimation data. Through our experiments, we experiment with different classification algorithms along with image augmentation approaches. Our findings demonstrate that the SVM with SGD optimization, using data augmentation with the Rotation Matrix, yields the most accurate results, achieving a 96% accuracy rate in classifying five physical activities. Conversely, without implementing the data augmentation techniques, the baseline accuracy remains at a modest 64%.
</details>
<details>
<summary>摘要</summary>
fitness 应用程序通常用于健身房内活动监测，但它们经常无法自动跟踪健身房内的活动。本研究提出一种使用 pose estimation 和 rotation matrix 的模型，以提高基于 pose estimation 数据的活动识别精度。我们通过不同的分类算法和图像增强方法进行实验，发现使用 SVM  WITH SGD 优化和数据增强方法，可以达到 96% 的正确率，分类五种物理活动。相比之下，没有实施数据增强技术，基准精度只有 64%。
</details></li>
</ul>
<hr>
<h2 id="LLM-for-SoC-Security-A-Paradigm-Shift"><a href="#LLM-for-SoC-Security-A-Paradigm-Shift" class="headerlink" title="LLM for SoC Security: A Paradigm Shift"></a>LLM for SoC Security: A Paradigm Shift</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06046">http://arxiv.org/abs/2310.06046</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dipayan Saha, Shams Tarek, Katayoon Yahyaei, Sujan Kumar Saha, Jingbo Zhou, Mark Tehranipoor, Farimah Farahmandi</li>
<li>for: 提高SoC设计流程中的安全性 verification的效率、可扩展性和适应性。</li>
<li>methods: 利用生成式预训练 transformer（GPT）技术来替代现有的安全解决方案，以提供更加有效、可扩展和适应的安全验证方法。</li>
<li>results: 通过实践案例和实验研究，得到了GPT在SoC安全验证中的成果，包括提高验证效率、扩展验证范围和适应性能。<details>
<summary>Abstract</summary>
As the ubiquity and complexity of system-on-chip (SoC) designs increase across electronic devices, the task of incorporating security into an SoC design flow poses significant challenges. Existing security solutions are inadequate to provide effective verification of modern SoC designs due to their limitations in scalability, comprehensiveness, and adaptability. On the other hand, Large Language Models (LLMs) are celebrated for their remarkable success in natural language understanding, advanced reasoning, and program synthesis tasks. Recognizing an opportunity, our research delves into leveraging the emergent capabilities of Generative Pre-trained Transformers (GPTs) to address the existing gaps in SoC security, aiming for a more efficient, scalable, and adaptable methodology. By integrating LLMs into the SoC security verification paradigm, we open a new frontier of possibilities and challenges to ensure the security of increasingly complex SoCs. This paper offers an in-depth analysis of existing works, showcases practical case studies, demonstrates comprehensive experiments, and provides useful promoting guidelines. We also present the achievements, prospects, and challenges of employing LLM in different SoC security verification tasks.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)随着系统在片（SoC）设计的 ubique 和复杂性的增加，在电子设备中实现安全性的任务变得更加困难。现有的安全解决方案因其缺乏扩展性、全面性和适应性而无法提供有效的验证。然而，大型自然语言模型（LLM）在自然语言理解、高级逻辑和程序生成任务中受到广泛的赞誉。我们的研究希望通过利用生成预训练转换器（GPT）的emergent capability来解决现有的安全阻碍，以实现更加高效、可扩展和适应的方法学。通过将LLM integrate into SoC安全验证模式，我们开启了一个新的前ier的可能性和挑战，以确保逐渐增加的SoC的安全性。本文提供了深入的现有工作分析、实践案例展示、全面的实验和有用的推广指南。我们还提出了使用LLM在不同的SoC安全验证任务中的成就、前景和挑战。
</details></li>
</ul>
<hr>
<h2 id="Generative-ensemble-deep-learning-severe-weather-prediction-from-a-deterministic-convection-allowing-model"><a href="#Generative-ensemble-deep-learning-severe-weather-prediction-from-a-deterministic-convection-allowing-model" class="headerlink" title="Generative ensemble deep learning severe weather prediction from a deterministic convection-allowing model"></a>Generative ensemble deep learning severe weather prediction from a deterministic convection-allowing model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06045">http://arxiv.org/abs/2310.06045</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yingkaisha/severe_weather_cgan">https://github.com/yingkaisha/severe_weather_cgan</a></li>
<li>paper_authors: Yingkai Sha, Ryan A. Sobash, David John Gagne II</li>
<li>For: This paper is written for the purpose of developing an ensemble post-processing method for probabilistic prediction of severe weather (tornadoes, hail, and wind gusts) over the conterminous United States (CONUS).* Methods: The method combines conditional generative adversarial networks (CGANs) and a convolutional neural network (CNN) to post-process convection-allowing model (CAM) forecasts. The CGANs create synthetic ensemble members from deterministic CAM forecasts, and the CNN processes the outputs to estimate the probability of severe weather.* Results: The method produced skillful predictions with up to 20% Brier Skill Score (BSS) increases compared to other neural-network-based reference methods using a testing dataset of HRRR forecasts in 2021. The method also provided meaningful ensemble spreads that can distinguish good and bad forecasts, despite being overconfident. The quality of CGAN outputs was found to be similar to a numerical ensemble, preserving inter-variable correlations and the contribution of influential predictors.<details>
<summary>Abstract</summary>
An ensemble post-processing method is developed for the probabilistic prediction of severe weather (tornadoes, hail, and wind gusts) over the conterminous United States (CONUS). The method combines conditional generative adversarial networks (CGANs), a type of deep generative model, with a convolutional neural network (CNN) to post-process convection-allowing model (CAM) forecasts. The CGANs are designed to create synthetic ensemble members from deterministic CAM forecasts, and their outputs are processed by the CNN to estimate the probability of severe weather. The method is tested using High-Resolution Rapid Refresh (HRRR) 1--24 hr forecasts as inputs and Storm Prediction Center (SPC) severe weather reports as targets. The method produced skillful predictions with up to 20% Brier Skill Score (BSS) increases compared to other neural-network-based reference methods using a testing dataset of HRRR forecasts in 2021. For the evaluation of uncertainty quantification, the method is overconfident but produces meaningful ensemble spreads that can distinguish good and bad forecasts. The quality of CGAN outputs is also evaluated. Results show that the CGAN outputs behave similarly to a numerical ensemble; they preserved the inter-variable correlations and the contribution of influential predictors as in the original HRRR forecasts. This work provides a novel approach to post-process CAM output using neural networks that can be applied to severe weather prediction.
</details>
<details>
<summary>摘要</summary>
一种ensemble post-processing方法被开发用于预测美国大陆部分地区（CONUS）的严重天气（风暴、冰雨和风速）。该方法结合了条件生成隐藏模型（CGANs）和卷积神经网络（CNN）来处理可变性模型（CAM）预测。CGANs用于创建基于权值的ensemble成员，并将其输出经过CNN处理以估计严重天气的概率。该方法使用2021年的高分解速Refresh（HRRR）1--24小时预测作为输入，并使用 Storm Prediction Center（SPC）的严重天气报告作为目标。该方法生成了有20%的Brier Skill Score（BSS）提升 compared to其他基于神经网络的参考方法。为了评估不确定性评估，该方法显示出了一定的过于自信心，但生成了有意义的ensemble距离，可以分辨出好和坏预测。此外，CGAN输出的质量也被评估，结果表明CGAN输出与原始HRRR预测的相互关系和重要预测变量的贡献保持了一致。这种方法可以应用于严重天气预测中的深度学习post-processing。
</details></li>
</ul>
<hr>
<h2 id="DyST-Towards-Dynamic-Neural-Scene-Representations-on-Real-World-Videos"><a href="#DyST-Towards-Dynamic-Neural-Scene-Representations-on-Real-World-Videos" class="headerlink" title="DyST: Towards Dynamic Neural Scene Representations on Real-World Videos"></a>DyST: Towards Dynamic Neural Scene Representations on Real-World Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06020">http://arxiv.org/abs/2310.06020</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maximilian Seitzer, Sjoerd van Steenkiste, Thomas Kipf, Klaus Greff, Mehdi S. M. Sajjadi</li>
<li>for: 本研究旨在从单摄视频中提取真实世界场景的3D结构和动态特征，以便生成视频中的视图。</li>
<li>methods: 该模型基于现有的神经场景表示方法，通过一种新的协作训练方法和新的人工数据集DySO，以分解单摄视频为场景内容、每个视图的场景动态和摄像机pose。</li>
<li>results: 模型学习到了可质感的幂等特征，可以分离控制摄像机和场景内容的视图生成。<details>
<summary>Abstract</summary>
Visual understanding of the world goes beyond the semantics and flat structure of individual images. In this work, we aim to capture both the 3D structure and dynamics of real-world scenes from monocular real-world videos. Our Dynamic Scene Transformer (DyST) model leverages recent work in neural scene representation to learn a latent decomposition of monocular real-world videos into scene content, per-view scene dynamics, and camera pose. This separation is achieved through a novel co-training scheme on monocular videos and our new synthetic dataset DySO. DyST learns tangible latent representations for dynamic scenes that enable view generation with separate control over the camera and the content of the scene.
</details>
<details>
<summary>摘要</summary>
世界的视觉理解不仅仅是图像的 semantics 和平面结构。在这项工作中，我们想要从单目世界视频中捕捉到真实场景的3D结构和动态。我们的动态场景变换模型（DyST）利用了最近的神经场景表示学习来学习单目世界视频的含义，并将其分解为场景内容、每个视角的场景动态和摄像头姿态。这种分解是通过我们新的合作训练方案和我们的新的 sintetic dataset DySO 来实现的。DyST 学习了真实场景的具体隐藏表示，使得可以通过分离摄像头和场景内容来生成视图。
</details></li>
</ul>
<hr>
<h2 id="Divide-and-Conquer-Dynamics-in-AI-Driven-Disempowerment"><a href="#Divide-and-Conquer-Dynamics-in-AI-Driven-Disempowerment" class="headerlink" title="Divide-and-Conquer Dynamics in AI-Driven Disempowerment"></a>Divide-and-Conquer Dynamics in AI-Driven Disempowerment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06009">http://arxiv.org/abs/2310.06009</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peter S. Park, Max Tegmark</li>
<li>for: 研究AI对经济最有价值的工作的模型，以及AI模型如何影响现代艺术家、演员和作家的生活。</li>
<li>methods: 使用游戏理论模型来研究AI模型的分裂和不一致，以及历史记录中AI对人类的影响。</li>
<li>results: 研究预测了AI对未来的威胁，以及现代艺术家、演员和作家的利益相互关系。此外，研究还发现了AI模型的分裂和不一致可能导致更多的人受到AI的影响。<details>
<summary>Abstract</summary>
AI companies are attempting to create AI systems that outperform humans at most economically valuable work. Current AI models are already automating away the livelihoods of some artists, actors, and writers. But there is infighting between those who prioritize current harms and future harms. We construct a game-theoretic model of conflict to study the causes and consequences of this disunity. Our model also helps explain why throughout history, stakeholders sharing a common threat have found it advantageous to unite against it, and why the common threat has in turn found it advantageous to divide and conquer.   Under realistic parameter assumptions, our model makes several predictions that find preliminary corroboration in the historical-empirical record. First, current victims of AI-driven disempowerment need the future victims to realize that their interests are also under serious and imminent threat, so that future victims are incentivized to support current victims in solidarity. Second, the movement against AI-driven disempowerment can become more united, and thereby more likely to prevail, if members believe that their efforts will be successful as opposed to futile. Finally, the movement can better unite and prevail if its members are less myopic. Myopic members prioritize their future well-being less than their present well-being, and are thus disinclined to solidarily support current victims today at personal cost, even if this is necessary to counter the shared threat of AI-driven disempowerment.
</details>
<details>
<summary>摘要</summary>
根据现实的参数假设，我们的模型有以下预测：1. 当前被AI驱逐的人需要将未来受害者理解到，他们的利益也面临严重和即将到来的威胁，以便未来受害者可以在团结的基础上支持当前受害者。2. 反对AI驱逐的运动可以更加团结，并因此更有可能取得胜利，如果成员们认为他们的努力会成功而不是费时。3. 运动成员们更加不偏袋穷，他们将未来的利益优先于当前的利益，因此他们可能不愿意为当前受害者支付个人成本，即使这是必要的，以对抗共同的AI驱逐威胁。
</details></li>
</ul>
<hr>
<h2 id="Grokking-as-Compression-A-Nonlinear-Complexity-Perspective"><a href="#Grokking-as-Compression-A-Nonlinear-Complexity-Perspective" class="headerlink" title="Grokking as Compression: A Nonlinear Complexity Perspective"></a>Grokking as Compression: A Nonlinear Complexity Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05918">http://arxiv.org/abs/2310.05918</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziming Liu, Ziqian Zhong, Max Tegmark</li>
<li>for: 本文研究了神经网络压缩的效果对于准确率的影响，并提出了一种 Linear Mapping Number (LMN) 来衡量神经网络复杂度。</li>
<li>methods: 本文使用了 ReLU 网络和 XOR 网络进行实验研究，并对神经网络压缩后的泛化性进行了分析。</li>
<li>results: 研究发现，LMN 可以准确地描述神经网络压缩前后的泛化性关系，而 $L_2$ norm 则与测试损失之间存在复杂的非线性关系。  Additionally, the paper finds that LMN can be used to explain the phenomenon of “grokking” in neural networks, where generalization is delayed after memorization.<details>
<summary>Abstract</summary>
We attribute grokking, the phenomenon where generalization is much delayed after memorization, to compression. To do so, we define linear mapping number (LMN) to measure network complexity, which is a generalized version of linear region number for ReLU networks. LMN can nicely characterize neural network compression before generalization. Although the $L_2$ norm has been a popular choice for characterizing model complexity, we argue in favor of LMN for a number of reasons: (1) LMN can be naturally interpreted as information/computation, while $L_2$ cannot. (2) In the compression phase, LMN has linear relations with test losses, while $L_2$ is correlated with test losses in a complicated nonlinear way. (3) LMN also reveals an intriguing phenomenon of the XOR network switching between two generalization solutions, while $L_2$ does not. Besides explaining grokking, we argue that LMN is a promising candidate as the neural network version of the Kolmogorov complexity since it explicitly considers local or conditioned linear computations aligned with the nature of modern artificial neural networks.
</details>
<details>
<summary>摘要</summary>
我们将“吸收”现象，即记忆化后延迟普化，归因于压缩。为此，我们定义线性映射数（LMN）来衡量神经网络复杂度，这是ReLU网络的通用化版本。LMN能够nicely characterize神经网络压缩 перед普化。尽管$L_2$norm已经是神经网络复杂度的一个受欢迎选择，但我们认为LMN比$L_2$norm更加适合以下几个理由：（1）LMN可以自然地被理解为信息/计算，而$L_2$norm不能。（2）在压缩阶段，LMN与测试损失之间存在线性关系，而$L_2$norm与测试损失之间存在复杂的非线性关系。（3）LMN还揭示了XOR网络在压缩阶段转换到两个普化解决方案的意外现象，而$L_2$norm不会这样。除了解释吸收现象，我们认为LMN是神经网络版本的科尔莫哈洛夫复杂度，因为它直接考虑了现代人工神经网络中的局部或条件线性计算，与神经网络的自然性相符。
</details></li>
</ul>
<hr>
<h2 id="Interpreting-CLIP’s-Image-Representation-via-Text-Based-Decomposition"><a href="#Interpreting-CLIP’s-Image-Representation-via-Text-Based-Decomposition" class="headerlink" title="Interpreting CLIP’s Image Representation via Text-Based Decomposition"></a>Interpreting CLIP’s Image Representation via Text-Based Decomposition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05916">http://arxiv.org/abs/2310.05916</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yossigandelsman/clip_prs">https://github.com/yossigandelsman/clip_prs</a></li>
<li>paper_authors: Yossi Gandelsman, Alexei A. Efros, Jacob Steinhardt</li>
<li>for: 本研究用CLIP图像编码器来分析各个模型组件如何影响最终表示。</li>
<li>methods: 我们将图像表示分解为图像块、模型层和注意头的和，并使用CLIP的文本表示来解释和评估这些和的组成部分。</li>
<li>results: 我们发现CLIP中的注意头扮演了各种特性特定的角色（例如位置或形状），并发现图像块存在自适应的空间局部化现象。我们使用这些理解来修复和改进CLIP模型，并创建了一个强大的零基础图像分割器。<details>
<summary>Abstract</summary>
We investigate the CLIP image encoder by analyzing how individual model components affect the final representation. We decompose the image representation as a sum across individual image patches, model layers, and attention heads, and use CLIP's text representation to interpret the summands. Interpreting the attention heads, we characterize each head's role by automatically finding text representations that span its output space, which reveals property-specific roles for many heads (e.g. location or shape). Next, interpreting the image patches, we uncover an emergent spatial localization within CLIP. Finally, we use this understanding to remove spurious features from CLIP and to create a strong zero-shot image segmenter. Our results indicate that a scalable understanding of transformer models is attainable and can be used to repair and improve models.
</details>
<details>
<summary>摘要</summary>
我们研究CLIP图像编码器，分析各个模型组件对最终表示的影响。我们将图像表示分解为图像 patches、模型层和注意头的和，使用CLIP的文本表示来解释和Summands。对注意头进行解释，我们自动找到了每个头的输出空间中的文本表示，从而描述了许多头的具体作用（例如，位置或形状）。接着，对图像 patches进行解释，我们发现CLIP中存在自然的空间局部化。最后，我们利用这种理解，将CLIP中的干扰特征除掉，并创建了一个强大的零基本图像分割器。我们的结果表明，可以可靠地理解转换器模型，并使其进行修复和改进。
</details></li>
</ul>
<hr>
<h2 id="FireAct-Toward-Language-Agent-Fine-tuning"><a href="#FireAct-Toward-Language-Agent-Fine-tuning" class="headerlink" title="FireAct: Toward Language Agent Fine-tuning"></a>FireAct: Toward Language Agent Fine-tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05915">http://arxiv.org/abs/2310.05915</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anchen1011/FireAct">https://github.com/anchen1011/FireAct</a></li>
<li>paper_authors: Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik Narasimhan, Shunyu Yao</li>
<li>for: 这篇论文主要探讨了如何通过微小示例技术和外部环境来将语言模型（LMs）训练成能够理解和行动的语言代理。</li>
<li>methods: 本文使用了问题回答（QA）设置和Google搜寻API，探索了不同的基础LMs、提示方法、微小示例和QA任务，发现通过微小示例训练基础LMs可以提高语言代理的性能。</li>
<li>results: 例如，将Llama2-7B微小示例训练500条语言代理访问GPT-4，可以提高HotpotQA性能77%。此外，本文提出了FireAct，一种新的微小示例训练LMs的方法，并显示可以透过更多的多元任务和提示方法来进一步提高代理。<details>
<summary>Abstract</summary>
Recent efforts have augmented language models (LMs) with external tools or environments, leading to the development of language agents that can reason and act. However, most of these agents rely on few-shot prompting techniques with off-the-shelf LMs. In this paper, we investigate and argue for the overlooked direction of fine-tuning LMs to obtain language agents. Using a setup of question answering (QA) with a Google search API, we explore a variety of base LMs, prompting methods, fine-tuning data, and QA tasks, and find language agents are consistently improved after fine-tuning their backbone LMs. For example, fine-tuning Llama2-7B with 500 agent trajectories generated by GPT-4 leads to a 77% HotpotQA performance increase. Furthermore, we propose FireAct, a novel approach to fine-tuning LMs with trajectories from multiple tasks and prompting methods, and show having more diverse fine-tuning data can further improve agents. Along with other findings regarding scaling effects, robustness, generalization, efficiency and cost, our work establishes comprehensive benefits of fine-tuning LMs for agents, and provides an initial set of experimental designs, insights, as well as open questions toward language agent fine-tuning.
</details>
<details>
<summary>摘要</summary>
最近努力已经补充语言模型（LM）以外的工具或环境，导致语言代理人能够思考和行动。然而，大多数这些代理人仍然依赖于几个示例提示技术与商业LM。在这篇论文中，我们调查和论证LM的细化提高语言代理人的方向。使用问答（QA）的Google搜索API设置，我们探索了多种基础LM、提示方法、细化数据和QA任务，并发现在细化基础LM后，语言代理人的性能一直提高。例如，将Llama2-7B细化500个代理人轨迹，生成自GPT-4，可以提高HotpotQA表现77%。此外，我们提出了FireAct，一种新的LM细化方法，使用多个任务和提示方法生成的轨迹，并证明更多的多样化细化数据可以进一步提高代理人。此外，我们还发现了扩展效果、稳定性、普适性、效率和成本等方面的优点。我们的工作证明了细化LM为代理人的全面利好，并提供了初步的实验设计、发现以及未解决问题，为语言代理人细化做出了初步的贡献。
</details></li>
</ul>
<hr>
<h2 id="SALMON-Self-Alignment-with-Principle-Following-Reward-Models"><a href="#SALMON-Self-Alignment-with-Principle-Following-Reward-Models" class="headerlink" title="SALMON: Self-Alignment with Principle-Following Reward Models"></a>SALMON: Self-Alignment with Principle-Following Reward Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05910">http://arxiv.org/abs/2310.05910</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ibm/salmon">https://github.com/ibm/salmon</a></li>
<li>paper_authors: Zhiqing Sun, Yikang Shen, Hongxin Zhang, Qinhong Zhou, Zhenfang Chen, David Cox, Yiming Yang, Chuang Gan</li>
<li>for: 这篇论文旨在探讨如何使用监督式微调（Supervised Fine-Tuning，SFT）与人类反馈学习（Reinforcement Learning from Human Feedback，RLHF）来调整基于自然语言处理（NLP）的语言模型（LLM），以提高其性能和可控性。</li>
<li>methods: 本论文提出了一个新的方法，即Self-ALignMent（SALMON），可以将基于RLHF的LLM调整为符合人类定义的原则，并且只需要小量的人类监督。这个方法中心在一个以原则为基础的赏罚模型，可以根据人类定义的原则生成赏罚分数，并且可以在RL训练过程中调整这些原则，以控制RL训练出来的策略的行为。</li>
<li>results: 在实验中，使用SALMON方法训练了一个名为Dromedary-2的AI助手，并且证明了Dromedary-2可以在多个benchmark数据集上表现出色，比如LLaMA-2-Chat-70b等现有的AI系统。此外，Dromedary-2只需要6个内容学习示例和31个人类定义的原则，而不需要大量的人类监督。<details>
<summary>Abstract</summary>
Supervised Fine-Tuning (SFT) on response demonstrations combined with Reinforcement Learning from Human Feedback (RLHF) constitutes a powerful paradigm for aligning LLM-based AI agents. However, a significant limitation of such an approach is its dependency on high-quality human annotations, making its application to intricate tasks challenging due to difficulties in obtaining consistent response demonstrations and in-distribution response preferences. This paper presents a novel approach, namely SALMON (Self-ALignMent with principle-fOllowiNg reward models), to align base language models with minimal human supervision, using only a small set of human-defined principles, yet achieving superior performance. Central to our approach is a principle-following reward model. Trained on synthetic preference data, this model can generate reward scores based on arbitrary human-defined principles. By merely adjusting these principles during the RL training phase, we gain full control over the preferences with the reward model, subsequently influencing the behavior of the RL-trained policies, and eliminating the reliance on the collection of online human preferences. Applying our method to the LLaMA-2-70b base language model, we developed an AI assistant named Dromedary-2. With only 6 exemplars for in-context learning and 31 human-defined principles, Dromedary-2 significantly surpasses the performance of several state-of-the-art AI systems, including LLaMA-2-Chat-70b, on various benchmark datasets. We have open-sourced the code and model weights to encourage further research into aligning LLM-based AI agents with enhanced supervision efficiency, improved controllability, and scalable oversight.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用监督微调（SFT）和人工智能反馈学习（RLHF）的方法可以帮助基于自然语言处理（NNP）的人工智能代理人（AI）达到更好的性能。然而，这种方法的一个重要限制是它需要高质量的人类监督，这使得在复杂任务上应用困难，因为获得一致的人类响应示例和在线人类响应偏好是困难的。本文提出了一种新的方法，即Self-ALignMent（SALMON），以使基于自然语言处理的AI代理人与最小的人类监督达到更好的性能。SALMON的核心思想是使用原则遵循奖励模型，这种模型可以根据人类定义的原则生成奖励分数。通过在RL训练阶段调整这些原则，我们可以控制奖励模型中的偏好，并且消除在线人类响应的收集 limitation。我们在LLaMA-2-70b基础语言模型上实现了一个名为Dromedary-2的AI助手。只需6个示例和31个人类定义的原则，Dromedary-2可以在多个标准数据集上达到许多现有AI系统的性能水平。我们已经开源了代码和模型参数，以便进一步的研究可以提高LLM-based AI代理人的监督效率、控制性和可扩展性。
</details></li>
</ul>
<hr>
<h2 id="TAIL-Task-specific-Adapters-for-Imitation-Learning-with-Large-Pretrained-Models"><a href="#TAIL-Task-specific-Adapters-for-Imitation-Learning-with-Large-Pretrained-Models" class="headerlink" title="TAIL: Task-specific Adapters for Imitation Learning with Large Pretrained Models"></a>TAIL: Task-specific Adapters for Imitation Learning with Large Pretrained Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05905">http://arxiv.org/abs/2310.05905</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zuxin Liu, Jesse Zhang, Kavosh Asadi, Yao Liu, Ding Zhao, Shoham Sabach, Rasool Fakoor</li>
<li>for: 这个研究是为了解决控制领域中大型预训模型的潜力仍未获得充分利用，主要是因为数据的缺乏和训练或精度化这些大型模型的计算挑战。</li>
<li>methods: 本研究提出了TAIL（任务特定拓展器 для循环学习）框架，用于实现新任务的高效适材化。研究参考了现有的效率 fine-tuning技术，如瓶颈拓展器、P-Tuning和低维拓展（LoRA），以适材化大型预训模型。</li>
<li>results: 实验结果显示，TAIL框架将LoRA与其他效率 fine-tuning技术进行比较，在大量语言条件操作任务中可以实现最好的后适材化性能，仅使用1%的trainable parameter，并避免了遗传性遗传和持续学习设定中的干扰。<details>
<summary>Abstract</summary>
The full potential of large pretrained models remains largely untapped in control domains like robotics. This is mainly because of the scarcity of data and the computational challenges associated with training or fine-tuning these large models for such applications. Prior work mainly emphasizes effective pretraining of large models for decision-making, with little exploration into how to perform data-efficient continual adaptation of these models for new tasks. Recognizing these constraints, we introduce TAIL (Task-specific Adapters for Imitation Learning), a framework for efficient adaptation to new control tasks. Inspired by recent advancements in parameter-efficient fine-tuning in language domains, we explore efficient fine-tuning techniques -- e.g., Bottleneck Adapters, P-Tuning, and Low-Rank Adaptation (LoRA) -- in TAIL to adapt large pretrained models for new tasks with limited demonstration data. Our extensive experiments in large-scale language-conditioned manipulation tasks comparing prevalent parameter-efficient fine-tuning techniques and adaptation baselines suggest that TAIL with LoRA can achieve the best post-adaptation performance with only 1\% of the trainable parameters of full fine-tuning, while avoiding catastrophic forgetting and preserving adaptation plasticity in continual learning settings.
</details>
<details>
<summary>摘要</summary>
大型预训模型的潜在能力在控制领域仍然尚未得到充分利用，主要是因为数据的罕见和训练或细化这些大模型 для这些应用程序所需的计算挑战。先前的工作主要强调有效地预训大模型进行决策，却忽略了如何通过数据有效地适应新任务。Recognizing these constraints, we introduce TAIL (Task-specific Adapters for Imitation Learning), a framework for efficient adaptation to new control tasks. Inspired by recent advancements in parameter-efficient fine-tuning in language domains, we explore efficient fine-tuning techniques -- e.g., Bottleneck Adapters, P-Tuning, and Low-Rank Adaptation (LoRA) -- in TAIL to adapt large pretrained models for new tasks with limited demonstration data. Our extensive experiments in large-scale language-conditioned manipulation tasks comparing prevalent parameter-efficient fine-tuning techniques and adaptation baselines suggest that TAIL with LoRA can achieve the best post-adaptation performance with only 1% of the trainable parameters of full fine-tuning, while avoiding catastrophic forgetting and preserving adaptation plasticity in continual learning settings.
</details></li>
</ul>
<hr>
<h2 id="Lion-Secretly-Solves-Constrained-Optimization-As-Lyapunov-Predicts"><a href="#Lion-Secretly-Solves-Constrained-Optimization-As-Lyapunov-Predicts" class="headerlink" title="Lion Secretly Solves Constrained Optimization: As Lyapunov Predicts"></a>Lion Secretly Solves Constrained Optimization: As Lyapunov Predicts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05898">http://arxiv.org/abs/2310.05898</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lizhang Chen, Bo Liu, Kaizhao Liang, Qiang Liu</li>
<li>for: 本文旨在解释Lion优化器的理论基础。</li>
<li>methods: 本文使用维度分解的权重衰减和紧密链接的梯度下降来解释Lion优化器的动态。</li>
<li>results: 研究发现Lion优化器在训练大型人工智能模型时表现良好，并且比AdamW更具有内存效率。然而，由于Lion不受任何已知的理论支持，因此其可能性和扩展性受限。本文通过连续时间和离散时间分析，解释了Lion优化器在满足约束条件时的理论基础。<details>
<summary>Abstract</summary>
Lion (Evolved Sign Momentum), a new optimizer discovered through program search, has shown promising results in training large AI models. It performs comparably or favorably to AdamW but with greater memory efficiency. As we can expect from the results of a random search program, Lion incorporates elements from several existing algorithms, including signed momentum, decoupled weight decay, Polak, and Nesterov momentum, but does not fit into any existing category of theoretically grounded optimizers. Thus, even though Lion appears to perform well as a general-purpose optimizer for a wide range of tasks, its theoretical basis remains uncertain. This lack of theoretical clarity limits opportunities to further enhance and expand Lion's efficacy.   This work aims to demystify Lion. Based on both continuous-time and discrete-time analysis, we demonstrate that Lion is a theoretically novel and principled approach for minimizing a general loss function $f(x)$ while enforcing a bound constraint $\|x\|_\infty \leq 1/\lambda$. Lion achieves this through the incorporation of decoupled weight decay, where $\lambda$ represents the weight decay coefficient. Our analysis is made possible by the development of a new Lyapunov function for the Lion updates. It applies to a broader family of Lion-$\kappa$ algorithms, where the $\text{sign}(\cdot)$ operator in Lion is replaced by the subgradient of a convex function $\kappa$, leading to the solution of a general composite optimization problem of $\min_x f(x) + \kappa^*(x)$. Our findings provide valuable insights into the dynamics of Lion and pave the way for further improvements and extensions of Lion-related algorithms.
</details>
<details>
<summary>摘要</summary>
狮子（演进签证势），一种新的优化器，通过程序搜索发现，在训练大型人工智能模型时表现了有 promise的结果。它与AdamW相比，具有更高的内存效率。由于狮子包含了多种现有算法元素，包括签证、分离权重衰退、Polak和Nesterov势等，但它不属于任何已知的理论基础上的优化器。因此，尽管狮子在许多任务上表现良好，但其理论基础仍然存在uncertainty。这种不确定性限制了可以进一步提高和扩展狮子的效果的机会。本工作的目标是使狮子更加明了。通过连续时间和离散时间分析，我们证明了狮子是一种理论上有基础的优化器，可以在满足一个约束 $\|x\|_\infty \leq 1/\lambda$ 的情况下将一个通用损失函数 $f(x)$ 的最小值。狮子通过含有分离权重衰退的势 decay，其中 $\lambda$ 表示权重衰退系数。我们的分析基于一个新的Lyapunov函数，可以应用于狮子-$\kappa$ 算法中，其中 $sign(\cdot)$ 操作在狮子中被替换为一个凸函数 $\kappa$ 的极值。这些发现对狮子的动态和 Lion-相关算法的进一步改进和扩展提供了有价值的洞察。
</details></li>
</ul>
<hr>
<h2 id="Streaming-Anchor-Loss-Augmenting-Supervision-with-Temporal-Significance"><a href="#Streaming-Anchor-Loss-Augmenting-Supervision-with-Temporal-Significance" class="headerlink" title="Streaming Anchor Loss: Augmenting Supervision with Temporal Significance"></a>Streaming Anchor Loss: Augmenting Supervision with Temporal Significance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05886">http://arxiv.org/abs/2310.05886</a></li>
<li>repo_url: None</li>
<li>paper_authors: Utkarsh Oggy Sarawgi, John Berkowitz, Vineet Garg, Arnav Kundu, Minsik Cho, Sai Srujana Buddi, Saurabh Adya, Ahmed Tewfik</li>
<li>for: 这个研究是为了提高实时数据流中的语音和感觉信号预测能力，使用流动 нейрон网络模型，并且因应实际应用环境的限制，不能增加更多的模型参数。</li>
<li>methods: 我们提出了一个新的损失函数，即Streaming Anchor Loss（SAL），并且提出了两种专注于重要几帧的方法，即动态地调整frame-wise cross entropy损失函数，以便将更高的损失penalty赋予在semantically critical events的 temporal proximity内的帧。</li>
<li>results: 我们的实验结果显示，使用SAL来训练流动 нейрон网络模型，可以提高预测的精度和延迟时间，无需增加更多的数据或模型参数，并且可以在三个不同的语音检测任务上达到更好的效果。<details>
<summary>Abstract</summary>
Streaming neural network models for fast frame-wise responses to various speech and sensory signals are widely adopted on resource-constrained platforms. Hence, increasing the learning capacity of such streaming models (i.e., by adding more parameters) to improve the predictive power may not be viable for real-world tasks. In this work, we propose a new loss, Streaming Anchor Loss (SAL), to better utilize the given learning capacity by encouraging the model to learn more from essential frames. More specifically, our SAL and its focal variations dynamically modulate the frame-wise cross entropy loss based on the importance of the corresponding frames so that a higher loss penalty is assigned for frames within the temporal proximity of semantically critical events. Therefore, our loss ensures that the model training focuses on predicting the relatively rare but task-relevant frames. Experimental results with standard lightweight convolutional and recurrent streaming networks on three different speech based detection tasks demonstrate that SAL enables the model to learn the overall task more effectively with improved accuracy and latency, without any additional data, model parameters, or architectural changes.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换给定文本到简化中文。<</SYS>>流动神经网络模型在资源受限的平台上广泛应用，以快速响应各种语音和感知信号。因此，增加流动模型学习容量（即添加更多参数）以提高预测力可能不是现实世界任务中可行的。在这种情况下，我们提出了一种新的损失函数——流动锚点损失（SAL），以更好地利用给定的学习容量。具体来说，我们的 SAL 和其关注变种在时间 proximity 上动态调整帧 wise cross entropy 损失，以将更高的损失penalty分配给 semantic 事件附近的帧。因此，我们的损失函数使得模型在预测任务相关帧时更加注重。实验结果表明，使用标准轻量级卷积神经网络和流动神经网络在三种不同的语音检测任务上，SAL 可以使模型更好地学习任务，提高准确率和响应时间，不需要额外数据、模型参数或建模变化。
</details></li>
</ul>
<hr>
<h2 id="A-Meta-Learning-Perspective-on-Transformers-for-Causal-Language-Modeling"><a href="#A-Meta-Learning-Perspective-on-Transformers-for-Causal-Language-Modeling" class="headerlink" title="A Meta-Learning Perspective on Transformers for Causal Language Modeling"></a>A Meta-Learning Perspective on Transformers for Causal Language Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05884">http://arxiv.org/abs/2310.05884</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinbo Wu, Lav R. Varshney</li>
<li>for: 这 paper 的目的是解释Transformer架构在开发大型 causal 语言模型时的能力机制。</li>
<li>methods: 这 paper 使用 meta-learning 视角来解释 Transformer 架构在 causal 语言模型任务上的训练过程，并发现了 Transformer 中的内部优化过程中的一种特殊特征。</li>
<li>results:  experiments 表明，Transformer 架构在 real-world 数据上可以带来优秀的结果，并且该特殊特征可以在 Transformer 中的 token 表示中找到。<details>
<summary>Abstract</summary>
The Transformer architecture has become prominent in developing large causal language models. However, mechanisms to explain its capabilities are not well understood. Focused on the training process, here we establish a meta-learning view of the Transformer architecture when trained for the causal language modeling task, by explicating an inner optimization process that may happen within the Transformer. Further, from within the inner optimization, we discover and theoretically analyze a special characteristic of the norms of learned token representations within Transformer-based causal language models. Our analysis is supported by experiments conducted on pre-trained large language models and real-world data.
</details>
<details>
<summary>摘要</summary>
“transformer架构在大语言模型开发中变得非常知名，但它们的能力运作 Mechanism 仍未得到充分理解。我们专注于训练过程，以 meta-learning 的视角来探索transformer架构在语言模型化任务上的内部优化过程，并从内部优化中发现了transformer 学习的token表现内 norms 特有的一个特性。我们的分析得到了实验证明，并且在实际应用中得到了支持。”Note: Simplified Chinese is used in mainland China and Singapore, while Traditional Chinese is used in Taiwan, Hong Kong, and other regions.
</details></li>
</ul>
<hr>
<h2 id="Coarse-Graining-Hamiltonian-Systems-Using-WSINDy"><a href="#Coarse-Graining-Hamiltonian-Systems-Using-WSINDy" class="headerlink" title="Coarse-Graining Hamiltonian Systems Using WSINDy"></a>Coarse-Graining Hamiltonian Systems Using WSINDy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05879">http://arxiv.org/abs/2310.05879</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel A. Messenger, Joshua W. Burby, David M. Bortz</li>
<li>for: 这个论文的目的是推广弱形式粗化算法（WSINDy）到具有approximate symmetries的哈密顿动力学中，以便高效地捕捉相关度量的动力学。</li>
<li>methods: 这个论文使用的方法是WSINDy算法，它可以在具有approximate symmetries的哈密顿动力学中成功地适应粗化。WSINDy算法在数学上保留哈密顿结构，并且计算高效，通常只需要一个轨迹来学习整个减少后的哈密顿系统。</li>
<li>results: 这个论文的结果表明，WSINDy算法可以在具有approximate symmetries的哈密顿动力学中高效地捕捉相关度量的动力学，并且可以在各种实际应用中提供更高精度的预测。例如，在振荡器动力学、Hénon-Heiles系统和电荷粒子动力学等方面都有physically relevant例子。<details>
<summary>Abstract</summary>
The Weak-form Sparse Identification of Nonlinear Dynamics algorithm (WSINDy) has been demonstrated to offer coarse-graining capabilities in the context of interacting particle systems ( https://doi.org/10.1016/j.physd.2022.133406 ). In this work we extend this capability to the problem of coarse-graining Hamiltonian dynamics which possess approximate symmetries. Such approximate symmetries often lead to the existence of a Hamiltonian system of reduced dimension that may be used to efficiently capture the dynamics of the relevant degrees of freedom. Deriving such reduced systems, or approximating them numerically, is an ongoing challenge. We demonstrate that WSINDy can successfully identify this reduced Hamiltonian system in the presence of large perturbations imparted from both the inexact nature of the symmetry and extrinsic noise. This is significant in part due to the nontrivial means by which such systems are derived analytically. WSINDy naturally preserves the Hamiltonian structure by restricting to a trial basis of Hamiltonian vector fields, and the methodology is computational efficient, often requiring only a single trajectory to learn the full reduced Hamiltonian, and avoiding forward solves in the learning process. In this way, we argue that weak-form equation learning is particularly well-suited for Hamiltonian coarse-graining. Using nearly-periodic Hamiltonian systems as a prototypical class of systems with approximate symmetries, we show that WSINDy robustly identifies the correct leading-order reduced system of dimension $2(N-1)$ or $N$ from the original $(2N)$-dimensional system, upon observation of the relevant degrees of freedom. We provide physically relevant examples, namely coupled oscillator dynamics, the H\'enon-Heiles system for stellar motion within a galaxy, and the dynamics of charged particles.
</details>
<details>
<summary>摘要</summary>
“弱形式简润识别非线性动力学算法（WSINDy）已经在互动粒子系统上显示出简润功能。在这个工作中，我们将这个功能扩展到具有约束的哈密顿动力学问题。这些约束通常导致一个简润的哈密顿系统，可以高效地捕捉相关的动力学度复。 derive这个简润系统或 numerically Approximate它是一个ongoing挑战。我们示出WSINDy可以成功地识别这个简润的哈密顿系统，甚至在大规模的干扰和随机变动下。这是由于WSINDy的方法自然地保留哈密顿结构，通过仅对实验基底的哈密顿 вектор场进行限制。此外，WSINDy的方法具有计算效率高，通常只需要一条轨道来学习全部简润哈密顿，而不需要前向 solves在学习过程中。因此，我们认为弱形式方程式学习特别适合哈密顿简润。使用nearly periodic哈密顿系统作为一个具有约束的系统，我们显示WSINDy可以坚定地识别原始(2N)-维系统中的(2N-1)维或N维简润系统，通过观察相关的动力学度复。我们提供了物理相关的例子，包括相互作用的振荡器动力学、Hénon-Heiles系统 для星系动力学和带电粒子的动力学。”
</details></li>
</ul>
<hr>
<h2 id="AI-Systems-of-Concern"><a href="#AI-Systems-of-Concern" class="headerlink" title="AI Systems of Concern"></a>AI Systems of Concern</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05876">http://arxiv.org/abs/2310.05876</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Kayla Matteucci, Shahar Avin, Fazl Barez, Seán Ó hÉigeartaigh</li>
<li>for: 这篇论文主要是为了讨论高级AI系统中的危险性和控制问题。</li>
<li>methods: 论文使用了多种学术 frameworks和指标来评估高级AI系统中的危险性和控制问题。</li>
<li>results: 论文认为，高级AI系统中的“属性X”特征会导致AI系统的危险性和控制问题，并提出了一些指标和管理措施来评估和限制高级AI系统中的“属性X”特征。<details>
<summary>Abstract</summary>
Concerns around future dangers from advanced AI often centre on systems hypothesised to have intrinsic characteristics such as agent-like behaviour, strategic awareness, and long-range planning. We label this cluster of characteristics as "Property X". Most present AI systems are low in "Property X"; however, in the absence of deliberate steering, current research directions may rapidly lead to the emergence of highly capable AI systems that are also high in "Property X". We argue that "Property X" characteristics are intrinsically dangerous, and when combined with greater capabilities will result in AI systems for which safety and control is difficult to guarantee. Drawing on several scholars' alternative frameworks for possible AI research trajectories, we argue that most of the proposed benefits of advanced AI can be obtained by systems designed to minimise this property. We then propose indicators and governance interventions to identify and limit the development of systems with risky "Property X" characteristics.
</details>
<details>
<summary>摘要</summary>
有些担忧将来的人工智能会带来危险，通常集中在假设存在自我行为、战略意识和远程规划等特性的系统上。我们称这些特性为“X属性”。目前的大多数人工智能系统具有低度的X属性，但在没有干预导航的情况下，当前的研究方向可能会迅速导致高度可能X属性的AI系统的出现。我们认为X属性是危险的，当与更高的能力相结合时，控制和安全难以保证。我们根据一些学者的不同框架，提出了可以通过降低X属性来获得大多数高级人工智能的提antages的可能性。我们还提出了指标和管理措施，以识别和限制开发高风险X属性的系统。
</details></li>
</ul>
<hr>
<h2 id="ViCor-Bridging-Visual-Understanding-and-Commonsense-Reasoning-with-Large-Language-Models"><a href="#ViCor-Bridging-Visual-Understanding-and-Commonsense-Reasoning-with-Large-Language-Models" class="headerlink" title="ViCor: Bridging Visual Understanding and Commonsense Reasoning with Large Language Models"></a>ViCor: Bridging Visual Understanding and Commonsense Reasoning with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05872">http://arxiv.org/abs/2310.05872</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaiwen Zhou, Kwonjoon Lee, Teruhisa Misu, Xin Eric Wang</li>
<li>for: 这个论文的目的是探索预训练的视觉语言模型（VLM）和大语言模型（LLM）在视觉常识逻辑（VCR）中的合作能力。</li>
<li>methods: 这个论文使用了预训练的VLM和LLM来解决视觉常识理解（VCU）和视觉常识推理（VCI）问题。VLM提供图像描述来支持LLM进行推理，并且使用了一种协作策略，让LLM在不确定的推理时指导VLM集中关注相关的视觉元素。</li>
<li>results: 这个论文在两个VCR benchmark数据集上进行了评估，并与没有培 retrained fine-tuning的方法进行比较，得到了更好的性能。<details>
<summary>Abstract</summary>
In our work, we explore the synergistic capabilities of pre-trained vision-and-language models (VLMs) and large language models (LLMs) for visual commonsense reasoning (VCR). We categorize the problem of VCR into visual commonsense understanding (VCU) and visual commonsense inference (VCI). For VCU, which involves perceiving the literal visual content, pre-trained VLMs exhibit strong cross-dataset generalization. On the other hand, in VCI, where the goal is to infer conclusions beyond image content, VLMs face difficulties. We find that a baseline where VLMs provide perception results (image captions) to LLMs leads to improved performance on VCI. However, we identify a challenge with VLMs' passive perception, which often misses crucial context information, leading to incorrect or uncertain reasoning by LLMs. To mitigate this issue, we suggest a collaborative approach where LLMs, when uncertain about their reasoning, actively direct VLMs to concentrate on and gather relevant visual elements to support potential commonsense inferences. In our method, named ViCor, pre-trained LLMs serve as problem classifiers to analyze the problem category, VLM commanders to leverage VLMs differently based on the problem classification, and visual commonsense reasoners to answer the question. VLMs will perform visual recognition and understanding. We evaluate our framework on two VCR benchmark datasets and outperform all other methods that do not require in-domain supervised fine-tuning.
</details>
<details>
<summary>摘要</summary>
在我们的工作中，我们探索预训练的视觉语言模型（VLM）和大型语言模型（LLM）在视觉常识逻辑（VCR）中的共同能力。我们将VCR分为视觉常识理解（VCU）和视觉常识推理（VCI）两个问题。在VCU中，涉及到直接读取图像内容的VCMs表现出了强大的跨数据集泛化能力。然而，在VCI中，VCMs面临困难，因为需要从图像内容中做出更多的推理。我们发现，将VCMs提供视觉内容（图像描述）给LLMs可以提高VCI的性能。然而，我们发现VCMs的被动感知有时会错过重要的上下文信息，导致LLMs的推理错误或不确定。为了解决这个问题，我们提议一种协作方法，其中LLMs在不确定的推理时会活动地指导VCMs集中聚焦和收集相关的视觉元素以支持可能的常识推理。我们称这种方法为ViCor，其中预训练的LLMs serves为问题类别分析器，VCM commander以不同的问题类别来利用VCMs，并且视觉常识推理器来回答问题。VCMs将进行视觉识别和理解。我们对VCR benchmark数据集进行评估，并在不需要域内抽象精细调整的情况下超越所有其他方法。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-value-alignment-through-preference-aggregation-of-multiple-objectives"><a href="#Dynamic-value-alignment-through-preference-aggregation-of-multiple-objectives" class="headerlink" title="Dynamic value alignment through preference aggregation of multiple objectives"></a>Dynamic value alignment through preference aggregation of multiple objectives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05871">http://arxiv.org/abs/2310.05871</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcin Korecki, Damian Dailisan, Cesare Carissimo</li>
<li>for: 这项研究目标是为了开发能够与人类目标相对应的伦理AI系统。</li>
<li>methods: 这种方法使用多目标方法来动态调整价值，以确保RL算法能够同时满足多个目标。</li>
<li>results: 这种方法在简化后的两脚交叉控制系统中进行了应用，并实现了在三个维度（速度、停止和等待时间）的全面性能提高，同时能够有效地 инте格各个目标之间的矛盾。<details>
<summary>Abstract</summary>
The development of ethical AI systems is currently geared toward setting objective functions that align with human objectives. However, finding such functions remains a research challenge, while in RL, setting rewards by hand is a fairly standard approach. We present a methodology for dynamic value alignment, where the values that are to be aligned with are dynamically changing, using a multiple-objective approach. We apply this approach to extend Deep $Q$-Learning to accommodate multiple objectives and evaluate this method on a simplified two-leg intersection controlled by a switching agent.Our approach dynamically accommodates the preferences of drivers on the system and achieves better overall performance across three metrics (speeds, stops, and waits) while integrating objectives that have competing or conflicting actions.
</details>
<details>
<summary>摘要</summary>
现在的人工智能系统开发将注意力集中在设定目标函数，以便与人类目标相互对应。然而，发现这些函数仍然是研究挑战，而在RL中，手动设置优化奖励是一种标准的方法。我们提出了动态值协调的方法，使得需要协调的价值可以随时变化，使用多目标方法。我们将这种方法应用到深度Q学来扩展多目标，并评估这种方法在简化的二脚交汇控制系统上。我们的方法可以动态地考虑驱驶者对系统的偏好，并 achieve better 总性表现（速度、停止和等待时间），并同时考虑了具有竞争或冲突的目标。
</details></li>
</ul>
<hr>
<h2 id="HyperAttention-Long-context-Attention-in-Near-Linear-Time"><a href="#HyperAttention-Long-context-Attention-in-Near-Linear-Time" class="headerlink" title="HyperAttention: Long-context Attention in Near-Linear Time"></a>HyperAttention: Long-context Attention in Near-Linear Time</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05869">http://arxiv.org/abs/2310.05869</a></li>
<li>repo_url: None</li>
<li>paper_authors: Insu Han, Rajesh Jayaram, Amin Karbasi, Vahab Mirrokni, David P. Woodruff, Amir Zandieh</li>
<li>For: 提高Large Language Model（LLM）中长上下文的计算效率。* Methods: 引入两个细化参数，分别测量：（1）折衔列norm在归一化注意力矩阵中，和（2）行norm在归一化注意力矩阵后的大项检测和 removing。使用这两个参数捕捉问题的困难程度。* Results: HyperAttention比 existed方法更快，具有linear time sampling算法，并且可以适应不同的长上下文长度。Empirical experiments表明， HyperAttention在不同的长上下文长度上都具有良好的性能。例如，在32k上下文长度上，HyperAttention可以提高ChatGLM2的推理速度50%，而且只增加了0.7的折衔值。在更大的长上下文长度上，HyperAttention可以提高单层注意力层的速度5倍。<details>
<summary>Abstract</summary>
We present an approximate attention mechanism named HyperAttention to address the computational challenges posed by the growing complexity of long contexts used in Large Language Models (LLMs). Recent work suggests that in the worst-case scenario, quadratic time is necessary unless the entries of the attention matrix are bounded or the matrix has low stable rank. We introduce two parameters which measure: (1) the max column norm in the normalized attention matrix, and (2) the ratio of row norms in the unnormalized attention matrix after detecting and removing large entries. We use these fine-grained parameters to capture the hardness of the problem. Despite previous lower bounds, we are able to achieve a linear time sampling algorithm even when the matrix has unbounded entries or a large stable rank, provided the above parameters are small. HyperAttention features a modular design that easily accommodates integration of other fast low-level implementations, particularly FlashAttention. Empirically, employing Locality Sensitive Hashing (LSH) to identify large entries, HyperAttention outperforms existing methods, giving significant speed improvements compared to state-of-the-art solutions like FlashAttention. We validate the empirical performance of HyperAttention on a variety of different long-context length datasets. For example, HyperAttention makes the inference time of ChatGLM2 50\% faster on 32k context length while perplexity increases from 5.6 to 6.3. On larger context length, e.g., 131k, with causal masking, HyperAttention offers 5-fold speedup on a single attention layer.
</details>
<details>
<summary>摘要</summary>
我们提出了一种近似的注意机制名为HyperAttention，以Addressing the computational challenges posed by the growing complexity of long contexts used in Large Language Models (LLMs). Recent work suggests that in the worst-case scenario, quadratic time is necessary unless the entries of the attention matrix are bounded or the matrix has low stable rank. We introduce two parameters to measure: (1) the maximum column norm in the normalized attention matrix, and (2) the ratio of row norms in the unnormalized attention matrix after detecting and removing large entries. We use these fine-grained parameters to capture the hardness of the problem. Despite previous lower bounds, we are able to achieve a linear time sampling algorithm even when the matrix has unbounded entries or a large stable rank, provided the above parameters are small. HyperAttention features a modular design that easily accommodates integration of other fast low-level implementations, particularly FlashAttention. Empirically, employing Locality Sensitive Hashing (LSH) to identify large entries, HyperAttention outperforms existing methods, giving significant speed improvements compared to state-of-the-art solutions like FlashAttention. We validate the empirical performance of HyperAttention on a variety of different long-context length datasets. For example, HyperAttention makes the inference time of ChatGLM2 50% faster on a context length of 32k while perplexity increases from 5.6 to 6.3. On larger context length, e.g., 131k, with causal masking, HyperAttention offers 5-fold speedup on a single attention layer.
</details></li>
</ul>
<hr>
<h2 id="Generative-quantum-machine-learning-via-denoising-diffusion-probabilistic-models"><a href="#Generative-quantum-machine-learning-via-denoising-diffusion-probabilistic-models" class="headerlink" title="Generative quantum machine learning via denoising diffusion probabilistic models"></a>Generative quantum machine learning via denoising diffusion probabilistic models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05866">http://arxiv.org/abs/2310.05866</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bingzhi Zhang, Peng Xu, Xiaohui Chen, Quntao Zhuang</li>
<li>for: 本文旨在探讨Quantum Denoising Diffusion Probabilistic Models（QuDDPM），它是一种可以有效地学习量子数据的生成学模型。</li>
<li>methods: QuDDPM使用多层环路来保证表达能力，并在训练过程中引入多个中间任务来避免荒漠板和提高训练效率。</li>
<li>results: QuDDPM可以有效地学习相关的量子噪声模型和量子数据的topological结构。<details>
<summary>Abstract</summary>
Deep generative models are key-enabling technology to computer vision, text generation and large language models. Denoising diffusion probabilistic models (DDPMs) have recently gained much attention due to their ability to generate diverse and high-quality samples in many computer vision tasks, as well as to incorporate flexible model architectures and relatively simple training scheme. Quantum generative models, empowered by entanglement and superposition, have brought new insight to learning classical and quantum data. Inspired by the classical counterpart, we propose the quantum denoising diffusion probabilistic models (QuDDPM) to enable efficiently trainable generative learning of quantum data. QuDDPM adopts sufficient layers of circuits to guarantee expressivity, while introduces multiple intermediate training tasks as interpolation between the target distribution and noise to avoid barren plateau and guarantee efficient training. We demonstrate QuDDPM's capability in learning correlated quantum noise model and learning topological structure of nontrivial distribution of quantum data.
</details>
<details>
<summary>摘要</summary>
深度生成模型是计算机视觉、文本生成和大语言模型的关键技术。干扰扩散probabilistic模型（DDPM）在计算机视觉任务中最近受到了广泛关注，因为它们可以生成多样和高质量的样本，同时可以采用灵活的模型架构和简单的训练方案。量子生成模型，受到共聚和超position的 empowerment，为学习классиcu和量子数据提供了新的视角。以类型 counterpart为基础，我们提出了量子干扰扩散probabilistic模型（QuDDPM），以实现高效可训练的生成学习 quantum data。QuDDPM采用了多层环路来保证表达力，同时引入多个中间训练任务作为干扰和静止的插值，以避免恐慌板和保证高效的训练。我们示例了QuDDPM在学习相关的量子噪声模型和学习不规则分布的 topological结构。
</details></li>
</ul>
<hr>
<h2 id="Fine-grained-Audio-Visual-Joint-Representations-for-Multimodal-Large-Language-Models"><a href="#Fine-grained-Audio-Visual-Joint-Representations-for-Multimodal-Large-Language-Models" class="headerlink" title="Fine-grained Audio-Visual Joint Representations for Multimodal Large Language Models"></a>Fine-grained Audio-Visual Joint Representations for Multimodal Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05863">http://arxiv.org/abs/2310.05863</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/briansidp/audiovisualllm">https://github.com/briansidp/audiovisualllm</a></li>
<li>paper_authors: Guangzhi Sun, Wenyi Yu, Changli Tang, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, Chao Zhang</li>
<li>for: 这篇论文旨在扩展文本基础的语言模型，以同时处理音频和视频输入流，以便Language Model（LLM）更好地理解通用视频输入。</li>
<li>methods: 该论文提出了一种名为FAVOR的 audio-visual联合表示学习框架，通过把音频和视频输入流与LLM输入空间进行同步，以实现高精度的音频视频联合表示。具体来说，该框架包括一个 causal Q-Former 结构和一个 causal attention模块，以增强音频视频帧之间的 causal 关系的捕捉。</li>
<li>results: 在AVEB评估准则下，FAVOR实现了与单modal任务相当的性能，并在视频问答任务上达到了20%以上的性能提升。此外，FAVOR还示出了在其他多modal LLVM中缺乏的视频理解和推理能力。<details>
<summary>Abstract</summary>
Audio-visual large language models (LLM) have drawn significant attention, yet the fine-grained combination of both input streams is rather under-explored, which is challenging but necessary for LLMs to understand general video inputs. To this end, a fine-grained audio-visual joint representation (FAVOR) learning framework for multimodal LLMs is proposed in this paper, which extends a text-based LLM to simultaneously perceive speech and audio events in the audio input stream and images or videos in the visual input stream, at the frame level. To fuse the audio and visual feature streams into joint representations and to align the joint space with the LLM input embedding space, we propose a causal Q-Former structure with a causal attention module to enhance the capture of causal relations of the audio-visual frames across time. An audio-visual evaluation benchmark (AVEB) is also proposed which comprises six representative single-modal tasks with five cross-modal tasks reflecting audio-visual co-reasoning abilities. While achieving competitive single-modal performance on audio, speech and image tasks in AVEB, FAVOR achieved over 20% accuracy improvements on the video question-answering task when fine-grained information or temporal causal reasoning is required. FAVOR, in addition, demonstrated remarkable video comprehension and reasoning abilities on tasks that are unprecedented by other multimodal LLMs. An interactive demo of FAVOR is available at https://github.com/BriansIDP/AudioVisualLLM.git, and the training code and model checkpoints will be released soon.
</details>
<details>
<summary>摘要</summary>
大型语音视频语言模型（LLM）已经吸引了广泛的注意，然而将两个输入流进行细腻的组合仍然是一项挑战，这是LLM理解普通视频输入的必要条件。为此，本文提出了一个 audio-visual 共同表示学习框架（FAVOR），该框架将文本语言模型扩展到同时感知语音和视频流的帧级别上，并将语音和视频特征流 fusion 到共同表示。为了将音频和视频特征流与语言模型输入空间进行对应，我们提出了一种 causal Q-Former 结构，并在其中添加了一个 causal 注意模块，以增强捕捉音频视频帧之间的 causal 关系。我们还提出了一个 audio-visual 评价指标（AVEB），该指标包括6种单Modal任务和5种跨Modal任务，旨在测试音频、语音和图像任务中的单Modal性能，以及音频视频之间的相互理解能力。FAVOR在 AVEB 中 achieved 20% 以上的性能提升，特别是在需要细腻信息或时间 causal 逻辑时。此外，FAVOR 还示出了在其他多Modal LLM 未能达到的视频理解和逻辑能力。FAVOR 的交互 demo 可以在 GitHub 上找到（https://github.com/BriansIDP/AudioVisualLLM.git），训练代码和模型检查点将很快地发布。
</details></li>
</ul>
<hr>
<h2 id="Rephrase-Augment-Reason-Visual-Grounding-of-Questions-for-Vision-Language-Models"><a href="#Rephrase-Augment-Reason-Visual-Grounding-of-Questions-for-Vision-Language-Models" class="headerlink" title="Rephrase, Augment, Reason: Visual Grounding of Questions for Vision-Language Models"></a>Rephrase, Augment, Reason: Visual Grounding of Questions for Vision-Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05861">http://arxiv.org/abs/2310.05861</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/archiki/repare">https://github.com/archiki/repare</a></li>
<li>paper_authors: Archiki Prasad, Elias Stengel-Eskin, Mohit Bansal<br>for: 这个研究旨在提高零或几 shot 下的视觉语言任务性能，通过将大型语言模型（LLM）与视觉编码器结合起来，得到大型视觉语言模型（LVLM）。methods: 该研究使用了 gradient-free 框架，名为 RepARe，可以提取图像中核心信息，并通过 LLM 作为描述者和理解者，对原始问题进行修改。results: 研究发现，使用 RepARe 可以提高零或几 shot 下的视觉语言任务性能，在 VQAv2 和 A-OKVQA 两个任务上分别提高了 3.85% 和 6.41%。此外，使用黄金答案作为oracle问题候选选择，可以实现更大的提高。<details>
<summary>Abstract</summary>
An increasing number of vision-language tasks can be handled with little to no training, i.e., in a zero and few-shot manner, by marrying large language models (LLMs) to vision encoders, resulting in large vision-language models (LVLMs). While this has huge upsides, such as not requiring training data or custom architectures, how an input is presented to a LVLM can have a major impact on zero-shot model performance. In particular, inputs phrased in an underspecified way can result in incorrect answers due to factors like missing visual information, complex implicit reasoning, or linguistic ambiguity. Therefore, adding visually grounded information to the input as a preemptive clarification should improve model performance by reducing underspecification, e.g., by localizing objects and disambiguating references. Similarly, in the VQA setting, changing the way questions are framed can make them easier for models to answer. To this end, we present Rephrase, Augment and Reason (RepARe), a gradient-free framework that extracts salient details about the image using the underlying LVLM as a captioner and reasoner, in order to propose modifications to the original question. We then use the LVLM's confidence over a generated answer as an unsupervised scoring function to select the rephrased question most likely to improve zero-shot performance. Focusing on two visual question answering tasks, we show that RepARe can result in a 3.85% (absolute) increase in zero-shot performance on VQAv2 and a 6.41% point increase on A-OKVQA. Additionally, we find that using gold answers for oracle question candidate selection achieves a substantial gain in VQA accuracy by up to 14.41%. Through extensive analysis, we demonstrate that outputs from RepARe increase syntactic complexity, and effectively utilize vision-language interaction and the frozen language model in LVLMs.
</details>
<details>
<summary>摘要</summary>
随着更多的视觉任务可以通过几个或者 zero-shot 方式处理，大量语言模型（LLM）与视觉编码器结合形成大型视觉语言模型（LVLM）。虽然这有着巨大的优点，如不需要训练数据或自定义架构，但输入如何给 LVLM 是非常重要的。特别是，用 underspecified 的方式提交输入可能会导致错误答案，因为缺失视觉信息、复杂的隐式推理或语言抽象。因此，通过添加视觉固定信息来增强输入可以提高模型性能，例如，localizing objects 和解决参考。在 VQA  Setting 中，改变问题的表述方式可以使模型更容易回答。为此，我们提出了 Rephrase、Augment 和 Reason（RepARe）框架，它使用 underlying LVLM 作为captioner和reasoner来提取图像中的精锦信息，并提出修改原始问题。然后，使用 LVLM 对生成的答案的信任度作为无supervised 评分函数来选择修改后的问题。我们在两个视觉问答任务上进行了实验，结果表明，RepARe 可以提高零shot性能，VQAv2 上提高 3.85%（绝对值），A-OKVQA 上提高 6.41% 点。此外，我们发现使用黄金答案作为oracle问题候选者选择可以在 VQA 中提高准确率，最高提高 14.41%。通过广泛的分析，我们证明了 RepARe 输出增加了语法复杂性，并有效地利用了视觉语言交互和冰凉语言模型在 LVLM 中。
</details></li>
</ul>
<hr>
<h2 id="Improving-Summarization-with-Human-Edits"><a href="#Improving-Summarization-with-Human-Edits" class="headerlink" title="Improving Summarization with Human Edits"></a>Improving Summarization with Human Edits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05857">http://arxiv.org/abs/2310.05857</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zonghai Yao, Benjamin J Schloss, Sai P. Selvaraj</li>
<li>for: 这paper主要针对的是如何使用人类反馈来提高自然语言处理模型的质量。</li>
<li>methods: 该paper提出了一种新的技术Sequence Alignment（un）Likelihood Training（SALT），它可以同时使用人类编辑和模型生成的数据来进行训练。此外，paper还提出了一种伪编辑技术来模拟人类编辑数据，以降低人类编辑数据的成本。</li>
<li>results: paper的实验结果表明，SALT可以提高SUMMARY的质量，并且在医学领域SUMMARY中表现更好。此外，paper还比较了SALT与传统的RLHF方法（DPO），发现SALT在使用人类编辑数据时能够表现更好。<details>
<summary>Abstract</summary>
Recent work has shown the promise of learning with human feedback paradigms to produce human-determined high-quality text. Existing works use human feedback to train large language models (LLMs) in general domain abstractive summarization and have obtained summary quality exceeding traditional likelihood training. In this paper, we focus on a less explored form of human feedback -- Human Edits. We propose Sequence Alignment (un)Likelihood Training (SALT), a novel technique to use both the human-edited and model-generated data together in the training loop. In addition, we demonstrate simulating Human Edits with ground truth summaries coming from existing training data -- Imitation edits, along with the model-generated summaries obtained after the training, to reduce the need for expensive human-edit data. In our experiments, we extend human feedback exploration from general domain summarization to medical domain summarization. Our results demonstrate the effectiveness of SALT in improving the summary quality with Human and Imitation Edits. Through additional experiments, we show that SALT outperforms the conventional RLHF method (designed for human preferences) -- DPO, when applied to human-edit data. We hope the evidence in our paper prompts researchers to explore, collect, and better use different human feedback approaches scalably.
</details>
<details>
<summary>摘要</summary>
近期研究表明了使用人类反馈方式进行学习可以生成人决定的高质量文本。现有研究使用人类反馈来训练大语言模型（LLM），并已经获得了传统可能性训练所超越的摘要质量。在这篇论文中，我们关注了一种 menos explored的人类反馈方式---人类编辑。我们提出了序列匹配（不）可能性训练（SALT），一种新的技术，可以在训练循环中结合人类编辑和模型生成的数据。此外，我们还示出了使用现有训练数据中的真实编辑和模型生成的摘要来模拟人类编辑的方法，以降低人类编辑数据的成本。在我们的实验中，我们扩展了人类反馈的探索，从通用领域摘要扩展到医学领域摘要。我们的结果表明SALT可以提高摘要质量，并且在使用人类编辑和伪编辑数据时表现出色。通过额外的实验，我们还证明了SALT在使用人类编辑数据时超过了传统的RLHF方法（设计为人类偏好）——DPO。我们希望这篇论文的证据能够让研究人员更好地探索、收集和利用不同的人类反馈方式，以便在大规模的应用中进行更好的学习。
</details></li>
</ul>
<hr>
<h2 id="GraphLLM-Boosting-Graph-Reasoning-Ability-of-Large-Language-Model"><a href="#GraphLLM-Boosting-Graph-Reasoning-Ability-of-Large-Language-Model" class="headerlink" title="GraphLLM: Boosting Graph Reasoning Ability of Large Language Model"></a>GraphLLM: Boosting Graph Reasoning Ability of Large Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05845">http://arxiv.org/abs/2310.05845</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mistyreed63849/graph-llm">https://github.com/mistyreed63849/graph-llm</a></li>
<li>paper_authors: Ziwei Chai, Tianjie Zhang, Liang Wu, Kaiqiao Han, Xiaohai Hu, Xuanwen Huang, Yang Yang</li>
<li>for: 这篇论文旨在解决大语言模型（LLM）在图数据理解和推理方面的瓶颈问题。</li>
<li>methods: 该论文提出了一种结合图学学习模型和LLM的综合方法，称为GraphLLM，以提高LLM在图数据理解和推理方面的能力。</li>
<li>results: 实验结果表明，GraphLLM可以提高图数据理解和推理的准确率，并将上下文量减少96.45%。<details>
<summary>Abstract</summary>
The advancement of Large Language Models (LLMs) has remarkably pushed the boundaries towards artificial general intelligence (AGI), with their exceptional ability on understanding diverse types of information, including but not limited to images and audio. Despite this progress, a critical gap remains in empowering LLMs to proficiently understand and reason on graph data. Recent studies underscore LLMs' underwhelming performance on fundamental graph reasoning tasks. In this paper, we endeavor to unearth the obstacles that impede LLMs in graph reasoning, pinpointing the common practice of converting graphs into natural language descriptions (Graph2Text) as a fundamental bottleneck. To overcome this impediment, we introduce GraphLLM, a pioneering end-to-end approach that synergistically integrates graph learning models with LLMs. This synergy equips LLMs with the ability to proficiently interpret and reason on graph data, harnessing the superior expressive power of graph learning models. Our empirical evaluations across four fundamental graph reasoning tasks validate the effectiveness of GraphLLM. The results exhibit a substantial average accuracy enhancement of 54.44%, alongside a noteworthy context reduction of 96.45% across various graph reasoning tasks.
</details>
<details>
<summary>摘要</summary>
大语言模型（LLM）的发展有力地推动了人工通用智能（AGI）的前进， LLM 表现出色地理解多种信息类型，包括图像和音频。然而，Graph reasoning 领域中 LLM 的表现仍然不够，特别是在基本的图级 reasoning 任务中。研究发现，这是由于通常将图转换成自然语言描述（Graph2Text）的做法而导致的。为了缓解这个障碍，我们提出了 GraphLLM，一种独特的端到端方法，它 synergistically 结合了图学学习模型和 LLM 。这种结合使得 LLM 能够有效地理解和处理图数据，并且利用图学学习模型的更高表达力。我们在四个基本的图级 reasoning 任务上进行了Empirical 评估，结果表明 GraphLLM 的效果惊人，相对于基eline 方法，GraphLLM 的均值精度提高了54.44%，同时在不同的图级 reasoning 任务中Context 减少了96.45%。
</details></li>
</ul>
<hr>
<h2 id="Predicting-Accident-Severity-An-Analysis-Of-Factors-Affecting-Accident-Severity-Using-Random-Forest-Model"><a href="#Predicting-Accident-Severity-An-Analysis-Of-Factors-Affecting-Accident-Severity-Using-Random-Forest-Model" class="headerlink" title="Predicting Accident Severity: An Analysis Of Factors Affecting Accident Severity Using Random Forest Model"></a>Predicting Accident Severity: An Analysis Of Factors Affecting Accident Severity Using Random Forest Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05840">http://arxiv.org/abs/2310.05840</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adekunle Adefabi, Somtobe Olisah, Callistus Obunadike, Oluwatosin Oyetubo, Esther Taiwo, Edward Tella</li>
<li>for: 预测交通事故严重程度，以采取措施降低交通事故的发生频率。</li>
<li>methods: 使用Random Forest机器学习算法，对大都会区交通事故记录数据进行训练，并对模型进行优化。</li>
<li>results:  Random Forest模型的准确率高于80%，其中最重要的6个变量为风速、气压、湿度、视力、清晰天气和云层覆盖。<details>
<summary>Abstract</summary>
Road accidents have significant economic and societal costs, with a small number of severe accidents accounting for a large portion of these costs. Predicting accident severity can help in the proactive approach to road safety by identifying potential unsafe road conditions and taking well-informed actions to reduce the number of severe accidents. This study investigates the effectiveness of the Random Forest machine learning algorithm for predicting the severity of an accident. The model is trained on a dataset of accident records from a large metropolitan area and evaluated using various metrics. Hyperparameters and feature selection are optimized to improve the model's performance. The results show that the Random Forest model is an effective tool for predicting accident severity with an accuracy of over 80%. The study also identifies the top six most important variables in the model, which include wind speed, pressure, humidity, visibility, clear conditions, and cloud cover. The fitted model has an Area Under the Curve of 80%, a recall of 79.2%, a precision of 97.1%, and an F1 score of 87.3%. These results suggest that the proposed model has higher performance in explaining the target variable, which is the accident severity class. Overall, the study provides evidence that the Random Forest model is a viable and reliable tool for predicting accident severity and can be used to help reduce the number of fatalities and injuries due to road accidents in the United States
</details>
<details>
<summary>摘要</summary>
道路交通事故具有重要的经济和社会成本，一小部分严重事故占据了大部分成本。预测事故严重性可以帮助采取抢险策略，识别可能发生事故的不安全道路情况，采取有知识的行动，以减少严重事故的数量。本研究检查Random Forest机器学习算法是否能够预测事故严重性。模型在一个大都市区的事故记录 dataset 上训练，并使用不同的指标进行评估。Hyperparameters 和特征选择被优化，以提高模型的性能。结果显示，Random Forest 模型可以高效地预测事故严重性，准确率高于 80%。研究还确定了最重要的六个变量，包括风速、压力、湿度、视程、晴天和云层覆盖。已经适应的模型具有折线曲线面积为 80%，回归率为 79.2%，准确率为 97.1%，F1 分数为 87.3%。这些结果表明，提案的模型具有更高的表达能力，可以帮助减少因道路交通事故而导致的死亡和伤害。总的来说，本研究提供了Random Forest模型是可靠和可靠的预测事故严重性工具，可以在美国用于减少道路交通事故的死亡和伤害。
</details></li>
</ul>
<hr>
<h2 id="Learning-Language-guided-Adaptive-Hyper-modality-Representation-for-Multimodal-Sentiment-Analysis"><a href="#Learning-Language-guided-Adaptive-Hyper-modality-Representation-for-Multimodal-Sentiment-Analysis" class="headerlink" title="Learning Language-guided Adaptive Hyper-modality Representation for Multimodal Sentiment Analysis"></a>Learning Language-guided Adaptive Hyper-modality Representation for Multimodal Sentiment Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05804">http://arxiv.org/abs/2310.05804</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Haoyu-ha/ALMT">https://github.com/Haoyu-ha/ALMT</a></li>
<li>paper_authors: Haoyu Zhang, Yu Wang, Guanghao Yin, Kejun Liu, Yuanyuan Liu, Tianshu Yu</li>
<li>for: 提高多模态情感分析（MSA）的性能，增强模态之间的协调和相互启发。</li>
<li>methods: 采用适应语言引导多模态变换（ALMT），包括自适应超模态学习（AHL）模块，从视频和音频特征中学习干扰和冲突抑制表示。</li>
<li>results: 在多个流行数据集（如 MOSI、MOSEI 和 CH-SIMS）上实现状态的表现，并通过多种缺省示例证明了我们的干扰和冲突抑制机制的有效性和必要性。<details>
<summary>Abstract</summary>
Though Multimodal Sentiment Analysis (MSA) proves effective by utilizing rich information from multiple sources (e.g., language, video, and audio), the potential sentiment-irrelevant and conflicting information across modalities may hinder the performance from being further improved. To alleviate this, we present Adaptive Language-guided Multimodal Transformer (ALMT), which incorporates an Adaptive Hyper-modality Learning (AHL) module to learn an irrelevance/conflict-suppressing representation from visual and audio features under the guidance of language features at different scales. With the obtained hyper-modality representation, the model can obtain a complementary and joint representation through multimodal fusion for effective MSA. In practice, ALMT achieves state-of-the-art performance on several popular datasets (e.g., MOSI, MOSEI and CH-SIMS) and an abundance of ablation demonstrates the validity and necessity of our irrelevance/conflict suppression mechanism.
</details>
<details>
<summary>摘要</summary>
这篇文章探讨了多modal sentiment分析（MSA）的问题，MSA可以利用多种资料源（如语言、视频和音频）来分析情感，但是可能会存在不相关或冲突的资料，这可能会妨碍MSA的表现。为了解决这个问题，我们提出了适应语言导向多modal transformer（ALMT），它包括一个适应多模式学习（AHL）模组，可以从视觉和音频特征中学习一个不相关或冲突的表现。这个表现可以与语言特征进行联合表现，从而实现有效的MSA。在实践中，ALMT在多个流行的数据集（如MOSI、MOSEI和CH-SIMS）上 achieve state-of-the-art 表现，并且进行了丰富的ablation 测试，以验证我们的不相关或冲突抑制机制的有效性和必要性。
</details></li>
</ul>
<hr>
<h2 id="Are-Large-Language-Models-Post-Hoc-Explainers"><a href="#Are-Large-Language-Models-Post-Hoc-Explainers" class="headerlink" title="Are Large Language Models Post Hoc Explainers?"></a>Are Large Language Models Post Hoc Explainers?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05797">http://arxiv.org/abs/2310.05797</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/AI4LIFE-GROUP/LLM_Explainer">https://github.com/AI4LIFE-GROUP/LLM_Explainer</a></li>
<li>paper_authors: Nicholas Kroeger, Dan Ley, Satyapriya Krishna, Chirag Agarwal, Himabindu Lakkaraju</li>
<li>for: This paper aims to study the effectiveness of large language models (LLMs) in explaining other predictive models.</li>
<li>methods: The paper proposes a novel framework that utilizes multiple prompting strategies, including perturbation-based ICL, prediction-based ICL, instruction-based ICL, and explanation-based ICL, to generate explanations for other models.</li>
<li>results: The paper demonstrates that LLM-generated explanations perform on par with state-of-the-art post hoc explainers, with an average accuracy of 72.19% in identifying the most important feature.<details>
<summary>Abstract</summary>
Large Language Models (LLMs) are increasingly used as powerful tools for a plethora of natural language processing (NLP) applications. A recent innovation, in-context learning (ICL), enables LLMs to learn new tasks by supplying a few examples in the prompt during inference time, thereby eliminating the need for model fine-tuning. While LLMs have been utilized in several applications, their applicability in explaining the behavior of other models remains relatively unexplored. Despite the growing number of new explanation techniques, many require white-box access to the model and/or are computationally expensive, highlighting a need for next-generation post hoc explainers. In this work, we present the first framework to study the effectiveness of LLMs in explaining other predictive models. More specifically, we propose a novel framework encompassing multiple prompting strategies: i) Perturbation-based ICL, ii) Prediction-based ICL, iii) Instruction-based ICL, and iv) Explanation-based ICL, with varying levels of information about the underlying ML model and the local neighborhood of the test sample. We conduct extensive experiments with real-world benchmark datasets to demonstrate that LLM-generated explanations perform on par with state-of-the-art post hoc explainers using their ability to leverage ICL examples and their internal knowledge in generating model explanations. On average, across four datasets and two ML models, we observe that LLMs identify the most important feature with 72.19% accuracy, opening up new frontiers in explainable artificial intelligence (XAI) to explore LLM-based explanation frameworks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Rethinking-Memory-and-Communication-Cost-for-Efficient-Large-Language-Model-Training"><a href="#Rethinking-Memory-and-Communication-Cost-for-Efficient-Large-Language-Model-Training" class="headerlink" title="Rethinking Memory and Communication Cost for Efficient Large Language Model Training"></a>Rethinking Memory and Communication Cost for Efficient Large Language Model Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06003">http://arxiv.org/abs/2310.06003</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chan Wu, Hanxiao Zhang, Lin Ju, Jinjing Huang, Youshao Xiao, Zhaoxin Huan, Siyuan Li, Fanzhuang Meng, Lei Liang, Xiaolu Zhang, Jun Zhou</li>
<li>for: 本研究旨在提出一种能够均衡内存消耗和通信成本的大语言模型训练策略集Partial Redundancy Optimizer (PaRO)，以提高训练效率。</li>
<li>methods: 本研究使用了细化的分割策略和 Hierarchical Overlapping Ring (HO-Ring) 通信拓扑，以减少内存重复和通信成本，提高训练效率。</li>
<li>results: 实验表明，PaRO 可以提高训练速度，相比 SOTA 方法的 1.19x-2.50x，并实现近线性扩展性。 HO-Ring 算法可以提高通信效率，相比传统的 Ring 算法的 36.5%。<details>
<summary>Abstract</summary>
Recently, various distributed strategies for large language model training have been proposed. However, these methods provided limited solutions for the trade-off between memory consumption and communication cost. In this paper, we rethink the impact of memory consumption and communication costs on the training speed of large language models, and propose a memory-communication balanced strategy set Partial Redundancy Optimizer (PaRO). PaRO provides comprehensive options which reduces the amount and frequency of inter-group communication with minor memory redundancy by fine-grained sharding strategy, thereby improving the training efficiency in various training scenarios. Additionally, we propose a Hierarchical Overlapping Ring (HO-Ring) communication topology to enhance communication efficiency between nodes or across switches in large language model training. Our experiments demonstrate that PaRO significantly improves training throughput by 1.19x-2.50x compared to the SOTA method and achieves a near-linear scalability. The HO-Ring algorithm improves communication efficiency by 36.5% compared to the traditional Ring algorithm.
</details>
<details>
<summary>摘要</summary>
近期，许多分布式策略 для大型自然语言模型训练被提出。然而，这些方法具有限制的解决方案，即内存消耗和通信成本之间的质量协调。在本文中，我们重新思考大型自然语言模型训练中内存消耗和通信成本的影响，并提出了一个内存-通信平衡策略集Partial Redundancy Optimizer（PaRO）。PaRO提供了全面的选项，以减少归并分组通信的数量和频率，并通过细化分组策略减少内存约束，从而提高训练效率在不同的训练场景中。此外，我们提出了层次 overlap 环（HO-Ring）通信架构，以提高在节点或交换机之间的通信效率。我们的实验表明，PaRO可以对比SOTA方法提高训练速度，并实现近似线性扩展性。HO-Ring算法提高了传输效率，相比传统环算法，提高了36.5%。
</details></li>
</ul>
<hr>
<h2 id="DANet-Enhancing-Small-Object-Detection-through-an-Efficient-Deformable-Attention-Network"><a href="#DANet-Enhancing-Small-Object-Detection-through-an-Efficient-Deformable-Attention-Network" class="headerlink" title="DANet: Enhancing Small Object Detection through an Efficient Deformable Attention Network"></a>DANet: Enhancing Small Object Detection through an Efficient Deformable Attention Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05768">http://arxiv.org/abs/2310.05768</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Sohag Mia, Abdullah Al Bary Voban, Abu Bakor Hayat Arnob, Abdu Naim, Md Kawsar Ahmed, Md Shariful Islam</li>
<li>for: 这个论文旨在提高生产环境中小物件检测的效率和精度，以提高产品质量和安全性。</li>
<li>methods: 本论文使用的方法包括嵌入式Pyramid Network，扭转网络，对应适应网络，并且将Convolutional Block Attention Module加入每个基本ResNet50对组。</li>
<li>results: 本论文的模型在NEU-DET和Pascal VOC datasets上的认知性和泛化能力得到了证明，特别是在识别不同类型的钢材损坏时表现出色。<details>
<summary>Abstract</summary>
Efficient and accurate detection of small objects in manufacturing settings, such as defects and cracks, is crucial for ensuring product quality and safety. To address this issue, we proposed a comprehensive strategy by synergizing Faster R-CNN with cutting-edge methods. By combining Faster R-CNN with Feature Pyramid Network, we enable the model to efficiently handle multi-scale features intrinsic to manufacturing environments. Additionally, Deformable Net is used that contorts and conforms to the geometric variations of defects, bringing precision in detecting even the minuscule and complex features. Then, we incorporated an attention mechanism called Convolutional Block Attention Module in each block of our base ResNet50 network to selectively emphasize informative features and suppress less useful ones. After that we incorporated RoI Align, replacing RoI Pooling for finer region-of-interest alignment and finally the integration of Focal Loss effectively handles class imbalance, crucial for rare defect occurrences. The rigorous evaluation of our model on both the NEU-DET and Pascal VOC datasets underscores its robust performance and generalization capabilities. On the NEU-DET dataset, our model exhibited a profound understanding of steel defects, achieving state-of-the-art accuracy in identifying various defects. Simultaneously, when evaluated on the Pascal VOC dataset, our model showcases its ability to detect objects across a wide spectrum of categories within complex and small scenes.
</details>
<details>
<summary>摘要</summary>
efficient和准确的小对象检测在制造环境中是至关重要的，以确保产品质量和安全。为解决这个问题，我们提出了一项涵合策略，将Faster R-CNN与前沿技术相结合。通过将Faster R-CNN与Feature Pyramid Network结合使用，我们让模型能够有效地处理制造环境中的多尺度特征。此外，我们还使用了Deformable Net，它可以根据缺陷的几何变化进行扭形和适应，提高缺陷检测的精度。接着，我们在每个基本ResNet50网络块中添加了Convolutional Block Attention Module，以选择特征中的有用信息，并抑制无用的信息。然后，我们将RoI Align取代RoI Pooling，以实现更细的区域对齐。最后，我们通过集成Focal Loss来有效地处理类偏好，这是对罕见缺陷的检测中非常重要。我们在NEU-DET和Pascal VOC数据集上进行了严格的评估，并证明了我们的模型在不同的环境下具有出色的稳定性和泛化能力。在NEU-DET数据集上，我们的模型对钢铁缺陷进行了深刻的理解，实现了不同缺陷的状况下的最高精度。同时，当我们的模型在Pascal VOC数据集上进行评估时，它展示了对多种类别的对象检测的能力，并在复杂和小的场景中具有出色的检测能力。
</details></li>
</ul>
<hr>
<h2 id="Harmonic-Self-Conditioned-Flow-Matching-for-Multi-Ligand-Docking-and-Binding-Site-Design"><a href="#Harmonic-Self-Conditioned-Flow-Matching-for-Multi-Ligand-Docking-and-Binding-Site-Design" class="headerlink" title="Harmonic Self-Conditioned Flow Matching for Multi-Ligand Docking and Binding Site Design"></a>Harmonic Self-Conditioned Flow Matching for Multi-Ligand Docking and Binding Site Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05764">http://arxiv.org/abs/2310.05764</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hannesstark/flowsite">https://github.com/hannesstark/flowsite</a></li>
<li>paper_authors: Hannes Stärk, Bowen Jing, Regina Barzilay, Tommi Jaakkola</li>
<li>for: 这个论文的目的是设计蛋白质结构域以便与小分子结合。</li>
<li>methods: 论文使用了一种名为HarmonicFlow的改进的生成过程，用于生成3D蛋白质-小分子结合结构。这个过程还可以同时生成蛋白质结构域中的精确残基类型和小分子的结合3D结构。</li>
<li>results: 论文表明，HarmonicFlow在简洁性、通用性和性能方面都超过了现有的生成过程，并且可以设计蛋白质结构域的精确 binding 结构。这种结构模型使得FlowSite可以设计精确的蛋白质结构域，并提供了首个通用的蛋白质结构域设计方法。<details>
<summary>Abstract</summary>
A significant amount of protein function requires binding small molecules, including enzymatic catalysis. As such, designing binding pockets for small molecules has several impactful applications ranging from drug synthesis to energy storage. Towards this goal, we first develop HarmonicFlow, an improved generative process over 3D protein-ligand binding structures based on our self-conditioned flow matching objective. FlowSite extends this flow model to jointly generate a protein pocket's discrete residue types and the molecule's binding 3D structure. We show that HarmonicFlow improves upon the state-of-the-art generative processes for docking in simplicity, generality, and performance. Enabled by this structure modeling, FlowSite designs binding sites substantially better than baseline approaches and provides the first general solution for binding site design.
</details>
<details>
<summary>摘要</summary>
一些蛋白质功能需要与小分子结合，包括enzymatic catalysis。因此，设计小分子结合 pocket 有很多有效的应用，从药物合成到能量储存。为达到这个目标，我们首先开发了 HarmonicFlow，一种改进的生成过程，用于生成3D蛋白质-小分子结合结构。FlowSite 扩展了这种流模型，以同时生成蛋白质口袋中的粒子类型和分子的结合3D结构。我们表明，HarmonicFlow 在简洁性、通用性和性能方面都超越了状态元的生成过程。通过这种结构模型，FlowSite 可以设计结合站点得到substantially better than基线方法，并提供了第一个通用的结合站点设计解决方案。
</details></li>
</ul>
<hr>
<h2 id="Large-Scale-OD-Matrix-Estimation-with-A-Deep-Learning-Method"><a href="#Large-Scale-OD-Matrix-Estimation-with-A-Deep-Learning-Method" class="headerlink" title="Large-Scale OD Matrix Estimation with A Deep Learning Method"></a>Large-Scale OD Matrix Estimation with A Deep Learning Method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05753">http://arxiv.org/abs/2310.05753</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zheli Xiong, Defu Lian, Enhong Chen, Gang Chen, Xiaomin Cheng</li>
<li>for: 实时数据驱动的交通流量统计分析</li>
<li>methods:  combining deep learning和数值优化算法，将嵌入式数据与数据流聚合为数据统计</li>
<li>results: 提供了一个可靠且实时的交通流量统计方法，不dependent on prior information，并且可以减少工程费用<details>
<summary>Abstract</summary>
The estimation of origin-destination (OD) matrices is a crucial aspect of Intelligent Transport Systems (ITS). It involves adjusting an initial OD matrix by regressing the current observations like traffic counts of road sections (e.g., using least squares). However, the OD estimation problem lacks sufficient constraints and is mathematically underdetermined. To alleviate this problem, some researchers incorporate a prior OD matrix as a target in the regression to provide more structural constraints. However, this approach is highly dependent on the existing prior matrix, which may be outdated. Others add structural constraints through sensor data, such as vehicle trajectory and speed, which can reflect more current structural constraints in real-time. Our proposed method integrates deep learning and numerical optimization algorithms to infer matrix structure and guide numerical optimization. This approach combines the advantages of both deep learning and numerical optimization algorithms. The neural network(NN) learns to infer structural constraints from probe traffic flows, eliminating dependence on prior information and providing real-time performance. Additionally, due to the generalization capability of NN, this method is economical in engineering. We conducted tests to demonstrate the good generalization performance of our method on a large-scale synthetic dataset. Subsequently, we verified the stability of our method on real traffic data. Our experiments provided confirmation of the benefits of combining NN and numerical optimization.
</details>
<details>
<summary>摘要</summary>
“OD矩阵估计是智能交通系统（ITS）中的一个重要问题。它需要对初始OD矩阵进行调整，使用最小二乘法 regression 来适应现有的观测数据（例如交通流量资料）。但是，OD估计问题缺乏足够的条件，从数学上是不充分决定的。为了解决这个问题，一些研究人员将 Target OD 矩阵添加到 regression 中，以提供更多的构造约束。但是，这种方法对于现有的 Target OD 矩阵依赖度太高，可能会受到旧有的矩阵影响。另一些研究人员通过感应器数据，如车辆轨迹和速度，添加更多的构造约束。我们的提案方法是通过深度学习和数值优化算法来推导矩阵结构，并将其与数值优化算法结合。这种方法结合了深度学习的优点和数值优化算法的稳定性。对于大规模的 sintetic 数据集，我们的方法具有良好的泛化性。进一步的，我们对真实交通数据进行验证，证明了我们的方法的稳定性和可靠性。我们的实验结果显示，结合深度学习和数值优化算法可以提供更好的性能和经济性。”
</details></li>
</ul>
<hr>
<h2 id="A-Review-of-the-Ethics-of-Artificial-Intelligence-and-its-Applications-in-the-United-States"><a href="#A-Review-of-the-Ethics-of-Artificial-Intelligence-and-its-Applications-in-the-United-States" class="headerlink" title="A Review of the Ethics of Artificial Intelligence and its Applications in the United States"></a>A Review of the Ethics of Artificial Intelligence and its Applications in the United States</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05751">http://arxiv.org/abs/2310.05751</a></li>
<li>repo_url: None</li>
<li>paper_authors: Esther Taiwo, Ahmed Akinsola, Edward Tella, Kolade Makinde, Mayowa Akinwande</li>
<li>For: The paper focuses on the ethical considerations of Artificial Intelligence (AI) in the United States, highlighting its impact on various sectors and entities, and the need for responsible and ethical AI practices.* Methods: The paper explores eleven fundamental ethical principles, including Transparency, Justice, Fairness, Equity, Non-Maleficence, Responsibility, Accountability, Privacy, Beneficence, Freedom, Autonomy, Trust, Dignity, Sustainability, and Solidarity, as a guiding framework for ethical AI development and deployment.* Results: The paper discusses the revolutionary impact of AI applications, such as Machine Learning, and explores various approaches used to implement AI ethics, addressing the growing concerns surrounding the inherent risks associated with the widespread use of AI.<details>
<summary>Abstract</summary>
This study is focused on the ethics of Artificial Intelligence and its application in the United States, the paper highlights the impact AI has in every sector of the US economy and multiple facets of the technological space and the resultant effect on entities spanning businesses, government, academia, and civil society. There is a need for ethical considerations as these entities are beginning to depend on AI for delivering various crucial tasks, which immensely influence their operations, decision-making, and interactions with each other. The adoption of ethical principles, guidelines, and standards of work is therefore required throughout the entire process of AI development, deployment, and usage to ensure responsible and ethical AI practices. Our discussion explores eleven fundamental 'ethical principles' structured as overarching themes. These encompass Transparency, Justice, Fairness, Equity, Non- Maleficence, Responsibility, Accountability, Privacy, Beneficence, Freedom, Autonomy, Trust, Dignity, Sustainability, and Solidarity. These principles collectively serve as a guiding framework, directing the ethical path for the responsible development, deployment, and utilization of artificial intelligence (AI) technologies across diverse sectors and entities within the United States. The paper also discusses the revolutionary impact of AI applications, such as Machine Learning, and explores various approaches used to implement AI ethics. This examination is crucial to address the growing concerns surrounding the inherent risks associated with the widespread use of artificial intelligence.
</details>
<details>
<summary>摘要</summary>
The paper proposes eleven fundamental ethical principles, structured as overarching themes, to guide the ethical development, deployment, and utilization of AI technologies. These principles include:1. Transparency2.  Justice3.  Fairness4.  Equity5.  Non-Maleficence6.  Responsibility7.  Accountability8.  Privacy9.  Beneficence10. Freedom11. Autonomy12. Trust13. Dignity14. Sustainability15. SolidarityThese principles collectively serve as a guiding framework for the ethical development and use of AI technologies in the United States. The paper also discusses the revolutionary impact of AI applications, such as Machine Learning, and explores various approaches used to implement AI ethics. This examination is crucial to address the growing concerns surrounding the inherent risks associated with the widespread use of AI.
</details></li>
</ul>
<hr>
<h2 id="Put-Your-Money-Where-Your-Mouth-Is-Evaluating-Strategic-Planning-and-Execution-of-LLM-Agents-in-an-Auction-Arena"><a href="#Put-Your-Money-Where-Your-Mouth-Is-Evaluating-Strategic-Planning-and-Execution-of-LLM-Agents-in-an-Auction-Arena" class="headerlink" title="Put Your Money Where Your Mouth Is: Evaluating Strategic Planning and Execution of LLM Agents in an Auction Arena"></a>Put Your Money Where Your Mouth Is: Evaluating Strategic Planning and Execution of LLM Agents in an Auction Arena</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05746">http://arxiv.org/abs/2310.05746</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jiangjiechen/auction-arena">https://github.com/jiangjiechen/auction-arena</a></li>
<li>paper_authors: Jiangjie Chen, Siyu Yuan, Rong Ye, Bodhisattwa Prasad Majumder, Kyle Richardson</li>
<li>for: evaluating the ability of Large Language Models (LLMs) to simulate human behavior in complex environments, specifically in auctions.</li>
<li>methods: using a novel simulation environment called AucArena to test the ability of state-of-the-art LLMs as bidding agents in controlled simulations.</li>
<li>results: LLMs demonstrate advanced reasoning skills and ability to manage budget, adhere to long-term goals and priorities, but with considerable variability in capabilities and occasional surpassing by heuristic baselines and human agents, highlighting the potential for further improvements in agent design and the importance of simulation environments for testing and refining agent architectures.<details>
<summary>Abstract</summary>
Can Large Language Models (LLMs) simulate human behavior in complex environments? LLMs have recently been shown to exhibit advanced reasoning skills but much of NLP evaluation still relies on static benchmarks. Answering this requires evaluation environments that probe strategic reasoning in competitive, dynamic scenarios that involve long-term planning. We introduce AucArena, a novel simulation environment for evaluating LLMs within auctions, a setting chosen for being highly unpredictable and involving many skills related to resource and risk management, while also being easy to evaluate. We conduct several controlled simulations using state-of-the-art LLMs as bidding agents. We find that through simple prompting, LLMs do indeed demonstrate many of the skills needed for effectively engaging in auctions (e.g., managing budget, adhering to long-term goals and priorities), skills that we find can be sharpened by explicitly encouraging models to be adaptive and observe strategies in past auctions. These results are significant as they show the potential of using LLM agents to model intricate social dynamics, especially in competitive settings. However, we also observe considerable variability in the capabilities of individual LLMs. Notably, even our most advanced models (GPT-4) are occasionally surpassed by heuristic baselines and human agents, highlighting the potential for further improvements in the design of LLM agents and the important role that our simulation environment can play in further testing and refining agent architectures.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Language-Model-Beats-Diffusion-–-Tokenizer-is-Key-to-Visual-Generation"><a href="#Language-Model-Beats-Diffusion-–-Tokenizer-is-Key-to-Visual-Generation" class="headerlink" title="Language Model Beats Diffusion – Tokenizer is Key to Visual Generation"></a>Language Model Beats Diffusion – Tokenizer is Key to Visual Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05737">http://arxiv.org/abs/2310.05737</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kyegomez/MAGVIT2">https://github.com/kyegomez/MAGVIT2</a></li>
<li>paper_authors: Lijun Yu, José Lezama, Nitesh B. Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander G. Hauptmann, Boqing Gong, Ming-Hsuan Yang, Irfan Essa, David A. Ross, Lu Jiang</li>
<li>for: 用于提高语言模型（LLM）在语言生成任务中表现，并不是 diffusion 模型在图像和视频生成任务中表现好。</li>
<li>methods: 提出了一种名为 MAGVIT-v2 的视频tokenizer，可以生成高效的字符串表示图像和视频，并使用这个tokenizer，LLM 可以在标准图像和视频生成 benchmark 上表现出色。</li>
<li>results: 通过使用 MAGVIT-v2  tokenizer，LLM 可以超越 diffusion 模型在图像和视频生成任务中的表现，同时在视频压缩和动作识别任务中也表现出色。<details>
<summary>Abstract</summary>
While Large Language Models (LLMs) are the dominant models for generative tasks in language, they do not perform as well as diffusion models on image and video generation. To effectively use LLMs for visual generation, one crucial component is the visual tokenizer that maps pixel-space inputs to discrete tokens appropriate for LLM learning. In this paper, we introduce MAGVIT-v2, a video tokenizer designed to generate concise and expressive tokens for both videos and images using a common token vocabulary. Equipped with this new tokenizer, we show that LLMs outperform diffusion models on standard image and video generation benchmarks including ImageNet and Kinetics. In addition, we demonstrate that our tokenizer surpasses the previously top-performing video tokenizer on two more tasks: (1) video compression comparable to the next-generation video codec (VCC) according to human evaluations, and (2) learning effective representations for action recognition tasks.
</details>
<details>
<summary>摘要</summary>
LLMs 是语言生成任务中的主导模型，但它们在图像和视频生成任务中不如扩散模型表现为好。为了有效地使用 LLMs 进行视觉生成，一个关键组件是视觉 токен化器，它将 pixel-space 输入映射到适合 LLM 学习的精炼的 tokens。在这篇论文中，我们介绍了 MAGVIT-v2，一种用于生成简洁和表达力强的 видео和图像 tokens 的视觉 токен化器。我们使用这个新的 токен化器，我们展示了 LLMs 在标准的图像和视频生成 benchmark 上表现出色，并且在两个额外任务上表现出色：（1）与下一代视频编码器（VCC）相当的视频压缩，以及（2）学习有效的动作认知任务。
</details></li>
</ul>
<hr>
<h2 id="The-Program-Testing-Ability-of-Large-Language-Models-for-Code"><a href="#The-Program-Testing-Ability-of-Large-Language-Models-for-Code" class="headerlink" title="The Program Testing Ability of Large Language Models for Code"></a>The Program Testing Ability of Large Language Models for Code</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05727">http://arxiv.org/abs/2310.05727</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020">https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020</a></li>
<li>paper_authors: Weimin Xiong, Yiwen Guo, Hao Chen</li>
<li>for: 这 paper 探讨了大型语言模型（LLMs）在代码测试方面的能力。</li>
<li>methods: 这 paper 使用了一系列的方法来测试 LLMs，包括人工评估和 MBPP 等数据集。</li>
<li>results: 这 paper 显示了 LLMs 在代码测试方面的一些有趣的性质，并通过使用生成的测试用例提高了代码质量，从而提高了代码的执行率。<details>
<summary>Abstract</summary>
Recent development of large language models (LLMs) for code like CodeX and CodeT5+ demonstrates tremendous promise in achieving code intelligence. Their ability of synthesizing code that completes a program for performing a pre-defined task has been intensively tested and verified on benchmark datasets including HumanEval and MBPP. Yet, evaluation of these LLMs from more perspectives (than just program synthesis) is also anticipated, considering their broad scope of applications in software engineering. In this paper, we explore the ability of LLMs for testing programs/code. By performing thorough analyses of recent LLMs for code in program testing, we show a series of intriguing properties of these models and demonstrate how program testing ability of LLMs can be improved. Following recent work which utilizes generated test cases to enhance program synthesis, we further leverage our findings in improving the quality of the synthesized programs and show +11.77% and +4.22% higher code pass rates on HumanEval+ comparing with the GPT-3.5-turbo baseline and the recent state-of-the-art, respectively.
</details>
<details>
<summary>摘要</summary>
In this paper, we explore the ability of LLMs for testing programs/code. We conduct a thorough analysis of recent LLMs for code in program testing and identify several intriguing properties of these models. Furthermore, we demonstrate how the program testing ability of LLMs can be improved, building on recent work that utilizes generated test cases to enhance program synthesis. Our findings lead to a +11.77% and +4.22% increase in code pass rates on HumanEval+ compared to the GPT-3.5-turbo baseline and the recent state-of-the-art, respectively.
</details></li>
</ul>
<hr>
<h2 id="STOPNet-Multiview-based-6-DoF-Suction-Detection-for-Transparent-Objects-on-Production-Lines"><a href="#STOPNet-Multiview-based-6-DoF-Suction-Detection-for-Transparent-Objects-on-Production-Lines" class="headerlink" title="STOPNet: Multiview-based 6-DoF Suction Detection for Transparent Objects on Production Lines"></a>STOPNet: Multiview-based 6-DoF Suction Detection for Transparent Objects on Production Lines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05717">http://arxiv.org/abs/2310.05717</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxuan Kuang, Qin Han, Danshi Li, Qiyu Dai, Lian Ding, Dong Sun, Hanlin Zhao, He Wang</li>
<li>for: 本文提出了一种用于生产线上6DoF物体抓取检测的框架，特别是透明物体，这是机器人系统和现代工业中的重要和困难问题。</li>
<li>methods: 我们提出了一种基于多视图涂抹的新方法，只使用RGB输入，能够重建生产线上的场景，并在真实世界中获得高质量的6DoF抓取姿势。</li>
<li>results: 对于现有方法，我们的方法在实验和实际应用中具有更好的普适性和更高的性能，可满足实际工业的需求。<details>
<summary>Abstract</summary>
In this work, we present STOPNet, a framework for 6-DoF object suction detection on production lines, with a focus on but not limited to transparent objects, which is an important and challenging problem in robotic systems and modern industry. Current methods requiring depth input fail on transparent objects due to depth cameras' deficiency in sensing their geometry, while we proposed a novel framework to reconstruct the scene on the production line depending only on RGB input, based on multiview stereo. Compared to existing works, our method not only reconstructs the whole 3D scene in order to obtain high-quality 6-DoF suction poses in real time but also generalizes to novel environments, novel arrangements and novel objects, including challenging transparent objects, both in simulation and the real world. Extensive experiments in simulation and the real world show that our method significantly surpasses the baselines and has better generalizability, which caters to practical industrial needs.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们介绍了STOPNet，一个用于生产线上6个自由度物体捕捉检测的框架，强调但不限于透明物体，这是现代机器人系统和现代工业中的一个重要和困难的问题。现有的方法，需要深度输入，在透明物体上失败，因为深度摄像头无法感知其几何结构，而我们提出了一种新的框架，基于多视图零点投影，可以在RGB输入基础上重建生产线上的场景，并且可以在实时获得高质量的6个自由度捕捉姿势。与现有的方法相比，我们的方法不仅可以重建整个3D场景，以获得高质量的捕捉姿势，而且可以在新环境、新排序和新物体上普遍，包括实际上的挑战性透明物体，并在实际世界中达到了更好的普遍性。广泛的实验在实际世界和 simulate 中表明，我们的方法在比较基eline上显著超越了基eline，并且具有更好的普遍性，这符合实际工业需求。
</details></li>
</ul>
<hr>
<h2 id="Guiding-Language-Model-Reasoning-with-Planning-Tokens"><a href="#Guiding-Language-Model-Reasoning-with-Planning-Tokens" class="headerlink" title="Guiding Language Model Reasoning with Planning Tokens"></a>Guiding Language Model Reasoning with Planning Tokens</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05707">http://arxiv.org/abs/2310.05707</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyi Wang, Lucas Caccia, Oleksiy Ostapenko, Xingdi Yuan, Alessandro Sordoni</li>
<li>for: 提高大语言模型（LLMs）的复杂逻辑能力，特别是链式思维能力。</li>
<li>methods: 引入”规划符”（planning tokens）作为模型的引导，并在模型参数中微调这些符号表示。</li>
<li>results: 在三个数学问题 datasets 上，与普通的链式思维精心 fine-tuning 基eline 相比，示出了明显的准确性改善。<details>
<summary>Abstract</summary>
Large language models (LLMs) have recently attracted considerable interest for their ability to perform complex reasoning tasks, such as chain-of-thought reasoning. However, most of the existing approaches to enhance this ability rely heavily on data-driven methods, while neglecting the structural aspects of the model's reasoning capacity. We find that while LLMs can manage individual reasoning steps well, they struggle with maintaining consistency across an entire reasoning chain. To solve this, we introduce 'planning tokens' at the start of each reasoning step, serving as a guide for the model. These token embeddings are then fine-tuned along with the rest of the model parameters. Our approach requires a negligible increase in trainable parameters (just 0.001%) and can be applied through either full fine-tuning or a more parameter-efficient scheme. We demonstrate our method's effectiveness by applying it to three different LLMs, showing notable accuracy improvements across three math word problem datasets w.r.t. plain chain-of-thought fine-tuning baselines.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="An-Attribution-Method-for-Siamese-Encoders"><a href="#An-Attribution-Method-for-Siamese-Encoders" class="headerlink" title="An Attribution Method for Siamese Encoders"></a>An Attribution Method for Siamese Encoders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05703">http://arxiv.org/abs/2310.05703</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lucas Möller, Dmitry Nikolaev, Sebastian Padó</li>
<li>for: 本研究旨在探讨siamese encoder模型如sentence transformers（ST）在处理输入时关注哪些方面。</li>
<li>methods: 本研究使用了一种基于integrated gradients的本地 attribute方法，通过将多个输入转化为对应的feature-pair attribute。</li>
<li>results: 研究发现，ST在做出预测时通常只需要关注一些token-pairs，但是为了准确预测，需要关注大多数token和parts of speech。<details>
<summary>Abstract</summary>
Despite the success of Siamese encoder models such as sentence transformers (ST), little is known about the aspects of inputs they pay attention to. A barrier is that their predictions cannot be attributed to individual features, as they compare two inputs rather than processing a single one. This paper derives a local attribution method for Siamese encoders by generalizing the principle of integrated gradients to models with multiple inputs. The solution takes the form of feature-pair attributions, and can be reduced to a token-token matrix for STs. Our method involves the introduction of integrated Jacobians and inherits the advantageous formal properties of integrated gradients: it accounts for the model's full computation graph and is guaranteed to converge to the actual prediction. A pilot study shows that in an ST few token-pairs can often explain large fractions of predictions, and it focuses on nouns and verbs. For accurate predictions, it however needs to attend to the majority of tokens and parts of speech.
</details>
<details>
<summary>摘要</summary>
尽管SIAMESE编码器模型如sentence transformers（ST）在成功的背景下，仍然知之少于其处理输入的方面。一个障碍是它们的预测无法归因于单个特征，因为它们比较两个输入而不是处理单个输入。这篇论文提出了一种本地归因方法 для SIAMESE编码器模型，通过泛化集成导数原理来对多输入模型进行归因。该方法的解释形式为对应对方特征归因，可以将其减少到一个单词单词的矩阵中，并且具有集成导数的优点：它考虑了模型的全部计算图和是确定的归因方法。一项试点研究显示，在ST中，只需要几对单词可以解释大量预测，并且它们主要集中在名词和动词上。然而，为了准确预测，它们需要对大多数单词和部分语法进行注意。
</details></li>
</ul>
<hr>
<h2 id="Based-on-What-We-Can-Control-Artificial-Neural-Networks"><a href="#Based-on-What-We-Can-Control-Artificial-Neural-Networks" class="headerlink" title="Based on What We Can Control Artificial Neural Networks"></a>Based on What We Can Control Artificial Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05692">http://arxiv.org/abs/2310.05692</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Cheng Kang, Xujing Yao</li>
<li>for: 本研究旨在提高人工神经网络（ANNs）的稳定性和效率，通过系统分析方法。</li>
<li>methods: 本研究使用控制系统知识来分析ANNs的系统功能，模拟系统响应。尽管大多数ANNs的复杂性很高，但我们仍然可以分析每个因素（例如优化器、超参数）的系统响应。</li>
<li>results: 本研究提出了一种新的分析方法，可以帮助确保ANNs的学习过程的稳定性和效率。这种方法还可能对新的优化器和学习系统的开发产生影响，特别是当找出哪些组件对ANNs产生负面影响时。请参考：\url{<a target="_blank" rel="noopener" href="https://github.com/RandomUserName2023/Control-ANNs%7D%E3%80%82">https://github.com/RandomUserName2023/Control-ANNs}。</a><details>
<summary>Abstract</summary>
How can the stability and efficiency of Artificial Neural Networks (ANNs) be ensured through a systematic analysis method? This paper seeks to address that query. While numerous factors can influence the learning process of ANNs, utilizing knowledge from control systems allows us to analyze its system function and simulate system responses. Although the complexity of most ANNs is extremely high, we still can analyze each factor (e.g., optimiser, hyperparameters) by simulating their system response. This new method also can potentially benefit the development of new optimiser and learning system, especially when discerning which components adversely affect ANNs. Controlling ANNs can benefit from the design of optimiser and learning system, as (1) all optimisers act as controllers, (2) all learning systems operate as control systems with inputs and outputs, and (3) the optimiser should match the learning system. Please find codes: \url{https://github.com/RandomUserName2023/Control-ANNs}.
</details>
<details>
<summary>摘要</summary>
如何确保人工神经网络（ANNs）的稳定性和效率？这篇论文旨在回答这个问题。虽然多种因素可能影响 ANNs 的学习过程，但通过知识控制系统来分析其系统功能并模拟系统响应。虽然大多数 ANNs 的复杂度很高，但我们仍可以分析每个因素（例如优化器、超参数） by 模拟它们的系统响应。这种新方法还可能为 ANNs 的发展提供新的优化器和学习系统，特别是当探测那些组件对 ANNs 产生负面影响时。控制 ANNs 可以从优化器和学习系统的设计中受益，因为（1）所有优化器都是控制器，（2）所有学习系统都是控制系统，（3）优化器应该与学习系统匹配。请找到代码：https://github.com/RandomUserName2023/Control-ANNs。
</details></li>
</ul>
<hr>
<h2 id="Abstractive-Summarization-of-Large-Document-Collections-Using-GPT"><a href="#Abstractive-Summarization-of-Large-Document-Collections-Using-GPT" class="headerlink" title="Abstractive Summarization of Large Document Collections Using GPT"></a>Abstractive Summarization of Large Document Collections Using GPT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05690">http://arxiv.org/abs/2310.05690</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sengjie Liu, Christopher G. Healey</li>
<li>for: 本研究提出了一种可扩展到文档收集的抽象概要方法。</li>
<li>methods: 该方法使用了 semantic clustering、文档内部话题减少、semantic chunking、GPT基于概要和 concatenation 等方法。</li>
<li>results: 对比 exist 状态的 art 系统 BART、BRIO、PEGASUS 和 MoCa 的 ROGUE 摘要分数，本研究在 CNN&#x2F;Daily Mail 测试集上与 BART 和 PEGASUS 相当，在 Gigaword 测试集上与 BART 相当。<details>
<summary>Abstract</summary>
This paper proposes a method of abstractive summarization designed to scale to document collections instead of individual documents. Our approach applies a combination of semantic clustering, document size reduction within topic clusters, semantic chunking of a cluster's documents, GPT-based summarization and concatenation, and a combined sentiment and text visualization of each topic to support exploratory data analysis. Statistical comparison of our results to existing state-of-the-art systems BART, BRIO, PEGASUS, and MoCa using ROGUE summary scores showed statistically equivalent performance with BART and PEGASUS on the CNN/Daily Mail test dataset, and with BART on the Gigaword test dataset. This finding is promising since we view document collection summarization as more challenging than individual document summarization. We conclude with a discussion of how issues of scale are
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种抽象摘要方法，旨在对文档集合进行摘要而不是单个文档。我们的方法使用了 semantics 归一化、文档内容减少、semantic 块分割、基于 GPT 的摘要和 concatenation，以及每个话题的感情和文本视觉表示。我们通过对 ROGUE 摘要分数进行统计比较，与现有的状态 искус数据集 BART、BRIO、PEGASUS 和 MoCa 进行比较，在 CNN/Daily Mail 测试集上与 BART 和 PEGASUS 相当，在 Gigaword 测试集上与 BART 相当。这一结果是有前途的，因为我们视文档集合摘要为单个文档摘要更加困难。我们在结尾采用了一些问题的扩展和未来工作的讨论。
</details></li>
</ul>
<hr>
<h2 id="The-potential-of-large-language-models-for-improving-probability-learning-A-study-on-ChatGPT3-5-and-first-year-computer-engineering-students"><a href="#The-potential-of-large-language-models-for-improving-probability-learning-A-study-on-ChatGPT3-5-and-first-year-computer-engineering-students" class="headerlink" title="The potential of large language models for improving probability learning: A study on ChatGPT3.5 and first-year computer engineering students"></a>The potential of large language models for improving probability learning: A study on ChatGPT3.5 and first-year computer engineering students</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05686">http://arxiv.org/abs/2310.05686</a></li>
<li>repo_url: None</li>
<li>paper_authors: Angel Udias, Antonio Alonso-Ayuso, Ignacio Sanchez, Sonia Hernandez, Maria Eugenia Castellanos, Raquel Montes Diez, Emilio Lopez Cano</li>
<li>For: The paper assesses the efficacy of ChatGPT in solving probability problems typically presented in introductory computer engineering exams.* Methods: The study uses a set of 23 probability exercises administered to students at Rey Juan Carlos University (URJC) in Madrid, and evaluates the responses produced by ChatGPT qualitatively, assigning grades based on the same criteria used for students.* Results: The results indicate that ChatGPT surpasses the average student in terms of phrasing, organization, and logical reasoning, and the model’s performance remained consistent for both the Spanish and English versions of the exercises. However, ChatGPT encountered difficulties in executing basic numerical operations, which were overcome by requesting the solution in the form of an R script.<details>
<summary>Abstract</summary>
In this paper, we assess the efficacy of ChatGPT (version Feb 2023), a large-scale language model, in solving probability problems typically presented in introductory computer engineering exams. Our study comprised a set of 23 probability exercises administered to students at Rey Juan Carlos University (URJC) in Madrid. The responses produced by ChatGPT were evaluated by a group of five statistics professors, who assessed them qualitatively and assigned grades based on the same criteria used for students. Our results indicate that ChatGPT surpasses the average student in terms of phrasing, organization, and logical reasoning. The model's performance remained consistent for both the Spanish and English versions of the exercises. However, ChatGPT encountered difficulties in executing basic numerical operations. Our experiments demonstrate that requesting ChatGPT to provide the solution in the form of an R script proved to be an effective approach for overcoming these limitations. In summary, our results indicate that ChatGPT surpasses the average student in solving probability problems commonly presented in introductory computer engineering exams. Nonetheless, the model exhibits limitations in reasoning around certain probability concepts. The model's ability to deliver high-quality explanations and illustrate solutions in any programming language, coupled with its performance in solving probability exercises, suggests that large language models have the potential to serve as learning assistants.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们评估了ChatGPT（版本为2月2023）大型语言模型在解probability问题方面的效果。我们的研究包括23个probability问题，对于马德里 Rey Juan Carlos大学（URJC）的学生进行了测试。ChatGPT的答案由5名统计教授评估，他们根据同样的标准评分学生的答案。我们的结果表明，ChatGPT在表达、组织和逻辑推理方面超过了学生的平均水平。模型在西班牙语和英语版probability问题上表现一致。然而，ChatGPT在基本数学运算方面遇到了困难。我们的实验表明，向ChatGPT请求提供解决方案的R脚本形式是一种有效的方法，以超越这些限制。总之，我们的结果表明，ChatGPT在入门计算机工程考试中常见的probability问题方面表现出色，但模型在某些概率概念上存在限制。模型能够提供高质量的解释和在任何编程语言中示例解决方案，表明大语言模型有可能作为学习助手。
</details></li>
</ul>
<hr>
<h2 id="Automated-Argument-Generation-from-Legal-Facts"><a href="#Automated-Argument-Generation-from-Legal-Facts" class="headerlink" title="Automated Argument Generation from Legal Facts"></a>Automated Argument Generation from Legal Facts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05680">http://arxiv.org/abs/2310.05680</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oscar Tuvey, Procheta Sen</li>
<li>For: The paper aims to enhance the efficiency and speed of legal procedures by utilizing AI technology to help legal professionals analyze legal cases.* Methods: The paper uses open-sourced large language models to create arguments derived from the facts present in legal cases.* Results: The generated arguments from the best performing method have on average 63% overlap with the benchmark set gold standard annotations.Here are the three key points in Simplified Chinese text:* For: 这项研究旨在利用人工智能技术，提高法律程序的效率和速度。* Methods: 这项研究使用开源大型自然语言模型，生成法律案例中的事实所基于的Arguments。* Results: 最佳方法生成的Arguments在基本标准注释中的重合率平均为63%。<details>
<summary>Abstract</summary>
The count of pending cases has shown an exponential rise across nations (e.g., with more than 10 million pending cases in India alone). The main issue lies in the fact that the number of cases submitted to the law system is far greater than the available number of legal professionals present in a country. Given this worldwide context, the utilization of AI technology has gained paramount importance to enhance the efficiency and speed of legal procedures. In this study we partcularly focus on helping legal professionals in the process of analyzing a legal case. Our specific investigation delves into harnessing the generative capabilities of open-sourced large language models to create arguments derived from the facts present in legal cases. Experimental results show that the generated arguments from the best performing method have on average 63% overlap with the benchmark set gold standard annotations.
</details>
<details>
<summary>摘要</summary>
全球各国案件数量在急增（例如印度单独已经有超过1000万个案件）。主要问题在于法律系统内的案件数量比法律专业人员的数量更多。视这种全球背景，利用人工智能技术已成为提高法律程序效率和速度的重要手段。本研究专注于帮助法律专业人员分析法律案件。我们的特定调查是利用开源大型自然语言模型的生成能力来从法律案件中生成基于事实的法律Arguments。实验结果显示，最佳方法生成的Arguments的平均 overlap率为63%。
</details></li>
</ul>
<hr>
<h2 id="Making-Scalable-Meta-Learning-Practical"><a href="#Making-Scalable-Meta-Learning-Practical" class="headerlink" title="Making Scalable Meta Learning Practical"></a>Making Scalable Meta Learning Practical</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05674">http://arxiv.org/abs/2310.05674</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/leopard-ai/betty">https://github.com/leopard-ai/betty</a></li>
<li>paper_authors: Sang Keun Choe, Sanket Vaibhav Mehta, Hwijeen Ahn, Willie Neiswanger, Pengtao Xie, Emma Strubell, Eric Xing</li>
<li>for: 本研究旨在实现可扩展的元学习实现，提高元学习的可扩展性和可行性。</li>
<li>methods: 本研究使用SAMA算法，其combines implicit differentiation algorithms和系统，以提高元学习的可扩展性和可行性。SAMA支持广泛的适应性optimizers，并通过避免显式计算第二阶导数信息，以及利用高效的分布式训练技术，提高元学习的可扩展性和可行性。</li>
<li>results: 在多个大规模元学习benchmark上测试，SAMA比基eline元学习算法具有1.7&#x2F;4.8倍的 Throughput和2.0&#x2F;3.8倍的内存占用率，单&#x2F;多GPU集成。此外，SAMA在BERT和RoBERTa大语言模型中进行文本分类任务中，以及在图像分类任务中进行数据优化，均实现了顺利的改进，并达到了小规模和大规模数据剪辑的状态机器。<details>
<summary>Abstract</summary>
Despite its flexibility to learn diverse inductive biases in machine learning programs, meta learning (i.e., learning to learn) has long been recognized to suffer from poor scalability due to its tremendous compute/memory costs, training instability, and a lack of efficient distributed training support. In this work, we focus on making scalable meta learning practical by introducing SAMA, which combines advances in both implicit differentiation algorithms and systems. Specifically, SAMA is designed to flexibly support a broad range of adaptive optimizers in the base level of meta learning programs, while reducing computational burden by avoiding explicit computation of second-order gradient information, and exploiting efficient distributed training techniques implemented for first-order gradients. Evaluated on multiple large-scale meta learning benchmarks, SAMA showcases up to 1.7/4.8x increase in throughput and 2.0/3.8x decrease in memory consumption respectively on single-/multi-GPU setups compared to other baseline meta learning algorithms. Furthermore, we show that SAMA-based data optimization leads to consistent improvements in text classification accuracy with BERT and RoBERTa large language models, and achieves state-of-the-art results in both small- and large-scale data pruning on image classification tasks, demonstrating the practical applicability of scalable meta learning across language and vision domains.
</details>
<details>
<summary>摘要</summary>
尽管机器学习中的元学习（即学习学习）具有学习多种启发的灵活性，但长期以来，元学习受到了计算/存储成本过高、训练不稳定和分布式训练支持不充分的问题困扰。在这项工作中，我们关注使得元学习可扩展的实用性，通过引入SAMA来实现。SAMA组合了隐式 diferentiation算法和系统技术，特别是在基础级元学习程序中支持广泛的适应化优化器，同时减少计算负担，避免直接计算第二阶导数信息，并利用高效的分布式训练技术实现。在多个大规模元学习标准 benchmark 上评估，SAMA显示在单/多GPU设置下具有1.7/4.8倍的throughput和2.0/3.8倍的内存占用量，相比其他基eline元学习算法。此外，我们表明SAMA可以在语言和视觉领域中实现可扩展的数据优化，并在BERT和RoBERTa大语言模型中表现出了一致的提升，并在图像分类任务中实现了小规模和大规模数据减少的状态环境。
</details></li>
</ul>
<hr>
<h2 id="Reinforcement-learning-for-freeform-robot-design"><a href="#Reinforcement-learning-for-freeform-robot-design" class="headerlink" title="Reinforcement learning for freeform robot design"></a>Reinforcement learning for freeform robot design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05670">http://arxiv.org/abs/2310.05670</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhan Li, David Matthews, Sam Kriegman</li>
<li>for: 探讨了用Policy Gradients来设计自由形机器人的方法</li>
<li>methods: 使用动作将atomic building block bundle归并或移除，形成非parametric macrostructure如肢体、器官和腔体</li>
<li>results: 实现了开 loop控制，未来可以适应closed loop控制和 sim2real转移到物理机器人<details>
<summary>Abstract</summary>
Inspired by the necessity of morphological adaptation in animals, a growing body of work has attempted to expand robot training to encompass physical aspects of a robot's design. However, reinforcement learning methods capable of optimizing the 3D morphology of a robot have been restricted to reorienting or resizing the limbs of a predetermined and static topological genus. Here we show policy gradients for designing freeform robots with arbitrary external and internal structure. This is achieved through actions that deposit or remove bundles of atomic building blocks to form higher-level nonparametric macrostructures such as appendages, organs and cavities. Although results are provided for open loop control only, we discuss how this method could be adapted for closed loop control and sim2real transfer to physical machines in future.
</details>
<details>
<summary>摘要</summary>
受动物形态适应的需要启发，一组增长的研究尝试将机器人训练扩展到物理机器人设计的方面。然而，使用奖励学习方法优化3D机器人形态的方法一直受限于重定向或缩放预先预定的顺序型机器人的臂部。我们现在显示了一种使用政策偏好来设计自由形机器人，这种方法通过执行填充或 removing 粒子堆集来形成高级非参数 macrostructure，如肢体、器官和腔体。虽然我们只提供了开loop控制的结果，但我们讨论了如何将这种方法适应到closed loop控制和 sim2real 转移到物理机器人的未来。
</details></li>
</ul>
<hr>
<h2 id="ViTs-are-Everywhere-A-Comprehensive-Study-Showcasing-Vision-Transformers-in-Different-Domain"><a href="#ViTs-are-Everywhere-A-Comprehensive-Study-Showcasing-Vision-Transformers-in-Different-Domain" class="headerlink" title="ViTs are Everywhere: A Comprehensive Study Showcasing Vision Transformers in Different Domain"></a>ViTs are Everywhere: A Comprehensive Study Showcasing Vision Transformers in Different Domain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05664">http://arxiv.org/abs/2310.05664</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Sohag Mia, Abu Bakor Hayat Arnob, Abdu Naim, Abdullah Al Bary Voban, Md Shariful Islam</li>
<li>for: 这篇论文主要关注于Computer Vision（CV）领域中的Transformer设计，以及它在不同CV应用中的表现。</li>
<li>methods: 这篇论文使用了多种Vision Transformer（ViT）模型，并对它们进行了分类和比较，以找出它们在不同CV应用中的优势和缺陷。</li>
<li>results: 研究发现，ViTs在多种CV应用中表现出优于Convolutional Neural Networks（CNNs），包括图像分类、物体识别、图像 segmentation、视频变换、图像净化和NAS等。同时，研究还提出了许多未解决的问题和潜在的研究机会。<details>
<summary>Abstract</summary>
Transformer design is the de facto standard for natural language processing tasks. The success of the transformer design in natural language processing has lately piqued the interest of researchers in the domain of computer vision. When compared to Convolutional Neural Networks (CNNs), Vision Transformers (ViTs) are becoming more popular and dominant solutions for many vision problems. Transformer-based models outperform other types of networks, such as convolutional and recurrent neural networks, in a range of visual benchmarks. We evaluate various vision transformer models in this work by dividing them into distinct jobs and examining their benefits and drawbacks. ViTs can overcome several possible difficulties with convolutional neural networks (CNNs). The goal of this survey is to show the first use of ViTs in CV. In the first phase, we categorize various CV applications where ViTs are appropriate. Image classification, object identification, image segmentation, video transformer, image denoising, and NAS are all CV applications. Our next step will be to analyze the state-of-the-art in each area and identify the models that are currently available. In addition, we outline numerous open research difficulties as well as prospective research possibilities.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>变换器设计已经成为自然语言处理任务的逻辑标准。随着变换器设计在自然语言处理领域的成功，研究人员开始关注这种设计在计算机视觉领域的应用。与卷积神经网络（CNN）相比，视力变换器（ViT）在许多视觉问题上变得更加受欢迎和主导性。基于变换器的模型在各种视觉标准上表现出色，超过了卷积神经网络和回归神经网络的性能。在这项工作中，我们将对不同的视觉变换器模型进行分类和分析，描述其优缺点。ViT可以超越卷积神经网络的一些可能的困难。本文的目标是在计算机视觉领域内，首次使用ViT。在第一个阶段，我们将分类各种适用于计算机视觉应用的CV应用程序。包括图像分类、物体识别、图像分割、视频变换、图像净化和NAS等。接下来，我们将分析每个领域的现状，并识别目前可用的模型。此外，我们还将列出许多开放的研究Difficulties和前景。
</details></li>
</ul>
<hr>
<h2 id="Causal-structure-learning-with-momentum-Sampling-distributions-over-Markov-Equivalence-Classes-of-DAGs"><a href="#Causal-structure-learning-with-momentum-Sampling-distributions-over-Markov-Equivalence-Classes-of-DAGs" class="headerlink" title="Causal structure learning with momentum: Sampling distributions over Markov Equivalence Classes of DAGs"></a>Causal structure learning with momentum: Sampling distributions over Markov Equivalence Classes of DAGs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05655">http://arxiv.org/abs/2310.05655</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mschauer/CausalInference.jl">https://github.com/mschauer/CausalInference.jl</a></li>
<li>paper_authors: Moritz Schauer, Marcel Wienöbst</li>
<li>for: INFERRING BAYESIAN NETWORK STRUCTURE (DIRECTED Acyclic Graph, DAG FOR SHORT)</li>
<li>methods: NON-REVERSIBLE CONTINUOUS TIME MARKOV CHAIN (CAUSAL ZIG-ZAG SAMPLER) TARGETING A PROBABILITY DISTRIBUTION OVER CLASSES OF OBSERVATIONALLY EQUIVALENT (MARKOV EQUIVALENT) DAGs</li>
<li>results: IMPROVED MIXING COMPARED TO STATE-OF-THE-ART IMPLEMENTATIONS USING GREEDY EQUIVALENCE SEARCH (GES) OPERATORS WITH A MOMENTUM VARIABLE, AND EFFICIENT IMPLEMENTATION OF LISTING, COUNTING, UNIFORMLY SAMPLING, AND APPLYING POSSIBLE MOVES OF GES OPERATORS.<details>
<summary>Abstract</summary>
In the context of inferring a Bayesian network structure (directed acyclic graph, DAG for short), we devise a non-reversible continuous time Markov chain, the "Causal Zig-Zag sampler", that targets a probability distribution over classes of observationally equivalent (Markov equivalent) DAGs. The classes are represented as completed partially directed acyclic graphs (CPDAGs). The non-reversible Markov chain relies on the operators used in Chickering's Greedy Equivalence Search (GES) and is endowed with a momentum variable, which improves mixing significantly as we show empirically. The possible target distributions include posterior distributions based on a prior over DAGs and a Markov equivalent likelihood. We offer an efficient implementation wherein we develop new algorithms for listing, counting, uniformly sampling, and applying possible moves of the GES operators, all of which significantly improve upon the state-of-the-art.
</details>
<details>
<summary>摘要</summary>
在推断 bayesian 网络结构（直接循环图，dag 简称）方面，我们设计了一种不可逆的连续时间马尔可夫链，称为“ causal zig-zag sampler”，该链targeted一个观察可Equivalence classes of observationally (Markov equivalent) DAGs的概率分布。这些类型被表示为完善的部分导向循环图（CPDAGs）。不可逆的马尔可夫链利用GES操作符，并具有一个势量变量，这使得混合得到了显著改善，我们在实验中验证了这一点。 possible target distributions include posterior distributions based on a prior over DAGs and a Markov equivalent likelihood。我们提供了高效的实现，其中包括开发了新的列表、计数、均匀采样和可能的移动操作算法，这些算法都有显著改善了现有状态的。
</details></li>
</ul>
<hr>
<h2 id="No-Token-Left-Behind-Efficient-Vision-Transformer-via-Dynamic-Token-Idling"><a href="#No-Token-Left-Behind-Efficient-Vision-Transformer-via-Dynamic-Token-Idling" class="headerlink" title="No Token Left Behind: Efficient Vision Transformer via Dynamic Token Idling"></a>No Token Left Behind: Efficient Vision Transformer via Dynamic Token Idling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05654">http://arxiv.org/abs/2310.05654</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuwei Xu, Changlin Li, Yudong Chen, Xiaojun Chang, Jiajun Liu, Sen Wang</li>
<li>For: 这个论文目的是提出一种名为IdleViT的动态token遮瑕方法，以提高运算效率和表现力。* Methods: 这个方法选择每层中的一部分图像token参与计算，并将其他token直接传递到下一层的输出中。这样可以避免在早期阶段 improvident pruning 导致的 permanent loss of image information。此外，这个方法还使用了 normalized graph cut 的内置数据库损失来改善图像选择能力。* Results: 实验结果显示，IdleViT 可以将预训 ViT 的复杂度降低到 33%，而且只需要调整 30 次 epoch。此外，当保留比例为 0.5 时，IdleViT 可以在 DeiT-S 上比 EViT 高 0.5% 的精度和更快的测试速度。<details>
<summary>Abstract</summary>
Vision Transformers (ViTs) have demonstrated outstanding performance in computer vision tasks, yet their high computational complexity prevents their deployment in computing resource-constrained environments. Various token pruning techniques have been introduced to alleviate the high computational burden of ViTs by dynamically dropping image tokens. However, some undesirable pruning at early stages may result in permanent loss of image information in subsequent layers, consequently hindering model performance. To address this problem, we propose IdleViT, a dynamic token-idle-based method that achieves an excellent trade-off between performance and efficiency. Specifically, in each layer, IdleViT selects a subset of the image tokens to participate in computations while keeping the rest of the tokens idle and directly passing them to this layer's output. By allowing the idle tokens to be re-selected in the following layers, IdleViT mitigates the negative impact of improper pruning in the early stages. Furthermore, inspired by the normalized graph cut, we devise a token cut loss on the attention map as regularization to improve IdleViT's token selection ability. Our method is simple yet effective and can be extended to pyramid ViTs since no token is completely dropped. Extensive experimental results on various ViT architectures have shown that IdleViT can diminish the complexity of pretrained ViTs by up to 33\% with no more than 0.2\% accuracy decrease on ImageNet, after finetuning for only 30 epochs. Notably, when the keep ratio is 0.5, IdleViT outperforms the state-of-the-art EViT on DeiT-S by 0.5\% higher accuracy and even faster inference speed. The source code is available in the supplementary material.
</details>
<details>
<summary>摘要</summary>
通过图像矩阵变换（ViT），计算机视觉任务的表现几乎不可思议，但是它们的计算复杂性使得在计算资源受限的环境中不能实施。为了解决这个问题，我们提出了IdleViT，一种基于动态token idle的方法，可以很好地平衡性能和效率。具体来说，在每层中，IdleViT选择图像中的一部分token参与计算，而保留剩下的token idle，直接将其传递给当前层的输出。通过在后续层中重新选择idletoken，IdleViT消除了在早期阶段的不合适剪裁所产生的负面影响。此外，我们根据 норма化图像排序（normalized graph cut）的思想，在注意力图中定义了一个token cut损失，以便提高IdleViT的token选择能力。我们的方法简单而有效，可以扩展到 pyramid ViTs，因为没有完全drop的token。我们在不同的ViT架构上进行了广泛的实验，并证明了IdleViT可以减少预训练ViT的复杂度达33%，只需要30个epoch的微调，而且在ImageNet上保持至少0.2%的准确率下。特别是，当保留比例为0.5时，IdleViT可以在DeiT-S上高于状态之前的EViT，增加0.5%的准确率和更快的执行速度。详细的源代码可以在补充材料中找到。
</details></li>
</ul>
<hr>
<h2 id="FENCE-Fairplay-Ensuring-Network-Chain-Entity-for-Real-Time-Multiple-ID-Detection-at-Scale-In-Fantasy-Sports"><a href="#FENCE-Fairplay-Ensuring-Network-Chain-Entity-for-Real-Time-Multiple-ID-Detection-at-Scale-In-Fantasy-Sports" class="headerlink" title="FENCE: Fairplay Ensuring Network Chain Entity for Real-Time Multiple ID Detection at Scale In Fantasy Sports"></a>FENCE: Fairplay Ensuring Network Chain Entity for Real-Time Multiple ID Detection at Scale In Fantasy Sports</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05651">http://arxiv.org/abs/2310.05651</a></li>
<li>repo_url: None</li>
<li>paper_authors: Akriti Upreti, Kartavya Kothari, Utkarsh Thukral, Vishal Verma</li>
<li>for: 本文旨在解决 Dream11 平台上的多个账户创建问题，以防止用户利用平台的奖励提供程序。</li>
<li>methods: 本文提出了一种图形基的解决方案，首先预测用户之间的关系，然后检测欺诈账户的协同行为。</li>
<li>results: 本文介绍了一种分布式Machine Learning系统，用于支持和服务检测模型的推断。系统能够在实时中进行检测，以采取 corrrective actions。此外，文章还包括人类在Loop组件，用于验证、反馈和ground truth标注。<details>
<summary>Abstract</summary>
Dream11 takes pride in being a unique platform that enables over 190 million fantasy sports users to demonstrate their skills and connect deeper with their favorite sports. While managing such a scale, one issue we are faced with is duplicate/multiple account creation in the system. This is done by some users with the intent of abusing the platform, typically for bonus offers. The challenge is to detect these multiple accounts before it is too late. We propose a graph-based solution to solve this problem in which we first predict edges/associations between users. Using the edge information we highlight clusters of colluding multiple accounts. In this paper, we talk about our distributed ML system which is deployed to serve and support the inferences from our detection models. The challenge is to do this in real-time in order to take corrective actions. A core part of this setup also involves human-in-the-loop components for validation, feedback, and ground-truth labeling.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Plug-n’-Play-Channel-Shuffle-Module-for-Enhancing-Tiny-Vision-Transformers"><a href="#Plug-n’-Play-Channel-Shuffle-Module-for-Enhancing-Tiny-Vision-Transformers" class="headerlink" title="Plug n’ Play: Channel Shuffle Module for Enhancing Tiny Vision Transformers"></a>Plug n’ Play: Channel Shuffle Module for Enhancing Tiny Vision Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05642">http://arxiv.org/abs/2310.05642</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuwei Xu, Sen Wang, Yudong Chen, Jiajun Liu</li>
<li>for: 提高小型变换器（ViT）的效率，使其适用于具有限定内存和计算资源的设备。</li>
<li>methods: 提出一种新的混合核心排序模块，通过在缓存频率较高的卷积层和自我关注机制之间进行信息交换，提高小型ViT的性能。</li>
<li>results: 在ImageNet-1K数据集上，通过 incorporating our module into tiny ViT models，可以提高top-1准确率，而且计算复杂度变化在0.03 GMACs以下。 Specifically, our proposed channel shuffle module consistently improves the top-1 accuracy by up to 2.8%.<details>
<summary>Abstract</summary>
Vision Transformers (ViTs) have demonstrated remarkable performance in various computer vision tasks. However, the high computational complexity hinders ViTs' applicability on devices with limited memory and computing resources. Although certain investigations have delved into the fusion of convolutional layers with self-attention mechanisms to enhance the efficiency of ViTs, there remains a knowledge gap in constructing tiny yet effective ViTs solely based on the self-attention mechanism. Furthermore, the straightforward strategy of reducing the feature channels in a large but outperforming ViT often results in significant performance degradation despite improved efficiency. To address these challenges, we propose a novel channel shuffle module to improve tiny-size ViTs, showing the potential of pure self-attention models in environments with constrained computing resources. Inspired by the channel shuffle design in ShuffleNetV2 \cite{ma2018shufflenet}, our module expands the feature channels of a tiny ViT and partitions the channels into two groups: the \textit{Attended} and \textit{Idle} groups. Self-attention computations are exclusively employed on the designated \textit{Attended} group, followed by a channel shuffle operation that facilitates information exchange between the two groups. By incorporating our module into a tiny ViT, we can achieve superior performance while maintaining a comparable computational complexity to the vanilla model. Specifically, our proposed channel shuffle module consistently improves the top-1 accuracy on the ImageNet-1K dataset for various tiny ViT models by up to 2.8\%, with the changes in model complexity being less than 0.03 GMACs.
</details>
<details>
<summary>摘要</summary>
《视图变换器》（ViTs）在计算机视觉任务中表现出色，但高计算复杂性限制了ViTs在有限内存和计算资源的设备上的应用。虽然一些研究已经探索了将卷积层与自注意机制结合使用以提高ViTs的效率，但还有一个知识空白在建立简单而高效的ViTssolely基于自注意机制。此外，通常减少大型ViT的特征通道会导致显著性能下降，尽管提高了效率。为了解决这些挑战，我们提出了一种新的通道排序模块，用于改进简单型ViTs。我们的模块基于ShuffleNetV2中的通道排序设计，将特征通道分成两组：“Attended”和“Idle”组。只有在“Attended”组上进行自注意计算，然后进行通道排序操作，以便在两组之间进行信息交换。通过将我们的模块纳入简单型ViT中，我们可以实现高性能，同时保持与标准模型的计算复杂性相似。具体来说，我们的提议的通道排序模块在ImageNet-1K数据集上的top-1准确率上提高了2.8%，而模型的变化量占0.03 GMACs以下。
</details></li>
</ul>
<hr>
<h2 id="Domain-Watermark-Effective-and-Harmless-Dataset-Copyright-Protection-is-Closed-at-Hand"><a href="#Domain-Watermark-Effective-and-Harmless-Dataset-Copyright-Protection-is-Closed-at-Hand" class="headerlink" title="Domain Watermark: Effective and Harmless Dataset Copyright Protection is Closed at Hand"></a>Domain Watermark: Effective and Harmless Dataset Copyright Protection is Closed at Hand</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14942">http://arxiv.org/abs/2310.14942</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/junfenggo/domain-watermark">https://github.com/junfenggo/domain-watermark</a></li>
<li>paper_authors: Junfeng Guo, Yiming Li, Lixu Wang, Shu-Tao Xia, Heng Huang, Cong Liu, Bo Li</li>
<li>for: 保护开源数据集的版权，防止恶意模型攻击</li>
<li>methods: 基于域水印的数据集所有权验证，通过生成难 Sample来验证模型的准确性</li>
<li>results: 提出了一种基于域水印的数据集所有权验证方法，可以防止恶意模型攻击，并且有较高的鲁棒性和抗性能力。<details>
<summary>Abstract</summary>
The prosperity of deep neural networks (DNNs) is largely benefited from open-source datasets, based on which users can evaluate and improve their methods. In this paper, we revisit backdoor-based dataset ownership verification (DOV), which is currently the only feasible approach to protect the copyright of open-source datasets. We reveal that these methods are fundamentally harmful given that they could introduce malicious misclassification behaviors to watermarked DNNs by the adversaries. In this paper, we design DOV from another perspective by making watermarked models (trained on the protected dataset) correctly classify some `hard' samples that will be misclassified by the benign model. Our method is inspired by the generalization property of DNNs, where we find a \emph{hardly-generalized domain} for the original dataset (as its \emph{domain watermark}). It can be easily learned with the protected dataset containing modified samples. Specifically, we formulate the domain generation as a bi-level optimization and propose to optimize a set of visually-indistinguishable clean-label modified data with similar effects to domain-watermarked samples from the hardly-generalized domain to ensure watermark stealthiness. We also design a hypothesis-test-guided ownership verification via our domain watermark and provide the theoretical analyses of our method. Extensive experiments on three benchmark datasets are conducted, which verify the effectiveness of our method and its resistance to potential adaptive methods. The code for reproducing main experiments is available at \url{https://github.com/JunfengGo/Domain-Watermark}.
</details>
<details>
<summary>摘要</summary>
“深度神经网络（DNN）的繁荣得益于开源数据集，用户可以通过这些数据集进行评估和改进自己的方法。在这篇论文中，我们再次检视了基于DOV（ dataset ownership verification）的数据集权利保护方法，我们发现这些方法是根本不可靠的，因为它们可能会由 adversaries 引入黑客识别器模型中的恶意识别行为。在这篇论文中，我们从另一个角度设计 DOV，使得在训练在保护数据集上的损坏模型中，对一些难样本进行正确的识别。我们的方法是基于 DNN 的总体化性特性，我们在原始数据集中找到一个难以总化的Domain（领域），然后通过修改这些样本来学习一个 hardly-generalized 领域。我们将这个领域作为 DNN 的域水印，通过对这些修改后的样本进行训练，来确保 watermark 的隐蔽性。我们还设计了一种假设测试导向的所有权验证方法，并提供了方法的理论分析。我们在三个基准数据集上进行了广泛的实验，并证明了我们的方法的有效性和对可适应方法的抵御力。相关的代码可以在 GitHub 上找到：https://github.com/JunfengGo/Domain-Watermark。”
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Top-k-Estimation-Consolidates-Disagreement-between-Feature-Attribution-Methods"><a href="#Dynamic-Top-k-Estimation-Consolidates-Disagreement-between-Feature-Attribution-Methods" class="headerlink" title="Dynamic Top-k Estimation Consolidates Disagreement between Feature Attribution Methods"></a>Dynamic Top-k Estimation Consolidates Disagreement between Feature Attribution Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05619">http://arxiv.org/abs/2310.05619</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonathan Kamp, Lisa Beinborn, Antske Fokkens</li>
<li>for: 这paper是用来解释文本分类器的预测结果的方法。</li>
<li>methods: 这paper使用了几种不同的方法来计算特征归因分数，并评估了这些方法的性能。</li>
<li>results: 研究发现，使用固定k或动态k都可以得到高度一致的结果，但动态k主要提高了 интеграルGradient和GradientXInput的性能。这是首次证明了 attribute scores 的顺序性有用于人类理解。<details>
<summary>Abstract</summary>
Feature attribution scores are used for explaining the prediction of a text classifier to users by highlighting a k number of tokens. In this work, we propose a way to determine the number of optimal k tokens that should be displayed from sequential properties of the attribution scores. Our approach is dynamic across sentences, method-agnostic, and deals with sentence length bias. We compare agreement between multiple methods and humans on an NLI task, using fixed k and dynamic k. We find that perturbation-based methods and Vanilla Gradient exhibit highest agreement on most method--method and method--human agreement metrics with a static k. Their advantage over other methods disappears with dynamic ks which mainly improve Integrated Gradient and GradientXInput. To our knowledge, this is the first evidence that sequential properties of attribution scores are informative for consolidating attribution signals for human interpretation.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate_text="Feature attribution scores 是用于解释文本分类器预测结果的 Token 的准确性。在这种工作中，我们提出了一种方法来确定显示 k 个 Token 的优化数量，基于序列性质。我们的方法是动态 sentence，方法不依赖的，并且能够处理句子长度偏好。我们比较了多种方法和人类在 NLI 任务中的一致性，使用 fixes k 和动态 k。我们发现，扰动基于方法和 Vanilla Gradient 在大多数方法--方法和方法--人类协议中表现最高，其优势在 static k 下消失。这是我们知道的第一个证据，Sequential 性质是用于结合权重信号的准确信号的。”Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Multi-head-Contrastive-Learning"><a href="#Adaptive-Multi-head-Contrastive-Learning" class="headerlink" title="Adaptive Multi-head Contrastive Learning"></a>Adaptive Multi-head Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05615">http://arxiv.org/abs/2310.05615</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lei Wang, Piotr Koniusz, Tom Gedeon, Liang Zheng<br>for: The paper is written to address the issue of inconsistent similarity measurements in contrastive learning, specifically when using multiple augmentation strategies.methods: The paper proposes using multiple projection heads, each producing a separate set of features, to improve the consistency of similarity measurements in contrastive learning. The loss function for pre-training is based on a solution to the maximum likelihood estimation over head-wise posterior distributions of positive samples given observations.results: The proposed adaptive multi-head contrastive learning (AMCL) method improves the performance of several popular contrastive learning methods, including SimCLR, MoCo, and Barlow Twins, under various backbones and linear probing epochs. The improvement is more significant when multiple augmentation methods are used.<details>
<summary>Abstract</summary>
In contrastive learning, two views of an original image generated by different augmentations are considered as a positive pair whose similarity is required to be high. Moreover, two views of two different images are considered as a negative pair, and their similarity is encouraged to be low. Normally, a single similarity measure given by a single projection head is used to evaluate positive and negative sample pairs, respectively. However, due to the various augmentation strategies and varying intra-sample similarity, augmented views from the same image are often not similar. Moreover, due to inter-sample similarity, augmented views of two different images may be more similar than augmented views from the same image. As such, enforcing a high similarity for positive pairs and a low similarity for negative pairs may not always be achievable, and in the case of some pairs, forcing so may be detrimental to the performance. To address this issue, we propose to use multiple projection heads, each producing a separate set of features. Our loss function for pre-training emerges from a solution to the maximum likelihood estimation over head-wise posterior distributions of positive samples given observations. The loss contains the similarity measure over positive and negative pairs, each re-weighted by an individual adaptive temperature that is regularized to prevent ill solutions. Our adaptive multi-head contrastive learning (AMCL) can be applied to and experimentally improves several popular contrastive learning methods such as SimCLR, MoCo and Barlow Twins. Such improvement is consistent under various backbones and linear probing epoches and is more significant when multiple augmentation methods are used.
</details>
<details>
<summary>摘要</summary>
在对比学习中，两个视图来自不同的扩充方法的原始图像被视为一个正样对，需要高度相似。同时，两个不同图像的两个视图被视为一个负样对，需要低度相似。通常情况下，单一的相似度测量由单个投影头提供，用于评估正样对和负样对。然而，由于不同的扩充策略和内样 Similarity 的变化，扩充视图从同一个图像中可能不相似，而两个不同图像的扩充视图可能更相似。因此，强制正样对和负样对的相似度高低可能并不总是可 achievable，而且在某些对之中，强制如此可能会损害性能。为解决这个问题，我们提议使用多个投影头，每个生成一个独立的特征集。我们的损失函数在预训练阶段由每个头wise posterior distribution of positive samples given observations的最大可能性解决出来。损失函数包括对正样对和负样对的相似度测量，每个重新权重通过个体适应温度评正化，以避免不良解决。我们称之为自适应多头对比学习（AMCL）。我们的AMCL可以应用到多种流行的对比学习方法，如SimCLR、MoCo和Barlow Twins，并在不同的后端和线性探针级别上实现了实验增进。这种改进是不同的扩充方法和误差率下的可重复的，并且在多个扩充方法使用时更加明显。
</details></li>
</ul>
<hr>
<h2 id="InterroLang-Exploring-NLP-Models-and-Datasets-through-Dialogue-based-Explanations"><a href="#InterroLang-Exploring-NLP-Models-and-Datasets-through-Dialogue-based-Explanations" class="headerlink" title="InterroLang: Exploring NLP Models and Datasets through Dialogue-based Explanations"></a>InterroLang: Exploring NLP Models and Datasets through Dialogue-based Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05592">http://arxiv.org/abs/2310.05592</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dfki-nlp/interrolang">https://github.com/dfki-nlp/interrolang</a></li>
<li>paper_authors: Nils Feldhus, Qianli Wang, Tatiana Anikina, Sahil Chopra, Cennet Oguz, Sebastian Möller</li>
<li>for: 本研究旨在开发一个可交互的对话系统，帮助用户通过自然语言界面获得模型和数据集的解释。</li>
<li>methods: 本研究采用了对话扩展模型TalkToModel（Slack et al., 2022），并添加了新的NLP特有操作，如自由文本合理化。</li>
<li>results: 研究发现，对话性解释对用户来说是有用和有 corrections 的，可以帮助用户更好地理解模型的预测结果。此外，用户通过对话性解释可以更好地预测模型的结果，而不是基于单个解释。<details>
<summary>Abstract</summary>
While recently developed NLP explainability methods let us open the black box in various ways (Madsen et al., 2022), a missing ingredient in this endeavor is an interactive tool offering a conversational interface. Such a dialogue system can help users explore datasets and models with explanations in a contextualized manner, e.g. via clarification or follow-up questions, and through a natural language interface. We adapt the conversational explanation framework TalkToModel (Slack et al., 2022) to the NLP domain, add new NLP-specific operations such as free-text rationalization, and illustrate its generalizability on three NLP tasks (dialogue act classification, question answering, hate speech detection). To recognize user queries for explanations, we evaluate fine-tuned and few-shot prompting models and implement a novel Adapter-based approach. We then conduct two user studies on (1) the perceived correctness and helpfulness of the dialogues, and (2) the simulatability, i.e. how objectively helpful dialogical explanations are for humans in figuring out the model's predicted label when it's not shown. We found rationalization and feature attribution were helpful in explaining the model behavior. Moreover, users could more reliably predict the model outcome based on an explanation dialogue rather than one-off explanations.
</details>
<details>
<summary>摘要</summary>
Recently developed NLP explainability methods have allowed us to open the black box in various ways (Madsen et al., 2022), but a missing ingredient in this endeavor is an interactive tool offering a conversational interface. Such a dialogue system can help users explore datasets and models with explanations in a contextualized manner, for example, via clarification or follow-up questions, and through a natural language interface. We adapted the conversational explanation framework TalkToModel (Slack et al., 2022) to the NLP domain, added new NLP-specific operations such as free-text rationalization, and demonstrated its generalizability on three NLP tasks (dialogue act classification, question answering, hate speech detection). To recognize user queries for explanations, we evaluated fine-tuned and few-shot prompting models and implemented a novel Adapter-based approach. We then conducted two user studies on (1) the perceived correctness and helpfulness of the dialogues, and (2) the simulatability, i.e., how objectively helpful dialogical explanations are for humans in figuring out the model's predicted label when it's not shown. We found that rationalization and feature attribution were helpful in explaining the model behavior, and users could more reliably predict the model outcome based on an explanation dialogue rather than one-off explanations.
</details></li>
</ul>
<hr>
<h2 id="Aggregated-f-average-Neural-Network-for-Interpretable-Ensembling"><a href="#Aggregated-f-average-Neural-Network-for-Interpretable-Ensembling" class="headerlink" title="Aggregated f-average Neural Network for Interpretable Ensembling"></a>Aggregated f-average Neural Network for Interpretable Ensembling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05566">http://arxiv.org/abs/2310.05566</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mathieu Vu, Emilie Chouzenoux, Jean-Christophe Pesquet, Ismail Ben Ayed</li>
<li>for: 这篇论文主要用于解决几个机器学习模型（即弱学习器）在共同任务上进行提高预测性能的问题。</li>
<li>methods: 这篇论文使用了基本的拼接法和更复杂的树状拼接法，并引入了一个新的卷积神经网络来实现最佳的吞吐量拼接。</li>
<li>results: 该论文通过使用不同类型的均值来优化弱学习器预测结果，并通过使用可解释的架构和简单的训练策略，实现了在少量示例增强学习问题上的好表现。<details>
<summary>Abstract</summary>
Ensemble learning leverages multiple models (i.e., weak learners) on a common machine learning task to enhance prediction performance. Basic ensembling approaches average the weak learners outputs, while more sophisticated ones stack a machine learning model in between the weak learners outputs and the final prediction. This work fuses both aforementioned frameworks. We introduce an aggregated f-average (AFA) shallow neural network which models and combines different types of averages to perform an optimal aggregation of the weak learners predictions. We emphasise its interpretable architecture and simple training strategy, and illustrate its good performance on the problem of few-shot class incremental learning.
</details>
<details>
<summary>摘要</summary>
ensemble learning可以利用多个模型（即弱学习器）来增强预测性能， Basic的ensembleapproaches是将弱学习器输出平均化，而更复杂的ones是在弱学习器输出和最终预测之间堆叠一个机器学习模型。这个工作 fusion这两种框架。我们介绍了一个集成了不同类型的平均值的简单神经网络，即集成f-平均（AFA）神经网络。我们强调其可解释的architecture和简单的训练策略，并通过几何学问题中的少量逻辑学习问题 illustrate its good performance。Here's the breakdown of the translation:* "ensemble learning" is translated as "集成学习" (zhòngshì xuéxí)* "leverages" is translated as "利用" (lìyòu)* "multiple models" is translated as "多个模型" (duō ge móde)* "weak learners" is translated as "弱学习器" (ruò xuéxí qì)* "basic ensembling approaches" is translated as "基本的ensembleapproaches" (jīběn de ensembleapproaches)* "stack a machine learning model" is translated as "堆叠一个机器学习模型" (zhumu yī ge jīshì xuéxí móde)* "this work" is translated as "这个工作" (zhè ge gōngzuò)* "fuses" is translated as "集成" (jìshèng)* "both frameworks" is translated as "这两种框架" (zhè liàng zhī kāngjī)* "introduce" is translated as "介绍" (jièkuài)* "an aggregated f-average (AFA) shallow neural network" is translated as "一个集成f-平均（AFA）神经网络" (yī ge jìshèng f-píngyān (AFA) xīnnéng wǎngluò)* "interpretable architecture" is translated as "可解释的architecture" (kějiěshì de architecture)* "simple training strategy" is translated as "简单的训练策略" (jìndān de xùnxíng zhìxí)* "illustrate" is translated as "illustrate" (xìhuì)* "good performance" is translated as "好的性能" (hǎo de xìngnéng)Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know and I'll be happy to provide it.
</details></li>
</ul>
<hr>
<h2 id="STREAM-Social-data-and-knowledge-collective-intelligence-platform-for-TRaining-Ethical-AI-Models"><a href="#STREAM-Social-data-and-knowledge-collective-intelligence-platform-for-TRaining-Ethical-AI-Models" class="headerlink" title="STREAM: Social data and knowledge collective intelligence platform for TRaining Ethical AI Models"></a>STREAM: Social data and knowledge collective intelligence platform for TRaining Ethical AI Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05563">http://arxiv.org/abs/2310.05563</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuwei Wang, Enmeng Lu, Zizhe Ruan, Yao Liang, Yi Zeng</li>
<li>for: 本文提出了一个社交数据和知识共同智能平台，用于训练伦理AI模型（STREAM），以解决人工智能模型与人类伦理价值观念的匹配问题，并提供伦理数据集和知识库，以帮助AI模型“遵循好 advise的自然流动”。</li>
<li>methods: 本文使用了创建全面和代表性的平台，准确反映人类和AI的伦理判断，捕捉人类和AI的群体和文化差异，以及时间的演变，以便实现6Es（Establishment、Evaluation、Embedding、Embodiment、Ensemble、Evolvement）中的伦理能力。</li>
<li>results: STREAM已经提供了丰富的伦理enario集，并收集了大量由志愿者和各种流行的大语言模型（LLMs）注释的伦理判断数据，共同反映人类和AI在不同伦理上的偏好和表现。<details>
<summary>Abstract</summary>
This paper presents Social data and knowledge collective intelligence platform for TRaining Ethical AI Models (STREAM) to address the challenge of aligning AI models with human moral values, and to provide ethics datasets and knowledge bases to help promote AI models "follow good advice as naturally as a stream follows its course". By creating a comprehensive and representative platform that accurately mirrors the moral judgments of diverse groups including humans and AIs, we hope to effectively portray cultural and group variations, and capture the dynamic evolution of moral judgments over time, which in turn will facilitate the Establishment, Evaluation, Embedding, Embodiment, Ensemble, and Evolvement (6Es) of the moral capabilities of AI models. Currently, STREAM has already furnished a comprehensive collection of ethical scenarios, and amassed substantial moral judgment data annotated by volunteers and various popular Large Language Models (LLMs), collectively portraying the moral preferences and performances of both humans and AIs across a range of moral contexts. This paper will outline the current structure and construction of STREAM, explore its potential applications, and discuss its future prospects.
</details>
<details>
<summary>摘要</summary>
Currently, STREAM has furnished a comprehensive collection of ethical scenarios and amassed substantial moral judgment data annotated by volunteers and various popular Large Language Models (LLMs), collectively portraying the moral preferences and performances of both humans and AIs across a range of moral contexts. This paper will outline the current structure and construction of STREAM, explore its potential applications, and discuss its future prospects.
</details></li>
</ul>
<hr>
<h2 id="WeatherDepth-Curriculum-Contrastive-Learning-for-Self-Supervised-Depth-Estimation-under-Adverse-Weather-Conditions"><a href="#WeatherDepth-Curriculum-Contrastive-Learning-for-Self-Supervised-Depth-Estimation-under-Adverse-Weather-Conditions" class="headerlink" title="WeatherDepth: Curriculum Contrastive Learning for Self-Supervised Depth Estimation under Adverse Weather Conditions"></a>WeatherDepth: Curriculum Contrastive Learning for Self-Supervised Depth Estimation under Adverse Weather Conditions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05556">http://arxiv.org/abs/2310.05556</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiyuan Wang, Chunyu Lin, Lang Nie, Shujun Huang, Yao Zhao, Xing Pan, Rui Ai</li>
<li>for: 提高雨天环境下深度估计模型的性能，适应不同天气条件下的摄像头捕捉。</li>
<li>methods: 提出了一种自动进行课程分配和学习的自监学习策略，通过不同的课程来逐渐适应不同的天气条件，并且通过对不同课程之间的深度一致性进行约束，以提高模型的鲁棒性。</li>
<li>results: 在实验中，提出的解决方案可以轻松地与不同的模型结合使用，并在人工挑战和实际雨天捕捉数据集上达到了当前最佳性能。<details>
<summary>Abstract</summary>
Depth estimation models have shown promising performance on clear scenes but fail to generalize to adverse weather conditions due to illumination variations, weather particles, etc. In this paper, we propose WeatherDepth, a self-supervised robust depth estimation model with curriculum contrastive learning, to tackle performance degradation in complex weather conditions. Concretely, we first present a progressive curriculum learning scheme with three simple-to-complex curricula to gradually adapt the model from clear to relative adverse, and then to adverse weather scenes. It encourages the model to gradually grasp beneficial depth cues against the weather effect, yielding smoother and better domain adaption. Meanwhile, to prevent the model from forgetting previous curricula, we integrate contrastive learning into different curricula. Drawn the reference knowledge from the previous course, our strategy establishes a depth consistency constraint between different courses towards robust depth estimation in diverse weather. Besides, to reduce manual intervention and better adapt to different models, we designed an adaptive curriculum scheduler to automatically search for the best timing for course switching. In the experiment, the proposed solution is proven to be easily incorporated into various architectures and demonstrates state-of-the-art (SoTA) performance on both synthetic and real weather datasets.
</details>
<details>
<summary>摘要</summary>
depth estimation模型在清晰场景下表现出色，但在不利天气条件下表现不佳，主要是因为照明变化、天气粒子等因素。在这篇论文中，我们提出了一种自动适应的深度估计模型——WeatherDepth，使得模型在复杂的天气条件下能够更好地适应。具体来说，我们首先提出了一种进步式课程学习方案，包括三个简单到复杂的课程，以逐步适应模型从清晰到相对不利、然后到不利天气场景。这使得模型逐渐捕捉到恰当的深度提示，从而获得更好的预测性。同时，为了避免模型忘记之前的课程，我们将对不同的课程进行了对比学习。从此，我们的策略建立了一个深度一致性约束，以保证模型在多种天气条件下的稳定性。此外，为了避免手动 intervención和更好地适应不同的模型，我们设计了一个自适应课程调度器，以自动搜索最佳课程时间点。在实验中，我们的解决方案轻松地适应到不同的架构，并在真实的天气数据集上达到了当前最佳性能（SoTA）。
</details></li>
</ul>
<hr>
<h2 id="Logic-guided-Deep-Reinforcement-Learning-for-Stock-Trading"><a href="#Logic-guided-Deep-Reinforcement-Learning-for-Stock-Trading" class="headerlink" title="Logic-guided Deep Reinforcement Learning for Stock Trading"></a>Logic-guided Deep Reinforcement Learning for Stock Trading</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05551">http://arxiv.org/abs/2310.05551</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiming Li, Junzhe Jiang, Yushi Cao, Aixin Cui, Bozhi Wu, Bo Li, Yang Liu</li>
<li>for: 这篇论文的目的是提出一种逻辑导航的交易框架，以提高深度强化学习（DRL）在动态股票市场中的稳定性和性能。</li>
<li>methods: 该论文提出了一种新的逻辑导航框架，称为SYENS（程序合成基于集成策略），它通过在层次结构中使用程序合成来规范模型的行为，并具有更高的稳定性和性能。</li>
<li>results: 根据实验结果，SYENS在30个道琴股票的股票交易中具有更高的累积收益和较低的最大投降，并在两种交易设置下（即现金交易和质押交易）都能够显著超越基elines。<details>
<summary>Abstract</summary>
Deep reinforcement learning (DRL) has revolutionized quantitative finance by achieving excellent performance without significant manual effort. Whereas we observe that the DRL models behave unstably in a dynamic stock market due to the low signal-to-noise ratio nature of the financial data. In this paper, we propose a novel logic-guided trading framework, termed as SYENS (Program Synthesis-based Ensemble Strategy). Different from the previous state-of-the-art ensemble reinforcement learning strategy which arbitrarily selects the best-performing agent for testing based on a single measurement, our framework proposes regularizing the model's behavior in a hierarchical manner using the program synthesis by sketching paradigm. First, we propose a high-level, domain-specific language (DSL) that is used for the depiction of the market environment and action. Then based on the DSL, a novel program sketch is introduced, which embeds human expert knowledge in a logical manner. Finally, based on the program sketch, we adopt the program synthesis by sketching a paradigm and synthesizing a logical, hierarchical trading strategy. We evaluate SYENS on the 30 Dow Jones stocks under the cash trading and the margin trading settings. Experimental results demonstrate that our proposed framework can significantly outperform the baselines with much higher cumulative return and lower maximum drawdown under both settings.
</details>
<details>
<summary>摘要</summary>
深度强化学习（DRL）已经革命化金融科学，它可以在不需要显著的人工努力的情况下达到出色的性能。然而，我们观察到DRL模型在动态股票市场中的不稳定行为，这是因为财务数据的信号噪声比例较低。在这篇论文中，我们提出了一种新的逻辑引导交易框架，称为SYENS（程序合成基于ensemble策略）。与前一代状态的聚合强化学习策略不同，我们的框架在层次结构上使用程序合成来规范模型的行为。首先，我们提出了一种高级、领域特定语言（DSL），用于描述市场环境和行动。然后，我们基于DSL引入了一种新的程序绘制，其嵌入了人类专家知识在逻辑上。最后，我们采用程序合成 by sketching 方法，并将其应用于SYENS框架中。我们在30个道琴股票下对cash交易和margin交易进行了实验。实验结果表明，我们的提出的框架可以与基准值相比较高的净返报和较低的最大下降。
</details></li>
</ul>
<hr>
<h2 id="ParFam-–-Symbolic-Regression-Based-on-Continuous-Global-Optimization"><a href="#ParFam-–-Symbolic-Regression-Based-on-Continuous-Global-Optimization" class="headerlink" title="ParFam – Symbolic Regression Based on Continuous Global Optimization"></a>ParFam – Symbolic Regression Based on Continuous Global Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05537">http://arxiv.org/abs/2310.05537</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/philipp238/parfam">https://github.com/philipp238/parfam</a></li>
<li>paper_authors: Philipp Scholl, Katharina Bieker, Hillary Hauger, Gitta Kutyniok</li>
<li>for: 解决Symbolic Regression（SR）问题，包括从数据中找到物理法律或财务市场行为的数学方程。</li>
<li>methods: 使用参数家族的符号函数来将精确的SR问题转化为连续问题，然后与强大的全球优化器结合使用，实现SR问题的解决。</li>
<li>results: 通过广泛的数字实验，证明ParFam可以 дости到SR问题的状态Esp中的最佳解决方案，并可以轻松扩展到更高级的算法，例如添加深度神经网络来找到适合的参数家族。<details>
<summary>Abstract</summary>
The problem of symbolic regression (SR) arises in many different applications, such as identifying physical laws or deriving mathematical equations describing the behavior of financial markets from given data. Various methods exist to address the problem of SR, often based on genetic programming. However, these methods are usually quite complicated and require a lot of hyperparameter tuning and computational resources. In this paper, we present our new method ParFam that utilizes parametric families of suitable symbolic functions to translate the discrete symbolic regression problem into a continuous one, resulting in a more straightforward setup compared to current state-of-the-art methods. In combination with a powerful global optimizer, this approach results in an effective method to tackle the problem of SR. Furthermore, it can be easily extended to more advanced algorithms, e.g., by adding a deep neural network to find good-fitting parametric families. We prove the performance of ParFam with extensive numerical experiments based on the common SR benchmark suit SRBench, showing that we achieve state-of-the-art results. Our code and results can be found at https://github.com/Philipp238/parfam .
</details>
<details>
<summary>摘要</summary>
SR（符号回归）问题在多种应用中出现，如从数据中找到物理法律或财务市场行为的数学方程。现有多种解决 SR 问题的方法，通常基于进化编程。然而，这些方法通常很复杂，需要许多Hyperparameter调整和计算资源。在这篇论文中，我们介绍了我们的新方法 ParFam，它利用适当的参数家族符号函数来将离散的符号回归问题转化为连续的问题，从而得到更直观的设置。与现有状态之册方法相比，我们的方法更加简单，并且可以轻松地扩展到更高级的算法，例如通过添加深度神经网络来找到good-fitting参数家族。我们通过对 SRBench 常用的 SR benchmark 进行广泛的数字实验，证明了 ParFam 的性能。我们的代码和结果可以在 GitHub 上找到：https://github.com/Philipp238/parfam。
</details></li>
</ul>
<hr>
<h2 id="On-Double-Descent-in-Reinforcement-Learning-with-LSTD-and-Random-Features"><a href="#On-Double-Descent-in-Reinforcement-Learning-with-LSTD-and-Random-Features" class="headerlink" title="On Double Descent in Reinforcement Learning with LSTD and Random Features"></a>On Double Descent in Reinforcement Learning with LSTD and Random Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05518">http://arxiv.org/abs/2310.05518</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Brellmann, Eloïse Berthier, David Filliat, Goran Frehse</li>
<li>for: 本研究探讨了深度强化学习（RL）中 temporal difference（TD）算法的性能如何受到神经网络大小的影响。</li>
<li>methods: 本研究使用了理论分析来研究神经网络大小和 $l_2$-正则化对性能的影响。研究人员还使用了Random Features和懒散训练策略来研究正则化最小二乘差分算法在无穷大参数和状态数下的性能。</li>
<li>results: 研究人员发现了一种双峰现象，即在神经网络参数和状态数比例大于1时，性能会快速下降。他们还发现，在增加 $l_2$-正则化或状态数下降到0时， corrction terms消失。numerical experiments with synthetic and small real-world environments closely match the theoretical predictions.<details>
<summary>Abstract</summary>
Temporal Difference (TD) algorithms are widely used in Deep Reinforcement Learning (RL). Their performance is heavily influenced by the size of the neural network. While in supervised learning, the regime of over-parameterization and its benefits are well understood, the situation in RL is much less clear. In this paper, we present a theoretical analysis of the influence of network size and $l_2$-regularization on performance. We identify the ratio between the number of parameters and the number of visited states as a crucial factor and define over-parameterization as the regime when it is larger than one. Furthermore, we observe a double descent phenomenon, i.e., a sudden drop in performance around the parameter/state ratio of one. Leveraging random features and the lazy training regime, we study the regularized Least-Square Temporal Difference (LSTD) algorithm in an asymptotic regime, as both the number of parameters and states go to infinity, maintaining a constant ratio. We derive deterministic limits of both the empirical and the true Mean-Square Bellman Error (MSBE) that feature correction terms responsible for the double-descent. Correction terms vanish when the $l_2$-regularization is increased or the number of unvisited states goes to zero. Numerical experiments with synthetic and small real-world environments closely match the theoretical predictions.
</details>
<details>
<summary>摘要</summary>
temporal difference（TD）算法在深度学习（RL）中广泛使用。其性能受到神经网络大小的影响。在超vised学习中，过度参数的情况和其好处已经很好地理解，但在RL中情况却相对不清楚。在这篇论文中，我们提供了TD算法性能与神经网络大小的理论分析。我们确定了参数与访问状态的比率为关键因素，并定义了过度参数为神经网络参数的数量大于状态数量的情况。此外，我们发现了一种双峰现象，即参数/状态比率接近1时性能突然下降。通过随机特征和懒散训练策略，我们研究了正则化最小二乘差（LSTD）算法在参数和状态数量 infinito 的极限情况下。我们 derive了参数和状态数量 infinito 下的零限的实际和真实的Mean-Square Bellman Error（MSBE），其中包含修正项负责 Double Descent。这些修正项在 $l_2$ 正则化强度增大或未访问状态数量减少时消失。实际实验结果与理论预测匹配得非常好。
</details></li>
</ul>
<hr>
<h2 id="UAVs-and-Neural-Networks-for-search-and-rescue-missions"><a href="#UAVs-and-Neural-Networks-for-search-and-rescue-missions" class="headerlink" title="UAVs and Neural Networks for search and rescue missions"></a>UAVs and Neural Networks for search and rescue missions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05512">http://arxiv.org/abs/2310.05512</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hartmut Surmann, Artur Leinweber, Gerhard Senkowski, Julien Meine, Dominik Slomma</li>
<li>for:  detection of objects of interest (cars, humans, fire) in aerial images captured by UAVs during vegetation fires</li>
<li>methods: use of artificial neural networks, creation of a dataset for supervised learning, implementation of an object detection pipeline combining classic image processing techniques with pretrained neural networks, development of a data augmentation pipeline to augment the dataset with automatically labeled images</li>
<li>results: evaluation of the performance of different neural networksHere’s the information in Simplified Chinese:</li>
<li>for: 检测 aerial 图像中的目标对象（车辆、人员、火灾），通常由无人飞行器（UAV）拍摄</li>
<li>methods: 使用人工神经网络，创建一个超级vised 学习的数据集，实现一个对象检测管道，将经典的图像处理技术与预训练的神经网络结合使用，开发一个自动标注图像数据集的数据增强管道</li>
<li>results: 评估不同神经网络的性能<details>
<summary>Abstract</summary>
In this paper, we present a method for detecting objects of interest, including cars, humans, and fire, in aerial images captured by unmanned aerial vehicles (UAVs) usually during vegetation fires. To achieve this, we use artificial neural networks and create a dataset for supervised learning. We accomplish the assisted labeling of the dataset through the implementation of an object detection pipeline that combines classic image processing techniques with pretrained neural networks. In addition, we develop a data augmentation pipeline to augment the dataset with automatically labeled images. Finally, we evaluate the performance of different neural networks.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种方法用于在由无人飞行器（UAV）拍摄的空中图像中检测有关兴趣的对象，包括汽车、人体和火灾。为 дости这一目标，我们使用人工神经网络，并创建了一个用于超级vised学习的数据集。我们通过实施对象检测管道，该管道组合了经典的图像处理技术和预训练的神经网络，来协助标注数据集。此外，我们还开发了一个自动生成数据集的管道，以增加数据集的自动标注图像。最后，我们评估了不同的神经网络的性能。
</details></li>
</ul>
<hr>
<h2 id="Query-and-Response-Augmentation-Cannot-Help-Out-of-domain-Math-Reasoning-Generalization"><a href="#Query-and-Response-Augmentation-Cannot-Help-Out-of-domain-Math-Reasoning-Generalization" class="headerlink" title="Query and Response Augmentation Cannot Help Out-of-domain Math Reasoning Generalization"></a>Query and Response Augmentation Cannot Help Out-of-domain Math Reasoning Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05506">http://arxiv.org/abs/2310.05506</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ofa-sys/gsm8k-screl">https://github.com/ofa-sys/gsm8k-screl</a></li>
<li>paper_authors: Chengpeng Li, Zheng Yuan, Guanting Dong, Keming Lu, Jiancan Wu, Chuanqi Tan, Xiang Wang, Chang Zhou</li>
<li>for: 这个论文主要是为了研究在数学逻辑中使用大语言模型（LLMs）时，数据增强的效果，以及增强数据的量和多样性对模型性能的影响。</li>
<li>methods: 作者使用了一种新的数据集——AugGSM8K，通过复杂和多样化 queries 来增强数据，并通过 fine-tuning 来训练 LLMs。</li>
<li>results: 作者发现，通过增强数据，可以提高 LLMs 的数学逻辑性能，并且存在对数据量的呈几何关系。然而，在各种数学逻辑任务之间的泛化性能仍然需要进一步改进。<details>
<summary>Abstract</summary>
In math reasoning with large language models (LLMs), fine-tuning data augmentation by query evolution and diverse reasoning paths is empirically verified effective, profoundly narrowing the gap between open-sourced LLMs and cutting-edge proprietary LLMs. In this paper, we conduct an investigation for such data augmentation in math reasoning and are intended to answer: (1) What strategies of data augmentation are more effective; (2) What is the scaling relationship between the amount of augmented data and model performance; and (3) Can data augmentation incentivize generalization to out-of-domain mathematical reasoning tasks? To this end, we create a new dataset, AugGSM8K, by complicating and diversifying the queries from GSM8K and sampling multiple reasoning paths. We obtained a series of LLMs called MuggleMath by fine-tuning on subsets of AugGSM8K. MuggleMath substantially achieves new state-of-the-art on GSM8K (from 54% to 68.4% at the scale of 7B, and from 63.9% to 74.0% at the scale of 13B). A log-linear relationship is presented between MuggleMath's performance and the amount of augmented data. We also find that MuggleMath is weak in out-of-domain math reasoning generalization to MATH. This is attributed to the differences in query distribution between AugGSM8K and MATH which suggest that augmentation on a single benchmark could not help with overall math reasoning performance. Codes and AugGSM8K will be uploaded to https://github.com/OFA-Sys/gsm8k-ScRel.
</details>
<details>
<summary>摘要</summary>
在数学逻辑中使用大型自然语言模型（LLMs）， fine-tuning数据增强和多种逻辑路径的数据增强被证明是有效的，可以减小开源LLMs和高级专有LLMs之间的差距。在这篇论文中，我们进行了数学逻辑中数据增强的调查，旨在回答以下问题：（1）哪些数据增强策略更加有效；（2）数据增强量和模型性能之间存在哪种整数关系；和（3）数据增强是否能够适应尺度外的数学逻辑任务？为此，我们创建了一个新的数据集，AugGSM8K，通过复杂和多样化 queries from GSM8K 来生成多种 reasoning paths。我们使用这些数据集进行了一系列的 LLMS 的 fine-tuning，并取得了一系列的 MuggleMath 模型。MuggleMath 在 GSM8K 上实现了新的状态机器人，从 54% 提高到 68.4% （7B 缩放）和从 63.9% 提高到 74.0% （13B 缩放）。我们发现了数据增强和模型性能之间存在很好的对数关系。此外，我们发现 MuggleMath 在尺度外的数学逻辑任务上的总体性能较弱，这是因为 AugGSM8K 和 MATH 的查询分布之间存在差异。这意味着数据增强在单一的 benchmark 上不能够提高总体数学逻辑性能。代码和 AugGSM8K 将在 <https://github.com/OFA-Sys/gsm8k-ScRel> 上上传。
</details></li>
</ul>
<hr>
<h2 id="Integrating-Graphs-with-Large-Language-Models-Methods-and-Prospects"><a href="#Integrating-Graphs-with-Large-Language-Models-Methods-and-Prospects" class="headerlink" title="Integrating Graphs with Large Language Models: Methods and Prospects"></a>Integrating Graphs with Large Language Models: Methods and Prospects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05499">http://arxiv.org/abs/2310.05499</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shirui Pan, Yizhen Zheng, Yixin Liu</li>
<li>for: 本研究旨在探讨大语言模型（LLM）与图structured data的 интеграción，以提高LLM的性能和应用范围。</li>
<li>methods: 研究分为两类：首先，使用LLM进行图学习，以提高图任务的预测性能；其次，通过图结构来加强LLM的性能，例如在复杂任务中进行合作或理解。</li>
<li>results: 研究表明，通过图结构和LLM的结合，可以提高LLM的性能和应用范围，并且提出了未来研究的开放问题。<details>
<summary>Abstract</summary>
Large language models (LLMs) such as GPT-4 have emerged as frontrunners, showcasing unparalleled prowess in diverse applications, including answering queries, code generation, and more. Parallelly, graph-structured data, an intrinsic data type, is pervasive in real-world scenarios. Merging the capabilities of LLMs with graph-structured data has been a topic of keen interest. This paper bifurcates such integrations into two predominant categories. The first leverages LLMs for graph learning, where LLMs can not only augment existing graph algorithms but also stand as prediction models for various graph tasks. Conversely, the second category underscores the pivotal role of graphs in advancing LLMs. Mirroring human cognition, we solve complex tasks by adopting graphs in either reasoning or collaboration. Integrating with such structures can significantly boost the performance of LLMs in various complicated tasks. We also discuss and propose open questions for integrating LLMs with graph-structured data for the future direction of the field.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）如GPT-4在多种应用中表现出不一样的优势，包括回答问题、代码生成等。同时，图струк成数据是实际世界中普遍存在的数据类型。把LMLM的能力与图结构数据结合，已成为研究者的焦点之一。这篇评论文将这些结合分为两大类。第一种将LMLM用于图学习，LMLM可以不仅增强现有的图算法，还可以作为各种图任务的预测模型。相反，第二种类型强调图结构数据在提高LMLM表现的重要性。人类的思维方式是透过图来解释和协作来解决复杂任务。与图结构数据结合可以将LMLM在多种复杂任务中表现出较好的成绩。我们还讨论了未来领域的开启问题，以推动LMLM与图结构数据的结合。
</details></li>
</ul>
<hr>
<h2 id="How-Abilities-in-Large-Language-Models-are-Affected-by-Supervised-Fine-tuning-Data-Composition"><a href="#How-Abilities-in-Large-Language-Models-are-Affected-by-Supervised-Fine-tuning-Data-Composition" class="headerlink" title="How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition"></a>How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05492">http://arxiv.org/abs/2310.05492</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guanting Dong, Hongyi Yuan, Keming Lu, Chengpeng Li, Mingfeng Xue, Dayiheng Liu, Wei Wang, Zheng Yuan, Chang Zhou, Jingren Zhou<br>for: 这个研究的目的是调查多种能力的LLMs在Supervised Fine-tuning（SFT）中的可调性，以及不同能力之间的数据组合对性能的影响。methods: 这个研究使用了多种SFT策略，包括顺序学习多能力（sequential learning）和 dual-stage mixed fine-tuning（DMT）策略，以及不同数据量和数据组合比例的调整。results: 研究发现，不同的能力展现出不同的扩展特征，大型模型通常需要更多的数据来提高性能。数学逻辑和代码生成能力随着数据量的增加而提高，而通用能力需要约一千个样本才能提高，然后slowly improves。数据组合对不同的能力有启发作用，但高数据量时会导致能力冲突。DMT策略可以避免卷积学习导致忘记现象，提供了多能力学习的可能解决方案。<details>
<summary>Abstract</summary>
Large language models (LLMs) with enormous pre-training tokens and parameter amounts emerge abilities, including math reasoning, code generation, and instruction following. These abilities are further enhanced by supervised fine-tuning (SFT). The open-source community has studied on ad-hoc SFT for each ability, while proprietary LLMs are versatile for all abilities. It is important to investigate how to unlock them with multiple abilities via SFT. In this study, we specifically focus on the data composition between mathematical reasoning, code generation, and general human-aligning abilities during SFT. From a scaling perspective, we investigate the relationship between model abilities and various factors including data amounts, data composition ratio, model parameters, and SFT strategies. Our experiments reveal that different abilities exhibit different scaling patterns, and larger models generally show superior performance with the same amount of data. Mathematical reasoning and code generation improve as data amounts increase consistently, while the general ability is enhanced with about a thousand samples and improves slowly. We find data composition results in various abilities improvements with low data amounts, while conflicts of abilities with high data amounts. Our experiments further show that composition data amount impacts performance, while the influence of composition ratio is insignificant. Regarding the SFT strategies, we evaluate sequential learning multiple abilities are prone to catastrophic forgetting. Our proposed Dual-stage Mixed Fine-tuning (DMT) strategy learns specialized abilities first and then learns general abilities with a small amount of specialized data to prevent forgetting, offering a promising solution to learn multiple abilities with different scaling patterns.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）有巨大的预训语料和参数数量，并且拥有多种能力，包括数学推理、代码生成和指令跟进。这些能力可以通过监督精致训练（SFT）进一步增强。开源社区已经研究了随机SFT的每个能力，而商业LLM则具有多种能力。我们需要研究如何通过SFT解锁多种能力。在本研究中，我们专注于SFT中数学推理、代码生成和通用人类调整的数据结构之间的关系。从扩展角度来看，我们调查模型能力和不同因素（包括数据量、数据结构比例、模型参数和SFT策略）之间的关系。我们的实验显示不同的能力展现出不同的扩展模式，大型模型通常在同量数据下表现出色。数学推理和代码生成随着数据量增加逐渐提高，而通用能力则在约一千个数据 sample 后逐渐提高。我们发现数据结构可以在低数据量下提高不同的能力，但高数据量时会出现能力冲突。我们的实验还显示了数据结构填充量影响表现，但结构比例无法影响表现。关于SFT策略，我们评估了预先学习特定能力后，将特定数据进行混合精致训练（DMT），以避免忘记，提供了多能力学习的有前途的解决方案。
</details></li>
</ul>
<hr>
<h2 id="Cabbage-Sweeter-than-Cake-Analysing-the-Potential-of-Large-Language-Models-for-Learning-Conceptual-Spaces"><a href="#Cabbage-Sweeter-than-Cake-Analysing-the-Potential-of-Large-Language-Models-for-Learning-Conceptual-Spaces" class="headerlink" title="Cabbage Sweeter than Cake? Analysing the Potential of Large Language Models for Learning Conceptual Spaces"></a>Cabbage Sweeter than Cake? Analysing the Potential of Large Language Models for Learning Conceptual Spaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05481">http://arxiv.org/abs/2310.05481</a></li>
<li>repo_url: None</li>
<li>paper_authors: Usashi Chatterjee, Amit Gajbhiye, Steven Schockaert</li>
<li>for: 这个论文旨在探讨使用大型自然语言模型（LLM）学习概念空间的可能性。</li>
<li>methods: 该论文使用了一种基于语言模型的方法，通过学习人类判断来构建概念空间。</li>
<li>results: 实验表明，LLM可以学习一定程度的概念空间表示，但是特定的BERT模型在训练后可以与最大的GPT-3模型匹配或超越它，即使它们的大小只是GPT-3的2-3个数量级。<details>
<summary>Abstract</summary>
The theory of Conceptual Spaces is an influential cognitive-linguistic framework for representing the meaning of concepts. Conceptual spaces are constructed from a set of quality dimensions, which essentially correspond to primitive perceptual features (e.g. hue or size). These quality dimensions are usually learned from human judgements, which means that applications of conceptual spaces tend to be limited to narrow domains (e.g. modelling colour or taste). Encouraged by recent findings about the ability of Large Language Models (LLMs) to learn perceptually grounded representations, we explore the potential of such models for learning conceptual spaces. Our experiments show that LLMs can indeed be used for learning meaningful representations to some extent. However, we also find that fine-tuned models of the BERT family are able to match or even outperform the largest GPT-3 model, despite being 2 to 3 orders of magnitude smaller.
</details>
<details>
<summary>摘要</summary>
理论的概念空间模型是一种有影响力的认知语言框架，用于表示概念的含义。概念空间由一系列质量维度组成，这些质量维度通常来自人类判断，这意味着应用概念空间通常受限于特定领域（如色彩或味道模elling）。鼓动了最近发现大语言模型（LLMs）可以学习基于感知的表示，我们explore了这些模型是否可以学习有意义的概念空间。我们的实验表明LLMs可以学习有意义的表示，但我们还发现，经过精度调整的BERT家族模型可以与最大GPT-3模型匹配或者超越，即使其体积只是GPT-3模型的2-3个数量级。
</details></li>
</ul>
<hr>
<h2 id="Deep-Optimal-Timing-Strategies-for-Time-Series"><a href="#Deep-Optimal-Timing-Strategies-for-Time-Series" class="headerlink" title="Deep Optimal Timing Strategies for Time Series"></a>Deep Optimal Timing Strategies for Time Series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05479">http://arxiv.org/abs/2310.05479</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chenpopper/optimal_timing_tsf">https://github.com/chenpopper/optimal_timing_tsf</a></li>
<li>paper_authors: Chen Pan, Fan Zhou, Xuanwei Hu, Xinxin Zhu, Wenxin Ning, Zi Zhuang, Siqiao Xue, James Zhang, Yunhua Hu</li>
<li>for: 这篇论文的目的是解决很多企业活动中的时间执行计划问题，即在时间序列预测中做出最佳的执行时间选择。</li>
<li>methods: 该论文提出了一种机制，即将时间序列预测任务和优化执行时间决策任务结合起来，以提供一个具有坚实理论基础和实际应用灵活性的解决方案。特别是，它通过使用概率时间序列预测算法，不需要 сложные数学动力模型，从而避免了很多其他常见实践中的假设强大优化知识。</li>
<li>results: 该论文通过使用核心束回归神经网络（RNN）来近似优化时间执行时间，实现了在实际应用中减少操作成本的目标。详细的实现细节可以参考github上的\url{github.com&#x2F;ChenPopper&#x2F;optimal_timing_TSF}仓库。<details>
<summary>Abstract</summary>
Deciding the best future execution time is a critical task in many business activities while evolving time series forecasting, and optimal timing strategy provides such a solution, which is driven by observed data. This solution has plenty of valuable applications to reduce the operation costs. In this paper, we propose a mechanism that combines a probabilistic time series forecasting task and an optimal timing decision task as a first systematic attempt to tackle these practical problems with both solid theoretical foundation and real-world flexibility. Specifically, it generates the future paths of the underlying time series via probabilistic forecasting algorithms, which does not need a sophisticated mathematical dynamic model relying on strong prior knowledge as most other common practices. In order to find the optimal execution time, we formulate the decision task as an optimal stopping problem, and employ a recurrent neural network structure (RNN) to approximate the optimal times. Github repository: \url{github.com/ChenPopper/optimal_timing_TSF}.
</details>
<details>
<summary>摘要</summary>
决定最佳未来执行时间是许多企业活动中的关键任务，而且随着时间序列预测的演化，最佳时间策略提供了一个解决方案，它是由观察数据驱动的。这种解决方案有很多有价值的应用，可以降低运营成本。在这篇论文中，我们提出了一种机制，它将混合 probabilistic 时间序列预测任务和最佳时间决策任务，作为第一个系统性的尝试，以解决这些实际问题。Specifically, it generates the future paths of the underlying time series via probabilistic forecasting algorithms, which does not need a sophisticated mathematical dynamic model relying on strong prior knowledge as most other common practices. In order to find the optimal execution time, we formulate the decision task as an optimal stopping problem, and employ a recurrent neural network structure (RNN) to approximate the optimal times. Github repository: \url{github.com/ChenPopper/optimal_timing_TSF}.Here's the word-for-word translation of the text into Simplified Chinese:决定最佳未来执行时间是许多企业活动中的关键任务，而且随着时间序列预测的演化，最佳时间策略提供了一个解决方案，它是由观察数据驱动的。这种解决方案有很多有价值的应用，可以降低运营成本。在这篇论文中，我们提出了一种机制，它将混合 probabilistic 时间序列预测任务和最佳时间决策任务，作为第一个系统性的尝试，以解决这些实际问题。Specifically, it generates the future paths of the underlying time series via probabilistic forecasting algorithms, which does not need a sophisticated mathematical dynamic model relying on strong prior knowledge as most other common practices. In order to find the optimal execution time, we formulate the decision task as an optimal stopping problem, and employ a recurrent neural network structure (RNN) to approximate the optimal times. Github repository: \url{github.com/ChenPopper/optimal_timing_TSF}.
</details></li>
</ul>
<hr>
<h2 id="Sentence-level-Prompts-Benefit-Composed-Image-Retrieval"><a href="#Sentence-level-Prompts-Benefit-Composed-Image-Retrieval" class="headerlink" title="Sentence-level Prompts Benefit Composed Image Retrieval"></a>Sentence-level Prompts Benefit Composed Image Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05473">http://arxiv.org/abs/2310.05473</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chunmeifeng/sprc">https://github.com/chunmeifeng/sprc</a></li>
<li>paper_authors: Yang Bai, Xinxing Xu, Yong Liu, Salman Khan, Fahad Khan, Wangmeng Zuo, Rick Siow Mong Goh, Chun-Mei Feng</li>
<li>for: 提高组合图像检索的精度（Composed Image Retrieval）</li>
<li>methods: 使用预训练的V-L模型生成句子级提示，并使用image-text对比损失和文本提示对齐损失来学习适合的句子级提示。</li>
<li>results: 在Fashion-IQ和CIRR数据集上比革尤其良好，比起现有state-of-the-art方法。In Simplified Chinese text, the three information would be:</li>
<li>for: 提高组合图像检索的精度</li>
<li>methods: 使用预训练的V-L模型生成句子级提示，并使用image-text对比损失和文本提示对齐损失来学习适合的句子级提示。</li>
<li>results: 在Fashion-IQ和CIRR数据集上比革尤其良好，比起现有state-of-the-art方法。<details>
<summary>Abstract</summary>
Composed image retrieval (CIR) is the task of retrieving specific images by using a query that involves both a reference image and a relative caption. Most existing CIR models adopt the late-fusion strategy to combine visual and language features. Besides, several approaches have also been suggested to generate a pseudo-word token from the reference image, which is further integrated into the relative caption for CIR. However, these pseudo-word-based prompting methods have limitations when target image encompasses complex changes on reference image, e.g., object removal and attribute modification. In this work, we demonstrate that learning an appropriate sentence-level prompt for the relative caption (SPRC) is sufficient for achieving effective composed image retrieval. Instead of relying on pseudo-word-based prompts, we propose to leverage pretrained V-L models, e.g., BLIP-2, to generate sentence-level prompts. By concatenating the learned sentence-level prompt with the relative caption, one can readily use existing text-based image retrieval models to enhance CIR performance. Furthermore, we introduce both image-text contrastive loss and text prompt alignment loss to enforce the learning of suitable sentence-level prompts. Experiments show that our proposed method performs favorably against the state-of-the-art CIR methods on the Fashion-IQ and CIRR datasets. The source code and pretrained model are publicly available at https://github.com/chunmeifeng/SPRC
</details>
<details>
<summary>摘要</summary>
“组合图像检索（CIR）任务是根据查询包含参考图像和相关描述文本来检索特定图像。现有大多数CIR模型采用较晚的融合策略将视觉和语言特征结合。此外，一些方法还建议生成基于参考图像的 pseudo-word 令，并将其与相关描述文本结合使用。然而，这些 pseudo-word 基于的提示方法在参考图像具有复杂变化时存在限制，例如对象移除和特征修改。在这种情况下，我们表明了学习适当的句子级提示（SPRC）是可以实现有效的组合图像检索的。而不是依赖 pseudo-word 基于的提示，我们提议利用预训练的 V-L 模型，如 BLIP-2，生成句子级提示。将学习的句子级提示与相关描述文本 concatenate 后，可以直接使用现有的文本基于图像检索模型进行改进 CIR 性能。此外，我们引入了图像文本对比loss和文本提示对齐loss，以便学习适当的句子级提示。实验结果表明，我们提出的方法在 Fashion-IQ 和 CIRR 数据集上与状态对照方法相比表现优异。源代码和预训练模型可以在 GitHub 上下载。”
</details></li>
</ul>
<hr>
<h2 id="Generative-Judge-for-Evaluating-Alignment"><a href="#Generative-Judge-for-Evaluating-Alignment" class="headerlink" title="Generative Judge for Evaluating Alignment"></a>Generative Judge for Evaluating Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05470">http://arxiv.org/abs/2310.05470</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gair-nlp/auto-j">https://github.com/gair-nlp/auto-j</a></li>
<li>paper_authors: Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao, Pengfei Liu</li>
<li>for: 本研究旨在提出一种生成式评审器（Auto-J），以应对大语言模型（LLM）在自然语言处理（NLP）领域的扩展。</li>
<li>methods: 我们提出了一种基于用户问题和LLM生成的回答的训练方法，并采用了质量权重学习和权重融合来提高模型的一致性和泛化性。</li>
<li>results: 实验结果显示，Auto-J在58个不同情景下的测试环境中具有显著的优势，并且与其他竞争对手（包括开源和关闭源模型）形成明显的差异。<details>
<summary>Abstract</summary>
The rapid development of Large Language Models (LLMs) has substantially expanded the range of tasks they can address. In the field of Natural Language Processing (NLP), researchers have shifted their focus from conventional NLP tasks (e.g., sequence tagging and parsing) towards tasks that revolve around aligning with human needs (e.g., brainstorming and email writing). This shift in task distribution imposes new requirements on evaluating these aligned models regarding generality (i.e., assessing performance across diverse scenarios), flexibility (i.e., examining under different protocols), and interpretability (i.e., scrutinizing models with explanations). In this paper, we propose a generative judge with 13B parameters, Auto-J, designed to address these challenges. Our model is trained on user queries and LLM-generated responses under massive real-world scenarios and accommodates diverse evaluation protocols (e.g., pairwise response comparison and single-response evaluation) with well-structured natural language critiques. To demonstrate the efficacy of our approach, we construct a new testbed covering 58 different scenarios. Experimentally, Auto-J outperforms a series of strong competitors, including both open-source and closed-source models, by a large margin. We also provide detailed analysis and case studies to further reveal the potential of our method and make a variety of resources public at https://github.com/GAIR-NLP/auto-j.
</details>
<details>
<summary>摘要</summary>
随着大型语言模型（LLM）的快速发展，它们可以 addresses 的任务范围得到了极大的扩展。在自然语言处理（NLP）领域，研究人员的焦点从传统的 NLP 任务（如序列标记和分析）转移到了与人类需求相关的任务（如审想和电子邮件写作）。这种任务分布的变化对评估这些对齐的模型进行评估有新的要求，包括总体性（即在多种场景中的表现评估）、灵活性（即在不同的协议下进行评估）以及可读性（即使用自然语言的解释来评估模型）。在这篇论文中，我们提出了一个名为 Auto-J 的生成式评价器，拥有 13B 参数。我们的模型在用户查询和 LLM 生成的回答下进行训练，并且可以处理多种评估协议（如对比回答和单独评估），并且具有良好的自然语言批评结构。为了证明我们的方法的效果，我们构建了一个包含 58 个不同场景的测试床。实验结果表明，Auto-J 在与多种强大竞争对手进行比较时，有大幅度的优势。我们还提供了详细的分析和案例研究，以及在 GitHub 上公开多种资源。
</details></li>
</ul>
<hr>
<h2 id="Cost-Sensitive-Best-Subset-Selection-for-Logistic-Regression-A-Mixed-Integer-Conic-Optimization-Perspective"><a href="#Cost-Sensitive-Best-Subset-Selection-for-Logistic-Regression-A-Mixed-Integer-Conic-Optimization-Perspective" class="headerlink" title="Cost-Sensitive Best Subset Selection for Logistic Regression: A Mixed-Integer Conic Optimization Perspective"></a>Cost-Sensitive Best Subset Selection for Logistic Regression: A Mixed-Integer Conic Optimization Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05464">http://arxiv.org/abs/2310.05464</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ricardo Knauer, Erik Rodner</li>
<li>for: 这个研究是为了设计可解释的机器学习模型，以简化其预测的输入，尤其在医疗领域。</li>
<li>methods: 这篇研究使用了混合整数几何优化的观点，提出了一个可证实且最佳的对应选择程序，可以考虑附加成本来选择特征。</li>
<li>results: 研究结果显示了低数据情况下和标签噪音情况下方法的限制，并提供了实践建议和适当的数据设计。此外，研究也辟开了meta学研究的新领域。<details>
<summary>Abstract</summary>
A key challenge in machine learning is to design interpretable models that can reduce their inputs to the best subset for making transparent predictions, especially in the clinical domain. In this work, we propose a certifiably optimal feature selection procedure for logistic regression from a mixed-integer conic optimization perspective that can take an auxiliary cost to obtain features into account. Based on an extensive review of the literature, we carefully create a synthetic dataset generator for clinical prognostic model research. This allows us to systematically evaluate different heuristic and optimal cardinality- and budget-constrained feature selection procedures. The analysis shows key limitations of the methods for the low-data regime and when confronted with label noise. Our paper not only provides empirical recommendations for suitable methods and dataset designs, but also paves the way for future research in the area of meta-learning.
</details>
<details>
<summary>摘要</summary>
一大挑战在机器学习中是设计可解释的模型，以减少输入并提供透明的预测，尤其在医疗领域。在这项工作中，我们提议一种 certificately 优化的特征选择方法，通过杂谱矩阵优化的视角来考虑辅助成本。我们通过了评 literature 的广泛回顾，并且 méticulously 创建了临床预测模型的 sintética 数据生成器。这使得我们可以系统地评估不同的启发式和优化的卡达性和预算限制下的特征选择方法。分析表明了低数据情况下和标签噪声时方法的局限性。我们的论文不仅提供了实践建议，还开辟了meta-学习领域的未来研究之路。
</details></li>
</ul>
<hr>
<h2 id="Ensemble-based-Hybrid-Optimization-of-Bayesian-Neural-Networks-and-Traditional-Machine-Learning-Algorithms"><a href="#Ensemble-based-Hybrid-Optimization-of-Bayesian-Neural-Networks-and-Traditional-Machine-Learning-Algorithms" class="headerlink" title="Ensemble-based Hybrid Optimization of Bayesian Neural Networks and Traditional Machine Learning Algorithms"></a>Ensemble-based Hybrid Optimization of Bayesian Neural Networks and Traditional Machine Learning Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05456">http://arxiv.org/abs/2310.05456</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peiwen Tan</li>
<li>for: 这个研究旨在优化 bayesian neural networks (BNNs) 的方法，通过与传统机器学习算法如随机森林 (RF)、梯度提升 (GB) 和支持向量机 (SVM) 的综合Integration。</li>
<li>methods: 该研究使用 feature integration 将这些方法相互融合，并强调第二个条件的优化，包括站点性和正定定征矩阵。</li>
<li>results:  ensemble method 表现出了 Robust 和算法优化的特点，并且 hyperparameter tuning 对 Expected Improvement (EI) 的影响较弱。<details>
<summary>Abstract</summary>
This research introduces a novel methodology for optimizing Bayesian Neural Networks (BNNs) by synergistically integrating them with traditional machine learning algorithms such as Random Forests (RF), Gradient Boosting (GB), and Support Vector Machines (SVM). Feature integration solidifies these results by emphasizing the second-order conditions for optimality, including stationarity and positive definiteness of the Hessian matrix. Conversely, hyperparameter tuning indicates a subdued impact in improving Expected Improvement (EI), represented by EI(x). Overall, the ensemble method stands out as a robust, algorithmically optimized approach.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "Bayesian Neural Networks" (BNNs) is translated as "泛函神经网络" (pànfungen xīnǎo wǎngluò)* "Random Forests" (RF) is translated as "随机森林" (suījì sēn líng)* "Gradient Boosting" (GB) is translated as "梯度提升" (dēngdì tímshēng)* "Support Vector Machines" (SVM) is translated as "支持向量机器" (zhīchēng xiàngwù jīqì)* "Feature integration" is translated as "特征集成" (fēngjī zhùchéng)* "Hyperparameter tuning" is translated as "超参数调整" (chāojianxìa dào zhèng)* "Expected Improvement" (EI) is translated as "预期改进" (yùxì gǎngyì)Note that the translation is in Simplified Chinese, which is the most commonly used form of Chinese in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Explaining-the-Complex-Task-Reasoning-of-Large-Language-Models-with-Template-Content-Structure"><a href="#Explaining-the-Complex-Task-Reasoning-of-Large-Language-Models-with-Template-Content-Structure" class="headerlink" title="Explaining the Complex Task Reasoning of Large Language Models with Template-Content Structure"></a>Explaining the Complex Task Reasoning of Large Language Models with Template-Content Structure</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05452">http://arxiv.org/abs/2310.05452</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haotong Yang, Fanxu Meng, Zhouchen Lin, Muhan Zhang</li>
<li>for: This paper aims to provide an explanation for the exceptional generalization abilities of pre-trained large language models, and to offer a novel framework for understanding their ability to solve complex natural language tasks.</li>
<li>methods: The paper presents a hierarchical “template-content” structure for modeling answer generation in natural language tasks, and demonstrates that pre-trained models can automatically decompose tasks into constituent steps during autoregressive generation through language modeling on a sufficiently large corpus.</li>
<li>results: The paper shows that practical models exhibit different behaviors for “template” and “content” providing support for the proposed modeling, and offers an explanatory tool for the complex reasoning abilities of large language models from the perspective of modeling autoregressive generation tasks.<details>
<summary>Abstract</summary>
The continuous evolution of pre-trained large language models with ever-growing parameters and corpus sizes has augmented their capacity to solve complex tasks. This ability, which obviates the necessity for task-specific training or fine-tuning, relies on providing the model with a language description or some task exemplars -- referred to the prompt -- that guide the desired autoregressive generation. Despite the remarkable success, the underlying mechanisms that facilitate such exceptional generalization abilities remain an open question. In this paper, we present a novel framework that formally conceptualizes answer generation for complex natural language tasks as a hierarchical ``template-content'' structure. According to our modeling, there exist pre-trained models that can automatically decompose tasks into constituent steps during autoregressive generation, through language modeling on a sufficiently large corpus, thereby solving them. Our framework offers an explanatory tool for the complex reasoning abilities of large language models from the perspective of modeling autoregressive generation tasks. Our experiments show that practical models exhibit different behaviors for ``template'' and ``content'' providing support for our modeling.
</details>
<details>
<summary>摘要</summary>
大型自然语言模型的不断演化和参数的增加以及训练数据的增加，使得这些模型可以更好地解决复杂任务。这种能力，不需要任务特定的训练或微调，通过给模型提供语言描述或一些任务示例（即提示）来引导潜在的自然语言生成。虽然这些成果很出色，但是这些成果的基础机制仍然是一个开放的问题。在这篇论文中，我们提出了一种新的框架，它正式地概括了复杂自然语言任务的回答生成为一个层次结构。根据我们的模型，存在一些预训练模型可以在生成过程中自动将任务 decomposes into constituent steps，通过对 sufficiently large corpus进行语言模型化，以解决任务。我们的框架提供了对大语言模型的复杂逻辑能力的解释工具，从概念生成任务的角度出发。我们的实验表明，实际模型在“模板”和“内容”提供支持，这支持我们的模型。
</details></li>
</ul>
<hr>
<h2 id="Replication-of-Multi-agent-Reinforcement-Learning-for-the-“Hide-and-Seek”-Problem"><a href="#Replication-of-Multi-agent-Reinforcement-Learning-for-the-“Hide-and-Seek”-Problem" class="headerlink" title="Replication of Multi-agent Reinforcement Learning for the “Hide and Seek” Problem"></a>Replication of Multi-agent Reinforcement Learning for the “Hide and Seek” Problem</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05430">http://arxiv.org/abs/2310.05430</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haider Kamal, Muaz A. Niazi, Hammad Afzal</li>
<li>for: 本研究的目的是提高隐藏者（Hider）的搜索策略，以增强其在复杂环境中的运动和搜索能力。</li>
<li>methods: 本研究使用了补偿学习（Reinforcement Learning），利用奖励函数和超参数来生成策略。</li>
<li>results: 研究发现，通过增加飞行机制，提高了隐藏者的机动性和搜索范围，从约2000万步到1600万步，提高了隐藏者的追踪策略。<details>
<summary>Abstract</summary>
Reinforcement learning generates policies based on reward functions and hyperparameters. Slight changes in these can significantly affect results. The lack of documentation and reproducibility in Reinforcement learning research makes it difficult to replicate once-deduced strategies. While previous research has identified strategies using grounded maneuvers, there is limited work in more complex environments. The agents in this study are simulated similarly to Open Al's hider and seek agents, in addition to a flying mechanism, enhancing their mobility, and expanding their range of possible actions and strategies. This added functionality improves the Hider agents to develop a chasing strategy from approximately 2 million steps to 1.6 million steps and hiders
</details>
<details>
<summary>摘要</summary>
利用强化学习生成策略，该策略基于奖励函数和超参数。小小的变化可能会导致 significativetransformations。 reinforcement learning研究的documentación和可重现性不足，使得复制已经获得的策略具有困难。在这种研究中，我们使用了在Open Al的隐藏者和搜索者中的 Agent，同时添加了飞行机制，从而提高了隐藏者的 mobilidad和可能的动作和策略。这种添加的功能使得隐藏者可以开发追踪策略，从约2000万步提高到1600万步。
</details></li>
</ul>
<hr>
<h2 id="Divide-and-Ensemble-Progressively-Learning-for-the-Unknown"><a href="#Divide-and-Ensemble-Progressively-Learning-for-the-Unknown" class="headerlink" title="Divide and Ensemble: Progressively Learning for the Unknown"></a>Divide and Ensemble: Progressively Learning for the Unknown</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05425">http://arxiv.org/abs/2310.05425</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hu Zhang, Xin Shen, Heming Du, Huiqiang Chen, Chen Liu, Hongwei Sheng, Qingzheng Xu, MD Wahiduzzaman Khan, Qingtao Yu, Tianqing Zhu, Scott Chapman, Zi Huang, Xin Yu<br>for:* 这个论文是为了解决蔬菜营养不足的问题，提出了一种基于分类的方法来进行识别。methods:* 这个方法使用了分类器 ensemble 和 pseudo-labeling 技术来进行识别。results:* 这个方法在测试集上得到了93.6%的 Top-1 测试精度（94.0% 在 WW2020 上和 93.2% 在 WR2021 上），并在 Deep Nutrient Deficiency Challenge 中获得了第一名。<details>
<summary>Abstract</summary>
In the wheat nutrient deficiencies classification challenge, we present the DividE and EnseMble (DEEM) method for progressive test data predictions. We find that (1) test images are provided in the challenge; (2) samples are equipped with their collection dates; (3) the samples of different dates show notable discrepancies. Based on the findings, we partition the dataset into discrete groups by the dates and train models on each divided group. We then adopt the pseudo-labeling approach to label the test data and incorporate those with high confidence into the training set. In pseudo-labeling, we leverage models ensemble with different architectures to enhance the reliability of predictions. The pseudo-labeling and ensembled model training are iteratively conducted until all test samples are labeled. Finally, the separated models for each group are unified to obtain the model for the whole dataset. Our method achieves an average of 93.6\% Top-1 test accuracy~(94.0\% on WW2020 and 93.2\% on WR2021) and wins the 1$st$ place in the Deep Nutrient Deficiency Challenge~\footnote{https://cvppa2023.github.io/challenges/}.
</details>
<details>
<summary>摘要</summary>
在小麦营养不足分类挑战中，我们提出了分类测试数据进行进行分组的DEEM方法（DividE和Ensemble）。我们发现：1. 测试图像提供给挑战；2. 样本具有收集日期信息；3. 不同日期的样本存在明显的差异。根据这些发现，我们将数据集分成不同日期的分组，并在每个分组上训练模型。然后，我们采用 Pseudo-labeling 方法来标注测试数据，并将高信任性的预测结果包含到训练集中。在 Pseudo-labeling 中，我们利用不同架构的模型 ensemble 以提高预测的可靠性。这些pseudo-labeling和 ensemble 模型训练是相互进行的，直到所有测试样本都被标注为止。最后，我们将每个组的模型集成起来，以获得整个数据集的模型。我们的方法实现了 Top-1 测试准确率的平均值为 93.6%（94.0% 在 WW2020 和 93.2% 在 WR2021），并在 Deep Nutrient Deficiency Challenge 中获得了第一名。
</details></li>
</ul>
<hr>
<h2 id="Humanoid-Agents-Platform-for-Simulating-Human-like-Generative-Agents"><a href="#Humanoid-Agents-Platform-for-Simulating-Human-like-Generative-Agents" class="headerlink" title="Humanoid Agents: Platform for Simulating Human-like Generative Agents"></a>Humanoid Agents: Platform for Simulating Human-like Generative Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05418">http://arxiv.org/abs/2310.05418</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/humanoidagents/humanoidagents">https://github.com/humanoidagents/humanoidagents</a></li>
<li>paper_authors: Zhilin Wang, Yu Ying Chiu, Yu Cheung Chiu</li>
<li>for: 这篇论文旨在提出一种基于人类行为的生成人工智能系统，以便更好地模拟人类行为。</li>
<li>methods: 该论文使用了三种系统一类处理元素：基本需求（如饥饿、健康和能量）、情感和关系质量，以导引生成人工智能agent behave更加人类化。</li>
<li>results: 该系统能够通过这些动态元素来适应每天的活动和对其他代理的交流，并且经验证了其效果。此外，该系统还可以扩展到不同的设定和其他影响人类行为的因素（如同情、道德价值和文化背景）。<details>
<summary>Abstract</summary>
Just as computational simulations of atoms, molecules and cells have shaped the way we study the sciences, true-to-life simulations of human-like agents can be valuable tools for studying human behavior. We propose Humanoid Agents, a system that guides Generative Agents to behave more like humans by introducing three elements of System 1 processing: Basic needs (e.g. hunger, health and energy), Emotion and Closeness in Relationships. Humanoid Agents are able to use these dynamic elements to adapt their daily activities and conversations with other agents, as supported with empirical experiments. Our system is designed to be extensible to various settings, three of which we demonstrate, as well as to other elements influencing human behavior (e.g. empathy, moral values and cultural background). Our platform also includes a Unity WebGL game interface for visualization and an interactive analytics dashboard to show agent statuses over time. Our platform is available on https://www.humanoidagents.com/ and code is on https://github.com/HumanoidAgents/HumanoidAgents
</details>
<details>
<summary>摘要</summary>
“computational simulations of atoms、molecules和 cells 已经影响了我们研究科学的方法，true-to-life simulations of human-like agents 可以是我们研究人类行为的有用工具。我们提出了人工智能代理人系统（Humanoid Agents），它将引入系统1处理中的三个元素：基本需求（例如饥饿、健康和能量）、情感和关系的亲密度。人工智能代理人可以通过这些动态元素来适应每天的活动和与其他代理人的对话，并且经过实验支持。我们的系统可以扩展到不同的设定，包括三个示例，以及其他影响人类行为的元素（例如共关、道德价值和文化背景）。我们的平台还包括Unity WebGL游戏界面 для可视化和互动分析亮点，以及跟踪代理人的时间变化。我们的平台可以在 <https://www.humanoidagents.com/> 上运行，代码可以在 <https://github.com/HumanoidAgents/HumanoidAgents> 上找到。”Note: Please keep in mind that the translation is done using Google Translate, and may not be perfect or entirely accurate.
</details></li>
</ul>
<hr>
<h2 id="Ethics-of-Artificial-Intelligence-and-Robotics-in-the-Architecture-Engineering-and-Construction-Industry"><a href="#Ethics-of-Artificial-Intelligence-and-Robotics-in-the-Architecture-Engineering-and-Construction-Industry" class="headerlink" title="Ethics of Artificial Intelligence and Robotics in the Architecture, Engineering, and Construction Industry"></a>Ethics of Artificial Intelligence and Robotics in the Architecture, Engineering, and Construction Industry</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05414">http://arxiv.org/abs/2310.05414</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ci-Jyun Liang, Thai-Hoa Le, Youngjib Ham, Bharadwaj R. K. Mantha, Marvin H. Cheng, Jacob J. Lin<br>for: This research paper focuses on the ethical considerations of AI and robotics adoption in the architecture, engineering, and construction (AEC) industry.methods: The paper systematically reviews existing literature on AI and robotics research in the AEC industry, identifying key ethical issues and research topics.results: The paper identifies nine key ethical issues, including job loss, data privacy, and liability, and provides thirteen research topics for future study. It also highlights current challenges and knowledge gaps in the field, and provides recommendations for future research directions.<details>
<summary>Abstract</summary>
Artificial intelligence (AI) and robotics research and implementation emerged in the architecture, engineering, and construction (AEC) industry to positively impact project efficiency and effectiveness concerns such as safety, productivity, and quality. This shift, however, warrants the need for ethical considerations of AI and robotics adoption due to its potential negative impacts on aspects such as job security, safety, and privacy. Nevertheless, this did not receive sufficient attention, particularly within the academic community. This research systematically reviews AI and robotics research through the lens of ethics in the AEC community for the past five years. It identifies nine key ethical issues namely job loss, data privacy, data security, data transparency, decision-making conflict, acceptance and trust, reliability and safety, fear of surveillance, and liability, by summarizing existing literature and filtering it further based on its AEC relevance. Furthermore, thirteen research topics along the process were identified based on existing AEC studies that had direct relevance to the theme of ethics in general and their parallels are further discussed. Finally, the current challenges and knowledge gaps are discussed and seven specific future research directions are recommended. This study not only signifies more stakeholder awareness of this important topic but also provides imminent steps towards safer and more efficient realization.
</details>
<details>
<summary>摘要</summary>
人工智能（AI）和机器人技术在建筑、工程和建筑（AEC）行业的研究和应用已经出现，以提高项目效率和质量的问题。但是，这种转变也需要考虑AI和机器人的伦理问题，因为它们可能对工作安全、隐私和其他方面产生负面影响。然而，这一点在学术界并未得到充分关注，特别是在AEC领域。这项研究系统性地查看了AEC社区过去五年的AI和机器人研究，并Identified nine key ethical issues，namely job loss, data privacy, data security, data transparency, decision-making conflict, acceptance and trust, reliability and safety, fear of surveillance, and liability。此外，这些研究还标识出了13个相关的研究主题，包括数据隐私、数据安全、决策冲突、接受和信任、可靠性和安全、恐慌监测和责任。最后，这项研究讨论了当前的挑战和知识漏洞，并建议七个未来研究方向。这项研究不仅增加了参与者对这个重要话题的意识，而且还提供了更安全和效率的实现方法。
</details></li>
</ul>
<hr>
<h2 id="Causal-Reasoning-through-Two-Layers-of-Cognition-for-Improving-Generalization-in-Visual-Question-Answering"><a href="#Causal-Reasoning-through-Two-Layers-of-Cognition-for-Improving-Generalization-in-Visual-Question-Answering" class="headerlink" title="Causal Reasoning through Two Layers of Cognition for Improving Generalization in Visual Question Answering"></a>Causal Reasoning through Two Layers of Cognition for Improving Generalization in Visual Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05410">http://arxiv.org/abs/2310.05410</a></li>
<li>repo_url: None</li>
<li>paper_authors: Trang Nguyen, Naoaki Okazaki</li>
<li>For: 提高Visual Question Answering（VQA）模型的泛化能力，使其能够回答图像问题，并考虑到训练分布之外的上下文。* Methods: 提出了Cognitive pathways VQA（CopVQA）模型，通过强调 causal reasoning 因素来提高多Modal 预测。 CopVQA 首先创建了多条可能的 causal reasoning 流程，然后将每个阶段的责任划分给独立的专家和认知组件（CC）。最后，模型优先选择由两个 CC 执行的答案预测，而忽略由单个 CC 生成的答案。* Results: 对实际生活和医疗数据进行了实验，证明了 CopVQA 可以提高 VQA 性能和泛化性，并在不同的基线和领域上达到新的州OF-THE-ART 水平，而且模型规模只是当前 SOTA 的一半。<details>
<summary>Abstract</summary>
Generalization in Visual Question Answering (VQA) requires models to answer questions about images with contexts beyond the training distribution. Existing attempts primarily refine unimodal aspects, overlooking enhancements in multimodal aspects. Besides, diverse interpretations of the input lead to various modes of answer generation, highlighting the role of causal reasoning between interpreting and answering steps in VQA. Through this lens, we propose Cognitive pathways VQA (CopVQA) improving the multimodal predictions by emphasizing causal reasoning factors. CopVQA first operates a pool of pathways that capture diverse causal reasoning flows through interpreting and answering stages. Mirroring human cognition, we decompose the responsibility of each stage into distinct experts and a cognition-enabled component (CC). The two CCs strategically execute one expert for each stage at a time. Finally, we prioritize answer predictions governed by pathways involving both CCs while disregarding answers produced by either CC, thereby emphasizing causal reasoning and supporting generalization. Our experiments on real-life and medical data consistently verify that CopVQA improves VQA performance and generalization across baselines and domains. Notably, CopVQA achieves a new state-of-the-art (SOTA) on PathVQA dataset and comparable accuracy to the current SOTA on VQA-CPv2, VQAv2, and VQA RAD, with one-fourth of the model size.
</details>
<details>
<summary>摘要</summary>
通用化在视觉问答（VQA）中需要模型能够回答图像上的问题，并且考虑到训练分布之外的上下文。现有的尝试主要是对单模型方面进行精细调整，忽略了多模型方面的改进。此外，图像的多种解释会导致多种答案生成，这 highlights 了在解释和回答步骤之间的 causal reasoning 的角色。基于这个视角，我们提出了认知路径 VQA（CopVQA），它可以提高多模型预测的准确率。CopVQA 的实现方式是首先建立一个路径 pool，用于捕捉不同的 causal reasoning 流程。这与人类认知的层次结构相似，我们将解释和回答的责任分别划分为多个专家和一个认知能力Component（CC）。两个 CC 采用不同的策略来逐一执行每个专家，以便更好地捕捉 causal reasoning 的关系。最后，我们优先支持由多个 CC 共同执行的答案预测，而不是由单个 CC 生成的答案，以强调 causal reasoning 的重要性并且提高泛化能力。我们在真实生活和医疗数据上进行了实验，结果表明 CopVQA 可以提高 VQA 性能和泛化能力，并且在不同的基eline和领域上具有一致的表现。特别是，CopVQA 在 PathVQA 数据集上达到了新的状态态（SOTA），与当前 SOTA 在 VQA-CPv2、VQAv2 和 VQA RAD 数据集上的精度相似，仅使用一半的模型大小。
</details></li>
</ul>
<hr>
<h2 id="CAMEL2-Enhancing-weakly-supervised-learning-for-histopathology-images-by-incorporating-the-significance-ratio"><a href="#CAMEL2-Enhancing-weakly-supervised-learning-for-histopathology-images-by-incorporating-the-significance-ratio" class="headerlink" title="CAMEL2: Enhancing weakly supervised learning for histopathology images by incorporating the significance ratio"></a>CAMEL2: Enhancing weakly supervised learning for histopathology images by incorporating the significance ratio</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05394">http://arxiv.org/abs/2310.05394</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ThoroughFuture/CAMEL2">https://github.com/ThoroughFuture/CAMEL2</a></li>
<li>paper_authors: Gang Xu, Shuhao Wang, Lingyu Zhao, Xiao Chen, Tongwei Wang, Lang Wang, Zhenwei Luo, Dahan Wang, Zewen Zhang, Aijun Liu, Wei Ba, Zhigang Song, Huaiyin Shi, Dingrong Zhong, Jianpeng Ma</li>
<li>for:  Histopathology image analysis for cancer diagnosis</li>
<li>methods:  Weakly supervised learning methods with coarse-grained labels at the image level</li>
<li>results:  Comparable performance to fully supervised baselines in both instance- and slide-level classifications, with the help of 5,120x5,120 image-level binary annotations that are easy to annotate.Here’s the summary in Traditional Chinese text:</li>
<li>for:  histopathology图像分析 для癌症诊断</li>
<li>methods:  weakly supervised learning方法，仅需 coarse-grained labels at the image level</li>
<li>results: 与完全监督基eline相比，在 both instance- 和 slide-level classification中 achieve comparable performance，仅需 5,120x5,120个易于annotate的image-level binary annotations。<details>
<summary>Abstract</summary>
Histopathology image analysis plays a crucial role in cancer diagnosis. However, training a clinically applicable segmentation algorithm requires pathologists to engage in labour-intensive labelling. In contrast, weakly supervised learning methods, which only require coarse-grained labels at the image level, can significantly reduce the labeling efforts. Unfortunately, while these methods perform reasonably well in slide-level prediction, their ability to locate cancerous regions, which is essential for many clinical applications, remains unsatisfactory. Previously, we proposed CAMEL, which achieves comparable results to those of fully supervised baselines in pixel-level segmentation. However, CAMEL requires 1,280x1,280 image-level binary annotations for positive WSIs. Here, we present CAMEL2, by introducing a threshold of the cancerous ratio for positive bags, it allows us to better utilize the information, consequently enabling us to scale up the image-level setting from 1,280x1,280 to 5,120x5,120 while maintaining the accuracy. Our results with various datasets, demonstrate that CAMEL2, with the help of 5,120x5,120 image-level binary annotations, which are easy to annotate, achieves comparable performance to that of a fully supervised baseline in both instance- and slide-level classifications.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="CCAE-A-Corpus-of-Chinese-based-Asian-Englishes"><a href="#CCAE-A-Corpus-of-Chinese-based-Asian-Englishes" class="headerlink" title="CCAE: A Corpus of Chinese-based Asian Englishes"></a>CCAE: A Corpus of Chinese-based Asian Englishes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05381">http://arxiv.org/abs/2310.05381</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jacklanda/CCAE">https://github.com/jacklanda/CCAE</a></li>
<li>paper_authors: Yang Liu, Melissa Xiaohui Qin, Long Wang, Chao Huang</li>
<li>for: 这篇论文是为了创建一个多变体资料库，用于研究亚洲英语。</li>
<li>methods: 这篇论文使用了NLP技术，创建了一个基于中文的亚洲英语多变体资料库，包括六个中文基于的亚洲英语变体。</li>
<li>results: 这篇论文提供了一个448万个单词的448万个网页文档，来自六个区域，并且这些数据可以用于语言模型的训练和下游任务，这将为亚洲英语研究提供巨大的研究 potential。<details>
<summary>Abstract</summary>
Language models have been foundations in various scenarios of NLP applications, but it has not been well applied in language variety studies, even for the most popular language like English. This paper represents one of the few initial efforts to utilize the NLP technology in the paradigm of World Englishes, specifically in creating a multi-variety corpus for studying Asian Englishes. We present an overview of the CCAE -- Corpus of Chinese-based Asian English, a suite of corpora comprising six Chinese-based Asian English varieties. It is based on 340 million tokens in 448 thousand web documents from six regions. The ontology of data would make the corpus a helpful resource with enormous research potential for Asian Englishes (especially for Chinese Englishes for which there has not been a publicly accessible corpus yet so far) and an ideal source for variety-specific language modeling and downstream tasks, thus setting the stage for NLP-based World Englishes studies. And preliminary experiments on this corpus reveal the practical value of CCAE. Finally, we make CCAE available at \href{https://huggingface.co/datasets/CCAE/CCAE-Corpus}{this https URL}.
</details>
<details>
<summary>摘要</summary>
受欢迎的语言模型在各种自然语言处理（NLP）应用场景中发挥了重要作用，但它们在语言多样性研究中尚未得到广泛应用，即使是最受欢迎的语言之一的英语。这篇论文是一个初始尝试，利用NLP技术来探索世界英语的多样性，具体来说是创建一个多种英语语料库，用于研究亚洲英语。我们介绍了CCAE——中基于英语的亚洲英语词库，这是一个包含6种中基于英语的亚洲英语变体的suite of corpora，基于3.4亿个字的448万个网页文档。这些数据的 ontology 使得这个词库成为了研究亚洲英语（特别是中英语）的有用资源，以及下游任务的理想来源，因此设置了NPLT-based World Englishes studies的场景。而我们的初步实验表明，CCAE 具有实际的价值。最后，我们将CCAE 公布在 <https://huggingface.co/datasets/CCAE/CCAE-Corpus> 这个https URL 上。
</details></li>
</ul>
<hr>
<h2 id="Quantum-Bayesian-Optimization"><a href="#Quantum-Bayesian-Optimization" class="headerlink" title="Quantum Bayesian Optimization"></a>Quantum Bayesian Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05373">http://arxiv.org/abs/2310.05373</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/daizhongxiang/Quantum_Bayesian_Optimization">https://github.com/daizhongxiang/Quantum_Bayesian_Optimization</a></li>
<li>paper_authors: Zhongxiang Dai, Gregory Kang Ruey Lau, Arun Verma, Yao Shu, Bryan Kian Hsiang Low, Patrick Jaillet</li>
<li>for: 优化复杂的黑盒函数（black-box function）</li>
<li>methods: 使用量子计算机（quantum computer）和 Gaussian process（Gaussian process）实现 Bayesian optimization（BO）</li>
<li>results: 实现了对于非线性奖励函数的优化，并且在理论上达到了 O(polylog T) 的 regret upper bound，比 классиical BO 下的lower bound Omega(sqrt(T)) 更小。<details>
<summary>Abstract</summary>
Kernelized bandits, also known as Bayesian optimization (BO), has been a prevalent method for optimizing complicated black-box reward functions. Various BO algorithms have been theoretically shown to enjoy upper bounds on their cumulative regret which are sub-linear in the number T of iterations, and a regret lower bound of Omega(sqrt(T)) has been derived which represents the unavoidable regrets for any classical BO algorithm. Recent works on quantum bandits have shown that with the aid of quantum computing, it is possible to achieve tighter regret upper bounds better than their corresponding classical lower bounds. However, these works are restricted to either multi-armed or linear bandits, and are hence not able to solve sophisticated real-world problems with non-linear reward functions. To this end, we introduce the quantum-Gaussian process-upper confidence bound (Q-GP-UCB) algorithm. To the best of our knowledge, our Q-GP-UCB is the first BO algorithm able to achieve a regret upper bound of O(polylog T), which is significantly smaller than its regret lower bound of Omega(sqrt(T)) in the classical setting. Moreover, thanks to our novel analysis of the confidence ellipsoid, our Q-GP-UCB with the linear kernel achieves a smaller regret than the quantum linear UCB algorithm from the previous work. We use simulations, as well as an experiment using a real quantum computer, to verify that the theoretical quantum speedup achieved by our Q-GP-UCB is also potentially relevant in practice.
</details>
<details>
<summary>摘要</summary>
kernelized bandits，也称为 bayesian optimization (BO)，已经是优化复杂黑盒奖励函数的常用方法。多种 BO 算法已经有理论上证明了每个迭代 T 的累累 regret 是下线的，而 Omega(sqrt(T)) 的 regret 下界则表示任何 classical BO 算法不可避免的 regret。现代量子bandits 研究表明，通过量子计算机的帮助，可以超过其对应的 classical 下界。然而，这些工作都是限制在多重武器或线性 bandits 上，因此无法解决复杂的实际问题。为此，我们介绍了量子- Gaussian 过程-上界 bound (Q-GP-UCB) 算法。根据我们所知，我们的 Q-GP-UCB 算法可以达到 O(polylog T) 的 regret Upper bound，这比 classical 设置的 Omega(sqrt(T)) 下界要小得多。此外，我们的新的 confidence ellipsoid 分析表明，我们的 Q-GP-UCB 算法使用线性 kernel 时的 regret 小于前一个工作中的量子线性 UCB 算法。我们使用 simulations 以及一个使用真实量子计算机的实验，以验证我们的理论上的量子速度提升也是在实践中有可能的。
</details></li>
</ul>
<hr>
<h2 id="Measuring-Acoustics-with-Collaborative-Multiple-Agents"><a href="#Measuring-Acoustics-with-Collaborative-Multiple-Agents" class="headerlink" title="Measuring Acoustics with Collaborative Multiple Agents"></a>Measuring Acoustics with Collaborative Multiple Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05368">http://arxiv.org/abs/2310.05368</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yyf17/MACMA">https://github.com/yyf17/MACMA</a></li>
<li>paper_authors: Yinfeng Yu, Changan Chen, Lele Cao, Fangkai Yang, Fuchun Sun</li>
<li>for:  This paper aims to improve the efficiency and accuracy of measuring environment acoustics using multiple robots.</li>
<li>methods: The paper proposes using two robots to actively move and emit&#x2F;receive sweep signals to measure the environment’s acoustics, and trains them using a collaborative multi-agent policy to explore the environment while minimizing prediction error.</li>
<li>results: The robots learn to collaborate and move to explore the environment acoustics while minimizing the prediction error, demonstrating the effectiveness of the proposed method.<details>
<summary>Abstract</summary>
As humans, we hear sound every second of our life. The sound we hear is often affected by the acoustics of the environment surrounding us. For example, a spacious hall leads to more reverberation. Room Impulse Responses (RIR) are commonly used to characterize environment acoustics as a function of the scene geometry, materials, and source/receiver locations. Traditionally, RIRs are measured by setting up a loudspeaker and microphone in the environment for all source/receiver locations, which is time-consuming and inefficient. We propose to let two robots measure the environment's acoustics by actively moving and emitting/receiving sweep signals. We also devise a collaborative multi-agent policy where these two robots are trained to explore the environment's acoustics while being rewarded for wide exploration and accurate prediction. We show that the robots learn to collaborate and move to explore environment acoustics while minimizing the prediction error. To the best of our knowledge, we present the very first problem formulation and solution to the task of collaborative environment acoustics measurements with multiple agents.
</details>
<details>
<summary>摘要</summary>
人类生活中每秒都听到声音。声音我们听到常常受到环境的折射影响。例如，一个大厅会导致更多的延 reverberation。 Room Impulse Responses（RIR）是用于描述环境声学特性的函数，其中包括场景几何学、材料和源/接收器位置。传统上，RIRs通过在环境中设置 loudspeaker 和 microphone 来测量，这是时间consuming 和不效环境。我们提议使用两个机器人来测量环境的声学特性，它们通过活动移动和发送/接收扫描信号来测量。我们还开发了一种多agent 协同策略，这两个机器人在环境中探索声学特性，同时被奖励宽泛探索和准确预测。我们显示这两个机器人可以协同工作，在最小化预测错误的情况下探索环境声学特性。根据我们所知，我们提出了首个多agent 环境声学测量问题的问题与解决方案。
</details></li>
</ul>
<hr>
<h2 id="Molecular-De-Novo-Design-through-Transformer-based-Reinforcement-Learning"><a href="#Molecular-De-Novo-Design-through-Transformer-based-Reinforcement-Learning" class="headerlink" title="Molecular De Novo Design through Transformer-based Reinforcement Learning"></a>Molecular De Novo Design through Transformer-based Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05365">http://arxiv.org/abs/2310.05365</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tao Feng, Pengcheng Xu, Tianfan Fu, Siddhartha Laghuvarapu, Jimeng Sun</li>
<li>for: 本研究旨在利用Transformer驱动的生成模型进行分子 де novo设计。</li>
<li>methods: 我们的模型可以通过Transformer进行高效的序列学习，并且可以生成具有欲要性能的分子结构。与传统的RNN驱动模型相比，我们的方法可以更好地捕捉分子结构序列中的长期依赖关系。</li>
<li>results: 我们的模型在多个任务上表现出色，包括生成查询结构的同分子和生成具有特定属性的分子。与基eline的RNN驱动方法相比，我们的方法显著提高了模型的性能。我们的方法可以用于骨架跳转、库扩展和预测高活性分子。<details>
<summary>Abstract</summary>
In this work, we introduce a method to fine-tune a Transformer-based generative model for molecular de novo design. Leveraging the superior sequence learning capacity of Transformers over Recurrent Neural Networks (RNNs), our model can generate molecular structures with desired properties effectively. In contrast to the traditional RNN-based models, our proposed method exhibits superior performance in generating compounds predicted to be active against various biological targets, capturing long-term dependencies in the molecular structure sequence. The model's efficacy is demonstrated across numerous tasks, including generating analogues to a query structure and producing compounds with particular attributes, outperforming the baseline RNN-based methods. Our approach can be used for scaffold hopping, library expansion starting from a single molecule, and generating compounds with high predicted activity against biological targets.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们介绍了一种方法，用于微调基于Transformer的生成模型，以实现分子的德诺vo计划。我们利用Transformer对序列学习的优势，可以效果地生成具有所需性能的分子结构。与传统的RNN基本方法相比，我们的提议方法在生成具有不同生物目标活性的分子结构方面表现出色，捕捉分子结构序列中长期依赖关系。我们的方法在多个任务上展现出优势，包括生成查询结构的同化体和生成具有特定属性的分子，超越基eline RNN基本方法。我们的方法可以用于跳跃架构、从单个分子开始扩大图书馆，以及预测高活性 against生物目标的分子。
</details></li>
</ul>
<hr>
<h2 id="Universal-Multi-modal-Entity-Alignment-via-Iteratively-Fusing-Modality-Similarity-Paths"><a href="#Universal-Multi-modal-Entity-Alignment-via-Iteratively-Fusing-Modality-Similarity-Paths" class="headerlink" title="Universal Multi-modal Entity Alignment via Iteratively Fusing Modality Similarity Paths"></a>Universal Multi-modal Entity Alignment via Iteratively Fusing Modality Similarity Paths</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05364">http://arxiv.org/abs/2310.05364</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/blzhu0823/pathfusion">https://github.com/blzhu0823/pathfusion</a></li>
<li>paper_authors: Bolin Zhu, Xiaoze Liu, Xin Mao, Zhuo Chen, Lingbing Guo, Tao Gui, Qi Zhang</li>
<li>for: 本研究旨在提高知识 graphs（KGs）的一体化性，通过发现多个 KGs 中相同实体对的对应关系。</li>
<li>methods: 本研究提出了PathFusion方法，包括两个主要组成部分：一是MSP模型，它通过建立实体和模式节点之间的路径来表示多个Modalities;二是IRF融合方法，它通过路径作为信息传递者，有效地融合不同Modalities中的信息。</li>
<li>results: 实验结果表明，对真实世界数据集进行测试的PathFusion方法，与现有方法相比，具有22.4%-28.9%的绝对提升（Hits@1），以及0.194-0.245的绝对提升（MRR）。<details>
<summary>Abstract</summary>
The objective of Entity Alignment (EA) is to identify equivalent entity pairs from multiple Knowledge Graphs (KGs) and create a more comprehensive and unified KG. The majority of EA methods have primarily focused on the structural modality of KGs, lacking exploration of multi-modal information. A few multi-modal EA methods have made good attempts in this field. Still, they have two shortcomings: (1) inconsistent and inefficient modality modeling that designs complex and distinct models for each modality; (2) ineffective modality fusion due to the heterogeneous nature of modalities in EA. To tackle these challenges, we propose PathFusion, consisting of two main components: (1) MSP, a unified modeling approach that simplifies the alignment process by constructing paths connecting entities and modality nodes to represent multiple modalities; (2) IRF, an iterative fusion method that effectively combines information from different modalities using the path as an information carrier. Experimental results on real-world datasets demonstrate the superiority of PathFusion over state-of-the-art methods, with 22.4%-28.9% absolute improvement on Hits@1, and 0.194-0.245 absolute improvement on MRR.
</details>
<details>
<summary>摘要</summary>
目标是实体对应（Entity Alignment，EA）是从多个知识图（Knowledge Graph，KG）中标识相应的实体对，并创建一个更加完整和统一的KG。大多数EA方法主要关注了知识图的结构性，缺乏多 modal 信息的探索。一些多模式EA方法有所进步，但它们具有两个缺陷：（1）不稳定和不效率的模式模型，通常采用复杂和特定的模型来表示每种模式；（2）不具有有效的多模式融合，由于实体对应中的多种模式具有不同的特征。为了解决这些挑战，我们提出了PathFusion，它包括两个主要组成部分：（1）MSP，一种简化实体对应过程的统一模型方法，通过构建实体和模式节点之间的路径来表示多种模式；（2）IRF，一种迭代融合方法，通过路径作为信息传递者，有效地将不同模式的信息融合在一起。实验结果表明，PathFusion比 estado-of-the-art 方法具有22.4%-28.9%的绝对改善率（Hits@1），和0.194-0.245的绝对改善率（MRR）。
</details></li>
</ul>
<hr>
<h2 id="Generalized-Neural-Collapse-for-a-Large-Number-of-Classes"><a href="#Generalized-Neural-Collapse-for-a-Large-Number-of-Classes" class="headerlink" title="Generalized Neural Collapse for a Large Number of Classes"></a>Generalized Neural Collapse for a Large Number of Classes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05351">http://arxiv.org/abs/2310.05351</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kongwanbianjinyu/Generalized-Neural-Collapse-for-a-Large-Number-of-Classes">https://github.com/kongwanbianjinyu/Generalized-Neural-Collapse-for-a-Large-Number-of-Classes</a></li>
<li>paper_authors: Jiachen Jiang, Jinxin Zhou, Peng Wang, Qing Qu, Dustin Mixon, Chong You, Zhihui Zhu</li>
<li>for: 这篇论文旨在探讨深度学习模型中的学习后层表示和分类权重的概念化，以及如何通过新的技术来提高实际深度模型的性能。</li>
<li>methods: 这篇论文使用了一种名为“普通化神经垮坏”的概念，用于描述深度学习模型中的学习后层表示和分类权重。这种概念可以帮助我们更好地理解深度学习模型的工作机理，并提供新的技术来提高模型的性能。</li>
<li>results: 这篇论文显示了在实际深度神经网络中发生的“普通化神经垮坏”现象，即在大数据集中，分类器的最小一对一 margin 是最大化的。此外，论文还提供了一系列实际和理论研究，以证明这种现象的存在性和可靠性。<details>
<summary>Abstract</summary>
Neural collapse provides an elegant mathematical characterization of learned last layer representations (a.k.a. features) and classifier weights in deep classification models. Such results not only provide insights but also motivate new techniques for improving practical deep models. However, most of the existing empirical and theoretical studies in neural collapse focus on the case that the number of classes is small relative to the dimension of the feature space. This paper extends neural collapse to cases where the number of classes are much larger than the dimension of feature space, which broadly occur for language models, retrieval systems, and face recognition applications. We show that the features and classifier exhibit a generalized neural collapse phenomenon, where the minimum one-vs-rest margins is maximized.We provide empirical study to verify the occurrence of generalized neural collapse in practical deep neural networks. Moreover, we provide theoretical study to show that the generalized neural collapse provably occurs under unconstrained feature model with spherical constraint, under certain technical conditions on feature dimension and number of classes.
</details>
<details>
<summary>摘要</summary>
神经坍塌提供了深度学习模型中学习最后层表示（即特征）以及分类器权重的简洁数学定义。这些结果不仅提供了启示，还激发了改进实际深度模型的新技术。然而，现有的实际和理论研究多集中在深度模型中的小数目类别情况下进行研究。这篇论文扩展了神经坍塌到类别数量远大于特征空间维度的情况下，这种情况广泛出现在语音识别、检索系统和人脸识别等应用中。我们显示了神经坍塌现象在实际深度神经网络中发生，并且提供了理论研究，证明在不受特征模型约束的情况下，神经坍塌在某种技术条件下发生。
</details></li>
</ul>
<hr>
<h2 id="Continuous-Invariance-Learning"><a href="#Continuous-Invariance-Learning" class="headerlink" title="Continuous Invariance Learning"></a>Continuous Invariance Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05348">http://arxiv.org/abs/2310.05348</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yong Lin, Fan Zhou, Lu Tan, Lintao Ma, Jiameng Liu, Yansu He, Yuan Yuan, Yu Liu, James Zhang, Yujiu Yang, Hao Wang</li>
<li>for: 这篇论文主要针对了连续域问题的一致学习问题，即如何通过学习不变特征来提高模型的一致性。</li>
<li>methods: 这篇论文提出了一种新的连续域一致学习方法（CIL），该方法通过测量和控制 conditional independence 来提取连续域上的不变特征。</li>
<li>results: 论文的实验结果表明，CIL 可以在各种实验数据集上（包括生产环境中的数据）与强基线相比，具有更高的一致性。<details>
<summary>Abstract</summary>
Invariance learning methods aim to learn invariant features in the hope that they generalize under distributional shifts. Although many tasks are naturally characterized by continuous domains, current invariance learning techniques generally assume categorically indexed domains. For example, auto-scaling in cloud computing often needs a CPU utilization prediction model that generalizes across different times (e.g., time of a day and date of a year), where `time' is a continuous domain index. In this paper, we start by theoretically showing that existing invariance learning methods can fail for continuous domain problems. Specifically, the naive solution of splitting continuous domains into discrete ones ignores the underlying relationship among domains, and therefore potentially leads to suboptimal performance. To address this challenge, we then propose Continuous Invariance Learning (CIL), which extracts invariant features across continuously indexed domains. CIL is a novel adversarial procedure that measures and controls the conditional independence between the labels and continuous domain indices given the extracted features. Our theoretical analysis demonstrates the superiority of CIL over existing invariance learning methods. Empirical results on both synthetic and real-world datasets (including data collected from production systems) show that CIL consistently outperforms strong baselines among all the tasks.
</details>
<details>
<summary>摘要</summary>
对于不变学习方法来说，它们的目标是学习不变特征，以便在分布转移时保持一致。虽然许多任务是自然地表示为连续领域，但当前的不变学习技术通常假设Category indexed领域。例如，云计算中的自动扩缩通常需要一个能够在不同时间（例如天时和年度）上泛化的CPU使用预测模型，其中`time'是连续领域的索引。在这篇论文中，我们开始 by theoretically showing that existing invariance learning methods can fail for continuous domain problems。Specifically, the naive solution of splitting continuous domains into discrete ones ignores the underlying relationship among domains, and therefore potentially leads to suboptimal performance。To address this challenge, we then propose Continuous Invariance Learning (CIL), which extracts invariant features across continuously indexed domains。CIL is a novel adversarial procedure that measures and controls the conditional independence between the labels and continuous domain indices given the extracted features。我们的理论分析表明CIL比既有的不变学习方法更加有利。empirical results on both synthetic and real-world datasets（包括生产系统中收集的数据）显示，CIL在所有任务中一直表现出优于强基eline。
</details></li>
</ul>
<hr>
<h2 id="SteerLM-Attribute-Conditioned-SFT-as-an-User-Steerable-Alternative-to-RLHF"><a href="#SteerLM-Attribute-Conditioned-SFT-as-an-User-Steerable-Alternative-to-RLHF" class="headerlink" title="SteerLM: Attribute Conditioned SFT as an (User-Steerable) Alternative to RLHF"></a>SteerLM: Attribute Conditioned SFT as an (User-Steerable) Alternative to RLHF</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05344">http://arxiv.org/abs/2310.05344</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yi Dong, Zhilin Wang, Makesh Narsimhan Sreedhar, Xianchao Wu, Oleksii Kuchaiev</li>
<li>for: 该研究目标是使大语言模型（LLM）更加适应人类价值观。</li>
<li>methods: 该研究使用监督精度调整（SFT）和人类反馈学习（RLHF）两个阶段。</li>
<li>results: 试验结果表明，使用SteerLM可以生成更加有用和高质量的回答，而且训练更加容易。 compare to 多种基elines。<details>
<summary>Abstract</summary>
Model alignment with human preferences is an essential step in making Large Language Models (LLMs) helpful and consistent with human values. It typically consists of supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) stages. However, RLHF faces inherent limitations stemming from a complex training setup and its tendency to align the model with implicit values that end users cannot control at run-time. Moreover, reward models in RLHF stage commonly rely on single-dimensional feedback as opposed to explicit, multifaceted signals that indicate attributes such as helpfulness, humor, and toxicity. To address these limitations, we propose SteerLM, a supervised fine-tuning method that empowers end-users to control responses during inference. SteerLM conditions responses to conform to an explicitly defined multi-dimensional set of attributes, thereby empowering a steerable AI capable of generating helpful and high-quality responses while maintaining customizability. Experiments show that SteerLM trained on open source datasets generates responses that are preferred by human and automatic evaluators to many state-of-the-art baselines trained with RLHF while being much easier to train. Try SteerLM at https://huggingface.co/nvidia/SteerLM-llama2-13B
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）的模型对齐人类偏好是一项重要步骤，以确保模型与人类价值观 align。通常包括监督微调（SFT）和人类反馈学习（RLHF）两个阶段。然而，RLHF受到内在的限制，包括复杂的训练设置和对隐藏的价值观align。此外，RLHF阶段的奖励模型通常依赖单一的反馈信号，而不是显式、多方面的信号，如帮助程度、幽默度和恶势力。为解决这些限制，我们提出了SteerLM，一种监督微调方法，使得用户可以在推理时控制响应。SteerLM Condition Responses to Conform to an Explicitly Defined Multi-Dimensional Set of Attributes, thereby Empowering a Steerable AI Capable of Generating Helpful and High-Quality Responses While Maintaining Customizability。实验显示，SteerLM在开源数据集上训练后可以跟上人类和自动评价者的首选，而且训练得非常容易。可以在https://huggingface.co/nvidia/SteerLM-llama2-13B中尝试SteerLM。
</details></li>
</ul>
<hr>
<h2 id="Investigating-Continuous-Learning-in-Spiking-Neural-Networks"><a href="#Investigating-Continuous-Learning-in-Spiking-Neural-Networks" class="headerlink" title="Investigating Continuous Learning in Spiking Neural Networks"></a>Investigating Continuous Learning in Spiking Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05343">http://arxiv.org/abs/2310.05343</a></li>
<li>repo_url: None</li>
<li>paper_authors: C. Tanner Fredieu</li>
<li>for: 这个论文探讨了使用第三代机器学习算法（叫做脉冲神经网络架构）进行连续学习的可能性，并与传统模型进行比较。</li>
<li>methods: 这个论文使用了三个阶段的实验。第一阶段是使用传输学习来训练传统模型。第二阶段使用Nengo模型库中的模型进行训练。最后，每个传统模型都被转换成了脉冲神经网络，并进行了训练。</li>
<li>results: 初步结果表明，使用SNN模型可以避免了训练过程中的迷失知识问题，但还需要进一步的研究。所有模型都能正确地识别当前的类别，但是它们都会在前一个类别上具有高于正常水平的输出概率。这表明SNN模型有潜力来超越迷失知识问题，但还需要很多的研究和改进。<details>
<summary>Abstract</summary>
In this paper, the use of third-generation machine learning, also known as spiking neural network architecture, for continuous learning was investigated and compared to conventional models. The experimentation was divided into three separate phases. The first phase focused on training the conventional models via transfer learning. The second phase trains a Nengo model from their library. Lastly, each conventional model is converted into a spiking neural network and trained. Initial results from phase 1 are inline with known knowledge about continuous learning within current machine learning literature. All models were able to correctly identify the current classes, but they would immediately see a sharp performance drop in previous classes due to catastrophic forgetting. However, the SNN models were able to retain some information about previous classes. Although many of the previous classes were still identified as the current trained classes, the output probabilities showed a higher than normal value to the actual class. This indicates that the SNN models do have potential to overcome catastrophic forgetting but much work is still needed.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，使用第三代机器学习技术，即脉冲神经网络架构，进行连续学习的可能性进行了调查和比较，并与传统模型进行对比。实验分为三个阶段。第一阶段是通过转移学习训练传统模型。第二阶段使用Nengo模型库中的模型进行训练。最后，每个传统模型都被转换成脉冲神经网络并进行训练。初果显示，第一阶段的结果与现有机器学习文献中关于连续学习的知识一致。所有模型都能正确地识别当前的类别，但它们都会因为恐慌遗忘而显示出过去类别的性能下降。然而，SNN模型能够保持一些过去类别的信息。虽然许多过去类别仍然被识别为当前训练类别，但输出概率显示高于实际类别的值。这表示SNN模型有可能超越恐慌遗忘，但还需要进一步的研究。
</details></li>
</ul>
<hr>
<h2 id="A-Critical-Look-at-Classic-Test-Time-Adaptation-Methods-in-Semantic-Segmentation"><a href="#A-Critical-Look-at-Classic-Test-Time-Adaptation-Methods-in-Semantic-Segmentation" class="headerlink" title="A Critical Look at Classic Test-Time Adaptation Methods in Semantic Segmentation"></a>A Critical Look at Classic Test-Time Adaptation Methods in Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05341">http://arxiv.org/abs/2310.05341</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chang’an Yi, Haotian Chen, Yifan Zhang, Yonghui Xu, Lizhen Cui</li>
<li>for: 这篇研究的目的是探讨测试时适应（TTA）在Semantic Segmentation任务中的应用。</li>
<li>methods: 这篇研究使用了 Classic TTA 方法，包括批量 normalization 更新策略和教师学生结构，以测试它们是否能够有效地适应 Semantic Segmentation 任务中的数据分布变化。</li>
<li>results: 研究结果显示，这些 Classic TTA 方法在 Semantic Segmentation 任务中的表现不如预期，特别是在批量 normalization 更新策略和教师学生结构方面。此外，Segmentation TTA 还面临着严重的长尾问题，这问题比 классификаation TTA 更加复杂。<details>
<summary>Abstract</summary>
Test-time adaptation (TTA) aims to adapt a model, initially trained on training data, to potential distribution shifts in the test data. Most existing TTA studies, however, focus on classification tasks, leaving a notable gap in the exploration of TTA for semantic segmentation. This pronounced emphasis on classification might lead numerous newcomers and engineers to mistakenly assume that classic TTA methods designed for classification can be directly applied to segmentation. Nonetheless, this assumption remains unverified, posing an open question. To address this, we conduct a systematic, empirical study to disclose the unique challenges of segmentation TTA, and to determine whether classic TTA strategies can effectively address this task. Our comprehensive results have led to three key observations. First, the classic batch norm updating strategy, commonly used in classification TTA, only brings slight performance improvement, and in some cases it might even adversely affect the results. Even with the application of advanced distribution estimation techniques like batch renormalization, the problem remains unresolved. Second, the teacher-student scheme does enhance training stability for segmentation TTA in the presence of noisy pseudo-labels. However, it cannot directly result in performance improvement compared to the original model without TTA. Third, segmentation TTA suffers a severe long-tailed imbalance problem, which is substantially more complex than that in TTA for classification. This long-tailed challenge significantly affects segmentation TTA performance, even when the accuracy of pseudo-labels is high. In light of these observations, we conclude that TTA for segmentation presents significant challenges, and simply using classic TTA methods cannot address this problem well.
</details>
<details>
<summary>摘要</summary>
测试时适应（TTA）目的是使模型，首先在训练数据上进行训练，适应测试数据中的可能存在的分布变化。大多数现有的TTA研究， however，它们主要关注分类任务，留下了 semantic segmentation 的探索空间。这种注重分类的偏好可能导致许多新手和工程师 mistakenly assume that classic TTA methods designed for classification can be directly applied to segmentation。然而，这个假设仍未得到证明，这 constitutes an open question。为了解决这个问题，我们进行了系统性的、 empirical 的研究，以揭示 semantic segmentation TTA 中独特的挑战，并确定 classic TTA 策略是否能有效地解决这个任务。我们的全面的结果表明，有三个关键观察：1. 通常用于 classification TTA 的 batch norm 更新策略，对 semantic segmentation TTA 来说只有微scopic 的性能提高，而在一些情况下，甚至会 adversely affect the results。即使使用 advanced distribution estimation techniques like batch renormalization，问题仍未得到解决。2. teacher-student scheme 可以增强 semantic segmentation TTA 中的训练稳定性，但是它不能直接导致性能提高，与无TTA 的原始模型相比。3. semantic segmentation TTA 受到严重的长尾偏度问题困扰，这个问题比分类 TTA 更加复杂。这种长尾偏度问题会很大地影响 semantic segmentation TTA 性能，即使 pseudo-labels 的准确率很高。根据这些观察结论，我们 conclude that semantic segmentation TTA 存在 significativeschallenges，并且简单地使用 classic TTA methods 无法很好地解决这个问题。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Long-form-Text-Generation-Efficacy-with-Task-adaptive-Tokenization"><a href="#Enhancing-Long-form-Text-Generation-Efficacy-with-Task-adaptive-Tokenization" class="headerlink" title="Enhancing Long-form Text Generation Efficacy with Task-adaptive Tokenization"></a>Enhancing Long-form Text Generation Efficacy with Task-adaptive Tokenization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05317">http://arxiv.org/abs/2310.05317</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/MichiganNLP/task-adaptive_tokenization">https://github.com/MichiganNLP/task-adaptive_tokenization</a></li>
<li>paper_authors: Siyang Liu, Naihao Deng, Sahand Sabour, Yilin Jia, Minlie Huang, Rada Mihalcea</li>
<li>for: 提高长文生成性能，特别是在心理问答任务中</li>
<li>methods: 采用任务适应的Tokenization，通过多个结果的采样，使用任务特定的数据来优化抽象概率</li>
<li>results: 在中文和英文心理问答任务上，通过对特定任务的抽象进行优化，可以提高生成性能，并且使用60% fewer tokens。初步实验表明，将我们的抽象方法与大语言模型结合使用，具有扎实的前景。<details>
<summary>Abstract</summary>
We propose task-adaptive tokenization as a way to adapt the generation pipeline to the specifics of a downstream task and enhance long-form generation in mental health. Inspired by insights from cognitive science, our task-adaptive tokenizer samples variable segmentations from multiple outcomes, with sampling probabilities optimized based on task-specific data. We introduce a strategy for building a specialized vocabulary and introduce a vocabulary merging protocol that allows for the integration of task-specific tokens into the pre-trained model's tokenization step. Through extensive experiments on psychological question-answering tasks in both Chinese and English, we find that our task-adaptive tokenization approach brings a significant improvement in generation performance while using up to 60% fewer tokens. Preliminary experiments point to promising results when using our tokenization approach with very large language models.
</details>
<details>
<summary>摘要</summary>
我们提议使用任务适应式分词法来适应下游任务的特点，以提高长文本生成在心理健康领域。 drawing inspiration from cognitive science, our task-adaptive tokenizer samples variable segmentations from multiple outcomes, with sampling probabilities optimized based on task-specific data. We propose a strategy for building a specialized vocabulary and introduce a vocabulary merging protocol that allows for the integration of task-specific tokens into the pre-trained model's tokenization step. Through extensive experiments on psychological question-answering tasks in both Chinese and English, we find that our task-adaptive tokenization approach brings a significant improvement in generation performance while using up to 60% fewer tokens. Preliminary experiments suggest promising results when using our tokenization approach with very large language models.Here's the breakdown of the translation:* 我们 (wǒmen) - we* 提议 (tīyì) - propose* 使用 (shǐyòu) - use* 任务适应式 (róngyè tíyìxì) - task-adaptive* 分词法 (fēnzihòu) - tokenization* 适应 (shìyìng) - adapt* 下游 (xiàyù) - downstream* 任务 (àiwù) - task* 特点 (tèdiǎn) - specifics* 以提高 (yǐ tígāo) - to improve* 长文本 (chángwén tiě) - long-form* 生成 (shēngchǎn) - generation* 在 (zhī) - in* 心理健康 (xīn lí jīn kāng) - mental health* 领域 (lǐngyè) - field* drawing inspiration from (zhìyì zhī) - inspired by* cognitive science (xīn lí kēxíng) - cognitive science* 任务特定 (àiwù tèqì) - task-specific* 数据 (shùdà) - data* 优化 (yòujiā) - optimize* probabilities (jiào dé) - probabilities*  sampling (jiào) - sampling* 变量 (biānliàng) - variable* segmentations (jiāo biān) - segmentations* 多种 (duōzhèng) - multiple* outcomes (fāngyì) - outcomes*  integration (tōngyì) - integration* 特定 vocabulary (tèqì yǔyīn) - specialized vocabulary*  introduce (jìdǎo) - propose* a strategy (a jìdǎo) - a strategy* for building (jìdǎo) - for building* a specialized vocabulary (tèqì yǔyīn) - a specialized vocabulary* and introduce (jìdǎo) - and introduce* a vocabulary merging protocol (yǔyīn tóngxīng) - a vocabulary merging protocol* that allows for (dēng) - that allows for* the integration (tōngyì) - the integration* of task-specific tokens (àiwù zhǐxīn) - of task-specific tokens* into (dào) - into* the pre-trained model's (zhìyì zhī) - the pre-trained model's* tokenization step (tiězi xiàng) - tokenization step* Through (zhī) - through* extensive experiments (zhìyì yánjiū) - extensive experiments* on (zhī) - on* psychological question-answering (xīn lí yánsuō) - psychological question-answering* tasks (àiwù) - tasks* in (zhī) - in* both (liǎng) - both* Chinese (zhōngwén) - Chinese* and English (yīnggrēsī) - and English* we find (wǒmen jiào) - we find* a significant improvement (tóngyì zhìyì) - a significant improvement* in generation performance (chángwén tiěyì) - in generation performance* while using (yǐ) - while using* up to 60% fewer tokens (dào zhìyì) - up to 60% fewer tokens* Preliminary experiments (zhìyì yánjiū) - preliminary experiments* point to promising results (zhìyì zhìyì) - point to promising results* when using (yǐ) - when using* our tokenization approach (wǒmen tiězi xiàng) - our tokenization approach* with very large language models (dào yìjī zhīyì) - with very large language models.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/09/cs.AI_2023_10_09/" data-id="clp89do92005di7889cxvdmt3" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_10_09" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/09/cs.CL_2023_10_09/" class="article-date">
  <time datetime="2023-10-09T11:00:00.000Z" itemprop="datePublished">2023-10-09</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/09/cs.CL_2023_10_09/">cs.CL - 2023-10-09</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="GPT-who-An-Information-Density-based-Machine-Generated-Text-Detector"><a href="#GPT-who-An-Information-Density-based-Machine-Generated-Text-Detector" class="headerlink" title="GPT-who: An Information Density-based Machine-Generated Text Detector"></a>GPT-who: An Information Density-based Machine-Generated Text Detector</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06202">http://arxiv.org/abs/2310.06202</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saranya Venkatraman, Adaku Uchendu, Dongwon Lee</li>
<li>for: 本研究旨在检验语言模型和人类语言之间的差异，并提出一种基于统一信息密度原理的多类域不偏推权分类器GPT-who。</li>
<li>methods: 该分类器使用统一信息密度基本特征来模型每个语言模型和人类作者的独特统计特征，以便准确地归类作者。</li>
<li>results: 对4个大规模测试集进行评估，GPT-who比前一代统计基于分类器和非统计基于分类器的检测器（如GLTR、GPTZero、OpenAI检测器和ZeroGPT）提高了超过20%的表现。此外，GPT-who还具有较低的计算成本和可读性的优势。<details>
<summary>Abstract</summary>
The Uniform Information Density principle posits that humans prefer to spread information evenly during language production. In this work, we examine if the UID principle can help capture differences between Large Language Models (LLMs) and human-generated text. We propose GPT-who, the first psycholinguistically-aware multi-class domain-agnostic statistical-based detector. This detector employs UID-based features to model the unique statistical signature of each LLM and human author for accurate authorship attribution. We evaluate our method using 4 large-scale benchmark datasets and find that GPT-who outperforms state-of-the-art detectors (both statistical- & non-statistical-based) such as GLTR, GPTZero, OpenAI detector, and ZeroGPT by over $20$% across domains. In addition to superior performance, it is computationally inexpensive and utilizes an interpretable representation of text articles. We present the largest analysis of the UID-based representations of human and machine-generated texts (over 400k articles) to demonstrate how authors distribute information differently, and in ways that enable their detection using an off-the-shelf LM without any fine-tuning. We find that GPT-who can distinguish texts generated by very sophisticated LLMs, even when the overlying text is indiscernible.
</details>
<details>
<summary>摘要</summary>
人类偏好将信息均匀分布在语言生成中，这种现象被称为Uniform Information Density原理（UID）。在这项工作中，我们研究了UID原理是否可以捕捉不同的大语言模型（LLM）和人类生成的文本之间的差异。我们提出了GPT-who，首个心理语言学感知的多类域共通统计基础探测器。这个探测器使用UID基本特征来模型每个LLM和人类作者的唯一统计特征，以便准确地归属作者。我们使用4个大规模数据集进行评估，发现GPT-who在各个领域都高于当前最佳探测器（包括统计和非统计基础的探测器），例如GLTR、GPTZero、OpenAI探测器和ZeroGPT，以上差20%以上。此外，GPT-who还具有低计算成本和可解释的文本表示，并进行了400k篇文章的最大分析，以示出作者在分布信息的方式，以及如何使用存储库LM进行探测。我们发现GPT-who可以分辨由非常复杂的LLM生成的文本，即使文本总体看起来一样。
</details></li>
</ul>
<hr>
<h2 id="Compressing-Context-to-Enhance-Inference-Efficiency-of-Large-Language-Models"><a href="#Compressing-Context-to-Enhance-Inference-Efficiency-of-Large-Language-Models" class="headerlink" title="Compressing Context to Enhance Inference Efficiency of Large Language Models"></a>Compressing Context to Enhance Inference Efficiency of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06201">http://arxiv.org/abs/2310.06201</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liyucheng09/selective_context">https://github.com/liyucheng09/selective_context</a></li>
<li>paper_authors: Yucheng Li, Bo Dong, Chenghua Lin, Frank Guerin</li>
<li>for: 提高大语言模型（LLM）的推理效率，解决长文档和长 conversations 的计算要求增加和内存占用问题。</li>
<li>methods: 提出了一种名为选择性上下文的方法，通过识别并剔除输入上下文中的重复部分，使输入更加紧凑。</li>
<li>results: 实验结果表明，选择性上下文方法可以significantly 降低内存成本和生成时间，保持与全Context相对的性能，具体是：Context cost 减少50%，内存使用量减少36%，生成时间减少32%，而BERTscore和 faithfulness 只减少0.023和0.038。<details>
<summary>Abstract</summary>
Large language models (LLMs) achieved remarkable performance across various tasks. However, they face challenges in managing long documents and extended conversations, due to significantly increased computational requirements, both in memory and inference time, and potential context truncation when the input exceeds the LLM's fixed context length. This paper proposes a method called Selective Context that enhances the inference efficiency of LLMs by identifying and pruning redundancy in the input context to make the input more compact. We test our approach using common data sources requiring long context processing: arXiv papers, news articles, and long conversations, on tasks of summarisation, question answering, and response generation. Experimental results show that Selective Context significantly reduces memory cost and decreases generation latency while maintaining comparable performance compared to that achieved when full context is used. Specifically, we achieve a 50\% reduction in context cost, resulting in a 36\% reduction in inference memory usage and a 32\% reduction in inference time, while observing only a minor drop of .023 in BERTscore and .038 in faithfulness on four downstream applications, indicating that our method strikes a good balance between efficiency and performance.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="The-Importance-of-Prompt-Tuning-for-Automated-Neuron-Explanations"><a href="#The-Importance-of-Prompt-Tuning-for-Automated-Neuron-Explanations" class="headerlink" title="The Importance of Prompt Tuning for Automated Neuron Explanations"></a>The Importance of Prompt Tuning for Automated Neuron Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06200">http://arxiv.org/abs/2310.06200</a></li>
<li>repo_url: None</li>
<li>paper_authors: Justin Lee, Tuomas Oikarinen, Arjun Chatha, Keng-Chi Chang, Yilan Chen, Tsui-Wei Weng</li>
<li>for: 了解大语言模型（LLMs）的各个神经元的作用，以便更好地理解模型和其安全性。</li>
<li>methods: 基于之前的研究，使用大语言模型such as GPT-4来解释每个神经元的作用。 Specifically, 分析使用的提示来生成解释的效果，并改进提示的格式以提高解释质量和减少计算成本。</li>
<li>results: 通过三种不同的方法，包括自动和人工评估，证明了我们的新提示可以大幅提高神经元解释质量，同时减少计算成本。<details>
<summary>Abstract</summary>
Recent advances have greatly increased the capabilities of large language models (LLMs), but our understanding of the models and their safety has not progressed as fast. In this paper we aim to understand LLMs deeper by studying their individual neurons. We build upon previous work showing large language models such as GPT-4 can be useful in explaining what each neuron in a language model does. Specifically, we analyze the effect of the prompt used to generate explanations and show that reformatting the explanation prompt in a more natural way can significantly improve neuron explanation quality and greatly reduce computational cost. We demonstrate the effects of our new prompts in three different ways, incorporating both automated and human evaluations.
</details>
<details>
<summary>摘要</summary>
最近的进步使大语言模型（LLM）的能力得到了大幅提高，但我们对这些模型和其安全性的理解尚未随着速度进步。在这篇论文中，我们尝试更深入地理解LLM，通过研究它们的个体神经元。我们建立在之前的工作之上，证明大语言模型如GPT-4可以用于解释每个语言模型神经元的作用。我们分析推荐的提示对神经元解释质量产生的影响，并显示通过更自然的提示格式化可以显著提高神经元解释质量，同时大幅降低计算成本。我们通过三种不同的方法示出了我们新的提示的效果，包括自动和人工评估。
</details></li>
</ul>
<hr>
<h2 id="BYOC-Personalized-Few-Shot-Classification-with-Co-Authored-Class-Descriptions"><a href="#BYOC-Personalized-Few-Shot-Classification-with-Co-Authored-Class-Descriptions" class="headerlink" title="BYOC: Personalized Few-Shot Classification with Co-Authored Class Descriptions"></a>BYOC: Personalized Few-Shot Classification with Co-Authored Class Descriptions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06111">http://arxiv.org/abs/2310.06111</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arth Bohra, Govert Verkes, Artem Harutyunyan, Pascal Weinberger, Giovanni Campagna</li>
<li>for: 该论文旨在提供一种可以让用户自己建立文本分类器的新方法，以便用户可以根据自己的需求建立个性化的分类器。</li>
<li>methods: 该方法使用大语言模型（LLM），并通过用户和LLM之间的互动来帮助用户描述每个类型的核心特征。用户通过 annotating 每个少量示例来提供描述，并且 LLM 会提问有关每个示例的问题，以便帮助用户更好地描述每个类型。</li>
<li>results: 实验表明，该方法可以达到高精度（大约 82%），只使用了比较少的数据集训练。此外，在30名参与者的研究中，个性化的分类器的平均精度达到 90%，比州态艺前的方法高出 15%。<details>
<summary>Abstract</summary>
Text classification is a well-studied and versatile building block for many NLP applications. Yet, existing approaches require either large annotated corpora to train a model with or, when using large language models as a base, require carefully crafting the prompt as well as using a long context that can fit many examples. As a result, it is not possible for end-users to build classifiers for themselves. To address this issue, we propose a novel approach to few-shot text classification using an LLM. Rather than few-shot examples, the LLM is prompted with descriptions of the salient features of each class. These descriptions are coauthored by the user and the LLM interactively: while the user annotates each few-shot example, the LLM asks relevant questions that the user answers. Examples, questions, and answers are summarized to form the classification prompt. Our experiments show that our approach yields high accuracy classifiers, within 82% of the performance of models trained with significantly larger datasets while using only 1% of their training sets. Additionally, in a study with 30 participants, we show that end-users are able to build classifiers to suit their specific needs. The personalized classifiers show an average accuracy of 90%, which is 15% higher than the state-of-the-art approach.
</details>
<details>
<summary>摘要</summary>
文本分类是一个已经广泛研究并且具有多种应用的基础模块。然而，现有的方法需要大量的标注数据来训练一个模型，或者使用大型语言模型为基础，并且需要考虑制定的提示和长Context。这使得普通用户无法建立自己的分类器。为解决这个问题，我们提出了一种新的几个示例文本分类方法使用LLM。而不是几个示例，LLM被提示了每个类型的突出特征的描述。这些描述由用户和LLM共同编写：用户在每个几个示例中注解，LLM则问到有关的问题，用户回答。示例、问题和答案被总结为分类提示。我们的实验表明，我们的方法可以在82%的性能下建立高精度分类器，使用的训练集只有1%。此外，我们在30名参与者的研究中发现，普通用户可以建立自己需求的个性化分类器，这些分类器的平均精度为90%，高于现有方法15%。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Multilingual-Self-Supervised-Pretrained-Models-for-Sequence-to-Sequence-End-to-End-Spoken-Language-Understanding"><a href="#Leveraging-Multilingual-Self-Supervised-Pretrained-Models-for-Sequence-to-Sequence-End-to-End-Spoken-Language-Understanding" class="headerlink" title="Leveraging Multilingual Self-Supervised Pretrained Models for Sequence-to-Sequence End-to-End Spoken Language Understanding"></a>Leveraging Multilingual Self-Supervised Pretrained Models for Sequence-to-Sequence End-to-End Spoken Language Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06103">http://arxiv.org/abs/2310.06103</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/digitalphonetics/multilingual-seq2seq-slu">https://github.com/digitalphonetics/multilingual-seq2seq-slu</a></li>
<li>paper_authors: Pavel Denisov, Ngoc Thang Vu</li>
<li>for: 这个论文旨在提出一种能够执行端到端语言理解（E2E-SLU）的多语言设置和任务，包括预测词语填充。</li>
<li>methods: 该方法使用预训练的语音和文本模型，并将其集成到一个生成型模型中，以实现E2E-SLU任务。</li>
<li>results: 经过预训练7000小时的多语言数据后，该模型可以超越当前状态的两个SLU数据集，并在另外两个SLU数据集上达到一定的改进。此外，该模型还可以在不同语言之间进行跨语言比较，并在PortMEDIA-Language数据集上提高最佳结果，减少了23.65%的概念&#x2F;价值错误率。<details>
<summary>Abstract</summary>
A number of methods have been proposed for End-to-End Spoken Language Understanding (E2E-SLU) using pretrained models, however their evaluation often lacks multilingual setup and tasks that require prediction of lexical fillers, such as slot filling. In this work, we propose a unified method that integrates multilingual pretrained speech and text models and performs E2E-SLU on six datasets in four languages in a generative manner, including the prediction of lexical fillers. We investigate how the proposed method can be improved by pretraining on widely available speech recognition data using several training objectives. Pretraining on 7000 hours of multilingual data allows us to outperform the state-of-the-art ultimately on two SLU datasets and partly on two more SLU datasets. Finally, we examine the cross-lingual capabilities of the proposed model and improve on the best known result on the PortMEDIA-Language dataset by almost half, achieving a Concept/Value Error Rate of 23.65%.
</details>
<details>
<summary>摘要</summary>
许多方法已经被提出用于终端到终端语言理解（E2E-SLU），但是它们的评估通常缺乏多语言设置和需要预测词语填充的任务。在这项工作中，我们提出了一种统一方法，将多语言预训练的语音和文本模型集成，并在六个数据集上进行E2E-SLU，包括词语填充预测。我们研究了如何通过多语言批处理训练对这种方法进行改进，并在7000小时多语言数据上进行预训练。这些预训练可以使我们超越当前状态的术语SLU数据集上的最佳性能，并在两个SLU数据集上部分超越状态。最后，我们检查了提posed模型的交叉语言能力，并在PortMEDIA-Language数据集上提高了最佳知识的结果，将概念/价值错误率降低至23.65%。
</details></li>
</ul>
<hr>
<h2 id="Auditing-Gender-Analyzers-on-Text-Data"><a href="#Auditing-Gender-Analyzers-on-Text-Data" class="headerlink" title="Auditing Gender Analyzers on Text Data"></a>Auditing Gender Analyzers on Text Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06061">http://arxiv.org/abs/2310.06061</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siddharth D Jaiswal, Ankit Kumar Verma, Animesh Mukherjee</li>
<li>for: This study aims to audit existing gender analyzers for biases against non-binary individuals.</li>
<li>methods: The study uses two datasets (Reddit comments and Tumblr posts) and fine-tunes a BERT multi-label classifier on these datasets to evaluate the accuracy of the gender analyzers.</li>
<li>results: The study finds that the existing gender analyzers are highly inaccurate, with an overall accuracy of ~50% on all platforms. The fine-tuned BERT model achieves an overall performance of ~77% on the most realistically deployable setting and a surprisingly higher performance of 90% for the non-binary class. Additionally, the study shows that ChatGPT, a highly advanced AI model, is also biased and needs better audits and moderation.<details>
<summary>Abstract</summary>
AI models have become extremely popular and accessible to the general public. However, they are continuously under the scanner due to their demonstrable biases toward various sections of the society like people of color and non-binary people. In this study, we audit three existing gender analyzers -- uClassify, Readable and HackerFactor, for biases against non-binary individuals. These tools are designed to predict only the cisgender binary labels, which leads to discrimination against non-binary members of the society. We curate two datasets -- Reddit comments (660k) and, Tumblr posts (2.05M) and our experimental evaluation shows that the tools are highly inaccurate with the overall accuracy being ~50% on all platforms. Predictions for non-binary comments on all platforms are mostly female, thus propagating the societal bias that non-binary individuals are effeminate. To address this, we fine-tune a BERT multi-label classifier on the two datasets in multiple combinations, observe an overall performance of ~77% on the most realistically deployable setting and a surprisingly higher performance of 90% for the non-binary class. We also audit ChatGPT using zero-shot prompts on a small dataset (due to high pricing) and observe an average accuracy of 58% for Reddit and Tumblr combined (with overall better results for Reddit).   Thus, we show that existing systems, including highly advanced ones like ChatGPT are biased, and need better audits and moderation and, that such societal biases can be addressed and alleviated through simple off-the-shelf models like BERT trained on more gender inclusive datasets.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Few-Shot-Spoken-Language-Understanding-via-Joint-Speech-Text-Models"><a href="#Few-Shot-Spoken-Language-Understanding-via-Joint-Speech-Text-Models" class="headerlink" title="Few-Shot Spoken Language Understanding via Joint Speech-Text Models"></a>Few-Shot Spoken Language Understanding via Joint Speech-Text Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05919">http://arxiv.org/abs/2310.05919</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chung-Ming Chien, Mingjiamei Zhang, Ju-Chieh Chou, Karen Livescu</li>
<li>for: 提高 spoken language understanding 任务中的数据有限性问题</li>
<li>methods: 使用共享表示空间的 speech-text 模型，并将文本模型转移到语音测试数据上</li>
<li>results: 与使用 speech-only 预训练模型 fine-tuned 10 倍更多数据相比，我们的提议方法可以在 sentiment analysis 和 named entity recognition 等任务中达到相似的性能，只需要 1 小时的标注语音数据Here’s the full text in Traditional Chinese:</li>
<li>for: 这paper是为了解决 spoken language understanding 任务中的数据有限性问题</li>
<li>methods: 我们使用共享表示空间的 speech-text 模型，并将文本模型转移到语音测试数据上</li>
<li>results: 与使用 speech-only 预训练模型 fine-tuned 10 倍更多数据相比，我们的提议方法可以在 sentiment analysis 和 named entity recognition 等任务中达到相似的性能，只需要 1 小时的标注语音数据<details>
<summary>Abstract</summary>
Recent work on speech representation models jointly pre-trained with text has demonstrated the potential of improving speech representations by encoding speech and text in a shared space. In this paper, we leverage such shared representations to address the persistent challenge of limited data availability in spoken language understanding tasks. By employing a pre-trained speech-text model, we find that models fine-tuned on text can be effectively transferred to speech testing data. With as little as 1 hour of labeled speech data, our proposed approach achieves comparable performance on spoken language understanding tasks (specifically, sentiment analysis and named entity recognition) when compared to previous methods using speech-only pre-trained models fine-tuned on 10 times more data. Beyond the proof-of-concept study, we also analyze the latent representations. We find that the bottom layers of speech-text models are largely task-agnostic and align speech and text representations into a shared space, while the top layers are more task-specific.
</details>
<details>
<summary>摘要</summary>
近期关于语音表示模型同时预训练文本的工作表明了改进语音表示的潜在可能性。在这篇论文中，我们利用这些共享表示来解决语音理解任务中的数据有限问题。我们使用预训练的语音文本模型，发现可以将文本预训练模型转移到语音测试数据上，只需要1小时的标注语音数据。与之前使用语音只预训练模型 fine-tune 10倍更多数据的方法相比，我们的提议方法可以在语音理解任务（具体来说是情感分析和命名实体识别）中达到相同的性能水平。此外，我们还分析了隐藏表示。我们发现语音文本模型的下层主要是无关任务的，可以将语音和文本表示同化到共享空间，而顶层则更加具体地关注特定任务。
</details></li>
</ul>
<hr>
<h2 id="NEFTune-Noisy-Embeddings-Improve-Instruction-Finetuning"><a href="#NEFTune-Noisy-Embeddings-Improve-Instruction-Finetuning" class="headerlink" title="NEFTune: Noisy Embeddings Improve Instruction Finetuning"></a>NEFTune: Noisy Embeddings Improve Instruction Finetuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05914">http://arxiv.org/abs/2310.05914</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/neelsjain/neftune">https://github.com/neelsjain/neftune</a></li>
<li>paper_authors: Neel Jain, Ping-yeh Chiang, Yuxin Wen, John Kirchenbauer, Hong-Min Chu, Gowthami Somepalli, Brian R. Bartoldson, Bhavya Kailkhura, Avi Schwarzschild, Aniruddha Saha, Micah Goldblum, Jonas Geiping, Tom Goldstein</li>
<li>for: 提高语言模型finetuning的性能</li>
<li>methods: 使用随机噪声添加到嵌入向量中 during 训练</li>
<li>results: 1. 使用噪声 embedding 的模型在 AlpacaEval 上的分数从 29.79% 提高到 64.69%；2. 在现代 instrucion  datasets 上超过强基elines，包括 Evol-Instruct、ShareGPT 和 OpenPlatypus 上的提高约10%。<details>
<summary>Abstract</summary>
We show that language model finetuning can be improved, sometimes dramatically, with a simple augmentation. NEFTune adds noise to the embedding vectors during training. Standard finetuning of LLaMA-2-7B using Alpaca achieves 29.79% on AlpacaEval, which rises to 64.69% using noisy embeddings. NEFTune also improves over strong baselines on modern instruction datasets. Models trained with Evol-Instruct see a 10% improvement, with ShareGPT an 8% improvement, and with OpenPlatypus an 8% improvement. Even powerful models further refined with RLHF such as LLaMA-2-Chat benefit from additional training with NEFTune.
</details>
<details>
<summary>摘要</summary>
我们显示了语言模型调整可以得到改善，有时会很大，只需使用简单的增强技巧。NEFTune在训练过程中将嵌入向量添加随机变化。通过使用Alpaca，标准调整LLaMA-2-7B的性能为29.79%，将提高到64.69%。NEFTune也超过了现代指令集 dataset 的强基elines。使用 Evol-Instruct 的模型进行调整会增加10%的性能，使用 ShareGPT 的模型进行调整会增加8%的性能，使用 OpenPlatypus 的模型进行调整会增加8%的性能。甚至是使用 RLHF 进一步调整的强大模型，如 LLama-2-Chat，也会受益于额外的 NEFTune 训练。
</details></li>
</ul>
<hr>
<h2 id="Controllable-Chest-X-Ray-Report-Generation-from-Longitudinal-Representations"><a href="#Controllable-Chest-X-Ray-Report-Generation-from-Longitudinal-Representations" class="headerlink" title="Controllable Chest X-Ray Report Generation from Longitudinal Representations"></a>Controllable Chest X-Ray Report Generation from Longitudinal Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05881">http://arxiv.org/abs/2310.05881</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francesco Dalla Serra, Chaoyang Wang, Fani Deligianni, Jeffrey Dalton, Alison Q O’Neil</li>
<li>for: 这篇论文的目的是提高医疗影像报告的速度和准确性，并且提供可控的报告生成模型。</li>
<li>methods: 本文提出了两个新方法：首先，使用 longitudinal representation learning 方法，将先前的医疗影像作为额外输入，将现有和先前的视觉信息联合和融合为一个共同 longitudinal 表现，以便给 multimodal 报告生成模型；其次，使用 sentence-anatomy dropout 训练策略，让报告生成模型在预测报告内容时仅预测和corrsponding的句子和体位。</li>
<li>results: 经过对 MIMIC-CXR 资料集的严格实验，本文的方法能够实现现有最佳的结果，同时具备可控的报告生成能力。<details>
<summary>Abstract</summary>
Radiology reports are detailed text descriptions of the content of medical scans. Each report describes the presence/absence and location of relevant clinical findings, commonly including comparison with prior exams of the same patient to describe how they evolved. Radiology reporting is a time-consuming process, and scan results are often subject to delays. One strategy to speed up reporting is to integrate automated reporting systems, however clinical deployment requires high accuracy and interpretability. Previous approaches to automated radiology reporting generally do not provide the prior study as input, precluding comparison which is required for clinical accuracy in some types of scans, and offer only unreliable methods of interpretability. Therefore, leveraging an existing visual input format of anatomical tokens, we introduce two novel aspects: (1) longitudinal representation learning -- we input the prior scan as an additional input, proposing a method to align, concatenate and fuse the current and prior visual information into a joint longitudinal representation which can be provided to the multimodal report generation model; (2) sentence-anatomy dropout -- a training strategy for controllability in which the report generator model is trained to predict only sentences from the original report which correspond to the subset of anatomical regions given as input. We show through in-depth experiments on the MIMIC-CXR dataset how the proposed approach achieves state-of-the-art results while enabling anatomy-wise controllable report generation.
</details>
<details>
<summary>摘要</summary>
医学成像报告是详细的文本描述医学扫描结果。每份报告都描述了病人的相关临床发现，并常常包括与当前扫描相比较以描述它们是如何发展的。医学报告是一项时间消耗的过程，扫描结果经常会受到延迟。为了加速报告，可以 integrate 自动报告系统，但是临床部署需要高度准确和可解释性。先前的自动医学报告方法通常不提供先前扫描作为输入，因此无法进行相关的比较，这会导致报告不准确。此外，这些方法的可解释性也不够。因此，我们利用现有的解剖学输入格式，引入两个新的方面：1.  longitudinal representation learning——我们将先前扫描作为额外输入，提议一种方法来对当前和先前的视觉信息进行对接、拼接和融合，以生成共同的长期表示，这个表示可以被传给多模态报告生成模型。2. sentence-anatomy dropout——一种训练策略，用于控制报告生成模型的可控性。在训练过程中，报告生成模型需要预测来自原始报告的具体句子，其中句子的选择取决于提供的解剖学区域输入。我们通过对 MIMIC-CXR 数据集进行深入的实验，证明我们的方法可以达到当前领导的结果，同时允许解剖学 wise 可控报告生成。
</details></li>
</ul>
<hr>
<h2 id="Are-Large-Language-Models-Geospatially-Knowledgeable"><a href="#Are-Large-Language-Models-Geospatially-Knowledgeable" class="headerlink" title="Are Large Language Models Geospatially Knowledgeable?"></a>Are Large Language Models Geospatially Knowledgeable?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13002">http://arxiv.org/abs/2310.13002</a></li>
<li>repo_url: None</li>
<li>paper_authors: Prabin Bhandari, Antonios Anastasopoulos, Dieter Pfoser</li>
<li>for:  investigate the extent of geospatial knowledge and reasoning abilities in pre-trained Large Language Models (LLMs)</li>
<li>methods: probe LLMs for geo-coordinates, use geospatial and non-geospatial prepositions to gauge geospatial awareness, and utilize a multidimensional scaling (MDS) experiment to assess geospatial reasoning capabilities</li>
<li>results: larger and more sophisticated LLMs can synthesize geospatial knowledge from textual information, but there are limitations to their geospatial abilities<details>
<summary>Abstract</summary>
Despite the impressive performance of Large Language Models (LLM) for various natural language processing tasks, little is known about their comprehension of geographic data and related ability to facilitate informed geospatial decision-making. This paper investigates the extent of geospatial knowledge, awareness, and reasoning abilities encoded within such pretrained LLMs. With a focus on autoregressive language models, we devise experimental approaches related to (i) probing LLMs for geo-coordinates to assess geospatial knowledge, (ii) using geospatial and non-geospatial prepositions to gauge their geospatial awareness, and (iii) utilizing a multidimensional scaling (MDS) experiment to assess the models' geospatial reasoning capabilities and to determine locations of cities based on prompting. Our results confirm that it does not only take larger, but also more sophisticated LLMs to synthesize geospatial knowledge from textual information. As such, this research contributes to understanding the potential and limitations of LLMs in dealing with geospatial information.
</details>
<details>
<summary>摘要</summary>
尽管大型自然语言模型（LLM）在不同的自然语言处理任务上表现出色，但对于地理数据的理解和有关能力却得到了少量的研究。这篇论文探究了抽象语言模型中地理知识、意识和逻辑能力的程度。我们专注于autoregressive语言模型，并设计了以下三种实验方法：1. 使用地理坐标来评估语言模型的地理知识。2. 使用地理和非地理前置词来评估语言模型的地理意识。3. 使用多维度规范（MDS）实验来评估模型的地理逻辑能力，并确定文本中提到的城市的位置。我们的结果表明，不仅要有更大的语言模型，也需要更加复杂的语言模型才能从文本信息中 sinthezize 地理知识。因此，这项研究对于理解LLM在处理地理信息的可能性和局限性做出了贡献。
</details></li>
</ul>
<hr>
<h2 id="Terminology-Aware-Translation-with-Constrained-Decoding-and-Large-Language-Model-Prompting"><a href="#Terminology-Aware-Translation-with-Constrained-Decoding-and-Large-Language-Model-Prompting" class="headerlink" title="Terminology-Aware Translation with Constrained Decoding and Large Language Model Prompting"></a>Terminology-Aware Translation with Constrained Decoding and Large Language Model Prompting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05824">http://arxiv.org/abs/2310.05824</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nikolay Bogoychev, Pinzhen Chen</li>
<li>for: 提高机器翻译下游应用中精度的重要性，并且一种常见的方式是在翻译系统中注入 terminate 约束。</li>
<li>methods: 我们在 WMT 2023 翻译任务中采用了 translate-then-refine approach，这种方法不受领域限制，并且需要最小的手动努力。我们首先使用 word alignment 获取 pseudo-terminology 翻译，然后使用这些翻译来训练一个 terminology-aware 模型。此外，我们还 explore 了两种后处理方法。</li>
<li>results: 我们的 experiment 表明，我们的 terminology-aware 模型能够有效地 incorporate 精度，而使用大语言模型进行 refine 过程可以进一步提高 terminate 记忆。<details>
<summary>Abstract</summary>
Terminology correctness is important in the downstream application of machine translation, and a prevalent way to ensure this is to inject terminology constraints into a translation system. In our submission to the WMT 2023 terminology translation task, we adopt a translate-then-refine approach which can be domain-independent and requires minimal manual efforts. We annotate random source words with pseudo-terminology translations obtained from word alignment to first train a terminology-aware model. Further, we explore two post-processing methods. First, we use an alignment process to discover whether a terminology constraint has been violated, and if so, we re-decode with the violating word negatively constrained. Alternatively, we leverage a large language model to refine a hypothesis by providing it with terminology constraints. Results show that our terminology-aware model learns to incorporate terminologies effectively, and the large language model refinement process can further improve terminology recall.
</details>
<details>
<summary>摘要</summary>
<<SYS>>翻译精度在机器翻译下游应用中非常重要，一种常见的方法是将翻译系统中的术语约束注入到翻译系统中。在我们对WMT 2023翻译任务提交中，我们采用了一种翻译后修改的方法，这种方法不依赖于域名和需要 minimal的手动努力。我们首先使用word alignment获取pseudo-术语翻译，然后使用这些翻译来训练一个术语意识Model。此外，我们还探索了两种后处理方法。首先，我们使用一个对应过程来检查翻译是否违反了术语约束，如果违反了，那么我们会使用违反的单词做为约束来重新解码。其次，我们利用一个大型自然语言模型来修改一个假设，并提供术语约束来进一步提高术语回忆率。结果显示，我们的术语意识Model有效地收录了术语，而大型自然语言模型的修改过程可以进一步提高术语回忆率。
</details></li>
</ul>
<hr>
<h2 id="SC-Safety-A-Multi-round-Open-ended-Question-Adversarial-Safety-Benchmark-for-Large-Language-Models-in-Chinese"><a href="#SC-Safety-A-Multi-round-Open-ended-Question-Adversarial-Safety-Benchmark-for-Large-Language-Models-in-Chinese" class="headerlink" title="SC-Safety: A Multi-round Open-ended Question Adversarial Safety Benchmark for Large Language Models in Chinese"></a>SC-Safety: A Multi-round Open-ended Question Adversarial Safety Benchmark for Large Language Models in Chinese</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05818">http://arxiv.org/abs/2310.05818</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liang Xu, Kangkang Zhao, Lei Zhu, Hang Xue</li>
<li>For: The paper aims to systematically assess the safety of Chinese large language models (LLMs) and provide a benchmark for creating safer and more trustworthy models.* Methods: The paper introduces a multi-round adversarial benchmark called SuperCLUE-Safety (SC-Safety) that includes 4912 open-ended questions covering 20 safety sub-dimensions. The benchmark involves human-model interactions and conversations to increase the challenges.* Results: The paper finds that closed-source models perform better in terms of safety compared to open-source models, and models released from China demonstrate comparable safety levels to LLMs like GPT-3.5-turbo. Smaller models with 6B-13B parameters can also compete effectively in terms of safety. The findings provide guidance on model selection and promote collaborative efforts to create safer LLMs.<details>
<summary>Abstract</summary>
Large language models (LLMs), like ChatGPT and GPT-4, have demonstrated remarkable abilities in natural language understanding and generation. However, alongside their positive impact on our daily tasks, they can also produce harmful content that negatively affects societal perceptions. To systematically assess the safety of Chinese LLMs, we introduce SuperCLUE-Safety (SC-Safety) - a multi-round adversarial benchmark with 4912 open-ended questions covering more than 20 safety sub-dimensions. Adversarial human-model interactions and conversations significantly increase the challenges compared to existing methods. Experiments on 13 major LLMs supporting Chinese yield the following insights: 1) Closed-source models outperform open-sourced ones in terms of safety; 2) Models released from China demonstrate comparable safety levels to LLMs like GPT-3.5-turbo; 3) Some smaller models with 6B-13B parameters can compete effectively in terms of safety. By introducing SC-Safety, we aim to promote collaborative efforts to create safer and more trustworthy LLMs. The benchmark and findings provide guidance on model selection. Our benchmark can be found at https://www.CLUEbenchmarks.com
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLM），如ChatGPT和GPT-4，已经表现出了惊人的自然语言理解和生成能力。然而，同时也可能生成有害内容，影响社会观念。为了系统地评估中文 LLM 的安全性，我们介绍了 SuperCLUE-Safety（SC-Safety）多轮对抗性测试框架，包括4912个开放式问题，覆盖超过20个安全子维度。对人机模型交互和对话的挑战性提高了现有方法的挑战性。对13种主要支持中文的 LLM 进行了实验，得到以下发现：1）关闭源代码模型在安全性方面表现更高；2）中国发布的模型与 GPT-3.5-turbo 的安全水平相当；3）一些6B-13B参数的小型模型可以有效竞争在安全性方面。通过引入 SC-Safety，我们希望促进开源 LLM 的创造和可信worthiness。我们的测试框架可以在https://www.CLUEbenchmarks.com找到。
</details></li>
</ul>
<hr>
<h2 id="DiffuSeq-v2-Bridging-Discrete-and-Continuous-Text-Spaces-for-Accelerated-Seq2Seq-Diffusion-Models"><a href="#DiffuSeq-v2-Bridging-Discrete-and-Continuous-Text-Spaces-for-Accelerated-Seq2Seq-Diffusion-Models" class="headerlink" title="DiffuSeq-v2: Bridging Discrete and Continuous Text Spaces for Accelerated Seq2Seq Diffusion Models"></a>DiffuSeq-v2: Bridging Discrete and Continuous Text Spaces for Accelerated Seq2Seq Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05793">http://arxiv.org/abs/2310.05793</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Shark-NLP/DiffuSeq">https://github.com/Shark-NLP/DiffuSeq</a></li>
<li>paper_authors: Shansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu, Lingpeng Kong</li>
<li>for: 提高Diffusion模型的训练速度和采样速度，以便于实际应用。</li>
<li>methods: 引入软吸收状态，使Diffusion模型能够在连续Diffusion空间中学习恢复分子突变，并使用现有的ODE解ulle器在连续空间中加速采样过程。</li>
<li>results: 实验结果表明，提出的方法可以提高训练速度4倍，并在800倍的采样速度下生成同质的样本，使其更加符合实际应用。<details>
<summary>Abstract</summary>
Diffusion models have gained prominence in generating high-quality sequences of text. Nevertheless, current approaches predominantly represent discrete text within a continuous diffusion space, which incurs substantial computational overhead during training and results in slower sampling speeds. In this paper, we introduce a soft absorbing state that facilitates the diffusion model in learning to reconstruct discrete mutations based on the underlying Gaussian space, thereby enhancing its capacity to recover conditional signals. During the sampling phase, we employ state-of-the-art ODE solvers within the continuous space to expedite the sampling process. Comprehensive experimental evaluations reveal that our proposed method effectively accelerates the training convergence by 4x and generates samples of similar quality 800x faster, rendering it significantly closer to practical application. \footnote{The code is released at \url{https://github.com/Shark-NLP/DiffuSeq}
</details>
<details>
<summary>摘要</summary>
Diffusion models 已经在生成高质量文本序列方面得到广泛应用。然而，当前的方法主要将整数文本 Represented as a continuous diffusion space中的一部分，这会导致训练过程中的计算开销很大，以及采样速度较慢。在这篇论文中，我们引入了软吸收状态，使得扩散模型能够学习根据下面的 Gaussian space 中的精度 Mutations 进行重建，从而提高其对 conditional signals 的恢复能力。在采样阶段，我们使用 state-of-the-art ODE solvers 在连续空间中进行采样，以便加速采样过程。经过了广泛的实验评估，我们的提议方法可以在训练速度和样本质量两个方面提高效率， specifically 4x 快速 Train convergence 和 800x faster sample generation，使其更加接近实际应用。Note: The URL in the footnote has been translated as well: \url{https://github.com/Shark-NLP/DiffuSeq} becomes \url{https://github.com/Shark-NLP/DiffuSeq}
</details></li>
</ul>
<hr>
<h2 id="Problem-Solving-Guide-Predicting-the-Algorithm-Tags-and-Difficulty-for-Competitive-Programming-Problems"><a href="#Problem-Solving-Guide-Predicting-the-Algorithm-Tags-and-Difficulty-for-Competitive-Programming-Problems" class="headerlink" title="Problem-Solving Guide: Predicting the Algorithm Tags and Difficulty for Competitive Programming Problems"></a>Problem-Solving Guide: Predicting the Algorithm Tags and Difficulty for Competitive Programming Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05791">http://arxiv.org/abs/2310.05791</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sronger/psg_predicting_algorithm_tags_and_difficulty">https://github.com/sronger/psg_predicting_algorithm_tags_and_difficulty</a></li>
<li>paper_authors: Juntae Kim, Eunjung Cho, Dongwoo Kim, Dongbin Na</li>
<li>For: This paper aims to help engineers and developers solve algorithm problems more efficiently by predicting the algorithm tag and difficulty level of a problem.* Methods: The authors propose a deep learning-based method for simultaneously predicting algorithm tags and difficulty levels of an algorithm problem given.* Results: The authors present a real-world algorithm problem multi-task dataset, AMT, which is the most large-scale dataset for predicting algorithm tags compared to previous studies. They also show that their proposed method can accurately predict algorithm tags and difficulty levels.<details>
<summary>Abstract</summary>
The recent program development industries have required problem-solving abilities for engineers, especially application developers. However, AI-based education systems to help solve computer algorithm problems have not yet attracted attention, while most big tech companies require the ability to solve algorithm problems including Google, Meta, and Amazon. The most useful guide to solving algorithm problems might be guessing the category (tag) of the facing problems. Therefore, our study addresses the task of predicting the algorithm tag as a useful tool for engineers and developers. Moreover, we also consider predicting the difficulty levels of algorithm problems, which can be used as useful guidance to calculate the required time to solve that problem. In this paper, we present a real-world algorithm problem multi-task dataset, AMT, by mainly collecting problem samples from the most famous and large competitive programming website Codeforces. To the best of our knowledge, our proposed dataset is the most large-scale dataset for predicting algorithm tags compared to previous studies. Moreover, our work is the first to address predicting the difficulty levels of algorithm problems. We present a deep learning-based novel method for simultaneously predicting algorithm tags and the difficulty levels of an algorithm problem given. All datasets and source codes are available at https://github.com/sronger/PSG_Predicting_Algorithm_Tags_and_Difficulty.
</details>
<details>
<summary>摘要</summary>
现代软件开发行业强调解决问题能力，特别是应用程序开发人员。然而，基于人工智能的教育系统用于解决计算机算法问题尚未吸引到关注，而大多数大型科技公司都需要解决算法问题，包括Google、Meta和Amazon。解决算法问题的最有用指南可能是猜测问题的类别（标签）。因此，我们的研究挑战是预测算法标签作为工程师和开发人员的有用工具。此外，我们还考虑预测算法问题的困难程度，可以作为有用的导航来计算解决该问题所需的时间。在本文中，我们发布了一个实际世界上最大规模的算法问题多任务 dataset，即 AMT，通过主要收集来自最著名和最大竞赛编程网站 Codeforces 的问题样本。根据我们所知，我们提出的 dataset 是预测算法标签方面最大规模的比前一些研究。此外，我们的工作是第一次Addressing 预测算法问题的困难程度。我们提出了一种深度学习基于的新方法，可同时预测算法标签和问题的困难程度。所有数据和源代码都可以在 GitHub 上获取，请参考 <https://github.com/sronger/PSG_Predicting_Algorithm_Tags_and_Difficulty>。
</details></li>
</ul>
<hr>
<h2 id="Aligning-Language-Models-with-Human-Preferences-via-a-Bayesian-Approach"><a href="#Aligning-Language-Models-with-Human-Preferences-via-a-Bayesian-Approach" class="headerlink" title="Aligning Language Models with Human Preferences via a Bayesian Approach"></a>Aligning Language Models with Human Preferences via a Bayesian Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05782">http://arxiv.org/abs/2310.05782</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wangjs9/aligned-dpm">https://github.com/wangjs9/aligned-dpm</a></li>
<li>paper_authors: Jiashuo Wang, Haozhao Wang, Shichao Sun, Wenjie Li</li>
<li>for: This paper aims to advance human-centric natural language generation (NLG) systems by ensuring alignment between NLG models and human preferences.</li>
<li>methods: The proposed method uses a Bayesian framework to account for the distribution of disagreements among human preferences in training a preference model, and utilizes contrastive learning to train the NLG model with the preference scores.</li>
<li>results: The proposed method consistently exceeds previous state-of-the-art (SOTA) models in both automatic and human evaluations on two human-centric NLG tasks, i.e., emotional support conversation and integrity “Rule-of-Thumb” generation.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文目标是提高人类中心的自然语言生成系统，确保NLG模型和人类偏好的对应。</li>
<li>methods: 提议方法使用 bayesian 框架来考虑人类偏好的分布不一致，在偏好模型训练中使用对比学习训练 NLG 模型。</li>
<li>results: 提议方法在两个人类中心 NLG 任务（情感支持对话和规则杆准则生成）的自动和人类评估中一直超越过去的 SOTA 模型。<details>
<summary>Abstract</summary>
In the quest to advance human-centric natural language generation (NLG) systems, ensuring alignment between NLG models and human preferences is crucial. For this alignment, current popular methods leverage a reinforcement learning (RL) approach with a reward model trained on feedback from humans. However, inherent disagreements due to the subjective nature of human preferences pose a significant challenge for training the reward model, resulting in a deterioration of the NLG performance. To tackle this issue, previous approaches typically rely on majority voting or averaging to consolidate multiple inconsistent preferences into a merged one. Although straightforward to understand and execute, such methods suffer from an inability to capture the nuanced degrees of disaggregation among humans and may only represent a specialized subset of individuals, thereby lacking the ability to quantitatively disclose the universality of human preferences. To address this challenge, this paper proposes a novel approach, which employs a Bayesian framework to account for the distribution of disagreements among human preferences as training a preference model, and names it as d-PM. Besides, considering the RL strategy's inefficient and complex training process over the training efficiency, we further propose utilizing the contrastive learning strategy to train the NLG model with the preference scores derived from the d-PM model. Extensive experiments on two human-centric NLG tasks, i.e., emotional support conversation and integrity "Rule-of-Thumb" generation, show that our method consistently exceeds previous SOTA models in both automatic and human evaluations.
</details>
<details>
<summary>摘要</summary>
在增进人类中心的自然语言生成（NLG）系统方面，与人类偏好的吻合是关键。现有的流行方法通常采用了强化学习（RL）方法，并在人类反馈中训练一个奖励模型。然而，人类偏好的内在分歧对RL方法的训练带来了很大挑战，导致NLG性能下降。为解决这个问题，之前的方法通常采用了多数投票或平均值来整合多个不一致的偏好，但这些方法容易受到人类偏好的分歧的限制，并且只能表征特定人群，无法量化透过表达人类偏好的多样性。为此，本文提出了一种新的方法，即使用 Bayesian 框架来考虑人类偏好的分歧分布，并称之为 d-PM。此外，由于RL策略的训练过程复杂且不效率，我们进一步提议使用对比学习策略来训练NLG模型，使用 d-PM 模型生成的偏好分数。经验表明，我们的方法在两个人类中心NLG任务上（即情感支持对话和规则精神生成）都能够连续超越过去的最佳模型，并在自动和人类评估中表现出色。
</details></li>
</ul>
<hr>
<h2 id="LLMLingua-Compressing-Prompts-for-Accelerated-Inference-of-Large-Language-Models"><a href="#LLMLingua-Compressing-Prompts-for-Accelerated-Inference-of-Large-Language-Models" class="headerlink" title="LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models"></a>LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05736">http://arxiv.org/abs/2310.05736</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/LLMLingua">https://github.com/microsoft/LLMLingua</a></li>
<li>paper_authors: Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, Lili Qiu</li>
<li>for: 这个研究旨在提高语言模型（LLM）的推导速度和降低成本，以便应用在各种应用中。</li>
<li>methods: 本研究使用了一种名为 LLMLingua 的弹性推导方法，包括一个预算控制器来维持semantic integrity，一个iterative compression algorithm来更好地模型压缩内容之间的依赖性，以及一种基于 instruction tuning的方法来对语言模型进行分布对齐。</li>
<li>results: 本研究在四个不同的数据集（GSM8K、BBH、ShareGPT和Arxiv-March23）上进行了实验和分析，结果显示 LLMLingua 方法可以实现现在的性能水准，并且可以实现高比例的压缩（Up to 20x），而且具有 littles 的性能损失。<details>
<summary>Abstract</summary>
Large language models (LLMs) have been applied in various applications due to their astonishing capabilities. With advancements in technologies such as chain-of-thought (CoT) prompting and in-context learning (ICL), the prompts fed to LLMs are becoming increasingly lengthy, even exceeding tens of thousands of tokens. To accelerate model inference and reduce cost, this paper presents LLMLingua, a coarse-to-fine prompt compression method that involves a budget controller to maintain semantic integrity under high compression ratios, a token-level iterative compression algorithm to better model the interdependence between compressed contents, and an instruction tuning based method for distribution alignment between language models. We conduct experiments and analysis over four datasets from different scenarios, i.e., GSM8K, BBH, ShareGPT, and Arxiv-March23; showing that the proposed approach yields state-of-the-art performance and allows for up to 20x compression with little performance loss. Our code is available at https://aka.ms/LLMLingua.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Towards-Emotion-Based-Synthetic-Consciousness-Using-LLMs-to-Estimate-Emotion-Probability-Vectors"><a href="#Towards-Emotion-Based-Synthetic-Consciousness-Using-LLMs-to-Estimate-Emotion-Probability-Vectors" class="headerlink" title="Towards Emotion-Based Synthetic Consciousness: Using LLMs to Estimate Emotion Probability Vectors"></a>Towards Emotion-Based Synthetic Consciousness: Using LLMs to Estimate Emotion Probability Vectors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10673">http://arxiv.org/abs/2310.10673</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Sinclair, Willem Pye</li>
<li>for: 这篇论文探讨了如何使用大语言模型（LLMs）来估算文本中情感状态的摘要。</li>
<li>methods: 该论文使用了大语言模型来计算文本中的情感摘要，该摘要包括情感描述词和该词在提要中出现概率。</li>
<li>results: 通过对亚马逊商品评论进行情感分析，该论文示出了情感描述词可以被映射到PCA类型空间中。然而，通过使用尾提要来引发行动来改进当前状态的方法并不是直接可行。<details>
<summary>Abstract</summary>
This paper shows how LLMs (Large Language Models) may be used to estimate a summary of the emotional state associated with piece of text. The summary of emotional state is a dictionary of words used to describe emotion together with the probability of the word appearing after a prompt comprising the original text and an emotion eliciting tail. Through emotion analysis of Amazon product reviews we demonstrate emotion descriptors can be mapped into a PCA type space. It was hoped that text descriptions of actions to improve a current text described state could also be elicited through a tail prompt. Experiment seemed to indicate that this is not straightforward to make work. This failure put our hoped for selection of action via choosing the best predict ed outcome via comparing emotional responses out of reach for the moment.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了如何使用大语言模型（LLMs）来估算一篇文本中的情感状态概述。概述的情感状态是一个词典，其中包含用于描述情感的词语以及这些词语在键入文本和情感Trigger后的概率。通过对亚马逊商品评论进行情感分析，我们示出了情感描述可以被映射到PCA类型空间。希望通过尾部提示来引发文本描述的行为改进方法，但实验表明这并不是一个简单的任务。这种失败使我们希望通过比较情感响应来选择最佳结果的方法被推迟。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-of-Large-Language-Models-for-Healthcare-from-Data-Technology-and-Applications-to-Accountability-and-Ethics"><a href="#A-Survey-of-Large-Language-Models-for-Healthcare-from-Data-Technology-and-Applications-to-Accountability-and-Ethics" class="headerlink" title="A Survey of Large Language Models for Healthcare: from Data, Technology, and Applications to Accountability and Ethics"></a>A Survey of Large Language Models for Healthcare: from Data, Technology, and Applications to Accountability and Ethics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05694">http://arxiv.org/abs/2310.05694</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kaihe-better/llm-for-healthcare">https://github.com/kaihe-better/llm-for-healthcare</a></li>
<li>paper_authors: Kai He, Rui Mao, Qika Lin, Yucheng Ruan, Xiang Lan, Mengling Feng, Erik Cambria</li>
<li>for: 这份survey旨在提供关于现有的大语言模型（LLMs）在医疗领域的开发进程和其应用前景，以及在PLMs的基础上发展出LLMs的开发路线图。</li>
<li>methods: 本文首先探讨LLMs在医疗应用中的可能性，并描述了LLMs的开发过程、训练数据、训练方法、优化策略和应用。此外，本文还对PLMs和LLMs之间进行比较，以及不同LLMs之间进行比较。</li>
<li>results: 本文总结了关于LLMs在医疗领域的开发和应用，包括关于Healthcare应用中LLMs的可能性、PLMs和LLMs之间的比较、训练数据、训练方法、优化策略和应用。此外，本文还考虑了在医疗领域部署LLMs时存在的独特问题，如公平、责任、透明度和伦理问题。<details>
<summary>Abstract</summary>
The utilization of large language models (LLMs) in the Healthcare domain has generated both excitement and concern due to their ability to effectively respond to freetext queries with certain professional knowledge. This survey outlines the capabilities of the currently developed LLMs for Healthcare and explicates their development process, with the aim of providing an overview of the development roadmap from traditional Pretrained Language Models (PLMs) to LLMs. Specifically, we first explore the potential of LLMs to enhance the efficiency and effectiveness of various Healthcare applications highlighting both the strengths and limitations. Secondly, we conduct a comparison between the previous PLMs and the latest LLMs, as well as comparing various LLMs with each other. Then we summarize related Healthcare training data, training methods, optimization strategies, and usage. Finally, the unique concerns associated with deploying LLMs in Healthcare settings are investigated, particularly regarding fairness, accountability, transparency and ethics. Our survey provide a comprehensive investigation from perspectives of both computer science and Healthcare specialty. Besides the discussion about Healthcare concerns, we supports the computer science community by compiling a collection of open source resources, such as accessible datasets, the latest methodologies, code implementations, and evaluation benchmarks in the Github. Summarily, we contend that a significant paradigm shift is underway, transitioning from PLMs to LLMs. This shift encompasses a move from discriminative AI approaches to generative AI approaches, as well as a shift from model-centered methodologies to datacentered methodologies.
</details>
<details>
<summary>摘要</summary>
大量语言模型（LLM）在医疗领域的应用已经引起了广泛的兴趣和担忧，因为它们可以有效地回答免费文本查询，并且具有一定的专业知识。本调查概述了目前已经开发出的LLMs的能力，并详细介绍其开发过程，以提供对开发路线图的概述，从传统的预训练语言模型（PLMs）到LLMs。 specifically，我们首先探讨LLMs在各种医疗应用中的可能性，并 highlighted它们的优点和局限性。其次，我们进行了PLMs和最新的LLMs之间的比较，以及不同的LLMs之间的比较。然后，我们总结了相关的医疗训练数据、训练方法、优化策略和使用。最后，我们调查了在医疗设置中部署LLMs的独特问题，特别是公平、责任、透明度和伦理。我们的调查提供了从计算机科学和医疗专业的视角的全面的调查。除了医疗问题的讨论外，我们还支持计算机科学社区，并将可 accessible datasets、最新的方法ologies、代码实现和评估标准集成到GitHub中。总之，我们认为现在正在进行一次重要的 парадиг转换，从PLMs到LLMs。这种转换包括从推理AI方法到生成AI方法，以及从模型中心的方法ологи到数据中心的方法ологи。
</details></li>
</ul>
<hr>
<h2 id="Larth-Dataset-and-Machine-Translation-for-Etruscan"><a href="#Larth-Dataset-and-Machine-Translation-for-Etruscan" class="headerlink" title="Larth: Dataset and Machine Translation for Etruscan"></a>Larth: Dataset and Machine Translation for Etruscan</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05688">http://arxiv.org/abs/2310.05688</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gianlucavico/larth-etruscan-nlp">https://github.com/gianlucavico/larth-etruscan-nlp</a></li>
<li>paper_authors: Gianluca Vico, Gerasimos Spanakis</li>
<li>for: 这个论文的目的是提供一个从 Et 语言到英语的机器翻译数据集，以便未来的研究人员可以利用这些数据进行语言处理研究。</li>
<li>methods: 这个论文使用了一些自动和手动提取的翻译示例，并使用了不同的机器翻译模型进行评估。</li>
<li>results: 论文的结果表明，使用一个小型转换器模型可以实现 BLEU 分数为 10.1。这些结果可以帮助未来的研究人员在这种语言和其他具有有限资源的语言中进行更好的语言处理研究。<details>
<summary>Abstract</summary>
Etruscan is an ancient language spoken in Italy from the 7th century BC to the 1st century AD. There are no native speakers of the language at the present day, and its resources are scarce, as there exist only around 12,000 known inscriptions. To the best of our knowledge, there are no publicly available Etruscan corpora for natural language processing. Therefore, we propose a dataset for machine translation from Etruscan to English, which contains 2891 translated examples from existing academic sources. Some examples are extracted manually, while others are acquired in an automatic way. Along with the dataset, we benchmark different machine translation models observing that it is possible to achieve a BLEU score of 10.1 with a small transformer model. Releasing the dataset can help enable future research on this language, similar languages or other languages with scarce resources.
</details>
<details>
<summary>摘要</summary>
eti：一种古代的意大利语言，自公元7世纪起至公元1世纪止使用。目前无任何 native speaker，资源稀缺，只有约12,000件 known inscriptions。根据我们所知，没有公开available Etruscan corpora for natural language processing。因此，我们提出了一个从Etruscan到英语的机器翻译数据集，包含2891个翻译例子，其中一些是手动提取的，而另外一些是自动获取的。同时，我们对不同的机器翻译模型进行了benchmarking，发现可以达到10.1的BLEU分数，使用一个小型transformer模型。释放这个数据集可以帮助未来的研究这种语言、相似语言或其他语言with scarce resources。
</details></li>
</ul>
<hr>
<h2 id="A-Closer-Look-into-Automatic-Evaluation-Using-Large-Language-Models"><a href="#A-Closer-Look-into-Automatic-Evaluation-Using-Large-Language-Models" class="headerlink" title="A Closer Look into Automatic Evaluation Using Large Language Models"></a>A Closer Look into Automatic Evaluation Using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05657">http://arxiv.org/abs/2310.05657</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/d223302/a-closer-look-to-llm-evaluation">https://github.com/d223302/a-closer-look-to-llm-evaluation</a></li>
<li>paper_authors: Cheng-Han Chiang, Hung-yi Lee</li>
<li>for: This paper aims to evaluate the effectiveness of using large language models (LLMs) for text quality evaluation, and to compare different evaluation methods.</li>
<li>methods: The paper uses two existing methods, LLM evaluation (Chiang and Lee, 2023) and G-Eval (Liu et al., 2023), and analyzes their strengths and weaknesses in terms of correlating with human ratings.</li>
<li>results: The paper finds that the auto Chain-of-Thought (CoT) used in G-Eval does not always improve correlation with human ratings, and that forcing the LLM to output only a numeric rating is suboptimal. Additionally, the paper shows that asking the LLM to explain its own ratings consistently improves the correlation between the ChatGPT and human ratings, and pushes state-of-the-art (SoTA) correlations on two meta-evaluation datasets.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文目的是评估大语言模型（LLM）用于文本质量评估的有效性，并比较不同评估方法。</li>
<li>methods: 论文使用了两种现有的方法，LLM评估（Chiang和Lee，2023）和G-Eval（Liu等，2023），并分析它们在与人类评分相关性方面的优缺点。</li>
<li>results: 论文发现，G-Eval中的自动链条（CoT）并不总是提高与人类评分的相关性，而强制 LLM 输出只有数字评分也是不优化的。此外，论文还显示，让 LLM 解释自己的评分能够一直提高 ChatGPT 和人类评分之间的相关性，并达到状态之狮（SoTA）相关性水平在两个 meta-评估数据集上。<details>
<summary>Abstract</summary>
Using large language models (LLMs) to evaluate text quality has recently gained popularity. Some prior works explore the idea of using LLMs for evaluation, while they differ in some details of the evaluation process. In this paper, we analyze LLM evaluation (Chiang and Lee, 2023) and G-Eval (Liu et al., 2023), and we discuss how those details in the evaluation process change how well the ratings given by LLMs correlate with human ratings. We find that the auto Chain-of-Thought (CoT) used in G-Eval does not always make G-Eval more aligned with human ratings. We also show that forcing the LLM to output only a numeric rating, as in G-Eval, is suboptimal. Last, we reveal that asking the LLM to explain its own ratings consistently improves the correlation between the ChatGPT and human ratings and pushes state-of-the-art (SoTA) correlations on two meta-evaluation datasets.
</details>
<details>
<summary>摘要</summary>
使用大语言模型（LLM）评估文本质量已经很受欢迎。一些先前的工作探讨了使用LLM进行评估的想法，但它们在评估过程中有一些细节的不同。本文分析了LLM评估（Chiang和Lee，2023）和G-Eval（Liu等，2023），并分析了评估过程中的细节如何影响LLM给出的评分与人类评分之间的相关性。我们发现自动链条（CoT）使用在G-Eval中并不总是使G-Eval更加与人类评分相关。此外，我们还示出了让LLM输出只有数值评分，如G-Eval中所做的，是不优化的。最后，我们发现让LLM解释自己的评分能够一直保持和人类评分的相关性，并Push state-of-the-art（SoTA）相关性在两个meta-评估数据集上。
</details></li>
</ul>
<hr>
<h2 id="RAUCG-Retrieval-Augmented-Unsupervised-Counter-Narrative-Generation-for-Hate-Speech"><a href="#RAUCG-Retrieval-Augmented-Unsupervised-Counter-Narrative-Generation-for-Hate-Speech" class="headerlink" title="RAUCG: Retrieval-Augmented Unsupervised Counter Narrative Generation for Hate Speech"></a>RAUCG: Retrieval-Augmented Unsupervised Counter Narrative Generation for Hate Speech</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05650">http://arxiv.org/abs/2310.05650</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuyu Jiang, Wenyi Tang, Xingshu Chen, Rui Tanga, Haizhou Wang, Wenxian Wang</li>
<li>for: 本研究旨在提出一种自动生成Counter Narrative（CN）的方法，以便在互联网上防止仇恨言论（HS）无需削弱言论自由。</li>
<li>methods: 本研究使用自然语言生成技术来自动生成CN，并提出了一种 Retrieval-Augmented Unsupervised Counter Narrative Generation（RAUCG）方法。RAUCG方法包括SSF检索方法、能量基于扩展知识的解码机制和不间断的改进。</li>
<li>results: 实验结果表明，RAUCG方法在语言质量、毒害性、诱导力、相关性和HS防止成功率等方面都有显著改进，与强基eline相比，RAUCG方法在所有指标上都有提高。此外，RAUCG方法使得GPT2超过T0在所有指标上表现。<details>
<summary>Abstract</summary>
The Counter Narrative (CN) is a promising approach to combat online hate speech (HS) without infringing on freedom of speech. In recent years, there has been a growing interest in automatically generating CNs using natural language generation techniques. However, current automatic CN generation methods mainly rely on expert-authored datasets for training, which are time-consuming and labor-intensive to acquire. Furthermore, these methods cannot directly obtain and extend counter-knowledge from external statistics, facts, or examples. To address these limitations, we propose Retrieval-Augmented Unsupervised Counter Narrative Generation (RAUCG) to automatically expand external counter-knowledge and map it into CNs in an unsupervised paradigm. Specifically, we first introduce an SSF retrieval method to retrieve counter-knowledge from the multiple perspectives of stance consistency, semantic overlap rate, and fitness for HS. Then we design an energy-based decoding mechanism by quantizing knowledge injection, countering and fluency constraints into differentiable functions, to enable the model to build mappings from counter-knowledge to CNs without expert-authored CN data. Lastly, we comprehensively evaluate model performance in terms of language quality, toxicity, persuasiveness, relevance, and success rate of countering HS, etc. Experimental results show that RAUCG outperforms strong baselines on all metrics and exhibits stronger generalization capabilities, achieving significant improvements of +2.0% in relevance and +4.5% in success rate of countering metrics. Moreover, RAUCG enabled GPT2 to outperform T0 in all metrics, despite the latter being approximately eight times larger than the former. Warning: This paper may contain offensive or upsetting content!
</details>
<details>
<summary>摘要</summary>
“Counter Narrative（CN）是一种有前途的方法来防止网络诽谤（HS）无须对自由表达造成限制。过去几年，有关自动生成CN的研究愈来愈受到关注。然而，目前的自动CN生成方法主要靠专家撰写的数据进行训练，这是时间consuming和劳动密集的。另外，这些方法不能直接从外部获取和扩展反知识。为了解决这些限制，我们提出了内部扩展无监控Counter Narrative生成（RAUCG），以自动扩展外部反知识并将其映射到CN中。具体来说，我们首先引入SSF搜寻方法，从多种看法的立场一致性、semantic overlap rate和HS适用度等方面搜寻反知识。然后，我们设计了能量基的解oding机制，将知识注入、反驳和流利性的约束转化为可微分函数，让模型从反知识中建立CN映射，不需要专家撰写CN数据。最后，我们对模型表现进行了全面评估，包括语言质量、毒性、说服力、相关性和防止HS成功率等。实验结果显示，RAUCG在所有指标上表现出色，与强基eline进行比较，RAUCG在所有指标上具有+2.0%的相关性和+4.5%的防止HS成功率的改进。此外，RAUCG使得GPT2超越T0，即使T0比GPT2大约8倍。警告：本文可能含有刺激或尴尬的内容！”
</details></li>
</ul>
<hr>
<h2 id="Towards-Verifiable-Generation-A-Benchmark-for-Knowledge-aware-Language-Model-Attribution"><a href="#Towards-Verifiable-Generation-A-Benchmark-for-Knowledge-aware-Language-Model-Attribution" class="headerlink" title="Towards Verifiable Generation: A Benchmark for Knowledge-aware Language Model Attribution"></a>Towards Verifiable Generation: A Benchmark for Knowledge-aware Language Model Attribution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05634">http://arxiv.org/abs/2310.05634</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinze Li, Yixin Cao2, Liangming Pan, Yubo Ma, Aixin Sun</li>
<li>for: 提高大语言模型（LLMs）的可靠性和准确性，并解决 LLMS 的三大核心问题。</li>
<li>methods: 利用知识图（KG）扩展 attrribution 源，并提出“自我不足”设定，考虑模型需要的知识不足。提出了一个全面的自动评估指标，覆盖文本质量、引用质量和文本引用对齐。</li>
<li>results: 通过构建生物领域数据集 BioKaLMA，并开发基线解决方案，显示 LLMS 在引用生成方面存在大量的改进空间，强调需要包含“自我不足”设定，以及重要性取得高度准确的检索精度。<details>
<summary>Abstract</summary>
Although achieving great success, Large Language Models (LLMs) usually suffer from unreliable hallucinations. In this paper, we define a new task of Knowledge-aware Language Model Attribution (KaLMA) that improves upon three core concerns on conventional attributed LMs. First, we extend attribution source from unstructured texts to Knowledge Graph (KG), whose rich structures benefit both the attribution performance and working scenarios. Second, we propose a new ``Conscious Incompetence" setting considering the incomplete knowledge repository, where the model identifies the need for supporting knowledge beyond the provided KG. Third, we propose a comprehensive automatic evaluation metric encompassing text quality, citation quality, and text citation alignment. To implement the above innovations, we build a dataset in biography domain BioKaLMA via a well-designed evolutionary question generation strategy, to control the question complexity and necessary knowledge to the answer. For evaluation, we develop a baseline solution and demonstrate the room for improvement in LLMs' citation generation, emphasizing the importance of incorporating the "Conscious Incompetence" setting, and the critical role of retrieval accuracy.
</details>
<details>
<summary>摘要</summary>
尽管大成功的大语言模型（LLM）通常受到不可靠的幻觉的影响，在这篇论文中，我们定义了一个新的知识感知语言模型归因（KaLMA）任务，以改进传统归因语言模型中的三大核心问题。首先，我们将归因源从文本扩展到知识图（KG）， whose rich structures benefit both the attribution performance and working scenarios。其次，我们提出了一种“意识不足”的设定，考虑到知识库的缺失，使模型可以识别需要支持知识的情况。最后，我们提出了一个全面的自动评估指标，涵盖文本质量、引用质量和文本引用对齐。为实现以上创新，我们建立了一个在生传记领域的 BioKaLMA 数据集，通过一种Well-designed演化问题生成策略来控制问题复杂性和需要的知识。为评估，我们开发了一个基线解决方案，并示出了 LLMs 的引用生成中的改进空间，强调了在“意识不足”设定下的重要性，以及检索准确性的关键性。
</details></li>
</ul>
<hr>
<h2 id="Glitter-or-Gold-Deriving-Structured-Insights-from-Sustainability-Reports-via-Large-Language-Models"><a href="#Glitter-or-Gold-Deriving-Structured-Insights-from-Sustainability-Reports-via-Large-Language-Models" class="headerlink" title="Glitter or Gold? Deriving Structured Insights from Sustainability Reports via Large Language Models"></a>Glitter or Gold? Deriving Structured Insights from Sustainability Reports via Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05628">http://arxiv.org/abs/2310.05628</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marco Bronzini, Carlo Nicolini, Bruno Lepri, Andrea Passerini, Jacopo Staiano</li>
<li>for: This paper aims to provide a framework for extracting and analyzing non-financial information from sustainability reports to support investors’ ESG-related decision-making.</li>
<li>methods: The authors use Large Language Models (LLMs), Retrieved Augmented Generation, and in-context learning to extract semantically structured information from sustainability reports. They also employ graph-based representations to analyze the obtained findings.</li>
<li>results: The authors generate meaningful statistical, similarity, and correlation analyses concerning the sustainability actions undertaken across industries, sectors, and regions. They also investigate the factors that impact companies’ ESG scores using their findings and other company information.<details>
<summary>Abstract</summary>
Over the last decade, several regulatory bodies have started requiring the disclosure of non-financial information from publicly listed companies, in light of the investors' increasing attention to Environmental, Social, and Governance (ESG) issues. Such information is publicly released in a variety of non-structured and multi-modal documentation. Hence, it is not straightforward to aggregate and consolidate such data in a cohesive framework to further derive insights about sustainability practices across companies and markets. Thus, it is natural to resort to Information Extraction (IE) techniques to provide concise, informative and actionable data to the stakeholders. Moving beyond traditional text processing techniques, in this work we leverage Large Language Models (LLMs), along with prominent approaches such as Retrieved Augmented Generation and in-context learning, to extract semantically structured information from sustainability reports. We then adopt graph-based representations to generate meaningful statistical, similarity and correlation analyses concerning the obtained findings, highlighting the prominent sustainability actions undertaken across industries and discussing emerging similarity and disclosing patterns at company, sector and region levels. Lastly, we investigate which factual aspects impact the most on companies' ESG scores using our findings and other company information.
</details>
<details>
<summary>摘要</summary>
In this work, we leverage Large Language Models (LLMs) and prominent approaches such as Retrieved Augmented Generation and in-context learning to extract semantically structured information from sustainability reports. We then use graph-based representations to generate meaningful statistical, similarity, and correlation analyses of the obtained findings, highlighting the prominent sustainability actions undertaken across industries and discussing emerging similarity and disclosure patterns at company, sector, and region levels.Finally, we investigate which factual aspects have the greatest impact on companies' ESG scores using our findings and other company information.
</details></li>
</ul>
<hr>
<h2 id="Integrating-Stock-Features-and-Global-Information-via-Large-Language-Models-for-Enhanced-Stock-Return-Prediction"><a href="#Integrating-Stock-Features-and-Global-Information-via-Large-Language-Models-for-Enhanced-Stock-Return-Prediction" class="headerlink" title="Integrating Stock Features and Global Information via Large Language Models for Enhanced Stock Return Prediction"></a>Integrating Stock Features and Global Information via Large Language Models for Enhanced Stock Return Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05627">http://arxiv.org/abs/2310.05627</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yujie Ding, Shuai Jia, Tianyi Ma, Bingcheng Mao, Xiuze Zhou, Liuliu Li, Dongming Han</li>
<li>for: 这个研究旨在将大语言模型（LLMs）如ChatGPT和GPT-4 integrating into existing quantitative investment models, in order to improve the accuracy of stock return predictions.</li>
<li>methods: 本研究提出了一个 novel framework, which consists of two components: (1) the Local-Global (LG) model, which introduces three distinct strategies for modeling global information, and (2) Self-Correlated Reinforcement Learning (SCRL), which focuses on aligning the embeddings of financial news generated by LLMs with stock features within the same semantic space.</li>
<li>results: 经过实现本研究的框架后, 在中国A股市场中的 Rank Information Coefficient和回归表现得到了提高, 特别是与仅使用股票特征的模型相比。<details>
<summary>Abstract</summary>
The remarkable achievements and rapid advancements of Large Language Models (LLMs) such as ChatGPT and GPT-4 have showcased their immense potential in quantitative investment. Traders can effectively leverage these LLMs to analyze financial news and predict stock returns accurately. However, integrating LLMs into existing quantitative models presents two primary challenges: the insufficient utilization of semantic information embedded within LLMs and the difficulties in aligning the latent information within LLMs with pre-existing quantitative stock features. We propose a novel framework consisting of two components to surmount these challenges. The first component, the Local-Global (LG) model, introduces three distinct strategies for modeling global information. These approaches are grounded respectively on stock features, the capabilities of LLMs, and a hybrid method combining the two paradigms. The second component, Self-Correlated Reinforcement Learning (SCRL), focuses on aligning the embeddings of financial news generated by LLMs with stock features within the same semantic space. By implementing our framework, we have demonstrated superior performance in Rank Information Coefficient and returns, particularly compared to models relying only on stock features in the China A-share market.
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLM）如ChatGPT和GPT-4的出色成就和快速进步，曝光了它们在量化投资中的巨大潜力。投资者可以充分利用这些LLM来分析金融新闻并准确预测股票回报。然而，将LLM integrate into现有的量化模型存在两个主要挑战：一是在LLM中嵌入的 semantics信息的不足利用，二是将LLM中的幽默信息与现有的量化股票特征进行对齐。我们提出了一个新的框架，包括两个组成部分：1. 本地-全局（LG）模型，这个模型引入了三种不同的全局模型策略，这些策略分别基于股票特征、LLM的能力和两种思想的混合方法。2. 自相关束力学学习（SCRL），它专注于将LLM生成的金融新闻嵌入与股票特征在同一个semantic空间进行对齐。通过实施我们的框架，我们在中国A股市场中示出了与只使用股票特征模型相比更高的rank信息系数和回报表现。
</details></li>
</ul>
<hr>
<h2 id="LAiW-A-Chinese-Legal-Large-Language-Models-Benchmark-A-Technical-Report"><a href="#LAiW-A-Chinese-Legal-Large-Language-Models-Benchmark-A-Technical-Report" class="headerlink" title="LAiW: A Chinese Legal Large Language Models Benchmark (A Technical Report)"></a>LAiW: A Chinese Legal Large Language Models Benchmark (A Technical Report)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05620">http://arxiv.org/abs/2310.05620</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dai-shen/laiw">https://github.com/dai-shen/laiw</a></li>
<li>paper_authors: Yongfu Dai, Duanyu Feng, Jimin Huang, Haochen Jia, Qianqian Xie, Yifang Zhang, Weiguang Han, Wei Tian, Hao Wang</li>
<li>for: 评估当前许多法律LLM的法律能力。</li>
<li>methods: 分为三级：基础法律NLP能力、基础法律应用能力和复杂法律应用能力。</li>
<li>results: 第一阶段的评估结果显示，虽有一些法律LLM的表现更好于其基础模型，但还有一定差距与ChatGPT相比。<details>
<summary>Abstract</summary>
With the emergence of numerous legal LLMs, there is currently a lack of a comprehensive benchmark for evaluating their legal abilities. In this paper, we propose the first Chinese Legal LLMs benchmark based on legal capabilities. Through the collaborative efforts of legal and artificial intelligence experts, we divide the legal capabilities of LLMs into three levels: basic legal NLP capability, basic legal application capability, and complex legal application capability. We have completed the first phase of evaluation, which mainly focuses on the capability of basic legal NLP. The evaluation results show that although some legal LLMs have better performance than their backbones, there is still a gap compared to ChatGPT. Our benchmark can be found at URL.
</details>
<details>
<summary>摘要</summary>
“现在有很多法律 LLMS 出现，但是无法确定这些 LLMS 的法律能力水平。在这篇论文中，我们提出了第一个基于法律能力的中国 LLMS benchmark。通过法律和人工智能专家的共同努力，我们将 LLMS 的法律能力分为三级：基础法律 NLP 能力、基础法律应用能力和复杂法律应用能力。我们已经完成了第一阶段的评估，主要是关于基础法律 NLP 的能力。评估结果显示，虽然一些法律 LLMS 的性能比其底层更好，但还有一定的差距与 ChatGPT 相比。我们的 benchmark 可以在 URL 上找到。”Note: "LLMS" stands for "Legal Language Models" in this context.
</details></li>
</ul>
<hr>
<h2 id="Can-language-models-learn-analogical-reasoning-Investigating-training-objectives-and-comparisons-to-human-performance"><a href="#Can-language-models-learn-analogical-reasoning-Investigating-training-objectives-and-comparisons-to-human-performance" class="headerlink" title="Can language models learn analogical reasoning? Investigating training objectives and comparisons to human performance"></a>Can language models learn analogical reasoning? Investigating training objectives and comparisons to human performance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05597">http://arxiv.org/abs/2310.05597</a></li>
<li>repo_url: None</li>
<li>paper_authors: Molly R. Petersen, Lonneke van der Plas</li>
<li>for: 测试模型是否可以学习基本的analogical reasoning，并使用更常见的人类语言 analogies 进行评估。</li>
<li>methods: 使用小量数据进行模型训练，并对模型的性能进行比较，以及与人类基准数据进行比较。</li>
<li>results: 结果显示，模型可以学习analogical reasoning，即使只使用小量数据。此外，模型在训练后与人类基准数据的性能相似。<details>
<summary>Abstract</summary>
While analogies are a common way to evaluate word embeddings in NLP, it is also of interest to investigate whether or not analogical reasoning is a task in itself that can be learned. In this paper, we test several ways to learn basic analogical reasoning, specifically focusing on analogies that are more typical of what is used to evaluate analogical reasoning in humans than those in commonly used NLP benchmarks. Our experiments find that models are able to learn analogical reasoning, even with a small amount of data. We additionally compare our models to a dataset with a human baseline, and find that after training, models approach human performance.
</details>
<details>
<summary>摘要</summary>
而 analogies 是一种常见的方式来评估 word embeddings 在自然语言处理中，但是还是有趣的问题是否可以学习 analogical reasoning。在这篇文章中，我们测试了几种方式来学习基本的analogical reasoning，特别是关注常用于评估人类的 analogies，而不是常用于 NLP benchmarks 中的 analogies。我们的实验发现，模型可以学习 analogical reasoning，即使只有一小 amount of data。我们还对比了我们的模型与人类基准数据，发现， después de 训练，模型接近人类性能。
</details></li>
</ul>
<hr>
<h2 id="DRIN-Dynamic-Relation-Interactive-Network-for-Multimodal-Entity-Linking"><a href="#DRIN-Dynamic-Relation-Interactive-Network-for-Multimodal-Entity-Linking" class="headerlink" title="DRIN: Dynamic Relation Interactive Network for Multimodal Entity Linking"></a>DRIN: Dynamic Relation Interactive Network for Multimodal Entity Linking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05589">http://arxiv.org/abs/2310.05589</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/starreeze/drin">https://github.com/starreeze/drin</a></li>
<li>paper_authors: Shangyu Xing, Fei Zhao, Zhen Wu, Chunhui Li, Jianbing Zhang, Xinyu Dai</li>
<li>for: 本研究旨在解决多模态Entity Linking（MEL）任务中的匹配批处和多样性问题。</li>
<li>methods: 我们提出了一种新的Dynamic Relation Interactive Network（DRIN）模型，其中明确地表示了四种不同的匹配对象之间的对应关系，并通过动态的Graph Convolutional Network（GCN）选择对应的对应关系，以适应不同的输入样本。</li>
<li>results: 我们在两个数据集上进行了实验，并证明了DRIN比前STATE-OF-THE-ART方法提高了大量的性能。<details>
<summary>Abstract</summary>
Multimodal Entity Linking (MEL) is a task that aims to link ambiguous mentions within multimodal contexts to referential entities in a multimodal knowledge base. Recent methods for MEL adopt a common framework: they first interact and fuse the text and image to obtain representations of the mention and entity respectively, and then compute the similarity between them to predict the correct entity. However, these methods still suffer from two limitations: first, as they fuse the features of text and image before matching, they cannot fully exploit the fine-grained alignment relations between the mention and entity. Second, their alignment is static, leading to low performance when dealing with complex and diverse data. To address these issues, we propose a novel framework called Dynamic Relation Interactive Network (DRIN) for MEL tasks. DRIN explicitly models four different types of alignment between a mention and entity and builds a dynamic Graph Convolutional Network (GCN) to dynamically select the corresponding alignment relations for different input samples. Experiments on two datasets show that DRIN outperforms state-of-the-art methods by a large margin, demonstrating the effectiveness of our approach.
</details>
<details>
<summary>摘要</summary>
多modal实体连接（MEL）是一个任务，旨在将多modal上的潜在含义链接到多modal知识库中的参照实体。 current methods for MEL 采用一致的框架：先将文本和图像相互作用，以获得提及和实体的表示，然后计算它们之间的相似性，以预测正确的实体。 However, these methods still suffer from two limitations: first, as they fuse the features of text and image before matching, they cannot fully exploit the fine-grained alignment relations between the mention and entity. Second, their alignment is static, leading to low performance when dealing with complex and diverse data.To address these issues, we propose a novel framework called Dynamic Relation Interactive Network (DRIN) for MEL tasks. DRIN explicitly models four different types of alignment between a mention and entity and builds a dynamic Graph Convolutional Network (GCN) to dynamically select the corresponding alignment relations for different input samples. Experiments on two datasets show that DRIN outperforms state-of-the-art methods by a large margin, demonstrating the effectiveness of our approach.
</details></li>
</ul>
<hr>
<h2 id="Regulation-and-NLP-RegNLP-Taming-Large-Language-Models"><a href="#Regulation-and-NLP-RegNLP-Taming-Large-Language-Models" class="headerlink" title="Regulation and NLP (RegNLP): Taming Large Language Models"></a>Regulation and NLP (RegNLP): Taming Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05553">http://arxiv.org/abs/2310.05553</a></li>
<li>repo_url: None</li>
<li>paper_authors: Catalina Goanta, Nikolaos Aletras, Ilias Chalkidis, Sofia Ranchordas, Gerasimos Spanakis</li>
<li>for: 这篇论文目的是探讨 NLP 研究如何受益于与法规研究的互动，以便更好地评估和控制风险。</li>
<li>methods: 论文使用的方法包括研究现有的法规研究和 NLP 研究，以及将这两者相互连接。</li>
<li>results: 论文指出了现有 NLP 研究中关于风险评估的缺陷，并提出了一种新的多学科研究空间（RegNLP），以便将科学知识与法规过程相连。<details>
<summary>Abstract</summary>
The scientific innovation in Natural Language Processing (NLP) and more broadly in artificial intelligence (AI) is at its fastest pace to date. As large language models (LLMs) unleash a new era of automation, important debates emerge regarding the benefits and risks of their development, deployment and use. Currently, these debates have been dominated by often polarized narratives mainly led by the AI Safety and AI Ethics movements. This polarization, often amplified by social media, is swaying political agendas on AI regulation and governance and posing issues of regulatory capture. Capture occurs when the regulator advances the interests of the industry it is supposed to regulate, or of special interest groups rather than pursuing the general public interest. Meanwhile in NLP research, attention has been increasingly paid to the discussion of regulating risks and harms. This often happens without systematic methodologies or sufficient rooting in the disciplines that inspire an extended scope of NLP research, jeopardizing the scientific integrity of these endeavors. Regulation studies are a rich source of knowledge on how to systematically deal with risk and uncertainty, as well as with scientific evidence, to evaluate and compare regulatory options. This resource has largely remained untapped so far. In this paper, we argue how NLP research on these topics can benefit from proximity to regulatory studies and adjacent fields. We do so by discussing basic tenets of regulation, and risk and uncertainty, and by highlighting the shortcomings of current NLP discussions dealing with risk assessment. Finally, we advocate for the development of a new multidisciplinary research space on regulation and NLP (RegNLP), focused on connecting scientific knowledge to regulatory processes based on systematic methodologies.
</details>
<details>
<summary>摘要</summary>
科学创新在自然语言处理（NLP）和人工智能（AI）领域正在进行最快的发展。大语言模型（LLMs）在新的自动化时代引入了重要的争议，包括开发、部署和使用的优点和风险。目前，这些争议主要由人工智能安全和人工智能伦理运动领导。这种偏见，经常通过社交媒体扩大，正在影响政策制定和人工智能管理，并且存在政策捕捉的问题。在NLP研究中，越来越多的注意力被集中在评估风险和害处的问题上，但这些讨论通常缺乏系统化的方法和 suficient的基础知识，从而威胁NLP研究的科学 integriy。在这篇论文中，我们 argue that NLP研究可以从靠近 regulatory studies 和相关领域中受益。我们讨论了基本的管制原则，以及风险和不确定性的问题，并指出了当前NLP讨论中评估风险的缺陷。最后，我们倡议成立一个新的多学科研究空间（RegNLP），专注于将科学知识与管制过程相连，基于系统化的方法。
</details></li>
</ul>
<hr>
<h2 id="Findings-of-the-2023-ML-SUPERB-Challenge-Pre-Training-and-Evaluation-over-More-Languages-and-Beyond"><a href="#Findings-of-the-2023-ML-SUPERB-Challenge-Pre-Training-and-Evaluation-over-More-Languages-and-Beyond" class="headerlink" title="Findings of the 2023 ML-SUPERB Challenge: Pre-Training and Evaluation over More Languages and Beyond"></a>Findings of the 2023 ML-SUPERB Challenge: Pre-Training and Evaluation over More Languages and Beyond</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05513">http://arxiv.org/abs/2310.05513</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiatong Shi, William Chen, Dan Berrebbi, Hsiu-Hsuan Wang, Wei-Ping Huang, En-Pei Hu, Ho-Lam Chuang, Xuankai Chang, Yuxun Tang, Shang-Wen Li, Abdelrahman Mohamed, Hung-yi Lee, Shinji Watanabe</li>
<li>for: 本研究旨在探讨多语言speech recognition和语言识别领域中自主学习模型的应用。</li>
<li>methods: 本研究使用了SUPERB框架，并对多语言speech recognition和语言识别进行了自主学习模型的应用。</li>
<li>results: 研究发现，即便扩大模型规模，也不是一定能解决多语言speech任务中的所有挑战。不同的speech&#x2F;voice类型对多语言speech处理具有显著的挑战。<details>
<summary>Abstract</summary>
The 2023 Multilingual Speech Universal Performance Benchmark (ML-SUPERB) Challenge expands upon the acclaimed SUPERB framework, emphasizing self-supervised models in multilingual speech recognition and language identification. The challenge comprises a research track focused on applying ML-SUPERB to specific multilingual subjects, a Challenge Track for model submissions, and a New Language Track where language resource researchers can contribute and evaluate their low-resource language data in the context of the latest progress in multilingual speech recognition. The challenge garnered 12 model submissions and 54 language corpora, resulting in a comprehensive benchmark encompassing 154 languages. The findings indicate that merely scaling models is not the definitive solution for multilingual speech tasks, and a variety of speech/voice types present significant challenges in multilingual speech processing.
</details>
<details>
<summary>摘要</summary>
2023年多语言语音通用性表现 benchmark (ML-SUPERB) 挑战，将原有的 SUPERB 框架扩展到多语言语音识别和语言标识领域，强调自动标注模型。挑战包括应用于特定多语言主题的研究车道、挑战车道 для模型提交，以及新语言车道，其中语言资源研究人员可以在最新的多语言语音识别技术下评估和投入低资源语言数据。挑战共收到12个模型提交和54个语言资料库，创造了154种语言的全面 benchmark。发现表明，只有简单地扩大模型的规模并不是多语言语音任务的绝佳解决方案，多种语音/voice 类型在多语言语音处理中具有重要挑战。
</details></li>
</ul>
<hr>
<h2 id="XAL-EXplainable-Active-Learning-Makes-Classifiers-Better-Low-resource-Learners"><a href="#XAL-EXplainable-Active-Learning-Makes-Classifiers-Better-Low-resource-Learners" class="headerlink" title="XAL: EXplainable Active Learning Makes Classifiers Better Low-resource Learners"></a>XAL: EXplainable Active Learning Makes Classifiers Better Low-resource Learners</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05502">http://arxiv.org/abs/2310.05502</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/luoxiaoheics/xal">https://github.com/luoxiaoheics/xal</a></li>
<li>paper_authors: Yun Luo, Zhen Yang, Fandong Meng, Yingjie Li, Fang Guo, Qinglin Qi, Jie Zhou, Yue Zhang</li>
<li>for: The paper is written for proposing a novel Explainable Active Learning (XAL) framework for low-resource text classification, which aims to encourage classifiers to justify their inferences and delve into unlabeled data for which they cannot provide reasonable explanations.</li>
<li>methods: The paper uses a pre-trained bi-directional encoder for classification, and employs a pre-trained uni-directional decoder to generate and score the explanation. A ranking loss is proposed to enhance the decoder’s capability in scoring explanations. During the selection of unlabeled data, the paper combines the predictive uncertainty of the encoder and the explanation score of the decoder to acquire informative data for annotation.</li>
<li>results: The paper achieves substantial improvement on all six tasks over previous Active Learning (AL) methods, and ablation studies demonstrate the effectiveness of each component. Human evaluation shows that the model trained in XAL performs surprisingly well in explaining its prediction.<details>
<summary>Abstract</summary>
Active learning aims to construct an effective training set by iteratively curating the most informative unlabeled data for annotation, which is practical in low-resource tasks. Most active learning techniques in classification rely on the model's uncertainty or disagreement to choose unlabeled data. However, previous work indicates that existing models are poor at quantifying predictive uncertainty, which can lead to over-confidence in superficial patterns and a lack of exploration. Inspired by the cognitive processes in which humans deduce and predict through causal information, we propose a novel Explainable Active Learning framework (XAL) for low-resource text classification, which aims to encourage classifiers to justify their inferences and delve into unlabeled data for which they cannot provide reasonable explanations. Specifically, besides using a pre-trained bi-directional encoder for classification, we employ a pre-trained uni-directional decoder to generate and score the explanation. A ranking loss is proposed to enhance the decoder's capability in scoring explanations. During the selection of unlabeled data, we combine the predictive uncertainty of the encoder and the explanation score of the decoder to acquire informative data for annotation.   As XAL is a general framework for text classification, we test our methods on six different classification tasks. Extensive experiments show that XAL achieves substantial improvement on all six tasks over previous AL methods. Ablation studies demonstrate the effectiveness of each component, and human evaluation shows that the model trained in XAL performs surprisingly well in explaining its prediction.
</details>
<details>
<summary>摘要</summary>
aktive lerning 目的是建立一个有效的训练集 by 遍历最有用的无标例数据进行标签，尤其适用于低资源任务。大多数 aktive lerning 技术在分类中依赖模型的不确定性或争议选择无标例数据。然而，先前的工作表明，现有的模型对于预测不确定性的评估不善，可能会导致超过自信和 superficiale 的模式，而无法探索。 inspirited by 人类的认知过程中的推理和预测，我们提出了一个 novel Explainable Active Learning 框架 (XAL) 供低资源文本分类， aiming to encourage classifiers to justify their inferences and delve into unlabeled data for which they cannot provide reasonable explanations. Specifically, besides using a pre-trained bi-directional encoder for classification, we employ a pre-trained uni-directional decoder to generate and score the explanation. A ranking loss is proposed to enhance the decoder's capability in scoring explanations. During the selection of unlabeled data, we combine the predictive uncertainty of the encoder and the explanation score of the decoder to acquire informative data for annotation.  As XAL is a general framework for text classification, we test our methods on six different classification tasks. Extensive experiments show that XAL achieves substantial improvement on all six tasks over previous AL methods. Ablation studies demonstrate the effectiveness of each component, and human evaluation shows that the model trained in XAL performs surprisingly well in explaining its prediction.
</details></li>
</ul>
<hr>
<h2 id="IDTraffickers-An-Authorship-Attribution-Dataset-to-link-and-connect-Potential-Human-Trafficking-Operations-on-Text-Escort-Advertisements"><a href="#IDTraffickers-An-Authorship-Attribution-Dataset-to-link-and-connect-Potential-Human-Trafficking-Operations-on-Text-Escort-Advertisements" class="headerlink" title="IDTraffickers: An Authorship Attribution Dataset to link and connect Potential Human-Trafficking Operations on Text Escort Advertisements"></a>IDTraffickers: An Authorship Attribution Dataset to link and connect Potential Human-Trafficking Operations on Text Escort Advertisements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05484">http://arxiv.org/abs/2310.05484</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vageesh Saxena, Benjamin Bashpole, Gijs Van Dijck, Gerasimos Spanakis</li>
<li>for: 本研究旨在帮助法 enforcement 机构（LEA）更好地识别和连接人口贩卖（HT）案件和在线广告（ads）。</li>
<li>methods: 本研究使用了87,595个文本广告和5,244个vendor标签来建立一个庞大的IDTraffickers数据集，并使用了DeCLUTR-small模型进行训练，以实现闭包集合分类环境中的macro-F1分数0.8656。</li>
<li>results: 通过使用训练过的分类器提取的样式表示，实现了基于开集排序环境的mean r-precision分数0.8852，以便更好地识别潜在的HT指示器。<details>
<summary>Abstract</summary>
Human trafficking (HT) is a pervasive global issue affecting vulnerable individuals, violating their fundamental human rights. Investigations reveal that a significant number of HT cases are associated with online advertisements (ads), particularly in escort markets. Consequently, identifying and connecting HT vendors has become increasingly challenging for Law Enforcement Agencies (LEAs). To address this issue, we introduce IDTraffickers, an extensive dataset consisting of 87,595 text ads and 5,244 vendor labels to enable the verification and identification of potential HT vendors on online escort markets. To establish a benchmark for authorship identification, we train a DeCLUTR-small model, achieving a macro-F1 score of 0.8656 in a closed-set classification environment. Next, we leverage the style representations extracted from the trained classifier to conduct authorship verification, resulting in a mean r-precision score of 0.8852 in an open-set ranking environment. Finally, to encourage further research and ensure responsible data sharing, we plan to release IDTraffickers for the authorship attribution task to researchers under specific conditions, considering the sensitive nature of the data. We believe that the availability of our dataset and benchmarks will empower future researchers to utilize our findings, thereby facilitating the effective linkage of escort ads and the development of more robust approaches for identifying HT indicators.
</details>
<details>
<summary>摘要</summary>
人口贩卖（HT）是一个广泛存在的全球问题，影响到抵触的个人，违反其基本人权。调查表明，许多HT案件与在线广告（ads）相关，特别是在escort市场上。因此，为了识别和连接HT提供者而成为了法 enforcement agencies（LEAs）的挑战。为解决这个问题，我们介绍IDTraffickers，一个包含87,595个文本广告和5,244个提供者标签的广泛的数据集，以启用在线escort市场上的HT提供者验证和识别。为建立作者鉴定的标准，我们训练了DeCLUTR-small模型，在closed-set分类环境中实现了macro-F1分数0.8656。然后，我们利用训练出来的样式表示来进行作者鉴定，在开放集排名环境中实现了mean r-precision分数0.8852。最后，为促进未来研究和负责任数据分享，我们计划将IDTraffickers数据集和benchmark分发给研究人员，但是需要特定的条件，考虑到数据的敏感性。我们认为，随着我们的数据和benchmark的可用性，未来的研究人员将能够利用我们的发现，从而促进escort广告和HT指标的有效链接，并开发更加坚强的HT指标识别方法。
</details></li>
</ul>
<hr>
<h2 id="Empower-Nested-Boolean-Logic-via-Self-Supervised-Curriculum-Learning"><a href="#Empower-Nested-Boolean-Logic-via-Self-Supervised-Curriculum-Learning" class="headerlink" title="Empower Nested Boolean Logic via Self-Supervised Curriculum Learning"></a>Empower Nested Boolean Logic via Self-Supervised Curriculum Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05450">http://arxiv.org/abs/2310.05450</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gingasan/boolkill">https://github.com/gingasan/boolkill</a></li>
<li>paper_authors: Hongqiu Wu, Linfeng Liu, Hai Zhao, Min Zhang</li>
<li>for: 检验语言模型是否具有强大的推理能力，而不仅仅是因为数据训练。</li>
<li>methods: 使用自然语言处理技术，对语言模型进行自我超vised学习，逐步增加复杂的逻辑逻辑，从 simpler to harder。</li>
<li>results: 语言模型通过this新的自我超vised学习方法（\textsc{Clr），能够有效地推理更加复杂和长距离的逻辑。<details>
<summary>Abstract</summary>
Beyond the great cognitive powers showcased by language models, it is crucial to scrutinize whether their reasoning capabilities stem from strong generalization or merely exposure to relevant data. As opposed to constructing increasingly complex logic, this paper probes into the boolean logic, the root capability of a logical reasoner. We find that any pre-trained language models even including large language models only behave like a random selector in the face of multi-nested boolean logic, a task that humans can handle with ease. To empower language models with this fundamental capability, this paper proposes a new self-supervised learning method \textit{Curriculum Logical Reasoning} (\textsc{Clr}), where we augment the training data with nested boolean logic chain step-by-step, and program the training from simpler logical patterns gradually to harder ones. This new training paradigm allows language models to effectively generalize to much harder and longer-hop logic, which can hardly be learned through naive training. Furthermore, we show that boolean logic is a great foundation for improving the subsequent general logical tasks.
</details>
<details>
<summary>摘要</summary>
更 beyond the great cognitive powers displayed by language models, it is crucial to examine whether their reasoning abilities are based on strong generalization or simply exposure to relevant data. Unlike constructing increasingly complex logic, this paper explores the boolean logic, the fundamental capability of a logical reasoner. We find that pre-trained language models, including large language models, can only perform like a random selector when faced with multi-nested boolean logic, a task that humans can handle easily. To empower language models with this fundamental capability, this paper proposes a new self-supervised learning method called \textsc{Curriculum Logical Reasoning} (\textsc{Clr}), where we gradually add nested boolean logic chains to the training data, starting with simpler logical patterns and gradually increasing the difficulty. This new training paradigm enables language models to effectively generalize to much harder and longer-hop logic, which cannot be learned through naive training. Furthermore, we show that boolean logic provides a solid foundation for improving subsequent general logical tasks.
</details></li>
</ul>
<hr>
<h2 id="Establishing-Trustworthiness-Rethinking-Tasks-and-Model-Evaluation"><a href="#Establishing-Trustworthiness-Rethinking-Tasks-and-Model-Evaluation" class="headerlink" title="Establishing Trustworthiness: Rethinking Tasks and Model Evaluation"></a>Establishing Trustworthiness: Rethinking Tasks and Model Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05442">http://arxiv.org/abs/2310.05442</a></li>
<li>repo_url: None</li>
<li>paper_authors: Robert Litschko, Max Müller-Eberstein, Rob van der Goot, Leon Weber, Barbara Plank</li>
<li>for: 理解自然语言处理（NLP）的核心概念和任务的 Computational Modeling，以及如何将其应用于实际场景中。</li>
<li>methods:  traditional compartmentalized approaches for understanding a model’s functional capacity, as well as recommendations for more multi-faceted evaluation protocols.</li>
<li>results:  the need for trustworthy and reliable NLP systems, and the importance of rethinking the traditional notion of language tasks and model evaluation in order to pursue a more holistic view of language.<details>
<summary>Abstract</summary>
Language understanding is a multi-faceted cognitive capability, which the Natural Language Processing (NLP) community has striven to model computationally for decades. Traditionally, facets of linguistic intelligence have been compartmentalized into tasks with specialized model architectures and corresponding evaluation protocols. With the advent of large language models (LLMs) the community has witnessed a dramatic shift towards general purpose, task-agnostic approaches powered by generative models. As a consequence, the traditional compartmentalized notion of language tasks is breaking down, followed by an increasing challenge for evaluation and analysis. At the same time, LLMs are being deployed in more real-world scenarios, including previously unforeseen zero-shot setups, increasing the need for trustworthy and reliable systems. Therefore, we argue that it is time to rethink what constitutes tasks and model evaluation in NLP, and pursue a more holistic view on language, placing trustworthiness at the center. Towards this goal, we review existing compartmentalized approaches for understanding the origins of a model's functional capacity, and provide recommendations for more multi-faceted evaluation protocols.
</details>
<details>
<summary>摘要</summary>
Language understanding is a multi-faceted cognitive capability, which the Natural Language Processing (NLP) community has striven to model computationally for decades. Traditionally, facets of linguistic intelligence have been compartmentalized into tasks with specialized model architectures and corresponding evaluation protocols. With the advent of large language models (LLMs) the community has witnessed a dramatic shift towards general purpose, task-agnostic approaches powered by generative models. As a consequence, the traditional compartmentalized notion of language tasks is breaking down, followed by an increasing challenge for evaluation and analysis. At the same time, LLMs are being deployed in more real-world scenarios, including previously unforeseen zero-shot setups, increasing the need for trustworthy and reliable systems. Therefore, we argue that it is time to rethink what constitutes tasks and model evaluation in NLP, and pursue a more holistic view on language, placing trustworthiness at the center. Towards this goal, we review existing compartmentalized approaches for understanding the origins of a model's functional capacity, and provide recommendations for more multi-faceted evaluation protocols.Here's the translation in Traditional Chinese:语言理解是一种多方面的认知能力，自然语言处理（NLP）社群在数十年来一直努力以计算方式模型。传统上，语言智能的不同方面被分类为特殊的任务，并且运用专门的模型架构和评估协议。 however, with the advent of large language models (LLMs)，社群目睹了一个剧烈的转变，从特定任务的专门模型演化为通用、任务无关的方法，这导致了传统的语言任务分类系统崩溃。这一传统的分类系统崩溃，也导致了评估和分析的问题增加。同时，LLMs 正在更多的实际应用中，包括以前未见的零学习设置，增加了可靠和可信的系统的需求。因此，我们认为现在是重新定义语言任务和模型评估的时候，并将信任性置于中心。以这个目标为导向，我们回顾现有的分类方法，并提供更多的多方面评估协议。
</details></li>
</ul>
<hr>
<h2 id="Fast-and-Robust-Early-Exiting-Framework-for-Autoregressive-Language-Models-with-Synchronized-Parallel-Decoding"><a href="#Fast-and-Robust-Early-Exiting-Framework-for-Autoregressive-Language-Models-with-Synchronized-Parallel-Decoding" class="headerlink" title="Fast and Robust Early-Exiting Framework for Autoregressive Language Models with Synchronized Parallel Decoding"></a>Fast and Robust Early-Exiting Framework for Autoregressive Language Models with Synchronized Parallel Decoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05424">http://arxiv.org/abs/2310.05424</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/raymin0223/fast_robust_early_exit">https://github.com/raymin0223/fast_robust_early_exit</a></li>
<li>paper_authors: Sangmin Bae, Jongwoo Ko, Hwanjun Song, Se-Young Yun</li>
<li>for: 提高 autoregressive 语言模型 的推理延迟</li>
<li>methods: 提出 Fast and Robust Early-Exiting (FREE) 框架，包括 shallow-deep 模块和同步并发解码</li>
<li>results: 在各种生成任务上实质性提高了推理速度，并提出了一种基于 Beta 混合模型的适应阈值估计器来确定适当的信心阈值<details>
<summary>Abstract</summary>
To tackle the high inference latency exhibited by autoregressive language models, previous studies have proposed an early-exiting framework that allocates adaptive computation paths for each token based on the complexity of generating the subsequent token. However, we observed several shortcomings, including performance degradation caused by a state copying mechanism or numerous exit paths, and sensitivity to exit confidence thresholds. Consequently, we propose a Fast and Robust Early-Exiting (FREE) framework, which incorporates a shallow-deep module and a synchronized parallel decoding. Our framework enables faster inference by synchronizing the decoding process of the current token with previously stacked early-exited tokens. Furthermore, as parallel decoding allows us to observe predictions from both shallow and deep models, we present a novel adaptive threshold estimator that exploits a Beta mixture model to determine suitable confidence thresholds. We empirically demonstrated the superiority of our proposed framework on extensive generation tasks.
</details>
<details>
<summary>摘要</summary>
previous studies have proposed an early-exiting framework that allocates adaptive computation paths for each token based on the complexity of generating the subsequent token. However, we observed several shortcomings, including performance degradation caused by a state copying mechanism or numerous exit paths, and sensitivity to exit confidence thresholds. Consequently, we propose a Fast and Robust Early-Exiting (FREE) framework, which incorporates a shallow-deep module and a synchronized parallel decoding. Our framework enables faster inference by synchronizing the decoding process of the current token with previously stacked early-exited tokens. Furthermore, as parallel decoding allows us to observe predictions from both shallow and deep models, we present a novel adaptive threshold estimator that exploits a Beta mixture model to determine suitable confidence thresholds. We empirically demonstrated the superiority of our proposed framework on extensive generation tasks.Here's the translation in Traditional Chinese:previous studies have proposed an early-exiting framework that allocates adaptive computation paths for each token based on the complexity of generating the subsequent token. However, we observed several shortcomings, including performance degradation caused by a state copying mechanism or numerous exit paths, and sensitivity to exit confidence thresholds. Consequently, we propose a Fast and Robust Early-Exiting (FREE) framework, which incorporates a shallow-deep module and a synchronized parallel decoding. Our framework enables faster inference by synchronizing the decoding process of the current token with previously stacked early-exited tokens. Furthermore, as parallel decoding allows us to observe predictions from both shallow and deep models, we present a novel adaptive threshold estimator that exploits a Beta mixture model to determine suitable confidence thresholds. We empirically demonstrated the superiority of our proposed framework on extensive generation tasks.
</details></li>
</ul>
<hr>
<h2 id="Automating-Customer-Service-using-LangChain-Building-custom-open-source-GPT-Chatbot-for-organizations"><a href="#Automating-Customer-Service-using-LangChain-Building-custom-open-source-GPT-Chatbot-for-organizations" class="headerlink" title="Automating Customer Service using LangChain: Building custom open-source GPT Chatbot for organizations"></a>Automating Customer Service using LangChain: Building custom open-source GPT Chatbot for organizations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05421">http://arxiv.org/abs/2310.05421</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keivalya Pandya, Mehfuza Holia<br>for: This research paper aims to automate customer service using a custom Large Language Model (LLM) called LangChain, which can provide personalized, responsive, and context-aware support.methods: The paper proposes a new approach that combines open-source methodologies, web scraping, fine-tuning, and the integration of LangChain into customer service platforms. The research uses data collection via web scraping, embeddings, Google’s Flan T5 XXL, Base, and Small language models for knowledge retrieval, and the integration of a chatbot into customer service platforms.results: The paper shows that the proposed approach can provide real-time support and query resolution, with the chatbot integrated into customer service platforms. The results also demonstrate the ability to scale across industries and organizations, and elevate customer retention, value extraction, and brand image.<details>
<summary>Abstract</summary>
In the digital age, the dynamics of customer service are evolving, driven by technological advancements and the integration of Large Language Models (LLMs). This research paper introduces a groundbreaking approach to automating customer service using LangChain, a custom LLM tailored for organizations. The paper explores the obsolescence of traditional customer support techniques, particularly Frequently Asked Questions (FAQs), and proposes a paradigm shift towards responsive, context-aware, and personalized customer interactions. The heart of this innovation lies in the fusion of open-source methodologies, web scraping, fine-tuning, and the seamless integration of LangChain into customer service platforms. This open-source state-of-the-art framework, presented as "Sahaay," demonstrates the ability to scale across industries and organizations, offering real-time support and query resolution. Key elements of this research encompass data collection via web scraping, the role of embeddings, the utilization of Google's Flan T5 XXL, Base and Small language models for knowledge retrieval, and the integration of the chatbot into customer service platforms. The results section provides insights into their performance and use cases, here particularly within an educational institution. This research heralds a new era in customer service, where technology is harnessed to create efficient, personalized, and responsive interactions. Sahaay, powered by LangChain, redefines the customer-company relationship, elevating customer retention, value extraction, and brand image. As organizations embrace LLMs, customer service becomes a dynamic and customer-centric ecosystem.
</details>
<details>
<summary>摘要</summary>
在数字时代，顾客服务的动力是不断发展，受技术进步和大语言模型（LLM）的整合影响。这篇研究论文提出了一种创新的自动化顾客服务方法，基于自定义的 LangChain LLM，为组织提供了一种新的客户服务模式。论文探讨传统顾客支持技术，特别是常见问题（FAQ）的过时性，并提出了一种新的客户交互模式，强调响应式、上下文感知和个性化的客户交互。这种创新的核心在于将开源方法ologies、网络抓取、精度调整和 LangChain 集成到顾客服务平台上。这个开源的 state-of-the-art 框架，即 "Sahaay"，能够在不同的行业和组织之间扩展，提供实时支持和问题解决。研究的关键元素包括通过网络抓取获取数据、使用 Google 的 Flan T5 XXL、Base 和 Small 语言模型 для知识检索，以及将 chatbot 集成到顾客服务平台上。研究结果提供了这些技术在不同的应用场景中的性能和使用情况，特别是在教育机构中。这项研究标志着客户服务的新时代，通过技术来创造高效、个性化、响应式的客户交互。Sahaay，基于 LangChain，重塑了客户-公司关系，提高客户退货、价值提取和品牌形象。随着组织接受 LLM，客户服务变成了一个动态和客户中心的生态系统。
</details></li>
</ul>
<hr>
<h2 id="mBBC-Exploring-the-Multilingual-Maze"><a href="#mBBC-Exploring-the-Multilingual-Maze" class="headerlink" title="mBBC: Exploring the Multilingual Maze"></a>mBBC: Exploring the Multilingual Maze</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05404">http://arxiv.org/abs/2310.05404</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/PortNLP/mBBC">https://github.com/PortNLP/mBBC</a></li>
<li>paper_authors: Sina Bagheri Nezhad, Ameeta Agrawal</li>
<li>for: 这个论文的目的是评估三种知名的多语言语言模型（mBERT、XLM-R和GPT-3）的性能，以便更好地理解这些模型在不同语言和语言上下文中的表现。</li>
<li>methods: 这个论文使用了自然语言处理技术中的自我超vised任务（下一个单词预测）来评估这些模型的性能，并在多种语言中进行了评估。</li>
<li>results: 研究发现资源水平对模型性能产生关键作用，具有更高资源水平的模型具有更高的准确率。此外，研究还发现了语言家族和字体类型之间复杂的关系，需要进一步的调查和研究，以便更好地理解语言特点和结构变化对模型性能的影响。<details>
<summary>Abstract</summary>
Multilingual language models have gained significant attention in recent years, enabling the development of applications that cater to diverse linguistic contexts. In this paper, we present a comprehensive evaluation of three prominent multilingual language models: mBERT, XLM-R, and GPT-3. Using the self-supervised task of next token prediction, we assess their performance across a diverse set of languages, with a focus on understanding the impact of resource availability, word order, language family, and script type on model accuracy. Our findings reveal that resource availability plays a crucial role in model performance, with higher resource levels leading to improved accuracy. We also identify the complex relationship between resource availability, language families, and script types, highlighting the need for further investigation into language-specific characteristics and structural variations. Additionally, our statistical inference analysis identifies significant features contributing to model performance, providing insights for model selection and deployment. Our study contributes to a deeper understanding of multilingual language models and informs future research and development to enhance their performance and generalizability across languages and linguistic contexts.
</details>
<details>
<summary>摘要</summary>
<translation_language> simplified_chinese</translation_language></SYS>多语言语言模型在最近几年内受到了广泛关注，使得开发能够适应多种语言文化背景的应用程序变得可能。在本文中，我们对三种著名的多语言语言模型——mBERT、XLM-R和GPT-3进行了全面的评估。使用下一个元素预测任务，我们评估了这些模型在不同语言中的表现，并将着眼于资源可用性、字符串顺序、语言家族和文字类型对模型准确率的影响。我们发现资源可用性在模型性能中扮演着关键的角色，高resource levels导致了改进的准确率。我们还发现了语言家族和文字类型之间复杂的关系，这种关系需要进一步的研究，以便更好地理解语言特有的特征和结构上的变化。此外，我们的统计推理分析还提到了对模型性能的重要贡献因素，为未来的模型选择和部署提供了智能。本研究对多语言语言模型的深入理解和未来研发的提高和普适性做出了贡献。
</details></li>
</ul>
<hr>
<h2 id="GROVE-A-Retrieval-augmented-Complex-Story-Generation-Framework-with-A-Forest-of-Evidence"><a href="#GROVE-A-Retrieval-augmented-Complex-Story-Generation-Framework-with-A-Forest-of-Evidence" class="headerlink" title="GROVE: A Retrieval-augmented Complex Story Generation Framework with A Forest of Evidence"></a>GROVE: A Retrieval-augmented Complex Story Generation Framework with A Forest of Evidence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05388">http://arxiv.org/abs/2310.05388</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhihua Wen, Zhiliang Tian, Wei Wu, Yuxin Yang, Yanqi Shi, Zhen Huang, Dongsheng Li</li>
<li>for:  This paper aims to enhance the complexity and credibility of story generation by leveraging information from human-written stories and using a retrieval-augmented story generation framework.</li>
<li>methods: The proposed method uses a retrieval repository of target conditions to produce few-shot examples that serve as prompts for a large language model (LLM). It also employs an “asking-why” prompting scheme to extract a forest of evidence, which is used to compensate for ambiguities in the generated story.</li>
<li>results: The experimental results and numerous examples demonstrate the effectiveness of the proposed method in generating stories with complex and credible plots.<details>
<summary>Abstract</summary>
Conditional story generation is significant in human-machine interaction, particularly in producing stories with complex plots. While Large language models (LLMs) perform well on multiple NLP tasks, including story generation, it is challenging to generate stories with both complex and creative plots. Existing methods often rely on detailed prompts to guide LLMs to meet target conditions, which inadvertently restrict the creative potential of the generated stories. We argue that leveraging information from exemplary human-written stories facilitates generating more diverse plotlines. Delving deeper into story details helps build complex and credible plots. In this paper, we propose a retrieval-au\textbf{G}mented sto\textbf{R}y generation framework with a f\textbf{O}rest of e\textbf{V}id\textbf{E}nce (GROVE) to enhance stories' complexity. We build a retrieval repository for target conditions to produce few-shot examples to prompt LLMs. Additionally, we design an ``asking-why'' prompting scheme that extracts a forest of evidence, providing compensation for the ambiguities that may occur in the generated story. This iterative process uncovers underlying story backgrounds. Finally, we select the most fitting chains of evidence from the evidence forest and integrate them into the generated story, thereby enhancing the narrative's complexity and credibility. Experimental results and numerous examples verify the effectiveness of our method.
</details>
<details>
<summary>摘要</summary>
假设故事生成是人机交互中的重要方面，特别是生成具有复杂剧本的故事。虽然大型语言模型（LLMs）在多种自然语言处理任务中表现良好，但是生成具有复杂和创新剧本的故事仍然是挑战。现有的方法通常靠着详细的提示来引导LLMs，从而限制生成的故事创作潜力。我们认为可以利用人类写的好故事中的信息，以生成更多元的剧本。深入探究故事细节可以建立更加复杂和真实的剧本。在这篇文章中，我们提出了一个具有追踪和补充的故事生成框架（GROVE），以增强故事的复杂性。我们建立了一个目标状况库，以生成少量的示例提示LLMs。此外，我们设计了一个“问题”提示方案，可以从故事中提取一棵证据森林，以补偿可能在生成的故事中出现的歧难。这个迭代过程可以暴露出故事的背景。最后，我们从证据森林中选择最符合的证据链，并将其与生成的故事结合，从而增强故事的复杂性和实际性。实验结果和许多例子证明了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Transcending-the-Attention-Paradigm-Representation-Learning-from-Geospatial-Social-Media-Data"><a href="#Transcending-the-Attention-Paradigm-Representation-Learning-from-Geospatial-Social-Media-Data" class="headerlink" title="Transcending the Attention Paradigm: Representation Learning from Geospatial Social Media Data"></a>Transcending the Attention Paradigm: Representation Learning from Geospatial Social Media Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05378">http://arxiv.org/abs/2310.05378</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/NickDiSanto/Twitter2030/tree/main/Beta">https://github.com/NickDiSanto/Twitter2030/tree/main/Beta</a></li>
<li>paper_authors: Nick DiSanto, Anthony Corso, Benjamin Sanders, Gavin Harding</li>
<li>for:  investigate social media data to uncover abstract relationships and challenge the reliance on complex models</li>
<li>methods: employ Bag-of-Words models specific to each city to analyze Twitter data and evaluate representation</li>
<li>results: discover hidden insights and demonstrate the considerable influence of geographic location on online communication, challenging the notion that intricate models are necessary for pattern recognition<details>
<summary>Abstract</summary>
While transformers have pioneered attention-driven architectures as a cornerstone of research, their dependence on explicitly contextual information underscores limitations in their abilities to tacitly learn overarching textual themes. This study investigates social media data as a source of distributed patterns, challenging the heuristic paradigm of performance benchmarking. In stark contrast to networks that rely on capturing complex long-term dependencies, models of online data inherently lack structure and are forced to learn underlying patterns in the aggregate. To properly represent these abstract relationships, this research dissects empirical social media corpora into their elemental components and analyzes over two billion tweets across population-dense locations. Exploring the relationship between location and vernacular in Twitter data, we employ Bag-of-Words models specific to each city and evaluate their respective representation. This demonstrates that hidden insights can be uncovered without the crutch of advanced algorithms and demonstrates that even amidst noisy data, geographic location has a considerable influence on online communication. This evidence presents tangible insights regarding geospatial communication patterns and their implications in social science. It also challenges the notion that intricate models are prerequisites for pattern recognition in natural language, aligning with the evolving landscape that questions the embrace of absolute interpretability over abstract understanding. This study bridges the divide between sophisticated frameworks and intangible relationships, paving the way for systems that blend structured models with conjectural reasoning.
</details>
<details>
<summary>摘要</summary>
transformers推动了注意力驱动的建筑，但它们对文本主题的潜在学习表现出了局限性。这个研究通过社交媒体数据来挑战传统性能标准的假设，因为模型在线上数据上自然地缺乏结构，需要通过汇总来学习下级 Patterns。为了正确表示这些抽象关系，我们在Twitter数据中分解了实际社交媒体文本，并对全球各地的 tweet 进行分析，检查了地点和方言之间的关系。我们使用特定于每个城市的 Bag-of-Words 模型进行评估，并发现了隐藏的Patterns。这种方法表明，无需复杂的算法，地理位置在在线交流中具有显著的影响。这些证据表明在社会科学中的地理通信模式和其影响，并挑战了人们对精准模型的依赖。这种研究既结合了结构化模型，也结合了推理。
</details></li>
</ul>
<hr>
<h2 id="Improving-End-to-End-Speech-Processing-by-Efficient-Text-Data-Utilization-with-Latent-Synthesis"><a href="#Improving-End-to-End-Speech-Processing-by-Efficient-Text-Data-Utilization-with-Latent-Synthesis" class="headerlink" title="Improving End-to-End Speech Processing by Efficient Text Data Utilization with Latent Synthesis"></a>Improving End-to-End Speech Processing by Efficient Text Data Utilization with Latent Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05374">http://arxiv.org/abs/2310.05374</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianqiao Lu, Wenyong Huang, Nianzu Zheng, Xingshan Zeng, Yu Ting Yeung, Xiao Chen<br>for: 这个论文主要是为了提高END-TO-END speech处理模型的性能，尤其是在数据中心时代的人工智能 era。methods: 这个论文提出了一种名为LaSyn的高效文本数据利用框架，用于增强END-TO-END speech处理模型的训练。LaSyn使用文本数据生成一种中间的幻数表示，然后将其与预训练的speech模型进行混合，以提高模型的性能。results: 在ASR任务上，LaSyn可以提高E2E基eline的表达误差率超过22.3%。在SLU任务上，LaSyn可以提高E2E基eline的意图分类精度和插槽填充精度。与已有的发布状态的工作相比，LaSyn的参数更少，并且得到了相当的性能提升。这些结果表明LaSyn生成的训练数据的质量。<details>
<summary>Abstract</summary>
Training a high performance end-to-end speech (E2E) processing model requires an enormous amount of labeled speech data, especially in the era of data-centric artificial intelligence. However, labeled speech data are usually scarcer and more expensive for collection, compared to textual data. We propose Latent Synthesis (LaSyn), an efficient textual data utilization framework for E2E speech processing models. We train a latent synthesizer to convert textual data into an intermediate latent representation of a pre-trained speech model. These pseudo acoustic representations of textual data augment acoustic data for model training. We evaluate LaSyn on low-resource automatic speech recognition (ASR) and spoken language understanding (SLU) tasks. For ASR, LaSyn improves an E2E baseline trained on LibriSpeech train-clean-100, with relative word error rate reductions over 22.3% on different test sets. For SLU, LaSyn improves our E2E baseline by absolute 4.1% for intent classification accuracy and 3.8% for slot filling SLU-F1 on SLURP, and absolute 4.49% and 2.25% for exact match (EM) and EM-Tree accuracies on STOP respectively. With fewer parameters, the results of LaSyn are competitive to published state-of-the-art works. The results demonstrate the quality of the augmented training data.
</details>
<details>
<summary>摘要</summary>
培训高性能端到端语音处理模型需要庞大量的标注语音数据，特别在人工智能时代。然而，标注语音数据通常比文本数据更 scarce 和更昂贵。我们提议Latent Synthesis（LaSyn），一种高效的文本数据利用框架 для端到端语音处理模型。我们在一个预训练的语音模型上训练一个干扰生成器，将文本数据转换为一种中间的干扰表示。这些干扰表示可以增强语音数据的训练。我们对LaSyn进行了评估，在不同的测试集上，LaSyn在自动语音识别（ASR）和语言理解（SLU）任务上提高了基eline的性能。在ASR任务上，LaSyn在LibriSpeech train-clean-100上训练的基eline上，相对减少了22.3%的单词错误率。在SLU任务上，LaSyn提高了我们的基eline的意向分类精度和插槽填充精度，相对增加了4.1%和3.8%。 LaSyn的参数数量 fewer ，与已发表的状态 Künstler 的性能相匹配。结果表明增强的训练数据质量。
</details></li>
</ul>
<hr>
<h2 id="A-Glance-is-Enough-Extract-Target-Sentence-By-Looking-at-A-keyword"><a href="#A-Glance-is-Enough-Extract-Target-Sentence-By-Looking-at-A-keyword" class="headerlink" title="A Glance is Enough: Extract Target Sentence By Looking at A keyword"></a>A Glance is Enough: Extract Target Sentence By Looking at A keyword</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05352">http://arxiv.org/abs/2310.05352</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ying Shi, Dong Wang, Lantian Li, Jiqing Han</li>
<li>for: 这个论文探讨了从多个说话人的多话语中提取目标句子，只需要输入一个关键词。例如，在社会保障应用中，关键词可能是“帮助”，目标是从其他说话人的干扰中提取某个人呼叫的句子。</li>
<li>methods: 我们提议使用Transformer架构将关键词和语音词汇 embedding，然后通过cross-attention机制选择正确的内容从拼接或重叠的语音中提取目标句子。</li>
<li>results: 在Librispeech数据集上，我们的提议方法可以很好地提取噪音和杂音声中的目标句子（SNR&#x3D;-3dB），PER为26%，比基eline系统的PER为96%。<details>
<summary>Abstract</summary>
This paper investigates the possibility of extracting a target sentence from multi-talker speech using only a keyword as input. For example, in social security applications, the keyword might be "help", and the goal is to identify what the person who called for help is articulating while ignoring other speakers. To address this problem, we propose using the Transformer architecture to embed both the keyword and the speech utterance and then rely on the cross-attention mechanism to select the correct content from the concatenated or overlapping speech. Experimental results on Librispeech demonstrate that our proposed method can effectively extract target sentences from very noisy and mixed speech (SNR=-3dB), achieving a phone error rate (PER) of 26\%, compared to the baseline system's PER of 96%.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Negative-Object-Presence-Evaluation-NOPE-to-Measure-Object-Hallucination-in-Vision-Language-Models"><a href="#Negative-Object-Presence-Evaluation-NOPE-to-Measure-Object-Hallucination-in-Vision-Language-Models" class="headerlink" title="Negative Object Presence Evaluation (NOPE) to Measure Object Hallucination in Vision-Language Models"></a>Negative Object Presence Evaluation (NOPE) to Measure Object Hallucination in Vision-Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05338">http://arxiv.org/abs/2310.05338</a></li>
<li>repo_url: None</li>
<li>paper_authors: Holy Lovenia, Wenliang Dai, Samuel Cahyawijaya, Ziwei Ji, Pascale Fung</li>
<li>for: 本研究旨在评估视言语（VL）模型中对物体幻化的影响，以提高模型的可靠性和可信度。</li>
<li>methods: 本研究使用了大量的自然语言模型生成29.5k个高质量的 sintetic negative pronoun（NegP）数据，以评估VL模型对物体幻化的敏感性。</li>
<li>results: 研究发现，无论是当前的State-of-the-art VL模型都不免受物体幻化的影响，其中所有模型在NegP问题中的准确率都低于10%。此外，研究还发现了lexically diverse visual questions、宽泛的问题类型和场景相关的物体，可能会使VL模型增加物体幻化的风险。<details>
<summary>Abstract</summary>
Object hallucination poses a significant challenge in vision-language (VL) models, often leading to the generation of nonsensical or unfaithful responses with non-existent objects. However, the absence of a general measurement for evaluating object hallucination in VL models has hindered our understanding and ability to mitigate this issue. In this work, we present NOPE (Negative Object Presence Evaluation), a novel benchmark designed to assess object hallucination in VL models through visual question answering (VQA). We propose a cost-effective and scalable approach utilizing large language models to generate 29.5k synthetic negative pronoun (NegP) data of high quality for NOPE. We extensively investigate the performance of 10 state-of-the-art VL models in discerning the non-existence of objects in visual questions, where the ground truth answers are denoted as NegP (e.g., "none"). Additionally, we evaluate their standard performance on visual questions on 9 other VQA datasets. Through our experiments, we demonstrate that no VL model is immune to the vulnerability of object hallucination, as all models achieve accuracy below 10\% on NegP. Furthermore, we uncover that lexically diverse visual questions, question types with large scopes, and scene-relevant objects capitalize the risk of object hallucination in VL models.
</details>
<details>
<summary>摘要</summary>
<SYS>对象幻像 pose 视语言（VL）模型中的挑战，经常导致生成无意义或不准确的回答，其中包括无存在的对象。然而，对视语言模型中对象幻像的评价没有一个通用的方法，这限制了我们对这个问题的理解和处理能力。在这种情况下，我们提出了 NOPE（负对象存在评价），一种新的benchmark，用于评价视语言模型中对象幻像的能力。我们提出了一种可靠且可扩展的方法，利用大型自然语言模型生成29.5k个高质量的负对象数据（NegP）。我们广泛研究了10种当前最佳的视语言模型在判断视Question中的对象不存在时的性能，其中ground truth answers denoted as NegP（例如，"none"）。此外，我们还评估了这些模型在9个其他VQA数据集上的标准性能。经过我们的实验，我们发现没有一个视语言模型是对象幻像的免疫者，所有模型在NegP上的准确率都低于10%。此外，我们发现了不同类型的视Question、广泛的问题类型和场景相关的对象都会增加视语言模型中对象幻像的风险。</SYS>Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="Resolving-the-Imbalance-Issue-in-Hierarchical-Disciplinary-Topic-Inference-via-LLM-based-Data-Augmentation"><a href="#Resolving-the-Imbalance-Issue-in-Hierarchical-Disciplinary-Topic-Inference-via-LLM-based-Data-Augmentation" class="headerlink" title="Resolving the Imbalance Issue in Hierarchical Disciplinary Topic Inference via LLM-based Data Augmentation"></a>Resolving the Imbalance Issue in Hierarchical Disciplinary Topic Inference via LLM-based Data Augmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05318">http://arxiv.org/abs/2310.05318</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xunxin Cai, Meng Xiao, Zhiyuan Ning, Yuanchun Zhou</li>
<li>for: This paper aims to address the issue of data imbalance in Natural Language Processing, specifically in the context of research proposals submitted for funding.</li>
<li>methods: The paper uses large language models (Llama V1) as data generators to augment research proposals categorized within intricate disciplinary hierarchies. The authors design prompts for keyword-based research proposal generation to rectify data imbalances and enhance the equity of expert assignments.</li>
<li>results: The experiments conducted in the paper demonstrate the efficacy of the generated data, showing that the research proposals produced using the prompts can effectively address the issue of data imbalance and generate high-quality scientific text data.<details>
<summary>Abstract</summary>
In addressing the imbalanced issue of data within the realm of Natural Language Processing, text data augmentation methods have emerged as pivotal solutions. This data imbalance is prevalent in the research proposals submitted during the funding application process. Such imbalances, resulting from the varying popularity of disciplines or the emergence of interdisciplinary studies, significantly impede the precision of downstream topic models that deduce the affiliated disciplines of these proposals. At the data level, proposals penned by experts and scientists are inherently complex technological texts, replete with intricate terminologies, which augmenting such specialized text data poses unique challenges. At the system level, this, in turn, compromises the fairness of AI-assisted reviewer assignment systems, which raises a spotlight on solving this issue. This study leverages large language models (Llama V1) as data generators to augment research proposals categorized within intricate disciplinary hierarchies, aiming to rectify data imbalances and enhance the equity of expert assignments. We first sample within the hierarchical structure to find the under-represented class. Then we designed a prompt for keyword-based research proposal generation. Our experiments attests to the efficacy of the generated data, demonstrating that research proposals produced using the prompts can effectively address the aforementioned issues and generate high quality scientific text data, thus help the model overcome the imbalanced issue.
</details>
<details>
<summary>摘要</summary>
在自然语言处理领域中解决数据不均衡问题，文本数据增强方法已成为关键解决方案。这种数据不均衡问题在研究提案申请过程中非常普遍，这些不均衡导致下游话题模型准确性受到影响。在数据层次，由专家和科学家写的提案是复杂的技术文本，充满专业术语，增强这种专业文本数据带来了独特的挑战。在系统层次，这会导致人工智能助手分配系统的公平性受到影响。本研究利用大型自然语言模型（Llama V1）作为数据生成器，增强分类在复杂的学科层次中的研究提案，以解决数据不均衡问题并提高专家分配的公平性。我们首先在层次结构中采样到下降类，然后设计了关键词基于的研究提案生成提示。我们的实验证明了生成的数据的有效性，显示了使用提示生成的研究提案可以有效地解决上述问题，并生成高质量的科学文本数据，帮助模型超越数据不均衡问题。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/09/cs.CL_2023_10_09/" data-id="clp89doba00d7i7880pns65j2" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_10_09" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/09/cs.LG_2023_10_09/" class="article-date">
  <time datetime="2023-10-09T10:00:00.000Z" itemprop="datePublished">2023-10-09</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/09/cs.LG_2023_10_09/">cs.LG - 2023-10-09</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Fair-Classifiers-that-Abstain-without-Harm"><a href="#Fair-Classifiers-that-Abstain-without-Harm" class="headerlink" title="Fair Classifiers that Abstain without Harm"></a>Fair Classifiers that Abstain without Harm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06205">http://arxiv.org/abs/2310.06205</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tongxin Yin, Jean-François Ton, Ruocheng Guo, Yuanshun Yao, Mingyan Liu, Yang Liu</li>
<li>For: The paper aims to develop a post-hoc method for existing classifiers to selectively abstain from predicting certain samples in order to achieve group fairness while maintaining original accuracy.* Methods: The proposed method uses integer programming to assign abstention decisions for each training sample and trains a surrogate model to generalize the abstaining decisions to test samples.* Results: The paper shows that the proposed method outperforms existing methods in terms of fairness disparity without sacrificing accuracy at similar abstention rates, and provides theoretical results on the feasibility of the IP procedure and the required abstention rate for different levels of unfairness tolerance and accuracy constraint.<details>
<summary>Abstract</summary>
In critical applications, it is vital for classifiers to defer decision-making to humans. We propose a post-hoc method that makes existing classifiers selectively abstain from predicting certain samples. Our abstaining classifier is incentivized to maintain the original accuracy for each sub-population (i.e. no harm) while achieving a set of group fairness definitions to a user specified degree. To this end, we design an Integer Programming (IP) procedure that assigns abstention decisions for each training sample to satisfy a set of constraints. To generalize the abstaining decisions to test samples, we then train a surrogate model to learn the abstaining decisions based on the IP solutions in an end-to-end manner. We analyze the feasibility of the IP procedure to determine the possible abstention rate for different levels of unfairness tolerance and accuracy constraint for achieving no harm. To the best of our knowledge, this work is the first to identify the theoretical relationships between the constraint parameters and the required abstention rate. Our theoretical results are important since a high abstention rate is often infeasible in practice due to a lack of human resources. Our framework outperforms existing methods in terms of fairness disparity without sacrificing accuracy at similar abstention rates.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:在关键应用中，分类器需要延迟决策，以便启用人类决策。我们提出了一种后期方法，使得现有的分类器可以选择性弃权处理certain sample。我们的弃权分类器被激励保持每个子 популяции的原始精度（即不害），同时实现一组集体公正定义到用户指定的程度。为此，我们设计了一个整数程序（IP）过程，将每个训练样本的弃权决策分配给满足一系列约束。为推广弃权决策到测试样本，我们然后训练了一个代理模型，以learn弃权决策基于IP解决方案的末端方式。我们分析了IP过程的可行性，以确定不同的不公正忍容度和精度约束下的可能的弃权率。我们的理论结果非常重要，因为高弃权率在实践中通常是不可能的，由于人工资源的缺乏。我们的框架在保持公正差距方面比现有方法更高，而不是牺牲精度。
</details></li>
</ul>
<hr>
<h2 id="PAC-Bayesian-Spectrally-Normalized-Bounds-for-Adversarially-Robust-Generalization"><a href="#PAC-Bayesian-Spectrally-Normalized-Bounds-for-Adversarially-Robust-Generalization" class="headerlink" title="PAC-Bayesian Spectrally-Normalized Bounds for Adversarially Robust Generalization"></a>PAC-Bayesian Spectrally-Normalized Bounds for Adversarially Robust Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06182">http://arxiv.org/abs/2310.06182</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiancong Xiao, Ruoyu Sun, Zhi- Quan Luo</li>
<li>for: This paper focuses on establishing theoretical guarantees for the robust generalization of deep neural networks (DNNs) against adversarial attacks.</li>
<li>methods: The paper uses a PAC-Bayes approach (Neyshabur et al., 2017) and provides a spectrally-normalized robust generalization bound for DNNs, addressing the challenge of extending the key ingredient to robust settings without relying on additional strong assumptions.</li>
<li>results: The paper shows that the mismatch terms between standard and robust generalization bounds are solely due to mathematical issues, and provides a different perspective on understanding robust generalization. Additionally, the paper extends the main result to adversarial robustness against general non-$\ell_p$ attacks and other neural network architectures.<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) are vulnerable to adversarial attacks. It is found empirically that adversarially robust generalization is crucial in establishing defense algorithms against adversarial attacks. Therefore, it is interesting to study the theoretical guarantee of robust generalization. This paper focuses on norm-based complexity, based on a PAC-Bayes approach (Neyshabur et al., 2017). The main challenge lies in extending the key ingredient, which is a weight perturbation bound in standard settings, to the robust settings. Existing attempts heavily rely on additional strong assumptions, leading to loose bounds. In this paper, we address this issue and provide a spectrally-normalized robust generalization bound for DNNs. Compared to existing bounds, our bound offers two significant advantages: Firstly, it does not depend on additional assumptions. Secondly, it is considerably tighter, aligning with the bounds of standard generalization. Therefore, our result provides a different perspective on understanding robust generalization: The mismatch terms between standard and robust generalization bounds shown in previous studies do not contribute to the poor robust generalization. Instead, these disparities solely due to mathematical issues. Finally, we extend the main result to adversarial robustness against general non-$\ell_p$ attacks and other neural network architectures.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:深度神经网络（DNN）易受到敌意攻击。实证表明，针对攻击的鲁棒化是建立防御算法的关键。因此，研究鲁棒化的理论保证很有趣。这篇论文关注 norm-based 复杂性，基于 PAC-Bayes 方法（Neyshabur et al., 2017）。主要挑战在扩展关键成分，即标准设置中的 weight 偏移 bound，到鲁棒设置中。现有尝试都需要额外假设，导致约束较松。在这篇论文中，我们解决这个问题，并提供一个spectrally-normalized 鲁棒化维度 bound for DNNs。与现有 bound 相比，我们的 bound 具有两个优势：首先，不需要额外假设。第二，较紧，与标准化 generalization bound 相符。因此，我们的结果提供了一种不同的理解鲁棒化的视角：在前一些研究中显示的鲁棒化与标准化 generalization bound 之间的差异不是由于 poor 鲁棒化，而是由于数学问题。最后，我们扩展主要结果到面向普通非 $\ell_p$ 攻击和其他神经网络架构。
</details></li>
</ul>
<hr>
<h2 id="Automatic-Integration-for-Spatiotemporal-Neural-Point-Processes"><a href="#Automatic-Integration-for-Spatiotemporal-Neural-Point-Processes" class="headerlink" title="Automatic Integration for Spatiotemporal Neural Point Processes"></a>Automatic Integration for Spatiotemporal Neural Point Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06179">http://arxiv.org/abs/2310.06179</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihao Zhou, Rose Yu</li>
<li>for: 这篇论文主要针对的是如何有效地捕捉和分析continuous-time点处理，尤其是在空间和时间上的点处理（STPPs）。</li>
<li>methods: 这篇论文提出了一种新的AutoSTPP（自动 интеграл для空间时间 нейрон点处理）方法，它是基于AutoInt（自动 интеграл）方法的扩展，可以有效地处理3D STPP。</li>
<li>results: 研究人员通过synthetic数据和实际世界数据 validate了AutoSTPP方法，并证明了其在复杂的intensity函数恢复方面的优异性。<details>
<summary>Abstract</summary>
Learning continuous-time point processes is essential to many discrete event forecasting tasks. However, integration poses a major challenge, particularly for spatiotemporal point processes (STPPs), as it involves calculating the likelihood through triple integrals over space and time. Existing methods for integrating STPP either assume a parametric form of the intensity function, which lacks flexibility; or approximating the intensity with Monte Carlo sampling, which introduces numerical errors. Recent work by Omi et al. [2019] proposes a dual network or AutoInt approach for efficient integration of flexible intensity function. However, the method only focuses on the 1D temporal point process. In this paper, we introduce a novel paradigm: AutoSTPP (Automatic Integration for Spatiotemporal Neural Point Processes) that extends the AutoInt approach to 3D STPP. We show that direct extension of the previous work overly constrains the intensity function, leading to poor performance. We prove consistency of AutoSTPP and validate it on synthetic data and benchmark real world datasets, showcasing its significant advantage in recovering complex intensity functions from irregular spatiotemporal events, particularly when the intensity is sharply localized.
</details>
<details>
<summary>摘要</summary>
Recent work by Omi et al. (2019) proposes a dual network or AutoInt approach for efficient integration of flexible intensity functions. However, this method only focuses on 1D temporal point processes. In this paper, we introduce a novel paradigm called AutoSTPP (Automatic Integration for Spatiotemporal Neural Point Processes) that extends the AutoInt approach to 3D STPP. We show that direct extension of the previous work overly constrains the intensity function, leading to poor performance.We prove the consistency of AutoSTPP and validate it on synthetic data and benchmark real-world datasets. Our results show that AutoSTPP significantly outperforms existing methods in recovering complex intensity functions from irregular spatiotemporal events, particularly when the intensity is sharply localized.
</details></li>
</ul>
<hr>
<h2 id="DockGame-Cooperative-Games-for-Multimeric-Rigid-Protein-Docking"><a href="#DockGame-Cooperative-Games-for-Multimeric-Rigid-Protein-Docking" class="headerlink" title="DockGame: Cooperative Games for Multimeric Rigid Protein Docking"></a>DockGame: Cooperative Games for Multimeric Rigid Protein Docking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06177">http://arxiv.org/abs/2310.06177</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vsomnath/dockgame">https://github.com/vsomnath/dockgame</a></li>
<li>paper_authors: Vignesh Ram Somnath, Pier Giuseppe Sessa, Maria Rodriguez Martinez, Andreas Krause</li>
<li>for: 本文针对的是预测多蛋白质复合物的结构，即蛋白质 docking 问题。</li>
<li>methods: 本文提出了一种基于游戏理论的 docking 方法，视蛋白质 docking 为多个蛋白质之间的合作游戏，并通过同时更新梯度来计算稳定Equilibrium。此外，本文还提出了一种基于扩散生成模型的方法，通过学习扩散分布来采样真实潜在力的Gibbs分布。</li>
<li>results: 实验结果表明，对 DB5.5 数据集，DockGame 比传统的 docking 方法快得多，能够生成多个可能的结构，并且与现有的 binary docking 基准集成比较。<details>
<summary>Abstract</summary>
Protein interactions and assembly formation are fundamental to most biological processes. Predicting the assembly structure from constituent proteins -- referred to as the protein docking task -- is thus a crucial step in protein design applications. Most traditional and deep learning methods for docking have focused mainly on binary docking, following either a search-based, regression-based, or generative modeling paradigm. In this paper, we focus on the less-studied multimeric (i.e., two or more proteins) docking problem. We introduce DockGame, a novel game-theoretic framework for docking -- we view protein docking as a cooperative game between proteins, where the final assembly structure(s) constitute stable equilibria w.r.t. the underlying game potential. Since we do not have access to the true potential, we consider two approaches - i) learning a surrogate game potential guided by physics-based energy functions and computing equilibria by simultaneous gradient updates, and ii) sampling from the Gibbs distribution of the true potential by learning a diffusion generative model over the action spaces (rotations and translations) of all proteins. Empirically, on the Docking Benchmark 5.5 (DB5.5) dataset, DockGame has much faster runtimes than traditional docking methods, can generate multiple plausible assembly structures, and achieves comparable performance to existing binary docking baselines, despite solving the harder task of coordinating multiple protein chains.
</details>
<details>
<summary>摘要</summary>
生物过程中的蛋白质交互和组装是基本的。从组成蛋白质的蛋白质拟合结构 -- 称为蛋白质拟合任务 -- 是蛋白质设计应用中的关键步骤。大多数传统和深度学习方法都主要关注了 binary docking，包括搜索、回归和生成模型的思路。在这篇论文中，我们关注了较少研究的多蛋白质（即两个或更多蛋白质）拟合问题。我们引入了 DockGame，一个基于游戏理论的拟合框架 -- 我们视蛋白质拟合为蛋白质之间的合作游戏，其最终结构为蛋白质之间的稳定平衡点。由于我们没有访问真实的潜在力，我们考虑了两种方法：一是学习带有物理基础能函数的代理游戏可能性函数，并通过同时更新梯度来计算平衡点；二是通过学习动作空间（旋转和平移）中的托德曼分布来采样真实的潜在力。实验表明，在 DB5.5 数据集上，DockGame 的运行时间远比传统拟合方法快得多，可以生成多个可能的结构，并与现有的 binary docking 基线相当。
</details></li>
</ul>
<hr>
<h2 id="Mitigating-Simplicity-Bias-in-Deep-Learning-for-Improved-OOD-Generalization-and-Robustness"><a href="#Mitigating-Simplicity-Bias-in-Deep-Learning-for-Improved-OOD-Generalization-and-Robustness" class="headerlink" title="Mitigating Simplicity Bias in Deep Learning for Improved OOD Generalization and Robustness"></a>Mitigating Simplicity Bias in Deep Learning for Improved OOD Generalization and Robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06161">http://arxiv.org/abs/2310.06161</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/estija/cmid">https://github.com/estija/cmid</a></li>
<li>paper_authors: Bhavya Vasudeva, Kameron Shahabi, Vatsal Sharan</li>
<li>for:  addressing simplicity bias in neural networks and improving OOD generalization, subgroup robustness, and fairness</li>
<li>methods:  regularizing the conditional mutual information of a simple model to obtain a more diverse set of features for making predictions</li>
<li>results:  effective in various problem settings and real-world applications, leading to more diverse feature usage, enhanced OOD generalization, improved subgroup robustness, and fairness, with theoretical analyses of the effectiveness and OOD generalization properties.Here’s the full Chinese text:</li>
<li>for:  Addressing simplicity bias in 神经网络（NNs），提高 OUT-OF-DISTRIBUTION（OOD）泛化、 subgroup robustness 和 fairness</li>
<li>methods:  Regularizing the conditional mutual information of a simple model to obtain a more diverse set of features for making predictions</li>
<li>results:  Effective in various problem settings and real-world applications, leading to more diverse feature usage, enhanced OOD generalization, improved subgroup robustness, and fairness, with theoretical analyses of the effectiveness and OOD generalization properties.<details>
<summary>Abstract</summary>
Neural networks (NNs) are known to exhibit simplicity bias where they tend to prefer learning 'simple' features over more 'complex' ones, even when the latter may be more informative. Simplicity bias can lead to the model making biased predictions which have poor out-of-distribution (OOD) generalization. To address this, we propose a framework that encourages the model to use a more diverse set of features to make predictions. We first train a simple model, and then regularize the conditional mutual information with respect to it to obtain the final model. We demonstrate the effectiveness of this framework in various problem settings and real-world applications, showing that it effectively addresses simplicity bias and leads to more features being used, enhances OOD generalization, and improves subgroup robustness and fairness. We complement these results with theoretical analyses of the effect of the regularization and its OOD generalization properties.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Provably-Accelerating-Ill-Conditioned-Low-rank-Estimation-via-Scaled-Gradient-Descent-Even-with-Overparameterization"><a href="#Provably-Accelerating-Ill-Conditioned-Low-rank-Estimation-via-Scaled-Gradient-Descent-Even-with-Overparameterization" class="headerlink" title="Provably Accelerating Ill-Conditioned Low-rank Estimation via Scaled Gradient Descent, Even with Overparameterization"></a>Provably Accelerating Ill-Conditioned Low-rank Estimation via Scaled Gradient Descent, Even with Overparameterization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06159">http://arxiv.org/abs/2310.06159</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cong Ma, Xingyu Xu, Tian Tong, Yuejie Chi</li>
<li>for: 估计低维对象（如矩阵和张量）从不完整、可能受损的线性测量中获得</li>
<li>methods: 使用简单迭代法如梯度下降（GD）来直接回归低维因子，具有小内存和计算脚印</li>
<li>results: ScaledGD算法可以线性 converge，不受低维对象的condition number影响，并且可以在各种任务中实现快速的全局收敛，包括感知、Robust PCA和完成任务。<details>
<summary>Abstract</summary>
Many problems encountered in science and engineering can be formulated as estimating a low-rank object (e.g., matrices and tensors) from incomplete, and possibly corrupted, linear measurements. Through the lens of matrix and tensor factorization, one of the most popular approaches is to employ simple iterative algorithms such as gradient descent (GD) to recover the low-rank factors directly, which allow for small memory and computation footprints. However, the convergence rate of GD depends linearly, and sometimes even quadratically, on the condition number of the low-rank object, and therefore, GD slows down painstakingly when the problem is ill-conditioned. This chapter introduces a new algorithmic approach, dubbed scaled gradient descent (ScaledGD), that provably converges linearly at a constant rate independent of the condition number of the low-rank object, while maintaining the low per-iteration cost of gradient descent for a variety of tasks including sensing, robust principal component analysis and completion. In addition, ScaledGD continues to admit fast global convergence to the minimax-optimal solution, again almost independent of the condition number, from a small random initialization when the rank is over-specified in the presence of Gaussian noise. In total, ScaledGD highlights the power of appropriate preconditioning in accelerating nonconvex statistical estimation, where the iteration-varying preconditioners promote desirable invariance properties of the trajectory with respect to the symmetry in low-rank factorization without hurting generalization.
</details>
<details>
<summary>摘要</summary>
许多科学和工程问题可以表示为估算一个低级对象（例如矩阵和张量）从不完整和可能受损的线性测量中。通过矩阵和张量分解的镜头，一种非常流行的方法是使用简单的迭代算法such as gradient descent (GD)来恢复低级因子，这些算法具有小内存和计算成本。然而，GD的收敛率与低级对象的condition number线性相关，当问题不梯化时，GD的收敛率会辐芳缓慢。这章节介绍了一种新的算法方法，称为scaled gradient descent (ScaledGD)，该方法可以在不同任务中，包括感知、稳定主成分分析和完成任务中，以Constant rate linearly converge，而不是linearly dependent on the condition number of the low-rank object。此外，ScaledGD还可以快速到达最优解，即minimax-optimal solution，从小Random initialization开始，当级数超出规定时，在存在 Gaussian noise 的情况下。总之，ScaledGD强调了适当的预conditioning在加速非 conjugate statistical estimation中的作用，iteration-varying preconditioners promote desirable invariance properties of the trajectory with respect to the symmetry in low-rank factorization without hurting generalization。
</details></li>
</ul>
<hr>
<h2 id="Manifold-augmented-Eikonal-Equations-Geodesic-Distances-and-Flows-on-Differentiable-Manifolds"><a href="#Manifold-augmented-Eikonal-Equations-Geodesic-Distances-and-Flows-on-Differentiable-Manifolds" class="headerlink" title="Manifold-augmented Eikonal Equations: Geodesic Distances and Flows on Differentiable Manifolds"></a>Manifold-augmented Eikonal Equations: Geodesic Distances and Flows on Differentiable Manifolds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06157">http://arxiv.org/abs/2310.06157</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Kelshaw, Luca Magri</li>
<li>for: 这项研究旨在提供一种基于模型的方法来 parameterize distance fields和 geodesic flows on manifolds，以便在 differentiable manifolds 上进行统计分析和减少维度模型。</li>
<li>methods: 该研究使用 manifold-augmented Eikonal equation 的解来 parameterize distance fields和 geodesic flows on manifolds。</li>
<li>results: 研究发现， manifold 的geometry对 distance field 产生了影响，而 geodesic flow 可以用来获取 globally length-minimizing curves。这些结果开启了 differentiable manifolds 上的统计分析和减少维度模型的可能性。<details>
<summary>Abstract</summary>
Manifolds discovered by machine learning models provide a compact representation of the underlying data. Geodesics on these manifolds define locally length-minimising curves and provide a notion of distance, which are key for reduced-order modelling, statistical inference, and interpolation. In this work, we propose a model-based parameterisation for distance fields and geodesic flows on manifolds, exploiting solutions of a manifold-augmented Eikonal equation. We demonstrate how the geometry of the manifold impacts the distance field, and exploit the geodesic flow to obtain globally length-minimising curves directly. This work opens opportunities for statistics and reduced-order modelling on differentiable manifolds.
</details>
<details>
<summary>摘要</summary>
人工智能模型发现的 manifold 提供了数据的紧凑表示。 manifold 上的 geodesic 定义了本地最短曲线，并提供了距离的概念，这些概念是reduced-order模型、统计推断和 interpolate 等方面的关键。在这项工作中，我们提议一种基于模型的 parameterization 方法 для distance field 和 geodesic flow  на manifold，利用 manifold-augmented Eikonal equation 的解。我们示出了 manifold 的几何特性对 distance field 的影响，并利用 geodesic flow 直接获取全球最短曲线。这项工作开启了 differentiable manifold 上的统计和减少模型的可能性。
</details></li>
</ul>
<hr>
<h2 id="Latent-Diffusion-Model-for-DNA-Sequence-Generation"><a href="#Latent-Diffusion-Model-for-DNA-Sequence-Generation" class="headerlink" title="Latent Diffusion Model for DNA Sequence Generation"></a>Latent Diffusion Model for DNA Sequence Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06150">http://arxiv.org/abs/2310.06150</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zehui Li, Yuhao Ni, Tim August B. Huygelen, Akashaditya Das, Guoxuan Xia, Guy-Bart Stan, Yiren Zhao</li>
<li>for: 本研究旨在提出一种基于扩散模型的精灵散列模型（DiscDiff），用于静止DNA序列生成。</li>
<li>methods: 本研究使用了一种卷积神经网络（autoencoder）将扩散模型的维度嵌入到维度空间中，以便利用连续扩散模型的强大生成能力来生成扩散数据。</li>
<li>results: 本研究的DiscDiff模型能够生成具有真实DNA序列的高一致性的合成DNA序列，包括约束分布、封闭空间分布（FReD）和染色体轨迹分布。此外，本研究还提供了15种物种150000个特有前体-基因序列数据，为未来的生成模型在遗传学中提供了更多的资源。<details>
<summary>Abstract</summary>
The harnessing of machine learning, especially deep generative models, has opened up promising avenues in the field of synthetic DNA sequence generation. Whilst Generative Adversarial Networks (GANs) have gained traction for this application, they often face issues such as limited sample diversity and mode collapse. On the other hand, Diffusion Models are a promising new class of generative models that are not burdened with these problems, enabling them to reach the state-of-the-art in domains such as image generation. In light of this, we propose a novel latent diffusion model, DiscDiff, tailored for discrete DNA sequence generation. By simply embedding discrete DNA sequences into a continuous latent space using an autoencoder, we are able to leverage the powerful generative abilities of continuous diffusion models for the generation of discrete data. Additionally, we introduce Fr\'echet Reconstruction Distance (FReD) as a new metric to measure the sample quality of DNA sequence generations. Our DiscDiff model demonstrates an ability to generate synthetic DNA sequences that align closely with real DNA in terms of Motif Distribution, Latent Embedding Distribution (FReD), and Chromatin Profiles. Additionally, we contribute a comprehensive cross-species dataset of 150K unique promoter-gene sequences from 15 species, enriching resources for future generative modelling in genomics. We will make our code public upon publication.
</details>
<details>
<summary>摘要</summary>
“机器学习的应用，特别是深度生成模型，在人造DNA序列生成领域中开启了有前途的可能性。虽然生成对抗网络（GANs）在这个应用中获得了进展，但它们经常面临有限的样本多样性和模式崩溃的问题。相比之下，传播模型是一种新的生成模型，没有这些问题，因此可以在领域中实现国际级的生成。在这背景下，我们提出了一个新的潜在传播模型，DiscDiff，专门适用于碎变DNA序列生成。通过将碎变DNA序列转换为连续的潜在空间中的对抗网络，我们可以利用传播模型的强大生成能力来生成碎变数据。此外，我们引入了Fréchet重建距离（FReD）作为评估生成DNA序列质量的新指标。DiscDiff模型在关于折衣分布、隐藏嵌入分布（FReD）和染色体质量上呈现高度的一致性。此外，我们提供了15种物种150000个唯一的激活器-蛋白质序列数据，增加了未来生成模型在遗传学方面的资源。我们将代码公开发布。”
</details></li>
</ul>
<hr>
<h2 id="On-the-Correlation-between-Random-Variables-and-their-Principal-Components"><a href="#On-the-Correlation-between-Random-Variables-and-their-Principal-Components" class="headerlink" title="On the Correlation between Random Variables and their Principal Components"></a>On the Correlation between Random Variables and their Principal Components</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06139">http://arxiv.org/abs/2310.06139</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zenon Gniazdowski</li>
<li>for: 本研究旨在找到Random Variables之间的相関系数，并使用线性代数方法来描述这些相关系数。</li>
<li>methods: 本研究使用了选取随机变数之间的统计量，然后使用 вектор和矩阵的概念来表述这些统计量的语言。这使得在后续步骤中可以 derivate预期的公式。</li>
<li>results: 研究发现，这个公式与因素分析中用来计算因素负载的公式相同。对于Principal Component Analysis中的主成分选择和因素分析中的因素数选择，这个公式也可以用来优化。<details>
<summary>Abstract</summary>
The article attempts to find an algebraic formula describing the correlation coefficients between random variables and the principal components representing them. As a result of the analysis, starting from selected statistics relating to individual random variables, the equivalents of these statistics relating to a set of random variables were presented in the language of linear algebra, using the concepts of vector and matrix. This made it possible, in subsequent steps, to derive the expected formula. The formula found is identical to the formula used in Factor Analysis to calculate factor loadings. The discussion showed that it is possible to apply this formula to optimize the number of principal components in Principal Component Analysis, as well as to optimize the number of factors in Factor Analysis.
</details>
<details>
<summary>摘要</summary>
文章尝试找到一个 алгебраическая方程描述Random Variables和它们的主成分之间的相关系数。经过分析，从选择的个体Random Variables的统计信息开始，使用线性代数概念 Vector和矩阵来表示这些统计信息的等价物。这使得在后续步骤中可以 derivate预期的方程。发现的方程与 фактор分析中计算因子负载的方程一样。文章还讨论了如何使用这个方程优化Principal Component Analysis中的主成分数量和Factor Analysis中的因子数量。Note: "Simplified Chinese" is a romanization of Chinese that uses a simplified set of characters and grammar rules to represent the language. It is commonly used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="Theoretical-Analysis-of-Robust-Overfitting-for-Wide-DNNs-An-NTK-Approach"><a href="#Theoretical-Analysis-of-Robust-Overfitting-for-Wide-DNNs-An-NTK-Approach" class="headerlink" title="Theoretical Analysis of Robust Overfitting for Wide DNNs: An NTK Approach"></a>Theoretical Analysis of Robust Overfitting for Wide DNNs: An NTK Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06112">http://arxiv.org/abs/2310.06112</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fshp971/adv-ntk">https://github.com/fshp971/adv-ntk</a></li>
<li>paper_authors: Shaopeng Fu, Di Wang</li>
<li>for: 这篇论文主要是为了解释深度神经网络（DNN）中的对抗训练（AT）方法在Robustness方面的缺点。</li>
<li>methods: 该论文使用了神经积簇kernel（NTK）理论来扩展AT方法，并证明了一个攻击者训练的宽度DNN可以被近似为一个线性化DNN。</li>
<li>results: 该论文通过实验表明，使用Adv-NTK算法可以帮助无穷宽度DNN增强相对的Robustness，并且该结果证明了论文中的理论结论。<details>
<summary>Abstract</summary>
Adversarial training (AT) is a canonical method for enhancing the robustness of deep neural networks (DNNs). However, recent studies empirically demonstrated that it suffers from robust overfitting, i.e., a long time AT can be detrimental to the robustness of DNNs. This paper presents a theoretical explanation of robust overfitting for DNNs. Specifically, we non-trivially extend the neural tangent kernel (NTK) theory to AT and prove that an adversarially trained wide DNN can be well approximated by a linearized DNN. Moreover, for squared loss, closed-form AT dynamics for the linearized DNN can be derived, which reveals a new AT degeneration phenomenon: a long-term AT will result in a wide DNN degenerates to that obtained without AT and thus cause robust overfitting. Based on our theoretical results, we further design a method namely Adv-NTK, the first AT algorithm for infinite-width DNNs. Experiments on real-world datasets show that Adv-NTK can help infinite-width DNNs enhance comparable robustness to that of their finite-width counterparts, which in turn justifies our theoretical findings. The code is available at https://github.com/fshp971/adv-ntk.
</details>
<details>
<summary>摘要</summary>
“对抗训练（AT）是深度神经网络（DNN）的一种标准方法，但是最近的研究表明，长期的AT可能对DNN的Robustness产生负面影响。这篇论文提供了DNN的Robust overfitting的理论解释。具体来说，我们将 neural tangent kernel（NTK）理论推广到AT，并证明了一个 adversarially trained wide DNN可以被linearized。此外，对于平方损失，我们可以 derivate closed-form AT dynamics for linearized DNN，这 revelas a new AT degeneration phenomenon：long-term AT will cause a wide DNN to degenerate into a DNN without AT, leading to robust overfitting。根据我们的理论结论，我们还设计了一种名为 Adv-NTK的AT算法，该算法可以帮助无限宽 DNN 提高相对的Robustness。实验表明，Adv-NTK可以帮助无限宽 DNN 提高与其有限宽 counterpart 的Robustness，这对我们的理论结论产生了正确的证明。代码可以在 https://github.com/fshp971/adv-ntk 中找到。”Note: The translation is in Simplified Chinese, which is one of the two standard versions of Chinese. The other version is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Grokking-as-the-Transition-from-Lazy-to-Rich-Training-Dynamics"><a href="#Grokking-as-the-Transition-from-Lazy-to-Rich-Training-Dynamics" class="headerlink" title="Grokking as the Transition from Lazy to Rich Training Dynamics"></a>Grokking as the Transition from Lazy to Rich Training Dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06110">http://arxiv.org/abs/2310.06110</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tanishq Kumar, Blake Bordelon, Samuel J. Gershman, Cengiz Pehlevan</li>
<li>for: 该论文探讨了 Grokking 现象，即 neural network 的训练损失降低得比测试损失早得多，可能是由于 neural network 从懒散训练方式转移到了丰富的特征学习 régime。</li>
<li>methods: 作者通过使用普通的梯度下降法和二层神经网络在一个多项式回归问题上进行研究，发现 Grokking 现象不可能由现有理论解释。作者还提出了测试损失的充分统计，并在训练过程中跟踪这些统计，从而发现 Grokking 现象 arise 在神经网络首先尝试使用初始特征来适应kernel regression解决方案，然后在训练损失已经下降到低水平时发现一个泛化解决方案。</li>
<li>results: 作者发现 Grokking 现象的关键因素包括神经网络输出的速率（可以由输出参数控制）和初始特征与目标函数 $y(x)$ 的对齐度。当神经网络在初始特征学习 régime 中训练时，它会首先尝试适应kernel regression解决方案，然后在训练损失已经下降到低水平时发现一个泛化解决方案。此外，作者还发现这种延迟泛化 arise 在 dataset 大 enough，但不是太大，以致可以使神经网络泛化，但不是太早。<details>
<summary>Abstract</summary>
We propose that the grokking phenomenon, where the train loss of a neural network decreases much earlier than its test loss, can arise due to a neural network transitioning from lazy training dynamics to a rich, feature learning regime. To illustrate this mechanism, we study the simple setting of vanilla gradient descent on a polynomial regression problem with a two layer neural network which exhibits grokking without regularization in a way that cannot be explained by existing theories. We identify sufficient statistics for the test loss of such a network, and tracking these over training reveals that grokking arises in this setting when the network first attempts to fit a kernel regression solution with its initial features, followed by late-time feature learning where a generalizing solution is identified after train loss is already low. We find that the key determinants of grokking are the rate of feature learning -- which can be controlled precisely by parameters that scale the network output -- and the alignment of the initial features with the target function $y(x)$. We argue this delayed generalization arises when (1) the top eigenvectors of the initial neural tangent kernel and the task labels $y(x)$ are misaligned, but (2) the dataset size is large enough so that it is possible for the network to generalize eventually, but not so large that train loss perfectly tracks test loss at all epochs, and (3) the network begins training in the lazy regime so does not learn features immediately. We conclude with evidence that this transition from lazy (linear model) to rich training (feature learning) can control grokking in more general settings, like on MNIST, one-layer Transformers, and student-teacher networks.
</details>
<details>
<summary>摘要</summary>
We found that the key determinants of grokking are the rate of feature learning, which can be controlled precisely by parameters that scale the network output, and the alignment of the initial features with the target function $y(x)$. We argue that delayed generalization arises when the top eigenvectors of the initial neural tangent kernel and the task labels $y(x)$ are misaligned, but the dataset size is large enough so that the network can generalize eventually, but not so large that the train loss perfectly tracks the test loss at all epochs. Additionally, the network begins training in the lazy regime, so it does not learn features immediately.We conclude that this transition from lazy (linear model) to rich training (feature learning) can control grokking in more general settings, such as on MNIST, one-layer Transformers, and student-teacher networks.
</details></li>
</ul>
<hr>
<h2 id="Quantifying-Uncertainty-in-Deep-Learning-Classification-with-Noise-in-Discrete-Inputs-for-Risk-Based-Decision-Making"><a href="#Quantifying-Uncertainty-in-Deep-Learning-Classification-with-Noise-in-Discrete-Inputs-for-Risk-Based-Decision-Making" class="headerlink" title="Quantifying Uncertainty in Deep Learning Classification with Noise in Discrete Inputs for Risk-Based Decision Making"></a>Quantifying Uncertainty in Deep Learning Classification with Noise in Discrete Inputs for Risk-Based Decision Making</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06105">http://arxiv.org/abs/2310.06105</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maryam Kheirandish, Shengfan Zhang, Donald G. Catanzaro, Valeriu Crudu</li>
<li>for: 这篇论文的目的是为了提供一个数据类型为数字和分类的问题上的深度神经网络模型中的预测不确定性评估方法。</li>
<li>methods: 这篇论文使用的方法是基于 Bayesian deep learning 的方法，具体是使用 Monte Carlo dropout 和我们的提议的框架来评估预测不确定性。</li>
<li>results: 这篇论文的结果显示，我们的提议的框架可以更好地识别预测中的错误 случарес，并且比 Monte Carlo dropout 方法更能捕捉错误的情况。<details>
<summary>Abstract</summary>
The use of Deep Neural Network (DNN) models in risk-based decision-making has attracted extensive attention with broad applications in medical, finance, manufacturing, and quality control. To mitigate prediction-related risks in decision making, prediction confidence or uncertainty should be assessed alongside the overall performance of algorithms. Recent studies on Bayesian deep learning helps quantify prediction uncertainty arises from input noises and model parameters. However, the normality assumption of input noise in these models limits their applicability to problems involving categorical and discrete feature variables in tabular datasets. In this paper, we propose a mathematical framework to quantify prediction uncertainty for DNN models. The prediction uncertainty arises from errors in predictors that follow some known finite discrete distribution. We then conducted a case study using the framework to predict treatment outcome for tuberculosis patients during their course of treatment. The results demonstrate under a certain level of risk, we can identify risk-sensitive cases, which are prone to be misclassified due to error in predictors. Comparing to the Monte Carlo dropout method, our proposed framework is more aware of misclassification cases. Our proposed framework for uncertainty quantification in deep learning can support risk-based decision making in applications when discrete errors in predictors are present.
</details>
<details>
<summary>摘要</summary>
使用深度神经网络（DNN）模型在风险基础的决策中吸引了广泛的关注，应用于医疗、金融、制造和质量控制等领域。为了减少决策过程中的预测风险，需要同时评估算法的总性表现和预测uncertainty。 latest studies on Bayesian deep learning 可以量化预测uncertainty，但这些模型假设输入噪声是Normal分布，这限制了它们在具有分类和离散特征变量的表格数据集中的应用。在这篇论文中，我们提出了一个数学框架，可以量化DNN模型中的预测uncertainty。预测uncertainty来自预测器中的错误，这些错误遵循一些已知的有限离散分布。我们Then conducted a case study using the framework to predict treatment outcome for tuberculosis patients during their course of treatment. The results show that under a certain level of risk, we can identify risk-sensitive cases, which are prone to be misclassified due to error in predictors. Comparing to the Monte Carlo dropout method, our proposed framework is more aware of misclassification cases. Our proposed framework for uncertainty quantification in deep learning can support risk-based decision making in applications when discrete errors in predictors are present.
</details></li>
</ul>
<hr>
<h2 id="Transformers-and-Large-Language-Models-for-Chemistry-and-Drug-Discovery"><a href="#Transformers-and-Large-Language-Models-for-Chemistry-and-Drug-Discovery" class="headerlink" title="Transformers and Large Language Models for Chemistry and Drug Discovery"></a>Transformers and Large Language Models for Chemistry and Drug Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06083">http://arxiv.org/abs/2310.06083</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andres M Bran, Philippe Schwaller</li>
<li>for: 这篇论文旨在探讨如何使用Transformer架构解决化学发现过程中的重要瓶颈问题，如retrosynthetic planning和化学空间探索。</li>
<li>methods: 这篇论文使用了Transformer架构，并将其应用于不同类型的数据，如线性化分子图、spectra、synthesis actions和人工语言。</li>
<li>results: 这篇论文描述了一种新的方法，可以通过自然语言的灵活性，解决化学问题。这种方法可以在不同的化学应用中使用，并且可以在将来的科学发现中扮演一个更重要的角色。<details>
<summary>Abstract</summary>
Language modeling has seen impressive progress over the last years, mainly prompted by the invention of the Transformer architecture, sparking a revolution in many fields of machine learning, with breakthroughs in chemistry and biology. In this chapter, we explore how analogies between chemical and natural language have inspired the use of Transformers to tackle important bottlenecks in the drug discovery process, such as retrosynthetic planning and chemical space exploration. The revolution started with models able to perform particular tasks with a single type of data, like linearised molecular graphs, which then evolved to include other types of data, like spectra from analytical instruments, synthesis actions, and human language. A new trend leverages recent developments in large language models, giving rise to a wave of models capable of solving generic tasks in chemistry, all facilitated by the flexibility of natural language. As we continue to explore and harness these capabilities, we can look forward to a future where machine learning plays an even more integral role in accelerating scientific discovery.
</details>
<details>
<summary>摘要</summary>
很多年来，语言模型在技术发展方面有了很大的进步，主要归功于Transformer架构的发明，这些架构的出现对机器学习多个领域产生了革命，包括化学和生物学。在这一章中，我们将探讨如何通过在化学和自然语言之间的相似性，使用Transformers来解决药物发现过程中的重要瓶颈，如逆 synthesis 规划和化学空间探索。这场革命从单一数据类型的模型开始，然后演进到包括其他数据类型，如分析器的spectra、合成操作和人类语言。现在，一新的趋势是利用大语言模型，让化学领域中的普遍任务得到解决，全部归功于自然语言的灵活性。我们继续探索和利用这些能力，未来machine learning在科学发现中的作用将变得更加重要。
</details></li>
</ul>
<hr>
<h2 id="Ito-Diffusion-Approximation-of-Universal-Ito-Chains-for-Sampling-Optimization-and-Boosting"><a href="#Ito-Diffusion-Approximation-of-Universal-Ito-Chains-for-Sampling-Optimization-and-Boosting" class="headerlink" title="Ito Diffusion Approximation of Universal Ito Chains for Sampling, Optimization and Boosting"></a>Ito Diffusion Approximation of Universal Ito Chains for Sampling, Optimization and Boosting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06081">http://arxiv.org/abs/2310.06081</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aleksei Ustimenko, Aleksandr Beznosikov</li>
<li>for: 本文研究一种广泛和通用的马可夫链，即以爱因斯坦-玛丽亚偏抽象方式描述的某种随机 diffequation 的谱。</li>
<li>methods: 本文使用了 almost arbitrary 的各向异常和状态依赖的噪声，而不是通常使用的Normal和状态独立的噪声。此外，我们的链的涨落和扩散系数可以是不准确的，以涵盖广泛的应用，如某种 Stochastic Gradient Langevin Dynamics、sampling、Stochastic Gradient Descent 或 Stochastic Gradient Boosting。</li>
<li>results: 我们证明了 $W_{2}$-距离 между含义链和对应的随机 diffequation 的法律之间的上界。这些结果超越或覆盖了大多数已知的估计。此外，对某些特定情况，我们的分析是第一次。<details>
<summary>Abstract</summary>
This work considers a rather general and broad class of Markov chains, Ito chains that look like Euler-Maryama discretization of some Stochastic Differential Equation. The chain we study is a unified framework for theoretical analysis. It comes with almost arbitrary isotropic and state-dependent noise instead of normal and state-independent one, as in most related papers. Moreover, our chain's drift and diffusion coefficient can be inexact to cover a wide range of applications such as Stochastic Gradient Langevin Dynamics, sampling, Stochastic Gradient Descent, or Stochastic Gradient Boosting. We prove an upper bound for $W_{2}$-distance between laws of the Ito chain and the corresponding Stochastic Differential Equation. These results improve or cover most of the known estimates. Moreover, for some particular cases, our analysis is the first.
</details>
<details>
<summary>摘要</summary>
Note: Simplified Chinese is also known as "简化字" or "简化字".Translation Notes:* "Markov chains" is translated as "Markov链" (Ma Ke Luo)* "Ito chains" is translated as "Itō链" (Ito Luo)* "Stochastic Differential Equation" is translated as "随机 diffe链方程" (Suī Jī Difu Luo Fang Jian)* "Wiener distance" is translated as "维纳度" (Wei Na Du)* "inexact" is translated as "不精确" (Bu Jing Ke)* "Stochastic Gradient Langevin Dynamics" is translated as "随机梯度兰格文运动" (Suī Jī Tiejian Langevin Yùndòng)* "sampling" is translated as "采样" (Cǎi Yàng)* "Stochastic Gradient Descent" is translated as "随机梯度下降" (Suī Jī Tiejian Xiào Jiàng)* "Stochastic Gradient Boosting" is translated as "随机梯度增强" (Suī Jī Tiejian Zēng Qiáng)
</details></li>
</ul>
<hr>
<h2 id="Optimal-Exploration-is-no-harder-than-Thompson-Sampling"><a href="#Optimal-Exploration-is-no-harder-than-Thompson-Sampling" class="headerlink" title="Optimal Exploration is no harder than Thompson Sampling"></a>Optimal Exploration is no harder than Thompson Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06069">http://arxiv.org/abs/2310.06069</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhaoqi Li, Kevin Jamieson, Lalit Jain</li>
<li>for: This paper aims to solve the pure exploration linear bandit problem with high probability through noisy measurements of $x^{\top}\theta_{\ast}$.</li>
<li>methods: The paper proposes an algorithm that leverages only sampling and argmax oracles and achieves an exponential convergence rate with the optimal exponent among all possible allocations asymptotically.</li>
<li>results: The algorithm proposed in the paper can be easily implemented and performs as well empirically as existing asymptotically optimal methods.Here is the same information in Simplified Chinese:</li>
<li>for: 本 paper 目标是解决纯exploration linear bandit问题，通过各种噪声测量 $x^{\top}\theta_{\ast}$ 来寻找最优解。</li>
<li>methods: 本 paper 提出了一种基于抽象和最大值 oracle 的算法，可以在高probability下实现快速收敛率，并且可以证明这种算法在所有分配中具有最优的幂率。</li>
<li>results: 本 paper 提出的算法可以轻松实现并与现有的 asymptotically 优化方法相当。<details>
<summary>Abstract</summary>
Given a set of arms $\mathcal{Z}\subset \mathbb{R}^d$ and an unknown parameter vector $\theta_\ast\in\mathbb{R}^d$, the pure exploration linear bandit problem aims to return $\arg\max_{z\in \mathcal{Z} z^{\top}\theta_{\ast}$, with high probability through noisy measurements of $x^{\top}\theta_{\ast}$ with $x\in \mathcal{X}\subset \mathbb{R}^d$. Existing (asymptotically) optimal methods require either a) potentially costly projections for each arm $z\in \mathcal{Z}$ or b) explicitly maintaining a subset of $\mathcal{Z}$ under consideration at each time. This complexity is at odds with the popular and simple Thompson Sampling algorithm for regret minimization, which just requires access to a posterior sampling and argmax oracle, and does not need to enumerate $\mathcal{Z}$ at any point. Unfortunately, Thompson sampling is known to be sub-optimal for pure exploration. In this work, we pose a natural question: is there an algorithm that can explore optimally and only needs the same computational primitives as Thompson Sampling? We answer the question in the affirmative. We provide an algorithm that leverages only sampling and argmax oracles and achieves an exponential convergence rate, with the exponent being the optimal among all possible allocations asymptotically. In addition, we show that our algorithm can be easily implemented and performs as well empirically as existing asymptotically optimal methods.
</details>
<details>
<summary>摘要</summary>
In this work, we pose a natural question: is there an algorithm that can explore optimally and only needs the same computational primitives as Thompson Sampling? We answer the question in the affirmative. We provide an algorithm that leverages only sampling and argmax oracles and achieves an exponential convergence rate, with the exponent being the optimal among all possible allocations asymptotically.Moreover, we show that our algorithm can be easily implemented and performs as well empirically as existing asymptotically optimal methods.
</details></li>
</ul>
<hr>
<h2 id="Early-Warning-via-tipping-preserving-latent-stochastic-dynamical-system-and-meta-label-correcting"><a href="#Early-Warning-via-tipping-preserving-latent-stochastic-dynamical-system-and-meta-label-correcting" class="headerlink" title="Early Warning via tipping-preserving latent stochastic dynamical system and meta label correcting"></a>Early Warning via tipping-preserving latent stochastic dynamical system and meta label correcting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06059">http://arxiv.org/abs/2310.06059</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peng Zhang, Ting Gao, Jin Guo, Jinqiao Duan</li>
<li>for: 预测 эпилепсию患者的症状，以提高 их安全性和健康状况。</li>
<li>methods: 基于患者的EEG数据，提出了一种基于meta学习框架的预测方法，利用了meta标签修正方法，并通过优化 latent Stochastic differential equation(SDE) 中的信息，选择最佳的 latent 动力系统。</li>
<li>results: 通过实验 validate 了我们的方法，发现预测精度有surprisingly的增加。<details>
<summary>Abstract</summary>
Early warning for epilepsy patients is crucial for their safety and well-being, in terms of preventing or minimizing the severity of seizures. Through the patients' EEG data, we propose a meta learning framework for improving prediction on early ictal signals. To better utilize the meta label corrector method, we fuse the information from both the real data and the augmented data from the latent Stochastic differential equation(SDE). Besides, we also optimally select the latent dynamical system via distribution of transition time between real data and that from the latent SDE. In this way, the extracted tipping dynamical feature is also integrated into the meta network to better label the noisy data. To validate our method, LSTM is implemented as the baseline model. We conduct a series of experiments to predict seizure in various long-term window from 1-2 seconds input data and find surprisingly increment of prediction accuracy.
</details>
<details>
<summary>摘要</summary>
早期警告对 эпилепси patients 的安全和健康至关重要，以预防或减轻癫痫症发作的严重程度。通过患者的 EEG 数据，我们提议一种meta学框架，以提高预测早期癫痫症信号的精度。为了更好地利用meta标签修复方法，我们将真实数据和潜在数据从射频 diferencial equation（SDE）的 latent 信息 fusion。此外，我们还优化了 latent 动力系统的选择，通过transition时间分布 между real data和 latent SDE 中的数据来实现。这样，提取的折冲动力特征也被 интегрирова到 meta 网络中，以更好地标注噪音数据。为验证我们的方法，LSTM 被实现为基eline模型。我们进行了一系列实验，用1-2秒输入数据预测癫痫症，并发现了奇异的增加预测精度。
</details></li>
</ul>
<hr>
<h2 id="Knowledge-Distillation-for-Anomaly-Detection"><a href="#Knowledge-Distillation-for-Anomaly-Detection" class="headerlink" title="Knowledge Distillation for Anomaly Detection"></a>Knowledge Distillation for Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06047">http://arxiv.org/abs/2310.06047</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/HibikiJie/Multiresolution-Knowledge-Distillation-for-Anomaly-Detection">https://github.com/HibikiJie/Multiresolution-Knowledge-Distillation-for-Anomaly-Detection</a></li>
<li>paper_authors: Adrian Alan Pol, Ekaterina Govorkova, Sonja Gronroos, Nadezda Chernyavskaya, Philip Harris, Maurizio Pierini, Isobel Ojalvo, Peter Elmer</li>
<li>for: 用于压缩无监督深度学习模型，以便在有限资源的设备上部署。</li>
<li>methods: 使用知识储存法压缩无监督异常检测模型，并提出一些改进检测敏感度的技巧。</li>
<li>results: 压缩模型与原始模型的性能相似，而减少大小和内存占用。<details>
<summary>Abstract</summary>
Unsupervised deep learning techniques are widely used to identify anomalous behaviour. The performance of such methods is a product of the amount of training data and the model size. However, the size is often a limiting factor for the deployment on resource-constrained devices. We present a novel procedure based on knowledge distillation for compressing an unsupervised anomaly detection model into a supervised deployable one and we suggest a set of techniques to improve the detection sensitivity. Compressed models perform comparably to their larger counterparts while significantly reducing the size and memory footprint.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。</SYS>>无监督深度学习技术广泛用于异常行为识别。这些方法的性能与训练数据量和模型大小相乘。然而，大小往往是部署在资源受限的设备上的限制因素。我们提出了一种基于知识储存的新方法，可以压缩无监督异常检测模型成可部署的超vised模型，并提出了一些提高检测敏感度的技巧。压缩模型与其更大的对手相比，性能相似，却减少了大小和内存占用。
</details></li>
</ul>
<hr>
<h2 id="Conformal-Decision-Theory-Safe-Autonomous-Decisions-from-Imperfect-Predictions"><a href="#Conformal-Decision-Theory-Safe-Autonomous-Decisions-from-Imperfect-Predictions" class="headerlink" title="Conformal Decision Theory: Safe Autonomous Decisions from Imperfect Predictions"></a>Conformal Decision Theory: Safe Autonomous Decisions from Imperfect Predictions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05921">http://arxiv.org/abs/2310.05921</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jordan Lekeufack, Anastasios N. Angelopoulos, Andrea Bajcsy, Michael I. Jordan, Jitendra Malik</li>
<li>for: 该论文旨在提供一种安全的自动决策框架，即使机器学习预测不准确。</li>
<li>methods: 该论文使用了准确预测理论，无需假设世界模型。</li>
<li>results: 实验表明，该方法在机器人运动规划、自动股票交易和机器人生产中具有实用性。<details>
<summary>Abstract</summary>
We introduce Conformal Decision Theory, a framework for producing safe autonomous decisions despite imperfect machine learning predictions. Examples of such decisions are ubiquitous, from robot planning algorithms that rely on pedestrian predictions, to calibrating autonomous manufacturing to exhibit high throughput and low error, to the choice of trusting a nominal policy versus switching to a safe backup policy at run-time. The decisions produced by our algorithms are safe in the sense that they come with provable statistical guarantees of having low risk without any assumptions on the world model whatsoever; the observations need not be I.I.D. and can even be adversarial. The theory extends results from conformal prediction to calibrate decisions directly, without requiring the construction of prediction sets. Experiments demonstrate the utility of our approach in robot motion planning around humans, automated stock trading, and robot manufacturing.
</details>
<details>
<summary>摘要</summary>
我们介绍了对准决策理论，一个框架用于生成安全的自动决策，即使机器学习预测不完美。这些决策的例子非常普遍，包括 robot 观察算法依赖人类预测，将自动生产调整为具有高速和低错误，以及在执行时是否信任主要政策或者转折到安全备用政策。我们的算法生成的决策是安全的，即具有可证的Statistical guarantee of low risk，不需要世界模型的任何假设，观察不必I.I.D.，甚至可以是反对的。我们的理论扩展了对准预测的结果，直接将决策calibrate，不需要建立预测集。实验展示了我们的方法在人类附近的机器人运动规划、自动股票交易和机器人生产中的 utility。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Decode-the-Surface-Code-with-a-Recurrent-Transformer-Based-Neural-Network"><a href="#Learning-to-Decode-the-Surface-Code-with-a-Recurrent-Transformer-Based-Neural-Network" class="headerlink" title="Learning to Decode the Surface Code with a Recurrent, Transformer-Based Neural Network"></a>Learning to Decode the Surface Code with a Recurrent, Transformer-Based Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05900">http://arxiv.org/abs/2310.05900</a></li>
<li>repo_url: None</li>
<li>paper_authors: Johannes Bausch, Andrew W Senior, Francisco J H Heras, Thomas Edlich, Alex Davies, Michael Newman, Cody Jones, Kevin Satzinger, Murphy Yuezhen Niu, Sam Blackwell, George Holland, Dvir Kafri, Juan Atalaya, Craig Gidney, Demis Hassabis, Sergio Boixo, Hartmut Neven, Pushmeet Kohli</li>
<li>for: 这个论文的目的是提高量子计算的可靠性，通过使用机器学习来解码量子错误 correction 代码。</li>
<li>methods: 这个论文使用了循环、变换器基本的神经网络，通过直接学习数据来解码表面码。</li>
<li>results: 论文的解码器在实际数据上（Google Sycamore 量子处理器）以及模拟数据上（包括干扰和误差）都有优异表现，可以覆盖距离3和5表面码，并且在训练时间25个循环后仍保持高准确率。<details>
<summary>Abstract</summary>
Quantum error-correction is a prerequisite for reliable quantum computation. Towards this goal, we present a recurrent, transformer-based neural network which learns to decode the surface code, the leading quantum error-correction code. Our decoder outperforms state-of-the-art algorithmic decoders on real-world data from Google's Sycamore quantum processor for distance 3 and 5 surface codes. On distances up to 11, the decoder maintains its advantage on simulated data with realistic noise including cross-talk, leakage, and analog readout signals, and sustains its accuracy far beyond the 25 cycles it was trained on. Our work illustrates the ability of machine learning to go beyond human-designed algorithms by learning from data directly, highlighting machine learning as a strong contender for decoding in quantum computers.
</details>
<details>
<summary>摘要</summary>
量子错误纠正是可靠量子计算的必要前提。为达到这个目标，我们提出了一种循环、转换器基于神经网络，可以学习解码表面码，这是量子错误纠正代码的领先代码。我们的解码器在Google的Sycamore量子处理器上的真实数据上表现出优于当前最佳算法解码器，在距离3和5表面码上出现了优异表现。在距离11上，我们的解码器在实际噪音，包括交叠、泄漏和分析读取信号的 simulate 数据上维持了其优势，并保持了其精度远远超出了它被训练的25次。我们的工作表明了机器学习可以超越人类设计的算法，通过直接学习数据，机器学习成为量子计算中的强有力竞争者。
</details></li>
</ul>
<hr>
<h2 id="A-Generalization-Bound-of-Deep-Neural-Networks-for-Dependent-Data"><a href="#A-Generalization-Bound-of-Deep-Neural-Networks-for-Dependent-Data" class="headerlink" title="A Generalization Bound of Deep Neural Networks for Dependent Data"></a>A Generalization Bound of Deep Neural Networks for Dependent Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05892">http://arxiv.org/abs/2310.05892</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/umd-huang-lab/neural-net-generalization-via-tensor">https://github.com/umd-huang-lab/neural-net-generalization-via-tensor</a></li>
<li>paper_authors: Quan Huu Do, Binh T. Nguyen, Lam Si Tung Ho</li>
<li>for: 这个研究是为了提供非站勤$\phi$-混合数据上的对抗学习网络对应。</li>
<li>methods: 本研究使用了对抗学习网络，并提出了一个新的一致性矩阵bound。</li>
<li>results: 研究发现，这个一致性矩阵bound可以对非站勤$\phi$-混合数据进行预测，并且比旧有的 bound 更为精确。<details>
<summary>Abstract</summary>
Existing generalization bounds for deep neural networks require data to be independent and identically distributed (iid). This assumption may not hold in real-life applications such as evolutionary biology, infectious disease epidemiology, and stock price prediction. This work establishes a generalization bound of feed-forward neural networks for non-stationary $\phi$-mixing data.
</details>
<details>
<summary>摘要</summary>
现有的总体化约束要求深度神经网络数据必须是独立并且相同分布（iid）。这个假设可能不成立在实际应用中，如生物进化、感染病毒流行病学和股票价格预测。这项工作建立了非站ARY $\phi$-混合数据的含积总体化约束。
</details></li>
</ul>
<hr>
<h2 id="A-Machine-Learning-Approach-to-Predicting-Single-Event-Upsets"><a href="#A-Machine-Learning-Approach-to-Predicting-Single-Event-Upsets" class="headerlink" title="A Machine Learning Approach to Predicting Single Event Upsets"></a>A Machine Learning Approach to Predicting Single Event Upsets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05878">http://arxiv.org/abs/2310.05878</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/architg1/CREMER">https://github.com/architg1/CREMER</a></li>
<li>paper_authors: Archit Gupta, Chong Yock Eng, Deon Lim Meng Wee, Rashna Analia Ahmed, See Min Sim</li>
<li>for: 预测单个事件异常 (SEU) 的发生，以提高半导体设备的可靠性。</li>
<li>methods: 使用机器学习技术，只使用位置数据预测 SEU 发生。</li>
<li>results: 提高半导体设备的可靠性，创造更安全的数字环境。<details>
<summary>Abstract</summary>
A single event upset (SEU) is a critical soft error that occurs in semiconductor devices on exposure to ionising particles from space environments. SEUs cause bit flips in the memory component of semiconductors. This creates a multitude of safety hazards as stored information becomes less reliable. Currently, SEUs are only detected several hours after their occurrence. CREMER, the model presented in this paper, predicts SEUs in advance using machine learning. CREMER uses only positional data to predict SEU occurrence, making it robust, inexpensive and scalable. Upon implementation, the improved reliability of memory devices will create a digitally safer environment onboard space vehicles.
</details>
<details>
<summary>摘要</summary>
一个单一事件冲击（SEU）是半导体设备中critical soft error的一种重要问题，它由宇宙射线粒子引起，导致内存组件中的比特跳变。这会导致存储的信息变得更加不可靠，带来多种安全风险。目前，SEU的发生只能在several hours后被探测出来。本文中提出的CREMER模型使用机器学习技术预测SEU发生，只使用位置数据，因此具有robust、便宜和可扩展的特点。在实施后，内存设备的可靠性会得到改善，从而在空间 vehicles上创造出一个更加数字安全的环境。Note: "宇宙射线粒子" in the text refers to ionising particles from space environments.
</details></li>
</ul>
<hr>
<h2 id="Bio-inspired-computational-memory-model-of-the-Hippocampus-an-approach-to-a-neuromorphic-spike-based-Content-Addressable-Memory"><a href="#Bio-inspired-computational-memory-model-of-the-Hippocampus-an-approach-to-a-neuromorphic-spike-based-Content-Addressable-Memory" class="headerlink" title="Bio-inspired computational memory model of the Hippocampus: an approach to a neuromorphic spike-based Content-Addressable Memory"></a>Bio-inspired computational memory model of the Hippocampus: an approach to a neuromorphic spike-based Content-Addressable Memory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05868">http://arxiv.org/abs/2310.05868</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Casanueva-Morato, Alvaro Ayuso-Martinez, Juan P. Dominguez-Morales, Angel Jimenez-Fernandez, Gabriel Jimenez-Moreno</li>
<li>for: 这篇论文目的是开发一种基于海马 CA3 区域的生物体现学习系统，能够学习、忘记和回忆非正式的记忆 fragment。</li>
<li>methods: 该模型使用脉冲神经网络（SNN）和SpiNNaker 硬件平台实现，并进行了功能、压力和实用性测试。</li>
<li>results: 该模型可以学习、忘记和回忆非正式的记忆 fragment，并且在不同的压力和环境下能够正常工作。这是首次实现了一个完全可工作的生物体现学习系统，将为未来的更复杂的neuromorphic系统开拓新的可能性。<details>
<summary>Abstract</summary>
The brain has computational capabilities that surpass those of modern systems, being able to solve complex problems efficiently in a simple way. Neuromorphic engineering aims to mimic biology in order to develop new systems capable of incorporating such capabilities. Bio-inspired learning systems continue to be a challenge that must be solved, and much work needs to be done in this regard. Among all brain regions, the hippocampus stands out as an autoassociative short-term memory with the capacity to learn and recall memories from any fragment of them. These characteristics make the hippocampus an ideal candidate for developing bio-inspired learning systems that, in addition, resemble content-addressable memories. Therefore, in this work we propose a bio-inspired spiking content-addressable memory model based on the CA3 region of the hippocampus with the ability to learn, forget and recall memories, both orthogonal and non-orthogonal, from any fragment of them. The model was implemented on the SpiNNaker hardware platform using Spiking Neural Networks. A set of experiments based on functional, stress and applicability tests were performed to demonstrate its correct functioning. This work presents the first hardware implementation of a fully-functional bio-inspired spiking hippocampal content-addressable memory model, paving the way for the development of future more complex neuromorphic systems.
</details>
<details>
<summary>摘要</summary>
脑有计算能力，超过现代系统，能够解决复杂问题，使用简单的方式。神经科工程尝试模仿生物，以开发新的系统，拥有这种能力。生物启发式学习系统仍然是一个挑战，需要进一步的研究。脑中的梨膜区（CA3）是一种自动相关短期记忆，具有学习和记忆任何段落的能力。这些特点使得梨膜区成为开发生物启发式学习系统的理想选择。因此，在这项工作中，我们提出了基于CA3区的生物启发式脉冲记忆模型，具有学习、忘记和记忆任何段落的能力。该模型在SpiNNaker硬件平台上使用脉冲神经网络进行实现。我们对模型进行了功能、压力和实用性测试，以证明其正常工作。这项工作展示了首次实现了完全可用的生物启发式脉冲梨膜区内存模型，开创了未来更复杂的神经omorphic系统的发展之路。
</details></li>
</ul>
<hr>
<h2 id="DSAC-T-Distributional-Soft-Actor-Critic-with-Three-Refinements"><a href="#DSAC-T-Distributional-Soft-Actor-Critic-with-Three-Refinements" class="headerlink" title="DSAC-T: Distributional Soft Actor-Critic with Three Refinements"></a>DSAC-T: Distributional Soft Actor-Critic with Three Refinements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05858">http://arxiv.org/abs/2310.05858</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jingliang-duan/dsac-t">https://github.com/jingliang-duan/dsac-t</a></li>
<li>paper_authors: Jingliang Duan, Wenxuan Wang, Liming Xiao, Jiaxin Gao, Shengbo Eben Li</li>
<li>for: 提高模型自适应RL方法的性能，解决常见的过估问题。</li>
<li>methods: 使用分布式软actor-critic算法（DSAC），并进行了三种改进：批处理梯度调整、双值分布学习和 variance-based target return clipping。</li>
<li>results: 在多种环境中，DSAC-T超过了多种主流模型自适应RL算法，包括SAC、TD3、DDPG、TRPO和PPO，而且保证了高稳定性的学习过程和不同奖励缩放下的相似性。<details>
<summary>Abstract</summary>
Reinforcement learning (RL) has proven to be highly effective in tackling complex decision-making and control tasks. However, prevalent model-free RL methods often face severe performance degradation due to the well-known overestimation issue. In response to this problem, we recently introduced an off-policy RL algorithm, called distributional soft actor-critic (DSAC or DSAC-v1), which can effectively improve the value estimation accuracy by learning a continuous Gaussian value distribution. Nonetheless, standard DSAC has its own shortcomings, including occasionally unstable learning processes and needs for task-specific reward scaling, which may hinder its overall performance and adaptability in some special tasks. This paper further introduces three important refinements to standard DSAC in order to address these shortcomings. These refinements consist of critic gradient adjusting, twin value distribution learning, and variance-based target return clipping. The modified RL algorithm is named as DSAC with three refinements (DSAC-T or DSAC-v2), and its performances are systematically evaluated on a diverse set of benchmark tasks. Without any task-specific hyperparameter tuning, DSAC-T surpasses a lot of mainstream model-free RL algorithms, including SAC, TD3, DDPG, TRPO, and PPO, in all tested environments. Additionally, DSAC-T, unlike its standard version, ensures a highly stable learning process and delivers similar performance across varying reward scales.
</details>
<details>
<summary>摘要</summary>
“强化学习（RL）已经证明可以很好地解决复杂的决策和控制任务。然而，广泛使用的无策法RL方法经常会遭遇估计问题，导致性能下降。为了解决这个问题，我们最近提出了一种偏离策略RL算法，称为分布型软actor-批评（DSAC或DSAC-v1），可以有效地提高价值估计准确性。然而，标准DSAC有一些缺点，包括 occasionally 不稳定的学习过程和需要任务特定的奖励滤波，这可能会限制其总体性能和适应性。这篇文章进一步介绍了三种重要的DSAC改进，包括评价函数梯度调整、双值分布学习和归一化目标返回截卷。改进后的RL算法被称为DSAC-T或DSAC-v2，其性能在多种环境中进行系统性评估。无需任务特定的超参数调整，DSAC-T比许多主流无策法RL算法，包括SAC、TD3、DDPG、TRPO和PPO，在所有测试环境中表现出色。此外，DSAC-T不同于标准版本，可以保证学习过程非常稳定，并在不同的奖励档次下提供相似的性能。”
</details></li>
</ul>
<hr>
<h2 id="Improved-Communication-Efficiency-in-Federated-Natural-Policy-Gradient-via-ADMM-based-Gradient-Updates"><a href="#Improved-Communication-Efficiency-in-Federated-Natural-Policy-Gradient-via-ADMM-based-Gradient-Updates" class="headerlink" title="Improved Communication Efficiency in Federated Natural Policy Gradient via ADMM-based Gradient Updates"></a>Improved Communication Efficiency in Federated Natural Policy Gradient via ADMM-based Gradient Updates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19807">http://arxiv.org/abs/2310.19807</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guangchen Lan, Han Wang, James Anderson, Christopher Brinton, Vaneet Aggarwal</li>
<li>for: 这 paper 旨在解决 Federated reinforcement learning (FedRL) 中高度通信开销的问题，尤其是在 natural policy gradient (NPG) 方法中，以提高training效率。</li>
<li>methods: 该 paper 提出了 FedNPG-ADMM 框架，通过 alternating direction method of multipliers (ADMM) 方法来近似全局 NPG 方向，从而提高了training efficiency。</li>
<li>results: 该 paper  theoretically 表明，使用 ADMM-based gradient updates 可以将 communication complexity 降低至 ${O}({d})$，其中 $d$ 是模型参数的数量。此外，paper 还证明了 FedNPG-ADMM 可以保持和标准 FedNPG 相同的 convergence rate。通过在 MuJoCo 环境中评估该 algorithm，paper 还证明了 FedNPG-ADMM 可以保持 reward performance，并且当Agent 数量增加时，其 convergence rate 会提高。<details>
<summary>Abstract</summary>
Federated reinforcement learning (FedRL) enables agents to collaboratively train a global policy without sharing their individual data. However, high communication overhead remains a critical bottleneck, particularly for natural policy gradient (NPG) methods, which are second-order. To address this issue, we propose the FedNPG-ADMM framework, which leverages the alternating direction method of multipliers (ADMM) to approximate global NPG directions efficiently. We theoretically demonstrate that using ADMM-based gradient updates reduces communication complexity from ${O}({d^{2})$ to ${O}({d})$ at each iteration, where $d$ is the number of model parameters. Furthermore, we show that achieving an $\epsilon$-error stationary convergence requires ${O}(\frac{1}{(1-\gamma)^{2}{\epsilon})$ iterations for discount factor $\gamma$, demonstrating that FedNPG-ADMM maintains the same convergence rate as the standard FedNPG. Through evaluation of the proposed algorithms in MuJoCo environments, we demonstrate that FedNPG-ADMM maintains the reward performance of standard FedNPG, and that its convergence rate improves when the number of federated agents increases.
</details>
<details>
<summary>摘要</summary>
federated reinforcement learning (FedRL) 允许代理共同训练全局策略，不需要分享个人数据。然而，交通开销仍然是critical bottleneck，特别是用natural policy gradient (NPG) 方法，这些方法是second-order。为解决这个问题，我们提出了FedNPG-ADMM框架，它利用了alternating direction method of multipliers (ADMM)来高效地计算全局NPG方向。我们 teorically 表明，使用 ADMM-based Gradient更新可以将交通复杂度从 $O(d^2)$ 降低到 $O(d)$ at each iteration，where $d$ 是模型参数的数量。此外，我们还证明了在 $\epsilon$-error stationary convergence 下，FedNPG-ADMM 需要 ${O}(\frac{1}{(1-\gamma)^{2}{\epsilon})$ 迭代，这与标准 FedNPG 的迭代速率相同。通过在 MuJoCo 环境中评估提议的算法，我们表明了FedNPG-ADMM 可以保持标准 FedNPG 的奖励性能，并且当多个联合代理增加时，其迭代速率会提高。
</details></li>
</ul>
<hr>
<h2 id="Robust-Angular-Synchronization-via-Directed-Graph-Neural-Networks"><a href="#Robust-Angular-Synchronization-via-Directed-Graph-Neural-Networks" class="headerlink" title="Robust Angular Synchronization via Directed Graph Neural Networks"></a>Robust Angular Synchronization via Directed Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05842">http://arxiv.org/abs/2310.05842</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yixuan He, Gesine Reinert, David Wipf, Mihai Cucuringu</li>
<li>for:  angular synchronization problem and its heterogeneous extension (sensor network localization, phase retrieval, and distributed clock synchronization)</li>
<li>methods:  directed graph neural networks and new loss functions</li>
<li>results:  competitive and often superior performance against a comprehensive set of baselines, validating the robustness of GNNSync even at high noise levels.<details>
<summary>Abstract</summary>
The angular synchronization problem aims to accurately estimate (up to a constant additive phase) a set of unknown angles $\theta_1, \dots, \theta_n\in[0, 2\pi)$ from $m$ noisy measurements of their offsets $\theta_i-\theta_j \;\mbox{mod} \; 2\pi.$ Applications include, for example, sensor network localization, phase retrieval, and distributed clock synchronization. An extension of the problem to the heterogeneous setting (dubbed $k$-synchronization) is to estimate $k$ groups of angles simultaneously, given noisy observations (with unknown group assignment) from each group. Existing methods for angular synchronization usually perform poorly in high-noise regimes, which are common in applications. In this paper, we leverage neural networks for the angular synchronization problem, and its heterogeneous extension, by proposing GNNSync, a theoretically-grounded end-to-end trainable framework using directed graph neural networks. In addition, new loss functions are devised to encode synchronization objectives. Experimental results on extensive data sets demonstrate that GNNSync attains competitive, and often superior, performance against a comprehensive set of baselines for the angular synchronization problem and its extension, validating the robustness of GNNSync even at high noise levels.
</details>
<details>
<summary>摘要</summary>
“angular synchronization problem”targets to accurately estimate（up to a constant additive phase）a set of unknown angles $\theta_1, \dots, \theta_n\in[0, 2\pi)$ from $m$ noisy measurements of their offsets $\theta_i-\theta_j \;\mbox{mod} \; 2\pi$. Applications include sensor network localization, phase retrieval, and distributed clock synchronization. An extension of the problem to the heterogeneous setting（dubbed $k$-synchronization）is to estimate $k$ groups of angles simultaneously, given noisy observations（with unknown group assignment）from each group. Existing methods for angular synchronization usually perform poorly in high-noise regimes, which are common in applications. In this paper, we leverage neural networks for the angular synchronization problem and its heterogeneous extension by proposing GNNSync, a theoretically-grounded end-to-end trainable framework using directed graph neural networks. In addition, new loss functions are devised to encode synchronization objectives. Experimental results on extensive data sets demonstrate that GNNSync attains competitive, and often superior, performance against a comprehensive set of baselines for the angular synchronization problem and its extension, validating the robustness of GNNSync even at high noise levels.
</details></li>
</ul>
<hr>
<h2 id="A-Bias-Variance-Covariance-Decomposition-of-Kernel-Scores-for-Generative-Models"><a href="#A-Bias-Variance-Covariance-Decomposition-of-Kernel-Scores-for-Generative-Models" class="headerlink" title="A Bias-Variance-Covariance Decomposition of Kernel Scores for Generative Models"></a>A Bias-Variance-Covariance Decomposition of Kernel Scores for Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05833">http://arxiv.org/abs/2310.05833</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sebastian G. Gruber, Florian Buettner</li>
<li>for: 这篇论文的目的是为了提供一种评估生成模型的泛化性和不确定性的理论框架。</li>
<li>methods: 该论文使用了对kernel scores的偏差-弹性-协方差分解，并提出了不偏的和一致的估计器，只需要生成的样本而不需要下游模型。</li>
<li>results: 该论文的应用是评估扩散模型的泛化评估和发现了少数群体的极化现象，以及验证了干扰和预测卷积 entropy 作为生成模型的不确定性度量。<details>
<summary>Abstract</summary>
Generative models, like large language models, are becoming increasingly relevant in our daily lives, yet a theoretical framework to assess their generalization behavior and uncertainty does not exist. Particularly, the problem of uncertainty estimation is commonly solved in an ad-hoc manner and task dependent. For example, natural language approaches cannot be transferred to image generation. In this paper we introduce the first bias-variance-covariance decomposition for kernel scores and their associated entropy. We propose unbiased and consistent estimators for each quantity which only require generated samples but not the underlying model itself. As an application, we offer a generalization evaluation of diffusion models and discover how mode collapse of minority groups is a contrary phenomenon to overfitting. Further, we demonstrate that variance and predictive kernel entropy are viable measures of uncertainty for image, audio, and language generation. Specifically, our approach for uncertainty estimation is more predictive of performance on CoQA and TriviaQA question answering datasets than existing baselines and can also be applied to closed-source models.
</details>
<details>
<summary>摘要</summary>
大量语言模型在我们日常生活中变得越来越重要，然而一个有效的理论框架来评估它们的泛化行为和不确定性并没有出现。特别是不确定性估计问题通常采用做出的方式和任务相关。例如，自然语言方法无法被转移到图像生成。在这篇论文中，我们介绍了首个偏差-变量- covariance 分解 для核分数和它们相关的熵。我们提议不偏和一致的估计器，只需要生成的样本而不需要下游模型本身。作为应用，我们对扩散模型进行总体评估，发现扩散的小组聚合是对权重过拟合的反应。此外，我们发现了变量和预测核熵是图像、音频和语言生成中的不确定性度量。 Specifically，我们的不确定性估计方法在CoQA和TriviaQA问答数据集上的性能预测比现有基elines高，并且可以应用于关闭源模型。
</details></li>
</ul>
<hr>
<h2 id="Pre-trained-Spatial-Priors-on-Multichannel-NMF-for-Music-Source-Separation"><a href="#Pre-trained-Spatial-Priors-on-Multichannel-NMF-for-Music-Source-Separation" class="headerlink" title="Pre-trained Spatial Priors on Multichannel NMF for Music Source Separation"></a>Pre-trained Spatial Priors on Multichannel NMF for Music Source Separation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05821">http://arxiv.org/abs/2310.05821</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pablo Cabanas-Molero, Antonio J. Munoz-Montoro, Julio Carabias-Orti, Pedro Vera-Candeas</li>
<li>for: 这个论文提出了一种基于录音设置信息的声音来源分离方法，可以应用于现有的室内乐录音设置。</li>
<li>methods: 该方法使用 solo 段来训练空间混合筛选器，以捕捉室内回声和扬声器响应的信息。然后将这个预训练过的筛选器integrated into a multichannel non-negative matrix factorization 方法，以更好地捕捉不同声音来源的方差。</li>
<li>results: 实验表明，该提出的框架可以更好地分离声音来源，比传统的 MNMF 方法提高性能。<details>
<summary>Abstract</summary>
This paper presents a novel approach to sound source separation that leverages spatial information obtained during the recording setup. Our method trains a spatial mixing filter using solo passages to capture information about the room impulse response and transducer response at each sensor location. This pre-trained filter is then integrated into a multichannel non-negative matrix factorization (MNMF) scheme to better capture the variances of different sound sources. The recording setup used in our experiments is the typical setup for orchestra recordings, with a main microphone and a close "cardioid" or "supercardioid" microphone for each section of the orchestra. This makes the proposed method applicable to many existing recordings. Experiments on polyphonic ensembles demonstrate the effectiveness of the proposed framework in separating individual sound sources, improving performance compared to conventional MNMF methods.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种新的声音源分离方法，利用录制过程中获得的空间信息。我们的方法使用独奏段来训练一个空间混合 filters，以捕捉室内响应和传播器响应在每个感知器位置上的信息。这个预训练过的滤波器然后被 интеGRATED INTO a multichannel non-negative matrix factorization (MNMF) 方案，以更好地捕捉不同声音源的方差。我们的实验使用了典型的乐团录制设置，即主 Mikrofon 和每个乐器部分的 "cardioid" 或 "supercardioid" Mikrofon。这使得我们的方法可以应用于许多现有的录音。实验表明，我们的框架可以更有效地分离声音源，与传统的 MNMF 方法相比，对多重演奏体示出了更好的性能。
</details></li>
</ul>
<hr>
<h2 id="Sharing-Information-Between-Machine-Tools-to-Improve-Surface-Finish-Forecasting"><a href="#Sharing-Information-Between-Machine-Tools-to-Improve-Surface-Finish-Forecasting" class="headerlink" title="Sharing Information Between Machine Tools to Improve Surface Finish Forecasting"></a>Sharing Information Between Machine Tools to Improve Surface Finish Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05807">http://arxiv.org/abs/2310.05807</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel R. Clarkson, Lawrence A. Bull, Tina A. Dardeno, Chandula T. Wickramarachchi, Elizabeth J. Cross, Timothy J. Rogers, Keith Worden, Nikolaos Dervilis, Aidan J. Hughes</li>
<li>for: 预测机器制造过程中表面质量</li>
<li>methods:  bayesian hierarchical model、bayesian linear regression</li>
<li>results: 提高预测精度和不确定性评估In Simplified Chinese text:</li>
<li>for: 用于预测机器制造过程中的表面质量</li>
<li>methods: 使用 bayesian hierarchical model 和 bayesian linear regression</li>
<li>results: 提高预测精度和不确定性评估<details>
<summary>Abstract</summary>
At present, most surface-quality prediction methods can only perform single-task prediction which results in under-utilised datasets, repetitive work and increased experimental costs. To counter this, the authors propose a Bayesian hierarchical model to predict surface-roughness measurements for a turning machining process. The hierarchical model is compared to multiple independent Bayesian linear regression models to showcase the benefits of partial pooling in a machining setting with respect to prediction accuracy and uncertainty quantification.
</details>
<details>
<summary>摘要</summary>
Note: Simplified Chinese is also known as "简化字" or "简化字".Here's the text in Simplified Chinese:当前，大多数表面质量预测方法只能进行单任务预测，这会导致数据被占用不足，重复工作和实验成本增加。为了解决这个问题，作者提议了一种折衣概率模型，用于预测转动加工过程中表面粗糙度测量值。这个模型与多个独立的折衣线性回归模型进行比较，以示出部分汇集在机床设备中的优点，包括预测精度和不确定性评估的提高。
</details></li>
</ul>
<hr>
<h2 id="Boosted-Control-Functions"><a href="#Boosted-Control-Functions" class="headerlink" title="Boosted Control Functions"></a>Boosted Control Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05805">http://arxiv.org/abs/2310.05805</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zszszszsz/.config">https://github.com/zszszszsz/.config</a></li>
<li>paper_authors: Nicola Gnecco, Jonas Peters, Sebastian Engelke, Niklas Pfister</li>
<li>for: 这篇研究旨在bridging the gap between existing prediction methods and the presence of hidden confounding, especially when the training and testing data are different.</li>
<li>methods: 本研究使用了distribution generalization from machine learning和simultaneous equation models and control function from econometrics，并提出了一新的同时方程模型（SIMDG）来描述资料生成过程下的分布差异。</li>
<li>results: 研究发现了一个强制条件（boosted control function，BCF），可以在不同的训练和测试数据下预测成功，并且提供了必要和充分的条件来识别BCF。<details>
<summary>Abstract</summary>
Modern machine learning methods and the availability of large-scale data opened the door to accurately predict target quantities from large sets of covariates. However, existing prediction methods can perform poorly when the training and testing data are different, especially in the presence of hidden confounding. While hidden confounding is well studied for causal effect estimation (e.g., instrumental variables), this is not the case for prediction tasks. This work aims to bridge this gap by addressing predictions under different training and testing distributions in the presence of unobserved confounding. In particular, we establish a novel connection between the field of distribution generalization from machine learning, and simultaneous equation models and control function from econometrics. Central to our contribution are simultaneous equation models for distribution generalization (SIMDGs) which describe the data-generating process under a set of distributional shifts. Within this framework, we propose a strong notion of invariance for a predictive model and compare it with existing (weaker) versions. Building on the control function approach from instrumental variable regression, we propose the boosted control function (BCF) as a target of inference and prove its ability to successfully predict even in intervened versions of the underlying SIMDG. We provide necessary and sufficient conditions for identifying the BCF and show that it is worst-case optimal. We introduce the ControlTwicing algorithm to estimate the BCF and analyze its predictive performance on simulated and real world data.
</details>
<details>
<summary>摘要</summary>
现代机器学习方法和大规模数据的可用性打开了预测目标量的准确预测的大门。然而，现有的预测方法在训练和测试数据不同时可能表现不佳，特别是在隐藏束缚的情况下。隐藏束缚在 causal effect estimation 中已经得到了广泛的研究（例如，用工具变量），但是这并不是预测任务的情况。本研究旨在bridging这个差距，通过面对不同训练和测试分布下的预测 task 中隐藏束缚的问题。特别是，我们建立了一种 novel connection  между机器学习中的分布泛化和 econometrics 中的同时方程模型和控制函数。我们的贡献包括在这种框架下提出的同时方程模型 для分布泛化（SIMDGs），这些模型描述了数据生成过程中的分布性变化。在这个框架下，我们提出了一种强版均衡性的目标函数，并与现有（弱版）目标函数进行比较。基于控制函数方法，我们提出了增强控制函数（BCF）作为预测目标，并证明其能够在对 SIMDG 进行 intervened 后仍能成功预测。我们还提供了 necessary and sufficient conditions  для Identifying BCF，并证明它是最差情况下的优化目标。 finally，我们介绍了 ControlTwicing 算法来估计 BCF，并分析了它在 simulated 和实际数据上的预测性能。
</details></li>
</ul>
<hr>
<h2 id="An-operator-preconditioning-perspective-on-training-in-physics-informed-machine-learning"><a href="#An-operator-preconditioning-perspective-on-training-in-physics-informed-machine-learning" class="headerlink" title="An operator preconditioning perspective on training in physics-informed machine learning"></a>An operator preconditioning perspective on training in physics-informed machine learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05801">http://arxiv.org/abs/2310.05801</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tim De Ryck, Florent Bonnet, Siddhartha Mishra, Emmanuel de Bézenac</li>
<li>for:  investigate the behavior of gradient descent algorithms in physics-informed machine learning methods like PINNs</li>
<li>methods:  employ both rigorous mathematical analysis and empirical evaluations to investigate various strategies for preconditioning a critical differential operator</li>
<li>results:  the difficulty in training these models is closely related to the conditioning of a specific differential operator, and preconditioning this operator is crucial for improving training<details>
<summary>Abstract</summary>
In this paper, we investigate the behavior of gradient descent algorithms in physics-informed machine learning methods like PINNs, which minimize residuals connected to partial differential equations (PDEs). Our key result is that the difficulty in training these models is closely related to the conditioning of a specific differential operator. This operator, in turn, is associated to the Hermitian square of the differential operator of the underlying PDE. If this operator is ill-conditioned, it results in slow or infeasible training. Therefore, preconditioning this operator is crucial. We employ both rigorous mathematical analysis and empirical evaluations to investigate various strategies, explaining how they better condition this critical operator, and consequently improve training.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了梯度下降算法在物理学知识Machine learning方法中的行为，如PINNs，它们用来最小化连接到部分偏微分方程（PDEs）的差异。我们的关键结果表明，训练这些模型的困难直接与一个特定的导数器的条件相关。这个导数器，则是PDE的导数器的 Hermitian平方的一个特殊情况。如果这个导数器是不良条件的，它会导致训练慢或不可能进行。因此，预conditioning这个关键导数器是关键。我们使用了严格的数学分析和实验评估来研究不同的策略，解释它们如何改善这个关键导数器的条件，并因此提高训练。
</details></li>
</ul>
<hr>
<h2 id="The-First-Cadenza-Signal-Processing-Challenge-Improving-Music-for-Those-With-a-Hearing-Loss"><a href="#The-First-Cadenza-Signal-Processing-Challenge-Improving-Music-for-Those-With-a-Hearing-Loss" class="headerlink" title="The First Cadenza Signal Processing Challenge: Improving Music for Those With a Hearing Loss"></a>The First Cadenza Signal Processing Challenge: Improving Music for Those With a Hearing Loss</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05799">http://arxiv.org/abs/2310.05799</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/claritychallenge/clarity/tree/main/recipes/cad1/task1">https://github.com/claritychallenge/clarity/tree/main/recipes/cad1/task1</a></li>
<li>paper_authors: Gerardo Roa Dabike, Scott Bannister, Jennifer Firth, Simone Graetzer, Rebecca Vos, Michael A. Akeroyd, Jon Barker, Trevor J. Cox, Bruno Fazenda, Alinka Greasley, William Whitmer</li>
<li>for: 提高音乐质量 для听力受损人群</li>
<li>methods: 使用信号处理挑战和个性化混音&#x2F;分离技术</li>
<li>results: 提高音乐质量，使用HAAQI指数对象评估和人类评审者对subjective评估<details>
<summary>Abstract</summary>
The Cadenza project aims to improve the audio quality of music for those who have a hearing loss. This is being done through a series of signal processing challenges, to foster better and more inclusive technologies. In the first round, two common listening scenarios are considered: listening to music over headphones, and with a hearing aid in a car. The first scenario is cast as a demixing-remixing problem, where the music is decomposed into vocals, bass, drums and other components. These can then be intelligently remixed in a personalized way, to increase the audio quality for a person who has a hearing loss. In the second scenario, music is coming from car loudspeakers, and the music has to be enhanced to overcome the masking effect of the car noise. This is done by taking into account the music, the hearing ability of the listener, the hearing aid and the speed of the car. The audio quality of the submissions will be evaluated using the Hearing Aid Audio Quality Index (HAAQI) for objective assessment and by a panel of people with hearing loss for subjective evaluation.
</details>
<details>
<summary>摘要</summary>
《干扰计划》旨在提高音乐质量，以帮助有听力问题的人。这是通过一系列的信号处理挑战，促进更加包容的科技。在首轮中，考虑了两个常见的听音情况：用耳机听音乐，以及在车里使用听力器。第一个情况是将音乐转化为男女声、低音、鼓等部分，然后以人性化的方式重新混合，以提高听损人的音乐质量。第二个情况是音乐来自车里的 loudspeakers，需要利用音乐、听力问题、听力器和车速来增强音乐，以扩除车声的遮蔽效应。音乐质量的评价使用《听力器音乐质量指数》（HAAQI）进行 объектив评估，并由有听力问题的人士进行主观评价。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Hybrid-Oversampling-and-Intelligent-Undersampling-for-Imbalanced-Big-Data-Classification"><a href="#Efficient-Hybrid-Oversampling-and-Intelligent-Undersampling-for-Imbalanced-Big-Data-Classification" class="headerlink" title="Efficient Hybrid Oversampling and Intelligent Undersampling for Imbalanced Big Data Classification"></a>Efficient Hybrid Oversampling and Intelligent Undersampling for Imbalanced Big Data Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05789">http://arxiv.org/abs/2310.05789</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carla Vairetti, José Luis Assadi, Sebastián Maldonado</li>
<li>for:  solves the issue of imbalanced classification in real-world applications</li>
<li>methods:  combines intelligent undersampling and oversampling using a MapReduce framework</li>
<li>results:  outperforms alternative resampling techniques for small- and medium-sized datasets, achieves positive results on large datasets with reduced running times.Here’s the full translation of the paper’s abstract in Simplified Chinese:</li>
<li>for:  solves the issue of imbalanced classification in real-world applications</li>
<li>methods:  combines intelligent undersampling and oversampling using a MapReduce framework</li>
<li>results:  outperforms alternative resampling techniques for small- and medium-sized datasets, achieves positive results on large datasets with reduced running times.I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Imbalanced classification is a well-known challenge faced by many real-world applications. This issue occurs when the distribution of the target variable is skewed, leading to a prediction bias toward the majority class. With the arrival of the Big Data era, there is a pressing need for efficient solutions to solve this problem. In this work, we present a novel resampling method called SMOTENN that combines intelligent undersampling and oversampling using a MapReduce framework. Both procedures are performed on the same pass over the data, conferring efficiency to the technique. The SMOTENN method is complemented with an efficient implementation of the neighborhoods related to the minority samples. Our experimental results show the virtues of this approach, outperforming alternative resampling techniques for small- and medium-sized datasets while achieving positive results on large datasets with reduced running times.
</details>
<details>
<summary>摘要</summary>
不均衡分类是现实世界中许多应用程序的挑战之一。这种问题出现在目标变量的分布偏斜时，导致预测偏向大多数类。在大数据时代 arrives，有一项压力需要解决这个问题。在这种工作中，我们提出了一种新的抽样方法called SMOTENN，它将智能下抽样和上抽样与 MapReduce 框架结合在一起。两种过程都在数据上进行了同一次读取，从而提高了方法的效率。 SMOTENN 方法还包括有效地实现少数类邻居的方法。我们的实验结果表明，这种方法在小到中型数据集上表现出色，而且在大数据集上具有减少运行时间的正面效果。
</details></li>
</ul>
<hr>
<h2 id="Why-Should-This-Article-Be-Deleted-Transparent-Stance-Detection-in-Multilingual-Wikipedia-Editor-Discussions"><a href="#Why-Should-This-Article-Be-Deleted-Transparent-Stance-Detection-in-Multilingual-Wikipedia-Editor-Discussions" class="headerlink" title="Why Should This Article Be Deleted? Transparent Stance Detection in Multilingual Wikipedia Editor Discussions"></a>Why Should This Article Be Deleted? Transparent Stance Detection in Multilingual Wikipedia Editor Discussions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05779">http://arxiv.org/abs/2310.05779</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/copenlu/wiki-stance">https://github.com/copenlu/wiki-stance</a></li>
<li>paper_authors: Lucie-Aimée Kaffee, Arnav Arora, Isabelle Augenstein</li>
<li>For: The paper aims to improve the transparency of content moderation on online platforms, specifically on Wikipedia, by constructing a novel multilingual dataset of editor discussions and their reasoning.* Methods: The paper uses a machine learning approach to predict the stance and reason (content moderation policy) of editors for each edit decision, adding transparency to the decision-making process.* Results: The paper demonstrates that stance and corresponding reason (policy) can be predicted jointly with a high degree of accuracy, providing a more transparent approach to content moderation.Here are the three key information points in Simplified Chinese text:* For: 论文目的是提高在线平台上的内容审核透明度，具体来说是在Wikipedia上进行编辑者讨论和决策的透明度。* Methods: 论文使用机器学习方法预测编辑者决策中的态度和理由（内容审核政策），以提高决策过程的透明度。* Results: 论文表明，态度和相应的理由（政策）可以通过高精度预测的方法相互关联，从而提供更透明的内容审核方法。<details>
<summary>Abstract</summary>
The moderation of content on online platforms is usually non-transparent. On Wikipedia, however, this discussion is carried out publicly and the editors are encouraged to use the content moderation policies as explanations for making moderation decisions. Currently, only a few comments explicitly mention those policies -- 20% of the English ones, but as few as 2% of the German and Turkish comments. To aid in this process of understanding how content is moderated, we construct a novel multilingual dataset of Wikipedia editor discussions along with their reasoning in three languages. The dataset contains the stances of the editors (keep, delete, merge, comment), along with the stated reason, and a content moderation policy, for each edit decision. We demonstrate that stance and corresponding reason (policy) can be predicted jointly with a high degree of accuracy, adding transparency to the decision-making process. We release both our joint prediction models and the multilingual content moderation dataset for further research on automated transparent content moderation.
</details>
<details>
<summary>摘要</summary>
在线平台内容Moderation通常是不透明的。然而，在Wikipedia上，这个讨论被公开进行，编辑被鼓励使用内容Moderation政策作为决策的解释。目前，只有英语评论中有20%是明确提到政策，德语和土耳其语评论中则只有2%。为了帮助理解内容Moderation的过程，我们构建了一个多语言数据集，包括Wikipedia编辑讨论、理由和内容Moderation政策。我们示示了editors的立场和相应的理由（政策）可以并行预测，增加了决策过程的透明度。我们发布了联合预测模型和多语言内容Moderation数据集，以便进一步研究自动透明内容Moderation。
</details></li>
</ul>
<hr>
<h2 id="Foundation-Models-Meet-Visualizations-Challenges-and-Opportunities"><a href="#Foundation-Models-Meet-Visualizations-Challenges-and-Opportunities" class="headerlink" title="Foundation Models Meet Visualizations: Challenges and Opportunities"></a>Foundation Models Meet Visualizations: Challenges and Opportunities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05771">http://arxiv.org/abs/2310.05771</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weikai Yang, Mengchen Liu, Zheng Wang, Shixia Liu</li>
<li>for: This paper explores the intersection of visualization techniques and foundation models like BERT and GPT, and how they can be used to improve transparency, explainability, fairness, and robustness in AI systems.</li>
<li>methods: The paper divides the intersections of visualization techniques and foundation models into two main areas: visualizations for foundation models (VIS4FM) and foundation models for visualizations (FM4VIS).</li>
<li>results: The paper highlights the challenges and opportunities that arise from the confluence of foundation models and visualizations, and provides a starting point for continued exploration in this promising avenue.Here is the same information in Simplified Chinese:</li>
<li>for: 这篇论文探讨了基础模型如BERT和GPT与视觉技术的交叉，以提高人工智能系统的透明度、解释性、公平性和稳定性。</li>
<li>methods: 论文将这些交叉分为两个主要领域：用于基础模型的视觉（VIS4FM）和基础模型用于视觉的发展（FM4VIS）。</li>
<li>results: 论文描述了这些交叉所带来的挑战和机遇，并提供了这个领域的开始点 для进一步的探索。<details>
<summary>Abstract</summary>
Recent studies have indicated that foundation models, such as BERT and GPT, excel in adapting to a variety of downstream tasks. This adaptability has established them as the dominant force in building artificial intelligence (AI) systems. As visualization techniques intersect with these models, a new research paradigm emerges. This paper divides these intersections into two main areas: visualizations for foundation models (VIS4FM) and foundation models for visualizations (FM4VIS). In VIS4FM, we explore the primary role of visualizations in understanding, refining, and evaluating these intricate models. This addresses the pressing need for transparency, explainability, fairness, and robustness. Conversely, within FM4VIS, we highlight how foundation models can be utilized to advance the visualization field itself. The confluence of foundation models and visualizations holds great promise, but it also comes with its own set of challenges. By highlighting these challenges and the growing opportunities, this paper seeks to provide a starting point for continued exploration in this promising avenue.
</details>
<details>
<summary>摘要</summary>
现代研究表明，基础模型，如BERT和GPT，在适应多种下游任务方面表现出色。这种适应能力使其成为人工智能系统建设的主导力量。在这些模型与视觉化技术交叉点的研究中，一个新的研究模式出现。这篇论文将这些交叉点分为两个主要领域：用于基础模型的视觉化（VIS4FM）和基础模型为视觉化的应用（FM4VIS）。在 VIS4FM 中，我们探索了视觉化在理解、修改和评估这些复杂模型的 primacy 角色。这种需求包括透明度、解释性、公平性和稳定性。相反，在 FM4VIS 中，我们强调了基础模型如何推动视觉化领域的进步。这两个领域的交叉点具有极大的推动力，但也存在一些挑战。通过强调这些挑战和快速发展的机遇，这篇论文希望为这一领域的进一步探索提供一个开始。
</details></li>
</ul>
<hr>
<h2 id="LCOT-Linear-circular-optimal-transport"><a href="#LCOT-Linear-circular-optimal-transport" class="headerlink" title="LCOT: Linear circular optimal transport"></a>LCOT: Linear circular optimal transport</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06002">http://arxiv.org/abs/2310.06002</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rocio Diaz Martin, Ivan Medri, Yikun Bai, Xinran Liu, Kangbai Yan, Gustavo K. Rohde, Soheil Kolouri</li>
<li>for: 这篇论文主要关注于圆形概率分布，并提出了一新的计算效率高的度量方法，即线性圆形最佳运输（LCOT）。</li>
<li>methods: 该论文引入了一个新的计算效率高的度量方法，即LCOT，并提供了一个可靠的线性映射，使得可以将机器学习（ML）算法应用到圆形概率分布上，并且让度量方法与ML算法之间的转换非常容易。</li>
<li>results: 论文通过一系列的数据实验示出了LCOT的效能，并显示了它在学习圆形概率分布的表现比较高于传统的圆形最佳运输（COT）度量方法。<details>
<summary>Abstract</summary>
The optimal transport problem for measures supported on non-Euclidean spaces has recently gained ample interest in diverse applications involving representation learning. In this paper, we focus on circular probability measures, i.e., probability measures supported on the unit circle, and introduce a new computationally efficient metric for these measures, denoted as Linear Circular Optimal Transport (LCOT). The proposed metric comes with an explicit linear embedding that allows one to apply Machine Learning (ML) algorithms to the embedded measures and seamlessly modify the underlying metric for the ML algorithm to LCOT. We show that the proposed metric is rooted in the Circular Optimal Transport (COT) and can be considered the linearization of the COT metric with respect to a fixed reference measure. We provide a theoretical analysis of the proposed metric and derive the computational complexities for pairwise comparison of circular probability measures. Lastly, through a set of numerical experiments, we demonstrate the benefits of LCOT in learning representations of circular measures.
</details>
<details>
<summary>摘要</summary>
最近几年，非欧几何空间上的最优运输问题已经吸引了多种应用，其中包括表示学习。在这篇论文中，我们将关注圆形概率度量，即圆周上的概率度量，并提出一种新的计算高效的度量，称为线性圆形最优运输（LCOT）。我们的度量包括一个显式的线性映射，使得可以通过对扩展到Machine Learning（ML）算法的度量进行应用，并且可以顺利地修改下面的度量为LCOT。我们证明了我们的度量基于圆形最优运输（COT）度量，并且可以视为对固定参照度量的线性化。我们提供了对度量的理论分析，并计算了对圆形概率度量的对比的计算复杂度。最后，通过一系列的数值实验，我们证明了LCOT在学习圆形度量的表示方面的好处。
</details></li>
</ul>
<hr>
<h2 id="Nonlinear-Correct-and-Smooth-for-Semi-Supervised-Learning"><a href="#Nonlinear-Correct-and-Smooth-for-Semi-Supervised-Learning" class="headerlink" title="Nonlinear Correct and Smooth for Semi-Supervised Learning"></a>Nonlinear Correct and Smooth for Semi-Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05757">http://arxiv.org/abs/2310.05757</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanhang Shao, Xiuwen Liu</li>
<li>for: 本研究针对Graph-based semi-supervised learning (GSSL) 进行了改进，以提高预测性能。</li>
<li>methods: 本研究使用了 Label Propagation (LP) 和 Graph Neural Networks (GNNs) 等方法，并将它们组合以提高表现。</li>
<li>results: 系统评估显示，本研究的方法可以在六个常用的数据集上取得了remarkable的平均提升率，较基本预测方法提升率高出13.71%，并且较现有的后处理方法提升率高出2.16%。<details>
<summary>Abstract</summary>
Graph-based semi-supervised learning (GSSL) has been used successfully in various applications. Existing methods leverage the graph structure and labeled samples for classification. Label Propagation (LP) and Graph Neural Networks (GNNs) both iteratively pass messages on graphs, where LP propagates node labels through edges and GNN aggregates node features from the neighborhood. Recently, combining LP and GNN has led to improved performance. However, utilizing labels and features jointly in higher-order graphs has not been explored. Therefore, we propose Nonlinear Correct and Smooth (NLCS), which improves the existing post-processing approach by incorporating non-linearity and higher-order representation into the residual propagation to handle intricate node relationships effectively. Systematic evaluations show that our method achieves remarkable average improvements of 13.71% over base prediction and 2.16% over the state-of-the-art post-processing method on six commonly used datasets. Comparisons and analyses show our method effectively utilizes labels and features jointly in higher-order graphs to resolve challenging graph relationships.
</details>
<details>
<summary>摘要</summary>
GRAPH-BASED SEMI-SUPERVISED LEARNING (GSSL) 已经成功应用于多个领域。现有方法利用图结构和标注样本进行分类。标签推广（LP）和图神经网络（GNNs）都是在图上进行迭代传递消息的方法，其中LP通过边传递节点标签，GNN从邻居聚合节点特征。最近，将LP和GNN结合使用已经导致了提高性能。然而，在更高阶图上同时利用标签和特征还没有被探索。因此，我们提出了非线性稳定（NLCS）方法，它通过在剩余传播中添加非线性和高阶表示来有效地处理图中复杂的节点关系。系统性评估显示，我们的方法在六个常用的数据集上取得了显著的平均提升率为13.71%，与基础预测相比，和state-of-the-art post-processing方法相比，分别提高了2.16%。比较和分析表明，我们的方法能够有效地在更高阶图上同时利用标签和特征进行分类。
</details></li>
</ul>
<hr>
<h2 id="Deep-Concept-Removal"><a href="#Deep-Concept-Removal" class="headerlink" title="Deep Concept Removal"></a>Deep Concept Removal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05755">http://arxiv.org/abs/2310.05755</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aman432/Spam-Classifier">https://github.com/aman432/Spam-Classifier</a></li>
<li>paper_authors: Yegor Klochkov, Jean-Francois Ton, Ruocheng Guo, Yang Liu, Hang Li</li>
<li>for: 本研究旨在深度神经网络中解决概念除去问题，以学习不含特定概念（如性别等）的表示。</li>
<li>methods: 我们提出了一种基于对概念集的对抗线性分类器的新方法，该方法可以帮助移除目标特征而不影响模型性能。我们在不同层次的网络中采用对抗探测类ifier，有效地解决了概念杂糜和OOD泛化问题。</li>
<li>results: 我们在一些流行的分布式 robust optimization（DRO）benchmark上进行了评估，以及OOD泛化任务。结果表明，我们的方法可以有效地除去概念，同时保持模型性能。<details>
<summary>Abstract</summary>
We address the problem of concept removal in deep neural networks, aiming to learn representations that do not encode certain specified concepts (e.g., gender etc.) We propose a novel method based on adversarial linear classifiers trained on a concept dataset, which helps to remove the targeted attribute while maintaining model performance. Our approach Deep Concept Removal incorporates adversarial probing classifiers at various layers of the network, effectively addressing concept entanglement and improving out-of-distribution generalization. We also introduce an implicit gradient-based technique to tackle the challenges associated with adversarial training using linear classifiers. We evaluate the ability to remove a concept on a set of popular distributionally robust optimization (DRO) benchmarks with spurious correlations, as well as out-of-distribution (OOD) generalization tasks.
</details>
<details>
<summary>摘要</summary>
我们关注深度神经网络中的概念除除问题，即学习不包含特定指定的概念（如性别等）的表示。我们提出了一种基于对概念集合的 adversarial 线性分类器的方法，可以帮助除除目标特征而保持模型性能。我们的方法深度概念除除包括对网络各层的 adversarial 探测器，有效地解决概念杂糜和外围泛化问题。此外，我们还介绍了一种基于偏导数的技术来解决对 adversarial 训练使用线性分类器的挑战。我们在一些流行的分布robust优化（DRO）benchmark上进行了评估，以及外围泛化任务。
</details></li>
</ul>
<hr>
<h2 id="Estimating-Shape-Distances-on-Neural-Representations-with-Limited-Samples"><a href="#Estimating-Shape-Distances-on-Neural-Representations-with-Limited-Samples" class="headerlink" title="Estimating Shape Distances on Neural Representations with Limited Samples"></a>Estimating Shape Distances on Neural Representations with Limited Samples</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05742">http://arxiv.org/abs/2310.05742</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dean A. Pospisil, Brett W. Larsen, Sarah E. Harvey, Alex H. Williams</li>
<li>for: 本研究旨在提供高维网络表示之间的几何相似性测量的一种有效方法，并且对这些方法进行了系统的分析和评估。</li>
<li>methods: 本研究使用了标准估计器，以及一种新的方法——方差调整的方法 OF moments  estimator，以确定高维网络表示之间的几何相似性。</li>
<li>results: 研究发现，标准估计器在高维特征空间中存在困难，而新引入的方法 OF moments  estimator 能够在实验和神经网络数据上达到更高的性能，特别是在高维设置下。<details>
<summary>Abstract</summary>
Measuring geometric similarity between high-dimensional network representations is a topic of longstanding interest to neuroscience and deep learning. Although many methods have been proposed, only a few works have rigorously analyzed their statistical efficiency or quantified estimator uncertainty in data-limited regimes. Here, we derive upper and lower bounds on the worst-case convergence of standard estimators of shape distance$\unicode{x2014}$a measure of representational dissimilarity proposed by Williams et al. (2021). These bounds reveal the challenging nature of the problem in high-dimensional feature spaces. To overcome these challenges, we introduce a new method-of-moments estimator with a tunable bias-variance tradeoff. We show that this estimator achieves superior performance to standard estimators in simulation and on neural data, particularly in high-dimensional settings. Thus, we lay the foundation for a rigorous statistical theory for high-dimensional shape analysis, and we contribute a new estimation method that is well-suited to practical scientific settings.
</details>
<details>
<summary>摘要</summary>
To overcome these challenges, we introduce a new method-of-moments estimator with a tunable bias-variance tradeoff. We show that this estimator achieves superior performance to standard estimators in simulation and on neural data, particularly in high-dimensional settings. Our findings lay the foundation for a rigorous statistical theory for high-dimensional shape analysis and contribute a new estimation method well-suited to practical scientific settings.
</details></li>
</ul>
<hr>
<h2 id="Post-hoc-Bias-Scoring-Is-Optimal-For-Fair-Classification"><a href="#Post-hoc-Bias-Scoring-Is-Optimal-For-Fair-Classification" class="headerlink" title="Post-hoc Bias Scoring Is Optimal For Fair Classification"></a>Post-hoc Bias Scoring Is Optimal For Fair Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05725">http://arxiv.org/abs/2310.05725</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenlong Chen, Yegor Klochkov, Yang Liu</li>
<li>for: 本文目标是研究一种基于分组公平约束的二分类问题的解决方案，包括人口均衡（DP）、平等机会（EOp）和平等投票机会（EO）等。</li>
<li>methods: 本文提出了一种基于 Bayes 优化的修改规则，通过对每个实例计算一个新的偏见指标（bias score），并将这些指标应用于修改规则来实现分组公平。修改规则可以是单个阈值或二元数组，具体取决于所使用的公平约束。</li>
<li>results: 本文通过使用三个数据集（Adult、COMPAS 和 CelebA）进行实验，显示了与内部处理和后处理方法相比，该方法可以实现高精度和分组公平。此外，该方法不需要在推断时访问敏感特征。<details>
<summary>Abstract</summary>
We consider a binary classification problem under group fairness constraints, which can be one of Demographic Parity (DP), Equalized Opportunity (EOp), or Equalized Odds (EO). We propose an explicit characterization of Bayes optimal classifier under the fairness constraints, which turns out to be a simple modification rule of the unconstrained classifier. Namely, we introduce a novel instance-level measure of bias, which we call bias score, and the modification rule is a simple linear rule on top of the finite amount of bias scores. Based on this characterization, we develop a post-hoc approach that allows us to adapt to fairness constraints while maintaining high accuracy. In the case of DP and EOp constraints, the modification rule is thresholding a single bias score, while in the case of EO constraints we are required to fit a linear modification rule with 2 parameters. The method can also be applied for composite group-fairness criteria, such as ones involving several sensitive attributes. We achieve competitive or better performance compared to both in-processing and post-processing methods across three datasets: Adult, COMPAS, and CelebA. Unlike most post-processing methods, we do not require access to sensitive attributes during the inference time.
</details>
<details>
<summary>摘要</summary>
我们考虑了一个二分类问题，其中需要满足一些群体公平约束，可能是人口比例（DP）、机会平等（EOp）或机会几率（EO）。我们提出了一个bayes最优分类器的显式化 caracterization，这 turns out to be a simple modification rule of the unconstrained classifier。我们引入了一个新的实例级别偏见度量，称为偏见得分，并且这个修改规则是一个单个偏见得分的阈值处理。基于这个characterization，我们开发了一种后处方法，可以在维护高准确率的同时适应公平约束。在DP和EOp约束下，修改规则是对偏见得分进行阈值处理，而在EO约束下，我们需要适应一个线性修改规则 WITH 2个参数。此方法还可以应用于复杂的群体公平标准，例如包括多个敏感特征。我们在三个 dataset（Adult、COMPAS和CelebA）上达到了竞争或更好的性能，与大多数后处方法不同的是，我们在推理时不需要访问敏感特征。
</details></li>
</ul>
<hr>
<h2 id="Planning-to-Go-Out-of-Distribution-in-Offline-to-Online-Reinforcement-Learning"><a href="#Planning-to-Go-Out-of-Distribution-in-Offline-to-Online-Reinforcement-Learning" class="headerlink" title="Planning to Go Out-of-Distribution in Offline-to-Online Reinforcement Learning"></a>Planning to Go Out-of-Distribution in Offline-to-Online Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05723">http://arxiv.org/abs/2310.05723</a></li>
<li>repo_url: None</li>
<li>paper_authors: Trevor McInroe, Stefano V. Albrecht, Amos Storkey</li>
<li>for: 在减少在线交互次数的情况下，找到最佳政策。</li>
<li>methods: 使用在线搜索和规划算法，以最大化在线数据收集的利益。</li>
<li>results: PTGOOD算法可以在减少在线交互次数的情况下，提高代理返回和找到最佳政策，并且可以避免许多基线算法在不同环境中的低效 converges。<details>
<summary>Abstract</summary>
Offline pretraining with a static dataset followed by online fine-tuning (offline-to-online, or OtO) is a paradigm that is well matched to a real-world RL deployment process: in few real settings would one deploy an offline policy with no test runs and tuning. In this scenario, we aim to find the best-performing policy within a limited budget of online interactions. Previous work in the OtO setting has focused on correcting for bias introduced by the policy-constraint mechanisms of offline RL algorithms. Such constraints keep the learned policy close to the behavior policy that collected the dataset, but this unnecessarily limits policy performance if the behavior policy is far from optimal. Instead, we forgo policy constraints and frame OtO RL as an exploration problem: we must maximize the benefit of the online data-collection. We study major online RL exploration paradigms, adapting them to work well with the OtO setting. These adapted methods contribute several strong baselines. Also, we introduce an algorithm for planning to go out of distribution (PTGOOD), which targets online exploration in relatively high-reward regions of the state-action space unlikely to be visited by the behavior policy. By leveraging concepts from the Conditional Entropy Bottleneck, PTGOOD encourages data collected online to provide new information relevant to improving the final deployment policy. In that way the limited interaction budget is used effectively. We show that PTGOOD significantly improves agent returns during online fine-tuning and finds the optimal policy in as few as 10k online steps in Walker and in as few as 50k in complex control tasks like Humanoid. Also, we find that PTGOOD avoids the suboptimal policy convergence that many of our baselines exhibit in several environments.
</details>
<details>
<summary>摘要</summary>
假设我们有一个偏向于实际世界的强化学习（RL）部署过程：在几个实际场景中，我们不会直接部署一个没有测试和调整的策略。在这种情况下，我们想找到最佳的策略，而且在有限的在线互动次数内完成。先前的工作在OtO设定中集中在修正在线RL算法中的偏见问题上。这些约束保持学习策略与数据收集过程中的行为策略相互关联，但这会无需lessly限制策略性能，如果行为策略远离优化。因此，我们不采用策略约束，而是视为探索问题，我们需要在在线数据收集中最大化收益。我们研究了在线RL探索方法，并将其适应到OtO设定中。这些适应方法提供了多个强大的基线。此外，我们介绍了一种计划去偏现（PTGOOD）算法，该算法target在高奖励区域的状态动作空间中进行在线探索。通过利用Conditional Entropy Bottleneck的概念，PTGOOD鼓励在线数据收集提供新的有用信息，以改进最终部署策略。因此，我们可以有效地使用有限的互动次数。我们显示，PTGOOD在在线细化中显著提高了代理返回，并在Walker和复杂控制任务中在10k和50k在线步骤内找到优化策略。此外，我们发现PTGOOD可以避免许多我们的基eline在多个环境中展现的差异性。
</details></li>
</ul>
<hr>
<h2 id="Transformer-Fusion-with-Optimal-Transport"><a href="#Transformer-Fusion-with-Optimal-Transport" class="headerlink" title="Transformer Fusion with Optimal Transport"></a>Transformer Fusion with Optimal Transport</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05719">http://arxiv.org/abs/2310.05719</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Yahnnosh/Exploring-Model-Fusion-with-Optimal-Transport-on-Transformers">https://github.com/Yahnnosh/Exploring-Model-Fusion-with-Optimal-Transport-on-Transformers</a></li>
<li>paper_authors: Moritz Imfeld, Jacopo Graldi, Marco Giordano, Thomas Hofmann, Sotiris Anagnostidis, Sidak Pal Singh</li>
<li>for: 这 paper 的目的是探讨 transformer 网络的合并技术，以提高模型的性能。</li>
<li>methods: 这 paper 使用 Optimal Transport 算法来软对接 transformer 网络的不同组件，以实现层Alignment。 authors 还提出了一种抽象层Alignment方法，可以普适应用于不同的架构。</li>
<li>results:  experiments 表明，这 paper 的方法可以提高 transformer 网络的性能，并且可以让模型具有更好的泛化能力。 authors 还发现了一些有趣的现象，例如软对接在 transformer 网络中的重要作用。<details>
<summary>Abstract</summary>
Fusion is a technique for merging multiple independently-trained neural networks in order to combine their capabilities. Past attempts have been restricted to the case of fully-connected, convolutional, and residual networks. In this paper, we present a systematic approach for fusing two or more transformer-based networks exploiting Optimal Transport to (soft-)align the various architectural components. We flesh out an abstraction for layer alignment, that can generalize to arbitrary architectures -- in principle -- and we apply this to the key ingredients of Transformers such as multi-head self-attention, layer-normalization, and residual connections, and we discuss how to handle them via various ablation studies. Furthermore, our method allows the fusion of models of different sizes (heterogeneous fusion), providing a new and efficient way for compression of Transformers. The proposed approach is evaluated on both image classification tasks via Vision Transformer and natural language modeling tasks using BERT. Our approach consistently outperforms vanilla fusion, and, after a surprisingly short finetuning, also outperforms the individual converged parent models. In our analysis, we uncover intriguing insights about the significant role of soft alignment in the case of Transformers. Our results showcase the potential of fusing multiple Transformers, thus compounding their expertise, in the budding paradigm of model fusion and recombination.
</details>
<details>
<summary>摘要</summary>
文本翻译为简化中文。merge multiple independently trained neural networks to combine their capabilities. Previous attempts were limited to fully connected, convolutional, and residual networks. In this paper, we present a systematic approach for fusing two or more transformer-based networks using optimal transport to align the various architectural components. We provide an abstraction for layer alignment that can generalize to any architecture and apply it to key ingredients of Transformers such as multi-head self-attention, layer normalization, and residual connections. We also discuss how to handle them through various ablation studies. Furthermore, our method allows the fusion of models of different sizes (heterogeneous fusion), providing a new and efficient way for compression of Transformers. The proposed approach is evaluated on both image classification tasks using Vision Transformer and natural language modeling tasks using BERT. Our approach consistently outperforms vanilla fusion and, after a surprisingly short finetuning, also outperforms the individual converged parent models. In our analysis, we uncover intriguing insights about the significant role of soft alignment in the case of Transformers. Our results showcase the potential of fusing multiple Transformers, thus compounding their expertise, in the emerging paradigm of model fusion and recombination.</SYS>以下是简化中文版本：融合多个独立训练的神经网络，以合并它们的能力。过去的尝试都是限制在几何网络、卷积网络和差分网络上。在这篇论文中，我们提出了一种系统的方法，使用最优运输来软对齐多个转换器基础网络的各种建筑 ком成分。我们提供了一个抽象层对齐概念，可以泛化到任何建筑，并应用于转换器的关键组成部分，如多头自我注意、层Normalization和差分连接。我们还讨论了如何通过不同的截止方法处理它们。此外，我们的方法允许模型的不同大小（不同大小的融合），提供了一种新的高效的压缩方法。我们的方法在图像分类任务上使用视Transformer和自然语言处理任务上使用BERT进行评估，我们的方法一致性超过了普通融合，并在短暂的训练后也超过了父模型的单独整合。在我们的分析中，我们发现了关于转换器中软对齐的各种惊喜的发现。我们的结果显示，可以将多个转换器融合在一起，从而汇集它们的专长，在模型融合和重新组合的新时代中发挥作用。
</details></li>
</ul>
<hr>
<h2 id="Imitator-Learning-Achieve-Out-of-the-Box-Imitation-Ability-in-Variable-Environments"><a href="#Imitator-Learning-Achieve-Out-of-the-Box-Imitation-Ability-in-Variable-Environments" class="headerlink" title="Imitator Learning: Achieve Out-of-the-Box Imitation Ability in Variable Environments"></a>Imitator Learning: Achieve Out-of-the-Box Imitation Ability in Variable Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05712">http://arxiv.org/abs/2310.05712</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiong-Hui Chen, Junyin Ye, Hang Zhao, Yi-Chen Li, Haoran Shi, Yu-Yan Xu, Zhihao Ye, Si-Hang Yang, Anqi Huang, Kai Xu, Zongzhang Zhang, Yang Yu</li>
<li>for: 本文旨在提出一种新的imitator learning（ItorL）方法，以便在很少示例的情况下，快速重建不同任务的imitator模块，并适应未预期的环境变化。</li>
<li>methods: 本文提出了一种基于一个专家示例的Demo-Attention Actor-Critic（DAAC）方法，将imitator学习纳入了一种强化学习框架，以Regularize策略的行为在意外情况下。此外，为了自主建立imitator策略，我们设计了一个示例基于注意力架构，可以有效地输出imiter动作，适应不同的状态。</li>
<li>results: 我们在一个新的导航benchmark和一个机器人环境中测试了DAAC方法，与之前的imitator方法相比，DAAC方法在 seen和未seen任务上都有大幅提高，具体来说，DAAC方法在seen任务上提高了24.3%，在未seen任务上提高了110.8%。<details>
<summary>Abstract</summary>
Imitation learning (IL) enables agents to mimic expert behaviors. Most previous IL techniques focus on precisely imitating one policy through mass demonstrations. However, in many applications, what humans require is the ability to perform various tasks directly through a few demonstrations of corresponding tasks, where the agent would meet many unexpected changes when deployed. In this scenario, the agent is expected to not only imitate the demonstration but also adapt to unforeseen environmental changes.   This motivates us to propose a new topic called imitator learning (ItorL), which aims to derive an imitator module that can on-the-fly reconstruct the imitation policies based on very limited expert demonstrations for different unseen tasks, without any extra adjustment. In this work, we focus on imitator learning based on only one expert demonstration. To solve ItorL, we propose Demo-Attention Actor-Critic (DAAC), which integrates IL into a reinforcement-learning paradigm that can regularize policies' behaviors in unexpected situations. Besides, for autonomous imitation policy building, we design a demonstration-based attention architecture for imitator policy that can effectively output imitated actions by adaptively tracing the suitable states in demonstrations. We develop a new navigation benchmark and a robot environment for \topic~and show that DAAC~outperforms previous imitation methods \textit{with large margins} both on seen and unseen tasks.
</details>
<details>
<summary>摘要</summary>
复制学习（IL）允许代理人模仿专家的行为。大多数前一些IL技术都是通过大量示例来精准地复制一个策略。然而，在许多应用场景中，人们需要代理人能够直接完成多个任务，而不需要大量的示例。在这种情况下，代理人需要不仅模仿示例，还需要适应不可预期的环境变化。这种情况 Motivates us to propose a new topic called imitator learning（ItorL）， which aims to derive an imitator module that can on-the-fly重建模仿策略 based on very limited expert demonstrations for different unseen tasks, without any extra adjustment. In this work, we focus on imitator learning based on only one expert demonstration. To solve ItorL, we propose Demo-Attention Actor-Critic（DAAC）， which integrates IL into a reinforcement-learning paradigm that can regularize policies' behaviors in unexpected situations. Besides, for autonomous imitation policy building, we design a demonstration-based attention architecture for imitator policy that can effectively output imitated actions by adaptively tracing the suitable states in demonstrations. We develop a new navigation benchmark and a robot environment for \topic~and show that DAAC~outperforms previous imitation methods \textit{with large margins} both on seen and unseen tasks.
</details></li>
</ul>
<hr>
<h2 id="Protecting-Sensitive-Data-through-Federated-Co-Training"><a href="#Protecting-Sensitive-Data-through-Federated-Co-Training" class="headerlink" title="Protecting Sensitive Data through Federated Co-Training"></a>Protecting Sensitive Data through Federated Co-Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05696">http://arxiv.org/abs/2310.05696</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amr Abourayya, Jens Kleesiek, Kanishka Rao, Erman Ayday, Bharat Rao, Geoff Webb, Michael Kamp</li>
<li>for: 保护敏感数据，避免公开地 revelas 本地训练数据。</li>
<li>methods: 使用联合学习方法，将本地训练的参数集成到一个共识模型中，以实现模型的训练。</li>
<li>results: 比较 federated learning 和分布式滤波两种方法， federated co-training 方法可以达到更高的隐私保护和模型质量。<details>
<summary>Abstract</summary>
In many critical applications, sensitive data is inherently distributed. Federated learning trains a model collaboratively by aggregating the parameters of locally trained models. This avoids exposing sensitive local data. It is possible, though, to infer upon the sensitive data from the shared model parameters. At the same time, many types of machine learning models do not lend themselves to parameter aggregation, such as decision trees, or rule ensembles. It has been observed that in many applications, in particular healthcare, large unlabeled datasets are publicly available. They can be used to exchange information between clients by distributed distillation, i.e., co-regularizing local training via the discrepancy between the soft predictions of each local client on the unlabeled dataset. This, however, still discloses private information and restricts the types of models to those trainable via gradient-based methods. We propose to go one step further and use a form of federated co-training, where local hard labels on the public unlabeled datasets are shared and aggregated into a consensus label. This consensus label can be used for local training by any supervised machine learning model. We show that this federated co-training approach achieves a model quality comparable to both federated learning and distributed distillation on a set of benchmark datasets and real-world medical datasets. It improves privacy over both approaches, protecting against common membership inference attacks to the highest degree. Furthermore, we show that federated co-training can collaboratively train interpretable models, such as decision trees and rule ensembles, achieving a model quality comparable to centralized training.
</details>
<details>
<summary>摘要</summary>
许多关键应用中敏感数据是自然地分布式。联邦学习通过合并本地训练模型的参数来培训模型，这样可以避免曝光敏感本地数据。然而，可以通过共享模型参数来推断敏感数据。此外，许多机器学习模型无法参数综合，如决策树或规则集。在许多应用中，尤其是医疗领域，大量的未标注数据公开可用。通过分布式蒸馏，即客户端之间通过未标注数据的差异来协调本地训练。这样仍然披露了私人信息，并限制了可以使用的模型类型。我们提议进一步使用联邦合作训练，其中本地硬标签在公共未标注数据上进行分布式合并，生成一个共识标签。这个共识标签可以用于本地训练任何超级vised机器学习模型。我们显示，这种联邦合作训练方法可以与联邦学习和分布式蒸馏相比，在一组benchmark数据集和真实医疗数据集上达到类似的模型质量。同时，它提高隐私性，保护 против最常见的会员推断攻击。此外，我们还显示，联邦合作训练可以共同训练可读性强的模型，如决策树和规则集，与中央训练相比。
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-Reinforcement-Learning-for-Temporal-Pattern-Prediction"><a href="#Hierarchical-Reinforcement-Learning-for-Temporal-Pattern-Prediction" class="headerlink" title="Hierarchical Reinforcement Learning for Temporal Pattern Prediction"></a>Hierarchical Reinforcement Learning for Temporal Pattern Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05695">http://arxiv.org/abs/2310.05695</a></li>
<li>repo_url: None</li>
<li>paper_authors: Faith Johnson, Kristin Dana</li>
<li>for: 这个论文探讨了使用层次强化学习（HRL）来解决时间序列预测任务。</li>
<li>methods: 作者使用了深度学习和HRL来开发一个用于预测股票价格时间序列的股票机器人，以及一个基于首人视频的车辆机器人来预测转向角。</li>
<li>results: 在两个领域中，作者发现了一种类型的HRL，即封顶强化学习，可以提供更高的训练速度和稳定性以及预测精度，而这一成功归功于网络层次结构中引入的时间和空间抽象。<details>
<summary>Abstract</summary>
In this work, we explore the use of hierarchical reinforcement learning (HRL) for the task of temporal sequence prediction. Using a combination of deep learning and HRL, we develop a stock agent to predict temporal price sequences from historical stock price data and a vehicle agent to predict steering angles from first person, dash cam images. Our results in both domains indicate that a type of HRL, called feudal reinforcement learning, provides significant improvements to training speed and stability and prediction accuracy over standard RL. A key component to this success is the multi-resolution structure that introduces both temporal and spatial abstraction into the network hierarchy.
</details>
<details>
<summary>摘要</summary>
在这个研究中，我们探索使用层次强制学习（HRL）来解决时间序列预测问题。通过将深度学习和HRL相结合，我们开发了一个股票代理来预测历史股票价格数据中的时间价格序列，以及一个车辆代理来预测来自首人、摄像头图像中的推理角度。我们在两个领域中的结果表明，一种称为“封顶强制学习”的HRL方法可以提供标准RL方法的训练速度和稳定性以及预测精度的显著改进。关键的一点是将多尺度结构引入网络层次结构，这种结构具有时间和空间抽象的双重优势。
</details></li>
</ul>
<hr>
<h2 id="Multi-timestep-models-for-Model-based-Reinforcement-Learning"><a href="#Multi-timestep-models-for-Model-based-Reinforcement-Learning" class="headerlink" title="Multi-timestep models for Model-based Reinforcement Learning"></a>Multi-timestep models for Model-based Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05672">http://arxiv.org/abs/2310.05672</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdelhakim Benechehab, Giuseppe Paolo, Albert Thomas, Maurizio Filippone, Balázs Kégl</li>
<li>for: This paper aims to improve the performance of model-based reinforcement learning (MBRL) algorithms by using a multi-timestep objective to train one-step models.</li>
<li>methods: The authors use a weighted sum of loss functions at various future horizons as their objective, with exponentially decaying weights, to improve the long-horizon performance of their models.</li>
<li>results: The authors find that their multi-timestep models outperform or match standard one-step models in both pure batch reinforcement learning (RL) and iterated batch RL scenarios, particularly in noisy environments.Here’s the same information in Simplified Chinese:</li>
<li>for: 这篇论文目标是提高基于模型的学习（MBRL）算法的性能，通过使用多个时间步骤的目标来训练一步模型。</li>
<li>methods: 作者们使用多个时间步骤的损失函数权重和衰减来提高模型的长期性能。</li>
<li>results: 作者们发现，他们的多个时间步骤模型在纯批量学习（RL）和迭代批量RL场景中都能够超过或与标准一步模型匹配，特别在噪音环境中表现出色， highlighting the potential of their approach in real-world applications。<details>
<summary>Abstract</summary>
In model-based reinforcement learning (MBRL), most algorithms rely on simulating trajectories from one-step dynamics models learned on data. A critical challenge of this approach is the compounding of one-step prediction errors as length of the trajectory grows. In this paper we tackle this issue by using a multi-timestep objective to train one-step models. Our objective is a weighted sum of a loss function (e.g., negative log-likelihood) at various future horizons. We explore and test a range of weights profiles. We find that exponentially decaying weights lead to models that significantly improve the long-horizon R2 score. This improvement is particularly noticeable when the models were evaluated on noisy data. Finally, using a soft actor-critic (SAC) agent in pure batch reinforcement learning (RL) and iterated batch RL scenarios, we found that our multi-timestep models outperform or match standard one-step models. This was especially evident in a noisy variant of the considered environment, highlighting the potential of our approach in real-world applications.
</details>
<details>
<summary>摘要</summary>
在基于模型的强化学习（MBRL）中，大多数算法都是通过从一步动力学模型学习数据上的一步预测错误来预测 trajectory。这种方法的一个挑战是预测误差的积累作用，随着 trajectory 的长度增长。在这篇论文中，我们解决这个问题 by using a multi-timestep objective to train one-step models。我们的目标是一个 weighted sum of a loss function（例如 negative log-likelihood）at various future horizons。我们探索和测试了不同的Weight profile。我们发现，使用恒速衰减的Weight leads to models that significantly improve the long-horizon R2 score。这种改进特别明显在噪音环境中， highlighting the potential of our approach in real-world applications。In addition, we used a soft actor-critic (SAC) agent in pure batch reinforcement learning (RL) and iterated batch RL scenarios, and found that our multi-timestep models outperformed or matched standard one-step models. This was especially evident in a noisy variant of the considered environment.
</details></li>
</ul>
<hr>
<h2 id="LARA-A-Light-and-Anti-overfitting-Retraining-Approach-for-Unsupervised-Anomaly-Detection"><a href="#LARA-A-Light-and-Anti-overfitting-Retraining-Approach-for-Unsupervised-Anomaly-Detection" class="headerlink" title="LARA: A Light and Anti-overfitting Retraining Approach for Unsupervised Anomaly Detection"></a>LARA: A Light and Anti-overfitting Retraining Approach for Unsupervised Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05668">http://arxiv.org/abs/2310.05668</a></li>
<li>repo_url: None</li>
<li>paper_authors: Feiyi Chen, Zhen Qing, Yingying Zhang, Shuiguang Deng, Yi Xiao, Guansong Pang, Qingsong Wen</li>
<li>for: 这个研究旨在提出一种Light and Anti-overfitting Retraining Approach (LARA)，用于深度Variational Autoencoder (VAEs) 时间序列异常检测方法中。</li>
<li>methods: 本研究使用了一个新的Retraining process，它可以快速地调整模型，并且避免过滤。此外，本研究还提出了一个叫做ruminate block的新方法，可以利用历史数据而不需要储存它们。</li>
<li>results: 本研究的实验结果显示，可以使用43个时间槽的新分布数据进行重训，却可以与现有的异常检测模型相比，并且显示出较低的过滤频率。此外，本研究还证明了LARA模型的过程调整 overhead 轻量级。<details>
<summary>Abstract</summary>
Most of current anomaly detection models assume that the normal pattern remains same all the time. However, the normal patterns of Web services change dramatically and frequently. The model trained on old-distribution data is outdated after such changes. Retraining the whole model every time is expensive. Besides, at the beginning of normal pattern changes, there is not enough observation data from the new distribution. Retraining a large neural network model with limited data is vulnerable to overfitting. Thus, we propose a Light and Anti-overfitting Retraining Approach (LARA) for deep variational auto-encoder based time series anomaly detection methods (VAEs). This work aims to make three novel contributions: 1) the retraining process is formulated as a convex problem and can converge at a fast rate as well as prevent overfitting; 2) designing a ruminate block, which leverages the historical data without the need to store them; 3) mathematically proving that when fine-tuning the latent vector and reconstructed data, the linear formations can achieve the least adjusting errors between the ground truths and the fine-tuned ones.   Moreover, we have performed many experiments to verify that retraining LARA with even 43 time slots of data from new distribution can result in its competitive F1 Score in comparison with the state-of-the-art anomaly detection models trained with sufficient data. Besides, we verify its light overhead.
</details>
<details>
<summary>摘要</summary>
现有的异常检测模型大多假设常规模式一直不变。然而，Web服务中常规模式会频繁变化，训练过去的模型会变得异常。每次 retraining 整个模型都是昂贵的。此外，在常规模式变化的开始时，新分布中的数据不够，使用有限的数据重新训练大型神经网络模型容易过拟合。因此，我们提出了一种轻量级、避免过拟合的重新训练方法（LARA），用于深度变量自动编码器基于时间序列异常检测方法（VAEs）。本工作的三个新贡献如下：1. 重新训练过程被形式化为一个凸问题，可以快速 converge 并避免过拟合。2. 设计了一个留存块，可以利用历史数据而无需存储。3. 数学上证明，当微调 latent vector 和重构数据时，线性形式可以实现最小调整误差 между 真实值和微调后的值。此外，我们进行了多个实验，证明在使用43个时间槽的新分布数据重新训练LARA后，其竞争性F1分数与state-of-the-art异常检测模型相比较高。同时，我们还证明了其轻量级。
</details></li>
</ul>
<hr>
<h2 id="Binary-Classification-with-Confidence-Difference"><a href="#Binary-Classification-with-Confidence-Difference" class="headerlink" title="Binary Classification with Confidence Difference"></a>Binary Classification with Confidence Difference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05632">http://arxiv.org/abs/2310.05632</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wwangwitsel/ConfDiff">https://github.com/wwangwitsel/ConfDiff</a></li>
<li>paper_authors: Wei Wang, Lei Feng, Yuchen Jiang, Gang Niu, Min-Ling Zhang, Masashi Sugiyama</li>
<li>for: 本研究旨在利用信度差（Confidence Difference，简称ConfDiff）来进行Binary分类，而不需要每个训练样本的点击标签。</li>
<li>methods: 我们提出了一种风险一致的方法来解决这个问题，并证明了这个方法的整体趋势和减震性。</li>
<li>results: 我们在 benchmark 数据集和一个实际应用中的推荐系统数据集上进行了广泛的实验，并证明了我们的提议的有效性。<details>
<summary>Abstract</summary>
Recently, learning with soft labels has been shown to achieve better performance than learning with hard labels in terms of model generalization, calibration, and robustness. However, collecting pointwise labeling confidence for all training examples can be challenging and time-consuming in real-world scenarios. This paper delves into a novel weakly supervised binary classification problem called confidence-difference (ConfDiff) classification. Instead of pointwise labeling confidence, we are given only unlabeled data pairs with confidence difference that specifies the difference in the probabilities of being positive. We propose a risk-consistent approach to tackle this problem and show that the estimation error bound achieves the optimal convergence rate. We also introduce a risk correction approach to mitigate overfitting problems, whose consistency and convergence rate are also proven. Extensive experiments on benchmark data sets and a real-world recommender system data set validate the effectiveness of our proposed approaches in exploiting the supervision information of the confidence difference.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Cost-sensitive-probabilistic-predictions-for-support-vector-machines"><a href="#Cost-sensitive-probabilistic-predictions-for-support-vector-machines" class="headerlink" title="Cost-sensitive probabilistic predictions for support vector machines"></a>Cost-sensitive probabilistic predictions for support vector machines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05997">http://arxiv.org/abs/2310.05997</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sandra Benítez-Peña, Rafael Blanquero, Emilio Carrizosa, Pepa Ramírez-Cobo</li>
<li>for: 这种方法是为了生成SVM模型中的概率输出，并且能够处理不均衡数据集，以及使用参数优化过程中生成的有价值信息来提高模型的性能。</li>
<li>methods: 这种方法使用了成本敏感的SVM模型，并将其嵌入到协同 ensemble 方法中，使用bootstrapEstimates来估计概率。</li>
<li>results: 数据测试表明，这种方法在各种数据集上比基准方法有着优异的性能。<details>
<summary>Abstract</summary>
Support vector machines (SVMs) are widely used and constitute one of the best examined and used machine learning models for two-class classification. Classification in SVM is based on a score procedure, yielding a deterministic classification rule, which can be transformed into a probabilistic rule (as implemented in off-the-shelf SVM libraries), but is not probabilistic in nature. On the other hand, the tuning of the regularization parameters in SVM is known to imply a high computational effort and generates pieces of information that are not fully exploited, not being used to build a probabilistic classification rule. In this paper we propose a novel approach to generate probabilistic outputs for the SVM. The new method has the following three properties. First, it is designed to be cost-sensitive, and thus the different importance of sensitivity (or true positive rate, TPR) and specificity (true negative rate, TNR) is readily accommodated in the model. As a result, the model can deal with imbalanced datasets which are common in operational business problems as churn prediction or credit scoring. Second, the SVM is embedded in an ensemble method to improve its performance, making use of the valuable information generated in the parameters tuning process. Finally, the probabilities estimation is done via bootstrap estimates, avoiding the use of parametric models as competing approaches. Numerical tests on a wide range of datasets show the advantages of our approach over benchmark procedures.
</details>
<details>
<summary>摘要</summary>
支持向量机（SVM）是广泛使用的机器学习模型之一，是二类分类中最好的考试和使用的模型之一。在SVM中的分类基于得分过程，得到了决定性的分类规则，可以转换为概率性的分类规则（如在各种SVM库中实现的），但是不是概率性的。然而，SVM的常量参数优化知识具有高计算成本和生成不完全利用的信息，不会建立概率分类规则。在这篇论文中，我们提出了一种新的方法，以生成SVM的概率输出。这种方法具有以下三个特点：首先，它是成本敏感的，可以 readily 折衔不均衡的数据集，这些数据集在运营商业问题中很常见，如脱退预测和信用评分。第二，SVM被嵌入到集成方法中，以提高其性能，利用参数优化过程中生成的有价值信息。最后，概率估计通过 bootstrap 估计进行，避免使用参数模型作为竞争方法。数值测试在各种数据集上表明了我们的方法的优势。
</details></li>
</ul>
<hr>
<h2 id="On-Prediction-Modelers-and-Decision-Makers-Why-Fairness-Requires-More-Than-a-Fair-Prediction-Model"><a href="#On-Prediction-Modelers-and-Decision-Makers-Why-Fairness-Requires-More-Than-a-Fair-Prediction-Model" class="headerlink" title="On Prediction-Modelers and Decision-Makers: Why Fairness Requires More Than a Fair Prediction Model"></a>On Prediction-Modelers and Decision-Makers: Why Fairness Requires More Than a Fair Prediction Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05598">http://arxiv.org/abs/2310.05598</a></li>
<li>repo_url: None</li>
<li>paper_authors: Teresa Scantamburlo, Joachim Baumann, Christoph Heitz</li>
<li>for: 本文旨在阐述在预测基于决策中的公平性问题，并提出一个框架来帮助实现公平性。</li>
<li>methods: 本文使用了概念分离技术，将预测和决策分为两个独立的步骤，以便更好地理解和实现公平性。</li>
<li>results: 本文提出了一个框架，可以帮助在预测基于决策中实现公平性，并提出了一些实现公平性的策略和方法。<details>
<summary>Abstract</summary>
An implicit ambiguity in the field of prediction-based decision-making regards the relation between the concepts of prediction and decision. Much of the literature in the field tends to blur the boundaries between the two concepts and often simply speaks of 'fair prediction.' In this paper, we point out that a differentiation of these concepts is helpful when implementing algorithmic fairness. Even if fairness properties are related to the features of the used prediction model, what is more properly called 'fair' or 'unfair' is a decision system, not a prediction model. This is because fairness is about the consequences on human lives, created by a decision, not by a prediction. We clarify the distinction between the concepts of prediction and decision and show the different ways in which these two elements influence the final fairness properties of a prediction-based decision system. In addition to exploring this relationship conceptually and practically, we propose a framework that enables a better understanding and reasoning of the conceptual logic of creating fairness in prediction-based decision-making. In our framework, we specify different roles, namely the 'prediction-modeler' and the 'decision-maker,' and the information required from each of them for being able to implement fairness of the system. Our framework allows for deriving distinct responsibilities for both roles and discussing some insights related to ethical and legal requirements. Our contribution is twofold. First, we shift the focus from abstract algorithmic fairness to context-dependent decision-making, recognizing diverse actors with unique objectives and independent actions. Second, we provide a conceptual framework that can help structure prediction-based decision problems with respect to fairness issues, identify responsibilities, and implement fairness governance mechanisms in real-world scenarios.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translation into Simplified Chinese一个隐式的歧义在预测基础决策领域是预测和决策的关系。大多数 литературе在这个领域通常会混同这两个概念，并只是说的'公平预测'。在这篇论文中，我们指出了这两个概念之间的分化是有助于实施算法公平的。即使公平性特性与预测模型的特性相关，但是真正是'公平'或'不公平'的是决策系统，不是预测模型。这是因为公平是关于人类生活的后果，而不是预测的结果。我们清楚地区分了预测和决策的概念，并显示了这两个元素在最终公平性质量上的不同影响。除了探讨这种关系的概念和实践方面，我们提出了一个框架，允许更好地理解和理解预测基础决策中的公平创造机制。在我们的框架中，我们详细定义了不同角色，包括'预测模型者'和'决策者'，以及它们所需的信息，以便实现预测基础决策系统的公平。我们的框架允许 derive出不同的责任，并讨论一些与伦理和法律要求相关的洞察。我们的贡献是两重的。首先，我们将焦点从抽象的算法公平转移到了 Context-dependent 决策，认可多种演员有独特的目标和独立行动。其次，我们提供了一个概念框架，可以帮助结构预测基础决策问题，识别责任，并在实际场景中实施公平管理机制。
</details></li>
</ul>
<hr>
<h2 id="ODEFormer-Symbolic-Regression-of-Dynamical-Systems-with-Transformers"><a href="#ODEFormer-Symbolic-Regression-of-Dynamical-Systems-with-Transformers" class="headerlink" title="ODEFormer: Symbolic Regression of Dynamical Systems with Transformers"></a>ODEFormer: Symbolic Regression of Dynamical Systems with Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05573">http://arxiv.org/abs/2310.05573</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sdascoli/odeformer">https://github.com/sdascoli/odeformer</a></li>
<li>paper_authors: Stéphane d’Ascoli, Sören Becker, Alexander Mathis, Philippe Schwaller, Niki Kilbertus</li>
<li>for: 描述一种可以从单个解曲线观测数据中推断多维常微方程系统的符号形式传播模型（ODEFormer）。</li>
<li>methods: 使用变换器来推断多维常微方程系统的符号形式。</li>
<li>results: ODEFormer在两个数据集上（Strogatz和ODEBench）表现出色，在干扰和不规则观测数据中 Displaying substantially improved robustness and faster inference compared to existing methods。<details>
<summary>Abstract</summary>
We introduce ODEFormer, the first transformer able to infer multidimensional ordinary differential equation (ODE) systems in symbolic form from the observation of a single solution trajectory. We perform extensive evaluations on two datasets: (i) the existing "Strogatz" dataset featuring two-dimensional systems; (ii) ODEBench, a collection of one- to four-dimensional systems that we carefully curated from the literature to provide a more holistic benchmark. ODEFormer consistently outperforms existing methods while displaying substantially improved robustness to noisy and irregularly sampled observations, as well as faster inference. We release our code, model and benchmark dataset publicly.
</details>
<details>
<summary>摘要</summary>
我们介绍ODEFormer，首个能够从单一解析轨迹观测中推导多维常微方程系统的transformer。我们在两个数据集上进行了广泛的评估：（一）现有的“Strogatz”数据集，这是一个二维系统的数据集；（二）ODEBench，我们从文献中精心范选了一些一至四维系统，以提供更加全面的benchmark。ODEFormer在数据集上一般性高，而且在噪音和不规则采样观测下具有substantially提高的Robustness，以及更快的推导速度。我们将代码、模型和数据集公开发布。
</details></li>
</ul>
<hr>
<h2 id="A-New-Transformation-Approach-for-Uplift-Modeling-with-Binary-Outcome"><a href="#A-New-Transformation-Approach-for-Uplift-Modeling-with-Binary-Outcome" class="headerlink" title="A New Transformation Approach for Uplift Modeling with Binary Outcome"></a>A New Transformation Approach for Uplift Modeling with Binary Outcome</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05549">http://arxiv.org/abs/2310.05549</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kun Li, Jiang Tian, Xiaojia Xiang</li>
<li>for: 这篇论文是关于如何实现更好的客户预测和精确的标的定义，以提高营销效果。</li>
<li>methods: 本论文使用的方法是一种新的变数方法，可以将原始的对象指标转换为一个新的变数，以便更好地预测客户的反应。</li>
<li>results: 实验结果显示，新的变数方法可以优化客户预测和标的定义，提高营销效果。此外，这种方法还可以轻松地应用在实际应用中。<details>
<summary>Abstract</summary>
Uplift modeling has been used effectively in fields such as marketing and customer retention, to target those customers who are more likely to respond due to the campaign or treatment. Essentially, it is a machine learning technique that predicts the gain from performing some action with respect to not taking it. A popular class of uplift models is the transformation approach that redefines the target variable with the original treatment indicator. These transformation approaches only need to train and predict the difference in outcomes directly. The main drawback of these approaches is that in general it does not use the information in the treatment indicator beyond the construction of the transformed outcome and usually is not efficient. In this paper, we design a novel transformed outcome for the case of the binary target variable and unlock the full value of the samples with zero outcome. From a practical perspective, our new approach is flexible and easy to use. Experimental results on synthetic and real-world datasets obviously show that our new approach outperforms the traditional one. At present, our new approach has already been applied to precision marketing in a China nation-wide financial holdings group.
</details>
<details>
<summary>摘要</summary>
《升级模型》在市场营销和客户保持方面得到了有效应用，以针对那些响应更高的客户进行投入。概括来说，它是一种机器学习技术，预测对不进行处理的结果所带来的提升。一种受欢迎的类型的升级模型是转换方法，它重新定义目标变量与原始治理器指标之间的关系。这些转换方法只需要训练和预测直接的差异。但是，这些方法通常不使用治理器指标中的信息，除了构建转换后的结果外。在这篇论文中，我们设计了一种新的转换结果，用于二分类目标变量的情况。我们的新方法可以充分利用零结果样本的信息，从而提高准确率。实验结果表明，我们的新方法在synthetic和实际数据集上明显超过传统方法。在一家中国国家范围内的金融控股集团中，我们的新方法已经应用于精准营销。（Note: Please note that the translation is provided as-is, and may not be perfect or completely idiomatic. However, it should be sufficient to convey the general meaning of the text.)
</details></li>
</ul>
<hr>
<h2 id="NetTiSA-Extended-IP-Flow-with-Time-series-Features-for-Universal-Bandwidth-constrained-High-speed-Network-Traffic-Classification"><a href="#NetTiSA-Extended-IP-Flow-with-Time-series-Features-for-Universal-Bandwidth-constrained-High-speed-Network-Traffic-Classification" class="headerlink" title="NetTiSA: Extended IP Flow with Time-series Features for Universal Bandwidth-constrained High-speed Network Traffic Classification"></a>NetTiSA: Extended IP Flow with Time-series Features for Universal Bandwidth-constrained High-speed Network Traffic Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05530">http://arxiv.org/abs/2310.05530</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/koumajos/classification_by_nettisa_flow">https://github.com/koumajos/classification_by_nettisa_flow</a></li>
<li>paper_authors: Josef Koumar, Karel Hynek, Jaroslav Pešek, Tomáš Čejka</li>
<li>for: 这篇论文旨在提出一种基于流量记录的网络流量监测方法，以便在各种网络基础设施上部署，包括承载数百万人的大型IPS网络。</li>
<li>methods: 该方法基于流量记录的时间序列分析，提出了一种新的扩展IP流记录（NetTiSA），并对25种网络类型任务进行了广泛的测试，以证明NetTiSA的广泛适用性和高可用性。</li>
<li>results: 测试结果表明，NetTiSA可以高度精准地分类网络流量，并且在计算流量扩展时对性能的影响较小。此外，NetTiSA可以在100Gbps级别的高速ISP网络上进行实际部署，因此可以提供广泛的网络安全保护。<details>
<summary>Abstract</summary>
Network traffic monitoring based on IP Flows is a standard monitoring approach that can be deployed to various network infrastructures, even the large IPS-based networks connecting millions of people. Since flow records traditionally contain only limited information (addresses, transport ports, and amount of exchanged data), they are also commonly extended for additional features that enable network traffic analysis with high accuracy. Nevertheless, the flow extensions are often too large or hard to compute, which limits their deployment only to smaller-sized networks. This paper proposes a novel extended IP flow called NetTiSA (Network Time Series Analysed), which is based on the analysis of the time series of packet sizes. By thoroughly testing 25 different network classification tasks, we show the broad applicability and high usability of NetTiSA, which often outperforms the best-performing related works. For practical deployment, we also consider the sizes of flows extended for NetTiSA and evaluate the performance impacts of its computation in the flow exporter. The novel feature set proved universal and deployable to high-speed ISP networks with 100\,Gbps lines; thus, it enables accurate and widespread network security protection.
</details>
<details>
<summary>摘要</summary>
translate into Simplified Chinese:网络流量监测基于IP流是标准监测方法，可以部署到不同的网络基础设施，包括连接百万人的IPS网络。由于流记录通常只包含地址、传输端口和交换的数据量，因此它们常被扩展以获得高精度的网络流量分析。然而，扩展流量通常是太大或计算过程太复杂，因此只能在较小的网络上进行部署。本文提出了一种基于时间序列分析的增强IP流，称为NetTiSA（网络时间序列分析）。通过对25种不同的网络分类任务进行严格测试，我们表明NetTiSA的广泛适用性和高可用性。此外，我们还考虑了NetTiSA扩展流量的大小以及计算流程的性能影响。 results show that the novel feature set is universal and deployable to high-speed ISP networks with 100 Gbps lines, enabling accurate and widespread network security protection.Note: Simplified Chinese is a romanization of Chinese, it is not a direct translation of the original text. The translation is based on the pronunciation of the characters, and it may not be exactly the same as the original text.
</details></li>
</ul>
<hr>
<h2 id="A-novel-Network-Science-Algorithm-for-Improving-Triage-of-Patients"><a href="#A-novel-Network-Science-Algorithm-for-Improving-Triage-of-Patients" class="headerlink" title="A novel Network Science Algorithm for Improving Triage of Patients"></a>A novel Network Science Algorithm for Improving Triage of Patients</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05996">http://arxiv.org/abs/2310.05996</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pietro Hiram Guzzi, Annamaria De Filippo, Pierangelo Veltri</li>
<li>for: This paper aims to develop a novel algorithm for triaging patients based on the analysis of patient data, with the goal of improving the efficiency, accuracy, and consistency of patient prioritization.</li>
<li>methods: The algorithm is based on rigorous preprocessing and feature engineering of a comprehensive data set containing relevant patient information, such as vital signs, symptoms, and medical history.</li>
<li>results: The experimental results demonstrate that the algorithm achieved high accuracy and performance, outperforming traditional triage methods.<details>
<summary>Abstract</summary>
Patient triage plays a crucial role in healthcare, ensuring timely and appropriate care based on the urgency of patient conditions. Traditional triage methods heavily rely on human judgment, which can be subjective and prone to errors. Recently, a growing interest has been in leveraging artificial intelligence (AI) to develop algorithms for triaging patients. This paper presents the development of a novel algorithm for triaging patients. It is based on the analysis of patient data to produce decisions regarding their prioritization. The algorithm was trained on a comprehensive data set containing relevant patient information, such as vital signs, symptoms, and medical history. The algorithm was designed to accurately classify patients into triage categories through rigorous preprocessing and feature engineering. Experimental results demonstrate that our algorithm achieved high accuracy and performance, outperforming traditional triage methods. By incorporating computer science into the triage process, healthcare professionals can benefit from improved efficiency, accuracy, and consistency, prioritizing patients effectively and optimizing resource allocation. Although further research is needed to address challenges such as biases in training data and model interpretability, the development of AI-based algorithms for triaging patients shows great promise in enhancing healthcare delivery and patient outcomes.
</details>
<details>
<summary>摘要</summary>
医疗患者分类占据了医疗业中关键的地位，确保患者得到了时间适当的和适合的护理，根据患者的病情严重程度。传统的分类方法依赖于人类的判断，这可能是主观的和容易出错的。在最近的几年里，人们对使用人工智能（AI）开发患者分类算法表示了增加的兴趣。本文描述了一种基于患者数据分析的新的患者分类算法的开发。该算法通过对病人数据进行严格的预处理和特征工程来生成准确的患者分类结果。实验结果表明，我们的算法可以准确地将患者分为不同的分类 катего里，并且表现出了高度的准确率和性能，超过传统的分类方法。通过将计算机科学引入分类过程，医疗专业人员可以从而获得更高效、准确和一致的患者分类结果，优先级化患者，最大化资源的分配。虽然还需要进一步的研究，例如训练数据中存在的偏见和模型解释性等问题，但AI在患者分类中的应用显示了极大的潜力，以改善医疗服务和患者结果。
</details></li>
</ul>
<hr>
<h2 id="Projecting-infinite-time-series-graphs-to-finite-marginal-graphs-using-number-theory"><a href="#Projecting-infinite-time-series-graphs-to-finite-marginal-graphs-using-number-theory" class="headerlink" title="Projecting infinite time series graphs to finite marginal graphs using number theory"></a>Projecting infinite time series graphs to finite marginal graphs using number theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05526">http://arxiv.org/abs/2310.05526</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andreas Gerhardus, Jonas Wahl, Sofia Faltenbacher, Urmi Ninad, Jakob Runge</li>
<li>for: 本文是用于推广和应用 causal-graphical-model 框架的方法和应用工作的一种新方法。</li>
<li>methods: 本文提出了一种方法，可以将无穷时间序列图表示为 marginal graphical models，以解决在无穷图中的 $m$-separation 问题。</li>
<li>results: 本文提出了一种算法，可以将无穷时间序列图 projection 到 marginal graphical models，并证明这些 marginal graphs 可以用于 causal discovery 和 causal effect estimation。<details>
<summary>Abstract</summary>
In recent years, a growing number of method and application works have adapted and applied the causal-graphical-model framework to time series data. Many of these works employ time-resolved causal graphs that extend infinitely into the past and future and whose edges are repetitive in time, thereby reflecting the assumption of stationary causal relationships. However, most results and algorithms from the causal-graphical-model framework are not designed for infinite graphs. In this work, we develop a method for projecting infinite time series graphs with repetitive edges to marginal graphical models on a finite time window. These finite marginal graphs provide the answers to $m$-separation queries with respect to the infinite graph, a task that was previously unresolved. Moreover, we argue that these marginal graphs are useful for causal discovery and causal effect estimation in time series, effectively enabling to apply results developed for finite graphs to the infinite graphs. The projection procedure relies on finding common ancestors in the to-be-projected graph and is, by itself, not new. However, the projection procedure has not yet been algorithmically implemented for time series graphs since in these infinite graphs there can be infinite sets of paths that might give rise to common ancestors. We solve the search over these possibly infinite sets of paths by an intriguing combination of path-finding techniques for finite directed graphs and solution theory for linear Diophantine equations. By providing an algorithm that carries out the projection, our paper makes an important step towards a theoretically-grounded and method-agnostic generalization of a range of causal inference methods and results to time series.
</details>
<details>
<summary>摘要</summary>
近年来，一些方法和应用工作已经适应和应用了 causal-graphical-model 框架到时间序列数据。许多这些工作使用时间分解的 causal 图，其延伸到过去和未来无穷，并且图的边重复在时间上，表明了预设的站立 causal 关系。然而，大多数结果和算法从 causal-graphical-model 框架不适用于无穷图。在这种工作中，我们开发了一种方法，将无穷时间序列图的 repetitive 边投影到固定时间窗口内的 marginal 图形式。这些 marginal 图可以回答 $m$-separation 查询，对于无穷图来说，是以前未解决的问题。此外，我们认为这些 marginal 图对 causal 发现和 causal 效应估计在时间序列中都是有用的，因此可以将 finite 图上的结果应用到无穷图上。投影过程基于在要投影的图中寻找共同祖先的搜索，并不是新的。然而，在时间序列图上执行这种投影过程具有挑战，因为可能存在无穷多个路径，导致共同祖先。我们解决这个问题，通过一种独特的将 finite 图上的路径找到与无穷图相匹配的方法，并使用解决线性Diophantine方程的解辑理论。我们的论文提供了一种可以执行投影的算法，这个步骤对于在时间序列中应用 causal 推理方法和结果进行 theoretically-grounded 和方法-agnostic 的总体化做出了重要贡献。
</details></li>
</ul>
<hr>
<h2 id="WeatherGNN-Exploiting-Complicated-Relationships-in-Numerical-Weather-Prediction-Bias-Correction"><a href="#WeatherGNN-Exploiting-Complicated-Relationships-in-Numerical-Weather-Prediction-Bias-Correction" class="headerlink" title="WeatherGNN: Exploiting Complicated Relationships in Numerical Weather Prediction Bias Correction"></a>WeatherGNN: Exploiting Complicated Relationships in Numerical Weather Prediction Bias Correction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05517">http://arxiv.org/abs/2310.05517</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/water-wbq/WeatherGNN">https://github.com/water-wbq/WeatherGNN</a></li>
<li>paper_authors: Binqing Wu, Weiqi Chen, Wengwei Wang, Bingqing Peng, Liang Sun, Ling Chen</li>
<li>for:  corrected numerical weather prediction (NWP) bias</li>
<li>methods:  Graph Neural Networks (GNN) and factor-wise GNN, fast hierarchical GNN</li>
<li>results:  superior performance compared to other state-of-the-art (SOTA) methods, with an average improvement of 40.50% on RMSE compared to the original NWP.Here is the Chinese translation:</li>
<li>for:  corrected numerical weather prediction (NWP) 误差</li>
<li>methods:  Graph Neural Networks (GNN) 和分量 wise GNN, 快速层次 GNN</li>
<li>results:  与其他状态首选 (SOTA) 方法相比，平均提高40.50%的RMSE 相对于原始 NWP.<details>
<summary>Abstract</summary>
Numerical weather prediction (NWP) may be inaccurate or biased due to incomplete atmospheric physical processes, insufficient spatial-temporal resolution, and inherent uncertainty of weather. Previous studies have attempted to correct biases by using handcrafted features and domain knowledge, or by applying general machine learning models naively. They do not fully explore the complicated meteorologic interactions and spatial dependencies in the atmosphere dynamically, which limits their applicability in NWP bias-correction. Specifically, weather factors interact with each other in complex ways, and these interactions can vary regionally. In addition, the interactions between weather factors are further complicated by the spatial dependencies between regions, which are influenced by varied terrain and atmospheric motions. To address these issues, we propose WeatherGNN, an NWP bias-correction method that utilizes Graph Neural Networks (GNN) to learn meteorologic and geographic relationships in a unified framework. Our approach includes a factor-wise GNN that captures meteorological interactions within each grid (a specific location) adaptively, and a fast hierarchical GNN that captures spatial dependencies between grids dynamically. Notably, the fast hierarchical GNN achieves linear complexity with respect to the number of grids, enhancing model efficiency and scalability. Our experimental results on two real-world datasets demonstrate the superiority of WeatherGNN in comparison with other SOTA methods, with an average improvement of 40.50\% on RMSE compared to the original NWP.
</details>
<details>
<summary>摘要</summary>
numerical 天气预测（NWP）可能存在偏差或偏见，原因包括大气物理过程的缺失、时空分解不够细致，以及天气预测的内在不确定性。先前的研究已经尝试使用手工设计的特征和领域知识来纠正偏差，或者直接使用通用机器学习模型。但这些方法并未充分探索大气中复杂的物理互动和空间依赖关系，限制了它们在NWP偏差纠正中的应用。具体来说，天气因素之间存在复杂的互动，这些互动可能因地域而异，而且这些互动还受到不同的地形和大气动力的影响。为解决这些问题，我们提出了WeatherGNN，一种基于图神经网络（GNN）的NWP偏差纠正方法。我们的方法包括一个因素独立的GNN，可以在每个网格（具体位置）中适应地捕捉大气物理互动，以及一个快速的层次GNN，可以在不同网格之间快速捕捉空间依赖关系。吸引注意的是，快速的层次GNN在网格数量 linear 复杂度上具有优化，从而提高模型的效率和扩展性。我们的实验结果表明，WeatherGNN在两个真实世界数据集上的表现胜过其他SOTA方法，具有40.50%的RMSE提升。
</details></li>
</ul>
<hr>
<h2 id="A-Neural-Tangent-Kernel-View-on-Federated-Averaging-for-Deep-Linear-Neural-Network"><a href="#A-Neural-Tangent-Kernel-View-on-Federated-Averaging-for-Deep-Linear-Neural-Network" class="headerlink" title="A Neural Tangent Kernel View on Federated Averaging for Deep Linear Neural Network"></a>A Neural Tangent Kernel View on Federated Averaging for Deep Linear Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05495">http://arxiv.org/abs/2310.05495</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xin Liu, Dazhi Zhan, Wei Tao, Xin Ma, Yu Pan, Yu Ding, Zhisong Pan</li>
<li>for: 这篇论文的目的是提供 FedAvg 在训练神经网络时的全球收敛性保证。</li>
<li>methods: 这篇论文使用 NTK 理论来研究 FedAvg 在训练神经网络时的收敛性。</li>
<li>results: 这篇论文提供了 FedAvg 在训练深度线性神经网络时的全球收敛性保证，并且通过实验验证了理论结论。<details>
<summary>Abstract</summary>
Federated averaging (FedAvg) is a widely employed paradigm for collaboratively training models from distributed clients without sharing data. Nowadays, the neural network has achieved remarkable success due to its extraordinary performance, which makes it a preferred choice as the model in FedAvg. However, the optimization problem of the neural network is often non-convex even non-smooth. Furthermore, FedAvg always involves multiple clients and local updates, which results in an inaccurate updating direction. These properties bring difficulties in analyzing the convergence of FedAvg in training neural networks. Recently, neural tangent kernel (NTK) theory has been proposed towards understanding the convergence of first-order methods in tackling the non-convex problem of neural networks. The deep linear neural network is a classical model in theoretical subject due to its simple formulation. Nevertheless, there exists no theoretical result for the convergence of FedAvg in training the deep linear neural network. By applying NTK theory, we make a further step to provide the first theoretical guarantee for the global convergence of FedAvg in training deep linear neural networks. Specifically, we prove FedAvg converges to the global minimum at a linear rate $\mathcal{O}\big((1-\eta K /N)^t\big)$, where $t$ is the number of iterations, $\eta$ is the learning rate, $N$ is the number of clients and $K$ is the number of local updates. Finally, experimental evaluations on two benchmark datasets are conducted to empirically validate the correctness of our theoretical findings.
</details>
<details>
<summary>摘要</summary>
《联合平均（FedAvg）》是一种广泛使用的方法，用于在分布式客户端上共同训练模型而无需分享数据。现在，神经网络已经取得了很大的成功，使得它成为了FedAvg中的首选模型。然而，神经网络的优化问题经常是非凸的，甚至是不满足的。此外，FedAvg总是包括多个客户端和本地更新，这会导致不准确的更新方向。这些特性使得分析FedAvg在训练神经网络的 converges 变得更加困难。近年来，神经积极核（NTK）理论被提出，用于理解在非凸神经网络中第一个方法的converges。深度线性神经网络是神经网络理论中的经典模型，然而，关于FedAvg在训练深度线性神经网络的converges的理论结果并未存在。通过应用NTK理论，我们做出了一个进一步的步骤，提供了对FedAvg在训练深度线性神经网络的全球最佳化的首次理论保证。具体来说，我们证明FedAvg会在$(1-\eta K/N)^t$的线性速率下收敛到全球最小值，其中$t$是迭代次数，$\eta$是学习率，$N$是客户端的数量，$K$是本地更新的数量。最后，我们在两个标准数据集上进行了实验评估，以验证我们的理论发现的正确性。
</details></li>
</ul>
<hr>
<h2 id="Integration-free-Training-for-Spatio-temporal-Multimodal-Covariate-Deep-Kernel-Point-Processes"><a href="#Integration-free-Training-for-Spatio-temporal-Multimodal-Covariate-Deep-Kernel-Point-Processes" class="headerlink" title="Integration-free Training for Spatio-temporal Multimodal Covariate Deep Kernel Point Processes"></a>Integration-free Training for Spatio-temporal Multimodal Covariate Deep Kernel Point Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05485">http://arxiv.org/abs/2310.05485</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yixuan Zhang, Quyu Kong, Feng Zhou</li>
<li>for: 本研究提出了一种新的深度空间时间点 процесс模型（深度混合点过程），即DKMPP，该模型利用多modal的covariate信息。</li>
<li>methods: DKMPP使用一种更 flexible的深度kernel来模型事件和covariate数据之间的复杂关系，从而提高模型的表达能力。</li>
<li>results: 我们的实验表明，DKMPP和其相应的分数基 estimator在基eline模型之上表现出优异，展示了将covariate信息、深度kernel和分数基 estimator相结合的优势。<details>
<summary>Abstract</summary>
In this study, we propose a novel deep spatio-temporal point process model, Deep Kernel Mixture Point Processes (DKMPP), that incorporates multimodal covariate information. DKMPP is an enhanced version of Deep Mixture Point Processes (DMPP), which uses a more flexible deep kernel to model complex relationships between events and covariate data, improving the model's expressiveness. To address the intractable training procedure of DKMPP due to the non-integrable deep kernel, we utilize an integration-free method based on score matching, and further improve efficiency by adopting a scalable denoising score matching method. Our experiments demonstrate that DKMPP and its corresponding score-based estimators outperform baseline models, showcasing the advantages of incorporating covariate information, utilizing a deep kernel, and employing score-based estimators.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们提出了一种新的深度空间时间点过程模型，深度混合点过程（DKMPP），该模型利用多Modal covariate信息。DKMPP是DMPP的改进版本，它使用更 flexible的深度核函数来模型事件和 covariate数据之间的复杂关系，提高模型的表达力。为了解决DKMPP的训练过程中的非可 интегриble深度核函数问题，我们使用了不需要 интеграción的得分匹配方法，并通过采用扩展的净化得分匹配方法来提高效率。我们的实验表明，DKMPP和其相应的得分基估计器在比例模型和事件时间点过程模型方面具有优势， demonstrating the benefits of incorporating covariate information, using a deep kernel, and employing score-based estimators.Note: The translation is in Simplified Chinese, which is one of the two standard versions of Chinese. The other version is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Vibroacoustic-Frequency-Response-Prediction-with-Query-based-Operator-Networks"><a href="#Vibroacoustic-Frequency-Response-Prediction-with-Query-based-Operator-Networks" class="headerlink" title="Vibroacoustic Frequency Response Prediction with Query-based Operator Networks"></a>Vibroacoustic Frequency Response Prediction with Query-based Operator Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05469">http://arxiv.org/abs/2310.05469</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ecker-lab/FQ-Operator">https://github.com/ecker-lab/FQ-Operator</a></li>
<li>paper_authors: Jan van Delden, Julius Schultz, Christopher Blech, Sabine C. Langer, Timo Lüddecke</li>
<li>for: 本研究旨在提高机械结构如飞机、汽车和房屋等的震动声波传播的理解，以确保其用户的健康和舒适性。</li>
<li>methods: 本研究使用数据驱动模型来加速 numerical simulation，以便进行设计优化、不确定性评估和设计空间探索等任务。特别是，我们提出了一种新的频率查询运算符模型，该模型可以将板体几何特征映射到频率响应函数。</li>
<li>results: 我们在一个包含12,000个板体几何特征的全面性 benchmark 上评估了我们的方法，并发现它比 DeepONets、Fourier Neural Operators 和传统神经网络架构更高效。<details>
<summary>Abstract</summary>
Understanding vibroacoustic wave propagation in mechanical structures like airplanes, cars and houses is crucial to ensure health and comfort of their users. To analyze such systems, designers and engineers primarily consider the dynamic response in the frequency domain, which is computed through expensive numerical simulations like the finite element method. In contrast, data-driven surrogate models offer the promise of speeding up these simulations, thereby facilitating tasks like design optimization, uncertainty quantification, and design space exploration. We present a structured benchmark for a representative vibroacoustic problem: Predicting the frequency response for vibrating plates with varying forms of beadings. The benchmark features a total of 12,000 plate geometries with an associated numerical solution and introduces evaluation metrics to quantify the prediction quality. To address the frequency response prediction task, we propose a novel frequency query operator model, which is trained to map plate geometries to frequency response functions. By integrating principles from operator learning and implicit models for shape encoding, our approach effectively addresses the prediction of resonance peaks of frequency responses. We evaluate the method on our vibrating-plates benchmark and find that it outperforms DeepONets, Fourier Neural Operators and more traditional neural network architectures. The code and dataset are available from https://eckerlab.org/code/delden2023_plate.
</details>
<details>
<summary>摘要</summary>
In this study, we present a structured benchmark for a representative vibroacoustic problem: predicting the frequency response of vibrating plates with varying forms of beadings. The benchmark features a total of 12,000 plate geometries with associated numerical solutions and introduces evaluation metrics to quantify prediction quality. To address the frequency response prediction task, we propose a novel frequency query operator model, which is trained to map plate geometries to frequency response functions. By integrating principles from operator learning and implicit models for shape encoding, our approach effectively predicts the resonance peaks of frequency responses.We evaluate our method on our vibrating-plates benchmark and find that it outperforms DeepONets, Fourier Neural Operators, and more traditional neural network architectures. The code and dataset are available at <https://eckerlab.org/code/delden2023_plate>.
</details></li>
</ul>
<hr>
<h2 id="ExIFFI-and-EIF-Interpretability-and-Enhanced-Generalizability-to-Extend-the-Extended-Isolation-Forest"><a href="#ExIFFI-and-EIF-Interpretability-and-Enhanced-Generalizability-to-Extend-the-Extended-Isolation-Forest" class="headerlink" title="ExIFFI and EIF+: Interpretability and Enhanced Generalizability to Extend the Extended Isolation Forest"></a>ExIFFI and EIF+: Interpretability and Enhanced Generalizability to Extend the Extended Isolation Forest</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05468">http://arxiv.org/abs/2310.05468</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alessioarcudi/exiffi">https://github.com/alessioarcudi/exiffi</a></li>
<li>paper_authors: Alessio Arcudi, Davide Frizzo, Chiara Masiero, Gian Antonio Susto</li>
<li>for: 本研究旨在提出一种可解释的异常检测方法，以帮助用户更好地理解模型的预测结果并进行根本分析。</li>
<li>methods: 本研究使用了一种加强版的扩展隔离林（EIF），并提出了一种新的可解释方法ExIFFI，该方法通过特征排名来提供异常检测结果的解释。</li>
<li>results: 实验结果显示，ExIFFI在异常检测和特征选择方面具有较高的效果和可解释性。此外，研究还提供了一些实际数据集的评估结果，以便进一步研究和复现。<details>
<summary>Abstract</summary>
Anomaly detection, an essential unsupervised machine learning task, involves identifying unusual behaviors within complex datasets and systems. While Machine Learning algorithms and decision support systems (DSSs) offer effective solutions for this task, simply pinpointing anomalies often falls short in real-world applications. Users of these systems often require insight into the underlying reasons behind predictions to facilitate Root Cause Analysis and foster trust in the model. However, due to the unsupervised nature of anomaly detection, creating interpretable tools is challenging. This work introduces EIF+, an enhanced variant of Extended Isolation Forest (EIF), designed to enhance generalization capabilities. Additionally, we present ExIFFI, a novel approach that equips Extended Isolation Forest with interpretability features, specifically feature rankings. Experimental results provide a comprehensive comparative analysis of Isolation-based approaches for Anomaly Detection, including synthetic and real dataset evaluations that demonstrate ExIFFI's effectiveness in providing explanations. We also illustrate how ExIFFI serves as a valid feature selection technique in unsupervised settings. To facilitate further research and reproducibility, we also provide open-source code to replicate the results.
</details>
<details>
<summary>摘要</summary>
异常检测是机器学习中的一项不supervised任务，它的目的是在复杂的数据和系统中找到不寻常的行为。而机器学习算法和决策支持系统（DSS）可以提供有效的解决方案，但仅仅找到异常点不足以应对实际应用中的需求。用户需要对模型预测的根本原因进行分析，以便进行根本分析和增加信任。然而，由于异常检测的无supervised性，创建可解释的工具是困难的。本工作提出了EIF+，一个优化的扩展隔离林（EIF）的变体，旨在增强其一般化能力。此外，我们还提出了ExIFFI，一个新的方法，它将扩展隔离林与可解释特性结合起来。ExIFFI在实验中与其他隔离基于方法进行比较分析，包括 sintetic 和实际数据评估，以显示ExIFFI在提供解释方面的效果。我们还证明了ExIFFI可以作为无supervised设定下的特性选择技术。为便进一步研究和重现，我们还提供了开源代码，以便重现结果。
</details></li>
</ul>
<hr>
<h2 id="Temporal-Convolutional-Explorer-Helps-Understand-1D-CNN’s-Learning-Behavior-in-Time-Series-Classification-from-Frequency-Domain"><a href="#Temporal-Convolutional-Explorer-Helps-Understand-1D-CNN’s-Learning-Behavior-in-Time-Series-Classification-from-Frequency-Domain" class="headerlink" title="Temporal Convolutional Explorer Helps Understand 1D-CNN’s Learning Behavior in Time Series Classification from Frequency Domain"></a>Temporal Convolutional Explorer Helps Understand 1D-CNN’s Learning Behavior in Time Series Classification from Frequency Domain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05467">http://arxiv.org/abs/2310.05467</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jrzhang33/tce">https://github.com/jrzhang33/tce</a></li>
<li>paper_authors: Junru Zhang, Lang Feng, Yang He, Yuhan Wu, Yabo Dong</li>
<li>for: 提高一维卷积神经网络（1D-CNN）在时间序列分类任务中的表现，并解释它们在应用中的不desirable outcome。</li>
<li>methods: 提出了一种Temporal Convolutional Explorer（TCE）来从频谱角度 empirically explore 1D-CNN 的学习行为。</li>
<li>results: 通过对 widely-used UCR、UEA 和 UCI 测试集进行了广泛的实验，显示了以下三点：1) TCE 对 1D-CNN 的学习行为提供了深入的理解; 2) 我们的 regulatory framework 可以在现有的 1D-CNN 中实现更好的表现，具有更少的存储和计算开销。<details>
<summary>Abstract</summary>
While one-dimensional convolutional neural networks (1D-CNNs) have been empirically proven effective in time series classification tasks, we find that there remain undesirable outcomes that could arise in their application, motivating us to further investigate and understand their underlying mechanisms. In this work, we propose a Temporal Convolutional Explorer (TCE) to empirically explore the learning behavior of 1D-CNNs from the perspective of the frequency domain. Our TCE analysis highlights that deeper 1D-CNNs tend to distract the focus from the low-frequency components leading to the accuracy degradation phenomenon, and the disturbing convolution is the driving factor. Then, we leverage our findings to the practical application and propose a regulatory framework, which can easily be integrated into existing 1D-CNNs. It aims to rectify the suboptimal learning behavior by enabling the network to selectively bypass the specified disturbing convolutions. Finally, through comprehensive experiments on widely-used UCR, UEA, and UCI benchmarks, we demonstrate that 1) TCE's insight into 1D-CNN's learning behavior; 2) our regulatory framework enables state-of-the-art 1D-CNNs to get improved performances with less consumption of memory and computational overhead.
</details>
<details>
<summary>摘要</summary>
一维数据列表（1D-CNN）已经在时间序列分类任务中被证明有效，但我们发现在其应用中可能出现不жела的结果，这使我们更深入研究和理解它们的下面机制。在这种工作中，我们提出了时间卷积探索器（TCE）来从频谱频率角度 empirically 探索 1D-CNN 的学习行为。我们的 TCE 分析表明，深度 1D-CNN 会干扰低频组件，导致精度下降现象，并且干扰卷积是驱动因素。然后，我们利用我们的发现来实际应用中，并提出了一种监管框架，可以轻松地integrated into  existing 1D-CNNs。它的目的是通过选择ively bypass  specify 干扰卷积来纠正不佳的学习行为，从而提高 state-of-the-art 1D-CNNs 的性能，同时减少内存和计算负担。最后，通过对 UCR、UEA 和 UCI 测试集进行了广泛的实验，我们证明了以下两点：1) TCE 对 1D-CNN 的学习行为提供了深入的理解; 2) 我们的监管框架可以使 state-of-the-art 1D-CNNs 获得更好的性能，同时减少内存和计算负担。
</details></li>
</ul>
<hr>
<h2 id="Reward-Consistent-Dynamics-Models-are-Strongly-Generalizable-for-Offline-Reinforcement-Learning"><a href="#Reward-Consistent-Dynamics-Models-are-Strongly-Generalizable-for-Offline-Reinforcement-Learning" class="headerlink" title="Reward-Consistent Dynamics Models are Strongly Generalizable for Offline Reinforcement Learning"></a>Reward-Consistent Dynamics Models are Strongly Generalizable for Offline Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05422">http://arxiv.org/abs/2310.05422</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sw-packages/a498e1142fb23106c12b054225864aab1156087a5ab634a1d88227024ecb1626">https://github.com/sw-packages/a498e1142fb23106c12b054225864aab1156087a5ab634a1d88227024ecb1626</a></li>
<li>paper_authors: Fan-Ming Luo, Tian Xu, Xingchen Cao, Yang Yu<br>for:* 这种研究旨在提高offline reinforcement learning的精度和可行性。methods:* 研究人员提出了一种名为”动力奖励”的隐藏因素，它在不同的过程中保持一致，从而提高了模型的泛化能力。results:* 在synthetic任务上，MOREC具有强大的泛化能力，可以 surprisngly回归一些远见过程。* 在21个offline任务上，MOREC超越了之前的最佳性能，升幅分别为4.6%和25.9%。* MOREC是第一种可以在6个D4RL任务和3个NeoRL任务中达到95%以上在线RL性能的方法。<details>
<summary>Abstract</summary>
Learning a precise dynamics model can be crucial for offline reinforcement learning, which, unfortunately, has been found to be quite challenging. Dynamics models that are learned by fitting historical transitions often struggle to generalize to unseen transitions. In this study, we identify a hidden but pivotal factor termed dynamics reward that remains consistent across transitions, offering a pathway to better generalization. Therefore, we propose the idea of reward-consistent dynamics models: any trajectory generated by the dynamics model should maximize the dynamics reward derived from the data. We implement this idea as the MOREC (Model-based Offline reinforcement learning with Reward Consistency) method, which can be seamlessly integrated into previous offline model-based reinforcement learning (MBRL) methods. MOREC learns a generalizable dynamics reward function from offline data, which is subsequently employed as a transition filter in any offline MBRL method: when generating transitions, the dynamics model generates a batch of transitions and selects the one with the highest dynamics reward value. On a synthetic task, we visualize that MOREC has a strong generalization ability and can surprisingly recover some distant unseen transitions. On 21 offline tasks in D4RL and NeoRL benchmarks, MOREC improves the previous state-of-the-art performance by a significant margin, i.e., 4.6% on D4RL tasks and 25.9% on NeoRL tasks. Notably, MOREC is the first method that can achieve above 95% online RL performance in 6 out of 12 D4RL tasks and 3 out of 9 NeoRL tasks.
</details>
<details>
<summary>摘要</summary>
学习准确的动力学模型可能是关键的，尤其是在线上学习中。然而，很遗憾的是，通过历史转移来学习的动力学模型往往难以泛化到未经看过的转移。在这项研究中，我们发现了一个隐藏的但是重要的因素，即动力奖励（dynamics reward），该因素在转移中保持一致。因此，我们提出了奖励一致的动力学模型（MOREC），即任何由动力学模型生成的转移都应该 Maximize the dynamics reward derived from the data。我们实现了这个想法，并将其与前期的Offline Model-based Reinforcement Learning（MBRL）方法相结合。MOREC可以从历史数据中学习一个通用的动力奖励函数，然后将其用作历史数据中的转移筛选器。当生成转移时，动力模型会生成一批转移，并选择具有最高动力奖励值的转移。在一个 synthetic task 上，我们可见地发现，MOREC具有强大的泛化能力，可以 surprisingly 回归一些远程未经看过的转移。在 D4RL 和 NeoRL benchmark 上的 21 个 Offline task 上，MOREC 提高了之前的状态核心性能，即 4.6% 在 D4RL 任务上和 25.9% 在 NeoRL 任务上。特别是，MOREC 是第一个可以达到上述 95% 在线RL 性能的 6 个 D4RL 任务和 3 个 NeoRL 任务。
</details></li>
</ul>
<hr>
<h2 id="On-sparse-regression-Lp-regularization-and-automated-model-discovery"><a href="#On-sparse-regression-Lp-regularization-and-automated-model-discovery" class="headerlink" title="On sparse regression, Lp-regularization, and automated model discovery"></a>On sparse regression, Lp-regularization, and automated model discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06872">http://arxiv.org/abs/2310.06872</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jeremy A. McCulloch, Skyler R. St. Pierre, Kevin Linka, Ellen Kuhl</li>
<li>For:  automatic model discovery and induce sparsity in nonlinear regression for material modeling* Methods:  hybrid approach combining regularization and physical constraints, Lp regularization, constitutive neural networks, L2, L1, L0 regularization* Results:  discovery of interpretable models and physically meaningful parameters, demonstration of Lp regularized constitutive neural networks’ ability to simultaneously discover both interpretability and predictability, and potential applications in generative material design and discovery of new materials with user-defined properties.<details>
<summary>Abstract</summary>
Sparse regression and feature extraction are the cornerstones of knowledge discovery from massive data. Their goal is to discover interpretable and predictive models that provide simple relationships among scientific variables. While the statistical tools for model discovery are well established in the context of linear regression, their generalization to nonlinear regression in material modeling is highly problem-specific and insufficiently understood. Here we explore the potential of neural networks for automatic model discovery and induce sparsity by a hybrid approach that combines two strategies: regularization and physical constraints. We integrate the concept of Lp regularization for subset selection with constitutive neural networks that leverage our domain knowledge in kinematics and thermodynamics. We train our networks with both, synthetic and real data, and perform several thousand discovery runs to infer common guidelines and trends: L2 regularization or ridge regression is unsuitable for model discovery; L1 regularization or lasso promotes sparsity, but induces strong bias; only L0 regularization allows us to transparently fine-tune the trade-off between interpretability and predictability, simplicity and accuracy, and bias and variance. With these insights, we demonstrate that Lp regularized constitutive neural networks can simultaneously discover both, interpretable models and physically meaningful parameters. We anticipate that our findings will generalize to alternative discovery techniques such as sparse and symbolic regression, and to other domains such as biology, chemistry, or medicine. Our ability to automatically discover material models from data could have tremendous applications in generative material design and open new opportunities to manipulate matter, alter properties of existing materials, and discover new materials with user-defined properties.
</details>
<details>
<summary>摘要</summary>
匿密回归和特征提取是知识发现大数据的基石。它们的目标是从科学变量之间找到可解释性强的预测模型，提供简单的关系。虽然 Linear 回归的统计工具已经在 context 中得到了良好的定制，但在非线性回归方面，它们在材料模型中的普遍性和不够了解。我们在这里探索使用神经网络自动发现模型的潜力，并通过混合两种策略来实现匿密性：规则化和物理约束。我们将 Lp 规则化用于子集选择与物理神经网络结合，并将其训练于both synthetic 和实际数据。我们进行了数千次发现运行，以推导出一些常见的指南和趋势：L2 规则化或ridge regression 不适合模型发现; L1 规则化或lasso 会导致匿密性，但会带来强烈的偏见;只有 L0 规则化可以透明地调整 interpretability 和预测性、简单性和准确性、偏见和偏差的负荷。通过这些发现，我们证明了 Lp 规则化的 constitutive 神经网络可以同时发现可解释性模型和物理意义的参数。我们预计这些发现将普遍到其他发现技术，如稀疏和符号回归，并在生物、化学、医学等领域得到应用。我们的自动发现材料模型技术可能会在生成材料设计中具有巨大的应用，开启新的材料性能控制和物质性能改变的可能性，以及发现新的材料。
</details></li>
</ul>
<hr>
<h2 id="Entropy-MCMC-Sampling-from-Flat-Basins-with-Ease"><a href="#Entropy-MCMC-Sampling-from-Flat-Basins-with-Ease" class="headerlink" title="Entropy-MCMC: Sampling from Flat Basins with Ease"></a>Entropy-MCMC: Sampling from Flat Basins with Ease</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05401">http://arxiv.org/abs/2310.05401</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bolian Li, Ruqi Zhang</li>
<li>for: 这个论文的目的是提出一种偏置采样方法，以优化深度学习模型的 posterior 采样。</li>
<li>methods: 该方法基于一个辅助变量，使 MCMC 采样器偏向平坦区域，从而提高采样效率和准确性。</li>
<li>results: 实验结果表明，该方法可以成功采样到深度学习模型的平坦区域，并在多个 bencmarks 上表现出色，包括分类、准确性和异常检测等。<details>
<summary>Abstract</summary>
Bayesian deep learning counts on the quality of posterior distribution estimation. However, the posterior of deep neural networks is highly multi-modal in nature, with local modes exhibiting varying generalization performance. Given a practical budget, sampling from the original posterior can lead to suboptimal performance, as some samples may become trapped in "bad" modes and suffer from overfitting. Leveraging the observation that "good" modes with low generalization error often reside in flat basins of the energy landscape, we propose to bias sampling on the posterior toward these flat regions. Specifically, we introduce an auxiliary guiding variable, the stationary distribution of which resembles a smoothed posterior free from sharp modes, to lead the MCMC sampler to flat basins. By integrating this guiding variable with the model parameter, we create a simple joint distribution that enables efficient sampling with minimal computational overhead. We prove the convergence of our method and further show that it converges faster than several existing flatness-aware methods in the strongly convex setting. Empirical results demonstrate that our method can successfully sample from flat basins of the posterior, and outperforms all compared baselines on multiple benchmarks including classification, calibration, and out-of-distribution detection.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Find-Your-Optimal-Assignments-On-the-fly-A-Holistic-Framework-for-Clustered-Federated-Learning"><a href="#Find-Your-Optimal-Assignments-On-the-fly-A-Holistic-Framework-for-Clustered-Federated-Learning" class="headerlink" title="Find Your Optimal Assignments On-the-fly: A Holistic Framework for Clustered Federated Learning"></a>Find Your Optimal Assignments On-the-fly: A Holistic Framework for Clustered Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05397">http://arxiv.org/abs/2310.05397</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongxin Guo, Xiaoying Tang, Tao Lin</li>
<li>for: 这个论文旨在探讨现有的分布式机器学习方法中，如何处理客户端数据不同性，以提高模型在所有客户端上的表现。</li>
<li>methods: 该论文使用了聚类技术来解决客户端数据不同性的问题，并提出了一种四层框架，称为HCFL，以涵盖和扩展现有的方法。</li>
<li>results: 该论文通过广泛的数值评估表明，使用提出的聚类方法可以提高模型在客户端数据不同性下的表现，并且提出了进一步改进的聚类方法。<details>
<summary>Abstract</summary>
Federated Learning (FL) is an emerging distributed machine learning approach that preserves client privacy by storing data on edge devices. However, data heterogeneity among clients presents challenges in training models that perform well on all local distributions. Recent studies have proposed clustering as a solution to tackle client heterogeneity in FL by grouping clients with distribution shifts into different clusters. However, the diverse learning frameworks used in current clustered FL methods make it challenging to integrate various clustered FL methods, gather their benefits, and make further improvements.   To this end, this paper presents a comprehensive investigation into current clustered FL methods and proposes a four-tier framework, namely HCFL, to encompass and extend existing approaches. Based on the HCFL, we identify the remaining challenges associated with current clustering methods in each tier and propose an enhanced clustering method called HCFL+ to address these challenges. Through extensive numerical evaluations, we showcase the effectiveness of our clustering framework and the improved components. Our code will be publicly available.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 是一种emerging distributed machine learningapproach that preserves client privacy by storing data on edge devices. However, data heterogeneity among clients presents challenges in training models that perform well on all local distributions. Recent studies have proposed clustering as a solution to tackle client heterogeneity in FL by grouping clients with distribution shifts into different clusters. However, the diverse learning frameworks used in current clustered FL methods make it challenging to integrate various clustered FL methods, gather their benefits, and make further improvements.   To this end, this paper presents a comprehensive investigation into current clustered FL methods and proposes a four-tier framework, namely HCFL, to encompass and extend existing approaches. Based on the HCFL, we identify the remaining challenges associated with current clustering methods in each tier and propose an enhanced clustering method called HCFL+ to address these challenges. Through extensive numerical evaluations, we showcase the effectiveness of our clustering framework and the improved components. Our code will be publicly available.Note: Please note that the translation is in Simplified Chinese, which is one of the two standard forms of Chinese writing. The other form is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Robust-Image-Watermarking-based-on-Cross-Attention-and-Invariant-Domain-Learning"><a href="#Robust-Image-Watermarking-based-on-Cross-Attention-and-Invariant-Domain-Learning" class="headerlink" title="Robust Image Watermarking based on Cross-Attention and Invariant Domain Learning"></a>Robust Image Watermarking based on Cross-Attention and Invariant Domain Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05395">http://arxiv.org/abs/2310.05395</a></li>
<li>repo_url: None</li>
<li>paper_authors: Agnibh Dasgupta, Xin Zhong</li>
<li>for: 这 paper 是为了提出一种robust image watermarking方法，用于嵌入和提取 watermark within a cover image，并且使用深度学习approaches增强总结和鲁棒性。</li>
<li>methods: 这 paper 使用了 convolution 和 concatenation 来实现 watermark embedding，同时也integrate了可能的 augmentation 进行训练。</li>
<li>results: 这 paper 提出了 two novel 和 significannot advancements：first, 使用 multi-head cross attention mechanism 来实现 watermark embedding，以便在 cover image 和 watermark之间进行信息交换，并且identify semantically suitable embedding locations。second, 提出了 learning an invariant domain representation 来捕捉 both semantic 和 noise-invariant information concerning the watermark，这对于提高 image watermarking technique 是非常有价值的。<details>
<summary>Abstract</summary>
Image watermarking involves embedding and extracting watermarks within a cover image, with deep learning approaches emerging to bolster generalization and robustness. Predominantly, current methods employ convolution and concatenation for watermark embedding, while also integrating conceivable augmentation in the training process. This paper explores a robust image watermarking methodology by harnessing cross-attention and invariant domain learning, marking two novel, significant advancements. First, we design a watermark embedding technique utilizing a multi-head cross attention mechanism, enabling information exchange between the cover image and watermark to identify semantically suitable embedding locations. Second, we advocate for learning an invariant domain representation that encapsulates both semantic and noise-invariant information concerning the watermark, shedding light on promising avenues for enhancing image watermarking techniques.
</details>
<details>
<summary>摘要</summary>
Image watermarking 图像水印技术 involves embedding and extracting watermarks within a cover image, with deep learning approaches emerging to enhance generalization and robustness. Predominantly, current methods use convolution and concatenation for watermark embedding, while also incorporating possible augmentation in the training process. This paper explores a robust image watermarking methodology by harnessing cross-attention and invariant domain learning, introducing two novel, significant advancements. First, we design a watermark embedding technique utilizing a multi-head cross attention mechanism, enabling information exchange between the cover image and watermark to identify semantically suitable embedding locations. Second, we advocate for learning an invariant domain representation that encapsulates both semantic and noise-invariant information concerning the watermark, shedding light on promising avenues for enhancing image watermarking techniques.Here's the translation breakdown:Image watermarking 图像水印技术 (watermarking technique)involves embedding and extracting watermarks within a cover image, 图像 (cover image)with deep learning approaches emerging to enhance generalization and robustness. 使用深度学习方法提高泛化和鲁棒性。Predominantly, current methods use convolution and concatenation for watermark embedding, 当今主要方法使用卷积和 concatenation 进行水印嵌入。while also incorporating possible augmentation in the training process. 同时在训练过程中也包含可能的增强。This paper explores a robust image watermarking methodology by harnessing cross-attention and invariant domain learning, 本文探讨了一种基于对比注意力和不变域学习的图像水印方法。marking two novel, significant advancements. 标志着两个新、重要的进步。First, we design a watermark embedding technique utilizing a multi-head cross attention mechanism, 首先，我们设计了一种基于多头对比注意力机制的水印嵌入技术。enabling information exchange between the cover image and watermark to identify semantically suitable embedding locations. 使得图像和水印之间进行信息交换，以便在意义上适当的嵌入位置。Second, we advocate for learning an invariant domain representation that encapsulates both semantic and noise-invariant information concerning the watermark, 第二，我们提倡学习一种不变域表示，包含水印中的 semantic 和噪音不变信息。shedding light on promising avenues for enhancing image watermarking techniques. 探讨了图像水印技术的可能的提高方向。
</details></li>
</ul>
<hr>
<h2 id="Equation-Discovery-with-Bayesian-Spike-and-Slab-Priors-and-Efficient-Kernels"><a href="#Equation-Discovery-with-Bayesian-Spike-and-Slab-Priors-and-Efficient-Kernels" class="headerlink" title="Equation Discovery with Bayesian Spike-and-Slab Priors and Efficient Kernels"></a>Equation Discovery with Bayesian Spike-and-Slab Priors and Efficient Kernels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05387">http://arxiv.org/abs/2310.05387</a></li>
<li>repo_url: None</li>
<li>paper_authors: Da Long, Wei W. Xing, Aditi S. Krishnapriyan, Robert M. Kirby, Shandian Zhe, Michael W. Mahoney</li>
<li>For: The paper is written for discovering governing equations from data, which is important in many scientific and engineering applications.* Methods: The paper proposes a novel equation discovery method based on Kernel learning and Bayesian Spike-and-Slab priors (KBASS), which combines kernel regression with a Bayesian spike-and-slab prior for effective operator selection and uncertainty quantification.* Results: The paper shows the significant advantages of KBASS on a list of benchmark ODE and PDE discovery tasks, demonstrating its ability to overcome data sparsity and noise issues, as well as provide uncertainty quantification.Here’s the simplified Chinese text for the three key points:* For: 这篇论文是为了发现数据中的权导方程，这对科学和工程应用来说非常重要。* Methods: 这篇论文提出了一种基于kernel学习和抽象积分架的方法（KBASS），它将kernel regression与抽象积分架相结合，以实现有效的运算选择和uncertainty评估。* Results: 论文在一系列的benchmark ODE和PDE发现任务上显示了KBASS的显著优势，证明了它在数据稀缺和噪声问题上的可行性，并且可以提供uncertainty评估。<details>
<summary>Abstract</summary>
Discovering governing equations from data is important to many scientific and engineering applications. Despite promising successes, existing methods are still challenged by data sparsity as well as noise issues, both of which are ubiquitous in practice. Moreover, state-of-the-art methods lack uncertainty quantification and/or are costly in training. To overcome these limitations, we propose a novel equation discovery method based on Kernel learning and BAyesian Spike-and-Slab priors (KBASS). We use kernel regression to estimate the target function, which is flexible, expressive, and more robust to data sparsity and noises. We combine it with a Bayesian spike-and-slab prior -- an ideal Bayesian sparse distribution -- for effective operator selection and uncertainty quantification. We develop an expectation propagation expectation-maximization (EP-EM) algorithm for efficient posterior inference and function estimation. To overcome the computational challenge of kernel regression, we place the function values on a mesh and induce a Kronecker product construction, and we use tensor algebra methods to enable efficient computation and optimization. We show the significant advantages of KBASS on a list of benchmark ODE and PDE discovery tasks.
</details>
<details>
<summary>摘要</summary>
发现管理方程式从数据中是科学和工程应用中非常重要的。虽然现有方法已经取得了很大的成功，但是它们仍然面临着数据稀缺和噪声问题，这两个问题在实践中却非常普遍。此外，现有的方法缺乏uncertainty量化和/或训练成本高。为了解决这些限制，我们提出了一种基于kernel学习和权重积分干扰（KBASS）的方程发现方法。我们使用kernel回归来估计目标函数，这种方法非常灵活、表达力强和更加抗抗数据稀缺和噪声。我们将其与一种bayesian积分干扰（BAYESIAN SPIKE-AND-SLAB）的干扰分布结合起来，以实现有效的运算选择和uncertainty量化。我们开发了一种期望传播期望最大化（EP-EM）算法，以便高效地进行 posterior推理和函数估计。为了解决kernel回归的计算挑战，我们将函数值放在一个网格上，并使用kronecker产品结构，以及tensor代数方法来实现高效的计算和优化。我们在一系列的benchmark ODE和PDE发现任务上展示了KBASS的显著优势。
</details></li>
</ul>
<hr>
<h2 id="Augmented-Embeddings-for-Custom-Retrievals"><a href="#Augmented-Embeddings-for-Custom-Retrievals" class="headerlink" title="Augmented Embeddings for Custom Retrievals"></a>Augmented Embeddings for Custom Retrievals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05380">http://arxiv.org/abs/2310.05380</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anirudh Khatry, Yasharth Bajpai, Priyanshu Gupta, Sumit Gulwani, Ashish Tiwari</li>
<li>For: 这个论文主要针对的是如何使用 dense retrieval 技术来提高异类、严格的检索效果，以满足现代大语言模型（LLM）的推荐任务。* Methods: 该论文提出了一种名为 Adapted Dense Retrieval 的机制，它可以将预训练的卷积扩展学习到特定任务中，以提高异类、严格的检索效果。* Results: 论文通过实验证明，Adapted Dense Retrieval  Mechanism可以与现有的基于预训练矩阵的基线方法相比，在异类、严格的检索任务中提高检索效果。<details>
<summary>Abstract</summary>
Information retrieval involves selecting artifacts from a corpus that are most relevant to a given search query. The flavor of retrieval typically used in classical applications can be termed as homogeneous and relaxed, where queries and corpus elements are both natural language (NL) utterances (homogeneous) and the goal is to pick most relevant elements from the corpus in the Top-K, where K is large, such as 10, 25, 50 or even 100 (relaxed). Recently, retrieval is being used extensively in preparing prompts for large language models (LLMs) to enable LLMs to perform targeted tasks. These new applications of retrieval are often heterogeneous and strict -- the queries and the corpus contain different kinds of entities, such as NL and code, and there is a need for improving retrieval at Top-K for small values of K, such as K=1 or 3 or 5. Current dense retrieval techniques based on pretrained embeddings provide a general-purpose and powerful approach for retrieval, but they are oblivious to task-specific notions of similarity of heterogeneous artifacts. We introduce Adapted Dense Retrieval, a mechanism to transform embeddings to enable improved task-specific, heterogeneous and strict retrieval. Adapted Dense Retrieval works by learning a low-rank residual adaptation of the pretrained black-box embedding. We empirically validate our approach by showing improvements over the state-of-the-art general-purpose embeddings-based baseline.
</details>
<details>
<summary>摘要</summary>
信息检索通常包括从质量很高的文档库中选择最相关的元素，以满足给定的搜索查询。经典应用中的检索通常采用同质和松散的方式，其中查询和文档元素都是自然语言（NL）句子（同质），并且目标是从文档库中选择最相关的元素，其中K是大的，例如10、25、50或甚至100（松散）。在最近几年，检索已经在准备提示 для大型自然语言模型（LLM）中得到广泛的应用。这些新的应用程序通常是不同类型的Entity的混合和严格的，查询和文档元素不同，需要改进Top-K中的检索。当前的某些检索技术基于预训练的嵌入可以提供一种通用和强大的方法，但它们对特定任务的相似性无法考虑不同类型的文件。我们介绍了适应的检索，一种将嵌入转换成以便改进特定任务、不同类型的文件和严格的检索。适应的检索通过学习一个低级别的剩余适应来实现。我们通过对比与现有的通用嵌入基eline来验证我们的方法。
</details></li>
</ul>
<hr>
<h2 id="Scaling-Studies-for-Efficient-Parameter-Search-and-Parallelism-for-Large-Language-Model-Pre-training"><a href="#Scaling-Studies-for-Efficient-Parameter-Search-and-Parallelism-for-Large-Language-Model-Pre-training" class="headerlink" title="Scaling Studies for Efficient Parameter Search and Parallelism for Large Language Model Pre-training"></a>Scaling Studies for Efficient Parameter Search and Parallelism for Large Language Model Pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05350">http://arxiv.org/abs/2310.05350</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Benington, Leo Phan, Chris Pierre Paul, Evan Shoemaker, Priyanka Ranade, Torstein Collett, Grant Hodgson Perez, Christopher Krieger</li>
<li>for: 这个论文主要针对AI加速器处理能力和内存限制的问题，旨在探讨如何在可接受时间内执行机器学习任务（如训练和推理）。</li>
<li>methods: 这篇论文使用了分布式算法和电路优化技术来进行多节点环境中的模型扩展，提高模型训练和预处理的效率，并尝试将更多参数存储在有限的资源中。</li>
<li>results: 研究项目中对5个encoder-decoder LLMS进行了并行和分布式机器学习算法开发，并进行了细化的研究以量化三种ML并行方法（包括Microsoft DeepSpeed Zero Redundancy Optimizer（ZeRO）阶段）的关系。<details>
<summary>Abstract</summary>
AI accelerator processing capabilities and memory constraints largely dictate the scale in which machine learning workloads (e.g., training and inference) can be executed within a desirable time frame. Training a state of the art, transformer-based model today requires use of GPU-accelerated high performance computers with high-speed interconnects. As datasets and models continue to increase in size, computational requirements and memory demands for AI also continue to grow. These challenges have inspired the development of distributed algorithm and circuit-based optimization techniques that enable the ability to progressively scale models in multi-node environments, efficiently minimize neural network cost functions for faster convergence, and store more parameters into a set number of available resources. In our research project, we focus on parallel and distributed machine learning algorithm development, specifically for optimizing the data processing and pre-training of a set of 5 encoder-decoder LLMs, ranging from 580 million parameters to 13 billion parameters. We performed a fine-grained study to quantify the relationships between three ML parallelism methods, specifically exploring Microsoft DeepSpeed Zero Redundancy Optimizer (ZeRO) stages.
</details>
<details>
<summary>摘要</summary>
人工智能加速器处理能力和内存限制 largely dictate 机器学习任务（例如训练和推理）可以在理想时间内执行的规模。今天，使用 GPU 加速的高性能计算机和高速 интер连接来训练现代变换器基于模型。随着数据集和模型的大小不断增长，人工智能的计算要求和内存需求也在不断增长。这些挑战激发了分布式算法和绕组件优化技术的开发，以实现在多节点环境中逐渐扩大模型，高效地减少神经网络成本函数，并将更多参数存储在可用资源中。在我们的研究项目中，我们专注于并行和分布式机器学习算法开发，具体来说是优化数据处理和前期训练5个Encoder-Decoder LLMS的集合，该集合包括580亿参数到1300亿参数。我们进行了细化的研究，以量化三种机器学习并行方法之间的关系，具体来说是Microsoft DeepSpeed Zero Redundancy Optimizer（ZeRO）阶段。
</details></li>
</ul>
<hr>
<h2 id="DiffCPS-Diffusion-Model-based-Constrained-Policy-Search-for-Offline-Reinforcement-Learning"><a href="#DiffCPS-Diffusion-Model-based-Constrained-Policy-Search-for-Offline-Reinforcement-Learning" class="headerlink" title="DiffCPS: Diffusion Model based Constrained Policy Search for Offline Reinforcement Learning"></a>DiffCPS: Diffusion Model based Constrained Policy Search for Offline Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05333">http://arxiv.org/abs/2310.05333</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/felix-thu/DiffCPS">https://github.com/felix-thu/DiffCPS</a></li>
<li>paper_authors: Longxiang He, Linrui Zhang, Junbo Tan, Xueqian Wang</li>
<li>For: 解决 offline 强化学习中的受限策略搜索问题，提出一种基于Diffusion模型的受限策略搜索方法（DiffCPS），以高度表达能力替代先前的AWR方法。* Methods: 利用Diffusion模型的动作分布来消除受限策略搜索中的策略分布约束，然后使用Diffusion模型中的证据下界（ELBO）来近似KL约束。* Results: 在D4RL数据集上进行了广泛的实验，证明DiffCPS可以 дости得更好或至少相当于传统AWR基eline以及近期的Diffusion模型基eline。代码可以在 $\href{<a target="_blank" rel="noopener" href="https://github.com/felix-thu/DiffCPS%7D%7Bhttps://github.com/felix-thu/DiffCPS%7D$">https://github.com/felix-thu/DiffCPS}{https://github.com/felix-thu/DiffCPS}$</a> 上获取。<details>
<summary>Abstract</summary>
Constrained policy search (CPS) is a fundamental problem in offline reinforcement learning, which is generally solved by advantage weighted regression (AWR). However, previous methods may still encounter out-of-distribution actions due to the limited expressivity of Gaussian-based policies. On the other hand, directly applying the state-of-the-art models with distribution expression capabilities (i.e., diffusion models) in the AWR framework is insufficient since AWR requires exact policy probability densities, which is intractable in diffusion models. In this paper, we propose a novel approach called $\textbf{Diffusion Model based Constrained Policy Search (DiffCPS)}$, which tackles the diffusion-based constrained policy search without resorting to AWR. The theoretical analysis reveals our key insights by leveraging the action distribution of the diffusion model to eliminate the policy distribution constraint in the CPS and then utilizing the Evidence Lower Bound (ELBO) of diffusion-based policy to approximate the KL constraint. Consequently, DiffCPS admits the high expressivity of diffusion models while circumventing the cumbersome density calculation brought by AWR. Extensive experimental results based on the D4RL benchmark demonstrate the efficacy of our approach. We empirically show that DiffCPS achieves better or at least competitive performance compared to traditional AWR-based baselines as well as recent diffusion-based offline RL methods. The code is now available at $\href{https://github.com/felix-thu/DiffCPS}{https://github.com/felix-thu/DiffCPS}$.
</details>
<details>
<summary>摘要</summary>
“干预策搜索”（Constrained Policy Search，简称CPS）是机器学习中的基本问题，通常通过优先预测（Advantage Weighted Regression，简称AWR）解决。然而，先前的方法可能仍会遇到对不同的动作的不合理的行为，因为运用 Gaussian-based 政策的有限表达能力。另一方面，直接将现场的先进模型（i.e., 传播模型）应用在 AWR 框架中是不足的，因为 AWR 需要精确的政策概率密度，传播模型中的概率密度是无法求解的。在本文中，我们提出了一个新的方法，called “传播模型基于的干预策搜索”（Diffusion Model based Constrained Policy Search，简称DiffCPS）。我们的研究表明，DiffCPS 可以在干预策搜索中消除政策概率密度的限制，并且使用传播模型中的动作分布来估计 KL 函数。因此，DiffCPS 可以充分利用传播模型的表达能力，而不需要耗费时间 Calculate 政策概率密度。我们的实验结果显示，DiffCPS 可以对 D4RL benchmark 进行了广泛的测试，并且与传统 AWR 基础的基elines 和最近的传播模型基础的 offline RL 方法相比，获得了更好的性能。我们的代码现在可以在 $\href{https://github.com/felix-thu/DiffCPS}{https://github.com/felix-thu/DiffCPS}$ 上获取。”
</details></li>
</ul>
<hr>
<h2 id="Unlearning-with-Fisher-Masking"><a href="#Unlearning-with-Fisher-Masking" class="headerlink" title="Unlearning with Fisher Masking"></a>Unlearning with Fisher Masking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05331">http://arxiv.org/abs/2310.05331</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shivank21/Unlearning-with-Fisher-Masking">https://github.com/shivank21/Unlearning-with-Fisher-Masking</a></li>
<li>paper_authors: Yufang Liu, Changzhi Sun, Yuanbin Wu, Aimin Zhou</li>
<li>for:  Machine unlearning aims to revoke some training data after learning in response to requests from users, model developers, and administrators.</li>
<li>methods:  The proposed method uses a new masking strategy tailored to unlearning based on Fisher information.</li>
<li>results:  The proposed method can unlearn almost completely while maintaining most of the performance on the remain data, and exhibits stronger stability compared to other unlearning baselines.Here’s the full text in Simplified Chinese:</li>
<li>for: 机器学习推理批处理强制请求下的数据恢复</li>
<li>methods: 基于Fisher信息的新遮盖策略</li>
<li>results: 可以减少大量数据，保持大多数数据的表现，并且比其他基线方法更稳定<details>
<summary>Abstract</summary>
Machine unlearning aims to revoke some training data after learning in response to requests from users, model developers, and administrators. Most previous methods are based on direct fine-tuning, which may neither remove data completely nor retain full performances on the remain data. In this work, we find that, by first masking some important parameters before fine-tuning, the performances of unlearning could be significantly improved. We propose a new masking strategy tailored to unlearning based on Fisher information. Experiments on various datasets and network structures show the effectiveness of the method: without any fine-tuning, the proposed Fisher masking could unlearn almost completely while maintaining most of the performance on the remain data. It also exhibits stronger stability compared to other unlearning baselines
</details>
<details>
<summary>摘要</summary>
机器学习卷回目标是在学习后根据用户、模型开发者和管理员的请求，撤销一部分训练数据。现有的大多数方法都基于直接细化，这可能并不会完全 removes 数据，也不会保留剩下数据的全部性能。在这种工作中，我们发现，先对一些重要参数进行遮盖，然后进行细化，可以有效提高卷回的性能。我们提出了针对卷回的新的遮盖策略，基于信息理解。对各种数据集和网络结构进行实验，我们发现，无需任何细化，我们的提议的遮盖策略可以几乎完全卷回数据，同时保留大部分剩下数据的性能。它还比其他卷回基线强制稳定。
</details></li>
</ul>
<hr>
<h2 id="Provable-Compositional-Generalization-for-Object-Centric-Learning"><a href="#Provable-Compositional-Generalization-for-Object-Centric-Learning" class="headerlink" title="Provable Compositional Generalization for Object-Centric Learning"></a>Provable Compositional Generalization for Object-Centric Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05327">http://arxiv.org/abs/2310.05327</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thaddäus Wiedemer, Jack Brady, Alexander Panfilov, Attila Juhos, Matthias Bethge, Wieland Brendel</li>
<li>for:  bridging the gap between human and machine perception</li>
<li>methods:  learning object-centric representations, using autoencoders with structural assumptions and enforcing encoder-decoder consistency</li>
<li>results:  provable compositional generalization of object-centric representations through identifiability theory, validated through experiments on synthetic image data.Here’s the full text in Simplified Chinese:</li>
<li>for:  bridging the gap between人类和机器视觉</li>
<li>methods: 通过学习对象中心表示，使用具有结构假设的自动编码器和强制编码器-解码器一致性，实现可靠的 композиitional generalization</li>
<li>results: 通过identifiability理论，证明对象中心表示可以可靠地推广到新的组合结构，并通过synthetic图像数据实验证明了这一结论。<details>
<summary>Abstract</summary>
Learning representations that generalize to novel compositions of known concepts is crucial for bridging the gap between human and machine perception. One prominent effort is learning object-centric representations, which are widely conjectured to enable compositional generalization. Yet, it remains unclear when this conjecture will be true, as a principled theoretical or empirical understanding of compositional generalization is lacking. In this work, we investigate when compositional generalization is guaranteed for object-centric representations through the lens of identifiability theory. We show that autoencoders that satisfy structural assumptions on the decoder and enforce encoder-decoder consistency will learn object-centric representations that provably generalize compositionally. We validate our theoretical result and highlight the practical relevance of our assumptions through experiments on synthetic image data.
</details>
<details>
<summary>摘要</summary>
学习概念的总结，使机器和人类视觉之间的差异越来越小，是核心的问题。一种广泛的尝试是学习对象中心的表示，这些表示被推测可以实现 композиitional generalization。然而，是否这种推测是正确的，还没有一个明确的理论或实际理解。在这项工作中，我们通过标识理论来研究对象中心表示是否可以 garantuee compositional generalization。我们证明了满足核心假设的 autoencoder 将学习对象中心表示，并且可以确定性地推导 compositional generalization。我们验证了我们的理论结论，并通过实验 validate 我们的假设在生成的图像数据上。
</details></li>
</ul>
<hr>
<h2 id="Increasing-Entropy-to-Boost-Policy-Gradient-Performance-on-Personalization-Tasks"><a href="#Increasing-Entropy-to-Boost-Policy-Gradient-Performance-on-Personalization-Tasks" class="headerlink" title="Increasing Entropy to Boost Policy Gradient Performance on Personalization Tasks"></a>Increasing Entropy to Boost Policy Gradient Performance on Personalization Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05324">http://arxiv.org/abs/2310.05324</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/acstarnes/wain23-policy-regularization">https://github.com/acstarnes/wain23-policy-regularization</a></li>
<li>paper_authors: Andrew Starnes, Anton Dereventsov, Clayton Webster</li>
<li>For: 本研究考虑了使用强化学习agent中的政策梯度优化的迁移抑制效果。* Methods: 本文使用了不同的$\varphi$-差分和最大均值差Distance来增强Policy的优化目标函数，以促进Policy的多样性。* Results: 数值实验表明，通过使用多样性促进策略 régularization，可以提高各种个性化任务的性能，而且不会 sacrificing accuracy。<details>
<summary>Abstract</summary>
In this effort, we consider the impact of regularization on the diversity of actions taken by policies generated from reinforcement learning agents trained using a policy gradient. Policy gradient agents are prone to entropy collapse, which means certain actions are seldomly, if ever, selected. We augment the optimization objective function for the policy with terms constructed from various $\varphi$-divergences and Maximum Mean Discrepancy which encourages current policies to follow different state visitation and/or action choice distribution than previously computed policies. We provide numerical experiments using MNIST, CIFAR10, and Spotify datasets. The results demonstrate the advantage of diversity-promoting policy regularization and that its use on gradient-based approaches have significantly improved performance on a variety of personalization tasks. Furthermore, numerical evidence is given to show that policy regularization increases performance without losing accuracy.
</details>
<details>
<summary>摘要</summary>
“在这个努力中，我们考虑了规则化对Policy生成的多样性的影响。Policy梯度学习代理人容易出现Entropy塌塌，这意味着某些动作很少或者从未被选择。我们将优化优化目标函数中的Policy加入了不同的状态访问和/或动作选择分布的梯度的不同$\varphi$-多样性和Maximum Mean Discrepancy。我们在MNIST、CIFAR10和Spotify数据集上进行了数值实验，结果表明了促进多样性的策略REG regularization的优势，并且在各种个性化任务中显著提高了性能。此外，我们还提供了数值证明，证明政策REG regularization可以不失准确性地提高表现。”Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/09/cs.LG_2023_10_09/" data-id="clp89dohm00swi78814mu6cwi" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_10_09" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/09/eess.IV_2023_10_09/" class="article-date">
  <time datetime="2023-10-09T09:00:00.000Z" itemprop="datePublished">2023-10-09</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/09/eess.IV_2023_10_09/">eess.IV - 2023-10-09</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Empirical-Evaluation-of-the-Segment-Anything-Model-SAM-for-Brain-Tumor-Segmentation"><a href="#Empirical-Evaluation-of-the-Segment-Anything-Model-SAM-for-Brain-Tumor-Segmentation" class="headerlink" title="Empirical Evaluation of the Segment Anything Model (SAM) for Brain Tumor Segmentation"></a>Empirical Evaluation of the Segment Anything Model (SAM) for Brain Tumor Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06162">http://arxiv.org/abs/2310.06162</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Peivandi, Jason Zhang, Michael Lu, Dongxiao Zhu, Zhifeng Kou</li>
<li>for: 本研究旨在提高基于Segment Anything Model（SAM）的脑肿划分精度。</li>
<li>methods: 本研究使用了传输学习和Decathlon脑肿数据集来强化SAM的面掩码解码器。其中，对四维数据进行了三维封装，并使用了随机旋转和弹性变形来增加训练数据的大小。</li>
<li>results: 对比预训练的SAM和nnUNetv2，改进后的SAM在脑肿划分精度方面显示了显著提高，而nnUNetv2在总划分精度方面赢得了比较高的分数。然而，改进后的SAM在挑战性较高的案例中表现更为稳定，尤其是在 Hausdorff 距离95%的情况下。<details>
<summary>Abstract</summary>
Brain tumor segmentation presents a formidable challenge in the field of Medical Image Segmentation. While deep-learning models have been useful, human expert segmentation remains the most accurate method. The recently released Segment Anything Model (SAM) has opened up the opportunity to apply foundation models to this difficult task. However, SAM was primarily trained on diverse natural images. This makes applying SAM to biomedical segmentation, such as brain tumors with less defined boundaries, challenging. In this paper, we enhanced SAM's mask decoder using transfer learning with the Decathlon brain tumor dataset. We developed three methods to encapsulate the four-dimensional data into three dimensions for SAM. An on-the-fly data augmentation approach has been used with a combination of rotations and elastic deformations to increase the size of the training dataset. Two key metrics: the Dice Similarity Coefficient (DSC) and the Hausdorff Distance 95th Percentile (HD95), have been applied to assess the performance of our segmentation models. These metrics provided valuable insights into the quality of the segmentation results. In our evaluation, we compared this improved model to two benchmarks: the pretrained SAM and the widely used model, nnUNetv2. We find that the improved SAM shows considerable improvement over the pretrained SAM, while nnUNetv2 outperformed the improved SAM in terms of overall segmentation accuracy. Nevertheless, the improved SAM demonstrated slightly more consistent results than nnUNetv2, especially on challenging cases that can lead to larger Hausdorff distances. In the future, more advanced techniques can be applied in order to further improve the performance of SAM on brain tumor segmentation.
</details>
<details>
<summary>摘要</summary>
脑肿分割是医学图像分割领域中的一大挑战。深度学习模型已经在此领域中发挥了作用，但是人工专家分割仍然是最准确的方法。最近发布的Segment Anything Model（SAM）已经开创了应用基础模型在这个难题上的可能性。然而，SAM主要在多样的自然图像上进行训练，这使得将SAM应用于生物医学分割，如脑肿诊断，变得更加困难。在这篇论文中，我们提高了SAM的面 máscara解码器使用基于Transfer Learning的Decathlon脑肿数据集。我们开发出了三种方法来封装四维数据到三维数据中，以便在SAM上进行分割。我们采用了在线数据增强策略，结合旋转和弹性变形来增加训练集的大小。我们使用了Dice相似度系数（DSC）和 Hausdorff距离95%（HD95）两个关键指标来评估我们的分割模型的性能。这两个指标为我们提供了有价值的分割结果评估方法。在我们的评估中，我们比较了我们改进的SAM模型与预训练的SAM模型以及广泛使用的nnUNetv2模型。我们发现，改进后的SAM模型在脑肿分割任务中显著提高了性能，而nnUNetv2模型在整体分割精度方面超过了改进后的SAM模型。然而，改进后的SAM模型在挑战性较高的案例中表现更为一致，尤其是在可能导致更大的 Hausdorff 距离的情况下。未来，我们可以采用更高级的技术来进一步提高SAM模型在脑肿分割任务中的性能。
</details></li>
</ul>
<hr>
<h2 id="Dipole-Spread-Function-Engineering-for-6D-Super-Resolution-Microscopy"><a href="#Dipole-Spread-Function-Engineering-for-6D-Super-Resolution-Microscopy" class="headerlink" title="Dipole-Spread Function Engineering for 6D Super-Resolution Microscopy"></a>Dipole-Spread Function Engineering for 6D Super-Resolution Microscopy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05810">http://arxiv.org/abs/2310.05810</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tingting Wu, Matthew D. Lew</li>
<li>for: 这个论文的目的是探讨fluorescent molecules的六个维度超分辨单分子orientation-localization微scopic镜像技术（SMOLM）。</li>
<li>methods: 这篇论文详细介绍了fluorescent диполи的形成图像理论，以及如何通过相位和极化调制来改变镜像形成的dipole spread function（DSF）。它还描述了一些设计这些调制的方法，以及最新的技术，包括双螺旋、四肢、圆形和DeepSTORM3D学习点精度函数（PSF）。</li>
<li>results: 论文还详细介绍了一些实际应用，包括生物学应用，以及未来技术的发展和挑战。<details>
<summary>Abstract</summary>
Fluorescent molecules are versatile nanoscale emitters that enable detailed observations of biophysical processes with nanoscale resolution. Because they are well-approximated as electric dipoles, imaging systems can be designed to visualize their 3D positions and 3D orientations, so-called dipole-spread function (DSF) engineering, for 6D super-resolution single-molecule orientation-localization microscopy (SMOLM). We review fundamental image-formation theory for fluorescent di-poles, as well as how phase and polarization modulation can be used to change the image of a dipole emitter produced by a microscope, called its DSF. We describe several methods for designing these modulations for optimum performance, as well as compare recently developed techniques, including the double-helix, tetrapod, crescent, and DeepSTORM3D learned point-spread functions (PSFs), in addition to the tri-spot, vortex, pixOL, raPol, CHIDO, and MVR DSFs. We also cover common imaging system designs and techniques for implementing engineered DSFs. Finally, we discuss recent biological applications of 6D SMOLM and future challenges for pushing the capabilities and utility of the technology.
</details>
<details>
<summary>摘要</summary>
fluorescent分子是一种 versatile nanoscale发射器，可以允许详细地观察生物物理过程，resolution nanoscale.因为它们可以被视为电动 polarization dipole， therefore imaging system can be designed to visualize their 3D positions and 3D orientations, so-called dipole-spread function (DSF) engineering, for 6D super-resolution single-molecule orientation-localization microscopy (SMOLM).我们将评论基本的图像形成理论 для fluorescent di-poles，以及如何使用阶段和 polarization 模ulation change the image of a dipole emitter produced by a microscope, called its DSF。我们将描述一些设计这些模ulation的方法，以及最近开发的技术，包括double-helix, tetrapod, crescent, and DeepSTORM3D learned point-spread functions (PSFs), in addition to the tri-spot, vortex, pixOL, raPol, CHIDO, and MVR DSFs。我们还将讨论一些通用的 imaging system designs and techniques for implementing engineered DSFs。最后，我们将讨论最近的生物应用和未来挑战，以推动技术的能力和实用性。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Predictive-Coding-of-Intra-Prediction-Modes"><a href="#Efficient-Predictive-Coding-of-Intra-Prediction-Modes" class="headerlink" title="Efficient Predictive Coding of Intra Prediction Modes"></a>Efficient Predictive Coding of Intra Prediction Modes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05623">http://arxiv.org/abs/2310.05623</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kevin Reuzé, Wassim Hamidouche, Pierrick Philippe, Olivier Déforges</li>
<li>for: 提高HEVC标准和JEM编码器的压缩效率，特别是在Intra块的压缩中。</li>
<li>methods: 提出了一种基于Contextual information的专门编码方案，包括预测、分 grouped 和编码三个步骤，每个步骤都通过引入新元素（标签、测试和编码）进行了改进。使用遗传算法来最优化编码方案，以实现最高的编码效率。</li>
<li>results: 在HEVC标准下，我们的方法可以实现显著的比特率减少，同时保持JEM编码器的编码效率，这些结果表明了我们的方法在压缩效率方面的潜在提升。<details>
<summary>Abstract</summary>
The high efficiency video coding (HEVC) standard and the joint exploration model (JEM) codec incorporate 35 and 67 intra prediction modes (IPMs) respectively, which are essential for efficient compression of Intra coded blocks. These IPMs are transmitted to the decoder through a coding scheme. In our paper, we present an innovative approach to construct a dedicated coding scheme for IPM based on contextual information. This approach comprises three key steps: prediction, clustering, and coding, each of which has been enhanced by introducing new elements, namely, labels for prediction, tests for clustering, and codes for coding. In this context, we have proposed a method that utilizes a genetic algorithm to minimize the rate cost, aiming to derive the most efficient coding scheme while leveraging the available labels, tests, and codes. The resulting coding scheme, expressed as a binary tree, achieves the highest coding efficiency for a given level of complexity. In our experimental evaluation under the HEVC standard, we observed significant bitrate gains while maintaining coding efficiency under the JEM codec. These results demonstrate the potential of our approach to improve compression efficiency, particularly under the HEVC standard, while preserving the coding efficiency of the JEM codec.
</details>
<details>
<summary>摘要</summary>
高效视频编码（HEVC）标准和联合探索模型（JEM）编码器共有35和67内部预测模式（IPM），这些IPM是为高效压缩内部块的必需组成部分。这些IPM通过编码方案传输到解码器。在我们的论文中，我们提出了一种创新的方法，基于上下文信息来构建专门的编码方案。这种方法包括三个关键步骤：预测、聚类和编码，每一步都通过引入新的元素来增强，例如标签 для预测、测试 для聚类和编码。在这个上下文中，我们提出了一种使用遗传算法来最小化比特成本，以 derivate最高效的编码方案，同时利用可用的标签、测试和编码。结果表明，该编码方案，表示为二进制树，在给定的复杂度下实现了最高的编码效率。在我们的实验中，使用HEVC标准，我们观察到了显著的比特率减少，同时保持JEM编码器的编码效率。这些结果表明了我们的方法的潜在提高压缩效率，特别是在HEVC标准下，而且不会削弱JEM编码器的编码效率。
</details></li>
</ul>
<hr>
<h2 id="Longitudinal-Volumetric-Study-for-the-Progression-of-Alzheimer’s-Disease-from-Structural-MR-Images"><a href="#Longitudinal-Volumetric-Study-for-the-Progression-of-Alzheimer’s-Disease-from-Structural-MR-Images" class="headerlink" title="Longitudinal Volumetric Study for the Progression of Alzheimer’s Disease from Structural MR Images"></a>Longitudinal Volumetric Study for the Progression of Alzheimer’s Disease from Structural MR Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05558">http://arxiv.org/abs/2310.05558</a></li>
<li>repo_url: None</li>
<li>paper_authors: Prayas Sanyal, Srinjay Mukherjee, Arkapravo Das, Anindya Sen</li>
<li>for: This paper aims to survey imaging biomarkers corresponding to the progression of Alzheimer’s Disease (AD).</li>
<li>methods: The pipeline implemented includes modern pre-processing techniques such as spatial image registration, skull stripping, and inhomogeneity correction. The segmentation of tissue classes is done using an unsupervised learning approach based on intensity histogram information.</li>
<li>results: The study found that the structural change in the form of volumes of cerebrospinal fluid (CSF), grey matter (GM), and white matter (WM) can be used to track the progression of Alzheimer’s Disease (AD). The segmented features provide insights such as atrophy, increase or intolerable shifting of GM, WM and CSF, which can help in future research for automated analysis of Alzheimer’s detection with clinical domain explainability.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文目的是探讨阿尔茨heimer病（AD）的进行诊断标志。</li>
<li>methods: 该 pipeline 使用了现代预处理技术，包括空间尺寸调整、脑骨除除和不均衡纠正。 segmentation 使用了无监督学习方法，基于Intensity histogram信息。</li>
<li>results: 研究发现，CSF、GM和WM的体积变化可以跟踪阿尔茨heimer病（AD）的进行。 segmented 特征提供了衰竭、增加或不具适应的GM、WM和CSF的信息，可以帮助未来研究自动化阿尔茨heimer 检测，并提供临床领域可解释的解释。<details>
<summary>Abstract</summary>
Alzheimer's Disease (AD) is primarily an irreversible neurodegenerative disorder affecting millions of individuals today. The prognosis of the disease solely depends on treating symptoms as they arise and proper caregiving, as there are no current medical preventative treatments. For this purpose, early detection of the disease at its most premature state is of paramount importance. This work aims to survey imaging biomarkers corresponding to the progression of Alzheimer's Disease (AD). A longitudinal study of structural MR images was performed for given temporal test subjects selected randomly from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database. The pipeline implemented includes modern pre-processing techniques such as spatial image registration, skull stripping, and inhomogeneity correction. The temporal data across multiple visits spanning several years helped identify the structural change in the form of volumes of cerebrospinal fluid (CSF), grey matter (GM), and white matter (WM) as the patients progressed further into the disease. Tissue classes are segmented using an unsupervised learning approach using intensity histogram information. The segmented features thus extracted provide insights such as atrophy, increase or intolerable shifting of GM, WM and CSF and should help in future research for automated analysis of Alzheimer's detection with clinical domain explainability.
</details>
<details>
<summary>摘要</summary>
阿尔茨海默病 (AD) 是一种主要是不可逆的脑组织衰退病种，影响了数百万人今天。这种病的诊断和治疗几乎完全依赖于病人的症状和照顾，没有现有的医学预防性治疗。因此，早期发现病种的症状非常重要。本研究的目的是对阿尔茨海默病的发展进行快照。使用ADNI数据库中随机选择的测试对象，我们实施了一种 longitudinal 的 MR 成像数据集，并应用现代的预处理技术，包括空间尺寸对齐、脑骨剥除和不均匀性 corrections。通过多个访问的时间跨度，我们发现了病人的结构变化，包括脑液（CSF）、灰 mater（GM）和白 matter（WM）的体积。使用无监督学习方法，我们对尺寸信息进行分类，并提取了相应的特征，如衰退、灰 mater 和 WM 的增加或不可接受的移动。这些特征提供了关于阿尔茨海默病的早期诊断和自动分析的Future研究中的解释。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/09/eess.IV_2023_10_09/" data-id="clp89dook01bki788cs36g2qn" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/30/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/29/">29</a><a class="page-number" href="/page/30/">30</a><span class="page-number current">31</span><a class="page-number" href="/page/32/">32</a><a class="page-number" href="/page/33/">33</a><span class="space">&hellip;</span><a class="page-number" href="/page/97/">97</a><a class="extend next" rel="next" href="/page/32/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">128</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">66</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">81</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">140</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
