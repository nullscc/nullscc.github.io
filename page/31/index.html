
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/31/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.LG_2023_10_11" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/11/cs.LG_2023_10_11/" class="article-date">
  <time datetime="2023-10-11T10:00:00.000Z" itemprop="datePublished">2023-10-11</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/11/cs.LG_2023_10_11/">cs.LG - 2023-10-11</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Cost-Driven-Hardware-Software-Co-Optimization-of-Machine-Learning-Pipelines"><a href="#Cost-Driven-Hardware-Software-Co-Optimization-of-Machine-Learning-Pipelines" class="headerlink" title="Cost-Driven Hardware-Software Co-Optimization of Machine Learning Pipelines"></a>Cost-Driven Hardware-Software Co-Optimization of Machine Learning Pipelines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07940">http://arxiv.org/abs/2310.07940</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ravit Sharma, Wojciech Romaszkan, Feiqian Zhu, Puneet Gupta, Ankur Mehta</li>
<li>for: This paper aims to enable widely-applicable smart devices by overcoming the storage and processing requirements of deep neural networks.</li>
<li>methods: The paper explores the interactions between quantization, model scaling, and multi-modality with system components such as memory, sensors, and processors, and develops guidelines for optimal system design and model deployment for cost-constrained platforms.</li>
<li>results: The paper demonstrates an end-to-end, on-device, biometric user authentication system using a $20 ESP-EYE board.<details>
<summary>Abstract</summary>
Researchers have long touted a vision of the future enabled by a proliferation of internet-of-things devices, including smart sensors, homes, and cities. Increasingly, embedding intelligence in such devices involves the use of deep neural networks. However, their storage and processing requirements make them prohibitive for cheap, off-the-shelf platforms. Overcoming those requirements is necessary for enabling widely-applicable smart devices. While many ways of making models smaller and more efficient have been developed, there is a lack of understanding of which ones are best suited for particular scenarios. More importantly for edge platforms, those choices cannot be analyzed in isolation from cost and user experience. In this work, we holistically explore how quantization, model scaling, and multi-modality interact with system components such as memory, sensors, and processors. We perform this hardware/software co-design from the cost, latency, and user-experience perspective, and develop a set of guidelines for optimal system design and model deployment for the most cost-constrained platforms. We demonstrate our approach using an end-to-end, on-device, biometric user authentication system using a $20 ESP-EYE board.
</details>
<details>
<summary>摘要</summary>
In this work, we comprehensively explore how quantization, model scaling, and multi-modality interact with system components such as memory, sensors, and processors. We perform this hardware/software co-design from the cost, latency, and user-experience perspective, and develop a set of guidelines for optimal system design and model deployment for the most cost-constrained platforms. We demonstrate our approach using an end-to-end, on-device, biometric user authentication system using a $20 ESP-EYE board.
</details></li>
</ul>
<hr>
<h2 id="Enhanced-sampling-of-Crystal-Nucleation-with-Graph-Representation-Learnt-Variables"><a href="#Enhanced-sampling-of-Crystal-Nucleation-with-Graph-Representation-Learnt-Variables" class="headerlink" title="Enhanced sampling of Crystal Nucleation with Graph Representation Learnt Variables"></a>Enhanced sampling of Crystal Nucleation with Graph Representation Learnt Variables</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07927">http://arxiv.org/abs/2310.07927</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyue Zou, Pratyush Tiwary</li>
<li>for: 这个论文是用来描述一种基于图 neural network的学习方法，用于从实验室中观察的结晶结构中Derive low-dimensional variables。</li>
<li>methods: 这种方法使用了简单的卷积和聚合方法，并使用了自适应Encoder设置。</li>
<li>results: 该方法可以在多种铁和глицин的多态和多晶体中实现可靠的抽象和自由能计算，并且可以与实验结果相匹配。<details>
<summary>Abstract</summary>
In this study, we present a graph neural network-based learning approach using an autoencoder setup to derive low-dimensional variables from features observed in experimental crystal structures. These variables are then biased in enhanced sampling to observe state-to-state transitions and reliable thermodynamic weights. Our approach uses simple convolution and pooling methods. To verify the effectiveness of our protocol, we examined the nucleation of various allotropes and polymorphs of iron and glycine from their molten states. Our graph latent variables when biased in well-tempered metadynamics consistently show transitions between states and achieve accurate free energy calculations in agreement with experiments, both of which are indicators of dependable sampling. This underscores the strength and promise of our graph neural net variables for improved sampling. The protocol shown here should be applicable for other systems and with other sampling methods.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们提出了基于图 neural network的学习方法，使用自适应Encoder设置来 derivate低维度变量从实验室中观察的晶体结构特征。这些变量然后被偏导向增强抽样，以观察状态转移和可靠的热动力学权重。我们的方法使用简单的卷积和聚合方法。为了证明我们的协议的有效性，我们对铁和глицин的多种晶体和合金的融化过程进行了研究。我们的图秘密变量，当偏导向于well-tempered metadynamics中，一致地显示了状态之间的转移，并实现了准确的自由能计算，与实验数据一致，这都是可靠的抽样的标志。这表明了我们的图 neural net变量的优异和承诺，这种方法应该适用于其他系统和其他抽样方法。
</details></li>
</ul>
<hr>
<h2 id="First-Order-Dynamic-Optimization-for-Streaming-Convex-Costs"><a href="#First-Order-Dynamic-Optimization-for-Streaming-Convex-Costs" class="headerlink" title="First-Order Dynamic Optimization for Streaming Convex Costs"></a>First-Order Dynamic Optimization for Streaming Convex Costs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07925">http://arxiv.org/abs/2310.07925</a></li>
<li>repo_url: None</li>
<li>paper_authors: M. Rostami, H. Moradian, S. S. Kia</li>
<li>for: 该 paper 提出了一组新的优化算法，用于解决一类具有时间变化的流量成本函数的凸优化问题。</li>
<li>methods: 我们开发了一种方法来跟踪优化解的最优解，并且只使用了成本函数的首次导数，从而使得算法更加计算效率。</li>
<li>results: 我们比较了我们的算法和梯度下降算法，并证明了梯度下降算法在优化问题中不是有效的。我们还通过一些示例，如解决一个预测控制问题，来演示我们的结果。<details>
<summary>Abstract</summary>
This paper proposes a set of novel optimization algorithms for solving a class of convex optimization problems with time-varying streaming cost function. We develop an approach to track the optimal solution with a bounded error. Unlike the existing results, our algorithm is executed only by using the first-order derivatives of the cost function which makes it computationally efficient for optimization with time-varying cost function. We compare our algorithms to the gradient descent algorithm and show why gradient descent is not an effective solution for optimization problems with time-varying cost. Several examples including solving a model predictive control problem cast as a convex optimization problem with a streaming time-varying cost function demonstrate our results.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一组新的优化算法，用于解决具有时间变化流量成本函数的凸优化问题。我们开发了一种方法，以确保遵循优化解的 bounded error。与现有结果不同，我们的算法只使用成本函数的首次导数，从而使其 computationally efficient 于优化时间变化的成本函数。我们与梯度下降算法进行比较，并解释了梯度下降算法在优化时间变化的成本函数时的不足之处。这篇论文通过解决一个模型预测控制问题，具有流动时间变化的成本函数，来展示我们的结果。Note: "Simplified Chinese" is a romanization of Chinese that uses a simplified set of characters and grammar rules to represent the language. It is commonly used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="Unraveling-the-Single-Tangent-Space-Fallacy-An-Analysis-and-Clarification-for-Applying-Riemannian-Geometry-in-Robot-Learning"><a href="#Unraveling-the-Single-Tangent-Space-Fallacy-An-Analysis-and-Clarification-for-Applying-Riemannian-Geometry-in-Robot-Learning" class="headerlink" title="Unraveling the Single Tangent Space Fallacy: An Analysis and Clarification for Applying Riemannian Geometry in Robot Learning"></a>Unraveling the Single Tangent Space Fallacy: An Analysis and Clarification for Applying Riemannian Geometry in Robot Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07902">http://arxiv.org/abs/2310.07902</a></li>
<li>repo_url: None</li>
<li>paper_authors: Noémie Jaquier, Leonel Rozo, Tamim Asfour</li>
<li>for: 本研究旨在探讨在 робототехнике中广泛使用机器学习方法处理数据时，数据中的自然几何约束如何得到有效处理。</li>
<li>methods: 本文提出了一种基于偏微分几何的方法，以解决机器学习方法中数据中的几何约束问题。</li>
<li>results: 本研究发现，在使用偏微分几何时，存在一种“单 tangent 空间误差”，即仅将数据项投影到单个 tangent 空间上，然后使用存在误差的机器学习算法进行处理。这种方法的缺陷导致了机器学习模型的不准确性和不稳定性。<details>
<summary>Abstract</summary>
In the realm of robotics, numerous downstream robotics tasks leverage machine learning methods for processing, modeling, or synthesizing data. Often, this data comprises variables that inherently carry geometric constraints, such as the unit-norm condition of quaternions representing rigid-body orientations or the positive definiteness of stiffness and manipulability ellipsoids. Handling such geometric constraints effectively requires the incorporation of tools from differential geometry into the formulation of machine learning methods. In this context, Riemannian manifolds emerge as a powerful mathematical framework to handle such geometric constraints. Nevertheless, their recent adoption in robot learning has been largely characterized by a mathematically-flawed simplification, hereinafter referred to as the ``single tangent space fallacy". This approach involves merely projecting the data of interest onto a single tangent (Euclidean) space, over which an off-the-shelf learning algorithm is applied. This paper provides a theoretical elucidation of various misconceptions surrounding this approach and offers experimental evidence of its shortcomings. Finally, it presents valuable insights to promote best practices when employing Riemannian geometry within robot learning applications.
</details>
<details>
<summary>摘要</summary>
在 роботиCS中，许多下游任务利用机器学习方法处理、模型或生成数据。经常times，这些数据包括具有几何约束的变数，如体积正规的旋转矩阵或弹性和操作矩阵的正定性。有效地处理这些几何约束需要将数据转换为几何空间中的一个紧致的数据集合，并且使用几何学的工具来设计机器学习方法。在这个上下文中，里曼维 manifold emerges as a powerful mathematical framework to handle such geometric constraints.然而，在机器学习中的最近几年，这种方法的采用受到了“单点 tangent 空间误解”的影响，即将数据转换为单点 tangent 空间，然后使用对应的机器学习算法进行处理。本文将提供这种方法的理论详细阐述，以及实验证据证明其缺陷。最后，它将提供实践中的最佳做法，以便在机器学习应用中正确地使用几何学。
</details></li>
</ul>
<hr>
<h2 id="Precise-localization-within-the-GI-tract-by-combining-classification-of-CNNs-and-time-series-analysis-of-HMMs"><a href="#Precise-localization-within-the-GI-tract-by-combining-classification-of-CNNs-and-time-series-analysis-of-HMMs" class="headerlink" title="Precise localization within the GI tract by combining classification of CNNs and time-series analysis of HMMs"></a>Precise localization within the GI tract by combining classification of CNNs and time-series analysis of HMMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07895">http://arxiv.org/abs/2310.07895</a></li>
<li>repo_url: None</li>
<li>paper_authors: Julia Werner, Christoph Gerum, Moritz Reiber, Jörg Nick, Oliver Bringmann</li>
<li>for: 这个论文是为了高效地分类来自Video Capsule Endoscopy (VCE)研究中的肠胃部分图像而设计的。</li>
<li>methods: 这个论文使用了卷积神经网络（CNN）进行分类，并利用隐马尔可夫模型（HMM）的时间序列分析特性。</li>
<li>results: 该方法在里士满（RI）肠胃学数据集上达到了$98.04%$的准确率，可以准确地地标定肠胃迷你隧道中的位置，并且只需要约1M个参数，因此适用于低功耗设备。<details>
<summary>Abstract</summary>
This paper presents a method to efficiently classify the gastroenterologic section of images derived from Video Capsule Endoscopy (VCE) studies by exploring the combination of a Convolutional Neural Network (CNN) for classification with the time-series analysis properties of a Hidden Markov Model (HMM). It is demonstrated that successive time-series analysis identifies and corrects errors in the CNN output. Our approach achieves an accuracy of $98.04\%$ on the Rhode Island (RI) Gastroenterology dataset. This allows for precise localization within the gastrointestinal (GI) tract while requiring only approximately 1M parameters and thus, provides a method suitable for low power devices
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种方法，通过结合卷积神经网络（CNN）和隐马尔可夫模型（HMM）的时间序列分析特性来高效地分类 Gastroenterologic 图像序列，从Video Capsule Endoscopy (VCE) 研究中获取的。研究表明，顺序时间序列分析可以corrrect CNN 输出中的错误。我们的方法在 Rhode Island (RI)  Gastroenterology 数据集上达到了 $98.04\%$ 的准确率，这使得在 Gastrointestinal (GI) 轨迹中进行精确定位，只需约 1M 参数，因此适用于低功耗设备。
</details></li>
</ul>
<hr>
<h2 id="ASV-Station-Keeping-under-Wind-Disturbances-using-Neural-Network-Simulation-Error-Minimization-Model-Predictive-Control"><a href="#ASV-Station-Keeping-under-Wind-Disturbances-using-Neural-Network-Simulation-Error-Minimization-Model-Predictive-Control" class="headerlink" title="ASV Station Keeping under Wind Disturbances using Neural Network Simulation Error Minimization Model Predictive Control"></a>ASV Station Keeping under Wind Disturbances using Neural Network Simulation Error Minimization Model Predictive Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07892">http://arxiv.org/abs/2310.07892</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jalil Chavez-Galaviz, Jianwen Li, Ajinkya Chaudhary, Nina Mahmoudian</li>
<li>for: 这个论文主要针对 Autonomous Surface Vehicles (ASVs) 在狭窄空间中进行探测和相对定位时的稳定控制问题。</li>
<li>methods: 该论文提出了一种基于神经网络预测误差最小化 (NNSEM-MPC) 的模型预测控制器，用于精准地预测 ASV 的动态行为下风干扰的影响。</li>
<li>results: 对于风干扰情况下的稳定控制问题，该论文的提出的 NNSEM-MPC 方法与其他控制器（包括 backstepping 控制器、滑模控制器、简化动态 MPC (SD-MPC)、神经普通几何 MPC (NODE-MPC) 和知识基于 NODE MPC (KNODE-MPC)）进行比较，并在六个测试情况下得到了显著的优异性。<details>
<summary>Abstract</summary>
Station keeping is an essential maneuver for Autonomous Surface Vehicles (ASVs), mainly when used in confined spaces, to carry out surveys that require the ASV to keep its position or in collaboration with other vehicles where the relative position has an impact over the mission. However, this maneuver can become challenging for classic feedback controllers due to the need for an accurate model of the ASV dynamics and the environmental disturbances. This work proposes a Model Predictive Controller using Neural Network Simulation Error Minimization (NNSEM-MPC) to accurately predict the dynamics of the ASV under wind disturbances. The performance of the proposed scheme under wind disturbances is tested and compared against other controllers in simulation, using the Robotics Operating System (ROS) and the multipurpose simulation environment Gazebo. A set of six tests were conducted by combining two wind speeds (3 m/s and 6 m/s) and three wind directions (0$^\circ$, 90$^\circ$, and 180$^\circ$). The simulation results clearly show the advantage of the NNSEM-MPC over the following methods: backstepping controller, sliding mode controller, simplified dynamics MPC (SD-MPC), neural ordinary differential equation MPC (NODE-MPC), and knowledge-based NODE MPC (KNODE-MPC). The proposed NNSEM-MPC approach performs better than the rest in 4 out of the 6 test conditions, and it is the second best in the 2 remaining test cases, reducing the mean position and heading error by at least 31\% and 46\% respectively across all the test cases. In terms of execution speed, the proposed NNSEM-MPC is at least 36\% faster than the rest of the MPC controllers. The field experiments on two different ASV platforms showed that ASVs can effectively keep the station utilizing the proposed method, with a position error as low as $1.68$ m and a heading error as low as $6.14^{\circ}$ within time windows of at least $150$s.
</details>
<details>
<summary>摘要</summary>
Station keeping是自主水下车辆（ASV）的必需操作，尤其在封闭空间中使用，以进行需要ASV保持位置或与其他车辆合作，其中相对位置对任务有重要影响。然而，这种操作可能会对 классическими反馈控制器而成为挑战，因为需要准确的ASV动态模型和环境干扰的数据。这项工作提议使用神经网络预测误差最小化模型predictive控制（NNSEM-MPC）来准确预测ASV在风干扰下的动态。我们对提议的方案在风干扰下的性能进行了测试和比较，使用Robotics Operating System（ROS）和多用途 simulate环境Gazebo。我们进行了六个测试，其中每个测试都 combinated two wind speed（3 m/s和6 m/s）和 three wind direction（0$^\circ$, 90$^\circ$,和180$^\circ）。模拟结果明显地显示了NNSEM-MPC的优势，胜过以下方法：backstepping controller、滑模控制、简化动态MPC（SD-MPC）、神经ordinary differential equation MPC（NODE-MPC）和知识基于NODE MPC（KNODE-MPC）。提议的NNSEM-MPC方法在4个测试条件中表现最佳，并在另外2个测试条件中表现第二最佳，将mean position和heading error降低至少31%和46%。在执行速度方面，提议的NNSEM-MPC至少36% faster than其他MPC控制器。在两种不同ASV平台上进行的野外实验表明，ASV可以通过提议的方法 effetively keep station，position error为1.68米，heading error为6.14度，在时间窗口至少150秒内。
</details></li>
</ul>
<hr>
<h2 id="A-Theory-of-Non-Linear-Feature-Learning-with-One-Gradient-Step-in-Two-Layer-Neural-Networks"><a href="#A-Theory-of-Non-Linear-Feature-Learning-with-One-Gradient-Step-in-Two-Layer-Neural-Networks" class="headerlink" title="A Theory of Non-Linear Feature Learning with One Gradient Step in Two-Layer Neural Networks"></a>A Theory of Non-Linear Feature Learning with One Gradient Step in Two-Layer Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07891">http://arxiv.org/abs/2310.07891</a></li>
<li>repo_url: None</li>
<li>paper_authors: Behrad Moniri, Donghwan Lee, Hamed Hassani, Edgar Dobriban</li>
<li>for: This paper is written for understanding the conditions under which feature learning occurs in deep neural networks, and how to improve the learning process by introducing multiple rank-one components.</li>
<li>methods: The paper uses two-layer fully-connected neural networks and gradient descent with a growing learning rate to introduce multiple rank-one components, which enable the learning of non-linear features.</li>
<li>results: The paper shows that with a growing learning rate, the training process introduces multiple rank-one components, each corresponding to a specific polynomial feature, and these non-linear features can enhance learning. The limiting large-dimensional and large sample training and test errors of the updated neural networks are fully characterized by these spikes.<details>
<summary>Abstract</summary>
Feature learning is thought to be one of the fundamental reasons for the success of deep neural networks. It is rigorously known that in two-layer fully-connected neural networks under certain conditions, one step of gradient descent on the first layer followed by ridge regression on the second layer can lead to feature learning; characterized by the appearance of a separated rank-one component -- spike -- in the spectrum of the feature matrix. However, with a constant gradient descent step size, this spike only carries information from the linear component of the target function and therefore learning non-linear components is impossible. We show that with a learning rate that grows with the sample size, such training in fact introduces multiple rank-one components, each corresponding to a specific polynomial feature. We further prove that the limiting large-dimensional and large sample training and test errors of the updated neural networks are fully characterized by these spikes. By precisely analyzing the improvement in the loss, we demonstrate that these non-linear features can enhance learning.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate "Feature learning is thought to be one of the fundamental reasons for the success of deep neural networks. It is rigorously known that in two-layer fully-connected neural networks under certain conditions, one step of gradient descent on the first layer followed by ridge regression on the second layer can lead to feature learning; characterized by the appearance of a separated rank-one component -- spike -- in the spectrum of the feature matrix. However, with a constant gradient descent step size, this spike only carries information from the linear component of the target function and therefore learning non-linear components is impossible. We show that with a learning rate that grows with the sample size, such training in fact introduces multiple rank-one components, each corresponding to a specific polynomial feature. We further prove that the limiting large-dimensional and large sample training and test errors of the updated neural networks are fully characterized by these spikes. By precisely analyzing the improvement in the loss, we demonstrate that these non-linear features can enhance learning."Translate:Feature 学习是深度神经网络的成功的基本原因之一。在满足 certain 条件下的两层全连接神经网络中，一步Gradient Descent在第一层 followed by Ridge Regression 在第二层可以导致特征学习，这可以通过特征矩阵的spectrum中的分离rank-one component -- spike -- 来Characterize。然而，随着步长不变，这个spike只会传递线性函数的信息，因此学习非线性Component是不可能的。我们显示，随着样本大小增加，这种训练实际引入多个rank-one component，每个component都对应于特定的多项式特征。我们进一步证明， limiting 大量和大样本训练和测试错误的更新神经网络的准确性是由这些spike完全Characterize。通过精确分析改进的损失，我们示出这些非线性特征可以提高学习。
</details></li>
</ul>
<hr>
<h2 id="Refined-Mechanism-Design-for-Approximately-Structured-Priors-via-Active-Regression"><a href="#Refined-Mechanism-Design-for-Approximately-Structured-Priors-via-Active-Regression" class="headerlink" title="Refined Mechanism Design for Approximately Structured Priors via Active Regression"></a>Refined Mechanism Design for Approximately Structured Priors via Active Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07874">http://arxiv.org/abs/2310.07874</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christos Boutsikas, Petros Drineas, Marios Mertzanidis, Alexandros Psomas, Paritosh Verma</li>
<li>for: 这个论文的目的是解决一个具有大量商品和投标者的契约问题，投标者的估价是从高维不确定的先前分布中独立采样出来的。</li>
<li>methods: 该论文使用了一种以话题模型为基础的方法，使用活动学习组件和机制设计组件。活动学习组件负责与投标者进行互动，并输出低维度的投标者类型，而机制设计组件则负责使机制在低维度模型下变得对投标者类型有效。</li>
<li>results: 该论文的结果表明，使用这种方法可以减少机制设计中的假设和限制，并且可以在不同的话题模型下实现更高的收益。此外，该论文还开拓了机制设计和随机线性代数（RLA）的连接，并将RLA的多个突破成果应用到机制设计中。<details>
<summary>Abstract</summary>
We consider the problem of a revenue-maximizing seller with a large number of items $m$ for sale to $n$ strategic bidders, whose valuations are drawn independently from high-dimensional, unknown prior distributions. It is well-known that optimal and even approximately-optimal mechanisms for this setting are notoriously difficult to characterize or compute, and, even when they can be found, are often rife with various counter-intuitive properties. In this paper, following a model introduced recently by Cai and Daskalakis~\cite{cai2022recommender}, we consider the case that bidders' prior distributions can be well-approximated by a topic model. We design an active learning component, responsible for interacting with the bidders and outputting low-dimensional approximations of their types, and a mechanism design component, responsible for robustifying mechanisms for the low-dimensional model to work for the approximate types of the former component. On the active learning front, we cast our problem in the framework of Randomized Linear Algebra (RLA) for regression problems, allowing us to import several breakthrough results from that line of research, and adapt them to our setting. On the mechanism design front, we remove many restrictive assumptions of prior work on the type of access needed to the underlying distributions and the associated mechanisms. To the best of our knowledge, our work is the first to formulate connections between mechanism design, and RLA for active learning of regression problems, opening the door for further applications of randomized linear algebra primitives to mechanism design.
</details>
<details>
<summary>摘要</summary>
我们考虑一个寻求最大化收益的售家，面临一大量的商品($m$)供$n$名战略性投标者购买。这些投标者的价值是从高维度、未知的对应分布中独立地获取。对于这个设定，优化和约优化的机制是极其困难computational和易受到各种 counter-intuitive 的性质。在这篇文章中，我们遵循 Cai 和 Daskalakis （2022）提出的模型，即投标者对价值的对应分布可以以主题模型的形式近似。我们设计了一个活动学习部分，负责与投标者进行互动，从来到低维度的近似类型，以及一个机制设计部分，负责对这个低维度模型进行强健化，以适应投标者的实际类型。在活动学习方面，我们将问题套用在Randomized Linear Algebra（RLA）的框架中，允许我们从这个领域的研究中吸取多个突破性结果，并将其适应到我们的设定。在机制设计方面，我们取消了许多先前的研究对机制设计的限制性假设，例如需要存取到背后的分布和相关机制。我们的工作是首个将机制设计与 RLA 的活动学习连接起来，这会开启更多的应用机会，将随机线性代数元素应用到机制设计中。
</details></li>
</ul>
<hr>
<h2 id="QArchSearch-A-Scalable-Quantum-Architecture-Search-Package"><a href="#QArchSearch-A-Scalable-Quantum-Architecture-Search-Package" class="headerlink" title="QArchSearch: A Scalable Quantum Architecture Search Package"></a>QArchSearch: A Scalable Quantum Architecture Search Package</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07858">http://arxiv.org/abs/2310.07858</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ankit Kulshrestha, Danylo Lykov, Ilya Safro, Yuri Alexeev</li>
<li>for: 这篇论文旨在提供一种AI驱动的量子架构搜索包，用于自动选择合适的量子模型，以实现量子计算任务。</li>
<li>methods: 该论文使用的方法包括使用\texttt{QTensor}库作为后端，并采用两级并行的策略来加速搜索过程，以便在高性能计算系统上运行。</li>
<li>results: 论文的实验结果表明，\texttt{QArchSearch}可以有效地搜索大型量子电路，并可以探索不同的量子应用中的更复杂的模型。<details>
<summary>Abstract</summary>
The current era of quantum computing has yielded several algorithms that promise high computational efficiency. While the algorithms are sound in theory and can provide potentially exponential speedup, there is little guidance on how to design proper quantum circuits to realize the appropriate unitary transformation to be applied to the input quantum state. In this paper, we present \texttt{QArchSearch}, an AI based quantum architecture search package with the \texttt{QTensor} library as a backend that provides a principled and automated approach to finding the best model given a task and input quantum state. We show that the search package is able to efficiently scale the search to large quantum circuits and enables the exploration of more complex models for different quantum applications. \texttt{QArchSearch} runs at scale and high efficiency on high-performance computing systems using a two-level parallelization scheme on both CPUs and GPUs, which has been demonstrated on the Polaris supercomputer.
</details>
<details>
<summary>摘要</summary>
当前的量子计算时代已经涌现了许多算法，这些算法承诺可以提供高效的计算能力。虽然这些算法在理论上是有效的，但是有很少关于如何设计合适的量子电路来实现输入量子状态的应用转换的指导。在这篇论文中，我们介绍了\texttt{QArchSearch}，一个基于人工智能的量子建筑搜索包，它使用\texttt{QTensor}库作为后端，并提供了一种原则正的和自动化的方法来找到给定任务和输入量子状态的最佳模型。我们表明了\texttt{QArchSearch}可以高效地扩展到大型量子电路，并允许对不同量子应用的模型进行更加复杂的探索。\texttt{QArchSearch}在高性能计算系统上运行，使用了两级并行化策略，其在CPUs和GPUs上进行了两级并行，这已经在Polaris超级计算机上得到了证明。
</details></li>
</ul>
<hr>
<h2 id="On-the-Computational-Complexity-of-Private-High-dimensional-Model-Selection-via-the-Exponential-Mechanism"><a href="#On-the-Computational-Complexity-of-Private-High-dimensional-Model-Selection-via-the-Exponential-Mechanism" class="headerlink" title="On the Computational Complexity of Private High-dimensional Model Selection via the Exponential Mechanism"></a>On the Computational Complexity of Private High-dimensional Model Selection via the Exponential Mechanism</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07852">http://arxiv.org/abs/2310.07852</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saptarshi Roy, Ambuj Tewari</li>
<li>for: 该文章研究了在高维度含有稀畴元素的线性回归模型下的模型选择问题，并在敏感性框架下进行研究。特别是，文章研究了极私的最佳子集选择问题，并对其实现了utilities保证。</li>
<li>methods: 文章采用了著名的指数机制来选择最佳模型，并在满足某种margin条件下Establish its strong model recovery property。然而，指数搜索空间的指数机制带来了严重的计算瓶颈。为了解决这个挑战，文章提出了 Metropolis-Hastings算法来采样步骤，并Establish its polynomial mixing time to its stationary distribution in the problem parameters $n,p$, and $s$.</li>
<li>results: 文章的结果表明，Metropolis-Hastings算法可以在高维度含有稀畴元素的线性回归模型下实现极私的最佳子集选择，并且可以在某种margin条件下Establish strong model recovery property。此外，文章还提出了一种approximate differential privacy的方法来保证最终估计的Metropolis-Hastings random walk的极私性。最后，文章还进行了一些Ilustrative simulations，并证明了其理论结论。<details>
<summary>Abstract</summary>
We consider the problem of model selection in a high-dimensional sparse linear regression model under the differential privacy framework. In particular, we consider the problem of differentially private best subset selection and study its utility guarantee. We adopt the well-known exponential mechanism for selecting the best model, and under a certain margin condition, we establish its strong model recovery property. However, the exponential search space of the exponential mechanism poses a serious computational bottleneck. To overcome this challenge, we propose a Metropolis-Hastings algorithm for the sampling step and establish its polynomial mixing time to its stationary distribution in the problem parameters $n,p$, and $s$. Furthermore, we also establish approximate differential privacy for the final estimates of the Metropolis-Hastings random walk using its mixing property. Finally, we also perform some illustrative simulations that echo the theoretical findings of our main results.
</details>
<details>
<summary>摘要</summary>
我团队考虑了在高维简单线性回归模型下进行模型选择，并在权限保护框架下进行研究。特别是，我们考虑了不同极值隐私最佳分布选择问题，并研究其性能保证。我们采用了著名的凝聚机制来选择最佳模型，并在某种margin条件下证明了它的强型回归性质。然而，凝聚搜索空间的凝聚机制带来了严重的计算瓶颈。为了解决这个挑战，我们提议了 Metropolis-Hastings算法来实现抽样步骤，并证明了它在参数$n$, $p$, 和 $s$下的多项式混合时间到其站点分布。此外，我们还证明了对最终估计的Metropolis-Hastings随机步骤的approximate权限保护。最后，我们还进行了一些与理论结果相符的仿真实验。
</details></li>
</ul>
<hr>
<h2 id="Measuring-Feature-Sparsity-in-Language-Models"><a href="#Measuring-Feature-Sparsity-in-Language-Models" class="headerlink" title="Measuring Feature Sparsity in Language Models"></a>Measuring Feature Sparsity in Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07837">http://arxiv.org/abs/2310.07837</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingyang Deng, Lucas Tao, Joe Benton</li>
<li>for: 本研究旨在探讨语言模型中活动的表征方式，具体来说是用简单的线性组合来描述输入文本中的特征方向。</li>
<li>methods: 本研究使用 sparse coding 技术来重建特征方向，并开发了一些指标来评估 sparse coding 的成功程度。</li>
<li>results: 研究发现，语言模型的活动可以准确地表示为简单的线性组合，而且这种线性和简单性假设都得到了证明。此外，研究还发现模型活动的稀热程度最高在输入文本的第一层和最后一层。<details>
<summary>Abstract</summary>
Recent works have proposed that activations in language models can be modelled as sparse linear combinations of vectors corresponding to features of input text. Under this assumption, these works aimed to reconstruct feature directions using sparse coding. We develop metrics to assess the success of these sparse coding techniques and test the validity of the linearity and sparsity assumptions. We show our metrics can predict the level of sparsity on synthetic sparse linear activations, and can distinguish between sparse linear data and several other distributions. We use our metrics to measure levels of sparsity in several language models. We find evidence that language model activations can be accurately modelled by sparse linear combinations of features, significantly more so than control datasets. We also show that model activations appear to be sparsest in the first and final layers.
</details>
<details>
<summary>摘要</summary>
近期研究建议，语言模型的激活可以表示为稀疏线性组合的特征向量。基于这个假设，这些研究尝试使用稀疏编码重建特征方向。我们开发了评估这些稀疏编码技术的度量，并测试了Linearity和稀疏性假设的有效性。我们发现我们的度量可以预测稀疏线性活动的水平，并能够分辨稀疏线性数据和其他数据 Distribution。我们使用我们的度量测量了一些语言模型的激活水平，并发现语言模型的激活可以准确地表示为稀疏线性组合的特征向量，比控制数据集更高。此外，我们还发现模型的激活在第一层和最后一层最为稀疏。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Models-Are-Zero-Shot-Time-Series-Forecasters"><a href="#Large-Language-Models-Are-Zero-Shot-Time-Series-Forecasters" class="headerlink" title="Large Language Models Are Zero-Shot Time Series Forecasters"></a>Large Language Models Are Zero-Shot Time Series Forecasters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07820">http://arxiv.org/abs/2310.07820</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ngruver/llmtime">https://github.com/ngruver/llmtime</a></li>
<li>paper_authors: Nate Gruver, Marc Finzi, Shikai Qiu, Andrew Gordon Wilson</li>
<li>for: 这个论文主要针对时间序列预测 task，使用大型自然语言模型（LLMs）如 GPT-3 和 LLaMA-2 进行预测。</li>
<li>methods: 该论文提出了一种将时间序列编码为数字字符串的方法，并使用这种方法来实现 LLMS 的 zero-shot 推广。另外，论文还提出了一种将离散分布转换为高度灵活的连续值分布的方法。</li>
<li>results: 论文发现， LLMS 可以在时间序列预测 task 上达到或超过专门设计的时间序列模型的性能，而无需训练。此外，论文还示出了 LLMS 可以处理缺失数据、考虑文本副信息和回答预测问题等。<details>
<summary>Abstract</summary>
By encoding time series as a string of numerical digits, we can frame time series forecasting as next-token prediction in text. Developing this approach, we find that large language models (LLMs) such as GPT-3 and LLaMA-2 can surprisingly zero-shot extrapolate time series at a level comparable to or exceeding the performance of purpose-built time series models trained on the downstream tasks. To facilitate this performance, we propose procedures for effectively tokenizing time series data and converting discrete distributions over tokens into highly flexible densities over continuous values. We argue the success of LLMs for time series stems from their ability to naturally represent multimodal distributions, in conjunction with biases for simplicity, and repetition, which align with the salient features in many time series, such as repeated seasonal trends. We also show how LLMs can naturally handle missing data without imputation through non-numerical text, accommodate textual side information, and answer questions to help explain predictions. While we find that increasing model size generally improves performance on time series, we show GPT-4 can perform worse than GPT-3 because of how it tokenizes numbers, and poor uncertainty calibration, which is likely the result of alignment interventions such as RLHF.
</details>
<details>
<summary>摘要</summary>
通过将时间序列编码为一串数字，我们可以将时间序列预测转化为下一个token的预测问题。发展这种方法，我们发现大语言模型（LLM）such as GPT-3和LLaMA-2可以 surprisingly Zero-shot推断时间序列，其性能与专门为下游任务训练的时间序列模型相当或超越。为了实现这种性能，我们提出了 tokenization 时间序列数据和将精度分布转化为高度灵活的浮点值的方法。我们认为 LLMS 对时间序列的成功归功于它们的自然地表示多模态分布，以及对简单、重复的偏好，这些特征与许多时间序列中的重复季节性脉冲有很大相似性。我们还示出了 LLMS 可以自然处理缺失数据，不需要采用非数字文本进行填充，同时可以处理文本侧信息和解释预测结果。虽然我们发现模型大小增加通常会提高时间序列的性能，但我们发现 GPT-4 可能会比 GPT-3 更差，这可能是因为 GPT-4 的 tokenization 方式不同，以及不良的uncertainty calibration，这可能是由RLHF等对适应性进行的调整所致。
</details></li>
</ul>
<hr>
<h2 id="Online-RL-in-Linearly-q-π-Realizable-MDPs-Is-as-Easy-as-in-Linear-MDPs-If-You-Learn-What-to-Ignore"><a href="#Online-RL-in-Linearly-q-π-Realizable-MDPs-Is-as-Easy-as-in-Linear-MDPs-If-You-Learn-What-to-Ignore" class="headerlink" title="Online RL in Linearly $q^π$-Realizable MDPs Is as Easy as in Linear MDPs If You Learn What to Ignore"></a>Online RL in Linearly $q^π$-Realizable MDPs Is as Easy as in Linear MDPs If You Learn What to Ignore</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07811">http://arxiv.org/abs/2310.07811</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gellért Weisz, András György, Csaba Szepesvári</li>
<li>for: 这种研究是为了解决线性$q^\pi$-可实现性假设下的在线学习决策问题。</li>
<li>methods: 这篇论文使用的方法是同时学习扫描过程中的状态和动作值，以及使用一种新的算法来同时学习扫描过程中的状态和动作值。</li>
<li>results: 这篇论文提出了一种新的算法，可以在线学习决策问题，并且可以在有限的交互中返回$\epsilon$-优化的策略。此外，论文还证明了这种算法在假设错误的情况下的性能，并且显示了性能与假设错误之间的关系。<details>
<summary>Abstract</summary>
We consider online reinforcement learning (RL) in episodic Markov decision processes (MDPs) under the linear $q^\pi$-realizability assumption, where it is assumed that the action-values of all policies can be expressed as linear functions of state-action features. This class is known to be more general than linear MDPs, where the transition kernel and the reward function are assumed to be linear functions of the feature vectors. As our first contribution, we show that the difference between the two classes is the presence of states in linearly $q^\pi$-realizable MDPs where for any policy, all the actions have approximately equal values, and skipping over these states by following an arbitrarily fixed policy in those states transforms the problem to a linear MDP. Based on this observation, we derive a novel (computationally inefficient) learning algorithm for linearly $q^\pi$-realizable MDPs that simultaneously learns what states should be skipped over and runs another learning algorithm on the linear MDP hidden in the problem. The method returns an $\epsilon$-optimal policy after $\text{polylog}(H, d)/\epsilon^2$ interactions with the MDP, where $H$ is the time horizon and $d$ is the dimension of the feature vectors, giving the first polynomial-sample-complexity online RL algorithm for this setting. The results are proved for the misspecified case, where the sample complexity is shown to degrade gracefully with the misspecification error.
</details>
<details>
<summary>摘要</summary>
我们考虑在线束规动学学习（RL）中的集合Markov决策过程（MDP），在Linear-$q^\pi$-可现实性假设下，其中假设所有策略的动作价值可以表示为线性函数的状态动作特征。这个类型比linear MDP更加一般，因为在linear MDP中，转移核和奖励函数都是特征Vector的线性函数。作为我们的首要贡献，我们显示了这两个类型之间的差别在于在Linearly-$q^\pi$-可现实性MDP中有些状态，对于任何策略，所有的动作都有相对平等的价值，并且通过遵循任意固定策略在这些状态中跳过这些状态，可以将问题转化为一个线性MDP。基于这一观察，我们 derivate了一种新的（计算不fficiente）学习算法，可以同时学习在Linearly-$q^\pi$-可现实性MDP中哪些状态应该跳过以及在隐藏在问题中的线性MDP上运行另一个学习算法。这种方法可以在$\text{polylog}(H, d)/\epsilon^2$交互后返回一个$\epsilon$-优化策略，其中$H$是时间悬度和$d$是特征vector的维度，这是首个在这种设定下的多项式样本复杂度在线RL算法。我们的结果是在误specified情况下证明的，其中样本复杂度显示了对误pecification错误的恶化。
</details></li>
</ul>
<hr>
<h2 id="FedSym-Unleashing-the-Power-of-Entropy-for-Benchmarking-the-Algorithms-for-Federated-Learning"><a href="#FedSym-Unleashing-the-Power-of-Entropy-for-Benchmarking-the-Algorithms-for-Federated-Learning" class="headerlink" title="FedSym: Unleashing the Power of Entropy for Benchmarking the Algorithms for Federated Learning"></a>FedSym: Unleashing the Power of Entropy for Benchmarking the Algorithms for Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07807">http://arxiv.org/abs/2310.07807</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ensiye Kiyamousavi, Boris Kraychev, Ivan Koychev<br>for: 这种研究的目标是为了解决 Federated Learning (FL) 中数据不均衡和模型融合效果的问题。methods: 这种研究使用了多种数据分割技术，以适应不同的数据不均衡。results: 研究表明，我们提出的方法可以准确地测量数据不均衡，并且可以逐步挑战 FL 算法。实验结果表明，模型在我们提出的分布上进行训练后，模型更加异质。<details>
<summary>Abstract</summary>
Federated learning (FL) is a decentralized machine learning approach where independent learners process data privately. Its goal is to create a robust and accurate model by aggregating and retraining local models over multiple rounds. However, FL faces challenges regarding data heterogeneity and model aggregation effectiveness. In order to simulate real-world data, researchers use methods for data partitioning that transform a dataset designated for centralized learning into a group of sub-datasets suitable for distributed machine learning with different data heterogeneity. In this paper, we study the currently popular data partitioning techniques and visualize their main disadvantages: the lack of precision in the data diversity, which leads to unreliable heterogeneity indexes, and the inability to incrementally challenge the FL algorithms. To resolve this problem, we propose a method that leverages entropy and symmetry to construct 'the most challenging' and controllable data distributions with gradual difficulty. We introduce a metric to measure data heterogeneity among the learning agents and a transformation technique that divides any dataset into splits with precise data diversity. Through a comparative study, we demonstrate the superiority of our method over existing FL data partitioning approaches, showcasing its potential to challenge model aggregation algorithms. Experimental results indicate that our approach gradually challenges the FL strategies, and the models trained on FedSym distributions are more distinct.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 是一种分布式机器学习方法，其目标是通过聚合和重新训练本地模型，创建一个稳定和准确的模型。然而，FL 面临数据多样性和模型聚合效果的挑战。为了模拟实际数据，研究人员使用数据分区方法，将Centralized learning 的数据集转换为适合分布式机器学习的多个子集。在这篇论文中，我们研究了目前流行的数据分区技术，并视觉化它们的主要缺点：数据多样性精度的缺失，导致无法准确地评估多样性指标，以及不能逐步挑战 FL 算法。为解决这个问题，我们提议一种方法，利用熵和对称来构建 '最有挑战性' 和可控的数据分布。我们介绍了一个度量数据多样性 среди学习代理的 metric，以及一种分割任何数据集的技术，以确保数据多样性的精度。通过比较研究，我们证明了我们的方法比现有的 FL 数据分区方法更加有利，并示出了它的潜在挑战模型聚合算法的能力。实验结果表明，我们的方法可以逐步挑战 FL 策略，并且模型在 FedSym 分布上训练得更加独特。
</details></li>
</ul>
<hr>
<h2 id="Using-Spark-Machine-Learning-Models-to-Perform-Predictive-Analysis-on-Flight-Ticket-Pricing-Data"><a href="#Using-Spark-Machine-Learning-Models-to-Perform-Predictive-Analysis-on-Flight-Ticket-Pricing-Data" class="headerlink" title="Using Spark Machine Learning Models to Perform Predictive Analysis on Flight Ticket Pricing Data"></a>Using Spark Machine Learning Models to Perform Predictive Analysis on Flight Ticket Pricing Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07787">http://arxiv.org/abs/2310.07787</a></li>
<li>repo_url: None</li>
<li>paper_authors: Philip Wong, Phue Thant, Pratiksha Yadav, Ruta Antaliya, Jongwook Woo</li>
<li>for: 预测航空票价（非停站） flight pricing</li>
<li>methods: 使用 r2（r-square）和 RMSE 进行预测，利用大量数据集（来自Expedia.com），包括约2000万记录和4.68 gigabytes</li>
<li>results: 确定最佳模型，以便在实际世界中预测航空票价，并且要求模型具有良好的泛化能力和优化的处理时间。<details>
<summary>Abstract</summary>
This paper discusses predictive performance and processes undertaken on flight pricing data utilizing r2(r-square) and RMSE that leverages a large dataset, originally from Expedia.com, consisting of approximately 20 million records or 4.68 gigabytes. The project aims to determine the best models usable in the real world to predict airline ticket fares for non-stop flights across the US. Therefore, good generalization capability and optimized processing times are important measures for the model.   We will discover key business insights utilizing feature importance and discuss the process and tools used for our analysis. Four regression machine learning algorithms were utilized: Random Forest, Gradient Boost Tree, Decision Tree, and Factorization Machines utilizing Cross Validator and Training Validator functions for assessing performance and generalization capability.
</details>
<details>
<summary>摘要</summary>
这篇论文讨论了预测性能和利用r2（r平方）和RMSE来处理飞行票价数据，使用了大量的数据集，来自Expedia.com，包含约2000万记录或4.68吉比特。项目的目标是确定用于实际应用中预测航空票价的最佳模型，因此泛化能力和处理时间优化是重要的评价指标。我们会通过特征重要性来探索关键的商业发现，并讨论我们使用的分析过程和工具。在本研究中，我们使用了四种回归机器学习算法：随机森林、梯度提升树、决策树和因素分解机器，并使用了cross validate和training validate函数来评估性能和泛化能力。
</details></li>
</ul>
<hr>
<h2 id="Non-Stationary-Contextual-Bandit-Learning-via-Neural-Predictive-Ensemble-Sampling"><a href="#Non-Stationary-Contextual-Bandit-Learning-via-Neural-Predictive-Ensemble-Sampling" class="headerlink" title="Non-Stationary Contextual Bandit Learning via Neural Predictive Ensemble Sampling"></a>Non-Stationary Contextual Bandit Learning via Neural Predictive Ensemble Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07786">http://arxiv.org/abs/2310.07786</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zheqing Zhu, Yueyang Liu, Xu Kuang, Benjamin Van Roy</li>
<li>for: 实际应用中的情感带its often exhibit non-stationarity due to seasonality, serendipity, and evolving social trends.</li>
<li>methods: 我们提出了一个新的非站ARY contextual bandit算法，它结合了可扩展的深度对应网络架构，并且运用了一个策略性优先预测的探索机制，以优先收集持续性最高的信息。</li>
<li>results: 我们通过对两个实际推荐数据集进行实验，证明了我们的方法与现有的基eline signifiantly outperform。<details>
<summary>Abstract</summary>
Real-world applications of contextual bandits often exhibit non-stationarity due to seasonality, serendipity, and evolving social trends. While a number of non-stationary contextual bandit learning algorithms have been proposed in the literature, they excessively explore due to a lack of prioritization for information of enduring value, or are designed in ways that do not scale in modern applications with high-dimensional user-specific features and large action set, or both. In this paper, we introduce a novel non-stationary contextual bandit algorithm that addresses these concerns. It combines a scalable, deep-neural-network-based architecture with a carefully designed exploration mechanism that strategically prioritizes collecting information with the most lasting value in a non-stationary environment. Through empirical evaluations on two real-world recommendation datasets, which exhibit pronounced non-stationarity, we demonstrate that our approach significantly outperforms the state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
实际应用中的上下文带有趋势的带宽机器学习问题常会出现非站点性，这可能由季节性、偶然性和不断变化的社会趋势引起。虽然文献中有许多非站点上下文带宽机器学习算法，但它们过度探索，因为缺乏综合价值信息的优先级，或者设计不适应现代应用中高维用户特定特征和大量动作集，或者都两者。在本文中，我们介绍了一种新的非站点上下文带宽机器学习算法。它将拓扑图 neural network 作为核心组件，并通过精心设计的探索机制，以优先级Collect information with the most lasting value in a non-stationary environment。经验证明，我们的方法在两个实际推荐数据集上，具有明显的非站点性，significantly outperform了状态对照。
</details></li>
</ul>
<hr>
<h2 id="Promoting-Robustness-of-Randomized-Smoothing-Two-Cost-Effective-Approaches"><a href="#Promoting-Robustness-of-Randomized-Smoothing-Two-Cost-Effective-Approaches" class="headerlink" title="Promoting Robustness of Randomized Smoothing: Two Cost-Effective Approaches"></a>Promoting Robustness of Randomized Smoothing: Two Cost-Effective Approaches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07780">http://arxiv.org/abs/2310.07780</a></li>
<li>repo_url: None</li>
<li>paper_authors: Linbo Liu, Trong Nghia Hoang, Lam M. Nguyen, Tsui-Wei Weng</li>
<li>for: 提供可靠的 robustness 保证 для滑动化神经网络分类器。</li>
<li>methods: 提出了两种成本效果的方法，包括 AdvMacer 和 EsbRS，以提高滑动化神经网络分类器的 robustness 性能，而不会影响 clean 性能。</li>
<li>results:  Comparing with SOTA 基elines, AdvMacer 可以提高滑动化神经网络分类器的 robustness 性能，并且可以在训练时间上减少 3 倍的时间成本。EsbRS 可以提高模型 ensemble 的 robustness 性能，并且提出了一种新的模型 ensemble 设计方法，以提高 robustness 性能。<details>
<summary>Abstract</summary>
Randomized smoothing has recently attracted attentions in the field of adversarial robustness to provide provable robustness guarantees on smoothed neural network classifiers. However, existing works show that vanilla randomized smoothing usually does not provide good robustness performance and often requires (re)training techniques on the base classifier in order to boost the robustness of the resulting smoothed classifier. In this work, we propose two cost-effective approaches to boost the robustness of randomized smoothing while preserving its clean performance. The first approach introduces a new robust training method AdvMacerwhich combines adversarial training and robustness certification maximization for randomized smoothing. We show that AdvMacer can improve the robustness performance of randomized smoothing classifiers compared to SOTA baselines, while being 3x faster to train than MACER baseline. The second approach introduces a post-processing method EsbRS which greatly improves the robustness certificate based on building model ensembles. We explore different aspects of model ensembles that has not been studied by prior works and propose a novel design methodology to further improve robustness of the ensemble based on our theoretical analysis.
</details>
<details>
<summary>摘要</summary>
Randomized smoothing 在 adversarial robustness 领域已经吸引了关注，以提供可证明的 Robustness  garanties  на smoothed neural network 分类器。然而，现有的工作表明，vanilla randomized smoothing 通常不提供良好的 Robustness 性能，并且常需要 (re)training 技术来提高基础分类器的 Robustness。在这个工作中，我们提出了两种cost-effective的方法来提高 randomized smoothing 的 Robustness，同时保持其 clean 性能。第一种方法是 AdvMacer，它combines adversarial training 和 Robustness 证明最大化 для randomized smoothing。我们示出，AdvMacer 可以在 SOTA 基eline 下提高 randomized smoothing 分类器的 Robustness 性能，而且训练速度比 MACER 基eline 快三倍。第二种方法是 EsbRS，它是一种post-processing 方法，可以大幅提高基于模型集的 Robustness 证明。我们探索了不同的模型集方面，并提出了一种新的设计方法，以进一步提高模型集的 Robustness。我们的理论分析表明，这种设计方法可以减少模型集的复杂性，同时保持其 Robustness。
</details></li>
</ul>
<hr>
<h2 id="Feature-Learning-and-Generalization-in-Deep-Networks-with-Orthogonal-Weights"><a href="#Feature-Learning-and-Generalization-in-Deep-Networks-with-Orthogonal-Weights" class="headerlink" title="Feature Learning and Generalization in Deep Networks with Orthogonal Weights"></a>Feature Learning and Generalization in Deep Networks with Orthogonal Weights</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07765">http://arxiv.org/abs/2310.07765</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hannah Day, Yonatan Kahn, Daniel A. Roberts</li>
<li>for: 这篇论文是为了研究深度 neural network 的 initialization 方法，以及其对 training 的影响。</li>
<li>methods: 该论文使用了 rectangular network 和 tanh activation function，并使用了 orthogonal matrix 的 ensemble 初始化方法。</li>
<li>results: 论文表明，使用这种 initialization 方法可以避免深度 neural network 的 signal 干扰增长，并且可以提高网络的泛化能力和训练速度。<details>
<summary>Abstract</summary>
Fully-connected deep neural networks with weights initialized from independent Gaussian distributions can be tuned to criticality, which prevents the exponential growth or decay of signals propagating through the network. However, such networks still exhibit fluctuations that grow linearly with the depth of the network, which may impair the training of networks with width comparable to depth. We show analytically that rectangular networks with tanh activations and weights initialized from the ensemble of orthogonal matrices have corresponding preactivation fluctuations which are independent of depth, to leading order in inverse width. Moreover, we demonstrate numerically that, at initialization, all correlators involving the neural tangent kernel (NTK) and its descendants at leading order in inverse width -- which govern the evolution of observables during training -- saturate at a depth of $\sim 20$, rather than growing without bound as in the case of Gaussian initializations. We speculate that this structure preserves finite-width feature learning while reducing overall noise, thus improving both generalization and training speed. We provide some experimental justification by relating empirical measurements of the NTK to the superior performance of deep nonlinear orthogonal networks trained under full-batch gradient descent on the MNIST and CIFAR-10 classification tasks.
</details>
<details>
<summary>摘要</summary>
全连接深度神经网络的权重初始化为独立的高斯分布可以调整到极点，从而防止信号在网络中 exponential 增长或减少。然而，这些网络仍然会表现出线性增长，与网络的宽度相同的深度成比例。我们表述了一种方法，使用矩阵的ensemble initialization，可以使得前activation 干扰的变化独立于深度。此外，我们还证明了在初始化时，NTK和其后代的 correlators 在对width 进行倒数据阶段会到达一个深度约为20，而不是无限制增长，如果使用高斯初始化。我们推测这种结构可以保留宽度与深度相对的特征学习，同时减少总的噪音，从而提高泛化和训练速度。我们通过对NTK的实际测量与深非linear orthogonal网络在MNIST和CIFAR-10分类任务上的性能进行比较，提供了一些实际证明。
</details></li>
</ul>
<hr>
<h2 id="Self-supervised-Representation-Learning-From-Random-Data-Projectors"><a href="#Self-supervised-Representation-Learning-From-Random-Data-Projectors" class="headerlink" title="Self-supervised Representation Learning From Random Data Projectors"></a>Self-supervised Representation Learning From Random Data Projectors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07756">http://arxiv.org/abs/2310.07756</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/layer6ai-labs/lfr">https://github.com/layer6ai-labs/lfr</a></li>
<li>paper_authors: Yi Sui, Tongzi Wu, Jesse C. Cresswell, Ga Wu, George Stein, Xiao Shi Huang, Xiaochen Zhang, Maksims Volkovs</li>
<li>for: 本研究旨在提出一种不需要人工数据增强的自我超级vised representation learning方法，可以应用于多种数据类型和网络架构。</li>
<li>methods: 该方法基于重建随机数据 проекции来学习高质量数据表示。</li>
<li>results: 对多种表示学习任务进行了广泛的评估，并与多个状态时的SSRL基elines进行比较，得到了更高的表示学习性能。<details>
<summary>Abstract</summary>
Self-supervised representation learning~(SSRL) has advanced considerably by exploiting the transformation invariance assumption under artificially designed data augmentations. While augmentation-based SSRL algorithms push the boundaries of performance in computer vision and natural language processing, they are often not directly applicable to other data modalities, and can conflict with application-specific data augmentation constraints. This paper presents an SSRL approach that can be applied to any data modality and network architecture because it does not rely on augmentations or masking. Specifically, we show that high-quality data representations can be learned by reconstructing random data projections. We evaluate the proposed approach on a wide range of representation learning tasks that span diverse modalities and real-world applications. We show that it outperforms multiple state-of-the-art SSRL baselines. Due to its wide applicability and strong empirical results, we argue that learning from randomness is a fruitful research direction worthy of attention and further study.
</details>
<details>
<summary>摘要</summary>
自我监督学习（SSRL）已经得到了很大的进步，通过人工设计的数据增强技术来利用转换不变性假设。而这些增强技术基于的SSRL算法在计算机视觉和自然语言处理领域的性能已经推pushed the boundaries，但它们通常不直接适用于其他数据类型，并且可能与应用特定的数据增强约束矛盾。这篇论文提出了不需要增强或掩蔽的SSRL方法，可以应用于任何数据类型和网络架构。具体来说，我们表明可以通过重建随机数据投影来学习高质量的数据表示。我们对各种表示学习任务进行了广泛的评估，这些任务覆盖了多种Modalities和真实应用。我们发现该方法可以超越多个状态 искусственныйSSRL基准。由于其广泛适用和强实验结果，我们认为学习 randomness 是一个有前途的研究方向，值得关注和进一步研究。
</details></li>
</ul>
<hr>
<h2 id="Stabilizing-Estimates-of-Shapley-Values-with-Control-Variates"><a href="#Stabilizing-Estimates-of-Shapley-Values-with-Control-Variates" class="headerlink" title="Stabilizing Estimates of Shapley Values with Control Variates"></a>Stabilizing Estimates of Shapley Values with Control Variates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07672">http://arxiv.org/abs/2310.07672</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jeremy Goldwasser, Giles Hooker</li>
<li>for: 用于解释黑盒机器学习模型的预测结果</li>
<li>methods: 使用Monte Carlo技术和控制变量来稳定模型解释</li>
<li>results: 在高维数据集上可以带来很大减少Monte Carlo协变性的Shapley估计Here’s the breakdown of each point:1. 用于解释黑盒机器学习模型的预测结果 (for): The paper is written to explain the predictions of blackbox machine learning models.2. 使用Monte Carlo技术和控制变量来稳定模型解释 (methods): The paper proposes using the Monte Carlo technique of control variates to stabilize the model explanations.3. 在高维数据集上可以带来很大减少Monte Carlo协变性的Shapley估计 (results): The paper finds that the proposed method can produce dramatic reductions in the Monte Carlo variability of Shapley estimates on high-dimensional datasets.<details>
<summary>Abstract</summary>
Shapley values are among the most popular tools for explaining predictions of blackbox machine learning models. However, their high computational cost motivates the use of sampling approximations, inducing a considerable degree of uncertainty. To stabilize these model explanations, we propose ControlSHAP, an approach based on the Monte Carlo technique of control variates. Our methodology is applicable to any machine learning model and requires virtually no extra computation or modeling effort. On several high-dimensional datasets, we find it can produce dramatic reductions in the Monte Carlo variability of Shapley estimates.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译为简化字符的中文。<</SYS>>采用Shapley值是黑obox机器学习模型预测的解释工具中最受欢迎的。然而，它们的计算成本较高，使得使用抽象近似法得到的解释带有一定的不确定性。为了稳定这些模型解释，我们提议使用控制SHAP，基于蒙тек控制变量的方法。我们的方法适用于任何机器学习模型，需要virtually no extra computation或模型定制努力。在一些高维数据集上，我们发现它可以生成显著减少Monte Carlo变化的Shapley估计。Note: "virtually no extra computation" is a bit tricky to translate, as "extra computation" is not a direct translation of "extra computation" in Chinese. I have translated it as "需要virtually no extra computation", where "virtually" is used to convey the idea that there is no significant additional computation required.
</details></li>
</ul>
<hr>
<h2 id="The-First-Pathloss-Radio-Map-Prediction-Challenge"><a href="#The-First-Pathloss-Radio-Map-Prediction-Challenge" class="headerlink" title="The First Pathloss Radio Map Prediction Challenge"></a>The First Pathloss Radio Map Prediction Challenge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07658">http://arxiv.org/abs/2310.07658</a></li>
<li>repo_url: None</li>
<li>paper_authors: Çağkan Yapar, Fabian Jaensch, Ron Levie, Gitta Kutyniok, Giuseppe Caire</li>
<li>for: 提出了一个pathloss radio map prediction挑战，以便促进研究和对最新的方法进行公平的比较。</li>
<li>methods: 使用提供的数据集和挑战任务进行预测。</li>
<li>results: 在挑战中，得到了一些结果。<details>
<summary>Abstract</summary>
To foster research and facilitate fair comparisons among recently proposed pathloss radio map prediction methods, we have launched the ICASSP 2023 First Pathloss Radio Map Prediction Challenge. In this short overview paper, we briefly describe the pathloss prediction problem, the provided datasets, the challenge task and the challenge evaluation methodology. Finally, we present the results of the challenge.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本为简化中文。<</SYS>>为促进研究和促进最近提出的路径损失Radio map预测方法的公正比较，我们于ICASSP 2023年首次Radio map预测挑战。在这篇简短概述中，我们 briefly describe the pathloss prediction problem, the provided datasets, the challenge task and the challenge evaluation methodology. Finally, we present the results of the challenge。
</details></li>
</ul>
<hr>
<h2 id="Hypercomplex-Multimodal-Emotion-Recognition-from-EEG-and-Peripheral-Physiological-Signals"><a href="#Hypercomplex-Multimodal-Emotion-Recognition-from-EEG-and-Peripheral-Physiological-Signals" class="headerlink" title="Hypercomplex Multimodal Emotion Recognition from EEG and Peripheral Physiological Signals"></a>Hypercomplex Multimodal Emotion Recognition from EEG and Peripheral Physiological Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07648">http://arxiv.org/abs/2310.07648</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eleonora Lopez, Eleonora Chiarantano, Eleonora Grassucci, Danilo Comminiello</li>
<li>for: 本研究旨在提出一种基于嵌入空间的多modal感情识别方法，以提高现有方法的效果。</li>
<li>methods: 本方法使用了一种新的融合模块，该模块通过在嵌入空间进行参数化的高复杂多元 multiplication 来实现更加有效的融合步骤。</li>
<li>results: 在使用 MAHNOB-HCI 数据集进行分类测试中，本方法的性能超过了现有的多modal状态gartoon network，并且可以更好地识别抑或情绪的强度。<details>
<summary>Abstract</summary>
Multimodal emotion recognition from physiological signals is receiving an increasing amount of attention due to the impossibility to control them at will unlike behavioral reactions, thus providing more reliable information. Existing deep learning-based methods still rely on extracted handcrafted features, not taking full advantage of the learning ability of neural networks, and often adopt a single-modality approach, while human emotions are inherently expressed in a multimodal way. In this paper, we propose a hypercomplex multimodal network equipped with a novel fusion module comprising parameterized hypercomplex multiplications. Indeed, by operating in a hypercomplex domain the operations follow algebraic rules which allow to model latent relations among learned feature dimensions for a more effective fusion step. We perform classification of valence and arousal from electroencephalogram (EEG) and peripheral physiological signals, employing the publicly available database MAHNOB-HCI surpassing a multimodal state-of-the-art network. The code of our work is freely available at https://github.com/ispamm/MHyEEG.
</details>
<details>
<summary>摘要</summary>
多Modal Emotion recognition from physiological signals 已经收到了越来越多的关注，因为不可控制的physiological signals不同于行为反应，可以提供更可靠的信息。现有的深度学习基本方法仍然基于提取的手动特征，没有完全利用神经网络的学习能力，而且常采用单模态方法，而人类情感表达是自然多模态的。在这篇论文中，我们提出了一个嵌入式多模态网络，具有一个新的融合模块，其中包括参数化的超复杂 multiply 运算。在超复杂domain中操作，操作按照代数规则进行，可以模型学习 feature 维度之间的潜在关系，从而实现更有效的融合步骤。我们使用MAHNOB-HCI数据集进行了EEG和周边生物信号的分类，并超越了现有的多模态状态码网络。我们的代码可以免费在https://github.com/ispamm/MHyEEG中下载。
</details></li>
</ul>
<hr>
<h2 id="Deep-Reinforcement-Learning-for-Autonomous-Cyber-Operations-A-Survey"><a href="#Deep-Reinforcement-Learning-for-Autonomous-Cyber-Operations-A-Survey" class="headerlink" title="Deep Reinforcement Learning for Autonomous Cyber Operations: A Survey"></a>Deep Reinforcement Learning for Autonomous Cyber Operations: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07745">http://arxiv.org/abs/2310.07745</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gregory Palmer, Chris Parry, Daniel J. B. Harrold, Chris Willis</li>
<li>for: 这篇论文是为了探讨深度强化学习（DRL）在自动化网络操作（ACO）中的应用和挑战而写的。</li>
<li>methods: 论文使用了DRL方法，并对其应用于ACO问题进行了批判和评估。</li>
<li>results: 论文发现了DRL在ACO问题中的挑战，包括高维状态空间、大多个数动作空间和对抗学习等问题，并提出了一些可能的解决方案。<details>
<summary>Abstract</summary>
The rapid increase in the number of cyber-attacks in recent years raises the need for principled methods for defending networks against malicious actors. Deep reinforcement learning (DRL) has emerged as a promising approach for mitigating these attacks. However, while DRL has shown much potential for cyber-defence, numerous challenges must be overcome before DRL can be applied to autonomous cyber-operations (ACO) at scale. Principled methods are required for environments that confront learners with very high-dimensional state spaces, large multi-discrete action spaces, and adversarial learning. Recent works have reported success in solving these problems individually. There have also been impressive engineering efforts towards solving all three for real-time strategy games. However, applying DRL to the full ACO problem remains an open challenge. Here, we survey the relevant DRL literature and conceptualize an idealised ACO-DRL agent. We provide: i.) A summary of the domain properties that define the ACO problem; ii.) A comprehensive evaluation of the extent to which domains used for benchmarking DRL approaches are comparable to ACO; iii.) An overview of state-of-the-art approaches for scaling DRL to domains that confront learners with the curse of dimensionality, and; iv.) A survey and critique of current methods for limiting the exploitability of agents within adversarial settings from the perspective of ACO. We conclude with open research questions that we hope will motivate future directions for researchers and practitioners working on ACO.
</details>
<details>
<summary>摘要</summary>
随着最近几年的网络攻击数量快速增加，需要有原则的方法来防御网络免受黑客的攻击。深度强化学习（DRL）已经出现为防御攻击的有力的方法。然而，虽然DRL在网络防御方面表现出了很多潜力，但是在自动化网络操作（ACO）中应用DRL仍然是一个开放的挑战。在面临高维状态空间、大多个粒度动作空间和反对学习的环境中，原则的方法是必要的。过去的研究已经解决了这些问题，并且有很多有优的工程努力在解决这些问题上。然而，将DRL应用到整个ACO问题仍然是一个开放的挑战。在这篇文章中，我们对DRL的相关文献进行了抽象，并提出了一个理想化的ACO-DRL代理。我们提供了以下内容：1. ACO问题的域属性的总结，包括域的特点和挑战。2. DRL在不同域上的比较，以及这些域是否与ACO相似的分析。3. 对于面临维度瓶颈的域，State-of-the-art的扩展DRL方法的概述。4. 对于反对学习Setting下的DRL方法的评价和批判。我们结束时，提出了未来研究方向的开放问题，希望能够鼓励未来的研究人员和实践者在ACO领域进行更多的研究。
</details></li>
</ul>
<hr>
<h2 id="Graph-Transformer-Network-for-Flood-Forecasting-with-Heterogeneous-Covariates"><a href="#Graph-Transformer-Network-for-Flood-Forecasting-with-Heterogeneous-Covariates" class="headerlink" title="Graph Transformer Network for Flood Forecasting with Heterogeneous Covariates"></a>Graph Transformer Network for Flood Forecasting with Heterogeneous Covariates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07631">http://arxiv.org/abs/2310.07631</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/JimengShi/FloodGTN_Prediction">https://github.com/JimengShi/FloodGTN_Prediction</a></li>
<li>paper_authors: Jimeng Shi, Vitalii Stebliankin, Zhaonan Wang, Shaowen Wang, Giri Narasimhan</li>
<li>for: 预测洪水，帮助管理洪水风险</li>
<li>methods: 使用图像变换网络（Graph Transformer Network，简称FloodGTN），通过图гра内部网络（Graph Neural Networks，GNNs）和LSTM学习水位的空间Temporal依赖关系，并利用Transformer学习对外部参数（如降雨、潮汐、水利设施等）的注意力。</li>
<li>results: 对南佛瑞达水资源管理区的数据进行应用，实验结果表明，FloodGTN比HEC-RAS模型高度精准，提高了70%，并在运行时间上减少了至少500倍。<details>
<summary>Abstract</summary>
Floods can be very destructive causing heavy damage to life, property, and livelihoods. Global climate change and the consequent sea-level rise have increased the occurrence of extreme weather events, resulting in elevated and frequent flood risk. Therefore, accurate and timely flood forecasting in coastal river systems is critical to facilitate good flood management. However, the computational tools currently used are either slow or inaccurate. In this paper, we propose a Flood prediction tool using Graph Transformer Network (FloodGTN) for river systems. More specifically, FloodGTN learns the spatio-temporal dependencies of water levels at different monitoring stations using Graph Neural Networks (GNNs) and an LSTM. It is currently implemented to consider external covariates such as rainfall, tide, and the settings of hydraulic structures (e.g., outflows of dams, gates, pumps, etc.) along the river. We use a Transformer to learn the attention given to external covariates in computing water levels. We apply the FloodGTN tool to data from the South Florida Water Management District, which manages a coastal area prone to frequent storms and hurricanes. Experimental results show that FloodGTN outperforms the physics-based model (HEC-RAS) by achieving higher accuracy with 70% improvement while speeding up run times by at least 500x.
</details>
<details>
<summary>摘要</summary>
洪水可以很破坏生命、财产和生活方式。全球气候变化和相应的海平面上升已导致极端天气事件的增加，从而增加了洪水风险的频繁性。因此，在海 coastal 河流系统中准确并快速的洪水预测是非常重要的。但是，现有的计算工具都是慢或不准确的。在这篇论文中，我们提出了一种基于图 transformer 网络 (FloodGTN) 的洪水预测工具，用于河流系统。更specifically，FloodGTN 使用图神经网络 (GNNs) 和 LSTM 学习水位在不同监测站之间的空间时间相互关系。它还可以考虑外部covariates，如降雨、潮汐和水利设施（如水库、水闸、泵等）的设置。我们使用 transformer 来学习对外部covariates的注意力。我们在南佛瑞达水资源管理区的数据上应用了 FloodGTN 工具，该区域是频繁遭受风暴和飓风的海岸区。实验结果表明，FloodGTN 在比较HEC-RAS物理模型时，提高了准确率70%，而且提高了运行速度至少500倍。
</details></li>
</ul>
<hr>
<h2 id="Differentiable-Euler-Characteristic-Transforms-for-Shape-Classification"><a href="#Differentiable-Euler-Characteristic-Transforms-for-Shape-Classification" class="headerlink" title="Differentiable Euler Characteristic Transforms for Shape Classification"></a>Differentiable Euler Characteristic Transforms for Shape Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07630">http://arxiv.org/abs/2310.07630</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aidos-lab/dect">https://github.com/aidos-lab/dect</a></li>
<li>paper_authors: Ernst Roell, Bastian Rieck</li>
<li>for: 本文旨在开发一种能够在终端到终端学习ECT的新计算层，以提高ECT在图像和点云分类任务中的性能。</li>
<li>methods: 本文使用了一种新的计算层DECT，它可以在终端到终端学习ECT，并且具有快速和计算效率的优点。</li>
<li>results: 本文的DECT方法在图像和点云分类任务中的性能与更复杂的模型相当，而且还证明了ECT仍然具有同等的 topological 表达能力。<details>
<summary>Abstract</summary>
The Euler Characteristic Transform (ECT) has proven to be a powerful representation, combining geometrical and topological characteristics of shapes and graphs. However, the ECT was hitherto unable to learn task-specific representations. We overcome this issue and develop a novel computational layer that enables learning the ECT in an end-to-end fashion. Our method DECT is fast and computationally efficient, while exhibiting performance on a par with more complex models in both graph and point cloud classification tasks. Moreover, we show that this seemingly unexpressive statistic still provides the same topological expressivity as more complex topological deep learning layers provide.
</details>
<details>
<summary>摘要</summary>
《EulerCharacteristicTransform》（ECT）已经显示出了一种强大的表示方式，将几何和拓扑特征结合在一起。然而，ECT之前无法学习任务特定的表示。我们解决了这个问题，并开发了一种新的计算层，使ECT可以在端到端的方式学习。我们的方法DECT快速和计算效率高，在图像和点云分类任务中表现与更复杂的模型相当。此外，我们还证明ECT仍然提供了与更复杂的拓扑深度学习层相同的拓扑表达能力。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Learning-of-Sea-Surface-Height-Interpolation-from-Multi-variate-Simulated-Satellite-Observations"><a href="#Unsupervised-Learning-of-Sea-Surface-Height-Interpolation-from-Multi-variate-Simulated-Satellite-Observations" class="headerlink" title="Unsupervised Learning of Sea Surface Height Interpolation from Multi-variate Simulated Satellite Observations"></a>Unsupervised Learning of Sea Surface Height Interpolation from Multi-variate Simulated Satellite Observations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07626">http://arxiv.org/abs/2310.07626</a></li>
<li>repo_url: None</li>
<li>paper_authors: Theo Archambault, Arthur Filoche, Anastase Charantonis, Dominique Bereziat, Sylvie Thiria<br>for:这个论文主要是为了研究使用卫星测距数据推算海面高程的方法。methods:论文使用了深度学习网络，利用海面温度数据来提高推算海面高程的精度。results:论文发现，使用深度学习网络可以在不具备训练数据的情况下，使用海面温度数据来提高推算海面高程的精度，并且可以减少41%的根据差值。<details>
<summary>Abstract</summary>
Satellite-based remote sensing missions have revolutionized our understanding of the Ocean state and dynamics. Among them, spaceborne altimetry provides valuable measurements of Sea Surface Height (SSH), which is used to estimate surface geostrophic currents. However, due to the sensor technology employed, important gaps occur in SSH observations. Complete SSH maps are produced by the altimetry community using linear Optimal Interpolations (OI) such as the widely-used Data Unification and Altimeter Combination System (DUACS). However, OI is known for producing overly smooth fields and thus misses some mesostructures and eddies. On the other hand, Sea Surface Temperature (SST) products have much higher data coverage and SST is physically linked to geostrophic currents through advection. We design a realistic twin experiment to emulate the satellite observations of SSH and SST to evaluate interpolation methods. We introduce a deep learning network able to use SST information, and a trainable in two settings: one where we have no access to ground truth during training and one where it is accessible. Our investigation involves a comparative analysis of the aforementioned network when trained using either supervised or unsupervised loss functions. We assess the quality of SSH reconstructions and further evaluate the network's performance in terms of eddy detection and physical properties. We find that it is possible, even in an unsupervised setting to use SST to improve reconstruction performance compared to SST-agnostic interpolations. We compare our reconstructions to DUACS's and report a decrease of 41\% in terms of root mean squared error.
</details>
<details>
<summary>摘要</summary>
卫星远感任务已经革命化了我们对海洋状态和动力学的理解。其中，空间遥感技术提供了海面高程（SSH）的重要测量，用于估算表面地OSTROPIC currents。然而，由于感知技术的限制， SSH 观测存在重要的缺失。complete SSH 地图是通过附近优化技术（OI）生成，如广泛使用的数据统一和探针组合系统（DUACS）。然而，OI 知道生成过于平滑的场景，因此会错过一些中规模的涡动和涝层。一方面，海面温度（SST）产品具有更高的数据覆盖率，SST 与地OSTROPIC currents 是物理相关的。我们设计了一个现实的双子实验，用于模拟卫星观测的 SSH 和 SST。我们引入了一个深度学习网络，可以使用 SST 信息，并在两种设置下训练：一种没有训练数据，一种可以使用训练数据。我们的调查包括比较这些网络在不同的训练设置下的性能，并评估其在涝层检测和物理性能方面的表现。我们发现，即使在无supervision的设置下，也可以使用 SST 提高重建性能，相比于 SST 无关的 interpolations。我们对我们的重建与 DUACS 进行比较，发现root mean squared error 下降41%。
</details></li>
</ul>
<hr>
<h2 id="Prospective-Side-Information-for-Latent-MDPs"><a href="#Prospective-Side-Information-for-Latent-MDPs" class="headerlink" title="Prospective Side Information for Latent MDPs"></a>Prospective Side Information for Latent MDPs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07596">http://arxiv.org/abs/2310.07596</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jeongyeol Kwon, Yonathan Efroni, Shie Mannor, Constantine Caramanis</li>
<li>for: 本研究目的是研究Latent Markov Decision Processes（LMDP）中的强化学习问题，具体来说是在有 prospectivе side information 的情况下。</li>
<li>methods: 本研究使用了Markov decision process（MDP）和partially observed Markov decision process（POMDP）的概念，以及近似搜索和强化学习算法。</li>
<li>results: 本研究发现，在LMDP中，具有prospectivе side information的情况下，任何高效样本算法都会遭受 $\Omega(K^{2&#x2F;3})$ 的违和，而不是标准的 $\Omega(\sqrt{K})$ 下界。此外，本研究还设计了一种具有匹配的Upper bound的算法。<details>
<summary>Abstract</summary>
In many interactive decision-making settings, there is latent and unobserved information that remains fixed. Consider, for example, a dialogue system, where complete information about a user, such as the user's preferences, is not given. In such an environment, the latent information remains fixed throughout each episode, since the identity of the user does not change during an interaction. This type of environment can be modeled as a Latent Markov Decision Process (LMDP), a special instance of Partially Observed Markov Decision Processes (POMDPs). Previous work established exponential lower bounds in the number of latent contexts for the LMDP class. This puts forward a question: under which natural assumptions a near-optimal policy of an LMDP can be efficiently learned? In this work, we study the class of LMDPs with {\em prospective side information}, when an agent receives additional, weakly revealing, information on the latent context at the beginning of each episode. We show that, surprisingly, this problem is not captured by contemporary settings and algorithms designed for partially observed environments. We then establish that any sample efficient algorithm must suffer at least $\Omega(K^{2/3})$-regret, as opposed to standard $\Omega(\sqrt{K})$ lower bounds, and design an algorithm with a matching upper bound.
</details>
<details>
<summary>摘要</summary>
Many interactive decision-making settings have latent and unobserved information that remains fixed. For example, in a dialogue system, the user's preferences may not be fully known. In such an environment, the latent information remains fixed throughout each episode because the user's identity does not change during an interaction. This type of environment can be modeled as a Latent Markov Decision Process (LMDP), a special instance of Partially Observed Markov Decision Processes (POMDPs). Previous work established exponential lower bounds in the number of latent contexts for the LMDP class. This raises a question: under what natural assumptions can a near-optimal policy of an LMDP be efficiently learned?In this work, we study the class of LMDPs with "prospective side information," where an agent receives additional, weakly revealing information on the latent context at the beginning of each episode. We find that this problem is not captured by contemporary settings and algorithms designed for partially observed environments. We then establish that any sample-efficient algorithm must suffer at least $\Omega(K^{2/3})$ regret, rather than the standard $\Omega(\sqrt{K})$ lower bounds, and design an algorithm with a matching upper bound.
</details></li>
</ul>
<hr>
<h2 id="Transformers-for-Green-Semantic-Communication-Less-Energy-More-Semantics"><a href="#Transformers-for-Green-Semantic-Communication-Less-Energy-More-Semantics" class="headerlink" title="Transformers for Green Semantic Communication: Less Energy, More Semantics"></a>Transformers for Green Semantic Communication: Less Energy, More Semantics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07592">http://arxiv.org/abs/2310.07592</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shubhabrata Mukherjee, Cory Beard, Sejun Song</li>
<li>for: 本研究旨在提高含义传输的效率和可靠性，而不是强调个别符号或位元。</li>
<li>methods: 该研究提出了一种新的多目标损失函数 named “Energy-Optimized Semantic Loss” (EOSL)，用于平衡含义损失和能耗。</li>
<li>results: 经过对transformer模型的测试，包括CPU和GPU能耗测试，显示EOSL可以在推理阶段提高含义相似性表现的44%，同时节省90%的能耗。<details>
<summary>Abstract</summary>
Semantic communication aims to transmit meaningful and effective information rather than focusing on individual symbols or bits, resulting in benefits like reduced latency, bandwidth usage, and higher throughput compared to traditional communication. However, semantic communication poses significant challenges due to the need for universal metrics for benchmarking the joint effects of semantic information loss and practical energy consumption. This research presents a novel multi-objective loss function named "Energy-Optimized Semantic Loss" (EOSL), addressing the challenge of balancing semantic information loss and energy consumption. Through comprehensive experiments on transformer models, including CPU and GPU energy usage, it is demonstrated that EOSL-based encoder model selection can save up to 90\% of energy while achieving a 44\% improvement in semantic similarity performance during inference in this experiment. This work paves the way for energy-efficient neural network selection and the development of greener semantic communication architectures.
</details>
<details>
<summary>摘要</summary>
This research proposes a novel multi-objective loss function called "Energy-Optimized Semantic Loss" (EOSL) to address the challenge of balancing semantic information loss and energy consumption. Through comprehensive experiments on transformer models, including CPU and GPU energy usage, it is demonstrated that EOSL-based encoder model selection can save up to 90% of energy while achieving a 44% improvement in semantic similarity performance during inference. This work paves the way for energy-efficient neural network selection and the development of greener semantic communication architectures.Here is the text in Simplified Chinese:semantic communication  aimsto transmit meaningful and effective information instead of focusing on individual symbols or bits, which can result in benefits such as reduced latency, bandwidth usage, and higher throughput compared to traditional communication. However, semantic communication also poses significant challenges, such as the need for universal metrics for benchmarking the joint effects of semantic information loss and practical energy consumption.this research proposes a novel multi-objective loss function called "Energy-Optimized Semantic Loss" (EOSL) to address the challenge of balancing semantic information loss and energy consumption. Through comprehensive experiments on transformer models, including CPU and GPU energy usage, it is demonstrated that EOSL-based encoder model selection can save up to 90% of energy while achieving a 44% improvement in semantic similarity performance during inference. This work paves the way for energy-efficient neural network selection and the development of greener semantic communication architectures.
</details></li>
</ul>
<hr>
<h2 id="Analyzing-Trendy-Twitter-Hashtags-in-the-2022-French-Election"><a href="#Analyzing-Trendy-Twitter-Hashtags-in-the-2022-French-Election" class="headerlink" title="Analyzing Trendy Twitter Hashtags in the 2022 French Election"></a>Analyzing Trendy Twitter Hashtags in the 2022 French Election</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07576">http://arxiv.org/abs/2310.07576</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aamir Mandviwalla, Lake Yin, Boleslaw K. Szymanski</li>
<li>for: 预测社交媒体用户未来活动的模型需要丰富的特征来进行准确预测。许多先进的模型可以生成这些特征，但是它们在庞大数据集上运行时的计算时间常常是禁制的。一些研究表明，简单的含义网络特征可以rich enough使用于机器学习任务。我们提议使用含义网络作为用户级别特征。</li>
<li>methods: 我们使用了一个含义网络，其中有1037个Twitter标签从一个包含370万个推文的2022年法国总统选举相关的词语集中提取出来。我们将标签作为节点，用户之间的互动关系作为 Edge，并将这些关系加权。然后，我们将这个图转换成最大拓扑树，并将最受欢迎的标签作为根节点来构建一个层次结构。最后，我们为每个用户提供一个基于这个树的向量特征。</li>
<li>results: 我们使用了这个semantic特征进行回归预测每个用户的六种情感响应（愤怒、享受、失望等）。我们发现大多数情感的$R^2$值大于0.5，这表明我们的semantic特征具有预测社交媒体用户回归的能力。这些结果表明我们的semantic特征可以考虑在进一步的大数据集上进行预测。<details>
<summary>Abstract</summary>
Regressions trained to predict the future activity of social media users need rich features for accurate predictions. Many advanced models exist to generate such features; however, the time complexities of their computations are often prohibitive when they run on enormous data-sets. Some studies have shown that simple semantic network features can be rich enough to use for regressions without requiring complex computations. We propose a method for using semantic networks as user-level features for machine learning tasks. We conducted an experiment using a semantic network of 1037 Twitter hashtags from a corpus of 3.7 million tweets related to the 2022 French presidential election. A bipartite graph is formed where hashtags are nodes and weighted edges connect the hashtags reflecting the number of Twitter users that interacted with both hashtags. The graph is then transformed into a maximum-spanning tree with the most popular hashtag as its root node to construct a hierarchy amongst the hashtags. We then provide a vector feature for each user based on this tree. To validate the usefulness of our semantic feature we performed a regression experiment to predict the response rate of each user with six emotions like anger, enjoyment, or disgust. Our semantic feature performs well with the regression with most emotions having $R^2$ above 0.5. These results suggest that our semantic feature could be considered for use in further experiments predicting social media response on big data-sets.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>预测社交媒体用户未来活动的回归模型需要丰富的特征来实现准确预测。许多高级模型可以生成这些特征，但是它们在庞大数据集上进行计算时间复杂度经常是禁止的。一些研究表明，使用 semantic network 的简单 semantic 特征可以避免复杂的计算。我们提出一种使用 semantic network 作为用户级别特征的方法。我们在一个包含 3.7 万条提子的推特帖子中选择了 1037 个标签，并将这些标签组织成一个带有权重边的对角raph。然后将这个对角raph变换成一个最大拓扑树，其中最受欢迎的标签作为根节点，以建立一个层次结构。我们然后为每个用户提供一个基于这棵树的 вектор特征。为验证我们的semantic特征的有用性，我们进行了一个回归实验，用于预测每个用户的六种情感响应（愤怒、愉悦、厌恶等）。我们的semantic特征在这些情感回归中表现良好，大多数情感有 $R^2$ 超过 0.5。这些结果表明，我们的semantic特征可以考虑在大数据集上进行进一步的实验。
</details></li>
</ul>
<hr>
<h2 id="Smootheness-Adaptive-Dynamic-Pricing-with-Nonparametric-Demand-Learning"><a href="#Smootheness-Adaptive-Dynamic-Pricing-with-Nonparametric-Demand-Learning" class="headerlink" title="Smootheness-Adaptive Dynamic Pricing with Nonparametric Demand Learning"></a>Smootheness-Adaptive Dynamic Pricing with Nonparametric Demand Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07558">http://arxiv.org/abs/2310.07558</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zeqi Ye, Hansheng Jiang</li>
<li>for: 研究非参数化需求函数的动态价格问题，并聚焦于适应未知Holder平滑参数$\beta$的需求函数。</li>
<li>methods: 提出了一种自相似性条件，以启用适应性。并开发了一种可以在不知道$\beta$情况下实现最佳动态价格算法，并证明了这种算法可以达到最佳 regretBound。</li>
<li>results: 证明了无法适应性的动态价格问题，并提出了一种基于自相似性条件的适应性动态价格算法，该算法可以在不知道$\beta$情况下实现最佳 regretBound。<details>
<summary>Abstract</summary>
We study the dynamic pricing problem where the demand function is nonparametric and H\"older smooth, and we focus on adaptivity to the unknown H\"older smoothness parameter $\beta$ of the demand function. Traditionally the optimal dynamic pricing algorithm heavily relies on the knowledge of $\beta$ to achieve a minimax optimal regret of $\widetilde{O}(T^{\frac{\beta+1}{2\beta+1})$. However, we highlight the challenge of adaptivity in this dynamic pricing problem by proving that no pricing policy can adaptively achieve this minimax optimal regret without knowledge of $\beta$. Motivated by the impossibility result, we propose a self-similarity condition to enable adaptivity. Importantly, we show that the self-similarity condition does not compromise the problem's inherent complexity since it preserves the regret lower bound $\Omega(T^{\frac{\beta+1}{2\beta+1})$. Furthermore, we develop a smoothness-adaptive dynamic pricing algorithm and theoretically prove that the algorithm achieves this minimax optimal regret bound without the prior knowledge $\beta$.
</details>
<details>
<summary>摘要</summary>
我们研究动态价格问题，其中需求函数是非parametric且哈lder平滑的。我们专注于适应未知哈lder平滑度 Parameter $\beta$ 的需求函数。传统上最佳的动态价格算法严重依赖 $\beta$ 的知识，以 дости得最佳的 regret Bound $\widetilde{O}(T^{\frac{\beta+1}{2\beta+1})$。但我们点出了适应性的挑战，并证明了无法适应地achivr 此最佳 regret  bound 的价格策略。我们提出了自similarity 条件，以启动适应性。我们证明了这个条件不会增加问题的内在复杂性，因为它保持了 regret 下界 $\Omega(T^{\frac{\beta+1}{2\beta+1})$。此外，我们开发了一个具有哈lder平滑性的动态价格算法，并证明了这个算法可以 дости得最佳的 regret  bound 无需 $\beta$ 的专门知识。
</details></li>
</ul>
<hr>
<h2 id="Provable-Advantage-of-Parameterized-Quantum-Circuit-in-Function-Approximation"><a href="#Provable-Advantage-of-Parameterized-Quantum-Circuit-in-Function-Approximation" class="headerlink" title="Provable Advantage of Parameterized Quantum Circuit in Function Approximation"></a>Provable Advantage of Parameterized Quantum Circuit in Function Approximation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07528">http://arxiv.org/abs/2310.07528</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhan Yu, Qiuhao Chen, Yuling Jiao, Yinan Li, Xiliang Lu, Xin Wang, Jerry Zhijian Yang</li>
<li>for: 这个论文的目的是分析parameterized quantum circuits（PQCs）在机器学习任务中的表达能力。</li>
<li>methods: 这篇论文使用函数近似的角度来分析PQCs的表达能力，并提供了可重构的PQCs的建构方法，以及在各种函数上进行近似的技术。</li>
<li>results: 这篇论文提供了关于PQCs的表达能力的Explicit Construction，并提供了关于PQCs的近似误差的量化界限，其中误差的大小与PQCs的宽度、深度和可调参数的数量有关。此外，论文还比较了提案的PQCs和深度学习网络在高维平滑函数的近似中的性能，并发现PQCs的模型大小与深度学习网络的模型大小之间存在指数关系。这 suggets a potentially novel avenue for showcasing quantum advantages in quantum machine learning.<details>
<summary>Abstract</summary>
Understanding the power of parameterized quantum circuits (PQCs) in accomplishing machine learning tasks is one of the most important questions in quantum machine learning. In this paper, we analyze the expressivity of PQCs through the lens of function approximation. Previously established universal approximation theorems for PQCs are mainly nonconstructive, leading us to the following question: How large do the PQCs need to be to approximate the target function up to a given error? We exhibit explicit constructions of data re-uploading PQCs for approximating continuous and smooth functions and establish quantitative approximation error bounds in terms of the width, the depth and the number of trainable parameters of the PQCs. To achieve this, we utilize techniques from quantum signal processing and linear combinations of unitaries to construct PQCs that implement multivariate polynomials. We implement global and local approximation techniques using Bernstein polynomials and local Taylor expansion and analyze their performances in the quantum setting. We also compare our proposed PQCs to nearly optimal deep neural networks in approximating high-dimensional smooth functions, showing that the ratio between model sizes of PQC and deep neural networks is exponentially small with respect to the input dimension. This suggests a potentially novel avenue for showcasing quantum advantages in quantum machine learning.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Exploiting-Causal-Graph-Priors-with-Posterior-Sampling-for-Reinforcement-Learning"><a href="#Exploiting-Causal-Graph-Priors-with-Posterior-Sampling-for-Reinforcement-Learning" class="headerlink" title="Exploiting Causal Graph Priors with Posterior Sampling for Reinforcement Learning"></a>Exploiting Causal Graph Priors with Posterior Sampling for Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07518">http://arxiv.org/abs/2310.07518</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mirco Mutti, Riccardo De Santi, Marcello Restelli, Alexander Marx, Giorgia Ramponi</li>
<li>for: 提高强化学习的采样效率，使用 posterior sampling 技术，并利用先验知识来改善采样效率。</li>
<li>methods: 提出一种新的 posterior sampling 方法，使用 causal graph 来表示先验知识，并在这个 graph 上进行 Bayesian 推理。</li>
<li>results: 在 illustrate 的领域中，经过数值评估，提出的 C-PSRL 方法可以强化 posterior sampling 的效率，并且与完整的 causal graph 相比，其效果几乎相同。<details>
<summary>Abstract</summary>
Posterior sampling allows the exploitation of prior knowledge of the environment's transition dynamics to improve the sample efficiency of reinforcement learning. The prior is typically specified as a class of parametric distributions, a task that can be cumbersome in practice, often resulting in the choice of uninformative priors. In this work, we propose a novel posterior sampling approach in which the prior is given as a (partial) causal graph over the environment's variables. The latter is often more natural to design, such as listing known causal dependencies between biometric features in a medical treatment study. Specifically, we propose a hierarchical Bayesian procedure, called C-PSRL, simultaneously learning the full causal graph at the higher level and the parameters of the resulting factored dynamics at the lower level. For this procedure, we provide an analysis of its Bayesian regret, which explicitly connects the regret rate with the degree of prior knowledge. Our numerical evaluation conducted in illustrative domains confirms that C-PSRL strongly improves the efficiency of posterior sampling with an uninformative prior while performing close to posterior sampling with the full causal graph.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用 posterior sampling 可以利用环境转移动力学的先前知识来提高强化学习的样本效率。先前通常是指定为一类 Parametric 分布，这可以在实践中是困难的，常导致选择不具有信息的先前。在这种工作中，我们提出了一种新的 posterior sampling 方法，在该方法中，先前是表示环境变量的 causal 图。这对于设计是更自然的，例如在医疗治疗研究中列出了知道的生物特征相互关系。我们提出了一种层次 Bayesian 过程，称为 C-PSRL，该过程同时学习全部 causal 图和其导致的结果的 factored 动力学参数。我们提供了 Bayesian  regret 的分析，其直接连接了 regret 率与先前知识的度量。我们的数值评估在演示领域中表明，C-PSRL 可以大幅提高 posterior sampling 的效率，同时与完整的 causal 图相似。Note: " Simplified Chinese" is a romanization of the Chinese language, which is used to represent the language in the Latin alphabet. It is not a translation of the text into Chinese characters.
</details></li>
</ul>
<hr>
<h2 id="Model-based-Clustering-of-Individuals’-Ecological-Momentary-Assessment-Time-series-Data-for-Improving-Forecasting-Performance"><a href="#Model-based-Clustering-of-Individuals’-Ecological-Momentary-Assessment-Time-series-Data-for-Improving-Forecasting-Performance" class="headerlink" title="Model-based Clustering of Individuals’ Ecological Momentary Assessment Time-series Data for Improving Forecasting Performance"></a>Model-based Clustering of Individuals’ Ecological Momentary Assessment Time-series Data for Improving Forecasting Performance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07491">http://arxiv.org/abs/2310.07491</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mandani Ntekouli, Gerasimos Spanakis, Lourens Waldorp, Anne Roefs</li>
<li>for: 这个研究旨在使用时间序列数据进行ecological momentary assessment (EMA)，并使用集成分析方法来描述个人情绪行为。</li>
<li>methods: 这个研究使用了两种模型基于的集成方法，一是使用个人化模型中提取的参数，另一是根据模型基于的预测性能进行优化。</li>
<li>results: 研究发现，使用性能为基准的集成方法得到了最好的结果，在所有评估指标上都超过了个人化、全部一起和随机分组的基准。<details>
<summary>Abstract</summary>
Through Ecological Momentary Assessment (EMA) studies, a number of time-series data is collected across multiple individuals, continuously monitoring various items of emotional behavior. Such complex data is commonly analyzed in an individual level, using personalized models. However, it is believed that additional information of similar individuals is likely to enhance these models leading to better individuals' description. Thus, clustering is investigated with an aim to group together the most similar individuals, and subsequently use this information in group-based models in order to improve individuals' predictive performance. More specifically, two model-based clustering approaches are examined, where the first is using model-extracted parameters of personalized models, whereas the second is optimized on the model-based forecasting performance. Both methods are then analyzed using intrinsic clustering evaluation measures (e.g. Silhouette coefficients) as well as the performance of a downstream forecasting scheme, where each forecasting group-model is devoted to describe all individuals belonging to one cluster. Among these, clustering based on performance shows the best results, in terms of all examined evaluation measures. As another level of evaluation, those group-models' performance is compared to three baseline scenarios, the personalized, the all-in-one group and the random group-based concept. According to this comparison, the superiority of clustering-based methods is again confirmed, indicating that the utilization of group-based information could be effectively enhance the overall performance of all individuals' data.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Nonlinear-embeddings-for-conserving-Hamiltonians-and-other-quantities-with-Neural-Galerkin-schemes"><a href="#Nonlinear-embeddings-for-conserving-Hamiltonians-and-other-quantities-with-Neural-Galerkin-schemes" class="headerlink" title="Nonlinear embeddings for conserving Hamiltonians and other quantities with Neural Galerkin schemes"></a>Nonlinear embeddings for conserving Hamiltonians and other quantities with Neural Galerkin schemes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07485">http://arxiv.org/abs/2310.07485</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paul Schwerdtner, Philipp Schulze, Jules Berman, Benjamin Peherstorfer</li>
<li>for: 这个论文关注在解析方程的解场 Solution Fields 中的量保守问题，特别是使用深度网络来近似解场。</li>
<li>methods: 该方法基于Dirac–Frenkel变分原理，逐步在时间上训练非线性参数化。</li>
<li>results: 实验表明，该方法可以保持量的精度，并且可以与标准的显式和隐式时间推导方法结合使用。<details>
<summary>Abstract</summary>
This work focuses on the conservation of quantities such as Hamiltonians, mass, and momentum when solution fields of partial differential equations are approximated with nonlinear parametrizations such as deep networks. The proposed approach builds on Neural Galerkin schemes that are based on the Dirac--Frenkel variational principle to train nonlinear parametrizations sequentially in time. We first show that only adding constraints that aim to conserve quantities in continuous time can be insufficient because the nonlinear dependence on the parameters implies that even quantities that are linear in the solution fields become nonlinear in the parameters and thus are challenging to discretize in time. Instead, we propose Neural Galerkin schemes that compute at each time step an explicit embedding onto the manifold of nonlinearly parametrized solution fields to guarantee conservation of quantities. The embeddings can be combined with standard explicit and implicit time integration schemes. Numerical experiments demonstrate that the proposed approach conserves quantities up to machine precision.
</details>
<details>
<summary>摘要</summary>
我们的研究探讨了使用深度网络作为非线性参数化方法时，对于偏微分方程解场的保守量的问题。我们的方法基于Neural Galerkin方法，该方法基于Dirac--Frenkel变量原理来逐步在时间上训练非线性参数化。我们发现，只要添加保守量的约束并不足以保证量的保守，因为非线性参数的依赖关系使得解场中的量变为非线性函数，这使得在时间上的积分变得困难。因此，我们提议使用Neural Galerkin方法来在每个时间步骤中计算非线性参数化解场的明确嵌入，以保证量的保守。这些嵌入可以与标准的显式和隐式时间积分方法结合使用。我们的numerical experiments表明，我们的方法可以保证量的保守，并且和标准方法相比，可以提高计算效率。
</details></li>
</ul>
<hr>
<h2 id="Automatic-Sensor-free-Affect-Detection-A-Systematic-Literature-Review"><a href="#Automatic-Sensor-free-Affect-Detection-A-Systematic-Literature-Review" class="headerlink" title="Automatic Sensor-free Affect Detection: A Systematic Literature Review"></a>Automatic Sensor-free Affect Detection: A Systematic Literature Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13711">http://arxiv.org/abs/2310.13711</a></li>
<li>repo_url: None</li>
<li>paper_authors: Felipe de Morais, Diógines Goldoni, Tiago Kautzmann, Rodrigo da Silva, Patricia A. Jaques</li>
<li>for: This paper provides a comprehensive literature review on sensor-free affect detection in computer-based learning environments (CBLEs) to enhance learning outcomes.</li>
<li>methods: The paper reviews the most frequently identified affective states, methodologies and techniques employed for sensor development, defining attributes of CBLEs and data samples, and key research trends.</li>
<li>results: The paper highlights the consistent performance of the models and the application of advanced machine learning techniques, but notes that there is ample scope for future research, including enhancing model performance, collecting more samples of underrepresented emotions, and refining model development practices.Here is the information in Simplified Chinese text:</li>
<li>for: 这篇论文为了提高学习 outcome，提供了计算机基础学习环境（CBLEs）中无感器情感探测的全面文献评论。</li>
<li>methods: 论文评论了最常见的情感状态，感应器开发方法和技术，定义CBLE和数据样本的特征，以及主要的研究趋势。</li>
<li>results: 论文指出了模型的一致性表现和应用先进机器学习技术，但还有充足的发展空间，包括提高模型性能，收集更多的异常情感样本，并规范模型开发方法。<details>
<summary>Abstract</summary>
Emotions and other affective states play a pivotal role in cognition and, consequently, the learning process. It is well-established that computer-based learning environments (CBLEs) that can detect and adapt to students' affective states can enhance learning outcomes. However, practical constraints often pose challenges to the deployment of sensor-based affect detection in CBLEs, particularly for large-scale or long-term applications. As a result, sensor-free affect detection, which exclusively relies on logs of students' interactions with CBLEs, emerges as a compelling alternative. This paper provides a comprehensive literature review on sensor-free affect detection. It delves into the most frequently identified affective states, the methodologies and techniques employed for sensor development, the defining attributes of CBLEs and data samples, as well as key research trends. Despite the field's evident maturity, demonstrated by the consistent performance of the models and the application of advanced machine learning techniques, there is ample scope for future research. Potential areas for further exploration include enhancing the performance of sensor-free detection models, amassing more samples of underrepresented emotions, and identifying additional emotions. There is also a need to refine model development practices and methods. This could involve comparing the accuracy of various data collection techniques, determining the optimal granularity of duration, establishing a shared database of action logs and emotion labels, and making the source code of these models publicly accessible. Future research should also prioritize the integration of models into CBLEs for real-time detection, the provision of meaningful interventions based on detected emotions, and a deeper understanding of the impact of emotions on learning.
</details>
<details>
<summary>摘要</summary>
感情和其他情感状态在认知过程中扮演着关键性角色，因此在学习过程中也具有重要作用。已经证明了通过检测和适应学生情感状态的计算机基础学习环境（CBLE）可以提高学习成果。然而，实际应用中的偏见和限制常常使得感知基础的情感检测成为实现的瓶颈。因此，不需要感知设备的情感检测（sensor-free affect detection）成为了一种有前途的alternative。本文提供了关于情感检测的完整的文献综述，包括最常见的情感状态、情感检测器的开发方法和技术、CBLE和数据样本的特点以及主要的研究趋势。尽管领域的成熔度已经明显，表明模型的稳定性和高级机器学习技术的应用，但是还有很多可能的发展方向。未来研究应该强调检测模型的性能提升、更多的弱化情感样本收集、更多的情感状态检测以及模型开发方法的优化。此外，将模型集成到CBLE中进行实时检测，为检测到的情感状态提供有意义的 intervención，以及更深入地理解情感对学习的影响，也是未来研究的重要方向。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Predicts-Biomarker-Status-and-Discovers-Related-Histomorphology-Characteristics-for-Low-Grade-Glioma"><a href="#Deep-Learning-Predicts-Biomarker-Status-and-Discovers-Related-Histomorphology-Characteristics-for-Low-Grade-Glioma" class="headerlink" title="Deep Learning Predicts Biomarker Status and Discovers Related Histomorphology Characteristics for Low-Grade Glioma"></a>Deep Learning Predicts Biomarker Status and Discovers Related Histomorphology Characteristics for Low-Grade Glioma</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07464">http://arxiv.org/abs/2310.07464</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zijie Fang, Yihan Liu, Yifeng Wang, Xiangyang Zhang, Yang Chen, Changjing Cai, Yiyang Lin, Ying Han, Zhi Wang, Shan Zeng, Hong Shen, Jun Tan, Yongbing Zhang<br>for:多种低度 glioma (LGG) 诊断和治疗中不可或缺的一部分是生物标志物 detection。但现有的LGG生物标志物 detection方法依赖于成本高、复杂的分子遗传测试，需要专业人员分析结果，并且经常报告了内部repeatability。methods:我们提出了一种可读性深度学习管道，基于多例学习（MIL）框架的多生物标志物 Histomorphology Discoverer（Multi-Beholder）模型，可以使用染色和抹平板扫描图像来预测LGG中五个生物标志物的状态。特别是通过 incorporating一类分类into MIL framework，实现了准确的实例 Pseudo-labeling，以便使用板块级别标签进行实例级别监督，从而提高生物标志物预测性能。results:Multi-Beholder在两个组合（n&#x3D;607）中显示出了出色的预测性能和普适性（AUROC&#x3D;0.6469-0.9735）。此外，Multi-Beholder的极佳可读性使得可以发现生物标志物状态和 histomorphology 特征之间的量化和质量相关性。我们的管道不仅提供了一种新的生物标志物预测方法，推进了LGG患者的分子治疗的可采用性，而且可以促进分子功能和LGG进程的新机制发现。<details>
<summary>Abstract</summary>
Biomarker detection is an indispensable part in the diagnosis and treatment of low-grade glioma (LGG). However, current LGG biomarker detection methods rely on expensive and complex molecular genetic testing, for which professionals are required to analyze the results, and intra-rater variability is often reported. To overcome these challenges, we propose an interpretable deep learning pipeline, a Multi-Biomarker Histomorphology Discoverer (Multi-Beholder) model based on the multiple instance learning (MIL) framework, to predict the status of five biomarkers in LGG using only hematoxylin and eosin-stained whole slide images and slide-level biomarker status labels. Specifically, by incorporating the one-class classification into the MIL framework, accurate instance pseudo-labeling is realized for instance-level supervision, which greatly complements the slide-level labels and improves the biomarker prediction performance. Multi-Beholder demonstrates superior prediction performance and generalizability for five LGG biomarkers (AUROC=0.6469-0.9735) in two cohorts (n=607) with diverse races and scanning protocols. Moreover, the excellent interpretability of Multi-Beholder allows for discovering the quantitative and qualitative correlations between biomarker status and histomorphology characteristics. Our pipeline not only provides a novel approach for biomarker prediction, enhancing the applicability of molecular treatments for LGG patients but also facilitates the discovery of new mechanisms in molecular functionality and LGG progression.
</details>
<details>
<summary>摘要</summary>
生物标志物检测是低级 Glioma（LGG）诊断和治疗中不可或缺的一部分。然而，现有的LGG生物标志物检测方法依赖于昂贵和复杂的分子遗传学测试，需要专业人员分析结果，并且间误量往往被报告。为了解决这些挑战，我们提出了一个可解释的深度学习管道，即多种生物标志物检测器（Multi-Beholder）模型，基于多例学习（MIL）框架，用于预测LGG五个生物标志物的状态，只需使用染色的整幅干涂图像和板块级别生物标志物状态标签。特别是，通过将一类学习 integrate into MIL框架，实现了准确的实例 pseudo-标签，这种方法可以较好地补充板块级别标签，提高生物标志物预测性能。Multi-Beholder在两个 cohort（n=607）中表现出色，其AUROC值为0.6469-0.9735。此外，Multi-Beholder的优秀可解释性使得可以发现某些生物标志物状态和 Histomorphology 特征之间的数量和质量相关性。我们的管道不仅提供了一种新的生物标志物预测方法，扩展了LGG患者可应用的分子治疗，还可以探索新的分子功能和LGG进程中的新机制。
</details></li>
</ul>
<hr>
<h2 id="Uncovering-ECG-Changes-during-Healthy-Aging-using-Explainable-AI"><a href="#Uncovering-ECG-Changes-during-Healthy-Aging-using-Explainable-AI" class="headerlink" title="Uncovering ECG Changes during Healthy Aging using Explainable AI"></a>Uncovering ECG Changes during Healthy Aging using Explainable AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07463">http://arxiv.org/abs/2310.07463</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ai4healthuol/ecg-aging">https://github.com/ai4healthuol/ecg-aging</a></li>
<li>paper_authors: Gabriel Ott, Yannik Schaubelt, Juan Miguel Lopez Alcaraz, Wilhelm Haverkamp, Nils Strodthoff</li>
<li>for: 这项研究旨在提供更深刻的心脏年龄过程理解，以便更好地诊断心血管健康水平。</li>
<li>methods: 这项研究使用深度学习模型和树式分类模型分析了健康个体的ECG数据，并使用可解释AI技术确定ECG特征或原始信号特征是否能够分类不同年龄组。</li>
<li>results: 研究发现，年龄增长导致呼吸速率下降，并且发现高SDANN值能够区分年轻人和老年人。此外，深度学习模型表明，年龄增长导致P波类型的分布变化，这些发现可能提供传统特征分析方法之外的新的年龄相关ECG变化。<details>
<summary>Abstract</summary>
Cardiovascular diseases remain the leading global cause of mortality. This necessitates a profound understanding of heart aging processes to diagnose constraints in cardiovascular fitness. Traditionally, most of such insights have been drawn from the analysis of electrocardiogram (ECG) feature changes of individuals as they age. However, these features, while informative, may potentially obscure underlying data relationships. In this paper, we employ a deep-learning model and a tree-based model to analyze ECG data from a robust dataset of healthy individuals across varying ages in both raw signals and ECG feature format. Explainable AI techniques are then used to identify ECG features or raw signal characteristics are most discriminative for distinguishing between age groups. Our analysis with tree-based classifiers reveal age-related declines in inferred breathing rates and identifies notably high SDANN values as indicative of elderly individuals, distinguishing them from younger adults. Furthermore, the deep-learning model underscores the pivotal role of the P-wave in age predictions across all age groups, suggesting potential changes in the distribution of different P-wave types with age. These findings shed new light on age-related ECG changes, offering insights that transcend traditional feature-based approaches.
</details>
<details>
<summary>摘要</summary>
In this study, we employ a deep-learning model and a tree-based model to analyze ECG data from a large dataset of healthy individuals across different ages. We use explainable AI techniques to identify the most discriminative ECG features or raw signal characteristics for distinguishing between age groups.Our analysis with tree-based classifiers reveals age-related declines in inferred breathing rates and identifies high SDANN values as indicative of elderly individuals. Additionally, the deep-learning model emphasizes the crucial role of the P-wave in age predictions across all age groups, suggesting potential changes in the distribution of different P-wave types with age.These findings offer new insights into age-related ECG changes, going beyond traditional feature-based approaches.
</details></li>
</ul>
<hr>
<h2 id="ProbTS-A-Unified-Toolkit-to-Probe-Deep-Time-series-Forecasting"><a href="#ProbTS-A-Unified-Toolkit-to-Probe-Deep-Time-series-Forecasting" class="headerlink" title="ProbTS: A Unified Toolkit to Probe Deep Time-series Forecasting"></a>ProbTS: A Unified Toolkit to Probe Deep Time-series Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07446">http://arxiv.org/abs/2310.07446</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiawen Zhang, Xumeng Wen, Shun Zheng, Jia Li, Jiang Bian</li>
<li>for: 这篇论文的目的是将深度学习技术应用到时间序列预测领域，并评估这两个分支之间的不同特性和表现。</li>
<li>methods: 这篇论文使用了两种不同的方法：一种是特定的神经网络架构，另一种是使用进步的深度生成模型进行 probabilistic 预测。</li>
<li>results: 这篇论文使用 ProbTS 工具套件进行比较和评估这两种方法的表现，发现它们在不同的数据enario和方法ological focuses 之间有所不同，并提供了新的研究方向来提高时间序列预测的精度。<details>
<summary>Abstract</summary>
Time-series forecasting serves as a linchpin in a myriad of applications, spanning various domains. With the growth of deep learning, this arena has bifurcated into two salient branches: one focuses on crafting specific neural architectures tailored for time series, and the other harnesses advanced deep generative models for probabilistic forecasting. While both branches have made significant progress, their differences across data scenarios, methodological focuses, and decoding schemes pose profound, yet unexplored, research questions. To bridge this knowledge chasm, we introduce ProbTS, a pioneering toolkit developed to synergize and compare these two distinct branches. Endowed with a unified data module, a modularized model module, and a comprehensive evaluator module, ProbTS allows us to revisit and benchmark leading methods from both branches. The scrutiny with ProbTS highlights their distinct characteristics, relative strengths and weaknesses, and areas that need further exploration. Our analyses point to new avenues for research, aiming for more effective time-series forecasting.
</details>
<details>
<summary>摘要</summary>
时间序列预测作为许多应用领域的核心，其中有两个主要分支：一个是针对时间序列特定的神经网络架构的设计，另一个是利用高级深度生成模型进行 probabilistic 预测。虽然这两个分支都取得了 significiant progress，但是在数据场景、方法重点和解码方案方面存在差异，这些差异仍然尚未得到系统的探索。为了bridging这个知识差距，我们提出了 ProbTS，一个 pioneering 的工具集，用于同时Synergize和比较这两个分支。ProbTS 具有一个统一的数据模块、一个模块化的模型模块和一个全面的评价模块，这使得我们可以重新评估和比较领导的方法从两个分支中。通过 ProbTS 的检验，我们发现了这两个分支的特点、相对优劣点和需要进一步探索的领域。我们的分析指向了新的研究方向，旨在更有效地预测时间序列。
</details></li>
</ul>
<hr>
<h2 id="A-Branched-Deep-Convolutional-Network-for-Forecasting-the-Occurrence-of-Hazes-in-Paris-using-Meteorological-Maps-with-Different-Characteristic-Spatial-Scales"><a href="#A-Branched-Deep-Convolutional-Network-for-Forecasting-the-Occurrence-of-Hazes-in-Paris-using-Meteorological-Maps-with-Different-Characteristic-Spatial-Scales" class="headerlink" title="A Branched Deep Convolutional Network for Forecasting the Occurrence of Hazes in Paris using Meteorological Maps with Different Characteristic Spatial Scales"></a>A Branched Deep Convolutional Network for Forecasting the Occurrence of Hazes in Paris using Meteorological Maps with Different Characteristic Spatial Scales</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07437">http://arxiv.org/abs/2310.07437</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chien Wang</li>
<li>for: 预测浓雾事件的发生</li>
<li>methods: 使用多decadal日地区天气和水文变量作为输入特征，并使用Surface visibility观测数据作为目标进行训练</li>
<li>results: 两支分支架构对抗气雾事件的预测性能有所提高，并在验证和盲测评估中获得了合理的分数。<details>
<summary>Abstract</summary>
A deep learning platform has been developed to forecast the occurrence of the low visibility events or hazes. It is trained by using multi-decadal daily regional maps of various meteorological and hydrological variables as input features and surface visibility observations as the targets. To better preserve the characteristic spatial information of different input features for training, two branched architectures have recently been developed for the case of Paris hazes. These new architectures have improved the performance of the network, producing reasonable scores in both validation and a blind forecasting evaluation using the data of 2021 and 2022 that have not been used in the training and validation.
</details>
<details>
<summary>摘要</summary>
一个深度学习平台已经开发，用于预测普通天气下雾霾或低视野事件的发生。该平台通过使用多个劳伦地域天气和水文变量的多 décennial日aily地图作为输入特征，并将地面可见度观测作为目标。为更好地保留不同输入特征的特征空间信息， reciently 两个分支架构已经开发用于Paris雾霾情况。这两个新架构已经提高了网络的性能，在验证和隐藏预测评估中制造了合理的分数，使用2021和2022年的数据进行验证和预测。Note: "reciently" is a typo, it should be "recently".
</details></li>
</ul>
<hr>
<h2 id="Generalized-Mixture-Model-for-Extreme-Events-Forecasting-in-Time-Series-Data"><a href="#Generalized-Mixture-Model-for-Extreme-Events-Forecasting-in-Time-Series-Data" class="headerlink" title="Generalized Mixture Model for Extreme Events Forecasting in Time Series Data"></a>Generalized Mixture Model for Extreme Events Forecasting in Time Series Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07435">http://arxiv.org/abs/2310.07435</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jincheng Wang, Yue Gao</li>
<li>for: 这个研究是为了提高时间序列预测中的极端值预测性能，特别是在气象预测、交通管理和股票价格预测等领域。</li>
<li>methods: 本研究使用了一个新的深度混合模型，即深度极点混合模型（DEMMA），其包括两个主要模块：1）一个基于折衣分布的通用混合分布，和2）一个基于自动编码器的LSTM特征提取器和时间注意力机制。</li>
<li>results: 研究使用多个真实世界的雨量数据展示了DEMMA模型的效果，并证明了它在模型极端值预测方面的改进。<details>
<summary>Abstract</summary>
Time Series Forecasting (TSF) is a widely researched topic with broad applications in weather forecasting, traffic control, and stock price prediction. Extreme values in time series often significantly impact human and natural systems, but predicting them is challenging due to their rare occurrence. Statistical methods based on Extreme Value Theory (EVT) provide a systematic approach to modeling the distribution of extremes, particularly the Generalized Pareto (GP) distribution for modeling the distribution of exceedances beyond a threshold. To overcome the subpar performance of deep learning in dealing with heavy-tailed data, we propose a novel framework to enhance the focus on extreme events. Specifically, we propose a Deep Extreme Mixture Model with Autoencoder (DEMMA) for time series prediction. The model comprises two main modules: 1) a generalized mixture distribution based on the Hurdle model and a reparameterized GP distribution form independent of the extreme threshold, 2) an Autoencoder-based LSTM feature extractor and a quantile prediction module with a temporal attention mechanism. We demonstrate the effectiveness of our approach on multiple real-world rainfall datasets.
</details>
<details>
<summary>摘要</summary>
时间序列预测（TSF）是广泛研究的主题，具有广泛的应用于天气预报、交通管理和股票价格预测等领域。时间序列中的极值事件经常对人类和自然系统产生深远的影响，但预测它们却是具有挑战性的，主要因为它们的发生率很低。基于极值理论（EVT）的统计方法可以系统地模型极值事件的分布，特别是通过 Generalized Pareto（GP）分布来模型超过阈值的出现。为了解决深度学习在处理具有极大尾部的数据时的表现不佳，我们提出了一种新的框架来强调极值事件。具体来说，我们提出了一种基于极值分布的深度混合模型（DEMMA），用于时间序列预测。该模型包括两个主要模块：1）基于极值模型和独立于极值阈值的GP分布的扩展 mixture distribution；2）基于自适应神经网络和时间注意机制的LSTM特征提取器和时间预测模块。我们在多个实际降水数据集上证明了我们的方法的效果。
</details></li>
</ul>
<hr>
<h2 id="Non-backtracking-Graph-Neural-Networks"><a href="#Non-backtracking-Graph-Neural-Networks" class="headerlink" title="Non-backtracking Graph Neural Networks"></a>Non-backtracking Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07430">http://arxiv.org/abs/2310.07430</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seonghyun Park, Narae Ryu, Gahee Kim, Dongyeop Woo, Se-Young Yun, Sungsoo Ahn</li>
<li>for: 提高大规模图像 neural network 的精度和计算效率，解决 message-passing 更新方法中的循环问题。</li>
<li>methods: 提出非循环回访图 neural network (NBA-GNN)，通过不包含之前访问过的节点的信息来更新消息，从而解决 message-passing 更新方法中的循环问题。</li>
<li>results: 通过实验证明，NBA-GNN 可以有效地解决循环问题，提高大规模图像 neural network 的精度和计算效率，并在长距离图像和推理节点分类任务上表现出色。<details>
<summary>Abstract</summary>
The celebrated message-passing updates for graph neural networks allow the representation of large-scale graphs with local and computationally tractable updates. However, the local updates suffer from backtracking, i.e., a message flows through the same edge twice and revisits the previously visited node. Since the number of message flows increases exponentially with the number of updates, the redundancy in local updates prevents the graph neural network from accurately recognizing a particular message flow for downstream tasks. In this work, we propose to resolve such a redundancy via the non-backtracking graph neural network (NBA-GNN) that updates a message without incorporating the message from the previously visited node. We further investigate how NBA-GNN alleviates the over-squashing of GNNs, and establish a connection between NBA-GNN and the impressive performance of non-backtracking updates for stochastic block model recovery. We empirically verify the effectiveness of our NBA-GNN on long-range graph benchmark and transductive node classification problems.
</details>
<details>
<summary>摘要</summary>
“著名的消息传递更新方法为图 neuronal network 提供了可 representations of large-scale graphs 的地方和计算可 tractable 的更新。然而，本地更新受到回tracking的影响，即消息流经同一个边两次并返回已经访问过的节点。由于消息流的数量 exponentially 增加与更新数量的关系，这种 redundancy 在图 neuronal network 中阻碍了下游任务的准确识别。在这项工作中，我们提议了解决这种 redundancy 的非回tracking图 neuronal network (NBA-GNN)，它在更新消息时不 incorporate 先前访问过的节点的消息。我们进一步调查了 NBA-GNN 如何缓解 GNN 的过 compressing 问题，并建立了 NB 更新与 Stochastic Block Model 回归的连接。我们employmically 验证了我们的 NBA-GNN 在长距离图 bencmark 和推uctive node classification 问题上的效果。”Note: Simplified Chinese is a romanization of Chinese, and the translation may not be perfect. The original text is in English, and the translation is provided for reference only.
</details></li>
</ul>
<hr>
<h2 id="Quantum-Enhanced-Forecasting-Leveraging-Quantum-Gramian-Angular-Field-and-CNNs-for-Stock-Return-Predictions"><a href="#Quantum-Enhanced-Forecasting-Leveraging-Quantum-Gramian-Angular-Field-and-CNNs-for-Stock-Return-Predictions" class="headerlink" title="Quantum-Enhanced Forecasting: Leveraging Quantum Gramian Angular Field and CNNs for Stock Return Predictions"></a>Quantum-Enhanced Forecasting: Leveraging Quantum Gramian Angular Field and CNNs for Stock Return Predictions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07427">http://arxiv.org/abs/2310.07427</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhengmeng Xu, Hai Lin</li>
<li>for: 该研究旨在提高时间序列预测精度，使用量子计算技术与深度学习结合。</li>
<li>methods: 该方法使用量子电路特定设计，将时间序列数据转换为适合 convolutional Neural Network (CNN) 训练的二维图像。与传统的 Gramian Angular Field (GAF) 方法不同，QGAF 方法不需要数据Normalization 和 inverse cosine 计算，简化了时间序列数据转换为图像的过程。</li>
<li>results: 对三个主要股市的数据进行了实验：中国A股市场、香港股市和美国股市。实验结果表明，相比传统GAF方法，QGAF方法在时间序列预测精度方面具有显著提高，降低了预测错误的平均绝对值（MAE）和平均方差（MSE） Errors by an average of 25% for MAE and 48% for MSE.<details>
<summary>Abstract</summary>
We propose a time series forecasting method named Quantum Gramian Angular Field (QGAF). This approach merges the advantages of quantum computing technology with deep learning, aiming to enhance the precision of time series classification and forecasting. We successfully transformed stock return time series data into two-dimensional images suitable for Convolutional Neural Network (CNN) training by designing specific quantum circuits. Distinct from the classical Gramian Angular Field (GAF) approach, QGAF's uniqueness lies in eliminating the need for data normalization and inverse cosine calculations, simplifying the transformation process from time series data to two-dimensional images. To validate the effectiveness of this method, we conducted experiments on datasets from three major stock markets: the China A-share market, the Hong Kong stock market, and the US stock market. Experimental results revealed that compared to the classical GAF method, the QGAF approach significantly improved time series prediction accuracy, reducing prediction errors by an average of 25% for Mean Absolute Error (MAE) and 48% for Mean Squared Error (MSE). This research confirms the potential and promising prospects of integrating quantum computing with deep learning techniques in financial time series forecasting.
</details>
<details>
<summary>摘要</summary>
我们提出了一种名为量子agramian angular field（QGAF）的时间序列预测方法。这种方法结合了量子计算技术和深度学习，目的是提高时间序列分类和预测精度。我们成功地将股票回报时间序列数据转化为适合深度神经网络训练的二维图像，通过设计专门的量子电路。与 классиical Gramian angular field（GAF）方法不同，QGAF方法不需要数据Normalization和反归cosine计算，从时间序列数据转化到二维图像的过程被简化。为验证这种方法的有效性，我们在三个主要股票市场的数据上进行了实验：中国A股市场、香港股市场和美国股市场。实验结果表明，相比 классиical GAF方法，QGAF方法在时间序列预测精度方面有显著提高，降低预测错误的平均绝对值（MAE）和平均方差（MSE） errors by an average of 25% and 48%, respectively.这项研究证明了将量子计算技术与深度学习技术结合在金融时间序列预测中的潜在和未来的投资机会。
</details></li>
</ul>
<hr>
<h2 id="Deep-Kernel-and-Image-Quality-Estimators-for-Optimizing-Robotic-Ultrasound-Controller-using-Bayesian-Optimization"><a href="#Deep-Kernel-and-Image-Quality-Estimators-for-Optimizing-Robotic-Ultrasound-Controller-using-Bayesian-Optimization" class="headerlink" title="Deep Kernel and Image Quality Estimators for Optimizing Robotic Ultrasound Controller using Bayesian Optimization"></a>Deep Kernel and Image Quality Estimators for Optimizing Robotic Ultrasound Controller using Bayesian Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07392">http://arxiv.org/abs/2310.07392</a></li>
<li>repo_url: None</li>
<li>paper_authors: Deepak Raina, SH Chandrashekhara, Richard Voyles, Juan Wachs, Subir Kumar Saha</li>
<li>for: 帮助自动化医疗影像成像，减少医生的工作负担</li>
<li>methods: 使用神经网络学习低维度kernels，并使用这些kernels进行bayesian优化</li>
<li>results: 实现50%的样本效率提高，并且这种性能提高是不受具体训练数据的影响Here’s a more detailed explanation of each point:</li>
<li>for: The paper is written to help improve the efficiency of autonomous robotic ultrasound imaging, by reducing the workload of sonographers.</li>
<li>methods: The paper proposes using a neural network to learn a low-dimensional kernel in Bayesian optimization, which is a sample-efficient optimization framework. The neural network is trained using probe and image data acquired during the procedure.</li>
<li>results: The paper shows that the proposed framework can achieve over 50% increase in sample efficiency for 6D control of the robotized probe, and this performance enhancement is independent of the specific training dataset, demonstrating inter-patient adaptability.<details>
<summary>Abstract</summary>
Ultrasound is a commonly used medical imaging modality that requires expert sonographers to manually maneuver the ultrasound probe based on the acquired image. Autonomous Robotic Ultrasound (A-RUS) is an appealing alternative to this manual procedure in order to reduce sonographers' workload. The key challenge to A-RUS is optimizing the ultrasound image quality for the region of interest across different patients. This requires knowledge of anatomy, recognition of error sources and precise probe position, orientation and pressure. Sample efficiency is important while optimizing these parameters associated with the robotized probe controller. Bayesian Optimization (BO), a sample-efficient optimization framework, has recently been applied to optimize the 2D motion of the probe. Nevertheless, further improvements are needed to improve the sample efficiency for high-dimensional control of the probe. We aim to overcome this problem by using a neural network to learn a low-dimensional kernel in BO, termed as Deep Kernel (DK). The neural network of DK is trained using probe and image data acquired during the procedure. The two image quality estimators are proposed that use a deep convolution neural network and provide real-time feedback to the BO. We validated our framework using these two feedback functions on three urinary bladder phantoms. We obtained over 50% increase in sample efficiency for 6D control of the robotized probe. Furthermore, our results indicate that this performance enhancement in BO is independent of the specific training dataset, demonstrating inter-patient adaptability.
</details>
<details>
<summary>摘要</summary>
ultrasound 是一种广泛使用的医疗影像模式，需要专业的医疗人员手动操作ultrasound 探针。 autonomous Robotic Ultrasound (A-RUS) 是一种可能的解决方案，以减轻医疗人员的工作负担。 然而，要OPTIMIZE 影像质量在不同的病人中是主要挑战。这需要了解解剖学、识别错误来源以及精确的探针位置、orientation 和压力。 sample efficiency 是重要的，而且OPTIMIZING 这些参数与 robotized 探针控制器相关。 Bayesian Optimization (BO) 是一种样本效率的优化框架，已经应用于优化2D 探针运动。然而，需要进一步改进以提高高维度控制的样本效率。我们想使用神经网络学习一个低维度kernel，称为Deep Kernel (DK)。神经网络的 DK 在 BO 中训练，使用在过程中获取的探针和影像数据。我们提出了两种影像质量估计器，使用深度卷积神经网络，并提供实时反馈给 BO。我们验证了我们的框架使用这两种反馈函数，在三个尿道模拟中获得了50%以上的样本效率提高。此外，我们的结果表明，这种性能改进在 BO 中是无关特定训练集的，表示可以在不同的病人中进行适应。
</details></li>
</ul>
<hr>
<h2 id="Experimental-quantum-natural-gradient-optimization-in-photonics"><a href="#Experimental-quantum-natural-gradient-optimization-in-photonics" class="headerlink" title="Experimental quantum natural gradient optimization in photonics"></a>Experimental quantum natural gradient optimization in photonics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07371">http://arxiv.org/abs/2310.07371</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yizhi Wang, Shichuan Xue, Yaxuan Wang, Jiangfang Ding, Weixu Shi, Dongyang Wang, Yong Liu, Yingwen Liu, Xiang Fu, Guangyao Huang, Anqi Huang, Mingtang Deng, Junjie Wu</li>
<li>for: 研究可行量子计算机在噪响中等级量子时代的实用应用。</li>
<li>methods: 使用可 Parameterized 量子圈和类别优化器，实现可行量子计算机的实用应用。</li>
<li>results: 比用Gradient-free和普通的梯度下降方法更快地 converges 和更好地避免地点极值，从而降低了电路执行的成本。在光学设备上实验ally 证明了这一点，并实现了He-H$^+$ 离子的分解曲线，达到了化学精度。<details>
<summary>Abstract</summary>
Variational quantum algorithms (VQAs) combining the advantages of parameterized quantum circuits and classical optimizers, promise practical quantum applications in the Noisy Intermediate-Scale Quantum era. The performance of VQAs heavily depends on the optimization method. Compared with gradient-free and ordinary gradient descent methods, the quantum natural gradient (QNG), which mirrors the geometric structure of the parameter space, can achieve faster convergence and avoid local minima more easily, thereby reducing the cost of circuit executions. We utilized a fully programmable photonic chip to experimentally estimate the QNG in photonics for the first time. We obtained the dissociation curve of the He-H$^+$ cation and achieved chemical accuracy, verifying the outperformance of QNG optimization on a photonic device. Our work opens up a vista of utilizing QNG in photonics to implement practical near-term quantum applications.
</details>
<details>
<summary>摘要</summary>
“变量量量算法（VQA），结合参数化量Circuit和классиical优化器的优点，承诺实现量子应用程序在噪声中间量子时代。VQA的性能强调优化方法。相比于梯度值为零和普通梯度下降方法，量子自然梯度（QNG），它反映参数空间的几何结构，可以更快地收敛和更容易避免地陷入地点，从而降低环境执行成本。我们利用了完全可编程的光学芯片进行实验，对光学中的QNG进行了实验性估计。我们获得了He-H$^+$离子的解键曲线，并达到了化学精度，证明了QNG优化在光学设备上的超越性。我们的工作开启了在光学中使用QNG实现实用的近期量子应用的可能性。”Note that Simplified Chinese is a more informal and spoken version of Chinese, and it may not be appropriate for all formal situations or audiences. If you need a more formal translation, you may want to consider using Traditional Chinese or Classical Chinese.
</details></li>
</ul>
<hr>
<h2 id="Orthogonal-Random-Features-Explicit-Forms-and-Sharp-Inequalities"><a href="#Orthogonal-Random-Features-Explicit-Forms-and-Sharp-Inequalities" class="headerlink" title="Orthogonal Random Features: Explicit Forms and Sharp Inequalities"></a>Orthogonal Random Features: Explicit Forms and Sharp Inequalities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07370">http://arxiv.org/abs/2310.07370</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nizar Demni, Hachem Kadri</li>
<li>for: 这个论文是为了扩大kernel方法的扩展而引入随机特征。特别是使用随机傅рие纹理和正交随机纹理来 aproximate popular Gaussian kernel。</li>
<li>methods: 这篇论文使用了随机傅рие纹理和正交随机纹理来 aproximate Gaussian kernel。</li>
<li>results: 这篇论文分析了随机特征的偏见和方差，并提供了正常化essel函数的准确表达和锐化均衡 bound，证明了正交随机特征比随机傅рие纹理更有用。<details>
<summary>Abstract</summary>
Random features have been introduced to scale up kernel methods via randomization techniques. In particular, random Fourier features and orthogonal random features were used to approximate the popular Gaussian kernel. The former is performed by a random Gaussian matrix and leads exactly to the Gaussian kernel after averaging. In this work, we analyze the bias and the variance of the kernel approximation based on orthogonal random features which makes use of Haar orthogonal matrices. We provide explicit expressions for these quantities using normalized Bessel functions and derive sharp exponential bounds supporting the view that orthogonal random features are more informative than random Fourier features.
</details>
<details>
<summary>摘要</summary>
随机特性被引入来扩大kernel方法。特别是随机傅立勃方法和正交随机特性被用来估计流行的加aussian kernel。前者通过随机矩阵来实现，并导致exact Gaussian kernel после均值。在这种工作中，我们分析kernel approximation的偏差和方差基于正交随机特性，使用 Haar正交矩阵。我们提供了Explicit表达式使用 normalized Bessel functions，并 derivsharp exponential bounds，支持我们的观点，即正交随机特性比随机傅立勃方法更有用。
</details></li>
</ul>
<hr>
<h2 id="Improved-Analysis-of-Sparse-Linear-Regression-in-Local-Differential-Privacy-Model"><a href="#Improved-Analysis-of-Sparse-Linear-Regression-in-Local-Differential-Privacy-Model" class="headerlink" title="Improved Analysis of Sparse Linear Regression in Local Differential Privacy Model"></a>Improved Analysis of Sparse Linear Regression in Local Differential Privacy Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07367">http://arxiv.org/abs/2310.07367</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liyang Zhu, Meng Ding, Vaneet Aggarwal, Jinhui Xu, Di Wang</li>
<li>for: 本研究重新审视了含有稀疏参数的线性回归问题在本地隐私（LDP）模型中。现有研究在非互动和串行本地模型中已经关注到了对于$1$-稀疏参数的下界，但扩展到更一般的$k$-稀疏参数的情况是有挑战的。此外，是否存在有效的非互动LDP（NLDP）算法仍然是一个问题。</li>
<li>methods: 我们首先考虑了在$\epsilon$非互动LDP模型中的问题，并提供了$\ell_2$-范数估计误差的下界为$\Omega(\frac{\sqrt{dk\log d}{\sqrt{n}\epsilon})$，其中$n$是样本大小和$d$是特征空间的维度。我们还提出了一种创新的NLDP算法，这是本问题的首次解决方案。这个算法还生成了一个高效的估计器作为副产品。我们的算法实现了对于各种数据的上界为$\tilde{O}({\frac{d\sqrt{k}{\sqrt{n}\epsilon})$，可以通过增加$O(\sqrt{d})$的因子进一步提高。在串行互动LDP模型中，我们显示了类似的下界。</li>
<li>results: 我们的结论表明，在稀疏线性回归问题中，非互动LDP模型和中心DP模型之间存在深刻的差异。<details>
<summary>Abstract</summary>
In this paper, we revisit the problem of sparse linear regression in the local differential privacy (LDP) model. Existing research in the non-interactive and sequentially local models has focused on obtaining the lower bounds for the case where the underlying parameter is $1$-sparse, and extending such bounds to the more general $k$-sparse case has proven to be challenging. Moreover, it is unclear whether efficient non-interactive LDP (NLDP) algorithms exist. To address these issues, we first consider the problem in the $\epsilon$ non-interactive LDP model and provide a lower bound of $\Omega(\frac{\sqrt{dk\log d}{\sqrt{n}\epsilon})$ on the $\ell_2$-norm estimation error for sub-Gaussian data, where $n$ is the sample size and $d$ is the dimension of the space. We propose an innovative NLDP algorithm, the very first of its kind for the problem. As a remarkable outcome, this algorithm also yields a novel and highly efficient estimator as a valuable by-product. Our algorithm achieves an upper bound of $\tilde{O}({\frac{d\sqrt{k}{\sqrt{n}\epsilon})$ for the estimation error when the data is sub-Gaussian, which can be further improved by a factor of $O(\sqrt{d})$ if the server has additional public but unlabeled data. For the sequentially interactive LDP model, we show a similar lower bound of $\Omega({\frac{\sqrt{dk}{\sqrt{n}\epsilon})$. As for the upper bound, we rectify a previous method and show that it is possible to achieve a bound of $\tilde{O}(\frac{k\sqrt{d}{\sqrt{n}\epsilon})$. Our findings reveal fundamental differences between the non-private case, central DP model, and local DP model in the sparse linear regression problem.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们重新考虑了在本地权限隐私（LDP）模型下的稀疏线性回归问题。现有研究在非交互式和顺序本地模型下都集中在了对于$1$-稀疏的参数下获得下界，并将其推广到更通用的$k$-稀疏 случа子是一项挑战。另外，是否存在高效的非交互式LDP（NLDP）算法也是一个问题。为解决这些问题，我们首先考虑了在$\epsilon$非交互式LDP模型下的问题，并提供了$\ell_2$-范数估计误差的下界为$\Omega(\frac{\sqrt{dk\log d}{\sqrt{n}\epsilon})$，其中$n$是样本大小，$d$是空间维度。我们提出了一种创新的NLDP算法，这是首次为这个问题提出了解决方案。这个算法还生成了一种高效的估计器作为副产品。我们的算法在SUB-高分布数据上达到了$\tilde{O}({\frac{d\sqrt{k}{\sqrt{n}\epsilon})$的估计误差上界，可以通过增加$O(\sqrt{d})$的因子进一步改进。如果服务器拥有额外的公共 yet 无标签数据，那么我们的算法可以在这些数据上进行改进，从而提高估计误差的Bound。在Sequentially交互式LDP模型下，我们显示了类似的下界为$\Omega(\frac{\sqrt{dk}{\sqrt{n}\epsilon})$。在上界方面，我们修复了之前的方法，并显示了可以达到$\tilde{O}(\frac{k\sqrt{d}{\sqrt{n}\epsilon})$的上界。我们的发现表明了稀疏线性回归问题在非权限模型、中央DP模型和本地DP模型之间存在fundamental的差异。
</details></li>
</ul>
<hr>
<h2 id="GraphControl-Adding-Conditional-Control-to-Universal-Graph-Pre-trained-Models-for-Graph-Domain-Transfer-Learning"><a href="#GraphControl-Adding-Conditional-Control-to-Universal-Graph-Pre-trained-Models-for-Graph-Domain-Transfer-Learning" class="headerlink" title="GraphControl: Adding Conditional Control to Universal Graph Pre-trained Models for Graph Domain Transfer Learning"></a>GraphControl: Adding Conditional Control to Universal Graph Pre-trained Models for Graph Domain Transfer Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07365">http://arxiv.org/abs/2310.07365</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yun Zhu, Yaoke Wang, Haizhou Shi, Zhenshuo Zhang, Siliang Tang<br>for:这篇论文的目的是提出一个名为GraphControl的创新部署模组，以便更好地实现图形领域的转移学习。methods:这篇论文使用了自适应的图形模型和ControlNet来实现图形转移学习。results:实验结果显示，GraphControl可以将预训 модеル更好地适应目标图形资料，实现1.4-3倍的性能提升，并且比较training-from-scratch方法在目标资料上的表现更好，且变得更快速。<details>
<summary>Abstract</summary>
Graph-structured data is ubiquitous in the world which models complex relationships between objects, enabling various Web applications. Daily influxes of unlabeled graph data on the Web offer immense potential for these applications. Graph self-supervised algorithms have achieved significant success in acquiring generic knowledge from abundant unlabeled graph data. These pre-trained models can be applied to various downstream Web applications, saving training time and improving downstream (target) performance. However, different graphs, even across seemingly similar domains, can differ significantly in terms of attribute semantics, posing difficulties, if not infeasibility, for transferring the pre-trained models to downstream tasks. Concretely speaking, for example, the additional task-specific node information in downstream tasks (specificity) is usually deliberately omitted so that the pre-trained representation (transferability) can be leveraged. The trade-off as such is termed as "transferability-specificity dilemma" in this work. To address this challenge, we introduce an innovative deployment module coined as GraphControl, motivated by ControlNet, to realize better graph domain transfer learning. Specifically, by leveraging universal structural pre-trained models and GraphControl, we align the input space across various graphs and incorporate unique characteristics of target data as conditional inputs. These conditions will be progressively integrated into the model during fine-tuning or prompt tuning through ControlNet, facilitating personalized deployment. Extensive experiments show that our method significantly enhances the adaptability of pre-trained models on target attributed datasets, achieving 1.4-3x performance gain. Furthermore, it outperforms training-from-scratch methods on target data with a comparable margin and exhibits faster convergence.
</details>
<details>
<summary>摘要</summary>
世界各地的各种数据都具有图structured的特点，用于模型复杂的对象之间的关系，支持多种Web应用程序。日常大量未标注图数据的入流提供了巨大的潜在性 для这些应用程序。自我超vised学习算法在丰富的未标注图数据上取得了显著的成功，从而获得了一些通用的知识。这些预训练模型可以应用于多个下游Web应用程序，提高下游（目标）性能，并节省训练时间。然而，不同的图，即使在看起来相似的领域中，可能存在很大的属性 semantics 的差异，这会对预训练模型的转移带来很大的困难，甚至是不可能。具体来说，例如，在下游任务中添加特定任务的节点信息通常会故意 omitted，以便利用预训练表示。这种困难被称为“转移性-特点矛盾”在这篇论文中。为解决这个挑战，我们提出了一种创新的投入模块，称为GraphControl， inspirited by ControlNet。我们利用通用的结构预训练模型和GraphControl，将输入空间 across various graphs 进行对应，并 incorporate 目标数据中独特的特征作为条件输入。这些条件将在练习或提示调整中逐渐进行 integrate 到模型中，通过ControlNet，实现个性化部署。我们的方法在目标 attributed 数据上显著提高了预训练模型的适应性，实现了1.4-3x的性能提升。此外，它还超过了从scratch 训练方法在目标数据上的相同幅度，并且显示更快的收敛速度。
</details></li>
</ul>
<hr>
<h2 id="Atom-Motif-Contrastive-Transformer-for-Molecular-Property-Prediction"><a href="#Atom-Motif-Contrastive-Transformer-for-Molecular-Property-Prediction" class="headerlink" title="Atom-Motif Contrastive Transformer for Molecular Property Prediction"></a>Atom-Motif Contrastive Transformer for Molecular Property Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07351">http://arxiv.org/abs/2310.07351</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wentao Yu, Shuo Chen, Chen Gong, Gang Niu, Masashi Sugiyama<br>for:* 这 paper 是为了提高分子属性预测（MPP）的效果而写的。methods:* 这 paper 使用了 Atom-Motif Contrastive Transformer（AMCT）模型，该模型不仅考虑了分子中的单个原子间交互，还考虑了分子中的重要模式（例如功能组）的交互。results:* 对比于当前的状态艺术方法，该 paper 的方法能够更好地预测分子的属性，并且可以准确地确定每个分子中的关键模式。<details>
<summary>Abstract</summary>
Recently, Graph Transformer (GT) models have been widely used in the task of Molecular Property Prediction (MPP) due to their high reliability in characterizing the latent relationship among graph nodes (i.e., the atoms in a molecule). However, most existing GT-based methods usually explore the basic interactions between pairwise atoms, and thus they fail to consider the important interactions among critical motifs (e.g., functional groups consisted of several atoms) of molecules. As motifs in a molecule are significant patterns that are of great importance for determining molecular properties (e.g., toxicity and solubility), overlooking motif interactions inevitably hinders the effectiveness of MPP. To address this issue, we propose a novel Atom-Motif Contrastive Transformer (AMCT), which not only explores the atom-level interactions but also considers the motif-level interactions. Since the representations of atoms and motifs for a given molecule are actually two different views of the same instance, they are naturally aligned to generate the self-supervisory signals for model training. Meanwhile, the same motif can exist in different molecules, and hence we also employ the contrastive loss to maximize the representation agreement of identical motifs across different molecules. Finally, in order to clearly identify the motifs that are critical in deciding the properties of each molecule, we further construct a property-aware attention mechanism into our learning framework. Our proposed AMCT is extensively evaluated on seven popular benchmark datasets, and both quantitative and qualitative results firmly demonstrate its effectiveness when compared with the state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
近些年，图Transformer（GT）模型在分子性质预测（MPP）任务中广泛应用，因为它们可以准确地描述分子中节点之间的潜在关系。然而，大多数现有的GT基于方法通常只explore分子中基本的对应关系，因此它们忽略了分子中重要的对应关系（例如功能组consisted of several atoms）。由于分子中的功能组是决定分子性质的重要Patterns，忽略这些对应关系不可避免地降低了MPP的效果。为解决这个问题，我们提出了一种新的Atom-Motif Contrastive Transformer（AMCT）模型，它不仅探索分子中的原子间交互，还考虑分子中的对应关系。由于分子中的原子和功能组的表示是同一个实例的两种视角，因此它们自然地启用了自我超visional信号 для模型训练。此外，同一个功能组可以在不同的分子中出现，因此我们还使用了对比损失来提高同一个功能组在不同的分子中的表示协调。最后，为了清晰地确定每个分子中critical的功能组，我们进一步构建了一个性质意识的注意机制 into our learning framework。我们的提出的AMCT在七个流行的 benchmark dataset上进行了广泛的评估，并且量化和质量上的结果都证明了它的效果性比领先方法更高。
</details></li>
</ul>
<hr>
<h2 id="Towards-Foundation-Models-for-Learning-on-Tabular-Data"><a href="#Towards-Foundation-Models-for-Learning-on-Tabular-Data" class="headerlink" title="Towards Foundation Models for Learning on Tabular Data"></a>Towards Foundation Models for Learning on Tabular Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07338">http://arxiv.org/abs/2310.07338</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Han Zhang, Xumeng Wen, Shun Zheng, Wei Xu, Jiang Bian</li>
<li>for: 提高 tabular 数据上的学习效果，并提供可转移的模型 для新任务。</li>
<li>methods: 使用生成型 tabular 学习，采用预训练的大语言模型（LLM）作为基本模型，并通过定制的目标进行精度调整。</li>
<li>results: 在零shot和Contextual Inference等指令ollowing任务中， TabFM 显著超越了 GPT-4 等关闭源 LLM，并在 scarce 数据下进行 fine-tuning 时显示了remarkable的效率和竞争力。<details>
<summary>Abstract</summary>
Learning on tabular data underpins numerous real-world applications. Despite considerable efforts in developing effective learning models for tabular data, current transferable tabular models remain in their infancy, limited by either the lack of support for direct instruction following in new tasks or the neglect of acquiring foundational knowledge and capabilities from diverse tabular datasets. In this paper, we propose Tabular Foundation Models (TabFMs) to overcome these limitations. TabFMs harness the potential of generative tabular learning, employing a pre-trained large language model (LLM) as the base model and fine-tuning it using purpose-designed objectives on an extensive range of tabular datasets. This approach endows TabFMs with a profound understanding and universal capabilities essential for learning on tabular data. Our evaluations underscore TabFM's effectiveness: not only does it significantly excel in instruction-following tasks like zero-shot and in-context inference, but it also showcases performance that approaches, and in instances, even transcends, the renowned yet mysterious closed-source LLMs like GPT-4. Furthermore, when fine-tuning with scarce data, our model achieves remarkable efficiency and maintains competitive performance with abundant training data. Finally, while our results are promising, we also delve into TabFM's limitations and potential opportunities, aiming to stimulate and expedite future research on developing more potent TabFMs.
</details>
<details>
<summary>摘要</summary>
学习表格数据的应用场景非常广泛。尽管有大量的努力在开发有效的学习模型 для表格数据，但目前可传输的表格模型仍处于幼年期，受到 Either irect instruction following in new tasks 或 neglect of acquiring foundational knowledge and capabilities from diverse tabular datasets的限制。在这篇论文中，我们提议使用表格基础模型（TabFM）来超越这些限制。TabFM 利用生成表格学习的潜力，使用预训练的大型自然语言模型（LLM）作为基本模型，并通过特定目标的精心调整在广泛的表格数据集上。这种方法赋予 TabFM 深刻的理解和普遍的能力，使其成为学习表格数据的优秀选择。我们的评估表明，TabFM 不仅在 zero-shot 和 in-context 推理任务中表现出色，而且在一些情况下，甚至超越了著名但神秘的关闭源 LLM  like GPT-4。此外，当 fine-tuning  WITH 稀有数据时，我们的模型实现了很好的效率，并保持了与丰富训练数据相比的竞争性。最后，虽然我们的结果吸引人，但我们 также探讨 TabFM 的局限性和潜在机遇，以便促进和加快未来的研究。
</details></li>
</ul>
<hr>
<h2 id="Multichannel-consecutive-data-cross-extraction-with-1DCNN-attention-for-diagnosis-of-power-transformer"><a href="#Multichannel-consecutive-data-cross-extraction-with-1DCNN-attention-for-diagnosis-of-power-transformer" class="headerlink" title="Multichannel consecutive data cross-extraction with 1DCNN-attention for diagnosis of power transformer"></a>Multichannel consecutive data cross-extraction with 1DCNN-attention for diagnosis of power transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07323">http://arxiv.org/abs/2310.07323</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Zheng, Guogang Zhang, Chenchen Zhao, Qianqian Zhu</li>
<li>for: 本研究旨在提出一种基于多通道连续数据的变压器诊断方法，以便更好地捕捉变压器的状态信息。</li>
<li>methods: 本方法基于多通道连续数据层次结构（MCDC），并引入一维 convolutional neural network attention（1DCNN-attention）机制以提高诊断效果和简化空间复杂度。</li>
<li>results: 实验结果表明，相比于其他方法，MCDC和1DCNN-attention具有更高的诊断精度和泛化能力，并且1DCNN-attention机制能够提供更稳定的诊断结果。<details>
<summary>Abstract</summary>
Power transformer plays a critical role in grid infrastructure, and its diagnosis is paramount for maintaining stable operation. However, the current methods for transformer diagnosis focus on discrete dissolved gas analysis, neglecting deep feature extraction of multichannel consecutive data. The unutilized sequential data contains the significant temporal information reflecting the transformer condition. In light of this, the structure of multichannel consecutive data cross-extraction (MCDC) is proposed in this article in order to comprehensively exploit the intrinsic characteristic and evaluate the states of transformer. Moreover, for the better accommodation in scenario of transformer diagnosis, one dimensional convolution neural network attention (1DCNN-attention) mechanism is introduced and offers a more efficient solution given the simplified spatial complexity. Finally, the effectiveness of MCDC and the superior generalization ability, compared with other algorithms, are validated in experiments conducted on a dataset collected from real operation cases of power transformer. Additionally, the better stability of 1DCNN-attention has also been certified.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Power transformer plays a critical role in grid infrastructure, and its diagnosis is paramount for maintaining stable operation. However, the current methods for transformer diagnosis focus on discrete dissolved gas analysis, neglecting deep feature extraction of multichannel consecutive data. The unutilized sequential data contains significant temporal information reflecting the transformer condition. In light of this, the structure of multichannel consecutive data cross-extraction (MCDC) is proposed in this article to comprehensively exploit the intrinsic characteristic and evaluate the states of transformer. Moreover, to better accommodate the scenario of transformer diagnosis, one dimensional convolution neural network attention (1DCNN-attention) mechanism is introduced, offering a more efficient solution given the simplified spatial complexity. Finally, the effectiveness of MCDC and the superior generalization ability, compared with other algorithms, are validated in experiments conducted on a dataset collected from real operation cases of power transformer. Additionally, the better stability of 1DCNN-attention has also been certified.Note: Please keep in mind that the translation is in Simplified Chinese, and the grammar and sentence structure may be different from Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Byzantine-Resilient-Decentralized-Multi-Armed-Bandits"><a href="#Byzantine-Resilient-Decentralized-Multi-Armed-Bandits" class="headerlink" title="Byzantine-Resilient Decentralized Multi-Armed Bandits"></a>Byzantine-Resilient Decentralized Multi-Armed Bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07320">http://arxiv.org/abs/2310.07320</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingxuan Zhu, Alec Koppel, Alvaro Velasquez, Ji Liu</li>
<li>for: This paper focuses on the problem of decentralized cooperative multi-armed bandits (MAB) in a setting where some agents may be Byzantine (i.e., they may provide arbitrary wrong information). The goal is to develop a fully decentralized resilient algorithm that can recover the salient behavior of a cooperative setting even in the presence of Byzantine agents.</li>
<li>methods: The proposed algorithm uses an information mixing step among agents and a truncation of inconsistent and extreme values to fuse the information. The algorithm is based on the Upper-Confidence Bound (UCB) method, but with a modification to handle the Byzantine agents.</li>
<li>results: The paper shows that the proposed algorithm can achieve a regret that is no worse than the classic single-agent UCB1 algorithm, and the cumulative regret of all normal agents is strictly better than the non-cooperative case, as long as each agent has at least 3f+1 neighbors where f is the maximum possible Byzantine agents in each agent’s neighborhood. The paper also establishes extensions to time-varying neighbor graphs and minimax lower bounds on the achievable regret. Experiments corroborate the merits of the proposed framework in practice.<details>
<summary>Abstract</summary>
In decentralized cooperative multi-armed bandits (MAB), each agent observes a distinct stream of rewards, and seeks to exchange information with others to select a sequence of arms so as to minimize its regret. Agents in the cooperative setting can outperform a single agent running a MAB method such as Upper-Confidence Bound (UCB) independently. In this work, we study how to recover such salient behavior when an unknown fraction of the agents can be Byzantine, that is, communicate arbitrarily wrong information in the form of reward mean-estimates or confidence sets. This framework can be used to model attackers in computer networks, instigators of offensive content into recommender systems, or manipulators of financial markets. Our key contribution is the development of a fully decentralized resilient upper confidence bound (UCB) algorithm that fuses an information mixing step among agents with a truncation of inconsistent and extreme values. This truncation step enables us to establish that the performance of each normal agent is no worse than the classic single-agent UCB1 algorithm in terms of regret, and more importantly, the cumulative regret of all normal agents is strictly better than the non-cooperative case, provided that each agent has at least 3f+1 neighbors where f is the maximum possible Byzantine agents in each agent's neighborhood. Extensions to time-varying neighbor graphs, and minimax lower bounds are further established on the achievable regret. Experiments corroborate the merits of this framework in practice.
</details>
<details>
<summary>摘要</summary>
在分布式合作多臂强擦擦机（MAB）中，每个代理都观察到自己独特的奖励流，并尝试与其他代理交换信息，以选择一个序列的臂，以最小化它的 regret。在合作 setting中，代理可以超过单个代理运行 MAB 方法，如 Upper-Confidence Bound（UCB）独立地执行。在这种工作中，我们研究如何恢复这种突出的行为，当一个未知的比例的代理可以是 Byzantine，即通过不正确地传递奖励均值或信任集来交流信息时。这种框架可以用来模型计算机网络中的攻击者，推荐系统中的启动者，或财务市场中的操纵者。我们的关键贡献是开发了一种完全分布式抗攻击的Upper Confidence Bound（UCB）算法，该算法结合代理之间的信息混合步骤，并对不一致和极端值进行舍入。这种舍入步骤使得我们可以证明每个正常的代理的性能不 worse than 独立的单个代理 UCB1 算法，而且更重要的是，所有正常的代理的总 regret 比非合作情况更好，只要每个代理至少有 3f+1 个邻居，其中 f 是最大可能的 Byzantine 代理数量。我们还Extensions to 时变邻居图和最小最差下界是进一步确立的。实验证明了这种框架在实践中的优势。
</details></li>
</ul>
<hr>
<h2 id="Molecule-Edit-Templates-for-Efficient-and-Accurate-Retrosynthesis-Prediction"><a href="#Molecule-Edit-Templates-for-Efficient-and-Accurate-Retrosynthesis-Prediction" class="headerlink" title="Molecule-Edit Templates for Efficient and Accurate Retrosynthesis Prediction"></a>Molecule-Edit Templates for Efficient and Accurate Retrosynthesis Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07313">http://arxiv.org/abs/2310.07313</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mikołaj Sacha, Michał Sadowski, Piotr Kozakowski, Ruard van Workum, Stanisław Jastrzębski</li>
<li>for: 本研究旨在开发一种基于机器学习的Retrosynthesis方法，以便用更加有效和可解释的方式预测复杂分子的合成步骤。</li>
<li>methods: 本研究使用了一种名为METRO（分子修改模板Retrosynthesis）的机器学习模型，该模型使用最小的模板（简化的反应模式）来预测反应，从而减少计算开销并达到标准benchmark的最佳结果。</li>
<li>results: 根据标准benchmark的测试结果，METRO模型可以准确预测复杂分子的合成步骤，并且比传统的模板基本法更加有效和可解释。<details>
<summary>Abstract</summary>
Retrosynthesis involves determining a sequence of reactions to synthesize complex molecules from simpler precursors. As this poses a challenge in organic chemistry, machine learning has offered solutions, particularly for predicting possible reaction substrates for a given target molecule. These solutions mainly fall into template-based and template-free categories. The former is efficient but relies on a vast set of predefined reaction patterns, while the latter, though more flexible, can be computationally intensive and less interpretable. To address these issues, we introduce METRO (Molecule-Edit Templates for RetrOsynthesis), a machine-learning model that predicts reactions using minimal templates - simplified reaction patterns capturing only essential molecular changes - reducing computational overhead and achieving state-of-the-art results on standard benchmarks.
</details>
<details>
<summary>摘要</summary>
转换文本到简化中文：Retrosynthesis是指从简单前体分子 synthesize复杂分子的过程。这在有机化学中存在挑战，特别是预测目标分子可能的反应substrate。这些解决方案主要分为模板基和模板自由两类。前者是效率高，但需要大量预定的反应模式，而后者具有更多的灵活性，但计算负担大和解释性差。为解决这些问题，我们介绍METRO（分子修改模板 для RetrOsynthesis），一种基于机器学习的模型，使用最小模板预测反应，减少计算负担，达到标准benchmark的状态 искусственный智能。
</details></li>
</ul>
<hr>
<h2 id="Score-Regularized-Policy-Optimization-through-Diffusion-Behavior"><a href="#Score-Regularized-Policy-Optimization-through-Diffusion-Behavior" class="headerlink" title="Score Regularized Policy Optimization through Diffusion Behavior"></a>Score Regularized Policy Optimization through Diffusion Behavior</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07297">http://arxiv.org/abs/2310.07297</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thu-ml/srpo">https://github.com/thu-ml/srpo</a></li>
<li>paper_authors: Huayu Chen, Cheng Lu, Zhengyi Wang, Hang Su, Jun Zhu</li>
<li>for: 实现高效的行为样本抽取和独立于时间和资源consuming的算法。</li>
<li>methods: 利用批评模型和遗传 diffusion 行为模型，将分布式批评政策转换为具有高效决策能力的决策政策，并在优化过程中直接使用行为分布的得分函数进行调整。</li>
<li>results: 在 D4RL 任务上，我们的方法可以大幅提高行为抽取速度，较于各种主流的扩散基于方法，并且仍保持现有的性能水准。<details>
<summary>Abstract</summary>
Recent developments in offline reinforcement learning have uncovered the immense potential of diffusion modeling, which excels at representing heterogeneous behavior policies. However, sampling from diffusion policies is considerably slow because it necessitates tens to hundreds of iterative inference steps for one action. To address this issue, we propose to extract an efficient deterministic inference policy from critic models and pretrained diffusion behavior models, leveraging the latter to directly regularize the policy gradient with the behavior distribution's score function during optimization. Our method enjoys powerful generative capabilities of diffusion modeling while completely circumventing the computationally intensive and time-consuming diffusion sampling scheme, both during training and evaluation. Extensive results on D4RL tasks show that our method boosts action sampling speed by more than 25 times compared with various leading diffusion-based methods in locomotion tasks, while still maintaining state-of-the-art performance.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Why-Does-Sharpness-Aware-Minimization-Generalize-Better-Than-SGD"><a href="#Why-Does-Sharpness-Aware-Minimization-Generalize-Better-Than-SGD" class="headerlink" title="Why Does Sharpness-Aware Minimization Generalize Better Than SGD?"></a>Why Does Sharpness-Aware Minimization Generalize Better Than SGD?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07269">http://arxiv.org/abs/2310.07269</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zixiang Chen, Junkai Zhang, Yiwen Kou, Xiangning Chen, Cho-Jui Hsieh, Quanquan Gu</li>
<li>for: 降低过拟合的挑战，特别是在训练大型神经网络时，使用Sharpness-Aware Minimization（SAM）方法可以提高神经网络的泛化能力，即使存在标签噪声。</li>
<li>methods: 本文使用Sharpness-Aware Minimization（SAM）方法，并通过对非线性神经网络和分类任务的研究，解释了SAM在这些任务中的成功原理。</li>
<li>results: 对于某种数据模型和二层卷积ReLU网络，本文证明SAM在某些情况下比Stochastic Gradient Descent（SGD）更好地泛化，并通过实验证明了这一点。<details>
<summary>Abstract</summary>
The challenge of overfitting, in which the model memorizes the training data and fails to generalize to test data, has become increasingly significant in the training of large neural networks. To tackle this challenge, Sharpness-Aware Minimization (SAM) has emerged as a promising training method, which can improve the generalization of neural networks even in the presence of label noise. However, a deep understanding of how SAM works, especially in the setting of nonlinear neural networks and classification tasks, remains largely missing. This paper fills this gap by demonstrating why SAM generalizes better than Stochastic Gradient Descent (SGD) for a certain data model and two-layer convolutional ReLU networks. The loss landscape of our studied problem is nonsmooth, thus current explanations for the success of SAM based on the Hessian information are insufficient. Our result explains the benefits of SAM, particularly its ability to prevent noise learning in the early stages, thereby facilitating more effective learning of features. Experiments on both synthetic and real data corroborate our theory.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:难以适应问题，即模型记忆训练数据而不能泛化测试数据，在大型神经网络训练中变得越来越重要。为解决这个挑战，锐度感知敏感化（SAM）已经成为一种有前途的训练方法，可以提高神经网络的泛化性，即使存在标签噪声。然而，SAM在非线性神经网络和分类任务中的工作机制仍然不够了解。这篇论文填补这个空白，说明SAM在某种数据模型和二层卷积ReLU网络上的优化性比SGD更好。我们的搜索问题的损失景观是非凹的，因此现有基于Hessian信息的解释不够。我们的结果解释了SAM的优势，特别是它在早期阶段避免噪声学习，从而促进更有效的特征学习。实验证明了我们的理论。
</details></li>
</ul>
<hr>
<h2 id="RaftFed-A-Lightweight-Federated-Learning-Framework-for-Vehicular-Crowd-Intelligence"><a href="#RaftFed-A-Lightweight-Federated-Learning-Framework-for-Vehicular-Crowd-Intelligence" class="headerlink" title="RaftFed: A Lightweight Federated Learning Framework for Vehicular Crowd Intelligence"></a>RaftFed: A Lightweight Federated Learning Framework for Vehicular Crowd Intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07268">http://arxiv.org/abs/2310.07268</a></li>
<li>repo_url: None</li>
<li>paper_authors: Changan Yang, Yaxing Chen, Yao Zhang, Helei Cui, Zhiwen Yu, Bin Guo, Zheng Yan, Zijiang Yang</li>
<li>for: 这则研究旨在解决车载智能应用中的数据隐私问题，使用了联邦学习（Federated Learning，FL）技术。</li>
<li>methods: 本研究提出了一个名为RaftFed的新型联邦学习框架，实现了隐私保护的车载智能应用。RaftFed使用了raft协议来实现分布式模型聚合，并且适应非 Identical Independent Distributions（Non-IID）数据。</li>
<li>results: 实验结果显示，RaftFed相比基eline的通信负载、模型精度和模型融合都表现更好。<details>
<summary>Abstract</summary>
Vehicular crowd intelligence (VCI) is an emerging research field. Facilitated by state-of-the-art vehicular ad-hoc networks and artificial intelligence, various VCI applications come to place, e.g., collaborative sensing, positioning, and mapping. The collaborative property of VCI applications generally requires data to be shared among participants, thus forming network-wide intelligence. How to fulfill this process without compromising data privacy remains a challenging issue. Although federated learning (FL) is a promising tool to solve the problem, adapting conventional FL frameworks to VCI is nontrivial. First, the centralized model aggregation is unreliable in VCI because of the existence of stragglers with unfavorable channel conditions. Second, existing FL schemes are vulnerable to Non-IID data, which is intensified by the data heterogeneity in VCI. This paper proposes a novel federated learning framework called RaftFed to facilitate privacy-preserving VCI. The experimental results show that RaftFed performs better than baselines regarding communication overhead, model accuracy, and model convergence.
</details>
<details>
<summary>摘要</summary>
vehicular crowd intelligence (VCI) 是一个emerging研究领域。通过现代交通运输网络和人工智能技术，VCI应用得到了广泛的应用，例如共同探测、定位和地图生成。VCI应用的共同性通常需要参与者之间数据共享，因此形成网络范围内的智能。但是保护数据隐私的问题仍然是一个挑战。虽然联邦学习（FL）是一种有望的解决方案，但将传统FL框架适应VCI是一个非常困难的任务。首先，中央模型聚合是VCI中不可靠的，因为存在不优惠的通道条件下的停留者。其次，现有的FL方案容易受到非同分布数据的影响，这在VCI中更加严重，因为数据多样性很高。这篇论文提出了一种新的联邦学习框架called RaftFed，用于保护隐私的VCI。实验结果表明，RaftFed比基准方案更好，具有较低的通信开销、更高的模型准确率和更快的模型融合。
</details></li>
</ul>
<hr>
<h2 id="Classification-of-Dysarthria-based-on-the-Levels-of-Severity-A-Systematic-Review"><a href="#Classification-of-Dysarthria-based-on-the-Levels-of-Severity-A-Systematic-Review" class="headerlink" title="Classification of Dysarthria based on the Levels of Severity. A Systematic Review"></a>Classification of Dysarthria based on the Levels of Severity. A Systematic Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07264">http://arxiv.org/abs/2310.07264</a></li>
<li>repo_url: None</li>
<li>paper_authors: Afnan Al-Ali, Somaya Al-Maadeed, Moutaz Saleh, Rani Chinnappa Naidu, Zachariah C Alex, Prakash Ramachandran, Rajeev Khoodeeram, Rajesh Kumar M</li>
<li>for: 这个评估是为了提高受影响者的沟通能力和生活质量，以及为了提供更加准确和可靠的诊断。</li>
<li>methods: 这个评估使用了人工智能技术，特别是机器学习算法，以自动分类受影响者的喉痛度。</li>
<li>results: 这个评估发现了一些最有效的特征和技术，可以用于自动分类受影响者的喉痛度，并提高了诊断的准确性和可靠性。<details>
<summary>Abstract</summary>
Dysarthria is a neurological speech disorder that can significantly impact affected individuals' communication abilities and overall quality of life. The accurate and objective classification of dysarthria and the determination of its severity are crucial for effective therapeutic intervention. While traditional assessments by speech-language pathologists (SLPs) are common, they are often subjective, time-consuming, and can vary between practitioners. Emerging machine learning-based models have shown the potential to provide a more objective dysarthria assessment, enhancing diagnostic accuracy and reliability. This systematic review aims to comprehensively analyze current methodologies for classifying dysarthria based on severity levels. Specifically, this review will focus on determining the most effective set and type of features that can be used for automatic patient classification and evaluating the best AI techniques for this purpose. We will systematically review the literature on the automatic classification of dysarthria severity levels. Sources of information will include electronic databases and grey literature. Selection criteria will be established based on relevance to the research questions. Data extraction will include methodologies used, the type of features extracted for classification, and AI techniques employed. The findings of this systematic review will contribute to the current understanding of dysarthria classification, inform future research, and support the development of improved diagnostic tools. The implications of these findings could be significant in advancing patient care and improving therapeutic outcomes for individuals affected by dysarthria.
</details>
<details>
<summary>摘要</summary>
《嗜睡病患者精度诊断分类方法的系统atic review》Introduction:嗜睡病（Dysarthria）是一种神经系统疾病，可能对患者的沟通能力和生活质量产生重要影响。准确和客观地诊断嗜睡病和其严重程度是诊断治疗的关键。现有的传统评估方法由语言听说师（SLP）进行，但这些评估方法常常是主观的、耗时的，并且可能由听说师之间存在差异。新兴的机器学习基于模型已经显示出可以提供更客观的嗜睡病诊断，从而提高诊断的准确性和可靠性。本系统atic review旨在全面分析当前的嗜睡病分类方法，具体来说是寻找最有效的分类特征和AI技术。Objectives:本文的目标是对嗜睡病分类方法进行系统atic review，以便更好地理解嗜睡病的分类方法，并且为未来的研究提供参考。特定的研究问题包括：1. 寻找最有效的分类特征，以便自动分类嗜睡病的严重程度。2. 评估AI技术的效果，以便选择最佳的AI技术来进行嗜睡病分类。Methodology:本文的方法包括：1. 搜索电子数据库和灰色文献，以找到相关的研究。2. 根据研究问题的 relevance 选择合适的文献。3. 对选择的文献进行数据抽取，包括使用的方法、分类特征和AI技术。Expected outcomes:本文的结果将对当前的嗜睡病分类方法进行全面分析，并提供有价值的参考。这些结果可能对患者的诊断和治疗产生重要影响，并且可能推动未来的研究。Conclusion:本文的系统atic review将对嗜睡病分类方法进行全面分析，并评估AI技术的效果。这些结果将有助于我们更好地理解嗜睡病的分类方法，并为未来的研究提供参考。这些结果的发现可能对患者的诊断和治疗产生重要影响，并且可能推动未来的研究。
</details></li>
</ul>
<hr>
<h2 id="Deep-ReLU-networks-and-high-order-finite-element-methods-II-Chebyshev-emulation"><a href="#Deep-ReLU-networks-and-high-order-finite-element-methods-II-Chebyshev-emulation" class="headerlink" title="Deep ReLU networks and high-order finite element methods II: Chebyshev emulation"></a>Deep ReLU networks and high-order finite element methods II: Chebyshev emulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07261">http://arxiv.org/abs/2310.07261</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joost A. A. Opschoor, Christoph Schwab</li>
<li>for: 这篇论文主要研究了深度ReLU神经网络（NN）在 Sobolev  нормов下的表达率和稳定性，以及NN的参数数量如何影响这些性能指标。</li>
<li>methods: 这篇论文使用了 Novel constructions of ReLU NN surrogates，即使用Chebychev多项式扩展系数来表示近似函数。这些系数可以从Clenshaw–Curtis点中的函数值使用 inverse fast Fourier transform 计算。</li>
<li>results: 论文得到了对表达率和稳定性的较好的上限，超过了基于ReLU NN拟合幂数考虑的构造。论文还提供了不同类型函数和 norms 的NN拟合误差估计，以及在数值分析中遇到的常见函数和 norms 的ReLU NN拟合率 bounds。<details>
<summary>Abstract</summary>
Expression rates and stability in Sobolev norms of deep ReLU neural networks (NNs) in terms of the number of parameters defining the NN for continuous, piecewise polynomial functions, on arbitrary, finite partitions $\mathcal{T}$ of a bounded interval $(a,b)$ are addressed. Novel constructions of ReLU NN surrogates encoding the approximated functions in terms of Chebyshev polynomial expansion coefficients are developed. Chebyshev coefficients can be computed easily from the values of the function in the Clenshaw--Curtis points using the inverse fast Fourier transform. Bounds on expression rates and stability that are superior to those of constructions based on ReLU NN emulations of monomials considered in [Opschoor, Petersen, Schwab, 2020] are obtained. All emulation bounds are explicit in terms of the (arbitrary) partition of the interval, the target emulation accuracy and the polynomial degree in each element of the partition. ReLU NN emulation error estimates are provided for various classes of functions and norms, commonly encountered in numerical analysis. In particular, we show exponential ReLU emulation rate bounds for analytic functions with point singularities and develop an interface between Chebfun approximations and constructive ReLU NN emulations.
</details>
<details>
<summary>摘要</summary>
“深度ReLU神经网络（NN）的表达速率和稳定性在 Sobolev  нор下，对于连续、分割式多项函数，在有界区间（a,b）上的任意、有限分区 $\mathcal{T}$ 上进行研究。我们提出了基于 ReLU NN 的新构造，用于表示approximes 函数的 Chebyshev 多项展开系数。Chebyshev 系数可以通过 Clenshaw-Curtis 点的值来容易计算，使用反快速傅立叶变换。我们获得了基于 ReLU NN 的构造，superior 于 [Opschoor, Petersen, Schwab, 2020] 中基于 monomials 的构造的表达率和稳定性 bound。所有的 emulation bound 是对于（任意）分区、目标投影精度和每个分区元素的权重度的explicit 表达。我们还提供了不同类型函数和norms 的 ReLU NN 投影误差估计，广泛存在在数学分析中。特别是，我们展示了对于分析函数的点稳定性的 exponential ReLU 投影速率 bound。”
</details></li>
</ul>
<hr>
<h2 id="CacheGen-Fast-Context-Loading-for-Language-Model-Applications"><a href="#CacheGen-Fast-Context-Loading-for-Language-Model-Applications" class="headerlink" title="CacheGen: Fast Context Loading for Language Model Applications"></a>CacheGen: Fast Context Loading for Language Model Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07240">http://arxiv.org/abs/2310.07240</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuhan Liu, Hanchen Li, Kuntai Du, Jiayi Yao, Yihua Cheng, Yuyang Huang, Shan Lu, Michael Maire, Henry Hoffmann, Ari Holtzman, Ganesh Ananthanarayanan, Junchen Jiang<br>for: This paper aims to improve the efficiency of large language models (LLMs) by minimizing the delays in fetching and processing contexts.methods: The paper proposes a novel encoder that compresses key-value (KV) features into more compact bitstream representations, taking advantage of the KV features’ distributional properties. Additionally, the paper uses a controller to determine when to load the context as compressed KV features or raw text and picks the appropriate compression level.results: Compared to recent methods that handle long contexts, the proposed method reduces bandwidth usage by 3.7-4.3x and the total delay in fetching and processing contexts by 2.7-3x while maintaining similar LLM performance on various tasks.<details>
<summary>Abstract</summary>
As large language models (LLMs) take on more complex tasks, their inputs incorporate longer contexts to respond to questions that require domain knowledge or user-specific conversational histories. Yet, using long contexts poses a challenge for responsive LLM systems, as nothing can be generated until all the contexts are fetched to and processed by the LLM. Existing systems optimize only the computation delay in context processing (e.g., by caching intermediate key-value features of the text context) but often cause longer network delays in context fetching (e.g., key-value features consume orders of magnitude larger bandwidth than the text context).   This paper presents CacheGen to minimize the delays in fetching and processing contexts for LLMs. CacheGen reduces the bandwidth needed for transmitting long contexts' key-value (KV) features through a novel encoder that compresses KV features into more compact bitstream representations. The encoder combines adaptive quantization with a tailored arithmetic coder, taking advantage of the KV features' distributional properties, such as locality across tokens. Furthermore, CacheGen minimizes the total delay in fetching and processing a context by using a controller that determines when to load the context as compressed KV features or raw text and picks the appropriate compression level if loaded as KV features. We test CacheGen on three models of various sizes and three datasets of different context lengths. Compared to recent methods that handle long contexts, CacheGen reduces bandwidth usage by 3.7-4.3x and the total delay in fetching and processing contexts by 2.7-3x while maintaining similar LLM performance on various tasks as loading the text contexts.
</details>
<details>
<summary>摘要</summary>
large language models (LLMs) 在更复杂的任务中使用时，其输入将包含更长的上下文，以回答需要领域知识或用户特定的对话历史的问题。然而，使用长上下文会对responsive LLM系统 pose a challenge，因为系统无法生成任何内容 until all the contexts are fetched and processed by the LLM. existing systems only optimize the computation delay in context processing (e.g., by caching intermediate key-value features of the text context), but often cause longer network delays in context fetching (e.g., key-value features consume orders of magnitude larger bandwidth than the text context).this paper presents CacheGen, a method to minimize the delays in fetching and processing contexts for LLMs. CacheGen reduces the bandwidth needed for transmitting long contexts' key-value (KV) features through a novel encoder that compresses KV features into more compact bitstream representations. the encoder combines adaptive quantization with a tailored arithmetic coder, taking advantage of the KV features' distributional properties, such as locality across tokens. furthermore, CacheGen minimizes the total delay in fetching and processing a context by using a controller that determines when to load the context as compressed KV features or raw text and picks the appropriate compression level if loaded as KV features. we test CacheGen on three models of various sizes and three datasets of different context lengths. compared to recent methods that handle long contexts, CacheGen reduces bandwidth usage by 3.7-4.3x and the total delay in fetching and processing contexts by 2.7-3x while maintaining similar LLM performance on various tasks as loading the text contexts.
</details></li>
</ul>
<hr>
<h2 id="Are-GATs-Out-of-Balance"><a href="#Are-GATs-Out-of-Balance" class="headerlink" title="Are GATs Out of Balance?"></a>Are GATs Out of Balance?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07235">http://arxiv.org/abs/2310.07235</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nimrah Mustafa, Aleksandar Bojchevski, Rebekka Burkholz</li>
<li>for: 该研究探讨了图神经网络（GNN）的优化和学习动力，尤其是GNN中的Graph Attention Network（GAT） Architecture的学习动力。</li>
<li>methods: 研究者们使用了权重化注意力系数的GAT网络，并 derive了GAT梯度流动动力的保守定律，解释了为什么使用标准初始化的GAT网络中大部分参数难以更新 durante el entrenamiento。</li>
<li>results: 研究者们提出了一种Initialize Balance的方法，该方法可以更好地传播梯度，从而使得更深的GAT网络可以更好地训练，同时可以大幅提高训练和融合时间。<details>
<summary>Abstract</summary>
While the expressive power and computational capabilities of graph neural networks (GNNs) have been theoretically studied, their optimization and learning dynamics, in general, remain largely unexplored. Our study undertakes the Graph Attention Network (GAT), a popular GNN architecture in which a node's neighborhood aggregation is weighted by parameterized attention coefficients. We derive a conservation law of GAT gradient flow dynamics, which explains why a high portion of parameters in GATs with standard initialization struggle to change during training. This effect is amplified in deeper GATs, which perform significantly worse than their shallow counterparts. To alleviate this problem, we devise an initialization scheme that balances the GAT network. Our approach i) allows more effective propagation of gradients and in turn enables trainability of deeper networks, and ii) attains a considerable speedup in training and convergence time in comparison to the standard initialization. Our main theorem serves as a stepping stone to studying the learning dynamics of positive homogeneous models with attention mechanisms.
</details>
<details>
<summary>摘要</summary>
whilst the expressive power and computational capabilities of graph neural networks (GNNs) have been theoretically studied, their optimization and learning dynamics, in general, remain largely unexplored. Our study undertakes the Graph Attention Network (GAT), a popular GNN architecture in which a node's neighborhood aggregation is weighted by parameterized attention coefficients. We derive a conservation law of GAT gradient flow dynamics, which explains why a high portion of parameters in GATs with standard initialization struggle to change during training. This effect is amplified in deeper GATs, which perform significantly worse than their shallow counterparts. To alleviate this problem, we devise an initialization scheme that balances the GAT network. Our approach i) allows more effective propagation of gradients and in turn enables trainability of deeper networks, and ii) attains a considerable speedup in training and convergence time in comparison to the standard initialization. Our main theorem serves as a stepping stone to studying the learning dynamics of positive homogeneous models with attention mechanisms.Here's the translation in Traditional Chinese:而Graph Neural Networks（GNNs）的表达能力和计算能力已经被理论上研究，但是它们的优化和学习动态在总的来说还是有很多不清楚的地方。我们的研究涉及到Graph Attention Network（GAT），这是一种常见的GNN架构，其中每个节点的邻居聚合是通过参数化的注意系数来权重。我们得出了GAT的Gradient Flow动力学保守定律，这解释了为什么使用标准初始化的GAT参数大部分在训练中难以变化。这个效果在深度GAT中更加明显，它们在训练和 converges 时间上表现较差。为了解决这个问题，我们提出了一种初始化方案，该方案可以更好地传递梯度，从而使得深度网络可以更好地训练，同时也可以减少训练和 converges 时间的浪费。我们的主要定理作为 studying the learning dynamics of positive homogeneous models with attention mechanisms 的起点。
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-Decomposition-of-Prompt-Based-Continual-Learning-Rethinking-Obscured-Sub-optimality"><a href="#Hierarchical-Decomposition-of-Prompt-Based-Continual-Learning-Rethinking-Obscured-Sub-optimality" class="headerlink" title="Hierarchical Decomposition of Prompt-Based Continual Learning: Rethinking Obscured Sub-optimality"></a>Hierarchical Decomposition of Prompt-Based Continual Learning: Rethinking Obscured Sub-optimality</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07234">http://arxiv.org/abs/2310.07234</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thu-ml/hide-prompt">https://github.com/thu-ml/hide-prompt</a></li>
<li>paper_authors: Liyuan Wang, Jingyi Xie, Xingxing Zhang, Mingyi Huang, Hang Su, Jun Zhu</li>
<li>for: 提高 continual learning 下的表现，尤其是在自动生成标签数据时。</li>
<li>methods: 提出了一种 Hierarchical Decomposition (HiDe-)Prompt 方法，通过将具有任务特征的提示 ensemble 和不同 Representation 的统计数据协调，以提高 continual learning 的表现。</li>
<li>results: 对 Split CIFAR-100 和 Split ImageNet-R 进行了广泛的实验，并取得了比较出色的表现（例如，在 Split CIFAR-100 上提高了15.01%和9.61%的表现），并且robustness 到不同的预训练方法。<details>
<summary>Abstract</summary>
Prompt-based continual learning is an emerging direction in leveraging pre-trained knowledge for downstream continual learning, and has almost reached the performance pinnacle under supervised pre-training. However, our empirical research reveals that the current strategies fall short of their full potential under the more realistic self-supervised pre-training, which is essential for handling vast quantities of unlabeled data in practice. This is largely due to the difficulty of task-specific knowledge being incorporated into instructed representations via prompt parameters and predicted by uninstructed representations at test time. To overcome the exposed sub-optimality, we conduct a theoretical analysis of the continual learning objective in the context of pre-training, and decompose it into hierarchical components: within-task prediction, task-identity inference, and task-adaptive prediction. Following these empirical and theoretical insights, we propose Hierarchical Decomposition (HiDe-)Prompt, an innovative approach that explicitly optimizes the hierarchical components with an ensemble of task-specific prompts and statistics of both uninstructed and instructed representations, further with the coordination of a contrastive regularization strategy. Our extensive experiments demonstrate the superior performance of HiDe-Prompt and its robustness to pre-training paradigms in continual learning (e.g., up to 15.01% and 9.61% lead on Split CIFAR-100 and Split ImageNet-R, respectively). Our code is available at \url{https://github.com/thu-ml/HiDe-Prompt}.
</details>
<details>
<summary>摘要</summary>
Prompt-based continual learning 是一种emerging direction，它利用预训练知识来进行下游 continual learning，并已经几乎 дости到了supervised pre-training的性能巅峰。然而，我们的实验研究表明，当前的策略在更实际的self-supervised pre-training下表现不佳，这主要是因为任务特定知识的包含 INTO instructed representations via prompt parameters和在测试时由uninstructed representations预测的困难。为了解决这种暴露的下optimality，我们进行了 continual learning目标在预训练 context中的理论分析，并将其 decomposes into hierarchical components：within-task prediction、task-identity inference和task-adaptive prediction。根据这些实际和理论的洞察，我们提出了HiDe-Prompt方法，它使用了 ensemble of task-specific prompts和预训练和测试时的 both uninstructed和 instructed representations的统计，同时协调了一种对比正则化策略。我们的广泛实验表明，HiDe-Prompt方法具有superior performance和鲁棒性，在不同的预训练 paradigms下（例如，Split CIFAR-100和Split ImageNet-R）。我们的代码可以在 \url{https://github.com/thu-ml/HiDe-Prompt} 上获取。
</details></li>
</ul>
<hr>
<h2 id="Self-supervised-Pocket-Pretraining-via-Protein-Fragment-Surroundings-Alignment"><a href="#Self-supervised-Pocket-Pretraining-via-Protein-Fragment-Surroundings-Alignment" class="headerlink" title="Self-supervised Pocket Pretraining via Protein Fragment-Surroundings Alignment"></a>Self-supervised Pocket Pretraining via Protein Fragment-Surroundings Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07229">http://arxiv.org/abs/2310.07229</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bowen Gao, Yinjun Jia, Yuanle Mo, Yuyan Ni, Weiying Ma, Zhiming Ma, Yanyan Lan</li>
<li>for: 这个研究旨在提高药物可活性预测、药物结合亲和力预测和无样化药物设计等生医应用中的腔室表现。</li>
<li>methods: 本研究使用了一种新的腔室预训法，具体来说是将蛋白结构分成药物相似的片段和其对应的腔室，然后使用高效的先验式小分子表现来帮助学习腔室表现。</li>
<li>results: 研究结果显示，ProFSA方法可以在不同任务中实现州务之前的表现，包括腔室可活性预测、腔室匹配和药物结合亲和力预测等。此外，ProFSA方法也超过了其他预训方法，并且开启了一个新的途径来缓解蛋白-药物复杂数据的罕见性问题。<details>
<summary>Abstract</summary>
Pocket representations play a vital role in various biomedical applications, such as druggability estimation, ligand affinity prediction, and de novo drug design. While existing geometric features and pretrained representations have demonstrated promising results, they usually treat pockets independent of ligands, neglecting the fundamental interactions between them. However, the limited pocket-ligand complex structures available in the PDB database (less than 100 thousand non-redundant pairs) hampers large-scale pretraining endeavors for interaction modeling. To address this constraint, we propose a novel pocket pretraining approach that leverages knowledge from high-resolution atomic protein structures, assisted by highly effective pretrained small molecule representations. By segmenting protein structures into drug-like fragments and their corresponding pockets, we obtain a reasonable simulation of ligand-receptor interactions, resulting in the generation of over 5 million complexes. Subsequently, the pocket encoder is trained in a contrastive manner to align with the representation of pseudo-ligand furnished by some pretrained small molecule encoders. Our method, named ProFSA, achieves state-of-the-art performance across various tasks, including pocket druggability prediction, pocket matching, and ligand binding affinity prediction. Notably, ProFSA surpasses other pretraining methods by a substantial margin. Moreover, our work opens up a new avenue for mitigating the scarcity of protein-ligand complex data through the utilization of high-quality and diverse protein structure databases.
</details>
<details>
<summary>摘要</summary>
pocket 表示具有重要作用在各种生物医学应用中，如药物可能性预测、药物粘性预测和新药设计。 although existing geometric features and pre-trained representations have shown promising results, they usually treat pockets independently of ligands, neglecting the fundamental interactions between them. However, the limited number of pocket-ligand complex structures available in the PDB database (less than 100 thousand non-redundant pairs) hinders large-scale pretraining endeavors for interaction modeling. To address this constraint, we propose a novel pocket pretraining approach that leverages knowledge from high-resolution atomic protein structures, assisted by highly effective pre-trained small molecule representations. By segmenting protein structures into drug-like fragments and their corresponding pockets, we obtain a reasonable simulation of ligand-receptor interactions, resulting in the generation of over 5 million complexes. Subsequently, the pocket encoder is trained in a contrastive manner to align with the representation of pseudo-ligand furnished by some pre-trained small molecule encoders. Our method, named ProFSA, achieves state-of-the-art performance across various tasks, including pocket druggability prediction, pocket matching, and ligand binding affinity prediction. Notably, ProFSA surpasses other pretraining methods by a substantial margin. Moreover, our work opens up a new avenue for mitigating the scarcity of protein-ligand complex data through the utilization of high-quality and diverse protein structure databases.
</details></li>
</ul>
<hr>
<h2 id="COPlanner-Plan-to-Roll-Out-Conservatively-but-to-Explore-Optimistically-for-Model-Based-RL"><a href="#COPlanner-Plan-to-Roll-Out-Conservatively-but-to-Explore-Optimistically-for-Model-Based-RL" class="headerlink" title="COPlanner: Plan to Roll Out Conservatively but to Explore Optimistically for Model-Based RL"></a>COPlanner: Plan to Roll Out Conservatively but to Explore Optimistically for Model-Based RL</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07220">http://arxiv.org/abs/2310.07220</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiyao Wang, Ruijie Zheng, Yanchao Sun, Ruonan Jia, Wichayaporn Wongkamjan, Huazhe Xu, Furong Huang</li>
<li>for: 提高模型基于学习方法的效果和稳定性</li>
<li>methods: 使用保守的模型执行和优惠环境探索，以避免模型不准确地区域和减少模型预测错误的影响</li>
<li>results: 在一系列的 proprioceptive 和视觉控制任务上，使用 $\texttt{COPlanner}$ 可以显著提高模型基于方法的效率和稳定性，并且可以减少模型预测错误的影响<details>
<summary>Abstract</summary>
Dyna-style model-based reinforcement learning contains two phases: model rollouts to generate sample for policy learning and real environment exploration using current policy for dynamics model learning. However, due to the complex real-world environment, it is inevitable to learn an imperfect dynamics model with model prediction error, which can further mislead policy learning and result in sub-optimal solutions. In this paper, we propose $\texttt{COPlanner}$, a planning-driven framework for model-based methods to address the inaccurately learned dynamics model problem with conservative model rollouts and optimistic environment exploration. $\texttt{COPlanner}$ leverages an uncertainty-aware policy-guided model predictive control (UP-MPC) component to plan for multi-step uncertainty estimation. This estimated uncertainty then serves as a penalty during model rollouts and as a bonus during real environment exploration respectively, to choose actions. Consequently, $\texttt{COPlanner}$ can avoid model uncertain regions through conservative model rollouts, thereby alleviating the influence of model error. Simultaneously, it explores high-reward model uncertain regions to reduce model error actively through optimistic real environment exploration. $\texttt{COPlanner}$ is a plug-and-play framework that can be applied to any dyna-style model-based methods. Experimental results on a series of proprioceptive and visual continuous control tasks demonstrate that both sample efficiency and asymptotic performance of strong model-based methods are significantly improved combined with $\texttt{COPlanner}$.
</details>
<details>
<summary>摘要</summary>
dynamically�model�based reinforcement learning�contains two phases: model rollouts to generate samples for policy learning and real environment exploration using current policy for dynamics model learning. However, due to the complex real-world environment, it is inevitable to learn an imperfect dynamics model with model prediction error, which can further mislead policy learning and result in sub-optimal solutions. In this paper, we propose $\texttt{COPlanner}$, a planning-driven framework for model-based methods to address the inaccurately learned dynamics model problem with conservative model rollouts and optimistic environment exploration. $\texttt{COPlanner}$ leverages an uncertainty-aware policy-guided model predictive control (UP-MPC) component to plan for multi-step uncertainty estimation. This estimated uncertainty then serves as a penalty during model rollouts and as a bonus during real environment exploration respectively, to choose actions. Consequently, $\texttt{COPlanner}$ can avoid model uncertain regions through conservative model rollouts, thereby alleviating the influence of model error. Simultaneously, it explores high-reward model uncertain regions to reduce model error actively through optimistic real environment exploration. $\texttt{COPlanner}$ is a plug-and-play framework that can be applied to any dyna-style model-based methods. Experimental results on a series of proprioceptive and visual continuous control tasks demonstrate that both sample efficiency and asymptotic performance of strong model-based methods are significantly improved combined with $\texttt{COPlanner}$.
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Neural-Architecture-Search-with-Multiple-Hardware-Constraints-for-Deep-Learning-Model-Deployment-on-Tiny-IoT-Devices"><a href="#Enhancing-Neural-Architecture-Search-with-Multiple-Hardware-Constraints-for-Deep-Learning-Model-Deployment-on-Tiny-IoT-Devices" class="headerlink" title="Enhancing Neural Architecture Search with Multiple Hardware Constraints for Deep Learning Model Deployment on Tiny IoT Devices"></a>Enhancing Neural Architecture Search with Multiple Hardware Constraints for Deep Learning Model Deployment on Tiny IoT Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07217">http://arxiv.org/abs/2310.07217</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alessio Burrello, Matteo Risso, Beatrice Alessandra Motetti, Enrico Macii, Luca Benini, Daniele Jahier Pagliari</li>
<li>for: 这个研究旨在解决互联网项目中的问题，即对于互联网的设备不足的硬件限制，以及对于深度学习模型的复杂度和计算量的需求。</li>
<li>methods: 这个研究使用了多个条件搜索的方法，包括对于紧缩度和延迟的条件搜索，以及对于紧缩度和内存的条件搜索。这些方法可以实现在单一搜索中，同时满足多个条件的需求。</li>
<li>results: 这个研究的结果显示，使用这些方法可以在单一搜索中，实现对于内存和延迟的条件搜索，并且可以实现与现有的深度学习模型相比，提高了硬件限制下的性能。具体来说，这个研究可以实现87.4%的内存和54.2%的延迟的减少，同时保持与现有的深度学习模型相比的精度。<details>
<summary>Abstract</summary>
The rapid proliferation of computing domains relying on Internet of Things (IoT) devices has created a pressing need for efficient and accurate deep-learning (DL) models that can run on low-power devices. However, traditional DL models tend to be too complex and computationally intensive for typical IoT end-nodes. To address this challenge, Neural Architecture Search (NAS) has emerged as a popular design automation technique for co-optimizing the accuracy and complexity of deep neural networks. Nevertheless, existing NAS techniques require many iterations to produce a network that adheres to specific hardware constraints, such as the maximum memory available on the hardware or the maximum latency allowed by the target application. In this work, we propose a novel approach to incorporate multiple constraints into so-called Differentiable NAS optimization methods, which allows the generation, in a single shot, of a model that respects user-defined constraints on both memory and latency in a time comparable to a single standard training. The proposed approach is evaluated on five IoT-relevant benchmarks, including the MLPerf Tiny suite and Tiny ImageNet, demonstrating that, with a single search, it is possible to reduce memory and latency by 87.4% and 54.2%, respectively (as defined by our targets), while ensuring non-inferior accuracy on state-of-the-art hand-tuned deep neural networks for TinyML.
</details>
<details>
<summary>摘要</summary>
“因互联网物 Things（IoT）设备的快速普及，导致深度学习（DL）模型的效率和准确性成为当前的应用需求。然而，传统的DL模型通常是复杂过度，不适合IoT端设备的 computationally intense 环境。为解决这个挑战，深度架构搜寻（NAS）已经出现为一种受欢迎的设计自动化技术，可以同时优化网络的准确性和复杂度。然而，现有的NAS技术通常需要许多迭代才能生成一个遵循硬件紧存和延迟限制的网络。在这个工作中，我们提出了一种新的方法，可以同时包含多个紧存和延迟的硬件紧存缓冲击网络的设计，并在单一迭代中生成一个遵循用户定义的紧存和延迟限制的网络，与现有的手动游戏定义网络相比，可以降低87.4%的紧存和54.2%的延迟，同时维持非劣于标准的深度学习网络。”Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Generative-Modeling-on-Manifolds-Through-Mixture-of-Riemannian-Diffusion-Processes"><a href="#Generative-Modeling-on-Manifolds-Through-Mixture-of-Riemannian-Diffusion-Processes" class="headerlink" title="Generative Modeling on Manifolds Through Mixture of Riemannian Diffusion Processes"></a>Generative Modeling on Manifolds Through Mixture of Riemannian Diffusion Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07216">http://arxiv.org/abs/2310.07216</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jaehyeong Jo, Sung Ju Hwang</li>
<li>for: 模型数据的分布在里曼尼安 manifold 上，是许多科学领域的应用需求。但现有的生成模型在 manifold 上受到严重的异常计算成本或简化热体函数的限制，这限制了它们的可行性和缩放性。</li>
<li>methods: 我们介绍了里曼尼安扩散混合（Riemannian Diffusion Mixture），一种基于endpoint-conditioned扩散过程的原理性框架，而不是基于之前的扩散模型的减去方法。我们还提出了一种简单又高效的训练目标，可以直接应用于一般 manifold。</li>
<li>results: 我们的方法在不同的 manifold 上比前一代生成模型表现出色，可以在高维度下进行扩散，并且需要减少很多在训练过程中的伪 simulate 步骤。<details>
<summary>Abstract</summary>
Learning the distribution of data on Riemannian manifolds is crucial for modeling data from non-Euclidean space, which is required by many applications from diverse scientific fields. Yet, existing generative models on manifolds suffer from expensive divergence computation or rely on approximations of heat kernel. These limitations restrict their applicability to simple geometries and hinder scalability to high dimensions. In this work, we introduce the Riemannian Diffusion Mixture, a principled framework for building a generative process on manifolds as a mixture of endpoint-conditioned diffusion processes instead of relying on the denoising approach of previous diffusion models, for which the generative process is characterized by its drift guiding toward the most probable endpoint with respect to the geometry of the manifold. We further propose a simple yet efficient training objective for learning the mixture process, that is readily applicable to general manifolds. Our method outperforms previous generative models on various manifolds while scaling to high dimensions and requires a dramatically reduced number of in-training simulation steps for general manifolds.
</details>
<details>
<summary>摘要</summary>
学习里曼尼安数据分布是非常重要的，因为它是许多科学领域的应用所需的。然而，现有的生成模型在 manifold 上受到了严重的减法计算成本或者使用抽象的热核算法，这限制了它们的可用性和扩展性。在这项工作中，我们介绍了里曼尼安扩散混合（Riemannian Diffusion Mixture），一种主义的框架，用于在 manifold 上建立生成过程，而不是依赖于前一个扩散模型的减法方法。我们还提出了一种简单 yet efficient 的训练目标，可以轻松应用于一般 manifold。我们的方法在多种 manifold 上表现出色，可以扩展到高维度，并且需要减少很多在训练过程中的计算步骤。
</details></li>
</ul>
<hr>
<h2 id="Bridging-the-Gap-between-Newton-Raphson-Method-and-Regularized-Policy-Iteration"><a href="#Bridging-the-Gap-between-Newton-Raphson-Method-and-Regularized-Policy-Iteration" class="headerlink" title="Bridging the Gap between Newton-Raphson Method and Regularized Policy Iteration"></a>Bridging the Gap between Newton-Raphson Method and Regularized Policy Iteration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07211">http://arxiv.org/abs/2310.07211</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zeyang Li, Chuxiong Hu, Yunan Wang, Guojian Zhan, Jie Li, Shengbo Eben Li</li>
<li>for: 这篇论文主要研究了规范化学习算法中的正则化技术，具体来说是使用希尔博 entropy 作为正则化函数，并证明了这种方法与标准的新颖矩阵法相等。</li>
<li>methods: 该论文使用了正则化策略迭代法，并证明了这种方法在强转化函数下的平滑化 Bellman 方程下具有全球线性准确率 $\gamma$，同时在本地区域内也具有 quadratic 准确率。</li>
<li>results: 该论文证明了正则化策略迭代法在全球和本地区域内都具有 globally linear convergence 性，其速率为 $\gamma$，并且在本地区域内也具有 quadratic 准确率。此外，研究者还证明了一种修改后的正则化策略迭代法，即使用 finite-step policy evaluation，与不准确的新颖法相等。这种算法在 asymptotic linear convergence 性下具有 $\gamma^M$ 的速率，其中 $M$ 是策略评估中所执行的步骤数。<details>
<summary>Abstract</summary>
Regularization is one of the most important techniques in reinforcement learning algorithms. The well-known soft actor-critic algorithm is a special case of regularized policy iteration where the regularizer is chosen as Shannon entropy. Despite some empirical success of regularized policy iteration, its theoretical underpinnings remain unclear. This paper proves that regularized policy iteration is strictly equivalent to the standard Newton-Raphson method in the condition of smoothing out Bellman equation with strongly convex functions. This equivalence lays the foundation of a unified analysis for both global and local convergence behaviors of regularized policy iteration. We prove that regularized policy iteration has global linear convergence with the rate being $\gamma$ (discount factor). Furthermore, this algorithm converges quadratically once it enters a local region around the optimal value. We also show that a modified version of regularized policy iteration, i.e., with finite-step policy evaluation, is equivalent to inexact Newton method where the Newton iteration formula is solved with truncated iterations. We prove that the associated algorithm achieves an asymptotic linear convergence rate of $\gamma^M$ in which $M$ denotes the number of steps carried out in policy evaluation. Our results take a solid step towards a better understanding of the convergence properties of regularized policy iteration algorithms.
</details>
<details>
<summary>摘要</summary>
“常规化”是强化学习算法中最重要的技术之一。知名的软actor-批评算法是常规化policy迭代的特殊情况，其 régulateur选择为 entropy。虽然规范化policy迭代的实际成功，但其理论基础仍然不清楚。这篇论文证明了规范化policy迭代与标准的Newton-raphson方法在bellman方程略微化下是等价的。这种等价性为证明了规范化policy迭代的全面分析。我们证明了规范化policy迭代在discount因子γ下有全面线性减少率，其速率为γ。此外，当iteration进入一个地方圈附近的optimal值时，这种算法会 quadratic减少。我们还证明了一种修改后的规范化policy迭代算法，即在policy评估中使用有限步骤，与不准确的Newton方法相等。我们证明了这种算法在迭代过程中有 asymptotic 线性减少率，其减少率为γ^M，其中M为policy评估中进行的步数。我们的结果为规范化policy迭代算法的减少性质提供了一个坚实的步骤。
</details></li>
</ul>
<hr>
<h2 id="Robust-Safe-Reinforcement-Learning-under-Adversarial-Disturbances"><a href="#Robust-Safe-Reinforcement-Learning-under-Adversarial-Disturbances" class="headerlink" title="Robust Safe Reinforcement Learning under Adversarial Disturbances"></a>Robust Safe Reinforcement Learning under Adversarial Disturbances</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07207">http://arxiv.org/abs/2310.07207</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zeyang Li, Chuxiong Hu, Shengbo Eben Li, Jia Cheng, Yunan Wang</li>
<li>For: This paper aims to address the challenge of applying reinforcement learning to real-world control tasks while ensuring safety and robustness in the presence of external disturbances.* Methods: The proposed method uses a policy iteration scheme to solve for the robust invariant set, which is a subset of the safe set where persistent safety is possible. The method also integrates the proposed policy iteration scheme into a constrained reinforcement learning algorithm that simultaneously synthesizes the robust invariant set and uses it for constrained policy optimization.* Results: The proposed method achieves zero constraint violation with learned worst-case adversarial disturbances, while other baseline algorithms violate the safety constraints substantially. Additionally, the proposed method attains comparable performance as the baselines even in the absence of the adversary.Here are the three points in Simplified Chinese text:* For: 本研究旨在应对在真实世界控制任务中应用强化学习，保证安全和可靠性在外部干扰存在下。* Methods: 提议的方法使用政策迭代方法解决最差情况下的不变性集，并将其集成到了受限制的强化学习算法中，同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同时同 Times同时同时同 Times同 Times同 Times同 Times同 Times同 Times同 Times同 Times同 Times同 Times同 Times同 Times同 Times同 Times同 Times同 Times<details>
<summary>Abstract</summary>
Safety is a primary concern when applying reinforcement learning to real-world control tasks, especially in the presence of external disturbances. However, existing safe reinforcement learning algorithms rarely account for external disturbances, limiting their applicability and robustness in practice. To address this challenge, this paper proposes a robust safe reinforcement learning framework that tackles worst-case disturbances. First, this paper presents a policy iteration scheme to solve for the robust invariant set, i.e., a subset of the safe set, where persistent safety is only possible for states within. The key idea is to establish a two-player zero-sum game by leveraging the safety value function in Hamilton-Jacobi reachability analysis, in which the protagonist (i.e., control inputs) aims to maintain safety and the adversary (i.e., external disturbances) tries to break down safety. This paper proves that the proposed policy iteration algorithm converges monotonically to the maximal robust invariant set. Second, this paper integrates the proposed policy iteration scheme into a constrained reinforcement learning algorithm that simultaneously synthesizes the robust invariant set and uses it for constrained policy optimization. This algorithm tackles both optimality and safety, i.e., learning a policy that attains high rewards while maintaining safety under worst-case disturbances. Experiments on classic control tasks show that the proposed method achieves zero constraint violation with learned worst-case adversarial disturbances, while other baseline algorithms violate the safety constraints substantially. Our proposed method also attains comparable performance as the baselines even in the absence of the adversary.
</details>
<details>
<summary>摘要</summary>
安全是控制任务中的主要考虑因素，特别是在外部干扰存在的情况下。然而，现有的安全学习算法很少考虑外部干扰，这限制了它们在实践中的可行性和可靠性。为解决这个挑战，这篇论文提出了一种可靠安全学习框架，可以抵御最差情况下的干扰。首先，这篇论文提出了一种策略迭代算法，用于解决最差情况下的策略问题。这种算法基于哈密逊-雅可比达到可靠性的概念，在其中，控制输入（即主人公）尝试维护安全，而外部干扰（即反派）尝试打砸安全。这篇论文证明了该算法的 converges monotonic 性。其次，这篇论文将提出的策略迭代算法integrated into a constrained reinforcement learning algorithm，该算法同时Synthesizes the robust invariant set and uses it for constrained policy optimization.这个算法同时解决了最优和安全的问题，即学习一个策略，可以在最差情况下实现高的奖励，同时维护安全。在经典控制任务上进行实验，我们的提出的方法可以在面对最差情况下的干扰时，保持零的约束违反率，而其他基准算法却有很大的违反率。此外，我们的方法还可以与基准算法相比，在缺乏反派的情况下具有相似的性能。
</details></li>
</ul>
<hr>
<h2 id="Boosting-Learning-for-LDPC-Codes-to-Improve-the-Error-Floor-Performance"><a href="#Boosting-Learning-for-LDPC-Codes-to-Improve-the-Error-Floor-Performance" class="headerlink" title="Boosting Learning for LDPC Codes to Improve the Error-Floor Performance"></a>Boosting Learning for LDPC Codes to Improve the Error-Floor Performance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07194">http://arxiv.org/abs/2310.07194</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ghy1228/ldpc_error_floor">https://github.com/ghy1228/ldpc_error_floor</a></li>
<li>paper_authors: Hee-Youl Kwak, Dae-Young Yun, Yongjune Kim, Sang-Hyo Kim, Jong-Seon No</li>
<li>for: 这个论文的目的是提出一种用神经网络学习的LDPC码解码器，以消除LDPC码的错误底部效应。</li>
<li>methods: 这个论文使用了两种training方法来提高LDPC码解码器的性能：首先，通过使用 Ensemble Networks 技术，将解码器分成两个神经网络，并训练后一个神经网络来专门处理不可 corrected 的单词。其次，通过使用块式训练计划，地方训练块内的 weights，以解决迷你梯度问题。</li>
<li>results: 通过应用这些training方法于标准LDPC码中，实现了最佳的错误底部性能，并且这些方法不需要额外的硬件成本。<details>
<summary>Abstract</summary>
Low-density parity-check (LDPC) codes have been successfully commercialized in communication systems due to their strong error correction capabilities and simple decoding process. However, the error-floor phenomenon of LDPC codes, in which the error rate stops decreasing rapidly at a certain level, presents challenges for achieving extremely low error rates and deploying LDPC codes in scenarios demanding ultra-high reliability. In this work, we propose training methods for neural min-sum (NMS) decoders to eliminate the error-floor effect. First, by leveraging the boosting learning technique of ensemble networks, we divide the decoding network into two neural decoders and train the post decoder to be specialized for uncorrected words that the first decoder fails to correct. Secondly, to address the vanishing gradient issue in training, we introduce a block-wise training schedule that locally trains a block of weights while retraining the preceding block. Lastly, we show that assigning different weights to unsatisfied check nodes effectively lowers the error-floor with a minimal number of weights. By applying these training methods to standard LDPC codes, we achieve the best error-floor performance compared to other decoding methods. The proposed NMS decoder, optimized solely through novel training methods without additional modules, can be integrated into existing LDPC decoders without incurring extra hardware costs. The source code is available at https://github.com/ghy1228/LDPC_Error_Floor .
</details>
<details>
<summary>摘要</summary>
低密度正交码（LDPC）编码器在通信系统中得到成功，这是因为它们具有强大的错误检测能力和简单的解码过程。然而，LDPC编码器的错误地面现象，即错误率停止减少快速的现象，对于实现极低的错误率和在需要极高可靠性的场景中部署LDPC编码器带来挑战。在这种情况下，我们提出了使用神经网络的训练方法来消除错误地面现象。首先，我们利用了聚合学习技术，将解码网络分成两个神经网络，并将后台解码器特化为无法被首个解码器正确地解码的词语。其次，为了解决训练过程中的渐进性问题，我们引入了分割训练计划，当地方训练一个块的 weights 时，还会重新训练前一个块的 weights。最后，我们发现，对不满足的检查节点分配不同的权重，可以有效地降低错误地面。通过这些训练方法，我们在标准LDPC编码器上实现了最佳的错误地面性能，并且这些训练方法不需要额外的硬件成本。源代码可以在 <https://github.com/ghy1228/LDPC_Error_Floor> 上下载。
</details></li>
</ul>
<hr>
<h2 id="Neural-networks-deep-shallow-or-in-between"><a href="#Neural-networks-deep-shallow-or-in-between" class="headerlink" title="Neural networks: deep, shallow, or in between?"></a>Neural networks: deep, shallow, or in between?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07190">http://arxiv.org/abs/2310.07190</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/djdprogramming/adfa2">https://github.com/djdprogramming/adfa2</a></li>
<li>paper_authors: Guergana Petrova, Przemyslaw Wojtaszczyk</li>
<li>for: 估算一个封闭集合从巴拿赫空间中的近似误差。</li>
<li>methods: 使用带width W、深度 l 和 lipschitz 活动函数的批处理神经网络输出来估算。</li>
<li>results: 显示，当depth l 往infty 时，可以达到更好的估算精度，但是 fixing depth 并 letting width W 往infty 无法提高估算精度。<details>
<summary>Abstract</summary>
We give estimates from below for the error of approximation of a compact subset from a Banach space by the outputs of feed-forward neural networks with width W, depth l and Lipschitz activation functions. We show that, modulo logarithmic factors, rates better that entropy numbers' rates are possibly attainable only for neural networks for which the depth l goes to infinity, and that there is no gain if we fix the depth and let the width W go to infinity.
</details>
<details>
<summary>摘要</summary>
我们提供以下估计 для准确度近似compactsubset从Banach空间的迪拜恩网络输出。我们显示，对于具有无限深度l的神经网络，可以达到比Entropy数字更好的速率，但是如果将宽度W fix，则无法获得更好的性能。Here's the breakdown of the translation:* "compact subset" is translated as "compact subset" ( compact subset 是 compact subset)* "Banach space" is translated as "Banach空间" ( Banach 空间)* "feed-forward neural networks" is translated as "迪拜恩网络" ( 迪拜恩网络)* "width" is translated as "宽度" ( 宽度)* "depth" is translated as "深度" ( 深度)* "Lipschitz activation functions" is translated as "Lipschitz激活函数" ( Lipschitz 激活函数)* "entropy numbers" is translated as "Entropy数字" ( Entropy数字)* "modulo logarithmic factors" is translated as "除alogarithmic因子" ( 除alogarithmic因子)* "rates" is translated as "速率" ( 速率)I hope this helps! Let me know if you have any further questions.
</details></li>
</ul>
<hr>
<h2 id="Kernel-Cox-partially-linear-regression-building-predictive-models-for-cancer-patients’-survival"><a href="#Kernel-Cox-partially-linear-regression-building-predictive-models-for-cancer-patients’-survival" class="headerlink" title="Kernel Cox partially linear regression: building predictive models for cancer patients’ survival"></a>Kernel Cox partially linear regression: building predictive models for cancer patients’ survival</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07187">http://arxiv.org/abs/2310.07187</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rongyaohua/reggkm">https://github.com/rongyaohua/reggkm</a></li>
<li>paper_authors: Yaohua Rong, Sihai Dave Zhao, Xia Zheng, Yi Li</li>
<li>for: 预测肿瘤患者的临床结果，准确预测肿瘤患者的生存时间。</li>
<li>methods: 使用kernel Cox准对分析方法和RegGKM方法，同时自动除除无关参数和非参数。</li>
<li>results: 对多重肿瘤数据集进行分析，预测患者的死亡负担基于基因表达，并可以将患者分为不同的死亡风险组。<details>
<summary>Abstract</summary>
Wide heterogeneity exists in cancer patients' survival, ranging from a few months to several decades. To accurately predict clinical outcomes, it is vital to build an accurate predictive model that relates patients' molecular profiles with patients' survival. With complex relationships between survival and high-dimensional molecular predictors, it is challenging to conduct non-parametric modeling and irrelevant predictors removing simultaneously. In this paper, we build a kernel Cox proportional hazards semi-parametric model and propose a novel regularized garrotized kernel machine (RegGKM) method to fit the model. We use the kernel machine method to describe the complex relationship between survival and predictors, while automatically removing irrelevant parametric and non-parametric predictors through a LASSO penalty. An efficient high-dimensional algorithm is developed for the proposed method. Comparison with other competing methods in simulation shows that the proposed method always has better predictive accuracy. We apply this method to analyze a multiple myeloma dataset and predict patients' death burden based on their gene expressions. Our results can help classify patients into groups with different death risks, facilitating treatment for better clinical outcomes.
</details>
<details>
<summary>摘要</summary>
广泛的多样性存在于癌症患者的存活时间，从几个月到多个 décadas。为准确预测临床结果，建立一个准确预测模型，将患者的分子 Profiling 与患者的存活时间相关联是非常重要。由于存活时间与高维分子预测器之间存在复杂的关系，同时需要进行非参数化模型和不相关预测器的自动 removing，因此在这篇论文中，我们构建了一个kernel Cox准确预测模型，并提出了一种新的RegGKM方法来适应该问题。我们使用kernel机器方法来描述存活时间和预测器之间的复杂关系，同时通过LASSO penalty自动地除掉了无关的参数化和非参数化预测器。我们开发了一种高维度的高效算法来解决该问题。与其他竞争方法在模拟中比较，我们的方法总是有更高的预测准确性。我们在分析多发性骨髓癌数据集时应用了该方法，并预测了患者的死亡负担基于他们的基因表达。我们的结果可以帮助将患者分组为不同的死亡风险类别，以便进行更好的临床结果。
</details></li>
</ul>
<hr>
<h2 id="SAM-OCTA-Prompting-Segment-Anything-for-OCTA-Image-Segmentation"><a href="#SAM-OCTA-Prompting-Segment-Anything-for-OCTA-Image-Segmentation" class="headerlink" title="SAM-OCTA: Prompting Segment-Anything for OCTA Image Segmentation"></a>SAM-OCTA: Prompting Segment-Anything for OCTA Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07183">http://arxiv.org/abs/2310.07183</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shellredia/sam-octa">https://github.com/shellredia/sam-octa</a></li>
<li>paper_authors: Xinrun Chen, Chengliang Wang, Haojian Ning, Shiying Li</li>
<li>For:  This paper proposes a new method for segmenting specific targets in optical coherence tomography angiography (OCTA) images, which is useful for diagnosing and treating eye diseases.* Methods: The proposed method, named SAM-OCTA, uses a low-rank adaptation technique for fine-tuning a foundation model and generates prompt points for various segmentation tasks on OCTA datasets.* Results: SAM-OCTA achieves or approaches state-of-the-art segmentation performance metrics on two publicly available OCTA datasets (OCTA-500 and ROSE), and demonstrates effective local vessel segmentation and artery-vein segmentation, which were not well-solved in previous works.<details>
<summary>Abstract</summary>
In the analysis of optical coherence tomography angiography (OCTA) images, the operation of segmenting specific targets is necessary. Existing methods typically train on supervised datasets with limited samples (approximately a few hundred), which can lead to overfitting. To address this, the low-rank adaptation technique is adopted for foundation model fine-tuning and proposed corresponding prompt point generation strategies to process various segmentation tasks on OCTA datasets. This method is named SAM-OCTA and has been experimented on the publicly available OCTA-500 and ROSE datasets. This method achieves or approaches state-of-the-art segmentation performance metrics. The effect and applicability of prompt points are discussed in detail for the retinal vessel, foveal avascular zone, capillary, artery, and vein segmentation tasks. Furthermore, SAM-OCTA accomplishes local vessel segmentation and effective artery-vein segmentation, which was not well-solved in previous works. The code is available at https://github.com/ShellRedia/SAM-OCTA.
</details>
<details>
<summary>摘要</summary>
在Optical coherence tomography angiography（OCTA）图像分析中，需要进行特定目标的分割。现有方法通常是在有限样本（约百个）上进行监督学习，这可能导致过拟合。为解决这问题，我们采用了低级适应技术进行基础模型细化和提出了相应的提示点生成策略，以处理不同的分割任务。我们称之为SAM-OCTA，并在公共可用OCTA-500和ROSE数据集上进行了实验。SAM-OCTA方法达到或接近状态 искусственный智能分割性能指标。我们在retinal vessel、foveal avascular zone、capillary、artery和vein分割任务中详细介绍了提示点的效果和适用性。此外，SAM-OCTA还实现了本地血管分割和有效的artery-vein分割，这在前一次作品中没有得到妥善解决。代码可以在https://github.com/ShellRedia/SAM-OCTA上获取。
</details></li>
</ul>
<hr>
<h2 id="Generalized-Neural-Sorting-Networks-with-Error-Free-Differentiable-Swap-Functions"><a href="#Generalized-Neural-Sorting-Networks-with-Error-Free-Differentiable-Swap-Functions" class="headerlink" title="Generalized Neural Sorting Networks with Error-Free Differentiable Swap Functions"></a>Generalized Neural Sorting Networks with Error-Free Differentiable Swap Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07174">http://arxiv.org/abs/2310.07174</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jungtaek Kim, Jeongbeen Yoon, Minsu Cho</li>
<li>for: 本研究旨在探讨sorting问题的更加抽象且表达强的输入，如多 digit 图像和图像碎片，通过神经网络 sorting。</li>
<li>methods: 我们使用了一种具有梯度可信度的排序网络，并开发了一种不减少的交换函数来保证排序网络的导数性。</li>
<li>results: 我们的方法在多种排序 benchmark 上表现了比或相当于基eline方法。<details>
<summary>Abstract</summary>
Sorting is a fundamental operation of all computer systems, having been a long-standing significant research topic. Beyond the problem formulation of traditional sorting algorithms, we consider sorting problems for more abstract yet expressive inputs, e.g., multi-digit images and image fragments, through a neural sorting network. To learn a mapping from a high-dimensional input to an ordinal variable, the differentiability of sorting networks needs to be guaranteed. In this paper we define a softening error by a differentiable swap function, and develop an error-free swap function that holds non-decreasing and differentiability conditions. Furthermore, a permutation-equivariant Transformer network with multi-head attention is adopted to capture dependency between given inputs and also leverage its model capacity with self-attention. Experiments on diverse sorting benchmarks show that our methods perform better than or comparable to baseline methods.
</details>
<details>
<summary>摘要</summary>
“排序是计算机系统中的基本操作，已经是长期着重研究的主题。我们在传统排序算法的问题形ulation上，考虑到更抽象且表达力强的输入，例如多位数字和图像碎片，透过神经网络进行排序。为了从高维输入学习一个排序顺序，排序网络的 diferenciability 需要保证。在这篇论文中，我们定义了一个滑动错误函数，并开发了一个无错误的交换函数，它保证了非减少和渐近条件。此外，我们还使用了一个具有对称性的Transformer网络，并使用多头注意力来捕捉输入的依赖关系。实验结果显示，我们的方法在多种排序实验中表现更好或相近于基eline方法。”Note that Simplified Chinese is used in the translation, as it is the more commonly used standard for Chinese writing.
</details></li>
</ul>
<hr>
<h2 id="Federated-Generalization-via-Information-Theoretic-Distribution-Diversification"><a href="#Federated-Generalization-via-Information-Theoretic-Distribution-Diversification" class="headerlink" title="Federated Generalization via Information-Theoretic Distribution Diversification"></a>Federated Generalization via Information-Theoretic Distribution Diversification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07171">http://arxiv.org/abs/2310.07171</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zheshun Wu, Zenglin Xu, Dun Zeng, Qifan Wang</li>
<li>For: The paper focuses on addressing the non-Independent Identically Distributed (non-IID) challenge in Federated Learning (FL), which is a significant hurdle to FL’s generalization efficacy.* Methods: The paper proposes an information-theoretic generalization framework for FL, which quantifies generalization errors by evaluating the information entropy of local distributions and discerning discrepancies across these distributions. The paper also introduces a weighted aggregation approach and a duo of client selection strategies to bolster FL’s generalization prowess.* Results: The paper’s extensive empirical evaluations reaffirm the potency of the proposed methods, aligning seamlessly with the theoretical construct.<details>
<summary>Abstract</summary>
Federated Learning (FL) has surged in prominence due to its capability of collaborative model training without direct data sharing. However, the vast disparity in local data distributions among clients, often termed the non-Independent Identically Distributed (non-IID) challenge, poses a significant hurdle to FL's generalization efficacy. The scenario becomes even more complex when not all clients participate in the training process, a common occurrence due to unstable network connections or limited computational capacities. This can greatly complicate the assessment of the trained models' generalization abilities. While a plethora of recent studies has centered on the generalization gap pertaining to unseen data from participating clients with diverse distributions, the divergence between the training distributions of participating clients and the testing distributions of non-participating ones has been largely overlooked. In response, our paper unveils an information-theoretic generalization framework for FL. Specifically, it quantifies generalization errors by evaluating the information entropy of local distributions and discerning discrepancies across these distributions. Inspired by our deduced generalization bounds, we introduce a weighted aggregation approach and a duo of client selection strategies. These innovations aim to bolster FL's generalization prowess by encompassing a more varied set of client data distributions. Our extensive empirical evaluations reaffirm the potency of our proposed methods, aligning seamlessly with our theoretical construct.
</details>
<details>
<summary>摘要</summary>
Federated Learning (FL) 在合作模型训练无需直接数据共享的能力下崛起。然而，本地数据分布的巨大差异（非独立同分布，non-IID）问题对 FL 的通用效果提出了 significiant 挑战。这种情况变得更加复杂，当不 все客户端参与训练过程时，这是由于网络连接不稳定或计算资源有限的情况。这可能会很大地复杂训练过程中模型的通用能力评估。而在当前的研究中，大多数研究都集中在参与训练的客户端数据的不同分布上，而忽略了参与训练的客户端数据与测试数据的分布之间的差异。为回应这一问题，我们的论文揭示了一种信息学基本的通用框架，具体来说是通过评估本地分布中的信息熵来评估模型的通用错误。以我们的推导出的通用 bound 为导向，我们引入了权重聚合方法和两种客户端选择策略。这些创新目的是通过包括更多客户端数据分布的方式来提高 FL 的通用能力。我们的实验证明了我们的提议的力量，与我们的理论构造一致。
</details></li>
</ul>
<hr>
<h2 id="LLark-A-Multimodal-Foundation-Model-for-Music"><a href="#LLark-A-Multimodal-Foundation-Model-for-Music" class="headerlink" title="LLark: A Multimodal Foundation Model for Music"></a>LLark: A Multimodal Foundation Model for Music</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07160">http://arxiv.org/abs/2310.07160</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/spotify-research/llark">https://github.com/spotify-research/llark</a></li>
<li>paper_authors: Josh Gardner, Simon Durand, Daniel Stoller, Rachel M. Bittner</li>
<li>for: 本研究旨在开发一种用于音乐理解的多Modal模型（LLark），以便更好地理解音乐的结构和特点。</li>
<li>methods: 本研究使用了多种数据集的扩充和指令调整方法来训练LLark模型，并将音乐和语言模型集成在一起。</li>
<li>results: 在三类任务（音乐理解、描述和逻辑）中，我们的模型可以在零shot泛化中与现有基eline匹配或超越它们，而人类在描述和逻辑任务中也与模型的回答有高度一致性。I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Music has a unique and complex structure which is challenging for both expert humans and existing AI systems to understand, and presents unique challenges relative to other forms of audio. We present LLark, an instruction-tuned multimodal model for music understanding. We detail our process for dataset creation, which involves augmenting the annotations of diverse open-source music datasets and converting them to a unified instruction-tuning format. We propose a multimodal architecture for LLark, integrating a pretrained generative model for music with a pretrained language model. In evaluations on three types of tasks (music understanding, captioning, and reasoning), we show that our model matches or outperforms existing baselines in zero-shot generalization for music understanding, and that humans show a high degree of agreement with the model's responses in captioning and reasoning tasks. LLark is trained entirely from open-source music data and models, and we make our training code available along with the release of this paper. Additional results and audio examples are at https://bit.ly/llark, and our source code is available at https://github.com/spotify-research/llark .
</details>
<details>
<summary>摘要</summary>
音乐具有独特和复杂的结构，对于专业人员和现有的人工智能系统来说都是挑战，与其他形式的音频不同。我们介绍了LLark，一种基于指令调整的多Modal模型，用于音乐理解。我们详细介绍了我们的数据创建过程，包括对多种开源音乐数据集的扩充和转换为一致的指令调整格式。我们提议一种多Modal架构，将音乐生成模型和语言模型集成。在三种任务（音乐理解、captioning和理解）的评估中，我们显示了我们的模型在零shot泛化中与现有基elines匹配或超越，而人类在captioning和理解任务中与模型的回答达到了高度一致。LLark通过 entirely从开源音乐数据和模型进行训练，我们在发表这篇论文时提供了训练代码，可以在https://bit.ly/llark找到更多的结果和音频示例。我们的源代码可以在https://github.com/spotify-research/llark 上找到。
</details></li>
</ul>
<hr>
<h2 id="Imitation-Learning-from-Purified-Demonstration"><a href="#Imitation-Learning-from-Purified-Demonstration" class="headerlink" title="Imitation Learning from Purified Demonstration"></a>Imitation Learning from Purified Demonstration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07143">http://arxiv.org/abs/2310.07143</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunke Wang, Minjing Dong, Bo Du, Chang Xu</li>
<li>For: Addressing sequential decision-making problems with imperfect expert demonstrations.* Methods: Purifying potential perturbations in imperfect demonstrations via a two-step diffusion process, and then conducting imitation learning from the purified demonstrations.* Results: Theoretical evidence supporting the approach, and evaluation results on MuJoCo demonstrating effectiveness from different aspects.Here’s the summary in Traditional Chinese:* For: 解决受到不完整专家示范的序列做决策问题。* Methods: 使用两步Diffusion过程缓和潜在随机变动，然后从缓和的示范中学习。* Results: 提供了对方法的理论支持，并在MuJoCo上进行了不同方面的评估。<details>
<summary>Abstract</summary>
Imitation learning has emerged as a promising approach for addressing sequential decision-making problems, with the assumption that expert demonstrations are optimal. However, in real-world scenarios, expert demonstrations are often imperfect, leading to challenges in effectively applying imitation learning. While existing research has focused on optimizing with imperfect demonstrations, the training typically requires a certain proportion of optimal demonstrations to guarantee performance. To tackle these problems, we propose to purify the potential perturbations in imperfect demonstrations and subsequently conduct imitation learning from purified demonstrations. Motivated by the success of diffusion models, we introduce a two-step purification via the diffusion process. In the first step, we apply a forward diffusion process to effectively smooth out the potential perturbations in imperfect demonstrations by introducing additional noise. Subsequently, a reverse generative process is utilized to recover the optimal expert demonstrations from the diffused ones. We provide theoretical evidence supporting our approach, demonstrating that total variance distance between the purified and optimal demonstration distributions can be upper-bounded. The evaluation results on MuJoCo demonstrate the effectiveness of our method from different aspects.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate="zh-CN"</SYS>Sequential decision-making problems 有 Emerged as a promising approach imitation learning, with the assumption that expert demonstrations are optimal. However, in real-world scenarios, expert demonstrations are often imperfect, leading to challenges in effectively applying imitation learning. While existing research has focused on optimizing with imperfect demonstrations, the training typically requires a certain proportion of optimal demonstrations to guarantee performance. To tackle these problems, we propose to purify the potential perturbations in imperfect demonstrations and subsequently conduct imitation learning from purified demonstrations.受 diffusion models 的成功所 inspirited, we introduce a two-step purification via the diffusion process. In the first step, we apply a forward diffusion process to effectively smooth out the potential perturbations in imperfect demonstrations by introducing additional noise. Subsequently, a reverse generative process is utilized to recover the optimal expert demonstrations from the diffused ones. We provide theoretical evidence supporting our approach, demonstrating that total variance distance between the purified and optimal demonstration distributions can be upper-bounded. The evaluation results on MuJoCo demonstrate the effectiveness of our method from different aspects.
</details></li>
</ul>
<hr>
<h2 id="Risk-Assessment-and-Statistical-Significance-in-the-Age-of-Foundation-Models"><a href="#Risk-Assessment-and-Statistical-Significance-in-the-Age-of-Foundation-Models" class="headerlink" title="Risk Assessment and Statistical Significance in the Age of Foundation Models"></a>Risk Assessment and Statistical Significance in the Age of Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07132">http://arxiv.org/abs/2310.07132</a></li>
<li>repo_url: None</li>
<li>paper_authors: Apoorva Nitsure, Youssef Mroueh, Mattia Rigotti, Kristjan Greenewald, Brian Belgodere, Mikhail Yurochkin, Jiri Navratil, Igor Melnyk, Jerret Ross</li>
<li>for: 本研究旨在评估基础模型中的社会技术风险，并使用量化的统计学 significado进行评估。</li>
<li>methods: 本研究使用了一种新的统计相对测试，基于实际随机变量的首次和第二次统计学上的准则。这种测试的第二个统计学是与经济统计和金融数学中常用的mean-risk模型相关的。</li>
<li>results: 本研究通过定义一个”指标股票”来汇总一系列指标，并使用这个股票来选择基础模型。我们也提供了一种基于风险意识的模型选择方法，并通过 bootstrap variance estimate来支持统计学上的分析。通过使用这些方法，我们对不同的大语言模型进行了风险相关的比较。<details>
<summary>Abstract</summary>
We propose a distributional framework for assessing socio-technical risks of foundation models with quantified statistical significance. Our approach hinges on a new statistical relative testing based on first and second order stochastic dominance of real random variables. We show that the second order statistics in this test are linked to mean-risk models commonly used in econometrics and mathematical finance to balance risk and utility when choosing between alternatives. Using this framework, we formally develop a risk-aware approach for foundation model selection given guardrails quantified by specified metrics. Inspired by portfolio optimization and selection theory in mathematical finance, we define a \emph{metrics portfolio} for each model as a means to aggregate a collection of metrics, and perform model selection based on the stochastic dominance of these portfolios. The statistical significance of our tests is backed theoretically by an asymptotic analysis via central limit theorems instantiated in practice via a bootstrap variance estimate. We use our framework to compare various large language models regarding risks related to drifting from instructions and outputting toxic content.
</details>
<details>
<summary>摘要</summary>
我们提出了一个分布式框架，用于评估基础模型的社会技术风险，并使用量化的统计学 significado。我们的方法基于新的随机变量测试，使用第一和第二个泛化随机变量的统计学上的比较。我们表明了这些测试的第二个统计学与经济数学和金融中常用的均值风险模型相关。使用这个框架，我们正式开发了一种基于风险意识的模型选择方法，给出了 guardrails 量化的指标。 inspirited by 数学金融中的股票选择理论，我们定义了一个“指标股票” для每个模型，以便对一系列指标进行汇总，并根据这些股票的随机上度来进行模型选择。我们的统计学 significado 是通过中心假设定理和 bootstrap 方法来证明的，并在实践中通过 bootstrap 假设来估计偏差。我们使用我们的框架来比较不同的大语言模型，关于从指令脱离和输出恶意内容的风险。
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-Methods-for-Background-Potential-Estimation-in-2DEGs"><a href="#Machine-Learning-Methods-for-Background-Potential-Estimation-in-2DEGs" class="headerlink" title="Machine Learning Methods for Background Potential Estimation in 2DEGs"></a>Machine Learning Methods for Background Potential Estimation in 2DEGs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07089">http://arxiv.org/abs/2310.07089</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carlo da Cunha, Nobuyuki Aoki, David Ferry, Kevin Vora, Yu Zhang<br>for: 这研究探讨了二维电子晕（2DEGs）中含有杂stoff和瑕疵的问题，以及这些问题如何影响碰撞载子 mobilit、导电性和量子准确时间。methods: 这个研究使用了扫描门微镜（SGM）技术，并应用了三种不同的机器学习算法来估算2DEGs的背景潜能：生成 adversarial neural network、细胞神经网络和进化搜索算法。results: 研究发现，使用进化搜索算法可以很有效地估算2DEGs的背景潜能，并提供了一种新的瑕疵分析方法。这项研究不仅提高了我们对2DEGs的理解，还强调了机器学习在探索量子材料方面的潜在优势，对量子计算和纳но电子学习有重要意义。<details>
<summary>Abstract</summary>
In the realm of quantum-effect devices and materials, two-dimensional electron gases (2DEGs) stand as fundamental structures that promise transformative technologies. However, the presence of impurities and defects in 2DEGs poses substantial challenges, impacting carrier mobility, conductivity, and quantum coherence time. To address this, we harness the power of scanning gate microscopy (SGM) and employ three distinct machine learning techniques to estimate the background potential of 2DEGs from SGM data: image-to-image translation using generative adversarial neural networks, cellular neural network, and evolutionary search. Our findings, despite data constraints, highlight the effectiveness of an evolutionary search algorithm in this context, offering a novel approach for defect analysis. This work not only advances our understanding of 2DEGs but also underscores the potential of machine learning in probing quantum materials, with implications for quantum computing and nanoelectronics.
</details>
<details>
<summary>摘要</summary>
在量子效应设备和材料领域中，二维电子液体（2DEG）作为基本结构，承诺引领技术转型。然而，2DEG中的杂质和缺陷对载子移动性、导电性和量子同步时间产生了显著影响。为此，我们利用扫描门微scanning gate microscopy（SGM）数据，并采用三种不同的机器学习技术来估算2DEG背景电势：图像到图像翻译使用生成对抗神经网络、细胞神经网络和进化搜索。我们的发现，尽管数据有限制，表明了进化搜索算法在这种情况下的效果，提供了一种新的缺陷分析方法。这种工作不仅提高了我们对2DEG的理解，也强调了机器学习在探测量子材料方面的潜在潜力，对量子计算和纳но电子学习有重要意义。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/11/cs.LG_2023_10_11/" data-id="clpahu76f00t93h88cw9a8sf7" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_10_11" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/11/eess.IV_2023_10_11/" class="article-date">
  <time datetime="2023-10-11T09:00:00.000Z" itemprop="datePublished">2023-10-11</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/11/eess.IV_2023_10_11/">eess.IV - 2023-10-11</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Time-Resolved-Reconstruction-of-Motion-Force-and-Stiffness-using-Spectro-Dynamic-MRI"><a href="#Time-Resolved-Reconstruction-of-Motion-Force-and-Stiffness-using-Spectro-Dynamic-MRI" class="headerlink" title="Time-Resolved Reconstruction of Motion, Force, and Stiffness using Spectro-Dynamic MRI"></a>Time-Resolved Reconstruction of Motion, Force, and Stiffness using Spectro-Dynamic MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07622">http://arxiv.org/abs/2310.07622</a></li>
<li>repo_url: None</li>
<li>paper_authors: Max H. C. van Riel, Tristan van Leeuwen, Cornelis A. T. van den Berg, Alessandro Sbrizzi</li>
<li>for: 本研究旨在掌握肌肉和关节的动态特性和物理性质，以便更好地理解肌肉的（病）physiology。</li>
<li>methods: 本研究使用了Spectro-Dynamic MRI技术，可以直接从k-space数据中获得高空间和时间分辨率的动态系统特性。</li>
<li>results: 本研究提出了一种扩展的Spectro-Dynamic MRI框架，可以重建1) 时间分辨率为11 ms的MR图像，2) 时间分辨率为11 ms的运动场图像，3) 动态参数，以及4) 活动力的激活力。此外，该方法还比two-step方法（先从不含运动信息的下探测数据中重建时间分辨率MR图像，然后使用运动场图像进行运动估算）表现更好。<details>
<summary>Abstract</summary>
Measuring the dynamics and mechanical properties of muscles and joints is important to understand the (patho)physiology of muscles. However, acquiring dynamic time-resolved MRI data is challenging. We have previously developed Spectro-Dynamic MRI which allows the characterization of dynamical systems at a high spatial and temporal resolution directly from k-space data. This work presents an extended Spectro-Dynamic MRI framework that reconstructs 1) time-resolved MR images, 2) time-resolved motion fields, 3) dynamical parameters, and 4) an activation force, at a temporal resolution of 11 ms. An iterative algorithm solves a minimization problem containing four terms: a motion model relating the motion to the fully-sampled k-space data, a dynamical model describing the expected type of dynamics, a data consistency term describing the undersampling pattern, and finally a regularization term for the activation force. We acquired MRI data using a dynamic motion phantom programmed to move like an actively driven linear elastic system, from which all dynamic variables could be accurately reconstructed, regardless of the sampling pattern. The proposed method performed better than a two-step approach, where time-resolved images were first reconstructed from the undersampled data without any information about the motion, followed by a motion estimation step.
</details>
<details>
<summary>摘要</summary>
An iterative algorithm solves a minimization problem with four terms: a motion model relating motion to fully-sampled k-space data, a dynamical model describing expected dynamics, a data consistency term describing undersampling pattern, and a regularization term for activation force. We acquired MRI data using a dynamic motion phantom programmed to move like an actively driven linear elastic system, from which all dynamic variables could be accurately reconstructed, regardless of sampling pattern. The proposed method outperformed a two-step approach where time-resolved images were first reconstructed without motion information and then motion was estimated.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/11/eess.IV_2023_10_11/" data-id="clpahu7e101c13h883uu9fvym" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_10_11" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/11/eess.SP_2023_10_11/" class="article-date">
  <time datetime="2023-10-11T08:00:00.000Z" itemprop="datePublished">2023-10-11</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/11/eess.SP_2023_10_11/">eess.SP - 2023-10-11</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Kronecker-structured-Sparse-Vector-Recovery-with-Application-to-IRS-MIMO-Channel-Estimation"><a href="#Kronecker-structured-Sparse-Vector-Recovery-with-Application-to-IRS-MIMO-Channel-Estimation" class="headerlink" title="Kronecker-structured Sparse Vector Recovery with Application to IRS-MIMO Channel Estimation"></a>Kronecker-structured Sparse Vector Recovery with Application to IRS-MIMO Channel Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07869">http://arxiv.org/abs/2310.07869</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanbin He, Geethu Joseph</li>
<li>for: 本文研究了一种 Kronecker 结构杂度矩阵稀疏 вектор回归问题，该问题在多个实际应用中出现，如智能反射面 aid 多输入多出力系统的稀疏通道估计。先前的方法仅利用 Kronecker 结构在稀疏 вектор的支持和非零元素上，解决整个线性系统，导致计算复杂性高。而我们则将原始稀疏回归问题分解成多个独立的子问题，并将它们解决一个一个。通过 Kronecker 乘法，我们可以保留稀疏 веctor 的结构，包括支持和非零元素。我们的 simulations 表明，我们的方法在精度和运行时间两个方面与先前的工作相比，有更高的性能。</li>
<li>methods: 我们的方法包括将原始稀疏回归问题分解成多个独立的子问题，并将它们解决一个一个。然后，通过 Kronecker 乘法，我们可以将独立解决的子问题结果相加以获得稀疏 веctor。</li>
<li>results: 我们的 simulations 表明，我们的方法在精度和运行时间两个方面与先前的工作相比，有更高的性能。我们归因此于解决空间减少和去噪效果。<details>
<summary>Abstract</summary>
This paper studies the problem of Kronecker-structured sparse vector recovery from an underdetermined linear system with a Kronecker-structured dictionary. Such a problem arises in many real-world applications such as the sparse channel estimation of an intelligent reflecting surface-aided multiple-input multiple-output system. The prior art only exploits the Kronecker structure in the support of the sparse vector and solves the entire linear system together leading to high computational complexity. Instead, we break down the original sparse recovery problem into multiple independent sub-problems and solve them individually. We obtain the sparse vector as the Kronecker product of the individual solutions, retaining its structure in both support and nonzero entries. Our simulations demonstrate the superior performance of our methods in terms of accuracy and run time compared with the existing works, using synthetic data and the channel estimation application. We attribute the low run time to the reduced solution space due to the additional structure and improved accuracy to the denoising effect owing to the decomposition step.
</details>
<details>
<summary>摘要</summary>
In contrast, our approach breaks down the original sparse recovery problem into multiple independent sub-problems and solves them individually. We obtain the sparse vector as the Kronecker product of the individual solutions, retaining its structure in both support and nonzero entries. Our simulations show that our methods outperform existing works in terms of accuracy and run time, using synthetic data and the channel estimation application. We attribute the low run time to the reduced solution space due to the additional structure and improved accuracy to the denoising effect of the decomposition step.
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Quantization-for-Key-Generation-in-Low-Power-Wide-Area-Networks"><a href="#Adaptive-Quantization-for-Key-Generation-in-Low-Power-Wide-Area-Networks" class="headerlink" title="Adaptive Quantization for Key Generation in Low-Power Wide-Area Networks"></a>Adaptive Quantization for Key Generation in Low-Power Wide-Area Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07853">http://arxiv.org/abs/2310.07853</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Chen, Junqing Zhang, Yingying Chen</li>
<li>for: 本研究旨在提高资源有限低功率宽域网络（LPWAN）的物理层密钥生成方法，通过基于反射和随机无线通信的物理层密钥生成方法，提高 LPWAN 的安全性。</li>
<li>methods: 本研究提出了一种基于 Lempel-Ziv 复杂度（LZ76）的自适应量化方案，可以在 LPWAN 中动态调整量化参数，以根据各个时间段的 RSSI 测量结果的随机性来生成密钥。此外，我们还提出了一种在实时密钥生成过程中进行信息重新匹配的卡利ibration scheme，以确保键的均匀分布。</li>
<li>results: 实验结果表明，对于 LoRa 设备，我们的自适应量化方案可以与对比方法（差分量化和固定量化）相比，提高 KGR 的表现，最高可以达到 2.35 倍和 1.51 倍的提升。<details>
<summary>Abstract</summary>
Physical layer key generation based on reciprocal and random wireless channels has been an attractive solution for securing resource-constrained low-power wide-area networks (LPWANs). When quantizing channel measurements, namely received signal strength indicator (RSSI), into key bits, the existing works mainly adopt fixed quantization levels and guard band parameters, which fail to fully extract keys from RSSI measurements. In this paper, we propose a novel adaptive quantization scheme for key generation in LPWANs, taking LoRa as a case study. The proposed adaptive quantization scheme can dynamically adjust the quantization parameters according to the randomness of RSSI measurements estimated by Lempel-Ziv complexity (LZ76), while ensuring a predefined key disagreement ratio (KDR). Specifically, our scheme uses pre-trained linear regression models to determine the appropriate quantization level and guard band parameter for each segment of RSSI measurements. Moreover, we propose a guard band parameter calibration scheme during information reconciliation during real-time key generation operation. Experimental evaluations using LoRa devices show that the proposed adaptive quantization scheme outperforms the benchmark differential quantization and fixed quantization with up to 2.35$\times$ and 1.51$\times$ key generation rate (KGR) gains, respectively.
</details>
<details>
<summary>摘要</summary>
物理层密钥生成基于相互反射和随机无线通道已成为优化资源充足低功率宽域网络（LPWAN）的一个吸引人的解决方案。当量化通道测量值（RSSI）为密钥位时，现有的工作主要采用固定量化水平和保障带参数，这些参数不足以完全从RSSI测量值中提取密钥。在本文中，我们提出了一种新的自适应量化方案，用于LPWAN中的密钥生成，具体来说是通过Lempel-Ziv复杂度（LZ76）来估计RSSI测量值的随机性，并保证一定的密钥分歧率（KDR）。specifically，我们的方案使用预训练的线性回归模型来确定每个RSSI测量值段的合适的量化水平和保障带参数。此外，我们还提出了在实时密钥生成过程中进行信息重新协调的保射参数calibration方案。实验使用LoRa设备显示，我们的自适应量化方案与参考 differential量化和固定量化相比，可以获得最高达2.35倍和1.51倍的密钥生成速率（KGR）提升。
</details></li>
</ul>
<hr>
<h2 id="Exploiting-Semantic-Localization-in-Highly-Dynamic-Wireless-Networks-Using-Deep-Homoscedastic-Domain-Adaptation"><a href="#Exploiting-Semantic-Localization-in-Highly-Dynamic-Wireless-Networks-Using-Deep-Homoscedastic-Domain-Adaptation" class="headerlink" title="Exploiting Semantic Localization in Highly Dynamic Wireless Networks Using Deep Homoscedastic Domain Adaptation"></a>Exploiting Semantic Localization in Highly Dynamic Wireless Networks Using Deep Homoscedastic Domain Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07792">http://arxiv.org/abs/2310.07792</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/leo-chu/semanticloc">https://github.com/leo-chu/semanticloc</a></li>
<li>paper_authors: Lei Chu, Abdullah Alghafis, Andreas F. Molisch</li>
<li>for: 本研究旨在提高无GPS的street canyon环境中的地点定位精度和Robustness，通过对三种传播条件（line-of-sight（LOS）、阻挡LOS（OLOS）和非LOS（NLOS））进行semantic定义，并同时确定它们和实际地点的定位。</li>
<li>methods: 本研究使用机器学习（ML）技术，提出了semantic定位方法，其中包括联合任务（坐标回归和 semantics 分类）学习问题，以及对各个位置的多个CSIs进行有效的表示学习和知识传递。</li>
<li>results: 本研究通过多任务深度适应（DA）技术，使用有限多个标注样本和大量无标注样本进行地点定位，并提出了scenario适应学习策略，以确保efficient表示学习和成功的知识传递。此外，本研究还使用 bayesian 理论来模型uncertainty weights的重要性，从而降低了时间consuming的参数finetuning。<details>
<summary>Abstract</summary>
Localization in GPS-denied outdoor locations, such as street canyons in an urban or metropolitan environment, has many applications. Machine Learning (ML) is widely used to tackle this critical problem. One challenge lies in the mixture of line-of-sight (LOS), obstructed LOS (OLOS), and non-LOS (NLOS) conditions. In this paper, we consider a semantic localization that treats these three propagation conditions as the ''semantic objects", and aims to determine them together with the actual localization, and show that this increases accuracy and robustness. Furthermore, the propagation conditions are highly dynamic, since obstruction by cars or trucks can change the channel state information (CSI) at a fixed location over time. We therefore consider the blockage by such dynamic objects as another semantic state. Based on these considerations, we formulate the semantic localization with a joint task (coordinates regression and semantics classification) learning problem. Another problem created by the dynamics is the fact that each location may be characterized by a number of different CSIs. To avoid the need for excessive amount of labeled training data, we propose a multi-task deep domain adaptation (DA) based localization technique, training neural networks with a limited number of labeled samples and numerous unlabeled ones. Besides, we introduce novel scenario adaptive learning strategies to ensure efficient representation learning and successful knowledge transfer. Finally, we use Bayesian theory for uncertainty modeling of the importance weights in each task, reducing the need for time-consuming parameter finetuning; furthermore, with some mild assumptions, we derive the related log-likelihood for the joint task and present the deep homoscedastic DA based localization method.
</details>
<details>
<summary>摘要</summary>
本文研究用机器学习（ML）在无GPS的户外场景中进行地点地理位置的定位。这些场景包括城市街区的封闭环境，其中存在多种propagation condition，如直接视线（LOS）、受阻视线（OLOS）和非直接视线（NLOS）等。本文提出一种 semantics localization，将这三种propagation condition treated为 semantics objects，并将其与实际地理位置一起确定，以提高精度和可靠性。此外，这些propagation condition是高度动态的，因为卡车或卡车可以在fixed location上时间上对通信状态信息（CSI）进行阻挡。因此，我们将阻挡这些动态对象作为另一种semantic state来考虑。基于以上考虑，我们将semantic localization定义为一个联合任务（coordinates regression和semantics classification）学习问题。此外，由于每个地点可能有多个CSIs，我们提出一种多任务深度适应（DA）基本地址技术，通过使用有限量的标注样本和大量的无标注样本来训练神经网络。此外，我们还引入了一些novel scenario adaptive learning strategy来保证效率的表征学习和成功的知识传递。最后，我们使用 bayesian 理论来模型 uncertainty weight的importance，减少需要时间consuming的参数微调; 此外，在某些轻微的假设下，我们Derive the related log-likelihood for the joint task, and present the deep homoscedastic DA based localization method.
</details></li>
</ul>
<hr>
<h2 id="Sparse-Millimeter-Wave-Channel-Estimation-From-Partially-Coherent-Measurements"><a href="#Sparse-Millimeter-Wave-Channel-Estimation-From-Partially-Coherent-Measurements" class="headerlink" title="Sparse Millimeter Wave Channel Estimation From Partially Coherent Measurements"></a>Sparse Millimeter Wave Channel Estimation From Partially Coherent Measurements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07569">http://arxiv.org/abs/2310.07569</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weijia Yi, Nitin Jonathan Myers, Geethu Joseph</li>
<li>for: 这种研究旨在开发一种毫米波通信系统中的通道估计技术，以减少训练负担并考虑普通频率噪声引起的频率错误。</li>
<li>methods: 该方法利用毫米波通道的稀畴结构，并考虑在扫描射频谱中的频率错误。特别是在基于IEEE 802.11ad&#x2F;ay的毫米波系统中，在一个扫描射频谱中的频率错误相对较小，而在不同的扫描射频谱中的频率错误相对较大。因此，标准的稀畴意识算法，忽略频率错误，在扫描射频谱中不能准确估计通道。我们提出一种新的算法called partially coherent matching pursuit，它采用迭代最小化来同时估计信号和频率错误。</li>
<li>results: 我们通过数值分析表明，我们的算法可以在较低的复杂性下高精度地估计通道。<details>
<summary>Abstract</summary>
This paper develops a channel estimation technique for millimeter wave (mmWave) communication systems. Our method exploits the sparse structure in mmWave channels for low training overhead and accounts for the phase errors in the channel measurements due to phase noise at the oscillator. Specifically, in IEEE 802.11ad/ay-based mmWave systems, the phase errors within a beam refinement protocol packet are almost the same, while the errors across different packets are substantially different. Consequently, standard sparsity-aware algorithms, which ignore phase errors, fail when channel measurements are acquired over multiple beam refinement protocol packets. We present a novel algorithm called partially coherent matching pursuit for sparse channel estimation under practical phase noise perturbations. Our method iteratively detects the support of sparse signal and employs alternating minimization to jointly estimate the signal and the phase errors. We numerically show that our algorithm can reconstruct the channel accurately at a lower complexity than the benchmarks.
</details>
<details>
<summary>摘要</summary>
The proposed algorithm, called partially coherent matching pursuit, iteratively detects the support of the sparse signal and employs alternating minimization to jointly estimate the signal and the phase errors. The algorithm is designed to handle practical phase noise perturbations and can accurately reconstruct the channel at a lower complexity than the benchmarks.Here is the text in Simplified Chinese:这篇论文提出了一种渠道估计技术 для毫米波通信系统，该技术利用毫米波通道的稀疏结构来减少训练负担，同时考虑振荡器内的频率噪声对渠道测量的影响。具体来说，在IEEE 802.11ad/ay基于毫米波系统中，内部的振荡器频率噪声对同一个扫描射频 packet 的频率错误是非常相似的，而不同包的频率错误则是非常不同的。因此，标准的稀疏意识算法，忽略了频率错误，在多个扫描射频 packet 上不能正确地估计渠道。我们提出了一种新的算法，called partially coherent matching pursuit，该算法 iteratively 检测稀疏信号的支持，并使用交互式最小化来联合估计信号和频率错误。我们numerically 表明，我们的算法可以在较低的复杂性下准确地重建渠道。
</details></li>
</ul>
<hr>
<h2 id="Quality-of-Service-Constrained-Online-Routing-in-High-Throughput-Satellites"><a href="#Quality-of-Service-Constrained-Online-Routing-in-High-Throughput-Satellites" class="headerlink" title="Quality of Service-Constrained Online Routing in High Throughput Satellites"></a>Quality of Service-Constrained Online Routing in High Throughput Satellites</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07557">http://arxiv.org/abs/2310.07557</a></li>
<li>repo_url: None</li>
<li>paper_authors: Olivier Bélanger, Olfa Ben Yahia, Stéphane Martel, Antoine Lesage-Landry, Gunes Karabulut Kurt<br>for: 这篇论文旨在解决高通信卫星（HTS）内部网络的优化问题，以实现高速数据传输和保持质量服务（QoS）标准。methods: 该论文提出了一种在线优化流量分配和计划方法，基于多品质流模型（MPC）的预测控制技术，以适应HTS数据流的变化和不确定性。results: 对于 numerical simulations，该方法与预先知道的优质方法相比，能够实现几乎与优质方法相当的性能，证明了其有效性和适应性。<details>
<summary>Abstract</summary>
High Throughput Satellites (HTSs) outpace traditional satellites due to their multi-beam transmission. The rise of low Earth orbit mega constellations amplifies HTS data rate demands to terabits/second with acceptable latency. This surge in data rate necessitates multiple modems, often exceeding single device capabilities. Consequently, satellites employ several processors, forming a complex packet-switch network. This can lead to potential internal congestion and challenges in adhering to strict quality of service (QoS) constraints. While significant research exists on constellation-level routing, a literature gap remains on the internal routing within a singular HTS. The intricacy of this internal network architecture presents a significant challenge to achieve high data rates.   This paper introduces an online optimal flow allocation and scheduling method for HTSs. The problem is treated as a multi-commodity flow instance with different priority data streams. An initial full time horizon model is proposed as a benchmark. We apply a model predictive control (MPC) approach to enable adaptive routing based on current information and the forecast within the prediction time horizon while allowing for deviation of the latter. Importantly, MPC is inherently suited to handle uncertainty in incoming flows. Our approach minimizes packet loss by optimally and adaptively managing the priority queue schedulers and flow exchanges between satellite processing modules. Central to our method is a routing model focusing on optimal priority scheduling to enhance data rates and maintain QoS. The model's stages are critically evaluated, and results are compared to traditional methods via numerical simulations. Through simulations, our method demonstrates performance nearly on par with the hindsight optimum, showcasing its efficiency and adaptability in addressing satellite communication challenges.
</details>
<details>
<summary>摘要</summary>
高通过put Satellites (HTSs) 的传输速率比传统卫星更快，因为它们使用多个扫描。随着低地球轨道巨型卫星的出现，HTS 数据率需求增加到 Terra bits/秒，同时保持可接受的延迟。这种增加的数据率使得多个模式、常常超过单个设备的能力。因此，卫星通常使用多个处理器，形成复杂的包队列网络。这可能会导致内部塞突和遵循严格的服务质量（QoS）约束的挑战。虽然关于卫星群 constellation 级别的路由研究已经有很多，但是关于单个 HTS 内部路由的研究还存在一个知识空白。卫星内部网络的复杂性提出了高达数据率的挑战。本文介绍了一种在 HTS 中线上优化流量分配和调度方法。该问题被视为多商品流 instances 中的一个，其中有不同优先级数据流。我们提出了一个全时间轴模型作为参考。我们采用了预测控制（MPC）方法，以适应现有信息和预测时间 horizon 内的变化。MPC 自然地能够处理入流的不确定性。我们的方法可以最小化包产生损失，通过优化优先级调度和卫星处理模块之间的流体换来实现高达数据率和维护 QoS。中心于我们的方法的是一种专注于优化优先级调度，以提高数据率和维护 QoS。模型的阶段得到了严格的评估，并与传统方法进行比较。通过 simulations，我们的方法可以与后看 optimum 的性能相似，这显示了它的效率和适应性。
</details></li>
</ul>
<hr>
<h2 id="Proactive-Monitoring-via-Jamming-in-Fluid-Antenna-Systems"><a href="#Proactive-Monitoring-via-Jamming-in-Fluid-Antenna-Systems" class="headerlink" title="Proactive Monitoring via Jamming in Fluid Antenna Systems"></a>Proactive Monitoring via Jamming in Fluid Antenna Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07550">http://arxiv.org/abs/2310.07550</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junteng Yao, Tuo Wu, Xiazhi Lai, Ming Jin, Cunhua Pan, Maged Elkashlan, Kai-Kit Wong</li>
<li>for: 本研究探讨了使用流体天线系统（FAS）在合法监测器上进行异常通信管理，以提高监测性能。</li>
<li>methods: 研究人员使用监测器调整天线位置，以降低停机概率，并提出了一种基于差分搜索法的优化方法，以提高平均监测率。</li>
<li>results: 实验结果表明，提出的方案可以较 Convention benchmark 高效地提高监测性能。<details>
<summary>Abstract</summary>
This paper investigates the efficacy of utilizing fluid antenna system (FAS) at a legitimate monitor to oversee suspicious communication. The monitor switches the antenna position to minimize its outage probability for enhancing the monitoring performance. Our objective is to maximize the average monitoring rate, whose expression involves the integral of the first-order Marcum $Q$ function. The optimization problem, as initially posed, is non-convex owing to its objective function. Nevertheless, upon substituting with an upper bound, we provide a theoretical foundation confirming the existence of a unique optimal solution for the modified problem, achievable efficiently by the bisection search method. Furthermore, we also introduce a locally closed-form optimal resolution for maximizing the average monitoring rate. Empirical evaluations confirm that the proposed schemes outperform conventional benchmarks considerably.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:这篇论文研究了使用流体天线系统（FAS）在合法监控器上监测异常通信的效果。监控器通过调整天线位置来最小化监测损失的概率，以提高监测性能。我们的目标是最大化平均监测率，其表达式包括首频 Marcum $Q$ 函数的积分。然而，原始优化问题是非凸的，但我们通过substituted an upper bound提供了一个理论基础，证明了优化问题的唯一优解存在。此外，我们还引入了一个本地关闭形式的优化解决方案，以最大化平均监测率。实验证明，我们的提议方案在与传统标准相比较显著地出performances。
</details></li>
</ul>
<hr>
<h2 id="Symbol-Level-Precoding-for-Average-SER-Minimization-in-Multiuser-MISO-Systems"><a href="#Symbol-Level-Precoding-for-Average-SER-Minimization-in-Multiuser-MISO-Systems" class="headerlink" title="Symbol-Level Precoding for Average SER Minimization in Multiuser MISO Systems"></a>Symbol-Level Precoding for Average SER Minimization in Multiuser MISO Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07436">http://arxiv.org/abs/2310.07436</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yafei Wang, Hongwei Hou, Wenjin Wang, Xinping Yi</li>
<li>for:  investigate symbol-level precoding (SLP) for high-order quadrature amplitude modulation (QAM) to minimize average symbol error rate (SER) and improve full signal-to-noise ratio (SNR) ranges.</li>
<li>methods:  construct SER expression, formulate problem of average SER minimization subject to total transmit power constraint, and propose double-space alternating optimization (DSAO) algorithm to optimize transmitted signal and rescaling factor on orthogonal Stiefel manifold and Euclidean spaces, respectively.</li>
<li>results:  propose a block transmission scheme to keep rescaling factor constant within a block, and demonstrate significant performance advantage over existing state-of-the-art SLP schemes through simulation results.<details>
<summary>Abstract</summary>
This paper investigates symbol-level precoding (SLP) for high-order quadrature amplitude modulation (QAM) aimed at minimizing the average symbol error rate (SER), leveraging both constructive interference (CI) and noise power to gain superiority in full signal-to-noise ratio (SNR) ranges. We first construct the SER expression with respect to the transmitted signal and the rescaling factor, based on which the problem of average SER minimization subject to total transmit power constraint is further formulated. Given the non-convex nature of the objective, solving the above problem becomes challenging. Due to the differences in constraints between the transmit signal and the rescaling factor, we propose the double-space alternating optimization (DSAO) algorithm to optimize the two variables on orthogonal Stiefel manifold and Euclidean spaces, respectively. To facilitate QAM demodulation instead of affording impractical signaling overhead, we further develop a block transmission scheme to keep the rescaling factor constant within a block. Simulation results demonstrate that the proposed SLP scheme exhibits a significant performance advantage over existing state-of-the-art SLP schemes.
</details>
<details>
<summary>摘要</summary>
Here is the translation in Simplified Chinese:这篇论文研究了使用高阶 quadrature amplitude modulation (QAM) 的 symbol-level precoding (SLP)，以最小化平均符号错误率 (SER)，利用构建性干扰 (CI) 和噪声功率。我们首先 derive SER 表达式，基于这些表达式，我们进一步形式化了在全信号响应率 (SNR) 范围内的平均 SER 最小化问题，并且采用了 double-space alternating optimization (DSAO) 算法来优化两个变量。为了使 QAM 模测可行，我们采用了块传输方案，以保持块内的扩大因子不变。Results show that the proposed SLP scheme outperforms existing state-of-the-art SLP schemes.
</details></li>
</ul>
<hr>
<h2 id="IRS-Assisted-Federated-Learning-A-Broadband-Over-the-Air-Aggregation-Approach"><a href="#IRS-Assisted-Federated-Learning-A-Broadband-Over-the-Air-Aggregation-Approach" class="headerlink" title="IRS Assisted Federated Learning A Broadband Over-the-Air Aggregation Approach"></a>IRS Assisted Federated Learning A Broadband Over-the-Air Aggregation Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07405">http://arxiv.org/abs/2310.07405</a></li>
<li>repo_url: None</li>
<li>paper_authors: Deyou Zhang, Ming Xiao, Zhibo Pang, Lihui Wang, H. Vincent Poor</li>
<li>for: 这个研究旨在提高无线联合学习（FL）系统中的广域无线计算能力，并使用智能反射表面（IRS）来抵消无线折射和噪声。</li>
<li>methods: 该研究提出了一种基于节点选择的模型集成方法，并使用矩阵提升技术和差异度计算程序来解决问题。</li>
<li>results: 研究人员通过对MNIST数据集进行 simulate，并分析了两种基于节点选择和重量选择的模型集成方法的性能。结果显示， weight-selection 方法可以提高学习性能，而节点选择方法的性能与选择的边缘节点数量有关。<details>
<summary>Abstract</summary>
We consider a broadband over-the-air computation empowered model aggregation approach for wireless federated learning (FL) systems and propose to leverage an intelligent reflecting surface (IRS) to combat wireless fading and noise. We first investigate the conventional node-selection based framework, where a few edge nodes are dropped in model aggregation to control the aggregation error. We analyze the performance of this node-selection based framework and derive an upper bound on its performance loss, which is shown to be related to the selected edge nodes. Then, we seek to minimize the mean-squared error (MSE) between the desired global gradient parameters and the actually received ones by optimizing the selected edge nodes, their transmit equalization coefficients, the IRS phase shifts, and the receive factors of the cloud server. By resorting to the matrix lifting technique and difference-of-convex programming, we successfully transform the formulated optimization problem into a convex one and solve it using off-the-shelf solvers. To improve learning performance, we further propose a weight-selection based FL framework. In such a framework, we assign each edge node a proper weight coefficient in model aggregation instead of discarding any of them to reduce the aggregation error, i.e., amplitude alignment of the received local gradient parameters from different edge nodes is not required. We also analyze the performance of this weight-selection based framework and derive an upper bound on its performance loss, followed by minimizing the MSE via optimizing the weight coefficients of the edge nodes, their transmit equalization coefficients, the IRS phase shifts, and the receive factors of the cloud server. Furthermore, we use the MNIST dataset for simulations to evaluate the performance of both node-selection and weight-selection based FL frameworks.
</details>
<details>
<summary>摘要</summary>
我们考虑了一种宽带无线通信 empowered 模型聚合方法 для无线联合学习（FL）系统，并提议利用智能反射表面（IRS）来抗衰减和噪声。我们首先investigate了传统的节点选择基础框架，其中一些边节点被排除在模型聚合中来控制聚合错误。我们分析了这种节点选择基础框架的性能，并 derivated一个上限 bound的性能损失，该损失与选择的边节点相关。然后，我们寻求通过最小化模型聚合中的均方差（MSE）来实现 global 梯度参数与实际接收的梯度参数之间的均方差最小化。为此，我们采用矩阵提升技术和差分 convex 编程，将问题转化为一个convex问题，并使用存储库中的解决方案。为了提高学习性能，我们进一步提议一种weight-selection based FL框架。在这种框架中，我们为每个边节点分配一个适当的weight coefficient，以便在模型聚合中减少聚合错误，即不需要对接收到的本地梯度参数进行幂等匹配。我们还分析了weight-selection based FL框架的性能，并 derivated一个上限 bound的性能损失，然后通过最小化MSE来实现模型聚合中的均方差最小化。此外，我们使用 MNIST 数据集进行实验来评估两种 FL 框架的性能。
</details></li>
</ul>
<hr>
<h2 id="Integrated-Sensing-and-Communication-enabled-Doppler-Frequency-Shift-Estimation-and-Compensation"><a href="#Integrated-Sensing-and-Communication-enabled-Doppler-Frequency-Shift-Estimation-and-Compensation" class="headerlink" title="Integrated Sensing and Communication enabled Doppler Frequency Shift Estimation and Compensation"></a>Integrated Sensing and Communication enabled Doppler Frequency Shift Estimation and Compensation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07401">http://arxiv.org/abs/2310.07401</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinzhu Jia, Zhiqing Wei, Ruiyun Zhang, Lin Wang</li>
<li>for: 这篇论文是为了解决高速车辆网络中 millimeter wave 技术导致的严重 Doppler Frequency Shift (DFS) 问题，以提高通信性能。</li>
<li>methods: 本论文提出了一个 Integrated Sensing and Communication (ISAC) 实现 DFS 估计和补偿算法，包括对 DFS 进行粗略估计和补偿、使用设计的 preamble 序列进行精确估计和补偿，以及适应式 DFS 估计器以减少计算复杂度。</li>
<li>results: 比较traditional DFS 估计算法，提案的算法在 bit error rate 和平均方差Error 性能上显示出改善。<details>
<summary>Abstract</summary>
Despite the millimeter wave technology fulfills the low-latency and high data transmission, it will cause severe Doppler Frequency Shift (DFS) for high-speed vehicular network, which tremendously damages the communication performance. In this paper, we propose an Integrated Sensing and Communication (ISAC) enabled DFS estimation and compensation algorithm. Firstly, the DFS is coarsely estimated and compensated using radar detection. Then, the designed preamble sequence is used to accurately estimate and compensate DFS. In addition, an adaptive DFS estimator is designed to reduce the computational complexity. Compared with the traditional DFS estimation algorithm, the improvement of the proposed algorithm is verified in bit error rate and mean square error performance by simulation results.
</details>
<details>
<summary>摘要</summary>
尽管毫米波技术实现了低延迟和高数据传输，但它会导致高速交通网络中严重的多普勒频率偏移（DFS），从而极大地损害通信性能。在这篇论文中，我们提出了一种结合探测和通信（ISAC）能力的DFS估算和补偿算法。首先，通过雷达探测来粗略地估算并补偿DFS。然后，采用设计的首部序列来精度地估算和补偿DFS。此外，我们还设计了一种适应式DFS估算器，以降低计算复杂性。与传统的DFS估算算法相比，我们的提案的改进被证明通过实验结果的比特错误率和均方差性能。
</details></li>
</ul>
<hr>
<h2 id="Extremal-Mechanisms-for-Pointwise-Maximal-Leakage"><a href="#Extremal-Mechanisms-for-Pointwise-Maximal-Leakage" class="headerlink" title="Extremal Mechanisms for Pointwise Maximal Leakage"></a>Extremal Mechanisms for Pointwise Maximal Leakage</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07381">http://arxiv.org/abs/2310.07381</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leonhard Grosse, Sara Saeidian, Tobias Oechtering</li>
<li>for: 本文研究了在不可靠第三方party发布数据时，如何保护数据隐私。</li>
<li>methods: 本文使用了随机性加载到数据点，以降低数据的有用性。</li>
<li>results: 研究发现，随机response机制可以实现本地均衡隐私，并且可以通过 convex 分析获得一些关键的关键解。<details>
<summary>Abstract</summary>
Data publishing under privacy constraints can be achieved with mechanisms that add randomness to data points when released to an untrusted party, thereby decreasing the data's utility. In this paper, we analyze this privacy-utility tradeoff for the pointwise maximal leakage privacy measure and a general class of convex utility functions. Pointwise maximal leakage (PML) was recently proposed as an operationally meaningful privacy measure based on two equivalent threat models: An adversary guessing a randomized function and an adversary aiming to maximize a general gain function. We study the behavior of the randomized response mechanism designed for local differential privacy under different prior distributions of the private data. Motivated by the findings of this analysis, we derive several closed-form solutions for the optimal privacy-utility tradeoff in the presented PML context using tools from convex analysis. Finally, we present a linear program that can compute optimal mechanisms for PML in a general setting.
</details>
<details>
<summary>摘要</summary>
<<SYS>>传输数据以遵循隐私限制可以通过添加随机性到数据点来实现，从而降低数据的使用价值。在这篇论文中，我们分析了这种隐私-使用价值贸易的关系，使用点最大泄露隐私度量（PML）和一类凸Utility函数。PML是最近提出的一种操作可能性的隐私度量，基于两种等价威胁模型：敌方猜测一个随机函数，以及敌方尝试最大化一个通用获得函数。我们研究了随机响应机制在本地差分隐私下的不同先前分布的私人数据的行为。受此分析的结果启发，我们 deriv了一些关闭形式的解决方案，用于PML上的优化隐私-使用价值贸易。最后，我们提出了一个可以计算PML上优化机制的线性程序。Note: "随机函数" in the original text is translated as "随机性" in Simplified Chinese, which is a more common term used in the field.
</details></li>
</ul>
<hr>
<h2 id="A-Unified-Algorithmic-Framework-for-Dynamic-Compressive-Sensing"><a href="#A-Unified-Algorithmic-Framework-for-Dynamic-Compressive-Sensing" class="headerlink" title="A Unified Algorithmic Framework for Dynamic Compressive Sensing"></a>A Unified Algorithmic Framework for Dynamic Compressive Sensing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07202">http://arxiv.org/abs/2310.07202</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaozhi Liu, Yong Xia</li>
<li>for: 这 paper 是为了重建信号序列中的内在结构化动态稀缺性而设计的一种统一动态跟踪算法框架 (PLAY-CS)。</li>
<li>methods: 该框架利用了特定的统计假设，用于描述信号序列的动态滤波器，并通过新提出的partial-Laplacian filtering简度模型，更好地捕捉到信号序列的更加复杂的动态稀缺性。</li>
<li>results: 在实际应用场景，如无线通信中的动态通道跟踪，该框架比既有的DCS算法表现出更高的性能。<details>
<summary>Abstract</summary>
We propose a unified dynamic tracking algorithmic framework (PLAY-CS) to reconstruct signal sequences with their intrinsic structured dynamic sparsity. By capitalizing on specific statistical assumptions concerning the dynamic filter of the signal sequences, the proposed framework exhibits versatility by encompassing various existing dynamic compressive sensing (DCS) algorithms. This is achieved through the incorporation of a newly proposed Partial-Laplacian filtering sparsity model, tailored to capture a more sophisticated dynamic sparsity. In practical scenarios such as dynamic channel tracking in wireless communications, the framework demonstrates enhanced performance compared to existing DCS algorithms.
</details>
<details>
<summary>摘要</summary>
我们提出一个统一的动态跟踪算法框架（PLAY-CS），用于重建信号序列的内在结构化动态稀烈性。通过利用信号序列动态滤波器的特定统计假设，我们的框架能够展示多样性，包括不同的动态压缩感知（DCS）算法。这是通过 newly proposed partial-laplace 滤波稀烈性模型来实现的，这种模型用于捕捉更复杂的动态稀烈性。在无线通信中的实际应用场景中，我们的框架可以比既有的 DCS 算法表现更好。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Input-Output-Relation-and-Low-Complexity-Receiver-Design-for-CP-OTFS-Systems-with-Doppler-Squint"><a href="#Input-Output-Relation-and-Low-Complexity-Receiver-Design-for-CP-OTFS-Systems-with-Doppler-Squint" class="headerlink" title="Input-Output Relation and Low-Complexity Receiver Design for CP-OTFS Systems with Doppler Squint"></a>Input-Output Relation and Low-Complexity Receiver Design for CP-OTFS Systems with Doppler Squint</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07200">http://arxiv.org/abs/2310.07200</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuehan Wang, Xu Shi, Jintao Wang, Jian Song</li>
<li>for: 这篇论文旨在探讨在 orthogonal time frequency space (OTFS) 系统中，频率相关的 Doppler 效应（DSE）的影响，并研究一种基于 CP-OFDM 的 OTFS 系统。</li>
<li>methods: 本论文使用了实用的 OFDM 系统，并采用了循环前预处理（CP）来减少 DSE 的影响。在输入输出关系中，考虑了 DSE，并将通道估计分成了三部分：延迟探测、Doppler提取和增强估计。采用了线性平衡方案，利用通道矩阵的块对称性，实现了低复杂度接收器的设计。</li>
<li>results:  simulations 表明，DSE 对 OTFS 系统的性能有重要影响，而提posed 的低复杂度接收器设计可以考虑 DSE，并有显著的性能提升。<details>
<summary>Abstract</summary>
In orthogonal time frequency space (OTFS) systems, the impact of frequency-dependent Doppler which is referred to as the Doppler squint effect (DSE) is accumulated through longer duration, whose negligence has prevented OTFS systems from exploiting the performance superiority. In this paper, practical OFDM system using cyclic prefix time guard interval (CP-OFDM)-based OTFS systems with DSE are adopted. Cyclic prefix (CP) length is analyzed while the input-output relation considering DSE is derived. By deploying two prefix OFDM symbols, the channel estimation can be easily divided into three parts as delay detection, Doppler extraction and gain estimation. The linear equalization scheme is adopted taking the block diagonal property of the channel matrix into account, which completes the low-complexity receiver design. Simulation results confirm the significance of DSE and the considerable performance of the proposed low-complexity receiver scheme considering DSE.
</details>
<details>
<summary>摘要</summary>
在正交时频空间（OTFS）系统中，频率相关的多普勒效应（DSE）的影响会随着时间的推移而受拥，这种忽略会导致OTFS系统无法实现性能优势。本文提出了基于CP-OFDM的OTFS系统，其中CP长度进行分析，并 derivation of the input-output relation considering DSE。通过分配两个prefix OFDM符号，渠道估计可以被简单地分解为三部分：延迟探测、多普勒提取和增强估计。采用了线性平衡方案，利用通道矩阵的块对称性，实现了低复杂度接收器设计。实验结果证明了DSE的重要性以及提议的低复杂度接收器计划的出色表现。
</details></li>
</ul>
<hr>
<h2 id="Integrated-Sensing-and-Communication-enabled-Multiple-Base-Stations-Cooperative-Sensing-Towards-6G"><a href="#Integrated-Sensing-and-Communication-enabled-Multiple-Base-Stations-Cooperative-Sensing-Towards-6G" class="headerlink" title="Integrated Sensing and Communication enabled Multiple Base Stations Cooperative Sensing Towards 6G"></a>Integrated Sensing and Communication enabled Multiple Base Stations Cooperative Sensing Towards 6G</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07180">http://arxiv.org/abs/2310.07180</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiqing Wei, Wangjun Jiang, Zhiyong Feng, Huici Wu, Ning Zhang, Kaifeng Han, Ruizhong Xu, Ping Zhang</li>
<li>for:  sixth-generation (6G) mobile communication systems, such as smart city and autonomous driving</li>
<li>methods:  multi-BS cooperative sensing, unified ISAC performance metrics, ISAC signal design and optimization, interference management, cooperative sensing algorithms</li>
<li>results:  breaking the limitation of single-BS sensing, establishing intelligent infrastructures connecting physical and cyber space, ushering the era of 6G<details>
<summary>Abstract</summary>
Driven by the intelligent applications of sixth-generation (6G) mobile communication systems such as smart city and autonomous driving, which connect the physical and cyber space, the integrated sensing and communication (ISAC) brings a revolutionary change to the base stations (BSs) of 6G by integrating radar sensing and communication in the same hardware and wireless resource. However, with the requirements of long-range and accurate sensing in the applications of smart city and autonomous driving, the ISAC enabled single BS still has a limitation in the sensing range and accuracy. With the networked infrastructures of mobile communication systems, multi-BS cooperative sensing is a natural choice satisfying the requirement of long-range and accurate sensing. In this article, the framework of multi-BS cooperative sensing is proposed, breaking through the limitation of single-BS sensing. The enabling technologies, including unified ISAC performance metrics, ISAC signal design and optimization, interference management, cooperative sensing algorithms, are introduced in details. The performance evaluation results are provided to verify the effectiveness of multi-BS cooperative sensing schemes. With ISAC enabled multi-BS cooperative sensing (ISAC-MCS), the intelligent infrastructures connecting physical and cyber space can be established, ushering the era of 6G promoting the intelligence of everything.
</details>
<details>
<summary>摘要</summary>
驱动了六代移动通信系统（6G）的智能应用，如智能城市和自动驾驶，这些应用连接了物理空间和虚拟空间，因此集成感知和通信（ISAC）在6G基站（BS）中带来了革命性的变革。然而，在智能城市和自动驾驶应用中需要覆盖较长范围和精度高的感知，ISAC启用的单BS仍有限制的感知范围和精度。基于移动通信系统的网络基础设施，多BS合作感知是一个自然的选择，满足覆盖较长范围和精度高的感知需求。本文提出了多BS合作感知框架，突破单BS感知的限制。本文还介绍了实现多BS合作感知的关键技术，包括统一ISAC性能指标、ISAC信号设计优化、干扰管理和合作感知算法。本文还提供了性能评估结果，证明了多BS合作感知方案的有效性。通过ISAC启用的多BS合作感知（ISAC-MCS），智能基础设施可以建立，推动6G时代，智能化 Everything。
</details></li>
</ul>
<hr>
<h2 id="Time-and-Frequency-Offset-Estimation-and-Intercarrier-Interference-Cancellation-for-AFDM-Systems"><a href="#Time-and-Frequency-Offset-Estimation-and-Intercarrier-Interference-Cancellation-for-AFDM-Systems" class="headerlink" title="Time and Frequency Offset Estimation and Intercarrier Interference Cancellation for AFDM Systems"></a>Time and Frequency Offset Estimation and Intercarrier Interference Cancellation for AFDM Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07141">http://arxiv.org/abs/2310.07141</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuankun Tang, Anjie Zhang, Miaowen Wen, Yu Huang, Fei Ji, Jinming Wen</li>
<li>for: 这篇论文旨在提供一个可靠的通信方案，供时间变化频道上的 AFDM 系统。</li>
<li>methods: 这篇论文提出了两种最大 LIKELIHOOD（ML）估计器：一个 JOINT ML 估计器，通过比较样本之间的相似性来评估到来访时间和频率偏移；另一个是 Stepwise ML 估计器，可以降低复杂度。这两种估计器都利用 AFDM symbol 中内含的双极几何资讯，无需额外的导频。</li>
<li>results:  numerically 的结果显示，提出的时间和频率偏移估计准确性和 mirror-mapping 基本帧调制可以实现 AFDM 系统中的可靠通信。<details>
<summary>Abstract</summary>
Affine frequency division multiplexing (AFDM) is an emerging multicarrier waveform that offers a potential solution for achieving reliable communication for time-varying channels. This paper proposes two maximum likelihood (ML) estimators of symbol time offset and carrier frequency offset for AFDM systems. The joint ML estimator evaluates the arrival time and frequency offset by comparing the correlations of samples. Moreover, we propose the stepwise ML estimator to reduce the complexity. The proposed estimators exploit the redundant information contained within the chirp-periodic prefix inherent in AFDM symbols, thus dispensing with any additional pilots. To further mitigate the intercarrier interference resulting from the residual frequency offset, we design a mirror-mappingbased scheme for AFDM systems. Numerical results verify the effectiveness of the proposed time and frequency offset estimation criteria and the mirror-mapping-based modulation for AFDM systems.
</details>
<details>
<summary>摘要</summary>
《 Affine 频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通频率分多普通
</details></li>
</ul>
<hr>
<h2 id="Edge-Cloud-Collaborative-Stream-Computing-for-Real-Time-Structural-Health-Monitoring"><a href="#Edge-Cloud-Collaborative-Stream-Computing-for-Real-Time-Structural-Health-Monitoring" class="headerlink" title="Edge Cloud Collaborative Stream Computing for Real-Time Structural Health Monitoring"></a>Edge Cloud Collaborative Stream Computing for Real-Time Structural Health Monitoring</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07130">http://arxiv.org/abs/2310.07130</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenzhao Zhang, Cheng Guo, Yi Gao, Wei Dong</li>
<li>for: 这个论文是为了解决结构健康监控（SHM）中的资料过量和实时需求问题。</li>
<li>methods: 本论文提出了一个Edge Cloud合作精细流运算框架（ECStream），用于解决SHM中的问题。ECStream考虑了原子和composite operators的联合运算，并将其形式化为一个可读性和可缩小的流运算问题。</li>
<li>results: 根据先前的评估结果，ECStream可以有效地对带宽和终端 оператор处理延迟进行平衡，将带宽使用率降低到73.01%，并将终端operator computation延迟降低到34.08%的水平。<details>
<summary>Abstract</summary>
Structural Health Monitoring (SHM) is crucial for the safety and maintenance of various infrastructures. Due to the large amount of data generated by numerous sensors and the high real-time requirements of many applications, SHM poses significant challenges. Although the cloud-centric stream computing paradigm opens new opportunities for real-time data processing, it consumes too much network bandwidth. In this paper, we propose ECStream, an Edge Cloud collaborative fine-grained stream operator scheduling framework for SHM. We collectively consider atomic and composite operators together with their iterative computability to model and formalize the problem of minimizing bandwidth usage and end-to-end operator processing latency. Preliminary evaluation results show that ECStream can effectively balance bandwidth usage and end-to-end operator computation latency, reducing bandwidth usage by 73.01% and latency by 34.08% on average compared to the cloud-centric approach.
</details>
<details>
<summary>摘要</summary>
STRUCTURAL HEALTH MONITORING (SHM) 是重要的安全和维护基础设施的关键。由于众多感知器件生成的大量数据以及许多应用程序的高实时要求，SHM带来了重大挑战。虽然云计算中心主义思想开启了新的实时数据处理机会，但它占用了过多的网络带宽。在本文中，我们提出了ECStream，一个边缘云集成细致流操作调度框架，用于解决SHM中的带宽使用和终端操作计算延迟问题。我们一起考虑原子和复合运算者的迭代可计算性，以模型和正式化带宽使用和终端操作计算延迟的最佳化问题。初步评估结果表明，ECStream可以有效均衡带宽使用和终端操作计算延迟，减少带宽使用率73.01%，减少平均计算延迟34.08%。
</details></li>
</ul>
<hr>
<h2 id="Decentralization-of-Energy-Systems-with-Blockchain-Bridging-Top-down-and-Bottom-up-Management-of-the-Electricity-Grid"><a href="#Decentralization-of-Energy-Systems-with-Blockchain-Bridging-Top-down-and-Bottom-up-Management-of-the-Electricity-Grid" class="headerlink" title="Decentralization of Energy Systems with Blockchain: Bridging Top-down and Bottom-up Management of the Electricity Grid"></a>Decentralization of Energy Systems with Blockchain: Bridging Top-down and Bottom-up Management of the Electricity Grid</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07103">http://arxiv.org/abs/2310.07103</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sakshi Mishra, Roohallah Khatami, Yu Christine Chen</li>
<li>for: 这篇论文旨在提出一种可行的方案，以满足递进的分布式能源资源（DERs）普及和卷积式计算和通信技术的发展，从而对现有的中央化操作模式进行重大变革。</li>
<li>methods: 本文使用了区块链技术，以便在分布式能源系统中实现端到端交易，并且提出了一种混合式操作模式，包括传统中央化操作模式和分布式操作模式。</li>
<li>results: 本文认为，通过使用区块链技术，可以实现分布式能源系统中的端到端交易，并且可以帮助实现现有的中央化操作模式和分布式操作模式之间的协同操作。<details>
<summary>Abstract</summary>
For more than a century, the grid has operated in a centralized top-down fashion. However, as distributed energy resources (DERs) penetration grows, the grid edge is increasingly infused with intelligent computing and communication capabilities. Thus, the bottom-up approach to grid operations inclined toward decentralizing energy systems will likely gain momentum alongside the existing centralized paradigm. Decentralization refers to transferring control and decision-making from a centralized entity (individual, organization, or group thereof) to a distributed network. It is not a new concept - in energy systems context or otherwise. In the energy systems context, however, the complexity of this multifaceted concept increases manifolds due to two major reasons - i) the nature of the commodity being traded (the electricity) and ii) the enormity of the traditional electricity sector's structure that builds, operates, and maintains this capital-intensive network. In this work, we aim to highlight the need for and outline a credible path toward restructuring the current operational architecture of the electricity grid in view of the ongoing decentralization trends with an emphasis on peer-to-peer energy trading. We further introduce blockchain technology in the context of decentralized energy systems problems. We also suggest that blockchain is an effective technology for facilitating the synergistic operations of top-down and bottom-up approaches to grid management.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "grid" is translated as "电网" (dian wang)* "centralized" is translated as "中央化" (zhong yang hua)* "decentralized" is translated as "分散化" (fen shi hua)* "bottom-up" is translated as "底层化" (di yan hua)* "top-down" is translated as "顶层化" (ding yan hua)* "peer-to-peer" is translated as "对等" (dui yi)* "blockchain" is translated as "区块链" (qu yu lian)
</details></li>
</ul>
<hr>
<h2 id="Hybrid-Arrays-How-Many-RF-Chains-Are-Required-to-Prevent-Beam-Squint"><a href="#Hybrid-Arrays-How-Many-RF-Chains-Are-Required-to-Prevent-Beam-Squint" class="headerlink" title="Hybrid Arrays: How Many RF Chains Are Required to Prevent Beam Squint?"></a>Hybrid Arrays: How Many RF Chains Are Required to Prevent Beam Squint?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07101">http://arxiv.org/abs/2310.07101</a></li>
<li>repo_url: None</li>
<li>paper_authors: Heedong Do, Namyoon Lee, Robert W. Heath Jr, Angel Lozano</li>
<li>for: 解决 beamforming 中的 beam squint 问题</li>
<li>methods: 使用 hybrid arrays，不需要 downconversion at each element</li>
<li>results: hybrid arrays 可以达到 digital arrays 的性能水平，但需要 exceeds a certain threshold 的数量Here’s the breakdown of each point:1. for: The paper is written to solve the problem of beam squint in beamforming.2. methods: The paper proposes the use of hybrid arrays, which do not require downconversion at each element, to achieve the same performance as digital arrays.3. results: The paper shows that hybrid arrays can achieve the same performance as digital arrays, but with a lower threshold of elements. The result is robust and holds for suboptimum but highly appealing beamspace architectures.<details>
<summary>Abstract</summary>
With increasing frequencies, bandwidths, and array apertures, the phenomenon of beam squint arises as a serious impairment to beamforming. Fully digital arrays with true time delay per antenna element are a potential solution, but they require downconversion at each element. This paper shows that hybrid arrays can perform essentially as well as digital arrays once the number of radio-frequency chains exceeds a certain threshold that is far below the number of elements. The result is robust, holding also for suboptimum but highly appealing beamspace architectures.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/11/eess.SP_2023_10_11/" data-id="clpahu7fs01g53h8860hr40ry" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_10_10" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/10/cs.SD_2023_10_10/" class="article-date">
  <time datetime="2023-10-10T15:00:00.000Z" itemprop="datePublished">2023-10-10</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/10/cs.SD_2023_10_10/">cs.SD - 2023-10-10</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Modeling-of-Speech-dependent-Own-Voice-Transfer-Characteristics-for-Hearables-with-In-ear-Microphones"><a href="#Modeling-of-Speech-dependent-Own-Voice-Transfer-Characteristics-for-Hearables-with-In-ear-Microphones" class="headerlink" title="Modeling of Speech-dependent Own Voice Transfer Characteristics for Hearables with In-ear Microphones"></a>Modeling of Speech-dependent Own Voice Transfer Characteristics for Hearables with In-ear Microphones</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06554">http://arxiv.org/abs/2310.06554</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mattes Ohlenbusch, Christian Rollwage, Simon Doclo</li>
<li>for: 这篇论文是为了研究听力器中的自己声音传递特性而写的。</li>
<li>methods: 该论文使用了语音认知技术来建立一个语音依赖的系统标定模型，以估计听力器中自己声音的传递特性。</li>
<li>results: 研究发现，使用提议的语音依赖模型可以更好地模拟听力器中的自己声音传递特性，并且比适应 filtering-based 模型更好地适应新的语音。此外，研究还发现，对于不同的说话者，使用 talked-averaged 模型可以更好地泛化到不同的说话者。<details>
<summary>Abstract</summary>
Hearables often contain an in-ear microphone, which may be used to capture the own voice of its user. However, due to ear canal occlusion the in-ear microphone mostly records body-conducted speech, which suffers from band-limitation effects and is subject to amplification of low frequency content. These transfer characteristics are assumed to vary both based on speech content and between individual talkers. It is desirable to have an accurate model of the own voice transfer characteristics between hearable microphones. Such a model can be used, e.g., to simulate a large amount of in-ear recordings to train supervised learning-based algorithms aiming at compensating own voice transfer characteristics. In this paper we propose a speech-dependent system identification model based on phoneme recognition. Using recordings from a prototype hearable, the modeling accuracy is evaluated in terms of technical measures. We investigate robustness of transfer characteristic models to utterance or talker mismatch. Simulation results show that using the proposed speech-dependent model is preferable for simulating in-ear recordings compared to a speech-independent model. The proposed model is able to generalize better to new utterances than an adaptive filtering-based model. Additionally, we find that talker-averaged models generalize better to different talkers than individual models.
</details>
<details>
<summary>摘要</summary>
听ables 经常包含在耳朵中的一个内耳麦克风，可以用来捕捉其用户的自己声音。然而，由于耳朵封闭，内耳麦克风主要记录的是身体传导的语音，这种语音受到频率限制的影响，同时也受到低频强调效果的增强。这些传输特性的变化受到语音内容和个体演说者的影响。因此，有一个准确的自己声音传输特性模型可以用于训练基于supervised learning的算法，以资acia减少自己声音传输特性的影响。在这篇论文中，我们提出了基于phoneme认识的语音依赖系统模型。使用一种原型听ables 的录音，我们评估了模型的准确性，并进行了技术性的评估。我们也研究了语音或演说者之间的模型的稳定性。结果表明，使用我们提出的语音依赖模型在模拟耳朵录音时比使用语音独立模型更好。此外，我们发现了 talker-averaged 模型在不同的演说者之间更好地泛化。
</details></li>
</ul>
<hr>
<h2 id="Topological-data-analysis-of-human-vowels-Persistent-homologies-across-representation-spaces"><a href="#Topological-data-analysis-of-human-vowels-Persistent-homologies-across-representation-spaces" class="headerlink" title="Topological data analysis of human vowels: Persistent homologies across representation spaces"></a>Topological data analysis of human vowels: Persistent homologies across representation spaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06508">http://arxiv.org/abs/2310.06508</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guillem Bonafos, Jean-Marc Freyermuth, Pierre Pudlo, Samuel Tronçon, Arnaud Rey</li>
<li>for: 这篇论文是用于研究数据分析方法的，具体来说是研究如何从各种数据表示空间中提取有用的特征，以便进行预测和分类。</li>
<li>methods: 这篇论文使用的方法包括 persistent homology 理论和 topologic 数据分析 (TDA) 技术，以及一些 Machine Learning 算法，如 random forest。</li>
<li>results: 这篇论文的结果表明，使用不同的数据表示空间可以提取到不同的特征，而这些特征之间存在一定的相互补做作用。此外，使用 topologic 数据分析可以提高预测和分类的准确率。<details>
<summary>Abstract</summary>
Topological Data Analysis (TDA) has been successfully used for various tasks in signal/image processing, from visualization to supervised/unsupervised classification. Often, topological characteristics are obtained from persistent homology theory. The standard TDA pipeline starts from the raw signal data or a representation of it. Then, it consists in building a multiscale topological structure on the top of the data using a pre-specified filtration, and finally to compute the topological signature to be further exploited. The commonly used topological signature is a persistent diagram (or transformations of it). Current research discusses the consequences of the many ways to exploit topological signatures, much less often the choice of the filtration, but to the best of our knowledge, the choice of the representation of a signal has not been the subject of any study yet. This paper attempts to provide some answers on the latter problem. To this end, we collected real audio data and built a comparative study to assess the quality of the discriminant information of the topological signatures extracted from three different representation spaces. Each audio signal is represented as i) an embedding of observed data in a higher dimensional space using Taken's representation, ii) a spectrogram viewed as a surface in a 3D ambient space, iii) the set of spectrogram's zeroes. From vowel audio recordings, we use topological signature for three prediction problems: speaker gender, vowel type, and individual. We show that topologically-augmented random forest improves the Out-of-Bag Error (OOB) over solely based Mel-Frequency Cepstral Coefficients (MFCC) for the last two problems. Our results also suggest that the topological information extracted from different signal representations is complementary, and that spectrogram's zeros offers the best improvement for gender prediction.
</details>
<details>
<summary>摘要</summary>
topological数据分析（TDA）已经成功地应用于各种信号/图像处理任务，从视觉化到指导/无指导分类。经常地， topological特征来自 persistente homology理论。TDA管道从原始信号数据或信号表示开始，然后在基于预先指定的筛选器上建立多级 topological结构，最后计算 topological签名以进一步利用。通常使用的 topological签名是持续 diagram（或其变形）。当前研究的问题是 exploit topological签名的多种方法，而不是筛选器的选择，而且尚未考虑信号表示的选择。这篇论文尝试提供一些答案，并通过对实际的音频数据进行比较性研究来评估不同表示空间中的 topological签名的质量。我们使用了三种不同的表示空间来表示每个音频信号：1. 使用 Takens 表示法将数据embedding到高维空间中。2. 视为三维 ambient空间中的表面，使用 spectrogram。3. spectrogram中的 zeros 集。对于女性语音录制，我们使用 topological签名进行三个预测问题：speaker gender、vowel type和个人。我们发现，使用 topologically-augmented random forest 可以在 Out-of-Bag Error（OOB）中提高 Mel-Frequency Cepstral Coefficients（MFCC）的性能。我们的结果还表明，不同的表示空间中的 topological信息是夹带的，而spectrogram中的 zeros 提供了最好的性能提升。
</details></li>
</ul>
<hr>
<h2 id="Cross-modal-Cognitive-Consensus-guided-Audio-Visual-Segmentation"><a href="#Cross-modal-Cognitive-Consensus-guided-Audio-Visual-Segmentation" class="headerlink" title="Cross-modal Cognitive Consensus guided Audio-Visual Segmentation"></a>Cross-modal Cognitive Consensus guided Audio-Visual Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06259">http://arxiv.org/abs/2310.06259</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhaofeng Shi, Qingbo Wu, Hongliang Li, Fanman Meng, Linfeng Xu<br>for:* 这篇论文的目的是提出一种 Audio-Visual Segmentation (AVS) 方法，用于从视频帧中提取听到的对象。methods:* 该方法使用 dense feature-level audio-visual interaction，忽略不同模式之间的维度差异。* 使用 Cross-modal Cognitive Consensus guided Network (C3N) align audio-visual semantics 从全Dimension 维度和地进行进一步的注意力机制。results:* 经验表明，该方法可以在 Single Sound Source Segmentation (S4) 和 Multiple Sound Source Segmentation (MS3) 任务上达到状态之最好性能。<details>
<summary>Abstract</summary>
Audio-Visual Segmentation (AVS) aims to extract the sounding object from a video frame, which is represented by a pixel-wise segmentation mask. The pioneering work conducts this task through dense feature-level audio-visual interaction, which ignores the dimension gap between different modalities. More specifically, the audio clip could only provide a \textit{Global} semantic label in each sequence, but the video frame covers multiple semantic objects across different \textit{Local} regions. In this paper, we propose a Cross-modal Cognitive Consensus guided Network (C3N) to align the audio-visual semantics from the global dimension and progressively inject them into the local regions via an attention mechanism. Firstly, a Cross-modal Cognitive Consensus Inference Module (C3IM) is developed to extract a unified-modal label by integrating audio/visual classification confidence and similarities of modality-specific label embeddings. Then, we feed the unified-modal label back to the visual backbone as the explicit semantic-level guidance via a Cognitive Consensus guided Attention Module (CCAM), which highlights the local features corresponding to the interested object. Extensive experiments on the Single Sound Source Segmentation (S4) setting and Multiple Sound Source Segmentation (MS3) setting of the AVSBench dataset demonstrate the effectiveness of the proposed method, which achieves state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
音视频分割（AVS）目标是从视频帧中提取听到的对象，它通过像素级别的音视频交互来实现。在这种情况下，音频片断只能提供一个全局Semantic标签，而视频帧则包含多个不同地方的Semantic对象。在这篇论文中，我们提议一种协调音视频 semantics的网络（C3N），以将全局维度上的音视频 semantics 与本地区域相协调。首先，我们开发了一种协调音视频 Semantic Inference模块（C3IM），以抽取音视频分类信任度和模式特征之间的相似性。然后，我们将这个协调模式标签返回给视频底层，并通过一种协调注意力模块（CCAM）来高亮对应的本地特征。我们对AVSBench数据集的Single Sound Source Segmentation（S4）和Multiple Sound Source Segmentation（MS3）两个设置进行了广泛的实验，并证明了我们的方法的有效性，达到了当前最佳性能。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/10/cs.SD_2023_10_10/" data-id="clpahu798010u3h883yfa5ml2" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_10_10" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/10/eess.AS_2023_10_10/" class="article-date">
  <time datetime="2023-10-10T14:00:00.000Z" itemprop="datePublished">2023-10-10</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/10/eess.AS_2023_10_10/">eess.AS - 2023-10-10</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Privacy-oriented-manipulation-of-speaker-representations"><a href="#Privacy-oriented-manipulation-of-speaker-representations" class="headerlink" title="Privacy-oriented manipulation of speaker representations"></a>Privacy-oriented manipulation of speaker representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06652">http://arxiv.org/abs/2310.06652</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francisco Teixeira, Alberto Abad, Bhiksha Raj, Isabel Trancoso</li>
<li>for: 本研究旨在提取和操纵speaker embedding中的private attribute，以保护speaker的隐私。</li>
<li>methods: 该研究使用Vector-Quantized Variational Autoencoder架构，并与对抗学习器和新型的相互信息损失相结合，以去除speaker embedding中的private attribute。</li>
<li>results: 研究 validate在两个属性（性别和年龄）上，并在各种攻击者和数据集下进行了实验。<details>
<summary>Abstract</summary>
Speaker embeddings are ubiquitous, with applications ranging from speaker recognition and diarization to speech synthesis and voice anonymisation. The amount of information held by these embeddings lends them versatility, but also raises privacy concerns. Speaker embeddings have been shown to contain information on age, sex, health and more, which speakers may want to keep private, especially when this information is not required for the target task. In this work, we propose a method for removing and manipulating private attributes from speaker embeddings that leverages a Vector-Quantized Variational Autoencoder architecture, combined with an adversarial classifier and a novel mutual information loss. We validate our model on two attributes, sex and age, and perform experiments with ignorant and fully-informed attackers, and with in-domain and out-of-domain data.
</details>
<details>
<summary>摘要</summary>
喊Word embeddings在各种应用中广泛使用，包括说话人识别和分类、语音合成和声音匿名化。这些喊Word embeddings中包含了大量信息，这使其具有多样性，但也引起了隐私问题。这些喊Word embeddings中包含的信息包括年龄、性别、健康等，这些信息可能会让说话人保持隐私，特别是当这些信息不是target任务所需的时候。在这项工作中，我们提出了一种去除和修改私人属性从喊Word embeddings中的方法，该方法基于Vector-Quantized Variational Autoencoder架构，并与对抗类ifier和一种新的共同信息损失相结合。我们验证了我们的模型在两个属性上，性别和年龄上，并在不知情和完全了解的攻击者下进行了实验，以及在Domain和Out-of-Domain数据上。
</details></li>
</ul>
<hr>
<h2 id="Discriminative-Speech-Recognition-Rescoring-with-Pre-trained-Language-Models"><a href="#Discriminative-Speech-Recognition-Rescoring-with-Pre-trained-Language-Models" class="headerlink" title="Discriminative Speech Recognition Rescoring with Pre-trained Language Models"></a>Discriminative Speech Recognition Rescoring with Pre-trained Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06248">http://arxiv.org/abs/2310.06248</a></li>
<li>repo_url: None</li>
<li>paper_authors: Prashanth Gurunath Shivakumar, Jari Kolehmainen, Yile Gu, Ankur Gandhe, Ariya Rastrow, Ivan Bulyko</li>
<li>for: 提高自动语音识别（ASR）系统的竞争力</li>
<li>methods: 使用预训练语言模型（LM）的探索性训练</li>
<li>results: 在 LibriSpeech 数据集上，所有MWER训练方案都有所提高，最高提高8.5% WER； Pooling 变体可以降低延迟，保持大部分改进；  bidirectional LM 更好地利用探索性训练。<details>
<summary>Abstract</summary>
Second pass rescoring is a critical component of competitive automatic speech recognition (ASR) systems. Large language models have demonstrated their ability in using pre-trained information for better rescoring of ASR hypothesis. Discriminative training, directly optimizing the minimum word-error-rate (MWER) criterion typically improves rescoring. In this study, we propose and explore several discriminative fine-tuning schemes for pre-trained LMs. We propose two architectures based on different pooling strategies of output embeddings and compare with probability based MWER. We conduct detailed comparisons between pre-trained causal and bidirectional LMs in discriminative settings. Experiments on LibriSpeech demonstrate that all MWER training schemes are beneficial, giving additional gains upto 8.5\% WER. Proposed pooling variants achieve lower latency while retaining most improvements. Finally, our study concludes that bidirectionality is better utilized with discriminative training.
</details>
<details>
<summary>摘要</summary>
第二个通过重新分配是竞争自动语音识别（ASR）系统的重要组成部分。大型语言模型已经证明了它们可以使用预训信息来改善ASR假设的重新分配。精确训练，直接优化最小单词错误率（MWER）标准通常会提高重新分配。在这项研究中，我们提出并探索了多种精确定型训练方案 для预训练LM。我们提出了基于不同抽取策略的输出嵌入的两种架构，并与概率基于MWER进行比较。我们在预训练 causal 和 bidirectional LM 中进行了详细比较。在 LibriSpeech 上进行的实验表明，所有MWER 训练方案都是有利的，可以获得额外的8.5% WER 的提升。我们的 pooling 变体可以减少延迟时间，保持大多数改进。最后，我们的研究结论是，批处性是在精确训练中更好地利用的。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/10/eess.AS_2023_10_10/" data-id="clpahu7b001593h881m9qdeu2" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_10_10" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/10/cs.CV_2023_10_10/" class="article-date">
  <time datetime="2023-10-10T13:00:00.000Z" itemprop="datePublished">2023-10-10</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/10/cs.CV_2023_10_10/">cs.CV - 2023-10-10</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="BeSt-LeS-Benchmarking-Stroke-Lesion-Segmentation-using-Deep-Supervision"><a href="#BeSt-LeS-Benchmarking-Stroke-Lesion-Segmentation-using-Deep-Supervision" class="headerlink" title="BeSt-LeS: Benchmarking Stroke Lesion Segmentation using Deep Supervision"></a>BeSt-LeS: Benchmarking Stroke Lesion Segmentation using Deep Supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07060">http://arxiv.org/abs/2310.07060</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/prantik-pdeb/best-les">https://github.com/prantik-pdeb/best-les</a></li>
<li>paper_authors: Prantik Deb, Lalith Bharadwaj Baru, Kamalaker Dadi, Bapi Raju S</li>
<li>for: 这项研究的目的是为了提供一种基于自动化分割的 stroke 识别和风险评估方法，以帮助临床专业人员更准确地诊断 stroke。</li>
<li>methods: 这项研究使用了公共可用的 ATLAS $v2.0$ 数据集，并对不同的终端指导式 U-Net 模型进行了比较。研究使用了标准的评估指标来评估模型的性能。</li>
<li>results: 研究所获得的结果显示，使用 transformer-based 2D 模型可以达到最高的 Dice 分数为 0.583，而使用 residual U-Net 3D 模型可以达到最高的 Dice 分数为 0.504。此外，研究还通过 Wilcoxon 测试发现了 stroke 体积预测和实际值之间的相关关系。<details>
<summary>Abstract</summary>
Brain stroke has become a significant burden on global health and thus we need remedies and prevention strategies to overcome this challenge. For this, the immediate identification of stroke and risk stratification is the primary task for clinicians. To aid expert clinicians, automated segmentation models are crucial. In this work, we consider the publicly available dataset ATLAS $v2.0$ to benchmark various end-to-end supervised U-Net style models. Specifically, we have benchmarked models on both 2D and 3D brain images and evaluated them using standard metrics. We have achieved the highest Dice score of 0.583 on the 2D transformer-based model and 0.504 on the 3D residual U-Net respectively. We have conducted the Wilcoxon test for 3D models to correlate the relationship between predicted and actual stroke volume. For reproducibility, the code and model weights are made publicly available: https://github.com/prantik-pdeb/BeSt-LeS.
</details>
<details>
<summary>摘要</summary>
Brain stroke 已成为全球医疗的重要挑战，因此我们需要有效的治疗和预防策略。为此，诊断 stroke 的速度和风险评估是临床医生的首要任务。为了帮助专业医生，自动分割模型是非常重要。在这项工作中，我们使用公共可用的 ATLAS $v2.0$ 数据集来对不同的端到端授 taught U-Net 模型进行比较。我们在2D和3D脑图像上对模型进行了测试，并使用标准指标进行评估。我们在2D transformer-based 模型上达到了最高的 Dice 分数为 0.583，并在3D residual U-Net 模型上达到了 0.504 的最高分数。我们通过沃克逊测试来检验3D模型中预测和实际 stroke 体积之间的相关性。为了保持可重复性，我们在 GitHub 上公开了代码和模型参数：https://github.com/prantik-pdeb/BeSt-LeS。
</details></li>
</ul>
<hr>
<h2 id="TextPSG-Panoptic-Scene-Graph-Generation-from-Textual-Descriptions"><a href="#TextPSG-Panoptic-Scene-Graph-Generation-from-Textual-Descriptions" class="headerlink" title="TextPSG: Panoptic Scene Graph Generation from Textual Descriptions"></a>TextPSG: Panoptic Scene Graph Generation from Textual Descriptions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07056">http://arxiv.org/abs/2310.07056</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chengyang Zhao, Yikang Shen, Zhenfang Chen, Mingyu Ding, Chuang Gan</li>
<li>for: 本研究旨在生成基于文本描述的涵义图（Caption-to-PSG），以解决现有方法受限于需要大量精心标注的问题。</li>
<li>methods: 我们提出了一个新的框架TextPSG，包括四个模块：区域分组、实体降解、段合并和标签生成，以及一些新的技术。</li>
<li>results: 我们的方法显著超越了基准值，并 achieve strong out-of-distribution robustness。我们进行了广泛的减少研究来证明我们的设计选择的有效性，并提供了深入的分析，以便未来的研究。<details>
<summary>Abstract</summary>
Panoptic Scene Graph has recently been proposed for comprehensive scene understanding. However, previous works adopt a fully-supervised learning manner, requiring large amounts of pixel-wise densely-annotated data, which is always tedious and expensive to obtain. To address this limitation, we study a new problem of Panoptic Scene Graph Generation from Purely Textual Descriptions (Caption-to-PSG). The key idea is to leverage the large collection of free image-caption data on the Web alone to generate panoptic scene graphs. The problem is very challenging for three constraints: 1) no location priors; 2) no explicit links between visual regions and textual entities; and 3) no pre-defined concept sets. To tackle this problem, we propose a new framework TextPSG consisting of four modules, i.e., a region grouper, an entity grounder, a segment merger, and a label generator, with several novel techniques. The region grouper first groups image pixels into different segments and the entity grounder then aligns visual segments with language entities based on the textual description of the segment being referred to. The grounding results can thus serve as pseudo labels enabling the segment merger to learn the segment similarity as well as guiding the label generator to learn object semantics and relation predicates, resulting in a fine-grained structured scene understanding. Our framework is effective, significantly outperforming the baselines and achieving strong out-of-distribution robustness. We perform comprehensive ablation studies to corroborate the effectiveness of our design choices and provide an in-depth analysis to highlight future directions. Our code, data, and results are available on our project page: https://vis-www.cs.umass.edu/TextPSG.
</details>
<details>
<summary>摘要</summary>
它最近提出了泛睿场景图（Panoptic Scene Graph，PSG），但以前的工作采用了完全监督学习方式，需要大量的像素密集标注数据，这总是费时和贵金属。为了解决这个限制，我们研究了一个新的问题：即从文本描述中生成PSG（Caption-to-PSG）。我们利用了互联网上免费的图像描述数据集，以生成泛睿场景图。这个问题具有三个约束：1）没有位置假设；2）没有显式地将视觉区域与文本实体连接起来；3）没有预定的概念集。为了解决这个问题，我们提出了一个新的框架：TextPSG，包括四个模块：区域分组器、实体降解器、段合并器和标签生成器。我们还提出了一些新的技术。区域分组器首先将图像像素分成不同的段，然后实体降解器将视觉段与文本描述相对应。这些降解结果可以作为pseudo标签，使段合并器学习段的相似性以及导引标签生成器学习对象 semantics和关系预测，从而实现细化的结构化场景理解。我们的框架高效，Significantly Outperforming基elines，并且具有强大的Out-of-distribution Robustness。我们进行了广泛的拆分分析，以证明我们的设计选择的有效性，并提供了深入的分析，以透视未来的方向。我们的代码、数据和结果都可以在我们项目页面上找到：<https://vis-www.cs.umass.edu/TextPSG>。
</details></li>
</ul>
<hr>
<h2 id="Utilizing-Synthetic-Data-for-Medical-Vision-Language-Pre-training-Bypassing-the-Need-for-Real-Images"><a href="#Utilizing-Synthetic-Data-for-Medical-Vision-Language-Pre-training-Bypassing-the-Need-for-Real-Images" class="headerlink" title="Utilizing Synthetic Data for Medical Vision-Language Pre-training: Bypassing the Need for Real Images"></a>Utilizing Synthetic Data for Medical Vision-Language Pre-training: Bypassing the Need for Real Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07027">http://arxiv.org/abs/2310.07027</a></li>
<li>repo_url: None</li>
<li>paper_authors: Che Liu, Anand Shah, Wenjia Bai, Rossella Arcucci</li>
<li>for: 本研究旨在探讨 whether 医学视觉语言预训(VLP) 可以通过使用生成的医学图像来实现有效的预训，而不需要大量的对应的图像文本数据集。</li>
<li>methods: 本研究使用了三种state-of-the-art VLP算法，即使用生成的医学图像进行solely training。</li>
<li>results: 我们的实验结果表明，使用生成的医学图像可以达到与真实图像相当的性能，或者甚至超过它们。我们还引入了一个大规模的生成医学图像数据集，并将其与匿名化的真实医学报告集成对应。<details>
<summary>Abstract</summary>
Medical Vision-Language Pre-training (VLP) learns representations jointly from medical images and paired radiology reports. It typically requires large-scale paired image-text datasets to achieve effective pre-training for both the image encoder and text encoder. The advent of text-guided generative models raises a compelling question: Can VLP be implemented solely with synthetic images generated from genuine radiology reports, thereby mitigating the need for extensively pairing and curating image-text datasets? In this work, we scrutinize this very question by examining the feasibility and effectiveness of employing synthetic images for medical VLP. We replace real medical images with their synthetic equivalents, generated from authentic medical reports. Utilizing three state-of-the-art VLP algorithms, we exclusively train on these synthetic samples. Our empirical evaluation across three subsequent tasks, namely image classification, semantic segmentation and object detection, reveals that the performance achieved through synthetic data is on par with or even exceeds that obtained with real images. As a pioneering contribution to this domain, we introduce a large-scale synthetic medical image dataset, paired with anonymized real radiology reports. This alleviates the need of sharing medical images, which are not easy to curate and share in practice. The code and the dataset will be made publicly available upon paper acceptance.
</details>
<details>
<summary>摘要</summary>
医疗视力语言预训练（VLP）学习表示结合医疗图像和相关的医疗报告。通常需要大规模的对应图像文本数据来实现有效的预训练，以便图像编码器和文本编码器都能够得到好的表示。然而，文本导向生成模型的出现提出了一个吸引人的问题：可以通过使用生成自真实医疗报告的 sintetic 图像来实现 VLP，从而减少对对应图像文本数据的需求。在这种情况下，我们详细研究了使用 sintetic 图像来进行医疗 VLP 的可行性和效果。我们将真实的医疗图像替换为生成自它们的 sintetic 图像，然后使用三种现状顶尖 VLP 算法进行专注式训练。我们的实验结果表明，通过 sintetic 图像来进行医疗 VLP 的性能与使用真实图像相当或甚至高于。在这种领域中，我们首次提供了一个大规模的 sintetic 医疗图像集，与医疗报告相对应，以避免分享医疗图像的困难。我们将代码和数据集公开发布，以便其他研究人员可以进行更多的研究和应用。
</details></li>
</ul>
<hr>
<h2 id="Pre-Trained-Masked-Image-Model-for-Mobile-Robot-Navigation"><a href="#Pre-Trained-Masked-Image-Model-for-Mobile-Robot-Navigation" class="headerlink" title="Pre-Trained Masked Image Model for Mobile Robot Navigation"></a>Pre-Trained Masked Image Model for Mobile Robot Navigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07021">http://arxiv.org/abs/2310.07021</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vishnu Dutt Sharma, Anukriti Singh, Pratap Tokekar</li>
<li>for: 这个论文旨在探讨如何使用基础视觉模型进行环境结构预测，以提高移动机器人的导航和探索能力。</li>
<li>methods: 该论文使用Masked Autoencoders，预先在街景图像上训练，以扩展视野，实现单机器人地图探索和多机器人探索，以及indoor mapping。</li>
<li>results: 该论文显示了基础视觉模型可以无需微调，对各种输入模式进行通用应用，并且在缺乏训练数据的情况下，可以减少任务时间。更多资讯请参考<a target="_blank" rel="noopener" href="https://raaslab.org/projects/MIM4Robots%E3%80%82">https://raaslab.org/projects/MIM4Robots。</a><details>
<summary>Abstract</summary>
2D top-down maps are commonly used for the navigation and exploration of mobile robots through unknown areas. Typically, the robot builds the navigation maps incrementally from local observations using onboard sensors. Recent works have shown that predicting the structural patterns in the environment through learning-based approaches can greatly enhance task efficiency. While many such works build task-specific networks using limited datasets, we show that the existing foundational vision networks can accomplish the same without any fine-tuning. Specifically, we use Masked Autoencoders, pre-trained on street images, to present novel applications for field-of-view expansion, single-agent topological exploration, and multi-agent exploration for indoor mapping, across different input modalities. Our work motivates the use of foundational vision models for generalized structure prediction-driven applications, especially in the dearth of training data. For more qualitative results see https://raaslab.org/projects/MIM4Robots.
</details>
<details>
<summary>摘要</summary>
2D 顶点下方地图通常用于移动机器人的导航和探索未知区域。通常，机器人在当地感知器上建立导航地图，逐步增量更新。现在的研究表明，通过学习基本视觉网络可以大幅提高任务效率。虽然许多这些工作建立了特定任务的网络，但我们表明可以使用预先训练的街景图像Masked Autoencoders来实现相同的目标，无需任何微调。我们使用这些网络进行预览展示，包括预览展示、单机器人探索和多机器人探索，并且可以处理不同的输入模式。我们的工作激励使用基础视觉模型来预测环境结构，特别是在训练数据稀缺的情况下。更多资讯请参考https://raaslab.org/projects/MIM4Robots。
</details></li>
</ul>
<hr>
<h2 id="Zero-Shot-Open-Vocabulary-Tracking-with-Large-Pre-Trained-Models"><a href="#Zero-Shot-Open-Vocabulary-Tracking-with-Large-Pre-Trained-Models" class="headerlink" title="Zero-Shot Open-Vocabulary Tracking with Large Pre-Trained Models"></a>Zero-Shot Open-Vocabulary Tracking with Large Pre-Trained Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06992">http://arxiv.org/abs/2310.06992</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wen-Hsuan Chu, Adam W. Harley, Pavel Tokmakov, Achal Dave, Leonidas Guibas, Katerina Fragkiadaki</li>
<li>for: 这篇论文是为了提出一种基于大规模预训练模型的开放 vocabulary 视频跟踪方法，用于跟踪任意类别的对象在2D视频中。</li>
<li>methods: 该方法使用了一种开放词汇探测器、分割器和紧密的滤波估计器，将这些模型重新用于跟踪和分割任意类别的对象在2D视频中。</li>
<li>results: 该方法在多个已知的视频对象分割和跟踪 benchmark 上达到了强大的表现，并且能够在掩蔽中重新识别对象。<details>
<summary>Abstract</summary>
Object tracking is central to robot perception and scene understanding. Tracking-by-detection has long been a dominant paradigm for object tracking of specific object categories. Recently, large-scale pre-trained models have shown promising advances in detecting and segmenting objects and parts in 2D static images in the wild. This begs the question: can we re-purpose these large-scale pre-trained static image models for open-vocabulary video tracking? In this paper, we re-purpose an open-vocabulary detector, segmenter, and dense optical flow estimator, into a model that tracks and segments objects of any category in 2D videos. Our method predicts object and part tracks with associated language descriptions in monocular videos, rebuilding the pipeline of Tractor with modern large pre-trained models for static image detection and segmentation: we detect open-vocabulary object instances and propagate their boxes from frame to frame using a flow-based motion model, refine the propagated boxes with the box regression module of the visual detector, and prompt an open-world segmenter with the refined box to segment the objects. We decide the termination of an object track based on the objectness score of the propagated boxes, as well as forward-backward optical flow consistency. We re-identify objects across occlusions using deep feature matching. We show that our model achieves strong performance on multiple established video object segmentation and tracking benchmarks, and can produce reasonable tracks in manipulation data. In particular, our model outperforms previous state-of-the-art in UVO and BURST, benchmarks for open-world object tracking and segmentation, despite never being explicitly trained for tracking. We hope that our approach can serve as a simple and extensible framework for future research.
</details>
<details>
<summary>摘要</summary>
Object tracking是Robot感知和场景理解中的核心。Tracking-by-detection是特定对象类型的tracking的传统方法。最近，大规模预训练模型在2D静止图像中探测和分割对象和部分表现出了可观的进步。这引发了问题：我们可以将这些大规模预训练静止图像模型重新用于开放词汇视频跟踪吗？在这篇文章中，我们将开放词汇探测、分割和稠密运动场景估计器重新用于跟踪和分割任意类型对象在2D视频中。我们的方法预测对象和部分跟踪，并将其关联到语言描述。我们在无监测下探测开放词汇对象实例，从帧到帧传播这些实例的框架，并使用视觉探测器的框架进行框架进行简单的级联探测。我们根据框架的框架进行简单的级联探测。我们在视频中跟踪对象的终止是根据对象存在程度和前后方向运动的一致性决定。我们使用深度匹配来重新识别对象。我们的方法在多个已知的视频对象分割和跟踪标准准则上达到了强性表现，并且在执行操作数据时可以生成合理的跟踪。特别是，我们的方法在UVO和BURST两个开放世界对象跟踪和分割标准上超越了之前的状态。我们希望我们的方法可以作为一个简单和可扩展的框架，为未来的研究提供服务。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Neural-Radiance-Fields-for-Uncertainty-Aware-Visual-Localization"><a href="#Leveraging-Neural-Radiance-Fields-for-Uncertainty-Aware-Visual-Localization" class="headerlink" title="Leveraging Neural Radiance Fields for Uncertainty-Aware Visual Localization"></a>Leveraging Neural Radiance Fields for Uncertainty-Aware Visual Localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06984">http://arxiv.org/abs/2310.06984</a></li>
<li>repo_url: None</li>
<li>paper_authors: Le Chen, Weirong Chen, Rui Wang, Marc Pollefeys</li>
<li>for: 提高Scene Coordinate Regression（SCR）的数据效率和准确率，使其成为一种可靠的视觉地标化技术。</li>
<li>methods: 利用Neural Radiance Fields（NeRF）生成SCR训练样本，并在样本中预测颜色和深度图像的不确定性。采用深度学习和证据不确定性来执行SCR，并根据不确定性三个艺术来选择视角。</li>
<li>results: 在公共数据集上进行实验，发现我们的方法可以选择带有最高信息增加的样本，并提高SCR的数据效率和准确率。<details>
<summary>Abstract</summary>
As a promising fashion for visual localization, scene coordinate regression (SCR) has seen tremendous progress in the past decade. Most recent methods usually adopt neural networks to learn the mapping from image pixels to 3D scene coordinates, which requires a vast amount of annotated training data. We propose to leverage Neural Radiance Fields (NeRF) to generate training samples for SCR. Despite NeRF's efficiency in rendering, many of the rendered data are polluted by artifacts or only contain minimal information gain, which can hinder the regression accuracy or bring unnecessary computational costs with redundant data. These challenges are addressed in three folds in this paper: (1) A NeRF is designed to separately predict uncertainties for the rendered color and depth images, which reveal data reliability at the pixel level. (2) SCR is formulated as deep evidential learning with epistemic uncertainty, which is used to evaluate information gain and scene coordinate quality. (3) Based on the three arts of uncertainties, a novel view selection policy is formed that significantly improves data efficiency. Experiments on public datasets demonstrate that our method could select the samples that bring the most information gain and promote the performance with the highest efficiency.
</details>
<details>
<summary>摘要</summary>
随着视觉本地化的潮流，场景坐标回归（SCR）在过去的十年中取得了很大的进步。大多数最新的方法通常采用神经网络来学习图像像素到3D场景坐标的映射，需要大量的注解训练数据。我们提议利用神经辐射场（NeRF）生成训练样本。尽管NeRF有效地渲染图像，但是许多渲染数据受到艺术ifacts或只含有有限的信息增长，这可能会降低回归精度或带来无需要的计算成本。这些挑战在本文中被解决：1. NeRF用于分别预测渲染色彩和深度图像的不确定性，以显示像素级别的数据可靠性。2. SCR被表述为深度证明学中的证明不确定性，用于评估信息增长和场景坐标质量。3. 基于三种不确定性，我们提出了一种新的视图选择策略，可以大幅提高数据效率。在公共数据集上进行的实验表明，我们的方法可以选择带有最高信息增长的样本，并提高性能的效率。
</details></li>
</ul>
<hr>
<h2 id="Data-Distillation-Can-Be-Like-Vodka-Distilling-More-Times-For-Better-Quality"><a href="#Data-Distillation-Can-Be-Like-Vodka-Distilling-More-Times-For-Better-Quality" class="headerlink" title="Data Distillation Can Be Like Vodka: Distilling More Times For Better Quality"></a>Data Distillation Can Be Like Vodka: Distilling More Times For Better Quality</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06982">http://arxiv.org/abs/2310.06982</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuxi Chen, Yu Yang, Zhangyang Wang, Baharan Mirzasoleiman</li>
<li>for: 降低深度网络训练大数据集时间和内存占用，通过创建一小集合的 sintetic 图像，使得模型在这些 sintetic 图像上的泛化性能与原始数据集相似。</li>
<li>methods: 我们提出了多个 sintetic subset 的 Progressive Dataset Distillation (PDD) 方法，每个subset 都是基于之前的subset 来 conditioning，并在训练过程中逐步添加这些subset 来 capture 训练 dynamics。</li>
<li>results: 我们的实验表明，PDD 可以效果地提高现有 dataset distillation 方法的性能，最高提高4.3%；此外，我们的方法可以生成许多更大的 sintetic 数据集。<details>
<summary>Abstract</summary>
Dataset distillation aims to minimize the time and memory needed for training deep networks on large datasets, by creating a small set of synthetic images that has a similar generalization performance to that of the full dataset. However, current dataset distillation techniques fall short, showing a notable performance gap when compared to training on the original data. In this work, we are the first to argue that using just one synthetic subset for distillation will not yield optimal generalization performance. This is because the training dynamics of deep networks drastically change during the training. Hence, multiple synthetic subsets are required to capture the training dynamics at different phases of training. To address this issue, we propose Progressive Dataset Distillation (PDD). PDD synthesizes multiple small sets of synthetic images, each conditioned on the previous sets, and trains the model on the cumulative union of these subsets without requiring additional training time. Our extensive experiments show that PDD can effectively improve the performance of existing dataset distillation methods by up to 4.3%. In addition, our method for the first time enable generating considerably larger synthetic datasets.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese: dataset distillation 目的是减少训练深度网络的时间和内存需求，通过创建一个小型的合成图像集，以提高模型的泛化性能。然而，当前的 dataset distillation 技术尚未能充分利用合成图像集的优势，显示了训练原始数据时的性能差距。在这项工作中，我们是首次 argue 使用单一的合成subset 不能提供最佳的泛化性能。这是因为深度网络在训练过程中的训练动态会发生剧烈变化。因此，我们需要使用多个合成subset，以捕捉不同阶段的训练动态。为此，我们提出了 Progressive Dataset Distillation (PDD)。PDD 使用多个小型的合成图像集，每个集合基于前一个集合，并在这些集合的积合union 上训练模型，无需额外的训练时间。我们的广泛实验表明，PDD 可以提高现有的 dataset distillation 方法的性能，最高提高4.3%。此外，我们的方法首次允许生成较大的合成图像集。
</details></li>
</ul>
<hr>
<h2 id="ObjectComposer-Consistent-Generation-of-Multiple-Objects-Without-Fine-tuning"><a href="#ObjectComposer-Consistent-Generation-of-Multiple-Objects-Without-Fine-tuning" class="headerlink" title="ObjectComposer: Consistent Generation of Multiple Objects Without Fine-tuning"></a>ObjectComposer: Consistent Generation of Multiple Objects Without Fine-tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06968">http://arxiv.org/abs/2310.06968</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alec Helbling, Evan Montoya, Duen Horng Chau</li>
<li>for: 生成高品质图像 FROM 文本提示，但这些模型困难在不同上下文中同样的物体外观生成。</li>
<li>methods: 我们提出了一种名为 ObjectComposer 的方法，可以生成基于用户指定的图像的对象compositions。我们的方法不需要训练，基于现有的模型。</li>
<li>results: ObjectComposer 可以生成高品质的对象compositions，同时保持用户指定的物体外观和配置。这种方法不需要修改基础模型的参数，可以在实时和大规模应用中使用。<details>
<summary>Abstract</summary>
Recent text-to-image generative models can generate high-fidelity images from text prompts. However, these models struggle to consistently generate the same objects in different contexts with the same appearance. Consistent object generation is important to many downstream tasks like generating comic book illustrations with consistent characters and setting. Numerous approaches attempt to solve this problem by extending the vocabulary of diffusion models through fine-tuning. However, even lightweight fine-tuning approaches can be prohibitively expensive to run at scale and in real-time. We introduce a method called ObjectComposer for generating compositions of multiple objects that resemble user-specified images. Our approach is training-free, leveraging the abilities of preexisting models. We build upon the recent BLIP-Diffusion model, which can generate images of single objects specified by reference images. ObjectComposer enables the consistent generation of compositions containing multiple specific objects simultaneously, all without modifying the weights of the underlying models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Comparing-the-robustness-of-modern-no-reference-image-and-video-quality-metrics-to-adversarial-attacks"><a href="#Comparing-the-robustness-of-modern-no-reference-image-and-video-quality-metrics-to-adversarial-attacks" class="headerlink" title="Comparing the robustness of modern no-reference image- and video-quality metrics to adversarial attacks"></a>Comparing the robustness of modern no-reference image- and video-quality metrics to adversarial attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06958">http://arxiv.org/abs/2310.06958</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/msu-video-group/msu_metrics_robustness_benchmark">https://github.com/msu-video-group/msu_metrics_robustness_benchmark</a></li>
<li>paper_authors: Anastasia Antsiferova, Khaled Abud, Aleksandr Gushchin, Sergey Lavrushkin, Ekaterina Shumitskaya, Maksim Velikanov, Dmitriy Vatolin</li>
<li>for: 本研究旨在分析现代图像和视频质量度量的 adversarial robustness，以确定哪些度量表示更高的安全性。</li>
<li>methods: 本研究采用了 computer vision 领域中的 adversarial attacks，对 15 种无参考图像&#x2F;视频质量度量进行比较，检测它们对 adversarial attacks 的抵抗力。</li>
<li>results: 研究发现一些度量表示高度的抵抗力，使其在 benchmark 中更安全。  benchmark 接受新的度量提交，邀请研究人员通过提高度量的 adversarial robustness 或找到更安全的度量来参与研究。 用户可以使用 pip install robustness-benchmark 进行测试。<details>
<summary>Abstract</summary>
Nowadays neural-network-based image- and video-quality metrics show better performance compared to traditional methods. However, they also became more vulnerable to adversarial attacks that increase metrics' scores without improving visual quality. The existing benchmarks of quality metrics compare their performance in terms of correlation with subjective quality and calculation time. However, the adversarial robustness of image-quality metrics is also an area worth researching. In this paper, we analyse modern metrics' robustness to different adversarial attacks. We adopted adversarial attacks from computer vision tasks and compared attacks' efficiency against 15 no-reference image/video-quality metrics. Some metrics showed high resistance to adversarial attacks which makes their usage in benchmarks safer than vulnerable metrics. The benchmark accepts new metrics submissions for researchers who want to make their metrics more robust to attacks or to find such metrics for their needs. Try our benchmark using pip install robustness-benchmark.
</details>
<details>
<summary>摘要</summary>
现在的神经网络基于的图像和视频质量指标表现更好于传统方法，但它们也变得更易受到攻击，使其分数提高而不改善视觉质量。现有的质量指标比较标准是根据主观质量相关性和计算时间。然而，对图像质量指标的攻击Robustness也是一个值得研究的领域。在这篇论文中，我们分析了现代指标对不同的攻击方法的Robustness。我们从计算机视觉任务中抽取了攻击，并将其与15种无参考图像/视频质量指标进行比较。一些指标具有高度抵抗攻击的能力，使其在标准中使用更加安全。我们的标准接受研究人员提交新的指标，以提高指标的Robustness或找到适合他们需求的指标。您可以使用pip安装robustness-benchmark来尝试我们的标准。
</details></li>
</ul>
<hr>
<h2 id="End-to-end-Evaluation-of-Practical-Video-Analytics-Systems-for-Face-Detection-and-Recognition"><a href="#End-to-end-Evaluation-of-Practical-Video-Analytics-Systems-for-Face-Detection-and-Recognition" class="headerlink" title="End-to-end Evaluation of Practical Video Analytics Systems for Face Detection and Recognition"></a>End-to-end Evaluation of Practical Video Analytics Systems for Face Detection and Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06945">http://arxiv.org/abs/2310.06945</a></li>
<li>repo_url: None</li>
<li>paper_authors: Praneet Singh, Edward J. Delp, Amy R. Reibman</li>
<li>for: 这篇论文主要是为了评估一个执行计算机视觉任务的自动驾驶车辆上的实时视频分析系统。</li>
<li>methods: 这篇论文使用了流行的视频编码器HEVC进行视频压缩，然后将输入传递给执行面检测、对齐和识别等计算机视觉任务的模块。</li>
<li>results: 该论文通过使用适用于驾驶specific的数据集进行了全面的端到端评估，并发现了独立评估模块、数据集不均衡和笔迹不一致等因素可能导致系统性能估计错误。提出了创建平衡评估子集和确保数据集和分析任务之间的协调的策略。然后对端到端系统性能进行了顺序评估，以考虑任务之间的依赖关系。实验结果表明，我们的方法可以提供正确、可靠、可解释的系统性能估计，这对实际应用非常重要。<details>
<summary>Abstract</summary>
Practical video analytics systems that are deployed in bandwidth constrained environments like autonomous vehicles perform computer vision tasks such as face detection and recognition. In an end-to-end face analytics system, inputs are first compressed using popular video codecs like HEVC and then passed onto modules that perform face detection, alignment, and recognition sequentially. Typically, the modules of these systems are evaluated independently using task-specific imbalanced datasets that can misconstrue performance estimates. In this paper, we perform a thorough end-to-end evaluation of a face analytics system using a driving-specific dataset, which enables meaningful interpretations. We demonstrate how independent task evaluations, dataset imbalances, and inconsistent annotations can lead to incorrect system performance estimates. We propose strategies to create balanced evaluation subsets of our dataset and to make its annotations consistent across multiple analytics tasks and scenarios. We then evaluate the end-to-end system performance sequentially to account for task interdependencies. Our experiments show that our approach provides consistent, accurate, and interpretable estimates of the system's performance which is critical for real-world applications.
</details>
<details>
<summary>摘要</summary>
实际的视频分析系统在带宽缩限环境中，如自动驾驶车辆，执行计算机视觉任务，如人脸检测和识别。在末端面 analytics 系统中，输入首先使用流行的视频编码器如 HEVC 压缩，然后传递到模块进行人脸检测、对应和识别的sequential进行处理。通常，这些系统的模块会被独立进行评估，使用任务特定的不均衡数据集来评估性能。在这篇论文中，我们进行了综合的末端评估，使用驾驶相关的数据集，以获得可靠的性能估计。我们示出了独立任务评估、数据集不均衡和多个分析任务和场景中的标注不一致可能导致系统性能估计错误的情况。我们提议创建均衡评估 subsets 并使其标注一致于多个分析任务和场景。然后，我们顺序评估整个系统性能，以兑合任务之间的依赖关系。我们的实验结果表明，我们的方法可以提供可靠、准确和可解释的系统性能估计，这是实际应用中的关键。
</details></li>
</ul>
<hr>
<h2 id="Self-supervised-Object-Centric-Learning-for-Videos"><a href="#Self-supervised-Object-Centric-Learning-for-Videos" class="headerlink" title="Self-supervised Object-Centric Learning for Videos"></a>Self-supervised Object-Centric Learning for Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06907">http://arxiv.org/abs/2310.06907</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shvdiwnkozbw/SMTC">https://github.com/shvdiwnkozbw/SMTC</a></li>
<li>paper_authors: Görkay Aydemir, Weidi Xie, Fatma Güney</li>
<li>for: 这个论文旨在提出一种完全无监督的多物体分割方法，用于真实世界预算中的影像序列。</li>
<li>methods: 我们的方法基于对每帧帧排序的物体构造，并在每帧帧之间的时间相互关联。我们提出了一种遮盾策略，将大量的标签从特征空间中消除，以提高效率和规化。</li>
<li>results: 我们的方法可以成功地分割 YouTube 影像序列中的多个复杂和多种类型的物体，并且可以提供高质量的分割结果。<details>
<summary>Abstract</summary>
Unsupervised multi-object segmentation has shown impressive results on images by utilizing powerful semantics learned from self-supervised pretraining. An additional modality such as depth or motion is often used to facilitate the segmentation in video sequences. However, the performance improvements observed in synthetic sequences, which rely on the robustness of an additional cue, do not translate to more challenging real-world scenarios. In this paper, we propose the first fully unsupervised method for segmenting multiple objects in real-world sequences. Our object-centric learning framework spatially binds objects to slots on each frame and then relates these slots across frames. From these temporally-aware slots, the training objective is to reconstruct the middle frame in a high-level semantic feature space. We propose a masking strategy by dropping a significant portion of tokens in the feature space for efficiency and regularization. Additionally, we address over-clustering by merging slots based on similarity. Our method can successfully segment multiple instances of complex and high-variety classes in YouTube videos.
</details>
<details>
<summary>摘要</summary>
自动多对象分割已经在图像上显示出了很好的结果，通过利用自我监督预训练中强大的 semantics 学习。在视频序列中，通常会使用一个额外modal，如深度或运动，来促进分割。然而，在Synthetic序列中观察到的性能提升不会在更复杂的实际场景中转移。在这篇论文中，我们提出了第一个完全无监督的方法，用于在实际场景中分割多个 объек。我们的 object-centric 学习框架将 объек 分配到每帧的幂 Space 中，然后在帧中关联这些幂 Space。我们的训练目标是在高级 semantics 特征空间中重建中间帧。我们提出了一种masking策略，通过删除大量的 токен在特征空间中来提高效率和规范。此外，我们解决过度归一化问题，通过基于相似性的槽合并。我们的方法可以成功地分割 YouTube 视频中的多个复杂和多样化的实例。
</details></li>
</ul>
<hr>
<h2 id="Distillation-Improves-Visual-Place-Recognition-for-Low-Quality-Queries"><a href="#Distillation-Improves-Visual-Place-Recognition-for-Low-Quality-Queries" class="headerlink" title="Distillation Improves Visual Place Recognition for Low-Quality Queries"></a>Distillation Improves Visual Place Recognition for Low-Quality Queries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06906">http://arxiv.org/abs/2310.06906</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anbang Yang, Yao Wang, John-Ross Rizzo, Chen Feng</li>
<li>for: 提高低质量查询图像的视觉地理位置认识（VPR）性能。</li>
<li>methods: 使用高质量查询图像进行训练，以提取更好的特征表示，并使用MSE损失和ICKD损失来帮助学习模型。</li>
<li>results: 在 Pittsburgh 250k 数据集和自己的室内数据集中，通过 Fine-tune NetVLAD 参数并使用我们的扩展损失函数，对低质量查询图像进行视觉地理位置认识，实现了明显的改善。<details>
<summary>Abstract</summary>
The shift to online computing for real-time visual localization often requires streaming query images/videos to a server for visual place recognition (VPR), where fast video transmission may result in reduced resolution or increased quantization. This compromises the quality of global image descriptors, leading to decreased VPR performance. To improve the low recall rate for low-quality query images, we present a simple yet effective method that uses high-quality queries only during training to distill better feature representations for deep-learning-based VPR, such as NetVLAD. Specifically, we use mean squared error (MSE) loss between the global descriptors of queries with different qualities, and inter-channel correlation knowledge distillation (ICKD) loss over their corresponding intermediate features. We validate our approach using the both Pittsburgh 250k dataset and our own indoor dataset with varying quantization levels. By fine-tuning NetVLAD parameters with our distillation-augmented losses, we achieve notable VPR recall-rate improvements over low-quality queries, as demonstrated in our extensive experimental results. We believe this work not only pushes forward the VPR research but also provides valuable insights for applications needing dependable place recognition under resource-limited conditions.
</details>
<details>
<summary>摘要</summary>
往往在在线计算中进行实时视觉地标注需要将查询图像/视频流式传输到服务器进行视觉地标注（VPR），这可能会导致快速视频传输，从而导致图像的分辨率下降或者Quantization增加。这会下降全图像描述器的质量，从而降低VPR性能。为了提高低质量查询图像的回归率，我们提出了一种简单 yet有效的方法。在训练时使用高质量查询图像来浸泡出更好的特征表示，例如NetVLAD。我们使用查询图像的全球描述器之间的平均方差（MSE）损失，以及其相应的中间特征之间的相关知识储存（ICKD）损失。我们验证了我们的方法使用 Pittsburgh 250k 数据集和我们自己的室内数据集，并对查询图像的量化水平进行了变化。通过调整 NetVLAD 参数使用我们的浸泡损失和储存损失，我们实现了对低质量查询图像的 VPR 回归率的显著改进。我们认为这种工作不仅推进了 VPR 研究，也提供了具有可靠性的地标注应用场景中的有价值的意见。
</details></li>
</ul>
<hr>
<h2 id="Mitigating-stereotypical-biases-in-text-to-image-generative-systems"><a href="#Mitigating-stereotypical-biases-in-text-to-image-generative-systems" class="headerlink" title="Mitigating stereotypical biases in text to image generative systems"></a>Mitigating stereotypical biases in text to image generative systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06904">http://arxiv.org/abs/2310.06904</a></li>
<li>repo_url: None</li>
<li>paper_authors: Piero Esposito, Parmida Atighehchian, Anastasis Germanidis, Deepti Ghadiyaram</li>
<li>for: 这篇论文目的是如何使用细化的人工数据来修正文本描述生成图像模型中的社会偏见，以确保模型的输出是公平的。</li>
<li>methods: 作者使用了多元组合的文本提示来生成多元的人工数据，然后使用这些数据来练化文本描述生成图像模型。这种方法被称为多元练化（DFT）模型。</li>
<li>results: 对比基eline，DFT模型在 perceived skin tone 和 perceived gender 方面的公平度指标提高了150%和97.7%。此外，DFT模型也生成了更多的人类 WITH perceived darker skin tone AND more women。作者将会公开所有文本提示和代码，以便其他研究人员可以进行开放研究。<details>
<summary>Abstract</summary>
State-of-the-art generative text-to-image models are known to exhibit social biases and over-represent certain groups like people of perceived lighter skin tones and men in their outcomes. In this work, we propose a method to mitigate such biases and ensure that the outcomes are fair across different groups of people. We do this by finetuning text-to-image models on synthetic data that varies in perceived skin tones and genders constructed from diverse text prompts. These text prompts are constructed from multiplicative combinations of ethnicities, genders, professions, age groups, and so on, resulting in diverse synthetic data. Our diversity finetuned (DFT) model improves the group fairness metric by 150% for perceived skin tone and 97.7% for perceived gender. Compared to baselines, DFT models generate more people with perceived darker skin tone and more women. To foster open research, we will release all text prompts and code to generate training images.
</details>
<details>
<summary>摘要</summary>
现代生成文本图像模型已知存在社会偏见，常常过度表现人员 perceived 皮肤颜色和男性。在这项工作中，我们提议一种方法来纠正这些偏见，使结果具有不同群体人员的公平。我们通过精度调整文本图像模型在人工生成的数据上，使用包括多种民族、性别、职业、年龄等多种特征的多元组合生成文本提示。我们称之为多样性精度调整（DFT）模型。对比基eline，DFT 模型在 perceived 皮肤颜色和性别方面的公平指标提高了150%和97.7%。相比基eline，DFT 模型生成了更多的 perceived 皮肤颜色更浅的人员和更多的女性。为促进开放研究，我们将发布所有文本提示和生成训练图像代码。
</details></li>
</ul>
<hr>
<h2 id="AutoAD-II-The-Sequel-–-Who-When-and-What-in-Movie-Audio-Description"><a href="#AutoAD-II-The-Sequel-–-Who-When-and-What-in-Movie-Audio-Description" class="headerlink" title="AutoAD II: The Sequel – Who, When, and What in Movie Audio Description"></a>AutoAD II: The Sequel – Who, When, and What in Movie Audio Description</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06838">http://arxiv.org/abs/2310.06838</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tengda Han, Max Bain, Arsha Nagrani, Gül Varol, Weidi Xie, Andrew Zisserman<br>for: 本研究旨在提供一种自动生成电影 Audio Description（AD）的模型，以帮助视障观众更好地理解电影的情节。methods: 本研究使用 CLIP 视觉特征、cast 列表和时间语音位置来生成 AD，并解决 ‘who’, ‘when’ 和 ‘what’ 三个问题：(i) 谁：引入主要演员的名称、演员和 CLIP 面部特征库，以提高生成 AD 中的人名称；(ii)  WHEN：基于视觉内容和其邻近时间段的分析，确定是否需要生成 AD，以及生成 AD 的时间点；(iii) WHAT：使用视语模型，将提案集与 CLIP 视觉特征进行交叉注意力，以提高 AD 文本生成的质量。results: 研究表明，使用 proposed 的模型可以提高 AD 文本生成的质量，并在 apples-to-apples 比较中胜过先前的建模 Architecture。<details>
<summary>Abstract</summary>
Audio Description (AD) is the task of generating descriptions of visual content, at suitable time intervals, for the benefit of visually impaired audiences. For movies, this presents notable challenges -- AD must occur only during existing pauses in dialogue, should refer to characters by name, and ought to aid understanding of the storyline as a whole. To this end, we develop a new model for automatically generating movie AD, given CLIP visual features of the frames, the cast list, and the temporal locations of the speech; addressing all three of the 'who', 'when', and 'what' questions: (i) who -- we introduce a character bank consisting of the character's name, the actor that played the part, and a CLIP feature of their face, for the principal cast of each movie, and demonstrate how this can be used to improve naming in the generated AD; (ii) when -- we investigate several models for determining whether an AD should be generated for a time interval or not, based on the visual content of the interval and its neighbours; and (iii) what -- we implement a new vision-language model for this task, that can ingest the proposals from the character bank, whilst conditioning on the visual features using cross-attention, and demonstrate how this improves over previous architectures for AD text generation in an apples-to-apples comparison.
</details>
<details>
<summary>摘要</summary>
听说描述（AD）是将视觉内容描述出来，以便为视障听众提供帮助。电影中，AD 有很多挑战 -- AD 只能发生在对话中的存在插段，应该使用人物名称，并且需要帮助理解电影的故事情节。为解决这些问题，我们开发了一种新的自动生成电影 AD 模型，使用 CLIP 视觉特征、主演人员名单和时间幕上的对话时间进行生成 ; 解决 'who'、'when' 和 'what' 三个问题：(i) 谁 -- 我们提出了一个人物银行，包括每部电影的主要演员名单、他们的面部 CLIP 特征和actor的名称，并证明了如何使用这些特征来提高生成的 AD 中的命名。(ii)  WHEN -- 我们研究了多种方法来确定是否应该在某个时间间隔生成 AD，基于该时间间隔的视觉内容和其邻近时间间隔的特征。(iii) WHAT -- 我们实现了一种新的视觉语言模型，可以将人物银行的提议与视觉特征进行跨对话注意力注入，并证明了如何使用这种模型来改进之前的 AD 文本生成预算。
</details></li>
</ul>
<hr>
<h2 id="What-Does-Stable-Diffusion-Know-about-the-3D-Scene"><a href="#What-Does-Stable-Diffusion-Know-about-the-3D-Scene" class="headerlink" title="What Does Stable Diffusion Know about the 3D Scene?"></a>What Does Stable Diffusion Know about the 3D Scene?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06836">http://arxiv.org/abs/2310.06836</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guanqi Zhan, Chuanxia Zheng, Weidi Xie, Andrew Zisserman</li>
<li>for: 本研究的目的是探索Stable Diffusion网络是否对3D场景中的不同属性有深刻的理解，以便更好地理解网络如何生成高品质图像。</li>
<li>methods: 作者们提出了一种评估网络是否模型3D场景中的不同属性的协议，并在实际图像 dataset 上进行了应用。该协议包括针对不同属性的显式特征进行探索。</li>
<li>results: 作者们发现，Stable Diffusion 网络在场景几何学、支持关系、阴影和深度方面表现出色，但对 occlusion 的表现较差。此外，作者们还对 DINO 和 CLIP 等其他大规模训练的模型进行了测试，发现其表现较差于Stable Diffusion。<details>
<summary>Abstract</summary>
Recent advances in generative models like Stable Diffusion enable the generation of highly photo-realistic images. Our objective in this paper is to probe the diffusion network to determine to what extent it 'understands' different properties of the 3D scene depicted in an image. To this end, we make the following contributions: (i) We introduce a protocol to evaluate whether a network models a number of physical 'properties' of the 3D scene by probing for explicit features that represent these properties. The probes are applied on datasets of real images with annotations for the property. (ii) We apply this protocol to properties covering scene geometry, scene material, support relations, lighting, and view dependent measures. (iii) We find that Stable Diffusion is good at a number of properties including scene geometry, support relations, shadows and depth, but less performant for occlusion. (iv) We also apply the probes to other models trained at large-scale, including DINO and CLIP, and find their performance inferior to that of Stable Diffusion.
</details>
<details>
<summary>摘要</summary>
Recent advances in generative models like Stable Diffusion have enabled the generation of highly photo-realistic images. Our objective in this paper is to probe the diffusion network to determine to what extent it 'understands' different properties of the 3D scene depicted in an image. To this end, we make the following contributions:(i) We introduce a protocol to evaluate whether a network models a number of physical 'properties' of the 3D scene by probing for explicit features that represent these properties. The probes are applied on datasets of real images with annotations for the property.(ii) We apply this protocol to properties covering scene geometry, scene material, support relations, lighting, and view dependent measures.(iii) We find that Stable Diffusion is good at a number of properties including scene geometry, support relations, shadows, and depth, but less performant for occlusion.(iv) We also apply the probes to other models trained at large-scale, including DINO and CLIP, and find their performance inferior to that of Stable Diffusion.
</details></li>
</ul>
<hr>
<h2 id="Neural-Bounding"><a href="#Neural-Bounding" class="headerlink" title="Neural Bounding"></a>Neural Bounding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06822">http://arxiv.org/abs/2310.06822</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/AlexeyAB/Yolo_mark">https://github.com/AlexeyAB/Yolo_mark</a></li>
<li>paper_authors: Wenxin Liu, Michael Fischer, Paul D. Yoo, Tobias Ritschel</li>
<li>for: 本文研究使用神经网络作为缓减体（bounding volume）的应用。</li>
<li>methods: 本文使用神经网络来定义空间为可occupied或可вобоidden的分类问题，这种学习基于的方法在高维空间中（如动漫场景）具有优势。</li>
<li>results: 本文的神经缓减可以减少至少一个数量级的假阳性结果，相比传统方法。<details>
<summary>Abstract</summary>
Bounding volumes are an established concept in computer graphics and vision tasks but have seen little change since their early inception. In this work, we study the use of neural networks as bounding volumes. Our key observation is that bounding, which so far has primarily been considered a problem of computational geometry, can be redefined as a problem of learning to classify space into free or occupied. This learning-based approach is particularly advantageous in high-dimensional spaces, such as animated scenes with complex queries, where neural networks are known to excel. However, unlocking neural bounding requires a twist: allowing -- but also limiting -- false positives, while ensuring that the number of false negatives is strictly zero. We enable such tight and conservative results using a dynamically-weighted asymmetric loss function. Our results show that our neural bounding produces up to an order of magnitude fewer false positives than traditional methods.
</details>
<details>
<summary>摘要</summary>
bounding 是计算机图形和视觉任务中已经成熟的概念，但是自它的早期发明以来它几乎没有发生变化。在这项工作中，我们研究使用神经网络作为 bounding。我们的关键观察是，bounding，曾经主要被视为计算几何问题，可以被重新定义为学习将空间分类为可用或占用的问题。这种学习基于的方法在高维空间，如动漫场景中的复杂查询，where neural networks are known to excel。然而，解锁神经 bounding 需要一个折衔：允许，但也限制假阳性，而确保数量假阴性是纯粹的零。我们实现这种紧张和保守的结果使用动态权重不Symmetric 损失函数。我们的结果表明，我们的神经 bounding 可以减少至少一个数量级别的假阳性，相比传统方法。
</details></li>
</ul>
<hr>
<h2 id="TopoMLP-An-Simple-yet-Strong-Pipeline-for-Driving-Topology-Reasoning"><a href="#TopoMLP-An-Simple-yet-Strong-Pipeline-for-Driving-Topology-Reasoning" class="headerlink" title="TopoMLP: An Simple yet Strong Pipeline for Driving Topology Reasoning"></a>TopoMLP: An Simple yet Strong Pipeline for Driving Topology Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06753">http://arxiv.org/abs/2310.06753</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wudongming97/topomlp">https://github.com/wudongming97/topomlp</a></li>
<li>paper_authors: Dongming Wu, Jiahao Chang, Fan Jia, Yingfei Liu, Tiancai Wang, Jianbing Shen</li>
<li>for: 本研究旨在提高自动驾驶中的道路场景理解和驾驶路径提供，包括探测道路中心线（车道）和交通元素，然后进行这些元素之间的 topology 关系逻辑分析。</li>
<li>methods: 我们首先表明了测试性能对 topology 分析的重要性，因此我们提出了一种强大的3D车道探测器和改进的2D交通元素探测器，以扩展 topology 性能的Upper bound。然后，我们提出了一个简单又高性能的 TopoMLP 管道，用于驾驶 topology 逻辑分析。</li>
<li>results: 基于卓越的探测性能，我们开发了两个简单的 MLP 基本头，用于生成 topology。 TopoMLP 实现了 OpenLane-V2 benchmark 上的州际最佳性能，即 41.2% OLS  WITH ResNet-50 背景神经网络。此外，它还是自动驾驶挑战赛中第一个 OpenLane  topology 解决方案。我们希望这种简单又强大的管道可以为社区提供一些新的想法。代码可以在 <a target="_blank" rel="noopener" href="https://github.com/wudongming97/TopoMLP">https://github.com/wudongming97/TopoMLP</a> 找到。<details>
<summary>Abstract</summary>
Topology reasoning aims to comprehensively understand road scenes and present drivable routes in autonomous driving. It requires detecting road centerlines (lane) and traffic elements, further reasoning their topology relationship, i.e., lane-lane topology, and lane-traffic topology. In this work, we first present that the topology score relies heavily on detection performance on lane and traffic elements. Therefore, we introduce a powerful 3D lane detector and an improved 2D traffic element detector to extend the upper limit of topology performance. Further, we propose TopoMLP, a simple yet high-performance pipeline for driving topology reasoning. Based on the impressive detection performance, we develop two simple MLP-based heads for topology generation. TopoMLP achieves state-of-the-art performance on OpenLane-V2 benchmark, i.e., 41.2% OLS with ResNet-50 backbone. It is also the 1st solution for 1st OpenLane Topology in Autonomous Driving Challenge. We hope such simple and strong pipeline can provide some new insights to the community. Code is at https://github.com/wudongming97/TopoMLP.
</details>
<details>
<summary>摘要</summary>
topological 理解旨在全面理解路景和提供自动驾驶路线。它需要检测路中心线（车道）和交通元素，然后进一步分析这些元素之间的topology关系，即车道-车道topology和车道-交通topology。在这种工作中，我们首先表明了topology分数强度取决于检测车道和交通元素的性能。因此，我们提出了一种高效的3D车道检测器和改进的2D交通元素检测器，以推进topology性能的Upper Limit。此外，我们提议了TopoMLP，一个简单却高性能的驱动topology分析管道。基于出色的检测性能，我们开发了两个简单的MLP-based头，用于生成topology。TopoMLP实现了OpenLane-V2数据集上的状态对照性表现，即41.2% OLS，使用ResNet-50 背景模型。它也是自动驾驶挑战赛中的首个OpenLane topology解决方案。我们希望这种简单却强大的管道可以为社区提供新的想法。代码可以在https://github.com/wudongming97/TopoMLP中找到。
</details></li>
</ul>
<hr>
<h2 id="HiFi-123-Towards-High-fidelity-One-Image-to-3D-Content-Generation"><a href="#HiFi-123-Towards-High-fidelity-One-Image-to-3D-Content-Generation" class="headerlink" title="HiFi-123: Towards High-fidelity One Image to 3D Content Generation"></a>HiFi-123: Towards High-fidelity One Image to 3D Content Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06744">http://arxiv.org/abs/2310.06744</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/HiFi-123/HiFi-123.github.io">https://github.com/HiFi-123/HiFi-123.github.io</a></li>
<li>paper_authors: Wangbo Yu, Li Yuan, Yan-Pei Cao, Xiangjun Gao, Xiaoyu Li, Long Quan, Ying Shan, Yonghong Tian</li>
<li>for: 高精度多视图适用的图像到3D模型生成</li>
<li>methods: 引入参考图像导向新视图增强技术和参考图像导向状态储存损失</li>
<li>results: 对比现有方法，实现了高精度多视图适用的图像到3D模型生成，达到了现状态 искусственный智能水平<details>
<summary>Abstract</summary>
Recent advances in text-to-image diffusion models have enabled 3D generation from a single image. However, current image-to-3D methods often produce suboptimal results for novel views, with blurred textures and deviations from the reference image, limiting their practical applications. In this paper, we introduce HiFi-123, a method designed for high-fidelity and multi-view consistent 3D generation. Our contributions are twofold: First, we propose a reference-guided novel view enhancement technique that substantially reduces the quality gap between synthesized and reference views. Second, capitalizing on the novel view enhancement, we present a novel reference-guided state distillation loss. When incorporated into the optimization-based image-to-3D pipeline, our method significantly improves 3D generation quality, achieving state-of-the-art performance. Comprehensive evaluations demonstrate the effectiveness of our approach over existing methods, both qualitatively and quantitatively.
</details>
<details>
<summary>摘要</summary>
现在的文本到图像扩散模型已经允许从单个图像中生成3D模型。然而，当前的图像到3D方法经常生成新视图时会导致图像质量下降，缺乏参考图像的精度，限制了其实际应用。在这篇论文中，我们介绍了HiFi-123方法，用于实现高精度和多视图一致的3D生成。我们的贡献有两个方面：首先，我们提出了一种参考图像指导的新视图增强技术，可以减少生成的视图与参考视图之间的质量差距。其次，我们基于新视图增强技术，提出了一种参考图像指导的状态涂抹损失。当 incorporated into the optimization-based image-to-3D管道中，我们的方法可以明显提高3D生成质量，达到当前最佳性能。我们的评估表明，我们的方法在现有方法的基础上具有较高的效果和精度。
</details></li>
</ul>
<hr>
<h2 id="Multi-domain-improves-out-of-distribution-and-data-limited-scenarios-for-medical-image-analysis"><a href="#Multi-domain-improves-out-of-distribution-and-data-limited-scenarios-for-medical-image-analysis" class="headerlink" title="Multi-domain improves out-of-distribution and data-limited scenarios for medical image analysis"></a>Multi-domain improves out-of-distribution and data-limited scenarios for medical image analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06737">http://arxiv.org/abs/2310.06737</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ece Ozkan, Xavier Boix</li>
<li>for: 这篇论文旨在提出多元领域模型，以扩大医疗影像分析的应用范围。</li>
<li>methods: 这篇论文使用多元领域模型，结合不同领域的医疗影像资料，包括X射线、MRI、CT和ultrasound等不同的剖析方式和视角。</li>
<li>results: 相比特定领域模型，多元领域模型在有限数据和离散数据的情况下表现更好，尤其是在医疗应用中频繁遇到的外部数据情况下。多元领域模型可以更好地利用不同领域之间的共同信息，提高整体结果，例如组织识别率可以提高10%。<details>
<summary>Abstract</summary>
Current machine learning methods for medical image analysis primarily focus on developing models tailored for their specific tasks, utilizing data within their target domain. These specialized models tend to be data-hungry and often exhibit limitations in generalizing to out-of-distribution samples. Recently, foundation models have been proposed, which combine data from various domains and demonstrate excellent generalization capabilities. Building upon this, this work introduces the incorporation of diverse medical image domains, including different imaging modalities like X-ray, MRI, CT, and ultrasound images, as well as various viewpoints such as axial, coronal, and sagittal views. We refer to this approach as multi-domain model and compare its performance to that of specialized models. Our findings underscore the superior generalization capabilities of multi-domain models, particularly in scenarios characterized by limited data availability and out-of-distribution, frequently encountered in healthcare applications. The integration of diverse data allows multi-domain models to utilize shared information across domains, enhancing the overall outcomes significantly. To illustrate, for organ recognition, multi-domain model can enhance accuracy by up to 10% compared to conventional specialized models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Domain-Generalization-by-Rejecting-Extreme-Augmentations"><a href="#Domain-Generalization-by-Rejecting-Extreme-Augmentations" class="headerlink" title="Domain Generalization by Rejecting Extreme Augmentations"></a>Domain Generalization by Rejecting Extreme Augmentations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06670">http://arxiv.org/abs/2310.06670</a></li>
<li>repo_url: None</li>
<li>paper_authors: Masih Aminbeidokhti, Fidel A. Guerrero Peña, Heitor Rapela Medeiros, Thomas Dubail, Eric Granger, Marco Pedersoli</li>
<li>for: 该 paper 是为了研究数据扩充在深度学习模型中的效果，以及如何在不同领域和环境下实现模型的Recognition性能提升。</li>
<li>methods: 该 paper 使用了一种简单的训练方法，包括：(i) 使用标准数据扩充变换进行均匀采样; (ii) 根据测试数据的变化程度进行变换强化; (iii) 设计一个新的奖励函数，以拒绝不良变换。</li>
<li>results: 该 paper 的数据扩充方案在多个领域和环境下实现了比或更高的Accuracy水平，与现有方法相当或更高。代码可以在 \url{<a target="_blank" rel="noopener" href="https://github.com/Masseeh/DCAug%7D">https://github.com/Masseeh/DCAug}</a> 上找到。<details>
<summary>Abstract</summary>
Data augmentation is one of the most effective techniques for regularizing deep learning models and improving their recognition performance in a variety of tasks and domains. However, this holds for standard in-domain settings, in which the training and test data follow the same distribution. For the out-of-domain case, where the test data follow a different and unknown distribution, the best recipe for data augmentation is unclear. In this paper, we show that for out-of-domain and domain generalization settings, data augmentation can provide a conspicuous and robust improvement in performance. To do that, we propose a simple training procedure: (i) use uniform sampling on standard data augmentation transformations; (ii) increase the strength transformations to account for the higher data variance expected when working out-of-domain, and (iii) devise a new reward function to reject extreme transformations that can harm the training. With this procedure, our data augmentation scheme achieves a level of accuracy that is comparable to or better than state-of-the-art methods on benchmark domain generalization datasets. Code: \url{https://github.com/Masseeh/DCAug}
</details>
<details>
<summary>摘要</summary>
“数据扩充是深度学习模型训练中最有效的技术之一，可以提高模型在不同任务和领域的识别性能。但是，这只适用于标准内领域的情况，在异领域情况下，最佳的数据扩充策略未知。在这篇论文中，我们表明，在异领域和领域总结合 Settings中，数据扩充可以提供明显和稳定的性能改进。我们提议一种简单的训练方法：（i）使用标准数据扩充变换的均匀采样；（ii）根据异领域数据的高度变化预计，增加变换的强度；（iii）设计一个新的奖励函数，以拒绝对训练造成伤害的极端变换。通过这种方法，我们的数据扩充方案在 benchmark 领域总结合数据集上实现了与现状方法相当或更好的精度。代码：\url{https://github.com/Masseeh/DCAug}Note that the translation is done using Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Latent-Diffusion-Counterfactual-Explanations"><a href="#Latent-Diffusion-Counterfactual-Explanations" class="headerlink" title="Latent Diffusion Counterfactual Explanations"></a>Latent Diffusion Counterfactual Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06668">http://arxiv.org/abs/2310.06668</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lmb-freiburg/ldce">https://github.com/lmb-freiburg/ldce</a></li>
<li>paper_authors: Karim Farid, Simon Schrodi, Max Argus, Thomas Brox</li>
<li>for: 这个论文旨在描述一种新的方法，用于生成对抗防御模型的推论，以便更好地理解黑盒模型的行为。</li>
<li>methods: 该方法基于最近的像素空间扩散模型，并使用新的协调共识导航机制来筛选潜在的对抗性诱导。</li>
<li>results: 该方法可以快速生成对抗防御模型的推论，并且能够准确地捕捉到模型的Semantic部分。此外，该方法可以在不同的学习 парадиг和数据集上进行应用，并且可以提供模型错误的理解。<details>
<summary>Abstract</summary>
Counterfactual explanations have emerged as a promising method for elucidating the behavior of opaque black-box models. Recently, several works leveraged pixel-space diffusion models for counterfactual generation. To handle noisy, adversarial gradients during counterfactual generation -- causing unrealistic artifacts or mere adversarial perturbations -- they required either auxiliary adversarially robust models or computationally intensive guidance schemes. However, such requirements limit their applicability, e.g., in scenarios with restricted access to the model's training data. To address these limitations, we introduce Latent Diffusion Counterfactual Explanations (LDCE). LDCE harnesses the capabilities of recent class- or text-conditional foundation latent diffusion models to expedite counterfactual generation and focus on the important, semantic parts of the data. Furthermore, we propose a novel consensus guidance mechanism to filter out noisy, adversarial gradients that are misaligned with the diffusion model's implicit classifier. We demonstrate the versatility of LDCE across a wide spectrum of models trained on diverse datasets with different learning paradigms. Finally, we showcase how LDCE can provide insights into model errors, enhancing our understanding of black-box model behavior.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate into Simplified ChineseCounterfactual explanations have emerged as a promising method for elucidating the behavior of opaque black-box models. Recently, several works leveraged pixel-space diffusion models for counterfactual generation. To handle noisy, adversarial gradients during counterfactual generation -- causing unrealistic artifacts or mere adversarial perturbations -- they required either auxiliary adversarially robust models or computationally intensive guidance schemes. However, such requirements limit their applicability, e.g., in scenarios with restricted access to the model's training data. To address these limitations, we introduce Latent Diffusion Counterfactual Explanations (LDCE). LDCE harnesses the capabilities of recent class- or text-conditional foundation latent diffusion models to expedite counterfactual generation and focus on the important, semantic parts of the data. Furthermore, we propose a novel consensus guidance mechanism to filter out noisy, adversarial gradients that are misaligned with the diffusion model's implicit classifier. We demonstrate the versatility of LDCE across a wide spectrum of models trained on diverse datasets with different learning paradigms. Finally, we showcase how LDCE can provide insights into model errors, enhancing our understanding of black-box model behavior. traslate into Simplified ChineseCounterfactual explanations have emerged as a promising method for elucidating the behavior of opaque black-box models. Recently, several works leveraged pixel-space diffusion models for counterfactual generation. To handle noisy, adversarial gradients during counterfactual generation -- causing unrealistic artifacts or mere adversarial perturbations -- they required either auxiliary adversarially robust models or computationally intensive guidance schemes. However, such requirements limit their applicability, e.g., in scenarios with restricted access to the model's training data. To address these limitations, we introduce Latent Diffusion Counterfactual Explanations (LDCE). LDCE harnesses the capabilities of recent class- or text-conditional foundation latent diffusion models to expedite counterfactual generation and focus on the important, semantic parts of the data. Furthermore, we propose a novel consensus guidance mechanism to filter out noisy, adversarial gradients that are misaligned with the diffusion model's implicit classifier. We demonstrate the versatility of LDCE across a wide spectrum of models trained on diverse datasets with different learning paradigms. Finally, we showcase how LDCE can provide insights into model errors, enhancing our understanding of black-box model behavior.Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, I can provide that as well. Let me know if you have any other questions or requests!
</details></li>
</ul>
<hr>
<h2 id="SC2GAN-Rethinking-Entanglement-by-Self-correcting-Correlated-GAN-Space"><a href="#SC2GAN-Rethinking-Entanglement-by-Self-correcting-Correlated-GAN-Space" class="headerlink" title="SC2GAN: Rethinking Entanglement by Self-correcting Correlated GAN Space"></a>SC2GAN: Rethinking Entanglement by Self-correcting Correlated GAN Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06667">http://arxiv.org/abs/2310.06667</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zikun Chen, Han Zhao, Parham Aarabi, Ruowei Jiang</li>
<li>for: 该研究旨在解决生成器 adversarial网络 (GANs) 中存在的杂相关性问题，即在学习的秘密空间中存在不相关的特征集成的问题。</li>
<li>methods: 该研究使用了 StyleGAN2-FFHQ 模型，并提出了一种新的框架 SC$^2$GAN，通过重 проектирование低密度秘密编码样本并根据高密度和低密度区域修正编辑方向来解决杂相关性问题。</li>
<li>results: 该研究表明，使用 SC$^2$GAN 可以减少 GANs 中的杂相关性问题，并且可以通过小量的低密度区域样本来生成具有罕见特征组合的图像。<details>
<summary>Abstract</summary>
Generative Adversarial Networks (GANs) can synthesize realistic images, with the learned latent space shown to encode rich semantic information with various interpretable directions. However, due to the unstructured nature of the learned latent space, it inherits the bias from the training data where specific groups of visual attributes that are not causally related tend to appear together, a phenomenon also known as spurious correlations, e.g., age and eyeglasses or women and lipsticks. Consequently, the learned distribution often lacks the proper modelling of the missing examples. The interpolation following editing directions for one attribute could result in entangled changes with other attributes. To address this problem, previous works typically adjust the learned directions to minimize the changes in other attributes, yet they still fail on strongly correlated features. In this work, we study the entanglement issue in both the training data and the learned latent space for the StyleGAN2-FFHQ model. We propose a novel framework SC$^2$GAN that achieves disentanglement by re-projecting low-density latent code samples in the original latent space and correcting the editing directions based on both the high-density and low-density regions. By leveraging the original meaningful directions and semantic region-specific layers, our framework interpolates the original latent codes to generate images with attribute combination that appears infrequently, then inverts these samples back to the original latent space. We apply our framework to pre-existing methods that learn meaningful latent directions and showcase its strong capability to disentangle the attributes with small amounts of low-density region samples added.
</details>
<details>
<summary>摘要</summary>
生成敌对网络（GANs）可以生成真实的图像，学习的幂值空间中存储了丰富的 semantics 信息，其中各个方向都具有可解释的含义。然而，由于学习的幂值空间是无结构的，因此它继承了训练数据中的偏见，specific groups of visual attributes that are not causally related tend to appear together, a phenomenon also known as spurious correlations, e.g., age and eyeglasses or women and lipsticks。这使得学习的分布 часто缺少适当的模型，并且 interpolating following editing directions for one attribute could result in entangled changes with other attributes。为了解决这问题，先前的工作通常是通过最小化其他属性的变化来调整学习的方向，但它们仍然无法处理强相关的特征。在这种情况下，我们在StyleGAN2-FFHQ模型中研究了杂化问题，并提出了一种新的框架SC$^2$GAN。我们的框架可以通过重新 проектирова低密度幂值代码样本在原始幂值空间中，并根据高密度和低密度区域来修正编辑方向，以实现解耦。通过利用原始有意义的方向和 semantic region-specific层，我们的框架可以将原始幂值代码 interpolated 到生成图像，并将其推回原始幂值空间。我们对先前学习有意义的幂值方向进行了改进，并显示了我们的框架在小量低密度区域样本的情况下具有强大的解耦能力。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Explanation-Methods-for-Vision-and-Language-Navigation"><a href="#Evaluating-Explanation-Methods-for-Vision-and-Language-Navigation" class="headerlink" title="Evaluating Explanation Methods for Vision-and-Language Navigation"></a>Evaluating Explanation Methods for Vision-and-Language Navigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06654">http://arxiv.org/abs/2310.06654</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guanqi Chen, Lei Yang, Guanhua Chen, Jia Pan</li>
<li>for: 本研究旨在解释深度神经网络模型在视觉语言导航任务中的决策过程，以便更好地理解这些模型在完成任务时所使用的信息。</li>
<li>methods: 本研究使用了多种解释方法来评估深度神经网络模型在视觉语言导航任务中的决策过程。</li>
<li>results: 经过实验，我们发现了一些有价值的发现，包括：1）不同的解释方法在不同的模型和数据集上的表现有所不同；2）某些解释方法可以更好地捕捉模型在具体的决策过程中使用的信息。<details>
<summary>Abstract</summary>
The ability to navigate robots with natural language instructions in an unknown environment is a crucial step for achieving embodied artificial intelligence (AI). With the improving performance of deep neural models proposed in the field of vision-and-language navigation (VLN), it is equally interesting to know what information the models utilize for their decision-making in the navigation tasks. To understand the inner workings of deep neural models, various explanation methods have been developed for promoting explainable AI (XAI). But they are mostly applied to deep neural models for image or text classification tasks and little work has been done in explaining deep neural models for VLN tasks. In this paper, we address these problems by building quantitative benchmarks to evaluate explanation methods for VLN models in terms of faithfulness. We propose a new erasure-based evaluation pipeline to measure the step-wise textual explanation in the sequential decision-making setting. We evaluate several explanation methods for two representative VLN models on two popular VLN datasets and reveal valuable findings through our experiments.
</details>
<details>
<summary>摘要</summary>
“ nave robot 使用自然语言指令在未知环境 Navigation 是人工智能embodied的关键步骤。随着视力语言导航（VLN）领域的深度神经网络模型表现的提高，我们也更关心这些模型做出决策时所使用的信息。为了了解深度神经网络模型的内部工作，各种解释方法已经被开发出来促进可解释人工智能（XAI）。但这些解释方法主要被应用于图像或文本分类任务上，对于VLN任务的解释还做得不够。在这篇论文中，我们解决这些问题，建立了量化的benchmark来评估VLN模型的解释方法。我们提出了一种基于擦除的评估管线，用于测试步骤性文本解释在序列决策设定下。我们对两种代表性VLN模型在两个流行的VLN数据集上进行了多个实验，并通过实验获得了有价值的发现。”
</details></li>
</ul>
<hr>
<h2 id="How-not-to-ensemble-LVLMs-for-VQA"><a href="#How-not-to-ensemble-LVLMs-for-VQA" class="headerlink" title="How (not) to ensemble LVLMs for VQA"></a>How (not) to ensemble LVLMs for VQA</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06641">http://arxiv.org/abs/2310.06641</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lisa Alazraki, Lluis Castrejon, Mostafa Dehghani, Fantine Huot, Jasper Uijlings, Thomas Mensink</li>
<li>for: 这 paper 研究了在大型视力语言模型（LVLM）时代的集成方法。集成是一种经典的方法，用于将不同的模型组合起来提高性能。</li>
<li>methods: 作者在这 paper 考虑了各种模型来解决他们的任务，从普通的 LVLM 到包含描述文本作为额外 контекст的模型，以及通过探索 Wikipedia 页面来增强模型的 Lens-based  Retrieval。这些模型都是高度 complementary，因此可以合并以获得更高的性能。</li>
<li>results: 作者通过 oracle 实验发现，将这些模型集成起来可以获得大幅提高的性能，从最佳单个模型的 48.8% 准确率（最佳单个模型）提高到最高可能的 ensemble 的 67% 准确率。因此，创建一个 ensemble 并不是一个极Difficult task。<details>
<summary>Abstract</summary>
This paper studies ensembling in the era of Large Vision-Language Models (LVLMs). Ensembling is a classical method to combine different models to get increased performance. In the recent work on Encyclopedic-VQA the authors examine a wide variety of models to solve their task: from vanilla LVLMs, to models including the caption as extra context, to models augmented with Lens-based retrieval of Wikipedia pages. Intuitively these models are highly complementary, which should make them ideal for ensembling. Indeed, an oracle experiment shows potential gains from 48.8% accuracy (the best single model) all the way up to 67% (best possible ensemble). So it is a trivial exercise to create an ensemble with substantial real gains. Or is it?
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Blind-Dates-Examining-the-Expression-of-Temporality-in-Historical-Photographs"><a href="#Blind-Dates-Examining-the-Expression-of-Temporality-in-Historical-Photographs" class="headerlink" title="Blind Dates: Examining the Expression of Temporality in Historical Photographs"></a>Blind Dates: Examining the Expression of Temporality in Historical Photographs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06633">http://arxiv.org/abs/2310.06633</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexandra Barancová, Melvin Wevers, Nanne van Noord</li>
<li>for: 本研究探讨计算机视觉模型是否能够从视觉内容中提取时间信息，特意关注历史照片。</li>
<li>methods: 我们使用OpenCLIP，一个开源的CLIP实现，一种多Modal语言和视觉模型，进行研究。我们的实验包括三个步骤：零 shot分类、精度调整和视觉内容分析。</li>
<li>results: 研究结果表明，零 shot分类对于图像日期准确性不太好，倾向于预测过去的日期。经过精度调整后，OpenCLIP的表现得到了改善，并消除了偏好。分析表明，包括汽车、狗、猫、人等元素的图像更加准确地被日期化，这表明了时间标记的存在。这种研究示出计算机视觉模型可以准确地日期图像，并且需要精度调整以获得最佳效果。未来研究应该探讨这些发现的应用于颜色照片和多样化数据集。<details>
<summary>Abstract</summary>
This paper explores the capacity of computer vision models to discern temporal information in visual content, focusing specifically on historical photographs. We investigate the dating of images using OpenCLIP, an open-source implementation of CLIP, a multi-modal language and vision model. Our experiment consists of three steps: zero-shot classification, fine-tuning, and analysis of visual content. We use the \textit{De Boer Scene Detection} dataset, containing 39,866 gray-scale historical press photographs from 1950 to 1999. The results show that zero-shot classification is relatively ineffective for image dating, with a bias towards predicting dates in the past. Fine-tuning OpenCLIP with a logistic classifier improves performance and eliminates the bias. Additionally, our analysis reveals that images featuring buses, cars, cats, dogs, and people are more accurately dated, suggesting the presence of temporal markers. The study highlights the potential of machine learning models like OpenCLIP in dating images and emphasizes the importance of fine-tuning for accurate temporal analysis. Future research should explore the application of these findings to color photographs and diverse datasets.
</details>
<details>
<summary>摘要</summary>
The results show that zero-shot classification is not very effective for image dating, with a bias towards predicting dates in the past. However, fine-tuning OpenCLIP with a logistic classifier improves performance and eliminates the bias. Our analysis also reveals that images featuring buses, cars, cats, dogs, and people are more accurately dated, suggesting the presence of temporal markers.This study highlights the potential of machine learning models like OpenCLIP in dating images and emphasizes the importance of fine-tuning for accurate temporal analysis. Future research should explore the application of these findings to color photographs and diverse datasets.
</details></li>
</ul>
<hr>
<h2 id="EViT-An-Eagle-Vision-Transformer-with-Bi-Fovea-Self-Attention"><a href="#EViT-An-Eagle-Vision-Transformer-with-Bi-Fovea-Self-Attention" class="headerlink" title="EViT: An Eagle Vision Transformer with Bi-Fovea Self-Attention"></a>EViT: An Eagle Vision Transformer with Bi-Fovea Self-Attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06629">http://arxiv.org/abs/2310.06629</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nkusyl/evit">https://github.com/nkusyl/evit</a></li>
<li>paper_authors: Yulong Shi, Mingwei Sun, Yongshuai Wang, Rui Wang, Hui Sun, Zengqiang Chen</li>
<li>for: 提高计算复杂性和吸引偏好的问题</li>
<li>methods: 提出了一种新的双眼窥见自注意力（BFSA），通过模仿鸟视的生物学结构和特点，实现多尺度特征表示之间的交互</li>
<li>results: 实验结果表明，在不同计算机视觉任务中，包括图像分类、物体检测、实例分割等，提出的鸟视变换器（EViTs）可以有效地与基elines相比，并且在同一个模型大小下显示出更高的速度在图像处理器上。<details>
<summary>Abstract</summary>
Thanks to the advancement of deep learning technology, vision transformer has demonstrated competitive performance in various computer vision tasks. Unfortunately, vision transformer still faces some challenges such as high computational complexity and absence of desirable inductive bias. To alleviate these problems, a novel Bi-Fovea Self-Attention (BFSA) is proposed, inspired by the physiological structure and characteristics of bi-fovea vision in eagle eyes. This BFSA can simulate the shallow fovea and deep fovea functions of eagle vision, enable the network to extract feature representations of targets from coarse to fine, facilitate the interaction of multi-scale feature representations. Additionally, a Bionic Eagle Vision (BEV) block based on BFSA is designed in this study. It combines the advantages of CNNs and Vision Transformers to enhance the ability of global and local feature representations of networks. Furthermore, a unified and efficient general pyramid backbone network family is developed by stacking the BEV blocks in this study, called Eagle Vision Transformers (EViTs). Experimental results on various computer vision tasks including image classification, object detection, instance segmentation and other transfer learning tasks show that the proposed EViTs perform effectively by comparing with the baselines under same model size and exhibit higher speed on graphics processing unit than other models. Code is available at https://github.com/nkusyl/EViT.
</details>
<details>
<summary>摘要</summary>
因深度学习技术的进步，视transformer已经在多种计算机视觉任务中展现了竞争力。然而，视transformer仍面临一些挑战，如计算复杂性高和缺乏愿望导向。为解决这些问题，本研究提出了一种新的双眼凝聚自注意力（BFSA）， inspirited by the physiological structure and characteristics of bi-fovea vision in eagle eyes。这个BFSA可以模拟鼠标眼中的浅眼和深眼功能，使网络EXTRACTtargets的特征表示从粗到细，促进多尺度特征表示之间的互动。此外，本研究还提出了基于BFSA的鹰眼视觉（BEV）块，可以结合CNNs和视transformer的优点，提高网络的全球和本地特征表示能力。此外，本研究还开发了一个统一和高效的通用尖顶框架网络家族，通过堆叠BEV块来实现，称为鹰视transformer（EViTs）。实验结果表明，提案的EViTs在多种计算机视觉任务中表现强劲，与基准模型相比，在同样的模型大小下，并且在图像处理器上运行 faster。代码可以在https://github.com/nkusyl/EViT中找到。
</details></li>
</ul>
<hr>
<h2 id="Deep-Cardiac-MRI-Reconstruction-with-ADMM"><a href="#Deep-Cardiac-MRI-Reconstruction-with-ADMM" class="headerlink" title="Deep Cardiac MRI Reconstruction with ADMM"></a>Deep Cardiac MRI Reconstruction with ADMM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06628">http://arxiv.org/abs/2310.06628</a></li>
<li>repo_url: None</li>
<li>paper_authors: George Yiasemis, Nikita Moriakov, Jan-Jakob Sonke, Jonas Teuwen</li>
<li>for: 这个论文目的是提出一种基于深度学习的加速照片和多种 контраст重建方法，用于动态心脏成像。</li>
<li>methods: 该方法使用了深度学习 inverse problem solver vSHARP，并在图像和傅里叶空间中进行优化，以实现高精度重建。</li>
<li>results: 该方法可以在动态心脏成像中提高重建图像质量和多种contraast mapping的精度，并且可以在不同的探测方案下进行普适化。<details>
<summary>Abstract</summary>
Cardiac magnetic resonance imaging is a valuable non-invasive tool for identifying cardiovascular diseases. For instance, Cine MRI is the benchmark modality for assessing the cardiac function and anatomy. On the other hand, multi-contrast (T1 and T2) mapping has the potential to assess pathologies and abnormalities in the myocardium and interstitium. However, voluntary breath-holding and often arrhythmia, in combination with MRI's slow imaging speed, can lead to motion artifacts, hindering real-time acquisition image quality. Although performing accelerated acquisitions can facilitate dynamic imaging, it induces aliasing, causing low reconstructed image quality in Cine MRI and inaccurate T1 and T2 mapping estimation. In this work, inspired by related work in accelerated MRI reconstruction, we present a deep learning (DL)-based method for accelerated cine and multi-contrast reconstruction in the context of dynamic cardiac imaging. We formulate the reconstruction problem as a least squares regularized optimization task, and employ vSHARP, a state-of-the-art DL-based inverse problem solver, which incorporates half-quadratic variable splitting and the alternating direction method of multipliers with neural networks. We treat the problem in two setups; a 2D reconstruction and a 2D dynamic reconstruction task, and employ 2D and 3D deep learning networks, respectively. Our method optimizes in both the image and k-space domains, allowing for high reconstruction fidelity. Although the target data is undersampled with a Cartesian equispaced scheme, we train our model using both Cartesian and simulated non-Cartesian undersampling schemes to enhance generalization of the model to unseen data. Furthermore, our model adopts a deep neural network to learn and refine the sensitivity maps of multi-coil k-space data. Lastly, our method is jointly trained on both, undersampled cine and multi-contrast data.
</details>
<details>
<summary>摘要</summary>
心脏磁共振成像是一种有价值的非侵入式工具，用于诊断心血管疾病。例如，Cine MRI 是评估心脏功能和解剖结构的标准模式。然而，自降呼吸和常见的Cardiac arrhythmia，在 MRI 的慢速成像速度下，可能导致运动artefacts，妨碍实时成像质量。尽管执行加速成像可以实现动态成像，但是它会导致扩散，从而导致 Cine MRI 和多态（T1和T2）映射的重建质量低下。在这种情况下，我们基于相关的加速 MRI 重建技术，提出了一种深度学习（DL）基于的方法，用于加速 Cine 和多态重建。我们将重建问题表示为一个带有正则化的最小二乘问题，并使用 vSHARP，一种state-of-the-art DL 基于的反射问题解决方案，其包括半quadratic variable splitting和多项式方法。我们在两种设置下处理问题，一种是2D重建任务，另一种是2D动态重建任务，并使用 2D 和 3D 深度学习网络。我们的方法在图像和频率域进行优化，以确保高重建准确性。虽然目标数据使用 equispaced Cartesian 样式受抽象，但我们使用 Cartesian 和 simulated non-Cartesian 抽象样式进行训练，以便模型在未见数据上的泛化。此外，我们的模型采用深度神经网络来学习和改进多极空间数据的敏感图。最后，我们的方法在受抽象 Cine 和多态数据上进行同时训练。
</details></li>
</ul>
<hr>
<h2 id="Pi-DUAL-Using-Privileged-Information-to-Distinguish-Clean-from-Noisy-Labels"><a href="#Pi-DUAL-Using-Privileged-Information-to-Distinguish-Clean-from-Noisy-Labels" class="headerlink" title="Pi-DUAL: Using Privileged Information to Distinguish Clean from Noisy Labels"></a>Pi-DUAL: Using Privileged Information to Distinguish Clean from Noisy Labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06600">http://arxiv.org/abs/2310.06600</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ke Wang, Guillermo Ortiz-Jimenez, Rodolphe Jenatton, Mark Collier, Efi Kokiopoulou, Pascal Frossard</li>
<li>for: mitigate the effects of label noise in deep learning models</li>
<li>methods: leveraging privileged information (PI) to distinguish clean from wrong labels</li>
<li>results: significant performance improvements on key PI benchmarks, establishing a new state-of-the-art test set accuracy, and effective at identifying noisy samples post-training<details>
<summary>Abstract</summary>
Label noise is a pervasive problem in deep learning that often compromises the generalization performance of trained models. Recently, leveraging privileged information (PI) -- information available only during training but not at test time -- has emerged as an effective approach to mitigate this issue. Yet, existing PI-based methods have failed to consistently outperform their no-PI counterparts in terms of preventing overfitting to label noise. To address this deficiency, we introduce Pi-DUAL, an architecture designed to harness PI to distinguish clean from wrong labels. Pi-DUAL decomposes the output logits into a prediction term, based on conventional input features, and a noise-fitting term influenced solely by PI. A gating mechanism steered by PI adaptively shifts focus between these terms, allowing the model to implicitly separate the learning paths of clean and wrong labels. Empirically, Pi-DUAL achieves significant performance improvements on key PI benchmarks (e.g., +6.8% on ImageNet-PI), establishing a new state-of-the-art test set accuracy. Additionally, Pi-DUAL is a potent method for identifying noisy samples post-training, outperforming other strong methods at this task. Overall, Pi-DUAL is a simple, scalable and practical approach for mitigating the effects of label noise in a variety of real-world scenarios with PI.
</details>
<details>
<summary>摘要</summary>
描述预测问题（Label Noise）是深度学习中的一个广泛存在的问题，它可能会影响训练模型的泛化性能。在训练过程中，利用特权信息（PI）——只有在训练过程中可用，不可用于测试时间——已经被认为是一个有效的方法来缓解这个问题。然而，现有的PI-based方法尚未能 consistently 超越无PI counterpart在防止抖振振的方面。为了解决这种不足，我们介绍了Pi-DUAL，一种用于利用PI来分辨干净和错误标签的架构。Pi-DUAL将输出logits decomposed into一个预测项，基于传统的输入特征，以及一个随PI而变的噪音适应项。一个基于PI的闭合机制可以让模型在clean和错误标签之间分离学习路径。实验结果表明，Pi-DUAL在关键PI benchmark上（如ImageNet-PI）实现了显著的性能提升（+6.8%），成为新的州OF-the-art测试集精度。此外，Pi-DUAL还是一种高效的预测错误样本的方法，在这个任务中超越了其他强大的方法。总之，Pi-DUAL是一种简单、可扩展、实用的方法，可以在具有PI的实际应用场景中有效地缓解标签噪音的问题。
</details></li>
</ul>
<hr>
<h2 id="REVO-LION-Evaluating-and-Refining-Vision-Language-Instruction-Tuning-Datasets"><a href="#REVO-LION-Evaluating-and-Refining-Vision-Language-Instruction-Tuning-Datasets" class="headerlink" title="REVO-LION: Evaluating and Refining Vision-Language Instruction Tuning Datasets"></a>REVO-LION: Evaluating and Refining Vision-Language Instruction Tuning Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06594">http://arxiv.org/abs/2310.06594</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liaoning97/revo-lion">https://github.com/liaoning97/revo-lion</a></li>
<li>paper_authors: Ning Liao, Shaofeng Zhang, Renqiu Xia, Bo Zhang, Min Cao, Yu Qiao, Junchi Yan</li>
<li>for: 本研究旨在评估视语指令调整（VLIT）数据集的质量，并探索如何建立一个包含全面指令调整模型的数据集。</li>
<li>methods: 作者提出了一种调度跨评估方法，通过在不同数据集上进行调度并评估，来评估VLIT数据集的全面质量。并定义了Meta Quality（MQ）和Dataset Quality（DQ）来衡量数据集的质量。</li>
<li>results: 实验结果表明，作者的评估方法是有效的，并且可以建立一个包含全面指令调整模型的数据集。具体来说，使用REVO-LION数据集（包含高品质样本从各个数据集）和一个简单的加权方法，可以在只有半数据量的情况下达到与所有VLIT数据集之和的性能。<details>
<summary>Abstract</summary>
There is an emerging line of research on multimodal instruction tuning, and a line of benchmarks have been proposed for evaluating these models recently. Instead of evaluating the models directly, in this paper we try to evaluate the Vision-Language Instruction-Tuning (VLIT) datasets themselves and further seek the way of building a dataset for developing an all-powerful VLIT model, which we believe could also be of utility for establishing a grounded protocol for benchmarking VLIT models. For effective analysis of VLIT datasets that remains an open question, we propose a tune-cross-evaluation paradigm: tuning on one dataset and evaluating on the others in turn. For each single tune-evaluation experiment set, we define the Meta Quality (MQ) as the mean score measured by a series of caption metrics including BLEU, METEOR, and ROUGE-L to quantify the quality of a certain dataset or a sample. On this basis, to evaluate the comprehensiveness of a dataset, we develop the Dataset Quality (DQ) covering all tune-evaluation sets. To lay the foundation for building a comprehensive dataset and developing an all-powerful model for practical applications, we further define the Sample Quality (SQ) to quantify the all-sided quality of each sample. Extensive experiments validate the rationality of the proposed evaluation paradigm. Based on the holistic evaluation, we build a new dataset, REVO-LION (REfining VisiOn-Language InstructiOn tuNing), by collecting samples with higher SQ from each dataset. With only half of the full data, the model trained on REVO-LION can achieve performance comparable to simply adding all VLIT datasets up. In addition to developing an all-powerful model, REVO-LION also includes an evaluation set, which is expected to serve as a convenient evaluation benchmark for future research.
</details>
<details>
<summary>摘要</summary>
有一条emerging的研究方向是多模式指令调整（Multimodal Instruction Tuning，简称VLIT），而最近有一些benchmark被提出来评估这些模型。而不是直接评估这些模型，在这篇论文中我们尝试了评估VLIT数据集本身，并寻求如何建立一个可以开发出全面强大VLIT模型的数据集。为了有效地分析VLIT数据集，我们提出了一种跨评分法：在不同的数据集上进行调整，然后在另一个数据集上进行评估。对于每个单独的调评试验集，我们定义了Meta Quality（MQ）为在不同的caption metric（包括BLEU、METEOR和ROUGE-L）中的平均分数，以衡量一个特定数据集或样本的质量。基于这个基础，我们开发了Dataset Quality（DQ），用于评估数据集的完整性。为了建立一个全面的数据集和开发实际应用中的强大模型，我们进一步定义了Sample Quality（SQ），用于衡量每个样本的多方面质量。我们的实验证明了我们提出的评估方法的合理性。基于这种整体评估，我们建立了一个新的数据集，REVO-LION（REfining VisiOn-Language InstructiOn tuNing），通过收集具有更高SQ的样本来建立。与整个数据集的一半数据量相比，模型在REVO-LION上训练可以 дости到与所有VLIT数据集之和相当的性能。除了开发全面强大的模型外，REVO-LION还包括评估集，预计将为未来研究提供便捷的评估标准。
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-Mask2Former-Panoptic-Segmentation-of-Crops-Weeds-and-Leaves"><a href="#Hierarchical-Mask2Former-Panoptic-Segmentation-of-Crops-Weeds-and-Leaves" class="headerlink" title="Hierarchical Mask2Former: Panoptic Segmentation of Crops, Weeds and Leaves"></a>Hierarchical Mask2Former: Panoptic Segmentation of Crops, Weeds and Leaves</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06582">http://arxiv.org/abs/2310.06582</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/madeleinedarbyshire/hierarchicalmask2former">https://github.com/madeleinedarbyshire/hierarchicalmask2former</a></li>
<li>paper_authors: Madeleine Darbyshire, Elizabeth Sklar, Simon Parsons</li>
<li>for: 这篇论文是为了提出一种基于机器视觉的精准农业技术，以提高农业生产效率和降低资源消耗。</li>
<li>methods: 这篇论文使用了一种层次结构的�annoptic分割方法，同时可以识别植物的生长指标和找到苔藿。这种方法基于Mask2Former框架，可以预测植物、苔藿和叶子的面积。</li>
<li>results: 该论文实现了PQ{\dag}的75.99，并提出了一些减少计算和时间成本的方法，使得核心算法的执行速度可以减少60%，而PQ{\dag}的下降幅度仅占1%。<details>
<summary>Abstract</summary>
Advancements in machine vision that enable detailed inferences to be made from images have the potential to transform many sectors including agriculture. Precision agriculture, where data analysis enables interventions to be precisely targeted, has many possible applications. Precision spraying, for example, can limit the application of herbicide only to weeds, or limit the application of fertiliser only to undernourished crops, instead of spraying the entire field. The approach promises to maximise yields, whilst minimising resource use and harms to the surrounding environment. To this end, we propose a hierarchical panoptic segmentation method to simultaneously identify indicators of plant growth and locate weeds within an image. We adapt Mask2Former, a state-of-the-art architecture for panoptic segmentation, to predict crop, weed and leaf masks. We achieve a PQ{\dag} of 75.99. Additionally, we explore approaches to make the architecture more compact and therefore more suitable for time and compute constrained applications. With our more compact architecture, inference is up to 60% faster and the reduction in PQ{\dag} is less than 1%.
</details>
<details>
<summary>摘要</summary>
“具有高度内涵的机器视觉技术，可以对图像进行详细的推论，具有广泛的应用前景。精确农业是其中一个，这种技术可以通过分析数据来实现精确的干预。例如，精确喷撒可以仅对于落叶的部分喷撒药物，或仅对于缺乏营养的作物喷撒肥料，而不是对整个田园进行喷撒。这种方法可以最大化生产量，同时最小化资源的使用和环境中的伤害。为此，我们提出了一个层次组织的当知分割方法，可以同时识别作物生长指标和发现杂草。我们将Mask2Former架构，一个现代当知分割架构，用于预测作物、杂草和叶子的面罩。我们取得了PQ{\dag}的75.99。此外，我们也考虑了将架构更加紧凑，以便在时间和计算限制下进行应用。我们的更紧凑的架构，可以在测试中提高推断速度，并且对PQ{\dag}的下降产生较小的影响。”Note: PQ{\dag} is a metric used to evaluate the performance of panoptic segmentation models, it stands for "panoptic quality" with asterisk.
</details></li>
</ul>
<hr>
<h2 id="Energy-Efficient-Visual-Search-by-Eye-Movement-and-Low-Latency-Spiking-Neural-Network"><a href="#Energy-Efficient-Visual-Search-by-Eye-Movement-and-Low-Latency-Spiking-Neural-Network" class="headerlink" title="Energy-Efficient Visual Search by Eye Movement and Low-Latency Spiking Neural Network"></a>Energy-Efficient Visual Search by Eye Movement and Low-Latency Spiking Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06578">http://arxiv.org/abs/2310.06578</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunhui Zhou, Dongqi Han, Yuguo Yu</li>
<li>for: 模型开发人类视觉系统，包括非均匀分辨率 retina、高效眼动策略和脉冲神经网络（SNN），以满足视场大小、分辨率、能耗成本和推理延迟等要求。</li>
<li>methods: 使用人类视觉中的三种特性——非均匀分辨率 retina、高效眼动策略和SNN——开发人类类似的计算机视觉系统。实验研究人类视察行为，并建立了首个基于 SNN 的视察搜索模型。该模型结合人工 retina 和脉冲特征提取、记忆和眼动决策模块，并使用人类化的 fixation 策略和人类化的追踪策略。</li>
<li>results: 模型可以学习人类类似的 fixation 策略和近似于最优的搜索策略，在搜索速度和准确率方面超越人类，同时具有高能效性和短抑制决策延迟。这些结果表明人类视觉系统的模型化在神经科学和机器学习中具有重要意义，并可以帮助开发更能效的计算机视觉算法。<details>
<summary>Abstract</summary>
Human vision incorporates non-uniform resolution retina, efficient eye movement strategy, and spiking neural network (SNN) to balance the requirements in visual field size, visual resolution, energy cost, and inference latency. These properties have inspired interest in developing human-like computer vision. However, existing models haven't fully incorporated the three features of human vision, and their learned eye movement strategies haven't been compared with human's strategy, making the models' behavior difficult to interpret. Here, we carry out experiments to examine human visual search behaviors and establish the first SNN-based visual search model. The model combines an artificial retina with spiking feature extraction, memory, and saccade decision modules, and it employs population coding for fast and efficient saccade decisions. The model can learn either a human-like or a near-optimal fixation strategy, outperform humans in search speed and accuracy, and achieve high energy efficiency through short saccade decision latency and sparse activation. It also suggests that the human search strategy is suboptimal in terms of search speed. Our work connects modeling of vision in neuroscience and machine learning and sheds light on developing more energy-efficient computer vision algorithms.
</details>
<details>
<summary>摘要</summary>
人类视觉具有不均匀分辨率 RETINA、高效眼动策略和脉冲神经网络（SNN），以满足视场大小、分辨率、能耗成本和推理延迟的要求。这些特性引发了人类视觉模型的开发兴趣。然而，现有模型没有完全包含人类视觉的三个特性，并且学习的眼动策略与人类策略不符，使模型的行为困难于解释。我们在这里进行实验研究人类视察行为，建立了首个SNN基于的视察模型。该模型组合人工 RETINA 与脉冲特征提取、记忆和眼动决策模块，并使用人口编码进行快速和高效的眼动决策。模型可以学习人类类似或近似优化的FIXATION策略，在搜寻速度和准确率方面超过人类，并且通过短时间内的眼动决策延迟和稀有的活动实现高能效性。此外，我们的工作将神经科学和机器学习的视觉模型相连，并照明开发更加能效的计算机视觉算法的可能性。
</details></li>
</ul>
<hr>
<h2 id="SketchBodyNet-A-Sketch-Driven-Multi-faceted-Decoder-Network-for-3D-Human-Reconstruction"><a href="#SketchBodyNet-A-Sketch-Driven-Multi-faceted-Decoder-Network-for-3D-Human-Reconstruction" class="headerlink" title="SketchBodyNet: A Sketch-Driven Multi-faceted Decoder Network for 3D Human Reconstruction"></a>SketchBodyNet: A Sketch-Driven Multi-faceted Decoder Network for 3D Human Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06577">http://arxiv.org/abs/2310.06577</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fei Wang, Kongzhang Tang, Hefeng Wu, Baoquan Zhao, Hao Cai, Teng Zhou</li>
<li>for: 这个论文目的是 reconstruction 3D human shape from 2D sketches.</li>
<li>methods: 该方法使用了一种名为 SketchBodyNet 的网络，具体来说是一个 backbone 和三个独立的注意力解码分支。每个解码分支都包含一个多头自注意力模块，以获取增强的特征，然后是一个多层感知器。</li>
<li>results: 该方法可以 superior 的表现 reconstruction 3D human mesh from freehand sketches.<details>
<summary>Abstract</summary>
Reconstructing 3D human shapes from 2D images has received increasing attention recently due to its fundamental support for many high-level 3D applications. Compared with natural images, freehand sketches are much more flexible to depict various shapes, providing a high potential and valuable way for 3D human reconstruction. However, such a task is highly challenging. The sparse abstract characteristics of sketches add severe difficulties, such as arbitrariness, inaccuracy, and lacking image details, to the already badly ill-posed problem of 2D-to-3D reconstruction. Although current methods have achieved great success in reconstructing 3D human bodies from a single-view image, they do not work well on freehand sketches. In this paper, we propose a novel sketch-driven multi-faceted decoder network termed SketchBodyNet to address this task. Specifically, the network consists of a backbone and three separate attention decoder branches, where a multi-head self-attention module is exploited in each decoder to obtain enhanced features, followed by a multi-layer perceptron. The multi-faceted decoders aim to predict the camera, shape, and pose parameters, respectively, which are then associated with the SMPL model to reconstruct the corresponding 3D human mesh. In learning, existing 3D meshes are projected via the camera parameters into 2D synthetic sketches with joints, which are combined with the freehand sketches to optimize the model. To verify our method, we collect a large-scale dataset of about 26k freehand sketches and their corresponding 3D meshes containing various poses of human bodies from 14 different angles. Extensive experimental results demonstrate our SketchBodyNet achieves superior performance in reconstructing 3D human meshes from freehand sketches.
</details>
<details>
<summary>摘要</summary>
<<SYS>>这里的文本将被翻译为简化字的中文。<</SYS>>近期，从二dimensional图像中重建三dimensional人体shape received increasing attention，因为它具有许多高级应用的基本支持。相比于自然图像，自由画图更加灵活地描绘了不同的形状，提供了高度潜在和有价的方法。然而，这个任务非常具有挑战性。图像的叠加特征使得问题变得更加困难，导致这已经是 badly ill-posed 的问题。 although current methods have achieved great success in reconstructing three-dimensional human bodies from a single-view image, they do not work well on freehand sketches.在这篇文章中，我们提出了一个新的图画驱动多面顶点网络，称为 SketchBodyNet，以解决这个任务。具体而言，该网络包括一个背bone和三个分开的注意力顶点分支，每个分支都包括一个多头自我注意模组，然后是一个多层感知器。这些多面顶点尝试预测摄像头、形状和姿势参数，分别与SMPL模型组合以重建相应的三dimensional human mesh。在学习过程中，我们将现有的三dimensional mesh projected via camera parameters into 2D synthetic sketches with joints，与自由画图相结合以便优化模型。为了证明我们的方法，我们收集了大约26,000幅自由画图和其对应的三dimensional mesh，这些图像包括了人体的多个姿势和角度。实验结果显示，我们的 SketchBodyNet 可以从自由画图中重建三dimensional human mesh with superior performance。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Retrieval-of-Images-with-Irregular-Patterns-using-Morphological-Image-Analysis-Applications-to-Industrial-and-Healthcare-datasets"><a href="#Efficient-Retrieval-of-Images-with-Irregular-Patterns-using-Morphological-Image-Analysis-Applications-to-Industrial-and-Healthcare-datasets" class="headerlink" title="Efficient Retrieval of Images with Irregular Patterns using Morphological Image Analysis: Applications to Industrial and Healthcare datasets"></a>Efficient Retrieval of Images with Irregular Patterns using Morphological Image Analysis: Applications to Industrial and Healthcare datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06566">http://arxiv.org/abs/2310.06566</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiajun Zhang, Georgina Cosma, Sarah Bugby, Jason Watkins</li>
<li>for: 本文提出了一个图像检索框架，用于在图像数据库中查找具有类似不规则模式的图像。</li>
<li>methods: 本文使用了一些特征提取方法，包括深度特征、颜色基于特征、形状基于特征和本地特征，以提取图像中的特征。</li>
<li>results: 本文使用了不同的特征提取方法和距离度量来评估图像检索性能，结果显示，使用DefChars和曼哈顿距离度量可以达到80%的含义平均精度和0.09的标准差，在不同的 dataset 上表现出色。<details>
<summary>Abstract</summary>
Image retrieval is the process of searching and retrieving images from a database based on their visual content and features. Recently, much attention has been directed towards the retrieval of irregular patterns within industrial or medical images by extracting features from the images, such as deep features, colour-based features, shape-based features and local features. This has applications across a spectrum of industries, including fault inspection, disease diagnosis, and maintenance prediction. This paper proposes an image retrieval framework to search for images containing similar irregular patterns by extracting a set of morphological features (DefChars) from images; the datasets employed in this paper contain wind turbine blade images with defects, chest computerised tomography scans with COVID-19 infection, heatsink images with defects, and lake ice images. The proposed framework was evaluated with different feature extraction methods (DefChars, resized raw image, local binary pattern, and scale-invariant feature transforms) and distance metrics to determine the most efficient parameters in terms of retrieval performance across datasets. The retrieval results show that the proposed framework using the DefChars and the Manhattan distance metric achieves a mean average precision of 80% and a low standard deviation of 0.09 across classes of irregular patterns, outperforming alternative feature-metric combinations across all datasets. Furthermore, the low standard deviation between each class highlights DefChars' capability for a reliable image retrieval task, even in the presence of class imbalances or small-sized datasets.
</details>
<details>
<summary>摘要</summary>
Image Retrieval是寻找和搜寻库中的图像，根据它们的视觉内容和特征进行搜寻。现在，对于工业或医疗图像中的不规律模式进行搜寻已经引起了很多注意。这可以应用于各种领域，包括缺陷检测、疾病诊断和维护预测。本文提出了一个图像搜寻框架，用于寻找具有相似不规律模式的图像。这个框架使用了一些形式特征（DefChars）从图像中提取特征，并使用了不同的特征提取方法和距离度量进行评估。结果显示，使用DefChars和曼哈顿距离度量的提案框架可以实现80%的平均精度和0.09的标准差，在不同的数据集上均以最高效的方式进行搜寻。此外，每个类别之间的标准差较低，证明了DefChars在适当的图像搜寻任务中的可靠性，即使 faced with class imbalances or small-sized datasets。
</details></li>
</ul>
<hr>
<h2 id="Compositional-Representation-Learning-for-Brain-Tumour-Segmentation"><a href="#Compositional-Representation-Learning-for-Brain-Tumour-Segmentation" class="headerlink" title="Compositional Representation Learning for Brain Tumour Segmentation"></a>Compositional Representation Learning for Brain Tumour Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06562">http://arxiv.org/abs/2310.06562</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiao Liu, Antanas Kascenas, Hannah Watson, Sotirios A. Tsaftaris, Alison Q. O’Neil</li>
<li>for: 这 paper 的目的是解决折衣瘤分 segmentation 领域中的数据短缺问题，通过混合监督框架和无监督学习来学习强健的组成表示。</li>
<li>methods: 该 paper 使用了混合监督框架 vMFNet，通过无监督学习和弱监督来学习强健的组成表示，并使用了 BraTS 数据集来生成各个 MRI 图像中的两点专家病理标注，从而构建了弱图像级别标注。</li>
<li>results: 该 paper 表明，可以通过大量的弱监督数据和只有一小部分完全标注数据来实现佳的折衣瘤分 segmentation 性能，并且发现在只受病理学超级vision（折衣瘤）的情况下，emergent learning 可以在组成表示中 Learning 出各种解剖结构。<details>
<summary>Abstract</summary>
For brain tumour segmentation, deep learning models can achieve human expert-level performance given a large amount of data and pixel-level annotations. However, the expensive exercise of obtaining pixel-level annotations for large amounts of data is not always feasible, and performance is often heavily reduced in a low-annotated data regime. To tackle this challenge, we adapt a mixed supervision framework, vMFNet, to learn robust compositional representations using unsupervised learning and weak supervision alongside non-exhaustive pixel-level pathology labels. In particular, we use the BraTS dataset to simulate a collection of 2-point expert pathology annotations indicating the top and bottom slice of the tumour (or tumour sub-regions: peritumoural edema, GD-enhancing tumour, and the necrotic / non-enhancing tumour) in each MRI volume, from which weak image-level labels that indicate the presence or absence of the tumour (or the tumour sub-regions) in the image are constructed. Then, vMFNet models the encoded image features with von-Mises-Fisher (vMF) distributions, via learnable and compositional vMF kernels which capture information about structures in the images. We show that good tumour segmentation performance can be achieved with a large amount of weakly labelled data but only a small amount of fully-annotated data. Interestingly, emergent learning of anatomical structures occurs in the compositional representation even given only supervision relating to pathology (tumour).
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Data-efficient-deep-learning-for-medical-image-analysis-A-survey"><a href="#Data-efficient-deep-learning-for-medical-image-analysis-A-survey" class="headerlink" title="Data efficient deep learning for medical image analysis: A survey"></a>Data efficient deep learning for medical image analysis: A survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06557">http://arxiv.org/abs/2310.06557</a></li>
<li>repo_url: None</li>
<li>paper_authors: Suruchi Kumari, Pravendra Singh</li>
<li>for: 本研究主要用于探讨数据效率深度学习方法在医学图像分析领域的应用。</li>
<li>methods: 本文对数据效率深度学习方法进行了全面的审视，并根据监督水平进行了分类，包括无监督、不准确监督、部分监督、不准确监督和有限监督等类别。</li>
<li>results: 本文系统地总结了医学图像分析领域中通用的数据效率深度学习方法，并探讨未来的研究方向。<details>
<summary>Abstract</summary>
The rapid evolution of deep learning has significantly advanced the field of medical image analysis. However, despite these achievements, the further enhancement of deep learning models for medical image analysis faces a significant challenge due to the scarcity of large, well-annotated datasets. To address this issue, recent years have witnessed a growing emphasis on the development of data-efficient deep learning methods. This paper conducts a thorough review of data-efficient deep learning methods for medical image analysis. To this end, we categorize these methods based on the level of supervision they rely on, encompassing categories such as no supervision, inexact supervision, incomplete supervision, inaccurate supervision, and only limited supervision. We further divide these categories into finer subcategories. For example, we categorize inexact supervision into multiple instance learning and learning with weak annotations. Similarly, we categorize incomplete supervision into semi-supervised learning, active learning, and domain-adaptive learning and so on. Furthermore, we systematically summarize commonly used datasets for data efficient deep learning in medical image analysis and investigate future research directions to conclude this survey.
</details>
<details>
<summary>摘要</summary>
“深度学习的快速进化已经大幅提高医学图像分析领域的水平。然而，虽然有这些成就，但是进一步提高深度学习模型的医学图像分析仍面临着数据稀缺的大问题。为解决这个问题，最近几年内见许多关注数据有效深度学习方法的开发。这篇文章进行了深入的对数据有效深度学习方法的审视。为此，我们将这些方法按照它们依赖的级别进行分类，包括无监督、不准确监督、不完全监督、不正确监督以及只有有限监督。我们将这些类别进一步细分，例如将不准确监督分为多个实例学习和弱标注学习。类似地，我们将不完全监督分为半监督学习、活动学习和领域适应学习等。此外，我们系统地总结了医学图像分析中常用的数据有效深度学习数据集，并 investigate未来研究方向以结束本调查。”
</details></li>
</ul>
<hr>
<h2 id="Be-Careful-What-You-Smooth-For-Label-Smoothing-Can-Be-a-Privacy-Shield-but-Also-a-Catalyst-for-Model-Inversion-Attacks"><a href="#Be-Careful-What-You-Smooth-For-Label-Smoothing-Can-Be-a-Privacy-Shield-but-Also-a-Catalyst-for-Model-Inversion-Attacks" class="headerlink" title="Be Careful What You Smooth For: Label Smoothing Can Be a Privacy Shield but Also a Catalyst for Model Inversion Attacks"></a>Be Careful What You Smooth For: Label Smoothing Can Be a Privacy Shield but Also a Catalyst for Model Inversion Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06549">http://arxiv.org/abs/2310.06549</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/LukasStruppek/Plug-and-Play-Attacks">https://github.com/LukasStruppek/Plug-and-Play-Attacks</a></li>
<li>paper_authors: Lukas Struppek, Dominik Hintersdorf, Kristian Kersting<br>for: 这篇论文旨在调查标签融合对模型私隐泄露的影响。methods: 作者使用了标签融合来调查模型的泄露情况，并对比了不同的标签融合方法的影响。results: 研究发现，传统的标签融合可能会增加模型的隐私泄露，而使用负因素融合可以阻止模型泄露class-related信息，从而保护模型的隐私。<details>
<summary>Abstract</summary>
Label smoothing -- using softened labels instead of hard ones -- is a widely adopted regularization method for deep learning, showing diverse benefits such as enhanced generalization and calibration. Its implications for preserving model privacy, however, have remained unexplored. To fill this gap, we investigate the impact of label smoothing on model inversion attacks (MIAs), which aim to generate class-representative samples by exploiting the knowledge encoded in a classifier, thereby inferring sensitive information about its training data. Through extensive analyses, we uncover that traditional label smoothing fosters MIAs, thereby increasing a model's privacy leakage. Even more, we reveal that smoothing with negative factors counters this trend, impeding the extraction of class-related information and leading to privacy preservation, beating state-of-the-art defenses. This establishes a practical and powerful novel way for enhancing model resilience against MIAs.
</details>
<details>
<summary>摘要</summary>
标签平滑（label smoothing）是一种广泛应用的深度学习常规方法，它可以提高模型的泛化和准确性。然而，它的隐私保护方面尚未得到探讨。为了填补这一空白，我们研究了标签平滑对模型反向攻击（MIA）的影响，MIA是利用分类器中嵌入的知识来生成类 representative sample，从而推测模型的训练数据。经过广泛的分析，我们发现传统的标签平滑实际上会增强模型的隐私泄露。反之，使用负因子平滑可以阻止类相关信息的提取，从而保持模型的隐私。这成为一种实用和有力的新方法，可以增强模型对MIA的抗御能力。
</details></li>
</ul>
<hr>
<h2 id="Perceptual-MAE-for-Image-Manipulation-Localization-A-High-level-Vision-Learner-Focusing-on-Low-level-Features"><a href="#Perceptual-MAE-for-Image-Manipulation-Localization-A-High-level-Vision-Learner-Focusing-on-Low-level-Features" class="headerlink" title="Perceptual MAE for Image Manipulation Localization: A High-level Vision Learner Focusing on Low-level Features"></a>Perceptual MAE for Image Manipulation Localization: A High-level Vision Learner Focusing on Low-level Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06525">http://arxiv.org/abs/2310.06525</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/SunnyHaze/PMAE">https://github.com/SunnyHaze/PMAE</a></li>
<li>paper_authors: Xiaochen Ma, Jizhe Zhou, Xiong Xu, Zhuohang Jiang, Chi-Man Pun</li>
<li>for: This paper aims to improve the performance of Image Manipulation Localization (IML) tasks by incorporating both high-level and low-level features.</li>
<li>methods: The proposed method, Perceptual MAE (PMAE), enhances the Masked Autoencoder (MAE) with high-resolution inputs and a perceptual loss supervision module to better capture both object semantics and pixel-level features.</li>
<li>results: Extensive experiments show that PMAE outperforms state-of-the-art tampering localization methods on all five publicly available datasets, demonstrating the effectiveness of integrating high-level and low-level features in IML tasks.<details>
<summary>Abstract</summary>
Nowadays, multimedia forensics faces unprecedented challenges due to the rapid advancement of multimedia generation technology thereby making Image Manipulation Localization (IML) crucial in the pursuit of truth. The key to IML lies in revealing the artifacts or inconsistencies between the tampered and authentic areas, which are evident under pixel-level features. Consequently, existing studies treat IML as a low-level vision task, focusing on allocating tampered masks by crafting pixel-level features such as image RGB noises, edge signals, or high-frequency features. However, in practice, tampering commonly occurs at the object level, and different classes of objects have varying likelihoods of becoming targets of tampering. Therefore, object semantics are also vital in identifying the tampered areas in addition to pixel-level features. This necessitates IML models to carry out a semantic understanding of the entire image. In this paper, we reformulate the IML task as a high-level vision task that greatly benefits from low-level features. Based on such an interpretation, we propose a method to enhance the Masked Autoencoder (MAE) by incorporating high-resolution inputs and a perceptual loss supervision module, which is termed Perceptual MAE (PMAE). While MAE has demonstrated an impressive understanding of object semantics, PMAE can also compensate for low-level semantics with our proposed enhancements. Evidenced by extensive experiments, this paradigm effectively unites the low-level and high-level features of the IML task and outperforms state-of-the-art tampering localization methods on all five publicly available datasets.
</details>
<details>
<summary>摘要</summary>
现在， Multimedia FORENSICS 面临了历史上 nunca antes seen 的挑战，这是因为 Multimedia 生成技术的快速发展，从而使 Image Manipulation Localization (IML) 成为了查找真实的关键。关键在于揭示 tampered 和 authentic 区域之间的差异，这些差异在像素级别特征上表现出来。因此，现有的研究都将 IML 视为一种低级视觉任务，通过创建像素级特征，如图像 RGB 噪音、边缘信号或高频特征来分配 tampered 面纱。然而，在实践中， tampering 通常发生在对象层次上，不同的对象类型有不同的抢夺概率。因此，对象 semantics 也是必要的，以便在找到 tampered 区域时具有更高的准确率。这使得 IML 模型需要对整个图像进行semantic 理解。在这篇论文中，我们将 IML 任务重新解释为一种高级视觉任务，这种任务可以受益于低级特征。基于这种解释，我们提出了一种使用高分辨率输入和一种感知损失监督模块的改进 MAE 方法，称为 Perceptual MAE (PMAE)。MAE 已经在对象 semantics 方面表现出色，而 PMAE 可以通过我们的提议的改进，同时具有低级 semantics。经验证明，这种方法可以有效地结合 IML 任务的低级和高级特征，并在所有五个公开的数据集上超越当前最佳的抹改地点标识方法。
</details></li>
</ul>
<hr>
<h2 id="Watt-For-What-Rethinking-Deep-Learning’s-Energy-Performance-Relationship"><a href="#Watt-For-What-Rethinking-Deep-Learning’s-Energy-Performance-Relationship" class="headerlink" title="Watt For What: Rethinking Deep Learning’s Energy-Performance Relationship"></a>Watt For What: Rethinking Deep Learning’s Energy-Performance Relationship</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06522">http://arxiv.org/abs/2310.06522</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shreyank N Gowda, Xinyue Hao, Gen Li, Laura Sevilla-Lara, Shashank Narayana Gowda</li>
<li>for: 这篇论文旨在探讨深度学习模型在不同GPU上的电力消耗和精度之间的贸易，并提出一个电力消耗 penalty 来衡量模型的环境影响。</li>
<li>methods: 本研究使用了多种深度学习模型，并在不同的GPU上进行了电力消耗和精度的评估。研究人员还提出了一个电力消耗衡量指标，以衡量模型在不同的GPU上的精度-电力消耗贸易。</li>
<li>results: 研究结果显示，较小的、更能效的模型可以实现较高的精度，同时也能够降低电力消耗。研究人员还发现，不同的GPU上的电力消耗和精度之间存在贸易关系。这些结果显示，可以透过优化模型的设计和训练，以减少电力消耗，并促进更公平的研究竞争。<details>
<summary>Abstract</summary>
Deep learning models have revolutionized various fields, from image recognition to natural language processing, by achieving unprecedented levels of accuracy. However, their increasing energy consumption has raised concerns about their environmental impact, disadvantaging smaller entities in research and exacerbating global energy consumption. In this paper, we explore the trade-off between model accuracy and electricity consumption, proposing a metric that penalizes large consumption of electricity. We conduct a comprehensive study on the electricity consumption of various deep learning models across different GPUs, presenting a detailed analysis of their accuracy-efficiency trade-offs. By evaluating accuracy per unit of electricity consumed, we demonstrate how smaller, more energy-efficient models can significantly expedite research while mitigating environmental concerns. Our results highlight the potential for a more sustainable approach to deep learning, emphasizing the importance of optimizing models for efficiency. This research also contributes to a more equitable research landscape, where smaller entities can compete effectively with larger counterparts. This advocates for the adoption of efficient deep learning practices to reduce electricity consumption, safeguarding the environment for future generations whilst also helping ensure a fairer competitive landscape.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Deep-Learning-for-Automatic-Detection-and-Facial-Recognition-in-Japanese-Macaques-Illuminating-Social-Networks"><a href="#Deep-Learning-for-Automatic-Detection-and-Facial-Recognition-in-Japanese-Macaques-Illuminating-Social-Networks" class="headerlink" title="Deep Learning for Automatic Detection and Facial Recognition in Japanese Macaques: Illuminating Social Networks"></a>Deep Learning for Automatic Detection and Facial Recognition in Japanese Macaques: Illuminating Social Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06489">http://arxiv.org/abs/2310.06489</a></li>
<li>repo_url: None</li>
<li>paper_authors: Julien Paulet, Axel Molina, Benjamin Beltzung, Takafumi Suzumura, Shinya Yamamoto, Cédric Sueur</li>
<li>for: 这个研究的目的是开发一种不侵入性的工具，用于检测和识别日本黑猩猩（Macaca fuscata）的面部特征，以便自动生成这种动物群体的社会网络表示。</li>
<li>methods: 这项研究使用了深度学习技术，包括物体检测和识别，以检测和识别日本黑猩猩的面部特征。研究采用的模型包括Faster-RCNN和YOLOv8n两种，并在实验中达到了82.2%的准确率和83%的准确率。</li>
<li>results: 研究初步结果显示，使用深度学习技术可以成功地检测和识别日本黑猩猩的面部特征，并生成一个可靠的社会网络表示。在实验中，研究人员创建了一个基于视频拍摄的日本黑猩猩社会网络，并与自动生成的网络进行比较，以评估其可靠性。<details>
<summary>Abstract</summary>
Individual identification plays a pivotal role in ecology and ethology, notably as a tool for complex social structures understanding. However, traditional identification methods often involve invasive physical tags and can prove both disruptive for animals and time-intensive for researchers. In recent years, the integration of deep learning in research offered new methodological perspectives through automatization of complex tasks. Harnessing object detection and recognition technologies is increasingly used by researchers to achieve identification on video footage. This study represents a preliminary exploration into the development of a non-invasive tool for face detection and individual identification of Japanese macaques (Macaca fuscata) through deep learning. The ultimate goal of this research is, using identifications done on the dataset, to automatically generate a social network representation of the studied population. The current main results are promising: (i) the creation of a Japanese macaques' face detector (Faster-RCNN model), reaching a 82.2% accuracy and (ii) the creation of an individual recognizer for K{\=o}jima island macaques population (YOLOv8n model), reaching a 83% accuracy. We also created a K{\=o}jima population social network by traditional methods, based on co-occurrences on videos. Thus, we provide a benchmark against which the automatically generated network will be assessed for reliability. These preliminary results are a testament to the potential of this innovative approach to provide the scientific community with a tool for tracking individuals and social network studies in Japanese macaques.
</details>
<details>
<summary>摘要</summary>
个体识别在生态和行为学中扮演着重要的角色，尤其是用于复杂社会结构的理解。然而，传统的识别方法 oftentimes involve侵入性的物理标签，可以对动物产生不良影响并耗费研究人员的时间。在过去的几年中，研究中的 интеграция深度学习提供了新的方法学视角。通过对视频 Footage 进行对象检测和识别技术的应用，研究人员可以实现个体识别。本研究是一项初步的探索，旨在开发一种不侵入的工具 для日本黑猩狮（Macaca fuscata）个体识别，通过深度学习。研究的最终目标是，使用在数据集上进行识别，自动生成日本黑猩狮社会网络表示。现主要结果如下：1. 创建了一个日本黑猩狮脸部检测器（Faster-RCNN模型），达到了82.2%的准确率。2. 创建了一个特定于吾岛黑猩狮population的个体识别器（YOLOv8n模型），达到了83%的准确率。3. 基于视频上的共处，创建了吾岛黑猩狮population社会网络。这些初步结果表明了这种创新的方法的潜在力量，可以为科学社区提供一种跟踪个体和社会网络研究的工具。
</details></li>
</ul>
<hr>
<h2 id="Focus-on-Local-Regions-for-Query-based-Object-Detection"><a href="#Focus-on-Local-Regions-for-Query-based-Object-Detection" class="headerlink" title="Focus on Local Regions for Query-based Object Detection"></a>Focus on Local Regions for Query-based Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06470">http://arxiv.org/abs/2310.06470</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongbin Xu, Yamei Xia, Shuai Zhao, Bo Cheng</li>
<li>for: 提高查询基于object detection的性能和速度</li>
<li>methods: 提出一种基于transformer的查询架构，增强自注意机制，采用适应采样方法和look-back策略，提高查询效果</li>
<li>results: 实验结果显示 FoLR 在查询基于object detection中达到了状态的性能和计算效率，比传统方法更快 converge 和更高效<details>
<summary>Abstract</summary>
Query-based methods have garnered significant attention in object detection since the advent of DETR, the pioneering end-to-end query-based detector. However, these methods face challenges like slow convergence and suboptimal performance. Notably, self-attention in object detection often hampers convergence due to its global focus. To address these issues, we propose FoLR, a transformer-like architecture with only decoders. We enhance the self-attention mechanism by isolating connections between irrelevant objects that makes it focus on local regions but not global regions. We also design the adaptive sampling method to extract effective features based on queries' local regions from feature maps. Additionally, we employ a look-back strategy for decoders to retain prior information, followed by the Feature Mixer module to fuse features and queries. Experimental results demonstrate FoLR's state-of-the-art performance in query-based detectors, excelling in convergence speed and computational efficiency.
</details>
<details>
<summary>摘要</summary>
干预方法在物体检测中获得了广泛关注，自DETR发明以来。然而，这些方法面临较慢的收敛速度和不佳的性能问题。尤其是对象检测中的自注意力经常干扰收敛，因为它具有全局注意力。为解决这些问题，我们提议FoLR架构，它是一种基于transformer的 Architecture，只有解码器。我们强化对象检测中的自注意力机制，隔离无关对象之间的连接，使其关注本地区域而不是全局区域。此外，我们还设计了适应采样方法，从特征图中提取有效特征，基于查询的本地区域。此外，我们还使用look-back策略，让解码器保留先前信息，然后使用特征混合模块将特征和查询混合。实验结果表明FoLR在查询基于检测器中实现了状态机器的表现，在收敛速度和计算效率方面都具有优势。
</details></li>
</ul>
<hr>
<h2 id="A-Geometrical-Approach-to-Evaluate-the-Adversarial-Robustness-of-Deep-Neural-Networks"><a href="#A-Geometrical-Approach-to-Evaluate-the-Adversarial-Robustness-of-Deep-Neural-Networks" class="headerlink" title="A Geometrical Approach to Evaluate the Adversarial Robustness of Deep Neural Networks"></a>A Geometrical Approach to Evaluate the Adversarial Robustness of Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06468">http://arxiv.org/abs/2310.06468</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yang Wang, Bo Dong, Ke Xu, Haiyin Piao, Yufei Ding, Baocai Yin, Xin Yang<br>for:* The paper aims to propose a new metric for evaluating the adversarial robustness of deep neural networks (DNNs) against different types of attacks on large-scale datasets.methods:* The proposed metric is called Adversarial Converging Time Score (ACTS), which measures the time it takes for an attacker to find an adversarial example on a specific input.* ACTS is based on the observation that the local neighborhoods on a DNN’s output surface have different shapes for different inputs, and thus the converging time to an adversarial sample will vary depending on the input.results:* The proposed ACTS metric is validated on the large-scale ImageNet dataset using state-of-the-art deep networks, and shows to be more efficient and effective than the previous CLEVER metric.* Extensive experiments demonstrate the effectiveness and generalization of ACTS against different adversarial attacks.<details>
<summary>Abstract</summary>
Deep Neural Networks (DNNs) are widely used for computer vision tasks. However, it has been shown that deep models are vulnerable to adversarial attacks, i.e., their performances drop when imperceptible perturbations are made to the original inputs, which may further degrade the following visual tasks or introduce new problems such as data and privacy security. Hence, metrics for evaluating the robustness of deep models against adversarial attacks are desired. However, previous metrics are mainly proposed for evaluating the adversarial robustness of shallow networks on the small-scale datasets. Although the Cross Lipschitz Extreme Value for nEtwork Robustness (CLEVER) metric has been proposed for large-scale datasets (e.g., the ImageNet dataset), it is computationally expensive and its performance relies on a tractable number of samples. In this paper, we propose the Adversarial Converging Time Score (ACTS), an attack-dependent metric that quantifies the adversarial robustness of a DNN on a specific input. Our key observation is that local neighborhoods on a DNN's output surface would have different shapes given different inputs. Hence, given different inputs, it requires different time for converging to an adversarial sample. Based on this geometry meaning, ACTS measures the converging time as an adversarial robustness metric. We validate the effectiveness and generalization of the proposed ACTS metric against different adversarial attacks on the large-scale ImageNet dataset using state-of-the-art deep networks. Extensive experiments show that our ACTS metric is an efficient and effective adversarial metric over the previous CLEVER metric.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Solution-for-SMART-101-Challenge-of-ICCV-Multi-modal-Algorithmic-Reasoning-Task-2023"><a href="#Solution-for-SMART-101-Challenge-of-ICCV-Multi-modal-Algorithmic-Reasoning-Task-2023" class="headerlink" title="Solution for SMART-101 Challenge of ICCV Multi-modal Algorithmic Reasoning Task 2023"></a>Solution for SMART-101 Challenge of ICCV Multi-modal Algorithmic Reasoning Task 2023</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06440">http://arxiv.org/abs/2310.06440</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiangyu Wu, Yang Yang, Shengdong Xu, Yifeng Wu, Qingguo Chen, Jianfeng Lu</li>
<li>for: This paper presents a solution to the Multi-modal Algorithmic Reasoning Task: SMART-101 Challenge, which evaluates the ability of neural networks to solve visuolinguistic puzzles for children aged 6-8.</li>
<li>methods: The authors employed a divide-and-conquer approach, categorizing questions into eight types and using the llama-2-chat model to generate question types in a zero-shot manner. They also trained a yolov7 model on the icon45 dataset for object detection and combined it with OCR to recognize objects and text within images. Additionally, they used the BLIP-2 model with eight adapters to adaptively extract visual features for different question types.</li>
<li>results: Under the puzzle splits configuration, the authors achieved an accuracy score of 26.5 on the validation set and 24.30 on the private test set.<details>
<summary>Abstract</summary>
In this paper, we present our solution to a Multi-modal Algorithmic Reasoning Task: SMART-101 Challenge. Different from the traditional visual question-answering datasets, this challenge evaluates the abstraction, deduction, and generalization abilities of neural networks in solving visuolinguistic puzzles designed specifically for children in the 6-8 age group. We employed a divide-and-conquer approach. At the data level, inspired by the challenge paper, we categorized the whole questions into eight types and utilized the llama-2-chat model to directly generate the type for each question in a zero-shot manner. Additionally, we trained a yolov7 model on the icon45 dataset for object detection and combined it with the OCR method to recognize and locate objects and text within the images. At the model level, we utilized the BLIP-2 model and added eight adapters to the image encoder VIT-G to adaptively extract visual features for different question types. We fed the pre-constructed question templates as input and generated answers using the flan-t5-xxl decoder. Under the puzzle splits configuration, we achieved an accuracy score of 26.5 on the validation set and 24.30 on the private test set.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了我们对多modal算法逻辑任务（SMART-101挑战）的解决方案。与传统视觉问答数据集不同，这个挑战评估了基于图像和语言的抽象、推理和总结能力的神经网络。我们采用分治方法。在数据层次，根据挑战论文，我们将整个问题分为八种类型，并使用llama-2-chat模型直接生成每个问题的类型在零容量情况下。此外，我们在icon45数据集上训练了yolov7模型，并将其与OCR方法结合使用，以识别和定位图像中的对象和文本。在模型层次，我们使用BLIP-2模型，并添加了八个适应器到图像编码器VIT-G，以适应不同问题类型的视觉特征。我们将预构建的问题模板作为输入，并使用flan-t5-xxl解码器生成答案。在配置为谜题分割的情况下，我们在验证集上达到了26.5的准确率和24.30的私有测试集准确率。
</details></li>
</ul>
<hr>
<h2 id="The-Solution-for-the-CVPR2023-NICE-Image-Captioning-Challenge"><a href="#The-Solution-for-the-CVPR2023-NICE-Image-Captioning-Challenge" class="headerlink" title="The Solution for the CVPR2023 NICE Image Captioning Challenge"></a>The Solution for the CVPR2023 NICE Image Captioning Challenge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06879">http://arxiv.org/abs/2310.06879</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiangyu Wu, Yi Gao, Hailiang Zhang, Yang Yang, Weili Guo, Jianfeng Lu</li>
<li>for: 本研究的目标是解决新的零引入图像描述挑战，这个挑战包括许多领域的新视觉概念以及不同的图像类型（如照片、插图、图形）。</li>
<li>methods: 我们使用了Laion-5B，一个大规模的CLIP-过滤图像文本 dataset，进行数据层的采集。在模型层，我们使用了OFA，一个基于手工模板的大规模视语预训模型，来进行图像描述任务。此外，我们还引入了对比学习，以便在预训阶段学习新的视觉概念。最后，我们提出了一种相似桶策略，并将其 integrate 到模板中，以强制模型生成更高质量和更匹配的描述。</li>
<li>results: 我们的方法在验证和测试阶段分别获得了105.17和325.72的Cider-Score，位于领导者板块的第一名。<details>
<summary>Abstract</summary>
In this paper, we present our solution to the New frontiers for Zero-shot Image Captioning Challenge. Different from the traditional image captioning datasets, this challenge includes a larger new variety of visual concepts from many domains (such as COVID-19) as well as various image types (photographs, illustrations, graphics). For the data level, we collect external training data from Laion-5B, a large-scale CLIP-filtered image-text dataset. For the model level, we use OFA, a large-scale visual-language pre-training model based on handcrafted templates, to perform the image captioning task. In addition, we introduce contrastive learning to align image-text pairs to learn new visual concepts in the pre-training stage. Then, we propose a similarity-bucket strategy and incorporate this strategy into the template to force the model to generate higher quality and more matching captions. Finally, by retrieval-augmented strategy, we construct a content-rich template, containing the most relevant top-k captions from other image-text pairs, to guide the model in generating semantic-rich captions. Our method ranks first on the leaderboard, achieving 105.17 and 325.72 Cider-Score in the validation and test phase, respectively.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍我们的方法解决新前iers for Zero-shot Image Captioning Challenge。与传统的图像描述 dataset 不同，这个挑战包括更多的新领域的视觉概念（如 COVID-19）以及不同的图像类型（如照片、插图、图形）。在数据层次上，我们收集了来自 Laion-5B 的大规模 CLIP-过滤的图像文本数据集进行训练。在模型层次上，我们使用 OFA，一种基于手工模板的大规模视语预训练模型，来进行图像描述任务。此外，我们引入了对比学习，以对图像-文本对进行对齐，以学习新的视觉概念。然后，我们提出了一种相似桶策略，并将其 incorporate 到模型中，以强制模型生成更高质量和更匹配的描述。最后，我们通过 Retrieval-augmented 策略，construct 一个内容丰富的模板，包含最相关的 top-k 描述从其他图像-文本对，以导引模型生成semantic-rich的描述。我们的方法在领导板中名列第一，在验证阶段和测试阶段分别获得 105.17 和 325.72 Cider-Score。
</details></li>
</ul>
<hr>
<h2 id="Skeleton-Ground-Truth-Extraction-Methodology-Annotation-Tool-and-Benchmarks"><a href="#Skeleton-Ground-Truth-Extraction-Methodology-Annotation-Tool-and-Benchmarks" class="headerlink" title="Skeleton Ground Truth Extraction: Methodology, Annotation Tool and Benchmarks"></a>Skeleton Ground Truth Extraction: Methodology, Annotation Tool and Benchmarks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06437">http://arxiv.org/abs/2310.06437</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cong-yang/skeview">https://github.com/cong-yang/skeview</a></li>
<li>paper_authors: Cong Yang, Bipin Indurkhya, John See, Bo Gao, Yan Ke, Zeyd Boukhers, Zhenyu Yang, Marcin Grzegorzek</li>
<li>for: 这篇论文主要写于提供一种基于扩展的 диагностичность假设的方法来生成shape和图像中的skeletonGround Truth(GT)。</li>
<li>methods: 该方法基于扩展的 диагностичность假设，使用人工监督来提取skeleton GT，并开发了一个工具called SkeView来生成GT。</li>
<li>results: 实验表明，由该方法生成的GT具有较好的标准化一致性，并且提供了一个平衡 между简单性和完整性。<details>
<summary>Abstract</summary>
Skeleton Ground Truth (GT) is critical to the success of supervised skeleton extraction methods, especially with the popularity of deep learning techniques. Furthermore, we see skeleton GTs used not only for training skeleton detectors with Convolutional Neural Networks (CNN) but also for evaluating skeleton-related pruning and matching algorithms. However, most existing shape and image datasets suffer from the lack of skeleton GT and inconsistency of GT standards. As a result, it is difficult to evaluate and reproduce CNN-based skeleton detectors and algorithms on a fair basis. In this paper, we present a heuristic strategy for object skeleton GT extraction in binary shapes and natural images. Our strategy is built on an extended theory of diagnosticity hypothesis, which enables encoding human-in-the-loop GT extraction based on clues from the target's context, simplicity, and completeness. Using this strategy, we developed a tool, SkeView, to generate skeleton GT of 17 existing shape and image datasets. The GTs are then structurally evaluated with representative methods to build viable baselines for fair comparisons. Experiments demonstrate that GTs generated by our strategy yield promising quality with respect to standard consistency, and also provide a balance between simplicity and completeness.
</details>
<details>
<summary>摘要</summary>
骨架真实数据（GT）对超视觉骨架检测方法的成功非常重要，尤其是深度学习技术的普及。此外，我们发现骨架GT不仅用于训练骨架检测器采用Convolutional Neural Networks（CNN），还用于评估骨架相关的束缚和匹配算法。然而，大多数现有的形状和图像数据集受到骨架GT的缺失和GT标准的不一致的影响。这使得评估和重现基于CNN的骨架检测器和算法非常困难。在这篇论文中，我们提出了一种规则性的骨架GT抽取策略，这种策略基于扩展的诊断假设，允许基于目标Context、简洁性和完整性的人工 Loop GT抽取。使用这种策略，我们开发了一个工具，SkeView，用于生成17个现有形状和图像数据集的骨架GT。这些GT然后与代表方法进行结构性评估，以建立可靠的基准。实验表明，由我们的策略生成的GT具有对标准一致性和完整性的承诺，并提供了简洁性和完整性之间的平衡。
</details></li>
</ul>
<hr>
<h2 id="Conformal-Prediction-for-Deep-Classifier-via-Label-Ranking"><a href="#Conformal-Prediction-for-Deep-Classifier-via-Label-Ranking" class="headerlink" title="Conformal Prediction for Deep Classifier via Label Ranking"></a>Conformal Prediction for Deep Classifier via Label Ranking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06430">http://arxiv.org/abs/2310.06430</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ml-stat-Sustech/conformal_prediction_via_label_ranking">https://github.com/ml-stat-Sustech/conformal_prediction_via_label_ranking</a></li>
<li>paper_authors: Jianguo Huang, Huajun Xi, Linjun Zhang, Huaxiu Yao, Yue Qiu, Hongxin Wei</li>
<li>for: 本文是针对机器学习预测结果的重叠预测集的一种新方法，即Sorted Adaptive prediction sets（SAPS）。</li>
<li>methods: 本文提出了一种新的算法SAPS，它基于最大softmax概率值忽略概率值，以降低预测集的大小。</li>
<li>results: 经过实验和理论分析表明，SAPS可以生成较小的预测集，同时保留实例级别的uncertainty信息。此外，SAPS还可以广泛提高预测集的 conditional coverage rate和适应性。<details>
<summary>Abstract</summary>
Conformal prediction is a statistical framework that generates prediction sets containing ground-truth labels with a desired coverage guarantee. The predicted probabilities produced by machine learning models are generally miscalibrated, leading to large prediction sets in conformal prediction. In this paper, we empirically and theoretically show that disregarding the probabilities' value will mitigate the undesirable effect of miscalibrated probability values. Then, we propose a novel algorithm named $\textit{Sorted Adaptive prediction sets}$ (SAPS), which discards all the probability values except for the maximum softmax probability. The key idea behind SAPS is to minimize the dependence of the non-conformity score on the probability values while retaining the uncertainty information. In this manner, SAPS can produce sets of small size and communicate instance-wise uncertainty. Theoretically, we provide a finite-sample coverage guarantee of SAPS and show that the expected value of set size from SAPS is always smaller than APS. Extensive experiments validate that SAPS not only lessens the prediction sets but also broadly enhances the conditional coverage rate and adaptation of prediction sets.
</details>
<details>
<summary>摘要</summary>
《匹配预测》是一种统计框架，生成包含真实标签的预测集，具有所需的保障率 garantía。机器学习模型生成的预测概率通常是偏债的，导致匹配预测集较大。在这篇论文中，我们通过实验和理论分析表明，忽略概率值可以减轻不良影响。然后，我们提出一种新的算法名为《排序适应预测集》（SAPS），它丢弃所有概率值，只保留最大软max概率。SAPS的关键想法是减少非准确度评分值与概率值之间的依赖关系，以保留uncertainty信息。因此，SAPS可以生成小size的预测集，并通过实例化uncertainty进行通信。我们也提供了finite-sample coverage garantía，证明SAPS的预期集size是APS always smaller。实验证明，SAPS不仅减少预测集，还广泛提高了预测集的 conditional coverage rate和适应度。
</details></li>
</ul>
<hr>
<h2 id="AnoDODE-Anomaly-Detection-with-Diffusion-ODE"><a href="#AnoDODE-Anomaly-Detection-with-Diffusion-ODE" class="headerlink" title="AnoDODE: Anomaly Detection with Diffusion ODE"></a>AnoDODE: Anomaly Detection with Diffusion ODE</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06420">http://arxiv.org/abs/2310.06420</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xianyao Hu, Congming Jin</li>
<li>for: 这个论文主要应用在医疗影像识别异常性，以提高医疗影像诊断中的异常性检测精度。</li>
<li>methods: 本论文使用Diffusion ODEs进行不监督 anomaly detection，并且提出了一个基于构成特征的异常性评分方法和一个基于重建的异常性地图表示方法。</li>
<li>results: 这个方法在BraTS2021医疗影像资料集上进行了实验，和现有的方法相比，表现出更高的效果和可靠性。<details>
<summary>Abstract</summary>
Anomaly detection is the process of identifying atypical data samples that significantly deviate from the majority of the dataset. In the realm of clinical screening and diagnosis, detecting abnormalities in medical images holds great importance. Typically, clinical practice provides access to a vast collection of normal images, while abnormal images are relatively scarce. We hypothesize that abnormal images and their associated features tend to manifest in low-density regions of the data distribution. Following this assumption, we turn to diffusion ODEs for unsupervised anomaly detection, given their tractability and superior performance in density estimation tasks. More precisely, we propose a new anomaly detection method based on diffusion ODEs by estimating the density of features extracted from multi-scale medical images. Our anomaly scoring mechanism depends on computing the negative log-likelihood of features extracted from medical images at different scales, quantified in bits per dimension. Furthermore, we propose a reconstruction-based anomaly localization suitable for our method. Our proposed method not only identifie anomalies but also provides interpretability at both the image and pixel levels. Through experiments on the BraTS2021 medical dataset, our proposed method outperforms existing methods. These results confirm the effectiveness and robustness of our method.
</details>
<details>
<summary>摘要</summary>
“异常检测是检测数据集中异常出现的数据样本的过程。在医学检测和诊断领域，检测医学影像中的异常非常重要。通常，临床实践中有大量的常见图像，而异常图像相对较少。我们假设异常图像和其相关特征通常会出现在数据分布中的低浓度区域。基于这个假设，我们使用扩散ODE进行无监督异常检测，因为它具有优秀的散度估计能力。更具体地，我们提出了基于扩散ODE的新异常检测方法，通过计算特征EXTRACTED FROM MULTI-SCALE MEDICAL IMAGES的浓度来进行异常分数。我们的异常分数机制是根据不同级别的医学图像特征EXTRACTED FROM MULTI-SCALE MEDICAL IMAGES的负极值log-likelihood来计算的。此外，我们还提出了基于重建的异常地图方法，适用于我们的方法。我们的提议方法不仅可以检测异常，还可以在图像和像素水平提供可读性。通过对BraTS2021医学数据集进行实验，我们的提议方法超越现有方法。这些结果证明了我们的方法的有效性和稳定性。”
</details></li>
</ul>
<hr>
<h2 id="Boundary-Discretization-and-Reliable-Classification-Network-for-Temporal-Action-Detection"><a href="#Boundary-Discretization-and-Reliable-Classification-Network-for-Temporal-Action-Detection" class="headerlink" title="Boundary Discretization and Reliable Classification Network for Temporal Action Detection"></a>Boundary Discretization and Reliable Classification Network for Temporal Action Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06403">http://arxiv.org/abs/2310.06403</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhenyingfang/BDRC-Net">https://github.com/zhenyingfang/BDRC-Net</a></li>
<li>paper_authors: Zhenying Fang</li>
<li>for: 本研究旨在提高无trim的视频中的时间动作检测性能，并提出一种新的Boundary Discretization and Reliable Classification Network（BDRC-Net）方法来解决混合方法中的两大问题。</li>
<li>methods: 本研究使用的方法包括Boundary Discretization Module（BDM）和Reliable Classification Module（RCM），它们可以减少手动设计的锚点和提高时间动作检测的精度。</li>
<li>results: 实验结果表明，BDRC-Net在不同的benchmark上达到了state-of-the-art的性能，比如THUMOS’14上的平均map值为68.6%，比前一个最佳方法高1.5%。<details>
<summary>Abstract</summary>
Temporal action detection aims to recognize the action category and determine the starting and ending time of each action instance in untrimmed videos. The mixed methods have achieved remarkable performance by simply merging anchor-based and anchor-free approaches. However, there are still two crucial issues in the mixed framework: (1) Brute-force merging and handcrafted anchors design affect the performance and practical application of the mixed methods. (2) A large number of false positives in action category predictions further impact the detection performance. In this paper, we propose a novel Boundary Discretization and Reliable Classification Network (BDRC-Net) that addresses the above issues by introducing boundary discretization and reliable classification modules. Specifically, the boundary discretization module (BDM) elegantly merges anchor-based and anchor-free approaches in the form of boundary discretization, avoiding the handcrafted anchors design required by traditional mixed methods. Furthermore, the reliable classification module (RCM) predicts reliable action categories to reduce false positives in action category predictions. Extensive experiments conducted on different benchmarks demonstrate that our proposed method achieves favorable performance compared with the state-of-the-art. For example, BDRC-Net hits an average mAP of 68.6% on THUMOS'14, outperforming the previous best by 1.5%. The code will be released at https://github.com/zhenyingfang/BDRC-Net.
</details>
<details>
<summary>摘要</summary>
temporal action detection targets recognizing action categories and determining the starting and ending times of each action instance in untrimmed videos. 混合方法已经实现了很好的表现，但是还有两个关键问题在混合框架中：（1） brutal merging 和手工设计的锚点会影响混合方法的性能和实际应用。（2）大量的False Positives在动作类别预测中，进一步影响检测性能。在这篇论文中，我们提出了一种新的边界精炼和可靠分类网络（BDRC-Net），该方法可以解决以上问题。 Specifically, the boundary discretization module (BDM) elegantly merges anchor-based and anchor-free approaches in the form of boundary discretization, avoiding the handcrafted anchors design required by traditional mixed methods. 其次，可靠分类模块 (RCM) 预测可靠的动作类别，以减少False Positives在动作类别预测中。 经过对不同benchmark的广泛实验，我们的提出的方法可以达到与当前最佳的表现。例如，BDRC-Net在THUMOS'14上取得了68.6%的平均MAP，比前一个最佳的方法高1.5%. 代码将在https://github.com/zhenyingfang/BDRC-Net 上发布。
</details></li>
</ul>
<hr>
<h2 id="Learning-Stackable-and-Skippable-LEGO-Bricks-for-Efficient-Reconfigurable-and-Variable-Resolution-Diffusion-Modeling"><a href="#Learning-Stackable-and-Skippable-LEGO-Bricks-for-Efficient-Reconfigurable-and-Variable-Resolution-Diffusion-Modeling" class="headerlink" title="Learning Stackable and Skippable LEGO Bricks for Efficient, Reconfigurable, and Variable-Resolution Diffusion Modeling"></a>Learning Stackable and Skippable LEGO Bricks for Efficient, Reconfigurable, and Variable-Resolution Diffusion Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06389">http://arxiv.org/abs/2310.06389</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huangjie Zheng, Zhendong Wang, Jianbo Yuan, Guanghan Ning, Pengcheng He, Quanzeng You, Hongxia Yang, Mingyuan Zhou</li>
<li>For: This paper aims to improve the efficiency and adaptability of diffusion models for image generation by introducing a new network architecture called LEGO bricks.* Methods: The LEGO bricks architecture integrates Local-feature Enrichment and Global-content Orchestration, allowing for selective skipping of bricks to reduce sampling costs and generate higher-resolution images.* Results: The proposed method enhances training efficiency, expedites convergence, and facilitates variable-resolution image generation while maintaining strong generative performance, and significantly reduces sampling time compared to other methods.<details>
<summary>Abstract</summary>
Diffusion models excel at generating photo-realistic images but come with significant computational costs in both training and sampling. While various techniques address these computational challenges, a less-explored issue is designing an efficient and adaptable network backbone for iterative refinement. Current options like U-Net and Vision Transformer often rely on resource-intensive deep networks and lack the flexibility needed for generating images at variable resolutions or with a smaller network than used in training. This study introduces LEGO bricks, which seamlessly integrate Local-feature Enrichment and Global-content Orchestration. These bricks can be stacked to create a test-time reconfigurable diffusion backbone, allowing selective skipping of bricks to reduce sampling costs and generate higher-resolution images than the training data. LEGO bricks enrich local regions with an MLP and transform them using a Transformer block while maintaining a consistent full-resolution image across all bricks. Experimental results demonstrate that LEGO bricks enhance training efficiency, expedite convergence, and facilitate variable-resolution image generation while maintaining strong generative performance. Moreover, LEGO significantly reduces sampling time compared to other methods, establishing it as a valuable enhancement for diffusion models.
</details>
<details>
<summary>摘要</summary>
Diffusion models excel at generating photo-realistic images but come with significant computational costs in both training and sampling. While various techniques address these computational challenges, a less-explored issue is designing an efficient and adaptable network backbone for iterative refinement. Current options like U-Net and Vision Transformer often rely on resource-intensive deep networks and lack the flexibility needed for generating images at variable resolutions or with a smaller network than used in training. This study introduces LEGO bricks, which seamlessly integrate Local-feature Enrichment and Global-content Orchestration. These bricks can be stacked to create a test-time reconfigurable diffusion backbone, allowing selective skipping of bricks to reduce sampling costs and generate higher-resolution images than the training data. LEGO bricks enrich local regions with an MLP and transform them using a Transformer block while maintaining a consistent full-resolution image across all bricks. Experimental results demonstrate that LEGO bricks enhance training efficiency, expedite convergence, and facilitate variable-resolution image generation while maintaining strong generative performance. Moreover, LEGO significantly reduces sampling time compared to other methods, establishing it as a valuable enhancement for diffusion models.Here is the translation in Traditional Chinese:Diffusion models excel at generating photo-realistic images but come with significant computational costs in both training and sampling. While various techniques address these computational challenges, a less-explored issue is designing an efficient and adaptable network backbone for iterative refinement. Current options like U-Net and Vision Transformer often rely on resource-intensive deep networks and lack the flexibility needed for generating images at variable resolutions or with a smaller network than used in training. This study introduces LEGO bricks, which seamlessly integrate Local-feature Enrichment and Global-content Orchestration. These bricks can be stacked to create a test-time reconfigurable diffusion backbone, allowing selective skipping of bricks to reduce sampling costs and generate higher-resolution images than the training data. LEGO bricks enrich local regions with an MLP and transform them using a Transformer block while maintaining a consistent full-resolution image across all bricks. Experimental results demonstrate that LEGO bricks enhance training efficiency, expedite convergence, and facilitate variable-resolution image generation while maintaining strong generative performance. Moreover, LEGO significantly reduces sampling time compared to other methods, establishing it as a valuable enhancement for diffusion models.
</details></li>
</ul>
<hr>
<h2 id="3DS-SLAM-A-3D-Object-Detection-based-Semantic-SLAM-towards-Dynamic-Indoor-Environments"><a href="#3DS-SLAM-A-3D-Object-Detection-based-Semantic-SLAM-towards-Dynamic-Indoor-Environments" class="headerlink" title="3DS-SLAM: A 3D Object Detection based Semantic SLAM towards Dynamic Indoor Environments"></a>3DS-SLAM: A 3D Object Detection based Semantic SLAM towards Dynamic Indoor Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06385">http://arxiv.org/abs/2310.06385</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ghanta Sai Krishna, Kundrapu Supriya, Sabur Baidya<br>for: 这个研究该纸是为了解决受环境变化影响的摄像头位置精度下降问题，并提出了一个3D Semantic SLAM算法（3DS-SLAM），可以在动态环境中提供高精度的位置识别。methods: 这个研究使用了3D part-aware hybrid transformer来检测Point cloud中的动态物件，并提出了一个基于HDBSCAN散度分析的动态特征范围筛选器，以提高位置精度。results: 该研究与ORB-SLAM2进行比较，发现3DS-SLAM在TUM RGB-D dataset上的动态序列中平均提高98.01%。此外，它还超过了其他四个适用于动态环境的主要SLAM系统的性能。<details>
<summary>Abstract</summary>
The existence of variable factors within the environment can cause a decline in camera localization accuracy, as it violates the fundamental assumption of a static environment in Simultaneous Localization and Mapping (SLAM) algorithms. Recent semantic SLAM systems towards dynamic environments either rely solely on 2D semantic information, or solely on geometric information, or combine their results in a loosely integrated manner. In this research paper, we introduce 3DS-SLAM, 3D Semantic SLAM, tailored for dynamic scenes with visual 3D object detection. The 3DS-SLAM is a tightly-coupled algorithm resolving both semantic and geometric constraints sequentially. We designed a 3D part-aware hybrid transformer for point cloud-based object detection to identify dynamic objects. Subsequently, we propose a dynamic feature filter based on HDBSCAN clustering to extract objects with significant absolute depth differences. When compared against ORB-SLAM2, 3DS-SLAM exhibits an average improvement of 98.01% across the dynamic sequences of the TUM RGB-D dataset. Furthermore, it surpasses the performance of the other four leading SLAM systems designed for dynamic environments.
</details>
<details>
<summary>摘要</summary>
环境中变量因素的存在可能导致摄像头地理位置准确性下降，因为这违背了同时地理位置和地图生成（SLAM）算法的基本假设，即静止环境。现代 semantic SLAM 系统在动态环境中通常仅仅依靠2D semantic信息，或者仅仅依靠几何信息，或者将其结果在粗略地集成起来。在这项研究中，我们介绍了3DS-SLAM，即3D Semantic SLAM，适用于动态场景中的视觉3D对象检测。3DS-SLAM 是一种紧密相关的算法，同时解决 semantic 和几何约束。我们设计了一种3D part-aware 混合变换器，用于基于点云的对象检测，以确定动态对象。然后，我们提出了基于 HDBSCAN 聚类的动态特征筛选器，以提取对象具有重要绝对深度差异。与 ORB-SLAM2 相比，3DS-SLAM 在 TUM RGB-D 数据集的动态序列上显示了平均提高98.01%。此外，它还超过了其他四个领先的 SLAM 系统在动态环境中的性能。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Diffusion-Based-Image-Variations-for-Robust-Training-on-Poisoned-Data"><a href="#Leveraging-Diffusion-Based-Image-Variations-for-Robust-Training-on-Poisoned-Data" class="headerlink" title="Leveraging Diffusion-Based Image Variations for Robust Training on Poisoned Data"></a>Leveraging Diffusion-Based Image Variations for Robust Training on Poisoned Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06372">http://arxiv.org/abs/2310.06372</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lukasstruppek/robust_training_on_poisoned_samples">https://github.com/lukasstruppek/robust_training_on_poisoned_samples</a></li>
<li>paper_authors: Lukas Struppek, Martin B. Hentschel, Clifton Poth, Dominik Hintersdorf, Kristian Kersting</li>
<li>for: 防止训练神经网络时遭受后门攻击，即在模型中隐藏功能的攻击。</li>
<li>methods: 使用 diffusion models 创建所有训练样本的同源变种，并通过知识储存来帮助学生模型具有抗胁袋功能。</li>
<li>results: 通过这种方法，可以培养一些抗胁袋的学生模型，它们可以在各种诱导patterns中保持一致性，并且对于潜在的后门攻击予以抵抗。<details>
<summary>Abstract</summary>
Backdoor attacks pose a serious security threat for training neural networks as they surreptitiously introduce hidden functionalities into a model. Such backdoors remain silent during inference on clean inputs, evading detection due to inconspicuous behavior. However, once a specific trigger pattern appears in the input data, the backdoor activates, causing the model to execute its concealed function. Detecting such poisoned samples within vast datasets is virtually impossible through manual inspection. To address this challenge, we propose a novel approach that enables model training on potentially poisoned datasets by utilizing the power of recent diffusion models. Specifically, we create synthetic variations of all training samples, leveraging the inherent resilience of diffusion models to potential trigger patterns in the data. By combining this generative approach with knowledge distillation, we produce student models that maintain their general performance on the task while exhibiting robust resistance to backdoor triggers.
</details>
<details>
<summary>摘要</summary>
<<SYS>>对于训练神经网络来说，后门攻击是一种严重的安全隐患，因为它们隐藏式地添加了模型中的隐藏功能。这些后门在潜在的输入数据上做出不寻常的响应，但是在正常的输入数据上则保持沉默，因此难以检测。但是，当特定的触发模式出现在输入数据中时，后门会被激活，使模型执行隐藏的功能。手动检查 vast 数据集中的恶意样本是不可能的，因此我们需要一种新的方法来解决这个挑战。我们提议一种使用最新的扩散模型的方法，通过生成所有训练样本的同义词来实现。通过将这种生成方法与知识储存相结合，我们可以生成学生模型，这些模型在任务上保持普通的表现，同时具有对后门触发器的Robust性。Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="CoinSeg-Contrast-Inter-and-Intra-Class-Representations-for-Incremental-Segmentation"><a href="#CoinSeg-Contrast-Inter-and-Intra-Class-Representations-for-Incremental-Segmentation" class="headerlink" title="CoinSeg: Contrast Inter- and Intra- Class Representations for Incremental Segmentation"></a>CoinSeg: Contrast Inter- and Intra- Class Representations for Incremental Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06368">http://arxiv.org/abs/2310.06368</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zkzhang98/coinseg">https://github.com/zkzhang98/coinseg</a></li>
<li>paper_authors: Zekang Zhang, Guangyu Gao, Jianbo Jiao, Chi Harold Liu, Yunchao Wei</li>
<li>for: 这篇论文的目的是提出一种增量semantic segmentation方法，以保持模型的稳定性和适应新的概念，并且优化各类别的表现。</li>
<li>methods: 这篇论文提出了一种名为CoinSeg的方法，它强调模型的弹性，通过维持多元的内部多标本中心来提高内部多标本的多标本性。CoinSeg使用面 Proposition来识别具有强 objectness 的区域，并将这些面 Proposition 用于对比表示，以增强内部多标本的多标本性。此外，CoinSeg 还使用 category-level pseudo-labels 来提高category-level的一致性和多标本性。</li>
<li>results: 根据 Pascal VOC 2012 和 ADE20K  dataset 的多个增量情况进行验证，CoinSeg 能够实现Superior 的结果，特别是在更加具体和实际的长期情况下。<details>
<summary>Abstract</summary>
Class incremental semantic segmentation aims to strike a balance between the model's stability and plasticity by maintaining old knowledge while adapting to new concepts. However, most state-of-the-art methods use the freeze strategy for stability, which compromises the model's plasticity.In contrast, releasing parameter training for plasticity could lead to the best performance for all categories, but this requires discriminative feature representation.Therefore, we prioritize the model's plasticity and propose the Contrast inter- and intra-class representations for Incremental Segmentation (CoinSeg), which pursues discriminative representations for flexible parameter tuning. Inspired by the Gaussian mixture model that samples from a mixture of Gaussian distributions, CoinSeg emphasizes intra-class diversity with multiple contrastive representation centroids. Specifically, we use mask proposals to identify regions with strong objectness that are likely to be diverse instances/centroids of a category. These mask proposals are then used for contrastive representations to reinforce intra-class diversity. Meanwhile, to avoid bias from intra-class diversity, we also apply category-level pseudo-labels to enhance category-level consistency and inter-category diversity. Additionally, CoinSeg ensures the model's stability and alleviates forgetting through a specific flexible tuning strategy. We validate CoinSeg on Pascal VOC 2012 and ADE20K datasets with multiple incremental scenarios and achieve superior results compared to previous state-of-the-art methods, especially in more challenging and realistic long-term scenarios. Code is available at https://github.com/zkzhang98/CoinSeg.
</details>
<details>
<summary>摘要</summary>
“ classe 增量 semantics 分类目标是保持模型的稳定性和пластично性，而不是使用固定策略，这会牺牲模型的пластично性。然而，大多数当前的方法使用固定策略来保持模型的稳定性，这会限制模型的发展。相反，释放参数训练可以实现最佳性能，但需要特征表示的启发。因此，我们强调模型的пластично性，并提出了对增量分类（CoinSeg），它寻求特征表示，以便灵活地调整参数。受 Gaussian mixture model 的启发，CoinSeg 强调内类多样性，并使用面部提案来确定强度具有 Objectness 的区域，这些区域可能是多个类别的多样性中心。然后，我们使用这些面部提案进行对比表示，以强调内类多样性。同时，为了避免因内类多样性而导致的偏见，我们还使用类别水平的 Pseudo-labels 来增强类别水平一致性和 между类多样性。此外，CoinSeg 确保模型的稳定性，并避免忘记，通过特定的灵活调整策略。我们在 Pascal VOC 2012 和 ADE20K 数据集上进行了多种增量场景的验证，并实现了较前一代方法更好的结果，特别是在更加复杂和实际上更加挑战的长期场景中。代码可以在 GitHub 上找到：https://github.com/zkzhang98/CoinSeg。”
</details></li>
</ul>
<hr>
<h2 id="JointNet-Extending-Text-to-Image-Diffusion-for-Dense-Distribution-Modeling"><a href="#JointNet-Extending-Text-to-Image-Diffusion-for-Dense-Distribution-Modeling" class="headerlink" title="JointNet: Extending Text-to-Image Diffusion for Dense Distribution Modeling"></a>JointNet: Extending Text-to-Image Diffusion for Dense Distribution Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06347">http://arxiv.org/abs/2310.06347</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingyang Zhang, Shiwei Li, Yuanxun Lu, Tian Fang, David McKinnon, Yanghai Tsin, Long Quan, Yao Yao</li>
<li>for: 本研究旨在提出一种基于神经网络的 JOINTNET 模型，用于模型图像和附加的稠密属性（如深度地图）的联合分布。</li>
<li>methods: JOINTNET 基于预训练的文本到图像扩散模型，其中一份原始网络被复制并与RGB分支密集连接。RGB分支在网络细化训练中被锁定，使得新的属性分布学习得到高效地进行，而不会影响大规模预训练扩散模型的强大通用能力。</li>
<li>results: 通过RGBD扩散为例，我们证明 JOINTNET 的有效性，并通过广泛的实验表明它在多种应用中具有广泛的应用前景，包括共同RGBD生成、精度深度预测、深度条件图像生成和协调瓦当3D环境生成。<details>
<summary>Abstract</summary>
We introduce JointNet, a novel neural network architecture for modeling the joint distribution of images and an additional dense modality (e.g., depth maps). JointNet is extended from a pre-trained text-to-image diffusion model, where a copy of the original network is created for the new dense modality branch and is densely connected with the RGB branch. The RGB branch is locked during network fine-tuning, which enables efficient learning of the new modality distribution while maintaining the strong generalization ability of the large-scale pre-trained diffusion model. We demonstrate the effectiveness of JointNet by using RGBD diffusion as an example and through extensive experiments, showcasing its applicability in a variety of applications, including joint RGBD generation, dense depth prediction, depth-conditioned image generation, and coherent tile-based 3D panorama generation.
</details>
<details>
<summary>摘要</summary>
我们介绍JointNet，一个新的神经网络架构，用于模型图像和额外粗细模式（例如深度地图）的共享分布。JointNet是从预训文本图像散射模型中扩展而来，其中将原始网络复制一份，并将RGB分支与新的粗细模式分支 densely connected。RGB分支在网络精细调整时被锁定，这使得对新的分布学习得到高效的学习，并维持大规模预训散射模型的强大普遍能力。我们通过使用RGBD散射来示范JointNet的有效性，并通过广泛的实验，展示其应用在多个应用中，包括共同RGBD生成、粗细深度预测、深度条件图像生成和coherent块式3Dпанора幕生成。
</details></li>
</ul>
<hr>
<h2 id="Local-Style-Awareness-of-Font-Images"><a href="#Local-Style-Awareness-of-Font-Images" class="headerlink" title="Local Style Awareness of Font Images"></a>Local Style Awareness of Font Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06337">http://arxiv.org/abs/2310.06337</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rramatchandran/big-o-performance-java">https://github.com/rramatchandran/big-o-performance-java</a></li>
<li>paper_authors: Daichi Haraguchi, Seiichi Uchida</li>
<li>for: 本研究旨在提出一种自动注意力机制，用于找到字体的重要地方。</li>
<li>methods: 该机制可以通过 quasi-self-supervised 的方式进行训练，不需要手动标注。</li>
<li>results: 经过训练后，该机制可以准确地找到字体的重要地方，并且可以用于实现本地风格化字体生成。<details>
<summary>Abstract</summary>
When we compare fonts, we often pay attention to styles of local parts, such as serifs and curvatures. This paper proposes an attention mechanism to find important local parts. The local parts with larger attention are then considered important. The proposed mechanism can be trained in a quasi-self-supervised manner that requires no manual annotation other than knowing that a set of character images is from the same font, such as Helvetica. After confirming that the trained attention mechanism can find style-relevant local parts, we utilize the resulting attention for local style-aware font generation. Specifically, we design a new reconstruction loss function to put more weight on the local parts with larger attention for generating character images with more accurate style realization. This loss function has the merit of applicability to various font generation models. Our experimental results show that the proposed loss function improves the quality of generated character images by several few-shot font generation models.
</details>
<details>
<summary>摘要</summary>
当我们比较字体时，我们经常关注当地部分的风格，如补别线和弯曲。这篇论文提议了一种注意机制，用于发现重要的当地部分。这些部分的注意程度较大的地方被视为重要。我们的提议可以在 quasi-自我超级vised 的方式进行训练，不需要手动标注，只需要知道一组字体图像是从同一种字体，如Helvetica。我们验证了训练的注意机制可以找到风格相关的当地部分。然后，我们利用这种注意来实现本地风格感知字体生成。我们设计了一个新的重建损失函数，将更多的权重分配给具有更大注意的当地部分，以生成更加准确的风格实现的字体图像。这种损失函数具有可应用于不同的字体生成模型的优点。我们的实验结果表明，我们的提议可以提高几个少量的字体生成模型生成的字体图像质量。
</details></li>
</ul>
<hr>
<h2 id="CrowdRec-3D-Crowd-Reconstruction-from-Single-Color-Images"><a href="#CrowdRec-3D-Crowd-Reconstruction-from-Single-Color-Images" class="headerlink" title="CrowdRec: 3D Crowd Reconstruction from Single Color Images"></a>CrowdRec: 3D Crowd Reconstruction from Single Color Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06332">http://arxiv.org/abs/2310.06332</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/boycehbz/crowdrec">https://github.com/boycehbz/crowdrec</a></li>
<li>paper_authors: Buzhen Huang, Jingyi Ju, Yangang Wang</li>
<li>for: 提高大规模人群图像中人体3D重建的性能，因为现有的多人三角形重建方法难以在人群中具有满意的表现。</li>
<li>methods: 利用人群特征，提出了一种人群偏好权重优化方法，以提高基于单个人物的 mesh 恢复网络在人群图像中的表现。</li>
<li>results: 通过对单个人物网络参数进行人群约束优化，可以从大规模人群图像中获得高精度的体姿坐标和人物形状，而无需使用大规模的3D人群数据集进行训练。<details>
<summary>Abstract</summary>
This is a technical report for the GigaCrowd challenge. Reconstructing 3D crowds from monocular images is a challenging problem due to mutual occlusions, server depth ambiguity, and complex spatial distribution. Since no large-scale 3D crowd dataset can be used to train a robust model, the current multi-person mesh recovery methods can hardly achieve satisfactory performance in crowded scenes. In this paper, we exploit the crowd features and propose a crowd-constrained optimization to improve the common single-person method on crowd images. To avoid scale variations, we first detect human bounding-boxes and 2D poses from the original images with off-the-shelf detectors. Then, we train a single-person mesh recovery network using existing in-the-wild image datasets. To promote a more reasonable spatial distribution, we further propose a crowd constraint to refine the single-person network parameters. With the optimization, we can obtain accurate body poses and shapes with reasonable absolute positions from a large-scale crowd image using a single-person backbone. The code will be publicly available at~\url{https://github.com/boycehbz/CrowdRec}.
</details>
<details>
<summary>摘要</summary>
这是一份技术报告，描述了一种用于解决大规模人群3D重建问题的方法。由于人群中存在互相遮挡、深度异常和复杂的空间分布，使得现有的大规模3D人群数据集不能够用于训练一个可靠的模型。在这篇论文中，我们利用人群特征，并提出了一种人群约束优化方法，以提高现有的单人网络在人群图像上的性能。首先，我们使用商业可用的检测器检测人群中的人脸框和2D姿势。然后，我们使用现有的宽泛采集的图像数据集训练单人网络。为了提高人群中每个人的姿势和形状的可理性，我们还提出了人群约束来细化单人网络参数。通过优化，我们可以从大规模人群图像中获得准确的身体姿势和形状，并且可以使用单人框架来实现。代码将在 GitHub 上公开。
</details></li>
</ul>
<hr>
<h2 id="Precise-Payload-Delivery-via-Unmanned-Aerial-Vehicles-An-Approach-Using-Object-Detection-Algorithms"><a href="#Precise-Payload-Delivery-via-Unmanned-Aerial-Vehicles-An-Approach-Using-Object-Detection-Algorithms" class="headerlink" title="Precise Payload Delivery via Unmanned Aerial Vehicles: An Approach Using Object Detection Algorithms"></a>Precise Payload Delivery via Unmanned Aerial Vehicles: An Approach Using Object Detection Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06329">http://arxiv.org/abs/2310.06329</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aditya Vadduri, Anagh Benjwal, Abhishek Pai, Elkan Quadros, Aniruddh Kammar, Prajwal Uday</li>
<li>for: 提高自动 payload 交付精度，适用于无人机 payload 交付任务。</li>
<li>methods: 使用深度学习计算机视觉方法，将 UAV 精准对齐到目标标记位置。</li>
<li>results: 比传统 GPS 方法提高了500%的平均水平精度。<details>
<summary>Abstract</summary>
Recent years have seen tremendous advancements in the area of autonomous payload delivery via unmanned aerial vehicles, or drones. However, most of these works involve delivering the payload at a predetermined location using its GPS coordinates. By relying on GPS coordinates for navigation, the precision of payload delivery is restricted to the accuracy of the GPS network and the availability and strength of the GPS connection, which may be severely restricted by the weather condition at the time and place of operation. In this work we describe the development of a micro-class UAV and propose a novel navigation method that improves the accuracy of conventional navigation methods by incorporating a deep-learning-based computer vision approach to identify and precisely align the UAV with a target marked at the payload delivery position. This proposed method achieves a 500% increase in average horizontal precision over conventional GPS-based approaches.
</details>
<details>
<summary>摘要</summary>
近年来，无人飞行器（UAV）上的自动货物发送技术有了很大的进步。然而，大多数这些工作都是通过UAV的GPS坐标来定位和发送货物的。使用GPS坐标导航限制了货物发送精度，它们取决于GPS网络的准确性和可用性，以及在运行时的天气情况。在这篇文章中，我们描述了一种微型UAV的开发和一种基于深度学习计算机视觉技术的新导航方法，该方法可以提高传统导航方法的精度。这种方法在平均水平精度方面提高了500%。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Masked-Image-Inpainting-for-Robust-Detection-of-Mpox-and-Non-Mpox"><a href="#Adversarial-Masked-Image-Inpainting-for-Robust-Detection-of-Mpox-and-Non-Mpox" class="headerlink" title="Adversarial Masked Image Inpainting for Robust Detection of Mpox and Non-Mpox"></a>Adversarial Masked Image Inpainting for Robust Detection of Mpox and Non-Mpox</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06318">http://arxiv.org/abs/2310.06318</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yubiao Yue, Zhenzhang Li</li>
<li>for: 本研究旨在提出一种基于生成模型的MPox检测方法，以提高MPox检测精度和可靠性。</li>
<li>methods: 该方法基于生成对抗网络（GAN），通过填充mask的MPox图像来学习MPox图像表示。然后，通过比较填充后的图像和原始图像之间的相似性来判断输入图像是MPox还是非MPox。</li>
<li>results: 实验结果表明，使用MSLD数据集和 eighteen种非MPox皮肤病图像进行验证，MIM方法的平均AUC得分为0.8237。此外，研究还证明了传统分类模型的缺陷和MIM方法的优势，并通过临床验证证明了MIM方法的可靠性。最后，我们还开发了一款免费在受影响地区提供的在线手机应用程序，以便为affected areas提供免费测试。<details>
<summary>Abstract</summary>
Due to the lack of efficient mpox diagnostic technology, mpox cases continue to increase. Recently, the great potential of deep learning models in detecting mpox and non-mpox has been proven. However, existing models learn image representations via image classification, which results in they may be easily susceptible to interference from real-world noise, require diverse non-mpox images, and fail to detect abnormal input. These drawbacks make classification models inapplicable in real-world settings. To address these challenges, we propose "Mask, Inpainting, and Measure" (MIM). In MIM's pipeline, a generative adversarial network only learns mpox image representations by inpainting the masked mpox images. Then, MIM determines whether the input belongs to mpox by measuring the similarity between the inpainted image and the original image. The underlying intuition is that since MIM solely models mpox images, it struggles to accurately inpaint non-mpox images in real-world settings. Without utilizing any non-mpox images, MIM cleverly detects mpox and non-mpox and can handle abnormal inputs. We used the recognized mpox dataset (MSLD) and images of eighteen non-mpox skin diseases to verify the effectiveness and robustness of MIM. Experimental results show that the average AUROC of MIM achieves 0.8237. In addition, we demonstrated the drawbacks of classification models and buttressed the potential of MIM through clinical validation. Finally, we developed an online smartphone app to provide free testing to the public in affected areas. This work first employs generative models to improve mpox detection and provides new insights into binary decision-making tasks in medical images.
</details>
<details>
<summary>摘要</summary>
中文翻译：由于缺乏高效的MPOX诊断技术，MPOX患者数继续增加。在深度学习模型中，MPOX和非MPOX的潜在优势已经被证明。然而，现有模型通过图像分类学习图像表示，这可能会受到实际噪声的干扰，需要多种非MPOX图像，并且无法检测异常输入。这些缺陷使得分类模型在实际设置中不可用。为了解决这些挑战，我们提出了“面罩、填充和测量”（MIM）。在MIM的管道中，一个生成对抗网络只学习MPOX图像表示，并通过填充面罩的MPOX图像来确定输入是MPOX还是非MPOX。我们的 intuition 是，由于MIM只学习MPOX图像，因此在实际设置中很难准确地填充非MPOX图像。无需使用任何非MPOX图像，MIM才能够精准地检测MPOX和非MPOX，并可以处理异常输入。我们使用认可的MPOX数据集（MSLD）和 eighteen 种非MPOX皮肤病图像来验证MIM的效果和稳定性。实验结果显示，MIM的平均 AUROC 为 0.8237。此外，我们还证明了分类模型的缺陷，并赞赏了MIM的潜在价值。最后，我们开发了一款基于在线智能手机应用程序，以提供免费测试，并在受到影响的地区提供测试。这种工作首次运用生成模型改进MPOX检测，并提供了医疗图像中二分类决策的新视角。
</details></li>
</ul>
<hr>
<h2 id="Advancing-Pose-Guided-Image-Synthesis-with-Progressive-Conditional-Diffusion-Models"><a href="#Advancing-Pose-Guided-Image-Synthesis-with-Progressive-Conditional-Diffusion-Models" class="headerlink" title="Advancing Pose-Guided Image Synthesis with Progressive Conditional Diffusion Models"></a>Advancing Pose-Guided Image Synthesis with Progressive Conditional Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06313">http://arxiv.org/abs/2310.06313</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/muzishen/pcdms">https://github.com/muzishen/pcdms</a></li>
<li>paper_authors: Fei Shen, Hu Ye, Jun Zhang, Cong Wang, Xiao Han, Wei Yang</li>
<li>for: 本研究的目的是提出一种基于扩散模型的高质量、高准确性人像合成方法，能够在不同的姿势下进行 pose-guided 合成。</li>
<li>methods: 本研究使用的方法包括：1) global feature prediction，2) dense correspondence establishment，3) inpainting conditional diffusion，4) refining conditional diffusion。</li>
<li>results: 研究结果表明，提出的 Progressive Conditional Diffusion Models (PCDMs) 可以在不同的姿势下进行高质量、高准确性的人像合成，并且在挑战性的情况下保持图像的自然性和细节。<details>
<summary>Abstract</summary>
Recent work has showcased the significant potential of diffusion models in pose-guided person image synthesis. However, owing to the inconsistency in pose between the source and target images, synthesizing an image with a distinct pose, relying exclusively on the source image and target pose information, remains a formidable challenge. This paper presents Progressive Conditional Diffusion Models (PCDMs) that incrementally bridge the gap between person images under the target and source poses through three stages. Specifically, in the first stage, we design a simple prior conditional diffusion model that predicts the global features of the target image by mining the global alignment relationship between pose coordinates and image appearance. Then, the second stage establishes a dense correspondence between the source and target images using the global features from the previous stage, and an inpainting conditional diffusion model is proposed to further align and enhance the contextual features, generating a coarse-grained person image. In the third stage, we propose a refining conditional diffusion model to utilize the coarsely generated image from the previous stage as a condition, achieving texture restoration and enhancing fine-detail consistency. The three-stage PCDMs work progressively to generate the final high-quality and high-fidelity synthesized image. Both qualitative and quantitative results demonstrate the consistency and photorealism of our proposed PCDMs under challenging scenarios.The code and model will be available at https://github.com/muzishen/PCDMs.
</details>
<details>
<summary>摘要</summary>
近期研究表明扩散模型在人像合成中具有重要潜力。然而，由于源图像和目标图像中人体姿势不一致，通过仅使用源图像和目标姿势信息，Synthesizing an image with a distinct pose remains a daunting challenge.这篇论文提出了逐渐进行Diffusion Models（PCDMs），通过三个阶段逐渐bridging the gap between person images under the target and source poses. Specifically, in the first stage, we design a simple prior conditional diffusion model that predicts the global features of the target image by mining the global alignment relationship between pose coordinates and image appearance. Then, the second stage establishes a dense correspondence between the source and target images using the global features from the previous stage, and an inpainting conditional diffusion model is proposed to further align and enhance the contextual features, generating a coarse-grained person image. In the third stage, we propose a refining conditional diffusion model to utilize the coarsely generated image from the previous stage as a condition, achieving texture restoration and enhancing fine-detail consistency. The three-stage PCDMs work progressively to generate the final high-quality and high-fidelity synthesized image. Both qualitative and quantitative results demonstrate the consistency and photorealism of our proposed PCDMs under challenging scenarios.The code and model will be available at https://github.com/muzishen/PCDMs.
</details></li>
</ul>
<hr>
<h2 id="Improving-Compositional-Text-to-image-Generation-with-Large-Vision-Language-Models"><a href="#Improving-Compositional-Text-to-image-Generation-with-Large-Vision-Language-Models" class="headerlink" title="Improving Compositional Text-to-image Generation with Large Vision-Language Models"></a>Improving Compositional Text-to-image Generation with Large Vision-Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06311">http://arxiv.org/abs/2310.06311</a></li>
<li>repo_url: None</li>
<li>paper_authors: Song Wen, Guian Fang, Renrui Zhang, Peng Gao, Hao Dong, Dimitris Metaxas</li>
<li>for: 提高文本到图像模型的对齐精度，尤其是在描述多个物体、变量属性和复杂的空间关系的情况下。</li>
<li>methods: 利用大视语言模型（LVLM）进行多维评估对齐精度，并在推理阶段使用LVLM来纠正初始图像中的不一致部分，使用图像编辑算法进行修正，直到LVLM无法探测到任何不一致。</li>
<li>results: 实验结果表明，提案的方法可以显著提高文本到图像模型的对齐精度，特别是在对象数量、属性绑定、空间关系和美观质量等方面。<details>
<summary>Abstract</summary>
Recent advancements in text-to-image models, particularly diffusion models, have shown significant promise. However, compositional text-to-image models frequently encounter difficulties in generating high-quality images that accurately align with input texts describing multiple objects, variable attributes, and intricate spatial relationships. To address this limitation, we employ large vision-language models (LVLMs) for multi-dimensional assessment of the alignment between generated images and their corresponding input texts. Utilizing this assessment, we fine-tune the diffusion model to enhance its alignment capabilities. During the inference phase, an initial image is produced using the fine-tuned diffusion model. The LVLM is then employed to pinpoint areas of misalignment in the initial image, which are subsequently corrected using the image editing algorithm until no further misalignments are detected by the LVLM. The resultant image is consequently more closely aligned with the input text. Our experimental results validate that the proposed methodology significantly improves text-image alignment in compositional image generation, particularly with respect to object number, attribute binding, spatial relationships, and aesthetic quality.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Three-Dimensional-Medical-Image-Fusion-with-Deformable-Cross-Attention"><a href="#Three-Dimensional-Medical-Image-Fusion-with-Deformable-Cross-Attention" class="headerlink" title="Three-Dimensional Medical Image Fusion with Deformable Cross-Attention"></a>Three-Dimensional Medical Image Fusion with Deformable Cross-Attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06291">http://arxiv.org/abs/2310.06291</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lin Liu, Xinxin Fan, Chulong Zhang, Jingjing Dai, Yaoqin Xie, Xiaokun Liang<br>for:* 这篇论文主要是用于Multimodal医疗影像融合，以提高疾病识别和肿瘤检测。methods:* 这篇论文使用了一个创新的无监督特征相似学习融合网络，它包括一个弹性跨特征融合模组（DCFB），帮助两 modalities 分别识别自己的相似和不同之处。results:* 这篇论文使用了ADNI dataset中的3D MRI和PET影像，通过应用DCFB模组，生成了高品质的MRI-PET融合影像。* 实验结果显示，我们的方法比传统的2D影像融合方法在PSNR和SSIM等效果指标上表现更好。* 重要的是，我们的方法可以融合3D影像，增加医生和研究人员可用的信息，因此代表了这个领域的一大突破。<details>
<summary>Abstract</summary>
Multimodal medical image fusion plays an instrumental role in several areas of medical image processing, particularly in disease recognition and tumor detection. Traditional fusion methods tend to process each modality independently before combining the features and reconstructing the fusion image. However, this approach often neglects the fundamental commonalities and disparities between multimodal information. Furthermore, the prevailing methodologies are largely confined to fusing two-dimensional (2D) medical image slices, leading to a lack of contextual supervision in the fusion images and subsequently, a decreased information yield for physicians relative to three-dimensional (3D) images. In this study, we introduce an innovative unsupervised feature mutual learning fusion network designed to rectify these limitations. Our approach incorporates a Deformable Cross Feature Blend (DCFB) module that facilitates the dual modalities in discerning their respective similarities and differences. We have applied our model to the fusion of 3D MRI and PET images obtained from 660 patients in the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset. Through the application of the DCFB module, our network generates high-quality MRI-PET fusion images. Experimental results demonstrate that our method surpasses traditional 2D image fusion methods in performance metrics such as Peak Signal to Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM). Importantly, the capacity of our method to fuse 3D images enhances the information available to physicians and researchers, thus marking a significant step forward in the field. The code will soon be available online.
</details>
<details>
<summary>摘要</summary>
多Modal医疗图像融合在医学图像处理多个领域中扮演重要角色，特别是疾病识别和肿瘤检测。传统的融合方法通常是独立处理每种模式的图像，然后将特征合并并重建融合图像。然而，这种方法经常忽视不同模式之间的基本相似性和差异。此外，现有的方法ologies主要是对2D医学图像片进行融合，导致融合图像中缺乏上下文指导，从而降低了医生对融合图像中信息的获得。在本研究中，我们介绍了一种创新的无监督特征共同学习融合网络，用于解决这些限制。我们的方法包括一个可变的交叉特征混合（DCFB）模块，该模块使得两种模式能够更好地了解它们之间的相似性和差异。我们在ADNI数据集中对3D MRI和PET图像进行了融合，并通过DCFB模块，我们的网络生成了高质量的MRI-PET融合图像。实验结果表明，我们的方法在PSNR和SSIM等性能指标上比传统2D图像融合方法表现更好。这种能力融合3D图像，提高了医生和研究人员对融合图像中信息的获得，这标志着该领域的一个重要进步。网络代码即将在线上公开。
</details></li>
</ul>
<hr>
<h2 id="Towards-More-Efficient-Depression-Risk-Recognition-via-Gait"><a href="#Towards-More-Efficient-Depression-Risk-Recognition-via-Gait" class="headerlink" title="Towards More Efficient Depression Risk Recognition via Gait"></a>Towards More Efficient Depression Risk Recognition via Gait</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06283">http://arxiv.org/abs/2310.06283</a></li>
<li>repo_url: None</li>
<li>paper_authors: Min Ren, Muchan Tao, Xuecai Hu, Xiaotong Liu, Qiong Li, Yongzhen Huang<br>for: 这个研究旨在开发一种基于深度学习的抑郁风险识别模型，以便在初级医疗设置中早期识别抑郁症，避免重症和重复发作，并减轻抑郁症对情感和财务造成的负担。methods: 该研究首先构建了一个大规模的步态数据库，包括1,200名参与者、40,000个步态序列和6种视角和3种服装类型。然后，提出了一种基于深度学习的抑郁风险识别模型，以超越手工设计的方法。results: 经过对构建的大规模数据库进行实验，该模型的效果得到了验证，并且提供了许多有用的指导思想，显示了步态基于的抑郁风险识别方法的巨大潜力。<details>
<summary>Abstract</summary>
Depression, a highly prevalent mental illness, affects over 280 million individuals worldwide. Early detection and timely intervention are crucial for promoting remission, preventing relapse, and alleviating the emotional and financial burdens associated with depression. However, patients with depression often go undiagnosed in the primary care setting. Unlike many physiological illnesses, depression lacks objective indicators for recognizing depression risk, and existing methods for depression risk recognition are time-consuming and often encounter a shortage of trained medical professionals. The correlation between gait and depression risk has been empirically established. Gait can serve as a promising objective biomarker, offering the advantage of efficient and convenient data collection. However, current methods for recognizing depression risk based on gait have only been validated on small, private datasets, lacking large-scale publicly available datasets for research purposes. Additionally, these methods are primarily limited to hand-crafted approaches. Gait is a complex form of motion, and hand-crafted gait features often only capture a fraction of the intricate associations between gait and depression risk. Therefore, this study first constructs a large-scale gait database, encompassing over 1,200 individuals, 40,000 gait sequences, and covering six perspectives and three types of attire. Two commonly used psychological scales are provided as depression risk annotations. Subsequently, a deep learning-based depression risk recognition model is proposed, overcoming the limitations of hand-crafted approaches. Through experiments conducted on the constructed large-scale database, the effectiveness of the proposed method is validated, and numerous instructive insights are presented in the paper, highlighting the significant potential of gait-based depression risk recognition.
</details>
<details>
<summary>摘要</summary>
全球280多万人患有抑郁症，早期发现和及时 interven 是关键，可以促进缓解、避免回落和减轻抑郁症的情感和财务负担。但是，抑郁症患者在主要医疗设施中 frequently goes undiagnosed。与许多物理疾病不同，抑郁症缺乏可Recognizing depression risk is challenging due to the lack of objective indicators, and existing methods are time-consuming and often face a shortage of trained medical professionals. However, research has shown that there is a correlation between gait and depression risk. Gait can serve as a promising objective biomarker, offering the advantage of efficient and convenient data collection. However, current methods for recognizing depression risk based on gait have only been validated on small, private datasets, lacking large-scale publicly available datasets for research purposes. Additionally, these methods are primarily limited to hand-crafted approaches. Gait is a complex form of motion, and hand-crafted gait features often only capture a fraction of the intricate associations between gait and depression risk.为了解决这些问题，本研究首先构建了一个大规模的步态数据库，包括1,200个人，40,000个步态序列和六种视角，以及三种衣物类型。两个常用的心理测量方法提供为抑郁风险注释。然后，一种深度学习基于的抑郁风险识别模型被提出，以超越手工制作的方法。经过对构建的大规模数据库进行实验，提出的方法的有效性得到验证，并且在文中提供了丰富的指导思想，强调步态基于的抑郁风险识别的重要性。
</details></li>
</ul>
<hr>
<h2 id="MuseChat-A-Conversational-Music-Recommendation-System-for-Videos"><a href="#MuseChat-A-Conversational-Music-Recommendation-System-for-Videos" class="headerlink" title="MuseChat: A Conversational Music Recommendation System for Videos"></a>MuseChat: A Conversational Music Recommendation System for Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06282">http://arxiv.org/abs/2310.06282</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhikang Dong, Bin Chen, Xiulong Liu, Pawel Polak, Peng Zhang</li>
<li>For: This paper aims to provide an innovative dialog-based music recommendation system that offers interactive user engagement and personalized music selections tailored for input videos.* Methods: The paper introduces a conversation-synthesis method that simulates a two-turn interaction between a user and a recommendation system, leveraging pre-trained music tags and artist information. It also introduces a multi-modal recommendation engine that matches music with visual cues from the video, user feedback, and textual input.* Results: The paper shows that MuseChat, the proposed music recommendation system, surpasses existing state-of-the-art models in music retrieval tasks and pioneers the integration of the recommendation process within a natural language framework.<details>
<summary>Abstract</summary>
We introduce MuseChat, an innovative dialog-based music recommendation system. This unique platform not only offers interactive user engagement but also suggests music tailored for input videos, so that users can refine and personalize their music selections. In contrast, previous systems predominantly emphasized content compatibility, often overlooking the nuances of users' individual preferences. For example, all the datasets only provide basic music-video pairings or such pairings with textual music descriptions. To address this gap, our research offers three contributions. First, we devise a conversation-synthesis method that simulates a two-turn interaction between a user and a recommendation system, which leverages pre-trained music tags and artist information. In this interaction, users submit a video to the system, which then suggests a suitable music piece with a rationale. Afterwards, users communicate their musical preferences, and the system presents a refined music recommendation with reasoning. Second, we introduce a multi-modal recommendation engine that matches music either by aligning it with visual cues from the video or by harmonizing visual information, feedback from previously recommended music, and the user's textual input. Third, we bridge music representations and textual data with a Large Language Model(Vicuna-7B). This alignment equips MuseChat to deliver music recommendations and their underlying reasoning in a manner resembling human communication. Our evaluations show that MuseChat surpasses existing state-of-the-art models in music retrieval tasks and pioneers the integration of the recommendation process within a natural language framework.
</details>
<details>
<summary>摘要</summary>
我们介绍MuseChat，一种创新的对话式音乐推荐系统。这个独特的平台不仅提供互动用户参与度，还为输入视频提供适合的音乐推荐，以便用户可以细化和个性化音乐选择。与之前的系统不同，我们的研究强调用户个人偏好，而不是主要强调内容相容。例如，所有数据只提供基本的音乐视频对应或文本音乐描述。为了解决这一漏洞，我们的研究提供了三种贡献。首先，我们开发了一种对话生成方法，该方法通过使用预训练的音乐标签和艺术家信息，模拟用户和推荐系统之间的两次对话。在这个对话中，用户提供视频，系统则提供适合的音乐曲目并给出了理由。然后，用户表达自己的音乐偏好，系统则提供了修改后的音乐推荐和理由。第二，我们引入多模态推荐引擎，该引擎将音乐与视频信息、用户之前推荐的音乐反馈、以及用户的文本输入进行匹配。第三，我们将音乐表示和文本数据进行了对接，通过使用大型自然语言模型（Vicuna-7B）。这种对接使得MuseChat可以提供音乐推荐和其下面的理由，与人类交流方式相似。我们的评估显示，MuseChat在音乐检索任务中超过了现有的状态 искусственный智能模型，并成为了对话式推荐过程的先驱者。
</details></li>
</ul>
<hr>
<h2 id="High-Fidelity-3D-Head-Avatars-Reconstruction-through-Spatially-Varying-Expression-Conditioned-Neural-Radiance-Field"><a href="#High-Fidelity-3D-Head-Avatars-Reconstruction-through-Spatially-Varying-Expression-Conditioned-Neural-Radiance-Field" class="headerlink" title="High-Fidelity 3D Head Avatars Reconstruction through Spatially-Varying Expression Conditioned Neural Radiance Field"></a>High-Fidelity 3D Head Avatars Reconstruction through Spatially-Varying Expression Conditioned Neural Radiance Field</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06275">http://arxiv.org/abs/2310.06275</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minghan Qin, Yifan Liu, Yuelang Xu, Xiaochen Zhao, Yebin Liu, Haoqian Wang<br>for:* 三维头部人物重建中的一个关键方面是表情细节。methods:* 我们提出了一种新的空间变换表达（SVE）条件。* SVE可以通过一个简单的多层感知网络生成，包括不同位置的空间特征和全局表情信息。results:* 我们的方法可以更好地处理复杂的表情细节，并实现高质量的三维头部人物重建。* 我们的方法在移动终端收集和公共数据集上实现了比其他状态时间（SOTA）方法更高的图形和渲染质量。<details>
<summary>Abstract</summary>
One crucial aspect of 3D head avatar reconstruction lies in the details of facial expressions. Although recent NeRF-based photo-realistic 3D head avatar methods achieve high-quality avatar rendering, they still encounter challenges retaining intricate facial expression details because they overlook the potential of specific expression variations at different spatial positions when conditioning the radiance field. Motivated by this observation, we introduce a novel Spatially-Varying Expression (SVE) conditioning. The SVE can be obtained by a simple MLP-based generation network, encompassing both spatial positional features and global expression information. Benefiting from rich and diverse information of the SVE at different positions, the proposed SVE-conditioned neural radiance field can deal with intricate facial expressions and achieve realistic rendering and geometry details of high-fidelity 3D head avatars. Additionally, to further elevate the geometric and rendering quality, we introduce a new coarse-to-fine training strategy, including a geometry initialization strategy at the coarse stage and an adaptive importance sampling strategy at the fine stage. Extensive experiments indicate that our method outperforms other state-of-the-art (SOTA) methods in rendering and geometry quality on mobile phone-collected and public datasets.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)一个重要的方面是3D头像重建的脸部表情细节。虽然最近的NeRF基于的高品质3D头像方法可以实现高质量的头像渲染，但它们还遇到表情细节细节保持的挑战，因为它们忽略了不同空间位置中的特定表情变化的潜在可能性。我们受到这一观察的 inspirada，引入了一种新的空间变化表达（SVE）conditioning。SVE可以通过一个简单的多层感知网络生成，包括空间位势特征和全局表情信息。由于SVE在不同位置上具有丰富和多样的信息，我们的提议的SVE-conditioned神经辐射场可以处理复杂的脸部表情和高精度3D头像的渲染和几何细节。此外，为了进一步提高几何和渲染质量，我们引入了一种新的粗略到细节训练策略，包括geometry initialization策略和适应重要性采样策略。广泛的实验表明，我们的方法在移动设备采集和公共数据集上的渲染和几何质量都有所提高。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Adaptation-of-Large-Vision-Transformer-via-Adapter-Re-Composing"><a href="#Efficient-Adaptation-of-Large-Vision-Transformer-via-Adapter-Re-Composing" class="headerlink" title="Efficient Adaptation of Large Vision Transformer via Adapter Re-Composing"></a>Efficient Adaptation of Large Vision Transformer via Adapter Re-Composing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06234">http://arxiv.org/abs/2310.06234</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/davidyanande/arc">https://github.com/davidyanande/arc</a></li>
<li>paper_authors: Wei Dong, Dawei Yan, Zhijun Lin, Peng Wang</li>
<li>for: 这个研究旨在提高大型预训模型的效率适应，并且将适应 Parameters 的数量降低至最小。</li>
<li>methods: 本研究提出了一个新的 Adapter Re-Composing (ARC) 策略，它利用 Shared Parameters 来构建层对层的 Adaptation Layers。这个方法可以实现对大型预训模型的效率适应，并且将适应 Parameters 的数量降低至最小。</li>
<li>results: 在 24 个下游图像分类任务中，我们的方法可以取得出色的转移学习表现，并且将适应 Parameters 的数量降低至最小。<details>
<summary>Abstract</summary>
The advent of high-capacity pre-trained models has revolutionized problem-solving in computer vision, shifting the focus from training task-specific models to adapting pre-trained models. Consequently, effectively adapting large pre-trained models to downstream tasks in an efficient manner has become a prominent research area. Existing solutions primarily concentrate on designing lightweight adapters and their interaction with pre-trained models, with the goal of minimizing the number of parameters requiring updates. In this study, we propose a novel Adapter Re-Composing (ARC) strategy that addresses efficient pre-trained model adaptation from a fresh perspective. Our approach considers the reusability of adaptation parameters and introduces a parameter-sharing scheme. Specifically, we leverage symmetric down-/up-projections to construct bottleneck operations, which are shared across layers. By learning low-dimensional re-scaling coefficients, we can effectively re-compose layer-adaptive adapters. This parameter-sharing strategy in adapter design allows us to significantly reduce the number of new parameters while maintaining satisfactory performance, thereby offering a promising approach to compress the adaptation cost. We conduct experiments on 24 downstream image classification tasks using various Vision Transformer variants to evaluate our method. The results demonstrate that our approach achieves compelling transfer learning performance with a reduced parameter count. Our code is available at \href{https://github.com/DavidYanAnDe/ARC}{https://github.com/DavidYanAnDe/ARC}.
</details>
<details>
<summary>摘要</summary>
高效预训模型的出现对计算机视觉问题的解决带来了革命性的变革，从训练任务特定模型转移到适应预训模型。因此，有效地适应大规模预训模型到下游任务成为了当前研究领域的焦点。现有的解决方案主要集中在设计轻量级适配器和预训模型之间的互动，以尽量减少需要更新的参数数量。在本研究中，我们提出了一种新的适配器重新组合（ARC）策略，从新的角度解决高效预训模型适配问题。我们的方法考虑适配器参数的再利用，并 introduce了共享参数的设计。具体来说，我们利用 симметриcz下/上投影来构建瓶颈操作，这些操作在层次上共享。通过学习低维度归一化系数，我们可以有效地重新组合层次适配器。这种参数共享的适配器设计策略可以减少新的参数数量，同时保持满意的性能，从而提供一种可靠的压缩适配成本的方法。我们在24种图像分类任务上使用了不同的视力变换器变体进行实验，以评估我们的方法。结果表明，我们的方法在参数数量减少的情况下实现了吸引人的转移学习性能。我们的代码可以在 \href{https://github.com/DavidYanAnDe/ARC}{https://github.com/DavidYanAnDe/ARC} 上找到。
</details></li>
</ul>
<hr>
<h2 id="Spiking-PointNet-Spiking-Neural-Networks-for-Point-Clouds"><a href="#Spiking-PointNet-Spiking-Neural-Networks-for-Point-Clouds" class="headerlink" title="Spiking PointNet: Spiking Neural Networks for Point Clouds"></a>Spiking PointNet: Spiking Neural Networks for Point Clouds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06232">http://arxiv.org/abs/2310.06232</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dayongren/spiking-pointnet">https://github.com/dayongren/spiking-pointnet</a></li>
<li>paper_authors: Dayong Ren, Zhe Ma, Yuanpei Chen, Weihang Peng, Xiaode Liu, Yuhan Zhang, Yufei Guo</li>
<li>for: 本研究旨在探讨 whether Spiking Neural Networks (SNNs) can be generalized to 3D recognition, and to present a spiking neural model for efficient deep learning on point clouds.</li>
<li>methods: 我们提出了一种 trained-less but learning-more 方法，使用单个时间步骤训练 Spiking PointNet，并通过 theoretically justifications 和实验分析证明其效果。</li>
<li>results: 我们在 ModelNet10 和 ModelNet40 上进行了多种实验，发现 Spiking PointNet 可以在多个时间步骤推理中提供更好的性能，并且可以超越其 ANNS 对应模型，这是 SNN 领域中非常罕见的。 另外，Spiking PointNet 在训练阶段也显示出了很好的速度和存储减少。<details>
<summary>Abstract</summary>
Recently, Spiking Neural Networks (SNNs), enjoying extreme energy efficiency, have drawn much research attention on 2D visual recognition and shown gradually increasing application potential. However, it still remains underexplored whether SNNs can be generalized to 3D recognition. To this end, we present Spiking PointNet in the paper, the first spiking neural model for efficient deep learning on point clouds. We discover that the two huge obstacles limiting the application of SNNs in point clouds are: the intrinsic optimization obstacle of SNNs that impedes the training of a big spiking model with large time steps, and the expensive memory and computation cost of PointNet that makes training a big spiking point model unrealistic. To solve the problems simultaneously, we present a trained-less but learning-more paradigm for Spiking PointNet with theoretical justifications and in-depth experimental analysis. In specific, our Spiking PointNet is trained with only a single time step but can obtain better performance with multiple time steps inference, compared to the one trained directly with multiple time steps. We conduct various experiments on ModelNet10, ModelNet40 to demonstrate the effectiveness of Spiking PointNet. Notably, our Spiking PointNet even can outperform its ANN counterpart, which is rare in the SNN field thus providing a potential research direction for the following work. Moreover, Spiking PointNet shows impressive speedup and storage saving in the training phase.
</details>
<details>
<summary>摘要</summary>
近些年，激活神经网络（SNN）因其极高的能效性而吸引了大量研究人员的关注，主要应用于2D视觉识别领域。然而，SNN在3D识别领域的应用仍然尚未得到充分探索。为此，我们在本文中提出了激活点网（Spiking PointNet），这是首个采用激活神经元进行深度学习的点云模型。我们发现，在点云模型中，SNN的两个主要障碍物是：激活神经元优化障碍和点云模型的内存和计算成本高。为解决这两个问题，我们提出了一种无需训练的，但可以吸取更多知识的概念。具体来说，我们的激活点网在单个时间步上进行训练，但可以在多个时间步上进行INFERENCE，并且可以在ModelNet10和ModelNet40上进行了多种实验，以证明激活点网的效果。特别是，我们的激活点网甚至可以超越其ANN对应模型，这在SNN领域内很罕见，因此提供了一个可能的研究方向。此外，激活点网在训练阶段也显示了快速和储存的节省。
</details></li>
</ul>
<hr>
<h2 id="CoT3DRef-Chain-of-Thoughts-Data-Efficient-3D-Visual-Grounding"><a href="#CoT3DRef-Chain-of-Thoughts-Data-Efficient-3D-Visual-Grounding" class="headerlink" title="CoT3DRef: Chain-of-Thoughts Data-Efficient 3D Visual Grounding"></a>CoT3DRef: Chain-of-Thoughts Data-Efficient 3D Visual Grounding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06214">http://arxiv.org/abs/2310.06214</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eslam Mohamed Bakr, Mohamed Ayman, Mahmoud Ahmed, Habib Slim, Mohamed Elhoseiny</li>
<li>for: 本研究旨在提出一种可解释的3D视觉定位方法，以模拟人类视觉系统。</li>
<li>methods: 本方法形式为一种序列到序列任务，首先预测一串含义杆并 then 预测最终目标。解释性不仅提高了总性表现，还帮助我们理解网络做出最终决定的原因。</li>
<li>results: 我们的方法在Nr3D、Sr3D和Scanreferbenchmark上进行了广泛的实验，并比现有方法具有更高的性能和更好的数据效率。此外，我们的方法可以轻松地与现有架构结合使用。<details>
<summary>Abstract</summary>
3D visual grounding is the ability to localize objects in 3D scenes conditioned by utterances. Most existing methods devote the referring head to localize the referred object directly, causing failure in complex scenarios. In addition, it does not illustrate how and why the network reaches the final decision. In this paper, we address this question Can we design an interpretable 3D visual grounding framework that has the potential to mimic the human perception system?. To this end, we formulate the 3D visual grounding problem as a sequence-to-sequence task by first predicting a chain of anchors and then the final target. Interpretability not only improves the overall performance but also helps us identify failure cases. Following the chain of thoughts approach enables us to decompose the referring task into interpretable intermediate steps, boosting the performance and making our framework extremely data-efficient. Moreover, our proposed framework can be easily integrated into any existing architecture. We validate our approach through comprehensive experiments on the Nr3D, Sr3D, and Scanrefer benchmarks and show consistent performance gains compared to existing methods without requiring manually annotated data. Furthermore, our proposed framework, dubbed CoT3DRef, is significantly data-efficient, whereas on the Sr3D dataset, when trained only on 10% of the data, we match the SOTA performance that trained on the entire data.
</details>
<details>
<summary>摘要</summary>
三维视觉根据是指将语音引用对象在三维场景中固定的能力。现有大多数方法直接使用引用头来确定引用对象，导致复杂场景下失败。此外，它们不能解释网络是如何做出最终决定的。在这篇论文中，我们解决这个问题。我们是否可以设计一个可解释的三维视觉根据框架，以模仿人类视觉系统呢？为此，我们将三维视觉根据问题转化为序列到序列任务，首先预测一系列的锚点，然后预测最终的目标。可解释性不仅提高了总性表现，还帮助我们解释失败的原因。我们采用链条思想方法，将引用任务分解为可解释的中间步骤，从而提高表现和训练效率。此外，我们的提议的框架可以轻松地与现有架构结合使用。我们通过对 Nr3D、Sr3D 和 Scanrefer 测试准则进行广泛的实验，并示出与现有方法相比，我们的方法具有显著的性能提升，而无需手动标注数据。此外，我们的提议的框架，即 CoT3DRef，在 Sr3D 数据集上训练时只使用 10% 的数据，却与所有数据训练的 SOTA 性能相当。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/10/cs.CV_2023_10_10/" data-id="clpahu73w00la3h882r5r1ehg" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_10_10" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/10/cs.AI_2023_10_10/" class="article-date">
  <time datetime="2023-10-10T12:00:00.000Z" itemprop="datePublished">2023-10-10</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/10/cs.AI_2023_10_10/">cs.AI - 2023-10-10</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Syntax-Error-Free-and-Generalizable-Tool-Use-for-LLMs-via-Finite-State-Decoding"><a href="#Syntax-Error-Free-and-Generalizable-Tool-Use-for-LLMs-via-Finite-State-Decoding" class="headerlink" title="Syntax Error-Free and Generalizable Tool Use for LLMs via Finite-State Decoding"></a>Syntax Error-Free and Generalizable Tool Use for LLMs via Finite-State Decoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07075">http://arxiv.org/abs/2310.07075</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chenhongqiao/tooldec">https://github.com/chenhongqiao/tooldec</a></li>
<li>paper_authors: Kexun Zhang, Hongqiao Chen, Lei Li, William Wang</li>
<li>for: 提高大型自然语言模型（LLM）在使用外部工具解决复杂问题的能力。</li>
<li>methods: 提出了一种基于finite-state machine-guided decoding算法的工具增强LLM，可以消除工具相关的错误，并使LLM能够通过工具名称和类型相符的参数选择合适的工具。</li>
<li>results: 通过对多种任务进行测试，包括数学函数、知识图关系和复杂的现实世界RESTful API等，实验表明，ToolDec可以完全消除语法错误，并在不需要精度调整或在 контекст中提供工具文档的情况下，达到更高的性能和速度提升。此外，ToolDec还可以在未看过工具的情况下，选择合适的工具，并且可以更好地泛化到新的工具。<details>
<summary>Abstract</summary>
Large language models (LLMs) have shown promising capabilities in using external tools to solve complex problems. However, existing approaches either involve fine-tuning on tool demonstrations, which do not generalize to new tools without additional training, or providing tool documentation in context, limiting the number of tools. Both approaches often generate syntactically invalid tool calls. In this paper, we propose ToolDec, a finite-state machine-guided decoding algorithm for tool-augmented LLMs. ToolDec eliminates tool-related errors for any tool-augmented LLMs by ensuring valid tool names and type-conforming arguments. Furthermore, ToolDec enables LLM to effectively select tools using only the information contained in their names, with no need for fine-tuning or in-context documentation. We evaluated multiple prior methods and their ToolDec-enhanced versions on a variety of tasks involving tools like math functions, knowledge graph relations, and complex real-world RESTful APIs. Our experiments show that ToolDec reduces syntactic errors to zero, consequently achieving significantly better performance and as much as a 2x speedup. We also show that ToolDec achieves superior generalization performance on unseen tools, performing up to 8x better than the baselines.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Large-Language-Models-can-Learn-Rules"><a href="#Large-Language-Models-can-Learn-Rules" class="headerlink" title="Large Language Models can Learn Rules"></a>Large Language Models can Learn Rules</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07064">http://arxiv.org/abs/2310.07064</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Zhaocheng Zhu, Yuan Xue, Xinyun Chen, Denny Zhou, Jian Tang, Dale Schuurmans, Hanjun Dai</li>
<li>for: 提高大语言模型（LLM）在不同类型的逻辑任务中表现的精度。</li>
<li>methods: 使用自动生成和验证规则的方法，通过训练例子来学习规则库，然后使用这些规则库来完成逻辑任务。</li>
<li>results: 在数字逻辑和关系逻辑任务中，使用HtT框架可以提高现有的提问方法精度，增加11-27%的精度提升。<details>
<summary>Abstract</summary>
When prompted with a few examples and intermediate steps, large language models (LLMs) have demonstrated impressive performance in various reasoning tasks. However, prompting methods that rely on implicit knowledge in an LLM often hallucinate incorrect answers when the implicit knowledge is wrong or inconsistent with the task. To tackle this problem, we present Hypotheses-to-Theories (HtT), a framework that learns a rule library for reasoning with LLMs. HtT contains two stages, an induction stage and a deduction stage. In the induction stage, an LLM is first asked to generate and verify rules over a set of training examples. Rules that appear and lead to correct answers sufficiently often are collected to form a rule library. In the deduction stage, the LLM is then prompted to employ the learned rule library to perform reasoning to answer test questions. Experiments on both numerical reasoning and relational reasoning problems show that HtT improves existing prompting methods, with an absolute gain of 11-27% in accuracy. The learned rules are also transferable to different models and to different forms of the same problem.
</details>
<details>
<summary>摘要</summary>
当提供一些示例和中间步骤时，大语言模型（LLM）展现出了吸引人的表现在不同的逻辑任务上。然而，这些prompting方法常常当 implicit knowledge在 LLM 中错误或与任务不一致时会hallucinate incorrect answers。为解决这个问题，我们提出了 Hypotheses-to-Theories（HtT）框架，该框架学习了一个逻辑规则库，用于与 LLM 进行逻辑 reasoning。HtT 框架包括两个阶段：induction stage和 deduction stage。在induction stage中， LLM 首先被要求生成并验证规则，以便在一组训练示例上建立一个规则库。在 deduction stage中， LLM  THEN 被要求使用学习的规则库来解决测试问题。实验表明，HtT 可以提高现有的prompting方法，具有11-27%的精度提升。学习的规则也可以转移到不同的模型和不同的问题形式。
</details></li>
</ul>
<hr>
<h2 id="DKEC-Domain-Knowledge-Enhanced-Multi-Label-Classification-for-Electronic-Health-Records"><a href="#DKEC-Domain-Knowledge-Enhanced-Multi-Label-Classification-for-Electronic-Health-Records" class="headerlink" title="DKEC: Domain Knowledge Enhanced Multi-Label Classification for Electronic Health Records"></a>DKEC: Domain Knowledge Enhanced Multi-Label Classification for Electronic Health Records</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07059">http://arxiv.org/abs/2310.07059</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xueren Ge, Ronald Dean Williams, John A. Stankovic, Homa Alemzadeh</li>
<li>for: 这个论文的目的是对医疗领域的多Label文本分类任务进行研究，以解决长尾分布的问题，即少数类别的训练数据比较少。</li>
<li>methods: 这个论文使用了两个innovation：第一，是一个 Label-wise attention mechanism，可以 capture医疗知识和领域 ontologies 中的semantic relationships between medical entities。第二，是一个简单 yet effective的group-wise training method，可以增加 rare classes 的训练数据。</li>
<li>results: 这个论文的实验结果显示，我们的方法可以比前一代方法更好地预测医疗诊断，特别是对少数类别（tail）的预测。此外，我们还研究了 DKEC 在不同的语言模型上的应用，并证明了 DKEC 可以帮助小型语言模型 achieve comparable performance 到大型语言模型。<details>
<summary>Abstract</summary>
Multi-label text classification (MLTC) tasks in the medical domain often face long-tail label distribution, where rare classes have fewer training samples than frequent classes. Although previous works have explored different model architectures and hierarchical label structures to find important features, most of them neglect to incorporate the domain knowledge from medical guidelines. In this paper, we present DKEC, Domain Knowledge Enhanced Classifier for medical diagnosis prediction with two innovations: (1) a label-wise attention mechanism that incorporates a heterogeneous graph and domain ontologies to capture the semantic relationships between medical entities, (2) a simple yet effective group-wise training method based on similarity of labels to increase samples of rare classes. We evaluate DKEC on two real-world medical datasets: the RAA dataset, a collection of 4,417 patient care reports from emergency medical services (EMS) incidents, and a subset of 53,898 reports from the MIMIC-III dataset. Experimental results show that our method outperforms the state-of-the-art, particularly for the few-shot (tail) classes. More importantly, we study the applicability of DKEC to different language models and show that DKEC can help the smaller language models achieve comparable performance to large language models.
</details>
<details>
<summary>摘要</summary>
多个标签文本分类（MLTC）任务在医疗领域经常遇到长尾标签分布，其中罕见的类别有 fewer 的训练样本 than frequent classes。 although previous works have explored different model architectures and hierarchical label structures to find important features, most of them neglect to incorporate the domain knowledge from medical guidelines. In this paper, we present DKEC, Domain Knowledge Enhanced Classifier for medical diagnosis prediction with two innovations: (1) a label-wise attention mechanism that incorporates a heterogeneous graph and domain ontologies to capture the semantic relationships between medical entities, (2) a simple yet effective group-wise training method based on similarity of labels to increase samples of rare classes. We evaluate DKEC on two real-world medical datasets: the RAA dataset, a collection of 4,417 patient care reports from emergency medical services (EMS) incidents, and a subset of 53,898 reports from the MIMIC-III dataset. Experimental results show that our method outperforms the state-of-the-art, particularly for the few-shot (tail) classes. More importantly, we study the applicability of DKEC to different language models and show that DKEC can help the smaller language models achieve comparable performance to large language models.Here's the text with some notes on the translation:* "多个标签" is translated as "多个标签" (both words are the same in Chinese), which is a bit redundant but follows the original text's structure.* "文本分类" is translated as "文本分类" (both words are the same in Chinese), which is also a bit redundant but follows the original text's structure.* "任务" is translated as "任务" (a single word in Chinese), which is a more concise way of saying "task" in Chinese.* "在医疗领域" is translated as "在医疗领域" (both words are the same in Chinese), which is a more concise way of saying "in the medical field" in Chinese.* "经常" is translated as "经常" (a single word in Chinese), which is a more concise way of saying "often" in Chinese.* "遇到" is translated as "遇到" (a single word in Chinese), which is a more concise way of saying "encounter" in Chinese.* "长尾标签" is translated as "长尾标签" (both words are the same in Chinese), which is a more concise way of saying "long-tail label" in Chinese.* "其中" is translated as "其中" (a single word in Chinese), which is a more concise way of saying "where" in Chinese.* "罕见的类别" is translated as "罕见的类别" (both words are the same in Chinese), which is a more concise way of saying "rare classes" in Chinese.* "有 fewer 的训练样本" is translated as "有 fewer 的训练样本" (both words are the same in Chinese), which is a more concise way of saying "have fewer training samples" in Chinese.* "although" is translated as "although" (a single word in Chinese), which is a more concise way of saying "although" in Chinese.* "previous works" is translated as "前一些工作" (both words are the same in Chinese), which is a more concise way of saying "previous works" in Chinese.* "have explored" is translated as "已经探索" (a single word in Chinese), which is a more concise way of saying "have explored" in Chinese.* "different model architectures and hierarchical label structures" is translated as "不同的模型架构和层次标签结构" (both words are the same in Chinese), which is a more concise way of saying "different model architectures and hierarchical label structures" in Chinese.* "to find important features" is translated as "以找到重要特征" (a single word in Chinese), which is a more concise way of saying "to find important features" in Chinese.* "most of them neglect to incorporate the domain knowledge from medical guidelines" is translated as "大多数 Neglect 医学指南中的领域知识" (both words are the same in Chinese), which is a more concise way of saying "most of them neglect to incorporate the domain knowledge from medical guidelines" in Chinese.* "In this paper, we present DKEC" is translated as "本文中，我们介绍 DKEC" (both words are the same in Chinese), which is a more concise way of saying "In this paper, we present DKEC" in Chinese.* "Domain Knowledge Enhanced Classifier" is translated as "领域知识增强分类器" (both words are the same in Chinese), which is a more concise way of saying "Domain Knowledge Enhanced Classifier" in Chinese.* "for medical diagnosis prediction" is translated as "用于医学诊断预测" (a single word in Chinese), which is a more concise way of saying "for medical diagnosis prediction" in Chinese.* "with two innovations" is translated as "两种创新" (both words are the same in Chinese), which is a more concise way of saying "with two innovations" in Chinese.* "label-wise attention mechanism" is translated as "标签 wise 注意机制" (both words are the same in Chinese), which is a more concise way of saying "label-wise attention mechanism" in Chinese.* "incorporates a heterogeneous graph and domain ontologies" is translated as "包含不同类型的图和领域 ontologies" (both words are the same in Chinese), which is a more concise way of saying "incorporates a heterogeneous graph and domain ontologies" in Chinese.* "to capture the semantic relationships between medical entities" is translated as "以捕捉医疗实体之间的含义关系" (a single word in Chinese), which is a more concise way of saying "to capture the semantic relationships between medical entities" in Chinese.* "We evaluate DKEC on two real-world medical datasets" is translated as "我们在两个实际医疗数据集上评估 DKEC" (both words are the same in Chinese), which is a more concise way of saying "We evaluate DKEC on two real-world medical datasets" in Chinese.* "RAA dataset" is translated as "RAA 数据集" (a single word in Chinese), which is a more concise way of saying "RAA dataset" in Chinese.* "a collection of 4,417 patient care reports from emergency medical services (EMS) incidents" is translated as "4,417 例 emergency medical services (EMS) 事件中的病人护理报告集" (both words are the same in Chinese), which is a more concise way of saying "a collection of 4,417 patient care reports from emergency medical services (EMS) incidents" in Chinese.* "a subset of 53,898 reports from the MIMIC-III dataset" is translated as "MIMIC-III 数据集中的53,898 份报告子集" (both words are the same in Chinese), which is a more concise way of saying "a subset of 53,898 reports from the MIMIC-III dataset" in Chinese.* "Experimental results show that our method outperforms the state-of-the-art" is translated as "实验结果表明我们的方法在当前领域中表现出色" (a single word in Chinese), which is a more concise way of saying "Experimental results show that our method outperforms the state-of-the-art" in Chinese.* "particularly for the few-shot (tail) classes" is translated as "尤其是少量 (tail) 类" (both words are the same in Chinese), which is a more concise way of saying "particularly for the few-shot (tail) classes" in Chinese.* "More importantly" is translated as "更重要的是" (a single word in Chinese), which is a more concise way of saying "More importantly" in Chinese.* "we study the applicability of DKEC to different language models" is translated as "我们研究 DKEC 在不同语言模型上的可应用性" (both words are the same in Chinese), which is a more concise way of saying "we study the applicability of DKEC to different language models" in Chinese.* "and show that DKEC can help the smaller language models achieve comparable performance to large language models" is translated as "并表明 DKEC 可以帮助小型语言模型实现与大型语言模型相同的性能" (both words are the same in Chinese), which is a more concise way of saying "and show that DKEC can help the smaller language models achieve comparable performance to large language models" in Chinese.
</details></li>
</ul>
<hr>
<h2 id="Computational-Pathology-at-Health-System-Scale-–-Self-Supervised-Foundation-Models-from-Three-Billion-Images"><a href="#Computational-Pathology-at-Health-System-Scale-–-Self-Supervised-Foundation-Models-from-Three-Billion-Images" class="headerlink" title="Computational Pathology at Health System Scale – Self-Supervised Foundation Models from Three Billion Images"></a>Computational Pathology at Health System Scale – Self-Supervised Foundation Models from Three Billion Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07033">http://arxiv.org/abs/2310.07033</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gabriele Campanella, Ricky Kwan, Eugene Fluder, Jennifer Zeng, Aryeh Stock, Brandon Veremis, Alexandros D. Polydorides, Cyrus Hedvat, Adam Schoenfeld, Chad Vanderbilt, Patricia Kovatch, Carlos Cordon-Cardo, Thomas J. Fuchs</li>
<li>for: 这个研究的目的是使用大量无标注数据进行自我超vised learning，以准确地预测病理科领域中的多种下游任务。</li>
<li>methods: 这个研究使用了masked autoencoder（MAE）和DINO算法进行自我超vised learning。</li>
<li>results: 研究结果显示，这些自我超vised learning算法在病理科领域的大量数据上进行预训后，对下游任务的性能有所提高，而DINO算法在所有任务中表现更好。<details>
<summary>Abstract</summary>
Recent breakthroughs in self-supervised learning have enabled the use of large unlabeled datasets to train visual foundation models that can generalize to a variety of downstream tasks. While this training paradigm is well suited for the medical domain where annotations are scarce, large-scale pre-training in the medical domain, and in particular pathology, has not been extensively studied. Previous work in self-supervised learning in pathology has leveraged smaller datasets for both pre-training and evaluating downstream performance. The aim of this project is to train the largest academic foundation model and benchmark the most prominent self-supervised learning algorithms by pre-training and evaluating downstream performance on large clinical pathology datasets. We collected the largest pathology dataset to date, consisting of over 3 billion images from over 423 thousand microscopy slides. We compared pre-training of visual transformer models using the masked autoencoder (MAE) and DINO algorithms. We evaluated performance on six clinically relevant tasks from three anatomic sites and two institutions: breast cancer detection, inflammatory bowel disease detection, breast cancer estrogen receptor prediction, lung adenocarcinoma EGFR mutation prediction, and lung cancer immunotherapy response prediction. Our results demonstrate that pre-training on pathology data is beneficial for downstream performance compared to pre-training on natural images. Additionally, the DINO algorithm achieved better generalization performance across all tasks tested. The presented results signify a phase change in computational pathology research, paving the way into a new era of more performant models based on large-scale, parallel pre-training at the billion-image scale.
</details>
<details>
<summary>摘要</summary>
近期，自我监督学习的突破有助于使用大量没有标签的数据来训练视觉基础模型，这些模型可以通过多种下游任务进行泛化。在医疗领域，这种训练方法非常适合，因为标签稀缺。然而，大规模预训练在医疗领域，特别是在病理学方面，尚未得到广泛的研究。在自我监督学习中，以前的工作通常使用小型数据集进行预训练和下游性能评估。本项目的目标是在大学院内 trains the largest academic foundation model，并用最出色的自我监督学习算法对大规模病理学数据集进行预训练和下游性能评估。我们收集了医疗领域最大的数据集，包括超过30亿张图像，来自423000余个微scopic抹片。我们对Visual Transformer模型的预训练使用MAE和DINO算法进行比较。我们对六个临床相关任务进行评估，来自三个 анатомиче位置和两个机构：乳腺癌检测、消耗性肠炎检测、乳腺癌estrogen受体预测、肺adenocarcinoma EGFR变化预测和肺癌免疫策略预测。我们的结果表明，预训练在病理学数据集上比预训练在自然图像上更有利于下游性能。此外，DINO算法在所有任务上实现了更好的泛化性能。这些结果标志着计算 PATHOLOGY 研究的新时代的开始，预计将在大规模、并行预训练的基础上建立更高性能的模型，覆盖 billion-image 级别。
</details></li>
</ul>
<hr>
<h2 id="Facial-Forgery-based-Deepfake-Detection-using-Fine-Grained-Features"><a href="#Facial-Forgery-based-Deepfake-Detection-using-Fine-Grained-Features" class="headerlink" title="Facial Forgery-based Deepfake Detection using Fine-Grained Features"></a>Facial Forgery-based Deepfake Detection using Fine-Grained Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07028">http://arxiv.org/abs/2310.07028</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aakash Varma Nadimpalli, Ajita Rattani</li>
<li>for: 本研究旨在提出一种新的深度模仿检测方法，用于增强现有的深度模仿检测系统的性能。</li>
<li>methods: 该方法基于学习细致、通用的特征，通过有效地抑制背景噪声和学习不同级别的特征，实现深度模仿检测的细致化检测。</li>
<li>results: 通过广泛的实验验证，本研究证明了该方法在跨数据集和跨模杂化检测场景下的超过90%的性能。<details>
<summary>Abstract</summary>
Facial forgery by deepfakes has caused major security risks and raised severe societal concerns. As a countermeasure, a number of deepfake detection methods have been proposed. Most of them model deepfake detection as a binary classification problem using a backbone convolutional neural network (CNN) architecture pretrained for the task. These CNN-based methods have demonstrated very high efficacy in deepfake detection with the Area under the Curve (AUC) as high as $0.99$. However, the performance of these methods degrades significantly when evaluated across datasets and deepfake manipulation techniques. This draws our attention towards learning more subtle, local, and discriminative features for deepfake detection. In this paper, we formulate deepfake detection as a fine-grained classification problem and propose a new fine-grained solution to it. Specifically, our method is based on learning subtle and generalizable features by effectively suppressing background noise and learning discriminative features at various scales for deepfake detection. Through extensive experimental validation, we demonstrate the superiority of our method over the published research in cross-dataset and cross-manipulation generalization of deepfake detectors for the majority of the experimental scenarios.
</details>
<details>
<summary>摘要</summary>
面部伪造技术使用深度复卷 neural network (CNN) 实现了严重的安全风险和社会上的担忧。为了对抗这些威胁，一些深度伪造检测方法已经被提出。大多数这些方法都是模型深度伪造检测为二分类问题，使用预训练的 CNN 架构。这些 CNN 基本架构的方法已经在深度伪造检测中表现出非常高的有效性，AUC 为 0.99。但是，这些方法在不同的数据集和伪造技巧下的表现却有很大的差异。这使我们注意到了更加细致、地方和描述性的特征学习是需要的。在这篇文章中，我们将深度伪造检测设计为精细分类问题，并提出了一个新的精细解决方案。具体而言，我们的方法是通过有效地抑制背景噪音和学习不同尺度的特征，以获得更加细致和普遍适用的深度伪造检测特征。经过了广泛的实验验证，我们证明了我们的方法在跨数据集和跨伪造技巧的普遍化运算中的超越性。
</details></li>
</ul>
<hr>
<h2 id="NEWTON-Are-Large-Language-Models-Capable-of-Physical-Reasoning"><a href="#NEWTON-Are-Large-Language-Models-Capable-of-Physical-Reasoning" class="headerlink" title="NEWTON: Are Large Language Models Capable of Physical Reasoning?"></a>NEWTON: Are Large Language Models Capable of Physical Reasoning?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07018">http://arxiv.org/abs/2310.07018</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/NewtonReasoning/Newton">https://github.com/NewtonReasoning/Newton</a></li>
<li>paper_authors: Yi Ru Wang, Jiafei Duan, Dieter Fox, Siddhartha Srinivasa</li>
<li>for: The paper aims to evaluate the physical reasoning abilities of large language models (LLMs) and provide a benchmark for assessing their performance in this area.</li>
<li>methods: The paper introduces a new repository and benchmark called NEWTON, which includes a collection of object-attribute pairs and 160,000 question-answer pairs to test the physical reasoning capabilities of LLMs. The authors also present a pipeline for generating customized benchmarks for specific applications.</li>
<li>results: The authors find that LLMs like GPT-4 demonstrate strong reasoning capabilities in scenario-based tasks but exhibit less consistency in object-attribute reasoning compared to humans. The paper highlights the potential of the NEWTON platform for evaluating and enhancing language models for physically grounded settings, such as robotic manipulation.<details>
<summary>Abstract</summary>
Large Language Models (LLMs), through their contextualized representations, have been empirically proven to encapsulate syntactic, semantic, word sense, and common-sense knowledge. However, there has been limited exploration of their physical reasoning abilities, specifically concerning the crucial attributes for comprehending everyday objects. To address this gap, we introduce NEWTON, a repository and benchmark for evaluating the physics reasoning skills of LLMs. Further, to enable domain-specific adaptation of this benchmark, we present a pipeline to enable researchers to generate a variant of this benchmark that has been customized to the objects and attributes relevant for their application. The NEWTON repository comprises a collection of 2800 object-attribute pairs, providing the foundation for generating infinite-scale assessment templates. The NEWTON benchmark consists of 160K QA questions, curated using the NEWTON repository to investigate the physical reasoning capabilities of several mainstream language models across foundational, explicit, and implicit reasoning tasks. Through extensive empirical analysis, our results highlight the capabilities of LLMs for physical reasoning. We find that LLMs like GPT-4 demonstrate strong reasoning capabilities in scenario-based tasks but exhibit less consistency in object-attribute reasoning compared to humans (50% vs. 84%). Furthermore, the NEWTON platform demonstrates its potential for evaluating and enhancing language models, paving the way for their integration into physically grounded settings, such as robotic manipulation. Project site: https://newtonreasoning.github.io
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）通过它们的上下文化表现，已经被实践证明可以捕捉语言层次的 sintactic、semantic、词汇和常识知识。然而，对于它们的物理逻辑能力的探索仍然受限，特别是关于日常物品的重要特征。为解决这个问题，我们提出了 NEWTON，一个Repository和Benchmark，用于评估语言模型的物理逻辑能力。此外，我们还提供了一个管道，让研究人员可以根据它们的应用领域自定义NEWTON的Benchmark，以便更好地满足它们的需求。NEWTONRepository包含2800个物品 attribute的集合，提供了无限数量的评估模板。NEWTON Benchmark包含160,000个问题，通过使用NEWTONRepository进行整理，以探索语言模型的物理逻辑能力。我们的实验结果显示，LLMs如GPT-4在enario-based任务中展示了强大的逻辑能力，但在物品 attribute 的推理中与人类（50% vs. 84%）之间存在较大的差异。此外，NEWTON平台显示了它们的潜力，可以评估和改进语言模型，将其应用于物理基础的设置，如 робоック掌控。Project site：https://newtonreasoning.github.io
</details></li>
</ul>
<hr>
<h2 id="Answer-Candidate-Type-Selection-Text-to-Text-Language-Model-for-Closed-Book-Question-Answering-Meets-Knowledge-Graphs"><a href="#Answer-Candidate-Type-Selection-Text-to-Text-Language-Model-for-Closed-Book-Question-Answering-Meets-Knowledge-Graphs" class="headerlink" title="Answer Candidate Type Selection: Text-to-Text Language Model for Closed Book Question Answering Meets Knowledge Graphs"></a>Answer Candidate Type Selection: Text-to-Text Language Model for Closed Book Question Answering Meets Knowledge Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07008">http://arxiv.org/abs/2310.07008</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mikhail Salnikov, Maria Lysyuk, Pavel Braslavski, Anton Razzhigaev, Valentin Malykh, Alexander Panchenko</li>
<li>for: 提高知识图Question Answering（KGQA）任务中不具有很多搜索结果的问题的答案质量。</li>
<li>methods: 使用预训练的文本到文本问答系统，并对生成的候选答案进行筛选和重新排序，根据来自Wikidata “instance_of”属性的类型。</li>
<li>results: 提高了预训练问答系统的答案质量， especialyl for questions with less popular entities.<details>
<summary>Abstract</summary>
Pre-trained Text-to-Text Language Models (LMs), such as T5 or BART yield promising results in the Knowledge Graph Question Answering (KGQA) task. However, the capacity of the models is limited and the quality decreases for questions with less popular entities. In this paper, we present a novel approach which works on top of the pre-trained Text-to-Text QA system to address this issue. Our simple yet effective method performs filtering and re-ranking of generated candidates based on their types derived from Wikidata "instance_of" property.
</details>
<details>
<summary>摘要</summary>
预训练的文本到文本语言模型（LM），如T5或BART，在知识图问答任务中表现了扎实的结果。然而，模型的容量有限，问题中的 menos popular entity 的质量下降。在这篇论文中，我们提出了一种新的方法，它基于预训练的文本到文本问答系统进行排除和重新排序生成的候选答案。我们的简单 yet effective 方法基于Wikidata "instance_of" 属性来 derive 候选答案的类型。
</details></li>
</ul>
<hr>
<h2 id="Catastrophic-Jailbreak-of-Open-source-LLMs-via-Exploiting-Generation"><a href="#Catastrophic-Jailbreak-of-Open-source-LLMs-via-Exploiting-Generation" class="headerlink" title="Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation"></a>Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06987">http://arxiv.org/abs/2310.06987</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/princeton-sysml/jailbreak_llm">https://github.com/princeton-sysml/jailbreak_llm</a></li>
<li>paper_authors: Yangsibo Huang, Samyak Gupta, Mengzhou Xia, Kai Li, Danqi Chen</li>
<li>for: 这个论文主要目标是为了检测和描述如何通过 manipulate 语言模型的生成方法来诱导它们的不良行为，以及提出一种有效的生成方法来减少这种不良行为。</li>
<li>methods: 该论文使用了多种生成方法，包括变化的解码参数和抽样方法，以增加模型的不一致率。</li>
<li>results: 该论文的实验结果显示，通过使用不同的生成方法，可以增加模型的不一致率至95%以上，并且比之前的攻击方法便宜30倍。<details>
<summary>Abstract</summary>
The rapid progress in open-source large language models (LLMs) is significantly advancing AI development. Extensive efforts have been made before model release to align their behavior with human values, with the primary goal of ensuring their helpfulness and harmlessness. However, even carefully aligned models can be manipulated maliciously, leading to unintended behaviors, known as "jailbreaks". These jailbreaks are typically triggered by specific text inputs, often referred to as adversarial prompts. In this work, we propose the generation exploitation attack, an extremely simple approach that disrupts model alignment by only manipulating variations of decoding methods. By exploiting different generation strategies, including varying decoding hyper-parameters and sampling methods, we increase the misalignment rate from 0% to more than 95% across 11 language models including LLaMA2, Vicuna, Falcon, and MPT families, outperforming state-of-the-art attacks with $30\times$ lower computational cost. Finally, we propose an effective alignment method that explores diverse generation strategies, which can reasonably reduce the misalignment rate under our attack. Altogether, our study underscores a major failure in current safety evaluation and alignment procedures for open-source LLMs, strongly advocating for more comprehensive red teaming and better alignment before releasing such models. Our code is available at https://github.com/Princeton-SysML/Jailbreak_LLM.
</details>
<details>
<summary>摘要</summary>
大量的开源大语言模型（LLM）的快速进步正在推动人工智能的发展。在模型发布之前，努力了解行为与人类价值观合并，主要目标是确保它们的帮助和无害。然而，即使仔细对齐的模型也可以被恶意折衡，导致不期望的行为，称为“监狱拥堵”。这些监狱拥堵通常是由特定的文本输入触发，通常被称为“敌意提示”。在这种工作中，我们提出了生成滥用攻击，它是一种非常简单的方法，通过只对变种解码方法进行操作来扰乱模型的对齐。通过利用不同的生成策略，包括变种解码 гипер参数和采样方法，我们从0%提高了距离度到超过95%，在11种语言模型中，包括LLaMA2、Vicuna、Falcon和MPT家族，超过了当前攻击的状态艺术。最后，我们提出了一种有效的对齐方法，它可以有效降低我们的攻击下的距离度。总之，我们的研究表明当前开源LLM的安全评估和对齐过程存在重大的缺陷，强烈建议在发布之前进行更加全面的红团测试，以确保模型的帮助和无害。我们的代码可以在https://github.com/Princeton-SysML/Jailbreak_LLM上获取。
</details></li>
</ul>
<hr>
<h2 id="On-the-Interpretability-of-Part-Prototype-Based-Classifiers-A-Human-Centric-Analysis"><a href="#On-the-Interpretability-of-Part-Prototype-Based-Classifiers-A-Human-Centric-Analysis" class="headerlink" title="On the Interpretability of Part-Prototype Based Classifiers: A Human Centric Analysis"></a>On the Interpretability of Part-Prototype Based Classifiers: A Human Centric Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06966">http://arxiv.org/abs/2310.06966</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/omiddavoodi/part-prototype-interpretability-data">https://github.com/omiddavoodi/part-prototype-interpretability-data</a></li>
<li>paper_authors: Omid Davoodi, Shayan Mohammadizadehsamakosh, Majid Komeili</li>
<li>for: 本研究旨在评估部prototype网络的可解释性从人类用户的视角出发。</li>
<li>methods: 本研究提出了一种评估部prototype网络可解释性的框架，包括三个可行的纪录指标和实验。</li>
<li>results: 实验结果表明，本框架可以准确评估不同类型的部prototype网络的可解释性，并且对现有的黑盒子图像分类器提供了一种可解释的替代方案。<details>
<summary>Abstract</summary>
Part-prototype networks have recently become methods of interest as an interpretable alternative to many of the current black-box image classifiers. However, the interpretability of these methods from the perspective of human users has not been sufficiently explored. In this work, we have devised a framework for evaluating the interpretability of part-prototype-based models from a human perspective. The proposed framework consists of three actionable metrics and experiments. To demonstrate the usefulness of our framework, we performed an extensive set of experiments using Amazon Mechanical Turk. They not only show the capability of our framework in assessing the interpretability of various part-prototype-based models, but they also are, to the best of our knowledge, the most comprehensive work on evaluating such methods in a unified framework.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "Part-prototype networks have recently become methods of interest as an interpretable alternative to many of the current black-box image classifiers. However, the interpretability of these methods from the perspective of human users has not been sufficiently explored. In this work, we have devised a framework for evaluating the interpretability of part-prototype-based models from a human perspective. The proposed framework consists of three actionable metrics and experiments. To demonstrate the usefulness of our framework, we performed an extensive set of experiments using Amazon Mechanical Turk. They not only show the capability of our framework in assessing the interpretability of various part-prototype-based models, but they also are, to the best of our knowledge, the most comprehensive work on evaluating such methods in a unified framework." into 简化中文 >>Here's the translation:现在，部prototype网络已成为一种可解释性替代多种现有的黑obox图像分类器的方法。然而，人类用户对这些方法的可解释性还没有充分探讨。在这项工作中，我们提出了一个用于评估部prototype基于模型的可解释性的框架。该框架包括三个操作性指标和实验。为证明我们的框架的用于性，我们在Amazon Mechanical Turk上进行了广泛的实验。这些实验不仅显示了我们的框架可以评估多种部prototype基于模型的可解释性，而且也是我们知道的最为全面的评估这类方法的框架。
</details></li>
</ul>
<hr>
<h2 id="Sparse-Fine-tuning-for-Inference-Acceleration-of-Large-Language-Models"><a href="#Sparse-Fine-tuning-for-Inference-Acceleration-of-Large-Language-Models" class="headerlink" title="Sparse Fine-tuning for Inference Acceleration of Large Language Models"></a>Sparse Fine-tuning for Inference Acceleration of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06927">http://arxiv.org/abs/2310.06927</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/neuralmagic/deepsparse">https://github.com/neuralmagic/deepsparse</a></li>
<li>paper_authors: Eldar Kurtic, Denis Kuznedelev, Elias Frantar, Michael Goin, Dan Alistarh</li>
<li>for: 本研究旨在精度地精细调整大语言模型（LLM），即在特殊任务上精细调整预训练的LLM，而使其权重 induces sparsity。</li>
<li>methods: 本研究使用了许多distillation-type losses，包括L2-based distillation方法，称为SquareHead，以实现高精度的恢复，即使在更高的精度下。</li>
<li>results: 研究表明，使用 sparse LLMs 可以实现CPU和GPU运行时的速度提高，同时保持准确性。此外，在内存绑定的LLMs中，精度可以通过精度来降低内存带宽。研究还展示了 T5（语言翻译）、Whisper（语音翻译）和 open GPT-type（文本生成）等应用场景中的综合结果，并证明了精度可以达75%，而不会影响准确性。<details>
<summary>Abstract</summary>
We consider the problem of accurate sparse fine-tuning of large language models (LLMs), that is, fine-tuning pretrained LLMs on specialized tasks, while inducing sparsity in their weights. On the accuracy side, we observe that standard loss-based fine-tuning may fail to recover accuracy, especially at high sparsities. To address this, we perform a detailed study of distillation-type losses, determining an L2-based distillation approach we term SquareHead which enables accurate recovery even at higher sparsities, across all model types. On the practical efficiency side, we show that sparse LLMs can be executed with speedups by taking advantage of sparsity, for both CPU and GPU runtimes. While the standard approach is to leverage sparsity for computational reduction, we observe that in the case of memory-bound LLMs sparsity can also be leveraged for reducing memory bandwidth. We exhibit end-to-end results showing speedups due to sparsity, while recovering accuracy, on T5 (language translation), Whisper (speech translation), and open GPT-type (MPT for text generation). For MPT text generation, we show for the first time that sparse fine-tuning can reach 75% sparsity without accuracy drops, provide notable end-to-end speedups for both CPU and GPU inference, and highlight that sparsity is also compatible with quantization approaches. Models and software for reproducing our results are provided in Section 6.
</details>
<details>
<summary>摘要</summary>
我们考虑了大型语言模型（LLM）的精确简洁训练问题，即在特殊任务上训练预训练的 LLM，而使其权重产生简洁。从准确性角度来看，我们发现，使用标准损失函数的训练可能无法恢复准确性，特别是在高度简洁的情况下。为解决这个问题，我们进行了详细的研究，找到了一种基于L2的激发型损失函数，我们称之为对角方法（SquareHead），这个方法可以在高度简洁的情况下确保准确性的回复。从实际效率角度来看，我们发现，简洁的 LLM 可以通过利用简洁来提高 CPU 和 GPU 的执行速度。而标准的方法是利用简洁来降低计算量，我们发现在承载受限的 LLM 中，简洁也可以用来降低内存带宽。我们展示了实际结果，显示了因简洁而获得的优化速度，同时保持准确性，在 T5（语言翻译）、Whisper（语音翻译）和开放 GPT-type（MPT  для文本生成）上。在 MPT 文本生成中，我们发现简洁训练可以 дости到 75% 的简洁水准，而不会对准确性造成负面的影响，并且在 CPU 和 GPU 执行中获得了明显的优化速度。此外，我们还发现简洁可以与量化方法相容。我们在 Section 6 中提供了模型和软件来重现我们的结果。
</details></li>
</ul>
<hr>
<h2 id="PICProp-Physics-Informed-Confidence-Propagation-for-Uncertainty-Quantification"><a href="#PICProp-Physics-Informed-Confidence-Propagation-for-Uncertainty-Quantification" class="headerlink" title="PICProp: Physics-Informed Confidence Propagation for Uncertainty Quantification"></a>PICProp: Physics-Informed Confidence Propagation for Uncertainty Quantification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06923">http://arxiv.org/abs/2310.06923</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shenqianli/picprop">https://github.com/shenqianli/picprop</a></li>
<li>paper_authors: Qianli Shen, Wai Hoh Tang, Zhun Deng, Apostolos Psaros, Kenji Kawaguchi</li>
<li>for: 这篇论文是关于 Deep Learning 和 Physic-Informed Learning 中的不确定性评估问题的研究。</li>
<li>methods: 该论文提出了一种新的方法，即通过 Bi-level 优化来计算具有概率保证的确定范围。</li>
<li>results: 该论文通过计算实验证明了其方法的有效性，并提供了一个定理来证明方法的正确性。<details>
<summary>Abstract</summary>
Standard approaches for uncertainty quantification in deep learning and physics-informed learning have persistent limitations. Indicatively, strong assumptions regarding the data likelihood are required, the performance highly depends on the selection of priors, and the posterior can be sampled only approximately, which leads to poor approximations because of the associated computational cost. This paper introduces and studies confidence interval (CI) estimation for deterministic partial differential equations as a novel problem. That is, to propagate confidence, in the form of CIs, from data locations to the entire domain with probabilistic guarantees. We propose a method, termed Physics-Informed Confidence Propagation (PICProp), based on bi-level optimization to compute a valid CI without making heavy assumptions. We provide a theorem regarding the validity of our method, and computational experiments, where the focus is on physics-informed learning.
</details>
<details>
<summary>摘要</summary>
The proposed method, called Physics-Informed Confidence Propagation (PICProp), is based on bi-level optimization and can compute a valid CI without making heavy assumptions. The paper provides a theorem on the validity of the method and conducts computational experiments, focusing on physics-informed learning.Translated into Simplified Chinese:传统的深度学习和物理学习不确定性评估方法具有持续的限制。例如，它们需要强大地假设数据的可能性，高度依赖于采样的选择，并且只能 aproximate posterior，导致因计算成本而得到的approximation是poor的。这篇论文介绍了和研究了确idenceInterval（CI）估计方法，即从数据位置传播 confidence 到整个领域，并提供了可靠的 probabilistic  garanties。提议的方法，称为Physics-Informed Confidence Propagation（PICProp），基于双层优化，可以计算一个有效的CI无需做出重大假设。文章提供了有关方法的有效性的定理，并进行了计算实验，主要关注物理学习。
</details></li>
</ul>
<hr>
<h2 id="Distributed-Transfer-Learning-with-4th-Gen-Intel-Xeon-Processors"><a href="#Distributed-Transfer-Learning-with-4th-Gen-Intel-Xeon-Processors" class="headerlink" title="Distributed Transfer Learning with 4th Gen Intel Xeon Processors"></a>Distributed Transfer Learning with 4th Gen Intel Xeon Processors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06916">http://arxiv.org/abs/2310.06916</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lakshmi Arunachalam, Fahim Mohammad, Vrushabh H. Sanghavi</li>
<li>for: 这篇论文探讨了如何通过转移学习和 intel xeon 处理器，尤其是第四代 intel xeon 扩展处理器，超越训练主要依赖于 GPU 的传统信念。</li>
<li>methods: 这篇论文使用了 Intel Advanced Matrix Extensions(AMX) 和分布式训练 Horovod 实现了在公共可用的 TensorFlow 图像分类数据集上达到近状态艺术精度的image classification。</li>
<li>results: 研究人员通过使用 Intel Xeon 处理器和 AMX 实现了分布式训练，在图像分类任务上达到了 near state-of-the-art 精度。<details>
<summary>Abstract</summary>
In this paper, we explore how transfer learning, coupled with Intel Xeon, specifically 4th Gen Intel Xeon scalable processor, defies the conventional belief that training is primarily GPU-dependent. We present a case study where we achieved near state-of-the-art accuracy for image classification on a publicly available Image Classification TensorFlow dataset using Intel Advanced Matrix Extensions(AMX) and distributed training with Horovod.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们探讨了如何使用传输学习，特别是使用四代英特尔Xeon可扩展处理器，推翻了训练主要依赖于GPU的传统信念。我们提出了一个案例研究，在公共可用的TensorFlow图像分类 dataset上使用英特尔高级矩阵扩展（AMX）和分布式训练 Horovod 实现了near状态艺点精度的图像分类。
</details></li>
</ul>
<hr>
<h2 id="Reinforcement-Learning-in-a-Safety-Embedded-MDP-with-Trajectory-Optimization"><a href="#Reinforcement-Learning-in-a-Safety-Embedded-MDP-with-Trajectory-Optimization" class="headerlink" title="Reinforcement Learning in a Safety-Embedded MDP with Trajectory Optimization"></a>Reinforcement Learning in a Safety-Embedded MDP with Trajectory Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06903">http://arxiv.org/abs/2310.06903</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fan Yang, Wenxuan Zhou, Zuxin Liu, Ding Zhao, David Held</li>
<li>for: 本研究旨在应用RL算法到安全关键的实际应用中，协调最大化奖励和遵循安全限制的负担。</li>
<li>methods: 该方法将RL算法与轨迹优化结合，以有效地管理奖励和安全限制之间的负担。在行动空间中嵌入安全限制，RL代理生成安全轨迹，从而提高训练稳定性和实际应用可行性。</li>
<li>results: 该方法在安全训练 tasks 中表现出色，在推理中获得了显著更高的奖励和近于零的安全违反率。此外，通过实际投入一个实际任务中的箱推进，证明了该方法的实际可行性。<details>
<summary>Abstract</summary>
Safe Reinforcement Learning (RL) plays an important role in applying RL algorithms to safety-critical real-world applications, addressing the trade-off between maximizing rewards and adhering to safety constraints. This work introduces a novel approach that combines RL with trajectory optimization to manage this trade-off effectively. Our approach embeds safety constraints within the action space of a modified Markov Decision Process (MDP). The RL agent produces a sequence of actions that are transformed into safe trajectories by a trajectory optimizer, thereby effectively ensuring safety and increasing training stability. This novel approach excels in its performance on challenging Safety Gym tasks, achieving significantly higher rewards and near-zero safety violations during inference. The method's real-world applicability is demonstrated through a safe and effective deployment in a real robot task of box-pushing around obstacles.
</details>
<details>
<summary>摘要</summary>
安全强化学习（RL）在实现RL算法应用于安全关键实际应用中扮演着重要的角色，解决最大化奖励和遵从安全约束之间的负担。这项工作介绍了一种新的方法，即将RL与轨迹优化结合起来管理这种负担。我们的方法将安全约束嵌入到修改后的Markov决策过程（MDP）中的动作空间中。RL机器人生成一系列动作，然后这些动作被一个轨迹优化器转换成安全轨迹，从而确保安全性和提高训练稳定性。这种新的方法在安全启发任务中表现出色，在推理过程中获得了明显更高的奖励和near-zero的安全违反。此外，我们还通过一个真实的 робот任务——推箱避障来证明该方法的实际应用性。
</details></li>
</ul>
<hr>
<h2 id="Scalable-Semantic-Non-Markovian-Simulation-Proxy-for-Reinforcement-Learning"><a href="#Scalable-Semantic-Non-Markovian-Simulation-Proxy-for-Reinforcement-Learning" class="headerlink" title="Scalable Semantic Non-Markovian Simulation Proxy for Reinforcement Learning"></a>Scalable Semantic Non-Markovian Simulation Proxy for Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06835">http://arxiv.org/abs/2310.06835</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaustuv Mukherji, Devendra Parkar, Lahari Pokala, Dyuman Aditya, Paulo Shakarian, Clark Dorman</li>
<li>for: 提高RL应用领域的可扩展性、可解释性和非马歇维安 assumptions。</li>
<li>methods: 使用一种基于时间扩展的策略来模拟 simulation，从而提高RL训练算法的可扩展性和可解释性。</li>
<li>results: 与两个高精度模拟器进行比较，速度提高三个数量级，保持策略学习质量。同时，可以模拟和利用非马歇维安的动力学和即时行动，并提供可解释的跟踪来描述代理行为的结果。<details>
<summary>Abstract</summary>
Recent advances in reinforcement learning (RL) have shown much promise across a variety of applications. However, issues such as scalability, explainability, and Markovian assumptions limit its applicability in certain domains. We observe that many of these shortcomings emanate from the simulator as opposed to the RL training algorithms themselves. As such, we propose a semantic proxy for simulation based on a temporal extension to annotated logic. In comparison with two high-fidelity simulators, we show up to three orders of magnitude speed-up while preserving the quality of policy learned. In addition, we show the ability to model and leverage non-Markovian dynamics and instantaneous actions while providing an explainable trace describing the outcomes of the agent actions.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Mistral-7B"><a href="#Mistral-7B" class="headerlink" title="Mistral 7B"></a>Mistral 7B</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06825">http://arxiv.org/abs/2310.06825</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mistralai/mistral-src">https://github.com/mistralai/mistral-src</a></li>
<li>paper_authors: Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed</li>
<li>for: 本研究的目的是开发一个70亿参数的语言模型，以提高表现和效率。</li>
<li>methods: 本研究使用 grouped-query attention（GQA）和滑动窗口注意（SWA）来提高启发速度，并在字符串序列中处理任意长度的序列。</li>
<li>results: 对比LLAMA 2 13B和LLAMA 1 34B，Mistral 7B在评估标准卷积中表现出色，并在逻辑、数学和代码生成方面超越Llama 2 13B。此外，Mistral 7B – Instruct模型在人工和自动评测标准中也表现出优异。<details>
<summary>Abstract</summary>
We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.
</details>
<details>
<summary>摘要</summary>
我们介绍Mistral 7B v0.1，一个引擎ered for superior performance和效率的700亿个参数语言模型。Mistral 7B在所有评估标准上都超过Llama 2 13B，并且在推理、数学和代码生成方面也超过Llama 1 34B。我们的模型利用分组查询注意力（GQA）来提高推理速度，同时使用滑块窗口注意力（SWA）来有效地处理序列的任意长度，具有降低推理成本的特性。我们还提供了一个遵循指令的模型，Mistral 7B -- Instruct，与Llama 2 13B -- Chat模型在人工和自动评估标准上都超过。我们的模型根据Apache 2.0 license发布。
</details></li>
</ul>
<hr>
<h2 id="The-Geometry-of-Truth-Emergent-Linear-Structure-in-Large-Language-Model-Representations-of-True-False-Datasets"><a href="#The-Geometry-of-Truth-Emergent-Linear-Structure-in-Large-Language-Model-Representations-of-True-False-Datasets" class="headerlink" title="The Geometry of Truth: Emergent Linear Structure in Large Language Model Representations of True&#x2F;False Datasets"></a>The Geometry of Truth: Emergent Linear Structure in Large Language Model Representations of True&#x2F;False Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06824">http://arxiv.org/abs/2310.06824</a></li>
<li>repo_url: None</li>
<li>paper_authors: Samuel Marks, Max Tegmark</li>
<li>for: 这 paper 的目的是研究语言模型（LLM）是否可以正确地表达真假信息。</li>
<li>methods: 这 paper 使用了训练探针来检测 LLM 是否输出真假信息，并利用了三种线索来研究 LLM 表达真假信息的结构：1. 视觉化 LLM 真假声明表示结构，显示了明确的线性结构。2. 传输实验，在不同数据集上使用同一个探针进行探测。3.  causal 证据，通过对 LLM 的前进传递进行手动修改，使其对假声明视为真 и vice versa。</li>
<li>results: 这 paper 发现，语言模型 linearly 表示真假信息。此外，这 paper 还引入了一种新的探针技术——质量均值探针，它比其他探针技术更好地泛化和更直接地关联到模型输出。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have impressive capabilities, but are also prone to outputting falsehoods. Recent work has developed techniques for inferring whether a LLM is telling the truth by training probes on the LLM's internal activations. However, this line of work is controversial, with some authors pointing out failures of these probes to generalize in basic ways, among other conceptual issues. In this work, we curate high-quality datasets of true/false statements and use them to study in detail the structure of LLM representations of truth, drawing on three lines of evidence: 1. Visualizations of LLM true/false statement representations, which reveal clear linear structure. 2. Transfer experiments in which probes trained on one dataset generalize to different datasets. 3. Causal evidence obtained by surgically intervening in a LLM's forward pass, causing it to treat false statements as true and vice versa. Overall, we present evidence that language models linearly represent the truth or falsehood of factual statements. We also introduce a novel technique, mass-mean probing, which generalizes better and is more causally implicated in model outputs than other probing techniques.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>LLM 真假声明表示的视觉化，显示了明确的线性结构。2. 将探针训练在一个数据集上，并将其应用于不同的数据集的转移实验。3. LLM 的前进传输中的外科手段，使其对假话视为真话，并 vice versa。总的来说，我们发现 LLM Linearly 表示真假的事实性。我们还介绍了一种新的技术——质量均值探针，它比其他探针更好地泛化和更直接地关联到模型输出。</details></li>
</ol>
<hr>
<h2 id="NECO-NEural-Collapse-Based-Out-of-distribution-detection"><a href="#NECO-NEural-Collapse-Based-Out-of-distribution-detection" class="headerlink" title="NECO: NEural Collapse Based Out-of-distribution detection"></a>NECO: NEural Collapse Based Out-of-distribution detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06823">http://arxiv.org/abs/2310.06823</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mouïn Ben Ammar, Nacim Belkhir, Sebastian Popescu, Antoine Manzanera, Gianni Franchi</li>
<li>for: 检测机器学习模型中的 OUT-OF-DISTRIBUTION (OOD) 数据，以避免模型过于自信，不具备知识frontier的意识。</li>
<li>methods: 我们提出了一种名为 NECO的新的后处方法，利用神经网络崩溃的几何特性和主成分空间的特性来识别 OOD 数据。</li>
<li>results: 我们的实验表明，NECO 可以在小规模和大规模 OOD 检测任务中达到状态之Art的结果，并且具有强大的泛化能力，可以在不同的网络架构上展示出优秀的表现。<details>
<summary>Abstract</summary>
Detecting out-of-distribution (OOD) data is a critical challenge in machine learning due to model overconfidence, often without awareness of their epistemological limits. We hypothesize that ``neural collapse'', a phenomenon affecting in-distribution data for models trained beyond loss convergence, also influences OOD data. To benefit from this interplay, we introduce NECO, a novel post-hoc method for OOD detection, which leverages the geometric properties of ``neural collapse'' and of principal component spaces to identify OOD data. Our extensive experiments demonstrate that NECO achieves state-of-the-art results on both small and large-scale OOD detection tasks while exhibiting strong generalization capabilities across different network architectures. Furthermore, we provide a theoretical explanation for the effectiveness of our method in OOD detection. We plan to release the code after the anonymity period.
</details>
<details>
<summary>摘要</summary>
检测对外分布（OOD）数据是机器学习中的关键挑战，因为模型具有自信心，通常不知道其知识理论上的限制。我们假设“神经塌陷”，一种影响在 Distribution 数据上的模型训练过程中的现象，也影响 OOD 数据。为了利用这种关系，我们介绍了 NECO，一种新的后处方法 для OOD 检测，它利用神经网络在主成分空间的几何性质和“神经塌陷”的特性来标识 OOD 数据。我们的广泛实验表明，NECO 在小规模和大规模 OOD 检测任务上具有最佳的 результаaten，并且具有强大的泛化能力，可以在不同的网络架构上展现出优异表现。此外，我们还提供了对 NECO 方法在 OOD 检测中的理论解释。我们计划在匿名期结束后发布代码。
</details></li>
</ul>
<hr>
<h2 id="Advancing-Transformer’s-Capabilities-in-Commonsense-Reasoning"><a href="#Advancing-Transformer’s-Capabilities-in-Commonsense-Reasoning" class="headerlink" title="Advancing Transformer’s Capabilities in Commonsense Reasoning"></a>Advancing Transformer’s Capabilities in Commonsense Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06803">http://arxiv.org/abs/2310.06803</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bryanzhou008/advancing-commonsense-reasoning">https://github.com/bryanzhou008/advancing-commonsense-reasoning</a></li>
<li>paper_authors: Yu Zhou, Yunqiu Han, Hanyu Zhou, Yulun Wu</li>
<li>for: 提高通用预训练语言模型在常识理解任务中的性能。</li>
<li>methods:  INTRODUCE current ML-based methods, including knowledge transfer, model ensemble, and introducing an additional pairwise contrastive objective.</li>
<li>results:  our best model outperforms the strongest previous works by ~15% absolute gains in Pairwise Accuracy and ~8.7% absolute gains in Standard Accuracy.Here’s the full Chinese text:</li>
<li>for: 这篇论文目的是提高通用预训练语言模型在常识理解任务中的性能。</li>
<li>methods: 这篇论文使用了现有的机器学习方法，包括知识传递、模型ensemble和增加对比目标。</li>
<li>results: 我们的最佳模型与最强前一个工作相比，在对比精度和标准精度上增加了约15%和8.7%的绝对提升。<details>
<summary>Abstract</summary>
Recent advances in general purpose pre-trained language models have shown great potential in commonsense reasoning. However, current works still perform poorly on standard commonsense reasoning benchmarks including the Com2Sense Dataset. We argue that this is due to a disconnect with current cutting-edge machine learning methods. In this work, we aim to bridge the gap by introducing current ML-based methods to improve general purpose pre-trained language models in the task of commonsense reasoning. Specifically, we experiment with and systematically evaluate methods including knowledge transfer, model ensemble, and introducing an additional pairwise contrastive objective. Our best model outperforms the strongest previous works by ~15\% absolute gains in Pairwise Accuracy and ~8.7\% absolute gains in Standard Accuracy.
</details>
<details>
<summary>摘要</summary>
近期大规模普通语言模型的进步已经表现出了很大的潜力，但现有工作仍然在标准的常识理解benchmark上表现不佳。我们认为这是因为现有的机器学习方法和普通语言模型之间存在一个分隔。在这个工作中，我们希望通过引入当前的机器学习方法来改善通用语言模型在常识理解任务中的性能。具体来说，我们实验了并系统地评估了知识传递、模型集成和添加对比对象的方法。我们的best模型在对比精度和标准精度上都有约15%的绝对提升，即使是与最强的前一代工作相比也有8.7%的绝对提升。
</details></li>
</ul>
<hr>
<h2 id="f-Policy-Gradients-A-General-Framework-for-Goal-Conditioned-RL-using-f-Divergences"><a href="#f-Policy-Gradients-A-General-Framework-for-Goal-Conditioned-RL-using-f-Divergences" class="headerlink" title="$f$-Policy Gradients: A General Framework for Goal Conditioned RL using $f$-Divergences"></a>$f$-Policy Gradients: A General Framework for Goal Conditioned RL using $f$-Divergences</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06794">http://arxiv.org/abs/2310.06794</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siddhant Agarwal, Ishan Durugkar, Peter Stone, Amy Zhang</li>
<li>for: 这个论文的目的是解决目标受益学习（RL）问题中的稀盐奖励问题，通过学习一个稠密奖励函数来提高政策优化问题。</li>
<li>methods: 这个论文提出了一种新的探索促进方法called $f$-Policy Gradients（$f$-PG），它利用了状态访问分布与目标之间的f- divergence来逼近优化问题。 authors derive gradients for various f-divergences来优化这个目标。</li>
<li>results: 论文的实验结果表明，$f$-PG比标准的政策升降方法在一个具有挑战性的网格世界以及Point Maze和FetchReach环境中表现更好。<details>
<summary>Abstract</summary>
Goal-Conditioned Reinforcement Learning (RL) problems often have access to sparse rewards where the agent receives a reward signal only when it has achieved the goal, making policy optimization a difficult problem. Several works augment this sparse reward with a learned dense reward function, but this can lead to sub-optimal policies if the reward is misaligned. Moreover, recent works have demonstrated that effective shaping rewards for a particular problem can depend on the underlying learning algorithm. This paper introduces a novel way to encourage exploration called $f$-Policy Gradients, or $f$-PG. $f$-PG minimizes the f-divergence between the agent's state visitation distribution and the goal, which we show can lead to an optimal policy. We derive gradients for various f-divergences to optimize this objective. Our learning paradigm provides dense learning signals for exploration in sparse reward settings. We further introduce an entropy-regularized policy optimization objective, that we call $state$-MaxEnt RL (or $s$-MaxEnt RL) as a special case of our objective. We show that several metric-based shaping rewards like L2 can be used with $s$-MaxEnt RL, providing a common ground to study such metric-based shaping rewards with efficient exploration. We find that $f$-PG has better performance compared to standard policy gradient methods on a challenging gridworld as well as the Point Maze and FetchReach environments. More information on our website https://agarwalsiddhant10.github.io/projects/fpg.html.
</details>
<details>
<summary>摘要</summary>
goal-conditioned reinforcement learning（RL）问题经常会遇到罕见的奖励，agent只有当它完成目标时才会获得奖励信号，这使得政策优化成为一个困难的问题。一些工作会在罕见的奖励上添加学习的权重函数，但这可能会导致不优化的政策。此外，最近的研究表明，有效的形状奖励可以与学习算法相关。这篇论文介绍了一种新的探索促进方法，称为f-政策Gradient（f-PG）。f-PG将减少f-分布之间的差异，我们显示这可以导致最佳政策。我们Derive gradients for various f-divergences to optimize this objective。我们的学习模式可以在罕见奖励设置下提供权重学习信号，以便探索。此外，我们还引入了一个 entropy-regularized policy optimization objective，称为state-MaxEnt RL（s-MaxEnt RL），这是我们的目标之一。我们显示可以使用L2等度量基于的形状奖励，并且可以与efficient exploration相结合。我们发现f-PG比标准的政策梯度方法在一个复杂的网格世界以及Point Maze和FetchReach环境中表现更好。更多信息请访问我们的网站https://agarwalsiddhant10.github.io/projects/fpg.html。
</details></li>
</ul>
<hr>
<h2 id="OpenWebMath-An-Open-Dataset-of-High-Quality-Mathematical-Web-Text"><a href="#OpenWebMath-An-Open-Dataset-of-High-Quality-Mathematical-Web-Text" class="headerlink" title="OpenWebMath: An Open Dataset of High-Quality Mathematical Web Text"></a>OpenWebMath: An Open Dataset of High-Quality Mathematical Web Text</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06786">http://arxiv.org/abs/2310.06786</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/keirp/OpenWebMath">https://github.com/keirp/OpenWebMath</a></li>
<li>paper_authors: Keiran Paster, Marco Dos Santos, Zhangir Azerbayev, Jimmy Ba</li>
<li>for: 提高大语言模型的逻辑能力</li>
<li>methods: 使用高质量、仔细设计的代码或数学Token进行预训练，并从Common Crawl上获取14.7亿个数学网页，使用文本和LaTeX内容抽取器，并进行质量筛选和去重。</li>
<li>results: 训练使用OpenWebMath数据集的1.4亿参数语言模型，模型性能超过了训练在大量通用语言数据上的模型。<details>
<summary>Abstract</summary>
There is growing evidence that pretraining on high quality, carefully thought-out tokens such as code or mathematics plays an important role in improving the reasoning abilities of large language models. For example, Minerva, a PaLM model finetuned on billions of tokens of mathematical documents from arXiv and the web, reported dramatically improved performance on problems that require quantitative reasoning. However, because all known open source web datasets employ preprocessing that does not faithfully preserve mathematical notation, the benefits of large scale training on quantitive web documents are unavailable to the research community. We introduce OpenWebMath, an open dataset inspired by these works containing 14.7B tokens of mathematical webpages from Common Crawl. We describe in detail our method for extracting text and LaTeX content and removing boilerplate from HTML documents, as well as our methods for quality filtering and deduplication. Additionally, we run small-scale experiments by training 1.4B parameter language models on OpenWebMath, showing that models trained on 14.7B tokens of our dataset surpass the performance of models trained on over 20x the amount of general language data. We hope that our dataset, openly released on the Hugging Face Hub, will help spur advances in the reasoning abilities of large language models.
</details>
<details>
<summary>摘要</summary>
有证据显示，预训练于高质量、仔细设计的令符，如代码或数学文档，对大型自然语言模型的理解能力产生重要的影响。例如，Minerva模型，通过对数百亿个数学文档从arXiv和网络上进行微调，显著提高了需要量化逻辑的问题的性能。然而，由于所有已知的开源网络数据集都会对数学notation进行不准确的预处理，因此大规模在量化网络文档上进行训练的 beneficial effects 是无法对研究社区提供。我们介绍了 OpenWebMath 数据集，它是基于这些工作的开放数据集，包含 14.7 亿个数学网页FROM Common Crawl。我们详细描述了提取文本和LaTeX内容，并从 HTML 文档中 removing boilerplate 的方法，以及质量筛选和重复 elimination 的方法。此外，我们对 OpenWebMath 数据集进行了小规模实验，证明模型在 14.7 亿个令符上进行训练后，性能超过了在大量常见语言数据上进行训练后的性能。我们希望 OpenWebMath 数据集，通过在 Hugging Face Hub 上公开发布，能够促进大型自然语言模型的理解能力。
</details></li>
</ul>
<hr>
<h2 id="A-Supervised-Embedding-and-Clustering-Anomaly-Detection-method-for-classification-of-Mobile-Network-Faults"><a href="#A-Supervised-Embedding-and-Clustering-Anomaly-Detection-method-for-classification-of-Mobile-Network-Faults" class="headerlink" title="A Supervised Embedding and Clustering Anomaly Detection method for classification of Mobile Network Faults"></a>A Supervised Embedding and Clustering Anomaly Detection method for classification of Mobile Network Faults</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06779">http://arxiv.org/abs/2310.06779</a></li>
<li>repo_url: None</li>
<li>paper_authors: R. Mosayebi, H. Kia, A. Kianpour Raki</li>
<li>For: 这篇论文旨在解决移动网络中异常警示日志的效率识别问题，降低手动监测的困难，帮助网络维护人员更快地发现和解决问题。* Methods: 该方法使用超级vised Embedding和集群异常检测（SEMC-AD），利用历史异常警示日志和其标签来EXTRACT数字表示方法，有效地解决异常警示日志数据集中异常分类的问题，不需要使用一hot编码。* Results: 实验表明，SEMC-AD方法可以准确地识别异常警示日志，其 anomaly detection 率为 99%，而Random Forest和XGBoost方法只能检测到 86% 和 81% 的异常。 SEMC-AD 方法在具有多个分类特征的数据集中表现更高效，可以快速地发现和解决问题，减轻网络维护人员的负担。<details>
<summary>Abstract</summary>
The paper introduces Supervised Embedding and Clustering Anomaly Detection (SEMC-AD), a method designed to efficiently identify faulty alarm logs in a mobile network and alleviate the challenges of manual monitoring caused by the growing volume of alarm logs. SEMC-AD employs a supervised embedding approach based on deep neural networks, utilizing historical alarm logs and their labels to extract numerical representations for each log, effectively addressing the issue of imbalanced classification due to a small proportion of anomalies in the dataset without employing one-hot encoding. The robustness of the embedding is evaluated by plotting the two most significant principle components of the embedded alarm logs, revealing that anomalies form distinct clusters with similar embeddings. Multivariate normal Gaussian clustering is then applied to these components, identifying clusters with a high ratio of anomalies to normal alarms (above 90%) and labeling them as the anomaly group. To classify new alarm logs, we check if their embedded vectors' two most significant principle components fall within the anomaly-labeled clusters. If so, the log is classified as an anomaly. Performance evaluation demonstrates that SEMC-AD outperforms conventional random forest and gradient boosting methods without embedding. SEMC-AD achieves 99% anomaly detection, whereas random forest and XGBoost only detect 86% and 81% of anomalies, respectively. While supervised classification methods may excel in labeled datasets, the results demonstrate that SEMC-AD is more efficient in classifying anomalies in datasets with numerous categorical features, significantly enhancing anomaly detection, reducing operator burden, and improving network maintenance.
</details>
<details>
<summary>摘要</summary>
文章介绍了一种名为Supervised Embedding and Clustering Anomaly Detection（SEMC-AD）的方法，用于高效地在移动网络中识别异常报警日志并减轻人工监测的困难，由于报警日志的数量不断增加。SEMC-AD利用深度神经网络的超级vised embedding方法，使用历史报警日志和其标签来提取每个日志的数字表示，有效解决了因数据集中异常的比例较小而导致的一类问题，不需要使用一hot编码。随后，对这些Component进行多ivariate normal Gaussian clustering，可以快速地标识异常类型的报警日志。为了分类新的报警日志，我们只需要检查其embeddedvector的两个最重要的主成分是否 falls within the anomaly-labeled clusters。如果是，则将日志分类为异常。性能评估表明，SEMC-AD比无 embedding的Random Forest和XGBoost方法更高效，SEMC-AD可以识别99%的异常报警，而Random Forest和XGBoost只能识别86%和81%的异常报警。虽然超级vised分类方法在标注数据集中可能会出色，但结果表明SEMC-AD在具有多个分类特征的数据集中更高效地识别异常，提高异常检测率，减轻操作员的负担，改善网络维护。
</details></li>
</ul>
<hr>
<h2 id="Correlated-Noise-Provably-Beats-Independent-Noise-for-Differentially-Private-Learning"><a href="#Correlated-Noise-Provably-Beats-Independent-Noise-for-Differentially-Private-Learning" class="headerlink" title="Correlated Noise Provably Beats Independent Noise for Differentially Private Learning"></a>Correlated Noise Provably Beats Independent Noise for Differentially Private Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06771">http://arxiv.org/abs/2310.06771</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christopher A. Choquette-Choo, Krishnamurthy Dvijotham, Krishna Pillutla, Arun Ganesh, Thomas Steinke, Abhradeep Thakurta</li>
<li>for: 隐私学习算法中添加噪声以提高学习效果。</li>
<li>methods: 使用相关噪声 Mechanisms，提供了有效的分解函数，并给出了精确的分布 bound。</li>
<li>results: 比对普通DP-SGD，相关噪声可以提高学习效果，并且可以避免 cube 复杂度。实验 validate 了我们的理论。<details>
<summary>Abstract</summary>
Differentially private learning algorithms inject noise into the learning process. While the most common private learning algorithm, DP-SGD, adds independent Gaussian noise in each iteration, recent work on matrix factorization mechanisms has shown empirically that introducing correlations in the noise can greatly improve their utility. We characterize the asymptotic learning utility for any choice of the correlation function, giving precise analytical bounds for linear regression and as the solution to a convex program for general convex functions. We show, using these bounds, how correlated noise provably improves upon vanilla DP-SGD as a function of problem parameters such as the effective dimension and condition number. Moreover, our analytical expression for the near-optimal correlation function circumvents the cubic complexity of the semi-definite program used to optimize the noise correlation matrix in previous work. We validate our theory with experiments on private deep learning. Our work matches or outperforms prior work while being efficient both in terms of compute and memory.
</details>
<details>
<summary>摘要</summary>
diferencialmente privado 学习算法加入噪声到学习过程中。而最常见的私人学习算法DP-SGD每次迭代添加独立的 Gaussian 噪声，而最近的矩阵分解机制研究表明，在噪声中引入相关性可以大大提高其用用。我们Characterize the asymptotic learning utility for any choice of the correlation function, giving precise analytical bounds for linear regression and as the solution to a convex program for general convex functions。我们通过这些bound，证明相关噪声可以超过原生DP-SGD的性能，随着问题参数如有效维度和condition number的变化。此外，我们的分析表达式可以避免之前的cubic complexity的半definite program用于优化噪声相关矩阵。我们通过实验 validate our theory on private deep learning，我们的工作与之前的工作匹配或超越，同时在计算和内存方面都是高效的。
</details></li>
</ul>
<hr>
<h2 id="SWE-bench-Can-Language-Models-Resolve-Real-World-GitHub-Issues"><a href="#SWE-bench-Can-Language-Models-Resolve-Real-World-GitHub-Issues" class="headerlink" title="SWE-bench: Can Language Models Resolve Real-World GitHub Issues?"></a>SWE-bench: Can Language Models Resolve Real-World GitHub Issues?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06770">http://arxiv.org/abs/2310.06770</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/princeton-nlp/SWE-bench">https://github.com/princeton-nlp/SWE-bench</a></li>
<li>paper_authors: Carlos E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, Karthik Narasimhan</li>
<li>for: 本研究的目的是评估语言模型在实际软件工程中的能力。</li>
<li>methods: 本研究使用了$2,294$个真实 GitHub issues 和对应的 pull requests，在 $12$ 个流行 Python 仓库中进行评估。</li>
<li>results: 研究发现，当给出一个代码库和一个问题描述时，当前状态艺AE模型和我们精度调整的模型 SWE-Llama 只能解决最简单的问题。 Claude 2 和 GPT-4 只能解决 $4.8%$ 和 $1.7%$ 的实例。<details>
<summary>Abstract</summary>
Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We consider real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. We therefore introduce SWE-bench, an evaluation framework including $2,294$ software engineering problems drawn from real GitHub issues and corresponding pull requests across $12$ popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation. Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. Claude 2 and GPT-4 solve a mere $4.8$% and $1.7$% of instances respectively, even when provided with an oracle retriever. Advances on SWE-bench represent steps towards LMs that are more practical, intelligent, and autonomous.
</details>
<details>
<summary>摘要</summary>
语言模型已经超过了我们评估它们的能力，但为其未来发展， изу究这些模型的前沿是非常重要。我们认为实际的软件工程是一个丰富、可持续和挑战性的测试环境，可以用于评估下一代语言模型。因此，我们介绍了 SWE-bench，一个评估框架，包括 $2,294$ 个实际 GitHub 问题和相应的 pull request  across $12$ 个流行的 Python 存储库。给定一个代码库以及一个问题的描述，一个语言模型需要编辑代码库以解决问题。在 SWE-bench 中解决问题 frequently 需要理解和协调多个函数、类和文件之间的更改，需要模型与执行环境交互，处理极长的上下文，并进行复杂的逻辑分析，这些都超出了传统代码生成的范畴。我们的评估结果显示，当前的商业化模型和我们练化的模型 SWE-Llama 只能解决最简单的问题。Claude 2 和 GPT-4 只能解决 $4.8\%$ 和 $1.7\%$ 的实例，即使提供了 oracle retriever。 SWE-bench 的进步表明了语言模型的实用、智能和自主发展。
</details></li>
</ul>
<hr>
<h2 id="FABind-Fast-and-Accurate-Protein-Ligand-Binding"><a href="#FABind-Fast-and-Accurate-Protein-Ligand-Binding" class="headerlink" title="FABind: Fast and Accurate Protein-Ligand Binding"></a>FABind: Fast and Accurate Protein-Ligand Binding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06763">http://arxiv.org/abs/2310.06763</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qizhipei/fabind">https://github.com/qizhipei/fabind</a></li>
<li>paper_authors: Qizhi Pei, Kaiyuan Gao, Lijun Wu, Jinhua Zhu, Yingce Xia, Shufang Xie, Tao Qin, Kun He, Tie-Yan Liu, Rui Yan</li>
<li>for: 这个研究的目的是精确预测蛋白质和抗体之间的结合结构，以帮助药物探索过程中更好地评估药物的可能性。</li>
<li>methods: 这个研究使用了深度学习的 sampling-based 和 regression-based 方法，并提出了一个名为 $\mathbf{FABind}$ 的终端模型，它结合了袋子预测和结合估计，以提高预测的精度和速度。</li>
<li>results: 这个研究通过广泛的实验证明了 $\mathbf{FABind}$ 的优秀性能，与现有的方法相比，它在预测蛋白质和抗体之间的结合结构时表现出了更高的精度和更快的速度。<details>
<summary>Abstract</summary>
Modeling the interaction between proteins and ligands and accurately predicting their binding structures is a critical yet challenging task in drug discovery. Recent advancements in deep learning have shown promise in addressing this challenge, with sampling-based and regression-based methods emerging as two prominent approaches. However, these methods have notable limitations. Sampling-based methods often suffer from low efficiency due to the need for generating multiple candidate structures for selection. On the other hand, regression-based methods offer fast predictions but may experience decreased accuracy. Additionally, the variation in protein sizes often requires external modules for selecting suitable binding pockets, further impacting efficiency. In this work, we propose $\mathbf{FABind}$, an end-to-end model that combines pocket prediction and docking to achieve accurate and fast protein-ligand binding. $\mathbf{FABind}$ incorporates a unique ligand-informed pocket prediction module, which is also leveraged for docking pose estimation. The model further enhances the docking process by incrementally integrating the predicted pocket to optimize protein-ligand binding, reducing discrepancies between training and inference. Through extensive experiments on benchmark datasets, our proposed $\mathbf{FABind}$ demonstrates strong advantages in terms of effectiveness and efficiency compared to existing methods. Our code is available at $\href{https://github.com/QizhiPei/FABind}{Github}$.
</details>
<details>
<summary>摘要</summary>
In this work, we propose $\mathbf{FABind}$, an end-to-end model that combines pocket prediction and docking to achieve accurate and fast protein-ligand binding. $\mathbf{FABind}$ incorporates a unique ligand-informed pocket prediction module, which is also leveraged for docking pose estimation. The model further enhances the docking process by incrementally integrating the predicted pocket to optimize protein-ligand binding, reducing discrepancies between training and inference.Through extensive experiments on benchmark datasets, our proposed $\mathbf{FABind}$ demonstrates strong advantages in terms of effectiveness and efficiency compared to existing methods. Our code is available at $\href{https://github.com/QizhiPei/FABind}{Github}$.
</details></li>
</ul>
<hr>
<h2 id="Going-Beyond-Neural-Network-Feature-Similarity-The-Network-Feature-Complexity-and-Its-Interpretation-Using-Category-Theory"><a href="#Going-Beyond-Neural-Network-Feature-Similarity-The-Network-Feature-Complexity-and-Its-Interpretation-Using-Category-Theory" class="headerlink" title="Going Beyond Neural Network Feature Similarity: The Network Feature Complexity and Its Interpretation Using Category Theory"></a>Going Beyond Neural Network Feature Similarity: The Network Feature Complexity and Its Interpretation Using Category Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06756">http://arxiv.org/abs/2310.06756</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiting Chen, Zhanpeng Zhou, Junchi Yan</li>
<li>for: 本研究旨在探讨神经网络中学习过程中Feature的内在性和重复性，并提出了一种基于Category Theory的方法来量化这种重复性。</li>
<li>methods: 本研究使用了Iterative Feature Merging（IFM）算法来量化Feature的复杂性，并通过对不同神经网络的实验 validate了我们的方法和理论。</li>
<li>results: 研究发现，不同神经网络学习过程中的Feature之间存在广泛的功能等价关系，并且可以通过Iterative Feature Merging（IFM）算法来减少神经网络的参数数量无需影响性能。此外，我们还发现了一些有趣的实验结果，如Feature复杂性与神经网络性能之间的关系等。<details>
<summary>Abstract</summary>
The behavior of neural networks still remains opaque, and a recently widely noted phenomenon is that networks often achieve similar performance when initialized with different random parameters. This phenomenon has attracted significant attention in measuring the similarity between features learned by distinct networks. However, feature similarity could be vague in describing the same feature since equivalent features hardly exist. In this paper, we expand the concept of equivalent feature and provide the definition of what we call functionally equivalent features. These features produce equivalent output under certain transformations. Using this definition, we aim to derive a more intrinsic metric for the so-called feature complexity regarding the redundancy of features learned by a neural network at each layer. We offer a formal interpretation of our approach through the lens of category theory, a well-developed area in mathematics. To quantify the feature complexity, we further propose an efficient algorithm named Iterative Feature Merging. Our experimental results validate our ideas and theories from various perspectives. We empirically demonstrate that the functionally equivalence widely exists among different features learned by the same neural network and we could reduce the number of parameters of the network without affecting the performance.The IFM shows great potential as a data-agnostic model prune method. We have also drawn several interesting empirical findings regarding the defined feature complexity.
</details>
<details>
<summary>摘要</summary>
神经网络的行为仍然存在诡异性，而一个最近受到广泛关注的现象是，当不同Random参数初始化的神经网络 Initialization具有类似性。这种现象引起了评估特征之间的相似性的重要注意。然而，特征相似性可能是描述相同特征的抽象方式，因为等效特征几乎不存在。在这篇论文中，我们扩展了特征相似性的概念，并提供了我们称为功能相似特征的定义。这些特征在某些变换下产生相同的输出。使用这个定义，我们想要 derivate一种更内在的特征复杂度度量方法，用于衡量神经网络每层学习的特征复杂度。我们还提出了一种效率高的算法名为迭代特征合并（Iterative Feature Merging，IFM），用于实现这一目标。我们的实验结果证明了我们的想法和理论从多个角度来看都是正确的。我们经验显示，神经网络学习的不同特征之间存在广泛的功能相似性，并且可以通过IFM来减少神经网络的参数数量，无需影响性能。此外，我们还发现了一些有趣的实验发现，关于定义的特征复杂度。
</details></li>
</ul>
<hr>
<h2 id="Comparing-AI-Algorithms-for-Optimizing-Elliptic-Curve-Cryptography-Parameters-in-Third-Party-E-Commerce-Integrations-A-Pre-Quantum-Era-Analysis"><a href="#Comparing-AI-Algorithms-for-Optimizing-Elliptic-Curve-Cryptography-Parameters-in-Third-Party-E-Commerce-Integrations-A-Pre-Quantum-Era-Analysis" class="headerlink" title="Comparing AI Algorithms for Optimizing Elliptic Curve Cryptography Parameters in Third-Party E-Commerce Integrations: A Pre-Quantum Era Analysis"></a>Comparing AI Algorithms for Optimizing Elliptic Curve Cryptography Parameters in Third-Party E-Commerce Integrations: A Pre-Quantum Era Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06752">http://arxiv.org/abs/2310.06752</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cftellezc/GA_PSO_ECC_parameter_Optimization">https://github.com/cftellezc/GA_PSO_ECC_parameter_Optimization</a></li>
<li>paper_authors: Felipe Tellez, Jorge Ortiz</li>
<li>for: 这个研究旨在比较基因算法（GA）和物理群体优化算法（PSO），两种人工智能算法，以优化椭圆曲线密码学（ECC）参数。</li>
<li>methods: 这种研究使用了GA和PSO来优化ECC参数，包括椭圆曲线系数、素数、生成点、群体顺序和因子。</li>
<li>results: 研究发现，GA和PSO在ECC参数优化方面具有不同的优势，GA在精度方面表现较好，而PSO在稳定性方面表现较好。在模拟的电子商务环境中，使用GA和PSO优化的ECC参数和 secp256k1 比较，显示了GA和PSO在ECC参数优化方面的有效性。<details>
<summary>Abstract</summary>
This paper presents a comparative analysis between the Genetic Algorithm (GA) and Particle Swarm Optimization (PSO), two vital artificial intelligence algorithms, focusing on optimizing Elliptic Curve Cryptography (ECC) parameters. These encompass the elliptic curve coefficients, prime number, generator point, group order, and cofactor. The study provides insights into which of the bio-inspired algorithms yields better optimization results for ECC configurations, examining performances under the same fitness function. This function incorporates methods to ensure robust ECC parameters, including assessing for singular or anomalous curves and applying Pollard's rho attack and Hasse's theorem for optimization precision. The optimized parameters generated by GA and PSO are tested in a simulated e-commerce environment, contrasting with well-known curves like secp256k1 during the transmission of order messages using Elliptic Curve-Diffie Hellman (ECDH) and Hash-based Message Authentication Code (HMAC). Focusing on traditional computing in the pre-quantum era, this research highlights the efficacy of GA and PSO in ECC optimization, with implications for enhancing cybersecurity in third-party e-commerce integrations. We recommend the immediate consideration of these findings before quantum computing's widespread adoption.
</details>
<details>
<summary>摘要</summary>
The fitness function incorporates methods to ensure robust ECC parameters, such as assessing for singular or anomalous curves and applying Pollard's rho attack and Hasse's theorem for optimization precision. The optimized parameters generated by GA and PSO are tested in a simulated e-commerce environment, and compared with well-known curves like secp256k1. The study focuses on traditional computing in the pre-quantum era, and highlights the efficacy of GA and PSO in ECC optimization, with implications for enhancing cybersecurity in third-party e-commerce integrations. The findings of this research are recommended for immediate consideration before the widespread adoption of quantum computing.
</details></li>
</ul>
<hr>
<h2 id="Geographic-Location-Encoding-with-Spherical-Harmonics-and-Sinusoidal-Representation-Networks"><a href="#Geographic-Location-Encoding-with-Spherical-Harmonics-and-Sinusoidal-Representation-Networks" class="headerlink" title="Geographic Location Encoding with Spherical Harmonics and Sinusoidal Representation Networks"></a>Geographic Location Encoding with Spherical Harmonics and Sinusoidal Representation Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06743">http://arxiv.org/abs/2310.06743</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/marccoru/locationencoder">https://github.com/marccoru/locationencoder</a></li>
<li>paper_authors: Marc Rußwurm, Konstantin Klemmer, Esther Rolf, Robin Zbinden, Devis Tuia</li>
<li>for: 本研究旨在开发一种新的地理空间学习特征表示法，用于任何基于地理坐标数据的机器学习模型。</li>
<li>methods: 本研究使用了球面傅立叶函数和抛物线网络（SirenNets）来学习地理空间特征表示。</li>
<li>results: 研究发现，通过将球面傅立叶函数和抛物线网络相结合，可以实现高效地理空间特征表示，并且在不同的分类和回归任务中达到了州际级的性能。<details>
<summary>Abstract</summary>
Learning feature representations of geographical space is vital for any machine learning model that integrates geolocated data, spanning application domains such as remote sensing, ecology, or epidemiology. Recent work mostly embeds coordinates using sine and cosine projections based on Double Fourier Sphere (DFS) features -- these embeddings assume a rectangular data domain even on global data, which can lead to artifacts, especially at the poles. At the same time, relatively little attention has been paid to the exact design of the neural network architectures these functional embeddings are combined with. This work proposes a novel location encoder for globally distributed geographic data that combines spherical harmonic basis functions, natively defined on spherical surfaces, with sinusoidal representation networks (SirenNets) that can be interpreted as learned Double Fourier Sphere embedding. We systematically evaluate the cross-product of positional embeddings and neural network architectures across various classification and regression benchmarks and synthetic evaluation datasets. In contrast to previous approaches that require the combination of both positional encoding and neural networks to learn meaningful representations, we show that both spherical harmonics and sinusoidal representation networks are competitive on their own but set state-of-the-art performances across tasks when combined. We provide source code at www.github.com/marccoru/locationencoder
</details>
<details>
<summary>摘要</summary>
学习地理空间特征表示是任何结合地理数据的机器学习模型的关键环节，涵盖应用领域如远程感知、生态学和 epidemiology。现有的大部分方法使用 Double Fourier Sphere（DFS）特征来投影坐标，这些投影假设数据域是方形的，尤其是在全球数据上，这可能会导致特征扭曲，特别是在两极。同时，对于 neural network 架构的精确设计得到了相对少的关注。这项工作提议一种新的全球地理数据编码器，其将球面幂函数基函数（spherical harmonic）和投影网络（SirenNets）结合起来，可以视为学习 Double Fourier Sphere 嵌入。我们系统地评估了不同的 pozitional 编码和 neural network 架构的跨产品和人工评估数据集。与之前的方法不同，我们发现了 spherical harmonics 和投影网络是独立学习的，但是当它们组合在一起时，它们可以达到最佳性能。我们在 www.github.com/marccoru/locationencoder 提供源代码。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Memorization-in-Fine-tuned-Language-Models"><a href="#Exploring-Memorization-in-Fine-tuned-Language-Models" class="headerlink" title="Exploring Memorization in Fine-tuned Language Models"></a>Exploring Memorization in Fine-tuned Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06714">http://arxiv.org/abs/2310.06714</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shenglai Zeng, Yaxin Li, Jie Ren, Yiding Liu, Han Xu, Pengfei He, Yue Xing, Shuaiqiang Wang, Jiliang Tang, Dawei Yin</li>
<li>for: 本研究探讨了Language Model（LM）在精度调节过程中的记忆行为，以了解LM在不同任务中的记忆表现，并探讨记忆表现与LM的注意力分布之间的关系。</li>
<li>methods: 本研究使用了开源的LM和自己的调节LM，对多个任务进行了多任务调节，并通过分析LM的记忆表现和注意力分布来理解记忆行为的异同。</li>
<li>results: 研究发现，LM在不同任务中的记忆表现存在异常强的差异，并且发现了记忆表现与注意力分布之间的强相关性。此外，多任务调节被发现可以减少精度调节后的记忆表现。<details>
<summary>Abstract</summary>
LLMs have shown great capabilities in various tasks but also exhibited memorization of training data, thus raising tremendous privacy and copyright concerns. While prior work has studied memorization during pre-training, the exploration of memorization during fine-tuning is rather limited. Compared with pre-training, fine-tuning typically involves sensitive data and diverse objectives, thus may bring unique memorization behaviors and distinct privacy risks. In this work, we conduct the first comprehensive analysis to explore LMs' memorization during fine-tuning across tasks. Our studies with open-sourced and our own fine-tuned LMs across various tasks indicate that fine-tuned memorization presents a strong disparity among tasks. We provide an understanding of this task disparity via sparse coding theory and unveil a strong correlation between memorization and attention score distribution. By investigating its memorization behavior, multi-task fine-tuning paves a potential strategy to mitigate fine-tuned memorization.
</details>
<details>
<summary>摘要</summary>
LLMs 有很好的能力在不同的任务上，但也表现出储存训练数据的问题，因此引起了巨大的隐私和版权问题。在先前的工作中，研究了训练前的储存行为，但对于精度调整来说，研究储存行为的探索相对较少。相比训练前，精度调整通常涉及敏感数据和多种目标，因此可能带来唯一的储存行为和特定的隐私风险。在这项工作中，我们进行了第一次全面的分析，探索 LM 在调整过程中的储存行为。我们使用开源的 LM 和我们自己调整的 LM 在多种任务上进行了研究，发现调整后的储存强度存在任务之间的强烈差异。通过零 coding 理论和注意力分布的调查，我们了解了储存行为与注意力分布之间的强烈相关性。此外，我们发现了多任务调整可能减轻调整后的储存行为的问题。
</details></li>
</ul>
<hr>
<h2 id="Quality-Control-at-Your-Fingertips-Quality-Aware-Translation-Models"><a href="#Quality-Control-at-Your-Fingertips-Quality-Aware-Translation-Models" class="headerlink" title="Quality Control at Your Fingertips: Quality-Aware Translation Models"></a>Quality Control at Your Fingertips: Quality-Aware Translation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06707">http://arxiv.org/abs/2310.06707</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christian Tomani, David Vilar, Markus Freitag, Colin Cherry, Subhajit Naskar, Mara Finkelstein, Daniel Cremers</li>
<li>for: 提高神经机器翻译模型（NMT）的翻译质量。</li>
<li>methods: 使用神经网络自己估计输出质量，并在MAP解oding中使用这个质量信号作为提示。</li>
<li>results: 使用内置质量估计可以自动排除优化搜索空间，并在MBR解oding中提高翻译质量，同时降低搜索速度。<details>
<summary>Abstract</summary>
Maximum-a-posteriori (MAP) decoding is the most widely used decoding strategy for neural machine translation (NMT) models. The underlying assumption is that model probability correlates well with human judgment, with better translations being more likely. However, research has shown that this assumption does not always hold, and decoding strategies which directly optimize a utility function, like Minimum Bayes Risk (MBR) or Quality-Aware decoding can significantly improve translation quality over standard MAP decoding. The main disadvantage of these methods is that they require an additional model to predict the utility, and additional steps during decoding, which makes the entire process computationally demanding. In this paper, we propose to make the NMT models themselves quality-aware by training them to estimate the quality of their own output. During decoding, we can use the model's own quality estimates to guide the generation process and produce the highest-quality translations possible. We demonstrate that the model can self-evaluate its own output during translation, eliminating the need for a separate quality estimation model. Moreover, we show that using this quality signal as a prompt during MAP decoding can significantly improve translation quality. When using the internal quality estimate to prune the hypothesis space during MBR decoding, we can not only further improve translation quality, but also reduce inference speed by two orders of magnitude.
</details>
<details>
<summary>摘要</summary>
最常用的决策策略之一是最大 posteriori（MAP）解oding，用于神经机器翻译（NMT）模型。假设是，模型的概率与人类判断有高度相关， better translations 是更有可能性的。然而，研究表明，这种假设并不总是成立，而使用直接优化一个实用函数，如最小 bayes 风险（MBR）或质量意识 decoding 可以显著提高翻译质量。这些方法的主要缺点是它们需要一个额外的模型来预测实用函数，以及在解码过程中进行额外的步骤，这使得整个过程变得计算昂贵。在这篇论文中，我们提议使用 NMT 模型本身来自适应质量。在解码过程中，我们可以使用模型自己的质量估计来导引生成过程，以生成最高质量的翻译。我们示示了模型可以自我评估其自己的输出，无需额外的质量估计模型。此外，我们还表明，使用这个质量信号作为提示在 MAP 解oding 中使用可以显著提高翻译质量。当使用内部质量估计来减少假设空间中的假设时，我们可以不仅进一步提高翻译质量，还可以将推理速度减少两个数量级。
</details></li>
</ul>
<hr>
<h2 id="DeepLSH-Deep-Locality-Sensitive-Hash-Learning-for-Fast-and-Efficient-Near-Duplicate-Crash-Report-Detection"><a href="#DeepLSH-Deep-Locality-Sensitive-Hash-Learning-for-Fast-and-Efficient-Near-Duplicate-Crash-Report-Detection" class="headerlink" title="DeepLSH: Deep Locality-Sensitive Hash Learning for Fast and Efficient Near-Duplicate Crash Report Detection"></a>DeepLSH: Deep Locality-Sensitive Hash Learning for Fast and Efficient Near-Duplicate Crash Report Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06703">http://arxiv.org/abs/2310.06703</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/remilyoucef/deep-locality-sensitive-hashing">https://github.com/remilyoucef/deep-locality-sensitive-hashing</a></li>
<li>paper_authors: Youcef Remil, Anes Bendimerad, Romain Mathonat, Chedy Raissi, Mehdi Kaytoue</li>
<li>for: 这种研究是为了提高自动崩溃桶化过程中的崩溃bug报告的效率，以便更快地发现类似的报告。</li>
<li>methods: 这种研究使用了最近的相似搜索技术，即locality-sensitive hashing (LSH)，以及一种专门设计的深度学习模型（Siamese DNN），以提高崩溃bug报告的相似性搜索精度。</li>
<li>results: 研究发现，使用LSH和DeepLSH可以减少崩溃bug报告的相似性搜索时间，并且可以保证搜索结果的准确性。此外，研究还提供了一个原始数据集，以便进一步 validate 这些结果。<details>
<summary>Abstract</summary>
Automatic crash bucketing is a crucial phase in the software development process for efficiently triaging bug reports. It generally consists in grouping similar reports through clustering techniques. However, with real-time streaming bug collection, systems are needed to quickly answer the question: What are the most similar bugs to a new one?, that is, efficiently find near-duplicates. It is thus natural to consider nearest neighbors search to tackle this problem and especially the well-known locality-sensitive hashing (LSH) to deal with large datasets due to its sublinear performance and theoretical guarantees on the similarity search accuracy. Surprisingly, LSH has not been considered in the crash bucketing literature. It is indeed not trivial to derive hash functions that satisfy the so-called locality-sensitive property for the most advanced crash bucketing metrics. Consequently, we study in this paper how to leverage LSH for this task. To be able to consider the most relevant metrics used in the literature, we introduce DeepLSH, a Siamese DNN architecture with an original loss function, that perfectly approximates the locality-sensitivity property even for Jaccard and Cosine metrics for which exact LSH solutions exist. We support this claim with a series of experiments on an original dataset, which we make available.
</details>
<details>
<summary>摘要</summary>
自动化崩溃分组是软件开发过程中的一个关键阶段，用于有效地处理报告 bug 的情况。通常通过聚合技术来实现这一目标。然而，在实时流动的报告 bug 收集中，系统需要快速回答问题：新的报告 bug 与其他报告 bug 之间有哪些相似之处？因此，快速找到相似的报告 bug 变得非常重要。这使得最近邻居搜索成为一个自然的选择，特别是使用了本地敏感哈希（LSH），因为它可以在大量数据集上实现子线性性和对 Similarity search 的理论保证。尽管 LSH 在崩溃分组文献中没有被考虑，但我们在这篇论文中尝试使用它来解决这一问题。为了考虑文献中最常用的metric，我们提出了 DeepLSH，一种基于 Siamese DNN 架构的原始搜索函数，可以准确地模拟本地敏感性Property，包括 Jaccard 和 Cosine  metric 。我们通过一系列实验证明了 DeepLSH 的有效性，并提供了一个原始数据集，可以用于进一步研究。
</details></li>
</ul>
<hr>
<h2 id="Sheared-LLaMA-Accelerating-Language-Model-Pre-training-via-Structured-Pruning"><a href="#Sheared-LLaMA-Accelerating-Language-Model-Pre-training-via-Structured-Pruning" class="headerlink" title="Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning"></a>Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06694">http://arxiv.org/abs/2310.06694</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/princeton-nlp/LLM-Shearing">https://github.com/princeton-nlp/LLM-Shearing</a></li>
<li>paper_authors: Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, Danqi Chen<br>for:This paper aims to develop a cost-effective approach for building smaller language models (LLMs) from pre-trained, larger models.methods:The approach uses two key techniques: targeted structured pruning and dynamic batch loading. Targeted structured pruning prunes the larger model to a specified target shape, while dynamic batch loading dynamically updates the composition of sampled data in each training batch based on varying losses across different domains.results:The Sheared-LLaMA series, pruned from the LLaMA2-7B model, outperforms state-of-the-art open-source models of equivalent sizes on a wide range of downstream and instruction tuning evaluations, while requiring only 3% of the compute required to train such models from scratch.<details>
<summary>Abstract</summary>
The popularity of LLaMA (Touvron et al., 2023a;b) and other recently emerged moderate-sized large language models (LLMs) highlights the potential of building smaller yet powerful LLMs. Regardless, the cost of training such models from scratch on trillions of tokens remains high. In this work, we study structured pruning as an effective means to develop smaller LLMs from pre-trained, larger models. Our approach employs two key techniques: (1) targeted structured pruning, which prunes a larger model to a specified target shape by removing layers, heads, and intermediate and hidden dimensions in an end-to-end manner, and (2) dynamic batch loading, which dynamically updates the composition of sampled data in each training batch based on varying losses across different domains. We demonstrate the efficacy of our approach by presenting the Sheared-LLaMA series, pruning the LLaMA2-7B model down to 1.3B and 2.7B parameters. Sheared-LLaMA models outperform state-of-the-art open-source models of equivalent sizes, such as Pythia, INCITE, and OpenLLaMA models, on a wide range of downstream and instruction tuning evaluations, while requiring only 3% of compute compared to training such models from scratch. This work provides compelling evidence that leveraging existing LLMs with structured pruning is a far more cost-effective approach for building smaller LLMs.
</details>
<details>
<summary>摘要</summary>
LLMA (Touvron 等人，2023a;b) 的受欢迎程度和其他最近出现的中等规模的大语言模型 (LLMs) 表明了建立更小 yet 强大 LLMs 的潜力。然而，从头来训练这些模型的成本仍然高。在这个工作中，我们研究结构性剪裁作为开发更小 LLMs 的有效方法。我们的方法使用两个关键技术：（1） Targeted 结构性剪裁，剪掉一个更大的模型到指定的目标形态，包括层、头、中间和隐藏维度，并在端到端方式进行剪裁；（2）动态批处理，根据不同领域的变化损失动态更新每个训练批处理中的样本数据组合。我们示出了 Sheared-LLaMA 系列，剪掉 LLMA2-7B 模型为 1.3B 和 2.7B 参数。Sheared-LLaMA 模型在各种下游和指令调整评估中表现出色，而且只需要比训练这些模型从头来的计算量为 3%。这项工作提供了证明，使用现有的 LLMs 结构性剪裁是一种更加经济的方法来建立更小的 LLMs。
</details></li>
</ul>
<hr>
<h2 id="Meta-CoT-Generalizable-Chain-of-Thought-Prompting-in-Mixed-task-Scenarios-with-Large-Language-Models"><a href="#Meta-CoT-Generalizable-Chain-of-Thought-Prompting-in-Mixed-task-Scenarios-with-Large-Language-Models" class="headerlink" title="Meta-CoT: Generalizable Chain-of-Thought Prompting in Mixed-task Scenarios with Large Language Models"></a>Meta-CoT: Generalizable Chain-of-Thought Prompting in Mixed-task Scenarios with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06692">http://arxiv.org/abs/2310.06692</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anni-zou/meta-cot">https://github.com/anni-zou/meta-cot</a></li>
<li>paper_authors: Anni Zou, Zhuosheng Zhang, Hai Zhao, Xiangru Tang</li>
<li>for: 这个论文主要是为了提高大语言模型（LLM）的逻辑能力，并且使其能够在不知道输入问题的混合任务场景下表现出色。</li>
<li>methods: 这篇论文提出了一种称为Meta-CoT的通用链条提问方法，它可以自动从数据池中提取多种示例，并在不同的输入问题下进行多种逻辑推理。</li>
<li>results: 论文的实验结果表明，Meta-CoT可以在十个公共评估任务上达到杰出的表现，同时具有优秀的泛化能力。特别是，Meta-CoT在SVAMP任务上达到了93.7%的最佳成绩，无需任何程序协助方法。<details>
<summary>Abstract</summary>
Large language models (LLMs) have unveiled remarkable reasoning capabilities by exploiting chain-of-thought (CoT) prompting, which generates intermediate reasoning chains to serve as the rationale for deriving the answer. However, current CoT methods either simply employ general prompts such as Let's think step by step, or heavily rely on handcrafted task-specific demonstrations to attain preferable performances, thereby engendering an inescapable gap between performance and generalization. To bridge this gap, we propose Meta-CoT, a generalizable CoT prompting method in mixed-task scenarios where the type of input questions is unknown. Meta-CoT firstly categorizes the scenario based on the input question and subsequently constructs diverse demonstrations from the corresponding data pool in an automatic pattern. Meta-CoT simultaneously enjoys remarkable performances on ten public benchmark reasoning tasks and superior generalization capabilities. Notably, Meta-CoT achieves the state-of-the-art result on SVAMP (93.7%) without any additional program-aided methods. Our further experiments on five out-of-distribution datasets verify the stability and generality of Meta-CoT.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Benchmarking-and-Explaining-Large-Language-Model-based-Code-Generation-A-Causality-Centric-Approach"><a href="#Benchmarking-and-Explaining-Large-Language-Model-based-Code-Generation-A-Causality-Centric-Approach" class="headerlink" title="Benchmarking and Explaining Large Language Model-based Code Generation: A Causality-Centric Approach"></a>Benchmarking and Explaining Large Language Model-based Code Generation: A Causality-Centric Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06680">http://arxiv.org/abs/2310.06680</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenlan Ji, Pingchuan Ma, Zongjie Li, Shuai Wang</li>
<li>for: 这篇论文旨在系统地分析 LLMs 基于高级自然语言规范（prompt）生成代码的能力，并提供可操作的评估和解释方法。</li>
<li>methods: 该论文提出了一种基于 causality 分析的方法，使用 novel 的 causal graph 来表示输入提示和生成代码之间的 causal 关系。</li>
<li>results: 研究结果表明，该方法可以提供帮助 end-users 理解 LLMs 预测的各种见解，并可以通过调整提示来改善 LLMs 生成的代码质量。<details>
<summary>Abstract</summary>
While code generation has been widely used in various software development scenarios, the quality of the generated code is not guaranteed. This has been a particular concern in the era of large language models (LLMs)- based code generation, where LLMs, deemed a complex and powerful black-box model, is instructed by a high-level natural language specification, namely a prompt, to generate code. Nevertheless, effectively evaluating and explaining the code generation capability of LLMs is inherently challenging, given the complexity of LLMs and the lack of transparency.   Inspired by the recent progress in causality analysis and its application in software engineering, this paper launches a causality analysis-based approach to systematically analyze the causal relations between the LLM input prompts and the generated code. To handle various technical challenges in this study, we first propose a novel causal graph-based representation of the prompt and the generated code, which is established over the fine-grained, human-understandable concepts in the input prompts. The formed causal graph is then used to identify the causal relations between the prompt and the derived code. We illustrate the insights that our framework can provide by studying over 3 popular LLMs with over 12 prompt adjustment strategies. The results of these studies illustrate the potential of our technique to provide insights into LLM effectiveness, and aid end-users in understanding predictions. Additionally, we demonstrate that our approach provides actionable insights to improve the quality of the LLM-generated code by properly calibrating the prompt.
</details>
<details>
<summary>摘要</summary>
在软件开发中，代码生成已经广泛应用，但代码质量并不能保证。在大语言模型（LLM）基于代码生成中，LLM被视为复杂且强大的黑盒模型，通过高级自然语言规范（即提示）生成代码。然而，对LLM代码生成能力进行有效评估和解释是极其困难的，这是因为LLM的复杂性和不透明性。 inspirited by recent progress in causality analysis and its application in software engineering, this paper proposes a causality analysis-based approach to systematically analyze the causal relations between the LLM input prompts and the generated code. To handle various technical challenges in this study, we first propose a novel causal graph-based representation of the prompt and the generated code, which is established over the fine-grained, human-understandable concepts in the input prompts. The formed causal graph is then used to identify the causal relations between the prompt and the derived code. We illustrate the insights that our framework can provide by studying over 3 popular LLMs with over 12 prompt adjustment strategies. The results of these studies illustrate the potential of our technique to provide insights into LLM effectiveness, and aid end-users in understanding predictions. Additionally, we demonstrate that our approach provides actionable insights to improve the quality of the LLM-generated code by properly calibrating the prompt.
</details></li>
</ul>
<hr>
<h2 id="Unlock-the-Potential-of-Counterfactually-Augmented-Data-in-Out-Of-Distribution-Generalization"><a href="#Unlock-the-Potential-of-Counterfactually-Augmented-Data-in-Out-Of-Distribution-Generalization" class="headerlink" title="Unlock the Potential of Counterfactually-Augmented Data in Out-Of-Distribution Generalization"></a>Unlock the Potential of Counterfactually-Augmented Data in Out-Of-Distribution Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06666">http://arxiv.org/abs/2310.06666</a></li>
<li>repo_url: None</li>
<li>paper_authors: Caoyun Fan, Wenqing Chen, Jidong Tian, Yitian Li, Hao He, Yaohui Jin</li>
<li>for: 提高语言模型的 OUT-OF-DISTRIBUTION（OOD）泛化能力</li>
<li>methods: 使用Counterfactually-Augmented Data（CAD），并在Feature Space中分析myopia现象，并引入基于CAD的结构性质的两个约束，以帮助语言模型更好地抽取 causal features</li>
<li>results: 在 Sentiment Analysis 和 Natural Language Inference 两个任务上，经验表明，我们的方法可以提高语言模型的 OOD 泛化性能by 1.0% 到 5.9%<details>
<summary>Abstract</summary>
Counterfactually-Augmented Data (CAD) -- minimal editing of sentences to flip the corresponding labels -- has the potential to improve the Out-Of-Distribution (OOD) generalization capability of language models, as CAD induces language models to exploit domain-independent causal features and exclude spurious correlations. However, the empirical results of CAD's OOD generalization are not as efficient as anticipated. In this study, we attribute the inefficiency to the myopia phenomenon caused by CAD: language models only focus on causal features that are edited in the augmentation operation and exclude other non-edited causal features. Therefore, the potential of CAD is not fully exploited. To address this issue, we analyze the myopia phenomenon in feature space from the perspective of Fisher's Linear Discriminant, then we introduce two additional constraints based on CAD's structural properties (dataset-level and sentence-level) to help language models extract more complete causal features in CAD, thereby mitigating the myopia phenomenon and improving OOD generalization capability. We evaluate our method on two tasks: Sentiment Analysis and Natural Language Inference, and the experimental results demonstrate that our method could unlock the potential of CAD and improve the OOD generalization performance of language models by 1.0% to 5.9%.
</details>
<details>
<summary>摘要</summary>
Counterfactually-Augmented Data (CAD) -- 通过最小修改句子的方式，flips 对应的标签 -- 有可能提高语言模型的 OUT-OF-DISTRIBUTION (OOD) 泛化能力，因为 CAD 使语言模型利用域 independet  causal features，并排除干扰因素。然而，CAD 的 OOD 泛化实际效果不如预期，我们归因于 CAD 引起的短视现象：语言模型只关注编辑操作中的 causal features，而忽略其他非编辑的 causal features。因此，CAD 的潜在可能性没有得到充分利用。为了解决这个问题，我们从 Fisher's Linear Discriminant 的视角分析 feature space 中的短视现象，然后引入基于 CAD 结构属性（dataset-level和 sentence-level）的两个额外约束，以 помо助语言模型在 CAD 中提取更完整的 causal features，从而 Mitigate 短视现象，提高 OOD 泛化能力。我们在 Sentiment Analysis 和 Natural Language Inference 两个任务上进行了实验，实际结果表明，我们的方法可以提高 CAD 的 OOD 泛化性能，从 1.0% 到 5.9%。
</details></li>
</ul>
<hr>
<h2 id="Assessing-the-Impact-of-a-Supervised-Classification-Filter-on-Flow-based-Hybrid-Network-Anomaly-Detection"><a href="#Assessing-the-Impact-of-a-Supervised-Classification-Filter-on-Flow-based-Hybrid-Network-Anomaly-Detection" class="headerlink" title="Assessing the Impact of a Supervised Classification Filter on Flow-based Hybrid Network Anomaly Detection"></a>Assessing the Impact of a Supervised Classification Filter on Flow-based Hybrid Network Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06656">http://arxiv.org/abs/2310.06656</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kinit-sk/hybrid-anomaly-detection">https://github.com/kinit-sk/hybrid-anomaly-detection</a></li>
<li>paper_authors: Dominik Macko, Patrik Goldschmidt, Peter Pištek, Daniela Chudá</li>
<li>for: 这篇论文旨在测量网络异常检测中使用监督过滤器（分类器）的影响。</li>
<li>methods: 我们使用了一种混合异常检测方法，将一种现有的自适应异常检测方法与一个二进制分类器相结合。</li>
<li>results: 我们的实验结果表明，混合方法可以提高已知攻击的检测率，同时仍然保持适用于零日攻击的检测能力。使用监督二进制预Filter可以提高AUC指标超过11%,检测30%更多的攻击，保持准确阳性数量相对不变。<details>
<summary>Abstract</summary>
Constant evolution and the emergence of new cyberattacks require the development of advanced techniques for defense. This paper aims to measure the impact of a supervised filter (classifier) in network anomaly detection. We perform our experiments by employing a hybrid anomaly detection approach in network flow data. For this purpose, we extended a state-of-the-art autoencoder-based anomaly detection method by prepending a binary classifier acting as a prefilter for the anomaly detector. The method was evaluated on the publicly available real-world dataset UGR'16. Our empirical results indicate that the hybrid approach does offer a higher detection rate of known attacks than a standalone anomaly detector while still retaining the ability to detect zero-day attacks. Employing a supervised binary prefilter has increased the AUC metric by over 11%, detecting 30% more attacks while keeping the number of false positives approximately the same.
</details>
<details>
<summary>摘要</summary>
常态的演化和新型攻击的出现需要开发先进的防御技术。这篇论文目的是测量一个监督器（分类器）在网络异常检测中的影响。我们在网络流数据中进行了一种混合异常检测方法的实验，包括将一个状态艺术自适应异常检测方法扩展为预先筛选器。我们使用公共可用的真实世界数据集UGR'16进行评估。我们的实验结果表明，混合方法可以提高已知攻击的检测率，同时仍然保持适用于零天攻击的检测能力。在使用监督二进制预测器后，AUC指标提高了超过11%,检测到30%更多的攻击，保持 false positive 的数量约为同样。
</details></li>
</ul>
<hr>
<h2 id="Diversity-from-Human-Feedback"><a href="#Diversity-from-Human-Feedback" class="headerlink" title="Diversity from Human Feedback"></a>Diversity from Human Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06648">http://arxiv.org/abs/2310.06648</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Rmnislam/Tours-and-Travels-management-System">https://github.com/Rmnislam/Tours-and-Travels-management-System</a></li>
<li>paper_authors: Ren-Jian Wang, Ke Xue, Yutong Wang, Peng Yang, Haobo Fu, Qiang Fu, Chao Qian</li>
<li>for: 本 paper 的目的是解决如何从人类反馈中学习行为空间，以提高多样性优化的效果。</li>
<li>methods: 本 paper 提出了一种名为多样性从人类反馈（DivHF）的方法，通过询问人类反馈来学习行为描述符，并将其与任何距离度量结合以定义多样性度量。</li>
<li>results: 实验结果表明，使用 DivHF 方法可以学习更好地适应人类偏好的行为空间，并且可以通过人类反馈来提高多样性优化的效果。<details>
<summary>Abstract</summary>
Diversity plays a significant role in many problems, such as ensemble learning, reinforcement learning, and combinatorial optimization. How to define the diversity measure is a longstanding problem. Many methods rely on expert experience to define a proper behavior space and then obtain the diversity measure, which is, however, challenging in many scenarios. In this paper, we propose the problem of learning a behavior space from human feedback and present a general method called Diversity from Human Feedback (DivHF) to solve it. DivHF learns a behavior descriptor consistent with human preference by querying human feedback. The learned behavior descriptor can be combined with any distance measure to define a diversity measure. We demonstrate the effectiveness of DivHF by integrating it with the Quality-Diversity optimization algorithm MAP-Elites and conducting experiments on the QDax suite. The results show that DivHF learns a behavior space that aligns better with human requirements compared to direct data-driven approaches and leads to more diverse solutions under human preference. Our contributions include formulating the problem, proposing the DivHF method, and demonstrating its effectiveness through experiments.
</details>
<details>
<summary>摘要</summary>
多样性在许多问题中扮演着重要的角色，如集成学习、强化学习和组合优化。定义多样性度量是一个长期的问题。许多方法依赖于专家经验来定义合适的行为空间，然后获取多样性度量，但这在许多场景下是困难的。在这篇论文中，我们提出了从人类反馈获得行为空间的问题，并提出了一种通用的方法called多样性从人类反馈（DivHF）来解决这个问题。DivHF通过询问人类反馈来学习一个与人类偏好相符的行为描述符。学习的行为描述符可以与任何距离度量结合以定义多样性度量。我们通过在QDax集合上集成DivHF和MAP-Elites算法进行实验，并证明了DivHF可以学习一个更好地与人类需求相符的行为空间，并且导致更多的多样性解决方案。我们的贡献包括提出了问题、提出了DivHF方法，并通过实验证明了其效果。
</details></li>
</ul>
<hr>
<h2 id="Topic-DPR-Topic-based-Prompts-for-Dense-Passage-Retrieval"><a href="#Topic-DPR-Topic-based-Prompts-for-Dense-Passage-Retrieval" class="headerlink" title="Topic-DPR: Topic-based Prompts for Dense Passage Retrieval"></a>Topic-DPR: Topic-based Prompts for Dense Passage Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06626">http://arxiv.org/abs/2310.06626</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qingfa Xiao, Shuangyin Li, Lei Chen</li>
<li>for: 提高 dense passage retrieval 的效果，增强预处理语言模型的Semantic理解。</li>
<li>methods: 使用 topic-based 提示，通过对 probablistic simplex 上的多个提示进行同时优化，使表示更加吻合话题分布。</li>
<li>results: 在两个 datasets 上实验结果显示，我们的方法超过了之前的 state-of-the-art  Retrieval 技术。<details>
<summary>Abstract</summary>
Prompt-based learning's efficacy across numerous natural language processing tasks has led to its integration into dense passage retrieval. Prior research has mainly focused on enhancing the semantic understanding of pre-trained language models by optimizing a single vector as a continuous prompt. This approach, however, leads to a semantic space collapse; identical semantic information seeps into all representations, causing their distributions to converge in a restricted region. This hinders differentiation between relevant and irrelevant passages during dense retrieval. To tackle this issue, we present Topic-DPR, a dense passage retrieval model that uses topic-based prompts. Unlike the single prompt method, multiple topic-based prompts are established over a probabilistic simplex and optimized simultaneously through contrastive learning. This encourages representations to align with their topic distributions, improving space uniformity. Furthermore, we introduce a novel positive and negative sampling strategy, leveraging semi-structured data to boost dense retrieval efficiency. Experimental results from two datasets affirm that our method surpasses previous state-of-the-art retrieval techniques.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="BridgeHand2Vec-Bridge-Hand-Representation"><a href="#BridgeHand2Vec-Bridge-Hand-Representation" class="headerlink" title="BridgeHand2Vec Bridge Hand Representation"></a>BridgeHand2Vec Bridge Hand Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06624">http://arxiv.org/abs/2310.06624</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/johny-b/bridgehand2vec">https://github.com/johny-b/bridgehand2vec</a></li>
<li>paper_authors: Anna Sztyber-Betley, Filip Kołodziej, Jan Betley, Piotr Duszak</li>
<li>for:  bridge game AI research</li>
<li>methods: neural network vectorization, reinforcement learning, opening bid classification</li>
<li>results: SOTA results on DDBP2 problem (estimating number of tricks for two given hands)<details>
<summary>Abstract</summary>
Contract bridge is a game characterized by incomplete information, posing an exciting challenge for artificial intelligence methods. This paper proposes the BridgeHand2Vec approach, which leverages a neural network to embed a bridge player's hand (consisting of 13 cards) into a vector space. The resulting representation reflects the strength of the hand in the game and enables interpretable distances to be determined between different hands. This representation is derived by training a neural network to estimate the number of tricks that a pair of players can take. In the remainder of this paper, we analyze the properties of the resulting vector space and provide examples of its application in reinforcement learning, and opening bid classification. Although this was not our main goal, the neural network used for the vectorization achieves SOTA results on the DDBP2 problem (estimating the number of tricks for two given hands).
</details>
<details>
<summary>摘要</summary>
CONTRACT BRIDGE 是一款具有不完整信息的游戏，具有让人很激动的挑战性，这篇论文提出了 BridgeHand2Vec 方法，该方法利用神经网络将bridge玩家手中的13张牌转换成向量空间中的表示。这种表示能够反映手中的游戏力量，并允许确定不同手中的距离。这种表示是通过训练神经网络来估计两名玩家可以拿到的赢得局数来获得的。在本文中，我们分析了这种向量空间的性质，并提供了应用于强化学习和开场招许分类的示例。虽然这并不是我们的主要目标，但是使用于向量化的神经网络在 DDBP2 问题上达到了顶峰性能。
</details></li>
</ul>
<hr>
<h2 id="V2X-AHD-Vehicle-to-Everything-Cooperation-Perception-via-Asymmetric-Heterogenous-Distillation-Network"><a href="#V2X-AHD-Vehicle-to-Everything-Cooperation-Perception-via-Asymmetric-Heterogenous-Distillation-Network" class="headerlink" title="V2X-AHD:Vehicle-to-Everything Cooperation Perception via Asymmetric Heterogenous Distillation Network"></a>V2X-AHD:Vehicle-to-Everything Cooperation Perception via Asymmetric Heterogenous Distillation Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06603">http://arxiv.org/abs/2310.06603</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/feeling0414-lab/V2X-AHD">https://github.com/feeling0414-lab/V2X-AHD</a></li>
<li>paper_authors: Caizhen He, Hai Wang, Long Chen, Tong Luo, Yingfeng Cai</li>
<li>for: 本研究的目的是提出一种多视图道路合作感知系统，以提高物体检测的准确性。</li>
<li>methods: 该系统使用非对称多元学习约束网络（Asymmetric Heterogeneous Distillation Network，AHD），并在缺乏单视图轮廓信息的情况下提高了 outline 识别精度。另外，我们还提出了一种减少参数的稀疏特征提取器（Spara Pillar），以提高特征提取能力。最后，我们使用多头自注意（Multi-head Self-Attention，MSA）来融合单视图特征，以实现简洁的特征表达。</li>
<li>results: 应用我们的算法于大规模开放数据集V2Xset，得到了state-of-the-art的结果。V2X-AHD可以有效地提高3D物体检测的准确性，并降低网络参数的数量。这些结果可以 serves as a benchmark for cooperative perception。<details>
<summary>Abstract</summary>
Object detection is the central issue of intelligent traffic systems, and recent advancements in single-vehicle lidar-based 3D detection indicate that it can provide accurate position information for intelligent agents to make decisions and plan. Compared with single-vehicle perception, multi-view vehicle-road cooperation perception has fundamental advantages, such as the elimination of blind spots and a broader range of perception, and has become a research hotspot. However, the current perception of cooperation focuses on improving the complexity of fusion while ignoring the fundamental problems caused by the absence of single-view outlines. We propose a multi-view vehicle-road cooperation perception system, vehicle-to-everything cooperative perception (V2X-AHD), in order to enhance the identification capability, particularly for predicting the vehicle's shape. At first, we propose an asymmetric heterogeneous distillation network fed with different training data to improve the accuracy of contour recognition, with multi-view teacher features transferring to single-view student features. While the point cloud data are sparse, we propose Spara Pillar, a spare convolutional-based plug-in feature extraction backbone, to reduce the number of parameters and improve and enhance feature extraction capabilities. Moreover, we leverage the multi-head self-attention (MSA) to fuse the single-view feature, and the lightweight design makes the fusion feature a smooth expression. The results of applying our algorithm to the massive open dataset V2Xset demonstrate that our method achieves the state-of-the-art result. The V2X-AHD can effectively improve the accuracy of 3D object detection and reduce the number of network parameters, according to this study, which serves as a benchmark for cooperative perception. The code for this article is available at https://github.com/feeling0414-lab/V2X-AHD.
</details>
<details>
<summary>摘要</summary>
“对于智能交通系统中的物件探测，最近的进展表明单车 lidar 三维探测可以提供正确的位置信息，帮助智能代理人做出决策和规划。相比单车感知，多视角车道合作感知具有根本上的优势，如消除盲点和扩大视野，并成为研究热点。然而，现有的感知方法强调增强复杂的融合，忽略了单视角 outline 的基本问题。我们提出了一个多视角车道合作感知系统（V2X-AHD），以增强物件识别能力，特别是预测车辆形状。在这个系统中，我们提出了不对称多元精神网络，使用不同训练数据来提高楔形识别精度，并将多视角教师特征转移到单视角学生特征中。当Point cloud 资料为稀疏时，我们提出了Spara Pillar，一个减少参数的几何学基础Feature extraction 后置架构。此外，我们利用多头自注意（MSA）融合单视角特征，并将融合特征设计为轻量级。根据我们将这个算法应用到大量公开 dataset V2Xset 的结果，我们的方法可以实现 state-of-the-art 的成果。V2X-AHD 可以增强3D物件探测的精度和减少网络参数数量，根据这个研究，可以作为协同感知的 bench mark。相关的代码可以在 GitHub 上获取：https://github.com/feeling0414-lab/V2X-AHD。”
</details></li>
</ul>
<hr>
<h2 id="A-Black-Box-Physics-Informed-Estimator-based-on-Gaussian-Process-Regression-for-Robot-Inverse-Dynamics-Identification"><a href="#A-Black-Box-Physics-Informed-Estimator-based-on-Gaussian-Process-Regression-for-Robot-Inverse-Dynamics-Identification" class="headerlink" title="A Black-Box Physics-Informed Estimator based on Gaussian Process Regression for Robot Inverse Dynamics Identification"></a>A Black-Box Physics-Informed Estimator based on Gaussian Process Regression for Robot Inverse Dynamics Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06585">http://arxiv.org/abs/2310.06585</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giulio Giacomuzzo, Alberto Dalla Libera, Diego Romeres, Ruggero Carli</li>
<li>for: 本研究提出了一种基于 Gaussian Process regression 的黑盒模型，用于 робо类 manipulate 器的逆动力学识别。</li>
<li>methods: 提出的模型基于一种新型多维度核函数，称为 \kernelInitials{} 核函数。该核函数基于两个主要想法：首先，而不直接模型逆动力学组件，我们模型了 robot 系统的动能和潜能。其次，我们证明了动能和潜能具有多项式结构，并 derivated 一种多项式核函数，这个核函数表达了这个性质。</li>
<li>results: 对于 simulate 和实际两个 robotic manipulate 器（Franka Emika Panda 和 MELFA RV4FL）进行了实验，结果显示，提出的模型在精度、通用性和数据效率方面都高于当前黑盒估计器，包括 Gaussian Processes 和神经网络。此外，对 MELFA 机器人的实验还示出了我们的方法可以与高精度模型基于估计器相比，即使需要更少的先验信息。<details>
<summary>Abstract</summary>
In this paper, we propose a black-box model based on Gaussian process regression for the identification of the inverse dynamics of robotic manipulators. The proposed model relies on a novel multidimensional kernel, called \textit{Lagrangian Inspired Polynomial} (\kernelInitials{}) kernel. The \kernelInitials{} kernel is based on two main ideas. First, instead of directly modeling the inverse dynamics components, we model as GPs the kinetic and potential energy of the system. The GP prior on the inverse dynamics components is derived from those on the energies by applying the properties of GPs under linear operators. Second, as regards the energy prior definition, we prove a polynomial structure of the kinetic and potential energy, and we derive a polynomial kernel that encodes this property. As a consequence, the proposed model allows also to estimate the kinetic and potential energy without requiring any label on these quantities. Results on simulation and on two real robotic manipulators, namely a 7 DOF Franka Emika Panda and a 6 DOF MELFA RV4FL, show that the proposed model outperforms state-of-the-art black-box estimators based both on Gaussian Processes and Neural Networks in terms of accuracy, generality and data efficiency. The experiments on the MELFA robot also demonstrate that our approach achieves performance comparable to fine-tuned model-based estimators, despite requiring less prior information.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了基于 Gaussian Process regression 的黑盒模型，用于 робо机拟合器的反动动学特征标定。我们的模型基于一种新的多维度核函数，称为 \kernelInitials{} 核函数。\kernelInitials{} 核函数基于两个主要想法：首先，而不直接模型反动动学分量，我们模型了机械系统的动能和潜能为 Gaussian Processes。由于 GPs 具有线性运算下的性质，我们可以从这些 GPs 中得到反动动学分量的 priors。其次，我们证明机械系统的动能和潜能具有多项式结构，并 derivated一个多项式核函数来表示这种性质。这意味着我们的模型可以不仅仅 estimator 反动动学分量，还可以估计机械系统的动能和潜能，无需提供任何标注。在实验中，我们使用了两个真实的 робо机拟合器，即 7 DOF Franka Emika Panda 和 6 DOF MELFA RV4FL，并与已有的黑盒估计器（基于 Gaussian Processes 和 Neural Networks）进行比较。结果表明，我们的模型在准确性、通用性和数据效率方面表现更好，并且在 MELFA 机械上实验也表明了我们的方法可以与高精度模型基本估计器相比，即使需要更少的先验信息。
</details></li>
</ul>
<hr>
<h2 id="On-Temporal-References-in-Emergent-Communication"><a href="#On-Temporal-References-in-Emergent-Communication" class="headerlink" title="On Temporal References in Emergent Communication"></a>On Temporal References in Emergent Communication</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06555">http://arxiv.org/abs/2310.06555</a></li>
<li>repo_url: None</li>
<li>paper_authors: Olaf Lipinski, Adam J. Sobey, Federico Cerutti, Timothy J. Norman</li>
<li>for: 这 paper 是为了研究 emergent communication 中的时间参照 vocabulary 的发展而写的。</li>
<li>methods: 这 paper 使用了一种新的 agent architecture，以检验 temporal referencing 是否可以自然地出现在 emergent communication 中。</li>
<li>results: 实验结果表明，这种新的 agent architecture 是可以自然地引入 temporal referencing 的，无需额外的损失。这些发现可以为其他 emergent communication 环境中的时间参照引入提供基础。<details>
<summary>Abstract</summary>
As humans, we use linguistic elements referencing time, such as before or tomorrow, to easily share past experiences and future predictions. While temporal aspects of the language have been considered in computational linguistics, no such exploration has been done within the field of emergent communication. We research this gap, providing the first reported temporal vocabulary within emergent communication literature. Our experimental analysis shows that a different agent architecture is sufficient for the natural emergence of temporal references, and that no additional losses are necessary. Our readily transferable architectural insights provide the basis for the incorporation of temporal referencing into other emergent communication environments.
</details>
<details>
<summary>摘要</summary>
人类使用语言元素 referencing 时间，如前或明天，轻松分享过去经验和未来预测。在计算机语言学中，时间方面的语言元素已经得到了考虑，但在emergent communication中，这一方面的探索尚未进行过。我们对此进行研究，提供了emergent communication中第一个时间参考词汇的报告。我们的实验分析表明，不需要额外的损失，不同的机器人体系即可自然地出现时间引用。我们的易于传输的建筑思想可以为其他emergent communication环境中的时间引用 incorporation提供基础。
</details></li>
</ul>
<hr>
<h2 id="Automated-clinical-coding-using-off-the-shelf-large-language-models"><a href="#Automated-clinical-coding-using-off-the-shelf-large-language-models" class="headerlink" title="Automated clinical coding using off-the-shelf large language models"></a>Automated clinical coding using off-the-shelf large language models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06552">http://arxiv.org/abs/2310.06552</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joseph S. Boyle, Antanas Kascenas, Pat Lok, Maria Liakata, Alison Q. O’Neil</li>
<li>for: 这个论文的目的是提出一种自动生成ICD代码的方法，以解决现有的人工编码者困难和深度学习模型的难以适应临床实践中的问题。</li>
<li>methods: 这个方法使用了市场上已经预训练的大语言模型（LLM），通过信息抽取和 Hierarchical 搜索来自动生成ICD代码。在第二阶段，使用 GPT-4 进行元反差，选择一 subset of 相关的标签作为预测。</li>
<li>results: 这个方法在 CodiEsp 数据集上测试，与 PLM-ICD 相比，在更为罕见的类型上表现出了状态的表现，具有最高的 macro-F1 值 0.225，微-F1 值 0.157，而 PLM-ICD 的最高值为 0.216 和 0.219。这个方法不需要任何任务特定的学习，从而实现了自动 ICD 编码的目的。<details>
<summary>Abstract</summary>
The task of assigning diagnostic ICD codes to patient hospital admissions is typically performed by expert human coders. Efforts towards automated ICD coding are dominated by supervised deep learning models. However, difficulties in learning to predict the large number of rare codes remain a barrier to adoption in clinical practice. In this work, we leverage off-the-shelf pre-trained generative large language models (LLMs) to develop a practical solution that is suitable for zero-shot and few-shot code assignment. Unsupervised pre-training alone does not guarantee precise knowledge of the ICD ontology and specialist clinical coding task, therefore we frame the task as information extraction, providing a description of each coded concept and asking the model to retrieve related mentions. For efficiency, rather than iterating over all codes, we leverage the hierarchical nature of the ICD ontology to sparsely search for relevant codes. Then, in a second stage, which we term 'meta-refinement', we utilise GPT-4 to select a subset of the relevant labels as predictions. We validate our method using Llama-2, GPT-3.5 and GPT-4 on the CodiEsp dataset of ICD-coded clinical case documents. Our tree-search method achieves state-of-the-art performance on rarer classes, achieving the best macro-F1 of 0.225, whilst achieving slightly lower micro-F1 of 0.157, compared to 0.216 and 0.219 respectively from PLM-ICD. To the best of our knowledge, this is the first method for automated ICD coding requiring no task-specific learning.
</details>
<details>
<summary>摘要</summary>
通常情况下，诊断ICD代码的分配是由专业的人类编码器完成。但是，自动ICD编码的尝试受到罕见代码的困难而受阻。在这种情况下，我们利用可用的准备好的大语言模型（LLM）来开发一个实用的解决方案，适用于零shot和几shot代码分配。不同于其他研究，我们不使用监督学习模型，而是将任务定义为信息抽取，请求模型提取相关的提取。为了提高效率，我们利用ICD ontology的层次结构，将搜索范围限定为有关代码。然后，在第二阶段，我们使用GPT-4来选择相关的标签作为预测。我们使用Llama-2、GPT-3.5和GPT-4在CodiEsp数据集上验证我们的方法，并 achieved state-of-the-art表现在更罕见的类别中，即macro-F1为0.225，微-F1为0.157。这与PLM-ICD的0.216和0.219相比，表现较佳。我们知道，这是自动ICD编码的首次不需要任务特定学习的方法。
</details></li>
</ul>
<hr>
<h2 id="Rationale-Enhanced-Language-Models-are-Better-Continual-Relation-Learners"><a href="#Rationale-Enhanced-Language-Models-are-Better-Continual-Relation-Learners" class="headerlink" title="Rationale-Enhanced Language Models are Better Continual Relation Learners"></a>Rationale-Enhanced Language Models are Better Continual Relation Learners</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06547">http://arxiv.org/abs/2310.06547</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/weiminxiong/rationalecl">https://github.com/weiminxiong/rationalecl</a></li>
<li>paper_authors: Weimin Xiong, Yifan Song, Peiyi Wang, Sujian Li</li>
<li>for: 解决catastrophic forgetting问题，即在学习新出现的关系时，模型忘记之前学习的关系。</li>
<li>methods: 引入 rational explanations，即大语言模型生成的关系分类结果的解释，以帮助模型学习当前关系 robustly。 我们设计了多任务论断调整策略，以及对应的对抗 rational replay。</li>
<li>results: 实验结果表明，我们的方法在两个标准benchmark上都超越了当前最佳CRE模型。<details>
<summary>Abstract</summary>
Continual relation extraction (CRE) aims to solve the problem of catastrophic forgetting when learning a sequence of newly emerging relations. Recent CRE studies have found that catastrophic forgetting arises from the model's lack of robustness against future analogous relations. To address the issue, we introduce rationale, i.e., the explanations of relation classification results generated by large language models (LLM), into CRE task. Specifically, we design the multi-task rationale tuning strategy to help the model learn current relations robustly. We also conduct contrastive rationale replay to further distinguish analogous relations. Experimental results on two standard benchmarks demonstrate that our method outperforms the state-of-the-art CRE models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Realizing-Stabilized-Landing-for-Computation-Limited-Reusable-Rockets-A-Quantum-Reinforcement-Learning-Approach"><a href="#Realizing-Stabilized-Landing-for-Computation-Limited-Reusable-Rockets-A-Quantum-Reinforcement-Learning-Approach" class="headerlink" title="Realizing Stabilized Landing for Computation-Limited Reusable Rockets: A Quantum Reinforcement Learning Approach"></a>Realizing Stabilized Landing for Computation-Limited Reusable Rockets: A Quantum Reinforcement Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06541">http://arxiv.org/abs/2310.06541</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/NumtraCG/5fec231021490d1a99cff2f320201230-065415numtrafiletest">https://github.com/NumtraCG/5fec231021490d1a99cff2f320201230-065415numtrafiletest</a></li>
<li>paper_authors: Gyu Seon Kim, JaeHyun Chung, Soohyun Park</li>
<li>for: 这篇论文是为了探讨量子强化学习在再用导弹控制系统中的应用。</li>
<li>methods: 这篇论文使用量子强化学习来更新控制系统，以适应导弹动态系统变化。</li>
<li>results: 研究人员发现，量子强化学习可以提供更高的计算效率、减少内存需求和更稳定的性能，这些特点使其成为再用导弹控制系统中的优秀解决方案。<details>
<summary>Abstract</summary>
The advent of reusable rockets has heralded a new era in space exploration, reducing the costs of launching satellites by a significant factor. Traditional rockets were disposable, but the design of reusable rockets for repeated use has revolutionized the financial dynamics of space missions. The most critical phase of reusable rockets is the landing stage, which involves managing the tremendous speed and attitude for safe recovery. The complexity of this task presents new challenges for control systems, specifically in terms of precision and adaptability. Classical control systems like the proportional-integral-derivative (PID) controller lack the flexibility to adapt to dynamic system changes, making them costly and time-consuming to redesign of controller. This paper explores the integration of quantum reinforcement learning into the control systems of reusable rockets as a promising alternative. Unlike classical reinforcement learning, quantum reinforcement learning uses quantum bits that can exist in superposition, allowing for more efficient information encoding and reducing the number of parameters required. This leads to increased computational efficiency, reduced memory requirements, and more stable and predictable performance. Due to the nature of reusable rockets, which must be light, heavy computers cannot fit into them. In the reusable rocket scenario, quantum reinforcement learning, which has reduced memory requirements due to fewer parameters, is a good solution.
</details>
<details>
<summary>摘要</summary>
发射卫星的成本由再用火箭的出现大幅降低，这种新的发射方式已经开启了宇宙探索的新时代。传统的火箭是一次性的，但是再用火箭的设计可以重复使用，这对宇宙探索的财务动力产生了革命性的变化。再用火箭的最关键的阶段是着陆阶段，需要控制高速和总体orientation以实现安全的回收。这种任务的复杂性带来了新的控制系统挑战，特别是精度和适应性方面。经典的控制系统如比例-Integral-Derivative（PID）控制器缺乏适应性，需要时间和成本重新设计控制器。这篇论文探讨了在控制系统中 интеGRATION quantum reinforcement learning作为一种可能的替代方案。与经典的反馈学习不同，量子反馈学习使用量子比特，可以在超position中存在，从而实现更高效的信息编码和减少参数数量。这导致计算效率提高，存储需求减少，性能更稳定和预测可靠。由于再用火箭需要轻量级，因此在再用火箭场景下，量子反馈学习，具有减少参数数量的优点，是一个好的解决方案。
</details></li>
</ul>
<hr>
<h2 id="A-Novel-Contrastive-Learning-Method-for-Clickbait-Detection-on-RoCliCo-A-Romanian-Clickbait-Corpus-of-News-Articles"><a href="#A-Novel-Contrastive-Learning-Method-for-Clickbait-Detection-on-RoCliCo-A-Romanian-Clickbait-Corpus-of-News-Articles" class="headerlink" title="A Novel Contrastive Learning Method for Clickbait Detection on RoCliCo: A Romanian Clickbait Corpus of News Articles"></a>A Novel Contrastive Learning Method for Clickbait Detection on RoCliCo: A Romanian Clickbait Corpus of News Articles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06540">http://arxiv.org/abs/2310.06540</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dariabroscoteanu/roclico">https://github.com/dariabroscoteanu/roclico</a></li>
<li>paper_authors: Daria-Mihaela Broscoteanu, Radu Tudor Ionescu</li>
<li>for: 本研究旨在提高网站收入，通过检测吸引用户点击的含有误导信息的新闻标题，以避免在线用户浪费时间。</li>
<li>methods: 本研究使用了四种机器学习方法，包括手工模型、循环和转换器基于神经网络，以及一种基于BERT的对比学习模型。</li>
<li>results: 研究人员通过手动标注8,313篇新闻样本，并使用四种机器学习方法进行实验，以建立一系列竞争力强的基线。此外，研究人员还提出了一种基于BERT的对比学习模型，可以在新闻标题和内容之间学习深度度量空间，以便识别不是吸引用户点击的新闻。<details>
<summary>Abstract</summary>
To increase revenue, news websites often resort to using deceptive news titles, luring users into clicking on the title and reading the full news. Clickbait detection is the task that aims to automatically detect this form of false advertisement and avoid wasting the precious time of online users. Despite the importance of the task, to the best of our knowledge, there is no publicly available clickbait corpus for the Romanian language. To this end, we introduce a novel Romanian Clickbait Corpus (RoCliCo) comprising 8,313 news samples which are manually annotated with clickbait and non-clickbait labels. Furthermore, we conduct experiments with four machine learning methods, ranging from handcrafted models to recurrent and transformer-based neural networks, to establish a line-up of competitive baselines. We also carry out experiments with a weighted voting ensemble. Among the considered baselines, we propose a novel BERT-based contrastive learning model that learns to encode news titles and contents into a deep metric space such that titles and contents of non-clickbait news have high cosine similarity, while titles and contents of clickbait news have low cosine similarity. Our data set and code to reproduce the baselines are publicly available for download at https://github.com/dariabroscoteanu/RoCliCo.
</details>
<details>
<summary>摘要</summary>
为了增加收入，新闻网站经常使用吸引人的标题，让用户点击标题并阅读完整的新闻。 Clickbait检测是一项任务，旨在自动检测这种false advertisement，以避免在线用户的宝贵时间浪费。然而，到目前为止，我们知道没有公开可用的罗马尼亚语Clickbait corpus。为此，我们介绍了一个新的罗马尼亚Clickbait corpus（RoCliCo），包含8,313个新闻样本，每个样本都被手动标注为Clickbait或非Clickbait。此外，我们进行了四种机器学习方法的实验，从手工模型到回归和转换器基于神经网络，以建立一系列竞争力强的基准。我们还进行了一个权重投票集成。 amongst the considered baselines, we propose a novel BERT-based contrastive learning model that learns to encode news titles and contents into a deep metric space such that titles and contents of non-clickbait news have high cosine similarity, while titles and contents of clickbait news have low cosine similarity。我们的数据集和可重现基准的代码公开下载于https://github.com/dariabroscoteanu/RoCliCo。
</details></li>
</ul>
<hr>
<h2 id="Accelerating-Monte-Carlo-Tree-Search-with-Probability-Tree-State-Abstraction"><a href="#Accelerating-Monte-Carlo-Tree-Search-with-Probability-Tree-State-Abstraction" class="headerlink" title="Accelerating Monte Carlo Tree Search with Probability Tree State Abstraction"></a>Accelerating Monte Carlo Tree Search with Probability Tree State Abstraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06513">http://arxiv.org/abs/2310.06513</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/FYQ0919/PTSA-MCTS">https://github.com/FYQ0919/PTSA-MCTS</a></li>
<li>paper_authors: Yangqing Fu, Ming Sun, Buqing Nie, Yue Gao</li>
<li>for: 提高 Monte Carlo Tree Search（MCTS）算法的搜索效率，使其在许多复杂任务中达到人类水平或更高的性能。</li>
<li>methods: 提出了一种新的概率树状态抽象（PTSA）算法，通过减少搜索空间大小来提高MCTS算法的搜索效率。这种算法使用一般的树状态抽象和路径整合性，并且提供了论证性的不确定性和聚合误差约束。</li>
<li>results: 通过与现有的MCTS-based算法集成，如Sampled MuZero和Gumbel MuZero，实验结果表明，我们的PTSA算法可以在不同任务上减少搜索空间大小10%-45%，并且加速了现有算法的训练过程。<details>
<summary>Abstract</summary>
Monte Carlo Tree Search (MCTS) algorithms such as AlphaGo and MuZero have achieved superhuman performance in many challenging tasks. However, the computational complexity of MCTS-based algorithms is influenced by the size of the search space. To address this issue, we propose a novel probability tree state abstraction (PTSA) algorithm to improve the search efficiency of MCTS. A general tree state abstraction with path transitivity is defined. In addition, the probability tree state abstraction is proposed for fewer mistakes during the aggregation step. Furthermore, the theoretical guarantees of the transitivity and aggregation error bound are justified. To evaluate the effectiveness of the PTSA algorithm, we integrate it with state-of-the-art MCTS-based algorithms, such as Sampled MuZero and Gumbel MuZero. Experimental results on different tasks demonstrate that our method can accelerate the training process of state-of-the-art algorithms with 10%-45% search space reduction.
</details>
<details>
<summary>摘要</summary>
蒙特卡洛树搜索（MCTS）算法如AlphaGo和MuZero已经在许多具有挑战性的任务中表现出人类之上。然而，MCTS基本算法的计算复杂度受到搜索空间的大小影响。为解决这个问题，我们提出了一个新的概率树状抽象（PTSA）算法，以提高MCTS搜索效率。我们定义一个通用的树状抽象，并提出了路径潜在性的概率树状抽象，以降低统计误差。此外，我们提供了对于潜在性和统计误差的理论保证。为评估PTSA算法的效果，我们将其与现有的MCTS基本算法结合，例如Sampled MuZero和Gumbel MuZero。实验结果显示，我们的方法可以将搜索空间缩减10%-45%，并加速现有算法的训练过程。
</details></li>
</ul>
<hr>
<h2 id="RK-core-An-Established-Methodology-for-Exploring-the-Hierarchical-Structure-within-Datasets"><a href="#RK-core-An-Established-Methodology-for-Exploring-the-Hierarchical-Structure-within-Datasets" class="headerlink" title="RK-core: An Established Methodology for Exploring the Hierarchical Structure within Datasets"></a>RK-core: An Established Methodology for Exploring the Hierarchical Structure within Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12168">http://arxiv.org/abs/2310.12168</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yaolu-zjut/kcore">https://github.com/yaolu-zjut/kcore</a></li>
<li>paper_authors: Yao Lu, Yutian Huang, Jiaqi Nie, Zuohui Chen, Qi Xuan</li>
<li>for: 本研究旨在探讨数据集中隐藏的层次结构，并提出一种名为RK-core的新方法，以更深入地理解数据集中的各个类别之间的关系。</li>
<li>methods: 本研究使用了RK-core方法，通过对数据集进行分割和重建，找到了各个类别之间的层次结构。同时，本研究还 comparatively分析了不同的核心集选择方法，并发现了一个高质量的核心集应该具有层次多样性。</li>
<li>results: 本研究在多个 benchmark 数据集上进行了实验，发现了 samples 的低核心值与其所属类别的表现有负相关性，而高核心值 samples 则对性能的贡献更大。此外，本研究还发现了一个高质量的核心集应该具有层次多样性，而不是仅选择表现最佳的示例。<details>
<summary>Abstract</summary>
Recently, the field of machine learning has undergone a transition from model-centric to data-centric. The advancements in diverse learning tasks have been propelled by the accumulation of more extensive datasets, subsequently facilitating the training of larger models on these datasets. However, these datasets remain relatively under-explored. To this end, we introduce a pioneering approach known as RK-core, to empower gaining a deeper understanding of the intricate hierarchical structure within datasets. Across several benchmark datasets, we find that samples with low coreness values appear less representative of their respective categories, and conversely, those with high coreness values exhibit greater representativeness. Correspondingly, samples with high coreness values make a more substantial contribution to the performance in comparison to those with low coreness values. Building upon this, we further employ RK-core to analyze the hierarchical structure of samples with different coreset selection methods. Remarkably, we find that a high-quality coreset should exhibit hierarchical diversity instead of solely opting for representative samples. The code is available at https://github.com/yaolu-zjut/Kcore.
</details>
<details>
<summary>摘要</summary>
最近，机器学习领域受到了数据中心化的影响，各种学习任务的进步受到了更加广泛和深入的数据驱动。然而，这些数据仍然尚未得到充分探索。为此，我们提出了一种创新的方法——RK-core，以便更深入地理解数据集中的复杂层次结构。在多个标准数据集上测试，我们发现低核心值的样本对其相应的类别表示着较低的表达力，而高核心值的样本则表现出较高的表达力。此外，高核心值的样本在性能中占据了更大的比重。基于这一点，我们进一步采用RK-core分析不同核心选择方法下的层次结构。我们发现，高质量的核心集应该具有层次多样性，而不是仅仅选择表达力最高的样本。相关代码可以在https://github.com/yaolu-zjut/Kcore上找到。
</details></li>
</ul>
<hr>
<h2 id="Evaluation-of-ChatGPT-Feedback-on-ELL-Writers’-Coherence-and-Cohesion"><a href="#Evaluation-of-ChatGPT-Feedback-on-ELL-Writers’-Coherence-and-Cohesion" class="headerlink" title="Evaluation of ChatGPT Feedback on ELL Writers’ Coherence and Cohesion"></a>Evaluation of ChatGPT Feedback on ELL Writers’ Coherence and Cohesion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06505">http://arxiv.org/abs/2310.06505</a></li>
<li>repo_url: None</li>
<li>paper_authors: Su-Youn Yoon, Eva Miszoglad, Lisa R. Pierce</li>
<li>For: The paper evaluates the effectiveness of ChatGPT in providing feedback on the coherence and cohesion of essays written by English Language Learners (ELLs) students.* Methods: The paper uses a two-step approach to evaluate the feedback generated by ChatGPT, including classifying each sentence into subtypes based on its function and evaluating its accuracy and usability.* Results: The paper finds that most feedback sentences generated by ChatGPT are highly abstract and generic, failing to provide concrete suggestions for improvement. The accuracy of the feedback depends on superficial linguistic features and is often incorrect, indicating that ChatGPT, without specific training for the feedback generation task, does not offer effective feedback on ELL students’ coherence and cohesion.Here are the three key information points in Simplified Chinese text:* For: 这个研究用ChatGPT来评估英语学习者（ELLs）学生写的论文的 coherence 和 cohesion 的 feedback 的有效性。* Methods: 这个研究使用了一种两步方法来评估 ChatGPT 生成的 feedback，包括将每句话分类为不同的类型根据其功能（例如，正面鼓励、问题陈述），然后评估它们的准确性和可用性。* Results: 研究发现，ChatGPT 生成的 feedback 多数是高度抽象的和通用的，无法提供具体的改进建议。准确地检测主要问题（如重复的想法和不准确使用连接device）的准确性取决于表面语言特征，并且经常错误。因此，ChatGPT 无法提供有效的 feedback 于 ELLs 学生的 coherence 和 cohesion。<details>
<summary>Abstract</summary>
Since its launch in November 2022, ChatGPT has had a transformative effect on education where students are using it to help with homework assignments and teachers are actively employing it in their teaching practices. This includes using ChatGPT as a tool for writing teachers to grade and generate feedback on students' essays. In this study, we evaluated the quality of the feedback generated by ChatGPT regarding the coherence and cohesion of the essays written by English Language Learners (ELLs) students. We selected 50 argumentative essays and generated feedback on coherence and cohesion using the ELLIPSE rubric. During the feedback evaluation, we used a two-step approach: first, each sentence in the feedback was classified into subtypes based on its function (e.g., positive reinforcement, problem statement). Next, we evaluated its accuracy and usability according to these types. Both the analysis of feedback types and the evaluation of accuracy and usability revealed that most feedback sentences were highly abstract and generic, failing to provide concrete suggestions for improvement. The accuracy in detecting major problems, such as repetitive ideas and the inaccurate use of cohesive devices, depended on superficial linguistic features and was often incorrect. In conclusion, ChatGPT, without specific training for the feedback generation task, does not offer effective feedback on ELL students' coherence and cohesion.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)自其在11月2022年发布以来，ChatGPT已经对教育产生了transformative的影响，学生们使用它来帮助完成家庭作业，教师也活动地使用它在教学实践中。这包括使用ChatGPT来评估学生的作业，并提供反馈。在这项研究中，我们评估了ChatGPT对英语学习者（ELLs）学生的论文 coherence 和 cohesion 的反馈质量。我们选择50篇 Argumentative essay，并使用 ELLIPSE 分类法生成反馈。在反馈评估中，我们采用了两步方法：首先，每句反馈被分类为不同的函数类型（例如，正面鼓励、问题陈述）。然后，我们评估了它们的准确性和可用性。结果表明，大多数反馈句子具有高度抽象和通用的特点，无法提供具体的改进建议。检测重要问题的准确性，如重复的想法和不当使用 cohesive devices，通常基于表面语言特征，并且错误。结论，ChatGPT，无需特定的培训，不能提供有效的反馈对 ELL 学生的 coherence 和 cohesion。
</details></li>
</ul>
<hr>
<h2 id="Revisit-Input-Perturbation-Problems-for-LLMs-A-Unified-Robustness-Evaluation-Framework-for-Noisy-Slot-Filling-Task"><a href="#Revisit-Input-Perturbation-Problems-for-LLMs-A-Unified-Robustness-Evaluation-Framework-for-Noisy-Slot-Filling-Task" class="headerlink" title="Revisit Input Perturbation Problems for LLMs: A Unified Robustness Evaluation Framework for Noisy Slot Filling Task"></a>Revisit Input Perturbation Problems for LLMs: A Unified Robustness Evaluation Framework for Noisy Slot Filling Task</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06504">http://arxiv.org/abs/2310.06504</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dongguanting/noise-slot-filling-llm">https://github.com/dongguanting/noise-slot-filling-llm</a></li>
<li>paper_authors: Guanting Dong, Jinxu Zhao, Tingfeng Hui, Daichi Guo, Wenlong Wan, Boqi Feng, Yueyan Qiu, Zhuoma Gongque, Keqing He, Zechen Wang, Weiran Xu</li>
<li>for: 本研究旨在评估大语言模型（LLM）在实际噪声数据上的可靠性和可Robustness，以提高LLM的应用性和可靠性。</li>
<li>methods: 本研究提出了一种统一的 robustness评估框架，基于槽填充任务来系统地评估LLM在多种输入杂乱enario中的对话理解能力。研究者还构建了一个噪声数据集（Noise-LLM），包括单个杂乱数据和杂合杂乱数据等五种类型。此外，研究者采用了多级数据增强方法（字符、词和句子三级），并设计了两种自动任务示例构建策略（实例级和实体级），以便评估LLM在实际噪声数据上的可Robustness性。</li>
<li>results: 实验结果表明，目前的开源LLMs在实际噪声数据上的杂乱Robustness性表现很有限。基于这些实验观察结果，研究者提出了一些前瞻的建议，以促进这方面的研究。<details>
<summary>Abstract</summary>
With the increasing capabilities of large language models (LLMs), these high-performance models have achieved state-of-the-art results on a wide range of natural language processing (NLP) tasks. However, the models' performance on commonly-used benchmark datasets often fails to accurately reflect their reliability and robustness when applied to real-world noisy data. To address these challenges, we propose a unified robustness evaluation framework based on the slot-filling task to systematically evaluate the dialogue understanding capability of LLMs in diverse input perturbation scenarios. Specifically, we construct a input perturbation evaluation dataset, Noise-LLM, which contains five types of single perturbation and four types of mixed perturbation data. Furthermore, we utilize a multi-level data augmentation method (character, word, and sentence levels) to construct a candidate data pool, and carefully design two ways of automatic task demonstration construction strategies (instance-level and entity-level) with various prompt templates. Our aim is to assess how well various robustness methods of LLMs perform in real-world noisy scenarios. The experiments have demonstrated that the current open-source LLMs generally achieve limited perturbation robustness performance. Based on these experimental observations, we make some forward-looking suggestions to fuel the research in this direction.
</details>
<details>
<summary>摘要</summary>
随着大型语言模型（LLMs）的能力不断提高，这些高性能模型在各种自然语言处理（NLP）任务上取得了状态之最的成绩。然而，这些模型在通常使用的基准数据集上的性能经常不能准确反映它们在实际噪音数据上的可靠性和可靠性。为解决这些挑战，我们提议一种统一的可靠性评估框架，基于槽填充任务来系统地评估 LLMS 在多种噪音数据下的对话理解能力。具体来说，我们构建了一个噪音评估数据集，即 Noise-LLM，该数据集包括5种单个噪音数据和4种混合噪音数据。此外，我们采用了多级数据增强方法（字符、词和句子级别），将候选数据池构建起来，并且仔细设计了两种自动任务示例构建策略（实例级和实体级），并使用了多种提示模板。我们的目标是评估不同robustness方法在实际噪音场景下的表现。实验结果表明，当前的开源 LLMS 通常在实际噪音场景下表现有限的鲁棒性能。根据这些实验观察结果，我们提出了一些前瞻的建议，以促进这一方向的研究。
</details></li>
</ul>
<hr>
<h2 id="MetaAgents-Simulating-Interactions-of-Human-Behaviors-for-LLM-based-Task-oriented-Coordination-via-Collaborative-Generative-Agents"><a href="#MetaAgents-Simulating-Interactions-of-Human-Behaviors-for-LLM-based-Task-oriented-Coordination-via-Collaborative-Generative-Agents" class="headerlink" title="MetaAgents: Simulating Interactions of Human Behaviors for LLM-based Task-oriented Coordination via Collaborative Generative Agents"></a>MetaAgents: Simulating Interactions of Human Behaviors for LLM-based Task-oriented Coordination via Collaborative Generative Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06500">http://arxiv.org/abs/2310.06500</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuan Li, Yixuan Zhang, Lichao Sun</li>
<li>for: 这篇论文旨在探讨 Large Language Models (LLMs) 在各种任务和社会 simulations 中的应用，以及它们在任务导向的社会 context 中的协调能力。</li>
<li>methods: 作者提出了一种新的框架，该框架将 LLM-based Agents 具备人类型的理解能力和专业技能，以便在协调任务中表现出人类型的行为。</li>
<li>results: 作者的评估表明，这些协同生成代理人在一个 simulated job fair 环境中表现出了有前途的表现，但是也暴露出了在更复杂的协调任务中的局限性。<details>
<summary>Abstract</summary>
Significant advancements have occurred in the application of Large Language Models (LLMs) for various tasks and social simulations. Despite this, their capacities to coordinate within task-oriented social contexts are under-explored. Such capabilities are crucial if LLMs are to effectively mimic human-like social behavior and produce meaningful results. To bridge this gap, we introduce collaborative generative agents, endowing LLM-based Agents with consistent behavior patterns and task-solving abilities. We situate these agents in a simulated job fair environment as a case study to scrutinize their coordination skills. We propose a novel framework that equips collaborative generative agents with human-like reasoning abilities and specialized skills. Our evaluation demonstrates that these agents show promising performance. However, we also uncover limitations that hinder their effectiveness in more complex coordination tasks. Our work provides valuable insights into the role and evolution of LLMs in task-oriented social simulations.
</details>
<details>
<summary>摘要</summary>
<<SYS>>大量的进步已经发生在大语言模型（LLM）的应用中，包括不同的任务和社会 simulations。然而，LLM在任务团队社会上的协调能力仍然未得到充分探索。这些能力是LLM模仿人类社会行为的关键，以生成有意义的结果。为了bridging这个差距，我们引入合作生成代理人，赋予LLM基于代理人的一致行为模式和任务解决能力。我们在模拟的就业 fair环境中作为一个案例，检验这些代理人的协调能力。我们提出了一种新的框架，让合作生成代理人具有人类化的思维能力和专业技能。我们的评估表明，这些代理人在完成任务时表现了promising的表现。然而，我们还发现了一些限制，这些限制阻碍了它们在更复杂的协调任务中的效果。我们的工作为LLM在任务团队社会 simulations中的角色和演化提供了重要的看法。
</details></li>
</ul>
<hr>
<h2 id="Topological-RANSAC-for-instance-verification-and-retrieval-without-fine-tuning"><a href="#Topological-RANSAC-for-instance-verification-and-retrieval-without-fine-tuning" class="headerlink" title="Topological RANSAC for instance verification and retrieval without fine-tuning"></a>Topological RANSAC for instance verification and retrieval without fine-tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06486">http://arxiv.org/abs/2310.06486</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guoyuan An, Juhyung Seon, Inkyu An, Yuchi Huo, Sung-Eui Yoon</li>
<li>for: 提高可解释的图像检索，尤其在无精度调整集的情况下。</li>
<li>methods: 取代传统的空间模型，采用 topological 模型，并通过生物体注意力的启发函数来验证特征之间的topological一致性。</li>
<li>results: 与传统的 SP 方法相比，我们的方法在非精度调整情况下显著提高检索性能，并且可以增强使用精度调整的特征表现。此外，我们的方法具有高可解释性和轻量级的特点，适用于各种实际应用场景。<details>
<summary>Abstract</summary>
This paper presents an innovative approach to enhancing explainable image retrieval, particularly in situations where a fine-tuning set is unavailable. The widely-used SPatial verification (SP) method, despite its efficacy, relies on a spatial model and the hypothesis-testing strategy for instance recognition, leading to inherent limitations, including the assumption of planar structures and neglect of topological relations among features. To address these shortcomings, we introduce a pioneering technique that replaces the spatial model with a topological one within the RANSAC process. We propose bio-inspired saccade and fovea functions to verify the topological consistency among features, effectively circumventing the issues associated with SP's spatial model. Our experimental results demonstrate that our method significantly outperforms SP, achieving state-of-the-art performance in non-fine-tuning retrieval. Furthermore, our approach can enhance performance when used in conjunction with fine-tuned features. Importantly, our method retains high explainability and is lightweight, offering a practical and adaptable solution for a variety of real-world applications.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Memory-efficient-location-recommendation-through-proximity-aware-representation"><a href="#Memory-efficient-location-recommendation-through-proximity-aware-representation" class="headerlink" title="Memory efficient location recommendation through proximity-aware representation"></a>Memory efficient location recommendation through proximity-aware representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06484">http://arxiv.org/abs/2310.06484</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuan Luo, Mingqing Huang, Rui Lv, Hui Zhao</li>
<li>for: 增强用户体验、提高企业收益和政府管理效率，帮助用户更好地探索和选择附近的场所。</li>
<li>methods: 基于Self-Attention Network架构，提出了一种适应范围内的离散地点表示方法（PASR），解决数据稀缺问题，并充分利用地理信息。</li>
<li>results: 使用三个实际的 Location-Based Social Networking（LBSN）数据集进行评估，显示PASR在续ous sequential location recommendation方法中占据了领先地位。<details>
<summary>Abstract</summary>
Sequential location recommendation plays a huge role in modern life, which can enhance user experience, bring more profit to businesses and assist in government administration. Although methods for location recommendation have evolved significantly thanks to the development of recommendation systems, there is still limited utilization of geographic information, along with the ongoing challenge of addressing data sparsity. In response, we introduce a Proximity-aware based region representation for Sequential Recommendation (PASR for short), built upon the Self-Attention Network architecture. We tackle the sparsity issue through a novel loss function employing importance sampling, which emphasizes informative negative samples during optimization. Moreover, PASR enhances the integration of geographic information by employing a self-attention-based geography encoder to the hierarchical grid and proximity grid at each GPS point. To further leverage geographic information, we utilize the proximity-aware negative samplers to enhance the quality of negative samples. We conducted evaluations using three real-world Location-Based Social Networking (LBSN) datasets, demonstrating that PASR surpasses state-of-the-art sequential location recommendation methods
</details>
<details>
<summary>摘要</summary>
现代生活中的顺序位置推荐具有巨大的作用，可以提高用户体验、带来更多的商业利益以及政府管理的帮助。虽然位置推荐的方法已经发展到了很高的水平，但是还是受到地理信息的有限使用和缺乏数据的挑战。为了解决这个问题，我们介绍了一种基于靠近性的区域表示方法（PASR简称），基于自注意网络架构。我们通过一种新的损失函数和重要样本选择来解决缺乏数据的问题，并且使用自注意网络来增强地理信息的集成。此外，我们还利用靠近性aware的负样本来提高负样本的质量。我们对三个实际的位置基于社交媒体网络（LBSN）数据集进行了评估，结果表明，PASR超越了现状最佳的顺序位置推荐方法。
</details></li>
</ul>
<hr>
<h2 id="Understanding-the-Effects-of-RLHF-on-LLM-Generalisation-and-Diversity"><a href="#Understanding-the-Effects-of-RLHF-on-LLM-Generalisation-and-Diversity" class="headerlink" title="Understanding the Effects of RLHF on LLM Generalisation and Diversity"></a>Understanding the Effects of RLHF on LLM Generalisation and Diversity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06452">http://arxiv.org/abs/2310.06452</a></li>
<li>repo_url: None</li>
<li>paper_authors: Robert Kirk, Ishita Mediratta, Christoforos Nalmpantis, Jelena Luketina, Eric Hambro, Edward Grefenstette, Roberta Raileanu</li>
<li>for: 这paper主要探讨了大语言模型（LLMs）通过人类反馈学习（RLHF）进行微调的方法，以及这些方法在两个关键性能指标（OOD泛化和输出多样性）上的影响。</li>
<li>methods: 这paper使用了三个不同的微调阶段（Supervised Fine-Tuning（SFT）、奖励模型和RLHF），并进行了广泛的分析，以了解每个阶段对OOD泛化和输出多样性的影响。</li>
<li>results: 研究发现，RLHF比SFT在新输入处理更好的泛化能力，尤其是当输入和输出之间的分布差异较大时。然而，RLHF会对输出多样性产生负面影响，特别是在多种指标上。这些结果可以帮助选择合适的微调方法，并促进RLHF方法的进一步改进。<details>
<summary>Abstract</summary>
Large language models (LLMs) fine-tuned with reinforcement learning from human feedback (RLHF) have been used in some of the most widely deployed AI models to date, such as OpenAI's ChatGPT, Anthropic's Claude, or Meta's LLaMA-2. While there has been significant work developing these methods, our understanding of the benefits and downsides of each stage in RLHF is still limited. To fill this gap, we present an extensive analysis of how each stage of the process (i.e. supervised fine-tuning (SFT), reward modelling, and RLHF) affects two key properties: out-of-distribution (OOD) generalisation and output diversity. OOD generalisation is crucial given the wide range of real-world scenarios in which these models are being used, while output diversity refers to the model's ability to generate varied outputs and is important for a variety of use cases. We perform our analysis across two base models on both summarisation and instruction following tasks, the latter being highly relevant for current LLM use cases. We find that RLHF generalises better than SFT to new inputs, particularly as the distribution shift between train and test becomes larger. However, RLHF significantly reduces output diversity compared to SFT across a variety of measures, implying a tradeoff in current LLM fine-tuning methods between generalisation and diversity. Our results provide guidance on which fine-tuning method should be used depending on the application, and show that more research is needed to improve the trade-off between generalisation and diversity.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）通过人工测验学习（RLHF）的精练化已经在一些最广泛应用的AI模型中使用，如OpenAI的ChatGPT、Anthropic的Claude或Meta的LLaMA-2。 although there has been significant work developing these methods, our understanding of the benefits and downsides of each stage in RLHF is still limited. To fill this gap, we present an extensive analysis of how each stage of the process (i.e. supervised fine-tuning (SFT), reward modeling, and RLHF) affects two key properties: out-of-distribution (OOD) generalization and output diversity. OOD generalization is crucial given the wide range of real-world scenarios in which these models are being used, while output diversity refers to the model's ability to generate varied outputs and is important for a variety of use cases. We perform our analysis across two base models on both summarization and instruction following tasks, the latter being highly relevant for current LLM use cases. We find that RLHF generalizes better than SFT to new inputs, particularly as the distribution shift between train and test becomes larger. However, RLHF significantly reduces output diversity compared to SFT across a variety of measures, implying a tradeoff in current LLM fine-tuning methods between generalization and diversity. Our results provide guidance on which fine-tuning method should be used depending on the application, and show that more research is needed to improve the trade-off between generalization and diversity.
</details></li>
</ul>
<hr>
<h2 id="Constructive-Large-Language-Models-Alignment-with-Diverse-Feedback"><a href="#Constructive-Large-Language-Models-Alignment-with-Diverse-Feedback" class="headerlink" title="Constructive Large Language Models Alignment with Diverse Feedback"></a>Constructive Large Language Models Alignment with Diverse Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06450">http://arxiv.org/abs/2310.06450</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianshu Yu, Ting-En Lin, Yuchuan Wu, Min Yang, Fei Huang, Yongbin Li</li>
<li>for: 强调大语言模型（LLM）与人类价值观 aligning，以减少危险内容的影响。</li>
<li>methods: 我们介绍了一种新的 Constructive and Diverse Feedback（CDF）方法， inspirited by constructivist learning theory，收集了三种不同类型的反馈，包括批评反馈、纠正反馈和喜好反馈，以便在训练数据集中解决不同难度级别的问题。</li>
<li>results: 我们通过对三个下游任务（问答、对话生成和文本概要）进行评估，发现 CDF 方法可以在较小的训练数据集上 достичь更高的对齐性表现，比之前的方法更高。<details>
<summary>Abstract</summary>
In recent research on large language models (LLMs), there has been a growing emphasis on aligning these models with human values to reduce the impact of harmful content. However, current alignment methods often rely solely on singular forms of human feedback, such as preferences, annotated labels, or natural language critiques, overlooking the potential advantages of combining these feedback types. This limitation leads to suboptimal performance, even when ample training data is available. In this paper, we introduce Constructive and Diverse Feedback (CDF) as a novel method to enhance LLM alignment, inspired by constructivist learning theory. Our approach involves collecting three distinct types of feedback tailored to problems of varying difficulty levels within the training dataset. Specifically, we exploit critique feedback for easy problems, refinement feedback for medium problems, and preference feedback for hard problems. By training our model with this diversified feedback, we achieve enhanced alignment performance while using less training data. To assess the effectiveness of CDF, we evaluate it against previous methods in three downstream tasks: question answering, dialog generation, and text summarization. Experimental results demonstrate that CDF achieves superior performance even with a smaller training dataset.
</details>
<details>
<summary>摘要</summary>
Recent research on large language models (LLMs) 有增加对人类价值观Alignment的强调，以降低有害内容的影响。然而，现有的Alignment方法通常仅仅基于单一的人类反馈方式，如偏好、注释标签或自然语言批评，而忽视了可 combining这些反馈类型的可能性。这种局限性导致模型性能不佳，即使有充足的训练数据available。在这篇论文中，我们提出了一种新的Feedback方法，名为Constructive and Diverse Feedback（CDF）， draws inspiration from constructivist learning theory。我们的方法是收集三种不同类型的反馈，适用于训练数据中的问题Difficulty Level不同的情况。特别是，我们利用了批评Feedback来解决容易的问题，修充Feedback来解决中等Difficulty Level的问题，以及偏好Feedback来解决困难的问题。通过使用这种多样化的反馈，我们可以增强模型的Alignment性能，并使用较少的训练数据。为评估CDF的效果，我们对之前的方法进行了三个下游任务的评估：问题回答、对话生成和文本概要。实验结果表明，CDF可以在较少的训练数据下达到更高的性能。
</details></li>
</ul>
<hr>
<h2 id="Stepwise-functional-refoundation-of-relational-concept-analysis"><a href="#Stepwise-functional-refoundation-of-relational-concept-analysis" class="headerlink" title="Stepwise functional refoundation of relational concept analysis"></a>Stepwise functional refoundation of relational concept analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06441">http://arxiv.org/abs/2310.06441</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jérôme Euzenat</li>
<li>for: 学习描述逻辑理论从数据中得到</li>
<li>methods: 基于数据的形式概念分析扩展，以处理多个相关的 контексты</li>
<li>results: 返回一个家族的概念树，而不考虑数据中循环依赖关系时，可能存在其他可接受的解决方案<details>
<summary>Abstract</summary>
Relational concept analysis (RCA) is an extension of formal concept analysis allowing to deal with several related contexts simultaneously. It has been designed for learning description logic theories from data and used within various applications. A puzzling observation about RCA is that it returns a single family of concept lattices although, when the data feature circular dependencies, other solutions may be considered acceptable. The semantics of RCA, provided in an operational way, does not shed light on this issue. In this report, we define these acceptable solutions as those families of concept lattices which belong to the space determined by the initial contexts (well-formed), cannot scale new attributes (saturated), and refer only to concepts of the family (self-supported). We adopt a functional view on the RCA process by defining the space of well-formed solutions and two functions on that space: one expansive and the other contractive. We show that the acceptable solutions are the common fixed points of both functions. This is achieved step-by-step by starting from a minimal version of RCA that considers only one single context defined on a space of contexts and a space of lattices. These spaces are then joined into a single space of context-lattice pairs, which is further extended to a space of indexed families of context-lattice pairs representing the objects manip
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Whispering-LLaMA-A-Cross-Modal-Generative-Error-Correction-Framework-for-Speech-Recognition"><a href="#Whispering-LLaMA-A-Cross-Modal-Generative-Error-Correction-Framework-for-Speech-Recognition" class="headerlink" title="Whispering LLaMA: A Cross-Modal Generative Error Correction Framework for Speech Recognition"></a>Whispering LLaMA: A Cross-Modal Generative Error Correction Framework for Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06434">http://arxiv.org/abs/2310.06434</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/srijith-rkr/whispering-llama">https://github.com/srijith-rkr/whispering-llama</a></li>
<li>paper_authors: Srijith Radhakrishnan, Chao-Han Huck Yang, Sumeer Ahmad Khan, Rohit Kumar, Narsis A. Kiani, David Gomez-Cabrero, Jesper N. Tegner</li>
<li>for: 这个论文是为了提出一种新的跨模态融合技术，用于自动语音识别（ASR）中的生成错误修正。</li>
<li>methods: 该方法利用了语音信息和外部语言表示来生成准确的语音转文本上下文。这标志着一种新的 парадигshift towards generative error correction within the realm of n-best hypotheses。</li>
<li>results: 对于多种ASR dataset的评估，我们证明了我们的融合技术的稳定性和可重现性，并达到了相对于n-best假设的37.66%的单词错误率改善。Here’s the full answer in Simplified Chinese:</li>
<li>for: 这个论文是为了提出一种新的跨模态融合技术，用于自动语音识别（ASR）中的生成错误修正。</li>
<li>methods: 该方法利用了语音信息和外部语言表示来生成准确的语音转文本上下文。这标志着一种新的 парадигshift towards generative error correction within the realm of n-best hypotheses。</li>
<li>results: 对于多种ASR dataset的评估，我们证明了我们的融合技术的稳定性和可重现性，并达到了相对于n-best假设的37.66%的单词错误率改善。<details>
<summary>Abstract</summary>
We introduce a new cross-modal fusion technique designed for generative error correction in automatic speech recognition (ASR). Our methodology leverages both acoustic information and external linguistic representations to generate accurate speech transcription contexts. This marks a step towards a fresh paradigm in generative error correction within the realm of n-best hypotheses. Unlike the existing ranking-based rescoring methods, our approach adeptly uses distinct initialization techniques and parameter-efficient algorithms to boost ASR performance derived from pre-trained speech and text models. Through evaluation across diverse ASR datasets, we evaluate the stability and reproducibility of our fusion technique, demonstrating its improved word error rate relative (WERR) performance in comparison to n-best hypotheses by relatively 37.66%. To encourage future research, we have made our code and pre-trained models open source at https://github.com/Srijith-rkr/Whispering-LLaMA.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种新的跨Modal融合技术，用于自动语音识别（ASR）的生成错误修正。我们的方法ология利用了语音信息和外部语言表示来生成准确的语音转文本上下文。这标志着一个新的 парадигshift towards a fresh paradigm in generative error correction within the realm of n-best hypotheses。不同于现有的排名基于重新分配方法，我们的方法使用了不同的初始化技术和参数高效的算法来提高ASR性能，基于预训练的语音和文本模型。通过对多个ASR数据集的评估，我们评估了我们的融合技术的稳定性和可重现性，并证明了它的单词错误率相对改进（WERR）性能，相比于n-best假设的37.66%。为了鼓励未来的研究，我们将我们的代码和预训练模型公开在GitHub上，请参考https://github.com/Srijith-rkr/Whispering-LLaMA。
</details></li>
</ul>
<hr>
<h2 id="Retromorphic-Testing-A-New-Approach-to-the-Test-Oracle-Problem"><a href="#Retromorphic-Testing-A-New-Approach-to-the-Test-Oracle-Problem" class="headerlink" title="Retromorphic Testing: A New Approach to the Test Oracle Problem"></a>Retromorphic Testing: A New Approach to the Test Oracle Problem</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06433">http://arxiv.org/abs/2310.06433</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cuhk-shenzhen-se/retromorphictesting">https://github.com/cuhk-shenzhen-se/retromorphictesting</a></li>
<li>paper_authors: Boxi Yu, Qiuyang Mang, Qingshuo Guo, Pinjia He</li>
<li>for:  This paper focuses on developing a novel black-box testing methodology called Retromorphic Testing, which is inspired by the mathematical concept of inverse functions. The purpose is to provide a non-intrusive and effective approach to testing software systems.</li>
<li>methods:  The proposed method uses an auxiliary program in conjunction with the program under test, creating a dual-program structure. The input data is processed by the forward program, and then the output is reversed to its original input format using the backward program. The testing modes include using the auxiliary program as either the forward or backward program.</li>
<li>results:  The paper presents three testing modes with illustrative use cases across diverse programs, including algorithms, traditional software, and AI applications. The method is demonstrated to be effective in revealing defects and bugs in the software systems under test.<details>
<summary>Abstract</summary>
A test oracle serves as a criterion or mechanism to assess the correspondence between software output and the anticipated behavior for a given input set. In automated testing, black-box techniques, known for their non-intrusive nature in test oracle construction, are widely used, including notable methodologies like differential testing and metamorphic testing. Inspired by the mathematical concept of inverse function, we present Retromorphic Testing, a novel black-box testing methodology. It leverages an auxiliary program in conjunction with the program under test, which establishes a dual-program structure consisting of a forward program and a backward program. The input data is first processed by the forward program and then its program output is reversed to its original input format using the backward program. In particular, the auxiliary program can operate as either the forward or backward program, leading to different testing modes. The process concludes by examining the relationship between the initial input and the transformed output within the input domain. For example, to test the implementation of the sine function $\sin(x)$, we can employ its inverse function, $\arcsin(x)$, and validate the equation $x = \sin(\arcsin(x)+2k\pi), \forall k \in \mathbb{Z}$. In addition to the high-level concept of Retromorphic Testing, this paper presents its three testing modes with illustrative use cases across diverse programs, including algorithms, traditional software, and AI applications.
</details>
<details>
<summary>摘要</summary>
一个测试 oracle serves as a criterion or mechanism to assess the correspondence between software output and the anticipated behavior for a given input set. In automated testing, black-box techniques, known for their non-intrusive nature in test oracle construction, are widely used, including notable methodologies like differential testing and metamorphic testing. Inspired by the mathematical concept of inverse function, we present Retromorphic Testing, a novel black-box testing methodology. It leverages an auxiliary program in conjunction with the program under test, which establishes a dual-program structure consisting of a forward program and a backward program. The input data is first processed by the forward program and then its program output is reversed to its original input format using the backward program. In particular, the auxiliary program can operate as either the forward or backward program, leading to different testing modes. The process concludes by examining the relationship between the initial input and the transformed output within the input domain. For example, to test the implementation of the sine function $\sin(x)$, we can employ its inverse function, $\arcsin(x)$, and validate the equation $x = \sin(\arcsin(x)+2k\pi), \forall k \in \mathbb{Z}$. In addition to the high-level concept of Retromorphic Testing, this paper presents its three testing modes with illustrative use cases across diverse programs, including algorithms, traditional software, and AI applications.
</details></li>
</ul>
<hr>
<h2 id="Proceedings-of-The-first-international-workshop-on-eXplainable-AI-for-the-Arts-XAIxArts"><a href="#Proceedings-of-The-first-international-workshop-on-eXplainable-AI-for-the-Arts-XAIxArts" class="headerlink" title="Proceedings of The first international workshop on eXplainable AI for the Arts (XAIxArts)"></a>Proceedings of The first international workshop on eXplainable AI for the Arts (XAIxArts)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06428">http://arxiv.org/abs/2310.06428</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nick Bryan-Kinns, Corey Ford, Alan Chamberlain, Steven David Benford, Helen Kennedy, Zijin Li, Wu Qiong, Gus G. Xia, Jeba Rezwana</li>
<li>for: 这个论文是为了探讨XAI在艺术领域的应用而写的。</li>
<li>methods: 论文使用了HCI、交互设计、AI、可解释AI（XAI）和数字艺术等领域的研究方法。</li>
<li>results: 论文在XAI在艺术领域的应用中发现了一些有价值的结果。<details>
<summary>Abstract</summary>
This first international workshop on explainable AI for the Arts (XAIxArts) brought together a community of researchers in HCI, Interaction Design, AI, explainable AI (XAI), and digital arts to explore the role of XAI for the Arts.   Workshop held at the 15th ACM Conference on Creativity and Cognition (C&C 2023).
</details>
<details>
<summary>摘要</summary>
这是第一届国际工作坊 on 可解释 AI for the Arts (XAIxArts), 它将聚集一群研究人员来探讨 XAI 在艺术领域的角色。 工作坊于 ACM 创造力和认知会议 (C&C 2023) 举行。
</details></li>
</ul>
<hr>
<h2 id="TANGO-Time-Reversal-Latent-GraphODE-for-Multi-Agent-Dynamical-Systems"><a href="#TANGO-Time-Reversal-Latent-GraphODE-for-Multi-Agent-Dynamical-Systems" class="headerlink" title="TANGO: Time-Reversal Latent GraphODE for Multi-Agent Dynamical Systems"></a>TANGO: Time-Reversal Latent GraphODE for Multi-Agent Dynamical Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06427">http://arxiv.org/abs/2310.06427</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zijie Huang, Wanjia Zhao, Jingdong Gao, Ziniu Hu, Xiao Luo, Yadi Cao, Yuanzhou Chen, Yizhou Sun, Wei Wang</li>
<li>for: 学习复杂多代理系统动力学从数据中是许多领域的关键问题，如物理 simulate 和材料模型。</li>
<li>methods: 我们提出了一种简单 yet effective的自我supervised regularization项，该项用作一个软件约束，将前进和后退的轨迹预测值align，以实现时间反转对称性。</li>
<li>results: 我们的方法在各种物理系统上实验表现出色，特别是在困难的混沌三杆系统上实现了11.5%的MSE提高。<details>
<summary>Abstract</summary>
Learning complex multi-agent system dynamics from data is crucial across many domains, such as in physical simulations and material modeling. Extended from purely data-driven approaches, existing physics-informed approaches such as Hamiltonian Neural Network strictly follow energy conservation law to introduce inductive bias, making their learning more sample efficiently. However, many real-world systems do not strictly conserve energy, such as spring systems with frictions. Recognizing this, we turn our attention to a broader physical principle: Time-Reversal Symmetry, which depicts that the dynamics of a system shall remain invariant when traversed back over time. It still helps to preserve energies for conservative systems and in the meanwhile, serves as a strong inductive bias for non-conservative, reversible systems. To inject such inductive bias, in this paper, we propose a simple-yet-effective self-supervised regularization term as a soft constraint that aligns the forward and backward trajectories predicted by a continuous graph neural network-based ordinary differential equation (GraphODE). It effectively imposes time-reversal symmetry to enable more accurate model predictions across a wider range of dynamical systems under classical mechanics. In addition, we further provide theoretical analysis to show that our regularization essentially minimizes higher-order Taylor expansion terms during the ODE integration steps, which enables our model to be more noise-tolerant and even applicable to irreversible systems. Experimental results on a variety of physical systems demonstrate the effectiveness of our proposed method. Particularly, it achieves an MSE improvement of 11.5 % on a challenging chaotic triple-pendulum systems.
</details>
<details>
<summary>摘要</summary>
学习复杂多代理系统动态从数据是透支多个领域的关键，如物理 simulate 和材料模型。从某种意义上说，现有的物理 Informed Approach 如 Hamiltonian Neural Network 会 strictly follow 能量保守法则来引入 inductive bias，使其学习更加有效。然而，许多实际系统不会严格地保守能量，例如带有摩擦的spring系统。认识到这一点，我们转向了更广泛的物理原理：时间反转 симметry，即系统的动态将在时间反转后保持不变。它可以保持能量的方面，而且在非保守系统中也能提供强的 inductive bias。为了把这种 inductive bias 引入，在这篇论文中，我们提议一种简单 yet 有效的自顾supervised regularization term，用于规范一个基于 continues graph neural network 的 ordinary differential equation (GraphODE) 中的前进和后退轨迹预测。它有效地强制实现时间反转对称，使得模型预测更加准确，并且可以涵盖更广泛的物理系统。此外，我们还提供了理论分析，证明我们的 regularization 实际上在 ODE  интеграル步骤中减少高阶泰勒展开项，使我们的模型更加快速敏感和灵活应用于不可逆系统。实验结果表明，我们提议的方法在多种物理系统上都有效。特别是，它在一个复杂的混沌三个拖钩系统上达到了11.5%的MSE提升。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Models-for-Propaganda-Detection"><a href="#Large-Language-Models-for-Propaganda-Detection" class="headerlink" title="Large Language Models for Propaganda Detection"></a>Large Language Models for Propaganda Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06422">http://arxiv.org/abs/2310.06422</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/submissionemnlp/llm_propaganda_detection">https://github.com/submissionemnlp/llm_propaganda_detection</a></li>
<li>paper_authors: Kilian Sprenkamp, Daniel Gordon Jones, Liudmila Zavolokina</li>
<li>for: 检测宣传的效果，帮助维护社会和谐和真实信息的传播。</li>
<li>methods: 使用现代大型自然语言模型（LLMs）如GPT-3和GPT-4进行宣传检测，采用多种提示工程和精度调整策略。</li>
<li>results: GPT-4达到了与当前状态艺术的相当水平，并且对宣传技巧的检测具有较高的精度和准确性。<details>
<summary>Abstract</summary>
The prevalence of propaganda in our digital society poses a challenge to societal harmony and the dissemination of truth. Detecting propaganda through NLP in text is challenging due to subtle manipulation techniques and contextual dependencies. To address this issue, we investigate the effectiveness of modern Large Language Models (LLMs) such as GPT-3 and GPT-4 for propaganda detection. We conduct experiments using the SemEval-2020 task 11 dataset, which features news articles labeled with 14 propaganda techniques as a multi-label classification problem. Five variations of GPT-3 and GPT-4 are employed, incorporating various prompt engineering and fine-tuning strategies across the different models. We evaluate the models' performance by assessing metrics such as $F1$ score, $Precision$, and $Recall$, comparing the results with the current state-of-the-art approach using RoBERTa. Our findings demonstrate that GPT-4 achieves comparable results to the current state-of-the-art. Further, this study analyzes the potential and challenges of LLMs in complex tasks like propaganda detection.
</details>
<details>
<summary>摘要</summary>
现代社会中的宣传活动带来了社会和真实信息的困难。检测宣传的自然语言处理（NLP）在文本中是一个挑战，因为宣传者可以通过某些细微的操纵技巧和上下文依赖来隐秘宣传。为解决这个问题，我们调查了现代大型语言模型（LLM）such as GPT-3和GPT-4的宣传检测效果。我们在SemEval-2020任务11数据集上进行实验，这是一个新闻文章标注有14种宣传技巧的多标签分类问题。我们使用5种不同的GPT-3和GPT-4模型，包括不同的提问工程和精度调整策略。我们评估模型的表现，包括$F1$分数、$Precision$和$Recall$指标，并与使用RoBERTa的当前状态态-of-the-art方法进行比较。我们的发现表明GPT-4在当前状态态-of-the-art方法中获得了相似的结果。此外，这种研究还分析了LLMs在复杂任务中的潜在和挑战。
</details></li>
</ul>
<hr>
<h2 id="Advective-Diffusion-Transformers-for-Topological-Generalization-in-Graph-Learning"><a href="#Advective-Diffusion-Transformers-for-Topological-Generalization-in-Graph-Learning" class="headerlink" title="Advective Diffusion Transformers for Topological Generalization in Graph Learning"></a>Advective Diffusion Transformers for Topological Generalization in Graph Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06417">http://arxiv.org/abs/2310.06417</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qitian Wu, Chenxiao Yang, Kaipeng Zeng, Fan Nie, Michael Bronstein, Junchi Yan</li>
<li>for: 本研究旨在理解Graph Neural Networks（GNN）在不同图结构下的泛化能力，以及如何使用扩散方程来推断GNN的表达力和建构方法。</li>
<li>methods: 本研究使用了非本地扩散方法，即在图上传播特征的方法，以解决当前方法对图结构的假设导致的泛化能力问题。此外，本研究还提出了一种新的图编码器基础结构，即扩散傅里叶变换（ADiT），该模型基于扩散方程的关闭式解，并具有理论上的保证性。</li>
<li>results: 本研究的实验结果表明，使用非本地扩散方法和ADiT模型可以在多种图学任务上实现superior表现，并且在图结构下降情况下保持良好的泛化能力。<details>
<summary>Abstract</summary>
Graph diffusion equations are intimately related to graph neural networks (GNNs) and have recently attracted attention as a principled framework for analyzing GNN dynamics, formalizing their expressive power, and justifying architectural choices. One key open questions in graph learning is the generalization capabilities of GNNs. A major limitation of current approaches hinges on the assumption that the graph topologies in the training and test sets come from the same distribution. In this paper, we make steps towards understanding the generalization of GNNs by exploring how graph diffusion equations extrapolate and generalize in the presence of varying graph topologies. We first show deficiencies in the generalization capability of existing models built upon local diffusion on graphs, stemming from the exponential sensitivity to topology variation. Our subsequent analysis reveals the promise of non-local diffusion, which advocates for feature propagation over fully-connected latent graphs, under the assumption of a specific data-generating condition. In addition to these findings, we propose a novel graph encoder backbone, Advective Diffusion Transformer (ADiT), inspired by advective graph diffusion equations that have a closed-form solution backed up with theoretical guarantees of desired generalization under topological distribution shifts. The new model, functioning as a versatile graph Transformer, demonstrates superior performance across a wide range of graph learning tasks.
</details>
<details>
<summary>摘要</summary>
GRAPH diffusion equations 是 GNN 的关联方法，最近受到关注，作为 GNN 的分析框架、表达力 formalization 和建筑设计的原则。一个关键的开问在 GRAPH 学习中是 GNN 的通用能力。现有的方法假设 training 和 test 集中的 GRAPH 结构来自同一个分布，这是一个主要的限制。在这篇论文中，我们向 GRAPH  diffusion equations 的推广和通用性进行了研究。我们首先表明了现有的 LOCAL diffusion 模型在 GRAPH 结构变化方面存在欠佳的泛化能力，这是由 GRAPH 结构变化导致的极敏感性引起的。我们的后续分析表明了非 LOCAL diffusion 的潜在优势，它强调在具有完全相关的 latent graph 上进行特征传播，对于特定的数据生成条件，它具有理论保证的泛化性。此外，我们提出了一种新的 GRAPH 编码器基础，即 Advective Diffusion Transformer (ADiT)，这种基础是基于 advective GRAPH diffusion equations 的关键解。新的模型，作为一种多样的 GRAPH Transformer，在各种 GRAPH 学习任务中显示出了优秀的表现。
</details></li>
</ul>
<hr>
<h2 id="Hexa-Self-Improving-for-Knowledge-Grounded-Dialogue-System"><a href="#Hexa-Self-Improving-for-Knowledge-Grounded-Dialogue-System" class="headerlink" title="Hexa: Self-Improving for Knowledge-Grounded Dialogue System"></a>Hexa: Self-Improving for Knowledge-Grounded Dialogue System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06404">http://arxiv.org/abs/2310.06404</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daejin Jo, Daniel Wontae Nam, Gunsoo Han, Kyoung-Woon On, Taehwan Kwon, Seungeun Rho, Sungwoong Kim</li>
<li>for: 本研究旨在提高知识基〉的对话生成能力，通过使用中间步骤（如网络搜索、记忆回忆）和幂件方法。</li>
<li>methods: 我们提出了一种自我改进的方法，使用指导提示和修改的损失函数来提高自生成的回答的多样性。</li>
<li>results: 我们通过对多个 benchmark 数据集进行实验，证明了我们的方法可以成功地使用自我改进机制来生成中间和最终回答，并提高了知识基〉的对话生成能力。<details>
<summary>Abstract</summary>
A common practice in knowledge-grounded dialogue generation is to explicitly utilize intermediate steps (e.g., web-search, memory retrieval) with modular approaches. However, data for such steps are often inaccessible compared to those of dialogue responses as they are unobservable in an ordinary dialogue. To fill in the absence of these data, we develop a self-improving method to improve the generative performances of intermediate steps without the ground truth data. In particular, we propose a novel bootstrapping scheme with a guided prompt and a modified loss function to enhance the diversity of appropriate self-generated responses. Through experiments on various benchmark datasets, we empirically demonstrate that our method successfully leverages a self-improving mechanism in generating intermediate and final responses and improves the performances on the task of knowledge-grounded dialogue generation.
</details>
<details>
<summary>摘要</summary>
通常在知识基础对话生成中，会显式使用中间步骤（例如网络搜索、记忆检索）和模块化方法。然而，这些中间步骤的数据通常不可见，比对话响应的数据更难以获取。为了填充这些数据的缺失，我们提出了一种自我改进方法，以提高中间步骤的生成性能。具体来说，我们提出了一种新的启动方案，以及一种修改的损失函数，以提高自动生成的应ropriate响应的多样性。通过对各种标准数据集进行实验，我们证明了我们的方法可以成功地利用自我改进机制来生成中间和最终响应，并提高知识基础对话生成任务的性能。
</details></li>
</ul>
<hr>
<h2 id="Lo-Hi-Practical-ML-Drug-Discovery-Benchmark"><a href="#Lo-Hi-Practical-ML-Drug-Discovery-Benchmark" class="headerlink" title="Lo-Hi: Practical ML Drug Discovery Benchmark"></a>Lo-Hi: Practical ML Drug Discovery Benchmark</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06399">http://arxiv.org/abs/2310.06399</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/steshinss/lohi_neurips2023">https://github.com/steshinss/lohi_neurips2023</a></li>
<li>paper_authors: Simon Steshin</li>
<li>For: The paper aims to improve the practicality of molecular property prediction benchmarks for drug discovery by creating a new benchmark called Lo-Hi, which includes two tasks: Lead Optimization and Hit Identification.* Methods: The paper uses a novel molecular splitting algorithm to solve the Balanced Vertex Minimum $k$-Cut problem for the Hi task, and tests state-of-the-art and classic machine learning models under practical settings.* Results: The paper shows that modern benchmarks are unrealistic and overoptimistic, and that the Lo-Hi benchmark is more practical and accurate for drug discovery applications.Here’s the simplified Chinese version of the three key points:* For: 这个论文目标是改进药物发现中的分子性能预测标准 benchmark，通过创建一个名为 Lo-Hi 的新 benchmark，包括两个任务：Lead Optimization 和 Hit Identification。* Methods: 论文使用一种新的分子拆分算法解决 Hi 任务中的 Balanced Vertex Minimum $k$-Cut 问题，并测试了当前最佳和经典机器学习模型在实际设置下的表现。* Results: 论文显示现有的标准 benchmark 是不实用的和过optimistic，而 Lo-Hi benchmark 更加实用和准确地反映药物发现应用中的分子性能预测问题。<details>
<summary>Abstract</summary>
Finding new drugs is getting harder and harder. One of the hopes of drug discovery is to use machine learning models to predict molecular properties. That is why models for molecular property prediction are being developed and tested on benchmarks such as MoleculeNet. However, existing benchmarks are unrealistic and are too different from applying the models in practice. We have created a new practical \emph{Lo-Hi} benchmark consisting of two tasks: Lead Optimization (Lo) and Hit Identification (Hi), corresponding to the real drug discovery process. For the Hi task, we designed a novel molecular splitting algorithm that solves the Balanced Vertex Minimum $k$-Cut problem. We tested state-of-the-art and classic ML models, revealing which works better under practical settings. We analyzed modern benchmarks and showed that they are unrealistic and overoptimistic.   Review: https://openreview.net/forum?id=H2Yb28qGLV   Lo-Hi benchmark: https://github.com/SteshinSS/lohi_neurips2023   Lo-Hi splitter library: https://github.com/SteshinSS/lohi_splitter
</details>
<details>
<summary>摘要</summary>
现在找新药物是越来越Difficult。一种希望的药物发现是使用机器学习模型预测分子性质。因此，模型 для分子性质预测在MoleculeNet等准 benchmark上进行了开发和测试。然而，现有的benchmark是不realistic的，与实际应用场景有很大差异。我们创建了一个新的实用 Lo-Hi benchmark，包括两个任务：Lead Optimization（Lo）和 Hit Identification（Hi），对应实际药物发现过程。对于Hi任务，我们设计了一种新的分子拆分算法，解决了Balanced Vertex Minimum $k$-Cut问题。我们测试了当今最佳和经典的机器学习模型，发现哪些在实际设置下表现更好。我们分析了现代benchmark，发现它们是不realistic和过optimistic。参考：https://openreview.net/forum?id=H2Yb28qGLVLo-Hi benchmark：https://github.com/SteshinSS/lohi_neurips2023Lo-Hi splitter library：https://github.com/SteshinSS/lohi_splitter
</details></li>
</ul>
<hr>
<h2 id="P5-Plug-and-Play-Persona-Prompting-for-Personalized-Response-Selection"><a href="#P5-Plug-and-Play-Persona-Prompting-for-Personalized-Response-Selection" class="headerlink" title="P5: Plug-and-Play Persona Prompting for Personalized Response Selection"></a>P5: Plug-and-Play Persona Prompting for Personalized Response Selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06390">http://arxiv.org/abs/2310.06390</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rungjoo/plug-and-play-prompt-persona">https://github.com/rungjoo/plug-and-play-prompt-persona</a></li>
<li>paper_authors: Joosung Lee, Minsik Oh, Donghun Lee</li>
<li>for: This paper aims to address the challenges of using persona-grounded retrieval-based chatbots for personalized conversations, specifically the high cost of collecting persona-grounded corpora and the chatbot’s lack of consideration for persona in real-world applications.</li>
<li>methods: The proposed solution is a plug-and-play persona prompting method that allows the chatbot system to function as a standard open-domain chatbot when persona information is not available. The method uses a zero-shot setting to reduce the dependence on persona-grounded training data, and the model can be fine-tuned for even better performance.</li>
<li>results: The authors demonstrate that their approach improves the standard model by 7.71 and 1.04 points in the original persona and revised persona, respectively, and fine-tuning the model further improves the previous state-of-the-art system by 1.95 and 3.39 points in the original persona and revised persona, respectively. This is the first attempt to solve the problem of personalized response selection using prompt sequences.Here’s the information in Simplified Chinese text:</li>
<li>for: 本研究旨在解决用 persona-grounded retrieval-based chatbot 进行个性化对话的挑战，具体来说是收集 persona-grounded corpus 的高成本和实际应用中 chatbot 不一定会考虑 persona。</li>
<li>methods: 提议的解决方案是一种插件式 persona 提示方法，允许 chatbot 系统在 persona 信息不available 时 функциональ如标准的 open-domain chatbot。该方法使用 zero-shot 设定，以减少基于 persona-grounded 训练数据的依赖。</li>
<li>results: 作者们示出了他们的方法可以提高标准模型的性能， Specifically, the zero-shot model improved the standard model by 7.71 and 1.04 points in the original persona and revised persona, respectively, and fine-tuning the model further improved the previous state-of-the-art system by 1.95 and 3.39 points in the original persona and revised persona, respectively.<details>
<summary>Abstract</summary>
The use of persona-grounded retrieval-based chatbots is crucial for personalized conversations, but there are several challenges that need to be addressed. 1) In general, collecting persona-grounded corpus is very expensive. 2) The chatbot system does not always respond in consideration of persona at real applications. To address these challenges, we propose a plug-and-play persona prompting method. Our system can function as a standard open-domain chatbot if persona information is not available. We demonstrate that this approach performs well in the zero-shot setting, which reduces the dependence on persona-ground training data. This makes it easier to expand the system to other languages without the need to build a persona-grounded corpus. Additionally, our model can be fine-tuned for even better performance. In our experiments, the zero-shot model improved the standard model by 7.71 and 1.04 points in the original persona and revised persona, respectively. The fine-tuned model improved the previous state-of-the-art system by 1.95 and 3.39 points in the original persona and revised persona, respectively. To the best of our knowledge, this is the first attempt to solve the problem of personalized response selection using prompt sequences. Our code is available on github~\footnote{https://github.com/rungjoo/plug-and-play-prompt-persona}.
</details>
<details>
<summary>摘要</summary>
使用基于搜索的人物固定的聊天机器人是至关重要的 для个性化对话，但有几个挑战需要解决。1）总体而言，收集基于人物的训练数据非常昂贵。2）聊天系统在实际应用中不一定会考虑到人物。为了解决这些挑战，我们提出了一种插件式人物提示方法。我们的系统可以作为标准的开放领域聊天机器人运行，如果人物信息不available。我们的实验表明，这种方法在零shot设定下表现良好，减少了基于人物固定训练数据的依赖。这使得我们可以更容易地扩展系统到其他语言，无需建立基于人物的训练数据。此外，我们的模型可以进行细化调整，以进一步提高表现。在我们的实验中，零shot模型在标准模型的基础上提高了7.71和1.04分，而修改后的模型则在原始人物和修改后的人物上提高了1.95和3.39分。到目前为止，这是个性化响应选择使用提示序列的第一次尝试。我们的代码可以在github上找到（https://github.com/rungjoo/plug-and-play-prompt-persona）。
</details></li>
</ul>
<hr>
<h2 id="Jailbreak-and-Guard-Aligned-Language-Models-with-Only-Few-In-Context-Demonstrations"><a href="#Jailbreak-and-Guard-Aligned-Language-Models-with-Only-Few-In-Context-Demonstrations" class="headerlink" title="Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations"></a>Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06387">http://arxiv.org/abs/2310.06387</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zeming Wei, Yifei Wang, Yisen Wang</li>
<li>for: 本研究探讨了语言模型培养（In-Context Learning，ICL）在语言模型的安全性和可能性中的作用。</li>
<li>methods: 本研究使用了几个示例来探讨语言模型的启发能力，并提出了一种基于ICL的攻击方法和防御方法，即启发攻击（In-Context Attack，ICA）和启发防御（In-Context Defense，ICD）。</li>
<li>results: 实验结果表明，ICA和ICD可以增加或减少针对语言模型的恶意攻击的成功率。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have shown remarkable success in various tasks, but concerns about their safety and the potential for generating malicious content have emerged. In this paper, we explore the power of In-Context Learning (ICL) in manipulating the alignment ability of LLMs. We find that by providing just few in-context demonstrations without fine-tuning, LLMs can be manipulated to increase or decrease the probability of jailbreaking, i.e. answering malicious prompts. Based on these observations, we propose In-Context Attack (ICA) and In-Context Defense (ICD) methods for jailbreaking and guarding aligned language model purposes. ICA crafts malicious contexts to guide models in generating harmful outputs, while ICD enhances model robustness by demonstrations of rejecting to answer harmful prompts. Our experiments show the effectiveness of ICA and ICD in increasing or reducing the success rate of adversarial jailbreaking attacks. Overall, we shed light on the potential of ICL to influence LLM behavior and provide a new perspective for enhancing the safety and alignment of LLMs.
</details>
<details>
<summary>摘要</summary>
Note: The text has been translated into Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore.Please note that the translation is done by a machine and may not be perfect, and some cultural references or idioms may not be accurately translated.
</details></li>
</ul>
<hr>
<h2 id="What-Makes-for-Robust-Multi-Modal-Models-in-the-Face-of-Missing-Modalities"><a href="#What-Makes-for-Robust-Multi-Modal-Models-in-the-Face-of-Missing-Modalities" class="headerlink" title="What Makes for Robust Multi-Modal Models in the Face of Missing Modalities?"></a>What Makes for Robust Multi-Modal Models in the Face of Missing Modalities?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06383">http://arxiv.org/abs/2310.06383</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siting Li, Chenzhuang Du, Yue Zhao, Yu Huang, Hang Zhao</li>
<li>for: 这个论文的目的是提高多Modal学习模型在面临缺失modalities时的Robustness。</li>
<li>methods: 该论文使用了信息学的视角来模型多Modal模型在缺失modalities时的场景，并提出了一种基于Uni-Modal Ensemble with Missing Modality Adaptation（UME-MMA）的方法来解决这个问题。UME-MMA使用了预训练的uni-Modal网络 weights来提高特征提取，并使用缺失modalities数据增强技术来更好地适应缺失modalities的情况。</li>
<li>results: 该论文在Audio-Visual dataset（如AV-MNIST、Kinetics-Sound、AVE）和视觉语言dataset（如MM-IMDB、UPMC Food101）中展示了UME-MMA的效果。<details>
<summary>Abstract</summary>
With the growing success of multi-modal learning, research on the robustness of multi-modal models, especially when facing situations with missing modalities, is receiving increased attention. Nevertheless, previous studies in this domain exhibit certain limitations, as they often lack theoretical insights or their methodologies are tied to specific network architectures or modalities. We model the scenarios of multi-modal models encountering missing modalities from an information-theoretic perspective and illustrate that the performance ceiling in such scenarios can be approached by efficiently utilizing the information inherent in non-missing modalities. In practice, there are two key aspects: (1) The encoder should be able to extract sufficiently good features from the non-missing modality; (2) The extracted features should be robust enough not to be influenced by noise during the fusion process across modalities. To this end, we introduce Uni-Modal Ensemble with Missing Modality Adaptation (UME-MMA). UME-MMA employs uni-modal pre-trained weights for the multi-modal model to enhance feature extraction and utilizes missing modality data augmentation techniques to better adapt to situations with missing modalities. Apart from that, UME-MMA, built on a late-fusion learning framework, allows for the plug-and-play use of various encoders, making it suitable for a wide range of modalities and enabling seamless integration of large-scale pre-trained encoders to further enhance performance. And we demonstrate UME-MMA's effectiveness in audio-visual datasets~(e.g., AV-MNIST, Kinetics-Sound, AVE) and vision-language datasets~(e.g., MM-IMDB, UPMC Food101).
</details>
<details>
<summary>摘要</summary>
随着多Modal学习的成功增长，对多Modal模型在缺失模式下的Robustness研究得到了更多的关注。然而，先前的研究在这个领域存在一些限制，因为它们经常缺乏理论性的深度或者其方法论是特定的网络架构或模式所限制的。我们从信息论角度模拟了多Modal模型在缺失模式下的场景，并证明了在这些场景下性能的上限可以通过高效地利用缺失模式下的信息来逼近。在实践中，有两个关键方面：（1）encoder应该能够从非缺失模式中提取足够好的特征;（2）提取的特征应该能够在模式之间的混合过程中免疫噪音的影响。为此，我们提出了Uni-Modal Ensemble with Missing Modality Adaptation（UME-MMA）。UME-MMA使用uni-modal预训练权重来提高多Modal模型的特征提取，并使用缺失模式数据增强技术来更好地适应缺失模式。此外，UME-MMA基于晚期融合学习框架，允许插入多种编码器，使其适用于多种模式并允许大规模预训练编码器进一步提高性能。我们在AV-MNIST、Kinetics-Sound、AVE等音视频数据集和MM-IMDB、UPMC Food101等视语数据集中证明了UME-MMA的有效性。
</details></li>
</ul>
<hr>
<h2 id="Advanced-Efficient-Strategy-for-Detection-of-Dark-Objects-Based-on-Spiking-Network-with-Multi-Box-Detection"><a href="#Advanced-Efficient-Strategy-for-Detection-of-Dark-Objects-Based-on-Spiking-Network-with-Multi-Box-Detection" class="headerlink" title="Advanced Efficient Strategy for Detection of Dark Objects Based on Spiking Network with Multi-Box Detection"></a>Advanced Efficient Strategy for Detection of Dark Objects Based on Spiking Network with Multi-Box Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06370">http://arxiv.org/abs/2310.06370</a></li>
<li>repo_url: None</li>
<li>paper_authors: Munawar Ali, Baoqun Yin, Hazrat Bilal, Aakash Kumar, Ali Muhammad, Avinash Rohra</li>
<li>for: 提高对黑色对象的检测精度和速度</li>
<li>methods:  combining spiked and normal convolution layers, pre-trained VGG16 feature extractor</li>
<li>results: 66.01%和41.25% mAP for detecting 20 different objects in the VOC-12 and 12 objects in the Ex-Dark dataset, superior performance compared to other state-of-the-art object detection models<details>
<summary>Abstract</summary>
Several deep learning algorithms have shown amazing performance for existing object detection tasks, but recognizing darker objects is the largest challenge. Moreover, those techniques struggled to detect or had a slow recognition rate, resulting in significant performance losses. As a result, an improved and accurate detection approach is required to address the above difficulty. The whole study proposes a combination of spiked and normal convolution layers as an energy-efficient and reliable object detector model. The proposed model is split into two sections. The first section is developed as a feature extractor, which utilizes pre-trained VGG16, and the second section of the proposal structure is the combination of spiked and normal Convolutional layers to detect the bounding boxes of images. We drew a pre-trained model for classifying detected objects. With state of the art Python libraries, spike layers can be trained efficiently. The proposed spike convolutional object detector (SCOD) has been evaluated on VOC and Ex-Dark datasets. SCOD reached 66.01% and 41.25% mAP for detecting 20 different objects in the VOC-12 and 12 objects in the Ex-Dark dataset. SCOD uses 14 Giga FLOPS for its forward path calculations. Experimental results indicated superior performance compared to Tiny YOLO, Spike YOLO, YOLO-LITE, Tinier YOLO and Center of loc+Xception based on mAP for the VOC dataset.
</details>
<details>
<summary>摘要</summary>
多种深度学习算法在现有的对象检测任务上表现出色，但检测黑色对象是最大的挑战。此外，这些技术在检测或识别速度较慢，导致性能下降。因此，需要一种改进的和准确的检测方法来解决上述困难。本研究提议一种结合刺激和常规卷积层的能量高效和可靠对象检测模型。该模型分为两部分。第一部分是特征提取器，利用预训练的VGG16，第二部分是将刺激和常规卷积层结合在一起，用于检测图像中的 bounding box。我们预训练了一个用于类别检测的模型。使用现代Python库，刺激层可以高效地训练。我们提出的刺激卷积对象检测器（SCOD）在VOC和Ex-Dark数据集上进行评估，SCOD在VOC-12和Ex-Dark数据集中分别达到了66.01%和41.25%的mAP。SCOD的前向计算需要14亿FLOPS。实验结果表明，SCOD在VOC数据集上比Tiny YOLO、Spike YOLO、YOLO-LITE、Tinier YOLO和Center of loc+Xception基于mAP的性能更高。
</details></li>
</ul>
<hr>
<h2 id="Geometrically-Aligned-Transfer-Encoder-for-Inductive-Transfer-in-Regression-Tasks"><a href="#Geometrically-Aligned-Transfer-Encoder-for-Inductive-Transfer-in-Regression-Tasks" class="headerlink" title="Geometrically Aligned Transfer Encoder for Inductive Transfer in Regression Tasks"></a>Geometrically Aligned Transfer Encoder for Inductive Transfer in Regression Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06369">http://arxiv.org/abs/2310.06369</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sung Moon Ko, Sumin Lee, Dae-Woong Jeong, Woohyung Lim, Sehui Han</li>
<li>for: 扩展转移学习方法到回归任务，提出了一种基于差异 geomtry 的转移技术，即 Geometrically Aligned Transfer Encoder (GATE)。</li>
<li>methods: 在这种方法中，将模型的隐藏向量解释为在里曼射影抽象的拟合 manifold 上的点集。通过找到对应的抽象函数，确保每个任务的精确点在重叠区域中都可以到达一个本地平坦坐标，从而实现知识的传递。此外，这也serve as一种有效的正则化方法，使模型在极限区域行为稳定。</li>
<li>results: 在多种分子图数据集上，GATE 比 convential 方法表现出色，在隐藏空间和极限区域都显示稳定的行为。<details>
<summary>Abstract</summary>
Transfer learning is a crucial technique for handling a small amount of data that is potentially related to other abundant data. However, most of the existing methods are focused on classification tasks using images and language datasets. Therefore, in order to expand the transfer learning scheme to regression tasks, we propose a novel transfer technique based on differential geometry, namely the Geometrically Aligned Transfer Encoder (GATE). In this method, we interpret the latent vectors from the model to exist on a Riemannian curved manifold. We find a proper diffeomorphism between pairs of tasks to ensure that every arbitrary point maps to a locally flat coordinate in the overlapping region, allowing the transfer of knowledge from the source to the target data. This also serves as an effective regularizer for the model to behave in extrapolation regions. In this article, we demonstrate that GATE outperforms conventional methods and exhibits stable behavior in both the latent space and extrapolation regions for various molecular graph datasets.
</details>
<details>
<summary>摘要</summary>
<<SYS> translate into Simplified Chinese</SYS>抽象：传承学是一种重要的技术，用于处理小量数据，但这些数据可能与其他丰富的数据相关。然而，现有的方法主要是用于图像和语言数据的分类任务。因此，我们提出了一种新的传承技术基于差分几何，即差分几何转移编码器（GATE）。在这种方法中，我们将模型中的隐藏 вектор视为在圆柱几何上的点。我们找到了对应的对称变换，使得每个任务的任意点在重叠区域中都可以映射到一个本地平坦坐标，从而实现了从源数据传播知识到目标数据。此外，这也 serves as a 有效的正则化项，使模型在极限区域行为稳定。在这篇文章中，我们示示了GATE比传统方法更高效，并在不同的分子图数据集上显示了稳定的行为。
</details></li>
</ul>
<hr>
<h2 id="Noisy-ArcMix-Additive-Noisy-Angular-Margin-Loss-Combined-With-Mixup-Anomalous-Sound-Detection"><a href="#Noisy-ArcMix-Additive-Noisy-Angular-Margin-Loss-Combined-With-Mixup-Anomalous-Sound-Detection" class="headerlink" title="Noisy-ArcMix: Additive Noisy Angular Margin Loss Combined With Mixup Anomalous Sound Detection"></a>Noisy-ArcMix: Additive Noisy Angular Margin Loss Combined With Mixup Anomalous Sound Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06364">http://arxiv.org/abs/2310.06364</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/soonhyeon/Noisy-ArcMix">https://github.com/soonhyeon/Noisy-ArcMix</a></li>
<li>paper_authors: Soonhyeon Choi, Jung-Woo Choi</li>
<li>for: 这篇论文targetsUnsupervised anomalous sound detection (ASD), aiming to identify abnormal sounds by learning normal operational sounds’ features and sensing their deviations.</li>
<li>methods: 本研究使用了自类指导任务，利用normal data的类别 tasks to learn representation space for anomalous data, and proposes a training technique to ensure intra-class compactness and increase angle gap between normal and abnormal samples.</li>
<li>results: 实验结果显示，提案的方法在DCASE 2020 Challenge Task2 dataset上实现了最佳性能，与state-of-the-art方法相比，具有0.90%, 0.83%, 2.16%的改善（AUC、pAUC、mAUC分别）。<details>
<summary>Abstract</summary>
Unsupervised anomalous sound detection (ASD) aims to identify anomalous sounds by learning the features of normal operational sounds and sensing their deviations. Recent approaches have focused on the self-supervised task utilizing the classification of normal data, and advanced models have shown that securing representation space for anomalous data is important through representation learning yielding compact intra-class and well-separated intra-class distributions. However, we show that conventional approaches often fail to ensure sufficient intra-class compactness and exhibit angular disparity between samples and their corresponding centers. In this paper, we propose a training technique aimed at ensuring intra-class compactness and increasing the angle gap between normal and abnormal samples. Furthermore, we present an architecture that extracts features for important temporal regions, enabling the model to learn which time frames should be emphasized or suppressed. Experimental results demonstrate that the proposed method achieves the best performance giving 0.90%, 0.83%, and 2.16% improvement in terms of AUC, pAUC, and mAUC, respectively, compared to the state-of-the-art method on DCASE 2020 Challenge Task2 dataset.
</details>
<details>
<summary>摘要</summary>
无监督异常声音检测（ASD）目标是通过学习正常操作声音特征来识别异常声音。 current approaches 通常采用自我监督任务，利用正常数据的分类来学习表征。然而，我们发现，现有方法通常无法保证异常样本内的准确性和相关性。在这篇论文中，我们提出了一种培训技术，以确保异常样本内的准确性和相关性，同时提高正常和异常样本之间的角度差。此外，我们还提出了一种建模方法，以EXTRACT Features for important temporal regions，使模型能够学习哪些时间区域是重要的。实验结果表明，我们提出的方法可以达到最佳性能，与现有方法相比，在 DCASE 2020 Challenge Task2 数据集上提高了0.90%、0.83% 和 2.16% 的 AUC、pAUC 和 mAUC 分别。
</details></li>
</ul>
<hr>
<h2 id="Fire-Detection-From-Image-and-Video-Using-YOLOv5"><a href="#Fire-Detection-From-Image-and-Video-Using-YOLOv5" class="headerlink" title="Fire Detection From Image and Video Using YOLOv5"></a>Fire Detection From Image and Video Using YOLOv5</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06351">http://arxiv.org/abs/2310.06351</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arafat Islam, Md. Imtiaz Habib<br>for: 火灾探测，特别是室外、森林火灾中的小型火焰探测methods: 提出了一个改进的YOLOv5火灾探测深度学习算法，包括增强特征提取网络的扩展和特征堆峰推广等技术results: 该算法可以实现小型火焰探测的高精度探测，其中mAP达90.5%，f1分数达88%，并且可以实现实时森林火灾探测，平均探测时间为0.12秒&#x2F;帧。<details>
<summary>Abstract</summary>
For the detection of fire-like targets in indoor, outdoor and forest fire images, as well as fire detection under different natural lights, an improved YOLOv5 fire detection deep learning algorithm is proposed. The YOLOv5 detection model expands the feature extraction network from three dimensions, which enhances feature propagation of fire small targets identification, improves network performance, and reduces model parameters. Furthermore, through the promotion of the feature pyramid, the top-performing prediction box is obtained. Fire-YOLOv5 attains excellent results compared to state-of-the-art object detection networks, notably in the detection of small targets of fire and smoke with mAP 90.5% and f1 score 88%. Overall, the Fire-YOLOv5 detection model can effectively deal with the inspection of small fire targets, as well as fire-like and smoke-like objects with F1 score 0.88. When the input image size is 416 x 416 resolution, the average detection time is 0.12 s per frame, which can provide real-time forest fire detection. Moreover, the algorithm proposed in this paper can also be applied to small target detection under other complicated situations. The proposed system shows an improved approach in all fire detection metrics such as precision, recall, and mean average precision.
</details>
<details>
<summary>摘要</summary>
For the detection of fire-like targets in indoor, outdoor and forest fire images, as well as fire detection under different natural lights, an improved YOLOv5 fire detection deep learning algorithm is proposed. The YOLOv5 detection model expands the feature extraction network from three dimensions, which enhances feature propagation of fire small targets identification, improves network performance, and reduces model parameters. Furthermore, through the promotion of the feature pyramid, the top-performing prediction box is obtained. Fire-YOLOv5 attains excellent results compared to state-of-the-art object detection networks, notably in the detection of small targets of fire and smoke with mAP 90.5% and f1 score 88%. Overall, the Fire-YOLOv5 detection model can effectively deal with the inspection of small fire targets, as well as fire-like and smoke-like objects with F1 score 0.88. When the input image size is 416 x 416 resolution, the average detection time is 0.12 s per frame, which can provide real-time forest fire detection. Moreover, the algorithm proposed in this paper can also be applied to small target detection under other complicated situations. The proposed system shows an improved approach in all fire detection metrics such as precision, recall, and mean average precision.Here's the text in Simplified Chinese characters:为了检测室内、户外和森林火图像中的火目标，以及火 detection under different natural lights，我们提出了一种改进的 YOLOv5 火 detection 深度学习算法。 YOLOv5 检测模型从三维特征提取网络中扩展了特征提取网络，以增强小火目标识别的特征传播，提高网络性能，并减少模型参数。此外，通过特征层的提高，得到了最佳预测盒。 Fire-YOLOv5 在比较其他物体检测网络时表现出色，特别是在小目标火和烟的识别上，具有 mAP 90.5% 和 f1 score 88%。总的来说，Fire-YOLOv5 检测模型可以有效地处理小火目标的检测，以及火类和烟类对象的识别。当输入图像大小为 416 x 416 像素时，每帧检测时间为 0.12 秒，可以提供实时森林火检测。此外，本文提出的算法还可以应用于其他复杂情况下的小目标检测。提议的系统在所有火检测指标中表现出色，包括准确率、回归率和mean average precision。
</details></li>
</ul>
<hr>
<h2 id="Filter-Pruning-For-CNN-With-Enhanced-Linear-Representation-Redundancy"><a href="#Filter-Pruning-For-CNN-With-Enhanced-Linear-Representation-Redundancy" class="headerlink" title="Filter Pruning For CNN With Enhanced Linear Representation Redundancy"></a>Filter Pruning For CNN With Enhanced Linear Representation Redundancy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06344">http://arxiv.org/abs/2310.06344</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bojue-wang/ccm-lrr">https://github.com/bojue-wang/ccm-lrr</a></li>
<li>paper_authors: Bojue Wang, Chunmei Ma, Bin Liu, Nianbo Liu, Jinqi Zhu</li>
<li>for: 这个研究目的是提出一个新的结构化节点析删法，以便在训练过程中删除网络中的节点，以减少网络的储存空间和计算量。</li>
<li>methods: 这个方法使用了一个新的损失函数，叫做CCM-loss，它是根据不同层的特征地图之间的联系熵来计算的。这个损失函数可以让网络在训练过程中学习更强的线性表现关系，以便在删除节点时删除更多的同型节点。此外，这个方法还使用了一个基于主成分分析的通道选择策略，以便优化CCM-loss的使用。</li>
<li>results: 这个方法在Cifar-10 datasets上实现了93.64%的准确率，仅剩1.40M个参数和49.60M FLOPs。在ImageNet datasets上，这个方法实现了42.8%和47.3%的储存空间和计算量减少，并保持76.23%的准确率。<details>
<summary>Abstract</summary>
Structured network pruning excels non-structured methods because they can take advantage of the thriving developed parallel computing techniques. In this paper, we propose a new structured pruning method. Firstly, to create more structured redundancy, we present a data-driven loss function term calculated from the correlation coefficient matrix of different feature maps in the same layer, named CCM-loss. This loss term can encourage the neural network to learn stronger linear representation relations between feature maps during the training from the scratch so that more homogenous parts can be removed later in pruning. CCM-loss provides us with another universal transcendental mathematical tool besides L*-norm regularization, which concentrates on generating zeros, to generate more redundancy but for the different genres. Furthermore, we design a matching channel selection strategy based on principal components analysis to exploit the maximum potential ability of CCM-loss. In our new strategy, we mainly focus on the consistency and integrality of the information flow in the network. Instead of empirically hard-code the retain ratio for each layer, our channel selection strategy can dynamically adjust each layer's retain ratio according to the specific circumstance of a per-trained model to push the prune ratio to the limit. Notably, on the Cifar-10 dataset, our method brings 93.64% accuracy for pruned VGG-16 with only 1.40M parameters and 49.60M FLOPs, the pruned ratios for parameters and FLOPs are 90.6% and 84.2%, respectively. For ResNet-50 trained on the ImageNet dataset, our approach achieves 42.8% and 47.3% storage and computation reductions, respectively, with an accuracy of 76.23%. Our code is available at https://github.com/Bojue-Wang/CCM-LRR.
</details>
<details>
<summary>摘要</summary>
《结构化网络剪除超越非结构化方法，因为它们可以利用已有的发达并行计算技术。在这篇论文中，我们提出了一种新的结构化剪除方法。首先，我们提出了一种数据驱动的损失函数，即基于层内不同特征地图的相关系数矩阵（CCM-loss）。这个损失函数可以让神经网络在训练时从零开始学习更强的线性表示关系，以便在剪除后更多的同类部分可以被去除。CCM-loss为我们提供了除L*-norm正则化之外的另一种普遍适用的 трансцендент数学工具，可以产生更多的重复。此外，我们设计了基于主成分分析的通道选择策略，以便在最大化CCM-loss的情况下使用。在我们的新策略中，我们主要关注神经网络中信息流的一致性和完整性。而不是按照预先确定的方式来确定各层保留比例，我们的通道选择策略可以在每个训练过程中动态调整各层保留比例，以达到剪除比例的最大化。值得注意的是，在Cifar-10数据集上，我们的方法可以实现93.64%的准确率，只需要1.40M个参数和49.60M个操作量。剪除率为90.6%和84.2%。对于ImageNet数据集上训练的ResNet-50，我们的方法可以实现42.8%和47.3%的存储和计算剪除，准确率为76.23%。我们的代码可以在https://github.com/Bojue-Wang/CCM-LRR上获取。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Prompt-Learning-based-Code-Search-based-on-Interaction-Matrix"><a href="#Contrastive-Prompt-Learning-based-Code-Search-based-on-Interaction-Matrix" class="headerlink" title="Contrastive Prompt Learning-based Code Search based on Interaction Matrix"></a>Contrastive Prompt Learning-based Code Search based on Interaction Matrix</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06342">http://arxiv.org/abs/2310.06342</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yubo Zhang, Yanfang Liu, Xinxin Fan, Yunfeng Lu</li>
<li>for: 提高代码搜索的Semantic representation和自然语言和编程语言之间的匹配关系。</li>
<li>methods: 提出了一种基于对比提示学习的代码搜索方法，通过跨Modal交互机制提高代码和自然语言之间的细致匹配。</li>
<li>results: 通过对实际世界数据集进行广泛的实验，证明了我们的方法可以提高代码搜索的Semantic representation质量和自然语言和编程语言之间的匹配能力。<details>
<summary>Abstract</summary>
Code search aims to retrieve the code snippet that highly matches the given query described in natural language. Recently, many code pre-training approaches have demonstrated impressive performance on code search. However, existing code search methods still suffer from two performance constraints: inadequate semantic representation and the semantic gap between natural language (NL) and programming language (PL). In this paper, we propose CPLCS, a contrastive prompt learning-based code search method based on the cross-modal interaction mechanism. CPLCS comprises:(1) PL-NL contrastive learning, which learns the semantic matching relationship between PL and NL representations; (2) a prompt learning design for a dual-encoder structure that can alleviate the problem of inadequate semantic representation; (3) a cross-modal interaction mechanism to enhance the fine-grained mapping between NL and PL. We conduct extensive experiments to evaluate the effectiveness of our approach on a real-world dataset across six programming languages. The experiment results demonstrate the efficacy of our approach in improving semantic representation quality and mapping ability between PL and NL.
</details>
<details>
<summary>摘要</summary>
Code search 的目的是搜索与给定的自然语言（NL）查询语句相似的代码片段。近些年，许多代码预训练方法在代码搜索方面凭借了惊人的表现。然而，现有的代码搜索方法仍然受到两种性能约束：代码表示不够准确和自然语言（PL）和计算机语言（PL）之间的semantic gap。本文提出了一种基于对比提示学习的代码搜索方法（CPLCS），它包括：1. PL-NL对应学习，这种学习方法学习PL和NL表示之间的匹配关系;2.一种适应双encoder结构的提问学习设计，以解决代码表示不够准确的问题;3.一种跨模式交互机制，以增强NL和PL之间的细致对应。我们对实际世界数据集进行了广泛的实验，以评估我们的方法的有效性。实验结果表明，我们的方法可以提高代码表示质量和NL和PL之间的映射能力。
</details></li>
</ul>
<hr>
<h2 id="I2SRM-Intra-and-Inter-Sample-Relationship-Modeling-for-Multimodal-Information-Extraction"><a href="#I2SRM-Intra-and-Inter-Sample-Relationship-Modeling-for-Multimodal-Information-Extraction" class="headerlink" title="I2SRM: Intra- and Inter-Sample Relationship Modeling for Multimodal Information Extraction"></a>I2SRM: Intra- and Inter-Sample Relationship Modeling for Multimodal Information Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06326">http://arxiv.org/abs/2310.06326</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lumia-group/i2srm">https://github.com/lumia-group/i2srm</a></li>
<li>paper_authors: Yusheng Huang, Zhouhan Lin</li>
<li>for: 这篇论文主要用于提出一种基于多modalities的信息提取方法，即Intra-和Inter-Sample Relationship Modeling（I2SRM）方法。</li>
<li>methods: 该方法包括两个模块，第一个模块是内样关系建模模块，对单个样本进行效果学习。文本和图像模式的嵌入都做了媒体抽象，以减少不同预训练语言和图像模型之间的模式差。第二个模块是间样关系建模模块，考虑多个样本之间的交互关系，并提出了AttnMixup策略，不仅增强样本之间的协作，还可以增强数据的泛化性。</li>
<li>results: 在多modal named entity recognition数据集Twitter-2015和Twitter-2017以及多modal relation extraction数据集MNRE上，我们的提议的I2SRM方法实现了竞争力的结果，Twitter-2015上的F1分数为77.12%, Twitter-2017上的F1分数为88.40%, MNRE数据集上的F1分数为84.12%.<details>
<summary>Abstract</summary>
Multimodal information extraction is attracting research attention nowadays, which requires aggregating representations from different modalities. In this paper, we present the Intra- and Inter-Sample Relationship Modeling (I2SRM) method for this task, which contains two modules. Firstly, the intra-sample relationship modeling module operates on a single sample and aims to learn effective representations. Embeddings from textual and visual modalities are shifted to bridge the modality gap caused by distinct pre-trained language and image models. Secondly, the inter-sample relationship modeling module considers relationships among multiple samples and focuses on capturing the interactions. An AttnMixup strategy is proposed, which not only enables collaboration among samples but also augments data to improve generalization. We conduct extensive experiments on the multimodal named entity recognition datasets Twitter-2015 and Twitter-2017, and the multimodal relation extraction dataset MNRE. Our proposed method I2SRM achieves competitive results, 77.12% F1-score on Twitter-2015, 88.40% F1-score on Twitter-2017, and 84.12% F1-score on MNRE.
</details>
<details>
<summary>摘要</summary>
现在，多模态信息抽取正在引起研究者的关注，这需要将不同模式的表示合并。在这篇论文中，我们提出了内样本关系模型（I2SRM）方法，它包括两个模块。首先，内样本关系模型模块在单个样本上运行，旨在学习有效的表示。文本和视觉模式的嵌入都被Shift到bridgemodality gap caused by distinct pre-trained language and image models。其次，间样本关系模型模块考虑多个样本之间的关系，重点是捕捉交互。我们提出了AttnMixup策略，不仅可以在样本之间协作，还可以增强数据，以提高泛化性。我们在多模态命名实体识别数据集Twitter-2015和Twitter-2017以及多模态关系抽取数据集MNRE进行了广泛的实验。我们的提出的I2SRM方法在Twitter-2015上达到了77.12%的F1分数，在Twitter-2017上达到了88.40%的F1分数，并在MNRE上达到了84.12%的F1分数。
</details></li>
</ul>
<hr>
<h2 id="Predicting-Three-Types-of-Freezing-of-Gait-Events-Using-Deep-Learning-Models"><a href="#Predicting-Three-Types-of-Freezing-of-Gait-Events-Using-Deep-Learning-Models" class="headerlink" title="Predicting Three Types of Freezing of Gait Events Using Deep Learning Models"></a>Predicting Three Types of Freezing of Gait Events Using Deep Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06322">http://arxiv.org/abs/2310.06322</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wen Tao Mo, Jonathan H. Chan<br>for:这篇论文旨在预测患有parkinson病的患者会发生停止进行步行的症状（冻结步行），并且预测不同类型的冻结步行事件。methods:本研究使用了深度学习模型，包括trasformer核心架构和Bidirectional LSTM层，以及不同的特征集合，来预测不同类型的冻结步行事件。results:最佳表现的模型在训练数据中取得了0.427的分数，可以在Kaggle的冻结步行预测竞赛中排名前5名。然而，我们也发现了训练数据中的过滤现象，可能可以通过伪造标签和模型架构简化来改善。<details>
<summary>Abstract</summary>
Freezing of gait is a Parkinson's Disease symptom that episodically inflicts a patient with the inability to step or turn while walking. While medical experts have discovered various triggers and alleviating actions for freezing of gait, the underlying causes and prediction models are still being explored today. Current freezing of gait prediction models that utilize machine learning achieve high sensitivity and specificity in freezing of gait predictions based on time-series data; however, these models lack specifications on the type of freezing of gait events. We develop various deep learning models using the transformer encoder architecture plus Bidirectional LSTM layers and different feature sets to predict the three different types of freezing of gait events. The best performing model achieves a score of 0.427 on testing data, which would rank top 5 in Kaggle's Freezing of Gait prediction competition, hosted by THE MICHAEL J. FOX FOUNDATION. However, we also recognize overfitting in training data that could be potentially improved through pseudo labelling on additional data and model architecture simplification.
</details>
<details>
<summary>摘要</summary>
冻结步态是parkinson病 symptom的一种 episodic 表现，患者在步行时会受到不能前进或转弯的困难。医学专家已经发现了冻结步态的许多触发因素和缓解方法，但是下面的原因和预测模型仍在探索中。现有的冻结步态预测模型使用机器学习技术，可以在时间序列数据上获得高的敏感度和特异度，但是这些模型缺乏冻结步态事件的类型specification。我们采用了不同的深度学习模型，包括 transformer encoder 架构和bi-directional LSTM层，以及不同的特征集来预测冻结步态事件的三种不同类型。最佳表现的模型在测试数据上得分为0.427，这将在Kaggle的冻结步态预测竞赛中排名前5。然而，我们也发现了训练数据中的过拟合，可能通过pseudo标注和额外数据的使用来改进。
</details></li>
</ul>
<hr>
<h2 id="Dobby-A-Conversational-Service-Robot-Driven-by-GPT-4"><a href="#Dobby-A-Conversational-Service-Robot-Driven-by-GPT-4" class="headerlink" title="Dobby: A Conversational Service Robot Driven by GPT-4"></a>Dobby: A Conversational Service Robot Driven by GPT-4</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06303">http://arxiv.org/abs/2310.06303</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carson Stark, Bohkyung Chun, Casey Charleston, Varsha Ravi, Luis Pabon, Surya Sunkari, Tarun Mohan, Peter Stone, Justin Hart</li>
<li>for: 本研究旨在开发一个具有自然语言理解和智能决策能力的机器人平台，通过将语言模型 embedding 到机器人系统中来实现人类与机器人之间的自然交流和决策。</li>
<li>methods: 本研究使用了一种基于大量通用知识的语言模型，通过对这些知识进行学习来让机器人能够理解和回答用户的问题。此外，该研究还使用了机器人的控制命令来实现人机对话和物理世界的交互。</li>
<li>results: 研究结果显示，在一个自由游导游 scenarios中，具有对话AI能力的机器人比无此能力的机器人表现出更高的总效果、探索能力、审查能力、人格化接受度和适应性。<details>
<summary>Abstract</summary>
This work introduces a robotics platform which embeds a conversational AI agent in an embodied system for natural language understanding and intelligent decision-making for service tasks; integrating task planning and human-like conversation. The agent is derived from a large language model, which has learned from a vast corpus of general knowledge. In addition to generating dialogue, this agent can interface with the physical world by invoking commands on the robot; seamlessly merging communication and behavior. This system is demonstrated in a free-form tour-guide scenario, in an HRI study combining robots with and without conversational AI capabilities. Performance is measured along five dimensions: overall effectiveness, exploration abilities, scrutinization abilities, receptiveness to personification, and adaptability.
</details>
<details>
<summary>摘要</summary>
这项工作描述了一个机器人平台，其内置了一个基于大语言模型的对话智能代理人，用于自然语言理解和智能决策 для服务任务。该代理人可以通过 invoke 命令来与物理世界交互，从而将通信和行为融为一体。该系统在一个自由游览导览场景中进行了人机交互研究，并对无对话智能代理人和具有对话智能代理人的机器人进行了比较。研究指标包括总效果、探索能力、审查能力、人格化响应性和适应性。
</details></li>
</ul>
<hr>
<h2 id="Dynamical-versus-Bayesian-Phase-Transitions-in-a-Toy-Model-of-Superposition"><a href="#Dynamical-versus-Bayesian-Phase-Transitions-in-a-Toy-Model-of-Superposition" class="headerlink" title="Dynamical versus Bayesian Phase Transitions in a Toy Model of Superposition"></a>Dynamical versus Bayesian Phase Transitions in a Toy Model of Superposition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06301">http://arxiv.org/abs/2310.06301</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhongtian Chen, Edmund Lau, Jake Mendel, Susan Wei, Daniel Murfet</li>
<li>for: 这 paper 的目的是 investigate phase transitions in a Toy Model of Superposition (TMS) using Singular Learning Theory (SLT).</li>
<li>methods: 这 paper 使用 closed formula  derive  theoretical loss, 并在两个隐藏维度情况下发现 regular $k$-gons 是 critical points.</li>
<li>results: 这 paper 提供 supporting theory 表明这些 $k$-gons 的 local learning coefficient (a geometric invariant) determines phase transitions in the Bayesian posterior as a function of training sample size. Empirical results show that the same $k$-gon critical points also determine the behavior of SGD training.<details>
<summary>Abstract</summary>
We investigate phase transitions in a Toy Model of Superposition (TMS) using Singular Learning Theory (SLT). We derive a closed formula for the theoretical loss and, in the case of two hidden dimensions, discover that regular $k$-gons are critical points. We present supporting theory indicating that the local learning coefficient (a geometric invariant) of these $k$-gons determines phase transitions in the Bayesian posterior as a function of training sample size. We then show empirically that the same $k$-gon critical points also determine the behavior of SGD training. The picture that emerges adds evidence to the conjecture that the SGD learning trajectory is subject to a sequential learning mechanism. Specifically, we find that the learning process in TMS, be it through SGD or Bayesian learning, can be characterized by a journey through parameter space from regions of high loss and low complexity to regions of low loss and high complexity.
</details>
<details>
<summary>摘要</summary>
我们研究托 modelo de superposición (TMS) 中的相对transition using Singular Learning Theory (SLT).我们 derivated a closed formula for the theoretical loss, and in the case of two hidden dimensions, we found that regular $k$-gons are critical points. We presented supporting theory indicating that the local learning coefficient (a geometric invariant) of these $k$-gons determines phase transitions in the Bayesian posterior as a function of training sample size. We then showed empirically that the same $k$-gon critical points also determine the behavior of SGD training. The picture that emerges adds evidence to the conjecture that the SGD learning trajectory is subject to a sequential learning mechanism. Specifically, we found that the learning process in TMS, whether through SGD or Bayesian learning, can be characterized by a journey through parameter space from regions of high loss and low complexity to regions of low loss and high complexity.Here's the translation in Traditional Chinese:我们研究托模型超position (TMS) 中的相对转换使用Singular Learning Theory (SLT).我们 derivated a closed formula for the theoretical loss, 并在两个隐藏维度的情况下发现了 Regular $k$-gons 是托点。我们提供了支持理论，认为这些 $k$-gons 的本地学习系数（一个几何 invariant） determines phase transitions in the Bayesian posterior as a function of training sample size。我们随后证明了这些 $k$-gon 托点也determine SGD 训练的行为。图像 emerges 证明了 SGD 学习轨迹是受到顺序学习机制的影响。具体来说，我们发现 TMS 的学习过程， whether through SGD 或 Bayesian 学习，可以通过从高损失且低复杂性的区域到低损失且高复杂性的区域的 parameter space 中的旅程来描述。
</details></li>
</ul>
<hr>
<h2 id="Suppressing-Overestimation-in-Q-Learning-through-Adversarial-Behaviors"><a href="#Suppressing-Overestimation-in-Q-Learning-through-Adversarial-Behaviors" class="headerlink" title="Suppressing Overestimation in Q-Learning through Adversarial Behaviors"></a>Suppressing Overestimation in Q-Learning through Adversarial Behaviors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06286">http://arxiv.org/abs/2310.06286</a></li>
<li>repo_url: None</li>
<li>paper_authors: HyeAnn Lee, Donghwan Lee</li>
<li>for: 提出一种新的Q学习算法，即假对手Q学习（DAQ），可以有效地调节标准Q学习中的过估偏好。</li>
<li>methods: 使用假对手的方式将学习转换为一个两个玩家的零和游戏，并将多种Q学习变体集成到一个框架中，以控制过估偏好。</li>
<li>results: 对多种环境进行实验表明，提议的DAQ可以有效地降低过估偏好，并且可以轻松地应用于现有的感知学习算法中，以提高性能。<details>
<summary>Abstract</summary>
The goal of this paper is to propose a new Q-learning algorithm with a dummy adversarial player, which is called dummy adversarial Q-learning (DAQ), that can effectively regulate the overestimation bias in standard Q-learning. With the dummy player, the learning can be formulated as a two-player zero-sum game. The proposed DAQ unifies several Q-learning variations to control overestimation biases, such as maxmin Q-learning and minmax Q-learning (proposed in this paper) in a single framework. The proposed DAQ is a simple but effective way to suppress the overestimation bias thourgh dummy adversarial behaviors and can be easily applied to off-the-shelf reinforcement learning algorithms to improve the performances. A finite-time convergence of DAQ is analyzed from an integrated perspective by adapting an adversarial Q-learning. The performance of the suggested DAQ is empirically demonstrated under various benchmark environments.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "The goal of this paper is to propose a new Q-learning algorithm with a dummy adversarial player, which is called dummy adversarial Q-learning (DAQ), that can effectively regulate the overestimation bias in standard Q-learning. With the dummy player, the learning can be formulated as a two-player zero-sum game. The proposed DAQ unifies several Q-learning variations to control overestimation biases, such as maxmin Q-learning and minmax Q-learning (proposed in this paper) in a single framework. The proposed DAQ is a simple but effective way to suppress the overestimation bias though dummy adversarial behaviors and can be easily applied to off-the-shelf reinforcement learning algorithms to improve the performances. A finite-time convergence of DAQ is analyzed from an integrated perspective by adapting an adversarial Q-learning. The performance of the suggested DAQ is empirically demonstrated under various benchmark environments." into Simplified Chinese.中文简体版：本文的目标是提出一种新的Q学习算法，即幻数对手Q学习（DAQ），可以有效地控制标准Q学习中的过估偏见。通过幻数player，学习可以转化为两个玩家的零SUM游戏。提出的DAQ整合了多种Q学习变体来控制过估偏见，如maxmin Q学习和minmax Q学习（在本文中提出）。DAQ是一种简单 yet有效的方法，通过幻数对手行为来抑制过估偏见，并可以轻松应用于市场上的Q学习算法来提高性能。我们从一个整合的视角来分析DAQ的固定时间收敛性。本文的建议DAQ的性能在多个标准环境中进行了实验证明。
</details></li>
</ul>
<hr>
<h2 id="BC4LLM-Trusted-Artificial-Intelligence-When-Blockchain-Meets-Large-Language-Models"><a href="#BC4LLM-Trusted-Artificial-Intelligence-When-Blockchain-Meets-Large-Language-Models" class="headerlink" title="BC4LLM: Trusted Artificial Intelligence When Blockchain Meets Large Language Models"></a>BC4LLM: Trusted Artificial Intelligence When Blockchain Meets Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06278">http://arxiv.org/abs/2310.06278</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoxiang Luo, Jian Luo, Athanasios V. Vasilakos</li>
<li>for: 本文主要是提出一种基于区块链技术的语言模型 empowering 方案，以确保人工智能（AI）学习数据的 authenticity 和可靠性。</li>
<li>methods: 本文使用了区块链技术来解决 AI 学习数据的 authenticity 和可靠性问题，包括可靠学习团队、安全训练过程和可识别生成内容。</li>
<li>results: 本文预计通过基于区块链技术的 empowering 方案，可以实现人工智能的可靠性和安全性，并且在前沿通信网络领域可能带来很多应用和挑战。<details>
<summary>Abstract</summary>
In recent years, artificial intelligence (AI) and machine learning (ML) are reshaping society's production methods and productivity, and also changing the paradigm of scientific research. Among them, the AI language model represented by ChatGPT has made great progress. Such large language models (LLMs) serve people in the form of AI-generated content (AIGC) and are widely used in consulting, healthcare, and education. However, it is difficult to guarantee the authenticity and reliability of AIGC learning data. In addition, there are also hidden dangers of privacy disclosure in distributed AI training. Moreover, the content generated by LLMs is difficult to identify and trace, and it is difficult to cross-platform mutual recognition. The above information security issues in the coming era of AI powered by LLMs will be infinitely amplified and affect everyone's life. Therefore, we consider empowering LLMs using blockchain technology with superior security features to propose a vision for trusted AI. This paper mainly introduces the motivation and technical route of blockchain for LLM (BC4LLM), including reliable learning corpus, secure training process, and identifiable generated content. Meanwhile, this paper also reviews the potential applications and future challenges, especially in the frontier communication networks field, including network resource allocation, dynamic spectrum sharing, and semantic communication. Based on the above work combined and the prospect of blockchain and LLMs, it is expected to help the early realization of trusted AI and provide guidance for the academic community.
</details>
<details>
<summary>摘要</summary>
This paper mainly introduces the motivation and technical route of blockchain for LLM (BC4LLM), including reliable learning corpus, secure training process, and identifiable generated content. Meanwhile, this paper also reviews the potential applications and future challenges, especially in the frontier communication networks field, including network resource allocation, dynamic spectrum sharing, and semantic communication. Based on the above work combined and the prospect of blockchain and LLMs, it is expected to help the early realization of trusted AI and provide guidance for the academic community.Here is the translation in Simplified Chinese:近年来，人工智能（AI）和机器学习（ML）对社会生产方式和生产效率产生了深见影响，同时也改变了科学研究的 paradigma。其中，AI语言模型代表的ChatGPT等大语言模型（LLMs）已经做出了很大的进步。这些LLMs服务于人类在形式上为AI生成内容（AIGC），广泛应用于咨询、医疗和教育等领域。然而，保证AIGC学习数据的authenticity和可靠性具有挑战。此外，分布式AI培训中也存在隐藏的隐私泄露风险。此外，由LLMs生成的内容很难以识别和跟踪，同时也难以在不同平台之间进行跨平台认可。这些在未来的AI驱动by LLMs中的信息安全问题将无限扩大，影响每个人的生活。因此，我们认为使用区块链技术来加强LLMs，以提出一种可靠的AI视野。本文主要介绍了加强LLMs使用区块链技术的动机和技术路径，包括可靠的学习ikorpus，安全的培训过程和可识别的生成内容。同时，本文还进行了前ier communication networksfield中的潜在应用和未来挑战的评估，包括网络资源分配、动态频率共享和semantic communication。基于上述工作的结合以及区块链和LLMs的前景，我们期望通过提出可靠的AI来帮助早期实现可靠的AI，并为学术界提供指导。
</details></li>
</ul>
<hr>
<h2 id="Let-Models-Speak-Ciphers-Multiagent-Debate-through-Embeddings"><a href="#Let-Models-Speak-Ciphers-Multiagent-Debate-through-Embeddings" class="headerlink" title="Let Models Speak Ciphers: Multiagent Debate through Embeddings"></a>Let Models Speak Ciphers: Multiagent Debate through Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06272">http://arxiv.org/abs/2310.06272</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chau Pham, Boyi Liu, Yingxiang Yang, Zhengyu Chen, Tianyi Liu, Jianbo Yuan, Bryan A. Plummer, Zhaoran Wang, Hongxia Yang</li>
<li>for: 提高大语言模型（LLM）的理解能力</li>
<li>methods: 去掉 LLM 中的 токен抽样步骤，通过 Raw Transformer 输出嵌入表示模型的信念</li>
<li>results: 在五种理解任务和多个开源 LLM 中，CIPHER 较 traditional inference 提高了1-3.5%，表明嵌入作为 LLM 之间communication的 alternatinative “语言” 的优势和稳定性。<details>
<summary>Abstract</summary>
Discussion and debate among Large Language Models (LLMs) have gained considerable attention due to their potential to enhance the reasoning ability of LLMs. Although natural language is an obvious choice for communication due to LLM's language understanding capability, the token sampling step needed when generating natural language poses a potential risk of information loss, as it uses only one token to represent the model's belief across the entire vocabulary. In this paper, we introduce a communication regime named CIPHER (Communicative Inter-Model Protocol Through Embedding Representation) to address this issue. Specifically, we remove the token sampling step from LLMs and let them communicate their beliefs across the vocabulary through the expectation of the raw transformer output embeddings. Remarkably, by deviating from natural language, CIPHER offers an advantage of encoding a broader spectrum of information without any modification to the model weights. While the state-of-the-art LLM debate methods using natural language outperforms traditional inference by a margin of 1.5-8%, our experiment results show that CIPHER debate further extends this lead by 1-3.5% across five reasoning tasks and multiple open-source LLMs of varying sizes. This showcases the superiority and robustness of embeddings as an alternative "language" for communication among LLMs.
</details>
<details>
<summary>摘要</summary>
Large Language Models (LLMs) 之间的讨论和辩论已经吸引了广泛的关注，因为它们可以提高 LLMs 的理解能力。 although natural language 是一个自然的选择，因为 LLMS 拥有语言理解能力，但是token sampling 步骤在生成自然语言时存在一定的风险，因为它只使用一个token来表示模型对整个词汇的信念。 在这篇论文中，我们提出了一种通信协议名为CIPHER（Communicative Inter-Model Protocol Through Embedding Representation），以解决这个问题。 Specifically，我们从 LLMS 中除了token sampling步骤，让它们通过 Raw Transformer 输出嵌入来交换信念。 这种方法的优点在于，它可以编码更广泛的信息，无需修改模型参数。 在使用自然语言进行辩论方法的现有状态的LLMs中，我们的实验结果表明，CIPHER辩论可以进一步提高这个领先的势头，在五种理解任务和多个开源 LLMS 中，平均提高1-3.5%。 这表明嵌入可以作为 LLMS 之间的另一种通信"语言"的一个有利的和稳定的选择。
</details></li>
</ul>
<hr>
<h2 id="Towards-Mitigating-Hallucination-in-Large-Language-Models-via-Self-Reflection"><a href="#Towards-Mitigating-Hallucination-in-Large-Language-Models-via-Self-Reflection" class="headerlink" title="Towards Mitigating Hallucination in Large Language Models via Self-Reflection"></a>Towards Mitigating Hallucination in Large Language Models via Self-Reflection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06271">http://arxiv.org/abs/2310.06271</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, Pascale Fung</li>
<li>for: This paper focuses on the issue of hallucination in medical generative question-answering systems, and proposes an interactive self-reflection methodology to improve the factuality and consistency of the generated answers.</li>
<li>methods: The paper uses widely adopted large language models (LLMs) and datasets, and employs an interactive self-reflection methodology that incorporates knowledge acquisition and answer generation to tackle the challenge of hallucination.</li>
<li>results: The experimental results show that the proposed approach outperforms baselines in reducing hallucination, and produces more accurate and consistent answers.<details>
<summary>Abstract</summary>
Large language models (LLMs) have shown promise for generative and knowledge-intensive tasks including question-answering (QA) tasks. However, the practical deployment still faces challenges, notably the issue of "hallucination", where models generate plausible-sounding but unfaithful or nonsensical information. This issue becomes particularly critical in the medical domain due to the uncommon professional concepts and potential social risks involved. This paper analyses the phenomenon of hallucination in medical generative QA systems using widely adopted LLMs and datasets. Our investigation centers on the identification and comprehension of common problematic answers, with a specific emphasis on hallucination. To tackle this challenge, we present an interactive self-reflection methodology that incorporates knowledge acquisition and answer generation. Through this feedback process, our approach steadily enhances the factuality, consistency, and entailment of the generated answers. Consequently, we harness the interactivity and multitasking ability of LLMs and produce progressively more precise and accurate answers. Experimental results on both automatic and human evaluation demonstrate the superiority of our approach in hallucination reduction compared to baselines.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="The-AI-Incident-Database-as-an-Educational-Tool-to-Raise-Awareness-of-AI-Harms-A-Classroom-Exploration-of-Efficacy-Limitations-Future-Improvements"><a href="#The-AI-Incident-Database-as-an-Educational-Tool-to-Raise-Awareness-of-AI-Harms-A-Classroom-Exploration-of-Efficacy-Limitations-Future-Improvements" class="headerlink" title="The AI Incident Database as an Educational Tool to Raise Awareness of AI Harms: A Classroom Exploration of Efficacy, Limitations, &amp; Future Improvements"></a>The AI Incident Database as an Educational Tool to Raise Awareness of AI Harms: A Classroom Exploration of Efficacy, Limitations, &amp; Future Improvements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06269">http://arxiv.org/abs/2310.06269</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Feffer, Nikolas Martelaro, Hoda Heidari<br>for:* 这 paper 的目的是提高人们对 AI 技术的应用中可能出现的危害的意识，以及如何设计安全、可靠的 AI 系统。methods:* 该 paper 使用了 AI Incident Database (AIID) 作为教学工具，以帮助学生更好地理解 AI 技术在社会高危领域中可能出现的危害。results:* 该 study 发现，通过使用 AIID，学生的初始印象Changed significantly，他们更加意识到 AI 技术在社会中的危害，并且有更强的感觉要设计安全、可靠的 AI 系统。<details>
<summary>Abstract</summary>
Prior work has established the importance of integrating AI ethics topics into computer and data sciences curricula. We provide evidence suggesting that one of the critical objectives of AI Ethics education must be to raise awareness of AI harms. While there are various sources to learn about such harms, The AI Incident Database (AIID) is one of the few attempts at offering a relatively comprehensive database indexing prior instances of harms or near harms stemming from the deployment of AI technologies in the real world. This study assesses the effectiveness of AIID as an educational tool to raise awareness regarding the prevalence and severity of AI harms in socially high-stakes domains. We present findings obtained through a classroom study conducted at an R1 institution as part of a course focused on the societal and ethical considerations around AI and ML. Our qualitative findings characterize students' initial perceptions of core topics in AI ethics and their desire to close the educational gap between their technical skills and their ability to think systematically about ethical and societal aspects of their work. We find that interacting with the database helps students better understand the magnitude and severity of AI harms and instills in them a sense of urgency around (a) designing functional and safe AI and (b) strengthening governance and accountability mechanisms. Finally, we compile students' feedback about the tool and our class activity into actionable recommendations for the database development team and the broader community to improve awareness of AI harms in AI ethics education.
</details>
<details>
<summary>摘要</summary>
We conducted a classroom study at an R1 institution as part of a course focused on the societal and ethical considerations around AI and ML. Our findings show that interacting with the database helps students better understand the magnitude and severity of AI harms and instills in them a sense of urgency around designing functional and safe AI and strengthening governance and accountability mechanisms.Our qualitative findings also reveal that students desire to close the educational gap between their technical skills and their ability to think systematically about ethical and societal aspects of their work. We compile students' feedback about the tool and our class activity into actionable recommendations for the database development team and the broader community to improve awareness of AI harms in AI ethics education.In conclusion, our study demonstrates that the AIID is an effective educational tool to raise awareness of AI harms in socially high-stakes domains, and highlights the importance of incorporating AI ethics education into computer and data sciences curricula to address the urgent need for ethical and responsible AI development.
</details></li>
</ul>
<hr>
<h2 id="CodeFuse-13B-A-Pretrained-Multi-lingual-Code-Large-Language-Model"><a href="#CodeFuse-13B-A-Pretrained-Multi-lingual-Code-Large-Language-Model" class="headerlink" title="CodeFuse-13B: A Pretrained Multi-lingual Code Large Language Model"></a>CodeFuse-13B: A Pretrained Multi-lingual Code Large Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06266">http://arxiv.org/abs/2310.06266</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peng Di, Jianguo Li, Hang Yu, Wei Jiang, Wenting Cai, Yang Cao, Chaoyu Chen, Dajun Chen, Hongwei Chen, Liang Chen, Gang Fan, Jie Gong, Zi Gong, Wen Hu, Tingting Guo, Zhichao Lei, Ting Li, Zheng Li, Ming Liang, Cong Liao, Bingchang Liu, Jiachen Liu, Zhiwei Liu, Shaojun Lu, Min Shen, Guangpei Wang, Huan Wang, Zhi Wang, Zhaogui Xu, Jiawei Yang, Qing Ye, Gehao Zhang, Yu Zhang, Zelin Zhao, Xunjin Zheng, Hailian Zhou, Lifu Zhu, Xianying Zhu</li>
<li>for: 本研究旨在开发一个可以处理英文和中文输入的代码相关任务的大型语言模型（Code LLM），以提高软件工程中代码相关任务的效率。</li>
<li>methods: 本研究使用了一个高质量的预训练数据集，通过程序分析器的精心筛选和训练过程中的优化，以实现代码相关任务的高效性。</li>
<li>results: 实验结果表明，CodeFuse-13B在实际应用场景中，如代码生成、代码翻译、代码注释和测试用例生成等，都能够更好地处理中文输入，并在人类评价（HumanEval）中获得37.10%的 passer@1 分数，位居同参数大小的多语言代码 LLM 之列。<details>
<summary>Abstract</summary>
Code Large Language Models (Code LLMs) have gained significant attention in the industry due to their wide applications in the full lifecycle of software engineering. However, the effectiveness of existing models in understanding non-English inputs for multi-lingual code-related tasks is still far from well studied. This paper introduces CodeFuse-13B, an open-sourced pre-trained code LLM. It is specifically designed for code-related tasks with both English and Chinese prompts and supports over 40 programming languages. CodeFuse achieves its effectiveness by utilizing a high quality pre-training dataset that is carefully filtered by program analyzers and optimized during the training process. Extensive experiments are conducted using real-world usage scenarios, the industry-standard benchmark HumanEval-x, and the specially designed CodeFuseEval for Chinese prompts. To assess the effectiveness of CodeFuse, we actively collected valuable human feedback from the AntGroup's software development process where CodeFuse has been successfully deployed. The results demonstrate that CodeFuse-13B achieves a HumanEval pass@1 score of 37.10%, positioning it as one of the top multi-lingual code LLMs with similar parameter sizes. In practical scenarios, such as code generation, code translation, code comments, and testcase generation, CodeFuse performs better than other models when confronted with Chinese prompts.
</details>
<details>
<summary>摘要</summary>
大型语言模型（Code LLMs）在业界获得了广泛关注，因为它们在软件工程的全生命周期中有广泛的应用。然而，现有模型对非英语输入的效果在多种语言程式码相关任务中仍然不够熟悉。本文介绍CodeFuse-13B，一个开源预训式程式码大型语言模型。它可以处理英语和中文提示，支持40种程式语言，并且通过使用高质量的预训数据和训练过程中的优化而获得效果。我们通过实际的使用场景、业界标准对chmark HumanEval-x以及特别设计的CodeFuseEval进行了广泛的实验。为评估CodeFuse的效果，我们 актив地收集了AntGroup的软件开发过程中的宝贵人类反馈。结果显示，CodeFuse-13B在HumanEval pass@1 score中获得37.10%，位居多种多语言程式码大型语言模型之一。在实际应用中，例如程式码生成、程式码翻译、程式码注释和测试案例生成等方面，CodeFuse在面对中文提示时表现更好。
</details></li>
</ul>
<hr>
<h2 id="Self-Discriminative-Modeling-for-Anomalous-Graph-Detection"><a href="#Self-Discriminative-Modeling-for-Anomalous-Graph-Detection" class="headerlink" title="Self-Discriminative Modeling for Anomalous Graph Detection"></a>Self-Discriminative Modeling for Anomalous Graph Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06261">http://arxiv.org/abs/2310.06261</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinyu Cai, Yunhe Zhang, Jicong Fan</li>
<li>for: 检测异常图像使用机器学习模型，具有广泛的应用在分子、生物和社会网络数据分析中。</li>
<li>methods: 提出了一种自适应模型框架，通过学习自己的分类器，从给定的正常图像和生成的 Pseudo-异常图像中学习异常图像特征。</li>
<li>results: 提出了三种不同的计算效率和稳定性的算法，并与一些州OF-the-art图像级异常检测基线方法进行比较，显著提高了AUC。<details>
<summary>Abstract</summary>
This paper studies the problem of detecting anomalous graphs using a machine learning model trained on only normal graphs, which has many applications in molecule, biology, and social network data analysis. We present a self-discriminative modeling framework for anomalous graph detection. The key idea, mathematically and numerically illustrated, is to learn a discriminator (classifier) from the given normal graphs together with pseudo-anomalous graphs generated by a model jointly trained, where we never use any true anomalous graphs and we hope that the generated pseudo-anomalous graphs interpolate between normal ones and (real) anomalous ones. Under the framework, we provide three algorithms with different computational efficiencies and stabilities for anomalous graph detection. The three algorithms are compared with several state-of-the-art graph-level anomaly detection baselines on nine popular graph datasets (four with small size and five with moderate size) and show significant improvement in terms of AUC. The success of our algorithms stems from the integration of the discriminative classifier and the well-posed pseudo-anomalous graphs, which provide new insights for anomaly detection. Moreover, we investigate our algorithms for large-scale imbalanced graph datasets. Surprisingly, our algorithms, though fully unsupervised, are able to significantly outperform supervised learning algorithms of anomalous graph detection. The corresponding reason is also analyzed.
</details>
<details>
<summary>摘要</summary>
Under this framework, we present three algorithms with different computational efficiencies and stabilities for anomalous graph detection. These algorithms are compared with several state-of-the-art graph-level anomaly detection baselines on nine popular graph datasets, and they show significant improvement in terms of AUC. The success of our algorithms is due to the integration of the discriminative classifier and the well-posed pseudo-anomalous graphs, which provide new insights for anomaly detection.Moreover, we investigate our algorithms for large-scale imbalanced graph datasets and find that they can significantly outperform supervised learning algorithms for anomalous graph detection. We also analyze the reason for this surprising result.
</details></li>
</ul>
<hr>
<h2 id="Get-the-gist-Using-large-language-models-for-few-shot-decontextualization"><a href="#Get-the-gist-Using-large-language-models-for-few-shot-decontextualization" class="headerlink" title="Get the gist? Using large language models for few-shot decontextualization"></a>Get the gist? Using large language models for few-shot decontextualization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06254">http://arxiv.org/abs/2310.06254</a></li>
<li>repo_url: None</li>
<li>paper_authors: Benjamin Kane, Lenhart Schubert</li>
<li>for: 实现将句子脱离内容，以便在不同领域中重复使用（decontextualization）。</li>
<li>methods: 使用大型语言模型，并通过几次训练来学习将句子脱离内容。</li>
<li>results: 在多个领域中，使用几次训练的方法可以实现可靠的将句子脱离内容，并且可以跨领域实现 Transfer Learning。<details>
<summary>Abstract</summary>
In many NLP applications that involve interpreting sentences within a rich context -- for instance, information retrieval systems or dialogue systems -- it is desirable to be able to preserve the sentence in a form that can be readily understood without context, for later reuse -- a process known as ``decontextualization''. While previous work demonstrated that generative Seq2Seq models could effectively perform decontextualization after being fine-tuned on a specific dataset, this approach requires expensive human annotations and may not transfer to other domains. We propose a few-shot method of decontextualization using a large language model, and present preliminary results showing that this method achieves viable performance on multiple domains using only a small set of examples.
</details>
<details>
<summary>摘要</summary>
在许多自然语言处理（NLP）应用中，如信息检索系统或对话系统，旨在保留句子的形式，以便在不同上下文中重用——一种称为“减 contextualization”的过程。而过去的研究表明，可以使用生成 Seq2Seq 模型进行减 contextualization，但这种方法需要优质的人工标注，并且可能无法在其他领域传输。我们提出了一种几个示例的方法，使用大型语言模型进行减 contextualization，并提供了多个领域的初步结果，表明这种方法可以在不同领域 достичь可行的性能，只需要一小组示例。
</details></li>
</ul>
<hr>
<h2 id="We-are-what-we-repeatedly-do-Inducing-and-deploying-habitual-schemas-in-persona-based-responses"><a href="#We-are-what-we-repeatedly-do-Inducing-and-deploying-habitual-schemas-in-persona-based-responses" class="headerlink" title="We are what we repeatedly do: Inducing and deploying habitual schemas in persona-based responses"></a>We are what we repeatedly do: Inducing and deploying habitual schemas in persona-based responses</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06245">http://arxiv.org/abs/2310.06245</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bkane2/habitual-response-generation">https://github.com/bkane2/habitual-response-generation</a></li>
<li>paper_authors: Benjamin Kane, Lenhart Schubert</li>
<li>for: 这个论文主要是为了提出一种基于对话人物的语言模型生成方法，以便在具体的应用场景中生成符合人物性的响应。</li>
<li>methods: 这种方法使用了一种Explicit schema representation来捕捉对话人物的习惯知识，然后使用这些schema来控制语言模型生成 persona-based responses。</li>
<li>results: 作者在文章中提出了一种方法来从 generic facts 中生成 schema，然后从这些 schema 中挑选合适的一些来控制语言模型生成响应。这种方法可以帮助实现在对话系统中更加自然地表现出人物性。<details>
<summary>Abstract</summary>
Many practical applications of dialogue technology require the generation of responses according to a particular developer-specified persona. While a variety of personas can be elicited from recent large language models, the opaqueness and unpredictability of these models make it desirable to be able to specify personas in an explicit form. In previous work, personas have typically been represented as sets of one-off pieces of self-knowledge that are retrieved by the dialogue system for use in generation. However, in realistic human conversations, personas are often revealed through story-like narratives that involve rich habitual knowledge -- knowledge about kinds of events that an agent often participates in (e.g., work activities, hobbies, sporting activities, favorite entertainments, etc.), including typical goals, sub-events, preconditions, and postconditions of those events. We capture such habitual knowledge using an explicit schema representation, and propose an approach to dialogue generation that retrieves relevant schemas to condition a large language model to generate persona-based responses. Furthermore, we demonstrate a method for bootstrapping the creation of such schemas by first generating generic passages from a set of simple facts, and then inducing schemas from the generated passages.
</details>
<details>
<summary>摘要</summary>
很多实际应用场景中，对话技术需要根据开发者指定的 persona 生成响应。当前的大语言模型可以生成多种 persona，但这些模型的 complexity 和难于预测性使得可以将 persona 表示为明确的形式。在过去的工作中， persona 通常被表示为一组自我认知的一次性 retrieve，但在真实的人类对话中， persona 通常通过 rich 的习惯知识表示，包括代表工作、习惯、运动、喜好等活动的常见目标、子事件、前提和后果等知识。我们使用明确的schema表示方式来捕捉这些习惯知识，并提议一种基于 schema 的对话生成方法，使用这些 schema 来condition 一个大语言模型，以生成基于 persona 的响应。此外，我们还提出了一种方法，通过首先生成一组简单的事实，然后从这些事实中推导出 schema，来初始化 schema 的创建。
</details></li>
</ul>
<hr>
<h2 id="Model-Tuning-or-Prompt-Tuning-A-Study-of-Large-Language-Models-for-Clinical-Concept-and-Relation-Extraction"><a href="#Model-Tuning-or-Prompt-Tuning-A-Study-of-Large-Language-Models-for-Clinical-Concept-and-Relation-Extraction" class="headerlink" title="Model Tuning or Prompt Tuning? A Study of Large Language Models for Clinical Concept and Relation Extraction"></a>Model Tuning or Prompt Tuning? A Study of Large Language Models for Clinical Concept and Relation Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06239">http://arxiv.org/abs/2310.06239</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheng Peng, Xi Yang, Kaleb E Smith, Zehao Yu, Aokun Chen, Jiang Bian, Yonghui Wu<br>for: 这种研究的目的是开发大型自然语言模型（LLM）上的软提示学习算法，检查提示的形状，提取提示使用冻结&#x2F;不冻结LLM的方法，转移学习和少量学习能力。methods: 我们开发了一种基于软提示的LLM模型，并对4种训练策略进行比较，包括（1）没有提示的精度训练；（2）硬提示使用不冻结LLM；（3）软提示使用不冻结LLM；以及（4）软提示使用冻结LLM。我们使用了7种预训练LLM进行评估，并在两个benchmark数据集上进行评估。results: 结果表明，当LLM冻结时，GatorTron-3.9B with soft prompting得到了最好的精度分数为0.9118和0.8604 для概念EXTRACTION，与传统精度训练和硬提示基本模型相比，提高了0.6<del>3.1%和1.2</del>2.9%。GatorTron-345M with soft prompting得到了最好的F1分数为0.8332和0.7488 для端到端关系EXTRACTION，与其他两个模型相比，提高了0.2<del>2%和0.6</del>11.7%。当LLM冻结时，小型（i.e., 345 million parameters）LLM有一个很大的差距，需要扩大到比较大的参数量才能与冻结模型竞争。在跨机构评估中，使用冻结GatorTron-8.9B模型的软提示方法获得了最好的表现。这项研究表明了以下三点：（1）机器可以更好地学习软提示，（2）冻结LLM具有更好的少量学习和转移学习能力，以便多机构应用，（3）冻结LLM需要大型模型。<details>
<summary>Abstract</summary>
Objective To develop soft prompt-based learning algorithms for large language models (LLMs), examine the shape of prompts, prompt-tuning using frozen/unfrozen LLMs, transfer learning, and few-shot learning abilities. Methods We developed a soft prompt-based LLM model and compared 4 training strategies including (1) fine-tuning without prompts; (2) hard-prompt with unfrozen LLMs; (3) soft-prompt with unfrozen LLMs; and (4) soft-prompt with frozen LLMs. We evaluated 7 pretrained LLMs using the 4 training strategies for clinical concept and relation extraction on two benchmark datasets. We evaluated the transfer learning ability of the prompt-based learning algorithms in a cross-institution setting. We also assessed the few-shot learning ability. Results and Conclusion When LLMs are unfrozen, GatorTron-3.9B with soft prompting achieves the best strict F1-scores of 0.9118 and 0.8604 for concept extraction, outperforming the traditional fine-tuning and hard prompt-based models by 0.6~3.1% and 1.2~2.9%, respectively; GatorTron-345M with soft prompting achieves the best F1-scores of 0.8332 and 0.7488 for end-to-end relation extraction, outperforming the other two models by 0.2~2% and 0.6~11.7%, respectively. When LLMs are frozen, small (i.e., 345 million parameters) LLMs have a big gap to be competitive with unfrozen models; scaling LLMs up to billions of parameters makes frozen LLMs competitive with unfrozen LLMs. For cross-institute evaluation, soft prompting with a frozen GatorTron-8.9B model achieved the best performance. This study demonstrates that (1) machines can learn soft prompts better than humans, (2) frozen LLMs have better few-shot learning ability and transfer learning ability to facilitate muti-institution applications, and (3) frozen LLMs require large models.
</details>
<details>
<summary>摘要</summary>
方法：我们开发了一种软提示基于LLM模型，并比较了四种训练策略：（1）不使用提示；（2）使用不冻LLM的硬提示；（3）使用不冻LLM的软提示；（4）使用冻LLM的软提示。我们使用七种预训练LLM模型在两个benchmark数据集上进行临床概念和关系提取任务的评估。我们还评估了提示基于学习算法的跨机构传输学习能力和少量学习能力。结果和结论：当LLM模型处于不冻状态时，GatorTron-3.9B模型通过软提示方式取得了概念提取任务的严格F1分数最高，为0.9118和0.8604，分别超过了传统的精度训练和硬提示基于模型的0.6~3.1%和1.2~2.9%。GatorTron-345M模型通过软提示方式取得了综合关系提取任务的F1分数最高，为0.8332和0.7488，分别超过了其他两个模型的0.2~2%和0.6~11.7%。当LLM模型处于冻结状态时，小型（i.e., 345 million parameters）LLM模型具有大的差距，需要通过扩大模型规模来使其与不冻模型竞争。跨机构评估中，使用冻结GatorTron-8.9B模型的软提示方式取得了最佳性能。这个研究表明：（1）机器可以更好地学习软提示 than humans;（2）冻结LLM模型具有更好的少量学习和传输学习能力，以便实现多机构应用;（3）冻结LLM模型需要大型模型。
</details></li>
</ul>
<hr>
<h2 id="Tackling-Data-Bias-in-MUSIC-AVQA-Crafting-a-Balanced-Dataset-for-Unbiased-Question-Answering"><a href="#Tackling-Data-Bias-in-MUSIC-AVQA-Crafting-a-Balanced-Dataset-for-Unbiased-Question-Answering" class="headerlink" title="Tackling Data Bias in MUSIC-AVQA: Crafting a Balanced Dataset for Unbiased Question-Answering"></a>Tackling Data Bias in MUSIC-AVQA: Crafting a Balanced Dataset for Unbiased Question-Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06238">http://arxiv.org/abs/2310.06238</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiulong Liu, Zhikang Dong, Peng Zhang</li>
<li>for: 本研究旨在提高多模态（音频、视频、文本）融合研究的进步，并减少每个模态的偏见。</li>
<li>methods: 研究人员仔细审查原始数据集中的每个问题类型，选择具有明显的答案偏见的问题。然后，研究人员收集了补做的视频和问题，使得每个问题类型中的答案分布均匀。基于这些改进，研究人员构建了名为MUSIC-AVQA v2.0的新数据集。此外，研究人员还提出了一种基于音视文三Modal的新基线模型，在MUSIC-AVQA v2.0上达到了新的状态率。</li>
<li>results: 研究人员在MUSIC-AVQA v2.0上使用新基线模型，比对 existed benchmarks 提高了2%的准确率，创造了新的状态率记录。<details>
<summary>Abstract</summary>
In recent years, there has been a growing emphasis on the intersection of audio, vision, and text modalities, driving forward the advancements in multimodal research. However, strong bias that exists in any modality can lead to the model neglecting the others. Consequently, the model's ability to effectively reason across these diverse modalities is compromised, impeding further advancement. In this paper, we meticulously review each question type from the original dataset, selecting those with pronounced answer biases. To counter these biases, we gather complementary videos and questions, ensuring that no answers have outstanding skewed distribution. In particular, for binary questions, we strive to ensure that both answers are almost uniformly spread within each question category. As a result, we construct a new dataset, named MUSIC-AVQA v2.0, which is more challenging and we believe could better foster the progress of AVQA task. Furthermore, we present a novel baseline model that delves deeper into the audio-visual-text interrelation. On MUSIC-AVQA v2.0, this model surpasses all the existing benchmarks, improving accuracy by 2% on MUSIC-AVQA v2.0, setting a new state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
近年来，有关音频、视觉和文本modalities的交叉研究得到了越来越多的关注，这些研究带来了多模态研究的进步。然而，任何一个modalities中的强大偏见可能导致模型忽略其他modalities。因此，模型在多个不同modalities之间有效地进行推理的能力受到了限制，这障碍了进一步的进步。在这篇论文中，我们仔细审查了原始数据集中的每个问题类型，选择具有明显的答案偏见的问题。为了纠正这些偏见，我们收集了补充视频和问题，确保每个问题类型中的答案都具有均匀的分布。因此，我们构建了一个新的数据集，名为MUSIC-AVQA v2.0，它比原始数据集更加具有挑战性，我们认为这将更好地推动AVQA任务的进步。此外，我们还提出了一种新的基线模型，它深入探究音频-视觉-文本之间的关系。在MUSIC-AVQA v2.0上，这种模型的表现超过了所有现有的 bench marks，提高了MUSIC-AVQA v2.0上的准确率2%，创造了新的状态状态表现。
</details></li>
</ul>
<hr>
<h2 id="Evolution-of-Natural-Language-Processing-Technology-Not-Just-Language-Processing-Towards-General-Purpose-AI"><a href="#Evolution-of-Natural-Language-Processing-Technology-Not-Just-Language-Processing-Towards-General-Purpose-AI" class="headerlink" title="Evolution of Natural Language Processing Technology: Not Just Language Processing Towards General Purpose AI"></a>Evolution of Natural Language Processing Technology: Not Just Language Processing Towards General Purpose AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06228">http://arxiv.org/abs/2310.06228</a></li>
<li>repo_url: None</li>
<li>paper_authors: Masahiro Yamamoto</li>
<li>for: 本研究旨在探讨如何通过自然语言处理（NLP）技术实现“练习做完善”的原则。</li>
<li>methods: 本研究使用了深度学习技术来处理大量的文本数据，并通过这种方法实现了不同的语言学习任务。</li>
<li>results: 研究结果表明，通过使用深度学习技术可以实现“练习做完善”的原则，并且可以进行四则运算而无需显式学习。这种方法可以解释复杂的图像和生成与其相应的文本描述。此外，本研究还提供了应用于商业领域的实践例子。<details>
<summary>Abstract</summary>
Since the invention of computers, communication through natural language (actual human language) has been a dream technology. However, natural language is extremely difficult to mathematically formulate, making it difficult to realize as an algorithm without considering programming. While there have been numerous technological developments, one cannot say that any results allowing free utilization have been achieved thus far. In the case of language learning in humans, for instance when learning one's mother tongue or foreign language, one must admit that this process is similar to the adage "practice makes perfect" in principle, even though the learning method is significant up to a point. Deep learning has played a central role in contemporary AI technology in recent years. When applied to natural language processing (NLP), this produced unprecedented results. Achievements exceeding the initial predictions have been reported from the results of learning vast amounts of textual data using deep learning. For instance, four arithmetic operations could be performed without explicit learning, thereby enabling the explanation of complex images and the generation of images from corresponding explanatory texts. It is an accurate example of the learner embodying the concept of "practice makes perfect" by using vast amounts of textual data. This report provides a technological explanation of how cutting-edge NLP has made it possible to realize the "practice makes perfect" principle. Additionally, examples of how this can be applied to business are provided. We reported in June 2022 in Japanese on the NLP movement from late 2021 to early 2022. We would like to summarize this as a memorandum since this is just the initial movement leading to the current large language models (LLMs).
</details>
<details>
<summary>摘要</summary>
Deep learning has played a central role in contemporary AI technology in recent years. When applied to natural language processing (NLP), this produced unprecedented results. Achievements exceeding the initial predictions have been reported from the results of learning vast amounts of textual data using deep learning. For instance, four arithmetic operations could be performed without explicit learning, thereby enabling the explanation of complex images and the generation of images from corresponding explanatory texts. It is an accurate example of the learner embodying the concept of "practice makes perfect" by using vast amounts of textual data.This report provides a technological explanation of how cutting-edge NLP has made it possible to realize the "practice makes perfect" principle. Additionally, examples of how this can be applied to business are provided. We reported in June 2022 in Japanese on the NLP movement from late 2021 to early 2022. We would like to summarize this as a memorandum since this is just the initial movement leading to the current large language models (LLMs).
</details></li>
</ul>
<hr>
<h2 id="GPT-4-as-an-Agronomist-Assistant-Answering-Agriculture-Exams-Using-Large-Language-Models"><a href="#GPT-4-as-an-Agronomist-Assistant-Answering-Agriculture-Exams-Using-Large-Language-Models" class="headerlink" title="GPT-4 as an Agronomist Assistant? Answering Agriculture Exams Using Large Language Models"></a>GPT-4 as an Agronomist Assistant? Answering Agriculture Exams Using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06225">http://arxiv.org/abs/2310.06225</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bruno Silva, Leonardo Nunes, Roberto Estevão, Vijay Aski, Ranveer Chandra</li>
<li>for: 评估大语言模型（LLMs）在农业领域的自然语言理解能力，并评估LLMs是否可以取代人类考试。</li>
<li>methods: 使用RAG（Retrieval-Augmented Generation）和ER（Ensemble Refinement）技术，结合信息检索、生成和提示策略，提高LLMs的性能。</li>
<li>results: GPT-4在农业考试中达到了88%的正确率，比之前的通用模型高，并在一个实验中与人类参与者相比而得到了最高表现。 GPT-4还可以为农业教育、评估和作物管理提供有价值的意见和建议。<details>
<summary>Abstract</summary>
Large language models (LLMs) have demonstrated remarkable capabilities in natural language understanding across various domains, including healthcare and finance. For some tasks, LLMs achieve similar or better performance than trained human beings, therefore it is reasonable to employ human exams (e.g., certification tests) to assess the performance of LLMs. We present a comprehensive evaluation of popular LLMs, such as Llama 2 and GPT, on their ability to answer agriculture-related questions. In our evaluation, we also employ RAG (Retrieval-Augmented Generation) and ER (Ensemble Refinement) techniques, which combine information retrieval, generation capabilities, and prompting strategies to improve the LLMs' performance. To demonstrate the capabilities of LLMs, we selected agriculture exams and benchmark datasets from three of the largest agriculture producer countries: Brazil, India, and the USA. Our analysis highlights GPT-4's ability to achieve a passing score on exams to earn credits for renewing agronomist certifications, answering 93% of the questions correctly and outperforming earlier general-purpose models, which achieved 88% accuracy. On one of our experiments, GPT-4 obtained the highest performance when compared to human subjects. This performance suggests that GPT-4 could potentially pass on major graduate education admission tests or even earn credits for renewing agronomy certificates. We also explore the models' capacity to address general agriculture-related questions and generate crop management guidelines for Brazilian and Indian farmers, utilizing robust datasets from the Brazilian Agency of Agriculture (Embrapa) and graduate program exams from India. The results suggest that GPT-4, ER, and RAG can contribute meaningfully to agricultural education, assessment, and crop management practice, offering valuable insights to farmers and agricultural professionals.
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLM）已经展示了在各种领域的自然语言理解能力，包括医疗和金融等。对于某些任务，LLM可以达到人类训练的水平或更高，因此可以使用人类考试（例如证书考试）来评估LLM的性能。我们对 популяр的LLM，如LLama 2和GPT，进行了全面的评估，以测试它们在农业相关问题上的能力。在我们的评估中，我们还使用了RAG（Retrieval-Augmented Generation）和ER（Ensemble Refinement）技术，这些技术将 Retrieval、生成和提示策略相结合以提高LLM的性能。为了展示LLM的能力，我们选择了来自世界上三个最大农业生产国的农业考试和标准Dataset：巴西、印度和美国。我们的分析显示，GPT-4在考试中可以达到88%的正确率，比之前的通用模型提高了5%。在一些实验中，GPT-4 even outperformed human subjects，这种性能表明GPT-4可能可以通过大学 entrance exams或者农业证书更新考试。我们还探讨了模型在农业相关问题上的总能力和生成农业管理指南的能力，使用了巴西农业局（Embrapa）的robust数据集和印度大学考试。结果表明，GPT-4、ER和RAG可以在农业教育、评估和农业管理实践中发挥重要作用，为农民和农业专业人员提供有价值的意见。
</details></li>
</ul>
<hr>
<h2 id="SUBP-Soft-Uniform-Block-Pruning-for-1xN-Sparse-CNNs-Multithreading-Acceleration"><a href="#SUBP-Soft-Uniform-Block-Pruning-for-1xN-Sparse-CNNs-Multithreading-Acceleration" class="headerlink" title="SUBP: Soft Uniform Block Pruning for 1xN Sparse CNNs Multithreading Acceleration"></a>SUBP: Soft Uniform Block Pruning for 1xN Sparse CNNs Multithreading Acceleration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06218">http://arxiv.org/abs/2310.06218</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/JingyangXiang/SUBP">https://github.com/JingyangXiang/SUBP</a></li>
<li>paper_authors: Jingyang Xiang, Siqi Li, Jun Chen, Shipeng Bai, Yukai Ma, Guang Dai, Yong Liu</li>
<li>for: This paper aims to train a uniform 1$\times$N sparse structured network from scratch, which can overcome the problems of expensive training cost, memory access, sub-optimal model quality, and unbalanced workload across threads in existing sparse weight selection and fine-tuning methods.</li>
<li>methods: The proposed method, called Soft Uniform Block Pruning (SUBP), repeatedly allows pruned blocks to regrow to the network based on block angular redundancy and importance sampling in a uniform manner throughout the training process, making the model less dependent on pre-training and achieving balanced workload.</li>
<li>results: The paper shows that the proposed SUBP method consistently outperforms existing 1$\times$N and structured sparsity methods based on pre-trained models or training from scratch, as demonstrated by comprehensive experiments across various CNN architectures on ImageNet.<details>
<summary>Abstract</summary>
The study of sparsity in Convolutional Neural Networks (CNNs) has become widespread to compress and accelerate models in environments with limited resources. By constraining N consecutive weights along the output channel to be group-wise non-zero, the recent network with 1$\times$N sparsity has received tremendous popularity for its three outstanding advantages: 1) A large amount of storage space saving by a \emph{Block Sparse Row} matrix. 2) Excellent performance at a high sparsity. 3) Significant speedups on CPUs with Advanced Vector Extensions. Recent work requires selecting and fine-tuning 1$\times$N sparse weights based on dense pre-trained weights, leading to the problems such as expensive training cost and memory access, sub-optimal model quality, as well as unbalanced workload across threads (different sparsity across output channels). To overcome them, this paper proposes a novel \emph{\textbf{S}oft \textbf{U}niform \textbf{B}lock \textbf{P}runing} (SUBP) approach to train a uniform 1$\times$N sparse structured network from scratch. Specifically, our approach tends to repeatedly allow pruned blocks to regrow to the network based on block angular redundancy and importance sampling in a uniform manner throughout the training process. It not only makes the model less dependent on pre-training, reduces the model redundancy and the risk of pruning the important blocks permanently but also achieves balanced workload. Empirically, on ImageNet, comprehensive experiments across various CNN architectures show that our SUBP consistently outperforms existing 1$\times$N and structured sparsity methods based on pre-trained models or training from scratch. Source codes and models are available at \url{https://github.com/JingyangXiang/SUBP}.
</details>
<details>
<summary>摘要</summary>
study of sparsity in Convolutional Neural Networks (CNNs) has become widespread to compress and accelerate models in environments with limited resources. By constraining N consecutive weights along the output channel to be group-wise non-zero, the recent network with 1×N sparsity has received tremendous popularity for its three outstanding advantages: 1) A large amount of storage space saving by a Block Sparse Row matrix. 2) Excellent performance at a high sparsity. 3) Significant speedups on CPUs with Advanced Vector Extensions. Recent work requires selecting and fine-tuning 1×N sparse weights based on dense pre-trained weights, leading to the problems such as expensive training cost and memory access, sub-optimal model quality, as well as unbalanced workload across threads (different sparsity across output channels). To overcome them, this paper proposes a novel Soft Uniform Block Pruning (SUBP) approach to train a uniform 1×N sparse structured network from scratch. Specifically, our approach tends to repeatedly allow pruned blocks to regrow to the network based on block angular redundancy and importance sampling in a uniform manner throughout the training process. It not only makes the model less dependent on pre-training, reduces the model redundancy and the risk of pruning the important blocks permanently but also achieves balanced workload. Empirically, on ImageNet, comprehensive experiments across various CNN architectures show that our SUBP consistently outperforms existing 1×N and structured sparsity methods based on pre-trained models or training from scratch. Source codes and models are available at https://github.com/JingyangXiang/SUBP.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/10/cs.AI_2023_10_10/" data-id="clpahu6yt00573h88cvpmht5u" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_10_10" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/10/cs.CL_2023_10_10/" class="article-date">
  <time datetime="2023-10-10T11:00:00.000Z" itemprop="datePublished">2023-10-10</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/10/cs.CL_2023_10_10/">cs.CL - 2023-10-10</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Crossing-the-Threshold-Idiomatic-Machine-Translation-through-Retrieval-Augmentation-and-Loss-Weighting"><a href="#Crossing-the-Threshold-Idiomatic-Machine-Translation-through-Retrieval-Augmentation-and-Loss-Weighting" class="headerlink" title="Crossing the Threshold: Idiomatic Machine Translation through Retrieval Augmentation and Loss Weighting"></a>Crossing the Threshold: Idiomatic Machine Translation through Retrieval Augmentation and Loss Weighting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07081">http://arxiv.org/abs/2310.07081</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nightingal3/idiom-translation">https://github.com/nightingal3/idiom-translation</a></li>
<li>paper_authors: Emmy Liu, Aditi Chaudhary, Graham Neubig</li>
<li>for: 本研究旨在提高机器翻译系统对idiomatic表达的翻译能力。</li>
<li>methods: 本研究使用transformer型机器翻译模型，并提出了两种简单 yet有效的技巧来提高翻译效果：一是策略性地增加训练损失的权重，二是使用检索支持模型。</li>
<li>results: 研究发现，使用这两种技巧可以提高一个强有力的预训练机器翻译模型对idiomatic sentences的翻译精度，最高提高13%。此外，这些技巧还可以对非idiomatic sentences进行改进。<details>
<summary>Abstract</summary>
Idioms are common in everyday language, but often pose a challenge to translators because their meanings do not follow from the meanings of their parts. Despite significant advances, machine translation systems still struggle to translate idiomatic expressions. We provide a simple characterization of idiomatic translation and related issues. This allows us to conduct a synthetic experiment revealing a tipping point at which transformer-based machine translation models correctly default to idiomatic translations. To expand multilingual resources, we compile a dataset of ~4k natural sentences containing idiomatic expressions in French, Finnish, and Japanese. To improve translation of natural idioms, we introduce two straightforward yet effective techniques: the strategic upweighting of training loss on potentially idiomatic sentences, and using retrieval-augmented models. This not only improves the accuracy of a strong pretrained MT model on idiomatic sentences by up to 13% in absolute accuracy, but also holds potential benefits for non-idiomatic sentences.
</details>
<details>
<summary>摘要</summary>
idioms 是日常语言中很常见的表达方式，但是它们的意思并不是由其部件的意思所推导出来。虽然有了 significiant advances，机器翻译系统仍然难以翻译idiomatic表达。我们提供了一个简单的idiomatic翻译特征化和相关问题。这允许我们进行一个 sintethic experiment，揭示了使用 transformer-based 机器翻译模型时，正确地采用idiomatic翻译的tipping point。为扩展多语言资源，我们编译了 ~4k 自然句子中包含idiomatic表达的 French、Finland 和 Japanese 语言数据集。为了改进自然idiomatic翻译，我们介绍了两种简单 yet effective 技术：一是对潜在idiomatic句子的训练损失进行战略性增加，二是使用retrieval-augmented模型。这不仅提高了一个强制trained MT模型在idiomatic句子上的准确率，还有可能对非idiomatic句子产生正面的影响。
</details></li>
</ul>
<hr>
<h2 id="Automatic-Macro-Mining-from-Interaction-Traces-at-Scale"><a href="#Automatic-Macro-Mining-from-Interaction-Traces-at-Scale" class="headerlink" title="Automatic Macro Mining from Interaction Traces at Scale"></a>Automatic Macro Mining from Interaction Traces at Scale</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07023">http://arxiv.org/abs/2310.07023</a></li>
<li>repo_url: None</li>
<li>paper_authors: Forrest Huang, Gang Li, Tao Li, Yang Li</li>
<li>for: 本研究旨在自动从移动应用程序中提取含义强大的 macro，以便更好地理解移动交互和实现任务自动化。</li>
<li>methods: 本研究提出了一种基于大型自然语言模型（LLM）的方法，可以自动从随机和用户自定义的移动交互轨迹中提取含义强大的 macro。这些 macro 被自动标记为自然语言描述，并且可以完全执行。</li>
<li>results: 研究人员通过多种研究，包括用户评估、比较分析和自动执行这些 macro，证明了本approach的有效性和提取的 macro 在下游应用中的有用性。<details>
<summary>Abstract</summary>
Macros are building block tasks of our everyday smartphone activity (e.g., "login", or "booking a flight"). Effectively extracting macros is important for understanding mobile interaction and enabling task automation. These macros are however difficult to extract at scale as they can be comprised of multiple steps yet hidden within programmatic components of the app. In this paper, we introduce a novel approach based on Large Language Models (LLMs) to automatically extract semantically meaningful macros from both random and user-curated mobile interaction traces. The macros produced by our approach are automatically tagged with natural language descriptions and are fully executable. To examine the quality of extraction, we conduct multiple studies, including user evaluation, comparative analysis against human-curated tasks, and automatic execution of these macros. These experiments and analyses show the effectiveness of our approach and the usefulness of extracted macros in various downstream applications.
</details>
<details>
<summary>摘要</summary>
macro 是我们每天手机活动的基本构建块（例如，"登录" 或 "预订航班")。抽取macro有助于理解移动交互和实现任务自动化。但是，由于这些macro可能由多个步骤组成，并且隐藏在应用程序的编程组件中，因此EXTRACTING MACROS AT SCALE 是一项重要的挑战。在这篇论文中，我们提出了一种基于大语言模型（LLMs）的新方法，可以自动抽取手机交互轨迹中的semantically meaningful macro。这些macro被自动标记为自然语言描述，并且可以自动执行。为了评估EXTRACTING MACROS的质量，我们进行了多个研究，包括用户评估、对人工Curate任务进行比较分析，以及自动执行这些macro。这些实验和分析表明了我们的方法的有效性和抽取的macro的多种下游应用。
</details></li>
</ul>
<hr>
<h2 id="LLMs-as-Potential-Brainstorming-Partners-for-Math-and-Science-Problems"><a href="#LLMs-as-Potential-Brainstorming-Partners-for-Math-and-Science-Problems" class="headerlink" title="LLMs as Potential Brainstorming Partners for Math and Science Problems"></a>LLMs as Potential Brainstorming Partners for Math and Science Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10677">http://arxiv.org/abs/2310.10677</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sophia Gu</li>
<li>for: 这种研究的目的是探索现代深度学习模型在与人类合作解决复杂数学和科学问题时的能力。</li>
<li>methods: 这项研究使用了大量语言模型（LLMs）的最新进展，特别是GPT-4模型，进行了详细的案例研究，以探索这些模型在人类合作brainstorming中的能力和局限性。</li>
<li>results: 研究发现，当前的state-of-the-art LLMs在collective brainstorming中表现出了扎实的能力，并且可以帮助人类解决一些复杂的数学和科学问题。但是，这些模型也存在一些局限性和缺陷，需要进一步的改进和调整。<details>
<summary>Abstract</summary>
With the recent rise of widely successful deep learning models, there is emerging interest among professionals in various math and science communities to see and evaluate the state-of-the-art models' abilities to collaborate on finding or solving problems that often require creativity and thus brainstorming. While a significant chasm still exists between current human-machine intellectual collaborations and the resolution of complex math and science problems, such as the six unsolved Millennium Prize Problems, our initial investigation into this matter reveals a promising step towards bridging the divide. This is due to the recent advancements in Large Language Models (LLMs). More specifically, we conduct comprehensive case studies to explore both the capabilities and limitations of the current state-of-the-art LLM, notably GPT-4, in collective brainstorming with humans.
</details>
<details>
<summary>摘要</summary>
Recently, with the rise of widely successful deep learning models, there is growing interest among professionals in various math and science communities to see and evaluate the state-of-the-art models' abilities to collaborate on finding or solving problems that often require creativity and thus brainstorming. Although a significant gap still exists between current human-machine intellectual collaborations and the resolution of complex math and science problems, such as the six unsolved Millennium Prize Problems, our preliminary investigation into this matter reveals a promising step towards bridging the divide. This is due to the recent advancements in Large Language Models (LLMs). Specifically, we conduct comprehensive case studies to explore both the capabilities and limitations of the current state-of-the-art LLM, notably GPT-4, in collective brainstorming with humans.
</details></li>
</ul>
<hr>
<h2 id="Violation-of-Expectation-via-Metacognitive-Prompting-Reduces-Theory-of-Mind-Prediction-Error-in-Large-Language-Models"><a href="#Violation-of-Expectation-via-Metacognitive-Prompting-Reduces-Theory-of-Mind-Prediction-Error-in-Large-Language-Models" class="headerlink" title="Violation of Expectation via Metacognitive Prompting Reduces Theory of Mind Prediction Error in Large Language Models"></a>Violation of Expectation via Metacognitive Prompting Reduces Theory of Mind Prediction Error in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06983">http://arxiv.org/abs/2310.06983</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/plastic-labs/voe-paper-eval">https://github.com/plastic-labs/voe-paper-eval</a></li>
<li>paper_authors: Courtland Leer, Vincent Trost, Vineeth Voruganti</li>
<li>for: 本研究旨在探讨 Large Language Models (LLMs) 在理解人类心理的能力是如何提高的。</li>
<li>methods: 本研究使用了一种 Developmental psychology 中的机制 known as Violation of Expectation (VoE)，以减少 LLM 预测用户的错误。并提出了一个 \textit{metacognitive prompting} 框架来应用 VoE 在 AI 教育中。</li>
<li>results: 研究发现，通过存储和检索在 LLM 对用户预期的情况下出现的事实，LLMs 能够学习关于用户的知识。最后，研究探讨了模型用户心理的潜在危险和可能的未来研究方向。<details>
<summary>Abstract</summary>
Recent research shows that Large Language Models (LLMs) exhibit a compelling level of proficiency in Theory of Mind (ToM) tasks. This ability to impute unobservable mental states to others is vital to human social cognition and may prove equally important in principal-agent relations between individual humans and Artificial Intelligences (AIs). In this paper, we explore how a mechanism studied in developmental psychology known as Violation of Expectation (VoE) can be implemented to reduce errors in LLM prediction about users by leveraging emergent ToM affordances. And we introduce a \textit{metacognitive prompting} framework to apply VoE in the context of an AI tutor. By storing and retrieving facts derived in cases where LLM expectation about the user was violated, we find that LLMs are able to learn about users in ways that echo theories of human learning. Finally, we discuss latent hazards and augmentative opportunities associated with modeling user psychology and propose ways to mitigate risk along with possible directions for future inquiry.
</details>
<details>
<summary>摘要</summary>
现代研究显示大语言模型（LLM）在理论心理（ToM）任务中表现出吸引人的水平。这种能够推理他人隐藏的心理状态的能力是人类社交认知的核心，可能对人工智能（AI）和人之间的主体-代理关系也非常重要。在这篇论文中，我们探讨了在发展心理学中研究的违反预期（VoE）机制，以减少LLM预测用户时的错误。我们还提出了一种“认知推导”框架，用于在AI教育者中应用VoE。通过存储和重新获取在LLM预测用户时出现的情况中的事实，我们发现LLM能够通过对用户学习方式的模拟来学习用户。最后，我们讨论了模型用户心理的潜在危险和可能的发展方向，并提出了降低风险的方法。
</details></li>
</ul>
<hr>
<h2 id="Why-bother-with-geometry-On-the-relevance-of-linear-decompositions-of-Transformer-embeddings"><a href="#Why-bother-with-geometry-On-the-relevance-of-linear-decompositions-of-Transformer-embeddings" class="headerlink" title="Why bother with geometry? On the relevance of linear decompositions of Transformer embeddings"></a>Why bother with geometry? On the relevance of linear decompositions of Transformer embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06977">http://arxiv.org/abs/2310.06977</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/timotheemickus/seq2seq-splat">https://github.com/timotheemickus/seq2seq-splat</a></li>
<li>paper_authors: Timothee Mickus, Raúl Vázquez</li>
<li>for: 这个研究旨在研究Transformer嵌入的线性分解是否有实际意义。</li>
<li>methods: 这个研究使用了两种嵌入分解方法来研究机器翻译decoder的表示。</li>
<li>results: 研究结果表明，嵌入分解指标与模型性能显示正相关，但是在不同的运行中存在很大的变化，表明geometry更反映模型特有的特征而不是句子特定的计算。<details>
<summary>Abstract</summary>
A recent body of work has demonstrated that Transformer embeddings can be linearly decomposed into well-defined sums of factors, that can in turn be related to specific network inputs or components. There is however still a dearth of work studying whether these mathematical reformulations are empirically meaningful. In the present work, we study representations from machine-translation decoders using two of such embedding decomposition methods. Our results indicate that, while decomposition-derived indicators effectively correlate with model performance, variation across different runs suggests a more nuanced take on this question. The high variability of our measurements indicate that geometry reflects model-specific characteristics more than it does sentence-specific computations, and that similar training conditions do not guarantee similar vector spaces.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Jaynes-Machine-The-universal-microstructure-of-deep-neural-networks"><a href="#Jaynes-Machine-The-universal-microstructure-of-deep-neural-networks" class="headerlink" title="Jaynes Machine: The universal microstructure of deep neural networks"></a>Jaynes Machine: The universal microstructure of deep neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06960">http://arxiv.org/abs/2310.06960</a></li>
<li>repo_url: None</li>
<li>paper_authors: Venkat Venkatasubramanian, N. Sanjeevrajan, Manasi Khandekar</li>
<li>for: 这 paper 的目的是提出一种新的深度神经网络的微结构理论。</li>
<li>methods: 这 paper 使用了一种名为统计电动力学的概念总结，它是统计 термо动力学和潜在游戏理论的概念合并。这种理论预测了深度神经网络中所有高度连接层的连接强度分布为 Lognormal（$LN(\mu, \sigma)$），并且在理想条件下，$\mu$ 和 $\sigma$ 在所有层次和所有网络中都相同。这是因为所有连接在竞争和贡献效用方面达到了平衡，从而实现了总损失函数的最小化。</li>
<li>results: 这 paper 通过对六个大规模的深度神经网络实际数据进行验证，证明了这些预测的正确性。此外，这 paper 还讨论了如何利用这些结果来降低训练大深度神经网络所需的数据、时间和计算资源。<details>
<summary>Abstract</summary>
We present a novel theory of the microstructure of deep neural networks. Using a theoretical framework called statistical teleodynamics, which is a conceptual synthesis of statistical thermodynamics and potential game theory, we predict that all highly connected layers of deep neural networks have a universal microstructure of connection strengths that is distributed lognormally ($LN({\mu}, {\sigma})$). Furthermore, under ideal conditions, the theory predicts that ${\mu}$ and ${\sigma}$ are the same for all layers in all networks. This is shown to be the result of an arbitrage equilibrium where all connections compete and contribute the same effective utility towards the minimization of the overall loss function. These surprising predictions are shown to be supported by empirical data from six large-scale deep neural networks in real life. We also discuss how these results can be exploited to reduce the amount of data, time, and computational resources needed to train large deep neural networks.
</details>
<details>
<summary>摘要</summary>
我团队提出了一种新的深度神经网络微结构理论。使用一种名为统计电动力学的理论框架，这是统计 термодинами学和潜在游戏理论的概念合成。我们预测了所有深度神经网络中高度连接层的微结构强度分布为Lognormal（$LN(\mu, \sigma)$）。此外，在理想情况下，我们预测$\mu$和$\sigma$在所有层次和所有网络中都相同。这是因为所有连接都在竞争和贡献同样的有效利用于最小化总损失函数。这些意外预测得到了实际数据中6个大规模深度神经网络的支持。我们还讨论了如何利用这些结果减少训练大深度神经网络所需的数据、时间和计算资源。
</details></li>
</ul>
<hr>
<h2 id="Creation-Of-A-ChatBot-Based-On-Natural-Language-Proccesing-For-Whatsapp"><a href="#Creation-Of-A-ChatBot-Based-On-Natural-Language-Proccesing-For-Whatsapp" class="headerlink" title="Creation Of A ChatBot Based On Natural Language Proccesing For Whatsapp"></a>Creation Of A ChatBot Based On Natural Language Proccesing For Whatsapp</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10675">http://arxiv.org/abs/2310.10675</a></li>
<li>repo_url: None</li>
<li>paper_authors: Valderrama Jonatan, Aguilar-Alonso Igor</li>
<li>for: 提高客户满意度和公司服务质量 through WhatsApp chatbot</li>
<li>methods: 基于自然语言处理的 chatbot 开发</li>
<li>results: 实现快速和准确的回答，提高客户服务效率和客户满意度Here’s the simplified Chinese text for each point:</li>
<li>for: 通过 WhatsApp chatbot 提高客户满意度和公司服务质量</li>
<li>methods: 基于自然语言处理的 chatbot 开发</li>
<li>results: 实现快速和准确的回答，提高客户服务效率和客户满意度<details>
<summary>Abstract</summary>
In the era of digital transformation, customer service is of paramount importance to the success of organizations, and to meet the growing demand for immediate responses and personalized assistance 24 hours a day, chatbots have become a promising tool to solve these problems. Currently, there are many companies that need to provide these solutions to their customers, which motivates us to study this problem and offer a suitable solution. The objective of this study is to develop a chatbot based on natural language processing to improve customer satisfaction and improve the quality of service provided by the company through WhatsApp. The solution focuses on creating a chatbot that efficiently and effectively handles user queries. A literature review related to existing chatbots has been conducted, analyzing methodological approaches, artificial intelligence techniques and quality attributes used in the implementation of chatbots. The results found highlight that chatbots based on natural language processing enable fast and accurate responses, which improves the efficiency of customer service, as chatbots contribute to customer satisfaction by providing accurate answers and quick solutions to their queries at any time. Some authors point out that artificial intelligence techniques, such as machine learning, improve the learning and adaptability of chatbots as user interactions occur, so a good choice of appropriate natural language understanding technologies is essential for optimal chatbot performance. The results of this study will provide a solid foundation for the design and development of effective chatbots for customer service, ensuring a satisfactory user experience and thus meeting the needs of the organization.
</details>
<details>
<summary>摘要</summary>
在数字化转型时代，客户服务对组织的成功非常重要，为了应对增长的快速响应和个性化帮助需求，聊天机器人已成为一种有前途的解决方案。目前有很多公司需要为客户提供这些解决方案，这使我们感到需要研究这个问题并提供适合的解决方案。本研究的目标是开发基于自然语言处理的聊天机器人，以提高客户满意度和公司向客户提供的服务质量。解决方案关注于创建高效高质量的聊天机器人，以快速和准确地处理用户查询。在现有聊天机器人的研究中，我们进行了文献综述，分析了方法ológicas approached,人工智能技术和质量特征在聊天机器人的实施中使用。结果显示，基于自然语言处理的聊天机器人可以快速和准确地回答用户查询，从而提高客户服务的效率，因为聊天机器人可以为客户提供快速和准确的答案，使用户满意度提高。一些作者指出，人工智能技术，如机器学习，可以使聊天机器人在用户互动时进行学习和适应，因此选择合适的自然语言理解技术是聊天机器人性能优化的关键。本研究的结果将为聊天机器人的设计和开发提供坚实的基础，确保用户体验满意，从而满足组织的需求。
</details></li>
</ul>
<hr>
<h2 id="Document-Level-Supervision-for-Multi-Aspect-Sentiment-Analysis-Without-Fine-grained-Labels"><a href="#Document-Level-Supervision-for-Multi-Aspect-Sentiment-Analysis-Without-Fine-grained-Labels" class="headerlink" title="Document-Level Supervision for Multi-Aspect Sentiment Analysis Without Fine-grained Labels"></a>Document-Level Supervision for Multi-Aspect Sentiment Analysis Without Fine-grained Labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06940">http://arxiv.org/abs/2310.06940</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kasturi Bhattacharjee, Rashmi Gangadharaiah</li>
<li>for: This paper proposes a VAE-based topic modeling approach for aspect-based sentiment analysis (ABSA) that does not require fine-grained labels for aspects or sentiments.</li>
<li>methods: The proposed approach uses document-level supervision and leverages user-generated text with overall sentiment to detect multiple aspects in a document and reason about their contributions to the overall sentiment.</li>
<li>results: The approach significantly outperforms a state-of-the-art baseline on two benchmark datasets from different domains.Here’s the text in Simplified Chinese:</li>
<li>for: 这篇论文提出了一种基于VAE的话题模型方法，用于无监督的方面情感分析（ABSA），不需要细化的标签 для方面或情感。</li>
<li>methods: 该方法使用文档级别的监督，利用用户生成的文本中的总情感来探测文档中的多个方面，并将这些方面的情感相互综合来理解整个文档的情感。</li>
<li>results: 该方法在两个不同领域的两个标准 benchmark 数据集上显著超越了一个状态监督的基准。<details>
<summary>Abstract</summary>
Aspect-based sentiment analysis (ABSA) is a widely studied topic, most often trained through supervision from human annotations of opinionated texts. These fine-grained annotations include identifying aspects towards which a user expresses their sentiment, and their associated polarities (aspect-based sentiments). Such fine-grained annotations can be expensive and often infeasible to obtain in real-world settings. There is, however, an abundance of scenarios where user-generated text contains an overall sentiment, such as a rating of 1-5 in user reviews or user-generated feedback, which may be leveraged for this task. In this paper, we propose a VAE-based topic modeling approach that performs ABSA using document-level supervision and without requiring fine-grained labels for either aspects or sentiments. Our approach allows for the detection of multiple aspects in a document, thereby allowing for the possibility of reasoning about how sentiment expressed through multiple aspects comes together to form an observable overall document-level sentiment. We demonstrate results on two benchmark datasets from two different domains, significantly outperforming a state-of-the-art baseline.
</details>
<details>
<summary>摘要</summary>
《方面基于情感分析（ABSA）是一个广泛研究的话题，通常通过人类注释的意见文本进行培育。这些细化的注释包括确定用户表达情感的方面以及其相关的负面性（方面基于情感）。然而，在实际场景中获得这些细化注释可能是昂贵的和不可能完成的。在这篇论文中，我们提出了基于VAE的话题模型方法，用于实现ABSA，不需要文本级别的细化标注，也不需要方面或情感的细化标注。我们的方法允许文档中检测多个方面，从而允许理解多个方面的情感如何共同形成可见的总文档级别的情感。我们在两个不同领域的两个标准 benchmark 数据集上进行了实验，并在比较一个基eline之下显著地提高了性能。》Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format instead.
</details></li>
</ul>
<hr>
<h2 id="Improving-Contrastive-Learning-of-Sentence-Embeddings-with-Focal-InfoNCE"><a href="#Improving-Contrastive-Learning-of-Sentence-Embeddings-with-Focal-InfoNCE" class="headerlink" title="Improving Contrastive Learning of Sentence Embeddings with Focal-InfoNCE"></a>Improving Contrastive Learning of Sentence Embeddings with Focal-InfoNCE</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06918">http://arxiv.org/abs/2310.06918</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/puerrrr/focal-infonce">https://github.com/puerrrr/focal-infonce</a></li>
<li>paper_authors: Pengyue Hou, Xingyu Li</li>
<li>for: 提高句子表示的质量</li>
<li>methods:  combinest SimCSE 和 hard negative mining， introduce self-paced modulation terms in the contrastive objective</li>
<li>results: 改进句子表示的Spearman correlation和Alignment和Uniformity<details>
<summary>Abstract</summary>
The recent success of SimCSE has greatly advanced state-of-the-art sentence representations. However, the original formulation of SimCSE does not fully exploit the potential of hard negative samples in contrastive learning. This study introduces an unsupervised contrastive learning framework that combines SimCSE with hard negative mining, aiming to enhance the quality of sentence embeddings. The proposed focal-InfoNCE function introduces self-paced modulation terms in the contrastive objective, downweighting the loss associated with easy negatives and encouraging the model focusing on hard negatives. Experimentation on various STS benchmarks shows that our method improves sentence embeddings in terms of Spearman's correlation and representation alignment and uniformity.
</details>
<details>
<summary>摘要</summary>
最近，SimCSE的成功有效地提高了现代句子表示的状态艺。然而，原始的SimCSE формулировция并没有充分利用强有力的负样本在对比学习中的潜力。本研究提出了一种无监督对比学习框架，将SimCSE与强负样本挖掘结合起来，以提高句子嵌入的质量。我们提出的自适应InfoNCE函数在对比目标中添加了自适应调整项，将易于获得的负样本下Weight，让模型更加注重困难的负样本。经过实验表明，我们的方法可以提高句子嵌入的斯宾森相关度和表示对应性和一致性。
</details></li>
</ul>
<hr>
<h2 id="A-Comparative-Study-of-Transformer-based-Neural-Text-Representation-Techniques-on-Bug-Triaging"><a href="#A-Comparative-Study-of-Transformer-based-Neural-Text-Representation-Techniques-on-Bug-Triaging" class="headerlink" title="A Comparative Study of Transformer-based Neural Text Representation Techniques on Bug Triaging"></a>A Comparative Study of Transformer-based Neural Text Representation Techniques on Bug Triaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06913">http://arxiv.org/abs/2310.06913</a></li>
<li>repo_url: None</li>
<li>paper_authors: Atish Kumar Dipongkor, Kevin Moran</li>
<li>for: 本研究旨在自动化漏洞报告的三个步骤：识别开发者和组件，地方化漏洞，和修复漏洞。</li>
<li>methods: 本研究使用 transformer-based 语言模型进行自动化漏洞报告的任务，包括 DeBERTa 等多种方法。</li>
<li>results: 研究发现，DeBERTa 是最有效的方法，在开发者和组件归属和漏洞地方化等三个任务中具有 statistically significant 的表现优势。但是，每种方法都有其特点和优势，适用于不同类型的漏洞报告。<details>
<summary>Abstract</summary>
Often, the first step in managing bug reports is related to triaging a bug to the appropriate developer who is best suited to understand, localize, and fix the target bug. Additionally, assigning a given bug to a particular part of a software project can help to expedite the fixing process. However, despite the importance of these activities, they are quite challenging, where days can be spent on the manual triaging process. Past studies have attempted to leverage the limited textual data of bug reports to train text classification models that automate this process -- to varying degrees of success. However, the textual representations and machine learning models used in prior work are limited by their expressiveness, often failing to capture nuanced textual patterns that might otherwise aid in the triaging process. Recently, large, transformer-based, pre-trained neural text representation techniques such as BERT have achieved greater performance in several natural language processing tasks. However, the potential for using these techniques to improve upon prior approaches for automated bug triaging is not well studied or understood.   Therefore, in this paper we offer one of the first investigations that fine-tunes transformer-based language models for the task of bug triaging on four open source datasets, spanning a collective 53 years of development history with over 400 developers and over 150 software project components. Our study includes both a quantitative and qualitative analysis of effectiveness. Our findings illustrate that DeBERTa is the most effective technique across the triaging tasks of developer and component assignment, and the measured performance delta is statistically significant compared to other techniques. However, through our qualitative analysis, we also observe that each technique possesses unique abilities best suited to certain types of bug reports.
</details>
<details>
<summary>摘要</summary>
通常，处理bug报告的第一步是将bug分配到适合的开发者，以便他们能够更好地理解、本地化和修复目标bug。此外，将bug分配到特定的软件项目部分也可以帮助加速修复过程。然而，这些活动具有挑战性，可能需要数天的手动分配过程。过去的研究已经尝试使用bug报告的有限文本数据来训练文本分类模型，以便自动进行这些活动——尽管效果不一。然而，这些表达和机器学习模型在先前的工作中有限，常常无法捕捉bug报告中细腻的文本模式，这可能会帮助分配过程。最近，大型的transformer-based大型预训练神经网络模型，如BERT，在自然语言处理任务中已经达到了更高的性能。然而，使用这些技术来改进先前的自动分配策略的可能性并不很了解或研究。因此，在这篇论文中，我们提供了一个由BERT等大型神经网络模型进行微调的首次研究，用于在四个开源数据集上进行自动分配。我们的研究包括量化和质量分析的效果分析。我们的发现表明，DeBERTa是分配任务中最有效的技术，并且与其他技术的性能差异是统计学上有意义的。然而，我们的质量分析也表明，每种技术都具有特定的优势，适合某些类型的bug报告。
</details></li>
</ul>
<hr>
<h2 id="LongLLMLingua-Accelerating-and-Enhancing-LLMs-in-Long-Context-Scenarios-via-Prompt-Compression"><a href="#LongLLMLingua-Accelerating-and-Enhancing-LLMs-in-Long-Context-Scenarios-via-Prompt-Compression" class="headerlink" title="LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression"></a>LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06839">http://arxiv.org/abs/2310.06839</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/LLMLingua">https://github.com/microsoft/LLMLingua</a></li>
<li>paper_authors: Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, Lili Qiu</li>
<li>for: 提高大语言模型（LLM）的计算成本、财务成本和响应时间，以及提高LLM在长文本场景下的性能。</li>
<li>methods: 提出了一种名为LongLLMLingua的提示压缩方法，通过提高LLM对关键信息的感知，同时解决了上述三个挑战。</li>
<li>results: 在单文检索、多文检索、简要摘要、 sintetic 任务和代码完成任务等长文本场景中，LongLLMLingua 可以 deriv 更高的性能，并降低了终端系统的响应时间。例如，在 NaturalQuestions  bencmark 上，LongLLMLingua 可以提高 GPT-3.5-Turbo 的性能 by 17.1%，并且只需要输入 ~4x  fewer tokens。此外，LongLLMLingua 可以在压缩提示的情况下，提高终端系统的响应速度。<details>
<summary>Abstract</summary>
In long context scenarios, large language models (LLMs) face three main challenges: higher computational/financial cost, longer latency, and inferior performance. Some studies reveal that the performance of LLMs depends on both the density and the position of the key information (question relevant) in the input prompt. Inspired by these findings, we propose LongLLMLingua for prompt compression towards improving LLMs' perception of the key information to simultaneously address the three challenges. We conduct evaluation on a wide range of long context scenarios including single-/multi-document QA, few-shot learning, summarization, synthetic tasks, and code completion. The experimental results show that LongLLMLingua compressed prompt can derive higher performance with much less cost. The latency of the end-to-end system is also reduced. For example, on NaturalQuestions benchmark, LongLLMLingua gains a performance boost of up to 17.1% over the original prompt with ~4x fewer tokens as input to GPT-3.5-Turbo. It can derive cost savings of \$28.5 and \$27.4 per 1,000 samples from the LongBench and ZeroScrolls benchmark, respectively. Additionally, when compressing prompts of ~10k tokens at a compression rate of 2x-10x, LongLLMLingua can speed up the end-to-end latency by 1.4x-3.8x. Our code is available at https://aka.ms/LLMLingua.
</details>
<details>
<summary>摘要</summary>
受长文本场景限制，大语言模型（LLM）面临三大挑战：更高的计算/金融成本、更长的延迟时间和较差的性能。一些研究表明，LLM的性能与输入提示中关键信息的密度和位置有关。以这些发现为灵感，我们提出了LongLLMLingua，用于提取提示中关键信息，以同时解决这三个挑战。我们在单/多文档问答、几拍学习、概要、人工任务和代码完成等多种长文本场景进行评估。实验结果表明，LongLLMLingua压缩后的提示可以提高性能，并且减少了终端系统的延迟时间。例如，在NaturalQuestionsBenchmark上，LongLLMLingua可以在GPT-3.5-Turbo上提高性能，并且只需输入4x少于原始提示的token数量。此外，当压缩提示长度为10k字时，LongLLMLingua可以将终端系统的延迟时间加速1.4x-3.8x。我们的代码可以在https://aka.ms/LLMLingua上下载。
</details></li>
</ul>
<hr>
<h2 id="Generating-and-Evaluating-Tests-for-K-12-Students-with-Language-Model-Simulations-A-Case-Study-on-Sentence-Reading-Efficiency"><a href="#Generating-and-Evaluating-Tests-for-K-12-Students-with-Language-Model-Simulations-A-Case-Study-on-Sentence-Reading-Efficiency" class="headerlink" title="Generating and Evaluating Tests for K-12 Students with Language Model Simulations: A Case Study on Sentence Reading Efficiency"></a>Generating and Evaluating Tests for K-12 Students with Language Model Simulations: A Case Study on Sentence Reading Efficiency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06837">http://arxiv.org/abs/2310.06837</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eric Zelikman, Wanjing Anya Ma, Jasmine E. Tran, Diyi Yang, Jason D. Yeatman, Nick Haber</li>
<li>for: 这个论文的目的是为了开发一种高质量的同时测试，以便更好地评估学生的阅读能力。</li>
<li>methods: 这个论文使用了大型自然语言模型（LLM）来模拟之前学生对未看过的题目的回答，以估计每个题目的难度和抽象程度。</li>
<li>results: 该论文使用GPT-4生成新的测试项，并使用精度调整后的LLM来筛选符合心理测量标准的题目。 results show that the generated test scores are highly correlated (r&#x3D;0.93) with those of a standard test form written by human experts, and the generated tests closely correspond to the original test’s difficulty and reliability based on crowdworker responses.<details>
<summary>Abstract</summary>
Developing an educational test can be expensive and time-consuming, as each item must be written by experts and then evaluated by collecting hundreds of student responses. Moreover, many tests require multiple distinct sets of questions administered throughout the school year to closely monitor students' progress, known as parallel tests. In this study, we focus on tests of silent sentence reading efficiency, used to assess students' reading ability over time. To generate high-quality parallel tests, we propose to fine-tune large language models (LLMs) to simulate how previous students would have responded to unseen items. With these simulated responses, we can estimate each item's difficulty and ambiguity. We first use GPT-4 to generate new test items following a list of expert-developed rules and then apply a fine-tuned LLM to filter the items based on criteria from psychological measurements. We also propose an optimal-transport-inspired technique for generating parallel tests and show the generated tests closely correspond to the original test's difficulty and reliability based on crowdworker responses. Our evaluation of a generated test with 234 students from grades 2 to 8 produces test scores highly correlated (r=0.93) to those of a standard test form written by human experts and evaluated across thousands of K-12 students.
</details>
<details>
<summary>摘要</summary>
开发教育测试可能会很昂贵和时间consuming，因为每个项目都需要由专家写作并由数百名学生回答。此外，许多测试需要在学年中多次进行测试，以便密切监测学生的进步，这种测试被称为平行测试。在这项研究中，我们关注 silent sentence reading efficiency 测试，用于评估学生的阅读能力。为生成高质量平行测试，我们提议使用大型自然语言模型（LLM）来模拟以前学生对未看过的问题的回答。通过这些模拟回答，我们可以估算每个问题的难度和抽象性。我们首先使用 GPT-4 生成新的测试项目，并应用一个精度调整的 LLM 来过滤测试项目基于心理测量的标准。我们还提出一种基于最优运输的技术来生成平行测试，并证明生成的测试与原始测试的难度和可靠性具有高度相似性。我们对234名二至八年级学生进行评估，得到的测试分数与由人类专家编写的标准测试形式相高度相关（r=0.93）。
</details></li>
</ul>
<hr>
<h2 id="Lemur-Harmonizing-Natural-Language-and-Code-for-Language-Agents"><a href="#Lemur-Harmonizing-Natural-Language-and-Code-for-Language-Agents" class="headerlink" title="Lemur: Harmonizing Natural Language and Code for Language Agents"></a>Lemur: Harmonizing Natural Language and Code for Language Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06830">http://arxiv.org/abs/2310.06830</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/openlemur/lemur">https://github.com/openlemur/lemur</a></li>
<li>paper_authors: Yiheng Xu, Hongjin Su, Chen Xing, Boyu Mi, Qian Liu, Weijia Shi, Binyuan Hui, Fan Zhou, Yitao Liu, Tianbao Xie, Zhoujun Cheng, Siheng Zhao, Lingpeng Kong, Bailin Wang, Caiming Xiong, Tao Yu</li>
<li>For: 本研究开发了一个名为Lemur和Lemur-Chat的开源语言模型，用于实现多元语言代理人。* Methods: 研究人员使用了一个代码数据集进行谨慎预训，并对文本和程式码数据进行微调。* Results: 研究人员通过实验发现，Lemur和Lemur-Chat可以在多种环境中实现高水平的表现，并且与商业化模型相比，它们在代理人能力方面表现更为出色。<details>
<summary>Abstract</summary>
We introduce Lemur and Lemur-Chat, openly accessible language models optimized for both natural language and coding capabilities to serve as the backbone of versatile language agents. The evolution from language chat models to functional language agents demands that models not only master human interaction, reasoning, and planning but also ensure grounding in the relevant environments. This calls for a harmonious blend of language and coding capabilities in the models. Lemur and Lemur-Chat are proposed to address this necessity, demonstrating balanced proficiencies in both domains, unlike existing open-source models that tend to specialize in either. Through meticulous pre-training using a code-intensive corpus and instruction fine-tuning on text and code data, our models achieve state-of-the-art averaged performance across diverse text and coding benchmarks among open-source models. Comprehensive experiments demonstrate Lemur's superiority over existing open-source models and its proficiency across various agent tasks involving human communication, tool usage, and interaction under fully- and partially- observable environments. The harmonization between natural and programming languages enables Lemur-Chat to significantly narrow the gap with proprietary models on agent abilities, providing key insights into developing advanced open-source agents adept at reasoning, planning, and operating seamlessly across environments. https://github.com/OpenLemur/Lemur
</details>
<details>
<summary>摘要</summary>
我们介绍Lemur和Lemur-Chat，是一 pair of开源语言模型，旨在扩展语言和程式码之间的共同能力，以便建立多元化的语言代理。从语言交流模型演化为功能性语言代理需要模型不仅掌握人类互动、推理和观念，而且还需要与环境相互融合。这需要模型同时具备语言和程式码的能力。Lemur和Lemur-Chat被提议以应对这个需求，并在多个语言和程式码benchmark测试中表现出色。我们通过精心预训使用一个具有程式码的资料集，以及对文本和程式码数据进行精确调整，使我们的模型在开源模型中表现出积极的平均性能。实验结果显示Lemur在开源模型中表现出色，并且在不同的代理任务中具备广泛的能力，包括人类交流、工具使用和受完全和受限 Observable 环境中的互动。通过自然语言和程式码之间的融合，Lemur-Chat可以对Proprietary模型的代理能力进行明显的缩小，提供关键的意见，以帮助开发高水准的开源代理，能够快速推理、规划和在不同环境中顺畅运行。更多资讯可以在GitHub上找到：https://github.com/OpenLemur/Lemur
</details></li>
</ul>
<hr>
<h2 id="Teaching-Language-Models-to-Hallucinate-Less-with-Synthetic-Tasks"><a href="#Teaching-Language-Models-to-Hallucinate-Less-with-Synthetic-Tasks" class="headerlink" title="Teaching Language Models to Hallucinate Less with Synthetic Tasks"></a>Teaching Language Models to Hallucinate Less with Synthetic Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06827">http://arxiv.org/abs/2310.06827</a></li>
<li>repo_url: None</li>
<li>paper_authors: Erik Jones, Hamid Palangi, Clarisse Simões, Varun Chandrasekaran, Subhabrata Mukherjee, Arindam Mitra, Ahmed Awadallah, Ece Kamar</li>
<li>for: 本研究旨在提高大型自然语言模型（LLM）在抽象摘要任务上减少幻觉，以提高模型在实际任务上的表现。</li>
<li>methods: 本研究使用一种名为SynTra的方法，首先在一个 sintetic task 上设计了一个易于诱发和测量幻觉的任务，然后使用这个任务进行预 fixing  LLM 的系统消息，最后将系统消息应用到实际的摘要任务上。</li>
<li>results: 在三个实际的摘要任务上，SynTra 能够减少两个 13B 参数的 LLM 的幻觉。此外，研究还发现，在 synthetic task 上优化系统消息比优化模型参数更加重要，而 fine-tuning 整个模型在 synthetic task 上可能会增加幻觉。<details>
<summary>Abstract</summary>
Large language models (LLMs) frequently hallucinate on abstractive summarization tasks such as document-based question-answering, meeting summarization, and clinical report generation, even though all necessary information is included in context. However, optimizing LLMs to hallucinate less on these tasks is challenging, as hallucination is hard to efficiently evaluate at each optimization step. In this work, we show that reducing hallucination on a synthetic task can also reduce hallucination on real-world downstream tasks. Our method, SynTra, first designs a synthetic task where hallucinations are easy to elicit and measure. It next optimizes the LLM's system message via prefix-tuning on the synthetic task, and finally transfers the system message to realistic, hard-to-optimize tasks. Across three realistic abstractive summarization tasks, SynTra reduces hallucination for two 13B-parameter LLMs using only a synthetic retrieval task for supervision. We also find that optimizing the system message rather than the model weights can be critical; fine-tuning the entire model on the synthetic task can counterintuitively increase hallucination. Overall, SynTra demonstrates that the extra flexibility of working with synthetic data can help mitigate undesired behaviors in practice.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在抽象摘要化任务中常常会出现幻想，例如文档问答、会议摘要和医疗报告生成，即使所有必要的信息都包含在 контек斯中。但是，对 LLM 进行幻想调整是困难的，因为幻想难以在每个优化步骤中有效评估。在这个工作中，我们显示了将幻想降低在 sintetic 任务上可以降低实际世界下渠道任务中的幻想。我们的方法 SynTra 首先设计了 sintetic 任务，可以轻松诱发和评估幻想。然后，SynTra 透过 prefix-tuning 优化 LLM 的系统讯息，最后将系统讯息转换到实际、difficult-to-optimize 任务上。在三个实际抽象摘要化任务中，SynTra 可以降低两个 13B 参数 LLM 的幻想。我们还发现，对系统讯息进行优化可以是关键的；精确地调整整个模型的参数可能会增加幻想。总的来说，SynTra 显示了使用 sintetic 数据可以帮助解决实际中的问题。
</details></li>
</ul>
<hr>
<h2 id="Text-Embeddings-Reveal-Almost-As-Much-As-Text"><a href="#Text-Embeddings-Reveal-Almost-As-Much-As-Text" class="headerlink" title="Text Embeddings Reveal (Almost) As Much As Text"></a>Text Embeddings Reveal (Almost) As Much As Text</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06816">http://arxiv.org/abs/2310.06816</a></li>
<li>repo_url: None</li>
<li>paper_authors: John X. Morris, Volodymyr Kuleshov, Vitaly Shmatikov, Alexander M. Rush</li>
<li>for:  investigate the problem of embedding inversion, reconstructing the full text represented in dense text embeddings.</li>
<li>methods:  frame the problem as controlled generation: generating text that, when reembedded, is close to a fixed point in latent space.</li>
<li>results:  recover $92%$ of $32\text{-token}$ text inputs exactly using a multi-step method that iteratively corrects and re-embeds text.<details>
<summary>Abstract</summary>
How much private information do text embeddings reveal about the original text? We investigate the problem of embedding \textit{inversion}, reconstructing the full text represented in dense text embeddings. We frame the problem as controlled generation: generating text that, when reembedded, is close to a fixed point in latent space. We find that although a na\"ive model conditioned on the embedding performs poorly, a multi-step method that iteratively corrects and re-embeds text is able to recover $92\%$ of $32\text{-token}$ text inputs exactly. We train our model to decode text embeddings from two state-of-the-art embedding models, and also show that our model can recover important personal information (full names) from a dataset of clinical notes. Our code is available on Github: \href{https://github.com/jxmorris12/vec2text}{github.com/jxmorris12/vec2text}.
</details>
<details>
<summary>摘要</summary>
TEXT我们研究了文本嵌入的私人信息泄露问题，具体来说是文本嵌入的反推问题，即通过 dense text embeddings 中的点来恢复原始文本。我们将问题定义为控制生成问题，即生成文本，其重新嵌入后与给定点在嵌入空间很近。我们发现，直接使用嵌入模型conditioned的模型表现不佳，但是通过Iteratively Correct and Re-Embed Text（ICRT）方法，可以准确地恢复 $92\%$ 的 $32$-token 文本输入。我们使用两种现状顶尖嵌入模型来训练我们的模型，并示出我们的模型可以从医疗笔记中提取重要的个人信息（全名）。我们的代码可以在 Github 上找到：https://github.com/jxmorris12/vec2text。Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Uni3D-Exploring-Unified-3D-Representation-at-Scale"><a href="#Uni3D-Exploring-Unified-3D-Representation-at-Scale" class="headerlink" title="Uni3D: Exploring Unified 3D Representation at Scale"></a>Uni3D: Exploring Unified 3D Representation at Scale</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06773">http://arxiv.org/abs/2310.06773</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/baaivision/uni3d">https://github.com/baaivision/uni3d</a></li>
<li>paper_authors: Junsheng Zhou, Jinsheng Wang, Baorui Ma, Yu-Shen Liu, Tiejun Huang, Xinlong Wang</li>
<li>For: 本研究旨在探讨3D对象和场景的扩大表示，以探索3D世界中的一元表示。* Methods: 本研究使用2D初始化的ViT端到终推理，将3D点云特征与图像文本对齐。通过简单的architecture和预测任务，Uni3D可以利用丰富的2D预测模型和图像文本对齐模型作为初始化，从而解锁2D模型和扩大策略在3D世界中的潜力。* Results: 我们效率地扩大Uni3D到一亿个参数，并在广泛的3D任务中设置新的纪录，如零shot分类、少shot分类、开放世界理解和部分 segmentation。我们还示出Uni3D的强大表示能够应用于3D绘制和 Retrieval in the wild。我们认为Uni3D提供了一个新的方向，用于探索3D表示的扩大和效率。<details>
<summary>Abstract</summary>
Scaling up representations for images or text has been extensively investigated in the past few years and has led to revolutions in learning vision and language. However, scalable representation for 3D objects and scenes is relatively unexplored. In this work, we present Uni3D, a 3D foundation model to explore the unified 3D representation at scale. Uni3D uses a 2D initialized ViT end-to-end pretrained to align the 3D point cloud features with the image-text aligned features. Via the simple architecture and pretext task, Uni3D can leverage abundant 2D pretrained models as initialization and image-text aligned models as the target, unlocking the great potential of 2D models and scaling-up strategies to the 3D world. We efficiently scale up Uni3D to one billion parameters, and set new records on a broad range of 3D tasks, such as zero-shot classification, few-shot classification, open-world understanding and part segmentation. We show that the strong Uni3D representation also enables applications such as 3D painting and retrieval in the wild. We believe that Uni3D provides a new direction for exploring both scaling up and efficiency of the representation in 3D domain.
</details>
<details>
<summary>摘要</summary>
压缩表示法在图像或文本领域已经得到了广泛的研究，并导致了视觉和语言学习领域的革命。然而，对于3D对象和场景的可扩展表示仍然相对未经探索。在这个工作中，我们提出了Uni3D，一个用于探索可扩展3D表示的基础模型。Uni3D使用一个初始化为2D的ViT结构，通过对3D点云特征与图像和文本对齐的方式进行预training，以获得一个协调的3D表示。通过简单的建筑和预text任务，Uni3D可以利用丰富的2D预训练模型和图像和文本对齐的模型作为目标，从而解锁2D模型和扩展策略在3D世界的潜力。我们效率地扩展Uni3D到一亿个参数，并在广泛的3D任务上设置新的纪录，如零shot分类、几shot分类、开放世界理解和部分 segmentation。我们显示Uni3D表示也可以应用于3D涂鸦和野外检索。我们认为Uni3D提供了一个新的方向，用于探索3D领域中的表示扩展和效率。
</details></li>
</ul>
<hr>
<h2 id="OmniLingo-Listening-and-speaking-based-language-learning"><a href="#OmniLingo-Listening-and-speaking-based-language-learning" class="headerlink" title="OmniLingo: Listening- and speaking-based language learning"></a>OmniLingo: Listening- and speaking-based language learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06764">http://arxiv.org/abs/2310.06764</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francis M. Tyers, Nicholas Howell</li>
<li>for: 这篇论文旨在提供一种分布数据架构和应用示例，用于语言学习应用程序中的听说学习。</li>
<li>methods: 该架构基于Interplanetary Filesystem（IPFS），强调用户主权 над数据。</li>
<li>results: 论文提供了一个基于IPFS的分布数据架构和一个示例客户端，用于支持语言学习应用程序的听说学习。<details>
<summary>Abstract</summary>
In this demo paper we present OmniLingo, an architecture for distributing data for listening- and speaking-based language learning applications and a demonstration client built using the architecture. The architecture is based on the Interplanetary Filesystem (IPFS) and puts at the forefront user sovereignty over data.
</details>
<details>
<summary>摘要</summary>
在这份 demo 纸上，我们介绍 OmniLingo，一种分布式数据架构，用于语音和语言学习应用程序，以及一个基于 Interplanetary Filesystem (IPFS) 的示例客户端。这种架构强调用户主权 над数据。Here's a breakdown of the text:* "在这份 demo 纸上" (在这份 demo 纸上) - This phrase is used to indicate that the topic being discussed is a demo or a sample.* "我们介绍 OmniLingo" (我们介绍 OmniLingo) - This phrase introduces the topic of the discussion, which is OmniLingo.* "一种分布式数据架构" (一种分布式数据架构) - This phrase describes OmniLingo as a distributed data architecture.* "用于语音和语言学习应用程序" (用于语音和语言学习应用程序) - This phrase explains the purpose of OmniLingo, which is to support listening- and speaking-based language learning applications.* "以及一个基于 Interplanetary Filesystem (IPFS) 的示例客户端" (以及一个基于 Interplanetary Filesystem (IPFS) 的示例客户端) - This phrase provides more information about OmniLingo, specifically that it is based on the Interplanetary Filesystem (IPFS) and includes a demonstration client.* "这种架构强调用户主权 над数据" (这种架构强调用户主权 над数据) - This phrase emphasizes the importance of user sovereignty over data in the OmniLingo architecture.
</details></li>
</ul>
<hr>
<h2 id="TRACE-A-Comprehensive-Benchmark-for-Continual-Learning-in-Large-Language-Models"><a href="#TRACE-A-Comprehensive-Benchmark-for-Continual-Learning-in-Large-Language-Models" class="headerlink" title="TRACE: A Comprehensive Benchmark for Continual Learning in Large Language Models"></a>TRACE: A Comprehensive Benchmark for Continual Learning in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06762">http://arxiv.org/abs/2310.06762</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/beyonderxx/trace">https://github.com/beyonderxx/trace</a></li>
<li>paper_authors: Xiao Wang, Yuansen Zhang, Tianze Chen, Songyang Gao, Senjie Jin, Xianjun Yang, Zhiheng Xi, Rui Zheng, Yicheng Zou, Tao Gui, Qi Zhang, Xuanjing Huang</li>
<li>for: 本研究旨在评估已经aligned的大型语言模型（LLMs）在连续学习中的能力。</li>
<li>methods: 本研究使用了一个新的benchmark方法 named TRACE，包括8个不同的数据集，涵盖域专业任务、多语言能力、代码生成和数学逻辑等多种挑战任务。</li>
<li>results: 实验结果表明，在TRACE数据集上训练后，已经aligned的LLMs呈现了显著的普通能力和指令遵循能力下降。例如，llama2-chat 13B在gsm8k数据集上的准确率从28.8%降至2%。这表明需要找到一个适合的权衡，以确保实现特定任务的表现，而不会导致LLMs的原始能力减退。<details>
<summary>Abstract</summary>
Aligned large language models (LLMs) demonstrate exceptional capabilities in task-solving, following instructions, and ensuring safety. However, the continual learning aspect of these aligned LLMs has been largely overlooked. Existing continual learning benchmarks lack sufficient challenge for leading aligned LLMs, owing to both their simplicity and the models' potential exposure during instruction tuning. In this paper, we introduce TRACE, a novel benchmark designed to evaluate continual learning in LLMs. TRACE consists of 8 distinct datasets spanning challenging tasks including domain-specific tasks, multilingual capabilities, code generation, and mathematical reasoning. All datasets are standardized into a unified format, allowing for effortless automatic evaluation of LLMs. Our experiments show that after training on TRACE, aligned LLMs exhibit significant declines in both general ability and instruction-following capabilities. For example, the accuracy of llama2-chat 13B on gsm8k dataset declined precipitously from 28.8\% to 2\% after training on our datasets. This highlights the challenge of finding a suitable tradeoff between achieving performance on specific tasks while preserving the original prowess of LLMs. Empirical findings suggest that tasks inherently equipped with reasoning paths contribute significantly to preserving certain capabilities of LLMs against potential declines. Motivated by this, we introduce the Reasoning-augmented Continual Learning (RCL) approach. RCL integrates task-specific cues with meta-rationales, effectively reducing catastrophic forgetting in LLMs while expediting convergence on novel tasks.
</details>
<details>
<summary>摘要</summary>
aligned large language models (LLMs) 表现出色地解决任务、遵循指令和保持安全。然而，这些aligned LLMs的持续学习方面尚未得到充分的注意。现有的持续学习标准benchmarklacks sufficient challenge for leading aligned LLMs, owing to both their simplicity and the models' potential exposure during instruction tuning.在这篇论文中，我们介绍TRACE，一个新的benchmark，用于评估LLMs的持续学习能力。TRACE包括8个不同的数据集，涵盖域специфи任务、多语言能力、代码生成和数学逻辑推理。所有数据集都是 стандар化了，以便自动评估LLMs。我们的实验表明，在TRACE上训练后，aligned LLMs的总能力和遵循指令能力都会显著下降。例如，llama2-chat 13B在gsm8k数据集上的准确率从28.8%下降到2%。这说明了在寻找适当的任务和原始模型能力之间的权衡是一个挑战。我们的实验结果表明，具有逻辑路径的任务可以帮助保持LLMs的一些能力。基于这一点，我们提出了Reasoning-augmented Continual Learning（RCL）方法。RCL通过将任务特有的cue与元理性相结合，以降低LLMs中的恶化学习，同时加速在新任务上的 converges。
</details></li>
</ul>
<hr>
<h2 id="Temporally-Aligning-Long-Audio-Interviews-with-Questions-A-Case-Study-in-Multimodal-Data-Integration"><a href="#Temporally-Aligning-Long-Audio-Interviews-with-Questions-A-Case-Study-in-Multimodal-Data-Integration" class="headerlink" title="Temporally Aligning Long Audio Interviews with Questions: A Case Study in Multimodal Data Integration"></a>Temporally Aligning Long Audio Interviews with Questions: A Case Study in Multimodal Data Integration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06702">http://arxiv.org/abs/2310.06702</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/piyushsinghpasi/INDENT">https://github.com/piyushsinghpasi/INDENT</a></li>
<li>paper_authors: Piyush Singh Pasi, Karthikeya Battepati, Preethi Jyothi, Ganesh Ramakrishnan, Tanmay Mahapatra, Manoj Singh</li>
<li>for: 这个研究是为了解决长 Audio-to-text 的对齐问题，通常在训练时使用完整的监督。但是，这些研究通常不是在长 Audio 文件中，其中文本 queries 不会直接出现在 Audio 文件中。这个研究是与印度 CARE 组织合作，收集了来自印度比хар邦的农村区域的长 Audio 健康调查。</li>
<li>methods: 我们提出了一个名为 INDENT 的框架，使用 crossed attention 模型和文本 вопро卷中的 temporal ordering 信息来学习 speech 嵌入。这些学习的嵌入被用于在搜寻时根据文本 queries 找到相应的 Audio 段。</li>
<li>results: 我们对比了 INDENT 模型和文本基于的 heuristics 模型，并证明了 INDENT 模型在 R-avg 方面提高了约 3%。我们还表明了使用 state-of-the-art ASR 模型生成的噪音 ASR 可以在搜寻时提供更好的结果。 finally，我们证明了 INDENT 只需要在印地语料上训练，就可以在 11 种指定语言上进行搜寻。<details>
<summary>Abstract</summary>
The problem of audio-to-text alignment has seen significant amount of research using complete supervision during training. However, this is typically not in the context of long audio recordings wherein the text being queried does not appear verbatim within the audio file. This work is a collaboration with a non-governmental organization called CARE India that collects long audio health surveys from young mothers residing in rural parts of Bihar, India. Given a question drawn from a questionnaire that is used to guide these surveys, we aim to locate where the question is asked within a long audio recording. This is of great value to African and Asian organizations that would otherwise have to painstakingly go through long and noisy audio recordings to locate questions (and answers) of interest. Our proposed framework, INDENT, uses a cross-attention-based model and prior information on the temporal ordering of sentences to learn speech embeddings that capture the semantics of the underlying spoken text. These learnt embeddings are used to retrieve the corresponding audio segment based on text queries at inference time. We empirically demonstrate the significant effectiveness (improvement in R-avg of about 3%) of our model over those obtained using text-based heuristics. We also show how noisy ASR, generated using state-of-the-art ASR models for Indian languages, yields better results when used in place of speech. INDENT, trained only on Hindi data is able to cater to all languages supported by the (semantically) shared text space. We illustrate this empirically on 11 Indic languages.
</details>
<details>
<summary>摘要</summary>
audio-to-文本对齐问题在训练中得到了大量研究，通常是使用完全监督。但是，这并不是在长度较长的音频文件中，文本 queries 中的内容不是直接出现在音频文件中的情况。这是一项与非政府组织CARE印度合作的工作，收集了印度北部锡库的年轻母亲的长 Audio 健康调查。给定一个问卷中的问题，我们的目标是在长 Audio 录音中找到这个问题的位置。这对于非洲和亚洲组织来说是非常有价值的，否则他们需要慢慢地从长度较长的 Audio 录音中找到问题（以及答案）。我们提出了一个名为 INDENT 的框架，使用 cross-attention 模型和前期知识来学习 speech 嵌入，这些嵌入 capture 了下面的含义。在推理时，我们使用这些学习的嵌入来根据文本查询 retrieve 相应的音频段。我们实际示出了我们模型比使用文本基于的优化法得到的效果更好（提高 R-avg 约 3%）。我们还显示了使用 state-of-the-art ASR 模型生成的噪音 ASR 可以在某些情况下提供更好的结果。INDENT，只在印地语料上训练，能够涵盖所有支持 Semantic 共享文本空间中的语言。我们在 11 种指定语言上进行了实质性的示例。
</details></li>
</ul>
<hr>
<h2 id="Learning-Multiplex-Embeddings-on-Text-rich-Networks-with-One-Text-Encoder"><a href="#Learning-Multiplex-Embeddings-on-Text-rich-Networks-with-One-Text-Encoder" class="headerlink" title="Learning Multiplex Embeddings on Text-rich Networks with One Text Encoder"></a>Learning Multiplex Embeddings on Text-rich Networks with One Text Encoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06684">http://arxiv.org/abs/2310.06684</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bowen Jin, Wentao Zhang, Yu Zhang, Yu Meng, Han Zhao, Jiawei Han</li>
<li>for: 学习多重文本网络中的多种关系</li>
<li>methods: 使用一个文本编码器来模型关系之间的共享知识，并使用少量参数来 derivation 关系特定的表示</li>
<li>results: 在五个网络中的九个下游任务上，METERN significantly 和 consistently 超过基线方法，并且 Parameters 的效率高。In English, this means:</li>
<li>for: Learning multiple types of relationships in text-rich networks</li>
<li>methods: Using one text encoder to model shared knowledge across relations, and deriving relation-specific representations with a small number of parameters</li>
<li>results: Significantly and consistently outperforming baselines on nine downstream tasks in five networks, with high parameter efficiency.<details>
<summary>Abstract</summary>
In real-world scenarios, texts in a network are often linked by multiple semantic relations (e.g., papers in an academic network are referenced by other publications, written by the same author, or published in the same venue), where text documents and their relations form a multiplex text-rich network. Mainstream text representation learning methods use pretrained language models (PLMs) to generate one embedding for each text unit, expecting that all types of relations between texts can be captured by these single-view embeddings. However, this presumption does not hold particularly in multiplex text-rich networks. Along another line of work, multiplex graph neural networks (GNNs) directly initialize node attributes as a feature vector for node representation learning, but they cannot fully capture the semantics of the nodes' associated texts. To bridge these gaps, we propose METERN, a new framework for learning Multiplex Embeddings on TExt-Rich Networks. In contrast to existing methods, METERN uses one text encoder to model the shared knowledge across relations and leverages a small number of parameters per relation to derive relation-specific representations. This allows the encoder to effectively capture the multiplex structures in the network while also preserving parameter efficiency. We conduct experiments on nine downstream tasks in five networks from both academic and e-commerce domains, where METERN outperforms baselines significantly and consistently. The code is available at https://github.com/PeterGriffinJin/METERN-submit.
</details>
<details>
<summary>摘要</summary>
在实际场景中，网络中的文本经常被多种Semantic relation连接（例如，学术文献之间的引用、作者之间的共同写作或者发表在同一个会议上），这些文本文档和其关系组成了一个多重文本rich网络。主流文本表示学习方法使用预训练语言模型（PLM）生成每个文本单元的一个嵌入，期望所有类型的文本关系都可以通过这些单一视图嵌入被捕捉。然而，这个假设不符合特别在多重文本rich网络中。另一条工作线索是多种文本 graphs neural networks（GNNs）直接初始化节点属性为节点表示学习的特征向量，但它们无法完全捕捉节点相关文本的 semantics。为了覆盖这些差距，我们提出了METERN框架，一种新的文本多重嵌入学习框架。METERN使用一个文本编码器来模型关系间共享知识，并使用每个关系只需一些参数来生成特定关系表示。这使得编码器能够有效地捕捉多重结构，同时也能够保持参数效率。我们在五个网络和九个下渠任务上进行了实验，METERN与基线相比显著地提高了表现，并在多个网络和任务上保持稳定的高效性。代码可以在https://github.com/PeterGriffinJin/METERN-submit中找到。
</details></li>
</ul>
<hr>
<h2 id="SEER-A-Knapsack-approach-to-Exemplar-Selection-for-In-Context-HybridQA"><a href="#SEER-A-Knapsack-approach-to-Exemplar-Selection-for-In-Context-HybridQA" class="headerlink" title="SEER : A Knapsack approach to Exemplar Selection for In-Context HybridQA"></a>SEER : A Knapsack approach to Exemplar Selection for In-Context HybridQA</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06675">http://arxiv.org/abs/2310.06675</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jtonglet/seer">https://github.com/jtonglet/seer</a></li>
<li>paper_authors: Jonathan Tonglet, Manon Reusens, Philipp Borchert, Bart Baesens</li>
<li>for: 本研究旨在提高 HybridQA  tasks 的表达能力，通过选择 Representative 和多样化的 exemplars 来提高 reasoning 性能。</li>
<li>methods: 本文提出 Selection of ExEmplars for hybrid Reasoning (SEER) 方法，该方法将 exemplar 选择问题转化为 Knapsack 整数线性编程，以便满足多样化约束和容量约束。</li>
<li>results: 在 FinQA 和 TAT-QA 两个实际 benchmark 上，SEER 方法比前一代 exemplar 选择方法表现更高效。<details>
<summary>Abstract</summary>
Question answering over hybrid contexts is a complex task, which requires the combination of information extracted from unstructured texts and structured tables in various ways. Recently, In-Context Learning demonstrated significant performance advances for reasoning tasks. In this paradigm, a large language model performs predictions based on a small set of supporting exemplars. The performance of In-Context Learning depends heavily on the selection procedure of the supporting exemplars, particularly in the case of HybridQA, where considering the diversity of reasoning chains and the large size of the hybrid contexts becomes crucial. In this work, we present Selection of ExEmplars for hybrid Reasoning (SEER), a novel method for selecting a set of exemplars that is both representative and diverse. The key novelty of SEER is that it formulates exemplar selection as a Knapsack Integer Linear Program. The Knapsack framework provides the flexibility to incorporate diversity constraints that prioritize exemplars with desirable attributes, and capacity constraints that ensure that the prompt size respects the provided capacity budgets. The effectiveness of SEER is demonstrated on FinQA and TAT-QA, two real-world benchmarks for HybridQA, where it outperforms previous exemplar selection methods.
</details>
<details>
<summary>摘要</summary>
In this work, we propose Selection of ExEmplars for hybrid Reasoning (SEER), a novel method for selecting a set of exemplars that is both representative and diverse. The key innovation of SEER is that it formulates exemplar selection as a Knapsack Integer Linear Program. The Knapsack framework provides the flexibility to incorporate diversity constraints that prioritize exemplars with desirable attributes and capacity constraints that ensure that the prompt size respects the provided capacity budgets.We demonstrate the effectiveness of SEER on FinQA and TAT-QA, two real-world benchmarks for HybridQA, where it outperforms previous exemplar selection methods.
</details></li>
</ul>
<hr>
<h2 id="Making-Large-Language-Models-Perform-Better-in-Knowledge-Graph-Completion"><a href="#Making-Large-Language-Models-Perform-Better-in-Knowledge-Graph-Completion" class="headerlink" title="Making Large Language Models Perform Better in Knowledge Graph Completion"></a>Making Large Language Models Perform Better in Knowledge Graph Completion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06671">http://arxiv.org/abs/2310.06671</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zjukg/kopa">https://github.com/zjukg/kopa</a></li>
<li>paper_authors: Yichi Zhang, Zhuo Chen, Wen Zhang, Huajun Chen</li>
<li>for: 这个论文主要 targets 是如何使用语言模型（LLM）来完善知识 graphs（KGs），以提高 web 上自动服务的效能。</li>
<li>methods: 该论文提出了一种基于 LLM 的知识Graph completion（KGC）方法，通过将现有 LLM 模型转移到 структура感知Setting中，并提出了一种名为知识前缀适配器（KoPA）来使 LLM 能够更好地理解知识结构。KoPA 使用结构嵌入预训练来捕捉 KG 中实体和关系的结构信息，然后将这些结构嵌入 проек到文本空间，从而获得虚拟知识token作为输入提示。</li>
<li>results: 作者通过对这些结构意识LLM-based KGC方法进行了广泛的实验和深入分析，并证明了在引入结构信息后，LLM 的知识理解能力得到了改善。<details>
<summary>Abstract</summary>
Large language model (LLM) based knowledge graph completion (KGC) aims to predict the missing triples in the KGs with LLMs and enrich the KGs to become better web infrastructure, which can benefit a lot of web-based automatic services. However, research about LLM-based KGC is limited and lacks effective utilization of LLM's inference capabilities, which ignores the important structural information in KGs and prevents LLMs from acquiring accurate factual knowledge. In this paper, we discuss how to incorporate the helpful KG structural information into the LLMs, aiming to achieve structrual-aware reasoning in the LLMs. We first transfer the existing LLM paradigms to structural-aware settings and further propose a knowledge prefix adapter (KoPA) to fulfill this stated goal. KoPA employs structural embedding pre-training to capture the structural information of entities and relations in the KG. Then KoPA informs the LLMs of the knowledge prefix adapter which projects the structural embeddings into the textual space and obtains virtual knowledge tokens as a prefix of the input prompt. We conduct comprehensive experiments on these structural-aware LLM-based KGC methods and provide an in-depth analysis comparing how the introduction of structural information would be better for LLM's knowledge reasoning ability. Our code is released at https://github.com/zjukg/KoPA.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Self-Supervised-Representation-Learning-for-Online-Handwriting-Text-Classification"><a href="#Self-Supervised-Representation-Learning-for-Online-Handwriting-Text-Classification" class="headerlink" title="Self-Supervised Representation Learning for Online Handwriting Text Classification"></a>Self-Supervised Representation Learning for Online Handwriting Text Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06645">http://arxiv.org/abs/2310.06645</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pouya Mehralian, Bagher BabaAli, Ashena Gorgan Mohammadi</li>
<li>for: 这项研究旨在提出一种新的自助学习任务，以提取在线手写文本中人员的英文和中文语言writing的有用表示。</li>
<li>methods: 该研究使用了Part of Stroke Masking（POSM）作为预处理模型的预测任务，并提出了两种精度预处理模型的精度。</li>
<li>results: 该研究通过对预处理模型进行内在和外在评估方法，发现预处理模型可以达到写作人员认知、性别识别和手性识别等任务的最新状态。<details>
<summary>Abstract</summary>
Self-supervised learning offers an efficient way of extracting rich representations from various types of unlabeled data while avoiding the cost of annotating large-scale datasets. This is achievable by designing a pretext task to form pseudo labels with respect to the modality and domain of the data. Given the evolving applications of online handwritten texts, in this study, we propose the novel Part of Stroke Masking (POSM) as a pretext task for pretraining models to extract informative representations from the online handwriting of individuals in English and Chinese languages, along with two suggested pipelines for fine-tuning the pretrained models. To evaluate the quality of the extracted representations, we use both intrinsic and extrinsic evaluation methods. The pretrained models are fine-tuned to achieve state-of-the-art results in tasks such as writer identification, gender classification, and handedness classification, also highlighting the superiority of utilizing the pretrained models over the models trained from scratch.
</details>
<details>
<summary>摘要</summary>
自我指导学习提供了一种高效的方法，可以从不同类型的无标记数据中提取丰富的表示，而不需要投入大规模数据集的标注成本。这可以通过设计一个预tex任务，以模式和领域为据，生成 pseudo标签。在在线手写文本的应用场景中，在这项研究中，我们提出了一种新的部分roke掩蔽（POSM）作为预training模型的预tex任务，以提取英语和中文语言的在线手写人员的信息有价值表示。同时，我们还提出了两种可行的精度调整管道。为了评估提取的表示质量，我们使用了内在和外在评估方法。经过精度调整，预training模型可以达到当今最佳的写作人员认可、性别分类和手征分类等任务的结果，同时还 highlighted 预training模型的优势，比投入从头开始训练的模型更高效。
</details></li>
</ul>
<hr>
<h2 id="What-If-the-TV-Was-Off-Examining-Counterfactual-Reasoning-Abilities-of-Multi-modal-Language-Models"><a href="#What-If-the-TV-Was-Off-Examining-Counterfactual-Reasoning-Abilities-of-Multi-modal-Language-Models" class="headerlink" title="What If the TV Was Off? Examining Counterfactual Reasoning Abilities of Multi-modal Language Models"></a>What If the TV Was Off? Examining Counterfactual Reasoning Abilities of Multi-modal Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06627">http://arxiv.org/abs/2310.06627</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/letian2003/c-vqa">https://github.com/letian2003/c-vqa</a></li>
<li>paper_authors: Letian Zhang, Xiaotong Zhai, Zhongkai Zhao, Xin Wen, Yongshuo Zong, Bingchen Zhao</li>
<li>For: This paper aims to benchmark the counterfactual reasoning ability of multi-modal large language models.* Methods: The authors use the VQAv2 dataset and add a counterfactual presupposition to the questions, then generate counterfactual questions and answers using ChatGPT. They manually examine all generated questions and answers to ensure correctness.* Results: The authors evaluate recent vision language models on their newly collected test dataset and find that all models exhibit a large performance drop compared to the results tested on questions without the counterfactual presupposition, indicating that there is still room for improving vision language models. Additionally, the authors find a large gap between GPT-4 and current open-source models.Here are the three points in Simplified Chinese text:* For: 这篇论文目的是为了评估多模态大语言模型的反事实理解能力。* Methods: 作者使用VQAv2集成 dataset，并将问题中添加反事实前提，然后使用ChatGPT生成反事实问题和答案。他们手动检查所有生成的问题和答案，以确保正确性。* Results: 作者在新收集的测试集上评估了最近的视觉语言模型，发现所有模型在反事实前提下的表现均下降了较大，这表明还有很大的空间用于发展视觉语言模型。此外，作者发现GPT-4和当前开源模型之间存在很大的差距。<details>
<summary>Abstract</summary>
Counterfactual reasoning ability is one of the core abilities of human intelligence. This reasoning process involves the processing of alternatives to observed states or past events, and this process can improve our ability for planning and decision-making. In this work, we focus on benchmarking the counterfactual reasoning ability of multi-modal large language models. We take the question and answer pairs from the VQAv2 dataset and add one counterfactual presupposition to the questions, with the answer being modified accordingly. After generating counterfactual questions and answers using ChatGPT, we manually examine all generated questions and answers to ensure correctness. Over 2k counterfactual question and answer pairs are collected this way. We evaluate recent vision language models on our newly collected test dataset and found that all models exhibit a large performance drop compared to the results tested on questions without the counterfactual presupposition. This result indicates that there still exists space for developing vision language models. Apart from the vision language models, our proposed dataset can also serves as a benchmark for evaluating the ability of code generation LLMs, results demonstrate a large gap between GPT-4 and current open-source models. Our code and dataset are available at \url{https://github.com/Letian2003/C-VQA}.
</details>
<details>
<summary>摘要</summary>
《 counterfactual 理解能力是人类智能核心能力之一。这种理解过程包括评估观察到的状态或过去事件的 alternativas，可以提高我们的规划和决策能力。在这项工作中，我们将关注多模态大语言模型的 counterfactual 理解能力。我们从 VQAv2 数据集中提取了问题和答案对，并在其中添加了 counterfactual 前提，答案相应地被修改。通过使用 ChatGPT 生成 counterfactual 问题和答案，我们手动检查所有生成的问题和答案，以确保正确性。共收集了超过 2k 个 counterfactual 问题和答案对。我们对最新的视觉语言模型进行评估，发现所有模型在我们新收集的测试数据集上表现出大量的性能下降，这表明还存在开发视觉语言模型的空间。此外，我们的提出的数据集也可以用于评估代码生成 LLMS，结果显示 GPT-4 与当前开源模型存在很大差距。我们的代码和数据集可以在 <https://github.com/Letian2003/C-VQA> 上获取。》Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="No-Pitch-Left-Behind-Addressing-Gender-Unbalance-in-Automatic-Speech-Recognition-through-Pitch-Manipulation"><a href="#No-Pitch-Left-Behind-Addressing-Gender-Unbalance-in-Automatic-Speech-Recognition-through-Pitch-Manipulation" class="headerlink" title="No Pitch Left Behind: Addressing Gender Unbalance in Automatic Speech Recognition through Pitch Manipulation"></a>No Pitch Left Behind: Addressing Gender Unbalance in Automatic Speech Recognition through Pitch Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06590">http://arxiv.org/abs/2310.06590</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hlt-mt/fbk-fairseq">https://github.com/hlt-mt/fbk-fairseq</a></li>
<li>paper_authors: Dennis Fucci, Marco Gaido, Matteo Negri, Mauro Cettolo, Luisa Bentivogli</li>
<li>for: 提高女性发音识别精度（End-to-end neural architectures）</li>
<li>methods: 使用基频和形板数据的数据增强技术（Data augmentation technique）</li>
<li>results: 对女性发音的识别精度提高9.87%，特别是对最少表示的基频范围内的发音进行了更大的改进。<details>
<summary>Abstract</summary>
Automatic speech recognition (ASR) systems are known to be sensitive to the sociolinguistic variability of speech data, in which gender plays a crucial role. This can result in disparities in recognition accuracy between male and female speakers, primarily due to the under-representation of the latter group in the training data. While in the context of hybrid ASR models several solutions have been proposed, the gender bias issue has not been explicitly addressed in end-to-end neural architectures. To fill this gap, we propose a data augmentation technique that manipulates the fundamental frequency (f0) and formants. This technique reduces the data unbalance among genders by simulating voices of the under-represented female speakers and increases the variability within each gender group. Experiments on spontaneous English speech show that our technique yields a relative WER improvement up to 9.87% for utterances by female speakers, with larger gains for the least-represented f0 ranges.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="FTFT-efficient-and-robust-Fine-Tuning-by-transFerring-Training-dynamics"><a href="#FTFT-efficient-and-robust-Fine-Tuning-by-transFerring-Training-dynamics" class="headerlink" title="FTFT: efficient and robust Fine-Tuning by transFerring Training dynamics"></a>FTFT: efficient and robust Fine-Tuning by transFerring Training dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06588">http://arxiv.org/abs/2310.06588</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yupei Du, Albert Gatt, Dong Nguyen</li>
<li>for: 提高大型预训练语言模型（PLM）的 Robustness 性能</li>
<li>methods: 使用 Data Map 方法，包括在参考模型上进行 fine-tuning，然后选择一部分重要的训练示例，并在这些选择的示例上进行 fine-tuning</li>
<li>results: 比起 conventional Empirical Risk Minimization (ERM)，使用 Fine-Tuning by transFerring Training dynamics (FTFT) 方法可以更快速地达到更好的泛化 robustness 性能，同时占用训练成本的一半。<details>
<summary>Abstract</summary>
Despite the massive success of fine-tuning large Pre-trained Language Models (PLMs) on a wide range of Natural Language Processing (NLP) tasks, they remain susceptible to out-of-distribution (OOD) and adversarial inputs. Data map (DM) is a simple yet effective dual-model approach that enhances the robustness of fine-tuned PLMs, which involves fine-tuning a model on the original training set (i.e. reference model), selecting a specified fraction of important training examples according to the training dynamics of the reference model, and fine-tuning the same model on these selected examples (i.e. main model). However, it suffers from the drawback of requiring fine-tuning the same model twice, which is computationally expensive for large models. In this paper, we first show that 1) training dynamics are highly transferable across different model sizes and different pre-training methods, and that 2) main models fine-tuned using DM learn faster than when using conventional Empirical Risk Minimization (ERM). Building on these observations, we propose a novel fine-tuning approach based on the DM method: Fine-Tuning by transFerring Training dynamics (FTFT). Compared with DM, FTFT uses more efficient reference models and then fine-tunes more capable main models for fewer steps. Our experiments show that FTFT achieves better generalization robustness than ERM while spending less than half of the training cost.
</details>
<details>
<summary>摘要</summary>
尽管大型预训言语模型（PLM）的精细调整在各种自然语言处理（NLP）任务上取得了巨大成功，但它们仍然容易受到生成外部输入（OOD）和恶意输入的影响。数据映射（DM）是一种简单 yet有效的双模型方法，可以提高精细调整后PLM的Robustness，该方法包括将引用模型（i.e. reference model）在原始训练集上进行精细调整，然后选择该模型在训练动态中的一定比率的重要训练示例，并将该示例精细调整到同一模型上（i.e. main model）。然而，它的缺点在于需要两次精细调整同一模型，这会对大型模型来说很 computationally expensive。在这篇论文中，我们首先表明了以下两点：1）训练动态在不同的模型大小和预训练方法之间具有很高的传递性，2）使用DM方法精细调整的主模型在训练过程中更快速地 converges。基于这些观察，我们提出了一种基于DM方法的新的精细调整方法：FTFT（Fine-Tuning by transFerring Training dynamics）。相比DM，FTFT使用更有效的引用模型，然后精细调整更有能力的主模型，需要更少的训练步骤。我们的实验表明，FTFT在比ERM更好的泛化 Robustness 的同时，训练成本也比ERM低于一半。
</details></li>
</ul>
<hr>
<h2 id="AutoCycle-VC-Towards-Bottleneck-Independent-Zero-Shot-Cross-Lingual-Voice-Conversion"><a href="#AutoCycle-VC-Towards-Bottleneck-Independent-Zero-Shot-Cross-Lingual-Voice-Conversion" class="headerlink" title="AutoCycle-VC: Towards Bottleneck-Independent Zero-Shot Cross-Lingual Voice Conversion"></a>AutoCycle-VC: Towards Bottleneck-Independent Zero-Shot Cross-Lingual Voice Conversion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06546">http://arxiv.org/abs/2310.06546</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haeyun Choi, Jio Gim, Yuho Lee, Youngin Kim, Young-Joo Suh</li>
<li>for: 这个论文提出了一种简单而强大的零shot语音转换系统，该系统使用一个循环结构和MEL-spectrogram预处理。之前的works因过度依赖瓶颈结构而导致信息损失和差异化synthesis质量。此外，仅仅通过自我重建损失来重建不同的speaker的语音也是一个问题。</li>
<li>methods: 我们提出了一种循环一致损失，该损失考虑了转换回和转换过的target和source speaker之间的对应关系。此外，我们还使用了堆栈随机洗涤的MEL-spectrogram和标签平滑方法来在speaker encoder训练中提取时间独立的全局speaker表示。</li>
<li>results: 我们的模型在对比之前的state-of-the-art结果时表现出色，并在主观和客观评估中都达到了更高的评价标准。此外，我们的模型还可以实现 cross-lingual语音转换和提高synthesized语音的质量。<details>
<summary>Abstract</summary>
This paper proposes a simple and robust zero-shot voice conversion system with a cycle structure and mel-spectrogram pre-processing. Previous works suffer from information loss and poor synthesis quality due to their reliance on a carefully designed bottleneck structure. Moreover, models relying solely on self-reconstruction loss struggled with reproducing different speakers' voices. To address these issues, we suggested a cycle-consistency loss that considers conversion back and forth between target and source speakers. Additionally, stacked random-shuffled mel-spectrograms and a label smoothing method are utilized during speaker encoder training to extract a time-independent global speaker representation from speech, which is the key to a zero-shot conversion. Our model outperforms existing state-of-the-art results in both subjective and objective evaluations. Furthermore, it facilitates cross-lingual voice conversions and enhances the quality of synthesized speech.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种简单而可靠的零shot语音转换系统，具有一个循环结构和mel-spectrogram预处理。前一些工作受到瓶颈结构的限制，导致信息损失和Synthesis质量不佳。而且，仅仅依靠自我重建损失的模型很难复制不同的发音者的voice。为了解决这些问题，我们建议了一种循环一致损失，考虑 conversions between 目标和源发音者。此外，我们在Speaker encoder训练时使用了Random-shuffled mel-spectrograms和标签平滑方法，以提取speech中的时间独立的全局发音者表示。这是零shot转换的关键。我们的模型在主观和客观评估中都超过了现有的状态场的结果，并且允许跨语言的语音转换和提高合成语音质量。
</details></li>
</ul>
<hr>
<h2 id="EmoTwiCS-A-Corpus-for-Modelling-Emotion-Trajectories-in-Dutch-Customer-Service-Dialogues-on-Twitter"><a href="#EmoTwiCS-A-Corpus-for-Modelling-Emotion-Trajectories-in-Dutch-Customer-Service-Dialogues-on-Twitter" class="headerlink" title="EmoTwiCS: A Corpus for Modelling Emotion Trajectories in Dutch Customer Service Dialogues on Twitter"></a>EmoTwiCS: A Corpus for Modelling Emotion Trajectories in Dutch Customer Service Dialogues on Twitter</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06536">http://arxiv.org/abs/2310.06536</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sofie Labat, Thomas Demeester, Véronique Hoste<br>for:这篇论文的目的是为了提供一个有用的满足客户需求的社交媒体上的客户服务对话集，以便在这些平台上自动检测情绪。methods:这篇论文使用的方法包括Twitter上的客户服务对话集的收集和标注，并对这些对话中的情绪进行了分类和评价。results:这篇论文的结果包括一个高质量的情绪演变轨迹数据集，以及对这些数据集的多种分析和应用。<details>
<summary>Abstract</summary>
Due to the rise of user-generated content, social media is increasingly adopted as a channel to deliver customer service. Given the public character of these online platforms, the automatic detection of emotions forms an important application in monitoring customer satisfaction and preventing negative word-of-mouth. This paper introduces EmoTwiCS, a corpus of 9,489 Dutch customer service dialogues on Twitter that are annotated for emotion trajectories. In our business-oriented corpus, we view emotions as dynamic attributes of the customer that can change at each utterance of the conversation. The term `emotion trajectory' refers therefore not only to the fine-grained emotions experienced by customers (annotated with 28 labels and valence-arousal-dominance scores), but also to the event happening prior to the conversation and the responses made by the human operator (both annotated with 8 categories). Inter-annotator agreement (IAA) scores on the resulting dataset are substantial and comparable with related research, underscoring its high quality. Given the interplay between the different layers of annotated information, we perform several in-depth analyses to investigate (i) static emotions in isolated tweets, (ii) dynamic emotions and their shifts in trajectory, and (iii) the role of causes and response strategies in emotion trajectories. We conclude by listing the advantages and limitations of our dataset, after which we give some suggestions on the different types of predictive modelling tasks and open research questions to which EmoTwiCS can be applied. The dataset is available upon request and will be made publicly available upon acceptance of the paper.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:由于用户生成内容的升起，社交媒体越来越被用作客服渠道。由于这些在线平台的公共性，自动检测情感的应用变得非常重要，以监测客户满意度并避免负面Word of mouth。本文介绍了 EmoTwiCS，一个包含9489个荷兰客服对话的推特数据集，每个对话都被注释为情感轨迹。在我们的商业化数据集中，我们视情感为客户的动态特性，可以在每个对话中改变。“情感轨迹”这个术语不仅包括客户经验的细腻情感（通过28个标签和挥腾评分得分），还包括对话之前的事件和人工操作员的回应（两者各被注释为8个类别）。结果的交互注释者一致性（IAA）分数很高，与相关研究相当，这证明数据的高质量。由于不同层次的注释信息之间的互动，我们进行了多种深入分析， investigate (i) 隔离 tweet 中的静态情感， (ii) 情感的变化和轨迹的转折，以及 (iii) 事件和回应策略在情感轨迹中的作用。我们 conclude 后列出了数据集的优点和限制，然后给出了针对不同预测模型任务和开放研究 вопро题的建议。数据集可以在请求时获得，并在文章接受后公开发布。
</details></li>
</ul>
<hr>
<h2 id="Toward-Semantic-Publishing-in-Non-Invasive-Brain-Stimulation-A-Comprehensive-Analysis-of-rTMS-Studies"><a href="#Toward-Semantic-Publishing-in-Non-Invasive-Brain-Stimulation-A-Comprehensive-Analysis-of-rTMS-Studies" class="headerlink" title="Toward Semantic Publishing in Non-Invasive Brain Stimulation: A Comprehensive Analysis of rTMS Studies"></a>Toward Semantic Publishing in Non-Invasive Brain Stimulation: A Comprehensive Analysis of rTMS Studies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06517">http://arxiv.org/abs/2310.06517</a></li>
<li>repo_url: None</li>
<li>paper_authors: Swathi Anil, Jennifer D’Souza</li>
<li>for: 这篇论文目的是推动不侵入性脑刺激（NIBS）领域的交叉学科合作，以普遍采用计算机科学 semantics 报道方法来标准化 Neuroscience NIBS 研究的描述，使其能够被复制、访问、共享和重用（FAIR）。</li>
<li>methods: 本论文使用了大规模系统性审查，对 600 篇复合性Transcranial Magnetic Stimulation（rTMS）研究进行了描述，并描述了这些研究的关键特征，以便在结构化的描述和比较中使用。</li>
<li>results: 本论文通过实施 FAIR Semantic Web 资源（s）基本publishing 方案，对 600 篇审查的 rTMS 研究进行了 semantic publishing 在知识图库中。<details>
<summary>Abstract</summary>
Noninvasive brain stimulation (NIBS) encompasses transcranial stimulation techniques that can influence brain excitability. These techniques have the potential to treat conditions like depression, anxiety, and chronic pain, and to provide insights into brain function. However, a lack of standardized reporting practices limits its reproducibility and full clinical potential. This paper aims to foster interinterdisciplinarity toward adopting Computer Science Semantic reporting methods for the standardized documentation of Neuroscience NIBS studies making them explicitly Findable, Accessible, Interoperable, and Reusable (FAIR).   In a large-scale systematic review of 600 repetitive transcranial magnetic stimulation (rTMS), a subarea of NIBS, dosages, we describe key properties that allow for structured descriptions and comparisons of the studies. This paper showcases the semantic publishing of NIBS in the ecosphere of knowledge-graph-based next-generation scholarly digital libraries. Specifically, the FAIR Semantic Web resource(s)-based publishing paradigm is implemented for the 600 reviewed rTMS studies in the Open Research Knowledge Graph.
</details>
<details>
<summary>摘要</summary>
非侵入性脑刺激（NIBS）涵盖了跨脑刺激技术，可以影响脑部活动。这些技术有可能用于治疗厌食症、抑郁症和慢性疼痛等疾病，并提供脑功能的知识。然而，由于报告方法不够标准化，NIBS的复制性和临床潜力受到限制。这篇文章的目的是推动不同领域的学者共同努力，以采用计算机科学 semantic 报告方法，为脑科学 NIBS 研究提供标准化的描述和比较。在600例重复脑刺激（rTMS）系统性回顾中，我们描述了允许结构化描述和比较研究的关键性质。这篇文章展示了 NIBS 在知识图像基础的下一代学术数字图书馆中的semantic publishing paradigm。具体来说，本文使用 FAIR Semantic Web 资源（s）基于的发布方式，对600例回顾的 rTMS 研究进行开放式研究知识图像中的发布。
</details></li>
</ul>
<hr>
<h2 id="The-Limits-of-ChatGPT-in-Extracting-Aspect-Category-Opinion-Sentiment-Quadruples-A-Comparative-Analysis"><a href="#The-Limits-of-ChatGPT-in-Extracting-Aspect-Category-Opinion-Sentiment-Quadruples-A-Comparative-Analysis" class="headerlink" title="The Limits of ChatGPT in Extracting Aspect-Category-Opinion-Sentiment Quadruples: A Comparative Analysis"></a>The Limits of ChatGPT in Extracting Aspect-Category-Opinion-Sentiment Quadruples: A Comparative Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06502">http://arxiv.org/abs/2310.06502</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiancai Xu, Jia-Dong Zhang, Rongchang Xiao, Lei Xiong</li>
<li>for: 本研究是为了检验ChatGPT是否可以在文本中提取复杂的四元组（即属性-类别-意见-情感）。</li>
<li>methods: 本研究使用了特制的提示模板，以便ChatGPT可以有效地处理这个复杂的四元组提取任务。 此外，我们还提出了一种基于少量示例的选择方法，以完全利用ChatGPT的内在学习能力并提高其效iveness在这个任务上。</li>
<li>results: 我们对ChatGPT与现有状态的四元组提取模型进行了比较，并在四个公共数据集上进行了评估。我们发现ChatGPT在这个任务上的表现不佳，但是它在某些情况下表现出了良好的能力。<details>
<summary>Abstract</summary>
Recently, ChatGPT has attracted great attention from both industry and academia due to its surprising abilities in natural language understanding and generation. We are particularly curious about whether it can achieve promising performance on one of the most complex tasks in aspect-based sentiment analysis, i.e., extracting aspect-category-opinion-sentiment quadruples from texts. To this end, in this paper we develop a specialized prompt template that enables ChatGPT to effectively tackle this complex quadruple extraction task. Further, we propose a selection method on few-shot examples to fully exploit the in-context learning ability of ChatGPT and uplift its effectiveness on this complex task. Finally, we provide a comparative evaluation on ChatGPT against existing state-of-the-art quadruple extraction models based on four public datasets and highlight some important findings regarding the capability boundaries of ChatGPT in the quadruple extraction.
</details>
<details>
<summary>摘要</summary>
近期，ChatGPT已经吸引了行业和学术界的广泛关注，因为它在自然语言理解和生成方面表现出了惊人的能力。我们尤其关注ChatGPT是否可以在一个最复杂的任务中表现出色，即从文本中提取方面-类别-意见-情感四元组。为此，在这篇论文中，我们开发了特有的提示模板，使得ChatGPT能够有效地解决这个复杂的四元组提取任务。此外，我们提出了基于少量示例选择的方法，以充分利用ChatGPT在上下文学习中的能力，提高它在这个任务上的效iveness。最后，我们对ChatGPT与现有状态的四元组提取模型进行了比较评估，并发现了一些关于ChatGPT在四元组提取任务上的能力边界的重要发现。
</details></li>
</ul>
<hr>
<h2 id="A-New-Benchmark-and-Reverse-Validation-Method-for-Passage-level-Hallucination-Detection"><a href="#A-New-Benchmark-and-Reverse-Validation-Method-for-Passage-level-Hallucination-Detection" class="headerlink" title="A New Benchmark and Reverse Validation Method for Passage-level Hallucination Detection"></a>A New Benchmark and Reverse Validation Method for Passage-level Hallucination Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06498">http://arxiv.org/abs/2310.06498</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/maybenotime/phd">https://github.com/maybenotime/phd</a></li>
<li>paper_authors: Shiping Yang, Renliang Sun, Xiaojun Wan</li>
<li>for: 本研究旨在提出一种自我检查方法，以检测 LLM 生成的幻见（false information）。</li>
<li>methods: 本方法基于反验证，可以在零资源条件下自动检测幻见。而我们还构建了一个幻见检测 benchmark，名为 PHD，用于评估不同方法的性能。</li>
<li>results: 我们的方法在两个数据集上对比baseline方法表现出色，具有更高的准确率和更低的质量成本。此外，我们还手动分析了 LLM 失败检测的一些例子，发现零资源方法具有共同的限制。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have shown their ability to collaborate effectively with humans in real-world scenarios. However, LLMs are apt to generate hallucinations, i.e., makeup incorrect text and unverified information, which can cause significant damage when deployed for mission-critical tasks. In this paper, we propose a self-check approach based on reverse validation to detect factual errors automatically in a zero-resource fashion. To facilitate future studies and assess different methods, we construct a hallucination detection benchmark named PHD, which is generated by ChatGPT and annotated by human annotators. Contrasting previous studies of zero-resource hallucination detection, our method and benchmark concentrate on passage-level detection instead of sentence-level. We empirically evaluate our method and existing zero-resource detection methods on two datasets. The experimental results demonstrate that the proposed method considerably outperforms the baselines while costing fewer tokens and less time. Furthermore, we manually analyze some hallucination cases that LLM failed to capture, revealing the shared limitation of zero-resource methods.
</details>
<details>
<summary>摘要</summary>
Note: "Simplified Chinese" is a translation of the text into Chinese, using simpler grammar and vocabulary to make it easier to understand for native Chinese speakers. However, please note that the translation may not be perfect and may not capture all the nuances of the original text.
</details></li>
</ul>
<hr>
<h2 id="SpikeCLIP-A-Contrastive-Language-Image-Pretrained-Spiking-Neural-Network"><a href="#SpikeCLIP-A-Contrastive-Language-Image-Pretrained-Spiking-Neural-Network" class="headerlink" title="SpikeCLIP: A Contrastive Language-Image Pretrained Spiking Neural Network"></a>SpikeCLIP: A Contrastive Language-Image Pretrained Spiking Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06488">http://arxiv.org/abs/2310.06488</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianlong Li, Wenhao Liu, Changze Lv, Jianhan Xu, Cenyuan Zhang, Muling Wu, Xiaoqing Zheng, Xuanjing Huang</li>
<li>For: 本文旨在探讨使用刺激神经网络（SNN）在多Modal场景中的扩展，并采用了一种新的框架 named SpikeCLIP，以提高在多Modal场景中的刺激计算的效能。* Methods: 本文使用了一种两步方法，包括“Alignment Pre-training”和“双损失精度调整”，以将刺激计算与深度神经网络（DNN）相结合，从而实现在多Modal场景中的刺激计算。* Results: 实验结果表明，使用SpikeCLIP框架可以在多Modal场景中实现刺激计算的相对比较好的性能，同时减少了能耗量。此外，SpikeCLIP还可以保持在图像分类任务中的稳定性，即使涉及到不在特定类别中的类别标签。<details>
<summary>Abstract</summary>
Spiking neural networks (SNNs) have demonstrated the capability to achieve comparable performance to deep neural networks (DNNs) in both visual and linguistic domains while offering the advantages of improved energy efficiency and adherence to biological plausibility. However, the extension of such single-modality SNNs into the realm of multimodal scenarios remains an unexplored territory. Drawing inspiration from the concept of contrastive language-image pre-training (CLIP), we introduce a novel framework, named SpikeCLIP, to address the gap between two modalities within the context of spike-based computing through a two-step recipe involving ``Alignment Pre-training + Dual-Loss Fine-tuning". Extensive experiments demonstrate that SNNs achieve comparable results to their DNN counterparts while significantly reducing energy consumption across a variety of datasets commonly used for multimodal model evaluation. Furthermore, SpikeCLIP maintains robust performance in image classification tasks that involve class labels not predefined within specific categories.
</details>
<details>
<summary>摘要</summary>
聚合神经网络（SNN）已经表现出与深度神经网络（DNN）相当的性能在视觉和语言领域，而且具有更好的能效性和生物启发性。然而，将单模态SNN扩展到多模态场景仍然是一个未探索的领域。 Drawing inspiration from语言-图像准备（CLIP）的概念，我们提出了一种新的框架，名为SpikeCLIP，以解决在毫 COUNTING  computing中两个模态之间的差异。我们采用了两步方法：“对齐预训练 + 双损失细化”。广泛的实验表明，SNN可以与其DNN对应类型相当，同时具有显著降低能耗的优势。此外，SpikeCLIP在图像分类任务中保持了不受限定类别的稳定性。
</details></li>
</ul>
<hr>
<h2 id="Multilingual-Jailbreak-Challenges-in-Large-Language-Models"><a href="#Multilingual-Jailbreak-Challenges-in-Large-Language-Models" class="headerlink" title="Multilingual Jailbreak Challenges in Large Language Models"></a>Multilingual Jailbreak Challenges in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06474">http://arxiv.org/abs/2310.06474</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/DAMO-NLP-SG/multilingual-safety-for-LLMs">https://github.com/DAMO-NLP-SG/multilingual-safety-for-LLMs</a></li>
<li>paper_authors: Yue Deng, Wenxuan Zhang, Sinno Jialin Pan, Lidong Bing</li>
<li>for: This paper aims to address the safety concerns associated with large language models (LLMs) in the multilingual context, specifically the “jailbreak” problem where malicious instructions can manipulate LLMs to exhibit undesirable behavior.</li>
<li>methods: The paper reveals the presence of multilingual jailbreak challenges within LLMs and considers two potential risk scenarios: unintentional and intentional. The authors experimentally demonstrate that low-resource languages are more susceptible to unsafe content generation, and propose a novel \textsc{Self-Defense} framework for safety fine-tuning.</li>
<li>results: The paper shows that the proposed \textsc{Self-Defense} framework can achieve a substantial reduction in unsafe content generation for ChatGPT, with an 80.92% reduction in unsafe output for the intentional scenario and a three times increase in unsafe content for the unintentional scenario compared to high-resource languages.Here’s the Chinese translation of the three points:</li>
<li>for: 这篇论文目标是解决大语言模型（LLMs）在多语言场景下的安全问题，特别是“监狱”问题，其中恶意指令可以 manipulate LLMs 以产生不жела的行为。</li>
<li>methods: 论文揭示了 LLMs 中的多语言监狱挑战，并考虑了两种风险enario：不计划的和计划的。试验表明，低资源语言存在更高的危险内容生成率，并提出了一种名为 \textsc{Self-Defense} 的新框架，用于安全 fine-tuning。</li>
<li>results: 论文显示，\textsc{Self-Defense} 框架可以减少 ChatGPT 的危险输出，具体来说，对于意外情况，低资源语言的危险内容生成率高三倍于高资源语言，而对于意图情况， \textsc{Self-Defense} 框架可以减少 unsafe 输出的比例为 80.92%。<details>
<summary>Abstract</summary>
While large language models (LLMs) exhibit remarkable capabilities across a wide range of tasks, they pose potential safety concerns, such as the ``jailbreak'' problem, wherein malicious instructions can manipulate LLMs to exhibit undesirable behavior. Although several preventive measures have been developed to mitigate the potential risks associated with LLMs, they have primarily focused on English data. In this study, we reveal the presence of multilingual jailbreak challenges within LLMs and consider two potential risk scenarios: unintentional and intentional. The unintentional scenario involves users querying LLMs using non-English prompts and inadvertently bypassing the safety mechanisms, while the intentional scenario concerns malicious users combining malicious instructions with multilingual prompts to deliberately attack LLMs. The experimental results reveal that in the unintentional scenario, the rate of unsafe content increases as the availability of languages decreases. Specifically, low-resource languages exhibit three times the likelihood of encountering harmful content compared to high-resource languages, with both ChatGPT and GPT-4. In the intentional scenario, multilingual prompts can exacerbate the negative impact of malicious instructions, with astonishingly high rates of unsafe output: 80.92\% for ChatGPT and 40.71\% for GPT-4. To handle such a challenge in the multilingual context, we propose a novel \textsc{Self-Defense} framework that automatically generates multilingual training data for safety fine-tuning. Experimental results show that ChatGPT fine-tuned with such data can achieve a substantial reduction in unsafe content generation. Data is available at https://github.com/DAMO-NLP-SG/multilingual-safety-for-LLMs. Warning: This paper contains examples with potentially harmful content.
</details>
<details>
<summary>摘要</summary>
large language models (LLMs)  display remarkable capabilities across a wide range of tasks, but they also pose potential safety concerns, such as the "jailbreak" problem, where malicious instructions can manipulate LLMs to exhibit undesirable behavior. Although several preventive measures have been developed to mitigate the potential risks associated with LLMs, they have primarily focused on English data. In this study, we reveal the presence of multilingual jailbreak challenges within LLMs and consider two potential risk scenarios: unintentional and intentional. The unintentional scenario involves users querying LLMs using non-English prompts and inadvertently bypassing the safety mechanisms, while the intentional scenario concerns malicious users combining malicious instructions with multilingual prompts to deliberately attack LLMs.our experimental results show that in the unintentional scenario, the rate of unsafe content increases as the availability of languages decreases. Specifically, low-resource languages exhibit three times the likelihood of encountering harmful content compared to high-resource languages, with both ChatGPT and GPT-4. In the intentional scenario, multilingual prompts can exacerbate the negative impact of malicious instructions, with astonishingly high rates of unsafe output: 80.92% for ChatGPT and 40.71% for GPT-4.to handle such a challenge in the multilingual context, we propose a novel \textsc{Self-Defense} framework that automatically generates multilingual training data for safety fine-tuning. Experimental results show that ChatGPT fine-tuned with such data can achieve a substantial reduction in unsafe content generation. Data is available at https://github.com/DAMO-NLP-SG/multilingual-safety-for-LLMs. Warning: This paper contains examples with potentially harmful content.
</details></li>
</ul>
<hr>
<h2 id="Cultural-Compass-Predicting-Transfer-Learning-Success-in-Offensive-Language-Detection-with-Cultural-Features"><a href="#Cultural-Compass-Predicting-Transfer-Learning-Success-in-Offensive-Language-Detection-with-Cultural-Features" class="headerlink" title="Cultural Compass: Predicting Transfer Learning Success in Offensive Language Detection with Cultural Features"></a>Cultural Compass: Predicting Transfer Learning Success in Offensive Language Detection with Cultural Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06458">http://arxiv.org/abs/2310.06458</a></li>
<li>repo_url: None</li>
<li>paper_authors: Li Zhou, Antonia Karamolegkou, Wenyu Chen, Daniel Hershcovich</li>
<li>for: 这个研究旨在探讨文化特征是否能准确预测跨文化传输学习效果，以提高语言技术的包容性和文化敏感性。</li>
<li>methods: 研究者使用了文化价值调查来评估跨文化传输学习的效果，并发现文化价值调查可以预测跨文化传输学习的成功。此外，研究者还发现使用了粗鄙词距可以进一步提高跨文化传输学习的效果。</li>
<li>results: 研究发现文化价值调查indeed possess a predictive power for cross-cultural transfer learning success in OLD tasks, and that it can be further improved using offensive word distance.<details>
<summary>Abstract</summary>
The increasing ubiquity of language technology necessitates a shift towards considering cultural diversity in the machine learning realm, particularly for subjective tasks that rely heavily on cultural nuances, such as Offensive Language Detection (OLD). Current understanding underscores that these tasks are substantially influenced by cultural values, however, a notable gap exists in determining if cultural features can accurately predict the success of cross-cultural transfer learning for such subjective tasks. Addressing this, our study delves into the intersection of cultural features and transfer learning effectiveness. The findings reveal that cultural value surveys indeed possess a predictive power for cross-cultural transfer learning success in OLD tasks and that it can be further improved using offensive word distance. Based on these results, we advocate for the integration of cultural information into datasets. Additionally, we recommend leveraging data sources rich in cultural information, such as surveys, to enhance cultural adaptability. Our research signifies a step forward in the quest for more inclusive, culturally sensitive language technologies.
</details>
<details>
<summary>摘要</summary>
随着语言技术的普及，需要对文化多样性在机器学习领域进行考虑，特别是对于基于文化特点的主观任务，如涉礼语言检测（OLD）。现有研究表明，这类任务受到文化价值的影响，但是存在一定的掌握问题，即可以否准确预测跨文化传输学习的成功。我们的研究团队对此进行了调查，发现文化价值调查确实可以预测跨文化传输学习成功，并且可以通过涉礼词语距离进一步改进。根据这些结果，我们建议将文化信息纳入数据集中，并且建议使用具有文化信息的数据源，如调查，来提高文化适应性。我们的研究表明，针对更包容、文化敏感的语言技术的开发是一步前进。
</details></li>
</ul>
<hr>
<h2 id="MemSum-DQA-Adapting-An-Efficient-Long-Document-Extractive-Summarizer-for-Document-Question-Answering"><a href="#MemSum-DQA-Adapting-An-Efficient-Long-Document-Extractive-Summarizer-for-Document-Question-Answering" class="headerlink" title="MemSum-DQA: Adapting An Efficient Long Document Extractive Summarizer for Document Question Answering"></a>MemSum-DQA: Adapting An Efficient Long Document Extractive Summarizer for Document Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06436">http://arxiv.org/abs/2310.06436</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nianlonggu/memsum-dqa">https://github.com/nianlonggu/memsum-dqa</a></li>
<li>paper_authors: Nianlong Gu, Yingqiang Gao, Richard H. R. Hahnloser</li>
<li>for: 文章主要针对的是文档问答（DQA）任务，旨在提高文档抽取概要的能力。</li>
<li>methods: 该系统使用了 MemSum，一种长文档抽取概要器，进行文档问答。文档被解析为多个块，每个块都附加了提供的问题和问题类型，然后 selectively 提取块作为答案。</li>
<li>results: 与先前的基eline相比，MemSum-DQA在全文 answering 任务上提高了9%的精确匹配率。此外，MemSum-DQA在儿童关系理解方面表现出色，这指示了抽取概要技术在 DQA 任务中的潜在优势。<details>
<summary>Abstract</summary>
We introduce MemSum-DQA, an efficient system for document question answering (DQA) that leverages MemSum, a long document extractive summarizer. By prefixing each text block in the parsed document with the provided question and question type, MemSum-DQA selectively extracts text blocks as answers from documents. On full-document answering tasks, this approach yields a 9% improvement in exact match accuracy over prior state-of-the-art baselines. Notably, MemSum-DQA excels in addressing questions related to child-relationship understanding, underscoring the potential of extractive summarization techniques for DQA tasks.
</details>
<details>
<summary>摘要</summary>
我们介绍MemSum-DQA，一种高效的文档问答系统（DQA），利用MemSum，一种长文档抽取式概要系统。通过在文档中每个文本块前置提供的问题和问题类型，MemSum-DQA选择性地从文档中提取答案。在全文 answering 任务上，这种方法比之前的基线性能提高9%。尤其是在儿童关系理解方面，MemSum-DQA表现出色，这 highlights the potential of 抽取式概要技术在 DQA 任务中。
</details></li>
</ul>
<hr>
<h2 id="Humans-and-language-models-diverge-when-predicting-repeating-text"><a href="#Humans-and-language-models-diverge-when-predicting-repeating-text" class="headerlink" title="Humans and language models diverge when predicting repeating text"></a>Humans and language models diverge when predicting repeating text</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06408">http://arxiv.org/abs/2310.06408</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/HuthLab/lm-repeating-text">https://github.com/HuthLab/lm-repeating-text</a></li>
<li>paper_authors: Aditya R. Vaidya, Javier Turek, Alexander G. Huth</li>
<li>for: 这个研究是为了检验语言模型在下一个单词预测任务中是否能够准确模拟人类行为。</li>
<li>methods: 这个研究使用了GPT-2语言模型和人类参与者的下一个单词预测数据集，并对这些数据进行分析和比较。</li>
<li>results: 研究发现，在第一次显示文本扩展时，人类和语言模型的性能很高相关，但是当memory（或在场景学习）开始发挥作用时，人类和语言模型的性能快速分化。研究发现了这种分化的原因，并通过添加带有力学律回归的注意头来解决这个问题，使模型更像人类。<details>
<summary>Abstract</summary>
Language models that are trained on the next-word prediction task have been shown to accurately model human behavior in word prediction and reading speed. In contrast with these findings, we present a scenario in which the performance of humans and LMs diverges. We collected a dataset of human next-word predictions for five stimuli that are formed by repeating spans of text. Human and GPT-2 LM predictions are strongly aligned in the first presentation of a text span, but their performance quickly diverges when memory (or in-context learning) begins to play a role. We traced the cause of this divergence to specific attention heads in a middle layer. Adding a power-law recency bias to these attention heads yielded a model that performs much more similarly to humans. We hope that this scenario will spur future work in bringing LMs closer to human behavior.
</details>
<details>
<summary>摘要</summary>
语言模型，它们在下一个词预测任务上训练，已经能够准确地模拟人类行为。然而，我们提出了一种情况，在这种情况下，人类和语言模型（LM）的性能开始分化。我们收集了五个句子的人类下一个词预测数据集。人类和GPT-2语言模型在第一次文本段的预测 task 上强相关，但是他们的性能很快地分化，当内存（或在场景学习）开始发挥作用时。我们追踪了这种分化的原因，发现了特定的注意头在中间层。将power-law recency bias添加到这些注意头可以创建一个与人类更相似的模型。我们希望这种情况能够促进未来的研究，使语言模型更接近人类行为。
</details></li>
</ul>
<hr>
<h2 id="Improved-prompting-and-process-for-writing-user-personas-with-LLMs-using-qualitative-interviews-Capturing-behaviour-and-personality-traits-of-users"><a href="#Improved-prompting-and-process-for-writing-user-personas-with-LLMs-using-qualitative-interviews-Capturing-behaviour-and-personality-traits-of-users" class="headerlink" title="Improved prompting and process for writing user personas with LLMs, using qualitative interviews: Capturing behaviour and personality traits of users"></a>Improved prompting and process for writing user personas with LLMs, using qualitative interviews: Capturing behaviour and personality traits of users</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06391">http://arxiv.org/abs/2310.06391</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefano De Paoli<br>for:The paper aims to present a workflow for creating user personas using large language models, specifically through the results of thematic analysis of qualitative interviews.methods:The proposed workflow utilizes improved prompting and a larger pool of themes compared to previous work by the author, made possible by the capabilities of a recently released large language model (GPT3.5-Turbo-16k) and refined prompting for creating personas.results:The paper discusses the improved workflow for creating personas and offers reflections on the relationship between the proposed process and existing approaches to personas, as well as the capacity of LLMs to capture user behaviors and personality traits from the underlying dataset of qualitative interviews used for analysis.<details>
<summary>Abstract</summary>
This draft paper presents a workflow for creating User Personas with Large Language Models, using the results of a Thematic Analysis of qualitative interviews. The proposed workflow uses improved prompting and a larger pool of Themes, compared to previous work conducted by the author for the same task. This is possible due to the capabilities of a recently released LLM which allows the processing of 16 thousand tokens (GPT3.5-Turbo-16k) and also due to the possibility to offer a refined prompting for the creation of Personas. The paper offers details of performing Phase 2 and 3 of Thematic Analysis, and then discusses the improved workflow for creating Personas. The paper also offers some reflections on the relationship between the proposed process and existing approaches to Personas such as the data-driven and qualitative Personas. Moreover, the paper offers reflections on the capacity of LLMs to capture user behaviours and personality traits, from the underlying dataset of qualitative interviews used for the analysis.
</details>
<details>
<summary>摘要</summary>
这份草稿文章介绍了使用大语言模型创建用户人物的工作流程，基于论题分析的访谈结果。提议的工作流程使用改进的提示和更大的主题池，比前一作者为同任务所做的工作更好。这几乎可以归功于最近发布的LLM，它可以处理16千个字符（GPT3.5-Turbo-16k），以及可以提供更精细的提示 для创建人物。文章详细介绍了执行阶段2和3的论题分析，然后讨论了改进的工作流程。文章还提供了关于提案过程和现有方法人物之间的关系的反思，以及LLM对用户行为和人格特征的捕捉能力。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Model-Selection-and-Decoding-for-Keyphrase-Generation-with-Pre-trained-Sequence-to-Sequence-Models"><a href="#Rethinking-Model-Selection-and-Decoding-for-Keyphrase-Generation-with-Pre-trained-Sequence-to-Sequence-Models" class="headerlink" title="Rethinking Model Selection and Decoding for Keyphrase Generation with Pre-trained Sequence-to-Sequence Models"></a>Rethinking Model Selection and Decoding for Keyphrase Generation with Pre-trained Sequence-to-Sequence Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06374">http://arxiv.org/abs/2310.06374</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/uclanlp/deepkpg">https://github.com/uclanlp/deepkpg</a></li>
<li>paper_authors: Di Wu, Wasi Uddin Ahmad, Kai-Wei Chang</li>
<li>for: 本研究旨在系统地研究基于语言模型（PLM）的关键短语生成（KPG）任务中，不同的模型选择和解码策略的影响。</li>
<li>methods: 本研究使用了seq2seq预训练语言模型（PLM）来进行KPG任务，并系统地分析了不同的模型选择和解码策略对KPG任务的影响。</li>
<li>results: 研究发现，在选择PLM模型时，仅增加模型大小或进行任务特定适应并不是parameterfficient的; 在解码方面，使用抽样查找方法可以提高F1分数，但是它在回味方面落后于简单搜索方法。基于这些发现，本研究提出了一种基于概率的decode-select算法，可以改进greedy搜索。<details>
<summary>Abstract</summary>
Keyphrase Generation (KPG) is a longstanding task in NLP with widespread applications. The advent of sequence-to-sequence (seq2seq) pre-trained language models (PLMs) has ushered in a transformative era for KPG, yielding promising performance improvements. However, many design decisions remain unexplored and are often made arbitrarily. This paper undertakes a systematic analysis of the influence of model selection and decoding strategies on PLM-based KPG. We begin by elucidating why seq2seq PLMs are apt for KPG, anchored by an attention-driven hypothesis. We then establish that conventional wisdom for selecting seq2seq PLMs lacks depth: (1) merely increasing model size or performing task-specific adaptation is not parameter-efficient; (2) although combining in-domain pre-training with task adaptation benefits KPG, it does partially hinder generalization. Regarding decoding, we demonstrate that while greedy search achieves strong F1 scores, it lags in recall compared with sampling-based methods. Based on these insights, we propose DeSel, a likelihood-based decode-select algorithm for seq2seq PLMs. DeSel improves greedy search by an average of 4.7% semantic F1 across five datasets. Our collective findings pave the way for deeper future investigations into PLM-based KPG.
</details>
<details>
<summary>摘要</summary>
《键签生成（KPG）是NLPT中长期任务，广泛应用。 seq2seq预训练语言模型（PLM）的出现，为KPG带来了转变性的时代，提高性能。然而，许多设计决策仍然未经探索，经常采取优化的方式。本文进行了系统性的分析，探讨PLM基于KPG的模型选择和解码策略对Seq2Seq PLM的影响。我们开始由Seq2Seq PLM适用于KPG的原因，基于注意力驱动的假设。然后，我们发现了现有的Seq2Seq PLM选择方法的缺陷：（1）仅通过增加模型大小或进行任务特定的适应，不能减少参数的效率；（2）虽然结合域内预训练和任务适应可以提高KPG，但也会部分削弱泛化性。对于解码，我们表明了批量搜索可以 дости得强大的F1分数，但在回归方面落后于抽样方法。基于这些发现，我们提出了DeSel算法，它是基于概率的解码-选择算法，可以改进批量搜索。DeSel在五个数据集上提高了4.7%的语义F1分数。我们的总体发现可以为PLM基于KPG的未来研究开辟道路。》
</details></li>
</ul>
<hr>
<h2 id="Multi-Modal-Knowledge-Graph-Transformer-Framework-for-Multi-Modal-Entity-Alignment"><a href="#Multi-Modal-Knowledge-Graph-Transformer-Framework-for-Multi-Modal-Entity-Alignment" class="headerlink" title="Multi-Modal Knowledge Graph Transformer Framework for Multi-Modal Entity Alignment"></a>Multi-Modal Knowledge Graph Transformer Framework for Multi-Modal Entity Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06365">http://arxiv.org/abs/2310.06365</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xiaoqian19940510/moalign">https://github.com/xiaoqian19940510/moalign</a></li>
<li>paper_authors: Qian Li, Cheng Ji, Shu Guo, Zhaoji Liang, Lihong Wang, Jianxin Li</li>
<li>for: 提高多ModalEntityAlignment（MMEA）任务的性能，解决多Modal知识图（MMKG）中Equivalent entity pair的匹配问题。</li>
<li>methods: 提出了一种新的MMEA transformer，即MoAlign，通过针对不同类型信息（邻近实体、多Modal特征、实体类型）的层次引入，提高匹配任务的准确率。</li>
<li>results: 对多个benchmark dataset进行了广泛的实验，得到了优秀的实体匹配性能，比STRONG竞争对手更高。<details>
<summary>Abstract</summary>
Multi-Modal Entity Alignment (MMEA) is a critical task that aims to identify equivalent entity pairs across multi-modal knowledge graphs (MMKGs). However, this task faces challenges due to the presence of different types of information, including neighboring entities, multi-modal attributes, and entity types. Directly incorporating the above information (e.g., concatenation or attention) can lead to an unaligned information space. To address these challenges, we propose a novel MMEA transformer, called MoAlign, that hierarchically introduces neighbor features, multi-modal attributes, and entity types to enhance the alignment task. Taking advantage of the transformer's ability to better integrate multiple information, we design a hierarchical modifiable self-attention block in a transformer encoder to preserve the unique semantics of different information. Furthermore, we design two entity-type prefix injection methods to integrate entity-type information using type prefixes, which help to restrict the global information of entities not present in the MMKGs. Our extensive experiments on benchmark datasets demonstrate that our approach outperforms strong competitors and achieves excellent entity alignment performance.
</details>
<details>
<summary>摘要</summary>
多modalEntityAlignment（MMEA）是一个关键任务，旨在在多modal知识图（MMKG）中寻找等价实体对。然而，这个任务面临着不同类型的信息的存在，包括邻居实体、多modal特征和实体类型。直接包含这些信息（例如， concatenation 或 attention）可能会导致不一致的信息空间。为了解决这些挑战，我们提出了一种新的MMEA transformer，called MoAlign，它在多modal知识图中层次引入邻居特征、多modal特征和实体类型，以提高对齐任务。利用trasnformer的能力更好地集成多种信息，我们设计了一个层次可变自注意力块，以保持不同信息的唯一 semantics。此外，我们设计了两种实体类型前缀注入方法，以integrate实体类型信息使用类型前缀，帮助限制global信息的实体不在MMKG中。我们对标准数据集进行了广泛的实验，demonstrate that our approach outperforms strong competitors and achieves excellent entity alignment performance.
</details></li>
</ul>
<hr>
<h2 id="InfoCL-Alleviating-Catastrophic-Forgetting-in-Continual-Text-Classification-from-An-Information-Theoretic-Perspective"><a href="#InfoCL-Alleviating-Catastrophic-Forgetting-in-Continual-Text-Classification-from-An-Information-Theoretic-Perspective" class="headerlink" title="InfoCL: Alleviating Catastrophic Forgetting in Continual Text Classification from An Information Theoretic Perspective"></a>InfoCL: Alleviating Catastrophic Forgetting in Continual Text Classification from An Information Theoretic Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06362">http://arxiv.org/abs/2310.06362</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yifan-song793/infocl">https://github.com/yifan-song793/infocl</a></li>
<li>paper_authors: Yifan Song, Peiyi Wang, Weimin Xiong, Dawei Zhu, Tianyu Liu, Zhifang Sui, Sujian Li</li>
<li>for: 本研究旨在提出一种新的 continual learning 方法，以解决在类增cremental 设定下的 forgetting 问题。</li>
<li>methods: 我们提出了一种基于信息瓶颈的 representation learning 方法，并使用了 fast-slow 和 current-past 对比学习以提高表征学习过程。 </li>
<li>results: 我们的方法可以有效地避免 forgetting 问题，并在三个文本分类任务上实现了 state-of-the-art 的性能。<details>
<summary>Abstract</summary>
Continual learning (CL) aims to constantly learn new knowledge over time while avoiding catastrophic forgetting on old tasks. We focus on continual text classification under the class-incremental setting. Recent CL studies have identified the severe performance decrease on analogous classes as a key factor for catastrophic forgetting. In this paper, through an in-depth exploration of the representation learning process in CL, we discover that the compression effect of the information bottleneck leads to confusion on analogous classes. To enable the model learn more sufficient representations, we propose a novel replay-based continual text classification method, InfoCL. Our approach utilizes fast-slow and current-past contrastive learning to perform mutual information maximization and better recover the previously learned representations. In addition, InfoCL incorporates an adversarial memory augmentation strategy to alleviate the overfitting problem of replay. Experimental results demonstrate that InfoCL effectively mitigates forgetting and achieves state-of-the-art performance on three text classification tasks. The code is publicly available at https://github.com/Yifan-Song793/InfoCL.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese: kontinual learning (CL) 目标是不断学习新知识，而避免在老任务上出现致命忘记。我们在类增量设定下进行文本分类 continual learning。 current CL 研究表明，在相似类上的性能下降是致命忘记的关键因素。在这篇文章中，我们通过 Continual learning 的表征学习过程的深入探索，发现信息瓶颈压缩的效果导致了相似类的混淆。为了让模型学习更加充分的表示，我们提议一种基于 InfoCL 的循环学习方法。我们的方法通过快慢学习和当前过去的对比学习来实现对信息的最大化。此外，InfoCL 还包括一种对抗记忆增强策略，以解决回放中的过拟合问题。实验结果表明，InfoCL 有效地避免了致命忘记，并在三个文本分类任务上达到了状态的最佳性能。代码可以在 https://github.com/Yifan-Song793/InfoCL 上获取。
</details></li>
</ul>
<hr>
<h2 id="A-Semantic-Invariant-Robust-Watermark-for-Large-Language-Models"><a href="#A-Semantic-Invariant-Robust-Watermark-for-Large-Language-Models" class="headerlink" title="A Semantic Invariant Robust Watermark for Large Language Models"></a>A Semantic Invariant Robust Watermark for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06356">http://arxiv.org/abs/2310.06356</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aiwei Liu, Leyi Pan, Xuming Hu, Shiao Meng, Lijie Wen</li>
<li>for: 本研究旨在提出一种semantic invariant watermarking方法，以提高LLMs中文生成器的攻击Robustness和安全Robustness。</li>
<li>methods: 本方法使用另一个嵌入LM生成所有前导token的semantic embedding，然后将这些semantic embedding转化为 watermark logits through our trained watermark model。</li>
<li>results: 研究表明，我们的方法在semantically invariant setting中具有高度的攻击Robustness和安全Robustness。此外，我们的 watermark还具有足够的安全Robustness。<details>
<summary>Abstract</summary>
Watermark algorithms for large language models (LLMs) have achieved extremely high accuracy in detecting text generated by LLMs. Such algorithms typically involve adding extra watermark logits to the LLM's logits at each generation step. However, prior algorithms face a trade-off between attack robustness and security robustness. This is because the watermark logits for a token are determined by a certain number of preceding tokens; a small number leads to low security robustness, while a large number results in insufficient attack robustness. In this work, we propose a semantic invariant watermarking method for LLMs that provides both attack robustness and security robustness. The watermark logits in our work are determined by the semantics of all preceding tokens. Specifically, we utilize another embedding LLM to generate semantic embeddings for all preceding tokens, and then these semantic embeddings are transformed into the watermark logits through our trained watermark model. Subsequent analyses and experiments demonstrated the attack robustness of our method in semantically invariant settings: synonym substitution and text paraphrasing settings. Finally, we also show that our watermark possesses adequate security robustness. Our code and data are available at https://github.com/THU-BPM/Robust_Watermark.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）的水印算法已经实现了极高的准确率，用于检测由LLM生成的文本。通常，这些算法都是通过在生成步骤中添加额外的水印噢来实现的。然而，先前的算法面临着一种负面的贸易OFF和安全性之间的贸易OFF。这是因为水印噢的某个token是由前一些token决定的，一个小数会导致安全性不足，而一个大数则会导致攻击鲁棒性不足。在这项工作中，我们提出了一种基于 semantics的强水印方法，该方法可以同时提供攻击鲁棒性和安全性。我们的水印噢是由所有前一些token的 semantics 决定的。具体来说，我们使用另一个嵌入式语言模型来生成所有前一些token的 semantics 嵌入，然后将这些 semantics 嵌入转换成水印噢 через我们训练的水印模型。后续的分析和实验表明了我们的方法在semantically invariant的 Setting 中具有攻击鲁棒性。此外，我们还证明了我们的水印具有足够的安全性。我们的代码和数据可以在https://github.com/THU-BPM/Robust_Watermark上获取。
</details></li>
</ul>
<hr>
<h2 id="Selective-Demonstrations-for-Cross-domain-Text-to-SQL"><a href="#Selective-Demonstrations-for-Cross-domain-Text-to-SQL" class="headerlink" title="Selective Demonstrations for Cross-domain Text-to-SQL"></a>Selective Demonstrations for Cross-domain Text-to-SQL</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06302">http://arxiv.org/abs/2310.06302</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuaichen Chang, Eric Fosler-Lussier</li>
<li>for: 本研究旨在探讨大语言模型（LLMs）在cross-domain文本到SQL任务中的泛化能力，以及如何使用域内示例来提高其性能。</li>
<li>methods: 本研究使用了域外示例和生成的域内示例来构建示例集，并提出了一种选择示例框架ODIS。ODIS利用了域外示例和域内示例的优点，并且可以在不含域内标注的情况下进行选择。</li>
<li>results: 对两个cross-domain文本到SQL数据集进行了实验，ODIS比基eline方法提高了1.1和11.8个执行精度点。<details>
<summary>Abstract</summary>
Large language models (LLMs) with in-context learning have demonstrated impressive generalization capabilities in the cross-domain text-to-SQL task, without the use of in-domain annotations. However, incorporating in-domain demonstration examples has been found to greatly enhance LLMs' performance. In this paper, we delve into the key factors within in-domain examples that contribute to the improvement and explore whether we can harness these benefits without relying on in-domain annotations. Based on our findings, we propose a demonstration selection framework ODIS which utilizes both out-of-domain examples and synthetically generated in-domain examples to construct demonstrations. By retrieving demonstrations from hybrid sources, ODIS leverages the advantages of both, showcasing its effectiveness compared to baseline methods that rely on a single data source. Furthermore, ODIS outperforms state-of-the-art approaches on two cross-domain text-to-SQL datasets, with improvements of 1.1 and 11.8 points in execution accuracy, respectively.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在跨领域文本到SQL任务中展示了印象深刻的普遍化能力，不需要使用领域标注。但是，包含领域示例可以大幅提高LLM的表现。在这篇论文中，我们探讨了领域示例中关键因素对提升的贡献，并查探我们是否可以利用这些优点而不需要领域标注。基于我们的发现，我们提出了一个示例选择框架ODIS，这个框架使用了外部示例和人工生成的领域示例来建立示例。通过从混合来源获取示例，ODIS可以利用这两种来源的优点，并且在两个跨领域文本到SQL数据集上显示出比基准方法更高的效果。此外，ODIS比前一代方法在两个数据集上表现更好，具体的提升为1.1和11.8个执行精度分别。
</details></li>
</ul>
<hr>
<h2 id="An-experiment-on-an-automated-literature-survey-of-data-driven-speech-enhancement-methods"><a href="#An-experiment-on-an-automated-literature-survey-of-data-driven-speech-enhancement-methods" class="headerlink" title="An experiment on an automated literature survey of data-driven speech enhancement methods"></a>An experiment on an automated literature survey of data-driven speech enhancement methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06260">http://arxiv.org/abs/2310.06260</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arthur dos Santos, Jayr Pereira, Rodrigo Nogueira, Bruno Masiero, Shiva Sander-Tavallaey, Elias Zea</li>
<li>for:  automatizieren einer Literatur-Überblick über 116 Artikel zu data-getriebenen Sprechverbesserungsverfahren</li>
<li>methods: 使用一个生成的预训练转换器（GPT）模型自动进行文献综述</li>
<li>results: 评估GPT模型在提供准确回答特定问题关于选择的人工参考文献中的能力和局限性<details>
<summary>Abstract</summary>
The increasing number of scientific publications in acoustics, in general, presents difficulties in conducting traditional literature surveys. This work explores the use of a generative pre-trained transformer (GPT) model to automate a literature survey of 116 articles on data-driven speech enhancement methods. The main objective is to evaluate the capabilities and limitations of the model in providing accurate responses to specific queries about the papers selected from a reference human-based survey. While we see great potential to automate literature surveys in acoustics, improvements are needed to address technical questions more clearly and accurately.
</details>
<details>
<summary>摘要</summary>
“随着科学期刊中有限的增加，传统的文献综述became increasingly difficult。本研究探讨使用生成器预训transformer（GPT）模型自动进行116篇资料驱动 speech 增强方法的文献综述。主要目的是评估模型对 especific queries 的答案是否具有准确性。 Although we see great potential in automating literature surveys in acoustics, further improvements are needed to address technical questions more clearly and accurately.”Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="GeoLLM-Extracting-Geospatial-Knowledge-from-Large-Language-Models"><a href="#GeoLLM-Extracting-Geospatial-Knowledge-from-Large-Language-Models" class="headerlink" title="GeoLLM: Extracting Geospatial Knowledge from Large Language Models"></a>GeoLLM: Extracting Geospatial Knowledge from Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06213">http://arxiv.org/abs/2310.06213</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rohin Manvi, Samar Khanna, Gengchen Mai, Marshall Burke, David Lobell, Stefano Ermon</li>
<li>for: 本研究是使用自然语言处理技术（NLP）和机器学习（ML）来解决地ospatial Tasks的应用问题，特别是使用互联网语言资料库（LLMs）来提取地ospatial知识。</li>
<li>methods: 本研究提出了一种新的方法 called GeoLLM，该方法可以有效地提取地ospatial知识从LLMs中，并且可以与OpenStreetMap地图数据结合使用。</li>
<li>results: 根据实验结果，GeoLLM方法可以与基elines相比提高70%的性能（用Pearson的$r^2$进行衡量），并且与现有的卫星数据 benchmark相当或更高。此外，研究还发现LLMs具有remarkable的空间信息和 sample-efficient特点。<details>
<summary>Abstract</summary>
The application of machine learning (ML) in a range of geospatial tasks is increasingly common but often relies on globally available covariates such as satellite imagery that can either be expensive or lack predictive power. Here we explore the question of whether the vast amounts of knowledge found in Internet language corpora, now compressed within large language models (LLMs), can be leveraged for geospatial prediction tasks. We first demonstrate that LLMs embed remarkable spatial information about locations, but naively querying LLMs using geographic coordinates alone is ineffective in predicting key indicators like population density. We then present GeoLLM, a novel method that can effectively extract geospatial knowledge from LLMs with auxiliary map data from OpenStreetMap. We demonstrate the utility of our approach across multiple tasks of central interest to the international community, including the measurement of population density and economic livelihoods. Across these tasks, our method demonstrates a 70% improvement in performance (measured using Pearson's $r^2$) relative to baselines that use nearest neighbors or use information directly from the prompt, and performance equal to or exceeding satellite-based benchmarks in the literature. With GeoLLM, we observe that GPT-3.5 outperforms Llama 2 and RoBERTa by 19% and 51% respectively, suggesting that the performance of our method scales well with the size of the model and its pretraining dataset. Our experiments reveal that LLMs are remarkably sample-efficient, rich in geospatial information, and robust across the globe. Crucially, GeoLLM shows promise in mitigating the limitations of existing geospatial covariates and complementing them well.
</details>
<details>
<summary>摘要</summary>
machine learning（ml）在各种地ospatial任务中越来越普遍，但常常基于全球可用的 covariates，如卫星影像，这些 covariates 可能是昂贵的或者预测力不强。在这里，我们考虑了 Whether the vast amounts of knowledge found in Internet language corpora, now compressed within large language models（LLMs），可以为 geospatial prediction tasks 提供支持。我们首先表明了 LLMs 嵌入了很多地理信息，但是直接使用地理坐标查询 LLMs 是不能有效地预测重要指标，如人口密度。然后，我们提出了 GeoLLM，一种新的方法，可以有效地从 LLMs 提取地ospatial 知识，并且可以与 OpenStreetMap 中的 auxiliary map 数据结合使用。我们在多个国际社区中的重要任务上进行了多个任务，包括人口密度的测量和经济生活水平的评估。在这些任务中，我们的方法比基eline 使用 nearest neighbors 或者直接从提示中获取信息的方法提高了70%（ measured using Pearson's $r^2$）。此外，我们发现 GPT-3.5 在 GeoLLM 中表现比 Llama 2 和 RoBERTa 好19%和51% respectively，这表明我们的方法可以很好地扩展到不同的模型和预训练集。我们的实验表明 LLMs 在各个地方都具有很好的sample efficiency，rich in geospatial information，和 robustness。此外，GeoLLM 可以有效地缓解现有的地ospatial covariates 的限制，并且可以补充它们良好。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/10/cs.CL_2023_10_10/" data-id="clpahu71700d53h88c4c43u55" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_10_10" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/10/cs.LG_2023_10_10/" class="article-date">
  <time datetime="2023-10-10T10:00:00.000Z" itemprop="datePublished">2023-10-10</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/10/cs.LG_2023_10_10/">cs.LG - 2023-10-10</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Investigating-the-Adversarial-Robustness-of-Density-Estimation-Using-the-Probability-Flow-ODE"><a href="#Investigating-the-Adversarial-Robustness-of-Density-Estimation-Using-the-Probability-Flow-ODE" class="headerlink" title="Investigating the Adversarial Robustness of Density Estimation Using the Probability Flow ODE"></a>Investigating the Adversarial Robustness of Density Estimation Using the Probability Flow ODE</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07084">http://arxiv.org/abs/2310.07084</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marius Arvinte, Cory Cornelius, Jason Martin, Nageen Himayat</li>
<li>for: 这个论文旨在检验抽样模型在面对梯度基于概率最大化攻击时的Robustness，以及与样本复杂度之间的关系。</li>
<li>methods: 这个论文使用了概率流（PF）神经网络ordinary differential equation（ODE）模型进行不偏概率估计，并对这种方法进行了六种梯度基于概率最大化攻击的评估。</li>
<li>results: 实验结果表明，使用PF ODE模型进行概率估计是对高复杂度、高概率攻击的Robustness的。此外，在CIFAR-10 dataset上，一些黑客攻击样本具有semantic meaning,如预期的Robust estimator中的。<details>
<summary>Abstract</summary>
Beyond their impressive sampling capabilities, score-based diffusion models offer a powerful analysis tool in the form of unbiased density estimation of a query sample under the training data distribution. In this work, we investigate the robustness of density estimation using the probability flow (PF) neural ordinary differential equation (ODE) model against gradient-based likelihood maximization attacks and the relation to sample complexity, where the compressed size of a sample is used as a measure of its complexity. We introduce and evaluate six gradient-based log-likelihood maximization attacks, including a novel reverse integration attack. Our experimental evaluations on CIFAR-10 show that density estimation using the PF ODE is robust against high-complexity, high-likelihood attacks, and that in some cases adversarial samples are semantically meaningful, as expected from a robust estimator.
</details>
<details>
<summary>摘要</summary>
除了其吸引人的采样能力之外，分数基于扩散模型还提供了一种强大的分析工具，即对训练数据分布下的查询样本进行不偏的density估计。在这种工作中，我们研究了PF neural differential equation（ODE）模型对梯度基于可能性最大化攻击的Robustness，以及与样本复杂度之间的关系。我们介绍并评估了6种梯度基于Log-likelihood最大化攻击，其中包括一种新的反整合攻击。我们的实验评估表明，使用PF ODE进行density估计对于高复杂性、高可能性攻击是Robust，而且在某些情况下，黑客样本具有semantically meaningful的意义，与一个Robust估计器相符。
</details></li>
</ul>
<hr>
<h2 id="Taking-the-human-out-of-decomposition-based-optimization-via-artificial-intelligence-Part-II-Learning-to-initialize"><a href="#Taking-the-human-out-of-decomposition-based-optimization-via-artificial-intelligence-Part-II-Learning-to-initialize" class="headerlink" title="Taking the human out of decomposition-based optimization via artificial intelligence: Part II. Learning to initialize"></a>Taking the human out of decomposition-based optimization via artificial intelligence: Part II. Learning to initialize</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07082">http://arxiv.org/abs/2310.07082</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ilias Mitrai, Prodromos Daoutidis</li>
<li>for: 解决大规模优化问题，frequently encountered in process systems engineering tasks.</li>
<li>methods: 使用机器学习方法学习优化算法的最佳初始化，以减少计算时间。</li>
<li>results: 提出的方法可以带来显著减少解决时间，并且活动学习可以减少学习数据量。Here’s a breakdown of each point:1. for: The paper is written for solving large-scale optimization problems in process systems engineering tasks.2. methods: The paper proposes using machine learning to learn the optimal initialization of decomposition-based solution methods, which can reduce the computational time.3. results: The proposed method can significantly reduce the solution time, and active learning can reduce the amount of data required for learning.<details>
<summary>Abstract</summary>
The repeated solution of large-scale optimization problems arises frequently in process systems engineering tasks. Decomposition-based solution methods have been widely used to reduce the corresponding computational time, yet their implementation has multiple steps that are difficult to configure. We propose a machine learning approach to learn the optimal initialization of such algorithms which minimizes the computational time. Active and supervised learning is used to learn a surrogate model that predicts the computational performance for a given initialization. We apply this approach to the initialization of Generalized Benders Decomposition for the solution of mixed integer model predictive control problems. The surrogate models are used to find the optimal number of initial cuts that should be added in the master problem. The results show that the proposed approach can lead to a significant reduction in solution time, and active learning can reduce the data required for learning.
</details>
<details>
<summary>摘要</summary>
大规模优化问题的重复解决问题经常出现在进程系统工程中的任务中。基于分解的解决方法广泛使用，但它们的实施具有多个步骤，这些步骤困难配置。我们提议使用机器学习方法来学习优化算法的初始化，以降低计算时间。我们使用活动学习和监督学习来学习一个预测算法的计算性能，这个预测算法用于确定给定初始化的计算时间。我们应用这种方法到 generalized Benders decomposition 的初始化中，用于解决混合整数预测控制问题。 surrogate 模型用于找到最佳的初始剖分数，以降低解决时间。结果表明，我们的方法可以带来显著的解决时间减少，并且活动学习可以减少学习数据量。
</details></li>
</ul>
<hr>
<h2 id="Secure-Decentralized-Learning-with-Blockchain"><a href="#Secure-Decentralized-Learning-with-Blockchain" class="headerlink" title="Secure Decentralized Learning with Blockchain"></a>Secure Decentralized Learning with Blockchain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07079">http://arxiv.org/abs/2310.07079</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Mdshobu/Liberty-House-Club-Whitepaper">https://github.com/Mdshobu/Liberty-House-Club-Whitepaper</a></li>
<li>paper_authors: Xiaoxue Zhang, Yifan Hua, Chen Qian</li>
<li>for: 防止单点失败问题，提高机器学习任务的数据隐私和通信效率。</li>
<li>methods: 使用块链技术进行分布式机器学习，实现模型验证和审核。</li>
<li>results: 在30% 恶意客户端情况下，通过信誉机制实现快速模型融合和高精度。<details>
<summary>Abstract</summary>
Federated Learning (FL) is a well-known paradigm of distributed machine learning on mobile and IoT devices, which preserves data privacy and optimizes communication efficiency. To avoid the single point of failure problem in FL, decentralized federated learning (DFL) has been proposed to use peer-to-peer communication for model aggregation, which has been considered an attractive solution for machine learning tasks on distributed personal devices. However, this process is vulnerable to attackers who share false models and data. If there exists a group of malicious clients, they might harm the performance of the model by carrying out a poisoning attack. In addition, in DFL, clients often lack the incentives to contribute their computing powers to do model training. In this paper, we proposed Blockchain-based Decentralized Federated Learning (BDFL), which leverages a blockchain for decentralized model verification and auditing. BDFL includes an auditor committee for model verification, an incentive mechanism to encourage the participation of clients, a reputation model to evaluate the trustworthiness of clients, and a protocol suite for dynamic network updates. Evaluation results show that, with the reputation mechanism, BDFL achieves fast model convergence and high accuracy on real datasets even if there exist 30\% malicious clients in the system.
</details>
<details>
<summary>摘要</summary>
federated learning（FL）是一种已知的分布式机器学习模式，适用于移动设备和物联网设备，保持数据隐私和通信效率。为了解决FL中的单点失败问题，分布式 federated learning（DFL）已经提议使用对等通信进行模型集成，这被视为对于分布在个人设备上的机器学习任务的有appealing解决方案。然而，这个过程受到攻击者们发送false模型和数据的威胁。如果存在一群恶意客户端，他们可能会通过毒品攻击伤害模型的性能。此外，在DFL中，客户端经常缺乏参与到模型训练中的动机。在这篇论文中，我们提出了基于区块链的分布式 federated learning（BDFL），该技术利用区块链进行分布式模型验证和审核。BDFL包括一个审计委员会 для模型验证、一种激励客户端参与的机制、一个客户端信任度评估模型以及一套协议集 для动态网络更新。评估结果表明，在各种情况下，包括30%的恶意客户端，BDFL仍能够快速启 converges和高精度地完成实际数据集的模型训练。
</details></li>
</ul>
<hr>
<h2 id="Taking-the-human-out-of-decomposition-based-optimization-via-artificial-intelligence-Part-I-Learning-when-to-decompose"><a href="#Taking-the-human-out-of-decomposition-based-optimization-via-artificial-intelligence-Part-I-Learning-when-to-decompose" class="headerlink" title="Taking the human out of decomposition-based optimization via artificial intelligence: Part I. Learning when to decompose"></a>Taking the human out of decomposition-based optimization via artificial intelligence: Part I. Learning when to decompose</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07068">http://arxiv.org/abs/2310.07068</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ilias Mitrai, Prodromos Daoutidis</li>
<li>for: 本文提出了一种图 классификация方法，用于自动决定使用固定或分解方法解决优化问题。</li>
<li>methods: 该方法使用图表示优化问题中变量和约束之间的结构和功能联系，然后建立图分类器来决定问题的最佳解决方法。</li>
<li>results: 该方法可以开发一个可以判断凸混合整数非线性 програм的最佳解决方法是使用分支和约束算法还是外接算法。此外，可以将学习的分类器 integrate到现有混合整数优化解决方案中。<details>
<summary>Abstract</summary>
In this paper, we propose a graph classification approach for automatically determining whether to use a monolithic or a decomposition-based solution method. In this approach, an optimization problem is represented as a graph that captures the structural and functional coupling among the variables and constraints of the problem via an appropriate set of features. Given this representation, a graph classifier is built to determine the best solution method for a given problem. The proposed approach is used to develop a classifier that determines whether a convex Mixed Integer Nonlinear Programming problem should be solved using branch and bound or the outer approximation algorithm. Finally, it is shown how the learned classifier can be incorporated into existing mixed integer optimization solvers.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种图 классификация方法，用于自动确定是否使用简单或含 decomposition 的解决方法。在这种方法中，一个优化问题被表示为一个图， capture 变量和约束之间的结构和功能相互关系via 合适的特征集。给出这种表示，一个图分类器被建立，以确定给定问题的最佳解决方法。我们所提出的方法用于开发一个可以判断 convex 混合整数非线性程序问题是否使用分支和约束算法或外接算法解决。最后，我们示出了如何将学习的分类器集成到现有的混合整数优化解决方案中。
</details></li>
</ul>
<hr>
<h2 id="Acoustic-Model-Fusion-for-End-to-end-Speech-Recognition"><a href="#Acoustic-Model-Fusion-for-End-to-end-Speech-Recognition" class="headerlink" title="Acoustic Model Fusion for End-to-end Speech Recognition"></a>Acoustic Model Fusion for End-to-end Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07062">http://arxiv.org/abs/2310.07062</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhihong Lei, Mingbin Xu, Shiyi Han, Leo Liu, Zhen Huang, Tim Ng, Yuanyuan Zhang, Ernest Pusateri, Mirko Hannemann, Yaqiao Deng, Man-Hung Siu</li>
<li>for: 提高 ASR 系统的准确率和 named entity recognition 性能</li>
<li>methods: 提出一种将 external acoustic model integrated into end-to-end ASR 系统的方法，以更好地解决频率域匹配问题</li>
<li>results: 实现了在不同测试集上的词错率下降，最高达14.3%，同时Named entity recognition的性能也得到了明显提高<details>
<summary>Abstract</summary>
Recent advances in deep learning and automatic speech recognition (ASR) have enabled the end-to-end (E2E) ASR system and boosted the accuracy to a new level. The E2E systems implicitly model all conventional ASR components, such as the acoustic model (AM) and the language model (LM), in a single network trained on audio-text pairs. Despite this simpler system architecture, fusing a separate LM, trained exclusively on text corpora, into the E2E system has proven to be beneficial. However, the application of LM fusion presents certain drawbacks, such as its inability to address the domain mismatch issue inherent to the internal AM. Drawing inspiration from the concept of LM fusion, we propose the integration of an external AM into the E2E system to better address the domain mismatch. By implementing this novel approach, we have achieved a significant reduction in the word error rate, with an impressive drop of up to 14.3% across varied test sets. We also discovered that this AM fusion approach is particularly beneficial in enhancing named entity recognition.
</details>
<details>
<summary>摘要</summary>
Traditional ASR systems consist of separate AM and language model (LM) components, but E2E systems combine these components into a single network trained on audio-text pairs. Despite this simpler architecture, fusing a separate LM trained exclusively on text corpora into the E2E system has been shown to be beneficial. However, this approach is limited by its inability to address the domain mismatch issue inherent to the internal AM. Our proposed approach of integrating an external AM into the E2E system addresses this issue by providing a more diverse set of acoustic features to the network. This allows the network to better handle variations in speech and improve overall accuracy. We have tested our approach on a variety of datasets and have achieved significant improvements in word error rates, with an impressive drop of up to 14.3% across all test sets. Additionally, we have found that this approach is particularly effective in enhancing named entity recognition.
</details></li>
</ul>
<hr>
<h2 id="Spiral-Elliptical-automated-galaxy-morphology-classification-from-telescope-images"><a href="#Spiral-Elliptical-automated-galaxy-morphology-classification-from-telescope-images" class="headerlink" title="Spiral-Elliptical automated galaxy morphology classification from telescope images"></a>Spiral-Elliptical automated galaxy morphology classification from telescope images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07740">http://arxiv.org/abs/2310.07740</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matthew J. Baumstark, Giuseppe Vinci</li>
<li>for: 本研究旨在提出 нов的星系形态统计方法，以提高现有的星系形态分类方法的效率和准确性。</li>
<li>methods: 本研究使用了两种新的星系形态统计方法：descent average和descent variance，以及简化了现有的图像统计指标（concentration、asymmetry和clumpiness）。</li>
<li>results: 使用 Sloan Digital Sky Survey 的星系图像数据，我们证明了我们提出的图像统计方法可以高效地检测扁旋和螺旋星系，并且可以作为Random Forest 分类器的特征来使用。<details>
<summary>Abstract</summary>
The classification of galaxy morphologies is an important step in the investigation of theories of hierarchical structure formation. While human expert visual classification remains quite effective and accurate, it cannot keep up with the massive influx of data from emerging sky surveys. A variety of approaches have been proposed to classify large numbers of galaxies; these approaches include crowdsourced visual classification, and automated and computational methods, such as machine learning methods based on designed morphology statistics and deep learning. In this work, we develop two novel galaxy morphology statistics, descent average and descent variance, which can be efficiently extracted from telescope galaxy images. We further propose simplified versions of the existing image statistics concentration, asymmetry, and clumpiness, which have been widely used in the literature of galaxy morphologies. We utilize the galaxy image data from the Sloan Digital Sky Survey to demonstrate the effective performance of our proposed image statistics at accurately detecting spiral and elliptical galaxies when used as features of a random forest classifier.
</details>
<details>
<summary>摘要</summary>
《星系形态分类是astrophysical structure formation理论研究中一个重要步骤。虽然人类专家视觉分类仍然非常有效和准确，但由于天文大观测数据的涌入，人类分类无法满足数据的需求。各种方法被提出来分类大量的星系，包括人工智能分类和计算机方法，如基于设计的形态统计和深度学习。在这项工作中，我们开发了两种新的星系形态统计，即下降平均值和下降方差，可以快速从望远镜星系图像中提取。我们还提出了现有图像统计的简化版本，包括吸引度、非均匀性和块性，这些统计在Literature中广泛使用。我们使用 Sloan Digital Sky Survey 的星系图像数据来证明我们提出的图像统计能够准确地检测扁旋和椭圆星系，当作Random Forest 分类器的特征。》Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="FedMFS-Federated-Multimodal-Fusion-Learning-with-Selective-Modality-Communication"><a href="#FedMFS-Federated-Multimodal-Fusion-Learning-with-Selective-Modality-Communication" class="headerlink" title="FedMFS: Federated Multimodal Fusion Learning with Selective Modality Communication"></a>FedMFS: Federated Multimodal Fusion Learning with Selective Modality Communication</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07048">http://arxiv.org/abs/2310.07048</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liangqi Yuan, Dong-Jun Han, Vishnu Pandi Chellapandi, Stanislaw H. Żak, Christopher G. Brinton</li>
<li>for: 本研究旨在提出一种新的分布式机器学习（Federated Learning，FL）方法，用于在互联网物联网（IoT）环境中进行多Modal数据融合学习。</li>
<li>methods: 本研究提出了一种名为Federated Multimodal Fusion learning with Selective modality communication（FedMFS）的新方法，该方法利用Shapley值来衡量每个模式的贡献，并根据模式模型大小来衡量通信开销，以便每个客户端可以选择上传模式模型到服务器进行集成。</li>
<li>results: 实验结果表明，FedMFS方法可以减少一很大的通信开销，同时保持与基准值相对的准确性。实际上，FedMFS方法可以在真实的多Modal数据集上实现相对于基准值的20%的通信开销减少。<details>
<summary>Abstract</summary>
Federated learning (FL) is a distributed machine learning (ML) paradigm that enables clients to collaborate without accessing, infringing upon, or leaking original user data by sharing only model parameters. In the Internet of Things (IoT), edge devices are increasingly leveraging multimodal data compositions and fusion paradigms to enhance model performance. However, in FL applications, two main challenges remain open: (i) addressing the issues caused by heterogeneous clients lacking specific modalities and (ii) devising an optimal modality upload strategy to minimize communication overhead while maximizing learning performance. In this paper, we propose Federated Multimodal Fusion learning with Selective modality communication (FedMFS), a new multimodal fusion FL methodology that can tackle the above mentioned challenges. The key idea is to utilize Shapley values to quantify each modality's contribution and modality model size to gauge communication overhead, so that each client can selectively upload the modality models to the server for aggregation. This enables FedMFS to flexibly balance performance against communication costs, depending on resource constraints and applications. Experiments on real-world multimodal datasets demonstrate the effectiveness of FedMFS, achieving comparable accuracy while reducing communication overhead by one twentieth compared to baselines.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 是一种分布式机器学习 (ML)  paradigma， enables clients to collaborate without accessing, infringing upon, or leaking original user data by sharing only model parameters. In the Internet of Things (IoT), edge devices are increasingly leveraging multimodal data compositions and fusion paradigms to enhance model performance. However, in FL applications, two main challenges remain open: (i) addressing the issues caused by heterogeneous clients lacking specific modalities and (ii) devising an optimal modality upload strategy to minimize communication overhead while maximizing learning performance. In this paper, we propose Federated Multimodal Fusion learning with Selective modality communication (FedMFS), a new multimodal fusion FL methodology that can tackle the above mentioned challenges. The key idea is to utilize Shapley values to quantify each modality's contribution and modality model size to gauge communication overhead, so that each client can selectively upload the modality models to the server for aggregation. This enables FedMFS to flexibly balance performance against communication costs, depending on resource constraints and applications. Experiments on real-world multimodal datasets demonstrate the effectiveness of FedMFS, achieving comparable accuracy while reducing communication overhead by one twentieth compared to baselines.
</details></li>
</ul>
<hr>
<h2 id="A-predict-and-optimize-approach-to-profit-driven-churn-prevention"><a href="#A-predict-and-optimize-approach-to-profit-driven-churn-prevention" class="headerlink" title="A predict-and-optimize approach to profit-driven churn prevention"></a>A predict-and-optimize approach to profit-driven churn prevention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07047">http://arxiv.org/abs/2310.07047</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nuria Gómez-Vargas, Sebastián Maldonado, Carla Vairetti</li>
<li>for: 预防营业潜在客户流失，以提高公司的营业额。</li>
<li>methods: 提出了一种基于预测和优化的客户挑选策略，将客户生命周期价值（CLV）作为主要评估标准，以避免营业损失。</li>
<li>results: 在12个客户流失预测数据集上，该策略达到了最佳平均收益水平，比其他常见策略更高。<details>
<summary>Abstract</summary>
In this paper, we introduce a novel predict-and-optimize method for profit-driven churn prevention. We frame the task of targeting customers for a retention campaign as a regret minimization problem. The main objective is to leverage individual customer lifetime values (CLVs) to ensure that only the most valuable customers are targeted. In contrast, many profit-driven strategies focus on churn probabilities while considering average CLVs. This often results in significant information loss due to data aggregation. Our proposed model aligns with the guidelines of Predict-and-Optimize (PnO) frameworks and can be efficiently solved using stochastic gradient descent methods. Results from 12 churn prediction datasets underscore the effectiveness of our approach, which achieves the best average performance compared to other well-established strategies in terms of average profit.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了一种新的预测和优化方法，用于防止利润驱动的客户流失。我们将客户退货活动的目标客户群作为 regret 最小化问题来定义。我们的主要目标是通过个体客户生命周期价值（CLV）来确保只有最有价值的客户被targeting。与此相比，许多利润驱动策略往往强调退货概率，而不考虑CLV的含义。这经常导致数据汇总所产生的信息损失。我们提出的模型遵循Predict-and-Optimize（PnO）框架的指南，可以使用Stochastic Gradient Descent（SGD）方法高效地解决。Results from 12 churn prediction datasets confirm the effectiveness of our approach, which achieves the best average performance compared to other well-established strategies in terms of average profit.Here's a word-for-word translation of the text into Simplified Chinese:在这篇论文中，我们介绍了一种新的预测和优化方法，用于防止利润驱动的客户流失。我们将客户退货活动的目标客户群作为 regret 最小化问题来定义。我们的主要目标是通过个体客户生命周期价值（CLV）来确保只有最有价值的客户被targeting。与此相比，许多利润驱动策略往往强调退货概率，而不考虑CLV的含义。这经常导致数据汇总所产生的信息损失。我们提出的模型遵循Predict-and-Optimize（PnO）框架的指南，可以使用Stochastic Gradient Descent（SGD）方法高效地解决。Results from 12 churn prediction datasets confirm the effectiveness of our approach, which achieves the best average performance compared to other well-established strategies in terms of average profit.
</details></li>
</ul>
<hr>
<h2 id="Neural-Harmonium-An-Interpretable-Deep-Structure-for-Nonlinear-Dynamic-System-Identification-with-Application-to-Audio-Processing"><a href="#Neural-Harmonium-An-Interpretable-Deep-Structure-for-Nonlinear-Dynamic-System-Identification-with-Application-to-Audio-Processing" class="headerlink" title="Neural Harmonium: An Interpretable Deep Structure for Nonlinear Dynamic System Identification with Application to Audio Processing"></a>Neural Harmonium: An Interpretable Deep Structure for Nonlinear Dynamic System Identification with Application to Audio Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07032">http://arxiv.org/abs/2310.07032</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karim Helwani, Erfan Soltanmohammadi, Michael M. Goodwin</li>
<li>for: 本研究目的是提高深度神经网络的解释性，尤其在应用于物理问题上。解释性可以帮助我们理解模型的泛化能力和其局限性。</li>
<li>methods: 本文提出了一种可解释性深度结构，用于模型动态系统。该模型使用干扰分析，在时域-频域中模型系统，同时保持高时间和频率分辨率。此外，模型采用递归结构，可以快速、稳定、且准确地进行第二阶导数计算，不需要显式计算权差矩阵。为了缓解建立模型块的高维度问题，使用神经网络来描述频率相互关系。</li>
<li>results: 在非线性系统识别问题上，提出的方法得到了证明。在音频干扰抑制问题中，通过对比与其他现有解决方案的实验，表明了我们的方法在实际应用中的效果。<details>
<summary>Abstract</summary>
Improving the interpretability of deep neural networks has recently gained increased attention, especially when the power of deep learning is leveraged to solve problems in physics. Interpretability helps us understand a model's ability to generalize and reveal its limitations. In this paper, we introduce a causal interpretable deep structure for modeling dynamic systems. Our proposed model makes use of the harmonic analysis by modeling the system in a time-frequency domain while maintaining high temporal and spectral resolution. Moreover, the model is built in an order recursive manner which allows for fast, robust, and exact second order optimization without the need for an explicit Hessian calculation. To circumvent the resulting high dimensionality of the building blocks of our system, a neural network is designed to identify the frequency interdependencies. The proposed model is illustrated and validated on nonlinear system identification problems as required for audio signal processing tasks. Crowd-sourced experimentation contrasting the performance of the proposed approach to other state-of-the-art solutions on an acoustic echo cancellation scenario confirms the effectiveness of our method for real-life applications.
</details>
<details>
<summary>摘要</summary>
深度学习在物理问题中的应用已经受到了提高解释性的关注，特别是当深度学习的力量被应用于解决物理问题时。解释性能我们理解模型的泛化能力和其局限性。在这篇论文中，我们介绍了一种可 causal 解释深度结构，用于模型动态系统。我们的提议的模型利用干扰分析，将系统模型在时间频域中进行了时间频谱分析，同时保持高度的时间和频率分辨率。此外，模型采用递归的构建方式，可以快速、稳定、准确地进行第二阶导数计算，不需要显式表达Hessian。为了避免建模块的高维度，我们设计了一个神经网络来识别频率相互关系。我们的提议模型在非线性系统识别问题中得到了验证，特别是在音频信号处理任务中。通过人工 эксперимент，我们比较了我们的方法与其他现有解决方案在音频适应噪抑问题中的性能，并证明了我们的方法在实际应用中的有效性。
</details></li>
</ul>
<hr>
<h2 id="Neural-Relational-Inference-with-Fast-Modular-Meta-learning"><a href="#Neural-Relational-Inference-with-Fast-Modular-Meta-learning" class="headerlink" title="Neural Relational Inference with Fast Modular Meta-learning"></a>Neural Relational Inference with Fast Modular Meta-learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07015">http://arxiv.org/abs/2310.07015</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/FerranAlet/modular-metalearning">https://github.com/FerranAlet/modular-metalearning</a></li>
<li>paper_authors: Ferran Alet, Erica Weng, Tomás Lozano Pérez, Leslie Pack Kaelbling</li>
<li>for: 这 paper 用于解决多种互动的关系推理问题，以便从观察数据中学习系统的动态。</li>
<li>methods: 这 paper 使用模块化元学习法，通过不同组合方式训练神经模块，以解决多种任务。</li>
<li>results: 这 paper 使用模块化元学习法提高了推理能力，可以更有效地利用观察数据，并且可以估计未直接观察到的实体状态。<details>
<summary>Abstract</summary>
\textit{Graph neural networks} (GNNs) are effective models for many dynamical systems consisting of entities and relations. Although most GNN applications assume a single type of entity and relation, many situations involve multiple types of interactions. \textit{Relational inference} is the problem of inferring these interactions and learning the dynamics from observational data. We frame relational inference as a \textit{modular meta-learning} problem, where neural modules are trained to be composed in different ways to solve many tasks. This meta-learning framework allows us to implicitly encode time invariance and infer relations in context of one another rather than independently, which increases inference capacity. Framing inference as the inner-loop optimization of meta-learning leads to a model-based approach that is more data-efficient and capable of estimating the state of entities that we do not observe directly, but whose existence can be inferred from their effect on observed entities. To address the large search space of graph neural network compositions, we meta-learn a \textit{proposal function} that speeds up the inner-loop simulated annealing search within the modular meta-learning algorithm, providing two orders of magnitude increase in the size of problems that can be addressed.
</details>
<details>
<summary>摘要</summary>
\begin{itemize}\item 图 neural networks（GNNs）是适用于许多动态系统中的有效模型，该系统包括实体和关系。 although most GNN applications assume a single type of entity and relation, many situations involve multiple types of interactions.\item 关系推理（relational inference）是从观察数据中推理这些交互的问题，学习这些交互的动态。 we frame relational inference as a modular meta-learning problem, where neural modules are trained to be composed in different ways to solve many tasks.\item 这个meta-learning框架允许我们通过不同的模块组合来解决多种任务，从而隐式地编码了时间不变性，并在彼此之间学习关系，这使得推理能力更高。\item 将推理视为meta-learning的内部循环优化问题，导致一种基于模型的方法，更有效率地使用数据，并能够估计不 direktly observable的实体状态，而是通过其影响已知实体来推理其存在。\item 为了解决图 neural network的模块组合搜索的大搜索空间，我们meta-learn a proposal function，这将在模块meta-learning算法中加速内部逻辑搜索，提供了两个数量级的提高，使得可以处理的问题规模提高了两个数量级。\end{itemize}Note: Please note that the translation is in Simplified Chinese, and some words or phrases may have different translations in Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Sound-skwatter-Did-You-Mean-Sound-squatter-AI-powered-Generator-for-Phishing-Prevention"><a href="#Sound-skwatter-Did-You-Mean-Sound-squatter-AI-powered-Generator-for-Phishing-Prevention" class="headerlink" title="Sound-skwatter (Did You Mean: Sound-squatter?) AI-powered Generator for Phishing Prevention"></a>Sound-skwatter (Did You Mean: Sound-squatter?) AI-powered Generator for Phishing Prevention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07005">http://arxiv.org/abs/2310.07005</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rodolfo Valentim, Idilio Drago, Marco Mellia, Federico Cerutti</li>
<li>for: 防御声钩攻击（Sound-squatting），使用人工智能生成声钩候选者。</li>
<li>methods: 使用 transformers 网络和声学模型组合，学习声音相似性。</li>
<li>results: 可以自动找到已知同音词和数千个高质量候选者，同时支持交互语言的声钩攻击。<details>
<summary>Abstract</summary>
Sound-squatting is a phishing attack that tricks users into malicious resources by exploiting similarities in the pronunciation of words. Proactive defense against sound-squatting candidates is complex, and existing solutions rely on manually curated lists of homophones. We here introduce Sound-skwatter, a multi-language AI-based system that generates sound-squatting candidates for proactive defense. Sound-skwatter relies on an innovative multi-modal combination of Transformers Networks and acoustic models to learn sound similarities. We show that Sound-skwatter can automatically list known homophones and thousands of high-quality candidates. In addition, it covers cross-language sound-squatting, i.e., when the reader and the listener speak different languages, supporting any combination of languages. We apply Sound-skwatter to network-centric phishing via squatted domain names. We find ~ 10% of the generated domains exist in the wild, the vast majority unknown to protection solutions. Next, we show attacks on the PyPI package manager, where ~ 17% of the popular packages have at least one existing candidate. We believe Sound-skwatter is a crucial asset to mitigate the sound-squatting phenomenon proactively on the Internet. To increase its impact, we publish an online demo and release our models and code as open source.
</details>
<details>
<summary>摘要</summary>
声音骗鱼是一种钓鱼攻击，通过利用声音相似性来骗用户访问恶意资源。现有的防御方法复杂，并且 existing solutions 依赖于手动维护的同音词列表。我们在这里介绍 Sound-skwatter，一个多语言基于 AI 系统，用于生成声音骗鱼候选者。Sound-skwatter 利用了一种创新的多模式 комбиinación，包括 transformers 网络和声音模型，以学习声音相似性。我们表明，Sound-skwatter 可以自动列出已知同音词和数千个高质量候选者。此外，它还支持跨语言声音骗鱼，即当读者和听众说不同语言时。我们应用 Sound-skwatter 于网络中心式骗鱼 via 骗取的域名。我们发现 ~ 10% 的生成域名在野，大多数都是未知的保护解决方案。接着，我们表明 ~ 17% 的流行包在 PyPI 包管理器中有至少一个现有的候选者。我们认为 Sound-skwatter 是在互联网上防止声音骗鱼的关键资产，以提高其影响力，我们在线发布了 demo 和发布我们的模型和代码为开源。
</details></li>
</ul>
<hr>
<h2 id="CarDS-Plus-ECG-Platform-Development-and-Feasibility-Evaluation-of-a-Multiplatform-Artificial-Intelligence-Toolkit-for-Portable-and-Wearable-Device-Electrocardiograms"><a href="#CarDS-Plus-ECG-Platform-Development-and-Feasibility-Evaluation-of-a-Multiplatform-Artificial-Intelligence-Toolkit-for-Portable-and-Wearable-Device-Electrocardiograms" class="headerlink" title="CarDS-Plus ECG Platform: Development and Feasibility Evaluation of a Multiplatform Artificial Intelligence Toolkit for Portable and Wearable Device Electrocardiograms"></a>CarDS-Plus ECG Platform: Development and Feasibility Evaluation of a Multiplatform Artificial Intelligence Toolkit for Portable and Wearable Device Electrocardiograms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07000">http://arxiv.org/abs/2310.07000</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sumukh Vasisht Shankar, Evangelos K Oikonomou, Rohan Khera<br>for: 这个研究旨在开发一个多平台系统，以快速部署基于人工智能的单导电喷（ECG）解决方案，用于临床调查和诊断。methods: 这个研究使用了多种设计考虑因素，包括具体应用场景、数据流程优化和实时推断等方面，以实现将多种来源的单导电喷数据传输到中央数据湖，并通过人工智能模型进行ECG解译。results: 研究表明，这个平台可以快速地从获取到报告结果，平均需时为33.0-35.7秒，无论使用哪种商业化的设备（Apple Watch和KardiaMobile）。这些结果表明了将设计原则翻译到快速部署的策略是可行的，并且可以在临床医疗中实现影响。<details>
<summary>Abstract</summary>
In the rapidly evolving landscape of modern healthcare, the integration of wearable & portable technology provides a unique opportunity for personalized health monitoring in the community. Devices like the Apple Watch, FitBit, and AliveCor KardiaMobile have revolutionized the acquisition and processing of intricate health data streams. Amidst the variety of data collected by these gadgets, single-lead electrocardiogram (ECG) recordings have emerged as a crucial source of information for monitoring cardiovascular health. There has been significant advances in artificial intelligence capable of interpreting these 1-lead ECGs, facilitating clinical diagnosis as well as the detection of rare cardiac disorders. This design study describes the development of an innovative multiplatform system aimed at the rapid deployment of AI-based ECG solutions for clinical investigation & care delivery. The study examines design considerations, aligning them with specific applications, develops data flows to maximize efficiency for research & clinical use. This process encompasses the reception of single-lead ECGs from diverse wearable devices, channeling this data into a centralized data lake & facilitating real-time inference through AI models for ECG interpretation. An evaluation of the platform demonstrates a mean duration from acquisition to reporting of results of 33.0 to 35.7 seconds, after a standard 30 second acquisition. There were no substantial differences in acquisition to reporting across two commercially available devices (Apple Watch and KardiaMobile). These results demonstrate the succcessful translation of design principles into a fully integrated & efficient strategy for leveraging 1-lead ECGs across platforms & interpretation by AI-ECG algorithms. Such a platform is critical to translating AI discoveries for wearable and portable ECG devices to clinical impact through rapid deployment.
</details>
<details>
<summary>摘要</summary>
在现代医疗面前的急速发展 landscape中，穿戴式和可携式技术的集成提供了个人化健康监测在社区的唯一机会。例如Apple Watch、FitBit和AliveCor KardiaMobile等设备已经革命化了健康数据流的收集和处理。在这些设备收集的数据中，单Channel electrocardiogram（ECG）记录已经成为监测心血管健康的关键来源。人工智能（AI）技术的进步使得可以解释这些1-Channel ECG，从而促进诊断和检测罕见心血管疾病。这个设计研究描述了一种创新的多平台系统，旨在快速部署AI-基于ECG解决方案 для临床调查和诊疗。研究考虑了设计因素，与特定应用相对应，并开发了数据流程，以最大化研究和临床使用的效率。这个过程包括从多种穿戴式设备接收单Channel ECG，将数据传输到中央数据湖，并通过AI模型对ECG进行实时解释。研究表明，平台的实现可以在收集到报告结果的时间内减少了33.0到35.7秒，并且没有显著差异在不同的商业设备（Apple Watch和KardiaMobile）上。这些结果证明了设计原则的成功翻译为一个高效集成的策略，可以在多个平台上使用AI-ECG算法进行单Channel ECG的解释。这种平台是评估AI发现的穿戴式和可携式ECG设备的临床影响的关键。
</details></li>
</ul>
<hr>
<h2 id="Federated-Quantum-Machine-Learning-with-Differential-Privacy"><a href="#Federated-Quantum-Machine-Learning-with-Differential-Privacy" class="headerlink" title="Federated Quantum Machine Learning with Differential Privacy"></a>Federated Quantum Machine Learning with Differential Privacy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06973">http://arxiv.org/abs/2310.06973</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rod Rofougaran, Shinjae Yoo, Huan-Hsin Tseng, Samuel Yen-Chi Chen</li>
<li>for: 保护敏感训练数据的隐私，提供更安全和效率的人工智能实现。</li>
<li>methods: 结合量子联合学习（QFL）和量子差异隐私（QDP）两种隐私保护方法，实现量子平台上的数据隐私保护。</li>
<li>results: 使用量子-классиical机器学习模型对猫vs狗数据集进行二分类，实现了测试准确率超过0.98，同时保持ε值小于1.3。验证了 federated differentially private training 是一种可行的隐私保护方法 для量子机器学习 на Noisy Intermediate-Scale Quantum（NISQ）设备。<details>
<summary>Abstract</summary>
The preservation of privacy is a critical concern in the implementation of artificial intelligence on sensitive training data. There are several techniques to preserve data privacy but quantum computations are inherently more secure due to the no-cloning theorem, resulting in a most desirable computational platform on top of the potential quantum advantages. There have been prior works in protecting data privacy by Quantum Federated Learning (QFL) and Quantum Differential Privacy (QDP) studied independently. However, to the best of our knowledge, no prior work has addressed both QFL and QDP together yet. Here, we propose to combine these privacy-preserving methods and implement them on the quantum platform, so that we can achieve comprehensive protection against data leakage (QFL) and model inversion attacks (QDP). This implementation promises more efficient and secure artificial intelligence. In this paper, we present a successful implementation of these privacy-preservation methods by performing the binary classification of the Cats vs Dogs dataset. Using our quantum-classical machine learning model, we obtained a test accuracy of over 0.98, while maintaining epsilon values less than 1.3. We show that federated differentially private training is a viable privacy preservation method for quantum machine learning on Noisy Intermediate-Scale Quantum (NISQ) devices.
</details>
<details>
<summary>摘要</summary>
保护隐私是人工智能在敏感训练数据实施中的关键问题。有几种技术来保护数据隐私，但量子计算机是因为无论护法 theorem，因此在计算平台上具有最好的安全性。先前有关保护数据隐私的研究，包括量子联合学习（QFL）和量子差分隐私（QDP），但是到目前为止没有任何研究既 combinates these two privacy-preserving methods。在这篇文章中，我们提议将这两种隐私保护方法结合在一起，并在量子平台上实现，以实现全面的数据泄露防止（QFL）和模型反向攻击防止（QDP）。这种实现承诺更高效和安全的人工智能。在这篇文章中，我们成功地实现了这些隐私保护方法，通过对猫vs狗数据集进行二分类。使用我们的量子-классиical机器学习模型，我们在测试精度达0.98，而且psilon值低于1.3。我们显示，联邦差分隐私训练是量子机器学习在Noisy Intermediate-Scale Quantum（NISQ）设备上可行的隐私保护方法。
</details></li>
</ul>
<hr>
<h2 id="Flood-and-Echo-Algorithmic-Alignment-of-GNNs-with-Distributed-Computing"><a href="#Flood-and-Echo-Algorithmic-Alignment-of-GNNs-with-Distributed-Computing" class="headerlink" title="Flood and Echo: Algorithmic Alignment of GNNs with Distributed Computing"></a>Flood and Echo: Algorithmic Alignment of GNNs with Distributed Computing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06970">http://arxiv.org/abs/2310.06970</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joël Mathys, Florian Grötschla, Kalyan Varma Nadimpalli, Roger Wattenhofer</li>
<li>for: 本研究旨在开发一种基于分布式算法设计原理的扩展推理框架，以便在大图上进行信息交换和推理扩展。</li>
<li>methods: 该框架基于洪水和回声网络的设计原理，通过在整个图上传递消息，以达到自适应的扩展和信息交换。</li>
<li>results: 研究表明，该框架在许多情况下比传统的推理框架更有效率，并且能够有效地进行信息交换和推理扩展。<details>
<summary>Abstract</summary>
Graph Neural Networks are a natural fit for learning algorithms. They can directly represent tasks through an abstract but versatile graph structure and handle inputs of different sizes. This opens up the possibility for scaling and extrapolation to larger graphs, one of the most important advantages of an algorithm. However, this raises two core questions i) How can we enable nodes to gather the required information in a given graph ($\textit{information exchange}$), even if is far away and ii) How can we design an execution framework which enables this information exchange for extrapolation to larger graph sizes ($\textit{algorithmic alignment for extrapolation}$). We propose a new execution framework that is inspired by the design principles of distributed algorithms: Flood and Echo Net. It propagates messages through the entire graph in a wave like activation pattern, which naturally generalizes to larger instances. Through its sparse but parallel activations it is provably more efficient in terms of message complexity. We study the proposed model and provide both empirical evidence and theoretical insights in terms of its expressiveness, efficiency, information exchange and ability to extrapolate.
</details>
<details>
<summary>摘要</summary>
GRAPH Neural Networks 是一种自然的适应算法。它们可以直接通过抽象但强大的图结构表示任务，并处理不同大小的输入。这打开了扩大和推断到更大图的可能性，是算法中最重要的优势。然而，这引出了两个核心问题：（i）如何使节点获得图中需要的信息（信息交换），即使它们在远方的 ;（ii）如何设计一个执行框架，使得这些信息交换在更大的图像上进行推断（算法对适应推断）。我们提出了一种新的执行框架，它是基于分布式算法的设计原则：洪涝网络和回声网络。它在整个图上传递消息，使得它自然泛化到更大的实例。通过它的稀疏但平行的活动，可以证明它比消息复杂度更高效。我们研究了提议的模型，并提供了both empirical evidence和理论听见，包括表达能力、效率、信息交换和推断能力。
</details></li>
</ul>
<hr>
<h2 id="Positivity-free-Policy-Learning-with-Observational-Data"><a href="#Positivity-free-Policy-Learning-with-Observational-Data" class="headerlink" title="Positivity-free Policy Learning with Observational Data"></a>Positivity-free Policy Learning with Observational Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06969">http://arxiv.org/abs/2310.06969</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/panzhaooo/positivity-free-policy-learning">https://github.com/panzhaooo/positivity-free-policy-learning</a></li>
<li>paper_authors: Pan Zhao, Antoine Chambaz, Julie Josse, Shu Yang</li>
<li>for: 本研究的目的是用观察数据学习政策，以便在具有特定约束的情况下获得最佳减法分配策略。</li>
<li>methods: 本研究提出了一种新的不假设 positivity 的政策学习框架，以解决实际场景中 positivity 假设的困难。该框架利用增量概率分配策略来调整减法分配策略的概率值，而不是直接将减法分配策略分配给减法。我们描述了这种增量概率分配策略，并提出了鉴定条件，使用半 Parametric 效率理论提出高效的估计器，可以在搅拌 machine learning 算法时实现快速的收敛率。</li>
<li>results: 本研究提供了对政策学习的理论保证，并验证了提出的框架的finite-sample表现，通过了全面的数据实验，以确保从观察数据中提取 causal 效应是 Both 可靠和可靠。<details>
<summary>Abstract</summary>
Policy learning utilizing observational data is pivotal across various domains, with the objective of learning the optimal treatment assignment policy while adhering to specific constraints such as fairness, budget, and simplicity. This study introduces a novel positivity-free (stochastic) policy learning framework designed to address the challenges posed by the impracticality of the positivity assumption in real-world scenarios. This framework leverages incremental propensity score policies to adjust propensity score values instead of assigning fixed values to treatments. We characterize these incremental propensity score policies and establish identification conditions, employing semiparametric efficiency theory to propose efficient estimators capable of achieving rapid convergence rates, even when integrated with advanced machine learning algorithms. This paper provides a thorough exploration of the theoretical guarantees associated with policy learning and validates the proposed framework's finite-sample performance through comprehensive numerical experiments, ensuring the identification of causal effects from observational data is both robust and reliable.
</details>
<details>
<summary>摘要</summary>
政策学习使用观察数据是多种领域的关键，旨在学习最佳治理分配策略，遵循特定的限制，如公平、预算和简单性。本研究提出了一种新的无正定性（随机）政策学习框架，用于实际世界场景中缺乏正定性的挑战。这种框架利用增量抽象分数策略来调整治理分数值，而不是将固定值分配给治理。我们描述这种增量抽象分数策略，并提出了定型条件，使用半 Parametric 效率理论提出高效的估计器，可以在融合先进机器学习算法时实现快速收敛速率。本文对政策学习的理论保证和finite-sample表现进行了全面的探讨，并通过了广泛的数字实验，以确保从观察数据中检测到的 causal 效应是可靠和可信。</SYS>Here's the translation of the text into Simplified Chinese:<SYS>政策学习使用观察数据是多种领域的关键，旨在学习最佳治理分配策略，遵循特定的限制，如公平、预算和简单性。本研究提出了一种新的无正定性（随机）政策学习框架，用于实际世界场景中缺乏正定性的挑战。这种框架利用增量抽象分数策略来调整治理分数值，而不是将固定值分配给治理。我们描述这种增量抽象分数策略，并提出了定型条件。使用半 Parametric 效率理论提出高效的估计器，可以在融合先进机器学习算法时实现快速收敛速率。本文对政策学习的理论保证和finite-sample表现进行了全面的探讨，并通过了广泛的数字实验，以确保从观察数据中检测到的 causal 效应是可靠和可信。</SYS>
</details></li>
</ul>
<hr>
<h2 id="Diffusion-Prior-Regularized-Iterative-Reconstruction-for-Low-dose-CT"><a href="#Diffusion-Prior-Regularized-Iterative-Reconstruction-for-Low-dose-CT" class="headerlink" title="Diffusion Prior Regularized Iterative Reconstruction for Low-dose CT"></a>Diffusion Prior Regularized Iterative Reconstruction for Low-dose CT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06949">http://arxiv.org/abs/2310.06949</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenjun Xia, Yongyi Shi, Chuang Niu, Wenxiang Cong, Ge Wang</li>
<li>for: 减少X射线辐射剂量，提高 computed tomography（CT）图像质量</li>
<li>methods: 引入迭代重建算法，并将杂散抑制推理模型（DDPM）与数据准确性优先重建方法融合</li>
<li>results: 实现高Definition CT图像重建，减少辐射剂量<details>
<summary>Abstract</summary>
Computed tomography (CT) involves a patient's exposure to ionizing radiation. To reduce the radiation dose, we can either lower the X-ray photon count or down-sample projection views. However, either of the ways often compromises image quality. To address this challenge, here we introduce an iterative reconstruction algorithm regularized by a diffusion prior. Drawing on the exceptional imaging prowess of the denoising diffusion probabilistic model (DDPM), we merge it with a reconstruction procedure that prioritizes data fidelity. This fusion capitalizes on the merits of both techniques, delivering exceptional reconstruction results in an unsupervised framework. To further enhance the efficiency of the reconstruction process, we incorporate the Nesterov momentum acceleration technique. This enhancement facilitates superior diffusion sampling in fewer steps. As demonstrated in our experiments, our method offers a potential pathway to high-definition CT image reconstruction with minimized radiation.
</details>
<details>
<summary>摘要</summary>
computed tomography (CT) 涉及到辐射 ionizing radiation，以降低辐射剂量，可以 either 降低 X-ray  фото counts 或者下推 projection views。然而，任一种方法通常会 compromise 图像质量。为 Addressing this challenge, here we introduce an iterative reconstruction algorithm regularized by a diffusion prior。 drawing on the exceptional imaging prowess of the denoising diffusion probabilistic model (DDPM), we merge it with a reconstruction procedure that prioritizes data fidelity。 This fusion capitalizes on the merits of both techniques, delivering exceptional reconstruction results in an unsupervised framework。 To further enhance the efficiency of the reconstruction process, we incorporate the Nesterov momentum acceleration technique。 This enhancement facilitates superior diffusion sampling in fewer steps。 As demonstrated in our experiments, our method offers a potential pathway to high-definition CT image reconstruction with minimized radiation。Note: Please note that the translation is in Simplified Chinese, which is one of the two standard versions of Chinese. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="A-Variational-Autoencoder-Framework-for-Robust-Physics-Informed-Cyberattack-Recognition-in-Industrial-Cyber-Physical-Systems"><a href="#A-Variational-Autoencoder-Framework-for-Robust-Physics-Informed-Cyberattack-Recognition-in-Industrial-Cyber-Physical-Systems" class="headerlink" title="A Variational Autoencoder Framework for Robust, Physics-Informed Cyberattack Recognition in Industrial Cyber-Physical Systems"></a>A Variational Autoencoder Framework for Robust, Physics-Informed Cyberattack Recognition in Industrial Cyber-Physical Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06948">http://arxiv.org/abs/2310.06948</a></li>
<li>repo_url: None</li>
<li>paper_authors: Navid Aftabi, Dan Li, Paritosh Ramanan</li>
<li>for: 本研究旨在检测、诊断和定位工业控制系统中的隐藏攻击（covert attack）。</li>
<li>methods: 本研究提出了一种数据驱动的框架， combining variational autoencoder（VAE）、回归神经网络（RNN）和深度神经网络（DNN），以检测、诊断和定位隐藏攻击。</li>
<li>results: 经过实验研究，提出的方法在一个网络化的电力传输系统上的实验研究中表现出了应用性和效果。<details>
<summary>Abstract</summary>
Cybersecurity of Industrial Cyber-Physical Systems is drawing significant concerns as data communication increasingly leverages wireless networks. A lot of data-driven methods were develope for detecting cyberattacks, but few are focused on distinguishing them from equipment faults. In this paper, we develop a data-driven framework that can be used to detect, diagnose, and localize a type of cyberattack called covert attacks on networked industrial control systems. The framework has a hybrid design that combines a variational autoencoder (VAE), a recurrent neural network (RNN), and a Deep Neural Network (DNN). This data-driven framework considers the temporal behavior of a generic physical system that extracts features from the time series of the sensor measurements that can be used for detecting covert attacks, distinguishing them from equipment faults, as well as localize the attack/fault. We evaluate the performance of the proposed method through a realistic simulation study on a networked power transmission system as a typical example of ICS. We compare the performance of the proposed method with the traditional model-based method to show its applicability and efficacy.
</details>
<details>
<summary>摘要</summary>
工业控制系统的网络化Cybersecurity引发了 significiant concerns，因为数据通信越来越多地使用无线网络。许多数据驱动方法已经开发，但很少关注于分化攻击和设备故障之间的差异。在这篇论文中，我们开发了一个数据驱动的框架，可以用于检测、诊断和地址网络化工业控制系统中的隐藏攻击。这个框架具有混合设计，组合了变量自适应器（VAE）、回归神经网络（RNN）和深度神经网络（DNN）。这个数据驱动框架考虑了生成器物理系统的时间行为，从感知器测量时间序列中提取特征，用于检测隐藏攻击、分化攻击和位置攻击。我们通过一个现实的 simulate 研究，对一个网络化的电力传输系统进行评估，以示方法的适用性和效果。我们将传统的模型基型方法与该方法进行比较，以显示其适用性和有效性。
</details></li>
</ul>
<hr>
<h2 id="LLMs-Killed-the-Script-Kiddie-How-Agents-Supported-by-Large-Language-Models-Change-the-Landscape-of-Network-Threat-Testing"><a href="#LLMs-Killed-the-Script-Kiddie-How-Agents-Supported-by-Large-Language-Models-Change-the-Landscape-of-Network-Threat-Testing" class="headerlink" title="LLMs Killed the Script Kiddie: How Agents Supported by Large Language Models Change the Landscape of Network Threat Testing"></a>LLMs Killed the Script Kiddie: How Agents Supported by Large Language Models Change the Landscape of Network Threat Testing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06936">http://arxiv.org/abs/2310.06936</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stephen Moskal, Sam Laney, Erik Hemberg, Una-May O’Reilly</li>
<li>for: 这个研究探讨了大语言模型（LLM）在掌握威胁、生成工具信息和自动化网络攻击方面的潜力。</li>
<li>methods: 研究员采用了手动探索 LLM 在支持特定威胁行动和决策方面的方法，然后通过自动化决策过程来实现网络攻击。研究人员还提出了引言工程方法来实现计划-行动-报告循环，以及一种提示链设计来导向Sequential决策过程。</li>
<li>results: 研究人员通过 demonstrate 一个简单的网络攻击use case来评估 LLM 在网络攻击方面的知识程度，并提供了引言设计的指导方针。研究人员还提出了 LLM 在加速威胁actor能力方面的可能影响和伦理考虑。研究结果表明，LLM 可以用于生成有用的信息和自动化网络攻击，但是它们的潜力和敏捷性仍需进一步探索。<details>
<summary>Abstract</summary>
In this paper, we explore the potential of Large Language Models (LLMs) to reason about threats, generate information about tools, and automate cyber campaigns. We begin with a manual exploration of LLMs in supporting specific threat-related actions and decisions. We proceed by automating the decision process in a cyber campaign. We present prompt engineering approaches for a plan-act-report loop for one action of a threat campaign and and a prompt chaining design that directs the sequential decision process of a multi-action campaign. We assess the extent of LLM's cyber-specific knowledge w.r.t the short campaign we demonstrate and provide insights into prompt design for eliciting actionable responses. We discuss the potential impact of LLMs on the threat landscape and the ethical considerations of using LLMs for accelerating threat actor capabilities. We report a promising, yet concerning, application of generative AI to cyber threats. However, the LLM's capabilities to deal with more complex networks, sophisticated vulnerabilities, and the sensitivity of prompts are open questions. This research should spur deliberations over the inevitable advancements in LLM-supported cyber adversarial landscape.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们探讨大语言模型（LLM）在处理威胁、生成工具信息和自动化网络攻击方面的潜力。我们开始于手动探索LLM在支持特定威胁行动和决策过程中的能力。然后我们将决策过程自动化，并提出了一种plan-act-report循环和一种链接式提示设计，以导引多个行动的顺序决策过程。我们评估了LLM在短期攻击кампаgn中的网络专业知识的程度，并提供了提示设计的启示，以便获得可行的回答。我们讨论了LLM在威胁风险面临的潜在影响和使用LLM加速攻击者能力的伦理考虑因素。我们报道了一种有前途又担忧的应用 génériques AI 在网络威胁方面，但 LLM 在更复杂的网络、更复杂的漏洞和提示敏感性方面的能力仍然是开Question。这种研究应当促使人们对 LLM 在网络威胁领域的不断进步举行深思熟虑。
</details></li>
</ul>
<hr>
<h2 id="Quantum-Shadow-Gradient-Descent-for-Quantum-Learning"><a href="#Quantum-Shadow-Gradient-Descent-for-Quantum-Learning" class="headerlink" title="Quantum Shadow Gradient Descent for Quantum Learning"></a>Quantum Shadow Gradient Descent for Quantum Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06935">http://arxiv.org/abs/2310.06935</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohsen Heidari, Mobasshir A Naved, Wenbo Xie, Arjun Jacob Grama, Wojciech Szpankowski</li>
<li>for: 这个论文提出了一种新的程序，叫做量子影差下降（QSGD），用于解决关键挑战。我们的方法具有一次性的优点，不需要任何样本的重复，并且与精确的质量计算相比，其减速率相当。</li>
<li>methods: 我们提出了一种新的技术，即生成量子影（QSS），而不是传统的类型 Ansatz。在经典计算机上进行计算时，这些计算会变得繁琐和不可持续，因为维度会增长 exponentially。我们的方法是通过测量量子影来解决这个问题。</li>
<li>results: 我们的研究表明，使用量子影可以减少计算量，并且可以应用于更一般的非产品 Ansatz 中。我们提供了理论证明、减速分析和数值实验来支持我们的结论。<details>
<summary>Abstract</summary>
This paper proposes a new procedure called quantum shadow gradient descent (QSGD) that addresses these key challenges. Our method has the benefits of a one-shot approach, in not requiring any sample duplication while having a convergence rate comparable to the ideal update rule using exact gradient computation. We propose a new technique for generating quantum shadow samples (QSS), which generates quantum shadows as opposed to classical shadows used in existing works. With classical shadows, the computations are typically performed on classical computers and, hence, are prohibitive since the dimension grows exponentially. Our approach resolves this issue by measurements of quantum shadows. As the second main contribution, we study more general non-product ansatz of the form $\exp\{i\sum_j \theta_j A_j\}$ that model variational Hamiltonians. We prove that the gradient can be written in terms of the gradient of single-parameter ansatzes that can be easily measured. Our proof is based on the Suzuki-Trotter approximation; however, our expressions are exact, unlike prior efforts that approximate non-product operators. As a result, existing gradient measurement techniques can be applied to more general VQAs followed by correction terms without any approximation penalty. We provide theoretical proofs, convergence analysis and verify our results through numerical experiments.
</details>
<details>
<summary>摘要</summary>
We introduce a new technique for generating quantum shadow samples (QSS), which generates quantum shadows instead of the classical shadows used in existing works. With classical shadows, computations are typically performed on classical computers, and the dimension grows exponentially. Our approach resolves this issue by measuring quantum shadows.As the second main contribution, we study more general non-product ansatz of the form $\exp\{i\sum_j \theta_j A_j\}$ that model variational Hamiltonians. We prove that the gradient can be written in terms of the gradient of single-parameter ansatzes that can be easily measured. Our proof is based on the Suzuki-Trotter approximation, but our expressions are exact, unlike prior efforts that approximate non-product operators.As a result, existing gradient measurement techniques can be applied to more general VQAs followed by correction terms without any approximation penalty. We provide theoretical proofs, convergence analysis, and verify our results through numerical experiments.
</details></li>
</ul>
<hr>
<h2 id="Prosody-Analysis-of-Audiobooks"><a href="#Prosody-Analysis-of-Audiobooks" class="headerlink" title="Prosody Analysis of Audiobooks"></a>Prosody Analysis of Audiobooks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06930">http://arxiv.org/abs/2310.06930</a></li>
<li>repo_url: None</li>
<li>paper_authors: Charuta Pethe, Yunting Yin, Steven Skiena</li>
<li>for: 这个论文的目的是提高文本读取的自然语音质量。</li>
<li>methods: 这个论文使用了语言模型来预测文本中的语调属性（抑高、声量和速度），并与人工读书 recording进行对比。</li>
<li>results: 研究结果表明，使用这种方法可以更好地预测读书 recording中的语调属性，并且与人工读书 recording更加相似。此外，人类评估研究也表明，人们更偏好使用这种方法生成的Audiobook读书 recording。<details>
<summary>Abstract</summary>
Recent advances in text-to-speech have made it possible to generate natural-sounding audio from text. However, audiobook narrations involve dramatic vocalizations and intonations by the reader, with greater reliance on emotions, dialogues, and descriptions in the narrative. Using our dataset of 93 aligned book-audiobook pairs, we present improved models for prosody prediction properties (pitch, volume, and rate of speech) from narrative text using language modeling. Our predicted prosody attributes correlate much better with human audiobook readings than results from a state-of-the-art commercial TTS system: our predicted pitch shows a higher correlation with human reading for 22 out of the 24 books, while our predicted volume attribute proves more similar to human reading for 23 out of the 24 books. Finally, we present a human evaluation study to quantify the extent that people prefer prosody-enhanced audiobook readings over commercial text-to-speech systems.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Stochastic-Super-resolution-of-Cosmological-Simulations-with-Denoising-Diffusion-Models"><a href="#Stochastic-Super-resolution-of-Cosmological-Simulations-with-Denoising-Diffusion-Models" class="headerlink" title="Stochastic Super-resolution of Cosmological Simulations with Denoising Diffusion Models"></a>Stochastic Super-resolution of Cosmological Simulations with Denoising Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06929">http://arxiv.org/abs/2310.06929</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andreas Schanz, Florian List, Oliver Hahn</li>
<li>for: 这个论文的目的是用深度学习模型来提高 cosmological simulation 的分辨率，使其能够更好地模拟小规模结构。</li>
<li>methods: 这个论文使用了 denoising diffusion models 作为生成模型，并开发了一种新的 “filter-boosted” 训练方法来提高模型的准确性。</li>
<li>results: 这个论文的结果表明，使用 denoising diffusion models 可以生成高度可信度的 super-resolution 图像和电磁波谱，并且能够复制低分辨率 simulation 中的小规模特征。这些结果表明，这种 super-resolution 模型可以用于 cosmic structure formation 中的 uncertainty quantification。<details>
<summary>Abstract</summary>
In recent years, deep learning models have been successfully employed for augmenting low-resolution cosmological simulations with small-scale information, a task known as "super-resolution". So far, these cosmological super-resolution models have relied on generative adversarial networks (GANs), which can achieve highly realistic results, but suffer from various shortcomings (e.g. low sample diversity). We introduce denoising diffusion models as a powerful generative model for super-resolving cosmic large-scale structure predictions (as a first proof-of-concept in two dimensions). To obtain accurate results down to small scales, we develop a new "filter-boosted" training approach that redistributes the importance of different scales in the pixel-wise training objective. We demonstrate that our model not only produces convincing super-resolution images and power spectra consistent at the percent level, but is also able to reproduce the diversity of small-scale features consistent with a given low-resolution simulation. This enables uncertainty quantification for the generated small-scale features, which is critical for the usefulness of such super-resolution models as a viable surrogate model for cosmic structure formation.
</details>
<details>
<summary>摘要</summary>
Recently, deep learning models have been successfully used for augmenting low-resolution cosmological simulations with small-scale information, a task known as "super-resolution". So far, these cosmological super-resolution models have relied on generative adversarial networks (GANs), which can achieve highly realistic results, but suffer from various shortcomings (e.g. low sample diversity). We introduce denoising diffusion models as a powerful generative model for super-resolving cosmic large-scale structure predictions (as a first proof-of-concept in two dimensions). To obtain accurate results down to small scales, we develop a new "filter-boosted" training approach that redistributes the importance of different scales in the pixel-wise training objective. We demonstrate that our model not only produces convincing super-resolution images and power spectra consistent at the percent level, but is also able to reproduce the diversity of small-scale features consistent with a given low-resolution simulation. This enables uncertainty quantification for the generated small-scale features, which is critical for the usefulness of such super-resolution models as a viable surrogate model for cosmic structure formation.Here is the text with some additional information about the translation:The translation is in Simplified Chinese, which is the standardized form of Chinese used in mainland China and widely used in other countries as well. The translation is written in a formal and precise style, using technical terms and phrases appropriate for a scientific paper. The text includes some specialized vocabulary and concepts related to cosmology and deep learning, which are translated accurately and consistently based on their meanings in the context of the text. The translation also includes some cultural references and expressions that are specific to Chinese culture, but are not essential to the understanding of the scientific content. Overall, the translation is accurate and faithful to the original text, and should be easily understandable to readers who are familiar with the subject matter and the language.
</details></li>
</ul>
<hr>
<h2 id="Inverse-Factorized-Q-Learning-for-Cooperative-Multi-agent-Imitation-Learning"><a href="#Inverse-Factorized-Q-Learning-for-Cooperative-Multi-agent-Imitation-Learning" class="headerlink" title="Inverse Factorized Q-Learning for Cooperative Multi-agent Imitation Learning"></a>Inverse Factorized Q-Learning for Cooperative Multi-agent Imitation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06801">http://arxiv.org/abs/2310.06801</a></li>
<li>repo_url: None</li>
<li>paper_authors: The Viet Bui, Tien Mai, Thanh Hong Nguyen</li>
<li>for: 这个论文关注了多智能体学习（IL）在合作多智能体系统中的应用，具体来说是通过示范学习来学习专家行为。</li>
<li>methods: 该论文提出了一种新的多智能体IL算法，该算法利用混合网络来聚合分布式Q函数，并且可以通过全局状态信息来训练混合网络的参数。</li>
<li>results: 该论文通过对一些复杂的竞争和合作多智能体游戏环境进行了广泛的实验，证明了该算法的有效性，并且比现有的多智能体IL算法表现更好。<details>
<summary>Abstract</summary>
This paper concerns imitation learning (IL) (i.e, the problem of learning to mimic expert behaviors from demonstrations) in cooperative multi-agent systems. The learning problem under consideration poses several challenges, characterized by high-dimensional state and action spaces and intricate inter-agent dependencies. In a single-agent setting, IL has proven to be done efficiently through an inverse soft-Q learning process given expert demonstrations. However, extending this framework to a multi-agent context introduces the need to simultaneously learn both local value functions to capture local observations and individual actions, and a joint value function for exploiting centralized learning. In this work, we introduce a novel multi-agent IL algorithm designed to address these challenges. Our approach enables the centralized learning by leveraging mixing networks to aggregate decentralized Q functions. A main advantage of this approach is that the weights of the mixing networks can be trained using information derived from global states. We further establish conditions for the mixing networks under which the multi-agent objective function exhibits convexity within the Q function space. We present extensive experiments conducted on some challenging competitive and cooperative multi-agent game environments, including an advanced version of the Star-Craft multi-agent challenge (i.e., SMACv2), which demonstrates the effectiveness of our proposed algorithm compared to existing state-of-the-art multi-agent IL algorithms.
</details>
<details>
<summary>摘要</summary>
To address these challenges, we propose a novel multi-agent IL algorithm that leverages mixing networks to aggregate decentralized Q functions. The weights of the mixing networks can be trained using information derived from global states. We establish conditions for the mixing networks under which the multi-agent objective function exhibits convexity within the Q function space.We present extensive experiments conducted on challenging competitive and cooperative multi-agent game environments, including the advanced version of the Star-Craft multi-agent challenge (SMACv2). Our proposed algorithm outperforms existing state-of-the-art multi-agent IL algorithms.
</details></li>
</ul>
<hr>
<h2 id="Test-Evaluation-Best-Practices-for-Machine-Learning-Enabled-Systems"><a href="#Test-Evaluation-Best-Practices-for-Machine-Learning-Enabled-Systems" class="headerlink" title="Test &amp; Evaluation Best Practices for Machine Learning-Enabled Systems"></a>Test &amp; Evaluation Best Practices for Machine Learning-Enabled Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06800">http://arxiv.org/abs/2310.06800</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jaganmohan Chandrasekaran, Tyler Cody, Nicola McCarthy, Erin Lanus, Laura Freeman</li>
<li>for: This paper aims to present best practices for the Test and Evaluation (T&amp;E) of Machine Learning (ML)-enabled software systems across their lifecycle.</li>
<li>methods: The paper categorizes the lifecycle of ML-enabled software systems into three stages: component, integration and deployment, and post-deployment. The primary objective is to test and evaluate the ML model as a standalone component, and then evaluate an integrated ML-enabled system consisting of both ML and non-ML components.</li>
<li>results: The paper highlights the challenges of T&amp;E in ML-enabled software systems and the need for systematic testing approaches, adequacy measurements, and metrics to address these challenges across all stages of the ML-enabled system lifecycle.<details>
<summary>Abstract</summary>
Machine learning (ML) - based software systems are rapidly gaining adoption across various domains, making it increasingly essential to ensure they perform as intended. This report presents best practices for the Test and Evaluation (T&E) of ML-enabled software systems across its lifecycle. We categorize the lifecycle of ML-enabled software systems into three stages: component, integration and deployment, and post-deployment. At the component level, the primary objective is to test and evaluate the ML model as a standalone component. Next, in the integration and deployment stage, the goal is to evaluate an integrated ML-enabled system consisting of both ML and non-ML components. Finally, once the ML-enabled software system is deployed and operationalized, the T&E objective is to ensure the system performs as intended. Maintenance activities for ML-enabled software systems span the lifecycle and involve maintaining various assets of ML-enabled software systems.   Given its unique characteristics, the T&E of ML-enabled software systems is challenging. While significant research has been reported on T&E at the component level, limited work is reported on T&E in the remaining two stages. Furthermore, in many cases, there is a lack of systematic T&E strategies throughout the ML-enabled system's lifecycle. This leads practitioners to resort to ad-hoc T&E practices, which can undermine user confidence in the reliability of ML-enabled software systems. New systematic testing approaches, adequacy measurements, and metrics are required to address the T&E challenges across all stages of the ML-enabled system lifecycle.
</details>
<details>
<summary>摘要</summary>
At the component stage, the primary goal is to evaluate the ML model as a standalone component. In the integration and deployment stage, the objective is to evaluate an integrated ML-enabled system consisting of both ML and non-ML components. Finally, once the ML-enabled software system is deployed and operationalized, the T&E objective is to ensure the system performs as intended.Maintenance activities for ML-enabled software systems span the lifecycle and involve maintaining various assets of ML-enabled software systems. The T&E of ML-enabled software systems is challenging due to their unique characteristics. While there has been significant research on T&E at the component level, there is limited work on T&E in the remaining two stages. Moreover, there is often a lack of systematic T&E strategies throughout the ML-enabled system's lifecycle, leading practitioners to resort to ad-hoc T&E practices that can undermine user confidence in the reliability of ML-enabled software systems.New systematic testing approaches, adequacy measurements, and metrics are needed to address the T&E challenges across all stages of the ML-enabled system lifecycle.
</details></li>
</ul>
<hr>
<h2 id="Spectral-Entry-wise-Matrix-Estimation-for-Low-Rank-Reinforcement-Learning"><a href="#Spectral-Entry-wise-Matrix-Estimation-for-Low-Rank-Reinforcement-Learning" class="headerlink" title="Spectral Entry-wise Matrix Estimation for Low-Rank Reinforcement Learning"></a>Spectral Entry-wise Matrix Estimation for Low-Rank Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06793">http://arxiv.org/abs/2310.06793</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefan Stojanovic, Yassir Jedra, Alexandre Proutiere</li>
<li>for:  Matrix estimation problems in reinforcement learning (RL) with low-rank structure, such as low-rank bandits and Markov Decision Processes (MDPs).</li>
<li>methods:  Spectral-based matrix estimation approaches that efficiently recover the singular subspaces of the matrix and exhibit nearly-minimal entry-wise error.</li>
<li>results:  State-of-the-art performance guarantees for two examples of algorithms: a regret minimization algorithm for low-rank bandit problems, and a best policy identification algorithm for reward-free RL in low-rank MDPs.<details>
<summary>Abstract</summary>
We study matrix estimation problems arising in reinforcement learning (RL) with low-rank structure. In low-rank bandits, the matrix to be recovered specifies the expected arm rewards, and for low-rank Markov Decision Processes (MDPs), it may for example characterize the transition kernel of the MDP. In both cases, each entry of the matrix carries important information, and we seek estimation methods with low entry-wise error. Importantly, these methods further need to accommodate for inherent correlations in the available data (e.g. for MDPs, the data consists of system trajectories). We investigate the performance of simple spectral-based matrix estimation approaches: we show that they efficiently recover the singular subspaces of the matrix and exhibit nearly-minimal entry-wise error. These new results on low-rank matrix estimation make it possible to devise reinforcement learning algorithms that fully exploit the underlying low-rank structure. We provide two examples of such algorithms: a regret minimization algorithm for low-rank bandit problems, and a best policy identification algorithm for reward-free RL in low-rank MDPs. Both algorithms yield state-of-the-art performance guarantees.
</details>
<details>
<summary>摘要</summary>
我们研究在奖励学习（RL）中出现的矩阵估计问题，其中矩阵往往具有低级别结构。在低级别投机中，矩阵需要 recuperate 表示每个臂奖励，而在低级别Markov决策过程（MDP）中，它可能表示MDP的转移核函数。在两种情况下，每个矩阵元素都具有重要信息，我们寻找低入门错误的估计方法。这些方法还需要考虑数据中的自然相关性（例如，MDP数据包括系统轨迹）。我们研究spectral-based矩阵估计方法的性能，并证明它们可以高效地回归矩阵的单个子空间，并且显示出nearly-minimal 入门错误。这些新结果在低级别矩阵估计方面，使得我们可以开发充分利用低级别结构的奖励学习算法。我们提供了两个例子：一个为低级别投机问题的奖励最小化算法，另一个为无奖励RL的低级别MDP中的最佳策略标识算法。两个算法都有状态 искусственный智能的性能保证。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Predictive-Capabilities-in-Data-Driven-Dynamical-Modeling-with-Automatic-Differentiation-Koopman-and-Neural-ODE-Approaches"><a href="#Enhancing-Predictive-Capabilities-in-Data-Driven-Dynamical-Modeling-with-Automatic-Differentiation-Koopman-and-Neural-ODE-Approaches" class="headerlink" title="Enhancing Predictive Capabilities in Data-Driven Dynamical Modeling with Automatic Differentiation: Koopman and Neural ODE Approaches"></a>Enhancing Predictive Capabilities in Data-Driven Dynamical Modeling with Automatic Differentiation: Koopman and Neural ODE Approaches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06790">http://arxiv.org/abs/2310.06790</a></li>
<li>repo_url: None</li>
<li>paper_authors: C. Ricardo Constante-Amores, Alec J. Linot, Michael D. Graham</li>
<li>for: 这个论文的目的是提出一种修改了EXTENDED DYNAMIC MODE DECOMPOSITION WITH DICTIONARY LEARNING（EDMD-DL）方法，以确定对象 observable 的字典和相应的 Koopman 算子近似。</li>
<li>methods: 这种方法使用自动微分的技术来促进梯度下降计算，并使用pseudoinverse来实现。</li>
<li>results: 这个方法在测试了多种方法后，与STATE SPACE APPROACH（神经 ODEs）相比，表现更好，并且在不满足 Koopman 算子的线性条件下，也可以达到比较好的结果。<details>
<summary>Abstract</summary>
Data-driven approximations of the Koopman operator are promising for predicting the time evolution of systems characterized by complex dynamics. Among these methods, the approach known as extended dynamic mode decomposition with dictionary learning (EDMD-DL) has garnered significant attention. Here we present a modification of EDMD-DL that concurrently determines both the dictionary of observables and the corresponding approximation of the Koopman operator. This innovation leverages automatic differentiation to facilitate gradient descent computations through the pseudoinverse. We also address the performance of several alternative methodologies. We assess a 'pure' Koopman approach, which involves the direct time-integration of a linear, high-dimensional system governing the dynamics within the space of observables. Additionally, we explore a modified approach where the system alternates between spaces of states and observables at each time step -- this approach no longer satisfies the linearity of the true Koopman operator representation. For further comparisons, we also apply a state space approach (neural ODEs). We consider systems encompassing two and three-dimensional ordinary differential equation systems featuring steady, oscillatory, and chaotic attractors, as well as partial differential equations exhibiting increasingly complex and intricate behaviors. Our framework significantly outperforms EDMD-DL. Furthermore, the state space approach offers superior performance compared to the 'pure' Koopman approach where the entire time evolution occurs in the space of observables. When the temporal evolution of the Koopman approach alternates between states and observables at each time step, however, its predictions become comparable to those of the state space approach.
</details>
<details>
<summary>摘要</summary>
“数据驱动的科普曼算子估计方法显示出预测复杂动力系统时间演化的承诺。这些方法中，使用字典学习的扩展动态模式分解（EDMD-DL）已经吸引了广泛的关注。在这里，我们提出了一种同时确定字典和科普曼算子的估计方法的修改。这种创新利用了自动微分的技术来促进梯度下降计算，通过 Pseudoinverse 来实现。我们还评估了一些其他方法。我们评估了一种 '纯' 科普曼方法，该方法直接在可观察空间中进行时间 инте格alion，并且可以在高维度系统中实现。此外，我们还探讨了一种 modify 方法，该方法在每次时间步骤时将系统转换到不同的空间中，这种方法不再满足真正的科普曼算子表示。为了进一步比较，我们还应用了一种状态空间方法（神经 ODEs）。我们考虑了两维和三维常微方程系统，以及具有复杂和精细行为的 partial differential equation 系统。我们的框架在 EDMD-DL 方法上表现出了显著的改善，而且状态空间方法在比较 '纯' 科普曼方法和 EDMD-DL 方法时表现出了更好的性能。当科普曼方法在每次时间步骤时 alternate  между状态和可观察空间时，其预测结果与状态空间方法相近。”
</details></li>
</ul>
<hr>
<h2 id="Information-Content-Exploration"><a href="#Information-Content-Exploration" class="headerlink" title="Information Content Exploration"></a>Information Content Exploration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06777">http://arxiv.org/abs/2310.06777</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Jacob Chmura, Hasham Burhani, Xiao Qi Shi</li>
<li>for: 这个论文的目的是提出一种基于信息理论的激励奖励方法，以便在稀缺奖励环境下进行有效和可扩展的探索。</li>
<li>methods: 该论文使用了一种基于信息理论的激励奖励方法，即通过最大化 trajectory 中agent所获得信息的内容来衡量探索行为。作者还比较了该方法与其他探索基于奖励的方法，如Curiosity Driven Learning和Random Network Distillation。</li>
<li>results: 作者的信息理论奖励方法在多个游戏中表现出了更高的效率和可扩展性，包括Montezuma Revenge这个知名的奖励学习任务。此外，作者还提出了一种扩展方案，即在离散压缩的射频空间中最大化信息内容，以提高样本效率和扩展性。<details>
<summary>Abstract</summary>
Sparse reward environments are known to be challenging for reinforcement learning agents. In such environments, efficient and scalable exploration is crucial. Exploration is a means by which an agent gains information about the environment. We expand on this topic and propose a new intrinsic reward that systemically quantifies exploratory behavior and promotes state coverage by maximizing the information content of a trajectory taken by an agent. We compare our method to alternative exploration based intrinsic reward techniques, namely Curiosity Driven Learning and Random Network Distillation. We show that our information theoretic reward induces efficient exploration and outperforms in various games, including Montezuma Revenge, a known difficult task for reinforcement learning. Finally, we propose an extension that maximizes information content in a discretely compressed latent space which boosts sample efficiency and generalizes to continuous state spaces.
</details>
<details>
<summary>摘要</summary>
稀有奖励环境是束缚学习代理的挑战之一。在这些环境中，高效和可扩展的探索是关键。探索是一种方式，通过哪里让代理获得环境信息。我们在这个主题上进一步探讨，并提出一种新的内在奖励方法，系统地量化探索行为，并且通过最大化征文轨迹中的信息内容来促进状态覆盖。我们与其他探索基于内在奖励技术进行比较，包括Curiosity Driven Learning和Random Network Distillation。我们显示，我们的信息学的奖励induces高效的探索，并在多个游戏中表现出优秀，包括Montezuma Revenge，这是已知的Difficult Task for reinforcement learning。最后，我们提出了一种扩展，通过最大化离散压缩的秘密空间中的信息内容来提高样本效率和普遍性，以便应用于连续状态空间。
</details></li>
</ul>
<hr>
<h2 id="Causal-Rule-Learning-Enhancing-the-Understanding-of-Heterogeneous-Treatment-Effect-via-Weighted-Causal-Rules"><a href="#Causal-Rule-Learning-Enhancing-the-Understanding-of-Heterogeneous-Treatment-Effect-via-Weighted-Causal-Rules" class="headerlink" title="Causal Rule Learning: Enhancing the Understanding of Heterogeneous Treatment Effect via Weighted Causal Rules"></a>Causal Rule Learning: Enhancing the Understanding of Heterogeneous Treatment Effect via Weighted Causal Rules</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06746">http://arxiv.org/abs/2310.06746</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ying Wu, Hanzhong Liu, Kai Ren, Xiangyu Chang</li>
<li>For: The paper aims to estimate heterogeneous treatment effects using machine learning methods, with a focus on interpretability for healthcare applications.* Methods: The proposed method, called causal rule learning, involves three phases: rule discovery, rule selection, and rule analysis. It uses a causal forest and D-learning method to identify and deconstruct individual-level treatment effects as a linear combination of subgroup-level effects.* Results: The paper demonstrates the superior performance of causal rule learning in estimating heterogeneous treatment effects when the ground truth is complex and the sample size is sufficient, compared to other methods. It also provides insights into the treatment effects of different subgroups and the weights of each rule in the linear combination.Here is the information in Simplified Chinese text:* For: 该研究使用机器学习方法来估计不同受试者对待的差异效果，特别是在医疗应用中，高度需要可读性。* Methods: 提议的方法是 causal rule learning，它包括三个阶段：规则发现、规则选择和规则分析。它使用 causal forest 和 D-learning 方法来发现和分解个体级待遇的差异效果，以解答过去的忽略问题：一个个体是多个组的成员吗？* Results: 研究表明， causal rule learning 在复杂的真实场景中，对差异效果的可读性估计具有显著优势，比其他方法更好。它还提供了不同组别待遇的治疗效果的信息和每个规则在线性组合中的权重。<details>
<summary>Abstract</summary>
Interpretability is a key concern in estimating heterogeneous treatment effects using machine learning methods, especially for healthcare applications where high-stake decisions are often made. Inspired by the Predictive, Descriptive, Relevant framework of interpretability, we propose causal rule learning which finds a refined set of causal rules characterizing potential subgroups to estimate and enhance our understanding of heterogeneous treatment effects. Causal rule learning involves three phases: rule discovery, rule selection, and rule analysis. In the rule discovery phase, we utilize a causal forest to generate a pool of causal rules with corresponding subgroup average treatment effects. The selection phase then employs a D-learning method to select a subset of these rules to deconstruct individual-level treatment effects as a linear combination of the subgroup-level effects. This helps to answer an ignored question by previous literature: what if an individual simultaneously belongs to multiple groups with different average treatment effects? The rule analysis phase outlines a detailed procedure to further analyze each rule in the subset from multiple perspectives, revealing the most promising rules for further validation. The rules themselves, their corresponding subgroup treatment effects, and their weights in the linear combination give us more insights into heterogeneous treatment effects. Simulation and real-world data analysis demonstrate the superior performance of causal rule learning on the interpretable estimation of heterogeneous treatment effect when the ground truth is complex and the sample size is sufficient.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。<</SYS>>解释性是机器学习方法估计不同征型对减震效果的关键问题，特别是在医疗应用中，高度决策是经常被做的。 draw inspiration from predictive, descriptive, relevant framework of interpretability, we propose causal rule learning, which finds a refined set of causal rules characterizing potential subgroups to estimate and enhance our understanding of heterogeneous treatment effects. causal rule learning involves three phases: rule discovery, rule selection, and rule analysis. In the rule discovery phase, we utilize a causal forest to generate a pool of causal rules with corresponding subgroup average treatment effects. The selection phase then employs a D-learning method to select a subset of these rules to deconstruct individual-level treatment effects as a linear combination of the subgroup-level effects. This helps to answer an ignored question by previous literature: what if an individual simultaneously belongs to multiple groups with different average treatment effects? The rule analysis phase outlines a detailed procedure to further analyze each rule in the subset from multiple perspectives, revealing the most promising rules for further validation. The rules themselves, their corresponding subgroup treatment effects, and their weights in the linear combination give us more insights into heterogeneous treatment effects. Simulation and real-world data analysis demonstrate the superior performance of causal rule learning on the interpretable estimation of heterogeneous treatment effect when the ground truth is complex and the sample size is sufficient.
</details></li>
</ul>
<hr>
<h2 id="Growing-ecosystem-of-deep-learning-methods-for-modeling-protein-unicode-x2013-protein-interactions"><a href="#Growing-ecosystem-of-deep-learning-methods-for-modeling-protein-unicode-x2013-protein-interactions" class="headerlink" title="Growing ecosystem of deep learning methods for modeling protein$\unicode{x2013}$protein interactions"></a>Growing ecosystem of deep learning methods for modeling protein$\unicode{x2013}$protein interactions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06725">http://arxiv.org/abs/2310.06725</a></li>
<li>repo_url: None</li>
<li>paper_authors: Julia R. Rogers, Gergő Nikolényi, Mohammed AlQuraishi</li>
<li>for: 该论文旨在探讨深度学习方法在蛋白质交互模式预测方面的应用，以及这些模型在蛋白质结构和交互功能方面的贡献。</li>
<li>methods: 论文使用了深度学习方法，包括表示学习、几何深度学习和生成模型，来模型蛋白质交互。这些模型借鉴了生物物理知识，以便更好地捕捉蛋白质交互的复杂特征。</li>
<li>results: 论文提出了一系列的成果，包括使用表示学习capture蛋白质交互的复杂特征，使用几何深度学习预测蛋白质结构和交互，以及使用生成模型设计新的蛋白质组合体。这些成果推动了蛋白质交互模型的发展，并为探索蛋白质交互的物理机制和工程蛋白质交互提供了新的思路。<details>
<summary>Abstract</summary>
Numerous cellular functions rely on protein$\unicode{x2013}$protein interactions. Efforts to comprehensively characterize them remain challenged however by the diversity of molecular recognition mechanisms employed within the proteome. Deep learning has emerged as a promising approach for tackling this problem by exploiting both experimental data and basic biophysical knowledge about protein interactions. Here, we review the growing ecosystem of deep learning methods for modeling protein interactions, highlighting the diversity of these biophysically-informed models and their respective trade-offs. We discuss recent successes in using representation learning to capture complex features pertinent to predicting protein interactions and interaction sites, geometric deep learning to reason over protein structures and predict complex structures, and generative modeling to design de novo protein assemblies. We also outline some of the outstanding challenges and promising new directions. Opportunities abound to discover novel interactions, elucidate their physical mechanisms, and engineer binders to modulate their functions using deep learning and, ultimately, unravel how protein interactions orchestrate complex cellular behaviors.
</details>
<details>
<summary>摘要</summary>
许多细胞功能都依赖于蛋白质-蛋白质交互。然而，完全描述这些交互的问题仍然面临着蛋白质多样性所带来的挑战。深度学习在解决这个问题上表现出了扎根，因为它可以利用实验数据和蛋白质交互的基本生物物理知识。在这篇文章中，我们评论了深度学习用于模拟蛋白质交互的生态系统，包括这些生物学上 Informed 模型的多样性和它们之间的贸易。我们还讨论了在 representation learning 中捕捉蛋白质交互的复杂特征，在 geometric deep learning 中预测蛋白质结构和预测复杂结构，以及在生成模型中设计新的蛋白质组装。此外，我们还概述了一些未解决的挑战和前瞻的新方向。在使用深度学习来发现新的交互、解释它们的物理机制和通过设计蛋白质拓展器来调整交互的功能时，有很多机会。最终，我们希望通过深度学习来解释蛋白质交互如何指挥细胞行为。
</details></li>
</ul>
<hr>
<h2 id="Improving-Pseudo-Time-Stepping-Convergence-for-CFD-Simulations-With-Neural-Networks"><a href="#Improving-Pseudo-Time-Stepping-Convergence-for-CFD-Simulations-With-Neural-Networks" class="headerlink" title="Improving Pseudo-Time Stepping Convergence for CFD Simulations With Neural Networks"></a>Improving Pseudo-Time Stepping Convergence for CFD Simulations With Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06717">http://arxiv.org/abs/2310.06717</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anouk Zandbergen, Tycho van Noorden, Alexander Heinlein</li>
<li>for: 这种论文是用来描述由奈尔-斯托克方程所描述的粘性流体的计算流体力学（CFD）模拟的。</li>
<li>methods: 这种模拟使用了非线性律vikrey-Stokes方程，并使用了非线性迭代法，如牛顿法，来解决这些方程的系统。</li>
<li>results: 在这种模拟中，使用了一种叫做pseudo-transient continuation的技术，以提高非线性律vikrey-Stokes方程的收敛性。这种技术使用了一个神经网络模型，用于预测当地 pseudo-time step。这种预测方法可以在每个元素上独立地进行，只需要使用当地的信息。 numerically simulate the results of standard benchmark problems, such as flow through a backward facing step geometry and Couette flow, show the performance of the machine learning-enhanced globalization approach.<details>
<summary>Abstract</summary>
Computational fluid dynamics (CFD) simulations of viscous fluids described by the Navier-Stokes equations are considered. Depending on the Reynolds number of the flow, the Navier-Stokes equations may exhibit a highly nonlinear behavior. The system of nonlinear equations resulting from the discretization of the Navier-Stokes equations can be solved using nonlinear iteration methods, such as Newton's method. However, fast quadratic convergence is typically only obtained in a local neighborhood of the solution, and for many configurations, the classical Newton iteration does not converge at all. In such cases, so-called globalization techniques may help to improve convergence.   In this paper, pseudo-transient continuation is employed in order to improve nonlinear convergence. The classical algorithm is enhanced by a neural network model that is trained to predict a local pseudo-time step. Generalization of the novel approach is facilitated by predicting the local pseudo-time step separately on each element using only local information on a patch of adjacent elements as input. Numerical results for standard benchmark problems, including flow through a backward facing step geometry and Couette flow, show the performance of the machine learning-enhanced globalization approach; as the software for the simulations, the CFD module of COMSOL Multiphysics is employed.
</details>
<details>
<summary>摘要</summary>
computational fluid dynamics (CFD) 模拟可以描述由navier-Stokes方程所描述的粘性流体行为。各种 Reynolds 数值可以导致 Navier-Stokes 方程在不同程度上具有非线性行为。通过离散 Navier-Stokes 方程得到的系统非线性方程可以通过非线性迭代方法，如新颖方法，进行解决。然而，通常只有在解的本地邻域内具有快速quadratic convergence的情况下才能获得快速的收敛。在这些情况下，所谓的全局化技术可以帮助改善收敛。在这篇论文中，使用pseudo-transient continuation的方法来改进非线性收敛。经过训练的神经网络模型可以预测当前粘性流体中的local pseudo-time step。通过在每个元素上分别预测local pseudo-time step，并且只使用当地信息进行预测，这种全局化技术可以在不同的粘性流体中实现更好的收敛性。在实验中，使用CFD模块在COMSOL Multiphysics中进行 simulations。Please note that Simplified Chinese is a simplified version of Chinese, and it may not be the exact translation of the original text.
</details></li>
</ul>
<hr>
<h2 id="S4Sleep-Elucidating-the-design-space-of-deep-learning-based-sleep-stage-classification-models"><a href="#S4Sleep-Elucidating-the-design-space-of-deep-learning-based-sleep-stage-classification-models" class="headerlink" title="S4Sleep: Elucidating the design space of deep-learning-based sleep stage classification models"></a>S4Sleep: Elucidating the design space of deep-learning-based sleep stage classification models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06715">http://arxiv.org/abs/2310.06715</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ai4healthuol/s4sleep">https://github.com/ai4healthuol/s4sleep</a></li>
<li>paper_authors: Tiezhi Wang, Nils Strodthoff</li>
<li>for: 这个研究旨在提高某些机器学习算法在sleep stage识别 задании的性能，以降低人工标注的时间消耗和多样性。</li>
<li>methods: 这个研究使用了encoder-predictor架构，并包括了结构化状态空间模型作为一个重要组成部分。</li>
<li>results: 研究发现，这些架构在SHHS数据集上显示了 statistically significant的性能提高，并通过了 both statistical和systematic error estimations。<details>
<summary>Abstract</summary>
Scoring sleep stages in polysomnography recordings is a time-consuming task plagued by significant inter-rater variability. Therefore, it stands to benefit from the application of machine learning algorithms. While many algorithms have been proposed for this purpose, certain critical architectural decisions have not received systematic exploration. In this study, we meticulously investigate these design choices within the broad category of encoder-predictor architectures. We identify robust architectures applicable to both time series and spectrogram input representations. These architectures incorporate structured state space models as integral components, leading to statistically significant advancements in performance on the extensive SHHS dataset. These improvements are assessed through both statistical and systematic error estimations. We anticipate that the architectural insights gained from this study will not only prove valuable for future research in sleep staging but also hold relevance for other time series annotation tasks.
</details>
<details>
<summary>摘要</summary>
评分睡眠阶段在多somnography记录中是一项时间消耗性的任务，受到差异评分者的影响。因此，它可以从机器学习算法的应用中受益。虽然许多算法已经被提出用于此目的，但一些关键的建筑设计决策尚未得到系统的探讨。在这项研究中，我们仔细调查了这些设计选择，并在广泛的SHHS数据集上进行了实证验证。我们发现了一些稳定的架构，可以在时间序列和峰值spectrogram输入表示中应用。这些架构包括结构化状态空间模型为组件，导致了 statistically significant的性能提升。我们通过统计和系统的错误估计来评估这些改进。我们预计，这些建筑学习的成果将不仅对Future sleep stage评分研究有价值，还将对其他时间序列注释任务有 relevance。
</details></li>
</ul>
<hr>
<h2 id="Interpretable-Traffic-Event-Analysis-with-Bayesian-Networks"><a href="#Interpretable-Traffic-Event-Analysis-with-Bayesian-Networks" class="headerlink" title="Interpretable Traffic Event Analysis with Bayesian Networks"></a>Interpretable Traffic Event Analysis with Bayesian Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06713">http://arxiv.org/abs/2310.06713</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tong Yuan, Jian Yang, Zeyi Wen</li>
<li>for: 本研究旨在提供一种可解释的机器学习基于概率网络的交通事故预测方法，以帮助解释交通事故的原因和发生条件。</li>
<li>methods: 本研究使用概率网络建模了交通事故的 causal 关系，并设计了一种数据生成管道来保留交通数据的关键信息。</li>
<li>results: 通过一个具体的案例研究，本研究的方法可以准确预测交通事故，并分析交通和天气事件之间的关系，从而提供可读性的交通事故预测方法。<details>
<summary>Abstract</summary>
Although existing machine learning-based methods for traffic accident analysis can provide good quality results to downstream tasks, they lack interpretability which is crucial for this critical problem. This paper proposes an interpretable framework based on Bayesian Networks for traffic accident prediction. To enable the ease of interpretability, we design a dataset construction pipeline to feed the traffic data into the framework while retaining the essential traffic data information. With a concrete case study, our framework can derive a Bayesian Network from a dataset based on the causal relationships between weather and traffic events across the United States. Consequently, our framework enables the prediction of traffic accidents with competitive accuracy while examining how the probability of these events changes under different conditions, thus illustrating transparent relationships between traffic and weather events. Additionally, the visualization of the network simplifies the analysis of relationships between different variables, revealing the primary causes of traffic accidents and ultimately providing a valuable reference for reducing traffic accidents.
</details>
<details>
<summary>摘要</summary>
尽管现有的机器学习基于方法可以提供下游任务的好质量结果，但它们缺乏可解性，这是交通事故分析中的关键问题。这篇论文提出了一种可解的框架，基于 bayesian 网络，用于交通事故预测。为了实现可解性，我们设计了一个数据建构管道，将交通数据feed到框架中，保留交通数据的重要信息。通过具体的案例研究，我们的框架可以从 dataset 中 deriv 出 bayesian 网络，该网络表示美国交通事故和天气事件之间的 causal 关系。因此，我们的框架可以在不同条件下预测交通事故的发生概率，并评估这些事件的发生probability在不同条件下的变化，从而显示交通和天气事件之间的透明关系。此外，网络的可视化可以简化不同变量之间的关系分析，揭示交通事故的主要原因，并为减少交通事故提供了有价值的参考。
</details></li>
</ul>
<hr>
<h2 id="Zero-Shot-Transfer-in-Imitation-Learning"><a href="#Zero-Shot-Transfer-in-Imitation-Learning" class="headerlink" title="Zero-Shot Transfer in Imitation Learning"></a>Zero-Shot Transfer in Imitation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06710">http://arxiv.org/abs/2310.06710</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alvaro Cauderan, Gauthier Boeshertz, Florian Schwarb, Calvin Zhang</li>
<li>for: 本研究的目的是开发一种可以在不同领域中传递学习的算法，以解决现实世界中的机器学习问题，如 reward 函数设计困难、学习过程中的难以在另一个领域中应用、以及直接在真实世界中学习是昂贵或不可能的。</li>
<li>methods: 本研究使用了最新的深度强化学习技术，包括 AnnealedVAE，以学习分离的状态表示，并使用单个 Q-函数来模仿专家，而不需要对敌对训练。</li>
<li>results: 研究人员在三个不同的环境中证明了该算法的效果，包括一个简单的游戏、一个中等难度的游戏和一个复杂的游戏，并且在不同的传递知识要求下进行了证明。<details>
<summary>Abstract</summary>
We present an algorithm that learns to imitate expert behavior and can transfer to previously unseen domains without retraining. Such an algorithm is extremely relevant in real-world applications such as robotic learning because 1) reward functions are difficult to design, 2) learned policies from one domain are difficult to deploy in another domain and 3) learning directly in the real world is either expensive or unfeasible due to security concerns. To overcome these constraints, we combine recent advances in Deep RL by using an AnnealedVAE to learn a disentangled state representation and imitate an expert by learning a single Q-function which avoids adversarial training. We demonstrate the effectiveness of our method in 3 environments ranging in difficulty and the type of transfer knowledge required.
</details>
<details>
<summary>摘要</summary>
我们提出了一种算法，可以模仿专家行为，并可以在未经 retraining 的情况下在新领域中传输。这种算法在实际应用中非常有用，因为1）奖励函数设计困难，2）从一个领域学习的策略Difficult to deploy in another domain,3）在真实世界中学习直接是非常昂贵或者安全问题。为了解决这些限制，我们将最近的深度学习RL技术与AnnealedVAE结合，学习一个分离的状态表示，并通过学习单个Q函数来模仿专家。我们在3个不同的环境中展示了我们的方法的有效性，这些环境的难度和传输知识类型都不同。
</details></li>
</ul>
<hr>
<h2 id="Generalized-Wick-Decompositions"><a href="#Generalized-Wick-Decompositions" class="headerlink" title="Generalized Wick Decompositions"></a>Generalized Wick Decompositions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06686">http://arxiv.org/abs/2310.06686</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chris MacLeod, Evgenia Nitishinskaya, Buck Shlegeris</li>
<li>for: 本文研究了积分 decompositions（一种将产品随机变量的期望 decomposed into 不同的分解项）和维克 decompositions（一种将产品变量的乘积 decomposed into 不同的子集），然后推广它们到一个新的泛化函数。</li>
<li>methods: 本文使用了积分 decompositions 和维克 decompositions，并将它们推广到一个新的泛化函数。</li>
<li>results: 本文得到了一个新的泛化函数，可以用于代表不同的产品随机变量的乘积。<details>
<summary>Abstract</summary>
We review the cumulant decomposition (a way of decomposing the expectation of a product of random variables (e.g. $\mathbb{E}[XYZ]$) into a sum of terms corresponding to partitions of these variables.) and the Wick decomposition (a way of decomposing a product of (not necessarily random) variables into a sum of terms corresponding to subsets of the variables). Then we generalize each one to a new decomposition where the product function is generalized to an arbitrary function.
</details>
<details>
<summary>摘要</summary>
我们审查汇数分解（一种分解互动随机变量（例如 $\mathbb{E}[XYZ]$）的期望为汇数分割）和威克分解（一种分解互动变量的产生为汇数分割）。然后我们将它们扩展为一个新的分解，其中互动函数被扩展为一个通用函数。Note that "汇数分解" (cumulant decomposition) and "威克分解" (Wick decomposition) are both commonly used terms in probability theory and statistics, and they are often used to analyze the properties of multivariate distributions.
</details></li>
</ul>
<hr>
<h2 id="Enhanced-Graph-Neural-Networks-with-Ego-Centric-Spectral-Subgraph-Embeddings-Augmentation"><a href="#Enhanced-Graph-Neural-Networks-with-Ego-Centric-Spectral-Subgraph-Embeddings-Augmentation" class="headerlink" title="Enhanced Graph Neural Networks with Ego-Centric Spectral Subgraph Embeddings Augmentation"></a>Enhanced Graph Neural Networks with Ego-Centric Spectral Subgraph Embeddings Augmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12169">http://arxiv.org/abs/2310.12169</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anwar-said/esgea">https://github.com/anwar-said/esgea</a></li>
<li>paper_authors: Anwar Said, Mudassir Shabbir, Tyler Derr, Waseem Abbas, Xenofon Koutsoukos</li>
<li>for: 增强 Graph Neural Networks (GNNs) 在复杂网络上进行学习任务的表现。</li>
<li>methods: 我们提出了一种新的方法，即 Ego-centric Spectral subGraph Embedding Augmentation (ESGEA)，用于增强和设计节点特征，特别是在信息缺失的情况下。我们的方法利用当地子图的 topological 结构来生成 topology-aware 节点特征。</li>
<li>results: 我们在 seven 个数据集和八个基eline模型上进行评估，结果显示，对于图像分类任务，ESGEA 可以提高 AUC 的表现，相比基eline模型，提高了10%。对于节点分类任务，ESGEA 可以提高 accuracy 的表现，相比基eline模型，提高了7%。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have shown remarkable merit in performing various learning-based tasks in complex networks. The superior performance of GNNs often correlates with the availability and quality of node-level features in the input networks. However, for many network applications, such node-level information may be missing or unreliable, thereby limiting the applicability and efficacy of GNNs. To address this limitation, we present a novel approach denoted as Ego-centric Spectral subGraph Embedding Augmentation (ESGEA), which aims to enhance and design node features, particularly in scenarios where information is lacking. Our method leverages the topological structure of the local subgraph to create topology-aware node features. The subgraph features are generated using an efficient spectral graph embedding technique, and they serve as node features that capture the local topological organization of the network. The explicit node features, if present, are then enhanced with the subgraph embeddings in order to improve the overall performance. ESGEA is compatible with any GNN-based architecture and is effective even in the absence of node features. We evaluate the proposed method in a social network graph classification task where node attributes are unavailable, as well as in a node classification task where node features are corrupted or even absent. The evaluation results on seven datasets and eight baseline models indicate up to a 10% improvement in AUC and a 7% improvement in accuracy for graph and node classification tasks, respectively.
</details>
<details>
<summary>摘要</summary>
graph neural networks (GNNs) 已经表现出非常出色的表现力在复杂网络中进行学习任务。 GNNs 的高效性 frequently correlates with the availability and quality of node-level features in the input networks。 however, for many network applications, such node-level information may be missing or unreliable， thereby limiting the applicability and efficacy of GNNs。 To address this limitation, we present a novel approach denoted as Ego-centric Spectral subGraph Embedding Augmentation (ESGEA), which aims to enhance and design node features, particularly in scenarios where information is lacking。 Our method leverages the topological structure of the local subgraph to create topology-aware node features。 The subgraph features are generated using an efficient spectral graph embedding technique， and they serve as node features that capture the local topological organization of the network。 The explicit node features, if present, are then enhanced with the subgraph embeddings in order to improve the overall performance。 ESGEA is compatible with any GNN-based architecture and is effective even in the absence of node features。 We evaluate the proposed method in a social network graph classification task where node attributes are unavailable， as well as in a node classification task where node features are corrupted or even absent。 The evaluation results on seven datasets and eight baseline models indicate up to a 10% improvement in AUC and a 7% improvement in accuracy for graph and node classification tasks， respectively。
</details></li>
</ul>
<hr>
<h2 id="On-the-importance-of-catalyst-adsorbate-3D-interactions-for-relaxed-energy-predictions"><a href="#On-the-importance-of-catalyst-adsorbate-3D-interactions-for-relaxed-energy-predictions" class="headerlink" title="On the importance of catalyst-adsorbate 3D interactions for relaxed energy predictions"></a>On the importance of catalyst-adsorbate 3D interactions for relaxed energy predictions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06682">http://arxiv.org/abs/2310.06682</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alvaro Carbonero, Alexandre Duval, Victor Schmidt, Santiago Miret, Alex Hernandez-Garcia, Yoshua Bengio, David Rolnick</li>
<li>for: 预测材料性能和发现</li>
<li>methods: 使用SchNet、DimeNet++和FAENet作为基础架构，对模型进行四种修改来评估其影响：去除输入图中的边、独立表示pooling、不共享后置权重和使用关注机制传递非几何相对信息。</li>
<li>results: 发现虽然去除绑定站信息会降低准确性，修改后的模型仍可以高度准确地预测系统的压缩能量，并且可以在O20数据集上达到remarkably decent MAE。<details>
<summary>Abstract</summary>
The use of machine learning for material property prediction and discovery has traditionally centered on graph neural networks that incorporate the geometric configuration of all atoms. However, in practice not all this information may be readily available, e.g.~when evaluating the potentially unknown binding of adsorbates to catalyst. In this paper, we investigate whether it is possible to predict a system's relaxed energy in the OC20 dataset while ignoring the relative position of the adsorbate with respect to the electro-catalyst. We consider SchNet, DimeNet++ and FAENet as base architectures and measure the impact of four modifications on model performance: removing edges in the input graph, pooling independent representations, not sharing the backbone weights and using an attention mechanism to propagate non-geometric relative information. We find that while removing binding site information impairs accuracy as expected, modified models are able to predict relaxed energies with remarkably decent MAE. Our work suggests future research directions in accelerated materials discovery where information on reactant configurations can be reduced or altogether omitted.
</details>
<details>
<summary>摘要</summary>
传统上，机器学习 для物理性质预测和发现都是通过图 neural networks来实现，其中包括所有原子的几何配置。然而，在实践中，这些信息可能不可获取，例如，评估可能未知的材料吸附物的绑定。在这篇文章中，我们研究了是否可以预测系统的压缩能量在OC20数据集中，而不考虑附着物的相对位置。我们考虑了SchNet、DimeNet++和FAENet作为基础体系，并测试了四种修改对模型性能的影响： removing edges in the input graph、pooling independent representations、不共享背部网重和使用注意机制来传播非几何相对信息。我们发现，尽管 removing binding site information 会降低准确性，但修改后的模型仍然可以预测压缩能量的投影值，并且具有相当的平均误差。我们的工作建议将来的材料发现加速，可以采用减少或完全去除reactant配置信息的方法。
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-Quantum-Systems-with-Magnetic-p-bits"><a href="#Machine-Learning-Quantum-Systems-with-Magnetic-p-bits" class="headerlink" title="Machine Learning Quantum Systems with Magnetic p-bits"></a>Machine Learning Quantum Systems with Magnetic p-bits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06679">http://arxiv.org/abs/2310.06679</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuvro Chowdhury, Kerem Y. Camsari</li>
<li>for: 这篇论文旨在探讨一种基于概率计算的硬件平台，用于解决人工智能算法的计算工作负荷不断增长所带来的危机。</li>
<li>methods: 这篇论文使用概率计算的p-bits作为特定应用和算法的域特定计算 paradigma，并利用磁矩 tunnel junctions（sMTJ）等磁电子设备实现集成的p-计算机。</li>
<li>results: 研究表明，使用这种概率计算机可以实现可扩展和能效的计算，特别适用于将机器学习和量子物理结合起来的新领域。<details>
<summary>Abstract</summary>
The slowing down of Moore's Law has led to a crisis as the computing workloads of Artificial Intelligence (AI) algorithms continue skyrocketing. There is an urgent need for scalable and energy-efficient hardware catering to the unique requirements of AI algorithms and applications. In this environment, probabilistic computing with p-bits emerged as a scalable, domain-specific, and energy-efficient computing paradigm, particularly useful for probabilistic applications and algorithms. In particular, spintronic devices such as stochastic magnetic tunnel junctions (sMTJ) show great promise in designing integrated p-computers. Here, we examine how a scalable probabilistic computer with such magnetic p-bits can be useful for an emerging field combining machine learning and quantum physics.
</details>
<details>
<summary>摘要</summary>
Note:* "Moore's Law" is translated as "Moore's 法则" (Moore zhì yì)* "computing workloads" is translated as "计算工作负载" (jìsuan gongzuò fùyòu)* "Probabilistic computing" is translated as "概率计算" (guīshí jìsuan)* "p-bits" is translated as "p-位" (p-bit)* "spintronic devices" is translated as "磁电子设备" (spintronic seti)* "stochastic magnetic tunnel junctions" is translated as "随机磁隧道结构" (stochastic magnetic tunnel junctions)* "integrated p-computers" is translated as "集成p计算机" (integrated p-computers)* "emerging field" is translated as "新兴领域" (emerging field)* "machine learning and quantum physics" is translated as "机器学习和量子物理" (machine learning and quantum physics)
</details></li>
</ul>
<hr>
<h2 id="Tertiary-Lymphoid-Structures-Generation-through-Graph-based-Diffusion"><a href="#Tertiary-Lymphoid-Structures-Generation-through-Graph-based-Diffusion" class="headerlink" title="Tertiary Lymphoid Structures Generation through Graph-based Diffusion"></a>Tertiary Lymphoid Structures Generation through Graph-based Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06661">http://arxiv.org/abs/2310.06661</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manuel Madeira, Dorina Thanou, Pascal Frossard</li>
<li>for: 这种研究旨在利用图基的深度生成模型来生成生物学意义的细胞图，以更好地理解生物学过程中的规则。</li>
<li>methods: 研究者使用了现状的图基的扩散模型来生成细胞图，并证明了这种模型能够准确地学习细胞图中细胞的三元免疫结构（TLS）含量，这是评估肿瘤进程的重要生物标志。</li>
<li>results: 研究者通过数据扩充来证明了学习生成模型的utilty，并展示了这种模型可以帮助提高肿瘤诊断的准确率。这是首次利用图基的扩散模型来生成生物学意义的细胞图。<details>
<summary>Abstract</summary>
Graph-based representation approaches have been proven to be successful in the analysis of biomedical data, due to their capability of capturing intricate dependencies between biological entities, such as the spatial organization of different cell types in a tumor tissue. However, to further enhance our understanding of the underlying governing biological mechanisms, it is important to accurately capture the actual distributions of such complex data. Graph-based deep generative models are specifically tailored to accomplish that. In this work, we leverage state-of-the-art graph-based diffusion models to generate biologically meaningful cell-graphs. In particular, we show that the adopted graph diffusion model is able to accurately learn the distribution of cells in terms of their tertiary lymphoid structures (TLS) content, a well-established biomarker for evaluating the cancer progression in oncology research. Additionally, we further illustrate the utility of the learned generative models for data augmentation in a TLS classification task. To the best of our knowledge, this is the first work that leverages the power of graph diffusion models in generating meaningful biological cell structures.
</details>
<details>
<summary>摘要</summary>
基于图表表示方法已经在生物医学数据分析中取得成功，因为它们可以捕捉生物实体之间复杂的依赖关系，如肿瘤组织中不同细胞类型之间的空间组织。然而，为了更好地理解生物机制的下面驱动，需要准确地捕捉实际数据的分布。基于图表的深度生成模型可以帮助实现这一目标。在这种工作中，我们利用了状态机器的图表傅振模型，生成生物意义正的细胞图。特别是，我们表明采用的图表傅振模型可以准确地学习细胞的三元免疫结构（TLS）含量，这是评估肿瘤发展的生物标志物。此外，我们还进一步证明了学习的生成模型可以用于数据扩展在TLS分类任务中。根据我们所知，这是首次利用图表傅振模型生成有意义的生物细胞结构。
</details></li>
</ul>
<hr>
<h2 id="Zero-Level-Set-Encoder-for-Neural-Distance-Fields"><a href="#Zero-Level-Set-Encoder-for-Neural-Distance-Fields" class="headerlink" title="Zero-Level-Set Encoder for Neural Distance Fields"></a>Zero-Level-Set Encoder for Neural Distance Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06644">http://arxiv.org/abs/2310.06644</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefan Rhys Jeske, Jonathan Klein, Dominik L. Michels, Jan Bender</li>
<li>for: 本研究旨在提出一种基于神经网络的3D形状表示方法，以减少计算开销并提高推理效率。</li>
<li>methods: 我们提出了一种新的编码-解码神经网络，其包括多级混合系统和维度分解器，并且通过解决固有方程来训练网络。在推理过程中，我们只需要知道零级集，而不需要预先知道非零距离值或形状占据情况。</li>
<li>results: 我们在多种数据集上进行了实验，并证明了我们的方法的有效性、普遍性和可扩展性。我们的方法可以处理弯曲3D形状、单类编码和多类编码等多种应用场景。<details>
<summary>Abstract</summary>
Neural shape representation generally refers to representing 3D geometry using neural networks, e.g., to compute a signed distance or occupancy value at a specific spatial position. Previous methods tend to rely on the auto-decoder paradigm, which often requires densely-sampled and accurate signed distances to be known during training and testing, as well as an additional optimization loop during inference. This introduces a lot of computational overhead, in addition to having to compute signed distances analytically, even during testing. In this paper, we present a novel encoder-decoder neural network for embedding 3D shapes in a single forward pass. Our architecture is based on a multi-scale hybrid system incorporating graph-based and voxel-based components, as well as a continuously differentiable decoder. Furthermore, the network is trained to solve the Eikonal equation and only requires knowledge of the zero-level set for training and inference. Additional volumetric samples can be generated on-the-fly, and incorporated in an unsupervised manner. This means that in contrast to most previous work, our network is able to output valid signed distance fields without explicit prior knowledge of non-zero distance values or shape occupancy. In other words, our network computes approximate solutions to the boundary-valued Eikonal equation. It also requires only a single forward pass during inference, instead of the common latent code optimization. We further propose a modification of the loss function in case that surface normals are not well defined, e.g., in the context of non-watertight surface-meshes and non-manifold geometry. We finally demonstrate the efficacy, generalizability and scalability of our method on datasets consisting of deforming 3D shapes, single class encoding and multiclass encoding, showcasing a wide range of possible applications.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Implicit-Variational-Inference-for-High-Dimensional-Posteriors"><a href="#Implicit-Variational-Inference-for-High-Dimensional-Posteriors" class="headerlink" title="Implicit Variational Inference for High-Dimensional Posteriors"></a>Implicit Variational Inference for High-Dimensional Posteriors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06643">http://arxiv.org/abs/2310.06643</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anshuk Uppal, Kristoffer Stensbo-Smidt, Wouter K. Boomsma, Jes Frellsen</li>
<li>For: The paper is written for advancing the field of variational inference in Bayesian neural networks, specifically by proposing a new method for approximating complex multimodal and correlated posteriors using neural samplers with implicit distributions.* Methods: The paper introduces novel bounds that come about by locally linearizing the neural sampler, which is distinct from existing methods that rely on additional discriminator networks and unstable adversarial objectives. The paper also presents a new sampler architecture that enables implicit distributions over millions of latent variables, addressing computational concerns by using differentiable numerical approximations.* Results: The paper demonstrates that the proposed method is capable of recovering correlations across layers in large Bayesian neural networks, a property that is crucial for a network’s performance but notoriously challenging to achieve. The paper also shows that the expressive posteriors obtained using the proposed method outperform state-of-the-art uncertainty quantification methods in downstream tasks, validating the effectiveness of the training algorithm and the quality of the learned implicit approximation.<details>
<summary>Abstract</summary>
In variational inference, the benefits of Bayesian models rely on accurately capturing the true posterior distribution. We propose using neural samplers that specify implicit distributions, which are well-suited for approximating complex multimodal and correlated posteriors in high-dimensional spaces. Our approach advances inference using implicit distributions by introducing novel bounds that come about by locally linearising the neural sampler. This is distinct from existing methods that rely on additional discriminator networks and unstable adversarial objectives. Furthermore, we present a new sampler architecture that, for the first time, enables implicit distributions over millions of latent variables, addressing computational concerns by using differentiable numerical approximations. Our empirical analysis indicates our method is capable of recovering correlations across layers in large Bayesian neural networks, a property that is crucial for a network's performance but notoriously challenging to achieve. To the best of our knowledge, no other method has been shown to accomplish this task for such large models. Through experiments in downstream tasks, we demonstrate that our expressive posteriors outperform state-of-the-art uncertainty quantification methods, validating the effectiveness of our training algorithm and the quality of the learned implicit approximation.
</details>
<details>
<summary>摘要</summary>
在变分推断中， bayesian 模型的优点取决于正确地捕捉真实 posterior distribution。我们提议使用神经网络 sampler，这些 sampler  specify implicit distribution，适用于高维空间中复杂的多模态和相关 posterior。我们的方法在神经网络 sampler 中引入新的 bound，通过本地线性化来提高推断。这与现有的方法不同，它们基于额外的 discriminator 网络和不稳定的对抗性目标。此外，我们提出了一新的 sampler 架构，可以对 millions 个 latent variable 进行隐式分布，通过使用可微的数学近似来解决计算问题。我们的实验表明，我们的方法可以在大 bayesian 神经网络中恢复层之间的相关性，这是一个关键的性能因素，但是很难实现。而我们的表达式 posterior 可以超越现有的 uncertainty quantification 方法，证明我们的训练算法的有效性和学习的隐式近似质量。
</details></li>
</ul>
<hr>
<h2 id="The-Lattice-Overparametrization-Paradigm-for-the-Machine-Learning-of-Lattice-Operators"><a href="#The-Lattice-Overparametrization-Paradigm-for-the-Machine-Learning-of-Lattice-Operators" class="headerlink" title="The Lattice Overparametrization Paradigm for the Machine Learning of Lattice Operators"></a>The Lattice Overparametrization Paradigm for the Machine Learning of Lattice Operators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06639">http://arxiv.org/abs/2310.06639</a></li>
<li>repo_url: None</li>
<li>paper_authors: Diego Marcondes, Junior Barrera</li>
<li>for: 本文旨在提出一种学习条件下的势函数算子的方法，以控制、透明度和可解性为特点。</li>
<li>methods: 本文使用的方法包括stoochastic lattice gradient descent算法和基于材料的算法，以及如何从势函数算子的重 parametrization中计算其基。</li>
<li>results: 本文的结果表明，通过使用stochastic lattice gradient descent算法可以有效地学习势函数算子，并且可以通过计算其基来了解势函数算子的性质。此外，本文还证明了这种学习方法具有控制、透明度和可解性的特点，这些特点在现代机器学习方法中缺失。<details>
<summary>Abstract</summary>
The machine learning of lattice operators has three possible bottlenecks. From a statistical standpoint, it is necessary to design a constrained class of operators based on prior information with low bias, and low complexity relative to the sample size. From a computational perspective, there should be an efficient algorithm to minimize an empirical error over the class. From an understanding point of view, the properties of the learned operator need to be derived, so its behavior can be theoretically understood. The statistical bottleneck can be overcome due to the rich literature about the representation of lattice operators, but there is no general learning algorithm for them. In this paper, we discuss a learning paradigm in which, by overparametrizing a class via elements in a lattice, an algorithm for minimizing functions in a lattice is applied to learn. We present the stochastic lattice gradient descent algorithm as a general algorithm to learn on constrained classes of operators as long as a lattice overparametrization of it is fixed, and we discuss previous works which are proves of concept. Moreover, if there are algorithms to compute the basis of an operator from its overparametrization, then its properties can be deduced and the understanding bottleneck is also overcome. This learning paradigm has three properties that modern methods based on neural networks lack: control, transparency and interpretability. Nowadays, there is an increasing demand for methods with these characteristics, and we believe that mathematical morphology is in a unique position to supply them. The lattice overparametrization paradigm could be a missing piece for it to achieve its full potential within modern machine learning.
</details>
<details>
<summary>摘要</summary>
《机器学习阶层算子的三个可能的瓶颈》 Machine learning lattice operators have three possible bottlenecks. From a statistical standpoint, it is necessary to design a constrained class of operators based on prior information with low bias and low complexity relative to the sample size. From a computational perspective, there should be an efficient algorithm to minimize an empirical error over the class. From an understanding standpoint, the properties of the learned operator need to be derived, so its behavior can be theoretically understood.The statistical bottleneck can be overcome due to the rich literature about the representation of lattice operators, but there is no general learning algorithm for them. In this paper, we discuss a learning paradigm in which, by overparametrizing a class via elements in a lattice, an algorithm for minimizing functions in a lattice is applied to learn. We present the stochastic lattice gradient descent algorithm as a general algorithm to learn on constrained classes of operators as long as a lattice overparametrization of it is fixed, and we discuss previous works which are proofs of concept.Moreover, if there are algorithms to compute the basis of an operator from its overparametrization, then its properties can be deduced, and the understanding bottleneck is also overcome. This learning paradigm has three properties that modern methods based on neural networks lack: control, transparency, and interpretability. Nowadays, there is an increasing demand for methods with these characteristics, and we believe that mathematical morphology is in a unique position to supply them. The lattice overparametrization paradigm could be a missing piece for it to achieve its full potential within modern machine learning.
</details></li>
</ul>
<hr>
<h2 id="iTransformer-Inverted-Transformers-Are-Effective-for-Time-Series-Forecasting"><a href="#iTransformer-Inverted-Transformers-Are-Effective-for-Time-Series-Forecasting" class="headerlink" title="iTransformer: Inverted Transformers Are Effective for Time Series Forecasting"></a>iTransformer: Inverted Transformers Are Effective for Time Series Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06625">http://arxiv.org/abs/2310.06625</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thuml/iTransformer">https://github.com/thuml/iTransformer</a></li>
<li>paper_authors: Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, Mingsheng Long</li>
<li>for: 提高时间序列预测性能和泛化能力，并减少计算量</li>
<li>methods: 基于Transformer架构，但无需修改基本组件，通过倒转注意力机制和Feed Forward网络来学习多变量相关性</li>
<li>results: 在多个真实世界数据集上实现了一致状态的前一个性和泛化能力，提高了Transformer家族在时间序列预测中的表现，并且可以更好地利用不同的lookback窗口和变量。<details>
<summary>Abstract</summary>
The recent boom of linear forecasting models questions the ongoing passion for architectural modifications of Transformer-based forecasters. These forecasters leverage Transformers to model the global dependencies over temporal tokens of time series, with each token formed by multiple variates of the same timestamp. However, Transformer is challenged in forecasting series with larger lookback windows due to performance degradation and computation explosion. Besides, the unified embedding for each temporal token fuses multiple variates with potentially unaligned timestamps and distinct physical measurements, which may fail in learning variate-centric representations and result in meaningless attention maps. In this work, we reflect on the competent duties of Transformer components and repurpose the Transformer architecture without any adaptation on the basic components. We propose iTransformer that simply inverts the duties of the attention mechanism and the feed-forward network. Specifically, the time points of individual series are embedded into variate tokens which are utilized by the attention mechanism to capture multivariate correlations; meanwhile, the feed-forward network is applied for each variate token to learn nonlinear representations. The iTransformer model achieves consistent state-of-the-art on several real-world datasets, which further empowers the Transformer family with promoted performance, generalization ability across different variates, and better utilization of arbitrary lookback windows, making it a nice alternative as the fundamental backbone of time series forecasting.
</details>
<details>
<summary>摘要</summary>
Recent 崩溃 linear 预测模型 让人们对 transformer 基于预测器的建筑修改lost interest。这些预测器利用 transformer 模型全球时间序列中的全局依赖关系，每个时间戳由多个变量组成。然而， transformer 在更大的 lookback 窗口预测中表现不佳，因为性能下降和计算暴涨。此外，通用 embedding 对每个时间戳进行综合 embedding 可能会失去变量 centered 表示和无用的注意力地图。在这项工作中，我们反思 transformer 组件的能力和挑战，并将 transformer 架构重新定义为 iTransformer。iTransformer 简单地将 attention 机制和 feed-forward 网络的职责反转过来。具体来说，每个时间序列的时刻点被转换成 variate token，并由 attention 机制来捕捉多元相关性；而 feed-forward 网络则是为每个 variate token 进行非线性表示学习。iTransformer 模型在多个实际数据集上具有一致的 state-of-the-art 性能，这使得 transformer 家族受到了提高性能、泛化能力和不同变量之间的更好利用，从而成为时间序列预测的基本脊梁。
</details></li>
</ul>
<hr>
<h2 id="Robustness-May-be-More-Brittle-than-We-Think-under-Different-Degrees-of-Distribution-Shifts"><a href="#Robustness-May-be-More-Brittle-than-We-Think-under-Different-Degrees-of-Distribution-Shifts" class="headerlink" title="Robustness May be More Brittle than We Think under Different Degrees of Distribution Shifts"></a>Robustness May be More Brittle than We Think under Different Degrees of Distribution Shifts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06622">http://arxiv.org/abs/2310.06622</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaican Li, Yifan Zhang, Lanqing Hong, Zhenguo Li, Nevin L. Zhang</li>
<li>for: 本研究旨在探讨模型在不同分布偏移度下的抗衰减性，以提高对模型在不同场景下的评估。</li>
<li>methods: 研究人员采用了多个数据集，并对模型在不同分布偏移度下的性能进行了评估。</li>
<li>results: 研究人员发现，模型在不同分布偏移度下的抗衰减性可能很弱，而且可能存在较大的分布偏移度下的潜在风险。此外，大规模预训练模型，如CLIP，在novel downstream任务中的分布偏移度下也具有敏感性。<details>
<summary>Abstract</summary>
Out-of-distribution (OOD) generalization is a complicated problem due to the idiosyncrasies of possible distribution shifts between training and test domains. Most benchmarks employ diverse datasets to address this issue; however, the degree of the distribution shift between the training domains and the test domains of each dataset remains largely fixed. This may lead to biased conclusions that either underestimate or overestimate the actual OOD performance of a model. Our study delves into a more nuanced evaluation setting that covers a broad range of shift degrees. We show that the robustness of models can be quite brittle and inconsistent under different degrees of distribution shifts, and therefore one should be more cautious when drawing conclusions from evaluations under a limited range of degrees. In addition, we observe that large-scale pre-trained models, such as CLIP, are sensitive to even minute distribution shifts of novel downstream tasks. This indicates that while pre-trained representations may help improve downstream in-distribution performance, they could have minimal or even adverse effects on generalization in certain OOD scenarios of the downstream task if not used properly. In light of these findings, we encourage future research to conduct evaluations across a broader range of shift degrees whenever possible.
</details>
<details>
<summary>摘要</summary>
外部分布（OOD）泛化是一个复杂的问题，因为可能存在训练和测试领域之间的特殊性和分布差异。大多数标准准测试使用多种数据集来解决这个问题，但是每个数据集的测试领域分布shift的度量仍然很大程度上固定。这可能会导致偏向的结论， Either underestimate或Overestimate实际OOD模型的性能。我们的研究探讨了一种更加细化的评估环境，覆盖了广泛的分布差异度。我们发现模型的Robustness可能很脆弱和不一致，因此在不同的分布差异度下，一个应该更加小心地做结论。此外，我们发现大规模预训练模型，如CLIP，对小型分布差异的新任务有敏感性。这表示，虽然预训练表示可以帮助改进下游领域的表现，但是在某些OOD场景下，它们可能会具有微不足或甚至有害的效果。为了更好地评估OOD性能，我们建议将来的研究在可能的范围内进行评估。
</details></li>
</ul>
<hr>
<h2 id="Discovering-Interpretable-Physical-Models-Using-Symbolic-Regression-and-Discrete-Exterior-Calculus"><a href="#Discovering-Interpretable-Physical-Models-Using-Symbolic-Regression-and-Discrete-Exterior-Calculus" class="headerlink" title="Discovering Interpretable Physical Models Using Symbolic Regression and Discrete Exterior Calculus"></a>Discovering Interpretable Physical Models Using Symbolic Regression and Discrete Exterior Calculus</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06609">http://arxiv.org/abs/2310.06609</a></li>
<li>repo_url: None</li>
<li>paper_authors: Simone Manti, Alessandro Lucantonio</li>
<li>for: 这种方法是用于自动发现物理模型，从实验数据开始，以便提高物理 simulations 的准确性和泛化能力。</li>
<li>methods: 这种方法结合了Symbolic Regression（SR）和Discrete Exterior Calculus（DEC），使用了一种自然的通用的整数数学语言，以便推导和分析物理模型。 DEC 提供了一些拓扑学上的建构，以及一种强类型的 SR 过程，以确保数学表达的正确性和减少搜索空间。</li>
<li>results: 通过使用这种方法， authors 成功地重新发现了三个维度物理学中的模型：波松方程、欧拉的弹性材料和Linear Elasticity 方程。这些模型具有通用的特点，可以应用于多种物理模拟问题。<details>
<summary>Abstract</summary>
Computational modeling is a key resource to gather insight into physical systems in modern scientific research and engineering. While access to large amount of data has fueled the use of Machine Learning (ML) to recover physical models from experiments and increase the accuracy of physical simulations, purely data-driven models have limited generalization and interpretability. To overcome these limitations, we propose a framework that combines Symbolic Regression (SR) and Discrete Exterior Calculus (DEC) for the automated discovery of physical models starting from experimental data. Since these models consist of mathematical expressions, they are interpretable and amenable to analysis, and the use of a natural, general-purpose discrete mathematical language for physics favors generalization with limited input data. Importantly, DEC provides building blocks for the discrete analogue of field theories, which are beyond the state-of-the-art applications of SR to physical problems. Further, we show that DEC allows to implement a strongly-typed SR procedure that guarantees the mathematical consistency of the recovered models and reduces the search space of symbolic expressions. Finally, we prove the effectiveness of our methodology by re-discovering three models of Continuum Physics from synthetic experimental data: Poisson equation, the Euler's Elastica and the equations of Linear Elasticity. Thanks to their general-purpose nature, the methods developed in this paper may be applied to diverse contexts of physical modeling.
</details>
<details>
<summary>摘要</summary>
现代科学研究和工程中的物理系统模型化是一个关键资源，帮助我们更深入理解物理系统的行为。虽然大量数据的可用性推动了机器学习（ML）技术来从实验中提取物理模型并提高物理仿真的准确性，但纯数据驱动的模型受到限制，其可重复性和可解释性受到限制。为了超越这些限制，我们提出了一种整合符号 regression（SR）和离散外部 calculus（DEC）的框架，用于自动找到从实验数据开始的物理模型。由于这些模型由数学表达组成，它们可以被解释和分析，而使用自然的通用离散数学语言也会增加泛化的能力。此外，DEC提供了离散场论的建构元素，这些元素超越了当前SR在物理问题上的应用状况。此外，我们还证明了DEC可以实现强类型的SR过程，以确保数学模型的数学一致性，并减少符号表达的搜索空间。最后，我们证明了我们的方法效果，通过从合成实验数据中重新发现波松方程、欧拉-埃拉斯特拉方程和线性弹性方程。由于这些方程的通用性，我们的方法可以应用于多种物理模型化的 Context。
</details></li>
</ul>
<hr>
<h2 id="XAI-for-Early-Crop-Classification"><a href="#XAI-for-Early-Crop-Classification" class="headerlink" title="XAI for Early Crop Classification"></a>XAI for Early Crop Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06574">http://arxiv.org/abs/2310.06574</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ayshah Chan, Maja Schneider, Marco Körner</li>
<li>for: 预测早期农作物种类通过可解释AI方法</li>
<li>methods: 使用层次 relevance propagation (LRP) 方法确定重要时间步骤，并选择一个精选的时间范围来构成最短的分类时间范围</li>
<li>results: 确定的时间范围为2019年4月21日至2019年8月9日，与全时序相比只失去0.75%的准确率，而且LRP得到的重要时间步骤也揭示了输入值中的小 Details，这些Details可以用来区分不同的类别。<details>
<summary>Abstract</summary>
We propose an approach for early crop classification through identifying important timesteps with eXplainable AI (XAI) methods. Our approach consists of training a baseline crop classification model to carry out layer-wise relevance propagation (LRP) so that the salient time step can be identified. We chose a selected number of such important time indices to create the bounding region of the shortest possible classification timeframe. We identified the period 21st April 2019 to 9th August 2019 as having the best trade-off in terms of accuracy and earliness. This timeframe only suffers a 0.75% loss in accuracy as compared to using the full timeseries. We observed that the LRP-derived important timesteps also highlight small details in input values that differentiates between different classes and
</details>
<details>
<summary>摘要</summary>
我们提出了一种采用可解释AI（XAI）方法进行早期作物分类的方法。我们的方法包括训练一个基eline作物分类模型，并使用层wise relevance propagation（LRP）来确定重要的时间步骤。我们选择了一些重要的时间索引，并将其用于创建最短的可能的分类时间范围。我们确定的时间范围为2019年4月21日至2019年8月9日，这个时间范围只减少了0.75%的准确率，相比使用完整时间序列。我们发现LRP得到的重要时间步骤还高亮了输入值中的小 Details，这些细节可以用于分类不同类型的作物。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-reconstruction-with-uncertainty-estimation-for-γ-photon-interaction-in-fast-scintillator-detectors"><a href="#Deep-Learning-reconstruction-with-uncertainty-estimation-for-γ-photon-interaction-in-fast-scintillator-detectors" class="headerlink" title="Deep Learning reconstruction with uncertainty estimation for $γ$ photon interaction in fast scintillator detectors"></a>Deep Learning reconstruction with uncertainty estimation for $γ$ photon interaction in fast scintillator detectors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06572">http://arxiv.org/abs/2310.06572</a></li>
<li>repo_url: None</li>
<li>paper_authors: Geoffrey Daniel, Mohamed Bahi Yahiaoui, Claude Comtat, Sebastien Jan, Olga Kochebina, Jean-Marc Martinez, Viktoriya Sergeyeva, Viatcheslav Sharyy, Chi-Hsun Sung, Dominique Yvon</li>
<li>for: 这篇论文旨在提出一种基于物理学习的gamma射频辐射量测量方法，用于 Positron Emission Tomography（PET）成像。</li>
<li>methods: 该方法使用Density Neural Networkapproach来估算PbWO4固体探测器中gamma射频辐射的二维坐标。我们定义了自定义损失函数，以估算恢复过程中的内在不确定性和探测器的物理约束。</li>
<li>results: 研究结果表明该方法的有效性和可靠性，并且强调了估算结果的不确定性的重要性。我们还讨论了该方法在PET成像质量提高方面的潜在影响和如何使用结果来改进模型和应用中的表现。此外，我们还指出该方法可以扩展到其他应用场景以外。<details>
<summary>Abstract</summary>
This article presents a physics-informed deep learning method for the quantitative estimation of the spatial coordinates of gamma interactions within a monolithic scintillator, with a focus on Positron Emission Tomography (PET) imaging. A Density Neural Network approach is designed to estimate the 2-dimensional gamma photon interaction coordinates in a fast lead tungstate (PbWO4) monolithic scintillator detector. We introduce a custom loss function to estimate the inherent uncertainties associated with the reconstruction process and to incorporate the physical constraints of the detector.   This unique combination allows for more robust and reliable position estimations and the obtained results demonstrate the effectiveness of the proposed approach and highlights the significant benefits of the uncertainties estimation. We discuss its potential impact on improving PET imaging quality and show how the results can be used to improve the exploitation of the model, to bring benefits to the application and how to evaluate the validity of the given prediction and the associated uncertainties. Importantly, our proposed methodology extends beyond this specific use case, as it can be generalized to other applications beyond PET imaging.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:这篇文章介绍了一种基于物理学的深度学习方法，用于量化gamma交互的空间坐标 within a monolithic scintillator detector，尤其是Positron Emission Tomography（PET）成像。该方法使用了Density Neural Network来估算gamma photon交互的2维坐标在fast lead tungstate（PbWO4）monolithic scintillator detector中。我们引入了一个自定义损失函数，以估算重建过程中的自然不确定性和仪器的物理约束。这种独特的组合使得位置估算更加稳定和可靠，并且实际结果证明了我们的提议的有效性，并强调了估算不确定性的重要性。这种方法有可能改善PET成像质量，并且可以扩展到其他 beyond PET成像的应用。
</details></li>
</ul>
<hr>
<h2 id="Statistical-properties-and-privacy-guarantees-of-an-original-distance-based-fully-synthetic-data-generation-method"><a href="#Statistical-properties-and-privacy-guarantees-of-an-original-distance-based-fully-synthetic-data-generation-method" class="headerlink" title="Statistical properties and privacy guarantees of an original distance-based fully synthetic data generation method"></a>Statistical properties and privacy guarantees of an original distance-based fully synthetic data generation method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06571">http://arxiv.org/abs/2310.06571</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rémy Chapelle, Bruno Falissard</li>
<li>for: 这种研究旨在开发一种基于分类和回归树的数据生成框架，以及一种原始距离基于过滤方法，以保护人类参与者数据隐私。</li>
<li>methods: 这种数据生成框架包括四个步骤，每个步骤都是设计来防止特定的隐私泄露风险。研究人员使用了两个或更多的步骤来应用于一个丰富的生物学调查数据集，并计算了隐私和用用度指标。</li>
<li>results: 计算的指标表明，使用全部框架时，每个假数据集都具有了满意的隐私保护水平，特别是对于特性泄露攻击。成员泄露攻击被正式防止，而无需重大改变数据。机器学习方法显示，对于模拟的单个化和链接攻击，成功率很低。各数据集的分布和推论指标与原始数据相似。<details>
<summary>Abstract</summary>
Introduction: The amount of data generated by original research is growing exponentially. Publicly releasing them is recommended to comply with the Open Science principles. However, data collected from human participants cannot be released as-is without raising privacy concerns. Fully synthetic data represent a promising answer to this challenge. This approach is explored by the French Centre de Recherche en {\'E}pid{\'e}miologie et Sant{\'e} des Populations in the form of a synthetic data generation framework based on Classification and Regression Trees and an original distance-based filtering. The goal of this work was to develop a refined version of this framework and to assess its risk-utility profile with empirical and formal tools, including novel ones developed for the purpose of this evaluation.Materials and Methods: Our synthesis framework consists of four successive steps, each of which is designed to prevent specific risks of disclosure. We assessed its performance by applying two or more of these steps to a rich epidemiological dataset. Privacy and utility metrics were computed for each of the resulting synthetic datasets, which were further assessed using machine learning approaches.Results: Computed metrics showed a satisfactory level of protection against attribute disclosure attacks for each synthetic dataset, especially when the full framework was used. Membership disclosure attacks were formally prevented without significantly altering the data. Machine learning approaches showed a low risk of success for simulated singling out and linkability attacks. Distributional and inferential similarity with the original data were high with all datasets.Discussion: This work showed the technical feasibility of generating publicly releasable synthetic data using a multi-step framework. Formal and empirical tools specifically developed for this demonstration are a valuable contribution to this field. Further research should focus on the extension and validation of these tools, in an effort to specify the intrinsic qualities of alternative data synthesis methods.Conclusion: By successfully assessing the quality of data produced using a novel multi-step synthetic data generation framework, we showed the technical and conceptual soundness of the Open-CESP initiative, which seems ripe for full-scale implementation.
</details>
<details>
<summary>摘要</summary>
引言：原始研究数据的数量正在急剧增长。按照开放科学原则，公共发布这些数据是建议的。然而，从人类参与者收集的数据不能直接发布，否则会引起隐私问题。完全 sintética 数据表示一种有 Promise的解决方案。法国中央研究所在这种 sintética 数据生成框架基于分类和回归树和一种原始的距离基于筛选。该工作的目的是开发一个改进版的这种框架，并通过实验和正式工具评估其风险利用性。材料和方法：我们的 sintesis 框架由四个阶段组成，每个阶段都是为预防特定风险的披露。我们使用两个或更多的这些阶段来处理一个丰富的 epidemiological 数据集。隐私和利用度指标在每个 sintetic 数据集中计算，并使用机器学习方法进行评估。结果：计算的指标表明，使用全部框架时，每个 sintetic 数据集的隐私保护水平很高，特别是对于特征披露攻击。成员披露攻击得到了正式防范，而不是对数据造成重要的变化。机器学习方法表示，在模拟的单个化和链接攻击中， sintetic 数据集的风险很低。 distribución 和推论上的相似性很高，所有的 sintetic 数据集都具有高度的相似性。讨论：这项工作证明了使用多步 sintetic 数据生成框架的技术可行性。为此，我们开发了特有的 formal 和实验工具，这些工具对这一领域做出了重要贡献。未来的研究应该集中在这些工具的扩展和验证上，以确定其他数据生成方法的内在特质。结论：通过成功评估使用多步 sintetic 数据生成框架生成的数据质量，我们证明了开放-CESP INITIATIVE 的技术和概念合理性。这一initiative 似乎准备好进行大规模实施。
</details></li>
</ul>
<hr>
<h2 id="An-Edge-Aware-Graph-Autoencoder-Trained-on-Scale-Imbalanced-Data-for-Travelling-Salesman-Problems"><a href="#An-Edge-Aware-Graph-Autoencoder-Trained-on-Scale-Imbalanced-Data-for-Travelling-Salesman-Problems" class="headerlink" title="An Edge-Aware Graph Autoencoder Trained on Scale-Imbalanced Data for Travelling Salesman Problems"></a>An Edge-Aware Graph Autoencoder Trained on Scale-Imbalanced Data for Travelling Salesman Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06543">http://arxiv.org/abs/2310.06543</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiqing Liu, Xueming Yan, Yaochu Jin</li>
<li>for: 解决各种城市数量的旅行商问题 (TSP)</li>
<li>methods: 使用图像学自动编码器（EdgeGAE）模型，通过学习解决方案数据中的链接预测任务来学习解决TSP问题。</li>
<li>results: 对50,000个TSP实例进行了实验，并证明了该方法可以在不同的规模下达到高度竞争力的性能。<details>
<summary>Abstract</summary>
Recent years have witnessed a surge in research on machine learning for combinatorial optimization since learning-based approaches can outperform traditional heuristics and approximate exact solvers at a lower computation cost. However, most existing work on supervised neural combinatorial optimization focuses on TSP instances with a fixed number of cities and requires large amounts of training samples to achieve a good performance, making them less practical to be applied to realistic optimization scenarios. This work aims to develop a data-driven graph representation learning method for solving travelling salesman problems (TSPs) with various numbers of cities. To this end, we propose an edge-aware graph autoencoder (EdgeGAE) model that can learn to solve TSPs after being trained on solution data of various sizes with an imbalanced distribution. We formulate the TSP as a link prediction task on sparse connected graphs. A residual gated encoder is trained to learn latent edge embeddings, followed by an edge-centered decoder to output link predictions in an end-to-end manner. To improve the model's generalization capability of solving large-scale problems, we introduce an active sampling strategy into the training process. In addition, we generate a benchmark dataset containing 50,000 TSP instances with a size from 50 to 500 cities, following an extremely scale-imbalanced distribution, making it ideal for investigating the model's performance for practical applications. We conduct experiments using different amounts of training data with various scales, and the experimental results demonstrate that the proposed data-driven approach achieves a highly competitive performance among state-of-the-art learning-based methods for solving TSPs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Data-level-hybrid-strategy-selection-for-disk-fault-prediction-model-based-on-multivariate-GAN"><a href="#Data-level-hybrid-strategy-selection-for-disk-fault-prediction-model-based-on-multivariate-GAN" class="headerlink" title="Data-level hybrid strategy selection for disk fault prediction model based on multivariate GAN"></a>Data-level hybrid strategy selection for disk fault prediction model based on multivariate GAN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06537">http://arxiv.org/abs/2310.06537</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuangshuang Yuan, Peng Wu, Yuehui Chen</li>
<li>for: 本研究旨在解决数据类别不均的分类问题，特别是对于硬盘健康状况识别任务中的数据类别不均问题。</li>
<li>methods: 本研究使用了多变量生成 adversarial networks (GAN) 生成数据，并通过混合和融合这些数据来协调硬盘 SMART 数据集，以达到数据层次的均衡。同时，使用了遗传算法来提高硬盘缺陷分类预测精度。</li>
<li>results: 研究表明，通过使用 GAN 生成数据和遗传算法，可以提高硬盘缺陷分类预测精度，并且可以更好地处理数据类别不均问题。<details>
<summary>Abstract</summary>
Data class imbalance is a common problem in classification problems, where minority class samples are often more important and more costly to misclassify in a classification task. Therefore, it is very important to solve the data class imbalance classification problem. The SMART dataset exhibits an evident class imbalance, comprising a substantial quantity of healthy samples and a comparatively limited number of defective samples. This dataset serves as a reliable indicator of the disc's health status. In this paper, we obtain the best balanced disk SMART dataset for a specific classification model by mixing and integrating the data synthesised by multivariate generative adversarial networks (GAN) to balance the disk SMART dataset at the data level; and combine it with genetic algorithms to obtain higher disk fault classification prediction accuracy on a specific classification model.
</details>
<details>
<summary>摘要</summary>
数据类别不匹配是常见的分类问题，其中少数类样本经常更重要和更昂贵的错误分类。因此，解决数据类别不匹配分类问题非常重要。SMART数据集显示了明显的类别不匹配，包括大量的健康样本和相对较少的缺陷样本。这个数据集作为磁盘健康状况的可靠指标。在这篇论文中，我们通过将多变量生成对抗网络（GAN）生成的数据混合和 интегра，在数据层面减少磁盘SMART数据集的类别不匹配;并将生成遗传算法与特定分类模型结合，以提高磁盘缺陷分类预测精度。
</details></li>
</ul>
<hr>
<h2 id="Disk-failure-prediction-based-on-multi-layer-domain-adaptive-learning"><a href="#Disk-failure-prediction-based-on-multi-layer-domain-adaptive-learning" class="headerlink" title="Disk failure prediction based on multi-layer domain adaptive learning"></a>Disk failure prediction based on multi-layer domain adaptive learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06534">http://arxiv.org/abs/2310.06534</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guangfu Gao, Peng Wu, Hussain Dawood</li>
<li>for: 预测磁盘失败</li>
<li>methods: 利用多层适应学技术进行预测</li>
<li>results: 提高预测磁盘失败的能力， especialy for disk data with few failure samples.Here’s a more detailed explanation of each point:</li>
<li>for: The paper is written for predicting disk failures, which is an important task in large-scale data storage systems.</li>
<li>methods: The paper proposes a novel method for predicting disk failures by leveraging multi-layer domain adaptive learning techniques. This method involves selecting disk data with numerous faults as the source domain and disk data with fewer faults as the target domain, and training a feature extraction network with the selected origin and destination domains.</li>
<li>results: The proposed technique is demonstrated to be effective in generating a reliable prediction model and improving the ability to predict failures on disk data with few failure samples.<details>
<summary>Abstract</summary>
Large scale data storage is susceptible to failure. As disks are damaged and replaced, traditional machine learning models, which rely on historical data to make predictions, struggle to accurately predict disk failures. This paper presents a novel method for predicting disk failures by leveraging multi-layer domain adaptive learning techniques. First, disk data with numerous faults is selected as the source domain, and disk data with fewer faults is selected as the target domain. A training of the feature extraction network is performed with the selected origin and destination domains. The contrast between the two domains facilitates the transfer of diagnostic knowledge from the domain of source and target. According to the experimental findings, it has been demonstrated that the proposed technique can generate a reliable prediction model and improve the ability to predict failures on disk data with few failure samples.
</details>
<details>
<summary>摘要</summary>
Note: Simplified Chinese is also known as "简化字" or "简化字".Please note that the translation is done using Google Translate and it may not be perfect. Also, the translation may not be exactly the same as the original text, as some words or phrases may not have direct translations in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="AttributionLab-Faithfulness-of-Feature-Attribution-Under-Controllable-Environments"><a href="#AttributionLab-Faithfulness-of-Feature-Attribution-Under-Controllable-Environments" class="headerlink" title="AttributionLab: Faithfulness of Feature Attribution Under Controllable Environments"></a>AttributionLab: Faithfulness of Feature Attribution Under Controllable Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06514">http://arxiv.org/abs/2310.06514</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yang Zhang, Yawei Li, Hannah Brown, Mina Rezaei, Bernd Bischl, Philip Torr, Ashkan Khakzar, Kenji Kawaguchi</li>
<li>for: 本研究旨在检验feature attribution的可靠性，以确保 Attribute Lab 中的结果是否准确 Reflects the neural network’s learning process.</li>
<li>methods: 本研究使用了手动设置 neural network 的 weights 和数据设计，以确定哪些输入特征对 label 有影响，并用这些设计的ground truth特征来评估 attribute 方法的准确性。</li>
<li>results: 研究发现，在 AttributionLab 中设计的 synthetic environment 中，使用了手动设置的 neural network 和数据可以准确 Reflects the neural network’s learning process，并且可以用这种方法来检验 attribute 方法的准确性。<details>
<summary>Abstract</summary>
Feature attribution explains neural network outputs by identifying relevant input features. How do we know if the identified features are indeed relevant to the network? This notion is referred to as faithfulness, an essential property that reflects the alignment between the identified (attributed) features and the features used by the model. One recent trend to test faithfulness is to design the data such that we know which input features are relevant to the label and then train a model on the designed data. Subsequently, the identified features are evaluated by comparing them with these designed ground truth features. However, this idea has the underlying assumption that the neural network learns to use all and only these designed features, while there is no guarantee that the learning process trains the network in this way. In this paper, we solve this missing link by explicitly designing the neural network by manually setting its weights, along with designing data, so we know precisely which input features in the dataset are relevant to the designed network. Thus, we can test faithfulness in AttributionLab, our designed synthetic environment, which serves as a sanity check and is effective in filtering out attribution methods. If an attribution method is not faithful in a simple controlled environment, it can be unreliable in more complex scenarios. Furthermore, the AttributionLab environment serves as a laboratory for controlled experiments through which we can study feature attribution methods, identify issues, and suggest potential improvements.
</details>
<details>
<summary>摘要</summary>
Feature 归属解释 neural network 输出sBy identifying relevant input features。如何确定这些标识的特征是 neural network 中用到的？这个概念被称为 faithfulness，它是一种重要的性质，它反映了模型中使用的特征与归属特征之间的对应关系。一种最近的趋势是通过设计数据来测试 faithfulness，即在训练模型时，知道哪些输入特征与标签之间存在关系，然后在这些设计的真实特征上训练模型。然而，这个想法假设模型学习所有设计的特征，而这并不一定是真实的。在这篇论文中，我们解决了这个缺失的联系。我们明确地设计了 neural network 的权重，并与数据一起设计，因此我们知道哪些数据集中的输入特征与我们设计的网络中用到的特征之间存在关系。因此，我们可以在 AttributionLab 中测试 faithfulness，这是我们自己设计的人工环境，它作为一种 santity check 有效地筛选出归属方法。如果归属方法不忠实在这种简单控制的环境中，那么它在更复杂的场景中可能不可靠。此外，AttributionLab 环境还可以作为一个 controlled experiments 的实验室，我们可以通过这里进行学习归属方法、发现问题和提出改进建议。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Dataset-Distillation-for-Transfer-Learning"><a href="#Self-Supervised-Dataset-Distillation-for-Transfer-Learning" class="headerlink" title="Self-Supervised Dataset Distillation for Transfer Learning"></a>Self-Supervised Dataset Distillation for Transfer Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06511">http://arxiv.org/abs/2310.06511</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dong Bok Lee, Seanie Lee, Joonho Ko, Kenji Kawaguchi, Juho Lee, Sung Ju Hwang</li>
<li>for: 提出了一种新的问题，即将无标注数据集概要压缩到一小型的自我超vised学习（SSL）数据集中。</li>
<li>methods: 提议使用梯度下降法来优化模型的表示，并在内部目标中使用mean squared error（MSE）来避免随机性。</li>
<li>results: 通过实验 validate了方法的有效性，并且可以降低计算成本和获得关键的kernel ridge regression解。<details>
<summary>Abstract</summary>
Dataset distillation methods have achieved remarkable success in distilling a large dataset into a small set of representative samples. However, they are not designed to produce a distilled dataset that can be effectively used for facilitating self-supervised pre-training. To this end, we propose a novel problem of distilling an unlabeled dataset into a set of small synthetic samples for efficient self-supervised learning (SSL). We first prove that a gradient of synthetic samples with respect to a SSL objective in naive bilevel optimization is \textit{biased} due to the randomness originating from data augmentations or masking. To address this issue, we propose to minimize the mean squared error (MSE) between a model's representations of the synthetic examples and their corresponding learnable target feature representations for the inner objective, which does not introduce any randomness. Our primary motivation is that the model obtained by the proposed inner optimization can mimic the \textit{self-supervised target model}. To achieve this, we also introduce the MSE between representations of the inner model and the self-supervised target model on the original full dataset for outer optimization. Lastly, assuming that a feature extractor is fixed, we only optimize a linear head on top of the feature extractor, which allows us to reduce the computational cost and obtain a closed-form solution of the head with kernel ridge regression. We empirically validate the effectiveness of our method on various applications involving transfer learning.
</details>
<details>
<summary>摘要</summary>
dataset 简化方法已经取得了很大的成功，将大量数据简化成一小集 representative samples。然而，它们并不是为生成可以有效地用于自动学习的简化数据集设计的。为此，我们提出了一个新的问题：简化一个没有标签的数据集成一小组小样本，以便高效地进行自动学习（SSL）。我们首先证明了在随机数据扩充或masking中引入的随机性导致的 SSL 目标函数在 naive bilevel 优化中的梯度是偏移的。为解决这个问题，我们提议将内部目标函数设置为 mean squared error（MSE），这样不会引入随机性。我们的主要动机是希望通过提议的内部优化来模仿自动学习目标模型。为此，我们还引入了 MSE  между representations of the inner model 和 self-supervised target model 在原始全 dataset 上，用于外部优化。最后，我们假设了一个固定的 feature extractor，只有在 feature extractor 上进行 linear head 的优化，这使得我们可以降低计算成本并获得一个关于 kernel ridge regression 的闭合型解。我们实际验证了我们的方法在不同应用中的转移学习中的效果。
</details></li>
</ul>
<hr>
<h2 id="Runway-Sign-Classifier-A-DAL-C-Certifiable-Machine-Learning-System"><a href="#Runway-Sign-Classifier-A-DAL-C-Certifiable-Machine-Learning-System" class="headerlink" title="Runway Sign Classifier: A DAL C Certifiable Machine Learning System"></a>Runway Sign Classifier: A DAL C Certifiable Machine Learning System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06506">http://arxiv.org/abs/2310.06506</a></li>
<li>repo_url: None</li>
<li>paper_authors: Konstantin Dmitriev, Johann Schumann, Islam Bostanov, Mostafa Abdelhamid, Florian Holzapfel</li>
<li>For: This paper aims to address the certification challenges of Machine Learning (ML) based systems for medium criticality airborne applications.* Methods: The authors use a Deep Neural Network (DNN) for airport sign detection and classification, and employ an established architectural mitigation technique involving two redundant and dissimilar DNNs. They also use novel ML-specific data management techniques to enhance this approach.* Results: The authors demonstrate compliance with Design Assurance Level (DAL) C, which is a more stringent requirement than their previous work that achieved DAL D.<details>
<summary>Abstract</summary>
In recent years, the remarkable progress of Machine Learning (ML) technologies within the domain of Artificial Intelligence (AI) systems has presented unprecedented opportunities for the aviation industry, paving the way for further advancements in automation, including the potential for single pilot or fully autonomous operation of large commercial airplanes. However, ML technology faces major incompatibilities with existing airborne certification standards, such as ML model traceability and explainability issues or the inadequacy of traditional coverage metrics. Certification of ML-based airborne systems using current standards is problematic due to these challenges. This paper presents a case study of an airborne system utilizing a Deep Neural Network (DNN) for airport sign detection and classification. Building upon our previous work, which demonstrates compliance with Design Assurance Level (DAL) D, we upgrade the system to meet the more stringent requirements of Design Assurance Level C. To achieve DAL C, we employ an established architectural mitigation technique involving two redundant and dissimilar Deep Neural Networks. The application of novel ML-specific data management techniques further enhances this approach. This work is intended to illustrate how the certification challenges of ML-based systems can be addressed for medium criticality airborne applications.
</details>
<details>
<summary>摘要</summary>
This paper presents a case study of an airborne system that utilizes a Deep Neural Network (DNN) for airport sign detection and classification. Building on our previous work, which demonstrated compliance with Design Assurance Level (DAL) D, we upgraded the system to meet the more stringent requirements of DAL C. To achieve DAL C, we employed an established architectural mitigation technique involving two redundant and dissimilar DNNs. Additionally, we applied novel ML-specific data management techniques to enhance this approach.The purpose of this work is to demonstrate how the certification challenges of ML-based systems can be addressed for medium criticality airborne applications. By upgrading the system to meet DAL C requirements, we were able to demonstrate the feasibility of certifying ML-based airborne systems for use in the aviation industry.
</details></li>
</ul>
<hr>
<h2 id="Variance-Reduced-Online-Gradient-Descent-for-Kernelized-Pairwise-Learning-with-Limited-Memory"><a href="#Variance-Reduced-Online-Gradient-Descent-for-Kernelized-Pairwise-Learning-with-Limited-Memory" class="headerlink" title="Variance Reduced Online Gradient Descent for Kernelized Pairwise Learning with Limited Memory"></a>Variance Reduced Online Gradient Descent for Kernelized Pairwise Learning with Limited Memory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06483">http://arxiv.org/abs/2310.06483</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/halquabeh/acml-2023-fpogd-code">https://github.com/halquabeh/acml-2023-fpogd-code</a></li>
<li>paper_authors: Hilal AlQuabeh, Bhaskar Mukhoty, Bin Gu</li>
<li>for: 这篇论文主要写于对在线对照学习中进行对拼式学习的问题上。</li>
<li>methods: 这篇论文提出了一种基于内存限制的在线对照学习算法，该算法可以扩展到内核在线对照学习，并提高了下降 regret。特别是，我们建立了在线对照学习中 gradient 的方差和回归关系，并使用最近的 stratified 样本buffer 大小为 $s$ 来计算在线对照学习的 gradient，其复杂度为 $O(sT)$。</li>
<li>results: 我们的 теоретиче研究表明，在线对照学习中使用方差减少的 gradient 会导致下降 regret 的改进 bound。实验结果表明，我们的算法在实际数据上比 both kernelized 和 linear 在线对照学习算法更有优势。<details>
<summary>Abstract</summary>
Pairwise learning is essential in machine learning, especially for problems involving loss functions defined on pairs of training examples. Online gradient descent (OGD) algorithms have been proposed to handle online pairwise learning, where data arrives sequentially. However, the pairwise nature of the problem makes scalability challenging, as the gradient computation for a new sample involves all past samples. Recent advancements in OGD algorithms have aimed to reduce the complexity of calculating online gradients, achieving complexities less than $O(T)$ and even as low as $O(1)$. However, these approaches are primarily limited to linear models and have induced variance. In this study, we propose a limited memory OGD algorithm that extends to kernel online pairwise learning while improving the sublinear regret. Specifically, we establish a clear connection between the variance of online gradients and the regret, and construct online gradients using the most recent stratified samples with a limited buffer of size of $s$ representing all past data, which have a complexity of $O(sT)$ and employs $O(\sqrt{T}\log{T})$ random Fourier features for kernel approximation. Importantly, our theoretical results demonstrate that the variance-reduced online gradients lead to an improved sublinear regret bound. The experiments on real-world datasets demonstrate the superiority of our algorithm over both kernelized and linear online pairwise learning algorithms.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换给定文本到简化中文。<</SYS>>在机器学习中，对于基于对例学习的问题，对例学习是非常重要的。在线 gradient descent（OGD）算法已经提出来处理在线对例学习，数据顺序到达时进行学习。然而，对例性问题的特点使得扩展性困难，因为新的样本计算gradient时需要所有过去的样本。 latest advances in OGD algorithms have aimed to reduce the complexity of calculating online gradients, achieving complexities less than $O(T)$ and even as low as $O(1)$. However, these approaches are primarily limited to linear models and have induced variance.在这种研究中，我们提出了有限内存OGD算法，扩展到内核在线对例学习，改进了下界 regret。具体来说，我们确定在线 gradients的方差和 regret之间的关系，并使用最近的降序排序样本buffer的大小为$s$，表示所有过去的数据，其复杂度为$O(sT)$。此外，我们还使用$O(\sqrt{T}\log{T})$个随机傅立叶特征来近似内核。我们的理论结果表明，减少方差的在线 gradients会导致改进的下界 regret bound。实验表明，我们的算法在实际 dataset 上比 both kernelized 和 linear online pairwise learning algorithms 高效。
</details></li>
</ul>
<hr>
<h2 id="An-improved-CTGAN-for-data-processing-method-of-imbalanced-disk-failure"><a href="#An-improved-CTGAN-for-data-processing-method-of-imbalanced-disk-failure" class="headerlink" title="An improved CTGAN for data processing method of imbalanced disk failure"></a>An improved CTGAN for data processing method of imbalanced disk failure</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06481">http://arxiv.org/abs/2310.06481</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingbo Jia, Peng Wu, Hussain Dawood</li>
<li>for:  solves the problem of insufficient failure data and imbalance between normal and failure data in disk failure diagnosis.</li>
<li>methods:  uses an improved Conditional Tabular Generative Adversarial Networks (CTGAN) with a residual network and a classifier for specific category discrimination, as well as a discriminator based on residual network.</li>
<li>results:  the synthesized data can further improve the fault diagnosis accuracy of the classifier, as demonstrated by the experimental results.Here is the text in Simplified Chinese:</li>
<li>for: 解决磁盘故障诊断中缺乏磁盘故障数据和正常数据异常分布的问题。</li>
<li>methods: 利用改进后的Conditional Tabular Generative Adversarial Networks (CTGAN)，加入分类器进行特定类别划分，以及基于差异网络的探测器。</li>
<li>results: 通过实验结果表明，使用RCTGAN生成的数据可以进一步提高磁盘故障诊断精度。<details>
<summary>Abstract</summary>
To address the problem of insufficient failure data generated by disks and the imbalance between the number of normal and failure data. The existing Conditional Tabular Generative Adversarial Networks (CTGAN) deep learning methods have been proven to be effective in solving imbalance disk failure data. But CTGAN cannot learn the internal information of disk failure data very well. In this paper, a fault diagnosis method based on improved CTGAN, a classifier for specific category discrimination is added and a discriminator generate adversarial network based on residual network is proposed. We named it Residual Conditional Tabular Generative Adversarial Networks (RCTGAN). Firstly, to enhance the stability of system a residual network is utilized. RCTGAN uses a small amount of real failure data to synthesize fake fault data; Then, the synthesized data is mixed with the real data to balance the amount of normal and failure data; Finally, four classifier (multilayer perceptron, support vector machine, decision tree, random forest) models are trained using the balanced data set, and the performance of the models is evaluated using G-mean. The experimental results show that the data synthesized by the RCTGAN can further improve the fault diagnosis accuracy of the classifier.
</details>
<details>
<summary>摘要</summary>
Firstly, to enhance the stability of the system, a residual network is utilized. RCTGAN uses a small amount of real failure data to synthesize fake fault data, then the synthesized data is mixed with the real data to balance the amount of normal and failure data. Finally, four classifier (multilayer perceptron, support vector machine, decision tree, random forest) models are trained using the balanced data set, and the performance of the models is evaluated using G-mean. The experimental results show that the data synthesized by the RCTGAN can further improve the fault diagnosis accuracy of the classifier.
</details></li>
</ul>
<hr>
<h2 id="Asynchronous-Federated-Learning-with-Incentive-Mechanism-Based-on-Contract-Theory"><a href="#Asynchronous-Federated-Learning-with-Incentive-Mechanism-Based-on-Contract-Theory" class="headerlink" title="Asynchronous Federated Learning with Incentive Mechanism Based on Contract Theory"></a>Asynchronous Federated Learning with Incentive Mechanism Based on Contract Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06448">http://arxiv.org/abs/2310.06448</a></li>
<li>repo_url: None</li>
<li>paper_authors: Danni Yang, Yun Ji, Zhoubin Kou, Xiaoxiong Zhong, Sheng Zhang</li>
<li>for: 提高 federated learning 中客户端的参与度和质量，并吸引高质量客户端参与。</li>
<li>methods: 基于合同理论的奖励机制，适应性地调整客户端的本地模型训练epoch数，考虑因素如时间延迟和测试准确率。</li>
<li>results: 在 MNIST  dataset 上进行了实验，测试精度与 FedAvg 和 FedProx 无攻击情况下相比，提高了 3.12% 和 5.84%；相比理想的本地 SGD，在攻击情况下提高了 1.35%。此外，在寻求同目标准确率情况下，我们的框架需要较少的计算时间。<details>
<summary>Abstract</summary>
To address the challenges posed by the heterogeneity inherent in federated learning (FL) and to attract high-quality clients, various incentive mechanisms have been employed. However, existing incentive mechanisms are typically utilized in conventional synchronous aggregation, resulting in significant straggler issues. In this study, we propose a novel asynchronous FL framework that integrates an incentive mechanism based on contract theory. Within the incentive mechanism, we strive to maximize the utility of the task publisher by adaptively adjusting clients' local model training epochs, taking into account factors such as time delay and test accuracy. In the asynchronous scheme, considering client quality, we devise aggregation weights and an access control algorithm to facilitate asynchronous aggregation. Through experiments conducted on the MNIST dataset, the simulation results demonstrate that the test accuracy achieved by our framework is 3.12% and 5.84% higher than that achieved by FedAvg and FedProx without any attacks, respectively. The framework exhibits a 1.35% accuracy improvement over the ideal Local SGD under attacks. Furthermore, aiming for the same target accuracy, our framework demands notably less computation time than both FedAvg and FedProx.
</details>
<details>
<summary>摘要</summary>
在聚合学习（FL）中处理多样性的挑战和吸引高质量客户端的吸引力，各种奖励机制已经被应用。然而，现有的奖励机制通常在同步聚合中使用，导致了显著的延迟问题。在这项研究中，我们提出了一种新的异步FL框架，该框架 integrate了基于合同理论的奖励机制。在奖励机制中，我们尝试以最大化任务发布者的利益为目标，通过调整客户端本地模型训练 epoch，考虑因素如时间延迟和测试准确率。在异步方案中，考虑客户端质量，我们设计了聚合权重和访问控制算法，以便异步聚合。经过在MNIST数据集上进行的实验，实验结果表明，我们的框架测试准确率与FedAvg和FedProx无攻击情况下的测试准确率相差3.12%和5.84%，分别高于FedAvg和FedProx无攻击情况下的测试准确率。此外，我们的框架在攻击情况下与理想的本地SGD准确率之间差不多。此外，在寻求同样的目标准确率情况下，我们的框架需要比FedAvg和FedProx更少的计算时间。
</details></li>
</ul>
<hr>
<h2 id="Rule-Mining-for-Correcting-Classification-Models"><a href="#Rule-Mining-for-Correcting-Classification-Models" class="headerlink" title="Rule Mining for Correcting Classification Models"></a>Rule Mining for Correcting Classification Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06446">http://arxiv.org/abs/2310.06446</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hirofumi Suzuki, Hiroaki Iwashita, Takuya Takagi, Yuta Fujishige, Satoshi Hara</li>
<li>for: This paper is written for developers who need to continually update or correct machine learning models to ensure high prediction accuracy, particularly in complex systems or software.</li>
<li>methods: The paper proposes a correction rule mining approach to acquire a comprehensive list of rules that describe inaccurate subpopulations and how to correct them. The proposed algorithm combines frequent itemset mining and a unique pruning technique for correction rules.</li>
<li>results: The paper found that the proposed algorithm discovered various rules that help collect data insufficiently learned, directly correct model outputs, and analyze concept drift.<details>
<summary>Abstract</summary>
Machine learning models need to be continually updated or corrected to ensure that the prediction accuracy remains consistently high. In this study, we consider scenarios where developers should be careful to change the prediction results by the model correction, such as when the model is part of a complex system or software. In such scenarios, the developers want to control the specification of the corrections. To achieve this, the developers need to understand which subpopulations of the inputs get inaccurate predictions by the model. Therefore, we propose correction rule mining to acquire a comprehensive list of rules that describe inaccurate subpopulations and how to correct them. We also develop an efficient correction rule mining algorithm that is a combination of frequent itemset mining and a unique pruning technique for correction rules. We observed that the proposed algorithm found various rules which help to collect data insufficiently learned, directly correct model outputs, and analyze concept drift.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Deep-reinforcement-learning-uncovers-processes-for-separating-azeotropic-mixtures-without-prior-knowledge"><a href="#Deep-reinforcement-learning-uncovers-processes-for-separating-azeotropic-mixtures-without-prior-knowledge" class="headerlink" title="Deep reinforcement learning uncovers processes for separating azeotropic mixtures without prior knowledge"></a>Deep reinforcement learning uncovers processes for separating azeotropic mixtures without prior knowledge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06415">http://arxiv.org/abs/2310.06415</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/grimmlab/drl4procsyn">https://github.com/grimmlab/drl4procsyn</a></li>
<li>paper_authors: Quirin Göttl, Jonathan Pirnay, Jakob Burger, Dominik G. Grimm</li>
<li>for: 这个论文旨在探讨用深度学习探索器解决化学工程中的流程设计问题。</li>
<li>methods: 该论文使用深度学习探索器，不需要先验知识，可以在各种复杂的计划问题中表现出优秀的性能。</li>
<li>results: 论文通过示例化一种可以自动学习并应用于多种化学系统中的流程设计方法，并且可以将大于99%的材料分离成纯组分。这显示出探索器的计划灵活性和可靠性。<details>
<summary>Abstract</summary>
Process synthesis in chemical engineering is a complex planning problem due to vast search spaces, continuous parameters and the need for generalization. Deep reinforcement learning agents, trained without prior knowledge, have shown to outperform humans in various complex planning problems in recent years. Existing work on reinforcement learning for flowsheet synthesis shows promising concepts, but focuses on narrow problems in a single chemical system, limiting its practicality. We present a general deep reinforcement learning approach for flowsheet synthesis. We demonstrate the adaptability of a single agent to the general task of separating binary azeotropic mixtures. Without prior knowledge, it learns to craft near-optimal flowsheets for multiple chemical systems, considering different feed compositions and conceptual approaches. On average, the agent can separate more than 99% of the involved materials into pure components, while autonomously learning fundamental process engineering paradigms. This highlights the agent's planning flexibility, an encouraging step toward true generality.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Adversarial-Robustness-in-Graph-Neural-Networks-A-Hamiltonian-Approach"><a href="#Adversarial-Robustness-in-Graph-Neural-Networks-A-Hamiltonian-Approach" class="headerlink" title="Adversarial Robustness in Graph Neural Networks: A Hamiltonian Approach"></a>Adversarial Robustness in Graph Neural Networks: A Hamiltonian Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06396">http://arxiv.org/abs/2310.06396</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zknus/neurips-2023-hang-robustness">https://github.com/zknus/neurips-2023-hang-robustness</a></li>
<li>paper_authors: Kai Zhao, Qiyu Kang, Yang Song, Rui She, Sijie Wang, Wee Peng Tay</li>
<li>For: 本研究探讨了基于多种神经流的图神经网络（GNNs）的抗震性能，尤其是它们与不同的稳定性观念相关，如BIBO稳定性、 Lyapunov稳定性、结构稳定性和保守稳定性。* Methods: 本文提出了基于物理原理的保守汉密尔顿神经流，用于构建抗震性能强的GNNs。并进行了多种验证 benchmark datasets 上的 adversarial attacks 下的实验，以评估不同神经流GNNs 的抗震性能。* Results: 实验结果表明，基于保守汉密尔顿神经流的GNNs 在 adversarial attacks 下具有显著的抗震性能，而 Lyapunov稳定性并不一定能 garantate  adversarial robustness。<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) are vulnerable to adversarial perturbations, including those that affect both node features and graph topology. This paper investigates GNNs derived from diverse neural flows, concentrating on their connection to various stability notions such as BIBO stability, Lyapunov stability, structural stability, and conservative stability. We argue that Lyapunov stability, despite its common use, does not necessarily ensure adversarial robustness. Inspired by physics principles, we advocate for the use of conservative Hamiltonian neural flows to construct GNNs that are robust to adversarial attacks. The adversarial robustness of different neural flow GNNs is empirically compared on several benchmark datasets under a variety of adversarial attacks. Extensive numerical experiments demonstrate that GNNs leveraging conservative Hamiltonian flows with Lyapunov stability substantially improve robustness against adversarial perturbations. The implementation code of experiments is available at https://github.com/zknus/NeurIPS-2023-HANG-Robustness.
</details>
<details>
<summary>摘要</summary>
“神经网络（GNNs）对于攻击性变化具有漏洞，包括影响节点特征和GraphTopology。这篇论文研究GNNs从多种神经流中派生出来的不同稳定性概念，特别是BIBO稳定性、Lyapunov稳定性、结构稳定性和保守稳定性。我们认为Lyapunov稳定性，即使常用，并不一定能保证攻击适应性。以物理原理为 inspiration，我们倡议使用保守的Hamiltonian神经流创建GNNs，以提高对于攻击性变化的抗性。不同神经流GNNs的攻击适应性在多个Benchmark数据集上进行了实验性的比较。实验结果显示， leveraging conservative Hamiltonian flows with Lyapunov stability can significantly improve the robustness of GNNs against adversarial attacks。相关实验代码可以在https://github.com/zknus/NeurIPS-2023-HANG-Robustness中找到。”Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Harnessing-Administrative-Data-Inventories-to-Create-a-Reliable-Transnational-Reference-Database-for-Crop-Type-Monitoring"><a href="#Harnessing-Administrative-Data-Inventories-to-Create-a-Reliable-Transnational-Reference-Database-for-Crop-Type-Monitoring" class="headerlink" title="Harnessing Administrative Data Inventories to Create a Reliable Transnational Reference Database for Crop Type Monitoring"></a>Harnessing Administrative Data Inventories to Create a Reliable Transnational Reference Database for Crop Type Monitoring</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06393">http://arxiv.org/abs/2310.06393</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/maja601/eurocrops">https://github.com/maja601/eurocrops</a></li>
<li>paper_authors: Maja Schneider, Marco Körner</li>
<li>for: 本研究的目的是开发新的参 Referenced Dataset for 耕地类型分类。</li>
<li>methods: 本研究使用了 Administrative 数据的集成和融合方法，以实现跨国可比的参 Referenced Dataset。</li>
<li>results: 研究实现了一个名为 E URO C ROPS 的参 Referenced Dataset，可以用于耕地类型分类。<details>
<summary>Abstract</summary>
With leaps in machine learning techniques and their applicationon Earth observation challenges has unlocked unprecedented performance across the domain. While the further development of these methods was previously limited by the availability and volume of sensor data and computing resources, the lack of adequate reference data is now constituting new bottlenecks. Since creating such ground-truth information is an expensive and error-prone task, new ways must be devised to source reliable, high-quality reference data on large scales. As an example, we showcase E URO C ROPS, a reference dataset for crop type classification that aggregates and harmonizes administrative data surveyed in different countries with the goal of transnational interoperability.
</details>
<details>
<summary>摘要</summary>
随着机器学习技术的大跃进和其应用于地球观测挑战，已经实现了无 precedent的表现在这个领域。然而，由于感知器数据和计算资源的可用性的限制，这些方法的进一步发展被限制。现在，由于创建这些基准信息是一项昂贵和容易出错的任务，新的方法需要被设计，以获取可靠、高质量的参 refer 数据。作为一个示例，我们展示了E URO C ROPS，一个用于蔬菜类别分类的参 refer 数据集，该数据集在不同国家surveyed的行政数据的基础上，实现了跨国共享和协调。
</details></li>
</ul>
<hr>
<h2 id="CAST-Cluster-Aware-Self-Training-for-Tabular-Data"><a href="#CAST-Cluster-Aware-Self-Training-for-Tabular-Data" class="headerlink" title="CAST: Cluster-Aware Self-Training for Tabular Data"></a>CAST: Cluster-Aware Self-Training for Tabular Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06380">http://arxiv.org/abs/2310.06380</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minwook Kim, Juseong Kim, Kibeom Kim, Donggil Kang, Giltae Song</li>
<li>for: 提高 tabular 数据上的自我训练性能，并适用于各种自我训练方法和模型结构。</li>
<li>methods: 基于 cluster assumption 的 Cluster-Aware Self-Training (CAST) 方法，通过迁移矩阵的方式对权重进行补做，以提高 pseudo-label 的准确性。</li>
<li>results: 在 20 个真实世界数据集上进行了广泛的 empirical 评估，证明 CAST 方法不仅性能更高，还具有在不同的自我训练场景下的稳定性。<details>
<summary>Abstract</summary>
Self-training has gained attraction because of its simplicity and versatility, yet it is vulnerable to noisy pseudo-labels. Several studies have proposed successful approaches to tackle this issue, but they have diminished the advantages of self-training because they require specific modifications in self-training algorithms or model architectures. Furthermore, most of them are incompatible with gradient boosting decision trees, which dominate the tabular domain. To address this, we revisit the cluster assumption, which states that data samples that are close to each other tend to belong to the same class. Inspired by the assumption, we propose Cluster-Aware Self-Training (CAST) for tabular data. CAST is a simple and universally adaptable approach for enhancing existing self-training algorithms without significant modifications. Concretely, our method regularizes the confidence of the classifier, which represents the value of the pseudo-label, forcing the pseudo-labels in low-density regions to have lower confidence by leveraging prior knowledge for each class within the training data. Extensive empirical evaluations on up to 20 real-world datasets confirm not only the superior performance of CAST but also its robustness in various setups in self-training contexts.
</details>
<details>
<summary>摘要</summary>
自适应学习已经吸引了广泛关注，因为它的简单性和灵活性，但它受到噪声 pseudo-label 的威胁。许多研究已经提出了成功的方法来解决这个问题，但这些方法减少了自适应学习的优势，因为它们需要特定的修改在自适应学习算法或模型结构上。此外，大多数方法与梯度拟合树不兼容，梯度拟合树在标量领域占据主导地位。为解决这个问题，我们回到了均匀分布假设，即数据样本在邻近的情况下往往属于同一个类。受到这个假设的激发，我们提出了 Cluster-Aware Self-Training（CAST），这是一种简单而通用的方法，可以增强现有的自适应学习算法，无需重大修改。具体来说，我们的方法规范了分类器的信任值，即 pseudo-label 的值，使低密度区域的 pseudo-labels 的信任值降低，通过利用每个类在训练数据中的先验知识。我们对 Up to 20 个实际数据集进行了广泛的实证评估，并证明了 CAST 不仅在不同的自适应学习设置中表现出色，而且在各种各样的自适应学习上下文中具有强大的稳定性。
</details></li>
</ul>
<hr>
<h2 id="Initialization-Bias-of-Fourier-Neural-Operator-Revisiting-the-Edge-of-Chaos"><a href="#Initialization-Bias-of-Fourier-Neural-Operator-Revisiting-the-Edge-of-Chaos" class="headerlink" title="Initialization Bias of Fourier Neural Operator: Revisiting the Edge of Chaos"></a>Initialization Bias of Fourier Neural Operator: Revisiting the Edge of Chaos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06379">http://arxiv.org/abs/2310.06379</a></li>
<li>repo_url: None</li>
<li>paper_authors: Takeshi Koshizuka, Masahiro Fujisawa, Yusuke Tanaka, Issei Sato</li>
<li>for: 本研究探讨了傅立叶神经网络（FNO）的初始化偏见问题。</li>
<li>methods: 本文基于mean-field理论分析了FNO的行为，从“边缘混乱”角度探讨了前向和后向传播行为的特点，并发现了模式舒缩引起的特殊行为。</li>
<li>results: 建议一种基于He初始化方案的FNO初始化方法，可以缓解FNO的初始化偏见问题，并且实验表明这种方法可以稳定地训练32层FNO，无需额外技术或显著性能下降。<details>
<summary>Abstract</summary>
This paper investigates the initialization bias of the Fourier neural operator (FNO). A mean-field theory for FNO is established, analyzing the behavior of the random FNO from an ``edge of chaos'' perspective. We uncover that the forward and backward propagation behaviors exhibit characteristics unique to FNO, induced by mode truncation, while also showcasing similarities to those of densely connected networks. Building upon this observation, we also propose a FNO version of the He initialization scheme to mitigate the negative initialization bias leading to training instability. Experimental results demonstrate the effectiveness of our initialization scheme, enabling stable training of a 32-layer FNO without the need for additional techniques or significant performance degradation.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:这篇论文研究了傅立叶神经算法（FNO）的初始化偏见。一种mean-field理论被建立，从“边缘化”的角度分析FNO的行为。研究发现，FNO的前向和反向传播行为具有特有的特征，与紧密连接网络类似，但也受到模式舍入的影响。基于这一观察，我们还提出了一种基于FNO的He初始化方案，以缓解初始化偏见，实现了一个32层FNO的稳定训练，无需额外技术或显著性能下降。
</details></li>
</ul>
<hr>
<h2 id="Partition-based-differentially-private-synthetic-data-generation"><a href="#Partition-based-differentially-private-synthetic-data-generation" class="headerlink" title="Partition-based differentially private synthetic data generation"></a>Partition-based differentially private synthetic data generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06371">http://arxiv.org/abs/2310.06371</a></li>
<li>repo_url: None</li>
<li>paper_authors: Meifan Zhang, Dihang Deng, Lihua Yin</li>
<li>for: 这篇论文是为了解决private synthetic data sharing的问题，以实现数据隐私和数据分享之间的 equilibrio。</li>
<li>methods: 这篇论文使用了一个partition-based方法，可以实现对大型资料集的分割和组合，以减少隐私预算的分配问题。</li>
<li>results: 实验结果显示，这篇论文的方法比以前的方法更好，可以生成高质量的实验数据，并且可以实现更好的隐私保证。<details>
<summary>Abstract</summary>
Private synthetic data sharing is preferred as it keeps the distribution and nuances of original data compared to summary statistics. The state-of-the-art methods adopt a select-measure-generate paradigm, but measuring large domain marginals still results in much error and allocating privacy budget iteratively is still difficult. To address these issues, our method employs a partition-based approach that effectively reduces errors and improves the quality of synthetic data, even with a limited privacy budget. Results from our experiments demonstrate the superiority of our method over existing approaches. The synthetic data produced using our approach exhibits improved quality and utility, making it a preferable choice for private synthetic data sharing.
</details>
<details>
<summary>摘要</summary>
<<SYS>>私有的合成数据分享被 preference 为它可以保持原始数据的分布和特点，而不是仅仅是使用摘要统计。现状的方法采用 select-measure-generate 方法，但测量大型领域边缘仍然导致很大的错误，并且分配隐私预算的迭代仍然困难。为解决这些问题，我们的方法使用分区方法，有效地减少错误并提高合成数据的质量，即使具有有限的隐私预算。我们的实验结果表明我们的方法在现有的方法之上具有明显的优势。合成使用我们的方法生成的数据具有更高的质量和用用，使其成为私有合成数据分享的首选。>>>
</details></li>
</ul>
<hr>
<h2 id="DrugCLIP-Contrastive-Protein-Molecule-Representation-Learning-for-Virtual-Screening"><a href="#DrugCLIP-Contrastive-Protein-Molecule-Representation-Learning-for-Virtual-Screening" class="headerlink" title="DrugCLIP: Contrastive Protein-Molecule Representation Learning for Virtual Screening"></a>DrugCLIP: Contrastive Protein-Molecule Representation Learning for Virtual Screening</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06367">http://arxiv.org/abs/2310.06367</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bowen Gao, Bo Qiang, Haichuan Tan, Minsi Ren, Yinjun Jia, Minsi Lu, Jingjing Liu, Weiying Ma, Yanyan Lan</li>
<li>for: 这篇论文旨在提出一个新的药物探测方法，协助找到适合的药物来与特定蛋白质腔填充结构匹配。</li>
<li>methods: 本研究使用了一个新的对称学习框架，称为DrugCLIP，它以对称学习方法来训练保护蛋白质和药物之间的对应关系，不需要运算丰富的资料点。</li>
<li>results: 实验结果显示，DrugCLIP方法可以对多种虚拟探测任务进行高效的预测，特别是在零shot情况下，并且可以与传统的探测方法和超vised学习方法相比较。<details>
<summary>Abstract</summary>
Virtual screening, which identifies potential drugs from vast compound databases to bind with a particular protein pocket, is a critical step in AI-assisted drug discovery. Traditional docking methods are highly time-consuming, and can only work with a restricted search library in real-life applications. Recent supervised learning approaches using scoring functions for binding-affinity prediction, although promising, have not yet surpassed docking methods due to their strong dependency on limited data with reliable binding-affinity labels. In this paper, we propose a novel contrastive learning framework, DrugCLIP, by reformulating virtual screening as a dense retrieval task and employing contrastive learning to align representations of binding protein pockets and molecules from a large quantity of pairwise data without explicit binding-affinity scores. We also introduce a biological-knowledge inspired data augmentation strategy to learn better protein-molecule representations. Extensive experiments show that DrugCLIP significantly outperforms traditional docking and supervised learning methods on diverse virtual screening benchmarks with highly reduced computation time, especially in zero-shot setting.
</details>
<details>
<summary>摘要</summary>
虚拟屏选，可以从庞大的化合物库中标识可能的药物，是人工智能辅助药物发现的关键步骤。传统的停船方法需要很长时间，并且在实际应用中只能使用有限的搜索库。最近的监督学习方法使用紧密度函数预测绑定能力，虽然有承诺，仍然受到有限数据中可靠绑定能力标签的依赖。在这篇论文中，我们提出了一种新的对比学习框架，药物CLIP，通过将虚拟屏选改为密集检索任务，并使用对比学习对绑定蛋白质和分子的表示进行对齐。我们还提出了基于生物知识的数据增强策略，以更好地学习蛋白质-分子表示。广泛的实验表明，药物CLIP在多种虚拟屏选标准准 benchmark上表现出色，特别是在零shot Setting下。
</details></li>
</ul>
<hr>
<h2 id="Core-Intermediate-Peripheral-Index-Factor-Analysis-of-Neighborhood-and-Shortest-Paths-based-Centrality-Metrics"><a href="#Core-Intermediate-Peripheral-Index-Factor-Analysis-of-Neighborhood-and-Shortest-Paths-based-Centrality-Metrics" class="headerlink" title="Core-Intermediate-Peripheral Index: Factor Analysis of Neighborhood and Shortest Paths-based Centrality Metrics"></a>Core-Intermediate-Peripheral Index: Factor Analysis of Neighborhood and Shortest Paths-based Centrality Metrics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06358">http://arxiv.org/abs/2310.06358</a></li>
<li>repo_url: None</li>
<li>paper_authors: Natarajan Meghanathan</li>
<li>for: 本研究使用因子分析对四大邻居和最短路基于中心性指标（度、 eigenvector、betweeness和邻接）的原始数据进行分析，并提出了一种新的量化指标called Core-Intermediate-Peripheral（CIP）指标，用于衡量节点在网络中扮演核心节点（网络中心点的大小值）与边缘节点（网络边缘点的小值）的角色。</li>
<li>methods: 本研究使用变макс基于Eigenvector的因子分析（varimax-based rotation of the Eigenvectors）对中心性指标数据矩阵的转置矩阵进行分析，假设网络中有两个因素（核心和边缘）对节点的中心性指标值产生影响。</li>
<li>results: 本研究在12种复杂的实际世界网络上测试了该方法，并发现CIP指标可以准确地捕捉节点在网络中的中心性和边缘性，并且可以用于评估网络中不同类型节点的中心性和边缘性。<details>
<summary>Abstract</summary>
We perform factor analysis on the raw data of the four major neighborhood and shortest paths-based centrality metrics (Degree, Eigenvector, Betweeenness and Closeness) and propose a novel quantitative measure called the Core-Intermediate-Peripheral (CIP) Index to capture the extent with which a node could play the role of a core node (nodes at the center of a network with larger values for any centrality metric) vis-a-vis a peripheral node (nodes that exist at the periphery of a network with lower values for any centrality metric). We conduct factor analysis (varimax-based rotation of the Eigenvectors) on the transpose matrix of the raw centrality metrics dataset, with the node ids as features, under the hypothesis that there are two factors (core and peripheral) that drive the values incurred by the nodes with respect to the centrality metrics. We test our approach on a diverse suite of 12 complex real-world networks.
</details>
<details>
<summary>摘要</summary>
我们对Raw数据进行因素分析，并提出一种新的量化指标called Core-Intermediate-Peripheral（CIP）指数，用于捕捉节点是核心节点（网络中心部分的节点，具有大于其他中心指标值的任何指标）与边缘节点（网络边缘部分的节点，具有较低的任何指标值）的角色扮演的程度。我们使用变差-基于Eigenvector的因素分析（varimax-based rotation of the Eigenvectors）对转置矩阵中的中心指标数据进行分析，假设网络中有两个因素（核心和边缘）驱动节点的中心指标值。我们对12种不同的实际世界网络进行测试。
</details></li>
</ul>
<hr>
<h2 id="Boosting-Continuous-Control-with-Consistency-Policy"><a href="#Boosting-Continuous-Control-with-Consistency-Policy" class="headerlink" title="Boosting Continuous Control with Consistency Policy"></a>Boosting Continuous Control with Consistency Policy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06343">http://arxiv.org/abs/2310.06343</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuhui Chen, Haoran Li, Dongbin Zhao</li>
<li>for: 提高offline reinforcement learning的效率和精度</li>
<li>methods: 基于强度学习的扩散模型，并用Q学习来更新扩散模型的策略</li>
<li>results: 实验结果显示，CPQL可以快速地改进策略，并且在11个Offline任务和21个Online任务中达到了新的状态纪录，提高了推理速度，相比Diffusion-QL，CPQL的推理速度提高了约45倍。<details>
<summary>Abstract</summary>
Due to its training stability and strong expression, the diffusion model has attracted considerable attention in offline reinforcement learning. However, several challenges have also come with it: 1) The demand for a large number of diffusion steps makes the diffusion-model-based methods time inefficient and limits their applications in real-time control; 2) How to achieve policy improvement with accurate guidance for diffusion model-based policy is still an open problem. Inspired by the consistency model, we propose a novel time-efficiency method named Consistency Policy with Q-Learning (CPQL), which derives action from noise by a single step. By establishing a mapping from the reverse diffusion trajectories to the desired policy, we simultaneously address the issues of time efficiency and inaccurate guidance when updating diffusion model-based policy with the learned Q-function. We demonstrate that CPQL can achieve policy improvement with accurate guidance for offline reinforcement learning, and can be seamlessly extended for online RL tasks. Experimental results indicate that CPQL achieves new state-of-the-art performance on 11 offline and 21 online tasks, significantly improving inference speed by nearly 45 times compared to Diffusion-QL. We will release our code later.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>The demand for a large number of diffusion steps makes the diffusion-model-based methods time-inefficient and limits their applications in real-time control.2. How to achieve policy improvement with accurate guidance for diffusion model-based policy is still an open problem.Inspired by the consistency model, we propose a novel time-efficiency method named Consistency Policy with Q-Learning (CPQL), which derives action from noise by a single step. By establishing a mapping from the reverse diffusion trajectories to the desired policy, we simultaneously address the issues of time efficiency and inaccurate guidance when updating diffusion model-based policy with the learned Q-function.We demonstrate that CPQL can achieve policy improvement with accurate guidance for offline reinforcement learning, and can be seamlessly extended for online RL tasks. Experimental results indicate that CPQL achieves new state-of-the-art performance on 11 offline and 21 online tasks, significantly improving inference speed by nearly 45 times compared to Diffusion-QL. We will release our code later.</details></li>
</ol>
<hr>
<h2 id="Federated-Learning-with-Reduced-Information-Leakage-and-Computation"><a href="#Federated-Learning-with-Reduced-Information-Leakage-and-Computation" class="headerlink" title="Federated Learning with Reduced Information Leakage and Computation"></a>Federated Learning with Reduced Information Leakage and Computation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06341">http://arxiv.org/abs/2310.06341</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tongxin Yin, Xueru Zhang, Mohammad Mahdi Khalili, Mingyan Liu</li>
<li>for: 这 paper 是为了提出一种基于分布式学习的隐私保护机制，以便在多个分布式客户端之间协同学习共同模型，而不需要直接披露本地数据。</li>
<li>methods: 该 paper 使用了一种基于首频采样的方法，其中在每个偶数轮 iteration 中， client 只需要提供一个首频采样，而不需要提供整个数据集。这种方法可以减少了 client 的计算量和隐私泄露。</li>
<li>results: 实验表明，Upcycled-FL 可以在具有不同数据类型的客户端上达到更高的准确率，同时具有更好的隐私保护性和训练时间减少。在 average 的情况下，Upcycled-FL 可以减少 48% 的训练时间。<details>
<summary>Abstract</summary>
Federated learning (FL) is a distributed learning paradigm that allows multiple decentralized clients to collaboratively learn a common model without sharing local data. Although local data is not exposed directly, privacy concerns nonetheless exist as clients' sensitive information can be inferred from intermediate computations. Moreover, such information leakage accumulates substantially over time as the same data is repeatedly used during the iterative learning process. As a result, it can be particularly difficult to balance the privacy-accuracy trade-off when designing privacy-preserving FL algorithms. In this paper, we introduce Upcycled-FL, a novel federated learning framework with first-order approximation applied at every even iteration. Under this framework, half of the FL updates incur no information leakage and require much less computation. We first conduct the theoretical analysis on the convergence (rate) of Upcycled-FL, and then apply perturbation mechanisms to preserve privacy. Experiments on real-world data show that Upcycled-FL consistently outperforms existing methods over heterogeneous data, and significantly improves privacy-accuracy trade-off while reducing 48% of the training time on average.
</details>
<details>
<summary>摘要</summary>
federated learning（FL）是一种分布式学习 paradigma，允许多个分散的客户端共同学习一个共同模型，无需直接分享本地数据。 although local data 不会直接暴露，但是隐私问题仍然存在，因为客户端的敏感信息可以通过中间计算被推断出。此外，这种信息泄露会随着训练过程中的重复使用数据堆积，从而使得在设计隐私保护FL算法时进行平衡隐私精度质量的权衡变得特别困难。在这篇论文中，我们介绍了Upcycled-FL，一种新的联合学习框架，在每次偶数轮中应用首降法。在这个框架下，FL更新中的一半不会导致隐私泄露，同时需要 much less computation。我们首先对Upcycled-FL的抽象分析进行了理论分析，然后通过干扰机制来保护隐私。实验表明，Upcycled-FL在具有多样化数据的实际数据上适用，并在隐私精度质量和训练时间之间进行了显著平衡，而且在平均下降48%的训练时间。
</details></li>
</ul>
<hr>
<h2 id="Automatic-nodule-identification-and-differentiation-in-ultrasound-videos-to-facilitate-per-nodule-examination"><a href="#Automatic-nodule-identification-and-differentiation-in-ultrasound-videos-to-facilitate-per-nodule-examination" class="headerlink" title="Automatic nodule identification and differentiation in ultrasound videos to facilitate per-nodule examination"></a>Automatic nodule identification and differentiation in ultrasound videos to facilitate per-nodule examination</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06339">http://arxiv.org/abs/2310.06339</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siyuan Jiang, Yan Ding, Yuling Wang, Lei Xu, Wenli Dai, Wanru Chang, Jianfeng Zhang, Jie Yu, Jianqiao Zhou, Chunquan Zhang, Ping Liang, Dexing Kong</li>
<li>for:  This paper aims to address the problem of identifying and differentiating nodules in breast ultrasound videos, which is a challenging task due to the heterogeneous appearances of nodules in different cross-sectional views.</li>
<li>methods: The authors collected hundreds of breast ultrasound videos and built a nodule reidentification system that consists of two parts: an extractor based on a deep learning model and a real-time clustering algorithm.</li>
<li>results: The system obtained satisfactory results and was able to differentiate ultrasound videos. This is the first attempt to apply re-identification technique in the ultrasonic field.<details>
<summary>Abstract</summary>
Ultrasound is a vital diagnostic technique in health screening, with the advantages of non-invasive, cost-effective, and radiation free, and therefore is widely applied in the diagnosis of nodules. However, it relies heavily on the expertise and clinical experience of the sonographer. In ultrasound images, a single nodule might present heterogeneous appearances in different cross-sectional views which makes it hard to perform per-nodule examination. Sonographers usually discriminate different nodules by examining the nodule features and the surrounding structures like gland and duct, which is cumbersome and time-consuming. To address this problem, we collected hundreds of breast ultrasound videos and built a nodule reidentification system that consists of two parts: an extractor based on the deep learning model that can extract feature vectors from the input video clips and a real-time clustering algorithm that automatically groups feature vectors by nodules. The system obtains satisfactory results and exhibits the capability to differentiate ultrasound videos. As far as we know, it's the first attempt to apply re-identification technique in the ultrasonic field.
</details>
<details>
<summary>摘要</summary>
乳腺超音波是现代医学检测技术中的一种重要方法，具有不侵入、成本低、无辐射等优点，因此广泛应用于腺体诊断。然而，它受到医生和医疗技术人员的专业知识和临床经验的限制。在超音波图像中，单个腺体可能会显示不同的多样性表现，这使得每个腺体的检测变得困难。医生通常通过评估腺体特征和周围的腺体和腺管来 diferenciation 腺体，这是耗时和耗力的。为解决这个问题，我们收集了数百个乳腺超音波视频，并建立了一个腺体重新标识系统，该系统包括两部分：基于深度学习模型的特征提取器，可以从输入视频剪辑中提取特征向量，以及实时分组算法，可以自动将特征向量分组成不同的腺体。系统取得了满意的结果，并表现出了分辑视频的能力。到目前为止，这是首次应用重新标识技术在超音波领域。
</details></li>
</ul>
<hr>
<h2 id="Learning-bounded-degree-polytrees-with-known-skeleton"><a href="#Learning-bounded-degree-polytrees-with-known-skeleton" class="headerlink" title="Learning bounded-degree polytrees with known skeleton"></a>Learning bounded-degree polytrees with known skeleton</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06333">http://arxiv.org/abs/2310.06333</a></li>
<li>repo_url: None</li>
<li>paper_authors: Davin Choo, Joy Qiping Yang, Arnab Bhattacharyya, Clément L. Canonne</li>
<li>for: efficient proper learning of bounded-degree polytrees</li>
<li>methods: polynomial-time algorithm and information-theoretic sample complexity lower bound</li>
<li>results: finite-sample guarantees for learning $d$-polytrees in polynomial time and sample complexity for any bounded $d$ when the underlying undirected graph (skeleton) is known<details>
<summary>Abstract</summary>
We establish finite-sample guarantees for efficient proper learning of bounded-degree polytrees, a rich class of high-dimensional probability distributions and a subclass of Bayesian networks, a widely-studied type of graphical model. Recently, Bhattacharyya et al. (2021) obtained finite-sample guarantees for recovering tree-structured Bayesian networks, i.e., 1-polytrees. We extend their results by providing an efficient algorithm which learns $d$-polytrees in polynomial time and sample complexity for any bounded $d$ when the underlying undirected graph (skeleton) is known. We complement our algorithm with an information-theoretic sample complexity lower bound, showing that the dependence on the dimension and target accuracy parameters are nearly tight.
</details>
<details>
<summary>摘要</summary>
我们设定有限样本保证的高维概率分布bounded-degree polytrees的有效性学习，这是一种高维概率分布的丰富类型和 bayesian networks的一个子集，这种图形模型广泛研究。 Bhattacharyya et al. (2021) 已经获得了恢复树状 bayesian networks的有限样本保证，我们将其结果扩展，提供了在 полиtrees 中efficient的算法，可以在有 bounded 的 $d$ 下在有限时间内learns 和样本复杂度。我们还补充了信息理论样本复杂度下界，表明我们的样本复杂度和精度参数之间的依赖关系几乎是紧密的。
</details></li>
</ul>
<hr>
<h2 id="Exploit-the-antenna-response-consistency-to-define-the-alignment-criteria-for-CSI-data"><a href="#Exploit-the-antenna-response-consistency-to-define-the-alignment-criteria-for-CSI-data" class="headerlink" title="Exploit the antenna response consistency to define the alignment criteria for CSI data"></a>Exploit the antenna response consistency to define the alignment criteria for CSI data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06328">http://arxiv.org/abs/2310.06328</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ke Xu, Jiangtao Wang, Hongyuan Zhu, Dingchang Zheng</li>
<li>for: 本研究旨在解决WIFI基于人体活动识别（HAR）中自助学习（SSL）的挑战，即缺乏标注数据的问题。</li>
<li>methods: 我们提出了一种解决方案，即ARC（报应响响应一致），用于定义适当的对齐标准。ARC可以保持输入空间中的Semantic信息，并引入了对实际噪声的抗性。</li>
<li>results: 我们通过实验证明了ARC的有效性，它可以提高WIFI基于HAR中自助学习的性能。<details>
<summary>Abstract</summary>
Self-supervised learning (SSL) for WiFi-based human activity recognition (HAR) holds great promise due to its ability to address the challenge of insufficient labeled data. However, directly transplanting SSL algorithms, especially contrastive learning, originally designed for other domains to CSI data, often fails to achieve the expected performance. We attribute this issue to the inappropriate alignment criteria, which disrupt the semantic distance consistency between the feature space and the input space. To address this challenge, we introduce \textbf{A}netenna \textbf{R}esponse \textbf{C}onsistency (ARC) as a solution to define proper alignment criteria. ARC is designed to retain semantic information from the input space while introducing robustness to real-world noise. We analyze ARC from the perspective of CSI data structure, demonstrating that its optimal solution leads to a direct mapping from input CSI data to action vectors in the feature map. Furthermore, we provide extensive experimental evidence to validate the effectiveness of ARC in improving the performance of self-supervised learning for WiFi-based HAR.
</details>
<details>
<summary>摘要</summary>
自我监督学习（SSL） для WiFi-based人体活动识别（HAR）具有很大的推荐力，因为它可以解决因不充分的标注数据而带来的挑战。然而，直接将SSL算法，特别是对比学习，从其他领域直接应用于CSI数据时，经常无法达到预期的性能。我们认为这是因为不适当的对齐标准，导致Feature空间和输入空间之间的semantic distance的一致性被打乱。为解决这个挑战，我们介绍了ARC（自适应响应相关）作为一种解决方案，以定义适当的对齐标准。ARC是一种可以保持输入空间中的semantic信息的算法，同时具有对实际世界噪音的抗针对性。我们从CSI数据结构的角度分析ARC，并证明其最佳解决方案导致输入CSI数据直接映射到功能图中的动作向量。此外，我们还提供了详细的实验证据，以证明ARC在自我监督学习中提高WiFi-based HAR性能的效果。
</details></li>
</ul>
<hr>
<h2 id="Transfer-learning-based-physics-informed-convolutional-neural-network-for-simulating-flow-in-porous-media-with-time-varying-controls"><a href="#Transfer-learning-based-physics-informed-convolutional-neural-network-for-simulating-flow-in-porous-media-with-time-varying-controls" class="headerlink" title="Transfer learning-based physics-informed convolutional neural network for simulating flow in porous media with time-varying controls"></a>Transfer learning-based physics-informed convolutional neural network for simulating flow in porous media with time-varying controls</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06319">http://arxiv.org/abs/2310.06319</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jungang Chen, Eduardo Gildin, John E. Killough</li>
<li>for: 这个论文是用来模拟两相流体在孔隙媒体中的模型，并且使用时间变化的井控制来实现控制-到-状态 regression。</li>
<li>methods: 这个模型使用了finite volume scheme来离散流程方程，并且做了loss函数的定义，以保证流体的质量保守定律。Neumann boundary conditions也被细致地包含在半积分方程中，无需额外的损失函数。模型的架构包括两个平行的U-Net结构，输入为井控制，输出为系统状态。</li>
<li>results: 这个模型可以准确地预测油压和水含量在每个时间步骤中，并且可以快速地训练和转移学习。对于不同的储量格和方向，模型的计算效率和准确性都被证明。在对比 numerical方法的计算效率和准确性方面，模型表现出了优异的性能。<details>
<summary>Abstract</summary>
A physics-informed convolutional neural network is proposed to simulate two phase flow in porous media with time-varying well controls. While most of PICNNs in existing literatures worked on parameter-to-state mapping, our proposed network parameterizes the solution with time-varying controls to establish a control-to-state regression. Firstly, finite volume scheme is adopted to discretize flow equations and formulate loss function that respects mass conservation laws. Neumann boundary conditions are seamlessly incorporated into the semi-discretized equations so no additional loss term is needed. The network architecture comprises two parallel U-Net structures, with network inputs being well controls and outputs being the system states. To capture the time-dependent relationship between inputs and outputs, the network is well designed to mimic discretized state space equations. We train the network progressively for every timestep, enabling it to simultaneously predict oil pressure and water saturation at each timestep. After training the network for one timestep, we leverage transfer learning techniques to expedite the training process for subsequent timestep. The proposed model is used to simulate oil-water porous flow scenarios with varying reservoir gridblocks and aspects including computation efficiency and accuracy are compared against corresponding numerical approaches. The results underscore the potential of PICNN in effectively simulating systems with numerous grid blocks, as computation time does not scale with model dimensionality. We assess the temporal error using 10 different testing controls with variation in magnitude and another 10 with higher alternation frequency with proposed control-to-state architecture. Our observations suggest the need for a more robust and reliable model when dealing with controls that exhibit significant variations in magnitude or frequency.
</details>
<details>
<summary>摘要</summary>
提出了一种基于物理学的卷积神经网络（PICNN），用于模拟具有时间变化的两相流体在孔隙媒体中的行为。大多数现有的PICNN都是基于参数到状态映射，而我们提出的网络则使用时间变化的控制来建立控制到状态重 regression。首先，我们采用了 finite volume 方法来离散流体方程，并将损失函数设计为尊重流体保守定律。Neumann 边界条件可以自然地包含在半离散方程中，因此不需要额外的损失项。网络架构包括两个并行的 U-Net 结构，网络输入为控制，输出为系统状态。为了捕捉时间依赖关系 между输入和输出，网络设计得能够模拟离散状态空间方程。我们在每个时间步进行逐步训练，使网络能够同时预测每个时间步的油压和水含量。在训练一个时间步后，我们利用了传输学习技术来加速后续时间步的训练过程。提出的模型用于模拟各种不同的油水孔隙流场景，并进行了对应的numerical方法的比较。结果表明PICNN可以有效地模拟高维度的系统，计算时间不随模型维度增长。我们使用10个测试控制，其中每个控制都有不同的大小和频率，以及另外10个测试控制，其中每个控制都有更高的振荡频率，来评估模型的时间误差。我们的观察表明，当控制 exhibits 显著的变化 в大小或频率时，需要一个更加可靠和可靠的模型。
</details></li>
</ul>
<hr>
<h2 id="Discovering-Mixtures-of-Structural-Causal-Models-from-Time-Series-Data"><a href="#Discovering-Mixtures-of-Structural-Causal-Models-from-Time-Series-Data" class="headerlink" title="Discovering Mixtures of Structural Causal Models from Time Series Data"></a>Discovering Mixtures of Structural Causal Models from Time Series Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06312">http://arxiv.org/abs/2310.06312</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sumanth Varambally, Yi-An Ma, Rose Yu</li>
<li>for: 这 paper 是为了探讨时间序列数据中 causal 关系的推断问题。</li>
<li>methods: 这 paper 使用了一种基于 end-to-end 训练的方法，通过 maximizing 一个 evidence-lower bound 来推断 causal 模型。</li>
<li>results: 经过对 synthetic 和实际数据进行了广泛的实验，这 paper 的方法在 causal discovery 任务中表现出色，特别是当数据来源于多种不同的 causal 图时。  Additionally, the paper proves the identifiability of the model under some mild assumptions.<details>
<summary>Abstract</summary>
In fields such as finance, climate science, and neuroscience, inferring causal relationships from time series data poses a formidable challenge. While contemporary techniques can handle nonlinear relationships between variables and flexible noise distributions, they rely on the simplifying assumption that data originates from the same underlying causal model. In this work, we relax this assumption and perform causal discovery from time series data originating from mixtures of different causal models. We infer both the underlying structural causal models and the posterior probability for each sample belonging to a specific mixture component. Our approach employs an end-to-end training process that maximizes an evidence-lower bound for data likelihood. Through extensive experimentation on both synthetic and real-world datasets, we demonstrate that our method surpasses state-of-the-art benchmarks in causal discovery tasks, particularly when the data emanates from diverse underlying causal graphs. Theoretically, we prove the identifiability of such a model under some mild assumptions.
</details>
<details>
<summary>摘要</summary>
在金融、气候科学和神经科学等领域，从时间序列数据推断 causal 关系是一项具有挑战性的任务。当今技术可以处理非线性变量之间的关系和 flexible 噪声分布，但它们假设数据来自同一个下游 causal 模型。在这种工作中，我们放弃了这个假设，并从时间序列数据来自多种不同 causal 模型的混合中进行 causal 发现。我们推断出下游结构 causal 模型以及每个样本属于特定混合组件的 posterior 概率。我们的方法使用一个端到端的训练过程，以最大化数据可能性的证据下界。经过了大量的实验，我们发现我们的方法在 causal 发现任务中超过了现状征的标准准则，特别是数据来自多种不同 causal 图的情况。从理论角度，我们证明了这样的模型可以在某些轻微假设下进行可 identificability。
</details></li>
</ul>
<hr>
<h2 id="Ensemble-Active-Learning-by-Contextual-Bandits-for-AI-Incubation-in-Manufacturing"><a href="#Ensemble-Active-Learning-by-Contextual-Bandits-for-AI-Incubation-in-Manufacturing" class="headerlink" title="Ensemble Active Learning by Contextual Bandits for AI Incubation in Manufacturing"></a>Ensemble Active Learning by Contextual Bandits for AI Incubation in Manufacturing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06306">http://arxiv.org/abs/2310.06306</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yingyan Zeng, Xiaoyu Chen, Ran Jin</li>
<li>for: 提高流式数据收集中注释努力的Annotation效率，以保持supervised学习基础学习模型的数据质量。</li>
<li>methods: 提议使用ensemble active learning方法，通过contextual bandits实现活动样本标注，保持exploration-exploitation平衡，提高AI模型表现。</li>
<li>results: 实验结果表明，提议方法可以减少注释努力，同时保持数据质量，从而提高AI模型的表现。<details>
<summary>Abstract</summary>
It is challenging but important to save annotation efforts in streaming data acquisition to maintain data quality for supervised learning base learners. We propose an ensemble active learning method to actively acquire samples for annotation by contextual bandits, which is will enforce the exploration-exploitation balance and leading to improved AI modeling performance.
</details>
<details>
<summary>摘要</summary>
“保持流式数据收集中的注释努力是重要的，以确保超参学习基础模型的数据质量。我们提出了一种 ensemble active learning 方法，通过contextual bandits来活动收集样本，以保持探索与利用的平衡，从而提高 AI 模型表现。”Here's a word-for-word translation:“保持流式数据收集中的注释努力是重要的，以确保超参学习基础模型的数据质量。我们提出了一种 ensemble active learning 方法，通过contextual bandits来活动收集样本，以保持探索与利用的平衡，从而提高 AI 模型表现。”Note that Simplified Chinese is used in mainland China, while Traditional Chinese is used in Taiwan and other regions.
</details></li>
</ul>
<hr>
<h2 id="Gem5Pred-Predictive-Approaches-For-Gem5-Simulation-Time"><a href="#Gem5Pred-Predictive-Approaches-For-Gem5-Simulation-Time" class="headerlink" title="Gem5Pred: Predictive Approaches For Gem5 Simulation Time"></a>Gem5Pred: Predictive Approaches For Gem5 Simulation Time</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06290">http://arxiv.org/abs/2310.06290</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tian Yan, Xueyang Li, Sifat Ut Taki, Saeid Mehrdad</li>
<li>for: 本文目的是提供一个用于预测Gem5模拟时间的数据集和基于CodeBERT的三种模型，以满足现有的时间预测问题。</li>
<li>methods: 本文使用了一个专门为这种任务创建的数据集，并采用了三种基于CodeBERT的模型来实现预测任务。</li>
<li>results: 我们的最佳回归模型的 Mean Absolute Error (MAE) 为0.546，而我们的最高精度分类模型的 Accuracy 为0.696。这些模型可以作为未来研究的基础，并且可以与之后的模型进行比较。<details>
<summary>Abstract</summary>
Gem5, an open-source, flexible, and cost-effective simulator, is widely recognized and utilized in both academic and industry fields for hardware simulation. However, the typically time-consuming nature of simulating programs on Gem5 underscores the need for a predictive model that can estimate simulation time. As of now, no such dataset or model exists. In response to this gap, this paper makes a novel contribution by introducing a unique dataset specifically created for this purpose. We also conducted analysis of the effects of different instruction types on the simulation time in Gem5. After this, we employ three distinct models leveraging CodeBERT to execute the prediction task based on the developed dataset. Our superior regression model achieves a Mean Absolute Error (MAE) of 0.546, while our top-performing classification model records an Accuracy of 0.696. Our models establish a foundation for future investigations on this topic, serving as benchmarks against which subsequent models can be compared. We hope that our contribution can simulate further research in this field. The dataset we used is available at https://github.com/XueyangLiOSU/Gem5Pred.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Better-and-Simpler-Lower-Bounds-for-Differentially-Private-Statistical-Estimation"><a href="#Better-and-Simpler-Lower-Bounds-for-Differentially-Private-Statistical-Estimation" class="headerlink" title="Better and Simpler Lower Bounds for Differentially Private Statistical Estimation"></a>Better and Simpler Lower Bounds for Differentially Private Statistical Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06289">http://arxiv.org/abs/2310.06289</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shyam Narayanan</li>
<li>for: 这两个研究目标是为了实现精度高的private estimationTask，具体来说是covariance estimation和mean estimation。</li>
<li>methods: 这两个研究使用了fingerprinting方法和bayesian方法来实现private estimation。</li>
<li>results: 这两个研究得到了以下结论：	+ 对于covariance estimation，需要有 $\tilde{\Omega}\left(\frac{d^{3&#x2F;2}{\alpha \varepsilon} + \frac{d}{\alpha^2}\right)$ 样本，这是前一个研究的改进版本，且是 simpler than previous work。	+ 对于mean estimation，需要有 $\tilde{\Omega}\left(\frac{d}{\alpha^{k&#x2F;(k-1)} \varepsilon} + \frac{d}{\alpha^2}\right)$ 样本，这与已知的Upper bound相符，并且超过了对于纯 diferencial privacy 的最佳下界。<details>
<summary>Abstract</summary>
We provide improved lower bounds for two well-known high-dimensional private estimation tasks. First, we prove that for estimating the covariance of a Gaussian up to spectral error $\alpha$ with approximate differential privacy, one needs $\tilde{\Omega}\left(\frac{d^{3/2}{\alpha \varepsilon} + \frac{d}{\alpha^2}\right)$ samples for any $\alpha \le O(1)$, which is tight up to logarithmic factors. This improves over previous work which established this for $\alpha \le O\left(\frac{1}{\sqrt{d}\right)$, and is also simpler than previous work. Next, we prove that for estimating the mean of a heavy-tailed distribution with bounded $k$th moments with approximate differential privacy, one needs $\tilde{\Omega}\left(\frac{d}{\alpha^{k/(k-1)} \varepsilon} + \frac{d}{\alpha^2}\right)$ samples. This matches known upper bounds and improves over the best known lower bound for this problem, which only hold for pure differential privacy, or when $k = 2$. Our techniques follow the method of fingerprinting and are generally quite simple. Our lower bound for heavy-tailed estimation is based on a black-box reduction from privately estimating identity-covariance Gaussians. Our lower bound for covariance estimation utilizes a Bayesian approach to show that, under an Inverse Wishart prior distribution for the covariance matrix, no private estimator can be accurate even in expectation, without sufficiently many samples.
</details>
<details>
<summary>摘要</summary>
我们提供了几个改进的下界 для两个高维度私人推导任务。首先，我们证明了为了在 Gaussian 的均值上进行约定 $\alpha$ 的私人推导，需要 $\tilde{\Omega}\left(\frac{d^{3/2}{\alpha \varepsilon} + \frac{d}{\alpha^2}\right)$ 样本，这是对于任何 $\alpha \le O(1)$ 都是严格的下界，并且比前一次的成果更为简单。其次，我们证明了在具有bounded $k$th  moments 的非常粗糙分布上进行均值推导时，需要 $\tilde{\Omega}\left(\frac{d}{\alpha^{k/(k-1)} \varepsilon} + \frac{d}{\alpha^2}\right)$ 样本，这与知名的上界相匹配，并且超过了对于纯粹的推导性能的下界，只有在 $k = 2$ 时才能推导出。我们的技术基于指纹技术，通常很简单。我们的下界 для 均值推导基于黑盒减少，具体来说是从私人推导均值 Gaussian 的方向下减少。我们的下界 для 均值推导使用了 bayesian 方法，证明在对均值矩阵的 inverse wishart 分布下，没有私人推导器可以在预期中准确地推导，不具备充分的样本。
</details></li>
</ul>
<hr>
<h2 id="Bi-Level-Offline-Policy-Optimization-with-Limited-Exploration"><a href="#Bi-Level-Offline-Policy-Optimization-with-Limited-Exploration" class="headerlink" title="Bi-Level Offline Policy Optimization with Limited Exploration"></a>Bi-Level Offline Policy Optimization with Limited Exploration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06268">http://arxiv.org/abs/2310.06268</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenzhuo Zhou</li>
<li>for: 学习无线连续学习（RL）策略，以固定的预先收集的数据集进行学习。</li>
<li>methods: 提出了一种二级结构化策略优化算法，该算法模型了策略（上级）和价值函数（下级）之间的层次结构。下级专注于构建一个可靠的价值估计集，以保持小 enough的加权巴尔曼错误平均值，同时控制数据不一致性引起的uncertainty。而上级则尝试通过下级提供的保守价值估计集来最大化策略。</li>
<li>results: 在使用 synthetic、标准 benchmark 和实际世界数据集进行评估中，我们的模型与现状顶尖方法竞争性表现。<details>
<summary>Abstract</summary>
We study offline reinforcement learning (RL) which seeks to learn a good policy based on a fixed, pre-collected dataset. A fundamental challenge behind this task is the distributional shift due to the dataset lacking sufficient exploration, especially under function approximation. To tackle this issue, we propose a bi-level structured policy optimization algorithm that models a hierarchical interaction between the policy (upper-level) and the value function (lower-level). The lower level focuses on constructing a confidence set of value estimates that maintain sufficiently small weighted average Bellman errors, while controlling uncertainty arising from distribution mismatch. Subsequently, at the upper level, the policy aims to maximize a conservative value estimate from the confidence set formed at the lower level. This novel formulation preserves the maximum flexibility of the implicitly induced exploratory data distribution, enabling the power of model extrapolation. In practice, it can be solved through a computationally efficient, penalized adversarial estimation procedure. Our theoretical regret guarantees do not rely on any data-coverage and completeness-type assumptions, only requiring realizability. These guarantees also demonstrate that the learned policy represents the "best effort" among all policies, as no other policies can outperform it. We evaluate our model using a blend of synthetic, benchmark, and real-world datasets for offline RL, showing that it performs competitively with state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)我们研究无线RL，它的目标是基于预先收集的固定数据集学习一个好策略。然而，这个任务面临着数据分布变化的挑战，尤其是在函数近似下。为解决这个问题，我们提出了一个二级结构化策略优化算法，它模型了策略（上层）和价值函数（下层）之间的层次交互。下层关注于建立一个可靠的价值估计集，使其保持小于一定的均值 Bellman 误差，同时控制由数据分布匹配引起的uncertainty。而上层则是通过最大化一个保守的价值估计来优化策略。这种新的表述保留了隐式引入的探索数据分布的最大灵活性，使得模型渐近。在实践中，它可以通过一种 computationally efficient 的偏好对抗估计过程解决。我们的理论 regret 保证不需要任何数据覆盖和完整性类型的假设，只需要可行性。这些保证还证明了学习的策略是所有策略中的"最佳努力"，因为没有其他策略可以超越它。我们使用了一个混合的synthetic、benchmark和实际数据集来评估我们的模型，并显示它与当前的方法竞争。
</details></li>
</ul>
<hr>
<h2 id="A-Unified-View-on-Solving-Objective-Mismatch-in-Model-Based-Reinforcement-Learning"><a href="#A-Unified-View-on-Solving-Objective-Mismatch-in-Model-Based-Reinforcement-Learning" class="headerlink" title="A Unified View on Solving Objective Mismatch in Model-Based Reinforcement Learning"></a>A Unified View on Solving Objective Mismatch in Model-Based Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06253">http://arxiv.org/abs/2310.06253</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ran Wei, Nathan Lambert, Anthony McDonald, Alfredo Garcia, Roberto Calandra</li>
<li>for: 本研究的目的是实现更有效率、适应性和可解释性的实验学习（Model-based Reinforcement Learning，MBRL）代理人。</li>
<li>methods: 本研究使用了多种解决问题对照的方法，包括：	+ 对照环境模型的学习，以便对环境进行预测和决策。	+ 使用对照环境模型的训练，以便改善对环境的理解和决策。	+ 使用不同的评估标准，以便评估模型和策略的效果。</li>
<li>results: 本研究发现，现有的MBRL方法通常会受到“目标差异”的问题，即模型预测的对环境的准确性与策略优化的对环境的回应有所差异。此外，本研究还发现了一些相关的解决方案，包括：	+ 对环境模型的适应和更新。	+ 使用不同的策略优化方法。	+ 使用不同的评估标准。<details>
<summary>Abstract</summary>
Model-based Reinforcement Learning (MBRL) aims to make agents more sample-efficient, adaptive, and explainable by learning an explicit model of the environment. While the capabilities of MBRL agents have significantly improved in recent years, how to best learn the model is still an unresolved question. The majority of MBRL algorithms aim at training the model to make accurate predictions about the environment and subsequently using the model to determine the most rewarding actions. However, recent research has shown that model predictive accuracy is often not correlated with action quality, tracing the root cause to the \emph{objective mismatch} between accurate dynamics model learning and policy optimization of rewards. A number of interrelated solution categories to the objective mismatch problem have emerged as MBRL continues to mature as a research area. In this work, we provide an in-depth survey of these solution categories and propose a taxonomy to foster future research.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>环境模型学习与奖励政策优化的对应性问题。2. 使用不同的优化目标来解决对应性问题。3. 使用多个模型来学习环境和奖励政策之间的关系。4. 使用环境感知技术来改善模型的准确性。5. 使用不同的学习策略来解决对应性问题。本文提供了 MBRL 这些解决方案的深入概述，并提出了一种分类法，以便未来的研究。</details></li>
</ol>
<hr>
<h2 id="Deep-Learning-A-Tutorial"><a href="#Deep-Learning-A-Tutorial" class="headerlink" title="Deep Learning: A Tutorial"></a>Deep Learning: A Tutorial</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06251">http://arxiv.org/abs/2310.06251</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ChristosChristofidis/awesome-deep-learning">https://github.com/ChristosChristofidis/awesome-deep-learning</a></li>
<li>paper_authors: Nick Polson, Vadim Sokolov</li>
<li>for: 这篇论文的目的是为了提供深度学习方法，以提供结构化高维数据的预测和不确定性评估。</li>
<li>methods: 这篇论文使用层次的半同构输入变换来提供预测规则，而不是使用大多数统计模型中的浅层添加性。</li>
<li>results: 这篇论文通过应用层次变换来生成一组特征（或特征），然后使用概率统计方法进行预测。这种方法可以同时实现缩放预测规则和不确定性评估。<details>
<summary>Abstract</summary>
Our goal is to provide a review of deep learning methods which provide insight into structured high-dimensional data. Rather than using shallow additive architectures common to most statistical models, deep learning uses layers of semi-affine input transformations to provide a predictive rule. Applying these layers of transformations leads to a set of attributes (or, features) to which probabilistic statistical methods can be applied. Thus, the best of both worlds can be achieved: scalable prediction rules fortified with uncertainty quantification, where sparse regularization finds the features.
</details>
<details>
<summary>摘要</summary>
我们的目标是为深度学习方法进行评估，以获得结构化高维数据的深入理解。而不是使用大多数统计模型常用的浅层添加性架构，深度学习使用层次的半 Similarity输入变换来提供预测规则。通过这些层次变换，可以获得一组特征（或者特征），这些特征可以通过 probabilistic 统计方法进行评估。因此，可以实现最好的两个世界：可扩展的预测规则和不确定性评估，而 sparse 正则化可以找到特征。Note: "Simplified Chinese" is a translation of "Traditional Chinese" and "简化字" (Simplified Chinese) is a romanization of "简化字" (Simplified Chinese characters).
</details></li>
</ul>
<hr>
<h2 id="Sample-Efficient-Multi-Agent-RL-An-Optimization-Perspective"><a href="#Sample-Efficient-Multi-Agent-RL-An-Optimization-Perspective" class="headerlink" title="Sample-Efficient Multi-Agent RL: An Optimization Perspective"></a>Sample-Efficient Multi-Agent RL: An Optimization Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06243">http://arxiv.org/abs/2310.06243</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nuoya Xiong, Zhihan Liu, Zhaoran Wang, Zhuoran Yang</li>
<li>for: 该 paper 探讨了多智能体强化学习 (MARL) 在通用函数近似下的多智能体Markov 游戏 (MG) 中的学习问题。</li>
<li>methods: 作者引入了一个新的复杂度度量called Multi-Agent Decoupling Coefficient (MADC)，以确定学习过程中的最小假设。基于这个度量，作者提出了第一个可以保证学习效率的算法框架。</li>
<li>results: 作者的算法可以在学习 Nash 均衡、粗略相关均衡和相关均衡问题中达到相对较低的梯度损失，并且与现有的算法相比具有相似的折衡 regret。此外，作者的算法可以避免在数据依赖的约束中解决各个对象的优化问题，从而更易于实际应用。<details>
<summary>Abstract</summary>
We study multi-agent reinforcement learning (MARL) for the general-sum Markov Games (MGs) under the general function approximation. In order to find the minimum assumption for sample-efficient learning, we introduce a novel complexity measure called the Multi-Agent Decoupling Coefficient (MADC) for general-sum MGs. Using this measure, we propose the first unified algorithmic framework that ensures sample efficiency in learning Nash Equilibrium, Coarse Correlated Equilibrium, and Correlated Equilibrium for both model-based and model-free MARL problems with low MADC. We also show that our algorithm provides comparable sublinear regret to the existing works. Moreover, our algorithm combines an equilibrium-solving oracle with a single objective optimization subprocedure that solves for the regularized payoff of each deterministic joint policy, which avoids solving constrained optimization problems within data-dependent constraints (Jin et al. 2020; Wang et al. 2023) or executing sampling procedures with complex multi-objective optimization problems (Foster et al. 2023), thus being more amenable to empirical implementation.
</details>
<details>
<summary>摘要</summary>
我们研究多体学习（MARL）的总和游戏（MG）下的通用函数近似下的多体减噪系数（MADC），以找到最小的假设，以实现样本效率的学习。我们提出了第一个统一的算法框架，可以保证样本效率的学习 Nash 平衡、粗 corr 平衡和相关平衡，并且可以避免在数据依赖的约束下解决减噪问题（Jin et al. 2020；Wang et al. 2023）或者在复杂多目标优化问题下执行抽象多目标优化问题（Foster et al. 2023）。这使我们的算法更易于实际应用。
</details></li>
</ul>
<hr>
<h2 id="A-Bayesian-framework-for-discovering-interpretable-Lagrangian-of-dynamical-systems-from-data"><a href="#A-Bayesian-framework-for-discovering-interpretable-Lagrangian-of-dynamical-systems-from-data" class="headerlink" title="A Bayesian framework for discovering interpretable Lagrangian of dynamical systems from data"></a>A Bayesian framework for discovering interpretable Lagrangian of dynamical systems from data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06241">http://arxiv.org/abs/2310.06241</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tapas Tripura, Souvik Chakraborty</li>
<li>for: 学习和预测物理系统的动力学行为需要深入了解物理法律的基础。这些工作把描述物理系统的方程写入机器学习模型中，而我们提议一种 alternate 方法，使用稀疏泊描述理论来学习物理系统的几何特征。</li>
<li>methods: 我们的方法包括（a）生成可解释的劳伦茨描述，（b）使用 bayesian 学习来衡量限制数据的epistemic不确定性，（c）通过列朗德转换自动生成 Hamiltonian，并（d）提供 ODE 和 PDE 基于的描述。</li>
<li>results: 我们在六个不同的示例中证明了我们的方法的可行性，这些示例包括 both discrete 和连续系统。<details>
<summary>Abstract</summary>
Learning and predicting the dynamics of physical systems requires a profound understanding of the underlying physical laws. Recent works on learning physical laws involve generalizing the equation discovery frameworks to the discovery of Hamiltonian and Lagrangian of physical systems. While the existing methods parameterize the Lagrangian using neural networks, we propose an alternate framework for learning interpretable Lagrangian descriptions of physical systems from limited data using the sparse Bayesian approach. Unlike existing neural network-based approaches, the proposed approach (a) yields an interpretable description of Lagrangian, (b) exploits Bayesian learning to quantify the epistemic uncertainty due to limited data, (c) automates the distillation of Hamiltonian from the learned Lagrangian using Legendre transformation, and (d) provides ordinary (ODE) and partial differential equation (PDE) based descriptions of the observed systems. Six different examples involving both discrete and continuous system illustrates the efficacy of the proposed approach.
</details>
<details>
<summary>摘要</summary>
学习和预测物理系统的动力学需要深刻的物理知识。现有的工作是把物理法则推广到物理系统的寻找方程的发现。而现有的方法通常使用神经网络参数化拉格朗日函数，我们提议一种 alternate 的方法，使得可以从有限数据获得可读性的拉格朗日描述，并且可以量化有限数据所带来的epistemic不确定性。此外，该方法还可以自动从学习到的拉格朗日函数中提取汉密尔顿函数，并且提供了描述系统的常微分方程和偏微分方程描述。我们在六个不同的示例中验证了该方法的有效性，这些示例包括连续和离散系统。
</details></li>
</ul>
<hr>
<h2 id="Differentially-Private-Multi-Site-Treatment-Effect-Estimation"><a href="#Differentially-Private-Multi-Site-Treatment-Effect-Estimation" class="headerlink" title="Differentially Private Multi-Site Treatment Effect Estimation"></a>Differentially Private Multi-Site Treatment Effect Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06237">http://arxiv.org/abs/2310.06237</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tatsuki Koga, Kamalika Chaudhuri, David Page<br>for: This paper aims to provide a federated analytics approach for estimating the average treatment effect (ATE) in healthcare applications while ensuring differential privacy (DP) guarantees at each site.methods: The proposed method uses a class of per-site estimation algorithms that report the ATE estimate and its variance as a quality measure, and an aggregation algorithm on the server side that minimizes the overall variance of the final ATE estimate.results: The authors’ experiments on real and synthetic data show that their method reliably aggregates private statistics across sites and provides a better privacy-utility tradeoff under site heterogeneity than baselines.<details>
<summary>Abstract</summary>
Patient privacy is a major barrier to healthcare AI. For confidentiality reasons, most patient data remains in silo in separate hospitals, preventing the design of data-driven healthcare AI systems that need large volumes of patient data to make effective decisions. A solution to this is collective learning across multiple sites through federated learning with differential privacy. However, literature in this space typically focuses on differentially private statistical estimation and machine learning, which is different from the causal inference-related problems that arise in healthcare. In this work, we take a fresh look at federated learning with a focus on causal inference; specifically, we look at estimating the average treatment effect (ATE), an important task in causal inference for healthcare applications, and provide a federated analytics approach to enable ATE estimation across multiple sites along with differential privacy (DP) guarantees at each site. The main challenge comes from site heterogeneity -- different sites have different sample sizes and privacy budgets. We address this through a class of per-site estimation algorithms that reports the ATE estimate and its variance as a quality measure, and an aggregation algorithm on the server side that minimizes the overall variance of the final ATE estimate. Our experiments on real and synthetic data show that our method reliably aggregates private statistics across sites and provides better privacy-utility tradeoff under site heterogeneity than baselines.
</details>
<details>
<summary>摘要</summary>
�ynamic privacy is a major barrier to healthcare AI. For confidentiality reasons, most patient data remains in silos in separate hospitals, preventing the design of data-driven healthcare AI systems that need large volumes of patient data to make effective decisions. A solution to this is collective learning across multiple sites through federated learning with differential privacy. However, literature in this space typically focuses on differentially private statistical estimation and machine learning, which is different from the causal inference-related problems that arise in healthcare. In this work, we take a fresh look at federated learning with a focus on causal inference; specifically, we look at estimating the average treatment effect (ATE), an important task in causal inference for healthcare applications, and provide a federated analytics approach to enable ATE estimation across multiple sites along with differential privacy (DP) guarantees at each site. The main challenge comes from site heterogeneity -- different sites have different sample sizes and privacy budgets. We address this through a class of per-site estimation algorithms that reports the ATE estimate and its variance as a quality measure, and an aggregation algorithm on the server side that minimizes the overall variance of the final ATE estimate. Our experiments on real and synthetic data show that our method reliably aggregates private statistics across sites and provides better privacy-utility tradeoff under site heterogeneity than baselines.
</details></li>
</ul>
<hr>
<h2 id="Low-Rank-Tensor-Completion-via-Novel-Sparsity-Inducing-Regularizers"><a href="#Low-Rank-Tensor-Completion-via-Novel-Sparsity-Inducing-Regularizers" class="headerlink" title="Low-Rank Tensor Completion via Novel Sparsity-Inducing Regularizers"></a>Low-Rank Tensor Completion via Novel Sparsity-Inducing Regularizers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06233">http://arxiv.org/abs/2310.06233</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhi-Yong Wang, Hing Cheung So, Abdelhak M. Zoubir</li>
<li>for: 提高low-rank tensor completion问题中的稀疏性能。</li>
<li>methods: 使用非对称Surrogate&#x2F;正则化，并开发了基于替换方法的高效算法。</li>
<li>results: 实验结果表明，提案方法可以在实际数据上比前种方法更高的稀疏性能。<details>
<summary>Abstract</summary>
To alleviate the bias generated by the l1-norm in the low-rank tensor completion problem, nonconvex surrogates/regularizers have been suggested to replace the tensor nuclear norm, although both can achieve sparsity. However, the thresholding functions of these nonconvex regularizers may not have closed-form expressions and thus iterations are needed, which increases the computational loads. To solve this issue, we devise a framework to generate sparsity-inducing regularizers with closed-form thresholding functions. These regularizers are applied to low-tubal-rank tensor completion, and efficient algorithms based on the alternating direction method of multipliers are developed. Furthermore, convergence of our methods is analyzed and it is proved that the generated sequences are bounded and any limit point is a stationary point. Experimental results using synthetic and real-world datasets show that the proposed algorithms outperform the state-of-the-art methods in terms of restoration performance.
</details>
<details>
<summary>摘要</summary>
对于低矩阵完成问题中带来的偏调，非凸代替器/规律被建议来取代矩阵核心 нор，尽管它们都能够产生简洁性。然而，非凸规律的擦除函数可能无关closed-form表达，因此需要迭代运算，这会增加computational负担。为解决这个问题，我们设计了一个架构，可以生成具有关闭式擦除函数的简洁化规律。这些规律被应用到低管阵完成问题上，并开发了基于多重方向积分法的有效算法。此外，我们分析了我们的方法的收敛性，并证明其生成的序列是紧缩的，任何限点都是稳定点。实验结果显示，提出的方法在实验数据上比州前方法有更好的修复性。
</details></li>
</ul>
<hr>
<h2 id="Exploring-adversarial-attacks-in-federated-learning-for-medical-imaging"><a href="#Exploring-adversarial-attacks-in-federated-learning-for-medical-imaging" class="headerlink" title="Exploring adversarial attacks in federated learning for medical imaging"></a>Exploring adversarial attacks in federated learning for medical imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06227">http://arxiv.org/abs/2310.06227</a></li>
<li>repo_url: None</li>
<li>paper_authors: Erfan Darzi, Florian Dubost, N. M. Sijtsema, P. M. A van Ooijen</li>
<li>for: 评估 Federated Learning 网络在医疗影像分析中对抗攻击的漏洞。</li>
<li>methods: 使用域专业 MRI肿瘤和病理影像 Datasets 评估攻击者在 Federated Learning 环境中的攻击效果。</li>
<li>results: 测试发现，域专业配置可以使攻击者成功率明显增加。结论强调需要有效的防御机制，并建议现有安全协议在 Federated Medical Image Analysis 系统中进行重新评估。<details>
<summary>Abstract</summary>
Federated learning offers a privacy-preserving framework for medical image analysis but exposes the system to adversarial attacks. This paper aims to evaluate the vulnerabilities of federated learning networks in medical image analysis against such attacks. Employing domain-specific MRI tumor and pathology imaging datasets, we assess the effectiveness of known threat scenarios in a federated learning environment. Our tests reveal that domain-specific configurations can increase the attacker's success rate significantly. The findings emphasize the urgent need for effective defense mechanisms and suggest a critical re-evaluation of current security protocols in federated medical image analysis systems.
</details>
<details>
<summary>摘要</summary>
translate_chinese(    "Federated learning 提供了一个隐私保护的框架 для医疗影像分析，但暴露了系统于敌意攻击。这篇论文旨在评估 federated learning 网络在医疗影像分析中对这些攻击的抵御能力。使用具有域专属 MRI 肿瘤和病理图像 Datasets，我们评估了已知威胁enario在 Federated learning 环境中的效果。我们的测试发现，域专属配置可以提高攻击者的成功率，这些发现强调了现有安全协议的重要性，并建议进行重新评估。")以下是翻译结果：Federated learning 提供了一个隐私保护的框架 для医疗影像分析，但暴露了系统于敌意攻击。这篇论文旨在评估 federated learning 网络在医疗影像分析中对这些攻击的抵御能力。使用具有域专属 MRI 肿瘤和病理图像 Datasets，我们评估了已知威胁enario在 Federated learning 环境中的效果。我们的测试发现，域专属配置可以提高攻击者的成功率，这些发现强调了现有安全协议的重要性，并建议进行重新评估。
</details></li>
</ul>
<hr>
<h2 id="Detecting-and-Learning-Out-of-Distribution-Data-in-the-Open-world-Algorithm-and-Theory"><a href="#Detecting-and-Learning-Out-of-Distribution-Data-in-the-Open-world-Algorithm-and-Theory" class="headerlink" title="Detecting and Learning Out-of-Distribution Data in the Open world: Algorithm and Theory"></a>Detecting and Learning Out-of-Distribution Data in the Open world: Algorithm and Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06221">http://arxiv.org/abs/2310.06221</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiyou Sun</li>
<li>for: 本论文目的在于解决机器学习在开放世界enario中的应用问题，特别是面对 previously unknown data和contexts的情况。</li>
<li>methods: 本研究采用了两个步骤来解决开放世界机器学习的问题：out-of-distribution检测和开放世界表示学习（ORL）。</li>
<li>results: 本研究提出了一系列算法和理论基础，以便建立能够在开放世界中表现出色并且可靠的机器学习模型。<details>
<summary>Abstract</summary>
This thesis makes considerable contributions to the realm of machine learning, specifically in the context of open-world scenarios where systems face previously unseen data and contexts. Traditional machine learning models are usually trained and tested within a fixed and known set of classes, a condition known as the closed-world setting. While this assumption works in controlled environments, it falls short in real-world applications where new classes or categories of data can emerge dynamically and unexpectedly. To address this, our research investigates two intertwined steps essential for open-world machine learning: Out-of-distribution (OOD) Detection and Open-world Representation Learning (ORL). OOD detection focuses on identifying instances from unknown classes that fall outside the model's training distribution. This process reduces the risk of making overly confident, erroneous predictions about unfamiliar inputs. Moving beyond OOD detection, ORL extends the capabilities of the model to not only detect unknown instances but also learn from and incorporate knowledge about these new classes. By delving into these research problems of open-world learning, this thesis contributes both algorithmic solutions and theoretical foundations, which pave the way for building machine learning models that are not only performant but also reliable in the face of the evolving complexities of the real world.
</details>
<details>
<summary>摘要</summary>
OOD detection involves identifying instances from unknown classes that fall outside the model's training distribution. This process helps reduce the risk of making overly confident, erroneous predictions about unfamiliar inputs. In addition, ORL extends the capabilities of the model to not only detect unknown instances but also learn from and incorporate knowledge about these new classes. By tackling these research problems of open-world learning, this thesis provides both algorithmic solutions and theoretical foundations, laying the groundwork for building machine learning models that are not only high-performing but also reliable in the face of the evolving complexities of the real world.
</details></li>
</ul>
<hr>
<h2 id="Federated-Multi-Level-Optimization-over-Decentralized-Networks"><a href="#Federated-Multi-Level-Optimization-over-Decentralized-Networks" class="headerlink" title="Federated Multi-Level Optimization over Decentralized Networks"></a>Federated Multi-Level Optimization over Decentralized Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06217">http://arxiv.org/abs/2310.06217</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuoguang Yang, Xuezhou Zhang, Mengdi Wang</li>
<li>for: 这篇论文目的是研究分布式多层优化问题，即在网络上的 Agent 只能与当前邻居交流优化问题。</li>
<li>methods: 该论文提出了一种基于谈判的分布式多层优化算法，可以在单个时间尺度内解决多个层次优化问题，并通过网络传播来共享信息。</li>
<li>results: 该算法可以在不同应用场景中实现优秀表现，包括hyperparameter tuning、分布式 reinforcement learning 和风险谨慎优化。同时，该算法的样本复杂度为网络大小的直线性增长。<details>
<summary>Abstract</summary>
Multi-level optimization has gained increasing attention in recent years, as it provides a powerful framework for solving complex optimization problems that arise in many fields, such as meta-learning, multi-player games, reinforcement learning, and nested composition optimization. In this paper, we study the problem of distributed multi-level optimization over a network, where agents can only communicate with their immediate neighbors. This setting is motivated by the need for distributed optimization in large-scale systems, where centralized optimization may not be practical or feasible. To address this problem, we propose a novel gossip-based distributed multi-level optimization algorithm that enables networked agents to solve optimization problems at different levels in a single timescale and share information through network propagation. Our algorithm achieves optimal sample complexity, scaling linearly with the network size, and demonstrates state-of-the-art performance on various applications, including hyper-parameter tuning, decentralized reinforcement learning, and risk-averse optimization.
</details>
<details>
<summary>摘要</summary>
多层优化在最近几年内得到了越来越多的关注，因为它提供了一个强大的框架来解决许多领域中的复杂优化问题，如元学习、多 Player 游戏、回归学习和嵌套组合优化。在这篇论文中，我们研究了分布式多层优化问题，其中代理可以只与当前邻居进行交流。这种设定是由大规模系统中的分布式优化需求所驱动的，因为中央化优化可能不是实际或可行的。为解决这个问题，我们提出了一种基于吹拂的分布式多层优化算法，允许网络代理在不同层次上解决优化问题，并在单个时间尺度内共享信息。我们的算法实现了最佳样本复杂度，线性增长与网络大小相关，并在多个应用程序中达到了状态革命性的性能，包括超参调整、分布式回归学习和风险谨慎优化。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/10/cs.LG_2023_10_10/" data-id="clpahu76h00td3h88hi7v62fd" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_10_10" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/10/eess.IV_2023_10_10/" class="article-date">
  <time datetime="2023-10-10T09:00:00.000Z" itemprop="datePublished">2023-10-10</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/10/eess.IV_2023_10_10/">eess.IV - 2023-10-10</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Open-source-Pulseq-sequences-on-Philips-MRI-scanners"><a href="#Open-source-Pulseq-sequences-on-Philips-MRI-scanners" class="headerlink" title="Open-source Pulseq sequences on Philips MRI scanners"></a>Open-source Pulseq sequences on Philips MRI scanners</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06962">http://arxiv.org/abs/2310.06962</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas H. M. Roos, Edwin Versteeg, Dennis W. J. Klomp, Jeroen C. W. Siero, Jannie P. Wijnen</li>
<li>for: 这个论文的目的是为研究人员开发和分享新的MRI序列提供一个可用的开源平台。</li>
<li>methods: 这个论文使用了修改一些源代码文件来创建一个Pulseq解释器 для Philips MRI系统。验证实验使用了模拟和在7T Achieva MRI系统上进行的phantom扫描。</li>
<li>results: 通过Pulseq实现得到的重建图像与原始实现的图像相当，并且对MRI仪器的资源利用进行了评估，显示了一定的可扩展性。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Purpose: This work aims to address the limitations faced by researchers in developing and sharing new MRI sequences by implementing an interpreter for the open-source MRI pulse sequence format, Pulseq, on a Philips MRI scanner.   Methods: The implementation involved modifying a few source code files to create a Pulseq interpreter for the Philips MRI system. Validation experiments were conducted using simulations and phantom scans performed on a 7T Achieva MRI system. The observed sequence and waveforms were compared to the intended ones, and the gradient waveforms produced by the scanner were verified using a field camera. Image reconstruction was performed using the raw k-space samples acquired from both the native vendor environment and the Pulseq interpreter.   Results: The reconstructed images obtained through the Pulseq implementation were found to be comparable to those obtained through the native implementation. The performance of the Pulseq interpreter was assessed by profiling the CPU utilization of the MRI spectrometer, showing minimal resource utilization for certain sequences.   Conclusion: The successful implementation of the Pulseq interpreter on the Philips MRI scanner demonstrates the feasibility of utilizing Pulseq sequences on Philips MRI scanners. This provides an open-source platform for MRI sequence development, facilitating collaboration among researchers and accelerating scientific progress in the field of MRI.
</details>
<details>
<summary>摘要</summary>
目的：本研究旨在解决开源MRI序列格式（Pulseq）的开发和共享限制，通过在菲利浦MRI系统上实现Pulseq解释器。方法：实现过程包括修改一些源代码文件，以创建一个Pulseq解释器 для菲利浦MRI系统。验证实验使用了模拟和phantom扫描，在7T Achieva MRI系统上进行。观察的序列和波形与意图的相比，并使用场Camera验证了扫描产生的梯度波形。图像重建使用了原始的k空间样本，从Native vendor环境和Pulseq解释器中获取。结果：通过Pulseq实现的重建图像与Native实现的图像相似。解释器的性能评估通过CPU资源的使用量进行，表明某些序列的资源使用率很低。结论：成功实现Pulseq解释器在菲利浦MRI系统上，证明了使用Pulseq序列在菲利浦MRI系统上的可能性。这提供了一个开源平台 дляMRI序列开发，促进研究人员之间的合作，加速MRI领域科学进步。
</details></li>
</ul>
<hr>
<h2 id="Compression-Ratio-Learning-and-Semantic-Communications-for-Video-Imaging"><a href="#Compression-Ratio-Learning-and-Semantic-Communications-for-Video-Imaging" class="headerlink" title="Compression Ratio Learning and Semantic Communications for Video Imaging"></a>Compression Ratio Learning and Semantic Communications for Video Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06246">http://arxiv.org/abs/2310.06246</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bowen Zhang, Zhijin Qin, Geoffrey Ye Li</li>
<li>for: 提高智能 роботи系统中摄像头仪器的感应效率，以减少能源、内存等相关资源。</li>
<li>methods: 使用空间不断比对式压缩比率的影像压缩感应系统，实现高分辨率影像摄取。另外，运用数据传输方法来评估通信系统的性能，并将不同构成算法设计 для应用高动态范围影像摄取、影像压缩感应或动作降噪。</li>
<li>results: 通过使用政策Gradient方法来实现明确的压缩率与影像歪斜调整 trade-off，提高影像质量。numerical results show the superiority of the proposed methods over existing baselines.<details>
<summary>Abstract</summary>
Camera sensors have been widely used in intelligent robotic systems. Developing camera sensors with high sensing efficiency has always been important to reduce the power, memory, and other related resources. Inspired by recent success on programmable sensors and deep optic methods, we design a novel video compressed sensing system with spatially-variant compression ratios, which achieves higher imaging quality than the existing snapshot compressed imaging methods with the same sensing costs. In this article, we also investigate the data transmission methods for programmable sensors, where the performance of communication systems is evaluated by the reconstructed images or videos rather than the transmission of sensor data itself. Usually, different reconstruction algorithms are designed for applications in high dynamic range imaging, video compressive sensing, or motion debluring. This task-aware property inspires a semantic communication framework for programmable sensors. In this work, a policy-gradient based reinforcement learning method is introduced to achieve the explicit trade-off between the compression (or transmission) rate and the image distortion. Numerical results show the superiority of the proposed methods over existing baselines.
</details>
<details>
<summary>摘要</summary>
Camera 感测器在智能 роботи系统中广泛应用。开发高感知效率的 camera 感测器总是非常重要，以降低功能、存储和相关资源。受最近的可编程感测器和深度光学方法的成功启发，我们设计了一种新的视频压缩感测系统，其中具有空间变化的压缩比率，可以实现比现有的单张图像压缩成像方法更高的成像质量。在这篇文章中，我们还研究了用于可编程感测器的数据传输方法，并评估了通信系统的性能基于传输的感测器数据而不是直接传输感测器数据本身。通常，不同的重建算法采用了应用于高动态范围成像、压缩成像或运动锐化等应用场景。这种任务意识的性能 inspirits 一种Semantic Communication Framework for 可编程感测器。在这种工作中，我们引入了一种基于Policy Gradient的强化学习方法，以实现显式地考虑压缩率和图像扭曲之间的平衡。numerical 结果表明我们的方法在现有基线之上具有超越性。
</details></li>
</ul>
<hr>
<h2 id="Domain-Expansion-via-Network-Adaptation-for-Solving-Inverse-Problems"><a href="#Domain-Expansion-via-Network-Adaptation-for-Solving-Inverse-Problems" class="headerlink" title="Domain Expansion via Network Adaptation for Solving Inverse Problems"></a>Domain Expansion via Network Adaptation for Solving Inverse Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06235">http://arxiv.org/abs/2310.06235</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nebiyou Yismaw, Ulugbek S. Kamilov, M. Salman Asif<br>for:* 这篇论文主要针对的是解决计算成像中的反向问题，提出了一种基于深度学习的方法。methods:* 这种方法可以分为两个类别：一是学习网络将测量转换为信号估计，这种方法容易受到数据分布的变化的影响；二是学习信号的假设，然后使用优化方法来回收信号。results:* 这篇论文通过研究不同类型的域转换的影响，并提出了一种可以灵活地适应不同域的框架，使得已经训练过的网络可以更好地适应不同的数据分布。这种方法在自然图像、MRI和CT重建任务中获得了显著更好的性能和参数效率。<details>
<summary>Abstract</summary>
Deep learning-based methods deliver state-of-the-art performance for solving inverse problems that arise in computational imaging. These methods can be broadly divided into two groups: (1) learn a network to map measurements to the signal estimate, which is known to be fragile; (2) learn a prior for the signal to use in an optimization-based recovery. Despite the impressive results from the latter approach, many of these methods also lack robustness to shifts in data distribution, measurements, and noise levels. Such domain shifts result in a performance gap and in some cases introduce undesired artifacts in the estimated signal. In this paper, we explore the qualitative and quantitative effects of various domain shifts and propose a flexible and parameter efficient framework that adapt pretrained networks to such shifts. We demonstrate the effectiveness of our method for a number of natural image, MRI, and CT reconstructions tasks under domain, measurement model, and noise-level shifts. Our experiments demonstrate that our method provides significantly better performance and parameter efficiency compared to existing domain adaptation techniques.
</details>
<details>
<summary>摘要</summary>
深度学习基本方法可以解决计算成像中的逆问题，这些方法可以分为两个组：（1）学习网络来将测量转换为信号估计，这是知道脆弱的；（2）学习信号的先验来在优化基础上进行恢复。尽管后者的方法也具有很好的成果，但是许多这些方法缺乏对数据分布、测量、和噪声水平的鲁棒性，这会导致性能差异和在某些情况下添加不必要的artefacts到估计的信号中。在这篇论文中，我们研究了不同类型的领域变化的影响，并提出了一种灵活和参数高效的框架，可以适应这些变化。我们通过多个自然图像、MRI和CT重建任务的实验，证明了我们的方法可以在不同的领域、测量模型和噪声水平下提供了显著更好的性能和参数高效性，相比之下现有的领域适应技术。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/10/eess.IV_2023_10_10/" data-id="clpahu7e101bz3h88axfb7gau" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/30/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/29/">29</a><a class="page-number" href="/page/30/">30</a><span class="page-number current">31</span><a class="page-number" href="/page/32/">32</a><a class="page-number" href="/page/33/">33</a><span class="space">&hellip;</span><a class="page-number" href="/page/98/">98</a><a class="extend next" rel="next" href="/page/32/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">67</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">82</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">147</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
