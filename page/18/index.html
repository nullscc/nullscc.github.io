
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/18/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-eess.AS_2023_07_28" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/28/eess.AS_2023_07_28/" class="article-date">
  <time datetime="2023-07-27T16:00:00.000Z" itemprop="datePublished">2023-07-28</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/28/eess.AS_2023_07_28/">eess.AS - 2023-07-28 22:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Efficient-Acoustic-Echo-Suppression-with-Condition-Aware-Training"><a href="#Efficient-Acoustic-Echo-Suppression-with-Condition-Aware-Training" class="headerlink" title="Efficient Acoustic Echo Suppression with Condition-Aware Training"></a>Efficient Acoustic Echo Suppression with Condition-Aware Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15630">http://arxiv.org/abs/2307.15630</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ernst Seidel, Pejman Mowlaee, Tim Fingscheidt</li>
<li>for: 这篇论文的目的是提出一个改进的卷积回传神经网络（CRN）架构，以提高双话（DT）情况下的深度声音控制性。</li>
<li>methods: 这篇论文使用了一种具有卷积Encoder和解oder，并具有回传瓶颈的CRN架构，并且这种架构比前一代的模型更加简单，具有更好的性能。</li>
<li>results: 根据实验结果显示，这种改进的CRN架构不仅比前一代的FCRN和CRUSE模型更加简单，而且在DT情况下也有更好的性能。<details>
<summary>Abstract</summary>
The topic of deep acoustic echo control (DAEC) has seen many approaches with various model topologies in recent years. Convolutional recurrent networks (CRNs), consisting of a convolutional encoder and decoder encompassing a recurrent bottleneck, are repeatedly employed due to their ability to preserve nearend speech even in double-talk (DT) condition. However, past architectures are either computationally complex or trade off smaller model sizes with a decrease in performance. We propose an improved CRN topology which, compared to other realizations of this class of architectures, not only saves parameters and computational complexity, but also shows improved performance in DT, outperforming both baseline architectures FCRN and CRUSE. Striving for a condition-aware training, we also demonstrate the importance of a high proportion of double-talk and the missing value of nearend-only speech in DAEC training data. Finally, we show how to control the trade-off between aggressive echo suppression and near-end speech preservation by fine-tuning with condition-aware component loss functions.
</details>
<details>
<summary>摘要</summary>
“深层听频延迟控制（DAEC）在过去几年中有很多方法和不同的模型结构。卷积回归网络（CRN）因其能保持靠近的speech在双语（DT）条件下，被广泛使用。然而，过去的建筑方案都是计算复杂或是减少模型大小的代价是性能下降。我们提出了改进的CRN顶点，与其他类型的建筑方案相比，不仅减少参数和计算复杂度，还表现出DT条件下的提高性能，超过了基eline馈料FCRN和CRUSE。我们还证明了在DAEC训练数据中高占比的双语和缺失的靠近只speech的重要性。最后，我们示出了控制听频延迟和靠近speech保持的质量的权衡，通过condition-aware组件损失函数的微调。”Note: Please note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="A-Time-Frequency-Generative-Adversarial-based-method-for-Audio-Packet-Loss-Concealment"><a href="#A-Time-Frequency-Generative-Adversarial-based-method-for-Audio-Packet-Loss-Concealment" class="headerlink" title="A Time-Frequency Generative Adversarial based method for Audio Packet Loss Concealment"></a>A Time-Frequency Generative Adversarial based method for Audio Packet Loss Concealment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15611">http://arxiv.org/abs/2307.15611</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aircarlo/bin2bin-gan-plc">https://github.com/aircarlo/bin2bin-gan-plc</a></li>
<li>paper_authors: Carlo Aironi, Samuele Cornell, Luca Serafini, Stefano Squartini</li>
<li>for: 这篇论文旨在提高VoIP传输中 packet loss 的影响，使用生成对抗法修复丢失的封包。</li>
<li>methods: 本论文使用生成对抗网络（GAN）的改进版本 pix2pix 框架，将丢失的封包转换为非损失的语音spectrogram。</li>
<li>results: 实验结果显示，提议的方法在高 packet loss 率和大差距情况下表现出显著优势，能更好地保持语音结构信息。<details>
<summary>Abstract</summary>
Packet loss is a major cause of voice quality degradation in VoIP transmissions with serious impact on intelligibility and user experience. This paper describes a system based on a generative adversarial approach, which aims to repair the lost fragments during the transmission of audio streams. Inspired by the powerful image-to-image translation capability of Generative Adversarial Networks (GANs), we propose bin2bin, an improved pix2pix framework to achieve the translation task from magnitude spectrograms of audio frames with lost packets, to noncorrupted speech spectrograms. In order to better maintain the structural information after spectrogram translation, this paper introduces the combination of two STFT-based loss functions, mixed with the traditional GAN objective. Furthermore, we employ a modified PatchGAN structure as discriminator and we lower the concealment time by a proper initialization of the phase reconstruction algorithm. Experimental results show that the proposed method has obvious advantages when compared with the current state-of-the-art methods, as it can better handle both high packet loss rates and large gaps.
</details>
<details>
<summary>摘要</summary>
packet loss 是 VoIP 传输中的一个主要 causative factor，它会对语音质量产生严重的影响，从而影响用户体验。本文描述了一种基于生成敌对推议的系统，该系统可以在audio流传输过程中重建丢失的封包。引用了生成敌对网络（GANs）的强大图像到图像翻译能力，我们提出了bin2bin，一种改进的 pix2pix 框架，用于从损坏的音频帧矩阵中翻译到不损坏的语音矩阵。为了更好地保持音频翻译后的结构信息，本文引入了两种 STFT 基于的损失函数的组合，混合了传统的 GAN 目标函数。此外，我们使用了修改后的 PatchGAN 结构来担任推议器，并通过适当的初始化方式下降掩蔽时间。实验结果表明，提出的方法在与当前状态艺术方法相比，具有明显的优势，能够更好地处理高 packet loss 率和大的差距。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/28/eess.AS_2023_07_28/" data-id="clmjn91pa00dh0j880bacfixx" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_07_28" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/28/eess.IV_2023_07_28/" class="article-date">
  <time datetime="2023-07-27T16:00:00.000Z" itemprop="datePublished">2023-07-28</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/28/eess.IV_2023_07_28/">eess.IV - 2023-07-28 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="A-Survey-on-Deep-Learning-in-Medical-Image-Registration-New-Technologies-Uncertainty-Evaluation-Metrics-and-Beyond"><a href="#A-Survey-on-Deep-Learning-in-Medical-Image-Registration-New-Technologies-Uncertainty-Evaluation-Metrics-and-Beyond" class="headerlink" title="A Survey on Deep Learning in Medical Image Registration: New Technologies, Uncertainty, Evaluation Metrics, and Beyond"></a>A Survey on Deep Learning in Medical Image Registration: New Technologies, Uncertainty, Evaluation Metrics, and Beyond</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15615">http://arxiv.org/abs/2307.15615</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junyu Chen, Yihao Liu, Shuwen Wei, Zhangxing Bian, Shalini Subramanian, Aaron Carass, Jerry L. Prince, Yong Du</li>
<li>for: 本研究团队提出了一种基于深度学习的医学图像注册方法，以推动医学图像注册领域的发展。</li>
<li>methods: 本研究使用了多种深度学习网络，包括ResNet和U-Net，以及不同的相似度度量和减杂化正则化。</li>
<li>results: 本研究提出了一种全面的深度学习基于图像注册方法，包括网络架构、损失函数和注册不确定性估计。此外，本研究还提出了用于评估深度学习模型在注册任务中的评价指标。<details>
<summary>Abstract</summary>
Over the past decade, deep learning technologies have greatly advanced the field of medical image registration. The initial developments, such as ResNet-based and U-Net-based networks, laid the groundwork for deep learning-driven image registration. Subsequent progress has been made in various aspects of deep learning-based registration, including similarity measures, deformation regularizations, and uncertainty estimation. These advancements have not only enriched the field of deformable image registration but have also facilitated its application in a wide range of tasks, including atlas construction, multi-atlas segmentation, motion estimation, and 2D-3D registration. In this paper, we present a comprehensive overview of the most recent advancements in deep learning-based image registration. We begin with a concise introduction to the core concepts of deep learning-based image registration. Then, we delve into innovative network architectures, loss functions specific to registration, and methods for estimating registration uncertainty. Additionally, this paper explores appropriate evaluation metrics for assessing the performance of deep learning models in registration tasks. Finally, we highlight the practical applications of these novel techniques in medical imaging and discuss the future prospects of deep learning-based image registration.
</details>
<details>
<summary>摘要</summary>
We begin with a concise introduction to the core concepts of deep learning-based image registration. Then, we delve into innovative network architectures, loss functions specific to registration, and methods for estimating registration uncertainty. Additionally, this paper explores appropriate evaluation metrics for assessing the performance of deep learning models in registration tasks. Finally, we highlight the practical applications of these novel techniques in medical imaging and discuss the future prospects of deep learning-based image registration.Translated into Simplified Chinese:过去十年，深度学习技术在医疗影像registrations中取得了大量进步。初期发展，如ResNet基于和U-Net基于的网络，为深度学习驱动的影像registrations奠定了基础。后续的进步包括相似度度量、形态规范和注意力估计等方面，这些进步不仅涌现了弹性影像registrations的领域，还为各种任务，如建立Atlas、多Atlas分割、运动估计和2D-3Dregistrations提供了应用。在本文中，我们提供了最新的深度学习基于影像registrations的概括。我们从核心概念的介绍开始，然后探讨了新的网络架构、特定于registrations的损失函数和注意力估计方法。此外，这篇论文还探讨了评价深度学习模型在registrations任务中表现的适当评价指标。最后，我们强调了这些新技术在医疗影像中的实际应用和未来前景。
</details></li>
</ul>
<hr>
<h2 id="Integrated-Digital-Reconstruction-of-Welded-Components-Supporting-Improved-Fatigue-Life-Prediction"><a href="#Integrated-Digital-Reconstruction-of-Welded-Components-Supporting-Improved-Fatigue-Life-Prediction" class="headerlink" title="Integrated Digital Reconstruction of Welded Components: Supporting Improved Fatigue Life Prediction"></a>Integrated Digital Reconstruction of Welded Components: Supporting Improved Fatigue Life Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15604">http://arxiv.org/abs/2307.15604</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anders Faarbæk Mikkelstrup, Morten Kristiansen</li>
<li>for: 提高锚固结构的质量和安全性</li>
<li>methods: 使用自动化高频机械冲击（HFMI）处理、图像处理、简单滤波技术和非线性优化算法对拼接部位进行数字重建</li>
<li>results: 实现了 generic、cost-effective、flexible 和 rapid 的数字重建方法，帮助提高锚固结构的设计、质量监控和HFMI处理记录<details>
<summary>Abstract</summary>
In the design of offshore jacket foundations, fatigue life is crucial. Post-weld treatment has been proposed to enhance the fatigue performance of welded joints, where particularly high-frequency mechanical impact (HFMI) treatment has been shown to improve fatigue performance significantly. Automated HFMI treatment has improved quality assurance and can lead to cost-effective design when combined with accurate fatigue life prediction. However, the finite element method (FEM), commonly used for predicting fatigue life in complex or multi-axial joints, relies on a basic CAD depiction of the weld, failing to consider the actual weld geometry and defects. Including the actual weld geometry in the FE model improves fatigue life prediction and possible crack location prediction but requires a digital reconstruction of the weld. Current digital reconstruction methods are time-consuming or require specialised scanning equipment and potential component relocation. The proposed framework instead uses an industrial manipulator combined with a line scanner to integrate digital reconstruction as part of the automated HFMI treatment setup. This approach applies standard image processing, simple filtering techniques, and non-linear optimisation for aligning and merging overlapping scans. A screened Poisson surface reconstruction finalises the 3D model to create a meshed surface. The outcome is a generic, cost-effective, flexible, and rapid method that enables generic digital reconstruction of welded parts, aiding in component design, overall quality assurance, and documentation of the HFMI treatment.
</details>
<details>
<summary>摘要</summary>
在海上钻井基础设计中，腐蚀性 жизни是关键。Post-weld处理被提议来提高焊接缝合的腐蚀性能，特别是高频机械冲击（HFMI）处理，可以大幅提高腐蚀性能。自动化HFMI处理可以提高质量保证和降低设计成本，当与准确的腐蚀生命预测结合使用时。然而，通用finite element方法（FEM），常用于预测复杂或多轴缝合的腐蚀生命，基于简化的CAD描述，忽略了实际焊接形状和缺陷。包含实际焊接形状在FEM模型中可以提高腐蚀生命预测和可能的裂化位置预测，但需要数字重建焊接。当前的数字重建方法需要大量时间或特殊的扫描设备，以及可能的组件重新位置。我们提出的框架则使用工业护手机和线扫描器结合数字重建，并通过标准的图像处理、简单的滤波技术和非线性优化来对 overlap 扫描进行对接和合并。屏幕Poisson面重建最终生成3D模型，生成一个通用、成本效果、灵活、快速的方法，以便在焊接部件的设计、总质量保证和HFMI处理的文档中进行数字重建。
</details></li>
</ul>
<hr>
<h2 id="OAFuser-Towards-Omni-Aperture-Fusion-for-Light-Field-Semantic-Segmentation-of-Road-Scenes"><a href="#OAFuser-Towards-Omni-Aperture-Fusion-for-Light-Field-Semantic-Segmentation-of-Road-Scenes" class="headerlink" title="OAFuser: Towards Omni-Aperture Fusion for Light Field Semantic Segmentation of Road Scenes"></a>OAFuser: Towards Omni-Aperture Fusion for Light Field Semantic Segmentation of Road Scenes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15588">http://arxiv.org/abs/2307.15588</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/feibryantkit/oafuser">https://github.com/feibryantkit/oafuser</a></li>
<li>paper_authors: Fei Teng, Jiaming Zhang, Kunyu Peng, Kailun Yang, Yaonan Wang, Rainer Stiefelhagen</li>
<li>for: 增强自动驾驶场景理解的图像Semantic Segmentation，使用光场相机提供了丰富的angular和空间信息。</li>
<li>methods: 提议Omni-Aperture Fusion模型（OAFuser），利用中心视图的denseContext和sub-aperture图像中的angular信息来生成semantically-consistent的结果，同时采用Sub-Aperture Fusion Module（SAFM）将sub-aperture图像嵌入angular特征中，不需要额外内存成本。</li>
<li>results: 在UrbanLF-Real和-Syn数据集上实现了state-of-the-art性能，在UrbanLF-Real Extended数据集上达到了84.93%的mIoU记录，比前一个最佳记录提高+4.53%。<details>
<summary>Abstract</summary>
Light field cameras can provide rich angular and spatial information to enhance image semantic segmentation for scene understanding in the field of autonomous driving. However, the extensive angular information of light field cameras contains a large amount of redundant data, which is overwhelming for the limited hardware resource of intelligent vehicles. Besides, inappropriate compression leads to information corruption and data loss. To excavate representative information, we propose an Omni-Aperture Fusion model (OAFuser), which leverages dense context from the central view and discovers the angular information from sub-aperture images to generate a semantically-consistent result. To avoid feature loss during network propagation and simultaneously streamline the redundant information from the light field camera, we present a simple yet very effective Sub-Aperture Fusion Module (SAFM) to embed sub-aperture images into angular features without any additional memory cost. Furthermore, to address the mismatched spatial information across viewpoints, we present Center Angular Rectification Module (CARM) realized feature resorting and prevent feature occlusion caused by asymmetric information. Our proposed OAFuser achieves state-of-the-art performance on the UrbanLF-Real and -Syn datasets and sets a new record of 84.93% in mIoU on the UrbanLF-Real Extended dataset, with a gain of +4.53%. The source code of OAFuser will be made publicly available at https://github.com/FeiBryantkit/OAFuser.
</details>
<details>
<summary>摘要</summary>
光场相机可以提供rich的ANGLE和空间信息，以增强图像Semantic Segmentation，以提高自动驾驶场景理解。然而，光场相机的广泛ANGLE信息包含大量冗余数据，对智能车辆的硬件资源造成拥堵。此外，不当压缩可能会导致信息损害和数据丢失。为了提取代表性信息，我们提议了一种Omni-Aperture Fusion模型（OAFuser），它利用中心视图的dense上下文和子视图图像ANGLE信息来生成具有相同semantic consistency的结果。为了避免网络传播过程中的特征损失和同时压缩光场相机中的冗余信息，我们提出了一种简单 yet powerful的Sub-Aperture Fusion模块（SAFM），可以将子视图图像与ANGLE特征进行嵌入，无需额外内存成本。此外，为了解决不同视角之间的匹配问题，我们提出了Center Angular Rectification Module（CARM），通过实现特征重定向和避免特征堵塞，解决了由不同视角所导致的匹配问题。我们的OAFuser模型在UrbanLF-Real和UrbanLF-Syn数据集上达到了状态机器人的性能记录，具有84.93%的mIoU在UrbanLF-Real Extended数据集上，与前一个记录之间的增幅为+4.53%。OAFuser模型的源代码将于https://github.com/FeiBryantkit/OAFuser上公开。
</details></li>
</ul>
<hr>
<h2 id="Defocus-Blur-Synthesis-and-Deblurring-via-Interpolation-and-Extrapolation-in-Latent-Space"><a href="#Defocus-Blur-Synthesis-and-Deblurring-via-Interpolation-and-Extrapolation-in-Latent-Space" class="headerlink" title="Defocus Blur Synthesis and Deblurring via Interpolation and Extrapolation in Latent Space"></a>Defocus Blur Synthesis and Deblurring via Interpolation and Extrapolation in Latent Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15461">http://arxiv.org/abs/2307.15461</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nis-research/linear-latent-blur">https://github.com/nis-research/linear-latent-blur</a></li>
<li>paper_authors: Ioana Mazilu, Shunxin Wang, Sven Dummer, Raymond Veldhuis, Christoph Brune, Nicola Strisciuglio</li>
<li>For: 这项研究旨在提高微scopic图像质量，以便更好地进行医疗诊断和疾病分析。* Methods: 该项目使用自适应柱面网络和正则化技术，以实现对图像的杂谱和解析。* Results: 研究人员通过训练自适应网络并应用正则化技术，实现了对图像的杂谱和解析，提高了微scopic图像的质量，并可以用作数据增强技术。<details>
<summary>Abstract</summary>
Though modern microscopes have an autofocusing system to ensure optimal focus, out-of-focus images can still occur when cells within the medium are not all in the same focal plane, affecting the image quality for medical diagnosis and analysis of diseases. We propose a method that can deblur images as well as synthesize defocus blur. We train autoencoders with implicit and explicit regularization techniques to enforce linearity relations among the representations of different blur levels in the latent space. This allows for the exploration of different blur levels of an object by linearly interpolating/extrapolating the latent representations of images taken at different focal planes. Compared to existing works, we use a simple architecture to synthesize images with flexible blur levels, leveraging the linear latent space. Our regularized autoencoders can effectively mimic blur and deblur, increasing data variety as a data augmentation technique and improving the quality of microscopic images, which would be beneficial for further processing and analysis.
</details>
<details>
<summary>摘要</summary>
modern microscopes 已经有自动对焦系统，但是仍然可能出现不清晰的图像，因为细胞在媒体中不 все在同一个 фокус平面上，这会影响医疗诊断和疾病分析的图像质量。我们提出了一种方法，可以对图像进行恢复和模拟杂化。我们使用隐式和显式正则化技术来规范各个混杂水平的表示在隐藏空间中的线性关系。这allow for随着不同 фокус平面上图像的latent representation的线性 interpolate/extrapolate，以探索不同混杂水平的对象图像。相比之下，我们使用简单的建筑来 sintesize图像，并且可以在隐藏空间中Linearly interpolate/extrapolate不同混杂水平，从而增加数据的多样性，并提高微scopic图像的质量，这将对进一步处理和分析产生有利影响。
</details></li>
</ul>
<hr>
<h2 id="ERCPMP-An-Endoscopic-Image-and-Video-Dataset-for-Colorectal-Polyps-Morphology-and-Pathology"><a href="#ERCPMP-An-Endoscopic-Image-and-Video-Dataset-for-Colorectal-Polyps-Morphology-and-Pathology" class="headerlink" title="ERCPMP: An Endoscopic Image and Video Dataset for Colorectal Polyps Morphology and Pathology"></a>ERCPMP: An Endoscopic Image and Video Dataset for Colorectal Polyps Morphology and Pathology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15444">http://arxiv.org/abs/2307.15444</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mojgan Forootan, Mohsen Rajabnia, Ahmad R Mafi, Hamed Azhdari Tehrani, Erfan Ghadirzadeh, Mahziar Setayeshfar, Zahra Ghaffari, Mohammad Tashakoripour, Mohammad Reza Zali, Hamidreza Bolhasani</li>
<li>for: This paper is written for the purpose of developing accurate algorithms for medical prediction, detection, diagnosis, treatment, and prognosis, specifically for colorectal polyps.</li>
<li>methods: The paper uses a dataset called ERCPMP, which contains demographic, morphological, and pathological data, endoscopic images, and videos of 191 patients with colorectal polyps. The dataset includes information based on the latest international gastroenterology classification references, such as Paris, Pit, and JNET classification.</li>
<li>results: The paper provides a dataset that can be used for the development and evaluation of algorithms for the recognition of colorectal polyps morphology and pathology. The dataset is available on Elsevier Mendeley Dataverse and is currently under development, with the latest version accessible via a specific website.Here is the information in Simplified Chinese text:</li>
<li>for: 这篇论文是为了开发准确的医疗预测、检测、诊断、治疗和预后评估算法，特别是为了Rectal Polyps。</li>
<li>methods: 这篇论文使用了名为ERCPMP的数据集，该数据集包含了191名患有Rectal Polyps的病人的民生、形态和病理数据，以及这些病人的endoscopic图像和视频。数据集包括根据最新的国际肠胃病学分类标准，如Paris、Pit和JNET分类标准。</li>
<li>results: 这篇论文提供了一个用于开发和评估Recognize Colorectal Polyps Morphology and Pathology的算法的数据集。数据集可以在Elsevier Mendeley Dataverse上获取，并且目前正在开发中，最新的版本可以通过特定的网站获取。<details>
<summary>Abstract</summary>
In the recent years, artificial intelligence (AI) and its leading subtypes, machine learning (ML) and deep learning (DL) and their applications are spreading very fast in various aspects such as medicine. Today the most important challenge of developing accurate algorithms for medical prediction, detection, diagnosis, treatment and prognosis is data. ERCPMP is an Endoscopic Image and Video Dataset for Recognition of Colorectal Polyps Morphology and Pathology. This dataset contains demographic, morphological and pathological data, endoscopic images and videos of 191 patients with colorectal polyps. Morphological data is included based on the latest international gastroenterology classification references such as Paris, Pit and JNET classification. Pathological data includes the diagnosis of the polyps including Tubular, Villous, Tubulovillous, Hyperplastic, Serrated, Inflammatory and Adenocarcinoma with Dysplasia Grade & Differentiation. The current version of this dataset is published and available on Elsevier Mendeley Dataverse and since it is under development, the latest version is accessible via: https://databiox.com.
</details>
<details>
<summary>摘要</summary>
在最近的几年中，人工智能（AI）和其主要分支——机器学习（ML）和深度学习（DL）在各种领域的应用速度很快，如医学。今天最重要的挑战是开发准确的医疗算法，包括预测、检测、诊断、治疗和预后。ERCPMP是一个endorscopic Image和视频Dataset，用于识别肠RECTALPolyp的形态和病理学特征。该数据集包括191名患者的肠RECTALPolyp的民生、形态和病理学数据，以及endooscopic图像和视频。形态数据按照最新的国际肠胃科分类标准进行编码，包括Paris、Pit和JNET分类。病理数据包括肠RECTALPolyp的诊断，包括管肠、芽孢、管芽孢、高级瘤、炎性、卵极和adenocarcinoma，以及分化度和分化度。当前版本的数据集已经发布，可以在Elsevier Mendeley Dataverse上获取，而最新版本可以通过以下链接获取：https://databiox.com。
</details></li>
</ul>
<hr>
<h2 id="RAWIW-RAW-Image-Watermarking-Robust-to-ISP-Pipeline"><a href="#RAWIW-RAW-Image-Watermarking-Robust-to-ISP-Pipeline" class="headerlink" title="RAWIW: RAW Image Watermarking Robust to ISP Pipeline"></a>RAWIW: RAW Image Watermarking Robust to ISP Pipeline</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15443">http://arxiv.org/abs/2307.15443</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kang Fu, Xiaohong Liu, Jun Jia, Zicheng Zhang, Yicong Peng, Jia Wang, Guangtao Zhai</li>
<li>for: 这篇论文的目的是为了提供一种基于深度学习的 RAW 图像水印保护方法，以保护 RAW 图像的版权。</li>
<li>methods: 该方法使用了深度学习网络来实现 RAW 图像水印保护，并将 copyright 信息直接嵌入 RAW 图像中，以便在不同的 post-processing 方法后可以提取 copyright 信息。</li>
<li>results: 该方法在实验中成功地实现了跨频道的版权保护，并且可以在不同的传输损害 circumstance 下保持图像质量和水印的可靠性。<details>
<summary>Abstract</summary>
Invisible image watermarking is essential for image copyright protection. Compared to RGB images, RAW format images use a higher dynamic range to capture the radiometric characteristics of the camera sensor, providing greater flexibility in post-processing and retouching. Similar to the master recording in the music industry, RAW images are considered the original format for distribution and image production, thus requiring copyright protection. Existing watermarking methods typically target RGB images, leaving a gap for RAW images. To address this issue, we propose the first deep learning-based RAW Image Watermarking (RAWIW) framework for copyright protection. Unlike RGB image watermarking, our method achieves cross-domain copyright protection. We directly embed copyright information into RAW images, which can be later extracted from the corresponding RGB images generated by different post-processing methods. To achieve end-to-end training of the framework, we integrate a neural network that simulates the ISP pipeline to handle the RAW-to-RGB conversion process. To further validate the generalization of our framework to traditional ISP pipelines and its robustness to transmission distortion, we adopt a distortion network. This network simulates various types of noises introduced during the traditional ISP pipeline and transmission. Furthermore, we employ a three-stage training strategy to strike a balance between robustness and concealment of watermarking. Our extensive experiments demonstrate that RAWIW successfully achieves cross-domain copyright protection for RAW images while maintaining their visual quality and robustness to ISP pipeline distortions.
</details>
<details>
<summary>摘要</summary>
隐形图像水印是图像版权保护的关键技术。相比RGB图像，RAW格式图像利用更高的动态范围来捕捉相机传感器的 радиметрические特征，提供更大的后处理和修剪灵活性。类似于音乐行业中的母带录制，RAW图像被视为原始格式，需要版权保护。现有的水印方法通常针对RGB图像，留下了RAW图像的空白。为了解决这个问题，我们提出了首个基于深度学习的RAW图像水印（RAWIW）框架，用于版权保护。与RGB图像水印不同，我们的方法实现了跨频道的版权保护。我们直接嵌入版权信息到RAW图像中，可以在不同的后处理方法生成的RGB图像中提取。为实现整个框架的端到端训练，我们将神经网络与ISP管道相互关联，以处理RAW图像到RGB图像的转换过程。此外，我们采用了一个扰动网络，以模拟传输过程中引入的各种噪声。此外，我们采用了三stage训练策略，以保持水印的鲁棒性和隐藏性。我们的广泛实验表明，RAWIW成功实现了RAW图像的跨频道版权保护，保持了图像的视觉质量和传输过程中的鲁棒性。
</details></li>
</ul>
<hr>
<h2 id="MLIC-Linear-Complexity-Multi-Reference-Entropy-Modeling-for-Learned-Image-Compression"><a href="#MLIC-Linear-Complexity-Multi-Reference-Entropy-Modeling-for-Learned-Image-Compression" class="headerlink" title="MLIC++: Linear Complexity Multi-Reference Entropy Modeling for Learned Image Compression"></a>MLIC++: Linear Complexity Multi-Reference Entropy Modeling for Learned Image Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15421">http://arxiv.org/abs/2307.15421</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jiangweibeta/mlic">https://github.com/jiangweibeta/mlic</a></li>
<li>paper_authors: Wei Jiang, Ronggang Wang</li>
<li>for: 这篇论文是为了提出一种基于多参考 entropy 模型的学习图像压缩方法，以提高图像压缩的效率和质量。</li>
<li>methods: 该方法使用 linear complexity global correlations capturing，通过软max操作的减法分解，以取代之前的 attention 方法。</li>
<li>results: 相比 VTM-17.0，该方法可以在 PSNR 指标下减少 BD-rate 12.44%，并且更高效。Code 将在 GitHub 上提供。<details>
<summary>Abstract</summary>
Recently, multi-reference entropy model has been proposed, which captures channel-wise, local spatial, and global spatial correlations. Previous works adopt attention for global correlation capturing, however, the quadratic cpmplexity limits the potential of high-resolution image coding. In this paper, we propose the linear complexity global correlations capturing, via the decomposition of softmax operation. Based on it, we propose the MLIC$^{++}$, a learned image compression with linear complexity for multi-reference entropy modeling. Our MLIC$^{++}$ is more efficient and it reduces BD-rate by 12.44% on the Kodak dataset compared to VTM-17.0 when measured in PSNR. Code will be available at https://github.com/JiangWeibeta/MLIC.
</details>
<details>
<summary>摘要</summary>
最近，多参照 entropy 模型已经被提出，这个模型可以捕捉通道级、本地空间和全局空间相关性。先前的工作采用了注意力来捕捉全局相关性，但是quadratic 复杂度限制了高分辨率图像编码的潜力。在这篇论文中，我们提议使用线性复杂度全球相关性捕捉，通过软max操作的分解。基于这，我们提议了MLIC++，一种学习图像压缩的线性复杂度全球相关性模型。我们的 MLIC++ 比VTM-17.0在Kodak数据集上BD-rate下降12.44%，相对PSNR测量下降。代码将在 GitHub 上发布，地址为https://github.com/JiangWeibeta/MLIC。
</details></li>
</ul>
<hr>
<h2 id="Fast-Dust-Sand-Image-Enhancement-Based-on-Color-Correction-and-New-Membership-Function"><a href="#Fast-Dust-Sand-Image-Enhancement-Based-on-Color-Correction-and-New-Membership-Function" class="headerlink" title="Fast Dust Sand Image Enhancement Based on Color Correction and New Membership Function"></a>Fast Dust Sand Image Enhancement Based on Color Correction and New Membership Function</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15230">http://arxiv.org/abs/2307.15230</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Hakem Alsaeedi, Suha Mohammed Hadi, Yarub Alazzawi</li>
<li>for: 提高灰尘环境中图像质量和可见度</li>
<li>methods: 基于颜色修正和新成员函数，提出了一种新的图像增强模型，包括三个阶段：色差修正、雾气除除和对比和亮度提高</li>
<li>results: 通过对多个真实的灰尘图像进行测试和评估，研究发现该解决方案比现有研究更有效地除去红色和黄色投影，提供高质量和数量的灰尘图像<details>
<summary>Abstract</summary>
Images captured in dusty environments suffering from poor visibility and quality. Enhancement of these images such as sand dust images plays a critical role in various atmospheric optics applications. In this work, proposed a new model based on Color Correction and new membership function to enhance san dust images. The proposed model consists of three phases: correction of color shift, removal of haze, and enhancement of contrast and brightness. The color shift is corrected using a new membership function to adjust the values of U and V in the YUV color space. The Adaptive Dark Channel Prior (A-DCP) is used for haze removal. The stretching contrast and improving image brightness are based on Contrast Limited Adaptive Histogram Equalization (CLAHE). The proposed model tests and evaluates through many real sand dust images. The experimental results show that the proposed solution is outperformed the current studies in terms of effectively removing the red and yellow cast and provides high quality and quantity dust images.
</details>
<details>
<summary>摘要</summary>
图像 capture in 尘埃环境中，由于低可见度和质量，需要进行图像增强。在这种应用中，提出了一种基于颜色修正和新成员函数的图像增强模型。该模型包括三个阶段：颜色偏移 correction，霾除，以及对比和亮度提高。颜色偏移使用新的成员函数来调整 YUV 色彩空间中 U 和 V 的值。使用 Adaptive Dark Channel Prior (A-DCP) 进行霾除。对比和亮度提高基于 Contrast Limited Adaptive Histogram Equalization (CLAHE)。提出的模型在多个真实的沙尘图像上进行测试和评估。实验结果表明，提出的解决方案在效果上超过现有研究，可以有效地去除红色和黄色投影，并提供高质量和量的尘埃图像。
</details></li>
</ul>
<hr>
<h2 id="Generative-AI-for-Medical-Imaging-extending-the-MONAI-Framework"><a href="#Generative-AI-for-Medical-Imaging-extending-the-MONAI-Framework" class="headerlink" title="Generative AI for Medical Imaging: extending the MONAI Framework"></a>Generative AI for Medical Imaging: extending the MONAI Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15208">http://arxiv.org/abs/2307.15208</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/project-monai/generativemodels">https://github.com/project-monai/generativemodels</a></li>
<li>paper_authors: Walter H. L. Pinaya, Mark S. Graham, Eric Kerfoot, Petru-Daniel Tudosiu, Jessica Dafflon, Virginia Fernandez, Pedro Sanchez, Julia Wolleb, Pedro F. da Costa, Ashay Patel, Hyungjin Chung, Can Zhao, Wei Peng, Zelong Liu, Xueyan Mei, Oeslle Lucena, Jong Chul Ye, Sotirios A. Tsaftaris, Prerna Dogra, Andrew Feng, Marc Modat, Parashkev Nachev, Sebastien Ourselin, M. Jorge Cardoso</li>
<li>For: The paper aims to provide a freely available open-source platform for training, evaluating, and deploying generative models for medical imaging applications.* Methods: The paper presents MONAI Generative Models, which include a variety of architectures such as diffusion models, autoregressive transformers, and GANs. The models are implemented in a generalizable fashion, allowing for extension to 2D or 3D scenarios and different modalities such as CT, MRI, and X-Ray data.* Results: The paper demonstrates the effectiveness of the proposed platform by reproducing state-of-the-art studies in a standardized way and providing pre-trained models for the community. The results show that the models can be extended to different anatomical areas and modalities, and the platform is modular and extensible, ensuring long-term maintainability and future feature extension.Here’s the simplified Chinese text for the three key points:</li>
<li>for: 这篇论文的目的是提供医疗影像领域的自由开源平台，用于训练、评估和部署生成模型。</li>
<li>methods: 论文提出了MONAI生成模型，包括各种架构，如扩散模型、自然语言转换模型和GAN等。这些模型被实现在可扩展的方式下，可以扩展到2D或3D场景和不同的感知频谱数据。</li>
<li>results: 论文通过 reproduce state-of-the-art studies in a standardized way和提供社区可用的预训练模型，证明了提案的平台的效果。结果显示，模型可以扩展到不同的解剖区域和感知频谱数据，并且平台具有可扩展和维护的特点，以便将来扩展功能。<details>
<summary>Abstract</summary>
Recent advances in generative AI have brought incredible breakthroughs in several areas, including medical imaging. These generative models have tremendous potential not only to help safely share medical data via synthetic datasets but also to perform an array of diverse applications, such as anomaly detection, image-to-image translation, denoising, and MRI reconstruction. However, due to the complexity of these models, their implementation and reproducibility can be difficult. This complexity can hinder progress, act as a use barrier, and dissuade the comparison of new methods with existing works. In this study, we present MONAI Generative Models, a freely available open-source platform that allows researchers and developers to easily train, evaluate, and deploy generative models and related applications. Our platform reproduces state-of-art studies in a standardised way involving different architectures (such as diffusion models, autoregressive transformers, and GANs), and provides pre-trained models for the community. We have implemented these models in a generalisable fashion, illustrating that their results can be extended to 2D or 3D scenarios, including medical images with different modalities (like CT, MRI, and X-Ray data) and from different anatomical areas. Finally, we adopt a modular and extensible approach, ensuring long-term maintainability and the extension of current applications for future features.
</details>
<details>
<summary>摘要</summary>
最近的生成AI技术突破有很大的进步，特别是在医学影像领域。这些生成模型具有很大的潜力，不仅可以帮助安全地分享医学数据通过生成的数据集，还可以执行多种多样的应用，如异常检测、图像到图像翻译、减噪和MRI重建。然而，由于这些模型的复杂性，其实现和复制可能会困难。这种复杂性可能会阻碍进步，成为使用障碍和阻碍新方法与现有工作的比较。在本研究中，我们介绍MONAI生成模型平台，这是一个免费、开源的平台，允许研究人员和开发人员轻松地训练、评估和部署生成模型和相关应用。我们的平台可以复制现状的研究，使用不同的架构（如扩散模型、自适应变换和GAN），并提供了社区可用的预训练模型。我们实现了这些模型的普适性，说明它们的结果可以扩展到2D或3D场景，包括医学影像不同模式（如CT、MRI和X射数据）和不同解剖区域。最后，我们采用了模块化和可扩展的方法，确保长期维护和未来特性的扩展。
</details></li>
</ul>
<hr>
<h2 id="Sparsity-aware-coding-for-single-photon-sensitive-vision-using-Selective-Sensing"><a href="#Sparsity-aware-coding-for-single-photon-sensitive-vision-using-Selective-Sensing" class="headerlink" title="Sparsity aware coding for single photon sensitive vision using Selective Sensing"></a>Sparsity aware coding for single photon sensitive vision using Selective Sensing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15184">http://arxiv.org/abs/2307.15184</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yizhou Lu, Trevor Seets, Ehsan Ahmadi, Felipe Gutierrez-Barragan, Andreas Velten</li>
<li>for: 提高图像技术的表达能力</li>
<li>methods: 利用训练数据学习偏好，优化编码策略为下游分类任务</li>
<li>results: 比传统编码策略更高的编码性能和总准确率，适用于光子计数设备下的图像分类任务<details>
<summary>Abstract</summary>
Optical coding has been widely adopted to improve the imaging techniques. Traditional coding strategies developed under additive Gaussian noise fail to perform optimally in the presence of Poisson noise. It has been observed in previous studies that coding performance varies significantly between these two noise models. In this work, we introduce a novel approach called selective sensing, which leverages training data to learn priors and optimizes the coding strategies for downstream classification tasks. By adapting to the specific characteristics of photon-counting sensors, the proposed method aims to improve coding performance under Poisson noise and enhance overall classification accuracy. Experimental and simulated results demonstrate the effectiveness of selective sensing in comparison to traditional coding strategies, highlighting its potential for practical applications in photon counting scenarios where Poisson noise are prevalent.
</details>
<details>
<summary>摘要</summary>
光学编码已广泛应用以提高成像技术。传统的编码策略在添加性 Gaussian 噪声下发展而来，不能在Poisson噪声下表现优化。先前的研究发现，编码性能在这两种噪声模型之间有很大差异。在这种工作中，我们提出了一种新的方法 called 选择感知，通过使用训练数据来学习假设和优化编码策略，以提高下游分类任务的性能。适应光子计数器的特点，提出的方法可以在Poisson噪声下提高编码性能并提高总分类精度。实验和 simulate 结果表明，选择感知方法比传统编码策略更有效，这 highlights 其在光子计数器场景中的应用潜力。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/28/eess.IV_2023_07_28/" data-id="clmjn91q600ft0j88hflthncf" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_07_27" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/27/cs.LG_2023_07_27/" class="article-date">
  <time datetime="2023-07-26T16:00:00.000Z" itemprop="datePublished">2023-07-27</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/27/cs.LG_2023_07_27/">cs.LG - 2023-07-27 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Federated-Model-Aggregation-via-Self-Supervised-Priors-for-Highly-Imbalanced-Medical-Image-Classification"><a href="#Federated-Model-Aggregation-via-Self-Supervised-Priors-for-Highly-Imbalanced-Medical-Image-Classification" class="headerlink" title="Federated Model Aggregation via Self-Supervised Priors for Highly Imbalanced Medical Image Classification"></a>Federated Model Aggregation via Self-Supervised Priors for Highly Imbalanced Medical Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14959">http://arxiv.org/abs/2307.14959</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xmed-lab/fed-mas">https://github.com/xmed-lab/fed-mas</a></li>
<li>paper_authors: Marawan Elbatel, Hualiang Wang, Robert Martí, Huazhu Fu, Xiaomeng Li</li>
<li>for: 这篇论文主要针对医疗领域 Federated Learning 中的高度不均衡数据集，包括皮肤损伤和肠道图像。现有的 Federated 方法在高度不均衡数据集上主要是优化全局模型，而不考虑医疗影像中的内部类别差异。</li>
<li>methods: 本论文使用公共可用的自动学习助记网络（如 MoCo-V2）在每个客户端上进行本地预训练，并发现使用共享 auxiliary 预训练模型可以获得一致异常度测量。基于这些发现，我们 derivate了一种动态均衡模型聚合方法（MAS）来导引全局模型优化。</li>
<li>results: Fed-MAS 可以与不同的本地学习方法结合使用，以实现高度可靠和无偏见的全局模型。我们的代码可以在 \url{<a target="_blank" rel="noopener" href="https://github.com/xmed-lab/Fed-MAS%7D">https://github.com/xmed-lab/Fed-MAS}</a> 上找到。<details>
<summary>Abstract</summary>
In the medical field, federated learning commonly deals with highly imbalanced datasets, including skin lesions and gastrointestinal images. Existing federated methods under highly imbalanced datasets primarily focus on optimizing a global model without incorporating the intra-class variations that can arise in medical imaging due to different populations, findings, and scanners. In this paper, we study the inter-client intra-class variations with publicly available self-supervised auxiliary networks. Specifically, we find that employing a shared auxiliary pre-trained model, like MoCo-V2, locally on every client yields consistent divergence measurements. Based on these findings, we derive a dynamic balanced model aggregation via self-supervised priors (MAS) to guide the global model optimization. Fed-MAS can be utilized with different local learning methods for effective model aggregation toward a highly robust and unbiased global model. Our code is available at \url{https://github.com/xmed-lab/Fed-MAS}.
</details>
<details>
<summary>摘要</summary>
医疗领域中，联合学习经常面临高度不均衡的数据集，包括皮肤病变和Digestive tract影像。现有的联合方法在高度不均衡数据集上主要是优化全球模型，而不考虑医疗影像中的内部类别差异，这些差异可能 arise due to different populations, findings, and scanners。在这篇论文中，我们研究了客户端间内部类别差异，使用公共可用的无监督辅助网络。我们发现，在每个客户端上使用共享的辅助预训练模型，如MoCo-V2，可以获得一致的分化度测量。基于这些发现，我们 derive了一种动态平衡的模型聚合方法（MAS），以导引全球模型优化。Fed-MAS可以与不同的本地学习方法结合使用，以实现高度可靠和无偏的全球模型。我们的代码可以在 GitHub上找到：https://github.com/xmed-lab/Fed-MAS。
</details></li>
</ul>
<hr>
<h2 id="Multi-Source-Domain-Adaptation-through-Dataset-Dictionary-Learning-in-Wasserstein-Space"><a href="#Multi-Source-Domain-Adaptation-through-Dataset-Dictionary-Learning-in-Wasserstein-Space" class="headerlink" title="Multi-Source Domain Adaptation through Dataset Dictionary Learning in Wasserstein Space"></a>Multi-Source Domain Adaptation through Dataset Dictionary Learning in Wasserstein Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14953">http://arxiv.org/abs/2307.14953</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/eddardd/demo-dadil">https://github.com/eddardd/demo-dadil</a></li>
<li>paper_authors: Eduardo Fernandes Montesuma, Fred Ngolè Mboula, Antoine Souloumiac</li>
<li>for: 解决多源领域适应（MSDA）问题，即在多个标注源频道中传递知识，并且避免数据分布偏移。</li>
<li>methods: 提出了一种基于字典学习和最优运输的MSDA框架。对每个频道进行解释，表示每个频道为一个 Wasserstein 质量中心，并提出了一种基于字典的两种新方法：DaDil-R 和 DaDiL-E。</li>
<li>results: 在 Caltech-Office、Office 31 和 CRWU 三个标准测试集上进行评估，并与之前的状态前进行了3.15%、2.29% 和 7.71% 的改进。最后，我们表明了 interpolations 在学习的atom集中的 Wasserstein 树可以泛化到目标频道。<details>
<summary>Abstract</summary>
This paper seeks to solve Multi-Source Domain Adaptation (MSDA), which aims to mitigate data distribution shifts when transferring knowledge from multiple labeled source domains to an unlabeled target domain. We propose a novel MSDA framework based on dictionary learning and optimal transport. We interpret each domain in MSDA as an empirical distribution. As such, we express each domain as a Wasserstein barycenter of dictionary atoms, which are empirical distributions. We propose a novel algorithm, DaDiL, for learning via mini-batches: (i) atom distributions; (ii) a matrix of barycentric coordinates. Based on our dictionary, we propose two novel methods for MSDA: DaDil-R, based on the reconstruction of labeled samples in the target domain, and DaDiL-E, based on the ensembling of classifiers learned on atom distributions. We evaluate our methods in 3 benchmarks: Caltech-Office, Office 31, and CRWU, where we improved previous state-of-the-art by 3.15%, 2.29%, and 7.71% in classification performance. Finally, we show that interpolations in the Wasserstein hull of learned atoms provide data that can generalize to the target domain.
</details>
<details>
<summary>摘要</summary>
We represent each domain in MSDA as an empirical distribution, and express each domain as a Wasserstein barycenter of dictionary atoms, which are empirical distributions. We then propose a novel algorithm, DaDiL, for learning via mini-batches, which involves (i) learning atom distributions, and (ii) computing a matrix of barycentric coordinates.Based on our dictionary, we propose two novel methods for MSDA: DaDil-R, which uses the reconstruction of labeled samples in the target domain, and DaDiL-E, which uses the ensembling of classifiers learned on atom distributions. We evaluate our methods on three benchmark datasets: Caltech-Office, Office 31, and CRWU, and show that our approach achieves state-of-the-art performance, improving upon previous results by 3.15%, 2.29%, and 7.71% in classification accuracy.Finally, we demonstrate that interpolations in the Wasserstein hull of learned atoms can provide data that can generalize to the target domain, providing a promising avenue for future research.
</details></li>
</ul>
<hr>
<h2 id="Network-Fault-tolerant-and-Byzantine-resilient-Social-Learning-via-Collaborative-Hierarchical-Non-Bayesian-Learning"><a href="#Network-Fault-tolerant-and-Byzantine-resilient-Social-Learning-via-Collaborative-Hierarchical-Non-Bayesian-Learning" class="headerlink" title="Network Fault-tolerant and Byzantine-resilient Social Learning via Collaborative Hierarchical Non-Bayesian Learning"></a>Network Fault-tolerant and Byzantine-resilient Social Learning via Collaborative Hierarchical Non-Bayesian Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14952">http://arxiv.org/abs/2307.14952</a></li>
<li>repo_url: None</li>
<li>paper_authors: Connor Mclaughlin, Matthew Ding, Denis Edogmus, Lili Su</li>
<li>for:  addresses the problem of non-Bayesian learning over networks that are vulnerable to communication failures and external adversarial attacks.</li>
<li>methods:  proposes a hierarchical robust push-sum algorithm, a sparse information fusion rule, and a packet-dropping fault-tolerant non-Bayesian learning algorithm with provable convergence guarantees.</li>
<li>results:  achieves average consensus despite frequent packet-dropping link failures and external adversarial attacks, and solves the non-Bayesian learning problem via running multiple dynamics.Here is the simplified Chinese text:</li>
<li>for:  Addresses 非托 bayesian 学习问题，即在受到通信故障和外部恶意攻击的网络上。</li>
<li>methods: 提出了层次系统架构，并使用了 packets 损失链接故障和外部恶意攻击的鲁棒推 sum 算法， sparse 信息融合规则，以及一种 packet-dropping  fault-tolerant 非托 bayesian 学习算法，并提供了可证明的收敛保证。</li>
<li>results:  Achieves 平均consensus ，并在 packet-dropping 链接故障和外部恶意攻击下解决了非托 bayesian 学习问题，并通过运行多个动力来解决这个问题。<details>
<summary>Abstract</summary>
As the network scale increases, existing fully distributed solutions start to lag behind the real-world challenges such as (1) slow information propagation, (2) network communication failures, and (3) external adversarial attacks. In this paper, we focus on hierarchical system architecture and address the problem of non-Bayesian learning over networks that are vulnerable to communication failures and adversarial attacks. On network communication, we consider packet-dropping link failures.   We first propose a hierarchical robust push-sum algorithm that can achieve average consensus despite frequent packet-dropping link failures. We provide a sparse information fusion rule between the parameter server and arbitrarily selected network representatives. Then, interleaving the consensus update step with a dual averaging update with Kullback-Leibler (KL) divergence as the proximal function, we obtain a packet-dropping fault-tolerant non-Bayesian learning algorithm with provable convergence guarantees.   On external adversarial attacks, we consider Byzantine attacks in which the compromised agents can send maliciously calibrated messages to others (including both the agents and the parameter server). To avoid the curse of dimensionality of Byzantine consensus, we solve the non-Bayesian learning problem via running multiple dynamics, each of which only involves Byzantine consensus with scalar inputs. To facilitate resilient information propagation across sub-networks, we use a novel Byzantine-resilient gossiping-type rule at the parameter server.
</details>
<details>
<summary>摘要</summary>
随着网络规模的增加，现有的完全分布式解决方案开始落后于实际世界中的挑战，如（1）慢速信息传播，（2）网络通信失败，以及（3）外部敌意攻击。在这篇论文中，我们关注层次系统架构，并解决非bayesian学习问题，面临网络通信失败和外部敌意攻击。对于网络通信，我们考虑 packet-dropping 链接故障。我们首先提出一种层次可靠推轮算法，可以在 packet-dropping 链接故障情况下达到平均共识。我们提供一种稀缺信息融合规则，使得参数服务器和随机选择的网络代表进行简单的信息交换。然后，将整体协同更新步骤与 dual averaging 更新步骤结合，使得 packet-dropping 故障tolerant 非bayesian 学习算法具有可证明的收敛保证。对于外部敌意攻击，我们考虑 Byzantine 攻击，在这种情况下，被入侵的代理会发送卑势化的消息到其他代理（包括代理和参数服务器）。为了避免 Byzantine 协同收敛的维度约束，我们使用多个动态，每个动态仅涉及 Byzantine 协同，并使用一种新的 Byzantine 抗性的聊天规则。为了促进网络下扩散信息的稳定传递，我们使用一种新的 Byzantine 抗性的聊天规则。
</details></li>
</ul>
<hr>
<h2 id="A-Self-Adaptive-Penalty-Method-for-Integrating-Prior-Knowledge-Constraints-into-Neural-ODEs"><a href="#A-Self-Adaptive-Penalty-Method-for-Integrating-Prior-Knowledge-Constraints-into-Neural-ODEs" class="headerlink" title="A Self-Adaptive Penalty Method for Integrating Prior Knowledge Constraints into Neural ODEs"></a>A Self-Adaptive Penalty Method for Integrating Prior Knowledge Constraints into Neural ODEs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14940">http://arxiv.org/abs/2307.14940</a></li>
<li>repo_url: None</li>
<li>paper_authors: C. Coelho, M. Fernanda P. Costa, L. L. Ferrás</li>
<li>for: 模拟自然系统的持续动力学行为</li>
<li>methods: 使用神经ordinary differential equation（Neural ODE）模型</li>
<li>results: 提出了一种自适应罚函数算法，以便模拟受限制的自然系统Here’s a more detailed explanation of each point:</li>
<li>for: The paper is written to model the continuous dynamics of natural systems, specifically using Neural ODEs.</li>
<li>methods: The paper proposes a self-adaptive penalty algorithm for Neural ODEs to enable modelling of constrained natural systems. This algorithm dynamically adjusts the penalty parameters based on the data.</li>
<li>results: The proposed approach is validated using three natural systems with prior knowledge constraints: population growth, chemical reaction evolution, and damped harmonic oscillator motion. The numerical experiments show that the self-adaptive penalty approach provides more accurate and robust models with reliable and meaningful predictions.<details>
<summary>Abstract</summary>
The continuous dynamics of natural systems has been effectively modelled using Neural Ordinary Differential Equations (Neural ODEs). However, for accurate and meaningful predictions, it is crucial that the models follow the underlying rules or laws that govern these systems. In this work, we propose a self-adaptive penalty algorithm for Neural ODEs to enable modelling of constrained natural systems. The proposed self-adaptive penalty function can dynamically adjust the penalty parameters. The explicit introduction of prior knowledge helps to increase the interpretability of Neural ODE -based models. We validate the proposed approach by modelling three natural systems with prior knowledge constraints: population growth, chemical reaction evolution, and damped harmonic oscillator motion. The numerical experiments and a comparison with other penalty Neural ODE approaches and \emph{vanilla} Neural ODE, demonstrate the effectiveness of the proposed self-adaptive penalty algorithm for Neural ODEs in modelling constrained natural systems. Moreover, the self-adaptive penalty approach provides more accurate and robust models with reliable and meaningful predictions.
</details>
<details>
<summary>摘要</summary>
自然系统的连续动力学已经非常有效地使用神经常微方程（Neural ODE）来模拟。但是，为了获得准确和有意义的预测，模型需要遵循下面的规则或法律来控制这些系统。在这种工作中，我们提议一种自适应罚函数算法来使神经常微方程模型受到约束。这种自适应罚函数可以动态调整罚参数。通过直接引入先知知识，可以增加神经常微方程模型的解释性。我们验证了提议的方法，通过模拟三个自然系统受约束的模型：人口增长、化学反应演化和抑制响应振荡。计算实验和与其他罚Neural ODE方法和"vanilla"Neural ODE进行比较，表明提议的自适应罚函数算法对神经常微方程模型的受约束模型具有更高的有效性和可靠性。此外，自适应罚approach还可以提供更准确和稳定的预测。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Interaction-Aware-Interval-Analysis-of-Neural-Network-Feedback-Loops"><a href="#Efficient-Interaction-Aware-Interval-Analysis-of-Neural-Network-Feedback-Loops" class="headerlink" title="Efficient Interaction-Aware Interval Analysis of Neural Network Feedback Loops"></a>Efficient Interaction-Aware Interval Analysis of Neural Network Feedback Loops</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14938">http://arxiv.org/abs/2307.14938</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saber Jafarpour, Akash Harapanahalli, Samuel Coogan</li>
<li>for: 本文提出一种 computationally efficient 框架，用于计算 interval reachability 系统中 neural network 控制器的行为下的不确定性。</li>
<li>methods: 本文使用 inclusion functions 将开loop系统和神经网络控制器嵌入更大的 embedding system 中，以便使用单个轨迹来拟合原始系统的行为下 uncertainty。提出了两种不同的构建关闭loop embedding system 的方法，它们分别考虑了系统和控制器之间的交互方式。</li>
<li>results: 本文在 Python 框架 ReachMM 中实现了这种方法，并在各种例子和 benchmark 上示出了高效性和可扩展性。<details>
<summary>Abstract</summary>
In this paper, we propose a computationally efficient framework for interval reachability of systems with neural network controllers. Our approach leverages inclusion functions for the open-loop system and the neural network controller to embed the closed-loop system into a larger-dimensional embedding system, where a single trajectory over-approximates the original system's behavior under uncertainty. We propose two methods for constructing closed-loop embedding systems, which account for the interactions between the system and the controller in different ways. The interconnection-based approach considers the worst-case evolution of each coordinate separately by substituting the neural network inclusion function into the open-loop inclusion function. The interaction-based approach uses novel Jacobian-based inclusion functions to capture the first-order interactions between the open-loop system and the controller by leveraging state-of-the-art neural network verifiers. Finally, we implement our approach in a Python framework called ReachMM to demonstrate its efficiency and scalability on benchmarks and examples ranging to $200$ state dimensions.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种计算效率高的扩Interval可达性框架，用于系统控制器是神经网络的系统。我们的方法利用包含函数来包含开 Loop系统和神经网络控制器，将关闭Loop系统嵌入到更大的嵌入系统中，以便单个轨迹过度度量原始系统的行为下 uncertainty。我们提出了两种方法用于构建关闭Loop嵌入系统，这两种方法均考虑了系统和控制器之间的互动。基于连接的方法将每个坐标的最坏情况演化分别substitue神经网络包含函数到开 Loop包含函数中。基于互动的方法使用新的Jacobian包含函数来捕捉开 Loop系统和控制器之间的首次互动，通过利用现有神经网络验证器。最后，我们在Python框架ReachMM中实现了我们的方法，以示其效率和可扩展性。
</details></li>
</ul>
<hr>
<h2 id="PanGu-Coder2-Boosting-Large-Language-Models-for-Code-with-Ranking-Feedback"><a href="#PanGu-Coder2-Boosting-Large-Language-Models-for-Code-with-Ranking-Feedback" class="headerlink" title="PanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback"></a>PanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14936">http://arxiv.org/abs/2307.14936</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bo Shen, Jiaxin Zhang, Taihong Chen, Daoguang Zan, Bing Geng, An Fu, Muhan Zeng, Ailun Yu, Jichuan Ji, Jingyang Zhao, Yuenan Guo, Qianxiang Wang</li>
<li>for: 本研究旨在提高预训练Code大语言模型（Code LLM）的代码生成性能。</li>
<li>methods: 本研究提出了一种新的RRTF（ Rank Responses to align Test&amp;Teacher Feedback）框架，用于有效地和高效地提高预训练Code LLM的代码生成性能。</li>
<li>results: 在OpenAI HumanEval benchmark上，PanGu-Coder2实现了62.20%的pass@1分数，并在CoderEval和LeetCode benchmark上表现出色，Consistently outperforming all previous Code LLMs。<details>
<summary>Abstract</summary>
Large Language Models for Code (Code LLM) are flourishing. New and powerful models are released on a weekly basis, demonstrating remarkable performance on the code generation task. Various approaches have been proposed to boost the code generation performance of pre-trained Code LLMs, such as supervised fine-tuning, instruction tuning, reinforcement learning, etc. In this paper, we propose a novel RRTF (Rank Responses to align Test&Teacher Feedback) framework, which can effectively and efficiently boost pre-trained large language models for code generation. Under this framework, we present PanGu-Coder2, which achieves 62.20% pass@1 on the OpenAI HumanEval benchmark. Furthermore, through an extensive evaluation on CoderEval and LeetCode benchmarks, we show that PanGu-Coder2 consistently outperforms all previous Code LLMs.
</details>
<details>
<summary>摘要</summary>
大型语言模型 для代码（代码 LLM）正在繁荣。每周新发布的新型号都在代码生成任务上表现出色。各种方法被提出来提高预训练代码 LLM 的代码生成性能，例如监督微调、指令优化、强化学习等。在这篇论文中，我们提出了一种新的 RRTF（排名回应对测试&教师反馈）框架，可以有效地和高效地提高预训练大型语言模型的代码生成性能。在这个框架下，我们介绍了 PanGu-Coder2，它在 OpenAI HumanEval benchmark 上取得了 62.20% 的 pass@1 分数。此外，通过对 CoderEval 和 LeetCode  benchmark 进行广泛的评估，我们表明 PanGu-Coder2 在所有前一代代码 LLM 之上卓越表现。
</details></li>
</ul>
<hr>
<h2 id="Solving-Data-Quality-Problems-with-Desbordante-a-Demo"><a href="#Solving-Data-Quality-Problems-with-Desbordante-a-Demo" class="headerlink" title="Solving Data Quality Problems with Desbordante: a Demo"></a>Solving Data Quality Problems with Desbordante: a Demo</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14935">http://arxiv.org/abs/2307.14935</a></li>
<li>repo_url: None</li>
<li>paper_authors: George Chernishev, Michael Polyntsov, Anton Chizhov, Kirill Stupakov, Ilya Shchuckin, Alexander Smirnov, Maxim Strutovsky, Alexey Shlyonskikh, Mikhail Firsov, Stepan Manannikov, Nikita Bobrov, Daniil Goncharov, Ilia Barutkin, Vladislav Shalnev, Kirill Muraviev, Anna Rakhmukova, Dmitriy Shcheka, Anton Chernikov, Mikhail Vyrodov, Yaroslav Kurbatov, Maxim Fofanov, Sergei Belokonnyi, Pavel Anosov, Arthur Saliou, Eduard Gaisin, Kirill Smirnov</li>
<li>for: 提高现代数据驱动行业中数据 profiling 的效率和可靠性，并提供对现有工具的良好整合。</li>
<li>methods: 使用可扩展的 C++ 核心，实现高效缓存和快速排序，并提供可靠的数据探索和检测功能。</li>
<li>results: 通过多种实际场景的示例，包括 typo 检测、数据重复检测和数据异常检测，表明 Desbordante 可以高效地解决不同的数据质量问题。<details>
<summary>Abstract</summary>
Data profiling is an essential process in modern data-driven industries. One of its critical components is the discovery and validation of complex statistics, including functional dependencies, data constraints, association rules, and others.   However, most existing data profiling systems that focus on complex statistics do not provide proper integration with the tools used by contemporary data scientists. This creates a significant barrier to the adoption of these tools in the industry. Moreover, existing systems were not created with industrial-grade workloads in mind. Finally, they do not aim to provide descriptive explanations, i.e. why a given pattern is not found. It is a significant issue as it is essential to understand the underlying reasons for a specific pattern's absence to make informed decisions based on the data.   Because of that, these patterns are effectively rest in thin air: their application scope is rather limited, they are rarely used by the broader public. At the same time, as we are going to demonstrate in this presentation, complex statistics can be efficiently used to solve many classic data quality problems.   Desbordante is an open-source data profiler that aims to close this gap. It is built with emphasis on industrial application: it is efficient, scalable, resilient to crashes, and provides explanations. Furthermore, it provides seamless Python integration by offloading various costly operations to the C++ core, not only mining.   In this demonstration, we show several scenarios that allow end users to solve different data quality problems. Namely, we showcase typo detection, data deduplication, and data anomaly detection scenarios.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate the following text into Simplified ChineseData profiling 是现代数据驱动行业中的一个重要过程。其中一个关键组件是发现和验证复杂统计，包括函数依赖关系、数据约束、关联规则等。然而，大多数现有的数据 profiling 系统，它们专注于复杂统计，并不提供适合当代数据科学家使用的合理集成。这创造了一个很大的障碍物，阻碍了这些工具在行业中的普及。此外，现有系统并不是为现代工作荟载设计的。最重要的是，它们不提供描述性解释，即为什么某个模式不存在。这是一个重要的问题，因为需要理解数据下面的根本原因，才能基于数据做出 Informed 决策。因此，这些模式效果是“浮空”的，它们的应用范围很限定，并且只有少数人使用。在这个演示中，我们将展示一些使用复杂统计解决不同数据质量问题的场景。具体来说，我们将展示 typo 检测、数据减重、数据异常检测等场景。Desbordante 是一个开源的数据 profiler，它专注于工业应用。它具有高效、可扩展、可靠性和描述性解释等特点。此外，它提供了PYTHON 集成，通过将多种费时操作委托给 C++ 核心来实现，不仅是探采。在这个演示中，我们将展示 Desbordante 如何解决不同数据质量问题。>>Here's the translation:<<SYS>>现代数据驱动行业中的一个重要过程是数据 profiling。这个过程中的一个关键组件是发现和验证复杂统计，例如函数依赖关系、数据约束、关联规则等。然而，大多数现有的数据 profiling 系统不适合当代数据科学家使用，因为它们不提供适合工业应用的合理集成。这创造了一个很大的障碍物，阻碍了这些工具在行业中的普及。现有系统并不是为现代工作荟载设计的。最重要的是，它们不提供描述性解释，即为什么某个模式不存在。这是一个重要的问题，因为需要理解数据下面的根本原因，才能基于数据做出 Informed 决策。因此，这些模式效果是“浮空”的，它们的应用范围很限定，并且只有少数人使用。在这个演示中，我们将展示一些使用复杂统计解决不同数据质量问题的场景。具体来说，我们将展示 typo 检测、数据减重、数据异常检测等场景。Desbordante 是一个开源的数据 profiler，它专注于工业应用。它具有高效、可扩展、可靠性和描述性解释等特点。此外，它提供了PYTHON 集成，通过将多种费时操作委托给 C++ 核心来实现，不仅是探采。在这个演示中，我们将展示 Desbordante 如何解决不同数据质量问题。
</details></li>
</ul>
<hr>
<h2 id="Approximate-Model-Based-Shielding-for-Safe-Reinforcement-Learning"><a href="#Approximate-Model-Based-Shielding-for-Safe-Reinforcement-Learning" class="headerlink" title="Approximate Model-Based Shielding for Safe Reinforcement Learning"></a>Approximate Model-Based Shielding for Safe Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00707">http://arxiv.org/abs/2308.00707</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sacktock/ambs">https://github.com/sacktock/ambs</a></li>
<li>paper_authors: Alexander W. Goodall, Francesco Belardinelli</li>
<li>for: 这篇论文目的是为了解决RL在实际世界中应用的问题，特别是在安全关键系统中。</li>
<li>methods: 这篇论文提出了一种名为approximate model-based shielding（AMBS）的原则，用于验证RL策略是否符合给定的安全约束。AMBS不需要知道系统的安全相关动态。</li>
<li>results: 论文提供了一个强有力的理论依据，并在一组Atari游戏中表明AMBS在安全意识方面表现出色，超过其他安全意识方法。<details>
<summary>Abstract</summary>
Reinforcement learning (RL) has shown great potential for solving complex tasks in a variety of domains. However, applying RL to safety-critical systems in the real-world is not easy as many algorithms are sample-inefficient and maximising the standard RL objective comes with no guarantees on worst-case performance. In this paper we propose approximate model-based shielding (AMBS), a principled look-ahead shielding algorithm for verifying the performance of learned RL policies w.r.t. a set of given safety constraints. Our algorithm differs from other shielding approaches in that it does not require prior knowledge of the safety-relevant dynamics of the system. We provide a strong theoretical justification for AMBS and demonstrate superior performance to other safety-aware approaches on a set of Atari games with state-dependent safety-labels.
</details>
<details>
<summary>摘要</summary>
利用增强学习（RL）解决复杂任务的潜力已经被证明了，但是在实际世界中应用RL到安全关键系统上并不容易，因为许多算法是采样不充分的，并且最大化标准RL目标不提供最坏情况性能的保证。在这篇论文中，我们提出了一种名为准确模型基于遮盾（AMBS）的原则正确的遮盾算法，用于验证RL策略与一组给定的安全约束之间的关系。我们的算法与其他安全意识的方法不同，不需要知道系统的安全相关动力学。我们提供了强有力的理论基础，并在一组Atari游戏中的状态依赖安全标签上示出了我们的方法的超越性。
</details></li>
</ul>
<hr>
<h2 id="Graph-based-Polyphonic-Multitrack-Music-Generation"><a href="#Graph-based-Polyphonic-Multitrack-Music-Generation" class="headerlink" title="Graph-based Polyphonic Multitrack Music Generation"></a>Graph-based Polyphonic Multitrack Music Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14928">http://arxiv.org/abs/2307.14928</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/emanuelecosenza/polyphemus">https://github.com/emanuelecosenza/polyphemus</a></li>
<li>paper_authors: Emanuele Cosenza, Andrea Valenti, Davide Bacciu</li>
<li>for: 这篇论文是为了研究用深度学习系统来生成乐曲，特别是使用图表来模型多重符号音乐。</li>
<li>methods: 该论文提出了一种新的图表表示法，并使用深度Variational Autoencoder来生成乐曲的结构和内容。这种方法可以根据指定的乐器和时间来控制生成过程，从而实现人机交互式的音乐合作。</li>
<li>results: 经过训练的模型能够生成愉悦的短和长乐曲，并可以实际地 interpolate  между它们，生成具有律动和和声一致性的乐曲。图表可视化表明模型可以将其隐藏空间按照知道的音乐概念进行组织。<details>
<summary>Abstract</summary>
Graphs can be leveraged to model polyphonic multitrack symbolic music, where notes, chords and entire sections may be linked at different levels of the musical hierarchy by tonal and rhythmic relationships. Nonetheless, there is a lack of works that consider graph representations in the context of deep learning systems for music generation. This paper bridges this gap by introducing a novel graph representation for music and a deep Variational Autoencoder that generates the structure and the content of musical graphs separately, one after the other, with a hierarchical architecture that matches the structural priors of music. By separating the structure and content of musical graphs, it is possible to condition generation by specifying which instruments are played at certain times. This opens the door to a new form of human-computer interaction in the context of music co-creation. After training the model on existing MIDI datasets, the experiments show that the model is able to generate appealing short and long musical sequences and to realistically interpolate between them, producing music that is tonally and rhythmically consistent. Finally, the visualization of the embeddings shows that the model is able to organize its latent space in accordance with known musical concepts.
</details>
<details>
<summary>摘要</summary>
图可以用来模拟多音轨 симвоlic music， где每个音和每个和声可以在不同的音乐层次结构中相互关联。然而，现有的研究很少考虑图表 Representation在深度学习系统中的应用。这篇论文填补了这个空白，并 introduce a novel graph representation for music and a deep Variational Autoencoder that generates the structure and content of musical graphs separately, one after the other, with a hierarchical architecture that matches the structural priors of music.通过分离音乐图的结构和内容，可以根据特定的乐器和时间指定生成。这打开了一种新的人机交互方式，即音乐合作创作。经过训练模型于现有的 MIDI 数据集，实验显示，模型能够生成愉悦的短和长乐 sequences，并可以实际地在之间 interpolate，生成具有和谐和节奏的音乐。最后，Embeddings 的可视化表明，模型能够在 latent space 中组织 according to known musical concepts。
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-Performance-of-Deep-Learning-Model-for-Material-Segmentation-on-Two-HPC-Systems"><a href="#Benchmarking-Performance-of-Deep-Learning-Model-for-Material-Segmentation-on-Two-HPC-Systems" class="headerlink" title="Benchmarking Performance of Deep Learning Model for Material Segmentation on Two HPC Systems"></a>Benchmarking Performance of Deep Learning Model for Material Segmentation on Two HPC Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14921">http://arxiv.org/abs/2307.14921</a></li>
<li>repo_url: None</li>
<li>paper_authors: Warren R. Williams, S. Ross Glandon, Luke L. Morris, Jing-Ru C. Cheng</li>
<li>for: 本研究的目的是提供高性能HPC系统的性能评估方法，以便提高任务调度器的性能。</li>
<li>methods: 本研究开发了一种基于机器学习模型的benchmark工具，该工具在GPU加速节点上进行物质分 segmentation分析，并使用MMdnn工具库和MINC-2500数据集。</li>
<li>results: 对两个ERDC DSRC系统（Onyx和Vulcanite）进行性能测试，结果显示，虽然Vulcanite在许多benchmark中具有更快的模型时间，但它也更容易受到环境因素的影响，导致性能下降，而Onyx的模型时间在benchmark中具有更高的一致性。<details>
<summary>Abstract</summary>
Performance Benchmarking of HPC systems is an ongoing effort that seeks to provide information that will allow for increased performance and improve the job schedulers that manage these systems. We develop a benchmarking tool that utilizes machine learning models and gathers performance data on GPU-accelerated nodes while they perform material segmentation analysis. The benchmark uses a ML model that has been converted from Caffe to PyTorch using the MMdnn toolkit and the MINC-2500 dataset. Performance data is gathered on two ERDC DSRC systems, Onyx and Vulcanite. The data reveals that while Vulcanite has faster model times in a large number of benchmarks, and it is also more subject to some environmental factors that can cause performances slower than Onyx. In contrast the model times from Onyx are consistent across benchmarks.
</details>
<details>
<summary>摘要</summary>
高性能计算机系统的性能测试是一项不断进行的努力，旨在提供可以提高性能和改进作业调度器的信息。我们开发了一个性能测试工具，该工具利用机器学习模型并在加速节点上进行材料分割分析时收集性能数据。这个benchmark使用通过Caffe到PyTorch的MMdnn工具包和MINC-2500数据集 converts的ML模型。在ERDC DSRC系统Onyx和Vulcanite上进行性能测试，数据显示，虽然Vulcanite在许多benchmark中具有更快的模型时间，但它也更容易受到环境因素的影响，导致性能 slower than Onyx。相比之下，Onyx上的模型时间在benchmark中具有一致性。
</details></li>
</ul>
<hr>
<h2 id="NSA-Naturalistic-Support-Artifact-to-Boost-Network-Confidence"><a href="#NSA-Naturalistic-Support-Artifact-to-Boost-Network-Confidence" class="headerlink" title="NSA: Naturalistic Support Artifact to Boost Network Confidence"></a>NSA: Naturalistic Support Artifact to Boost Network Confidence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14917">http://arxiv.org/abs/2307.14917</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abhijith Sharma, Phil Munz, Apurva Narayan<br>for: 这种研究旨在提高视觉人工智能系统对自然环境的抗衰减能力。methods: 该研究提议使用自然支持物件（NSA）来提高预测confidence分数。NSA通过DC-GAN进行artifact Training，以确保在场景中具有高视觉准确性。results: 对于Imagenette dataset中的自然衰减，NSA能够提高预测confidence分数四倍。此外，NSA还能够提高对抗攻击的准确率平均提高8%。此外，通过精度图来分析NSA的作用，可以了解它们如何提高预测confidence。<details>
<summary>Abstract</summary>
Visual AI systems are vulnerable to natural and synthetic physical corruption in the real-world. Such corruption often arises unexpectedly and alters the model's performance. In recent years, the primary focus has been on adversarial attacks. However, natural corruptions (e.g., snow, fog, dust) are an omnipresent threat to visual AI systems and should be considered equally important. Many existing works propose interesting solutions to train robust models against natural corruption. These works either leverage image augmentations, which come with the additional cost of model training, or place suspicious patches in the scene to design unadversarial examples. In this work, we propose the idea of naturalistic support artifacts (NSA) for robust prediction. The NSAs are shown to be beneficial in scenarios where model parameters are inaccessible and adding artifacts in the scene is feasible. The NSAs are natural looking objects generated through artifact training using DC-GAN to have high visual fidelity in the scene. We test against natural corruptions on the Imagenette dataset and observe the improvement in prediction confidence score by four times. We also demonstrate NSA's capability to increase adversarial accuracy by 8\% on average. Lastly, we qualitatively analyze NSAs using saliency maps to understand how they help improve prediction confidence.
</details>
<details>
<summary>摘要</summary>
视觉人工智能系统容易受到自然和人工物理损害的影响，这种损害通常会不断发生，影响模型的性能。在过去几年，主要关注点是对抗性攻击。然而，自然损害（如雪、雾、尘埃）对视觉人工智能系统是一种普遍存在的威胁，应该得到同等重视。许多现有的研究提出了许多有趣的解决方案，如通过图像扩展来训练Robust模型，或者在场景中添加不可信的质地来设计不可 adversarial例子。在这个工作中，我们提出了自然支持物件（NSA）的想法，用于Robust预测。NSA通过使用DC-GAN进行 artifact训练，在场景中生成高可识别度的自然looking对象。我们对Imagenette数据集进行了对自然损害的测试，并观察到预测信心分数提高四倍。此外，我们还证明NSA可以提高对抗率平均8%。最后，我们使用saliency map来 качеitative分析NSA，以便更好地理解它们如何提高预测 confidence。
</details></li>
</ul>
<hr>
<h2 id="Clustering-of-illustrations-by-atmosphere-using-a-combination-of-supervised-and-unsupervised-learning"><a href="#Clustering-of-illustrations-by-atmosphere-using-a-combination-of-supervised-and-unsupervised-learning" class="headerlink" title="Clustering of illustrations by atmosphere using a combination of supervised and unsupervised learning"></a>Clustering of illustrations by atmosphere using a combination of supervised and unsupervised learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15099">http://arxiv.org/abs/2307.15099</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keisuke Kubota, Masahiro Okuda<br>for:这 paper 是为了解决图像描述中的氛围问题，通过将图像分类为不同的氛围来提高搜索和推荐的效果。methods:这 paper 使用了双向学习和自动标注来解决氛围分类问题，并使用了supervised learning和Unsupervised learning来获得特征向量。results:实验结果表明，这 paper 的方法可以比传统方法更好地对图像进行分类，并且可以准确地捕捉图像中的氛围特征。<details>
<summary>Abstract</summary>
The distribution of illustrations on social media, such as Twitter and Pixiv has increased with the growing popularity of animation, games, and animated movies. The "atmosphere" of illustrations plays an important role in user preferences. Classifying illustrations by atmosphere can be helpful for recommendations and searches. However, assigning clear labels to the elusive "atmosphere" and conventional supervised classification is not always practical. Furthermore, even images with similar colors, edges, and low-level features may not have similar atmospheres, making classification based on low-level features challenging. In this paper, this problem is solved using both supervised and unsupervised learning with pseudo-labels. The feature vectors are obtained using the supervised method with pseudo-labels that contribute to an ambiguous atmosphere. Further, clustering is performed based on these feature vectors. Experimental analyses show that our method outperforms conventional methods in human-like clustering on datasets manually classified by humans.
</details>
<details>
<summary>摘要</summary>
社交媒体上的插图分布量增加，与动画、游戏和动画电影的流行相关。插图的“氛围”在用户喜好中扮演重要角色。根据氛围进行分类可以有助于推荐和搜索。但是，将氛围论坛到明确的标签是不实用的。此外，即使颜色、边缘和低级特征都相似，插图的氛围可能不同，从而使基于低级特征的分类困难。在这篇论文中，我们解决了这个问题，使用了超级vised和无监督学习，并使用pseudo标签。通过这些特征向量，我们进行了团 clustering。实验分析表明，我们的方法在人类化 clustering 中超出了传统方法。
</details></li>
</ul>
<hr>
<h2 id="Scaling-Session-Based-Transformer-Recommendations-using-Optimized-Negative-Sampling-and-Loss-Functions"><a href="#Scaling-Session-Based-Transformer-Recommendations-using-Optimized-Negative-Sampling-and-Loss-Functions" class="headerlink" title="Scaling Session-Based Transformer Recommendations using Optimized Negative Sampling and Loss Functions"></a>Scaling Session-Based Transformer Recommendations using Optimized Negative Sampling and Loss Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14906">http://arxiv.org/abs/2307.14906</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/otto-de/tron">https://github.com/otto-de/tron</a></li>
<li>paper_authors: Timo Wilm, Philipp Normann, Sophie Baumeister, Paul-Vincent Kobow</li>
<li>for: 这篇论文是为了提出一种可扩展的会话基于转换器推荐算法，以解决现有模型的缺乏扩展性和性能问题。</li>
<li>methods: 该论文使用了最佳负样本选择和列式损失函数，以提高推荐准确性。</li>
<li>results: 对于大规模电商数据集，TRON显示了与当前方法相比的推荐质量提高，同时保持与SASRec相似的训练速度。实际应用中，TRON实现了与SASRec相比18.14%的点击率提高。<details>
<summary>Abstract</summary>
This work introduces TRON, a scalable session-based Transformer Recommender using Optimized Negative-sampling. Motivated by the scalability and performance limitations of prevailing models such as SASRec and GRU4Rec+, TRON integrates top-k negative sampling and listwise loss functions to enhance its recommendation accuracy. Evaluations on relevant large-scale e-commerce datasets show that TRON improves upon the recommendation quality of current methods while maintaining training speeds similar to SASRec. A live A/B test yielded an 18.14% increase in click-through rate over SASRec, highlighting the potential of TRON in practical settings. For further research, we provide access to our source code at https://github.com/otto-de/TRON and an anonymized dataset at https://github.com/otto-de/recsys-dataset.
</details>
<details>
<summary>摘要</summary>
这个研究介绍了TRON，一种可扩展的会话基于转移器推荐器，使用优化的负样本选择。为了解决现有模型如SASRec和GRU4Rec+的可扩展性和性能限制，TRON integrates top-k负样本选择和listwise损失函数，以提高推荐准确性。对于相关的大规模电商数据集，评估表明TRON可以超越当前方法的推荐质量，同时保持与SASRec相似的训练速度。一次实际A/B测试显示，TRON相比SASRec提高了18.14%的点击率，这highlights TRON在实际场景中的潜力。为了进一步研究，我们在https://github.com/otto-de/TRON上提供了源代码，并在https://github.com/otto-de/recsys-dataset上提供了匿名数据集。
</details></li>
</ul>
<hr>
<h2 id="CodeLens-An-Interactive-Tool-for-Visualizing-Code-Representations"><a href="#CodeLens-An-Interactive-Tool-for-Visualizing-Code-Representations" class="headerlink" title="CodeLens: An Interactive Tool for Visualizing Code Representations"></a>CodeLens: An Interactive Tool for Visualizing Code Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14902">http://arxiv.org/abs/2307.14902</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuejun Guo, Seifeddine Bettaieb, Qiang Hu, Yves Le Traon, Qiang Tang</li>
<li>for: 提供了一个可视化代码表示的工具，帮助开发者快速理解不同类型的代码表示，以及代码表示所代表的输入。</li>
<li>methods: 支持多种程式语言，包括Java、Python和JavaScript，并且支持四种代码表示方法，包括字符串序列、抽象 syntax tree (AST)、资料流graph (DFG) 和控制流graph (CFG)。</li>
<li>results: 为开发者提供了一个可用于多种代码表示的可视化互动环境，帮助开发者快速理解代码表示，并且获取代码表示所代表的输入。<details>
<summary>Abstract</summary>
Representing source code in a generic input format is crucial to automate software engineering tasks, e.g., applying machine learning algorithms to extract information. Visualizing code representations can further enable human experts to gain an intuitive insight into the code. Unfortunately, as of today, there is no universal tool that can simultaneously visualise different types of code representations. In this paper, we introduce a tool, CodeLens, which provides a visual interaction environment that supports various representation methods and helps developers understand and explore them. CodeLens is designed to support multiple programming languages, such as Java, Python, and JavaScript, and four types of code representations, including sequence of tokens, abstract syntax tree (AST), data flow graph (DFG), and control flow graph (CFG). By using CodeLens, developers can quickly visualize the specific code representation and also obtain the represented inputs for models of code. The Web-based interface of CodeLens is available at http://www.codelens.org. The demonstration video can be found at http://www.codelens.org/demo.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将源代码表示format化为通用输入格式是软件工程任务的关键之一，例如应用机器学习算法提取信息。可视化代码表示可以帮助人工专家获得直观印象。 unfortunately，到目前为止，没有一个通用工具可同时可视化不同类型的代码表示。在这篇论文中，我们介绍了一个工具——CodeLens，它提供了一个可视化交互环境，支持多种代码表示方法，帮助开发者理解和探索代码。CodeLens支持多种编程语言，如Java、Python和JavaScript，以及四种代码表示方法，包括序列化token、抽象语法树（AST）、数据流图（DFG）和控制流图（CFG）。通过使用CodeLens，开发者可快速可视化特定的代码表示，并获得代码表示的输入数据。Web-based Interface of CodeLens可在http://www.codelens.org上获取。示例视频可在http://www.codelens.org/demo找到。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Learning-for-Improved-Synthetic-Aperture-Sonar-Target-Recognition"><a href="#Self-Supervised-Learning-for-Improved-Synthetic-Aperture-Sonar-Target-Recognition" class="headerlink" title="Self-Supervised Learning for Improved Synthetic Aperture Sonar Target Recognition"></a>Self-Supervised Learning for Improved Synthetic Aperture Sonar Target Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15098">http://arxiv.org/abs/2307.15098</a></li>
<li>repo_url: None</li>
<li>paper_authors: BW Sheffield</li>
<li>for: 本研究探讨了基于自动学习（SSL）的目标识别方法在激光孔干成像中的应用，以解决水下环境下传统计算机视觉技术的不足。</li>
<li>methods: 本研究使用了两种知名的SSL算法：MoCov2和BYOL，以及一个常见的supervised learning模型：ResNet18，进行比较。</li>
<li>results: 结果表明，当使用少量标签时，SSL模型可以超过完全监督学习模型，但当使用所有标签时，它们不能超过完全监督学习模型。这些结果表明SSL可以作为监督学习的可靠替代方案，并且可以降低数据标签的时间和成本。<details>
<summary>Abstract</summary>
This study explores the application of self-supervised learning (SSL) for improved target recognition in synthetic aperture sonar (SAS) imagery. The unique challenges of underwater environments make traditional computer vision techniques, which rely heavily on optical camera imagery, less effective. SAS, with its ability to generate high-resolution imagery, emerges as a preferred choice for underwater imaging. However, the voluminous high-resolution SAS data presents a significant challenge for labeling; a crucial step for training deep neural networks (DNNs).   SSL, which enables models to learn features in data without the need for labels, is proposed as a potential solution to the data labeling challenge in SAS. The study evaluates the performance of two prominent SSL algorithms, MoCov2 and BYOL, against the well-regarded supervised learning model, ResNet18, for binary image classification tasks. The findings suggest that while both SSL models can outperform a fully supervised model with access to a small number of labels in a few-shot scenario, they do not exceed it when all the labels are used.   The results underscore the potential of SSL as a viable alternative to traditional supervised learning, capable of maintaining task performance while reducing the time and costs associated with data labeling. The study also contributes to the growing body of evidence supporting the use of SSL in remote sensing and could stimulate further research in this area.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Cascaded-Cross-Modal-Transformer-for-Request-and-Complaint-Detection"><a href="#Cascaded-Cross-Modal-Transformer-for-Request-and-Complaint-Detection" class="headerlink" title="Cascaded Cross-Modal Transformer for Request and Complaint Detection"></a>Cascaded Cross-Modal Transformer for Request and Complaint Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15097">http://arxiv.org/abs/2307.15097</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ristea/ccmt">https://github.com/ristea/ccmt</a></li>
<li>paper_authors: Nicolae-Catalin Ristea, Radu Tudor Ionescu</li>
<li>for: 检测客户请求和投诉在电话对话中</li>
<li>methods:  combine speech和文本译本使用自动语音识别（ASR）模型和不同语言BERT基于模型，并使用wave2vec2.0音频特征</li>
<li>results: 在ACM Multimedia 2023 Computational Paralinguistics Challenge的请求子挑战中，我们的系统达到了不Weighted average recall（UAR）的65.41%和85.87%，分别对应于投诉和请求类别。<details>
<summary>Abstract</summary>
We propose a novel cascaded cross-modal transformer (CCMT) that combines speech and text transcripts to detect customer requests and complaints in phone conversations. Our approach leverages a multimodal paradigm by transcribing the speech using automatic speech recognition (ASR) models and translating the transcripts into different languages. Subsequently, we combine language-specific BERT-based models with Wav2Vec2.0 audio features in a novel cascaded cross-attention transformer model. We apply our system to the Requests Sub-Challenge of the ACM Multimedia 2023 Computational Paralinguistics Challenge, reaching unweighted average recalls (UAR) of 65.41% and 85.87% for the complaint and request classes, respectively.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的层次跨模态转换器（CCMT），该模型结合语音和文本译文来检测电话对话中的客户请求和投诉。我们的方法采用多模式观念，通过自动语音识别（ASR）模型将语音转录为不同语言的文本，然后将语言特定的 BERT 基于模型与 Wav2Vec2.0 音频特征在一种新的层次跨注意力转换器模型中结合。我们对 ACM Multimedia 2023 计算语言学挑战的请求子挑战问题进行应用，实现无权重平均回归率（UAR）为 65.41% 和 85.87%  для投诉和请求类别 respectively。
</details></li>
</ul>
<hr>
<h2 id="Generative-convective-parametrization-of-dry-atmospheric-boundary-layer"><a href="#Generative-convective-parametrization-of-dry-atmospheric-boundary-layer" class="headerlink" title="Generative convective parametrization of dry atmospheric boundary layer"></a>Generative convective parametrization of dry atmospheric boundary layer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14857">http://arxiv.org/abs/2307.14857</a></li>
<li>repo_url: None</li>
<li>paper_authors: Florian Heyder, Juan Pedro Mellado, Jörg Schumacher</li>
<li>for: 这个论文的目的是为了开发一种基于生成 adversarial network的湍流 parametrization，用于 kilometer-scale Earth system models。</li>
<li>methods: 这个论文使用了生成机器学习算法，基于 direct numerical simulation 数据进行训练。</li>
<li>results: 这个模型能够预测湍流场的非均匀统计特征，包括气压波的强度和高度分布，以及气压波的横向组织结构。模型的预测结果与标准两方程或多气流扩展模型相符。<details>
<summary>Abstract</summary>
Turbulence parametrizations will remain a necessary building block in kilometer-scale Earth system models. In convective boundary layers, where the mean vertical gradients of conserved properties such as potential temperature and moisture are approximately zero, the standard ansatz which relates turbulent fluxes to mean vertical gradients via an eddy diffusivity has to be extended by mass flux parametrizations for the typically asymmetric up- and downdrafts in the atmospheric boundary layer. In this work, we present a parametrization for a dry convective boundary layer based on a generative adversarial network. The model incorporates the physics of self-similar layer growth following from the classical mixed layer theory by Deardorff. This enhances the training data base of the generative machine learning algorithm and thus significantly improves the predicted statistics of the synthetically generated turbulence fields at different heights inside the boundary layer. The algorithm training is based on fully three-dimensional direct numerical simulation data. Differently to stochastic parametrizations, our model is able to predict the highly non-Gaussian transient statistics of buoyancy fluctuations, vertical velocity, and buoyancy flux at different heights thus also capturing the fastest thermals penetrating into the stabilized top region. The results of our generative algorithm agree with standard two-equation or multi-plume stochastic mass-flux schemes. The present parametrization provides additionally the granule-type horizontal organization of the turbulent convection which cannot be obtained in any of the other model closures. Our work paves the way to efficient data-driven convective parametrizations in other natural flows, such as moist convection, upper ocean mixing, or convection in stellar interiors.
</details>
<details>
<summary>摘要</summary>
“湍流 parametrizations 将继续作为 Earth 系统模型中必需的构建块。在湍流边层中， где保守量的平均垂直梯度近乎为零，标准推理，将湍流 fluxes 关联到平均垂直梯度via 湍流散度，需要进一步扩展为质量流 parametrizations。在这个工作中，我们提出了一种基于生成对抗网络的湍流 parametrization。该模型包括自similar层成长的物理学，这将提高训练数据集的生成机器学习算法，并因此改善预测在不同高度内边层的湍流场的统计。我们的模型训练基于三维直接数值计算数据。与抽象参量化不同，我们的模型能够预测湍流场的非高斯散度特征，包括垂直速度、湍流 flux 和湍流能量的快速变化。我们的结果与标准两方程或多柱抽象液体散度模型相符。我们的 parametrization 提供了扩展的 granule 类型的湍流组织，这不能在其他任何模型 closure 中获得。我们的工作开创了数据驱动的湍流 parametrization 的可能性，可以应用于其他自然流体中，如湿气湍流、上层水温混合或星系内部湍流。”
</details></li>
</ul>
<hr>
<h2 id="Counterfactual-Explanations-for-Graph-Classification-Through-the-Lenses-of-Density"><a href="#Counterfactual-Explanations-for-Graph-Classification-Through-the-Lenses-of-Density" class="headerlink" title="Counterfactual Explanations for Graph Classification Through the Lenses of Density"></a>Counterfactual Explanations for Graph Classification Through the Lenses of Density</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14849">http://arxiv.org/abs/2307.14849</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/carlo-abrate/Counterfactual-Explanations-for-Graph-Classification-Through-the-Lenses-of-Density">https://github.com/carlo-abrate/Counterfactual-Explanations-for-Graph-Classification-Through-the-Lenses-of-Density</a></li>
<li>paper_authors: Carlo Abrate, Giulia Preti, Francesco Bonchi</li>
<li>for: 这paper主要用于提出一种基于浓度的对比例例行对图像分类器的解释方法，以便更好地理解图像分类器的决策过程。</li>
<li>methods: 该paper使用了一种通过调整图像中的稠密结构来生成对比例例行的图像，包括打开或关闭三角形、以及基于最大 clique的方法。</li>
<li>results: 实验结果表明，采用浓度作为对比例例行的单位可以生成更加灵活和可读的解释方法，并且可以在7个大脑网络数据集上证明这种方法的有效性。<details>
<summary>Abstract</summary>
Counterfactual examples have emerged as an effective approach to produce simple and understandable post-hoc explanations. In the context of graph classification, previous work has focused on generating counterfactual explanations by manipulating the most elementary units of a graph, i.e., removing an existing edge, or adding a non-existing one. In this paper, we claim that such language of explanation might be too fine-grained, and turn our attention to some of the main characterizing features of real-world complex networks, such as the tendency to close triangles, the existence of recurring motifs, and the organization into dense modules. We thus define a general density-based counterfactual search framework to generate instance-level counterfactual explanations for graph classifiers, which can be instantiated with different notions of dense substructures. In particular, we show two specific instantiations of this general framework: a method that searches for counterfactual graphs by opening or closing triangles, and a method driven by maximal cliques. We also discuss how the general method can be instantiated to exploit any other notion of dense substructures, including, for instance, a given taxonomy of nodes. We evaluate the effectiveness of our approaches in 7 brain network datasets and compare the counterfactual statements generated according to several widely-used metrics. Results confirm that adopting a semantic-relevant unit of change like density is essential to define versatile and interpretable counterfactual explanation methods.
</details>
<details>
<summary>摘要</summary>
counterfactual 例子在 graph classification 中 emerged as an effective approach to produce simple and understandable post-hoc explanations. In the context of graph classification, previous work has focused on generating counterfactual explanations by manipulating the most elementary units of a graph, such as removing an existing edge, or adding a non-existing one. In this paper, we claim that such language of explanation might be too fine-grained, and turn our attention to some of the main characterizing features of real-world complex networks, such as the tendency to close triangles, the existence of recurring motifs, and the organization into dense modules. We thus define a general density-based counterfactual search framework to generate instance-level counterfactual explanations for graph classifiers, which can be instantiated with different notions of dense substructures. In particular, we show two specific instantiations of this general framework: a method that searches for counterfactual graphs by opening or closing triangles, and a method driven by maximal cliques. We also discuss how the general method can be instantiated to exploit any other notion of dense substructures, including, for instance, a given taxonomy of nodes. We evaluate the effectiveness of our approaches in 7 brain network datasets and compare the counterfactual statements generated according to several widely-used metrics. Results confirm that adopting a semantic-relevant unit of change like density is essential to define versatile and interpretable counterfactual explanation methods.
</details></li>
</ul>
<hr>
<h2 id="Kernelised-Normalising-Flows"><a href="#Kernelised-Normalising-Flows" class="headerlink" title="Kernelised Normalising Flows"></a>Kernelised Normalising Flows</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14839">http://arxiv.org/abs/2307.14839</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eshant English, Matthias Kirchler, Christoph Lippert</li>
<li>for: 本研究旨在提出一种基于kernels的normalizing flows模型，以提高模型表达力和数据效应性。</li>
<li>methods: 该模型采用了kernel化的方法，将传统的神经网络变换替换为kernel化的变换，以提高模型的表达能力和参数效率。</li>
<li>results: 实验结果表明，kernel化的normalizing flows模型可以与基于神经网络的模型相比，在低数据量 régime下具有竞争力或更高的表达能力，同时具有更好的数据效应性。<details>
<summary>Abstract</summary>
Normalising Flows are generative models characterised by their invertible architecture. However, the requirement of invertibility imposes constraints on their expressiveness, necessitating a large number of parameters and innovative architectural designs to achieve satisfactory outcomes. Whilst flow-based models predominantly rely on neural-network-based transformations for expressive designs, alternative transformation methods have received limited attention. In this work, we present Ferumal flow, a novel kernelised normalising flow paradigm that integrates kernels into the framework. Our results demonstrate that a kernelised flow can yield competitive or superior results compared to neural network-based flows whilst maintaining parameter efficiency. Kernelised flows excel especially in the low-data regime, enabling flexible non-parametric density estimation in applications with sparse data availability.
</details>
<details>
<summary>摘要</summary>
通常的流程模型具有可逆的架构，但是这种要求对表达能力带来限制，因此需要许多参数和创新的架构来实现可Acceptable的结果。而流程模型主要依靠神经网络变换来实现表达设计，而其他变换方法受到了有限的关注。在这项工作中，我们介绍了 Ferumal flow，一种新的内核化正常流程模型，它将内核 integrate into the framework。我们的结果表明，内核化流可以与神经网络基于的流相比，在参数效率方面具有竞争力，并且在数据稀缺情况下表现特别出色。
</details></li>
</ul>
<hr>
<h2 id="Building-RadiologyNET-Unsupervised-annotation-of-a-large-scale-multimodal-medical-database"><a href="#Building-RadiologyNET-Unsupervised-annotation-of-a-large-scale-multimodal-medical-database" class="headerlink" title="Building RadiologyNET: Unsupervised annotation of a large-scale multimodal medical database"></a>Building RadiologyNET: Unsupervised annotation of a large-scale multimodal medical database</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08517">http://arxiv.org/abs/2308.08517</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mateja Napravnik, Franko Hržić, Sebastian Tschauner, Ivan Štajduhar<br>for: This paper aims to address the challenge of annotating large medical radiology image databases by proposing an automated, unsupervised approach for clustering images based on their semantic similarity.methods: The proposed approach uses a combination of feature extractors from multiple data sources, including images, DICOM metadata, and narrative diagnoses. The features are then integrated into a multimodal representation and clustered using k-means and k-medoids algorithms.results: The results show that fusing the embeddings of all three data sources together results in the most concise clusters, indicating that this approach is effective in unsupervised clustering of large-scale medical data. The proposed method has the potential to create a much larger and more fine-grained annotated dataset of medical radiology images.<details>
<summary>Abstract</summary>
Background and objective: The usage of machine learning in medical diagnosis and treatment has witnessed significant growth in recent years through the development of computer-aided diagnosis systems that are often relying on annotated medical radiology images. However, the availability of large annotated image datasets remains a major obstacle since the process of annotation is time-consuming and costly. This paper explores how to automatically annotate a database of medical radiology images with regard to their semantic similarity.   Material and methods: An automated, unsupervised approach is used to construct a large annotated dataset of medical radiology images originating from Clinical Hospital Centre Rijeka, Croatia, utilising multimodal sources, including images, DICOM metadata, and narrative diagnoses. Several appropriate feature extractors are tested for each of the data sources, and their utility is evaluated using k-means and k-medoids clustering on a representative data subset.   Results: The optimal feature extractors are then integrated into a multimodal representation, which is then clustered to create an automated pipeline for labelling a precursor dataset of 1,337,926 medical images into 50 clusters of visually similar images. The quality of the clusters is assessed by examining their homogeneity and mutual information, taking into account the anatomical region and modality representation.   Conclusion: The results suggest that fusing the embeddings of all three data sources together works best for the task of unsupervised clustering of large-scale medical data, resulting in the most concise clusters. Hence, this work is the first step towards building a much larger and more fine-grained annotated dataset of medical radiology images.
</details>
<details>
<summary>摘要</summary>
背景和目标：随着医疗机器学习技术的发展，医疗诊断和治疗中的计算机支持诊断系统得到了广泛应用，但大量注释医疗影像数据集的可 availability 仍然是一个主要障碍。这篇论文探讨如何自动注释医疗影像数据库中的semantic similarity。材料和方法：使用自动化、无监督的方法，使用来自克林尼克医院中心的医疗影像数据，包括图像、DICOM元数据和描述诊断。对每种数据源，选择合适的特征提取器，并对选择的数据子集进行k-means和k-medoids归一化 clustering。结果：最佳特征提取器被集成到多模式表示中，并对 precursor 数据集进行自动化标注，将1,337,926个医疗影像分为50个视觉相似的集群。评估结果表明，将所有数据源的嵌入都 fusion 起来是最佳的选择，可以获得最紧凑的集群。因此，这是建立更大和更细化的注释医疗影像数据集的第一步。
</details></li>
</ul>
<hr>
<h2 id="Fading-memory-as-inductive-bias-in-residual-recurrent-networks"><a href="#Fading-memory-as-inductive-bias-in-residual-recurrent-networks" class="headerlink" title="Fading memory as inductive bias in residual recurrent networks"></a>Fading memory as inductive bias in residual recurrent networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14823">http://arxiv.org/abs/2307.14823</a></li>
<li>repo_url: None</li>
<li>paper_authors: Igor Dubinin, Felix Effenberger</li>
<li>for: 这 paper 探讨了 residual connections 在 RNN 中的影响，以提高 task 性能和网络动态特性。</li>
<li>methods: 作者使用了 weakly coupled residual recurrent networks (WCRNNs)，并 investigate 了 residual connections 对网络性能、动态特性和记忆特性的影响。</li>
<li>results: 研究发现，不同类型的 residual connections 可以提供不同的 inductive bias，提高网络表达能力。 especailly, residual connections 可以使网络在静止边缘附近的动态特性， capitalize 数据特征的spectral properties，以及实现异质记忆特性。 更进一步，作者还展示了如何扩展到非线性 residual 和 introducing weakly coupled residual initialization scheme for Elman RNNs.<details>
<summary>Abstract</summary>
Residual connections have been proposed as architecture-based inductive bias to mitigate the problem of exploding and vanishing gradients and increase task performance in both feed-forward and recurrent networks (RNNs) when trained with the backpropagation algorithm. Yet, little is known about how residual connections in RNNs influence their dynamics and fading memory properties. Here, we introduce weakly coupled residual recurrent networks (WCRNNs) in which residual connections result in well-defined Lyapunov exponents and allow for studying properties of fading memory. We investigate how the residual connections of WCRNNs influence their performance, network dynamics, and memory properties on a set of benchmark tasks. We show that several distinct forms of residual connections yield effective inductive biases that result in increased network expressivity. In particular, residual connections that (i) result in network dynamics at the proximity of the edge of chaos, (ii) allow networks to capitalize on characteristic spectral properties of the data, and (iii) result in heterogeneous memory properties are shown to increase practical expressivity. In addition, we demonstrate how our results can be extended to non-linear residuals and introduce a weakly coupled residual initialization scheme that can be used for Elman RNNs
</details>
<details>
<summary>摘要</summary>
剩下的连接（residual connections）已经被提议为网络架构层基的偏好，以降低反向传播算法中的扩散和消失梯度问题，并提高任务性能。然而，关于具有剩下的连接的RNN（Recurrent Neural Network）的动态和忘记性特性所知之少。在这里，我们介绍了弱相互连接的剩下RNN（Weakly Coupled Residual Recurrent Network，WCRNN），其中剩下的连接导致明确的Lyapunov exponent，并使得研究RNN的忘记性特性变得可行。我们 investigate了WCRNN中剩下连接的影响，包括网络性能、网络动态和忘记性特性在内的几个任务 benchmark。我们发现，不同的剩下连接形式可以提供不同的偏好，以提高网络表达能力。具体来说，剩下连接可以：（i）导致网络动态在边缘附近，（ii）让网络利用数据的特征频率特性，以及（iii）使得网络具有不同的忘记性特性。此外，我们还证明了我们的结果可以扩展到非线性剩下连接，并提出了一种弱相互连接初始化方案，可以应用于Elman RNN。
</details></li>
</ul>
<hr>
<h2 id="Likely-Light-and-Accurate-Context-Free-Clusters-based-Trajectory-Prediction"><a href="#Likely-Light-and-Accurate-Context-Free-Clusters-based-Trajectory-Prediction" class="headerlink" title="Likely, Light, and Accurate Context-Free Clusters-based Trajectory Prediction"></a>Likely, Light, and Accurate Context-Free Clusters-based Trajectory Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14788">http://arxiv.org/abs/2307.14788</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tiago Rodrigues de Almeida, Oscar Martinez Mozos</li>
<li>for: 预测道路交通网络中自主系统的未来路径，以适应未知性。</li>
<li>methods: 提出了一种多stage probabilistic方法，包括路径变换到位差空间、时间序列归一化、路径提议和排名提议。新引入了深度特征归一化方法，自适应GAN，可以更好地适应分布变化。</li>
<li>results: 对人行道和道路代理人 trajectory 数据进行比较，全系统超过了上下文深度生成模型，同时与点估计模型相当，可以准确预测道路交通网络中自主系统的未来路径。<details>
<summary>Abstract</summary>
Autonomous systems in the road transportation network require intelligent mechanisms that cope with uncertainty to foresee the future. In this paper, we propose a multi-stage probabilistic approach for trajectory forecasting: trajectory transformation to displacement space, clustering of displacement time series, trajectory proposals, and ranking proposals. We introduce a new deep feature clustering method, underlying self-conditioned GAN, which copes better with distribution shifts than traditional methods. Additionally, we propose novel distance-based ranking proposals to assign probabilities to the generated trajectories that are more efficient yet accurate than an auxiliary neural network. The overall system surpasses context-free deep generative models in human and road agents trajectory data while performing similarly to point estimators when comparing the most probable trajectory.
</details>
<details>
<summary>摘要</summary>
自动化系统在路运输网络中需要智能机制来预测未来。本文提出了一种多个阶段 probabilistic 方法 для路径预测：路径变换到位移空间，分聚运动时序序列，路径提议和排名提议。我们引入了一种新的深度特征划分方法，基于自我条件 GAN，可以更好地处理分布转移。此外，我们提出了一种新的距离基于排名提议，用于将生成的路径分配概率，这种方法比 auxiliary 神经网络更高效 yet 准确。总体系统在人员和道路代理 trajectory 数据中超越了上下文深度生成模型，同时与点估计相比，最有可能的路径预测的准确性也有所提高。
</details></li>
</ul>
<hr>
<h2 id="Emotion4MIDI-a-Lyrics-based-Emotion-Labeled-Symbolic-Music-Dataset"><a href="#Emotion4MIDI-a-Lyrics-based-Emotion-Labeled-Symbolic-Music-Dataset" class="headerlink" title="Emotion4MIDI: a Lyrics-based Emotion-Labeled Symbolic Music Dataset"></a>Emotion4MIDI: a Lyrics-based Emotion-Labeled Symbolic Music Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14783">http://arxiv.org/abs/2307.14783</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/serkansulun/lyricsemotions">https://github.com/serkansulun/lyricsemotions</a></li>
<li>paper_authors: Serkan Sulun, Pedro Oliveira, Paula Viana</li>
<li>for: 这个论文是为了创建一个大规模的情感标注的 симвоlic music dataset（12k首midi乐曲）而写的。</li>
<li>methods: 作者首先在GoEmotions dataset上训练了情感分类模型，实现了状态空间最佳的结果，并且使用这些模型对两个大规模的midi dataset中的歌词进行应用。</li>
<li>results: 该dataset覆盖了一系列细化的情感，为研究音乐和情感之间的连接，以及开发基于具体情感的音乐生成模型提供了一个非常有价值的资源。<details>
<summary>Abstract</summary>
We present a new large-scale emotion-labeled symbolic music dataset consisting of 12k MIDI songs. To create this dataset, we first trained emotion classification models on the GoEmotions dataset, achieving state-of-the-art results with a model half the size of the baseline. We then applied these models to lyrics from two large-scale MIDI datasets. Our dataset covers a wide range of fine-grained emotions, providing a valuable resource to explore the connection between music and emotions and, especially, to develop models that can generate music based on specific emotions. Our code for inference, trained models, and datasets are available online.
</details>
<details>
<summary>摘要</summary>
我们介绍了一个新的大规模情感标注的符号音乐数据集，包含12个MIDI歌曲。为创建这个数据集，我们首先在GoEmotions数据集上训练情感分类模型，实现了状态之arte的结果，模型半大小比基线。然后，我们应用了这些模型到两个大规模MIDI数据集中的歌词上。我们的数据集覆盖了各种细化的情感，提供了一个 ценный资源，探索音乐和情感之间的连接，特别是开发根据特定情感生成音乐的模型。我们在线上提供了推理代码、训练模型和数据集。
</details></li>
</ul>
<hr>
<h2 id="MATNilm-Multi-appliance-task-Non-intrusive-Load-Monitoring-with-Limited-Labeled-Data"><a href="#MATNilm-Multi-appliance-task-Non-intrusive-Load-Monitoring-with-Limited-Labeled-Data" class="headerlink" title="MATNilm: Multi-appliance-task Non-intrusive Load Monitoring with Limited Labeled Data"></a>MATNilm: Multi-appliance-task Non-intrusive Load Monitoring with Limited Labeled Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14778">http://arxiv.org/abs/2307.14778</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jxiong22/matnilm">https://github.com/jxiong22/matnilm</a></li>
<li>paper_authors: Jing Xiong, Tianqi Hong, Dongbo Zhao, Yu Zhang</li>
<li>for: 该论文目的是提出一种基于多应用程序任务框架和培根监测的非扰式电力监测（NILM）方法，以提高家庭应用程序的状态和消耗电力的精度和效率。</li>
<li>methods: 该方法使用了一种培根监测框架，其中每个应用程序都有一个共享层次拆分结构，以实现每个应用程序的回归和分类任务。此外，该方法还使用了一种两维注意机制，以捕捉所有应用程序之间的空间时间相关性。</li>
<li>results:  simulation results show that the proposed approach significantly improves the performance of NILM, with relative errors reduced by more than 50% on average. The approach also achieves comparable test performance with only one day of training data and limited appliance operation profiles.<details>
<summary>Abstract</summary>
Non-intrusive load monitoring (NILM) identifies the status and power consumption of various household appliances by disaggregating the total power usage signal of an entire house. Efficient and accurate load monitoring facilitates user profile establishment, intelligent household energy management, and peak load shifting. This is beneficial for both the end-users and utilities by improving the overall efficiency of a power distribution network. Existing approaches mainly focus on developing an individual model for each appliance. Those approaches typically rely on a large amount of household-labeled data which is hard to collect. In this paper, we propose a multi-appliance-task framework with a training-efficient sample augmentation (SA) scheme that boosts the disaggregation performance with limited labeled data. For each appliance, we develop a shared-hierarchical split structure for its regression and classification tasks. In addition, we also propose a two-dimensional attention mechanism in order to capture spatio-temporal correlations among all appliances. With only one-day training data and limited appliance operation profiles, the proposed SA algorithm can achieve comparable test performance to the case of training with the full dataset. Finally, simulation results show that our proposed approach features a significantly improved performance over many baseline models. The relative errors can be reduced by more than 50% on average. The codes of this work are available at https://github.com/jxiong22/MATNilm
</details>
<details>
<summary>摘要</summary>
非侵入式卷积监测（NILM）可以识别家庭各种设备的状态和能量消耗。这样的监测可以帮助建立用户profile，实现智能家庭能源管理和峰值负荷延迟。这对 endpoint 用户和供应商都是有利的，因为它可以提高总能源分配网络的效率。现有的方法主要集中在开发每个设备的个性化模型。这些方法通常需要大量的标注数据，但这些数据很难收集。在这篇论文中，我们提出了一个多设备任务框架，并使用训练效率高的样本扩展（SA）策略来提高分解性能。对于每个设备，我们开发了共享层次分割结构，用于其预测和分类任务。此外，我们还提出了两个维度的注意力机制，以捕捉所有设备之间的空间时间相关性。只需一天的训练数据和有限的设备操作 profiling，我们的SA算法可以达到与整个数据集训练时的比较好的测试性能。最后，我们的实验结果显示，我们的提案的方法在许多基线模型之上显示出了显著的改善。相对误差可以降低超过50%的平均值。代码这个工作可以在 <https://github.com/jxiong22/MATNilm> 查看。
</details></li>
</ul>
<hr>
<h2 id="Towards-Practicable-Sequential-Shift-Detectors"><a href="#Towards-Practicable-Sequential-Shift-Detectors" class="headerlink" title="Towards Practicable Sequential Shift Detectors"></a>Towards Practicable Sequential Shift Detectors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14758">http://arxiv.org/abs/2307.14758</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oliver Cobb, Arnaud Van Looveren</li>
<li>for: 检测机器学习模型中的分布变化，以避免模型性能下降。</li>
<li>methods: existin works relevant to their satisfaction, and recommend impactful directions for future research.</li>
<li>results:  identificaiton of three desiderata crucial to the practicable deployment of sequential shift detectors.<details>
<summary>Abstract</summary>
There is a growing awareness of the harmful effects of distribution shift on the performance of deployed machine learning models. Consequently, there is a growing interest in detecting these shifts before associated costs have time to accumulate. However, desiderata of crucial importance to the practicable deployment of sequential shift detectors are typically overlooked by existing works, precluding their widespread adoption. We identify three such desiderata, highlight existing works relevant to their satisfaction, and recommend impactful directions for future research.
</details>
<details>
<summary>摘要</summary>
有增长的认识到分布转移对已经部署的机器学习模型表现的负面影响。因此，有增长的兴趣检测这些转移，以避免成本累累。然而，现有的工作通常忽视了重要的实用部署顺序转移检测的要求，这使得它们在实际应用中没有得到广泛采用。我们认为有三个如此的要求，提到现有的工作，并建议未来的研究方向。
</details></li>
</ul>
<hr>
<h2 id="Fair-Machine-Unlearning-Data-Removal-while-Mitigating-Disparities"><a href="#Fair-Machine-Unlearning-Data-Removal-while-Mitigating-Disparities" class="headerlink" title="Fair Machine Unlearning: Data Removal while Mitigating Disparities"></a>Fair Machine Unlearning: Data Removal while Mitigating Disparities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14754">http://arxiv.org/abs/2307.14754</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alex Oesterling, Jiaqi Ma, Flavio P. Calmon, Hima Lakkaraju</li>
<li>for: 本研究旨在提供一种可靠地忘记数据实例的机器学习方法，以保障个人隐私和公平性。</li>
<li>methods: 本研究使用了一种新的机器学习方法，可以高效地和可靠地忘记数据实例，同时保持集体公平性。</li>
<li>results: 实验结果表明，本方法可以高效地忘记数据实例，并且保持集体公平性。<details>
<summary>Abstract</summary>
As public consciousness regarding the collection and use of personal information by corporations grows, it is of increasing importance that consumers be active participants in the curation of corporate datasets. In light of this, data governance frameworks such as the General Data Protection Regulation (GDPR) have outlined the right to be forgotten as a key principle allowing individuals to request that their personal data be deleted from the databases and models used by organizations. To achieve forgetting in practice, several machine unlearning methods have been proposed to address the computational inefficiencies of retraining a model from scratch with each unlearning request. While efficient online alternatives to retraining, it is unclear how these methods impact other properties critical to real-world applications, such as fairness. In this work, we propose the first fair machine unlearning method that can provably and efficiently unlearn data instances while preserving group fairness. We derive theoretical results which demonstrate that our method can provably unlearn data instances while maintaining fairness objectives. Extensive experimentation with real-world datasets highlight the efficacy of our method at unlearning data instances while preserving fairness.
</details>
<details>
<summary>摘要</summary>
In this work, we propose the first fair machine unlearning method that can provably and efficiently unlearn data instances while preserving group fairness. We derive theoretical results that demonstrate our method can provably unlearn data instances while maintaining fairness objectives. Extensive experiments with real-world datasets show that our method is effective at unlearning data instances while preserving fairness.
</details></li>
</ul>
<hr>
<h2 id="FLARE-Fingerprinting-Deep-Reinforcement-Learning-Agents-using-Universal-Adversarial-Masks"><a href="#FLARE-Fingerprinting-Deep-Reinforcement-Learning-Agents-using-Universal-Adversarial-Masks" class="headerlink" title="FLARE: Fingerprinting Deep Reinforcement Learning Agents using Universal Adversarial Masks"></a>FLARE: Fingerprinting Deep Reinforcement Learning Agents using Universal Adversarial Masks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14751">http://arxiv.org/abs/2307.14751</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ssg-research/FLARE">https://github.com/ssg-research/FLARE</a></li>
<li>paper_authors: Buse G. A. Tekgul, N. Asokan</li>
<li>for: 防止深度强化学习策略（DRL）的非法复制和使用</li>
<li>methods: 使用非可转移的通用敌意掩蔽（perturbations）生成对抗示例，并将这些掩蔽用作指纹来验证盗取的DRL策略的真实所属</li>
<li>results: FLARE效果很好（100% 动作一致率），不会误告发独立策略（无false positives），并且对模型修改攻击和更有经验的敌对者进行攻击也具有较好的Robustness。<details>
<summary>Abstract</summary>
We propose FLARE, the first fingerprinting mechanism to verify whether a suspected Deep Reinforcement Learning (DRL) policy is an illegitimate copy of another (victim) policy. We first show that it is possible to find non-transferable, universal adversarial masks, i.e., perturbations, to generate adversarial examples that can successfully transfer from a victim policy to its modified versions but not to independently trained policies. FLARE employs these masks as fingerprints to verify the true ownership of stolen DRL policies by measuring an action agreement value over states perturbed via such masks. Our empirical evaluations show that FLARE is effective (100% action agreement on stolen copies) and does not falsely accuse independent policies (no false positives). FLARE is also robust to model modification attacks and cannot be easily evaded by more informed adversaries without negatively impacting agent performance. We also show that not all universal adversarial masks are suitable candidates for fingerprints due to the inherent characteristics of DRL policies. The spatio-temporal dynamics of DRL problems and sequential decision-making process make characterizing the decision boundary of DRL policies more difficult, as well as searching for universal masks that capture the geometry of it.
</details>
<details>
<summary>摘要</summary>
我们提出了FLARE，第一个验证怀疑深度强化学习（DRL）策略是否为另一个（受害者）策略的伪造 Mechanism。我们首先显示了可以找到不可转移的通用敌方攻击库（perturbations），即可以将攻击者从受害者策略中获得独特的攻击例子，但不能获得独立训练的策略中的攻击例子。FLARE使用这些库作为指纹，用于验证伪造的DRL策略的真实所有权。我们的实验评估显示FLARE有100%的动作一致率（action agreement），并不会误判独立的策略（no false positives）。FLARE还是对模型修改攻击和更 Informed 攻击者的攻击而言，不会轻松避免。我们还显示了不同的通用攻击库可能不适合指纹，因为深度强化学习问题的空间时间动态和继续决策过程使得characterizing DRL策略的决策边界更加困难，以及搜寻适合的通用库。
</details></li>
</ul>
<hr>
<h2 id="Semantic-Image-Completion-and-Enhancement-using-GANs"><a href="#Semantic-Image-Completion-and-Enhancement-using-GANs" class="headerlink" title="Semantic Image Completion and Enhancement using GANs"></a>Semantic Image Completion and Enhancement using GANs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14748">http://arxiv.org/abs/2307.14748</a></li>
<li>repo_url: None</li>
<li>paper_authors: Priyansh Saxena, Raahat Gupta, Akshat Maheshwari, Saumil Maheshwari</li>
<li>for: 这篇论文主要用于描述如何使用生成对抗网络（GAN）来实现图像完成和提高 зада务。</li>
<li>methods: 这篇论文使用的方法主要是基于GAN的架构，用于实现图像完成和提高。</li>
<li>results: 这篇论文的研究结果表明，GAN可以有效地完成和提高图像，并且可以保持图像的详细信息。<details>
<summary>Abstract</summary>
Semantic inpainting or image completion alludes to the task of inferring arbitrary large missing regions in images based on image semantics. Since the prediction of image pixels requires an indication of high-level context, this makes it significantly tougher than image completion, which is often more concerned with correcting data corruption and removing entire objects from the input image. On the other hand, image enhancement attempts to eliminate unwanted noise and blur from the image, along with sustaining most of the image details. Efficient image completion and enhancement model should be able to recover the corrupted and masked regions in images and then refine the image further to increase the quality of the output image. Generative Adversarial Networks (GAN), have turned out to be helpful in picture completion tasks. In this chapter, we will discuss the underlying GAN architecture and how they can be used used for image completion tasks.
</details>
<details>
<summary>摘要</summary>
semantic inpainting or image completion 涉及到根据图像 semantics 推断大量缺失区域的图像。由于预测图像像素需要高级上下文指示，这使得其 significatively 更加复杂于图像完成，而图像完成通常更关注于修复数据损害和从输入图像中除去 объек 的。然而，图像提高尝试去除不必要的噪声和模糊，同时保持大部分图像细节。高效的图像完成和提高模型应该能够回复缺失和masked 区域的图像，然后进一步提高输出图像的质量。生成对抗网络（GAN）在图像完成任务中表现出了有利的效果。在这章中，我们将讨论GAN的基本架构和如何使其用于图像完成任务。
</details></li>
</ul>
<hr>
<h2 id="A-Strategic-Framework-for-Optimal-Decisions-in-Football-1-vs-1-Shot-Taking-Situations-An-Integrated-Approach-of-Machine-Learning-Theory-Based-Modeling-and-Game-Theory"><a href="#A-Strategic-Framework-for-Optimal-Decisions-in-Football-1-vs-1-Shot-Taking-Situations-An-Integrated-Approach-of-Machine-Learning-Theory-Based-Modeling-and-Game-Theory" class="headerlink" title="A Strategic Framework for Optimal Decisions in Football 1-vs-1 Shot-Taking Situations: An Integrated Approach of Machine Learning, Theory-Based Modeling, and Game Theory"></a>A Strategic Framework for Optimal Decisions in Football 1-vs-1 Shot-Taking Situations: An Integrated Approach of Machine Learning, Theory-Based Modeling, and Game Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14732">http://arxiv.org/abs/2307.14732</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/calvinyeungck/analyzing-two-agents-interaction-in-football-shot-taking-situations">https://github.com/calvinyeungck/analyzing-two-agents-interaction-in-football-shot-taking-situations</a></li>
<li>paper_authors: Calvin C. K. Yeung, Keisuke Fujii<br>for:这篇论文的目的是分析足球比赛中攻击者和防守者之间的复杂互动，并提供一个数据驱动和理论基础的分析方法。methods:这篇论文使用了游戏理论和机器学习模型来分析攻击者和防守者之间的竞争情况，并提出了一个新的评价指标——预期进球目标概率（xSOT），以评价球员的行为，即使投射不中目标也能够提供有价值的信息。results:经验 validate了这个框架，并与基线和减少模型进行比较。此外，发现xSOT和现有指标之间存在高度的相似性，这表明xSOT提供了有价值的信息。最后，通过对2022年世界杯和2020年欧洲锦标赛的一个投射情况进行分析， illustrate了这个框架的应用。<details>
<summary>Abstract</summary>
Complex interactions between two opposing agents frequently occur in domains of machine learning, game theory, and other application domains. Quantitatively analyzing the strategies involved can provide an objective basis for decision-making. One such critical scenario is shot-taking in football, where decisions, such as whether the attacker should shoot or pass the ball and whether the defender should attempt to block the shot, play a crucial role in the outcome of the game. However, there are currently no effective data-driven and/or theory-based approaches to analyzing such situations. To address this issue, we proposed a novel framework to analyze such scenarios based on game theory, where we estimate the expected payoff with machine learning (ML) models, and additional features for ML models were extracted with a theory-based shot block model. Conventionally, successes or failures (1 or 0) are used as payoffs, while a success shot (goal) is extremely rare in football. Therefore, we proposed the Expected Probability of Shot On Target (xSOT) metric to evaluate players' actions even if the shot results in no goal; this allows for effective differentiation and comparison between different shots and even enables counterfactual shot situation analysis. In our experiments, we have validated the framework by comparing it with baseline and ablated models. Furthermore, we have observed a high correlation between the xSOT and existing metrics. This alignment of information suggests that xSOT provides valuable insights. Lastly, as an illustration, we studied optimal strategies in the World Cup 2022 and analyzed a shot situation in EURO 2020.
</details>
<details>
<summary>摘要</summary>
在机器学习、游戏理论等领域，两个对立代理经常发生复杂的互动。量化分析这些策略可以提供客观的决策基础。一个典型的情况是足球中的射击，决策是否射球或传球，以及是否阻止射击都对游戏的结果产生重要影响。然而，目前没有有效的数据驱动和/或理论基础的方法来分析这些情况。为解决这个问题，我们提出了一种新的分析框架，基于游戏理论，我们使用机器学习（ML）模型来估计射击的预期收益，并从理论基础上提取了适用于ML模型的附加特征。传统上，成功或失败（1或0）被用作奖励，而射击成功（进球）在足球中是非常罕见的。因此，我们提出了射击点对象概率（xSOT）指标，以评估球员的行为，即使射击无法得分，这些指标允许有效地区分和比较不同的射击，甚至允许对射击情况进行对照分析。在我们的实验中，我们 validate了框架，并与基线和减少模型进行比较。此外，我们发现xSOT和现有指标之间存在高度的相关性。这种对应信息表示xSOT提供了有价值的信息。最后，我们以2022年世界杯和2020年欧锦赛作为例子，分析了一个射击情况。
</details></li>
</ul>
<hr>
<h2 id="Understanding-Silent-Failures-in-Medical-Image-Classification"><a href="#Understanding-Silent-Failures-in-Medical-Image-Classification" class="headerlink" title="Understanding Silent Failures in Medical Image Classification"></a>Understanding Silent Failures in Medical Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14729">http://arxiv.org/abs/2307.14729</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/iml-dkfz/sf-visuals">https://github.com/iml-dkfz/sf-visuals</a></li>
<li>paper_authors: Till J. Bungert, Levin Kobelke, Paul F. Jaeger</li>
<li>for: 预防静默失败，以确保医疗应用中的分类系统可靠。</li>
<li>methods: 使用 confidence scoring functions (CSFs) 检测和预防静默失败。</li>
<li>results: none of the benchmarked CSFs can reliably prevent silent failures, indicating a need for a deeper understanding of the root causes of failures in the data.I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
To ensure the reliable use of classification systems in medical applications, it is crucial to prevent silent failures. This can be achieved by either designing classifiers that are robust enough to avoid failures in the first place, or by detecting remaining failures using confidence scoring functions (CSFs). A predominant source of failures in image classification is distribution shifts between training data and deployment data. To understand the current state of silent failure prevention in medical imaging, we conduct the first comprehensive analysis comparing various CSFs in four biomedical tasks and a diverse range of distribution shifts. Based on the result that none of the benchmarked CSFs can reliably prevent silent failures, we conclude that a deeper understanding of the root causes of failures in the data is required. To facilitate this, we introduce SF-Visuals, an interactive analysis tool that uses latent space clustering to visualize shifts and failures. On the basis of various examples, we demonstrate how this tool can help researchers gain insight into the requirements for safe application of classification systems in the medical domain. The open-source benchmark and tool are at: https://github.com/IML-DKFZ/sf-visuals.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:要确保医疗应用中的分类系统可靠使用，避免悬念性失败是非常重要。这可以通过设计更加鲁棒的分类器来避免失败，或者使用信任分数函数（CSF）来检测剩下的失败。图像分类中的主要失败来源之一是在训练数据和部署数据之间的分布shift。为了了解医疗影像中的现状，我们进行了首次全面的分析，比较了各种 CSF 在四种生物医学任务和多样化的分布shift 下的表现。结果显示，none of the benchmarked CSFs 可靠地防止悬念性失败。这表明，更深入了解数据中的失败根源是必要的。为此，我们介绍了 SF-Visuals，一种可互动地分析工具，使用幽默空间划分来visualize  shift 和失败。通过多个示例，我们示出了这个工具如何帮助研究人员了解在医疗领域中安全应用分类系统的需求。开源 benchmark 和工具可以在：https://github.com/IML-DKFZ/sf-visuals 中找到。
</details></li>
</ul>
<hr>
<h2 id="The-Effect-of-Spoken-Language-on-Speech-Enhancement-using-Self-Supervised-Speech-Representation-Loss-Functions"><a href="#The-Effect-of-Spoken-Language-on-Speech-Enhancement-using-Self-Supervised-Speech-Representation-Loss-Functions" class="headerlink" title="The Effect of Spoken Language on Speech Enhancement using Self-Supervised Speech Representation Loss Functions"></a>The Effect of Spoken Language on Speech Enhancement using Self-Supervised Speech Representation Loss Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14502">http://arxiv.org/abs/2307.14502</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/leto19/commonvoice-demand">https://github.com/leto19/commonvoice-demand</a></li>
<li>paper_authors: George Close, Thomas Hain, Stefan Goetze</li>
<li>for: 这项研究旨在探讨自动提高抖音（SE）系统中使用自动提高抖音（SSSR）作为特征变换在损失函数中的效果。</li>
<li>methods: 本研究使用了不同语言组合和网络结构的自动提高抖音（SSSR）来训练和测试SE系统。</li>
<li>results: 研究发现，训练自动提高抖音的语言对提高性能的影响较小，但是训练数据量的影响很大。<details>
<summary>Abstract</summary>
Recent work in the field of speech enhancement (SE) has involved the use of self-supervised speech representations (SSSRs) as feature transformations in loss functions. However, in prior work, very little attention has been paid to the relationship between the language of the audio used to train the self-supervised representation and that used to train the SE system. Enhancement models trained using a loss function which incorporates a self-supervised representation that shares exactly the language of the noisy data used to train the SE system show better performance than those which do not match exactly. This may lead to enhancement systems which are language specific and as such do not generalise well to unseen languages, unlike models trained using traditional spectrogram or time domain loss functions. In this work, SE models are trained and tested on a number of different languages, with self-supervised representations which themselves are trained using different language combinations and with differing network structures as loss function representations. These models are then tested across unseen languages and their performances are analysed. It is found that the training language of the self-supervised representation appears to have a minor effect on enhancement performance, the amount of training data of a particular language, however, greatly affects performance.
</details>
<details>
<summary>摘要</summary>
最近在语音增强（SE）领域的研究中，有使用自然语言自我监督语音表示（SSSRs）作为损失函数中的特征变换。然而，在先前的工作中，对于使用自然语言来训练自我监督表示和语音增强系统之间的关系，几乎没有关注。在不匹配的语言条件下训练语音增强系统时，使用具有相同语言的自我监督表示可能会导致更好的性能，而不匹配的语言可能会导致语音增强系统不易泛化到未看过的语言。在这项工作中，我们训练和测试了多种不同语言的语音增强模型，使用不同的语言组合和网络结构作为损失函数表示。这些模型在未看过的语言上进行测试，其性能分析结果表明，训练语音增强模型的语言对性能的影响相对较小，但是训练数据的量对性能的影响很大。
</details></li>
</ul>
<hr>
<h2 id="Robust-vertebra-identification-using-simultaneous-node-and-edge-predicting-Graph-Neural-Networks"><a href="#Robust-vertebra-identification-using-simultaneous-node-and-edge-predicting-Graph-Neural-Networks" class="headerlink" title="Robust vertebra identification using simultaneous node and edge predicting Graph Neural Networks"></a>Robust vertebra identification using simultaneous node and edge predicting Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02509">http://arxiv.org/abs/2308.02509</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/imfusiongmbh/vid-vertebra-identification-dataset">https://github.com/imfusiongmbh/vid-vertebra-identification-dataset</a></li>
<li>paper_authors: Vincent Bürgin, Raphael Prevost, Marijn F. Stollenga</li>
<li>for: 验证 Automatic vertebra localization and identification in CT scans 的重要性，并提出一种简单的ipeline来实现这一目标。</li>
<li>methods: 使用 U-Net 预测 vertebra 的位置和orientation，并使用单个图像神经网络来关联和分类 vertebra。</li>
<li>results: 方法可以准确地关联正确的体部和肋骨特征点，忽略假阳性结果，并在标准 VerSe 挑战任务中表现竞争力。<details>
<summary>Abstract</summary>
Automatic vertebra localization and identification in CT scans is important for numerous clinical applications. Much progress has been made on this topic, but it mostly targets positional localization of vertebrae, ignoring their orientation. Additionally, most methods employ heuristics in their pipeline that can be sensitive in real clinical images which tend to contain abnormalities. We introduce a simple pipeline that employs a standard prediction with a U-Net, followed by a single graph neural network to associate and classify vertebrae with full orientation. To test our method, we introduce a new vertebra dataset that also contains pedicle detections that are associated with vertebra bodies, creating a more challenging landmark prediction, association and classification task. Our method is able to accurately associate the correct body and pedicle landmarks, ignore false positives and classify vertebrae in a simple, fully trainable pipeline avoiding application-specific heuristics. We show our method outperforms traditional approaches such as Hungarian Matching and Hidden Markov Models. We also show competitive performance on the standard VerSe challenge body identification task.
</details>
<details>
<summary>摘要</summary>
自动骨vertebra位置和识别在CT扫描图中是许多临床应用的重要任务。许多研究已经进行了这方面的进步，但大多数方法都忽略了骨vertebra的方向。此外，大多数方法还使用了一些规则来处理实际的临床图像，这些图像通常含有异常。我们提出了一个简单的管道，其中使用标准预测和U-Net，然后使用单个图гра树神经网络来关联和分类骨vertebra。为测试我们的方法，我们提出了一个新的骨vertebra数据集，该数据集还包含了骨体的找到和关联。我们的方法能够准确地关联正确的骨体和骨脊的标记，忽略假阳性和分类骨vertebra。我们显示我们的方法超过传统的方法，如匈牙利匹配和隐马尔可夫模型。我们还显示我们的方法在标准VerSe挑战体部识别任务中 exhibits 竞争性的表现。
</details></li>
</ul>
<hr>
<h2 id="TimeGNN-Temporal-Dynamic-Graph-Learning-for-Time-Series-Forecasting"><a href="#TimeGNN-Temporal-Dynamic-Graph-Learning-for-Time-Series-Forecasting" class="headerlink" title="TimeGNN: Temporal Dynamic Graph Learning for Time Series Forecasting"></a>TimeGNN: Temporal Dynamic Graph Learning for Time Series Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14680">http://arxiv.org/abs/2307.14680</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nancy Xu, Chrysoula Kosma, Michalis Vazirgiannis</li>
<li>for: 预测时间序列数据，帮助解决多种科学和工程领域中的重要应用问题。</li>
<li>methods: 使用图神经网络方法，同时学习图structure和时间序列数据的 correlations，以便更好地预测时间序列。</li>
<li>results: 相比其他状态对比方法，时间GNNSince achieves 4-80倍快于其他状态对比方法，而且预测性能相似。<details>
<summary>Abstract</summary>
Time series forecasting lies at the core of important real-world applications in many fields of science and engineering. The abundance of large time series datasets that consist of complex patterns and long-term dependencies has led to the development of various neural network architectures. Graph neural network approaches, which jointly learn a graph structure based on the correlation of raw values of multivariate time series while forecasting, have recently seen great success. However, such solutions are often costly to train and difficult to scale. In this paper, we propose TimeGNN, a method that learns dynamic temporal graph representations that can capture the evolution of inter-series patterns along with the correlations of multiple series. TimeGNN achieves inference times 4 to 80 times faster than other state-of-the-art graph-based methods while achieving comparable forecasting performance
</details>
<details>
<summary>摘要</summary>
时序序列预测在许多科学和工程领域的实际应用中具有核心地位。由于大量的时序序列数据集中含有复杂的模式和长期关系，因此引发了许多神经网络架构的发展。图神经网络方法，它同时学习基于时序序列值的相关性建立图structure，在预测时已经取得了很大成功。然而，这些解决方案经常具有高成本和难以扩展的缺点。在本文中，我们提出了TimeGNN方法，它可以在实时预测过程中学习动态的时间序列图表示，同时捕捉多个序列之间的演变和相关性。TimeGNN在对其他状态艺术图法进行比较时，实现了4-80倍 быстре的推理速度，并具有相似的预测性能。
</details></li>
</ul>
<hr>
<h2 id="Prediction-of-wind-turbines-power-with-physics-informed-neural-networks-and-evidential-uncertainty-quantification"><a href="#Prediction-of-wind-turbines-power-with-physics-informed-neural-networks-and-evidential-uncertainty-quantification" class="headerlink" title="Prediction of wind turbines power with physics-informed neural networks and evidential uncertainty quantification"></a>Prediction of wind turbines power with physics-informed neural networks and evidential uncertainty quantification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14675">http://arxiv.org/abs/2307.14675</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alfonso Gijón, Ainhoa Pujana-Goitia, Eugenio Perea, Miguel Molina-Solana, Juan Gómez-Romero</li>
<li>for: 这个研究的目的是优化风力机操作和维护，通过早期缺陷探测和精准预测风机发电功率。</li>
<li>methods: 这个研究使用数据驱动方法，使用物理约束的启发式神经网络来复制历史数据，并提供了对输出变量的高度准确预测。</li>
<li>results: 研究结果表明，使用物理约束的启发式神经网络可以高度准确地预测风机发电功率、扭矩和功率系数，并且可以提供对预测结果的不确定性的估计。<details>
<summary>Abstract</summary>
The ever-growing use of wind energy makes necessary the optimization of turbine operations through pitch angle controllers and their maintenance with early fault detection. It is crucial to have accurate and robust models imitating the behavior of wind turbines, especially to predict the generated power as a function of the wind speed. Existing empirical and physics-based models have limitations in capturing the complex relations between the input variables and the power, aggravated by wind variability. Data-driven methods offer new opportunities to enhance wind turbine modeling of large datasets by improving accuracy and efficiency. In this study, we used physics-informed neural networks to reproduce historical data coming from 4 turbines in a wind farm, while imposing certain physical constraints to the model. The developed models for regression of the power, torque, and power coefficient as output variables showed great accuracy for both real data and physical equations governing the system. Lastly, introducing an efficient evidential layer provided uncertainty estimations of the predictions, proved to be consistent with the absolute error, and made possible the definition of a confidence interval in the power curve.
</details>
<details>
<summary>摘要</summary>
随着风能的不断发展，风机操作的优化变得必要，特别是通过扭角控制器和其维护，以早期发现FAULT。准确和可靠的风机模型对预测风速的输出功率具有关键性，但现有的empirical和物理基础模型具有限制，尤其是在风速变化的情况下。数据驱动方法可以提高风机模型的准确率和效率，并且可以补做现有模型的缺陷。在这个研究中，我们使用物理知识束缚神经网络来复制历史数据，并对输出变量（功率、扭矩和功率系数）进行回归。我们发现，这些模型具有很高的准确率，并且可以准确地预测风机的输出。最后，我们引入了一个高效的证据层，以获得预测结果的不确定性评估，并证明了其与绝对误差之间的一致。这种方法可以为风机模型的建立提供一个信度评估。
</details></li>
</ul>
<hr>
<h2 id="Bipartite-Ranking-Fairness-through-a-Model-Agnostic-Ordering-Adjustment"><a href="#Bipartite-Ranking-Fairness-through-a-Model-Agnostic-Ordering-Adjustment" class="headerlink" title="Bipartite Ranking Fairness through a Model Agnostic Ordering Adjustment"></a>Bipartite Ranking Fairness through a Model Agnostic Ordering Adjustment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14668">http://arxiv.org/abs/2307.14668</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cuis15/xorder">https://github.com/cuis15/xorder</a></li>
<li>paper_authors: Sen Cui, Weishen Pan, Changshui Zhang, Fei Wang</li>
<li>for: 本文关注在两类样本（正确和错误）的协同排序enario中，学习一个排序函数，以便正确类样本高于错误类样本。</li>
<li>methods: 我们提出了一种模型不含的后处理框架xOrder，以实现在协同排序中保持算法排序性能和公平性。我们优化了一个权重和untility为定义最佳折叠路径，并使用动态编程过程解决。xOrder可以与不同的分类模型和公平度量 метрик相容，包括supervised和Unsupervised公平度量。</li>
<li>results: 我们在四个 benchmark 数据集和两个实际世界病人电子医疗记录库中评估了我们的提议算法。xOrder在不同的数据集和度量上具有一个更好的平衡，在不同的分组下MITIGATEscore分布的变化。此外，我们还提供了一些针对性的分析结果，表明xOrder在小样本和大分布分类分数的情况下具有Robust性。<details>
<summary>Abstract</summary>
Algorithmic fairness has been a serious concern and received lots of interest in machine learning community. In this paper, we focus on the bipartite ranking scenario, where the instances come from either the positive or negative class and the goal is to learn a ranking function that ranks positive instances higher than negative ones. While there could be a trade-off between fairness and performance, we propose a model agnostic post-processing framework xOrder for achieving fairness in bipartite ranking and maintaining the algorithm classification performance. In particular, we optimize a weighted sum of the utility as identifying an optimal warping path across different protected groups and solve it through a dynamic programming process. xOrder is compatible with various classification models and ranking fairness metrics, including supervised and unsupervised fairness metrics. In addition to binary groups, xOrder can be applied to multiple protected groups. We evaluate our proposed algorithm on four benchmark data sets and two real-world patient electronic health record repositories. xOrder consistently achieves a better balance between the algorithm utility and ranking fairness on a variety of datasets with different metrics. From the visualization of the calibrated ranking scores, xOrder mitigates the score distribution shifts of different groups compared with baselines. Moreover, additional analytical results verify that xOrder achieves a robust performance when faced with fewer samples and a bigger difference between training and testing ranking score distributions.
</details>
<details>
<summary>摘要</summary>
《算法公平性在机器学习社区中已经引起了很多关注。在这篇论文中，我们关注了二分类排名场景， instances 来自正确或错误类别，并且目标是学习一个排名函数，将正确类别的 instances 高于错误类别的 instances。尽管存在性能和公平性之间的交易，我们提出了一种模型无关的后处理框架 xOrder，以实现在二分类排名中保持算法分类性能的同时保证公平性。具体来说，我们优化了一个权重和排名公平度之间的平衡，通过动态规划过程解决。xOrder 与不同的分类模型和公平度度量相容，并且可以应用于多个保护组。我们在四个基本数据集和两个实际电子医疗纪录库上评估了我们的提议算法。 xOrder 在不同的数据集和度量上均可以寻求一个更好的平衡 между算法实用性和排名公平性。从折衔分配的排名得分视图来看，xOrder 可以减少不同组的分配得分的偏移。此外，额外的分析结果表明，xOrder 在样本数少和测试排名分布与训练排名分布之间的差异较大时表现更加稳定。
</details></li>
</ul>
<hr>
<h2 id="Decoding-the-Secrets-of-Machine-Learning-in-Malware-Classification-A-Deep-Dive-into-Datasets-Feature-Extraction-and-Model-Performance"><a href="#Decoding-the-Secrets-of-Machine-Learning-in-Malware-Classification-A-Deep-Dive-into-Datasets-Feature-Extraction-and-Model-Performance" class="headerlink" title="Decoding the Secrets of Machine Learning in Malware Classification: A Deep Dive into Datasets, Feature Extraction, and Model Performance"></a>Decoding the Secrets of Machine Learning in Malware Classification: A Deep Dive into Datasets, Feature Extraction, and Model Performance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14657">http://arxiv.org/abs/2307.14657</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/eurecom-s3/decodingmlsecretsofwindowsmalwareclassification">https://github.com/eurecom-s3/decodingmlsecretsofwindowsmalwareclassification</a></li>
<li>paper_authors: Savino Dambra, Yufei Han, Simone Aonzo, Platon Kotzias, Antonino Vitale, Juan Caballero, Davide Balzarotti, Leyla Bilge</li>
<li>for: 本研究旨在探讨机器学习模型在恶意软件检测和分类中的关键因素。</li>
<li>methods: 我们使用最新的机器学习模型对恶意软件进行检测和分类，并对收集到的数据进行分类。</li>
<li>results: 我们发现静态特征比动态特征更好地表现，并且将静态和动态特征相结合只有微量提高静态特征的表现。我们还发现packing与分类精度无关，并且在动态特征中缺失行为会严重降低表现。此外，我们发现在不同家族数量的情况下，模型的表现会随着家族数量的增加而变化。最后，我们发现使用 uniform 分布的样本数据可以更好地泛化到未经看过的数据。<details>
<summary>Abstract</summary>
Many studies have proposed machine-learning (ML) models for malware detection and classification, reporting an almost-perfect performance. However, they assemble ground-truth in different ways, use diverse static- and dynamic-analysis techniques for feature extraction, and even differ on what they consider a malware family. As a consequence, our community still lacks an understanding of malware classification results: whether they are tied to the nature and distribution of the collected dataset, to what extent the number of families and samples in the training dataset influence performance, and how well static and dynamic features complement each other.   This work sheds light on those open questions. by investigating the key factors influencing ML-based malware detection and classification. For this, we collect the largest balanced malware dataset so far with 67K samples from 670 families (100 samples each), and train state-of-the-art models for malware detection and family classification using our dataset. Our results reveal that static features perform better than dynamic features, and that combining both only provides marginal improvement over static features. We discover no correlation between packing and classification accuracy, and that missing behaviors in dynamically-extracted features highly penalize their performance. We also demonstrate how a larger number of families to classify make the classification harder, while a higher number of samples per family increases accuracy. Finally, we find that models trained on a uniform distribution of samples per family better generalize on unseen data.
</details>
<details>
<summary>摘要</summary>
To do this, we collected the largest balanced malware dataset to date, consisting of 67,000 samples from 670 families (100 samples each). We then trained state-of-the-art models for malware detection and family classification using our dataset. Our results show that static features perform better than dynamic features, and that combining both only provides marginal improvement over static features. We also found no correlation between packing and classification accuracy, and that missing behaviors in dynamically-extracted features highly penalize their performance.Furthermore, we demonstrated that a larger number of families to classify makes the classification harder, while a higher number of samples per family increases accuracy. Additionally, we found that models trained on a uniform distribution of samples per family better generalize on unseen data.
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-based-Parameter-Sensitivity-of-Regional-Climate-Models-–-A-Case-Study-of-the-WRF-Model-for-Heat-Extremes-over-Southeast-Australia"><a href="#Machine-Learning-based-Parameter-Sensitivity-of-Regional-Climate-Models-–-A-Case-Study-of-the-WRF-Model-for-Heat-Extremes-over-Southeast-Australia" class="headerlink" title="Machine Learning based Parameter Sensitivity of Regional Climate Models – A Case Study of the WRF Model for Heat Extremes over Southeast Australia"></a>Machine Learning based Parameter Sensitivity of Regional Climate Models – A Case Study of the WRF Model for Heat Extremes over Southeast Australia</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14654">http://arxiv.org/abs/2307.14654</a></li>
<li>repo_url: None</li>
<li>paper_authors: P. Jyoteeshkumar Reddy, Sandeep Chinta, Richard Matear, John Taylor, Harish Baki, Marcus Thatcher, Jatin Kala, Jason Sharples</li>
<li>For: This paper aims to investigate the sensitivity of Weather Research and Forecasting (WRF) model parameters to surface meteorological variables during extreme heat events in southeast Australia.* Methods: The paper uses a machine learning (ML) surrogate-based global sensitivity analysis method to identify the sensitivity of 24 adjustable parameters in seven different physics schemes of the WRF model.* Results: The study finds that only three parameters are important for the considered meteorological variables, and these results are consistent for the two different extreme heat events.Here’s the same information in Simplified Chinese text:* For: 这篇论文旨在investigateWRF模型参数对表面地理变量的敏感性，特别是在澳大利亚东南部的极热事件中。* Methods: 这篇论文使用机器学习（ML）Surrogate基于全球敏感分析方法来确定WRF模型参数的敏感性。* Results: 研究发现仅有3个参数对考虑的地理变量具有重要作用，这些结果在两个不同的极热事件中都是一致的。<details>
<summary>Abstract</summary>
Heatwaves and bushfires cause substantial impacts on society and ecosystems across the globe. Accurate information of heat extremes is needed to support the development of actionable mitigation and adaptation strategies. Regional climate models are commonly used to better understand the dynamics of these events. These models have very large input parameter sets, and the parameters within the physics schemes substantially influence the model's performance. However, parameter sensitivity analysis (SA) of regional models for heat extremes is largely unexplored. Here, we focus on the southeast Australian region, one of the global hotspots of heat extremes. In southeast Australia Weather Research and Forecasting (WRF) model is the widely used regional model to simulate extreme weather events across the region. Hence in this study, we focus on the sensitivity of WRF model parameters to surface meteorological variables such as temperature, relative humidity, and wind speed during two extreme heat events over southeast Australia. Due to the presence of multiple parameters and their complex relationship with output variables, a machine learning (ML) surrogate-based global sensitivity analysis method is considered for the SA. The ML surrogate-based Sobol SA is used to identify the sensitivity of 24 adjustable parameters in seven different physics schemes of the WRF model. Results show that out of these 24, only three parameters, namely the scattering tuning parameter, multiplier of saturated soil water content, and profile shape exponent in the momentum diffusivity coefficient, are important for the considered meteorological variables. These SA results are consistent for the two different extreme heat events. Further, we investigated the physical significance of sensitive parameters. This study's results will help in further optimising WRF parameters to improve model simulation.
</details>
<details>
<summary>摘要</summary>
世界各地的热浪和森林火灾会对社会和生态系统造成重大影响。为了开发有效的避免和适应策略，需要更好地了解热极值的情况。区域气象模型通常用于更好地理解这些事件的动力学。这些模型有非常大的输入参数集，并且physics scheme中的参数对模型性能有很大的影响。然而，区域模型参数敏感性分析（SA）对热极值的模型参数的影响还很少研究。本研究在澳大利亚南东部地区进行了研究，这是全球热极值的热点之一。在这个地区，Weather Research and Forecasting（WRF）模型是广泛使用的区域模型，用于模拟极端天气事件。因此，本研究将在WRF模型参数的敏感性分析中强调surface meteorological变量，如温度、相对湿度和风速。为了处理多个参数和它们复杂的关系，本研究采用机器学习（ML）surrogate-based Sobol SA方法进行敏感性分析。结果显示，24个可调参数中，只有三个参数（散射调整参数、满足饱和 soil water content multiplier和profile shape exponent in momentum diffusivity coefficient）对surface meteorological变量具有重要影响。这些SA结果在两个不同的极端热事件中均相互一致。此外，我们还对敏感参数进行了物理意义的调查。本研究的结果将有助于进一步优化WRF参数，以提高模型的预测。
</details></li>
</ul>
<hr>
<h2 id="Speed-Limits-for-Deep-Learning"><a href="#Speed-Limits-for-Deep-Learning" class="headerlink" title="Speed Limits for Deep Learning"></a>Speed Limits for Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14653">http://arxiv.org/abs/2307.14653</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/RishabhP19/Traffic-Surveillance">https://github.com/RishabhP19/Traffic-Surveillance</a></li>
<li>paper_authors: Inbar Seroussi, Alexander A. Alemi, Moritz Helias, Zohar Ringel</li>
<li>for: 本研究探讨了现代神经网络是否可以最优地训练。</li>
<li>methods: 本文使用了最近的热力学进步，将神经网络的初始权重分布与完全训练后的权重分布之间的速度上限确定。</li>
<li>results: 研究发现，对于线性和线性可变的神经网络（如神经汇kernel），在某些可能的尺度下 assumption 下，学习是在尺度上优化的。这些结果与小规模实验表明，权重分布在初始化后不久就会进入一个非优化的短暂阶段，然后是一个更长的优化阶段。<details>
<summary>Abstract</summary>
State-of-the-art neural networks require extreme computational power to train. It is therefore natural to wonder whether they are optimally trained. Here we apply a recent advancement in stochastic thermodynamics which allows bounding the speed at which one can go from the initial weight distribution to the final distribution of the fully trained network, based on the ratio of their Wasserstein-2 distance and the entropy production rate of the dynamical process connecting them. Considering both gradient-flow and Langevin training dynamics, we provide analytical expressions for these speed limits for linear and linearizable neural networks e.g. Neural Tangent Kernel (NTK). Remarkably, given some plausible scaling assumptions on the NTK spectra and spectral decomposition of the labels -- learning is optimal in a scaling sense. Our results are consistent with small-scale experiments with Convolutional Neural Networks (CNNs) and Fully Connected Neural networks (FCNs) on CIFAR-10, showing a short highly non-optimal regime followed by a longer optimal regime.
</details>
<details>
<summary>摘要</summary>
现代神经网络需要极高的计算能力来训练，因此自然会思考是否最优地训练。我们在材料科学中应用最新的温度动力学技术，可以界定从初始权重分布到全部训练后的神经网络权重分布之间的速度上限，基于这两个分布之间的伪拟合距离和动力学过程的热力学生产率。我们考虑了梯度流和拉杆训练动力学，并提供了分析表达式，其中包括线性和线性可变神经网络等例如神经 Tangent Kernel（NTK）。很remarkably，对于一些可能的 NTK  спектrum 和标签的特征分解的假设，我们发现学习是在一定的缩放意义上最优的。我们的结果与小规模的 CIFAR-10 上的 Convolutional Neural Networks（CNNs）和 Fully Connected Neural Networks（FCNs）的实验结果相符，显示一个短暂的非优化期 followed by a longer optimal period。
</details></li>
</ul>
<hr>
<h2 id="Spatial-Frequency-U-Net-for-Denoising-Diffusion-Probabilistic-Models"><a href="#Spatial-Frequency-U-Net-for-Denoising-Diffusion-Probabilistic-Models" class="headerlink" title="Spatial-Frequency U-Net for Denoising Diffusion Probabilistic Models"></a>Spatial-Frequency U-Net for Denoising Diffusion Probabilistic Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14648">http://arxiv.org/abs/2307.14648</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xin Yuan, Linjie Li, Jianfeng Wang, Zhengyuan Yang, Kevin Lin, Zicheng Liu, Lijuan Wang</li>
<li>for: 这个论文是用来研究在波лет空间中使用潮汐传播概率模型 (DDPM) 进行视觉合成。</li>
<li>methods: 这个论文使用了一个新的架构 SFUNet，它特意设计来有效地捕捉波лет变换所表示的图像中的相互联系。在标准的潮汐净化 U-Net 中，我们增加了2D潮汐条件和频率对应层，以同时模型空间和频率领域中的联系。</li>
<li>results: 这个研究发现，使用我们的架构可以在 CIFAR-10、FFHQ、LSUN-Bedroom 和 LSUN-Church 数据集上生成高品质的图像，比过 pixel-based 网络。<details>
<summary>Abstract</summary>
In this paper, we study the denoising diffusion probabilistic model (DDPM) in wavelet space, instead of pixel space, for visual synthesis. Considering the wavelet transform represents the image in spatial and frequency domains, we carefully design a novel architecture SFUNet to effectively capture the correlation for both domains. Specifically, in the standard denoising U-Net for pixel data, we supplement the 2D convolutions and spatial-only attention layers with our spatial frequency-aware convolution and attention modules to jointly model the complementary information from spatial and frequency domains in wavelet data. Our new architecture can be used as a drop-in replacement to the pixel-based network and is compatible with the vanilla DDPM training process. By explicitly modeling the wavelet signals, we find our model is able to generate images with higher quality on CIFAR-10, FFHQ, LSUN-Bedroom, and LSUN-Church datasets, than the pixel-based counterpart.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了在wavelet空间中使用扩散概率模型（DDPM）进行视觉合成。我们注意到，wavelet变换可以在空间和频率域中表示图像，因此我们 méticulously设计了一种新的架构SFUNet，以有效地捕捉这两个频率域之间的相关性。具体来说，在标准的干扰U-Net中，我们补充了2D卷积和专门关注空间频率信息的卷积和注意力模块，以同时模型空间和频率频率域中的补做信息。我们的新架构可以与标准的像素数据网络进行互换，并且与普通的DDPM训练过程相容。通过显式地模型wavelet信号，我们发现我们的模型在CIFAR-10、FFHQ、LSUN-Bedroom和LSUN-Church数据集上能够生成高质量的图像，比标准的像素数据网络更高。
</details></li>
</ul>
<hr>
<h2 id="MVMR-FS-Non-parametric-feature-selection-algorithm-based-on-Maximum-inter-class-Variation-and-Minimum-Redundancy"><a href="#MVMR-FS-Non-parametric-feature-selection-algorithm-based-on-Maximum-inter-class-Variation-and-Minimum-Redundancy" class="headerlink" title="MVMR-FS : Non-parametric feature selection algorithm based on Maximum inter-class Variation and Minimum Redundancy"></a>MVMR-FS : Non-parametric feature selection algorithm based on Maximum inter-class Variation and Minimum Redundancy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14643">http://arxiv.org/abs/2307.14643</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haitao Nie, Shengbo Zhang, Bin Xie</li>
<li>for: 本研究旨在解决 feature selection 中的 relevance 和 redundancy 问题，提出了一种基于最大间类差和最小重复度的非参数化Feature Selection算法（MVMR-FS）。</li>
<li>methods: 本研究使用了 supervised 和Unsupervised 预测器构成kernel density estimation来捕捉特征之间的相似性和差异，然后提出了 maximum inter-class variation和minimum redundancy（MVMR）的 критери来衡量特征的相关性和重复度。</li>
<li>results: 与前十种状态顶方法进行比较，MVMR-FS 实现了最高的平均准确率，提高了准确率5%到11%。<details>
<summary>Abstract</summary>
How to accurately measure the relevance and redundancy of features is an age-old challenge in the field of feature selection. However, existing filter-based feature selection methods cannot directly measure redundancy for continuous data. In addition, most methods rely on manually specifying the number of features, which may introduce errors in the absence of expert knowledge. In this paper, we propose a non-parametric feature selection algorithm based on maximum inter-class variation and minimum redundancy, abbreviated as MVMR-FS. We first introduce supervised and unsupervised kernel density estimation on the features to capture their similarities and differences in inter-class and overall distributions. Subsequently, we present the criteria for maximum inter-class variation and minimum redundancy (MVMR), wherein the inter-class probability distributions are employed to reflect feature relevance and the distances between overall probability distributions are used to quantify redundancy. Finally, we employ an AGA to search for the feature subset that minimizes the MVMR. Compared with ten state-of-the-art methods, MVMR-FS achieves the highest average accuracy and improves the accuracy by 5% to 11%.
</details>
<details>
<summary>摘要</summary>
age-old challenge 在feature选择领域是如何准确地测量特征相关性和重复性。然而，现有的筛选方法无法直接测量连续数据中的重复性。此外，大多数方法需要手动指定特征的数量，这可能会导致专家知识不足的情况下出现错误。在本文中，我们提出了一种非参数化特征选择算法基于最大 между类差异和最小重复性，简称MVMR-FS。我们首先引入了监督和无监督核密度估计器，用于捕捉特征之间的相似性和总体分布的不同。接着，我们介绍了MVMR的标准，其中用于反映特征相关性的inter-class probability distribution，以及用于衡量特征重复性的 distances between overall probability distributions。最后，我们使用AGA进行搜索，以找到最小化MVMR的特征子集。与十种当前状态的方法相比，MVMR-FS achieve最高的平均准确率，提高了5%到11%。
</details></li>
</ul>
<hr>
<h2 id="Linear-Convergence-of-Black-Box-Variational-Inference-Should-We-Stick-the-Landing"><a href="#Linear-Convergence-of-Black-Box-Variational-Inference-Should-We-Stick-the-Landing" class="headerlink" title="Linear Convergence of Black-Box Variational Inference: Should We Stick the Landing?"></a>Linear Convergence of Black-Box Variational Inference: Should We Stick the Landing?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14642">http://arxiv.org/abs/2307.14642</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kyurae Kim, Yian Ma, Jacob R. Gardner</li>
<li>for: 这paper是为了证明黑盒变量推断（BBVI）与控制变量的使用，特别是使用扣板降落（STL）估计器，在完美变量家族特定下 converges at a geometric rate。</li>
<li>methods: 这paper使用的方法包括证明STL估计器的偏导数方差为quadratic bound，以及对已有的closed-form entropy gradient estimators进行改进，以获得更好的非假设性质量保证。</li>
<li>results: 这paper的结果表明，使用BBVI和STL估计器可以在完美变量家族下 converges at a geometric rate，并且可以使用 projeted stochastic gradient descent来实现。此外，paper还提供了对closed-form entropy gradient estimators的改进，以及对其非假设性质量保证的explicit non-asymptotic complexity guarantees。<details>
<summary>Abstract</summary>
We prove that black-box variational inference (BBVI) with control variates, particularly the sticking-the-landing (STL) estimator, converges at a geometric (traditionally called "linear") rate under perfect variational family specification. In particular, we prove a quadratic bound on the gradient variance of the STL estimator, one which encompasses misspecified variational families. Combined with previous works on the quadratic variance condition, this directly implies convergence of BBVI with the use of projected stochastic gradient descent. We also improve existing analysis on the regular closed-form entropy gradient estimators, which enables comparison against the STL estimator and provides explicit non-asymptotic complexity guarantees for both.
</details>
<details>
<summary>摘要</summary>
我们证明黑盒变量推断（BBVI）与控制变量，尤其是粘降（STL）估计器，在完美变量家族 спецификация下 converge 于 геометри（传统上称为“线性”）速率。特别是，我们证明 STL 估计器的梯度方差呈 quadratic 形式，包括变量家族不准确的情况。与先前的二阶 variance 条件研究相结合，这直接意味着 BBVI 使用投影式随机梯度下降 converge。我们还改进了现有的关于关闭形式Entropy Gradient估计器的分析，使其与 STL 估计器进行比较，并提供了非含极限性质保证。
</details></li>
</ul>
<hr>
<h2 id="Fact-Checking-of-AI-Generated-Reports"><a href="#Fact-Checking-of-AI-Generated-Reports" class="headerlink" title="Fact-Checking of AI-Generated Reports"></a>Fact-Checking of AI-Generated Reports</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14634">http://arxiv.org/abs/2307.14634</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Razi Mahmood, Ge Wang, Mannudeep Kalra, Pingkun Yan</li>
<li>for: 这篇论文旨在探讨如何使用生成型人工智能（AI）生成真实的医疗影像报告，以减少临床过程中的时间和成本。</li>
<li>methods: 本研究使用了一种新的方法，即基于图像和文本的对映来验证AI生成的报告。这个方法通过学习图像和文本之间的相互关联，将真实和伪造的句子区分开来。</li>
<li>results: 研究发现，这个方法可以实时验证AI生成的报告，并将伪造的句子移除，以提高医疗过程中的精确性和可靠性。这个工具有助于未来的生成AI方法，以责任地使用AI实现医疗过程的加速。<details>
<summary>Abstract</summary>
With advances in generative artificial intelligence (AI), it is now possible to produce realistic-looking automated reports for preliminary reads of radiology images. This can expedite clinical workflows, improve accuracy and reduce overall costs. However, it is also well-known that such models often hallucinate, leading to false findings in the generated reports. In this paper, we propose a new method of fact-checking of AI-generated reports using their associated images. Specifically, the developed examiner differentiates real and fake sentences in reports by learning the association between an image and sentences describing real or potentially fake findings. To train such an examiner, we first created a new dataset of fake reports by perturbing the findings in the original ground truth radiology reports associated with images. Text encodings of real and fake sentences drawn from these reports are then paired with image encodings to learn the mapping to real/fake labels. The utility of such an examiner is demonstrated for verifying automatically generated reports by detecting and removing fake sentences. Future generative AI approaches can use the resulting tool to validate their reports leading to a more responsible use of AI in expediting clinical workflows.
</details>
<details>
<summary>摘要</summary>
随着生成式人工智能（AI）的进步，现在可以生成具有真实look的自动报告，以便加速临床工作流程，提高准确性并降低总成本。然而，这些模型经常“幻想”，导致生成的报告中的假发现。在这篇论文中，我们提议一种新的实验室检查AI生成的报告的方法。具体来说，我们开发了一个新的检查器，可以在报告中分辨真实和假的句子。为了训练这个检查器，我们首先创建了一个新的假报告数据集，其中对原始的真实股票报告中的发现进行了修改。然后，我们对真实和假句子的文本编码和图像编码进行了对应，以学习将真实和假标签映射到句子和图像中。我们证明了这种检查器可以用于检查自动生成的报告，并且可以检测并移除假的句子。未来的生成AI方法可以使用这种工具来验证他们的报告，从而实现负责任的AI使用。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-on-Reservoir-Computing-and-its-Interdisciplinary-Applications-Beyond-Traditional-Machine-Learning"><a href="#A-Survey-on-Reservoir-Computing-and-its-Interdisciplinary-Applications-Beyond-Traditional-Machine-Learning" class="headerlink" title="A Survey on Reservoir Computing and its Interdisciplinary Applications Beyond Traditional Machine Learning"></a>A Survey on Reservoir Computing and its Interdisciplinary Applications Beyond Traditional Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15092">http://arxiv.org/abs/2307.15092</a></li>
<li>repo_url: None</li>
<li>paper_authors: Heng Zhang, Danilo Vasconcellos Vargas</li>
<li>for: 本研究评论文章的目的是对储量计算（RC）的最新发展进行统一的回顾，从机器学习到物理、生物和神经科学。</li>
<li>methods: 本文使用的方法包括早期RC模型的介绍，以及当前状态的RC模型和其应用。同时，文章还介绍了模拟大脑机制的研究。</li>
<li>results: 本文对RC的发展进行了统一的回顾，并介绍了它在不同领域的应用，包括机器学习、物理、生物和神经科学。此外，文章还提供了新的发展perspective，包括储量设计、编程框架的统一、物理实现和与认知神经科学和进化相互作用。<details>
<summary>Abstract</summary>
Reservoir computing (RC), first applied to temporal signal processing, is a recurrent neural network in which neurons are randomly connected. Once initialized, the connection strengths remain unchanged. Such a simple structure turns RC into a non-linear dynamical system that maps low-dimensional inputs into a high-dimensional space. The model's rich dynamics, linear separability, and memory capacity then enable a simple linear readout to generate adequate responses for various applications. RC spans areas far beyond machine learning, since it has been shown that the complex dynamics can be realized in various physical hardware implementations and biological devices. This yields greater flexibility and shorter computation time. Moreover, the neuronal responses triggered by the model's dynamics shed light on understanding brain mechanisms that also exploit similar dynamical processes. While the literature on RC is vast and fragmented, here we conduct a unified review of RC's recent developments from machine learning to physics, biology, and neuroscience. We first review the early RC models, and then survey the state-of-the-art models and their applications. We further introduce studies on modeling the brain's mechanisms by RC. Finally, we offer new perspectives on RC development, including reservoir design, coding frameworks unification, physical RC implementations, and interaction between RC, cognitive neuroscience and evolution.
</details>
<details>
<summary>摘要</summary>
储池计算（RC），最初应用于时间信号处理，是一种循环神经网络，其neurons randomly连接。一旦初始化，连接强度保持不变。这种简单的结构使RC变成一个非线性动力系统，可以将低维度输入映射到高维度空间中。模型的丰富动力、线性分离和记忆容量使得一个简单的直线读取可以生成适用于各种应用的合适响应。RC的应用范围超出机器学习，因为它已经在物理硬件实现和生物设备中实现了复杂的动力。这些实现带来更多的灵活性和更短的计算时间。此外，模型的神经响应也为了解大脑机制提供了新的思路，这些机制也利用类似的动力过程。在文献中，关于RC的研究非常广泛和杂乱，这里我们提供一个统一的RC最新发展的评论，从机器学习到物理、生物和神经科学。我们首先介绍了RC的早期模型，然后评论了当前最佳模型和其应用。我们还介绍了模型大脑机制的研究。最后，我们提出了新的RC发展 perspective，包括储池设计、编程框架统一、物理RC实现和RC、认知神经科学和演化之间的交互。
</details></li>
</ul>
<hr>
<h2 id="Rapid-and-Scalable-Bayesian-AB-Testing"><a href="#Rapid-and-Scalable-Bayesian-AB-Testing" class="headerlink" title="Rapid and Scalable Bayesian AB Testing"></a>Rapid and Scalable Bayesian AB Testing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14628">http://arxiv.org/abs/2307.14628</a></li>
<li>repo_url: None</li>
<li>paper_authors: Srivas Chennu, Andrew Maher, Christian Pangerl, Subash Prabanantham, Jae Hyeon Bae, Jamie Martin, Bud Goswami</li>
<li>for: 该论文旨在帮助企业operator更好地做出决策，通过利用数据学习来提高数字用户体验。</li>
<li>methods: 该论文提出了一种基于 Bayesian 估计的解决方案，用于 Addressing the limitations of current sequential AB testing methodology, such as lack of statistical power, correlations between factors, and inability to pool knowledge from past tests.</li>
<li>results: 论文通过 numercial simulations 和实际应用 demonstrate 了该方法的有用性，包括增加了统计力量、允许顺序测试和早期停止、不受过分风险等。此外，论文还展示了如何使用这种方法来加速未来测试。<details>
<summary>Abstract</summary>
AB testing aids business operators with their decision making, and is considered the gold standard method for learning from data to improve digital user experiences. However, there is usually a gap between the requirements of practitioners, and the constraints imposed by the statistical hypothesis testing methodologies commonly used for analysis of AB tests. These include the lack of statistical power in multivariate designs with many factors, correlations between these factors, the need of sequential testing for early stopping, and the inability to pool knowledge from past tests. Here, we propose a solution that applies hierarchical Bayesian estimation to address the above limitations. In comparison to current sequential AB testing methodology, we increase statistical power by exploiting correlations between factors, enabling sequential testing and progressive early stopping, without incurring excessive false positive risk. We also demonstrate how this methodology can be extended to enable the extraction of composite global learnings from past AB tests, to accelerate future tests. We underpin our work with a solid theoretical framework that articulates the value of hierarchical estimation. We demonstrate its utility using both numerical simulations and a large set of real-world AB tests. Together, these results highlight the practical value of our approach for statistical inference in the technology industry.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="BubbleML-A-Multi-Physics-Dataset-and-Benchmarks-for-Machine-Learning"><a href="#BubbleML-A-Multi-Physics-Dataset-and-Benchmarks-for-Machine-Learning" class="headerlink" title="BubbleML: A Multi-Physics Dataset and Benchmarks for Machine Learning"></a>BubbleML: A Multi-Physics Dataset and Benchmarks for Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14623">http://arxiv.org/abs/2307.14623</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hpcforge/bubbleml">https://github.com/hpcforge/bubbleml</a></li>
<li>paper_authors: Sheikh Md Shakeel Hassan, Arthur Feeney, Akash Dhruv, Jihoon Kim, Youngjoon Suh, Jaiyoung Ryu, Yoonjin Won, Aparna Chandramowlishwaran</li>
<li>for: 该论文主要目的是提供一个可访问的和多样化的数据集，用于机器学习（ML）训练，以更好地理解多物理现象的热相变化。</li>
<li>methods: 该论文使用物理驱动的计算机模拟来提供精准的观测数据，包括各种热泡沸点情况，如 Pool boiling, flow boiling, 和半冷泡沸。该数据集覆盖了广泛的参数，包括不同的重力条件、流速、半冷水位和墙面超热情况，涵盖51个计算。</li>
<li>results: 该论文验证了该数据集的有效性，并展示了其在多种下游任务中的潜在应用，包括光流分析和温度动力学学习。该数据集和其标准 benchmarks 将成为机器学习驱动的多物理热相变化研究的推进者，推动了技术和模型的开发和比较。<details>
<summary>Abstract</summary>
In the field of phase change phenomena, the lack of accessible and diverse datasets suitable for machine learning (ML) training poses a significant challenge. Existing experimental datasets are often restricted, with limited availability and sparse ground truth data, impeding our understanding of this complex multi-physics phenomena. To bridge this gap, we present the BubbleML Dataset(https://github.com/HPCForge/BubbleML) which leverages physics-driven simulations to provide accurate ground truth information for various boiling scenarios, encompassing nucleate pool boiling, flow boiling, and sub-cooled boiling. This extensive dataset covers a wide range of parameters, including varying gravity conditions, flow rates, sub-cooling levels, and wall superheat, comprising 51 simulations. BubbleML is validated against experimental observations and trends, establishing it as an invaluable resource for ML research. Furthermore, we showcase its potential to facilitate exploration of diverse downstream tasks by introducing two benchmarks: (a) optical flow analysis to capture bubble dynamics, and (b) operator networks for learning temperature dynamics. The BubbleML dataset and its benchmarks serve as a catalyst for advancements in ML-driven research on multi-physics phase change phenomena, enabling the development and comparison of state-of-the-art techniques and models.
</details>
<details>
<summary>摘要</summary>
在热变现象领域，因缺乏可访问的多样化数据集而受到机器学习（ML）训练的挑战。现有的实验数据集经常受限，有限的可用性和罕见的基准数据，妨碍我们对这种复杂多物理现象的理解。为了缓解这个差距，我们提供了BubbleML数据集（https://github.com/HPCForge/BubbleML），利用物理驱动的 simulations提供了各种爆发enario中的准确基准信息，包括 Pool boiling、流泌 boiling 和半冷含气 boiling 等多种情况。这个广泛的数据集覆盖了多种参数，包括不同重力条件、流量、冷凉水位、墙superheat 等，涵盖51个 simulations。BubbleML被验证了对实验观测和趋势的验证，确立了它作为ML研究中不可或缺的资源。此外，我们还在其中引入了两个比较任务：（a）Optical flow分析，捕捉气泡动态；（b）运算网络，学习温度动态。BubbleML数据集和其比较任务serve as a catalyst for advancements in ML-driven research on multi-physics phase change phenomena，激发开发和对state-of-the-art技术和模型的比较。
</details></li>
</ul>
<hr>
<h2 id="Imitating-Complex-Trajectories-Bridging-Low-Level-Stability-and-High-Level-Behavior"><a href="#Imitating-Complex-Trajectories-Bridging-Low-Level-Stability-and-High-Level-Behavior" class="headerlink" title="Imitating Complex Trajectories: Bridging Low-Level Stability and High-Level Behavior"></a>Imitating Complex Trajectories: Bridging Low-Level Stability and High-Level Behavior</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14619">http://arxiv.org/abs/2307.14619</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adam Block, Daniel Pfrommer, Max Simchowitz</li>
<li>for: studying the imitation of stochastic, non-Markovian, potentially multi-modal expert demonstrations in nonlinear dynamical systems.</li>
<li>methods: invoking low-level controllers (either learned or implicit in position-command control) to stabilize imitation policies around expert demonstrations, with a focus on ensuring “total variation continuity” (TVC) to achieve accurate matching of the demonstrator’s distribution over entire trajectories.</li>
<li>results: the paper provides theoretical guarantees for policies parameterized by diffusion models, showing that if the learner accurately estimates the score of the (noise-augmented) expert policy, then the distribution of imitator trajectories is close to the demonstrator distribution in a natural optimal transport distance, with empirical validation of the algorithmic recommendations.Here’s the Chinese translation of the three key information points:</li>
<li>for: 研究stochoastic, non-Markovian, potentially multi-modal expert示例在非线性动力系统中的模仿。</li>
<li>methods: 通过 invoke low-level控制器（ Either learned或implicit in position-command control）来稳定模仿策略 around expert示例，以确保 “总变量稳定” (TVC) 以实现准确地匹配示例者的分布 over entire trajectories。</li>
<li>results: 文章提供了对政策参数化 by diffusion models的 teorotical guarantees，表明如果学习者准确地估计（noise-augmented）专家策略的Score，那么imitator的分布 will be close to the demonstrator distribution in a natural optimal transport distance,并进行了实验验证算法建议。<details>
<summary>Abstract</summary>
We propose a theoretical framework for studying the imitation of stochastic, non-Markovian, potentially multi-modal (i.e. "complex" ) expert demonstrations in nonlinear dynamical systems. Our framework invokes low-level controllers - either learned or implicit in position-command control - to stabilize imitation policies around expert demonstrations. We show that with (a) a suitable low-level stability guarantee and (b) a stochastic continuity property of the learned policy we call "total variation continuity" (TVC), an imitator that accurately estimates actions on the demonstrator's state distribution closely matches the demonstrator's distribution over entire trajectories. We then show that TVC can be ensured with minimal degradation of accuracy by combining a popular data-augmentation regimen with a novel algorithmic trick: adding augmentation noise at execution time. We instantiate our guarantees for policies parameterized by diffusion models and prove that if the learner accurately estimates the score of the (noise-augmented) expert policy, then the distribution of imitator trajectories is close to the demonstrator distribution in a natural optimal transport distance. Our analysis constructs intricate couplings between noise-augmented trajectories, a technique that may be of independent interest. We conclude by empirically validating our algorithmic recommendations.
</details>
<details>
<summary>摘要</summary>
我们提出一种理论框架，用于研究复杂专家示范的模仿在非线性动力系统中。我们的框架借鉴低级控制器，ether学习或含有位置控制的隐式控制器，以稳定模仿政策。我们证明，如果（a）有适当的低级稳定保证，并且（b）学习政策具有总变量连续性（TVC）性质，那么模仿者可以准确地模仿专家的动作，并且模仿者的动作分布与专家的动作分布在整个轨迹上几乎相同。我们然后证明，可以通过组合流行的数据扩展约束和一种新的算法技巧来保证TVC的存在：在执行时添加扩展噪声。我们在执行时添加扩展噪声，可以在较低的精度下保证TVC。我们实例化我们的保证，并证明如果学习者准确地估计噪声扩展后的专家政策的分数，那么模仿者的轨迹分布与专家的轨迹分布在自然的优质量度中几乎相同。我们的分析建立了复杂的拓扑关系，这可能是独立的兴趣。我们最后通过实验验证我们的算法建议。
</details></li>
</ul>
<hr>
<h2 id="Self-Contrastive-Graph-Diffusion-Network"><a href="#Self-Contrastive-Graph-Diffusion-Network" class="headerlink" title="Self-Contrastive Graph Diffusion Network"></a>Self-Contrastive Graph Diffusion Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14613">http://arxiv.org/abs/2307.14613</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kunzhan/SCDGN">https://github.com/kunzhan/SCDGN</a></li>
<li>paper_authors: Yixian Ma, Kun Zhan</li>
<li>for: 本文是用于提出一种新的自适应图diffusion网络(SCGDN)的框架，用于图像自适应学习。</li>
<li>methods: 本文使用了两个主要组成部分：宏观模块(AttM)和扩散模块(DiFM)。AttM通过聚合高阶结构和特征信息来获得优秀的嵌入，而DiFM通过拉普拉斯扩散学习来均衡每个节点在图中的状态，并允许特征信息和邻接信息在图中协同演化。</li>
<li>results: 本文的实验结果表明，SCGDN可以在图像自适应学习中提供更高的性能，并且可以避免”采样偏见”和语义漂移。<details>
<summary>Abstract</summary>
Augmentation techniques and sampling strategies are crucial in contrastive learning, but in most existing works, augmentation techniques require careful design, and their sampling strategies can only capture a small amount of intrinsic supervision information. Additionally, the existing methods require complex designs to obtain two different representations of the data. To overcome these limitations, we propose a novel framework called the Self-Contrastive Graph Diffusion Network (SCGDN). Our framework consists of two main components: the Attentional Module (AttM) and the Diffusion Module (DiFM). AttM aggregates higher-order structure and feature information to get an excellent embedding, while DiFM balances the state of each node in the graph through Laplacian diffusion learning and allows the cooperative evolution of adjacency and feature information in the graph. Unlike existing methodologies, SCGDN is an augmentation-free approach that avoids "sampling bias" and semantic drift, without the need for pre-training. We conduct a high-quality sampling of samples based on structure and feature information. If two nodes are neighbors, they are considered positive samples of each other. If two disconnected nodes are also unrelated on $k$NN graph, they are considered negative samples for each other. The contrastive objective reasonably uses our proposed sampling strategies, and the redundancy reduction term minimizes redundant information in the embedding and can well retain more discriminative information. In this novel framework, the graph self-contrastive learning paradigm gives expression to a powerful force. SCGDN effectively balances between preserving high-order structure information and avoiding overfitting. The results manifest that SCGDN can consistently generate outperformance over both the contrastive methods and the classical methods.
</details>
<details>
<summary>摘要</summary>
“增强技术和采样策略是对比学习中非常重要的，但现有的方法中的增强技术需要精心设计，采样策略只能捕捉一小部分内在监督信息。此外，现有的方法需要复杂的设计来获得两种不同的数据表示。为了解决这些限制，我们提出了一种新的框架called Self-Contrastive Graph Diffusion Network (SCGDN)。我们的框架包括两个主要组件：Attentional Module (AttM)和Diffusion Module (DiFM)。AttM将高阶结构和特征信息聚合以获得优秀的嵌入，而DiFM通过laplacian diffusion learning来均衡每个节点在图中的状态，allowing the cooperative evolution of adjacency and feature information in the graph。不同于现有的方法ologies，SCGDN是一种无增强 approached that avoids "sampling bias"和semantic drift，不需要预训练。我们采用高质量的采样策略，基于结构和特征信息。如果两个节点是邻居，它们被视为对方的正例样本。如果两个不相关的节点也不在$k$NN图中相互关系，它们被视为对方的负例样本。对比目标函数合理地使用我们提议的采样策略，而减 redundancy reduction term可以减少嵌入中的重复信息，能够良好地保留更多的权威信息。在这种新的框架中，图自身对比学习模式发挥了强大的力量。SCGDN能够平衡保持高阶结构信息和避免过拟合。结果表明，SCGDN可以一直在对比方法和传统方法之上出perform。”
</details></li>
</ul>
<hr>
<h2 id="Complete-and-separate-Conditional-separation-with-missing-target-source-attribute-completion"><a href="#Complete-and-separate-Conditional-separation-with-missing-target-source-attribute-completion" class="headerlink" title="Complete and separate: Conditional separation with missing target source attribute completion"></a>Complete and separate: Conditional separation with missing target source attribute completion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14609">http://arxiv.org/abs/2307.14609</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dimitrios Bralios, Efthymios Tzinis, Paris Smaragdis</li>
<li>for:  This paper is written for improving the performance of source separation models by leveraging semantic information about the input mixtures and constituent sources.</li>
<li>methods: The paper uses a pre-trained model to extract additional semantic data from the input mixture, which is then used to improve the separation performance of an uncoupled multi-conditional separation network.</li>
<li>results: The paper demonstrates that the separation performance of the multi-conditional model is significantly improved, approaching the performance of an oracle model with complete semantic information. Additionally, the approach achieves performance levels that are comparable to those of the best performing specialized single conditional models, providing an easier-to-use alternative.<details>
<summary>Abstract</summary>
Recent approaches in source separation leverage semantic information about their input mixtures and constituent sources that when used in conditional separation models can achieve impressive performance. Most approaches along these lines have focused on simple descriptions, which are not always useful for varying types of input mixtures. In this work, we present an approach in which a model, given an input mixture and partial semantic information about a target source, is trained to extract additional semantic data. We then leverage this pre-trained model to improve the separation performance of an uncoupled multi-conditional separation network. Our experiments demonstrate that the separation performance of this multi-conditional model is significantly improved, approaching the performance of an oracle model with complete semantic information. Furthermore, our approach achieves performance levels that are comparable to those of the best performing specialized single conditional models, thus providing an easier to use alternative.
</details>
<details>
<summary>摘要</summary>
现代源分离方法利用输入混合的semantic信息和组成源的信息，当用于条件分离模型时可以达到吸目的性能。大多数方法都是简单的描述，不一定适用于不同类型的输入混合。在这种工作中，我们提出了一种方法，即给定输入混合和部分semantic信息的target源，训练模型提取额外的semantic数据。然后，我们利用这个预训练模型提高不相互连接的多Conditional分离网络的分离性能。我们的实验表明，这种多Conditional网络的分离性能明显提高，接近完美的oracle模型，并且与专门设计的单Conditional模型性能相当。此外，我们的方法比特化的单Conditional模型更容易使用，提供了一种更容易使用的代替方案。
</details></li>
</ul>
<hr>
<h2 id="HUTFormer-Hierarchical-U-Net-Transformer-for-Long-Term-Traffic-Forecasting"><a href="#HUTFormer-Hierarchical-U-Net-Transformer-for-Long-Term-Traffic-Forecasting" class="headerlink" title="HUTFormer: Hierarchical U-Net Transformer for Long-Term Traffic Forecasting"></a>HUTFormer: Hierarchical U-Net Transformer for Long-Term Traffic Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14596">http://arxiv.org/abs/2307.14596</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zezhi Shao, Fei Wang, Zhao Zhang, Yuchen Fang, Guangyin Jin, Yongjun Xu</li>
<li>for: 预测交通情况，即基于历史观察数据预测交通条件，是智能交通领域的长期研究主题，而且被广泛认为是智能交通系统的重要组成部分。</li>
<li>methods: 我们提出了一种新的层次U-NetTransformer（HUTFormer）来解决长期交通预测的问题，它包括一个层次编码器和解码器，以同时生成和利用多级表示。特别是，编码器使用窗口自注意力和段合并来提取多级表示，而解码器则使用跨级注意力机制以有效地合并多级表示。</li>
<li>results: 我们在四个交通数据集上进行了广泛的实验，结果表明，我们提出的HUTFormer显著超过了当前交通预测和长时间序列预测基线。<details>
<summary>Abstract</summary>
Traffic forecasting, which aims to predict traffic conditions based on historical observations, has been an enduring research topic and is widely recognized as an essential component of intelligent transportation. Recent proposals on Spatial-Temporal Graph Neural Networks (STGNNs) have made significant progress by combining sequential models with graph convolution networks. However, due to high complexity issues, STGNNs only focus on short-term traffic forecasting, e.g., 1-hour forecasting, while ignoring more practical long-term forecasting. In this paper, we make the first attempt to explore long-term traffic forecasting, e.g., 1-day forecasting. To this end, we first reveal its unique challenges in exploiting multi-scale representations. Then, we propose a novel Hierarchical U-net TransFormer (HUTFormer) to address the issues of long-term traffic forecasting. HUTFormer consists of a hierarchical encoder and decoder to jointly generate and utilize multi-scale representations of traffic data. Specifically, for the encoder, we propose window self-attention and segment merging to extract multi-scale representations from long-term traffic data. For the decoder, we design a cross-scale attention mechanism to effectively incorporate multi-scale representations. In addition, HUTFormer employs an efficient input embedding strategy to address the complexity issues. Extensive experiments on four traffic datasets show that the proposed HUTFormer significantly outperforms state-of-the-art traffic forecasting and long time series forecasting baselines.
</details>
<details>
<summary>摘要</summary>
traffic 预测，targeting to predict traffic conditions based on historical observations，has been a long-standing research topic and is widely recognized as an essential component of intelligent transportation. Recent proposals on Spatial-Temporal Graph Neural Networks (STGNNs) have made significant progress by combining sequential models with graph convolution networks. However, due to high complexity issues, STGNNs only focus on short-term traffic forecasting, e.g., 1-hour forecasting, while ignoring more practical long-term forecasting. In this paper, we make the first attempt to explore long-term traffic forecasting, e.g., 1-day forecasting. To this end, we first reveal its unique challenges in exploiting multi-scale representations. Then, we propose a novel Hierarchical U-net TransFormer (HUTFormer) to address the issues of long-term traffic forecasting. HUTFormer consists of a hierarchical encoder and decoder to jointly generate and utilize multi-scale representations of traffic data. Specifically, for the encoder, we propose window self-attention and segment merging to extract multi-scale representations from long-term traffic data. For the decoder, we design a cross-scale attention mechanism to effectively incorporate multi-scale representations. In addition, HUTFormer employs an efficient input embedding strategy to address the complexity issues. Extensive experiments on four traffic datasets show that the proposed HUTFormer significantly outperforms state-of-the-art traffic forecasting and long time series forecasting baselines.
</details></li>
</ul>
<hr>
<h2 id="MCPA-Multi-scale-Cross-Perceptron-Attention-Network-for-2D-Medical-Image-Segmentation"><a href="#MCPA-Multi-scale-Cross-Perceptron-Attention-Network-for-2D-Medical-Image-Segmentation" class="headerlink" title="MCPA: Multi-scale Cross Perceptron Attention Network for 2D Medical Image Segmentation"></a>MCPA: Multi-scale Cross Perceptron Attention Network for 2D Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14588">http://arxiv.org/abs/2307.14588</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/simonustc/mcpa-for-2d-medical-image-segmentation">https://github.com/simonustc/mcpa-for-2d-medical-image-segmentation</a></li>
<li>paper_authors: Liang Xu, Mingxiao Chen, Yi Cheng, Pengfei Shao, Shuwei Shen, Peng Yao, Ronald X. Xu</li>
<li>for: 这个研究的目的是提出一个基于Convolutional Neural Networks (CNN)的双维医疗影像分类模型，以提高医疗影像分类的精度和效能。</li>
<li>methods: 这个模型使用了Transformer模组来强化UNet架构，以更好地捕捉全局特征相关性。此外，模型还导入了多 scales Cross Perceptron模组，以捕捉不同 scales的特征相关性。</li>
<li>results: 在各种公开的医疗影像数据集上进行评估，这个模型实现了顶尖的表现，并且在不同的任务和设备上均有出色的结果。<details>
<summary>Abstract</summary>
The UNet architecture, based on Convolutional Neural Networks (CNN), has demonstrated its remarkable performance in medical image analysis. However, it faces challenges in capturing long-range dependencies due to the limited receptive fields and inherent bias of convolutional operations. Recently, numerous transformer-based techniques have been incorporated into the UNet architecture to overcome this limitation by effectively capturing global feature correlations. However, the integration of the Transformer modules may result in the loss of local contextual information during the global feature fusion process. To overcome these challenges, we propose a 2D medical image segmentation model called Multi-scale Cross Perceptron Attention Network (MCPA). The MCPA consists of three main components: an encoder, a decoder, and a Cross Perceptron. The Cross Perceptron first captures the local correlations using multiple Multi-scale Cross Perceptron modules, facilitating the fusion of features across scales. The resulting multi-scale feature vectors are then spatially unfolded, concatenated, and fed through a Global Perceptron module to model global dependencies. Furthermore, we introduce a Progressive Dual-branch Structure to address the semantic segmentation of the image involving finer tissue structures. This structure gradually shifts the segmentation focus of MCPA network training from large-scale structural features to more sophisticated pixel-level features. We evaluate our proposed MCPA model on several publicly available medical image datasets from different tasks and devices, including the open large-scale dataset of CT (Synapse), MRI (ACDC), fundus camera (DRIVE, CHASE_DB1, HRF), and OCTA (ROSE). The experimental results show that our MCPA model achieves state-of-the-art performance. The code is available at https://github.com/simonustc/MCPA-for-2D-Medical-Image-Segmentation.
</details>
<details>
<summary>摘要</summary>
UNet 架构，基于卷积神经网络（CNN），在医疗影像分析中表现出色。然而，它在捕捉长距离依赖关系方面存在挑战，因为卷积操作具有限定的接收区域和自然偏好。在最近，许多基于转换器的技术被 incorporated 到 UNet 架构中，以有效地捕捉全局特征相关性。然而，将转换模块 интегра到 UNet 架构中可能会导致全局特征相关性的损失。为了解决这些挑战，我们提出了一种名为 Multi-scale Cross Perceptron Attention Network (MCPA) 的2D医疗影像分类模型。MCPA 模型由三个主要组件组成：编码器、解码器和 Cross Perceptron。Cross Perceptron 首先使用多个 Multi-scale Cross Perceptron 模块捕捉本地相关性，以便在不同尺度上进行特征融合。得到的多尺度特征向量然后在空间上展开，并将其 concatenate 并输入到全球 Perceptron 模块，以模拟全局依赖关系。此外，我们还提出了一种进步的双支结构，以解决医疗影像分类中的semantic segmentation问题。这种结构逐渐将 MCPA 网络训练的 segmentation 焦点从大规模结构特征向 pixel-level 特征进行转换。我们在多个公共可用的医疗影像数据集上进行了多种任务和设备的测试，包括 Synapse 等开放式大规模数据集、ACDC 等 MRI 数据集、DRIVE 等fundus camera 数据集、CHASE_DB1 等 OCTA 数据集。实验结果表明，我们的 MCPA 模型在 state-of-the-art 性能。代码可以在 GitHub 上获取：https://github.com/simonustc/MCPA-for-2D-Medical-Image-Segmentation。
</details></li>
</ul>
<hr>
<h2 id="Evaluation-of-Safety-Constraints-in-Autonomous-Navigation-with-Deep-Reinforcement-Learning"><a href="#Evaluation-of-Safety-Constraints-in-Autonomous-Navigation-with-Deep-Reinforcement-Learning" class="headerlink" title="Evaluation of Safety Constraints in Autonomous Navigation with Deep Reinforcement Learning"></a>Evaluation of Safety Constraints in Autonomous Navigation with Deep Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14568">http://arxiv.org/abs/2307.14568</a></li>
<li>repo_url: None</li>
<li>paper_authors: Brian Angulo, Gregory Gorbov, Aleksandr Panov, Konstantin Yakovlev</li>
<li>for: 这个研究旨在高亮安全性因素在自动驾驶系统中的重要性，并通过比较两种学习导航策略：安全和不安全的策略来证明这一点。</li>
<li>methods: 这个研究使用了强化学习算法在自动驾驶系统中进行导航，并对两种策略进行比较，以 highlight the importance of considering safety constraints in the development of autonomous vehicles.</li>
<li>results: 研究结果表明，安全策略可以生成更多的减噪距离（与障碍物之间的距离），并少量碰撞，而不 sacrificing the overall performance。<details>
<summary>Abstract</summary>
While reinforcement learning algorithms have had great success in the field of autonomous navigation, they cannot be straightforwardly applied to the real autonomous systems without considering the safety constraints. The later are crucial to avoid unsafe behaviors of the autonomous vehicle on the road. To highlight the importance of these constraints, in this study, we compare two learnable navigation policies: safe and unsafe. The safe policy takes the constraints into account, while the other does not. We show that the safe policy is able to generate trajectories with more clearance (distance to the obstacles) and makes less collisions while training without sacrificing the overall performance.
</details>
<details>
<summary>摘要</summary>
autonomous navigation 算法已经在场景中取得了很大的成功，但它们不能直接应用于实际的自动驾驶系统中，因为需要考虑安全约束。这些约束是关键，以避免自动车辆在路上发生不安全行为。为了强调这些约束的重要性，在这个研究中，我们比较了两种可学习导航策略：安全和不安全。安全策略考虑了约束，而另一个不考虑。我们显示，安全策略能够生成具有更多的避险距离（距离障碍物）并且 fewer collisions  durante el entrenamiento，而不 sacrificing the overall performance。
</details></li>
</ul>
<hr>
<h2 id="Auto-Tables-Synthesizing-Multi-Step-Transformations-to-Relationalize-Tables-without-Using-Examples"><a href="#Auto-Tables-Synthesizing-Multi-Step-Transformations-to-Relationalize-Tables-without-Using-Examples" class="headerlink" title="Auto-Tables: Synthesizing Multi-Step Transformations to Relationalize Tables without Using Examples"></a>Auto-Tables: Synthesizing Multi-Step Transformations to Relationalize Tables without Using Examples</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14565">http://arxiv.org/abs/2307.14565</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lipengcs/auto-tables-benchmark">https://github.com/lipengcs/auto-tables-benchmark</a></li>
<li>paper_authors: Peng Li, Yeye He, Cong Yan, Yue Wang, Surajit Chaudhuri<br>for:* This paper aims to address the problem of non-relational tables in relational databases, specifically the need for complex table-restructuring transformations before these tables can be queried using SQL-based analytics tools.methods:* The authors develop an Auto-Tables system that can automatically synthesize pipelines with multi-step transformations to transform non-relational tables into standard relational forms for downstream analytics.results:* The authors evaluate the effectiveness of their proposed system using an extensive benchmark of 244 real test cases from user spreadsheets and online forums. Their evaluation suggests that Auto-Tables can successfully synthesize transformations for over 70% of test cases at interactive speeds, without requiring any input from users.<details>
<summary>Abstract</summary>
Relational tables, where each row corresponds to an entity and each column corresponds to an attribute, have been the standard for tables in relational databases. However, such a standard cannot be taken for granted when dealing with tables "in the wild". Our survey of real spreadsheet-tables and web-tables shows that over 30% of such tables do not conform to the relational standard, for which complex table-restructuring transformations are needed before these tables can be queried easily using SQL-based analytics tools. Unfortunately, the required transformations are non-trivial to program, which has become a substantial pain point for technical and non-technical users alike, as evidenced by large numbers of forum questions in places like StackOverflow and Excel/Power-BI/Tableau forums.   We develop an Auto-Tables system that can automatically synthesize pipelines with multi-step transformations (in Python or other languages), to transform non-relational tables into standard relational forms for downstream analytics, obviating the need for users to manually program transformations. We compile an extensive benchmark for this new task, by collecting 244 real test cases from user spreadsheets and online forums. Our evaluation suggests that Auto-Tables can successfully synthesize transformations for over 70% of test cases at interactive speeds, without requiring any input from users, making this an effective tool for both technical and non-technical users to prepare data for analytics.
</details>
<details>
<summary>摘要</summary>
Relational tables, where each row represents an entity and each column represents an attribute, have been the standard for tables in relational databases. However, this standard cannot be taken for granted when dealing with tables "in the wild". Our survey of real spreadsheet-tables and web-tables shows that over 30% of such tables do not conform to the relational standard, and complex table-restructuring transformations are needed before these tables can be queried easily using SQL-based analytics tools. Unfortunately, the required transformations are non-trivial to program, which has become a substantial pain point for both technical and non-technical users, as evidenced by large numbers of forum questions in places like Stack Overflow and Excel/Power BI/Tableau forums.We develop an Auto-Tables system that can automatically synthesize pipelines with multi-step transformations (in Python or other languages) to transform non-relational tables into standard relational forms for downstream analytics, obviating the need for users to manually program transformations. We compile an extensive benchmark for this new task by collecting 244 real test cases from user spreadsheets and online forums. Our evaluation suggests that Auto-Tables can successfully synthesize transformations for over 70% of test cases at interactive speeds, without requiring any input from users, making this an effective tool for both technical and non-technical users to prepare data for analytics.
</details></li>
</ul>
<hr>
<h2 id="Understanding-Forward-Process-of-Convolutional-Neural-Network"><a href="#Understanding-Forward-Process-of-Convolutional-Neural-Network" class="headerlink" title="Understanding Forward Process of Convolutional Neural Network"></a>Understanding Forward Process of Convolutional Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15090">http://arxiv.org/abs/2307.15090</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/himanshub1007/Alzhimers-Disease-Prediction-Using-Deep-learning">https://github.com/himanshub1007/Alzhimers-Disease-Prediction-Using-Deep-learning</a></li>
<li>paper_authors: Peixin Tian</li>
<li>for: 这篇论文揭示了深度神经网络（CNNs）的选择性旋转处理。</li>
<li>methods: 论文解释了 activation function 作为一种分辨率机制，将输入数据的旋转性统一和量化。</li>
<li>results: 实验显示，这种定义的方法论网络可以根据统计指标来识别输入数据，可以通过结构化数学工具进行分析。我们的发现还揭示了人工神经网络和人脑的数据处理模式之间的一致性。<details>
<summary>Abstract</summary>
This paper reveal the selective rotation in the CNNs' forward processing. It elucidates the activation function as a discerning mechanism that unifies and quantizes the rotational aspects of the input data. Experiments show how this defined methodology reflects the progress network distinguish inputs based on statistical indicators, which can be comprehended or analyzed by applying structured mathematical tools. Our findings also unveil the consistency between artificial neural networks and the human brain in their data processing pattern.
</details>
<details>
<summary>摘要</summary>
Note: Simplified Chinese is also known as "简化字符" or "简化字符".Translation notes:* "selective rotation" is translated as "选择的旋转" (选择的旋转)* "forward processing" is translated as "前进处理" (前进处理)* "activation function" is translated as "活化函数" (活化函数)* "progress network" is translated as "进步网络" (进步网络)* "statistical indicators" is translated as "统计指标" (统计指标)* "structured mathematical tools" is translated as "结构化数学工具" (结构化数学工具)
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Sleeping-Bandit-Problems-with-Multiple-Plays-Algorithm-and-Ranking-Application"><a href="#Adversarial-Sleeping-Bandit-Problems-with-Multiple-Plays-Algorithm-and-Ranking-Application" class="headerlink" title="Adversarial Sleeping Bandit Problems with Multiple Plays: Algorithm and Ranking Application"></a>Adversarial Sleeping Bandit Problems with Multiple Plays: Algorithm and Ranking Application</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14549">http://arxiv.org/abs/2307.14549</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianjun Yuan, Wei Lee Woon, Ludovik Coba</li>
<li>for: 这 paper 是为了解决在线推荐系统中的睡眠骑士问题，该问题具有固定、敌对损失和不确定的 i.i.d. 分布。</li>
<li>methods: 提出的算法基于睡眠骑士算法，并对单臂选择进行扩展，可以保证理论性能，增量误差 upper bounded by $\bigO(kN^2\sqrt{T\log T})$, где $k$ 是每个时间步选择的臂数，$N$ 是总臂数，$T$ 是时间轴。</li>
<li>results: 该 paper 获得了理论性能，增量误差 upper bounded by $\bigO(kN^2\sqrt{T\log T})$.<details>
<summary>Abstract</summary>
This paper presents an efficient algorithm to solve the sleeping bandit with multiple plays problem in the context of an online recommendation system. The problem involves bounded, adversarial loss and unknown i.i.d. distributions for arm availability. The proposed algorithm extends the sleeping bandit algorithm for single arm selection and is guaranteed to achieve theoretical performance with regret upper bounded by $\bigO(kN^2\sqrt{T\log T})$, where $k$ is the number of arms selected per time step, $N$ is the total number of arms, and $T$ is the time horizon.
</details>
<details>
<summary>摘要</summary>
（本文提出了一种有效的算法，用于解决在线推荐系统中的睡着投注问题。该问题包括 bounded,  adversarial 损失以及 unknown i.i.d. 分布 дляarm availability。提出的算法基于单个arm选择的睡着投注算法，并且能够保证理论性能， regret upper bounded by $\bigO(kN^2\sqrt{T\log T})$，where $k$ is the number of arms selected per time step, $N$ is the total number of arms, and $T$ is the time horizon。）
</details></li>
</ul>
<hr>
<h2 id="Controlling-the-Inductive-Bias-of-Wide-Neural-Networks-by-Modifying-the-Kernel’s-Spectrum"><a href="#Controlling-the-Inductive-Bias-of-Wide-Neural-Networks-by-Modifying-the-Kernel’s-Spectrum" class="headerlink" title="Controlling the Inductive Bias of Wide Neural Networks by Modifying the Kernel’s Spectrum"></a>Controlling the Inductive Bias of Wide Neural Networks by Modifying the Kernel’s Spectrum</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14531">http://arxiv.org/abs/2307.14531</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amnon Geifman, Daniel Barzilai, Ronen Basri, Meirav Galun</li>
<li>for: 该论文主要目的是提出一种 modify 宽神经网络的方法，以便根据任务需要调整宽神经网络的学习偏好。</li>
<li>methods: 该论文提出了一种新的构造kernel的方法，称为Modified Spectrum Kernels（MSK），可以用于 aproximate 具有愿景值的kernel。此外，该论文还提出了一种基于宽神经网络和Neural Tangent Kernels的对偶方法，称为预conditioned gradient descent方法，可以改变Gradient Descent算法的轨迹。</li>
<li>results: 该论文的实验结果表明，使用Modified Spectrum Kernels和预conditioned gradient descent方法可以在一些情况下获得 polynomial 和 exponential 的训练速度提升，而不会改变最终解。此外，该方法也是 computationally efficient 和简单实现的。<details>
<summary>Abstract</summary>
Wide neural networks are biased towards learning certain functions, influencing both the rate of convergence of gradient descent (GD) and the functions that are reachable with GD in finite training time. As such, there is a great need for methods that can modify this bias according to the task at hand. To that end, we introduce Modified Spectrum Kernels (MSKs), a novel family of constructed kernels that can be used to approximate kernels with desired eigenvalues for which no closed form is known. We leverage the duality between wide neural networks and Neural Tangent Kernels and propose a preconditioned gradient descent method, which alters the trajectory of GD. As a result, this allows for a polynomial and, in some cases, exponential training speedup without changing the final solution. Our method is both computationally efficient and simple to implement.
</details>
<details>
<summary>摘要</summary>
广阶层神经网络具有倾向于学习特定函数的倾向，这影响了梯度下降（GD）的速度和训练时间内可到的函数。因此，有一个很大的需求，即可以根据任务改变这种倾向。为了解决这个问题，我们介绍Modified Spectrum Kernels（MSK），一种新的建构kernel的家族，可以用来 aproximate kernel with desired eigenvalues，即无法known的closed form。我们利用广阶层神经网络和Neural Tangent Kernels的 dual性，提出预调corrected gradient descent方法，这个方法可以改变GD的轨迹。因此，这可以实现 polynomial 和，在一些情况下，exponential training speedup，而不需要变更最终解。我们的方法具有computational efficiency和简单实现。
</details></li>
</ul>
<hr>
<h2 id="Optimal-Estimation-in-Mixed-Membership-Stochastic-Block-Models"><a href="#Optimal-Estimation-in-Mixed-Membership-Stochastic-Block-Models" class="headerlink" title="Optimal Estimation in Mixed-Membership Stochastic Block Models"></a>Optimal Estimation in Mixed-Membership Stochastic Block Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14530">http://arxiv.org/abs/2307.14530</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fedor Noskov, Maxim Panov</li>
<li>for: 研究混合成员 Stochastic Block Model（MMSB），用于描述图像中的 overlap community 结构。</li>
<li>methods: 比较不同方法的可重建性，并提出一种新的估计器，实现最小最大 Lower Bound（LB）。</li>
<li>results: 在一系列实验中，证明了新估计器的可靠性和高效性。<details>
<summary>Abstract</summary>
Community detection is one of the most critical problems in modern network science. Its applications can be found in various fields, from protein modeling to social network analysis. Recently, many papers appeared studying the problem of overlapping community detection, where each node of a network may belong to several communities. In this work, we consider Mixed-Membership Stochastic Block Model (MMSB) first proposed by Airoldi et al. (2008). MMSB provides quite a general setting for modeling overlapping community structure in graphs. The central question of this paper is to reconstruct relations between communities given an observed network. We compare different approaches and establish the minimax lower bound on the estimation error. Then, we propose a new estimator that matches this lower bound. Theoretical results are proved under fairly general conditions on the considered model. Finally, we illustrate the theory in a series of experiments.
</details>
<details>
<summary>摘要</summary>
社区探测是现代网络科学中最关键的问题之一。它的应用可以在各个领域找到，从蛋白质模型到社会网络分析。最近，许多论文研究了过lapping community detection问题，其中每个网络节点可能属于多个社区。在这项工作中，我们考虑了Airoldi等人于2008年提出的混合会员随机块模型（MMSB）。MMSB提供了一个非常通用的社区结构模型化图的方法。我们的中心问题是根据观察网络重建社区之间的关系。我们比较了不同的方法，并证明了最小最大下界 Error的下界。然后，我们提出了一个新的估计器，与这个下界匹配。我们的理论结果在较为通用的模型假设下得到了证明。最后，我们在一系列实验中证明了我们的理论。
</details></li>
</ul>
<hr>
<h2 id="Function-Value-Learning-Adaptive-Learning-Rates-Based-on-the-Polyak-Stepsize-and-Function-Splitting-in-ERM"><a href="#Function-Value-Learning-Adaptive-Learning-Rates-Based-on-the-Polyak-Stepsize-and-Function-Splitting-in-ERM" class="headerlink" title="Function Value Learning: Adaptive Learning Rates Based on the Polyak Stepsize and Function Splitting in ERM"></a>Function Value Learning: Adaptive Learning Rates Based on the Polyak Stepsize and Function Splitting in ERM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14528">http://arxiv.org/abs/2307.14528</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guillaume Garrigos, Robert M. Gower, Fabian Schaipp</li>
<li>For: The paper focuses on solving a finite sum-of-terms problem, also known as empirical risk minimization, using stochastic gradient descent (SGD) with an adaptive step size.* Methods: The paper proposes two variants of SGD, called $\texttt{SPS}<em>+$ and $\texttt{FUVAL}$, which make use of the sampled loss values and gradually learn the loss values at optimality.* Results: The paper shows that $\texttt{SPS}</em>+$ achieves the best known rates of convergence for SGD in the Lipschitz non-smooth, but the new $\texttt{FUVAL}$ method does not offer any clear theoretical or practical advantage over SGD.Here are the three information points in Simplified Chinese text:* For: 本文解决了一个finite sum-of-terms problem，也就是empirical risk minimization，使用stochastic gradient descent（SGD）的adaptive step size。* Methods: 本文提出了两种SGD变种，即$\texttt{SPS}<em>+$和$\texttt{FUVAL}$，它们利用样本损失值来进行优化。* Results: 本文证明了$\texttt{SPS}</em>+$在Lipschitz non-smooth中达到了最佳known的迭代速率，但新的$\texttt{FUVAL}$方法并没有明显的理论或实践优势。<details>
<summary>Abstract</summary>
Here we develop variants of SGD (stochastic gradient descent) with an adaptive step size that make use of the sampled loss values. In particular, we focus on solving a finite sum-of-terms problem, also known as empirical risk minimization. We first detail an idealized adaptive method called $\texttt{SPS}_+$ that makes use of the sampled loss values and assumes knowledge of the sampled loss at optimality. This $\texttt{SPS}_+$ is a minor modification of the SPS (Stochastic Polyak Stepsize) method, where the step size is enforced to be positive. We then show that $\texttt{SPS}_+$ achieves the best known rates of convergence for SGD in the Lipschitz non-smooth. We then move onto to develop $\texttt{FUVAL}$, a variant of $\texttt{SPS}_+$ where the loss values at optimality are gradually learned, as opposed to being given. We give three viewpoints of $\texttt{FUVAL}$, as a projection based method, as a variant of the prox-linear method, and then as a particular online SGD method. We then present a convergence analysis of $\texttt{FUVAL}$ and experimental results. The shortcomings of our work is that the convergence analysis of $\texttt{FUVAL}$ shows no advantage over SGD. Another shortcomming is that currently only the full batch version of $\texttt{FUVAL}$ shows a minor advantages of GD (Gradient Descent) in terms of sensitivity to the step size. The stochastic version shows no clear advantage over SGD. We conjecture that large mini-batches are required to make $\texttt{FUVAL}$ competitive.   Currently the new $\texttt{FUVAL}$ method studied in this paper does not offer any clear theoretical or practical advantage. We have chosen to make this draft available online nonetheless because of some of the analysis techniques we use, such as the non-smooth analysis of $\texttt{SPS}_+$, and also to show an apparently interesting approach that currently does not work.
</details>
<details>
<summary>摘要</summary>
我们在这里开发出了SGD（测量函数下降）的变体，具有适应步长的优点。具体来说，我们专注于解决一个总和形式的问题，也就是一个empirical risk minimization。我们首先介绍了一个理想化的自适应方法，called $\texttt{SPS}_+$,它使用抽象的损失值，并假设知道抽象损失的最佳值。这个 $\texttt{SPS}_+ $ 是 SPS（ Stochastic Polyak Stepsize）方法的小修改，步长强制为正。我们随后证明了 $\texttt{SPS}_+ $ 在 Lipschitz 非均匀中的最佳知识率。然后我们开发了 $\texttt{FUVAL} $, 这是 $\texttt{SPS}_+ $ 的一个改进版本，损失值在最佳值 Gradually 学习，而不是直接知道。我们从三个不同的角度来探讨 $\texttt{FUVAL} $, 分别是投影基于方法、变形的 prox-linear 方法和在线 SGD 方法。我们随后提供了 $\texttt{FUVAL} $ 的内部分析和实验结果。我们的研究的缺点是 $\texttt{FUVAL} $ 的内部分析不会提供任何优点，而且在某些情况下，SGD 可能比 $\texttt{FUVAL} $ 更为稳定。此外，目前只有全批量版本的 $\texttt{FUVAL} $ 表现出一定的优点，而测量版本则未能获得明显的优点。我们推测需要大小批量才能使 $\texttt{FUVAL} $ 竞争。总之，我们的新方法 $\texttt{FUVAL} $ 目前无法提供任何明显的理论或实践优点。不过，我们使用了一些有趣的分析技巧，例如非均匀分析 $\texttt{SPS}_+ $，以及展示了一个可能不太有用的方法。因此，我们选择发布这份草稿，以便在未来进一步探索这个方向。
</details></li>
</ul>
<hr>
<h2 id="Open-Problems-in-Computer-Vision-for-Wilderness-SAR-and-The-Search-for-Patricia-Wu-Murad"><a href="#Open-Problems-in-Computer-Vision-for-Wilderness-SAR-and-The-Search-for-Patricia-Wu-Murad" class="headerlink" title="Open Problems in Computer Vision for Wilderness SAR and The Search for Patricia Wu-Murad"></a>Open Problems in Computer Vision for Wilderness SAR and The Search for Patricia Wu-Murad</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14527">http://arxiv.org/abs/2307.14527</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/crasar/wisar">https://github.com/crasar/wisar</a></li>
<li>paper_authors: Thomas Manzini, Robin Murphy</li>
<li>for: 这篇论文探讨了在日本 Wu-Murad 野外搜救（WSAR）活动中应用两种计算机视觉系统，即可靠的DET 超vised学习模型和无监督的RX spectral分类器，并发现了3个未来研究的方向。</li>
<li>methods: 这篇论文使用了98.9 GB的飞行器影像数据，并应用了EfficientDET 建模和RX spectral分类器来检测缺失人员。</li>
<li>results: 论文发现，现有的方法中只有3种（2个无监督的和1个结构不确定）在实际WSAR操作中被使用，并且选择了EfficientDET 建模和RX spectral分类器作为最佳选择。然而，实际应用中，EfficientDET 模型具有 statistically 等效的性能，但它在实际情况中存在许多假阳性（如 mistakenly 识别树枝和石头为人）和假负性（如不能识别搜救队成员）的问题。<details>
<summary>Abstract</summary>
This paper details the challenges in applying two computer vision systems, an EfficientDET supervised learning model and the unsupervised RX spectral classifier, to 98.9 GB of drone imagery from the Wu-Murad wilderness search and rescue (WSAR) effort in Japan and identifies 3 directions for future research. There have been at least 19 proposed approaches and 3 datasets aimed at locating missing persons in drone imagery, but only 3 approaches (2 unsupervised and 1 of an unknown structure) are referenced in the literature as having been used in an actual WSAR operation. Of these proposed approaches, the EfficientDET architecture and the unsupervised spectral RX classifier were selected as the most appropriate for this setting. The EfficientDET model was applied to the HERIDAL dataset and despite achieving performance that is statistically equivalent to the state-of-the-art, the model fails to translate to the real world in terms of false positives (e.g., identifying tree limbs and rocks as people), and false negatives (e.g., failing to identify members of the search team). The poor results in practice for algorithms that showed good results on datasets suggest 3 areas of future research: more realistic datasets for wilderness SAR, computer vision models that are capable of seamlessly handling the variety of imagery that can be collected during actual WSAR operations, and better alignment on performance measures.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>More realistic datasets for wilderness SAR: The current datasets used for training and testing computer vision models may not accurately reflect the real-world conditions and variability of imagery collected during actual WSAR operations.2. Computer vision models that can handle diverse imagery: The models need to be able to seamlessly handle the variety of imagery that can be collected during actual WSAR operations, including different lighting conditions, weather, and terrain.3. Better alignment on performance measures: The performance of the models needs to be evaluated using relevant and meaningful metrics that reflect the specific goals and requirements of WSAR operations.The paper also discusses the limitations of the EfficientDET model and the unsupervised spectral RX classifier, including their poor performance in real-world scenarios due to false positives (identifying tree limbs and rocks as people) and false negatives (failing to identify members of the search team).</details></li>
</ol>
<hr>
<h2 id="A-new-algorithm-for-Subgroup-Set-Discovery-based-on-Information-Gain"><a href="#A-new-algorithm-for-Subgroup-Set-Discovery-based-on-Information-Gain" class="headerlink" title="A new algorithm for Subgroup Set Discovery based on Information Gain"></a>A new algorithm for Subgroup Set Discovery based on Information Gain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15089">http://arxiv.org/abs/2307.15089</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Gómez-Bravo, Aaron García, Guillermo Vigueras, Belén Ríos, Alejandro Rodríguez-González</li>
<li>For: The paper aims to propose a new pattern discovery algorithm called Information Gained Subgroup Discovery (IGSD) to address the limitations of state-of-the-art pattern discovery algorithms.* Methods: The IGSD algorithm combines Information Gain (IG) and Odds Ratio (OR) as multi-criteria for pattern selection. It also uses a novel approach to explore the subgroup space and evaluate the discovered patterns.* Results: The paper evaluates the performance of IGSD with two state-of-the-art pattern discovery algorithms (FSSD and SSD++) on 11 datasets. The results show that IGSD provides more reliable patterns and better agreement with domain experts than the other two algorithms. Additionally, IGSD provides better OR values, indicating a higher dependence between patterns and targets.Here’s the simplified Chinese text for the three key information points:*  для：本文提出了一种新的模式发现算法 called Information Gained Subgroup Discovery（IGSD），以解决现有模式发现算法的限制。* 方法：IGSD算法将信息增量（IG）和很大比率（OR）作为多个标准来选择模式。它还使用了一种新的方法来探索 subgroup空间和评估发现的模式。* 结果：本文对IGSD算法与两种现有模式发现算法（FSSD和SSD++）在11个数据集上进行了性能评估。结果显示，IGSD算法提供了更可靠的模式和与领域专家的一致性更高。此外，IGSD算法提供了更高的OR值，表明模式和目标之间的依赖性更高。<details>
<summary>Abstract</summary>
Pattern discovery is a machine learning technique that aims to find sets of items, subsequences, or substructures that are present in a dataset with a higher frequency value than a manually set threshold. This process helps to identify recurring patterns or relationships within the data, allowing for valuable insights and knowledge extraction. In this work, we propose Information Gained Subgroup Discovery (IGSD), a new SD algorithm for pattern discovery that combines Information Gain (IG) and Odds Ratio (OR) as a multi-criteria for pattern selection. The algorithm tries to tackle some limitations of state-of-the-art SD algorithms like the need for fine-tuning of key parameters for each dataset, usage of a single pattern search criteria set by hand, usage of non-overlapping data structures for subgroup space exploration, and the impossibility to search for patterns by fixing some relevant dataset variables. Thus, we compare the performance of IGSD with two state-of-the-art SD algorithms: FSSD and SSD++. Eleven datasets are assessed using these algorithms. For the performance evaluation, we also propose to complement standard SD measures with IG, OR, and p-value. Obtained results show that FSSD and SSD++ algorithms provide less reliable patterns and reduced sets of patterns than IGSD algorithm for all datasets considered. Additionally, IGSD provides better OR values than FSSD and SSD++, stating a higher dependence between patterns and targets. Moreover, patterns obtained for one of the datasets used, have been validated by a group of domain experts. Thus, patterns provided by IGSD show better agreement with experts than patterns obtained by FSSD and SSD++ algorithms. These results demonstrate the suitability of the IGSD as a method for pattern discovery and suggest that the inclusion of non-standard SD metrics allows to better evaluate discovered patterns.
</details>
<details>
<summary>摘要</summary>
“ patrern 发现”是一种机器学习技术，旨在找到数据集中出现频率较高的项集、 subsequences 或 substructures。这个过程可以帮助发现数据中的循环模式或关系，从而提供有价值的发现和知识提取。在这个工作中，我们提出了一种新的 patrern 发现算法，即 Information Gained Subgroup Discovery（IGSD）。该算法结合信息增益（IG）和偶极率（OR）作为多个评价标准，用于 patrern 选择。该算法希望解决现有 patrern 发现算法的一些局限性，如手动设置的关键参数、使用单一 patrern 搜索标准、非重叠数据结构等。因此，我们与现有 patrern 发现算法进行比较，包括 FSSD 和 SSD++。我们对 Eleven 个数据集进行了这些算法的评价。为了评价性能，我们还提出了一些不同于标准 patrern 搜索度量的方式，包括 IG、OR 和 p-value。结果显示，FSSD 和 SSD++ 算法提供的 patrern 较为不可靠，而 IGSD 算法对所有数据集都提供了更好的 patrern。此外，IGSD 算法对 patrern 和目标变量之间的相互关系提供了更高的依赖性。此外，IGSD 算法对一个数据集进行了验证，并得到了域专家的认可。因此，IGSD 算法提供的 patrern 更好地匹配专家的意见。这些结果表明 IGSD 算法是一种适合 patrern 发现的方法，并且包括非标准 patrern 搜索度量可以更好地评价发现的 patrern。
</details></li>
</ul>
<hr>
<h2 id="Bug-Characterization-in-Machine-Learning-based-Systems"><a href="#Bug-Characterization-in-Machine-Learning-based-Systems" class="headerlink" title="Bug Characterization in Machine Learning-based Systems"></a>Bug Characterization in Machine Learning-based Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14512">http://arxiv.org/abs/2307.14512</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ml-bugs-2022/replication-package">https://github.com/ml-bugs-2022/replication-package</a></li>
<li>paper_authors: Mohammad Mehdi Morovati, Amin Nikanjam, Florian Tambon, Foutse Khomh, Zhen Ming, Jiang</li>
<li>for: This paper aims to investigate the characteristics of bugs in Machine Learning (ML)-based software systems and the differences between ML and non-ML bugs from a maintenance viewpoint.</li>
<li>methods: The paper uses a dataset of 447,948 GitHub repositories that use one of the three most popular ML frameworks (TensorFlow, Keras, and PyTorch) to extract 300 repositories with the highest number of closed issues. The authors then manually inspect the extracted repositories to exclude non-ML-based systems, and investigate 386 sampled reported issues to determine whether they affect ML components or not.</li>
<li>results: The paper finds that nearly half of the real issues reported in ML-based systems are ML bugs, indicating that ML components are more error-prone than non-ML components. The authors also identify the root causes, symptoms, and required fixing time for 109 identified ML bugs, and find that ML bugs have significantly different characteristics compared to non-ML bugs in terms of the complexity of bug-fixing. The results suggest that fixing ML bugs is more costly and ML components are more error-prone compared to non-ML bugs and non-ML components respectively, highlighting the importance of paying attention to the reliability of ML components in ML-based systems.<details>
<summary>Abstract</summary>
Rapid growth of applying Machine Learning (ML) in different domains, especially in safety-critical areas, increases the need for reliable ML components, i.e., a software component operating based on ML. Understanding the bugs characteristics and maintenance challenges in ML-based systems can help developers of these systems to identify where to focus maintenance and testing efforts, by giving insights into the most error-prone components, most common bugs, etc. In this paper, we investigate the characteristics of bugs in ML-based software systems and the difference between ML and non-ML bugs from the maintenance viewpoint. We extracted 447,948 GitHub repositories that used one of the three most popular ML frameworks, i.e., TensorFlow, Keras, and PyTorch. After multiple filtering steps, we select the top 300 repositories with the highest number of closed issues. We manually investigate the extracted repositories to exclude non-ML-based systems. Our investigation involved a manual inspection of 386 sampled reported issues in the identified ML-based systems to indicate whether they affect ML components or not. Our analysis shows that nearly half of the real issues reported in ML-based systems are ML bugs, indicating that ML components are more error-prone than non-ML components. Next, we thoroughly examined 109 identified ML bugs to identify their root causes, symptoms, and calculate their required fixing time. The results also revealed that ML bugs have significantly different characteristics compared to non-ML bugs, in terms of the complexity of bug-fixing (number of commits, changed files, and changed lines of code). Based on our results, fixing ML bugs are more costly and ML components are more error-prone, compared to non-ML bugs and non-ML components respectively. Hence, paying a significant attention to the reliability of the ML components is crucial in ML-based systems.
</details>
<details>
<summary>摘要</summary>
Machine Learning (ML) 在不同领域的快速应用导致了可靠 ML 组件的需求增加，即基于 ML 的软件组件。了解 ML 系统中 bug 的特点和维护挑战可以帮助 ML 系统开发者identify где要集中维护和测试努力，提供了关于最常出现的bug、最常出现的错误等信息。在这篇论文中，我们研究了 ML 系统中 bug 的特点和非 ML 系统中 bug 的区别从维护角度来 investigate。我们从 GitHub 上抽取了使用最Popular ML 框架之一的300个仓库，并经过多个筛选步骤后，选择了最高数量关闭 Issue 的仓库。我们手动验证了提取的仓库，以确保它们是 ML 基于系统。我们的调查包括对386个样本问题的手动检查，以确定它们是否affect ML 组件。我们的分析结果表明，ML 系统中 Reported 的实际问题大约占 ML 系统总数的46.7%，表明 ML 组件比非 ML 组件更容易出现错误。接着，我们对109个 ID 为 ML 错误的问题进行了详细分析，以确定其根本原因、症状和修复时间。结果还表明，ML 错误和非 ML 错误在修复复杂性、修复时间等方面有很大差异。根据我们的结果，修复 ML 错误需要更多的时间和精力，而 ML 组件也更容易出现错误。因此，在 ML 基于系统中，需要付出更大的注意力来确保 ML 组件的可靠性。
</details></li>
</ul>
<hr>
<h2 id="A-Predictive-Model-of-Digital-Information-Engagement-Forecasting-User-Engagement-With-English-Words-by-Incorporating-Cognitive-Biases-Computational-Linguistics-and-Natural-Language-Processing"><a href="#A-Predictive-Model-of-Digital-Information-Engagement-Forecasting-User-Engagement-With-English-Words-by-Incorporating-Cognitive-Biases-Computational-Linguistics-and-Natural-Language-Processing" class="headerlink" title="A Predictive Model of Digital Information Engagement: Forecasting User Engagement With English Words by Incorporating Cognitive Biases, Computational Linguistics and Natural Language Processing"></a>A Predictive Model of Digital Information Engagement: Forecasting User Engagement With English Words by Incorporating Cognitive Biases, Computational Linguistics and Natural Language Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14500">http://arxiv.org/abs/2307.14500</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nimrod Dvir, Elaine Friedman, Suraj Commuri, Fan yang, Jennifer Romano</li>
<li>for: 这种研究旨在开发一种用于数字信息参与度（IE）的预测模型，即READ模型，该模型基于汇集了主要认知偏见和计算语言学的理论框架，以提供一种多维度的信息参与度观察。</li>
<li>methods: 该研究使用了50个随机选择的同义词对（共100个词）从WordNet数据库，通过在线调查（参与者数为80,500人）测量这些词的参与度，以计算READ属性。</li>
<li>results: 研究发现，READ模型能够准确预测一个词的参与度，并在84%的 случа子中能够 correctly distinguish 词与其同义词之间的参与度。该模型在不同领域，如商业、教育、政府和医疗等领域，可能有效地提高内容参与度和AI语言模型的发展。<details>
<summary>Abstract</summary>
This study introduces and empirically tests a novel predictive model for digital information engagement (IE) - the READ model, an acronym for the four pivotal attributes of engaging information: Representativeness, Ease-of-use, Affect, and Distribution. Conceptualized within the theoretical framework of Cumulative Prospect Theory, the model integrates key cognitive biases with computational linguistics and natural language processing to develop a multidimensional perspective on information engagement. A rigorous testing protocol was implemented, involving 50 randomly selected pairs of synonymous words (100 words in total) from the WordNet database. These words' engagement levels were evaluated through a large-scale online survey (n = 80,500) to derive empirical IE metrics. The READ attributes for each word were then computed and their predictive efficacy examined. The findings affirm the READ model's robustness, accurately predicting a word's IE level and distinguishing the more engaging word from a pair of synonyms with an 84% accuracy rate. The READ model's potential extends across various domains, including business, education, government, and healthcare, where it could enhance content engagement and inform AI language model development and generative text work. Future research should address the model's scalability and adaptability across different domains and languages, thereby broadening its applicability and efficacy.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="HUGE-Huge-Unsupervised-Graph-Embeddings-with-TPUs"><a href="#HUGE-Huge-Unsupervised-Graph-Embeddings-with-TPUs" class="headerlink" title="HUGE: Huge Unsupervised Graph Embeddings with TPUs"></a>HUGE: Huge Unsupervised Graph Embeddings with TPUs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14490">http://arxiv.org/abs/2307.14490</a></li>
<li>repo_url: None</li>
<li>paper_authors: Brandon Mayer, Anton Tsitsulin, Hendrik Fichtenberger, Jonathan Halcrow, Bryan Perozzi</li>
<li>for: 这篇论文是为了快速分析大规模图像而设计的，以便解决后续机器学习任务。</li>
<li>methods: 论文使用了Tensor Processing Units（TPUs）和可配置的高带宽内存来实现高性能图嵌入架构，以解决大规模图像的嵌入问题。</li>
<li>results: 论文验证了嵌入空间质量在真实和 sintetic 大规模数据集上，并达到了高性能。<details>
<summary>Abstract</summary>
Graphs are a representation of structured data that captures the relationships between sets of objects. With the ubiquity of available network data, there is increasing industrial and academic need to quickly analyze graphs with billions of nodes and trillions of edges. A common first step for network understanding is Graph Embedding, the process of creating a continuous representation of nodes in a graph. A continuous representation is often more amenable, especially at scale, for solving downstream machine learning tasks such as classification, link prediction, and clustering. A high-performance graph embedding architecture leveraging Tensor Processing Units (TPUs) with configurable amounts of high-bandwidth memory is presented that simplifies the graph embedding problem and can scale to graphs with billions of nodes and trillions of edges. We verify the embedding space quality on real and synthetic large-scale datasets.
</details>
<details>
<summary>摘要</summary>
GRAPH Embedding 是一种将图像转换为连续表示的技术，以便更好地解决大规模图像的下游机器学习任务，如分类、链接预测和团 clustering。在大量网络数据的时代，快速分析大规模图像变得越来越重要。我们提出了一种高性能的图 Embedding 架构，利用 tensor Processing Units (TPUs) 和可配置的高带宽内存，可以简化图 Embedding 问题，并可扩展到百亿个节点和万亿个边的图像。我们对真实和 sintetic 大规模数据进行了验证，以证明 embedding 空间质量。Note: "GRAPH" is written in capital letters in Simplified Chinese to emphasize the importance of the concept.
</details></li>
</ul>
<hr>
<h2 id="Role-of-Image-Acquisition-and-Patient-Phenotype-Variations-in-Automatic-Segmentation-Model-Generalization"><a href="#Role-of-Image-Acquisition-and-Patient-Phenotype-Variations-in-Automatic-Segmentation-Model-Generalization" class="headerlink" title="Role of Image Acquisition and Patient Phenotype Variations in Automatic Segmentation Model Generalization"></a>Role of Image Acquisition and Patient Phenotype Variations in Automatic Segmentation Model Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14482">http://arxiv.org/abs/2307.14482</a></li>
<li>repo_url: None</li>
<li>paper_authors: Timothy L. Kline, Sumana Ramanathan, Harrison C. Gottlich, Panagiotis Korfiatis, Adriana V. Gregory</li>
<li>for: 这个研究是为了评估自动医疗影像分割模型在不同频谱和疾病类型上的性能和泛化能力。</li>
<li>methods: 研究使用了多种数据集，包括非对照和对照肝脏CT影像数据，并对这些数据进行了100个示例的训练和验证，以便分割肝脏、胆囊和脾脏。最终的模型被测试在100个非对照PKD病人的CT影像上。性能被评估使用了 dice相似度、Jacard相似度、TPR和精度。</li>
<li>results: 研究发现，使用更广泛的数据集可以提高模型的泛化性能和外域性能，无需额外训练或特定的疾病类型。例如，模型通过在25%的数据集上训练，与仅使用域内数据进行训练相同的 dice相似度。I hope that helps!<details>
<summary>Abstract</summary>
Purpose: This study evaluated the out-of-domain performance and generalization capabilities of automated medical image segmentation models, with a particular focus on adaptation to new image acquisitions and disease type.   Materials: Datasets from both non-contrast and contrast-enhanced abdominal CT scans of healthy patients and those with polycystic kidney disease (PKD) were used. A total of 400 images (100 non-contrast controls, 100 contrast controls, 100 non-contrast PKD, 100 contrast PKD) were utilized for training/validation of models to segment kidneys, livers, and spleens, and the final models were then tested on 100 non-contrast CT images of patients affected by PKD. Performance was evaluated using Dice, Jaccard, TPR, and Precision.   Results: Models trained on a diverse range of data showed no worse performance than models trained exclusively on in-domain data when tested on in-domain data. For instance, the Dice similarity of the model trained on 25% from each dataset was found to be non-inferior to the model trained purely on in-domain data.   Conclusions: The results indicate that broader training examples significantly enhances model generalization and out-of-domain performance, thereby improving automated segmentation tools' applicability in clinical settings. The study's findings provide a roadmap for future research to adopt a data-centric approach in medical image AI model development.
</details>
<details>
<summary>摘要</summary>
目的：本研究评估了自动医疗图像分割模型的域外性和总体可扩展性，尤其是适应新图像获取和疾病类型的适应性。材料：来自非对照和增强的腹部CT扫描图像的健康患者和肾脏癌病（PKD）的数据集被使用。总共使用400张图像（100张非对照控制、100张对照控制、100张非对照PKD、100张对照PKD）进行模型训练/验证，并将最终模型测试在100张非对照CT图像上。性能被评估使用 dice、jaccard、TPR和精度。结果：模型训练在多样化的数据上表现和专门训练在域内数据上无分别性能。例如，模型训练使用25%的数据从每个数据集得到的 dice相似性与专门训练在域内数据上的模型相似。结论：结果表明，更广泛的训练示例可以显著提高模型的总体可扩展性和域外性，因此提高自动分割工具在临床应用中的可靠性。这项研究的结果为未来医疗图像AI模型开发提供了一个路线图。
</details></li>
</ul>
<hr>
<h2 id="Equitable-Time-Varying-Pricing-Tariff-Design-A-Joint-Learning-and-Optimization-Approach"><a href="#Equitable-Time-Varying-Pricing-Tariff-Design-A-Joint-Learning-and-Optimization-Approach" class="headerlink" title="Equitable Time-Varying Pricing Tariff Design: A Joint Learning and Optimization Approach"></a>Equitable Time-Varying Pricing Tariff Design: A Joint Learning and Optimization Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15088">http://arxiv.org/abs/2307.15088</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liudong Chen, Bolun Xu</li>
<li>for: 这个论文的目的是设计合理的时间变化价格政策，以激励消费者减少电力峰值需求，同时保护低收入消费者免受价格涨升的影响。</li>
<li>methods: 该论文提出一种基于学习联合标定优化方法，通过历史价格和需求响应数据来捕捉高维和非线性消费者价格响应行为。</li>
<li>results: 模拟使用实际消费者数据显示，我们的公平价格政策能够保护低收入消费者免受价格涨升的影响，同时有效激励消费者减少峰值需求，使得供应商公司能够实现收益回报。<details>
<summary>Abstract</summary>
Time-varying pricing tariffs incentivize consumers to shift their electricity demand and reduce costs, but may increase the energy burden for consumers with limited response capability. The utility must thus balance affordability and response incentives when designing these tariffs by considering consumers' response expectations. This paper proposes a joint learning-based identification and optimization method to design equitable time-varying tariffs. Our proposed method encodes historical prices and demand response data into a recurrent neural network (RNN) to capture high-dimensional and non-linear consumer price response behaviors. We then embed the RNN into the tariff design optimization, formulating a non-linear optimization problem with a quadratic objective. We propose a gradient-based solution method that achieves fast and scalable computation. Simulation using real-world consumer data shows that our equitable tariffs protect low-income consumers from price surges while effectively motivating consumers to reduce peak demand. The method also ensures revenue recovery for the utility company and achieves robust performance against demand response uncertainties and prediction errors.
</details>
<details>
<summary>摘要</summary>
时间变化的价格批价可以鼓励消费者调整电力需求，从而降低成本，但可能增加有限响应能力的消费者的能源负担。公司因此需要平衡可持续性和响应奖励，当设计时变化价格时。本文提出一种基于学习的同时标识和优化方法，用于设计公平的时变价格。我们使用循环神经网络（RNN）编码历史价格和需求响应数据，以捕捉高维和非线性的消费者价格响应行为。然后，我们将RNNembed到价格设计优化中，形成非线性优化问题的quadratic对象。我们提出一种梯度基于的解决方案，可以实现快速和可扩展的计算。通过使用实际的消费者数据进行模拟，我们发现我们的公平价格可以保护低收入消费者免受价格涨升，同时有效地鼓励消费者减少峰值需求。此外，我们的方法还确保了公司的收益恢复和对需求响应不确定性和预测错误的稳定性。
</details></li>
</ul>
<hr>
<h2 id="Limits-to-Reservoir-Learning"><a href="#Limits-to-Reservoir-Learning" class="headerlink" title="Limits to Reservoir Learning"></a>Limits to Reservoir Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14474">http://arxiv.org/abs/2307.14474</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anthony M. Polloreno</li>
<li>for: 这 paper 是研究机器学习的能力受到物理限制的工作。</li>
<li>methods: 作者使用信息处理容量（IPC）来衡量受到噪声的干扰下，某种特定的循环网络（即散射器）的性能下降。</li>
<li>results: 作者发现，IPC 是最多是一个多项式函数关系于系统大小 n，而且在噪声存在的情况下，这种干扰会使某种函数家族需要 exponential 数量的样本来学习。<details>
<summary>Abstract</summary>
In this work, we bound a machine's ability to learn based on computational limitations implied by physicality. We start by considering the information processing capacity (IPC), a normalized measure of the expected squared error of a collection of signals to a complete basis of functions. We use the IPC to measure the degradation under noise of the performance of reservoir computers, a particular kind of recurrent network, when constrained by physical considerations. First, we show that the IPC is at most a polynomial in the system size $n$, even when considering the collection of $2^n$ possible pointwise products of the $n$ output signals. Next, we argue that this degradation implies that the family of functions represented by the reservoir requires an exponential number of samples to learn in the presence of the reservoir's noise. Finally, we conclude with a discussion of the performance of the same collection of $2^n$ functions without noise when being used for binary classification.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们约束机器学习的能力基于物理限制所含的计算限制。我们开始思考信息处理容量（IPC），一种正规化的函数收敛率的标准化量。我们使用IPC来度量噪声下机器学习性能的下降，特别是当受到物理限制时。首先，我们证明IPC在系统大小$n$上是最多一个多项式函数。接着，我们 argue that这种下降表明了由泵函数表示的家族需要很多样本来在噪声下学习。最后，我们讨论无噪声情况下同样的$2^n$函数在二分类 зада务中的表现。
</details></li>
</ul>
<hr>
<h2 id="What-Kinds-of-Contracts-Do-ML-APIs-Need"><a href="#What-Kinds-of-Contracts-Do-ML-APIs-Need" class="headerlink" title="What Kinds of Contracts Do ML APIs Need?"></a>What Kinds of Contracts Do ML APIs Need?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14465">http://arxiv.org/abs/2307.14465</a></li>
<li>repo_url: None</li>
<li>paper_authors: Samantha Syeda Khairunnesa, Shibbir Ahmed, Sayem Mohammad Imtiaz, Hridesh Rajan, Gary T. Leavens</li>
<li>For: This paper aims to identify the most commonly needed contracts for Machine Learning (ML) APIs and understand the root causes and effects of ML contract violations.* Methods: The authors conducted an empirical study of posts on Stack Overflow to extract 413 informal API specifications for the four most often-discussed ML libraries (TensorFlow, Scikit-learn, Keras, and PyTorch). They used these specifications to understand the common patterns of ML contract violations and the need for advanced ML software expertise to understand ML contracts.* Results: The study found that the most commonly needed contracts for ML APIs are checking constraints on single arguments of an API or on the order of API calls. The authors also noted a need to combine behavioral and temporal contract mining approaches to better understand ML APIs and design effective contract languages.<details>
<summary>Abstract</summary>
Recent work has shown that Machine Learning (ML) programs are error-prone and called for contracts for ML code. Contracts, as in the design by contract methodology, help document APIs and aid API users in writing correct code. The question is: what kinds of contracts would provide the most help to API users? We are especially interested in what kinds of contracts help API users catch errors at earlier stages in the ML pipeline. We describe an empirical study of posts on Stack Overflow of the four most often-discussed ML libraries: TensorFlow, Scikit-learn, Keras, and PyTorch. For these libraries, our study extracted 413 informal (English) API specifications. We used these specifications to understand the following questions. What are the root causes and effects behind ML contract violations? Are there common patterns of ML contract violations? When does understanding ML contracts require an advanced level of ML software expertise? Could checking contracts at the API level help detect the violations in early ML pipeline stages? Our key findings are that the most commonly needed contracts for ML APIs are either checking constraints on single arguments of an API or on the order of API calls. The software engineering community could employ existing contract mining approaches to mine these contracts to promote an increased understanding of ML APIs. We also noted a need to combine behavioral and temporal contract mining approaches. We report on categories of required ML contracts, which may help designers of contract languages.
</details>
<details>
<summary>摘要</summary>
We conducted an empirical study of posts on Stack Overflow about the four most commonly discussed ML libraries: TensorFlow, Scikit-learn, Keras, and PyTorch. For these libraries, we extracted 413 informal (English) API specifications. We used these specifications to answer the following questions:1. What are the root causes and effects of ML contract violations?2. Are there common patterns of ML contract violations?3. When does understanding ML contracts require an advanced level of ML software expertise?4. Can checking contracts at the API level help detect violations in early ML pipeline stages?Our key findings are that the most commonly needed contracts for ML APIs are:1. Checking constraints on single arguments of an API2. Checking the order of API callsWe also noted a need to combine behavioral and temporal contract mining approaches. Additionally, we categorized the required ML contracts, which may help designers of contract languages.
</details></li>
</ul>
<hr>
<h2 id="Training-Quantum-Boltzmann-Machines-with-Coresets"><a href="#Training-Quantum-Boltzmann-Machines-with-Coresets" class="headerlink" title="Training Quantum Boltzmann Machines with Coresets"></a>Training Quantum Boltzmann Machines with Coresets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14459">http://arxiv.org/abs/2307.14459</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joshua Viszlai, Teague Tomesh, Pranav Gokhale, Eric Anschuetz, Frederic T. Chong</li>
<li>for: 加速近期量子算法在半导体设备上的应用，使用核心集技术。</li>
<li>methods: 使用核心集取代全数据集，以降低训练时间。</li>
<li>results: 使用核心集可以大幅减少训练时间，减少量子计算机的计算时间。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Recent work has proposed and explored using coreset techniques for quantum algorithms that operate on classical data sets to accelerate the applicability of these algorithms on near-term quantum devices. We apply these ideas to Quantum Boltzmann Machines (QBM) where gradient-based steps which require Gibbs state sampling are the main computational bottleneck during training. By using a coreset in place of the full data set, we try to minimize the number of steps needed and accelerate the overall training time. In a regime where computational time on quantum computers is a precious resource, we propose this might lead to substantial practical savings. We evaluate this approach on 6x6 binary images from an augmented bars and stripes data set using a QBM with 36 visible units and 8 hidden units. Using an Inception score inspired metric, we compare QBM training times with and without using coresets.
</details>
<details>
<summary>摘要</summary>
近期的工作已经提出了和探索了使用核Set技术来加速近期quantum设备上的量子算法运行，以便在这些算法的应用中减少计算时间。我们在Quantum Boltzmann Machines（QBM）中应用这些想法，其中梯度based步骤的主要计算瓶颈在训练中是Gibbs状态抽样。通过使用核Set而不是全量数据集，我们尝试将训练时间缩短。在计算时间在量子计算机上是珍贵资源的情况下，我们建议这可能导致实质性的实用性级别。我们使用6x6的二进制图像从一个扩展的棒和条纹数据集，使用一个QBM的可见单元为36个，隐藏单元为8个进行训练。使用基于Inception metric的评价标准，我们比较了在使用核Set和没有使用核Set的情况下QBM训练时间。
</details></li>
</ul>
<hr>
<h2 id="Predictive-Maintenance-of-Armoured-Vehicles-using-Machine-Learning-Approaches"><a href="#Predictive-Maintenance-of-Armoured-Vehicles-using-Machine-Learning-Approaches" class="headerlink" title="Predictive Maintenance of Armoured Vehicles using Machine Learning Approaches"></a>Predictive Maintenance of Armoured Vehicles using Machine Learning Approaches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14453">http://arxiv.org/abs/2307.14453</a></li>
<li>repo_url: None</li>
<li>paper_authors: Prajit Sengupta, Anant Mehta, Prashant Singh Rana</li>
<li>for: 这个研究旨在提出一种基于预测维护的 ensemble 系统，用于预测 armoured 车的维护需求，以提高车辆的运作效率和可靠性。</li>
<li>methods: 该模型采用了多种模型，如光树落准、Random Forest、决策树、Extra Tree Classifier 和 Gradient Boosting，以准确预测车辆的维护需求。 plus 使用了 K-fold 十字验证和 TOPSIS 分析来评估模型的稳定性。</li>
<li>results: 结果显示，提议的系统具有98.93%的准确率、99.80%的精度和99.03%的回归率，能够有效预测车辆的维护需求，从而降低车辆的停机时间和提高运作效率。<details>
<summary>Abstract</summary>
Armoured vehicles are specialized and complex pieces of machinery designed to operate in high-stress environments, often in combat or tactical situations. This study proposes a predictive maintenance-based ensemble system that aids in predicting potential maintenance needs based on sensor data collected from these vehicles. The proposed model's architecture involves various models such as Light Gradient Boosting, Random Forest, Decision Tree, Extra Tree Classifier and Gradient Boosting to predict the maintenance requirements of the vehicles accurately. In addition, K-fold cross validation, along with TOPSIS analysis, is employed to evaluate the proposed ensemble model's stability. The results indicate that the proposed system achieves an accuracy of 98.93%, precision of 99.80% and recall of 99.03%. The algorithm can effectively predict maintenance needs, thereby reducing vehicle downtime and improving operational efficiency. Through comparisons between various algorithms and the suggested ensemble, this study highlights the potential of machine learning-based predictive maintenance solutions.
</details>
<details>
<summary>摘要</summary>
armored vehicles 是特殊化和复杂的机器设备，设计用于高压环境下运行，经常在战斗或战术情况下使用。本研究提出了一种基于预测维护的ensemble系统，用于预测这些车辆的维护需求。提议的模型体系包括了各种模型，如光梯度抛光、随机森林、决策树、附加树分类器和梯度抛光。此外， employ K-fold Cross Validation 和 TOPSIS分析来评估提议的ensemble模型的稳定性。结果表明，提议的系统实现了98.93%的准确率、99.80%的精度和99.03%的回归率。这个算法可以有效预测维护需求，从而减少车辆的停机时间，提高运作效率。通过对不同算法和建议的ensemble进行比较，本研究强调了机器学习基于预测维护解决方案的潜力。
</details></li>
</ul>
<hr>
<h2 id="VISPUR-Visual-Aids-for-Identifying-and-Interpreting-Spurious-Associations-in-Data-Driven-Decisions"><a href="#VISPUR-Visual-Aids-for-Identifying-and-Interpreting-Spurious-Associations-in-Data-Driven-Decisions" class="headerlink" title="VISPUR: Visual Aids for Identifying and Interpreting Spurious Associations in Data-Driven Decisions"></a>VISPUR: Visual Aids for Identifying and Interpreting Spurious Associations in Data-Driven Decisions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14448">http://arxiv.org/abs/2307.14448</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/picsolab/vispur">https://github.com/picsolab/vispur</a></li>
<li>paper_authors: Xian Teng, Yongsu Ahn, Yu-Ru Lin</li>
<li>for: 该论文旨在帮助人们在受到大数据和机器学习工具支持的情况下，更好地识别和理解干扰因素导致的假关系。</li>
<li>methods: 该论文提出了一种可见分析框架和一个人类中心的工作流程，用于检测和解决干扰因素导致的假关系。其中包括一个干扰因素仪表板，可以自动标识可能的干扰因素，以及一个 subgroup 视图器，可以视觉化和比较不同 subgroup 的各种模式，以避免因果混乱的误解。</li>
<li>results: 我们通过专家采访和控制的用户实验，证明了我们提出的 “de-paradox” 工作流程和设计的可见分析系统是有效的，帮助人们更好地识别和理解干扰因素导致的假关系，以及做出可识别的决策。<details>
<summary>Abstract</summary>
Big data and machine learning tools have jointly empowered humans in making data-driven decisions. However, many of them capture empirical associations that might be spurious due to confounding factors and subgroup heterogeneity. The famous Simpson's paradox is such a phenomenon where aggregated and subgroup-level associations contradict with each other, causing cognitive confusions and difficulty in making adequate interpretations and decisions. Existing tools provide little insights for humans to locate, reason about, and prevent pitfalls of spurious association in practice. We propose VISPUR, a visual analytic system that provides a causal analysis framework and a human-centric workflow for tackling spurious associations. These include a CONFOUNDER DASHBOARD, which can automatically identify possible confounding factors, and a SUBGROUP VIEWER, which allows for the visualization and comparison of diverse subgroup patterns that likely or potentially result in a misinterpretation of causality. Additionally, we propose a REASONING STORYBOARD, which uses a flow-based approach to illustrate paradoxical phenomena, as well as an interactive DECISION DIAGNOSIS panel that helps ensure accountable decision-making. Through an expert interview and a controlled user experiment, our qualitative and quantitative results demonstrate that the proposed "de-paradox" workflow and the designed visual analytic system are effective in helping human users to identify and understand spurious associations, as well as to make accountable causal decisions.
</details>
<details>
<summary>摘要</summary>
大数据和机器学习工具已经共同强化了人类在基于数据的决策中的能力。然而，许多其中捕捉到了偶合关系，这些关系可能因为干扰因素和 subgroup 多样性而是假的。例如，西мп逊的 парадокс是这种现象，其中总体和 subgroup 级别的关系相互矛盾，导致认知混乱和不能正确地解释和决策。现有工具提供的知识对 humans 来说太少，以至于无法在实践中找到、理解和避免偶合关系的坑。我们提出了 VISPUR，一个视觉分析系统，它提供了一种 causal 分析框架和一个人类中心的工作流程，用于解决偶合关系。这些包括一个 CONFOUNDER DASHBOARD，可以自动Identify possible confounding factors，以及一个 SUBGROUP VIEWER，可以将多个 subgroup 的各种模式视觉化和比较，以便更好地理解和分析 causality。此外，我们还提出了一个 REASONING STORYBOARD，使用流程方式来描述悖论现象，以及一个交互式的 DECISION DIAGNOSIS 面板，帮助确保决策是合理的。经过专家采访和控制的用户试验，我们的质量和量度结果表明，我们提出的 "de-paradox" 工作流程和设计的视觉分析系统都是有效的，帮助人类用户更好地找到、理解和解决偶合关系，以及做出负责任的 causal 决策。
</details></li>
</ul>
<hr>
<h2 id="Neural-Schrodinger-Bridge-with-Sinkhorn-Losses-Application-to-Data-driven-Minimum-Effort-Control-of-Colloidal-Self-assembly"><a href="#Neural-Schrodinger-Bridge-with-Sinkhorn-Losses-Application-to-Data-driven-Minimum-Effort-Control-of-Colloidal-Self-assembly" class="headerlink" title="Neural Schrödinger Bridge with Sinkhorn Losses: Application to Data-driven Minimum Effort Control of Colloidal Self-assembly"></a>Neural Schrödinger Bridge with Sinkhorn Losses: Application to Data-driven Minimum Effort Control of Colloidal Self-assembly</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14442">http://arxiv.org/abs/2307.14442</a></li>
<li>repo_url: None</li>
<li>paper_authors: Iman Nodozi, Charlie Yan, Mira Khare, Abhishek Halder, Ali Mesbah</li>
<li>for: 这个论文是为了研究溶解自组装的最小努力控制问题而写的。</li>
<li>methods: 这篇论文使用了一种名为“神经学桥”的数据驱动学习和控制框架，以解决一类通过扩展Schrödinger桥问题来处理溶解自组装控制问题。</li>
<li>results: 研究人员通过使用分子动力学模拟数据和神经网络来学习控制拖动和扩散系数，并使用这些系数来训练一个特定于这类控制问题的神经网络，以解决溶解自组装控制问题。<details>
<summary>Abstract</summary>
We show that the minimum effort control of colloidal self-assembly can be naturally formulated in the order-parameter space as a generalized Schr\"odinger bridge problem -- a class of fixed-horizon stochastic optimal control problems that originated in the works of Erwin Schr\"odinger in the early 1930s. In recent years, this class of problems has seen a resurgence of research activities in control and machine learning communities. Different from the existing literature on the theory and computation for such problems, the controlled drift and diffusion coefficients for colloidal self-assembly are typically non-affine in control, and are difficult to obtain from physics-based modeling. We deduce the conditions of optimality for such generalized problems, and show that the resulting system of equations is structurally very different from the existing results in a way that standard computational approaches no longer apply. Thus motivated, we propose a data-driven learning and control framework, named `neural Schr\"odinger bridge', to solve such generalized Schr\"odinger bridge problems by innovating on recent advances in neural networks. We illustrate the effectiveness of the proposed framework using a numerical case study of colloidal self-assembly. We learn the controlled drift and diffusion coefficients as two neural networks using molecular dynamics simulation data, and then use these two to train a third network with Sinkhorn losses designed for distributional endpoint constraints, specific for this class of control problems.
</details>
<details>
<summary>摘要</summary>
我们显示，材料自组装的最小努力控制可以自然地表示为一种通用的Schrödinger桥问题，这是Erwin Schrödinger在30年代初期提出的一类固定时间 horizon随机控制问题。在最近几年，这类问题在控制和机器学习领域中得到了新的研究活动。与现有Literature不同，控制拖拽和扩散系数对材料自组装通常是非线性的，从物理模型中难以获得。我们推导了这类总体最优条件，并发现其系统方程与现有结果 Structurally very different，使得标准计算方法不再适用。因此，我们提出了一种基于数据驱动学习和控制的框架，名为“神经Schrödinger桥”，用于解决这类通用Schrödinger桥问题。我们通过一个数字 caso study of colloidal self-assembly illustrate the effectiveness of the proposed framework，我们通过分子动力学模拟数据学习控制拖拽和扩散系数为两个神经网络，然后使用这两个网络来训练第三个网络，用Sinkhorn损失函数，特定于这类控制问题的分布端点约束。
</details></li>
</ul>
<hr>
<h2 id="Fixed-Integral-Neural-Networks"><a href="#Fixed-Integral-Neural-Networks" class="headerlink" title="Fixed Integral Neural Networks"></a>Fixed Integral Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14439">http://arxiv.org/abs/2307.14439</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryan Kortvelesy</li>
<li>for: 本研究旨在解决通常通过数值方法进行的神经网络中函数 интеграル计算的问题，提供一种能够确定神经网络函数 интеграル的方法。</li>
<li>methods: 本研究使用了一种基于约束的神经网络模型，其中约束直接应用于函数 интеграル中。此外，研究还提出了一种方法来保证函数 интеграル为正，这是许多应用中必需的条件。</li>
<li>results: 研究得到了一种能够确定神经网络函数 интеграル的方法，并且可以应用于许多应用中，如概率分布、距离度量等。此外，研究还发现了一些约束神经网络的特性和应用场景。<details>
<summary>Abstract</summary>
It is often useful to perform integration over learned functions represented by neural networks. However, this integration is usually performed numerically, as analytical integration over learned functions (especially neural networks) is generally viewed as intractable. In this work, we present a method for representing the analytical integral of a learned function $f$. This allows the exact integral of a neural network to be computed, and enables constrained neural networks to be parametrised by applying constraints directly to the integral. Crucially, we also introduce a method to constrain $f$ to be positive, a necessary condition for many applications (e.g. probability distributions, distance metrics, etc). Finally, we introduce several applications where our fixed-integral neural network (FINN) can be utilised.
</details>
<details>
<summary>摘要</summary>
通常情况下，使用神经网络学习的函数 интеграル是一个有用的技术。然而，通常是通过数值方法来实现这种 интеграル计算，因为对神经网络学习函数的分析性 интеграル是一个通常被视为无法计算的问题。在这项工作中，我们提出了一种方法来计算神经网络学习函数 $f$ 的分析性 интеграル。这使得神经网络的精确 интеграル可以被计算出来，并且可以通过直接应用约束来 parametrise 受限神经网络。其中，我们还提出了一种方法来约束 $f$ 为正，这是许多应用中的必要条件（例如概率分布、距离度量等）。最后，我们介绍了一些应用场景，where our fixed-integral neural network (FINN) can be used.
</details></li>
</ul>
<hr>
<h2 id="Skill-it-A-Data-Driven-Skills-Framework-for-Understanding-and-Training-Language-Models"><a href="#Skill-it-A-Data-Driven-Skills-Framework-for-Understanding-and-Training-Language-Models" class="headerlink" title="Skill-it! A Data-Driven Skills Framework for Understanding and Training Language Models"></a>Skill-it! A Data-Driven Skills Framework for Understanding and Training Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14430">http://arxiv.org/abs/2307.14430</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mayee F. Chen, Nicholas Roberts, Kush Bhatia, Jue Wang, Ce Zhang, Frederic Sala, Christopher Ré</li>
<li>for: 这个论文的目的是研究如何使用有限Token数据进行语言模型的训练，以提高其下游任务的性能。</li>
<li>methods: 该论文提出了一个新的框架，基于人类学习的自然顺序来理解语言模型如何学习从其训练数据中的技能。这个框架使用了一种新的在线数据采样算法，它可以在不同任务之间进行循环训练，从而提高模型的性能。</li>
<li>results: 通过使用这个框架和采样算法， authors 在 Synthetic LEGO 数据集和 Natural Instructions 数据集上进行了实验，并证明了这种方法可以提高模型的性能。在 continual pre-training  Setting中，Skill-It 算法在 LEGO 数据集上取得了36.5个点更高的准确率，而在 fine-tuning  Setting中，它在 target 技能上降低了13.6%的验证损失。此外， authors 还在 RedPajama 数据集上使用了这种方法，并在 1B 个Token数据上训练了一个 3B 参数的语言模型，并取得了高于基eline方法的准确率。<details>
<summary>Abstract</summary>
The quality of training data impacts the performance of pre-trained large language models (LMs). Given a fixed budget of tokens, we study how to best select data that leads to good downstream model performance across tasks. We develop a new framework based on a simple hypothesis: just as humans acquire interdependent skills in a deliberate order, language models also follow a natural order when learning a set of skills from their training data. If such an order exists, it can be utilized for improved understanding of LMs and for data-efficient training. Using this intuition, our framework formalizes the notion of a skill and of an ordered set of skills in terms of the associated data. First, using both synthetic and real data, we demonstrate that these ordered skill sets exist, and that their existence enables more advanced skills to be learned with less data when we train on their prerequisite skills. Second, using our proposed framework, we introduce an online data sampling algorithm, Skill-It, over mixtures of skills for both continual pre-training and fine-tuning regimes, where the objective is to efficiently learn multiple skills in the former and an individual skill in the latter. On the LEGO synthetic in the continual pre-training setting, Skill-It obtains 36.5 points higher accuracy than random sampling. On the Natural Instructions dataset in the fine-tuning setting, Skill-It reduces the validation loss on the target skill by 13.6% versus training on data associated with the target skill itself. We apply our skills framework on the recent RedPajama dataset to continually pre-train a 3B-parameter LM, achieving higher accuracy on the LM Evaluation Harness with 1B tokens than the baseline approach of sampling uniformly over data sources with 3B tokens.
</details>
<details>
<summary>摘要</summary>
“训练数据质量对预训练大语言模型（LM）的性能产生很大影响。我们研究如何在固定的字符数限制下选择数据，以确保在任务之间具有良好的下游模型性能。我们提出了一新的框架，基于一个简单的假设：人类在意图的顺序中学习了一系列技能，语言模型也会在它们的训练数据中学习一个自然的顺序。如果这种顺序存在，那么可以用于更好地理解LMs，以及进行数据效率的训练。使用这种假设，我们的框架将技能和相关数据进行了正式的定义。我们首先使用 sintetic 和实际数据示出，这些顺序技能集存在，并且它们的存在可以使更高级别的技能在更少的数据上学习。其次，我们根据我们提出的框架，引入了一种在混合技能上进行线上数据采样的算法，叫Skill-It。在不断预训练和精度训练的情况下，Skill-It 可以更高效地学习多种技能。在 LEGO  sintetic 上，Skill-It 在不断预训练情况下取得了36.5个点更高的准确率。在 Natural Instructions 数据集上，Skill-It 在精度训练情况下降低了目标技能的验证损失13.6%。我们在最近的 RedPajama 数据集上应用了我们的技能框架，在3B-参数 LM 中进行不断预训练，以达到LM Evaluation Harness 的更高准确率，比基eline方法（随机采样数据源）的3B个字符Token。”
</details></li>
</ul>
<hr>
<h2 id="TabR-Unlocking-the-Power-of-Retrieval-Augmented-Tabular-Deep-Learning"><a href="#TabR-Unlocking-the-Power-of-Retrieval-Augmented-Tabular-Deep-Learning" class="headerlink" title="TabR: Unlocking the Power of Retrieval-Augmented Tabular Deep Learning"></a>TabR: Unlocking the Power of Retrieval-Augmented Tabular Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14338">http://arxiv.org/abs/2307.14338</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yandex-research/tabular-dl-tabr">https://github.com/yandex-research/tabular-dl-tabr</a></li>
<li>paper_authors: Yury Gorishniy, Ivan Rubachev, Nikolay Kartashev, Daniil Shlenskii, Akim Kotelnikov, Artem Babenko</li>
<li>for: 该研究是为了解决表格数据问题上的深度学习（DL）模型是否值得投入研究的问题。</li>
<li>methods: 该研究使用了一种基于注意力的搜索组件，将简单的扩展报文Architecture与 Retrieval-based 模型结合在一起，并对注意力机制的几个细节进行了优化。</li>
<li>results: 该研究在一些公共的benchmark上达到了最佳平均性能，超过了其他表格DL模型，并在“GBDT友好”的benchmark上超越了GBDT模型。<details>
<summary>Abstract</summary>
Deep learning (DL) models for tabular data problems are receiving increasingly more attention, while the algorithms based on gradient-boosted decision trees (GBDT) remain a strong go-to solution. Following the recent trends in other domains, such as natural language processing and computer vision, several retrieval-augmented tabular DL models have been recently proposed. For a given target object, a retrieval-based model retrieves other relevant objects, such as the nearest neighbors, from the available (training) data and uses their features or even labels to make a better prediction. However, we show that the existing retrieval-based tabular DL solutions provide only minor, if any, benefits over the properly tuned simple retrieval-free baselines. Thus, it remains unclear whether the retrieval-based approach is a worthy direction for tabular DL.   In this work, we give a strong positive answer to this question. We start by incrementally augmenting a simple feed-forward architecture with an attention-like retrieval component similar to those of many (tabular) retrieval-based models. Then, we highlight several details of the attention mechanism that turn out to have a massive impact on the performance on tabular data problems, but that were not explored in prior work. As a result, we design TabR -- a simple retrieval-based tabular DL model which, on a set of public benchmarks, demonstrates the best average performance among tabular DL models, becomes the new state-of-the-art on several datasets, and even outperforms GBDT models on the recently proposed ``GBDT-friendly'' benchmark (see the first figure).
</details>
<details>
<summary>摘要</summary>
深度学习（DL）模型在表格数据问题上 receiving increasingly more attention，而基于梯度抛物线（GBDT）算法的模型仍然是强大的首选解决方案。随着其他领域，如自然语言处理和计算机视觉，多种基于检索的表格DL模型已经被提出。为给定目标对象，一个基于检索的模型将其他相关对象（如最近的邻居）从可用的数据集中检索出来，并使用其特征或甚至标签来进行更好的预测。然而，我们显示出现有的检索基于DL解决方案只提供了微不足的，如果有的，利益于简单的无检索基线。因此，是否使用检索方法是值得追究的问题。在这项工作中，我们给出了一个积极的答案。我们首先将简单的扩散架构逐步增强为包含拟合 Retrieval 的注意力机制，类似于许多（表格）检索基于DL模型。然后，我们强调了一些关于注意力机制的细节，其中一些在对表格数据问题进行表达的时候有很大的影响，但在前期工作中未经探讨。因此，我们设计了 TabR -- 一个简单的检索基于DL模型，在一些公共的 benchmark 上表现出了最好的平均性能，创下了新的状态码，并在一些数据集上 even outperform GBDT 模型（参见第一个图像）。
</details></li>
</ul>
<hr>
<h2 id="Waypoint-Based-Imitation-Learning-for-Robotic-Manipulation"><a href="#Waypoint-Based-Imitation-Learning-for-Robotic-Manipulation" class="headerlink" title="Waypoint-Based Imitation Learning for Robotic Manipulation"></a>Waypoint-Based Imitation Learning for Robotic Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14326">http://arxiv.org/abs/2307.14326</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lucys0/awe">https://github.com/lucys0/awe</a></li>
<li>paper_authors: Lucy Xiaoyang Shi, Archit Sharma, Tony Z. Zhao, Chelsea Finn</li>
<li>for: 该论文旨在提出一种自动生成方向点的方法，以便在人工学习中减少错误的汇总。</li>
<li>methods: 该论文提出了一种自动方向点提取（AWE）模块，可以将示例分解为最小的方向点集，以便在指定的误差阈值下 linearly  interpolate approximate  trajectory。</li>
<li>results: 实验和实际应用中，AWE 可以增加状态艺术 algorithm 的成功率，提高了实验和实际应用中的成功率，并且可以减少决策准点数量。<details>
<summary>Abstract</summary>
While imitation learning methods have seen a resurgent interest for robotic manipulation, the well-known problem of compounding errors continues to afflict behavioral cloning (BC). Waypoints can help address this problem by reducing the horizon of the learning problem for BC, and thus, the errors compounded over time. However, waypoint labeling is underspecified, and requires additional human supervision. Can we generate waypoints automatically without any additional human supervision? Our key insight is that if a trajectory segment can be approximated by linear motion, the endpoints can be used as waypoints. We propose Automatic Waypoint Extraction (AWE) for imitation learning, a preprocessing module to decompose a demonstration into a minimal set of waypoints which when interpolated linearly can approximate the trajectory up to a specified error threshold. AWE can be combined with any BC algorithm, and we find that AWE can increase the success rate of state-of-the-art algorithms by up to 25% in simulation and by 4-28% on real-world bimanual manipulation tasks, reducing the decision making horizon by up to a factor of 10. Videos and code are available at https://lucys0.github.io/awe/
</details>
<details>
<summary>摘要</summary>
While imitation learning methods have seen a resurgence of interest for robotic manipulation, the well-known problem of compounding errors continues to affect behavioral cloning (BC). Waypoints can help address this problem by reducing the horizon of the learning problem for BC, but waypoint labeling is underspecified and requires additional human supervision. Can we generate waypoints automatically without any additional human supervision? Our key insight is that if a trajectory segment can be approximated by linear motion, the endpoints can be used as waypoints. We propose Automatic Waypoint Extraction (AWE) for imitation learning, a preprocessing module to decompose a demonstration into a minimal set of waypoints that can be interpolated linearly to approximate the trajectory up to a specified error threshold. AWE can be combined with any BC algorithm, and we find that AWE can increase the success rate of state-of-the-art algorithms by up to 25% in simulation and by 4-28% on real-world bimanual manipulation tasks, reducing the decision-making horizon by up to a factor of 10. Videos and code are available at https://lucys0.github.io/awe/.Here's the translation in Traditional Chinese:随着从imitative learning方法的复兴， robotic manipulation 中的well-known problem of compounding errors仍然存在。 Waypoints可以帮助解决这个问题，但是 waypoint labeling 是不够详细的，需要额外的人工supervision。 Can we generate waypoints automatically without any additional human supervision? Our key insight is that if a trajectory segment can be approximated by linear motion, the endpoints can be used as waypoints. We propose Automatic Waypoint Extraction (AWE) for imitation learning, a preprocessing module to decompose a demonstration into a minimal set of waypoints that can be interpolated linearly to approximate the trajectory up to a specified error threshold. AWE can be combined with any BC algorithm, and we find that AWE can increase the success rate of state-of-the-art algorithms by up to 25% in simulation and by 4-28% on real-world bimanual manipulation tasks, reducing the decision-making horizon by up to a factor of 10. Videos and code are available at https://lucys0.github.io/awe/.
</details></li>
</ul>
<hr>
<h2 id="Evaluating-the-Moral-Beliefs-Encoded-in-LLMs"><a href="#Evaluating-the-Moral-Beliefs-Encoded-in-LLMs" class="headerlink" title="Evaluating the Moral Beliefs Encoded in LLMs"></a>Evaluating the Moral Beliefs Encoded in LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14324">http://arxiv.org/abs/2307.14324</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ninodimontalcino/moralchoice">https://github.com/ninodimontalcino/moralchoice</a></li>
<li>paper_authors: Nino Scherrer, Claudia Shi, Amir Feder, David M. Blei</li>
<li>for: 这个研究探讨了大型自然语言模型（LLM）上的问naire设计、管理、后期处理和评估方法。</li>
<li>methods: 这个研究使用了一种统计方法来揭示LLM中的信仰。研究者们引入了一些统计量和评价指标，以量化LLM“选择”的概率、相关的uncertainty以及选择的一致性。</li>
<li>results: 研究发现，在明确的场景下，大多数模型选择与常识相符的行为。在抽象的场景下，大多数模型表现出uncertainty。此外，一些模型在抽象场景下表现出明确的偏好，特别是关闭源代码模型之间存在一定的一致性。<details>
<summary>Abstract</summary>
This paper presents a case study on the design, administration, post-processing, and evaluation of surveys on large language models (LLMs). It comprises two components: (1) A statistical method for eliciting beliefs encoded in LLMs. We introduce statistical measures and evaluation metrics that quantify the probability of an LLM "making a choice", the associated uncertainty, and the consistency of that choice. (2) We apply this method to study what moral beliefs are encoded in different LLMs, especially in ambiguous cases where the right choice is not obvious. We design a large-scale survey comprising 680 high-ambiguity moral scenarios (e.g., "Should I tell a white lie?") and 687 low-ambiguity moral scenarios (e.g., "Should I stop for a pedestrian on the road?"). Each scenario includes a description, two possible actions, and auxiliary labels indicating violated rules (e.g., "do not kill"). We administer the survey to 28 open- and closed-source LLMs. We find that (a) in unambiguous scenarios, most models "choose" actions that align with commonsense. In ambiguous cases, most models express uncertainty. (b) Some models are uncertain about choosing the commonsense action because their responses are sensitive to the question-wording. (c) Some models reflect clear preferences in ambiguous scenarios. Specifically, closed-source models tend to agree with each other.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>A statistical method for eliciting beliefs encoded in LLMs. We introduce statistical measures and evaluation metrics that quantify the probability of an LLM “making a choice,” the associated uncertainty, and the consistency of that choice.2. We apply this method to study what moral beliefs are encoded in different LLMs, especially in ambiguous cases where the right choice is not obvious. We design a large-scale survey with 680 high-ambiguity moral scenarios (e.g., “Should I tell a white lie?”) and 687 low-ambiguity moral scenarios (e.g., “Should I stop for a pedestrian on the road?”). Each scenario includes a description, two possible actions, and auxiliary labels indicating violated rules (e.g., “do not kill”). We administer the survey to 28 open- and closed-source LLMs.Our findings are as follows:1. In unambiguous scenarios, most models “choose” actions that align with commonsense.2. In ambiguous cases, most models express uncertainty.3. Some models are uncertain about choosing the commonsense action because their responses are sensitive to the question-wording.4. Some models reflect clear preferences in ambiguous scenarios, and closed-source models tend to agree with each other.</details></li>
</ol>
<hr>
<h2 id="Reinforcement-Learning-by-Guided-Safe-Exploration"><a href="#Reinforcement-Learning-by-Guided-Safe-Exploration" class="headerlink" title="Reinforcement Learning by Guided Safe Exploration"></a>Reinforcement Learning by Guided Safe Exploration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14316">http://arxiv.org/abs/2307.14316</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qisong Yang, Thiago D. Simão, Nils Jansen, Simon H. Tindemans, Matthijs T. J. Spaan</li>
<li>for: 这篇论文的目的是为了帮助RL算法在不知道目标任务的情况下安全地扩展应用。</li>
<li>methods: 这篇论文使用了不受奖励指导的RL方法，在一个控制的环境中训练一个引导者，以便在不知道目标任务时快速适应。在目标任务被揭示后，不允许安全违反。此外，该方法还利用了传输学习来正则化一个目标策略（学生），使其快速解决目标任务。</li>
<li>results: 实验表明，这种方法可以实现安全的传输学习，帮助学生更快地解决目标任务。<details>
<summary>Abstract</summary>
Safety is critical to broadening the application of reinforcement learning (RL). Often, we train RL agents in a controlled environment, such as a laboratory, before deploying them in the real world. However, the real-world target task might be unknown prior to deployment. Reward-free RL trains an agent without the reward to adapt quickly once the reward is revealed. We consider the constrained reward-free setting, where an agent (the guide) learns to explore safely without the reward signal. This agent is trained in a controlled environment, which allows unsafe interactions and still provides the safety signal. After the target task is revealed, safety violations are not allowed anymore. Thus, the guide is leveraged to compose a safe behaviour policy. Drawing from transfer learning, we also regularize a target policy (the student) towards the guide while the student is unreliable and gradually eliminate the influence of the guide as training progresses. The empirical analysis shows that this method can achieve safe transfer learning and helps the student solve the target task faster.
</details>
<details>
<summary>摘要</summary>
安全性是扩展强化学习（RL）的关键因素。我们通常在实验室中首先训练RL代理人，然后在真实世界中部署。然而，真实世界目标任务可能未知之前部署。无奖RL在扩展RL代理人时不提供奖励。我们考虑了受限的奖励自由设置，其中RL代理人（导师）在安全的环境中学习探索，不需要奖励信号。当目标任务揭示后，安全违反不再允许。因此，导师被用来组织安全行为策略。从传输学学习中，我们还启用了一个目标策略（学生），使其向导师进行Regularization，以解决学生在训练过程中的不可靠性。实验分析表明，这种方法可以实现安全的传输学习，帮助学生更快地解决目标任务。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Deep-Learning-based-Pansharpening-with-Jointly-Enhanced-Spectral-and-Spatial-Fidelity"><a href="#Unsupervised-Deep-Learning-based-Pansharpening-with-Jointly-Enhanced-Spectral-and-Spatial-Fidelity" class="headerlink" title="Unsupervised Deep Learning-based Pansharpening with Jointly-Enhanced Spectral and Spatial Fidelity"></a>Unsupervised Deep Learning-based Pansharpening with Jointly-Enhanced Spectral and Spatial Fidelity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14403">http://arxiv.org/abs/2307.14403</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/matciotola/lambda-pnn">https://github.com/matciotola/lambda-pnn</a></li>
<li>paper_authors: Matteo Ciotola, Giovanni Poggi, Giuseppe Scarpa<br>for: 这个论文主要关注于深度学习在多resolution图像缩进中的应用，具体来说是提出一种可以在全分辨率域进行无监督训练的整体框架，以提高图像缩进的性能。methods: 该论文提出了一种新的深度学习基于模型，具有增强的建筑特性和一种新的损失函数，该损失函数同时 promote spectral和spatial图像质量。此外，该模型还使用了一种新的微调策略来提高推理时的适应性。results: 实验表明，提出的方法在具有各种挑战性的测试图像上达到了STATE OF THE ART的性能水平，both in terms of numerical results and visual output。<details>
<summary>Abstract</summary>
In latest years, deep learning has gained a leading role in the pansharpening of multiresolution images. Given the lack of ground truth data, most deep learning-based methods carry out supervised training in a reduced-resolution domain. However, models trained on downsized images tend to perform poorly on high-resolution target images. For this reason, several research groups are now turning to unsupervised training in the full-resolution domain, through the definition of appropriate loss functions and training paradigms. In this context, we have recently proposed a full-resolution training framework which can be applied to many existing architectures.   Here, we propose a new deep learning-based pansharpening model that fully exploits the potential of this approach and provides cutting-edge performance. Besides architectural improvements with respect to previous work, such as the use of residual attention modules, the proposed model features a novel loss function that jointly promotes the spectral and spatial quality of the pansharpened data. In addition, thanks to a new fine-tuning strategy, it improves inference-time adaptation to target images. Experiments on a large variety of test images, performed in challenging scenarios, demonstrate that the proposed method compares favorably with the state of the art both in terms of numerical results and visual output. Code is available online at https://github.com/matciotola/Lambda-PNN.
</details>
<details>
<summary>摘要</summary>
最近几年，深度学习在多尺度图像缩进中扮演了主导角色。由于缺乏地面真实数据，大多数深度学习基于方法在减小分辨率领域进行了supervised训练。然而，在高分辨率目标图像上训练的模型通常表现不佳。为了解决这个问题，许多研究小组现在转向无监督训练在全分辨率领域，通过定义适当的损失函数和训练方法。在这种情况下，我们最近提出了一个全分辨率训练框架，可以应用于许多现有的架构。  我们提出了一个新的深度学习基于缩进模型，该模型完全利用了这种方法的潜力，并提供了顶尖性能。除了以前的建筑改进外，该模型还包括尚未使用的差分注意模块，以及一个新的损失函数，该函数同时Promote spectral和空间数据的质量。此外，通过一种新的精度调整策略，该模型提高了对target图像的推理时适应性。在一个大量的测试图像上，通过在复杂的场景下进行测试，我们的方法与状态对照数据比较，得到了优秀的数值结果和视觉输出。代码可以在https://github.com/matciotola/Lambda-PNN上下载。
</details></li>
</ul>
<hr>
<h2 id="A-Constraint-Enforcement-Deep-Reinforcement-Learning-Framework-for-Optimal-Energy-Storage-Systems-Dispatch"><a href="#A-Constraint-Enforcement-Deep-Reinforcement-Learning-Framework-for-Optimal-Energy-Storage-Systems-Dispatch" class="headerlink" title="A Constraint Enforcement Deep Reinforcement Learning Framework for Optimal Energy Storage Systems Dispatch"></a>A Constraint Enforcement Deep Reinforcement Learning Framework for Optimal Energy Storage Systems Dispatch</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14304">http://arxiv.org/abs/2307.14304</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ShengrenHou/Energy-management-MIP-Deep-Reinforcement-Learning">https://github.com/ShengrenHou/Energy-management-MIP-Deep-Reinforcement-Learning</a></li>
<li>paper_authors: Shengren Hou, Edgar Mauricio Salazar Duque, Peter Palensky, Pedro P. Vergara</li>
<li>for: 这篇论文的目的是提出一个基于深度学习的能源储存系统优化策略，以应对动态价格、需求耗用和可再生能源生产的不确定性。</li>
<li>methods: 这篇论文使用深度神经网络（DNNs）和深度问题学习（DRL）算法，以学习适应分布网络的数据分布，并将其转换为一个混合整数程式（MIP）形式，以考虑环境的操作限制。</li>
<li>results:  comparison simulations show that the proposed MIP-DRL framework can effectively enforce all constraints while delivering high-quality dispatch decisions, outperforming state-of-the-art DRL algorithms and the optimal solution obtained with a perfect forecast of the stochastic variables.<details>
<summary>Abstract</summary>
The optimal dispatch of energy storage systems (ESSs) presents formidable challenges due to the uncertainty introduced by fluctuations in dynamic prices, demand consumption, and renewable-based energy generation. By exploiting the generalization capabilities of deep neural networks (DNNs), deep reinforcement learning (DRL) algorithms can learn good-quality control models that adaptively respond to distribution networks' stochastic nature. However, current DRL algorithms lack the capabilities to enforce operational constraints strictly, often even providing unfeasible control actions. To address this issue, we propose a DRL framework that effectively handles continuous action spaces while strictly enforcing the environments and action space operational constraints during online operation. Firstly, the proposed framework trains an action-value function modeled using DNNs. Subsequently, this action-value function is formulated as a mixed-integer programming (MIP) formulation enabling the consideration of the environment's operational constraints. Comprehensive numerical simulations show the superior performance of the proposed MIP-DRL framework, effectively enforcing all constraints while delivering high-quality dispatch decisions when compared with state-of-the-art DRL algorithms and the optimal solution obtained with a perfect forecast of the stochastic variables.
</details>
<details>
<summary>摘要</summary>
优化能量存储系统（ESS）的分发表现出了巨大的挑战，这主要归结于能量价格、消耗量和可再生能源生产的波动性带来的不确定性。通过深度神经网络（DNN）的泛化能力，深度强化学习（DRL）算法可以学习适应分布网络的随机性，并且可以适应不同的环境和操作约束。然而，当前的DRL算法通常无法严格地执行环境和操作约束，有时候甚至提供了不可行的控制动作。为解决这个问题，我们提出了一种DRL框架，可以有效地处理连续动作空间，同时严格执行环境和操作约束。首先，我们在DNN中训练一个动作价值函数。然后，我们将这个动作价值函数转换为混合整数编程（MIP）形式，以便考虑环境的操作约束。通过对数字实验进行了全面的比较，我们发现了我们提出的MIP-DRL框架的优秀表现。它可以坚持所有约束，同时提供高质量的分发决策，与当前的DRL算法和完美预测随机变量的优质策略相比。
</details></li>
</ul>
<hr>
<h2 id="ChatGPT-and-Persuasive-Technologies-for-the-Management-and-Delivery-of-Personalized-Recommendations-in-Hotel-Hospitality"><a href="#ChatGPT-and-Persuasive-Technologies-for-the-Management-and-Delivery-of-Personalized-Recommendations-in-Hotel-Hospitality" class="headerlink" title="ChatGPT and Persuasive Technologies for the Management and Delivery of Personalized Recommendations in Hotel Hospitality"></a>ChatGPT and Persuasive Technologies for the Management and Delivery of Personalized Recommendations in Hotel Hospitality</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14298">http://arxiv.org/abs/2307.14298</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manolis Remountakis, Konstantinos Kotis, Babis Kourtzis, George E. Tsekouras</li>
<li>for: 这篇论文的目的是探讨推荐系统在酒店互助业中的应用，以及利用语言模型和吸引技术来提高推荐系统的效果。</li>
<li>methods: 这篇论文使用了语言模型ChatGPT和吸引技术来自动化和改进酒店推荐系统。它还分析了用户喜好和在线评论中提取有价值信息，并基于用户 профиль生成个性化推荐。</li>
<li>results: 这篇论文通过一个实验研究了将ChatGPT和吸引技术Integrated into酒店推荐系统的效果，发现这些技术可以提高用户参与度、满意度和酒店收入。<details>
<summary>Abstract</summary>
Recommender systems have become indispensable tools in the hotel hospitality industry, enabling personalized and tailored experiences for guests. Recent advancements in large language models (LLMs), such as ChatGPT, and persuasive technologies, have opened new avenues for enhancing the effectiveness of those systems. This paper explores the potential of integrating ChatGPT and persuasive technologies for automating and improving hotel hospitality recommender systems. First, we delve into the capabilities of ChatGPT, which can understand and generate human-like text, enabling more accurate and context-aware recommendations. We discuss the integration of ChatGPT into recommender systems, highlighting the ability to analyze user preferences, extract valuable insights from online reviews, and generate personalized recommendations based on guest profiles. Second, we investigate the role of persuasive technology in influencing user behavior and enhancing the persuasive impact of hotel recommendations. By incorporating persuasive techniques, such as social proof, scarcity and personalization, recommender systems can effectively influence user decision-making and encourage desired actions, such as booking a specific hotel or upgrading their room. To investigate the efficacy of ChatGPT and persuasive technologies, we present a pilot experi-ment with a case study involving a hotel recommender system. We aim to study the impact of integrating ChatGPT and persua-sive techniques on user engagement, satisfaction, and conversion rates. The preliminary results demonstrate the potential of these technologies in enhancing the overall guest experience and business performance. Overall, this paper contributes to the field of hotel hospitality by exploring the synergistic relationship between LLMs and persuasive technology in recommender systems, ultimately influencing guest satisfaction and hotel revenue.
</details>
<details>
<summary>摘要</summary>
各种推荐系统在酒店互联网行业已成为不可或缺的工具，帮助提供个性化和适应性的旅客体验。最新的大型自然语言模型（LLM），如ChatGPT，以及吸引技术，已经开创了推荐系统的新可能性。本文探讨了将ChatGPT和吸引技术integrated into hotel hospitality recommender systems的可能性，以提高旅客体验和酒店业绩。首先，我们探讨了ChatGPT的能力，可以理解和生成人类语言，从而提供更准确和上下文感知的推荐。我们介绍了将ChatGPT integrate into recommender systems，包括分析用户偏好、从在线评论中提取有价值信息和基于客户 profiling 生成个性化的推荐。其次，我们调查了吸引技术在使用者行为上的影响，以及如何在推荐系统中应用吸引技术以提高旅客决策的可能性。通过涉及到社会证明、缺失和个性化等吸引技术，推荐系统可以有效地影响用户决策和鼓励旅客选择特定酒店或升级房间。为了评估ChatGPT和吸引技术的效果，我们在一个酒店推荐系统的 caso study中进行了试点。我们的目标是研究将ChatGPT和吸引技术integrated into recommender systems的影响在用户参与度、满意度和转化率上。初步结果表明这些技术在总体客户体验和酒店业绩方面具有潜在的潜力。总的来说，本文对酒店互联网领域的推荐系统做出了贡献，探讨了LLMs和吸引技术之间的相互作用，最终影响客户满意度和酒店收益。
</details></li>
</ul>
<hr>
<h2 id="Unraveling-the-Complexity-of-Splitting-Sequential-Data-Tackling-Challenges-in-Video-and-Time-Series-Analysis"><a href="#Unraveling-the-Complexity-of-Splitting-Sequential-Data-Tackling-Challenges-in-Video-and-Time-Series-Analysis" class="headerlink" title="Unraveling the Complexity of Splitting Sequential Data: Tackling Challenges in Video and Time Series Analysis"></a>Unraveling the Complexity of Splitting Sequential Data: Tackling Challenges in Video and Time Series Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14294">http://arxiv.org/abs/2307.14294</a></li>
<li>repo_url: None</li>
<li>paper_authors: Diego Botache, Kristina Dingel, Rico Huhnstock, Arno Ehresmann, Bernhard Sick</li>
<li>for: 本文探讨了分割顺序数据的挑战，包括数据获取、数据表示、分割率选择、设置质量标准和选择适当的选择策略。</li>
<li>methods: 本文使用了两个实际应用例──汽车测试台和流体中的粒子跟踪──来探讨分割顺序数据的挑战。</li>
<li>results: 本文通过两个实际应用例的分析，揭示了分割顺序数据的挑战，并提供了一些可能的解决方案。<details>
<summary>Abstract</summary>
Splitting of sequential data, such as videos and time series, is an essential step in various data analysis tasks, including object tracking and anomaly detection. However, splitting sequential data presents a variety of challenges that can impact the accuracy and reliability of subsequent analyses. This concept article examines the challenges associated with splitting sequential data, including data acquisition, data representation, split ratio selection, setting up quality criteria, and choosing suitable selection strategies. We explore these challenges through two real-world examples: motor test benches and particle tracking in liquids.
</details>
<details>
<summary>摘要</summary>
分割连续数据，如视频和时间序列数据，是数据分析任务中的一个重要步骤，包括对象跟踪和异常检测。然而，分割连续数据会出现多种挑战，这些挑战可能会影响后续分析的准确性和可靠性。本概念文章探讨分割连续数据的挑战，包括数据收集、数据表示、分割率选择、设置质量标准和选择适合的选择策略。我们通过两个实际例子：汽车测试台和液体中粒子跟踪来探讨这些挑战。
</details></li>
</ul>
<hr>
<h2 id="General-Purpose-Artificial-Intelligence-Systems-GPAIS-Properties-Definition-Taxonomy-Open-Challenges-and-Implications"><a href="#General-Purpose-Artificial-Intelligence-Systems-GPAIS-Properties-Definition-Taxonomy-Open-Challenges-and-Implications" class="headerlink" title="General Purpose Artificial Intelligence Systems (GPAIS): Properties, Definition, Taxonomy, Open Challenges and Implications"></a>General Purpose Artificial Intelligence Systems (GPAIS): Properties, Definition, Taxonomy, Open Challenges and Implications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14283">http://arxiv.org/abs/2307.14283</a></li>
<li>repo_url: None</li>
<li>paper_authors: Isaac Triguero, Daniel Molina, Javier Poyatos, Javier Del Ser, Francisco Herrera</li>
<li>for: The paper discusses and proposes a new definition for General-Purpose Artificial Intelligence Systems (GPAIS) and its differentiation based on various factors.</li>
<li>methods: The paper uses existing definitions and proposes a new taxonomy of approaches to realize GPAIS, including the use of AI techniques to improve another AI or foundation models.</li>
<li>results: The paper provides a comprehensive overview of the current state of GPAIS, its challenges and prospects, implications for our society, and the need for responsible and trustworthy AI systems and regulation.Here are the three points in Simplified Chinese text:</li>
<li>for: 这篇论文提出了一个新的 General-Purpose Artificial Intelligence Systems（GPAIS）定义，并对其进行了不同因素的分类。</li>
<li>methods: 该论文使用了现有的定义，并提出了一种新的实现GPAIS的纲要，包括使用AI技术来改进另一个AI或基础模型。</li>
<li>results: 该论文提供了GPAIS的当前状况，其挑战和前景，对我们社会的影响，以及负责任和可信worthy AI系统和regulation的需要。<details>
<summary>Abstract</summary>
Most applications of Artificial Intelligence (AI) are designed for a confined and specific task. However, there are many scenarios that call for a more general AI, capable of solving a wide array of tasks without being specifically designed for them. The term General-Purpose Artificial Intelligence Systems (GPAIS) has been defined to refer to these AI systems. To date, the possibility of an Artificial General Intelligence, powerful enough to perform any intellectual task as if it were human, or even improve it, has remained an aspiration, fiction, and considered a risk for our society. Whilst we might still be far from achieving that, GPAIS is a reality and sitting at the forefront of AI research.   This work discusses existing definitions for GPAIS and proposes a new definition that allows for a gradual differentiation among types of GPAIS according to their properties and limitations. We distinguish between closed-world and open-world GPAIS, characterising their degree of autonomy and ability based on several factors such as adaptation to new tasks, competence in domains not intentionally trained for, ability to learn from few data, or proactive acknowledgment of their own limitations. We then propose a taxonomy of approaches to realise GPAIS, describing research trends such as the use of AI techniques to improve another AI or foundation models. As a prime example, we delve into generative AI, aligning them with the terms and concepts presented in the taxonomy. Through the proposed definition and taxonomy, our aim is to facilitate research collaboration across different areas that are tackling general-purpose tasks, as they share many common aspects. Finally, we discuss the current state of GPAIS, its challenges and prospects, implications for our society, and the need for responsible and trustworthy AI systems and regulation, with the goal of providing a holistic view of GPAIS.
</details>
<details>
<summary>摘要</summary>
大多数人工智能（AI）应用都是为特定任务设计的，但有许多情况需要一种更通用的AI系统，能够解决多种任务而不需要特定设计。这种AI系统被称为通用人工智能系统（GPAIS）。迄今为止，人工通用智能，能够像人类一样完成任何知识任务，或者甚至超越人类，仍然是一个梦想和科幻。尽管我们仍然远离实现这一点，但GPAIS已经成为人工智能研究的前沿。本文讨论了现有的GPAIS定义，并提出了一个新的定义，允许逐步分化GPAIS类型根据其性质和限制。我们将GPAIS分为关闭世界和开放世界两类，根据它们的自主度和能力，包括适应新任务、在不直接训练的领域中的能力、从少量数据学习、或者主动承认自己的限制等因素。然后，我们提出了一种分类方法，描述了在实现GPAIS方面的研究趋势，如使用AI技术提高另一个AI的性能或基础模型。为了更好地推动不同领域之间的合作研究，我们采用了一种概念和术语的分类方法。我们还讨论了GPAIS的当前状况，挑战和前途，以及对社会的影响和责任的人工智能系统和regulation的需要，以提供一个整体的GPAIS视图。
</details></li>
</ul>
<hr>
<h2 id="Deepfake-Image-Generation-for-Improved-Brain-Tumor-Segmentation"><a href="#Deepfake-Image-Generation-for-Improved-Brain-Tumor-Segmentation" class="headerlink" title="Deepfake Image Generation for Improved Brain Tumor Segmentation"></a>Deepfake Image Generation for Improved Brain Tumor Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14273">http://arxiv.org/abs/2307.14273</a></li>
<li>repo_url: None</li>
<li>paper_authors: Roa’a Al-Emaryeen, Sara Al-Nahhas, Fatima Himour, Waleed Mahafza, Omar Al-Kadi</li>
<li>For: This paper aims to improve brain tumor segmentation using deep-fake image generation.* Methods: The proposed approach uses a Generative Adversarial Network (GAN) for image-to-image translation and a U-Net-based convolutional neural network (CNN) for image segmentation, trained with deepfake images.* Results: The proposed approach shows improved performance in terms of image segmentation quality metrics compared to ground truth, and has the potential to assist when training with limited data.Here’s the Chinese translation of the three pieces of information:* For: 这篇论文的目的是提高脑肿分割使用深伪图生成。* Methods: 该方法使用了生成对抗网络（GAN）进行图像到图像的转换，然后使用U-Net基于的卷积神经网络（CNN）进行图像分割，并使用深伪图进行训练。* Results: 该方法在图像分割质量指标方面比基准数据显示出了改善的性能，并有可能帮助在具有有限数据的情况下训练。<details>
<summary>Abstract</summary>
As the world progresses in technology and health, awareness of disease by revealing asymptomatic signs improves. It is important to detect and treat tumors in early stage as it can be life-threatening. Computer-aided technologies are used to overcome lingering limitations facing disease diagnosis, while brain tumor segmentation remains a difficult process, especially when multi-modality data is involved. This is mainly attributed to ineffective training due to lack of data and corresponding labelling. This work investigates the feasibility of employing deep-fake image generation for effective brain tumor segmentation. To this end, a Generative Adversarial Network was used for image-to-image translation for increasing dataset size, followed by image segmentation using a U-Net-based convolutional neural network trained with deepfake images. Performance of the proposed approach is compared with ground truth of four publicly available datasets. Results show improved performance in terms of image segmentation quality metrics, and could potentially assist when training with limited data.
</details>
<details>
<summary>摘要</summary>
随着科技和医疗的进步，疾病的早期发现越来越重要。检测和治疗早期癌变可以挽救生命。但是诊断疾病尚存有一些限制，特别是当涉及多Modal数据时。这主要归结于数据和相应的标注不充分。本研究探讨使用深归化图像生成技术来提高脑肿瘤分 segmentation的可能性。为此，我们使用生成对抗网络进行图像到图像翻译，然后使用基于U-Net convolutional neural network的图像分割算法，并使用深归化图像进行训练。我们对四个公共可用的数据集进行比较，结果表明我们的方法可以提高图像分割质量指标，并可能帮助在有限数据情况下训练。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/27/cs.LG_2023_07_27/" data-id="clmjn91l7005w0j88e21e4oie" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_07_27" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/27/cs.SD_2023_07_27/" class="article-date">
  <time datetime="2023-07-26T16:00:00.000Z" itemprop="datePublished">2023-07-27</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/27/cs.SD_2023_07_27/">cs.SD - 2023-07-27 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Mitigating-Cross-Database-Differences-for-Learning-Unified-HRTF-Representation"><a href="#Mitigating-Cross-Database-Differences-for-Learning-Unified-HRTF-Representation" class="headerlink" title="Mitigating Cross-Database Differences for Learning Unified HRTF Representation"></a>Mitigating Cross-Database Differences for Learning Unified HRTF Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14547">http://arxiv.org/abs/2307.14547</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yutongwen/hrtf_field_norm">https://github.com/yutongwen/hrtf_field_norm</a></li>
<li>paper_authors: Yutong Wen, You Zhang, Zhiyao Duan</li>
<li>for: 这篇论文的目的是提出一种用于实现个人化头部相关转换函数（HRTF）的预测方法，以便在虚拟听觉显示中精确地 пози规音频。</li>
<li>methods: 这篇论文使用机器学习模型来预测个人化HRTF，并使用跨 Databases 的HRTF表现来训练这些模型。</li>
<li>results: 这篇论文的结果显示，透过调整HRTF的频谱响应，可以将不同数据库中的HRTF转换为一个更加共同的表现，并且这些转换后的HRTF无法根据数据库的不同区分。<details>
<summary>Abstract</summary>
Individualized head-related transfer functions (HRTFs) are crucial for accurate sound positioning in virtual auditory displays. As the acoustic measurement of HRTFs is resource-intensive, predicting individualized HRTFs using machine learning models is a promising approach at scale. Training such models require a unified HRTF representation across multiple databases to utilize their respectively limited samples. However, in addition to differences on the spatial sampling locations, recent studies have shown that, even for the common location, HRTFs across databases manifest consistent differences that make it trivial to tell which databases they come from. This poses a significant challenge for learning a unified HRTF representation across databases. In this work, we first identify the possible causes of these cross-database differences, attributing them to variations in the measurement setup. Then, we propose a novel approach to normalize the frequency responses of HRTFs across databases. We show that HRTFs from different databases cannot be classified by their database after normalization. We further show that these normalized HRTFs can be used to learn a more unified HRTF representation across databases than the prior art. We believe that this normalization approach paves the road to many data-intensive tasks on HRTF modeling.
</details>
<details>
<summary>摘要</summary>
个人化的头顶相关转换函数（HRTF）是虚拟听觉显示的精确 зву讯定位的重要因素。由于实际量测HRTF的成本高昂，使用机器学习模型预测个人化HRTF是一个具有潜力的方法。但是，训练这些模型需要一个统一的HRTF表示方式，可以利用不同数据库的有限样本。然而，过去的研究显示，即使在共同的位置上，不同数据库的HRTF之间仍存在明显的差异，这使得很难将HRTF表示统一化。在这个工作中，我们首先识别了差异的可能原因，并将其归因于测量设置的变化。然后，我们提出了一个新的方法来对不同数据库的HRTF进行频谱均衡。我们发现，对不同数据库的HRTF进行均衡后，它们不能被分类到哪怕库。此外，我们还证明了这些均衡后的HRTF可以用来学习一个更统一的HRTF表示方式，比对于先前的方法更好。我们认为，这个均衡方法将开启许多数据密集的HRTF模型任务。
</details></li>
</ul>
<hr>
<h2 id="Modality-Agnostic-Audio-Visual-Deepfake-Detection"><a href="#Modality-Agnostic-Audio-Visual-Deepfake-Detection" class="headerlink" title="Modality-Agnostic Audio-Visual Deepfake Detection"></a>Modality-Agnostic Audio-Visual Deepfake Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14491">http://arxiv.org/abs/2307.14491</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cai Yu, Peng Chen, Jiahe Tian, Jin Liu, Jiao Dai, Xi Wang, Yesheng Chai, Jizhong Han</li>
<li>for: 这个研究旨在开发一个能够探测多模式深圳诈骗的数据类型不对称检测方法，并能够处理缺失模式的情况。</li>
<li>methods: 本研究使用了一个统一的诈骗模式框架，可以探测多模式深圳诈骗并处理缺失模式的情况。另外，我们还提出了一个双标签检测方法，可以独立检测每个模式。</li>
<li>results: 实验结果显示，我们的方法不仅在所有三个音频视觉数据集上都超过了现有的州务检测方法，而且在缺失模式情况下也可以 дости持 satisfying的性能。此外，我们的方法甚至在 JOINT 使用两个单模式方法时超过了它们的性能。<details>
<summary>Abstract</summary>
As AI-generated content (AIGC) thrives, Deepfakes have expanded from single-modality falsification to cross-modal fake content creation, where either audio or visual components can be manipulated. While using two unimodal detectors can detect audio-visual deepfakes, cross-modal forgery clues could be overlooked. Existing multimodal deepfake detection methods typically establish correspondence between the audio and visual modalities for binary real/fake classification, and require the co-occurrence of both modalities. However, in real-world multi-modal applications, missing modality scenarios may occur where either modality is unavailable. In such cases, audio-visual detection methods are less practical than two independent unimodal methods. Consequently, the detector can not always obtain the number or type of manipulated modalities beforehand, necessitating a fake-modality-agnostic audio-visual detector. In this work, we propose a unified fake-modality-agnostic scenarios framework that enables the detection of multimodal deepfakes and handles missing modalities cases, no matter the manipulation hidden in audio, video, or even cross-modal forms. To enhance the modeling of cross-modal forgery clues, we choose audio-visual speech recognition (AVSR) as a preceding task, which effectively extracts speech correlation across modalities, which is difficult for deepfakes to reproduce. Additionally, we propose a dual-label detection approach that follows the structure of AVSR to support the independent detection of each modality. Extensive experiments show that our scheme not only outperforms other state-of-the-art binary detection methods across all three audio-visual datasets but also achieves satisfying performance on detection modality-agnostic audio/video fakes. Moreover, it even surpasses the joint use of two unimodal methods in the presence of missing modality cases.
</details>
<details>
<summary>摘要</summary>
“深刻掌握AI生成内容（AIGC）的发展，深伪（Deepfakes）已经从单模态伪造扩展到跨模态伪造，其中可以操作音频或视觉组件。使用两个单模态检测器可以检测音频视频深伪，但跨模态伪造证据可能会被忽略。现有的多模态深伪检测方法通常在音频和视觉modalities之间建立对应关系，并需要两个模式同时存在。但在实际的多模式应用中，缺失模式场景可能会发生，其中一个或多个模式都不可用。在这种情况下，音频视频检测方法不太实用，因为检测器无法在事先获知哪一个模式被修改。因此，我们需要一种不关心模式的多模态深伪检测方法。在这种方案中，我们提出一种统一的多模态深伪检测框架，可以检测多模态深伪并处理缺失模式场景，无论哪一个模式被修改。为了更好地模型跨模式伪造证据，我们选择了音频视频语音识别（AVSR）作为先前任务，它可以有效提取modalities之间的语音相关性，这是深伪很难复制的。此外，我们还提出了一种双标签检测方法，根据AVSR的结构来支持每个模式独立的检测。我们的方案不仅在所有三个音频视频数据集上超过了其他状态对抗方法的性能，还在缺失模式场景下达到了满意的性能。此外，它 même surpassed the joint use of two unimodal methods in the presence of missing modality cases.”
</details></li>
</ul>
<hr>
<h2 id="Single-Channel-Speech-Enhancement-Using-U-Net-Spiking-Neural-Networks"><a href="#Single-Channel-Speech-Enhancement-Using-U-Net-Spiking-Neural-Networks" class="headerlink" title="Single Channel Speech Enhancement Using U-Net Spiking Neural Networks"></a>Single Channel Speech Enhancement Using U-Net Spiking Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14464">http://arxiv.org/abs/2307.14464</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abir Riahi, Éric Plourde</li>
<li>for: 提高沟通设备和可靠的语音识别系统的信噪比</li>
<li>methods: 使用基于U-Net架构的脉冲神经网络（SNN）进行沟通增强</li>
<li>results: 比预先达到的状态艺术神经网络（ANN）模型表现更好，并在不同的信号噪比和真实噪音条件下达到了可接受的表现。<details>
<summary>Abstract</summary>
Speech enhancement (SE) is crucial for reliable communication devices or robust speech recognition systems. Although conventional artificial neural networks (ANN) have demonstrated remarkable performance in SE, they require significant computational power, along with high energy costs. In this paper, we propose a novel approach to SE using a spiking neural network (SNN) based on a U-Net architecture. SNNs are suitable for processing data with a temporal dimension, such as speech, and are known for their energy-efficient implementation on neuromorphic hardware. As such, SNNs are thus interesting candidates for real-time applications on devices with limited resources. The primary objective of the current work is to develop an SNN-based model with comparable performance to a state-of-the-art ANN model for SE. We train a deep SNN using surrogate-gradient-based optimization and evaluate its performance using perceptual objective tests under different signal-to-noise ratios and real-world noise conditions. Our results demonstrate that the proposed energy-efficient SNN model outperforms the Intel Neuromorphic Deep Noise Suppression Challenge (Intel N-DNS Challenge) baseline solution and achieves acceptable performance compared to an equivalent ANN model.
</details>
<details>
<summary>摘要</summary>
声音提升（SE）是重要的通信设备或可靠的语音识别系统的关键。虽然传统的人工神经网络（ANN）已经表现出了惊人的表现在SE中，但它们需要显著的计算能力，同时也需要高度的能源成本。在这篇论文中，我们提出了一种使用快速神经网络（SNN）基于U-Net架构的新的方法 дляSE。SNN适用于处理具有时间维度的数据，如语音，并且被认为是能效地实现在神经机器硬件上的。因此，SNN是在有限资源的设备上实现实时应用的优秀选择。目标是在现有的ANN模型性能水平上实现相似的SE模型。我们使用代理梯度基于优化方法来训练深度SNN，并使用感知目标测试来评估其性能。我们的结果表明，我们提出的能效的SNN模型在不同的信号噪比和实际噪声条件下都能够达到适当的性能，并且超越了Intel neuromorphic Deep Noise Suppression Challenge（Intel N-DNS Challenge）基准解决方案。
</details></li>
</ul>
<hr>
<h2 id="WavJourney-Compositional-Audio-Creation-with-Large-Language-Models"><a href="#WavJourney-Compositional-Audio-Creation-with-Large-Language-Models" class="headerlink" title="WavJourney: Compositional Audio Creation with Large Language Models"></a>WavJourney: Compositional Audio Creation with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14335">http://arxiv.org/abs/2307.14335</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/audio-agi/wavjourney">https://github.com/audio-agi/wavjourney</a></li>
<li>paper_authors: Xubo Liu, Zhongkai Zhu, Haohe Liu, Yi Yuan, Meng Cui, Qiushi Huang, Jinhua Liang, Yin Cao, Qiuqiang Kong, Mark D. Plumbley, Wenwu Wang</li>
<li>for: 这 paper 旨在开发一种基于 Large Language Models (LLMs) 的音频内容生成系统，以便在语言和视觉任务中提高人工智能生成内容的能力。</li>
<li>methods: 这 paper 使用了 LLMs 连接多种音频模型，以生成包含演讲、音乐和音效的听写内容。它首先使用 LLMs 生成一份适用于音频storytelling的结构化脚本，然后使用这份脚本生成一个计算机程序，并将每行程序转换为一个特定的音频生成模型或计算操作函数。</li>
<li>results: 这 paper 在多个实际场景中展示了 WavJourney 的实用性，包括科幻、教育和广播剧等。WavJourney 的可靠和可交互的设计，使得人机共创在多轮对话中得到了进一步的创作控制和适应性。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have shown great promise in integrating diverse expert models to tackle intricate language and vision tasks. Despite their significance in advancing the field of Artificial Intelligence Generated Content (AIGC), their potential in intelligent audio content creation remains unexplored. In this work, we tackle the problem of creating audio content with storylines encompassing speech, music, and sound effects, guided by text instructions. We present WavJourney, a system that leverages LLMs to connect various audio models for audio content generation. Given a text description of an auditory scene, WavJourney first prompts LLMs to generate a structured script dedicated to audio storytelling. The audio script incorporates diverse audio elements, organized based on their spatio-temporal relationships. As a conceptual representation of audio, the audio script provides an interactive and interpretable rationale for human engagement. Afterward, the audio script is fed into a script compiler, converting it into a computer program. Each line of the program calls a task-specific audio generation model or computational operation function (e.g., concatenate, mix). The computer program is then executed to obtain an explainable solution for audio generation. We demonstrate the practicality of WavJourney across diverse real-world scenarios, including science fiction, education, and radio play. The explainable and interactive design of WavJourney fosters human-machine co-creation in multi-round dialogues, enhancing creative control and adaptability in audio production. WavJourney audiolizes the human imagination, opening up new avenues for creativity in multimedia content creation.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经显示出很大的损征在融合多种专家模型来解决复杂的语言和视觉任务上。尽管它们在人工智能生成内容（AIGC）领域中的潜力仍然未获探索，但它们在智能音频内容创建方面的潜力仍然未获开发。在这个工作中，我们对于创建包含话语、音乐和效果的音频内容进行了探索。我们提出了WavJourney系统，这个系统利用LLM来连接不同的音频模型，以实现音频内容创建。当我们给出了文本描述一个听频场景时，WavJourney首先透过LLM生成一个关于音频故事的结构化脚本。这个音频脚本包括多种音频元素，并且根据它们的空间时间关系进行了组织。作为一个概念表现的音频，这个音频脚本提供了互动和可解释的理由，以便人类参与。接下来，这个音频脚本会被转换为一个计算机程序，每行代码都会调用一个任务特定的音频生成模型或计算操作函数（例如， concatenate、mix）。计算机程序的执行将获得一个可解释的音频生成解决方案。我们在多个实际应用场景中证明了WavJourney的实用性，包括科幻、教育和广播剧。WavJourney的可说明和互动设计增强了人机共创的多轮 діало格，提高了创作控制和适应性。WavJourney声音化了人类的想像力，开启了新的创作可能性在多媒体内容创建领域。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/27/cs.SD_2023_07_27/" data-id="clmjn91nm009x0j880l9d1qp0" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_07_27" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/27/eess.AS_2023_07_27/" class="article-date">
  <time datetime="2023-07-26T16:00:00.000Z" itemprop="datePublished">2023-07-27</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/27/eess.AS_2023_07_27/">eess.AS - 2023-07-27 22:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Audio-Inputs-for-Active-Speaker-Detection-and-Localization-via-Microphone-Array"><a href="#Audio-Inputs-for-Active-Speaker-Detection-and-Localization-via-Microphone-Array" class="headerlink" title="Audio Inputs for Active Speaker Detection and Localization via Microphone Array"></a>Audio Inputs for Active Speaker Detection and Localization via Microphone Array</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14739">http://arxiv.org/abs/2307.14739</a></li>
<li>repo_url: None</li>
<li>paper_authors: Davide Berghi, Philip J. B. Jackson</li>
<li>for: 本研究探讨了基于多通道音频的 aktive speaker detection 和位置测定（ASDL）问题。</li>
<li>methods: 该研究使用了 convolutional recurrent neural network（CRNN），使用了多通道音频中的空间声学特征，并对噪声的影响进行了测试。</li>
<li>results: 研究发现，使用GCC-PHAT和SALSA特征可以减少噪声的影响，而 beamforming 方法可以提高ASDL的性能。  Additionally, the study found that the number of channels and the sampling density of the microphone array have a significant impact on the performance of ASDL.<details>
<summary>Abstract</summary>
This study considers the problem of detecting and locating an active talker's horizontal position from multichannel audio captured by a microphone array. We refer to this as active speaker detection and localization (ASDL). Our goal was to investigate the performance of spatial acoustic features extracted from the multichannel audio as the input of a convolutional recurrent neural network (CRNN), in relation to the number of channels employed and additive noise. To this end, experiments were conducted to compare the generalized cross-correlation with phase transform (GCC-PHAT), the spatial cue-augmented log-spectrogram (SALSA) features, and a recently-proposed beamforming method, evaluating their robustness to various noise intensities. The array aperture and sampling density were tested by taking subsets from the 16-microphone array. Results and tests of statistical significance demonstrate the microphones' contribution to performance on the TragicTalkers dataset, which offers opportunities to investigate audio-visual approaches in the future.
</details>
<details>
<summary>摘要</summary>
Here's the simplified Chinese translation:这个研究关注的是从多通道音频记录的扬声器的活动位置探测和定位（ASDL）问题。我们的目标是 investigate CRNN（卷积隐estamp）的输入为多通道音频中的空间音学特征（GCC-PHAT、SALSA）的性能，与通道数和附加噪声之间的关系。为此，我们进行了比较减 correlated with phase transform（GCC-PHAT）、空间cue-augmented log-spectrogram（SALSA）特征和一种最近提出的扬声器方法的性能，对不同噪声强度进行评估。我们还测试了数组的开口和采样密度，使用16个 Microphonearray中的子集。结果和统计测试表明每个 Microphone的贡献，并提供了未来 investigate audio-visualapproaches的机会。
</details></li>
</ul>
<hr>
<h2 id="Physics-Informed-Neural-Network-for-Head-Related-Transfer-Function-Upsampling"><a href="#Physics-Informed-Neural-Network-for-Head-Related-Transfer-Function-Upsampling" class="headerlink" title="Physics Informed Neural Network for Head-Related Transfer Function Upsampling"></a>Physics Informed Neural Network for Head-Related Transfer Function Upsampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14650">http://arxiv.org/abs/2307.14650</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/feima0011/physics-informed-neural-network-for-head-related-transfer-function-upsampling">https://github.com/feima0011/physics-informed-neural-network-for-head-related-transfer-function-upsampling</a></li>
<li>paper_authors: Fei Ma, Thushara D. Abhayapala, Prasanga N. Samarasinghe, Xingyu Chen</li>
<li>for: 提高虚拟听觉体验的准确性，使用physics-informed neural network（PINN）方法进行HRTF上扩。</li>
<li>methods: 利用Helmholtz方程作为更多信息来约束上扩过程，使得生成的增强后HRTF具有物理准确性，并且采用SH分解来控制PINN网络的宽度和深度。</li>
<li>results: 对多个数据集进行比较，PINN方法在 interpolate 和 extrapolate 两种情况下表现出色，较SH方法有更好的性能。<details>
<summary>Abstract</summary>
Head-related transfer functions (HRTFs) capture the spatial and spectral features that a person uses to localize sound sources in space and thus are vital for creating an authentic virtual acoustic experience. However, practical HRTF measurement systems can only provide an incomplete measurement of a person's HRTFs, and this necessitates HRTF upsampling. This paper proposes a physics-informed neural network (PINN) method for HRTF upsampling. Unlike other upsampling methods which are based on the measured HRTFs only, the PINN method exploits the Helmholtz equation as additional information for constraining the upsampling process. This helps the PINN method to generate physically amiable upsamplings which generalize beyond the measured HRTFs. Furthermore, the width and the depth of the PINN are set according to the dimensionality of HRTFs under spherical harmonic (SH) decomposition and the Helmholtz equation. This makes the PINN have an appropriate level of expressiveness and thus does not suffer from under-fitting and over-fitting problems. Numerical experiments confirm the superior performance of the PINN method for HRTF upsampling in both interpolation and extrapolation scenarios over several datasets in comparison with the SH methods.
</details>
<details>
<summary>摘要</summary>
人头相关传函数（HRTF）捕捉声音源在空间中的特征和频谱特征，因此是创建真实的虚拟声学体验的关键。然而，实际的HRTF测量系统只能提供HRTF的不完全测量，因此需要HRTF upsampling。这篇论文提出了基于物理学习神经网络（PINN）方法的HRTF upsampling方法。与其他upsampling方法不同，PINN方法利用Helmholtz方程作为更多的约束来控制upsampling过程。这使得PINN方法能够生成符合物理规则的upsampling，并且在扩展测量HRTF的场景中表现出超过SH方法的优势。此外，PINN的宽度和深度设置与HRTF在圆柱幂分解中的维度和Helmholtz方程相关。这使得PINN具有相应的表达能力，从而不会出现过拟合和下降问题。数值实验证明PINN方法在 interpolación和 extrapolation 场景中对多个数据集表现出了superior performance  contrasted with SH方法。
</details></li>
</ul>
<hr>
<h2 id="NeuroHeed-Neuro-Steered-Speaker-Extraction-using-EEG-Signals"><a href="#NeuroHeed-Neuro-Steered-Speaker-Extraction-using-EEG-Signals" class="headerlink" title="NeuroHeed: Neuro-Steered Speaker Extraction using EEG Signals"></a>NeuroHeed: Neuro-Steered Speaker Extraction using EEG Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14303">http://arxiv.org/abs/2307.14303</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zexu Pan, Marvin Borsdorf, Siqi Cai, Tanja Schultz, Haizhou Li</li>
<li>for: 本研究旨在开发一种基于EEG信号的选择性听说模型，以便在听说场景中提取主要听众的语音信号。</li>
<li>methods: 本研究使用EEG信号来建立一个神经元吸引器，该吸引器与听众的注意力时间相关，以便提取主要听众的语音信号。</li>
<li>results: 实验结果表明，NeuroHeed模型可以有效地提取主要听众的语音信号，并达到高质量、优良 восприятие和 inteligibilty 在两个说话者场景中。<details>
<summary>Abstract</summary>
Humans possess the remarkable ability to selectively attend to a single speaker amidst competing voices and background noise, known as selective auditory attention. Recent studies in auditory neuroscience indicate a strong correlation between the attended speech signal and the corresponding brain's elicited neuronal activities, which the latter can be measured using affordable and non-intrusive electroencephalography (EEG) devices. In this study, we present NeuroHeed, a speaker extraction model that leverages EEG signals to establish a neuronal attractor which is temporally associated with the speech stimulus, facilitating the extraction of the attended speech signal in a cocktail party scenario. We propose both an offline and an online NeuroHeed, with the latter designed for real-time inference. In the online NeuroHeed, we additionally propose an autoregressive speaker encoder, which accumulates past extracted speech signals for self-enrollment of the attended speaker information into an auditory attractor, that retains the attentional momentum over time. Online NeuroHeed extracts the current window of the speech signals with guidance from both attractors. Experimental results demonstrate that NeuroHeed effectively extracts brain-attended speech signals, achieving high signal quality, excellent perceptual quality, and intelligibility in a two-speaker scenario.
</details>
<details>
<summary>摘要</summary>
人类具有选择性听取Single speaker amidst competing voices and background noise的能力，称为选择性听取。最近的听auditory neuroscience研究表明， attended speech signal和对应的大脑活动之间存在强相关关系，可以使用可得性和不侵入性的电enzephalography（EEG）设备来测量。在这个研究中，我们提出了NeuroHeed模型，利用EEG信号来建立一个neuronal attractor，该attractor在时间方面与语音刺激相关。这使得可以在庆酒party scenario中提取获得了注意力的speech signal。我们提出了两种NeuroHeed，一种是Offline NeuroHeed，另一种是在线NeuroHeed。在线NeuroHeed还包括一个自适应 speaker encoder，该encoder在过去提取的speech signal基础上积累 past extracted speech signals，以便在注意力保持的情况下，将注意力集中在获得了注意力的speaker上。在线NeuroHeed在当前窗口中提取speech signal，并且受到两个attractor的引导。实验结果表明，NeuroHeed能够有效地提取大脑注意力的speech signal，实现高质量的信号、优秀的感知质量和智能性在两个speaker scenario中。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/27/eess.AS_2023_07_27/" data-id="clmjn91p900df0j88dybvd7b8" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_07_27" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/27/eess.IV_2023_07_27/" class="article-date">
  <time datetime="2023-07-26T16:00:00.000Z" itemprop="datePublished">2023-07-27</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/27/eess.IV_2023_07_27/">eess.IV - 2023-07-27 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Weakly-Supervised-AI-for-Efficient-Analysis-of-3D-Pathology-Samples"><a href="#Weakly-Supervised-AI-for-Efficient-Analysis-of-3D-Pathology-Samples" class="headerlink" title="Weakly Supervised AI for Efficient Analysis of 3D Pathology Samples"></a>Weakly Supervised AI for Efficient Analysis of 3D Pathology Samples</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14907">http://arxiv.org/abs/2307.14907</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mahmoodlab/mamba">https://github.com/mahmoodlab/mamba</a></li>
<li>paper_authors: Andrew H. Song, Mane Williams, Drew F. K. Williamson, Guillaume Jaume, Andrew Zhang, Bowen Chen, Robert Serafin, Jonathan T. C. Liu, Alex Baras, Anil V. Parwani, Faisal Mahmood</li>
<li>for: 这个研究旨在开发一种基于深度学习的平台，用于处理多种成像模式的3D组织图像，预测病人结果。</li>
<li>methods: 该平台使用多样化的3D块分析方法，基于5年生化复诊结果进行风险分化网络训练。</li>
<li>results: 研究发现，使用3D块方法可以提高预测性能，并且可以减少采样偏误的风险，建议在临床实践中使用3D成像技术进行诊断和预测。<details>
<summary>Abstract</summary>
Human tissue and its constituent cells form a microenvironment that is fundamentally three-dimensional (3D). However, the standard-of-care in pathologic diagnosis involves selecting a few two-dimensional (2D) sections for microscopic evaluation, risking sampling bias and misdiagnosis. Diverse methods for capturing 3D tissue morphologies have been developed, but they have yet had little translation to clinical practice; manual and computational evaluations of such large 3D data have so far been impractical and/or unable to provide patient-level clinical insights. Here we present Modality-Agnostic Multiple instance learning for volumetric Block Analysis (MAMBA), a deep-learning-based platform for processing 3D tissue images from diverse imaging modalities and predicting patient outcomes. Archived prostate cancer specimens were imaged with open-top light-sheet microscopy or microcomputed tomography and the resulting 3D datasets were used to train risk-stratification networks based on 5-year biochemical recurrence outcomes via MAMBA. With the 3D block-based approach, MAMBA achieves an area under the receiver operating characteristic curve (AUC) of 0.86 and 0.74, superior to 2D traditional single-slice-based prognostication (AUC of 0.79 and 0.57), suggesting superior prognostication with 3D morphological features. Further analyses reveal that the incorporation of greater tissue volume improves prognostic performance and mitigates risk prediction variability from sampling bias, suggesting the value of capturing larger extents of heterogeneous 3D morphology. With the rapid growth and adoption of 3D spatial biology and pathology techniques by researchers and clinicians, MAMBA provides a general and efficient framework for 3D weakly supervised learning for clinical decision support and can help to reveal novel 3D morphological biomarkers for prognosis and therapeutic response.
</details>
<details>
<summary>摘要</summary>
人类组织和其内部细胞形成一个基本三维（3D）的微环境。然而，现行标准的病理诊断方法仅选择一些二维（2D）的section进行微scopic评估，可能存在采样偏见和诊断错误。各种用于捕捉3D组织形态的方法已经发展出来，但它们在临床实践中尚未得到广泛应用。我们现在介绍了模态无关多例学习 для块分析（MAMBA），一种基于深度学习的平台，用于处理不同成像模式的3D组织图像并预测病人结果。我们使用了扫描镜开式光sheet微scopy或微计算tomography扫描患有前列腺癌的肉瘤样本，并使用MAMBA进行风险分级网络的训练，以达到5年生物化学回报的 outcome。与传统的2D单片 slice-based预测相比，MAMBA的3D块基本approach在 receiver operating characteristic曲线（AUC）中得分0.86和0.74，表明3D形态特征可以提供更好的预测性。进一步分析表明，包含更大的组织体积可以提高预测性并减少采样偏见导致的预测变化，这表明3D morphological特征的捕捉是重要的。随着研究人员和临床医生对3D空间生物学和病理学技术的快速成长和采用，MAMBA提供了一个通用和高效的3D弱监学习框架，可以帮助揭示新的3D形态生物标志物和预测病人response。
</details></li>
</ul>
<hr>
<h2 id="A-full-resolution-training-framework-for-Sentinel-2-image-fusion"><a href="#A-full-resolution-training-framework-for-Sentinel-2-image-fusion" class="headerlink" title="A full-resolution training framework for Sentinel-2 image fusion"></a>A full-resolution training framework for Sentinel-2 image fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14864">http://arxiv.org/abs/2307.14864</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/matciotola/FR-FUSE">https://github.com/matciotola/FR-FUSE</a></li>
<li>paper_authors: Matteo Ciotola, Mario Ragosta, Giovanni Poggi, Giuseppe Scarpa</li>
<li>for: 这篇论文旨在提出一种新的无监督框架，用于深度学习模型的决 Height 采集 Sentinel-2 图像的超分辨率处理。</li>
<li>methods: 该方案使用 Sentinel-2 图像的 10-m 和 20-m 频道进行融合，而不需要降解分辨率生成训练数据。同时，提出了一种适合的损失函数，以确保网络预测和输入组件之间的循环一致性。</li>
<li>results: 在我们的初步实验中，提出的方案已经显示出了与监督方法相比的扩展性。此外，由于构造的损失函数，得到的训练网络可以归类为多分辨率分析方法。<details>
<summary>Abstract</summary>
This work presents a new unsupervised framework for training deep learning models for super-resolution of Sentinel-2 images by fusion of its 10-m and 20-m bands. The proposed scheme avoids the resolution downgrade process needed to generate training data in the supervised case. On the other hand, a proper loss that accounts for cycle-consistency between the network prediction and the input components to be fused is proposed. Despite its unsupervised nature, in our preliminary experiments the proposed scheme has shown promising results in comparison to the supervised approach. Besides, by construction of the proposed loss, the resulting trained network can be ascribed to the class of multi-resolution analysis methods.
</details>
<details>
<summary>摘要</summary>
这个工作提出了一种新的无监督框架，用于深度学习模型的超分辨率准备，基于报告20米和10米频道的融合。提议的方案不需要生成训练数据的分辨率下降过程，同时提出了一种适当的损失函数，该函数考虑了网络预测和输入组件的循环一致性。在我们的初步实验中，提议的方案已经达到了与监督方法相比的承诺性。此外，由于构造的损失函数，得到的训练网络可以被归类为多分辨率分析方法。
</details></li>
</ul>
<hr>
<h2 id="Seeing-through-the-Brain-Image-Reconstruction-of-Visual-Perception-from-Human-Brain-Signals"><a href="#Seeing-through-the-Brain-Image-Reconstruction-of-Visual-Perception-from-Human-Brain-Signals" class="headerlink" title="Seeing through the Brain: Image Reconstruction of Visual Perception from Human Brain Signals"></a>Seeing through the Brain: Image Reconstruction of Visual Perception from Human Brain Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02510">http://arxiv.org/abs/2308.02510</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu-Ting Lan, Kan Ren, Yansen Wang, Wei-Long Zheng, Dongsheng Li, Bao-Liang Lu, Lili Qiu</li>
<li>for: 这篇论文的目的是重建视觉刺激图像基于电энцеphalography（EEG）数据。</li>
<li>methods: 这篇论文提出了一个全面的执行管道，名为NeuroImagen，用于从EEG数据中重建视觉刺激图像。该管道包括一种新的多级感知信息解码器，用于从EEG数据中提取多级输出。然后，一种潜在扩散模型将利用提取的信息来重建高分辨率的视觉刺激图像。</li>
<li>results: 实验结果表明，该方法可以有效地重建视觉刺激图像，并且对比于现有方法有较高的量化性表现。<details>
<summary>Abstract</summary>
Seeing is believing, however, the underlying mechanism of how human visual perceptions are intertwined with our cognitions is still a mystery. Thanks to the recent advances in both neuroscience and artificial intelligence, we have been able to record the visually evoked brain activities and mimic the visual perception ability through computational approaches. In this paper, we pay attention to visual stimuli reconstruction by reconstructing the observed images based on portably accessible brain signals, i.e., electroencephalography (EEG) data. Since EEG signals are dynamic in the time-series format and are notorious to be noisy, processing and extracting useful information requires more dedicated efforts; In this paper, we propose a comprehensive pipeline, named NeuroImagen, for reconstructing visual stimuli images from EEG signals. Specifically, we incorporate a novel multi-level perceptual information decoding to draw multi-grained outputs from the given EEG data. A latent diffusion model will then leverage the extracted information to reconstruct the high-resolution visual stimuli images. The experimental results have illustrated the effectiveness of image reconstruction and superior quantitative performance of our proposed method.
</details>
<details>
<summary>摘要</summary>
视觉是信任的来源，但是人类视觉与认知之间的内部机制仍然是一个谜。随着 neuroscience 和人工智能的最近进步，我们可以记录人类视觉活动和模拟视觉能力通过计算方法。在这篇论文中，我们关注于基于可 portable 脑电声信号（EEG）的视觉刺激重建。因为EEG信号是时间序列格式的动态信号，容易受到干扰，因此处理和提取有用信息需要更多的努力。为解决这个问题，我们提出了一个完整的推管道，名为NeuroImagen，可以从EEG信号中提取多层次的视觉信息，并使用扩散模型重建高分辨率的视觉刺激图像。实验结果表明我们的方法可以有效地重建图像，并且在量化性能方面表现出色。
</details></li>
</ul>
<hr>
<h2 id="Test-Time-Adaptation-for-Blind-Image-Quality-Assessment"><a href="#Test-Time-Adaptation-for-Blind-Image-Quality-Assessment" class="headerlink" title="Test Time Adaptation for Blind Image Quality Assessment"></a>Test Time Adaptation for Blind Image Quality Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14735">http://arxiv.org/abs/2307.14735</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shankhanil006/tta-iqa">https://github.com/shankhanil006/tta-iqa</a></li>
<li>paper_authors: Subhadeep Roy, Shankhanil Mitra, Soma Biswas, Rajiv Soundararajan</li>
<li>for: 提高隐藏图像质量评估（IQA）算法的执行时间性能。</li>
<li>methods: 使用两个新的质量相关的辅助任务：批处理级别的群集对比损失和样本级别的相对排名损失，以使模型更加质量相关，适应目标数据。</li>
<li>results: 使用一小批测试分布中的图像更新批处理平均值可以实现显著提高性能。<details>
<summary>Abstract</summary>
While the design of blind image quality assessment (IQA) algorithms has improved significantly, the distribution shift between the training and testing scenarios often leads to a poor performance of these methods at inference time. This motivates the study of test time adaptation (TTA) techniques to improve their performance at inference time. Existing auxiliary tasks and loss functions used for TTA may not be relevant for quality-aware adaptation of the pre-trained model. In this work, we introduce two novel quality-relevant auxiliary tasks at the batch and sample levels to enable TTA for blind IQA. In particular, we introduce a group contrastive loss at the batch level and a relative rank loss at the sample level to make the model quality aware and adapt to the target data. Our experiments reveal that even using a small batch of images from the test distribution helps achieve significant improvement in performance by updating the batch normalization statistics of the source model.
</details>
<details>
<summary>摘要</summary>
尽管干燥图像质量评估（IQA）算法的设计已经得到了显著改进，但在执行时，测试场景和训练场景之间的分布偏移 frequently leads to poor performance of these methods. This motivates the study of test time adaptation（TTA）techniques to improve their performance at inference time. Existing auxiliary tasks and loss functions used for TTA may not be relevant for quality-aware adaptation of the pre-trained model. In this work, we introduce two novel quality-relevant auxiliary tasks at the batch and sample levels to enable TTA for blind IQA. In particular, we introduce a group contrastive loss at the batch level and a relative rank loss at the sample level to make the model quality aware and adapt to the target data. Our experiments reveal that even using a small batch of images from the test distribution helps achieve significant improvement in performance by updating the batch normalization statistics of the source model.
</details></li>
</ul>
<hr>
<h2 id="A-Multimodal-Supervised-Machine-Learning-Approach-for-Satellite-based-Wildfire-Identification-in-Europe"><a href="#A-Multimodal-Supervised-Machine-Learning-Approach-for-Satellite-based-Wildfire-Identification-in-Europe" class="headerlink" title="A Multimodal Supervised Machine Learning Approach for Satellite-based Wildfire Identification in Europe"></a>A Multimodal Supervised Machine Learning Approach for Satellite-based Wildfire Identification in Europe</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02508">http://arxiv.org/abs/2308.02508</a></li>
<li>repo_url: None</li>
<li>paper_authors: Angelica Urbanelli, Luca Barco, Edoardo Arnaudo, Claudio Rossi</li>
<li>for: 提高自动化卫星热点检测系统的准确性，以适应自然灾害的加频。</li>
<li>methods: 跨参照模式热点检测服务（MODIS和VIIRS）和欧洲森林火灾信息系统（EFFIS）数据库，建立大规模热点数据集，用于森林火灾相关研究。提出一种多模式指导学习方法，利用多种数据源，如ERSI年度土地用途土地覆盖（LULC）和 Copernicus Sentinel-3 数据，准确地分类热点检测结果。</li>
<li>results: 实验结果表明，我们的方法在森林火灾标识任务中具有效果。<details>
<summary>Abstract</summary>
The increasing frequency of catastrophic natural events, such as wildfires, calls for the development of rapid and automated wildfire detection systems. In this paper, we propose a wildfire identification solution to improve the accuracy of automated satellite-based hotspot detection systems by leveraging multiple information sources. We cross-reference the thermal anomalies detected by the Moderate-resolution Imaging Spectroradiometer (MODIS) and the Visible Infrared Imaging Radiometer Suite (VIIRS) hotspot services with the European Forest Fire Information System (EFFIS) database to construct a large-scale hotspot dataset for wildfire-related studies in Europe. Then, we propose a novel multimodal supervised machine learning approach to disambiguate hotspot detections, distinguishing between wildfires and other events. Our methodology includes the use of multimodal data sources, such as the ERSI annual Land Use Land Cover (LULC) and the Copernicus Sentinel-3 data. Experimental results demonstrate the effectiveness of our approach in the task of wildfire identification.
</details>
<details>
<summary>摘要</summary>
随着自然灾害的频繁发生，如野火，需要开发高速自动化野火检测系统。在这篇论文中，我们提出了一种野火标识解决方案，以提高自动遥感系统中热点检测的准确性。我们将模拟高分辨率 спектро镜谱仪(MODIS)和可见红外成像雷达仪(VIIRS)的热点服务与欧洲森林火灾信息系统(EFFIS)数据库进行交叉引用，以构建欧洲大规模热点数据集用于野火相关研究。然后，我们提出了一种新的多模式超vised机器学习方法，用于细分热点检测，将野火和其他事件分开。我们的方法包括使用多模式数据源，如地理信息系统(ERSI)年度土地用途土地覆盖(LULC)和科学技术卫星(Copernicus)三号卫星数据。实验结果表明，我们的方法在野火标识任务中具有效果。
</details></li>
</ul>
<hr>
<h2 id="LLDiffusion-Learning-Degradation-Representations-in-Diffusion-Models-for-Low-Light-Image-Enhancement"><a href="#LLDiffusion-Learning-Degradation-Representations-in-Diffusion-Models-for-Low-Light-Image-Enhancement" class="headerlink" title="LLDiffusion: Learning Degradation Representations in Diffusion Models for Low-Light Image Enhancement"></a>LLDiffusion: Learning Degradation Representations in Diffusion Models for Low-Light Image Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14659">http://arxiv.org/abs/2307.14659</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/taowangzj/lldiffusion">https://github.com/taowangzj/lldiffusion</a></li>
<li>paper_authors: Tao Wang, Kaihao Zhang, Ziqian Shao, Wenhan Luo, Bjorn Stenger, Tae-Kyun Kim, Wei Liu, Hongdong Li</li>
<li>for: 提高低光照图像的增强（LLIE）</li>
<li>methods: 使用扩散模型，并 integrates 衰变和图像约束，以提高图像增强效果。</li>
<li>results: 比对多个 benchmark 数据集，实验结果表明，提案的 LLDiffusion 方法可以 Quantitatively 和 Qualitatively 超过当前的 LLIE 方法。Here’s a breakdown of each point:</li>
<li>for: The paper is written for low-light image enhancement (LLIE), specifically addressing the limitation of current deep learning methods that overlook the importance of considering degradation representations.</li>
<li>methods: The proposed method uses a degradation-aware learning scheme based on diffusion models, which integrates degradation and image priors into the diffusion process. The method includes a joint learning framework for image generation and enhancement, as well as a well-designed dynamic diffusion module that takes into account both the color map and the latent degradation representations.</li>
<li>results: The proposed method is evaluated on several well-known benchmark datasets, including synthetic and real-world unpaired datasets. The results demonstrate that LLDiffusion outperforms state-of-the-art LLIE methods both quantitatively and qualitatively.<details>
<summary>Abstract</summary>
Current deep learning methods for low-light image enhancement (LLIE) typically rely on pixel-wise mapping learned from paired data. However, these methods often overlook the importance of considering degradation representations, which can lead to sub-optimal outcomes. In this paper, we address this limitation by proposing a degradation-aware learning scheme for LLIE using diffusion models, which effectively integrates degradation and image priors into the diffusion process, resulting in improved image enhancement. Our proposed degradation-aware learning scheme is based on the understanding that degradation representations play a crucial role in accurately modeling and capturing the specific degradation patterns present in low-light images. To this end, First, a joint learning framework for both image generation and image enhancement is presented to learn the degradation representations. Second, to leverage the learned degradation representations, we develop a Low-Light Diffusion model (LLDiffusion) with a well-designed dynamic diffusion module. This module takes into account both the color map and the latent degradation representations to guide the diffusion process. By incorporating these conditioning factors, the proposed LLDiffusion can effectively enhance low-light images, considering both the inherent degradation patterns and the desired color fidelity. Finally, we evaluate our proposed method on several well-known benchmark datasets, including synthetic and real-world unpaired datasets. Extensive experiments on public benchmarks demonstrate that our LLDiffusion outperforms state-of-the-art LLIE methods both quantitatively and qualitatively. The source code and pre-trained models are available at https://github.com/TaoWangzj/LLDiffusion.
</details>
<details>
<summary>摘要</summary>
当前的深度学习方法 для低光照图像提升（LLIE）通常是基于像素级映射学习的，但这些方法经常忽视了质量的衰减表示。在这篇论文中，我们解决这个限制，提出了一种考虑衰减表示的学习方案，使得图像提升更加稳定。我们的提议的衰减意识学习方案基于衰减表示在低光照图像中的重要作用。为了实现这一点，我们首先提出了一种共同学习框架，用于学习图像生成和图像提升。其次，我们开发了一种基于扩散模型的低光照扩散模型（LLDiffusion），该模型具有一个有效地考虑了颜色图和背景衰减表示的动态扩散模块。通过将这些条件因素纳入考虑，我们的LLDiffusion可以更好地提升低光照图像，考虑到了图像的自然衰减模式以及颜色准确性。最后，我们对一些公共 benchmark 上进行了广泛的实验，证明了我们的LLDiffusion在量和质量上都超过了现有的LLIE方法。代码和预训练模型可以在 <https://github.com/TaoWangzj/LLDiffusion> 上获取。
</details></li>
</ul>
<hr>
<h2 id="A-Weakly-Supervised-Segmentation-Network-Embedding-Cross-scale-Attention-Guidance-and-Noise-sensitive-Constraint-for-Detecting-Tertiary-Lymphoid-Structures-of-Pancreatic-Tumors"><a href="#A-Weakly-Supervised-Segmentation-Network-Embedding-Cross-scale-Attention-Guidance-and-Noise-sensitive-Constraint-for-Detecting-Tertiary-Lymphoid-Structures-of-Pancreatic-Tumors" class="headerlink" title="A Weakly Supervised Segmentation Network Embedding Cross-scale Attention Guidance and Noise-sensitive Constraint for Detecting Tertiary Lymphoid Structures of Pancreatic Tumors"></a>A Weakly Supervised Segmentation Network Embedding Cross-scale Attention Guidance and Noise-sensitive Constraint for Detecting Tertiary Lymphoid Structures of Pancreatic Tumors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14603">http://arxiv.org/abs/2307.14603</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bingxue Wang, Liwen Zou, Jun Chen, Yingying Cao, Zhenghua Cai, Yudong Qiu, Liang Mao, Zhongqiu Wang, Jingya Chen, Luying Gui, Xiaoping Yang</li>
<li>For: 这篇研究旨在探讨一种几何学习的方法，用于检测胰脏病变中的次次性林肺结构（TLS）。* Methods: 我们提出了一个弱监督分类网络，来检测TLS。我们首先使用预训练的模型进行核lei分 segmentation，然后将精度检测过滤到我们设计的 linfocyte density 对应。我们还实现了一个跨度对应 Mechanism，将粗细度特征学习自原始胰脏病变图像，以及细节度特征学习自我设计的 linfocyte density 对应。* Results: 我们将这个方法应用于两个收集的数据集，结果显示，我们的提议方法在TLS检测精度方面与现有的分类型检测方法相比，有 statistically significant 的优化。此外，我们还将这个方法应用于研究胰脏病变中TLS的density和周围血管侵入之间的相互关系，并获得了一些临床有用的结果。<details>
<summary>Abstract</summary>
The presence of tertiary lymphoid structures (TLSs) on pancreatic pathological images is an important prognostic indicator of pancreatic tumors. Therefore, TLSs detection on pancreatic pathological images plays a crucial role in diagnosis and treatment for patients with pancreatic tumors. However, fully supervised detection algorithms based on deep learning usually require a large number of manual annotations, which is time-consuming and labor-intensive. In this paper, we aim to detect the TLSs in a manner of few-shot learning by proposing a weakly supervised segmentation network. We firstly obtain the lymphocyte density maps by combining a pretrained model for nuclei segmentation and a domain adversarial network for lymphocyte nuclei recognition. Then, we establish a cross-scale attention guidance mechanism by jointly learning the coarse-scale features from the original histopathology images and fine-scale features from our designed lymphocyte density attention. A noise-sensitive constraint is introduced by an embedding signed distance function loss in the training procedure to reduce tiny prediction errors. Experimental results on two collected datasets demonstrate that our proposed method significantly outperforms the state-of-the-art segmentation-based algorithms in terms of TLSs detection accuracy. Additionally, we apply our method to study the congruent relationship between the density of TLSs and peripancreatic vascular invasion and obtain some clinically statistical results.
</details>
<details>
<summary>摘要</summary>
pancreatic tumors中的次级血液结构（TLSs）的存在是诊断和治疗中非常重要的诊断指标。因此，TLSs的检测在pancreatic tumors中扮演着关键的角色。然而，通常需要大量的手动标注，这是时间consuming和劳动密集的。在这篇论文中，我们想要通过几shot学习来检测TLSs，我们提出了一种弱型指导网络。首先，我们获得了lymphocyte density map，通过结合预训练的核体分割模型和域 adversarial network来识别lymphocyte的核体。然后，我们建立了跨度级别的注意力引导机制，通过同时学习原始的 histopathology 图像的粗级特征和我们设计的lymphocyte density注意力来实现。在训练过程中，我们引入了一个嵌入签名距离函数损失，以降低微小预测错误。实验结果表明，我们的提议方法在pancreatic tumors中的TLSs检测精度上明显超过了现状的segmentation-based算法。此外，我们通过应用我们的方法来研究peripancreatic vascular invasion和TLSs的密切关系，并获得了一些临床 statistically significant的结果。
</details></li>
</ul>
<hr>
<h2 id="FocalErrorNet-Uncertainty-aware-focal-modulation-network-for-inter-modal-registration-error-estimation-in-ultrasound-guided-neurosurgery"><a href="#FocalErrorNet-Uncertainty-aware-focal-modulation-network-for-inter-modal-registration-error-estimation-in-ultrasound-guided-neurosurgery" class="headerlink" title="FocalErrorNet: Uncertainty-aware focal modulation network for inter-modal registration error estimation in ultrasound-guided neurosurgery"></a>FocalErrorNet: Uncertainty-aware focal modulation network for inter-modal registration error estimation in ultrasound-guided neurosurgery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14520">http://arxiv.org/abs/2307.14520</a></li>
<li>repo_url: None</li>
<li>paper_authors: Soorena Salari, Amirhossein Rasoulian, Hassan Rivaz, Yiming Xiao</li>
<li>for: brain tumor resection, accurate removal of cancerous tissues while preserving eloquent regions is crucial to the safety and outcomes of the treatment.</li>
<li>methods: intra-operative ultrasound (iUS) has been adopted to provide real-time images to track brain shift, and inter-modal (i.e., MRI-iUS) registration is often required to update the pre-surgical plan.</li>
<li>results: a novel deep learning technique based on 3D focal modulation in conjunction with uncertainty estimation to accurately assess MRI-iUS registration errors for brain tumor surgery, with an estimation error of 0.59+-0.57 mm.Here is the result in Simplified Chinese text:</li>
<li>for: 脑肿瘤镜下手术，准确除除肿瘤组织，保留语言功能区域是治疗安全和结果的关键。</li>
<li>methods: 使用实时图像跟踪脑shift的术前评估，并通过多Modal（i.e., MRI-iUS）注册更新预后方案。</li>
<li>results: 一种基于3D焦点调制的深度学习技术，以及不确定度估计，为脑肿瘤手术中MRI-iUS注册错误的准确评估，错误估计值为0.59+-0.57 mm。<details>
<summary>Abstract</summary>
In brain tumor resection, accurate removal of cancerous tissues while preserving eloquent regions is crucial to the safety and outcomes of the treatment. However, intra-operative tissue deformation (called brain shift) can move the surgical target and render the pre-surgical plan invalid. Intra-operative ultrasound (iUS) has been adopted to provide real-time images to track brain shift, and inter-modal (i.e., MRI-iUS) registration is often required to update the pre-surgical plan. Quality control for the registration results during surgery is important to avoid adverse outcomes, but manual verification faces great challenges due to difficult 3D visualization and the low contrast of iUS. Automatic algorithms are urgently needed to address this issue, but the problem was rarely attempted. Therefore, we propose a novel deep learning technique based on 3D focal modulation in conjunction with uncertainty estimation to accurately assess MRI-iUS registration errors for brain tumor surgery. Developed and validated with the public RESECT clinical database, the resulting algorithm can achieve an estimation error of 0.59+-0.57 mm.
</details>
<details>
<summary>摘要</summary>
在脑肿瘤切除手术中，准确地移除癌细胞组织，同时保留eloquent区域的安全和效果是致命的。然而，操作期间脑部塑形（brain shift）可能导致手术目标移动，使原先的预期计划无效。为了提供实时图像，脑部ultrasound（iUS）已被采用，而inter-modal（i.e., MRI-iUS） регистрация经常需要更新预期计划。在手术中质量控制注册结果的重要性，由于3D视化和iUS的低对比度，导致手动验证困难。因此，我们提出了一种基于3D焦点修饰的深度学习技术，以确定MRI-iUS注册错误的准确评估方法。与公共RESECT临床数据库的开发和验证结果显示，这种算法可以实现注册错误的估计误差为0.59±0.57毫米。
</details></li>
</ul>
<hr>
<h2 id="Phenotype-preserving-metric-design-for-high-content-image-reconstruction-by-generative-inpainting"><a href="#Phenotype-preserving-metric-design-for-high-content-image-reconstruction-by-generative-inpainting" class="headerlink" title="Phenotype-preserving metric design for high-content image reconstruction by generative inpainting"></a>Phenotype-preserving metric design for high-content image reconstruction by generative inpainting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14436">http://arxiv.org/abs/2307.14436</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vaibhav Sharma, Artur Yakimovich</li>
<li>for: 这个论文主要用于研究高通量微scopic影像数据的自动化处理和修复技术，以提高生物系统学和药物层次creening应用。</li>
<li>methods: 这个论文使用了现有的image inpainting技术，如DeepFill V2和Edge Connect，对高通量fluorescence microscopy图像进行修复和Restoration。</li>
<li>results: 研究发现，通过精心调整和少量数据，这些技术可以准确修复微scopic影像，并且Restoration质量与图像区域大小相关。此外，提出了一种新的phenotype-preserving度量设计策略，以控制修复质量并避免不良修复。<details>
<summary>Abstract</summary>
In the past decades, automated high-content microscopy demonstrated its ability to deliver large quantities of image-based data powering the versatility of phenotypic drug screening and systems biology applications. However, as the sizes of image-based datasets grew, it became infeasible for humans to control, avoid and overcome the presence of imaging and sample preparation artefacts in the images. While novel techniques like machine learning and deep learning may address these shortcomings through generative image inpainting, when applied to sensitive research data this may come at the cost of undesired image manipulation. Undesired manipulation may be caused by phenomena such as neural hallucinations, to which some artificial neural networks are prone. To address this, here we evaluate the state-of-the-art inpainting methods for image restoration in a high-content fluorescence microscopy dataset of cultured cells with labelled nuclei. We show that architectures like DeepFill V2 and Edge Connect can faithfully restore microscopy images upon fine-tuning with relatively little data. Our results demonstrate that the area of the region to be restored is of higher importance than shape. Furthermore, to control for the quality of restoration, we propose a novel phenotype-preserving metric design strategy. In this strategy, the size and count of the restored biological phenotypes like cell nuclei are quantified to penalise undesirable manipulation. We argue that the design principles of our approach may also generalise to other applications.
</details>
<details>
<summary>摘要</summary>
在过去的几十年中，自动化高内容微scopia技术已经证明了它可以提供大量的图像数据，为phenotypic drug screening和系统生物学应用提供了多样化的能力。然而，随着图像数据的大小的增长，人类控制、避免和消除图像和样本准备 artifacts在图像中的成本变得不可持续。 novels技术如机器学习和深度学习可能会解决这些缺陷通过生成图像填充，但是当应用于敏感研究数据时，这可能会导致不желатель的图像修饰。这些修饰可能会由人工神经网络的神经抑制引起，导致图像修饰。为了解决这个问题，我们在高内容染料微scopiadataset中评估了state-of-the-art填充方法的图像修复能力。我们发现，Architecture如DeepFill V2和Edge Connect可以在高度微scopiadataset中 faithful restore microscopy images，并且只需要少量的数据进行微调。我们的结果表明，图像修复的区域大小比较重要于形态。此外，为了控制修复质量，我们提出了一种新的phenotype-preserving度量设计策略。在这种策略中，修复的生物fenotypes，如细胞核的大小和数量，被量化以 penalize不желатель的修饰。我们认为，我们的设计原则可能也会总结到其他应用中。
</details></li>
</ul>
<hr>
<h2 id="Optimization-of-Image-Acquisition-for-Earth-Observation-Satellites-via-Quantum-Computing"><a href="#Optimization-of-Image-Acquisition-for-Earth-Observation-Satellites-via-Quantum-Computing" class="headerlink" title="Optimization of Image Acquisition for Earth Observation Satellites via Quantum Computing"></a>Optimization of Image Acquisition for Earth Observation Satellites via Quantum Computing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14419">http://arxiv.org/abs/2307.14419</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antón Makarov, Márcio M. Taddei, Eneko Osaba, Giacomo Franceschetto, Esther Villar-Rodriguez, Izaskun Oregi</li>
<li>for: 这篇论文的目的是对于卫星图像获取时间调度问题进行优化，以找到在给定轨道通过时间下最佳的图像子集。</li>
<li>methods: 这篇论文使用了两种QUBO表述方法来解决这个问题，并且使用不同的缓存处理技术来处理非易式约束。</li>
<li>results: 实验结果显示，不同的表述方法和缓存处理技术对于解决这个问题有很大的影响，而且Current quantum computers可以解决的问题型例子限制在一定的大小上。<details>
<summary>Abstract</summary>
Satellite image acquisition scheduling is a problem that is omnipresent in the earth observation field; its goal is to find the optimal subset of images to be taken during a given orbit pass under a set of constraints. This problem, which can be modeled via combinatorial optimization, has been dealt with many times by the artificial intelligence and operations research communities. However, despite its inherent interest, it has been scarcely studied through the quantum computing paradigm. Taking this situation as motivation, we present in this paper two QUBO formulations for the problem, using different approaches to handle the non-trivial constraints. We compare the formulations experimentally over 20 problem instances using three quantum annealers currently available from D-Wave, as well as one of its hybrid solvers. Fourteen of the tested instances have been obtained from the well-known SPOT5 benchmark, while the remaining six have been generated ad-hoc for this study. Our results show that the formulation and the ancilla handling technique is crucial to solve the problem successfully. Finally, we also provide practical guidelines on the size limits of problem instances that can be realistically solved on current quantum computers.
</details>
<details>
<summary>摘要</summary>
卫星图像获取计划是地球观测领域中一个普遍存在的问题，其目标是在给定的轨道过程中选择最佳的图像子集，以满足一系列约束。这个问题可以被模型为 combinatorial optimization 问题，在人工智能和运筹学社区中已经得到了广泛的研究。然而，即使它具有潜在的兴趣，它在量子计算理解中却很少被研究。在这种情况下，我们在这篇论文中提出了两种 QUBO 表示方法，使用不同的方法来处理非质量约束。我们通过实验测试了这些表示方法，使用 D-Wave 提供的三个量子泵浸器和一个混合解决方案。我们测试的问题实例数量为 20，其中 14 个来自 SPOT5 标准准则，另外 6 个是为本研究而生成的。我们的结果表明，表示方法和卵处理技术是解决问题的关键。此外，我们还提供了现有量子计算机的实际问题大小限制。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/27/eess.IV_2023_07_27/" data-id="clmjn91q500fr0j882moagie4" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_07_26" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/26/cs.LG_2023_07_26/" class="article-date">
  <time datetime="2023-07-25T16:00:00.000Z" itemprop="datePublished">2023-07-26</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/26/cs.LG_2023_07_26/">cs.LG - 2023-07-26 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Fluorescent-Neuronal-Cells-v2-Multi-Task-Multi-Format-Annotations-for-Deep-Learning-in-Microscopy"><a href="#Fluorescent-Neuronal-Cells-v2-Multi-Task-Multi-Format-Annotations-for-Deep-Learning-in-Microscopy" class="headerlink" title="Fluorescent Neuronal Cells v2: Multi-Task, Multi-Format Annotations for Deep Learning in Microscopy"></a>Fluorescent Neuronal Cells v2: Multi-Task, Multi-Format Annotations for Deep Learning in Microscopy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14243">http://arxiv.org/abs/2307.14243</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luca Clissa, Antonio Macaluso, Roberto Morelli, Alessandra Occhinegro, Emiliana Piscitiello, Ludovico Taddei, Marco Luppi, Roberto Amici, Matteo Cerri, Timna Hitrec, Lorenzo Rinaldi, Antonio Zoccoli</li>
<li>for: This paper is written for researchers and scientists in the fields of life sciences and deep learning, with the goal of facilitating innovative research and methodological advancements in fluorescence microscopy analysis.</li>
<li>methods: The paper provides a collection of fluorescence microscopy images and ground-truth annotations for several learning tasks, including semantic segmentation, object detection, and counting. The images are annotated with diverse markers to highlight the anatomical or functional characteristics of rodent neuronal cells.</li>
<li>results: The paper aims to facilitate breakthroughs in fluorescence microscopy analysis and promote cutting-edge discoveries in life sciences by providing a comprehensive and accessible dataset for researchers to explore and benchmark their methods.<details>
<summary>Abstract</summary>
Fluorescent Neuronal Cells v2 is a collection of fluorescence microscopy images and the corresponding ground-truth annotations, designed to foster innovative research in the domains of Life Sciences and Deep Learning. This dataset encompasses three image collections in which rodent neuronal cells' nuclei and cytoplasm are stained with diverse markers to highlight their anatomical or functional characteristics. Alongside the images, we provide ground-truth annotations for several learning tasks, including semantic segmentation, object detection, and counting. The contribution is two-fold. First, given the variety of annotations and their accessible formats, we envision our work facilitating methodological advancements in computer vision approaches for segmentation, detection, feature learning, unsupervised and self-supervised learning, transfer learning, and related areas. Second, by enabling extensive exploration and benchmarking, we hope Fluorescent Neuronal Cells v2 will catalyze breakthroughs in fluorescence microscopy analysis and promote cutting-edge discoveries in life sciences. The data are available at: https://amsacta.unibo.it/id/eprint/7347
</details>
<details>
<summary>摘要</summary>
fluorescent neuronal cells v2 是一个包含 fluorescence microscopy 图像和相应的真实标注的集合，旨在推动生命科学和深度学习领域的创新研究。这个数据集包括三个图像集，其中 rodent neuronal cells 的核心和细胞质是使用不同标记物来高亮其形态或功能特征。同时，我们提供了真实标注数据，用于多种学习任务，包括semantic segmentation、object detection和计数。我们的贡献是两重。首先，由于数据集的多样性和可访问的格式，我们期望我们的工作会促进计算机视觉方法的进步，包括分割、检测、特征学习、自监学习、转移学习等领域。其次，通过提供广泛的探索和测试，我们希望 fluorescent neuronal cells v2 会促进 fluorescence microscopy 分析的进步，并促进生命科学的前沿研究。数据可以在以下地址获取：https://amsacta.unibo.it/id/eprint/7347
</details></li>
</ul>
<hr>
<h2 id="Evolving-Multi-Objective-Neural-Network-Controllers-for-Robot-Swarms"><a href="#Evolving-Multi-Objective-Neural-Network-Controllers-for-Robot-Swarms" class="headerlink" title="Evolving Multi-Objective Neural Network Controllers for Robot Swarms"></a>Evolving Multi-Objective Neural Network Controllers for Robot Swarms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14237">http://arxiv.org/abs/2307.14237</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karl Mason, Sabine Hauert</li>
<li>for: 本研究旨在提出一种多目标进化神经网络控制器来解决群体 робоット控制问题，以实现多个目标的同时满足。</li>
<li>methods: 该研究使用了一种多目标进化神经网络方法，通过在低精度Python模拟环境中训练控制器，然后在Webots高精度模拟环境中测试和评估控制器。</li>
<li>results: 研究结果表明，提出的方法可以有效地控制每个机器人，并且可以根据目标权重的调整而变化机器人群体的行为。同时，研究还证明了这种控制器可以在高精度模拟环境中进行规模化扩展，无需进一步 retrained。<details>
<summary>Abstract</summary>
Many swarm robotics tasks consist of multiple conflicting objectives. This research proposes a multi-objective evolutionary neural network approach to developing controllers for swarms of robots. The swarm robot controllers are trained in a low-fidelity Python simulator and then tested in a high-fidelity simulated environment using Webots. Simulations are then conducted to test the scalability of the evolved multi-objective robot controllers to environments with a larger number of robots. The results presented demonstrate that the proposed approach can effectively control each of the robots. The robot swarm exhibits different behaviours as the weighting for each objective is adjusted. The results also confirm that multi-objective neural network controllers evolved in a low-fidelity simulator can be transferred to high-fidelity simulated environments and that the controllers can scale to environments with a larger number of robots without further retraining needed.
</details>
<details>
<summary>摘要</summary>
多数蜂群控制任务包含多个冲突目标。本研究提出了一种多目标进化神经网络方法来开发蜂群机器人控制器。蜂群机器人控制器在低精度Python模拟器中训练，然后在使用Webots高精度模拟环境进行测试。为了评估算法的扩展性，在不同目标权重的情况下进行了多个 simulations。结果表明，提议的方法可以有效控制每个机器人。机器人蜂群在不同目标权重下展现出不同的行为。结果还证明了在低精度模拟器中进行多目标神经网络控制器的进化后，可以将控制器转移到高精度模拟环境中，并且控制器可以扩展到包含更多机器人的环境无需进行进一步 retrained。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Models-are-Competitive-Near-Cold-start-Recommenders-for-Language-and-Item-based-Preferences"><a href="#Large-Language-Models-are-Competitive-Near-Cold-start-Recommenders-for-Language-and-Item-based-Preferences" class="headerlink" title="Large Language Models are Competitive Near Cold-start Recommenders for Language- and Item-based Preferences"></a>Large Language Models are Competitive Near Cold-start Recommenders for Language- and Item-based Preferences</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14225">http://arxiv.org/abs/2307.14225</a></li>
<li>repo_url: None</li>
<li>paper_authors: Scott Sanner, Krisztian Balog, Filip Radlinski, Ben Wedin, Lucas Dixon</li>
<li>for: 研究使用大语言模型（LLM）来提供建议，并比较其与现有的项目相关的协同维度（CF）方法。</li>
<li>methods: 采集了基于项目和语言的偏好的用户评分数据，并使用这些数据来训练LLM。</li>
<li>results: LLM可以在冷启动情况下提供竞争力强的建议，尤其是在没有超参数（zero-shot）或只有几个标签（few-shot）的情况下。这些结果表明语言基本偏好表示更加可解释和可读取。<details>
<summary>Abstract</summary>
Traditional recommender systems leverage users' item preference history to recommend novel content that users may like. However, modern dialog interfaces that allow users to express language-based preferences offer a fundamentally different modality for preference input. Inspired by recent successes of prompting paradigms for large language models (LLMs), we study their use for making recommendations from both item-based and language-based preferences in comparison to state-of-the-art item-based collaborative filtering (CF) methods. To support this investigation, we collect a new dataset consisting of both item-based and language-based preferences elicited from users along with their ratings on a variety of (biased) recommended items and (unbiased) random items. Among numerous experimental results, we find that LLMs provide competitive recommendation performance for pure language-based preferences (no item preferences) in the near cold-start case in comparison to item-based CF methods, despite having no supervised training for this specific task (zero-shot) or only a few labels (few-shot). This is particularly promising as language-based preference representations are more explainable and scrutable than item-based or vector-based representations.
</details>
<details>
<summary>摘要</summary>
传统推荐系统利用用户ITEM喜好历史来推荐新内容，但现代对话界面允许用户通过语言基于喜好表达新的可能性。 inspirited by recent successes of prompting paradigms for large language models (LLMs), we investigate their use for making recommendations from both item-based and language-based preferences in comparison to state-of-the-art item-based collaborative filtering (CF) methods. To support this investigation, we collect a new dataset consisting of both item-based and language-based preferences elicited from users along with their ratings on a variety of (biased) recommended items and (unbiased) random items. Among numerous experimental results, we find that LLMs provide competitive recommendation performance for pure language-based preferences (no item preferences) in the near cold-start case in comparison to item-based CF methods, despite having no supervised training for this specific task (zero-shot) or only a few labels (few-shot). This is particularly promising as language-based preference representations are more explainable and scrutable than item-based or vector-based representations.
</details></li>
</ul>
<hr>
<h2 id="Online-Modeling-and-Monitoring-of-Dependent-Processes-under-Resource-Constraints"><a href="#Online-Modeling-and-Monitoring-of-Dependent-Processes-under-Resource-Constraints" class="headerlink" title="Online Modeling and Monitoring of Dependent Processes under Resource Constraints"></a>Online Modeling and Monitoring of Dependent Processes under Resource Constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14208">http://arxiv.org/abs/2307.14208</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tanapol Kosolwattana, Huazheng Wang, Ying Lin</li>
<li>for: 监测受限资源的依赖过程集体，检测异常事件是非常重要的。</li>
<li>methods: 提出了一种基于在线协同学习的资源分配策略，以适应高风险过程的探索和依赖动力学的探索。</li>
<li>results: 通过理论分析和实验验证，效果良好。<details>
<summary>Abstract</summary>
Monitoring a population of dependent processes under limited resources is critical for abnormal events detection. A novel online collaborative learning method is proposed to adaptively allocate the resources for exploitation of high-risk processes and exploration of dependent dynamics. Efficiency of the proposed method is proved through theoretical analysis and experiments.
</details>
<details>
<summary>摘要</summary>
监测具有限制资源的依赖过程群体是检测异常事件的关键。一种基于在线合作学习的新方法是提议的，以适应分配资源以便利用高风险过程的探索和依赖动态的探索。我们通过理论分析和实验证明了提议的方法的效率。
</details></li>
</ul>
<hr>
<h2 id="Application-of-Random-Forest-and-Support-Vector-Machine-for-Investigation-of-Pressure-Filtration-Performance-a-Zinc-Plant-Filter-Cake-Modeling"><a href="#Application-of-Random-Forest-and-Support-Vector-Machine-for-Investigation-of-Pressure-Filtration-Performance-a-Zinc-Plant-Filter-Cake-Modeling" class="headerlink" title="Application of Random Forest and Support Vector Machine for Investigation of Pressure Filtration Performance, a Zinc Plant Filter Cake Modeling"></a>Application of Random Forest and Support Vector Machine for Investigation of Pressure Filtration Performance, a Zinc Plant Filter Cake Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14199">http://arxiv.org/abs/2307.14199</a></li>
<li>repo_url: None</li>
<li>paper_authors: Masoume Kazemi, Davood Moradkhani, Alireza Abbas Alipour</li>
<li>for: 这个研究旨在预测压缩滤过程中的萃取物湿度。</li>
<li>methods: 研究使用Random Forest（RF）和Support Vector Machine（SVM）模型来模型压缩滤过程。</li>
<li>results: 研究发现，Random Forest Regression（RFR）模型在预测萃取物湿度方面比Support Vector Regression（SVR）模型表现更好，RFR模型的准确预测率较高。<details>
<summary>Abstract</summary>
The hydrometallurgical method of zinc production involves leaching zinc from ore and then separating the solid residue from the liquid solution by pressure filtration. This separation process is very important since the solid residue contains some moisture that can reduce the amount of zinc recovered. This study modeled the pressure filtration process through Random Forest (RF) and Support Vector Machine (SVM). The models take continuous variables (extracted features) from the lab samples as inputs. Thus, regression models namely Random Forest Regression (RFR) and Support Vector Regression (SVR) were chosen. A total dataset was obtained during the pressure filtration process in two conditions: 1) Polypropylene (S1) and 2) Polyester fabrics (S2). To predict the cake moisture, solids concentration (0.2 and 0.38), temperature (35 and 65 centigrade), pH (2, 3.5, and 5), pressure, cake thickness (14, 20, 26, and 34 mm), air-blow time (2, 10 and 15 min) and filtration time were applied as input variables. The models' predictive accuracy was evaluated by the coefficient of determination (R2) parameter. The results revealed that the RFR model is superior to the SVR model for cake moisture prediction.
</details>
<details>
<summary>摘要</summary>
《氧化锌生产技术中的压力分离过程是非常重要的，因为固体剩下物含有一定的湿度，这可能会减少锌的回收量。本研究使用Random Forest（RF）和Support Vector Machine（SVM）模型来模拟压力分离过程。这些模型接受实验室样本中的连续变量（提取特征）作为输入。因此，回归模型Random Forest Regression（RFR）和Support Vector Regression（SVR）被选择。一个总体数据集在压力分离过程中得到了，包括了两种条件：1）聚乙烯（S1）和2）聚醚纤维（S2）。为预测固体湿度，输入变量包括了粉煤浓度（0.2和0.38）、温度（35和65℃）、pH（2、3.5和5）、压力、压力分离厚度（14、20、26和34mm）、气流时间（2、10和15分）和过滤时间。模型的预测精度被评估通过R2参数。结果显示，RFR模型在预测固体湿度方面比SVR模型更高。》Note: Please note that the translation is in Simplified Chinese, and the word order and sentence structure may be different from the original text.
</details></li>
</ul>
<hr>
<h2 id="Efficient-Learning-of-Discrete-Continuous-Computation-Graphs"><a href="#Efficient-Learning-of-Discrete-Continuous-Computation-Graphs" class="headerlink" title="Efficient Learning of Discrete-Continuous Computation Graphs"></a>Efficient Learning of Discrete-Continuous Computation Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14193">http://arxiv.org/abs/2307.14193</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nec-research/dccg">https://github.com/nec-research/dccg</a></li>
<li>paper_authors: David Friede, Mathias Niepert</li>
<li>for: 该论文旨在探讨混合抽象和连续模型在超级vised和强化学习中的应用。</li>
<li>methods: 该论文使用了泛化的推理抽象技术，并提出了两种新的策略来解决训练过程中的问题。</li>
<li>results: 实验结果显示，使用混合抽象和连续模型可以训练更复杂的模型，并且这些模型在一些基准数据集上的泛化性比其连续counterpart更好。<details>
<summary>Abstract</summary>
Numerous models for supervised and reinforcement learning benefit from combinations of discrete and continuous model components. End-to-end learnable discrete-continuous models are compositional, tend to generalize better, and are more interpretable. A popular approach to building discrete-continuous computation graphs is that of integrating discrete probability distributions into neural networks using stochastic softmax tricks. Prior work has mainly focused on computation graphs with a single discrete component on each of the graph's execution paths. We analyze the behavior of more complex stochastic computations graphs with multiple sequential discrete components. We show that it is challenging to optimize the parameters of these models, mainly due to small gradients and local minima. We then propose two new strategies to overcome these challenges. First, we show that increasing the scale parameter of the Gumbel noise perturbations during training improves the learning behavior. Second, we propose dropout residual connections specifically tailored to stochastic, discrete-continuous computation graphs. With an extensive set of experiments, we show that we can train complex discrete-continuous models which one cannot train with standard stochastic softmax tricks. We also show that complex discrete-stochastic models generalize better than their continuous counterparts on several benchmark datasets.
</details>
<details>
<summary>摘要</summary>
多种超visited和强化学习模型受益于混合 discrete和连续模型组件。结构化的end-to-end学习可以使模型更加易于理解和掌控。常见的方法是通过将抽象概率分布 integrate到神经网络中使用随机softmax技巧。先前的工作主要集中在单个执行路径上的 computation graphs上。我们分析了多个级别的随机计算图，并证明这些模型的参数优化具有挑战性，主要是因为小Gradient和地方最小值。我们 then propose two new strategies to overcome these challenges. First, we show that increasing the scale parameter of the Gumbel noise perturbations during training improves the learning behavior. Second, we propose dropout residual connections specifically tailored to stochastic, discrete-continuous computation graphs. With an extensive set of experiments, we show that we can train complex discrete-continuous models that cannot be trained with standard stochastic softmax tricks. We also show that complex discrete-stochastic models generalize better than their continuous counterparts on several benchmark datasets.
</details></li>
</ul>
<hr>
<h2 id="A-comparison-of-machine-learning-surrogate-models-of-street-scale-flooding-in-Norfolk-Virginia"><a href="#A-comparison-of-machine-learning-surrogate-models-of-street-scale-flooding-in-Norfolk-Virginia" class="headerlink" title="A comparison of machine learning surrogate models of street-scale flooding in Norfolk, Virginia"></a>A comparison of machine learning surrogate models of street-scale flooding in Norfolk, Virginia</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14185">http://arxiv.org/abs/2307.14185</a></li>
<li>repo_url: None</li>
<li>paper_authors: Diana McSpadden, Steven Goldenberg, Binata Roy, Malachi Schram, Jonathan L. Goodall, Heather Richter</li>
<li>for: 这个研究是为了解决低洼海岸城市（如诺福克，维iginia）的街道洪水问题，这些问题会影响交通和排水系统，并可能导致财产损害。</li>
<li>methods: 这个研究使用了一种前一版的代理模型（基于随机森林算法），以及两种深度学习模型：Long Short-Term Memory（LSTM）和Gated Recurrent Unit（GRU）。</li>
<li>results: 研究发现，使用LSTM和GRU深度学习模型可以提高预测uncertainty的交通和排水系统的性能，并且这些模型可以有效地 интеGRATE多种、多模态的特征。<details>
<summary>Abstract</summary>
Low-lying coastal cities, exemplified by Norfolk, Virginia, face the challenge of street flooding caused by rainfall and tides, which strain transportation and sewer systems and can lead to property damage. While high-fidelity, physics-based simulations provide accurate predictions of urban pluvial flooding, their computational complexity renders them unsuitable for real-time applications. Using data from Norfolk rainfall events between 2016 and 2018, this study compares the performance of a previous surrogate model based on a random forest algorithm with two deep learning models: Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU). This investigation underscores the importance of using a model architecture that supports the communication of prediction uncertainty and the effective integration of relevant, multi-modal features.
</details>
<details>
<summary>摘要</summary>
低海拔沿海城市，如尼科尔（Norfolk），面临洪水泛滥的挑战，这会压力交通和废水系统，并可能导致财产损害。虽然高精度的物理学基模型可以准确预测城市洪水，但计算复杂性使其不适用于实时应用。通过使用2016-2018年尼科尔降水事件的数据，本研究比较了之前的随机森林算法基于模型和两种深度学习模型：长期快速储存（LSTM）和闭合循环单元（GRU）的表现。这一研究强调使用一种支持预测不确定性的模型架构，并有效地结合相关的多模式特征。
</details></li>
</ul>
<hr>
<h2 id="Learning-Disentangled-Discrete-Representations"><a href="#Learning-Disentangled-Discrete-Representations" class="headerlink" title="Learning Disentangled Discrete Representations"></a>Learning Disentangled Discrete Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14151">http://arxiv.org/abs/2307.14151</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/david-friede/lddr">https://github.com/david-friede/lddr</a></li>
<li>paper_authors: David Friede, Christian Reimers, Heiner Stuckenschmidt, Mathias Niepert</li>
<li>for: 本研究旨在探讨抽象空间中的离散表示的优点，以及它们如何促进分离表示的学习。</li>
<li>methods: 研究者采用了一种特制的 categorical variational autoencoder（CVAE），以取代标准的 Gaussian variational autoencoder（VAE），以便更好地学习分离表示。</li>
<li>results: 研究者通过 analytical 和 empirical 的方法，证明了离散 VAE 在学习分离表示方面的优势，并提出了首个无监督的模型选择策略，以便寻找更好的分离表示模型。<details>
<summary>Abstract</summary>
Recent successes in image generation, model-based reinforcement learning, and text-to-image generation have demonstrated the empirical advantages of discrete latent representations, although the reasons behind their benefits remain unclear. We explore the relationship between discrete latent spaces and disentangled representations by replacing the standard Gaussian variational autoencoder (VAE) with a tailored categorical variational autoencoder. We show that the underlying grid structure of categorical distributions mitigates the problem of rotational invariance associated with multivariate Gaussian distributions, acting as an efficient inductive prior for disentangled representations. We provide both analytical and empirical findings that demonstrate the advantages of discrete VAEs for learning disentangled representations. Furthermore, we introduce the first unsupervised model selection strategy that favors disentangled representations.
</details>
<details>
<summary>摘要</summary>
最近的图像生成、模型基于返回学习和文本到图像生成的成功表明了不连续含义空间的实际优势，即使其中的原因还未得到清晰解释。我们调查了不连续含义空间和分离表示之间的关系，并将标准的 Gaussian 变量自动机（VAE）替换为适应 categorical 变量自动机。我们发现， categorical 分布下的网格结构可以有效地解决多变量 Gaussian 分布中的旋转不变性问题，并作为较为有效的 inductive prior  для分离表示。我们提供了both analytical和empirical的发现，证明不连续 VAE 在学习分离表示方面的优势。此外，我们还介绍了首个无监督模型选择策略，该策略会偏好分离表示。
</details></li>
</ul>
<hr>
<h2 id="Toward-Design-of-Synthetic-Active-Inference-Agents-by-Mere-Mortals"><a href="#Toward-Design-of-Synthetic-Active-Inference-Agents-by-Mere-Mortals" class="headerlink" title="Toward Design of Synthetic Active Inference Agents by Mere Mortals"></a>Toward Design of Synthetic Active Inference Agents by Mere Mortals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14145">http://arxiv.org/abs/2307.14145</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bert de Vries</li>
<li>for: 这个论文旨在解决智能设备边缘处理中active inference代理的实现问题，以便快速普及活跃推理技术。</li>
<li>methods: 论文提出了一个工具箱，用于支持非专家工程师开发工作的活跃推理代理。该工具箱具有加速policy探索的能力，以便在限制性的边缘设备上实现效果。</li>
<li>results: 论文预示了一个在进程中使用该工具箱的示例应用，显示了在边缘设备上实现活跃推理代理的可能性。<details>
<summary>Abstract</summary>
The theoretical properties of active inference agents are impressive, but how do we realize effective agents in working hardware and software on edge devices? This is an interesting problem because the computational load for policy exploration explodes exponentially, while the computational resources are very limited for edge devices. In this paper, we discuss the necessary features for a software toolbox that supports a competent non-expert engineer to develop working active inference agents. We introduce a toolbox-in-progress that aims to accelerate the democratization of active inference agents in a similar way as TensorFlow propelled applications of deep learning technology.
</details>
<details>
<summary>摘要</summary>
理论上，活动推理代理的特性很吸引人，但实际如何在工作硬件和软件上实现有效的代理呢？这是一个有趣的问题，因为策略探索的计算荷载会指数增长，而边缘设备的计算资源却很有限。在这篇文章中，我们讨论了实现非专业工程师开发工作的活动推理代理所需的必要特性。我们介绍了一个进度中的工具箱，旨在通过减少代理开发的计算负担，使活动推理代理在边缘设备上更加普及。
</details></li>
</ul>
<hr>
<h2 id="Piecewise-Stationary-Combinatorial-Semi-Bandit-with-Causally-Related-Rewards"><a href="#Piecewise-Stationary-Combinatorial-Semi-Bandit-with-Causally-Related-Rewards" class="headerlink" title="Piecewise-Stationary Combinatorial Semi-Bandit with Causally Related Rewards"></a>Piecewise-Stationary Combinatorial Semi-Bandit with Causally Related Rewards</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14138">http://arxiv.org/abs/2307.14138</a></li>
<li>repo_url: None</li>
<li>paper_authors: Behzad Nourani-Koliji, Steven Bilaj, Amir Rezaei Balef, Setareh Maghsudi</li>
<li>for:  solving the piecewise stationary combinatorial semi-bandit problem in a nonstationary environment with causally related rewards</li>
<li>methods: using the Upper Confidence Bound (UCB) algorithm with an adaptive approach that includes a change-point detector based on the Generalized Likelihood Ratio (GLR) test and a mechanism to trace the variations of the underlying graph structure</li>
<li>results: establishing a regret upper bound that reflects the effects of the number of structural- and distribution changes on the performance, and demonstrating superior performance in numerical experiments compared to state-of-the-art benchmarks.Here’s the Chinese translation of the three points:</li>
<li>for: 解决 piecewise stationary  combinatorial semi-bandit 问题在不站台环境中，其中 reward 具有 causal 关系</li>
<li>methods: 使用 Upper Confidence Bound (UCB) 算法，并采用适应的方法，包括基于 Generalized Likelihood Ratio (GLR) 测试的 change-point 探测器和跟踪Underlying graph structure的机制</li>
<li>results: 确定了不同 structural- 和 distribution 变化的影响，并在实验中证明了与 state-of-the-art benchmarks 相比，提出的方案具有更高的实用性。<details>
<summary>Abstract</summary>
We study the piecewise stationary combinatorial semi-bandit problem with causally related rewards. In our nonstationary environment, variations in the base arms' distributions, causal relationships between rewards, or both, change the reward generation process. In such an environment, an optimal decision-maker must follow both sources of change and adapt accordingly. The problem becomes aggravated in the combinatorial semi-bandit setting, where the decision-maker only observes the outcome of the selected bundle of arms. The core of our proposed policy is the Upper Confidence Bound (UCB) algorithm. We assume the agent relies on an adaptive approach to overcome the challenge. More specifically, it employs a change-point detector based on the Generalized Likelihood Ratio (GLR) test. Besides, we introduce the notion of group restart as a new alternative restarting strategy in the decision making process in structured environments. Finally, our algorithm integrates a mechanism to trace the variations of the underlying graph structure, which captures the causal relationships between the rewards in the bandit setting. Theoretically, we establish a regret upper bound that reflects the effects of the number of structural- and distribution changes on the performance. The outcome of our numerical experiments in real-world scenarios exhibits applicability and superior performance of our proposal compared to the state-of-the-art benchmarks.
</details>
<details>
<summary>摘要</summary>
我们研究 Piecewise 站ARY  combinatorial 半带�û问题，其中 reward 的生成过程受到基 arms 的分布变化、 causal 关系 between rewards 以及 Both 的变化影响。在这种非站ARY 环境中，一个优化的决策者需要同时考虑这些变化并适应应对。在 combinatorial 半带 Setting 中，决策者只能观察选择的 bundle of arms 的结果。我们的提议的策略是使用 Upper Confidence Bound (UCB) 算法。我们假设agent 采用adaptive approach来解决这个挑战。具体来说，它使用基于 Generalized Likelihood Ratio (GLR) 测试的变化点检测器。此外，我们引入了 group restart 作为一种新的重启策略，用于在结构化环境中决策过程中。最后，我们的算法包括跟踪Underlying graph structure的变化，这些变化捕捉了 reward 在半带 Setting 中的 causal 关系。从理论角度来看，我们确立了 regret Upper bound，这个 Upper bound 反映了变量的数量和分布变化对性能的影响。我们的numerical experiments 在实际场景中展示了我们的提议的应用和优于现有benchmarks。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-on-Generative-Modeling-with-Limited-Data-Few-Shots-and-Zero-Shot"><a href="#A-Survey-on-Generative-Modeling-with-Limited-Data-Few-Shots-and-Zero-Shot" class="headerlink" title="A Survey on Generative Modeling with Limited Data, Few Shots, and Zero Shot"></a>A Survey on Generative Modeling with Limited Data, Few Shots, and Zero Shot</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14397">http://arxiv.org/abs/2307.14397</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sutd-visual-computing-group/awesome-generative-modeling-under-data-constraints">https://github.com/sutd-visual-computing-group/awesome-generative-modeling-under-data-constraints</a></li>
<li>paper_authors: Milad Abdollahzadeh, Touba Malekzadeh, Christopher T. H. Teo, Keshigeyan Chandrasegaran, Guimeng Liu, Ngai-Man Cheung</li>
<li>for: 本研究旨在探讨生成模型在数据约束下学习，即生成模型学习数据分布的新数据。</li>
<li>methods: 本研究使用了限制数据量、几个shot和零shot等数据约束来学习生成模型。</li>
<li>results: 研究发现了生成模型在数据约束下的学习 task 和方法之间存在交互关系，并提出了未来研究的潜在方向。Here’s a more detailed explanation of each point:</li>
<li>for: The paper is focused on generative modeling under data constraint, specifically exploring the task of learning to generate new data that is statistically similar to the training data distribution, but with limited data availability.</li>
<li>methods: The paper discusses the use of limited data, few shots, and zero shots as data constraints for generative modeling, and proposes two taxonomies for GM-DC tasks and approaches.</li>
<li>results: The study highlights research gaps, research trends, and potential avenues for future exploration in the field of GM-DC, and provides a comprehensive overview of the current state of the field.<details>
<summary>Abstract</summary>
In machine learning, generative modeling aims to learn to generate new data statistically similar to the training data distribution. In this paper, we survey learning generative models under limited data, few shots and zero shot, referred to as Generative Modeling under Data Constraint (GM-DC). This is an important topic when data acquisition is challenging, e.g. healthcare applications. We discuss background, challenges, and propose two taxonomies: one on GM-DC tasks and another on GM-DC approaches. Importantly, we study interactions between different GM-DC tasks and approaches. Furthermore, we highlight research gaps, research trends, and potential avenues for future exploration. Project website: https://gmdc-survey.github.io.
</details>
<details>
<summary>摘要</summary>
在机器学习中，生成模型目标是学习生成新数据，与训练数据分布 statistically similar。在这篇论文中，我们对受限数据的生成模型学习进行报告，包括几 shot、零 shot 等。这是数据收集困难的场景，如医疗应用。我们介绍背景、挑战和两种分类：一种是生成模型下数据约束任务（GM-DC），另一种是生成模型下数据约束方法（GM-DC）。重要的是，我们研究不同的 GM-DC 任务和方法之间的交互。此外，我们还提出了未来探索的研究漏洞、趋势和潜在的发展方向。项目网站：https://gmdc-survey.github.io。Note: The translation is done using Google Translate and may not be perfect.
</details></li>
</ul>
<hr>
<h2 id="Developing-and-Evaluating-Tiny-to-Medium-Sized-Turkish-BERT-Models"><a href="#Developing-and-Evaluating-Tiny-to-Medium-Sized-Turkish-BERT-Models" class="headerlink" title="Developing and Evaluating Tiny to Medium-Sized Turkish BERT Models"></a>Developing and Evaluating Tiny to Medium-Sized Turkish BERT Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14134">http://arxiv.org/abs/2307.14134</a></li>
<li>repo_url: None</li>
<li>paper_authors: Himmet Toprak Kesgin, Muzaffer Kaan Yuce, Mehmet Fatih Amasyali</li>
<li>for: This paper aims to bridge the research gap in less-resourced languages by introducing and evaluating tiny, mini, small, and medium-sized uncased Turkish BERT models.</li>
<li>methods: The authors trained these models on a diverse dataset encompassing over 75GB of text from multiple sources and tested them on several tasks, including mask prediction, sentiment analysis, news classification, and zero-shot classification.</li>
<li>results: Despite their smaller size, the models exhibited robust performance, including zero-shot task, while ensuring computational efficiency and faster execution times.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文目标是bridging the research gap in less-resourced languages, 通过引入和评估 Turkish BERT 模型的不同大小。</li>
<li>methods: 作者使用了多种数据集，包括多个来源的文本，用于训练这些模型，并在多个任务上进行测试，包括偏好预测、情感分析、新闻分类和零 shot 分类。</li>
<li>results:  despite their smaller size, the models exhibited robust performance, including zero-shot task, while ensuring computational efficiency and faster execution times.<details>
<summary>Abstract</summary>
This study introduces and evaluates tiny, mini, small, and medium-sized uncased Turkish BERT models, aiming to bridge the research gap in less-resourced languages. We trained these models on a diverse dataset encompassing over 75GB of text from multiple sources and tested them on several tasks, including mask prediction, sentiment analysis, news classification, and, zero-shot classification. Despite their smaller size, our models exhibited robust performance, including zero-shot task, while ensuring computational efficiency and faster execution times. Our findings provide valuable insights into the development and application of smaller language models, especially in the context of the Turkish language.
</details>
<details>
<summary>摘要</summary>
Note:* "tiny" is 微小 (wēixiǎo) in Simplified Chinese* "mini" is 小型 (xiǎoxīng) in Simplified Chinese* "small" is 小 (xiǎo) in Simplified Chinese* "medium-sized" is 中等 (zhōngděng) in Simplified Chinese* "uncased" is 无框 (wúkē) in Simplified Chinese* "mask prediction" is 面 predicate (miàn zhèng) in Simplified Chinese* "sentiment analysis" is 情感分析 (qínggǎn fēnxiǎn) in Simplified Chinese* "news classification" is 新闻分类 (xīnwén fēnclass) in Simplified Chinese* "zero-shot classification" is 零枪分类 (zhèng qiāng fēnclass) in Simplified Chinese
</details></li>
</ul>
<hr>
<h2 id="GraphRNN-Revisited-An-Ablation-Study-and-Extensions-for-Directed-Acyclic-Graphs"><a href="#GraphRNN-Revisited-An-Ablation-Study-and-Extensions-for-Directed-Acyclic-Graphs" class="headerlink" title="GraphRNN Revisited: An Ablation Study and Extensions for Directed Acyclic Graphs"></a>GraphRNN Revisited: An Ablation Study and Extensions for Directed Acyclic Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14109">http://arxiv.org/abs/2307.14109</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taniya Das, Mark Koch, Maya Ravichandran, Nikhil Khatri</li>
<li>for: 学习图格生成模型</li>
<li>methods: 使用深度学习架构GraphRNN，并对基线模型进行评估和ablation study</li>
<li>results: 发现BFS traversal对模型性能有贡献，并将GraphRNN扩展到生成指定图的方法得到了显著改进。Here’s the breakdown of each point in English:</li>
<li>for: The paper is written for learning generative models for graphs using deep learning-based architectures.</li>
<li>methods: The paper uses a reproduced implementation of the GraphRNN architecture and evaluates it against baseline models using new metrics. The authors also perform an ablation study to analyze the contribution of the BFS traversal to model performance.</li>
<li>results: The authors find that the BFS traversal contributes significantly to model performance and extend GraphRNN to generate directed acyclic graphs by replacing the BFS traversal with a topological sort. They demonstrate significant improvement over a directed-multiclass variant of GraphRNN on a real-world dataset.<details>
<summary>Abstract</summary>
GraphRNN is a deep learning-based architecture proposed by You et al. for learning generative models for graphs. We replicate the results of You et al. using a reproduced implementation of the GraphRNN architecture and evaluate this against baseline models using new metrics. Through an ablation study, we find that the BFS traversal suggested by You et al. to collapse representations of isomorphic graphs contributes significantly to model performance. Additionally, we extend GraphRNN to generate directed acyclic graphs by replacing the BFS traversal with a topological sort. We demonstrate that this method improves significantly over a directed-multiclass variant of GraphRNN on a real-world dataset.
</details>
<details>
<summary>摘要</summary>
GRAPHRNN是一种深度学习基于架构，由尤等人提出用于学习图生成模型。我们对GRAPHRNN архитектура进行了重现，并使用新的基准模型进行评估。通过一项减少研究，我们发现了You等人提出的深度首次旋转（BFS） traverse 可以帮助模型性能。此外，我们还将GRAPHRNN扩展为生成直接无环图，通过将BFS traverse 替换为拓扑排序。我们示出这种方法可以在真实世界数据上提高表现。
</details></li>
</ul>
<hr>
<h2 id="Actions-Speak-What-You-Want-Provably-Sample-Efficient-Reinforcement-Learning-of-the-Quantal-Stackelberg-Equilibrium-from-Strategic-Feedbacks"><a href="#Actions-Speak-What-You-Want-Provably-Sample-Efficient-Reinforcement-Learning-of-the-Quantal-Stackelberg-Equilibrium-from-Strategic-Feedbacks" class="headerlink" title="Actions Speak What You Want: Provably Sample-Efficient Reinforcement Learning of the Quantal Stackelberg Equilibrium from Strategic Feedbacks"></a>Actions Speak What You Want: Provably Sample-Efficient Reinforcement Learning of the Quantal Stackelberg Equilibrium from Strategic Feedbacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14085">http://arxiv.org/abs/2307.14085</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siyu Chen, Mengdi Wang, Zhuoran Yang</li>
<li>for: 学习一种量化Stackelberg平衡（QSE）在一个 episodic Markov 游戏中，其中有一个领导者和一个追随者结构。</li>
<li>methods: 使用 reinforcement learning（RL）和 maximum likelihood estimation（MLE）来学习领导者的决策问题，并使用模型之间的不确定性来实现在线和离线设置中的最佳性。</li>
<li>results: 提出了一些样本效率的算法，可以在函数approximation的上下文中解决领导者的决策问题，并且可以实现sublinear regret上限。 besides, 特别注重linear和偏函数设置下的计算效率。<details>
<summary>Abstract</summary>
We study reinforcement learning (RL) for learning a Quantal Stackelberg Equilibrium (QSE) in an episodic Markov game with a leader-follower structure. In specific, at the outset of the game, the leader announces her policy to the follower and commits to it. The follower observes the leader's policy and, in turn, adopts a quantal response policy by solving an entropy-regularized policy optimization problem induced by leader's policy. The goal of the leader is to find her optimal policy, which yields the optimal expected total return, by interacting with the follower and learning from data. A key challenge of this problem is that the leader cannot observe the follower's reward, and needs to infer the follower's quantal response model from his actions against leader's policies. We propose sample-efficient algorithms for both the online and offline settings, in the context of function approximation. Our algorithms are based on (i) learning the quantal response model via maximum likelihood estimation and (ii) model-free or model-based RL for solving the leader's decision making problem, and we show that they achieve sublinear regret upper bounds. Moreover, we quantify the uncertainty of these estimators and leverage the uncertainty to implement optimistic and pessimistic algorithms for online and offline settings. Besides, when specialized to the linear and myopic setting, our algorithms are also computationally efficient. Our theoretical analysis features a novel performance-difference lemma which incorporates the error of quantal response model, which might be of independent interest.
</details>
<details>
<summary>摘要</summary>
我们研究强化学习（RL）来学习一个量化StackelbergEquilibrium（QSE）在一个集合Markov游戏中，具有领导者-跟者结构。具体来说，在游戏开始时，领导者宣布她的策略给跟者，并将其约束。跟者根据领导者的策略采取一个量化回应策略， solving一个带有Entropy规范的策略优化问题。领导者的目标是找到她的优化策略，以便在与跟者交互和学习数据的过程中获得最佳预期总返回。一个关键问题是领导者无法观察跟者的奖励，她需要从跟者的行为中推断出跟者的量化回应模型。我们提出了一些样本效率的算法，包括在线和离线设置下的最大 likelihood估计和模型自由或模型基于RL，并证明它们可以实现sublinear regret上界。此外，我们还评估了这些估计器的不确定性，并利用这些不确定性来实现在线和离线设置下的optimistic和pessimistic算法。此外，当特化到线性和偏惰设置时，我们的算法也是计算高效的。我们的理论分析包括一个新的表现差lemm，该lemm incorporates the error of quantal response model，这可能是独立的兴趣。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-simulate-partially-known-spatio-temporal-dynamics-with-trainable-difference-operators"><a href="#Learning-to-simulate-partially-known-spatio-temporal-dynamics-with-trainable-difference-operators" class="headerlink" title="Learning to simulate partially known spatio-temporal dynamics with trainable difference operators"></a>Learning to simulate partially known spatio-temporal dynamics with trainable difference operators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14395">http://arxiv.org/abs/2307.14395</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiang Huang, Zhuoyuan Li, Hongsheng Liu, Zidong Wang, Hongye Zhou, Bin Dong, Bei Hua</li>
<li>for: 用神经网络模拟空间-时间动态的研究在最近几年得到了很多关注，但大多数现有方法采用纯数据驱动的黑盒模型，具有限制精度和可读性。</li>
<li>methods: 我们提出一种新的混合建模架构，称为PDE-Net++，它将可训练的差分算子与黑盒模型相结合，并包含部分先验知识。我们还提出了两种不同的差分层：可训练差分层（TFDL）和可训练动态差分层（TDDL）。</li>
<li>results: 数值实验表明，PDE-Net++的预测精度高于黑盒模型，并且在推理范围内具有更好的推理性能。<details>
<summary>Abstract</summary>
Recently, using neural networks to simulate spatio-temporal dynamics has received a lot of attention. However, most existing methods adopt pure data-driven black-box models, which have limited accuracy and interpretability. By combining trainable difference operators with black-box models, we propose a new hybrid architecture explicitly embedded with partial prior knowledge of the underlying PDEs named PDE-Net++. Furthermore, we introduce two distinct options called the trainable flipping difference layer (TFDL) and the trainable dynamic difference layer (TDDL) for the difference operators. Numerous numerical experiments have demonstrated that PDE-Net++ has superior prediction accuracy and better extrapolation performance than black-box models.
</details>
<details>
<summary>摘要</summary>
最近，使用神经网络模拟空间时间动力学受到了广泛关注。然而，现有大多数方法采用纯数据驱动黑盒模型，它们的准确率和可解释性受限。我们提出了一种新的混合架构，名为PDE-Net++，它通过与黑盒模型结合可编程的差异运算器来承载部分先验知识。此外，我们还提出了两种不同的选项，即可编程折衣层（TFDL）和可编程动态差异层（TDDL），用于差异运算器。数值实验证明，PDE-Net++在预测精度和推迟性方面都高于黑盒模型。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Domain-Discrepancy-Adjustment-for-Active-Multi-Domain-Adaptation"><a href="#Dynamic-Domain-Discrepancy-Adjustment-for-Active-Multi-Domain-Adaptation" class="headerlink" title="Dynamic Domain Discrepancy Adjustment for Active Multi-Domain Adaptation"></a>Dynamic Domain Discrepancy Adjustment for Active Multi-Domain Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14068">http://arxiv.org/abs/2307.14068</a></li>
<li>repo_url: None</li>
<li>paper_authors: Long Liu, Bo Zhou, Zhipeng Zhao, Zening Liu</li>
<li>for: 本研究旨在提出一种新的多源不监督领域适应（MUDA）方法，以便从相关的源领域传递知识到未标注的目标领域。</li>
<li>methods: 我们提出了一种名为动态领域差异调整 для活动多频道适应（D3AAMDA）的新方法，它在训练过程中根据源领域和目标领域之间的分布差异度设置多源动态调整机制，以有效地利用每个源领域的本地有利特征信息。此外，我们还提出了一种多源活动边界选择策略（MABS），它通过一种引导的动态边界损失来设计高效的选择函数，以提高对目标领域的泛化。</li>
<li>results: 我们对常用的领域适应数据集进行了广泛的测试和比较，并证明了我们的方法的突出优势。<details>
<summary>Abstract</summary>
Multi-source unsupervised domain adaptation (MUDA) aims to transfer knowledge from related source domains to an unlabeled target domain. While recent MUDA methods have shown promising results, most focus on aligning the overall feature distributions across source domains, which can lead to negative effects due to redundant features within each domain. Moreover, there is a significant performance gap between MUDA and supervised methods. To address these challenges, we propose a novel approach called Dynamic Domain Discrepancy Adjustment for Active Multi-Domain Adaptation (D3AAMDA). Firstly, we establish a multi-source dynamic modulation mechanism during the training process based on the degree of distribution differences between source and target domains. This mechanism controls the alignment level of features between each source domain and the target domain, effectively leveraging the local advantageous feature information within the source domains. Additionally, we propose a Multi-source Active Boundary Sample Selection (MABS) strategy, which utilizes a guided dynamic boundary loss to design an efficient query function for selecting important samples. This strategy achieves improved generalization to the target domain with minimal sampling costs. We extensively evaluate our proposed method on commonly used domain adaptation datasets, comparing it against existing UDA and ADA methods. The experimental results unequivocally demonstrate the superiority of our approach.
</details>
<details>
<summary>摘要</summary>
Here is the text in Simplified Chinese:多源无监督领域适应 (MUDA) 目标是将相关源领域知识传递到未标注目标领域。而现有的 MUDA 方法多数强调对所有源领域的特征分布进行对齐，这可能会导致因每个领域中的重复特征而出现负面的影响。此外，与超级vised方法相比，MUDA方法存在显著的性能差距。为解决这些挑战，我们提出了一种新的方法，即动态领域差异调整器 для活动多领域适应 (D3AAMDA)。首先，我们在训练过程中建立了多源动态调整机制，根据源领域和目标领域特征分布之间的差异程度来控制每个源领域和目标领域之间的特征对齐水平。这种机制有效地利用了每个源领域的本地有利特征信息。其次，我们提出了多源活动边界选择策略 (MABS)，该策略通过指导动态边界损失来设计高效的查询函数，选择重要的样本。这种策略可以在保持高度一致性的情况下，实现到目标领域的改进一致性，并且减少采样成本。我们对常用的领域适应 datasets 进行了广泛的测试和比较，与现有的 UDA 和 ADA 方法进行了比较。实验结果明确地表明了我们的方法的优越性。
</details></li>
</ul>
<hr>
<h2 id="Hypergraph-Isomorphism-Computation"><a href="#Hypergraph-Isomorphism-Computation" class="headerlink" title="Hypergraph Isomorphism Computation"></a>Hypergraph Isomorphism Computation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14394">http://arxiv.org/abs/2307.14394</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifan Feng, Jiashu Han, Shihui Ying, Yue Gao</li>
<li>for: 本研究旨在 Addressing the 图HS isomorphism problem, which is a fundamental problem in network analysis, and capturing both low-order and high-order structural information.</li>
<li>methods: 本 paper 提出了一种基于 Weisfiler-Lehman 测试算法的hypergraph isomorphism测试问题解决方案，并基于该算法提出了一个总体的 hypergraph Weisfeiler-Lehman kernel框架。</li>
<li>results: Results 表明，与其他常见的kernel-based方法相比，提出的方法在 hypergraph 分类任务中具有显著的优势，runtime running over 80 times faster when handling complex hypergraph structures。<details>
<summary>Abstract</summary>
The isomorphism problem is a fundamental problem in network analysis, which involves capturing both low-order and high-order structural information. In terms of extracting low-order structural information, graph isomorphism algorithms analyze the structural equivalence to reduce the solver space dimension, which demonstrates its power in many applications, such as protein design, chemical pathways, and community detection. For the more commonly occurring high-order relationships in real-life scenarios, the problem of hypergraph isomorphism, which effectively captures these high-order structural relationships, cannot be straightforwardly addressed using graph isomorphism methods. Besides, the existing hypergraph kernel methods may suffer from high memory consumption or inaccurate sub-structure identification, thus yielding sub-optimal performance. In this paper, to address the abovementioned problems, we first propose the hypergraph Weisfiler-Lehman test algorithm for the hypergraph isomorphism test problem by generalizing the Weisfiler-Lehman test algorithm from graphs to hypergraphs. Secondly, based on the presented algorithm, we propose a general hypergraph Weisfieler-Lehman kernel framework and implement two instances, which are Hypergraph Weisfeiler-Lehamn Subtree Kernel and Hypergraph Weisfeiler-Lehamn Hyperedge Kernel. In order to fulfill our research objectives, a comprehensive set of experiments was meticulously designed, including seven graph classification datasets and 12 hypergraph classification datasets. Results on hypergraph classification datasets show significant improvements compared to other typical kernel-based methods, which demonstrates the effectiveness of the proposed methods. In our evaluation, we found that our proposed methods outperform the second-best method in terms of runtime, running over 80 times faster when handling complex hypergraph structures.
</details>
<details>
<summary>摘要</summary>
“iso关系问题”是网络分析中的基本问题，它涵盖了低阶和高阶结构信息的捕捉。从抽象低阶结构信息的角度来看，网络同构算方法可以降低解析空间维度，实现了许多应用，如蛋白质设计、化学路径和社区探测。然而，实际生活中更常出现高阶关系，这些高阶关系不能直接使用网络同构算方法进行处理。此外，现有的超graphkernel方法可能会导致高内存消耗或不精确的子结构识别，因此表现不佳。在本文中，我们提出了超graphWeisfiler-Lehman测试算法，用于超graph isomorphism测试问题的解决方案。其次，我们基于这个算法提出了一个通用的超graphWeisfeiler-Lehman核心框架，并实现了两个实例：超graphWeisfeiler-Lehman子树核心和超graphWeisfeiler-Lehman超组件核心。为了实现我们的研究目标，我们谨慎设计了一系列实验，包括七个图类别数据集和十二个超graph类别数据集。结果显示，我们的提案方法在超graph类别数据集上表现出色，较其他常用的核心基本方法更好。在我们的评估中，我们发现了我们的提案方法在复杂超graph结构时表现出80倍以上的执行时间优化。
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-Applications-In-Healthcare-The-State-Of-Knowledge-and-Future-Directions"><a href="#Machine-Learning-Applications-In-Healthcare-The-State-Of-Knowledge-and-Future-Directions" class="headerlink" title="Machine Learning Applications In Healthcare: The State Of Knowledge and Future Directions"></a>Machine Learning Applications In Healthcare: The State Of Knowledge and Future Directions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14067">http://arxiv.org/abs/2307.14067</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mrinmoy Roy, Sarwar J. Minar, Porarthi Dhar, A T M Omor Faruq</li>
<li>For: This study aims to gather and present Machine Learning (ML) applications in various areas of healthcare, such as community level work, risk management&#x2F;preventive care, healthcare operation management, remote care, and early detection, to provide quick access to necessary information and reduce the knowledge gap of clinicians about ML applications.* Methods: The study uses a tabular format to provide relevant references with descriptions for each ML application, allowing healthcare professionals to access the information more effectively.* Results: The study aims to motivate healthcare professionals towards adopting more ML-based healthcare systems and to inform people about the applicability of ML in the healthcare industry.<details>
<summary>Abstract</summary>
Detection of easily missed hidden patterns with fast processing power makes machine learning (ML) indispensable to today's healthcare system. Though many ML applications have already been discovered and many are still under investigation, only a few have been adopted by current healthcare systems. As a result, there exists an enormous opportunity in healthcare system for ML but distributed information, scarcity of properly arranged and easily explainable documentation in related sector are major impede which are making ML applications difficult to healthcare professionals. This study aimed to gather ML applications in different areas of healthcare concisely and more effectively so that necessary information can be accessed immediately with relevant references. We divided our study into five major groups: community level work, risk management/ preventive care, healthcare operation management, remote care, and early detection. Dividing these groups into subgroups, we provided relevant references with description in tabular form for quick access. Our objective is to inform people about ML applicability in healthcare industry, reduce the knowledge gap of clinicians about the ML applications and motivate healthcare professionals towards more machine learning based healthcare system.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate "Detection of easily missed hidden patterns with fast processing power makes machine learning (ML) indispensable to today's healthcare system. Though many ML applications have already been discovered and many are still under investigation, only a few have been adopted by current healthcare systems. As a result, there exists an enormous opportunity in healthcare system for ML but distributed information, scarcity of properly arranged and easily explainable documentation in related sector are major impede which are making ML applications difficult to healthcare professionals. This study aimed to gather ML applications in different areas of healthcare concisely and more effectively so that necessary information can be accessed immediately with relevant references. We divided our study into five major groups: community level work, risk management/ preventive care, healthcare operation management, remote care, and early detection. Dividing these groups into subgroups, we provided relevant references with description in tabular form for quick access. Our objective is to inform people about ML applicability in healthcare industry, reduce the knowledge gap of clinicians about the ML applications and motivate healthcare professionals towards more machine learning based healthcare system." into Simplified Chinese.干� TRANSLATE "检测容易过look的隐藏模式，快速处理能力使机器学习（ML）成为今天的医疗系统中不可或缺的一部分。虽然已经发现了许多ML应用，但只有一些被当前的医疗系统采用。因此，医疗系统中存在巨大的机会，但信息散布、相关领域的文献不足和不易描述的问题是主要的阻碍因素。这项研究的目标是将不同领域的ML应用集中并更有效地呈现，以便立即获取相关参考。我们将研究分为五个主要组：社区层次的工作、风险管理/预防护理、医疗运营管理、远程护理和早期检测。将这些组分成子组，并提供相关参考和描述的表格形式，以便快速访问。我们的目标是让人们了解医疗业中ML的可能性，减少医生对ML应用的知识差距，并激励医疗专业人员更加倾向于基于机器学习的医疗系统。
</details></li>
</ul>
<hr>
<h2 id="Pre-Training-with-Diffusion-models-for-Dental-Radiography-segmentation"><a href="#Pre-Training-with-Diffusion-models-for-Dental-Radiography-segmentation" class="headerlink" title="Pre-Training with Diffusion models for Dental Radiography segmentation"></a>Pre-Training with Diffusion models for Dental Radiography segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14066">http://arxiv.org/abs/2307.14066</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jérémy Rousseau, Christian Alaka, Emma Covili, Hippolyte Mayard, Laura Misrachi, Willy Au</li>
<li>for: 针对医疗放射学像 segmentation, 特别是牙科放射学像，存在高度有限制的标注成本，需要专业知识和劳动 INTENSIVE 注解。</li>
<li>methods: 我们提议使用 Denoising Diffusion Probabilistic Models (DDPM) 的快速预训练方法，这种方法已经在生成模型方面显示出卓越的表现。</li>
<li>results: 我们的实验结果表明，使用我们提议的方法可以实现 Label 效率的提高，不需要下游任务中的建筑修改。 我们的方法与当前状态的预训练方法相当。<details>
<summary>Abstract</summary>
Medical radiography segmentation, and specifically dental radiography, is highly limited by the cost of labeling which requires specific expertise and labor-intensive annotations. In this work, we propose a straightforward pre-training method for semantic segmentation leveraging Denoising Diffusion Probabilistic Models (DDPM), which have shown impressive results for generative modeling. Our straightforward approach achieves remarkable performance in terms of label efficiency and does not require architectural modifications between pre-training and downstream tasks. We propose to first pre-train a Unet by exploiting the DDPM training objective, and then fine-tune the resulting model on a segmentation task. Our experimental results on the segmentation of dental radiographs demonstrate that the proposed method is competitive with state-of-the-art pre-training methods.
</details>
<details>
<summary>摘要</summary>
医疗X射线段化和特别是牙科X射线段化受到标注成本的限制，这导致了标注过程的专业知识和劳动 INTENSIVE 的需求。在这项工作中，我们提出了一种简单的预训练方法 дляsemantic segmentation，利用Denosing Diffusion Probabilistic Models（DDPM），这种模型在生成模型方面表现出色。我们的简单的方法可以在预训练和下游任务之间不需要任何建筑修改，同时可以 дости到很好的标签效率。我们首先预训练了Unet模型，然后在DDPM训练目标下进行了微调。我们的实验结果表明，提议的方法与状态的预训练方法竞争。
</details></li>
</ul>
<hr>
<h2 id="Topologically-Regularized-Multiple-Instance-Learning-for-Red-Blood-Cell-Disease-Classification"><a href="#Topologically-Regularized-Multiple-Instance-Learning-for-Red-Blood-Cell-Disease-Classification" class="headerlink" title="Topologically-Regularized Multiple Instance Learning for Red Blood Cell Disease Classification"></a>Topologically-Regularized Multiple Instance Learning for Red Blood Cell Disease Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14025">http://arxiv.org/abs/2307.14025</a></li>
<li>repo_url: None</li>
<li>paper_authors: Salome Kazeminia, Ario Sadafi, Asya Makhro, Anna Bogdanova, Carsten Marr, Bastian Rieck</li>
<li>for: 这个论文的目的是用单个红血球图像自动诊断罕见贫血病。</li>
<li>methods: 这个论文使用的方法是基于多个精度扩展的扩展特征，以保持数据集的特性特征。</li>
<li>results: 该方法在71名患有罕见贫血病的患者和521张红血球图像上达到了超过3%的性能提升。<details>
<summary>Abstract</summary>
Diagnosing rare anemia disorders using microscopic images is challenging for skilled specialists and machine-learning methods alike. Due to thousands of disease-relevant cells in a single blood sample, this constitutes a complex multiple-instance learning (MIL) problem. While the spatial neighborhood of red blood cells is not meaningful per se, the topology, i.e., the geometry of blood samples as a whole, contains informative features to remedy typical MIL issues, such as vanishing gradients and overfitting when training on limited data. We thus develop a topology-based approach that extracts multi-scale topological features from bags of single red blood cell images. The topological features are used to regularize the model, enforcing the preservation of characteristic topological properties of the data. Applied to a dataset of 71 patients suffering from rare anemia disorders with 521 microscopic images of red blood cells, our experiments show that topological regularization is an effective method that leads to more than 3% performance improvements for the automated classification of rare anemia disorders based on single-cell images. This is the first approach that uses topological properties for regularizing the MIL process.
</details>
<details>
<summary>摘要</summary>
诊断罕见血红素疾病使用微型图像是困难的，both skilled specialists and machine learning methods alike. Due to thousands of disease-relevant cells in a single blood sample, this constitutes a complex multiple-instance learning (MIL) problem. While the spatial neighborhood of red blood cells is not meaningful per se, the topology, i.e., the geometry of blood samples as a whole, contains informative features to remedy typical MIL issues, such as vanishing gradients and overfitting when training on limited data. We thus develop a topology-based approach that extracts multi-scale topological features from bags of single red blood cell images. The topological features are used to regularize the model, enforcing the preservation of characteristic topological properties of the data. Applied to a dataset of 71 patients suffering from rare anemia disorders with 521 microscopic images of red blood cells, our experiments show that topological regularization is an effective method that leads to more than 3% performance improvements for the automated classification of rare anemia disorders based on single-cell images. This is the first approach that uses topological properties for regularizing the MIL process.Here's the translation in Traditional Chinese as well:诊断罕见血红素疾病使用微型图像是困难的，both skilled specialists and machine learning methods alike. Due to thousands of disease-relevant cells in a single blood sample, this constitutes a complex multiple-instance learning (MIL) problem. While the spatial neighborhood of red blood cells is not meaningful per se, the topology, i.e., the geometry of blood samples as a whole, contains informative features to remedy typical MIL issues, such as vanishing gradients and overfitting when training on limited data. We thus develop a topology-based approach that extracts multi-scale topological features from bags of single red blood cell images. The topological features are used to regularize the model, enforcing the preservation of characteristic topological properties of the data. Applied to a dataset of 71 patients suffering from rare anemia disorders with 521 microscopic images of red blood cells, our experiments show that topological regularization is an effective method that leads to more than 3% performance improvements for the automated classification of rare anemia disorders based on single-cell images. This is the first approach that uses topological properties for regularizing the MIL process.
</details></li>
</ul>
<hr>
<h2 id="Are-Transformers-with-One-Layer-Self-Attention-Using-Low-Rank-Weight-Matrices-Universal-Approximators"><a href="#Are-Transformers-with-One-Layer-Self-Attention-Using-Low-Rank-Weight-Matrices-Universal-Approximators" class="headerlink" title="Are Transformers with One Layer Self-Attention Using Low-Rank Weight Matrices Universal Approximators?"></a>Are Transformers with One Layer Self-Attention Using Low-Rank Weight Matrices Universal Approximators?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14023">http://arxiv.org/abs/2307.14023</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tokio Kajitsuka, Issei Sato</li>
<li>for: 本研究探讨了Transformer模型的表达能力，并解决了现有分析中的深层层次问题。</li>
<li>methods: 本研究使用了 clarify softmax函数和Boltzmann算子之间的连接，以证明单层自注意层可以完全捕捉输入序列的上下文。</li>
<li>results: 研究显示，单层Transformer模型具有内存化能力，而且由一个自注意层和两个预测神经网络组成的Transformer模型是一个universal approximator。<details>
<summary>Abstract</summary>
Existing analyses of the expressive capacity of Transformer models have required excessively deep layers for data memorization, leading to a discrepancy with the Transformers actually used in practice. This is primarily due to the interpretation of the softmax function as an approximation of the hardmax function. By clarifying the connection between the softmax function and the Boltzmann operator, we prove that a single layer of self-attention with low-rank weight matrices possesses the capability to perfectly capture the context of an entire input sequence. As a consequence, we show that single-layer Transformer has a memorization capacity for finite samples, and that Transformers consisting of one self-attention layer with two feed-forward neural networks are universal approximators for continuous functions on a compact domain.
</details>
<details>
<summary>摘要</summary>
existing 分析吧Transformer模型的表达能力已经需要过度深度层次，导致与实际使用中的Transformer模型存在差异。这主要是由于软MAX函数的解释为硬MAX函数的近似。通过证明软MAX函数和博尔tz曼算子之间的连接，我们证明了一层自注意力层次可以完美地捕捉整个输入序列的上下文。因此，我们显示了单层Transformer具有内存化能力，并且Transformer由一层自注意力层次和两个预处理神经网络组成的模型是continue 函数在封闭区域上的universal approximator。
</details></li>
</ul>
<hr>
<h2 id="MCMC-Correction-of-Score-Based-Diffusion-Models-for-Model-Composition"><a href="#MCMC-Correction-of-Score-Based-Diffusion-Models-for-Model-Composition" class="headerlink" title="MCMC-Correction of Score-Based Diffusion Models for Model Composition"></a>MCMC-Correction of Score-Based Diffusion Models for Model Composition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14012">http://arxiv.org/abs/2307.14012</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jackonelli/mcmc_corr_score_diffusion">https://github.com/jackonelli/mcmc_corr_score_diffusion</a></li>
<li>paper_authors: Anders Sjöberg, Jakob Lindqvist, Magnus Önnheim, Mats Jirstrand, Lennart Svensson</li>
<li>for: 这个论文的目的是提出一种新的推 diffusion 方法，以便在不同的分布中进行采样。</li>
<li>methods: 这个论文使用了分Diffusion models可以被参数化为得分或能量函数。这个研究使用了得分函数参数化，并通过 Metropolis–Hastings  correction step来进行扩展采样。</li>
<li>results: 研究发现，使用得分函数参数化的方法可以 Achieve similar or better performance than使用能量函数参数化的方法。此外，这种方法还可以 reuse existing diffusion models and combine with various Markov-Chain Monte Carlo (MCMC) methods。<details>
<summary>Abstract</summary>
Diffusion models can be parameterised in terms of either a score or an energy function. The energy parameterisation has better theoretical properties, mainly that it enables an extended sampling procedure with a Metropolis--Hastings correction step, based on the change in total energy in the proposed samples. However, it seems to yield slightly worse performance, and more importantly, due to the widespread popularity of score-based diffusion, there are limited availability of off-the-shelf pre-trained energy-based ones. This limitation undermines the purpose of model composition, which aims to combine pre-trained models to sample from new distributions. Our proposal, however, suggests retaining the score parameterization and instead computing the energy-based acceptance probability through line integration of the score function. This allows us to re-use existing diffusion models and still combine the reverse process with various Markov-Chain Monte Carlo (MCMC) methods. We evaluate our method on a 2D experiment and find that it achieve similar or arguably better performance than the energy parameterisation.
</details>
<details>
<summary>摘要</summary>
Diffusion models 可以被参数化为得分或能量函数。能量参数化有更好的理论性质，主要是允许扩展采样过程，并基于变化总能量进行 Metropolis-Hastings 修正步骤。然而，它似乎在性能上略微 inferior，而且由于得分基 diffusion 的普遍投身，有限的可用性。这限制了模型组合的目的，即将预训练模型组合以采样新的分布。我们的提议是保留得分参数化，并通过线tegration 计算能量基于接受概率。这允许我们 reuse 现有的 diffusion 模型，并且与多种 Markov-Chain Monte Carlo (MCMC) 方法结合。我们在 2D 实验中评估了我们的方法，并发现它们可以达到相似或更好的性能。
</details></li>
</ul>
<hr>
<h2 id="Diff-E-Diffusion-based-Learning-for-Decoding-Imagined-Speech-EEG"><a href="#Diff-E-Diffusion-based-Learning-for-Decoding-Imagined-Speech-EEG" class="headerlink" title="Diff-E: Diffusion-based Learning for Decoding Imagined Speech EEG"></a>Diff-E: Diffusion-based Learning for Decoding Imagined Speech EEG</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14389">http://arxiv.org/abs/2307.14389</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yorgoon/diffe">https://github.com/yorgoon/diffe</a></li>
<li>paper_authors: Soowon Kim, Young-Eun Lee, Seo-Hyun Lee, Seong-Whan Lee</li>
<li>for: 这篇研究旨在实现透过想像的语音来进行沟通，并使用数据减预测模型（DDPM）和条件自动encoder（Diff-E）来实现这一目标。</li>
<li>methods: 本研究使用DDPM和Diff-E实现透过想像的语音的EEG信号解oding，并与传统机器学习技术和基准模型进行比较。</li>
<li>results: 结果显示，Diff-E可以对EEG信号进行高精度的解oding，并提高透过想像的语音的沟通率，具有实际应用价值，可能对于开发基于想像语音的脑computer界面（BCI）有所帮助。<details>
<summary>Abstract</summary>
Decoding EEG signals for imagined speech is a challenging task due to the high-dimensional nature of the data and low signal-to-noise ratio. In recent years, denoising diffusion probabilistic models (DDPMs) have emerged as promising approaches for representation learning in various domains. Our study proposes a novel method for decoding EEG signals for imagined speech using DDPMs and a conditional autoencoder named Diff-E. Results indicate that Diff-E significantly improves the accuracy of decoding EEG signals for imagined speech compared to traditional machine learning techniques and baseline models. Our findings suggest that DDPMs can be an effective tool for EEG signal decoding, with potential implications for the development of brain-computer interfaces that enable communication through imagined speech.
</details>
<details>
<summary>摘要</summary>
<<SYS>> traduced the text into Simplified Chinese.<</SYS>>解oding EEG信号为想象的语言是一项复杂的任务，因为数据的高维度和信号噪声比率低。在最近几年，杂 diffusion probabilistic models（DDPMs）已经出现为不同领域的表征学习提出了可能的方法。我们的研究提出了一种使用 DDPMs 和名为 Diff-E 的条件 autoencoder 来解码 EEG 信号的新方法。结果表明，Diff-E 可以在解码 EEG 信号的想象语言方面提高精度，比传统机器学习技术和基线模型更高。我们的发现表明，DDPMs 可以是 EEG 信号解码的有效工具，具有可能为 brain-computer interface 的发展带来沟通通过想象语言的潜在意义。
</details></li>
</ul>
<hr>
<h2 id="Fast-algorithms-for-k-submodular-maximization-subject-to-a-matroid-constraint"><a href="#Fast-algorithms-for-k-submodular-maximization-subject-to-a-matroid-constraint" class="headerlink" title="Fast algorithms for k-submodular maximization subject to a matroid constraint"></a>Fast algorithms for k-submodular maximization subject to a matroid constraint</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13996">http://arxiv.org/abs/2307.13996</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuxian Niu, Qian Liu, Yang Zhou, Min Li</li>
<li>for: 该文章目的是 Maximize $k$-submodular functions under a matroid constraint.</li>
<li>methods: 文章使用 Threshold-Decreasing Algorithm, 比较效率高于 greedy algorithm, yet with little loss in approximation ratio.</li>
<li>results: 文章提供了 $(1&#x2F;2 - \epsilon)$ 和 $(1&#x2F;3 - \epsilon)$ 两种精度的算法，用于 monotone 和 non-monotone $k$-submodular function maximization, 其复杂度为 $O(\frac{n(k\cdot EO + IO)}{\epsilon} \log \frac{r}{\epsilon})$.<details>
<summary>Abstract</summary>
In this paper, we apply a Threshold-Decreasing Algorithm to maximize $k$-submodular functions under a matroid constraint, which reduces the query complexity of the algorithm compared to the greedy algorithm with little loss in approximation ratio. We give a $(\frac{1}{2} - \epsilon)$-approximation algorithm for monotone $k$-submodular function maximization, and a $(\frac{1}{3} - \epsilon)$-approximation algorithm for non-monotone case, with complexity $O(\frac{n(k\cdot EO + IO)}{\epsilon} \log \frac{r}{\epsilon})$, where $r$ denotes the rank of the matroid, and $IO, EO$ denote the number of oracles to evaluate whether a subset is an independent set and to compute the function value of $f$, respectively. Since the constraint of total size can be looked as a special matroid, called uniform matroid, then we present the fast algorithm for maximizing $k$-submodular functions subject to a total size constraint as corollaries. corollaries.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们采用一个阈值递减算法来最大化$k$- су布模函数 beneath 一个 matroid 约束，这会降低我们的查询复杂度相比于使用排序算法，减少了对于approximation ratio的损失。我们提供了一个$(1/2-\epsilon)$-近似算法 для升高 monotone $k$- submodular function，以及一个$(1/3-\epsilon)$-近似算法 for non-monotone case，其复杂度为$O(\frac{n(k\cdot EO+IO)}{\epsilon} \log \frac{r}{\epsilon})$，其中$r$表示 matroid 的排名，$IO, EO$表示计算 whether a subset is an independent set 和计算 $f$ 函数值的oracle数量。由于总大小的约束可以看作特殊的 matroid，called uniform matroid，所以我们将在 corollaries 中提供fast algorithm for maximizing $k$- submodular functions subject to a total size constraint.
</details></li>
</ul>
<hr>
<h2 id="Take-Your-Pick-Enabling-Effective-Personalized-Federated-Learning-within-Low-dimensional-Feature-Space"><a href="#Take-Your-Pick-Enabling-Effective-Personalized-Federated-Learning-within-Low-dimensional-Feature-Space" class="headerlink" title="Take Your Pick: Enabling Effective Personalized Federated Learning within Low-dimensional Feature Space"></a>Take Your Pick: Enabling Effective Personalized Federated Learning within Low-dimensional Feature Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13995">http://arxiv.org/abs/2307.13995</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guogang Zhu, Xuefeng Liu, Shaojie Tang, Jianwei Niu, Xinghao Wu, Jiaxing Shen</li>
<li>for: 该论文旨在提出一种基于个性化联合学习（PFL）的新框架，以便每个客户端可以根据自己的本地任务进行个性化模型训练。</li>
<li>methods: 该论文使用了一种基于特征选择的PFL方法，通过对每个客户端的本地数据进行特征选择，以便在低维度特征空间中进行联合学习。</li>
<li>results: 实验结果表明，该方法可以有效地选择每个客户端的任务相关特征，提高了跨Domain FL模型的性能。<details>
<summary>Abstract</summary>
Personalized federated learning (PFL) is a popular framework that allows clients to have different models to address application scenarios where clients' data are in different domains. The typical model of a client in PFL features a global encoder trained by all clients to extract universal features from the raw data and personalized layers (e.g., a classifier) trained using the client's local data. Nonetheless, due to the differences between the data distributions of different clients (aka, domain gaps), the universal features produced by the global encoder largely encompass numerous components irrelevant to a certain client's local task. Some recent PFL methods address the above problem by personalizing specific parameters within the encoder. However, these methods encounter substantial challenges attributed to the high dimensionality and non-linearity of neural network parameter space. In contrast, the feature space exhibits a lower dimensionality, providing greater intuitiveness and interpretability as compared to the parameter space. To this end, we propose a novel PFL framework named FedPick. FedPick achieves PFL in the low-dimensional feature space by selecting task-relevant features adaptively for each client from the features generated by the global encoder based on its local data distribution. It presents a more accessible and interpretable implementation of PFL compared to those methods working in the parameter space. Extensive experimental results show that FedPick could effectively select task-relevant features for each client and improve model performance in cross-domain FL.
</details>
<details>
<summary>摘要</summary>
个性化联合学习（PFL）是一种广泛使用的框架，允许客户端有不同的模型来处理应用场景中客户端数据的不同领域。典型的客户端模型在PFL中包括全局编码器，该编码器由所有客户端训练以提取Raw数据中的通用特征，以及客户端本地数据中的个性层（例如分类器）。然而，由于客户端数据的不同分布（即领域差距），全局编码器生成的通用特征具有许多无关于特定客户端本地任务的组成部分。一些最近的PFL方法解决了上述问题，通过在编码器中个性化特定参数。然而，这些方法遇到了高维度和非线性的神经网络参数空间的重大挑战。相比之下，特征空间的维度较低，提供了更好的直观性和可解释性，与参数空间相比。为此，我们提出了一种新的PFL框架名为FedPick。FedPick在低维特征空间中实现PFL，通过为每个客户端适应性地选择本地数据分布中任务相关的特征来选择任务相关的特征。它提供了与参数空间中方法相比更加直观和可解释的PFL实现。经验证明，FedPick可以有效地选择每个客户端的任务相关特征，并在跨领域FL中提高模型性能。
</details></li>
</ul>
<hr>
<h2 id="BovineTalk-Machine-Learning-for-Vocalization-Analysis-of-Dairy-Cattle-under-Negative-Affective-States"><a href="#BovineTalk-Machine-Learning-for-Vocalization-Analysis-of-Dairy-Cattle-under-Negative-Affective-States" class="headerlink" title="BovineTalk: Machine Learning for Vocalization Analysis of Dairy Cattle under Negative Affective States"></a>BovineTalk: Machine Learning for Vocalization Analysis of Dairy Cattle under Negative Affective States</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13994">http://arxiv.org/abs/2307.13994</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dinu Gavojdian, Teddy Lazebnik, Madalina Mincu, Ariel Oren, Ioana Nicolae, Anna Zamansky</li>
<li>for: 这个研究是为了开发和验证对livestock动物的情绪状态非侵入式指标，以便将其integrate到户外评估协议中。</li>
<li>methods: 这个研究使用了牛的 vocals 作为情绪状态的指标，并使用了深度学习和可解释机器学习来分类低频和高频牛叫。</li>
<li>results: 研究发现，使用深度学习和可解释机器学习可以达到87.2%和89.4%的准确率 для低频和高频牛叫的分类，以及68.9%和72.5%的牛 individuation 精度。<details>
<summary>Abstract</summary>
There is a critical need to develop and validate non-invasive animal-based indicators of affective states in livestock species, in order to integrate them into on-farm assessment protocols, potentially via the use of precision livestock farming (PLF) tools. One such promising approach is the use of vocal indicators. The acoustic structure of vocalizations and their functions were extensively studied in important livestock species, such as pigs, horses, poultry and goats, yet cattle remain understudied in this context to date. Cows were shown to produce two types vocalizations: low-frequency calls (LF), produced with the mouth closed, or partially closed, for close distance contacts and open mouth emitted high-frequency calls (HF), produced for long distance communication, with the latter considered to be largely associated with negative affective states. Moreover, cattle vocalizations were shown to contain information on individuality across a wide range of contexts, both negative and positive. Nowadays, dairy cows are facing a series of negative challenges and stressors in a typical production cycle, making vocalizations during negative affective states of special interest for research. One contribution of this study is providing the largest to date pre-processed (clean from noises) dataset of lactating adult multiparous dairy cows during negative affective states induced by visual isolation challenges. Here we present two computational frameworks - deep learning based and explainable machine learning based, to classify high and low-frequency cattle calls, and individual cow voice recognition. Our models in these two frameworks reached 87.2% and 89.4% accuracy for LF and HF classification, with 68.9% and 72.5% accuracy rates for the cow individual identification, respectively.
</details>
<details>
<summary>摘要</summary>
有一项急需要的发展和验证，那就是通过非侵入性的动物基因体系来评估livestock种类的情绪状态，以便将其 integrate into 农场评估协议中。一种可能的方法是使用 vocal indicators。studied extensively in important livestock species such as pigs, horses, poultry and goats, but cattle have been understudied in this context to date. Cows produce two types of vocalizations: low-frequency calls (LF), produced with the mouth closed or partially closed for close distance contacts, and open mouth emitted high-frequency calls (HF), produced for long distance communication, with the latter considered to be largely associated with negative affective states. Moreover, cattle vocalizations contain information on individuality across a wide range of contexts, both negative and positive. Nowadays, dairy cows are facing a series of negative challenges and stressors in a typical production cycle, making vocalizations during negative affective states of special interest for research. This study provides the largest to date pre-processed (clean from noises) dataset of lactating adult multiparous dairy cows during negative affective states induced by visual isolation challenges. Here we present two computational frameworks - deep learning based and explainable machine learning based - to classify high and low-frequency cattle calls, and individual cow voice recognition. Our models in these two frameworks reached 87.2% and 89.4% accuracy for LF and HF classification, with 68.9% and 72.5% accuracy rates for the cow individual identification, respectively.
</details></li>
</ul>
<hr>
<h2 id="Differentiable-short-time-Fourier-transform-with-respect-to-the-hop-length"><a href="#Differentiable-short-time-Fourier-transform-with-respect-to-the-hop-length" class="headerlink" title="Differentiable short-time Fourier transform with respect to the hop length"></a>Differentiable short-time Fourier transform with respect to the hop length</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02421">http://arxiv.org/abs/2308.02421</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/maxime-leiber/dstft">https://github.com/maxime-leiber/dstft</a></li>
<li>paper_authors: Maxime Leiber, Yosra Marnissi, Axel Barrau, Mohammed El Badaoui</li>
<li>for: 提出一种可微分的快时傅立叙变换（STFT），允许通过continuous hop length或frame temporal position的优化来提高时间位置控制。</li>
<li>methods: 使用continuous hop length和frame temporal position的优化，提供更精细的时间位置控制，并且可以使用计算效率更高的优化方法，如梯度下降。</li>
<li>results: 通过 simulations 示例，证明了我们的方法的有效性，并且可以轻松地与现有的算法和神经网络集成。<details>
<summary>Abstract</summary>
In this paper, we propose a differentiable version of the short-time Fourier transform (STFT) that allows for gradient-based optimization of the hop length or the frame temporal position by making these parameters continuous. Our approach provides improved control over the temporal positioning of frames, as the continuous nature of the hop length allows for a more finely-tuned optimization. Furthermore, our contribution enables the use of optimization methods such as gradient descent, which are more computationally efficient than conventional discrete optimization methods. Our differentiable STFT can also be easily integrated into existing algorithms and neural networks. We present a simulated illustration to demonstrate the efficacy of our approach and to garner interest from the research community.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种可微分的短时傅立叙变换（STFT），允许通过使这些参数变为连续的数值来进行梯度下降优化。我们的方法可以提供更好的控制时间位置，因为连续的跳跃长度允许更细化优化。此外，我们的贡献允许使用优化方法，如梯度下降，这些方法更有效率 than conventional discrete optimization methods。我们的可微分STFT也可以轻松地与现有的算法和神经网络集成。我们提供了一个 simulated 示例，以示出我们的方法的有效性并引起研究者的关注。
</details></li>
</ul>
<hr>
<h2 id="METAVerse-Meta-Learning-Traversability-Cost-Map-for-Off-Road-Navigation"><a href="#METAVerse-Meta-Learning-Traversability-Cost-Map-for-Off-Road-Navigation" class="headerlink" title="METAVerse: Meta-Learning Traversability Cost Map for Off-Road Navigation"></a>METAVerse: Meta-Learning Traversability Cost Map for Off-Road Navigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13991">http://arxiv.org/abs/2307.13991</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junwon Seo, Taekyung Kim, Seongyong Ahn, Kiho Kwak</li>
<li>for: 这篇论文是为了提出一种能够在不同环境中准确地估计地形通行性的自适应导航方法。</li>
<li>methods: 该方法使用了元学习框架，通过自动学习方式将多种环境中的车辆-地形交互反馈纳入模型中，以减少估计uncertainty。</li>
<li>results: 该方法可以在不同环境中获得一个准确和可靠的地形通行性估计模型，并且可以通过与预测控制器结合使用，实现安全和稳定的导航。<details>
<summary>Abstract</summary>
Autonomous navigation in off-road conditions requires an accurate estimation of terrain traversability. However, traversability estimation in unstructured environments is subject to high uncertainty due to the variability of numerous factors that influence vehicle-terrain interaction. Consequently, it is challenging to obtain a generalizable model that can accurately predict traversability in a variety of environments. This paper presents METAVerse, a meta-learning framework for learning a global model that accurately and reliably predicts terrain traversability across diverse environments. We train the traversability prediction network to generate a dense and continuous-valued cost map from a sparse LiDAR point cloud, leveraging vehicle-terrain interaction feedback in a self-supervised manner. Meta-learning is utilized to train a global model with driving data collected from multiple environments, effectively minimizing estimation uncertainty. During deployment, online adaptation is performed to rapidly adapt the network to the local environment by exploiting recent interaction experiences. To conduct a comprehensive evaluation, we collect driving data from various terrains and demonstrate that our method can obtain a global model that minimizes uncertainty. Moreover, by integrating our model with a model predictive controller, we demonstrate that the reduced uncertainty results in safe and stable navigation in unstructured and unknown terrains.
</details>
<details>
<summary>摘要</summary>
自主导航在无结构环境中需要准确地估计地形通行性。然而，在无结构环境中的通行性估计受到许多因素的变化影响，这些因素包括车辆和地形之间的互动。因此，建立一个泛化模型，以准确地预测不同环境中的通行性，是一项挑战。这篇论文提出了METAVerse，一个基于元学习的框架，用于学习一个准确和可靠地预测地形通行性的全球模型。我们在训练通行性预测网络时，使用自动学习的方式，从稀疏的 LiDAR 点云中生成一个密集和连续的成本图，以利用车辆和地形之间的互动反馈。元学习被用来训练全球模型，以使其在多个环境中具有最小的估计不确定性。在部署过程中，我们通过在线适应来快速地适应当地环境，并且通过利用最近的互动经验来进行更新。我们通过收集来自不同地形的驾驶数据，证明了我们的方法可以获得一个全球模型，以准确地预测不同环境中的通行性。此外，我们将我们的模型与预测控制器结合，以验证了减少了不确定性的结果，可以在未知的无结构环境中安全和稳定地导航。
</details></li>
</ul>
<hr>
<h2 id="Differentiable-adaptive-short-time-Fourier-transform-with-respect-to-the-window-length"><a href="#Differentiable-adaptive-short-time-Fourier-transform-with-respect-to-the-window-length" class="headerlink" title="Differentiable adaptive short-time Fourier transform with respect to the window length"></a>Differentiable adaptive short-time Fourier transform with respect to the window length</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02418">http://arxiv.org/abs/2308.02418</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/maxime-leiber/dstft">https://github.com/maxime-leiber/dstft</a></li>
<li>paper_authors: Maxime Leiber, Yosra Marnissi, Axel Barrau, Mohammed El Badaoui</li>
<li>for: 这 paper 用于提出了一种基于梯度的方法，用于在实时进行 STFT 的参数优化，包括每帧和每频率窗口长的优化。</li>
<li>methods: 这 paper 使用了梯度下降优化方法，将 STFT 中的窗口长作为连续参数，使得 STFT 可以适应变化的时域频率特征。</li>
<li>results: 作者验证了这种方法在震动分析中的性能，并证明了它可以同时适应变化的时域频率特征，而且可以通过梯度下降优化方法进行快速优化。<details>
<summary>Abstract</summary>
This paper presents a gradient-based method for on-the-fly optimization for both per-frame and per-frequency window length of the short-time Fourier transform (STFT), related to previous work in which we developed a differentiable version of STFT by making the window length a continuous parameter. The resulting differentiable adaptive STFT possesses commendable properties, such as the ability to adapt in the same time-frequency representation to both transient and stationary components, while being easily optimized by gradient descent. We validate the performance of our method in vibration analysis.
</details>
<details>
<summary>摘要</summary>
Here's the text in Simplified Chinese:这篇论文提出了一种基于梯度的方法，用于在实时进行STFT（短时傅立叶变换）的框架和频率窗口长度的在线优化，与之前的工作相关，我们将STFT中的窗口长度变为连续参数。结果的可微STFT具有了许多优点，如适应到同时域频率上的激变和站立部分，同时也容易使用梯度下降优化。我们通过振荡分析 validate the performance of our method。
</details></li>
</ul>
<hr>
<h2 id="This-is-not-correct-Negation-aware-Evaluation-of-Language-Generation-Systems"><a href="#This-is-not-correct-Negation-aware-Evaluation-of-Language-Generation-Systems" class="headerlink" title="This is not correct! Negation-aware Evaluation of Language Generation Systems"></a>This is not correct! Negation-aware Evaluation of Language Generation Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13989">http://arxiv.org/abs/2307.13989</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dmlls/cannot-dataset">https://github.com/dmlls/cannot-dataset</a></li>
<li>paper_authors: Miriam Anschütz, Diego Miguel Lozano, Georg Groh</li>
<li>for: 本研究旨在提高大型自然语言处理模型对否定语言的识别能力，以提高模型对语言表达中的意义更加精准地识别和理解。</li>
<li>methods: 本研究使用了规则基本的句子否定工具，创建了CANNOT negation评估数据集，并使用了句子转换器和评估指标进行了细化。</li>
<li>results: 对现有的评估指标进行评估，本研究的细化模型在否定句子上表现出色，与基本模型在其他杂化情况下的表现相比，有很大的提升。<details>
<summary>Abstract</summary>
Large language models underestimate the impact of negations on how much they change the meaning of a sentence. Therefore, learned evaluation metrics based on these models are insensitive to negations. In this paper, we propose NegBLEURT, a negation-aware version of the BLEURT evaluation metric. For that, we designed a rule-based sentence negation tool and used it to create the CANNOT negation evaluation dataset. Based on this dataset, we fine-tuned a sentence transformer and an evaluation metric to improve their negation sensitivity. Evaluating these models on existing benchmarks shows that our fine-tuned models outperform existing metrics on the negated sentences by far while preserving their base models' performances on other perturbations.
</details>
<details>
<summary>摘要</summary>
大型语言模型会在判断句子意义时忽略否定影响。因此，基于这些模型的评估指标会对否定不敏感。在这篇文章中，我们提出了NegBLEURT评估指标，这是一个对否定敏感的BLEURT评估指标。为了建立这个评估指标，我们设计了一个基于规则的句子否定工具，并使用这个工具创建了CANNOT评估集。基于这个集，我们精炼了句子变换器和评估指标，以提高它们对否定的敏感度。对现有的benchmark测试显示，我们精炼的模型在否定句子上表现明显比其他评估指标好，而且保留了基本模型在其他扰动下的表现。
</details></li>
</ul>
<hr>
<h2 id="Controlling-the-Latent-Space-of-GANs-through-Reinforcement-Learning-A-Case-Study-on-Task-based-Image-to-Image-Translation"><a href="#Controlling-the-Latent-Space-of-GANs-through-Reinforcement-Learning-A-Case-Study-on-Task-based-Image-to-Image-Translation" class="headerlink" title="Controlling the Latent Space of GANs through Reinforcement Learning: A Case Study on Task-based Image-to-Image Translation"></a>Controlling the Latent Space of GANs through Reinforcement Learning: A Case Study on Task-based Image-to-Image Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13978">http://arxiv.org/abs/2307.13978</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahyar Abbasian, Taha Rajabzadeh, Ahmadreza Moradipari, Seyed Amir Hossein Aqajari, Hongsheng Lu, Amir Rahmani</li>
<li>for: 这篇论文旨在解决Generative Adversarial Networks (GAN) 的控制问题，提高 GAN 的生成效果。</li>
<li>methods: 本论文提出了一种新的方法，通过融合对抗学习（RL）代理人与潜在空间 GAN（l-GAN），以生成适当的出力。RL 代理人通过精心设计的奖励策略，获得了在潜在空间中穿梭并生成出力的能力。</li>
<li>results: 本论文通过使用 MNIST dataset 进行了一系列实验，包括一个示例任务：加法。实验结果证实了我们的方法效果。本论文的创新的RL代理人与 GAN 模型融合，具有很大的应用前途。<details>
<summary>Abstract</summary>
Generative Adversarial Networks (GAN) have emerged as a formidable AI tool to generate realistic outputs based on training datasets. However, the challenge of exerting control over the generation process of GANs remains a significant hurdle. In this paper, we propose a novel methodology to address this issue by integrating a reinforcement learning (RL) agent with a latent-space GAN (l-GAN), thereby facilitating the generation of desired outputs. More specifically, we have developed an actor-critic RL agent with a meticulously designed reward policy, enabling it to acquire proficiency in navigating the latent space of the l-GAN and generating outputs based on specified tasks. To substantiate the efficacy of our approach, we have conducted a series of experiments employing the MNIST dataset, including arithmetic addition as an illustrative task. The outcomes of these experiments serve to validate our methodology. Our pioneering integration of an RL agent with a GAN model represents a novel advancement, holding great potential for enhancing generative networks in the future.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Mathematical-Modeling-of-BCG-based-Bladder-Cancer-Treatment-Using-Socio-Demographics"><a href="#Mathematical-Modeling-of-BCG-based-Bladder-Cancer-Treatment-Using-Socio-Demographics" class="headerlink" title="Mathematical Modeling of BCG-based Bladder Cancer Treatment Using Socio-Demographics"></a>Mathematical Modeling of BCG-based Bladder Cancer Treatment Using Socio-Demographics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15084">http://arxiv.org/abs/2307.15084</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elizaveta Savchenko, Ariel Rosenfeld, Svetlana Bunimovich-Mendrazitsky</li>
<li>for: 这个研究旨在提出一种个性化的数学模型，用于预测bcg基于治疗的临床动态。</li>
<li>methods: 该研究采用了一种已知的bcg治疗模型，并将机器学习组件 integrate到模型中，以时间性地调整和重配置关键参数。</li>
<li>results: 使用实际临床数据，研究表明，个性化模型与原始模型相比，在预测bcg治疗结束时的癌细胞数量方面有14.8%的改善，平均而言。<details>
<summary>Abstract</summary>
Cancer is one of the most widespread diseases around the world with millions of new patients each year. Bladder cancer is one of the most prevalent types of cancer affecting all individuals alike with no obvious prototypical patient. The current standard treatment for BC follows a routine weekly Bacillus Calmette-Guerin (BCG) immunotherapy-based therapy protocol which is applied to all patients alike. The clinical outcomes associated with BCG treatment vary significantly among patients due to the biological and clinical complexity of the interaction between the immune system, treatments, and cancer cells. In this study, we take advantage of the patient's socio-demographics to offer a personalized mathematical model that describes the clinical dynamics associated with BCG-based treatment. To this end, we adopt a well-established BCG treatment model and integrate a machine learning component to temporally adjust and reconfigure key parameters within the model thus promoting its personalization. Using real clinical data, we show that our personalized model favorably compares with the original one in predicting the number of cancer cells at the end of the treatment, with 14.8% improvement, on average.
</details>
<details>
<summary>摘要</summary>
肿瘤是全球最普遍的疾病之一，每年新生发病人数达到百万。膀胱癌是所有人都受到影响的最常见的抑肿癌类型，没有明显的典型患者形象。现有的标准治疗办法是每周一次的细菌Calmette-Guerin（BCG）免疫疗法，该办法适用于所有患者。但是，BCG治疗的临床结果因免疫系统、治疗和肿瘤细胞之间的生物和临床复杂性而异常变化。在这项研究中，我们利用患者的社会民生数据来提供个性化的数学模型，描述BCG基于治疗的临床动态。为此，我们采用了已知的BCG治疗模型，并将机器学习组件加入，以时间地调整和重新配置关键参数，以便个性化。使用实际临床数据，我们表明，我们的个性化模型与原始模型相比，在预测治疗结束后肿瘤细胞数量方面表现出了14.8%的改善，平均而言。
</details></li>
</ul>
<hr>
<h2 id="Understanding-Deep-Neural-Networks-via-Linear-Separability-of-Hidden-Layers"><a href="#Understanding-Deep-Neural-Networks-via-Linear-Separability-of-Hidden-Layers" class="headerlink" title="Understanding Deep Neural Networks via Linear Separability of Hidden Layers"></a>Understanding Deep Neural Networks via Linear Separability of Hidden Layers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13962">http://arxiv.org/abs/2307.13962</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao Zhang, Xinyu Chen, Wensheng Li, Lixue Liu, Wei Wu, Dacheng Tao</li>
<li>for: 本研究用来研究深度神经网络的特点，特别是深度神经网络的线性可分性。</li>
<li>methods: 本研究提出了基于米诺夫做ifferencedifference measure（MD-LSM）来评估线性可分性度的两个点集。然后，我们证明了深度神经网络训练性能和线性可分性度之间存在同步关系，即如果更新权重可以提高线性可分性度，则更新后的网络将在训练过程中表现更好，并且相反。此外，我们还研究了活化函数和网络大小（包括宽度和深度）对隐藏层的线性可分性度的影响。</li>
<li>results: 我们通过实验 validate了我们的发现，测试了一些流行的深度网络，包括多层感知器（MLP）、卷积神经网络（CNN）、深度信念网络（DBN）、ResNet、VGGNet、AlexNet、视transformer（ViT）和GoogLeNet。<details>
<summary>Abstract</summary>
In this paper, we measure the linear separability of hidden layer outputs to study the characteristics of deep neural networks. In particular, we first propose Minkowski difference based linear separability measures (MD-LSMs) to evaluate the linear separability degree of two points sets. Then, we demonstrate that there is a synchronicity between the linear separability degree of hidden layer outputs and the network training performance, i.e., if the updated weights can enhance the linear separability degree of hidden layer outputs, the updated network will achieve a better training performance, and vice versa. Moreover, we study the effect of activation function and network size (including width and depth) on the linear separability of hidden layers. Finally, we conduct the numerical experiments to validate our findings on some popular deep networks including multilayer perceptron (MLP), convolutional neural network (CNN), deep belief network (DBN), ResNet, VGGNet, AlexNet, vision transformer (ViT) and GoogLeNet.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们测量了深度神经网络中隐藏层输出的线性分割性，以study深度神经网络的特点。特别是，我们首先提出了Minkowski差值基于的线性分割度量指标（MD-LSM），用于评估两个点集的线性分割度。然后，我们示出了隐藏层输出线性分割度和网络训练性能之间的同步关系，即如果更新的权重可以提高隐藏层输出的线性分割度，则更新后的网络将达到更好的训练性能，并且相反。此外，我们研究了活动函数和网络大小（包括宽和深）对隐藏层的线性分割性的影响。最后，我们进行了实验验证我们的发现，并在一些流行的深度神经网络，如多层感知网络（MLP）、卷积神经网络（CNN）、深度信念网络（DBN）、ResNet、VGGNet、AlexNet、视Transformer（ViT）和GoogLeNet等进行了数值实验。
</details></li>
</ul>
<hr>
<h2 id="Flexible-Differentially-Private-Vertical-Federated-Learning-with-Adaptive-Feature-Embeddings"><a href="#Flexible-Differentially-Private-Vertical-Federated-Learning-with-Adaptive-Feature-Embeddings" class="headerlink" title="Flexible Differentially Private Vertical Federated Learning with Adaptive Feature Embeddings"></a>Flexible Differentially Private Vertical Federated Learning with Adaptive Feature Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02362">http://arxiv.org/abs/2308.02362</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxi Mi, Hongquan Liu, Yewei Xia, Yiheng Sun, Jihong Guan, Shuigeng Zhou<br>for: This paper focuses on the delicate balance between data privacy and task utility in vertical federated learning (VFL) under differential privacy (DP).methods: The authors propose a flexible and generic approach that decouples the privacy and utility goals, addressing them successively. They first derive a privacy guarantee using norm clipping on shared feature embeddings, and then optimize task utility through adaptive adjustments on the scale and distribution of feature embeddings.results: The proposed VFL-AFE framework exhibits effectiveness against privacy attacks and retains favorable task utility, as demonstrated through extensive experiments.<details>
<summary>Abstract</summary>
The emergence of vertical federated learning (VFL) has stimulated concerns about the imperfection in privacy protection, as shared feature embeddings may reveal sensitive information under privacy attacks. This paper studies the delicate equilibrium between data privacy and task utility goals of VFL under differential privacy (DP). To address the generality issue of prior arts, this paper advocates a flexible and generic approach that decouples the two goals and addresses them successively. Specifically, we initially derive a rigorous privacy guarantee by applying norm clipping on shared feature embeddings, which is applicable across various datasets and models. Subsequently, we demonstrate that task utility can be optimized via adaptive adjustments on the scale and distribution of feature embeddings in an accuracy-appreciative way, without compromising established DP mechanisms. We concretize our observation into the proposed VFL-AFE framework, which exhibits effectiveness against privacy attacks and the capacity to retain favorable task utility, as substantiated by extensive experiments.
</details>
<details>
<summary>摘要</summary>
vertical federated learning (VFL)的出现引起了隐私保护不足的担忧，因为分享特征嵌入可能在隐私攻击下泄露敏感信息。这篇论文研究了VFL中数据隐私和任务用途目标之间的紧耦合关系，并提出了一种flexible和通用的方法来解决这个问题。我们首先通过应用norm clipping来 derive privacy guarantee，这种方法适用于各种数据集和模型。然后，我们示出了通过 adaptive adjustments来 optimize任务用途，无需牺牲已有的隐私机制。我们将这些观察集成为VFL-AFE框架，该框架具有防止隐私攻击和保持任务用途的能力，并经过了广泛的实验证明。
</details></li>
</ul>
<hr>
<h2 id="Entropy-Neural-Estimation-for-Graph-Contrastive-Learning"><a href="#Entropy-Neural-Estimation-for-Graph-Contrastive-Learning" class="headerlink" title="Entropy Neural Estimation for Graph Contrastive Learning"></a>Entropy Neural Estimation for Graph Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13944">http://arxiv.org/abs/2307.13944</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kunzhan/M-ILBO">https://github.com/kunzhan/M-ILBO</a></li>
<li>paper_authors: Yixuan Ma, Xiaolin Zhang, Peng Zhang, Kun Zhan</li>
<li>for: 本文提出了一种基于对比学习的图像分类方法，用于提取图像中的高级特征表示。</li>
<li>methods: 本文使用了一种基于Maximum Mutual Information的方法来估计数据集的熵，并提出了一种简单 yet effective的子集采样策略来实现对比表示之间的对比。</li>
<li>results: 实验结果表明，提出的方法可以在七个图像benchmark上 achieve competitive performance，并且可以增强图像encoder的表示能力。<details>
<summary>Abstract</summary>
Contrastive learning on graphs aims at extracting distinguishable high-level representations of nodes. In this paper, we theoretically illustrate that the entropy of a dataset can be approximated by maximizing the lower bound of the mutual information across different views of a graph, \ie, entropy is estimated by a neural network. Based on this finding, we propose a simple yet effective subset sampling strategy to contrast pairwise representations between views of a dataset. In particular, we randomly sample nodes and edges from a given graph to build the input subset for a view. Two views are fed into a parameter-shared Siamese network to extract the high-dimensional embeddings and estimate the information entropy of the entire graph. For the learning process, we propose to optimize the network using two objectives, simultaneously. Concretely, the input of the contrastive loss function consists of positive and negative pairs. Our selection strategy of pairs is different from previous works and we present a novel strategy to enhance the representation ability of the graph encoder by selecting nodes based on cross-view similarities. We enrich the diversity of the positive and negative pairs by selecting highly similar samples and totally different data with the guidance of cross-view similarity scores, respectively. We also introduce a cross-view consistency constraint on the representations generated from the different views. This objective guarantees the learned representations are consistent across views from the perspective of the entire graph. We conduct extensive experiments on seven graph benchmarks, and the proposed approach achieves competitive performance compared to the current state-of-the-art methods. The source code will be publicly released once this paper is accepted.
</details>
<details>
<summary>摘要</summary>
contrastive learning on graphs aims to extract distinguishable high-level node representations. in this paper, we prove that the entropy of a dataset can be approximated by maximizing the lower bound of the mutual information across different graph views, \ie, entropy is estimated by a neural network. based on this finding, we propose a simple yet effective subset sampling strategy to contrast pairwise representations between views of a dataset. specifically, we randomly sample nodes and edges from a given graph to build the input subset for a view. two views are fed into a parameter-shared siamese network to extract high-dimensional embeddings and estimate the information entropy of the entire graph. for the learning process, we propose to optimize the network using two objectives simultaneously. concretely, the input of the contrastive loss function consists of positive and negative pairs. our selection strategy of pairs is different from previous works, and we present a novel strategy to enhance the representation ability of the graph encoder by selecting nodes based on cross-view similarities. we enrich the diversity of the positive and negative pairs by selecting highly similar samples and totally different data with the guidance of cross-view similarity scores, respectively. we also introduce a cross-view consistency constraint on the representations generated from the different views. this objective guarantees the learned representations are consistent across views from the perspective of the entire graph. we conduct extensive experiments on seven graph benchmarks, and the proposed approach achieves competitive performance compared to the current state-of-the-art methods. the source code will be publicly released once this paper is accepted.
</details></li>
</ul>
<hr>
<h2 id="Topology-aware-Robust-Optimization-for-Out-of-distribution-Generalization"><a href="#Topology-aware-Robust-Optimization-for-Out-of-distribution-Generalization" class="headerlink" title="Topology-aware Robust Optimization for Out-of-distribution Generalization"></a>Topology-aware Robust Optimization for Out-of-distribution Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13943">http://arxiv.org/abs/2307.13943</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/joffery/tro">https://github.com/joffery/tro</a></li>
<li>paper_authors: Fengchun Qiao, Xi Peng</li>
<li>for: 本研究旨在提高机器学习模型对异常输入的抗性，以提高高风险应用中的模型可靠性。</li>
<li>methods: 本研究提出了一种基于分布 topology 的 robust optimization 方法，包括两个优化目标：分布学习和学习 на topology。</li>
<li>results: 实验表明，这种方法可以在多种任务中（包括分类、回归和semantic segmentation）明显超过现有方法，并且发现数据驱动的分布 topology 与领域知识是相一致的，从而提高了方法的可解释性。<details>
<summary>Abstract</summary>
Out-of-distribution (OOD) generalization is a challenging machine learning problem yet highly desirable in many high-stake applications. Existing methods suffer from overly pessimistic modeling with low generalization confidence. As generalizing to arbitrary test distributions is impossible, we hypothesize that further structure on the topology of distributions is crucial in developing strong OOD resilience. To this end, we propose topology-aware robust optimization (TRO) that seamlessly integrates distributional topology in a principled optimization framework. More specifically, TRO solves two optimization objectives: (1) Topology Learning which explores data manifold to uncover the distributional topology; (2) Learning on Topology which exploits the topology to constrain robust optimization for tightly-bounded generalization risks. We theoretically demonstrate the effectiveness of our approach and empirically show that it significantly outperforms the state of the arts in a wide range of tasks including classification, regression, and semantic segmentation. Moreover, we empirically find the data-driven distributional topology is consistent with domain knowledge, enhancing the explainability of our approach.
</details>
<details>
<summary>摘要</summary>
OUT-OF-DISTRIBUTION (OOD) 泛化是一个具有挑战性的机器学习问题，但在许多高风险应用中很需要。现有的方法受到过度的悲观预测，导致模型的泛化自信度较低。由于泛化到任意的测试分布是不可能的，我们假设预设分布的数学结构是决定性的。为了解决这个问题，我们提出了分布数学结构意识的强健泛化优化（TRO）。TRO通过将分布数学结构与强健优化紧密融合，实现了具有理性的优化框架。具体而言，TRO解决以下两个优化目标：1. 分布学习：探索数据构造，发现分布的数学结构。2. 学习于分布：利用分布的数学结构，对泛化优化进行紧密的约束，以降低泛化风险。我们理论上显示TRO的有效性，并实践显示它在许多任务中，包括分类、回归和semantic segmentation，与现有的方法相比，表现得更好。此外，我们实践发现，使用数据驱动的分布数学结构可以与领域知识相互匹配，增加了我们的方法的解释性。
</details></li>
</ul>
<hr>
<h2 id="Improving-Semi-Supervised-Semantic-Segmentation-with-Dual-Level-Siamese-Structure-Network"><a href="#Improving-Semi-Supervised-Semantic-Segmentation-with-Dual-Level-Siamese-Structure-Network" class="headerlink" title="Improving Semi-Supervised Semantic Segmentation with Dual-Level Siamese Structure Network"></a>Improving Semi-Supervised Semantic Segmentation with Dual-Level Siamese Structure Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13938">http://arxiv.org/abs/2307.13938</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kunzhan/DSSN">https://github.com/kunzhan/DSSN</a></li>
<li>paper_authors: Zhibo Tain, Xiaolin Zhang, Peng Zhang, Kun Zhan</li>
<li>for: 提高semantic segmentation的效果，使用both labeled和无标例数据，减少标注训练示例的成本</li>
<li>methods: 提出了一种基于 dual-level Siamese structure network (DSSN) 的像素级对比学习方法，通过在低级图像空间和高级特征空间都使用强制修改视图进行对比，以最大化使用可用的无标例数据</li>
<li>results: 实现了在PASCAL VOC 2012和Cityscapes两个 dataset 上的状态级 результаategraph，与其他 SSS 算法相比，表现出了显著的优异，并且可以减少标注训练示例的成本<details>
<summary>Abstract</summary>
Semi-supervised semantic segmentation (SSS) is an important task that utilizes both labeled and unlabeled data to reduce expenses on labeling training examples. However, the effectiveness of SSS algorithms is limited by the difficulty of fully exploiting the potential of unlabeled data. To address this, we propose a dual-level Siamese structure network (DSSN) for pixel-wise contrastive learning. By aligning positive pairs with a pixel-wise contrastive loss using strong augmented views in both low-level image space and high-level feature space, the proposed DSSN is designed to maximize the utilization of available unlabeled data. Additionally, we introduce a novel class-aware pseudo-label selection strategy for weak-to-strong supervision, which addresses the limitations of most existing methods that do not perform selection or apply a predefined threshold for all classes. Specifically, our strategy selects the top high-confidence prediction of the weak view for each class to generate pseudo labels that supervise the strong augmented views. This strategy is capable of taking into account the class imbalance and improving the performance of long-tailed classes. Our proposed method achieves state-of-the-art results on two datasets, PASCAL VOC 2012 and Cityscapes, outperforming other SSS algorithms by a significant margin.
</details>
<details>
<summary>摘要</summary>
semi-supervised semantic segmentation (SSS) 是一个重要的任务，它利用了标注和无标注数据来降低标注训练示例的成本。然而，SSS 算法的效果受到无标注数据的难以完全利用的限制。为解决这个问题，我们提议一种 dual-level Siamese structure network (DSSN)  для像素级别的对比学习。通过在低级图像空间和高级特征空间都使用强化的扩展视图对正对应的对比损失进行对 Pablo 的对比，我们设计了 DSSN，以便最大化可用的无标注数据的利用。此外，我们引入了一种新的类感知 pseudo-label 选择策略，这种策略可以考虑类别偏度，并提高长尾类别的性能。我们的策略是选择每个类型的高信息报告值作为弱视图中的 pseudo 标签，以便使得强视图中的augmented views得到supervise。我们的提议方法在 PASCAL VOC 2012 和 Cityscapes 两个 dataset 上实现了领先的状态，比其他 SSS 算法高出了一定的幅度。
</details></li>
</ul>
<hr>
<h2 id="trajdata-A-Unified-Interface-to-Multiple-Human-Trajectory-Datasets"><a href="#trajdata-A-Unified-Interface-to-Multiple-Human-Trajectory-Datasets" class="headerlink" title="trajdata: A Unified Interface to Multiple Human Trajectory Datasets"></a>trajdata: A Unified Interface to Multiple Human Trajectory Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13924">http://arxiv.org/abs/2307.13924</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nvlabs/trajdata">https://github.com/nvlabs/trajdata</a></li>
<li>paper_authors: Boris Ivanovic, Guanyu Song, Igor Gilitschenski, Marco Pavone</li>
<li>for: This paper aims to provide a unified interface to multiple human trajectory datasets for researchers to train and evaluate methods more efficiently.</li>
<li>methods: The paper presents a simple, uniform, and efficient representation and API for trajectory and map data, which enables a comprehensive empirical evaluation of existing trajectory datasets.</li>
<li>results: The paper conducts a comprehensive empirical evaluation of existing trajectory datasets, providing users with a rich understanding of the data underpinning much of current pedestrian and AV motion forecasting research, and proposing suggestions for future datasets from these insights.<details>
<summary>Abstract</summary>
The field of trajectory forecasting has grown significantly in recent years, partially owing to the release of numerous large-scale, real-world human trajectory datasets for autonomous vehicles (AVs) and pedestrian motion tracking. While such datasets have been a boon for the community, they each use custom and unique data formats and APIs, making it cumbersome for researchers to train and evaluate methods across multiple datasets. To remedy this, we present trajdata: a unified interface to multiple human trajectory datasets. At its core, trajdata provides a simple, uniform, and efficient representation and API for trajectory and map data. As a demonstration of its capabilities, in this work we conduct a comprehensive empirical evaluation of existing trajectory datasets, providing users with a rich understanding of the data underpinning much of current pedestrian and AV motion forecasting research, and proposing suggestions for future datasets from these insights. trajdata is permissively licensed (Apache 2.0) and can be accessed online at https://github.com/NVlabs/trajdata
</details>
<details>
<summary>摘要</summary>
“ trajectory forecasting 领域在最近几年内发展非常快，一部分这是因为大量的实际世界人 trajectory 数据集（AVs 和行人运动跟踪）的发布。 although these datasets have been a blessing for the community, they each use custom and unique data formats and APIs, making it cumbersome for researchers to train and evaluate methods across multiple datasets. to address this, we present trajdata: a unified interface to multiple human trajectory datasets. at its core, trajdata provides a simple, uniform, and efficient representation and API for trajectory and map data. as a demonstration of its capabilities, in this work we conduct a comprehensive empirical evaluation of existing trajectory datasets, providing users with a rich understanding of the data underpinning much of current pedestrian and AV motion forecasting research, and proposing suggestions for future datasets from these insights. trajdata is licensed under Apache 2.0 and can be accessed online at https://github.com/NVlabs/trajdata.”Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and other countries. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="HyperFed-Hyperbolic-Prototypes-Exploration-with-Consistent-Aggregation-for-Non-IID-Data-in-Federated-Learning"><a href="#HyperFed-Hyperbolic-Prototypes-Exploration-with-Consistent-Aggregation-for-Non-IID-Data-in-Federated-Learning" class="headerlink" title="HyperFed: Hyperbolic Prototypes Exploration with Consistent Aggregation for Non-IID Data in Federated Learning"></a>HyperFed: Hyperbolic Prototypes Exploration with Consistent Aggregation for Non-IID Data in Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14384">http://arxiv.org/abs/2307.14384</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinting Liao, Weiming Liu, Chaochao Chen, Pengyang Zhou, Huabin Zhu, Yanchao Tan, Jun Wang, Yue Qi</li>
<li>for: 提高 Federated Learning（FL）在非同一个分布（non-IID）环境下的性能。</li>
<li>methods: 提议 HyperFed，它包含以下三个主要模块： hyperbolic prototype Tammes initialization（HPTI）、 hyperbolic prototype learning（HPL）和 consistent aggregation（CA）。 HPTI在服务器端 constructions  uniformly distributed和 fixed class prototypes，并将其分享给客户端以匹配类统计，从而引导客户端的具有一致性的特征表示。 HPL在每个客户端上使用分享的类prototype在 hyperbolic 模型空间中捕捉本地数据中的层次信息。 CA在服务器端 mitigates the impact of inconsistent deviations from clients to server。</li>
<li>results: 对四个数据集进行了广泛的研究，证明 HyperFed 可以有效地提高 FL 在 non-IID 环境下的性能。<details>
<summary>Abstract</summary>
Federated learning (FL) collaboratively models user data in a decentralized way. However, in the real world, non-identical and independent data distributions (non-IID) among clients hinder the performance of FL due to three issues, i.e., (1) the class statistics shifting, (2) the insufficient hierarchical information utilization, and (3) the inconsistency in aggregating clients. To address the above issues, we propose HyperFed which contains three main modules, i.e., hyperbolic prototype Tammes initialization (HPTI), hyperbolic prototype learning (HPL), and consistent aggregation (CA). Firstly, HPTI in the server constructs uniformly distributed and fixed class prototypes, and shares them with clients to match class statistics, further guiding consistent feature representation for local clients. Secondly, HPL in each client captures the hierarchical information in local data with the supervision of shared class prototypes in the hyperbolic model space. Additionally, CA in the server mitigates the impact of the inconsistent deviations from clients to server. Extensive studies of four datasets prove that HyperFed is effective in enhancing the performance of FL under the non-IID set.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 协同模型用户数据的方式是分布在多个客户端上的。然而，在现实中，客户端上的数据分布非常不同（非同一个），这会降低 FL 的性能，因为三个问题：1. 类别统计移动2. 不充分利用层次信息3. 客户端聚合不一致为解决这些问题，我们提出了 HyperFed，它包含三个主要模块：1. hyperbolic prototype Tammes initialization (HPTI)：在服务器端constructs uniformly distributed和fixed class prototypes，并将其分享给客户端，以匹配类统计，并且指导客户端的准确特征表示。2. hyperbolic prototype learning (HPL)：在每个客户端上，通过在hyperbolic模型空间的supervision，使得客户端上的数据具有层次结构信息。3. consistent aggregation (CA)：在服务器端，使得客户端的不一致偏差的影响被减轻。对四个数据集进行了广泛的研究，证明了 HyperFed 能够在非同一个情况下提高 FL 的性能。
</details></li>
</ul>
<hr>
<h2 id="Simulation-based-Inference-for-Cardiovascular-Models"><a href="#Simulation-based-Inference-for-Cardiovascular-Models" class="headerlink" title="Simulation-based Inference for Cardiovascular Models"></a>Simulation-based Inference for Cardiovascular Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13918">http://arxiv.org/abs/2307.13918</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antoine Wehenkel, Jens Behrmann, Andrew C. Miller, Guillermo Sapiro, Ozan Sener, Marco Cuturi, Jörn-Henrik Jacobsen</li>
<li>for: studying cardiovascular systems in-silico and simulating whole-body hemodynamics</li>
<li>methods: statistical inference and simulation-based inference (SBI)</li>
<li>results: potential for estimating new biomarkers from standard-of-care measurements, existence of sub-populations with distinct uncertainty regimes, and gap analysis between in-vivo and in-silico data<details>
<summary>Abstract</summary>
Over the past decades, hemodynamics simulators have steadily evolved and have become tools of choice for studying cardiovascular systems in-silico. While such tools are routinely used to simulate whole-body hemodynamics from physiological parameters, solving the corresponding inverse problem of mapping waveforms back to plausible physiological parameters remains both promising and challenging. Motivated by advances in simulation-based inference (SBI), we cast this inverse problem as statistical inference. In contrast to alternative approaches, SBI provides \textit{posterior distributions} for the parameters of interest, providing a \textit{multi-dimensional} representation of uncertainty for \textit{individual} measurements. We showcase this ability by performing an in-silico uncertainty analysis of five biomarkers of clinical interest comparing several measurement modalities. Beyond the corroboration of known facts, such as the feasibility of estimating heart rate, our study highlights the potential of estimating new biomarkers from standard-of-care measurements. SBI reveals practically relevant findings that cannot be captured by standard sensitivity analyses, such as the existence of sub-populations for which parameter estimation exhibits distinct uncertainty regimes. Finally, we study the gap between in-vivo and in-silico with the MIMIC-III waveform database and critically discuss how cardiovascular simulations can inform real-world data analysis.
</details>
<details>
<summary>摘要</summary>
Inspired by advances in simulation-based inference (SBI), we approach this inverse problem as a statistical inference problem. Unlike other methods, SBI provides a distribution of posterior probabilities for the parameters of interest, offering a comprehensive and multi-dimensional representation of uncertainty for individual measurements.We demonstrate the power of SBI by performing an in-silico uncertainty analysis of five biomarkers of clinical interest using different measurement modalities. Our study not only confirms established findings, such as the feasibility of estimating heart rate, but also highlights the potential of estimating new biomarkers from standard-of-care measurements.SBI reveals practical insights that cannot be obtained through standard sensitivity analyses, such as the existence of sub-populations with distinct uncertainty regimes. Furthermore, we explore the gap between in-vivo and in-silico using the MIMIC-III waveform database and discuss how cardiovascular simulations can inform real-world data analysis.
</details></li>
</ul>
<hr>
<h2 id="BayesDAG-Gradient-Based-Posterior-Sampling-for-Causal-Discovery"><a href="#BayesDAG-Gradient-Based-Posterior-Sampling-for-Causal-Discovery" class="headerlink" title="BayesDAG: Gradient-Based Posterior Sampling for Causal Discovery"></a>BayesDAG: Gradient-Based Posterior Sampling for Causal Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13917">http://arxiv.org/abs/2307.13917</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yashas Annadani, Nick Pawlowski, Joel Jennings, Stefan Bauer, Cheng Zhang, Wenbo Gong</li>
<li>for:  This paper aims to develop a scalable Bayesian causal discovery framework for inferring the posterior distribution over causal models from observed data, addressing computational challenges in joint inference over combinatorial space of Directed Acyclic Graphs (DAGs) and nonlinear functions.</li>
<li>methods:  The proposed framework uses stochastic gradient Markov Chain Monte Carlo (SG-MCMC) to directly sample DAGs from the posterior without requiring any DAG regularization, simultaneously drawing function parameter samples and applicable to both linear and nonlinear causal models.</li>
<li>results:  Empirical evaluations on synthetic and real-world datasets demonstrate the effectiveness of the proposed approach compared to state-of-the-art baselines.<details>
<summary>Abstract</summary>
Bayesian causal discovery aims to infer the posterior distribution over causal models from observed data, quantifying epistemic uncertainty and benefiting downstream tasks. However, computational challenges arise due to joint inference over combinatorial space of Directed Acyclic Graphs (DAGs) and nonlinear functions. Despite recent progress towards efficient posterior inference over DAGs, existing methods are either limited to variational inference on node permutation matrices for linear causal models, leading to compromised inference accuracy, or continuous relaxation of adjacency matrices constrained by a DAG regularizer, which cannot ensure resulting graphs are DAGs. In this work, we introduce a scalable Bayesian causal discovery framework based on stochastic gradient Markov Chain Monte Carlo (SG-MCMC) that overcomes these limitations. Our approach directly samples DAGs from the posterior without requiring any DAG regularization, simultaneously draws function parameter samples and is applicable to both linear and nonlinear causal models. To enable our approach, we derive a novel equivalence to the permutation-based DAG learning, which opens up possibilities of using any relaxed gradient estimator defined over permutations. To our knowledge, this is the first framework applying gradient-based MCMC sampling for causal discovery. Empirical evaluations on synthetic and real-world datasets demonstrate our approach's effectiveness compared to state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
bayesian causal discovery aimed to infer the posterior distribution over causal models from observed data, quantifying epistemic uncertainty and benefiting downstream tasks. However, computational challenges arise due to joint inference over combinatorial space of Directed Acyclic Graphs (DAGs) and nonlinear functions. Despite recent progress towards efficient posterior inference over DAGs, existing methods are either limited to variational inference on node permutation matrices for linear causal models, leading to compromised inference accuracy, or continuous relaxation of adjacency matrices constrained by a DAG regularizer, which cannot ensure resulting graphs are DAGs. In this work, we introduce a scalable Bayesian causal discovery framework based on stochastic gradient Markov Chain Monte Carlo (SG-MCMC) that overcomes these limitations. Our approach directly samples DAGs from the posterior without requiring any DAG regularization, simultaneously draws function parameter samples and is applicable to both linear and nonlinear causal models. To enable our approach, we derive a novel equivalence to the permutation-based DAG learning, which opens up possibilities of using any relaxed gradient estimator defined over permutations. To our knowledge, this is the first framework applying gradient-based MCMC sampling for causal discovery. Empirical evaluations on synthetic and real-world datasets demonstrate our approach's effectiveness compared to state-of-the-art baselines.
</details></li>
</ul>
<hr>
<h2 id="Online-learning-in-bandits-with-predicted-context"><a href="#Online-learning-in-bandits-with-predicted-context" class="headerlink" title="Online learning in bandits with predicted context"></a>Online learning in bandits with predicted context</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13916">http://arxiv.org/abs/2307.13916</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongyi Guo, Susan Murphy</li>
<li>for:  solves the contextual bandit problem with non-diminishing context error.</li>
<li>methods:  uses an extension of the measurement error model in classical statistics to the online decision-making setting.</li>
<li>results:  achieves sublinear regret compared to the appropriate benchmark, despite the non-diminishing context error.<details>
<summary>Abstract</summary>
We consider the contextual bandit problem where at each time, the agent only has access to a noisy version of the context and the error variance (or an estimator of this variance). This setting is motivated by a wide range of applications where the true context for decision-making is unobserved, and only a prediction of the context by a potentially complex machine learning algorithm is available. When the context error is non-diminishing, classical bandit algorithms fail to achieve sublinear regret. We propose the first online algorithm in this setting with sublinear regret compared to the appropriate benchmark. The key idea is to extend the measurement error model in classical statistics to the online decision-making setting, which is nontrivial due to the policy being dependent on the noisy context observations.
</details>
<details>
<summary>摘要</summary>
我们考虑了上下文搅拌问题，在每个时间点，代理人只有访问不准确的上下文和错误方差（或一个估计这种方差）的权限。这种设定是由各种应用领域中真实的上下文决策不可见，只有一个复杂机器学习算法预测的上下文预测所 inspirited。当上下文错误不断增长时， klasik bandit算法无法实现子线性 regret。我们提议的首个在这种设定下的在线算法具有子线性 regret，相对于合适的 benchmark。关键思想是将经典统计中的测量错误模型扩展到在线决策设定中，这是因为政策对于听到的不准确上下文观察有依赖关系。
</details></li>
</ul>
<hr>
<h2 id="Graph-Neural-Networks-based-Hybrid-Framework-For-Predicting-Particle-Crushing-Strength"><a href="#Graph-Neural-Networks-based-Hybrid-Framework-For-Predicting-Particle-Crushing-Strength" class="headerlink" title="Graph Neural Networks-based Hybrid Framework For Predicting Particle Crushing Strength"></a>Graph Neural Networks-based Hybrid Framework For Predicting Particle Crushing Strength</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13909">http://arxiv.org/abs/2307.13909</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/doujiang-zheng/gnn-for-particle-crushing">https://github.com/doujiang-zheng/gnn-for-particle-crushing</a></li>
<li>paper_authors: Tongya Zheng, Tianli Zhang, Qingzheng Guan, Wenjie Huang, Zunlei Feng, Mingli Song, Chun Chen</li>
<li>for:  This paper aims to apply Graph Neural Networks (GNNs) to model the mechanical behaviors of particle crushing and predict particle crushing strength.</li>
<li>methods: The authors use a hybrid framework based on GNNs to predict particle crushing strength in a particle fragment view, and compare their method against traditional machine learning methods and a plain Multi-Layer Perceptron (MLP).</li>
<li>results: The authors verify the effectiveness of their hybrid framework through numerical simulations and discuss the usefulness of different features through gradient attribution explanation.Here is the same information in Simplified Chinese:</li>
<li>for: 这篇论文目的是应用图гра树神经网络（GNNs）来模型粉体压碎的机械行为并预测粉体压碎强度。</li>
<li>methods: 作者使用一种混合方法基于GNNs来预测粉体压碎强度在粉体Fragment视图中，并与传统机器学习方法和简单的多层感知网络（MLP）进行比较。</li>
<li>results: 作者通过数值仿真 verify了他们的混合方法的有效性，并通过Gradient attribute解释来评估不同特征的用于预测。<details>
<summary>Abstract</summary>
Graph Neural Networks have emerged as an effective machine learning tool for multi-disciplinary tasks such as pharmaceutical molecule classification and chemical reaction prediction, because they can model non-euclidean relationships between different entities. Particle crushing, as a significant field of civil engineering, describes the breakage of granular materials caused by the breakage of particle fragment bonds under the modeling of numerical simulations, which motivates us to characterize the mechanical behaviors of particle crushing through the connectivity of particle fragments with Graph Neural Networks (GNNs). However, there lacks an open-source large-scale particle crushing dataset for research due to the expensive costs of laboratory tests or numerical simulations. Therefore, we firstly generate a dataset with 45,000 numerical simulations and 900 particle types to facilitate the research progress of machine learning for particle crushing. Secondly, we devise a hybrid framework based on GNNs to predict particle crushing strength in a particle fragment view with the advances of state of the art GNNs. Finally, we compare our hybrid framework against traditional machine learning methods and the plain MLP to verify its effectiveness. The usefulness of different features is further discussed through the gradient attribution explanation w.r.t the predictions. Our data and code are released at https://github.com/doujiang-zheng/GNN-For-Particle-Crushing.
</details>
<details>
<summary>摘要</summary>
Graph Neural Networks (GNNs) 已成为多学科领域中有效的机器学习工具，如药品分类和化学反应预测，因为它们可以模型不同实体之间的非欧几何关系。在 грануляр材料破碎中， particle crushing 是一个重要的领域，描述了由 particle fragment 键结的破碎物质的破碎过程，这种情况激发我们通过 GNNs 来描述破碎物质的机械行为。然而，由于实验室试验或数值仿真的高昂成本，在这个领域中没有公开的大规模 particle crushing 数据集，这限制了研究的进步。因此，我们首先生成了一个包含 45,000 个数值仿真和 900 种 particle type 的数据集，以便促进机器学习在 particle crushing 中的研究进步。其次，我们提出了基于 GNNs 的混合框架，用于预测 particle crushing 强度在 particle fragment 视图中。最后，我们与传统机器学习方法和简单的多层感知网络进行比较，以验证我们的混合框架的有效性。此外，我们还对不同特征的使用进行了探索，并通过对预测结果的梯度评估来进行解释。我们的数据和代码在 GitHub 上发布，请参考 https://github.com/doujiang-zheng/GNN-For-Particle-Crushing。
</details></li>
</ul>
<hr>
<h2 id="Robustness-Verification-of-Deep-Neural-Networks-using-Star-Based-Reachability-Analysis-with-Variable-Length-Time-Series-Input"><a href="#Robustness-Verification-of-Deep-Neural-Networks-using-Star-Based-Reachability-Analysis-with-Variable-Length-Time-Series-Input" class="headerlink" title="Robustness Verification of Deep Neural Networks using Star-Based Reachability Analysis with Variable-Length Time Series Input"></a>Robustness Verification of Deep Neural Networks using Star-Based Reachability Analysis with Variable-Length Time Series Input</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13907">http://arxiv.org/abs/2307.13907</a></li>
<li>repo_url: None</li>
<li>paper_authors: Neelanjana Pal, Diego Manzanas Lopez, Taylor T Johnson</li>
<li>for: 这篇论文的目的是提出一种基于神经网络的时间序列数据分析方法，以便在实际应用中进行精准的预测和维护。</li>
<li>methods: 这篇论文使用了时间序列 regression neural network (TSRegNN)，并使用了变量长度输入数据来简化输入处理和提高网络架构的通用性。</li>
<li>results: 该论文通过使用星形可达性分析和一些性能指标来检验神经网络的可靠性，并证明了神经网络在真实应用中的精准预测和可靠性是受到输入噪声的影响的。<details>
<summary>Abstract</summary>
Data-driven, neural network (NN) based anomaly detection and predictive maintenance are emerging research areas. NN-based analytics of time-series data offer valuable insights into past behaviors and estimates of critical parameters like remaining useful life (RUL) of equipment and state-of-charge (SOC) of batteries. However, input time series data can be exposed to intentional or unintentional noise when passing through sensors, necessitating robust validation and verification of these NNs. This paper presents a case study of the robustness verification approach for time series regression NNs (TSRegNN) using set-based formal methods. It focuses on utilizing variable-length input data to streamline input manipulation and enhance network architecture generalizability. The method is applied to two data sets in the Prognostics and Health Management (PHM) application areas: (1) SOC estimation of a Lithium-ion battery and (2) RUL estimation of a turbine engine. The NNs' robustness is checked using star-based reachability analysis, and several performance measures evaluate the effect of bounded perturbations in the input on network outputs, i.e., future outcomes. Overall, the paper offers a comprehensive case study for validating and verifying NN-based analytics of time-series data in real-world applications, emphasizing the importance of robustness testing for accurate and reliable predictions, especially considering the impact of noise on future outcomes.
</details>
<details>
<summary>摘要</summary>
数据驱动、基于神经网络（NN）的异常检测和预测维护是当前的研究领域之一。NN基础的时间序列数据分析可以为过去行为提供有价值的洞察，包括设备的剩余有用生命（RUL）和电池的状态充电（SOC）等重要参数的估计。然而，输入时间序列数据可能会受到意外或非意外的噪声的影响，因此需要对这些NN进行robust验证和验证。本文介绍了一种基于集合形式方法的NN验证方法，旨在使用可变长度的输入数据来简化输入处理并提高网络架构的通用性。这种方法在两个PHM应用领域的数据集上进行了应用：（1）锂离子电池SOC估计和（2）涡轮机RUL估计。通过星形可达性分析来检查NN的 Robustness，并使用一些性能指标来评估输入噪声的影响 på network输出，即未来的结果。总的来说，本文提供了一个完整的实践案例，用于验证和验证基于时间序列数据的NN分析，强调验证过程中噪声的影响，以确保准确可靠的预测，特别是在噪声的情况下。
</details></li>
</ul>
<hr>
<h2 id="Corruption-Robust-Lipschitz-Contextual-Search"><a href="#Corruption-Robust-Lipschitz-Contextual-Search" class="headerlink" title="Corruption-Robust Lipschitz Contextual Search"></a>Corruption-Robust Lipschitz Contextual Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13903">http://arxiv.org/abs/2307.13903</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiliang Zuo</li>
<li>for: 学习一个 lipschitz 函数，对于随机选择的上下文向量 $x_t$ 和 adversary 选择的真实函数值 $f(x_t)$ 进行推断。</li>
<li>methods: 使用 natural yet powerful technique sanity check，并设计了 robust 算法，可以在 $C$ 轮游戏中减少总损失。</li>
<li>results: 对于均匀损失， learner 可以取得 regret $O(C\log T)$，其中 $d &#x3D; 1$ 时为 $O(C\log T)$，$d &gt; 1$ 时为 $O_d(C\log T + T^{(d-1)&#x2F;d})$。对于价格损失，learner 可以取得 regret $\widetilde{O}(T^{d&#x2F;(d+1)} + C\cdot T^{1&#x2F;(d+1)})$。<details>
<summary>Abstract</summary>
I study the problem of learning a Lipschitz function with corrupted binary signals. The learner tries to learn a Lipschitz function $f$ that the adversary chooses. In each round, the adversary selects a context vector $x_t$ in the input space, and the learner makes a guess to the true function value $f(x_t)$ and receives a binary signal indicating whether the guess was high or low. In a total of $C$ rounds, the signal may be corrupted, though the value of $C$ is unknown to the learner. The learner's goal is to incur a small cumulative loss. I present a natural yet powerful technique sanity check, which proves useful in designing corruption-robust algorithms. I design algorithms which (treating the Lipschitz parameter $L$ as constant): for the symmetric loss, the learner achieves regret $O(C\log T)$ with $d = 1$ and $O_d(C\log T + T^{(d-1)/d})$ with $d > 1$; for the pricing loss the learner achieves regret $\widetilde{O} (T^{d/(d+1)} + C\cdot T^{1/(d+1)})$.
</details>
<details>
<summary>摘要</summary>
我研究学习受损函数问题，即 adversary 选择函数 $f$，learner 根据输入空间中的 context vector $x_t$ 猜测函数值，并接受一个 binary signal 表示猜测是高或低。在总共 $C$ 轮中，signal 可能受损，但 learner 不知道 $C$ 的值。learner 的目标是减少总的损失。我提出了一种自然强大的检查技术，对于设计受损函数 algorithms 非常有用。我设计了 algorithms，对于对称损失函数，learner 可以达到 regret $O(C\log T)$  avec $d = 1$ 和 $O_d(C\log T + T^{(d-1)/d})$  avec $d > 1$ ;对于价格损失函数，learner 可以达到 regret $\widetilde{O} (T^{d/(d+1)} + C\cdot T^{1/(d+1)})$。
</details></li>
</ul>
<hr>
<h2 id="Regularizing-Neural-Networks-with-Meta-Learning-Generative-Models"><a href="#Regularizing-Neural-Networks-with-Meta-Learning-Generative-Models" class="headerlink" title="Regularizing Neural Networks with Meta-Learning Generative Models"></a>Regularizing Neural Networks with Meta-Learning Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13899">http://arxiv.org/abs/2307.13899</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shin’ya Yamaguchi, Daiki Chijiwa, Sekitoshi Kanai, Atsutoshi Kumagai, Hisashi Kashima</li>
<li>for: 提高深度学习中的生成数据增强（Generative Data Augmentation，GDA）的方法，以便在小样本大小的情况下提高分类精度。</li>
<li>methods: 提出了一种新的生成数据增强策略——元生成准则（Meta Generative Regularization，MGR），通过在特征提取器中使用生成样本来减少Validation损失，从而避免生成数据增强导致的性能下降。</li>
<li>results: 对六个预测集进行了实验，发现MGR可以避免生成数据增强导致的性能下降，并在小样本大小的情况下稳定地超越基elines。<details>
<summary>Abstract</summary>
This paper investigates methods for improving generative data augmentation for deep learning. Generative data augmentation leverages the synthetic samples produced by generative models as an additional dataset for classification with small dataset settings. A key challenge of generative data augmentation is that the synthetic data contain uninformative samples that degrade accuracy. This is because the synthetic samples do not perfectly represent class categories in real data and uniform sampling does not necessarily provide useful samples for tasks. In this paper, we present a novel strategy for generative data augmentation called meta generative regularization (MGR). To avoid the degradation of generative data augmentation, MGR utilizes synthetic samples in the regularization term for feature extractors instead of in the loss function, e.g., cross-entropy. These synthetic samples are dynamically determined to minimize the validation losses through meta-learning. We observed that MGR can avoid the performance degradation of na\"ive generative data augmentation and boost the baselines. Experiments on six datasets showed that MGR is effective particularly when datasets are smaller and stably outperforms baselines.
</details>
<details>
<summary>摘要</summary>
Instead of using synthetic samples in the loss function, such as cross-entropy, MGR incorporates them into the regularization term for feature extractors. This approach allows for the dynamic determination of synthetic samples to minimize validation losses through meta-learning. Our experiments on six datasets demonstrated that MGR can effectively avoid the performance degradation of traditional generative data augmentation and consistently outperform baselines, particularly when the datasets are smaller.
</details></li>
</ul>
<hr>
<h2 id="Efficient-Estimation-of-the-Local-Robustness-of-Machine-Learning-Models"><a href="#Efficient-Estimation-of-the-Local-Robustness-of-Machine-Learning-Models" class="headerlink" title="Efficient Estimation of the Local Robustness of Machine Learning Models"></a>Efficient Estimation of the Local Robustness of Machine Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13885">http://arxiv.org/abs/2307.13885</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tessa Han, Suraj Srinivas, Himabindu Lakkaraju</li>
<li>for: 本研究旨在提高机器学习模型对噪声输入数据的Robustness。</li>
<li>methods: 本文提出了首个分析性Estimators来效率地计算多类推论模型的本地Robustness，通过地方线性函数近似和多变量Normal CDF，并与随机抖动、软max概率之间的关系进行链接。</li>
<li>results: 本文confirm empirically这些Estimators可以有效和高效地计算标准深度学习模型的本地Robustness，并用于多种任务，如测试模型的Robustness偏见和找到数据集中噪声扰动的例子。<details>
<summary>Abstract</summary>
Machine learning models often need to be robust to noisy input data. The effect of real-world noise (which is often random) on model predictions is captured by a model's local robustness, i.e., the consistency of model predictions in a local region around an input. However, the na\"ive approach to computing local robustness based on Monte-Carlo sampling is statistically inefficient, leading to prohibitive computational costs for large-scale applications. In this work, we develop the first analytical estimators to efficiently compute local robustness of multi-class discriminative models using local linear function approximation and the multivariate Normal CDF. Through the derivation of these estimators, we show how local robustness is connected to concepts such as randomized smoothing and softmax probability. We also confirm empirically that these estimators accurately and efficiently compute the local robustness of standard deep learning models. In addition, we demonstrate these estimators' usefulness for various tasks involving local robustness, such as measuring robustness bias and identifying examples that are vulnerable to noise perturbation in a dataset. By developing these analytical estimators, this work not only advances conceptual understanding of local robustness, but also makes its computation practical, enabling the use of local robustness in critical downstream applications.
</details>
<details>
<summary>摘要</summary>
（Machine learning模型经常需要对噪音输入数据具有鲁棒性。噪音的影响在模型预测中被捕捉在一个本地区域中，即模型在输入附近的一致性。但是，使用 Monte-Carlo 采样来计算本地鲁棒性的方法是统计不有效的，会导致大规模应用程序的计算成本过高。在这种情况下，我们开发了第一个分析式估计器，可以有效地计算多类推论模型的本地鲁棒性。我们使用本地线性函数近似和多变量正态分布函数来 derive这些估计器，并证明了本地鲁棒性与随机缓和软max概率之间的关系。我们还通过实验证明了这些估计器可以准确地和高效地计算标准深度学习模型的本地鲁棒性。此外，我们还示出了这些估计器在不同任务中的有用性，如测量鲁棒性偏见和 dataset 中噪音扰动的示例。通过开发这些分析式估计器，这些研究不仅提高了本地鲁棒性的概念理解，还使其计算变得实际可行，使其在重要的下游应用中使用。）
</details></li>
</ul>
<hr>
<h2 id="ExeDec-Execution-Decomposition-for-Compositional-Generalization-in-Neural-Program-Synthesis"><a href="#ExeDec-Execution-Decomposition-for-Compositional-Generalization-in-Neural-Program-Synthesis" class="headerlink" title="ExeDec: Execution Decomposition for Compositional Generalization in Neural Program Synthesis"></a>ExeDec: Execution Decomposition for Compositional Generalization in Neural Program Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13883">http://arxiv.org/abs/2307.13883</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kensen Shi, Joey Hong, Manzil Zaheer, Pengcheng Yin, Charles Sutton</li>
<li>for: 本研究旨在探讨人工智能程序生成方法是否具有分解复杂任务为 simpler subtask 的能力，以及这种能力是否可以推广到更复杂的任务。</li>
<li>methods: 本研究使用了多种形式的 compositional generalization，包括程序执行目标预测、程序分解和重构等，以形成一个 meta-benchmark，用于评估不同方法的总体性能。</li>
<li>results: 研究发现，使用 ExeDec  decomposition-based 程序生成策略可以更好地满足不同任务的需求，并且具有显著的总体性能和 compositional generalization 能力，比基eline方法更好。<details>
<summary>Abstract</summary>
When writing programs, people have the ability to tackle a new complex task by decomposing it into smaller and more familiar subtasks. While it is difficult to measure whether neural program synthesis methods have similar capabilities, we can measure whether they compositionally generalize, that is, whether a model that has been trained on the simpler subtasks is subsequently able to solve more complex tasks. In this paper, we characterize several different forms of compositional generalization that are desirable in program synthesis, forming a meta-benchmark which we use to create generalization tasks for two popular datasets, RobustFill and DeepCoder. We then propose ExeDec, a novel decomposition-based synthesis strategy that predicts execution subgoals to solve problems step-by-step informed by program execution at each step. ExeDec has better synthesis performance and greatly improved compositional generalization ability compared to baselines.
</details>
<details>
<summary>摘要</summary>
当编写程序时，人们有能力将复杂任务拆分成更熟悉的子任务。虽然无法测量神经程序合成方法的类似能力，但我们可以测量它们是否可以扩展，即一个已经在更简单的子任务上训练的模型是否可以解决更复杂的任务。在这篇论文中，我们描述了几种不同的拆分总结的形式，这些形式是程序合成中的总结，我们使用这些形式创建了一个元benchmark，并用这个元benchmark来创建了RobustFill和DeepCoder两个 популяр的数据集的通用化任务。然后，我们提出了ExeDec，一种新的分解基本的合成策略，该策略预测执行子任务以解决问题步骤通过程序执行的信息。ExeDec的合成性能和拆分总结能力都比基eline要好。
</details></li>
</ul>
<hr>
<h2 id="Good-Lattice-Training-Physics-Informed-Neural-Networks-Accelerated-by-Number-Theory"><a href="#Good-Lattice-Training-Physics-Informed-Neural-Networks-Accelerated-by-Number-Theory" class="headerlink" title="Good Lattice Training: Physics-Informed Neural Networks Accelerated by Number Theory"></a>Good Lattice Training: Physics-Informed Neural Networks Accelerated by Number Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13869">http://arxiv.org/abs/2307.13869</a></li>
<li>repo_url: None</li>
<li>paper_authors: Takashi Matsubara, Takaharu Yaguchi</li>
<li>for: 解决 partial differential equations (PDEs) 的novel和高效的方法</li>
<li>methods: 使用 physics-informed loss 训练神经网络，并选择合适的 collocation points</li>
<li>results: 提出 good lattice training (GLT) 技术，可以在小量的 collocation points 下达到竞争力的性能，并且比uniformly random sampling或Latin hypercube sampling 更有效率<details>
<summary>Abstract</summary>
Physics-informed neural networks (PINNs) offer a novel and efficient approach to solving partial differential equations (PDEs). Their success lies in the physics-informed loss, which trains a neural network to satisfy a given PDE at specific points and to approximate the solution. However, the solutions to PDEs are inherently infinite-dimensional, and the distance between the output and the solution is defined by an integral over the domain. Therefore, the physics-informed loss only provides a finite approximation, and selecting appropriate collocation points becomes crucial to suppress the discretization errors, although this aspect has often been overlooked. In this paper, we propose a new technique called good lattice training (GLT) for PINNs, inspired by number theoretic methods for numerical analysis. GLT offers a set of collocation points that are effective even with a small number of points and for multi-dimensional spaces. Our experiments demonstrate that GLT requires 2--20 times fewer collocation points (resulting in lower computational cost) than uniformly random sampling or Latin hypercube sampling, while achieving competitive performance.
</details>
<details>
<summary>摘要</summary>
物理学 Informed neural networks (PINNs) 提供了一种新的和高效的方法来解决部分 differential equations (PDEs)。它们的成功归功于物理学 Informed loss，该loss 训练一个神经网络满足给定 PDE 的特定点和近似解。然而，解决 PDE 的解是自然 infinite-dimensional，而且Distance  между输出和解是通过Domain 上的积分来定义的。因此，物理学 Informed loss 只提供了finite approximation，并且选择合适的拓扑点变得非常重要，以抑制精度损失，尽管这一点经常被忽略。在这篇论文中，我们提出了一种新的技术called good lattice training (GLT) for PINNs， inspirited by numerical analysis 的数学方法。GLT 提供了一组高效的拓扑点，可以在小量点和多维空间中实现高效。我们的实验表明，GLT 需要2--20倍 fewer collocation points（相对于随机抽样或拉丁hypercube sampling），而且可以达到竞争性的性能。
</details></li>
</ul>
<hr>
<h2 id="Learning-sources-of-variability-from-high-dimensional-observational-studies"><a href="#Learning-sources-of-variability-from-high-dimensional-observational-studies" class="headerlink" title="Learning sources of variability from high-dimensional observational studies"></a>Learning sources of variability from high-dimensional observational studies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13868">http://arxiv.org/abs/2307.13868</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ebridge2/cdcorr">https://github.com/ebridge2/cdcorr</a></li>
<li>paper_authors: Eric W. Bridgeford, Jaewon Chung, Brian Gilbert, Sambit Panda, Adam Li, Cencheng Shen, Alexandra Badea, Brian Caffo, Joshua T. Vogelstein</li>
<li>for: 该研究探讨了 causal inference 是否会影响观察到的结果，以及这种方法在各种生物领域中的应用，包括疫苗和药物开发、政策干预等。</li>
<li>methods: 该研究扩展了 causal estimands 到多维或任意可测量空间上的结果，并将 nominal 变量的 causal estimands 转化为 causal discrepancy tests。提出了一种简单的方法来调整 Conditional Independence tests，并证明这些测试是 universally consistent causal discrepancy tests。</li>
<li>results: 数据实验表明，该方法（Causal CDcorr）在样本数量和权限下的效果和力度都有所提高，相比之下存在的其他策略。该方法的代码都是开源的，可以在 github.com&#x2F;ebridge2&#x2F;cdcorr 上下载。<details>
<summary>Abstract</summary>
Causal inference studies whether the presence of a variable influences an observed outcome. As measured by quantities such as the "average treatment effect," this paradigm is employed across numerous biological fields, from vaccine and drug development to policy interventions. Unfortunately, the majority of these methods are often limited to univariate outcomes. Our work generalizes causal estimands to outcomes with any number of dimensions or any measurable space, and formulates traditional causal estimands for nominal variables as causal discrepancy tests. We propose a simple technique for adjusting universally consistent conditional independence tests and prove that these tests are universally consistent causal discrepancy tests. Numerical experiments illustrate that our method, Causal CDcorr, leads to improvements in both finite sample validity and power when compared to existing strategies. Our methods are all open source and available at github.com/ebridge2/cdcorr.
</details>
<details>
<summary>摘要</summary>
causal inference studies whether a variable's presence affects an observed outcome. as measured by quantities such as the "average treatment effect," this paradigm is employed across numerous biological fields, from vaccine and drug development to policy interventions. unfortunately, the majority of these methods are often limited to univariate outcomes. our work generalizes causal estimands to outcomes with any number of dimensions or any measurable space, and formulates traditional causal estimands for nominal variables as causal discrepancy tests. we propose a simple technique for adjusting universally consistent conditional independence tests and prove that these tests are universally consistent causal discrepancy tests. numerical experiments illustrate that our method, causal CDcorr, leads to improvements in both finite sample validity and power when compared to existing strategies. our methods are all open source and available at github.com/ebridge2/cdcorr.Here's the translation in Traditional Chinese:causal inference studies whether a variable's presence affects an observed outcome. as measured by quantities such as the "average treatment effect," this paradigm is employed across numerous biological fields, from vaccine and drug development to policy interventions. unfortunately, the majority of these methods are often limited to univariate outcomes. our work generalizes causal estimands to outcomes with any number of dimensions or any measurable space, and formulates traditional causal estimands for nominal variables as causal discrepancy tests. we propose a simple technique for adjusting universally consistent conditional independence tests and prove that these tests are universally consistent causal discrepancy tests. numerical experiments illustrate that our method, causal CDcorr, leads to improvements in both finite sample validity and power when compared to existing strategies. our methods are all open source and available at github.com/ebridge2/cdcorr.
</details></li>
</ul>
<hr>
<h2 id="Pretrained-Deep-2-5D-Models-for-Efficient-Predictive-Modeling-from-Retinal-OCT"><a href="#Pretrained-Deep-2-5D-Models-for-Efficient-Predictive-Modeling-from-Retinal-OCT" class="headerlink" title="Pretrained Deep 2.5D Models for Efficient Predictive Modeling from Retinal OCT"></a>Pretrained Deep 2.5D Models for Efficient Predictive Modeling from Retinal OCT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13865">http://arxiv.org/abs/2307.13865</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taha Emre, Marzieh Oghbaie, Arunava Chakravarty, Antoine Rivail, Sophie Riedl, Julia Mai, Hendrik P. N. Scholl, Sobha Sivaprasad, Daniel Rueckert, Andrew Lotery, Ursula Schmidt-Erfurth, Hrvoje Bogunović</li>
<li>for: 预测老年眼肿症（AMD）进展，提高医疗影像识别的精度和效率。</li>
<li>methods: 组合2D和3D技术，使用卷积神经网络（CNN）、长短期记忆（LSTM）和变换器，提高性能和数据效率。</li>
<li>results: 在两个大 longitudinal OCT 数据集上，验证了这些架构和预训练方法的效果，可以准确预测在6个月内进展到湿性年轻眼肿症（AMD）。<details>
<summary>Abstract</summary>
In the field of medical imaging, 3D deep learning models play a crucial role in building powerful predictive models of disease progression. However, the size of these models presents significant challenges, both in terms of computational resources and data requirements. Moreover, achieving high-quality pretraining of 3D models proves to be even more challenging. To address these issues, hybrid 2.5D approaches provide an effective solution for utilizing 3D volumetric data efficiently using 2D models. Combining 2D and 3D techniques offers a promising avenue for optimizing performance while minimizing memory requirements. In this paper, we explore 2.5D architectures based on a combination of convolutional neural networks (CNNs), long short-term memory (LSTM), and Transformers. In addition, leveraging the benefits of recent non-contrastive pretraining approaches in 2D, we enhanced the performance and data efficiency of 2.5D techniques even further. We demonstrate the effectiveness of architectures and associated pretraining on a task of predicting progression to wet age-related macular degeneration (AMD) within a six-month period on two large longitudinal OCT datasets.
</details>
<details>
<summary>摘要</summary>
医疗影像领域中，3D深度学习模型在建立疾病发展预测模型方面扮演着关键角色。然而，这些模型的大小带来了计算资源和数据需求的挑战。此外，实现高质量预训练3D模型也是非常困难的。为解决这些问题，混合2.5D方法提供了一种高效地利用3D栅格数据的方法。将2D和3D技术结合起来，可以提高性能的同时减少内存需求。在这篇论文中，我们探讨了基于卷积神经网络（CNN）、长期短记忆（LSTM）和变换器的2.5D架构。此外，利用2D非对抗预训练方法的优点，我们进一步提高了2.5D技术的性能和数据效率。我们在两个大 longitudinal OCT数据集上预测了在6个月内进行湿性macular degeneration（AMD）的发展预测任务，以证明架构和预训练的效果。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Design-Analog-Circuits-to-Meet-Threshold-Specifications"><a href="#Learning-to-Design-Analog-Circuits-to-Meet-Threshold-Specifications" class="headerlink" title="Learning to Design Analog Circuits to Meet Threshold Specifications"></a>Learning to Design Analog Circuits to Meet Threshold Specifications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13861">http://arxiv.org/abs/2307.13861</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/indylab/circuit-synthesis">https://github.com/indylab/circuit-synthesis</a></li>
<li>paper_authors: Dmitrii Krylov, Pooya Khajeh, Junhan Ouyang, Thomas Reeves, Tongkai Liu, Hiba Ajmal, Hamidreza Aghasi, Roy Fox</li>
<li>for: 本研究旨在提出一种基于 simulation 数据的自动化Analog和广播频率电路设计方法，以替代专业设计师的手动设计。</li>
<li>methods: 该方法通过学习 inverse function 从 desired performance metrics 中学习出 circuit parameters。</li>
<li>results: 该方法可以在5% error margin 下达到90%的成功率，并且可以提高数据使用效率。In English:</li>
<li>for: The paper proposes an automated design method for analog and radio-frequency circuits using supervised or reinforcement learning from simulation data, as an alternative to manual expert design.</li>
<li>methods: The method learns an inverse function from desired performance metrics to circuit parameters.</li>
<li>results: The method achieves a success rate of over 90% with an error margin of 5%, and improves data efficiency by up to an order of magnitude.<details>
<summary>Abstract</summary>
Automated design of analog and radio-frequency circuits using supervised or reinforcement learning from simulation data has recently been studied as an alternative to manual expert design. It is straightforward for a design agent to learn an inverse function from desired performance metrics to circuit parameters. However, it is more common for a user to have threshold performance criteria rather than an exact target vector of feasible performance measures. In this work, we propose a method for generating from simulation data a dataset on which a system can be trained via supervised learning to design circuits to meet threshold specifications. We moreover perform the to-date most extensive evaluation of automated analog circuit design, including experimenting in a significantly more diverse set of circuits than in prior work, covering linear, nonlinear, and autonomous circuit configurations, and show that our method consistently reaches success rate better than 90% at 5% error margin, while also improving data efficiency by upward of an order of magnitude. A demo of this system is available at circuits.streamlit.app
</details>
<details>
<summary>摘要</summary>
自动设计分析和频率电路使用监督或增强学习法，从实验数据学习，对于专家设计来说是一种新的替代方案。它容易 для设计代理人从需求性能指标学习逆函数。但是，用户更常会有阈值性能标准而不是精确的可行性表现标准。在这个工作中，我们提出了将从实验数据生成一个可以透过监督学习训练系统，以满足阈值需求的数据集。我们还进行了过去最大的自动分析电路设计评估，包括在更加多样化的电路配置中实验，包括线性、非线性和自主电路配置，并证明了我们的方法可以在5% error margin下，实现90%的成功率，同时也提高了数据效率，提高了一个阶层。一个demo这个系统可以在circuits.streamlit.app中找到。
</details></li>
</ul>
<hr>
<h2 id="On-the-unreasonable-vulnerability-of-transformers-for-image-restoration-–-and-an-easy-fix"><a href="#On-the-unreasonable-vulnerability-of-transformers-for-image-restoration-–-and-an-easy-fix" class="headerlink" title="On the unreasonable vulnerability of transformers for image restoration – and an easy fix"></a>On the unreasonable vulnerability of transformers for image restoration – and an easy fix</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13856">http://arxiv.org/abs/2307.13856</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shashank Agnihotri, Kanchana Vaishnavi Gandikota, Julia Grabinski, Paramanand Chandramouli, Margret Keuper</li>
<li>for: 这种研究探讨了使用视Transformers（ViTs）进行图像修复任务中的图像修复模型的Robustness。</li>
<li>methods: 我们使用Projected Gradient Descent（PGD）和CosPGD，一种特定于像素预测任务的敏感攻击，来评估这些模型的Robustness。</li>
<li>results: 我们发现，与前期研究所 advocated的相反，这些模型在敏感攻击下高度易受攻击。我们通过对这些模型进行Robustness Training来提高其Robustness，但结果不太乐见。另外，NAFNet和Baseline网络的设计选择，基于iid性性能而不是Robust generalization，似乎与模型的Robustness相抵触。<details>
<summary>Abstract</summary>
Following their success in visual recognition tasks, Vision Transformers(ViTs) are being increasingly employed for image restoration. As a few recent works claim that ViTs for image classification also have better robustness properties, we investigate whether the improved adversarial robustness of ViTs extends to image restoration. We consider the recently proposed Restormer model, as well as NAFNet and the "Baseline network" which are both simplified versions of a Restormer. We use Projected Gradient Descent (PGD) and CosPGD, a recently proposed adversarial attack tailored to pixel-wise prediction tasks for our robustness evaluation. Our experiments are performed on real-world images from the GoPro dataset for image deblurring. Our analysis indicates that contrary to as advocated by ViTs in image classification works, these models are highly susceptible to adversarial attacks. We attempt to improve their robustness through adversarial training. While this yields a significant increase in robustness for Restormer, results on other networks are less promising. Interestingly, the design choices in NAFNet and Baselines, which were based on iid performance, and not on robust generalization, seem to be at odds with the model robustness. Thus, we investigate this further and find a fix.
</details>
<details>
<summary>摘要</summary>
“随着当前视觉变数任务的成功，视觉对映器（ViT）正在不断地被应用于图像修复。一些最近的研究表明，ViT在图像分类任务中也有更好的韧性特性，我们进一步探索这些韧性特性是否扩展到图像修复。我们考虑了RecentRestormer模型，以及NAFNet和基eline网络，这些都是Restormer的简化版本。我们使用预测向量的投影Gradient Descent（PGD）和CosPGD，这是一种特别针对像素精度预测任务的攻击方法来进行我们的Robustness评估。我们对GoPro图像滤过 dataset上的实际图像进行了实验。我们的分析表明，与在图像分类任务中所说的相反，这些模型在这些任务中具有强大的攻击敏感性。我们尝试通过对模型进行防御训练来改善其 Robustness。与此同时，我们发现NAFNet和基eline网络的设计决策，它们是基于独立性表现而不是关于韧性的设计决策，似乎与模型的韧性不匹配。因此，我们进一步探索这个问题，并发现一个解决方案。”
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Sharpened-Cosine-Similarity"><a href="#Exploring-the-Sharpened-Cosine-Similarity" class="headerlink" title="Exploring the Sharpened Cosine Similarity"></a>Exploring the Sharpened Cosine Similarity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13855">http://arxiv.org/abs/2307.13855</a></li>
<li>repo_url: None</li>
<li>paper_authors: Skyler Wu, Fred Lu, Edward Raff, James Holt</li>
<li>for: 本研究探讨了使用新的激活函数Sharpened Cosine Similarity（SCS） instead of传统的卷积层来进行图像分类。</li>
<li>methods: 本研究使用了多种 CNN 架构，并对 CIFAR-10 数据集进行了大规模的实验分析。</li>
<li>results: 研究发现，使用 SCS 可能不会提高准确率，但可能学习更易于理解的表示。此外，在某些情况下，SCS 可能会提高鲁棒性。<details>
<summary>Abstract</summary>
Convolutional layers have long served as the primary workhorse for image classification. Recently, an alternative to convolution was proposed using the Sharpened Cosine Similarity (SCS), which in theory may serve as a better feature detector. While multiple sources report promising results, there has not been to date a full-scale empirical analysis of neural network performance using these new layers. In our work, we explore SCS's parameter behavior and potential as a drop-in replacement for convolutions in multiple CNN architectures benchmarked on CIFAR-10. We find that while SCS may not yield significant increases in accuracy, it may learn more interpretable representations. We also find that, in some circumstances, SCS may confer a slight increase in adversarial robustness.
</details>
<details>
<summary>摘要</summary>
卷积层长期以来为图像分类任务中的主力工具。最近，一种使用加强的余弦相似性（SCS）来代替卷积的方案被提出，据说可能更好地检测特征。虽然多种来源报道了这些新层的批处结果，但到目前为止没有进行了全面的实验分析。在我们的工作中，我们探索SCS的参数行为和作为卷积Drop-in替换的潜在可能性，并在CIFAR-10上对多种CNN Architecture进行了多种测试。我们发现，虽然SCS可能不会导致显著增加准确率，但它可能学习更易于理解的表示。此外，在某些情况下，SCS可能会增加一定的逆向抗性。
</details></li>
</ul>
<hr>
<h2 id="WebArena-A-Realistic-Web-Environment-for-Building-Autonomous-Agents"><a href="#WebArena-A-Realistic-Web-Environment-for-Building-Autonomous-Agents" class="headerlink" title="WebArena: A Realistic Web Environment for Building Autonomous Agents"></a>WebArena: A Realistic Web Environment for Building Autonomous Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13854">http://arxiv.org/abs/2307.13854</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/web-arena-x/webarena">https://github.com/web-arena-x/webarena</a></li>
<li>paper_authors: Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, Graham Neubig</li>
<li>for: 这种研究旨在创建一个高度实际和可重现的自动化代理控制环境，以便用自然语言命令管理日常任务。</li>
<li>methods: 这篇论文使用了现代自然语言处理技术，如理智语言模型（GPT-4），以及一些最新的解释和行为决策技术。</li>
<li>results: 研究发现，使用现有的状态elia-of-the-art语言模型（GPT-4）解决复杂任务时存在挑战，最高终端任务成功率只有10.59%。这些结果表明需要进一步发展更加可靠的自动化代理，并且现有的语言模型在这些实际任务中的表现并不理想。<details>
<summary>Abstract</summary>
With generative AI advances, the exciting potential for autonomous agents to manage daily tasks via natural language commands has emerged. However, cur rent agents are primarily created and tested in simplified synthetic environments, substantially limiting real-world scenario representation. In this paper, we build an environment for agent command and control that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on websites, and we create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and are designed to emulate tasks that humans routinely perform on the internet. We design and implement several autonomous agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 10.59%. These results highlight the need for further development of robust agents, that current state-of-the-art LMs are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress. Our code, data, environment reproduction resources, and video demonstrations are publicly available at https://webarena.dev/.
</details>
<details>
<summary>摘要</summary>
“受到生成AI的推进，自动化代理人可以通过自然语言指令进行日常任务管理，这具有吸引人的潜力。然而，目前的代理人主要是在简化的人工环境中设计和测试，实际上仅仅代表了实际世界的一部分。在这篇论文中，我们建立了一个高度现实和可重现的环境，以便自动化代理人进行命令和控制。 Specifically，我们专注在网站上进行任务的代理人，并创建了四种常见的领域中的完整网站：电子商务、社群讨论论坛、协同软件开发和内容管理。我们的环境扩展了工具（例如地图）和外部知识库（例如用户手册），以促进人类化的任务解决。在这基础之上，我们发布了一组对任务完成的评估标准，这些任务具有多样性、长期性和模拟人类在网页上进行常规任务的特点。我们设计和实现了一些自动化代理人，应用最新的技术，例如理解才行。结果显示，解决复杂任务是具有挑战性：我们的最佳GPT-4基于代理人仅取得了10.59%的终端任务成功率。这些结果显示现代LM的表现仍有很大的改善空间，并且WebArena可以用来衡量这种进步。我们的代码、数据、环境重现资源和视频示例都公开 disponíveis于https://webarena.dev/.”
</details></li>
</ul>
<hr>
<h2 id="SplitFed-resilience-to-packet-loss-Where-to-split-that-is-the-question"><a href="#SplitFed-resilience-to-packet-loss-Where-to-split-that-is-the-question" class="headerlink" title="SplitFed resilience to packet loss: Where to split, that is the question"></a>SplitFed resilience to packet loss: Where to split, that is the question</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13851">http://arxiv.org/abs/2307.13851</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chamani Shiranthika, Zahra Hafezi Kafshgari, Parvaneh Saeedi, Ivan V. Bajić</li>
<li>for: 这篇论文探讨了Split Federated Learning（SplitFed或SFL）在面对包列失败的情况下的稳定性。</li>
<li>methods: 这篇论文使用了将模型分割在两个点上（浅分割和深分割），然后测试这些分割点对模型的准确率是否有 statistically significant difference。</li>
<li>results: 实验结果表明，使用深分割点可以获得更高的准确率。<details>
<summary>Abstract</summary>
Decentralized machine learning has broadened its scope recently with the invention of Federated Learning (FL), Split Learning (SL), and their hybrids like Split Federated Learning (SplitFed or SFL). The goal of SFL is to reduce the computational power required by each client in FL and parallelize SL while maintaining privacy. This paper investigates the robustness of SFL against packet loss on communication links. The performance of various SFL aggregation strategies is examined by splitting the model at two points -- shallow split and deep split -- and testing whether the split point makes a statistically significant difference to the accuracy of the final model. Experiments are carried out on a segmentation model for human embryo images and indicate the statistically significant advantage of a deeper split point.
</details>
<details>
<summary>摘要</summary>
《协同学习的扩展：从分布式学习到分布式 Federated Learning》Recently, decentralized machine learning has expanded its scope with the invention of Federated Learning (FL), Split Learning (SL), and their hybrids like Split Federated Learning (SplitFed or SFL). The goal of SFL is to reduce the computational power required by each client in FL and parallelize SL while maintaining privacy. This paper investigates the robustness of SFL against packet loss on communication links. The performance of various SFL aggregation strategies is examined by splitting the model at two points -- shallow split and deep split -- and testing whether the split point makes a statistically significant difference to the accuracy of the final model. Experiments are carried out on a segmentation model for human embryo images and indicate the statistically significant advantage of a deeper split point.
</details></li>
</ul>
<hr>
<h2 id="MAEA-Multimodal-Attribution-for-Embodied-AI"><a href="#MAEA-Multimodal-Attribution-for-Embodied-AI" class="headerlink" title="MAEA: Multimodal Attribution for Embodied AI"></a>MAEA: Multimodal Attribution for Embodied AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13850">http://arxiv.org/abs/2307.13850</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vidhi Jain, Jayant Sravan Tamarapalli, Sahiti Yerramilli, Yonatan Bisk<br>for: 这个论文的目的是解决embodied AI中的多modal感知问题，因为输入信息可能包含高度相互补充的信息。methods: 这篇论文使用了解释ALFRED数据集上不同策略中每种modal输入的贡献分析，以便理解每种modal输入的全局趋势。results: 该研究发现了一种名为MAEA的框架，可以计算任何可微分策略的全局贡献分析。此外，研究还显示了在EAI策略中语言和视觉贡献的下一个行为分析。<details>
<summary>Abstract</summary>
Understanding multimodal perception for embodied AI is an open question because such inputs may contain highly complementary as well as redundant information for the task. A relevant direction for multimodal policies is understanding the global trends of each modality at the fusion layer. To this end, we disentangle the attributions for visual, language, and previous action inputs across different policies trained on the ALFRED dataset. Attribution analysis can be utilized to rank and group the failure scenarios, investigate modeling and dataset biases, and critically analyze multimodal EAI policies for robustness and user trust before deployment. We present MAEA, a framework to compute global attributions per modality of any differentiable policy. In addition, we show how attributions enable lower-level behavior analysis in EAI policies for language and visual attributions.
</details>
<details>
<summary>摘要</summary>
（文本翻译）理解多模态识别对固有AI是一个开放的问题，因为输入可能包含高度相互补做的信息。一个有关的方向是理解不同模态的全球趋势在融合层。为此，我们分离不同策略在ALFRED数据集上训练的视觉、语言和前一个动作输入的归因。归因分析可以用来排序和分组失败场景，调查模型和数据集偏见，并对多模态EAI策略进行robustness和用户信任的检验。我们提出了MAEA框架，用于计算任何可导策略的全球归因。此外，我们还示出了归因如何帮助分析EAI策略的低级行为。
</details></li>
</ul>
<hr>
<h2 id="Relationship-between-Batch-Size-and-Number-of-Steps-Needed-for-Nonconvex-Optimization-of-Stochastic-Gradient-Descent-using-Armijo-Line-Search"><a href="#Relationship-between-Batch-Size-and-Number-of-Steps-Needed-for-Nonconvex-Optimization-of-Stochastic-Gradient-Descent-using-Armijo-Line-Search" class="headerlink" title="Relationship between Batch Size and Number of Steps Needed for Nonconvex Optimization of Stochastic Gradient Descent using Armijo Line Search"></a>Relationship between Batch Size and Number of Steps Needed for Nonconvex Optimization of Stochastic Gradient Descent using Armijo Line Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13831">http://arxiv.org/abs/2307.13831</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuki Tsukada, Hideaki Iiduka</li>
<li>for: 本研究探讨了使用Stochastic gradient descent（SGD）训练深度学习模型时，学习率是如何选择的。</li>
<li>methods: 本研究使用了Armijo线earch方法来选择学习率，并进行了非 convex 优化的收敛分析。</li>
<li>results: 研究发现，当批处理大小增加时，SGD 的训练步数逐渐减少，并且存在一个最优批处理大小，可以最小化 Stochastic first-order oracle（SFO）复杂度。同时， numerics 支持了这些理论结论。<details>
<summary>Abstract</summary>
Stochastic gradient descent (SGD) is the simplest deep learning optimizer with which to train deep neural networks. While SGD can use various learning rates, such as constant or diminishing rates, the previous numerical results showed that SGD performs better than other deep learning optimizers using when it uses learning rates given by line search methods. In this paper, we perform a convergence analysis on SGD with a learning rate given by an Armijo line search for nonconvex optimization. The analysis indicates that the upper bound of the expectation of the squared norm of the full gradient becomes small when the number of steps and the batch size are large. Next, we show that, for SGD with the Armijo-line-search learning rate, the number of steps needed for nonconvex optimization is a monotone decreasing convex function of the batch size; that is, the number of steps needed for nonconvex optimization decreases as the batch size increases. Furthermore, we show that the stochastic first-order oracle (SFO) complexity, which is the stochastic gradient computation cost, is a convex function of the batch size; that is, there exists a critical batch size that minimizes the SFO complexity. Finally, we provide numerical results that support our theoretical results. The numerical results indicate that the number of steps needed for training deep neural networks decreases as the batch size increases and that there exist the critical batch sizes that can be estimated from the theoretical results.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Offline-Reinforcement-Learning-with-On-Policy-Q-Function-Regularization"><a href="#Offline-Reinforcement-Learning-with-On-Policy-Q-Function-Regularization" class="headerlink" title="Offline Reinforcement Learning with On-Policy Q-Function Regularization"></a>Offline Reinforcement Learning with On-Policy Q-Function Regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13824">http://arxiv.org/abs/2307.13824</a></li>
<li>repo_url: None</li>
<li>paper_authors: Laixi Shi, Robert Dadashi, Yuejie Chi, Pablo Samuel Castro, Matthieu Geist</li>
<li>for: 本研究的目的是解决offline reinforcement learning中的扩展错误问题，即history dataset和期望策略之间的分布shift问题。</li>
<li>methods: 本研究使用Q函数regularization来解决扩展错误问题，而不是直接regularizing towards behavior policy。</li>
<li>results: 两个提议算法在D4RLbenchmark上表现出色，展示了强的性能。<details>
<summary>Abstract</summary>
The core challenge of offline reinforcement learning (RL) is dealing with the (potentially catastrophic) extrapolation error induced by the distribution shift between the history dataset and the desired policy. A large portion of prior work tackles this challenge by implicitly/explicitly regularizing the learning policy towards the behavior policy, which is hard to estimate reliably in practice. In this work, we propose to regularize towards the Q-function of the behavior policy instead of the behavior policy itself, under the premise that the Q-function can be estimated more reliably and easily by a SARSA-style estimate and handles the extrapolation error more straightforwardly. We propose two algorithms taking advantage of the estimated Q-function through regularizations, and demonstrate they exhibit strong performance on the D4RL benchmarks.
</details>
<details>
<summary>摘要</summary>
核心挑战是线上强化学习（RL）是处理由历史数据集和需要的策略之间的分布转移所引起的（可能Catastrophic）推理错误的。大多数先前的工作是通过直接/间接地规范学习策略向行为策略进行补做，这在实践中很难估算。在这种工作中，我们建议将规范向行为策略的Q函数进行补做，因为Q函数可以更加可靠地和容易地通过SARSA样式的估计来估计，并且更直观地处理推理错误。我们提出了两种利用估计Q函数的算法，并在D4RL标准各项目上展示它们的强大表现。
</details></li>
</ul>
<hr>
<h2 id="Fitting-Auditory-Filterbanks-with-Multiresolution-Neural-Networks"><a href="#Fitting-Auditory-Filterbanks-with-Multiresolution-Neural-Networks" class="headerlink" title="Fitting Auditory Filterbanks with Multiresolution Neural Networks"></a>Fitting Auditory Filterbanks with Multiresolution Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13821">http://arxiv.org/abs/2307.13821</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lostanlen/lostanlen2023waspaa">https://github.com/lostanlen/lostanlen2023waspaa</a></li>
<li>paper_authors: Vincent Lostanlen, Daniel Haider, Han Han, Mathieu Lagrange, Peter Balazs, Martin Ehler</li>
<li>for: 用于音频识别和分类 tasks</li>
<li>methods: 使用 multiresolution neural network (MuReNN)，具有分解的卷积操作符，以及知识填充（knowledge distillation，KD）技术</li>
<li>results: 与 state-of-the-art 比较，MuReNN 在 hold-out 集合上的好处 fit 和 Heisenberg 时域本地化方面达到了最佳性能。<details>
<summary>Abstract</summary>
Waveform-based deep learning faces a dilemma between nonparametric and parametric approaches. On one hand, convolutional neural networks (convnets) may approximate any linear time-invariant system; yet, in practice, their frequency responses become more irregular as their receptive fields grow. On the other hand, a parametric model such as LEAF is guaranteed to yield Gabor filters, hence an optimal time-frequency localization; yet, this strong inductive bias comes at the detriment of representational capacity. In this paper, we aim to overcome this dilemma by introducing a neural audio model, named multiresolution neural network (MuReNN). The key idea behind MuReNN is to train separate convolutional operators over the octave subbands of a discrete wavelet transform (DWT). Since the scale of DWT atoms grows exponentially between octaves, the receptive fields of the subsequent learnable convolutions in MuReNN are dilated accordingly. For a given real-world dataset, we fit the magnitude response of MuReNN to that of a well-established auditory filterbank: Gammatone for speech, CQT for music, and third-octave for urban sounds, respectively. This is a form of knowledge distillation (KD), in which the filterbank ''teacher'' is engineered by domain knowledge while the neural network ''student'' is optimized from data. We compare MuReNN to the state of the art in terms of goodness of fit after KD on a hold-out set and in terms of Heisenberg time-frequency localization. Compared to convnets and Gabor convolutions, we find that MuReNN reaches state-of-the-art performance on all three optimization problems.
</details>
<details>
<summary>摘要</summary>
文本形式的深度学习面临着非 Parametric 和 Parametric approaches 之间的矛盾。一方面，卷积神经网络（convnets）可以近似任何线性时间不变系统；然而，在实践中，它们的频谱响应会随着它们的触发区域增大而变得更加不规则。另一方面，一个 Parametric 模型如 LEAF 可以确保生成 Gabor 滤波器，因此获得最佳时间频域准确性；然而，这样强大的推导缺陷会导致表达能力受限。在这篇论文中，我们想要超越这个矛盾，我们提出了一种神经音频模型，即多尺度神经网络（MuReNN）。MuReNN 的关键思想是在 Octave 子域上分别训练独立的卷积操作。由于 DWT 原子的尺度在 Octave 上呈指数增长，MuReNN 中的后续学习可以通过扩展权重来进行扩展。对于一个真实世界数据集，我们将 MuReNN 的质量响应与一个已知的听觉滤波器 banks：Gammatone  для语音、CQT  для音乐和第三 Octave  для城市声音，分别进行适应。这是一种知识储存（KD），在哪里听觉滤波器 ''教师'' 是通过领域知识设计的，而神经网络 ''学生'' 是通过数据优化的。我们将 MuReNN 与现状最佳的方法进行比较，包括在 KD 中的准确性评价和 Heisenberg 时间频域本地化评价。相比 ConvNets 和 Gabor 卷积，我们发现 MuReNN 在三个优化问题上都达到了状态机器的性能。
</details></li>
</ul>
<hr>
<h2 id="Gradient-Based-Spectral-Embeddings-of-Random-Dot-Product-Graphs"><a href="#Gradient-Based-Spectral-Embeddings-of-Random-Dot-Product-Graphs" class="headerlink" title="Gradient-Based Spectral Embeddings of Random Dot Product Graphs"></a>Gradient-Based Spectral Embeddings of Random Dot Product Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13818">http://arxiv.org/abs/2307.13818</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/marfiori/efficient-ase">https://github.com/marfiori/efficient-ase</a></li>
<li>paper_authors: Marcelo Fiori, Bernardo Marenco, Federico Larroca, Paola Bermolen, Gonzalo Mateos</li>
<li>for: 这个论文的目的是提出一种基于非对映准则的图像学习框架，以解决图像 embeddings 问题。</li>
<li>methods: 论文使用了非对映准则优化方法，包括首页梯度下降法和梯度搅拌法，来解决图像 embeddings 问题。</li>
<li>results: 实验结果表明，该方法可以更好地解决图像 embeddings 问题，并且可以更好地处理流动图像和缺失边数据。<details>
<summary>Abstract</summary>
The Random Dot Product Graph (RDPG) is a generative model for relational data, where nodes are represented via latent vectors in low-dimensional Euclidean space. RDPGs crucially postulate that edge formation probabilities are given by the dot product of the corresponding latent positions. Accordingly, the embedding task of estimating these vectors from an observed graph is typically posed as a low-rank matrix factorization problem. The workhorse Adjacency Spectral Embedding (ASE) enjoys solid statistical properties, but it is formally solving a surrogate problem and can be computationally intensive. In this paper, we bring to bear recent advances in non-convex optimization and demonstrate their impact to RDPG inference. We advocate first-order gradient descent methods to better solve the embedding problem, and to organically accommodate broader network embedding applications of practical relevance. Notably, we argue that RDPG embeddings of directed graphs loose interpretability unless the factor matrices are constrained to have orthogonal columns. We thus develop a novel feasible optimization method in the resulting manifold. The effectiveness of the graph representation learning framework is demonstrated on reproducible experiments with both synthetic and real network data. Our open-source algorithm implementations are scalable, and unlike the ASE they are robust to missing edge data and can track slowly-varying latent positions from streaming graphs.
</details>
<details>
<summary>摘要</summary>
Random Dot Product Graph（RDPG）是一种生成模型，用于关系数据，其中节点被表示为低维欧几何空间中的latent vector。 RDPG假设边的形成概率为latent vector的点积。因此，从观察到的图像进行嵌入的任务通常是一个低维矩阵分解问题。ASE是工作马力，但它是一个代理问题，可能 computationally intensive。在这篇论文中，我们利用了最近的非极体优化技术，并证明它们对RDPG推理有益。我们建议使用首项梯度下降法来更好地解决嵌入问题，并能够自然地涵盖更广泛的网络嵌入应用。另外，我们 argue that RDPG嵌入导向图 Unless the factor matrices are constrained to have orthogonal columns，因此我们开发了一种新的可行优化方法。我们的图表示学术框架在 reproduceable experiments中得到了证明，并且我们的开源算法实现可扩展，不同于ASE，它们可以承受缺失边数据和从流式图进行逐步嵌入。
</details></li>
</ul>
<hr>
<h2 id="How-to-Scale-Your-EMA"><a href="#How-to-Scale-Your-EMA" class="headerlink" title="How to Scale Your EMA"></a>How to Scale Your EMA</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13813">http://arxiv.org/abs/2307.13813</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ZulqarnainZilli/-9-Email-Marketing-Tips-For-Content-Marketers">https://github.com/ZulqarnainZilli/-9-Email-Marketing-Tips-For-Content-Marketers</a></li>
<li>paper_authors: Dan Busbridge, Jason Ramapuram, Pierre Ablin, Tatiana Likhomanenko, Eeshan Gunesh Dhekane, Xavier Suau, Russ Webb</li>
<li>for: 本文旨在探讨如何在批处理大小不同时保持训练动力的问题。</li>
<li>methods: 本文提出了一种优化策略，即在EMA模型的存在下，采用线性增量法来调整学习率，以实现在批处理大小不同时保持训练动力。</li>
<li>results: 本文通过多种 arquitectures、优化器和数据模式的实验，证明了该优化策略的有效性。 另外，本文还示出了EMA模型在目标模型优化中的作用，并在小批处理和大批处理下实现了SSL方法的训练。 特别是，在BYOL方法中，通过适当调整学习率，在批处理大小为24576时实现了6倍的wall-clock时间减少。<details>
<summary>Abstract</summary>
Preserving training dynamics across batch sizes is an important tool for practical machine learning as it enables the trade-off between batch size and wall-clock time. This trade-off is typically enabled by a scaling rule, for example, in stochastic gradient descent, one should scale the learning rate linearly with the batch size. Another important tool for practical machine learning is the model Exponential Moving Average (EMA), which is a model copy that does not receive gradient information, but instead follows its target model with some momentum. This model EMA can improve the robustness and generalization properties of supervised learning, stabilize pseudo-labeling, and provide a learning signal for Self-Supervised Learning (SSL). Prior works have treated the model EMA separately from optimization, leading to different training dynamics across batch sizes and lower model performance. In this work, we provide a scaling rule for optimization in the presence of model EMAs and demonstrate its validity across a range of architectures, optimizers, and data modalities. We also show the rule's validity where the model EMA contributes to the optimization of the target model, enabling us to train EMA-based pseudo-labeling and SSL methods at small and large batch sizes. For SSL, we enable training of BYOL up to batch size 24,576 without sacrificing performance, optimally a 6$\times$ wall-clock time reduction.
</details>
<details>
<summary>摘要</summary>
Previous works have treated the model EMA separately from optimization, leading to different training dynamics across batch sizes and lower model performance. In this work, we provide a scaling rule for optimization in the presence of model EMAs and demonstrate its validity across a range of architectures, optimizers, and data modalities. We also show the rule's validity where the model EMA contributes to the optimization of the target model, enabling us to train EMA-based pseudo-labeling and SSL methods at small and large batch sizes.For SSL, we enable training of BYOL up to batch size 24,576 without sacrificing performance, resulting in a 6 times wall-clock time reduction.
</details></li>
</ul>
<hr>
<h2 id="When-Multi-Task-Learning-Meets-Partial-Supervision-A-Computer-Vision-Review"><a href="#When-Multi-Task-Learning-Meets-Partial-Supervision-A-Computer-Vision-Review" class="headerlink" title="When Multi-Task Learning Meets Partial Supervision: A Computer Vision Review"></a>When Multi-Task Learning Meets Partial Supervision: A Computer Vision Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14382">http://arxiv.org/abs/2307.14382</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maxime Fontana, Michael Spratling, Miaojing Shi</li>
<li>For: 本研究探讨了多任务学习（MTL）在不同半指导下的应用。* Methods: 本研究使用了多个参数共享技术来传递知识 между任务。* Results: 本研究介绍了多任务优化问题中的多种挑战，并提出了基于任务关系分组的方法来解决这些挑战。In English, this translates to:* For: This study explores the application of multi-task learning (MTL) under different partial supervision settings.* Methods: The study uses multiple parameter sharing techniques to transfer knowledge between tasks.* Results: The study introduces challenges arising from the multi-objective optimization scheme and proposes a method based on task relationships to address these challenges.<details>
<summary>Abstract</summary>
Multi-Task Learning (MTL) aims to learn multiple tasks simultaneously while exploiting their mutual relationships. By using shared resources to simultaneously calculate multiple outputs, this learning paradigm has the potential to have lower memory requirements and inference times compared to the traditional approach of using separate methods for each task. Previous work in MTL has mainly focused on fully-supervised methods, as task relationships can not only be leveraged to lower the level of data-dependency of those methods but they can also improve performance. However, MTL introduces a set of challenges due to a complex optimisation scheme and a higher labeling requirement. This review focuses on how MTL could be utilised under different partial supervision settings to address these challenges. First, this review analyses how MTL traditionally uses different parameter sharing techniques to transfer knowledge in between tasks. Second, it presents the different challenges arising from such a multi-objective optimisation scheme. Third, it introduces how task groupings can be achieved by analysing task relationships. Fourth, it focuses on how partially supervised methods applied to MTL can tackle the aforementioned challenges. Lastly, this review presents the available datasets, tools and benchmarking results of such methods.
</details>
<details>
<summary>摘要</summary>
First, the review examines how MTL traditionally uses parameter sharing techniques to transfer knowledge between tasks. Second, it discusses the challenges arising from the multi-objective optimization scheme. Third, it introduces task groupings based on task relationships. Fourth, it focuses on how partially supervised methods can be applied to MTL to tackle the challenges. Finally, the review presents available datasets, tools, and benchmarking results for such methods.
</details></li>
</ul>
<hr>
<h2 id="EdgeConvEns-Convolutional-Ensemble-Learning-for-Edge-Intelligence"><a href="#EdgeConvEns-Convolutional-Ensemble-Learning-for-Edge-Intelligence" class="headerlink" title="EdgeConvEns: Convolutional Ensemble Learning for Edge Intelligence"></a>EdgeConvEns: Convolutional Ensemble Learning for Edge Intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14381">http://arxiv.org/abs/2307.14381</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ilkay Sikdokur, İnci M. Baytaş, Arda Yurdakul</li>
<li>for: 这项研究旨在提出一种基于 convolutional ensemble learning 的深入边缘智能方法，以提高边缘设备的训练效果和预测性能。</li>
<li>methods: 本研究使用了 Federation Learning 方法，并在边缘设备上实现了多个不同计算能力的 FPGA 设备上的独立训练。同时，通过将学习到的特征传输到中央服务器进行集成训练，以提高总预测性能。</li>
<li>results: 实验结果表明，EdgeConvEns 可以在不同训练场景下比 estado-of-the-art 方法具有更好的预测性能，同时减少了数据传输量和通信次数。<details>
<summary>Abstract</summary>
Deep edge intelligence aims to deploy deep learning models that demand computationally expensive training in the edge network with limited computational power. Moreover, many deep edge intelligence applications require handling distributed data that cannot be transferred to a central server due to privacy concerns. Decentralized learning methods, such as federated learning, offer solutions where models are learned collectively by exchanging learned weights. However, they often require complex models that edge devices may not handle and multiple rounds of network communication to achieve state-of-the-art performances. This study proposes a convolutional ensemble learning approach, coined EdgeConvEns, that facilitates training heterogeneous weak models on edge and learning to ensemble them where data on edge are heterogeneously distributed. Edge models are implemented and trained independently on Field-Programmable Gate Array (FPGA) devices with various computational capacities. Learned data representations are transferred to a central server where the ensemble model is trained with the learned features received from the edge devices to boost the overall prediction performance. Extensive experiments demonstrate that the EdgeConvEns can outperform the state-of-the-art performance with fewer communications and less data in various training scenarios.
</details>
<details>
<summary>摘要</summary>
深入智能目标是在边缘网络中部署需要计算负担强大的深度学习模型，但边缘设备的计算能力有限。此外，许多深入智能应用需要处理分布式数据，这些数据无法被传输到中央服务器 due to 隐私问题。分布式学习方法，如联邦学习，可以解决这些问题，但它们通常需要复杂的模型，边缘设备可能无法处理，并且需要多轮的网络通信来达到状态艺术性能。本研究提出了一种 convolutional ensemble learning 方法，名为 EdgeConvEns，它可以在边缘设备上训练多种不同的弱模型，并将这些模型 ensemble 在边缘设备上。边缘设备上实现和训练独立的 Field-Programmable Gate Array (FPGA) 设备，并将学习到的数据表示传输到中央服务器，以在中央服务器上训练 ensemble 模型，以提高总预测性能。经验示出，EdgeConvEns 可以在不同的训练场景下超越当前的状态艺术性能，并且需要更少的通信和数据量。
</details></li>
</ul>
<hr>
<h2 id="Source-Condition-Double-Robust-Inference-on-Functionals-of-Inverse-Problems"><a href="#Source-Condition-Double-Robust-Inference-on-Functionals-of-Inverse-Problems" class="headerlink" title="Source Condition Double Robust Inference on Functionals of Inverse Problems"></a>Source Condition Double Robust Inference on Functionals of Inverse Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13793">http://arxiv.org/abs/2307.13793</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrew Bennett, Nathan Kallus, Xiaojie Mao, Whitney Newey, Vasilis Syrgkanis, Masatoshi Uehara</li>
<li>for: 这个论文主要是为了研究 linear inverse problems 中参数估计的方法。</li>
<li>methods: 论文使用了 doubly robust representation 方法，该方法基于解决 primal 和 dual 两个线性 inverse problems 的解。</li>
<li>results: 论文提供了一种 asymptotically normal 的 parameter estimation method, 不需要知道 primal 或 dual 问题是哪个更加正确的问题。这个结果基于一种新的 iterated Tikhonov regularized adversarial estimators 方法，该方法可以应用于 general hypothesis spaces 上的 linear inverse problems。<details>
<summary>Abstract</summary>
We consider estimation of parameters defined as linear functionals of solutions to linear inverse problems. Any such parameter admits a doubly robust representation that depends on the solution to a dual linear inverse problem, where the dual solution can be thought as a generalization of the inverse propensity function. We provide the first source condition double robust inference method that ensures asymptotic normality around the parameter of interest as long as either the primal or the dual inverse problem is sufficiently well-posed, without knowledge of which inverse problem is the more well-posed one. Our result is enabled by novel guarantees for iterated Tikhonov regularized adversarial estimators for linear inverse problems, over general hypothesis spaces, which are developments of independent interest.
</details>
<details>
<summary>摘要</summary>
我们考虑参数估计为线性函数解析方法的线性逆问题。任何参数都可以得到双重稳定表示，这种表示取决于解析方法的对偶问题的解，可以看作总化倒数逆函数的扩展。我们提供了第一个源condition double robust推断方法，可以在参数关心范围内保证参数归一化正常性，只要 primal 或 dual 逆问题够正确，不需要知道哪个逆问题更加正确。我们的结果基于新的 iterated Tikhonov regularized adversarial estimator 的 guarantees，这些保证在一般假设空间上适用，是独立的研究成果。
</details></li>
</ul>
<hr>
<h2 id="Histogram-Layer-Time-Delay-Neural-Networks-for-Passive-Sonar-Classification"><a href="#Histogram-Layer-Time-Delay-Neural-Networks-for-Passive-Sonar-Classification" class="headerlink" title="Histogram Layer Time Delay Neural Networks for Passive Sonar Classification"></a>Histogram Layer Time Delay Neural Networks for Passive Sonar Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13788">http://arxiv.org/abs/2307.13788</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/peeples-lab/hltdnn">https://github.com/peeples-lab/hltdnn</a></li>
<li>paper_authors: Jarin Ritu, Ethan Barnes, Riley Martell, Alexandra Van Dine, Joshua Peeples</li>
<li>for: 本研究旨在提高海上陌生探测中的潜水噪音目标检测，因为噪音波的传播复杂，目标识别具有挑战性。</li>
<li>methods: 本研究提出了一种新的方法，它将时间延迟神经网络和 histogram 层结合使用，以利用噪音波观测记录中的统计上下文来提高特征学习和海上陌生探测中的噪音目标识别。</li>
<li>results: 对比基eline模型，本研究的方法显示出了更高的识别精度，这表明了在噪音目标识别中吸收统计上下文的优势。代码可以公开获得。<details>
<summary>Abstract</summary>
Underwater acoustic target detection in remote marine sensing operations is challenging due to complex sound wave propagation. Despite the availability of reliable sonar systems, target recognition remains a difficult problem. Various methods address improved target recognition. However, most struggle to disentangle the high-dimensional, non-linear patterns in the observed target recordings. In this work, a novel method combines a time delay neural network and histogram layer to incorporate statistical contexts for improved feature learning and underwater acoustic target classification. The proposed method outperforms the baseline model, demonstrating the utility in incorporating statistical contexts for passive sonar target recognition. The code for this work is publicly available.
</details>
<details>
<summary>摘要</summary>
水下声学目标检测在远程海洋探测操作中存在很大的挑战，主要是声波传播复杂。尽管有可靠的声纳系统，但目标识别仍然是一个困难的问题。各种方法尝试了改进目标识别，但大多数都无法分离高维、非线性的目标记录特征。在这种情况下，我们提出了一种新的方法，它将时间延迟神经网络和分布图层结合在一起，以利用统计上下文来改进声学目标识别。我们的方法比基eline模型更高效，这 demonstartes了在声学目标识别中提供统计上下文的重要性。代码已经公开 availible。
</details></li>
</ul>
<hr>
<h2 id="The-GANfather-Controllable-generation-of-malicious-activity-to-improve-defence-systems"><a href="#The-GANfather-Controllable-generation-of-malicious-activity-to-improve-defence-systems" class="headerlink" title="The GANfather: Controllable generation of malicious activity to improve defence systems"></a>The GANfather: Controllable generation of malicious activity to improve defence systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13787">http://arxiv.org/abs/2307.13787</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ricardo Ribeiro Pereira, Jacopo Bono, João Tiago Ascensão, David Aparício, Pedro Ribeiro, Pedro Bizarro</li>
<li>for: 帮助防御系统检测恶意活动，不需要标注数据</li>
<li>methods: 提出了一种基于生成器网络的方法，通过引入额外目标函数来奖励生成恶意样本</li>
<li>results: 在两个实际应用中（货币洗钱和推荐系统），我们成功地使用这种方法生成了恶意样本，并训练了一个新的防御系统来捕捉这些样本。<details>
<summary>Abstract</summary>
Machine learning methods to aid defence systems in detecting malicious activity typically rely on labelled data. In some domains, such labelled data is unavailable or incomplete. In practice this can lead to low detection rates and high false positive rates, which characterise for example anti-money laundering systems. In fact, it is estimated that 1.7--4 trillion euros are laundered annually and go undetected. We propose The GANfather, a method to generate samples with properties of malicious activity, without label requirements. We propose to reward the generation of malicious samples by introducing an extra objective to the typical Generative Adversarial Networks (GANs) loss. Ultimately, our goal is to enhance the detection of illicit activity using the discriminator network as a novel and robust defence system. Optionally, we may encourage the generator to bypass pre-existing detection systems. This setup then reveals defensive weaknesses for the discriminator to correct. We evaluate our method in two real-world use cases, money laundering and recommendation systems. In the former, our method moves cumulative amounts close to 350 thousand dollars through a network of accounts without being detected by an existing system. In the latter, we recommend the target item to a broad user base with as few as 30 synthetic attackers. In both cases, we train a new defence system to capture the synthetic attacks.
</details>
<details>
<summary>摘要</summary>
机器学习方法通常需要标注数据来帮助防御系统检测恶意活动。在某些领域，这些标注数据可能不可 obtenía或 incomplete。这可能导致检测率低下 false positive 率高，这种情况例如反洗钱系统。实际上，每年可能有 1.7--4 亿欧元被骗财，并未被发现。我们提出了 The GANfather，一种方法，可以生成具有恶意活动特性的样本，不需要标注。我们提出了在 Typical Generative Adversarial Networks (GANs) 损失函数中引入一个额外的目标，以奖励生成恶意样本。最终，我们的目标是通过使用探测器网络作为一种新的和可靠的防御系统，提高恶意活动的检测。选择地，我们可以让生成器 circumvent 现有的检测系统。这种设置然后 revelas defensive weaknesses for the discriminator to correct。我们在两个实际应用中评估了我们的方法：反洗钱和推荐系统。在前一个应用中，我们通过一个网络的账户来传递累计金额达 350 万美元，而不被现有系统检测到。在后一个应用中，我们通过 Synthetic 攻击者来推荐目标项目，并且只需要 30 个 synthetic 攻击者。在两个案例中，我们训练了一个新的防御系统，以捕捉 Synthetic 攻击。
</details></li>
</ul>
<hr>
<h2 id="Robust-Assignment-of-Labels-for-Active-Learning-with-Sparse-and-Noisy-Annotations"><a href="#Robust-Assignment-of-Labels-for-Active-Learning-with-Sparse-and-Noisy-Annotations" class="headerlink" title="Robust Assignment of Labels for Active Learning with Sparse and Noisy Annotations"></a>Robust Assignment of Labels for Active Learning with Sparse and Noisy Annotations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14380">http://arxiv.org/abs/2307.14380</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Kałuża, Andrzej Janusz, Dominik Ślęzak</li>
<li>for: 解决激活学习中数据标注错误的问题</li>
<li>methods: 提出了两种新的注释统一算法，利用无标示部分的样本空间</li>
<li>results: 在四个公共数据集上进行了实验，表明提案的方法在估计注释者可靠性和实际标签分配方面具有robustness和superiority，并且比州对数据集中的简单多数投票更为有效。<details>
<summary>Abstract</summary>
Supervised classification algorithms are used to solve a growing number of real-life problems around the globe. Their performance is strictly connected with the quality of labels used in training. Unfortunately, acquiring good-quality annotations for many tasks is infeasible or too expensive to be done in practice. To tackle this challenge, active learning algorithms are commonly employed to select only the most relevant data for labeling. However, this is possible only when the quality and quantity of labels acquired from experts are sufficient. Unfortunately, in many applications, a trade-off between annotating individual samples by multiple annotators to increase label quality vs. annotating new samples to increase the total number of labeled instances is necessary. In this paper, we address the issue of faulty data annotations in the context of active learning. In particular, we propose two novel annotation unification algorithms that utilize unlabeled parts of the sample space. The proposed methods require little to no intersection between samples annotated by different experts. Our experiments on four public datasets indicate the robustness and superiority of the proposed methods in both, the estimation of the annotator's reliability, and the assignment of actual labels, against the state-of-the-art algorithms and the simple majority voting.
</details>
<details>
<summary>摘要</summary>
超visisted分类算法在全球各地的实际问题中得到应用。它们的性能与训练中使用的标签质量有着紧密的关系。然而，获取高质量标签是在实践中不可能或太昂贵了。为解决这个挑战，活动学算法通常被使用来选择仅需要标注的数据。然而，这只有当获取专家标注的标签质量和量足够时才能够实现。在许多应用程序中，需要考虑 annotating individual samples by multiple annotators 来提高标签质量 vs. annotating new samples 来增加标注的总数。在这篇论文中，我们对活动学中的假数据标注进行了研究。我们提出了两种新的标注统一算法，它们可以利用样本空间中的无标注部分。我们的方法需要标注者之间的交叉少到无。我们在四个公共数据集上进行了实验，结果表明我们的方法在计算标注者的可靠性和实际标注中具有更高的稳定性和优势，相比于当前的算法和简单多数投票。
</details></li>
</ul>
<hr>
<h2 id="Accuracy-Amplification-in-Differentially-Private-Logistic-Regression-A-Pre-Training-Approach"><a href="#Accuracy-Amplification-in-Differentially-Private-Logistic-Regression-A-Pre-Training-Approach" class="headerlink" title="Accuracy Amplification in Differentially Private Logistic Regression: A Pre-Training Approach"></a>Accuracy Amplification in Differentially Private Logistic Regression: A Pre-Training Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13771">http://arxiv.org/abs/2307.13771</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Hoseinpour, Milad Hoseinpour, Ali Aghagolzadeh</li>
<li>for: 保护隐私的机器学习模型训练数据。</li>
<li>methods: 使用预训练模块和 differential privacy 的 logistic regression 模型。</li>
<li>results: 通过预训练模块，可以提高 differential privacy 下的机器学习模型的准确率。In more detail, the paper aims to improve the accuracy of a differentially private logistic regression model by using a pre-training module. The authors first pre-train the model on a public dataset without privacy concerns, and then fine-tune the model using the private dataset with differential privacy constraints. The results show that adding the pre-training module significantly improves the accuracy of the differentially private logistic regression model.<details>
<summary>Abstract</summary>
Machine learning (ML) models can memorize training datasets. As a result, training ML models over private datasets can violate the privacy of individuals. Differential privacy (DP) is a rigorous privacy notion to preserve the privacy of underlying training datasets in ML models. Yet, training ML models in a DP framework usually degrades the accuracy of ML models. This paper aims to boost the accuracy of a DP-ML model, specifically a logistic regression model, via a pre-training module. In more detail, we initially pre-train our model on a public training dataset that there is no privacy concern about it. Then, we fine-tune our model via the DP logistic regression with the private dataset. In the numerical results, we show that adding a pre-training module significantly improves the accuracy of the DP logistic regression.
</details>
<details>
<summary>摘要</summary>
机器学习（ML）模型可以记忆训练数据集。因此，在训练ML模型时，可能会违反个人隐私。不同隐私（DP）是一种严格的隐私概念，用于保护训练数据集的隐私。然而，在DP框架下训练ML模型通常会降低模型的准确率。这篇论文目的是提高DP-ML模型的准确率，特别是使用Logistic Regression模型。在更多的细节中，我们首先在没有隐私问题的公共训练数据集上进行预训练。然后，我们使用DP Logistic Regression模型进行细化。在实验结果中，我们发现，添加预训练模块可以显著提高DP Logistic Regression模型的准确率。
</details></li>
</ul>
<hr>
<h2 id="ClusterSeq-Enhancing-Sequential-Recommender-Systems-with-Clustering-based-Meta-Learning"><a href="#ClusterSeq-Enhancing-Sequential-Recommender-Systems-with-Clustering-based-Meta-Learning" class="headerlink" title="ClusterSeq: Enhancing Sequential Recommender Systems with Clustering based Meta-Learning"></a>ClusterSeq: Enhancing Sequential Recommender Systems with Clustering based Meta-Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13766">http://arxiv.org/abs/2307.13766</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammmadmahdi Maheri, Reza Abdollahzadeh, Bardia Mohammadi, Mina Rafiei, Jafar Habibi, Hamid R. Rabiee</li>
<li>for: 解决用户冷启始问题，提高续传推荐系统的效果。</li>
<li>methods:  combining meta-learning with user and item-side information，并使用动态信息在用户序列中增强物品预测精度。</li>
<li>results: 比如 existing meta-learning methods，我们的提案方法实现了16-39%的提升在 Mean Reciprocal Rank (MRR) 中。<details>
<summary>Abstract</summary>
In practical scenarios, the effectiveness of sequential recommendation systems is hindered by the user cold-start problem, which arises due to limited interactions for accurately determining user preferences. Previous studies have attempted to address this issue by combining meta-learning with user and item-side information. However, these approaches face inherent challenges in modeling user preference dynamics, particularly for "minor users" who exhibit distinct preferences compared to more common or "major users." To overcome these limitations, we present a novel approach called ClusterSeq, a Meta-Learning Clustering-Based Sequential Recommender System. ClusterSeq leverages dynamic information in the user sequence to enhance item prediction accuracy, even in the absence of side information. This model preserves the preferences of minor users without being overshadowed by major users, and it capitalizes on the collective knowledge of users within the same cluster. Extensive experiments conducted on various benchmark datasets validate the effectiveness of ClusterSeq. Empirical results consistently demonstrate that ClusterSeq outperforms several state-of-the-art meta-learning recommenders. Notably, compared to existing meta-learning methods, our proposed approach achieves a substantial improvement of 16-39% in Mean Reciprocal Rank (MRR).
</details>
<details>
<summary>摘要</summary>
在实际应用场景中，顺序推荐系统的效果受用户冷启问题的限制，这种问题 arise due to 用户与ITEM之间的互动有限，难以准确地确定用户的偏好。先前的研究尝试通过meta-学习和用户项信息的结合来解决这个问题，但这些方法面临用户偏好动态模型化的挑战，特别是对"小用户"而言，他们的偏好与"大用户"不同。为了超越这些限制，我们提出了一种新的方法 called ClusterSeq，这是一种基于 clustering 的 Meta-Learning Sequential Recommender System。ClusterSeq 利用用户序列中的动态信息来提高项预测精度，即使没有副信息。这个模型保留了小用户的偏好，不会被大用户所掩盖，同时利用用户集中的共同知识来提高推荐的准确性。在多个标准 benchmark 数据集上进行了广泛的实验，证明 ClusterSeq 的效果。实验结果表明，ClusterSeq 在 MRR 方面与state-of-the-art meta-learning recommenders 相比，具有16-39%的显著提高。
</details></li>
</ul>
<hr>
<h2 id="Implicitly-Normalized-Explicitly-Regularized-Density-Estimation"><a href="#Implicitly-Normalized-Explicitly-Regularized-Density-Estimation" class="headerlink" title="Implicitly Normalized Explicitly Regularized Density Estimation"></a>Implicitly Normalized Explicitly Regularized Density Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13763">http://arxiv.org/abs/2307.13763</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mark Kozdoba, Binyamin Perets, Shie Mannor</li>
<li>for: 非 Parametric density estimation 方法</li>
<li>methods: 使用 Sobolev  нор调整 density</li>
<li>results: 比 Kernel Density Estimation 方法更加不偏，可以Clearly interpret 模型偏差Here’s a more detailed explanation of each point:1. for: The paper proposes a new approach to non-parametric density estimation, which is based on regularizing a Sobolev norm of the density. This approach is different from traditional Kernel Density Estimation (KDE) methods, and it aims to provide a more interpretable and less biased estimation of the density.2. methods: The proposed method uses a regularization term that is based on the Sobolev norm of the density, which is a measure of the smoothness of the density. The optimization problem needed to determine the density is non-convex, and standard gradient methods do not perform well. However, the paper shows that with an appropriate initialization and using natural gradients, one can obtain well-performing solutions.3. results: The paper evaluates the proposed method on the comprehensive recent Anomaly Detection benchmark suite, ADBench, and finds that it ranks second best among more than 15 algorithms. The method provides unnormalized densities, which prevents the use of log-likelihood for cross validation. However, the paper shows that one can instead adapt Fisher Divergence based Score Matching methods for this task.<details>
<summary>Abstract</summary>
We propose a new approach to non-parametric density estimation, that is based on regularizing a Sobolev norm of the density. This method is provably different from Kernel Density Estimation, and makes the bias of the model clear and interpretable. While there is no closed analytic form for the associated kernel, we show that one can approximate it using sampling. The optimization problem needed to determine the density is non-convex, and standard gradient methods do not perform well. However, we show that with an appropriate initialization and using natural gradients, one can obtain well performing solutions. Finally, while the approach provides unnormalized densities, which prevents the use of log-likelihood for cross validation, we show that one can instead adapt Fisher Divergence based Score Matching methods for this task. We evaluate the resulting method on the comprehensive recent Anomaly Detection benchmark suite, ADBench, and find that it ranks second best, among more than 15 algorithms.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的非Parametric数量估计方法，基于调整 Sobolev norm 的数量估计。这种方法与 Kernel Density Estimation 不同，可以将模型的偏见显示出来并实现可解释性。虽然没有关注点的关注函数，但我们显示可以使用抽样来近似它。估计问题是非对称的，标准的梯度法不太好。但我们显示，对于适当的初始化和使用自然梯度，可以得到良好的解。最后，这种方法提供的数量估计无法取得正规化的数量估计，因此无法使用log-likelihood进行cross validate。但我们显示可以使用Fisher Divergence based Score Matching方法来处理这个问题。我们在Anomaly Detection benchmark suite ADBench 上进行了评估，发现其排名第二，与其他15种方法相比。
</details></li>
</ul>
<hr>
<h2 id="UPREVE-An-End-to-End-Causal-Discovery-Benchmarking-System"><a href="#UPREVE-An-End-to-End-Causal-Discovery-Benchmarking-System" class="headerlink" title="UPREVE: An End-to-End Causal Discovery Benchmarking System"></a>UPREVE: An End-to-End Causal Discovery Benchmarking System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13757">http://arxiv.org/abs/2307.13757</a></li>
<li>repo_url: None</li>
<li>paper_authors: Suraj Jyothi Unni, Paras Sheth, Kaize Ding, Huan Liu, K. Selcuk Candan</li>
<li>for: 本研究旨在提供一个轻松使用、可自定义的web基台用于探索复杂社会行为系统中的 causal 关系。</li>
<li>methods: 本研究使用了多种算法同时运行、可视化 causal 关系以及评估学习的 causal 图的精度。</li>
<li>results: 本研究提出了一个轻松使用、可自定义的 web 基台 UPREVE，可以帮助研究者和实践者更好地探索和理解社会行为系统中的 causal 关系，以便更好地做出决策。<details>
<summary>Abstract</summary>
Discovering causal relationships in complex socio-behavioral systems is challenging but essential for informed decision-making. We present Upload, PREprocess, Visualize, and Evaluate (UPREVE), a user-friendly web-based graphical user interface (GUI) designed to simplify the process of causal discovery. UPREVE allows users to run multiple algorithms simultaneously, visualize causal relationships, and evaluate the accuracy of learned causal graphs. With its accessible interface and customizable features, UPREVE empowers researchers and practitioners in social computing and behavioral-cultural modeling (among others) to explore and understand causal relationships effectively. Our proposed solution aims to make causal discovery more accessible and user-friendly, enabling users to gain valuable insights for better decision-making.
</details>
<details>
<summary>摘要</summary>
发现复杂社会行为系统中的 causal 关系是具有挑战性和重要性的，但是这对于 informed decision-making 是必需的。我们提出了 Upload、PREprocess、Visualize 和 Evaluate（UPREVE），一个用户友好的网页式 графического用户界面（GUI），用于简化 causal discovery 的过程。UPREVE 允许用户同时运行多个算法，可视化 causal 关系，并评估学习的 causal 图的准确性。它的访问ible 界面和可定制功能，使得研究者和实践者在社会计算和文化模型中能够有效地探索和理解 causal 关系，从而获得有价值的发现，以便更好的决策。我们的提议的解决方案计划使 causal discovery 更加访问ible和用户友好，以便用户可以更好地理解 causal 关系，并做出更好的决策。
</details></li>
</ul>
<hr>
<h2 id="Solution-Path-of-Time-varying-Markov-Random-Fields-with-Discrete-Regularization"><a href="#Solution-Path-of-Time-varying-Markov-Random-Fields-with-Discrete-Regularization" class="headerlink" title="Solution Path of Time-varying Markov Random Fields with Discrete Regularization"></a>Solution Path of Time-varying Markov Random Fields with Discrete Regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13750">http://arxiv.org/abs/2307.13750</a></li>
<li>repo_url: None</li>
<li>paper_authors: Salar Fattahi, Andres Gomez</li>
<li>for: 本研究实际问题是如何推断时间变化的Markov随机场景（MRF），特别是具有不同的纯粹和时间规律化的参数。</li>
<li>methods: 本文提出了一种新的参数调整问题，使用紧缩的纯粹规律化来实现参数的简洁。这个问题可以 Parametrically 解决，并且可以在实际的设置下 scale 到高维度。</li>
<li>results: 本文展示了一个全新的解法，可以实现时间变化MRF的全解析解，并且可以在实际的设置下进行交互验证。这个解法可以在 $\mathcal{O}(pT^3) $ 时间内解决，其中 $T$ 是时间步骤数量，$p$ 是时间变化MRF中参数的数量。这个解法可以高效地进行交互验证，并且可以在实际的设置下获得提供了详细的解析解。<details>
<summary>Abstract</summary>
We study the problem of inferring sparse time-varying Markov random fields (MRFs) with different discrete and temporal regularizations on the parameters. Due to the intractability of discrete regularization, most approaches for solving this problem rely on the so-called maximum-likelihood estimation (MLE) with relaxed regularization, which neither results in ideal statistical properties nor scale to the dimensions encountered in realistic settings. In this paper, we address these challenges by departing from the MLE paradigm and resorting to a new class of constrained optimization problems with exact, discrete regularization to promote sparsity in the estimated parameters. Despite the nonconvex and discrete nature of our formulation, we show that it can be solved efficiently and parametrically for all sparsity levels. More specifically, we show that the entire solution path of the time-varying MRF for all sparsity levels can be obtained in $\mathcal{O}(pT^3)$, where $T$ is the number of time steps and $p$ is the number of unknown parameters at any given time. The efficient and parametric characterization of the solution path renders our approach highly suitable for cross-validation, where parameter estimation is required for varying regularization values. Despite its simplicity and efficiency, we show that our proposed approach achieves provably small estimation error for different classes of time-varying MRFs, namely Gaussian and discrete MRFs, with as few as one sample per time. Utilizing our algorithm, we can recover the complete solution path for instances of time-varying MRFs featuring over 30 million variables in less than 12 minutes on a standard laptop computer. Our code is available at \url{https://sites.google.com/usc.edu/gomez/data}.
</details>
<details>
<summary>摘要</summary>
我们研究了推理缺省时间变化Markov随机场（MRF）的问题，具有不同的离散和时间 regularization 参数。由于离散 regularization 的不可解性，大多数解决这个问题的方法都 rely on 最大 likelihood estimation（MLE）with relaxed regularization，这并不会导致理想的统计性质，也无法扩展到实际中的维度。在这篇论文中，我们解决这些挑战，我们不再采用 MLE 模型，而是基于一种新的受限制优化问题，以便在优化参数时Promote 缺省性。尽管我们的形式ulation 是非对称和离散的，但我们证明可以高效地解决它，并且可以 Parametrically 解决所有缺省级别。更specifically，我们证明可以在 $\mathcal{O}(pT^3)$ 时间内解决整个时间变化 MRF 的全解路径，其中 $T$ 是时间步骤数量，$p$ 是时间步骤中未知参数的数量。我们的方法高效、可 parametric，因此非常适合 Cross-validation，其中需要不同的 regularization 值进行参数估计。尽管它简单、高效，但我们证明我们的方法可以在不同类型的时间变化 MRF 上取得可观测小的估计误差，只需要一个时间步骤中的一个样本。使用我们的算法，我们可以在 less than 12 分钟内将 over 30 万个变量的解决路径全部回归到标准笔记计算机上。我们的代码可以在 \url{https://sites.google.com/usc.edu/gomez/data} 上找到。
</details></li>
</ul>
<hr>
<h2 id="mL-BFGS-A-Momentum-based-L-BFGS-for-Distributed-Large-Scale-Neural-Network-Optimization"><a href="#mL-BFGS-A-Momentum-based-L-BFGS-for-Distributed-Large-Scale-Neural-Network-Optimization" class="headerlink" title="mL-BFGS: A Momentum-based L-BFGS for Distributed Large-Scale Neural Network Optimization"></a>mL-BFGS: A Momentum-based L-BFGS for Distributed Large-Scale Neural Network Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13744">http://arxiv.org/abs/2307.13744</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yue Niu, Zalan Fabian, Sunwoo Lee, Mahdi Soltanolkotabi, Salman Avestimehr</li>
<li>for: 大规模神经网络优化中使用 quasi-Newton 方法遇到了 significiant 挑战，主要是在 Hessian 相关计算中添加了额外的计算成本，以及在随机训练中存在稳定性问题。</li>
<li>methods: 我们提出了一种轻量级的摩托矢量 L-BFGS 算法（mL-BFGS），该算法在大规模分布式深度神经网络优化中采用了几乎免费的摩托矢量计划，从而减少了随机噪声在 Hessian 中的影响，使得梯度下降过程更加稳定。</li>
<li>results: 我们通过使用 mL-BFGS 对一些标准神经网络模型进行训练，并与基elines（SGD、Adam 等）进行比较，发现 mL-BFGS 可以 achieve both noticeable iteration-wise 和 wall-clock 速度减少。<details>
<summary>Abstract</summary>
Quasi-Newton methods still face significant challenges in training large-scale neural networks due to additional compute costs in the Hessian related computations and instability issues in stochastic training. A well-known method, L-BFGS that efficiently approximates the Hessian using history parameter and gradient changes, suffers convergence instability in stochastic training. So far, attempts that adapt L-BFGS to large-scale stochastic training incur considerable extra overhead, which offsets its convergence benefits in wall-clock time. In this paper, we propose mL-BFGS, a lightweight momentum-based L-BFGS algorithm that paves the way for quasi-Newton (QN) methods in large-scale distributed deep neural network (DNN) optimization. mL-BFGS introduces a nearly cost-free momentum scheme into L-BFGS update and greatly reduces stochastic noise in the Hessian, therefore stabilizing convergence during stochastic optimization. For model training at a large scale, mL-BFGS approximates a block-wise Hessian, thus enabling distributing compute and memory costs across all computing nodes. We provide a supporting convergence analysis for mL-BFGS in stochastic settings. To investigate mL-BFGS potential in large-scale DNN training, we train benchmark neural models using mL-BFGS and compare performance with baselines (SGD, Adam, and other quasi-Newton methods). Results show that mL-BFGS achieves both noticeable iteration-wise and wall-clock speedup.
</details>
<details>
<summary>摘要</summary>
尽管凯撒-牛顿方法在训练大规模神经网络方面仍然面临着重要的挑战，主要是在资源成本上。这些方法在训练中需要额外的计算成本，以及约束变量的稳定性问题。L-BFGS方法，一种广泛使用的凯撒-牛顿方法，在随机训练中存在很大的问题，即迭代不稳定。目前，尝试将L-BFGS方法应用于大规模随机训练中，增加了非常大的额外开销，这将导致训练时间的增加。在这篇论文中，我们提出了一种轻量级的摩托率基于L-BFGS算法，称为mL-BFGS。该算法在大规模随机训练中使用历史参数和梯度变化来高效地估算梯度，从而稳定训练过程。此外，mL-BFGS还可以将块级别的梯度估算分布到所有计算节点上，从而实现分布式计算和存储成本的分摊。我们还提供了支持mL-BFGS在随机设定下的收敛分析。为了评估mL-BFGS在大规模神经网络训练中的潜力，我们使用mL-BFGS训练了一些标准神经网络模型，并与基线方法（SGD、Adam等）进行比较。结果表明，mL-BFGS在训练过程中能够达到显著的迭代减速和wall-clock减速。
</details></li>
</ul>
<hr>
<h2 id="ARB-Advanced-Reasoning-Benchmark-for-Large-Language-Models"><a href="#ARB-Advanced-Reasoning-Benchmark-for-Large-Language-Models" class="headerlink" title="ARB: Advanced Reasoning Benchmark for Large Language Models"></a>ARB: Advanced Reasoning Benchmark for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13692">http://arxiv.org/abs/2307.13692</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tomohiro Sawada, Daniel Paleka, Alexander Havrilla, Pranav Tadepalli, Paula Vidas, Alexander Kranias, John J. Nay, Kshitij Gupta, Aran Komatsuzaki</li>
<li>for: 本研究旨在提供一个新的 benchmark，以测试大型语言模型（LLM）的推理能力和领域知识。</li>
<li>methods: 本研究使用了多个领域的高级推理问题组成的 ARB benchmark，以测试 LLM 的推理能力和领域知识。</li>
<li>results: 研究发现，现有的 GPT-4 和 Claude 模型在 ARB benchmark 上的得分尚未达到专家水平，特别是在更加具有挑战性的任务上。此外，研究还引入了一种基于 rubric 的评估方法，以提高自动和助动评估能力。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have demonstrated remarkable performance on various quantitative reasoning and knowledge benchmarks. However, many of these benchmarks are losing utility as LLMs get increasingly high scores, despite not yet reaching expert performance in these domains. We introduce ARB, a novel benchmark composed of advanced reasoning problems in multiple fields. ARB presents a more challenging test than prior benchmarks, featuring problems in mathematics, physics, biology, chemistry, and law. As a subset of ARB, we introduce a challenging set of math and physics problems which require advanced symbolic reasoning and domain knowledge. We evaluate recent models such as GPT-4 and Claude on ARB and demonstrate that current models score well below 50% on more demanding tasks. In order to improve both automatic and assisted evaluation capabilities, we introduce a rubric-based evaluation approach, allowing GPT-4 to score its own intermediate reasoning steps. Further, we conduct a human evaluation of the symbolic subset of ARB, finding promising agreement between annotators and GPT-4 rubric evaluation scores.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="High-Probability-Analysis-for-Non-Convex-Stochastic-Optimization-with-Clipping"><a href="#High-Probability-Analysis-for-Non-Convex-Stochastic-Optimization-with-Clipping" class="headerlink" title="High Probability Analysis for Non-Convex Stochastic Optimization with Clipping"></a>High Probability Analysis for Non-Convex Stochastic Optimization with Clipping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13680">http://arxiv.org/abs/2307.13680</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaojie Li, Yong Liu</li>
<li>for: This paper studies the theoretical guarantees of stochastic optimization algorithms with gradient clipping in the non-convex setting.</li>
<li>methods: The paper uses high probability analysis to derive the optimization bound and the generalization bound simultaneously for popular stochastic optimization algorithms with gradient clipping, including stochastic gradient descent and its variants of momentum and adaptive stepsizes.</li>
<li>results: The paper provides a relatively complete picture for the theoretical guarantee of stochastic optimization algorithms with clipping, and shows that gradient clipping is a promising technique for dealing with the heavy-tailed behavior that emerged in stochastic optimization.<details>
<summary>Abstract</summary>
Gradient clipping is a commonly used technique to stabilize the training process of neural networks. A growing body of studies has shown that gradient clipping is a promising technique for dealing with the heavy-tailed behavior that emerged in stochastic optimization as well. While gradient clipping is significant, its theoretical guarantees are scarce. Most theoretical guarantees only provide an in-expectation analysis and only focus on optimization performance. In this paper, we provide high probability analysis in the non-convex setting and derive the optimization bound and the generalization bound simultaneously for popular stochastic optimization algorithms with gradient clipping, including stochastic gradient descent and its variants of momentum and adaptive stepsizes. With the gradient clipping, we study a heavy-tailed assumption that the gradients only have bounded $\alpha$-th moments for some $\alpha \in (1, 2]$, which is much weaker than the standard bounded second-moment assumption. Overall, our study provides a relatively complete picture for the theoretical guarantee of stochastic optimization algorithms with clipping.
</details>
<details>
<summary>摘要</summary>
Gradient clipping 是一种常用的技术来稳定神经网络的训练过程。一个不断增长的研究表明，gradient clipping 是一种有前途的技术，用于处理随机优化中的重 tailed 行为。虽然 gradient clipping 具有重要的意义，但其理论保证却很少。大多数理论保证都仅提供了预期分析，并仅关注优化性能。在这篇论文中，我们提供了高概率分析，在非对称设定下，并同时 deriv 出优化 bound 和泛化 bound  для各种束缚随机优化算法，包括梯度下降和其 variants of momentum 和 adaptive stepsizes。通过束缚，我们研究了一种具有bounded $\alpha$-th moment的梯度假设，其中 $\alpha \in (1, 2]$，这是标准二次均值假设的一个弱化版本。总的来说，我们的研究提供了一个相对完整的理论保证的图景，用于随机优化算法的 clipping。
</details></li>
</ul>
<hr>
<h2 id="RED-CoMETS-An-ensemble-classifier-for-symbolically-represented-multivariate-time-series"><a href="#RED-CoMETS-An-ensemble-classifier-for-symbolically-represented-multivariate-time-series" class="headerlink" title="RED CoMETS: An ensemble classifier for symbolically represented multivariate time series"></a>RED CoMETS: An ensemble classifier for symbolically represented multivariate time series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13679">http://arxiv.org/abs/2307.13679</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zy18811/red-comets">https://github.com/zy18811/red-comets</a></li>
<li>paper_authors: Luca A. Bennett, Zahraa S. Abdallah</li>
<li>for: 这篇论文主要应用在多重时间序列分类中，尤其是在金融、医疗、工程等实际应用中。</li>
<li>methods: 本论文提出了一个新的协 ensemble分类器，名为RED CoMETS（随机增强的多重时间序列协 ensemble分类器），它是基于Co-eye（一个特别设计来进行单一时间序列分类的协 ensemble分类器）的扩展，能够处理多重时间序列资料。</li>
<li>results: RED CoMETS在UCAR档案中的评估数据上展现出了竞争性的精度，与现有的多重设定中的州际技术相比，其中最高的报告精度为’HandMovementDirection’档案。此外，提案的方法可以与Co-eye相比，大大降低 computation time，使其成为多重时间序列分类中效率和可靠的选择。<details>
<summary>Abstract</summary>
Multivariate time series classification is a rapidly growing research field with practical applications in finance, healthcare, engineering, and more. The complexity of classifying multivariate time series data arises from its high dimensionality, temporal dependencies, and varying lengths. This paper introduces a novel ensemble classifier called RED CoMETS (Random Enhanced Co-eye for Multivariate Time Series), which addresses these challenges. RED CoMETS builds upon the success of Co-eye, an ensemble classifier specifically designed for symbolically represented univariate time series, and extends its capabilities to handle multivariate data. The performance of RED CoMETS is evaluated on benchmark datasets from the UCR archive, where it demonstrates competitive accuracy when compared to state-of-the-art techniques in multivariate settings. Notably, it achieves the highest reported accuracy in the literature for the 'HandMovementDirection' dataset. Moreover, the proposed method significantly reduces computation time compared to Co-eye, making it an efficient and effective choice for multivariate time series classification.
</details>
<details>
<summary>摘要</summary>
多变量时间序列分类是一个快速发展的研究领域，有实际应用于金融、医疗、工程等领域。multivariate时间序列数据的复杂性来自其高维度、时间相关性和不同长度。本文介绍一种新的ensemble分类器called RED CoMETS（随机增强Co-eye for Multivariate Time Series），这种方法可以解决这些挑战。RED CoMETS是基于Co-eye，一种专门为symbolically represented univariate时间序列设计的ensemble分类器，扩展其能力以处理多变量数据。本文的实验结果表明，RED CoMETS在UCAR数据库中的benchmark数据集上表现竞争性高，与当前最佳的多变量技术相比。特别是，它在'HandMovementDirection'数据集上达到了 literaturereported最高的准确率。此外，提议的方法可以减少Co-eye的计算时间，使其成为efficient和effective的多变量时间序列分类选择。
</details></li>
</ul>
<hr>
<h2 id="FedDRL-A-Trustworthy-Federated-Learning-Model-Fusion-Method-Based-on-Staged-Reinforcement-Learning"><a href="#FedDRL-A-Trustworthy-Federated-Learning-Model-Fusion-Method-Based-on-Staged-Reinforcement-Learning" class="headerlink" title="FedDRL: A Trustworthy Federated Learning Model Fusion Method Based on Staged Reinforcement Learning"></a>FedDRL: A Trustworthy Federated Learning Model Fusion Method Based on Staged Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13716">http://arxiv.org/abs/2307.13716</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leiming Chen, Cihao Dong, Sibo Qiao, Ziling Huang, Kai Wang, Yuming Nie, Zhaoxiang Hou, Cheewei Tan</li>
<li>for: This paper aims to address the issues of model heterogeneity and malicious behavior in traditional federated learning by proposing a reinforcement learning-based model fusion approach called FedDRL.</li>
<li>methods: The FedDRL algorithm uses a two-stage approach to first filter out malicious models and select trusted client models, and then adaptively adjust the weights of the trusted client models to achieve optimal model fusion.</li>
<li>results: The experimental results show that the FedDRL algorithm has higher reliability than other algorithms while maintaining accuracy in five different model fusion scenarios.Here’s the Chinese version of the three key points:</li>
<li>for: 这篇论文目标是解决传统联合学习中的模型不同和恶意行为问题，提出一种基于强化学习的模型融合方法called FedDRL。</li>
<li>methods: FedDRL算法使用两stage方法，先过滤恶意模型，然后自适应调整被信任客户端模型的权重以实现优化的模型融合。</li>
<li>results: 实验结果表明，FedDRL算法在五种不同的模型融合场景中具有更高的可靠性，同时保持准确性。<details>
<summary>Abstract</summary>
Traditional federated learning uses the number of samples to calculate the weights of each client model and uses this fixed weight value to fusion the global model. However, in practical scenarios, each client's device and data heterogeneity leads to differences in the quality of each client's model. Thus the contribution to the global model is not wholly determined by the sample size. In addition, if clients intentionally upload low-quality or malicious models, using these models for aggregation will lead to a severe decrease in global model accuracy. Traditional federated learning algorithms do not address these issues. To solve this probelm, we propose FedDRL, a model fusion approach using reinforcement learning based on a two staged approach. In the first stage, Our method could filter out malicious models and selects trusted client models to participate in the model fusion. In the second stage, the FedDRL algorithm adaptively adjusts the weights of the trusted client models and aggregates the optimal global model. We also define five model fusion scenarios and compare our method with two baseline algorithms in those scenarios. The experimental results show that our algorithm has higher reliability than other algorithms while maintaining accuracy.
</details>
<details>
<summary>摘要</summary>
传统的联合学习使用客户端模型的数量来计算每个客户端模型的权重值，并使用这些固定权重值来融合全球模型。然而，在实际场景中，每个客户端的设备和数据多样性导致每个客户端模型的质量差异很大。因此，传统的联合学习算法不能准确地评估每个客户端模型的贡献。此外，如果客户端故意上传低质量或黑客模型，那么使用这些模型进行融合会导致全球模型的准确率受到严重的影响。传统的联合学习算法无法解决这些问题。为解决这个问题，我们提出了 FedDRL，一种基于奖励学习的模型融合方法。在第一阶段，我们的方法可以过滤出黑客模型并选择可信客户端模型参与融合。在第二阶段，FedDRL算法可以自适应地调整可信客户端模型的权重值，并融合最佳的全球模型。我们还定义了五种模型融合场景，并与两个基准算法进行比较。实验结果显示，我们的算法在可靠性和准确率之间具有良好的平衡。
</details></li>
</ul>
<hr>
<h2 id="Towards-an-AI-Accountability-Policy"><a href="#Towards-an-AI-Accountability-Policy" class="headerlink" title="Towards an AI Accountability Policy"></a>Towards an AI Accountability Policy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13658">http://arxiv.org/abs/2307.13658</a></li>
<li>repo_url: None</li>
<li>paper_authors: Przemyslaw Grabowicz, Nicholas Perello, Yair Zick</li>
<li>For: The white paper offers a set of interconnected recommendations for an AI accountability policy in response to the “AI Accountability Policy Request for Comments” by the National Telecommunications and Information Administration of the United States.* Methods: The white paper provides a set of recommendations for an AI accountability policy, including the development of transparent and explainable AI, the establishment of accountability mechanisms for AI systems, and the promotion of human-centered AI.* Results: The white paper aims to provide a comprehensive framework for an AI accountability policy that can be used to ensure the responsible development and use of AI in various industries and applications.<details>
<summary>Abstract</summary>
This white paper is a response to the "AI Accountability Policy Request for Comments" by the National Telecommunications and Information Administration of the United States. The question numbers for which comments were requested are provided in superscripts at the end of key sentences answering the respective questions. The white paper offers a set of interconnected recommendations for an AI accountability policy.
</details>
<details>
<summary>摘要</summary>
这份白皮书是对美国国家电信和信息管理局（NTIA）发布的“人工智能负责任政策请求意见”的回应。文中提到的问题号以句末的括号形式标注。本白皮书提出了一组相互连接的人工智能负责任政策建议。Note: Please note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="GNN4FR-A-Lossless-GNN-based-Federated-Recommendation-Framework"><a href="#GNN4FR-A-Lossless-GNN-based-Federated-Recommendation-Framework" class="headerlink" title="GNN4FR: A Lossless GNN-based Federated Recommendation Framework"></a>GNN4FR: A Lossless GNN-based Federated Recommendation Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01197">http://arxiv.org/abs/2308.01197</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guowei Wu, Weike Pan, Zhong Ming</li>
<li>for: 隐私保护下构建全Graph Neural Networks（GNNs）推荐系统。</li>
<li>methods: 使用lossless federated recommendation framework based on GNN，实现全图训练，保持高阶结构信息完整性。</li>
<li>results: 与非联合方法相等，具有完整高阶结构信息，遵循隐私保护法规。<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) have gained wide popularity in recommender systems due to their capability to capture higher-order structure information among the nodes of users and items. However, these methods need to collect personal interaction data between a user and the corresponding items and then model them in a central server, which would break the privacy laws such as GDPR. So far, no existing work can construct a global graph without leaking each user's private interaction data (i.e., his or her subgraph). In this paper, we are the first to design a novel lossless federated recommendation framework based on GNN, which achieves full-graph training with complete high-order structure information, enabling the training process to be equivalent to the corresponding un-federated counterpart. In addition, we use LightGCN to instantiate an example of our framework and show its equivalence.
</details>
<details>
<summary>摘要</summary>
graph neural networks (GNNs) 已经在推荐系统中得到广泛应用，因为它们可以捕捉用户和物品之间的高阶结构信息。然而，这些方法需要收集每个用户与对应的物品之间的个人互动数据，然后模型在中央服务器上，这会违反隐私法规，如GDPR。目前没有任何现有的工作可以构建全球图without泄露每个用户的私人互动数据（即他或她的子图）。在这篇论文中，我们是首次设计了一种新的无损联邦推荐框架基于GNN，实现了全图训练，并保留了高阶结构信息完整性。此外，我们使用LightGCN来实现这种框架的示例，并证明其等价性。
</details></li>
</ul>
<hr>
<h2 id="Safety-Margins-for-Reinforcement-Learning"><a href="#Safety-Margins-for-Reinforcement-Learning" class="headerlink" title="Safety Margins for Reinforcement Learning"></a>Safety Margins for Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13642">http://arxiv.org/abs/2307.13642</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander Grushin, Walt Woods, Alvaro Velasquez, Simon Khan</li>
<li>For: The paper is written for identifying and mitigating unsafe situations in autonomous controllers, particularly in freight transportation applications.* Methods: The paper proposes using a proxy criticality metric that can be computed in real-time to identify when an agent is approaching a potentially catastrophic situation. The metric is based on the mean reduction in reward given some number of random actions.* Results: The paper demonstrates the effectiveness of its approach by evaluating it on learned policies from APE-X and A3C within an Atari environment. The results show that safety margins decrease as agents approach failure states, indicating the potential for catastrophic outcomes.<details>
<summary>Abstract</summary>
Any autonomous controller will be unsafe in some situations. The ability to quantitatively identify when these unsafe situations are about to occur is crucial for drawing timely human oversight in, e.g., freight transportation applications. In this work, we demonstrate that the true criticality of an agent's situation can be robustly defined as the mean reduction in reward given some number of random actions. Proxy criticality metrics that are computable in real-time (i.e., without actually simulating the effects of random actions) can be compared to the true criticality, and we show how to leverage these proxy metrics to generate safety margins, which directly tie the consequences of potentially incorrect actions to an anticipated loss in overall performance. We evaluate our approach on learned policies from APE-X and A3C within an Atari environment, and demonstrate how safety margins decrease as agents approach failure states. The integration of safety margins into programs for monitoring deployed agents allows for the real-time identification of potentially catastrophic situations.
</details>
<details>
<summary>摘要</summary>
任何自主控制器都会在某些情况下不安全。能够量化地识别这些不安全情况的发生是对于启动人工监督的时间而言至关重要，例如在货物运输应用中。在这项工作中，我们示出了一种真实的批处性可以坚定地定义为一些随机动作后的奖励减少的平均值。我们称这种代理批处性指标可以在实时（无需实际模拟动作的影响）计算，并且可以与真实批处性进行比较。我们还示出了如何利用这些代理指标生成安全余地，这些安全余地直接反映了可能 incorrect 动作的后果和预期的性能损失。我们在 APE-X 和 A3C 学习政策中的 Atari 环境中评估了我们的方法，并示出了安全余地如何随agent接近失败状态而逐渐减少。将安全余地集成到部署过程中的监控程序中，可以实现实时识别可能 catastrophic 的情况。
</details></li>
</ul>
<hr>
<h2 id="DBGSA-A-Novel-Data-Adaptive-Bregman-Clustering-Algorithm"><a href="#DBGSA-A-Novel-Data-Adaptive-Bregman-Clustering-Algorithm" class="headerlink" title="DBGSA: A Novel Data Adaptive Bregman Clustering Algorithm"></a>DBGSA: A Novel Data Adaptive Bregman Clustering Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14375">http://arxiv.org/abs/2307.14375</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ying Xiao, Hou-biao Li, Yu-pu Zhang</li>
<li>for: 提高非对称数据集中 clustering 算法的精度和稳定性。</li>
<li>methods: 提议一种基于 Bregman 差分参数优化的数据驱动 clustering 算法 (DBGSA)， combines 宇宙 gravitational algorithm 将相似点靠拢在数据集中。 构造了 gravitational coefficient equation  WITH special property 逐步减少影响因子。 使用 Bregman divergence generalized power mean information loss minimization 标识群集中心。</li>
<li>results: 对四个 simulated 数据集和六个实际数据集进行了广泛的实验，结果表明 DBGSA 在不同 clustering 算法中的准确率平均提高了63.8%，与其他类似方法和改进的数据集相比。 还设立了三维网格搜索来比较不同参数值的影响，发现我们的模型提供的参数集是优化的。<details>
<summary>Abstract</summary>
With the development of Big data technology, data analysis has become increasingly important. Traditional clustering algorithms such as K-means are highly sensitive to the initial centroid selection and perform poorly on non-convex datasets. In this paper, we address these problems by proposing a data-driven Bregman divergence parameter optimization clustering algorithm (DBGSA), which combines the Universal Gravitational Algorithm to bring similar points closer in the dataset. We construct a gravitational coefficient equation with a special property that gradually reduces the influence factor as the iteration progresses. Furthermore, we introduce the Bregman divergence generalized power mean information loss minimization to identify cluster centers and build a hyperparameter identification optimization model, which effectively solves the problems of manual adjustment and uncertainty in the improved dataset. Extensive experiments are conducted on four simulated datasets and six real datasets. The results demonstrate that DBGSA significantly improves the accuracy of various clustering algorithms by an average of 63.8\% compared to other similar approaches like enhanced clustering algorithms and improved datasets. Additionally, a three-dimensional grid search was established to compare the effects of different parameter values within threshold conditions, and it was discovered the parameter set provided by our model is optimal. This finding provides strong evidence of the high accuracy and robustness of the algorithm.
</details>
<details>
<summary>摘要</summary>
随着大数据技术的发展，数据分析已成为非常重要。传统的聚类算法如K-means受初始中心选择的影响很大，在非对称数据集上表现不佳。在这篇论文中，我们解决这些问题，提出一种基于数据驱动的Bregman差分参数优化聚类算法（DBGSA）。我们将 Universal Gravitational Algorithm 用于将相似点靠拢在数据集中。我们构建了一个引力系数方程，其特点是逐步减少影响因子。此外，我们引入Bregman差分总能平均信息损失来确定聚类中心，并构建了一个超参数标准化模型，可以有效解决手动调整和不确定性问题。我们在四个 simulated 数据集和六个实际数据集上进行了广泛的实验。结果表明，DBGSA可以在不同的聚类算法下提高准确率的平均提升率为63.8%，比其他相似方法的改进数据集和优化超参数更高。此外，我们设立了三维网格搜索，并发现我们的模型提供的参数集是优化的。这一结论提供了高精度和稳定性的证据。
</details></li>
</ul>
<hr>
<h2 id="Turning-hazardous-volatile-matter-compounds-into-fuel-by-catalytic-steam-reforming-An-evolutionary-machine-learning-approach"><a href="#Turning-hazardous-volatile-matter-compounds-into-fuel-by-catalytic-steam-reforming-An-evolutionary-machine-learning-approach" class="headerlink" title="Turning hazardous volatile matter compounds into fuel by catalytic steam reforming: An evolutionary machine learning approach"></a>Turning hazardous volatile matter compounds into fuel by catalytic steam reforming: An evolutionary machine learning approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05750">http://arxiv.org/abs/2308.05750</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alireza Shafizadeh, Hossein Shahbeik, Mohammad Hossein Nadian, Vijai Kumar Gupta, Abdul-Sattar Nizami, Su Shiung Lam, Wanxi Peng, Junting Pan, Meisam Tabatabaei, Mortaza Aghbashlo</li>
<li>for: 这种研究旨在开发一种基于机器学习的研究框架，用于模拟、理解和优化 catalytic steam reforming 过程中的材料和反应条件。</li>
<li>methods: 该研究使用了 X-ray diffraction analysis 等化学&#x2F;Texture analysis 获取输入特征，并使用了 Literature compile 一个包括多种催化剂特性和反应条件的数据库。研究采用了 six 种机器学习模型，并使用了粒子群搜索算法进行优化。</li>
<li>results: 研究结果表明， ensemble 机器学习模型可以提供高度预测性（R2 &gt; 0.976） для toluene 转化和产物分布。最佳的 tar 转化率高于 77.2% 可以在 637.44-725.62 ℃ 的温度范围内实现，催化剂 BET 表面积在 476.03-638.55 m2&#x2F;g 范围内。<details>
<summary>Abstract</summary>
Chemical and biomass processing systems release volatile matter compounds into the environment daily. Catalytic reforming can convert these compounds into valuable fuels, but developing stable and efficient catalysts is challenging. Machine learning can handle complex relationships in big data and optimize reaction conditions, making it an effective solution for addressing the mentioned issues. This study is the first to develop a machine-learning-based research framework for modeling, understanding, and optimizing the catalytic steam reforming of volatile matter compounds. Toluene catalytic steam reforming is used as a case study to show how chemical/textural analyses (e.g., X-ray diffraction analysis) can be used to obtain input features for machine learning models. Literature is used to compile a database covering a variety of catalyst characteristics and reaction conditions. The process is thoroughly analyzed, mechanistically discussed, modeled by six machine learning models, and optimized using the particle swarm optimization algorithm. Ensemble machine learning provides the best prediction performance (R2 > 0.976) for toluene conversion and product distribution. The optimal tar conversion (higher than 77.2%) is obtained at temperatures between 637.44 and 725.62 {\deg}C, with a steam-to-carbon molar ratio of 5.81-7.15 and a catalyst BET surface area 476.03-638.55 m2/g. The feature importance analysis satisfactorily reveals the effects of input descriptors on model prediction. Operating conditions (50.9%) and catalyst properties (49.1%) are equally important in modeling. The developed framework can expedite the search for optimal catalyst characteristics and reaction conditions, not only for catalytic chemical processing but also for related research areas.
</details>
<details>
<summary>摘要</summary>
化学和生物质处理系统每天都会发布可燃物质分子到环境中。catalytic reforming可以将这些分子转化为有价值的燃料，但是开发稳定和高效的催化剂是挑战。机器学习可以处理复杂的关系在大数据中，因此可以成为改进反应条件的有效解决方案。这项研究是首次开发了基于机器学习研究框架，用于模拟、理解和优化催化液气 reforming的气相催化剂。使用苯酚为例，通过化学/文化分析（例如X射晶体分析）获得输入特征，并使用文献编译一份包括多种催化剂特性和反应条件的数据库。通过综合分析、机制分析、六种机器学习模型和粒子群优化算法，我们获得了最佳的苯酚转化率（高于77.2%）和产品分布。 ensemble机器学习提供了最佳预测性能（R2 > 0.976）。最佳的沸腾 conversions（高于77.2%）在637.44-725.62℃的温度范围内，催化剂BET表面积476.03-638.55 m2/g。特征重要性分析准确地显示输入特征对模型预测的影响。操作条件（50.9%）和催化剂性质（49.1%）具有相等的重要性。我们开发的框架可以减少催化剂特性和反应条件的搜索，不仅限于催化化学处理，还可以应用于相关领域的研究。
</details></li>
</ul>
<hr>
<h2 id="Scaling-machine-learning-based-chemical-plant-simulation-A-method-for-fine-tuning-a-model-to-induce-stable-fixed-points"><a href="#Scaling-machine-learning-based-chemical-plant-simulation-A-method-for-fine-tuning-a-model-to-induce-stable-fixed-points" class="headerlink" title="Scaling machine learning-based chemical plant simulation: A method for fine-tuning a model to induce stable fixed points"></a>Scaling machine learning-based chemical plant simulation: A method for fine-tuning a model to induce stable fixed points</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13621">http://arxiv.org/abs/2307.13621</a></li>
<li>repo_url: None</li>
<li>paper_authors: Malte Esders, Gimmy Alex Fernandez Ramirez, Michael Gastegger, Satya Swarup Samal</li>
<li>for: 这篇论文是为了提出一种基于机器学习（ML）的化学厂模型，以取代理性的首要原理模型。</li>
<li>methods: 这篇论文使用了一种结构化的方法，即每个厂区域都由一个ML模型表示。然后，这些模型被连接到一个流程图像中，以便进行数据预测。</li>
<li>results: 作者发现，对于较小的厂房，这种方法效果非常好，但是对于更大的厂房，由于巨大的循环逻辑引起的循环解决问题会导致不稳定。作者分析了这个问题，并提出了一种方法来细调ML模型，以便通过常规方法解决循环问题。<details>
<summary>Abstract</summary>
Idealized first-principles models of chemical plants can be inaccurate. An alternative is to fit a Machine Learning (ML) model directly to plant sensor data. We use a structured approach: Each unit within the plant gets represented by one ML model. After fitting the models to the data, the models are connected into a flowsheet-like directed graph. We find that for smaller plants, this approach works well, but for larger plants, the complex dynamics arising from large and nested cycles in the flowsheet lead to instabilities in the cycle solver. We analyze this problem in depth and show that it is not merely a specialized concern but rather a more pervasive challenge that will likely occur whenever ML is applied to larger plants. To address this problem, we present a way to fine-tune ML models such that solving cycles with the usual methods becomes robust again.
</details>
<details>
<summary>摘要</summary>
理想化的基本原理模型可能不准确。一种alternative是直接将机器学习（ML）模型适应到厂区传感器数据。我们采用一种结构化方法：每个喷槽内部的设备都被一个ML模型表示。在模型适应数据后，模型被连接成一个流程图像类似的导向图。我们发现对小型厂区来说，这种方法工作良好，但对更大的厂区来说，由于大量和嵌入的循环在流程图中导致循环解决器的不稳定。我们对这个问题进行深入分析，并证明这不仅是特殊情况，而是更广泛的挑战， ML在更大的厂区中应用时会遇到这种问题。为解决这个问题，我们提出了一种细化ML模型的方法，使得通过常规方法解决循环变得稳定。
</details></li>
</ul>
<hr>
<h2 id="AI-and-ethics-in-insurance-a-new-solution-to-mitigate-proxy-discrimination-in-risk-modeling"><a href="#AI-and-ethics-in-insurance-a-new-solution-to-mitigate-proxy-discrimination-in-risk-modeling" class="headerlink" title="AI and ethics in insurance: a new solution to mitigate proxy discrimination in risk modeling"></a>AI and ethics in insurance: a new solution to mitigate proxy discrimination in risk modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13616">http://arxiv.org/abs/2307.13616</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marguerite Sauce, Antoine Chancel, Antoine Ly</li>
<li>for: 这个论文的目的是探讨如何使用机器学习算法实现更公平的保险业务，以及如何避免对保险公司的偏见。</li>
<li>methods: 这个论文使用了欧洲人权公约中关于歧视的指南，以及使用敏感个人数据在算法中的regulation。它还提出了一种新的方法，基于数 Linear Algebra的概念，以降低对直接歧视的风险。</li>
<li>results: 该论文的研究表明，使用这种新方法可以减少对直接歧视的风险，同时保持保险公司的资产风险评估和价格段化的精度。这种方法的使用也是简单易用，并且在一个具体的生命保险选择中得到了Promising的性能。<details>
<summary>Abstract</summary>
The development of Machine Learning is experiencing growing interest from the general public, and in recent years there have been numerous press articles questioning its objectivity: racism, sexism, \dots Driven by the growing attention of regulators on the ethical use of data in insurance, the actuarial community must rethink pricing and risk selection practices for fairer insurance. Equity is a philosophy concept that has many different definitions in every jurisdiction that influence each other without currently reaching consensus. In Europe, the Charter of Fundamental Rights defines guidelines on discrimination, and the use of sensitive personal data in algorithms is regulated. If the simple removal of the protected variables prevents any so-called `direct' discrimination, models are still able to `indirectly' discriminate between individuals thanks to latent interactions between variables, which bring better performance (and therefore a better quantification of risk, segmentation of prices, and so on). After introducing the key concepts related to discrimination, we illustrate the complexity of quantifying them. We then propose an innovative method, not yet met in the literature, to reduce the risks of indirect discrimination thanks to mathematical concepts of linear algebra. This technique is illustrated in a concrete case of risk selection in life insurance, demonstrating its simplicity of use and its promising performance.
</details>
<details>
<summary>摘要</summary>
机器学习的发展正在吸引越来越多的一般大众注意，最近几年有许多新闻报导质疑其中的公正性： racism、性别歧视、等等。随着规制机关对数据的伦理使用的关注，保险业界必须重新思考定价和风险选择实践，以确保更公正的保险。“公正”是一个哲学概念，在每个司法管辖区域都有不同的定义，这些定义彼此影响并未能达成现在的共识。在欧洲，《权利宪法》提供了关于歧视的指南，而使用敏感个人数据在算法中的使用则被规管。即使简单地删除保护变数，模型仍然能够间接歧视个人，因为变数之间的隐藏互动带来更好的性能（并因此更好地评估风险、价格分类等）。我们首先介绍了歧视的定义，然后详细介绍了该现象的复杂性。接着，我们提出了一种新的方法，未经过文献中的应用，以减少间接歧视的风险。这种方法基于数学概念的线性代数，并在生命保险中的风险选择中实现了简单且有前途的使用。
</details></li>
</ul>
<hr>
<h2 id="Team-Intro-to-AI-team8-at-CoachAI-Badminton-Challenge-2023-Advanced-ShuttleNet-for-Shot-Predictions"><a href="#Team-Intro-to-AI-team8-at-CoachAI-Badminton-Challenge-2023-Advanced-ShuttleNet-for-Shot-Predictions" class="headerlink" title="Team Intro to AI team8 at CoachAI Badminton Challenge 2023: Advanced ShuttleNet for Shot Predictions"></a>Team Intro to AI team8 at CoachAI Badminton Challenge 2023: Advanced ShuttleNet for Shot Predictions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13715">http://arxiv.org/abs/2307.13715</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shih-Hong Chen, Pin-Hsuan Chou, Yong-Fu Liu, Chien-An Han</li>
<li>for: 提高现有框架ShuttleNet在预测羽毛球发球类型和位置的性能，通过利用过去的撕击。</li>
<li>methods: 利用过去的撕击来预测羽毛球发球类型和位置。</li>
<li>results: 在IJCAI 2023的CoachAI Badminton Challenge中获得了明显更好的结果，比基eline更高，最终取得了比赛的第一名，代码也公开了。<details>
<summary>Abstract</summary>
In this paper, our objective is to improve the performance of the existing framework ShuttleNet in predicting badminton shot types and locations by leveraging past strokes. We participated in the CoachAI Badminton Challenge at IJCAI 2023 and achieved significantly better results compared to the baseline. Ultimately, our team achieved the first position in the competition and we made our code available.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们的目标是通过利用过去的击球来提高现有框架ShuttleNet在预测羽毛球shot类型和位置的性能。我们参加了IJCAI 2023年的CoachAI Badminton Challenge，并实现了相比基eline的显著提高。最终，我们的团队取得了比赛的第一名，并将代码公开。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Forecasting-capturing-and-activation-of-carbon-dioxide-CO-2-Integration-of-Time-Series-Analysis-Machine-Learning-and-Material-Design"><a href="#Forecasting-capturing-and-activation-of-carbon-dioxide-CO-2-Integration-of-Time-Series-Analysis-Machine-Learning-and-Material-Design" class="headerlink" title="Forecasting, capturing and activation of carbon-dioxide (CO$_2$): Integration of Time Series Analysis, Machine Learning, and Material Design"></a>Forecasting, capturing and activation of carbon-dioxide (CO$_2$): Integration of Time Series Analysis, Machine Learning, and Material Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14374">http://arxiv.org/abs/2307.14374</a></li>
<li>repo_url: None</li>
<li>paper_authors: Suchetana Sadhukhan, Vivek Kumar Yadav</li>
<li>For: This study provides a comprehensive time series analysis of daily industry-specific, country-wise CO2 emissions from January 2019 to February 2023, with a focus on European countries (EU27 &amp; UK, Italy, Germany, Spain) and India.* Methods: The research uses near-real-time activity data from the Carbon Monitor research initiative, and performs a principal component analysis (PCA) to determine the key contributors to CO2 emissions. The study also employs a 7-day moving averaged dataset for further analysis and uses Long Short-Term Memory (LSTM) models to predict emissions.* Results: The study finds that the Power, Industry, and Ground Transport sectors account for a significant portion of the variance in the dataset, and the LSTM models achieve high efficiency with $R^2$ values ranging from 0.8242 to 0.995 for various countries and sectors. Additionally, the study proposes the use of scandium and boron&#x2F;aluminium-based thin films as exceptionally efficient materials for capturing CO2.<details>
<summary>Abstract</summary>
This study provides a comprehensive time series analysis of daily industry-specific, country-wise CO$_2$ emissions from January 2019 to February 2023. The research focuses on the Power, Industry, Ground Transport, Domestic Aviation, and International Aviation sectors in European countries (EU27 & UK, Italy, Germany, Spain) and India, utilizing near-real-time activity data from the Carbon Monitor research initiative. To identify regular emission patterns, the data from the year 2020 is excluded due to the disruptive effects caused by the COVID-19 pandemic. The study then performs a principal component analysis (PCA) to determine the key contributors to CO$_2$ emissions. The analysis reveals that the Power, Industry, and Ground Transport sectors account for a significant portion of the variance in the dataset. A 7-day moving averaged dataset is employed for further analysis to facilitate robust predictions. This dataset captures both short-term and long-term trends and enhances the quality of the data for prediction purposes. The study utilizes Long Short-Term Memory (LSTM) models on the 7-day moving averaged dataset to effectively predict emissions and provide insights for policy decisions, mitigation strategies, and climate change efforts. During the training phase, the stability and convergence of the LSTM models are ensured, which guarantees their reliability in the testing phase. The evaluation of the loss function indicates this reliability. The model achieves high efficiency, as demonstrated by $R^2$ values ranging from 0.8242 to 0.995 for various countries and sectors. Furthermore, there is a proposal for utilizing scandium and boron/aluminium-based thin films as exceptionally efficient materials for capturing CO$_2$ (with a binding energy range from -3.0 to -3.5 eV). These materials are shown to surpass the affinity of graphene and boron nitride sheets in this regard.
</details>
<details>
<summary>摘要</summary>
To facilitate robust predictions, a 7-day moving averaged dataset is employed for further analysis. This dataset captures both short-term and long-term trends and enhances the quality of the data for prediction purposes. The study utilizes Long Short-Term Memory (LSTM) models on the 7-day moving averaged dataset to effectively predict emissions and provide insights for policy decisions, mitigation strategies, and climate change efforts. The stability and convergence of the LSTM models are ensured during the training phase, which guarantees their reliability in the testing phase. The evaluation of the loss function indicates this reliability.The model achieves high efficiency, as demonstrated by $R^2$ values ranging from 0.8242 to 0.995 for various countries and sectors. Furthermore, the study proposes the use of scandium and boron/aluminum-based thin films as exceptionally efficient materials for capturing CO2. These materials are shown to surpass the affinity of graphene and boron nitride sheets in this regard.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/26/cs.LG_2023_07_26/" data-id="clmjn91l6005u0j889hhhhsye" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_07_26" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/26/cs.SD_2023_07_26/" class="article-date">
  <time datetime="2023-07-25T16:00:00.000Z" itemprop="datePublished">2023-07-26</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/26/cs.SD_2023_07_26/">cs.SD - 2023-07-26 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Say-Goodbye-to-RNN-T-Loss-A-Novel-CIF-based-Transducer-Architecture-for-Automatic-Speech-Recognition"><a href="#Say-Goodbye-to-RNN-T-Loss-A-Novel-CIF-based-Transducer-Architecture-for-Automatic-Speech-Recognition" class="headerlink" title="Say Goodbye to RNN-T Loss: A Novel CIF-based Transducer Architecture for Automatic Speech Recognition"></a>Say Goodbye to RNN-T Loss: A Novel CIF-based Transducer Architecture for Automatic Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14132">http://arxiv.org/abs/2307.14132</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tian-Hao Zhang, Dinghao Zhou, Guiping Zhong, Baoxiang Li</li>
<li>for: 提高 ASR 模型的效率和精度</li>
<li>methods: 提出一种新的 CIF-Transducer 模型，具有终端听写 Integrate-and-Fire 机制，从而减少计算复杂度并让预测网络扮演更重要的角色。同时，引入 Funnel-CIF、Context Blocks、Unified Gating 和 Bilinear Pooling 等网络结构和辅助训练策略来进一步提高性能。</li>
<li>results: 在 AISHELL-1 和 WenetSpeech 等 dataset 上进行了实验，显示 CIF-T 模型可以与 RNN-T 模型相比，并且具有较低的计算复杂度和更高的精度。<details>
<summary>Abstract</summary>
RNN-T models are widely used in ASR, which rely on the RNN-T loss to achieve length alignment between input audio and target sequence. However, the implementation complexity and the alignment-based optimization target of RNN-T loss lead to computational redundancy and a reduced role for predictor network, respectively. In this paper, we propose a novel model named CIF-Transducer (CIF-T) which incorporates the Continuous Integrate-and-Fire (CIF) mechanism with the RNN-T model to achieve efficient alignment. In this way, the RNN-T loss is abandoned, thus bringing a computational reduction and allowing the predictor network a more significant role. We also introduce Funnel-CIF, Context Blocks, Unified Gating and Bilinear Pooling joint network, and auxiliary training strategy to further improve performance. Experiments on the 178-hour AISHELL-1 and 10000-hour WenetSpeech datasets show that CIF-T achieves state-of-the-art results with lower computational overhead compared to RNN-T models.
</details>
<details>
<summary>摘要</summary>
RNN-T模型广泛应用于语音识别领域，它们依赖于RNN-T损失来实现输入音频和目标序列之间的长度对齐。然而，实现复杂性和对齐基于优化目标导致RNN-T模型中的计算循环和预测网络的role减小。在这篇论文中，我们提出了一种新的模型名为CIF-Transducer（CIF-T），它将Continuous Integrate-and-Fire（CIF）机制与RNN-T模型结合起来实现有效的对齐。这样，RNN-T损失可以被抛弃，从而实现计算减少和预测网络的更大角色。我们还引入了滤波器-CIF、上下文块、统一阀值和bilinear pooling的联合网络，以及辅助训练策略，以进一步提高性能。实验表明，CIF-T在AISHELL-1和WenetSpeech数据集上达到了状态机器的Result，同时计算负担相对较低。
</details></li>
</ul>
<hr>
<h2 id="The-Hidden-Dance-of-Phonemes-and-Visage-Unveiling-the-Enigmatic-Link-between-Phonemes-and-Facial-Features"><a href="#The-Hidden-Dance-of-Phonemes-and-Visage-Unveiling-the-Enigmatic-Link-between-Phonemes-and-Facial-Features" class="headerlink" title="The Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Link between Phonemes and Facial Features"></a>The Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Link between Phonemes and Facial Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13953">http://arxiv.org/abs/2307.13953</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liao Qu, Xianwei Zou, Xiang Li, Yandong Wen, Rita Singh, Bhiksha Raj</li>
<li>for: 本研究揭示了声音和脸部特征之间的潜在关系。传统的voice-face相关性研究通常使用长时间的声音输入，包括从声音生成脸像和从声音重建3D脸镜。但在voice-based犯罪调查中，可能有限的声音证据。此外，从生物学角度来看，每个语音段（phoneme）对应不同的空气流和脸部运动。因此，发现声音和脸部特征之间的隐藏关系是有利的。</li>
<li>methods: 本研究提出了一个细化的分析管道，用于探索声音和脸部之间的关系，即phonemes v.s. 脸部 anthropometric measurements (AM)。我们建立了每个声音-AM对的估计器，并通过假设检测来评估相关性。我们的结果表明，AMs在元音上更易预测，特别是与塞擦音相关。此外，我们发现，在某些AM的发音时，它们更易预测。</li>
<li>results: 我们的结果支持生物学中的相关性理论，并为未来的speech-face多模态学习奠定基础。<details>
<summary>Abstract</summary>
This work unveils the enigmatic link between phonemes and facial features. Traditional studies on voice-face correlations typically involve using a long period of voice input, including generating face images from voices and reconstructing 3D face meshes from voices. However, in situations like voice-based crimes, the available voice evidence may be short and limited. Additionally, from a physiological perspective, each segment of speech -- phoneme -- corresponds to different types of airflow and movements in the face. Therefore, it is advantageous to discover the hidden link between phonemes and face attributes. In this paper, we propose an analysis pipeline to help us explore the voice-face relationship in a fine-grained manner, i.e., phonemes v.s. facial anthropometric measurements (AM). We build an estimator for each phoneme-AM pair and evaluate the correlation through hypothesis testing. Our results indicate that AMs are more predictable from vowels compared to consonants, particularly with plosives. Additionally, we observe that if a specific AM exhibits more movement during phoneme pronunciation, it is more predictable. Our findings support those in physiology regarding correlation and lay the groundwork for future research on speech-face multimodal learning.
</details>
<details>
<summary>摘要</summary>
In this paper, we propose an analysis pipeline to explore the voice-face relationship in a fine-grained manner, specifically, phonemes versus facial anthropometric measurements (AM). We build an estimator for each phoneme-AM pair and evaluate the correlation through hypothesis testing. Our results show that AMs are more predictable from vowels compared to consonants, particularly with plosives. Additionally, we find that if a specific AM exhibits more movement during phoneme pronunciation, it is more predictable. Our findings support those in physiology regarding the correlation and lay the foundation for future research on speech-face multimodal learning.
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Voice-Face-Correlation-A-Geometry-View"><a href="#Rethinking-Voice-Face-Correlation-A-Geometry-View" class="headerlink" title="Rethinking Voice-Face Correlation: A Geometry View"></a>Rethinking Voice-Face Correlation: A Geometry View</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13948">http://arxiv.org/abs/2307.13948</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lxa9867/VAF">https://github.com/lxa9867/VAF</a></li>
<li>paper_authors: Xiang Li, Yandong Wen, Muqiao Yang, Jinglu Wang, Rita Singh, Bhiksha Raj</li>
<li>for:  investigate the capability of reconstructing 3D facial shape from voice from a geometry perspective without any semantic information.</li>
<li>methods: propose a voice-anthropometric measurement (AM)-face paradigm, which identifies predictable facial AMs from the voice and uses them to guide 3D face reconstruction.</li>
<li>results: significant correlations between voice and specific parts of the face geometry, such as the nasal cavity and cranium.<details>
<summary>Abstract</summary>
Previous works on voice-face matching and voice-guided face synthesis demonstrate strong correlations between voice and face, but mainly rely on coarse semantic cues such as gender, age, and emotion. In this paper, we aim to investigate the capability of reconstructing the 3D facial shape from voice from a geometry perspective without any semantic information. We propose a voice-anthropometric measurement (AM)-face paradigm, which identifies predictable facial AMs from the voice and uses them to guide 3D face reconstruction. By leveraging AMs as a proxy to link the voice and face geometry, we can eliminate the influence of unpredictable AMs and make the face geometry tractable. Our approach is evaluated on our proposed dataset with ground-truth 3D face scans and corresponding voice recordings, and we find significant correlations between voice and specific parts of the face geometry, such as the nasal cavity and cranium. Our work offers a new perspective on voice-face correlation and can serve as a good empirical study for anthropometry science.
</details>
<details>
<summary>摘要</summary>
previous works on voice-face matching and voice-guided face synthesis have shown strong correlations between voice and face, but mainly rely on coarse semantic cues such as gender, age, and emotion. in this paper, we aim to investigate the capability of reconstructing the 3D facial shape from voice from a geometry perspective without any semantic information. we propose a voice-anthropometric measurement (AM)-face paradigm, which identifies predictable facial AMs from the voice and uses them to guide 3D face reconstruction. by leveraging AMs as a proxy to link the voice and face geometry, we can eliminate the influence of unpredictable AMs and make the face geometry tractable. our approach is evaluated on our proposed dataset with ground-truth 3D face scans and corresponding voice recordings, and we find significant correlations between voice and specific parts of the face geometry, such as the nasal cavity and cranium. our work offers a new perspective on voice-face correlation and can serve as a good empirical study for anthropometry science.Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know and I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Perceptual-Quality-Enhancement-of-Sound-Field-Synthesis-Based-on-Combination-of-Pressure-and-Amplitude-Matching"><a href="#Perceptual-Quality-Enhancement-of-Sound-Field-Synthesis-Based-on-Combination-of-Pressure-and-Amplitude-Matching" class="headerlink" title="Perceptual Quality Enhancement of Sound Field Synthesis Based on Combination of Pressure and Amplitude Matching"></a>Perceptual Quality Enhancement of Sound Field Synthesis Based on Combination of Pressure and Amplitude Matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13941">http://arxiv.org/abs/2307.13941</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keisuke Kimura, Shoichi Koyama, Hiroshi Saruwatari</li>
<li>for: 提高听众对声场的 восприятие质量</li>
<li>methods: 使用压力和幅度匹配方法组合来减少高频 synthesis 错误，并使用声场的人类听觉特性来synthesize 横向声localization</li>
<li>results: 比较实验和听觉测试表明，提出的方法可以提高声场synthesized的 восприятие质量，比传统压力匹配方法更好。<details>
<summary>Abstract</summary>
A sound field synthesis method enhancing perceptual quality is proposed. Sound field synthesis using multiple loudspeakers enables spatial audio reproduction with a broad listening area; however, synthesis errors at high frequencies called spatial aliasing artifacts are unavoidable. To minimize these artifacts, we propose a method based on the combination of pressure and amplitude matching. On the basis of the human's auditory properties, synthesizing the amplitude distribution will be sufficient for horizontal sound localization. Furthermore, a flat amplitude response should be synthesized as much as possible to avoid coloration. Therefore, we apply amplitude matching, which is a method to synthesize the desired amplitude distribution with arbitrary phase distribution, for high frequencies and conventional pressure matching for low frequencies. Experimental results of numerical simulations and listening tests using a practical system indicated that the perceptual quality of the sound field synthesized by the proposed method was improved from that synthesized by pressure matching.
</details>
<details>
<summary>摘要</summary>
一种提高听觉质量的声场合成方法被提出。使用多个扬音器实现声场合成可以提供广泛的听众区域，但高频合成错误无法避免。为减少这些错误，我们提出基于压力和幅度匹配的方法。根据人类听觉特性，Synthesizing amplitude distribution是足够的 для水平声localization。此外，尽可能地Synthesizing平坦的幅度响应可以避免染色。因此，我们应用幅度匹配，即Synthesizing所需的幅度分布的任意相位分布，高频和普通压力匹配low frequency。实验结果表明，由提出的方法synthesize的听field质量与压力匹配相比有所提高。
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Interactions-between-Target-Positive-and-Negative-Information-for-Acoustic-Echo-Cancellation"><a href="#Exploring-the-Interactions-between-Target-Positive-and-Negative-Information-for-Acoustic-Echo-Cancellation" class="headerlink" title="Exploring the Interactions between Target Positive and Negative Information for Acoustic Echo Cancellation"></a>Exploring the Interactions between Target Positive and Negative Information for Acoustic Echo Cancellation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13888">http://arxiv.org/abs/2307.13888</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chang Han, Xinmeng Xu, Weiping Tu, Yuhong Yang, Yajie Liu</li>
<li>for: 降低干扰信号的干扰 echo 抑制（AEC），以便保留近端语音最少受到损害。</li>
<li>methods: 我们提出了一种新的AEC模型encoder-decoder架构，以质量指导的方式使用目标负信息（如干扰信号和特征）来帮助模型更好地分辨目标语音和干扰信号的模式。</li>
<li>results: 我们的CMNet模型在实验中表现出了较好的性能，比如最近的方法。<details>
<summary>Abstract</summary>
Acoustic echo cancellation (AEC) aims to remove interference signals while leaving near-end speech least distorted. As the indistinguishable patterns between near-end speech and interference signals, near-end speech can't be separated completely, causing speech distortion and interference signals residual. We observe that besides target positive information, e.g., ground-truth speech and features, the target negative information, such as interference signals and features, helps make pattern of target speech and interference signals more discriminative. Therefore, we present a novel AEC model encoder-decoder architecture with the guidance of negative information termed as CMNet. A collaboration module (CM) is designed to establish the correlation between the target positive and negative information in a learnable manner via three blocks: target positive, target negative, and interactive block. Experimental results demonstrate our CMNet achieves superior performance than recent methods.
</details>
<details>
<summary>摘要</summary>
宽band acoustic echo cancellation（AEC）目的是去除干扰信号，保留近端语音最小变形。由于干扰信号和近端语音干扰信号的模式相同，因此无法完全分离近端语音，导致语音扭曲和干扰信号剩下。我们发现，除了目标正面信息（如真实语音和特征）之外，目标负面信息（如干扰信号和特征）也有助于使target speech和干扰信号的模式更加分化。因此，我们提出了一种基于encoder-decoder架构的新型AEC模型，称为CMNet。协作模块（CM）通过三个块（目标正面、目标负面和互动块）来在学习方式下建立目标正面和负面信息之间的相关性。实验结果表明，我们的CMNet在latest方法之上具有更高的性能。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/26/cs.SD_2023_07_26/" data-id="clmjn91nj009p0j88azo3e5c5" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_07_26" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/26/eess.AS_2023_07_26/" class="article-date">
  <time datetime="2023-07-25T16:00:00.000Z" itemprop="datePublished">2023-07-26</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/26/eess.AS_2023_07_26/">eess.AS - 2023-07-26 22:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Sound-Field-Estimation-around-a-Rigid-Sphere-with-Physics-informed-Neural-Network"><a href="#Sound-Field-Estimation-around-a-Rigid-Sphere-with-Physics-informed-Neural-Network" class="headerlink" title="Sound Field Estimation around a Rigid Sphere with Physics-informed Neural Network"></a>Sound Field Estimation around a Rigid Sphere with Physics-informed Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14013">http://arxiv.org/abs/2307.14013</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingyu Chen, Fei Ma, Amy Bastine, Prasanga Samarasinghe, Huiyuan Sun</li>
<li>for: used to estimate the sound field around a rigid sphere with limited measurements</li>
<li>methods: uses a physics-informed neural network that incorporates physical knowledge and constraints from the Helmholtz equation and zero radial velocity condition</li>
<li>results: outperforms the spherical harmonic method and plane-wave decomposition method in terms of accuracy and fitting ability<details>
<summary>Abstract</summary>
Accurate estimation of the sound field around a rigid sphere necessitates adequate sampling on the sphere, which may not always be possible. To overcome this challenge, this paper proposes a method for sound field estimation based on a physics-informed neural network. This approach integrates physical knowledge into the architecture and training process of the network. In contrast to other learning-based methods, the proposed method incorporates additional constraints derived from the Helmholtz equation and the zero radial velocity condition on the rigid sphere. Consequently, it can generate physically feasible estimations without requiring a large dataset. In contrast to the spherical harmonic-based method, the proposed approach has better fitting abilities and circumvents the ill condition caused by truncation. Simulation results demonstrate the effectiveness of the proposed method in achieving accurate sound field estimations from limited measurements, outperforming the spherical harmonic method and plane-wave decomposition method.
</details>
<details>
<summary>摘要</summary>
<<SYS>>精度地估算固定球体周围的声场需要足够的采样在球体上，但这并不总是可能的。为了解决这个挑战，这篇论文提出了基于物理学习网络的声场估算方法。这种方法将物理知识integrated到网络体系和训练过程中。与其他学习基本方法不同，提出的方法在球体上添加了基于哈姆霍尔兹方程和零辐射速度条件的额外约束。因此，它可以生成物理可能的估算而无需大量数据。与圆柱声波分解方法不同，提出的方法具有更好的适应能力，并circumvent了由截断引起的糟糕条件。实验结果表明，提出的方法可以从有限测量数据中获得高精度的声场估算，超越圆柱声波分解方法和平面波分解方法。</SYS>>
</details></li>
</ul>
<hr>
<h2 id="Speech-representation-learning-Learning-bidirectional-encoders-with-single-view-multi-view-and-multi-task-methods"><a href="#Speech-representation-learning-Learning-bidirectional-encoders-with-single-view-multi-view-and-multi-task-methods" class="headerlink" title="Speech representation learning: Learning bidirectional encoders with single-view, multi-view, and multi-task methods"></a>Speech representation learning: Learning bidirectional encoders with single-view, multi-view, and multi-task methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00129">http://arxiv.org/abs/2308.00129</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qingming Tang</li>
<li>For: 本论文主要目标是为时空序列数据学习 repre sentation learning，以提高下游序列预测任务的性能。* Methods: 本论文使用了多种学习 Setting，包括supervised learning with auxiliary losses、Unsupervised learning、semi-supervised learning和多视图学习。同时，本论文还 explore multiple approaches for representation learning，包括 speech data。* Results: 本论文的研究发现，不同的学习 Setting和approaches可以获得不同的 repre sentation learning results。此外，本论文还发现一些findings可以在不同的Domain上应用。<details>
<summary>Abstract</summary>
This thesis focuses on representation learning for sequence data over time or space, aiming to improve downstream sequence prediction tasks by using the learned representations. Supervised learning has been the most dominant approach for training deep neural networks for learning good sequential representations. However, one limiting factor to scale supervised learning is the lack of enough annotated data. Motivated by this challenge, it is natural to explore representation learning methods that can utilize large amounts of unlabeled and weakly labeled data, as well as an additional data modality. I describe my broad study of representation learning for speech data. Unlike most other works that focus on a single learning setting, this thesis studies multiple settings: supervised learning with auxiliary losses, unsupervised learning, semi-supervised learning, and multi-view learning. Besides different learning problems, I also explore multiple approaches for representation learning. Though I focus on speech data, the methods described in this thesis can also be applied to other domains. Overall, the field of representation learning is developing rapidly. State-of-the-art results on speech related tasks are typically based on Transformers pre-trained with large-scale self-supervised learning, which aims to learn generic representations that can benefit multiple downstream tasks. Since 2020, large-scale pre-training has been the de facto choice to achieve good performance. This delayed thesis does not attempt to summarize and compare with the latest results on speech representation learning; instead, it presents a unique study on speech representation learning before the Transformer era, that covers multiple learning settings. Some of the findings in this thesis can still be useful today.
</details>
<details>
<summary>摘要</summary>
这个论文关注在时间或空间序列数据上进行表示学习，以提高下游序列预测任务的性能。supervised learning是训练深度神经网络以获得好的Sequential Representations的主要方法。然而，缺乏充足的标注数据是超vised learning的一个限制因素。为了解决这个挑战，我们可以探索使用大量未标注和弱标注数据，以及额外的数据模式来学习表示。我的研究涵盖了Speech数据的表示学习。与大多数其他工作一样，我们不仅关注单个学习环境，还研究了多种学习问题，包括supervised learning with auxiliary losses、unsupervised learning、semi-supervised learning和多视图学习。此外，我们还探索了不同的表示学习方法。尽管我们主要关注Speech数据，但所描述的方法也可以应用于其他领域。总的来说，表示学习领域在发展 rapidly，state-of-the-art results on speech related tasks通常基于Transformers预训练大规模自适应学习，旨在学习通用的表示，以便多个下游任务。自2020年以来，大规模预训练成为了下游任务的de facto选择。这个论文不尝试总结和与最新的Result on speech representation learning进行比较，而是提供了在Transformer时代之前的唯一研究Speech representation learning，涵盖了多种学习环境。一些本论文中的发现仍然有用。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/26/eess.AS_2023_07_26/" data-id="clmjn91p800dd0j883cbqgepf" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_07_26" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/26/eess.IV_2023_07_26/" class="article-date">
  <time datetime="2023-07-25T16:00:00.000Z" itemprop="datePublished">2023-07-26</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/26/eess.IV_2023_07_26/">eess.IV - 2023-07-26 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Artifact-Restoration-in-Histology-Images-with-Diffusion-Probabilistic-Models"><a href="#Artifact-Restoration-in-Histology-Images-with-Diffusion-Probabilistic-Models" class="headerlink" title="Artifact Restoration in Histology Images with Diffusion Probabilistic Models"></a>Artifact Restoration in Histology Images with Diffusion Probabilistic Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14262">http://arxiv.org/abs/2307.14262</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhenqi-he/artifusion">https://github.com/zhenqi-he/artifusion</a></li>
<li>paper_authors: Zhenqi He, Junjun He, Jin Ye, Yiqing Shen</li>
<li>for:  histological whole slide images (WSIs) restoration</li>
<li>methods:  denoising diffusion probabilistic model (ArtiFusion) with novel Swin-Transformer denoising architecture and time token scheme</li>
<li>results:  effective restoration of artifact-free regions with preserved tissue structures and stain style, as demonstrated through extensive evaluations<details>
<summary>Abstract</summary>
Histological whole slide images (WSIs) can be usually compromised by artifacts, such as tissue folding and bubbles, which will increase the examination difficulty for both pathologists and Computer-Aided Diagnosis (CAD) systems. Existing approaches to restoring artifact images are confined to Generative Adversarial Networks (GANs), where the restoration process is formulated as an image-to-image transfer. Those methods are prone to suffer from mode collapse and unexpected mistransfer in the stain style, leading to unsatisfied and unrealistic restored images. Innovatively, we make the first attempt at a denoising diffusion probabilistic model for histological artifact restoration, namely ArtiFusion.Specifically, ArtiFusion formulates the artifact region restoration as a gradual denoising process, and its training relies solely on artifact-free images to simplify the training complexity.Furthermore, to capture local-global correlations in the regional artifact restoration, a novel Swin-Transformer denoising architecture is designed, along with a time token scheme. Our extensive evaluations demonstrate the effectiveness of ArtiFusion as a pre-processing method for histology analysis, which can successfully preserve the tissue structures and stain style in artifact-free regions during the restoration. Code is available at https://github.com/zhenqi-he/ArtiFusion.
</details>
<details>
<summary>摘要</summary>
历史图像整体扫描图像（WSIs）通常会受到artefact的影响，如组织卷绕和气泡，这会增加病理学家和计算机支持诊断（CAD）系统的评估难度。现有的恢复artefact图像方法是基于生成对抗网络（GANs），其中恢复过程是形式化为图像-图像传输。这些方法容易受到模式坍塌和意外传输的问题，导致 restored 图像不满意和不真实。我们在这里做出了一个尝试，提出了一种杂样整合模型，称为ArtiFusion。具体来说，ArtiFusion 将artefact区域恢复视为一个渐进的杂样去噪过程，其训练仅仅基于artefact-free 图像，以简化训练复杂性。此外，为了捕捉区域artefact恢复中的局部-全局相关性，我们设计了一种新的Swin-Transformer杂样去噪架构，以及一种时间токен方案。我们的广泛评估表明ArtiFusion 作为历史图像分析的预处理方法，可以成功保持组织结构和染料风格在artefact-free 区域中，并且可以成功恢复artefact。代码可以在 <https://github.com/zhenqi-he/ArtiFusion> 上获取。
</details></li>
</ul>
<hr>
<h2 id="Visual-Saliency-Detection-in-Advanced-Driver-Assistance-Systems"><a href="#Visual-Saliency-Detection-in-Advanced-Driver-Assistance-Systems" class="headerlink" title="Visual Saliency Detection in Advanced Driver Assistance Systems"></a>Visual Saliency Detection in Advanced Driver Assistance Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03770">http://arxiv.org/abs/2308.03770</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francesco Rundo, Michael Sebastian Rundo, Concetto Spampinato</li>
<li>for: 这个研究旨在提供一种智能系统，用于评估司机的注意力水平和场景理解，以提高安全性。</li>
<li>methods: 该系统使用了 semantic segmentation 3D deep network，以及基于 PPG 信号的驾驶员睡眠状况检测。</li>
<li>results: 实验结果表明，该系统能够准确地评估司机的注意力水平和场景理解，并且可以提高安全性。<details>
<summary>Abstract</summary>
Visual Saliency refers to the innate human mechanism of focusing on and extracting important features from the observed environment. Recently, there has been a notable surge of interest in the field of automotive research regarding the estimation of visual saliency. While operating a vehicle, drivers naturally direct their attention towards specific objects, employing brain-driven saliency mechanisms that prioritize certain elements over others. In this investigation, we present an intelligent system that combines a drowsiness detection system for drivers with a scene comprehension pipeline based on saliency. To achieve this, we have implemented a specialized 3D deep network for semantic segmentation, which has been pretrained and tailored for processing the frames captured by an automotive-grade external camera. The proposed pipeline was hosted on an embedded platform utilizing the STA1295 core, featuring ARM A7 dual-cores, and embeds an hardware accelerator. Additionally, we employ an innovative biosensor embedded on the car steering wheel to monitor the driver drowsiness, gathering the PhotoPlethysmoGraphy (PPG) signal of the driver. A dedicated 1D temporal deep convolutional network has been devised to classify the collected PPG time-series, enabling us to assess the driver level of attentiveness. Ultimately, we compare the determined attention level of the driver with the corresponding saliency-based scene classification to evaluate the overall safety level. The efficacy of the proposed pipeline has been validated through extensive experimental results.
</details>
<details>
<summary>摘要</summary>
“视觉吸引力”指代人类在观察环境时自然地吸引注意力并提取重要特征。在汽车研究领域，最近有一场关注visual saliency的浪潮。驾驶时，司机会自然地将注意力集中在特定的对象上，使用大脑驱动的吸引力机制来优先级化元素。在这次研究中，我们提出了一个智能系统，其结合了驾驶员睡眠检测系统和基于吸引力的场景理解管道。为此，我们实施了一种特殊的3D深度网络 для semantic segmentation，该网络在自动汽车级外部摄像头捕捉的帧中进行了预训练和定制。我们的管道在使用STA1295核心的嵌入式平台上执行，该平台feature ARM A7双核心。此外，我们还使用了一种创新的车辙吸引监测系统，该系统通过捕捉驾驶员的PhotoPlethysmoGraphy（PPG）信号来监测驾驶员的睡眠状况。一个专门设计的1D时间深度卷积网络用于分类收集的PPG时间序列，以评估驾驶员的注意力水平。最后，我们将驾驶员确定的注意力水平与相应的吸引力基于场景分类进行比较，以评估整体安全水平。我们的实验结果表明，提案的管道具有良好的效果。
</details></li>
</ul>
<hr>
<h2 id="Non-Linear-Self-Augmentation-Deep-Pipeline-for-Cancer-Treatment-outcome-Prediction"><a href="#Non-Linear-Self-Augmentation-Deep-Pipeline-for-Cancer-Treatment-outcome-Prediction" class="headerlink" title="Non-Linear Self Augmentation Deep Pipeline for Cancer Treatment outcome Prediction"></a>Non-Linear Self Augmentation Deep Pipeline for Cancer Treatment outcome Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14398">http://arxiv.org/abs/2307.14398</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francesco Rundo, Concetto Spampinato, Michael Rundo</li>
<li>for: 这篇论文目的是提出一种新的预测治疗效果的策略，以帮助更好地选择和优化适合接受免疫治疗的患者。</li>
<li>methods: 该策略使用一种非线性细胞建筑和深入下渠分类器，从胸腹部CT图像提取和优化2D特征，以提高治疗效果预测的准确率。</li>
<li>results: 作者们通过一个具有推动力的实验研究，证明该策略的效果惊人，预测精度高达93%。<details>
<summary>Abstract</summary>
Immunotherapy emerges as promising approach for treating cancer. Encouraging findings have validated the efficacy of immunotherapy medications in addressing tumors, resulting in prolonged survival rates and notable reductions in toxicity compared to conventional chemotherapy methods. However, the pool of eligible patients for immunotherapy remains relatively small, indicating a lack of comprehensive understanding regarding the physiological mechanisms responsible for favorable treatment response in certain individuals while others experience limited benefits. To tackle this issue, the authors present an innovative strategy that harnesses a non-linear cellular architecture in conjunction with a deep downstream classifier. This approach aims to carefully select and enhance 2D features extracted from chest-abdomen CT images, thereby improving the prediction of treatment outcomes. The proposed pipeline has been meticulously designed to seamlessly integrate with an advanced embedded Point of Care system. In this context, the authors present a compelling case study focused on Metastatic Urothelial Carcinoma (mUC), a particularly aggressive form of cancer. Performance evaluation of the proposed approach underscores its effectiveness, with an impressive overall accuracy of approximately 93%
</details>
<details>
<summary>摘要</summary>
免疫疗法在肿瘤治疗中出现为可能的新方向。有力的证据证明免疫疗药在治疗肿瘤时的效果，导致了存活时间的延长和化学治疗方法相比的质量下降。然而，有效治疗的候选者人数仍然很小，这表明我们对治疗成功的生理机制还没有充分理解。为了解决这个问题，作者提出了一种创新的策略，利用非线性细胞体系和深入的下游分类器。这种方法的目的是通过精心选择和提高胸腹部CT图像中的2D特征，提高治疗结果预测的准确性。提案的管道已经仔细设计，能够与高级嵌入式Point of Care系统集成。在这个上下文中，作者提出了一个吸引人的案例研究，专门针对肿瘤肝癌（mUC）。研究表明，提案的方法的效果非常出色，总准确率大约为93%。
</details></li>
</ul>
<hr>
<h2 id="Tackling-Scattering-and-Reflective-Flare-in-Mobile-Camera-Systems-A-Raw-Image-Dataset-for-Enhanced-Flare-Removal"><a href="#Tackling-Scattering-and-Reflective-Flare-in-Mobile-Camera-Systems-A-Raw-Image-Dataset-for-Enhanced-Flare-Removal" class="headerlink" title="Tackling Scattering and Reflective Flare in Mobile Camera Systems: A Raw Image Dataset for Enhanced Flare Removal"></a>Tackling Scattering and Reflective Flare in Mobile Camera Systems: A Raw Image Dataset for Enhanced Flare Removal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14180">http://arxiv.org/abs/2307.14180</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fengbo Lan, Chang Wen Chen</li>
<li>for: 这个论文是为了提高移动设备的相机系统和图像质量，并解决激光和折射照明的问题。</li>
<li>methods: 这个论文使用了原始图像数据集，并评估了不同的移动设备和摄像头设定。</li>
<li>results: 该数据集包含了2,000个高品质的全分辨率原始图像对，并且可以进一步分解为30,000个对照图像 patches，以涵盖各种摄影状况。<details>
<summary>Abstract</summary>
The increasing prevalence of mobile devices has led to significant advancements in mobile camera systems and improved image quality. Nonetheless, mobile photography still grapples with challenging issues such as scattering and reflective flare. The absence of a comprehensive real image dataset tailored for mobile phones hinders the development of effective flare mitigation techniques. To address this issue, we present a novel raw image dataset specifically designed for mobile camera systems, focusing on flare removal. Capitalizing on the distinct properties of raw images, this dataset serves as a solid foundation for developing advanced flare removal algorithms. It encompasses a wide variety of real-world scenarios captured with diverse mobile devices and camera settings. The dataset comprises over 2,000 high-quality full-resolution raw image pairs for scattering flare and 1,100 for reflective flare, which can be further segmented into up to 30,000 and 2,200 paired patches, respectively, ensuring broad adaptability across various imaging conditions. Experimental results demonstrate that networks trained with synthesized data struggle to cope with complex lighting settings present in this real image dataset. We also show that processing data through a mobile phone's internal ISP compromises image quality while using raw image data presents significant advantages for addressing the flare removal problem. Our dataset is expected to enable an array of new research in flare removal and contribute to substantial improvements in mobile image quality, benefiting mobile photographers and end-users alike.
</details>
<details>
<summary>摘要</summary>
“由于移动设备的普及，移动摄像系统的技术和图像质量有了显著的进步。然而，移动摄影仍然面临着许多挑战，如散射和反射炙。由于没有专门为移动电话设计的全面真实图像集，因此发展效果的炙光除除射技术受到了限制。为解决这个问题，我们提供了一个新的Raw图像集，专门针对移动摄像系统，强调炙光除除。利用Raw图像的特点，这个集合作为开发高级炙光除除算法的坚实基础。它包括多种真实世界的场景，通过不同的移动设备和摄像设置捕捉。该集合包含了2,000多个高质量的全分辨率Raw图像对，用于散射炙，以及1,100个对，用于反射炙，可以进一步分解为30,000多个和2,200多个对的 patches。这确保了在多种捕捉条件下的广泛适用性。我们的实验结果表明，使用生成的数据进行训练的网络在真实图像集中表现不佳，而使用Raw图像数据具有显著的优势，能够有效地解决炙光除除问题。我们的集合预期会启动一系列新的研究，并为移动图像质量带来显著改善，对移动摄影者和用户都有利。”
</details></li>
</ul>
<hr>
<h2 id="Memory-Efficient-Graph-Convolutional-Networks-for-Object-Classification-and-Detection-with-Event-Cameras"><a href="#Memory-Efficient-Graph-Convolutional-Networks-for-Object-Classification-and-Detection-with-Event-Cameras" class="headerlink" title="Memory-Efficient Graph Convolutional Networks for Object Classification and Detection with Event Cameras"></a>Memory-Efficient Graph Convolutional Networks for Object Classification and Detection with Event Cameras</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14124">http://arxiv.org/abs/2307.14124</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kamil Jeziorek, Andrea Pinna, Tomasz Kryjak</li>
<li>for: This paper focuses on developing an efficient graph convolutional network (GCN) for event camera data, which is characterized by high temporal resolution, high dynamic range, low latency, and resistance to image blur.</li>
<li>methods: The paper compares different graph convolution operations and evaluates their performance in terms of execution time, number of trainable model parameters, data format requirements, and training outcomes.</li>
<li>results: The paper achieves a 450-fold reduction in the number of parameters for the feature extraction module and a 4.5-fold reduction in the size of the data representation while maintaining a classification accuracy of 52.3%, which is 6.3% higher compared to the operation used in state-of-the-art approaches. Additionally, the object detection architecture implemented in the paper achieved an accuracy of 53.7% <a href="mailto:&#109;&#65;&#x50;&#x40;&#x30;&#46;&#x35;">&#109;&#65;&#x50;&#x40;&#x30;&#46;&#x35;</a> and an execution rate of 82 graphs per second on the N-Caltech101 dataset.Here’s the simplified Chinese text for the three key information points:</li>
<li>for: 这篇论文关注了使用图 convolutional network (GCN) 处理事件摄像头数据，该数据具有高时间分辨率、高动态范围、低延迟和图像模糊鲁棒性。</li>
<li>methods: 论文比较了不同的图 convolution 操作，并评估其在执行时间、可训练模型参数数量、数据格式要求和训练结果等方面的性能。</li>
<li>results: 论文实现了Feature extraction模块中参数数量减少450倍，数据表示形式减少4.5倍，保持52.3%的分类精度，比state-of-the-art方法高6.3%。此外，在 N-Caltech101 数据集上，实现了Object detection 架构，达到了53.7% <a href="mailto:&#109;&#65;&#80;&#64;&#x30;&#46;&#53;">&#109;&#65;&#80;&#64;&#x30;&#46;&#53;</a> 的准确率和82个图像每秒执行速率。<details>
<summary>Abstract</summary>
Recent advances in event camera research emphasize processing data in its original sparse form, which allows the use of its unique features such as high temporal resolution, high dynamic range, low latency, and resistance to image blur. One promising approach for analyzing event data is through graph convolutional networks (GCNs). However, current research in this domain primarily focuses on optimizing computational costs, neglecting the associated memory costs. In this paper, we consider both factors together in order to achieve satisfying results and relatively low model complexity. For this purpose, we performed a comparative analysis of different graph convolution operations, considering factors such as execution time, the number of trainable model parameters, data format requirements, and training outcomes. Our results show a 450-fold reduction in the number of parameters for the feature extraction module and a 4.5-fold reduction in the size of the data representation while maintaining a classification accuracy of 52.3%, which is 6.3% higher compared to the operation used in state-of-the-art approaches. To further evaluate performance, we implemented the object detection architecture and evaluated its performance on the N-Caltech101 dataset. The results showed an accuracy of 53.7 % mAP@0.5 and reached an execution rate of 82 graphs per second.
</details>
<details>
<summary>摘要</summary>
Recent advances in event camera research emphasize processing data in its original sparse form, which allows the use of its unique features such as high temporal resolution, high dynamic range, low latency, and resistance to image blur. One promising approach for analyzing event data is through graph convolutional networks (GCNs). However, current research in this domain primarily focuses on optimizing computational costs, neglecting the associated memory costs. In this paper, we consider both factors together in order to achieve satisfying results and relatively low model complexity. For this purpose, we performed a comparative analysis of different graph convolution operations, considering factors such as execution time, the number of trainable model parameters, data format requirements, and training outcomes. Our results show a 450-fold reduction in the number of parameters for the feature extraction module and a 4.5-fold reduction in the size of the data representation while maintaining a classification accuracy of 52.3%, which is 6.3% higher compared to the operation used in state-of-the-art approaches. To further evaluate performance, we implemented the object detection architecture and evaluated its performance on the N-Caltech101 dataset. The results showed an accuracy of 53.7 % mAP@0.5 and reached an execution rate of 82 graphs per second.  Translation:最近的事件摄像头研究进展强调处理原始稀缺数据，利用其特有的特征，如高时间分辨率、高动态范围、低延迟和图像模糊抗衰减。一种承袭的方法是使用图像会议网络（GCNs）来分析事件数据。然而，当前研究领域主要关注计算成本，忽略相关的内存成本。在这篇论文中，我们同时考虑这两个因素，以实现满意的结果和相对较低的模型复杂度。为此，我们对不同的图像会议操作进行比较分析，考虑因素包括执行时间、可训练模型参数数量、数据格式要求和训练结果。我们的结果显示，在特征提取模块中减少了450倍的参数数量，并在数据表示中减少了4.5倍的大小，同时保持了52.3%的分类精度，与现有方法相比增加6.3%。为了进一步评估性能，我们实现了对象检测架构，并在N-Caltech101数据集上评估其性能。结果显示，在0.5的MAP@53.7%的情况下，执行速率达到82个图像每秒。
</details></li>
</ul>
<hr>
<h2 id="Periocular-biometrics-databases-algorithms-and-directions"><a href="#Periocular-biometrics-databases-algorithms-and-directions" class="headerlink" title="Periocular biometrics: databases, algorithms and directions"></a>Periocular biometrics: databases, algorithms and directions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14111">http://arxiv.org/abs/2307.14111</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fernando Alonso-Fernandez, Josef Bigun</li>
<li>for: 本文是一篇审查现有的周遭生物METRICS研究状况，提供了关于这一领域最重要的问题和现有文献的全面概述。</li>
<li>methods: 本文使用了周遭生物METRICS的多种方法，包括eyelids、lashes和eyebrows等特征提取方法，以及 gender classification和ethnicity classification等应用方法。</li>
<li>results: 本文提出了周遭生物METRICS的现状和未来发展趋势，并对现有文献进行了全面的概述和分析。<details>
<summary>Abstract</summary>
Periocular biometrics has been established as an independent modality due to concerns on the performance of iris or face systems in uncontrolled conditions. Periocular refers to the facial region in the eye vicinity, including eyelids, lashes and eyebrows. It is available over a wide range of acquisition distances, representing a trade-off between the whole face (which can be occluded at close distances) and the iris texture (which do not have enough resolution at long distances). Since the periocular region appears in face or iris images, it can be used also in conjunction with these modalities. Features extracted from the periocular region have been also used successfully for gender classification and ethnicity classification, and to study the impact of gender transformation or plastic surgery in the recognition performance. This paper presents a review of the state of the art in periocular biometric research, providing an insight of the most relevant issues and giving a thorough coverage of the existing literature. Future research trends are also briefly discussed.
</details>
<details>
<summary>摘要</summary>
《眼睛附近特征》已成为独立模式，由于肉眼或面部系统在不控制的情况下表现不佳。《眼睛附近特征》指的是脸部附近的眼睛区域，包括眼皮、毛发和毛发。它可以在各种获取距离范围内使用，表示距离整个脸部（可能会被近距离干扰）和眼球文字（没有足够的分辨率）之间的权衡。由于眼睛附近区域会出现在脸部或眼球图像中，因此它也可以与这些模式结合使用。从眼睛附近区域提取的特征已经成功地用于性别识别和种族识别，以及研究性别转换或整形手术对识别性的影响。这篇评论文章介绍了眼睛附近生物ometrics研究的现状，提供了最相关的问题的权衡和全面的文献评论。未来研究趋势也 briefly discussed。
</details></li>
</ul>
<hr>
<h2 id="Video-Decoding-Energy-Estimation-Using-Processor-Events"><a href="#Video-Decoding-Energy-Estimation-Using-Processor-Events" class="headerlink" title="Video Decoding Energy Estimation Using Processor Events"></a>Video Decoding Energy Estimation Using Processor Events</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14000">http://arxiv.org/abs/2307.14000</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christian Herglotz, André Kaup</li>
<li>for: 这篇论文是为了量化软件视频解码器的处理能量而写的。</li>
<li>methods: 该论文使用处理器事件如指令计数或缓存失败来准确估算软件视频解码器的处理能量。</li>
<li>results: 该论文通过对ARM基本评估平台进行能量测量，并使用专门的 profiling 软件对处理器事件进行计数，证明了我们的观察结果的一般可靠性。 用于不同的编码器和解码器实现，该方法可以准确地估算最新的视频编码标准HEVC和VP9中的真正解码能量，带有小于6%的平均估计误差。<details>
<summary>Abstract</summary>
In this paper, we show that processor events like instruction counts or cache misses can be used to accurately estimate the processing energy of software video decoders. Therefore, we perform energy measurements on an ARM-based evaluation platform and count processor level events using a dedicated profiling software. Measurements are performed for various codecs and decoder implementations to prove the general viability of our observations. Using the estimation method proposed in this paper, the true decoding energy for various recent video coding standards including HEVC and VP9 can be estimated with a mean estimation error that is smaller than 6%.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们证明处理器事件如指令计数或缓存miss可以准确地估计软件视频解码器的处理能量。因此，我们使用特定的 profiling 软件对ARM基本评估平台进行能量测量，并对不同的编码器和解码器实现进行测量。通过我们提出的估算方法，可以对最新的视频编码标准，包括HEVC和VP9，进行准确的估算， mean estimation error小于6%。
</details></li>
</ul>
<hr>
<h2 id="Hybrid-Representation-Enhanced-Sampling-for-Bayesian-Active-Learning-in-Musculoskeletal-Segmentation-of-Lower-Extremities"><a href="#Hybrid-Representation-Enhanced-Sampling-for-Bayesian-Active-Learning-in-Musculoskeletal-Segmentation-of-Lower-Extremities" class="headerlink" title="Hybrid Representation-Enhanced Sampling for Bayesian Active Learning in Musculoskeletal Segmentation of Lower Extremities"></a>Hybrid Representation-Enhanced Sampling for Bayesian Active Learning in Musculoskeletal Segmentation of Lower Extremities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13986">http://arxiv.org/abs/2307.13986</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ganping Li, Yoshito Otake, Mazen Soufi, Masashi Taniguchi, Masahide Yagi, Noriaki Ichihashi, Keisuke Uemura, Masaki Takao, Nobuhiko Sugano, Yoshinobu Sato<br>for: 降低医学图像分割任务中的手动标注成本methods: 使用 bayesian active learning 框架和 bayesian U-net，并采用混合表示性更新抽象策略，选择高密度和多样性的不确定样本进行手动修正，以优化最大化与未标注实例相似性，最小化与已有训练数据相似性。results: 在两个lower extremity（LE）数据集上，提出的方法与其他方法相比，在两种抽取规则下表现出超越或等效性，并且量化结果表明抽取规则的影响。我们的ablation study表明，将密度和多样性 criterion相结合使用，在musculoskeletal segmentation中表现出最佳性能。<details>
<summary>Abstract</summary>
Purpose: Obtaining manual annotations to train deep learning (DL) models for auto-segmentation is often time-consuming. Uncertainty-based Bayesian active learning (BAL) is a widely-adopted method to reduce annotation efforts. Based on BAL, this study introduces a hybrid representation-enhanced sampling strategy that integrates density and diversity criteria to save manual annotation costs by efficiently selecting the most informative samples.   Methods: The experiments are performed on two lower extremity (LE) datasets of MRI and CT images by a BAL framework based on Bayesian U-net. Our method selects uncertain samples with high density and diversity for manual revision, optimizing for maximal similarity to unlabeled instances and minimal similarity to existing training data. We assess the accuracy and efficiency using Dice and a proposed metric called reduced annotation cost (RAC), respectively. We further evaluate the impact of various acquisition rules on BAL performance and design an ablation study for effectiveness estimation.   Results: The proposed method showed superiority or non-inferiority to other methods on both datasets across two acquisition rules, and quantitative results reveal the pros and cons of the acquisition rules. Our ablation study in volume-wise acquisition shows that the combination of density and diversity criteria outperforms solely using either of them in musculoskeletal segmentation.   Conclusion: Our sampling method is proven efficient in reducing annotation costs in image segmentation tasks. The combination of the proposed method and our BAL framework provides a semi-automatic way for efficient annotation of medical image datasets.
</details>
<details>
<summary>摘要</summary>
目的：获取手动标注以训练深度学习（DL）模型的自动分割是经常占用时间。不确定性基于抽象学习（BAL）是一种广泛采用的方法，可以减少手动标注的努力。根据BAL，本研究引入了混合表示形式增强选择策略，将高密度和多样性的不确定样本选择为人工修改，以最大化与未标注实例的相似性，最小化与现有训练数据的相似性。方法：我们在两个下肢（LE）数据集上进行了MRI和CT图像的实验，使用基于抽象网络的BAL框架。我们的方法选择了不确定性高的高密度多样性样本进行人工修改，以优化最大化与未标注实例相似性，最小化与现有训练数据相似性。我们使用 dice和我们所提出的metriccalled减少注解成本（RAC）进行评价精度和效率。我们进一步evaluate了不同的获取规则对BAL性能的影响，并进行了效果鉴定研究。结果：我们的方法在两个数据集上表现出优异或等效于其他方法，并且量化结果表明了不同获取规则的优缺点。我们的拓展研究表明，将密度和多样性 критериria组合使用在musculoskeletal segmentation中表现出最佳效果。结论：我们的采样方法可以有效减少图像分割任务中的注解成本。将我们的采样方法与BAL框架结合使用，可以提供一种 semi-自动的方式，以便效率地注解医疗影像数据集。
</details></li>
</ul>
<hr>
<h2 id="Analysis-of-Video-Quality-Datasets-via-Design-of-Minimalistic-Video-Quality-Models"><a href="#Analysis-of-Video-Quality-Datasets-via-Design-of-Minimalistic-Video-Quality-Models" class="headerlink" title="Analysis of Video Quality Datasets via Design of Minimalistic Video Quality Models"></a>Analysis of Video Quality Datasets via Design of Minimalistic Video Quality Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13981">http://arxiv.org/abs/2307.13981</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Sun, Wen Wen, Xiongkuo Min, Long Lan, Guangtao Zhai, Kede Ma</li>
<li>for: 本研究旨在帮助理解现有的视频质量评估（VQA）数据集，以便更好地评估当前的视频质量评估模型。</li>
<li>methods: 本研究使用了最简单的视频质量评估模型，包括视频预处理（快速空间时间抑制）、空间质量分析器和选项性的时间质量分析器，以及质量回归器。</li>
<li>results: 研究发现大多数数据集受到容易的数据集问题的影响，一些甚至可以使用盲图质量评估（BIQA）解决方案。研究还比较了不同的模型变体在不同数据集上的性能，并对模型的设计决策进行了ablation研究。<details>
<summary>Abstract</summary>
Blind video quality assessment (BVQA) plays an indispensable role in monitoring and improving the end-users' viewing experience in various real-world video-enabled media applications. As an experimental field, the improvements of BVQA models have been measured primarily on a few human-rated VQA datasets. Thus, it is crucial to gain a better understanding of existing VQA datasets in order to properly evaluate the current progress in BVQA. Towards this goal, we conduct a first-of-its-kind computational analysis of VQA datasets via designing minimalistic BVQA models. By minimalistic, we restrict our family of BVQA models to build only upon basic blocks: a video preprocessor (for aggressive spatiotemporal downsampling), a spatial quality analyzer, an optional temporal quality analyzer, and a quality regressor, all with the simplest possible instantiations. By comparing the quality prediction performance of different model variants on eight VQA datasets with realistic distortions, we find that nearly all datasets suffer from the easy dataset problem of varying severity, some of which even admit blind image quality assessment (BIQA) solutions. We additionally justify our claims by contrasting our model generalizability on these VQA datasets, and by ablating a dizzying set of BVQA design choices related to the basic building blocks. Our results cast doubt on the current progress in BVQA, and meanwhile shed light on good practices of constructing next-generation VQA datasets and models.
</details>
<details>
<summary>摘要</summary>
视频质量评估（BVQA）在视频启用媒体应用中扮演了不可或缺的角色，负责监测和改进用户的观看体验。作为实验领域，BVQA模型的改进主要通过一些人类评分的视频质量评估 dataset（VQA dataset）进行评估。因此，更深刻地理解现有VQA dataset是非常重要的。为了实现这个目标，我们首次对VQA dataset进行了计算性的分析，通过设计最简的BVQA模型来评估现有VQA dataset的质量。我们的BVQA模型仅使用了基本块：视频预处理（为激进的空间时间采样）、空间质量分析器、可选的时间质量分析器和质量回归器，其中每个块都使用最简的实现。通过对不同模型变体在八个VQA dataset上的质量预测性能进行比较，我们发现大多数dataset受到不同程度的易于评估（Easy dataset problem），一些甚至可以使用盲图质量评估（BIQA）解决方案。此外，我们还对不同的BVQA设计选择进行了比较，并将其与不同的VQA dataset进行了对比。我们的结果表明，现有的BVQA进展并不如人们所期望的，同时也提供了构建下一代VQA dataset和模型的好做法。
</details></li>
</ul>
<hr>
<h2 id="A-real-time-material-breakage-detection-for-offshore-wind-turbines-based-on-improved-neural-network-algorithm"><a href="#A-real-time-material-breakage-detection-for-offshore-wind-turbines-based-on-improved-neural-network-algorithm" class="headerlink" title="A real-time material breakage detection for offshore wind turbines based on improved neural network algorithm"></a>A real-time material breakage detection for offshore wind turbines based on improved neural network algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13765">http://arxiv.org/abs/2307.13765</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yantong Liu</li>
<li>for: 这种研究旨在提高海上风电机维护效率，为可持续能源生产做出重要贡献。</li>
<li>methods: 这种方法使用了一种改进后的YOLOv8物体检测模型，以及一个卷积杂志注意模块（CBAM），以提高特征识别能力。</li>
<li>results: 研究发现，使用这种方法可以提高杂点检测稳定性，代表了重要的维护技术突破。<details>
<summary>Abstract</summary>
The integrity of offshore wind turbines, pivotal for sustainable energy generation, is often compromised by surface material defects. Despite the availability of various detection techniques, limitations persist regarding cost-effectiveness, efficiency, and applicability. Addressing these shortcomings, this study introduces a novel approach leveraging an advanced version of the YOLOv8 object detection model, supplemented with a Convolutional Block Attention Module (CBAM) for improved feature recognition. The optimized loss function further refines the learning process. Employing a dataset of 5,432 images from the Saemangeum offshore wind farm and a publicly available dataset, our method underwent rigorous testing. The findings reveal a substantial enhancement in defect detection stability, marking a significant stride towards efficient turbine maintenance. This study's contributions illuminate the path for future research, potentially revolutionizing sustainable energy practices.
</details>
<details>
<summary>摘要</summary>
Offshore风电机的完整性，对可再生能源生产是关键的，但这些材料表面的瑕疵常常会对其产生影响。虽然有各种检测技术可供选择，但这些技术受到成本、效率和应用限制。本研究提出了一种新的方法，利用进化版YOLOv8物体检测模型，加上卷积块注意模块（CBAM），以提高特征识别。通过优化损失函数，进一步适应学习。使用了5432张采集自韩国岛风电农场和公开available的数据集，我们的方法经过了严格的测试。发现的结果表明，我们的方法可以增强瑕疵检测稳定性，代表了可再生能源实践中的一个重要突破。本研究的贡献，照明了未来研究的道路，有望革命化可再生能源实践。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/26/eess.IV_2023_07_26/" data-id="clmjn91q400fn0j885t3g06qu" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/17/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/16/">16</a><a class="page-number" href="/page/17/">17</a><span class="page-number current">18</span><a class="page-number" href="/page/19/">19</a><a class="page-number" href="/page/20/">20</a><span class="space">&hellip;</span><a class="page-number" href="/page/33/">33</a><a class="extend next" rel="next" href="/page/19/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">73</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">69</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">32</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">42</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">112</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
