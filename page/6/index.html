
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/6/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-eess.IV_2023_08_14" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/14/eess.IV_2023_08_14/" class="article-date">
  <time datetime="2023-08-13T16:00:00.000Z" itemprop="datePublished">2023-08-14</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/14/eess.IV_2023_08_14/">eess.IV - 2023-08-14 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Automated-Ensemble-Based-Segmentation-of-Adult-Brain-Tumors-A-Novel-Approach-Using-the-BraTS-AFRICA-Challenge-Data"><a href="#Automated-Ensemble-Based-Segmentation-of-Adult-Brain-Tumors-A-Novel-Approach-Using-the-BraTS-AFRICA-Challenge-Data" class="headerlink" title="Automated Ensemble-Based Segmentation of Adult Brain Tumors: A Novel Approach Using the BraTS AFRICA Challenge Data"></a>Automated Ensemble-Based Segmentation of Adult Brain Tumors: A Novel Approach Using the BraTS AFRICA Challenge Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07214">http://arxiv.org/abs/2308.07214</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chiranjeewee Prasad Koirala, Sovesh Mohapatra, Advait Gosai, Gottfried Schlaug</li>
<li>for: 这篇论文旨在利用深度学习对多Modalities MRI数据进行脑肿瘤精准分割，以优化在 SUB-SAHARAN AFRICA 患者群体中的诊断和治疗。</li>
<li>methods: 这篇论文提出了一种ensemble方法，包括eleven个不同的变种，基于三种核心架构：UNet3D、ONet3D 和 SphereNet3D，以及修改的损失函数。</li>
<li>results: 研究发现， ensemble方法可以在多Modalities MRI数据上提高脑肿瘤分割精度，特别是在 age-和 population-based 分割模型方面。 Results表明， ensemble方法的 dice分数为 0.82、0.82 和 0.87 分别用于提高脑肿瘤、脑肿瘤核心和全脑肿瘤标签。<details>
<summary>Abstract</summary>
Brain tumors, particularly glioblastoma, continue to challenge medical diagnostics and treatments globally. This paper explores the application of deep learning to multi-modality magnetic resonance imaging (MRI) data for enhanced brain tumor segmentation precision in the Sub-Saharan Africa patient population. We introduce an ensemble method that comprises eleven unique variations based on three core architectures: UNet3D, ONet3D, SphereNet3D and modified loss functions. The study emphasizes the need for both age- and population-based segmentation models, to fully account for the complexities in the brain. Our findings reveal that the ensemble approach, combining different architectures, outperforms single models, leading to improved evaluation metrics. Specifically, the results exhibit Dice scores of 0.82, 0.82, and 0.87 for enhancing tumor, tumor core, and whole tumor labels respectively. These results underline the potential of tailored deep learning techniques in precisely segmenting brain tumors and lay groundwork for future work to fine-tune models and assess performance across different brain regions.
</details>
<details>
<summary>摘要</summary>
脑肿，特别是 glioblastoma，仍然在全球医疗领域面临挑战。这篇论文探讨了深度学习在多Modal magnetic resonance imaging（MRI）数据上进行脑肿分 segmentation的精度提高。我们引入了一个ensemble方法，包括11个独特的变种，基于三个核心体系：UNet3D、ONet3D和SphereNet3D，以及修改的损失函数。该研究强调了需要根据年龄和人口进行分 segmentation模型，以全面考虑脑肿的复杂性。我们的发现表明， ensemble方法，将不同的体系结合起来，表现出了提高评价指标的效果。特别是，结果显示 dice分数为0.82、0.82和0.87，用于加强肿体、肿体核心和整个肿体标签。这些结果高亮了深度学习技术在精度地分 segmentation脑肿的潜在优势，并为未来细化模型和评价不同脑区的表现提供了基础。
</details></li>
</ul>
<hr>
<h2 id="SAM-Meets-Robotic-Surgery-An-Empirical-Study-on-Generalization-Robustness-and-Adaptation"><a href="#SAM-Meets-Robotic-Surgery-An-Empirical-Study-on-Generalization-Robustness-and-Adaptation" class="headerlink" title="SAM Meets Robotic Surgery: An Empirical Study on Generalization, Robustness and Adaptation"></a>SAM Meets Robotic Surgery: An Empirical Study on Generalization, Robustness and Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07156">http://arxiv.org/abs/2308.07156</a></li>
<li>repo_url: None</li>
<li>paper_authors: An Wang, Mobarakol Islam, Mengya Xu, Yang Zhang, Hongliang Ren</li>
<li>for: 本研究探讨了Segment Anything Model（SAM）在 роботиче外科中的 robustness和零shot泛化能力。</li>
<li>methods: 本研究使用了SAM模型，并对其进行了多种场景探讨，包括提示和无提示的情况，以及不同的提示方法。</li>
<li>results: 研究发现，SAM模型在提示情况下表现出了很好的零shot泛化能力，但在无提示情况下或者 Instrument部分重叠时，模型很难正确地分类Instrument。此外，模型在复杂的外科手术场景下也表现不佳，尤其是在血液、反射、模糊和阴影等情况下。<details>
<summary>Abstract</summary>
The Segment Anything Model (SAM) serves as a fundamental model for semantic segmentation and demonstrates remarkable generalization capabilities across a wide range of downstream scenarios. In this empirical study, we examine SAM's robustness and zero-shot generalizability in the field of robotic surgery. We comprehensively explore different scenarios, including prompted and unprompted situations, bounding box and points-based prompt approaches, as well as the ability to generalize under corruptions and perturbations at five severity levels. Additionally, we compare the performance of SAM with state-of-the-art supervised models. We conduct all the experiments with two well-known robotic instrument segmentation datasets from MICCAI EndoVis 2017 and 2018 challenges. Our extensive evaluation results reveal that although SAM shows remarkable zero-shot generalization ability with bounding box prompts, it struggles to segment the whole instrument with point-based prompts and unprompted settings. Furthermore, our qualitative figures demonstrate that the model either failed to predict certain parts of the instrument mask (e.g., jaws, wrist) or predicted parts of the instrument as wrong classes in the scenario of overlapping instruments within the same bounding box or with the point-based prompt. In fact, SAM struggles to identify instruments in complex surgical scenarios characterized by the presence of blood, reflection, blur, and shade. Additionally, SAM is insufficiently robust to maintain high performance when subjected to various forms of data corruption. We also attempt to fine-tune SAM using Low-rank Adaptation (LoRA) and propose SurgicalSAM, which shows the capability in class-wise mask prediction without prompt. Therefore, we can argue that, without further domain-specific fine-tuning, SAM is not ready for downstream surgical tasks.
</details>
<details>
<summary>摘要</summary>
Segment Anything Model (SAM) 是一种基本模型 для semantics segmentation，它在各种下游场景中表现出了很好的普适性。在这个实验性研究中，我们研究了SAM在 робо学手术场景中的 robustness和零shot普适性。我们全面探讨了不同的场景，包括提示和无提示的情况，以及 bounding box 和点based提示方法。此外，我们还评估了 SAM 与当前顶尖指导学习模型的性能比较。我们在 MICCAI EndoVis 2017 和 2018 挑战中获得的两个 robotic instrument segmentation 数据集进行了所有的实验。我们的广泛的评估结果表明，虽然 SAM 在 bounding box 提示下显示出了remarkable零shot普适性，但在点based提示和无提示情况下，它很难正确地分类整个工具。此外，我们的资深图示表明，当工具在同一个 bounding box 内或者点based提示情况下，模型会预测错误的部分或者完全错过certain parts of the instrument mask（例如，钩子、臂部）。实际上，SAM 在复杂的外科手术场景中，即血肉泛滥、反射、模糊和抑吸的情况下，也很难分类工具。此外，SAM 对数据损害不具备充分的Robustness，无法保持高性能。为了解决这些问题，我们尝试使用 LoRA 进行微调，并提出了 SurgicalSAM，它可以在无提示情况下进行类别 маска预测。因此，我们可以 argue ，无需进一步领域特定的微调，SAM 不够准备于下游外科任务。
</details></li>
</ul>
<hr>
<h2 id="FocusFlow-Boosting-Key-Points-Optical-Flow-Estimation-for-Autonomous-Driving"><a href="#FocusFlow-Boosting-Key-Points-Optical-Flow-Estimation-for-Autonomous-Driving" class="headerlink" title="FocusFlow: Boosting Key-Points Optical Flow Estimation for Autonomous Driving"></a>FocusFlow: Boosting Key-Points Optical Flow Estimation for Autonomous Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07104">http://arxiv.org/abs/2308.07104</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhonghuayi/focusflow_official">https://github.com/zhonghuayi/focusflow_official</a></li>
<li>paper_authors: Zhonghua Yi, Hao Shi, Kailun Yang, Qi Jiang, Yaozu Ye, Ze Wang, Kaiwei Wang</li>
<li>for: 提高数据驱动的光流估算精度，特别是关键点方面。</li>
<li>methods: 提出点 clouds模型，并使用混合损失函数和特定点控制损失函数进行多个点精度的监督。 Condition Control Encoder (CCE) 将经典特征编码器替换为 Condition Feature Encoder (CFE)，并将帧特征编码器 (FFE) 与 CFE 进行控制相互传输。</li>
<li>results: 与普通的数据驱动光流估算方法相比，FocusFlow 在关键点方面提高了精度，并且可以与普通的特征编码器进行比较，在整个帧上也能达到类似或更高的性能。<details>
<summary>Abstract</summary>
Key-point-based scene understanding is fundamental for autonomous driving applications. At the same time, optical flow plays an important role in many vision tasks. However, due to the implicit bias of equal attention on all points, classic data-driven optical flow estimation methods yield less satisfactory performance on key points, limiting their implementations in key-point-critical safety-relevant scenarios. To address these issues, we introduce a points-based modeling method that requires the model to learn key-point-related priors explicitly. Based on the modeling method, we present FocusFlow, a framework consisting of 1) a mix loss function combined with a classic photometric loss function and our proposed Conditional Point Control Loss (CPCL) function for diverse point-wise supervision; 2) a conditioned controlling model which substitutes the conventional feature encoder by our proposed Condition Control Encoder (CCE). CCE incorporates a Frame Feature Encoder (FFE) that extracts features from frames, a Condition Feature Encoder (CFE) that learns to control the feature extraction behavior of FFE from input masks containing information of key points, and fusion modules that transfer the controlling information between FFE and CFE. Our FocusFlow framework shows outstanding performance with up to +44.5% precision improvement on various key points such as ORB, SIFT, and even learning-based SiLK, along with exceptional scalability for most existing data-driven optical flow methods like PWC-Net, RAFT, and FlowFormer. Notably, FocusFlow yields competitive or superior performances rivaling the original models on the whole frame. The source code will be available at https://github.com/ZhonghuaYi/FocusFlow_official.
</details>
<details>
<summary>摘要</summary>
“键点基本Scene理解是自动驾驶应用的基础。同时，光流扮演了许多视觉任务中重要的角色。然而，由于预设所有点都受到同等的注意力， класи型数据驱动的光流估计方法对键点的表现不如预期，从而限制它们在键点敏感的安全相关enario中的实现。为解决这些问题，我们介绍了一个点 cloud Modeling 方法，让模型Explicitly learn键点相关的先验知识。基于这个方法，我们发表了FocusFlow框架，包括以下几个部分：1) 一个mix损失函数和类别摄影损失函数以及我们提出的Conditional Point Control Loss (CPCL)函数 для多点精确指导; 2) 一个受控制的模型，将传统的Feature Encoder取代为我们提出的Condition Control Encoder (CCE)。CCE包括Frame Feature Encoder (FFE)、Condition Feature Encoder (CFE) 和融合模块，从输入mask中学习控制FFE的特征提取行为，并将控制信息转移到FFE和CFE之间。我们的FocusFlow框架在不同的键点上显示出惊人的表现，包括ORB、SIFT 和甚至学习式SiLK，并且具有卓越的扩展性，可以与现有的大多数数据驱动的光流方法相容。尤其是，FocusFlow在整幅图上表现竞争或超越原始模型。代码将在https://github.com/ZhonghuaYi/FocusFlow_official中公开。”
</details></li>
</ul>
<hr>
<h2 id="When-Deep-Learning-Meets-Multi-Task-Learning-in-SAR-ATR-Simultaneous-Target-Recognition-and-Segmentation"><a href="#When-Deep-Learning-Meets-Multi-Task-Learning-in-SAR-ATR-Simultaneous-Target-Recognition-and-Segmentation" class="headerlink" title="When Deep Learning Meets Multi-Task Learning in SAR ATR: Simultaneous Target Recognition and Segmentation"></a>When Deep Learning Meets Multi-Task Learning in SAR ATR: Simultaneous Target Recognition and Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07093">http://arxiv.org/abs/2308.07093</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenwei Wang, Jifang Pei, Zhiyong Wang, Yulin Huang, Junjie Wu, Haiguang Yang, Jianyu Yang</li>
<li>for: 本研究旨在提出一种基于多任务学习的Synthetic Aperture Radar（SAR）自动目标识别（ATR）方法，以实现精准的目标类别和精确的目标形态同时识别。</li>
<li>methods: 该方法基于深度学习理论，提出了一种新的多任务学习框架，包括两个主要结构：编码器和解码器。编码器用于抽取不同缩放级别的图像特征，而解码器则是一个任务特有的结构，通过使用这些抽取的特征进行适应性和优化地满足不同识别和分割任务的特征需求。</li>
<li>results: 基于Moving and Stationary Target Acquisition and Recognition（MSTAR）数据集的实验结果表明，提出的方法在识别和分割任务中具有优越性。<details>
<summary>Abstract</summary>
With the recent advances of deep learning, automatic target recognition (ATR) of synthetic aperture radar (SAR) has achieved superior performance. By not being limited to the target category, the SAR ATR system could benefit from the simultaneous extraction of multifarious target attributes. In this paper, we propose a new multi-task learning approach for SAR ATR, which could obtain the accurate category and precise shape of the targets simultaneously. By introducing deep learning theory into multi-task learning, we first propose a novel multi-task deep learning framework with two main structures: encoder and decoder. The encoder is constructed to extract sufficient image features in different scales for the decoder, while the decoder is a tasks-specific structure which employs these extracted features adaptively and optimally to meet the different feature demands of the recognition and segmentation. Therefore, the proposed framework has the ability to achieve superior recognition and segmentation performance. Based on the Moving and Stationary Target Acquisition and Recognition (MSTAR) dataset, experimental results show the superiority of the proposed framework in terms of recognition and segmentation.
</details>
<details>
<summary>摘要</summary>
Our approach is based on a deep learning framework with two main structures: an encoder and a decoder. The encoder is designed to extract comprehensive image features at multiple scales, while the decoder is a task-specific structure that adaptively and optimally utilizes these features to meet the diverse demands of recognition and segmentation. This allows our framework to achieve superior performance in both recognition and segmentation.Experimental results on the Moving and Stationary Target Acquisition and Recognition (MSTAR) dataset demonstrate the superiority of our proposed framework. With the ability to accurately recognize and precisely segment targets, our approach offers a significant improvement over traditional SAR ATR methods.
</details></li>
</ul>
<hr>
<h2 id="Deepbet-Fast-brain-extraction-of-T1-weighted-MRI-using-Convolutional-Neural-Networks"><a href="#Deepbet-Fast-brain-extraction-of-T1-weighted-MRI-using-Convolutional-Neural-Networks" class="headerlink" title="Deepbet: Fast brain extraction of T1-weighted MRI using Convolutional Neural Networks"></a>Deepbet: Fast brain extraction of T1-weighted MRI using Convolutional Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07003">http://arxiv.org/abs/2308.07003</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lukas Fisch, Stefan Zumdick, Carlotta Barkhau, Daniel Emden, Jan Ernsting, Ramona Leenings, Kelvin Sarink, Nils R. Winter, Benjamin Risse, Udo Dannlowski, Tim Hahn</li>
<li>for: 这个论文主要是为了提出一个高精度、快速的Magnetic Resonance Imaging（MRI）数据中的脑部分 segmentation工具，以取代传统的脑部分分类方法。</li>
<li>methods: 这个论文使用了现代的深度学习方法，包括LinkNet的现代UNet架构，在两个阶段预测过程中进行预测。这将提高了脑部分分类的性能，在测验中得到了一个新的州OF-THE-ART性能， median Dice score（DSC）为99.0%，比现有的模型高出2.2%和1.9%。</li>
<li>results: 这个论文的模型可以实现高精度的脑部分分类，Dice score（DSC）高于96.9%，并且更敏感于噪音。此外，这个模型可以将脑部分分类的时间加速到了约10倍，可以在低级硬件上处理一个数据仅需2秒钟。<details>
<summary>Abstract</summary>
Brain extraction in magnetic resonance imaging (MRI) data is an important segmentation step in many neuroimaging preprocessing pipelines. Image segmentation is one of the research fields in which deep learning had the biggest impact in recent years enabling high precision segmentation with minimal compute. Consequently, traditional brain extraction methods are now being replaced by deep learning-based methods. Here, we used a unique dataset comprising 568 T1-weighted (T1w) MR images from 191 different studies in combination with cutting edge deep learning methods to build a fast, high-precision brain extraction tool called deepbet. deepbet uses LinkNet, a modern UNet architecture, in a two stage prediction process. This increases its segmentation performance, setting a novel state-of-the-art performance during cross-validation with a median Dice score (DSC) of 99.0% on unseen datasets, outperforming current state of the art models (DSC = 97.8% and DSC = 97.9%). While current methods are more sensitive to outliers, resulting in Dice scores as low as 76.5%, deepbet manages to achieve a Dice score of > 96.9% for all samples. Finally, our model accelerates brain extraction by a factor of ~10 compared to current methods, enabling the processing of one image in ~2 seconds on low level hardware.
</details>
<details>
<summary>摘要</summary>
magnetic resonance imaging (MRI) 数据中的脑部EXTRACTION是许多神经成像预处理管道中重要的 segmentation 步骤。图像 segmentation 是深度学习在过去几年中对神经成像领域产生了最大的影响，使得传统的脑部EXTRACTION 方法被深度学习基于的方法所取代。在本文中，我们使用了568张T1-weighted (T1w) MRI图像和 cutting-edge deep learning 方法建立了一个快速、高精度的脑部EXTRACTION 工具 called deepbet。deepbet 使用了 LinkNet，一种现代的 U-Net 架构，在两个阶段预测过程中。这使得它的 segmentation 性能得到了提高，在跨验证中 median Dice 分数 (DSC) 为 99.0%，超过当前的状态对照模型 (DSC = 97.8%和DSC = 97.9%)。而现有方法更敏感于异常值，导致 Dice 分数只有 76.5%，而 deepbet 则能够达到 > 96.9% 的 Dice 分数 для所有样本。最后，我们的模型将脑部EXTRACTION 加速了约10倍，使得一个图像只需要 ~2秒钟的处理时间。
</details></li>
</ul>
<hr>
<h2 id="How-inter-rater-variability-relates-to-aleatoric-and-epistemic-uncertainty-a-case-study-with-deep-learning-based-paraspinal-muscle-segmentation"><a href="#How-inter-rater-variability-relates-to-aleatoric-and-epistemic-uncertainty-a-case-study-with-deep-learning-based-paraspinal-muscle-segmentation" class="headerlink" title="How inter-rater variability relates to aleatoric and epistemic uncertainty: a case study with deep learning-based paraspinal muscle segmentation"></a>How inter-rater variability relates to aleatoric and epistemic uncertainty: a case study with deep learning-based paraspinal muscle segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06964">http://arxiv.org/abs/2308.06964</a></li>
<li>repo_url: None</li>
<li>paper_authors: Parinaz Roshanzamir, Hassan Rivaz, Joshua Ahn, Hamza Mirza, Neda Naghdi, Meagan Anstruther, Michele C. Battié, Maryse Fortin, Yiming Xiao</li>
<li>for: This paper aims to explore the relationship between inter-rater variability and uncertainties in deep learning models for medical image segmentation, and to compare the performance of different label fusion strategies and DL models.</li>
<li>methods: The paper uses test-time augmentation (TTA), test-time dropout (TTD), and deep ensemble to measure aleatoric and epistemic uncertainties, and compares the performance of UNet and TransUNet with two label fusion strategies.</li>
<li>results: The study reveals the interplay between inter-rater variability and uncertainties, and shows that choices of label fusion strategies and DL models can affect the resulting segmentation performance.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文目标是探讨医学影像分割任务中间质量标注人员之间的差异对深度学习模型的不确定性的关系，以及不同的标签汇集策略和深度学习模型的性能比较。</li>
<li>methods: 该论文使用测试时数据增强（TTA）、测试时dropout（TTD）和深度ensemble来测量 aleatoric 和 epistemic 不确定性，并比较 UNet 和 TransUNet 的性能。</li>
<li>results: 研究发现，医学影像分割任务中间质量标注人员之间的差异会影响深度学习模型的不确定性，并且选择不同的标签汇集策略和深度学习模型可以affect segmentation性能。<details>
<summary>Abstract</summary>
Recent developments in deep learning (DL) techniques have led to great performance improvement in medical image segmentation tasks, especially with the latest Transformer model and its variants. While labels from fusing multi-rater manual segmentations are often employed as ideal ground truths in DL model training, inter-rater variability due to factors such as training bias, image noise, and extreme anatomical variability can still affect the performance and uncertainty of the resulting algorithms. Knowledge regarding how inter-rater variability affects the reliability of the resulting DL algorithms, a key element in clinical deployment, can help inform better training data construction and DL models, but has not been explored extensively. In this paper, we measure aleatoric and epistemic uncertainties using test-time augmentation (TTA), test-time dropout (TTD), and deep ensemble to explore their relationship with inter-rater variability. Furthermore, we compare UNet and TransUNet to study the impacts of Transformers on model uncertainty with two label fusion strategies. We conduct a case study using multi-class paraspinal muscle segmentation from T2w MRIs. Our study reveals the interplay between inter-rater variability and uncertainties, affected by choices of label fusion strategies and DL models.
</details>
<details>
<summary>摘要</summary>
In this paper, we use test-time augmentation (TTA), test-time dropout (TTD), and deep ensemble to measure aleatoric and epistemic uncertainties and explore their relationship with inter-rater variability. We also compare UNet and TransUNet to study the impact of Transformers on model uncertainty with two label fusion strategies. We conduct a case study using multi-class paraspinal muscle segmentation from T2w MRIs. Our study reveals the interplay between inter-rater variability and uncertainties, which is influenced by choices of label fusion strategies and DL models.
</details></li>
</ul>
<hr>
<h2 id="Robustness-Stress-Testing-in-Medical-Image-Classification"><a href="#Robustness-Stress-Testing-in-Medical-Image-Classification" class="headerlink" title="Robustness Stress Testing in Medical Image Classification"></a>Robustness Stress Testing in Medical Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06889">http://arxiv.org/abs/2308.06889</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mobarakol/robustness_stress_testing">https://github.com/mobarakol/robustness_stress_testing</a></li>
<li>paper_authors: Mobarakol Islam, Zeju Li, Ben Glocker</li>
<li>For: 评估医学图像疾病检测算法的 клиниче验证性能。* Methods: 使用进行挑战测试来评估模型的可靠性和不同类型和地区的表现差异。* Results: 表明了一些模型在不同的图像挑战测试中的Robustness和公平性。还发现预训练特征对下游的可靠性产生了重要的影响。<details>
<summary>Abstract</summary>
Deep neural networks have shown impressive performance for image-based disease detection. Performance is commonly evaluated through clinical validation on independent test sets to demonstrate clinically acceptable accuracy. Reporting good performance metrics on test sets, however, is not always a sufficient indication of the generalizability and robustness of an algorithm. In particular, when the test data is drawn from the same distribution as the training data, the iid test set performance can be an unreliable estimate of the accuracy on new data. In this paper, we employ stress testing to assess model robustness and subgroup performance disparities in disease detection models. We design progressive stress testing using five different bidirectional and unidirectional image perturbations with six different severity levels. As a use case, we apply stress tests to measure the robustness of disease detection models for chest X-ray and skin lesion images, and demonstrate the importance of studying class and domain-specific model behaviour. Our experiments indicate that some models may yield more robust and equitable performance than others. We also find that pretraining characteristics play an important role in downstream robustness. We conclude that progressive stress testing is a viable and important tool and should become standard practice in the clinical validation of image-based disease detection models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/14/eess.IV_2023_08_14/" data-id="clltau95d00dxcr889seedqrm" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_13" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/13/cs.LG_2023_08_13/" class="article-date">
  <time datetime="2023-08-12T16:00:00.000Z" itemprop="datePublished">2023-08-13</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/13/cs.LG_2023_08_13/">cs.LG - 2023-08-13 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Faithful-to-Whom-Questioning-Interpretability-Measures-in-NLP"><a href="#Faithful-to-Whom-Questioning-Interpretability-Measures-in-NLP" class="headerlink" title="Faithful to Whom? Questioning Interpretability Measures in NLP"></a>Faithful to Whom? Questioning Interpretability Measures in NLP</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06795">http://arxiv.org/abs/2308.06795</a></li>
<li>repo_url: None</li>
<li>paper_authors: Evan Crothers, Herna Viktor, Nathalie Japkowicz</li>
<li>for: 这篇论文主要是为了量化模型解释性的方法。</li>
<li>methods: 这篇论文使用了基于循环屏蔽输入Token的方法来计算 faithfulness 度量。</li>
<li>results: 论文发现现有的 faithfulness 度量不适合比较不同的神经网络文本分类器的解释性，因为屏蔽输入的样本频繁出现在训练时未看到的分布外。<details>
<summary>Abstract</summary>
A common approach to quantifying model interpretability is to calculate faithfulness metrics based on iteratively masking input tokens and measuring how much the predicted label changes as a result. However, we show that such metrics are generally not suitable for comparing the interpretability of different neural text classifiers as the response to masked inputs is highly model-specific. We demonstrate that iterative masking can produce large variation in faithfulness scores between comparable models, and show that masked samples are frequently outside the distribution seen during training. We further investigate the impact of adversarial attacks and adversarial training on faithfulness scores, and demonstrate the relevance of faithfulness measures for analyzing feature salience in text adversarial attacks. Our findings provide new insights into the limitations of current faithfulness metrics and key considerations to utilize them appropriately.
</details>
<details>
<summary>摘要</summary>
一种常见的方法量化模型解释性是计算基于 iteratively 掩码输入Token 的 faithfulness 度量。然而，我们显示这些度量不适合比较不同的神经网络文本分类器的解释性，因为掩码输入的响应是高度特定的。我们示出了 iterative 掩码可以导致大量的 faithfulness 分数变化，并且masked 样本 часто处于训练过程中未见过的分布之外。我们进一步研究了对 faithfulness 度量的影响，以及对文本 adversarial 攻击的分析。我们的发现为现有的 faithfulness 度量带来新的认识和关键考虑因素。
</details></li>
</ul>
<hr>
<h2 id="Neural-Networks-at-a-Fraction-with-Pruned-Quaternions"><a href="#Neural-Networks-at-a-Fraction-with-Pruned-Quaternions" class="headerlink" title="Neural Networks at a Fraction with Pruned Quaternions"></a>Neural Networks at a Fraction with Pruned Quaternions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06780">http://arxiv.org/abs/2308.06780</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/smlab-niser/quartLT22">https://github.com/smlab-niser/quartLT22</a></li>
<li>paper_authors: Sahel Mohammad Iqbal, Subhankar Mishra</li>
<li>for: 这个研究旨在测试适用于具有限制性的处理器的现代神经网络，以及使用高维度数据嵌入（如复数或四元数）来实现更好的优化。</li>
<li>methods: 这个研究使用了剪除来简化神经网络中的 Parameters，并在分类任务上进行了不同架构和数据集的实验。</li>
<li>results: 研究发现，在某些架构和任务上，将神经网络转换为复数值的版本可以在具有很高简化水平的情况下提供更高的准确性。例如，在CIFAR-10中使用Conv-4架构，将 Parameters 剪除至3%以下，复数值版本可以与原始模型相比提高超过10%的准确性。<details>
<summary>Abstract</summary>
Contemporary state-of-the-art neural networks have increasingly large numbers of parameters, which prevents their deployment on devices with limited computational power. Pruning is one technique to remove unnecessary weights and reduce resource requirements for training and inference. In addition, for ML tasks where the input data is multi-dimensional, using higher-dimensional data embeddings such as complex numbers or quaternions has been shown to reduce the parameter count while maintaining accuracy. In this work, we conduct pruning on real and quaternion-valued implementations of different architectures on classification tasks. We find that for some architectures, at very high sparsity levels, quaternion models provide higher accuracies than their real counterparts. For example, at the task of image classification on CIFAR-10 using Conv-4, at $3\%$ of the number of parameters as the original model, the pruned quaternion version outperforms the pruned real by more than $10\%$. Experiments on various network architectures and datasets show that for deployment in extremely resource-constrained environments, a sparse quaternion network might be a better candidate than a real sparse model of similar architecture.
</details>
<details>
<summary>摘要</summary>
现代神经网络的参数数量逐渐增加，这使得具有有限计算能力的设备上进行训练和推理变得困难。剪枝技术可以将不必要的权重从神经网络中移除，以降低训练和推理的资源需求。此外，在多维输入数据的机器学习任务上，使用高维数域嵌入，如复数或四元数，可以降低参数数量而保持准确性。在这种情况下，我们对不同的架构和数据集进行了剪枝和权重融合的实验。我们发现，在某些架构上，在非常高的精灵度水平上，使用四元数模型可以提高准确性，比如在CIFAR-10上使用Conv-4，在3%的参数数量下，剪枝后的四元数模型的准确性高于剪枝后的实数模型的10%以上。各种网络架构和数据集的实验表明，在极其有限的资源环境下，一个稀疏的四元数网络可能比同样的架构的实数稀疏网络更适合进行部署。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-on-Deep-Neural-Network-Pruning-Taxonomy-Comparison-Analysis-and-Recommendations"><a href="#A-Survey-on-Deep-Neural-Network-Pruning-Taxonomy-Comparison-Analysis-and-Recommendations" class="headerlink" title="A Survey on Deep Neural Network Pruning-Taxonomy, Comparison, Analysis, and Recommendations"></a>A Survey on Deep Neural Network Pruning-Taxonomy, Comparison, Analysis, and Recommendations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06767">http://arxiv.org/abs/2308.06767</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hrcheng1066/awesome-pruning">https://github.com/hrcheng1066/awesome-pruning</a></li>
<li>paper_authors: Hongrong Cheng, Miao Zhang, Javen Qinfeng Shi</li>
<li>for: 本文提供了一份涵盖现有研究工作的深度神经网络减少报告，以便更好地理解现有的方法和技术。</li>
<li>methods: 本文分类了现有的减少方法，包括一般&#x2F;特定速度减少、 WHEN TO PRUNE、HOW TO PRUNE 和减少与其他压缩技术的融合。</li>
<li>results: 本文进行了七对对照设定的比较分析，探讨了不同级别的监督和不同应用场景，以便更好地了解现有方法的共同点和区别。<details>
<summary>Abstract</summary>
Modern deep neural networks, particularly recent large language models, come with massive model sizes that require significant computational and storage resources. To enable the deployment of modern models on resource-constrained environments and accelerate inference time, researchers have increasingly explored pruning techniques as a popular research direction in neural network compression. However, there is a dearth of up-to-date comprehensive review papers on pruning. To address this issue, in this survey, we provide a comprehensive review of existing research works on deep neural network pruning in a taxonomy of 1) universal/specific speedup, 2) when to prune, 3) how to prune, and 4) fusion of pruning and other compression techniques. We then provide a thorough comparative analysis of seven pairs of contrast settings for pruning (e.g., unstructured/structured) and explore emerging topics, including post-training pruning, different levels of supervision for pruning, and broader applications (e.g., adversarial robustness) to shed light on the commonalities and differences of existing methods and lay the foundation for further method development. To facilitate future research, we build a curated collection of datasets, networks, and evaluations on different applications. Finally, we provide some valuable recommendations on selecting pruning methods and prospect promising research directions. We build a repository at https://github.com/hrcheng1066/awesome-pruning.
</details>
<details>
<summary>摘要</summary>
现代深度神经网络，特别是最近的大型语言模型，具有庞大的计算和存储资源需求。为实现资源有限环境中部署现代模型和加速推理时间，研究人员已经不断探索剪裁技术作为神经网络压缩的流行研究方向。然而，目前的相关评论综述缺乏。为解决这问题，在这篇评论中，我们提供了一份全面的评论综述，分为以下四个方面：1) 通用/特定加速，2) 何时剪裁，3) 如何剪裁，和4) 剪裁与其他压缩技术的融合。然后，我们对七对对比设定进行了仔细的比较分析（例如，无结构/结构），并探讨了emerging topics，如后处理剪裁、不同水平的监督剪裁和更广泛的应用（例如，防御性鲁棒性），以抛光现有方法的相似和不同，并为未来的研究铺垫基础。为便于未来的研究，我们建立了一个 curaated 的数据集、网络和评估集。最后，我们提供了一些有价值的建议，包括选择剪裁方法和未来研究方向。我们在 GitHub 上建立了一个存储库，请参考 <https://github.com/hrcheng1066/awesome-pruning>。
</details></li>
</ul>
<hr>
<h2 id="Conic-Descent-Redux-for-Memory-Efficient-Optimization"><a href="#Conic-Descent-Redux-for-Memory-Efficient-Optimization" class="headerlink" title="Conic Descent Redux for Memory-Efficient Optimization"></a>Conic Descent Redux for Memory-Efficient Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07343">http://arxiv.org/abs/2308.07343</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bingcong Li, Georgios B. Giannakis</li>
<li>for: 该论文旨在提高首项壳programming的效率和精度，并应用于信号处理和机器学习领域。</li>
<li>methods: 该论文提出了三个方面的改进：一是具有直观的几何解释，二是基于对偶问题的理论基础，三是一种新的批处理算法。</li>
<li>results: 研究发现，首项壳programming可以通过增加矩阵的权重来加速对偶解释，并且可以通过采用偏好的初始值来加速搜索过程。<details>
<summary>Abstract</summary>
Conic programming has well-documented merits in a gamut of signal processing and machine learning tasks. This contribution revisits a recently developed first-order conic descent (CD) solver, and advances it in three aspects: intuition, theory, and algorithmic implementation. It is found that CD can afford an intuitive geometric derivation that originates from the dual problem. This opens the door to novel algorithmic designs, with a momentum variant of CD, momentum conic descent (MOCO) exemplified. Diving deeper into the dual behavior CD and MOCO reveals: i) an analytically justified stopping criterion; and, ii) the potential to design preconditioners to speed up dual convergence. Lastly, to scale semidefinite programming (SDP) especially for low-rank solutions, a memory efficient MOCO variant is developed and numerically validated.
</details>
<details>
<summary>摘要</summary>
带有较好的记录的圆形编程在信号处理和机器学习任务中具有良好的优点。本贡献将最近开发的首览圆形下降（CD）解决方案进行三个方面的提高：直观、理论和算法实现。研究发现CD可以从对准问题的 dual 问题中得到直观的几何 derivation，这打开了新的算法设计的门户，例如旋转圆形下降（MOCO）。钻 deeper into CD和MOCO的双重行为，发现：i) 可以分析正确的停止标准；ii) 可以设计加速对准速度的预conditioners。最后，为优化semidefinite程序（SDP），尤其是低维解决方案，我们开发了内存高效的MOCO变体并 NUMERICALLY 验证了其正确性。
</details></li>
</ul>
<hr>
<h2 id="Few-shot-Class-incremental-Learning-A-Survey"><a href="#Few-shot-Class-incremental-Learning-A-Survey" class="headerlink" title="Few-shot Class-incremental Learning: A Survey"></a>Few-shot Class-incremental Learning: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06764">http://arxiv.org/abs/2308.06764</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinghua Zhang, Li Liu, Olli Silven, Matti Pietikäinen, Dewen Hu</li>
<li>for: 这篇论文的目的是为了提供对几拟学习（Few-shot Class-Incremental Learning，FSCIL）的系统性和全面的综述。</li>
<li>methods: 这篇论文使用了多种方法来探讨FSCIL，包括问题定义、主要挑战的不可靠的实验准确风险和稳定性-多样性矛盾、通用方案和相关的增量学习和几拟学习方法。</li>
<li>results: 这篇论文提供了各种FSCIL中的分类方法和对象检测方法，包括数据基于、结构基于和优化基于的方法，以及 anchor-free和 anchor-based的对象检测方法。<details>
<summary>Abstract</summary>
Few-shot Class-Incremental Learning (FSCIL) presents a unique challenge in machine learning, as it necessitates the continuous learning of new classes from sparse labeled training samples without forgetting previous knowledge. While this field has seen recent progress, it remains an active area of exploration. This paper aims to provide a comprehensive and systematic review of FSCIL. In our in-depth examination, we delve into various facets of FSCIL, encompassing the problem definition, the discussion of primary challenges of unreliable empirical risk minimization and the stability-plasticity dilemma, general schemes, and relevant problems of incremental learning and few-shot learning. Besides, we offer an overview of benchmark datasets and evaluation metrics. Furthermore, we introduce the classification methods in FSCIL from data-based, structure-based, and optimization-based approaches and the object detection methods in FSCIL from anchor-free and anchor-based approaches. Beyond these, we illuminate several promising research directions within FSCIL that merit further investigation.
</details>
<details>
<summary>摘要</summary>
《几个shot类增长学习（FSCIL）》是机器学习中的一个特殊挑战，它需要不断学习新的类型从罕见的标签训练样本中学习，而不会忘记之前的知识。尽管这个领域已经有了一定的进步，但仍然是一个活跃的研究领域。本文的目的是提供了FSCIL的全面和系统性的综述。在我们的深入检查中，我们探讨了FSCIL的各个方面，包括问题定义、不可靠的实际风险最小化和稳定-柔软之间的矛盾、通用方案和相关的增量学习和几个shot学习问题。此外，我们介绍了FSCIL中的数据集和评价指标。此外，我们还介绍了FSCIL中的分类方法，包括数据基于、结构基于和优化基于的方法，以及对象检测方法，包括固定和无固定的方法。此外，我们还释明了FSCIL中的一些有前途的研究方向。
</details></li>
</ul>
<hr>
<h2 id="Discovering-the-Symptom-Patterns-of-COVID-19-from-Recovered-and-Deceased-Patients-Using-Apriori-Association-Rule-Mining"><a href="#Discovering-the-Symptom-Patterns-of-COVID-19-from-Recovered-and-Deceased-Patients-Using-Apriori-Association-Rule-Mining" class="headerlink" title="Discovering the Symptom Patterns of COVID-19 from Recovered and Deceased Patients Using Apriori Association Rule Mining"></a>Discovering the Symptom Patterns of COVID-19 from Recovered and Deceased Patients Using Apriori Association Rule Mining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06763">http://arxiv.org/abs/2308.06763</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Dehghani, Zahra Yazdanparast, Mobin Mohammadi</li>
<li>for: 本研究旨在利用关联规则挖掘技术对 COVID-19 患者症状进行分析，以提供临床医生管理疾病的有用信息。</li>
<li>methods: 本研究使用 Apriori 算法进行关联规则挖掘，分析 COVID-19 患者2875条病例记录，并发现最常见的症状为呼吸困难（72%）、咳嗽（64%）、发热（59%）、衰弱（18%）、 мышьяк（14.5%）和喉咙痛（12%）。</li>
<li>results: 本研究发现，Apriori 算法可以帮助临床医生更好地理解 COVID-19 疾病的表现形式，并提供有价值的信息来帮助他们更好地诊断和治疗疾病。<details>
<summary>Abstract</summary>
The COVID-19 pandemic has a devastating impact globally, claiming millions of lives and causing significant social and economic disruptions. In order to optimize decision-making and allocate limited resources, it is essential to identify COVID-19 symptoms and determine the severity of each case. Machine learning algorithms offer a potent tool in the medical field, particularly in mining clinical datasets for useful information and guiding scientific decisions. Association rule mining is a machine learning technique for extracting hidden patterns from data. This paper presents an application of association rule mining based Apriori algorithm to discover symptom patterns from COVID-19 patients. The study, using 2875 records of patient, identified the most common symptoms as apnea (72%), cough (64%), fever (59%), weakness (18%), myalgia (14.5%), and sore throat (12%). The proposed method provides clinicians with valuable insight into disease that can assist them in managing and treating it effectively.
</details>
<details>
<summary>摘要</summary>
COVID-19 大流行对全球造成了毁灭性的影响，负死亡人数和社会经济秩序受到了重大的影响。为了优化决策和分配有限的资源，必须能够识别 COVID-19 的症状和每个患者的严重程度。机器学习算法在医疗领域提供了一种极其有用的工具，特别是在挖掘医疗数据中找到有用信息并导引科学决策。在本文中，我们使用 Apriori 算法来应用关联规则挖掘技术，从 COVID-19 患者的记录中挖掘症状模式。研究使用了 2875 个病人记录，并发现最常见的症状为呼吸停止（72%）、咳嗽（64%）、发热（59%）、衰竭（18%）、肌肉疼痛（14.5%）和喉咙痛（12%）。该方法为临床医生提供了有价值的病理知识，可以帮助他们更好地诊断和治疗疾病。
</details></li>
</ul>
<hr>
<h2 id="Heterogeneous-Multi-Agent-Reinforcement-Learning-via-Mirror-Descent-Policy-Optimization"><a href="#Heterogeneous-Multi-Agent-Reinforcement-Learning-via-Mirror-Descent-Policy-Optimization" class="headerlink" title="Heterogeneous Multi-Agent Reinforcement Learning via Mirror Descent Policy Optimization"></a>Heterogeneous Multi-Agent Reinforcement Learning via Mirror Descent Policy Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06741">http://arxiv.org/abs/2308.06741</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Mehdi Nasiri, Mansoor Rezghi</li>
<li>for: 这个论文旨在解决多Agent Reinforcement Learning（MARL）中协同努力的挑战，其中Agent具有不同能力和个人策略。</li>
<li>methods: 该论文提出了一种基于镜像下降法的多Agent策略优化算法（HAMDPO），利用多Agent优化减少问题中的策略更新，保证总体性能提高。</li>
<li>results: 该论文通过在Multi-Agent MuJoCo和StarCraftII任务上评估HAMDPO算法，并证明其在相比之前的状态静态算法（HATRPO和HAPPO）的稳定性和性能提高。<details>
<summary>Abstract</summary>
This paper presents an extension of the Mirror Descent method to overcome challenges in cooperative Multi-Agent Reinforcement Learning (MARL) settings, where agents have varying abilities and individual policies. The proposed Heterogeneous-Agent Mirror Descent Policy Optimization (HAMDPO) algorithm utilizes the multi-agent advantage decomposition lemma to enable efficient policy updates for each agent while ensuring overall performance improvements. By iteratively updating agent policies through an approximate solution of the trust-region problem, HAMDPO guarantees stability and improves performance. Moreover, the HAMDPO algorithm is capable of handling both continuous and discrete action spaces for heterogeneous agents in various MARL problems. We evaluate HAMDPO on Multi-Agent MuJoCo and StarCraftII tasks, demonstrating its superiority over state-of-the-art algorithms such as HATRPO and HAPPO. These results suggest that HAMDPO is a promising approach for solving cooperative MARL problems and could potentially be extended to address other challenging problems in the field of MARL.
</details>
<details>
<summary>摘要</summary>
Here is the translation in Simplified Chinese:这篇论文提出了一种新的方法 called Heterogeneous-Agent Mirror Descent Policy Optimization (HAMDPO)，用于解决合作多代理演算学习（MARL）中的挑战，其中代理有不同的能力和个人策略。HAMDPO算法使用多代理优势分解 Lemma 来有效地更新代理策略，并保证总性的性能提高。通过迭代更新代理策略的近似解决方案，HAMDPO算法保证稳定性和性能提高。此外，该算法可以处理不同类型的动作空间，包括连续和离散动作空间，并在多种 MARL 问题中进行应用。作者们在 Multi-Agent MuJoCo 和 StarCraftII 任务上评估了 HAMDPO 算法，并证明其在当前状态的算法中具有优势，例如 HATRPO 和 HAPPO 等算法。这些结果表明，HAMDPO 是一种有前途的方法，可以解决合作 MARL 问题，并可能扩展到其他难题。
</details></li>
</ul>
<hr>
<h2 id="Weighted-Sparse-Partial-Least-Squares-for-Joint-Sample-and-Feature-Selection"><a href="#Weighted-Sparse-Partial-Least-Squares-for-Joint-Sample-and-Feature-Selection" class="headerlink" title="Weighted Sparse Partial Least Squares for Joint Sample and Feature Selection"></a>Weighted Sparse Partial Least Squares for Joint Sample and Feature Selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06740">http://arxiv.org/abs/2308.06740</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wenwenmin/wspls">https://github.com/wenwenmin/wspls</a></li>
<li>paper_authors: Wenwen Min, Taosheng Xu, Chris Ding</li>
<li>For: The paper proposes a method for joint sample and feature selection in data fusion using sparse partial least squares (sPLS) with $\ell_\infty&#x2F;\ell_0$-norm constrained weighted sparse PLS (wsPLS) and extends it to multi-view data fusion.* Methods: The proposed method uses the $\ell_\infty&#x2F;\ell_0$-norm constrains to select a subset of samples and the Kurdyka-\L{ojasiewicz}~property to ensure global convergence of the algorithm.* Results: The proposed method is demonstrated to be efficient through numerical and biomedical data experiments, and is shown to outperform traditional sPLS methods in terms of computational efficiency and accuracy.<details>
<summary>Abstract</summary>
Sparse Partial Least Squares (sPLS) is a common dimensionality reduction technique for data fusion, which projects data samples from two views by seeking linear combinations with a small number of variables with the maximum variance. However, sPLS extracts the combinations between two data sets with all data samples so that it cannot detect latent subsets of samples. To extend the application of sPLS by identifying a specific subset of samples and remove outliers, we propose an $\ell_\infty/\ell_0$-norm constrained weighted sparse PLS ($\ell_\infty/\ell_0$-wsPLS) method for joint sample and feature selection, where the $\ell_\infty/\ell_0$-norm constrains are used to select a subset of samples. We prove that the $\ell_\infty/\ell_0$-norm constrains have the Kurdyka-\L{ojasiewicz}~property so that a globally convergent algorithm is developed to solve it. Moreover, multi-view data with a same set of samples can be available in various real problems. To this end, we extend the $\ell_\infty/\ell_0$-wsPLS model and propose two multi-view wsPLS models for multi-view data fusion. We develop an efficient iterative algorithm for each multi-view wsPLS model and show its convergence property. As well as numerical and biomedical data experiments demonstrate the efficiency of the proposed methods.
</details>
<details>
<summary>摘要</summary>
“稀疏部分最小倍数（sPLS）是一种常见的维度减少技术 для数据融合，它通过寻找两个视图中数据样本之间的线性组合来降低维度。然而，sPLS会捕捉两个数据集中所有数据样本的组合，因此无法检测隐藏的样本subset。为了扩展sPLS的应用，我们提出了一种使用 $\ell_\infty/\ell_0$-norm constrained weighted sparse PLS（$\ell_\infty/\ell_0$-wsPLS）方法，该方法可以同时进行样本选择和特征选择。我们证明了 $\ell_\infty/\ell_0$-norm constrains possess Kurdyka-\L{ojasiewicz} 性质，因此可以开发一个全球收敛的算法来解决它。此外，多视图数据中可以有同一个样本集。为此，我们扩展了 $\ell_\infty/\ell_0$-wsPLS 模型，并提出了两种多视图 wsPLS 模型 для多视图数据融合。我们开发了一个高效的迭代算法，并证明其收敛性。numerical和生物医学数据实验表明提出的方法的效率。”Note: Simplified Chinese is a simplified version of Chinese that is used in mainland China and is different from Traditional Chinese, which is used in Taiwan and other countries.
</details></li>
</ul>
<hr>
<h2 id="Probabilistic-Imputation-for-Time-series-Classification-with-Missing-Data"><a href="#Probabilistic-Imputation-for-Time-series-Classification-with-Missing-Data" class="headerlink" title="Probabilistic Imputation for Time-series Classification with Missing Data"></a>Probabilistic Imputation for Time-series Classification with Missing Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06738">http://arxiv.org/abs/2308.06738</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuneg11/SupNotMIWAE-with-ObsDropout">https://github.com/yuneg11/SupNotMIWAE-with-ObsDropout</a></li>
<li>paper_authors: SeungHyun Kim, Hyunsu Kim, EungGu Yun, Hwangrae Lee, Jaehun Lee, Juho Lee</li>
<li>for: 用于处理多变量时间序列数据中的缺失值</li>
<li>methods: 使用深度生成模型进行缺失值填充，并使用分类器进行信号分类</li>
<li>results: 通过实验表明，该方法可以有效地处理多变量时间序列数据中的缺失值，并且可以提高信号分类的准确率<details>
<summary>Abstract</summary>
Multivariate time series data for real-world applications typically contain a significant amount of missing values. The dominant approach for classification with such missing values is to impute them heuristically with specific values (zero, mean, values of adjacent time-steps) or learnable parameters. However, these simple strategies do not take the data generative process into account, and more importantly, do not effectively capture the uncertainty in prediction due to the multiple possibilities for the missing values. In this paper, we propose a novel probabilistic framework for classification with multivariate time series data with missing values. Our model consists of two parts; a deep generative model for missing value imputation and a classifier. Extending the existing deep generative models to better capture structures of time-series data, our deep generative model part is trained to impute the missing values in multiple plausible ways, effectively modeling the uncertainty of the imputation. The classifier part takes the time series data along with the imputed missing values and classifies signals, and is trained to capture the predictive uncertainty due to the multiple possibilities of imputations. Importantly, we show that na\"ively combining the generative model and the classifier could result in trivial solutions where the generative model does not produce meaningful imputations. To resolve this, we present a novel regularization technique that can promote the model to produce useful imputation values that help classification. Through extensive experiments on real-world time series data with missing values, we demonstrate the effectiveness of our method.
</details>
<details>
<summary>摘要</summary>
多变量时间序列数据在实际应用中通常含有较大的缺失值。现有的主流方法为这种缺失值是单纯地假设缺失值为零、平均值或相邻时间步的值。然而，这些简单策略并不考虑数据生成过程，更重要的是，它们不能有效地捕捉预测中的不确定性，因为缺失值有多个可能性。在这篇论文中，我们提出了一种新的概率 frameworks for classification with multivariate time series data with missing values。我们的模型包括两部分：深度生成模型和分类器。 extending the existing deep generative models to better capture the structures of time-series data, our deep generative model part is trained to impute the missing values in multiple plausible ways, effectively modeling the uncertainty of the imputation。分类器部分接受时间序列数据和假设的缺失值，并将时间序列数据分类，并且是通过捕捉多个可能性的假设来捕捉预测中的不确定性。然而，我们发现在直接组合生成模型和分类器时，可能会导致生成模型不生成有用的假设值，从而影响分类的准确性。为了解决这问题，我们提出了一种新的正则化技术，可以推动模型生成有用的假设值，以便于分类。通过对实际时间序列数据进行广泛的实验，我们证明了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Precipitation-nowcasting-with-generative-diffusion-models"><a href="#Precipitation-nowcasting-with-generative-diffusion-models" class="headerlink" title="Precipitation nowcasting with generative diffusion models"></a>Precipitation nowcasting with generative diffusion models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06733">http://arxiv.org/abs/2308.06733</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fmerizzi/Precipitation-nowcasting-with-generative-diffusion-models">https://github.com/fmerizzi/Precipitation-nowcasting-with-generative-diffusion-models</a></li>
<li>paper_authors: Andrea Asperti, Fabio Merizzi, Alberto Paparella, Giorgio Pedrazzi, Matteo Angelinelli, Stefano Colamonaco</li>
<li>for: 本研究使用数字天气预测方法进行气象预测，旨在提高预测的准确性和可靠性。</li>
<li>methods: 本研究使用了泛化激发模型（Diffusion Model）来处理气象预测任务，并通过将多个激发模型 ensemble起来，使用post处理网络来组合可能的天气场景，以获得最终的可能性预测结果。</li>
<li>results: 对于中欧2016-2021年的 hourly 数据，与已有的U-Net模型相比，泛化激发模型在气象预测任务中表现出了明显的优势，并且可以substantially 提高总体性能。<details>
<summary>Abstract</summary>
In recent years traditional numerical methods for accurate weather prediction have been increasingly challenged by deep learning methods. Numerous historical datasets used for short and medium-range weather forecasts are typically organized into a regular spatial grid structure. This arrangement closely resembles images: each weather variable can be visualized as a map or, when considering the temporal axis, as a video. Several classes of generative models, comprising Generative Adversarial Networks, Variational Autoencoders, or the recent Denoising Diffusion Models have largely proved their applicability to the next-frame prediction problem, and is thus natural to test their performance on the weather prediction benchmarks. Diffusion models are particularly appealing in this context, due to the intrinsically probabilistic nature of weather forecasting: what we are really interested to model is the probability distribution of weather indicators, whose expected value is the most likely prediction.   In our study, we focus on a specific subset of the ERA-5 dataset, which includes hourly data pertaining to Central Europe from the years 2016 to 2021. Within this context, we examine the efficacy of diffusion models in handling the task of precipitation nowcasting. Our work is conducted in comparison to the performance of well-established U-Net models, as documented in the existing literature. Our proposed approach of Generative Ensemble Diffusion (GED) utilizes a diffusion model to generate a set of possible weather scenarios which are then amalgamated into a probable prediction via the use of a post-processing network. This approach, in comparison to recent deep learning models, substantially outperformed them in terms of overall performance.
</details>
<details>
<summary>摘要</summary>
近年来，传统的数学方法 для准确的天气预测遭受了深度学习方法的挑战。历史数据库用于短距离和中距离天气预测通常按照一定的正方形格结构进行组织。这种设置与图像非常相似，每个天气变量都可以视为地图或者在考虑时间轴时视为视频。许多类型的生成模型，包括生成对抗网络、自适应变换器和最近的干扰扩散模型，在下一帧预测问题上都有广泛的应用，因此在天气预测标准benchmark上进行测试是自然的。干扩散模型在这个上特别有吸引力，因为天气预测的本质是 probabilistic 的：我们真正 interesseted 的是天气指标的概率分布，而这个分布的期望值是最有可能的预测。在我们的研究中，我们关注了ERA-5数据集的一个特定子集，包括2016年至2021年中欧每小时的数据。在这个上下文中，我们研究了干扩散模型在降水预测中的效果。我们的方法与文献中已有的U-Net模型相比，并使用生成ensemble扩散（GED）模型，该模型使用干扩散模型生成一系列可能的天气场景，然后通过使用后处理网络将这些场景融合成一个可能的预测。与最新的深度学习模型相比，我们的方法在总性表现上substantially outperform了它们。
</details></li>
</ul>
<hr>
<h2 id="Generalized-Independent-Noise-Condition-for-Estimating-Causal-Structure-with-Latent-Variables"><a href="#Generalized-Independent-Noise-Condition-for-Estimating-Causal-Structure-with-Latent-Variables" class="headerlink" title="Generalized Independent Noise Condition for Estimating Causal Structure with Latent Variables"></a>Generalized Independent Noise Condition for Estimating Causal Structure with Latent Variables</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06718">http://arxiv.org/abs/2308.06718</a></li>
<li>repo_url: None</li>
<li>paper_authors: Feng Xie, Biwei Huang, Zhengming Chen, Ruichu Cai, Clark Glymour, Zhi Geng, Kun Zhang</li>
<li>for: 学习 causal structure 中存在 latent variables 的挑战任务 (learning causal structure with latent variables)</li>
<li>methods: 提出 Generalized Independent Noise (GIN) 条件，用于 Linear non-Gaussian acyclic causal models 中包含 latent variables (propose a GIN condition for linear non-Gaussian acyclic causal models with latent variables)</li>
<li>results: 提供 necessary and sufficient graphical criteria of the GIN condition, 并可以快速 estimate Linear, Non-Gaussian Latent Hierarchical Models (LiNGLaHs) (provide necessary and sufficient graphical criteria of the GIN condition and can efficiently estimate Linear, Non-Gaussian Latent Hierarchical Models)<details>
<summary>Abstract</summary>
We investigate the challenging task of learning causal structure in the presence of latent variables, including locating latent variables and determining their quantity, and identifying causal relationships among both latent and observed variables. To address this, we propose a Generalized Independent Noise (GIN) condition for linear non-Gaussian acyclic causal models that incorporate latent variables, which establishes the independence between a linear combination of certain measured variables and some other measured variables. Specifically, for two observed random vectors $\bf{Y}$ and $\bf{Z}$, GIN holds if and only if $\omega^{\intercal}\mathbf{Y}$ and $\mathbf{Z}$ are independent, where $\omega$ is a non-zero parameter vector determined by the cross-covariance between $\mathbf{Y}$ and $\mathbf{Z}$. We then give necessary and sufficient graphical criteria of the GIN condition in linear non-Gaussian acyclic causal models. Roughly speaking, GIN implies the existence of an exogenous set $\mathcal{S}$ relative to the parent set of $\mathbf{Y}$ (w.r.t. the causal ordering), such that $\mathcal{S}$ d-separates $\mathbf{Y}$ from $\mathbf{Z}$. Interestingly, we find that the independent noise condition (i.e., if there is no confounder, causes are independent of the residual derived from regressing the effect on the causes) can be seen as a special case of GIN. With such a connection between GIN and latent causal structures, we further leverage the proposed GIN condition, together with a well-designed search procedure, to efficiently estimate Linear, Non-Gaussian Latent Hierarchical Models (LiNGLaHs), where latent confounders may also be causally related and may even follow a hierarchical structure. We show that the underlying causal structure of a LiNGLaH is identifiable in light of GIN conditions under mild assumptions. Experimental results show the effectiveness of the proposed approach.
</details>
<details>
<summary>摘要</summary>
我们研究一个复杂的任务：在含有隐变量的情况下学习 causal structure。包括找到隐变量的位置和它们的数量，以及确定隐变量和观测变量之间的 causal 关系。为解决这个问题，我们提议一种 Generalized Independent Noise (GIN) 条件，该条件在 linear non-Gaussian acyclic causal models 中包含隐变量，并且确定了某些观测变量的线性组合和其他观测变量的独立性。具体来说，对两个观测变量 $\mathbf{Y}$ 和 $\mathbf{Z}$，GIN 条件成立只要 $\omega^\intercal \mathbf{Y}$ 和 $\mathbf{Z}$ 独立，其中 $\omega$ 是由 $\mathbf{Y}$ 和 $\mathbf{Z}$ 之间的叠加协方差确定的非零参数向量。然后，我们给出了 linear non-Gaussian acyclic causal models 中 GIN 条件的必要和 suficient 图形 критери产。总之，GIN 条件等价于隐变量集 $\mathcal{S}$ 对于 $\mathbf{Y}$ 的父集（根据 causal 顺序）是独立的，并且 $\mathcal{S}$  separates $\mathbf{Y}$ 和 $\mathbf{Z}$。具体来说，如果没有干扰者，则 causal 关系可以看作一种特殊情况。我们还发现，GIN 条件和隐变量的 causal 结构之间存在着直接的关系。因此，我们可以通过 GIN 条件和一种合理的搜索过程来效率地估计 Linear, Non-Gaussian Latent Hierarchical Models (LiNGLaHs)，其中隐变量可能也有 causal 关系，并且可能会 suivre 一种层次结构。我们证明了 LiNGLaHs 的下面 causal 结构是可Identifiable的，只要 GIN 条件 在轻量级的假设下成立。实验结果表明了我们的方法的效果。
</details></li>
</ul>
<hr>
<h2 id="Estimating-and-Incentivizing-Imperfect-Knowledge-Agents-with-Hidden-Rewards"><a href="#Estimating-and-Incentivizing-Imperfect-Knowledge-Agents-with-Hidden-Rewards" class="headerlink" title="Estimating and Incentivizing Imperfect-Knowledge Agents with Hidden Rewards"></a>Estimating and Incentivizing Imperfect-Knowledge Agents with Hidden Rewards</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06717">http://arxiv.org/abs/2308.06717</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ilgin Dogan, Zuo-Jun Max Shen, Anil Aswani</li>
<li>For: This paper explores a repeated adverse selection game between a self-interested learning agent and a learning principal, with the goal of addressing the challenges of information asymmetry and incentive design in real-life scenarios.* Methods: The paper introduces a novel estimator that uses the history of principal’s incentives and agent’s choices to estimate the agent’s unknown rewards, and proposes a data-driven incentive policy within a multi-armed bandit (MAB) framework.* Results: The paper proves the finite-sample consistency of the estimator and a rigorous regret bound for the principal, and provides simulations to demonstrate the applicability of the framework to green energy aggregator contracts.<details>
<summary>Abstract</summary>
In practice, incentive providers (i.e., principals) often cannot observe the reward realizations of incentivized agents, which is in contrast to many principal-agent models that have been previously studied. This information asymmetry challenges the principal to consistently estimate the agent's unknown rewards by solely watching the agent's decisions, which becomes even more challenging when the agent has to learn its own rewards. This complex setting is observed in various real-life scenarios ranging from renewable energy storage contracts to personalized healthcare incentives. Hence, it offers not only interesting theoretical questions but also wide practical relevance. This paper explores a repeated adverse selection game between a self-interested learning agent and a learning principal. The agent tackles a multi-armed bandit (MAB) problem to maximize their expected reward plus incentive. On top of the agent's learning, the principal trains a parallel algorithm and faces a trade-off between consistently estimating the agent's unknown rewards and maximizing their own utility by offering adaptive incentives to lead the agent. For a non-parametric model, we introduce an estimator whose only input is the history of principal's incentives and agent's choices. We unite this estimator with a proposed data-driven incentive policy within a MAB framework. Without restricting the type of the agent's algorithm, we prove finite-sample consistency of the estimator and a rigorous regret bound for the principal by considering the sequential externality imposed by the agent. Lastly, our theoretical results are reinforced by simulations justifying applicability of our framework to green energy aggregator contracts.
</details>
<details>
<summary>摘要</summary>
在实践中，奖励提供者（即主体）通常无法观察奖励的实现，这与许多主体-代理模型不同。这种信息不对称性使得主体在 solely 观察代理人的决策后难以一直估计代理人的未知奖励。这种复杂的设定在各种实际场景中出现，如可再生能源存储合同和个性化奖励。因此，它不仅提出了有趣的理论问题，还具有广泛的实际 relevance。本文研究了一个 repeat 的反对选择游戏，在这个游戏中，一个自私的学习代理人和一个学习主体之间发生对抗。代理人面临一个多重武器bandit（MAB）问题，以最大化他们的预期奖励加上奖励。主体则在代理人学习的同时，训练一个并行的算法，面临着在估计代理人的未知奖励和自己的利用之间的负担。为了不假设代理人的算法类型，我们引入了一个仅基于奖励提供者历史和代理人选择的估计器。我们将这个估计器与一个基于 MAB 框架的数据驱动奖励策略结合。我们证明了该估计器在非 Parametric 模型下的finite-sample consistency和主体的正确偏误 bound。最后，我们通过实验证明了我们的框架在绿色能源聚合合同中的适用性。
</details></li>
</ul>
<hr>
<h2 id="CDR-Conservative-Doubly-Robust-Learning-for-Debiased-Recommendation"><a href="#CDR-Conservative-Doubly-Robust-Learning-for-Debiased-Recommendation" class="headerlink" title="CDR: Conservative Doubly Robust Learning for Debiased Recommendation"></a>CDR: Conservative Doubly Robust Learning for Debiased Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08461">http://arxiv.org/abs/2308.08461</a></li>
<li>repo_url: None</li>
<li>paper_authors: ZiJie Song, JiaWei Chen, Sheng Zhou, QiHao Shi, Yan Feng, Chun Chen, Can Wang</li>
<li>For: 本研究旨在解决推荐系统中的偏见问题，提高推荐系统的准确性和可靠性。* Methods: 本研究使用了双重Robust学习策略（DR），并提出了一种名为保守双重Robust策略（CDR）来解决偏见问题。* Results: 实验结果表明，CDR可以显著提高推荐系统的性能，同时减少偏见的频率。<details>
<summary>Abstract</summary>
In recommendation systems (RS), user behavior data is observational rather than experimental, resulting in widespread bias in the data. Consequently, tackling bias has emerged as a major challenge in the field of recommendation systems. Recently, Doubly Robust Learning (DR) has gained significant attention due to its remarkable performance and robust properties. However, our experimental findings indicate that existing DR methods are severely impacted by the presence of so-called Poisonous Imputation, where the imputation significantly deviates from the truth and becomes counterproductive.   To address this issue, this work proposes Conservative Doubly Robust strategy (CDR) which filters imputations by scrutinizing their mean and variance. Theoretical analyses show that CDR offers reduced variance and improved tail bounds.In addition, our experimental investigations illustrate that CDR significantly enhances performance and can indeed reduce the frequency of poisonous imputation.
</details>
<details>
<summary>摘要</summary>
在推荐系统（RS）中，用户行为数据是观察性的而不是实验性的，导致数据中存在广泛的偏见。因此，解决偏见问题成为了推荐系统领域的主要挑战。近些年来，双重稳健学习（DR）已经受到了广泛关注，因为它在性能和稳健性方面表现出色。然而，我们的实验结果表明，现有的DR方法受到了叫做“毒补假设”的问题的影响，即假设的插入数据与事实有很大差异，导致计算结果变得对抗性很强。为解决这个问题，本文提出了保守的双重稳健策略（CDR），通过检验插入数据的均值和方差来筛选掉不符合事实的插入数据。理论分析表明，CDR可以降低方差并提高尾部上限。此外，我们的实验调查表明，CDR可以显著提高性能，并且可以减少毒补假设的频率。
</details></li>
</ul>
<hr>
<h2 id="Learning-on-Graphs-with-Out-of-Distribution-Nodes"><a href="#Learning-on-Graphs-with-Out-of-Distribution-Nodes" class="headerlink" title="Learning on Graphs with Out-of-Distribution Nodes"></a>Learning on Graphs with Out-of-Distribution Nodes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06714">http://arxiv.org/abs/2308.06714</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/songyyyy/kdd22-oodgat">https://github.com/songyyyy/kdd22-oodgat</a></li>
<li>paper_authors: Yu Song, Donglin Wang</li>
<li>for: 本文旨在解决图像学中存在非标准节点的问题，特别是检测图像中的异常节点和分类其他节点为已知类别。</li>
<li>methods: 本文提出了一种基于图像注意力网络的异常检测方法，称为Out-of-Distribution Graph Attention Network (OODGAT)，它通过显式地模型不同类型节点之间的交互来分离异常节点和正常节点。</li>
<li>results: EXTENSIVE EXPERIMENTS表明，OODGAT在异常检测方面比现有方法大幅提高，而且与已知类别分类中的性能相当或更好。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) are state-of-the-art models for performing prediction tasks on graphs. While existing GNNs have shown great performance on various tasks related to graphs, little attention has been paid to the scenario where out-of-distribution (OOD) nodes exist in the graph during training and inference. Borrowing the concept from CV and NLP, we define OOD nodes as nodes with labels unseen from the training set. Since a lot of networks are automatically constructed by programs, real-world graphs are often noisy and may contain nodes from unknown distributions. In this work, we define the problem of graph learning with out-of-distribution nodes. Specifically, we aim to accomplish two tasks: 1) detect nodes which do not belong to the known distribution and 2) classify the remaining nodes to be one of the known classes. We demonstrate that the connection patterns in graphs are informative for outlier detection, and propose Out-of-Distribution Graph Attention Network (OODGAT), a novel GNN model which explicitly models the interaction between different kinds of nodes and separate inliers from outliers during feature propagation. Extensive experiments show that OODGAT outperforms existing outlier detection methods by a large margin, while being better or comparable in terms of in-distribution classification.
</details>
<details>
<summary>摘要</summary>
图 neural network (GNN) 是现代图学习模型中的状态机器。而现有的 GNN 模型在许多图学习任务中表现出色，但是对于图中存在异常节点（OOD）的情况却很少受到关注。基于 CV 和 NLP 中的概念，我们定义 OOD 节点为训练集中未见过的标签。由于现实世界中的图 oftentimes 是自动生成的，因此真实的图可能会包含来自未知分布的节点。在这工作中，我们定义了图学习中的 OOD 节点问题。特别是，我们希望完成两个任务：1）检测图中不属于已知分布的节点，2）将剩下的节点分类为已知类别之一。我们发现图中的连接模式是有用的异常检测信息，并提出了 Out-of-Distribution Graph Attention Network (OODGAT)，一种新的 GNN 模型，可以在特征传播过程中分离异常节点和正常节点。我们的实验表明，OODGAT 在异常检测方面超过现有的方法，而且在已知分布下的分类性能也不丢下。
</details></li>
</ul>
<hr>
<h2 id="The-Hard-Constraint-PINNs-for-Interface-Optimal-Control-Problems"><a href="#The-Hard-Constraint-PINNs-for-Interface-Optimal-Control-Problems" class="headerlink" title="The Hard-Constraint PINNs for Interface Optimal Control Problems"></a>The Hard-Constraint PINNs for Interface Optimal Control Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06709">http://arxiv.org/abs/2308.06709</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tianyouzeng/pinns-interface-optimal-control">https://github.com/tianyouzeng/pinns-interface-optimal-control</a></li>
<li>paper_authors: Ming-Chih Lai, Yongcun Song, Xiaoming Yuan, Hangrui Yue, Tianyou Zeng</li>
<li>for: 解决部分泛函方程（PDEs）中的优化控制问题，包括界面和一些控制约束。</li>
<li>methods: 使用物理学 informed neural networks（PINNs）和最近开发的缺陷捕捉神经网络，解决 mesh-free 和可扩展的 PDEs 优化控制问题。</li>
<li>results: 提出了一种具有硬约束的 PINNs 方法，可以 garantuee 精确满足界面和边界条件，并且可以分离学习 PDEs 和约束。其效率在一些椭球和Parabolic 界面优化控制问题中得到了承诺。<details>
<summary>Abstract</summary>
We show that the physics-informed neural networks (PINNs), in combination with some recently developed discontinuity capturing neural networks, can be applied to solve optimal control problems subject to partial differential equations (PDEs) with interfaces and some control constraints. The resulting algorithm is mesh-free and scalable to different PDEs, and it ensures the control constraints rigorously. Since the boundary and interface conditions, as well as the PDEs, are all treated as soft constraints by lumping them into a weighted loss function, it is necessary to learn them simultaneously and there is no guarantee that the boundary and interface conditions can be satisfied exactly. This immediately causes difficulties in tuning the weights in the corresponding loss function and training the neural networks. To tackle these difficulties and guarantee the numerical accuracy, we propose to impose the boundary and interface conditions as hard constraints in PINNs by developing a novel neural network architecture. The resulting hard-constraint PINNs approach guarantees that both the boundary and interface conditions can be satisfied exactly and they are decoupled from the learning of the PDEs. Its efficiency is promisingly validated by some elliptic and parabolic interface optimal control problems.
</details>
<details>
<summary>摘要</summary>
我们证明，用物理知识整合的神经网络（PINNs），可以与最近开发的阶段错误捕捉神经网络（DCNNs）一起解决受到部分数据方程式（PDEs）的最佳控制问题。这个算法是不含网点的和可扩展到不同的PDEs，并且确保控制限制。由于边界和界面条件，以及PDEs，都是转化为转量的loss函数中的软类似，因此需要同时学习它们。但是，这会导致调整这些类似的变量的问题和训练神经网络的问题。为了解决这些问题和保证数据精度，我们提议将边界和界面条件转化为硬类似的PINNs架构。这个方法可以保证边界和界面条件能够精确地满足，并且与PDEs的学习分开。我们在一些elliptic和parabolic interface最佳控制问题中调查了这个方法的效率，结果显示了承认的效果。
</details></li>
</ul>
<hr>
<h2 id="Generating-observation-guided-ensembles-for-data-assimilation-with-denoising-diffusion-probabilistic-model"><a href="#Generating-observation-guided-ensembles-for-data-assimilation-with-denoising-diffusion-probabilistic-model" class="headerlink" title="Generating observation guided ensembles for data assimilation with denoising diffusion probabilistic model"></a>Generating observation guided ensembles for data assimilation with denoising diffusion probabilistic model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06708">http://arxiv.org/abs/2308.06708</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yasahi-hpc/generative-enkf">https://github.com/yasahi-hpc/generative-enkf</a></li>
<li>paper_authors: Yuuichi Asahi, Yuta Hasegawa, Naoyuki Onodera, Takashi Shimokawabe, Hayato Shiba, Yasuhiro Idomura</li>
<li>for: 本文提出了一种 ensemble data assimilation 方法，使用 pseudo ensemble 生成的推 diffusion  probabilistic model。</li>
<li>methods: 本方法使用 trained 模型生成不寻常的 ensemble，通过 variance 在 ensemble 中的差异来提高数据融合的性能。</li>
<li>results: 对比传统 ensemble data assimilation 方法，本方法在 simulation model 不完美的情况下显示更好的性能。I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
This paper presents an ensemble data assimilation method using the pseudo ensembles generated by denoising diffusion probabilistic model. Since the model is trained against noisy and sparse observation data, this model can produce divergent ensembles close to observations. Thanks to the variance in generated ensembles, our proposed method displays better performance than the well-established ensemble data assimilation method when the simulation model is imperfect.
</details>
<details>
<summary>摘要</summary>
Here is the text in Simplified Chinese:这篇论文提出了一种ensemble数据融合方法，使用 Pseudo Ensemble 生成的随机模型。由于模型在噪声和缺失观测数据上训练，因此可以生成具有较高差异的 ensemble，使得我们的提议方法在模型不完美的情况下表现更好。
</details></li>
</ul>
<hr>
<h2 id="Understanding-the-robustness-difference-between-stochastic-gradient-descent-and-adaptive-gradient-methods"><a href="#Understanding-the-robustness-difference-between-stochastic-gradient-descent-and-adaptive-gradient-methods" class="headerlink" title="Understanding the robustness difference between stochastic gradient descent and adaptive gradient methods"></a>Understanding the robustness difference between stochastic gradient descent and adaptive gradient methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06703">http://arxiv.org/abs/2308.06703</a></li>
<li>repo_url: None</li>
<li>paper_authors: Avery Ma, Yangchen Pan, Amir-massoud Farahmand</li>
<li>For: 这个论文的目的是研究权重迭代法和适应性权重迭代法在训练深度神经网络时的表现。* Methods: 这个论文使用了随机梯度下降（SGD）和适应性梯度下降（Adam、RMSProp）等方法来训练深度神经网络。* Results: 研究发现，使用SGD训练的神经网络比使用适应性梯度下降方法更具robustness，并且在输入变化时表现更好。此外，研究还发现了自然 dataset 中的无关频率，并证明了使用适应性梯度下降方法可能会导致模型对这些变化敏感。<details>
<summary>Abstract</summary>
Stochastic gradient descent (SGD) and adaptive gradient methods, such as Adam and RMSProp, have been widely used in training deep neural networks. We empirically show that while the difference between the standard generalization performance of models trained using these methods is small, those trained using SGD exhibit far greater robustness under input perturbations. Notably, our investigation demonstrates the presence of irrelevant frequencies in natural datasets, where alterations do not affect models' generalization performance. However, models trained with adaptive methods show sensitivity to these changes, suggesting that their use of irrelevant frequencies can lead to solutions sensitive to perturbations. To better understand this difference, we study the learning dynamics of gradient descent (GD) and sign gradient descent (signGD) on a synthetic dataset that mirrors natural signals. With a three-dimensional input space, the models optimized with GD and signGD have standard risks close to zero but vary in their adversarial risks. Our result shows that linear models' robustness to $\ell_2$-norm bounded changes is inversely proportional to the model parameters' weight norm: a smaller weight norm implies better robustness. In the context of deep learning, our experiments show that SGD-trained neural networks show smaller Lipschitz constants, explaining the better robustness to input perturbations than those trained with adaptive gradient methods.
</details>
<details>
<summary>摘要</summary>
Stochastic gradient descent (SGD) 和 adaptive gradient 方法，如 Adam 和 RMSProp，在深度神经网络训练中广泛应用。我们实验显示，虽然这些方法的标准化预测性表现相似，但SGD 训练的模型在输入扰动下的Robustness 表现强度远胜 adaptive 方法。尤其是，我们的调查发现自然数据集中存在无关频率，其变化不会影响模型的预测性表现。然而，使用 adaptive 方法训练的模型对这些变化具有敏感性，表明它们使用无关频率可能会导致敏感解决方案。为了更好地理解这种差异，我们研究了梯度 descend (GD) 和签名梯度 descend (signGD) 在人工数据集上的学习动态。在三维输入空间中，使用 GD 和 signGD 优化的模型具有标准风险几乎为零，但它们在 $\ell_2$-norm 约束的变化下的风险异常大。我们的结果表明，线性模型对 $\ell_2$-norm 约束的变化的Robustness 与模型参数的权重 нор呈反比关系：小权重 нор呈指示更好的Robustness。在深度学习中，我们的实验表明，SGD 训练的神经网络显示更小的 Lipschitz 常数，解释它们在输入扰动下的更好的Robustness 比 adaptive 方法训练的模型。
</details></li>
</ul>
<hr>
<h2 id="Camouflaged-Image-Synthesis-Is-All-You-Need-to-Boost-Camouflaged-Detection"><a href="#Camouflaged-Image-Synthesis-Is-All-You-Need-to-Boost-Camouflaged-Detection" class="headerlink" title="Camouflaged Image Synthesis Is All You Need to Boost Camouflaged Detection"></a>Camouflaged Image Synthesis Is All You Need to Boost Camouflaged Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06701">http://arxiv.org/abs/2308.06701</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haichao Zhang, Can Qin, Yu Yin, Yun Fu</li>
<li>for: 提高深度学习模型检测掩蔽物的能力</li>
<li>methods: 使用生成模型生成真实的掩蔽图像，用于训练现有的物体检测模型</li>
<li>results: 在三个数据集（COD10k、CAMO和CHAMELEON）上表现出色，超过当前状态的方法表现，证明了该方法的有效性。<details>
<summary>Abstract</summary>
Camouflaged objects that blend into natural scenes pose significant challenges for deep-learning models to detect and synthesize. While camouflaged object detection is a crucial task in computer vision with diverse real-world applications, this research topic has been constrained by limited data availability. We propose a framework for synthesizing camouflage data to enhance the detection of camouflaged objects in natural scenes. Our approach employs a generative model to produce realistic camouflage images, which can be used to train existing object detection models. Specifically, we use a camouflage environment generator supervised by a camouflage distribution classifier to synthesize the camouflage images, which are then fed into our generator to expand the dataset. Our framework outperforms the current state-of-the-art method on three datasets (COD10k, CAMO, and CHAMELEON), demonstrating its effectiveness in improving camouflaged object detection. This approach can serve as a plug-and-play data generation and augmentation module for existing camouflaged object detection tasks and provides a novel way to introduce more diversity and distributions into current camouflage datasets.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>隐形对象在自然场景中摒除成为深度学习模型检测和生成的挑战。隐形对象检测是计算机视觉中重要的应用领域之一，但这一研究领域受到有限的数据可用性的限制。我们提出了一种框架，用于增强自然场景中隐形对象的检测。我们的方法使用生成模型生成真实的隐形图像，这些图像可以用来训练现有的对象检测模型。我们的框架在三个数据集（COD10k、CAMO和CHAMELEON）上表现出比前一个状态的方法更高的性能，这说明了我们的方法的有效性。这种方法可以作为现有隐形对象检测任务的数据生成和扩展模块，并提供一种新的多样性和分布的引入方式，以提高当前的隐形图像数据集。
</details></li>
</ul>
<hr>
<h2 id="SimMatchV2-Semi-Supervised-Learning-with-Graph-Consistency"><a href="#SimMatchV2-Semi-Supervised-Learning-with-Graph-Consistency" class="headerlink" title="SimMatchV2: Semi-Supervised Learning with Graph Consistency"></a>SimMatchV2: Semi-Supervised Learning with Graph Consistency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06692">http://arxiv.org/abs/2308.06692</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mingkai-zheng/simmatchv2">https://github.com/mingkai-zheng/simmatchv2</a></li>
<li>paper_authors: Mingkai Zheng, Shan You, Lang Huang, Chen Luo, Fei Wang, Chen Qian, Chang Xu</li>
<li>for: This paper is written for the purpose of proposing a new semi-supervised learning algorithm called SimMatchV2, which can improve the performance of image classification tasks with limited labeled data.</li>
<li>methods: The SimMatchV2 algorithm uses a graph-based approach to formulate various consistencies between labeled and unlabeled data, and utilizes a message passing mechanism to improve the performance of the algorithm.</li>
<li>results: The paper reports that SimMatchV2 achieves state-of-the-art performance on multiple semi-supervised learning benchmarks, with Top-1 Accuracy of 71.9% and 76.2% on ImageNet using 1% and 10% labeled examples, respectively.Here are the three key points in Simplified Chinese text:</li>
<li>for: 这篇论文是为了介绍一种新的半监督学习算法SimMatchV2，可以在有限的标注数据的情况下提高图像分类任务的性能。</li>
<li>methods: SimMatchV2算法使用图表的方式来定义各种半监督数据之间的一致性，并利用消息传递机制来提高算法的性能。</li>
<li>results: 论文报告SimMatchV2在多个半监督学习benchmark上达到了状态的最佳性能，ImageNet上使用1%和10%标注样本时，Top-1准确率分别达到71.9%和76.2%。<details>
<summary>Abstract</summary>
Semi-Supervised image classification is one of the most fundamental problem in computer vision, which significantly reduces the need for human labor. In this paper, we introduce a new semi-supervised learning algorithm - SimMatchV2, which formulates various consistency regularizations between labeled and unlabeled data from the graph perspective. In SimMatchV2, we regard the augmented view of a sample as a node, which consists of a label and its corresponding representation. Different nodes are connected with the edges, which are measured by the similarity of the node representations. Inspired by the message passing and node classification in graph theory, we propose four types of consistencies, namely 1) node-node consistency, 2) node-edge consistency, 3) edge-edge consistency, and 4) edge-node consistency. We also uncover that a simple feature normalization can reduce the gaps of the feature norm between different augmented views, significantly improving the performance of SimMatchV2. Our SimMatchV2 has been validated on multiple semi-supervised learning benchmarks. Notably, with ResNet-50 as our backbone and 300 epochs of training, SimMatchV2 achieves 71.9\% and 76.2\% Top-1 Accuracy with 1\% and 10\% labeled examples on ImageNet, which significantly outperforms the previous methods and achieves state-of-the-art performance. Code and pre-trained models are available at \href{https://github.com/mingkai-zheng/SimMatchV2}{https://github.com/mingkai-zheng/SimMatchV2}.
</details>
<details>
<summary>摘要</summary>
“半支持学习图像分类是计算机视觉领域中最基本的问题之一，它可以大幅减少人工劳动。在这篇论文中，我们介绍了一种新的半支持学习算法——SimMatchV2，它通过图形视角来形式化各种一致性 regularization。在SimMatchV2中，我们将每个样本的扩展视图视为一个节点，该节点包含标签和其对应的表示。不同的节点之间连接起来，这些连接由节点表示的相似度来度量。受图形理论中的消息传递和节点分类的启发，我们提出了四种一致性，namely 1) 节点-节点一致性、2) 节点-边一致性、3) 边-边一致性、4) 边-节点一致性。我们还发现，一个简单的特征 нормализа可以大幅减少不同扩展视图之间的特征 нор值差距，从而显著提高 SimMatchV2 的性能。我们的 SimMatchV2 在多个半支持学习 benchmark 上进行验证，与 ResNet-50 作为 backing 和 300  epoch 训练，SimMatchV2 在 ImageNet 上 achiev 71.9% 和 76.2% Top-1 Accuracy with 1% 和 10% 标注样本，在前一些方法中显著超越，实现了状态的杰出性。代码和预训练模型可以在 \href{https://github.com/mingkai-zheng/SimMatchV2}{https://github.com/mingkai-zheng/SimMatchV2} 上获取。”
</details></li>
</ul>
<hr>
<h2 id="MDB-Interactively-Querying-Datasets-and-Models"><a href="#MDB-Interactively-Querying-Datasets-and-Models" class="headerlink" title="MDB: Interactively Querying Datasets and Models"></a>MDB: Interactively Querying Datasets and Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06686">http://arxiv.org/abs/2308.06686</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aaditya Naik, Adam Stein, Yinjun Wu, Eric Wong, Mayur Naik</li>
<li>for: 本文为了帮助开发人员系统地调试机器学习模型中出现的错误。</li>
<li>methods: 本文使用了函数编程和关系代数来构建对数据集和模型预测数据进行表达的查询框架。</li>
<li>results: 我们的实验表明，使用MDB可以比其他基elines快速40%短 queries，并且开发人员可以成功地构建复杂的查询来描述机器学习模型的错误。<details>
<summary>Abstract</summary>
As models are trained and deployed, developers need to be able to systematically debug errors that emerge in the machine learning pipeline. We present MDB, a debugging framework for interactively querying datasets and models. MDB integrates functional programming with relational algebra to build expressive queries over a database of datasets and model predictions. Queries are reusable and easily modified, enabling debuggers to rapidly iterate and refine queries to discover and characterize errors and model behaviors. We evaluate MDB on object detection, bias discovery, image classification, and data imputation tasks across self-driving videos, large language models, and medical records. Our experiments show that MDB enables up to 10x faster and 40\% shorter queries than other baselines. In a user study, we find developers can successfully construct complex queries that describe errors of machine learning models.
</details>
<details>
<summary>摘要</summary>
developers 需要可以系统地调试机器学习管道中出现的错误。我们提出了MDB，一个用于交互式查询数据集和模型的调试框架。MDB将函数编程与关系代数结合起来，以建立表达性的查询数据集和模型预测中的问题。查询可重用和容易修改，允许调试者快速 iterate和细化查询，以描述和Characterize错误和模型行为。我们在对自动驾驶视频、大语言模型和医疗记录进行对象检测、偏见探测、图像分类和数据填充任务中进行了实验，结果显示MDB可以提高查询速度和查询长度，相比于其他基elines。在用户研究中，我们发现开发者可以成功地构建复杂的查询，以描述机器学习模型的错误。
</details></li>
</ul>
<hr>
<h2 id="Separable-Gaussian-Neural-Networks-Structure-Analysis-and-Function-Approximations"><a href="#Separable-Gaussian-Neural-Networks-Structure-Analysis-and-Function-Approximations" class="headerlink" title="Separable Gaussian Neural Networks: Structure, Analysis, and Function Approximations"></a>Separable Gaussian Neural Networks: Structure, Analysis, and Function Approximations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06679">http://arxiv.org/abs/2308.06679</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siyuan Xing, Jianqiao Sun<br>for: 用于 tri-variate function approximations 和 complex geometry 函数近似methods: 使用 Separable Gaussian Neural Network (SGNN)，利用 Gaussian 函数的分离性，将输入数据拆分成多列并在平行层中逐步 feeding them into uni-variate Gaussian functionsresults: 与 GRBFNN 相比，SGNN 可以实现100倍减少计算时间，同时保持相同精度水平，并且在approximating 函数 with complex geometry 方面可以达到三个数量级更高的精度。同时，SGNN 也比 DNNs with RuLU 和 Sigmoid 函数更易于调试和优化。<details>
<summary>Abstract</summary>
The Gaussian-radial-basis function neural network (GRBFNN) has been a popular choice for interpolation and classification. However, it is computationally intensive when the dimension of the input vector is high. To address this issue, we propose a new feedforward network - Separable Gaussian Neural Network (SGNN) by taking advantage of the separable property of Gaussian functions, which splits input data into multiple columns and sequentially feeds them into parallel layers formed by uni-variate Gaussian functions. This structure reduces the number of neurons from O(N^d) of GRBFNN to O(dN), which exponentially improves the computational speed of SGNN and makes it scale linearly as the input dimension increases. In addition, SGNN can preserve the dominant subspace of the Hessian matrix of GRBFNN in gradient descent training, leading to a similar level of accuracy to GRBFNN. It is experimentally demonstrated that SGNN can achieve 100 times speedup with a similar level of accuracy over GRBFNN on tri-variate function approximations. The SGNN also has better trainability and is more tuning-friendly than DNNs with RuLU and Sigmoid functions. For approximating functions with complex geometry, SGNN can lead to three orders of magnitude more accurate results than a RuLU-DNN with twice the number of layers and the number of neurons per layer.
</details>
<details>
<summary>摘要</summary>
Gaussian-radial-basis function neural network (GRBFNN) 是一种广泛使用的插值和分类方法。然而，当输入向量维度高时，GRBFNN 的计算复杂性会增加很多。为了解决这个问题，我们提出了一个新的前向网络 - Separable Gaussian Neural Network (SGNN)，通过利用 Gaussian 函数的分离性，将输入数据分成多列，然后将其顺序地输入到由单变量 Gaussian 函数所组成的平行层中。这样的结构可以将 GRBFNN 的neuron 数量由 O(N^d) 降至 O(dN)，从而将 computacional speed 加速到 exponentially ，并且让 SGNN 在输入维度增加时阶段性地提高。此外，SGNN 可以保留 GRBFNN 的主对角线 Hessian 矩阵的主对角线，使得在梯度下降训练中可以达到相似的精度水准。实验表明，SGNN 可以在 tri-variate 函数插值中实现 100 倍的速度提升，并且保持相似的精度水准。此外，SGNN 的训练性和适配性比 DNNs  WITH RuLU 和 Sigmoid 函数更好。当插值函数具有复杂的几何结构时，SGNN 可以实现三倍的精度提升。
</details></li>
</ul>
<hr>
<h2 id="A-deep-learning-framework-for-multi-scale-models-based-on-physics-informed-neural-networks"><a href="#A-deep-learning-framework-for-multi-scale-models-based-on-physics-informed-neural-networks" class="headerlink" title="A deep learning framework for multi-scale models based on physics-informed neural networks"></a>A deep learning framework for multi-scale models based on physics-informed neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06672">http://arxiv.org/abs/2308.06672</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yong Wang, Yanzhong Yao, Jiawei Guo, Zhiming Gao</li>
<li>for: 解决多Scale问题</li>
<li>methods: 修改损失函数，对不同级别的损失项应用不同数量的Power运算，使损失函数中各个损失项的级别相近</li>
<li>results: 能同时优化不同级别的损失项，扩展PINN的应用范围<details>
<summary>Abstract</summary>
Physics-informed neural networks (PINN) combine deep neural networks with the solution of partial differential equations (PDEs), creating a new and promising research area for numerically solving PDEs. Faced with a class of multi-scale problems that include loss terms of different orders of magnitude in the loss function, it is challenging for standard PINN methods to obtain an available prediction. In this paper, we propose a new framework for solving multi-scale problems by reconstructing the loss function. The framework is based on the standard PINN method, and it modifies the loss function of the standard PINN method by applying different numbers of power operations to the loss terms of different magnitudes, so that the individual loss terms composing the loss function have approximately the same order of magnitude among themselves. In addition, we give a grouping regularization strategy, and this strategy can deal well with the problem which varies significantly in different subdomains. The proposed method enables loss terms with different magnitudes to be optimized simultaneously, and it advances the application of PINN for multi-scale problems.
</details>
<details>
<summary>摘要</summary>
物理学 informed neural networks (PINN) combine deep neural networks 与解决 partial differential equations (PDEs) 的解，创造了一个新的研究领域，用于数值解决 PDEs。面临多个尺度问题，其中loss function中的损失项有不同的量级，标准 PINN 方法难以获得可靠预测。在这篇论文中，我们提出了一种新的多尺度问题解决框架。该框架基于标准 PINN 方法，并对不同量级的损失项进行不同数量的幂运算，使得各个损失项组成的损失函数具有相似的量级。此外，我们还提出了一种分组常数化策略，可以有效地处理不同子区域中变化很大的问题。该方法可以同时优化不同量级的损失项，并推动 PINN 在多尺度问题上的应用。
</details></li>
</ul>
<hr>
<h2 id="Law-of-Balance-and-Stationary-Distribution-of-Stochastic-Gradient-Descent"><a href="#Law-of-Balance-and-Stationary-Distribution-of-Stochastic-Gradient-Descent" class="headerlink" title="Law of Balance and Stationary Distribution of Stochastic Gradient Descent"></a>Law of Balance and Stationary Distribution of Stochastic Gradient Descent</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06671">http://arxiv.org/abs/2308.06671</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liu Ziyin, Hongchao Li, Masahito Ueda</li>
<li>for: 本文研究了Stochastic Gradient Descent（SGD）算法如何训练神经网络，特别是SGD在神经网络的高维和潜在不稳定的损失函数空间中如何导航。</li>
<li>methods: 本文使用了Symmetry的概念来研究SGD的训练过程，并证明了SGD在损失函数包含Symmetry时可以减轻损失函数的不稳定性。</li>
<li>results: 本文研究发现，SGD在深度和宽度具有某些特定的Symmetry时可以导致神经网络的站点分布具有复杂非线性现象，如相转化、破碎Ergodicity和强制转换。这些现象只存在于深度具有某些特定Symmetry的神经网络中，这表明了深度和浅度模型之间的基本区别。<details>
<summary>Abstract</summary>
The stochastic gradient descent (SGD) algorithm is the algorithm we use to train neural networks. However, it remains poorly understood how the SGD navigates the highly nonlinear and degenerate loss landscape of a neural network. In this work, we prove that the minibatch noise of SGD regularizes the solution towards a balanced solution whenever the loss function contains a rescaling symmetry. Because the difference between a simple diffusion process and SGD dynamics is the most significant when symmetries are present, our theory implies that the loss function symmetries constitute an essential probe of how SGD works. We then apply this result to derive the stationary distribution of stochastic gradient flow for a diagonal linear network with arbitrary depth and width. The stationary distribution exhibits complicated nonlinear phenomena such as phase transitions, broken ergodicity, and fluctuation inversion. These phenomena are shown to exist uniquely in deep networks, implying a fundamental difference between deep and shallow models.
</details>
<details>
<summary>摘要</summary>
SGD算法是我们用来训练神经网络的算法，但是它在神经网络的高度不对称和缺失散射的损失函数空间中 Navigation remains poorly understood. In this work, we prove that SGD的小批量噪声规范化解决方案带有批处理的散射过程，当损失函数具有扩大Symmetry时。由于在对称性存在时，SGD动力学与批处理的差异最大，我们的理论表明损失函数的对称性是SGD工作的重要检验。我们 then apply this result to derive the stationary distribution of stochastic gradient flow for a diagonal linear network with arbitrary depth and width. The stationary distribution exhibits complicated nonlinear phenomena such as phase transitions, broken ergodicity, and fluctuation inversion. These phenomena are shown to exist uniquely in deep networks, implying a fundamental difference between deep and shallow models.
</details></li>
</ul>
<hr>
<h2 id="Foundation-Models-in-Smart-Agriculture-Basics-Opportunities-and-Challenges"><a href="#Foundation-Models-in-Smart-Agriculture-Basics-Opportunities-and-Challenges" class="headerlink" title="Foundation Models in Smart Agriculture: Basics, Opportunities, and Challenges"></a>Foundation Models in Smart Agriculture: Basics, Opportunities, and Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06668">http://arxiv.org/abs/2308.06668</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jiajiali04/agriculture-foundation-models">https://github.com/jiajiali04/agriculture-foundation-models</a></li>
<li>paper_authors: Jiajia Li, Mingle Xu, Lirong Xiang, Dong Chen, Weichao Zhuang, Xunyuan Yin, Zhaojian Li</li>
<li>for: 这项研究旨在探索基于 Machine Learning 和 Deep Learning 的智能农业领域中的Foundation Models（基础模型）的潜力。</li>
<li>methods: 我们首先回顾了最新的基础模型在通用计算机科学领域，并将它们分为四类：语言基础模型、视觉基础模型、多modal基础模型以及强化学习基础模型。然后，我们详细介绍了在农业领域开发基础模型的过程，以及它们在智能农业中的潜在应用。</li>
<li>results: 通过本研究，我们对智能农业领域中基础模型的应用提出了新的研究方向，并提供了一个概念工具和技术背景来促进理解问题空间和探索新的研究方向。此外，我们还讨论了在开发基础模型时存在的独特挑战，包括模型训练、验证和部署。通过这项研究，我们对农业 AI 系统的发展作出了贡献，并介绍了基础模型作为一种可能地减少大量标注数据的潜在解决方案。<details>
<summary>Abstract</summary>
The past decade has witnessed the rapid development of ML and DL methodologies in agricultural systems, showcased by great successes in variety of agricultural applications. However, these conventional ML/DL models have certain limitations: They heavily rely on large, costly-to-acquire labeled datasets for training, require specialized expertise for development and maintenance, and are mostly tailored for specific tasks, thus lacking generalizability. Recently, foundation models have demonstrated remarkable successes in language and vision tasks across various domains. These models are trained on a vast amount of data from multiple domains and modalities. Once trained, they can accomplish versatile tasks with just minor fine-tuning and minimal task-specific labeled data. Despite their proven effectiveness and huge potential, there has been little exploration of applying FMs to agriculture fields. Therefore, this study aims to explore the potential of FMs in the field of smart agriculture. In particular, we present conceptual tools and technical background to facilitate the understanding of the problem space and uncover new research directions in this field. To this end, we first review recent FMs in the general computer science domain and categorize them into four categories: language FMs, vision FMs, multimodal FMs, and reinforcement learning FMs. Subsequently, we outline the process of developing agriculture FMs and discuss their potential applications in smart agriculture. We also discuss the unique challenges associated with developing AFMs, including model training, validation, and deployment. Through this study, we contribute to the advancement of AI in agriculture by introducing AFMs as a promising paradigm that can significantly mitigate the reliance on extensive labeled datasets and enhance the efficiency, effectiveness, and generalization of agricultural AI systems.
</details>
<details>
<summary>摘要</summary>
过去一代，机器学习（ML）和深度学习（DL）方法在农业系统中得到了迅速发展，其中很多成果在各种农业应用中得到了证明。然而，传统的ML/DL模型具有一些局限性：它们需要大量、昂贵的标注数据进行训练，需要专业的技术人员进行开发和维护，而且主要针对特定任务，缺乏普适性。最近，基础模型（Foundation Models，FMs）在语言和视觉任务中获得了非常成功的结果，它们在多个领域和模式上训练，并且可以通过微调和少量任务特定的标注数据来完成多种任务。Despite their proven effectiveness and huge potential, there has been little exploration of applying FMs to agriculture fields. Therefore, this study aims to explore the potential of FMs in the field of smart agriculture. In particular, we present conceptual tools and technical background to facilitate the understanding of the problem space and uncover new research directions in this field.首先，我们回顾了最近的FMs在通用计算机科学领域中的发展，并将它们分为四类：语言FMs、视觉FMs、多模式FMs和强化学习FMs。然后，我们详细介绍了农业FMs的开发过程，并讨论了它们在智能农业中的潜在应用。我们还讨论了开发农业FMs的独特挑战，包括模型训练、验证和部署。通过本研究，我们对农业AI的发展做出了贡献，通过引入AFMs作为一种可靠的替代方案，以减少对广泛标注数据的依赖，提高农业AI系统的效率、有效性和普适性。
</details></li>
</ul>
<hr>
<h2 id="ALGAN-Time-Series-Anomaly-Detection-with-Adjusted-LSTM-GAN"><a href="#ALGAN-Time-Series-Anomaly-Detection-with-Adjusted-LSTM-GAN" class="headerlink" title="ALGAN: Time Series Anomaly Detection with Adjusted-LSTM GAN"></a>ALGAN: Time Series Anomaly Detection with Adjusted-LSTM GAN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06663">http://arxiv.org/abs/2308.06663</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Abul Bashar, Richi Nayak</li>
<li>for: 这篇论文目的是提出一个新的生成对抗网络模型（ALGAN），用于不监控的时间序列资料中的异常检测。</li>
<li>methods: 这篇论文使用的方法是基于LSTM网络的对抗网络（GAN）模型，并且对输出进行调整以提高异常检测精度。</li>
<li>results: 根据实验结果，ALGAN在46个真实世界单 Variate时间序列数据集和多个领域的大量多 Variate时间序列数据集上的异常检测精度较高，比较传统、神经网络基于的和其他GAN型方法更好。<details>
<summary>Abstract</summary>
Anomaly detection in time series data, to identify points that deviate from normal behaviour, is a common problem in various domains such as manufacturing, medical imaging, and cybersecurity. Recently, Generative Adversarial Networks (GANs) are shown to be effective in detecting anomalies in time series data. The neural network architecture of GANs (i.e. Generator and Discriminator) can significantly improve anomaly detection accuracy. In this paper, we propose a new GAN model, named Adjusted-LSTM GAN (ALGAN), which adjusts the output of an LSTM network for improved anomaly detection in both univariate and multivariate time series data in an unsupervised setting. We evaluate the performance of ALGAN on 46 real-world univariate time series datasets and a large multivariate dataset that spans multiple domains. Our experiments demonstrate that ALGAN outperforms traditional, neural network-based, and other GAN-based methods for anomaly detection in time series data.
</details>
<details>
<summary>摘要</summary>
《时序数据异常检测使用生成对抗网络》Introduction:时序数据异常检测是各个领域的常见问题，如制造、医疗影像和网络安全等。在这些领域中，检测时序数据中异常点的异常行为是非常重要的。Recently, Generative Adversarial Networks (GANs) have been shown to be effective in detecting anomalies in time series data. In this paper, we propose a new GAN model, named Adjusted-LSTM GAN (ALGAN), which adjusts the output of an LSTM network for improved anomaly detection in both univariate and multivariate time series data in an unsupervised setting.Methodology:我们的方法包括以下几个部分：1. 生成对抗网络模型（GAN）的概述2. 基于LSTM网络的异常检测模型（ALGAN）的提出3. 实验设计和结果分析Results:我们对46个实际时序数据集进行了实验，并对多个领域的大量多变量时序数据进行了分析。结果表明，ALGAN在无监督的情况下，对时序数据中异常点的检测性能有显著提高。在单变量和多变量时序数据中，ALGAN都能够准确地检测异常点。Conclusion:本文提出了一种基于GAN的新方法，可以在无监督的情况下，提高时序数据中异常点的检测性能。我们的实验结果表明，ALGAN在多个领域中都能够准确地检测异常点。这种方法可以广泛应用于各个领域中的时序数据异常检测问题。
</details></li>
</ul>
<hr>
<h2 id="Benign-Shortcut-for-Debiasing-Fair-Visual-Recognition-via-Intervention-with-Shortcut-Features"><a href="#Benign-Shortcut-for-Debiasing-Fair-Visual-Recognition-via-Intervention-with-Shortcut-Features" class="headerlink" title="Benign Shortcut for Debiasing: Fair Visual Recognition via Intervention with Shortcut Features"></a>Benign Shortcut for Debiasing: Fair Visual Recognition via Intervention with Shortcut Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08482">http://arxiv.org/abs/2308.08482</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yiiizhang/shortcutDebiasing">https://github.com/yiiizhang/shortcutDebiasing</a></li>
<li>paper_authors: Yi Zhang, Jitao Sang, Junyang Wang, Dongmei Jiang, Yaowei Wang</li>
<li>for: 降低机器学习模型中的偏见风险，特别是在社会应用中，如雇用、银行和刑事司法等领域。</li>
<li>methods: 我们提出了一种新的短路减震方法（Shortcut Debiasing），通过在训练阶段将偏见特征转换为短路特征，然后使用 causal intervention 来消除短路特征 durante la inferencia。</li>
<li>results: 我们在多个 benchmark 数据集上应用了短路减震方法，并实现了与状态前的减震方法相比的显著改善 both accuracy 和 fairness。<details>
<summary>Abstract</summary>
Machine learning models often learn to make predictions that rely on sensitive social attributes like gender and race, which poses significant fairness risks, especially in societal applications, such as hiring, banking, and criminal justice. Existing work tackles this issue by minimizing the employed information about social attributes in models for debiasing. However, the high correlation between target task and these social attributes makes learning on the target task incompatible with debiasing. Given that model bias arises due to the learning of bias features (\emph{i.e}., gender) that help target task optimization, we explore the following research question: \emph{Can we leverage shortcut features to replace the role of bias feature in target task optimization for debiasing?} To this end, we propose \emph{Shortcut Debiasing}, to first transfer the target task's learning of bias attributes from bias features to shortcut features, and then employ causal intervention to eliminate shortcut features during inference. The key idea of \emph{Shortcut Debiasing} is to design controllable shortcut features to on one hand replace bias features in contributing to the target task during the training stage, and on the other hand be easily removed by intervention during the inference stage. This guarantees the learning of the target task does not hinder the elimination of bias features. We apply \emph{Shortcut Debiasing} to several benchmark datasets, and achieve significant improvements over the state-of-the-art debiasing methods in both accuracy and fairness.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Polar-Collision-Grids-Effective-Interaction-Modelling-for-Pedestrian-Trajectory-Prediction-in-Shared-Space-Using-Collision-Checks"><a href="#Polar-Collision-Grids-Effective-Interaction-Modelling-for-Pedestrian-Trajectory-Prediction-in-Shared-Space-Using-Collision-Checks" class="headerlink" title="Polar Collision Grids: Effective Interaction Modelling for Pedestrian Trajectory Prediction in Shared Space Using Collision Checks"></a>Polar Collision Grids: Effective Interaction Modelling for Pedestrian Trajectory Prediction in Shared Space Using Collision Checks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06654">http://arxiv.org/abs/2308.06654</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahsa Golchoubian, Moojan Ghafurian, Kerstin Dautenhahn, Nasser Lashgarian Azad</li>
<li>for: 预测步行者的轨迹是自动驾驶汽车安全导航中的关键能力，特别是在与步行者共享空间时。步行者在共享空间中的运动受到汽车和其他步行者的影响，因此，可以准确地模拟步行者-汽车和步行者-步行者的互动，可以提高步行者轨迹预测模型的准确性。</li>
<li>methods: 我们提出了一种基于启发的方法，通过计算碰撞风险来选择互动对象。我们将关注与目标步行者之间的碰撞风险，并使用时间到碰撞和两个对象的接近方向角来编码互动效果。我们还提出了一种新的极天球碰撞网格图，以便更好地表示互动效果。</li>
<li>results: 我们的结果表明，使用我们提出的方法可以比基eline方法更加准确地预测步行者的轨迹，特别是在HBS数据集上。<details>
<summary>Abstract</summary>
Predicting pedestrians' trajectories is a crucial capability for autonomous vehicles' safe navigation, especially in spaces shared with pedestrians. Pedestrian motion in shared spaces is influenced by both the presence of vehicles and other pedestrians. Therefore, effectively modelling both pedestrian-pedestrian and pedestrian-vehicle interactions can increase the accuracy of the pedestrian trajectory prediction models. Despite the huge literature on ways to encode the effect of interacting agents on a pedestrian's predicted trajectory using deep-learning models, limited effort has been put into the effective selection of interacting agents. In the majority of cases, the interaction features used are mainly based on relative distances while paying less attention to the effect of the velocity and approaching direction in the interaction formulation. In this paper, we propose a heuristic-based process of selecting the interacting agents based on collision risk calculation. Focusing on interactions of potentially colliding agents with a target pedestrian, we propose the use of time-to-collision and the approach direction angle of two agents for encoding the interaction effect. This is done by introducing a novel polar collision grid map. Our results have shown predicted trajectories closer to the ground truth compared to existing methods (used as a baseline) on the HBS dataset.
</details>
<details>
<summary>摘要</summary>
预测行人轨迹是自动驾驶车辆安全导航中的关键能力，特别是在与行人共享空间时。行人运动在共享空间中受到车辆和其他行人的影响。因此，可以准确模拟行人与其他行人和车辆之间的互动，以提高行人轨迹预测模型的准确性。尽管有庞大的文献关于使用深度学习模型来编码互动对行人预测轨迹的影响，但是有限的努力被投入到有效选择互动者方面。在大多数情况下，互动特征主要基于相对距离，而忽略了互动形式中速度和接近方向的效果。在这篇文章中，我们提出了一种基于碰撞风险计算的互动者选择规则。我们关注了可能碰撞的两个代理人之间的时间到碰撞和接近方向角的互动效果。我们通过引入一种新的极地碰撞格图来实现这一点。我们的结果显示，与基eline方法相比，我们的方法在HBS数据集上预测轨迹更加准确。
</details></li>
</ul>
<hr>
<h2 id="Accelerating-Diffusion-based-Combinatorial-Optimization-Solvers-by-Progressive-Distillation"><a href="#Accelerating-Diffusion-based-Combinatorial-Optimization-Solvers-by-Progressive-Distillation" class="headerlink" title="Accelerating Diffusion-based Combinatorial Optimization Solvers by Progressive Distillation"></a>Accelerating Diffusion-based Combinatorial Optimization Solvers by Progressive Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06644">http://arxiv.org/abs/2308.06644</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jwrh/Accelerating-Diffusion-based-Combinatorial-Optimization-Solvers-by-Progressive-Distillation">https://github.com/jwrh/Accelerating-Diffusion-based-Combinatorial-Optimization-Solvers-by-Progressive-Distillation</a></li>
<li>paper_authors: Junwei Huang, Zhiqing Sun, Yiming Yang</li>
<li>for: 提高NP-完全 combinatorial优化问题的解决速度</li>
<li>methods: 使用进步压缩来减少推理步骤数</li>
<li>results: 在TSP-50 dataset上，提高推理速度16倍，性能下降0.019%<details>
<summary>Abstract</summary>
Graph-based diffusion models have shown promising results in terms of generating high-quality solutions to NP-complete (NPC) combinatorial optimization (CO) problems. However, those models are often inefficient in inference, due to the iterative evaluation nature of the denoising diffusion process. This paper proposes to use progressive distillation to speed up the inference by taking fewer steps (e.g., forecasting two steps ahead within a single step) during the denoising process. Our experimental results show that the progressively distilled model can perform inference 16 times faster with only 0.019% degradation in performance on the TSP-50 dataset.
</details>
<details>
<summary>摘要</summary>
GRAPH-based diffusion models have shown promising results in terms of generating high-quality solutions to NP-complete (NPC) combinatorial optimization (CO) problems. However, those models are often inefficient in inference, due to the iterative evaluation nature of the denoising diffusion process. This paper proposes to use progressive distillation to speed up the inference by taking fewer steps (e.g., forecasting two steps ahead within a single step) during the denoising process. Our experimental results show that the progressively distilled model can perform inference 16 times faster with only 0.019% degradation in performance on the TSP-50 dataset.Note: The translation is in Simplified Chinese, which is one of the two standard forms of Chinese writing. The other form is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Advances-in-Self-Supervised-Learning-for-Synthetic-Aperture-Sonar-Data-Processing-Classification-and-Pattern-Recognition"><a href="#Advances-in-Self-Supervised-Learning-for-Synthetic-Aperture-Sonar-Data-Processing-Classification-and-Pattern-Recognition" class="headerlink" title="Advances in Self-Supervised Learning for Synthetic Aperture Sonar Data Processing, Classification, and Pattern Recognition"></a>Advances in Self-Supervised Learning for Synthetic Aperture Sonar Data Processing, Classification, and Pattern Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11633">http://arxiv.org/abs/2308.11633</a></li>
<li>repo_url: None</li>
<li>paper_authors: Brandon Sheffield, Frank E. Bobe III, Bradley Marchand, Matthew S. Emigh</li>
<li>for: 本研究提出了一种基于自助学习的SAS数据处理方法，以解决SAS数据处理中缺乏标注数据的问题。</li>
<li>methods: 本研究使用了MoCo-SAS方法，即基于自助学习的SAS数据处理、分类和模式识别方法。</li>
<li>results: 实验结果表明，MoCo-SAS方法在SAS数据处理中显著超过了传统的指导学习方法，并且在F1分数方面得到了显著改善。这些发现提出了使用自助学习进行SAS数据处理的潜在可能性，并且对水下对象检测和分类提供了新的思路。<details>
<summary>Abstract</summary>
Synthetic Aperture Sonar (SAS) imaging has become a crucial technology for underwater exploration because of its unique ability to maintain resolution at increasing ranges, a characteristic absent in conventional sonar techniques. However, the effective application of deep learning to SAS data processing is often limited due to the scarcity of labeled data. To address this challenge, this paper proposes MoCo-SAS that leverages self-supervised learning (SSL) for SAS data processing, classification, and pattern recognition. The experimental results demonstrate that MoCo-SAS significantly outperforms traditional supervised learning methods, as evidenced by significant improvements observed in terms of the F1-score. These findings highlight the potential of SSL in advancing the state-of-the-art in SAS data processing, offering promising avenues for enhanced underwater object detection and classification.
</details>
<details>
<summary>摘要</summary>
这篇研究论文提出了一个名为MoCo-SAS的自动学习方法，用于对水下探索中的Synthetic Aperture Sonar（SAS）数据进行处理、分类和图像识别。这种方法利用自动学习的自我指导学习（SSL）技术，以提高SAS数据处理的精度和效率。实验结果显示，MoCo-SAS方法与传统的超级vised learning方法相比，有着明显的改善，特别是在F1分数上。这些结果显示出SSL在SAS数据处理中的应用潜力，并开启了更进一步的水下物体探测和分类技术的可能性。
</details></li>
</ul>
<hr>
<h2 id="ADRMX-Additive-Disentanglement-of-Domain-Features-with-Remix-Loss"><a href="#ADRMX-Additive-Disentanglement-of-Domain-Features-with-Remix-Loss" class="headerlink" title="ADRMX: Additive Disentanglement of Domain Features with Remix Loss"></a>ADRMX: Additive Disentanglement of Domain Features with Remix Loss</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06624">http://arxiv.org/abs/2308.06624</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/berkerdemirel/ADRMX">https://github.com/berkerdemirel/ADRMX</a></li>
<li>paper_authors: Berker Demirel, Erchan Aptoula, Huseyin Ozkan</li>
<li>for: 这个论文的目的是解决多个源领域中模型在新未经见过的领域中的泛化问题。</li>
<li>methods: 这个论文提出了一种新的架构 named Additive Disentanglement of Domain Features with Remix Loss (ADRMX)，它使用了添加式分解策略将域特征与域 invariants 相结合，以提高模型的泛化能力。</li>
<li>results: 经过广泛的实验，ADRMX 在 DomainBed 上实现了最佳性能。<details>
<summary>Abstract</summary>
The common assumption that train and test sets follow similar distributions is often violated in deployment settings. Given multiple source domains, domain generalization aims to create robust models capable of generalizing to new unseen domains. To this end, most of existing studies focus on extracting domain invariant features across the available source domains in order to mitigate the effects of inter-domain distributional changes. However, this approach may limit the model's generalization capacity by relying solely on finding common features among the source domains. It overlooks the potential presence of domain-specific characteristics that could be prevalent in a subset of domains, potentially containing valuable information. In this work, a novel architecture named Additive Disentanglement of Domain Features with Remix Loss (ADRMX) is presented, which addresses this limitation by incorporating domain variant features together with the domain invariant ones using an original additive disentanglement strategy. Moreover, a new data augmentation technique is introduced to further support the generalization capacity of ADRMX, where samples from different domains are mixed within the latent space. Through extensive experiments conducted on DomainBed under fair conditions, ADRMX is shown to achieve state-of-the-art performance. Code will be made available at GitHub after the revision process.
</details>
<details>
<summary>摘要</summary>
通常假设训练集和测试集遵循类似的分布是在部署场景下不成立。给定多个源领域，领域泛化目标是创建抗辐射的模型，以便在新未看过的领域中进行泛化。然而，现有的研究通常是通过找到源领域中共同的特征来减轻交领域分布变化的影响。这可能会限制模型的泛化能力，因为它仅仅依据源领域中共同的特征来泛化。这些研究忽略了可能存在的领域特有特征，这些特征可能在一些领域中占据主导地位，并且可能包含有价值信息。在这项工作中，一种新的架构名为加法解决方案（ADRMX）被提出，它解决了这一限制，通过将领域特征和领域不变特征结合在一起使用一种原始的加法解决方案。此外，一种新的数据增强技术也被引入，以支持ADRMX的泛化能力，其中各个领域的样本在离散空间中混合。经过对DomainBed进行了广泛的实验，ADRMX被证明可以在公正的条件下实现状态的泛化性能。代码将在GitHub上公布 после修订过程。
</details></li>
</ul>
<hr>
<h2 id="Can-Unstructured-Pruning-Reduce-the-Depth-in-Deep-Neural-Networks"><a href="#Can-Unstructured-Pruning-Reduce-the-Depth-in-Deep-Neural-Networks" class="headerlink" title="Can Unstructured Pruning Reduce the Depth in Deep Neural Networks?"></a>Can Unstructured Pruning Reduce the Depth in Deep Neural Networks?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06619">http://arxiv.org/abs/2308.06619</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhu Liao, Victor Quétu, Van-Tam Nguyen, Enzo Tartaglione</li>
<li>for: 降低深度神经网络的大小 while maintaining performance</li>
<li>methods: 引入Entropy Guided Pruning（EGP）算法，优先采用含有低 entropy 的连接进行剪除</li>
<li>results: 实验结果表明，EGP可以有效地剪除深度神经网络，同时保持竞争性性能水平<details>
<summary>Abstract</summary>
Pruning is a widely used technique for reducing the size of deep neural networks while maintaining their performance. However, such a technique, despite being able to massively compress deep models, is hardly able to remove entire layers from a model (even when structured): is this an addressable task? In this study, we introduce EGP, an innovative Entropy Guided Pruning algorithm aimed at reducing the size of deep neural networks while preserving their performance. The key focus of EGP is to prioritize pruning connections in layers with low entropy, ultimately leading to their complete removal. Through extensive experiments conducted on popular models like ResNet-18 and Swin-T, our findings demonstrate that EGP effectively compresses deep neural networks while maintaining competitive performance levels. Our results not only shed light on the underlying mechanism behind the advantages of unstructured pruning, but also pave the way for further investigations into the intricate relationship between entropy, pruning techniques, and deep learning performance. The EGP algorithm and its insights hold great promise for advancing the field of network compression and optimization. The source code for EGP is released open-source.
</details>
<details>
<summary>摘要</summary>
《剪除技术在深度神经网络中减小大小而保持性能的应用广泛。然而，这种技术，即使能够压缩深度模型，几乎不能完全从模型中移除整层（即使结构化）：是这个任务可解决吗？在这项研究中，我们介绍了EGP算法，一种基于熵指导的剪除算法，用于减小深度神经网络大小，保持性能水平。EGP的关键焦点是优先剪除层次熵低的连接，从而导致其完全移除。经过广泛的实验，我们发现EGP能够有效地减小深度神经网络，同时保持竞争性能水平。我们的结果不仅揭示了剪除技术的优势，还探讨了剪除、熵和深度学习性能之间的复杂关系。EGP算法和其理解拥有推动深度网络压缩和优化领域的前景。EGP算法的源代码已经公开发布。》Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="On-the-Interplay-of-Convolutional-Padding-and-Adversarial-Robustness"><a href="#On-the-Interplay-of-Convolutional-Padding-and-Adversarial-Robustness" class="headerlink" title="On the Interplay of Convolutional Padding and Adversarial Robustness"></a>On the Interplay of Convolutional Padding and Adversarial Robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06612">http://arxiv.org/abs/2308.06612</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paul Gavrikov, Janis Keuper</li>
<li>for: 本文旨在研究 padding 对 adversarial attack 的影响，以及不同 padding 模式对 adversarial robustness 的影响。</li>
<li>methods: 本文使用了 convolutional neural network (CNN) 和 adversarial attack 的方法，并进行了 padding 的分析和对比。</li>
<li>results: 本文发现了 perturbation anomalies 在图像边缘区域，这些区域是 padding 的应用区域。此外，本文还发现了不同 padding 模式对 adversarial robustness 的影响。<details>
<summary>Abstract</summary>
It is common practice to apply padding prior to convolution operations to preserve the resolution of feature-maps in Convolutional Neural Networks (CNN). While many alternatives exist, this is often achieved by adding a border of zeros around the inputs. In this work, we show that adversarial attacks often result in perturbation anomalies at the image boundaries, which are the areas where padding is used. Consequently, we aim to provide an analysis of the interplay between padding and adversarial attacks and seek an answer to the question of how different padding modes (or their absence) affect adversarial robustness in various scenarios.
</details>
<details>
<summary>摘要</summary>
通常情况下，在卷积神经网络（CNN）中，会将padding应用于特征地图以保持其分辨率。虽然有很多替代方案，通常是通过添加边界上的零值来实现。在这项工作中，我们发现了对抗攻击通常会在图像边界上产生异常的扰动，这是padding使用的区域。因此，我们想进行对padding和对抗攻击之间的交互分析，并查找不同的padding模式（或其缺失）对对抗鲁棒性在不同的场景中的影响。
</details></li>
</ul>
<hr>
<h2 id="LadleNet-Translating-Thermal-Infrared-Images-to-Visible-Light-Images-Using-A-Scalable-Two-stage-U-Net"><a href="#LadleNet-Translating-Thermal-Infrared-Images-to-Visible-Light-Images-Using-A-Scalable-Two-stage-U-Net" class="headerlink" title="LadleNet: Translating Thermal Infrared Images to Visible Light Images Using A Scalable Two-stage U-Net"></a>LadleNet: Translating Thermal Infrared Images to Visible Light Images Using A Scalable Two-stage U-Net</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06603">http://arxiv.org/abs/2308.06603</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ach-1914/ladlenet">https://github.com/ach-1914/ladlenet</a></li>
<li>paper_authors: Tonghui Zou</li>
<li>for: 这篇论文的目的是为了将抛光热成像（TIR）图像转换成可见光成像（VI）图像，这个问题在各个领域都有广泛的应用，例如TIR-VI图像匹配和融合。</li>
<li>methods: 这篇论文提出了一种算法，即LadleNet，基于U-Net架构。LadleNet使用了两个阶段的U-Net concatenation结构，并添加了跳过连接和精细特征聚合技术，从而提高了模型性能。</li>
<li>results: 在KAIST数据集上测试和分析了LadleNet和LadleNet+两种方法，结果显示，LadleNet+在图像清晰度和感知质量方面达到了当前最佳性能。<details>
<summary>Abstract</summary>
The translation of thermal infrared (TIR) images to visible light (VI) images presents a challenging task with potential applications spanning various domains such as TIR-VI image registration and fusion. Leveraging supplementary information derived from TIR image conversions can significantly enhance model performance and generalization across these applications. However, prevailing issues within this field include suboptimal image fidelity and limited model scalability. In this paper, we introduce an algorithm, LadleNet, based on the U-Net architecture. LadleNet employs a two-stage U-Net concatenation structure, augmented with skip connections and refined feature aggregation techniques, resulting in a substantial enhancement in model performance. Comprising 'Handle' and 'Bowl' modules, LadleNet's Handle module facilitates the construction of an abstract semantic space, while the Bowl module decodes this semantic space to yield mapped VI images. The Handle module exhibits extensibility by allowing the substitution of its network architecture with semantic segmentation networks, thereby establishing more abstract semantic spaces to bolster model performance. Consequently, we propose LadleNet+, which replaces LadleNet's Handle module with the pre-trained DeepLabv3+ network, thereby endowing the model with enhanced semantic space construction capabilities. The proposed method is evaluated and tested on the KAIST dataset, accompanied by quantitative and qualitative analyses. Compared to existing methodologies, our approach achieves state-of-the-art performance in terms of image clarity and perceptual quality. The source code will be made available at https://github.com/Ach-1914/LadleNet/tree/main/.
</details>
<details>
<summary>摘要</summary>
《热传 инфра红（TIR）图像到可见光（VI）图像的翻译 задача具有各种应用领域的潜在投入，如TIR-VI图像匹配和融合。利用TIR图像转换得到的补充信息可以显著提高模型性能和泛化性。然而，现有的问题包括低效图像准确性和限制模型可扩展性。在本文中，我们提出了一种算法，即LadleNet，基于U-Net架构。LadleNet使用了两个阶段的U-Net叠加结构，并添加了跳跃连接和细化特征聚合技术，从而实现了模型性能的明显提高。LadleNet由“托”和“碗”模块组成，其中“托”模块建立了一个抽象 semantic space，而“碗”模块将这个 semantic space 转换为生成的VI图像。“托”模块具有可扩展性，可以将其网络架构替换为semantic segmentation网络，从而建立更加抽象的semantic space，进一步提高模型性能。因此，我们提出了LadleNet+，其将LadleNet的“托”模块替换为预训练的DeepLabv3+网络，从而增强模型的semantic space建构能力。我们的方法在KAIST数据集上进行评估和测试，并通过量化和质量分析进行比较。与现有方法相比，我们的方法在图像清晰度和感知质量上达到了状态 искусственный的性能。代码将在https://github.com/Ach-1914/LadleNet/tree/main/中提供。》
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/13/cs.LG_2023_08_13/" data-id="clltau92r005zcr88bwmz9wk5" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_08_13" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/13/eess.IV_2023_08_13/" class="article-date">
  <time datetime="2023-08-12T16:00:00.000Z" itemprop="datePublished">2023-08-13</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/13/eess.IV_2023_08_13/">eess.IV - 2023-08-13 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Shape-guided-Conditional-Latent-Diffusion-Models-for-Synthesising-Brain-Vasculature"><a href="#Shape-guided-Conditional-Latent-Diffusion-Models-for-Synthesising-Brain-Vasculature" class="headerlink" title="Shape-guided Conditional Latent Diffusion Models for Synthesising Brain Vasculature"></a>Shape-guided Conditional Latent Diffusion Models for Synthesising Brain Vasculature</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06781">http://arxiv.org/abs/2308.06781</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yash Deo, Haoran Dou, Nishant Ravikumar, Alejandro F. Frangi, Toni Lassila</li>
<li>For: The paper aims to generate realistic 3D segmentations of the Circle of Willis (CoW) using a conditional latent diffusion model with shape and anatomical guidance, in order to advance research on cerebrovascular diseases and refine clinical interventions.* Methods: The authors propose a novel generative approach using a conditional latent diffusion model, which incorporates shape guidance to better preserve vessel continuity and demonstrate superior performance compared to alternative generative models, including conditional variants of 3D GAN and 3D VAE.* Results: The authors observed that their model generated CoW variants that are more realistic and demonstrate higher visual fidelity than competing approaches, with an FID score 53% better than the best-performing GAN-based model.Here are the three points in Simplified Chinese text:* For: 本研究旨在使用 conditional latent diffusion model 生成真实的 3D CoW 分割，以提高脑血管疾病研究和临床 interven 的技术水平。* Methods: 作者们提出了一种新的生成方法，使用 conditional latent diffusion model，该模型具有形态指导，以更好地保持血管连续性。* Results: 作者们发现，他们的模型可以生成更加真实的 CoW 变体，并且与其他方法相比，有 53% 更高的视觉质量。<details>
<summary>Abstract</summary>
The Circle of Willis (CoW) is the part of cerebral vasculature responsible for delivering blood to the brain. Understanding the diverse anatomical variations and configurations of the CoW is paramount to advance research on cerebrovascular diseases and refine clinical interventions. However, comprehensive investigation of less prevalent CoW variations remains challenging because of the dominance of a few commonly occurring configurations. We propose a novel generative approach utilising a conditional latent diffusion model with shape and anatomical guidance to generate realistic 3D CoW segmentations, including different phenotypical variations. Our conditional latent diffusion model incorporates shape guidance to better preserve vessel continuity and demonstrates superior performance when compared to alternative generative models, including conditional variants of 3D GAN and 3D VAE. We observed that our model generated CoW variants that are more realistic and demonstrate higher visual fidelity than competing approaches with an FID score 53\% better than the best-performing GAN-based model.
</details>
<details>
<summary>摘要</summary>
圆形的威廉圈（CoW）是脑血管系统中带来脑部血液的部分。了解各种不同的CoW结构和配置是研究脑血管疾病的进步和优化临床 intervención的关键。然而，对于较少seen CoW变化的全面调查仍然是一个挑战，因为一些常见的配置占据了主导地位。我们提出了一种新的生成方法，利用决定性液态扩散模型，包括形态指导来生成真实的3D CoW分割结果，包括不同的fenotipical变化。我们的决定性液态扩散模型包含形态指导，以更好地保持血管连续性，并且与其他生成模型相比，包括 conditional GAN和3D VAE的 conditional变种，显示出更高的性能。我们观察到，我们的模型生成的CoW变化更加真实，与竞争方法的FID分数比53%高。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Image-Denoising-in-Real-World-Scenarios-via-Self-Collaboration-Parallel-Generative-Adversarial-Branches"><a href="#Unsupervised-Image-Denoising-in-Real-World-Scenarios-via-Self-Collaboration-Parallel-Generative-Adversarial-Branches" class="headerlink" title="Unsupervised Image Denoising in Real-World Scenarios via Self-Collaboration Parallel Generative Adversarial Branches"></a>Unsupervised Image Denoising in Real-World Scenarios via Self-Collaboration Parallel Generative Adversarial Branches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06776">http://arxiv.org/abs/2308.06776</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/linxin0/scpgabnet">https://github.com/linxin0/scpgabnet</a></li>
<li>paper_authors: Xin Lin, Chao Ren, Xiao Liu, Jie Huang, Yinjie Lei</li>
<li>for: 提高无监督图像净化的性能，不需要大规模的对照数据集。</li>
<li>methods: 基于生成对抗网络的不监督方法，通过多个denoiser的级联使用，逐渐提高净化性能。</li>
<li>results: 与现有无监督方法相比，提出了新的SC策略，可以减少对GAN-based净化框架的计算复杂性，同时提高图像净化性能。实验结果表明，该方法可以在无监督下实现更高的净化性能。<details>
<summary>Abstract</summary>
Deep learning methods have shown remarkable performance in image denoising, particularly when trained on large-scale paired datasets. However, acquiring such paired datasets for real-world scenarios poses a significant challenge. Although unsupervised approaches based on generative adversarial networks offer a promising solution for denoising without paired datasets, they are difficult in surpassing the performance limitations of conventional GAN-based unsupervised frameworks without significantly modifying existing structures or increasing the computational complexity of denoisers. To address this problem, we propose a SC strategy for multiple denoisers. This strategy can achieve significant performance improvement without increasing the inference complexity of the GAN-based denoising framework. Its basic idea is to iteratively replace the previous less powerful denoiser in the filter-guided noise extraction module with the current powerful denoiser. This process generates better synthetic clean-noisy image pairs, leading to a more powerful denoiser for the next iteration. This baseline ensures the stability and effectiveness of the training network. The experimental results demonstrate the superiority of our method over state-of-the-art unsupervised methods.
</details>
<details>
<summary>摘要</summary>
深度学习方法在图像除噪方面表现了非常出色，特别是在大规模对应数据集上训练的情况下。然而，在实际场景中获得这些对应数据集是一项非常困难的任务。尽管使用生成对抗网络来实现无监督的推荐方法可以解决这个问题，但是这些方法往往难以超越传统的GAN基础架构下的性能限制，而且不需要明显地修改现有结构或增加推荐器的计算复杂度。为解决这个问题，我们提出了一种SC策略。这种策略可以在GAN基础架构下实现明显的性能提升，而无需增加推荐器的推理复杂度。其基本思想是在滤波器导向噪音提取模块中，iteratively替换之前的较弱推荐器，使得当前的强大推荐器可以在下一轮中使用。这个过程生成了更好的干净清噪图像对，从而导致更强大的推荐器。这个基准保证了训练网络的稳定性和效果。实验结果表明，我们的方法在无监督方法中表现出了superiority。
</details></li>
</ul>
<hr>
<h2 id="Tissue-Segmentation-of-Thick-Slice-Fetal-Brain-MR-Scans-with-Guidance-from-High-Quality-Isotropic-Volumes"><a href="#Tissue-Segmentation-of-Thick-Slice-Fetal-Brain-MR-Scans-with-Guidance-from-High-Quality-Isotropic-Volumes" class="headerlink" title="Tissue Segmentation of Thick-Slice Fetal Brain MR Scans with Guidance from High-Quality Isotropic Volumes"></a>Tissue Segmentation of Thick-Slice Fetal Brain MR Scans with Guidance from High-Quality Isotropic Volumes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06762">http://arxiv.org/abs/2308.06762</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shijie Huang, Xukun Zhang, Zhiming Cui, He Zhang, Geng Chen, Dinggang Shen</li>
<li>for: 这个研究旨在提高胎儿脑 MR 影像中的组织分类精度，以便重建高精度的胎儿脑 MR 影像量和评估胎儿脑发展。</li>
<li>methods: 这个研究使用了领域适应技术，将高品质的胎儿脑 MR 影像作为指导，对厚层胎儿脑 MR 影像进行分类。</li>
<li>results: 研究结果显示，这个方法可以很好地改善胎儿脑 MR 影像中的组织分类精度，与现有的方法相比，表现更加出色。<details>
<summary>Abstract</summary>
Accurate tissue segmentation of thick-slice fetal brain magnetic resonance (MR) scans is crucial for both reconstruction of isotropic brain MR volumes and the quantification of fetal brain development. However, this task is challenging due to the use of thick-slice scans in clinically-acquired fetal brain data. To address this issue, we propose to leverage high-quality isotropic fetal brain MR volumes (and also their corresponding annotations) as guidance for segmentation of thick-slice scans. Due to existence of significant domain gap between high-quality isotropic volume (i.e., source data) and thick-slice scans (i.e., target data), we employ a domain adaptation technique to achieve the associated knowledge transfer (from high-quality <source> volumes to thick-slice <target> scans). Specifically, we first register the available high-quality isotropic fetal brain MR volumes across different gestational weeks to construct longitudinally-complete source data. To capture domain-invariant information, we then perform Fourier decomposition to extract image content and style codes. Finally, we propose a novel Cycle-Consistent Domain Adaptation Network (C2DA-Net) to efficiently transfer the knowledge learned from high-quality isotropic volumes for accurate tissue segmentation of thick-slice scans. Our C2DA-Net can fully utilize a small set of annotated isotropic volumes to guide tissue segmentation on unannotated thick-slice scans. Extensive experiments on a large-scale dataset of 372 clinically acquired thick-slice MR scans demonstrate that our C2DA-Net achieves much better performance than cutting-edge methods quantitatively and qualitatively.
</details>
<details>
<summary>摘要</summary>
通过借助高质量的ISO体积脑MR图像（以及其相应的标注），我们提议利用这些图像作为厚层扫描图像的指导。由于高质量ISO体积图像和厚层扫描图像之间存在域之间的差距，我们采用域适应技术来实现相关的知识传递。特别是，我们首先将可用的高质量ISO体积脑MR图像进行了 longitudinally-complete的注册，以构建不同 gestational 周的完整的源数据。然后，我们通过Fourier分解来提取图像内容和风格代码。最后，我们提出了一种名为C2DA-Net的循环相互适应域适应网络，以高效地传递高质量ISO体积图像中学习的知识来进行厚层扫描图像的精准组织分割。我们的C2DA-Net可以充分利用一小组标注的ISO体积图像来导引厚层扫描图像的组织分割。我们对372个临床获取的厚层扫描MR扫描图像进行了广泛的实验，并证明了我们的C2DA-Net在量和质量上都与当今的方法相比较为出色。
</details></li>
</ul>
<hr>
<h2 id="FastLLVE-Real-Time-Low-Light-Video-Enhancement-with-Intensity-Aware-Lookup-Table"><a href="#FastLLVE-Real-Time-Low-Light-Video-Enhancement-with-Intensity-Aware-Lookup-Table" class="headerlink" title="FastLLVE: Real-Time Low-Light Video Enhancement with Intensity-Aware Lookup Table"></a>FastLLVE: Real-Time Low-Light Video Enhancement with Intensity-Aware Lookup Table</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06749">http://arxiv.org/abs/2308.06749</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wenhao-li-777/fastllve">https://github.com/wenhao-li-777/fastllve</a></li>
<li>paper_authors: Wenhao Li, Guangyang Wu, Wenyi Wang, Peiran Ren, Xiaohong Liu</li>
<li>for: 提高低光照视频质量，保持视频的时间协调性。</li>
<li>methods: 利用Look-Up-Table（LUT）技术，实现高效的低光照视频提高。设计了一个可学习的Intensity-Aware LUT（IA-LUT）模块，以适应低动态范围问题。</li>
<li>results: 实验结果表明，我们的方法在质量和时间协调性两个方面均达到了领先水平。与现有的单帧图像基于方法相比，我们的方法可以在1080p视频中实现50+帧&#x2F;秒的处理速度，并且可以保持高质量结果。<details>
<summary>Abstract</summary>
Low-Light Video Enhancement (LLVE) has received considerable attention in recent years. One of the critical requirements of LLVE is inter-frame brightness consistency, which is essential for maintaining the temporal coherence of the enhanced video. However, most existing single-image-based methods fail to address this issue, resulting in flickering effect that degrades the overall quality after enhancement. Moreover, 3D Convolution Neural Network (CNN)-based methods, which are designed for video to maintain inter-frame consistency, are computationally expensive, making them impractical for real-time applications. To address these issues, we propose an efficient pipeline named FastLLVE that leverages the Look-Up-Table (LUT) technique to maintain inter-frame brightness consistency effectively. Specifically, we design a learnable Intensity-Aware LUT (IA-LUT) module for adaptive enhancement, which addresses the low-dynamic problem in low-light scenarios. This enables FastLLVE to perform low-latency and low-complexity enhancement operations while maintaining high-quality results. Experimental results on benchmark datasets demonstrate that our method achieves the State-Of-The-Art (SOTA) performance in terms of both image quality and inter-frame brightness consistency. More importantly, our FastLLVE can process 1,080p videos at $\mathit{50+}$ Frames Per Second (FPS), which is $\mathit{2 \times}$ faster than SOTA CNN-based methods in inference time, making it a promising solution for real-time applications. The code is available at https://github.com/Wenhao-Li-777/FastLLVE.
</details>
<details>
<summary>摘要</summary>
低光照视频增强（LLVE）在过去几年内得到了广泛关注。一个重要的需求是 между帧亮度一致性，以保持视频增强后的时间一致性。然而，大多数单张图像基本方法无法解决这个问题，导致干扰效应，从而降低总质量。此外，基于3D convolutional neural network（CNN）的方法，它们是为视频维护间帧一致性而设计的，但它们计算成本高，使其在实时应用中不实际。为解决这些问题，我们提出了一个高效的排序管道，即快速LLVE，该管道利用Look-Up-Table（LUT）技术保持间帧亮度一致性。特别是，我们设计了一个可学习的Intensity-Aware LUT（IA-LUT）模块，用于自适应增强，解决低动态问题在低光照场景中。这使得快速LLVE可以在低延迟和低复杂度下进行增强操作，同时保持高质量结果。实验结果表明，我们的方法在标准测试集上达到了状态方法（SOTA）的性能，并且在帧率和亮度一致性两个指标上均有显著提高。此外，我们的快速LLVE可以处理1080P视频，并在50+帧/秒的速度下进行增强，这比SOTA CNN基本方法在推理时间上快两倍，使其成为实时应用的优秀解决方案。代码可以在https://github.com/Wenhao-Li-777/FastLLVE上获取。
</details></li>
</ul>
<hr>
<h2 id="Self-supervised-Noise2noise-Method-Utilizing-Corrupted-Images-with-a-Modular-Network-for-LDCT-Denoising"><a href="#Self-supervised-Noise2noise-Method-Utilizing-Corrupted-Images-with-a-Modular-Network-for-LDCT-Denoising" class="headerlink" title="Self-supervised Noise2noise Method Utilizing Corrupted Images with a Modular Network for LDCT Denoising"></a>Self-supervised Noise2noise Method Utilizing Corrupted Images with a Modular Network for LDCT Denoising</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06746">http://arxiv.org/abs/2308.06746</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xyuan01/self-supervised-noise2noise-for-ldct">https://github.com/xyuan01/self-supervised-noise2noise-for-ldct</a></li>
<li>paper_authors: Yuting Zhu, Qiang He, Yudong Yao, Yueyang Teng</li>
<li>for: 这篇论文旨在提出一种基于单静电 Tomatoes CT（LDCT）数据的自我监督噪声降低方法，并不需要对比的噪音和清洁数据。</li>
<li>methods: 本研究使用了一种组合自我监督噪声模型和降低噪声策略，包括在LDCT图像中添加多次相似的噪音，并使用这些次生噪音做为训练数据。</li>
<li>results: 实验结果显示，提案的方法在Mayo LDCT数据集上比前一些深度学习方法更有效率。<details>
<summary>Abstract</summary>
Deep learning is a very promising technique for low-dose computed tomography (LDCT) image denoising. However, traditional deep learning methods require paired noisy and clean datasets, which are often difficult to obtain. This paper proposes a new method for performing LDCT image denoising with only LDCT data, which means that normal-dose CT (NDCT) is not needed. We adopt a combination including the self-supervised noise2noise model and the noisy-as-clean strategy. First, we add a second yet similar type of noise to LDCT images multiple times. Note that we use LDCT images based on the noisy-as-clean strategy for corruption instead of NDCT images. Then, the noise2noise model is executed with only the secondary corrupted images for training. We select a modular U-Net structure from several candidates with shared parameters to perform the task, which increases the receptive field without increasing the parameter size. The experimental results obtained on the Mayo LDCT dataset show the effectiveness of the proposed method compared with that of state-of-the-art deep learning methods. The developed code is available at https://github.com/XYuan01/Self-supervised-Noise2Noise-for-LDCT.
</details>
<details>
<summary>摘要</summary>
深度学习是低剂量 computed tomography（LDCT）图像减噪的非常有前途的技术。然而，传统的深度学习方法通常需要对噪声和清晰图像的对应对进行预处理，这可能具有困难。这篇论文提出了一种使用仅LDCT数据进行LDCT图像减噪的新方法。我们采用了一种组合，包括自我超级vised noise2noise模型和噪声作为清晰Strategy。首先，我们在LDCT图像中添加了多个类似的噪声。请注意，我们使用LDCT图像来实现损害代替NDCT图像。然后，我们在噪声模型中进行训练，使用只有次噪声图像。我们选择了一种模块化U-Net结构，其中共享参数来完成任务，这将增加了感知场景而不会增加参数的大小。我们在Mayo LDCT数据集上进行实验，并证明了提议方法的有效性，比对已有的深度学习方法更高。开发的代码可以在https://github.com/XYuan01/Self-supervised-Noise2Noise-for-LDCT中找到。
</details></li>
</ul>
<hr>
<h2 id="Polyp-SAM-Can-A-Text-Guided-SAM-Perform-Better-for-Polyp-Segmentation"><a href="#Polyp-SAM-Can-A-Text-Guided-SAM-Perform-Better-for-Polyp-Segmentation" class="headerlink" title="Polyp-SAM++: Can A Text Guided SAM Perform Better for Polyp Segmentation?"></a>Polyp-SAM++: Can A Text Guided SAM Perform Better for Polyp Segmentation?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06623">http://arxiv.org/abs/2308.06623</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/RisabBiswas/Polyp-SAM-PlusPlus">https://github.com/RisabBiswas/Polyp-SAM-PlusPlus</a></li>
<li>paper_authors: Risab Biswas</li>
<li>for: 本研究旨在提高肠Rectal cancer的诊断和治疗，通过使用文本提示来改进SAM模型，以提高肠脏膜膜蛋白分 segmentation的精度和稳定性。</li>
<li>methods: 本研究使用的是Segment Anything Model (SAM)，并通过文本提示来改进SAM模型，提高其对肠脏膜膜蛋白分 segmentation的能力。</li>
<li>results: 研究表明，使用文本提示的SAM模型可以提高肠脏膜膜蛋白分 segmentation的精度和稳定性，并且比未使用文本提示的SAM模型更好地处理不同的肠脏膜膜蛋白分样本。<details>
<summary>Abstract</summary>
Meta recently released SAM (Segment Anything Model) which is a general-purpose segmentation model. SAM has shown promising results in a wide variety of segmentation tasks including medical image segmentation. In the field of medical image segmentation, polyp segmentation holds a position of high importance, thus creating a model which is robust and precise is quite challenging. Polyp segmentation is a fundamental task to ensure better diagnosis and cure of colorectal cancer. As such in this study, we will see how Polyp-SAM++, a text prompt-aided SAM, can better utilize a SAM using text prompting for robust and more precise polyp segmentation. We will evaluate the performance of a text-guided SAM on the polyp segmentation task on benchmark datasets. We will also compare the results of text-guided SAM vs unprompted SAM. With this study, we hope to advance the field of polyp segmentation and inspire more, intriguing research. The code and other details will be made publically available soon at https://github.com/RisabBiswas/Polyp-SAM++.
</details>
<details>
<summary>摘要</summary>
Meta 最近发布了 SAM（Segment Anything Model），这是一种通用分割模型。SAM 在各种分割任务中表现出了扎实的成果，包括医学影像分割。在医学影像分割领域，肿瘤分割具有非常高的重要性，因此创建一个精准和Robust的模型是非常挑战性的。肿瘤分割是检测和治疗抗Rectal cancer的基本任务之一。在这项研究中，我们将看到Polyp-SAM++ 是如何使用文本提示来更好地利用 SAM 进行肿瘤分割。我们将对 Polyp-SAM++ 在标准数据集上进行评估，并与不提示 SAM 进行比较。我们希望通过这项研究，推动肿瘤分割领域的进步，并鼓励更多的有趣的研究。代码和其他细节将于 https://github.com/RisabBiswas/Polyp-SAM++ 上公开。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/13/eess.IV_2023_08_13/" data-id="clltau95c00dvcr88aqaih465" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_12" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/12/cs.LG_2023_08_12/" class="article-date">
  <time datetime="2023-08-11T16:00:00.000Z" itemprop="datePublished">2023-08-12</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/12/cs.LG_2023_08_12/">cs.LG - 2023-08-12 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="CoverNav-Cover-Following-Navigation-Planning-in-Unstructured-Outdoor-Environment-with-Deep-Reinforcement-Learning"><a href="#CoverNav-Cover-Following-Navigation-Planning-in-Unstructured-Outdoor-Environment-with-Deep-Reinforcement-Learning" class="headerlink" title="CoverNav: Cover Following Navigation Planning in Unstructured Outdoor Environment with Deep Reinforcement Learning"></a>CoverNav: Cover Following Navigation Planning in Unstructured Outdoor Environment with Deep Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06594">http://arxiv.org/abs/2308.06594</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jumman Hossain, Abu-Zaher Faridee, Nirmalya Roy, Anjan Basak, Derrik E. Asher</li>
<li>For: 本研究旨在提出一种基于深度强化学习（DRL）算法，帮助无人地面车辆在隐蔽的情况下安全地导航到预定的目的地。* Methods: 本研究使用了DRL算法，计算了地方成本图，帮助机器人选择低高度的路径，并在检测到观察者时，使用自然障碍物（如岩石、房屋、瘫痪车辆、树木等）作为隐蔽物。* Results: 研究表明，CoverNav可以在 Unity 模拟环境中保证动态可行性，并在不同高度场景下实现最大目标距离和成功率。与当前最佳方法相比，CoverNav 没有妥协精度。<details>
<summary>Abstract</summary>
Autonomous navigation in offroad environments has been extensively studied in the robotics field. However, navigation in covert situations where an autonomous vehicle needs to remain hidden from outside observers remains an underexplored area. In this paper, we propose a novel Deep Reinforcement Learning (DRL) based algorithm, called CoverNav, for identifying covert and navigable trajectories with minimal cost in offroad terrains and jungle environments in the presence of observers. CoverNav focuses on unmanned ground vehicles seeking shelters and taking covers while safely navigating to a predefined destination. Our proposed DRL method computes a local cost map that helps distinguish which path will grant the maximal covertness while maintaining a low cost trajectory using an elevation map generated from 3D point cloud data, the robot's pose, and directed goal information. CoverNav helps robot agents to learn the low elevation terrain using a reward function while penalizing it proportionately when it experiences high elevation. If an observer is spotted, CoverNav enables the robot to select natural obstacles (e.g., rocks, houses, disabled vehicles, trees, etc.) and use them as shelters to hide behind. We evaluate CoverNav using the Unity simulation environment and show that it guarantees dynamically feasible velocities in the terrain when fed with an elevation map generated by another DRL based navigation algorithm. Additionally, we evaluate CoverNav's effectiveness in achieving a maximum goal distance of 12 meters and its success rate in different elevation scenarios with and without cover objects. We observe competitive performance comparable to state of the art (SOTA) methods without compromising accuracy.
</details>
<details>
<summary>摘要</summary>
自主导航在非路面环境中已经得到了机器人学Field的广泛研究。然而，在情报人员发现自动驾驶车辆的情况下，自主导航仍然是一个未得到充分研究的领域。在这篇论文中，我们提出了一种基于深度优化学习（DRL）算法，称为CoverNav，用于在非路面环境中寻找最佳隐蔽和可行的轨迹，并且尽量降低成本。CoverNav关注于无人地面车辆在安全地 navigate到预定目的地点时，找到遮盾和避险的方法。我们提出的DRL方法计算了当地的成本地图，以帮助选择最佳隐蔽的路径，同时维护低成本轨迹。如果检测到了观察者，CoverNav允许机器人使用自然障碍物（如岩石、房屋、瘫痪车辆、树木等）作为遮盾，隐藏自己。我们使用Unity simulate环境评估CoverNav，并证明它在地形图生成自 another DRL基 Navigation algorithm时能够保证动态可行速度。此外，我们在不同高度场景下评估CoverNav的效果，并发现其与SOTA方法相比，没有妥协精度。
</details></li>
</ul>
<hr>
<h2 id="Value-Distributional-Model-Based-Reinforcement-Learning"><a href="#Value-Distributional-Model-Based-Reinforcement-Learning" class="headerlink" title="Value-Distributional Model-Based Reinforcement Learning"></a>Value-Distributional Model-Based Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06590">http://arxiv.org/abs/2308.06590</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/djdprogramming/adfa2">https://github.com/djdprogramming/adfa2</a></li>
<li>paper_authors: Carlos E. Luis, Alessandro G. Bottero, Julia Vinogradska, Felix Berkenkamp, Jan Peters</li>
<li>for: 这个论文目的是为了解决sequential decision-making任务中的uncertainty quantification问题。</li>
<li>methods: 这个论文使用了model-based Bayesian reinforcement learning的方法，其中的目标是学习Markov决策过程中参数不确定性induced的 posterior distribution over value functions。</li>
<li>results: 论文的实验表明，EQR算法可以在 continuous-control tasks 中比Established model-based和model-free算法表现出性能优势。<details>
<summary>Abstract</summary>
Quantifying uncertainty about a policy's long-term performance is important to solve sequential decision-making tasks. We study the problem from a model-based Bayesian reinforcement learning perspective, where the goal is to learn the posterior distribution over value functions induced by parameter (epistemic) uncertainty of the Markov decision process. Previous work restricts the analysis to a few moments of the distribution over values or imposes a particular distribution shape, e.g., Gaussians. Inspired by distributional reinforcement learning, we introduce a Bellman operator whose fixed-point is the value distribution function. Based on our theory, we propose Epistemic Quantile-Regression (EQR), a model-based algorithm that learns a value distribution function that can be used for policy optimization. Evaluation across several continuous-control tasks shows performance benefits with respect to established model-based and model-free algorithms.
</details>
<details>
<summary>摘要</summary>
<<SYS>>量化政策长期表现的不确定性是解决sequential decision-making任务的重要问题。我们从model-based Bayesian reinforcement learning的视角 изуча这个问题，目标是学习Markov决策过程中参数（эпистемиче）不确定性引起的 posterior distribution over value functions。先前的工作只考虑了这些分布的一些瞬间或假设了特定的分布形式，例如 Gaussian。 inspirited by distributional reinforcement learning, we introduce a Bellman operator whose fixed-point is the value distribution function。 Based on our theory, we propose Epistemic Quantile-Regression (EQR), a model-based algorithm that learns a value distribution function that can be used for policy optimization. 评估在多个连续控制任务上表现出与已有的model-based和model-free算法相比的性能优势。Note: Please note that the translation is in Simplified Chinese, which is one of the two standard versions of Chinese. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Approximate-Answering-of-Graph-Queries"><a href="#Approximate-Answering-of-Graph-Queries" class="headerlink" title="Approximate Answering of Graph Queries"></a>Approximate Answering of Graph Queries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06585">http://arxiv.org/abs/2308.06585</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Cochez, Dimitrios Alivanistos, Erik Arakelyan, Max Berrendorf, Daniel Daza, Mikhail Galkin, Pasquale Minervini, Mathias Niepert, Hongyu Ren</li>
<li>for: 本文旨在介绍几种方法，以帮助回答含有不完整信息的知识图（KG）中的查询。</li>
<li>methods: 本文提出了多种方法，包括基于预测、基于潜在相似性、基于证据等方法，以满足不同类型的查询需求。</li>
<li>results: 这些方法可以帮助解决各种查询问题，如答案推断、 Entity Disambiguation、 Relation extraction 等。但是，这些方法受到图数据不完整和不准确的限制。<details>
<summary>Abstract</summary>
Knowledge graphs (KGs) are inherently incomplete because of incomplete world knowledge and bias in what is the input to the KG. Additionally, world knowledge constantly expands and evolves, making existing facts deprecated or introducing new ones. However, we would still want to be able to answer queries as if the graph were complete. In this chapter, we will give an overview of several methods which have been proposed to answer queries in such a setting. We will first provide an overview of the different query types which can be supported by these methods and datasets typically used for evaluation, as well as an insight into their limitations. Then, we give an overview of the different approaches and describe them in terms of expressiveness, supported graph types, and inference capabilities.
</details>
<details>
<summary>摘要</summary>
知识图（KG）自然而然地是不完整的，因为世界知识的不完整和输入KG中的偏见。此外，世界知识不断扩展和发展，使现有的事实过时或引入新的事实。然而，我们仍然希望能够回答问题，作为如果图完整一样。在这章中，我们将给出不同类型的查询支持的方法的概述，以及通常用于评估的数据集，以及这些方法的局限性。然后，我们将对不同的方法进行描述，包括表达力、支持的图类型和推理能力。
</details></li>
</ul>
<hr>
<h2 id="A-new-solution-and-concrete-implementation-steps-for-Artificial-General-Intelligence"><a href="#A-new-solution-and-concrete-implementation-steps-for-Artificial-General-Intelligence" class="headerlink" title="A new solution and concrete implementation steps for Artificial General Intelligence"></a>A new solution and concrete implementation steps for Artificial General Intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09721">http://arxiv.org/abs/2308.09721</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongcong Chen, Ting Zeng, Jun Zhang</li>
<li>for: 本文旨在探讨大型模型技术路径的局限性，并提出解决这些局限性的方案，以实现true AGI。</li>
<li>methods: 本文使用了现有技术和方法，包括注意机制、深度学习和补偿学习，并提出了一种新的解决方案。</li>
<li>results: 本文提出的解决方案可以解决大型模型技术路径中的缺陷，并实现true AGI。<details>
<summary>Abstract</summary>
At present, the mainstream artificial intelligence generally adopts the technical path of "attention mechanism + deep learning" + "reinforcement learning". It has made great progress in the field of AIGC (Artificial Intelligence Generated Content), setting off the technical wave of big models[ 2][13 ]. But in areas that need to interact with the actual environment, such as elderly care, home nanny, agricultural production, and vehicle driving, trial and error are expensive and a reinforcement learning process that requires much trial and error is difficult to achieve. Therefore, in order to achieve Artificial General Intelligence(AGI) that can be applied to any field, we need to use both existing technologies and solve the defects of existing technologies, so as to further develop the technological wave of artificial intelligence. In this paper, we analyze the limitations of the technical route of large models, and by addressing these limitations, we propose solutions, thus solving the inherent defects of large models. In this paper, we will reveal how to achieve true AGI step by step.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:现在，主流人工智能通常采用“注意机制+深度学习”+“奖励学习”的技术路径。这种方法在AIGC（人工智能生成内容）领域已经取得了 significanthistorical achievements[ 2][13 ], triggering a technological wave of big models. However, in areas that require interaction with the actual environment, such as elderly care, home nanny, agricultural production, and vehicle driving, trial and error are costly and a reinforcement learning process that requires much trial and error is difficult to achieve. Therefore, to achieve Artificial General Intelligence (AGI) that can be applied to any field, we need to leverage both existing technologies and address the limitations of existing technologies, thereby further developing the technological wave of artificial intelligence. In this paper, we analyze the limitations of the technical route of large models, and by addressing these limitations, we propose solutions, thus solving the inherent defects of large models. Through this paper, we will reveal how to achieve true AGI step by step.
</details></li>
</ul>
<hr>
<h2 id="EquiDiff-A-Conditional-Equivariant-Diffusion-Model-For-Trajectory-Prediction"><a href="#EquiDiff-A-Conditional-Equivariant-Diffusion-Model-For-Trajectory-Prediction" class="headerlink" title="EquiDiff: A Conditional Equivariant Diffusion Model For Trajectory Prediction"></a>EquiDiff: A Conditional Equivariant Diffusion Model For Trajectory Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06564">http://arxiv.org/abs/2308.06564</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kehua Chen, Xianda Chen, Zihan Yu, Meixin Zhu, Hai Yang</li>
<li>for: 预测自动驾驶车辆的未来轨迹是关键的，以确保安全和效率地运行。</li>
<li>methods: 我们提出了一种基于深度生成模型的轨迹预测方法，即EquiDiff。EquiDiff基于 Conditional Diffusion 模型，通过历史信息和随机抽样 Gaussian 噪声来生成未来轨迹。</li>
<li>results: 我们在 NGSIM 数据集上进行了广泛的实验，并证明了 EquiDiff 在短期预测方面的性能较高，但在长期预测方面有些较高的错误率。此外，我们还进行了一个ablation study，以调查各组件对预测精度的贡献。<details>
<summary>Abstract</summary>
Accurate trajectory prediction is crucial for the safe and efficient operation of autonomous vehicles. The growing popularity of deep learning has led to the development of numerous methods for trajectory prediction. While deterministic deep learning models have been widely used, deep generative models have gained popularity as they learn data distributions from training data and account for trajectory uncertainties. In this study, we propose EquiDiff, a deep generative model for predicting future vehicle trajectories. EquiDiff is based on the conditional diffusion model, which generates future trajectories by incorporating historical information and random Gaussian noise. The backbone model of EquiDiff is an SO(2)-equivariant transformer that fully utilizes the geometric properties of location coordinates. In addition, we employ Recurrent Neural Networks and Graph Attention Networks to extract social interactions from historical trajectories. To evaluate the performance of EquiDiff, we conduct extensive experiments on the NGSIM dataset. Our results demonstrate that EquiDiff outperforms other baseline models in short-term prediction, but has slightly higher errors for long-term prediction. Furthermore, we conduct an ablation study to investigate the contribution of each component of EquiDiff to the prediction accuracy. Additionally, we present a visualization of the generation process of our diffusion model, providing insights into the uncertainty of the prediction.
</details>
<details>
<summary>摘要</summary>
准确预测车辆轨迹是自动驾驶车辆运行的安全和效率的关键。随着深度学习的普及，许多方法 для轨迹预测得到了开发。而深度生成模型在训练数据中学习数据分布，并考虑轨迹不确定性，因此在这种情况下变得更加受欢迎。在这项研究中，我们提出了EquiDiff，一种基于 conditional diffusion 模型的深度生成模型，用于预测未来车辆轨迹。EquiDiff 使用 SO(2)-equivariant transformer 作为底层模型，并使用循环神经网络和 Graph Attention Networks 提取历史轨迹中的社会交互。为了评估EquiDiff的性能，我们在 NGSIM 数据集上进行了广泛的实验。我们的结果表明，EquiDiff 在短期预测方面表现出色，但是在长期预测方面有些微的错误。此外，我们进行了ablation study，以investigate EquiDiff 中每个组件对预测精度的贡献。此外，我们还提供了生成过程中 diffusion 模型的视觉化，为预测不确定性提供了更多的信息。
</details></li>
</ul>
<hr>
<h2 id="Human-Behavior-based-Personalized-Meal-Recommendation-and-Menu-Planning-Social-System"><a href="#Human-Behavior-based-Personalized-Meal-Recommendation-and-Menu-Planning-Social-System" class="headerlink" title="Human Behavior-based Personalized Meal Recommendation and Menu Planning Social System"></a>Human Behavior-based Personalized Meal Recommendation and Menu Planning Social System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06549">http://arxiv.org/abs/2308.06549</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tanvir Islam, Anika Rahman Joyita, Md. Golam Rabiul Alam, Mohammad Mehedi Hassan, Md. Rafiul Hassan, Raffaele Gravina</li>
<li>for: 这个研究的目的是为了提供一种基于情感计算的餐Menu建议系统，以满足用户的情感需求和营养需求。</li>
<li>methods: 这个系统使用了问卷调查和偏好认知来获取用户的餐食偏好，并使用EEG信号检测用户对不同餐食的情感反应。然后，使用一种层次ensemble方法预测餐食的情感反应，并使用TOPSIS算法生成一个基于预测结果的餐Menu。</li>
<li>results: 实验结果表明，提出的情感计算、餐Menu建议和自动菜单规划算法都能够在不同评价参数下表现良好。<details>
<summary>Abstract</summary>
The traditional dietary recommendation systems are basically nutrition or health-aware where the human feelings on food are ignored. Human affects vary when it comes to food cravings, and not all foods are appealing in all moods. A questionnaire-based and preference-aware meal recommendation system can be a solution. However, automated recognition of social affects on different foods and planning the menu considering nutritional demand and social-affect has some significant benefits of the questionnaire-based and preference-aware meal recommendations. A patient with severe illness, a person in a coma, or patients with locked-in syndrome and amyotrophic lateral sclerosis (ALS) cannot express their meal preferences. Therefore, the proposed framework includes a social-affective computing module to recognize the affects of different meals where the person's affect is detected using electroencephalography signals. EEG allows to capture the brain signals and analyze them to anticipate affective toward a food. In this study, we have used a 14-channel wireless Emotive Epoc+ to measure affectivity for different food items. A hierarchical ensemble method is applied to predict affectivity upon multiple feature extraction methods and TOPSIS (Technique for Order of Preference by Similarity to Ideal Solution) is used to generate a food list based on the predicted affectivity. In addition to the meal recommendation, an automated menu planning approach is also proposed considering a person's energy intake requirement, affectivity, and nutritional values of the different menus. The bin-packing algorithm is used for the personalized menu planning of breakfast, lunch, dinner, and snacks. The experimental findings reveal that the suggested affective computing, meal recommendation, and menu planning algorithms perform well across a variety of assessment parameters.
</details>
<details>
<summary>摘要</summary>
传统的饮食建议系统基本上是nutrition或健康意识的，忽略了人类的情感 toward food。人类的食欲情绪 varying degree，不同的情感状态下不同的食物都不能吸引人。问卷式和偏好意识的饭单推荐系统可以是一种解决方案。然而，通过自动认知不同食物的社会情感影响和根据饮食需求和社会情感规划饭单，有一些显著的优点。例如，患有严重疾病、昏迷状态或 locked-in syndrome 和 amyotrophic lateral sclerosis (ALS) 的患者无法表达他们的饭单首选。因此，我们的框架包括一个社交情感计算模块，用于识别不同饭物中的情感。我们使用了14核心无线Emotive Epoc+来测量不同食物的情感。我们使用了一种层次ensemble方法来预测情感，并使用TOPSIS (技术 дляOrder of Preference by Similarity to Ideal Solution)来生成基于预测情感的食品列表。此外，我们还提出了一种自动饭单规划方法，考虑人类的能量摄入需求、情感和不同饭单的营养价值。使用了bin-packing算法进行个性化饭单规划的早餐、午餐、晚餐和快餐。实验结果表明，我们提出的情感计算、饭单推荐和饭单规划算法在多种评估参数下表现良好。
</details></li>
</ul>
<hr>
<h2 id="Digital-elevation-model-correction-in-urban-areas-using-extreme-gradient-boosting-land-cover-and-terrain-parameters"><a href="#Digital-elevation-model-correction-in-urban-areas-using-extreme-gradient-boosting-land-cover-and-terrain-parameters" class="headerlink" title="Digital elevation model correction in urban areas using extreme gradient boosting, land cover and terrain parameters"></a>Digital elevation model correction in urban areas using extreme gradient boosting, land cover and terrain parameters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06545">http://arxiv.org/abs/2308.06545</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chukwuma Okolie, Jon Mills, Adedayo Adeleke, Julian Smit</li>
<li>For: The paper aims to enhance the accuracy of medium-resolution digital elevation models (DEMs) in urban areas, specifically in Cape Town, South Africa, for hydrological and environmental modelling.* Methods: The authors use the extreme gradient boosting (XGBoost) ensemble algorithm to correct the DEMs, with eleven predictor variables including elevation, urban footprints, slope, aspect, surface roughness, and more.* Results: The corrected DEMs achieved significant accuracy gains, with a root mean square error (RMSE) improvement of 46-53% for Copernicus DEM and 72-73% for AW3D DEM, compared to other proposed methods. These results demonstrate the potential of gradient boosted trees for enhancing DEM quality and improving hydrological modelling in urban catchments.Here is the same information in Simplified Chinese text, as requested:* For: 这个论文的目的是提高城市区域中的数字高程模型（DEM）的准确性，以便于水文和环境模型。* Methods: 作者使用极限Gradient Boosting（XGBoost）ensemble算法来修正DEM，使用的predictor变量包括高程、城市脚印、坡度、方向、表面荒凉、地形位置指数、地形荒凉指数、地形表面 текстура等 eleven个变量。* Results: 修正后的DEM实现了显著的准确性提高，比如 Copernicus DEM的RMSE提高46-53%，AW3D DEM的RMSE提高72-73%，与其他提议的方法相比。这些结果表明极限Gradient Boosting树可以提高DEM的质量，并且为城市catchments中的水文模型提供改善。<details>
<summary>Abstract</summary>
The accuracy of digital elevation models (DEMs) in urban areas is influenced by numerous factors including land cover and terrain irregularities. Moreover, building artifacts in global DEMs cause artificial blocking of surface flow pathways. This compromises their quality and adequacy for hydrological and environmental modelling in urban landscapes where precise and accurate terrain information is needed. In this study, the extreme gradient boosting (XGBoost) ensemble algorithm is adopted for enhancing the accuracy of two medium-resolution 30m DEMs over Cape Town, South Africa: Copernicus GLO-30 and ALOS World 3D (AW3D). XGBoost is a scalable, portable and versatile gradient boosting library that can solve many environmental modelling problems. The training datasets are comprised of eleven predictor variables including elevation, urban footprints, slope, aspect, surface roughness, topographic position index, terrain ruggedness index, terrain surface texture, vector roughness measure, forest cover and bare ground cover. The target variable (elevation error) was calculated with respect to highly accurate airborne LiDAR. After training and testing, the model was applied for correcting the DEMs at two implementation sites. The correction achieved significant accuracy gains which are competitive with other proposed methods. The root mean square error (RMSE) of Copernicus DEM improved by 46 to 53% while the RMSE of AW3D DEM improved by 72 to 73%. These results showcase the potential of gradient boosted trees for enhancing the quality of DEMs, and for improved hydrological modelling in urban catchments.
</details>
<details>
<summary>摘要</summary>
地数模型（DEM）在城市地区的准确性受到多种因素的影响，包括地表覆盖物和地形 irregularities。此外，全球 DEM 中的建筑物略导致表面流道路径的人工堵塞，从而降低其质量和适用性 для水文环境模型在城市景观中，需要精准和准确的地形信息。在这种研究中，我们采用了极限拟合搅拌（XGBoost）ensemble算法来提高两个中等分辨率 30 m DEM 的准确性，即 Copernicus GLO-30 和 ALOS World 3D（AW3D）。XGBoost 是一种可扩展、可移植和多样的拟合搅拌库，可以解决许多环境模型问题。训练数据集包括 eleven 个预测变量，包括高程、城市脚印、坡度、方向、表面粗糙度、地形坡度指数、地形表面文化、向量粗糙度度量、森林覆盖率和裸地覆盖率。target variable （高程误差）与高精度飞行 LiDAR 进行计算。之后，模型被应用于修正 DEM 的两个实施场景。修正后，DEM 的Root Mean Square Error（RMSE）提高了46%到53%，AW3D DEM 的 RMSE 提高了72%到73%。这些结果显示了拟合搅拌树的潜在可能性，以及对城市流域水文模型的改进。
</details></li>
</ul>
<hr>
<h2 id="Dealing-with-Small-Datasets-for-Deep-Learning-in-Medical-Imaging-An-Evaluation-of-Self-Supervised-Pre-Training-on-CT-Scans-Comparing-Contrastive-and-Masked-Autoencoder-Methods-for-Convolutional-Models"><a href="#Dealing-with-Small-Datasets-for-Deep-Learning-in-Medical-Imaging-An-Evaluation-of-Self-Supervised-Pre-Training-on-CT-Scans-Comparing-Contrastive-and-Masked-Autoencoder-Methods-for-Convolutional-Models" class="headerlink" title="Dealing with Small Datasets for Deep Learning in Medical Imaging: An Evaluation of Self-Supervised Pre-Training on CT Scans Comparing Contrastive and Masked Autoencoder Methods for Convolutional Models"></a>Dealing with Small Datasets for Deep Learning in Medical Imaging: An Evaluation of Self-Supervised Pre-Training on CT Scans Comparing Contrastive and Masked Autoencoder Methods for Convolutional Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06534">http://arxiv.org/abs/2308.06534</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wolfda95/ssl-medicalimagining-cl-mae">https://github.com/wolfda95/ssl-medicalimagining-cl-mae</a></li>
<li>paper_authors: Daniel Wolf, Tristan Payer, Catharina Silvia Lisson, Christoph Gerhard Lisson, Meinrad Beer, Timo Ropinski, Michael Götz</li>
<li>for: 这篇论文旨在探讨deep learning在医疗影像领域中的应用，以减少诊断错误、轻量化医生工作负担，并加快诊断。</li>
<li>methods: 这篇论文使用了自动标注学习方法，包括对大量无标注影像进行自动标注。</li>
<li>results: 研究发现，使用SparK预训方法可以更好地适应小型标注数据，并且在诊断任务中表现更好。<details>
<summary>Abstract</summary>
Deep learning in medical imaging has the potential to minimize the risk of diagnostic errors, reduce radiologist workload, and accelerate diagnosis. Training such deep learning models requires large and accurate datasets, with annotations for all training samples. However, in the medical imaging domain, annotated datasets for specific tasks are often small due to the high complexity of annotations, limited access, or the rarity of diseases. To address this challenge, deep learning models can be pre-trained on large image datasets without annotations using methods from the field of self-supervised learning. After pre-training, small annotated datasets are sufficient to fine-tune the models for a specific task. The most popular self-supervised pre-training approaches in medical imaging are based on contrastive learning. However, recent studies in natural image processing indicate a strong potential for masked autoencoder approaches. Our work compares state-of-the-art contrastive learning methods with the recently introduced masked autoencoder approach "SparK" for convolutional neural networks (CNNs) on medical images. Therefore we pre-train on a large unannotated CT image dataset and fine-tune on several CT classification tasks. Due to the challenge of obtaining sufficient annotated training data in medical imaging, it is of particular interest to evaluate how the self-supervised pre-training methods perform when fine-tuning on small datasets. By experimenting with gradually reducing the training dataset size for fine-tuning, we find that the reduction has different effects depending on the type of pre-training chosen. The SparK pre-training method is more robust to the training dataset size than the contrastive methods. Based on our results, we propose the SparK pre-training for medical imaging tasks with only small annotated datasets.
</details>
<details>
<summary>摘要</summary>
深度学习在医疗影像领域可能减少诊断错误风险，减轻放射学家的工作负担，并加速诊断。深度学习模型的训练需要大量和准确的数据集，并将所有训练样本标注。然而，在医疗影像领域，特定任务的标注数据集经常很小，这可能由标注的复杂性、访问限制或疾病的罕见性引起。为解决这个挑战，可以使用自动标注学习的方法进行深度学习模型的预训练。在预训练后，只需要小量的标注数据集来精度地调整模型 для特定任务。医疗影像领域最受欢迎的自动标注预训练方法是对比学习。然而，最近的自然图像处理研究表明，遮盲 autoencoder 方法有很强的潜在性。我们的工作比较了当前状态的对比学习方法和新引入的遮盲 autoencoder 方法 "SparK" 在医疗影像中的 convolutional neural networks (CNNs) 上。因此，我们预训练在大量无注释 CT 图像数据集上，并在多个 CT 分类任务上进行精度调整。由于医疗影像领域获得足够的注释训练数据是困难的，因此特别关心自动标注预训练方法在小型注释数据集上的性能。通过逐渐减少 fine-tuning 数据集大小的实验，我们发现降低的效果与预训练方法的类型有很大的差异。SparK 预训练方法在训练数据集尺寸减少后表现更加稳定。根据我们的结果，我们建议使用 SparK 预训练方法进行医疗影像任务，只需要小量的注释训练数据。
</details></li>
</ul>
<hr>
<h2 id="Learning-Abstract-Visual-Reasoning-via-Task-Decomposition-A-Case-Study-in-Raven-Progressive-Matrices"><a href="#Learning-Abstract-Visual-Reasoning-via-Task-Decomposition-A-Case-Study-in-Raven-Progressive-Matrices" class="headerlink" title="Learning Abstract Visual Reasoning via Task Decomposition: A Case Study in Raven Progressive Matrices"></a>Learning Abstract Visual Reasoning via Task Decomposition: A Case Study in Raven Progressive Matrices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06528">http://arxiv.org/abs/2308.06528</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jakubkwiatkowski/abstract_compositional_transformer">https://github.com/jakubkwiatkowski/abstract_compositional_transformer</a></li>
<li>paper_authors: Jakub Kwiatkowski, Krzysztof Krawiec</li>
<li>for: 本研究旨在提高 Ravens 进步矩阵（RPM）问题的抽象逻辑能力，通过预测图像中对象的视觉特征和排序来选择答案。</li>
<li>methods: 本研究使用了一种基于 transformer 框架的深度学习模型，通过预测图像中对象的视觉特征和排序来选择答案。研究还考虑了不同的图像分割方法和自我指导学习策略。</li>
<li>results: 实验结果表明，本研究的模型不仅超越了当前最佳方法，还提供了有趣的思路和部分解释，以帮助理解问题的含义。此外，模型的设计还使其具有免疫一些已知 RPM  bencmarks 中的偏见的能力。<details>
<summary>Abstract</summary>
One of the challenges in learning to perform abstract reasoning is that problems are often posed as monolithic tasks, with no intermediate subgoals. In Raven Progressive Matrices (RPM), the task is to choose one of the available answers given a context, where both contexts and answers are composite images featuring multiple objects in various spatial arrangements. As this high-level goal is the only guidance available, learning is challenging and most contemporary solvers tend to be opaque. In this study, we propose a deep learning architecture based on the transformer blueprint which, rather than directly making the above choice, predicts the visual properties of individual objects and their arrangements. The multidimensional predictions obtained in this way are then directly juxtaposed to choose the answer. We consider a few ways in which the model parses the visual input into tokens and several regimes of masking parts of the input in self-supervised training. In experimental assessment, the models not only outperform state-of-the-art methods but also provide interesting insights and partial explanations about the inference. The design of the method also makes it immune to biases that are known to exist in some RPM benchmarks.
</details>
<details>
<summary>摘要</summary>
一个挑战在抽象逻辑学习中是，问题经常是单一任务，没有中间目标。在萨瑟进步矩阵（RPM）中，任务是根据Context选择可用的答案，Context和答案都是复杂的图像，包含多个物体在不同的空间排列。由于高级目标是唯一的指导，学习是困难的，而大多数当代解决方案都是透明的。在这种研究中，我们提出了基于变换器蓝图的深度学习架构，而不是直接选择上述高级目标，而是预测图像中物体的视觉属性和排列。 obtained in this way are then directly juxtaposed to choose the answer. We consider a few ways in which the model parses the visual input into tokens and several regimes of masking parts of the input in self-supervised training. In experimental assessment, the models not only outperform state-of-the-art methods but also provide interesting insights and partial explanations about the inference. The design of the method also makes it immune to biases that are known to exist in some RPM benchmarks.Here's the translation in Traditional Chinese:一个挑战在抽象逻辑学习中是，问题经常是单一任务，没有中间目标。在萨瑟进步矩阵（RPM）中，任务是根据Context选择可用的答案，Context和答案都是复杂的图像，包含多个物体在不同的空间排列。由于高级目标是唯一的指导，学习是困难的，而大多数当代解决方案都是透明的。在这种研究中，我们提出了基于变数器蓝图的深度学习架构，而不是直接选择上述高级目标，而是预测图像中物体的视觉属性和排列。 obtained in this way are then directly juxtaposed to choose the answer. We consider a few ways in which the model parses the visual input into tokens and several regimes of masking parts of the input in self-supervised training. In experimental assessment, the models not only outperform state-of-the-art methods but also provide interesting insights and partial explanations about the inference. The design of the method also makes it immune to biases that are known to exist in some RPM benchmarks.
</details></li>
</ul>
<hr>
<h2 id="SLoRA-Federated-Parameter-Efficient-Fine-Tuning-of-Language-Models"><a href="#SLoRA-Federated-Parameter-Efficient-Fine-Tuning-of-Language-Models" class="headerlink" title="SLoRA: Federated Parameter Efficient Fine-Tuning of Language Models"></a>SLoRA: Federated Parameter Efficient Fine-Tuning of Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06522">http://arxiv.org/abs/2308.06522</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sara Babakniya, Ahmed Roushdy Elkordy, Yahya H. Ezzeldin, Qingfeng Liu, Kee-Bong Song, Mostafa El-Khamy, Salman Avestimehr</li>
<li>for: 这个论文主要针对的是如何使用 parameter efficient fine-tuning (PEFT) 方法在 Federated Learning (FL) 中进行语言任务的训练。</li>
<li>methods: 本文使用了 FL 技术和 PEFT 方法，并进行了实验研究以探讨在不同的数据场景下的可行性和挑战。</li>
<li>results: 实验结果表明，当用户数据变得更加多样化时，PEFT 方法与全量精度训练的性能差距逐渐增大。为了bridge这个性能差距，本文提出了一种名为 SLoRA 的方法，通过一种新的数据驱动初始化技术来超越 LoRA 在高多样数据场景下的限制。SLoRA 方法可以实现与全量精度训练相当的性能，并且可以减少训练时间，并且可以减少稀疏更新的数量。<details>
<summary>Abstract</summary>
Transfer learning via fine-tuning pre-trained transformer models has gained significant success in delivering state-of-the-art results across various NLP tasks. In the absence of centralized data, Federated Learning (FL) can benefit from distributed and private data of the FL edge clients for fine-tuning. However, due to the limited communication, computation, and storage capabilities of edge devices and the huge sizes of popular transformer models, efficient fine-tuning is crucial to make federated training feasible. This work explores the opportunities and challenges associated with applying parameter efficient fine-tuning (PEFT) methods in different FL settings for language tasks. Specifically, our investigation reveals that as the data across users becomes more diverse, the gap between fully fine-tuning the model and employing PEFT methods widens. To bridge this performance gap, we propose a method called SLoRA, which overcomes the key limitations of LoRA in high heterogeneous data scenarios through a novel data-driven initialization technique. Our experimental results demonstrate that SLoRA achieves performance comparable to full fine-tuning, with significant sparse updates with approximately $\sim 1\%$ density while reducing training time by up to $90\%$.
</details>
<details>
<summary>摘要</summary>
通过精细调整已经训练过的变换器模型，通过中央化数据的缺失， Federated Learning (FL) 可以利用分布式和私有的 Edge 客户端数据进行 fine-tuning。然而，由于 Edge 设备的通信、计算和存储能力的限制，以及各种变换器模型的巨大大小，高效的 fine-tuning 是在 federated 训练中实现可行的。这种工作探讨了在语言任务中应用 parameter efficient fine-tuning (PEFT) 方法的机会和挑战。具体来说，我们的调查发现，当用户数据变得更加多样化时，具有完全 fine-tuning 模型和使用 PEFT 方法之间的性能差距变得更加明显。为了补做这个性能差距，我们提出了一种名为 SLoRA 的方法，通过一种新的数据驱动初始化技术，超越 LoRA 在高多样化数据场景中的关键限制。我们的实验结果表明，SLoRA 可以与完全 fine-tuning 性能相似，使用约 $\sim 1\%$ 的稀疏更新，同时降低训练时间达到 $90\%$。
</details></li>
</ul>
<hr>
<h2 id="One-bit-Flip-is-All-You-Need-When-Bit-flip-Attack-Meets-Model-Training"><a href="#One-bit-Flip-is-All-You-Need-When-Bit-flip-Attack-Meets-Model-Training" class="headerlink" title="One-bit Flip is All You Need: When Bit-flip Attack Meets Model Training"></a>One-bit Flip is All You Need: When Bit-flip Attack Meets Model Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07934">http://arxiv.org/abs/2308.07934</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jianshuod/tba">https://github.com/jianshuod/tba</a></li>
<li>paper_authors: Jianshuo Dong, Han Qiu, Yiming Li, Tianwei Zhang, Yuanjie Li, Zeqi Lai, Chao Zhang, Shu-Tao Xia</li>
<li>for: 保持深度神经网络（DNNs）的安全性，因为它们在实际设备上广泛部署。</li>
<li>methods: 使用记忆FAULT INJECT技术，如行ammer，对量化模型进行攻击。只需要一些位置的变化，目标模型可以变成一个随机估计或者恶意功能模型。</li>
<li>results: 在基准数据集上，攻击者可以轻松地通过flipping一个关键位的变化，将高风险模型转换为恶意模型。此外，我们的攻击还能够绕过一些防御机制。代码可以在 \url{<a target="_blank" rel="noopener" href="https://github.com/jianshuod/TBA%7D">https://github.com/jianshuod/TBA}</a> 上复制。<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) are widely deployed on real-world devices. Concerns regarding their security have gained great attention from researchers. Recently, a new weight modification attack called bit flip attack (BFA) was proposed, which exploits memory fault inject techniques such as row hammer to attack quantized models in the deployment stage. With only a few bit flips, the target model can be rendered useless as a random guesser or even be implanted with malicious functionalities. In this work, we seek to further reduce the number of bit flips. We propose a training-assisted bit flip attack, in which the adversary is involved in the training stage to build a high-risk model to release. This high-risk model, obtained coupled with a corresponding malicious model, behaves normally and can escape various detection methods. The results on benchmark datasets show that an adversary can easily convert this high-risk but normal model to a malicious one on victim's side by \textbf{flipping only one critical bit} on average in the deployment stage. Moreover, our attack still poses a significant threat even when defenses are employed. The codes for reproducing main experiments are available at \url{https://github.com/jianshuod/TBA}.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Performance-Analysis-for-Resource-Constrained-Decentralized-Federated-Learning-Over-Wireless-Networks"><a href="#Performance-Analysis-for-Resource-Constrained-Decentralized-Federated-Learning-Over-Wireless-Networks" class="headerlink" title="Performance Analysis for Resource Constrained Decentralized Federated Learning Over Wireless Networks"></a>Performance Analysis for Resource Constrained Decentralized Federated Learning Over Wireless Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06496">http://arxiv.org/abs/2308.06496</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhigang Yan, Dong Li</li>
<li>for: 这个研究旨在测试和优化内存和通信参数以提高 Federated Learning（FL）的可靠性和效率。</li>
<li>methods: 这个研究使用了分布式 Federated Learning（DFL）框架，并使用了不同的通信方案（数位和分数）来分析它们的通信效率。</li>
<li>results: 研究发现，在不同的通信方案下，这个框架可以提供内存和通信参数的优化，以提高模型的训练效率和可靠性。<details>
<summary>Abstract</summary>
Federated learning (FL) can lead to significant communication overhead and reliance on a central server. To address these challenges, decentralized federated learning (DFL) has been proposed as a more resilient framework. DFL involves parameter exchange between devices through a wireless network. This study analyzes the performance of resource-constrained DFL using different communication schemes (digital and analog) over wireless networks to optimize communication efficiency. Specifically, we provide convergence bounds for both digital and analog transmission approaches, enabling analysis of the model performance trained on DFL. Furthermore, for digital transmission, we investigate and analyze resource allocation between computation and communication and convergence rates, obtaining its communication complexity and the minimum probability of correction communication required for convergence guarantee. For analog transmission, we discuss the impact of channel fading and noise on the model performance and the maximum errors accumulation with convergence guarantee over fading channels. Finally, we conduct numerical simulations to evaluate the performance and convergence rate of convolutional neural networks (CNNs) and Vision Transformer (ViT) trained in the DFL framework on fashion-MNIST and CIFAR-10 datasets. Our simulation results validate our analysis and discussion, revealing how to improve performance by optimizing system parameters under different communication conditions.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 可能会带来重要的通信负担和依赖中央服务器。为了解决这些挑战，分散式 federated learning (DFL) 已经被提议作为一个更可靠的框架。DFL 中各个设备之间的参数交换通过无线网络。本研究分析了受限制的 DFL 在无线网络上的执行效率，使用不同的通信方案（数位和分散）。特别是，我们提供了两种通信方案的整合边界值，以便分析模型在 DFL 中的表现。此外，我们还调查了在数位传输中的资源分配和计算和通信的复杂度，以及它们对模型表现的影响。另外，我们还分析了随机传输中频道折射和噪音对模型表现的影响，以及在折射频道上累累的最大错误累累。最后，我们对 fashion-MNIST 和 CIFAR-10 数据集上的 CNNs 和 ViT 在 DFL 框架中进行了数值模拟，以评估其表现和融合率。我们的模拟结果证实了我们的分析和讨论，显示了如何通过优化系统参数来改善表现，不同的通信条件下。
</details></li>
</ul>
<hr>
<h2 id="Flexible-Keyword-Spotting-based-on-Homogeneous-Audio-Text-Embedding"><a href="#Flexible-Keyword-Spotting-based-on-Homogeneous-Audio-Text-Embedding" class="headerlink" title="Flexible Keyword Spotting based on Homogeneous Audio-Text Embedding"></a>Flexible Keyword Spotting based on Homogeneous Audio-Text Embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06472">http://arxiv.org/abs/2308.06472</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kumari Nishu, Minsik Cho, Paul Dixon, Devang Naik</li>
<li>for: 这个论文的目的是提出一种高效的关键词检测方法，以便在 audio-text  embedding 空间中快速检测任意关键词。</li>
<li>methods: 该方法使用一个 audio-compliant 文本编码器，将文本转换为phonemes使用 G2P 模型，然后将phonemes转换为嵌入使用表示性强的音频编码器生成的phoneme вектор。此外，该方法还使用杂音词生成来提高audio-text embedding验证器的强度。</li>
<li>results: 实验结果表明，该方法在 Libriphrase 难 dataset 上超过了州�类-国度的Result（AUC：84.21% → 92.7%，EER：23.36% → 14.4%）， indicating that our scheme can efficiently detect arbitrary keywords in audio-text embedding space with high accuracy.<details>
<summary>Abstract</summary>
Spotting user-defined/flexible keywords represented in text frequently uses an expensive text encoder for joint analysis with an audio encoder in an embedding space, which can suffer from heterogeneous modality representation (i.e., large mismatch) and increased complexity. In this work, we propose a novel architecture to efficiently detect arbitrary keywords based on an audio-compliant text encoder which inherently has homogeneous representation with audio embedding, and it is also much smaller than a compatible text encoder. Our text encoder converts the text to phonemes using a grapheme-to-phoneme (G2P) model, and then to an embedding using representative phoneme vectors, extracted from the paired audio encoder on rich speech datasets. We further augment our method with confusable keyword generation to develop an audio-text embedding verifier with strong discriminative power. Experimental results show that our scheme outperforms the state-of-the-art results on Libriphrase hard dataset, increasing Area Under the ROC Curve (AUC) metric from 84.21% to 92.7% and reducing Equal-Error-Rate (EER) metric from 23.36% to 14.4%.
</details>
<details>
<summary>摘要</summary>
通常，用户定义/灵活关键词在文本中的检测使用昂贵的文本编码器进行联合分析与音频编码器在嵌入空间，这可能会导致不同类型的表达媒体表示（大匹配度差）和增加复杂性。在这种工作中，我们提出一种新的架构，可以有效地检测任意关键词基于兼容音频编码器的文本编码器，该编码器自然具有同 Audio embedding的同一个表示形式，而且比兼容的文本编码器更小。我们的文本编码器将文本转换为音频的phoneme使用图eme-to-phoneme（G2P）模型，然后将其转换为嵌入使用表示音频编码器中的可表示性phoneme вектор。我们进一步增强我们的方法，通过可混淆关键词生成来开发一个具有强 дискриминатив力的音频-文本嵌入验证器。实验结果表明，我们的方案在Libriphrase hard数据集上的Result outperform了状态的aru的结果，从84.21%提高到92.7%，并从23.36%降低到14.4%。
</details></li>
</ul>
<hr>
<h2 id="Volterra-Accentuated-Non-Linear-Dynamical-Admittance-VANYA-to-model-Deforestation-An-Exemplification-from-the-Amazon-Rainforest"><a href="#Volterra-Accentuated-Non-Linear-Dynamical-Admittance-VANYA-to-model-Deforestation-An-Exemplification-from-the-Amazon-Rainforest" class="headerlink" title="Volterra Accentuated Non-Linear Dynamical Admittance (VANYA) to model Deforestation: An Exemplification from the Amazon Rainforest"></a>Volterra Accentuated Non-Linear Dynamical Admittance (VANYA) to model Deforestation: An Exemplification from the Amazon Rainforest</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06471">http://arxiv.org/abs/2308.06471</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karthik R., Ramamoorthy A.</li>
<li>for: 这篇论文是为了研究森林覆盖率的预测，特别是通过VANYA模型，包括预测动物食肉动力学。</li>
<li>methods: 该论文使用了时间序列数据和人工智能技术，包括神经网络和推论算法。</li>
<li>results: 论文通过对亚马逊雨林数据进行预测，显示了VANYA模型的可靠性和准确性，并与其他预测器如LSTM、N-BEATS、RCN进行比较。<details>
<summary>Abstract</summary>
Intelligent automation supports us against cyclones, droughts, and seismic events with recent technology advancements. Algorithmic learning has advanced fields like neuroscience, genetics, and human-computer interaction. Time-series data boosts progress. Challenges persist in adopting these approaches in traditional fields. Neural networks face comprehension and bias issues. AI's expansion across scientific areas is due to adaptable descriptors and combinatorial argumentation. This article focuses on modeling Forest loss using the VANYA Model, incorporating Prey Predator Dynamics. VANYA predicts forest cover, demonstrated on Amazon Rainforest data against other forecasters like Long Short-Term Memory, N-BEATS, RCN.
</details>
<details>
<summary>摘要</summary>
智能自动化支持我们面对风暴、旱情和地震等自然灾害，由于最新的技术进步。算法学习在 neuroscience、遗传学和人机交互等领域得到了进步，时间序列数据也促进了进步。然而，在传统领域采用这些方法还存在挑战。神经网络具有理解和偏见问题。AI在科学领域的扩张归功于可变描述符和组合说服。本文通过使用VANYA模型，包括猎Predator Dinamics，预测森林覆盖率，并与Long Short-Term Memory、N-BEATS、RCN等预测器进行比较。
</details></li>
</ul>
<hr>
<h2 id="Tiny-and-Efficient-Model-for-the-Edge-Detection-Generalization"><a href="#Tiny-and-Efficient-Model-for-the-Edge-Detection-Generalization" class="headerlink" title="Tiny and Efficient Model for the Edge Detection Generalization"></a>Tiny and Efficient Model for the Edge Detection Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06468">http://arxiv.org/abs/2308.06468</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xavysp/teed">https://github.com/xavysp/teed</a></li>
<li>paper_authors: Xavier Soria, Yachuan Li, Mohammad Rouhani, Angel D. Sappa</li>
<li>for: 本研究旨在提高图像Edge detection的简洁性、效率和通用性，对现有的State-of-the-art（SOTA）Edge detection模型进行改进。</li>
<li>methods: 本文提出了一种名为Tiny and Efficient Edge Detector（TEED）的轻量级卷积神经网络，只有58K个参数，比SOTA模型少了99.8%。该模型训练在BIPED dataset上只需30分钟左右，每个epoch只需5分钟左右。</li>
<li>results: 本文的提出的模型具有训练容易、快速收敛的特点，并且预测的边映射质量高。此外，本文还提出了一个新的边检测测试集，包括图像Edge detection和图像分割中常用的样本。代码可以在<a target="_blank" rel="noopener" href="https://github.com/xavysp/TEED%E4%B8%8A%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/xavysp/TEED上下载。</a><details>
<summary>Abstract</summary>
Most high-level computer vision tasks rely on low-level image operations as their initial processes. Operations such as edge detection, image enhancement, and super-resolution, provide the foundations for higher level image analysis. In this work we address the edge detection considering three main objectives: simplicity, efficiency, and generalization since current state-of-the-art (SOTA) edge detection models are increased in complexity for better accuracy. To achieve this, we present Tiny and Efficient Edge Detector (TEED), a light convolutional neural network with only $58K$ parameters, less than $0.2$% of the state-of-the-art models. Training on the BIPED dataset takes $less than 30 minutes$, with each epoch requiring $less than 5 minutes$. Our proposed model is easy to train and it quickly converges within very first few epochs, while the predicted edge-maps are crisp and of high quality. Additionally, we propose a new dataset to test the generalization of edge detection, which comprises samples from popular images used in edge detection and image segmentation. The source code is available in https://github.com/xavysp/TEED.
</details>
<details>
<summary>摘要</summary>
大多数高级计算机视觉任务都基于低级图像操作作为初始过程。操作如边检测、图像提高和超分解，为更高级图像分析提供基础。在这项工作中，我们考虑了三个主要目标：简单、高效和通用，因为当前状态之arte（SOTA）边检测模型在精度方面增加了复杂度。为 достичь这一目标，我们提出了小型和高效的边检测器（TEED），这是一个具有58000个参数的小 convolutional neural network，相对于状态之arte模型的0.2%。在BIPE dataset上训练TEED只需几分钟时间，每个epoch仅需5分钟或更少。我们的提议的模型容易训练，快速 converges于第一些epoch，而预测的边映射具有高质量。此外，我们还提出了一个新的边检测检验集，该集包括来自popular图像的边检测和图像分割领域的样本。代码可以在https://github.com/xavysp/TEED中下载。
</details></li>
</ul>
<hr>
<h2 id="Not-So-Robust-After-All-Evaluating-the-Robustness-of-Deep-Neural-Networks-to-Unseen-Adversarial-Attacks"><a href="#Not-So-Robust-After-All-Evaluating-the-Robustness-of-Deep-Neural-Networks-to-Unseen-Adversarial-Attacks" class="headerlink" title="Not So Robust After All: Evaluating the Robustness of Deep Neural Networks to Unseen Adversarial Attacks"></a>Not So Robust After All: Evaluating the Robustness of Deep Neural Networks to Unseen Adversarial Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06467">http://arxiv.org/abs/2308.06467</a></li>
<li>repo_url: None</li>
<li>paper_authors: Roman Garaev, Bader Rasheed, Adil Khan</li>
<li>for: 挑战当代防御机制对于攻击性质变化的测试</li>
<li>methods: 使用 adversarial attacks  manipulate input data 以测试 DNN 的强项和普遍性</li>
<li>results: 发现 DNN 对于 $L_2$ 和 $L_{\infty}$ 攻击性质的差异，并通过对 DNN 表现的分析和可视化获得更深入的理解。<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) have gained prominence in various applications, such as classification, recognition, and prediction, prompting increased scrutiny of their properties. A fundamental attribute of traditional DNNs is their vulnerability to modifications in input data, which has resulted in the investigation of adversarial attacks. These attacks manipulate the data in order to mislead a DNN. This study aims to challenge the efficacy and generalization of contemporary defense mechanisms against adversarial attacks. Specifically, we explore the hypothesis proposed by Ilyas et. al, which posits that DNN image features can be either robust or non-robust, with adversarial attacks targeting the latter. This hypothesis suggests that training a DNN on a dataset consisting solely of robust features should produce a model resistant to adversarial attacks. However, our experiments demonstrate that this is not universally true. To gain further insights into our findings, we analyze the impact of adversarial attack norms on DNN representations, focusing on samples subjected to $L_2$ and $L_{\infty}$ norm attacks. Further, we employ canonical correlation analysis, visualize the representations, and calculate the mean distance between these representations and various DNN decision boundaries. Our results reveal a significant difference between $L_2$ and $L_{\infty}$ norms, which could provide insights into the potential dangers posed by $L_{\infty}$ norm attacks, previously underestimated by the research community.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-One-dimensional-HEVC-video-steganalysis-method-using-the-Optimality-of-Predicted-Motion-Vectors"><a href="#A-One-dimensional-HEVC-video-steganalysis-method-using-the-Optimality-of-Predicted-Motion-Vectors" class="headerlink" title="A One-dimensional HEVC video steganalysis method using the Optimality of Predicted Motion Vectors"></a>A One-dimensional HEVC video steganalysis method using the Optimality of Predicted Motion Vectors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06464">http://arxiv.org/abs/2308.06464</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jun Li, Minqing Zhang, Ke Niu, Yingnan Zhang, Xiaoyuan Yang</li>
<li>for: 本研究旨在提高掩埋检测性能，对高效视频编码标准（HEVC）中的动态vector域基于视频掩埋进行检测。</li>
<li>methods: 该研究提出了基于优化的动态vectorprediction（AMVP）技术的一种掩埋特征，即HEVC视频中的信息投射可能会破坏当地优化的动态vectorprediction（MVP）。然后，定义HEVC视频中的MVP优化率作为掩埋检测特征。</li>
<li>results: 通过在两个通用数据集上进行检测，研究发现，对于所有的覆盖视频，MVP优化率都为100%，而对于所有的掩埋视频，MVP优化率小于100%。因此，该掩埋方法可以准确地分辨覆盖视频和掩埋视频，并且在实际应用中具有无模型训练和低计算复杂度。<details>
<summary>Abstract</summary>
Among steganalysis techniques, detection against motion vector (MV) domain-based video steganography in High Efficiency Video Coding (HEVC) standard remains a hot and challenging issue. For the purpose of improving the detection performance, this paper proposes a steganalysis feature based on the optimality of predicted MVs with a dimension of one. Firstly, we point out that the motion vector prediction (MVP) of the prediction unit (PU) encoded using the Advanced Motion Vector Prediction (AMVP) technique satisfies the local optimality in the cover video. Secondly, we analyze that in HEVC video, message embedding either using MVP index or motion vector differences (MVD) may destroy the above optimality of MVP. And then, we define the optimal rate of MVP in HEVC video as a steganalysis feature. Finally, we conduct steganalysis detection experiments on two general datasets for three popular steganography methods and compare the performance with four state-of-the-art steganalysis methods. The experimental results show that the proposed optimal rate of MVP for all cover videos is 100\%, while the optimal rate of MVP for all stego videos is less than 100\%. Therefore, the proposed steganography scheme can accurately distinguish between cover videos and stego videos, and it is efficiently applied to practical scenarios with no model training and low computational complexity.
</details>
<details>
<summary>摘要</summary>
在隐藏分析技术中，对高效视频编码标准（HEVC）中的动态 vector domain-based 视频隐藏技术进行检测仍然是一个热点和挑战。为了提高检测性能，本文提出了基于预测动态 vector（MVP）的隐藏特征。首先，我们指出了HEVC视频中的预测单元（PU）使用高级动态 vector prediction（AMVP）技术预测的动态 vector prediction（MVP）满足了本地优化性。其次，我们分析了在HEVC视频中，使用MVP index或动态 vector differences（MVD）进行消息嵌入可能会破坏MVP的优化性。然后，我们定义HEVC视频中的MVP优化率作为隐藏特征。最后，我们对两个通用数据集上三种流行的隐藏方法进行了隐藏检测实验，并与四种现状顶尖隐藏检测方法进行比较。实验结果显示，提议的MVP优化率对所有封装视频是100%，而对所有隐藏视频是less than 100%。因此，提议的隐藏方法可以准确地分辨封装视频和隐藏视频，并且可以应用于实际场景中无模型训练和低计算复杂度。
</details></li>
</ul>
<hr>
<h2 id="Multi-Label-Knowledge-Distillation"><a href="#Multi-Label-Knowledge-Distillation" class="headerlink" title="Multi-Label Knowledge Distillation"></a>Multi-Label Knowledge Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06453">http://arxiv.org/abs/2308.06453</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/penghui-yang/l2d">https://github.com/penghui-yang/l2d</a></li>
<li>paper_authors: Penghui Yang, Ming-Kun Xie, Chen-Chen Zong, Lei Feng, Gang Niu, Masashi Sugiyama, Sheng-Jun Huang</li>
<li>for: 该 paper 是关于多标签学习的知识储存方法的研究，它针对现有的知识储存方法在多标签学习场景中的局限性，并提出了一种新的多标签知识储存方法。</li>
<li>methods: 该 paper 使用了分类预测器和学生网络，并将知识储存于 label-wise embeddings 中。它还利用了分类预测器的准确率来提高学生网络的分类性能。</li>
<li>results: 实验结果表明，该 paper 的方法可以避免知识冲突现象，并在多个 benchmark 数据集上达到了superior的性能。<details>
<summary>Abstract</summary>
Existing knowledge distillation methods typically work by imparting the knowledge of output logits or intermediate feature maps from the teacher network to the student network, which is very successful in multi-class single-label learning. However, these methods can hardly be extended to the multi-label learning scenario, where each instance is associated with multiple semantic labels, because the prediction probabilities do not sum to one and feature maps of the whole example may ignore minor classes in such a scenario. In this paper, we propose a novel multi-label knowledge distillation method. On one hand, it exploits the informative semantic knowledge from the logits by dividing the multi-label learning problem into a set of binary classification problems; on the other hand, it enhances the distinctiveness of the learned feature representations by leveraging the structural information of label-wise embeddings. Experimental results on multiple benchmark datasets validate that the proposed method can avoid knowledge counteraction among labels, thus achieving superior performance against diverse comparing methods. Our code is available at: https://github.com/penghui-yang/L2D
</details>
<details>
<summary>摘要</summary>
传统的知识填充方法通常是通过将教师网络的输出LOGIT或中间特征图传递给学生网络，这在多类单标学习中非常成功。然而，这些方法几乎无法扩展到多标学习场景，因为预测概率不同类别之间不相加，特征图中涉及到小类时可能被忽略。在这篇论文中，我们提出了一种新的多标知识填充方法。一方面，它利用多标学习问题的分类器的semantic知识，将问题分解成一系列的binary分类问题。另一方面，它利用标签wise嵌入结构来增强学习的特征表示的分化性。我们的实验结果表明，提案的方法可以避免标签之间的知识冲突，从而在多种 comparing方法的比较中达到更高的性能。我们的代码可以在：https://github.com/penghui-yang/L2D 查看。
</details></li>
</ul>
<hr>
<h2 id="Latent-Random-Steps-as-Relaxations-of-Max-Cut-Min-Cut-and-More"><a href="#Latent-Random-Steps-as-Relaxations-of-Max-Cut-Min-Cut-and-More" class="headerlink" title="Latent Random Steps as Relaxations of Max-Cut, Min-Cut, and More"></a>Latent Random Steps as Relaxations of Max-Cut, Min-Cut, and More</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06448">http://arxiv.org/abs/2308.06448</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sudhanshu Chanpuriya, Cameron Musco</li>
<li>for: 本 paper 是为了探讨图像 clustering 中的缺省结构，并提出了一种基于非正式矩阵分解的 probabilistic 模型，用于统一 clustering 和简化图像。</li>
<li>methods: 本 paper 使用的方法是基于非正式矩阵分解的一种 probabilistic 模型，用于模型图像的结构。该模型通过 Random Walk 过程的 фактор化来实现 clustering 和简化图像的同时进行。</li>
<li>results: 本 paper 的结果表明，使用该方法可以很好地处理具有缺省结构的图像，并且可以很好地处理一些涉及多类别的不约分类任务。<details>
<summary>Abstract</summary>
Algorithms for node clustering typically focus on finding homophilous structure in graphs. That is, they find sets of similar nodes with many edges within, rather than across, the clusters. However, graphs often also exhibit heterophilous structure, as exemplified by (nearly) bipartite and tripartite graphs, where most edges occur across the clusters. Grappling with such structure is typically left to the task of graph simplification. We present a probabilistic model based on non-negative matrix factorization which unifies clustering and simplification, and provides a framework for modeling arbitrary graph structure. Our model is based on factorizing the process of taking a random walk on the graph. It permits an unconstrained parametrization, allowing for optimization via simple gradient descent. By relaxing the hard clustering to a soft clustering, our algorithm relaxes potentially hard clustering problems to a tractable ones. We illustrate our algorithm's capabilities on a synthetic graph, as well as simple unsupervised learning tasks involving bipartite and tripartite clustering of orthographic and phonological data.
</details>
<details>
<summary>摘要</summary>
algorithm для clustering 通常是找到同类结点的结构。即它们在集群内部找到多个边，而不是跨集群。但是图 oftentimes 也具有异类结构，如（几乎）二分图和三分图，其中大多数边在集群之间。对这种结构的处理通常被归入图简化任务。我们提出了一种基于非负矩阵因子化的概率模型，该模型结合了 clustering 和简化，并提供了对任意图结构的模型化框架。我们的模型基于对图进行随机游走的过程的分解。它允许不受限制的参数化，通过简单的梯度下降优化。通过宽松化硬 clustering 到软 clustering，我们的算法将硬 clustering 问题转化为可解决的问题。我们在一个 sintetic 图上以及一些无监督学习任务中应用了我们的算法，包括orthographic 和 phonological 数据的二分和三分 clustering。
</details></li>
</ul>
<hr>
<h2 id="A-Sequential-Meta-Transfer-SMT-Learning-to-Combat-Complexities-of-Physics-Informed-Neural-Networks-Application-to-Composites-Autoclave-Processing"><a href="#A-Sequential-Meta-Transfer-SMT-Learning-to-Combat-Complexities-of-Physics-Informed-Neural-Networks-Application-to-Composites-Autoclave-Processing" class="headerlink" title="A Sequential Meta-Transfer (SMT) Learning to Combat Complexities of Physics-Informed Neural Networks: Application to Composites Autoclave Processing"></a>A Sequential Meta-Transfer (SMT) Learning to Combat Complexities of Physics-Informed Neural Networks: Application to Composites Autoclave Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06447">http://arxiv.org/abs/2308.06447</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/miladramzy/sequentialmetatransferpinns">https://github.com/miladramzy/sequentialmetatransferpinns</a></li>
<li>paper_authors: Milad Ramezankhani, Abbas S. Milani</li>
<li>for: 解决非线性偏微分方程（PDE）问题，即使在长时间域内。</li>
<li>methods: 使用 физи学 informed neural networks（PINNs）和sequential meta-transfer（SMT）学习框架。</li>
<li>results: 比传统PINNs更高效地解决复杂系统问题，并且具有更好的适应性。<details>
<summary>Abstract</summary>
Physics-Informed Neural Networks (PINNs) have gained popularity in solving nonlinear partial differential equations (PDEs) via integrating physical laws into the training of neural networks, making them superior in many scientific and engineering applications. However, conventional PINNs still fall short in accurately approximating the solution of complex systems with strong nonlinearity, especially in long temporal domains. Besides, since PINNs are designed to approximate a specific realization of a given PDE system, they lack the necessary generalizability to efficiently adapt to new system configurations. This entails computationally expensive re-training from scratch for any new change in the system. To address these shortfalls, in this work a novel sequential meta-transfer (SMT) learning framework is proposed, offering a unified solution for both fast training and efficient adaptation of PINNs in highly nonlinear systems with long temporal domains. Specifically, the framework decomposes PDE's time domain into smaller time segments to create "easier" PDE problems for PINNs training. Then for each time interval, a meta-learner is assigned and trained to achieve an optimal initial state for rapid adaptation to a range of related tasks. Transfer learning principles are then leveraged across time intervals to further reduce the computational cost.Through a composites autoclave processing case study, it is shown that SMT is clearly able to enhance the adaptability of PINNs while significantly reducing computational cost, by a factor of 100.
</details>
<details>
<summary>摘要</summary>
physics-informed neural networks (PINNs) 已经在解决非线性偏微分方程 (PDEs) 中获得了广泛应用，通过将物理法则 integrate 到 neural networks 的训练中，使其在科学和工程应用中脱颖而出。然而，传统 PINNs 仍然缺乏对复杂系统的准确描述能力，特别是在长时间领域中。此外，由于 PINNs 是设计来描述特定的 PDE 系统实现，因此缺乏能够快速适应新系统配置的一般化能力。这会导致 computationally expensive re-training from scratch 的问题。为了解决这些不足，这个研究提出了一个新的sequential meta-transfer (SMT) 学习框架，可以提供快速训练和高效适应 PINNs 的解决方案。具体来说，这个框架将 PDE 的时间领域 decomposed 为 smaller time segments，则将每个时间段赋予一个 meta-learner 进行训练，以实现快速适应一系列相关任务的能力。然后，通过将 transfer learning 原则应用到时间intervals，进一步降低 computional cost。通过一个 composite autoclave processing 案例研究，显示了 SMT 能够优化 PINNs 的适应能力，同时大幅降低 computional cost，比例为 100。
</details></li>
</ul>
<hr>
<h2 id="Neural-Latent-Aligner-Cross-trial-Alignment-for-Learning-Representations-of-Complex-Naturalistic-Neural-Data"><a href="#Neural-Latent-Aligner-Cross-trial-Alignment-for-Learning-Representations-of-Complex-Naturalistic-Neural-Data" class="headerlink" title="Neural Latent Aligner: Cross-trial Alignment for Learning Representations of Complex, Naturalistic Neural Data"></a>Neural Latent Aligner: Cross-trial Alignment for Learning Representations of Complex, Naturalistic Neural Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06443">http://arxiv.org/abs/2308.06443</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheol Jun Cho, Edward F. Chang, Gopala K. Anumanchipalli</li>
<li>for: 本研究旨在解决神经科学中复杂行为的神经实现问题，即找到真实表示神经数据的方法。</li>
<li>methods: 我们提出了一种新的无监督学习框架，神经幽默对应器（NLA），来找到有效、行为相关的神经表示。该方法通过对重复尝试的表示进行对应来学习交叉尝试中的共同信息。此外，我们还提出了一种完全可导时间扭曲模型（TWM）来解决尝试的时间不同问题。</li>
<li>results: 当应用于自然说话的内部电rocorticography（ECoG）数据时，我们的模型可以更好地表示行为，特别是在更低的维度空间中。TWM被验证了通过测量行为协调性 между对应的尝试。我们的框架比基线模型更好地学习了交叉尝试中的共同表示，并且当Visualized时，替换 manifold 显示了在尝试中共享的神经轨迹。<details>
<summary>Abstract</summary>
Understanding the neural implementation of complex human behaviors is one of the major goals in neuroscience. To this end, it is crucial to find a true representation of the neural data, which is challenging due to the high complexity of behaviors and the low signal-to-ratio (SNR) of the signals. Here, we propose a novel unsupervised learning framework, Neural Latent Aligner (NLA), to find well-constrained, behaviorally relevant neural representations of complex behaviors. The key idea is to align representations across repeated trials to learn cross-trial consistent information. Furthermore, we propose a novel, fully differentiable time warping model (TWM) to resolve the temporal misalignment of trials. When applied to intracranial electrocorticography (ECoG) of natural speaking, our model learns better representations for decoding behaviors than the baseline models, especially in lower dimensional space. The TWM is empirically validated by measuring behavioral coherence between aligned trials. The proposed framework learns more cross-trial consistent representations than the baselines, and when visualized, the manifold reveals shared neural trajectories across trials.
</details>
<details>
<summary>摘要</summary>
The key idea of NLA is to align representations across repeated trials to learn cross-trial consistent information. To achieve this, we propose a novel, fully differentiable time warping model (TWM) to resolve the temporal misalignment of trials. When applied to intracranial electrocorticography (ECoG) of natural speaking, our model learns better representations for decoding behaviors than the baseline models, especially in lower dimensional space.The TWM is empirically validated by measuring behavioral coherence between aligned trials. The proposed framework learns more cross-trial consistent representations than the baselines, and when visualized, the manifold reveals shared neural trajectories across trials.Here is the translation in Simplified Chinese:理解人类复杂行为的神经实现是神经科学的一个主要目标。为了实现这一目标，寻找真实的神经数据表示是非常困难的，因为行为的复杂性和神经信号的噪声比（SNR）都很低。在这里，我们提出了一种新的无监督学习框架——神经潜在适应器（NLA），以找到行为相关的神经表示。NLA的关键思想是将重复尝试的表示进行对齐，以学习跨试验可靠信息。为了实现这一点，我们提出了一种全部可导的时间折叠模型（TWM），以解决试验的时间不同问题。当应用于自然语言说话的电rocorticalography（ECoG）时，我们的模型可以比基eline模型更好地学习行为的表示，特别是在lower dimensional space中。TWM的实验验证了我们的模型可以更好地处理行为听起来的听起来的听起来，并且当Visualize的时候，曾经的折叠 manifold  revelas shared neural trajectories across trials。
</details></li>
</ul>
<hr>
<h2 id="A-Domain-adaptive-Physics-informed-Neural-Network-for-Inverse-Problems-of-Maxwell’s-Equations-in-Heterogeneous-Media"><a href="#A-Domain-adaptive-Physics-informed-Neural-Network-for-Inverse-Problems-of-Maxwell’s-Equations-in-Heterogeneous-Media" class="headerlink" title="A Domain-adaptive Physics-informed Neural Network for Inverse Problems of Maxwell’s Equations in Heterogeneous Media"></a>A Domain-adaptive Physics-informed Neural Network for Inverse Problems of Maxwell’s Equations in Heterogeneous Media</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06436">http://arxiv.org/abs/2308.06436</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiyuan Piao, Hong Gu, Aina Wang, Pan Qin</li>
<li>for: 解决Maxwell方程在不同媒质中的逆问题</li>
<li>methods: 使用Physics-informed neural networks (PINNs)和适应域训练策略</li>
<li>results: 在两个案例研究中证明了domain-adaptive PINN的有效性<details>
<summary>Abstract</summary>
Maxwell's equations are a collection of coupled partial differential equations (PDEs) that, together with the Lorentz force law, constitute the basis of classical electromagnetism and electric circuits. Effectively solving Maxwell's equations is crucial in various fields, like electromagnetic scattering and antenna design optimization. Physics-informed neural networks (PINNs) have shown powerful ability in solving PDEs. However, PINNs still struggle to solve Maxwell's equations in heterogeneous media. To this end, we propose a domain-adaptive PINN (da-PINN) to solve inverse problems of Maxwell's equations in heterogeneous media. First, we propose a location parameter of media interface to decompose the whole domain into several sub-domains. Furthermore, the electromagnetic interface conditions are incorporated into a loss function to improve the prediction performance near the interface. Then, we propose a domain-adaptive training strategy for da-PINN. Finally, the effectiveness of da-PINN is verified with two case studies.
</details>
<details>
<summary>摘要</summary>
马克斯威尔方程是一系列联动部分偏微分方程（PDEs），与 Лорен茨力学定律共同组成经典电磁学和电路Circuit。有效解决马克斯威尔方程是许多领域的关键，如电磁散射和天线设计优化。physics-informed neural networks（PINNs）已经表现出解决PDEs的强大能力。然而，PINNs仍然在不同媒体中解决马克斯威尔方程困难。为此，我们提出了域 adaptive PINN（da-PINN）解决Maxwell方程的 inverse problem在不同媒体中。首先，我们提出了媒体界面位置参数，将整个领域分解成多个子领域。然后，我们在损失函数中包含了电磁界面条件，以提高预测性能 near the interface。最后，我们提出了适应域训练策略 для da-PINN。Finally, da-PINN的效果被两个案例验证。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format instead.
</details></li>
</ul>
<hr>
<h2 id="Learn-Single-horizon-Disease-Evolution-for-Predictive-Generation-of-Post-therapeutic-Neovascular-Age-related-Macular-Degeneration"><a href="#Learn-Single-horizon-Disease-Evolution-for-Predictive-Generation-of-Post-therapeutic-Neovascular-Age-related-Macular-Degeneration" class="headerlink" title="Learn Single-horizon Disease Evolution for Predictive Generation of Post-therapeutic Neovascular Age-related Macular Degeneration"></a>Learn Single-horizon Disease Evolution for Predictive Generation of Post-therapeutic Neovascular Age-related Macular Degeneration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06432">http://arxiv.org/abs/2308.06432</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuhan Zhang, Kun Huang, Mingchao Li, Songtao Yuan, Qiang Chen</li>
<li>for: 这 paper 的目的是预测 age-related macular degeneration (nAMD) 的发展。</li>
<li>methods: 这 paper 使用的方法包括 feature encoder、graph evolution module 和 feature decoder。具体来说，feature encoder 将输入 SD-OCT 图像转换为深度特征，然后 graph evolution module 预测了疾病发展过程在高维 latent space 中，并输出了预测的深度特征。最后，feature decoder 将预测的深度特征转换回 SD-OCT 图像。</li>
<li>results: 这 paper 的结果表明，SHENet 可以生成高质量的预测 SD-OCT 图像，同时保持疾病结构和内容的准确性。 qualitative 评估也表明，SHENet 的生成的 SD-OCT 图像比其他方法更有Visual effect。<details>
<summary>Abstract</summary>
Most of the existing disease prediction methods in the field of medical image processing fall into two classes, namely image-to-category predictions and image-to-parameter predictions. Few works have focused on image-to-image predictions. Different from multi-horizon predictions in other fields, ophthalmologists prefer to show more confidence in single-horizon predictions due to the low tolerance of predictive risk. We propose a single-horizon disease evolution network (SHENet) to predictively generate post-therapeutic SD-OCT images by inputting pre-therapeutic SD-OCT images with neovascular age-related macular degeneration (nAMD). In SHENet, a feature encoder converts the input SD-OCT images to deep features, then a graph evolution module predicts the process of disease evolution in high-dimensional latent space and outputs the predicted deep features, and lastly, feature decoder recovers the predicted deep features to SD-OCT images. We further propose an evolution reinforcement module to ensure the effectiveness of disease evolution learning and obtain realistic SD-OCT images by adversarial training. SHENet is validated on 383 SD-OCT cubes of 22 nAMD patients based on three well-designed schemes based on the quantitative and qualitative evaluations. Compared with other generative methods, the generative SD-OCT images of SHENet have the highest image quality. Besides, SHENet achieves the best structure protection and content prediction. Qualitative evaluations also demonstrate that SHENet has a better visual effect than other methods. SHENet can generate post-therapeutic SD-OCT images with both high prediction performance and good image quality, which has great potential to help ophthalmologists forecast the therapeutic effect of nAMD.
</details>
<details>
<summary>摘要</summary>
大多数现有的疾病预测方法在医学图像处理领域都属于两类，即图像到类别预测和图像到参数预测。只有少数作品强调图像到图像预测。与其他多个 horizons 预测不同，眼科医生更偏向于在单个 horizons 上展示更高的预测信任度，这是因为眼科疾病风险预测的偏好。我们提出了单个 horizon 疾病进化网络（SHENet），用于预测基于前治疗 SD-OCT 图像的后治疗 SD-OCT 图像。在 SHENet 中，一个特征编码器将输入 SD-OCT 图像转换为深度特征，然后一个图像进化模块预测疾病进化的过程在高维latent空间中，并输出预测的深度特征。最后，特征解码器将预测的深度特征恢复为 SD-OCT 图像。我们还提出了进化权威模块，以确保疾病进化学习的效果和获得实际的 SD-OCT 图像。SHENet 在 383 个 SD-OCT 立方体上基于三种良好的方案进行验证，并通过量化和质量评价来评估其效果。与其他生成方法相比，SHENet 生成的 SD-OCT 图像的生成质量最高。此外，SHENet 还实现了最好的结构保护和内容预测。质量评价还表明，SHENet 的视觉效果比其他方法更好。SHENet 可以生成具有高预测性和好的图像质量的后治疗 SD-OCT 图像，这有很大的潜在价值，可以帮助眼科医生预测 nAMD 的治疗效果。
</details></li>
</ul>
<hr>
<h2 id="Genetic-heterogeneity-analysis-using-genetic-algorithm-and-network-science"><a href="#Genetic-heterogeneity-analysis-using-genetic-algorithm-and-network-science" class="headerlink" title="Genetic heterogeneity analysis using genetic algorithm and network science"></a>Genetic heterogeneity analysis using genetic algorithm and network science</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06429">http://arxiv.org/abs/2308.06429</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhendong Sha, Yuanzhu Chen, Ting Hu</li>
<li>For: This paper is written to address the challenges of identifying disease susceptible genetic variables using genome-wide association studies (GWAS) due to genetic heterogeneity and feature interactions.* Methods: The paper introduces a novel feature selection mechanism for GWAS called Feature Co-selection Network (FCSNet), which extracts heterogeneous subsets of genetic variables from a network constructed from multiple independent feature selection runs based on a genetic algorithm (GA) and a non-linear machine learning algorithm to detect feature interactions.* Results: The paper shows the effectiveness of the utilized GA-based feature selection method in identifying feature interactions through synthetic data analysis, and applies the novel approach to a case-control colorectal cancer GWAS dataset, resulting in synthetic features that explain the genetic heterogeneity in an additional case-only GWAS dataset.Here’s the simplified Chinese version of the three key points:* For: 这篇论文是为了解决基因组宽协调研究（GWAS）中疾病抵触性基因变量的难题，因为基因多样性和特征交互。* Methods: 论文提出了一种新的特征选择机制，即特征共选网络（FCSNet），它从多个独立的特征选择跑程中提取了多种不同的基因变量，并使用一种进化学习算法（GA）和一种非线性机器学习算法来检测特征交互。* Results: 论文通过synthetic数据分析表明了GA基于的特征选择方法的效果，并应用了这种新方法到一个case-control大肠癌GWAS数据集中，得到了解释基因多样性的Synthetic特征。<details>
<summary>Abstract</summary>
Through genome-wide association studies (GWAS), disease susceptible genetic variables can be identified by comparing the genetic data of individuals with and without a specific disease. However, the discovery of these associations poses a significant challenge due to genetic heterogeneity and feature interactions. Genetic variables intertwined with these effects often exhibit lower effect-size, and thus can be difficult to be detected using machine learning feature selection methods. To address these challenges, this paper introduces a novel feature selection mechanism for GWAS, named Feature Co-selection Network (FCSNet). FCS-Net is designed to extract heterogeneous subsets of genetic variables from a network constructed from multiple independent feature selection runs based on a genetic algorithm (GA), an evolutionary learning algorithm. We employ a non-linear machine learning algorithm to detect feature interaction. We introduce the Community Risk Score (CRS), a synthetic feature designed to quantify the collective disease association of each variable subset. Our experiment showcases the effectiveness of the utilized GA-based feature selection method in identifying feature interactions through synthetic data analysis. Furthermore, we apply our novel approach to a case-control colorectal cancer GWAS dataset. The resulting synthetic features are then used to explain the genetic heterogeneity in an additional case-only GWAS dataset.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:通过全 genomic协同asso ciation研究 (GWAS)，可以通过比较患病者和无病者的基因数据来确定疾病抵触性的基因变量。然而，发现这些相互作用具有一定的挑战，因为基因多样性和特征互动。基因变量与这些效应相互作用的情况经常表现出较低的效果大小，因此可能会难以通过机器学习特征选择方法探测。为解决这些挑战，本文提出了一种新的特征选择机制，名为特征合选网络 (FCSNet)。FCS-Net 是基于多个独立的特征选择跑目的基因算法 (GA) 构建的网络，并使用一种非线性机器学习算法探测特征互动。我们还引入了一个社区风险分数 (CRS)，用于量化每个变量subset 的疾病相关度。我们的实验表明，使用我们提出的 GA 基因选择方法可以通过 sintetic 数据分析来识别特征互动。此外，我们还应用了我们的新方法到一个 case-control 大陆癌 GWAS 数据集。得到的 sintetic 特征后来用于解释一个额外的 case-only GWAS 数据集中的基因多样性。
</details></li>
</ul>
<hr>
<h2 id="Multiclass-Learnability-Does-Not-Imply-Sample-Compression"><a href="#Multiclass-Learnability-Does-Not-Imply-Sample-Compression" class="headerlink" title="Multiclass Learnability Does Not Imply Sample Compression"></a>Multiclass Learnability Does Not Imply Sample Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06424">http://arxiv.org/abs/2308.06424</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chirag Pabbaraju</li>
<li>for: 本研究证明了一个假设集合可以 admit 一种样本压缩 schemes，即对于每个样本被标注为来自该假设集合中的某个假设，只需保留一小样本，可以推断出整个样本的标签。</li>
<li>methods: 本研究使用了learnable binary hypothesis class 和 multiclass hypothesis class，以及其们的VC dimension 和 DS dimension。</li>
<li>results: 本研究发现，learnable binary hypothesis class 总是可以 admit 一种样本压缩 schemes，但是 learnable multiclass hypothesis class 则不一定可以 admit 样本压缩 schemes，即不一定可以通过保留一小样本来推断出整个样本的标签。<details>
<summary>Abstract</summary>
A hypothesis class admits a sample compression scheme, if for every sample labeled by a hypothesis from the class, it is possible to retain only a small subsample, using which the labels on the entire sample can be inferred. The size of the compression scheme is an upper bound on the size of the subsample produced. Every learnable binary hypothesis class (which must necessarily have finite VC dimension) admits a sample compression scheme of size only a finite function of its VC dimension, independent of the sample size. For multiclass hypothesis classes, the analog of VC dimension is the DS dimension. We show that the analogous statement pertaining to sample compression is not true for multiclass hypothesis classes: every learnable multiclass hypothesis class, which must necessarily have finite DS dimension, does not admit a sample compression scheme of size only a finite function of its DS dimension.
</details>
<details>
<summary>摘要</summary>
一个假设集合满足样本压缩方案，如果每个样本被标注为从集合中的假设，那么只需保留一小样本，使得整个样本上的标签可以被推断出。压缩方案的大小是样本上的子样本的上界。每个可学习的二进制假设集合（必然具有有限VC维度）总是具有一个只具有有限函数与样本大小无关的压缩方案。对多类假设集合，相应的VC维度的概念是DS维度。我们显示了，相应的假设集合不是真的：每个可学习的多类假设集合，必然具有有限DS维度，但并不总是具有只具有有限函数与DS维度无关的压缩方案。
</details></li>
</ul>
<hr>
<h2 id="Sensitivity-Aware-Mixed-Precision-Quantization-and-Width-Optimization-of-Deep-Neural-Networks-Through-Cluster-Based-Tree-Structured-Parzen-Estimation"><a href="#Sensitivity-Aware-Mixed-Precision-Quantization-and-Width-Optimization-of-Deep-Neural-Networks-Through-Cluster-Based-Tree-Structured-Parzen-Estimation" class="headerlink" title="Sensitivity-Aware Mixed-Precision Quantization and Width Optimization of Deep Neural Networks Through Cluster-Based Tree-Structured Parzen Estimation"></a>Sensitivity-Aware Mixed-Precision Quantization and Width Optimization of Deep Neural Networks Through Cluster-Based Tree-Structured Parzen Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06422">http://arxiv.org/abs/2308.06422</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seyedarmin Azizi, Mahdi Nazemi, Arash Fayyazi, Massoud Pedram</li>
<li>for: 这个研究旨在提高深度学习模型的设计优化，以提高模型的效率。</li>
<li>methods: 本研究使用了一种新的搜寻机制，可以自动选择个别神经网络层的最佳位元和层宽。这些搜寻机制利用了希腊数-基于的删除，以确保删除不必要的参数。然后，我们使用了一种基于对应的树结构的Parzen估计器，以建立优化的对应模型。</li>
<li>results: 我们的方法在知名的数据集上进行了严谨的测试，与现有的方法相比， recording an impressive 20% decrease in model size without compromising accuracy. In addition, our method boasts a 12x reduction in search time relative to the best search-focused strategies currently available.<details>
<summary>Abstract</summary>
As the complexity and computational demands of deep learning models rise, the need for effective optimization methods for neural network designs becomes paramount. This work introduces an innovative search mechanism for automatically selecting the best bit-width and layer-width for individual neural network layers. This leads to a marked enhancement in deep neural network efficiency. The search domain is strategically reduced by leveraging Hessian-based pruning, ensuring the removal of non-crucial parameters. Subsequently, we detail the development of surrogate models for favorable and unfavorable outcomes by employing a cluster-based tree-structured Parzen estimator. This strategy allows for a streamlined exploration of architectural possibilities and swift pinpointing of top-performing designs. Through rigorous testing on well-known datasets, our method proves its distinct advantage over existing methods. Compared to leading compression strategies, our approach records an impressive 20% decrease in model size without compromising accuracy. Additionally, our method boasts a 12x reduction in search time relative to the best search-focused strategies currently available. As a result, our proposed method represents a leap forward in neural network design optimization, paving the way for quick model design and implementation in settings with limited resources, thereby propelling the potential of scalable deep learning solutions.
</details>
<details>
<summary>摘要</summary>
深度学习模型的复杂性和计算需求逐渐增长，因此选择最佳的神经网络层宽和批处理层宽成为了至关重要的一环。这项工作提出了一种新的搜索机制，可以自动选择具有最佳性能的神经网络层宽和批处理层宽。通过利用层次结构的Parzen估计器来构建封闭的搜索空间，我们可以快速地探索不同的建筑方案，并快速地找到最佳的设计。我们通过对知名数据集进行严格的测试，证明了我们的方法与现有方法相比，可以减少模型大小20%，同时保持准确性。此外，我们的方法可以减少搜索时间12倍，相比于目前最佳的搜索焦点策略。因此，我们的提议方法 represents a significant advance in neural network design optimization, paving the way for rapid model design and implementation in resource-constrained settings, and thereby accelerating the potential of scalable deep learning solutions.
</details></li>
</ul>
<hr>
<h2 id="Pedestrian-Trajectory-Prediction-in-Pedestrian-Vehicle-Mixed-Environments-A-Systematic-Review"><a href="#Pedestrian-Trajectory-Prediction-in-Pedestrian-Vehicle-Mixed-Environments-A-Systematic-Review" class="headerlink" title="Pedestrian Trajectory Prediction in Pedestrian-Vehicle Mixed Environments: A Systematic Review"></a>Pedestrian Trajectory Prediction in Pedestrian-Vehicle Mixed Environments: A Systematic Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06419">http://arxiv.org/abs/2308.06419</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahsa Golchoubian, Moojan Ghafurian, Kerstin Dautenhahn, Nasser Lashgarian Azad</li>
<li>for: 本研究旨在提供一种实用的行人轨迹预测算法，用于自动驾驶车辆（AV）在与行人共同使用的空间中规划路径。</li>
<li>methods: 本文系统性地查询了在文献中提出的不同方法，用于模拟行人轨迹预测在交通工具存在下。文中还详细讨论了与交通工具交互对行人未来动向的影响，以及不同变量如预测不确定性和行为差异如何在先前提出的预测模型中被考虑。</li>
<li>results: 文中提出了1260个唯一的同行评审文章，从ACM数字图书馆、IEEE Xplore和Scopus数据库中搜索到。64篇文章符合包含和排除条件，因此被包含在最终审查中。文中还提供了各种轨迹数据集的概述，包括行人和交通工具的轨迹数据。文中还讨论了未来研究中的潜在漏洞和方向，如更有效的交互代理在深度学习方法中定义，以及更多的混合交通环境中的数据采集。<details>
<summary>Abstract</summary>
Planning an autonomous vehicle's (AV) path in a space shared with pedestrians requires reasoning about pedestrians' future trajectories. A practical pedestrian trajectory prediction algorithm for the use of AVs needs to consider the effect of the vehicle's interactions with the pedestrians on pedestrians' future motion behaviours. In this regard, this paper systematically reviews different methods proposed in the literature for modelling pedestrian trajectory prediction in presence of vehicles that can be applied for unstructured environments. This paper also investigates specific considerations for pedestrian-vehicle interaction (compared with pedestrian-pedestrian interaction) and reviews how different variables such as prediction uncertainties and behavioural differences are accounted for in the previously proposed prediction models. PRISMA guidelines were followed. Articles that did not consider vehicle and pedestrian interactions or actual trajectories, and articles that only focused on road crossing were excluded. A total of 1260 unique peer-reviewed articles from ACM Digital Library, IEEE Xplore, and Scopus databases were identified in the search. 64 articles were included in the final review as they met the inclusion and exclusion criteria. An overview of datasets containing trajectory data of both pedestrians and vehicles used by the reviewed papers has been provided. Research gaps and directions for future work, such as having more effective definition of interacting agents in deep learning methods and the need for gathering more datasets of mixed traffic in unstructured environments are discussed.
</details>
<details>
<summary>摘要</summary>
планирование пути автономного транспортного средства (АВ) в пространстве, где присутствуют пешеходы, требует рассмотрения предполагаемых траекторий пешеходов в будущем. практический алгоритм предсказания траекторий пешеходов для использования АВов в неструктурированных средах должен учитывать влияние взаимодействия автомобиля с пешеходами на будущие движения пешеходов. в этом смысле, эта статья систематически обзорывает разные методы, предложенные в литературе для моделирования предсказания траекторий пешеходов в присутствии автомобилей, которые могут быть применены в неструктурированных средах. эта статья также рассматривает конкретные аспекты взаимодействия пешехода-автомобиль (в сравнении с взаимодействием пешеходов-пешеходов) и обсуждает, как различные переменные, такие как неопределенности предсказаний и различия в поведении, учитываются в предыдущих моделях предсказания. following PRISMA guidelines, articles that did not consider vehicle and pedestrian interactions or actual trajectories, and articles that only focused on road crossing were excluded. a total of 1260 unique peer-reviewed articles from ACM Digital Library, IEEE Xplore, and Scopus databases were identified in the search. 64 articles were included in the final review as they met the inclusion and exclusion criteria. an overview of datasets containing trajectory data of both pedestrians and vehicles used by the reviewed papers has been provided. research gaps and directions for future work, such as having more effective definition of interacting agents in deep learning methods and the need for gathering more datasets of mixed traffic in unstructured environments, are discussed.
</details></li>
</ul>
<hr>
<h2 id="Learning-Bayesian-Networks-with-Heterogeneous-Agronomic-Data-Sets-via-Mixed-Effect-Models-and-Hierarchical-Clustering"><a href="#Learning-Bayesian-Networks-with-Heterogeneous-Agronomic-Data-Sets-via-Mixed-Effect-Models-and-Hierarchical-Clustering" class="headerlink" title="Learning Bayesian Networks with Heterogeneous Agronomic Data Sets via Mixed-Effect Models and Hierarchical Clustering"></a>Learning Bayesian Networks with Heterogeneous Agronomic Data Sets via Mixed-Effect Models and Hierarchical Clustering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06399">http://arxiv.org/abs/2308.06399</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lorenzo Vallegi, Marco Scutari, Federico Mattia Stefanini</li>
<li>for:  agronomic studies, handling hierarchical data with complex networks of causal relationships</li>
<li>methods:  Bayesian networks (BNs) with integrated random effects, based on linear mixed-effects models</li>
<li>results:  enhanced structural learning, discovery of new connections, improved model specification, reduction in prediction errors from 28% to 17%<details>
<summary>Abstract</summary>
Research involving diverse but related data sets, where associations between covariates and outcomes may vary, is prevalent in various fields including agronomic studies. In these scenarios, hierarchical models, also known as multilevel models, are frequently employed to assimilate information from different data sets while accommodating their distinct characteristics. However, their structure extend beyond simple heterogeneity, as variables often form complex networks of causal relationships.   Bayesian networks (BNs) provide a powerful framework for modelling such relationships using directed acyclic graphs to illustrate the connections between variables. This study introduces a novel approach that integrates random effects into BN learning. Rooted in linear mixed-effects models, this approach is particularly well-suited for handling hierarchical data. Results from a real-world agronomic trial suggest that employing this approach enhances structural learning, leading to the discovery of new connections and the improvement of improved model specification. Furthermore, we observe a reduction in prediction errors from 28\% to 17\%. By extending the applicability of BNs to complex data set structures, this approach contributes to the effective utilisation of BNs for hierarchical agronomic data. This, in turn, enhances their value as decision-support tools in the field.
</details>
<details>
<summary>摘要</summary>
研究涉及多元相关数据集，其中covariates和结果变量之间存在关系的现象，在各个领域，如农学研究，非常普遍。在这些情况下，层次模型，也称为多级模型，经常被使用，以融合不同数据集的信息，同时适应它们的特点。然而，这些结构超出了简单的不同性，因为变量经常形成复杂的 causal 关系网络。� Bayesian networks（BNs）提供了一个强大的模型化这些关系的框架，使用导向的无环图来示出变量之间的连接。本研究提出了一种新的方法，将随机效应 integrate into BN 学习。基于线性混合效应模型，这种方法特别适用于处理层次数据。实际 agronomic 试验结果表明，通过使用这种方法，可以提高结构学习，发现新的连接，并提高模型规定的精度。此外，我们发现预测错误率从28%降至17%。通过扩展 BNs 的应用范围，使其能够更好地处理复杂数据集结构，这种方法增加了 BNs 作为决策支持工具的价值。
</details></li>
</ul>
<hr>
<h2 id="Detecting-and-Preventing-Hallucinations-in-Large-Vision-Language-Models"><a href="#Detecting-and-Preventing-Hallucinations-in-Large-Vision-Language-Models" class="headerlink" title="Detecting and Preventing Hallucinations in Large Vision Language Models"></a>Detecting and Preventing Hallucinations in Large Vision Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06394">http://arxiv.org/abs/2308.06394</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anisha Gunjal, Jihan Yin, Erhan Bas<br>for:The paper aims to address the issue of hallucinations in instruction-tuned large vision language models (LVLMs) for visual question answering (VQA).methods:The authors introduce a new dataset called M-HalDetect, which is a multi-modal hallucination detection dataset for detailed image descriptions. They also propose a novel optimization method called Fine-grained Direct Preference Optimization (FDPO) to reduce hallucinations in LVLMs.results:The authors evaluate the effectiveness of M-HalDetect and FDPO on several state-of-the-art LVLMs, including InstructBLIP, LLaVA, and mPLUG-OWL. They find that M-HalDetect can reduce hallucination rates in InstructBLIP by 41%, and FDPO can reduce hallucination rates by 55%. Additionally, they find that their reward model generalizes well to other multi-modal models and has a strong correlation with human evaluated accuracy scores.<details>
<summary>Abstract</summary>
Instruction tuned Large Vision Language Models (LVLMs) have significantly advanced in generalizing across a diverse set of multi-modal tasks, especially for Visual Question Answering (VQA). However, generating detailed responses that are visually grounded is still a challenging task for these models. We find that even the current state-of-the-art LVLMs (InstructBLIP) still contain a staggering 30 percent of the hallucinatory text in the form of non-existent objects, unfaithful descriptions, and inaccurate relationships. To address this, we introduce M-HalDetect, a (M)ultimodal (Hal)lucination (Detect)ion Dataset that can be used to train and benchmark models for hallucination detection and prevention. M-HalDetect consists of 16k fine-grained annotations on VQA examples, making it the first comprehensive multi-modal hallucination detection dataset for detailed image descriptions. Unlike previous work that only consider object hallucination, we additionally annotate both entity descriptions and relationships that are unfaithful. To demonstrate the potential of this dataset for hallucination prevention, we optimize InstructBLIP through our novel Fine-grained Direct Preference Optimization (FDPO). We also train fine-grained multi-modal reward models from InstructBLIP and evaluate their effectiveness with best-of-n rejection sampling. We perform human evaluation on both FDPO and rejection sampling, and find that they reduce hallucination rates in InstructBLIP by 41% and 55% respectively. We also find that our reward model generalizes to other multi-modal models, reducing hallucinations in LLaVA and mPLUG-OWL by 15% and 57% respectively, and has strong correlation with human evaluated accuracy scores.
</details>
<details>
<summary>摘要</summary>
现代化的启示抽象语言模型（LVLM）在多模态任务上进行总结的能力已经得到了显著提高，尤其是在视觉问答（VQA）领域。然而，使模型生成具有详细Visualgrounding的回答仍然是一个挑战。我们发现，even the current state-of-the-art LVLMs（InstructBLIP）仍然包含了30%的虚假文本，包括不存在的物体、不准确的描述和关系。为解决这个问题，我们介绍了M-HalDetect，一个多模态虚假检测 dataset，可以用于训练和测试模型，以避免虚假检测。M-HalDetect包含16k细致的VQA例子，使其成为了首个多模态虚假检测 dataset。与前一代研究只考虑对象虚假，我们还注释了不准确的实体描述和关系。为证明M-HalDetect的潜在性，我们通过我们的新的精细直接偏好优化（FDPO）来优化InstructBLIP。我们还通过多模态 reward models来训练精细的多模态奖励模型，并通过best-of-n拒绝采样来评估其效果。我们对FDPO和拒绝采样进行了人工评估，并发现它们可以降低InstructBLIP中的虚假率 by 41%和55%。此外，我们发现我们的奖励模型可以泛化到其他多模态模型，降低LLaVA和mPLUG-OWL中的虚假率 by 15%和57%，并与人类评估准确率有强相关性。
</details></li>
</ul>
<hr>
<h2 id="Phoneme-Hallucinator-One-shot-Voice-Conversion-via-Set-Expansion"><a href="#Phoneme-Hallucinator-One-shot-Voice-Conversion-via-Set-Expansion" class="headerlink" title="Phoneme Hallucinator: One-shot Voice Conversion via Set Expansion"></a>Phoneme Hallucinator: One-shot Voice Conversion via Set Expansion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06382">http://arxiv.org/abs/2308.06382</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/PhonemeHallucinator/Phoneme_Hallucinator">https://github.com/PhonemeHallucinator/Phoneme_Hallucinator</a></li>
<li>paper_authors: Siyuan Shan, Yang Li, Amartya Banerjee, Junier B. Oliva</li>
<li>for: 这篇论文的目的是提出一种新的语音变换技术，以提高语音变换的Intelligibility和Speaker Similarity。</li>
<li>methods: 这篇论文使用了一种新的模型，即“Phoneme Hallucinator”，可以基于短时间内的目标说话人声音（例如3秒）生成多样化和高质量的目标说话人音频。</li>
<li>results: 对比于现有的语音变换方法，“Phoneme Hallucinator”在Intelligibility和Speaker Similarity两个指标上均表现出色，并且不需要文本标注和支持任意转换。<details>
<summary>Abstract</summary>
Voice conversion (VC) aims at altering a person's voice to make it sound similar to the voice of another person while preserving linguistic content. Existing methods suffer from a dilemma between content intelligibility and speaker similarity; i.e., methods with higher intelligibility usually have a lower speaker similarity, while methods with higher speaker similarity usually require plenty of target speaker voice data to achieve high intelligibility. In this work, we propose a novel method \textit{Phoneme Hallucinator} that achieves the best of both worlds. Phoneme Hallucinator is a one-shot VC model; it adopts a novel model to hallucinate diversified and high-fidelity target speaker phonemes based just on a short target speaker voice (e.g. 3 seconds). The hallucinated phonemes are then exploited to perform neighbor-based voice conversion. Our model is a text-free, any-to-any VC model that requires no text annotations and supports conversion to any unseen speaker. Objective and subjective evaluations show that \textit{Phoneme Hallucinator} outperforms existing VC methods for both intelligibility and speaker similarity.
</details>
<details>
<summary>摘要</summary>
声音转换（VC）目标是使一个人的声音与另一个人的声音相似，同时保持语言内容的正确性。现有的方法受到一种权衡问题：即具有更高的智能可读性通常具有较低的说话人类似性，而具有更高的说话人类似性通常需要大量的目标说话人声音数据来实现高度的智能可读性。在这项工作中，我们提出了一种新的方法——《phoneme hallucinator》。这是一个一枚VC模型，它采用了一种新的模型来幻化具有多样性和高精度的目标说话人声音，基于短时间内的目标说话人声音（例如3秒）。这些幻化的声音然后被利用来进行邻居基于的声音转换。我们的模型是文本 libre，任何到任何的VC模型，不需要文本注释，并且支持转换到任何未看过的说话人。对象和主观评估表明，《phoneme hallucinator》比既有VC方法更高的智能可读性和说话人类似性。
</details></li>
</ul>
<hr>
<h2 id="DCNFIS-Deep-Convolutional-Neuro-Fuzzy-Inference-System"><a href="#DCNFIS-Deep-Convolutional-Neuro-Fuzzy-Inference-System" class="headerlink" title="DCNFIS: Deep Convolutional Neuro-Fuzzy Inference System"></a>DCNFIS: Deep Convolutional Neuro-Fuzzy Inference System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06378">http://arxiv.org/abs/2308.06378</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mojtaba Yeganejou, Kimia Honari, Ryan Kluzinski, Scott Dick, Michael Lipsett, James Miller</li>
<li>for: 提高可解释人工智能中的透明度和准确性之间的负面选择。</li>
<li>methods: 使用深度神经网络和规则引擎结合深度学习模型，设计了一种新的深度征函数逻辑决策系统（DCNFIS），并证明DCNFIS可以与现有的卷积神经网络相比，在四个常见的数据集上达到相同的准确性。</li>
<li>results: DCNFIS可以在Fashion-MNIST数据集上生成saliency map，并且对这些解释进行了进一步的研究。<details>
<summary>Abstract</summary>
A key challenge in eXplainable Artificial Intelligence is the well-known tradeoff between the transparency of an algorithm (i.e., how easily a human can directly understand the algorithm, as opposed to receiving a post-hoc explanation), and its accuracy. We report on the design of a new deep network that achieves improved transparency without sacrificing accuracy. We design a deep convolutional neuro-fuzzy inference system (DCNFIS) by hybridizing fuzzy logic and deep learning models and show that DCNFIS performs as accurately as three existing convolutional neural networks on four well-known datasets. We furthermore that DCNFIS outperforms state-of-the-art deep fuzzy systems. We then exploit the transparency of fuzzy logic by deriving explanations, in the form of saliency maps, from the fuzzy rules encoded in DCNFIS. We investigate the properties of these explanations in greater depth using the Fashion-MNIST dataset.
</details>
<details>
<summary>摘要</summary>
一个主要挑战在可解释人工智能中是论文知名的质量和可读性之间的贸易。我们报告了一种新的深度网络的设计，该网络可以提高可读性而不 sacrificing 精度。我们设计了一种深度卷积神经推理系统（DCNFIS），通过将神经网络和推理逻辑模型相结合，并证明 DCNFIS 与三种现有的卷积神经网络在四个常见数据集上的性能相同。此外，我们还证明 DCNFIS 在深度推理系统中表现更好。然后，我们利用推理逻辑的可读性，从 DCNFIS 中提取出解释，以干扰 maps 的形式。我们对 Fashion-MNIST 数据集进行更深入的调查，以explore 这些解释的性质。
</details></li>
</ul>
<hr>
<h2 id="UAMM-UBET-Automated-Market-Maker"><a href="#UAMM-UBET-Automated-Market-Maker" class="headerlink" title="UAMM: UBET Automated Market Maker"></a>UAMM: UBET Automated Market Maker</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06375">http://arxiv.org/abs/2308.06375</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Jiwoong Im, Alexander Kondratskiy, Vincent Harvey, Hsuan-Wei Fu</li>
<li>for: 这篇论文是为了解决传统自适应市场制定价机制（AMM）的局限性，提出了一种新的价格计算方法——UBET AMM（UAMM）。</li>
<li>methods: UAMM使用外部市场价格和流动性池的不稳定损失来计算价格，并保持愿意产品曲线的定量性。关键元素是根据目标均衡来确定合适的滑动量，以避免流动性池的不稳定损失。</li>
<li>results: 我们的方法可以在有效的外部市场价格下消除投资机会。<details>
<summary>Abstract</summary>
Automated market makers (AMMs) are pricing mechanisms utilized by decentralized exchanges (DEX). Traditional AMM approaches are constrained by pricing solely based on their own liquidity pool, without consideration of external markets or risk management for liquidity providers. In this paper, we propose a new approach known as UBET AMM (UAMM), which calculates prices by considering external market prices and the impermanent loss of the liquidity pool. Despite relying on external market prices, our method maintains the desired properties of a constant product curve when computing slippages. The key element of UAMM is determining the appropriate slippage amount based on the desired target balance, which encourages the liquidity pool to minimize impermanent loss. We demonstrate that our approach eliminates arbitrage opportunities when external market prices are efficient.
</details>
<details>
<summary>摘要</summary>
自动化市场制造机制（AMM）是分布式交易所（DEX）中使用的价格计算机制。传统的 AMM 方法受限于基于自己的流动性池价格计算，不考虑外部市场或流动性提供者风险管理。在这篇论文中，我们提出了一种新的方法，称为 UBET AMM（UAMM），它根据外部市场价格和流动性池的不稳定损失计算价格。尽管依赖于外部市场价格，我们的方法保持了恒定的产品曲线的属性，当计算滑块时。UBET AMM 的关键元素是确定合适的滑块量，以达到目标均衡。这种办法鼓励流动性池减少不稳定损失。我们示出，当外部市场价格高效时，我们的方法可以消除投资机会。
</details></li>
</ul>
<hr>
<h2 id="Topic-Level-Bayesian-Surprise-and-Serendipity-for-Recommender-Systems"><a href="#Topic-Level-Bayesian-Surprise-and-Serendipity-for-Recommender-Systems" class="headerlink" title="Topic-Level Bayesian Surprise and Serendipity for Recommender Systems"></a>Topic-Level Bayesian Surprise and Serendipity for Recommender Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06368">http://arxiv.org/abs/2308.06368</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ton-moy/surprise-and-serendipity">https://github.com/ton-moy/surprise-and-serendipity</a></li>
<li>paper_authors: Tonmoy Hasan, Razvan Bunescu<br>for: This paper aims to mitigate the filter bubble problem in recommender systems by incorporating serendipity into the recommendation process.methods: The paper proposes a content-based formulation of serendipity that is rooted in Bayesian surprise, and uses this formulation to measure the serendipity of items after they are consumed and rated by the user. The paper also introduces a collaborative-filtering component that identifies similar users.results: The experimental evaluations show that models that use Bayesian surprise correlate much better with the manual annotations of topic-level surprise than distance-based heuristics, and also obtain better serendipitous item recommendation performance.<details>
<summary>Abstract</summary>
A recommender system that optimizes its recommendations solely to fit a user's history of ratings for consumed items can create a filter bubble, wherein the user does not get to experience items from novel, unseen categories. One approach to mitigate this undesired behavior is to recommend items with high potential for serendipity, namely surprising items that are likely to be highly rated. In this paper, we propose a content-based formulation of serendipity that is rooted in Bayesian surprise and use it to measure the serendipity of items after they are consumed and rated by the user. When coupled with a collaborative-filtering component that identifies similar users, this enables recommending items with high potential for serendipity. To facilitate the evaluation of topic-level models for surprise and serendipity, we introduce a dataset of book reading histories extracted from Goodreads, containing over 26 thousand users and close to 1.3 million books, where we manually annotate 449 books read by 4 users in terms of their time-dependent, topic-level surprise. Experimental evaluations show that models that use Bayesian surprise correlate much better with the manual annotations of topic-level surprise than distance-based heuristics, and also obtain better serendipitous item recommendation performance.
</details>
<details>
<summary>摘要</summary>
一个推荐系统可以将推荐项目单独根据用户的项目点击历史进行最佳化，从而创建一个范本径（filter bubble），使用户不会获得来自新、未见类别的项目。为了解决这个问题，我们可以推荐项目具有高度的意外性，即让用户惊喜的项目，这些项目很可能会获得高度的评价。在这篇论文中，我们提出了一个基于 bayesian 的内容基式，用于衡量项目的意外性，并且与协同推荐 ком成组件相结合，从而为用户提供意外性高的项目推荐。为了促进项目级模型的surprise和serendipity的评估，我们创建了一个基于goodreads的阅读历史数据集，包含26,000名用户和1,300,000本书，并 manually annotate 449本书，其中4名用户在不同的时间点阅读这些书籍。实验结果显示，使用 bayesian  surprise 可以与距离基于的规律更好地与手动标注的题目级surprise相对较好，并且也可以获得更好的意外性项目推荐性能。
</details></li>
</ul>
<hr>
<h2 id="Learning-Distributions-via-Monte-Carlo-Marginalization"><a href="#Learning-Distributions-via-Monte-Carlo-Marginalization" class="headerlink" title="Learning Distributions via Monte-Carlo Marginalization"></a>Learning Distributions via Monte-Carlo Marginalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06352">http://arxiv.org/abs/2308.06352</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenqiu Zhao, Guanfang Dong, Anup Basu</li>
<li>for: 学习难以求解的分布。</li>
<li>methods: 使用参数化分布模型（如混合型分布）来 aproximate 难以求解的分布，并使用 Monte-Carlo Marginalization 和 Kernel Density Estimation 解决计算复杂性和优化过程不可导的问题。</li>
<li>results: 提出了一种可以学习复杂分布的方法，该方法可以替代变量推理（VAE），并在标准数据集和 sintetic 数据上进行了实验，证明了该方法的效果。<details>
<summary>Abstract</summary>
We propose a novel method to learn intractable distributions from their samples. The main idea is to use a parametric distribution model, such as a Gaussian Mixture Model (GMM), to approximate intractable distributions by minimizing the KL-divergence. Based on this idea, there are two challenges that need to be addressed. First, the computational complexity of KL-divergence is unacceptable when the dimensions of distributions increases. The Monte-Carlo Marginalization (MCMarg) is proposed to address this issue. The second challenge is the differentiability of the optimization process, since the target distribution is intractable. We handle this problem by using Kernel Density Estimation (KDE). The proposed approach is a powerful tool to learn complex distributions and the entire process is differentiable. Thus, it can be a better substitute of the variational inference in variational auto-encoders (VAE). One strong evidence of the benefit of our method is that the distributions learned by the proposed approach can generate better images even based on a pre-trained VAE's decoder. Based on this point, we devise a distribution learning auto-encoder which is better than VAE under the same network architecture. Experiments on standard dataset and synthetic data demonstrate the efficiency of the proposed approach.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的方法，用于从样本中学习不可解 Distribution。主要想法是使用参数化分布模型，如 Gaussian Mixture Model（GMM），来近似不可解 Distribution，并且通过最小化KL-分布来实现。然而，存在两个挑战：首先，在分布维度增加时，KL-分布的计算复杂度过高；其次，目标分布是不可导的，因此Optimization过程中的导数不存在。我们解决了这两个问题，使用Monte-Carlo Marginalization（MCMarg）和Kernel Density Estimation（KDE）。我们的方法可以学习复杂的分布，整个过程都是导数可导的，因此可以作为VAE中的更好的替补。我们的方法可以在标准数据集和synthetic数据上实现，并且在具有相同网络架构下，我们设计了一个更好的分布学习自动编码器，比VAE更好。Note: Please note that the translation is in Simplified Chinese, and some words or phrases may have been translated differently in Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Mirror-Diffusion-Models"><a href="#Mirror-Diffusion-Models" class="headerlink" title="Mirror Diffusion Models"></a>Mirror Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06342">http://arxiv.org/abs/2308.06342</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cran/DIMORA">https://github.com/cran/DIMORA</a></li>
<li>paper_authors: Jaesung Tae</li>
<li>for: 本研究旨在应用扩散模型到分类数据领域，并提出了一种基于镜像Langevin算法的受限采样问题的理论框架。</li>
<li>methods: 本文提出了一种基于镜像扩散模型的受限采样算法，并在简单的顺序扩散问题上进行了实验 validate。</li>
<li>results: 研究表明，镜像扩散模型在受限采样问题上具有良好的性能，并且可以在各种流行的领域，如图像和文本生成等，进行自然的扩展。<details>
<summary>Abstract</summary>
Diffusion models have successfully been applied to generative tasks in various continuous domains. However, applying diffusion to discrete categorical data remains a non-trivial task. Moreover, generation in continuous domains often requires clipping in practice, which motivates the need for a theoretical framework for adapting diffusion to constrained domains. Inspired by the mirror Langevin algorithm for the constrained sampling problem, in this theoretical report we propose Mirror Diffusion Models (MDMs). We demonstrate MDMs in the context of simplex diffusion and propose natural extensions to popular domains such as image and text generation.
</details>
<details>
<summary>摘要</summary>
Diffusion models have successfully been applied to generative tasks in various continuous domains. However, applying diffusion to discrete categorical data remains a non-trivial task. Moreover, generation in continuous domains often requires clipping in practice, which motivates the need for a theoretical framework for adapting diffusion to constrained domains. Inspired by the mirror Langevin algorithm for the constrained sampling problem, in this theoretical report we propose Mirror Diffusion Models (MDMs). We demonstrate MDMs in the context of simplex diffusion and propose natural extensions to popular domains such as image and text generation.Here's the translation in Simplified Chinese:diffusion模型在各种连续领域中已经成功应用于生成任务。然而，将diffusion应用于分类数据仍然是一个非常困难的任务。此外，在连续领域中的生成通常需要剪辑在实践中，这引发了需要适应受限的领域的理论框架。以mirror langevin算法为 inspirations，在这份理论报告中我们提出了镜像扩散模型（MDM）。我们在简单领域中示例了MDM，并提出了自然的扩展到流行的领域，如图像和文本生成。
</details></li>
</ul>
<hr>
<h2 id="Size-Lowerbounds-for-Deep-Operator-Networks"><a href="#Size-Lowerbounds-for-Deep-Operator-Networks" class="headerlink" title="Size Lowerbounds for Deep Operator Networks"></a>Size Lowerbounds for Deep Operator Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06338">http://arxiv.org/abs/2308.06338</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anirbit Mukherjee, Amartya Roy</li>
<li>for: 本研究目的是为了确定深度运算网络（DeepONet）解决杂项问题所需的数据大小下限。</li>
<li>methods: 本研究使用了数据依赖的下界来证明深度运算网络需要一定的数据大小来实现低训练错误。特别是，我们证明在$n$个数据点上获得低训练错误需要通过增加分支网络和主干网络的公共输出维度的扩展。</li>
<li>results: 我们通过实验示例，表明在固定模型大小下，通过增加公共输出维度，可以实现 monotonic 下降的训练错误。此外，我们还发现，随着数据大小的增加，训练错误会 quadratic 下降。<details>
<summary>Abstract</summary>
Deep Operator Networks are an increasingly popular paradigm for solving regression in infinite dimensions and hence solve families of PDEs in one shot. In this work, we aim to establish a first-of-its-kind data-dependent lowerbound on the size of DeepONets required for them to be able to reduce empirical error on noisy data. In particular, we show that for low training errors to be obtained on $n$ data points it is necessary that the common output dimension of the branch and the trunk net be scaling as $\Omega \left ( {\sqrt{n}} \right )$. This inspires our experiments with DeepONets solving the advection-diffusion-reaction PDE, where we demonstrate the possibility that at a fixed model size, to leverage increase in this common output dimension and get monotonic lowering of training error, the size of the training data might necessarily need to scale quadratically with it.
</details>
<details>
<summary>摘要</summary>
深度运算网络（DeepONet）是一种越来越受欢迎的方法，用于解决无穷维度上的回归问题，并且可以一步解决多个偏微分方程（PDE）。在这项工作中，我们想要建立一个数据依赖的下界，以确定深度运算网络的大小是否足够减少噪声数据上的实际错误。我们显示，为了在 $n$ 个数据点上获得低训练错误， THENET 的通用输出维度和树网络的输出维度必须Scaling as $\Omega \left ( \sqrt{n} \right )$.这种情况 inspirits我们对 DeepONet 解决扩散吸引反应PDE的实验，我们示出，随着模型大小不变，通过增加common output维度，在fixed模型大小下，可以实现 monotonic 下降的训练错误。这意味着，在训练数据规模增加时，可能需要随着common output维度的增加，来降低训练错误。
</details></li>
</ul>
<hr>
<h2 id="Foundation-Model-is-Efficient-Multimodal-Multitask-Model-Selector"><a href="#Foundation-Model-is-Efficient-Multimodal-Multitask-Model-Selector" class="headerlink" title="Foundation Model is Efficient Multimodal Multitask Model Selector"></a>Foundation Model is Efficient Multimodal Multitask Model Selector</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06262">http://arxiv.org/abs/2308.06262</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/opengvlab/multitask-model-selector">https://github.com/opengvlab/multitask-model-selector</a></li>
<li>paper_authors: Fanqing Meng, Wenqi Shao, Zhanglin Peng, Chonghe Jiang, Kaipeng Zhang, Yu Qiao, Ping Luo<br>for:This paper addresses an under-explored problem in the field of multi-modal multi-task learning: predicting the performance of pre-trained neural networks on various tasks without fine-tuning them.methods:The proposed method, called EMMS (Efficient Multi-task Model Selector), employs large-scale foundation models to transform diverse label formats into a unified noisy label embedding, and uses a simple weighted linear regression to estimate a model’s transferability.results:EMMS achieves significant performance gains (9.0%, 26.3%, 20.1%, 54.8%, 12.2%) and speedup (5.13x, 6.29x, 3.59x, 6.19x, 5.66x) compared to the state-of-the-art method LogME, while being fast and effective for assessing the transferability of pre-trained models in a multi-task scenario.<details>
<summary>Abstract</summary>
This paper investigates an under-explored but important problem: given a collection of pre-trained neural networks, predicting their performance on each multi-modal task without fine-tuning them, such as image recognition, referring, captioning, visual question answering, and text question answering. A brute-force approach is to finetune all models on all target datasets, bringing high computational costs. Although recent-advanced approaches employed lightweight metrics to measure models' transferability,they often depend heavily on the prior knowledge of a single task, making them inapplicable in a multi-modal multi-task scenario. To tackle this issue, we propose an efficient multi-task model selector (EMMS), which employs large-scale foundation models to transform diverse label formats such as categories, texts, and bounding boxes of different downstream tasks into a unified noisy label embedding. EMMS can estimate a model's transferability through a simple weighted linear regression, which can be efficiently solved by an alternating minimization algorithm with a convergence guarantee. Extensive experiments on 5 downstream tasks with 24 datasets show that EMMS is fast, effective, and generic enough to assess the transferability of pre-trained models, making it the first model selection method in the multi-task scenario. For instance, compared with the state-of-the-art method LogME enhanced by our label embeddings, EMMS achieves 9.0\%, 26.3\%, 20.1\%, 54.8\%, 12.2\% performance gain on image recognition, referring, captioning, visual question answering, and text question answering, while bringing 5.13x, 6.29x, 3.59x, 6.19x, and 5.66x speedup in wall-clock time, respectively. The code is available at https://github.com/OpenGVLab/Multitask-Model-Selector.
</details>
<details>
<summary>摘要</summary>
To address this issue, we propose an efficient multi-task model selector (EMMS), which uses large-scale foundation models to transform diverse label formats such as categories, texts, and bounding boxes of different downstream tasks into a unified noisy label embedding. EMMS estimates a model's transferability through a simple weighted linear regression, which can be efficiently solved by an alternating minimization algorithm with a convergence guarantee.Extensive experiments on 5 downstream tasks with 24 datasets show that EMMS is fast, effective, and generic enough to assess the transferability of pre-trained models. Compared with the state-of-the-art method LogME enhanced by our label embeddings, EMMS achieves a 9.0%, 26.3%, 20.1%, 54.8%, and 12.2% performance gain on image recognition, referring, captioning, visual question answering, and text question answering, respectively, while bringing a 5.13x, 6.29x, 3.59x, 6.19x, and 5.66x speedup in wall-clock time, respectively. The code is available at https://github.com/OpenGVLab/Multitask-Model-Selector.
</details></li>
</ul>
<hr>
<h2 id="Predicting-Resilience-with-Neural-Networks"><a href="#Predicting-Resilience-with-Neural-Networks" class="headerlink" title="Predicting Resilience with Neural Networks"></a>Predicting Resilience with Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06309">http://arxiv.org/abs/2308.06309</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karen da Mata, Priscila Silva, Lance Fiondella</li>
<li>for: 这个论文探讨了系统能够抵抗破坏性事件的能力，并应用于多个领域。</li>
<li>methods: 该论文提出了三种人工神经网络（ANN）方法，包括 искусственный神经网络（ANN）、循环神经网络（RNN）和长短期记忆神经网络（LSTM），用于模拟和预测系统性能，包括负因素和正因素对系统抵抗力的影响。</li>
<li>results: 研究结果显示，人工神经网络模型在所有评价指标上都超过了传统模型，特别是LSTM模型的可变R平方和预测误差分别下降了34倍和60%以上。这些结果表明，人工神经网络模型可能在许多重要领域中找到实际应用。<details>
<summary>Abstract</summary>
Resilience engineering studies the ability of a system to survive and recover from disruptive events, which finds applications in several domains. Most studies emphasize resilience metrics to quantify system performance, whereas recent studies propose statistical modeling approaches to project system recovery time after degradation. Moreover, past studies are either performed on data after recovering or limited to idealized trends. Therefore, this paper proposes three alternative neural network (NN) approaches including (i) Artificial Neural Networks, (ii) Recurrent Neural Networks, and (iii) Long-Short Term Memory (LSTM) to model and predict system performance, including negative and positive factors driving resilience to quantify the impact of disruptive events and restorative activities. Goodness-of-fit measures are computed to evaluate the models and compared with a classical statistical model, including mean squared error and adjusted R squared. Our results indicate that NN models outperformed the traditional model on all goodness-of-fit measures. More specifically, LSTMs achieved an over 60\% higher adjusted R squared, and decreased predictive error by 34-fold compared to the traditional method. These results suggest that NN models to predict resilience are both feasible and accurate and may find practical use in many important domains.
</details>
<details>
<summary>摘要</summary>
这篇研究探讨了系统的恢复能力，包括系统在干扰事件后的恢复和回复。这些研究通常强调系统的恢复指标数量，而最近的研究则提出了使用统计模型估计系统的复原时间。然而，以往的研究通常是在资料复原后进行，或仅仅是对理想化趋势进行研究。因此，这篇研究提出了三种人工神经网络（ANN）方法，包括人工神经网络（ANN）、回传神经网络（RNN）和长短期内存（LSTM），以模拟和预测系统的性能，包括负面和正面因素的影响，以量化干扰事件的影响和恢复活动。我们使用了一个传统的统计模型，包括平均方差和调整乘根，来评估模型的适合度。我们的结果显示，ANN模型在所有适合度检查中表现较好，特别是LSTM模型在调整乘根上高于60%，且预测误差下降34倍。这些结果表明，ANN模型可以实际地和精确地预测系统的恢复能力，并在许多重要领域中找到实际应用。
</details></li>
</ul>
<hr>
<h2 id="FunnyBirds-A-Synthetic-Vision-Dataset-for-a-Part-Based-Analysis-of-Explainable-AI-Methods"><a href="#FunnyBirds-A-Synthetic-Vision-Dataset-for-a-Part-Based-Analysis-of-Explainable-AI-Methods" class="headerlink" title="FunnyBirds: A Synthetic Vision Dataset for a Part-Based Analysis of Explainable AI Methods"></a>FunnyBirds: A Synthetic Vision Dataset for a Part-Based Analysis of Explainable AI Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06248">http://arxiv.org/abs/2308.06248</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/visinf/funnybirds">https://github.com/visinf/funnybirds</a></li>
<li>paper_authors: Robin Hesse, Simone Schaub-Meyer, Stefan Roth</li>
<li>for: The paper is written for the field of explainable artificial intelligence (XAI), specifically to address the challenge of evaluating the effectiveness of XAI methods in a fully automatic and systematic manner.</li>
<li>methods: The paper proposes a novel synthetic vision dataset called FunnyBirds and accompanying automatic evaluation protocols to evaluate XAI methods. The dataset allows for semantically meaningful image interventions, such as removing individual object parts, which enables analyzing explanations on a part level and estimating ground-truth part importances.</li>
<li>results: The paper reports results for 24 different combinations of neural models and XAI methods, demonstrating the strengths and weaknesses of the assessed methods in a fully automatic and systematic manner. The results show that the proposed evaluation protocols are effective in identifying the strengths and weaknesses of different XAI methods.Here are the three points in Simplified Chinese text:</li>
<li>for: 这篇论文是为了解释人工智能（XAI）领域的一个挑战，具体来说是如何在完全自动和系统atic的方式下评估XAI方法的有效性。</li>
<li>methods: 论文提出了一个新的 sintetic vision dataset名为FunnyBirds，以及一系列的自动评估协议，用于评估XAI方法。该dataset允许进行semantic meaningful的图像干扰，例如 removing各个物体部分，这使得可以分析解释在每个部分上的含义。</li>
<li>results: 论文报告了24种不同的神经网络模型和XAI方法的结果，这些结果表明了评估方法的优劣。<details>
<summary>Abstract</summary>
The field of explainable artificial intelligence (XAI) aims to uncover the inner workings of complex deep neural models. While being crucial for safety-critical domains, XAI inherently lacks ground-truth explanations, making its automatic evaluation an unsolved problem. We address this challenge by proposing a novel synthetic vision dataset, named FunnyBirds, and accompanying automatic evaluation protocols. Our dataset allows performing semantically meaningful image interventions, e.g., removing individual object parts, which has three important implications. First, it enables analyzing explanations on a part level, which is closer to human comprehension than existing methods that evaluate on a pixel level. Second, by comparing the model output for inputs with removed parts, we can estimate ground-truth part importances that should be reflected in the explanations. Third, by mapping individual explanations into a common space of part importances, we can analyze a variety of different explanation types in a single common framework. Using our tools, we report results for 24 different combinations of neural models and XAI methods, demonstrating the strengths and weaknesses of the assessed methods in a fully automatic and systematic manner.
</details>
<details>
<summary>摘要</summary>
领域的解释人工智能（XAI）目标是探索复杂的深度神经网络模型的内部工作原理。虽然在安全关键领域非常重要，但XAI自然lacks ground-truth explanations，making its automatic evaluation an unsolved problem。我们解决这个挑战 by proposing a novel synthetic vision dataset named FunnyBirds， accompanied by automatic evaluation protocols。我们的数据集允许进行semantically meaningful image interventions，例如移除个体物体部分，这有三个重要的后果。首先，它允许分析解释在部件层次上进行分析，这更加接近人类理解的水平than existing methods that evaluate on a pixel level。其次，通过比较模型输出对具有移除部件的输入的比较，我们可以估算ground-truth part importances，这些importances应该被反映在解释中。最后，将各种解释映射到一个共同的部件importances空间，我们可以分析多种不同的解释类型在一个共同框架中。使用我们的工具，我们报告了24种不同的神经网络模型和XAI方法的结果，这些结果 Demonstrate the strengths and weaknesses of the assessed methods in a fully automatic and systematic manner.
</details></li>
</ul>
<hr>
<h2 id="Private-Distribution-Learning-with-Public-Data-The-View-from-Sample-Compression"><a href="#Private-Distribution-Learning-with-Public-Data-The-View-from-Sample-Compression" class="headerlink" title="Private Distribution Learning with Public Data: The View from Sample Compression"></a>Private Distribution Learning with Public Data: The View from Sample Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06239">http://arxiv.org/abs/2308.06239</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shai Ben-David, Alex Bie, Clément L. Canonne, Gautam Kamath, Vikrant Singhal</li>
<li>for: 本文研究了在公共数据上进行隐私学习的问题，即公共私有学习。learner 是 Given public and private samples drawn from an unknown distribution $p$ belonging to a class $\mathcal Q$, with the goal of outputting an estimate of $p$ while adhering to privacy constraints (here, pure differential privacy) only with respect to the private samples.</li>
<li>methods: 本文使用了 sample compression scheme for $\mathcal Q$ 和 list learning 来研究公共私有学习的可行性。</li>
<li>results: 本文的结果包括：(1) 回归 previous results on Gaussians over $\mathbb R^d$ ；(2) 适用于任意 $k$-mixtures of Gaussians over $\mathbb R^d$ 的新结果，包括学习复杂性上下文和分布转移抗性学习者的结果，以及 closure properties for public-private learnability under taking mixtures and products of distributions。此外，通过连接到 list learning，本文还证明了对 Gaussian 在 $\mathbb R^d$ 中，至少需要 $d$ 个公共样本来保证私有学习可行性，这与知道的Upper bound of $d+1$ 公共样本很接近。<details>
<summary>Abstract</summary>
We study the problem of private distribution learning with access to public data. In this setup, which we refer to as public-private learning, the learner is given public and private samples drawn from an unknown distribution $p$ belonging to a class $\mathcal Q$, with the goal of outputting an estimate of $p$ while adhering to privacy constraints (here, pure differential privacy) only with respect to the private samples.   We show that the public-private learnability of a class $\mathcal Q$ is connected to the existence of a sample compression scheme for $\mathcal Q$, as well as to an intermediate notion we refer to as list learning. Leveraging this connection: (1) approximately recovers previous results on Gaussians over $\mathbb R^d$; and (2) leads to new ones, including sample complexity upper bounds for arbitrary $k$-mixtures of Gaussians over $\mathbb R^d$, results for agnostic and distribution-shift resistant learners, as well as closure properties for public-private learnability under taking mixtures and products of distributions. Finally, via the connection to list learning, we show that for Gaussians in $\mathbb R^d$, at least $d$ public samples are necessary for private learnability, which is close to the known upper bound of $d+1$ public samples.
</details>
<details>
<summary>摘要</summary>
我们研究了公共-私人学习问题，在这种设置下，学习者被公共和私人样本所访问，目标是输出一个未知分布$p$的估计，同时遵循隐私限制（这里是纯度ifferential privacy）只与私人样本相关。我们证明了公共-私人学习可能性与样本压缩 schemes for $\mathcal Q$ 以及 list learning 的存在有关，并且利用这种关系：1. 约束 previous results on Gaussians over $\mathbb R^d$ 的 approximately recovery;2. 导致新的结论，包括 $k$-mixtures of Gaussians over $\mathbb R^d$ 的 sample complexity upper bounds, agnostic 和 distribution-shift resistant learners,以及 distribution under taking mixtures and products of distributions. finally, via the connection to list learning, we show that for Gaussians in $\mathbb R^d$, at least $d$ public samples are necessary for private learnability, which is close to the known upper bound of $d+1$ public samples.
</details></li>
</ul>
<hr>
<h2 id="MaxFloodCast-Ensemble-Machine-Learning-Model-for-Predicting-Peak-Inundation-Depth-And-Decoding-Influencing-Features"><a href="#MaxFloodCast-Ensemble-Machine-Learning-Model-for-Predicting-Peak-Inundation-Depth-And-Decoding-Influencing-Features" class="headerlink" title="MaxFloodCast: Ensemble Machine Learning Model for Predicting Peak Inundation Depth And Decoding Influencing Features"></a>MaxFloodCast: Ensemble Machine Learning Model for Predicting Peak Inundation Depth And Decoding Influencing Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06228">http://arxiv.org/abs/2308.06228</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheng-Chun Lee, Lipai Huang, Federico Antolini, Matthew Garcia, Andrew Juanb, Samuel D. Brody, Ali Mostafavi</li>
<li>for: 提供快速、准确和可靠的洪水信息，以支持洪水事件中的决策者、紧急管理人员和基础设施运营人员。</li>
<li>methods: 使用机器学习模型MaxFloodCast，基于物理基础的水动力学模拟，在哈里斯县进行训练，以实现高效和可解释的洪水涵顶深度预测。</li>
<li>results: MaxFloodCast模型在未见数据上达到了0.949的平均R-squared值和0.61 ft的Root Mean Square Error，表明其可靠地预测洪水涵顶深度。验证了飓风哈维和飓风伊梅拉达，MaxFloodCast模型显示出在近实时洪水管理和紧急应急管理中的潜在作用。<details>
<summary>Abstract</summary>
Timely, accurate, and reliable information is essential for decision-makers, emergency managers, and infrastructure operators during flood events. This study demonstrates a proposed machine learning model, MaxFloodCast, trained on physics-based hydrodynamic simulations in Harris County, offers efficient and interpretable flood inundation depth predictions. Achieving an average R-squared of 0.949 and a Root Mean Square Error of 0.61 ft on unseen data, it proves reliable in forecasting peak flood inundation depths. Validated against Hurricane Harvey and Storm Imelda, MaxFloodCast shows the potential in supporting near-time floodplain management and emergency operations. The model's interpretability aids decision-makers in offering critical information to inform flood mitigation strategies, to prioritize areas with critical facilities and to examine how rainfall in other watersheds influences flood exposure in one area. The MaxFloodCast model enables accurate and interpretable inundation depth predictions while significantly reducing computational time, thereby supporting emergency response efforts and flood risk management more effectively.
</details>
<details>
<summary>摘要</summary>
时尚、准确、可靠的信息是决策者、紧急管理者和基础设施操作者在洪水事件中的基本需求。这个研究显示了一个提议的机器学习模型MaxFloodCast，在哈里斯县基于物理学 hydrodynamic 模拟中训练，可以提供高效和可解释的洪水淹没深度预测。在未见数据上，它实现了0.949的平均R-squared和0.61 ft的Root Mean Square Error，证明它在预测洪水淹没深度方面具有可靠性。验证了飓风哈维和飓风Imelda，MaxFloodCast显示了在近实时洪水平原管理和紧急作业中的潜在应用 potential。模型的可解释性帮助决策者对洪水缓和策略提供重要信息，优先级有critical facilities的区域，并考虑在一个区域中的降雨影响洪水暴露。MaxFloodCast模型可以提供高效和可解释的洪水淹没深度预测，同时大幅降低计算时间，以更好地支持紧急 Response efforts和洪水风险管理。
</details></li>
</ul>
<hr>
<h2 id="Automated-Sizing-and-Training-of-Efficient-Deep-Autoencoders-using-Second-Order-Algorithms"><a href="#Automated-Sizing-and-Training-of-Efficient-Deep-Autoencoders-using-Second-Order-Algorithms" class="headerlink" title="Automated Sizing and Training of Efficient Deep Autoencoders using Second Order Algorithms"></a>Automated Sizing and Training of Efficient Deep Autoencoders using Second Order Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06221">http://arxiv.org/abs/2308.06221</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kanishka Tyagi, Chinmay Rane, Michael Manry</li>
<li>for: 该论文旨在提出一种多步训练方法，用于设计通用线性分类器。</li>
<li>methods: 该方法包括初始化多类线性分类器，然后通过权重裁剪来降低验证错误，同时通过类似于霍-卡希普规则来改进期望输出。接着，输出推导器被拟合成为一个通用的多层感知器中的抑制器。</li>
<li>results: 该paper通过对多层感知器的搜索、验证错误的最小化和权重裁剪来提高总性能。此外，paper还提出了一种批处理算法，用于优化隐藏层大小和训练轮数。最终，paper通过对多层感知器进行权重裁剪和增长来提高其性能。<details>
<summary>Abstract</summary>
We propose a multi-step training method for designing generalized linear classifiers. First, an initial multi-class linear classifier is found through regression. Then validation error is minimized by pruning of unnecessary inputs. Simultaneously, desired outputs are improved via a method similar to the Ho-Kashyap rule. Next, the output discriminants are scaled to be net functions of sigmoidal output units in a generalized linear classifier. We then develop a family of batch training algorithm for the multi layer perceptron that optimizes its hidden layer size and number of training epochs. Next, we combine pruning with a growing approach. Later, the input units are scaled to be the net function of the sigmoidal output units that are then feed into as input to the MLP. We then propose resulting improvements in each of the deep learning blocks thereby improving the overall performance of the deep architecture. We discuss the principles and formulation regarding learning algorithms for deep autoencoders. We investigate several problems in deep autoencoders networks including training issues, the theoretical, mathematical and experimental justification that the networks are linear, optimizing the number of hidden units in each layer and determining the depth of the deep learning model. A direct implication of the current work is the ability to construct fast deep learning models using desktop level computational resources. This, in our opinion, promotes our design philosophy of building small but powerful algorithms. Performance gains are demonstrated at each step. Using widely available datasets, the final network's ten fold testing error is shown to be less than that of several other linear, generalized linear classifiers, multi layer perceptron and deep learners reported in the literature.
</details>
<details>
<summary>摘要</summary>
我们提出了一种多步训练方法用于设计通用线性分类器。首先，通过回归获得初始多类线性分类器。然后，通过剔除不必要的输入，降低验证错误。同时，通过类似于霍-卡希普规则进行改进。接着，输出推定器被映射到通用线性分类器中的sigmoid输出单元中的核函数。我们然后开发了一种批处理训练算法，以优化隐藏层大小和训练轮次数。接着，我们结合剔除和增长策略。最后，输入单元被映射到sigmoid输出单元中的核函数，然后被输入到多层感知器中。我们提出了改进每个深度学习块的方法，从而提高整体深度学习模型的性能。我们讨论了深度学习算法的学习原理和形式化表述，并 investigate了深度学习网络中的许多问题，包括训练问题、理论、数学和实验上的正当性。我们的研究表明，通过使用桌面级计算机资源，可以快速构建高性能的深度学习模型。这与我们的设计哲学相吻合，即建立小而强大的算法。我们的实验表明，使用常用的数据集，最终网络的十倍测试错误比其他线性、通用线性分类器、多层感知器和深度学习者报道的较低。
</details></li>
</ul>
<hr>
<h2 id="Change-Point-Detection-With-Conceptors"><a href="#Change-Point-Detection-With-Conceptors" class="headerlink" title="Change Point Detection With Conceptors"></a>Change Point Detection With Conceptors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06213">http://arxiv.org/abs/2308.06213</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/noahgade/changepointdetectionwithconceptors">https://github.com/noahgade/changepointdetectionwithconceptors</a></li>
<li>paper_authors: Noah D. Gade, Jordan Rodu</li>
<li>for:  Identifying changes in the data generating process in time series with increasing dimension and temporal dependence.</li>
<li>methods:  Using a conceptor matrix to learn the characteristic dynamics of a specified training window, and a random recurrent neural network to featurize the data.</li>
<li>results:  A method that provides a consistent estimate of the true change point, and quantile estimates for statistics are produced via a moving block bootstrap of the original data. The method is tested on simulations from several classes of processes and applied to publicly available neural data from rats experiencing bouts of non-REM sleep prior to exploration of a radial maze.<details>
<summary>Abstract</summary>
Offline change point detection seeks to identify points in a time series where the data generating process changes. This problem is well studied for univariate i.i.d. data, but becomes challenging with increasing dimension and temporal dependence. For the at most one change point problem, we propose the use of a conceptor matrix to learn the characteristic dynamics of a specified training window in a time series. The associated random recurrent neural network acts as a featurizer of the data, and change points are identified from a univariate quantification of the distance between the featurization and the space spanned by a representative conceptor matrix. This model agnostic method can suggest potential locations of interest that warrant further study. We prove that, under mild assumptions, the method provides a consistent estimate of the true change point, and quantile estimates for statistics are produced via a moving block bootstrap of the original data. The method is tested on simulations from several classes of processes, and we evaluate performance with clustering metrics, graphical methods, and observed Type 1 error control. We apply our method to publicly available neural data from rats experiencing bouts of non-REM sleep prior to exploration of a radial maze.
</details>
<details>
<summary>摘要</summary>
非线性变换点检测目的是检测时序序列中数据生成过程中的变化点。这个问题在独立Identical Distribution（i.i.d）数据上得到了广泛的研究，但是随着维度和时间相关性的增加，问题变得更加复杂。为了检测最多一个变换点，我们提议使用特征动力矩阵来学习指定的训练窗口中数据的特征动力。相关的随机回归神经网络作为数据的特征化器，变换点通过单variate量化的距离来确定与一个表征动力矩阵所生成的空间之间的距离。这种模型无关的方法可以提供有价值的可能性点，供进一步研究。我们证明，在某些假设下，该方法可以提供一个一致的变换点估计，并通过移动块bootstrap来生成quantile估计。我们在一些类型的过程的 simulations中测试了该方法，并根据集成度、图形方法和观察到的类型一错控制来评估性能。我们将该方法应用于公共可用的非 REM睡眠前的大鼠脑电信号。
</details></li>
</ul>
<hr>
<h2 id="Safety-in-Traffic-Management-Systems-A-Comprehensive-Survey"><a href="#Safety-in-Traffic-Management-Systems-A-Comprehensive-Survey" class="headerlink" title="Safety in Traffic Management Systems: A Comprehensive Survey"></a>Safety in Traffic Management Systems: A Comprehensive Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06204">http://arxiv.org/abs/2308.06204</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenlu Du, Ankan Dash, Jing Li, Hua Wei, Guiling Wang</li>
<li>for: 本研究提供了交通管理系统安全性的全面审查，包括交通管理系统中的安全问题、现有研究的当前状况以及提高交通管理系统安全性的技术和方法。</li>
<li>methods: 本研究审视了交通管理系统中的各种安全问题，包括技术问题、人因问题和管理问题，并评估了现有研究的当前状况和发展趋势。</li>
<li>results: 本研究结果表明，确保交通管理系统安全性是一个复杂的问题，需要考虑技术、人因和管理因素。现有的研究主要集中在技术方面，未来研究应该更加着重于人因和管理方面。<details>
<summary>Abstract</summary>
Traffic management systems play a vital role in ensuring safe and efficient transportation on roads. However, the use of advanced technologies in traffic management systems has introduced new safety challenges. Therefore, it is important to ensure the safety of these systems to prevent accidents and minimize their impact on road users. In this survey, we provide a comprehensive review of the literature on safety in traffic management systems. Specifically, we discuss the different safety issues that arise in traffic management systems, the current state of research on safety in these systems, and the techniques and methods proposed to ensure the safety of these systems. We also identify the limitations of the existing research and suggest future research directions.
</details>
<details>
<summary>摘要</summary>
交通管理系统在公路交通中扮演至关重要的角色，但是使用先进科技在交通管理系统中带来了新的安全挑战。因此，确保交通管理系统的安全性是非常重要的，以预防事故和最小化对交通路用者的影响。在本调查中，我们提供了交通管理系统安全的全面文献评审。具体来说，我们讨论了交通管理系统中不同的安全问题，现有的研究状况，以及确保交通管理系统安全的技术和方法。我们还识别了现有研究的限制，并建议未来研究的方向。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/12/cs.LG_2023_08_12/" data-id="clltau92r005xcr884qc2gabd" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_08_12" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/12/cs.SD_2023_08_12/" class="article-date">
  <time datetime="2023-08-11T16:00:00.000Z" itemprop="datePublished">2023-08-12</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/12/cs.SD_2023_08_12/">cs.SD - 2023-08-12 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Alternative-Pseudo-Labeling-for-Semi-Supervised-Automatic-Speech-Recognition"><a href="#Alternative-Pseudo-Labeling-for-Semi-Supervised-Automatic-Speech-Recognition" class="headerlink" title="Alternative Pseudo-Labeling for Semi-Supervised Automatic Speech Recognition"></a>Alternative Pseudo-Labeling for Semi-Supervised Automatic Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06547">http://arxiv.org/abs/2308.06547</a></li>
<li>repo_url: None</li>
<li>paper_authors: Han Zhu, Dongji Gao, Gaofeng Cheng, Daniel Povey, Pengyuan Zhang, Yonghong Yan</li>
<li>for: 提高自动语音识别器的性能在半监督学习中，当标注数据稀缺时</li>
<li>methods: 提出了一种名为替代pseudo-标签的框架，包括一个通用的CTC损失函数、一种自信度基于的错误检测方法和一种自动调整的阈值调整方法</li>
<li>results: 对比于传统的pseudo-标签筛选和改善pseudo-标签质量的方法，替代pseudo-标签的框架能够更好地适应噪音pseudo-标签的情况，并且不需要手动调整阈值<details>
<summary>Abstract</summary>
When labeled data is insufficient, semi-supervised learning with the pseudo-labeling technique can significantly improve the performance of automatic speech recognition. However, pseudo-labels are often noisy, containing numerous incorrect tokens. Taking noisy labels as ground-truth in the loss function results in suboptimal performance. Previous works attempted to mitigate this issue by either filtering out the nosiest pseudo-labels or improving the overall quality of pseudo-labels. While these methods are effective to some extent, it is unrealistic to entirely eliminate incorrect tokens in pseudo-labels. In this work, we propose a novel framework named alternative pseudo-labeling to tackle the issue of noisy pseudo-labels from the perspective of the training objective. The framework comprises several components. Firstly, a generalized CTC loss function is introduced to handle noisy pseudo-labels by accepting alternative tokens in the positions of incorrect tokens. Applying this loss function in pseudo-labeling requires detecting incorrect tokens in the predicted pseudo-labels. In this work, we adopt a confidence-based error detection method that identifies the incorrect tokens by comparing their confidence scores with a given threshold, thus necessitating the confidence score to be discriminative. Hence, the second proposed technique is the contrastive CTC loss function that widens the confidence gap between the correctly and incorrectly predicted tokens, thereby improving the error detection ability. Additionally, obtaining satisfactory performance with confidence-based error detection typically requires extensive threshold tuning. Instead, we propose an automatic thresholding method that uses labeled data as a proxy for determining the threshold, thus saving the pain of manual tuning.
</details>
<details>
<summary>摘要</summary>
当标注数据不充分时，半超vised学习采用 pseudo-labeling 技术可以显著提高自动语音识别的性能。然而，pseudo-标签通常含有许多错误的符号。将含有错误符号的标签作为真实标签在损失函数中使用会导致优化性不佳。先前的工作已经尝试过过滤 pseudo-标签中最含错误的符号或者提高 pseudo-标签的质量。虽然这些方法有一定的效果，但是完全消除 incorrect 符号是不现实的。在这种情况下，我们提出了一种新的框架，即 alternative pseudo-labeling，以解决 pseudo-标签中含有错误符号的问题。该框架包括以下几个组成部分：1. 一种通用的 CTC 损失函数，可以处理含有错误符号的 pseudo-标签。这种损失函数接受在 incorrect 符号的位置上的多个选项，以便在损失函数中处理多个可能的符号。2. 一种 confidence-based 错误检测方法，可以在预测 pseudo-标签时检测 incorrect 符号。这种方法比较预测 pseudo-标签中的符号 confidence 分数与一定的阈值，并将不符合阈值的符号标记为错误符号。3. 一种自动调整 threshold 的方法，可以使用标注数据作为代理，以便在不需要手动调整的情况下，自动地调整 threshold。通过这些技术，我们可以提高半超vised 学习中 pseudo-标签中含有错误符号的性能。
</details></li>
</ul>
<hr>
<h2 id="BigWavGAN-A-Wave-To-Wave-Generative-Adversarial-Network-for-Music-Super-Resolution"><a href="#BigWavGAN-A-Wave-To-Wave-Generative-Adversarial-Network-for-Music-Super-Resolution" class="headerlink" title="BigWavGAN: A Wave-To-Wave Generative Adversarial Network for Music Super-Resolution"></a>BigWavGAN: A Wave-To-Wave Generative Adversarial Network for Music Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06483">http://arxiv.org/abs/2308.06483</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yenan Zhang, Hiroshi Watanabe</li>
<li>for: 本研究旨在解决深度神经网络（DNNs）在音频超解像（SR）中的高性能问题，即大型DNN模型在SR中无法 producen高质量结果。</li>
<li>methods: 本研究提出了BigWavGAN模型，它通过结合大规模波形模型Demucs，SOTA的检测器和对抗训练策略来解锁大型DNN模型在SR中的潜力。我们的检测器包括多尺度检测器（MSD）和多分辨率检测器（MRD）。</li>
<li>results: 对jective和对jective评价表明，BigWavGAN在音频SR中具有显著高的感知质量，超过了基eline模型。此外，BigWavGAN在 simulated和实际场景中也超过了SOTA音频SR模型。同时，BigWavGAN还能够Address out-of-distribution数据的泛化问题。<details>
<summary>Abstract</summary>
Generally, Deep Neural Networks (DNNs) are expected to have high performance when their model size is large. However, large models failed to produce high-quality results commensurate with their scale in music Super-Resolution (SR). We attribute this to that DNNs cannot learn information commensurate with their size from standard mean square error losses. To unleash the potential of large DNN models in music SR, we propose BigWavGAN, which incorporates Demucs, a large-scale wave-to-wave model, with State-Of-The-Art (SOTA) discriminators and adversarial training strategies. Our discriminator consists of Multi-Scale Discriminator (MSD) and Multi-Resolution Discriminator (MRD). During inference, since only the generator is utilized, there are no additional parameters or computational resources required compared to the baseline model Demucs. Objective evaluation affirms the effectiveness of BigWavGAN in music SR. Subjective evaluations indicate that BigWavGAN can generate music with significantly high perceptual quality over the baseline model. Notably, BigWavGAN surpasses the SOTA music SR model in both simulated and real-world scenarios. Moreover, BigWavGAN represents its superior generalization ability to address out-of-distribution data. The conducted ablation study reveals the importance of our discriminators and training strategies. Samples are available on the demo page.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)通常情况下，深度神经网络（DNNs）预期在大型模型下表现出色。然而，大型模型在音乐超分辨率（SR）领域并未能够生成高质量结果，我们认为这是因为DNNs无法从标准的平方差损失中学习足够的信息。为了解 liberate大型DNN模型在音乐SR中的潜力，我们提出了BigWavGAN，它包括了大规模的波形模型Demucs，以及当前最佳的检测器和对抗训练策略。我们的检测器包括多尺度检测器（MSD）和多分辨率检测器（MRD）。在推理过程中，只有生成器被使用，因此无需额外的参数或计算资源，与Demucs相比。对象评估表明BigWavGAN在音乐SR中的效果极佳。主观评估表明BigWavGAN可以生成高度感知质量的音乐，超过基准模型。此外，BigWavGAN表现出了更好的总体化能力，可以 Addressing out-of-distribution data。我们进行了一项ablation Study，表明我们的检测器和训练策略的重要性。样本可以在 demo 页面上找到。
</details></li>
</ul>
<hr>
<h2 id="Bilingual-Streaming-ASR-with-Grapheme-units-and-Auxiliary-Monolingual-Loss"><a href="#Bilingual-Streaming-ASR-with-Grapheme-units-and-Auxiliary-Monolingual-Loss" class="headerlink" title="Bilingual Streaming ASR with Grapheme units and Auxiliary Monolingual Loss"></a>Bilingual Streaming ASR with Grapheme units and Auxiliary Monolingual Loss</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06327">http://arxiv.org/abs/2308.06327</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Soleymanpour, Mahmoud Al Ismail, Fahimeh Bahmaninezhad, Kshitiz Kumar, Jian Wu</li>
<li>for: 这个论文是为了提供一种混合自动声音识别（ASR）设置下的英语作为次要地区的双语解决方案。</li>
<li>methods: 作者们提出了以下四个关键发展：（a）用图形单元代替语音单元的发音词典，（b）完全双语对应模型和随后的双语流Transformer模型，（c）并行编码结构和语言标识（LID）损失，（d）并行编码器的辅助损失 для单语jective。</li>
<li>results: 作者们通过大规模训练和测试任务来评估他们的工作，并发现他们的提出的辅助损失可以更好地特化并行编码器到各自的单语本地，从而使得双语学习更加强。特别是，双语IT模型在一个code-mix IT任务中提高了单词错误率（WER）从46.5%降低到13.8%，同时也与单语IT模型（9.5%）在IT测试任务上几乎占据了同等水平（9.6%）。<details>
<summary>Abstract</summary>
We introduce a bilingual solution to support English as secondary locale for most primary locales in hybrid automatic speech recognition (ASR) settings. Our key developments constitute: (a) pronunciation lexicon with grapheme units instead of phone units, (b) a fully bilingual alignment model and subsequently bilingual streaming transformer model, (c) a parallel encoder structure with language identification (LID) loss, (d) parallel encoder with an auxiliary loss for monolingual projections. We conclude that in comparison to LID loss, our proposed auxiliary loss is superior in specializing the parallel encoders to respective monolingual locales, and that contributes to stronger bilingual learning. We evaluate our work on large-scale training and test tasks for bilingual Spanish (ES) and bilingual Italian (IT) applications. Our bilingual models demonstrate strong English code-mixing capability. In particular, the bilingual IT model improves the word error rate (WER) for a code-mix IT task from 46.5% to 13.8%, while also achieving a close parity (9.6%) with the monolingual IT model (9.5%) over IT tests.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种双语解决方案，以支持英语为次要本地语言在混合自动语音识别（ASR）设置中。我们的关键发展包括：（a）使用字符单位 instead of 语音单位的发音词典，（b）完全双语对应模型和随后的双语流Transformer模型，（c）并行编码结构与语言标识（LID）损失，（d）并行编码器和辅助损失 для单语Project。我们 conclude that在比较LID损失的情况下，我们提posed的辅助损失能够特化并行编码器到各自的单语本地语言，从而为双语学习提供更强的支持。我们在大规模训练和测试任务上评估了我们的工作，并在双语西班牙（ES）和双语意大利（IT）应用中显示出了英语代码混合能力。特别是，双语IT模型在一个代码混合IT任务上从46.5%降至13.8%，并同时与单语IT模型（9.5%）在IT测试上达到了9.6%的相似性。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/12/cs.SD_2023_08_12/" data-id="clltau93t008ycr88ge4b34ud" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_08_12" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/12/eess.IV_2023_08_12/" class="article-date">
  <time datetime="2023-08-11T16:00:00.000Z" itemprop="datePublished">2023-08-12</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/12/eess.IV_2023_08_12/">eess.IV - 2023-08-12 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Semantic-Communications-with-Explicit-Semantic-Base-for-Image-Transmission"><a href="#Semantic-Communications-with-Explicit-Semantic-Base-for-Image-Transmission" class="headerlink" title="Semantic Communications with Explicit Semantic Base for Image Transmission"></a>Semantic Communications with Explicit Semantic Base for Image Transmission</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06599">http://arxiv.org/abs/2308.06599</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuan Zheng, Fengyu Wang, Wenjun Xu, Miao Pan, Ping Zhang</li>
<li>for: 该 paper 的目的是提出一种基于协同知识 (Seb) 的 semantic image transmission 框架，以确保信息的含义在传输过程中得到正确的理解和传输。</li>
<li>methods: 该 paper 使用了 Seb 生成器和 Seb 基于图像编码&#x2F;解码器来表示图像，并使用 E2E 训练来优化核心组件。</li>
<li>results: 对比 state-of-art 方法，该 paper 在不同 SNR 下达到了 0.5-1.5 dB 的 PSNR 提升。<details>
<summary>Abstract</summary>
Semantic communications, aiming at ensuring the successful delivery of the meaning of information, are expected to be one of the potential techniques for the next generation communications. However, the knowledge forming and synchronizing mechanism that enables semantic communication systems to extract and interpret the semantics of information according to the communication intents is still immature. In this paper, we propose a semantic image transmission framework with explicit semantic base (Seb), where Sebs are generated and employed as the knowledge shared between the transmitter and the receiver with flexible granularity. To represent images with Sebs, a novel Seb-based reference image generator is proposed to generate Sebs and then decompose the transmitted images. To further encode/decode the residual information for precise image reconstruction, a Seb-based image encoder/decoder is proposed. The key components of the proposed framework are optimized jointly by end-to-end (E2E) training, where the loss function is dedicated designed to tackle the problem of nondifferentiable operation in Seb-based reference image generator by introducing a gradient approximation mechanism. Extensive experiments show that the proposed framework outperforms state-of-art works by 0.5 - 1.5 dB in peak signal-to-noise ratio (PSNR) w.r.t. different signal-to-noise ratio (SNR).
</details>
<details>
<summary>摘要</summary>
semantic communication, aiming at ensuring the successful delivery of information meaning, is expected to be one of the potential techniques for next-generation communications. however, the knowledge forming and synchronizing mechanism that enables semantic communication systems to extract and interpret the semantics of information according to communication intents is still immature. in this paper, we propose a semantic image transmission framework with explicit semantic base (Seb), where Sebs are generated and employed as the knowledge shared between the transmitter and the receiver with flexible granularity. to represent images with Sebs, a novel Seb-based reference image generator is proposed to generate Sebs and then decompose the transmitted images. to further encode/decode the residual information for precise image reconstruction, a Seb-based image encoder/decoder is proposed. the key components of the proposed framework are optimized jointly by end-to-end (E2E) training, where the loss function is dedicated designed to tackle the problem of nondifferentiable operation in Seb-based reference image generator by introducing a gradient approximation mechanism. extensive experiments show that the proposed framework outperforms state-of-art works by 0.5 - 1.5 dB in peak signal-to-noise ratio (PSNR) w.r.t. different signal-to-noise ratio (SNR).
</details></li>
</ul>
<hr>
<h2 id="On-Versatile-Video-Coding-at-UHD-with-Machine-Learning-Based-Super-Resolution"><a href="#On-Versatile-Video-Coding-at-UHD-with-Machine-Learning-Based-Super-Resolution" class="headerlink" title="On Versatile Video Coding at UHD with Machine-Learning-Based Super-Resolution"></a>On Versatile Video Coding at UHD with Machine-Learning-Based Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06570">http://arxiv.org/abs/2308.06570</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kristian Fischer, Christian Herglotz, André Kaup</li>
<li>for: 提高4K数据的编码质量</li>
<li>methods: 使用Machine Learning基于单张超解析算法和下一代VVC编码器</li>
<li>results: 可以在低比特率场景下获得12%-18%的Bjontegaard delta rate提升，并且减少了压缩残留和扩散 artifacts。<details>
<summary>Abstract</summary>
Coding 4K data has become of vital interest in recent years, since the amount of 4K data is significantly increasing. We propose a coding chain with spatial down- and upscaling that combines the next-generation VVC codec with machine learning based single image super-resolution algorithms for 4K. The investigated coding chain, which spatially downscales the 4K data before coding, shows superior quality than the conventional VVC reference software for low bitrate scenarios. Throughout several tests, we find that up to 12 % and 18 % Bjontegaard delta rate gains can be achieved on average when coding 4K sequences with VVC and QP values above 34 and 42, respectively. Additionally, the investigated scenario with up- and downscaling helps to reduce the loss of details and compression artifacts, as it is shown in a visual example.
</details>
<details>
<summary>摘要</summary>
“ coding 4K 数据在近年变得非常重要，因为4K 数据量在增长。我们提出了一个 coding chain，其 combining 下一代 VVC 编码器和基于机器学习的单张图像超分辨算法，用于4K。我们调查的 coding chain，先将4K 数据进行空间下降scaling，然后编码，在低比特率场景下显示出超越传统 VVC 参考软件的质量。在多个测试中，我们发现，在 VVC 和 QP 值高于 34 和 42 时，可以获得12% 到 18% Bjontegaard delta Rate 增强。此外，我们发现，这种升降scaling 场景可以避免失 Details 和压缩残差的损失，如图例所示。”Note that Simplified Chinese is used in mainland China, while Traditional Chinese is used in Taiwan and Hong Kong.
</details></li>
</ul>
<hr>
<h2 id="Three-dimensional-echo-shifted-EPI-with-simultaneous-blip-up-and-blip-down-acquisitions-for-correcting-geometric-distortion"><a href="#Three-dimensional-echo-shifted-EPI-with-simultaneous-blip-up-and-blip-down-acquisitions-for-correcting-geometric-distortion" class="headerlink" title="Three-dimensional echo-shifted EPI with simultaneous blip-up and blip-down acquisitions for correcting geometric distortion"></a>Three-dimensional echo-shifted EPI with simultaneous blip-up and blip-down acquisitions for correcting geometric distortion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06557">http://arxiv.org/abs/2308.06557</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaibao Sun, Zhifeng Chen, Guangyu Dan, Qingfei Luo, Lirong Yan, Feng Liu, Xiaohong Joe Zhou</li>
<li>for: 提高echo-planar imaging（EPI）的图像质量和动态误差 corrections，使其适用于更多应用。</li>
<li>methods: 使用三维（3D）echo-shifted EPI BUDA（esEPI-BUDA）技术，通过生成两个EPI读取轨迹，以实现单shot acquiring两个blip-up和blip-down数据集。</li>
<li>results: 在phantom和人类大脑图像中， geometric distortions 得到了有效地 correction，并在人类大脑中测试了视觉活化Volume和其BOLD响应，与普通3D echo-planar图像相当。<details>
<summary>Abstract</summary>
Purpose: Echo-planar imaging (EPI) with blip-up/down acquisition (BUDA) can provide high-quality images with minimal distortions by using two readout trains with opposing phase-encoding gradients. Because of the need for two separate acquisitions, BUDA doubles the scan time and degrades the temporal resolution when compared to single-shot EPI, presenting a major challenge for many applications, particularly functional MRI (fMRI). This study aims at overcoming this challenge by developing an echo-shifted EPI BUDA (esEPI-BUDA) technique to acquire both blip-up and blip-down datasets in a single shot. Methods: A three-dimensional (3D) esEPI-BUDA pulse sequence was designed by using an echo-shifting strategy to produce two EPI readout trains. These readout trains produced a pair of k-space datasets whose k-space trajectories were interleaved with opposite phase-encoding gradient directions. The two k-space datasets were separately reconstructed using a 3D SENSE algorithm, from which time-resolved B0-field maps were derived using TOPUP in FSL and then input into a forward model of joint parallel imaging reconstruction to correct for geometric distortion. In addition, Hankel structured low-rank constraint was incorporated into the reconstruction framework to improve image quality by mitigating the phase errors between the two interleaved k-space datasets. Results: The 3D esEPI-BUDA technique was demonstrated in a phantom and an fMRI study on healthy human subjects. Geometric distortions were effectively corrected in both phantom and human brain images. In the fMRI study, the visual activation volumes and their BOLD responses were comparable to those from conventional 3D echo-planar images. Conclusion: The improved imaging efficiency and dynamic distortion correction capability afforded by 3D esEPI-BUDA are expected to benefit many EPI applications.
</details>
<details>
<summary>摘要</summary>
目的：使用双向磁场增强/减强获取（BUDA）技术可以提供高质量图像，但因为需要两次获取，BUDA将扫描时间双倍，降低时间分辨率，对许多应用程序（特别是功能磁共振成像（fMRI））提出了主要挑战。这项研究的目的是解决这一挑战，通过开发一种三维echo-shifted EPI BUDA（esEPI-BUDA）技术，在单击中获取两个磁场增强/减强数据集。方法：设计了一种三维 esEPI-BUDA脉冲序列，使用抽象阶段生成两个 EPI 读取轨迹。这两个读取轨迹生成了具有相反方向磁场增强/减强方向的两个 k-空间数据集。这两个 k-空间数据集分别使用三维 SENSE 算法重构，并从而生成了时间解析B0场图像。在FSL中使用 TOPUP 算法，将这些时间解析B0场图像输入到了一种 JOINT 平行成像重建模型中，以 corrected  geometric distortion。此外，还在重建框架中添加了具有束缚低维度的 Hankel 结构低级数据约束，以提高图像质量，减少了两个交错 k-空间数据集之间的频率错误。结果：在一个模拟器和一个人类大脑功能磁共振成像研究中，三维 esEPI-BUDA 技术得到了证明。在这些研究中，人类大脑功能磁共振图像中的形态扭曲都得到了有效地 corrections。在人类大脑功能磁共振图像中，可见功能区域的激发量和其 BOLD 响应与普通三维 EPI 图像具有相同的水平。结论：三维 esEPI-BUDA 技术的改进的扫描效率和动态扭曲纠正能力，预期将对许多 EPI 应用程序产生积极的影响。
</details></li>
</ul>
<hr>
<h2 id="The-Color-Clifford-Hardy-Signal-Application-to-Color-Edge-Detection-and-Optical-Flow"><a href="#The-Color-Clifford-Hardy-Signal-Application-to-Color-Edge-Detection-and-Optical-Flow" class="headerlink" title="The Color Clifford Hardy Signal: Application to Color Edge Detection and Optical Flow"></a>The Color Clifford Hardy Signal: Application to Color Edge Detection and Optical Flow</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06485">http://arxiv.org/abs/2308.06485</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoxiao Hu, Kit Ian Kou, Cuiming Zou, Dong Cheng</li>
<li>for: 该论文提出了色彩Clifford Hardy信号的想法，用于处理色彩图像。</li>
<li>methods: 该论文提出了五种方法来识别色彩图像的边缘，基于色彩Clifford Hardy信号的本地特征表示。</li>
<li>results: 该论文通过多种比较研究表明，提出的方法具有高效性和抗噪能力。例如，通过多尺度结构的色彩Clifford Hardy信号，方法可以抗衰减噪。此外，论文还提供了一种颜色动向检测方法，用于应用示例。<details>
<summary>Abstract</summary>
This paper introduces the idea of the color Clifford Hardy signal, which can be used to process color images. As a complex analytic function's high-dimensional analogue, the color Clifford Hardy signal inherits many desirable qualities of analyticity. A crucial tool for getting the color and structural data is the local feature representation of a color image in the color Clifford Hardy signal. By looking at the extended Cauchy-Riemann equations in the high-dimensional space, it is possible to see the connection between the different parts of the color Clifford Hardy signal. Based on the distinctive and important local amplitude and local phase generated by the color Clifford Hardy signal, we propose five methods to identify the edges of color images with relation to a certain color. To prove the superiority of the offered methodologies, numerous comparative studies employing image quality assessment criteria are used. Specifically by using the multi-scale structure of the color Clifford Hardy signal, the proposed approaches are resistant to a variety of noises. In addition, a color optical flow detection method with anti-noise ability is provided as an example of application.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Out-of-distribution-multi-view-auto-encoders-for-prostate-cancer-lesion-detection"><a href="#Out-of-distribution-multi-view-auto-encoders-for-prostate-cancer-lesion-detection" class="headerlink" title="Out-of-distribution multi-view auto-encoders for prostate cancer lesion detection"></a>Out-of-distribution multi-view auto-encoders for prostate cancer lesion detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06481">http://arxiv.org/abs/2308.06481</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alvaro Fernandez-Quilez, Linas Vidziunas, Ørjan Kløvfjell Thoresen, Ketil Oppedal, Svein Reidar Kjosavik, Trygve Eftestøl</li>
<li>for: 这篇论文是为了检测抑制癌病变的方法。</li>
<li>methods: 这篇论文使用了多流程方法，以便利用不同的T2w方向来提高肿瘤检测的性能。</li>
<li>results: 这篇论文的结果显示，使用多流程方法可以提高肿瘤检测的精度，比单向方法更高（AUC&#x3D;73.1 vs 82.3）。<details>
<summary>Abstract</summary>
Traditional deep learning (DL) approaches based on supervised learning paradigms require large amounts of annotated data that are rarely available in the medical domain. Unsupervised Out-of-distribution (OOD) detection is an alternative that requires less annotated data. Further, OOD applications exploit the class skewness commonly present in medical data. Magnetic resonance imaging (MRI) has proven to be useful for prostate cancer (PCa) diagnosis and management, but current DL approaches rely on T2w axial MRI, which suffers from low out-of-plane resolution. We propose a multi-stream approach to accommodate different T2w directions to improve the performance of PCa lesion detection in an OOD approach. We evaluate our approach on a publicly available data-set, obtaining better detection results in terms of AUC when compared to a single direction approach (73.1 vs 82.3). Our results show the potential of OOD approaches for PCa lesion detection based on MRI.
</details>
<details>
<summary>摘要</summary>
传统的深度学习（DL）方法基于supervised learning paradigms需要大量的标注数据，而医疗领域中这些数据很少。不需要标注数据的Out-of-distribution（OOD）检测是一个 alternativa，并且可以利用医疗数据的类偏好。核磁共振成像（MRI）已经被证明是肠癌（PCa）诊断和管理的有用工具，但现有的DL方法仅仅使用T2w极向MRI，这种MRI受到外向分辨率的限制。我们提议一种多流处理方法，以便处理不同的T2w方向，以提高PCa患部检测的性能。我们对公共可用数据集进行了评估，并 obtient了与单向方法相比的更好的检测结果（AUC=73.1 vs AUC=82.3）。我们的结果表明，基于MRI的PCa患部检测可以通过OOD方法实现更高的检测精度。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-multi-view-data-without-annotations-for-prostate-MRI-segmentation-A-contrastive-approach"><a href="#Leveraging-multi-view-data-without-annotations-for-prostate-MRI-segmentation-A-contrastive-approach" class="headerlink" title="Leveraging multi-view data without annotations for prostate MRI segmentation: A contrastive approach"></a>Leveraging multi-view data without annotations for prostate MRI segmentation: A contrastive approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06477">http://arxiv.org/abs/2308.06477</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tim Nikolass Lindeijer, Tord Martin Ytredal, Trygve Eftestøl, Tobias Nordström, Fredrik Jäderling, Martin Eklund, Alvaro Fernandez-Quilez</li>
<li>for: 提高肾脏癌诊断的支持</li>
<li>methods: 使用对比学习方法，不需要manual注释，可以在部署时 missing views</li>
<li>results: 提高了自适应volumetric分割的精度，并且在多视图数据上实现了good external volumetric generalization<details>
<summary>Abstract</summary>
An accurate prostate delineation and volume characterization can support the clinical assessment of prostate cancer. A large amount of automatic prostate segmentation tools consider exclusively the axial MRI direction in spite of the availability as per acquisition protocols of multi-view data. Further, when multi-view data is exploited, manual annotations and availability at test time for all the views is commonly assumed. In this work, we explore a contrastive approach at training time to leverage multi-view data without annotations and provide flexibility at deployment time in the event of missing views. We propose a triplet encoder and single decoder network based on U-Net, tU-Net (triplet U-Net). Our proposed architecture is able to exploit non-annotated sagittal and coronal views via contrastive learning to improve the segmentation from a volumetric perspective. For that purpose, we introduce the concept of inter-view similarity in the latent space. To guide the training, we combine a dice score loss calculated with respect to the axial view and its manual annotations together with a multi-view contrastive loss. tU-Net shows statistical improvement in dice score coefficient (DSC) with respect to only axial view (91.25+-0.52% compared to 86.40+-1.50%,P<.001). Sensitivity analysis reveals the volumetric positive impact of the contrastive loss when paired with tU-Net (2.85+-1.34% compared to 3.81+-1.88%,P<.001). Further, our approach shows good external volumetric generalization in an in-house dataset when tested with multi-view data (2.76+-1.89% compared to 3.92+-3.31%,P=.002), showing the feasibility of exploiting non-annotated multi-view data through contrastive learning whilst providing flexibility at deployment in the event of missing views.
</details>
<details>
<summary>摘要</summary>
可以准确地定义和量化 prostata 的部分，可以支持肾癌的临床评估。许多自动 prostate 分割工具都忽略了多视图数据的可用性，即使据获取协议中有多视图数据可用。此外，当使用多视图数据时，通常需要手动标注和测试时 disponibility 的所有视图。在这种情况下，我们提出了一种对比方法，可以在训练时使用多视图数据而不需要手动标注，并在部署时提供可选的视图。我们提出了一种基于 U-Net 的 triplet 编码器和单个解码器网络，可以通过对比学习利用非标注的 sagittal 和极轴视图来提高分割。为此，我们引入了视图间的相似性在幂空间的概念。为了导航训练，我们将 dice 分数损失与 respect 到 axial 视图和其手动标注相加，并与多视图对比损失相结合。tU-Net 表示与只有 axial 视图相比（91.25+-0.52% 与 86.40+-1.50%，P<.001）显示了统计学上的改进。敏感分析表明，对于 tU-Net 来说，对比损失的volumetric 正面影响（2.85+-1.34% 与 3.81+-1.88%,P<.001）。此外，我们的方法在我们的内部数据集中进行了多视图数据的外部准确性测试（2.76+-1.89% 与 3.92+-3.31%,P=.002），表明可以通过对比学习在不具有标注的多视图数据上Exploiting 而提供可选的视图。
</details></li>
</ul>
<hr>
<h2 id="CATS-v2-Hybrid-encoders-for-robust-medical-segmentation"><a href="#CATS-v2-Hybrid-encoders-for-robust-medical-segmentation" class="headerlink" title="CATS v2: Hybrid encoders for robust medical segmentation"></a>CATS v2: Hybrid encoders for robust medical segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06377">http://arxiv.org/abs/2308.06377</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/haoli12345/cats">https://github.com/haoli12345/cats</a></li>
<li>paper_authors: Hao Li, Han Liu, Dewei Hu, Xing Yao, Jiacheng Wang, Ipek Oguz<br>for:* 这个研究是为了提高医疗影像分类 задачі的性能，特别是在 capture 高级（本地）信息和全球信息之间的平衡。methods:* 使用 hybrid encoders，包括一个 CNN-based Encoder 路径和一个 transformer 路径，以更好地利用本地和全球信息。* 使用 skip connections 将 convolutional encoder 和 transformer 融合为最终的分类结果。results:* 在 Cross-Modality Domain Adaptation (CrossMoDA) 和 Medical Segmentation Decathlon (MSD-5) 项目中，该方法与州际先进方法相比， exhibit 高的 Dice 分数。<details>
<summary>Abstract</summary>
Convolutional Neural Networks (CNNs) have exhibited strong performance in medical image segmentation tasks by capturing high-level (local) information, such as edges and textures. However, due to the limited field of view of convolution kernel, it is hard for CNNs to fully represent global information. Recently, transformers have shown good performance for medical image segmentation due to their ability to better model long-range dependencies. Nevertheless, transformers struggle to capture high-level spatial features as effectively as CNNs. A good segmentation model should learn a better representation from local and global features to be both precise and semantically accurate. In our previous work, we proposed CATS, which is a U-shaped segmentation network augmented with transformer encoder. In this work, we further extend this model and propose CATS v2 with hybrid encoders. Specifically, hybrid encoders consist of a CNN-based encoder path paralleled to a transformer path with a shifted window, which better leverage both local and global information to produce robust 3D medical image segmentation. We fuse the information from the convolutional encoder and the transformer at the skip connections of different resolutions to form the final segmentation. The proposed method is evaluated on two public challenge datasets: Cross-Modality Domain Adaptation (CrossMoDA) and task 5 of Medical Segmentation Decathlon (MSD-5), to segment vestibular schwannoma (VS) and prostate, respectively. Compared with the state-of-the-art methods, our approach demonstrates superior performance in terms of higher Dice scores.
</details>
<details>
<summary>摘要</summary>
卷积神经网络（CNN）在医疗图像分割任务中表现出色，通过捕捉高级（本地）信息，如边缘和xture，来捕捉高级信息。然而，由于卷积核心的局部视场限制，使得CNN很难完全表示全局信息。最近，transformer在医疗图像分割中表现良好，这是因为它们可以更好地模型距离的长距离依赖关系。然而，transformer尚未能如同CNN那样有效地捕捉高级空间特征。为了实现更好的分割模型，我们需要学习更好地捕捉本地和全局特征，以确保准确和semantic地正确。在我们之前的工作中，我们提出了CATS模型，这是一种U型卷积分割网络，其中包括transformer编码器。在这个工作中，我们进一步扩展了这个模型，并提出了CATS v2模型，它包括hybrid编码器。specifically，hybrid编码器包括一个CNN基于编码器路径，并与一个偏移窗口的transformer路径并行，这样更好地利用本地和全局信息来生成robust的3D医疗图像分割。我们在不同分辨率的 skip connections中 fusions the information from the convolutional encoder and the transformer，以生成最终的分割结果。我们的方法在两个公共挑战数据集上进行评估： Cross-Modality Domain Adaptation（CrossMoDA）和Medical Segmentation Decathlon（MSD-5），用于分割vestibular schwannoma（VS）和prostate，分别。与当前状态的方法相比，我们的方法在 terms of higher Dice scores表现出色。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Based-Open-Source-Toolkit-for-Eosinophil-Detection-in-Pediatric-Eosinophilic-Esophagitis"><a href="#Deep-Learning-Based-Open-Source-Toolkit-for-Eosinophil-Detection-in-Pediatric-Eosinophilic-Esophagitis" class="headerlink" title="Deep Learning-Based Open Source Toolkit for Eosinophil Detection in Pediatric Eosinophilic Esophagitis"></a>Deep Learning-Based Open Source Toolkit for Eosinophil Detection in Pediatric Eosinophilic Esophagitis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06333">http://arxiv.org/abs/2308.06333</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hrlblab/open-eoe">https://github.com/hrlblab/open-eoe</a></li>
<li>paper_authors: Juming Xiong, Yilin Liu, Ruining Deng, Regina N Tyree, Hernan Correa, Girish Hiremath, Yaohong Wang, Yuankai Huo<br>for: 这个研究旨在开发一个开源的工具集，用于检测食道病变中的嗜中性粒细胞（Eos）。methods: 该工具集使用三种现状最佳的深度学习基于对象检测模型，并实现了一种ensemble学习策略以提高结果的精度和可靠性。results: 实验结果表明，Open-EoE工具集可以效果地检测食道病变中的Eos，并达到了91%的准确率，与病理学家评估相符。<details>
<summary>Abstract</summary>
Eosinophilic Esophagitis (EoE) is a chronic, immune/antigen-mediated esophageal disease, characterized by symptoms related to esophageal dysfunction and histological evidence of eosinophil-dominant inflammation. Owing to the intricate microscopic representation of EoE in imaging, current methodologies which depend on manual identification are not only labor-intensive but also prone to inaccuracies. In this study, we develop an open-source toolkit, named Open-EoE, to perform end-to-end whole slide image (WSI) level eosinophil (Eos) detection using one line of command via Docker. Specifically, the toolkit supports three state-of-the-art deep learning-based object detection models. Furthermore, Open-EoE further optimizes the performance by implementing an ensemble learning strategy, and enhancing the precision and reliability of our results. The experimental results demonstrated that the Open-EoE toolkit can efficiently detect Eos on a testing set with 289 WSIs. At the widely accepted threshold of >= 15 Eos per high power field (HPF) for diagnosing EoE, the Open-EoE achieved an accuracy of 91%, showing decent consistency with pathologist evaluations. This suggests a promising avenue for integrating machine learning methodologies into the diagnostic process for EoE. The docker and source code has been made publicly available at https://github.com/hrlblab/Open-EoE.
</details>
<details>
<summary>摘要</summary>
《细胞滤镜检测诊断工具（Open-EoE）》是一个开源的检测工具，用于检测食管细胞滤镜病（EoE）的患者。该工具使用深度学习技术，可以通过一条命令在Docker环境中完成整个检测过程。工具支持三种当前顶峰的深度学习模型，并且通过 ensemble learning 策略来提高检测精度和可靠性。实验结果表明，Open-EoE 工具可以高效地检测食管细胞滤镜，在测试集上达到了91%的准确率。这表明，机器学习技术可以成功地应用于EoE 诊断过程中。工具和源代码已经在 GitHub 上公开，可以免费下载和使用。
</details></li>
</ul>
<hr>
<h2 id="Revolutionizing-Space-Health-Swin-FSR-Advancing-Super-Resolution-of-Fundus-Images-for-SANS-Visual-Assessment-Technology"><a href="#Revolutionizing-Space-Health-Swin-FSR-Advancing-Super-Resolution-of-Fundus-Images-for-SANS-Visual-Assessment-Technology" class="headerlink" title="Revolutionizing Space Health (Swin-FSR): Advancing Super-Resolution of Fundus Images for SANS Visual Assessment Technology"></a>Revolutionizing Space Health (Swin-FSR): Advancing Super-Resolution of Fundus Images for SANS Visual Assessment Technology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06332">http://arxiv.org/abs/2308.06332</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/FarihaHossain/SwinFSR">https://github.com/FarihaHossain/SwinFSR</a></li>
<li>paper_authors: Khondker Fariha Hossain, Sharif Amit Kamran, Joshua Ong, Andrew G. Lee, Alireza Tavakkoli</li>
<li>for: 这 paper 是为了提高肉眼图像的分辨率，以便在不同的地方进行早期差分诊断。</li>
<li>methods: 这 paper 使用了 Swin Transformer 与空间和深度精度注意力，实现了基于肉眼图像的超分辨率。</li>
<li>results: 这 paper 在三个公共数据集上 achieved PSNR 值为 47.89、49.00 和 45.32，并在 NASA 提供的私人数据集上达到了相当的结果。<details>
<summary>Abstract</summary>
The rapid accessibility of portable and affordable retinal imaging devices has made early differential diagnosis easier. For example, color funduscopy imaging is readily available in remote villages, which can help to identify diseases like age-related macular degeneration (AMD), glaucoma, or pathological myopia (PM). On the other hand, astronauts at the International Space Station utilize this camera for identifying spaceflight-associated neuro-ocular syndrome (SANS). However, due to the unavailability of experts in these locations, the data has to be transferred to an urban healthcare facility (AMD and glaucoma) or a terrestrial station (e.g, SANS) for more precise disease identification. Moreover, due to low bandwidth limits, the imaging data has to be compressed for transfer between these two places. Different super-resolution algorithms have been proposed throughout the years to address this. Furthermore, with the advent of deep learning, the field has advanced so much that x2 and x4 compressed images can be decompressed to their original form without losing spatial information. In this paper, we introduce a novel model called Swin-FSR that utilizes Swin Transformer with spatial and depth-wise attention for fundus image super-resolution. Our architecture achieves Peak signal-to-noise-ratio (PSNR) of 47.89, 49.00 and 45.32 on three public datasets, namely iChallenge-AMD, iChallenge-PM, and G1020. Additionally, we tested the model's effectiveness on a privately held dataset for SANS provided by NASA and achieved comparable results against previous architectures.
</details>
<details>
<summary>摘要</summary>
“现代化的眼科医学技术已经使得早期的医学诊断变得更加容易。例如，彩色基准摄影是在偏远村庄中可以提供的，可以帮助诊断年龄相关 macular degeneration（AMD）、 glaucoma 或 PATHOLOGICAL MYOPIA（PM）等疾病。然而，由于这些地点缺乏专家，因此需要将数据传输到城市医疗机构（AMD和 glaucoma）或地面站（例如，SANS）进行更加准确的疾病诊断。此外，由于带宽限制，摄影数据需要压缩传输。过去数年，有许多超解像算法的提案，以解决这个问题。另外，随着深度学习的发展，场景有了很大的进步，可以使用 x2 和 x4 压缩图像重新恢复到原始形态，无需失去空间信息。在本文中，我们介绍了一种新的模型called Swin-FSR，该模型利用SwinTransformer的空间和深度宽分注意力来进行基准图像超解像。我们的架构实现了 PSNR 的值为 47.89、49.00 和 45.32 在三个公共数据集上，namely iChallenge-AMD、iChallenge-PM 和 G1020。此外，我们对 NASA 提供的一个私有数据集进行了测试，并与之前的建筑物实现了相似的结果。”
</details></li>
</ul>
<hr>
<h2 id="A-Hierarchical-Descriptor-Framework-for-On-the-Fly-Anatomical-Location-Matching-between-Longitudinal-Studies"><a href="#A-Hierarchical-Descriptor-Framework-for-On-the-Fly-Anatomical-Location-Matching-between-Longitudinal-Studies" class="headerlink" title="A Hierarchical Descriptor Framework for On-the-Fly Anatomical Location Matching between Longitudinal Studies"></a>A Hierarchical Descriptor Framework for On-the-Fly Anatomical Location Matching between Longitudinal Studies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07337">http://arxiv.org/abs/2308.07337</a></li>
<li>repo_url: None</li>
<li>paper_authors: Halid Ziya Yerebakan, Yoshihisa Shinagawa, Mahesh Ranganath, Simon Allen-Raffl, Gerardo Hermosillo Valadez</li>
<li>for: 医疗影像比较 longitudinal 比较中医学影像之间的 анатомиче位置匹配</li>
<li>methods: 使用 hierarchical sparse sampling of image intensities 计算查询点在源图像中的描述子，然后使用 hierarchical search 操作找到目标图像中最相似的描述子。</li>
<li>results: 实现了减少计算时间到毫秒级别的易行医学影像比较，无需额外建筑、存储或训练步骤。在 Deep Lesion Tracking 数据集注释中观察到更高准确的匹配结果，并且比最精确的报告algorithm 24 倍 faster。<details>
<summary>Abstract</summary>
We propose a method to match anatomical locations between pairs of medical images in longitudinal comparisons. The matching is made possible by computing a descriptor of the query point in a source image based on a hierarchical sparse sampling of image intensities that encode the location information. Then, a hierarchical search operation finds the corresponding point with the most similar descriptor in the target image. This simple yet powerful strategy reduces the computational time of mapping points to a millisecond scale on a single CPU. Thus, radiologists can compare similar anatomical locations in near real-time without requiring extra architectural costs for precomputing or storing deformation fields from registrations. Our algorithm does not require prior training, resampling, segmentation, or affine transformation steps. We have tested our algorithm on the recently published Deep Lesion Tracking dataset annotations. We observed more accurate matching compared to Deep Lesion Tracker while being 24 times faster than the most precise algorithm reported therein. We also investigated the matching accuracy on CT and MR modalities and compared the proposed algorithm's accuracy against ground truth consolidated from multiple radiologists.
</details>
<details>
<summary>摘要</summary>
我们提出了一种方法，用于在医疗图像对比中匹配生物学位置。该方法基于源图像中点Query的层次稀疏抽象来计算描述符，以便在目标图像中找到最相似的点。这种简单 yet powerful的策略可以在单个CPU上减少计算时间至毫秒级，因此，辐射学家可以在实时比较相似的生物学位置，无需额外的建筑成本或存储扭转场景的预计算或存储。我们的算法不需要先期训练、重新采样、分割或 afine 变换步骤。我们在最近发布的 Deep Lesion Tracking 数据集注释中进行了测试，并观察到比 Deep Lesion Tracker 更准确的匹配，而且比最精确的报告算法更快24倍。我们还研究了该方法在 CT 和 MR 模式下的匹配精度，并与多个 radiologists 共同组织的ground truth进行了比较。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/12/eess.IV_2023_08_12/" data-id="clltau95b00dtcr8820ha1ju3" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_11" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/11/cs.LG_2023_08_11/" class="article-date">
  <time datetime="2023-08-10T16:00:00.000Z" itemprop="datePublished">2023-08-11</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/11/cs.LG_2023_08_11/">cs.LG - 2023-08-11 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Towards-a-Causal-Probabilistic-Framework-for-Prediction-Action-Selection-Explanations-for-Robot-Block-Stacking-Tasks"><a href="#Towards-a-Causal-Probabilistic-Framework-for-Prediction-Action-Selection-Explanations-for-Robot-Block-Stacking-Tasks" class="headerlink" title="Towards a Causal Probabilistic Framework for Prediction, Action-Selection &amp; Explanations for Robot Block-Stacking Tasks"></a>Towards a Causal Probabilistic Framework for Prediction, Action-Selection &amp; Explanations for Robot Block-Stacking Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06203">http://arxiv.org/abs/2308.06203</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ricardo Cannizzaro, Jonathan Routley, Lars Kunze</li>
<li>for: 本研究旨在提供一种基于 causal probabilistic 框架的自主 робоット，以便让 robot 能够更好地理解和描述它所处环境。</li>
<li>methods: 本研究使用 causal inference 和 physics simulation 技术，将 causal models 与 probabilistic representations 结合起来，以便 robot 能够更好地理解和描述它所处环境。</li>
<li>results: 研究提出了一种新的 causal probabilistic 框架，可以帮助 robot 更好地完成块排序任务，并提供了一些 exemplar 的下一步行动选择结果。 I hope this helps! Let me know if you have any further questions or if there’s anything else I can help with.<details>
<summary>Abstract</summary>
Uncertainties in the real world mean that is impossible for system designers to anticipate and explicitly design for all scenarios that a robot might encounter. Thus, robots designed like this are fragile and fail outside of highly-controlled environments. Causal models provide a principled framework to encode formal knowledge of the causal relationships that govern the robot's interaction with its environment, in addition to probabilistic representations of noise and uncertainty typically encountered by real-world robots. Combined with causal inference, these models permit an autonomous agent to understand, reason about, and explain its environment. In this work, we focus on the problem of a robot block-stacking task due to the fundamental perception and manipulation capabilities it demonstrates, required by many applications including warehouse logistics and domestic human support robotics. We propose a novel causal probabilistic framework to embed a physics simulation capability into a structural causal model to permit robots to perceive and assess the current state of a block-stacking task, reason about the next-best action from placement candidates, and generate post-hoc counterfactual explanations. We provide exemplar next-best action selection results and outline planned experimentation in simulated and real-world robot block-stacking tasks.
</details>
<details>
<summary>摘要</summary>
世界中的不确定性使得系统设计者无法预期和明确地设计机器人可能遇到的所有场景。因此，基于这种设计方式的机器人在控制环境外会失败。 causal模型提供了一个原则性的框架，用于编码机器人与环境之间的 causal 关系，以及通常遇到的实际世界机器人中的抽象概率和不确定性。这些模型与 causal 推理相结合，允许自主机器人理解、推理和解释其环境。在这项工作中，我们关注了机器人块堆垫任务，因为它涉及到机器人的基本感知和操作能力，这些能力是许多应用程序，如仓库自动化和家庭服务机器人所必需的。我们提出了一种新的 causal 概率 frameworks，用于在机器人块堆垫任务中嵌入物理模拟能力，让机器人可以识别和评估当前块堆垫任务的状态，从选择候选位置中选择下一步行动，并生成后续 counterfactual 解释。我们提供了示例下一步行动选择结果，并详细描述计划在模拟和实际机器人块堆垫任务中进行实验。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Predicate-Visual-Context-in-Detecting-of-Human-Object-Interactions"><a href="#Exploring-Predicate-Visual-Context-in-Detecting-of-Human-Object-Interactions" class="headerlink" title="Exploring Predicate Visual Context in Detecting of Human-Object Interactions"></a>Exploring Predicate Visual Context in Detecting of Human-Object Interactions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06202">http://arxiv.org/abs/2308.06202</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fredzzhang/pvic">https://github.com/fredzzhang/pvic</a></li>
<li>paper_authors: Frederic Z. Zhang, Yuhui Yuan, Dylan Campbell, Zhuoyao Zhong, Stephen Gould</li>
<li>for: 本研究旨在解决人–物交互（HOI）领域中现有的问题，即使用两stage transformer-based HOI检测器，但这些检测器常常基于物体特征，而忽略了姿态和方向信息，导致复杂或抽象的交互检测受到阻碍。</li>
<li>methods: 本研究使用了视觉化和优化的查询设计、广泛的键和值搜索以及盒对位嵌入作为空间引导，以提高 predicate visual context（PViC）模型的表现，并与当前状态顶峰方法在 HICO-DET 和 V-COCO 测试集上进行比较。</li>
<li>results: 根据测试结果，我们的 PViC 模型在 HICO-DET 和 V-COCO 测试集上具有更高的表现，同时保持了训练成本的低。<details>
<summary>Abstract</summary>
Recently, the DETR framework has emerged as the dominant approach for human--object interaction (HOI) research. In particular, two-stage transformer-based HOI detectors are amongst the most performant and training-efficient approaches. However, these often condition HOI classification on object features that lack fine-grained contextual information, eschewing pose and orientation information in favour of visual cues about object identity and box extremities. This naturally hinders the recognition of complex or ambiguous interactions. In this work, we study these issues through visualisations and carefully designed experiments. Accordingly, we investigate how best to re-introduce image features via cross-attention. With an improved query design, extensive exploration of keys and values, and box pair positional embeddings as spatial guidance, our model with enhanced predicate visual context (PViC) outperforms state-of-the-art methods on the HICO-DET and V-COCO benchmarks, while maintaining low training cost.
</details>
<details>
<summary>摘要</summary>
Here's the text in Simplified Chinese:Recently, DETR 框架在人-物互动（HOI）研究中成为主流方法。特别是两阶段转换器基于 HOI 检测器在性能和训练效率方面表现出色。然而，这些通常基于缺乏细化上下文信息的对象特征，忽略对象的姿势和方向信息，而仅仅依靠对象的视觉特征来确定对象的标识和边框极限。这会导致复杂或抽象的互动无法正确识别。在这项工作中，我们通过视觉化和仔细设计的实验来研究这些问题。我们尝试通过跨注意力来重新引入图像特征，并通过改进的查询设计、广泛探索键和值、以及对象对的位域嵌入来提高 predicate 视觉上下文（PViC）模型的性能。我们的 PViC 模型在 HICO-DET 和 V-COCO 测试数据集上的表现比 state-of-the-art 方法更高，而且训练成本仍然很低。
</details></li>
</ul>
<hr>
<h2 id="Complex-Facial-Expression-Recognition-Using-Deep-Knowledge-Distillation-of-Basic-Features"><a href="#Complex-Facial-Expression-Recognition-Using-Deep-Knowledge-Distillation-of-Basic-Features" class="headerlink" title="Complex Facial Expression Recognition Using Deep Knowledge Distillation of Basic Features"></a>Complex Facial Expression Recognition Using Deep Knowledge Distillation of Basic Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06197">http://arxiv.org/abs/2308.06197</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/angusmaiden/complex-fer">https://github.com/angusmaiden/complex-fer</a></li>
<li>paper_authors: Angus Maiden, Bahareh Nakisa</li>
<li>for: 该论文的目的是提出一种基于人类认知和学习的新型不间断学习方法，以便准确地识别复杂的人脸表达。</li>
<li>methods: 该方法基于人类认知和学习，包括知识储存、知识总结和预测排序记忆等技术。它还使用 GradCAM 视觉化来表明基本和复杂表达之间的关系。</li>
<li>results: 该方法可以准确地识别新的复杂表达类型，使用少量示例来学习新类别，并且在新类别上达到了74.28% 的总准确率。此外，该方法还证明了不间断学习方法在复杂表达识别中的优越性，比非不间断学习方法高出13.95%。此外，该方法还是首次应用了几shot学习到复杂表达识别中，达到了100% 的准确率。<details>
<summary>Abstract</summary>
Complex emotion recognition is a cognitive task that has so far eluded the same excellent performance of other tasks that are at or above the level of human cognition. Emotion recognition through facial expressions is particularly difficult due to the complexity of emotions expressed by the human face. For a machine to approach the same level of performance in this domain as a human, it may need to synthesise knowledge and understand new concepts in real-time as humans do. Humans are able to learn new concepts using only few examples, by distilling the important information from memories and discarding the rest. Similarly, continual learning methods learn new classes whilst retaining the knowledge of known classes, whilst few-shot learning methods are able to learn new classes using very few training examples. We propose a novel continual learning method inspired by human cognition and learning that can accurately recognise new compound expression classes using few training samples, by building on and retaining its knowledge of basic expression classes. Using GradCAM visualisations, we demonstrate the relationship between basic and compound facial expressions, which our method leverages through knowledge distillation and a novel Predictive Sorting Memory Replay. Our method achieves the current state-of-the-art in continual learning for complex facial expression recognition with 74.28% Overall Accuracy on new classes. We also demonstrate that using continual learning for complex facial expression recognition achieves far better performance than non-continual learning methods, improving on state-of-the-art non-continual learning methods by 13.95%. To the best of our knowledge, our work is also the first to apply few-shot learning to complex facial expression recognition, achieving the state-of-the-art with 100% accuracy using a single training sample for each expression class.
</details>
<details>
<summary>摘要</summary>
人工智能在复杂情绪认知方面的表现，虽然已经达到了其他一些任务的水平，但是情绪认知仍然是一个挑战。人脸表达的情绪认知特别Difficult，因为人脸上可以表达出的情感复杂多样。为了让机器达到人类水平，它可能需要合并知识并理解新概念，就像人类一样。人类可以通过几个示例学习新概念，从记忆中提炼出重要信息，并丢弃其他信息。我们提出了一种基于人类认知和学习的新型连续学习方法，可以准确地识别新的复杂表达类型，使用很少的训练样本。我们使用GradCAM视觉化来描述基本和复杂表达之间的关系，然后通过知识储存和一种新的预测排序记忆回放来利用这种关系。我们的方法实现了当前领域内连续学习的最佳性，新类别上的总准确率为74.28%。我们还证明了使用连续学习进行复杂表达认知比非连续学习方法更好，提高了领域内最佳非连续学习方法的13.95%。此外，我们是第一个将几个样本学习应用于复杂表达认知，并达到了领域内最佳性，每个表达类型的准确率为100%。
</details></li>
</ul>
<hr>
<h2 id="Assessing-Guest-Nationality-Composition-from-Hotel-Reviews"><a href="#Assessing-Guest-Nationality-Composition-from-Hotel-Reviews" class="headerlink" title="Assessing Guest Nationality Composition from Hotel Reviews"></a>Assessing Guest Nationality Composition from Hotel Reviews</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06175">http://arxiv.org/abs/2308.06175</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fabian Gröger, Marc Pouly, Flavia Tinner, Leif Brandes</li>
<li>for: 这篇论文旨在为企业提供方法，以优化客户端的客户分布。</li>
<li>methods: 该论文使用机器学习技术，从不结构化文本评论中提取出客户国籍信息，以动态评估和监测具体业务客户分布的变化。</li>
<li>results: 研究表明，使用简单的预训练embeddings和堆式LSTM层可以提供更好的性能-运行时间平衡，比较复杂的语言模型。<details>
<summary>Abstract</summary>
Many hotels target guest acquisition efforts to specific markets in order to best anticipate individual preferences and needs of their guests. Likewise, such strategic positioning is a prerequisite for efficient marketing budget allocation. Official statistics report on the number of visitors from different countries, but no fine-grained information on the guest composition of individual businesses exists. There is, however, growing interest in such data from competitors, suppliers, researchers and the general public. We demonstrate how machine learning can be leveraged to extract references to guest nationalities from unstructured text reviews in order to dynamically assess and monitor the dynamics of guest composition of individual businesses. In particular, we show that a rather simple architecture of pre-trained embeddings and stacked LSTM layers provides a better performance-runtime tradeoff than more complex state-of-the-art language models.
</details>
<details>
<summary>摘要</summary>
许多酒店会向特定市场进行客户营销努力，以最好地预测客人偏好和需求。这种策略也是市场营销预算的必要前提。官方统计数据会报告不同国家的游客数量，但没有细化的信息对具体的企业客户组成进行了报告。然而，有越来越多的竞争对手、供应商、研究人员和公众对这些数据感兴趣。我们示例如如何使用机器学习来从无结构文本评论中提取客人国籍信息，以动态评估和监测个体企业客户组成的动态变化。具体来说，我们发现使用简单的预训练 embedding 和堆叠 LSTM 层可以提供更好的性能-运行时间质量比例，比较复杂的现状语言模型。
</details></li>
</ul>
<hr>
<h2 id="Physical-Adversarial-Attacks-For-Camera-based-Smart-Systems-Current-Trends-Categorization-Applications-Research-Challenges-and-Future-Outlook"><a href="#Physical-Adversarial-Attacks-For-Camera-based-Smart-Systems-Current-Trends-Categorization-Applications-Research-Challenges-and-Future-Outlook" class="headerlink" title="Physical Adversarial Attacks For Camera-based Smart Systems: Current Trends, Categorization, Applications, Research Challenges, and Future Outlook"></a>Physical Adversarial Attacks For Camera-based Smart Systems: Current Trends, Categorization, Applications, Research Challenges, and Future Outlook</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06173">http://arxiv.org/abs/2308.06173</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amira Guesmi, Muhammad Abdullah Hanif, Bassem Ouni, Muhammed Shafique</li>
<li>for: 这篇论文旨在为研究人员、实践者和政策制定者提供关于物理抗击攻击的全面评估和概述，以便开发强健和安全的深度学习系统。</li>
<li>methods: 论文分析了物理抗击攻击的主要特征和特点，并描述了不同应用领域中的物理抗击攻击方法，包括分类、检测、人脸识别、 semantic segmentation 和深度估计。</li>
<li>results: 论文评估了这些攻击方法的效果、隐蔽性和可靠性，并探讨了如何在实际世界中执行攻击，以及如何提高防御机制。<details>
<summary>Abstract</summary>
In this paper, we present a comprehensive survey of the current trends focusing specifically on physical adversarial attacks. We aim to provide a thorough understanding of the concept of physical adversarial attacks, analyzing their key characteristics and distinguishing features. Furthermore, we explore the specific requirements and challenges associated with executing attacks in the physical world. Our article delves into various physical adversarial attack methods, categorized according to their target tasks in different applications, including classification, detection, face recognition, semantic segmentation and depth estimation. We assess the performance of these attack methods in terms of their effectiveness, stealthiness, and robustness. We examine how each technique strives to ensure the successful manipulation of DNNs while mitigating the risk of detection and withstanding real-world distortions. Lastly, we discuss the current challenges and outline potential future research directions in the field of physical adversarial attacks. We highlight the need for enhanced defense mechanisms, the exploration of novel attack strategies, the evaluation of attacks in different application domains, and the establishment of standardized benchmarks and evaluation criteria for physical adversarial attacks. Through this comprehensive survey, we aim to provide a valuable resource for researchers, practitioners, and policymakers to gain a holistic understanding of physical adversarial attacks in computer vision and facilitate the development of robust and secure DNN-based systems.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提供了物理 adversarial 攻击的现代趋势的全面检查。我们的目的是为您提供物理 adversarial 攻击的深入理解，包括其关键特征和区别特征。此外，我们还探讨了在物理世界中执行攻击的具体要求和挑战。我们的文章探讨了不同应用领域中的物理 adversarial 攻击方法，分为不同的目标任务，如分类、检测、识别、 semantic segmentation 和深度估计。我们评估了这些攻击方法的效果、隐蔽性和Robustness。我们探究每种技术如何在 DNN 上成功 manipulate 而 minimizing 检测和快速应对实际扭曲。最后，我们讨论了物理 adversarial 攻击领域的当前挑战和未来研究方向，包括增强防御机制、探索新的攻击策略、在不同应用领域中评估攻击、以及建立 DNN 领域的标准化评估标准和评估方法。通过这篇全面的检查，我们希望为研究人员、实践人员和政策制定者提供一份有价值的资源，以便更好地理解物理 adversarial 攻击，并促进 DNN 基于系统的开发。
</details></li>
</ul>
<hr>
<h2 id="Phased-Deep-Spatio-temporal-Learning-for-Highway-Traffic-Volume-Prediction"><a href="#Phased-Deep-Spatio-temporal-Learning-for-Highway-Traffic-Volume-Prediction" class="headerlink" title="Phased Deep Spatio-temporal Learning for Highway Traffic Volume Prediction"></a>Phased Deep Spatio-temporal Learning for Highway Traffic Volume Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06155">http://arxiv.org/abs/2308.06155</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weilong Ding, Tianpu Zhang, Zhe Wang</li>
<li>for: 预测高速公路日常交通量</li>
<li>methods: employs a hybrid model combining fully convolution network (FCN) and long short-term memory (LSTM), considering time, space, meteorology, and calendar from heterogeneous data</li>
<li>results: 实际使用一个中国省级高速公路的实际数据，对比traditional models，our method has distinct improvement for predictive accuracy, reaching 5.269 and 0.997 in MPAE and R-squre metrics, respectively.<details>
<summary>Abstract</summary>
Inter-city highway transportation is significant for citizens' modern urban life and generates heterogeneous sensory data with spatio-temporal characteristics. As a routine analysis in transportation domain, daily traffic volume estimation faces challenges for highway toll stations including lacking of exploration of correlative spatio-temporal features from a long-term perspective and effective means to deal with data imbalance which always deteriorates the predictive performance. In this paper, a deep spatio-temporal learning method is proposed to predict daily traffic volume in three phases. In feature pre-processing phase, data is normalized elaborately according to latent long-tail distribution. In spatio-temporal learning phase, a hybrid model is employed combining fully convolution network (FCN) and long short-term memory (LSTM), which considers time, space, meteorology, and calendar from heterogeneous data. In decision phase, traffic volumes on a coming day at network-wide toll stations would be achieved effectively, which is especially calibrated for vital few highway stations. Using real-world data from one Chinese provincial highway, extensive experiments show our method has distinct improvement for predictive accuracy than various traditional models, reaching 5.269 and 0.997 in MPAE and R-squre metrics, respectively.
</details>
<details>
<summary>摘要</summary>
市区间高速公路交通是现代城市居民日常生活中的重要一环，生成了多样化的感知数据，具有空间时间特征。为了解决高速公路收费站的日常交通量预测问题，我们面临着缺乏探索长期征特的相关空间时间特征以及有效地处理数据不均衡问题的挑战。在本文中，我们提出了一种深度空间时间学习方法，可以预测高速公路收费站的日常交通量。在特征预处理阶段，我们对数据进行了细心的Normal化，根据潜在的长尾分布。在空间时间学习阶段，我们采用了一种混合模型，结合了全连接网络（FCN）和长短期记忆（LSTM），考虑了时间、空间、气象和历法等多种不同数据。在决策阶段，我们可以准确预测高速公路收费站的未来一天内的交通量，特别是对于重要的一些高速公路站点进行了精准补做。使用了一个中国省道高速公路的实际数据，我们进行了广泛的实验，结果表明，我们的方法在预测精度方面与传统模型相比，具有明显的改善，分别达到了5.269和0.997的MPAE和R-squre指标。
</details></li>
</ul>
<hr>
<h2 id="Gaussian-Process-Regression-for-Maximum-Entropy-Distribution"><a href="#Gaussian-Process-Regression-for-Maximum-Entropy-Distribution" class="headerlink" title="Gaussian Process Regression for Maximum Entropy Distribution"></a>Gaussian Process Regression for Maximum Entropy Distribution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06149">http://arxiv.org/abs/2308.06149</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohsen Sadr, Manuel Torrilhon, M. Hossein Gorji</li>
<li>for: 用于闭合问题的最大熵分布</li>
<li>methods: 使用加aussian prior逼近Lagrange多个性数</li>
<li>results: 对不同的kernel函数和Hyperparameter进行优化，实现数据驱动的最大熵闭合，并应用于非平衡分布的relaxation和Bhatnagar-Gross-Krook等方程的解。<details>
<summary>Abstract</summary>
Maximum-Entropy Distributions offer an attractive family of probability densities suitable for moment closure problems. Yet finding the Lagrange multipliers which parametrize these distributions, turns out to be a computational bottleneck for practical closure settings. Motivated by recent success of Gaussian processes, we investigate the suitability of Gaussian priors to approximate the Lagrange multipliers as a map of a given set of moments. Examining various kernel functions, the hyperparameters are optimized by maximizing the log-likelihood. The performance of the devised data-driven Maximum-Entropy closure is studied for couple of test cases including relaxation of non-equilibrium distributions governed by Bhatnagar-Gross-Krook and Boltzmann kinetic equations.
</details>
<details>
<summary>摘要</summary>
最大熵分布可以提供一个有优点的可能性密度，适用于矩阵闭合问题。然而，计算这些分布的拉格朗日 Parameters是实际应用中的计算瓶颈。受最近 Gaussian 过程的成功启发，我们研究将 Gaussian 假设用于 Approximate 拉格朗日 Parameters 的Map 的可行性。对各种核函数进行优化，我们使用最大 log-likelihood 来优化 гипер参数。我们研究了这种数据驱动的最大熵闭合的性能，并在几个测试案例中，包括适应不平衡分布的Relaxation 和 Boltzmann 动力学方程的闭合。
</details></li>
</ul>
<hr>
<h2 id="A-New-Approach-to-Overcoming-Zero-Trade-in-Gravity-Models-to-Avoid-Indefinite-Values-in-Linear-Logarithmic-Equations-and-Parameter-Verification-Using-Machine-Learning"><a href="#A-New-Approach-to-Overcoming-Zero-Trade-in-Gravity-Models-to-Avoid-Indefinite-Values-in-Linear-Logarithmic-Equations-and-Parameter-Verification-Using-Machine-Learning" class="headerlink" title="A New Approach to Overcoming Zero Trade in Gravity Models to Avoid Indefinite Values in Linear Logarithmic Equations and Parameter Verification Using Machine Learning"></a>A New Approach to Overcoming Zero Trade in Gravity Models to Avoid Indefinite Values in Linear Logarithmic Equations and Parameter Verification Using Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06303">http://arxiv.org/abs/2308.06303</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mikrajuddin Abdullah</li>
<li>For: The paper aims to solve the challenge of identifying gravity parameters in the gravity model to explain international trade, specifically when there are zero flow trades.* Methods: The authors propose a two-step technique that involves performing linear regression locally to establish a dummy value for zero flow trades, and then estimating the gravity parameters using iterative techniques. Machine learning is also used to test the estimated parameters by analyzing their position in the cluster.* Results: The authors calculate international trade figures for 2004, 2009, 2014, and 2019 and find that the powers of GDP and distance are in the same cluster and are both worth roughly one. The strategy presented in the paper can be used to solve other problems involving log-linear regression.Here’s the simplified Chinese text for the three information points:* 用途：文章解决国际贸易预测模型中零流通问题，即预测零流通时的重力参数。* 方法：文章提出了一种两步技术，先本地线性回归以确定零流通的假值，然后使用迭代法估算重力参数。同时，文章使用机器学习测试估算参数的位置在集群中。* 结果：文章计算了2004年、2009年、2014年和2019年的国际贸易数据，发现GDP的势和距离的势都处在同一个集群，均值约为1。文章的策略可以解决其他带有封零的log-线性回归问题。<details>
<summary>Abstract</summary>
The presence of a high number of zero flow trades continues to provide a challenge in identifying gravity parameters to explain international trade using the gravity model. Linear regression with a logarithmic linear equation encounters an indefinite value on the logarithmic trade. Although several approaches to solving this problem have been proposed, the majority of them are no longer based on linear regression, making the process of finding solutions more complex. In this work, we suggest a two-step technique for determining the gravity parameters: first, perform linear regression locally to establish a dummy value to substitute trade flow zero, and then estimating the gravity parameters. Iterative techniques are used to determine the optimum parameters. Machine learning is used to test the estimated parameters by analyzing their position in the cluster. We calculated international trade figures for 2004, 2009, 2014, and 2019. We just examine the classic gravity equation and discover that the powers of GDP and distance are in the same cluster and are both worth roughly one. The strategy presented here can be used to solve other problems involving log-linear regression.
</details>
<details>
<summary>摘要</summary>
高数量的零流贸易仍然成为国际贸易使用重力模型确定重力参数的挑战。线性回归的对数几何方程遇到了对数贸易的不定值。虽然有几种解决方案被提议，但大多数都不再基于线性回归，使得解决问题的过程变得更加复杂。在这项工作中，我们提议一种两步技巧来确定重力参数：首先，在本地使用线性回归来设置一个占位符来替代零流贸易，然后估算重力参数。使用迭代技术来确定优化参数。机器学习被用来测试估算的参数，分析它们的位置在群集中。我们计算了2004、2009、2014和2019年的国际贸易数据。我们只考虑 классический重力方程，发现GDP的势和距离的势都在同一个群集中，它们的值约为1。这种策略可以用于解决其他带有对数几何方程的问题。
</details></li>
</ul>
<hr>
<h2 id="Identification-of-the-Relevance-of-Comments-in-Codes-Using-Bag-of-Words-and-Transformer-Based-Models"><a href="#Identification-of-the-Relevance-of-Comments-in-Codes-Using-Bag-of-Words-and-Transformer-Based-Models" class="headerlink" title="Identification of the Relevance of Comments in Codes Using Bag of Words and Transformer Based Models"></a>Identification of the Relevance of Comments in Codes Using Bag of Words and Transformer Based Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06144">http://arxiv.org/abs/2308.06144</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sruthisudheer/comment-classification-of-c-code">https://github.com/sruthisudheer/comment-classification-of-c-code</a></li>
<li>paper_authors: Sruthi S, Tanmay Basu</li>
<li>for: 本研究的目的是为了对不同代码段的注释进行分类。</li>
<li>methods: 本研究使用了不同的特征工程方案和文本分类技术，包括经典的袋包模型和基于变换器的模型。</li>
<li>results: 研究发现，使用袋包模型在训练集上表现最佳，但模型在训练和测试集上的表现不理想。研究还总结了模型的局限性和进一步改进的可能性。<details>
<summary>Abstract</summary>
The Forum for Information Retrieval (FIRE) started a shared task this year for classification of comments of different code segments. This is binary text classification task where the objective is to identify whether comments given for certain code segments are relevant or not. The BioNLP-IISERB group at the Indian Institute of Science Education and Research Bhopal (IISERB) participated in this task and submitted five runs for five different models. The paper presents the overview of the models and other significant findings on the training corpus. The methods involve different feature engineering schemes and text classification techniques. The performance of the classical bag of words model and transformer-based models were explored to identify significant features from the given training corpus. We have explored different classifiers viz., random forest, support vector machine and logistic regression using the bag of words model. Furthermore, the pre-trained transformer based models like BERT, RoBERT and ALBERT were also used by fine-tuning them on the given training corpus. The performance of different such models over the training corpus were reported and the best five models were implemented on the given test corpus. The empirical results show that the bag of words model outperforms the transformer based models, however, the performance of our runs are not reasonably well in both training and test corpus. This paper also addresses the limitations of the models and scope for further improvement.
</details>
<details>
<summary>摘要</summary>
forum for information retrieval (FIRE) 这年开始了代码段评注分类的共同任务。这是一个二进制文本分类任务，目标是判断给定的代码段评注是否相关。bioNLP-IISERB 组在印度科学教育研究所 Bhopal（IISERB）参加了这个任务，并提交了五个运行，每个运行使用了不同的模型。本文介绍了模型和其他有关训练集的发现。方法包括不同的特征工程方案和文本分类技术。我们研究了传统的袋子模型和基于 transformer 的模型，并使用了不同的分类器，如Random Forest、支持向量机和логисти准则回归。此外，我们还使用了预训练的 transformer 模型，如 BERT、RoBERT 和 ALBERT，并在给定的训练集上进行了微调。对于给定的训练集和测试集，我们Reported 不同模型的性能。 Results show that the bag of words model outperforms the transformer-based models, but the performance of our runs is not satisfactory in both the training and test corpora. This paper also discusses the limitations of the models and the scope for further improvement.
</details></li>
</ul>
<hr>
<h2 id="CompTLL-UNet-Compressed-Domain-Text-Line-Localization-in-Challenging-Handwritten-Documents-using-Deep-Feature-Learning-from-JPEG-Coefficients"><a href="#CompTLL-UNet-Compressed-Domain-Text-Line-Localization-in-Challenging-Handwritten-Documents-using-Deep-Feature-Learning-from-JPEG-Coefficients" class="headerlink" title="CompTLL-UNet: Compressed Domain Text-Line Localization in Challenging Handwritten Documents using Deep Feature Learning from JPEG Coefficients"></a>CompTLL-UNet: Compressed Domain Text-Line Localization in Challenging Handwritten Documents using Deep Feature Learning from JPEG Coefficients</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06142">http://arxiv.org/abs/2308.06142</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bulla Rajesh, Sk Mahafuz Zaman, Mohammed Javed, P. Nagabhushan</li>
<li>for: 本研究旨在提出一种能够直接在JPEG压缩表示中进行文本线Localization的方法，以避免 decompression 和重新压缩的过程，从而降低存储和计算成本。</li>
<li>methods: 提出了一种基于深度特征学习的Modified U-Net架构，称为Compressed Text-Line Localization Network (CompTLL-UNet)，用于实现文本线Localization。</li>
<li>results: 通过对ICDAR2017（cBAD）和ICDAR2019（cBAD）测试集进行训练和测试，实现了在JPEG压缩Domain中的文本线Localization，并reported state-of-the-art perfomance。<details>
<summary>Abstract</summary>
Automatic localization of text-lines in handwritten documents is still an open and challenging research problem. Various writing issues such as uneven spacing between the lines, oscillating and touching text, and the presence of skew become much more challenging when the case of complex handwritten document images are considered for segmentation directly in their respective compressed representation. This is because, the conventional way of processing compressed documents is through decompression, but here in this paper, we propose an idea that employs deep feature learning directly from the JPEG compressed coefficients without full decompression to accomplish text-line localization in the JPEG compressed domain. A modified U-Net architecture known as Compressed Text-Line Localization Network (CompTLL-UNet) is designed to accomplish it. The model is trained and tested with JPEG compressed version of benchmark datasets including ICDAR2017 (cBAD) and ICDAR2019 (cBAD), reporting the state-of-the-art performance with reduced storage and computational costs in the JPEG compressed domain.
</details>
<details>
<summary>摘要</summary>
自动化手写文档中文行的本地化仍然是一个打开的和挑战性的研究问题。不同的写作问题，如文本行间距不均匀、文本抖动和触摸、扭曲等问题，在考虑复杂手写文档图像时变得更加挑战。这是因为，传统的文档处理方法是通过解压缩来处理压缩文档，但在这篇论文中，我们提出了一个想法，即使用深度特征学习直接从JPEG压缩率中提取特征来实现文本行的本地化。我们称之为Compressed Text-Line Localization Network（CompTLL-UNet）。我们设计了一种修改后的U-Net架构，并对其进行训练和测试，使用JPEG压缩版本的标准评测数据集，包括ICDAR2017（cBAD）和ICDAR2019（cBAD）。我们的模型在JPEG压缩领域实现了状态之巅性表现，同时具有减少存储和计算成本的优点。
</details></li>
</ul>
<hr>
<h2 id="Application-of-Artificial-Neural-Networks-for-Investigation-of-Pressure-Filtration-Performance-a-Zinc-Leaching-Filter-Cake-Moisture-Modeling"><a href="#Application-of-Artificial-Neural-Networks-for-Investigation-of-Pressure-Filtration-Performance-a-Zinc-Leaching-Filter-Cake-Moisture-Modeling" class="headerlink" title="Application of Artificial Neural Networks for Investigation of Pressure Filtration Performance, a Zinc Leaching Filter Cake Moisture Modeling"></a>Application of Artificial Neural Networks for Investigation of Pressure Filtration Performance, a Zinc Leaching Filter Cake Moisture Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06138">http://arxiv.org/abs/2308.06138</a></li>
<li>repo_url: None</li>
<li>paper_authors: Masoume Kazemi, Davood Moradkhani, Alireza A. Alipour</li>
<li>for: 这项研究旨在开发一个人工神经网络模型，用于预测压filtering proces中的蛋白湿度。</li>
<li>methods: 该研究使用了人工神经网络技术，并在288次测试中采用了两种不同的 Filter Fabric（S1和S2）。</li>
<li>results: 研究结果显示，人工神经网络模型可以高度准确地预测压filtering proces中的蛋白湿度，R2值分别为0.88和0.83，MSE值分别为6.243x10-07和1.086x10-06，MAE值分别为0.00056和0.00088。<details>
<summary>Abstract</summary>
Machine Learning (ML) is a powerful tool for material science applications. Artificial Neural Network (ANN) is a machine learning technique that can provide high prediction accuracy. This study aimed to develop an ANN model to predict the cake moisture of the pressure filtration process of zinc production. The cake moisture was influenced by seven parameters: temperature (35 and 65 Celsius), solid concentration (0.2 and 0.38 g/L), pH (2, 3.5, and 5), air-blow time (2, 10, and 15 min), cake thickness (14, 20, 26, and 34 mm), pressure, and filtration time. The study conducted 288 tests using two types of fabrics: polypropylene (S1) and polyester (S2). The ANN model was evaluated by the Coefficient of determination (R2), the Mean Square Error (MSE), and the Mean Absolute Error (MAE) metrics for both datasets. The results showed R2 values of 0.88 and 0.83, MSE values of 6.243x10-07 and 1.086x10-06, and MAE values of 0.00056 and 0.00088 for S1 and S2, respectively. These results indicated that the ANN model could predict the cake moisture of pressure filtration in the zinc leaching process with high accuracy.
</details>
<details>
<summary>摘要</summary>
机器学习（ML）是一种强大的工具，可以应用于材料科学领域。人工神经网络（ANN）是一种机器学习技术，可以提供高精度预测。本研究目的是开发一个ANN模型，以预测压 filtering过程中锻生产中的蛋白湿度。蛋白湿度受到七个参数的影响：温度（35和65摄氏度）、固体浓度（0.2和0.38g/L）、pH（2、3.5和5）、空气喷流时间（2、10和15分）、蛋白厚度（14、20、26和34mm）、压力和过滤时间。研究通过288次测试，使用两种不同的 fabrics： polypropylene（S1）和 polyester（S2）。ANN模型被评估于 Coefficient of determination（R2）、Mean Square Error（MSE）和 Mean Absolute Error（MAE）三个指标，其中R2值分别为0.88和0.83，MSE值分别为6.243x10-07和1.086x10-06，MAE值分别为0.00056和0.00088。这些结果表明，ANN模型可以高精度预测压 filtering过程中的蛋白湿度。
</details></li>
</ul>
<hr>
<h2 id="PDE-Discovery-for-Soft-Sensors-Using-Coupled-Physics-Informed-Neural-Network-with-Akaike’s-Information-Criterion"><a href="#PDE-Discovery-for-Soft-Sensors-Using-Coupled-Physics-Informed-Neural-Network-with-Akaike’s-Information-Criterion" class="headerlink" title="PDE Discovery for Soft Sensors Using Coupled Physics-Informed Neural Network with Akaike’s Information Criterion"></a>PDE Discovery for Soft Sensors Using Coupled Physics-Informed Neural Network with Akaike’s Information Criterion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06132">http://arxiv.org/abs/2308.06132</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aina Wang, Pan Qin, Xi-Ming Sun</li>
<li>For: 这篇论文旨在探讨一种基于物理学习的软传感器PDE发现方法，以便更好地监测工业过程中的关键变量。* Methods: 该方法基于物理学习的软传感器PDE模型，并使用了Akaike的准则信息来找到合适的偏微分方程结构。* Results: 实验结果表明，CPINN-AIC方法可以准确地找到合适的偏微分方程结构，并且可以用来预测工业过程中的变量。<details>
<summary>Abstract</summary>
Soft sensors have been extensively used to monitor key variables using easy-to-measure variables and mathematical models. Partial differential equations (PDEs) are model candidates for soft sensors in industrial processes with spatiotemporal dependence. However, gaps often exist between idealized PDEs and practical situations. Discovering proper structures of PDEs, including the differential operators and source terms, can remedy the gaps. To this end, a coupled physics-informed neural network with Akaike's criterion information (CPINN-AIC) is proposed for PDE discovery of soft sensors. First, CPINN is adopted for obtaining solutions and source terms satisfying PDEs. Then, we propose a data-physics-hybrid loss function for training CPINN, in which undetermined combinations of differential operators are involved. Consequently, AIC is used to discover the proper combination of differential operators. Finally, the artificial and practical datasets are used to verify the feasibility and effectiveness of CPINN-AIC for soft sensors. The proposed CPINN-AIC is a data-driven method to discover proper PDE structures and neural network-based solutions for soft sensors.
</details>
<details>
<summary>摘要</summary>
Soft sensors 通过使用容易测量的变量和数学模型来监测关键变量。但是，实际情况中存在idealized PDEs和实际情况之间的差距。发现合适的 PDE 结构，包括偏微分运算和源项，可以解决这些差距。为此，我们提出了物理学习混合数据损失函数（CPINN-AIC），用于PDE发现。首先，我们采用CPINN来获取满足 PDE 的解和源项。然后，我们提出了一种数据物理混合损失函数，其中包含未知的偏微分运算。最后，我们使用AIC来发现合适的偏微分运算结构。 Finally, we use artificial and practical datasets to verify the feasibility and effectiveness of CPINN-AIC for soft sensors. The proposed CPINN-AIC is a data-driven method to discover proper PDE structures and neural network-based solutions for soft sensors.Note: The translation is done using the Simplified Chinese grammar and vocabulary, which is commonly used in mainland China. However, it should be noted that there are different dialects and variations of Chinese, and the translation may vary depending on the specific region or dialect.
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-Quantification-for-Image-based-Traffic-Prediction-across-Cities"><a href="#Uncertainty-Quantification-for-Image-based-Traffic-Prediction-across-Cities" class="headerlink" title="Uncertainty Quantification for Image-based Traffic Prediction across Cities"></a>Uncertainty Quantification for Image-based Traffic Prediction across Cities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06129">http://arxiv.org/abs/2308.06129</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alextimans/traffic4cast-uncertainty">https://github.com/alextimans/traffic4cast-uncertainty</a></li>
<li>paper_authors: Alexander Timans, Nina Wiedemann, Nishant Kumar, Ye Hong, Martin Raubal</li>
<li>for: 这个论文的目的是提高深度学习模型在智能交通系统中的可解释性，以便更好地做出决策和提高模型的部署潜力。</li>
<li>methods: 这个论文使用了两种 эпистемиче和两种 aleatoric 不确定性评估方法来评估深度学习模型的不确定性，并对多个城市和时间段进行比较。</li>
<li>results: 研究发现，可以通过使用不确定性评估方法来获得有意义的不确定性估计，并且可以用这些估计来检测城市交通动力学的异常情况。在一个 Moscow 的示例研究中，我们发现了时间和空间效应的影响于城市交通行为。<details>
<summary>Abstract</summary>
Despite the strong predictive performance of deep learning models for traffic prediction, their widespread deployment in real-world intelligent transportation systems has been restrained by a lack of interpretability. Uncertainty quantification (UQ) methods provide an approach to induce probabilistic reasoning, improve decision-making and enhance model deployment potential. To gain a comprehensive picture of the usefulness of existing UQ methods for traffic prediction and the relation between obtained uncertainties and city-wide traffic dynamics, we investigate their application to a large-scale image-based traffic dataset spanning multiple cities and time periods. We compare two epistemic and two aleatoric UQ methods on both temporal and spatio-temporal transfer tasks, and find that meaningful uncertainty estimates can be recovered. We further demonstrate how uncertainty estimates can be employed for unsupervised outlier detection on changes in city traffic dynamics. We find that our approach can capture both temporal and spatial effects on traffic behaviour in a representative case study for the city of Moscow. Our work presents a further step towards boosting uncertainty awareness in traffic prediction tasks, and aims to highlight the value contribution of UQ methods to a better understanding of city traffic dynamics.
</details>
<details>
<summary>摘要</summary>
启示深度学习模型对交通预测的强大预测能力，实际应用中的广泛部署却受到了不可预测性的限制。不确定性评估（UQ）方法可以带来 probabilistic reasoning，改善决策和提高模型部署的潜力。为了了解现有UQ方法对交通预测的用途和获得的不确定性与城市范围内交通动力学的关系，我们对多座城市和多个时间段的大规模图像基本交通数据进行了 investigate。我们比较了两种 эпистеمic和两种 aleatoric UQ方法的性能在时间和空间转移任务上，并发现了有意义的不确定性估计可以被恢复。我们还示出了如何使用不确定性估计进行无supervised outlier检测，检测城市交通动力学的变化。我们在 Moskva 城市的示例研究中发现，我们的方法可以捕捉到时间和空间效应的交通行为。我们的工作是 uncertainty awareness 在交通预测任务中的一个进一步步骤，旨在高亮不确定性评估对城市交通动力学的理解的重要性。
</details></li>
</ul>
<hr>
<h2 id="Learning-Control-Policies-for-Variable-Objectives-from-Offline-Data"><a href="#Learning-Control-Policies-for-Variable-Objectives-from-Offline-Data" class="headerlink" title="Learning Control Policies for Variable Objectives from Offline Data"></a>Learning Control Policies for Variable Objectives from Offline Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06127">http://arxiv.org/abs/2308.06127</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marc Weber, Phillip Swazinna, Daniel Hein, Steffen Udluft, Volkmar Sterzing</li>
<li>for: 本研究旨在提供一种可靠的机器学习方法，用于控制动态系统，特别是当直接与环境交互不可用时。</li>
<li>methods: 本研究使用了模型基于政策搜索方法的扩展，即变量目标策略（VOP）。这种方法使得政策可以高效地泛化到多种目标，即在奖励函数中的参数。</li>
<li>results: 通过对目标函数中的参数进行调整，用户可以在运行时调整政策的行为或重新平衡优化目标，无需收集更多的观察批量或重新训练。<details>
<summary>Abstract</summary>
Offline reinforcement learning provides a viable approach to obtain advanced control strategies for dynamical systems, in particular when direct interaction with the environment is not available. In this paper, we introduce a conceptual extension for model-based policy search methods, called variable objective policy (VOP). With this approach, policies are trained to generalize efficiently over a variety of objectives, which parameterize the reward function. We demonstrate that by altering the objectives passed as input to the policy, users gain the freedom to adjust its behavior or re-balance optimization targets at runtime, without need for collecting additional observation batches or re-training.
</details>
<details>
<summary>摘要</summary>
转换文本为简化中文：</SYS>在线上学习不可靠的方法可以提供先进的控制策略 для动力系统，特别是当直接与环境进行交互不可用时。本文提出了基于模型的政策搜索方法的概念扩展，称为变量目标策略（VOP）。通过这种方法，政策被训练以通用化效率地处理多种目标，这些目标参数化奖励函数。我们示例显示，通过在运行时更改目标，用户可以在不需要收集更多观察批处或重新训练的情况下，通过变化目标来调整行为或重新平衡优化目标。
</details></li>
</ul>
<hr>
<h2 id="Learning-Deductive-Reasoning-from-Synthetic-Corpus-based-on-Formal-Logic"><a href="#Learning-Deductive-Reasoning-from-Synthetic-Corpus-based-on-Formal-Logic" class="headerlink" title="Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic"></a>Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07336">http://arxiv.org/abs/2308.07336</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hitachi-nlp/fld">https://github.com/hitachi-nlp/fld</a></li>
<li>paper_authors: Terufumi Morishita, Gaku Morio, Atsuki Yamaguchi, Yasuhiro Sogawa</li>
<li>for: 本研究旨在使语言模型（LM）学习逻辑推理能力。 previous studies使用特定的推理规则生成推理例子，但这些规则有限或else arbitrary，这限制了获得的逻辑推理能力的一致性。</li>
<li>methods: 我们采用基于正式逻辑理论的准确的推理规则集，可以 derivation 任何其他推理规则。我们名为这种推理规则集为 $\textbf{FLD}$（正式逻辑推理）。</li>
<li>results: 我们实验表明，使用 $\textbf{FLD}$ 推理规则集训练LMs，LMs可以获得更一致的逻辑推理能力。此外，我们还识别了逻辑推理能力中哪些方面可以通过推理 corpora 增强LMs，那些方面无法增强。最后，基于这些结果，我们讨论未来如何使用推理 corpora 或其他方法来解决每个方面的问题。我们发布了代码、数据和模型。<details>
<summary>Abstract</summary>
We study a synthetic corpus-based approach for language models (LMs) to acquire logical deductive reasoning ability. The previous studies generated deduction examples using specific sets of deduction rules. However, these rules were limited or otherwise arbitrary. This can limit the generalizability of acquired deductive reasoning ability. We rethink this and adopt a well-grounded set of deduction rules based on formal logic theory, which can derive any other deduction rules when combined in a multistep way. We empirically verify that LMs trained on the proposed corpora, which we name $\textbf{FLD}$ ($\textbf{F}$ormal $\textbf{L}$ogic $\textbf{D}$eduction), acquire more generalizable deductive reasoning ability. Furthermore, we identify the aspects of deductive reasoning ability on which deduction corpora can enhance LMs and those on which they cannot. Finally, on the basis of these results, we discuss the future directions for applying deduction corpora or other approaches for each aspect. We release the code, data, and models.
</details>
<details>
<summary>摘要</summary>
我们研究了一种基于合成语料库的方法，用于语言模型（LM）学习逻辑推理能力。之前的研究通过特定的推理规则生成了推理示例，但这些规则是有限的或else是arbitrary的。这会限制学习得到的逻辑推理能力的通用性。我们重新思考了这一点，采用基于形式逻辑理论的固定的推理规则，可以在多步骤中组合以 derivation任何其他的推理规则。我们employmaterially verify that LMs trained on our proposed corpora，which we name $\textbf{FLD}$ ($\textbf{F}$ormal $\textbf{L}$ogic $\textbf{D}$eduction), acquire more generalizable deductive reasoning ability。此外，我们还确定了推理能力中哪些方面可以通过推理 corpora进行增强，以及哪些方面无法进行增强。最后，基于这些结果，我们讨论了将来采用推理 corpora或其他方法对每个方面的应用。我们释放了代码、数据和模型。
</details></li>
</ul>
<hr>
<h2 id="Hawkes-Processes-with-Delayed-Granger-Causality"><a href="#Hawkes-Processes-with-Delayed-Granger-Causality" class="headerlink" title="Hawkes Processes with Delayed Granger Causality"></a>Hawkes Processes with Delayed Granger Causality</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06106">http://arxiv.org/abs/2308.06106</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao Yang, Hengyuan Miao, Shuang Li</li>
<li>for: 本研究旨在Explicitly Modeling delayed Granger causal effects based on multivariate Hawkes processes, 即 causal event usually takes some time to exert an effect, 研究这个时间延迟自身的科学意义。</li>
<li>methods: 我们首先证明了延迟参数的可 identificability under mild conditions, 然后 investigate a model estimation method under complex setting, 即 want to infer the posterior distribution of time lags and understand how this distribution varies across different scenarios, 我们将时延 treated as latent variables, 并使用Variational Auto-Encoder (VAE) algorithm to approximate the posterior distribution of time lags.</li>
<li>results: 我们 empirically evaluate our model’s event prediction and time-lag inference accuracy on synthetic and real data, achieving promising results.<details>
<summary>Abstract</summary>
We aim to explicitly model the delayed Granger causal effects based on multivariate Hawkes processes. The idea is inspired by the fact that a causal event usually takes some time to exert an effect. Studying this time lag itself is of interest. Given the proposed model, we first prove the identifiability of the delay parameter under mild conditions. We further investigate a model estimation method under a complex setting, where we want to infer the posterior distribution of the time lags and understand how this distribution varies across different scenarios. We treat the time lags as latent variables and formulate a Variational Auto-Encoder (VAE) algorithm to approximate the posterior distribution of the time lags. By explicitly modeling the time lags in Hawkes processes, we add flexibility to the model. The inferred time-lag posterior distributions are of scientific meaning and help trace the original causal time that supports the root cause analysis. We empirically evaluate our model's event prediction and time-lag inference accuracy on synthetic and real data, achieving promising results.
</details>
<details>
<summary>摘要</summary>
我们目标是显式地模型延迟的格兰格 causal 效应基于多变量 Hawkes 过程。这个想法源于事件引起效应通常需要一些时间。研究这个时间延迟本身很有趣。给出的模型，我们首先证明延迟参数的可识别性于轻量级 услови下。我们进一步调查了一种复杂的设定下的模型估计方法，我们想要从多个enario中推断时延参数的 posterior 分布，并理解这个分布在不同enario下如何变化。我们将时延参数作为隐藏变量，并采用Variational Auto-Encoder（VAE）算法来近似 posterior 分布。通过显式地模型 Hawkes 过程中的时延参数，我们增加了模型的灵活性。经验证明我们的模型在实验数据上的事件预测和时延参数推断精度都很高。
</details></li>
</ul>
<hr>
<h2 id="Composable-Function-preserving-Expansions-for-Transformer-Architectures"><a href="#Composable-Function-preserving-Expansions-for-Transformer-Architectures" class="headerlink" title="Composable Function-preserving Expansions for Transformer Architectures"></a>Composable Function-preserving Expansions for Transformer Architectures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06103">http://arxiv.org/abs/2308.06103</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrea Gesmundo, Kaitlin Maile</li>
<li>for: 提高现代神经网络的训练成本，特别是计算和时间成本。</li>
<li>methods: 提出六种可 композиitely 的变换，用于逐步增加 transformer 类神经网络的大小，保持功能完整性。</li>
<li>results: 证明每种变换都可以保持函数完整性，并且可以有效地升级模型规模。<details>
<summary>Abstract</summary>
Training state-of-the-art neural networks requires a high cost in terms of compute and time. Model scale is recognized to be a critical factor to achieve and improve the state-of-the-art. Increasing the scale of a neural network normally requires restarting from scratch by randomly initializing all the parameters of the model, as this implies a change of architecture's parameters that does not allow for a straightforward transfer of knowledge from smaller size models. In this work, we propose six composable transformations to incrementally increase the size of transformer-based neural networks while preserving functionality, allowing to expand the capacity of the model as needed. We provide proof of exact function preservation under minimal initialization constraints for each transformation. The proposed methods may enable efficient training pipelines for larger and more powerful models by progressively expanding the architecture throughout training.
</details>
<details>
<summary>摘要</summary>
培训现代神经网络需要高效计算和时间成本。模型缩放被认为是提高现状的关键因素。在增加模型缩放时，通常需要从scratch开始，随机初始化整个模型的参数，因为这会导致模型结构中参数的变化，不允许小型模型知识的直接传递。在这项工作中，我们提出六种可组合的变换来逐步增加基于转换器的神经网络缩放，保持功能完整性，以便在训练过程中逐步扩展模型的容量。我们提供了准确功能保持的证明，并且在 minimal initialization constraints 下进行证明。这些方法可能会帮助建立更大更强的模型，并通过逐步扩展模型结构来实现高效的训练管道。
</details></li>
</ul>
<hr>
<h2 id="Diffusion-based-Visual-Counterfactual-Explanations-–-Towards-Systematic-Quantitative-Evaluation"><a href="#Diffusion-based-Visual-Counterfactual-Explanations-–-Towards-Systematic-Quantitative-Evaluation" class="headerlink" title="Diffusion-based Visual Counterfactual Explanations – Towards Systematic Quantitative Evaluation"></a>Diffusion-based Visual Counterfactual Explanations – Towards Systematic Quantitative Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06100">http://arxiv.org/abs/2308.06100</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cairo-thws/dbvce_eval">https://github.com/cairo-thws/dbvce_eval</a></li>
<li>paper_authors: Philipp Vaeth, Alexander M. Fruehwald, Benjamin Paassen, Magda Gregorova</li>
<li>For: This paper aims to provide a systematic and quantitative evaluation framework for visual counterfactual explanations (VCE) methods, and to explore the effects of crucial design choices in the latest diffusion-based generative models for VCEs of natural image classification (ImageNet).* Methods: The paper proposes a framework for evaluating VCE methods using a minimal set of metrics, and conducts a battery of ablation-like experiments generating thousands of VCEs for a suite of classifiers of various complexity, accuracy, and robustness.* Results: The paper finds multiple directions for future advancements and improvements of VCE methods, and provides a valuable guidance for researchers in the field fostering consistency and transparency in the assessment of counterfactual explanations.Here is the same information in Simplified Chinese text:* For: 这篇论文目标是提供一个系统的和量化的评估框架，以帮助评估视觉对称解释（VCE）方法，并探索最新的扩散基于生成模型中的关键设计选择对自然图像分类（ImageNet）VCE的影响。* Methods: 论文提出一个评估VCE方法的最小集合的度量，并通过大量的拟合实验生成了不同复杂性、准确率和稳定性的多个分类器的VCE。* Results: 论文发现了未来的进步和改进的方向，并提供了一个有价值的指南，以便在评估对称解释中增加一致性和透明度。<details>
<summary>Abstract</summary>
Latest methods for visual counterfactual explanations (VCE) harness the power of deep generative models to synthesize new examples of high-dimensional images of impressive quality. However, it is currently difficult to compare the performance of these VCE methods as the evaluation procedures largely vary and often boil down to visual inspection of individual examples and small scale user studies. In this work, we propose a framework for systematic, quantitative evaluation of the VCE methods and a minimal set of metrics to be used. We use this framework to explore the effects of certain crucial design choices in the latest diffusion-based generative models for VCEs of natural image classification (ImageNet). We conduct a battery of ablation-like experiments, generating thousands of VCEs for a suite of classifiers of various complexity, accuracy and robustness. Our findings suggest multiple directions for future advancements and improvements of VCE methods. By sharing our methodology and our approach to tackle the computational challenges of such a study on a limited hardware setup (including the complete code base), we offer a valuable guidance for researchers in the field fostering consistency and transparency in the assessment of counterfactual explanations.
</details>
<details>
<summary>摘要</summary>
最新的视觉对比解释方法（VCE）利用深度生成模型Synthesize高维像素图像的新示例，质量非常高。然而，目前很难比较这些VCE方法的性能，因为评估方法大多不同，经常降到视觉检查具体示例和小规模用户研究。在这项工作中，我们提出了一个系统性评估VCE方法的框架和最小的 metric集，并使用这些框架来探索 diffusion-based生成模型在自然图像分类（ImageNet）中VCE的效果。我们进行了一系列减少-like实验，生成了数千个VCE，用于一组不同的分类器，包括不同的复杂度、准确率和鲁棒性。我们的发现建议了未来VCE方法的进一步改进。通过分享我们的方法和我们对限制硬件设置（包括完整的代码库）的处理方式，我们提供了行业研究人员的有价值指南，促进了透明度和一致性在对对比解释的评估中。
</details></li>
</ul>
<hr>
<h2 id="Neural-Conversation-Models-and-How-to-Rein-Them-in-A-Survey-of-Failures-and-Fixes"><a href="#Neural-Conversation-Models-and-How-to-Rein-Them-in-A-Survey-of-Failures-and-Fixes" class="headerlink" title="Neural Conversation Models and How to Rein Them in: A Survey of Failures and Fixes"></a>Neural Conversation Models and How to Rein Them in: A Survey of Failures and Fixes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06095">http://arxiv.org/abs/2308.06095</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fabian Galetzka, Anne Beyer, David Schlangen</li>
<li>for: 本研究探讨了基于强大语言模型的开放领域对话系统，以做出合适的对话贡献。</li>
<li>methods: 研究人员使用了不同的截止点和训练策略来控制语言模型，以确保贡献的优质。</li>
<li>results: 研究人员发现了一些有前途的方法，并建议了未来研究的新方向。<details>
<summary>Abstract</summary>
Recent conditional language models are able to continue any kind of text source in an often seemingly fluent way. This fact encouraged research in the area of open-domain conversational systems that are based on powerful language models and aim to imitate an interlocutor by generating appropriate contributions to a written dialogue. From a linguistic perspective, however, the complexity of contributing to a conversation is high. In this survey, we interpret Grice's maxims of cooperative conversation from the perspective of this specific research area and systematize the literature under the aspect of what makes a contribution appropriate: A neural conversation model has to be fluent, informative, consistent, coherent, and follow social norms. In order to ensure these qualities, recent approaches try to tame the underlying language models at various intervention points, such as data, training regime or decoding. Sorted by these categories and intervention points, we discuss promising attempts and suggest novel ways for future research.
</details>
<details>
<summary>摘要</summary>
现代条件语言模型能够继续任何类型的文本源，并且在看起来很流畅地进行交流。这一事实激发了基于强大语言模型的开放领域对话系统的研究，旨在通过生成相应的贡献来模拟对话伙伴。从语言学角度来看，参与对话的复杂度很高。在这种情况下，我们根据这个特定的研究领域来解释格雷斯的协作对话原则，并将文献分为合适贡献的几个方面：一个神经网络对话模型需要流畅、有用、一致、 coherent 和遵循社会规范。为确保这些质量，当前的方法在不同的 intervening point 上尝试控制基础语言模型，例如数据、训练方法或解码。按照这些类别和 intervening point 排序，我们讨论了有前途的尝试，并建议未来研究的新方法。
</details></li>
</ul>
<hr>
<h2 id="Reinforcement-Logic-Rule-Learning-for-Temporal-Point-Processes"><a href="#Reinforcement-Logic-Rule-Learning-for-Temporal-Point-Processes" class="headerlink" title="Reinforcement Logic Rule Learning for Temporal Point Processes"></a>Reinforcement Logic Rule Learning for Temporal Point Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06094">http://arxiv.org/abs/2308.06094</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao Yang, Lu Wang, Kun Gao, Shuang Li</li>
<li>for: 用于解释 temporal events 的发生</li>
<li>methods: 使用 temporal point process modeling and learning framework，逐渐优化规则集和其重要性，并通过 neural search policy 生成新规则</li>
<li>results: 在 synthetic 和实际医疗数据上获得了promising的结果<details>
<summary>Abstract</summary>
We propose a framework that can incrementally expand the explanatory temporal logic rule set to explain the occurrence of temporal events. Leveraging the temporal point process modeling and learning framework, the rule content and weights will be gradually optimized until the likelihood of the observational event sequences is optimal. The proposed algorithm alternates between a master problem, where the current rule set weights are updated, and a subproblem, where a new rule is searched and included to best increase the likelihood. The formulated master problem is convex and relatively easy to solve using continuous optimization, whereas the subproblem requires searching the huge combinatorial rule predicate and relationship space. To tackle this challenge, we propose a neural search policy to learn to generate the new rule content as a sequence of actions. The policy parameters will be trained end-to-end using the reinforcement learning framework, where the reward signals can be efficiently queried by evaluating the subproblem objective. The trained policy can be used to generate new rules in a controllable way. We evaluate our methods on both synthetic and real healthcare datasets, obtaining promising results.
</details>
<details>
<summary>摘要</summary>
我们提出了一个框架，可以逐步扩展解释时间事件的发生。利用时间点处理模型和学习框架，规则内容和权重将被逐步优化，直到观测事件序列的可能性最高。我们的算法会 alternate между主问题和子问题。主问题中，当前规则集权重将被更新；而子问题中，一个新的规则将被搜索并添加到最大化可能性。我们形式ulated主问题是凸型的，可以使用连续优化来解决；而子问题则需要搜索庞大的 combinatorial 规则 predicate 和关系空间。为了解决这个挑战，我们提出了一种神经搜索策略，可以学习生成新规则的内容作为一个序列动作。这个策略的参数将通过可行学习框架进行培养，其中的奖励信号可以快速地查询由辅助问题的目标函数来提供。已经训练的策略可以用于生成新规则的控制方式。我们对具有 sintetic 和实际医疗数据的方法进行了评估，获得了有前途的结果。
</details></li>
</ul>
<hr>
<h2 id="Experts-Weights-Averaging-A-New-General-Training-Scheme-for-Vision-Transformers"><a href="#Experts-Weights-Averaging-A-New-General-Training-Scheme-for-Vision-Transformers" class="headerlink" title="Experts Weights Averaging: A New General Training Scheme for Vision Transformers"></a>Experts Weights Averaging: A New General Training Scheme for Vision Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06093">http://arxiv.org/abs/2308.06093</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongqi Huang, Peng Ye, Xiaoshui Huang, Sheng Li, Tao Chen, Wanli Ouyang</li>
<li>for: 提高 ViT 模型的性能而不增加推理成本</li>
<li>methods: 使用 Mixture-of-Experts (MoE) 实现对 ViT 模型的训练，并在训练和推理阶段之间进行分解</li>
<li>results: 对多个 2D 和 3D 视觉任务、ViT 架构和数据集进行了全面的实验 validate 提议的训练方法的效果和普适性，同时还可以应用于细化 ViT 模型的 fine-tuning 过程中提高性能。<details>
<summary>Abstract</summary>
Structural re-parameterization is a general training scheme for Convolutional Neural Networks (CNNs), which achieves performance improvement without increasing inference cost. As Vision Transformers (ViTs) are gradually surpassing CNNs in various visual tasks, one may question: if a training scheme specifically for ViTs exists that can also achieve performance improvement without increasing inference cost? Recently, Mixture-of-Experts (MoE) has attracted increasing attention, as it can efficiently scale up the capacity of Transformers at a fixed cost through sparsely activated experts. Considering that MoE can also be viewed as a multi-branch structure, can we utilize MoE to implement a ViT training scheme similar to structural re-parameterization? In this paper, we affirmatively answer these questions, with a new general training strategy for ViTs. Specifically, we decouple the training and inference phases of ViTs. During training, we replace some Feed-Forward Networks (FFNs) of the ViT with specially designed, more efficient MoEs that assign tokens to experts by random uniform partition, and perform Experts Weights Averaging (EWA) on these MoEs at the end of each iteration. After training, we convert each MoE into an FFN by averaging the experts, transforming the model back into original ViT for inference. We further provide a theoretical analysis to show why and how it works. Comprehensive experiments across various 2D and 3D visual tasks, ViT architectures, and datasets validate the effectiveness and generalizability of the proposed training scheme. Besides, our training scheme can also be applied to improve performance when fine-tuning ViTs. Lastly, but equally important, the proposed EWA technique can significantly improve the effectiveness of naive MoE in various 2D visual small datasets and 3D visual tasks.
</details>
<details>
<summary>摘要</summary>
《Structural re-parameterization是一种通用训练方案 для Convolutional Neural Networks (CNNs),它可以提高性能而不增加推理成本。在Vision Transformers (ViTs)逐渐超越CNNs的视觉任务中，有人可能会提问：如果存在专门 дляViTs的训练方案，可以提高性能而不增加推理成本？Recently, Mixture-of-Experts (MoE)has attracted increasing attention,因为它可以高效地扩展Transformers的容量在固定成本下。考虑到MoE可以被视为多支分支结构，那么我们可以使用MoE来实现ViTs的训练方案类似于structural re-parameterization。在这篇论文中，我们答于这些问题，并提出了一种新的通用训练策略 дляViTs。 Specifically,我们在训练阶段将ViTs中的一些Feed-Forward Networks (FFNs)替换为特制的、更高效的MoEs，并在每个迭代结束后进行Experts Weights Averaging (EWA)。之后，我们将MoEs转换成FFNs，并将模型转换回原始的ViTs模型进行推理。我们还提供了一种理论分析，以证明这种训练方案的有效性和如何工作。我们在多种2D和3D视觉任务、ViT结构和数据集上进行了广泛的实验，证明了提议的训练策略的效果和通用性。此外，我们的训练策略还可以用于改进ViTs的性能when fine-tuning。最后，但也非常重要的是，我们提出的EWA技术可以在多种2D视觉小数据集和3D视觉任务中显著提高MoE的效果。》
</details></li>
</ul>
<hr>
<h2 id="Toward-a-Better-Understanding-of-Loss-Functions-for-Collaborative-Filtering"><a href="#Toward-a-Better-Understanding-of-Loss-Functions-for-Collaborative-Filtering" class="headerlink" title="Toward a Better Understanding of Loss Functions for Collaborative Filtering"></a>Toward a Better Understanding of Loss Functions for Collaborative Filtering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06091">http://arxiv.org/abs/2308.06091</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/psm1206/mawu">https://github.com/psm1206/mawu</a></li>
<li>paper_authors: Seongmin Park, Mincheol Yoon, Jae-woong Lee, Hogun Park, Jongwuk Lee</li>
<li>for: 这篇论文主要研究了相互推荐系统中的协同推荐技术，具体来说是分析现有的损失函数之间的关系，并提出了一种新的损失函数来改进现有的协同推荐模型。</li>
<li>methods: 该论文使用了数学分析来探究现有损失函数的关系，并在这基础上提出了一种新的损失函数called Margin-aware Alignment and Weighted Uniformity (MAWU)，它通过（i）margin-aware alignment（MA）和（ii）weighted uniformity（WU）来改进协同推荐模型的设计。</li>
<li>results: 实验结果表明，当 equiped with MAWU，MF和LightGCN相比现有的协同推荐模型，在三个公共数据集上具有相当或更高的性能。<details>
<summary>Abstract</summary>
Collaborative filtering (CF) is a pivotal technique in modern recommender systems. The learning process of CF models typically consists of three components: interaction encoder, loss function, and negative sampling. Although many existing studies have proposed various CF models to design sophisticated interaction encoders, recent work shows that simply reformulating the loss functions can achieve significant performance gains. This paper delves into analyzing the relationship among existing loss functions. Our mathematical analysis reveals that the previous loss functions can be interpreted as alignment and uniformity functions: (i) the alignment matches user and item representations, and (ii) the uniformity disperses user and item distributions. Inspired by this analysis, we propose a novel loss function that improves the design of alignment and uniformity considering the unique patterns of datasets called Margin-aware Alignment and Weighted Uniformity (MAWU). The key novelty of MAWU is two-fold: (i) margin-aware alignment (MA) mitigates user/item-specific popularity biases, and (ii) weighted uniformity (WU) adjusts the significance between user and item uniformities to reflect the inherent characteristics of datasets. Extensive experimental results show that MF and LightGCN equipped with MAWU are comparable or superior to state-of-the-art CF models with various loss functions on three public datasets.
</details>
<details>
<summary>摘要</summary>
合作 filtering (CF) 是现代推荐系统中的关键技术。 CF 模型的学习过程通常包括三个组成部分：交互编码器、损失函数和负样本。虽然现有的研究已经提出了许多不同的 CF 模型，但是最近的研究表明，只是修改损失函数的设计可以获得显著性能提升。本文分析了现有损失函数之间的关系。我们的数学分析表明，前一些损失函数可以被解释为对用户和项目表示的对齐和分布均匀函数：（i）对用户和项目表示进行对齐（ii）对用户和项目分布进行均匀化。根据这一分析，我们提出了一种新的损失函数，称为 Margin-aware Alignment and Weighted Uniformity (MAWU)。MAWU 的关键创新有两个方面：（i）对用户/项目特有的流行偏好进行缓和（ii）根据数据集的特点进行加权均匀化。我们进行了广泛的实验研究，发现 MF 和 LightGCN 搭配 MAWU 与 state-of-the-art CF 模型相比，在三个公共数据集上具有相似或更高的性能。
</details></li>
</ul>
<hr>
<h2 id="Safeguarding-Learning-based-Control-for-Smart-Energy-Systems-with-Sampling-Specifications"><a href="#Safeguarding-Learning-based-Control-for-Smart-Energy-Systems-with-Sampling-Specifications" class="headerlink" title="Safeguarding Learning-based Control for Smart Energy Systems with Sampling Specifications"></a>Safeguarding Learning-based Control for Smart Energy Systems with Sampling Specifications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06069">http://arxiv.org/abs/2308.06069</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chih-Hong Cheng, Venkatesh Prasad Venkataramanan, Pragya Kirti Gupta, Yun-Fei Hsu, Simon Burton</li>
<li>for: 这篇论文是关于使用强化学习控制能源系统中的挑战，特别是在保证性和安全性两个方面的。</li>
<li>methods: 论文详细介绍了在实时逻辑中强化学习安全要求的方法，包括将实时逻辑转换为线性逻辑（LTL），以便利用高级工程技术，如安全学习盾和正式验证。</li>
<li>results: 论文表明，通过将实时逻辑转换为LTL，可以在强化学习过程中提供更高的安全性保证，并且可以通过统计模型检查来获得更高的满意度。<details>
<summary>Abstract</summary>
We study challenges using reinforcement learning in controlling energy systems, where apart from performance requirements, one has additional safety requirements such as avoiding blackouts. We detail how these safety requirements in real-time temporal logic can be strengthened via discretization into linear temporal logic (LTL), such that the satisfaction of the LTL formulae implies the satisfaction of the original safety requirements. The discretization enables advanced engineering methods such as synthesizing shields for safe reinforcement learning as well as formal verification, where for statistical model checking, the probabilistic guarantee acquired by LTL model checking forms a lower bound for the satisfaction of the original real-time safety requirements.
</details>
<details>
<summary>摘要</summary>
Here is the text in Simplified Chinese:我们研究和开发用控制能源系统的强化学习，同时考虑性能要求和安全要求，如避免黑OUT。我们详细说明如何通过离散到线性时间逻辑（LTL）来加强安全要求，使得满足LTL公式的满足性意味着满足原始的安全要求。这种离散Enabled advanced工程技术，如生成安全屏障和正式验证，以及统计模型检查中的概率保证，这个保证是原始时间安全要求满足的下界。
</details></li>
</ul>
<hr>
<h2 id="Deep-learning-based-flow-disaggregation-for-hydropower-plant-management"><a href="#Deep-learning-based-flow-disaggregation-for-hydropower-plant-management" class="headerlink" title="Deep learning-based flow disaggregation for hydropower plant management"></a>Deep learning-based flow disaggregation for hydropower plant management</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11631">http://arxiv.org/abs/2308.11631</a></li>
<li>repo_url: None</li>
<li>paper_authors: Duo Zhang</li>
<li>for:  Norwegian hydropower plant management</li>
<li>methods:  deep learning-based time series disaggregation model</li>
<li>results:  promising results for disaggregating daily flow into hourly flow<details>
<summary>Abstract</summary>
High temporal resolution data is a vital resource for hydropower plant management. Currently, only daily resolution data are available for most of Norwegian hydropower plant, however, to achieve more accurate management, sub-daily resolution data are often required. To deal with the wide absence of sub-daily data, time series disaggregation is a potential tool. In this study, we proposed a time series disaggregation model based on deep learning, the model is tested using flow data from a Norwegian flow station, to disaggregate the daily flow into hourly flow. Preliminary results show some promising aspects for the proposed model.
</details>
<details>
<summary>摘要</summary>
高时间分辨率数据是 Norway 水力发电厂的重要资源。目前，大多数 Norwegian 水力发电厂的数据只有每天的分辨率，但是为更准确的管理， often 需要更高的时间分辨率数据。为了解决宽泛的无法获得 sub-daily 数据的问题，时间序列分解是一种可能的工具。本研究提出了基于深度学习的时间序列分解模型，在使用挪威流站的流量数据进行测试，以分解每天的流量为每小时的流量。初步结果显示该模型具有一些有前途的特点。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-SGD-with-Polyak-stepsize-and-Line-search-Robust-Convergence-and-Variance-Reduction"><a href="#Adaptive-SGD-with-Polyak-stepsize-and-Line-search-Robust-Convergence-and-Variance-Reduction" class="headerlink" title="Adaptive SGD with Polyak stepsize and Line-search: Robust Convergence and Variance Reduction"></a>Adaptive SGD with Polyak stepsize and Line-search: Robust Convergence and Variance Reduction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06058">http://arxiv.org/abs/2308.06058</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaowen Jiang, Sebastian U. Stich</li>
<li>for: 这个论文目的是提出两种新的随机波兰梯（AdaSPS和AdaSLS），以确保在非 interpolative 设定下进行训练，并且在对 convex 和强 convex 函数进行训练时维持下线性和线性的 converges 速率。</li>
<li>methods: 这两种新算法使用了随机波兰梯和随机搜索，并且使用了一种新的减少偏差的技术来提高速度。</li>
<li>results: 这些新算法可以在非 interpolative 设定下进行训练，并且可以在对 convex 和强 convex 函数进行训练时维持下线性和线性的 converges 速率，并且可以和 AdaSVRG 的速率匹配，但是不需要内部外部循环结构。<details>
<summary>Abstract</summary>
The recently proposed stochastic Polyak stepsize (SPS) and stochastic line-search (SLS) for SGD have shown remarkable effectiveness when training over-parameterized models. However, in non-interpolation settings, both algorithms only guarantee convergence to a neighborhood of a solution which may result in a worse output than the initial guess. While artificially decreasing the adaptive stepsize has been proposed to address this issue (Orvieto et al. [2022]), this approach results in slower convergence rates for convex and over-parameterized models. In this work, we make two contributions: Firstly, we propose two new variants of SPS and SLS, called AdaSPS and AdaSLS, which guarantee convergence in non-interpolation settings and maintain sub-linear and linear convergence rates for convex and strongly convex functions when training over-parameterized models. AdaSLS requires no knowledge of problem-dependent parameters, and AdaSPS requires only a lower bound of the optimal function value as input. Secondly, we equip AdaSPS and AdaSLS with a novel variance reduction technique and obtain algorithms that require $\smash{\widetilde{\mathcal{O}}}(n+1/\epsilon)$ gradient evaluations to achieve an $\mathcal{O}(\epsilon)$-suboptimality for convex functions, which improves upon the slower $\mathcal{O}(1/\epsilon^2)$ rates of AdaSPS and AdaSLS without variance reduction in the non-interpolation regimes. Moreover, our result matches the fast rates of AdaSVRG but removes the inner-outer-loop structure, which is easier to implement and analyze. Finally, numerical experiments on synthetic and real datasets validate our theory and demonstrate the effectiveness and robustness of our algorithms.
</details>
<details>
<summary>摘要</summary>
Recently, the stochastic Polyak stepsize (SPS) and stochastic line-search (SLS) for stochastic gradient descent (SGD) have been proposed and have shown great effectiveness in training over-parameterized models. However, in non-interpolation settings, both algorithms only guarantee convergence to a neighborhood of a solution, which may result in a worse output than the initial guess. To address this issue, artificially decreasing the adaptive stepsize has been proposed (Orvieto et al., 2022), but this approach leads to slower convergence rates for convex and over-parameterized models.In this work, we make two contributions:Firstly, we propose two new variants of SPS and SLS, called AdaSPS and AdaSLS, which guarantee convergence in non-interpolation settings and maintain sub-linear and linear convergence rates for convex and strongly convex functions when training over-parameterized models. AdaSLS does not require knowledge of problem-dependent parameters, and AdaSPS only requires a lower bound of the optimal function value as input.Secondly, we equip AdaSPS and AdaSLS with a novel variance reduction technique, and obtain algorithms that require $\smash{\widetilde{\mathcal{O}}}(n+1/\epsilon)$ gradient evaluations to achieve an $\mathcal{O}(\epsilon)$-suboptimality for convex functions, which improves upon the slower $\mathcal{O}(1/\epsilon^2)$ rates of AdaSPS and AdaSLS without variance reduction in the non-interpolation regimes. Moreover, our result matches the fast rates of AdaSVRG but removes the inner-outer-loop structure, which is easier to implement and analyze.Finally, numerical experiments on synthetic and real datasets validate our theory and demonstrate the effectiveness and robustness of our algorithms.
</details></li>
</ul>
<hr>
<h2 id="Cost-effective-On-device-Continual-Learning-over-Memory-Hierarchy-with-Miro"><a href="#Cost-effective-On-device-Continual-Learning-over-Memory-Hierarchy-with-Miro" class="headerlink" title="Cost-effective On-device Continual Learning over Memory Hierarchy with Miro"></a>Cost-effective On-device Continual Learning over Memory Hierarchy with Miro</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06053">http://arxiv.org/abs/2308.06053</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyue Ma, Suyeon Jeong, Minjia Zhang, Di Wang, Jonghyun Choi, Myeongjae Jeon</li>
<li>for: 本研究旨在实现Edge设备上的持续学习（Continual Learning，CL）系统，以提高数据隐私和能源效率。</li>
<li>methods: 本研究使用层次memory replay的CL方法，并开发了一个名为Miro的系统运行时，用于在Edge设备上动态配置CL系统以实现最佳成本效率。</li>
<li>results: 对baseline系统进行比较，Miro显示了显著的成本效率提升。<details>
<summary>Abstract</summary>
Continual learning (CL) trains NN models incrementally from a continuous stream of tasks. To remember previously learned knowledge, prior studies store old samples over a memory hierarchy and replay them when new tasks arrive. Edge devices that adopt CL to preserve data privacy are typically energy-sensitive and thus require high model accuracy while not compromising energy efficiency, i.e., cost-effectiveness. Our work is the first to explore the design space of hierarchical memory replay-based CL to gain insights into achieving cost-effectiveness on edge devices. We present Miro, a novel system runtime that carefully integrates our insights into the CL framework by enabling it to dynamically configure the CL system based on resource states for the best cost-effectiveness. To reach this goal, Miro also performs online profiling on parameters with clear accuracy-energy trade-offs and adapts to optimal values with low overhead. Extensive evaluations show that Miro significantly outperforms baseline systems we build for comparison, consistently achieving higher cost-effectiveness.
</details>
<details>
<summary>摘要</summary>
Our work is the first to explore the design space of hierarchical memory replay-based CL to gain insights into achieving cost-effectiveness on edge devices. We present Miro, a novel system runtime that carefully integrates our insights into the CL framework by dynamically configuring the CL system based on resource states for the best cost-effectiveness. To reach this goal, Miro performs online profiling on parameters with clear accuracy-energy trade-offs and adapts to optimal values with low overhead.Extensive evaluations show that Miro significantly outperforms baseline systems we built for comparison, consistently achieving higher cost-effectiveness.
</details></li>
</ul>
<hr>
<h2 id="Towards-Instance-adaptive-Inference-for-Federated-Learning"><a href="#Towards-Instance-adaptive-Inference-for-Federated-Learning" class="headerlink" title="Towards Instance-adaptive Inference for Federated Learning"></a>Towards Instance-adaptive Inference for Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06051">http://arxiv.org/abs/2308.06051</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chunmeifeng/fedins">https://github.com/chunmeifeng/fedins</a></li>
<li>paper_authors: Chun-Mei Feng, Kai Yu, Nian Liu, Xinxing Xu, Salman Khan, Wangmeng Zuo</li>
<li>for: 这个论文的目的是提出一种基于联合学习（Federated Learning，FL）框架的实例适应性推理方法，以提高FL在复杂实际数据上的性能。</li>
<li>methods: 这个论文使用了一种基于缩放和偏移（scale and shift）的深度特征方法（SSF），以及一种客户端启发式推理方法（instance-adaptive inference），以适应实际数据上的实例差异性。</li>
<li>results: 对于Tiny-ImageNet dataset，这个方法比顶峰性能的方法提高6.64%，而且通信成本低于15%。<details>
<summary>Abstract</summary>
Federated learning (FL) is a distributed learning paradigm that enables multiple clients to learn a powerful global model by aggregating local training. However, the performance of the global model is often hampered by non-i.i.d. distribution among the clients, requiring extensive efforts to mitigate inter-client data heterogeneity. Going beyond inter-client data heterogeneity, we note that intra-client heterogeneity can also be observed on complex real-world data and seriously deteriorate FL performance. In this paper, we present a novel FL algorithm, i.e., FedIns, to handle intra-client data heterogeneity by enabling instance-adaptive inference in the FL framework. Instead of huge instance-adaptive models, we resort to a parameter-efficient fine-tuning method, i.e., scale and shift deep features (SSF), upon a pre-trained model. Specifically, we first train an SSF pool for each client, and aggregate these SSF pools on the server side, thus still maintaining a low communication cost. To enable instance-adaptive inference, for a given instance, we dynamically find the best-matched SSF subsets from the pool and aggregate them to generate an adaptive SSF specified for the instance, thereby reducing the intra-client as well as the inter-client heterogeneity. Extensive experiments show that our FedIns outperforms state-of-the-art FL algorithms, e.g., a 6.64\% improvement against the top-performing method with less than 15\% communication cost on Tiny-ImageNet. Our code and models will be publicly released.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 是一种分布式学习 paradigm，允许多个客户端学习一个强大的全球模型，通过Client中的本地训练数据进行汇总。然而，全球模型的性能经常受到客户端数据之间的非同质化的影响，需要广泛的减少客户端数据之间的不同性。此外，我们注意到了复杂的实际数据中的内部客户端数据不同性，也会严重降低 FL 性能。在这篇论文中，我们提出了一种新的 FL 算法，即 FedIns，以处理内部客户端数据不同性。我们在 FL 框架中实现了实例适应的推理，而不需要巨大的实例适应模型。我们首先在每个客户端上训练一个可缩放和调整的深度特征池（SSF），并在服务器端将这些 SSF 池进行汇总，以保持低的通信成本。为实现实例适应推理，对于一个给定的实例，我们在实例级别 dynamically 找到最佳适应的 SSF 子集，并将这些子集进行汇总，以生成适应该实例的 adaptive SSF。这有助于降低内部客户端数据不同性以及客户端数据之间的不同性。我们的 FedIns 在 Tiny-ImageNet 上比顶尖方法提供了6.64%的提升，并且与之前的最好方法在 less than 15% 的通信成本下。我们将代码和模型公开发布。
</details></li>
</ul>
<hr>
<h2 id="AI-Assisted-Investigation-of-On-Chain-Parameters-Risky-Cryptocurrencies-and-Price-Factors"><a href="#AI-Assisted-Investigation-of-On-Chain-Parameters-Risky-Cryptocurrencies-and-Price-Factors" class="headerlink" title="AI-Assisted Investigation of On-Chain Parameters: Risky Cryptocurrencies and Price Factors"></a>AI-Assisted Investigation of On-Chain Parameters: Risky Cryptocurrencies and Price Factors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08554">http://arxiv.org/abs/2308.08554</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdulrezzak Zekiye, Semih Utku, Fadi Amroush, Oznur Ozkasap<br>for:This paper aims to analyze historical data and use artificial intelligence algorithms to identify the factors affecting a cryptocurrency’s price and to find risky cryptocurrencies.methods:The paper uses on-chain parameters to analyze historical cryptocurrency data and employs clustering and classification techniques to group cryptocurrencies based on their on-chain characteristics. The paper also uses multiple classifiers to predict whether a cryptocurrency is risky or not.results:The analysis revealed that a significant proportion of cryptocurrencies (39%) disappeared from the market, while only a small fraction (10%) survived for more than 1000 days. The paper also found a significant negative correlation between cryptocurrency price and maximum and total supply, as well as a weak positive correlation between price and 24-hour trading volume. Additionally, the paper clustered cryptocurrencies into five distinct groups based on their on-chain parameters, and obtained the best f1-score of 76% using K-Nearest Neighbor for predicting risky cryptocurrencies.<details>
<summary>Abstract</summary>
Cryptocurrencies have become a popular and widely researched topic of interest in recent years for investors and scholars. In order to make informed investment decisions, it is essential to comprehend the factors that impact cryptocurrency prices and to identify risky cryptocurrencies. This paper focuses on analyzing historical data and using artificial intelligence algorithms on on-chain parameters to identify the factors affecting a cryptocurrency's price and to find risky cryptocurrencies. We conducted an analysis of historical cryptocurrencies' on-chain data and measured the correlation between the price and other parameters. In addition, we used clustering and classification in order to get a better understanding of a cryptocurrency and classify it as risky or not. The analysis revealed that a significant proportion of cryptocurrencies (39%) disappeared from the market, while only a small fraction (10%) survived for more than 1000 days. Our analysis revealed a significant negative correlation between cryptocurrency price and maximum and total supply, as well as a weak positive correlation between price and 24-hour trading volume. Moreover, we clustered cryptocurrencies into five distinct groups using their on-chain parameters, which provides investors with a more comprehensive understanding of a cryptocurrency when compared to those clustered with it. Finally, by implementing multiple classifiers to predict whether a cryptocurrency is risky or not, we obtained the best f1-score of 76% using K-Nearest Neighbor.
</details>
<details>
<summary>摘要</summary>
digital currencies 在最近几年内已经成为投资者和学者关注的热点话题。为了做出 Informed 投资决策，需要了解 криптовалюencies 价格的影响因素并确定风险较高的 криптовалюencies。这篇论文通过分析历史数据和使用人工智能算法对 chain 参数进行分析，以确定 криптовалюencies 价格的影响因素和风险评估。我们对历史 криптовалюencies 的 chain 数据进行分析，并测量价格和其他参数之间的相关性。此外，我们还使用聚类和分类来更好地理解 криптовалюencies，并将其分为五个不同类别。最后，我们通过应用多种分类器来预测 криптовалюencies 是否为风险的，并获得了最佳的 f1 分数为 76%。
</details></li>
</ul>
<hr>
<h2 id="Controlling-Character-Motions-without-Observable-Driving-Source"><a href="#Controlling-Character-Motions-without-Observable-Driving-Source" class="headerlink" title="Controlling Character Motions without Observable Driving Source"></a>Controlling Character Motions without Observable Driving Source</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06025">http://arxiv.org/abs/2308.06025</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weiyuan Li, Bin Dai, Ziyi Zhou, Qi Yao, Baoyuan Wang</li>
<li>for: 生成无驱动源的多样化、自然和无限长的头部&#x2F;身体序列</li>
<li>methods: 提议一个系统性框架，结合VQ-VAE和一种新的токен级控制策略，使用返回学习算法和经过设计的奖励函数来生成无限长的多样化和自然的头部&#x2F;身体序列</li>
<li>results: 通过全面的评估，发现提议的框架可以解决无驱动源生成中的各种挑战，并与其他强基线相比表现出众。<details>
<summary>Abstract</summary>
How to generate diverse, life-like, and unlimited long head/body sequences without any driving source? We argue that this under-investigated research problem is non-trivial at all, and has unique technical challenges behind it. Without semantic constraints from the driving sources, using the standard autoregressive model to generate infinitely long sequences would easily result in 1) out-of-distribution (OOD) issue due to the accumulated error, 2) insufficient diversity to produce natural and life-like motion sequences and 3) undesired periodic patterns along the time. To tackle the above challenges, we propose a systematic framework that marries the benefits of VQ-VAE and a novel token-level control policy trained with reinforcement learning using carefully designed reward functions. A high-level prior model can be easily injected on top to generate unlimited long and diverse sequences. Although we focus on no driving sources now, our framework can be generalized for controlled synthesis with explicit driving sources. Through comprehensive evaluations, we conclude that our proposed framework can address all the above-mentioned challenges and outperform other strong baselines very significantly.
</details>
<details>
<summary>摘要</summary>
如何生成无驱动源的多样化、生命般自然的头部/身体序列？我们认为这是一个未受抨拿的研究问题，具有独特的技术挑战。不受 semantics 驱动源的限制，使用标准的自然语言模型来生成无限长序列会导致1) OOD 问题 Due to the accumulated error, 2) 不够多样性来生成自然和生命般的动作序列和 3) 不想要的时间 periodic patterns.为了解决以上挑战，我们提议一个系统性的框架，该框架结合 VQ-VAE 的优点和一种基于 reinforcement learning 的新的 токен级控制策略。高级 prior model 可以轻松地注入到该框架中，以生成无限长和多样化的序列。虽然我们现在没有驱动源，但我们的框架可以通过 Carefully designed reward functions 来扩展到控制的 synthesis 中Explicit driving sources。通过全面的评估，我们结议了我们提议的框架可以解决所有以上挑战，并与其他强大的基准模型相比，表现非常出色。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Picture-Description-Speech-for-Dementia-Detection-using-Image-text-Alignment"><a href="#Evaluating-Picture-Description-Speech-for-Dementia-Detection-using-Image-text-Alignment" class="headerlink" title="Evaluating Picture Description Speech for Dementia Detection using Image-text Alignment"></a>Evaluating Picture Description Speech for Dementia Detection using Image-text Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07933">http://arxiv.org/abs/2308.07933</a></li>
<li>repo_url: None</li>
<li>paper_authors: Youxiang Zhu, Nana Lin, Xiaohui Liang, John A. Batsis, Robert M. Roth, Brian MacWhinney</li>
<li>for: 本研究旨在提高诊断老人痴呆症的精度，通过利用图像描述文本对应关系来提高检测精度。</li>
<li>methods: 本研究提出了首个将图像和描述文本作为输入，并利用大规模预训练图像文本对应模型的知识来进行诊断的模型。我们发现了健康和痴呆样本之间的文本与图像之间的差异，并使用文本与图像之间的相关性来排序和筛选样本。此外，我们还将图像分解成不同主题，并将文本分类为每个主题中的不同话题。</li>
<li>results: 我们的三种进阶模型，通过对样本进行预处理，使用图像与文本之间的相关性和图像分解、文本分类等技术，实现了诊断精度的提高。我们的最佳模型在83.44%的检测精度上得到了状元表现，高于文本只基线模型的79.91%。此外，我们还可视化样本和图像结果，以便解释我们的模型的优势。<details>
<summary>Abstract</summary>
Using picture description speech for dementia detection has been studied for 30 years. Despite the long history, previous models focus on identifying the differences in speech patterns between healthy subjects and patients with dementia but do not utilize the picture information directly. In this paper, we propose the first dementia detection models that take both the picture and the description texts as inputs and incorporate knowledge from large pre-trained image-text alignment models. We observe the difference between dementia and healthy samples in terms of the text's relevance to the picture and the focused area of the picture. We thus consider such a difference could be used to enhance dementia detection accuracy. Specifically, we use the text's relevance to the picture to rank and filter the sentences of the samples. We also identified focused areas of the picture as topics and categorized the sentences according to the focused areas. We propose three advanced models that pre-processed the samples based on their relevance to the picture, sub-image, and focused areas. The evaluation results show that our advanced models, with knowledge of the picture and large image-text alignment models, achieve state-of-the-art performance with the best detection accuracy at 83.44%, which is higher than the text-only baseline model at 79.91%. Lastly, we visualize the sample and picture results to explain the advantages of our models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Large-Language-Models-for-Telecom-Forthcoming-Impact-on-the-Industry"><a href="#Large-Language-Models-for-Telecom-Forthcoming-Impact-on-the-Industry" class="headerlink" title="Large Language Models for Telecom: Forthcoming Impact on the Industry"></a>Large Language Models for Telecom: Forthcoming Impact on the Industry</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06013">http://arxiv.org/abs/2308.06013</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Maatouk, Nicola Piovesan, Fadhel Ayed, Antonio De Domenico, Merouane Debbah</li>
<li>for: 本研究旨在探讨LLM技术在电信领域的应用和影响，以及如何在这些领域中充分利用LLM的潜力。</li>
<li>methods: 本研究采用了LLM技术的内部结构和应用场景的分析，以及在电信领域中可以立即实施的用例的探讨。</li>
<li>results: 研究发现了LLM技术在电信领域的现有能力和局限性，以及需要进一步研究的领域和挑战。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have emerged as a transformative force, revolutionizing numerous fields well beyond the conventional domain of Natural Language Processing (NLP) and garnering unprecedented attention. As LLM technology continues to progress, the telecom industry is facing the prospect of its potential impact on its landscape. To elucidate these implications, we delve into the inner workings of LLMs, providing insights into their current capabilities and limitations. We also examine the use cases that can be readily implemented in the telecom industry, streamlining numerous tasks that currently hinder operational efficiency and demand significant manpower and engineering expertise. Furthermore, we uncover essential research directions that deal with the distinctive challenges of utilizing the LLMs within the telecom domain. Addressing these challenges represents a significant stride towards fully harnessing the potential of LLMs and unlocking their capabilities to the fullest extent within the telecom domain.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Does-AI-for-science-need-another-ImageNet-Or-totally-different-benchmarks-A-case-study-of-machine-learning-force-fields"><a href="#Does-AI-for-science-need-another-ImageNet-Or-totally-different-benchmarks-A-case-study-of-machine-learning-force-fields" class="headerlink" title="Does AI for science need another ImageNet Or totally different benchmarks? A case study of machine learning force fields"></a>Does AI for science need another ImageNet Or totally different benchmarks? A case study of machine learning force fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05999">http://arxiv.org/abs/2308.05999</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yatao Li, Wanling Gao, Lei Wang, Lixin Sun, Zun Wang, Jianfeng Zhan</li>
<li>for: 这 paper 的目的是探讨 AI for science 领域中的模型性能评估方法，以便更好地适应科学计算任务中的特殊挑战。</li>
<li>methods: 该 paper 使用 machine learning force field (MLFF) 作为一个案例研究，检查了现有的 AI benchmarking 方法是否能够有效地评估 AI for science 模型的性能。它还提出了一些解决方案来评估 MLFF 模型，包括样本效率、时间域敏感性和交叉数据集泛化能力等方面。</li>
<li>results: 该 paper 通过设置问题实例类似于实际科学应用，提出了一些更加科学意义的性能指标，以评估 AI for science 模型的性能。这些指标在实际应用中表现出更高的泛化能力和更好的时间域敏感性，与传统的 AI 评估方法相比。<details>
<summary>Abstract</summary>
AI for science (AI4S) is an emerging research field that aims to enhance the accuracy and speed of scientific computing tasks using machine learning methods. Traditional AI benchmarking methods struggle to adapt to the unique challenges posed by AI4S because they assume data in training, testing, and future real-world queries are independent and identically distributed, while AI4S workloads anticipate out-of-distribution problem instances. This paper investigates the need for a novel approach to effectively benchmark AI for science, using the machine learning force field (MLFF) as a case study. MLFF is a method to accelerate molecular dynamics (MD) simulation with low computational cost and high accuracy. We identify various missed opportunities in scientifically meaningful benchmarking and propose solutions to evaluate MLFF models, specifically in the aspects of sample efficiency, time domain sensitivity, and cross-dataset generalization capabilities. By setting up the problem instantiation similar to the actual scientific applications, more meaningful performance metrics from the benchmark can be achieved. This suite of metrics has demonstrated a better ability to assess a model's performance in real-world scientific applications, in contrast to traditional AI benchmarking methodologies. This work is a component of the SAIBench project, an AI4S benchmarking suite. The project homepage is https://www.computercouncil.org/SAIBench.
</details>
<details>
<summary>摘要</summary>
人工智能 для科学（AI4S）是一个emerging研究领域，旨在使用机器学习方法提高科学计算任务的准确率和速度。传统的AI测试方法困难适应AI4S的特殊挑战，因为它们假设训练、测试和未来实际世界中的数据都是独立并且相同分布的，而AI4S工作负荷预期的问题实例将出现在不同的分布上。这篇论文研究了AI4S测试方法的需要，使用机器学习力场（MLFF）作为一个案例研究。MLFF是一种加速分子动力学（MD）仿真的方法，可以减少计算成本并保持高度准确。我们认为存在多种科学上有意义的测试机会被遗弃，并提出了一些解决方案来评估MLFF模型，包括样本效率、时间域敏感和cross-dataset泛化能力。通过设置问题实例类似于实际科学应用，可以更 meaningful的性能指标从测试中获得。这组指标已经表明可以更好地评估模型在实际科学应用中的性能，与传统的AI测试方法不同。这是SAIBench项目的一部分，SAIBench是一个AI4S测试集。项目主页在https://www.computercouncil.org/SAIBench。
</details></li>
</ul>
<hr>
<h2 id="Automatic-Classification-of-Blood-Cell-Images-Using-Convolutional-Neural-Network"><a href="#Automatic-Classification-of-Blood-Cell-Images-Using-Convolutional-Neural-Network" class="headerlink" title="Automatic Classification of Blood Cell Images Using Convolutional Neural Network"></a>Automatic Classification of Blood Cell Images Using Convolutional Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06300">http://arxiv.org/abs/2308.06300</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rabia Asghar, Sanjay Kumar, Paul Hynds, Abeera Mahfooz</li>
<li>For: The paper aims to automatically classify ten types of blood cells with increased accuracy using a convolutional neural network (CNN) model.* Methods: The authors use transfer learning with pre-trained CNN models, including VGG16, VGG19, ResNet-50, ResNet-101, ResNet-152, InceptionV3, MobileNetV2, and DenseNet-20, on the PBC dataset’s normal DIB. They also propose a novel CNN-based framework to improve accuracy.* Results: The authors achieve an accuracy of 99.91% on the PBC dataset with their proposed CNN model, outperforming earlier results reported in the literature.<details>
<summary>Abstract</summary>
Human blood primarily comprises plasma, red blood cells, white blood cells, and platelets. It plays a vital role in transporting nutrients to different organs, where it stores essential health-related data about the human body. Blood cells are utilized to defend the body against diverse infections, including fungi, viruses, and bacteria. Hence, blood analysis can help physicians assess an individual's physiological condition. Blood cells have been sub-classified into eight groups: Neutrophils, eosinophils, basophils, lymphocytes, monocytes, immature granulocytes (promyelocytes, myelocytes, and metamyelocytes), erythroblasts, and platelets or thrombocytes on the basis of their nucleus, shape, and cytoplasm. Traditionally, pathologists and hematologists in laboratories have examined these blood cells using a microscope before manually classifying them. The manual approach is slower and more prone to human error. Therefore, it is essential to automate this process. In our paper, transfer learning with CNN pre-trained models. VGG16, VGG19, ResNet-50, ResNet-101, ResNet-152, InceptionV3, MobileNetV2, and DenseNet-20 applied to the PBC dataset's normal DIB. The overall accuracy achieved with these models lies between 91.375 and 94.72%. Hence, inspired by these pre-trained architectures, a model has been proposed to automatically classify the ten types of blood cells with increased accuracy. A novel CNN-based framework has been presented to improve accuracy. The proposed CNN model has been tested on the PBC dataset normal DIB. The outcomes of the experiments demonstrate that our CNN-based framework designed for blood cell classification attains an accuracy of 99.91% on the PBC dataset. Our proposed convolutional neural network model performs competitively when compared to earlier results reported in the literature.
</details>
<details>
<summary>摘要</summary>
人体血液主要由血液溶解、红细胞、白细胞和板块组成。它扮演着将营养物质传递到不同器官的重要角色，同时也存储了人体重要的生物学信息。血液细胞可以用于防御体内各种感染，包括病毒、真菌和细菌。因此，血液分析可以帮助医生评估个体的生理状况。血液细胞被分为八种类型：neutrophils、eosinophils、basophils、lymphocytes、monocytes、immature granulocytes（promyelocytes、myelocytes和metamyelocytes）、erythroblasts和板块或血液板块。传统上，pathologists和hematologists在实验室中使用显微镜进行血液细胞的识别，这是一个慢速且容易出错的手动过程。因此，自动化这个过程是非常重要。在我们的论文中，我们采用了转移学习与CNN预训练模型。VGG16、VGG19、ResNet-50、ResNet-101、ResNet-152、InceptionV3、MobileNetV2和DenseNet-20在PBC数据集的正常DIB上应用了CNN预训练模型。这些模型的总准确率在91.375%到94.72%之间。因此，我们被这些预训练模型所 inspirited，并提出了一种自动化血液细胞类型分类的模型。我们提出了一种基于CNN的框架来提高准确率。我们的提议的CNN模型在PBC数据集的正常DIB上进行测试，实验结果表明，我们的CNN模型在血液细胞类型分类方面实现了99.91%的准确率。我们的提议的 convolutional neural network模型与文献中已经报道的结果相比，表现竞争力强。
</details></li>
</ul>
<hr>
<h2 id="Fast-and-Accurate-Transferability-Measurement-by-Evaluating-Intra-class-Feature-Variance"><a href="#Fast-and-Accurate-Transferability-Measurement-by-Evaluating-Intra-class-Feature-Variance" class="headerlink" title="Fast and Accurate Transferability Measurement by Evaluating Intra-class Feature Variance"></a>Fast and Accurate Transferability Measurement by Evaluating Intra-class Feature Variance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05986">http://arxiv.org/abs/2308.05986</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/snudatalab/TMI">https://github.com/snudatalab/TMI</a></li>
<li>paper_authors: Huiwen Xu, U Kang</li>
<li>for: 这个论文的目的是如何快速和准确地找到下游任务中最有用的预训练模型。</li>
<li>methods: 这个论文提出了一种名为TMI（转移性评估器）的算法，用于评估预训练模型的转移性。TMI视转移性为预训练模型在目标任务上的总体化，并通过评估模型内类差异来评估模型的适应性。</li>
<li>results: 对于多个实际数据集，TMI表现出了较好的选择性，可以快速和准确地选择预训练模型。与之前的研究相比，TMI在13个案例中展现出了更高的相关性。<details>
<summary>Abstract</summary>
Given a set of pre-trained models, how can we quickly and accurately find the most useful pre-trained model for a downstream task? Transferability measurement is to quantify how transferable is a pre-trained model learned on a source task to a target task. It is used for quickly ranking pre-trained models for a given task and thus becomes a crucial step for transfer learning. Existing methods measure transferability as the discrimination ability of a source model for a target data before transfer learning, which cannot accurately estimate the fine-tuning performance. Some of them restrict the application of transferability measurement in selecting the best supervised pre-trained models that have classifiers. It is important to have a general method for measuring transferability that can be applied in a variety of situations, such as selecting the best self-supervised pre-trained models that do not have classifiers, and selecting the best transferring layer for a target task. In this work, we propose TMI (TRANSFERABILITY MEASUREMENT WITH INTRA-CLASS FEATURE VARIANCE), a fast and accurate algorithm to measure transferability. We view transferability as the generalization of a pre-trained model on a target task by measuring intra-class feature variance. Intra-class variance evaluates the adaptability of the model to a new task, which measures how transferable the model is. Compared to previous studies that estimate how discriminative the models are, intra-class variance is more accurate than those as it does not require an optimal feature extractor and classifier. Extensive experiments on real-world datasets show that TMI outperforms competitors for selecting the top-5 best models, and exhibits consistently better correlation in 13 out of 17 cases.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese translation:给定一个集合先进模型，如何快速和准确地找到下游任务中最有用的先进模型？转移可量度是用于衡量先进模型在源任务上学习后，转移到目标任务上的抽象能力。现有的方法通常是通过计算源模型对目标数据的分类能力来衡量转移可量度，这不能准确地估计微调性能。一些方法还限制了转移可量度的测量在选择最佳监督式先进模型中应用。因此，有一个通用的方法可以在多种情况下测量转移可量度，如选择最佳无监督式先进模型和选择最佳转移层。在这项工作中，我们提出了TMI（转移可量度测量与内类特征异常）算法，它是一种快速和准确的转移可量度测量方法。我们视转移可量度为将先进模型在目标任务上通过测量内类特征异常来衡量。内类异常评估模型在新任务上适应度，这也衡量了模型的转移可量度。与之前的研究所计算的模型掌握性相比，内类异常更准确，因为它不需要优化特征提取器和分类器。我们在实际世界数据集上进行了广泛的实验，显示TMI在选择top-5最佳模型时高效，并在13个 случа中展现了更高的相关性。
</details></li>
</ul>
<hr>
<h2 id="Defensive-Perception-Estimation-and-Monitoring-of-Neural-Network-Performance-under-Deployment"><a href="#Defensive-Perception-Estimation-and-Monitoring-of-Neural-Network-Performance-under-Deployment" class="headerlink" title="Defensive Perception: Estimation and Monitoring of Neural Network Performance under Deployment"></a>Defensive Perception: Estimation and Monitoring of Neural Network Performance under Deployment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06299">http://arxiv.org/abs/2308.06299</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hendrik Vogt, Stefan Buehler, Mark Schutera</li>
<li>for: addressing the issue of unnoticed catastrophic deployment and domain shift in neural networks for semantic segmentation in autonomous driving</li>
<li>methods: encapsulating the neural network under deployment within an uncertainty estimation envelope based on Monte Carlo Dropout, without modifying the deployed neural network</li>
<li>results: demonstrating the applicability of the method for multiple different potential deployment shifts relevant to autonomous driving, including transitions into the night, rainy, or snowy domain, and enabling operational design domain recognition via uncertainty, which allows for defensive perception, safe state triggers, warning notifications, and feedback for testing or development and adaptation of the perception stack.Here’s the same information in Simplified Chinese:</li>
<li>for: 解决神经网络 semantic segmentation 自动驾驶中的不可预测性和域shift问题</li>
<li>methods: 基于 Monte Carlo Dropout 的 epistemic uncertainty 估计方法，不需要修改部署 neural network</li>
<li>results: 对各种自动驾驶中可能的部署变化进行了演示，包括夜晚、雨天和雪天等域shift，并实现了运行设计域认知via uncertainty，允许DEFENSIVE PERCEPTION、安全状态触发、警告通知和测试或开发和适应性改进的感知栈反馈。<details>
<summary>Abstract</summary>
In this paper, we propose a method for addressing the issue of unnoticed catastrophic deployment and domain shift in neural networks for semantic segmentation in autonomous driving. Our approach is based on the idea that deep learning-based perception for autonomous driving is uncertain and best represented as a probability distribution. As autonomous vehicles' safety is paramount, it is crucial for perception systems to recognize when the vehicle is leaving its operational design domain, anticipate hazardous uncertainty, and reduce the performance of the perception system. To address this, we propose to encapsulate the neural network under deployment within an uncertainty estimation envelope that is based on the epistemic uncertainty estimation through the Monte Carlo Dropout approach. This approach does not require modification of the deployed neural network and guarantees expected model performance. Our defensive perception envelope has the capability to estimate a neural network's performance, enabling monitoring and notification of entering domains of reduced neural network performance under deployment. Furthermore, our envelope is extended by novel methods to improve the application in deployment settings, including reducing compute expenses and confining estimation noise. Finally, we demonstrate the applicability of our method for multiple different potential deployment shifts relevant to autonomous driving, such as transitions into the night, rainy, or snowy domain. Overall, our approach shows great potential for application in deployment settings and enables operational design domain recognition via uncertainty, which allows for defensive perception, safe state triggers, warning notifications, and feedback for testing or development and adaptation of the perception stack.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种方法来解决自适应驾驶 neural network 中的不注意性投入和领域转换问题。我们的方法基于深度学习基于自适应驾驶的感知系统是不确定的，最好表示为一个概率分布。自驾驶车辆的安全性 Paramount，因此感知系统必须能够识别车辆离开操作设计领域，预测危险不确定性，并降低感知系统的性能。为此，我们提议将投入 neural network 内部的深度学习模型包装在一个不确定性估计膜中，该膜基于 Monte Carlo Dropout 方法来估计模型的 epistemic 不确定性。这种方法不需要修改已经部署的 neural network，并且保证模型的预期性能。我们的防御感知膜可以估计 neural network 的性能，并且可以监测和通知车辆进入性能下降的领域。此外，我们还提出了一些新的方法来改进在部署Setting中的应用，包括减少计算成本和限制估计噪声。最后，我们示出了我们方法在多种不同的部署转换中的应用可能性，例如在夜晚、雨天或雪天等领域。总之，我们的方法在部署Setting中表现出了很好的应用潜力，并允许操作设计领域的认知，以及发出警告通知、测试或开发和适应感知堆。
</details></li>
</ul>
<hr>
<h2 id="An-Encoder-Decoder-Approach-for-Packing-Circles"><a href="#An-Encoder-Decoder-Approach-for-Packing-Circles" class="headerlink" title="An Encoder-Decoder Approach for Packing Circles"></a>An Encoder-Decoder Approach for Packing Circles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07335">http://arxiv.org/abs/2308.07335</a></li>
<li>repo_url: None</li>
<li>paper_authors: Akshay Kiran Jose, Gangadhar Karevvanavar, Rajshekhar V Bhat</li>
<li>for: 本文关于如何封装小对象在大对象中，以实现不 overlap 和 minimum overlap 的目标。</li>
<li>methods: 本文提出了一种基于encoder-decoder架构的方法，包括encoder块、perturbation块和decoder块。encoder块通过normalization层输出中心点，perturbation块添加控制的偏移，确保中心点不超过小对象的半径，decoder块使用偏移中心点来估计 intend circle index。</li>
<li>results: 该方法可以 Parametrize encoder和decoder使用神经网络，并通过优化减少 decoder 估计的误差和实际输入 encoder 中心点的差异，从而实现不 overlap 和 minimum overlap 的目标。该方法可以对高维度和不同形状的对象进行扩展。<details>
<summary>Abstract</summary>
The problem of packing smaller objects within a larger object has been of interest since decades. In these problems, in addition to the requirement that the smaller objects must lie completely inside the larger objects, they are expected to not overlap or have minimum overlap with each other. Due to this, the problem of packing turns out to be a non-convex problem, obtaining whose optimal solution is challenging. As such, several heuristic approaches have been used for obtaining sub-optimal solutions in general, and provably optimal solutions for some special instances. In this paper, we propose a novel encoder-decoder architecture consisting of an encoder block, a perturbation block and a decoder block, for packing identical circles within a larger circle. In our approach, the encoder takes the index of a circle to be packed as an input and outputs its center through a normalization layer, the perturbation layer adds controlled perturbations to the center, ensuring that it does not deviate beyond the radius of the smaller circle to be packed, and the decoder takes the perturbed center as input and estimates the index of the intended circle for packing. We parameterize the encoder and decoder by a neural network and optimize it to reduce an error between the decoder's estimated index and the actual index of the circle provided as input to the encoder. The proposed approach can be generalized to pack objects of higher dimensions and different shapes by carefully choosing normalization and perturbation layers. The approach gives a sub-optimal solution and is able to pack smaller objects within a larger object with competitive performance with respect to classical methods.
</details>
<details>
<summary>摘要</summary>
这个问题已经引起关注了几十年。在这些问题中，除了要求小对象完全 locate 在大对象中之外，还要求小对象之间不会 overlap 或者最小化 overlap。由于这个原因，packing 问题变成了非凸问题，获得优化解决方案是困难的。为此，许多启发性方法被用来获得不优化解决方案，以及对特殊情况下的可证优化解决方案。在这篇论文中，我们提出了一种新的编码器-解码器架构，包括编码器块、抖动块和解码器块，用于将同形圆包含在大圆中。在我们的方法中，编码器接受圆的索引作为输入，并通过正规化层输出圆心，抖动层添加控制的偏移，使圆心不会超过小圆的半径，而解码器接受偏移后的圆心作为输入，并估算圆的索引。我们使用神经网络参数化编码器和解码器，并优化它们以降低由解码器估算的圆索引与实际输入圆索引之间的错误。我们的方法可以通过选择正规化和抖动层来扩展到包含高维度和不同形状的对象。该方法可以提供竞争性的非优化解决方案，并将小对象包含在大对象中。
</details></li>
</ul>
<hr>
<h2 id="Learning-nonparametric-DAGs-with-incremental-information-via-high-order-HSIC"><a href="#Learning-nonparametric-DAGs-with-incremental-information-via-high-order-HSIC" class="headerlink" title="Learning nonparametric DAGs with incremental information via high-order HSIC"></a>Learning nonparametric DAGs with incremental information via high-order HSIC</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05969">http://arxiv.org/abs/2308.05969</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yafei Wang, Jianguo Liu</li>
<li>for: 本文 targets at learning Bayesian networks (BN) by maximizing global score functions, but it addresses the issue of local variables having direct and indirect dependence simultaneously, which can lead to missed edges in the global optimization.</li>
<li>methods: 本文提出了一个可 identificability condition based on a determined subset of parents，并开发了一个两阶段算法（OT algorithm）来解决本问题。在第一阶段，使用 first-order Hilbert-Schmidt independence criterion (HSIC) 得到一个初始确定的父集。在第二阶段，使用 theoretically proved incremental properties of high-order HSIC 进行了本地调整。</li>
<li>results:  numrical experiments on different synthetic datasets and real-world datasets show that the OT algorithm outperforms existing methods, especially in Sigmoid Mix model with the size of the graph being d&#x3D;40, the structure intervention distance (SID) of the OT algorithm is 329.7 smaller than the one obtained by CAM, indicating that the graph estimated by the OT algorithm misses fewer edges compared with CAM.<details>
<summary>Abstract</summary>
Score-based methods for learning Bayesain networks(BN) aim to maximizing the global score functions. However, if local variables have direct and indirect dependence simultaneously, the global optimization on score functions misses edges between variables with indirect dependent relationship, of which scores are smaller than those with direct dependent relationship. In this paper, we present an identifiability condition based on a determined subset of parents to identify the underlying DAG. By the identifiability condition, we develop a two-phase algorithm namely optimal-tuning (OT) algorithm to locally amend the global optimization. In the optimal phase, an optimization problem based on first-order Hilbert-Schmidt independence criterion (HSIC) gives an estimated skeleton as the initial determined parents subset. In the tuning phase, the skeleton is locally tuned by deletion, addition and DAG-formalization strategies using the theoretically proved incremental properties of high-order HSIC. Numerical experiments for different synthetic datasets and real-world datasets show that the OT algorithm outperforms existing methods. Especially in Sigmoid Mix model with the size of the graph being ${\rm\bf d=40}$, the structure intervention distance (SID) of the OT algorithm is 329.7 smaller than the one obtained by CAM, which indicates that the graph estimated by the OT algorithm misses fewer edges compared with CAM.
</details>
<details>
<summary>摘要</summary>
Score-based方法学习 bayesian网络（BN）目的是最大化全局分数函数。然而，如果本地变量同时具有直接和间接依赖关系，全局优化分数函数会忽略变量之间的间接依赖关系中的边，其分数较直接依赖关系中的边小。在这篇论文中，我们提出了一个可识别条件，基于确定的父集来识别下面的DAG。通过可识别条件，我们开发了一个两阶段算法，称为最优调整（OT）算法。在优化阶段，基于第一阶段希尔伯特- Schmidt独立性标准（HSIC）的优化问题提供了一个初始确定父集的skeleton。在调整阶段，skeleton通过删除、添加和DAG-形式化策略进行了本地调整，使用了理论上证明的高阶HSIC的增量性质。numerical experiments表明，OT算法在不同的synthetic数据集和实际数据集上的性能都高于现有方法。特别是在sigmoid mix模型中，OT算法的结构间断距（SID）为329.7，与CAM所获得的结构间断距相比，表示OT算法估算的图 missed fewer edges。
</details></li>
</ul>
<hr>
<h2 id="Classification-of-White-Blood-Cells-Using-Machine-and-Deep-Learning-Models-A-Systematic-Review"><a href="#Classification-of-White-Blood-Cells-Using-Machine-and-Deep-Learning-Models-A-Systematic-Review" class="headerlink" title="Classification of White Blood Cells Using Machine and Deep Learning Models: A Systematic Review"></a>Classification of White Blood Cells Using Machine and Deep Learning Models: A Systematic Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06296">http://arxiv.org/abs/2308.06296</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rabia Asghar, Sanjay Kumar, Paul Hynds, Arslan Shaukat</li>
<li>for: 这篇论文的目的是对医疗影像分析中的白血球分类进行深入分析，并评估现代技术在这个领域的应用。</li>
<li>methods: 这篇论文使用了许多现代技术，包括机器学习（ML）和深度学习（DL），以提高医疗影像分析的准确性和分类精度。</li>
<li>results: 这篇论文发现，过去的17年间，医疗影像分析中的白血球分类方法有所进步，并且使用了许多不同的技术和数据来进行分析。但是，还有一些挑战需要解决，例如获得适当的数据集和增强医疗培训。<details>
<summary>Abstract</summary>
Machine learning (ML) and deep learning (DL) models have been employed to significantly improve analyses of medical imagery, with these approaches used to enhance the accuracy of prediction and classification. Model predictions and classifications assist diagnoses of various cancers and tumors. This review presents an in-depth analysis of modern techniques applied within the domain of medical image analysis for white blood cell classification. The methodologies that use blood smear images, magnetic resonance imaging (MRI), X-rays, and similar medical imaging domains are identified and discussed, with a detailed analysis of ML/DL techniques applied to the classification of white blood cells (WBCs) representing the primary focus of the review. The data utilized in this research has been extracted from a collection of 136 primary papers that were published between the years 2006 and 2023. The most widely used techniques and best-performing white blood cell classification methods are identified. While the use of ML and DL for white blood cell classification has concurrently increased and improved in recent year, significant challenges remain - 1) Availability of appropriate datasets remain the primary challenge, and may be resolved using data augmentation techniques. 2) Medical training of researchers is recommended to improve current understanding of white blood cell structure and subsequent selection of appropriate classification models. 3) Advanced DL networks including Generative Adversarial Networks, R-CNN, Fast R-CNN, and faster R-CNN will likely be increasingly employed to supplement or replace current techniques.
</details>
<details>
<summary>摘要</summary>
医学影像分析（ML）和深度学习（DL）模型已经被应用到医疗影像分析中，以提高预测和分类的准确性。这些方法可以帮助诊断多种恶性肿瘤和癌症。本文总结了现代医学影像分析领域中使用ML/DL技术进行白血球类型分类的方法。这些方法包括血液滴血图像、核磁共振成像（MRI）、X射线成像等医学影像领域，并进行了详细的ML/DL技术应用于白血球类型分类的分析。研究使用的数据来自于2006年至2023年发表的136篇原始论文。最常用的技术和最佳白血球类型分类方法被识别出来。虽然在过去几年内，用ML和DL进行白血球类型分类的使用和提高在不断增长，但还存在一些挑战，包括：1）获得适当数据集的可用性问题，可以通过数据扩展技术解决。2）医学研究人员的培训，以提高白血球结构的理解，并选择合适的分类模型。3）将来，高级的深度学习网络，如生成对抗网络、R-CNN、快速R-CNN和更快的R-CNN将被广泛应用，以补充或取代当前的方法。
</details></li>
</ul>
<hr>
<h2 id="Learned-Point-Cloud-Compression-for-Classification"><a href="#Learned-Point-Cloud-Compression-for-Classification" class="headerlink" title="Learned Point Cloud Compression for Classification"></a>Learned Point Cloud Compression for Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05959">http://arxiv.org/abs/2308.05959</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/multimedialabsfu/learned-point-cloud-compression-for-classification">https://github.com/multimedialabsfu/learned-point-cloud-compression-for-classification</a></li>
<li>paper_authors: Mateen Ulhaq, Ivan V. Bajić</li>
<li>for: 本研究旨在提出一种特种的点云编码器，用于在服务器端进行机器分析 tasks 的点云数据传输。</li>
<li>methods: 我们基于 PointNet 提出了一种特种的点云编码器，实现了与非特种编码器相比significantly better的Rate-Accuracy 质量比。</li>
<li>results: 我们的编码器在 ModelNet40 数据集上实现了94%的BD-比特率减少，而且对于低资源的终端设备，我们还提出了两种轻量级的编码器配置，可以实现相似的BD-比特率减少（93%和92%），同时只消耗0.470和0.048 encoder-side kMACs&#x2F;点。<details>
<summary>Abstract</summary>
Deep learning is increasingly being used to perform machine vision tasks such as classification, object detection, and segmentation on 3D point cloud data. However, deep learning inference is computationally expensive. The limited computational capabilities of end devices thus necessitate a codec for transmitting point cloud data over the network for server-side processing. Such a codec must be lightweight and capable of achieving high compression ratios without sacrificing accuracy. Motivated by this, we present a novel point cloud codec that is highly specialized for the machine task of classification. Our codec, based on PointNet, achieves a significantly better rate-accuracy trade-off in comparison to alternative methods. In particular, it achieves a 94% reduction in BD-bitrate over non-specialized codecs on the ModelNet40 dataset. For low-resource end devices, we also propose two lightweight configurations of our encoder that achieve similar BD-bitrate reductions of 93% and 92% with 3% and 5% drops in top-1 accuracy, while consuming only 0.470 and 0.048 encoder-side kMACs/point, respectively. Our codec demonstrates the potential of specialized codecs for machine analysis of point clouds, and provides a basis for extension to more complex tasks and datasets in the future.
</details>
<details>
<summary>摘要</summary>
深度学习在处理3D点云数据上进行机器视觉任务，如分类、物体检测和分割，日益受到欢迎。然而，深度学习推理过程具有计算成本高的问题，因此在终端设备上进行处理时需要一个点云编码器。这个编码器应该轻量级，能够实现高度压缩比，而无需牺牲准确性。为了解决这个问题，我们提出了一种特种的点云编码器，基于PointNet，可以在机器分类任务中实现显著更好的比例-准确性质量。具体来说，我们的编码器在ModelNet40数据集上实现了94%的BD-比特率减少，相比非特种编码器。而为了适应低资源的终端设备，我们还提出了两种轻量级的编码器配置，它们可以实现类似的BD-比特率减少，分别为93%和92%，但是消耗了0.470和0.048个encoder-side kMACs/点。我们的编码器表明特种编码器在机器分析点云数据时具有潜在的优势，并为未来扩展到更复杂的任务和数据集提供了基础。
</details></li>
</ul>
<hr>
<h2 id="Node-Embedding-for-Homophilous-Graphs-with-ARGEW-Augmentation-of-Random-walks-by-Graph-Edge-Weights"><a href="#Node-Embedding-for-Homophilous-Graphs-with-ARGEW-Augmentation-of-Random-walks-by-Graph-Edge-Weights" class="headerlink" title="Node Embedding for Homophilous Graphs with ARGEW: Augmentation of Random walks by Graph Edge Weights"></a>Node Embedding for Homophilous Graphs with ARGEW: Augmentation of Random walks by Graph Edge Weights</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05957">http://arxiv.org/abs/2308.05957</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ncsoft/argew">https://github.com/ncsoft/argew</a></li>
<li>paper_authors: Jun Hee Kim, Jaeman Son, Hyunsoo Kim, Eunjo Lee</li>
<li>for: 本文是针对 dense vector  represent nodes in network 的研究，尤其是Weighted homophilous graphs中 node pairs with stronger edges weights 应该有更加相近的 embedding。</li>
<li>methods: 本文提出了 ARGEW（Augmentation of Random walks by Graph Edge Weights），一种基于随机漫步的增强方法，可以使得 node embeddings 更加准确地反映 edge weights。</li>
<li>results: 在多个实际网络上，ARGEW 可以使得 node pairs with larger edge weights 有更加相近的 embedding，并且在 node classification 任务中，ARGEW 可以提高 node2vec 的性能，并且不受 hyperparameters 的影响。<details>
<summary>Abstract</summary>
Representing nodes in a network as dense vectors node embeddings is important for understanding a given network and solving many downstream tasks. In particular, for weighted homophilous graphs where similar nodes are connected with larger edge weights, we desire node embeddings where node pairs with strong weights have closer embeddings. Although random walk based node embedding methods like node2vec and node2vec+ do work for weighted networks via including edge weights in the walk transition probabilities, our experiments show that the embedding result does not adequately reflect edge weights. In this paper, we propose ARGEW (Augmentation of Random walks by Graph Edge Weights), a novel augmentation method for random walks that expands the corpus in such a way that nodes with larger edge weights end up with closer embeddings. ARGEW can work with any random walk based node embedding method, because it is independent of the random sampling strategy itself and works on top of the already-performed walks. With several real-world networks, we demonstrate that with ARGEW, compared to not using it, the desired pattern that node pairs with larger edge weights have closer embeddings is much clearer. We also examine ARGEW's performance in node classification: node2vec with ARGEW outperforms pure node2vec and is not sensitive to hyperparameters (i.e. consistently good). In fact, it achieves similarly good results as supervised GCN, even without any node feature or label information during training. Finally, we explain why ARGEW works consistently well by exploring the coappearance distributions using a synthetic graph with clear structural roles.
</details>
<details>
<summary>摘要</summary>
importance of representing nodes in a network as dense vectors (node embeddings) for understanding the network and solving downstream tasks. In particular, for weighted homophilous graphs where similar nodes are connected with larger edge weights, we desire node embeddings where node pairs with strong weights have closer embeddings. Although random walk based node embedding methods like node2vec and node2vec+ can work for weighted networks by including edge weights in the walk transition probabilities, our experiments show that the embedding result does not adequately reflect edge weights. In this paper, we propose ARGEW (Augmentation of Random walks by Graph Edge Weights), a novel augmentation method for random walks that expands the corpus in such a way that nodes with larger edge weights end up with closer embeddings. ARGEW can work with any random walk based node embedding method, because it is independent of the random sampling strategy itself and works on top of the already-performed walks. With several real-world networks, we demonstrate that with ARGEW, compared to not using it, the desired pattern that node pairs with larger edge weights have closer embeddings is much clearer. We also examine ARGEW's performance in node classification: node2vec with ARGEW outperforms pure node2vec and is not sensitive to hyperparameters (i.e. consistently good). In fact, it achieves similarly good results as supervised GCN, even without any node feature or label information during training. Finally, we explain why ARGEW works consistently well by exploring the coappearance distributions using a synthetic graph with clear structural roles.
</details></li>
</ul>
<hr>
<h2 id="INR-Arch-A-Dataflow-Architecture-and-Compiler-for-Arbitrary-Order-Gradient-Computations-in-Implicit-Neural-Representation-Processing"><a href="#INR-Arch-A-Dataflow-Architecture-and-Compiler-for-Arbitrary-Order-Gradient-Computations-in-Implicit-Neural-Representation-Processing" class="headerlink" title="INR-Arch: A Dataflow Architecture and Compiler for Arbitrary-Order Gradient Computations in Implicit Neural Representation Processing"></a>INR-Arch: A Dataflow Architecture and Compiler for Arbitrary-Order Gradient Computations in Implicit Neural Representation Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05930">http://arxiv.org/abs/2308.05930</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefan Abi-Karam, Rishov Sarkar, Dejia Xu, Zhiwen Fan, Zhangyang Wang, Cong Hao</li>
<li>for: 这个论文主要用于探讨nth-order gradient计算在图形学、元学习（MAML）、科学计算和最近的隐藏神经表示（INR）中的应用。</li>
<li>methods: 这个论文使用了一种叫做INR-Arch的框架，它可以将计算图的nth-order gradient转换成一个硬件优化的数据流体系结构。这个框架包括两个阶段：首先，设计了一个高效的数据流体系结构，其中使用了FIFO流和优化的计算kernels库，以确保高效的内存利用和并行计算。其次，提出了一种编译器，它可以自动从计算图中提取和优化计算，并配置硬件参数 such as 延迟和流深度以优化吞吐量，保证不会出现死锁现象，并生成高级合成（HLS）代码 дляFPGA实现。</li>
<li>results: 这个论文通过对INR编辑作为测试样本，实现了对CPU和GPU基线的1.8-4.8倍和1.5-3.6倍的速度提升，同时也实现了对内存使用的3.1-8.9倍和1.7-4.3倍的减少，以及对能效率的1.7-11.3倍和5.5-32.8倍的下降。<details>
<summary>Abstract</summary>
An increasing number of researchers are finding use for nth-order gradient computations for a wide variety of applications, including graphics, meta-learning (MAML), scientific computing, and most recently, implicit neural representations (INRs). Recent work shows that the gradient of an INR can be used to edit the data it represents directly without needing to convert it back to a discrete representation. However, given a function represented as a computation graph, traditional architectures face challenges in efficiently computing its nth-order gradient due to the higher demand for computing power and higher complexity in data movement. This makes it a promising target for FPGA acceleration. In this work, we introduce INR-Arch, a framework that transforms the computation graph of an nth-order gradient into a hardware-optimized dataflow architecture. We address this problem in two phases. First, we design a dataflow architecture that uses FIFO streams and an optimized computation kernel library, ensuring high memory efficiency and parallel computation. Second, we propose a compiler that extracts and optimizes computation graphs, automatically configures hardware parameters such as latency and stream depths to optimize throughput, while ensuring deadlock-free operation, and outputs High-Level Synthesis (HLS) code for FPGA implementation. We utilize INR editing as our benchmark, presenting results that demonstrate 1.8-4.8x and 1.5-3.6x speedup compared to CPU and GPU baselines respectively. Furthermore, we obtain 3.1-8.9x and 1.7-4.3x lower memory usage, and 1.7-11.3x and 5.5-32.8x lower energy-delay product. Our framework will be made open-source and available on GitHub.
</details>
<details>
<summary>摘要</summary>
更多研究人员正在发现使用 nth-order Gradient 计算在各种应用程序中，包括图形学、多学习（MAML）、科学计算和最近的隐藏神经表示（INRs）。最新的研究表明，INR 的 Gradient 可以直接编辑所表示的数据，而无需将其转换回分割表示。然而，传统架构在计算 Graph 中的 nth-order Gradient 计算时面临着更高的计算能力和数据移动复杂性的挑战，这使得它成为了可 acceleration 的目标。在这项工作中，我们介绍 INR-Arch，一个将计算 Graph 转换为硬件优化数据流架构的框架。我们解决这个问题在两个阶段。首先，我们设计了一个数据流架构，使用 FIFO 流和优化的计算内核库，保证高内存效率和并行计算。其次，我们提出了一个编译器，可以提取和优化计算 Graph，自动配置硬件参数 such as 延迟和流深度，以优化通过put，并确保不会出现堵塞的操作。我们使用 INR 编辑作为我们的标准，发表了结果，表明在 CPU 和 GPU 基线相比，得到了 1.8-4.8 倍和 1.5-3.6 倍的速度提升。此外，我们获得了 3.1-8.9 倍和 1.7-4.3 倍的内存使用量减少，以及 1.7-11.3 倍和 5.5-32.8 倍的能量延迟产品。我们的框架即将被开源，并在 GitHub 上发布。
</details></li>
</ul>
<hr>
<h2 id="On-the-equivalence-of-Occam-algorithms"><a href="#On-the-equivalence-of-Occam-algorithms" class="headerlink" title="On the equivalence of Occam algorithms"></a>On the equivalence of Occam algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05906">http://arxiv.org/abs/2308.05906</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zaman Keinath-Esmail</li>
<li>for: 本文提供了一个后验正则化的基础，即任何可学习的概念类都可以由Occam算法学习。</li>
<li>methods: 本文使用了Board和Pitt（1990）提出的一种受到例外列表的影响的Occam算法，并证明了这种算法可以学习任何可学习的概念类。</li>
<li>results: 本文证明了Board和Pitt（1990）的一种受到例外列表的影响的Occam算法可以学习任何可学习的概念类，并且这种算法的复杂度是$\delta$-无关的。<details>
<summary>Abstract</summary>
Blumer et al. (1987, 1989) showed that any concept class that is learnable by Occam algorithms is PAC learnable. Board and Pitt (1990) showed a partial converse of this theorem: for concept classes that are closed under exception lists, any class that is PAC learnable is learnable by an Occam algorithm. However, their Occam algorithm outputs a hypothesis whose complexity is $\delta$-dependent, which is an important limitation. In this paper, we show that their partial converse applies to Occam algorithms with $\delta$-independent complexities as well. Thus, we provide a posteriori justification of various theoretical results and algorithm design methods which use the partial converse as a basis for their work.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Comparing-the-quality-of-neural-network-uncertainty-estimates-for-classification-problems"><a href="#Comparing-the-quality-of-neural-network-uncertainty-estimates-for-classification-problems" class="headerlink" title="Comparing the quality of neural network uncertainty estimates for classification problems"></a>Comparing the quality of neural network uncertainty estimates for classification problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05903">http://arxiv.org/abs/2308.05903</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Ries, Joshua Michalenko, Tyler Ganter, Rashad Imad-Fayez Baiyasi, Jason Adams<br>for:这个论文的目的是评估深度学习模型中的不确定性评估方法的质量。methods:这个论文使用了统计方法来评估信任区间的覆盖率和信任间隔，以及预测分类预测信任的均值误差来评估深度学习模型的不确定性评估方法。results:研究发现，不同的不确定性评估方法在同一个数据集上可以生成不同的结果，而且这些结果之间存在差异。此外，研究还发现，使用MCMC和VI方法可以获得更好的不确定性评估结果，而使用DE和MC dropout方法的结果则更为不稳定。<details>
<summary>Abstract</summary>
Traditional deep learning (DL) models are powerful classifiers, but many approaches do not provide uncertainties for their estimates. Uncertainty quantification (UQ) methods for DL models have received increased attention in the literature due to their usefulness in decision making, particularly for high-consequence decisions. However, there has been little research done on how to evaluate the quality of such methods. We use statistical methods of frequentist interval coverage and interval width to evaluate the quality of credible intervals, and expected calibration error to evaluate classification predicted confidence. These metrics are evaluated on Bayesian neural networks (BNN) fit using Markov Chain Monte Carlo (MCMC) and variational inference (VI), bootstrapped neural networks (NN), Deep Ensembles (DE), and Monte Carlo (MC) dropout. We apply these different UQ for DL methods to a hyperspectral image target detection problem and show the inconsistency of the different methods' results and the necessity of a UQ quality metric. To reconcile these differences and choose a UQ method that appropriately quantifies the uncertainty, we create a simulated data set with fully parameterized probability distribution for a two-class classification problem. The gold standard MCMC performs the best overall, and the bootstrapped NN is a close second, requiring the same computational expense as DE. Through this comparison, we demonstrate that, for a given data set, different models can produce uncertainty estimates of markedly different quality. This in turn points to a great need for principled assessment methods of UQ quality in DL applications.
</details>
<details>
<summary>摘要</summary>
传统的深度学习（DL）模型是强大的分类器，但许多方法不提供不确定性的估计。不确定性量化（UQ）方法 для DL 模型在文献中收到了更多的关注，因为它们在决策中非常有用，特别是对高 conseqüência 的决策。然而，对 UQ 方法评价的研究相对较少。我们使用统计方法的频率interval coverage和interval width来评价credible interval的质量，以及预期准确性error来评价分类预测的自信度。这些指标在bayesian neural network（BNN）适用markov chain Monte Carlo（MCMC）和variational inference（VI）、bootstrapped neural network（NN）、deep ensembles（DE）和Monte Carlo（MC）dropout中被评价。我们对这些不同的 UQ 方法应用到一个 hyperspectral image target detection问题，并显示了不同方法的结果之间的不一致，以及需要一个 UQ 质量指标。为了解决这些不一致并选择一个正确地量化不确定性的 UQ 方法，我们创建了一个完全参数化的概率分布的 simulated data set，用于两类分类问题。金标准 MCMC 表现最佳，而 bootstrapped NN 紧随其后，需要与 DE 相同的计算成本。通过这种比较，我们证明了，对于给定的数据集，不同的模型可以生成不同质量的不确定性估计。这一点点到了深度学习应用中需要原则性评价 UQ 质量的强需求。
</details></li>
</ul>
<hr>
<h2 id="Target-Detection-on-Hyperspectral-Images-Using-MCMC-and-VI-Trained-Bayesian-Neural-Networks"><a href="#Target-Detection-on-Hyperspectral-Images-Using-MCMC-and-VI-Trained-Bayesian-Neural-Networks" class="headerlink" title="Target Detection on Hyperspectral Images Using MCMC and VI Trained Bayesian Neural Networks"></a>Target Detection on Hyperspectral Images Using MCMC and VI Trained Bayesian Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06293">http://arxiv.org/abs/2308.06293</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Ries, Jason Adams, Joshua Zollweg</li>
<li>for: 这篇论文的目的是提出了一种 bayesian neural network (BNN) 的应用，以便在图像分类 tasks 中提供 uncertainty quantification (UQ)。</li>
<li>methods: 这篇论文使用了 MCMC 和 VI 两种不同的训练方法，以评估 BNN 的训练效果。</li>
<li>results: 研究发现，MCMC 和 VI 两种训练方法都可以达到良好的检测效果，但是 VI 方法的计算速度较快。此外，研究还发现，不同的训练方法可能会导致不同的模型结果，特别是在高风险应用中。<details>
<summary>Abstract</summary>
Neural networks (NN) have become almost ubiquitous with image classification, but in their standard form produce point estimates, with no measure of confidence. Bayesian neural networks (BNN) provide uncertainty quantification (UQ) for NN predictions and estimates through the posterior distribution. As NN are applied in more high-consequence applications, UQ is becoming a requirement. BNN provide a solution to this problem by not only giving accurate predictions and estimates, but also an interval that includes reasonable values within a desired probability. Despite their positive attributes, BNN are notoriously difficult and time consuming to train. Traditional Bayesian methods use Markov Chain Monte Carlo (MCMC), but this is often brushed aside as being too slow. The most common method is variational inference (VI) due to its fast computation, but there are multiple concerns with its efficacy. We apply and compare MCMC- and VI-trained BNN in the context of target detection in hyperspectral imagery (HSI), where materials of interest can be identified by their unique spectral signature. This is a challenging field, due to the numerous permuting effects practical collection of HSI has on measured spectra. Both models are trained using out-of-the-box tools on a high fidelity HSI target detection scene. Both MCMC- and VI-trained BNN perform well overall at target detection on a simulated HSI scene. This paper provides an example of how to utilize the benefits of UQ, but also to increase awareness that different training methods can give different results for the same model. If sufficient computational resources are available, the best approach rather than the fastest or most efficient should be used, especially for high consequence problems.
</details>
<details>
<summary>摘要</summary>
neural networks (NN) 已经在图像分类中变得极其普遍，但在标准形式下产生点估计，无法提供信度量。 bayesian neural networks (BNN) 提供图像分类预测和估计的不确定性评估（UQ），通过 posterior distribution。 随着 NN 在高重要性应用中使用，UQ 变得必须。 BNN 不仅提供准确的预测和估计，还提供一个包含合理值的时间间隔，在所需的概率范围内。 despite their positive attributes, BNN 很难和时间consuming 进行训练。 traditional Bayesian methods 使用 markov chain Monte Carlo (MCMC)，但这经常被认为是太慢。 the most common method 是 variational inference (VI) due to its fast computation, but there are multiple concerns with its efficacy。 我们在 target detection 中应用和比较 MCMC- 和 VI-trained BNN 在 hyperspectral imagery (HSI) 中，where materials of interest can be identified by their unique spectral signature。 this is a challenging field, due to the numerous permuting effects practical collection of HSI has on measured spectra。 both models are trained using out-of-the-box tools on a high fidelity HSI target detection scene。 both MCMC- 和 VI-trained BNN perform well overall at target detection on a simulated HSI scene。 this paper provides an example of how to utilize the benefits of UQ, but also to increase awareness that different training methods can give different results for the same model。 if sufficient computational resources are available, the best approach rather than the fastest or most efficient should be used, especially for high consequence problems。
</details></li>
</ul>
<hr>
<h2 id="The-divergence-time-of-protein-structures-modelled-by-Markov-matrices-and-its-relation-to-the-divergence-of-sequences"><a href="#The-divergence-time-of-protein-structures-modelled-by-Markov-matrices-and-its-relation-to-the-divergence-of-sequences" class="headerlink" title="The divergence time of protein structures modelled by Markov matrices and its relation to the divergence of sequences"></a>The divergence time of protein structures modelled by Markov matrices and its relation to the divergence of sequences</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06292">http://arxiv.org/abs/2308.06292</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sandun Rajapaksa, Lloyd Allison, Peter J. Stuckey, Maria Garcia de la Banda, Arun S. Konagurthu</li>
<li>for: 这种研究是为了开发一种基于蛋白质结构的时间参数化统计模型，以量化蛋白质结构在演化过程中的差异进化。</li>
<li>methods: 该研究使用了一个大量的蛋白质三维结构对比来推断一种时间参数化的统计模型，并使用了 bayesian 和信息理论的框架来推断时间参数化的随机矩阵和 Dirichlet 模型。</li>
<li>results: 研究发现，使用这种时间参数化统计模型可以更准确地估计蛋白质结构之间的分化时间，并且可以与序列相关性的时间参数化模型进行比较。此外，该模型还可以在次结构预测中与常见的神经网络架构竞争。<details>
<summary>Abstract</summary>
A complete time-parameterized statistical model quantifying the divergent evolution of protein structures in terms of the patterns of conservation of their secondary structures is inferred from a large collection of protein 3D structure alignments. This provides a better alternative to time-parameterized sequence-based models of protein relatedness, that have clear limitations dealing with twilight and midnight zones of sequence relationships. Since protein structures are far more conserved due to the selection pressure directly placed on their function, divergence time estimates can be more accurate when inferred from structures. We use the Bayesian and information-theoretic framework of Minimum Message Length to infer a time-parameterized stochastic matrix (accounting for perturbed structural states of related residues) and associated Dirichlet models (accounting for insertions and deletions during the evolution of protein domains). These are used in concert to estimate the Markov time of divergence of tertiary structures, a task previously only possible using proxies (like RMSD). By analyzing one million pairs of homologous structures, we yield a relationship between the Markov divergence time of structures and of sequences. Using these inferred models and the relationship between the divergence of sequences and structures, we demonstrate a competitive performance in secondary structure prediction against neural network architectures commonly employed for this task. The source code and supplementary information are downloadable from \url{http://lcb.infotech.monash.edu.au/sstsum}.
</details>
<details>
<summary>摘要</summary>
一个完整的时间参数化统计模型，用于描述蛋白质结构的不同演化的 Patterns of conservation of secondary structures，从一个大量的蛋白质三维结构对Alignment中得到了推断。这提供了一个更好的代替时间参数化序列基于模型，该模型在处理晚上和午夜时区的序列关系时存在显著的限制。由于蛋白质结构受直接选择压力的影响，因此从结构来进行演化时间估计的准确性比序列基于模型更高。我们使用 bayesian 和信息理论的框架，Minimum Message Length 来推断时间参数化随机矩阵（考虑相关的结构态态）和 Dirichlet 模型（考虑插入和删除 durante protein domains 的演化）。这些模型在一起使用，以估计蛋白质结构的马克夫时间异同。在分析一百万对同源结构的情况下，我们发现了结构异同时间和序列异同时间之间的关系。使用这些推断出的模型和序列异同时间之间的关系，我们展示了在二级结构预测中与通用的神经网络架构相比，我们的表现是竞争性的。源代码和补充信息可以从 \url{http://lcb.infotech.monash.edu.au/sstsum} 下载。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Team-Based-Navigation-A-Review-of-Deep-Reinforcement-Learning-Techniques-for-Multi-Agent-Pathfinding"><a href="#Learning-to-Team-Based-Navigation-A-Review-of-Deep-Reinforcement-Learning-Techniques-for-Multi-Agent-Pathfinding" class="headerlink" title="Learning to Team-Based Navigation: A Review of Deep Reinforcement Learning Techniques for Multi-Agent Pathfinding"></a>Learning to Team-Based Navigation: A Review of Deep Reinforcement Learning Techniques for Multi-Agent Pathfinding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05893">http://arxiv.org/abs/2308.05893</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jaehoon Chung, Jamil Fayyad, Younes Al Younes, Homayoun Najjaran</li>
<li>for: 本研究旨在探讨 Deep Reinforcement Learning（DRL）在多体系统中的应用，尤其是在多体路径找索中。</li>
<li>methods: 本文使用了多种 Deep Reinforcement Learning（DRL）方法，包括价值函数法和策略法，以解决多体系统中的路径找索问题。</li>
<li>results: 本文提供了一个统一的评价指标集，以便比较不同的多体路径找索算法的性能。此外，本文还发现了模型基于的DRL在多体系统中的潜在应用潜力，并提供了相关的基础知识。<details>
<summary>Abstract</summary>
Multi-agent pathfinding (MAPF) is a critical field in many large-scale robotic applications, often being the fundamental step in multi-agent systems. The increasing complexity of MAPF in complex and crowded environments, however, critically diminishes the effectiveness of existing solutions. In contrast to other studies that have either presented a general overview of the recent advancements in MAPF or extensively reviewed Deep Reinforcement Learning (DRL) within multi-agent system settings independently, our work presented in this review paper focuses on highlighting the integration of DRL-based approaches in MAPF. Moreover, we aim to bridge the current gap in evaluating MAPF solutions by addressing the lack of unified evaluation metrics and providing comprehensive clarification on these metrics. Finally, our paper discusses the potential of model-based DRL as a promising future direction and provides its required foundational understanding to address current challenges in MAPF. Our objective is to assist readers in gaining insight into the current research direction, providing unified metrics for comparing different MAPF algorithms and expanding their knowledge of model-based DRL to address the existing challenges in MAPF.
</details>
<details>
<summary>摘要</summary>
多智能路径找索（MAPF）是许多大规模 роботи库应用中的关键领域，经常作为多智能系统的基础步骤。然而，随着环境的增加复杂性，MAPF的现有解决方案的效果逐渐减退。与其他研究不同，我们的工作在这篇评论文中不仅提供了近期MAPF的进展概述，还广泛评论了深度强化学习（DRL）在多智能系统设置中的应用。此外，我们的工作还强调了评价MAPF解决方案的缺乏统一评价指标，并提供了全面的解释。最后，我们的文章还讨论了基于模型的DRL作为未来方向的潜在发展，并提供了相应的基础理解，以解决当前MAPF中的挑战。我们的目标是帮助读者更深入了解当前的研究方向，提供统一的评价指标，以及扩展他们对基于模型的DRL的知识，以Addressing the existing challenges in MAPF。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="DF2-Distribution-Free-Decision-Focused-Learning"><a href="#DF2-Distribution-Free-Decision-Focused-Learning" class="headerlink" title="DF2: Distribution-Free Decision-Focused Learning"></a>DF2: Distribution-Free Decision-Focused Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05889">http://arxiv.org/abs/2308.05889</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lingkai Kong, Wenhao Mu, Jiaming Cui, Yuchen Zhuang, B. Aditya Prakash, Bo Dai, Chao Zhang</li>
<li>for: 这篇论文是针对predict-then-optimize问题的decision-focused learning（DFL）方法，对于exististing end-to-end DFL方法的三个瓶颈：模型误差错误、抽象描述错误和梯度近似错误。</li>
<li>methods: 我们的DF2方法是第一个不需要任务特定的预测器，直接在训练过程中学习预期优化目标函数。我们创造了一个注意力基于分布的模型架构，以便对预期目标函数进行有效的学习。</li>
<li>results: 我们在一个实验中，用DF2方法解决了一个 sintetic问题、一个风力发电问题和一个非凸疫苗分布问题，得到了DF2方法的有效性。<details>
<summary>Abstract</summary>
Decision-focused learning (DFL) has recently emerged as a powerful approach for predict-then-optimize problems by customizing a predictive model to a downstream optimization task. However, existing end-to-end DFL methods are hindered by three significant bottlenecks: model mismatch error, sample average approximation error, and gradient approximation error. Model mismatch error stems from the misalignment between the model's parameterized predictive distribution and the true probability distribution. Sample average approximation error arises when using finite samples to approximate the expected optimization objective. Gradient approximation error occurs as DFL relies on the KKT condition for exact gradient computation, while most methods approximate the gradient for backpropagation in non-convex objectives. In this paper, we present DF2 -- the first \textit{distribution-free} decision-focused learning method explicitly designed to address these three bottlenecks. Rather than depending on a task-specific forecaster that requires precise model assumptions, our method directly learns the expected optimization function during training. To efficiently learn the function in a data-driven manner, we devise an attention-based model architecture inspired by the distribution-based parameterization of the expected objective. Our method is, to the best of our knowledge, the first to address all three bottlenecks within a single model. We evaluate DF2 on a synthetic problem, a wind power bidding problem, and a non-convex vaccine distribution problem, demonstrating the effectiveness of DF2.
</details>
<details>
<summary>摘要</summary>
决策关注学习（DFL）是一种有力的方法，用于解决预测后优化问题，通过自适应一个预测模型来满足下游优化任务。然而，现有的端到端DFL方法受到三种主要瓶颈的限制：模型匹配错误、样本平均化错误和梯度估计错误。模型匹配错误来自预测模型中参数化的预测分布与真实概率分布之间的不一致。样本平均化错误发生在使用有限样本来估计优化目标函数的期望值时。梯度估计错误则是因为DFL通过KKT条件来计算梯度，而大多数方法在非拟合目标函数中使用较差的梯度估计进行反向传播。在这篇论文中，我们提出了DF2方法——首先的无模型匹配的决策关注学习方法，用于解决这三种瓶颈。而不是基于任务特定的预测器，我们的方法直接在训练过程中学习预测函数。为效率地学习函数，我们设计了一种注意力基于分布的模型架构，得益于分布基于参数化的预测目标函数。我们的方法是，到目前为止所知道的第一个能够同时解决这三种瓶颈的单一模型。我们在一个 sintetic问题、一个风力发电拍卖问题和一个非拟合疫苗分布问题上进行了评估，并证明了DF2的效果。
</details></li>
</ul>
<hr>
<h2 id="GPLaSDI-Gaussian-Process-based-Interpretable-Latent-Space-Dynamics-Identification-through-Deep-Autoencoder"><a href="#GPLaSDI-Gaussian-Process-based-Interpretable-Latent-Space-Dynamics-Identification-through-Deep-Autoencoder" class="headerlink" title="GPLaSDI: Gaussian Process-based Interpretable Latent Space Dynamics Identification through Deep Autoencoder"></a>GPLaSDI: Gaussian Process-based Interpretable Latent Space Dynamics Identification through Deep Autoencoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05882">http://arxiv.org/abs/2308.05882</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/llnl/gplasdi">https://github.com/llnl/gplasdi</a></li>
<li>paper_authors: Christophe Bonneville, Youngsoo Choi, Debojyoti Ghosh, Jonathan L. Belof</li>
<li>for:  This paper aims to provide a novel reduced-order modeling (ROM) framework that leverages machine learning to solve partial differential equations (PDEs) efficiently and accurately.</li>
<li>methods:  The proposed method, called GPLaSDI, utilizes Gaussian processes (GPs) for latent space ODE interpolations, which enables the quantification of uncertainty over the ROM predictions and allows for efficient adaptive training.</li>
<li>results:  The proposed method achieves between 200 and 100,000 times speed-up, with up to 7% relative error, on the Burgers equation, Vlasov equation for plasma physics, and a rising thermal bubble problem.<details>
<summary>Abstract</summary>
Numerically solving partial differential equations (PDEs) can be challenging and computationally expensive. This has led to the development of reduced-order models (ROMs) that are accurate but faster than full order models (FOMs). Recently, machine learning advances have enabled the creation of non-linear projection methods, such as Latent Space Dynamics Identification (LaSDI). LaSDI maps full-order PDE solutions to a latent space using autoencoders and learns the system of ODEs governing the latent space dynamics. By interpolating and solving the ODE system in the reduced latent space, fast and accurate ROM predictions can be made by feeding the predicted latent space dynamics into the decoder. In this paper, we introduce GPLaSDI, a novel LaSDI-based framework that relies on Gaussian process (GP) for latent space ODE interpolations. Using GPs offers two significant advantages. First, it enables the quantification of uncertainty over the ROM predictions. Second, leveraging this prediction uncertainty allows for efficient adaptive training through a greedy selection of additional training data points. This approach does not require prior knowledge of the underlying PDEs. Consequently, GPLaSDI is inherently non-intrusive and can be applied to problems without a known PDE or its residual. We demonstrate the effectiveness of our approach on the Burgers equation, Vlasov equation for plasma physics, and a rising thermal bubble problem. Our proposed method achieves between 200 and 100,000 times speed-up, with up to 7% relative error.
</details>
<details>
<summary>摘要</summary>
解决部分泛函方程（PDE）数学问题可以是困难的并且 computationally expensive。这导致了减少顺序模型（ROM）的发展，这些模型具有准确性，但速度比整个顺序模型（FOM）更快。近些年，机器学习的进步使得非线性投影方法，如潜在空间动力学标识（LaSDI）的创造。LaSDI将全序PDE解析到一个潜在空间使用自适应神经网络，并学习潜在空间动力学系统的ODE。通过在减少的潜在空间中预测和解决ODE系统，可以快速并准确地预测ROM。在这篇论文中，我们介绍了GPLaSDI，一种基于 Gaussian process（GP）的LaSDI框架。使用GP提供了两点优势。首先，它允许量化ROM预测中的uncertainty。其次，通过利用这种预测uncertainty，可以高效地进行适应性训练，通过滥见训练数据点。这种方法不需要先知道下辖PDE或其剩余。因此，GPLaSDI是非侵入的，可以应用于没有known PDE或其剩余的问题。我们在Burgers方程、Vlasov方程 для плазма物理和热气囊问题中展示了我们的方法的效果。我们的提posed方法可以实现200到100,000倍的速度增加，相对误差在7%之间。
</details></li>
</ul>
<hr>
<h2 id="Aphid-Cluster-Recognition-and-Detection-in-the-Wild-Using-Deep-Learning-Models"><a href="#Aphid-Cluster-Recognition-and-Detection-in-the-Wild-Using-Deep-Learning-Models" class="headerlink" title="Aphid Cluster Recognition and Detection in the Wild Using Deep Learning Models"></a>Aphid Cluster Recognition and Detection in the Wild Using Deep Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05881">http://arxiv.org/abs/2308.05881</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianxiao Zhang, Kaidong Li, Xiangyu Chen, Cuncong Zhong, Bo Luo, Ivan Grijalva, Brian McCornack, Daniel Flippo, Ajay Sharda, Guanghui Wang</li>
<li>for: 本研究旨在使用深度学习模型检测螟蛾群体，以实现Targeted pesticide application。</li>
<li>methods: 我们使用了大规模的实验数据和深度学习模型来检测螟蛾群体，并对数据进行了处理和分割以便机器学习模型的使用。</li>
<li>results: 我们的实验结果表明，使用深度学习模型可以准确地检测螟蛾群体，并且可以通过合并邻近的群体和移除小 clusters来进一步提高性能。<details>
<summary>Abstract</summary>
Aphid infestation poses a significant threat to crop production, rural communities, and global food security. While chemical pest control is crucial for maximizing yields, applying chemicals across entire fields is both environmentally unsustainable and costly. Hence, precise localization and management of aphids are essential for targeted pesticide application. The paper primarily focuses on using deep learning models for detecting aphid clusters. We propose a novel approach for estimating infection levels by detecting aphid clusters. To facilitate this research, we have captured a large-scale dataset from sorghum fields, manually selected 5,447 images containing aphids, and annotated each individual aphid cluster within these images. To facilitate the use of machine learning models, we further process the images by cropping them into patches, resulting in a labeled dataset comprising 151,380 image patches. Then, we implemented and compared the performance of four state-of-the-art object detection models (VFNet, GFLV2, PAA, and ATSS) on the aphid dataset. Extensive experimental results show that all models yield stable similar performance in terms of average precision and recall. We then propose to merge close neighboring clusters and remove tiny clusters caused by cropping, and the performance is further boosted by around 17%. The study demonstrates the feasibility of automatically detecting and managing insects using machine learning models. The labeled dataset will be made openly available to the research community.
</details>
<details>
<summary>摘要</summary>
螟蛀感染 pose 对农业生产、农村社区和全球食品安全构成了严重的威胁。虽然化学防治是提高产量的重要手段，但是在整个场景中应用化学品是环境不可持续和昂贵的。因此，准确地Localization和管理螟蛀是必要的。本文主要关注使用深度学习模型 для检测螟蛀群。我们提出了一种新的方法，通过检测螟蛀群来估算感染水平。为了进行这项研究，我们在高粮田中采集了大规模数据集，手动选择了5,447张图像中包含螟蛀的图像，并对每个个体螟蛀群进行了标注。为了使机器学习模型可以使用，我们进一步处理了图像，将其分割成 patches，得到了151,380个标注图像 patches。然后，我们实现了和比较了四种当前最佳 объек detection 模型（VFNet、GFLV2、PAA 和 ATSS）在螟蛀数据集上的性能。广泛的实验结果表明，所有模型在精度和准确性方面具有稳定的性能。我们 then propose 将邻近的螟蛀群合并并 removes 小于patches 的螟蛀群，性能得到了约17%的提高。该研究表明了使用机器学习模型自动检测和管理昆虫的可能性。我们将标注数据集公开提供给研究社区。
</details></li>
</ul>
<hr>
<h2 id="Composable-Core-sets-for-Diversity-Approximation-on-Multi-Dataset-Streams"><a href="#Composable-Core-sets-for-Diversity-Approximation-on-Multi-Dataset-Streams" class="headerlink" title="Composable Core-sets for Diversity Approximation on Multi-Dataset Streams"></a>Composable Core-sets for Diversity Approximation on Multi-Dataset Streams</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05878">http://arxiv.org/abs/2308.05878</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stephanie Wang, Michael Flynn, Fangyu Luo</li>
<li>for: 这篇论文旨在描述如何使用核心集来简化流处理数据，以便在实时训练机器学习模型时提高效率。</li>
<li>methods: 该论文提出了一种基于核心集的核心集建构算法，用于简化流处理数据并在活动学习环境中使用。此外，论文还提出了一种使用核心集和CRAIG等技术来加速建构速度。</li>
<li>results: 论文通过对推送数据进行预测分析，证明了这种方法可以在实时训练机器学习模型时提高效率。此外，论文还提出了一些改进建构速度的策略和技术。<details>
<summary>Abstract</summary>
Core-sets refer to subsets of data that maximize some function that is commonly a diversity or group requirement. These subsets are used in place of the original data to accomplish a given task with comparable or even enhanced performance if biases are removed. Composable core-sets are core-sets with the property that subsets of the core set can be unioned together to obtain an approximation for the original data; lending themselves to be used for streamed or distributed data. Recent work has focused on the use of core-sets for training machine learning models. Preceding solutions such as CRAIG have been proven to approximate gradient descent while providing a reduced training time. In this paper, we introduce a core-set construction algorithm for constructing composable core-sets to summarize streamed data for use in active learning environments. If combined with techniques such as CRAIG and heuristics to enhance construction speed, composable core-sets could be used for real time training of models when the amount of sensor data is large. We provide empirical analysis by considering extrapolated data for the runtime of such a brute force algorithm. This algorithm is then analyzed for efficiency through averaged empirical regression and key results and improvements are suggested for further research on the topic.
</details>
<details>
<summary>摘要</summary>
核心集（core-set）指的是一 subset of data 可以最大化某种函数，通常是多样性或组合要求。这些子集用于取代原始数据来完成一个给定任务，并且可以保持或提高性能，即使存在偏见。可搅 core-sets 是指可以将核心集中的子集 union 起来 obtaint 原始数据的一个近似。这些核心集具有可搅性，可以用于流处理或分布式数据。现有研究集中焦点在 core-sets 的使用，特别是用于训练机器学习模型。之前的解决方案，如 CRAIG，已经证明可以近似梯度下降，同时提供减少的训练时间。在这篇论文中，我们介绍一种用于构建可搅 core-sets 的算法，用于概要流处理数据，以便在活动学习环境中使用。如果与 CRAIG 和其他优化技术相结合，可搅 core-sets 可以用于实时训练模型，当感知数据量很大时。我们对此进行了实验分析，考虑了扩展数据的运行时间。这种算法的效率被分析了，并且提出了一些关键结果和改进建议。
</details></li>
</ul>
<hr>
<h2 id="Revisiting-N-CNN-for-Clinical-Practice"><a href="#Revisiting-N-CNN-for-Clinical-Practice" class="headerlink" title="Revisiting N-CNN for Clinical Practice"></a>Revisiting N-CNN for Clinical Practice</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05877">http://arxiv.org/abs/2308.05877</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leonardo Antunes Ferreira, Lucas Pereira Carlini, Gabriel de Almeida Sá Coutrin, Tatiany Marcondes Heideirich, Marina Carvalho de Moraes Barros, Ruth Guinsburg, Carlos Eduardo Thomaz</li>
<li>for: 这篇论文旨在优化Neonatal Convolutional Neural Network（N-CNN）的超参数，并评估这些超参数对类别指标、可解释性和可靠性的影响，以及它们在临床实践中的潜在影响。</li>
<li>methods: 我们选择了不改变原始N-CNN架构的超参数，主要是修改学习率和训练正则化。我们通过评估每个超参数的改进情况来选择最佳超参数，并创建了调整后的Tuned N-CNN。此外，我们还应用了基于新生脸部编码系统的软标签，提出了一种新的训练 expresión facial类型分类模型的方法，用于评估新生痛的评估。</li>
<li>results: 结果表明，调整后的Tuned N-CNN显示出了类别指标和可解释性的改进，但这些改进直接不对准确性表现出来。我们认为这些发现可能有助于开发更可靠的痛评估工具 для新生，帮助医疗专业人员提供适当的 intervención和改善病人结果。<details>
<summary>Abstract</summary>
This paper revisits the Neonatal Convolutional Neural Network (N-CNN) by optimizing its hyperparameters and evaluating how they affect its classification metrics, explainability and reliability, discussing their potential impact in clinical practice. We have chosen hyperparameters that do not modify the original N-CNN architecture, but mainly modify its learning rate and training regularization. The optimization was done by evaluating the improvement in F1 Score for each hyperparameter individually, and the best hyperparameters were chosen to create a Tuned N-CNN. We also applied soft labels derived from the Neonatal Facial Coding System, proposing a novel approach for training facial expression classification models for neonatal pain assessment. Interestingly, while the Tuned N-CNN results point towards improvements in classification metrics and explainability, these improvements did not directly translate to calibration performance. We believe that such insights might have the potential to contribute to the development of more reliable pain evaluation tools for newborns, aiding healthcare professionals in delivering appropriate interventions and improving patient outcomes.
</details>
<details>
<summary>摘要</summary>
（本文重新审查了新生儿 convolutional neural network（N-CNN）的超参数，并评估它们如何影响其分类指标、可解释性和可靠性，并讨论它们在临床实践中的潜在影响。我们选择了不改变原始 N-CNN 架构的超参数，主要是 modify 学习率和训练正则化。优化是通过评估每个超参数的改进情况来进行，并选择最佳超参数来创建一个优化后的 Tuned N-CNN。我们还应用了来自新生儿表情编码系统的软标签，提出了一种新的训练 facial expression 分类模型的方法，用于新生儿疼痛评估。有趣的是，改进后的 Tuned N-CNN 结果表明，对于分类指标和可解释性来说，有所改进，但这些改进并不直接对准报表性能产生影响。我们认为，这些发现可能对新生儿疼痛评估工具的开发产生影响，帮助医疗专业人员提供适当的 intervención和改善病人结果。）
</details></li>
</ul>
<hr>
<h2 id="UFed-GAN-A-Secure-Federated-Learning-Framework-with-Constrained-Computation-and-Unlabeled-Data"><a href="#UFed-GAN-A-Secure-Federated-Learning-Framework-with-Constrained-Computation-and-Unlabeled-Data" class="headerlink" title="UFed-GAN: A Secure Federated Learning Framework with Constrained Computation and Unlabeled Data"></a>UFed-GAN: A Secure Federated Learning Framework with Constrained Computation and Unlabeled Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05870">http://arxiv.org/abs/2308.05870</a></li>
<li>repo_url: None</li>
<li>paper_authors: Achintha Wijesinghe, Songyang Zhang, Siyu Qi, Zhi Ding</li>
<li>for: 这篇论文是为了解决在云端环境中部署低延迟多媒体数据分类和数据隐私问题而提出的一种学习平衡，尤其是在有限的计算资源和没有标签数据的情况下。</li>
<li>methods: 这篇论文提出了一个名为UFed-GAN的无监督联邦学习框架，可以在用户端数据分布下进行学习，不需要进行本地分类训练。此外，论文还进行了对UFed-GAN的参数分析和隐私分析。</li>
<li>results: 实验结果显示，UFed-GAN在有限的计算资源和没有标签数据的情况下可以实现高效的数据分类和隐私保护。<details>
<summary>Abstract</summary>
To satisfy the broad applications and insatiable hunger for deploying low latency multimedia data classification and data privacy in a cloud-based setting, federated learning (FL) has emerged as an important learning paradigm. For the practical cases involving limited computational power and only unlabeled data in many wireless communications applications, this work investigates FL paradigm in a resource-constrained and label-missing environment. Specifically, we propose a novel framework of UFed-GAN: Unsupervised Federated Generative Adversarial Network, which can capture user-side data distribution without local classification training. We also analyze the convergence and privacy of the proposed UFed-GAN. Our experimental results demonstrate the strong potential of UFed-GAN in addressing limited computational resources and unlabeled data while preserving privacy.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:为满足云端数据分类和隐私的广泛应用和不满足的需求，联邦学习（FL）已经成为一种重要的学习模式。在许多无线通信应用中，由于限制的计算资源和只有无标签数据，这项工作研究了在资源限制和标签缺失环境中的FL模式。我们提出了一种新的框架——无监督联邦生成敌战网络（UFed-GAN），可以在用户端 cattcapture数据分布without local classification training。我们还分析了UFed-GAN的 converges和隐私性。我们的实验结果表明，UFed-GAN在限制计算资源和无标签数据的情况下具有强大的潜在性，能够解决数据隐私和安全问题。
</details></li>
</ul>
<hr>
<h2 id="Using-Twitter-Data-to-Determine-Hurricane-Category-An-Experiment"><a href="#Using-Twitter-Data-to-Determine-Hurricane-Category-An-Experiment" class="headerlink" title="Using Twitter Data to Determine Hurricane Category: An Experiment"></a>Using Twitter Data to Determine Hurricane Category: An Experiment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05866">http://arxiv.org/abs/2308.05866</a></li>
<li>repo_url: None</li>
<li>paper_authors: Songhui Yue, Jyothsna Kondari, Aibek Musaev, Randy K. Smith, Songqing Yue</li>
<li>for: 本研究旨在找到社交媒体数据和自然灾害的严重程度之间的映射关系。</li>
<li>methods: 本研究使用数据挖掘技术来分析Twitter数据，并对各地区的Twitter数据和飓风等级之间进行相关性分析。</li>
<li>results: 实验结果表明，Twitter数据和飓风等级之间存在积极的相关性，并提出了一种使用Twitter数据预测飓风等级的方法。<details>
<summary>Abstract</summary>
Social media posts contain an abundant amount of information about public opinion on major events, especially natural disasters such as hurricanes. Posts related to an event, are usually published by the users who live near the place of the event at the time of the event. Special correlation between the social media data and the events can be obtained using data mining approaches. This paper presents research work to find the mappings between social media data and the severity level of a disaster. Specifically, we have investigated the Twitter data posted during hurricanes Harvey and Irma, and attempted to find the correlation between the Twitter data of a specific area and the hurricane level in that area. Our experimental results indicate a positive correlation between them. We also present a method to predict the hurricane category for a specific area using relevant Twitter data.
</details>
<details>
<summary>摘要</summary>
社交媒体帖子中含有很多关于大事件的公众意见信息，尤其是自然灾害such as 飓风。posts相关的事件通常由用户们在事件发生时在附近的地方发布。我们的研究旨在找到社交媒体数据和灾害严重程度之间的映射。 Specifically, we investigated Twitter data posted during hurricanes Harvey and Irma and attempted to find a correlation between the Twitter data of a specific area and the hurricane level in that area. Our experimental results indicate a positive correlation between them. We also present a method to predict the hurricane category for a specific area using relevant Twitter data.Here's the word-for-word translation:社交媒体帖子中含有很多关于大事件的公众意见信息，尤其是自然灾害such as 飓风。posts相关的事件通常由用户们在事件发生时在附近的地方发布。我们的研究旨在找到社交媒体数据和灾害严重程度之间的映射。Specifically, we investigated Twitter data posted during hurricanes Harvey and Irma and attempted to find a correlation between the Twitter data of a specific area and the hurricane level in that area. Our experimental results indicate a positive correlation between them. We also present a method to predict the hurricane category for a specific area using relevant Twitter data.
</details></li>
</ul>
<hr>
<h2 id="The-Multi-modality-Cell-Segmentation-Challenge-Towards-Universal-Solutions"><a href="#The-Multi-modality-Cell-Segmentation-Challenge-Towards-Universal-Solutions" class="headerlink" title="The Multi-modality Cell Segmentation Challenge: Towards Universal Solutions"></a>The Multi-modality Cell Segmentation Challenge: Towards Universal Solutions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05864">http://arxiv.org/abs/2308.05864</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jun Ma, Ronald Xie, Shamini Ayyadhury, Cheng Ge, Anubha Gupta, Ritu Gupta, Song Gu, Yao Zhang, Gihun Lee, Joonkee Kim, Wei Lou, Haofeng Li, Eric Upschulte, Timo Dickscheid, José Guilherme de Almeida, Yixin Wang, Lin Han, Xin Yang, Marco Labagnara, Sahand Jamal Rahi, Carly Kempster, Alice Pollitt, Leon Espinosa, Tâm Mignot, Jan Moritz Middeke, Jan-Niklas Eckardt, Wangkai Li, Zhaoyang Li, Xiaochen Cai, Bizhe Bai, Noah F. Greenwald, David Van Valen, Erin Weisbart, Beth A. Cimini, Zhuoshi Li, Chao Zuo, Oscar Brück, Gary D. Bader, Bo Wang</li>
<li>for: 单细胞分析中的细胞分割步骤是 kritical。</li>
<li>methods: 这些方法通常适应特定的modalities或需要手动参数调整来适应不同的实验设置。</li>
<li>results: 这个benchmark和改进的算法可以无需手动参数调整地应用于多种微显技术和组织类型的细胞图像。<details>
<summary>Abstract</summary>
Cell segmentation is a critical step for quantitative single-cell analysis in microscopy images. Existing cell segmentation methods are often tailored to specific modalities or require manual interventions to specify hyperparameters in different experimental settings. Here, we present a multi-modality cell segmentation benchmark, comprising over 1500 labeled images derived from more than 50 diverse biological experiments. The top participants developed a Transformer-based deep-learning algorithm that not only exceeds existing methods, but can also be applied to diverse microscopy images across imaging platforms and tissue types without manual parameter adjustments. This benchmark and the improved algorithm offer promising avenues for more accurate and versatile cell analysis in microscopy imaging.
</details>
<details>
<summary>摘要</summary>
cell 分 segmentation 是单细胞分析中的关键步骤，exist 的 cell 分 segmentation 方法 oftentailored 到特定Modalities 或需要手动 intervene  specify  hyperparameters  in different experimental settings. Here, we present a multi-modality cell segmentation benchmark, comprising over 1500 labeled images derived from more than 50 diverse biological experiments. The top participants developed a Transformer-based deep-learning algorithm that not only exceeds existing methods, but can also be applied to diverse microscopy images across imaging platforms and tissue types without manual parameter adjustments. This benchmark and the improved algorithm offer promising avenues for more accurate and versatile cell analysis in microscopy imaging.Note that the word "Transformer" in the original text was translated as "Transformer-based" in Simplified Chinese, as there is no direct equivalent of the word "Transformer" in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Knowledge-Propagation-over-Conditional-Independence-Graphs"><a href="#Knowledge-Propagation-over-Conditional-Independence-Graphs" class="headerlink" title="Knowledge Propagation over Conditional Independence Graphs"></a>Knowledge Propagation over Conditional Independence Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05857">http://arxiv.org/abs/2308.05857</a></li>
<li>repo_url: None</li>
<li>paper_authors: Urszula Chajewska, Harsh Shrivastava</li>
<li>for: 本研究旨在提出ci图表知识传播算法，用于从domain topology中提取有用信息。</li>
<li>methods: 本研究使用ci图表模型，并提出了一种基于ci图表的知识传播算法。</li>
<li>results: 实验结果表明，该算法在公共 disponibles的cora和pubmed datasets上有所提高，与现有技术相比。<details>
<summary>Abstract</summary>
Conditional Independence (CI) graph is a special type of a Probabilistic Graphical Model (PGM) where the feature connections are modeled using an undirected graph and the edge weights show the partial correlation strength between the features. Since the CI graphs capture direct dependence between features, they have been garnering increasing interest within the research community for gaining insights into the systems from various domains, in particular discovering the domain topology. In this work, we propose algorithms for performing knowledge propagation over the CI graphs. Our experiments demonstrate that our techniques improve upon the state-of-the-art on the publicly available Cora and PubMed datasets.
</details>
<details>
<summary>摘要</summary>
<<SYS>> traduced the text into Simplified Chinese.<</SYS>>conditional independence (CI) 图是一种特殊的概率图模型 (PGM)，其中特征连接使用无向图表示，边重量表示特征之间的半相关度。由于 CI 图表示直接相互关联的特征，因此在不同领域中的系统研究中备受关注，特别是发现领域 то波动。在这项工作中，我们提出了在 CI 图上进行知识传播的算法。我们的实验表明，我们的技术在公共可用的 Cora 和 PubMed 数据集上超过了当前最佳的状况。
</details></li>
</ul>
<hr>
<h2 id="CSPM-A-Contrastive-Spatiotemporal-Preference-Model-for-CTR-Prediction-in-On-Demand-Food-Delivery-Services"><a href="#CSPM-A-Contrastive-Spatiotemporal-Preference-Model-for-CTR-Prediction-in-On-Demand-Food-Delivery-Services" class="headerlink" title="CSPM: A Contrastive Spatiotemporal Preference Model for CTR Prediction in On-Demand Food Delivery Services"></a>CSPM: A Contrastive Spatiotemporal Preference Model for CTR Prediction in On-Demand Food Delivery Services</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08446">http://arxiv.org/abs/2308.08446</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guyu Jiang, Xiaoyun Li, Rongrong Jing, Ruoqi Zhao, Xingliang Ni, Guodong Cao, Ning Hu</li>
<li>for: 这篇论文的目的是为了提高在在线快递食品平台上的点击率预测（CTR）。</li>
<li>methods: 这篇论文使用了三个模块：对比性空间时间表示学习（CSRL）、空间时间喜好EXTRACTOR（StPE）和空间时间信息过滤器（StIF）。CSRL使用了对比学习框架来生成搜索行为中的空间时间活动表示（SAR）。StPE使用了SAR来活化用户的不同的位置和时间相关的喜好，使用多头注意机制。StIF将SARintegrated into a gating network来自动捕捉重要的隐藏空间时间效果。</li>
<li>results: 在两个大规模的工业数据集上进行了广泛的实验，并证明了CSPM的状态之前性表现。尤其是，CSPM已经成功部署在阿里巴巴的在线快递食品平台Ele.me上，导致了显著的0.88%提升Click-through rate，这有重要的商业意义。<details>
<summary>Abstract</summary>
Click-through rate (CTR) prediction is a crucial task in the context of an online on-demand food delivery (OFD) platform for precisely estimating the probability of a user clicking on food items. Unlike universal e-commerce platforms such as Taobao and Amazon, user behaviors and interests on the OFD platform are more location and time-sensitive due to limited delivery ranges and regional commodity supplies. However, existing CTR prediction algorithms in OFD scenarios concentrate on capturing interest from historical behavior sequences, which fails to effectively model the complex spatiotemporal information within features, leading to poor performance. To address this challenge, this paper introduces the Contrastive Sres under different search states using three modules: contrastive spatiotemporal representation learning (CSRL), spatiotemporal preference extractor (StPE), and spatiotemporal information filter (StIF). CSRL utilizes a contrastive learning framework to generate a spatiotemporal activation representation (SAR) for the search action. StPE employs SAR to activate users' diverse preferences related to location and time from the historical behavior sequence field, using a multi-head attention mechanism. StIF incorporates SAR into a gating network to automatically capture important features with latent spatiotemporal effects. Extensive experiments conducted on two large-scale industrial datasets demonstrate the state-of-the-art performance of CSPM. Notably, CSPM has been successfully deployed in Alibaba's online OFD platform Ele.me, resulting in a significant 0.88% lift in CTR, which has substantial business implications.
</details>
<details>
<summary>摘要</summary>
Click-through rate (CTR) 预测是在在线快递食品平台上关键的任务，准确地估计用户会点击食品项。不同于通用电商平台如淘宝和amazon，用户在食品平台上的行为和兴趣更加地受到地域和时间影响，因为交通范围和地域商品供应有限。然而，现有的 CTRL 预测算法在食品平台场景中集中在捕捉历史行为序列中的兴趣，而不能有效地模型特有的空间时间信息，导致表现不佳。为解决这个挑战，本文引入了不同搜索状态下的 Contrastive Sres，使用三个模块：对比空间时间表示学习（CSRL）、空间时间偏好提取器（StPE）和空间时间信息筛选器（StIF）。CSRL 利用对比学习框架生成一个空间时间活动表示（SAR） для搜索行为。StPE 使用 SAR 来激活用户的不同地域时间上的偏好，使用多头注意机制。StIF 将 SAR integrated into a gating network 自动捕捉特有的空间时间效应。经验表明，CSPM 在两个大规模的业务数据集上达到了领先的性能，并在阿里巴巴在线食品平台 Ele.me 上部署成功，导致了显著的0.88%增加 Click-through rate，这有substantial商业意义。
</details></li>
</ul>
<hr>
<h2 id="GaborPINN-Efficient-physics-informed-neural-networks-using-multiplicative-filtered-networks"><a href="#GaborPINN-Efficient-physics-informed-neural-networks-using-multiplicative-filtered-networks" class="headerlink" title="GaborPINN: Efficient physics informed neural networks using multiplicative filtered networks"></a>GaborPINN: Efficient physics informed neural networks using multiplicative filtered networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05843">http://arxiv.org/abs/2308.05843</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinquan Huang, Tariq Alkhalifah<br>for: 这篇论文的目的是提出一种改进的物理学 Informed Neural Network（PINN）方法，以加速冲击波场的计算。methods: 这篇论文使用了多个技术，包括physics-informed neural networks（PINNs）和Gabor基函数。它们在训练过程中嵌入了一些已知的冲击波场特征，如频率，以提高快速度。results: 论文的实验结果表明，使用GaborPINN方法可以大大提高冲击波场的计算速度，比传统的PINN方法快两个数量级。<details>
<summary>Abstract</summary>
The computation of the seismic wavefield by solving the Helmholtz equation is crucial to many practical applications, e.g., full waveform inversion. Physics-informed neural networks (PINNs) provide functional wavefield solutions represented by neural networks (NNs), but their convergence is slow. To address this problem, we propose a modified PINN using multiplicative filtered networks, which embeds some of the known characteristics of the wavefield in training, e.g., frequency, to achieve much faster convergence. Specifically, we use the Gabor basis function due to its proven ability to represent wavefields accurately and refer to the implementation as GaborPINN. Meanwhile, we incorporate prior information on the frequency of the wavefield into the design of the method to mitigate the influence of the discontinuity of the represented wavefield by GaborPINN. The proposed method achieves up to a two-magnitude increase in the speed of convergence as compared with conventional PINNs.
</details>
<details>
<summary>摘要</summary>
computations of seismic wavefield by solving Helmholtz equation is crucial to many practical applications, e.g., full waveform inversion. physics-informed neural networks (PINNs) provide functional wavefield solutions represented by neural networks (NNs), but their convergence is slow. to address this problem, we propose modified PINN using multiplicative filtered networks, which embeds some of the known characteristics of wavefield in training, e.g., frequency, to achieve much faster convergence. specifically, we use Gabor basis function due to its proven ability to represent wavefields accurately and refer to the implementation as GaborPINN. meanwhile, we incorporate prior information on frequency of wavefield into the design of method to mitigate influence of discontinuity of represented wavefield by GaborPINN. proposed method achieves up to two-magnitude increase in speed of convergence as compared with conventional PINNs.
</details></li>
</ul>
<hr>
<h2 id="FLShield-A-Validation-Based-Federated-Learning-Framework-to-Defend-Against-Poisoning-Attacks"><a href="#FLShield-A-Validation-Based-Federated-Learning-Framework-to-Defend-Against-Poisoning-Attacks" class="headerlink" title="FLShield: A Validation Based Federated Learning Framework to Defend Against Poisoning Attacks"></a>FLShield: A Validation Based Federated Learning Framework to Defend Against Poisoning Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05832">http://arxiv.org/abs/2308.05832</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ehsanul Kabir, Zeyu Song, Md Rafi Ur Rashid, Shagufta Mehnaz</li>
<li>for: 本研究旨在提出一种新的联合学习（Federated Learning，FL）框架，以保障FL系统的安全性和可靠性。</li>
<li>methods: 本研究使用了本地模型的有益数据来验证本地模型的可靠性，而不是依赖服务器访问spotless数据集，这种做法和FL的基本原则不匹配。</li>
<li>results: 研究人员通过对不同情况下的FLShield框架进行了广泛的实验，并证明了FLShield框架能够有效地防范各种毒素和后门攻击，同时保护本地数据的隐私。<details>
<summary>Abstract</summary>
Federated learning (FL) is revolutionizing how we learn from data. With its growing popularity, it is now being used in many safety-critical domains such as autonomous vehicles and healthcare. Since thousands of participants can contribute in this collaborative setting, it is, however, challenging to ensure security and reliability of such systems. This highlights the need to design FL systems that are secure and robust against malicious participants' actions while also ensuring high utility, privacy of local data, and efficiency. In this paper, we propose a novel FL framework dubbed as FLShield that utilizes benign data from FL participants to validate the local models before taking them into account for generating the global model. This is in stark contrast with existing defenses relying on server's access to clean datasets -- an assumption often impractical in real-life scenarios and conflicting with the fundamentals of FL. We conduct extensive experiments to evaluate our FLShield framework in different settings and demonstrate its effectiveness in thwarting various types of poisoning and backdoor attacks including a defense-aware one. FLShield also preserves privacy of local data against gradient inversion attacks.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 是如何改变我们如何从数据中学习的革命。随着其 Popularity 的增长，它现在在许多安全关键领域，如自动驾驶和医疗，中使用。由于千余名参与者可以在这种合作环境中贡献，因此保持安全性和可靠性的系统是挑战。这 Heightens 需要设计安全可靠的 FL 系统，能够抵御恶意参与者的行为，同时保持本地数据隐私和效率。在这篇论文中，我们提出了一种新的 FL 框架，名为 FLShield，它利用 FL 参与者的善意数据来验证本地模型，然后将其作为全球模型生成。这与现有防御方法，它们基于服务器访问干净的数据集的假设，不同。我们进行了广泛的实验来评估我们的 FLShield 框架在不同的设置下的效果，并证明它在不同类型的毒素和后门攻击中具有有效性。FLShield 还保持本地数据隐私性免受Gradient Inversion攻击。
</details></li>
</ul>
<hr>
<h2 id="Neural-Progressive-Meshes"><a href="#Neural-Progressive-Meshes" class="headerlink" title="Neural Progressive Meshes"></a>Neural Progressive Meshes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05741">http://arxiv.org/abs/2308.05741</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yun-Chun Chen, Vladimir G. Kim, Noam Aigerman, Alec Jacobson</li>
<li>for:  efficiently transmit large geometric data (e.g., 3D meshes) over the Internet</li>
<li>methods: subdivision-based encoder-decoder architecture trained on a large collection of surfaces, with progressive transmission of residual features</li>
<li>results: outperforms baselines in terms of compression ratio and reconstruction quality<details>
<summary>Abstract</summary>
The recent proliferation of 3D content that can be consumed on hand-held devices necessitates efficient tools for transmitting large geometric data, e.g., 3D meshes, over the Internet. Detailed high-resolution assets can pose a challenge to storage as well as transmission bandwidth, and level-of-detail techniques are often used to transmit an asset using an appropriate bandwidth budget. It is especially desirable for these methods to transmit data progressively, improving the quality of the geometry with more data. Our key insight is that the geometric details of 3D meshes often exhibit similar local patterns even across different shapes, and thus can be effectively represented with a shared learned generative space. We learn this space using a subdivision-based encoder-decoder architecture trained in advance on a large collection of surfaces. We further observe that additional residual features can be transmitted progressively between intermediate levels of subdivision that enable the client to control the tradeoff between bandwidth cost and quality of reconstruction, providing a neural progressive mesh representation. We evaluate our method on a diverse set of complex 3D shapes and demonstrate that it outperforms baselines in terms of compression ratio and reconstruction quality.
</details>
<details>
<summary>摘要</summary>
Our key insight is that the geometric details of 3D meshes often exhibit similar local patterns across different shapes, and can be effectively represented with a shared learned generative space. We use a subdivision-based encoder-decoder architecture trained in advance on a large collection of surfaces to learn this space. Additionally, we observe that residual features can be transmitted progressively between intermediate levels of subdivision, enabling the client to control the tradeoff between bandwidth cost and quality of reconstruction. This provides a neural progressive mesh representation.We evaluate our method on a diverse set of complex 3D shapes and demonstrate that it outperforms baselines in terms of compression ratio and reconstruction quality.
</details></li>
</ul>
<hr>
<h2 id="Zero-Grads-Ever-Given-Learning-Local-Surrogate-Losses-for-Non-Differentiable-Graphics"><a href="#Zero-Grads-Ever-Given-Learning-Local-Surrogate-Losses-for-Non-Differentiable-Graphics" class="headerlink" title="Zero Grads Ever Given: Learning Local Surrogate Losses for Non-Differentiable Graphics"></a>Zero Grads Ever Given: Learning Local Surrogate Losses for Non-Differentiable Graphics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05739">http://arxiv.org/abs/2308.05739</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Fischer, Tobias Ritschel</li>
<li>for: 解决 Graphics 中的Gradient-based优化问题，因为现有的搜索方法无法处理undefined或zero gradients。</li>
<li>methods: 提出了一种自动化替换损失函数的框架，即ZeroGrads，通过学习一个神经网络来 aproximate objective function，并使用这个神经网络来 differentiate through arbitrary black-box graphics pipelines。</li>
<li>results: 实现了在online和self-supervised的情况下，使用 actively smoothed version of the objective 进行训练，并且可以在 tractable run-times 和competitive performance下解决多个非对称、非 differentiable black-box problem in Graphics，如视力rendering、排序参数空间和物理驱动动画优化等。<details>
<summary>Abstract</summary>
Gradient-based optimization is now ubiquitous across graphics, but unfortunately can not be applied to problems with undefined or zero gradients. To circumvent this issue, the loss function can be manually replaced by a "surrogate" that has similar minima but is differentiable. Our proposed framework, ZeroGrads, automates this process by learning a neural approximation of the objective function, the surrogate, which in turn can be used to differentiate through arbitrary black-box graphics pipelines. We train the surrogate on an actively smoothed version of the objective and encourage locality, focusing the surrogate's capacity on what matters at the current training episode. The fitting is performed online, alongside the parameter optimization, and self-supervised, without pre-computed data or pre-trained models. As sampling the objective is expensive (it requires a full rendering or simulator run), we devise an efficient sampling scheme that allows for tractable run-times and competitive performance at little overhead. We demonstrate optimizing diverse non-convex, non-differentiable black-box problems in graphics, such as visibility in rendering, discrete parameter spaces in procedural modelling or optimal control in physics-driven animation. In contrast to more traditional algorithms, our approach scales well to higher dimensions, which we demonstrate on problems with up to 35k interlinked variables.
</details>
<details>
<summary>摘要</summary>
“梯度基本优化现在在图形处理中广泛应用，但它无法应用于无定义或 zeros 梯度的问题。为了缺 Bibliography 这个问题，我们可以手动替换损失函数，使其成为可导的。我们的提议的框架，ZeroGrads，可以自动实现这个过程，它通过学习一个神经网络来模拟目标函数，并使用这个模拟来分子 Graphics pipeline 中的任意黑盒问题。我们在训练过程中使用在目标函数上实时缓和的版本，并且强调本集成性，使得模拟的能量集中在当前训练集中。我们的整个训练过程是在线进行的，并且是自动的，不需要预计算数据或预训练模型。由于评估目标函数的成本 relativity 高（它需要一个完整的渲染或 simulator 运行），我们开发了一种有效的采样方案，使得我们可以在可耗时间和竞争性的情况下实现高性能。我们在不同的非等式、非导数的黑盒问题上进行了优化，例如渲染中的可见性、procedural 模型中的分配空间和物理驱动的动画中的优化问题。与传统算法相比，我们的方法可以很好地扩展到更高的维度，我们在问题中的35k个相互关联变量上进行了示例。”
</details></li>
</ul>
<hr>
<h2 id="Follow-Anything-Open-set-detection-tracking-and-following-in-real-time"><a href="#Follow-Anything-Open-set-detection-tracking-and-following-in-real-time" class="headerlink" title="Follow Anything: Open-set detection, tracking, and following in real-time"></a>Follow Anything: Open-set detection, tracking, and following in real-time</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05737">http://arxiv.org/abs/2308.05737</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alaamaalouf/followanything">https://github.com/alaamaalouf/followanything</a></li>
<li>paper_authors: Alaa Maalouf, Ninad Jadhav, Krishna Murthy Jatavallabhula, Makram Chahine, Daniel M. Vogt, Robert J. Wood, Antonio Torralba, Daniela Rus</li>
<li>for: 本研究旨在开发一种可以在实时控制循环中跟踪任何对象的机器人系统。</li>
<li>methods: 该系统基于大规模预训练模型（基础模型），可以在实时控制循环中检测、分割和跟踪对象，并且可以考虑 occlusion 和对象重新出现。</li>
<li>results: 研究人员在一架微型飞行器上部署了该系统，并成功地跟踪了各种对象。系统可以在一个简单的 laptop 上运行，并且可以达到 6-20 帧每秒的throughput。<details>
<summary>Abstract</summary>
Tracking and following objects of interest is critical to several robotics use cases, ranging from industrial automation to logistics and warehousing, to healthcare and security. In this paper, we present a robotic system to detect, track, and follow any object in real-time. Our approach, dubbed ``follow anything'' (FAn), is an open-vocabulary and multimodal model -- it is not restricted to concepts seen at training time and can be applied to novel classes at inference time using text, images, or click queries. Leveraging rich visual descriptors from large-scale pre-trained models (foundation models), FAn can detect and segment objects by matching multimodal queries (text, images, clicks) against an input image sequence. These detected and segmented objects are tracked across image frames, all while accounting for occlusion and object re-emergence. We demonstrate FAn on a real-world robotic system (a micro aerial vehicle) and report its ability to seamlessly follow the objects of interest in a real-time control loop. FAn can be deployed on a laptop with a lightweight (6-8 GB) graphics card, achieving a throughput of 6-20 frames per second. To enable rapid adoption, deployment, and extensibility, we open-source all our code on our project webpage at https://github.com/alaamaalouf/FollowAnything . We also encourage the reader the watch our 5-minutes explainer video in this https://www.youtube.com/watch?v=6Mgt3EPytrw .
</details>
<details>
<summary>摘要</summary>
Tracking and following objects of interest is critical to several robotics use cases, ranging from industrial automation to logistics and warehousing, to healthcare and security. In this paper, we present a robotic system to detect, track, and follow any object in real-time. Our approach, dubbed “follow anything” (FAn), is an open-vocabulary and multimodal model — it is not restricted to concepts seen at training time and can be applied to novel classes at inference time using text, images, or click queries. Leveraging rich visual descriptors from large-scale pre-trained models (foundation models), FAn can detect and segment objects by matching multimodal queries (text, images, clicks) against an input image sequence. These detected and segmented objects are tracked across image frames, all while accounting for occlusion and object re-emergence. We demonstrate FAn on a real-world robotic system (a micro aerial vehicle) and report its ability to seamlessly follow the objects of interest in a real-time control loop. FAn can be deployed on a laptop with a lightweight (6-8 GB) graphics card, achieving a throughput of 6-20 frames per second. To enable rapid adoption, deployment, and extensibility, we open-source all our code on our project webpage at <https://github.com/alaamaalouf/FollowAnything>. We also encourage the reader to watch our 5-minutes explainer video in this <https://www.youtube.com/watch?v=6Mgt3EPytrw>.
</details></li>
</ul>
<hr>
<h2 id="PDE-Refiner-Achieving-Accurate-Long-Rollouts-with-Neural-PDE-Solvers"><a href="#PDE-Refiner-Achieving-Accurate-Long-Rollouts-with-Neural-PDE-Solvers" class="headerlink" title="PDE-Refiner: Achieving Accurate Long Rollouts with Neural PDE Solvers"></a>PDE-Refiner: Achieving Accurate Long Rollouts with Neural PDE Solvers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05732">http://arxiv.org/abs/2308.05732</a></li>
<li>repo_url: None</li>
<li>paper_authors: Phillip Lippe, Bastiaan S. Veeling, Paris Perdikaris, Richard E. Turner, Johannes Brandstetter<br>for:This paper aims to improve the accuracy and stability of deep neural network-based solution techniques for partial differential equations (PDEs) by addressing the neglect of non-dominant spatial frequency information.methods:The authors use a large-scale analysis of common temporal rollout strategies and draw inspiration from recent advances in diffusion models to introduce a novel model class called PDE-Refiner, which uses a multistep refinement process to accurately model all frequency components of PDE solutions.results:The authors validate PDE-Refiner on challenging benchmarks of complex fluid dynamics and demonstrate stable and accurate rollouts that consistently outperform state-of-the-art models, including neural, numerical, and hybrid neural-numerical architectures. Additionally, PDE-Refiner is shown to greatly enhance data efficiency by implicitly inducing a novel form of spectral data augmentation, and the authors demonstrate an accurate and efficient assessment of the model’s predictive uncertainty.Here is the simplified Chinese text:for: 这篇论文目的是提高深度神经网络基于 partial differential equation (PDE) 的解决方法的精度和稳定性。methods: 作者使用大规模的时间滚动策略分析，并启发自latest advances in diffusion models ，引入了一种新的模型类叫 PDE-Refiner，该模型使用多步增强过程来准确地模型 PDE 解的所有频率组成部分。results: 作者验证 PDE-Refiner 在复杂的流体动力学 benchmark 上，并显示了稳定和准确的滚动性能，常常超越当前的模型，包括神经网络、数学、和混合神经网络-数学模型。此外， PDE-Refiner 能够大幅提高数据效率，因为净化目标意味着在数据增强过程中隐式地引入了一种新的频率数据增强。最后， PDE-Refiner 的连接到 diffusion models 使得可以准确地评估模型的预测不确定性，从而估计模型在滚动过程中的不确定性。<details>
<summary>Abstract</summary>
Time-dependent partial differential equations (PDEs) are ubiquitous in science and engineering. Recently, mostly due to the high computational cost of traditional solution techniques, deep neural network based surrogates have gained increased interest. The practical utility of such neural PDE solvers relies on their ability to provide accurate, stable predictions over long time horizons, which is a notoriously hard problem. In this work, we present a large-scale analysis of common temporal rollout strategies, identifying the neglect of non-dominant spatial frequency information, often associated with high frequencies in PDE solutions, as the primary pitfall limiting stable, accurate rollout performance. Based on these insights, we draw inspiration from recent advances in diffusion models to introduce PDE-Refiner; a novel model class that enables more accurate modeling of all frequency components via a multistep refinement process. We validate PDE-Refiner on challenging benchmarks of complex fluid dynamics, demonstrating stable and accurate rollouts that consistently outperform state-of-the-art models, including neural, numerical, and hybrid neural-numerical architectures. We further demonstrate that PDE-Refiner greatly enhances data efficiency, since the denoising objective implicitly induces a novel form of spectral data augmentation. Finally, PDE-Refiner's connection to diffusion models enables an accurate and efficient assessment of the model's predictive uncertainty, allowing us to estimate when the surrogate becomes inaccurate.
</details>
<details>
<summary>摘要</summary>
时间依赖的 partial differential equations (PDEs) 在科学和工程中具有广泛的应用。最近，主要由于传统解决方案的计算成本高，深度神经网络基于的 surrogate 获得了更多的关注。然而，实际应用中，这些神经网络 PDE 解决器的实用性取决于它们能够提供稳定、准确的预测，这是一个非常困难的问题。在这种情况下，我们提出了一项大规模分析 temporal rollout 策略，发现忽略非主要空间频率信息，通常与 PDE 解的高频成分相关，是主要的障碍物，限制稳定、准确的 rollout 性能。基于这些发现，我们启发自 recent advances in diffusion models，提出了 PDE-Refiner; 一种新的模型类，可以更好地模拟所有频率组成部分。我们验证 PDE-Refiner 在复杂的流体动力学 benchmark 上，可以实现稳定和准确的 rollouts，常常超越当前模型，包括神经网络、数值和混合神经网络-数值模型。此外，PDE-Refiner 可以大幅提高数据效率，因为净化目标隐式地导入了一种新的 spectral data augmentation。最后，PDE-Refiner 的连接到 diffusion models 使得可以准确地评估模型的预测不确定性，从而估计 surrogate 是否准确。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Integration-of-Prediction-and-Planning-in-Deep-Learning-Based-Automated-Driving-Systems-A-Review"><a href="#Rethinking-Integration-of-Prediction-and-Planning-in-Deep-Learning-Based-Automated-Driving-Systems-A-Review" class="headerlink" title="Rethinking Integration of Prediction and Planning in Deep Learning-Based Automated Driving Systems: A Review"></a>Rethinking Integration of Prediction and Planning in Deep Learning-Based Automated Driving Systems: A Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05731">http://arxiv.org/abs/2308.05731</a></li>
<li>repo_url: None</li>
<li>paper_authors: Steffen Hagedorn, Marcel Hallgarten, Martin Stoll, Alexandru Condurache</li>
<li>for: 这篇论文主要写于自动驾驶技术的发展，具体来说是关于 Prediction、Planning 和 Integrated Prediction and Planning 模型的系统性评估。</li>
<li>methods: 论文使用了深度学习技术来实现 Prediction 和 Planning 模型，并对不同的集成方法进行了系统性的比较和分析。</li>
<li>results: 论文发现了不同集成方法在自动驾驶中的优缺点，并指出了未来研究的方向和挑战。<details>
<summary>Abstract</summary>
Automated driving has the potential to revolutionize personal, public, and freight mobility. Besides the enormous challenge of perception, i.e. accurately perceiving the environment using available sensor data, automated driving comprises planning a safe, comfortable, and efficient motion trajectory. To promote safety and progress, many works rely on modules that predict the future motion of surrounding traffic. Modular automated driving systems commonly handle prediction and planning as sequential separate tasks. While this accounts for the influence of surrounding traffic on the ego-vehicle, it fails to anticipate the reactions of traffic participants to the ego-vehicle's behavior. Recent works suggest that integrating prediction and planning in an interdependent joint step is necessary to achieve safe, efficient, and comfortable driving. While various models implement such integrated systems, a comprehensive overview and theoretical understanding of different principles are lacking. We systematically review state-of-the-art deep learning-based prediction, planning, and integrated prediction and planning models. Different facets of the integration ranging from model architecture and model design to behavioral aspects are considered and related to each other. Moreover, we discuss the implications, strengths, and limitations of different integration methods. By pointing out research gaps, describing relevant future challenges, and highlighting trends in the research field, we identify promising directions for future research.
</details>
<details>
<summary>摘要</summary>
（简化中文）自动驾驶有可能改变个人、公共和货物运输方式。除了巨大的感知挑战以外，自动驾驶还包括规划安全、舒适和有效的运动轨迹。为促进安全和进步，许多工作依赖于周围交通的未来运动预测。现有的自动驾驶系统通常将预测和规划作为独立的两个任务进行处理。这种方法虽然考虑了周围交通对ego汽车的影响，但是忽略了ego汽车行为对交通参与者的反应。 latest works suggest that integrating prediction and planning in an interdependent joint step is necessary to achieve safe, efficient, and comfortable driving. We systematically review state-of-the-art deep learning-based prediction, planning, and integrated prediction and planning models. Different facets of the integration ranging from model architecture and model design to behavioral aspects are considered and related to each other. Moreover, we discuss the implications, strengths, and limitations of different integration methods. By pointing out research gaps, describing relevant future challenges, and highlighting trends in the research field, we identify promising directions for future research.
</details></li>
</ul>
<hr>
<h2 id="EXPRESSO-A-Benchmark-and-Analysis-of-Discrete-Expressive-Speech-Resynthesis"><a href="#EXPRESSO-A-Benchmark-and-Analysis-of-Discrete-Expressive-Speech-Resynthesis" class="headerlink" title="EXPRESSO: A Benchmark and Analysis of Discrete Expressive Speech Resynthesis"></a>EXPRESSO: A Benchmark and Analysis of Discrete Expressive Speech Resynthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05725">http://arxiv.org/abs/2308.05725</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tu Anh Nguyen, Wei-Ning Hsu, Antony D’Avirro, Bowen Shi, Itai Gat, Maryam Fazel-Zarani, Tal Remez, Jade Copet, Gabriel Synnaeve, Michael Hassid, Felix Kreuk, Yossi Adi, Emmanuel Dupoux</li>
<li>for: 本研究旨在开发一个高质量的自然语言表达 speech 数据集，用于无文本 speech 生成。</li>
<li>methods: 研究人员使用了自我超级vised学习的方法，将低比特率的精度单元学习到 speech 中，以捕捉expressive aspect of speech。</li>
<li>results: 研究人员在 Expresso 数据集上进行了一系列的evalution，并结果表明，这种方法可以实现高质量的 expressive speech 生成。Here’s a more detailed explanation of each point:</li>
<li>for: The paper is aimed at developing a high-quality expressive speech dataset for textless speech synthesis.</li>
<li>methods: The researchers use self-supervised learning methods to learn low-bitrate discrete units that can capture expressive aspects of speech, such as prosody and voice styles.</li>
<li>results: The researchers evaluate the effectiveness of their approach on the Expresso dataset, which includes both read speech and improvised dialogues in 26 spontaneous expressive styles. The results show that the method can achieve high-quality expressive speech synthesis.<details>
<summary>Abstract</summary>
Recent work has shown that it is possible to resynthesize high-quality speech based, not on text, but on low bitrate discrete units that have been learned in a self-supervised fashion and can therefore capture expressive aspects of speech that are hard to transcribe (prosody, voice styles, non-verbal vocalization). The adoption of these methods is still limited by the fact that most speech synthesis datasets are read, severely limiting spontaneity and expressivity. Here, we introduce Expresso, a high-quality expressive speech dataset for textless speech synthesis that includes both read speech and improvised dialogues rendered in 26 spontaneous expressive styles. We illustrate the challenges and potentials of this dataset with an expressive resynthesis benchmark where the task is to encode the input in low-bitrate units and resynthesize it in a target voice while preserving content and style. We evaluate resynthesis quality with automatic metrics for different self-supervised discrete encoders, and explore tradeoffs between quality, bitrate and invariance to speaker and style. All the dataset, evaluation metrics and baseline models are open source
</details>
<details>
<summary>摘要</summary>
近期研究表明，可以使用低比特率不同单元进行自我指导的高质量语音重建。这些单元可以捕捉到表达方面的声音特征，如声音态度、声音风格和非语言 vocalization，这些特征Difficult to transcribe。然而，目前这些方法的应用仍然受限于大多数语音重建数据集是阅读的，因此减少了自由和表达力。在这篇文章中，我们介绍了一个高质量表达语音数据集，即Expresso，该数据集包括了阅读语音和自由对话，并且在26种自然表达风格下进行了渲染。我们介绍了这个数据集的挑战和潜在性，并在表达重建中进行了一种表达编码和重建任务，以测试不同的自我指导精度单元的质量。我们使用了自动化 метри来评估重建质量，并 explore了不同精度单元的平衡点，包括质量、比特率和对 speaker和风格的不变性。所有的数据集、评估指标和基础模型都是开源的。
</details></li>
</ul>
<hr>
<h2 id="Optimizing-Performance-of-Feedforward-and-Convolutional-Neural-Networks-through-Dynamic-Activation-Functions"><a href="#Optimizing-Performance-of-Feedforward-and-Convolutional-Neural-Networks-through-Dynamic-Activation-Functions" class="headerlink" title="Optimizing Performance of Feedforward and Convolutional Neural Networks through Dynamic Activation Functions"></a>Optimizing Performance of Feedforward and Convolutional Neural Networks through Dynamic Activation Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05724">http://arxiv.org/abs/2308.05724</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chinmay Rane, Kanishka Tyagi, Michael Manry</li>
<li>for: 本研究旨在探讨 activation functions 在 convolutional neural networks (CNNs) 中的影响，并提出了一种复杂的 Piece-wise Linear (PWL) 活动函数来取代 Relu 活动函数。</li>
<li>methods: 本研究使用了 PyTorch 框架进行实验，并对 shallow CNNs 和深度 CNNs 进行比较。</li>
<li>results: 研究发现，使用 PWL 活动函数可以大大提高 CNNs 的性能，并且在深度 CNNs 中提供了更多的可能性空间。<details>
<summary>Abstract</summary>
Deep learning training training algorithms are a huge success in recent years in many fields including speech, text,image video etc. Deeper and deeper layers are proposed with huge success with resnet structures having around 152 layers. Shallow convolution neural networks(CNN's) are still an active research, where some phenomena are still unexplained. Activation functions used in the network are of utmost importance, as they provide non linearity to the networks. Relu's are the most commonly used activation function.We show a complex piece-wise linear(PWL) activation in the hidden layer. We show that these PWL activations work much better than relu activations in our networks for convolution neural networks and multilayer perceptrons. Result comparison in PyTorch for shallow and deep CNNs are given to further strengthen our case.
</details>
<details>
<summary>摘要</summary>
深度学习训练算法在最近几年内在多个领域取得了巨大成功，包括语音、文本、图像和视频等。随着更深层的提议，深度学习模型在152层以上的ResNet结构中得到了巨大成功。 however， shallow convolutional neural networks（CNN）仍然是一个活跃的研究领域，一些现象仍未得到解释。 activation functions在网络中具有重要的作用，它们提供了非线性性，使网络更加复杂。 ReLU activation function是最常用的activation function。我们在隐藏层使用复杂的 piece-wise linear（PWL）activation，并证明这些PWL activation在我们的网络中工作得更好than ReLU activation。我们还在PyTorch中对 shallow和深度CNN进行比较，以更加强化我们的论据。
</details></li>
</ul>
<hr>
<h2 id="A-Comparison-of-Classical-and-Deep-Reinforcement-Learning-Methods-for-HVAC-Control"><a href="#A-Comparison-of-Classical-and-Deep-Reinforcement-Learning-Methods-for-HVAC-Control" class="headerlink" title="A Comparison of Classical and Deep Reinforcement Learning Methods for HVAC Control"></a>A Comparison of Classical and Deep Reinforcement Learning Methods for HVAC Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05711">http://arxiv.org/abs/2308.05711</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marshall Wang, John Willes, Thomas Jiralerspong, Matin Moezzi</li>
<li>for: 这个论文旨在使用强化学习（RL）方法优化冷却空调系统的控制，提高系统性能，降低能源消耗，提高成本效益。</li>
<li>methods: 这篇论文使用了两种流行的 класси型和深度RL方法（Q-学习和深度Q-网络），在多个冷却空调环境下进行了比较。</li>
<li>results: 研究发现，RL方法可以在冷却空调系统中提高性能，降低能源消耗，且模型参数选择和奖励调整是RL agents配置的关键因素。<details>
<summary>Abstract</summary>
Reinforcement learning (RL) is a promising approach for optimizing HVAC control. RL offers a framework for improving system performance, reducing energy consumption, and enhancing cost efficiency. We benchmark two popular classical and deep RL methods (Q-Learning and Deep-Q-Networks) across multiple HVAC environments and explore the practical consideration of model hyper-parameter selection and reward tuning. The findings provide insight for configuring RL agents in HVAC systems, promoting energy-efficient and cost-effective operation.
</details>
<details>
<summary>摘要</summary>
现代控制技术（Reinforcement Learning，RL）可以有效地优化冷暖空调系统的控制。RL提供了一个框架，可以提高系统性能，降低能源消耗，提高成本效益。我们在多个冷暖空调环境中对两种流行的古典RL和深度RL方法（Q-学习和深度Q网络）进行了比较，并探讨RL代理人在冷暖空调系统中的实用考虑和奖励调整。发现提供了RL代理人配置的指导，推动了能源减少和成本效益的操作。
</details></li>
</ul>
<hr>
<h2 id="Shadow-Datasets-New-challenging-datasets-for-Causal-Representation-Learning"><a href="#Shadow-Datasets-New-challenging-datasets-for-Causal-Representation-Learning" class="headerlink" title="Shadow Datasets, New challenging datasets for Causal Representation Learning"></a>Shadow Datasets, New challenging datasets for Causal Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05707">http://arxiv.org/abs/2308.05707</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Jiagengzhu/Shadow-dataset-for-crl">https://github.com/Jiagengzhu/Shadow-dataset-for-crl</a></li>
<li>paper_authors: Jiageng Zhu, Hanchen Xie, Jianhua Wu, Jiazhi Li, Mahyar Khayatkhoei, Mohamed E. Hussein, Wael AbdAlmageed</li>
<li>for: 本研究的目的是探索语义因素之间的 causal 关系，以便进行更好的表征学习。</li>
<li>methods: 该研究使用了weakly supervised causal representation learning（CRL）方法，以解决高成本的标注问题。</li>
<li>results: 研究提出了两个新的数据集，以及对现有数据集的修改，以满足更复杂的 causal 图和更多的生成因素的需求。<details>
<summary>Abstract</summary>
Discovering causal relations among semantic factors is an emergent topic in representation learning. Most causal representation learning (CRL) methods are fully supervised, which is impractical due to costly labeling. To resolve this restriction, weakly supervised CRL methods were introduced. To evaluate CRL performance, four existing datasets, Pendulum, Flow, CelebA(BEARD) and CelebA(SMILE), are utilized. However, existing CRL datasets are limited to simple graphs with few generative factors. Thus we propose two new datasets with a larger number of diverse generative factors and more sophisticated causal graphs. In addition, current real datasets, CelebA(BEARD) and CelebA(SMILE), the originally proposed causal graphs are not aligned with the dataset distributions. Thus, we propose modifications to them.
</details>
<details>
<summary>摘要</summary>
发现 semantic 因素之间的 causal 关系是 representation learning 中一个emerging topic。大多数 causal representation learning（CRL）方法是完全supervised，这是因为标注成本太高。为解决这种限制，我们提出了弱标注 CRL 方法。为评估 CRL 性能，我们使用了四个现有的数据集：Pendulum、Flow、CelebA（BEARD）和 CelebA（SMILE）。然而，现有的 CRL 数据集受限于简单的图与少量生成因素。因此，我们提出了两个新的数据集，它们具有更多的多样化生成因素和更复杂的 causal 图。此外，原始的现实数据集 CelebA（BEARD）和 CelebA（SMILE）的 causal 图与数据分布不一致。因此，我们提出了修改。
</details></li>
</ul>
<hr>
<h2 id="Hard-No-Box-Adversarial-Attack-on-Skeleton-Based-Human-Action-Recognition-with-Skeleton-Motion-Informed-Gradient"><a href="#Hard-No-Box-Adversarial-Attack-on-Skeleton-Based-Human-Action-Recognition-with-Skeleton-Motion-Informed-Gradient" class="headerlink" title="Hard No-Box Adversarial Attack on Skeleton-Based Human Action Recognition with Skeleton-Motion-Informed Gradient"></a>Hard No-Box Adversarial Attack on Skeleton-Based Human Action Recognition with Skeleton-Motion-Informed Gradient</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05681">http://arxiv.org/abs/2308.05681</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/luyg45/hardnoboxattack">https://github.com/luyg45/hardnoboxattack</a></li>
<li>paper_authors: Zhengzhi Lu, He Wang, Ziyi Chang, Guoan Yang, Hubert P. H. Shum</li>
<li>for: 这 paper 探讨了 skeleton-based 人体活动识别系统 的攻击性评估问题。</li>
<li>methods: 该 paper 使用了一种新的攻击任务，即攻击者没有访问受害者模型或训练数据或标签。具体来说，它们首先学习了一个动作抽象空间，然后定义了一种对抗损失来计算一个新的攻击方向，称为skeleton-motion-informed（SMI）梯度。这个梯度包含了动作动态信息，与现有的梯度基于攻击方法不同。</li>
<li>results: 该 paper 的实验和比较结果表明，SMI 梯度可以在无框杆和转移基于黑盒 Setting 中提高攻击性和透明度。<details>
<summary>Abstract</summary>
Recently, methods for skeleton-based human activity recognition have been shown to be vulnerable to adversarial attacks. However, these attack methods require either the full knowledge of the victim (i.e. white-box attacks), access to training data (i.e. transfer-based attacks) or frequent model queries (i.e. black-box attacks). All their requirements are highly restrictive, raising the question of how detrimental the vulnerability is. In this paper, we show that the vulnerability indeed exists. To this end, we consider a new attack task: the attacker has no access to the victim model or the training data or labels, where we coin the term hard no-box attack. Specifically, we first learn a motion manifold where we define an adversarial loss to compute a new gradient for the attack, named skeleton-motion-informed (SMI) gradient. Our gradient contains information of the motion dynamics, which is different from existing gradient-based attack methods that compute the loss gradient assuming each dimension in the data is independent. The SMI gradient can augment many gradient-based attack methods, leading to a new family of no-box attack methods. Extensive evaluation and comparison show that our method imposes a real threat to existing classifiers. They also show that the SMI gradient improves the transferability and imperceptibility of adversarial samples in both no-box and transfer-based black-box settings.
</details>
<details>
<summary>摘要</summary>
近期，基于骨架的人体活动识别方法已经被证明容易受到敌意攻击。然而，这些攻击方法都需要受害者（白盒攻击）、训练数据（传输基于攻击）或模型查询（黑盒攻击）的访问权限。这些需求都是非常限制的，这引发了对攻击性的评估。在这篇论文中，我们证明了这种攻击性确实存在。为此，我们提出了一个新的攻击任务：攻击者无法访问受害者的模型或训练数据或标签。我们称之为“困难无框攻击”（hard no-box attack）。我们首先学习了一个运动拟合，并定义了一种对抗损失来计算一个新的攻击Gradient，称之为skeleton-motion-informed（SMI）梯度。我们的梯度包含运动动力学信息，与现有的梯度基本攻击方法不同，它们计算损失梯度，假设每个数据维度独立。SMI梯度可以增强许多梯度基本攻击方法，导致一个新的无框攻击家族。我们的评估和比较表明，我们的方法对现有分类器 pose a real threat。它们还表明了SMI梯度的传播性和隐蔽性在无框和传输基于黑盒 Setting 中得到了改进。
</details></li>
</ul>
<hr>
<h2 id="Finding-Already-Debunked-Narratives-via-Multistage-Retrieval-Enabling-Cross-Lingual-Cross-Dataset-and-Zero-Shot-Learning"><a href="#Finding-Already-Debunked-Narratives-via-Multistage-Retrieval-Enabling-Cross-Lingual-Cross-Dataset-and-Zero-Shot-Learning" class="headerlink" title="Finding Already Debunked Narratives via Multistage Retrieval: Enabling Cross-Lingual, Cross-Dataset and Zero-Shot Learning"></a>Finding Already Debunked Narratives via Multistage Retrieval: Enabling Cross-Lingual, Cross-Dataset and Zero-Shot Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05680">http://arxiv.org/abs/2308.05680</a></li>
<li>repo_url: None</li>
<li>paper_authors: Iknoor Singh, Carolina Scarton, Xingyi Song, Kalina Bontcheva</li>
<li>for: 本研究的目的是探讨跨语言验证答案已经证明为假的故事的检测，以减少专业验证者的手动努力并为防止谣言的传播做出贡献。</li>
<li>methods: 本研究使用了一个新的数据集，该数据集包含了用于验证的检查答案和各种语言的社交媒体帖子，以便进行跨语言验证答案已经证明为假的故事的检测。</li>
<li>results: 研究发现，跨语言验证答案已经证明为假的故事的检测是一项具有挑战性的任务，而一些常用的跨语言预处理 transformer 模型也未能超越一个强的基于词语的基线（BM25）。然而，我们的多Stage检索框架在大多数情况下能够超越 BM25，并具有跨频率和零扩展学习的能力。<details>
<summary>Abstract</summary>
The task of retrieving already debunked narratives aims to detect stories that have already been fact-checked. The successful detection of claims that have already been debunked not only reduces the manual efforts of professional fact-checkers but can also contribute to slowing the spread of misinformation. Mainly due to the lack of readily available data, this is an understudied problem, particularly when considering the cross-lingual task, i.e. the retrieval of fact-checking articles in a language different from the language of the online post being checked. This paper fills this gap by (i) creating a novel dataset to enable research on cross-lingual retrieval of already debunked narratives, using tweets as queries to a database of fact-checking articles; (ii) presenting an extensive experiment to benchmark fine-tuned and off-the-shelf multilingual pre-trained Transformer models for this task; and (iii) proposing a novel multistage framework that divides this cross-lingual debunk retrieval task into refinement and re-ranking stages. Results show that the task of cross-lingual retrieval of already debunked narratives is challenging and off-the-shelf Transformer models fail to outperform a strong lexical-based baseline (BM25). Nevertheless, our multistage retrieval framework is robust, outperforming BM25 in most scenarios and enabling cross-domain and zero-shot learning, without significantly harming the model's performance.
</details>
<details>
<summary>摘要</summary>
这个任务是检索已经证伪的故事，目的是检测已经被ifact-checked的故事。成功检测已经证伪的故事不仅可以减少专业ifact-checker的手动努力，还可以减速谣言的传播。但因为数据不足，这个问题尚未得到充分研究，特别是跨语言任务，即在不同语言的 онлайн帖子被检查时，检索ifact-checking文章的跨语言任务。这篇论文填补这一漏洞，通过以下三个方法：1. 创建一个新的数据集，用于启动研究跨语言检索已经证伪的故事。2. 进行了广泛的实验，以benchmark fine-tuned和off-the-shelf多语言预训练Transformer模型。3. 提出了一个新的多阶段框架，将跨语言检索已经证伪的故事任务分为两个阶段：精度阶段和重新排序阶段。结果表明，跨语言检索已经证伪的故事是一个具有挑战性的任务，off-the-shelf Transformer模型无法超过一个强的基于词语的基准值（BM25）。然而，我们的多阶段检索框架具有坚固性，在大多数情况下超过BM25，并且允许跨频域和零shot学习，无需对模型性能产生重要的影响。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/11/cs.LG_2023_08_11/" data-id="clltau92q005vcr887bku1q73" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_08_11" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/11/cs.SD_2023_08_11/" class="article-date">
  <time datetime="2023-08-10T16:00:00.000Z" itemprop="datePublished">2023-08-11</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/11/cs.SD_2023_08_11/">cs.SD - 2023-08-11 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Improving-Joint-Speech-Text-Representations-Without-Alignment"><a href="#Improving-Joint-Speech-Text-Representations-Without-Alignment" class="headerlink" title="Improving Joint Speech-Text Representations Without Alignment"></a>Improving Joint Speech-Text Representations Without Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06125">http://arxiv.org/abs/2308.06125</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cal Peyser, Zhong Meng, Ke Hu, Rohit Prabhavalkar, Andrew Rosenberg, Tara N. Sainath, Michael Picheny, Kyunghyun Cho</li>
<li>for: 这个论文旨在提出一种基于modal space的文本生成方法，用于处理 speech和text两种不同的模式。</li>
<li>methods: 该方法使用joint speech-text encoder，通过在modal space中对文本和语音进行共同表示，以减少模型的参数量。</li>
<li>results: 该方法可以自动解决模式长度不同的问题，并且在下游WER测试中显示了良好的表现，包括单语言和多语言系统。<details>
<summary>Abstract</summary>
The last year has seen astonishing progress in text-prompted image generation premised on the idea of a cross-modal representation space in which the text and image domains are represented jointly. In ASR, this idea has found application as joint speech-text encoders that can scale to the capacities of very large parameter models by being trained on both unpaired speech and text. While these methods show promise, they have required special treatment of the sequence-length mismatch inherent in speech and text, either by up-sampling heuristics or an explicit alignment model. In this work, we offer evidence that joint speech-text encoders naturally achieve consistent representations across modalities by disregarding sequence length, and argue that consistency losses could forgive length differences and simply assume the best alignment. We show that such a loss improves downstream WER in both a large-parameter monolingual and multilingual system.
</details>
<details>
<summary>摘要</summary>
最近一年内，文本承词生成技术呈现了宏まScale的进步，基于跨Modal表示空间的想法，在这个空间中，文本和图像领域都被合并表示。在ASR中，这个想法得到应用，通过将语音和文本域合并编码，可以让模型 Parameters scale 到非常大的规模。虽然这些方法显示了承诺，但它们需要特殊地处理语音和文本序列长度的差异，通过上映或者显式对齐模型。在这种工作中，我们提供证据，表明Join speech-text编码器可以自然地实现多Modal的一致表示，而不需要注意序列长度。我们还 argues that consistency损失可以宽容序列长度差异，并且可以假设最佳对齐。我们示出，这种损失可以提高下游 WER 在大参数 monolingual 和 multilingual 系统中。
</details></li>
</ul>
<hr>
<h2 id="Lip2Vec-Efficient-and-Robust-Visual-Speech-Recognition-via-Latent-to-Latent-Visual-to-Audio-Representation-Mapping"><a href="#Lip2Vec-Efficient-and-Robust-Visual-Speech-Recognition-via-Latent-to-Latent-Visual-to-Audio-Representation-Mapping" class="headerlink" title="Lip2Vec: Efficient and Robust Visual Speech Recognition via Latent-to-Latent Visual to Audio Representation Mapping"></a>Lip2Vec: Efficient and Robust Visual Speech Recognition via Latent-to-Latent Visual to Audio Representation Mapping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06112">http://arxiv.org/abs/2308.06112</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yasser Abdelaziz Dahou Djilali, Sanath Narayan, Haithem Boussaid, Ebtessam Almazrouei, Merouane Debbah</li>
<li>For: 这个论文的目的是提出一种简单的方法，以便在视频序列中进行舌头语音识别（VSR）任务。这种方法可以在训练集外的挑战性enario中表现出色，而不需要大量的标注数据。* Methods: 这个论文使用了一种名为Lip2Vec的简单方法，该方法基于学习一个先验模型。该网络将视频序列中的舌头编码后的干扰表示与其对应的音频对的干扰表示进行映射。然后，使用一个市场上的音频识别模型来解码音频，并将其转化为文本。* Results: 根据LRS3数据集的测试结果，这种方法可以与完全监督学习方法相比，达到26个WER的水平。与State-of-the-Art（SoTA）方法不同，我们的模型在VoxCeleb测试集上保持了合理的性能。<details>
<summary>Abstract</summary>
Visual Speech Recognition (VSR) differs from the common perception tasks as it requires deeper reasoning over the video sequence, even by human experts. Despite the recent advances in VSR, current approaches rely on labeled data to fully train or finetune their models predicting the target speech. This hinders their ability to generalize well beyond the training set and leads to performance degeneration under out-of-distribution challenging scenarios. Unlike previous works that involve auxiliary losses or complex training procedures and architectures, we propose a simple approach, named Lip2Vec that is based on learning a prior model. Given a robust visual speech encoder, this network maps the encoded latent representations of the lip sequence to their corresponding latents from the audio pair, which are sufficiently invariant for effective text decoding. The generated audio representation is then decoded to text using an off-the-shelf Audio Speech Recognition (ASR) model. The proposed model compares favorably with fully-supervised learning methods on the LRS3 dataset achieving 26 WER. Unlike SoTA approaches, our model keeps a reasonable performance on the VoxCeleb test set. We believe that reprogramming the VSR as an ASR task narrows the performance gap between the two and paves the way for more flexible formulations of lip reading.
</details>
<details>
<summary>摘要</summary>
视觉语音识别（VSR）与常见的观察任务不同，因为它需要对视频序列进行更深入的理解，即使人类专家也需要这样做。尽管最近有大量的进步在VSR方面，但现在的方法仍然依赖于标注数据来完全训练或微调其模型，以预测目标语音。这会导致其在不同于训练集的情况下表现不佳，并且会导致性能下降。与之前的工作不同，我们提出了一种简单的方法，即Lip2Vec，它基于学习一个先验模型。给定一个强大的视觉语音编码器，这个网络将编码的舌唇序列的 latent 表示与它们对应的 audio 对应的 latent 表示进行映射，这些表示够具有效果的文本解码。生成的音频表示然后被解码成文本使用一个可用的 Audio Speech Recognition（ASR）模型。我们提出的模型与完全监督学习方法在 LRS3 数据集上比较 favorably，实现 26 WER。与 SoTA 方法不同，我们的模型在 VoxCeleb 测试集上保持了合理的性能。我们认为将 VSR 转换为 ASR 任务，将两者之间的性能差减少，并且开创了更 flexible 的舌唇读取形式。
</details></li>
</ul>
<hr>
<h2 id="An-Autoethnographic-Exploration-of-XAI-in-Algorithmic-Composition"><a href="#An-Autoethnographic-Exploration-of-XAI-in-Algorithmic-Composition" class="headerlink" title="An Autoethnographic Exploration of XAI in Algorithmic Composition"></a>An Autoethnographic Exploration of XAI in Algorithmic Composition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06089">http://arxiv.org/abs/2308.06089</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ashley Noel-Hirst, Nick Bryan-Kinns</li>
<li>for: 这篇论文旨在探讨如何使用可解释的人工智能（XAI）生成模型来创作传统音乐。</li>
<li>methods: 本研究使用MeasureVAE生成模型，该模型具有可解释的秘密维度，并在爱尔兰传统音乐上进行训练。</li>
<li>results: 研究发现，在音乐创作过程中，探索性的音乐创作 workflow 会强调音乐训练数据中的音乐特征，而不是生成模型本身的特征。这种应用XAI模型在创作过程中的可能性可能会扩展到更复杂和多样化的工作流程。<details>
<summary>Abstract</summary>
Machine Learning models are capable of generating complex music across a range of genres from folk to classical music. However, current generative music AI models are typically difficult to understand and control in meaningful ways. Whilst research has started to explore how explainable AI (XAI) generative models might be created for music, no generative XAI models have been studied in music making practice. This paper introduces an autoethnographic study of the use of the MeasureVAE generative music XAI model with interpretable latent dimensions trained on Irish folk music. Findings suggest that the exploratory nature of the music-making workflow foregrounds musical features of the training dataset rather than features of the generative model itself. The appropriation of an XAI model within an iterative workflow highlights the potential of XAI models to form part of a richer and more complex workflow than they were initially designed for.
</details>
<details>
<summary>摘要</summary>
machine learning模型可以生成复杂的音乐，从民族音乐到古典音乐。但当前的生成音乐AI模型通常难以理解和控制有意义的方式。研究已经开始探索如何创建音乐XAI生成模型，但没有任何生成XAI模型在音乐创作实践中被研究。本文介绍了一个自传式研究，使用MeasureVAE生成音乐XAI模型，具有可解释的幂等维度，在爱尔兰传统音乐上进行训练。发现结果表明，音乐创作工作流程的探索性强调了训练集音乐特征而不是生成模型本身的特征。将XAI模型包含在迭代工作流程中，表明XAI模型可以成为更加丰富和复杂的工作流程的一部分。
</details></li>
</ul>
<hr>
<h2 id="Audio-is-all-in-one-speech-driven-gesture-synthetics-using-WavLM-pre-trained-model"><a href="#Audio-is-all-in-one-speech-driven-gesture-synthetics-using-WavLM-pre-trained-model" class="headerlink" title="Audio is all in one: speech-driven gesture synthetics using WavLM pre-trained model"></a>Audio is all in one: speech-driven gesture synthetics using WavLM pre-trained model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05995">http://arxiv.org/abs/2308.05995</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fan Zhang, Naye Ji, Fuxing Gao, Siyuan Zhao, Zhaohan Wang, Shunman Li<br>for:* 这篇论文旨在创造数字人类的合作语言姿势，以解决现有的挑战，包括复杂的语音、语义和人性等因素。methods:* 该论文提出了一种基于扩散的DiffMotion-v2模型，利用RawSpeech音频 directly生成个性化和风格化的全身语姿，不需要复杂的多Modal处理和手动标注。results:* 经验证明，DiffMotion-v2模型可以生成自然的语姿，并且可以适应不同的风格和人性特征。<details>
<summary>Abstract</summary>
The generation of co-speech gestures for digital humans is an emerging area in the field of virtual human creation. Prior research has made progress by using acoustic and semantic information as input and adopting classify method to identify the person's ID and emotion for driving co-speech gesture generation. However, this endeavour still faces significant challenges. These challenges go beyond the intricate interplay between co-speech gestures, speech acoustic, and semantics; they also encompass the complexities associated with personality, emotion, and other obscure but important factors. This paper introduces "diffmotion-v2," a speech-conditional diffusion-based and non-autoregressive transformer-based generative model with WavLM pre-trained model. It can produce individual and stylized full-body co-speech gestures only using raw speech audio, eliminating the need for complex multimodal processing and manually annotated. Firstly, considering that speech audio not only contains acoustic and semantic features but also conveys personality traits, emotions, and more subtle information related to accompanying gestures, we pioneer the adaptation of WavLM, a large-scale pre-trained model, to extract low-level and high-level audio information. Secondly, we introduce an adaptive layer norm architecture in the transformer-based layer to learn the relationship between speech information and accompanying gestures. Extensive subjective evaluation experiments are conducted on the Trinity, ZEGGS, and BEAT datasets to confirm the WavLM and the model's ability to synthesize natural co-speech gestures with various styles.
</details>
<details>
<summary>摘要</summary>
<<SYS>>传统的虚拟人物创造领域中的协调姿势生成技术是一个emerging领域。先前的研究使用了音响和语义信息作为输入，采用分类方法来确定人的ID和情绪，以驱动协调姿势生成。然而，这一领域仍面临着重大挑战。这些挑战不仅包括协调姿势、音响和语义之间的细微互动，还包括人性、情绪和其他一些重要而不那么明确的因素。本文介绍了“diffmotion-v2”，一种基于 transformer 架构的 speech-conditional 协同扩散型生成模型，使用 WavLM 预训练模型。该模型可以通过 Raw speech 音频alone 生成具有个性化和风格化特点的全身协调姿势，无需进行复杂的多Modal 处理和手动标注。首先，我们认为 speech 音频不仅包含了音响和语义特征，还拥有人性特征、情绪特征和更为细微的协调姿势相关信息。因此，我们采用 WavLM 预训练模型来提取低级和高级 audio 信息。其次，我们引入了 transformer 架构中的 adaptive layer norm 层，以学习 speech 信息和协调姿势之间的关系。我们对 Trinity、ZEGGS 和 BEAT 等三个 dataset 进行了许多主观评估实验，以确认 WavLM 和模型的能力以生成自然的协调姿势。
</details></li>
</ul>
<hr>
<h2 id="Advancing-the-study-of-Large-Scale-Learning-in-Overlapped-Speech-Detection"><a href="#Advancing-the-study-of-Large-Scale-Learning-in-Overlapped-Speech-Detection" class="headerlink" title="Advancing the study of Large-Scale Learning in Overlapped Speech Detection"></a>Advancing the study of Large-Scale Learning in Overlapped Speech Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05987">http://arxiv.org/abs/2308.05987</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhaohui Yin, Jingguang Tian, Xinhui Hu, Xinkang Xu</li>
<li>for: 多个党人对话分析中的干扰语音检测（OSD）是一个重要的应用领域，但现有的大多数OSD模型都是基于特定的数据集进行训练和评估，限制了这些模型的应用场景。</li>
<li>methods: 我们提出了大规模学习（LSL）在OSD任务中的应用，并设计了一种16K单annelOSD模型。我们使用了522小时不同语言和风格的标注音频作为大规模数据集，并进行了严格的比较实验来评估LSL在OSD任务中的效果和不同深度神经网络基于OSD模型的性能。</li>
<li>results: 我们的实验结果表明，LSL可以显著提高OSD模型的性能和鲁棒性，并且CF-OSD与LSL在Alimeeting测试集和DIHARD II评估集上的F1分数分别达到了80.8%和52.0%，创造了当前最佳的16K单annelOSD模型。<details>
<summary>Abstract</summary>
Overlapped Speech Detection (OSD) is an important part of speech applications involving analysis of multi-party conversations. However, Most of the existing OSD models are trained and evaluated on specific dataset, which limits the application scenarios of these models. In order to solve this problem, we conduct a study of large-scale learning (LSL) in OSD and propose a more general 16K single-channel OSD model. In our study, 522 hours of labeled audio in different languages and styles are collected and used as the large-scale dataset. Rigorous comparative experiments are designed and used to evaluate the effectiveness of LSL in OSD task and the performance of OSD models based on different deep neural networks. The results show that LSL can significantly improve the performance and robustness of OSD models, and the OSD model based on Conformer (CF-OSD) with LSL is currently the best 16K single-channel OSD model. Moreover, the CF-OSD with LSL establishes a state-of-the-art performance with a F1-score of 80.8% and 52.0% on the Alimeeting test set and DIHARD II evaluation set, respectively.
</details>
<details>
<summary>摘要</summary>
大量学习（LSL）在对话分析中的另 overlap speech detection（OSD）是一个重要的部分，但大多数现有的OSD模型都是基于特定的数据集进行训练和评估，这限制了这些模型的应用场景。为解决这个问题，我们在OSD领域进行了大规模学习的研究，并提出了一个16K单annelOSD模型。在我们的研究中，我们收集了522小时的不同语言和风格的标注音频数据，并使用这些数据作为大规模数据集进行训练和测试。我们设计了严格的比较实验，以评估LSL在OSD任务中的效果和不同深度神经网络基于的OSD模型的性能。结果显示LSL可以显著提高OSD模型的性能和可靠性，并且CF-OSD WITH LSL目前是最佳的16K单annelOSD模型。此外，CF-OSD WITH LSL在Alimeeting测试集和DIHARD II评估集上的F1分数分别为80.8%和52.0%，创造了当前最佳的州态。
</details></li>
</ul>
<hr>
<h2 id="AudioLDM-2-Learning-Holistic-Audio-Generation-with-Self-supervised-Pretraining"><a href="#AudioLDM-2-Learning-Holistic-Audio-Generation-with-Self-supervised-Pretraining" class="headerlink" title="AudioLDM 2: Learning Holistic Audio Generation with Self-supervised Pretraining"></a>AudioLDM 2: Learning Holistic Audio Generation with Self-supervised Pretraining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05734">http://arxiv.org/abs/2308.05734</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/haoheliu/AudioLDM2">https://github.com/haoheliu/AudioLDM2</a></li>
<li>paper_authors: Haohe Liu, Qiao Tian, Yi Yuan, Xubo Liu, Xinhao Mei, Qiuqiang Kong, Yuping Wang, Wenwu Wang, Yuxuan Wang, Mark D. Plumbley</li>
<li>for: 这 paper 的目的是提出一种框架，用于将 speech、music 和 sound effect 等不同类型的声音生成模型共同拟合。</li>
<li>methods: 该框架使用同一种学习方法，通过 AudioMAE 自然适应学习模型和 latent diffusion 模型来翻译不同类型的声音，并在生成过程中进行自我监督学习。</li>
<li>results: 经验表明，该框架可以在主要的 benchmark 上达到新的 state-of-the-art 或竞争性的性能，并且具有很好的培化学习能力和可重用的自然适应学习模型。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Although audio generation shares commonalities across different types of audio, such as speech, music, and sound effects, designing models for each type requires careful consideration of specific objectives and biases that can significantly differ from those of other types. To bring us closer to a unified perspective of audio generation, this paper proposes a framework that utilizes the same learning method for speech, music, and sound effect generation. Our framework introduces a general representation of audio, called language of audio (LOA). Any audio can be translated into LOA based on AudioMAE, a self-supervised pre-trained representation learning model. In the generation process, we translate any modalities into LOA by using a GPT-2 model, and we perform self-supervised audio generation learning with a latent diffusion model conditioned on LOA. The proposed framework naturally brings advantages such as in-context learning abilities and reusable self-supervised pretrained AudioMAE and latent diffusion models. Experiments on the major benchmarks of text-to-audio, text-to-music, and text-to-speech demonstrate new state-of-the-art or competitive performance to previous approaches. Our demo and code are available at https://audioldm.github.io/audioldm2.
</details>
<details>
<summary>摘要</summary>
尽管各种听音都有共同之处，如speech、音乐和声音效果，但设计模型时需要仔细考虑每种类型的特定目标和偏见，这些偏见可能与其他类型异常大。为了带领我们更近到一个统一的听音生成视角，这篇论文提出了一个框架，该框架利用同一种学习方法来生成speech、音乐和声音效果。我们的框架引入了一个通用的听音表示，称为语言听音（LOA）。任何听音都可以根据AudioMAE自我超vised学习表示学习模型翻译为LOA。在生成过程中，我们使用GPT-2模型将任何Modalities翻译为LOA，然后使用一个conditional on LOA的隐藏噪声模型进行自我超vised听音生成学习。我们的提议的框架自然带来了在上下文学习能力和可重用的自我超vised Pre-trained AudioMAE和隐藏噪声模型的优点。我们的实验在主要的文本到听音、文本到音乐和文本到语音的标准 benchmarcks 上达到了新的状态码或竞争性的性能。我们的 demo 和代码可以在https://audioldm.github.io/audioldm2 上获取。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/11/cs.SD_2023_08_11/" data-id="clltau93s008ucr88dtdk1v1c" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_08_11" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/11/eess.IV_2023_08_11/" class="article-date">
  <time datetime="2023-08-10T16:00:00.000Z" itemprop="datePublished">2023-08-11</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/11/eess.IV_2023_08_11/">eess.IV - 2023-08-11 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Towards-Packaging-Unit-Detection-for-Automated-Palletizing-Tasks"><a href="#Towards-Packaging-Unit-Detection-for-Automated-Palletizing-Tasks" class="headerlink" title="Towards Packaging Unit Detection for Automated Palletizing Tasks"></a>Towards Packaging Unit Detection for Automated Palletizing Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06306">http://arxiv.org/abs/2308.06306</a></li>
<li>repo_url: None</li>
<li>paper_authors: Markus Völk, Kilian Kleeberger, Werner Kraus, Richard Bormann</li>
<li>for: 本研究旨在解决工业机器人处理包装单位前的检测步骤，即检测包装单位的具体步骤。</li>
<li>methods: 本研究使用synthetically生成的数据进行完全训练，可以对实际世界的包装单位进行Robust应用而无需进一步训练或设置繁重。该方法可以处理稀疏和低质量的感知数据，可以利用可用的先验知识，并在各种产品和应用场景中广泛适用。</li>
<li>results: 我们对实际世界数据进行了广泛评估，证明了我们的方法可以对各种不同的零售产品进行实际应用。此外，我们还将该方法integrated into a lab demonstrator，并通过工业伙伴将其商业化。<details>
<summary>Abstract</summary>
For various automated palletizing tasks, the detection of packaging units is a crucial step preceding the actual handling of the packaging units by an industrial robot. We propose an approach to this challenging problem that is fully trained on synthetically generated data and can be robustly applied to arbitrary real world packaging units without further training or setup effort. The proposed approach is able to handle sparse and low quality sensor data, can exploit prior knowledge if available and generalizes well to a wide range of products and application scenarios. To demonstrate the practical use of our approach, we conduct an extensive evaluation on real-world data with a wide range of different retail products. Further, we integrated our approach in a lab demonstrator and a commercial solution will be marketed through an industrial partner.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Self-supervised-SAR-Image-Despeckling-Strategy-Based-on-Parameter-sharing-Convolutional-Neural-Networks"><a href="#A-Self-supervised-SAR-Image-Despeckling-Strategy-Based-on-Parameter-sharing-Convolutional-Neural-Networks" class="headerlink" title="A Self-supervised SAR Image Despeckling Strategy Based on Parameter-sharing Convolutional Neural Networks"></a>A Self-supervised SAR Image Despeckling Strategy Based on Parameter-sharing Convolutional Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05975">http://arxiv.org/abs/2308.05975</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liang Chen, Yifei Yin, Hao Shi, Qingqing Sheng, Wei Li<br>for: The paper is written to address the challenges of SAR image despeckling, specifically the lack of speckle-free SAR images for training deep learning models.methods: The paper proposes a self-supervised SAR despeckling strategy that uses a sub-sampler based on the adjacent-syntropy criteria to generate training image pairs from real-world SAR images. The proposed method also uses parameter sharing convolutional neural networks and a multi-feature loss function to improve the preservation of edges and textures in the despeckled images.results: The proposed method is validated on real-world SAR images and shows better performance than several advanced SAR image despeckling methods, with improved preservation of edges and textures.Here’s the simplified Chinese text for the three information points:for: 本文是为了解决 SAR 图像抑杂问题，具体来说是为了缺乏 SAR 图像的杂点自由样本来训练深度学习模型。methods: 本文提出了一种基于邻域同异性 criteria 的自我监督 SAR 抑杂策略，使用这种策略可以从实际 SAR 图像中生成训练对数据。同时，本方法使用共享参数 convolutional neural networks 和多个特征损失函数，以提高杂点自由样本中的边缘和Texture特征保持。results: 提出的方法在实际 SAR 图像上进行了证明，与一些先进的 SAR 图像抑杂方法相比，显示更好的表现，同时保持了边缘和Texture特征。<details>
<summary>Abstract</summary>
Speckle noise is generated due to the SAR imaging mechanism, which brings difficulties in SAR image interpretation. Hence, despeckling is a helpful step in SAR pre-processing. Nowadays, deep learning has been proved to be a progressive method for SAR image despeckling. Most deep learning methods for despeckling are based on supervised learning, which needs original SAR images and speckle-free SAR images to train the network. However, the speckle-free SAR images are generally not available. So, this issue was tackled by adding multiplicative noise to optical images synthetically for simulating speckled image. Therefore, there are following challenges in SAR image despeckling: (1) lack of speckle-free SAR image; (2) difficulty in keeping details such as edges and textures in heterogeneous areas. To address these issues, we propose a self-supervised SAR despeckling strategy that can be trained without speckle-free images. Firstly, the feasibility of SAR image despeckling without speckle-free images is proved theoretically. Then, the sub-sampler based on the adjacent-syntropy criteria is proposed. The training image pairs are generated by the sub-sampler from real-word SAR image to estimate the noise distribution. Furthermore, to make full use of training pairs, the parameter sharing convolutional neural networks are adopted. Finally, according to the characteristics of SAR images, a multi-feature loss function is proposed. The proposed loss function is composed of despeckling term, regular term and perception term, to constrain the gap between the generated paired images. The ability of edge and texture feature preserving is improved simultaneously. Finally, qualitative and quantitative experiments are validated on real-world SAR images, showing better performances than several advanced SAR image despeckling methods.
</details>
<details>
<summary>摘要</summary>
亮点噪声是由SAR成像机制产生的，导致SAR图像解读困难。因此，去掉亮点噪声是SAR预处理的有助步骤。目前，深度学习已经被证明是SAR图像去掉亮点噪声的进步方法。大多数深度学习方法是基于直接学习，需要原始SAR图像和噪声自由SAR图像来训练网络。然而，噪声自由SAR图像通常不可获得。因此，这个问题是通过人工添加光学图像中的multiplicative噪声来解决的。因此，SAR图像去掉亮点噪声面临以下挑战：（1）缺乏噪声自由SAR图像；（2）在不同区域中保持细节，如边缘和文本ure。为解决这些问题，我们提出了一种自动化SAR图像去掉亮点噪声策略，不需要噪声自由SAR图像。首先，我们证明了SAR图像去掉亮点噪声可行性。然后，我们提出了基于邻域合理性 criteria的子批量器。通过这个子批量器，从实际世界SAR图像中生成训练图像对。然后，为了充分利用训练对，我们采用了参数共享卷积神经网络。最后，根据SAR图像的特点，我们提出了多特征损失函数。该损失函数由去掉亮点噪声项、常规项和感知项组成，以避免训练对的差异。同时，我们改进了边缘和文本ure的特征保持能力。最后，我们对实际世界SAR图像进行了质量和kvantalitative实验，并证明了我们的方法在去掉亮点噪声方面的更好性。
</details></li>
</ul>
<hr>
<h2 id="Classification-Method-of-Road-Surface-Condition-and-Type-with-LiDAR-Using-Spatiotemporal-Information"><a href="#Classification-Method-of-Road-Surface-Condition-and-Type-with-LiDAR-Using-Spatiotemporal-Information" class="headerlink" title="Classification Method of Road Surface Condition and Type with LiDAR Using Spatiotemporal Information"></a>Classification Method of Road Surface Condition and Type with LiDAR Using Spatiotemporal Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05965">http://arxiv.org/abs/2308.05965</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ju Won Seo, Jin Sung Kim, Chung Choo Chung</li>
<li>for: 这个论文提出了一种基于LiDAR的道路表面条件和类型分类方法，用于实时识别道路表面的状况和类型。</li>
<li>methods: 该方法使用了深度神经网络（DNN）来分类道路表面的不同区域，首先构建了每个子区域的特征向量，然后使用DNN进行分类。最后，输出的DNN结果通过空间时间处理得到了最终的分类结果，考虑了车速和概率等因素。</li>
<li>results: 与其他五种算法进行比较研究后，该方法在两个子区域附近车速的情况下获得了最高的准确率为98.0%和98.6%。此外，该方法还在实时环境中验证了可行性。<details>
<summary>Abstract</summary>
This paper proposes a spatiotemporal architecture with a deep neural network (DNN) for road surface conditions and types classification using LiDAR. It is known that LiDAR provides information on the reflectivity and number of point clouds depending on a road surface. Thus, this paper utilizes the information to classify the road surface. We divided the front road area into four subregions. First, we constructed feature vectors using each subregion's reflectivity, number of point clouds, and in-vehicle information. Second, the DNN classifies road surface conditions and types for each subregion. Finally, the output of the DNN feeds into the spatiotemporal process to make the final classification reflecting vehicle speed and probability given by the outcomes of softmax functions of the DNN output layer. To validate the effectiveness of the proposed method, we performed a comparative study with five other algorithms. With the proposed DNN, we obtained the highest accuracy of 98.0\% and 98.6\% for two subregions near the vehicle. In addition, we implemented the proposed method on the Jetson TX2 board to confirm that it is applicable in real-time.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:这篇论文提出了一种使用LiDAR的深度神经网络（DNN）来分类道路表面条件和类型。LiDAR提供了道路表面反射率和点云数据的信息，因此这篇论文利用这些信息来分类道路表面。前方道路区分成四个子区域，并构建了每个子区域的特征向量，包括反射率、点云数量和车辆内部信息。然后，DNN将每个子区域的特征向量分类为道路表面条件和类型。最后，DNN的输出Feed into spatiotemporal processto make the final classification, considering the vehicle speed and probability output by the softmax function of the DNN output layer. To validate the effectiveness of the proposed method, a comparative study with five other algorithms was conducted, and the DNN achieved the highest accuracy of 98.0% and 98.6% for two subregions near the vehicle. In addition, the proposed method was implemented on the Jetson TX2 board to confirm its applicability in real-time.
</details></li>
</ul>
<hr>
<h2 id="Unleashing-the-Strengths-of-Unlabeled-Data-in-Pan-cancer-Abdominal-Organ-Quantification-the-FLARE22-Challenge"><a href="#Unleashing-the-Strengths-of-Unlabeled-Data-in-Pan-cancer-Abdominal-Organ-Quantification-the-FLARE22-Challenge" class="headerlink" title="Unleashing the Strengths of Unlabeled Data in Pan-cancer Abdominal Organ Quantification: the FLARE22 Challenge"></a>Unleashing the Strengths of Unlabeled Data in Pan-cancer Abdominal Organ Quantification: the FLARE22 Challenge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05862">http://arxiv.org/abs/2308.05862</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/junma11/flare">https://github.com/junma11/flare</a></li>
<li>paper_authors: Jun Ma, Yao Zhang, Song Gu, Cheng Ge, Shihao Ma, Adamo Young, Cheng Zhu, Kangkang Meng, Xin Yang, Ziyan Huang, Fan Zhang, Wentao Liu, YuanKe Pan, Shoujin Huang, Jiacheng Wang, Mingze Sun, Weixin Xu, Dengqiang Jia, Jae Won Choi, Natália Alves, Bram de Wilde, Gregor Koehler, Yajun Wu, Manuel Wiesenfarth, Qiongjie Zhu, Guoqiang Dong, Jian He, the FLARE Challenge Consortium, Bo Wang</li>
<li>for:  This paper aims to evaluate the accuracy and efficiency of artificial intelligence (AI) algorithms in automated abdominal disease diagnosis and treatment planning.</li>
<li>methods:  The authors organized the FLARE 2022 Challenge, the largest abdominal organ analysis challenge to date, to benchmark fast, low-resource, accurate, annotation-efficient, and generalized AI algorithms. They constructed an intercontinental and multinational dataset from more than 50 medical groups, including Computed Tomography (CT) scans with different races, diseases, phases, and manufacturers.</li>
<li>results:  The best-performing algorithms achieved a median Dice Similarity Coefficient (DSC) of 90.0% on a holdout external validation set, and successfully generalized to different cohorts from North America, Europe, and Asia. They also enabled automatic extraction of key organ biology features, which was labor-intensive with traditional manual measurements.<details>
<summary>Abstract</summary>
Quantitative organ assessment is an essential step in automated abdominal disease diagnosis and treatment planning. Artificial intelligence (AI) has shown great potential to automatize this process. However, most existing AI algorithms rely on many expert annotations and lack a comprehensive evaluation of accuracy and efficiency in real-world multinational settings. To overcome these limitations, we organized the FLARE 2022 Challenge, the largest abdominal organ analysis challenge to date, to benchmark fast, low-resource, accurate, annotation-efficient, and generalized AI algorithms. We constructed an intercontinental and multinational dataset from more than 50 medical groups, including Computed Tomography (CT) scans with different races, diseases, phases, and manufacturers. We independently validated that a set of AI algorithms achieved a median Dice Similarity Coefficient (DSC) of 90.0\% by using 50 labeled scans and 2000 unlabeled scans, which can significantly reduce annotation requirements. The best-performing algorithms successfully generalized to holdout external validation sets, achieving a median DSC of 89.5\%, 90.9\%, and 88.3\% on North American, European, and Asian cohorts, respectively. They also enabled automatic extraction of key organ biology features, which was labor-intensive with traditional manual measurements. This opens the potential to use unlabeled data to boost performance and alleviate annotation shortages for modern AI models.
</details>
<details>
<summary>摘要</summary>
“量化器官评估是自动化腹部疾病诊断和治疗规划的关键步骤。人工智能（AI）已经表现出很大的潜力来自动化这个过程。然而，现有的大多数AI算法都需要许多专家标注，并且缺乏真实世界多国场景下的全面评估精度和效率。为了解决这些限制，我们在FLARE 2022挑战中组织了最大的腹部器官分析挑战，以测试快速、低资源、准确、标注效率高和泛化AI算法。我们构建了跨大陆和多国的数据集，包括不同的种族、疾病、阶段和生产商的计算Tomography（CT）扫描。我们独立验证了一组AI算法在50个医疗机构的数据上实现了 médiane的 dice相似度系数（DSC）90.0%，使用50个标注扫描和2000个无标注扫描。这些算法可以减少标注需求。最佳算法成功泛化到外部验证集上，实现了 médiane的 DSC为89.5%、90.9%和88.3%在北美、欧洲和亚洲相应的协会上。它们还允许自动提取关键器官生物特征，这是传统的手动测量强度劳紧的。这开启了使用无标注数据来提高性能的可能性，并alleviate标注不足的问题 для现代AI模型。”
</details></li>
</ul>
<hr>
<h2 id="End-to-End-Optimization-of-JPEG-Based-Deep-Learning-Process-for-Image-Classification"><a href="#End-to-End-Optimization-of-JPEG-Based-Deep-Learning-Process-for-Image-Classification" class="headerlink" title="End-to-End Optimization of JPEG-Based Deep Learning Process for Image Classification"></a>End-to-End Optimization of JPEG-Based Deep Learning Process for Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05840">http://arxiv.org/abs/2308.05840</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siyu Qi, Lahiru D. Chamain, Zhi Ding</li>
<li>for: 这篇论文应用于图像分类任务中的分布式学习，需要有效地将图像压缩成码码装置在低成本的感知设备上，以便优化传输和储存。</li>
<li>methods: 本研究提出了一个统一的端到端训练模型，结合了JPEG图像压缩器和深度学习（DL）的分类器。这个模型可以调整通过大量部署的JPEG编码器设置，以提高分类精度，同时考虑到带宽限制。</li>
<li>results: 我们在CIFAR-100和ImageNet上进行了测试，结果显示，这个模型可以提高验证精度，比于预先设定的JPEG配置。<details>
<summary>Abstract</summary>
Among major deep learning (DL) applications, distributed learning involving image classification require effective image compression codecs deployed on low-cost sensing devices for efficient transmission and storage. Traditional codecs such as JPEG designed for perceptual quality are not configured for DL tasks. This work introduces an integrative end-to-end trainable model for image compression and classification consisting of a JPEG image codec and a DL-based classifier. We demonstrate how this model can optimize the widely deployed JPEG codec settings to improve classification accuracy in consideration of bandwidth constraint. Our tests on CIFAR-100 and ImageNet also demonstrate improved validation accuracy over preset JPEG configuration.
</details>
<details>
<summary>摘要</summary>
中文简体深度学习（DL）应用中的分布式学习，涉及到图像分类，需要有效的图像压缩编码器在低成本感知设备上部署，以提高传输和存储的效率。传统的编码器如JPEG，设计为 perceive 质量，并不适用于 DL 任务。这项工作介绍了一种综合性的末端到终端可调模型，包括 JPEG 图像编码器和基于 DL 的分类器。我们示出了如何这种模型可以优化广泛部署的 JPEG 编码器设置，以提高分类精度，同时考虑带宽约束。我们在 CIFAR-100 和 ImageNet 上进行了测试，并证明了该模型在预设 JPEG 配置下有更高的验证精度。
</details></li>
</ul>
<hr>
<h2 id="Spatial-Pathomics-Toolkit-for-Quantitative-Analysis-of-Podocyte-Nuclei-with-Histology-and-Spatial-Transcriptomics-Data-in-Renal-Pathology"><a href="#Spatial-Pathomics-Toolkit-for-Quantitative-Analysis-of-Podocyte-Nuclei-with-Histology-and-Spatial-Transcriptomics-Data-in-Renal-Pathology" class="headerlink" title="Spatial Pathomics Toolkit for Quantitative Analysis of Podocyte Nuclei with Histology and Spatial Transcriptomics Data in Renal Pathology"></a>Spatial Pathomics Toolkit for Quantitative Analysis of Podocyte Nuclei with Histology and Spatial Transcriptomics Data in Renal Pathology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06288">http://arxiv.org/abs/2308.06288</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hrlblab/spatial_pathomics">https://github.com/hrlblab/spatial_pathomics</a></li>
<li>paper_authors: Jiayuan Chen, Yu Wang, Ruining Deng, Quan Liu, Can Cui, Tianyuan Yao, Yilin Liu, Jianyong Zhong, Agnes B. Fogo, Haichun Yang, Shilin Zhao, Yuankai Huo<br>for:这篇论文的目的是为了开发一个用于评估肾脏病变的新型工具kit，帮助研究人员更好地评估肾脏细胞的形态特征。methods:这篇论文使用的方法包括：一、实例对象分割，帮助精准地 identific podocyte核oid; 二、pathomics特征生成，从分割后的核oid中提取了一系列量化特征; 三、robust统计分析，以便对这些量化特征进行全面的空间分析。results:这篇论文通过使用SPT工具kit，成功地提取和分析了许多podocyte形态特征，并通过统计分析发现了许多与肾脏病变相关的空间特征。<details>
<summary>Abstract</summary>
Podocytes, specialized epithelial cells that envelop the glomerular capillaries, play a pivotal role in maintaining renal health. The current description and quantification of features on pathology slides are limited, prompting the need for innovative solutions to comprehensively assess diverse phenotypic attributes within Whole Slide Images (WSIs). In particular, understanding the morphological characteristics of podocytes, terminally differentiated glomerular epithelial cells, is crucial for studying glomerular injury. This paper introduces the Spatial Pathomics Toolkit (SPT) and applies it to podocyte pathomics. The SPT consists of three main components: (1) instance object segmentation, enabling precise identification of podocyte nuclei; (2) pathomics feature generation, extracting a comprehensive array of quantitative features from the identified nuclei; and (3) robust statistical analyses, facilitating a comprehensive exploration of spatial relationships between morphological and spatial transcriptomics features.The SPT successfully extracted and analyzed morphological and textural features from podocyte nuclei, revealing a multitude of podocyte morphomic features through statistical analysis. Additionally, we demonstrated the SPT's ability to unravel spatial information inherent to podocyte distribution, shedding light on spatial patterns associated with glomerular injury. By disseminating the SPT, our goal is to provide the research community with a powerful and user-friendly resource that advances cellular spatial pathomics in renal pathology. The implementation and its complete source code of the toolkit are made openly accessible at https://github.com/hrlblab/spatial_pathomics.
</details>
<details>
<summary>摘要</summary>
PODO细胞，特殊化的胶质细胞，环绕肾脏小血管，对肾健康具有重要作用。现有的描述和量化方法有限，需要创新的解决方案来全面评估多样性特征。特别是理解PODO细胞的形态特征，对研究肾脏伤害非常重要。本文介绍了SPT工具箱（Spatial Pathomics Toolkit），并应用于PODO细胞pathomics。SPT包括三个主要组成部分：（1）实体对象分割，准确地识别PODO细胞核心；（2）pathomics特征生成，从分割的PODO细胞核心中提取广泛的量化特征；以及（3）Robust统计分析，促进了广泛的空间关系探索。SPT成功地提取和分析PODO细胞形态和文化特征，揭示了许多PODO细胞形态特征，并通过统计分析，探索了PODO细胞分布的空间信息。我们的目标是通过散布SPT，为研究社区提供一个强大且易用的资源，以提高细胞空间patomics在肾 pathology中的应用。SPT的实现和完整的源代码在https://github.com/hrlblab/spatial_pathomics上公开访问。
</details></li>
</ul>
<hr>
<h2 id="Leverage-Weakly-Annotation-to-Pixel-wise-Annotation-via-Zero-shot-Segment-Anything-Model-for-Molecular-empowered-Learning"><a href="#Leverage-Weakly-Annotation-to-Pixel-wise-Annotation-via-Zero-shot-Segment-Anything-Model-for-Molecular-empowered-Learning" class="headerlink" title="Leverage Weakly Annotation to Pixel-wise Annotation via Zero-shot Segment Anything Model for Molecular-empowered Learning"></a>Leverage Weakly Annotation to Pixel-wise Annotation via Zero-shot Segment Anything Model for Molecular-empowered Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05785">http://arxiv.org/abs/2308.05785</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xueyuan Li, Ruining Deng, Yucheng Tang, Shunxing Bao, Haichun Yang, Yuankai Huo</li>
<li>for: 这个研究的目的是为了开发一个能够实现多种细胞类型的识别，并且可以避免专家 annotators 的时间负担的方法。</li>
<li>methods: 这个研究使用了一种名为 Segment Anything Model (SAM) 的新型模型，它可以将单一的矩形标注转换为多个精确的像素标注。这些 SAM-generated 标注可以用于训练一个深度学习模型，以进行细胞类型的分类。</li>
<li>results: 研究发现，使用 SAM-assisted molecular-empowered learning (SAM-L) 可以将这些细胞类型的标注工作从专家 annotators 转移到非专家 annotators 身上，而且不需要专家 annotators 的时间负担。此外，SAM-L 可以保持 annotation accuracy 和深度学习模型的性能。这个研究代表了一个重要的进步，可以帮助普及化细胞类型的标注过程，并且仅需非专家 annotators 的帮助。<details>
<summary>Abstract</summary>
Precise identification of multiple cell classes in high-resolution Giga-pixel whole slide imaging (WSI) is critical for various clinical scenarios. Building an AI model for this purpose typically requires pixel-level annotations, which are often unscalable and must be done by skilled domain experts (e.g., pathologists). However, these annotations can be prone to errors, especially when distinguishing between intricate cell types (e.g., podocytes and mesangial cells) using only visual inspection. Interestingly, a recent study showed that lay annotators, when using extra immunofluorescence (IF) images for reference (referred to as molecular-empowered learning), can sometimes outperform domain experts in labeling. Despite this, the resource-intensive task of manual delineation remains a necessity during the annotation process. In this paper, we explore the potential of bypassing pixel-level delineation by employing the recent segment anything model (SAM) on weak box annotation in a zero-shot learning approach. Specifically, we harness SAM's ability to produce pixel-level annotations from box annotations and utilize these SAM-generated labels to train a segmentation model. Our findings show that the proposed SAM-assisted molecular-empowered learning (SAM-L) can diminish the labeling efforts for lay annotators by only requiring weak box annotations. This is achieved without compromising annotation accuracy or the performance of the deep learning-based segmentation. This research represents a significant advancement in democratizing the annotation process for training pathological image segmentation, relying solely on non-expert annotators.
</details>
<details>
<summary>摘要</summary>
高分辨率整幕扫描图像（WSI）中多个细胞类型的精确识别是许多临床应用场景中的关键。建立一个人工智能模型用于此目的通常需要像素级别的注释，但这些注释通常是不可扩展的，而且需要具有专业知识的域专家（例如病理学家）进行完成。然而，这些注释可能会受到误差的影响，特别是在使用 только视觉检查时分辨between intricate cell types（例如 podocytes 和 mesangial cells）。有趣的是，一项最近的研究发现，使用extra immunofluorescence（IF）图像作为参考（被称为分子力学学习），lay annotators可以在标注时与域专家相比而出现较好的表现。尽管这样，手动分割任务仍然是注释过程中的必需任务。在这篇文章中，我们探讨了可以通过跳过像素级别分割来使用最近的segment anything模型（SAM）在零扩学习方法中。我们利用SAM的能力生成像素级别标注从box标注，并使用这些SAM生成的标注来训练分 segmentation模型。我们的发现表明，提案的SAM-assisted molecular-empowered learning（SAM-L）可以使lay annotators只需要弱box标注，而无需投入大量的标注时间和努力。这是在不损失标注准确性或深度学习基于分 segmentation模型的性能下实现的。这种研究表明了让非专业注释者参与标注过程的可能性，只需要依靠于非专业注释者。
</details></li>
</ul>
<hr>
<h2 id="High-performance-Data-Management-for-Whole-Slide-Image-Analysis-in-Digital-Pathology"><a href="#High-performance-Data-Management-for-Whole-Slide-Image-Analysis-in-Digital-Pathology" class="headerlink" title="High-performance Data Management for Whole Slide Image Analysis in Digital Pathology"></a>High-performance Data Management for Whole Slide Image Analysis in Digital Pathology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05784">http://arxiv.org/abs/2308.05784</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hrlblab/adios">https://github.com/hrlblab/adios</a></li>
<li>paper_authors: Haoju Leng, Ruining Deng, Shunxing Bao, Dazheng Fang, Bryan A. Millis, Yucheng Tang, Haichun Yang, Xiao Wang, Yifan Peng, Lipeng Wan, Yuankai Huo</li>
<li>For:  This paper focuses on addressing the data access challenge in giga-pixel digital pathology using the Adaptable IO System version 2 (ADIOS2).* Methods: The authors implement a digital pathology-centric pipeline using ADIOS2 and develop strategies to curtail data retrieval times.* Results: The paper shows a two-fold speed-up in the CPU scenario and performance on par with NVIDIA Magnum IO GPU Direct Storage (GDS) in the GPU scenario, making it one of the initial instances of using ADIOS2 in digital pathology.<details>
<summary>Abstract</summary>
When dealing with giga-pixel digital pathology in whole-slide imaging, a notable proportion of data records holds relevance during each analysis operation. For instance, when deploying an image analysis algorithm on whole-slide images (WSI), the computational bottleneck often lies in the input-output (I/O) system. This is particularly notable as patch-level processing introduces a considerable I/O load onto the computer system. However, this data management process could be further paralleled, given the typical independence of patch-level image processes across different patches. This paper details our endeavors in tackling this data access challenge by implementing the Adaptable IO System version 2 (ADIOS2). Our focus has been constructing and releasing a digital pathology-centric pipeline using ADIOS2, which facilitates streamlined data management across WSIs. Additionally, we've developed strategies aimed at curtailing data retrieval times. The performance evaluation encompasses two key scenarios: (1) a pure CPU-based image analysis scenario ("CPU scenario"), and (2) a GPU-based deep learning framework scenario ("GPU scenario"). Our findings reveal noteworthy outcomes. Under the CPU scenario, ADIOS2 showcases an impressive two-fold speed-up compared to the brute-force approach. In the GPU scenario, its performance stands on par with the cutting-edge GPU I/O acceleration framework, NVIDIA Magnum IO GPU Direct Storage (GDS). From what we know, this appears to be among the initial instances, if any, of utilizing ADIOS2 within the field of digital pathology. The source code has been made publicly available at https://github.com/hrlblab/adios.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Multi-scale-Multi-site-Renal-Microvascular-Structures-Segmentation-for-Whole-Slide-Imaging-in-Renal-Pathology"><a href="#Multi-scale-Multi-site-Renal-Microvascular-Structures-Segmentation-for-Whole-Slide-Imaging-in-Renal-Pathology" class="headerlink" title="Multi-scale Multi-site Renal Microvascular Structures Segmentation for Whole Slide Imaging in Renal Pathology"></a>Multi-scale Multi-site Renal Microvascular Structures Segmentation for Whole Slide Imaging in Renal Pathology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05782">http://arxiv.org/abs/2308.05782</a></li>
<li>repo_url: None</li>
<li>paper_authors: Franklin Hu, Ruining Deng, Shunxing Bao, Haichun Yang, Yuankai Huo</li>
<li>for:  automatización de la segmentación de estructuras microvasculares en imágenes de tejido whole slide de riñón humano, como arteriolas, venulas y capilares, para mejorar la análisis cuantitativo en patología renal.</li>
<li>methods:  utilizó una red neuronal dinámica llamada Omni-Seg, que se entrenó con datos multisitio y multescale, y se benefició de etiquetas parciales en las imágenes de entrenamiento.</li>
<li>results:  el método Omni-Seg outperformió en términos de coeficiente de semejanza de Dice y de intersectión sobre unión, lo que demuestra su eficacia en la segmentación automática de microvasculares en imágenes de riñón.<details>
<summary>Abstract</summary>
Segmentation of microvascular structures, such as arterioles, venules, and capillaries, from human kidney whole slide images (WSI) has become a focal point in renal pathology. Current manual segmentation techniques are time-consuming and not feasible for large-scale digital pathology images. While deep learning-based methods offer a solution for automatic segmentation, most suffer from a limitation: they are designed for and restricted to training on single-site, single-scale data. In this paper, we present Omni-Seg, a novel single dynamic network method that capitalizes on multi-site, multi-scale training data. Unique to our approach, we utilize partially labeled images, where only one tissue type is labeled per training image, to segment microvascular structures. We train a singular deep network using images from two datasets, HuBMAP and NEPTUNE, across different magnifications (40x, 20x, 10x, and 5x). Experimental results indicate that Omni-Seg outperforms in terms of both the Dice Similarity Coefficient (DSC) and Intersection over Union (IoU). Our proposed method provides renal pathologists with a powerful computational tool for the quantitative analysis of renal microvascular structures.
</details>
<details>
<summary>摘要</summary>
Segmentation of microvascular structures, such as arterioles, venules, and capillaries, from human kidney whole slide images (WSI) has become a focal point in renal pathology. Current manual segmentation techniques are time-consuming and not feasible for large-scale digital pathology images. While deep learning-based methods offer a solution for automatic segmentation, most suffer from a limitation: they are designed for and restricted to training on single-site, single-scale data. In this paper, we present Omni-Seg, a novel single dynamic network method that capitalizes on multi-site, multi-scale training data. Unique to our approach, we utilize partially labeled images, where only one tissue type is labeled per training image, to segment microvascular structures. We train a singular deep network using images from two datasets, HuBMAP and NEPTUNE, across different magnifications (40x, 20x, 10x, and 5x). Experimental results indicate that Omni-Seg outperforms in terms of both the Dice Similarity Coefficient (DSC) and Intersection over Union (IoU). Our proposed method provides renal pathologists with a powerful computational tool for the quantitative analysis of renal microvascular structures.Here's the text in Traditional Chinese:过去的数位patology图像（WSI）中的微血管结构，例如arterioles、venules和capillaries，的分类已成为肾脏病理学的焦点。现有的手动分类技术是时间consuming且不适合大规模数位病理图像。而深度学习基础的方法则提供了自动分类的解决方案，但大多数方法受到一个限制：它们仅适用于单一站点、单一比例的数据。在这篇文章中，我们提出了Omni-Seg方法，这是一个单一动态网络方法，它利用多个站点、多个比例的训练数据。我们在两个数据集（HuBMAP和NEPTUNE）上进行了不同的放大（40x、20x、10x和5x）。实验结果显示Omni-Seg在Dice相似度系数（DSC）和交集过Union（IoU）方面都表现出色。我们的提议方法为肾脏病理学家提供了一个强大的计算工具，用于肾脏微血管结构的量化分析。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/11/eess.IV_2023_08_11/" data-id="clltau95a00dpcr884hkr6jcb" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_10" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/10/cs.LG_2023_08_10/" class="article-date">
  <time datetime="2023-08-09T16:00:00.000Z" itemprop="datePublished">2023-08-10</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/10/cs.LG_2023_08_10/">cs.LG - 2023-08-10 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="AST-MHSA-Code-Summarization-using-Multi-Head-Self-Attention"><a href="#AST-MHSA-Code-Summarization-using-Multi-Head-Self-Attention" class="headerlink" title="AST-MHSA : Code Summarization using Multi-Head Self-Attention"></a>AST-MHSA : Code Summarization using Multi-Head Self-Attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05646">http://arxiv.org/abs/2308.05646</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yeshwanth Nagaraj, Ujjwal Gupta</li>
<li>for:  Code summarization</li>
<li>methods:  Multi-head attention mechanism to extract important semantic information from AST, encoder-decoder architecture</li>
<li>results:  More comprehensive summaries of code with reduced computational overhead<details>
<summary>Abstract</summary>
Code summarization aims to generate concise natural language descriptions for source code. The prevailing approaches adopt transformer-based encoder-decoder architectures, where the Abstract Syntax Tree (AST) of the source code is utilized for encoding structural information. However, ASTs are much longer than the corresponding source code, and existing methods ignore this size constraint by directly feeding the entire linearized AST into the encoders. This simplistic approach makes it challenging to extract truly valuable dependency relations from the overlong input sequence and leads to significant computational overhead due to self-attention applied to all nodes in the AST.   To address this issue effectively and efficiently, we present a model, AST-MHSA that uses multi-head attention to extract the important semantic information from the AST. The model consists of two main components: an encoder and a decoder. The encoder takes as input the abstract syntax tree (AST) of the code and generates a sequence of hidden states. The decoder then takes these hidden states as input and generates a natural language summary of the code.   The multi-head attention mechanism allows the model to learn different representations of the input code, which can be combined to generate a more comprehensive summary. The model is trained on a dataset of code and summaries, and the parameters of the model are optimized to minimize the loss between the generated summaries and the ground-truth summaries.
</details>
<details>
<summary>摘要</summary>
code 摘要目标是生成源代码的自然语言描述。现有的方法多采用 transformer 基于 encoder-decoder 架构，其中源代码的抽象 syntax tree (AST) 用于编码结构信息。然而，AST 比源代码更长，现有方法直接将整个 linearized AST  feed into encoders，这种简单的方法使得EXTRACTING valuable dependency relations FROM THE OVERLONG INPUT SEQUENCE 困难，并且会产生巨大的计算开销由于所有节点 self-attention。To address this issue effectively and efficiently, we present a model called AST-MHSA that uses multi-head attention to extract important semantic information from the AST. The model consists of two main components: an encoder and a decoder. The encoder takes the AST of the code as input and generates a sequence of hidden states. The decoder then takes these hidden states as input and generates a natural language summary of the code.The multi-head attention mechanism allows the model to learn different representations of the input code, which can be combined to generate a more comprehensive summary. The model is trained on a dataset of code and summaries, and the parameters of the model are optimized to minimize the loss between the generated summaries and the ground-truth summaries.
</details></li>
</ul>
<hr>
<h2 id="IIHT-Medical-Report-Generation-with-Image-to-Indicator-Hierarchical-Transformer"><a href="#IIHT-Medical-Report-Generation-with-Image-to-Indicator-Hierarchical-Transformer" class="headerlink" title="IIHT: Medical Report Generation with Image-to-Indicator Hierarchical Transformer"></a>IIHT: Medical Report Generation with Image-to-Indicator Hierarchical Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05633">http://arxiv.org/abs/2308.05633</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keqiang Fan, Xiaohao Cai, Mahesan Niranjan</li>
<li>for: 这个研究旨在提出一个基于图像转换器的医疗报告生成方法，以便帮助医生更快速和更准确地生成医疗报告。</li>
<li>methods: 这个方法使用了一个图像转换器框架，包括三个模组：分类模组、指标扩展模组和生成模组。分类模组首先从医疗图像中提取图像特征，然后生成疾病相关的指标，并将这些指标转换为文本形式。生成模组则使用这些提取的特征和图像特征作为助け，以生成最终的医疗报告。</li>
<li>results: 实验结果显示，提出的方法可以实现高度的医疗报告生成精度，并且比起现有方法更具有语言流畅性和医学准确性。<details>
<summary>Abstract</summary>
Automated medical report generation has become increasingly important in medical analysis. It can produce computer-aided diagnosis descriptions and thus significantly alleviate the doctors' work. Inspired by the huge success of neural machine translation and image captioning, various deep learning methods have been proposed for medical report generation. However, due to the inherent properties of medical data, including data imbalance and the length and correlation between report sequences, the generated reports by existing methods may exhibit linguistic fluency but lack adequate clinical accuracy. In this work, we propose an image-to-indicator hierarchical transformer (IIHT) framework for medical report generation. It consists of three modules, i.e., a classifier module, an indicator expansion module and a generator module. The classifier module first extracts image features from the input medical images and produces disease-related indicators with their corresponding states. The disease-related indicators are subsequently utilised as input for the indicator expansion module, incorporating the "data-text-data" strategy. The transformer-based generator then leverages these extracted features along with image features as auxiliary information to generate final reports. Furthermore, the proposed IIHT method is feasible for radiologists to modify disease indicators in real-world scenarios and integrate the operations into the indicator expansion module for fluent and accurate medical report generation. Extensive experiments and comparisons with state-of-the-art methods under various evaluation metrics demonstrate the great performance of the proposed method.
</details>
<details>
<summary>摘要</summary>
自动化医疗报告生成已成为医学分析中越来越重要的一环。它可以生成计算机辅助诊断描述，从而减轻医生的工作负担。靠着神经机器翻译和图像描述的成功，各种深度学习方法已经被提议用于医疗报告生成。然而，由于医疗数据的内在性，包括数据不均衡和报告序列长度和相关性，所生成的报告可能具备语言流畅性，但缺乏实际的医学准确性。在这种情况下，我们提出了一种图像指标层次转换器（IIHT）框架，用于医疗报告生成。该框架包括三个模块：分类模块、指标扩展模块和生成模块。首先，分类模块从输入医学图像中提取图像特征，并生成相关疾病的指标，以及其状态。然后，指标扩展模块使用“数据-文本-数据”策略，将指标扩展为更多的疾病特征。最后，使用转换器生成器，通过这些提取的特征和图像特征作为辅助信息，生成最终的报告。此外，我们的IIHT方法可以让医生在实际应用中修改疾病指标，并将操作集成到指标扩展模块中，以实现流畅和准确的医疗报告生成。我们的实验和与当前最佳方法的比较，在不同的评价指标下，都显示了我们的方法的优秀性。
</details></li>
</ul>
<hr>
<h2 id="ReLU-and-Addition-based-Gated-RNN"><a href="#ReLU-and-Addition-based-Gated-RNN" class="headerlink" title="ReLU and Addition-based Gated RNN"></a>ReLU and Addition-based Gated RNN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05629">http://arxiv.org/abs/2308.05629</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rickard Brännvall, Henrik Forsgren, Fredrik Sandin, Marcus Liwicki</li>
<li>for: 减少计算成本，提高效率，以适用于受限硬件或各种权限系统。</li>
<li>methods: 使用添加和ReLU活化函数取代乘法和对数函数，以维持长期记忆并 capture long-term dependencies。</li>
<li>results: 实验结果表明，提档的门控机制可以在 synthetic sequence 学习任务中保持长期记忆，同时减少计算成本，其执行时间比 conventinal LSTM 和 GRU 减少一半（CPU）和一半（加密）。<details>
<summary>Abstract</summary>
We replace the multiplication and sigmoid function of the conventional recurrent gate with addition and ReLU activation. This mechanism is designed to maintain long-term memory for sequence processing but at a reduced computational cost, thereby opening up for more efficient execution or larger models on restricted hardware. Recurrent Neural Networks (RNNs) with gating mechanisms such as LSTM and GRU have been widely successful in learning from sequential data due to their ability to capture long-term dependencies. Conventionally, the update based on current inputs and the previous state history is each multiplied with dynamic weights and combined to compute the next state. However, multiplication can be computationally expensive, especially for certain hardware architectures or alternative arithmetic systems such as homomorphic encryption. It is demonstrated that the novel gating mechanism can capture long-term dependencies for a standard synthetic sequence learning task while significantly reducing computational costs such that execution time is reduced by half on CPU and by one-third under encryption. Experimental results on handwritten text recognition tasks furthermore show that the proposed architecture can be trained to achieve comparable accuracy to conventional GRU and LSTM baselines. The gating mechanism introduced in this paper may enable privacy-preserving AI applications operating under homomorphic encryption by avoiding the multiplication of encrypted variables. It can also support quantization in (unencrypted) plaintext applications, with the potential for substantial performance gains since the addition-based formulation can avoid the expansion to double precision often required for multiplication.
</details>
<details>
<summary>摘要</summary>
我们将传统的数 multiplication 和 sigmoid 函数更换为加法和ReLU 活化。这个机制可以保持序列处理中的长期记忆，但是降低计算成本，因此可以在更高效的执行或更大的模型中进行更多的硬件限制。回传神经网络（RNN）具有阀门机制，如 LSTM 和 GRU，在序列资料上学习了很成功，因为它们可以捕捉长期依赖。在传统的更新方式中，基于目前的输入和前一个状态历史的更新是multiplied with dynamic weights，然后合并以计算下一个状态。但是，Multiplication 可以是计算昂费的，尤其是在某些硬件架构或替代运算系统，如数学加密。在这篇论文中，我们提出了一个新的阀门机制，可以在标准的人工 синтеctic sequence 学习任务中捕捉长期依赖，并且将计算成本降低了一半在 CPU 上，并且在加密下降低了一个三分之一。实验结果显示，我们的架构可以与传统 GRU 和 LSTM 基准点进行相互比较，并且在手写文本识别任务中进行训练。这个阀门机制可以实现隐私保护的 AI 应用，例如在数学加密下运行，并且可以支持量化（ plaintext 应用中），这可能将带来重大的性能提升，因为加法形式可以避免对 double precision 的扩展，通常需要 multiplication。
</details></li>
</ul>
<hr>
<h2 id="Normalized-Gradients-for-All"><a href="#Normalized-Gradients-for-All" class="headerlink" title="Normalized Gradients for All"></a>Normalized Gradients for All</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05621">http://arxiv.org/abs/2308.05621</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Johnnywang1899/Credit_Risk_Analysis">https://github.com/Johnnywang1899/Credit_Risk_Analysis</a></li>
<li>paper_authors: Francesco Orabona</li>
<li>for: 这篇论文主要是用于掌握Holder平坦性的方法。</li>
<li>methods: 这篇论文使用了normalized gradients来实现Black-box方式下的Holder平坦性适应。另外， bound的计算还基于一个新的地方Holder平坦性的定义。主要思想来自Levy [2017]。</li>
<li>results: 论文得到了一个基于localHolder平坦性的bound，这个bound取决于一个新的地方Holder平坦性的定义。<details>
<summary>Abstract</summary>
In this short note, I show how to adapt to H\"{o}lder smoothness using normalized gradients in a black-box way. Moreover, the bound will depend on a novel notion of local H\"{o}lder smoothness. The main idea directly comes from Levy [2017].
</details>
<details>
<summary>摘要</summary>
这短い详细说明中，我们介绍了如何适应Holder平滑性使用 норциали化的梯度。此外， bound 的取值也取决于一种新的地方Holder平滑性。主要的想法直接来自Levy [2017]。Here's the breakdown of the translation:* "Holder smoothness" is translated as "Holder平滑性" (holder smoothness)* "normalized gradients" is translated as "norмаль化梯度" (normalized gradients)* "black-box way" is translated as "黑盒方式" (black-box way)* "novel notion of local H\"{o}lder smoothness" is translated as "一种新的地方Holder平滑性" (novel notion of local Holder smoothness)* "Levy [2017]" is translated as "Levy [2017]" (Levy [2017])
</details></li>
</ul>
<hr>
<h2 id="Updating-Clinical-Risk-Stratification-Models-Using-Rank-Based-Compatibility-Approaches-for-Evaluating-and-Optimizing-Clinician-Model-Team-Performance"><a href="#Updating-Clinical-Risk-Stratification-Models-Using-Rank-Based-Compatibility-Approaches-for-Evaluating-and-Optimizing-Clinician-Model-Team-Performance" class="headerlink" title="Updating Clinical Risk Stratification Models Using Rank-Based Compatibility: Approaches for Evaluating and Optimizing Clinician-Model Team Performance"></a>Updating Clinical Risk Stratification Models Using Rank-Based Compatibility: Approaches for Evaluating and Optimizing Clinician-Model Team Performance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05619">http://arxiv.org/abs/2308.05619</a></li>
<li>repo_url: None</li>
<li>paper_authors: Erkin Ötleş, Brian T. Denton, Jenna Wiens</li>
<li>for: 这篇论文的目的是提出一种新的rank-based兼容度指标($C^R$)和一种新的损失函数，用于优化风险分化模型的兼容性和推理性。</li>
<li>methods: 该论文使用了现有的模型选择技术和一种新的损失函数，并在MIMIC数据集上进行了实验 validate the proposed approach.</li>
<li>results: 相比 existed model selection techniques, the proposed approach yielded more compatible models while maintaining discriminative performance, with an increase in $C^R$ of $0.019$ ($95%$ confidence interval: $0.005$, $0.035$).<details>
<summary>Abstract</summary>
As data shift or new data become available, updating clinical machine learning models may be necessary to maintain or improve performance over time. However, updating a model can introduce compatibility issues when the behavior of the updated model does not align with user expectations, resulting in poor user-model team performance. Existing compatibility measures depend on model decision thresholds, limiting their applicability in settings where models are used to generate rankings based on estimated risk. To address this limitation, we propose a novel rank-based compatibility measure, $C^R$, and a new loss function that aims to optimize discriminative performance while encouraging good compatibility. Applied to a case study in mortality risk stratification leveraging data from MIMIC, our approach yields more compatible models while maintaining discriminative performance compared to existing model selection techniques, with an increase in $C^R$ of $0.019$ ($95\%$ confidence interval: $0.005$, $0.035$). This work provides new tools to analyze and update risk stratification models used in clinical care.
</details>
<details>
<summary>摘要</summary>
As data shifts or new data becomes available, updating clinical machine learning models may be necessary to maintain or improve performance over time. However, updating a model can introduce compatibility issues when the behavior of the updated model does not align with user expectations, resulting in poor user-model team performance. Existing compatibility measures depend on model decision thresholds, limiting their applicability in settings where models are used to generate rankings based on estimated risk. To address this limitation, we propose a novel rank-based compatibility measure, $C^R$, and a new loss function that aims to optimize discriminative performance while encouraging good compatibility. Applied to a case study in mortality risk stratification leveraging data from MIMIC, our approach yields more compatible models while maintaining discriminative performance compared to existing model selection techniques, with an increase in $C^R$ of $0.019$ ($95\%$ confidence interval: $0.005$, $0.035$). This work provides new tools to analyze and update risk stratification models used in clinical care.Here's the translation in Traditional Chinese:当数据shift或新数据可用时，对于供应链机器学习模型的更新可能是必要的，以维持或改善性能。然而，更新模型可能会导致用户预期不符的行为，导致用户模型团队性能差。现有的兼容度标准依赖于模型决策阈值，仅适用于基于估计风险的情况下。为了解决这些限制，我们提出了一个新的排名基于兼容度度量，$C^R$, 以及一个新的损失函数，旨在优化推理性能的同时鼓励好兼容。对于基于MIMIC的致死风险分组案例，我们的方法产生了更兼容的模型，保持了推理性能的同时，与现有的模型选择技术相比，$C^R$ 增加了0.019 ($95\%$ 信度interval: 0.005，0.035）。这个工作提供了新的工具来分析和更新在医疗保健中使用的风险分组模型。
</details></li>
</ul>
<hr>
<h2 id="Multi-graph-Spatio-temporal-Graph-Convolutional-Network-for-Traffic-Flow-Prediction"><a href="#Multi-graph-Spatio-temporal-Graph-Convolutional-Network-for-Traffic-Flow-Prediction" class="headerlink" title="Multi-graph Spatio-temporal Graph Convolutional Network for Traffic Flow Prediction"></a>Multi-graph Spatio-temporal Graph Convolutional Network for Traffic Flow Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05601">http://arxiv.org/abs/2308.05601</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weilong Ding, Tianpu Zhang, Jianwu Wang, Zhuofeng Zhao</li>
<li>for: 这篇论文主要是为了提出一种用于每日高速公路交通流量预测的深度学习方法。</li>
<li>methods: 该方法使用数据归一化策略处理数据不均衡问题，然后使用图 convolutional 网络捕捉空间时间特征。此外，还使用天气和历法特征在全连接Stage中增加外部特征。</li>
<li>results: 对一个中国省级高速公路进行了详细实验和案例研究，结果显示该方法与基eline比较，有明显的预测精度提升和实质性的商业应用优势。<details>
<summary>Abstract</summary>
Inter-city highway transportation is significant for urban life. As one of the key functions in intelligent transportation system (ITS), traffic evaluation always plays significant role nowadays, and daily traffic flow prediction still faces challenges at network-wide toll stations. On the one hand, the data imbalance in practice among various locations deteriorates the performance of prediction. On the other hand, complex correlative spatio-temporal factors cannot be comprehensively employed in long-term duration. In this paper, a prediction method is proposed for daily traffic flow in highway domain through spatio-temporal deep learning. In our method, data normalization strategy is used to deal with data imbalance, due to long-tail distribution of traffic flow at network-wide toll stations. And then, based on graph convolutional network, we construct networks in distinct semantics to capture spatio-temporal features. Beside that, meteorology and calendar features are used by our model in the full connection stage to extra external characteristics of traffic flow. By extensive experiments and case studies in one Chinese provincial highway, our method shows clear improvement in predictive accuracy than baselines and practical benefits in business.
</details>
<details>
<summary>摘要</summary>
城市间高速交通是城市生活中非常重要的一环。作为智能交通系统（ITS）中一项关键功能，交通评估总是在当今得到重要的应用，而日常交通流量预测仍然面临着网络覆盖站的挑战。一方面，实际应用中的数据不均衡问题使预测性能下降。另一方面，复杂的相关空间时间因素难以长期内部涵盖。本文提出了基于高速公路域的日常交通流量预测方法，使用数据归一化策略处理数据不均衡问题，并基于图 convolutional network 构建不同 semantics 的网络，捕捉空间时间特征。此外，我们的模型还在全连接阶段使用气象和历法特征，以捕捉交通流量的外部特征。经过广泛的实验和案例研究，我们的方法在一个中国省级高速公路上显示出了明显的预测精度提高和实践效益。
</details></li>
</ul>
<hr>
<h2 id="NUPES-Non-Uniform-Post-Training-Quantization-via-Power-Exponent-Search"><a href="#NUPES-Non-Uniform-Post-Training-Quantization-via-Power-Exponent-Search" class="headerlink" title="NUPES : Non-Uniform Post-Training Quantization via Power Exponent Search"></a>NUPES : Non-Uniform Post-Training Quantization via Power Exponent Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05600">http://arxiv.org/abs/2308.05600</a></li>
<li>repo_url: None</li>
<li>paper_authors: Edouard Yvinec, Arnaud Dapogny, Kevin Bailly<br>for: 这篇论文的目的是提出一种改进现有深度神经网络（DNN）优化技术，以提高优化后的预测性能。methods: 这篇论文使用了一种名为“自适应映射”的技术，将浮点数表示转换为低位元数字表示，以减少DNN模型的内存负载和延迟。此外，这篇论文还使用了一种名为“权函数”的技术，将DNN模型中的权值和活化函数转换为低位元数字表示。results: 这篇论文获得了顶尖的压缩率，并且可以在没有数据验证和数据验证下运行。此外，这篇论文还提出了一新的优化方法，可以在训练过程中对优化后的模型进行优化，以提高预测性能。<details>
<summary>Abstract</summary>
Deep neural network (DNN) deployment has been confined to larger hardware devices due to their expensive computational requirements. This challenge has recently reached another scale with the emergence of large language models (LLMs). In order to reduce both their memory footprint and latency, a promising technique is quantization. It consists in converting floating point representations to low bit-width fixed point representations, usually by assuming a uniform mapping onto a regular grid. This process, referred to in the literature as uniform quantization, may however be ill-suited as most DNN weights and activations follow a bell-shaped distribution. This is even worse on LLMs whose weight distributions are known to exhibit large, high impact, outlier values. In this work, we propose an improvement over the most commonly adopted way to tackle this limitation in deep learning models quantization, namely, non-uniform quantization. NUPES leverages automorphisms to preserve the scalar multiplications. Such transformations are derived from power functions. However, the optimization of the exponent parameter and weight values remains a challenging and novel problem which could not be solved with previous post training optimization techniques which only learn to round up or down weight values in order to preserve the predictive function. We circumvent this limitation with a new paradigm: learning new quantized weights over the entire quantized space. Similarly, we enable the optimization of the power exponent, i.e. the optimization of the quantization operator itself during training by alleviating all the numerical instabilities. The resulting predictive function is compatible with integer-only low-bit inference. We show the ability of the method to achieve state-of-the-art compression rates in both, data-free and data-driven configurations.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）的部署因其高效计算需求而受限于更大的硬件设备。这一挑战最近又由大型自然语言模型（LLM）的出现加剧。以减少它们的内存占用量和延迟，一种有前途的技术是量化。它通过将浮点表示转换为低位数字表示，通常通过假设一个固定格式的映射来实现。这个过程被称为固定量化，但是可能不适合大多数DNN的权重和活动值，因为它们通常follows a bell-shaped distribution。这个问题更加严重，因为LLMs的权重分布知道存在大、高影响的异常值。在这种情况下，我们提出一种改进了深度学习模型量化的方法，即非均匀量化（NUPES）。NUPES利用自同构来保持整数乘法。这些转换是基于力函数的。然而，优化剩余因子和权重值的问题仍然是一个挑战，而且不可以通过以前的训练优化技术解决，这些技术只是学习将权重值略减或略加以保持预测函数的正确性。我们绕过这个限制，通过学习新的量化权重，在整数乘法下保持预测函数的正确性。同时，我们启用量化运算符的优化，即在训练中优化量化操作符的权重和剩余因子。这些优化可以减少所有数值不稳定性。我们展示了该方法可以在数据驱动和数据隐藏配置下实现国际只有low-bit执行的预测函数，并且达到了国际最佳压缩率。
</details></li>
</ul>
<hr>
<h2 id="Symmetry-Defense-Against-XGBoost-Adversarial-Perturbation-Attacks"><a href="#Symmetry-Defense-Against-XGBoost-Adversarial-Perturbation-Attacks" class="headerlink" title="Symmetry Defense Against XGBoost Adversarial Perturbation Attacks"></a>Symmetry Defense Against XGBoost Adversarial Perturbation Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05575">http://arxiv.org/abs/2308.05575</a></li>
<li>repo_url: None</li>
<li>paper_authors: Blerta Lindqvist</li>
<li>for: This paper aims to defend tree-based ensemble classifiers like gradient-boosting decision trees (GBDTs) against adversarial perturbation attacks by utilizing the lack of invariance with respect to symmetries.</li>
<li>methods: The paper uses a symmetry defense approach that has been previously used for convolutional neural networks (CNNs) and applies it to GBDTs. The defense mechanism relies on the fact that GBDTs lack invariance with respect to symmetries, and it uses this lack of invariance to revert the incorrect classification of adversarial samples.</li>
<li>results: The paper evaluates the GBDT symmetry defense for nine datasets against six perturbation attacks with a threat model that ranges from zero-knowledge to perfect-knowledge adversaries. The results show that the defense mechanism can achieve up to 100% accuracy on adversarial samples even when default and robust classifiers have 0% accuracy, and up to over 95% accuracy on adversarial samples for the GBDT classifier of the F-MNIST dataset even when default and robust classifiers have 0% accuracy.Here is the information in Simplified Chinese text:</li>
<li>for: 这 paper 的目的是为了防御基于树的集成分类器，如Gradient-Boosting Decision Trees (GBDTs)，对于攻击性扰动攻击。</li>
<li>methods: 这 paper 使用了一种基于对称的防御方法，该方法已经在卷积神经网络 (CNNs) 上使用，并将其应用到 GBDTs 上。防御机制基于 GBDTs 缺乏对称性，并利用这种缺乏对称性来 revert 攻击样本的错误分类。</li>
<li>results: 这 paper 对 nine 个数据集进行了六种攻击，并评估了 GBDT 防御机制的性能。结果表明，防御机制可以在 zero-knowledge 到 perfect-knowledge 攻击者的威胁模型下，对 adversarial 样本进行100%的正确分类，以及对 F-MNIST 数据集的 GBDT 分类器进行95%以上的正确分类。<details>
<summary>Abstract</summary>
We examine whether symmetry can be used to defend tree-based ensemble classifiers such as gradient-boosting decision trees (GBDTs) against adversarial perturbation attacks. The idea is based on a recent symmetry defense for convolutional neural network classifiers (CNNs) that utilizes CNNs' lack of invariance with respect to symmetries. CNNs lack invariance because they can classify a symmetric sample, such as a horizontally flipped image, differently from the original sample. CNNs' lack of invariance also means that CNNs can classify symmetric adversarial samples differently from the incorrect classification of adversarial samples. Using CNNs' lack of invariance, the recent CNN symmetry defense has shown that the classification of symmetric adversarial samples reverts to the correct sample classification. In order to apply the same symmetry defense to GBDTs, we examine GBDT invariance and are the first to show that GBDTs also lack invariance with respect to symmetries. We apply and evaluate the GBDT symmetry defense for nine datasets against six perturbation attacks with a threat model that ranges from zero-knowledge to perfect-knowledge adversaries. Using the feature inversion symmetry against zero-knowledge adversaries, we achieve up to 100% accuracy on adversarial samples even when default and robust classifiers have 0% accuracy. Using the feature inversion and horizontal flip symmetries against perfect-knowledge adversaries, we achieve up to over 95% accuracy on adversarial samples for the GBDT classifier of the F-MNIST dataset even when default and robust classifiers have 0% accuracy.
</details>
<details>
<summary>摘要</summary>
我们研究使用对称来防御基于树状集成分类器（GBDT）的对抗攻击。这个想法基于现有的对称防御技术，该技术利用对称隐藏层（CNN）的不变性。CNNlacks变换不变性，这意味着它可以将水平翻转的图像分类为不同的样本，而不是原始样本。此外，CNN的不变性还意味着它可以将对称攻击样本分类为错误的样本。使用CNN的不变性，这个新的对称防御技术可以使得对称攻击样本的分类恢复到正确的样本分类。为了应用该技术到GBDT中，我们首先检查GBDT的不变性，并发现GBDT也缺乏对称性。我们应用和评估了GBDT对九个数据集的对称防御技术，并对六种攻击方法进行评估，包括零知识到完美知识的攻击者。使用对称性对零知识攻击者，我们达到了100%的正确率。使用对称和水平翻转 symmetry对完美知识攻击者，我们在F-MNIST数据集上达到了95%以上的正确率。
</details></li>
</ul>
<hr>
<h2 id="AutoGluon-TimeSeries-AutoML-for-Probabilistic-Time-Series-Forecasting"><a href="#AutoGluon-TimeSeries-AutoML-for-Probabilistic-Time-Series-Forecasting" class="headerlink" title="AutoGluon-TimeSeries: AutoML for Probabilistic Time Series Forecasting"></a>AutoGluon-TimeSeries: AutoML for Probabilistic Time Series Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05566">http://arxiv.org/abs/2308.05566</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oleksandr Shchur, Caner Turkmen, Nick Erickson, Huibin Shen, Alexander Shirkov, Tony Hu, Yuyang Wang</li>
<li>for: 这篇论文主要是为了提出一个开源的AutoML库，用于机会时间序列预测。</li>
<li>methods: 这篇论文使用了AutoGluon的设计哲学， combinig了传统的统计模型、机器学习基于预测方法、和ensemble技术。</li>
<li>results: 在29个benchmark dataset上进行评估，这篇论文展示了强大的实验性表现，在点预测和量预测方面都高于了一些预测方法，并且经常超越了最佳对照方法的结合。<details>
<summary>Abstract</summary>
We introduce AutoGluon-TimeSeries - an open-source AutoML library for probabilistic time series forecasting. Focused on ease of use and robustness, AutoGluon-TimeSeries enables users to generate accurate point and quantile forecasts with just 3 lines of Python code. Built on the design philosophy of AutoGluon, AutoGluon-TimeSeries leverages ensembles of diverse forecasting models to deliver high accuracy within a short training time. AutoGluon-TimeSeries combines both conventional statistical models, machine-learning based forecasting approaches, and ensembling techniques. In our evaluation on 29 benchmark datasets, AutoGluon-TimeSeries demonstrates strong empirical performance, outperforming a range of forecasting methods in terms of both point and quantile forecast accuracy, and often even improving upon the best-in-hindsight combination of prior methods.
</details>
<details>
<summary>摘要</summary>
我们介绍AutoGluon-TimeSeries - 一个开源AutoML库 для潜在时间序列预测。我们专注在使用方便和可靠性，使用3行Python代码可以生成高精度的点和量测预测。基于AutoGluon的设计哲学，AutoGluon-TimeSeries 利用多种不同预测模型的ensemble，以提供高精度的预测，仅需训练时间短。AutoGluon-TimeSeries 结合了传统的统计学模型、机器学习基于预测方法、和 ensemble技术。在我们的29个benchmark数据集评估中，AutoGluon-TimeSeries 示出了强大的实验性表现，在点和量测预测精度方面高于许多预测方法，并经常超过最佳组合的先前方法。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Variational-Inference-for-Large-Skew-t-Copulas-with-Application-to-Intraday-Equity-Returns"><a href="#Efficient-Variational-Inference-for-Large-Skew-t-Copulas-with-Application-to-Intraday-Equity-Returns" class="headerlink" title="Efficient Variational Inference for Large Skew-t Copulas with Application to Intraday Equity Returns"></a>Efficient Variational Inference for Large Skew-t Copulas with Application to Intraday Equity Returns</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05564">http://arxiv.org/abs/2308.05564</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lin Deng, Michael Stanley Smith, Worapree Maneesoonthorn</li>
<li>for: 这个论文旨在提出一种基于skew-t分布的高维束合模型，用于金融数据的模型化，因为这种模型允许对各对依存关系进行偏置和极端尾部依存。</li>
<li>methods: 这篇论文使用了一种基于束合变分的bayesian变分推断（VI）方法来估算高维skew-t分布。这种方法使用一种具有条件 Gaussian 发现的skew-t分布来定义一个增强后验，可以准确地估算高维束合模型。</li>
<li>results: 研究人员使用了这种新方法来估算2017年至2021年的93只美国股票的高维束合模型。结果显示，这种模型能够很好地捕捉到股票对之间的偏置和极端尾部依存，同时也能够更好地预测股票的实时返佣分布。此外，基于估算的对依存关系的股票组合策略也能够提高股票投资的性能。<details>
<summary>Abstract</summary>
Large skew-t factor copula models are attractive for the modeling of financial data because they allow for asymmetric and extreme tail dependence. We show that the copula implicit in the skew-t distribution of Azzalini and Capitanio (2003) allows for a higher level of pairwise asymmetric dependence than two popular alternative skew-t copulas. Estimation of this copula in high dimensions is challenging, and we propose a fast and accurate Bayesian variational inference (VI) approach to do so. The method uses a conditionally Gaussian generative representation of the skew-t distribution to define an augmented posterior that can be approximated accurately. A fast stochastic gradient ascent algorithm is used to solve the variational optimization. The new methodology is used to estimate copula models for intraday returns from 2017 to 2021 on 93 U.S. equities. The copula captures substantial heterogeneity in asymmetric dependence over equity pairs, in addition to the variability in pairwise correlations. We show that intraday predictive densities from the skew-t copula are more accurate than from some other copula models, while portfolio selection strategies based on the estimated pairwise tail dependencies improve performance relative to the benchmark index.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:大skew-t因子 copula模型是金融数据模型中吸引人的，因为它允许非对称和极端尾部依赖。我们表明，Azzalini和Capitanio（2003）中的skew-t分布下的 copula允许更高的对称依赖，than two popular alternative skew-t copulas。估计这种 copula 在高维度是挑战性的，我们提议一种快速和准确的 Bayesian variational inference（VI）方法来实现。该方法使用 conditionally Gaussian 生成表示法来定义增强 posterior，可以高度准确地 aproximate。一种快速的梯度下降算法用于解决variational优化。我们使用这种新方法来估计2017-2021年的93只美国股票的copula模型。该 copula 捕捉了股票对的非对称依赖和对比股票对的相关性的巨大多样性。我们表明，从skew-t copula 中的预测概率密度比其他 copula 模型更准确，而基于估计的对称尾部依赖而实现的股票选择策略也能够超越 referential 指数。
</details></li>
</ul>
<hr>
<h2 id="Critical-Points-An-Agile-Point-Cloud-Importance-Measure-for-Robust-Classification-Adversarial-Defense-and-Explainable-AI"><a href="#Critical-Points-An-Agile-Point-Cloud-Importance-Measure-for-Robust-Classification-Adversarial-Defense-and-Explainable-AI" class="headerlink" title="Critical Points ++: An Agile Point Cloud Importance Measure for Robust Classification, Adversarial Defense and Explainable AI"></a>Critical Points ++: An Agile Point Cloud Importance Measure for Robust Classification, Adversarial Defense and Explainable AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05525">http://arxiv.org/abs/2308.05525</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yossilevii100/critical_points2">https://github.com/yossilevii100/critical_points2</a></li>
<li>paper_authors: Meir Yossef Levi, Guy Gilboa</li>
<li>for: 本研究旨在提高实际应用中对异常样本的处理精度和速度，并 investigate critical points of 3D point clouds and their relationship with out-of-distribution (OOD) samples.</li>
<li>methods: 本文提出了一种基于重要性度量的方法，即使用训练分类网络仅使用不重要点进行训练，以提高模型的Robustness，并使用 норма化 entropy 来选择不重要点。</li>
<li>results: 研究结果表明，使用提出的方法可以在Robust Classification和抗击攻击任务上达到顶峰性能，并且可以快速和准确地处理异常样本。<details>
<summary>Abstract</summary>
The ability to cope accurately and fast with Out-Of-Distribution (OOD) samples is crucial in real-world safety demanding applications. In this work we first study the interplay between critical points of 3D point clouds and OOD samples. Our findings are that common corruptions and outliers are often interpreted as critical points. We generalize the notion of critical points into importance measures. We show that training a classification network based only on less important points dramatically improves robustness, at a cost of minor performance loss on the clean set. We observe that normalized entropy is highly informative for corruption analysis. An adaptive threshold based on normalized entropy is suggested for selecting the set of uncritical points. Our proposed importance measure is extremely fast to compute. We show it can be used for a variety of applications, such as Explainable AI (XAI), Outlier Removal, Uncertainty Estimation, Robust Classification and Adversarial Defense. We reach SOTA results on the two latter tasks. Code is available at: https://github.com/yossilevii100/critical_points2
</details>
<details>
<summary>摘要</summary>
能够快速和准确处理 OUT-OF-DISTRIBUTION（OOD）样本的能力在实际应用中是非常重要的。在这个工作中，我们首先研究了三维点云的极点与OOD样本之间的交互。我们发现，常见的损害和异常点经常被解释为极点。我们扩展了极点的概念，得到了重要度度量。我们发现，只使用不重要的点进行训练，可以很好地提高 robustness，但是会导致清洁集上的性能下降。我们发现，Normalized entropy 非常有用于损害分析。我们建议使用 Normalized entropy 来选择不重要的点。我们的提出的重要度度量非常快速计算。我们证明它可以用于多种应用，如 Explainable AI（XAI）、异常点除法、不确定度估计、Robust Classification 和对抗攻击。我们在两个后者任务上达到了 SOTA 结果。代码可以在：https://github.com/yossilevii100/critical_points2 中找到。
</details></li>
</ul>
<hr>
<h2 id="Models-Matter-The-Impact-of-Single-Step-Retrosynthesis-on-Synthesis-Planning"><a href="#Models-Matter-The-Impact-of-Single-Step-Retrosynthesis-on-Synthesis-Planning" class="headerlink" title="Models Matter: The Impact of Single-Step Retrosynthesis on Synthesis Planning"></a>Models Matter: The Impact of Single-Step Retrosynthesis on Synthesis Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05522">http://arxiv.org/abs/2308.05522</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paula Torren-Peraire, Alan Kai Hassen, Samuel Genheden, Jonas Verhoeven, Djork-Arne Clevert, Mike Preuss, Igor Tetko<br>for: 这篇论文的目的是提出一种结合单步逆synthesis预测和多步合成规划的方法，以提高合成路径的可靠性和效率。methods: 该方法首先应用多个单步逆synthesis模型，然后在多步合成规划中分析其影响。此外，该方法还使用公共和专用反应数据进行评估。results: 研究发现，单步逆synthesis模型在多步合成规划中的选择可以提高总成功率 by up to +28%，而且每个单步模型都找到了不同的合成路径，这些路径之间存在一定的不同，例如路径找到成功率、合成路径的数量和化学有效性等方面。<details>
<summary>Abstract</summary>
Retrosynthesis consists of breaking down a chemical compound recursively step-by-step into molecular precursors until a set of commercially available molecules is found with the goal to provide a synthesis route. Its two primary research directions, single-step retrosynthesis prediction, which models the chemical reaction logic, and multi-step synthesis planning, which tries to find the correct sequence of reactions, are inherently intertwined. Still, this connection is not reflected in contemporary research. In this work, we combine these two major research directions by applying multiple single-step retrosynthesis models within multi-step synthesis planning and analyzing their impact using public and proprietary reaction data. We find a disconnection between high single-step performance and potential route-finding success, suggesting that single-step models must be evaluated within synthesis planning in the future. Furthermore, we show that the commonly used single-step retrosynthesis benchmark dataset USPTO-50k is insufficient as this evaluation task does not represent model performance and scalability on larger and more diverse datasets. For multi-step synthesis planning, we show that the choice of the single-step model can improve the overall success rate of synthesis planning by up to +28% compared to the commonly used baseline model. Finally, we show that each single-step model finds unique synthesis routes, and differs in aspects such as route-finding success, the number of found synthesis routes, and chemical validity, making the combination of single-step retrosynthesis prediction and multi-step synthesis planning a crucial aspect when developing future methods.
</details>
<details>
<summary>摘要</summary>
<<SYS>>功能分解是一种分解化学物质的步骤，从分子前体开始，直到找到一组可用的化学物质，以实现化学合成。功能分解的两个主要研究方向是单步功能分解预测和多步合成规划。single-step retrosynthesis prediction models the chemical reaction logic, while multi-step synthesis planning tries to find the correct sequence of reactions. However, these two research directions are not well connected in contemporary research. In this work, we combine these two major research directions by applying multiple single-step retrosynthesis models within multi-step synthesis planning and analyzing their impact using public and proprietary reaction data.我们发现，高单步性能并不一定对应合成规划的成功。这表明，单步模型在合成规划中需要进行评估。此外，我们发现USPTO-50k单步功能分解数据集不够，因为这个评估任务不能反映模型在更大和更多样化的数据集上的性能和可扩展性。对多步合成规划，我们发现，选择合适的单步模型可以提高总成功率的synthesis planning by up to +28% compared to the commonly used baseline model。此外，我们发现每个单步模型都找到了不同的合成路径，这些路径之间存在差异，例如成功率、合成路径数量和化学有效性。因此，将单步功能分解预测和多步合成规划相结合是未来发展方法的关键。Note: The text has been translated using Google Translate, and some parts may not be exactly correct or idiomatic.
</details></li>
</ul>
<hr>
<h2 id="On-the-Optimal-Expressive-Power-of-ReLU-DNNs-and-Its-Application-in-Approximation-with-Kolmogorov-Superposition-Theorem"><a href="#On-the-Optimal-Expressive-Power-of-ReLU-DNNs-and-Its-Application-in-Approximation-with-Kolmogorov-Superposition-Theorem" class="headerlink" title="On the Optimal Expressive Power of ReLU DNNs and Its Application in Approximation with Kolmogorov Superposition Theorem"></a>On the Optimal Expressive Power of ReLU DNNs and Its Application in Approximation with Kolmogorov Superposition Theorem</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05509">http://arxiv.org/abs/2308.05509</a></li>
<li>repo_url: None</li>
<li>paper_authors: Juncai He</li>
<li>for: studying the optimal expressive power of ReLU deep neural networks (DNNs) and its application in approximation.</li>
<li>methods: constructive proof and investigation of the shattering capacity of ReLU DNNs.</li>
<li>results: achievement of an enhanced approximation rate for ReLU DNNs of arbitrary width and depth when dealing with continuous functions in high-dimensional spaces.Here is the Chinese translation of the three key information points:</li>
<li>for: 研究具有最佳表达力的ReLU深度神经网络（DNNs）以及其在拟合中的应用。</li>
<li>methods: 使用构造性证明和扰乱容量研究ReLU DNNs。</li>
<li>results: 通过高维空间中连续函数的拟合，实现ReLU DNNs的参数计数最佳化。<details>
<summary>Abstract</summary>
This paper is devoted to studying the optimal expressive power of ReLU deep neural networks (DNNs) and its application in approximation via the Kolmogorov Superposition Theorem. We first constructively prove that any continuous piecewise linear functions on $[0,1]$, comprising $O(N^2L)$ segments, can be represented by ReLU DNNs with $L$ hidden layers and $N$ neurons per layer. Subsequently, we demonstrate that this construction is optimal regarding the parameter count of the DNNs, achieved through investigating the shattering capacity of ReLU DNNs. Moreover, by invoking the Kolmogorov Superposition Theorem, we achieve an enhanced approximation rate for ReLU DNNs of arbitrary width and depth when dealing with continuous functions in high-dimensional spaces.
</details>
<details>
<summary>摘要</summary>
Here is the translation in Simplified Chinese:这篇论文关注研究ReLU深度神经网络（DNNs）的最佳表达能力和其在高维空间中的扩展应用。我们首先构造地证明任何连续划分线性函数在[0,1]中可以通过ReLU DNNs avec $L$层和$N$个神经元来表示，其中参数计数为$O(N^2L)$。然后我们证明这个构造是最佳的，通过研究ReLU DNNs的分化能力。此外，通过kolmogorov超position定理，我们得到了ReLU DNNs的任意宽度和深度时对连续函数的高级度扩展应用。
</details></li>
</ul>
<hr>
<h2 id="Quality-Diversity-under-Sparse-Reward-and-Sparse-Interaction-Application-to-Grasping-in-Robotics"><a href="#Quality-Diversity-under-Sparse-Reward-and-Sparse-Interaction-Application-to-Grasping-in-Robotics" class="headerlink" title="Quality Diversity under Sparse Reward and Sparse Interaction: Application to Grasping in Robotics"></a>Quality Diversity under Sparse Reward and Sparse Interaction: Application to Grasping in Robotics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05483">http://arxiv.org/abs/2308.05483</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Johann-Huber/qd_grasp">https://github.com/Johann-Huber/qd_grasp</a></li>
<li>paper_authors: J. Huber, F. Hélénon, M. Coninx, F. Ben Amar, S. Doncieux</li>
<li>for: 本研究旨在应用Quality-Diversity（QD）算法来解决机器人抓取问题，抓取是机器人控制领域中的一个重要任务。</li>
<li>methods: 本研究使用了15种不同的方法，在10个抓取领域中进行了实验，包括2种机器人抓取设置和5种标准物品。研究还提出了一个评价框架，以便公正地对各种算法进行评价。</li>
<li>results: 研究结果表明，MAP-Elites变体在研究中所用的评价指标上表现出了明显的优势，至少在比较中超过了所有其他方法。此外，研究还发现了稀有互动可能导致假新鲜度的现象。本研究所获得的抓取trajectory的生成能力在文献中无前例。<details>
<summary>Abstract</summary>
Quality-Diversity (QD) methods are algorithms that aim to generate a set of diverse and high-performing solutions to a given problem. Originally developed for evolutionary robotics, most QD studies are conducted on a limited set of domains - mainly applied to locomotion, where the fitness and the behavior signal are dense. Grasping is a crucial task for manipulation in robotics. Despite the efforts of many research communities, this task is yet to be solved. Grasping cumulates unprecedented challenges in QD literature: it suffers from reward sparsity, behavioral sparsity, and behavior space misalignment. The present work studies how QD can address grasping. Experiments have been conducted on 15 different methods on 10 grasping domains, corresponding to 2 different robot-gripper setups and 5 standard objects. An evaluation framework that distinguishes the evaluation of an algorithm from its internal components has also been proposed for a fair comparison. The obtained results show that MAP-Elites variants that select successful solutions in priority outperform all the compared methods on the studied metrics by a large margin. We also found experimental evidence that sparse interaction can lead to deceptive novelty. To our knowledge, the ability to efficiently produce examples of grasping trajectories demonstrated in this work has no precedent in the literature.
</details>
<details>
<summary>摘要</summary>
优质多样性（QD）算法目的是生成一组多样且高性能的解决方案，原本用于进化 робототех术。大多数QD研究都是在有限的领域上进行，主要是应用于行动，其中健能和行为信号都是密集的。抓取是机器人控制中的关键任务，尚未得到解决。抓取受到QD文献中的挑战，包括奖励稀少、行为稀少和行为空间不协调。本研究探讨了QD如何解决抓取问题。我们在15种方法上进行了10个抓取领域的实验，包括2种机器人夹仓设置和5种标准物品。我们还提出了一种评价框架，以便公正地比较不同算法的表现。实验结果表明，MAP-Elites变种选择成功解决方案在所研究的指标上大幅度超越所有比较的方法。我们还发现了实验证明，稀少的互动可能导致假新鲜。在这种情况下，我们所提出的能够高效生成抓取轨迹示例的能力，在文献中无先例。
</details></li>
</ul>
<hr>
<h2 id="LLM-As-DBA"><a href="#LLM-As-DBA" class="headerlink" title="LLM As DBA"></a>LLM As DBA</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05481">http://arxiv.org/abs/2308.05481</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tsinghuadatabasegroup/db-gpt">https://github.com/tsinghuadatabasegroup/db-gpt</a></li>
<li>paper_authors: Xuanhe Zhou, Guoliang Li, Zhiyuan Liu<br>for:The paper is written to propose a revolutionary framework for database maintenance using large language models (LLMs).methods:The framework uses LLMs to detect database maintenance knowledge from documents and tools, and uses tree of thought reasoning for root cause analysis.results:The paper presents preliminary experimental results that show D-Bot, the proposed LLM-based database administrator, can efficiently and effectively diagnose the root causes of database issues.<details>
<summary>Abstract</summary>
Database administrators (DBAs) play a crucial role in managing, maintaining and optimizing a database system to ensure data availability, performance, and reliability. However, it is hard and tedious for DBAs to manage a large number of database instances (e.g., millions of instances on the cloud databases). Recently large language models (LLMs) have shown great potential to understand valuable documents and accordingly generate reasonable answers. Thus, we propose D-Bot, a LLM-based database administrator that can continuously acquire database maintenance experience from textual sources, and provide reasonable, well-founded, in-time diagnosis and optimization advice for target databases. This paper presents a revolutionary LLM-centric framework for database maintenance, including (i) database maintenance knowledge detection from documents and tools, (ii) tree of thought reasoning for root cause analysis, and (iii) collaborative diagnosis among multiple LLMs. Our preliminary experimental results that D-Bot can efficiently and effectively diagnose the root causes and our code is available at github.com/TsinghuaDatabaseGroup/DB-GPT.
</details>
<details>
<summary>摘要</summary>
Database administrators (DBAs) play a crucial role in managing, maintaining, and optimizing a database system to ensure data availability, performance, and reliability. However, it is hard and tedious for DBAs to manage a large number of database instances (e.g., millions of instances on the cloud databases). Recently, large language models (LLMs) have shown great potential to understand valuable documents and accordingly generate reasonable answers. Thus, we propose D-Bot, a LLM-based database administrator that can continuously acquire database maintenance experience from textual sources, and provide reasonable, well-founded, in-time diagnosis and optimization advice for target databases. This paper presents a revolutionary LLM-centric framework for database maintenance, including (i) database maintenance knowledge detection from documents and tools, (ii) tree of thought reasoning for root cause analysis, and (iii) collaborative diagnosis among multiple LLMs. Our preliminary experimental results show that D-Bot can efficiently and effectively diagnose the root causes, and our code is available at github.com/TsinghuaDatabaseGroup/DB-GPT.Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Exploring-Machine-Learning-and-Transformer-based-Approaches-for-Deceptive-Text-Classification-A-Comparative-Analysis"><a href="#Exploring-Machine-Learning-and-Transformer-based-Approaches-for-Deceptive-Text-Classification-A-Comparative-Analysis" class="headerlink" title="Exploring Machine Learning and Transformer-based Approaches for Deceptive Text Classification: A Comparative Analysis"></a>Exploring Machine Learning and Transformer-based Approaches for Deceptive Text Classification: A Comparative Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05476">http://arxiv.org/abs/2308.05476</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anusuya Krishnan</li>
<li>for: 本研究旨在比较机器学习和变换器基于的方法在假信息涂抹中的效果。</li>
<li>methods: 研究使用了传统的机器学习算法以及当今最佳实践的变换器模型，如BERT、XLNET、DistilBERT和RoBERTa，来检测假信息。</li>
<li>results: 通过广泛的实验，研究对不同方法的性能指标，包括准确率、精度、回归率和F1分数，进行了比较。结果可以为研究人员和实践者提供有用的指导，帮助他们在遇到假信息时做出 Informed 决策。<details>
<summary>Abstract</summary>
Deceptive text classification is a critical task in natural language processing that aims to identify deceptive o fraudulent content. This study presents a comparative analysis of machine learning and transformer-based approaches for deceptive text classification. We investigate the effectiveness of traditional machine learning algorithms and state-of-the-art transformer models, such as BERT, XLNET, DistilBERT, and RoBERTa, in detecting deceptive text. A labeled dataset consisting of deceptive and non-deceptive texts is used for training and evaluation purposes. Through extensive experimentation, we compare the performance metrics, including accuracy, precision, recall, and F1 score, of the different approaches. The results of this study shed light on the strengths and limitations of machine learning and transformer-based methods for deceptive text classification, enabling researchers and practitioners to make informed decisions when dealing with deceptive content.
</details>
<details>
<summary>摘要</summary>
伪装文本分类是自然语言处理中一项重要任务，旨在识别伪装或诈骗性内容。本研究进行了机器学习和变换器基于方法的比较分析，以检验这些方法在检测伪装文本方面的效果。我们使用了一个标注的数据集，包括伪装和非伪装文本，进行训练和评估。通过广泛的实验，我们比较了不同方法的性能指标，包括准确率、精度、准确率和F1分数。研究结果为研究者和实践者提供了有用的指导，帮助他们在面临伪装内容时做出了 Informed decisions。
</details></li>
</ul>
<hr>
<h2 id="Comprehensive-Analysis-of-Network-Robustness-Evaluation-Based-on-Convolutional-Neural-Networks-with-Spatial-Pyramid-Pooling"><a href="#Comprehensive-Analysis-of-Network-Robustness-Evaluation-Based-on-Convolutional-Neural-Networks-with-Spatial-Pyramid-Pooling" class="headerlink" title="Comprehensive Analysis of Network Robustness Evaluation Based on Convolutional Neural Networks with Spatial Pyramid Pooling"></a>Comprehensive Analysis of Network Robustness Evaluation Based on Convolutional Neural Networks with Spatial Pyramid Pooling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08012">http://arxiv.org/abs/2308.08012</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenjun Jiang, Tianlong Fan, Changhao Li, Chuanfu Zhang, Tao Zhang, Zong-fu Luo</li>
<li>for: 这篇论文是用来理解、优化和修复复杂网络的连接Robustness的一种新方法。</li>
<li>methods: 这篇论文使用了卷积神经网络（CNN）模型和空间彩色堆叠网络（SPP-net）来解决连接Robustness的计算复杂性问题。</li>
<li>results: 该模型在不同的网络类型、失败组件类型和失败场景下的计算时间均有较高的效率，但在一些场景下表现不尽人意料。<details>
<summary>Abstract</summary>
Connectivity robustness, a crucial aspect for understanding, optimizing, and repairing complex networks, has traditionally been evaluated through time-consuming and often impractical simulations. Fortunately, machine learning provides a new avenue for addressing this challenge. However, several key issues remain unresolved, including the performance in more general edge removal scenarios, capturing robustness through attack curves instead of directly training for robustness, scalability of predictive tasks, and transferability of predictive capabilities. In this paper, we address these challenges by designing a convolutional neural networks (CNN) model with spatial pyramid pooling networks (SPP-net), adapting existing evaluation metrics, redesigning the attack modes, introducing appropriate filtering rules, and incorporating the value of robustness as training data. The results demonstrate the thoroughness of the proposed CNN framework in addressing the challenges of high computational time across various network types, failure component types and failure scenarios. However, the performance of the proposed CNN model varies: for evaluation tasks that are consistent with the trained network type, the proposed CNN model consistently achieves accurate evaluations of both attack curves and robustness values across all removal scenarios. When the predicted network type differs from the trained network, the CNN model still demonstrates favorable performance in the scenario of random node failure, showcasing its scalability and performance transferability. Nevertheless, the performance falls short of expectations in other removal scenarios. This observed scenario-sensitivity in the evaluation of network features has been overlooked in previous studies and necessitates further attention and optimization. Lastly, we discuss important unresolved questions and further investigation.
</details>
<details>
<summary>摘要</summary>
Traditionalmente, la evaluación de la robustez de redes complejas ha requerido simulaciones tiempo-consumidoras y prácticas imposibles. ¡Felizmente, la aprendizaje automático ofrece una nueva vía para abordar este desafío! Sin embargo, varias cuestiones clave aún no se han resuelto, como el desempeño en escenarios de eliminación de nodos más generales, capturar la robustez a través de curvas de ataques en lugar de entrenar directamente por robustez, la escalabilidad de tareas predictivas y la transferencia de habilidades predictivas.En este artículo, abordamos estos desafíos mediante el diseño de una red neuronal convolucional (CNN) con redes de pooling espiral (SPP-net), adaptando métricas de evaluación existentes, rediseñando los modos de ataque, estableciendo reglas de filtrado adecuadas e incorporando el valor de la robustez como datos de entrenamiento. Los resultados demuestran la thoroughness del marco de CNN propuesto en abordar los desafíos de tiempo de cálculo alto en diversas redes y escenarios de fracaso. Sin embargo, el rendimiento de la CNN propuesta varía: en tareas de evaluación consistentes con el tipo de red entrenada, la CNN consiste en evaluaciones precisas de curvas de ataques y valores de robustez en todos los escenarios de eliminación. Cuando la red predicted difiere de la red entrenada, la CNN aún demuestra un rendimiento favorable en el escenario de fracaso aleatorio de nodos, lo que muestra su escalabilidad y transferencia de rendimiento. Sin embargo, el rendimiento no cumple con las expectativas en otros escenarios de eliminación, lo que ha sido pasado por alto en estudios anteriores y requiere más atención y optimización.Finalmente, discutimos preguntas importantes sin resolver y investigaciones adicionales.
</details></li>
</ul>
<hr>
<h2 id="Provably-Efficient-Algorithm-for-Nonstationary-Low-Rank-MDPs"><a href="#Provably-Efficient-Algorithm-for-Nonstationary-Low-Rank-MDPs" class="headerlink" title="Provably Efficient Algorithm for Nonstationary Low-Rank MDPs"></a>Provably Efficient Algorithm for Nonstationary Low-Rank MDPs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05471">http://arxiv.org/abs/2308.05471</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuan Cheng, Jing Yang, Yingbin Liang</li>
<li>for: 非站台Markov决策过程（MDPs）在实际应用中模型了许多真实世界问题，因此吸引了广泛的研究兴趣。然而， литера图中关于非站台MDPs的理论研究主要集中在表格和线性（混合）MDPs上，这些模型不能捕捉深度学习RL中的未知表示。本文是首次研究非站台RL在 episodic low-rank MDPs 上，其中转移函数和奖励函数可能随时间变化，并且low-rank模型包含未知表示。</li>
<li>methods: 我们首先提出了一种参数 dependent policy 优化算法 called PORTAL，然后改进PORTAL到其参数自由版本Ada-PORTAL，可以在不知道非站台性的情况下自动调整参数。</li>
<li>results: 我们提供了两个算法的平均动态下optimality gap 上界，显示如果非站台性不太大，然后PORTAL和Ada-PORTAL在样本复杂度为多项式幂的情况下可以实现任意小的平均动态下optimality gap。<details>
<summary>Abstract</summary>
Reinforcement learning (RL) under changing environment models many real-world applications via nonstationary Markov Decision Processes (MDPs), and hence gains considerable interest. However, theoretical studies on nonstationary MDPs in the literature have mainly focused on tabular and linear (mixture) MDPs, which do not capture the nature of unknown representation in deep RL. In this paper, we make the first effort to investigate nonstationary RL under episodic low-rank MDPs, where both transition kernels and rewards may vary over time, and the low-rank model contains unknown representation in addition to the linear state embedding function. We first propose a parameter-dependent policy optimization algorithm called PORTAL, and further improve PORTAL to its parameter-free version of Ada-PORTAL, which is able to tune its hyper-parameters adaptively without any prior knowledge of nonstationarity. For both algorithms, we provide upper bounds on the average dynamic suboptimality gap, which show that as long as the nonstationarity is not significantly large, PORTAL and Ada-PORTAL are sample-efficient and can achieve arbitrarily small average dynamic suboptimality gap with polynomial sample complexity.
</details>
<details>
<summary>摘要</summary>
“强化学习（RL）在不同环境模型下多种实际应用，特别是非站ARY Markov Decision Processes（MDPs），因此受到了广泛关注。然而，现有的理论研究中的非站ARY MDPs主要集中在表格和线性（混合）MDPs上，这些模型不能捕捉深度RL中的未知表示。本文是第一次 investigate nonstationary RL under episodic low-rank MDPs，其中过程权重和奖励可能随时间变化，低维模型包含未知表示。我们首先提出了一种参数依赖的策略优化算法 called PORTAL，然后进一步改进了 PORTAL 为其参数自由版本 Ada-PORTAL，可以适应不同的非站ARY度。我们为这两种算法提供了平均动态落差的Upper bound，显示在非站ARY度不太大时，PORTAL 和 Ada-PORTAL 是可靠的，可以在有限样本复杂度下实现平均动态落差的任意小化。”Note: Simplified Chinese is used here, which is a standardized form of Chinese that is widely used in mainland China and other parts of the world. The translation is written in the formal style, which is appropriate for academic papers.
</details></li>
</ul>
<hr>
<h2 id="mathcal-G-2Pxy-Generative-Open-Set-Node-Classification-on-Graphs-with-Proxy-Unknowns"><a href="#mathcal-G-2Pxy-Generative-Open-Set-Node-Classification-on-Graphs-with-Proxy-Unknowns" class="headerlink" title="$\mathcal{G}^2Pxy$: Generative Open-Set Node Classification on Graphs with Proxy Unknowns"></a>$\mathcal{G}^2Pxy$: Generative Open-Set Node Classification on Graphs with Proxy Unknowns</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05463">http://arxiv.org/abs/2308.05463</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qin Zhang, Zelin Shi, Xiaolin Zhang, Xiaojun Chen, Philippe Fournier-Viger, Shirui Pan</li>
<li>for: 本研究旨在提出一种新的开放集分类方法，以便在无知类信息时进行分类。</li>
<li>methods: 该方法使用生成器来生成proxy未知节点，然后通过混合来准备未知类的分布。</li>
<li>results: 实验表明，该方法可以在开放集分类任务中具有优秀的效果，并且不受GNN架构的限制。<details>
<summary>Abstract</summary>
Node classification is the task of predicting the labels of unlabeled nodes in a graph. State-of-the-art methods based on graph neural networks achieve excellent performance when all labels are available during training. But in real-life, models are often applied on data with new classes, which can lead to massive misclassification and thus significantly degrade performance. Hence, developing open-set classification methods is crucial to determine if a given sample belongs to a known class. Existing methods for open-set node classification generally use transductive learning with part or all of the features of real unseen class nodes to help with open-set classification. In this paper, we propose a novel generative open-set node classification method, i.e. $\mathcal{G}^2Pxy$, which follows a stricter inductive learning setting where no information about unknown classes is available during training and validation. Two kinds of proxy unknown nodes, inter-class unknown proxies and external unknown proxies are generated via mixup to efficiently anticipate the distribution of novel classes. Using the generated proxies, a closed-set classifier can be transformed into an open-set one, by augmenting it with an extra proxy classifier. Under the constraints of both cross entropy loss and complement entropy loss, $\mathcal{G}^2Pxy$ achieves superior effectiveness for unknown class detection and known class classification, which is validated by experiments on benchmark graph datasets. Moreover, $\mathcal{G}^2Pxy$ does not have specific requirement on the GNN architecture and shows good generalizations.
</details>
<details>
<summary>摘要</summary>
Node 分类是指预测图中没有标签的节点的标签。现有的方法基于图神经网络可以在所有标签可用于训练时达到极优性。但在实际应用中，模型经常应用于具有新的类型的数据，可能导致大规模的误分类，从而很大地降低性能。因此，开发开放集分类方法是关键的，以确定给定的样本是否属于已知类。现有的开放集节点分类方法通常使用散度学习，使用真实未看过类节点的一部分或所有特征来帮助开放集分类。在这篇论文中，我们提出了一种新的生成型开放集节点分类方法，即 $\mathcal{G}^2Pxy$，它遵循一个更严格的散度学习设定，在训练和验证过程中不可能获得未知类的信息。通过混合来生成两种类型的代理未知节点，即间类未知代理和外部未知代理，以效率地预测新类的分布。使用生成的代理节点，一个关闭集分类器可以转换成一个开放集分类器，通过增加一个额外的代理分类器。在跨度 Entropy 损失和补偿 Entropy 损失的约束下， $\mathcal{G}^2Pxy$ 实现了对未知类探测和已知类分类的超越性，经过实验 validate 在图数据集上。此外， $\mathcal{G}^2Pxy$ 不受 GNN 架构的限制，并且具有良好的通用性。
</details></li>
</ul>
<hr>
<h2 id="A-Forecaster’s-Review-of-Judea-Pearl’s-Causality-Models-Reasoning-and-Inference-Second-Edition-2009"><a href="#A-Forecaster’s-Review-of-Judea-Pearl’s-Causality-Models-Reasoning-and-Inference-Second-Edition-2009" class="headerlink" title="A Forecaster’s Review of Judea Pearl’s Causality: Models, Reasoning and Inference, Second Edition, 2009"></a>A Forecaster’s Review of Judea Pearl’s Causality: Models, Reasoning and Inference, Second Edition, 2009</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05451">http://arxiv.org/abs/2308.05451</a></li>
<li>repo_url: None</li>
<li>paper_authors: Feng Li</li>
<li>for: 本文是一篇评论文章，涵盖了 Judy Pearl 的原始 causality 书籍第二版（2009）中的主要话题。</li>
<li>methods: 本文提出了一种简单易于遵循的 causal inference 策略，并在预测enario中进行了示例。</li>
<li>results: 本文讨论了在预测中 causal inference 的一些潜在利益和挑战，以及如何在不同的预测enario中 estimate causal effects。<details>
<summary>Abstract</summary>
With the big popularity and success of Judea Pearl's original causality book, this review covers the main topics updated in the second edition in 2009 and illustrates an easy-to-follow causal inference strategy in a forecast scenario. It further discusses some potential benefits and challenges for causal inference with time series forecasting when modeling the counterfactuals, estimating the uncertainty and incorporating prior knowledge to estimate causal effects in different forecasting scenarios.
</details>
<details>
<summary>摘要</summary>
根据朱德亚·珀尔的原始 causality 书的巨大受欢迎和成功，这篇评论介绍了第二版（2009年）中的主要话题，并提供了一个易于掌握的 causal inference 策略，用于预测enario。它还讨论了在模型 counterfactuals 时， causal inference 遇到的潜在利益和挑战，以及如何在不同的预测enario中 estimate  causal effects。
</details></li>
</ul>
<hr>
<h2 id="Explainable-AI-applications-in-the-Medical-Domain-a-systematic-review"><a href="#Explainable-AI-applications-in-the-Medical-Domain-a-systematic-review" class="headerlink" title="Explainable AI applications in the Medical Domain: a systematic review"></a>Explainable AI applications in the Medical Domain: a systematic review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05411">http://arxiv.org/abs/2308.05411</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicoletta Prentzas, Antonis Kakas, Constantinos S. Pattichis</li>
<li>for: 这篇论文旨在探讨医疗人工智能（AI）在医疗领域的应用，以及如何使用解释AI（XAI）解决方案来提高医疗决策支持系统的可靠性和可信worth。</li>
<li>methods: 这篇论文采用了一种系统性的文献综述方法，检索了过去几年发表的198篇有关医疗AI和XAI的研究论文，并进行了系统性的分析和总结。</li>
<li>results: 根据这篇论文的分析结果，以下是一些主要发现：（1）大多数解释AI技术是模型非依的，（2）深度学习模型在医疗AI中使用得更多，（3）解释是用于提高信任的方法，但很少有报道了医生参与的循环，（4）可视化和互动的用户界面更有用于理解解释和系统的建议。<details>
<summary>Abstract</summary>
Artificial Intelligence in Medicine has made significant progress with emerging applications in medical imaging, patient care, and other areas. While these applications have proven successful in retrospective studies, very few of them were applied in practice.The field of Medical AI faces various challenges, in terms of building user trust, complying with regulations, using data ethically.Explainable AI (XAI) aims to enable humans understand AI and trust its results. This paper presents a literature review on the recent developments of XAI solutions for medical decision support, based on a representative sample of 198 articles published in recent years. The systematic synthesis of the relevant articles resulted in several findings. (1) model-agnostic XAI techniques were mostly employed in these solutions, (2) deep learning models are utilized more than other types of machine learning models, (3) explainability was applied to promote trust, but very few works reported the physicians participation in the loop, (4) visual and interactive user interface is more useful in understanding the explanation and the recommendation of the system. More research is needed in collaboration between medical and AI experts, that could guide the development of suitable frameworks for the design, implementation, and evaluation of XAI solutions in medicine.
</details>
<details>
<summary>摘要</summary>
人工智能在医疗领域已经取得了 significiant 进步，其应用范围包括医疗影像、患者护理和其他领域。然而，这些应用在实践中并不多见。医疗领域的人工智能面临着多种挑战，包括建立用户信任、遵守法规和使用数据道德。可解释人工智能（XAI）旨在帮助人类理解人工智能和信任其结果。这篇文章提出了一种Literature Review，检查了最近几年发表的198篇文章，以获取最新的发展情况。系统性的分析这些文章所得到的结论包括以下几点：1. Model-agnostic XAI技术在这些解释中最常用。2. 深度学习模型比其他机器学习模型更常用。3. 解释的目的是促进信任，但很少有文章报道了医生参与的循环。4. 可视化和交互的用户界面更有用于理解解释和系统的建议。进一步的研究需要在医疗和人工智能专家之间合作，以开发适合医疗领域的XAI解释解决方案。
</details></li>
</ul>
<hr>
<h2 id="A-Comparative-Assessment-of-Multi-view-fusion-learning-for-Crop-Classification"><a href="#A-Comparative-Assessment-of-Multi-view-fusion-learning-for-Crop-Classification" class="headerlink" title="A Comparative Assessment of Multi-view fusion learning for Crop Classification"></a>A Comparative Assessment of Multi-view fusion learning for Crop Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05407">http://arxiv.org/abs/2308.05407</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fmenat/multiviewcropclassification">https://github.com/fmenat/multiviewcropclassification</a></li>
<li>paper_authors: Francisco Mena, Diego Arenas, Marlon Nuske, Andreas Dengel<br>for: 这个论文的目的是提出了多视图学习模型，以处理不同分辨率、大小和噪声等多种远程感知数据的复杂任务。methods: 这个论文使用了不同的多视图合并策略，包括输入级别合并、特征级别合并和卷积级别合并等。results: 论文表明，使用不同的多视图合并策略可以超过基于单个视图的模型和前期的合并策略。但是，不同的测试区域中，不同的方法可以取得最佳性能。<details>
<summary>Abstract</summary>
With a rapidly increasing amount and diversity of remote sensing (RS) data sources, there is a strong need for multi-view learning modeling. This is a complex task when considering the differences in resolution, magnitude, and noise of RS data. The typical approach for merging multiple RS sources has been input-level fusion, but other - more advanced - fusion strategies may outperform this traditional approach. This work assesses different fusion strategies for crop classification in the CropHarvest dataset. The fusion methods proposed in this work outperform models based on individual views and previous fusion methods. We do not find one single fusion method that consistently outperforms all other approaches. Instead, we present a comparison of multi-view fusion methods for three different datasets and show that, depending on the test region, different methods obtain the best performance. Despite this, we suggest a preliminary criterion for the selection of fusion methods.
</details>
<details>
<summary>摘要</summary>
With the rapidly increasing amount and diversity of remote sensing (RS) data sources, there is a strong need for multi-view learning modeling. This is a complex task when considering the differences in resolution, magnitude, and noise of RS data. The typical approach for merging multiple RS sources has been input-level fusion, but other - more advanced - fusion strategies may outperform this traditional approach. This work assesses different fusion strategies for crop classification in the CropHarvest dataset. The fusion methods proposed in this work outperform models based on individual views and previous fusion methods. We do not find one single fusion method that consistently outperforms all other approaches. Instead, we present a comparison of multi-view fusion methods for three different datasets and show that, depending on the test region, different methods obtain the best performance. Despite this, we suggest a preliminary criterion for the selection of fusion methods.Here's the translation in Traditional Chinese:有很多和多样化的远程感知（RS）数据源，需要多视角学习模型。这是一个复杂的任务，因为RS数据的分辨率、大小和噪声之间存在差异。传统的方法是输入级别 fusión，但是更先进的拟合策略可能会超越这种传统方法。这篇文章评估了不同的拟合策略，用于cropland classification in CropHarvest dataset。我们的方法超越了基于个体视角的模型和前一代的拟合方法。我们没有找到一个一直以来的拟合方法，可以在所有情况下表现最佳。相反，我们提供了不同数据集中的多视角拟合方法的比较，并显示，根据测试区域，不同的方法在不同的数据集中可以获得最佳性能。虽然如此，我们建议一种初步的选择标准，用于选择拟合方法。
</details></li>
</ul>
<hr>
<h2 id="Addressing-Data-Scarcity-in-Optical-Matrix-Multiplier-Modeling-Using-Transfer-Learning"><a href="#Addressing-Data-Scarcity-in-Optical-Matrix-Multiplier-Modeling-Using-Transfer-Learning" class="headerlink" title="Addressing Data Scarcity in Optical Matrix Multiplier Modeling Using Transfer Learning"></a>Addressing Data Scarcity in Optical Matrix Multiplier Modeling Using Transfer Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11630">http://arxiv.org/abs/2308.11630</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Cem, Ognjen Jovanovic, Siqi Yan, Yunhong Ding, Darko Zibar, Francesco Da Ros</li>
<li>for: 用 transferred learning 解决光学矩阵乘算器中的数据罕见性问题，即使使用少量实验数据进行模型训练。</li>
<li>methods: 采用先训练模型使用生成自 menos accurate analytical model的 sintetic数据，然后精度调整使用实验数据。</li>
<li>results: 该方法可以减少模型错误，比使用analytical模型或独立的 neural network模型在数据有限情况下。使用正则化技术和ensemble averaging，实现 &lt;1 dB的Root-Mean-Square Error在实际中实现了矩阵加 weights。<details>
<summary>Abstract</summary>
We present and experimentally evaluate using transfer learning to address experimental data scarcity when training neural network (NN) models for Mach-Zehnder interferometer mesh-based optical matrix multipliers. Our approach involves pre-training the model using synthetic data generated from a less accurate analytical model and fine-tuning with experimental data. Our investigation demonstrates that this method yields significant reductions in modeling errors compared to using an analytical model, or a standalone NN model when training data is limited. Utilizing regularization techniques and ensemble averaging, we achieve < 1 dB root-mean-square error on the matrix weights implemented by a photonic chip while using only 25% of the available data.
</details>
<details>
<summary>摘要</summary>
我团队在 meshes 基于光学矩阵乘数器中使用传输学习来解决数据稀缺问题，我们的方法是先使用基于analytical模型生成的synthetic数据进行预训练，然后使用实验数据进行微调。我们的调查表明，这种方法可以相比analytical模型或独立的神经网络模型在数据有限情况下获得显著的减少模型错误。通过使用常规化技术和ensemble平均，我们在 photonic chip 上实现了 < 1 dB 的平均平方误差，只使用25%的数据。
</details></li>
</ul>
<hr>
<h2 id="Product-Review-Image-Ranking-for-Fashion-E-commerce"><a href="#Product-Review-Image-Ranking-for-Fashion-E-commerce" class="headerlink" title="Product Review Image Ranking for Fashion E-commerce"></a>Product Review Image Ranking for Fashion E-commerce</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05390">http://arxiv.org/abs/2308.05390</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sangeet Jaiswal, Dhruv Patel, Sreekanth Vempati, Konduru Saiswaroop</li>
<li>for: 本研究旨在提出一种简单 yet effective的训练方法，以排名用户生成内容（UGC）中的图像。</li>
<li>methods: 我们使用Myntra（印度主要的时尚电商公司）的Studio Posts和高度参与（upvotes&#x2F;downvotes）UGC图像构成了我们的起点，并使用选择的扭曲技术将图像的质量提高到与差的图像水平。我们训练我们的网络，以便将差质图像排名在低于高质图像之前。</li>
<li>results: 我们的提议方法在两个纪录（相关系数和准确率）上超越基线模型，具有明显的优势。<details>
<summary>Abstract</summary>
In a fashion e-commerce platform where customers can't physically examine the products on their own, being able to see other customers' text and image reviews of the product is critical while making purchase decisions. Given the high reliance on these reviews, over the years we have observed customers proactively sharing their reviews. With an increase in the coverage of User Generated Content (UGC), there has been a corresponding increase in the number of customer images. It is thus imperative to display the most relevant images on top as it may influence users' online shopping choices and behavior. In this paper, we propose a simple yet effective training procedure for ranking customer images. We created a dataset consisting of Myntra (A Major Indian Fashion e-commerce company) studio posts and highly engaged (upvotes/downvotes) UGC images as our starting point and used selected distortion techniques on the images of the above dataset to bring their quality at par with those of bad UGC images. We train our network to rank bad-quality images lower than high-quality ones. Our proposed method outperforms the baseline models on two metrics, namely correlation coefficient, and accuracy, by substantial margins.
</details>
<details>
<summary>摘要</summary>
在一个无法购买者实际检查产品的电商平台上，可见其他客户的文本和图像评论对于购买决策是非常重要的。随着用户生成内容的覆盖率的增加，我们在年进行了评论的投稿。随着用户生成内容的增加，图像的数量也随之增加。因此，显示最相关的图像在前面是非常重要的，因为它们可能影响用户的在线购物选择和行为。在这篇论文中，我们提出了一种简单 yet 有效的训练方法，用于排序客户图像。我们使用 Myntra（印度主要的时尚电商公司）的Studio文章和高度参与度（投票/踢票）的用户生成内容图像作为我们的起点，并使用选择的扭曲技术来使图像的质量与坏用户生成内容图像相匹配。我们训练我们的网络，以便将坏质量图像排在低于高质量图像之前。我们的提议方法在两个纪录 coefficient和准确率两个纪录上，与基准模型相比，均有substantial的优势。
</details></li>
</ul>
<hr>
<h2 id="Trustworthy-LLMs-a-Survey-and-Guideline-for-Evaluating-Large-Language-Models’-Alignment"><a href="#Trustworthy-LLMs-a-Survey-and-Guideline-for-Evaluating-Large-Language-Models’-Alignment" class="headerlink" title="Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models’ Alignment"></a>Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models’ Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05374">http://arxiv.org/abs/2308.05374</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo, Hao Cheng, Yegor Klochkov, Muhammad Faaiz Taufiq, Hang Li</li>
<li>for: 本研究的目的是为了提供关键维度的评估语言模型可靠性的报告，以便实现可靠性的语言模型在各种应用中的顺利部署。</li>
<li>methods: 本研究使用了一种全面的评估方法，包括7个主要类别和29个子类别，以评估语言模型的可靠性。</li>
<li>results: 研究发现，更加适应的模型通常在总可靠性方面表现更好，但是对不同的可靠性类别的有效性强度各不相同。这指出了需要进行更细化的分析、测试和改进，以确保语言模型的适应性和伦理性。<details>
<summary>Abstract</summary>
Ensuring alignment, which refers to making models behave in accordance with human intentions [1,2], has become a critical task before deploying large language models (LLMs) in real-world applications. For instance, OpenAI devoted six months to iteratively aligning GPT-4 before its release [3]. However, a major challenge faced by practitioners is the lack of clear guidance on evaluating whether LLM outputs align with social norms, values, and regulations. This obstacle hinders systematic iteration and deployment of LLMs. To address this issue, this paper presents a comprehensive survey of key dimensions that are crucial to consider when assessing LLM trustworthiness. The survey covers seven major categories of LLM trustworthiness: reliability, safety, fairness, resistance to misuse, explainability and reasoning, adherence to social norms, and robustness. Each major category is further divided into several sub-categories, resulting in a total of 29 sub-categories. Additionally, a subset of 8 sub-categories is selected for further investigation, where corresponding measurement studies are designed and conducted on several widely-used LLMs. The measurement results indicate that, in general, more aligned models tend to perform better in terms of overall trustworthiness. However, the effectiveness of alignment varies across the different trustworthiness categories considered. This highlights the importance of conducting more fine-grained analyses, testing, and making continuous improvements on LLM alignment. By shedding light on these key dimensions of LLM trustworthiness, this paper aims to provide valuable insights and guidance to practitioners in the field. Understanding and addressing these concerns will be crucial in achieving reliable and ethically sound deployment of LLMs in various applications.
</details>
<details>
<summary>摘要</summary>
ensure alignment, which refers to making models behave in accordance with human intentions [1,2], has become a critical task before deploying large language models (LLMs) in real-world applications. For instance, OpenAI devoted six months to iteratively aligning GPT-4 before its release [3]. However, a major challenge faced by practitioners is the lack of clear guidance on evaluating whether LLM outputs align with social norms, values, and regulations. This obstacle hinders systematic iteration and deployment of LLMs. To address this issue, this paper presents a comprehensive survey of key dimensions that are crucial to consider when assessing LLM trustworthiness. The survey covers seven major categories of LLM trustworthiness: reliability, safety, fairness, resistance to misuse, explainability and reasoning, adherence to social norms, and robustness. Each major category is further divided into several sub-categories, resulting in a total of 29 sub-categories. Additionally, a subset of 8 sub-categories is selected for further investigation, where corresponding measurement studies are designed and conducted on several widely-used LLMs. The measurement results indicate that, in general, more aligned models tend to perform better in terms of overall trustworthiness. However, the effectiveness of alignment varies across the different trustworthiness categories considered. This highlights the importance of conducting more fine-grained analyses, testing, and making continuous improvements on LLM alignment. By shedding light on these key dimensions of LLM trustworthiness, this paper aims to provide valuable insights and guidance to practitioners in the field. Understanding and addressing these concerns will be crucial in achieving reliable and ethically sound deployment of LLMs in various applications.
</details></li>
</ul>
<hr>
<h2 id="Flexible-Isosurface-Extraction-for-Gradient-Based-Mesh-Optimization"><a href="#Flexible-Isosurface-Extraction-for-Gradient-Based-Mesh-Optimization" class="headerlink" title="Flexible Isosurface Extraction for Gradient-Based Mesh Optimization"></a>Flexible Isosurface Extraction for Gradient-Based Mesh Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05371">http://arxiv.org/abs/2308.05371</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianchang Shen, Jacob Munkberg, Jon Hasselgren, Kangxue Yin, Zian Wang, Wenzheng Chen, Zan Gojcic, Sanja Fidler, Nicholas Sharp, Jun Gao</li>
<li>for: 本文考虑了梯度基本的网格优化，通过 représenter mesh 为一个浮点场的iso surface，以便在摄grammetry、生成模型和反向物理等应用中进行优化。</li>
<li>methods: 我们引入了flexi Cubes，一种特定的iso surface表示方法，用于优化未知的网格，以达到几何、视觉和物理目标。我们的主要思想是通过引入地方的参数，使得mesh几何和连接性可以进行当地灵活调整。这些参数通过自动微分升级来与下游任务的对应scalar场一起更新。</li>
<li>results: 我们的实验表明，flexi Cubes 可以在synthetic benchmarks和实际应用中提供显著改善的网格质量和几何准确性。<details>
<summary>Abstract</summary>
This work considers gradient-based mesh optimization, where we iteratively optimize for a 3D surface mesh by representing it as the isosurface of a scalar field, an increasingly common paradigm in applications including photogrammetry, generative modeling, and inverse physics. Existing implementations adapt classic isosurface extraction algorithms like Marching Cubes or Dual Contouring; these techniques were designed to extract meshes from fixed, known fields, and in the optimization setting they lack the degrees of freedom to represent high-quality feature-preserving meshes, or suffer from numerical instabilities. We introduce FlexiCubes, an isosurface representation specifically designed for optimizing an unknown mesh with respect to geometric, visual, or even physical objectives. Our main insight is to introduce additional carefully-chosen parameters into the representation, which allow local flexible adjustments to the extracted mesh geometry and connectivity. These parameters are updated along with the underlying scalar field via automatic differentiation when optimizing for a downstream task. We base our extraction scheme on Dual Marching Cubes for improved topological properties, and present extensions to optionally generate tetrahedral and hierarchically-adaptive meshes. Extensive experiments validate FlexiCubes on both synthetic benchmarks and real-world applications, showing that it offers significant improvements in mesh quality and geometric fidelity.
</details>
<details>
<summary>摘要</summary>
To address this limitation, we introduce FlexiCubes, a new isosurface representation specifically designed for optimizing an unknown mesh. Our key insight is to introduce additional carefully-chosen parameters into the representation, which allow for local flexible adjustments to the extracted mesh geometry and connectivity. These parameters are updated along with the underlying scalar field via automatic differentiation when optimizing for a downstream task.We base our extraction scheme on Dual Marching Cubes for improved topological properties, and present extensions to optionally generate tetrahedral and hierarchically-adaptive meshes. Extensive experiments validate FlexiCubes on both synthetic benchmarks and real-world applications, showing that it offers significant improvements in mesh quality and geometric fidelity.
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-aided-Computer-Architecture-Design-for-CNN-Inferencing-Systems"><a href="#Machine-Learning-aided-Computer-Architecture-Design-for-CNN-Inferencing-Systems" class="headerlink" title="Machine Learning aided Computer Architecture Design for CNN Inferencing Systems"></a>Machine Learning aided Computer Architecture Design for CNN Inferencing Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05364">http://arxiv.org/abs/2308.05364</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christopher A. Metz</li>
<li>for: 这篇论文的目的是优化机器学习（ML）算法的效率计算，以应对智能交通、物联网（IoT）和边缘 Computing 等新兴技术的需求。</li>
<li>methods: 本论文使用了 Design Space Exploration（DSE）方法来选择最适合的加速器，并提出了一种快速和精准的预测方法来估计 CNN 的电力和性能。</li>
<li>results: 本论文的预测方法可以实现 MapE 的预测精度，对于 CNN 的推论运算可以提供快速且精准的电力和性能估计。<details>
<summary>Abstract</summary>
Efficient and timely calculations of Machine Learning (ML) algorithms are essential for emerging technologies like autonomous driving, the Internet of Things (IoT), and edge computing. One of the primary ML algorithms used in such systems is Convolutional Neural Networks (CNNs), which demand high computational resources. This requirement has led to the use of ML accelerators like GPGPUs to meet design constraints. However, selecting the most suitable accelerator involves Design Space Exploration (DSE), a process that is usually time-consuming and requires significant manual effort. Our work presents approaches to expedite the DSE process by identifying the most appropriate GPGPU for CNN inferencing systems. We have developed a quick and precise technique for forecasting the power and performance of CNNs during inference, with a MAPE of 5.03% and 5.94%, respectively. Our approach empowers computer architects to estimate power and performance in the early stages of development, reducing the necessity for numerous prototypes. This saves time and money while also improving the time-to-market period.
</details>
<details>
<summary>摘要</summary>
efficient and timely calculations of Machine Learning (ML) algorithms are essential for emerging technologies like autonomous driving, the Internet of Things (IoT), and edge computing. One of the primary ML algorithms used in such systems is Convolutional Neural Networks (CNNs), which demand high computational resources. This requirement has led to the use of ML accelerators like GPGPUs to meet design constraints. However, selecting the most suitable accelerator involves Design Space Exploration (DSE), a process that is usually time-consuming and requires significant manual effort. Our work presents approaches to expedite the DSE process by identifying the most appropriate GPGPU for CNN inferencing systems. We have developed a quick and precise technique for forecasting the power and performance of CNNs during inference, with a MAPE of 5.03% and 5.94%, respectively. Our approach empowers computer architects to estimate power and performance in the early stages of development, reducing the necessity for numerous prototypes. This saves time and money while also improving the time-to-market period.Here's the translation in Traditional Chinese as well:efficient and timely calculations of Machine Learning (ML) algorithms are essential for emerging technologies like autonomous driving, the Internet of Things (IoT), and edge computing. One of the primary ML algorithms used in such systems is Convolutional Neural Networks (CNNs), which demand high computational resources. This requirement has led to the use of ML accelerators like GPGPUs to meet design constraints. However, selecting the most suitable accelerator involves Design Space Exploration (DSE), a process that is usually time-consuming and requires significant manual effort. Our work presents approaches to expedite the DSE process by identifying the most appropriate GPGPU for CNN inferencing systems. We have developed a quick and precise technique for forecasting the power and performance of CNNs during inference, with a MAPE of 5.03% and 5.94%, respectively. Our approach empowers computer architects to estimate power and performance in the early stages of development, reducing the necessity for numerous prototypes. This saves time and money while also improving the time-to-market period.
</details></li>
</ul>
<hr>
<h2 id="FINER-Enhancing-State-of-the-art-Classifiers-with-Feature-Attribution-to-Facilitate-Security-Analysis"><a href="#FINER-Enhancing-State-of-the-art-Classifiers-with-Feature-Attribution-to-Facilitate-Security-Analysis" class="headerlink" title="FINER: Enhancing State-of-the-art Classifiers with Feature Attribution to Facilitate Security Analysis"></a>FINER: Enhancing State-of-the-art Classifiers with Feature Attribution to Facilitate Security Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05362">http://arxiv.org/abs/2308.05362</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/e0hyl/finer-explain">https://github.com/e0hyl/finer-explain</a></li>
<li>paper_authors: Yiling He, Jian Lou, Zhan Qin, Kui Ren</li>
<li>for: 这篇论文的目的是提出一种高精度和高可读性的风险检测类фикатор解释框架，以便减少安全分析人员的工作负担。</li>
<li>methods: 该论文使用了特征参与（FA）方法来解释深度学习模型，并通过自适应task知识调整和集成FA方法来提高解释的智能性。</li>
<li>results: 对于风险检测任务，FINER可以提供高精度和高可读性的解释，并且在恶意软件分析中表现更高效。<details>
<summary>Abstract</summary>
Deep learning classifiers achieve state-of-the-art performance in various risk detection applications. They explore rich semantic representations and are supposed to automatically discover risk behaviors. However, due to the lack of transparency, the behavioral semantics cannot be conveyed to downstream security experts to reduce their heavy workload in security analysis. Although feature attribution (FA) methods can be used to explain deep learning, the underlying classifier is still blind to what behavior is suspicious, and the generated explanation cannot adapt to downstream tasks, incurring poor explanation fidelity and intelligibility. In this paper, we propose FINER, the first framework for risk detection classifiers to generate high-fidelity and high-intelligibility explanations. The high-level idea is to gather explanation efforts from model developer, FA designer, and security experts. To improve fidelity, we fine-tune the classifier with an explanation-guided multi-task learning strategy. To improve intelligibility, we engage task knowledge to adjust and ensemble FA methods. Extensive evaluations show that FINER improves explanation quality for risk detection. Moreover, we demonstrate that FINER outperforms a state-of-the-art tool in facilitating malware analysis.
</details>
<details>
<summary>摘要</summary>
深度学习分类器在不同的风险检测应用中实现了状态的最佳性能。它们探索了丰富的 semantic 表示，并被认为可以自动发现风险行为。然而，由于lack of transparency，risk 行为的semantic不能被传递给下游安全专家进行安全分析，从而增加了安全分析的重量。虽然 feature attribution（FA）方法可以用来解释深度学习，但下游分类器仍然无法了解哪些行为是可疑的，并且生成的解释无法适应下游任务，导致低效解释准确性和可读性。在这篇论文中，我们提出了 FINER，第一个用于风险检测分类器生成高准确性和高可读性解释的框架。高级想法是将解释努力集中于模型开发者、FA设计者和安全专家。为了提高准确性，我们使用解释指导多任务学习策略来练化分类器。为了提高可读性，我们利用任务知识来调整和组合 FA 方法。广泛评估表明，FINER 可以提高风险检测解释质量。此外，我们还证明了 FINER 可以超越当前领域的一个状态的工具在攻击分析方面帮助更好。
</details></li>
</ul>
<hr>
<h2 id="Preemptive-Detection-of-Fake-Accounts-on-Social-Networks-via-Multi-Class-Preferential-Attachment-Classifiers"><a href="#Preemptive-Detection-of-Fake-Accounts-on-Social-Networks-via-Multi-Class-Preferential-Attachment-Classifiers" class="headerlink" title="Preemptive Detection of Fake Accounts on Social Networks via Multi-Class Preferential Attachment Classifiers"></a>Preemptive Detection of Fake Accounts on Social Networks via Multi-Class Preferential Attachment Classifiers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05353">http://arxiv.org/abs/2308.05353</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adam Breuer, Nazanin Khosravani, Michael Tingley, Bradford Cottel<br>for:这篇论文描述了一种新的算法 called Preferential Attachment k-class Classifier (PreAttacK)，用于检测社交网络中的假账户。这些算法在过去几年中已经达到了高精度水平，但是它们通常是通过利用假账户的朋友关系或它们与其他人分享的内容来实现的。PreAttacK是这些方法的巨大变革。methods:作者们提供了一些初次分布分析，描述了新的假账户如何在社交网络中首次请求朋友关系。他们发现，even before a new account has made friends or shared content, these initial friend request behaviors evoke a natural multi-class extension of the canonical Preferential Attachment model of social network growth。作者们使用这个模型来 derive a new algorithm，PreAttacK。results:作者们证明了，在相关的问题实例中，PreAttacK可以 Near-optimally approximate the posterior probability that a new account is fake under this multi-class Preferential Attachment model of new accounts’ (not-yet-answered) friend requests。这是首次提供了对新用户的假账户检测的证明保证，而不需要强烈的同化假设。这种原则化的方法也使得PreAttacK成为了唯一具有证明保证的算法，在全球Facebook网络上实现了状态当前的最佳性能。<details>
<summary>Abstract</summary>
In this paper, we describe a new algorithm called Preferential Attachment k-class Classifier (PreAttacK) for detecting fake accounts in a social network. Recently, several algorithms have obtained high accuracy on this problem. However, they have done so by relying on information about fake accounts' friendships or the content they share with others--the very things we seek to prevent. PreAttacK represents a significant departure from these approaches. We provide some of the first detailed distributional analyses of how new fake (and real) accounts first attempt to request friends after joining a major network (Facebook). We show that even before a new account has made friends or shared content, these initial friend request behaviors evoke a natural multi-class extension of the canonical Preferential Attachment model of social network growth. We use this model to derive a new algorithm, PreAttacK. We prove that in relevant problem instances, PreAttacK near-optimally approximates the posterior probability that a new account is fake under this multi-class Preferential Attachment model of new accounts' (not-yet-answered) friend requests. These are the first provable guarantees for fake account detection that apply to new users, and that do not require strong homophily assumptions. This principled approach also makes PreAttacK the only algorithm with provable guarantees that obtains state-of-the-art performance on new users on the global Facebook network, where it converges to AUC=0.9 after new users send + receive a total of just 20 not-yet-answered friend requests. For comparison, state-of-the-art benchmarks do not obtain this AUC even after observing additional data on new users' first 100 friend requests. Thus, unlike mainstream algorithms, PreAttacK converges before the median new fake account has made a single friendship (accepted friend request) with a human.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们描述了一种新的算法 called Preferential Attachment k-class Classifier (PreAttacK)，用于检测社交网络中的假账户。最近，一些算法已经在这个问题上取得了高准确率，但是它们通常是通过利用假账户的朋友关系或它们与其他人共享的内容来实现的—— precisamente el que we seek to prevent. PreAttacK represents a significant departure from these approaches. We provide some of the first detailed distributional analyses of how new fake (and real) accounts first attempt to request friends after joining a major network (Facebook). We show that even before a new account has made friends or shared content, these initial friend request behaviors evoke a natural multi-class extension of the canonical Preferential Attachment model of social network growth. We use this model to derive a new algorithm, PreAttacK. We prove that in relevant problem instances, PreAttacK near-optimally approximates the posterior probability that a new account is fake under this multi-class Preferential Attachment model of new accounts' (not-yet-answered) friend requests. These are the first provable guarantees for fake account detection that apply to new users, and that do not require strong homophily assumptions. This principled approach also makes PreAttacK the only algorithm with provable guarantees that obtains state-of-the-art performance on new users on the global Facebook network, where it converges to AUC=0.9 after new users send + receive a total of just 20 not-yet-answered friend requests. For comparison, state-of-the-art benchmarks do not obtain this AUC even after observing additional data on new users' first 100 friend requests. Thus, unlike mainstream algorithms, PreAttacK converges before the median new fake account has made a single friendship (accepted friend request) with a human.
</details></li>
</ul>
<hr>
<h2 id="RTLLM-An-Open-Source-Benchmark-for-Design-RTL-Generation-with-Large-Language-Model"><a href="#RTLLM-An-Open-Source-Benchmark-for-Design-RTL-Generation-with-Large-Language-Model" class="headerlink" title="RTLLM: An Open-Source Benchmark for Design RTL Generation with Large Language Model"></a>RTLLM: An Open-Source Benchmark for Design RTL Generation with Large Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05345">http://arxiv.org/abs/2308.05345</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yao Lu, Shang Liu, Qijun Zhang, Zhiyao Xie</li>
<li>for: 本研究旨在提出一个开源 benchmark，用于使用自然语言指令生成适用于快速硬件设计的 RTL 架构。</li>
<li>methods: 本研究使用了 GPT-3.5 进行自然语言指令生成，并提出了一种名为 “自然语言观察”的技术，可以帮助提高 GPT-3.5 的性能。</li>
<li>results: 本研究通过使用自然语言指令生成 RTL 架构，并通过三个进步目标进行评估，包括 syntax goal、functionality goal 和 design quality goal。结果显示，使用自然语言指令可以实现高质量的 RTL 架构生成。<details>
<summary>Abstract</summary>
Inspired by the recent success of large language models (LLMs) like ChatGPT, researchers start to explore the adoption of LLMs for agile hardware design, such as generating design RTL based on natural-language instructions. However, in existing works, their target designs are all relatively simple and in a small scale, and proposed by the authors themselves, making a fair comparison among different LLM solutions challenging. In addition, many prior works only focus on the design correctness, without evaluating the design qualities of generated design RTL. In this work, we propose an open-source benchmark named RTLLM, for generating design RTL with natural language instructions. To systematically evaluate the auto-generated design RTL, we summarized three progressive goals, named syntax goal, functionality goal, and design quality goal. This benchmark can automatically provide a quantitative evaluation of any given LLM-based solution. Furthermore, we propose an easy-to-use yet surprisingly effective prompt engineering technique named self-planning, which proves to significantly boost the performance of GPT-3.5 in our proposed benchmark.
</details>
<details>
<summary>摘要</summary>
受大语言模型（LLM）如ChatGPT的成功启发，研究人员开始探索将LLM应用于快速硬件设计，如通过自然语言指令生成设计RTL。然而，现有的工作都是相对较少规模和自身提出的设计，具有许多缺陷，不能准确地评估不同LLM解决方案之间的比较。此外，许多前期工作只关注设计正确性，而忽略生成设计RTL的设计质量。在这种情况下，我们提出了一个开源的标准套件名为RTLLM，用于生成设计RTL通过自然语言指令。通过系统地评估自动生成的设计RTL，我们提出了三个进攻性目标，即语法目标、功能目标和设计质量目标。这个套件可以自动提供任何给定LLM解决方案的量化评估。此外，我们还提出了一种易于使用却有效的自我规划技术，名为自我规划，可以帮助GPT-3.5在我们提出的套件中表现出色。
</details></li>
</ul>
<hr>
<h2 id="OpenProteinSet-Training-data-for-structural-biology-at-scale"><a href="#OpenProteinSet-Training-data-for-structural-biology-at-scale" class="headerlink" title="OpenProteinSet: Training data for structural biology at scale"></a>OpenProteinSet: Training data for structural biology at scale</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05326">http://arxiv.org/abs/2308.05326</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aqlaboratory/openfold">https://github.com/aqlaboratory/openfold</a></li>
<li>paper_authors: Gustaf Ahdritz, Nazim Bouatta, Sachin Kadyan, Lukas Jarosch, Daniel Berenberg, Ian Fisk, Andrew M. Watkins, Stephen Ra, Richard Bonneau, Mohammed AlQuraishi</li>
<li>for: 这个论文主要是为了提供一个大规模的蛋白质多序列对 alignment（MSA）数据集，以便用于蛋白质结构设计、蛋白质功能预测等 bioinformatics 任务。</li>
<li>methods: 这个论文使用了 transformers 来直接对大量的 raw MSA 进行attend，以及对 Protein Data Bank 中的结构同源者进行关联。</li>
<li>results: 该论文引入了 OpenProteinSet，一个开源的蛋白质多序列对数据集，包括 более than 16 万个 MSA，以及与 Protein Data Bank 中的结构同源者和 AlphaFold2 蛋白质结构预测结果。<details>
<summary>Abstract</summary>
Multiple sequence alignments (MSAs) of proteins encode rich biological information and have been workhorses in bioinformatic methods for tasks like protein design and protein structure prediction for decades. Recent breakthroughs like AlphaFold2 that use transformers to attend directly over large quantities of raw MSAs have reaffirmed their importance. Generation of MSAs is highly computationally intensive, however, and no datasets comparable to those used to train AlphaFold2 have been made available to the research community, hindering progress in machine learning for proteins. To remedy this problem, we introduce OpenProteinSet, an open-source corpus of more than 16 million MSAs, associated structural homologs from the Protein Data Bank, and AlphaFold2 protein structure predictions. We have previously demonstrated the utility of OpenProteinSet by successfully retraining AlphaFold2 on it. We expect OpenProteinSet to be broadly useful as training and validation data for 1) diverse tasks focused on protein structure, function, and design and 2) large-scale multimodal machine learning research.
</details>
<details>
<summary>摘要</summary>
多个序列对Alignment（MSA）的蛋白质编码着丰富的生物信息，在生物信息学方法中作为保持者工具用于蛋白质设计和蛋白质结构预测已经有几十年的历史。最近的突破，如AlphaFold2，使用转换器直接在大量的原始MSA上进行访问，重新确认了它们的重要性。生成MSA的计算极其计算昂贵，然而，与AlphaFold2训练所用数据集相同的大规模数据集没有被研究者社区公开，这阻碍了蛋白质机器学习的进步。为了解决这个问题，我们介绍OpenProteinSet，一个开源的蛋白质多序列对Alignment资源，包括More than 16 million MSAs，与蛋白质数据库相关的结构同源者，以及AlphaFold2蛋白质结构预测。我们在OpenProteinSet上成功重新训练AlphaFold2，并预计OpenProteinSet将广泛用于蛋白质结构、功能和设计的多种任务，以及大规模多Modal机器学习研究。
</details></li>
</ul>
<hr>
<h2 id="Homophily-enhanced-Structure-Learning-for-Graph-Clustering"><a href="#Homophily-enhanced-Structure-Learning-for-Graph-Clustering" class="headerlink" title="Homophily-enhanced Structure Learning for Graph Clustering"></a>Homophily-enhanced Structure Learning for Graph Clustering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05309">http://arxiv.org/abs/2308.05309</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/galogm/hole">https://github.com/galogm/hole</a></li>
<li>paper_authors: Ming Gu, Gaoming Yang, Sheng Zhou, Ning Ma, Jiawei Chen, Qiaoyu Tan, Meihan Liu, Jiajun Bu</li>
<li>for: 这个论文的目的是提出一种基于图 neural network 的图 clustering 方法，以提高图分类的性能。</li>
<li>methods: 该方法使用两种结构学习模块：幂等相关度估计和群体相关简化，以提高 GNN 的性能。</li>
<li>results: 对七种不同类型和规模的测试数据集进行了广泛的实验，并与状态的基eline进行比较，得到了 HoLe 的超越性。<details>
<summary>Abstract</summary>
Graph clustering is a fundamental task in graph analysis, and recent advances in utilizing graph neural networks (GNNs) have shown impressive results. Despite the success of existing GNN-based graph clustering methods, they often overlook the quality of graph structure, which is inherent in real-world graphs due to their sparse and multifarious nature, leading to subpar performance. Graph structure learning allows refining the input graph by adding missing links and removing spurious connections. However, previous endeavors in graph structure learning have predominantly centered around supervised settings, and cannot be directly applied to our specific clustering tasks due to the absence of ground-truth labels. To bridge the gap, we propose a novel method called \textbf{ho}mophily-enhanced structure \textbf{le}arning for graph clustering (HoLe). Our motivation stems from the observation that subtly enhancing the degree of homophily within the graph structure can significantly improve GNNs and clustering outcomes. To realize this objective, we develop two clustering-oriented structure learning modules, i.e., hierarchical correlation estimation and cluster-aware sparsification. The former module enables a more accurate estimation of pairwise node relationships by leveraging guidance from latent and clustering spaces, while the latter one generates a sparsified structure based on the similarity matrix and clustering assignments. Additionally, we devise a joint optimization approach alternating between training the homophily-enhanced structure learning and GNN-based clustering, thereby enforcing their reciprocal effects. Extensive experiments on seven benchmark datasets of various types and scales, across a range of clustering metrics, demonstrate the superiority of HoLe against state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
GRaph clustering是图像分析的基本任务，而最近的GRaph Neural Networks（GNN）的进步已经显示出了惊人的成果。尽管现有的GNN基于图 clustering方法已经取得了成功，但它们通常忽略图structure的质量，这导致了表现不佳。图structure学习可以改善输入图的精度，但以往的图structure学习尝试都是在监督学习设置下进行的，因此无法直接应用于我们的具体 clustering 任务。为了填补这个差距，我们提出了一种新的方法called homophily-enhanced structure learning for graph clustering（HoLe）。我们的动机来自于观察，通过轻度提高图中 Node 之间的同性度，可以显著改善 GNN 和 clustering 结果。为实现这个目标，我们开发了两种 clustering-oriented structure learning模块：层次相关度估计和集群意向简化。前者模块可以更准确地估计 Node 之间的对应关系，通过利用幽默和 clustering 空间的指导，而后者模块可以基于对应矩阵和 clustering 分配生成一个简化的结构。此外，我们提出了一种联合优化方法，通过在GNN-based clustering和homophily-enhanced structure learning之间交互训练，以便强制这两者之间的相互作用。广泛的实验表明，HoLe 在七种不同类型和规模的数据集上，以及不同的 clustering 维度上，均超过了状态艺术基eline。
</details></li>
</ul>
<hr>
<h2 id="From-CNN-to-Transformer-A-Review-of-Medical-Image-Segmentation-Models"><a href="#From-CNN-to-Transformer-A-Review-of-Medical-Image-Segmentation-Models" class="headerlink" title="From CNN to Transformer: A Review of Medical Image Segmentation Models"></a>From CNN to Transformer: A Review of Medical Image Segmentation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05305">http://arxiv.org/abs/2308.05305</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenjian Yao, Jiajun Bai, Wei Liao, Yuheng Chen, Mengjuan Liu, Yao Xie</li>
<li>for: 这篇论文的目的是对最近几年内的医疗影像分类模型进行评估和探讨。</li>
<li>methods: 这篇论文使用的方法包括U-Net和其变种，以及基于传播模型的TransUNet。</li>
<li>results: 论文评估了四种最具代表性的医疗影像分类模型，并量化评估它们在两个标准资料集（i.e., 肺结核X光和 ovary tumor）上的性能。<details>
<summary>Abstract</summary>
Medical image segmentation is an important step in medical image analysis, especially as a crucial prerequisite for efficient disease diagnosis and treatment. The use of deep learning for image segmentation has become a prevalent trend. The widely adopted approach currently is U-Net and its variants. Additionally, with the remarkable success of pre-trained models in natural language processing tasks, transformer-based models like TransUNet have achieved desirable performance on multiple medical image segmentation datasets. In this paper, we conduct a survey of the most representative four medical image segmentation models in recent years. We theoretically analyze the characteristics of these models and quantitatively evaluate their performance on two benchmark datasets (i.e., Tuberculosis Chest X-rays and ovarian tumors). Finally, we discuss the main challenges and future trends in medical image segmentation. Our work can assist researchers in the related field to quickly establish medical segmentation models tailored to specific regions.
</details>
<details>
<summary>摘要</summary>
医学图像分割是医学图像分析中非常重要的步骤，特别是为了高效的疾病诊断和治疗。深度学习在图像分割方面的应用已成为一种普遍的趋势。目前最广泛采用的是U-Net和其变体。此外，由于自然语言处理任务中预训练模型的出色成绩，如Transformer基于的TransUNet模型在多个医学图像分割数据集上实现了满意的表现。本文对最近几年内最具代表性的四种医学图像分割模型进行了抽查，分析了这些模型的特点，并对两个标准数据集（即肺部X射线和卵巢肿瘤）进行了量化评估。最后，我们讨论了医学图像分割中的主要挑战和未来趋势。本文可以帮助相关领域的研究人员快速设置适应特定地区的医学分割模型。
</details></li>
</ul>
<hr>
<h2 id="Byzantine-Robust-Decentralized-Stochastic-Optimization-with-Stochastic-Gradient-Noise-Independent-Learning-Error"><a href="#Byzantine-Robust-Decentralized-Stochastic-Optimization-with-Stochastic-Gradient-Noise-Independent-Learning-Error" class="headerlink" title="Byzantine-Robust Decentralized Stochastic Optimization with Stochastic Gradient Noise-Independent Learning Error"></a>Byzantine-Robust Decentralized Stochastic Optimization with Stochastic Gradient Noise-Independent Learning Error</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05292">http://arxiv.org/abs/2308.05292</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jie Peng, Weiyu Li, Qing Ling</li>
<li>for: 这paper研究了一种分布式网络上的奥尔拜托抗衡梯度下降优化方法，该方法在每个代理都 periodic communication with its neighbors，并使用梯度下降来更新自己的本地模型。</li>
<li>methods: 该paper使用了两种抗衡方法，即Stochastic Average Gradient Algorithm (SAGA)和Loopless Stochastic Variance-Reduced Gradient (LSVRG)，以消除梯度下降噪声的负面影响。</li>
<li>results: 该paper的两种方法BRAVO-SAGA和BRAVO-LSVRG都能同时实现线性减少速度和梯度下降噪声独立的学习误差，这些学习误差是对一类基于总变量（TV）范数regularization和随机下降更新的方法所optimal。<details>
<summary>Abstract</summary>
This paper studies Byzantine-robust stochastic optimization over a decentralized network, where every agent periodically communicates with its neighbors to exchange local models, and then updates its own local model by stochastic gradient descent (SGD). The performance of such a method is affected by an unknown number of Byzantine agents, which conduct adversarially during the optimization process. To the best of our knowledge, there is no existing work that simultaneously achieves a linear convergence speed and a small learning error. We observe that the learning error is largely dependent on the intrinsic stochastic gradient noise. Motivated by this observation, we introduce two variance reduction methods, stochastic average gradient algorithm (SAGA) and loopless stochastic variance-reduced gradient (LSVRG), to Byzantine-robust decentralized stochastic optimization for eliminating the negative effect of the stochastic gradient noise. The two resulting methods, BRAVO-SAGA and BRAVO-LSVRG, enjoy both linear convergence speeds and stochastic gradient noise-independent learning errors. Such learning errors are optimal for a class of methods based on total variation (TV)-norm regularization and stochastic subgradient update. We conduct extensive numerical experiments to demonstrate their effectiveness under various Byzantine attacks.
</details>
<details>
<summary>摘要</summary>
We observe that the learning error is largely dependent on the intrinsic stochastic gradient noise. To address this issue, we introduce two variance reduction methods, stochastic average gradient algorithm (SAGA) and loopless stochastic variance-reduced gradient (LSVRG), to Byzantine-robust decentralized stochastic optimization. These two methods eliminate the negative effect of stochastic gradient noise and achieve both linear convergence speeds and stochastic gradient noise-independent learning errors.The two resulting methods, BRAVO-SAGA and BRAVO-LSVRG, are optimal for a class of methods based on total variation (TV)-norm regularization and stochastic subgradient update. We conduct extensive numerical experiments to demonstrate their effectiveness under various Byzantine attacks.
</details></li>
</ul>
<hr>
<h2 id="Investigating-disaster-response-through-social-media-data-and-the-Susceptible-Infected-Recovered-SIR-model-A-case-study-of-2020-Western-U-S-wildfire-season"><a href="#Investigating-disaster-response-through-social-media-data-and-the-Susceptible-Infected-Recovered-SIR-model-A-case-study-of-2020-Western-U-S-wildfire-season" class="headerlink" title="Investigating disaster response through social media data and the Susceptible-Infected-Recovered (SIR) model: A case study of 2020 Western U.S. wildfire season"></a>Investigating disaster response through social media data and the Susceptible-Infected-Recovered (SIR) model: A case study of 2020 Western U.S. wildfire season</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05281">http://arxiv.org/abs/2308.05281</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihui Ma, Lingyao Li, Libby Hemphill, Gregory B. Baecher</li>
<li>for: This paper aims to provide decision-makers with a quantitative approach to measure disaster response and support their decision-making processes during a disaster.</li>
<li>methods: The authors use BERT topic modeling to cluster topics from Twitter data, and a Susceptible-Infected-Recovered (SIR) theory to explore the magnitude and velocity of topic diffusion on Twitter.</li>
<li>results: The results show that Twitter users mainly focused on three topics: “health impact,” “damage,” and “evacuation,” and the estimated parameters obtained from the SIR model in selected cities revealed that residents exhibited a high level of several concerns during the wildfire.<details>
<summary>Abstract</summary>
Effective disaster response is critical for affected communities. Responders and decision-makers would benefit from reliable, timely measures of the issues impacting their communities during a disaster, and social media offers a potentially rich data source. Social media can reflect public concerns and demands during a disaster, offering valuable insights for decision-makers to understand evolving situations and optimize resource allocation. We used Bidirectional Encoder Representations from Transformers (BERT) topic modeling to cluster topics from Twitter data. Then, we conducted a temporal-spatial analysis to examine the distribution of these topics across different regions during the 2020 western U.S. wildfire season. Our results show that Twitter users mainly focused on three topics:"health impact," "damage," and "evacuation." We used the Susceptible-Infected-Recovered (SIR) theory to explore the magnitude and velocity of topic diffusion on Twitter. The results displayed a clear relationship between topic trends and wildfire propagation patterns. The estimated parameters obtained from the SIR model in selected cities revealed that residents exhibited a high level of several concerns during the wildfire. Our study details how the SIR model and topic modeling using social media data can provide decision-makers with a quantitative approach to measure disaster response and support their decision-making processes.
</details>
<details>
<summary>摘要</summary>
Effective disaster response is crucial for affected communities. Responders and decision-makers can benefit from reliable, timely measures of the issues impacting their communities during a disaster, and social media offers a potentially rich data source. Social media can reflect public concerns and demands during a disaster, providing valuable insights for decision-makers to understand evolving situations and optimize resource allocation. 我们使用了自然语言处理技术，具体来说是使用Transformers（BERT）主题分类来分类Twitter数据中的主题。然后，我们进行了时间空间分析，检查不同地区在2020年西部美国野火季节中主题的分布。我们发现Twitter用户主要关注了“健康影响”、“损害”和“疏散”三个主题。我们使用了感染传播理论（SIR）模型来探究Twitter上主题的Diffusion特性。结果表明，主题趋势与野火传播模式之间存在明确的关系。我们在选择的城市中获得了SIR模型的参数估计结果，显示了居民在野火期间表现出了高水平的多种担忧。我们的研究详细介绍了如何使用社交媒体数据和SIR模型来为决策者提供量化的方法，以支持他们的决策过程。
</details></li>
</ul>
<hr>
<h2 id="Cross-heterogeneity-Graph-Few-shot-Learning"><a href="#Cross-heterogeneity-Graph-Few-shot-Learning" class="headerlink" title="Cross-heterogeneity Graph Few-shot Learning"></a>Cross-heterogeneity Graph Few-shot Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05275">http://arxiv.org/abs/2308.05275</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pengfei Ding, Yan Wang, Guanfeng Liu</li>
<li>for:  Addressing the label sparsity issue in heterogeneous graphs (HGs) with few-shot learning.</li>
<li>methods:  Propose a novel model for Cross-heterogeneity Graph Few-shot Learning (CGFL), including extracting meta-patterns and a multi-view heterogeneous graph neural network (MHGN) to learn meta-patterns across HGs, and a score module to measure the informativeness of labeled samples and determine the transferability of each source HG.</li>
<li>results:  Extensive experiments on four real-world datasets have demonstrated the superior performance of CGFL over the state-of-the-art methods.<details>
<summary>Abstract</summary>
In recent years, heterogeneous graph few-shot learning has been proposed to address the label sparsity issue in heterogeneous graphs (HGs), which contain various types of nodes and edges. The existing methods have achieved good performance by transferring generalized knowledge extracted from rich-labeled classes in source HG(s) to few-labeled classes in a target HG. However, these methods only consider the single-heterogeneity scenario where the source and target HGs share a fixed set of node/edge types, ignoring the more general scenario of cross-heterogeneity, where each HG can have a different and non-fixed set of node/edge types. To this end, we focus on the unexplored cross-heterogeneity scenario and propose a novel model for Cross-heterogeneity Graph Few-shot Learning, namely CGFL. In CGFL, we first extract meta-patterns to capture heterogeneous information and propose a multi-view heterogeneous graph neural network (MHGN) to learn meta-patterns across HGs. Then, we propose a score module to measure the informativeness of labeled samples and determine the transferability of each source HG. Finally, by integrating MHGN and the score module into a meta-learning mechanism, CGFL can effectively transfer generalized knowledge to predict new classes with few-labeled data. Extensive experiments on four real-world datasets have demonstrated the superior performance of CGFL over the state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
Recently, researchers have proposed heterogeneous graph few-shot learning (HGFSL) to address the label sparsity issue in heterogeneous graphs (HGs), which contain various types of nodes and edges. Existing methods have achieved good performance by transferring generalized knowledge extracted from rich-labeled classes in source HGs to few-labeled classes in a target HG. However, these methods only consider the single-heterogeneity scenario where the source and target HGs share a fixed set of node/edge types, ignoring the more general scenario of cross-heterogeneity, where each HG can have a different and non-fixed set of node/edge types. To address this issue, we focus on the unexplored cross-heterogeneity scenario and propose a novel model for Cross-heterogeneity Graph Few-shot Learning (CGFL).In CGFL, we first extract meta-patterns to capture heterogeneous information and propose a multi-view heterogeneous graph neural network (MHGN) to learn meta-patterns across HGs. Then, we propose a score module to measure the informativeness of labeled samples and determine the transferability of each source HG. Finally, by integrating MHGN and the score module into a meta-learning mechanism, CGFL can effectively transfer generalized knowledge to predict new classes with few-labeled data. Extensive experiments on four real-world datasets have demonstrated the superior performance of CGFL over the state-of-the-art methods.
</details></li>
</ul>
<hr>
<h2 id="Data-driven-Intra-Autonomous-Systems-Graph-Generator"><a href="#Data-driven-Intra-Autonomous-Systems-Graph-Generator" class="headerlink" title="Data-driven Intra-Autonomous Systems Graph Generator"></a>Data-driven Intra-Autonomous Systems Graph Generator</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05254">http://arxiv.org/abs/2308.05254</a></li>
<li>repo_url: None</li>
<li>paper_authors: Caio Vinicius Dadauto, Nelson Luis Saldanha da Fonseca, Ricardo da Silva Torres</li>
<li>for: 本研究旨在提出一种深度学习基于的图生成器，用于生成覆盖互联网络Autonomous System (AS)的 sintetic 图。</li>
<li>methods: 本研究使用了一种名为 Filtered Recurrent Multi-level (FRM) 算法来提取社区，并使用了 Internet Topology Data Kit (ITDK) 项目中的实际网络图形成一个大规模的实际网络图集合。</li>
<li>results: 研究表明，DGGI 生成的 sintetic 图能够准确地复制实际网络图中的性质，包括中心性、嵌入性、相互关联性和节点度。 DGGI 生成器比现有的互联网 topology 生成器更高效，在 Maximum Mean Discrepancy (MMD) 指标上提高了84.4%、95.1%、97.9% 和 94.7%。<details>
<summary>Abstract</summary>
This paper introduces a novel deep-learning based generator of synthetic graphs that represent intra-Autonomous System (AS) in the Internet, named Deep-generative graphs for the Internet (DGGI). It also presents a novel massive dataset of real intra-AS graphs extracted from the project Internet Topology Data Kit (ITDK), called Internet Graphs (IGraphs). To create IGraphs, the Filtered Recurrent Multi-level (FRM) algorithm for community extraction was developed. It is shown that DGGI creates synthetic graphs which accurately reproduce the properties of centrality, clustering, assortativity, and node degree. The DGGI generator overperforms existing Internet topology generators. On average, DGGI improves the Maximum Mean Discrepancy (MMD) metric 84.4%, 95.1%, 97.9%, and 94.7% for assortativity, betweenness, clustering, and node degree, respectively.
</details>
<details>
<summary>摘要</summary>
这份论文介绍了一种新的深度学习基于的生成 sintetic 互联网（AS）图表示法，名为深度生成互联网图（DGGI）。它还发布了一个大量的实际 intra-AS 图据集，从项目互联网Topology数据集（ITDK）中提取出来，称为互联网图（IGraphs）。为创建 IGraphs，开发了一种Filtered Recurrent Multi-level（FRM）算法 для社区提取。研究表明，DGGI 生成的 sintetic 图具有与实际 intra-AS 图的性质相似的中心性、嵌入性、归一化性和节点度等特征。相比之下，DGGI 生成器在 existing Internet topology 生成器之上表现出优异，在 Maximum Mean Discrepancy（MMD）指标上提高了84.4%、95.1%、97.9% 和 94.7% 的平均提升。
</details></li>
</ul>
<hr>
<h2 id="AI-Enabled-Software-and-System-Architecture-Frameworks-Focusing-on-smart-Cyber-Physical-Systems-CPS"><a href="#AI-Enabled-Software-and-System-Architecture-Frameworks-Focusing-on-smart-Cyber-Physical-Systems-CPS" class="headerlink" title="AI-Enabled Software and System Architecture Frameworks: Focusing on smart Cyber-Physical Systems (CPS)"></a>AI-Enabled Software and System Architecture Frameworks: Focusing on smart Cyber-Physical Systems (CPS)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05239">http://arxiv.org/abs/2308.05239</a></li>
<li>repo_url: None</li>
<li>paper_authors: Armin Moin, Atta Badii, Stephan Günnemann, Moharram Challenger</li>
<li>for: 本文旨在为现代应用和组织提供适应数据科学和机器学习（ML）相关担忧的建筑框架。</li>
<li>methods: 本研究使用了文献综述和问卷调查方法，收集、分析和结合了77名专家的意见来提出和验证提案的建筑框架。</li>
<li>results: 本研究提出了两个集合的价值标准，用于评估和优化ML启用的Cyber-Physical Systems（CPS）的开发和性能评价，以及用于评估和优化开发和模型生命周期管道支持工具的价值标准。<details>
<summary>Abstract</summary>
Several architecture frameworks for software, systems, and enterprises have been proposed in the literature. They identified various stakeholders and defined architecture viewpoints and views to frame and address stakeholder concerns. However, the stakeholders with data science and Machine Learning (ML) related concerns, such as data scientists and data engineers, are yet to be included in existing architecture frameworks. Therefore, they failed to address the architecture viewpoints and views responsive to the concerns of the data science community. In this paper, we address this gap by establishing the architecture frameworks adapted to meet the requirements of modern applications and organizations where ML artifacts are both prevalent and crucial. In particular, we focus on ML-enabled Cyber-Physical Systems (CPSs) and propose two sets of merit criteria for their efficient development and performance assessment, namely the criteria for evaluating and benchmarking ML-enabled CPSs, and the criteria for evaluation and benchmarking of the tools intended to support users through the modeling and development pipeline. In this study, we deploy multiple empirical and qualitative research methods based on literature review and survey instruments including expert interviews and an online questionnaire. We collect, analyze, and integrate the opinions of 77 experts from more than 25 organizations in over 10 countries to devise and validate the proposed framework.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Financial-Fraud-Detection-A-Comparative-Study-of-Quantum-Machine-Learning-Models"><a href="#Financial-Fraud-Detection-A-Comparative-Study-of-Quantum-Machine-Learning-Models" class="headerlink" title="Financial Fraud Detection: A Comparative Study of Quantum Machine Learning Models"></a>Financial Fraud Detection: A Comparative Study of Quantum Machine Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05237">http://arxiv.org/abs/2308.05237</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nouhaila Innan, Muhammad Al-Zafar Khan, Mohamed Bennai</li>
<li>for: 这个研究是为了进行金融领域的诈欺探测，使用四种量子机器学习（QML）模型进行比较性研究。</li>
<li>methods: 这个研究使用了量子支持向量分类器模型，并取得了最高性能，具有0.98的F1分数。其他模型，如量子预测器、量子神经网络（QNN）和抽样量子神经网络，也展现了潜在的应用前景。</li>
<li>results: 研究发现，量子支持向量分类器模型在诈欺和非诈欺类别上的F1分数为0.98，其他模型也取得了可靠的结果，但存在一些限制。这些结果对于未来QML发展提供了新的见解和依据，但还需要更有效的量子算法和更大和复杂的数据集。<details>
<summary>Abstract</summary>
In this research, a comparative study of four Quantum Machine Learning (QML) models was conducted for fraud detection in finance. We proved that the Quantum Support Vector Classifier model achieved the highest performance, with F1 scores of 0.98 for fraud and non-fraud classes. Other models like the Variational Quantum Classifier, Estimator Quantum Neural Network (QNN), and Sampler QNN demonstrate promising results, propelling the potential of QML classification for financial applications. While they exhibit certain limitations, the insights attained pave the way for future enhancements and optimisation strategies. However, challenges exist, including the need for more efficient Quantum algorithms and larger and more complex datasets. The article provides solutions to overcome current limitations and contributes new insights to the field of Quantum Machine Learning in fraud detection, with important implications for its future development.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们进行了四种量子机器学习（QML）模型的比较研究，用于金融领域的诈骗检测。我们证明了量子支持向量分类模型在诈骗和非诈骗类中的表现最高，具有0.98的F1分数。其他模型如变量量子分类器、估计量子神经网络（QNN）和抽样QNN也展现了良好的结果，这推动了量子机器学习分类在金融应用中的潜力。尽管它们存在一些限制，但获得的洞察能够为未来的改进和优化策略提供基础。然而，还存在一些挑战，包括需要更高效的量子算法和更大和复杂的数据集。这篇文章提供了解决当前的限制方法，并为量子机器学习在诈骗检测领域的未来发展提供新的洞察和意义。
</details></li>
</ul>
<hr>
<h2 id="Spatial-Gated-Multi-Layer-Perceptron-for-Land-Use-and-Land-Cover-Mapping"><a href="#Spatial-Gated-Multi-Layer-Perceptron-for-Land-Use-and-Land-Cover-Mapping" class="headerlink" title="Spatial Gated Multi-Layer Perceptron for Land Use and Land Cover Mapping"></a>Spatial Gated Multi-Layer Perceptron for Land Use and Land Cover Mapping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05235">http://arxiv.org/abs/2308.05235</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aj1365/sgumlp">https://github.com/aj1365/sgumlp</a></li>
<li>paper_authors: Ali Jamali, Swalpa Kumar Roy, Danfeng Hong, Peter M Atkinson, Pedram Ghamisi<br>for:* The paper is written for those interested in land use land cover (LULC) mapping and the application of machine learning algorithms in remote sensing.methods:* The paper uses a combination of multi-layer perceptrons (MLPs) and spatial gating units (SGUs) to develop a new learning algorithm called SGU-MLP.* The SGU-MLP algorithm leverages both the strengths of MLPs and SGUs to improve the accuracy of LULC mapping.results:* The SGU-MLP algorithm outperformed several state-of-the-art CNN and CNN-ViT-based models, including HybridSN, ResNet, iFormer, EfficientFormer, and CoAtNet, in terms of average accuracy.* The SGU-MLP algorithm consistently outperformed the benchmark models in three experiments conducted in Houston, USA, Berlin, Germany, and Augsburg, Germany.Here is the simplified Chinese translation of the three key information points:for:* 这篇论文是为了探讨遥感识别和land use land cover（LULC）映射的应用而写的。methods:* 这篇论文使用了多层感知器（MLPs）和空间闭合单元（SGUs）组合来开发一种新的学习算法——SGU-MLP。* SGU-MLP算法利用了MLPs和SGUs的优点，以提高LULC映射的准确性。results:* SGU-MLP算法在HybridSN、ResNet、iFormer、EfficientFormer和CoAtNet等状态对比中，以average accuracy为标准，与多个参考模型进行了比较。* SGU-MLP算法在三个实验中（在houston、berlin和augsburg） consistently outperform了参考模型。<details>
<summary>Abstract</summary>
Convolutional Neural Networks (CNNs) are models that are utilized extensively for the hierarchical extraction of features. Vision transformers (ViTs), through the use of a self-attention mechanism, have recently achieved superior modeling of global contextual information compared to CNNs. However, to realize their image classification strength, ViTs require substantial training datasets. Where the available training data are limited, current advanced multi-layer perceptrons (MLPs) can provide viable alternatives to both deep CNNs and ViTs. In this paper, we developed the SGU-MLP, a learning algorithm that effectively uses both MLPs and spatial gating units (SGUs) for precise land use land cover (LULC) mapping. Results illustrated the superiority of the developed SGU-MLP classification algorithm over several CNN and CNN-ViT-based models, including HybridSN, ResNet, iFormer, EfficientFormer and CoAtNet. The proposed SGU-MLP algorithm was tested through three experiments in Houston, USA, Berlin, Germany and Augsburg, Germany. The SGU-MLP classification model was found to consistently outperform the benchmark CNN and CNN-ViT-based algorithms. For example, for the Houston experiment, SGU-MLP significantly outperformed HybridSN, CoAtNet, Efficientformer, iFormer and ResNet by approximately 15%, 19%, 20%, 21%, and 25%, respectively, in terms of average accuracy. The code will be made publicly available at https://github.com/aj1365/SGUMLP
</details>
<details>
<summary>摘要</summary>
convolutional neural networks (CNNs) 是模型，广泛应用于层次提取特征。 vision transformers (ViTs) 通过自我注意机制，在全局上下文信息模型方面，最近超越了 CNNs。然而，要实现图像分类强度，ViTs 需要大量的训练数据。当有限的训练数据available时，当前的高级多层感知器 (MLPs) 可以提供可靠的替代品，包括深度 CNNs 和 ViTs。在这篇论文中，我们开发了SGU-MLP 学习算法，它有效地结合 MLPs 和空间闭合单元 (SGUs)  для精确的土地用途地图。结果表明，我们开发的SGU-MLP 分类算法在多个实验中，包括 HOUSTON、BERLIN 和 AUGSBURG，都有superiority 于许多 CNN 和 CNN-ViT 基于的模型，包括 HybridSN、ResNet、iFormer、EfficientFormer 和 CoAtNet。我们提出的SGU-MLP 分类模型在 HOUSTON 实验中，与 HybridSN、CoAtNet、Efficientformer、iFormer 和 ResNet 相比，提高了15%、19%、20%、21% 和 25% 的平均准确率。我们将代码公开在 GitHub 上，详细的实验结果和代码将在https://github.com/aj1365/SGUMLP 上公布。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-the-Edge-and-Cloud-for-V2X-Based-Real-Time-Object-Detection-in-Autonomous-Driving"><a href="#Leveraging-the-Edge-and-Cloud-for-V2X-Based-Real-Time-Object-Detection-in-Autonomous-Driving" class="headerlink" title="Leveraging the Edge and Cloud for V2X-Based Real-Time Object Detection in Autonomous Driving"></a>Leveraging the Edge and Cloud for V2X-Based Real-Time Object Detection in Autonomous Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05234">http://arxiv.org/abs/2308.05234</a></li>
<li>repo_url: None</li>
<li>paper_authors: Faisal Hawlader, François Robinet, Raphaël Frank</li>
<li>for: 本研究旨在找到实时感知中最佳的折衶点，以提高自动驾驶汽车的安全性和可靠性。</li>
<li>methods: 本研究使用了虚拟数据集来训练对象检测模型，并研究了不同的卫星和云端计算策略的可行性。</li>
<li>results: 研究结果显示，通过使用JPEG和H.265压缩，可以在云端实现实时对象检测，并且比地面上的检测性能更高。<details>
<summary>Abstract</summary>
Environmental perception is a key element of autonomous driving because the information received from the perception module influences core driving decisions. An outstanding challenge in real-time perception for autonomous driving lies in finding the best trade-off between detection quality and latency. Major constraints on both computation and power have to be taken into account for real-time perception in autonomous vehicles. Larger object detection models tend to produce the best results, but are also slower at runtime. Since the most accurate detectors cannot run in real-time locally, we investigate the possibility of offloading computation to edge and cloud platforms, which are less resource-constrained. We create a synthetic dataset to train object detection models and evaluate different offloading strategies. Using real hardware and network simulations, we compare different trade-offs between prediction quality and end-to-end delay. Since sending raw frames over the network implies additional transmission delays, we also explore the use of JPEG and H.265 compression at varying qualities and measure their impact on prediction metrics. We show that models with adequate compression can be run in real-time on the cloud while outperforming local detection performance.
</details>
<details>
<summary>摘要</summary>
环境感知是自动驾驶中关键的元素，因为感知模块的信息会影响自动驾驶的核心决策。现实时感知对自动驾驶而言是一个重要的挑战，需要找到最佳的折衔点 между检测质量和延迟。自动驾驶车辆的实时感知受到计算和功率的重要限制。大型对象检测模型通常会生成最佳的结果，但它们在运行时也比较慢。由于最准确的检测器不能在本地实时运行，我们 investigate了将计算卸载到边缘和云平台上，这些平台具有较强的资源。我们创建了一个 sintetic 数据集来训练对象检测模型，并评估不同的卸载策略。使用真实的硬件和网络 simulate，我们比较了不同的妥协点 между预测质量和总终端延迟。由于发送Raw帧数据到网络会添加额外的传输延迟，我们还探索了使用 JPEG 和 H.265 压缩，并测量其对预测指标的影响。我们显示，使用合适的压缩可以在云上实时运行模型，并超越本地检测性能。
</details></li>
</ul>
<hr>
<h2 id="SegMatch-A-semi-supervised-learning-method-for-surgical-instrument-segmentation"><a href="#SegMatch-A-semi-supervised-learning-method-for-surgical-instrument-segmentation" class="headerlink" title="SegMatch: A semi-supervised learning method for surgical instrument segmentation"></a>SegMatch: A semi-supervised learning method for surgical instrument segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05232">http://arxiv.org/abs/2308.05232</a></li>
<li>repo_url: None</li>
<li>paper_authors: Meng Wei, Charlie Budd, Luis C. Garcia-Peraza-Herrera, Reuben Dorent, Miaojing Shi, Tom Vercauteren</li>
<li>for: 提高 Laparoscopic 和 Robotic 手术图像的分割精度，减少高成本的标注成本。</li>
<li>methods: 提出 SegMatch，一种基于 FixMatch 的 semi-supervised learning方法，通过弱版 augmentation 生成 pseudo-label，并在挑战性训练中使用 adversarial augmentation 来增强模型的鲁棒性和稳定性。</li>
<li>results: 在 Robust-MIS 2019 和 EndoVis 2017 数据集上进行了测试，结果表明，通过添加无标注数据进行训练，SegMatch 可以超越完全监督学习方法，并在不同的标签到无标签数据比例中具有更高的性能。<details>
<summary>Abstract</summary>
Surgical instrument segmentation is recognised as a key enabler to provide advanced surgical assistance and improve computer assisted interventions. In this work, we propose SegMatch, a semi supervised learning method to reduce the need for expensive annotation for laparoscopic and robotic surgical images. SegMatch builds on FixMatch, a widespread semi supervised classification pipeline combining consistency regularization and pseudo labelling, and adapts it for the purpose of segmentation. In our proposed SegMatch, the unlabelled images are weakly augmented and fed into the segmentation model to generate a pseudo-label to enforce the unsupervised loss against the output of the model for the adversarial augmented image on the pixels with a high confidence score. Our adaptation for segmentation tasks includes carefully considering the equivariance and invariance properties of the augmentation functions we rely on. To increase the relevance of our augmentations, we depart from using only handcrafted augmentations and introduce a trainable adversarial augmentation strategy. Our algorithm was evaluated on the MICCAI Instrument Segmentation Challenge datasets Robust-MIS 2019 and EndoVis 2017. Our results demonstrate that adding unlabelled data for training purposes allows us to surpass the performance of fully supervised approaches which are limited by the availability of training data in these challenges. SegMatch also outperforms a range of state-of-the-art semi-supervised learning semantic segmentation models in different labelled to unlabelled data ratios.
</details>
<details>
<summary>摘要</summary>
外科器械分割是认可为提供高级别外科协助和改进计算助手的关键因素。在这项工作中，我们提议SegMatch，一种半supervised学习方法，以减少外科 Laparoscopic和Robotic 图像分割需要的昂贵标注。SegMatch基于FixMatch，一种广泛使用的半supervised分类管道，并将其改编为分割任务。在我们的提议SegMatch中，无标图像被弱地扩展并feed into分割模型，以生成一个 pseudo-标签，以便对模型输出的图像像素进行强制检查。我们的改进包括仔细考虑图像分割任务中的等价性和不变性属性。为了增加我们的扩展的相关性，我们不再仅使用手工设计的扩展，而是引入一种可学习的对抗扩展策略。我们的算法在MICCAI Instrument Segmentation Challenge数据集Robust-MIS 2019和EndoVis 2017上进行了评估。我们的结果表明，在训练目标数据有限时，通过添加无标图像进行训练可以超越完全supervised方法的性能。SegMatch还比一些状态的半supervised学习semantic segmentation模型在不同的标签到无标签数据比率中表现出色。
</details></li>
</ul>
<hr>
<h2 id="Training-neural-networks-with-end-to-end-optical-backpropagation"><a href="#Training-neural-networks-with-end-to-end-optical-backpropagation" class="headerlink" title="Training neural networks with end-to-end optical backpropagation"></a>Training neural networks with end-to-end optical backpropagation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05226">http://arxiv.org/abs/2308.05226</a></li>
<li>repo_url: None</li>
<li>paper_authors: James Spall, Xianxin Guo, A. I. Lvovsky</li>
<li>for: 这个论文是为了实现光学 neural network 的训练和推理两个任务而写的。</li>
<li>methods: 这篇论文使用了耗尽性吸收器来实现非线性活化函数，并通过激光-探测器过程来实现训练。</li>
<li>results: 这篇论文成功实现了完全基于光学 процесс的 neural network 训练和推理，并且可以在不同的光学平台、材料和网络结构上进行适应。<details>
<summary>Abstract</summary>
Optics is an exciting route for the next generation of computing hardware for machine learning, promising several orders of magnitude enhancement in both computational speed and energy efficiency. However, to reach the full capacity of an optical neural network it is necessary that the computing not only for the inference, but also for the training be implemented optically. The primary algorithm for training a neural network is backpropagation, in which the calculation is performed in the order opposite to the information flow for inference. While straightforward in a digital computer, optical implementation of backpropagation has so far remained elusive, particularly because of the conflicting requirements for the optical element that implements the nonlinear activation function. In this work, we address this challenge for the first time with a surprisingly simple and generic scheme. Saturable absorbers are employed for the role of the activation units, and the required properties are achieved through a pump-probe process, in which the forward propagating signal acts as the pump and backward as the probe. Our approach is adaptable to various analog platforms, materials, and network structures, and it demonstrates the possibility of constructing neural networks entirely reliant on analog optical processes for both training and inference tasks.
</details>
<details>
<summary>摘要</summary>
什么是optics？optics是未来计算机硬件的新一代激光学技术，可以提供数个数个级别的计算速度和能效率提升。然而，要达到激光神经网络的完整能力，不仅需要推理部分实现光学计算，还需要训练部分也实现光学计算。主要算法用于训练神经网络是反射吗，在神经网络的信息流向中进行计算，而光学实现反射吗的问题尚未得到解决。在这种情况下，我们第一次解决这个挑战，使用可饱和吸收器来实现非线性活化函数。我们的方法可以适应不同的分析平台、材料和网络结构，并示出了完全通过光学过程实现神经网络的训练和推理任务。
</details></li>
</ul>
<hr>
<h2 id="Conceptualizing-Machine-Learning-for-Dynamic-Information-Retrieval-of-Electronic-Health-Record-Notes"><a href="#Conceptualizing-Machine-Learning-for-Dynamic-Information-Retrieval-of-Electronic-Health-Record-Notes" class="headerlink" title="Conceptualizing Machine Learning for Dynamic Information Retrieval of Electronic Health Record Notes"></a>Conceptualizing Machine Learning for Dynamic Information Retrieval of Electronic Health Record Notes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08494">http://arxiv.org/abs/2308.08494</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sharon Jiang, Shannon Shen, Monica Agrawal, Barbara Lam, Nicholas Kurtzman, Steven Horng, David Karger, David Sontag</li>
<li>for: 降低临床医生疲劳，提高医疗记录的效率和质量。</li>
<li>methods: 利用电子医疗纪录（EHR）审核日志进行机器学习监督，动态检索相关病历记录，提高记录过程中的信息检索效率。</li>
<li>results: 在紧急医学设置下，我们的方法可以达到0.963的准确率，预测具体的病历记录会在个人记录写作过程中被读取。我们还进行了一些临床医生的用户研究，发现我们的框架可以帮助临床医生更加快速地检索相关信息。<details>
<summary>Abstract</summary>
The large amount of time clinicians spend sifting through patient notes and documenting in electronic health records (EHRs) is a leading cause of clinician burnout. By proactively and dynamically retrieving relevant notes during the documentation process, we can reduce the effort required to find relevant patient history. In this work, we conceptualize the use of EHR audit logs for machine learning as a source of supervision of note relevance in a specific clinical context, at a particular point in time. Our evaluation focuses on the dynamic retrieval in the emergency department, a high acuity setting with unique patterns of information retrieval and note writing. We show that our methods can achieve an AUC of 0.963 for predicting which notes will be read in an individual note writing session. We additionally conduct a user study with several clinicians and find that our framework can help clinicians retrieve relevant information more efficiently. Demonstrating that our framework and methods can perform well in this demanding setting is a promising proof of concept that they will translate to other clinical settings and data modalities (e.g., labs, medications, imaging).
</details>
<details>
<summary>摘要</summary>
临床医生 spent a large amount of time搜索病人笔记和在电子医疗记录（EHR）中记录，这是临床疲劳的主要原因。通过积极和动态检索病人历史记录，我们可以减少找到相关病人历史的努力。在这项工作中，我们将EHR审核日志用于机器学习的监督，以确定笔记相关性在特定临床上下文中。我们的评估将注重在急诊室中进行动态检索，这是一个高危性的设置，具有独特的信息检索和笔记写作模式。我们的方法可以实现的AUC为0.963，预测 individu笔记写作会读取哪些笔记。此外，我们还进行了一些临床医生的用户研究，发现我们的框架可以帮助临床医生更有效地检索相关信息。这表明我们的框架和方法在这种高危性的设置下可以表现良好，这也是一个有希望的证明，它们将在其他临床设置和数据模式（例如，实验室数据、药物数据、成像数据）中表现出色。
</details></li>
</ul>
<hr>
<h2 id="Decoding-Layer-Saliency-in-Language-Transformers"><a href="#Decoding-Layer-Saliency-in-Language-Transformers" class="headerlink" title="Decoding Layer Saliency in Language Transformers"></a>Decoding Layer Saliency in Language Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05219">http://arxiv.org/abs/2308.05219</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elizabeth M. Hou, Gregory Castanon</li>
<li>for: 这个论文是为了解决现代自然语言处理中的文本焦点问题。</li>
<li>methods: 这个论文使用了 gradient-based 粒子方法，并提出了一种用于评估各层语义一致度的方法。</li>
<li>results: 这个论文在多个benchmark数据集上demonstrated consistent improvement over other textual saliency methods, without requiring additional training or labelled data.<details>
<summary>Abstract</summary>
In this paper, we introduce a strategy for identifying textual saliency in large-scale language models applied to classification tasks. In visual networks where saliency is more well-studied, saliency is naturally localized through the convolutional layers of the network; however, the same is not true in modern transformer-stack networks used to process natural language. We adapt gradient-based saliency methods for these networks, propose a method for evaluating the degree of semantic coherence of each layer, and demonstrate consistent improvement over numerous other methods for textual saliency on multiple benchmark classification datasets. Our approach requires no additional training or access to labelled data, and is comparatively very computationally efficient.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了一种策略来确定语言模型中文本焦点的存在。在视觉网络中，焦点自然地局部化在网络的卷积层中，但这并不是现代使用Transformer堆栈网络处理自然语言的情况。我们采用梯度基于的焦点方法，提出了评估层次含义一致度的方法，并在多个benchmark分类 datasets上表现出了consistent的改进。我们的方法不需要额外的训练或标注数据，并且计算效率较高。
</details></li>
</ul>
<hr>
<h2 id="Conformer-based-Target-Speaker-Automatic-Speech-Recognition-for-Single-Channel-Audio"><a href="#Conformer-based-Target-Speaker-Automatic-Speech-Recognition-for-Single-Channel-Audio" class="headerlink" title="Conformer-based Target-Speaker Automatic Speech Recognition for Single-Channel Audio"></a>Conformer-based Target-Speaker Automatic Speech Recognition for Single-Channel Audio</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05218">http://arxiv.org/abs/2308.05218</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/NVIDIA/NeMo">https://github.com/NVIDIA/NeMo</a></li>
<li>paper_authors: Yang Zhang, Krishna C. Puvvada, Vitaly Lavrukhin, Boris Ginsburg</li>
<li>for: 这篇论文旨在提出一个非自适应的终端到缩时频域架构，用于单通道目标话者自动语音识别（TS-ASR）。</li>
<li>methods: 该模型包括基于TitaNet的话者嵌入模组、Conformer基于隐藏读取和ASR模组。这些模组组合地优化以识别目标话者的语音，而忽略其他话者的语音。</li>
<li>results: 在训练时使用Connectionist Temporal Classification（CTC）损失和对频域spectrogram进行标准化的数据重建损失，以鼓励模型更好地分离目标话者的spectrogram。在WSJ0-2mix-extr（4.2%）上获得了目标话者字元误差（TS-WER）的最新纪录。此外，我们首次在WSJ0-3mix-extr（12.4%）、LibriSpeech2Mix（4.2%）和LibriSpeech3Mix（7.6%） dataset上获得了TS-WER的纪录，创造了新的benchmarks для TS-ASR。<details>
<summary>Abstract</summary>
We propose CONF-TSASR, a non-autoregressive end-to-end time-frequency domain architecture for single-channel target-speaker automatic speech recognition (TS-ASR). The model consists of a TitaNet based speaker embedding module, a Conformer based masking as well as ASR modules. These modules are jointly optimized to transcribe a target-speaker, while ignoring speech from other speakers. For training we use Connectionist Temporal Classification (CTC) loss and introduce a scale-invariant spectrogram reconstruction loss to encourage the model better separate the target-speaker's spectrogram from mixture. We obtain state-of-the-art target-speaker word error rate (TS-WER) on WSJ0-2mix-extr (4.2%). Further, we report for the first time TS-WER on WSJ0-3mix-extr (12.4%), LibriSpeech2Mix (4.2%) and LibriSpeech3Mix (7.6%) datasets, establishing new benchmarks for TS-ASR. The proposed model will be open-sourced through NVIDIA NeMo toolkit.
</details>
<details>
<summary>摘要</summary>
For training, we use Connectionist Temporal Classification (CTC) loss and introduce a scale-invariant spectrogram reconstruction loss to encourage the model to better separate the target-speaker's spectrogram from the mixture. 我们使用Connectionist Temporal Classification（CTC）损失进行训练，并引入一个缩放不变的spectrogram重建损失，以促进模型更好地分离目标说话人的spectrogram和混合。We obtain state-of-the-art target-speaker word error rate (TS-WER) on WSJ0-2mix-extr (4.2%). 我们在WSJ0-2mix-extr上获得了单频道目标说话人单词错误率（TS-WER）的状态地表现，得到4.2%。Further, we report for the first time TS-WER on WSJ0-3mix-extr (12.4%), LibriSpeech2Mix (4.2%) and LibriSpeech3Mix (7.6%) datasets, establishing new benchmarks for TS-ASR. 此外，我们在WSJ0-3mix-extr（12.4%）、LibriSpeech2Mix（4.2%）和LibriSpeech3Mix（7.6%） datasets上首次报告TS-WER，创造了新的TS-ASR标准。The proposed model will be open-sourced through NVIDIA NeMo toolkit. 我们将提出的模型通过NVIDIA NeMo工具包开源。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Pedestrian-Trajectory-Prediction-Methods-for-the-Application-in-Autonomous-Driving"><a href="#Evaluating-Pedestrian-Trajectory-Prediction-Methods-for-the-Application-in-Autonomous-Driving" class="headerlink" title="Evaluating Pedestrian Trajectory Prediction Methods for the Application in Autonomous Driving"></a>Evaluating Pedestrian Trajectory Prediction Methods for the Application in Autonomous Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05194">http://arxiv.org/abs/2308.05194</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nico Uhlemann, Felix Fent, Markus Lienkamp</li>
<li>for: 评估预测行人轨迹领域的现状，以及常速模型（CVM）在自动驾驶车辆中的适用性。</li>
<li>methods: 使用ETH&#x2F;UCY数据集进行评估，并对初始提出的模型进行修改以适应实际应用需求。进行减噪研究，检验观察到的运动历史对预测性能的影响。</li>
<li>results: 显示简单的模型在生成单轨道时仍然竞争力强，一些通常被认为是有用的特征对于不同的架构都具有少量影响。根据这些发现，提出了预测轨迹算法的建议。<details>
<summary>Abstract</summary>
In this paper, the state of the art in the field of pedestrian trajectory prediction is evaluated alongside the constant velocity model (CVM) with respect to its applicability in autonomous vehicles. The evaluation is conducted on the widely-used ETH/UCY dataset where the Average Displacement Error (ADE) and the Final Displacement Error (FDE) are reported. To align with requirements in real-world applications, modifications are made to the input features of the initially proposed models. An ablation study is conducted to examine the influence of the observed motion history on the prediction performance, thereby establishing a better understanding of its impact. Additionally, the inference time of each model is measured to evaluate the scalability of each model when confronted with varying amounts of agents. The results demonstrate that simple models remain competitive when generating single trajectories, and certain features commonly thought of as useful have little impact on the overall performance across different architectures. Based on these findings, recommendations are proposed to guide the future development of trajectory prediction algorithms.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们评估了步行者轨迹预测领域的现状，同时与常速模型（CVM）进行比较，以确定其在自动驾驶汽车中的可行性。我们在广泛使用的 ETH/UCY 数据集上进行评估，并计算了平均偏移误差（ADE）和最终偏移误差（FDE）。为适应实际应用中的需求，我们对初始提出的模型的输入特征进行修改。我们还进行了减少研究，以确定观察到的运动历史对预测性能的影响，从而更好地理解其影响。此外，我们测量了每个模型的推断时间，以评估它们在不同数量的代理人面临的可扩展性。结果表明，简单的模型在生成单个轨迹时仍然竞争力强，而一些通常被认为是有用的特征在不同的架构下具有微不足的影响。基于这些发现，我们提出了指导未来轨迹预测算法发展的建议。
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-Representations-for-Spatio-Temporal-Visual-Attention-Modeling-and-Understanding"><a href="#Hierarchical-Representations-for-Spatio-Temporal-Visual-Attention-Modeling-and-Understanding" class="headerlink" title="Hierarchical Representations for Spatio-Temporal Visual Attention Modeling and Understanding"></a>Hierarchical Representations for Spatio-Temporal Visual Attention Modeling and Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05189">http://arxiv.org/abs/2308.05189</a></li>
<li>repo_url: None</li>
<li>paper_authors: Miguel-Ángel Fernández-Torres</li>
<li>for: 本PhD论文研究了视觉注意力模型和理解视频序列中的层次表示。</li>
<li>methods: 提出了两种计算模型，一是生成概率模型，二是深度网络架构，用于模型视觉注意力。</li>
<li>results: 提出了一种基于上下文的视觉注意力模型和理解方法，并实现了视频序列中的视觉注意力模型。<details>
<summary>Abstract</summary>
This PhD. Thesis concerns the study and development of hierarchical representations for spatio-temporal visual attention modeling and understanding in video sequences. More specifically, we propose two computational models for visual attention. First, we present a generative probabilistic model for context-aware visual attention modeling and understanding. Secondly, we develop a deep network architecture for visual attention modeling, which first estimates top-down spatio-temporal visual attention, and ultimately serves for modeling attention in the temporal domain.
</details>
<details>
<summary>摘要</summary>
这个博士论文关注了视觉注意力模型化和理解在视频序列中的层次表示和时间域注意力模型。更具体来说，我们提出了两种计算模型 для视觉注意力。首先，我们提出了一种基于概率理论的生成模型，用于 Context-aware 视觉注意力模型和理解。其次，我们开发了一种深度网络架构，用于模型视觉注意力，首先估计上下文视觉注意力，并最终用于模型时间域注意力。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-for-Morphological-Identification-of-Extended-Radio-Galaxies-using-Weak-Labels"><a href="#Deep-Learning-for-Morphological-Identification-of-Extended-Radio-Galaxies-using-Weak-Labels" class="headerlink" title="Deep Learning for Morphological Identification of Extended Radio Galaxies using Weak Labels"></a>Deep Learning for Morphological Identification of Extended Radio Galaxies using Weak Labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05166">http://arxiv.org/abs/2308.05166</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nikhel1/gal-cam">https://github.com/nikhel1/gal-cam</a></li>
<li>paper_authors: Nikhel Gupta, Zeeshan Hayder, Ray P. Norris, Minh Huynh, Lars Petersson, X. Rosalind Wang, Heinz Andernach, Bärbel S. Koribalski, Miranda Yew, Evan J. Crawford</li>
<li>for: 这项研究旨在开发一种基于深度学习算法，可以减少复杂Radio галактике多组件的标注成本。</li>
<li>methods: 该算法使用弱类标签的Radio галактике来获取类活动地图（CAM），然后使用间隔Pixel网络（IRNet）来更新CAM，以获得Radio галактике的实例分割mask和红外主 galaxy的位置。</li>
<li>results: 我们使用澳大利亚 Square Kilometre Array Pathfinder（ASKAP）望远镜的数据，具体是EMU Pilot Survey，覆盖了天空面积270平方度，具有RMS敏感度25-35微赫&#x2F;天线。我们表明，使用弱类标签的深度学习算法可以高精度预测像素级信息，包括Radio галактике的扩展辐射覆盖所有 галактиComponent的面积和红外主 galaxy的位置。我们使用mAP作为评价指标，并显示模型在多个类中的mAP$_{50}$为67.5%和76.8%。模型架构可以在以下链接中找到：<a target="_blank" rel="noopener" href="https://github.com/Nikhel1/Gal-CAM">https://github.com/Nikhel1/Gal-CAM</a><details>
<summary>Abstract</summary>
The present work discusses the use of a weakly-supervised deep learning algorithm that reduces the cost of labelling pixel-level masks for complex radio galaxies with multiple components. The algorithm is trained on weak class-level labels of radio galaxies to get class activation maps (CAMs). The CAMs are further refined using an inter-pixel relations network (IRNet) to get instance segmentation masks over radio galaxies and the positions of their infrared hosts. We use data from the Australian Square Kilometre Array Pathfinder (ASKAP) telescope, specifically the Evolutionary Map of the Universe (EMU) Pilot Survey, which covered a sky area of 270 square degrees with an RMS sensitivity of 25-35 $\mu$Jy/beam. We demonstrate that weakly-supervised deep learning algorithms can achieve high accuracy in predicting pixel-level information, including masks for the extended radio emission encapsulating all galaxy components and the positions of the infrared host galaxies. We evaluate the performance of our method using mean Average Precision (mAP) across multiple classes at a standard intersection over union (IoU) threshold of 0.5. We show that the model achieves a mAP$_{50}$ of 67.5\% and 76.8\% for radio masks and infrared host positions, respectively. The network architecture can be found at the following link: https://github.com/Nikhel1/Gal-CAM
</details>
<details>
<summary>摘要</summary>
现在的研究探讨了使用弱类标注深度学习算法来降低复杂radio galaxy的多 компонент pixel-level掩码的成本。这种算法在 radio galaxy 的弱类标签上进行训练，以获得类活化图（CAMs）。然后，使用间 pixel 关系网络（IRNet）来进一步修改 CAMs，以获得 radio galaxy 的实例分割mask和激发掩码。我们使用澳大利亚 Square Kilometre Array Pathfinder（ASKAP） telescope的数据，特别是Evolutionary Map of the Universe（EMU） Pilot Survey，覆盖了天空面积270平方度，具有RMS敏感度25-35微伏/槽。我们表明，弱类标注深度学习算法可以高精度预测像素级信息，包括涵盖所有 галакси组件的广泛电磁辐射掩码以及激发掩码。我们使用mean Average Precision（mAP）作为评价指标，并在多个类上进行标准的交叠 UNION（IoU）阈值0.5的评价。我们发现模型在 radio 掩码和激发位置上的mAP$_{50}$分别为67.5%和76.8%。网络架构可以在以下链接中找到：https://github.com/Nikhel1/Gal-CAM。
</details></li>
</ul>
<hr>
<h2 id="Improved-Multi-Shot-Diffusion-Weighted-MRI-with-Zero-Shot-Self-Supervised-Learning-Reconstruction"><a href="#Improved-Multi-Shot-Diffusion-Weighted-MRI-with-Zero-Shot-Self-Supervised-Learning-Reconstruction" class="headerlink" title="Improved Multi-Shot Diffusion-Weighted MRI with Zero-Shot Self-Supervised Learning Reconstruction"></a>Improved Multi-Shot Diffusion-Weighted MRI with Zero-Shot Self-Supervised Learning Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05103">http://arxiv.org/abs/2308.05103</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jaejin-cho/miccai2023">https://github.com/jaejin-cho/miccai2023</a></li>
<li>paper_authors: Jaejin Cho, Yohan Jun, Xiaoqing Wang, Caique Kobayashi, Berkin Bilgic</li>
<li>for:  This paper aims to improve the reconstruction of multi-shot echo-planar imaging (msEPI) data for diffusion MRI, addressing limitations due to magnetic field inhomogeneity and T2&#x2F;T2* relaxation effects.</li>
<li>methods: The proposed approach, called zero-MIRID, uses deep learning-based image regularization techniques, including CNN denoisers in both k- and image-spaces, and virtual coils to enhance image reconstruction conditioning.</li>
<li>results: Compared to the state-of-the-art parallel imaging method, the proposed approach achieves superior results in reconstructing msEPI data, as demonstrated in an in-vivo experiment.Here’s the same information in Simplified Chinese:</li>
<li>for: 这篇论文目的是提高多shotecho-planar imaging（msEPI）数据重建，以解决由磁场不均和T2&#x2F;T2*相互作用引起的限制。</li>
<li>methods: 提议的方法是使用深度学习基于图像规范化技术，包括k-和图像空间的CNN滤波器，以及虚拟天线来加强图像重建条件。</li>
<li>results: 相比领域内的并行成像方法，提议的方法在重建msEPI数据方面得到了更好的结果，实验中得到了良好的成果。<details>
<summary>Abstract</summary>
Diffusion MRI is commonly performed using echo-planar imaging (EPI) due to its rapid acquisition time. However, the resolution of diffusion-weighted images is often limited by magnetic field inhomogeneity-related artifacts and blurring induced by T2- and T2*-relaxation effects. To address these limitations, multi-shot EPI (msEPI) combined with parallel imaging techniques is frequently employed. Nevertheless, reconstructing msEPI can be challenging due to phase variation between multiple shots. In this study, we introduce a novel msEPI reconstruction approach called zero-MIRID (zero-shot self-supervised learning of Multi-shot Image Reconstruction for Improved Diffusion MRI). This method jointly reconstructs msEPI data by incorporating deep learning-based image regularization techniques. The network incorporates CNN denoisers in both k- and image-spaces, while leveraging virtual coils to enhance image reconstruction conditioning. By employing a self-supervised learning technique and dividing sampled data into three groups, the proposed approach achieves superior results compared to the state-of-the-art parallel imaging method, as demonstrated in an in-vivo experiment.
</details>
<details>
<summary>摘要</summary>
Diffusion MRI通常使用声波平均图像（EPI）进行取样，但是各种磁场不均的artefacts和T2-和T2*-征relaxation效应导致分子扩散图像的分辨率受到限制。为了解决这些限制，常用多shot EPI（msEPI）和并行技术。然而，重建msEPI可以困难，因为多个shot之间的阶段差异。在本研究中，我们介绍了一种新的msEPI重建方法，即zero-MIRID（零次自我超vised学习Multi-shot图像重建优化Diffusion MRI）。这种方法将msEPI数据重建并结合深度学习 Image Regularization技术。网络包含CNN杂谱denoiser在k-和图像空间中，同时利用虚拟磁场来增强图像重建条件。通过自我超vised学习技术和将样本数据分为三组，我们的方法在对照磁共振方法的实验中显示出了更好的效果。
</details></li>
</ul>
<hr>
<h2 id="DOST-–-Domain-Obedient-Self-supervised-Training-for-Multi-Label-Classification-with-Noisy-Labels"><a href="#DOST-–-Domain-Obedient-Self-supervised-Training-for-Multi-Label-Classification-with-Noisy-Labels" class="headerlink" title="DOST – Domain Obedient Self-supervised Training for Multi Label Classification with Noisy Labels"></a>DOST – Domain Obedient Self-supervised Training for Multi Label Classification with Noisy Labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05101">http://arxiv.org/abs/2308.05101</a></li>
<li>repo_url: None</li>
<li>paper_authors: Soumadeep Saha, Utpal Garain, Arijit Ukil, Arpan Pal, Sundeep Khandelwal</li>
<li>for: 这篇论文主要关注的是多标签分类（Multi-label Classification，MLC）任务中的标签噪声问题。</li>
<li>methods: 本文提出了一个叫做“领域套用自动训练”（Domain Obedient Self-supervised Training，DOST）的概念，它不仅使得深度学习模型更加遵循领域规则，而且可以提高学习效果和减少标签噪声的影响。</li>
<li>results: 实验结果显示，DOST方法在两个大规模的多标签分类数据集上均能够获得改善，并且可以全面抵消噪声的影响。<details>
<summary>Abstract</summary>
The enormous demand for annotated data brought forth by deep learning techniques has been accompanied by the problem of annotation noise. Although this issue has been widely discussed in machine learning literature, it has been relatively unexplored in the context of "multi-label classification" (MLC) tasks which feature more complicated kinds of noise. Additionally, when the domain in question has certain logical constraints, noisy annotations often exacerbate their violations, making such a system unacceptable to an expert. This paper studies the effect of label noise on domain rule violation incidents in the MLC task, and incorporates domain rules into our learning algorithm to mitigate the effect of noise. We propose the Domain Obedient Self-supervised Training (DOST) paradigm which not only makes deep learning models more aligned to domain rules, but also improves learning performance in key metrics and minimizes the effect of annotation noise. This novel approach uses domain guidance to detect offending annotations and deter rule-violating predictions in a self-supervised manner, thus making it more "data efficient" and domain compliant. Empirical studies, performed over two large scale multi-label classification datasets, demonstrate that our method results in improvement across the board, and often entirely counteracts the effect of noise.
</details>
<details>
<summary>摘要</summary>
“深度学习技术的巨大需求已导致笔记噪声问题的出现，而这个问题在多标签分类（MLC）任务中的噪声问题尚未得到广泛研究。此外，当领域具有特定的逻辑约束时，噪声笔记常会加剧逻辑约束的违反，使得这种系统不可接受于专家。本文研究MLC任务中标签噪声对领域规则违反事件的影响，并将领域规则 incorporated 到我们的学习算法中以mitigate噪声的影响。我们提出的领域遵循自我超vised Training（DOST）方法不仅使得深度学习模型更遵循领域规则，还提高了学习性能在重要指标上，并减少了笔记噪声的影响。这种新的方法通过领域指导检测噪声笔记并防止违反规则的预测，因此更“数据效率”和遵循领域规则。empirical studies 在两个大规模多标签分类数据集上进行，结果表明，我们的方法在全面上提高了性能，常常完全抵消噪声的影响。”
</details></li>
</ul>
<hr>
<h2 id="A-degree-of-image-identification-at-sub-human-scales-could-be-possible-with-more-advanced-clusters"><a href="#A-degree-of-image-identification-at-sub-human-scales-could-be-possible-with-more-advanced-clusters" class="headerlink" title="A degree of image identification at sub-human scales could be possible with more advanced clusters"></a>A degree of image identification at sub-human scales could be possible with more advanced clusters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05092">http://arxiv.org/abs/2308.05092</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/prateekjannu/imagescale2">https://github.com/prateekjannu/imagescale2</a></li>
<li>paper_authors: Prateek Y J</li>
<li>for: 本研究目的是判断当前可用的自动学习技术是否可以使人类水平的视觉图像理解，使用同样的感知输入量和度量。</li>
<li>methods: 本研究使用了涉及数据量缩放和图像质量缩放的自动学习方法，而无需外部资金支持。</li>
<li>results: 我们发现，同时缩放数据量和图像分辨率可以 дости得人类水平的物品检测性能，而无需超过人类大小。我们使用视transformer在200000张图像和256 ppi进行训练。<details>
<summary>Abstract</summary>
The purpose of the research is to determine if currently available self-supervised learning techniques can accomplish human level comprehension of visual images using the same degree and amount of sensory input that people acquire from. Initial research on this topic solely considered data volume scaling. Here, we scale both the volume of data and the quality of the image. This scaling experiment is a self-supervised learning method that may be done without any outside financing. We find that scaling up data volume and picture resolution at the same time enables human-level item detection performance at sub-human sizes.We run a scaling experiment with vision transformers trained on up to 200000 images up to 256 ppi.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "The purpose of the research is to determine if currently available self-supervised learning techniques can accomplish human level comprehension of visual images using the same degree and amount of sensory input that people acquire from. Initial research on this topic solely considered data volume scaling. Here, we scale both the volume of data and the quality of the image. This scaling experiment is a self-supervised learning method that may be done without any outside financing. We find that scaling up data volume and picture resolution at the same time enables human-level item detection performance at sub-human sizes.We run a scaling experiment with vision transformers trained on up to 200000 images up to 256 ppi." into Simplified Chinese.<<SYS>>当前的研究目标是判断当前可用的自我超vised learning技术是否可以通过同样的感知输入来达到人类水平的视觉理解。初步研究仅考虑数据量的扩大。在这里，我们同时扩大数据量和图像质量。这种扩大实验可以无需外部资金进行。我们发现同时扩大数据量和图像分辨率时，可以在子人类大小下达到人类水平的物体检测性能。我们使用视Transformers进行训练，并将数据量增加至200000张，图像分辨率达256ppi。
</details></li>
</ul>
<hr>
<h2 id="Bayesian-Inverse-Transition-Learning-for-Offline-Settings"><a href="#Bayesian-Inverse-Transition-Learning-for-Offline-Settings" class="headerlink" title="Bayesian Inverse Transition Learning for Offline Settings"></a>Bayesian Inverse Transition Learning for Offline Settings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05075">http://arxiv.org/abs/2308.05075</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leo Benac, Sonali Parbhoo, Finale Doshi-Velez</li>
<li>for:  sequential decision-making in domains such as healthcare and education, where the rewards are known and the transition dynamics $T$ must be estimated on the basis of batch data.</li>
<li>methods:  a new constraint-based approach that captures desiderata for reliably learning a posterior distribution of the transition dynamics $T$ that is free from gradients.</li>
<li>results:  learned a high-performing policy, while considerably reducing the policy’s variance over different datasets. Also, the paper demonstrates how combining uncertainty estimation with these constraints can help infer a partial ranking of actions that produce higher returns, and helps infer safer and more informative policies for planning.Here is the text in Simplified Chinese:</li>
<li>for: Sequential Decision-Making领域，如医疗和教育等，where reward known且transition dynamics $T$必须通过批处数据 estimating。</li>
<li>methods: 新的约束基本方法，可以准确地学习 posterior distribution of transition dynamics $T$，免于gradients的影响。</li>
<li>results: 学习出高性能策略，同时减少策略对不同数据集的差异。此外，文章还解释了如何通过uncertainty estimation和约束结合，推断出更高返回的动作partial ranking，以及更安全和更有信息的策略规划。<details>
<summary>Abstract</summary>
Offline Reinforcement learning is commonly used for sequential decision-making in domains such as healthcare and education, where the rewards are known and the transition dynamics $T$ must be estimated on the basis of batch data. A key challenge for all tasks is how to learn a reliable estimate of the transition dynamics $T$ that produce near-optimal policies that are safe enough so that they never take actions that are far away from the best action with respect to their value functions and informative enough so that they communicate the uncertainties they have. Using data from an expert, we propose a new constraint-based approach that captures our desiderata for reliably learning a posterior distribution of the transition dynamics $T$ that is free from gradients. Our results demonstrate that by using our constraints, we learn a high-performing policy, while considerably reducing the policy's variance over different datasets. We also explain how combining uncertainty estimation with these constraints can help us infer a partial ranking of actions that produce higher returns, and helps us infer safer and more informative policies for planning.
</details>
<details>
<summary>摘要</summary>
偏向学习是在医疗和教育等领域常用的sequential decision-making中广泛使用的。在这些领域中，报酬是知道的，并且需要根据批处数据来估算转移动力学T。任务的挑战之一是如何学习一个可靠地估算转移动力学T，以生成近似优质策略，并保证这些策略是安全的，即从价值函数中最佳动作的距离很远。使用专家数据，我们提出了一种新的约束基本方法，可以准确地学习 posterior分布中的转移动力学T，而不需要梯度。我们的结果表明，通过使用我们的约束，我们可以学习一个高性能的策略，同时减少策略的变量在不同数据集中。我们还解释了如何将不确定性估计与这些约束结合使用，以帮助我们推断更高的返回率的行为，并生成更安全和更有信息的策略。
</details></li>
</ul>
<hr>
<h2 id="An-Interpretable-and-Attention-based-Method-for-Gaze-Estimation-Using-Electroencephalography"><a href="#An-Interpretable-and-Attention-based-Method-for-Gaze-Estimation-Using-Electroencephalography" class="headerlink" title="An Interpretable and Attention-based Method for Gaze Estimation Using Electroencephalography"></a>An Interpretable and Attention-based Method for Gaze Estimation Using Electroencephalography</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05768">http://arxiv.org/abs/2308.05768</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nina Weng, Martyna Plomecka, Manuel Kaufmann, Ard Kastrati, Roger Wattenhofer, Nicolas Langer</li>
<li>for: 这paper是为了提出一种可解释的深度学习模型，用于基于EEG数据的视线估计。</li>
<li>methods: 该paper使用了一种新的注意力基于的深度学习框架，以便在EEG信号中提取最重要的信息，并且抑制问题的通道。</li>
<li>results: 该paper的研究表明，该模型在准确性和稳定性方面比现有方法更好，并且提供了可视化的结果，以便更好地理解EEG数据分析的结果。<details>
<summary>Abstract</summary>
Eye movements can reveal valuable insights into various aspects of human mental processes, physical well-being, and actions. Recently, several datasets have been made available that simultaneously record EEG activity and eye movements. This has triggered the development of various methods to predict gaze direction based on brain activity. However, most of these methods lack interpretability, which limits their technology acceptance. In this paper, we leverage a large data set of simultaneously measured Electroencephalography (EEG) and Eye tracking, proposing an interpretable model for gaze estimation from EEG data. More specifically, we present a novel attention-based deep learning framework for EEG signal analysis, which allows the network to focus on the most relevant information in the signal and discard problematic channels. Additionally, we provide a comprehensive evaluation of the presented framework, demonstrating its superiority over current methods in terms of accuracy and robustness. Finally, the study presents visualizations that explain the results of the analysis and highlights the potential of attention mechanism for improving the efficiency and effectiveness of EEG data analysis in a variety of applications.
</details>
<details>
<summary>摘要</summary>
眼动可以揭示人类心理过程、物理健康和行为方面的重要信息。最近，一些数据集被发布了，这些数据集同时记录了EEG活动和眼动。这些数据集的出现推动了基于EEG活动预测眼动方向的方法的开发。然而，大多数这些方法缺乏可解性，这限制了技术的接受度。在这篇论文中，我们利用了大量同时测量EEG和眼动的数据集，提出了一种可解的EEG信号分析模型，以便从EEG数据中预测眼动方向。更 Specifically，我们提出了一种基于注意力的深度学习框架，使得网络能够从EEG信号中提取最重要的信息，并且抛弃问题的通道。此外，我们进行了完整的评估，证明我们的方法在准确性和稳定性方面超过现有方法。最后，我们提供了可视化结果，解释分析结果并高亮了注意力机制的潜在改进EEG数据分析的效率和效果的潜在。
</details></li>
</ul>
<hr>
<h2 id="EEG-based-Emotion-Style-Transfer-Network-for-Cross-dataset-Emotion-Recognition"><a href="#EEG-based-Emotion-Style-Transfer-Network-for-Cross-dataset-Emotion-Recognition" class="headerlink" title="EEG-based Emotion Style Transfer Network for Cross-dataset Emotion Recognition"></a>EEG-based Emotion Style Transfer Network for Cross-dataset Emotion Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05767">http://arxiv.org/abs/2308.05767</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yijin Zhou, Fu Li, Yang Li, Youshuo Ji, Lijian Zhang, Yuanfang Chen, Wenming Zheng, Guangming Shi<br>for: 这篇研究旨在解决跨 dataset EEG 情感识别 tasks 中的 style mismatch 问题，以提高 EEG 情感识别的准确性。methods: 本研究提出了 EEG-based Emotion Style Transfer Network (E2STN)，用于从 source domain 和 target domain 中获取 EEG 表现，并将它们转换为新的类别化 EEG 表现，以具有 source domain 的内容信息和 target domain 的类别特征。results: 实验结果显示，E2STN 可以在跨 dataset EEG 情感识别任务中实现 state-of-the-art 的性能。<details>
<summary>Abstract</summary>
As the key to realizing aBCIs, EEG emotion recognition has been widely studied by many researchers. Previous methods have performed well for intra-subject EEG emotion recognition. However, the style mismatch between source domain (training data) and target domain (test data) EEG samples caused by huge inter-domain differences is still a critical problem for EEG emotion recognition. To solve the problem of cross-dataset EEG emotion recognition, in this paper, we propose an EEG-based Emotion Style Transfer Network (E2STN) to obtain EEG representations that contain the content information of source domain and the style information of target domain, which is called stylized emotional EEG representations. The representations are helpful for cross-dataset discriminative prediction. Concretely, E2STN consists of three modules, i.e., transfer module, transfer evaluation module, and discriminative prediction module. The transfer module encodes the domain-specific information of source and target domains and then re-constructs the source domain's emotional pattern and the target domain's statistical characteristics into the new stylized EEG representations. In this process, the transfer evaluation module is adopted to constrain the generated representations that can more precisely fuse two kinds of complementary information from source and target domains and avoid distorting. Finally, the generated stylized EEG representations are fed into the discriminative prediction module for final classification. Extensive experiments show that the E2STN can achieve the state-of-the-art performance on cross-dataset EEG emotion recognition tasks.
</details>
<details>
<summary>摘要</summary>
As the key to realizing aBCIs, EEG emotion recognition has been widely studied by many researchers. Previous methods have performed well for intra-subject EEG emotion recognition. However, the style mismatch between source domain (training data) and target domain (test data) EEG samples caused by huge inter-domain differences is still a critical problem for EEG emotion recognition. To solve the problem of cross-dataset EEG emotion recognition, in this paper, we propose an EEG-based Emotion Style Transfer Network (E2STN) to obtain EEG representations that contain the content information of source domain and the style information of target domain, which is called stylized emotional EEG representations. The representations are helpful for cross-dataset discriminative prediction. Concretely, E2STN consists of three modules, i.e., transfer module, transfer evaluation module, and discriminative prediction module. The transfer module encodes the domain-specific information of source and target domains and then re-constructs the source domain's emotional pattern and the target domain's statistical characteristics into the new stylized EEG representations. In this process, the transfer evaluation module is adopted to constrain the generated representations that can more precisely fuse two kinds of complementary information from source and target domains and avoid distorting. Finally, the generated stylized EEG representations are fed into the discriminative prediction module for final classification. Extensive experiments show that the E2STN can achieve the state-of-the-art performance on cross-dataset EEG emotion recognition tasks.Here's the translation in Traditional Chinese as well:As the key to realizing aBCIs, EEG emotion recognition has been widely studied by many researchers. Previous methods have performed well for intra-subject EEG emotion recognition. However, the style mismatch between source domain (training data) and target domain (test data) EEG samples caused by huge inter-domain differences is still a critical problem for EEG emotion recognition. To solve the problem of cross-dataset EEG emotion recognition, in this paper, we propose an EEG-based Emotion Style Transfer Network (E2STN) to obtain EEG representations that contain the content information of source domain and the style information of target domain, which is called stylized emotional EEG representations. The representations are helpful for cross-dataset discriminative prediction. Concretely, E2STN consists of three modules, i.e., transfer module, transfer evaluation module, and discriminative prediction module. The transfer module encodes the domain-specific information of source and target domains and then re-constructs the source domain's emotional pattern and the target domain's statistical characteristics into the new stylized EEG representations. In this process, the transfer evaluation module is adopted to constrain the generated representations that can more precisely fuse two kinds of complementary information from source and target domains and avoid distorting. Finally, the generated stylized EEG representations are fed into the discriminative prediction module for final classification. Extensive experiments show that the E2STN can achieve the state-of-the-art performance on cross-dataset EEG emotion recognition tasks.
</details></li>
</ul>
<hr>
<h2 id="Prompting-In-Context-Operator-Learning-with-Sensor-Data-Equations-and-Natural-Language"><a href="#Prompting-In-Context-Operator-Learning-with-Sensor-Data-Equations-and-Natural-Language" class="headerlink" title="Prompting In-Context Operator Learning with Sensor Data, Equations, and Natural Language"></a>Prompting In-Context Operator Learning with Sensor Data, Equations, and Natural Language</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05061">http://arxiv.org/abs/2308.05061</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liu Yang, Tingwei Meng, Siting Liu, Stanley J. Osher</li>
<li>for: 学习物理学中的运算符，使用自然语言描述和方程来捕捉人类知识。</li>
<li>methods: 提出了一种多模态启发学习方法，使用”caption”来整合人类知识，并使用更高效的ICON-LM神经网络架构。</li>
<li>results: 实验结果表明，这种方法不仅能够提高学习性能和减少数据需求，还可以拓宽物理学中的应用范围。<details>
<summary>Abstract</summary>
In the growing domain of scientific machine learning, in-context operator learning has demonstrated notable potential in learning operators from prompted data during inference stage without weight updates. However, the current model's overdependence on sensor data, may inadvertently overlook the invaluable human insight into the operator. To address this, we present a transformation of in-context operator learning into a multi-modal paradigm. We propose the use of "captions" to integrate human knowledge about the operator, expressed through natural language descriptions and equations. We illustrate how this method not only broadens the flexibility and generality of physics-informed learning, but also significantly boosts learning performance and reduces data needs. Furthermore, we introduce a more efficient neural network architecture for multi-modal in-context operator learning, referred to as "ICON-LM", based on a language-model-like architecture. We demonstrate the viability of "ICON-LM" for scientific machine learning tasks, which creates a new path for the application of language models.
</details>
<details>
<summary>摘要</summary>
在科学机器学习领域的发展中，在推理阶段从提示数据中学习操作符已经表现出了明显的潜力。然而，当前模型过于依赖感知数据，可能不充分利用人类知识对操作符的珍贵性。为此，我们提出一种将受ContextOperator学习转化为多模式 парадигмы的方法。我们提议使用“caption”来结合人类对操作符的知识，通过自然语言描述和方程表述。我们示出了这种方法不仅扩大了物理学习的灵活性和通用性，而且显著提高了学习性和数据需求。此外，我们介绍了一种更高效的多模式受ContextOperator学习神经网络架构，称为“ICON-LM”，基于语言模型类架构。我们示出了ICON-LM在科学机器学习任务中的可行性，创造了一条新的语言模型应用路径。
</details></li>
</ul>
<hr>
<h2 id="A-Novel-Method-for-improving-accuracy-in-neural-network-by-reinstating-traditional-back-propagation-technique"><a href="#A-Novel-Method-for-improving-accuracy-in-neural-network-by-reinstating-traditional-back-propagation-technique" class="headerlink" title="A Novel Method for improving accuracy in neural network by reinstating traditional back propagation technique"></a>A Novel Method for improving accuracy in neural network by reinstating traditional back propagation technique</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05059">http://arxiv.org/abs/2308.05059</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gokulprasath R</li>
<li>for: 这个论文主要是为了提出一种新的快速参数更新方法，以解决深度学习中的计算负担和衰减问题。</li>
<li>methods: 该方法利用了一种新的快速参数更新策略，即不需要在每层计算Gradient的方法。</li>
<li>results: 对比州值数据集，该方法能够快速学习，避免衰减问题，并超过了现有的方法。<details>
<summary>Abstract</summary>
Deep learning has revolutionized industries like computer vision, natural language processing, and speech recognition. However, back propagation, the main method for training deep neural networks, faces challenges like computational overhead and vanishing gradients. In this paper, we propose a novel instant parameter update methodology that eliminates the need for computing gradients at each layer. Our approach accelerates learning, avoids the vanishing gradient problem, and outperforms state-of-the-art methods on benchmark data sets. This research presents a promising direction for efficient and effective deep neural network training.
</details>
<details>
<summary>摘要</summary>
深度学习已经革命化了计算机视觉、自然语言处理和语音识别等领域。然而，返回层的主要训练方法——归并，面临着计算负担和衰减 gradient 的问题。在这篇论文中，我们提出了一种新的快速参数更新方法，它消除了每层计算 gradients 的需求。我们的方法可以加速学习，避免衰减 gradient 问题，并在标准数据集上超越当前的状态艺。这篇研究表明了深度神经网络训练的有效和高效的可能性。
</details></li>
</ul>
<hr>
<h2 id="Sound-propagation-in-realistic-interactive-3D-scenes-with-parameterized-sources-using-deep-neural-operators"><a href="#Sound-propagation-in-realistic-interactive-3D-scenes-with-parameterized-sources-using-deep-neural-operators" class="headerlink" title="Sound propagation in realistic interactive 3D scenes with parameterized sources using deep neural operators"></a>Sound propagation in realistic interactive 3D scenes with parameterized sources using deep neural operators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05141">http://arxiv.org/abs/2308.05141</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dtu-act/deeponet-acoustic-wave-prop">https://github.com/dtu-act/deeponet-acoustic-wave-prop</a></li>
<li>paper_authors: Nikolas Borrel-Jensen, Somdatta Goswami, Allan P. Engsig-Karup, George Em Karniadakis, Cheol-Ho Jeong</li>
<li>for: 用于虚拟&#x2F;增强现实、游戏声音和空间计算等领域中的声场 simulate。</li>
<li>methods: 使用深度运算网络来近似线性波方程算子，以实现快速的声场传播预测。</li>
<li>results: 实现了在实际3D声学场景中逐源点预测声场的准确性，计算时间在毫秒级别，并且与参照解决方案之间的误差在0.02Pa到0.10Pa之间。<details>
<summary>Abstract</summary>
We address the challenge of sound propagation simulations in $3$D virtual rooms with moving sources, which have applications in virtual/augmented reality, game audio, and spatial computing. Solutions to the wave equation can describe wave phenomena such as diffraction and interference. However, simulating them using conventional numerical discretization methods with hundreds of source and receiver positions is intractable, making stimulating a sound field with moving sources impractical. To overcome this limitation, we propose using deep operator networks to approximate linear wave-equation operators. This enables the rapid prediction of sound propagation in realistic 3D acoustic scenes with moving sources, achieving millisecond-scale computations. By learning a compact surrogate model, we avoid the offline calculation and storage of impulse responses for all relevant source/listener pairs. Our experiments, including various complex scene geometries, show good agreement with reference solutions, with root mean squared errors ranging from 0.02 Pa to 0.10 Pa. Notably, our method signifies a paradigm shift as no prior machine learning approach has achieved precise predictions of complete wave fields within realistic domains. We anticipate that our findings will drive further exploration of deep neural operator methods, advancing research in immersive user experiences within virtual environments.
</details>
<details>
<summary>摘要</summary>
我们面临处理三维虚拟房间内的声波传播模拟中的挑战，这些应用包括虚拟现实、增强现实和空间计算。我们的方法可以描述声波现象如扩散和折射，但是使用传统的数值积分方法来模拟这些现象需要数百个源和接收器位置，这使得实现声场的类比映射成为不可能的。为了解决这个限制，我们提议使用深度算子网络来近似线性波方程式的算子。这使得在真实的三维声学场景中，快速预测声波传播的 computation 可以在毫秒级时间内完成。通过学习一个紧凑的模型，我们可以避免在所有相关的源 listener 组合上进行维护和储存对应数据。我们的实验包括多种复杂的场景几何，结果显示与参考解析相符，误差范围为0.02 Pa 至 0.10 Pa。值得注意的是，我们的方法代表了一种新的思维方式，没有过去的机器学习方法能够精确地预测完整的波场在真实的领域中。我们预期这些发现将驱动更多的深度神经算子方法研究，促进虚拟环境中的充满人体验的探索。
</details></li>
</ul>
<hr>
<h2 id="RadGraph2-Modeling-Disease-Progression-in-Radiology-Reports-via-Hierarchical-Information-Extraction"><a href="#RadGraph2-Modeling-Disease-Progression-in-Radiology-Reports-via-Hierarchical-Information-Extraction" class="headerlink" title="RadGraph2: Modeling Disease Progression in Radiology Reports via Hierarchical Information Extraction"></a>RadGraph2: Modeling Disease Progression in Radiology Reports via Hierarchical Information Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05046">http://arxiv.org/abs/2308.05046</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sameer Khanna, Adam Dejl, Kibo Yoon, Quoc Hung Truong, Hanh Duong, Agustina Saenz, Pranav Rajpurkar</li>
<li>for: 这个论文的目的是开发一个新的医学报告信息EXTRACTION dataset，以捕捉医疗器械报告中的疾病状态和设备置换的变化。</li>
<li>methods: 该论文使用了一个层次结构，将实体按照其关系进行组织，并在训练过程中使用这个结构，以提高信息EXTRACTION模型的性能。具体来说，该论文提出了一种修改后的 DyGIE++ 框架，称为 HGIE，该模型在实体和关系EXTRACTION任务中表现出色。</li>
<li>results: 根据 RadGraph2 数据集，该论文的 HGIE 模型可以更好地捕捉医疗器械报告中的各种发现，并在关系EXTRACTION任务中表现出色，比前一代模型更高。这种成果提供了开发自动跟踪疾病进程和开发基于医疗领域自然层次标签的信息EXTRACTION模型的基础。<details>
<summary>Abstract</summary>
We present RadGraph2, a novel dataset for extracting information from radiology reports that focuses on capturing changes in disease state and device placement over time. We introduce a hierarchical schema that organizes entities based on their relationships and show that using this hierarchy during training improves the performance of an information extraction model. Specifically, we propose a modification to the DyGIE++ framework, resulting in our model HGIE, which outperforms previous models in entity and relation extraction tasks. We demonstrate that RadGraph2 enables models to capture a wider variety of findings and perform better at relation extraction compared to those trained on the original RadGraph dataset. Our work provides the foundation for developing automated systems that can track disease progression over time and develop information extraction models that leverage the natural hierarchy of labels in the medical domain.
</details>
<details>
<summary>摘要</summary>
我们介绍RadGraph2，一个新的医学报告信息提取数据集，专注于时间上的疾病状态和设备安装变化。我们提出了一种层次结构，将实体按照关系组织，并证明在训练过程中使用这种层次结构可以提高信息提取模型的性能。我们对 DyGIE++ 框架进行修改，得到了我们的 HGIE 模型，该模型在实体和关系提取任务中表现更好。我们示出 RadGraph2 可以让模型捕捉更多的发现和在关系提取任务中表现更好，相比于基于原始 RadGraph 数据集训练的模型。我们的工作为Automated systems的开发提供了基础，以便在医疗领域自动跟踪疾病进程和开发利用医疗领域自然的标签层次结构的信息提取模型。
</details></li>
</ul>
<hr>
<h2 id="Collaborative-Wideband-Spectrum-Sensing-and-Scheduling-for-Networked-UAVs-in-UTM-Systems"><a href="#Collaborative-Wideband-Spectrum-Sensing-and-Scheduling-for-Networked-UAVs-in-UTM-Systems" class="headerlink" title="Collaborative Wideband Spectrum Sensing and Scheduling for Networked UAVs in UTM Systems"></a>Collaborative Wideband Spectrum Sensing and Scheduling for Networked UAVs in UTM Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05036">http://arxiv.org/abs/2308.05036</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sravan Reddy Chintareddy, Keenan Roach, Kenny Cheung, Morteza Hashemi</li>
<li>for: 本文提出了一种基于数据驱动的协同宽频谱感知和调度框架，用于网络无人机飞行器（UAV）作为次级用户，机会性地利用探测到的谱带孔隙。</li>
<li>methods: 我们提出了一种多类分类问题来探测宽频谱中的空闲谱带，基于收集的I&#x2F;Q样本。为了提高谱感知模块的准确性，每个个人UAV的输出将在UTM生态系统中的服务器进行融合。</li>
<li>results: 我们在spectrum scheduling阶段使用了强化学习（RL）解决方案来动态分配探测到的谱带孔隙给次级用户（即UAV）。<details>
<summary>Abstract</summary>
In this paper, we propose a data-driven framework for collaborative wideband spectrum sensing and scheduling for networked unmanned aerial vehicles (UAVs), which act as the secondary users to opportunistically utilize detected spectrum holes. To this end, we propose a multi-class classification problem for wideband spectrum sensing to detect vacant spectrum spots based on collected I/Q samples. To enhance the accuracy of the spectrum sensing module, the outputs from the multi-class classification by each individual UAV are fused at a server in the unmanned aircraft system traffic management (UTM) ecosystem. In the spectrum scheduling phase, we leverage reinforcement learning (RL) solutions to dynamically allocate the detected spectrum holes to the secondary users (i.e., UAVs). To evaluate the proposed methods, we establish a comprehensive simulation framework that generates a near-realistic synthetic dataset using MATLAB LTE toolbox by incorporating base-station~(BS) locations in a chosen area of interest, performing ray-tracing, and emulating the primary users channel usage in terms of I/Q samples. This evaluation methodology provides a flexible framework to generate large spectrum datasets that could be used for developing ML/AI-based spectrum management solutions for aerial devices.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一个基于数据的框架，用于无人机（UAV）网络中进行共同宽频谱感知和调度。这些无人机作为次要用户，可以利用探测到的谱频孔的机会来进行兼抓式使用。为实现这一目标，我们提出了一个多类分类问题，用于宽频谱感知，以检测谱频孔的存在。为了提高谱频感知模块的准确性，每个无人机的多类分类输出将在UTM（无人机系统交通管理）环境中的服务器进行融合。在调度阶段，我们利用强化学习（RL）解决方案，动态地将探测到的谱频孔分配给次要用户（即无人机）。为评估我们的方法，我们建立了一个完整的 simulate评估框架，通过使用MATLAB LTE工具包，在选定的区域内，基站（BS）的位置、短距离通信和主要用户通信的I/Q样本来生成一个准确的数据集。这种评估方法ология提供了一个灵活的框架，可以用于开发ML/AI基于谱管理解决方案。
</details></li>
</ul>
<hr>
<h2 id="Kairos-Practical-Intrusion-Detection-and-Investigation-using-Whole-system-Provenance"><a href="#Kairos-Practical-Intrusion-Detection-and-Investigation-using-Whole-system-Provenance" class="headerlink" title="Kairos: Practical Intrusion Detection and Investigation using Whole-system Provenance"></a>Kairos: Practical Intrusion Detection and Investigation using Whole-system Provenance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05034">http://arxiv.org/abs/2308.05034</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/provenanceanalytics/kairos">https://github.com/provenanceanalytics/kairos</a></li>
<li>paper_authors: Zijun Cheng, Qiujian Lv, Jinyuan Liang, Yan Wang, Degang Sun, Thomas Pasquier, Xueyuan Han</li>
<li>for: 本研究旨在提出一种基于证据图的入侵检测系统（PIDS），以检测现代的攻击活动。</li>
<li>methods: 该研究使用了一种新的图神经网络编码器-解码器架构，通过学习系统审计日志中 Structural 变化的时间演化来评估系统事件的异常程度。</li>
<li>results: 根据实验结果，Kairos 可以同时满足四个维度的需求，而现有方法则缺乏至少一个维度。 Kairos 可以快速、高效地监控主机系统，并且可以生成精炼的攻击摘要图，以便系统管理员快速理解和应对系统入侵。<details>
<summary>Abstract</summary>
Provenance graphs are structured audit logs that describe the history of a system's execution. Recent studies have explored a variety of techniques to analyze provenance graphs for automated host intrusion detection, focusing particularly on advanced persistent threats. Sifting through their design documents, we identify four common dimensions that drive the development of provenance-based intrusion detection systems (PIDSes): scope (can PIDSes detect modern attacks that infiltrate across application boundaries?), attack agnosticity (can PIDSes detect novel attacks without a priori knowledge of attack characteristics?), timeliness (can PIDSes efficiently monitor host systems as they run?), and attack reconstruction (can PIDSes distill attack activity from large provenance graphs so that sysadmins can easily understand and quickly respond to system intrusion?). We present KAIROS, the first PIDS that simultaneously satisfies the desiderata in all four dimensions, whereas existing approaches sacrifice at least one and struggle to achieve comparable detection performance.   Kairos leverages a novel graph neural network-based encoder-decoder architecture that learns the temporal evolution of a provenance graph's structural changes to quantify the degree of anomalousness for each system event. Then, based on this fine-grained information, Kairos reconstructs attack footprints, generating compact summary graphs that accurately describe malicious activity over a stream of system audit logs. Using state-of-the-art benchmark datasets, we demonstrate that Kairos outperforms previous approaches.
</details>
<details>
<summary>摘要</summary>
Provenance graphs are 系统执行历史记录，现代研究探讨了多种分析方法，以检测系统执行过程中的攻击。从设计文档中，我们确定了四个纬度驱动了基于证明的攻击检测系统（PIDS）的开发：范围（可以检测现代攻击？）、不偏见（可以检测新型攻击？）、实时性（可以高效监控主机系统？）和攻击重建（可以将攻击活动简化为可读的形式？）。我们介绍了 Kairos，第一个满足所有四个纬度的 PIDS，而现有方法至少牺牲一个纬度，并具有相似的检测性能。 Kairos 使用一种新的图神经网络基本 Encoder-Decoder 架构，学习系统审计记录中的时间演化结构变化，以衡量每个系统事件的异常程度。然后，基于这些细腻信息，Kairos 重建攻击印记，生成系统审计记录流中的简洁摘要图，准确描述攻击活动。使用现代标准数据集，我们证明了 Kairos 在检测方面的优越性。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/10/cs.LG_2023_08_10/" data-id="clltau92p005tcr88gog11vfg" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/5/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><a class="page-number" href="/page/5/">5</a><span class="page-number current">6</span><a class="page-number" href="/page/7/">7</a><a class="page-number" href="/page/8/">8</a><span class="space">&hellip;</span><a class="page-number" href="/page/26/">26</a><a class="extend next" rel="next" href="/page/7/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">22</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">92</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">165</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
