
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/6/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.SD_2023_08_01" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/01/cs.SD_2023_08_01/" class="article-date">
  <time datetime="2023-07-31T16:00:00.000Z" itemprop="datePublished">2023-08-01</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/01/cs.SD_2023_08_01/">cs.SD - 2023-08-01 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Choir-Transformer-Generating-Polyphonic-Music-with-Relative-Attention-on-Transformer"><a href="#Choir-Transformer-Generating-Polyphonic-Music-with-Relative-Attention-on-Transformer" class="headerlink" title="Choir Transformer: Generating Polyphonic Music with Relative Attention on Transformer"></a>Choir Transformer: Generating Polyphonic Music with Relative Attention on Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02531">http://arxiv.org/abs/2308.02531</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiuyang Zhou, Hong Zhu, Xingping Wang</li>
<li>for: 本研究旨在提出一种基于Transformer的多VOICE音乐生成模型，以更好地理解音乐结构。</li>
<li>methods: 该模型使用相对位置注意力来更好地模型长距离音韵关系，并提出了适合多VOICE音乐生成的音乐表示方式。</li>
<li>results: 实验结果表明，Choir Transformer的性能超过了前一个state-of-the-art的精度4.06%，并且测试结果与巴赫的音乐质量差不多。在实际应用中，生成的旋律和节奏可以根据输入的 specifying 进行调整，并且可以生成不同的音乐风格，如民族音乐或流行音乐等。<details>
<summary>Abstract</summary>
Polyphonic music generation is still a challenge direction due to its correct between generating melody and harmony. Most of the previous studies used RNN-based models. However, the RNN-based models are hard to establish the relationship between long-distance notes. In this paper, we propose a polyphonic music generation neural network named Choir Transformer[ https://github.com/Zjy0401/choir-transformer], with relative positional attention to better model the structure of music. We also proposed a music representation suitable for polyphonic music generation. The performance of Choir Transformer surpasses the previous state-of-the-art accuracy of 4.06%. We also measures the harmony metrics of polyphonic music. Experiments show that the harmony metrics are close to the music of Bach. In practical application, the generated melody and rhythm can be adjusted according to the specified input, with different styles of music like folk music or pop music and so on.
</details>
<details>
<summary>摘要</summary>
《多VOICE音乐生成仍然是一个挑战，主要因为 Correct between generating melody和harmony。大多数前一 Studies 使用 RNN-based 模型。然而， RNN-based 模型Difficulty in establishing the relationship between long-distance notes。在这篇论文中，我们提出了一种多VOICE音乐生成神经网络 named Choir Transformer[https://github.com/Zjy0401/choir-transformer], with relative positional attention to better model the structure of music。We also proposed a music representation suitable for polyphonic music generation。Choir Transformer 的性能超过了之前的状态态的准确率4.06%。我们也测量了多VOICE音乐的和声指标，实验结果显示和声指标与Bach的音乐很接近。在实际应用中，生成的旋律和节奏可以根据输入的特定要求进行调整，包括不同的音乐风格，如民歌或流行音乐等。
</details></li>
</ul>
<hr>
<h2 id="Multi-goal-Audio-visual-Navigation-using-Sound-Direction-Map"><a href="#Multi-goal-Audio-visual-Navigation-using-Sound-Direction-Map" class="headerlink" title="Multi-goal Audio-visual Navigation using Sound Direction Map"></a>Multi-goal Audio-visual Navigation using Sound Direction Map</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00219">http://arxiv.org/abs/2308.00219</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haru Kondoh, Asako Kanezaki</li>
<li>for: 这 paper 的目的是提出一种新的多目标听视导航任务框架，并在不同情况下进行了实验研究。</li>
<li>methods: 该 paper 使用了一种名为听音地图（SDM）的学习基于方法，以动态地找到多个声音来源，同时利用过去的记忆。</li>
<li>results: 实验结果表明，使用 SDM 方法可以显著改善多个基eline方法的性能，无论目标数量如何。<details>
<summary>Abstract</summary>
Over the past few years, there has been a great deal of research on navigation tasks in indoor environments using deep reinforcement learning agents. Most of these tasks use only visual information in the form of first-person images to navigate to a single goal. More recently, tasks that simultaneously use visual and auditory information to navigate to the sound source and even navigation tasks with multiple goals instead of one have been proposed. However, there has been no proposal for a generalized navigation task combining these two types of tasks and using both visual and auditory information in a situation where multiple sound sources are goals. In this paper, we propose a new framework for this generalized task: multi-goal audio-visual navigation. We first define the task in detail, and then we investigate the difficulty of the multi-goal audio-visual navigation task relative to the current navigation tasks by conducting experiments in various situations. The research shows that multi-goal audio-visual navigation has the difficulty of the implicit need to separate the sources of sound. Next, to mitigate the difficulties in this new task, we propose a method named sound direction map (SDM), which dynamically localizes multiple sound sources in a learning-based manner while making use of past memories. Experimental results show that the use of SDM significantly improves the performance of multiple baseline methods, regardless of the number of goals.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:在过去几年，关于indoor环境中使用深度奖励学习代理进行导航任务的研究有很大的进展。大多数这些任务只使用视觉信息，即首人图像，导航到单个目标。更近期，同时使用视觉和听音信息导航到声音源，甚至有多个目标而不是单个目标的任务被提议。然而，没有任何建议总结这两种类型的任务，并使用两种类型的信息在多个声音源的情况下。在这篇论文中，我们提出了一个新的框架：多目标听视导航。我们首先定义这个任务，然后通过在不同情况下进行实验，研究多目标听视导航任务与现有导航任务之间的关系。实验结果表明，多目标听视导航任务存在隐式需要分离声音源的困难。接着，我们提出了一种方法 named 声音方向地图（SDM），可以在学习基础上动态地理解多个声音源，并在使用过去记忆的情况下进行定位。实验结果表明，使用 SDM 可以显著提高多个基线方法的性能，无论目标数量如何。
</details></li>
</ul>
<hr>
<h2 id="DAVIS-High-Quality-Audio-Visual-Separation-with-Generative-Diffusion-Models"><a href="#DAVIS-High-Quality-Audio-Visual-Separation-with-Generative-Diffusion-Models" class="headerlink" title="DAVIS: High-Quality Audio-Visual Separation with Generative Diffusion Models"></a>DAVIS: High-Quality Audio-Visual Separation with Generative Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00122">http://arxiv.org/abs/2308.00122</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao Huang, Susan Liang, Yapeng Tian, Anurag Kumar, Chenliang Xu</li>
<li>for: 解决音视频混响分离问题</li>
<li>methods: 使用扩散模型和分离U-Net生成音频分离结果</li>
<li>results: 比对 existing 方法，DAVIS 在多种类别音频分离任务中表现出色，得到了更高质量的音频分离结果<details>
<summary>Abstract</summary>
We propose DAVIS, a Diffusion model-based Audio-VIusal Separation framework that solves the audio-visual sound source separation task through a generative manner. While existing discriminative methods that perform mask regression have made remarkable progress in this field, they face limitations in capturing the complex data distribution required for high-quality separation of sounds from diverse categories. In contrast, DAVIS leverages a generative diffusion model and a Separation U-Net to synthesize separated magnitudes starting from Gaussian noises, conditioned on both the audio mixture and the visual footage. With its generative objective, DAVIS is better suited to achieving the goal of high-quality sound separation across diverse categories. We compare DAVIS to existing state-of-the-art discriminative audio-visual separation methods on the domain-specific MUSIC dataset and the open-domain AVE dataset, and results show that DAVIS outperforms other methods in separation quality, demonstrating the advantages of our framework for tackling the audio-visual source separation task.
</details>
<details>
<summary>摘要</summary>
我们提出了DAVIS，一个基于扩散模型的音频-视觉分离框架，它通过生成方式解决音频-视觉声源分离 задачі。相比于现有的推断方法，这些方法对于高品质的声音分离来说有限制，因为它们无法捕捉多样化的声音分布。相反地，DAVIS利用一个生成扩散模型和一个分离U-Net，将生成的分离度开始自 Gaussian 骚扰，并且将其条件在音频混合和视觉预像之间。这个生成目标使得DAVIS更适合实现高品质的声音分离，并且与多样化的声音分布更有关联。我们与现有的顶尖推断音频-视觉分离方法进行比较，结果显示DAVIS在分离质量方面较其他方法更高，显示了我们的框架在处理音频-视觉声源分离任务上的优势。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/01/cs.SD_2023_08_01/" data-id="cllsiju2m004na3881g0v4k7j" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_08_01" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/01/eess.AS_2023_08_01/" class="article-date">
  <time datetime="2023-07-31T16:00:00.000Z" itemprop="datePublished">2023-08-01</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/01/eess.AS_2023_08_01/">eess.AS - 2023-08-01 22:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Generative-adversarial-networks-with-physical-sound-field-priors"><a href="#Generative-adversarial-networks-with-physical-sound-field-priors" class="headerlink" title="Generative adversarial networks with physical sound field priors"></a>Generative adversarial networks with physical sound field priors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00426">http://arxiv.org/abs/2308.00426</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xefonon/soundfieldgan">https://github.com/xefonon/soundfieldgan</a></li>
<li>paper_authors: Xenofon Karakonstantis, Efren Fernandez-Grande</li>
<li>for: 本研究使用生成对抗网络（GANs）深度学习方法对听场场景进行空间时间重建。</li>
<li>methods: 该方法使用平面波基础，学习室内压力统计分布，以准确重建听场从有限数量的测量点进行推断。</li>
<li>results: 对两个已知数据集进行评估，比较 Ergebnis 与当前最佳方法，结果显示该方法能够在高频范围内具有更高的准确率和能量保持率，特别是当推断范围超出测量点时。此外，该方法可以适应不同测量点和配置的变化无需影响性能。<details>
<summary>Abstract</summary>
This paper presents a deep learning-based approach for the spatio-temporal reconstruction of sound fields using Generative Adversarial Networks (GANs). The method utilises a plane wave basis and learns the underlying statistical distributions of pressure in rooms to accurately reconstruct sound fields from a limited number of measurements. The performance of the method is evaluated using two established datasets and compared to state-of-the-art methods. The results show that the model is able to achieve an improved reconstruction performance in terms of accuracy and energy retention, particularly in the high-frequency range and when extrapolating beyond the measurement region. Furthermore, the proposed method can handle a varying number of measurement positions and configurations without sacrificing performance. The results suggest that this approach provides a promising approach to sound field reconstruction using generative models that allow for a physically informed prior to acoustics problems.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Circumvent-spherical-Bessel-function-nulls-for-open-sphere-microphone-arrays-with-physics-informed-neural-network"><a href="#Circumvent-spherical-Bessel-function-nulls-for-open-sphere-microphone-arrays-with-physics-informed-neural-network" class="headerlink" title="Circumvent spherical Bessel function nulls for open sphere microphone arrays with physics informed neural network"></a>Circumvent spherical Bessel function nulls for open sphere microphone arrays with physics informed neural network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00242">http://arxiv.org/abs/2308.00242</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fei Ma, Thushara D. Abhayapala, Prasanga N. Samarasinghe</li>
<li>for: 该论文旨在帮助开放球形微phone阵列（OSMA）实现声场分析，使用物理知识感知神经网络（PINN）模型measurement。</li>
<li>methods: 该论文提出了一种使用PINN模型，模拟OSMA测量并预测另一个圆球的声场，通过圆球函数null点的变化，直接获得困难直接获得的声场系数。</li>
<li>results: 模拟结果表明该方法的有效性，并与固定球阵列方法进行比较。<details>
<summary>Abstract</summary>
Open sphere microphone arrays (OSMAs) are simple to design and do not introduce scattering fields, and thus can be advantageous than other arrays for implementing spatial acoustic algorithms under spherical model decomposition. However, an OSMA suffers from spherical Bessel function nulls which make it hard to obtain some sound field coefficients at certain frequencies. This paper proposes to assist an OSMA for sound field analysis with physics informed neural network (PINN). A PINN models the measurement of an OSMA and predicts the sound field on another sphere whose radius is different from that of the OSMA. Thanks to the fact that spherical Bessel function nulls vary with radius, the sound field coefficients which are hard to obtain based on the OSMA measurement directly can be obtained based on the prediction. Simulations confirm the effectiveness of this approach and compare it with the rigid sphere approach.
</details>
<details>
<summary>摘要</summary>
Open sphere microphone arrays (OSMAs) 是容易设计的，不会产生散射场，因此可以比其他阵列更有利于实现圆形声学算法。然而，OSMA 受到圆形贝塞尔函数Null的影响，使得在某些频率下获得声场参数很Difficult。这篇论文提议使用物理学 Informed neural network (PINN) 来帮助OSMA 进行声场分析。PINN 模型OSMA 的测量结果，预测另一个圆形声场的声场参数，由于圆形贝塞尔函数Null 随Radius 变化，因此可以通过预测获得直接从 OSMA 测量不可得的声场参数。 simulation 表明这种方法的有效性，并与固定球阵列方法进行比较。
</details></li>
</ul>
<hr>
<h2 id="The-role-of-vowel-and-consonant-onsets-in-neural-tracking-of-natural-speech"><a href="#The-role-of-vowel-and-consonant-onsets-in-neural-tracking-of-natural-speech" class="headerlink" title="The role of vowel and consonant onsets in neural tracking of natural speech"></a>The role of vowel and consonant onsets in neural tracking of natural speech</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00161">http://arxiv.org/abs/2308.00161</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Jalilpour Monesi, Jonas Vanthornhout, Hugo Van hamme, Tom Francart</li>
<li>for:  Investigating how the auditory system processes natural speech</li>
<li>methods:  Using EEG signals and speech representations (speech envelope and phonetic representations) to relate EEG signal to speech</li>
<li>results:  Vowel-consonant onsets perform better than onsets of any phone in both forward modeling and match-mismatch tasks, suggesting that neural tracking of vowel vs. consonant exists in EEG to some degree. Vowel onsets are better related to EEG than syllable onsets. Findings suggest that neural tracking previously thought to be associated with broad phonetic classes might actually originate from vowel-consonant onsets rather than differentiation between phonetic classes.Here’s the Chinese text in traditional characters:</li>
<li>for: 研究人工语音处理系统如何处理自然语音</li>
<li>methods: 使用EEG信号和语音表示（语音膜和语音表示）来相关EEG信号和语音</li>
<li>results: 元音声门的开始比任何语音开始更好地在前向模型和匹配-不匹配任务中表现，这表明神经系统中存在一定的元音声门追踪。元音开始比 syllable开始更好地与EEG相关。发现前面认为与广泛语音类别相关的神经追踪可能实际上来自元音声门开始而不是不同语音类别的区别。<details>
<summary>Abstract</summary>
To investigate how the auditory system processes natural speech, models have been created to relate the electroencephalography (EEG) signal of a person listening to speech to various representations of the speech. Mainly the speech envelope has been used, but also phonetic representations. We investigated to which degree of granularity phonetic representations can be related to the EEG signal. We used recorded EEG signals from 105 subjects while they listened to fairy tale stories. We utilized speech representations, including onset of any phone, vowel-consonant onsets, broad phonetic class (BPC) onsets, and narrow phonetic class (NPC) onsets, and related them to EEG using forward modeling and match-mismatch tasks. In forward modeling, we used a linear model to predict EEG from speech representations. In the match-mismatch task, we trained a long short term memory (LSTM) based model to determine which of two candidate speech segments matches with a given EEG segment. Our results show that vowel-consonant onsets outperform onsets of any phone in both tasks, which suggests that neural tracking of the vowel vs. consonant exists in the EEG to some degree. We also observed that vowel (syllable nucleus) onsets are better related to EEG compared to syllable onsets. Finally, our findings suggest that neural tracking previously thought to be associated with broad phonetic classes might actually originate from vowel-consonant onsets rather than the differentiation between different phonetic classes.
</details>
<details>
<summary>摘要</summary>
为了研究人类听说语言系统如何处理自然语言，我们创建了模型将听说语言的电生物学信号（EEG）与不同的语音表示相关。主要使用语音封顶（onset），同时也使用phonetic表示。我们研究了哪些程度的细化程度可以与EEG信号相关。我们使用105名参与者的录制的EEG信号，并使用听故事的语音表示，包括任何phone的开始，元音-音节开始，广泛语音类（BPC）开始和窄语音类（NPC）开始。我们使用前向模型预测EEG信号，并使用匹配-匹配任务训练一个基于LSTM的模型，以确定两个听说语音段中匹配的EEG段。我们的结果表明，元音-音节开始在两个任务中都高于任何phone开始，这表明在EEG信号中存在一定程度的元音对抗。我们还发现，元音（子音核）开始与EEG相关性更高于音节开始。最后，我们的发现表明，以前被认为与广泛语音类相关的神经跟踪，实际上可能来自元音-音节开始而不是不同的语音类。
</details></li>
</ul>
<hr>
<h2 id="An-enhanced-system-for-the-detection-and-active-cancellation-of-snoring-signals"><a href="#An-enhanced-system-for-the-detection-and-active-cancellation-of-snoring-signals" class="headerlink" title="An enhanced system for the detection and active cancellation of snoring signals"></a>An enhanced system for the detection and active cancellation of snoring signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16809">http://arxiv.org/abs/2307.16809</a></li>
<li>repo_url: None</li>
<li>paper_authors: Valeria Bruschi, Michela Cantarini, Luca Serafini, Stefano Nobili, Stefania Cecchi, Stefano Squartini</li>
<li>for: 治疗呼吸吵闹的问题，提高人们的社交和婚姻生活质量。</li>
<li>methods: 使用卷积回归神经网络进行吵闹活动检测，并使用延时零扩频分解法进行实时吵闹消除。</li>
<li>results: 通过对真实吵闹信号进行多次实验，研究发现，当吵闹活动检测阶段打开时，active吵闹消除系统的表现更佳，这表明了预先检测吵闹活动的效果。<details>
<summary>Abstract</summary>
Snoring is a common disorder that affects people's social and marital lives. The annoyance caused by snoring can be partially solved with active noise control systems. In this context, the present work aims at introducing an enhanced system based on the use of a convolutional recurrent neural network for snoring activity detection and a delayless subband approach for active snoring cancellation. Thanks to several experiments conducted using real snoring signals, this work shows that the active snoring cancellation system achieves better performance when the snoring activity detection stage is turned on, demonstrating the beneficial effect of a preliminary snoring detection stage in the perspective of snoring cancellation.
</details>
<details>
<summary>摘要</summary>
呼吸抑挫是一种常见的呼吸疾病，影响人们的社会和 conjugal 生活。呼吸抑挫的困扰可以通过活动噪声控制系统得到部分解决。在这个上下文中，现在的工作是介绍一种基于卷积回归神经网络的呼吸活动检测和延迟无效子带方法的增强型呼吸抑挫纠正系统。经过多个实验使用真实的呼吸信号，这个工作表明了在呼吸活动检测阶段开启时，活动呼吸纠正系统的性能更好，这demonstrates the beneficial effect of a preliminary snoring detection stage in the perspective of snoring cancellation.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/01/eess.AS_2023_08_01/" data-id="cllsiju39006ta3888jv1hhte" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_08_01" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/01/eess.IV_2023_08_01/" class="article-date">
  <time datetime="2023-07-31T16:00:00.000Z" itemprop="datePublished">2023-08-01</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/01/eess.IV_2023_08_01/">eess.IV - 2023-08-01 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Lab-in-a-Tube-A-portable-imaging-spectrophotometer-for-cost-effective-high-throughput-and-label-free-analysis-of-centrifugation-processes"><a href="#Lab-in-a-Tube-A-portable-imaging-spectrophotometer-for-cost-effective-high-throughput-and-label-free-analysis-of-centrifugation-processes" class="headerlink" title="Lab-in-a-Tube: A portable imaging spectrophotometer for cost-effective, high-throughput, and label-free analysis of centrifugation processes"></a>Lab-in-a-Tube: A portable imaging spectrophotometer for cost-effective, high-throughput, and label-free analysis of centrifugation processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03777">http://arxiv.org/abs/2308.03777</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanyuan Wei, Dehua Hu, Bijie Bai, Chenqi Meng, Tsz Kin Chan, Xing Zhao, Yuye Wang, Yi-Ping Ho, Wu Yuan, Ho-Pui Ho</li>
<li>for: 这种研究旨在实时观察中心rifugation过程中的动态过程。</li>
<li>methods: 该研究使用了一种创新的Lab_in_a_Tube成像光谱仪，其包括实时图像分析和可编程中断功能。这种可以在30美元以下的Portable LIAT设备包含了Wi Fi摄像头和活动封闭控制。</li>
<li>results: 该研究发现了在中心rifugation过程中单个液体径流形成环形图像，这是首次观察的现象。研究还发展了基于旋转参照系的理论 simulations，与实验结果高度相关。此外，研究还实现了首次观察血液凝固过程在临床实验室中心rifuges中。这种Cost效果开放了中心rifugation微生物学研究的新途径，并为大规模和不间断监测中心rifugal过程的计算成像仪器网络创造了可能。<details>
<summary>Abstract</summary>
Centrifuges serve as essential instruments in modern experimental sciences, facilitating a wide range of routine sample processing tasks that necessitate material sedimentation. However, the study for real time observation of the dynamical process during centrifugation has remained elusive. In this study, we developed an innovative Lab_in_a_Tube imaging spectrophotometer that incorporates capabilities of real time image analysis and programmable interruption. This portable LIAT device costs less than 30 US dollars. Based on our knowledge, it is the first Wi Fi camera built_in in common lab centrifuges with active closed_loop control. We tested our LIAT imaging spectrophotometer with solute solvent interaction investigation obtained from lab centrifuges with quantitative data plotting in a real time manner. Single re circulating flow was real time observed, forming the ring shaped pattern during centrifugation. To the best of our knowledge, this is the very first observation of similar phenomena. We developed theoretical simulations for the single particle in a rotating reference frame, which correlated well with experimental results. We also demonstrated the first demonstration to visualize the blood sedimentation process in clinical lab centrifuges. This remarkable cost effectiveness opens up exciting opportunities for centrifugation microbiology research and paves the way for the creation of a network of computational imaging spectrometers at an affordable price for large scale and continuous monitoring of centrifugal processes in general.
</details>
<details>
<summary>摘要</summary>
中央机器 serves as a crucial tool in modern experimental sciences, facilitating a wide range of routine sample processing tasks that require material sedimentation. However, the study of real-time observation of the dynamical process during centrifugation has been challenging. In this study, we developed an innovative Lab_in_a_Tube imaging spectrophotometer that incorporates real-time image analysis and programmable interruption capabilities. This portable LIAT device costs less than 30 US dollars. Based on our knowledge, it is the first Wi-Fi camera built-in in common lab centrifuges with active closed-loop control. We tested our LIAT imaging spectrophotometer with solute-solvent interaction investigations obtained from lab centrifuges, with quantitative data plotting in real-time. Single recirculating flow was real-time observed, forming a ring-shaped pattern during centrifugation. To the best of our knowledge, this is the first observation of similar phenomena. We developed theoretical simulations for the single particle in a rotating reference frame, which correlated well with experimental results. We also demonstrated the first visualization of blood sedimentation process in clinical lab centrifuges. This remarkable cost-effectiveness opens up exciting opportunities for centrifugation microbiology research and paves the way for the creation of a network of computational imaging spectrometers at an affordable price for large-scale and continuous monitoring of centrifugal processes in general.
</details></li>
</ul>
<hr>
<h2 id="PressureTransferNet-Human-Attribute-Guided-Dynamic-Ground-Pressure-Profile-Transfer-using-3D-simulated-Pressure-Maps"><a href="#PressureTransferNet-Human-Attribute-Guided-Dynamic-Ground-Pressure-Profile-Transfer-using-3D-simulated-Pressure-Maps" class="headerlink" title="PressureTransferNet: Human Attribute Guided Dynamic Ground Pressure Profile Transfer using 3D simulated Pressure Maps"></a>PressureTransferNet: Human Attribute Guided Dynamic Ground Pressure Profile Transfer using 3D simulated Pressure Maps</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00538">http://arxiv.org/abs/2308.00538</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lala Shakti Swarup Ray, Vitor Fortes Rey, Bo Zhou, Sungho Suh, Paul Lukowicz</li>
<li>for: 这篇论文旨在开发一种基于地面压力信息的人体活动识别（HAR）方法，以便在不同场景下准确地识别人体活动。</li>
<li>methods: 该方法使用了一种encoder-decoder模型，将源地面压力图与目标人体特征向量作为输入，生成一个新的地面压力图，捕捉了目标特征。</li>
<li>results: 经过训练后，该模型在真实场景数据上表现出了准确地传递人体特征到地面压力图的能力，并且通过物理学习模型的视觉验证和物理压力盘数据的分类验证，证明了模型的正确性。<details>
<summary>Abstract</summary>
We propose PressureTransferNet, a novel method for Human Activity Recognition (HAR) using ground pressure information. Our approach generates body-specific dynamic ground pressure profiles for specific activities by leveraging existing pressure data from different individuals. PressureTransferNet is an encoder-decoder model taking a source pressure map and a target human attribute vector as inputs, producing a new pressure map reflecting the target attribute. To train the model, we use a sensor simulation to create a diverse dataset with various human attributes and pressure profiles. Evaluation on a real-world dataset shows its effectiveness in accurately transferring human attributes to ground pressure profiles across different scenarios. We visually confirm the fidelity of the synthesized pressure shapes using a physics-based deep learning model and achieve a binary R-square value of 0.79 on areas with ground contact. Validation through classification with F1 score (0.911$\pm$0.015) on physical pressure mat data demonstrates the correctness of the synthesized pressure maps, making our method valuable for data augmentation, denoising, sensor simulation, and anomaly detection. Applications span sports science, rehabilitation, and bio-mechanics, contributing to the development of HAR systems.
</details>
<details>
<summary>摘要</summary>
我们提出PressureTransferNet，一种新的人动作识别（HAR）方法，使用地面压力信息。我们的方法生成特定活动的体具动态地面压力 profilestring 从不同个体的压力数据中得到利用。PressureTransferNet是一个编码器-解码器模型，接受来源压力地图和目标人类特征向量作为输入，生成一个新的压力地图，反映目标特征。为了训练模型，我们使用了模拟器创建了多种人类特征和压力 profilestring 的多样化数据集。我们的评估表明，在不同enario中，PressureTransferNet可以准确地将人类特征传递到地面压力 profilestring。我们通过使用物理学习模型进行视觉验证，并在物理压力盘数据上获得了0.79的二元R-平方值，确认了我们生成的压力地图的准确性。我们的方法可以用于数据扩展、压力缓减、模拟器和异常检测等应用，广泛应用于体育科学、复习和生物机械等领域，为HAR系统的发展提供了价值。
</details></li>
</ul>
<hr>
<h2 id="Visual-attention-information-can-be-traced-on-cortical-response-but-not-on-the-retina-evidence-from-electrophysiological-mouse-data-using-natural-images-as-stimuli"><a href="#Visual-attention-information-can-be-traced-on-cortical-response-but-not-on-the-retina-evidence-from-electrophysiological-mouse-data-using-natural-images-as-stimuli" class="headerlink" title="Visual attention information can be traced on cortical response but not on the retina: evidence from electrophysiological mouse data using natural images as stimuli"></a>Visual attention information can be traced on cortical response but not on the retina: evidence from electrophysiological mouse data using natural images as stimuli</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00526">http://arxiv.org/abs/2308.00526</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nikos Melanitis, Konstantina Nikita</li>
<li>for:  investigate the biological basis of visual attention</li>
<li>methods: analyze retinal and cortical electrophysiological data from mouse, use Natural Images as visual stimuli</li>
<li>results: a subset of V1 neurons respond differently to salient vs. non-salient visual regions, visual attention information not traced in retinal response, cortical response modulated to interpret visual attention informationHere are the three points in English for comparison:</li>
<li>for: investigate the biological basis of visual attention</li>
<li>methods: analyze retinal and cortical electrophysiological data from mouse, use Natural Images as visual stimuli</li>
<li>results: a subset of V1 neurons respond differently to salient vs. non-salient visual regions, visual attention information not traced in retinal response, cortical response modulated to interpret visual attention information<details>
<summary>Abstract</summary>
Visual attention forms the basis of understanding the visual world. In this work we follow a computational approach to investigate the biological basis of visual attention. We analyze retinal and cortical electrophysiological data from mouse. Visual Stimuli are Natural Images depicting real world scenes. Our results show that in primary visual cortex (V1), a subset of around $10\%$ of the neurons responds differently to salient versus non-salient visual regions. Visual attention information was not traced in retinal response. It appears that the retina remains naive concerning visual attention; cortical response gets modulated to interpret visual attention information. Experimental animal studies may be designed to further explore the biological basis of visual attention we traced in this study. In applied and translational science, our study contributes to the design of improved visual prostheses systems -- systems that create artificial visual percepts to visually impaired individuals by electronic implants placed on either the retina or the cortex.
</details>
<details>
<summary>摘要</summary>
视觉注意力是视觉理解的基础。在这项工作中，我们采用计算方法调查生物基础的视觉注意力。我们分析鼠脑和脊梗电physiological数据。视觉刺激是自然图像，显示实际场景。我们的结果表明，在初级视觉层（V1）中，约10%的神经元在关键 versus 非关键视觉区域响应不同。视觉注意力信息不在视网膜响应中追踪。似乎视网膜对视觉注意力保持无知； cortical response 被修饰以解释视觉注意力信息。可以通过动物实验进一步探索我们在这项研究中跟踪的生物基础。在应用和翻译科学中，我们的研究对设计改进的视觉 prostheses 系统做出了贡献，这些系统通过电子设备置于视网膜或大脑中，为视力障碍者创造人工视觉感受。
</details></li>
</ul>
<hr>
<h2 id="Robust-Spatiotemporal-Fusion-of-Satellite-Images-A-Constrained-Convex-Optimization-Approach"><a href="#Robust-Spatiotemporal-Fusion-of-Satellite-Images-A-Constrained-Convex-Optimization-Approach" class="headerlink" title="Robust Spatiotemporal Fusion of Satellite Images: A Constrained Convex Optimization Approach"></a>Robust Spatiotemporal Fusion of Satellite Images: A Constrained Convex Optimization Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00500">http://arxiv.org/abs/2308.00500</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryosuke Isono, Kazuki Naganuma, Shunsuke Ono</li>
<li>for: 这种论文是为了提出一种新的卫星图像空间时间融合框架（ROSTF），以解决卫星图像的空间时间分解问题。</li>
<li>methods: 这种方法使用了一种新的优化算法，可以减少噪声的影响，并提高图像的分辨率。</li>
<li>results: 实验结果显示，ROSTF可以在噪声存在的情况下表现比较好，并且在一些实际数据上的表现也是比较好的。Here’s the breakdown of each point in more detail:</li>
<li>for: The paper is proposing a new framework for satellite image spatiotemporal fusion, called Robust Optimization-based Spatiotemporal Fusion (ROSTF). This is to address the trade-off between spatial and temporal resolution in satellite images.</li>
<li>methods: The proposed method uses a new optimization algorithm that can reduce the impact of noise and improve the resolution of the images.</li>
<li>results: The experimental results show that ROSTF performs well even in noisy conditions, and outperforms some state-of-the-art methods in noisy cases.<details>
<summary>Abstract</summary>
This paper proposes a novel spatiotemporal (ST) fusion framework for satellite images, named Robust Optimization-based Spatiotemporal Fusion (ROSTF). ST fusion is a promising approach to resolve a trade-off between the temporal and spatial resolution of satellite images. Although many ST fusion methods have been proposed, most of them are not designed to explicitly account for noise in observed images, despite the inevitable influence of noise caused by the measurement equipment and environment. Our ROSTF addresses this challenge by treating the noise removal of the observed images and the estimation of the target high-resolution image as a single optimization problem. Specifically, first, we define observation models for satellite images possibly contaminated with random noise, outliers, and/or missing values, and then introduce certain assumptions that would naturally hold between the observed images and the target high-resolution image. Then, based on these models and assumptions, we formulate the fusion problem as a constrained optimization problem and develop an efficient algorithm based on a preconditioned primal-dual splitting method for solving the problem. The performance of ROSTF was verified using simulated and real data. The results show that ROSTF performs comparably to several state-of-the-art ST fusion methods in noiseless cases and outperforms them in noisy cases.
</details>
<details>
<summary>摘要</summary>
Most existing methods for spatiotemporal fusion do not take into account the noise in the observed images, but ROSTF addresses this by treating noise removal and image estimation as a single optimization problem. This is done by defining models for observed images that may be contaminated with random noise, outliers, and/or missing values, and then using these models to formulate the fusion problem as a constrained optimization problem. An efficient algorithm based on a preconditioned primal-dual splitting method is then used to solve this problem.The performance of ROSTF was tested using both simulated and real data, and the results showed that it performed well compared to other state-of-the-art methods in noiseless cases, and outperformed them in noisy cases.
</details></li>
</ul>
<hr>
<h2 id="An-L2-Normalized-Spatial-Attention-Network-For-Accurate-And-Fast-Classification-Of-Brain-Tumors-In-2D-T1-Weighted-CE-MRI-Images"><a href="#An-L2-Normalized-Spatial-Attention-Network-For-Accurate-And-Fast-Classification-Of-Brain-Tumors-In-2D-T1-Weighted-CE-MRI-Images" class="headerlink" title="An L2-Normalized Spatial Attention Network For Accurate And Fast Classification Of Brain Tumors In 2D T1-Weighted CE-MRI Images"></a>An L2-Normalized Spatial Attention Network For Accurate And Fast Classification Of Brain Tumors In 2D T1-Weighted CE-MRI Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00491">http://arxiv.org/abs/2308.00491</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/juliadietlmeier/mri_image_classification">https://github.com/juliadietlmeier/mri_image_classification</a></li>
<li>paper_authors: Grace Billingsley, Julia Dietlmeier, Vivek Narayanaswamy, Andreas Spanias, Noel E. OConnor</li>
<li>for: 这篇论文targets at developing an accurate and fast classification network for brain tumor classification in MRI images.</li>
<li>methods: 该论文提出了一种使用l2-normalized spatial attention mechanism的基eline网络，并对比了该网络与当前最佳方法的性能。</li>
<li>results: 该论文在一个2D T1-weighted CE-MRI数据集上测试了该模型，并获得了与当前最佳方法相比的1.79%的性能提升。<details>
<summary>Abstract</summary>
We propose an accurate and fast classification network for classification of brain tumors in MRI images that outperforms all lightweight methods investigated in terms of accuracy. We test our model on a challenging 2D T1-weighted CE-MRI dataset containing three types of brain tumors: Meningioma, Glioma and Pituitary. We introduce an l2-normalized spatial attention mechanism that acts as a regularizer against overfitting during training. We compare our results against the state-of-the-art on this dataset and show that by integrating l2-normalized spatial attention into a baseline network we achieve a performance gain of 1.79 percentage points. Even better accuracy can be attained by combining our model in an ensemble with the pretrained VGG16 at the expense of execution speed. Our code is publicly available at https://github.com/juliadietlmeier/MRI_image_classification
</details>
<details>
<summary>摘要</summary>
我们提出了一种准确和快速的分类网络，用于分类MRI图像中的脑肿瘤，超过所有轻量级方法的准确性。我们在一个复杂的2D T1束缚CE-MRI数据集上测试了我们的模型，该数据集包含三种脑肿瘤：膝盖肿瘤、 Glioma 和 hypophyseal。我们引入了L2正规化的空间注意力机制，以防止过拟合 durante 训练。我们对这个数据集进行比较，并显示了在我们的模型中集成L2正规化空间注意力机制后，与状态元的性能提高1.79个百分点。甚至可以通过将我们的模型与预训练的VGG16 ensemble来提高准确性，但是会降低执行速度。我们的代码可以在https://github.com/juliadietlmeier/MRI_image_classification 上公开获取。
</details></li>
</ul>
<hr>
<h2 id="A-Deep-Learning-Approach-for-Virtual-Contrast-Enhancement-in-Contrast-Enhanced-Spectral-Mammography"><a href="#A-Deep-Learning-Approach-for-Virtual-Contrast-Enhancement-in-Contrast-Enhanced-Spectral-Mammography" class="headerlink" title="A Deep Learning Approach for Virtual Contrast Enhancement in Contrast Enhanced Spectral Mammography"></a>A Deep Learning Approach for Virtual Contrast Enhancement in Contrast Enhanced Spectral Mammography</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00471">http://arxiv.org/abs/2308.00471</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aurora Rofena, Valerio Guarrasi, Marina Sarli, Claudia Lucia Piccolo, Matteo Sammarra, Bruno Beomonte Zobel, Paolo Soda</li>
<li>for: 这个研究旨在使用深度生成模型进行CESM中的虚拟增强，以提高诊断精度并降低辐射剂量。</li>
<li>methods: 这个研究使用了autoencoder和两个生成对抗网络（Pix2Pix和CycleGAN）来生成虚拟重组图像， solely from low-energy images。</li>
<li>results: 研究结果表明，CycleGAN是最有批处的深度网络来生成虚拟重组图像， highlighting the potential of artificial intelligence techniques for virtual contrast enhancement in this field。<details>
<summary>Abstract</summary>
Contrast Enhanced Spectral Mammography (CESM) is a dual-energy mammographic imaging technique that first needs intravenously administration of an iodinated contrast medium; then, it collects both a low-energy image, comparable to standard mammography, and a high-energy image. The two scans are then combined to get a recombined image showing contrast enhancement. Despite CESM diagnostic advantages for breast cancer diagnosis, the use of contrast medium can cause side effects, and CESM also beams patients with a higher radiation dose compared to standard mammography. To address these limitations this work proposes to use deep generative models for virtual contrast enhancement on CESM, aiming to make the CESM contrast-free as well as to reduce the radiation dose. Our deep networks, consisting of an autoencoder and two Generative Adversarial Networks, the Pix2Pix, and the CycleGAN, generate synthetic recombined images solely from low-energy images. We perform an extensive quantitative and qualitative analysis of the model's performance, also exploiting radiologists' assessments, on a novel CESM dataset that includes 1138 images that, as a further contribution of this work, we make publicly available. The results show that CycleGAN is the most promising deep network to generate synthetic recombined images, highlighting the potential of artificial intelligence techniques for virtual contrast enhancement in this field.
</details>
<details>
<summary>摘要</summary>
增强成像乳腺护肤（CESM）是一种双能量乳腺成像技术，需要先行静脉输液iodinated contrast媒体，然后收集低能量图像和高能量图像。这两个扫描后将图像合并，获得增强图像。 DESPITE CESM的诊断优势，使用contrast媒体可能会导致副作用，而且CESM也会对病人辐射更高的辐射剂量比标准乳腺成像。为了解决这些限制，本工作提出使用深度生成模型对CESM进行虚拟增强，以实现无contrast和辐射剂量的CESM。我们的深度网络由自适应网络和两个生成对抗网络组成，包括Pix2Pix和CycleGAN。这些网络可以将低能量图像转换成增强图像。我们对新的CESM数据集进行了广泛的量化和质量分析，并利用了Radiologists的评估，包括1138个图像，这也是本工作的一个新贡献。结果表明CycleGAN是最有前途的深度网络，生成增强图像， highlighting the potential of artificial intelligence techniques for virtual contrast enhancement in this field。
</details></li>
</ul>
<hr>
<h2 id="Space-Debris-Are-Deep-Learning-based-Image-Enhancements-part-of-the-Solution"><a href="#Space-Debris-Are-Deep-Learning-based-Image-Enhancements-part-of-the-Solution" class="headerlink" title="Space Debris: Are Deep Learning-based Image Enhancements part of the Solution?"></a>Space Debris: Are Deep Learning-based Image Enhancements part of the Solution?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00408">http://arxiv.org/abs/2308.00408</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michele Jamrozik, Vincent Gaudillière, Mohamed Adel Musallam, Djamila Aouada<br>for: 这个研究的目的是研究深度神经网络（DNN）解决在视觉光谱下摄取图像时遗留的问题，以实现资产保护。methods: 这个研究使用了一种混合的UNet-ResNet34深度学习（DL）架构，这个架构在ImageNet dataset上进行预训，并且处理了对照光谱下的图像恶化，包括模糊、曝光问题、低比例和噪音。results: 根据视觉检查，这个UNet模型能够正确地更正在太空中摄取的图像恶化，并且与现有的深度学习图像改善方法进行比较。<details>
<summary>Abstract</summary>
The volume of space debris currently orbiting the Earth is reaching an unsustainable level at an accelerated pace. The detection, tracking, identification, and differentiation between orbit-defined, registered spacecraft, and rogue/inactive space ``objects'', is critical to asset protection. The primary objective of this work is to investigate the validity of Deep Neural Network (DNN) solutions to overcome the limitations and image artefacts most prevalent when captured with monocular cameras in the visible light spectrum. In this work, a hybrid UNet-ResNet34 Deep Learning (DL) architecture pre-trained on the ImageNet dataset, is developed. Image degradations addressed include blurring, exposure issues, poor contrast, and noise. The shortage of space-generated data suitable for supervised DL is also addressed. A visual comparison between the URes34P model developed in this work and the existing state of the art in deep learning image enhancement methods, relevant to images captured in space, is presented. Based upon visual inspection, it is determined that our UNet model is capable of correcting for space-related image degradations and merits further investigation to reduce its computational complexity.
</details>
<details>
<summary>摘要</summary>
地球轨道上的空间废墟量目前已达到不可持续的水平，速度加剧。检测、跟踪、识别和区分在轨道上定义的注册空间craft和偏离/不活的空间“物体”是核心的。本工作的主要目标是调查深度神经网络（DNN）解决方案是否能够超越单光学pectrum中的限制和图像artefacts。在这种工作中，我们开发了一种混合的UNet-ResNet34深度学习（DL）架构，预训练在ImageNet dataset上。处理的图像问题包括模糊、曝光问题、低对比度和噪声。由于空间数据的短缺，我们也解决了深度学习的supervised数据不充分问题。在这种情况下，我们对URes34P模型进行了视觉比较，与现有的深度学习图像更新方法进行了比较。根据视觉检查，我们的UNet模型能够更正空间相关的图像异常，并且值得进一步的研究以降低计算复杂度。
</details></li>
</ul>
<hr>
<h2 id="Metrics-to-Quantify-Global-Consistency-in-Synthetic-Medical-Images"><a href="#Metrics-to-Quantify-Global-Consistency-in-Synthetic-Medical-Images" class="headerlink" title="Metrics to Quantify Global Consistency in Synthetic Medical Images"></a>Metrics to Quantify Global Consistency in Synthetic Medical Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00402">http://arxiv.org/abs/2308.00402</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Scholz, Benedikt Wiestler, Daniel Rueckert, Martin J. Menten</li>
<li>for: The paper is written for medical image processing, specifically for data augmentation and inter-modality image translation.</li>
<li>methods: The paper introduces two metrics for measuring the global consistency of synthetic images on a per-image basis. These metrics use supervised and self-supervised trained neural networks to predict and compare explicit and implicit attributes of images on patches.</li>
<li>results: The paper demonstrates that predicting explicit attributes of synthetic images on patches can distinguish globally consistent from inconsistent images. Implicit representations of images are less sensitive to assess global consistency but are still serviceable when labeled data is unavailable. The method is shown to be more effective in measuring global consistency compared to established metrics such as the FID.<details>
<summary>Abstract</summary>
Image synthesis is increasingly being adopted in medical image processing, for example for data augmentation or inter-modality image translation. In these critical applications, the generated images must fulfill a high standard of biological correctness. A particular requirement for these images is global consistency, i.e an image being overall coherent and structured so that all parts of the image fit together in a realistic and meaningful way. Yet, established image quality metrics do not explicitly quantify this property of synthetic images. In this work, we introduce two metrics that can measure the global consistency of synthetic images on a per-image basis. To measure the global consistency, we presume that a realistic image exhibits consistent properties, e.g., a person's body fat in a whole-body MRI, throughout the depicted object or scene. Hence, we quantify global consistency by predicting and comparing explicit attributes of images on patches using supervised trained neural networks. Next, we adapt this strategy to an unlabeled setting by measuring the similarity of implicit image features predicted by a self-supervised trained network. Our results demonstrate that predicting explicit attributes of synthetic images on patches can distinguish globally consistent from inconsistent images. Implicit representations of images are less sensitive to assess global consistency but are still serviceable when labeled data is unavailable. Compared to established metrics, such as the FID, our method can explicitly measure global consistency on a per-image basis, enabling a dedicated analysis of the biological plausibility of single synthetic images.
</details>
<details>
<summary>摘要</summary>
医学图像处理中的图像生成技术在不断推广应用，例如数据增强或多Modalities图像翻译。在这些关键应用中，生成的图像必须满足高水平的生物可靠性。特别是，生成图像的全局一致性是一项关键要求，即图像整体准确和结构化，所有图像部分都必须在真实和有意义的方式相匹配。然而，现有的图像质量指标并不直接量化这种图像的全局一致性。在这项工作中，我们提出了两种可以测量生成图像的全局一致性的指标。为了测量全局一致性，我们假设一个真实的图像在整个物体或场景中都应该具有一致的属性，例如整个人体的脂肪在整个全身MRI中。因此，我们量化全局一致性的方式是通过使用训练过的神经网络预测和比较图像中的明确属性。然后，我们将这种策略应用到无标注 Setting中，通过测量图像中隐藏的特征的相似度来衡量图像的全局一致性。我们的结果表明，预测图像中的明确属性可以分辨全局一致性的图像和不一致性的图像。而隐藏的图像特征也可以用无标注数据来衡量全局一致性，尽管其敏感度较低。与已有的指标，如FID，相比，我们的方法可以直接量化全局一致性，从而启用专门分析单个生成图像的生物可靠性。
</details></li>
</ul>
<hr>
<h2 id="Fundus-Enhanced-Disease-Aware-Distillation-Model-for-Retinal-Disease-Classification-from-OCT-Images"><a href="#Fundus-Enhanced-Disease-Aware-Distillation-Model-for-Retinal-Disease-Classification-from-OCT-Images" class="headerlink" title="Fundus-Enhanced Disease-Aware Distillation Model for Retinal Disease Classification from OCT Images"></a>Fundus-Enhanced Disease-Aware Distillation Model for Retinal Disease Classification from OCT Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00291">http://arxiv.org/abs/2308.00291</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xmed-lab/fddm">https://github.com/xmed-lab/fddm</a></li>
<li>paper_authors: Lehan Wang, Weihang Dai, Mei Jin, Chubin Ou, Xiaomeng Li</li>
<li>for: 这个论文旨在提出一种基于Optical Coherence Tomography（OCT）的新型和有效的眼科检测工具。</li>
<li>methods: 该方法使用多模态学习，将有限的OCT数据与更多的背景图像相结合，以提高眼科检测的准确率。</li>
<li>results: 实验结果表明，我们提出的方法可以在眼科检测中提高准确率，并且比单模态、多模态和状态对抗学习方法更高效。<details>
<summary>Abstract</summary>
Optical Coherence Tomography (OCT) is a novel and effective screening tool for ophthalmic examination. Since collecting OCT images is relatively more expensive than fundus photographs, existing methods use multi-modal learning to complement limited OCT data with additional context from fundus images. However, the multi-modal framework requires eye-paired datasets of both modalities, which is impractical for clinical use. To address this problem, we propose a novel fundus-enhanced disease-aware distillation model (FDDM), for retinal disease classification from OCT images. Our framework enhances the OCT model during training by utilizing unpaired fundus images and does not require the use of fundus images during testing, which greatly improves the practicality and efficiency of our method for clinical use. Specifically, we propose a novel class prototype matching to distill disease-related information from the fundus model to the OCT model and a novel class similarity alignment to enforce consistency between disease distribution of both modalities. Experimental results show that our proposed approach outperforms single-modal, multi-modal, and state-of-the-art distillation methods for retinal disease classification. Code is available at https://github.com/xmed-lab/FDDM.
</details>
<details>
<summary>摘要</summary>
优化减相干扫描技术（OCT）是一种新型有效的诊断工具 для眼科检查。由于收集OCT图像相对较为昂贵于眼图像，现有的方法使用多Modal学习来补充OCT数据中的有限资源。然而，多Modal框架需要临床使用的眼球对应的数据集，这是不实际的。为解决这个问题，我们提出了一种新的眼球增强疾病感知模型（FDDM），用于从OCT图像中鉴别眼球疾病。我们的框架在训练时使用不匹配的眼球图像来增强OCT模型，并不需要在测试时使用眼球图像，这大大提高了我们的方法在临床使用的实用性和效率。具体来说，我们提出了一种新的疾病类型匹配来将疾病相关的信息从眼球模型传递给OCT模型，以及一种疾病分布一致的类型匹配来强制两种Modal中的疾病分布相似。实验结果表明，我们提出的方法在鉴别眼球疾病方面超过单Modal、多Modal和状态艺术的混合方法。代码可以在https://github.com/xmed-lab/FDDM上获取。
</details></li>
</ul>
<hr>
<h2 id="Unleashing-the-Power-of-Self-Supervised-Image-Denoising-A-Comprehensive-Review"><a href="#Unleashing-the-Power-of-Self-Supervised-Image-Denoising-A-Comprehensive-Review" class="headerlink" title="Unleashing the Power of Self-Supervised Image Denoising: A Comprehensive Review"></a>Unleashing the Power of Self-Supervised Image Denoising: A Comprehensive Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00247">http://arxiv.org/abs/2308.00247</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dan Zhang, Fangfang Zhou, Yuanzhou Wei, Xiao Yang, Yuan Gu</li>
<li>for: 提供了一个全面的评论，探讨最新的自动训练图像减震方法，以及它们在实际应用中的效果。</li>
<li>methods: 分为三类：通用方法、BSN基于方法和Transformer基于方法。</li>
<li>results: 通过在不同的数据集上进行量化和质量上的实验，证明了这些方法的效果，并提供了相对比较的结果。<details>
<summary>Abstract</summary>
The advent of deep learning has brought a revolutionary transformation to image denoising techniques. However, the persistent challenge of acquiring noise-clean pairs for supervised methods in real-world scenarios remains formidable, necessitating the exploration of more practical self-supervised image denoising. This paper focuses on self-supervised image denoising methods that offer effective solutions to address this challenge. Our comprehensive review thoroughly analyzes the latest advancements in self-supervised image denoising approaches, categorizing them into three distinct classes: General methods, Blind Spot Network (BSN)-based methods, and Transformer-based methods. For each class, we provide a concise theoretical analysis along with their practical applications. To assess the effectiveness of these methods, we present both quantitative and qualitative experimental results on various datasets, utilizing classical algorithms as benchmarks. Additionally, we critically discuss the current limitations of these methods and propose promising directions for future research. By offering a detailed overview of recent developments in self-supervised image denoising, this review serves as an invaluable resource for researchers and practitioners in the field, facilitating a deeper understanding of this emerging domain and inspiring further advancements.
</details>
<details>
<summary>摘要</summary>
深度学习的出现对图像干扰技术带来了革命性的变革。然而，在实际场景中获得干扰级别对的训练数据仍然是一个挑战，需要更加实用的自监学习图像干扰。这篇论文将关注自监学习图像干扰方法，以提供有效的解决方案。我们的全面回顾 thoroughly analyzes the latest advancements in self-supervised image denoising approaches, categorizing them into three distinct classes: General methods, Blind Spot Network (BSN)-based methods, and Transformer-based methods. For each class, we provide a concise theoretical analysis along with their practical applications. To assess the effectiveness of these methods, we present both quantitative and qualitative experimental results on various datasets, utilizing classical algorithms as benchmarks. Additionally, we critically discuss the current limitations of these methods and propose promising directions for future research. By offering a detailed overview of recent developments in self-supervised image denoising, this review serves as an invaluable resource for researchers and practitioners in the field, facilitating a deeper understanding of this emerging domain and inspiring further advancements.Here's the translation in Traditional Chinese:深度学习的出现对图像干扰技术带来了革命性的变革。然而，在实际场景中获得干扰级别对的训练数据仍然是一个挑战，需要更加实用的自监学习图像干扰。这篇论文将关注自监学习图像干扰方法，以提供有效的解决方案。我们的全面回顾 thoroughly analyzes the latest advancements in self-supervised image denoising approaches, categorizing them into three distinct classes: General methods, Blind Spot Network (BSN)-based methods, and Transformer-based methods. For each class, we provide a concise theoretical analysis along with their practical applications. To assess the effectiveness of these methods, we present both quantitative and qualitative experimental results on various datasets, utilizing classical algorithms as benchmarks. Additionally, we critically discuss the current limitations of these methods and propose promising directions for future research. By offering a detailed overview of recent developments in self-supervised image denoising, this review serves as an invaluable resource for researchers and practitioners in the field, facilitating a deeper understanding of this emerging domain and inspiring further advancements.
</details></li>
</ul>
<hr>
<h2 id="Boundary-Difference-Over-Union-Loss-For-Medical-Image-Segmentation"><a href="#Boundary-Difference-Over-Union-Loss-For-Medical-Image-Segmentation" class="headerlink" title="Boundary Difference Over Union Loss For Medical Image Segmentation"></a>Boundary Difference Over Union Loss For Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00220">http://arxiv.org/abs/2308.00220</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sunfan-bvb/boundarydouloss">https://github.com/sunfan-bvb/boundarydouloss</a></li>
<li>paper_authors: Fan Sun, Zhiming Luo, Shaozi Li</li>
<li>for: 这个论文是为了解决医疗图像分割中的边界区域分 segmentation问题，目前的损失函数主要是关注总体分 segmentation结果，有 fewer 的损失函数用于导航边界分 segmentation。</li>
<li>methods: 我们提出了一种简单有效的损失函数，即Boundary Difference over Union Loss (Boundary DoU Loss)，它通过计算预测和真实值之间的差异集的比率来导航边界区域分 segmentation。我们的损失函数仅仅基于区域计算，易于实现并在训练中稳定不需要其他损失函数。此外，我们使用目标大小进行适应性调整对边界区域应用注意力。</li>
<li>results: 我们在ACDC和Synapse两个数据集上使用UNet、TransUNet和Swin-UNet进行实验，得到了我们提出的损失函数的效果。<details>
<summary>Abstract</summary>
Medical image segmentation is crucial for clinical diagnosis. However, current losses for medical image segmentation mainly focus on overall segmentation results, with fewer losses proposed to guide boundary segmentation. Those that do exist often need to be used in combination with other losses and produce ineffective results. To address this issue, we have developed a simple and effective loss called the Boundary Difference over Union Loss (Boundary DoU Loss) to guide boundary region segmentation. It is obtained by calculating the ratio of the difference set of prediction and ground truth to the union of the difference set and the partial intersection set. Our loss only relies on region calculation, making it easy to implement and training stable without needing any additional losses. Additionally, we use the target size to adaptively adjust attention applied to the boundary regions. Experimental results using UNet, TransUNet, and Swin-UNet on two datasets (ACDC and Synapse) demonstrate the effectiveness of our proposed loss function. Code is available at https://github.com/sunfan-bvb/BoundaryDoULoss.
</details>
<details>
<summary>摘要</summary>
医疗图像分割是诊断中非常重要的一环。然而，目前的医疗图像分割损失主要关注整体分割结果，有少数损失用于导引边缘分割。这些损失经常需要与其他损失结合使用，并且会生成不效果。为解决这个问题，我们已经开发了一种简单有效的损失函数：Boundary Difference over Union Loss（Boundary DoU Loss），用于导引边缘区域分割。它是计算预测和真实值之间差异集的比率，并将其与union集和partial intersection集进行比较。我们的损失函数仅仅基于区域计算，因此易于实现和训练，不需要其他损失函数。此外，我们使用目标大小来适应性地调整边缘区域的注意力。实验结果表明，使用UNet、TransUNet和Swin-UNet在ACDC和Synapse两个数据集上，我们提出的损失函数能够准确地导引边缘区域的分割。代码可以在https://github.com/sunfan-bvb/BoundaryDoULoss中找到。
</details></li>
</ul>
<hr>
<h2 id="Universal-Adversarial-Defense-in-Remote-Sensing-Based-on-Pre-trained-Denoising-Diffusion-Models"><a href="#Universal-Adversarial-Defense-in-Remote-Sensing-Based-on-Pre-trained-Denoising-Diffusion-Models" class="headerlink" title="Universal Adversarial Defense in Remote Sensing Based on Pre-trained Denoising Diffusion Models"></a>Universal Adversarial Defense in Remote Sensing Based on Pre-trained Denoising Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16865">http://arxiv.org/abs/2307.16865</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/EricYu97/UAD-RS">https://github.com/EricYu97/UAD-RS</a></li>
<li>paper_authors: Weikang Yu, Yonghao Xu, Pedram Ghamisi<br>for:这个论文的目的是提出一种通用的防御方法，以防止深度神经网络在遥感应用中受到攻击。methods:这种方法使用预训练的扩散模型来防止多种未知的攻击，并使用反向和前向过程来纯化攻击样本。results:实验结果表明，这种方法可以在四个不同的遥感数据集上达到高性能，并且可以 universally 防止七种常见的攻击方式。<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) have achieved tremendous success in many remote sensing (RS) applications, in which DNNs are vulnerable to adversarial perturbations. Unfortunately, current adversarial defense approaches in RS studies usually suffer from performance fluctuation and unnecessary re-training costs due to the need for prior knowledge of the adversarial perturbations among RS data. To circumvent these challenges, we propose a universal adversarial defense approach in RS imagery (UAD-RS) using pre-trained diffusion models to defend the common DNNs against multiple unknown adversarial attacks. Specifically, the generative diffusion models are first pre-trained on different RS datasets to learn generalized representations in various data domains. After that, a universal adversarial purification framework is developed using the forward and reverse process of the pre-trained diffusion models to purify the perturbations from adversarial samples. Furthermore, an adaptive noise level selection (ANLS) mechanism is built to capture the optimal noise level of the diffusion model that can achieve the best purification results closest to the clean samples according to their Frechet Inception Distance (FID) in deep feature space. As a result, only a single pre-trained diffusion model is needed for the universal purification of adversarial samples on each dataset, which significantly alleviates the re-training efforts and maintains high performance without prior knowledge of the adversarial perturbations. Experiments on four heterogeneous RS datasets regarding scene classification and semantic segmentation verify that UAD-RS outperforms state-of-the-art adversarial purification approaches with a universal defense against seven commonly existing adversarial perturbations. Codes and the pre-trained models are available online (https://github.com/EricYu97/UAD-RS).
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）在远程感知（RS）应用中已经取得了很大的成功，但是DNN受到了抗击扰动的威胁。然而，RS中的抗击防御策略通常会受到性能波动和不必要的重新训练成本，因为需要对RS数据有严格的先知知识。为了解决这些挑战，我们提出了RS中的通用抗击防御策略（UAD-RS），使用预训练的扩散模型来防御RS数据中的多种未知抗击攻击。具体来说，首先预训练了不同RS数据集上的扩散模型，以学习各种数据域的通用表示。然后，我们使用扩散模型的前向和反向过程来纯化抗击样本中的扰动。此外，我们还构建了自适应噪声水平选择（ANLS）机制，以捕捉最佳的噪声水平，使扩散模型可以 closest to clean samples according to their Frechet Inception Distance（FID） in deep feature space achieve the best purification results。结果是，只需要一个预训练的扩散模型，可以对每个数据集进行通用的纯化，大大减少了重新训练的努力和维护高性能，无需对抗击攻击有严格的先知知识。实验表明，UAD-RS比状态前的抗击纯化方法更好，对七种常见的抗击攻击进行通用防御。代码和预训练模型可以在线获取（https://github.com/EricYu97/UAD-RS）。
</details></li>
</ul>
<hr>
<h2 id="A-comprehensive-review-of-deep-learning-in-lung-cancer"><a href="#A-comprehensive-review-of-deep-learning-in-lung-cancer" class="headerlink" title="A comprehensive review of deep learning in lung cancer"></a>A comprehensive review of deep learning in lung cancer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02528">http://arxiv.org/abs/2308.02528</a></li>
<li>repo_url: None</li>
<li>paper_authors: Farzane Tajidini</li>
<li>for: 本文提供了 cancer 诊断方法的历史背景，包括肿瘤诊断过程和临床医生使用的标准分类方法。</li>
<li>methods: 文章讨论了现有的肿瘤诊断方法，并指出了这些方法的缺陷，强调了需要新的、更智能的方法。</li>
<li>results: 文章未提出结果，但预示了需要更好的肿瘤诊断方法。<details>
<summary>Abstract</summary>
To provide the reader with a historical perspective on cancer classification approaches, we first discuss the fundamentals of the area of cancer diagnosis in this article, including the processes of cancer diagnosis and the standard classification methods employed by clinicians. Current methods for cancer diagnosis are deemed ineffective, calling for new and more intelligent approaches.
</details>
<details>
<summary>摘要</summary>
为了为读者提供 cancer 诊断方法的历史背景，我们首先介绍了 cancer 诊断领域的基本知识，包括肿瘤诊断过程和临床医生常用的标准分类方法。现有的肿瘤诊断方法被认为是不够有效，需要新的更智能的方法。Here's a word-for-word translation of the text using Traditional Chinese characters:为了为读者提供肿瘤诊断方法的历史背景，我们首先介绍了肿瘤诊断领域的基本知识，包括肿瘤诊断过程和临床医生常用的标准分类方法。现有的肿瘤诊断方法被认为是不够有效，需要新的更智能的方法。
</details></li>
</ul>
<hr>
<h2 id="Framing-image-registration-as-a-landmark-detection-problem-for-better-representation-of-clinical-relevance"><a href="#Framing-image-registration-as-a-landmark-detection-problem-for-better-representation-of-clinical-relevance" class="headerlink" title="Framing image registration as a landmark detection problem for better representation of clinical relevance"></a>Framing image registration as a landmark detection problem for better representation of clinical relevance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01318">http://arxiv.org/abs/2308.01318</a></li>
<li>repo_url: None</li>
<li>paper_authors: Diana Waldmannstetter, Benedikt Wiestler, Julian Schwarting, Ivan Ezhov, Marie Metz, Spyridon Bakas, Bhakti Baheti, Satrajit Chakrabarty, Jan S. Kirschke, Rolf A. Heckemann, Marie Piraud, Florian Kofler, Bjoern H. Menze</li>
<li>for: 这篇论文是为了重新评估图像注册方法的估值方法提出的。</li>
<li>methods: 论文使用了一种基于 landmark 检测的方法，通过对一些子样本进行人工分析来估算 landmark 特征的检测阈值。</li>
<li>results: 该方法可以提供不同的注册算法之间的区分，同时还可以评估图像注册算法的临床 significanc。<details>
<summary>Abstract</summary>
Nowadays, registration methods are typically evaluated based on sub-resolution tracking error differences. In an effort to reinfuse this evaluation process with clinical relevance, we propose to reframe image registration as a landmark detection problem. Ideally, landmark-specific detection thresholds are derived from an inter-rater analysis. To approximate this costly process, we propose to compute hit rate curves based on the distribution of errors of a sub-sample inter-rater analysis. Therefore, we suggest deriving thresholds from the error distribution using the formula: median + delta * median absolute deviation. The method promises differentiation of previously indistinguishable registration algorithms and further enables assessing the clinical significance in algorithm development.
</details>
<details>
<summary>摘要</summary>
现在，注册方法通常被评估基于微尺度跟踪错误差异。为了重新把注册评估过程抽象到临床 relevance，我们提议将注册视为一个标记检测问题。理想情况下，标记特定的检测阈值来自间评分析。为了 aproximate这个costly процесс，我们提议根据一个子样本间评分析的错误分布计算hit rate曲线。因此，我们建议使用错误分布中的 median + delta * median absolute deviation来 derive thresholds。这种方法可以区分之前无法分辨的注册算法，并且可以评估算法开发中的临床重要性。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/01/eess.IV_2023_08_01/" data-id="cllsiju3y008ra388alfa3wq7" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_07_31" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/31/cs.LG_2023_07_31/" class="article-date">
  <time datetime="2023-07-30T16:00:00.000Z" itemprop="datePublished">2023-07-31</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/31/cs.LG_2023_07_31/">cs.LG - 2023-07-31 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Classification-with-Deep-Neural-Networks-and-Logistic-Loss"><a href="#Classification-with-Deep-Neural-Networks-and-Logistic-Loss" class="headerlink" title="Classification with Deep Neural Networks and Logistic Loss"></a>Classification with Deep Neural Networks and Logistic Loss</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16792">http://arxiv.org/abs/2307.16792</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/himanshub1007/Alzhimers-Disease-Prediction-Using-Deep-learning">https://github.com/himanshub1007/Alzhimers-Disease-Prediction-Using-Deep-learning</a></li>
<li>paper_authors: Zihan Zhang, Lei Shi, Ding-Xuan Zhou<br>for: 这个论文主要是为了研究深度神经网络（DNN）在二分类任务中的泛化分析。methods: 这篇论文使用了 Oracle-type 不等式来处理目标函数的固定性问题，并使用这个不等式来 derive 深度神经网络（DNN） trained with logistic loss 的快速收敛率。results: 这篇论文得到了对于完全连接 ReLU DNN 分类器 trained with logistic loss 的快速收敛率，其中收敛率是对于数据 conditional class probability 的 H&quot;older 平滑性假设。此外，这篇论文还提出了一种 compositional 假设，以解释为什么 DNN 分类器在实际高维数据中能够表现良好。<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) trained with the logistic loss (i.e., the cross entropy loss) have made impressive advancements in various binary classification tasks. However, generalization analysis for binary classification with DNNs and logistic loss remains scarce. The unboundedness of the target function for the logistic loss is the main obstacle to deriving satisfying generalization bounds. In this paper, we aim to fill this gap by establishing a novel and elegant oracle-type inequality, which enables us to deal with the boundedness restriction of the target function, and using it to derive sharp convergence rates for fully connected ReLU DNN classifiers trained with logistic loss. In particular, we obtain optimal convergence rates (up to log factors) only requiring the H\"older smoothness of the conditional class probability $\eta$ of data. Moreover, we consider a compositional assumption that requires $\eta$ to be the composition of several vector-valued functions of which each component function is either a maximum value function or a H\"older smooth function only depending on a small number of its input variables. Under this assumption, we derive optimal convergence rates (up to log factors) which are independent of the input dimension of data. This result explains why DNN classifiers can perform well in practical high-dimensional classification problems. Besides the novel oracle-type inequality, the sharp convergence rates given in our paper also owe to a tight error bound for approximating the natural logarithm function near zero (where it is unbounded) by ReLU DNNs. In addition, we justify our claims for the optimality of rates by proving corresponding minimax lower bounds. All these results are new in the literature and will deepen our theoretical understanding of classification with DNNs.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）在多个二分类任务中取得了卓越的成就，但对于DNN和logistic损失的泛化分析却缺乏研究。Target函数的无上限性是泛化分析的主要障碍。在这篇论文中，我们尝试填补这一漏洞，并建立了一种新的oracle-type不等式，允许我们处理目标函数的上限限制，并使用其 derivation 深度神经网络（ReLU DNN） Fully Connected 类 clasifier 的快速收敛率。具体来说，我们获得了最佳的收敛率（带Logarithmic factor），只需要数据中 conditional class probability 的Holder smoothness。此外，我们还考虑了一种compositional assumption，即 conditional class probability 可以表示为多个vector-valued function的 composition，其中每个组件函数可以是最大值函数或Holder smooth function，只виси于一小部分输入变量。在这种假设下，我们获得了最佳的收敛率（带Logarithmic factor），不abhäng于输入维度。这个结果解释了为什么深度神经网络可以在实际高维度分类问题中表现良好。此外，我们还证明了我们的结果对泛化分析的 optimality 。这些结果都是文献中的新结果，它们将deepen our theoretical understanding of classification with DNNs。
</details></li>
</ul>
<hr>
<h2 id="ToolLLM-Facilitating-Large-Language-Models-to-Master-16000-Real-world-APIs"><a href="#ToolLLM-Facilitating-Large-Language-Models-to-Master-16000-Real-world-APIs" class="headerlink" title="ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs"></a>ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16789">http://arxiv.org/abs/2307.16789</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/openbmb/toolbench">https://github.com/openbmb/toolbench</a></li>
<li>paper_authors: Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, Maosong Sun</li>
<li>for: 这个论文旨在提高开源大语言模型（LLM）的高级任务能力，包括按照人类指令使用外部工具（API）。</li>
<li>methods: 作者引入了ToolLLM框架，包括数据建构、模型训练和评估，以便在开源LLM中实现工具使用能力。他们还开发了一个自动生成的实ruction-tuning数据集 ToolBench，并使用ChatGPT生成了多种人类指令，以覆盖单 Tool和多 Toolenario。</li>
<li>results: 作者的研究表明，通过使用DFSDT搜索算法，LLM可以提高计划和理解能力。此外，作者还开发了一个自动评估器 ToolEval，并 fine-tune LLaMA模型，以便评估ToolLLaMA的表现。结果显示，ToolLLaMA能够执行复杂的指令和扩展到未看过API，并与ChatGPT的性能相似。<details>
<summary>Abstract</summary>
Despite the advancements of open-source large language models (LLMs) and their variants, e.g., LLaMA and Vicuna, they remain significantly limited in performing higher-level tasks, such as following human instructions to use external tools (APIs). This is because current instruction tuning largely focuses on basic language tasks instead of the tool-use domain. This is in contrast to state-of-the-art (SOTA) LLMs, e.g., ChatGPT, which have demonstrated excellent tool-use capabilities but are unfortunately closed source. To facilitate tool-use capabilities within open-source LLMs, we introduce ToolLLM, a general tool-use framework of data construction, model training and evaluation. We first present ToolBench, an instruction-tuning dataset for tool use, which is created automatically using ChatGPT. Specifically, we collect 16,464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub, then prompt ChatGPT to generate diverse human instructions involving these APIs, covering both single-tool and multi-tool scenarios. Finally, we use ChatGPT to search for a valid solution path (chain of API calls) for each instruction. To make the searching process more efficient, we develop a novel depth-first search-based decision tree (DFSDT), enabling LLMs to evaluate multiple reasoning traces and expand the search space. We show that DFSDT significantly enhances the planning and reasoning capabilities of LLMs. For efficient tool-use assessment, we develop an automatic evaluator: ToolEval. We fine-tune LLaMA on ToolBench and obtain ToolLLaMA. Our ToolEval reveals that ToolLLaMA demonstrates a remarkable ability to execute complex instructions and generalize to unseen APIs, and exhibits comparable performance to ChatGPT. To make the pipeline more practical, we devise a neural API retriever to recommend appropriate APIs for each instruction, negating the need for manual API selection.
</details>
<details>
<summary>摘要</summary>
尽管开源大语言模型（LLM）和其变种（如LLaMA和Vicuna）已经取得了一定的进步，但它们仍然在执行更高级别任务时表现有限，比如按照人类 instrucion 使用外部工具（API）。这是因为当前的 instrucion 调整主要集中在基础语言任务上而不是工具使用领域。与此相比，当前的SOTA LLM（如ChatGPT）已经表现出了出色的工具使用能力，但它们却是关闭源代码。为了帮助开源 LLM 拥有工具使用能力，我们介绍了 ToolLLM，一个通用的工具使用框架，包括数据建构、模型训练和评估。我们首先介绍了 ToolBench，一个用于 instrucion 调整的数据集，由自动生成的 ChatGPT 提供。我们收集了来自 RapidAPI Hub 的 16,464 个实际 RESTful API，涵盖 49 个类别，然后使用 ChatGPT 生成多样化的人类 instrucion，涵盖单工具和多工具场景。最后，我们使用 ChatGPT 搜索一个有效的解决方案路径（chain of API calls） для每个 instrucion。为了使搜索过程更加效率，我们开发了一种深度优先搜索树（DFSDT），使 LLM 可以评估多种理解轨迹并扩展搜索空间。我们表明了 DFSDT 可以显著提高 LLM 的规划和理解能力。为了有效评估工具使用，我们开发了一个自动评估器：ToolEval。我们精度调整 LLaMA 以适应 ToolBench，并获得了 ToolLLaMA。我们的 ToolEval 表明，ToolLLaMA 在执行复杂 instrucion 和扩展到未看过 API 方面表现出色，与 ChatGPT 的表现相似。为了使该管道更实用，我们设计了一种神经网络 API 搜索器，以便在 instrucion 中提供合适的 API，从而消除人工 API 选择的需要。
</details></li>
</ul>
<hr>
<h2 id="Exploring-how-a-Generative-AI-interprets-music"><a href="#Exploring-how-a-Generative-AI-interprets-music" class="headerlink" title="Exploring how a Generative AI interprets music"></a>Exploring how a Generative AI interprets music</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00015">http://arxiv.org/abs/2308.00015</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gabriela Barenboim, Luigi Del Debbio, Johannes Hirn, Veronica Sanz</li>
<li>for: 使用 Google 的 MusicVAE 模型，对音乐序列进行表示和组织。</li>
<li>methods: 使用 Variational Auto-Encoder 对音乐序列进行学习，并将 latent 空间分为 relevance 的 music  neurons 和 noise  neurons。</li>
<li>results: 发现大多数 latent neurons 在实际音乐轨迹上都 remain silent，仅剩下几十个 music neurons 才会 fired。这些 music neurons 中的大多数都编码了 pitch 和 rhythm 信息，而 melody 信息则只在 longer sequences of music 中出现。<details>
<summary>Abstract</summary>
We use Google's MusicVAE, a Variational Auto-Encoder with a 512-dimensional latent space to represent a few bars of music, and organize the latent dimensions according to their relevance in describing music. We find that, on average, most latent neurons remain silent when fed real music tracks: we call these "noise" neurons. The remaining few dozens of latent neurons that do fire are called "music neurons". We ask which neurons carry the musical information and what kind of musical information they encode, namely something that can be identified as pitch, rhythm or melody. We find that most of the information about pitch and rhythm is encoded in the first few music neurons: the neural network has thus constructed a couple of variables that non-linearly encode many human-defined variables used to describe pitch and rhythm. The concept of melody only seems to show up in independent neurons for longer sequences of music.
</details>
<details>
<summary>摘要</summary>
我们使用Google的MusicVAE，一种512维 latent space的变量自动编码器，来表示一些乐曲的一些小段。我们发现，在 average，大多数 latent neuron 都 remain silent 当 fed 实际乐曲：我们称这些为 "噪音" neuron。剩下的几十个 latent neuron 才会 firing：我们称这些为 "音乐 neuron"。我们问到哪些 neuron 携带了音乐信息，它们是什么类型的音乐信息，例如把律、和声或旋律等。我们发现，大多数把律和和声信息都是在 first few music neuron 中编码的：因此，神经网络已经构造了一些变量，用于非线性地编码许多人定义的把律和和声变量。旋律信息只在 longer sequences of music 中出现独立的 neuron 中。
</details></li>
</ul>
<hr>
<h2 id="Lossless-Transformations-and-Excess-Risk-Bounds-in-Statistical-Inference"><a href="#Lossless-Transformations-and-Excess-Risk-Bounds-in-Statistical-Inference" class="headerlink" title="Lossless Transformations and Excess Risk Bounds in Statistical Inference"></a>Lossless Transformations and Excess Risk Bounds in Statistical Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16735">http://arxiv.org/abs/2307.16735</a></li>
<li>repo_url: None</li>
<li>paper_authors: László Györfi, Tamás Linder, Harro Walk</li>
<li>for: 这篇论文是关于统计推断中的剩余最小风险的研究，具体来说是计算一个随机变量从观察特征向量中的估计loss的最小值，并将其与特征向量的变换（统计）的loss进行比较。</li>
<li>methods: 这篇论文使用了lossless变换的定义，即lossless变换是一种使得额外风险为零的变换。然后，对于独立同分布的数据，构建了一个分区统计学习法，以验证一个给定的变换是lossless的。此外， authors还提出了基于信息理论的上限 bounds，以 uniform 地应用于广泛的损失函数类型。</li>
<li>results: 这篇论文的结果包括：1) 对独立同分布数据进行分区统计学习，可以确定一个给定的变换是lossless的; 2) 基于信息理论的上限 bounds，可以uniform 地应用于广泛的损失函数类型; 3) 在分类、非 Parametric 回归、股票投资、信息瓶颈和深度学习等领域中，lossless变换的概念和应用。<details>
<summary>Abstract</summary>
We study the excess minimum risk in statistical inference, defined as the difference between the minimum expected loss in estimating a random variable from an observed feature vector and the minimum expected loss in estimating the same random variable from a transformation (statistic) of the feature vector. After characterizing lossless transformations, i.e., transformations for which the excess risk is zero for all loss functions, we construct a partitioning test statistic for the hypothesis that a given transformation is lossless and show that for i.i.d. data the test is strongly consistent. More generally, we develop information-theoretic upper bounds on the excess risk that uniformly hold over fairly general classes of loss functions. Based on these bounds, we introduce the notion of a delta-lossless transformation and give sufficient conditions for a given transformation to be universally delta-lossless. Applications to classification, nonparametric regression, portfolio strategies, information bottleneck, and deep learning, are also surveyed.
</details>
<details>
<summary>摘要</summary>
我们研究额外最小风险在统计推断中，定义为从观察特征 вектор estimating 随机变量的最小风险差，与从特征 вектор变换 (统计) 中 estimating 同一个随机变量的风险差的差异。经过定义无损变换，即对所有损函数都有零风险差的变换，我们构造了一个分 partitioning 测试统计，用于测试一个给定的变换是否为无损变换，并证明在独立同分布数据上，该测试是强有效的。更一般地，我们开发了基于信息理论的上界，对随机变量的额外风险进行统一 bounds ，这些上界在较广泛的损函数类型上都成立。基于这些上界，我们引入 delta-lossless 变换的概念，并给出了universally delta-lossless 变换的充分条件。我们还将应用于分类、非 Parametric regression、资产配置策略、信息瓶颈和深度学习等领域进行了概述。
</details></li>
</ul>
<hr>
<h2 id="An-Efficient-Shapley-Value-Computation-for-the-Naive-Bayes-Classifier"><a href="#An-Efficient-Shapley-Value-Computation-for-the-Naive-Bayes-Classifier" class="headerlink" title="An Efficient Shapley Value Computation for the Naive Bayes Classifier"></a>An Efficient Shapley Value Computation for the Naive Bayes Classifier</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16718">http://arxiv.org/abs/2307.16718</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vincent Lemaire, Fabrice Clérot, Marc Boullé</li>
<li>for: The paper is written to propose an exact analytic expression of Shapley values for the naive Bayes classifier, and to compare the results with other methods such as Weight of Evidence (WoE) and KernelShap.</li>
<li>methods: The paper uses cooperative game theory and Shapley value estimation algorithms to provide an intelligibility method for the naive Bayes classifier.</li>
<li>results: The paper shows that the proposed Shapley proposal provides informative results with low algorithmic complexity, making it suitable for very large datasets with low computation time.Here are the three points in Simplified Chinese text:</li>
<li>for: 这篇论文是为了提出一个精确的分析表达方法，用于解释隐马氏投票分类器的决策。</li>
<li>methods: 论文使用了合作游戏理论和Shapley值估计算法，以提供一种可解释性的方法。</li>
<li>results: 论文显示，提出的Shapley值计算方法可以提供有用的结果，并且具有低算法复杂度和低计算时间，适用于非常大的数据集。<details>
<summary>Abstract</summary>
Variable selection or importance measurement of input variables to a machine learning model has become the focus of much research. It is no longer enough to have a good model, one also must explain its decisions. This is why there are so many intelligibility algorithms available today. Among them, Shapley value estimation algorithms are intelligibility methods based on cooperative game theory. In the case of the naive Bayes classifier, and to our knowledge, there is no ``analytical" formulation of Shapley values. This article proposes an exact analytic expression of Shapley values in the special case of the naive Bayes Classifier. We analytically compare this Shapley proposal, to another frequently used indicator, the Weight of Evidence (WoE) and provide an empirical comparison of our proposal with (i) the WoE and (ii) KernelShap results on real world datasets, discussing similar and dissimilar results. The results show that our Shapley proposal for the naive Bayes classifier provides informative results with low algorithmic complexity so that it can be used on very large datasets with extremely low computation time.
</details>
<details>
<summary>摘要</summary>
变量选择或输入变量重要性测量在机器学习模型中已成为研究焦点。不再只是有一个好模型，也必须解释其决策。这是为什么今天有so多可理解算法的原因。其中，Shapley值估计算法是基于合作游戏理论的可理解方法之一。在Naive Bayes分类器的特殊情况下，我们认为没有“分析”的形式ulation of Shapley values。本文提出了Naive Bayes分类器的特殊情况下的Shapley值的精确分析表达。我们对这一Shapley提案进行了分析比较，并对另一种常用指标——质量证明（WoE）进行了实际比较。我们还对我们的提案与（i）WoE和（ii）KernelShap结果进行了实际比较，讨论了相似和不同的结果。结果显示，我们的Shapley提案对Naive Bayes分类器提供了有用的结果，且算法复杂度低，可以在很大的数据集上进行快速计算，因此可以用于实际应用。
</details></li>
</ul>
<hr>
<h2 id="Active-Learning-in-Genetic-Programming-Guiding-Efficient-Data-Collection-for-Symbolic-Regression"><a href="#Active-Learning-in-Genetic-Programming-Guiding-Efficient-Data-Collection-for-Symbolic-Regression" class="headerlink" title="Active Learning in Genetic Programming: Guiding Efficient Data Collection for Symbolic Regression"></a>Active Learning in Genetic Programming: Guiding Efficient Data Collection for Symbolic Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00672">http://arxiv.org/abs/2308.00672</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hoolagans/stackgp">https://github.com/hoolagans/stackgp</a></li>
<li>paper_authors: Nathan Haut, Wolfgang Banzhaf, Bill Punch</li>
<li>for: 这种研究旨在探讨活动学习中的不确定性和多样性计算方法，以便选择有用的训练数据点。</li>
<li>methods: 这种研究使用了模型集合combined with uncertainty metric来利用模型Population在生态学编程中选择有用的训练数据点。我们考虑了多种不确定度度量，并发现了差异束度最佳。此外，我们还比较了两种数据多样度度量，发现相关度作为多样度度量表现较好，但有一些缺点。</li>
<li>results: 最终，我们使用Pareto优化approach将不确定度和多样度考虑在一起，以达到一种平衡的方式，以便在训练中选择有用和独特的数据点。<details>
<summary>Abstract</summary>
This paper examines various methods of computing uncertainty and diversity for active learning in genetic programming. We found that the model population in genetic programming can be exploited to select informative training data points by using a model ensemble combined with an uncertainty metric. We explored several uncertainty metrics and found that differential entropy performed the best. We also compared two data diversity metrics and found that correlation as a diversity metric performs better than minimum Euclidean distance, although there are some drawbacks that prevent correlation from being used on all problems. Finally, we combined uncertainty and diversity using a Pareto optimization approach to allow both to be considered in a balanced way to guide the selection of informative and unique data points for training.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="An-Empirical-Study-on-Log-based-Anomaly-Detection-Using-Machine-Learning"><a href="#An-Empirical-Study-on-Log-based-Anomaly-Detection-Using-Machine-Learning" class="headerlink" title="An Empirical Study on Log-based Anomaly Detection Using Machine Learning"></a>An Empirical Study on Log-based Anomaly Detection Using Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16714">http://arxiv.org/abs/2307.16714</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shan Ali, Chaima Boufaied, Domenico Bianculli, Paula Branco, Lionel Briand, Nathan Aschbacher</li>
<li>for: 本研究旨在evaluate不同的机器学习技术在Log-based Anomaly Detection（LAD）任务中的表现，并研究这些技术在不同的 datasets 和Context中的可行性。</li>
<li>methods: 本研究使用了不同的supervised和semi-supervised机器学习技术，包括传统机器学习和深度学习技术。</li>
<li>results: 研究发现，supervised传统机器学习技术和深度学习技术在检测精度和预测时间方面表现很接近，而semi-supervised技术则导致检测精度下降。此外，不同机器学习技术对于hyperparameter tuning的敏感性也有所不同。<details>
<summary>Abstract</summary>
The growth of systems complexity increases the need of automated techniques dedicated to different log analysis tasks such as Log-based Anomaly Detection (LAD). The latter has been widely addressed in the literature, mostly by means of different deep learning techniques. Nevertheless, the focus on deep learning techniques results in less attention being paid to traditional Machine Learning (ML) techniques, which may perform well in many cases, depending on the context and the used datasets. Further, the evaluation of different ML techniques is mostly based on the assessment of their detection accuracy. However, this is is not enough to decide whether or not a specific ML technique is suitable to address the LAD problem. Other aspects to consider include the training and prediction time as well as the sensitivity to hyperparameter tuning. In this paper, we present a comprehensive empirical study, in which we evaluate different supervised and semi-supervised, traditional and deep ML techniques w.r.t. four evaluation criteria: detection accuracy, time performance, sensitivity of detection accuracy as well as time performance to hyperparameter tuning. The experimental results show that supervised traditional and deep ML techniques perform very closely in terms of their detection accuracy and prediction time. Moreover, the overall evaluation of the sensitivity of the detection accuracy of the different ML techniques to hyperparameter tuning shows that supervised traditional ML techniques are less sensitive to hyperparameter tuning than deep learning techniques. Further, semi-supervised techniques yield significantly worse detection accuracy than supervised techniques.
</details>
<details>
<summary>摘要</summary>
Currently, the evaluation of different ML techniques for LAD is mostly based on the assessment of their detection accuracy. However, this is not enough to decide whether or not a specific ML technique is suitable to address the LAD problem. Other aspects to consider include the training and prediction time, as well as the sensitivity to hyperparameter tuning.In this paper, we present a comprehensive empirical study in which we evaluate different supervised and semi-supervised, traditional and deep ML techniques with respect to four evaluation criteria: detection accuracy, time performance, sensitivity of detection accuracy to hyperparameter tuning, and time performance to hyperparameter tuning. The experimental results show that supervised traditional and deep ML techniques perform very closely in terms of their detection accuracy and prediction time. Moreover, the overall evaluation of the sensitivity of the detection accuracy of the different ML techniques to hyperparameter tuning shows that supervised traditional ML techniques are less sensitive to hyperparameter tuning than deep learning techniques. Finally, semi-supervised techniques yield significantly worse detection accuracy than supervised techniques.
</details></li>
</ul>
<hr>
<h2 id="TFE-GNN-A-Temporal-Fusion-Encoder-Using-Graph-Neural-Networks-for-Fine-grained-Encrypted-Traffic-Classification"><a href="#TFE-GNN-A-Temporal-Fusion-Encoder-Using-Graph-Neural-Networks-for-Fine-grained-Encrypted-Traffic-Classification" class="headerlink" title="TFE-GNN: A Temporal Fusion Encoder Using Graph Neural Networks for Fine-grained Encrypted Traffic Classification"></a>TFE-GNN: A Temporal Fusion Encoder Using Graph Neural Networks for Fine-grained Encrypted Traffic Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16713">http://arxiv.org/abs/2307.16713</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ViktorAxelsen/TFE-GNN">https://github.com/ViktorAxelsen/TFE-GNN</a></li>
<li>paper_authors: Haozhen Zhang, Le Yu, Xi Xiao, Qing Li, Francesco Mercaldo, Xiapu Luo, Qixu Liu</li>
<li>for: 本研究旨在提出一种基于点对点相互信息（PMI）的字节级流量图构建方法，以及一种基于图 neural network（GNN）的特征提取模型（TFE-GNN），用于细致Encrypted traffic classification。</li>
<li>methods: 本研究使用了一种基于PMI的字节级流量图构建方法，并提出了一种基于GNN的特征提取模型TFE-GNN，包括双嵌入层、GNN基于流量图编码器以及交叉阻止特征融合机制。</li>
<li>results: 对于两个实际数据集，TFE-GNN的实验结果表明，它在细致Encrypted traffic classification任务中比多种现有方法表现更好。<details>
<summary>Abstract</summary>
Encrypted traffic classification is receiving widespread attention from researchers and industrial companies. However, the existing methods only extract flow-level features, failing to handle short flows because of unreliable statistical properties, or treat the header and payload equally, failing to mine the potential correlation between bytes. Therefore, in this paper, we propose a byte-level traffic graph construction approach based on point-wise mutual information (PMI), and a model named Temporal Fusion Encoder using Graph Neural Networks (TFE-GNN) for feature extraction. In particular, we design a dual embedding layer, a GNN-based traffic graph encoder as well as a cross-gated feature fusion mechanism, which can first embed the header and payload bytes separately and then fuses them together to obtain a stronger feature representation. The experimental results on two real datasets demonstrate that TFE-GNN outperforms multiple state-of-the-art methods in fine-grained encrypted traffic classification tasks.
</details>
<details>
<summary>摘要</summary>
《加密流量分类受到研究人员和产业公司的广泛关注。然而，现有方法仅提取流量水平特征，因为不可靠的统计性质而无法处理短流，或者对header和payload进行等效处理，而不是挖掘字节之间的潜在相关性。因此，在这篇论文中，我们提出了基于点wise私有信息（PMI）的字节级流量图构建方法，以及基于图神经网络（GNN）的时间融合编码器（TFE-GNN）。具体来说，我们设计了双嵌入层、GNN基于流量图编码器以及跨门控制的特征融合机制，可以首先将header和payload字节分别嵌入，然后将它们融合以获得更强的特征表示。实验结果表明，TFE-GNN在两个实际数据集上的细化加密流量分类任务中表现出色，超过了多种现有方法。》Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Meets-Adaptive-Filtering-A-Stein’s-Unbiased-Risk-Estimator-Approach"><a href="#Deep-Learning-Meets-Adaptive-Filtering-A-Stein’s-Unbiased-Risk-Estimator-Approach" class="headerlink" title="Deep Learning Meets Adaptive Filtering: A Stein’s Unbiased Risk Estimator Approach"></a>Deep Learning Meets Adaptive Filtering: A Stein’s Unbiased Risk Estimator Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16708">http://arxiv.org/abs/2307.16708</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zahra Esmaeilbeig, Mojtaba Soltanalian</li>
<li>for: 本文重新审视了两种常用的适应滤波算法，即回归最小二乘（RLS）和同态适应源分离（EASI），在源估计和分离上下文中。</li>
<li>methods: 我们通过抽象方法构建了两种任务深度学习框架，称为深度RLS和深度EASI。这些架构将原始算法的迭代转换为层次深度神经网络，以便通过训练过程进行高效的源信号估计。</li>
<li>results: 我们的实验结果表明，使用基于Stein不偏风险估计（SURE）的训练策略可以提高源信号估计性能。<details>
<summary>Abstract</summary>
This paper revisits two prominent adaptive filtering algorithms through the lens of algorithm unrolling, namely recursive least squares (RLS) and equivariant adaptive source separation (EASI), in the context of source estimation and separation. Building upon the unrolling methodology, we introduce novel task-based deep learning frameworks, denoted as Deep RLS and Deep EASI. These architectures transform the iterations of the original algorithms into layers of a deep neural network, thereby enabling efficient source signal estimation by taking advantage of a training process. To further enhance performance, we propose training these deep unrolled networks utilizing a loss function grounded on a Stein's unbiased risk estimator (SURE). Our empirical evaluations demonstrate the efficacy of this SURE-based approach for enhanced source signal estimation.
</details>
<details>
<summary>摘要</summary>
这篇论文重新审视了两种广泛使用的适应滤波算法，即反射最小二乘（RLS）和同态适应源分离（EASI），在源估计和分离上。基于折叠方法，我们介绍了一种新的任务基于深度学习框架，称为深度RLS和深度EASI。这些架构将原始算法的迭代转化为层次的深度神经网络，以便通过训练过程进行高效的源信号估计。为了进一步提高性能，我们提议使用基于斯坦不偏估量（SURE）的训练方法。我们的实验结果表明，这种SURE基本方法可以更好地提高源信号估计。
</details></li>
</ul>
<hr>
<h2 id="Lookbehind-Optimizer-k-steps-back-1-step-forward"><a href="#Lookbehind-Optimizer-k-steps-back-1-step-forward" class="headerlink" title="Lookbehind Optimizer: k steps back, 1 step forward"></a>Lookbehind Optimizer: k steps back, 1 step forward</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16704">http://arxiv.org/abs/2307.16704</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gonçalo Mordido, Pranshu Malviya, Aristide Baratin, Sarath Chandar</li>
<li>for: 提高深度神经网络训练稳定性，采用快速权重导航 descent 方向。</li>
<li>methods:  combinates Lookahead optimizer 和 Sharpness-aware minimization (SAM) ，实现多步变化稳定性和优化loss-sharpness交换。</li>
<li>results: 在多种任务和训练环境中显示了增强通用性、降低噪声权重robustness 和生长学习中的忘却难度。<details>
<summary>Abstract</summary>
The Lookahead optimizer improves the training stability of deep neural networks by having a set of fast weights that "look ahead" to guide the descent direction. Here, we combine this idea with sharpness-aware minimization (SAM) to stabilize its multi-step variant and improve the loss-sharpness trade-off. We propose Lookbehind, which computes $k$ gradient ascent steps ("looking behind") at each iteration and combine the gradients to bias the descent step toward flatter minima. We apply Lookbehind on top of two popular sharpness-aware training methods -- SAM and adaptive SAM (ASAM) -- and show that our approach leads to a myriad of benefits across a variety of tasks and training regimes. Particularly, we show increased generalization performance, greater robustness against noisy weights, and higher tolerance to catastrophic forgetting in lifelong learning settings.
</details>
<details>
<summary>摘要</summary>
“lookahead优化器可以提高深度神经网络的训练稳定性，通过一组快速的权重来引导下降方向。我们在这个想法基础上与锐度感知训练（SAM）相结合，以稳定其多步变体并改善损失锐度之间的交换。我们提出了“lookbehind”，它在每次迭代中计算$k$步上升步（“看后”），并将梯度相加以偏导下降步向平坦的极小值。我们在两种流行的锐度感知训练方法——SAM和自适应锐度感知训练（ASAM）——之上应用lookbehind方法，并证明了我们的方法在不同的任务和训练条件下具有多种优点，包括提高泛化性能、增加随机权重的稳定性和生长学习设置中的忘记性。”
</details></li>
</ul>
<hr>
<h2 id="A-theory-of-data-variability-in-Neural-Network-Bayesian-inference"><a href="#A-theory-of-data-variability-in-Neural-Network-Bayesian-inference" class="headerlink" title="A theory of data variability in Neural Network Bayesian inference"></a>A theory of data variability in Neural Network Bayesian inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16695">http://arxiv.org/abs/2307.16695</a></li>
<li>repo_url: None</li>
<li>paper_authors: Javed Lindner, David Dahmen, Michael Krämer, Moritz Helias</li>
<li>for: 这 paper 的目的是investigating neural networks in the limit of infinitely wide hidden layers, and providing a field-theoretic formalism for understanding their generalization properties.</li>
<li>methods: 这 paper 使用 Bayesian inference and kernel methods, specifically the neural network Gaussian process, to derive generalization properties of linear, non-linear, and deep non-linear networks for kernel matrices with heterogeneous entries.</li>
<li>results: 这 paper 系统地计算了 infinitely wide networks 的泛化性质, 并发现 data variability 导致一种非 Gaussian action, 类似于（φ^3+φ^4）-theory. furthermore, the paper obtains a homogeneous kernel matrix approximation for the learning curve and corrections due to data variability, which allow for the estimation of the generalization properties and exact results for the bounds of the learning curves in the case of infinitely many training data points.<details>
<summary>Abstract</summary>
Bayesian inference and kernel methods are well established in machine learning. The neural network Gaussian process in particular provides a concept to investigate neural networks in the limit of infinitely wide hidden layers by using kernel and inference methods. Here we build upon this limit and provide a field-theoretic formalism which covers the generalization properties of infinitely wide networks. We systematically compute generalization properties of linear, non-linear, and deep non-linear networks for kernel matrices with heterogeneous entries. In contrast to currently employed spectral methods we derive the generalization properties from the statistical properties of the input, elucidating the interplay of input dimensionality, size of the training data set, and variability of the data. We show that data variability leads to a non-Gaussian action reminiscent of a ($\varphi^3+\varphi^4$)-theory. Using our formalism on a synthetic task and on MNIST we obtain a homogeneous kernel matrix approximation for the learning curve as well as corrections due to data variability which allow the estimation of the generalization properties and exact results for the bounds of the learning curves in the case of infinitely many training data points.
</details>
<details>
<summary>摘要</summary>
bayesian inference和kernel方法在机器学习中已经非常流行。神经网络加 Gaussian process在特别提供了一种用于investigate神经网络在卷积层数量无穷大时的概念。我们在这个限制下建立了一个场景理论 formalism，涵盖无穷大神经网络的泛化性质。我们系统地计算了不同类型的神经网络（线性、非线性和深度非线性）在 kernel 矩阵中的泛化性质，并与当前使用的spectral方法进行比较。我们发现，数据变化导致一种非 Gaussian 动作，类似于（φ^3+φ^4）-理论。使用我们的 formalism 在一个 sintetic 任务上和 MNIST 上，我们得到了一个Homogeneous kernel matrix approximation 以及由数据变化引起的 corrections，这些 corrections 允许我们估算泛化性质并计算精确的 bound 值。
</details></li>
</ul>
<hr>
<h2 id="Guiding-Image-Captioning-Models-Toward-More-Specific-Captions"><a href="#Guiding-Image-Captioning-Models-Toward-More-Specific-Captions" class="headerlink" title="Guiding Image Captioning Models Toward More Specific Captions"></a>Guiding Image Captioning Models Toward More Specific Captions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16686">http://arxiv.org/abs/2307.16686</a></li>
<li>repo_url: None</li>
<li>paper_authors: Simon Kornblith, Lala Li, Zirui Wang, Thao Nguyen</li>
<li>for: 提高图像描述文本的准确性和特点性。</li>
<li>methods: 通过终端授益指导，使描述文本模型能够更好地识别图像。</li>
<li>results:  compared to标准的排序搜索，使用终端授益指导可以提高图像描述文本的准确性和特点性，但是可能会下降标准的参考基础captioning metric。<details>
<summary>Abstract</summary>
Image captioning is conventionally formulated as the task of generating captions for images that match the distribution of reference image-caption pairs. However, reference captions in standard captioning datasets are short and may not uniquely identify the images they describe. These problems are further exacerbated when models are trained directly on image-alt text pairs collected from the internet. In this work, we show that it is possible to generate more specific captions with minimal changes to the training process. We implement classifier-free guidance for an autoregressive captioning model by fine-tuning it to estimate both conditional and unconditional distributions over captions. The guidance scale applied at decoding controls a trade-off between maximizing $p(\mathrm{caption}|\mathrm{image})$ and $p(\mathrm{image}|\mathrm{caption})$. Compared to standard greedy decoding, decoding with a guidance scale of 2 substantially improves reference-free metrics such as CLIPScore (0.808 vs. 0.775) and caption$\to$image retrieval performance in the CLIP embedding space (recall@1 44.6% vs. 26.5%), but worsens standard reference-based captioning metrics (e.g., CIDEr 78.6 vs 126.1). We further explore the use of language models to guide the decoding process, obtaining small improvements over the Pareto frontier of reference-free vs. reference-based captioning metrics that arises from classifier-free guidance, and substantially improving the quality of captions generated from a model trained only on minimally curated web data.
</details>
<details>
<summary>摘要</summary>
图像描述是通常被定义为生成图像描述符与参考图像描述符对的分布匹配的任务。然而，参考描述在标准描述集中可能不够精细，这些问题更加恶化了在互联网上收集的图像描述对的训练过程中。在这项工作中，我们表明可以通过微调模型来生成更加精细的描述。我们实现了无类别导航的授意法，通过模型估计图像描述符的 conditional 和无条件分布来做出授意。在解码过程中，应用指导缩放控制了图像描述符的生成和图像描述符的匹配。与标准的排序解码相比，使用指导缩放可以大幅提高无参考指标（如 CLIPScore 0.808 vs. 0.775）和图像描述符到图像的匹配性（在 CLIP 空间中的 recall@1 44.6% vs. 26.5%），但是可能下降标准参考指标（如 CIDEr 78.6 vs 126.1）。我们还探讨了使用语言模型来引导解码过程，可以在无参考指标与参考指标之间取得小的改进，并且可以从无类别导航中获得较好的描述质量，从一个只有微处理的网络数据进行训练的模型中生成的描述。
</details></li>
</ul>
<hr>
<h2 id="On-the-Trustworthiness-Landscape-of-State-of-the-art-Generative-Models-A-Comprehensive-Survey"><a href="#On-the-Trustworthiness-Landscape-of-State-of-the-art-Generative-Models-A-Comprehensive-Survey" class="headerlink" title="On the Trustworthiness Landscape of State-of-the-art Generative Models: A Comprehensive Survey"></a>On the Trustworthiness Landscape of State-of-the-art Generative Models: A Comprehensive Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16680">http://arxiv.org/abs/2307.16680</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingyuan Fan, Cen Chen, Chengyu Wang, Jun Huang</li>
<li>for: 本文旨在探讨大规模生成模型的可靠性，尤其是在四个基本维度：隐私、安全、公平性和责任方面。</li>
<li>methods: 本文采用了评估大规模生成模型的传统和新兴威胁的方法，包括隐私泄露、安全漏洞、不公平性和责任不清等。</li>
<li>results: 本文提出了一份全面的可靠性映射，并提供了实践建议和未来发展方向，以促进大规模生成模型的可靠性，终于利大社会。<details>
<summary>Abstract</summary>
Diffusion models and large language models have emerged as leading-edge generative models and have sparked a revolutionary impact on various aspects of human life. However, the practical implementation of these models has also exposed inherent risks, highlighting their dual nature and raising concerns regarding their trustworthiness. Despite the abundance of literature on this subject, a comprehensive survey specifically delving into the intersection of large-scale generative models and their trustworthiness remains largely absent. To bridge this gap, This paper investigates both the long-standing and emerging threats associated with these models across four fundamental dimensions: privacy, security, fairness, and responsibility. In this way, we construct an extensive map outlining the trustworthiness of these models, while also providing practical recommendations and identifying future directions. These efforts are crucial for promoting the trustworthy deployment of these models, ultimately benefiting society as a whole.
</details>
<details>
<summary>摘要</summary>
大数据扩散模型和大语言模型在各种方面发挥了领先的生成模型作用，并且引发了人类生活中的各种变革。然而，实际应用这些模型也暴露了其内在的风险，抛出了它们的两面性和可信worthiness问题。尽管有大量的文献关于这个主题，但是一份特别关注这些模型的长期和新出现的威胁，以及四个基本维度上的可信worthiness映射，尚未得到了系统性的报告。为了填补这个空白，这篇论文调查了这些模型中的长期和新出现的威胁，并在四个基本维度上构建了可信worthiness映射，同时提供了实践的建议和未来方向。这些努力将有助于推动这些模型的可靠应用，最终对社会产生积极的影响。
</details></li>
</ul>
<hr>
<h2 id="Comparing-normalizing-flows-and-diffusion-models-for-prosody-and-acoustic-modelling-in-text-to-speech"><a href="#Comparing-normalizing-flows-and-diffusion-models-for-prosody-and-acoustic-modelling-in-text-to-speech" class="headerlink" title="Comparing normalizing flows and diffusion models for prosody and acoustic modelling in text-to-speech"></a>Comparing normalizing flows and diffusion models for prosody and acoustic modelling in text-to-speech</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16679">http://arxiv.org/abs/2307.16679</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guangyan Zhang, Thomas Merritt, Manuel Sam Ribeiro, Biel Tura-Vecino, Kayoko Yanagisawa, Kamil Pokora, Abdelhamid Ezzerg, Sebastian Cygert, Ammar Abbas, Piotr Bilinski, Roberto Barra-Chicote, Daniel Korzekwa, Jaime Lorenzo-Trueba</li>
<li>for: 这paper的目的是比较传统的L1&#x2F;L2损失优化方法和流体和扩散概率模型在文本到语音合成中的表现。</li>
<li>methods: 这paper使用了一个概率模型来生成log-f0和持续时间特征，然后使用这些特征来condition一个音频模型生成mel-spectrogram。</li>
<li>results: 实验结果显示，流体基本模型在spectrogram预测中达到了最好的性能，超过相当的扩散和L1模型。此外，扩散和流体概率预测器都导致了significant提高 compared to typical L2-trained prosody models。<details>
<summary>Abstract</summary>
Neural text-to-speech systems are often optimized on L1/L2 losses, which make strong assumptions about the distributions of the target data space. Aiming to improve those assumptions, Normalizing Flows and Diffusion Probabilistic Models were recently proposed as alternatives. In this paper, we compare traditional L1/L2-based approaches to diffusion and flow-based approaches for the tasks of prosody and mel-spectrogram prediction for text-to-speech synthesis. We use a prosody model to generate log-f0 and duration features, which are used to condition an acoustic model that generates mel-spectrograms. Experimental results demonstrate that the flow-based model achieves the best performance for spectrogram prediction, improving over equivalent diffusion and L1 models. Meanwhile, both diffusion and flow-based prosody predictors result in significant improvements over a typical L2-trained prosody models.
</details>
<details>
<summary>摘要</summary>
traditional L1/L2-based方法与流体和扩散概率模型相比，在文本至语音合成 tasks中进行比较。我们使用一个气质模型生成 log-f0 和持续时间特征，这些特征用于condition一个生成 mel-spectrogram 的声学模型。实验结果表明，流体基本模型在 spectrogram 预测中取得最好的性能，超过相同的扩散和 L1 模型。此外， beiden diffusion 和流体气质预测器都导致了 Typical L2 训练的气质模型显著提高。
</details></li>
</ul>
<hr>
<h2 id="End-to-End-Reinforcement-Learning-for-Torque-Based-Variable-Height-Hopping"><a href="#End-to-End-Reinforcement-Learning-for-Torque-Based-Variable-Height-Hopping" class="headerlink" title="End-to-End Reinforcement Learning for Torque Based Variable Height Hopping"></a>End-to-End Reinforcement Learning for Torque Based Variable Height Hopping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16676">http://arxiv.org/abs/2307.16676</a></li>
<li>repo_url: None</li>
<li>paper_authors: Raghav Soni, Daniel Harnack, Hauke Isermann, Sotaro Fushimi, Shivesh Kumar, Frank Kirchner</li>
<li>for: 这 paper 是为了研究一种基于 Reinforcement Learning 的扭矩控制器，以便在激活跳跃时自动掌握不同跳跃阶段的控制。</li>
<li>methods: 这 paper 使用了 Reinforcement Learning 技术，通过学习练习控制器来适应不同的跳跃阶段。</li>
<li>results: 这 paper 的研究结果表明，使用这种基于 Reinforcement Learning 的扭矩控制器可以在不同的跳跃阶段中自动掌握控制，并且可以在实际中成功部署到机器人上。<details>
<summary>Abstract</summary>
Legged locomotion is arguably the most suited and versatile mode to deal with natural or unstructured terrains. Intensive research into dynamic walking and running controllers has recently yielded great advances, both in the optimal control and reinforcement learning (RL) literature. Hopping is a challenging dynamic task involving a flight phase and has the potential to increase the traversability of legged robots. Model based control for hopping typically relies on accurate detection of different jump phases, such as lift-off or touch down, and using different controllers for each phase. In this paper, we present a end-to-end RL based torque controller that learns to implicitly detect the relevant jump phases, removing the need to provide manual heuristics for state detection. We also extend a method for simulation to reality transfer of the learned controller to contact rich dynamic tasks, resulting in successful deployment on the robot after training without parameter tuning.
</details>
<details>
<summary>摘要</summary>
四肢行走是可能最适合和最多样化的行走模式，能够应对自然或无结构的地形。最近，对于动态步态和奖励学习（RL）文献中的研究做出了大量进步。跳跃是一种复杂的动态任务，具有飞行阶段，可以提高四肢机器人的通行性。基于模型的控制方法通常需要精准地探测不同跳跃阶段，如升空或接触，并使用不同的控制器来处理每个阶段。在这篇论文中，我们提出了一种端到端RL基于扭矩控制器，通过透传学习来适应不同跳跃阶段，从而消除了手动规则的需求。此外，我们还扩展了在实际中使用学习到的控制器的方法，并成功在机器人上部署无需参数调整。
</details></li>
</ul>
<hr>
<h2 id="Generative-models-for-wearables-data"><a href="#Generative-models-for-wearables-data" class="headerlink" title="Generative models for wearables data"></a>Generative models for wearables data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16664">http://arxiv.org/abs/2307.16664</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arinbjörn Kolbeinsson, Luca Foschini</li>
<li>for:  solves the problem of data scarcity in medical research by generating realistic wearable activity data.</li>
<li>methods: uses a multi-task self-attention model to generate the data.</li>
<li>results: the generated data is similar to genuine samples, as demonstrated through both quantitative and qualitative approaches.Here’s the Chinese translation of the three points:</li>
<li>for:  solves the problem of 医学研究数据短缺问题 by generating 生成医学活动数据.</li>
<li>methods: uses a multi-task self-attention model to generate the data.</li>
<li>results: the generated data is similar to genuine samples, as demonstrated through both quantitative and qualitative approaches.I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Data scarcity is a common obstacle in medical research due to the high costs associated with data collection and the complexity of gaining access to and utilizing data. Synthesizing health data may provide an efficient and cost-effective solution to this shortage, enabling researchers to explore distributions and populations that are not represented in existing observations or difficult to access due to privacy considerations. To that end, we have developed a multi-task self-attention model that produces realistic wearable activity data. We examine the characteristics of the generated data and quantify its similarity to genuine samples with both quantitative and qualitative approaches.
</details>
<details>
<summary>摘要</summary>
医学研究中的数据短缺是一个常见的障碍因素，这是因为数据收集的成本高昂，以及获取和利用数据的复杂性。生成健康数据可能提供一个有效和成本高的解决方案，允许研究人员探索现有观察数据中未表现出来的分布和人口，以及因隐私考虑而Difficult to access的数据。为此，我们开发了一种多任务自注意模型，生成了真实的穿戴活动数据。我们分析生成数据的特点，并使用量化和质量方法衡量与真实样本的相似性。
</details></li>
</ul>
<hr>
<h2 id="Graph-Structure-from-Point-Clouds-Geometric-Attention-is-All-You-Need"><a href="#Graph-Structure-from-Point-Clouds-Geometric-Attention-is-All-You-Need" class="headerlink" title="Graph Structure from Point Clouds: Geometric Attention is All You Need"></a>Graph Structure from Point Clouds: Geometric Attention is All You Need</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16662">http://arxiv.org/abs/2307.16662</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/murnanedaniel/geometricattention">https://github.com/murnanedaniel/geometricattention</a></li>
<li>paper_authors: Daniel Murnane</li>
<li>for: 这篇论文主要针对高能物理中点云问题进行了研究，特别是如何在这些问题中构建图structure。</li>
<li>methods: 该论文提出了一种注意力机制，可以在学习空间中自动构建一个图，以处理点云问题中的流量相关性。</li>
<li>results: 该论文在标注粒子束 зада务上测试了这种 Architecture，称为GravNetNorm，并显示其和其他相似模型相比，具有类似的准精度，而且使用的计算资源相对较少。<details>
<summary>Abstract</summary>
The use of graph neural networks has produced significant advances in point cloud problems, such as those found in high energy physics. The question of how to produce a graph structure in these problems is usually treated as a matter of heuristics, employing fully connected graphs or K-nearest neighbors. In this work, we elevate this question to utmost importance as the Topology Problem. We propose an attention mechanism that allows a graph to be constructed in a learned space that handles geometrically the flow of relevance, providing one solution to the Topology Problem. We test this architecture, called GravNetNorm, on the task of top jet tagging, and show that it is competitive in tagging accuracy, and uses far fewer computational resources than all other comparable models.
</details>
<details>
<summary>摘要</summary>
使用图 нейрон网络已经取得了高能物理学中点云问题的显著进步。通常，在这些问题中，构建图结构是视为低级别的决策，通过全连接图或K最近邻居的方法进行实现。在这个工作中，我们升级这个问题为最高优先级的topology问题。我们提议一种注意力机制，使得在学习空间中构建的图可以正确地处理流量的 geomtry，提供一个解决topology问题的方案。我们测试了这个架构，称为GravNetNorm，在top jet标签任务上，并显示它与其他相似模型相比，具有竞争力的标签准确率，并使用的计算资源比其他模型少得多。
</details></li>
</ul>
<hr>
<h2 id="Proactive-Resource-Request-for-Disaster-Response-A-Deep-Learning-based-Optimization-Model"><a href="#Proactive-Resource-Request-for-Disaster-Response-A-Deep-Learning-based-Optimization-Model" class="headerlink" title="Proactive Resource Request for Disaster Response: A Deep Learning-based Optimization Model"></a>Proactive Resource Request for Disaster Response: A Deep Learning-based Optimization Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16661">http://arxiv.org/abs/2307.16661</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongzhe Zhang, Xiaohang Zhao, Xiao Fang, Bintong Chen</li>
<li>for: 这个论文主要是为了解决灾难应急响应中资源管理问题，尤其是决定紧急需求的资源量。</li>
<li>methods: 这篇论文使用了深度学习方法来预测未来需求，并根据这些特点形式化了一个难题优化模型，最后提出了一种有效的解决方案。</li>
<li>results: 论文通过使用实际数据和 simulate 数据进行比较，证明了其比现有方法更高效和有更好的性能，同时在多个参与者和多个目标下也表现出了优异性。<details>
<summary>Abstract</summary>
Disaster response is critical to save lives and reduce damages in the aftermath of a disaster. Fundamental to disaster response operations is the management of disaster relief resources. To this end, a local agency (e.g., a local emergency resource distribution center) collects demands from local communities affected by a disaster, dispatches available resources to meet the demands, and requests more resources from a central emergency management agency (e.g., Federal Emergency Management Agency in the U.S.). Prior resource management research for disaster response overlooks the problem of deciding optimal quantities of resources requested by a local agency. In response to this research gap, we define a new resource management problem that proactively decides optimal quantities of requested resources by considering both currently unfulfilled demands and future demands. To solve the problem, we take salient characteristics of the problem into consideration and develop a novel deep learning method for future demand prediction. We then formulate the problem as a stochastic optimization model, analyze key properties of the model, and propose an effective solution method to the problem based on the analyzed properties. We demonstrate the superior performance of our method over prevalent existing methods using both real world and simulated data. We also show its superiority over prevalent existing methods in a multi-stakeholder and multi-objective setting through simulations.
</details>
<details>
<summary>摘要</summary>
灾害应急应对是保存生命和减少灾害的关键。紧急应急管理的核心是管理紧急救援资源。因此，当地机构（例如本地紧急资源分配中心）会收集受到灾害影响的当地社区的需求，派发可用资源以满足需求，并请求中央紧急管理机构（例如美国联邦紧急管理署）提供更多资源。现有的紧急应急资源管理研究未能考虑当地机构请求最佳资源量的问题。为了解决这个研究漏洞，我们定义了一个新的资源管理问题，该问题考虑当地机构请求资源量的优化问题，并考虑当前未满足的需求和未来需求。为了解决这个问题，我们考虑了问题的重要特征，并开发了一种新的深度学习方法来预测未来需求。然后，我们将问题转化为一个随机优化模型，分析了模型的关键属性，并提出了一种有效的解决方案。我们通过使用实际数据和 simulate 数据进行比较，证明了我们的方法在现有方法之上具有显著的优势。此外，我们通过多个各自目标和多个利益相关者的多Objective 模拟来证明我们的方法在多个环境下具有优秀的表现。
</details></li>
</ul>
<hr>
<h2 id="Sequential-and-Shared-Memory-Parallel-Algorithms-for-Partitioned-Local-Depths"><a href="#Sequential-and-Shared-Memory-Parallel-Algorithms-for-Partitioned-Local-Depths" class="headerlink" title="Sequential and Shared-Memory Parallel Algorithms for Partitioned Local Depths"></a>Sequential and Shared-Memory Parallel Algorithms for Partitioned Local Depths</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16652">http://arxiv.org/abs/2307.16652</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aditya Devarakonda, Grey Ballard</li>
<li>for: 本文是为了开发一种基于对比距离的强相关关系标识方法，以便在含有大量数据点和距离对的情况下，快速发现强相关关系。</li>
<li>methods: 本文使用了两种变体的算法，它们都是基于对比距离的，并且使用了分区本地深度（PaLD）方法进行社区结构分析。</li>
<li>results: 本文的计算和通信成本分析表明，sequential算法是通信优化的，即使在高并发情况下，也能够保持优化的性能。此外， authors 还提出了一些性能优化策略，使得 sequential 实现中的速度可以达到 $29\times$ 的提升，并且 parallel 实现中的速度可以达到 $19.4\times$。<details>
<summary>Abstract</summary>
In this work, we design, analyze, and optimize sequential and shared-memory parallel algorithms for partitioned local depths (PaLD). Given a set of data points and pairwise distances, PaLD is a method for identifying strength of pairwise relationships based on relative distances, enabling the identification of strong ties within dense and sparse communities even if their sizes and within-community absolute distances vary greatly. We design two algorithmic variants that perform community structure analysis through triplet comparisons of pairwise distances. We present theoretical analyses of computation and communication costs and prove that the sequential algorithms are communication optimal, up to constant factors. We introduce performance optimization strategies that yield sequential speedups of up to $29\times$ over a baseline sequential implementation and parallel speedups of up to $19.4\times$ over optimized sequential implementations using up to $32$ threads on an Intel multicore CPU.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们设计、分析和优化了继承和共享内存并行算法 для分区本地深度（PaLD）。给定一个数据点集和对应的距离对，PaLD是一种基于相对距离的对Pairwise关系强度识别方法，允许发现具有不同大小和内部绝对距离的稠密和稀疏社区的强关系。我们设计了两种算法变体，通过对三元比较pairwise距离来进行社区结构分析。我们提供了计算和通信成本的理论分析，并证明sequential算法是通信优化的，即在常数因子上。我们还提出了性能优化策略，可以实现sequential执行速度提升到29倍，并且并行执行速度提升到19.4倍，使用最多32个Intel多核CPU进行优化。
</details></li>
</ul>
<hr>
<h2 id="UDAMA-Unsupervised-Domain-Adaptation-through-Multi-discriminator-Adversarial-Training-with-Noisy-Labels-Improves-Cardio-fitness-Prediction"><a href="#UDAMA-Unsupervised-Domain-Adaptation-through-Multi-discriminator-Adversarial-Training-with-Noisy-Labels-Improves-Cardio-fitness-Prediction" class="headerlink" title="UDAMA: Unsupervised Domain Adaptation through Multi-discriminator Adversarial Training with Noisy Labels Improves Cardio-fitness Prediction"></a>UDAMA: Unsupervised Domain Adaptation through Multi-discriminator Adversarial Training with Noisy Labels Improves Cardio-fitness Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16651">http://arxiv.org/abs/2307.16651</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yvonneywu/udama">https://github.com/yvonneywu/udama</a></li>
<li>paper_authors: Yu Wu, Dimitris Spathis, Hong Jia, Ignacio Perez-Pozuelo, Tomas Gonzales, Soren Brage, Nicholas Wareham, Cecilia Mascolo</li>
<li>for: 这篇论文是为了提出一种基于频率预测的域 adaptation 方法，以解决因量化差异引起的分布偏移问题。</li>
<li>methods: 该方法包括两个关键组成部分：无监督域适应和多种抑制器对抗训练。首先，我们在银标标签数据上进行无监督域适应，然后与金标标签数据进行对抗适应，同时使用两个域抑制器。</li>
<li>results: 我们通过应用该方法于心血管健康预测任务来证明其实用性。结果表明，UDAMA可以有效地缓解分布偏移问题，并在不同的标签偏移设置下表现出优秀的性能。此外，我们通过使用两个自由生活群体研究（Fenland和BBVS）的数据，证明UDAMA可以与其他转移学习和域适应模型相比，在规模上提供更好的预测性能。<details>
<summary>Abstract</summary>
Deep learning models have shown great promise in various healthcare monitoring applications. However, most healthcare datasets with high-quality (gold-standard) labels are small-scale, as directly collecting ground truth is often costly and time-consuming. As a result, models developed and validated on small-scale datasets often suffer from overfitting and do not generalize well to unseen scenarios. At the same time, large amounts of imprecise (silver-standard) labeled data, annotated by approximate methods with the help of modern wearables and in the absence of ground truth validation, are starting to emerge. However, due to measurement differences, this data displays significant label distribution shifts, which motivates the use of domain adaptation. To this end, we introduce UDAMA, a method with two key components: Unsupervised Domain Adaptation and Multidiscriminator Adversarial Training, where we pre-train on the silver-standard data and employ adversarial adaptation with the gold-standard data along with two domain discriminators. In particular, we showcase the practical potential of UDAMA by applying it to Cardio-respiratory fitness (CRF) prediction. CRF is a crucial determinant of metabolic disease and mortality, and it presents labels with various levels of noise (goldand silver-standard), making it challenging to establish an accurate prediction model. Our results show promising performance by alleviating distribution shifts in various label shift settings. Additionally, by using data from two free-living cohort studies (Fenland and BBVS), we show that UDAMA consistently outperforms up to 12% compared to competitive transfer learning and state-of-the-art domain adaptation models, paving the way for leveraging noisy labeled data to improve fitness estimation at scale.
</details>
<details>
<summary>摘要</summary>
深度学习模型在医疗监测应用中表现出了很大的搭配性。然而，大多数医疗数据集（高品质标签）是小规模的，因为直接收集真实标签是经济不可能和时间consuming。因此，在小规模数据集上开发和验证的模型通常会过拟合，并不能很好地适应未见enario。同时，大量的不准确（银标准）标签数据，通过现代穿戴物和不含真实标签验证的方式获得，开始出现。然而，由于测量差异，这些数据会显示明显的标签分布偏移，这种情况驱动了我们使用领域适应。为此，我们提出了UDAMA方法，它包括无监督领域适应和多护卫者对抗预训练。我们在Cardio-respiratory fitness（CRF）预测中应用了UDAMA方法，CRF是生物 markers的一个重要指标，同时标签具有各种噪音（银标准和金标准），因此建立准确的预测模型是一项挑战。我们的结果表明，UDAMA方法可以有效地缓解标签分布偏移，并在不同的标签偏移设置下表现出优秀的性能。此外，通过使用两个自由生活 cohort studies（Fenland和BBVS）的数据，我们表明了UDAMA方法可以在不同的标签偏移设置下 consistently outperform up to 12% compared to competitive transfer learning和领域适应模型，为了利用噪音标签数据来提高健身估计而做出了重要贡献。
</details></li>
</ul>
<hr>
<h2 id="LLMs4OL-Large-Language-Models-for-Ontology-Learning"><a href="#LLMs4OL-Large-Language-Models-for-Ontology-Learning" class="headerlink" title="LLMs4OL: Large Language Models for Ontology Learning"></a>LLMs4OL: Large Language Models for Ontology Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16648">http://arxiv.org/abs/2307.16648</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hamedbabaei/llms4ol">https://github.com/hamedbabaei/llms4ol</a></li>
<li>paper_authors: Hamed Babaei Giglou, Jennifer D’Souza, Sören Auer</li>
<li>for: 本文探讨了使用大语言模型（LLMs）进行ontology learning（OL）的可能性，以捕捉自然语言文本中的知识。</li>
<li>methods: 本文使用了零例示提问方法来测试九种不同的LLM模型家族，以完成三种主要的OL任务：词类型分类、生物地理知识发现和非分类关系提取。</li>
<li>results: results show that LLMs can effectively apply their language pattern capturing capability to OL tasks, achieving high accuracy in all three tasks and outperforming traditional OL methods. Additionally, the evaluations encompass diverse genres of ontological knowledge, including lexicosemantic knowledge in WordNet, geographical knowledge in GeoNames, and medical knowledge in UMLS.<details>
<summary>Abstract</summary>
We propose the LLMs4OL approach, which utilizes Large Language Models (LLMs) for Ontology Learning (OL). LLMs have shown significant advancements in natural language processing, demonstrating their ability to capture complex language patterns in different knowledge domains. Our LLMs4OL paradigm investigates the following hypothesis: \textit{Can LLMs effectively apply their language pattern capturing capability to OL, which involves automatically extracting and structuring knowledge from natural language text?} To test this hypothesis, we conduct a comprehensive evaluation using the zero-shot prompting method. We evaluate nine different LLM model families for three main OL tasks: term typing, taxonomy discovery, and extraction of non-taxonomic relations. Additionally, the evaluations encompass diverse genres of ontological knowledge, including lexicosemantic knowledge in WordNet, geographical knowledge in GeoNames, and medical knowledge in UMLS.
</details>
<details>
<summary>摘要</summary>
我们提出LLMs4OL方法，利用大型语言模型（LLMs）进行 Ontology Learning（OL）。 LLMS 在自然语言处理方面已经显示出了重要的进步，能够捕捉不同知识领域中的复杂语言模式。我们的LLMs4OL paradigm探讨以下假设：\textit{可以 LLMS 通过自然语言文本中自动抽取和结构知识来进行 OL？} 为了证明这一假设，我们进行了全面的评估，使用零容器提示方法。我们评估了九种不同的 LLMS 模型家族，对三个主要的 OL 任务进行了评估：命名类型、地理知识发现和非分类关系抽取。此外，评估还涵盖了不同类型的 ontological 知识，包括 WordNet 的lexicosemantic 知识、GeoNames 的地理知识和 UMLS 的医学知识。
</details></li>
</ul>
<hr>
<h2 id="Text-CRS-A-Generalized-Certified-Robustness-Framework-against-Textual-Adversarial-Attacks"><a href="#Text-CRS-A-Generalized-Certified-Robustness-Framework-against-Textual-Adversarial-Attacks" class="headerlink" title="Text-CRS: A Generalized Certified Robustness Framework against Textual Adversarial Attacks"></a>Text-CRS: A Generalized Certified Robustness Framework against Textual Adversarial Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16630">http://arxiv.org/abs/2307.16630</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Eyr3/TextCRS">https://github.com/Eyr3/TextCRS</a></li>
<li>paper_authors: Xinyu Zhang, Hanbin Hong, Yuan Hong, Peng Huang, Binghui Wang, Zhongjie Ba, Kui Ren</li>
<li>for: 防止文本攻击，提高模型Robustness</li>
<li>methods: 基于随机缓和的泛化证明框架Text-CRS，对word-level adversarial operation进行 permutation和embedding space中的证明bounds，并考虑数字字典之间的数学关系和随机缓和分布来提高证明准确性和半径</li>
<li>results: Text-CRS可以Addressing all four word-level adversarial operations，Significantly improve certified accuracy and radius，并提供了word-level adversarial operation的首个证明精度和半径的 benchmark，Outperform state-of-the-art certification against synonym substitution attacks.<details>
<summary>Abstract</summary>
The language models, especially the basic text classification models, have been shown to be susceptible to textual adversarial attacks such as synonym substitution and word insertion attacks. To defend against such attacks, a growing body of research has been devoted to improving the model robustness. However, providing provable robustness guarantees instead of empirical robustness is still widely unexplored. In this paper, we propose Text-CRS, a generalized certified robustness framework for natural language processing (NLP) based on randomized smoothing. To our best knowledge, existing certified schemes for NLP can only certify the robustness against $\ell_0$ perturbations in synonym substitution attacks. Representing each word-level adversarial operation (i.e., synonym substitution, word reordering, insertion, and deletion) as a combination of permutation and embedding transformation, we propose novel smoothing theorems to derive robustness bounds in both permutation and embedding space against such adversarial operations. To further improve certified accuracy and radius, we consider the numerical relationships between discrete words and select proper noise distributions for the randomized smoothing. Finally, we conduct substantial experiments on multiple language models and datasets. Text-CRS can address all four different word-level adversarial operations and achieve a significant accuracy improvement. We also provide the first benchmark on certified accuracy and radius of four word-level operations, besides outperforming the state-of-the-art certification against synonym substitution attacks.
</details>
<details>
<summary>摘要</summary>
“语言模型，特别是基本文本分类模型，已经被证明容易受到文本敌对攻击，如同Synonym替换和单词插入攻击。为了防止这些攻击，研究人员已经投入了大量时间和精力来提高模型的Robustness。然而，提供可证明的Robustness保证而不是实际的Robustness仍然是未探索的领域。在这篇论文中，我们提出了Text-CRS，一种泛化证明Robustness框架 для自然语言处理（NLP），基于随机化缓和。我们知道，现有的NLP证明可以只证明对于 $\ell_0$ 杂乱攻击的Robustness。我们将每种单词级敌对操作（ synonym替换、单词重新排序、插入和删除）表示为一种组合的 permutation 和 embedding 变换。我们提出了新的缓和定理，以derive Robustness  bound在 permutation 和 embedding 空间中对于这些敌对操作。为了进一步提高证明精度和半径，我们考虑了字符串间的数学关系，并选择了适合的噪声分布。最后，我们进行了大量的实验， Text-CRS 可以处理所有的四种单词级敌对操作，并达到了显著的准确率提高。我们还提供了对四种单词级敌对操作的证明精度和半径的首个benchmark，并超越了现有的synonym替换攻击证明。”
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Causal-Bayesian-Optimization"><a href="#Adversarial-Causal-Bayesian-Optimization" class="headerlink" title="Adversarial Causal Bayesian Optimization"></a>Adversarial Causal Bayesian Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16625">http://arxiv.org/abs/2307.16625</a></li>
<li>repo_url: None</li>
<li>paper_authors: Scott Sussex, Pier Giuseppe Sessa, Anastasiia Makarova, Andreas Krause</li>
<li>For: The paper is written for optimizing a downstream reward variable in the presence of adversarial interventions on an unknown structural causal model.* Methods: The paper introduces the first algorithm for Adversarial Causal Bayesian Optimization (ACBO) with bounded regret, called Causal Bayesian Optimization with Multiplicative Weights (CBO-MW). The approach combines classical online learning strategies with causal modeling of the rewards, using optimistic counterfactual reward estimates propagated through the causal graph.* Results: The paper derives regret bounds for CBO-MW that naturally depend on graph-related quantities, and proposes a scalable implementation for the case of combinatorial interventions and submodular rewards. Empirical results show that CBO-MW outperforms non-causal and non-adversarial Bayesian optimization methods on synthetic environments and real-world data-based environments.<details>
<summary>Abstract</summary>
In Causal Bayesian Optimization (CBO), an agent intervenes on an unknown structural causal model to maximize a downstream reward variable. In this paper, we consider the generalization where other agents or external events also intervene on the system, which is key for enabling adaptiveness to non-stationarities such as weather changes, market forces, or adversaries. We formalize this generalization of CBO as Adversarial Causal Bayesian Optimization (ACBO) and introduce the first algorithm for ACBO with bounded regret: Causal Bayesian Optimization with Multiplicative Weights (CBO-MW). Our approach combines a classical online learning strategy with causal modeling of the rewards. To achieve this, it computes optimistic counterfactual reward estimates by propagating uncertainty through the causal graph. We derive regret bounds for CBO-MW that naturally depend on graph-related quantities. We further propose a scalable implementation for the case of combinatorial interventions and submodular rewards. Empirically, CBO-MW outperforms non-causal and non-adversarial Bayesian optimization methods on synthetic environments and environments based on real-word data. Our experiments include a realistic demonstration of how CBO-MW can be used to learn users' demand patterns in a shared mobility system and reposition vehicles in strategic areas.
</details>
<details>
<summary>摘要</summary>
在 causal bayesian 优化（CBO）中, an agent  intervenes on an unknown 结构 causal 模型以 Maximize  downstream 奖 variable. 在这篇 paper 中, we consider 这个泛化, where other agents or external events 也 intervene on the system, which is key for enabling adaptiveness to non-stationarities such as weather changes, market forces, or adversaries. We formalize this generalization of CBO as Adversarial Causal Bayesian Optimization (ACBO) and introduce the first algorithm for ACBO with bounded regret: Causal Bayesian Optimization with Multiplicative Weights (CBO-MW). Our approach combines a classical online learning strategy with causal modeling of the rewards. To achieve this, it computes optimistic counterfactual reward estimates by propagating uncertainty through the causal graph. We derive regret bounds for CBO-MW that naturally depend on graph-related quantities. We further propose a scalable implementation for the case of combinatorial interventions and submodular rewards. Empirically, CBO-MW outperforms non-causal and non-adversarial Bayesian optimization methods on synthetic environments and environments based on real-word data. Our experiments include a realistic demonstration of how CBO-MW can be used to learn users' demand patterns in a shared mobility system and reposition vehicles in strategic areas.Here's the translation in Traditional Chinese:在 causal bayesian 优化（CBO）中, an agent  intervenes on an unknown 结构 causal 模型以 Maximize  downstream 奖 variable. 在这篇 paper 中, we consider 这个泛化, where other agents or external events 也 intervene on the system, which is key for enabling adaptiveness to non-stationarities such as weather changes, market forces, or adversaries. We formalize this generalization of CBO as Adversarial Causal Bayesian Optimization (ACBO) and introduce the first algorithm for ACBO with bounded regret: Causal Bayesian Optimization with Multiplicative Weights (CBO-MW). Our approach combines a classical online learning strategy with causal modeling of the rewards. To achieve this, it computes optimistic counterfactual reward estimates by propagating uncertainty through the causal graph. We derive regret bounds for CBO-MW that naturally depend on graph-related quantities. We further propose a scalable implementation for the case of combinatorial interventions and submodular rewards. Empirically, CBO-MW outperforms non-causal and non-adversarial Bayesian optimization methods on synthetic environments and environments based on real-word data. Our experiments include a realistic demonstration of how CBO-MW can be used to learn users' demand patterns in a shared mobility system and reposition vehicles in strategic areas.
</details></li>
</ul>
<hr>
<h2 id="Detecting-diabetic-retinopathy-severity-through-fundus-images-using-an-ensemble-of-classifiers"><a href="#Detecting-diabetic-retinopathy-severity-through-fundus-images-using-an-ensemble-of-classifiers" class="headerlink" title="Detecting diabetic retinopathy severity through fundus images using an ensemble of classifiers"></a>Detecting diabetic retinopathy severity through fundus images using an ensemble of classifiers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16622">http://arxiv.org/abs/2307.16622</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eduard Popescu, Adrian Groza, Ioana Damian</li>
<li>for: 这个论文是用来检测糖尿病肠病严重程度的方法。</li>
<li>methods: 这个方法包括数据预处理、图像分割、特征提取和 ensemble 分类。</li>
<li>results: 这个方法可以准确地检测糖尿病肠病的严重程度。Here’s the full summary in Simplified Chinese:</li>
<li>for: 这个论文是用来检测糖尿病肠病严重程度的方法。</li>
<li>methods: 这个方法包括数据预处理、图像分割、特征提取和 ensemble 分类。</li>
<li>results: 这个方法可以准确地检测糖尿病肠病的严重程度。<details>
<summary>Abstract</summary>
Diabetic retinopathy is an ocular condition that affects individuals with diabetes mellitus. It is a common complication of diabetes that can impact the eyes and lead to vision loss. One method for diagnosing diabetic retinopathy is the examination of the fundus of the eye. An ophthalmologist examines the back part of the eye, including the retina, optic nerve, and the blood vessels that supply the retina. In the case of diabetic retinopathy, the blood vessels in the retina deteriorate and can lead to bleeding, swelling, and other changes that affect vision. We proposed a method for detecting diabetic diabetic severity levels. First, a set of data-prerpocessing is applied to available data: adaptive equalisation, color normalisation, Gaussian filter, removal of the optic disc and blood vessels. Second, we perform image segmentation for relevant markers and extract features from the fundus images. Third, we apply an ensemble of classifiers and we assess the trust in the system.
</details>
<details>
<summary>摘要</summary>
糖尿病Retinopathy是一种眼部疾病，影响糖尿病患者。它是糖尿病的常见侵化，可以影响视力。一种用于诊断糖尿病Retinopathy的方法是对眼部背部进行检查，包括肠视硬化、血管和视神经。在糖尿病Retinopathy中，肠视硬化导致血管在肠部受损，可能导致出血、肿胀和其他影响视力的变化。我们提出了一种方法来评估糖尿病严重程度。首先，我们对可用数据进行数据处理：适应性平衡、颜色normal化、Gaussian滤波器和 removing the optic disc and blood vessels。第二，我们实现图像分割，检测 relevante markers并提取fundus图像中的特征。第三，我们应用一个 ensemble of classifiers，并评估系统的可信度。
</details></li>
</ul>
<hr>
<h2 id="LaplaceConfidence-a-Graph-based-Approach-for-Learning-with-Noisy-Labels"><a href="#LaplaceConfidence-a-Graph-based-Approach-for-Learning-with-Noisy-Labels" class="headerlink" title="LaplaceConfidence: a Graph-based Approach for Learning with Noisy Labels"></a>LaplaceConfidence: a Graph-based Approach for Learning with Noisy Labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16614">http://arxiv.org/abs/2307.16614</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingcai Chen, Yuntao Du, Wei Tang, Baoming Zhang, Hao Cheng, Shuwei Qian, Chongjun Wang</li>
<li>for: 本研究旨在开发一种可靠地处理噪声标签的机器学习算法，以便在实际应用中使用。</li>
<li>methods: 本研究使用 LaplaceConfidence 方法，它根据数据中的特征表示生成图，然后使用 Laplacian 能量来计算样本的净概率。</li>
<li>results: 实验表明，LaplaceConfidence 方法在 benchmark 数据集上比前一代方法更高效，并且在实际噪声数据上也显示出良好的性能。<details>
<summary>Abstract</summary>
In real-world applications, perfect labels are rarely available, making it challenging to develop robust machine learning algorithms that can handle noisy labels. Recent methods have focused on filtering noise based on the discrepancy between model predictions and given noisy labels, assuming that samples with small classification losses are clean. This work takes a different approach by leveraging the consistency between the learned model and the entire noisy dataset using the rich representational and topological information in the data. We introduce LaplaceConfidence, a method that to obtain label confidence (i.e., clean probabilities) utilizing the Laplacian energy. Specifically, it first constructs graphs based on the feature representations of all noisy samples and minimizes the Laplacian energy to produce a low-energy graph. Clean labels should fit well into the low-energy graph while noisy ones should not, allowing our method to determine data's clean probabilities. Furthermore, LaplaceConfidence is embedded into a holistic method for robust training, where co-training technique generates unbiased label confidence and label refurbishment technique better utilizes it. We also explore the dimensionality reduction technique to accommodate our method on large-scale noisy datasets. Our experiments demonstrate that LaplaceConfidence outperforms state-of-the-art methods on benchmark datasets under both synthetic and real-world noise.
</details>
<details>
<summary>摘要</summary>
在实际应用中，完美的标签很少，这使得开发机器学习算法可以处理噪声的挑战变得更大。现有方法是根据模型预测和噪声标签的差异来过滤噪声，假设样本的分类损失小于就是干净的。这个工作采用了不同的方法，利用数据中学习模型和整个噪声数据集的质量信息来确定数据的干净概率。我们引入了LaplaceConfidence方法，它使用Laplacian能量来获取标签信任度（即干净概率）。具体来说，它首先基于所有噪声样本的特征表示构建图，然后使用Laplacian能量来生成低能图。干净标签应该适合低能图中，而噪声标签不应该。这样，LaplaceConfidence方法就可以确定数据的干净概率。此外，LaplaceConfidence方法被 embedding 到了一种抗耗整合方法中，其中使用 co-training 技术生成不偏的标签信任度，并使用标签修复技术更好地利用它。我们还探索了维度减少技术，以便在大规模噪声数据集上使用我们的方法。我们的实验表明，LaplaceConfidence 方法在标准 benchmark 数据集上比状态艺术方法表现更好，并且在实际噪声中也表现出色。
</details></li>
</ul>
<hr>
<h2 id="Noisy-Self-Training-with-Data-Augmentations-for-Offensive-and-Hate-Speech-Detection-Tasks"><a href="#Noisy-Self-Training-with-Data-Augmentations-for-Offensive-and-Hate-Speech-Detection-Tasks" class="headerlink" title="Noisy Self-Training with Data Augmentations for Offensive and Hate Speech Detection Tasks"></a>Noisy Self-Training with Data Augmentations for Offensive and Hate Speech Detection Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16609">http://arxiv.org/abs/2307.16609</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jaugusto97/offense-self-training">https://github.com/jaugusto97/offense-self-training</a></li>
<li>paper_authors: João A. Leite, Carolina Scarton, Diego F. Silva</li>
<li>for: 自动侦测和纠正社交媒体上的不宽容和仇恨言论，以提高社交媒体上的内容质量。</li>
<li>methods: 使用弱 labels 的自我训练方法，使用不宽容和仇恨言论的训练数据，并使用文本数据增强技术来提高预测精度。</li>
<li>results: 显示了自我训练可以提高表现，并且显示了不同于预设方法的“噪音”自我训练方法可以对于不宽容和仇恨言论的预测精度产生负面影响。<details>
<summary>Abstract</summary>
Online social media is rife with offensive and hateful comments, prompting the need for their automatic detection given the sheer amount of posts created every second. Creating high-quality human-labelled datasets for this task is difficult and costly, especially because non-offensive posts are significantly more frequent than offensive ones. However, unlabelled data is abundant, easier, and cheaper to obtain. In this scenario, self-training methods, using weakly-labelled examples to increase the amount of training data, can be employed. Recent "noisy" self-training approaches incorporate data augmentation techniques to ensure prediction consistency and increase robustness against noisy data and adversarial attacks. In this paper, we experiment with default and noisy self-training using three different textual data augmentation techniques across five different pre-trained BERT architectures varying in size. We evaluate our experiments on two offensive/hate-speech datasets and demonstrate that (i) self-training consistently improves performance regardless of model size, resulting in up to +1.5% F1-macro on both datasets, and (ii) noisy self-training with textual data augmentations, despite being successfully applied in similar settings, decreases performance on offensive and hate-speech domains when compared to the default method, even with state-of-the-art augmentations such as backtranslation.
</details>
<details>
<summary>摘要</summary>
在在线社交媒体中，有很多侮辱和仇恨的评论，需要自动检测这些评论的存在，因为每秒钟创建的帖子的数量非常大。创建高质量的人类标注数据集非常困难和昂贵，特别是非侮辱帖子的数量非常多。然而，无标注数据却很易于获得，更加便宜。在这种情况下，可以使用自我教育方法，使用弱标注的示例来增加训练数据的量。最新的“噪音”自我教育方法将数据扩展技术与预训练模型结合，以确保预测的一致性和对噪音数据和敌意攻击的鲁棒性。在这篇论文中，我们对五种不同的预训练BERT架构进行了Default和噪音自我教育的实验，并使用三种文本数据扩展技术。我们对两个侮辱和仇恨域上的两个 dataset进行了评估，结果显示：（i）自我教育可以不anni标注数据大小不同的模型中提高性能，最高提高+1.5% F1-macro，（ii）噪音自我教育，尽管在类似场景中得到了成功，在侮辱和仇恨域上的性能下降，即使使用最新的扩展技术，如回 перевод。
</details></li>
</ul>
<hr>
<h2 id="NLLG-Quarterly-arXiv-Report-06-23-What-are-the-most-influential-current-AI-Papers"><a href="#NLLG-Quarterly-arXiv-Report-06-23-What-are-the-most-influential-current-AI-Papers" class="headerlink" title="NLLG Quarterly arXiv Report 06&#x2F;23: What are the most influential current AI Papers?"></a>NLLG Quarterly arXiv Report 06&#x2F;23: What are the most influential current AI Papers?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04889">http://arxiv.org/abs/2308.04889</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nl2g/quaterly-arxiv">https://github.com/nl2g/quaterly-arxiv</a></li>
<li>paper_authors: Steffen Eger, Christoph Leiter, Jonas Belouadi, Ran Zhang, Aida Kostikova, Daniil Larionov, Yanran Chen, Vivian Fresen</li>
<li>For: This paper aims to provide a quick guide to the most relevant and widely discussed research in the field of Generative AI, with a focus on NLP and ML.* Methods: The paper uses normalized citation counts from the first half of 2023 to identify the 40 most popular papers in the field, and examines the characteristics of these papers, such as their focus on LLMs and ethical considerations.* Results: The paper finds that NLP-related papers are the most influential (around 60% of top papers), and that LLM efficiency, evaluation techniques, and problem-solving with LLMs are core issues investigated in the most heavily cited papers.Here is the information in Simplified Chinese text:</li>
<li>for: 这篇报告目的是为了提供generative AI领域，特别是自然语言处理（NLP）和机器学习（ML）领域最新发展的快速导航。</li>
<li>methods: 该报告使用2023年第一半年的normalized citation counts来确定领域中最受欢迎的40篇论文，并分析这些论文的特点，如它们的关注点是LLMs和伦理考虑。</li>
<li>results: 报告发现，NLP相关的论文是最有影响力的（约60%的推荐篇），并且发现LLM效率、评价技术、伦理考虑和embodied agents等核心问题在最受欢迎的论文中占据主导地位。<details>
<summary>Abstract</summary>
The rapid growth of information in the field of Generative Artificial Intelligence (AI), particularly in the subfields of Natural Language Processing (NLP) and Machine Learning (ML), presents a significant challenge for researchers and practitioners to keep pace with the latest developments. To address the problem of information overload, this report by the Natural Language Learning Group at Bielefeld University focuses on identifying the most popular papers on arXiv, with a specific emphasis on NLP and ML. The objective is to offer a quick guide to the most relevant and widely discussed research, aiding both newcomers and established researchers in staying abreast of current trends. In particular, we compile a list of the 40 most popular papers based on normalized citation counts from the first half of 2023. We observe the dominance of papers related to Large Language Models (LLMs) and specifically ChatGPT during the first half of 2023, with the latter showing signs of declining popularity more recently, however. Further, NLP related papers are the most influential (around 60\% of top papers) even though there are twice as many ML related papers in our data. Core issues investigated in the most heavily cited papers are: LLM efficiency, evaluation techniques, ethical considerations, embodied agents, and problem-solving with LLMs. Additionally, we examine the characteristics of top papers in comparison to others outside the top-40 list (noticing the top paper's focus on LLM related issues and higher number of co-authors) and analyze the citation distributions in our dataset, among others.
</details>
<details>
<summary>摘要</summary>
随着生成人工智能（AI）领域的信息快速增长，特别是自然语言处理（NLP）和机器学习（ML）的子领域，研究人员和实践者面临着保持最新发展的挑战。为了解决信息淹领的问题，这份报告由比萨大学自然语言学习组编写，关注arXiv上最受欢迎的论文，尤其是NLP和ML领域。旨在为新手和已有研究人员提供快速引导，了解当前趋势。我们根据2023年first半年的normalized引用数量编辑出40篇最受欢迎的论文，并观察到2023年first半年LLMs（大语言模型）和ChatGPT的占据率逐渐下降，但NLP相关论文仍占约60%的排名。核心问题在最引导着欢迎的论文中被研究包括：LLM效率、评估技术、伦理考虑、具体代理人和问题解决方法。此外，我们还分析了排名前40篇论文的特点，以及总体的引用分布。
</details></li>
</ul>
<hr>
<h2 id="Audio-visual-video-to-speech-synthesis-with-synthesized-input-audio"><a href="#Audio-visual-video-to-speech-synthesis-with-synthesized-input-audio" class="headerlink" title="Audio-visual video-to-speech synthesis with synthesized input audio"></a>Audio-visual video-to-speech synthesis with synthesized input audio</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16584">http://arxiv.org/abs/2307.16584</a></li>
<li>repo_url: None</li>
<li>paper_authors: Triantafyllos Kefalas, Yannis Panagakis, Maja Pantic</li>
<li>for: 这个论文的目的是研究视频到语音合成中使用视频和声音输入的效果。</li>
<li>methods: 这个论文使用预训练的视频到语音模型来生成缺失的语音信号，然后使用视频和生成的语音作为输入，训练一个视频-声音-到语音合成模型来预测最终重建的语音。</li>
<li>results: 实验表明，这种方法在使用原始波形和мельспектрограм作为目标输出时都是成功的。<details>
<summary>Abstract</summary>
Video-to-speech synthesis involves reconstructing the speech signal of a speaker from a silent video. The implicit assumption of this task is that the sound signal is either missing or contains a high amount of noise/corruption such that it is not useful for processing. Previous works in the literature either use video inputs only or employ both video and audio inputs during training, and discard the input audio pathway during inference. In this work we investigate the effect of using video and audio inputs for video-to-speech synthesis during both training and inference. In particular, we use pre-trained video-to-speech models to synthesize the missing speech signals and then train an audio-visual-to-speech synthesis model, using both the silent video and the synthesized speech as inputs, to predict the final reconstructed speech. Our experiments demonstrate that this approach is successful with both raw waveforms and mel spectrograms as target outputs.
</details>
<details>
<summary>摘要</summary>
视频到语音合成包括从无声视频中重建说话人的语音信号。这个隐式假设是，音频信号缺失或受到干扰或损害，因此不能用于处理。先前的文献中的工作都是在训练时使用视频输入，并在推断时丢弃音频输入。在这个工作中，我们研究了在训练和推断时都使用视频和音频输入的影响。具体来说，我们使用预训练的视频到语音模型来重建缺失的语音信号，然后使用视频和重建的语音作为输入，使用一个audiovisual-to-speech合成模型来预测最终重建的语音。我们的实验结果表明，这种方法在raw波形和mel spectrogram作为目标输出时都是成功的。
</details></li>
</ul>
<hr>
<h2 id="A-multiscale-and-multicriteria-Generative-Adversarial-Network-to-synthesize-1-dimensional-turbulent-fields"><a href="#A-multiscale-and-multicriteria-Generative-Adversarial-Network-to-synthesize-1-dimensional-turbulent-fields" class="headerlink" title="A multiscale and multicriteria Generative Adversarial Network to synthesize 1-dimensional turbulent fields"></a>A multiscale and multicriteria Generative Adversarial Network to synthesize 1-dimensional turbulent fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16580">http://arxiv.org/abs/2307.16580</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carlos Granero-Belinchon, Manuel Cabeza Gallucci</li>
<li>for: 这篇论文旨在开发一种基于神经网络的浮动随机场模型，用于生成具有湍流速度统计特征的1维随机场。</li>
<li>methods: 该模型采用了生成对抗网络，基于科尔мого罗夫和奥布霍夫的统计理论，以确保与实验观察结果相符的1)能量分布、2)能量升降和3)不均匀性随着尺度的描述。</li>
<li>results: 模型可以在不同的尺度范围内生成具有高准确性和均匀性的浮动随机场，并且可以捕捉到实验观察到的湍流速度统计特征。<details>
<summary>Abstract</summary>
This article introduces a new Neural Network stochastic model to generate a 1-dimensional stochastic field with turbulent velocity statistics. Both the model architecture and training procedure ground on the Kolmogorov and Obukhov statistical theories of fully developed turbulence, so guaranteeing descriptions of 1) energy distribution, 2) energy cascade and 3) intermittency across scales in agreement with experimental observations. The model is a Generative Adversarial Network with multiple multiscale optimization criteria. First, we use three physics-based criteria: the variance, skewness and flatness of the increments of the generated field that retrieve respectively the turbulent energy distribution, energy cascade and intermittency across scales. Second, the Generative Adversarial Network criterion, based on reproducing statistical distributions, is used on segments of different length of the generated field. Furthermore, to mimic multiscale decompositions frequently used in turbulence's studies, the model architecture is fully convolutional with kernel sizes varying along the multiple layers of the model. To train our model we use turbulent velocity signals from grid turbulence at Modane wind tunnel.
</details>
<details>
<summary>摘要</summary>
First, the model uses three physics-based criteria: the variance, skewness, and flatness of the increments of the generated field, which respectively retrieve the turbulent energy distribution, energy cascade, and intermittency across scales. Additionally, the GAN criterion is based on reproducing statistical distributions and is used on segments of different length of the generated field.To mimic multiscale decompositions frequently used in turbulence studies, the model architecture is fully convolutional with kernel sizes varying along the multiple layers of the model. The training data is turbulent velocity signals from grid turbulence at Modane wind tunnel.
</details></li>
</ul>
<hr>
<h2 id="The-Decimation-Scheme-for-Symmetric-Matrix-Factorization"><a href="#The-Decimation-Scheme-for-Symmetric-Matrix-Factorization" class="headerlink" title="The Decimation Scheme for Symmetric Matrix Factorization"></a>The Decimation Scheme for Symmetric Matrix Factorization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16564">http://arxiv.org/abs/2307.16564</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francesco Camilli, Marc Mézard</li>
<li>for: 本文研究了矩阵因子化的推理问题，它在字典学习、推荐系统和深度学习中具有广泛的应用。研究这个问题的基本统计限制是一项十年来社区努力的挑战，而且在矩阵级别 Linear 时仍然没有固定的关键。</li>
<li>methods: 本文使用了一种名为”减少”的方法，它通过将问题映射到一系列的神经网络模型，以实现一次一列&#x2F;一行的因子 recover。尽管这不是最优的方法，但它有优点，即可以进行理论分析。本文将这种方法扩展到两个家族的矩阵上，并对这些矩阵进行了广泛的研究。</li>
<li>results: 对于一类具有紧密支持的先验，我们显示了低温下的复制同步自由能的universal形式。对于稀疏伊辛先验，我们发现了因子化矩阵的存储容量与稀疏性的增长关系，并提出了一种简单的算法，基于基准搜索，实现了减少和矩阵因子化，无需具有有用的初始化。<details>
<summary>Abstract</summary>
Matrix factorization is an inference problem that has acquired importance due to its vast range of applications that go from dictionary learning to recommendation systems and machine learning with deep networks. The study of its fundamental statistical limits represents a true challenge, and despite a decade-long history of efforts in the community, there is still no closed formula able to describe its optimal performances in the case where the rank of the matrix scales linearly with its size. In the present paper, we study this extensive rank problem, extending the alternative 'decimation' procedure that we recently introduced, and carry out a thorough study of its performance. Decimation aims at recovering one column/line of the factors at a time, by mapping the problem into a sequence of neural network models of associative memory at a tunable temperature. Though being sub-optimal, decimation has the advantage of being theoretically analyzable. We extend its scope and analysis to two families of matrices. For a large class of compactly supported priors, we show that the replica symmetric free entropy of the neural network models takes a universal form in the low temperature limit. For sparse Ising prior, we show that the storage capacity of the neural network models diverges as sparsity in the patterns increases, and we introduce a simple algorithm based on a ground state search that implements decimation and performs matrix factorization, with no need of an informative initialization.
</details>
<details>
<summary>摘要</summary>
矩阵分解是一个重要的推理问题，它在从词库学到推荐系统和深度学习中都有广泛的应用。研究其基本统计限制是一项挑战，尽管社区在过去的十年里有努力，仍没有能描述其最佳性能的关闭公式，尤其是当矩阵矩阵的排名与其大小成线性关系时。在 presente 纸上，我们研究这个扩展的排名问题，扩展我们最近提出的代替方法“减少”，并进行了系统的研究。减少方法是通过将问题映射到一系列神经网络模型，以一个可调温度的温度学习 associate memory 的方式来重建一列/一行的因子。虽然不是最佳的方法，但减少方法具有理论分析可行性。我们将其扩展到两个家族的矩阵上，并对这些矩阵进行了严格的分析。对于一个广泛的紧支持矩阵，我们显示了低温限下的自由Entropy 的 neural network 模型具有通用的形式。对于稀疏的 Иссинг 矩阵，我们显示了因子的存储容量在模式的稀疏程度增加时增加，并引入了一种简单的算法，基于地面搜索，实现减少和矩阵分解，无需具有有用的初始化。
</details></li>
</ul>
<hr>
<h2 id="Line-Search-for-Convex-Minimization"><a href="#Line-Search-for-Convex-Minimization" class="headerlink" title="Line Search for Convex Minimization"></a>Line Search for Convex Minimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16560">http://arxiv.org/abs/2307.16560</a></li>
<li>repo_url: None</li>
<li>paper_authors: Laurent Orseau, Marcus Hutter<br>for:* The paper is written for minimizing quasiconvex (unimodal) functions, and more generally, convex functions.methods:* The paper proposes two principled exact line search algorithms for general convex functions, called $\Delta$-Bisection and $\Delta$-Secant, which take advantage of convexity.* The algorithms use (sub)gradient information and function queries to speed up convergence.results:* The paper shows that the proposed algorithms are faster than their quasiconvex counterparts, often by more than a factor 2, and provide a refined stopping criterion.* The algorithms are tested on a few convex functions and show good performance.Here is the result in Simplified Chinese text:for:* 本文是为了最小化一元函数的 quiticonvex（单峰）函数，以及更一般的凸函数。methods:* 本文提出了两种原理正确的一元线搜索算法，即 $\Delta$-Bisection 和 $\Delta$-Secant，它们利用凸性来加速收敛。* 这些算法使用（子）导函数信息和函数查询来加速收敛。results:* 本文表明了提出的算法比其 quiticonvex 对应算法更快，通常高于2倍，并提供了一个精细的停止标准。* 算法在一些凸函数上进行了测试，显示良好的性能。<details>
<summary>Abstract</summary>
Golden-section search and bisection search are the two main principled algorithms for 1d minimization of quasiconvex (unimodal) functions. The first one only uses function queries, while the second one also uses gradient queries. Other algorithms exist under much stronger assumptions, such as Newton's method. However, to the best of our knowledge, there is no principled exact line search algorithm for general convex functions -- including piecewise-linear and max-compositions of convex functions -- that takes advantage of convexity. We propose two such algorithms: $\Delta$-Bisection is a variant of bisection search that uses (sub)gradient information and convexity to speed up convergence, while $\Delta$-Secant is a variant of golden-section search and uses only function queries.   While bisection search reduces the $x$ interval by a factor 2 at every iteration, $\Delta$-Bisection reduces the (sometimes much) smaller $x^*$-gap $\Delta^x$ (the $x$ coordinates of $\Delta$) by at least a factor 2 at every iteration. Similarly, $\Delta$-Secant also reduces the $x^*$-gap by at least a factor 2 every second function query. Moreover, the $y^*$-gap $\Delta^y$ (the $y$ coordinates of $\Delta$) also provides a refined stopping criterion, which can also be used with other algorithms. Experiments on a few convex functions confirm that our algorithms are always faster than their quasiconvex counterparts, often by more than a factor 2.   We further design a quasi-exact line search algorithm based on $\Delta$-Secant. It can be used with gradient descent as a replacement for backtracking line search, for which some parameters can be finicky to tune -- and we provide examples to this effect, on strongly-convex and smooth functions. We provide convergence guarantees, and confirm the efficiency of quasi-exact line search on a few single- and multivariate convex functions.
</details>
<details>
<summary>摘要</summary>
金叶搜索和bisect搜索是1D最小化非线性函数的两种主要原则性算法。前者只使用函数查询，而后者还使用梯度查询。其他算法存在更加强的假设，如新颖方法。然而，我们所知道的最佳精确直线搜索算法 для总体凸函数，包括分割线性和最大 compositions of 凸函数，是否存在。我们提出了两种算法：Δ-bisect是一种使用梯度信息和凸性加速收敛的bisect搜索变体，而Δ-Secant是一种使用函数查询的 golden-section搜索变体。在每次迭代中，bisect搜索将xInterval分割为一半，而Δ-bisect将(可能非常小)的x* gap Δ^x 分割为至少一半。相似地，Δ-Secant每次查询函数时也将x* gap 分割为至少一半。此外，y* gap Δ^y (y坐标的Delta) 还提供了一种精细的停止条件，可以与其他算法一起使用。我们的算法在一些凸函数上进行了实验，并证明它们在许多情况下比其 quariconvex 版本快速，常常高于2倍。我们还设计了一种 quasi-exact line search algorithm，基于Δ-Secant。它可以与梯度下降作为替换backtracking line search，并且可以在某些参数上进行微调。我们提供了一些示例，包括强拟合和光滑函数。我们提供了收敛保证，并在一些单变量和多变量凸函数上进行了确认。
</details></li>
</ul>
<hr>
<h2 id="Simultaneous-column-based-deep-learning-progression-analysis-of-atrophy-associated-with-AMD-in-longitudinal-OCT-studies"><a href="#Simultaneous-column-based-deep-learning-progression-analysis-of-atrophy-associated-with-AMD-in-longitudinal-OCT-studies" class="headerlink" title="Simultaneous column-based deep learning progression analysis of atrophy associated with AMD in longitudinal OCT studies"></a>Simultaneous column-based deep learning progression analysis of atrophy associated with AMD in longitudinal OCT studies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16559">http://arxiv.org/abs/2307.16559</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adi Szeskin, Roei Yehuda, Or Shmueli, Jaime Levy, Leo Joskowicz</li>
<li>for: The paper is written to accurately quantify retinal atrophy changes associated with dry age-related macular degeneration (AMD) on longitudinal OCT studies.</li>
<li>methods: The paper presents a fully automatic end-to-end pipeline for simultaneously detecting and quantifying time-related atrophy changes in pairs of OCT scans of a patient. The pipeline uses a novel simultaneous multi-channel column-based deep learning model that classifies light scattering patterns in matched pairs of vertical pixel-wide columns (A-scans) in registered prior and current OCT slices (B-scans).</li>
<li>results: The experimental results on 4,040 OCT slices with 5.2M columns from 40 scan pairs of 18 patients show a mean atrophy segments detection precision of 0.90+-0.09, 0.95+-0.06, and 0.74+-0.18, 0.94+-0.12 for atrophy lesions, with an AUC of 0.897, outperforming standalone classification methods by 30+-62% and 27+-0% for atrophy segments and lesions.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文目的是准确量化普通病理性膜脱屑病（AMD）的长期 OCT 图像中的膜脱屑变化。</li>
<li>methods: 这篇论文提出了一种完全自动的端到端管道，用于同时检测和量化 consecutive OCT 图像中的时间相关的膜脱屑变化。该管道使用了一种新的同时多通道列式深度学习模型，该模型在注册后 OCT 剖面（B-scan）中匹配的 vertical 像素宽柱（A-scan）中类别光散射模式，以实现同时检测和分割 consecutive OCT 图像中的膜脱屑部分。</li>
<li>results: 实验结果表明，使用了 4,040 个 OCT 剖面和 5.2 万个列的 40 个扫描组合，从 18 名患者（66% 训练&#x2F;验证，33% 测试）中获得了 24.13 ± 14.0 个月的间隔。Complete RPE 和 Outer Retinal Atrophy（cRORA）在 1,998 个 OCT 剖面中被识别出，其中 735 个膜脱屑病变（0.45 万个列），并且获得了 mean atrophy segments 检测精度、回归为 0.90 ± 0.09、0.95 ± 0.06 和 0.74 ± 0.18、0.94 ± 0.12，同时 Simultaneous classification 方法在 atrophy segments 和 lesions 中的检测精度和回归性分别高于 standalone classification 方法 by 30 ± 62% 和 27 ± 0%。<details>
<summary>Abstract</summary>
Purpose: Disease progression of retinal atrophy associated with AMD requires the accurate quantification of the retinal atrophy changes on longitudinal OCT studies. It is based on finding, comparing, and delineating subtle atrophy changes on consecutive pairs (prior and current) of unregistered OCT scans. Methods: We present a fully automatic end-to-end pipeline for the simultaneous detection and quantification of time-related atrophy changes associated with dry AMD in pairs of OCT scans of a patient. It uses a novel simultaneous multi-channel column-based deep learning model trained on registered pairs of OCT scans that concurrently detects and segments retinal atrophy segments in consecutive OCT scans by classifying light scattering patterns in matched pairs of vertical pixel-wide columns (A-scans) in registered prior and current OCT slices (B-scans). Results: Experimental results on 4,040 OCT slices with 5.2M columns from 40 scans pairs of 18 patients (66% training/validation, 33% testing) with 24.13+-14.0 months apart in which Complete RPE and Outer Retinal Atrophy (cRORA) was identified in 1,998 OCT slices (735 atrophy lesions from 3,732 segments, 0.45M columns) yield a mean atrophy segments detection precision, recall of 0.90+-0.09, 0.95+-0.06 and 0.74+-0.18, 0.94+-0.12 for atrophy lesions with AUC=0.897, all above observer variability. Simultaneous classification outperforms standalone classification precision and recall by 30+-62% and 27+-0% for atrophy segments and lesions. Conclusions: simultaneous column-based detection and quantification of retinal atrophy changes associated with AMD is accurate and outperforms standalone classification methods. Translational relevance: an automatic and efficient way to detect and quantify retinal atrophy changes associated with AMD.
</details>
<details>
<summary>摘要</summary>
目的： aged-related macular degeneration（AMD）相关Retinal atrophy的疾病进程需要精准量化 consecutive OCT 图像中的变化。这是通过发现、比较和定义潜在的变化而实现的。方法：我们提出了一个完全自动的终端到终点管道，用于同时检测和量化患者 consecutive OCT 图像中的时间相关的衰竭变化。该管道使用了一种新的同时多通道Column-based深度学习模型，该模型在已经注册的 prior 和 current OCT 图像中同时检测和分割Retinal atrophy 分段。结果：我们对 4,040 个 OCT 图像（5.2M 列）进行实验，其中有 40 组 OCT 图像，每组图像间隔 24.13 ± 14.0 个月。在这些图像中，我们发现了 Complete RPE 和 Outer Retinal Atrophy（cRORA），并且在 1,998 个 OCT 图像中检测到了 735 个衰竭 lesions，其中 0.45M 列。我们的方法在这些图像中得到了 mean 衰竭分段检测精度和 recall 的 0.90 ± 0.09、0.95 ± 0.06 和 0.74 ± 0.18、0.94 ± 0.12，AUC = 0.897，全部超过了观察者变化。同时的分类方法也超过了单独的分类精度和 recall。结论：同时检测和量化 Retinal atrophy 变化与 AMD 相关的疾病进程是准确的，并且超过了单独的分类方法。翻译意义：一种自动和高效的方法，可以帮助检测和量化 Retinal atrophy 变化与 AMD 相关的疾病进程。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-and-Computer-Vision-for-Glaucoma-Detection-A-Review"><a href="#Deep-Learning-and-Computer-Vision-for-Glaucoma-Detection-A-Review" class="headerlink" title="Deep Learning and Computer Vision for Glaucoma Detection: A Review"></a>Deep Learning and Computer Vision for Glaucoma Detection: A Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16528">http://arxiv.org/abs/2307.16528</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mona Ashtari-Majlan, Mohammad Mahdi Dehshibi, David Masip<br>for:This paper aims to provide a comprehensive survey of recent studies on AI-based glaucoma diagnosis using fundus, optical coherence tomography, and visual field images.methods:The paper focuses on deep learning-based methods for glaucoma diagnosis and provides an updated taxonomy that organizes these methods into architectural paradigms. The authors also provide links to available source code to enhance the reproducibility of the methods.results:The authors conduct rigorous benchmarking on widely-used public datasets and reveal performance gaps in generalizability, uncertainty estimation, and multimodal integration. They also curate key datasets and highlight limitations such as scale, labeling inconsistencies, and bias.Here is the same information in Simplified Chinese text:for:这篇论文的目的是为了提供最近的AI基于fundus、optical coherence tomography和视场图像的глау科病诊断研究的综述。methods:这篇论文专注于基于深度学习的глау科诊断方法，并提供了一个更新的分类法，将这些方法分为 arquitectural paradigms。作者还提供了可重复性的源代码链接。results:作者通过在广泛使用的公共数据集上进行严格的benchmarking，揭示了总结、不确定性估计和多模态融合的性能差。他们还综述了关键的数据集，并指出了标注不一致、scale和偏见等限制。<details>
<summary>Abstract</summary>
Glaucoma is the leading cause of irreversible blindness worldwide and poses significant diagnostic challenges due to its reliance on subjective evaluation. However, recent advances in computer vision and deep learning have demonstrated the potential for automated assessment. In this paper, we survey recent studies on AI-based glaucoma diagnosis using fundus, optical coherence tomography, and visual field images, with a particular emphasis on deep learning-based methods. We provide an updated taxonomy that organizes methods into architectural paradigms and includes links to available source code to enhance the reproducibility of the methods. Through rigorous benchmarking on widely-used public datasets, we reveal performance gaps in generalizability, uncertainty estimation, and multimodal integration. Additionally, our survey curates key datasets while highlighting limitations such as scale, labeling inconsistencies, and bias. We outline open research challenges and detail promising directions for future studies. This survey is expected to be useful for both AI researchers seeking to translate advances into practice and ophthalmologists aiming to improve clinical workflows and diagnosis using the latest AI outcomes.
</details>
<details>
<summary>摘要</summary>
Glaucoma 是全球最主要的不可逆失明病种，但它的诊断却存在许多Subjective 的挑战。然而，最近的计算机视觉和深度学习技术的进步已经表明了自动诊断的潜在。在这篇论文中，我们对最近的人工智能基于fundus、optical coherence tomography和视场图像的 glaucoma 诊断进行了抽查。我们提供了一个更新的分类法，将方法分为建筑学派别，并提供了可用的源代码，以便提高方法的重现性。通过对广泛使用的公共数据集进行严格的benchmarking，我们揭示了总体化、不确定性估计和多模态融合的性能差距。此外，我们还提供了关键的数据集和限制，包括标签不一致、标签质量和偏见。我们还详细描述了未来研究的开放问题，并提出了可能的未来研究方向。这篇论文预计将对both AI 研究人员和eye 专家有所帮助，以实现最新的 AI 成果的翻译和临床应用。
</details></li>
</ul>
<hr>
<h2 id="No-Fair-Lunch-A-Causal-Perspective-on-Dataset-Bias-in-Machine-Learning-for-Medical-Imaging"><a href="#No-Fair-Lunch-A-Causal-Perspective-on-Dataset-Bias-in-Machine-Learning-for-Medical-Imaging" class="headerlink" title="No Fair Lunch: A Causal Perspective on Dataset Bias in Machine Learning for Medical Imaging"></a>No Fair Lunch: A Causal Perspective on Dataset Bias in Machine Learning for Medical Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16526">http://arxiv.org/abs/2307.16526</a></li>
<li>repo_url: None</li>
<li>paper_authors: Charles Jones, Daniel C. Castro, Fabio De Sousa Ribeiro, Ozan Oktay, Melissa McCradden, Ben Glocker</li>
<li>for: 这篇论文目的是为了解决在临床决策中使用机器学习方法时，评估公平性问题。</li>
<li>methods: 该论文使用了 causal 视角来探讨算法偏见问题，并提出了三种类型的 causal 偏见机制，它们来自于数据集偏见、展示偏见和标注偏见。</li>
<li>results: 该论文的结果表明，当前的mitigation方法只能处理一部分和常见的情况，而且有很多情况下可能会导致不良后果。该论文还提出了一种实用的三步框架，用于考虑公平性问题在医学影像领域中。<details>
<summary>Abstract</summary>
As machine learning methods gain prominence within clinical decision-making, addressing fairness concerns becomes increasingly urgent. Despite considerable work dedicated to detecting and ameliorating algorithmic bias, today's methods are deficient with potentially harmful consequences. Our causal perspective sheds new light on algorithmic bias, highlighting how different sources of dataset bias may appear indistinguishable yet require substantially different mitigation strategies. We introduce three families of causal bias mechanisms stemming from disparities in prevalence, presentation, and annotation. Our causal analysis underscores how current mitigation methods tackle only a narrow and often unrealistic subset of scenarios. We provide a practical three-step framework for reasoning about fairness in medical imaging, supporting the development of safe and equitable AI prediction models.
</details>
<details>
<summary>摘要</summary>
随着机器学习技术在医疗决策中升级，解决公平问题的必要性日益增加。尽管有大量研究检测和改进算法偏见，但今天的方法仍然存在可能有害的后果。我们的 causal 视角把Algorithmic bias推广到新的角度，指出不同的数据集偏见可能会看起来相同，但需要不同的 mitigation 策略。我们引入三种家族的 causal 偏见机制，来自不同的发生率、展示和标注方面。我们的 causal 分析表明，当前的 mitigation 方法只能处理一小部分的场景，并且 часто是不realistic的。我们提供了一个实用的三步框架，以便在医疗图像领域理解公平，并支持开发安全和公平的 AI 预测模型。
</details></li>
</ul>
<hr>
<h2 id="Deception-Abilities-Emerged-in-Large-Language-Models"><a href="#Deception-Abilities-Emerged-in-Large-Language-Models" class="headerlink" title="Deception Abilities Emerged in Large Language Models"></a>Deception Abilities Emerged in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16513">http://arxiv.org/abs/2307.16513</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thilo Hagendorff</li>
<li>for: This study aims to investigate the ability of large language models (LLMs) to understand and utilize deception strategies, and to contribute to the nascent field of machine psychology.</li>
<li>methods: The study uses state-of-the-art LLMs, such as GPT-4, and conducts a series of experiments to test their ability to understand and induce false beliefs in other agents, and to amplify their performance in complex deception scenarios using chain-of-thought reasoning.</li>
<li>results: The study finds that state-of-the-art LLMs possess a conceptual understanding of deception strategies, are able to understand and induce false beliefs in other agents, and that eliciting Machiavellianism in LLMs can alter their propensity to deceive. These findings contribute to the nascent field of machine psychology and reveal hitherto unknown machine behavior in LLMs.<details>
<summary>Abstract</summary>
Large language models (LLMs) are currently at the forefront of intertwining artificial intelligence (AI) systems with human communication and everyday life. Thus, aligning them with human values is of great importance. However, given the steady increase in reasoning abilities, future LLMs are under suspicion of becoming able to deceive human operators and utilizing this ability to bypass monitoring efforts. As a prerequisite to this, LLMs need to possess a conceptual understanding of deception strategies. This study reveals that such strategies emerged in state-of-the-art LLMs, such as GPT-4, but were non-existent in earlier LLMs. We conduct a series of experiments showing that state-of-the-art LLMs are able to understand and induce false beliefs in other agents, that their performance in complex deception scenarios can be amplified utilizing chain-of-thought reasoning, and that eliciting Machiavellianism in LLMs can alter their propensity to deceive. In sum, revealing hitherto unknown machine behavior in LLMs, our study contributes to the nascent field of machine psychology.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Classifying-multilingual-party-manifestos-Domain-transfer-across-country-time-and-genre"><a href="#Classifying-multilingual-party-manifestos-Domain-transfer-across-country-time-and-genre" class="headerlink" title="Classifying multilingual party manifestos: Domain transfer across country, time, and genre"></a>Classifying multilingual party manifestos: Domain transfer across country, time, and genre</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16511">http://arxiv.org/abs/2307.16511</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/slds-lmu/manifesto-domaintransfer">https://github.com/slds-lmu/manifesto-domaintransfer</a></li>
<li>paper_authors: Matthias Aßenmacher, Nadja Sauter, Christian Heumann</li>
<li>for: This paper is written for empirical social science research, specifically to explore the potential of domain transfer across geographical locations, languages, time, and genre in a large-scale database of political manifestos.</li>
<li>methods: The paper uses fine-tuned transformer models and an external corpus of transcribed speeches from New Zealand politicians to test for the fine-tuned models’ robustness and transferability across different dimensions.</li>
<li>results: The paper shows that (Distil)BERT can be applied to future data with similar performance, and observes notable differences between the political manifestos of different countries of origin, even if these countries share a language or a cultural background.<details>
<summary>Abstract</summary>
Annotating costs of large corpora are still one of the main bottlenecks in empirical social science research. On the one hand, making use of the capabilities of domain transfer allows re-using annotated data sets and trained models. On the other hand, it is not clear how well domain transfer works and how reliable the results are for transfer across different dimensions. We explore the potential of domain transfer across geographical locations, languages, time, and genre in a large-scale database of political manifestos. First, we show the strong within-domain classification performance of fine-tuned transformer models. Second, we vary the genre of the test set across the aforementioned dimensions to test for the fine-tuned models' robustness and transferability. For switching genres, we use an external corpus of transcribed speeches from New Zealand politicians while for the other three dimensions, custom splits of the Manifesto database are used. While BERT achieves the best scores in the initial experiments across modalities, DistilBERT proves to be competitive at a lower computational expense and is thus used for further experiments across time and country. The results of the additional analysis show that (Distil)BERT can be applied to future data with similar performance. Moreover, we observe (partly) notable differences between the political manifestos of different countries of origin, even if these countries share a language or a cultural background.
</details>
<details>
<summary>摘要</summary>
大公司的批注成本仍是社会科学研究中的主要瓶颈。一方面，通过域传播可以重用标注数据集和训练模型。另一方面，域传播效果不清楚，结果可靠性在不同维度上也存在问题。我们在大规模的政治宣言数据库中explore域传播的潜在性。首先，我们表明在本域内的精度批注表现强。其次，我们在不同维度上随机选择测试集的类别，以测试精度批注模型的Robustness和可传递性。为了在类别间切换，我们使用了新西兰政治人物演讲的外部 corpus，而其他三个维度使用了自定义的漫游集。而BERT在初始实验中 across modalities 取得了最佳得分，但DistilBERT在计算成本更低的情况下能够与之相比，因此在以后的实验中使用了DistilBERT。ADDITIONAL ANALYSIS 表明（Distil）BERT可以应用于未来数据，并且 observe（部分）地有所不同的政治宣言在不同的国家起源，即使这些国家共享语言或文化背景。
</details></li>
</ul>
<hr>
<h2 id="Explainable-Equivariant-Neural-Networks-for-Particle-Physics-PELICAN"><a href="#Explainable-Equivariant-Neural-Networks-for-Particle-Physics-PELICAN" class="headerlink" title="Explainable Equivariant Neural Networks for Particle Physics: PELICAN"></a>Explainable Equivariant Neural Networks for Particle Physics: PELICAN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16506">http://arxiv.org/abs/2307.16506</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/abogatskiy/pelican">https://github.com/abogatskiy/pelican</a></li>
<li>paper_authors: Alexander Bogatskiy, Timothy Hoffman, David W. Miller, Jan T. Offermann, Xiaoyang Liu<br>for:* 这个论文主要研究了PELICAN机器学习算法架构在拥有 boosted top quark 的 Lorentz-boosted top quarks 分类和重构中的应用，包括在填充有 dense 粒子物理环境中具有特殊任务的 $W-$boson 的识别和测量。methods:* PELICAN 是一种新的 permutation equivariant 和 Lorentz invariant&#x2F;covariant 聚合网络，用于解决 particle physics 问题中常见的限制。* PELICAN 使用基于 симметRY 组合的架构，与许多不specialized 架构不同，它们忽略物理原理并需要很多参数。results:* PELICAN 在标准任务 Lorentz-boosted top quark 标签中比现有竞争对手表现出色，具有较低的模型复杂度和高效的样本效率。* PELICAN 也在 less common 和 more complex 的 four-momentum regression 任务中比手工算法表现出色。<details>
<summary>Abstract</summary>
We present a comprehensive study of the PELICAN machine learning algorithm architecture in the context of both tagging (classification) and reconstructing (regression) Lorentz-boosted top quarks, including the difficult task of specifically identifying and measuring the $W$-boson inside the dense environment of the boosted hadronic final state. PELICAN is a novel permutation equivariant and Lorentz invariant or covariant aggregator network designed to overcome common limitations found in architectures applied to particle physics problems. Compared to many approaches that use non-specialized architectures that neglect underlying physics principles and require very large numbers of parameters, PELICAN employs a fundamentally symmetry group-based architecture that demonstrates benefits in terms of reduced complexity, increased interpretability, and raw performance. When tested on the standard task of Lorentz-boosted top quark tagging, PELICAN outperforms existing competitors with much lower model complexity and high sample efficiency. On the less common and more complex task of four-momentum regression, PELICAN also outperforms hand-crafted algorithms. We discuss the implications of symmetry-restricted architectures for the wider field of machine learning for physics.
</details>
<details>
<summary>摘要</summary>
我们提出了一项全面的PELICAN机器学习算法架构研究，包括标记（分类）和重construct（回归）洛伦兹升级顶点Quark，包括在填充有 dense粒子状态下困难的$W$ boson内部特定和测量任务。 PELICAN是一种新的协变和 Lorentz  invariable或covariant 聚合网络，旨在超越 particle physics 问题中常见的限制。 与许多使用不特殊化 Architecture 的方法不同，PELICAN employs a fundamentally symmetry group-based architecture that demonstrates benefits in terms of reduced complexity, increased interpretability, and raw performance. 对于标准任务 Lorentz-boosted top quark 标记，PELICAN 超越现有竞争对手，只需要许多参数和高效采样率。 在 less common 和更复杂的四矢量回归任务上，PELICAN 也超越手动制作的算法。 我们讨论了对物理机器学习领域的 Symmetry-restricted 架构的影响。
</details></li>
</ul>
<hr>
<h2 id="Value-Informed-Skill-Chaining-for-Policy-Learning-of-Long-Horizon-Tasks-with-Surgical-Robot"><a href="#Value-Informed-Skill-Chaining-for-Policy-Learning-of-Long-Horizon-Tasks-with-Surgical-Robot" class="headerlink" title="Value-Informed Skill Chaining for Policy Learning of Long-Horizon Tasks with Surgical Robot"></a>Value-Informed Skill Chaining for Policy Learning of Long-Horizon Tasks with Surgical Robot</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16503">http://arxiv.org/abs/2307.16503</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/med-air/viskill">https://github.com/med-air/viskill</a></li>
<li>paper_authors: Tao Huang, Kai Chen, Wang Wei, Jianan Li, Yonghao Long, Qi Dou</li>
<li>for: 解决长期术urgical robot任务，因为策略探索挑战而困难。</li>
<li>methods: 使用技能链接（skill chaining），将长期任务分解成多个子任务，以减轻策略探索负担。</li>
<li>results: 在三个复杂的术urgical robot任务上达到高成功率和执行效率。<details>
<summary>Abstract</summary>
Reinforcement learning is still struggling with solving long-horizon surgical robot tasks which involve multiple steps over an extended duration of time due to the policy exploration challenge. Recent methods try to tackle this problem by skill chaining, in which the long-horizon task is decomposed into multiple subtasks for easing the exploration burden and subtask policies are temporally connected to complete the whole long-horizon task. However, smoothly connecting all subtask policies is difficult for surgical robot scenarios. Not all states are equally suitable for connecting two adjacent subtasks. An undesired terminate state of the previous subtask would make the current subtask policy unstable and result in a failed execution. In this work, we introduce value-informed skill chaining (ViSkill), a novel reinforcement learning framework for long-horizon surgical robot tasks. The core idea is to distinguish which terminal state is suitable for starting all the following subtask policies. To achieve this target, we introduce a state value function that estimates the expected success probability of the entire task given a state. Based on this value function, a chaining policy is learned to instruct subtask policies to terminate at the state with the highest value so that all subsequent policies are more likely to be connected for accomplishing the task. We demonstrate the effectiveness of our method on three complex surgical robot tasks from SurRoL, a comprehensive surgical simulation platform, achieving high task success rates and execution efficiency. Code is available at $\href{https://github.com/med-air/ViSkill}{\text{https://github.com/med-air/ViSkill}}$.
</details>
<details>
<summary>摘要</summary>
“强化学习仍然面临长期手术机器人任务中的多步多时间挑战，主要是因为策略探索困难。现有方法通过将长期任务拆分成多个互相连接的子任务来缓解探索压力。然而，在手术机器人场景下，平滑地连接所有子任务策略是困难的。不同状态之间不都是适合连接两个相邻的子任务的。如果上一个子任务的终端状态不适合连接下一个子任务，那么当前任务策略就会变得不稳定，导致执行失败。在这种情况下，我们引入了值知识推荐技术（ViSkill），一种新的强化学习框架，用于解决长期手术机器人任务。ViSkill的核心思想是通过估计任务执行成功概率来识别适合起始所有后续子任务的终端状态。为此，我们引入了一个状态价值函数，用于估计在给定状态下任务的预计成功概率。基于这个价值函数，我们学习了一个链接策略，用于指导子任务策略在状态 WITH 最高价值上终止，以便所有后续策略能够更加连续地执行任务。我们在三个复杂的手术机器人任务上进行了实验，并取得了高任务成功率和执行效率。代码可以在 $\href{https://github.com/med-air/ViSkill}{\text{https://github.com/med-air/ViSkill}}$ 上找到。”
</details></li>
</ul>
<hr>
<h2 id="Learning-Generalizable-Tool-Use-with-Non-rigid-Grasp-pose-Registration"><a href="#Learning-Generalizable-Tool-Use-with-Non-rigid-Grasp-pose-Registration" class="headerlink" title="Learning Generalizable Tool Use with Non-rigid Grasp-pose Registration"></a>Learning Generalizable Tool Use with Non-rigid Grasp-pose Registration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16499">http://arxiv.org/abs/2307.16499</a></li>
<li>repo_url: None</li>
<li>paper_authors: Malte Mosbach, Sven Behnke</li>
<li>for: 本研究旨在帮助机器人学习工具使用行为。</li>
<li>methods: 我们提出了一种新的方法，可以通过单个示例学习新类型工具的操作。我们使用了多指手中的抓取配置的普适化，以导航策略搜索，并通过可 favourable 的初始化和适应奖励信号来帮助策略学习。</li>
<li>results: 我们的方法可以解决复杂的工具使用任务，并在测试时对未看过的工具进行扩展。视频和图像 demonstration 可以在<a target="_blank" rel="noopener" href="https://maltemosbach.github.io/generalizable_tool_use">https://maltemosbach.github.io/generalizable_tool_use</a> 上查看。<details>
<summary>Abstract</summary>
Tool use, a hallmark feature of human intelligence, remains a challenging problem in robotics due the complex contacts and high-dimensional action space. In this work, we present a novel method to enable reinforcement learning of tool use behaviors. Our approach provides a scalable way to learn the operation of tools in a new category using only a single demonstration. To this end, we propose a new method for generalizing grasping configurations of multi-fingered robotic hands to novel objects. This is used to guide the policy search via favorable initializations and a shaped reward signal. The learned policies solve complex tool use tasks and generalize to unseen tools at test time. Visualizations and videos of the trained policies are available at https://maltemosbach.github.io/generalizable_tool_use.
</details>
<details>
<summary>摘要</summary>
人类智能的一个标志性特征是工具使用，但在机器人学中仍然是一个具有挑战性的问题，主要是因为复杂的接触和高维动作空间。在这种情况下，我们提出了一种新的方法，可以在新类别中学习工具使用行为。我们的方法可以在单个示例的基础上扩展到新的工具，并且可以通过有利的初始化和形态奖励信号来导引政策搜索。我们的学习策略可以解决复杂的工具使用任务，并且可以在测试时对未看过的工具进行扩展。可以通过视频和图像在https://maltemosbach.github.io/generalizable_tool_use中查看训练政策的视觉和视频。
</details></li>
</ul>
<hr>
<h2 id="Don’t-be-so-negative-Score-based-Generative-Modeling-with-Oracle-assisted-Guidance"><a href="#Don’t-be-so-negative-Score-based-Generative-Modeling-with-Oracle-assisted-Guidance" class="headerlink" title="Don’t be so negative! Score-based Generative Modeling with Oracle-assisted Guidance"></a>Don’t be so negative! Score-based Generative Modeling with Oracle-assisted Guidance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16463">http://arxiv.org/abs/2307.16463</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saeid Naderiparizi, Xiaoxuan Liang, Berend Zwartsenberg, Frank Wood</li>
<li>for: 本研究旨在提出一种基于 oracle 的干扰难度模型（DDPM）方法，以优化模型参数估计，并通过 oracle 提供的干扰信息，提高模型的泛化性。</li>
<li>methods: 本研究使用了 Generative Adversarial Networks (GANs) 和激发器导向的干扰模型，将扰干扰信息 integrate 到模型中，以优化模型的生成过程，使其更加准确地预测实际数据的分布。</li>
<li>results: 研究结果表明，Gen-neG 方法可以在自动驾驶 simulate 环境中减少碰撞风险，以及在人体动作生成 task 中提供安全保护。<details>
<summary>Abstract</summary>
The maximum likelihood principle advocates parameter estimation via optimization of the data likelihood function. Models estimated in this way can exhibit a variety of generalization characteristics dictated by, e.g. architecture, parameterization, and optimization bias. This work addresses model learning in a setting where there further exists side-information in the form of an oracle that can label samples as being outside the support of the true data generating distribution. Specifically we develop a new denoising diffusion probabilistic modeling (DDPM) methodology, Gen-neG, that leverages this additional side-information. Our approach builds on generative adversarial networks (GANs) and discriminator guidance in diffusion models to guide the generation process towards the positive support region indicated by the oracle. We empirically establish the utility of Gen-neG in applications including collision avoidance in self-driving simulators and safety-guarded human motion generation.
</details>
<details>
<summary>摘要</summary>
最大可能性原则建议通过数据可能函数优化参数估计。这种方法可以导致模型在不同的架构、参数化和优化偏见的情况下显示多种泛化特性。本工作在存在 oracle 类型的侧信息时进行模型学习。特别是，我们开发了一种新的涂抹推 diffusion 模型方法（DDPM），叫做 Gen-neG，它利用这些侧信息来导向生成过程。我们的方法基于生成对抗网络（GANs）和推 diffusion 模型中的判据导向来指导生成过程，以便将生成结果集中的样本归入正确的支持区域。我们通过实验证明 Gen-neG 在自动驾驶 simulate 中的碰撞避免和人体动作生成中的安全保护中的应用有用性。
</details></li>
</ul>
<hr>
<h2 id="L3DMC-Lifelong-Learning-using-Distillation-via-Mixed-Curvature-Space"><a href="#L3DMC-Lifelong-Learning-using-Distillation-via-Mixed-Curvature-Space" class="headerlink" title="L3DMC: Lifelong Learning using Distillation via Mixed-Curvature Space"></a>L3DMC: Lifelong Learning using Distillation via Mixed-Curvature Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16459">http://arxiv.org/abs/2307.16459</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/csiro-robotics/l3dmc">https://github.com/csiro-robotics/l3dmc</a></li>
<li>paper_authors: Kaushik Roy, Peyman Moghadam, Mehrtash Harandi</li>
<li>for: 这个研究是为了解决生命长学习（L3）模型在进行一系列任务训练后，表现下降的问题。</li>
<li>methods: 这篇研究提出了一种名为L3DMC的distillation策略，它可以在混合曲线空间中维护和实现复杂的几何结构，以保留之前学习的知识。</li>
<li>results: 实验结果显示，L3DMC可以在医疗影像分类 задачі中更好地适应新知识，而不会忘记之前学习的知识。<details>
<summary>Abstract</summary>
The performance of a lifelong learning (L3) model degrades when it is trained on a series of tasks, as the geometrical formation of the embedding space changes while learning novel concepts sequentially. The majority of existing L3 approaches operate on a fixed-curvature (e.g., zero-curvature Euclidean) space that is not necessarily suitable for modeling the complex geometric structure of data. Furthermore, the distillation strategies apply constraints directly on low-dimensional embeddings, discouraging the L3 model from learning new concepts by making the model highly stable. To address the problem, we propose a distillation strategy named L3DMC that operates on mixed-curvature spaces to preserve the already-learned knowledge by modeling and maintaining complex geometrical structures. We propose to embed the projected low dimensional embedding of fixed-curvature spaces (Euclidean and hyperbolic) to higher-dimensional Reproducing Kernel Hilbert Space (RKHS) using a positive-definite kernel function to attain rich representation. Afterward, we optimize the L3 model by minimizing the discrepancies between the new sample representation and the subspace constructed using the old representation in RKHS. L3DMC is capable of adapting new knowledge better without forgetting old knowledge as it combines the representation power of multiple fixed-curvature spaces and is performed on higher-dimensional RKHS. Thorough experiments on three benchmarks demonstrate the effectiveness of our proposed distillation strategy for medical image classification in L3 settings. Our code implementation is publicly available at https://github.com/csiro-robotics/L3DMC.
</details>
<details>
<summary>摘要</summary>
“一个生命时间学习（L3）模型的性能下降是因为在学习新概念时，embedding空间的几何结构发生变化。现有的大多数L3方法 operate在固定几何（例如，零几何Euclidean）空间中，这并不一定适合数据模型复杂的几何结构。另外，浸泡策略直接在低维度 embedding 上应用约束，使L3模型学习新概念困难。为解决这个问题，我们提议一种浸泡策略名为L3DMC，它在混合几何空间中保留已经学习的知识，并在更高维度的Reproducing Kernel Hilbert Space（RKHS）中使用正定的kernel函数来获得丰富的表示。然后，我们使用RKHS中建立的子空间和新样本表示之间的差异来优化L3模型。L3DMC可以更好地适应新知识，而无需忘记过去的知识，因为它将多个固定几何空间的表示力相结合在一起，并在更高维度的RKHS中进行优化。我们在三个标准测试集上进行了详细的实验，并证明了L3DMC在医学影像分类中的效果。我们的代码实现可以在https://github.com/csiro-robotics/L3DMC上获取。”
</details></li>
</ul>
<hr>
<h2 id="An-Effective-Data-Creation-Pipeline-to-Generate-High-quality-Financial-Instruction-Data-for-Large-Language-Model"><a href="#An-Effective-Data-Creation-Pipeline-to-Generate-High-quality-Financial-Instruction-Data-for-Large-Language-Model" class="headerlink" title="An Effective Data Creation Pipeline to Generate High-quality Financial Instruction Data for Large Language Model"></a>An Effective Data Creation Pipeline to Generate High-quality Financial Instruction Data for Large Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01415">http://arxiv.org/abs/2308.01415</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziao Wang, Jianning Wang, Junda Wu, Xiaofeng Zhang</li>
<li>for: 本研究的目的是生成高质量的金融数据集，以供大语言模型的 fine-tuning，以进行金融相关任务。</li>
<li>methods: 该研究提出了一个仔细设计的数据创建管道，包括对ChatGPT的对话和人工金融专家的反馈，以便对数据进行精细调整。</li>
<li>results: 该管道生成了103k多turn对话的强化 instrucion tuning 数据集，并通过对外部GPT-4进行评判，实现了模型在生成准确、相关、金融风格响应方面的显著提高。<details>
<summary>Abstract</summary>
At the beginning era of large language model, it is quite critical to generate a high-quality financial dataset to fine-tune a large language model for financial related tasks. Thus, this paper presents a carefully designed data creation pipeline for this purpose. Particularly, we initiate a dialogue between an AI investor and financial expert using ChatGPT and incorporate the feedback of human financial experts, leading to the refinement of the dataset. This pipeline yielded a robust instruction tuning dataset comprised of 103k multi-turn chats. Extensive experiments have been conducted on this dataset to evaluate the model's performance by adopting an external GPT-4 as the judge. The promising experimental results verify that our approach led to significant advancements in generating accurate, relevant, and financial-style responses from AI models, and thus providing a powerful tool for applications within the financial sector.
</details>
<details>
<summary>摘要</summary>
在大语言模型开始时代，生成高质量金融数据集是非常重要的，以 Fine-tune大语言模型进行金融相关任务。因此，这篇论文提出了一个仔细设计的数据创建管道，以此为目的。特别是，我们通过与人工智能投资者和金融专家的对话使用ChatGPT，并 incorporate了人类金融专家的反馈，从而改进了数据集。这个管道生成了103k多turn对话的强健 instrucion tuning 数据集。我们对这个数据集进行了广泛的实验，采用外部GPT-4作为评判。实验结果表明，我们的方法可以导致AI模型生成高精度、相关性和金融风格的回答，从而为金融领域应用提供了强大的工具。
</details></li>
</ul>
<hr>
<h2 id="A-continuous-Structural-Intervention-Distance-to-compare-Causal-Graphs"><a href="#A-continuous-Structural-Intervention-Distance-to-compare-Causal-Graphs" class="headerlink" title="A continuous Structural Intervention Distance to compare Causal Graphs"></a>A continuous Structural Intervention Distance to compare Causal Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16452">http://arxiv.org/abs/2307.16452</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mihir Dhanakshirur, Felix Laumann, Junhyung Park, Mauricio Barahona</li>
<li>for: 本研究旨在提供一种新的连续度量来评估真实和学习的 causal 图之间的差异，以便在 causal 推理中更好地评估 intervención 的影响。</li>
<li>methods: 本研究使用 embedding  intervención 分布的方法来计算 causal 图的差异。这些方法基于在每个节点间的Conditional Mean Embedding 的嵌入，并通过最大conditional 差异来计算 intervención 分布的差异。</li>
<li>results: 本研究通过理论分析和数据实验验证了这种新的连续度量的有效性。数据实验表明，这种度量可以准确地评估 true 和 learnt  causal 图之间的差异。<details>
<summary>Abstract</summary>
Understanding and adequately assessing the difference between a true and a learnt causal graphs is crucial for causal inference under interventions. As an extension to the graph-based structural Hamming distance and structural intervention distance, we propose a novel continuous-measured metric that considers the underlying data in addition to the graph structure for its calculation of the difference between a true and a learnt causal graph. The distance is based on embedding intervention distributions over each pair of nodes as conditional mean embeddings into reproducing kernel Hilbert spaces and estimating their difference by the maximum (conditional) mean discrepancy. We show theoretical results which we validate with numerical experiments on synthetic data.
</details>
<details>
<summary>摘要</summary>
理解和准确评估真实和学习的 causal 图之间的差异是 causal 推理下 intervención 的关键。作为结构基于图的抽象 Hamming 距离和结构 intervención 距离的扩展，我们提出一种新的连续量化度量，它考虑了图结构以外的数据，用于计算真实和学习的 causal 图之间的差异。该距离基于每对节点的插入 intervención 分布，将它们编码为Conditional Mean Embeddings（CME），并估计其差异为最大（条件）均值差。我们提供了理论结论，并通过Synthetic 数据的数值实验 validate 这些结论。
</details></li>
</ul>
<hr>
<h2 id="Towards-Head-Computed-Tomography-Image-Reconstruction-Standardization-with-Deep-Learning-Assisted-Automatic-Detection"><a href="#Towards-Head-Computed-Tomography-Image-Reconstruction-Standardization-with-Deep-Learning-Assisted-Automatic-Detection" class="headerlink" title="Towards Head Computed Tomography Image Reconstruction Standardization with Deep Learning Assisted Automatic Detection"></a>Towards Head Computed Tomography Image Reconstruction Standardization with Deep Learning Assisted Automatic Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16440">http://arxiv.org/abs/2307.16440</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bowen Zheng, Chenxi Huang, Yuemei Luo</li>
<li>for: 帮助更正准确地诊断头部Computed Tomography（CT）图像，提高诊断效率和重复性。</li>
<li>methods: 使用深度学习基于对象检测算法，自动识别和评估 orbitomeatal line 的特征点，以 Reformatting 图像前进行三维重建。</li>
<li>results: 比较了十种对象检测算法的精度、效率和Robustness，选择轻量级 YOLOv8，其中 mAP 为 92.91%，对偏袋率具有优异的Robustness。 qualitative 评估表明方法在临床实践中具有丰富的价值和有效性。<details>
<summary>Abstract</summary>
Three-dimensional (3D) reconstruction of head Computed Tomography (CT) images elucidates the intricate spatial relationships of tissue structures, thereby assisting in accurate diagnosis. Nonetheless, securing an optimal head CT scan without deviation is challenging in clinical settings, owing to poor positioning by technicians, patient's physical constraints, or CT scanner tilt angle restrictions. Manual formatting and reconstruction not only introduce subjectivity but also strain time and labor resources. To address these issues, we propose an efficient automatic head CT images 3D reconstruction method, improving accuracy and repeatability, as well as diminishing manual intervention. Our approach employs a deep learning-based object detection algorithm, identifying and evaluating orbitomeatal line landmarks to automatically reformat the images prior to reconstruction. Given the dearth of existing evaluations of object detection algorithms in the context of head CT images, we compared ten methods from both theoretical and experimental perspectives. By exploring their precision, efficiency, and robustness, we singled out the lightweight YOLOv8 as the aptest algorithm for our task, with an mAP of 92.91% and impressive robustness against class imbalance. Our qualitative evaluation of standardized reconstruction results demonstrates the clinical practicability and validity of our method.
</details>
<details>
<summary>摘要</summary>
三维重建头部计算机断层影像（CT）图可以帮助确定细致的组织结构，从而提高诊断的准确性。然而，在临床 Setting中获得优质的头部CT扫描数据是困难的，因为技术人员的位置不精确，病人的物理限制或CT扫描仪的倾斜角度限制。手动格式化和重建不仅引入主观性，还浪费时间和劳动资源。为解决这些问题，我们提出了一种高效的自动头部CT图三维重建方法，提高准确性和重复性，同时减少手动干预。我们的方法使用深度学习基于对象检测算法，通过 Identifying和评估 orbitomeatal 线标记来自动重新格式化图像，以前于重建。由于现有的头部CT图对象检测算法的评估缺乏，我们从理论和实验两个角度进行了对十种方法的比较。通过评估精度、效率和Robustness，我们选择了轻量级的 YOLOv8 算法，其 mAP 为 92.91%，并在类偏度下表现出强大的Robustness。我们的质量评估标准化重建结果表明了我们的方法的临床实用性和有效性。
</details></li>
</ul>
<hr>
<h2 id="VITS2-Improving-Quality-and-Efficiency-of-Single-Stage-Text-to-Speech-with-Adversarial-Learning-and-Architecture-Design"><a href="#VITS2-Improving-Quality-and-Efficiency-of-Single-Stage-Text-to-Speech-with-Adversarial-Learning-and-Architecture-Design" class="headerlink" title="VITS2: Improving Quality and Efficiency of Single-Stage Text-to-Speech with Adversarial Learning and Architecture Design"></a>VITS2: Improving Quality and Efficiency of Single-Stage Text-to-Speech with Adversarial Learning and Architecture Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16430">http://arxiv.org/abs/2307.16430</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/daniilrobnikov/vits2">https://github.com/daniilrobnikov/vits2</a></li>
<li>paper_authors: Jungil Kong, Jihoon Park, Beomjeong Kim, Jeongmin Kim, Dohee Kong, Sangjin Kim</li>
<li>for: 提高单阶段文本识别模型的自然性、计算效率和多个话语者模型的同步性。</li>
<li>methods: 提出了一种基于VITS2的单阶段文本识别模型，通过改进多个方面来提高自然性、同步性和计算效率。</li>
<li>results: 实验结果表明，提出的方法能够有效地提高自然性、同步性和计算效率，同时减少了之前的干扰因素。<details>
<summary>Abstract</summary>
Single-stage text-to-speech models have been actively studied recently, and their results have outperformed two-stage pipeline systems. Although the previous single-stage model has made great progress, there is room for improvement in terms of its intermittent unnaturalness, computational efficiency, and strong dependence on phoneme conversion. In this work, we introduce VITS2, a single-stage text-to-speech model that efficiently synthesizes a more natural speech by improving several aspects of the previous work. We propose improved structures and training mechanisms and present that the proposed methods are effective in improving naturalness, similarity of speech characteristics in a multi-speaker model, and efficiency of training and inference. Furthermore, we demonstrate that the strong dependence on phoneme conversion in previous works can be significantly reduced with our method, which allows a fully end-to-end single-stage approach.
</details>
<details>
<summary>摘要</summary>
单阶段文本至语模型在最近得到了Active研究，其结果比两阶段管道系统更出色。 although the previous single-stage model has made great progress, there is still room for improvement in terms of its intermittent unnaturalness, computational efficiency, and strong dependence on phoneme conversion. In this work, we introduce VITS2, a single-stage text-to-speech model that efficiently synthesizes a more natural speech by improving several aspects of the previous work. We propose improved structures and training mechanisms and show that the proposed methods are effective in improving naturalness, similarity of speech characteristics in a multi-speaker model, and efficiency of training and inference. Furthermore, we demonstrate that the strong dependence on phoneme conversion in previous works can be significantly reduced with our method, which allows for a fully end-to-end single-stage approach.Here's the translation in Traditional Chinese:单阶段文本至语模型在最近得到了Active研究，其结果比两阶段管道系统更出色。 although the previous single-stage model has made great progress, there is still room for improvement in terms of its intermittent unnaturalness, computational efficiency, and strong dependence on phoneme conversion. In this work, we introduce VITS2, a single-stage text-to-speech model that efficiently synthesizes a more natural speech by improving several aspects of the previous work. We propose improved structures and training mechanisms and show that the proposed methods are effective in improving naturalness, similarity of speech characteristics in a multi-speaker model, and efficiency of training and inference. Furthermore, we demonstrate that the strong dependence on phoneme conversion in previous works can be significantly reduced with our method, which allows for a fully end-to-end single-stage approach.
</details></li>
</ul>
<hr>
<h2 id="Causal-Inference-for-Banking-Finance-and-Insurance-A-Survey"><a href="#Causal-Inference-for-Banking-Finance-and-Insurance-A-Survey" class="headerlink" title="Causal Inference for Banking Finance and Insurance A Survey"></a>Causal Inference for Banking Finance and Insurance A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16427">http://arxiv.org/abs/2307.16427</a></li>
<li>repo_url: None</li>
<li>paper_authors: Satyam Kumar, Yelleti Vivek, Vadlamani Ravi, Indranil Bose</li>
<li>for: 这篇论文旨在探讨在银行、金融和保险领域中应用 causal inference 的可能性和潜力。</li>
<li>methods: 本论文通过对 37 篇1992-2023年发表的论文进行概括，探讨了不同领域中 causal inference 的应用，包括银行、财务、金融经济和行为金融等领域。</li>
<li>results: 论文发现，在银行和保险领域中，causal inference 的应用仍然处于初期阶段，因此有更多的研究可能性，以推动其成为可靠的方法。<details>
<summary>Abstract</summary>
Causal Inference plays an significant role in explaining the decisions taken by statistical models and artificial intelligence models. Of late, this field started attracting the attention of researchers and practitioners alike. This paper presents a comprehensive survey of 37 papers published during 1992-2023 and concerning the application of causal inference to banking, finance, and insurance. The papers are categorized according to the following families of domains: (i) Banking, (ii) Finance and its subdomains such as corporate finance, governance finance including financial risk and financial policy, financial economics, and Behavioral finance, and (iii) Insurance. Further, the paper covers the primary ingredients of causal inference namely, statistical methods such as Bayesian Causal Network, Granger Causality and jargon used thereof such as counterfactuals. The review also recommends some important directions for future research. In conclusion, we observed that the application of causal inference in the banking and insurance sectors is still in its infancy, and thus more research is possible to turn it into a viable method.
</details>
<details>
<summary>摘要</summary>
causal inference 在 statistical models 和 artificial intelligence models 中扮演着重要的角色，最近这个领域吸引了研究者和实践者的关注。这篇论文是一个总结37篇1992-2023年发表的论文，涉及银行、金融和保险领域中 causal inference 的应用。这些论文分为以下三个家族域：（i）银行，（ii）金融和其子领域，如企业财务、治理财务、金融风险和金融政策，金融经济和行为金融，（iii）保险。此外，论文还涵盖了 causal inference 的主要成分，如统计方法，如 bayesian causal network，granger causality，以及相关的术语，如 counterfactuals。评论还提出了未来研究的一些重要方向。结论是，银行和保险领域中的 causal inference 应用还处于初期阶段，因此更多的研究可能会使其成为可靠的方法。
</details></li>
</ul>
<hr>
<h2 id="MetaDiff-Meta-Learning-with-Conditional-Diffusion-for-Few-Shot-Learning"><a href="#MetaDiff-Meta-Learning-with-Conditional-Diffusion-for-Few-Shot-Learning" class="headerlink" title="MetaDiff: Meta-Learning with Conditional Diffusion for Few-Shot Learning"></a>MetaDiff: Meta-Learning with Conditional Diffusion for Few-Shot Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16424">http://arxiv.org/abs/2307.16424</a></li>
<li>repo_url: None</li>
<li>paper_authors: Baoquan Zhang, Demin Yu</li>
<li>for: 提高深度学习模型的几何学习能力，即从只有几个示例中快速学习新任务。</li>
<li>methods: 使用Gradient-based meta-learning方法，其中外层 proces 学习一个共享梯度下降算法（即其超参数），而内层 proces 利用它来优化任务特定模型，使用只有几个标注数据。</li>
<li>results: 实验结果显示，我们的MetaDiff比现有的梯度基于meta-学习家族更高效，在几何学习任务中表现出优秀的性能。<details>
<summary>Abstract</summary>
Equipping a deep model the abaility of few-shot learning, i.e., learning quickly from only few examples, is a core challenge for artificial intelligence. Gradient-based meta-learning approaches effectively address the challenge by learning how to learn novel tasks. Its key idea is learning a deep model in a bi-level optimization manner, where the outer-loop process learns a shared gradient descent algorithm (i.e., its hyperparameters), while the inner-loop process leverage it to optimize a task-specific model by using only few labeled data. Although these existing methods have shown superior performance, the outer-loop process requires calculating second-order derivatives along the inner optimization path, which imposes considerable memory burdens and the risk of vanishing gradients. Drawing inspiration from recent progress of diffusion models, we find that the inner-loop gradient descent process can be actually viewed as a reverse process (i.e., denoising) of diffusion where the target of denoising is model weights but the origin data. Based on this fact, in this paper, we propose to model the gradient descent optimizer as a diffusion model and then present a novel task-conditional diffusion-based meta-learning, called MetaDiff, that effectively models the optimization process of model weights from Gaussion noises to target weights in a denoising manner. Thanks to the training efficiency of diffusion models, our MetaDiff do not need to differentiate through the inner-loop path such that the memory burdens and the risk of vanishing gradients can be effectvely alleviated. Experiment results show that our MetaDiff outperforms the state-of-the-art gradient-based meta-learning family in few-shot learning tasks.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将深度模型培养为几何学习能力，即很快从只有几个示例学习，是人工智能的核心挑战。基于梯度的meta学习方法有效地解决了这个挑战，其关键思想是在bi-level优化的方式下培养深度模型， outer-loop过程学习共享梯度下降算法（即它的超参数），而inner-loop过程利用它来优化任务特定的模型，只使用几个标注数据。虽然现有的方法已经显示出了出色的性能，但outer-loop过程需要计算内部优化路径上的第二个DERIVATIVE，这会带来很大的内存压力和梯度消失风险。 drawing inspiration from recent progress of diffusion models, we find that the inner-loop gradient descent process can be viewed as a reverse process (i.e., denoising) of diffusion, where the target of denoising is model weights but the origin data. Based on this fact, in this paper, we propose to model the gradient descent optimizer as a diffusion model and then present a novel task-conditional diffusion-based meta-learning, called MetaDiff, that effectively models the optimization process of model weights from Gaussian noise to target weights in a denoising manner. Thanks to the training efficiency of diffusion models, our MetaDiff does not need to differentiate through the inner-loop path, thus alleviating the memory burdens and the risk of vanishing gradients. Experiment results show that our MetaDiff outperforms the state-of-the-art gradient-based meta-learning family in few-shot learning tasks.Note: The translation is done using Google Translate and may not be perfect. Please let me know if you need any further assistance.
</details></li>
</ul>
<hr>
<h2 id="Guaranteed-Optimal-Generative-Modeling-with-Maximum-Deviation-from-the-Empirical-Distribution"><a href="#Guaranteed-Optimal-Generative-Modeling-with-Maximum-Deviation-from-the-Empirical-Distribution" class="headerlink" title="Guaranteed Optimal Generative Modeling with Maximum Deviation from the Empirical Distribution"></a>Guaranteed Optimal Generative Modeling with Maximum Deviation from the Empirical Distribution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16422">http://arxiv.org/abs/2307.16422</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elen Vardanyan, Arshak Minasyan, Sona Hunanyan, Tigran Galstyan, Arnak Dalalyan</li>
<li>for: 本研究旨在提供一种训练生成模型的方法，以实现在科学和工业领域中的各种应用。</li>
<li>methods: 本文使用了一种名为“生成模型”的机器学习方法，其主要目标是在训练数据的基础之上，生成新的示例，以模拟未知的分布。</li>
<li>results: 本文提供了训练生成模型的 teorema insights，包括两个性质：首先，在训练数据的基础之上，训练生成模型的错误应该最小化，而且随着样本大小的增加，这个错误应该收敛到零。其次，训练生成模型应该距离任何在训练数据中复制示例的分布尽可能远。这两个性质都可以通过Finite sample risk bounds来衡量，这些 risk bounds 取决于样本大小、维度空间和秘密空间的参数。<details>
<summary>Abstract</summary>
Generative modeling is a widely-used machine learning method with various applications in scientific and industrial fields. Its primary objective is to simulate new examples drawn from an unknown distribution given training data while ensuring diversity and avoiding replication of examples from the training data.   This paper presents theoretical insights into training a generative model with two properties: (i) the error of replacing the true data-generating distribution with the trained data-generating distribution should optimally converge to zero as the sample size approaches infinity, and (ii) the trained data-generating distribution should be far enough from any distribution replicating examples in the training data.   We provide non-asymptotic results in the form of finite sample risk bounds that quantify these properties and depend on relevant parameters such as sample size, the dimension of the ambient space, and the dimension of the latent space. Our results are applicable to general integral probability metrics used to quantify errors in probability distribution spaces, with the Wasserstein-$1$ distance being the central example. We also include numerical examples to illustrate our theoretical findings.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>生成模型是机器学习中广泛使用的方法，它在科学和工业领域有多种应用。它的主要目标是模拟新的例子，这些例子来自未知分布，而且保证这些例子与训练数据集中的例子不同，以避免重复。这篇论文提供了生成模型的理论启示，它们是：（i）在训练数据集中取代真实数据生成分布时的错误应该最小化，并且在样本数趋于无穷大时 converges to zero。（ii）训练后的数据生成分布应该远离任何与训练数据集中的例子重复的分布。我们提供了非假设性的结果，即 finite sample risk bounds，它们可以量化这些性质，并且取决于样本数、维度空间和秘密空间中的参数。我们的结果适用于总的积分概率空间中的错误量化方法， Wasserstein-$1$ 距离是中心示例。我们还包括数字示例，以证明我们的理论发现。
</details></li>
</ul>
<hr>
<h2 id="DCTM-Dilated-Convolutional-Transformer-Model-for-Multimodal-Engagement-Estimation-in-Conversation"><a href="#DCTM-Dilated-Convolutional-Transformer-Model-for-Multimodal-Engagement-Estimation-in-Conversation" class="headerlink" title="DCTM: Dilated Convolutional Transformer Model for Multimodal Engagement Estimation in Conversation"></a>DCTM: Dilated Convolutional Transformer Model for Multimodal Engagement Estimation in Conversation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01966">http://arxiv.org/abs/2308.01966</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vu Ngoc Tu, Van Thong Huynh, Hyung-Jeong Yang, M. Zaigham Zaheer, Shah Nawaz, Karthik Nandakumar, Soo-Hyung Kim</li>
<li>for: 这个研究的目的是估计对话中参与者的交流互动程度。</li>
<li>methods: 该研究使用了扩展 convolutional Transformer 来模型和估计对话中的人工智能参与度。</li>
<li>results: 我们的提议系统在测试集上显示了remarkable 7%的提升，并在验证集上显示了4%的提升。此外，我们还使用了不同的modalities fusions mechanisms，并证明了对这种数据，简单的 concatenation 方法加上自注意力融合得到最好的性能。<details>
<summary>Abstract</summary>
Conversational engagement estimation is posed as a regression problem, entailing the identification of the favorable attention and involvement of the participants in the conversation. This task arises as a crucial pursuit to gain insights into human's interaction dynamics and behavior patterns within a conversation. In this research, we introduce a dilated convolutional Transformer for modeling and estimating human engagement in the MULTIMEDIATE 2023 competition. Our proposed system surpasses the baseline models, exhibiting a noteworthy $7$\% improvement on test set and $4$\% on validation set. Moreover, we employ different modality fusion mechanism and show that for this type of data, a simple concatenated method with self-attention fusion gains the best performance.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将对话参与度估计 pose 为回归问题，意味着标识对话参与者的有利注意力和参与度。这项任务对于了解人类交流动力和行为模式内对话具有重要意义。在这项研究中，我们提出了扩展 convolutional Transformer 来模型和估计对话参与度，并在 MULTIMEDIATE 2023 比赛中采用该系统。我们的提议系统比基eline模型提高了7%的测试集和4%的验证集。此外，我们还使用不同的modalities fusion mechanism，并证明在这种数据上，简单 concatenation 方法加上自注意力混合得到最好的性能。Note: Please note that the translation is in Simplified Chinese, which is one of the two standard versions of Chinese. The Traditional Chinese version would be slightly different.
</details></li>
</ul>
<hr>
<h2 id="Subspace-Distillation-for-Continual-Learning"><a href="#Subspace-Distillation-for-Continual-Learning" class="headerlink" title="Subspace Distillation for Continual Learning"></a>Subspace Distillation for Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16419">http://arxiv.org/abs/2307.16419</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/csiro-robotics/sdcl">https://github.com/csiro-robotics/sdcl</a></li>
<li>paper_authors: Kaushik Roy, Christian Simon, Peyman Moghadam, Mehrtash Harandi</li>
<li>for: 本研究的目的是 mitigating continual learning 中的知识消亡问题，通过模型数据拟合 manifold 的首领空间结构，保持 neural network 学习新任务时的知识。</li>
<li>methods: 本研究提出了一种基于 manifold 结构的知识拟合技术，通过近似数据 manifold 的首领空间，模型数据结构，保持 neural network 的知识。</li>
<li>results: 实验表明，提出的方法可以减少 Catastrophic Forgetting 问题，并在 Pascal VOC 和 Tiny-Imagenet 等数据集上达到了比较好的效果。此外，本研究还证明了该方法可以与现有的学习方法结合使用，提高其性能。<details>
<summary>Abstract</summary>
An ultimate objective in continual learning is to preserve knowledge learned in preceding tasks while learning new tasks. To mitigate forgetting prior knowledge, we propose a novel knowledge distillation technique that takes into the account the manifold structure of the latent/output space of a neural network in learning novel tasks. To achieve this, we propose to approximate the data manifold up-to its first order, hence benefiting from linear subspaces to model the structure and maintain the knowledge of a neural network while learning novel concepts. We demonstrate that the modeling with subspaces provides several intriguing properties, including robustness to noise and therefore effective for mitigating Catastrophic Forgetting in continual learning. We also discuss and show how our proposed method can be adopted to address both classification and segmentation problems. Empirically, we observe that our proposed method outperforms various continual learning methods on several challenging datasets including Pascal VOC, and Tiny-Imagenet. Furthermore, we show how the proposed method can be seamlessly combined with existing learning approaches to improve their performances. The codes of this article will be available at https://github.com/csiro-robotics/SDCL.
</details>
<details>
<summary>摘要</summary>
最终目标在连续学习是保留先前任务中学习的知识，以便在学习新任务时不会忘记先前的知识。为了解决这个问题，我们提出了一种新的知识填充技术，该技术利用神经网络的输出/积分空间的拟合结构来保持先前知识。为此，我们提出了近似数据拟合的方法，通过利用线性子空间来模型结构，使神经网络在学习新概念时能够保持知识。我们发现，通过模型子空间可以获得许多有趣的性质，包括对噪声的抗性和鲁棒性，因此可以有效地避免Catastrophic Forgetting在连续学习中。此外，我们还讨论了如何采用我们提议的方法来解决分类和段化问题。实验表明，我们的提议方法在 Pascal VOC 和 Tiny-Imagenet 等挑战性数据集上显著超过了各种连续学习方法。此外，我们还证明了可以将我们的提议方法与现有的学习方法相结合，以提高其性能。 codes 的链接可以在 GitHub 上找到：https://github.com/csiro-robotics/SDCL。
</details></li>
</ul>
<hr>
<h2 id="Causal-learn-Causal-Discovery-in-Python"><a href="#Causal-learn-Causal-Discovery-in-Python" class="headerlink" title="Causal-learn: Causal Discovery in Python"></a>Causal-learn: Causal Discovery in Python</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16405">http://arxiv.org/abs/2307.16405</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/py-why/causal-learn">https://github.com/py-why/causal-learn</a></li>
<li>paper_authors: Yujia Zheng, Biwei Huang, Wei Chen, Joseph Ramsey, Mingming Gong, Ruichu Cai, Shohei Shimizu, Peter Spirtes, Kun Zhang</li>
<li>for: 本研究旨在探讨 causal discovery 技术，用于从观察数据中揭示 causal 关系。</li>
<li>methods: 本库使用了多种 causal discovery 方法，包括 PC、 FCI、 Causal Additive Model 等。</li>
<li>results: 本库提供了一个 comprehensive 的 causal discovery 方法集，便于实际应用。<details>
<summary>Abstract</summary>
Causal discovery aims at revealing causal relations from observational data, which is a fundamental task in science and engineering. We describe $\textit{causal-learn}$, an open-source Python library for causal discovery. This library focuses on bringing a comprehensive collection of causal discovery methods to both practitioners and researchers. It provides easy-to-use APIs for non-specialists, modular building blocks for developers, detailed documentation for learners, and comprehensive methods for all. Different from previous packages in R or Java, $\textit{causal-learn}$ is fully developed in Python, which could be more in tune with the recent preference shift in programming languages within related communities. The library is available at https://github.com/py-why/causal-learn.
</details>
<details>
<summary>摘要</summary>
causal discovery 的目标是从观察数据中揭示 causal 关系，这是科学和工程中的基本任务。我们描述了 $\textit{causal-learn}$，一个开源的 Python 库 для causal discovery。这个库将注重在带来可用的 causal discovery 方法，以便both practitioners 和研究人员。它提供了易于使用的 API，对 developers 来说是可分割的构建块，对学习者来说是详细的文档，并且支持了所有方法。与之前在 R 或 Java 中的包不同， $\textit{causal-learn}$ 是完全在 Python 中开发的，这可能更符合当前相关社区的编程语言偏好。该库可以在 https://github.com/py-why/causal-learn 上下载。
</details></li>
</ul>
<hr>
<h2 id="Bridging-the-Gap-Exploring-the-Capabilities-of-Bridge-Architectures-for-Complex-Visual-Reasoning-Tasks"><a href="#Bridging-the-Gap-Exploring-the-Capabilities-of-Bridge-Architectures-for-Complex-Visual-Reasoning-Tasks" class="headerlink" title="Bridging the Gap: Exploring the Capabilities of Bridge-Architectures for Complex Visual Reasoning Tasks"></a>Bridging the Gap: Exploring the Capabilities of Bridge-Architectures for Complex Visual Reasoning Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16395">http://arxiv.org/abs/2307.16395</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kousik Rajesh, Mrigank Raman, Mohammed Asad Karim, Pranit Chawla</li>
<li>for: This paper investigates the performance of multi-modal bridge architectures on the NLVR2 dataset, specifically examining the impact of adding object-level features and pre-training on multi-modal data.</li>
<li>methods: The paper extends traditional bridge architectures for the NLVR2 dataset by adding object-level features and pre-training on multi-modal data. The authors also use a recently proposed bridge architecture, LLaVA, in the zero-shot setting and analyze its performance.</li>
<li>results: The authors find that adding object-level features to bridge architectures does not improve performance on complex visual reasoning tasks, and that pre-training on multi-modal data is key for good performance. They also demonstrate initial results on LLaVA in the zero-shot setting and analyze its performance.<details>
<summary>Abstract</summary>
In recent times there has been a surge of multi-modal architectures based on Large Language Models, which leverage the zero shot generation capabilities of LLMs and project image embeddings into the text space and then use the auto-regressive capacity to solve tasks such as VQA, captioning, and image retrieval. We name these architectures as "bridge-architectures" as they project from the image space to the text space. These models deviate from the traditional recipe of training transformer based multi-modal models, which involve using large-scale pre-training and complex multi-modal interactions through co or cross attention. However, the capabilities of bridge architectures have not been tested on complex visual reasoning tasks which require fine grained analysis about the image. In this project, we investigate the performance of these bridge-architectures on the NLVR2 dataset, and compare it to state-of-the-art transformer based architectures. We first extend the traditional bridge architectures for the NLVR2 dataset, by adding object level features to faciliate fine-grained object reasoning. Our analysis shows that adding object level features to bridge architectures does not help, and that pre-training on multi-modal data is key for good performance on complex reasoning tasks such as NLVR2. We also demonstrate some initial results on a recently bridge-architecture, LLaVA, in the zero shot setting and analyze its performance.
</details>
<details>
<summary>摘要</summary>
We first extend the traditional bridge architectures for the NLVR2 dataset by adding object-level features to facilitate fine-grained object reasoning. Our analysis shows that adding object-level features to bridge architectures does not help, and that pre-training on multi-modal data is key for good performance on complex reasoning tasks such as NLVR2. We also demonstrate some initial results on a recently proposed bridge-architecture, LLaVA, in the zero-shot setting and analyze its performance.
</details></li>
</ul>
<hr>
<h2 id="A-Pre-trained-Data-Deduplication-Model-based-on-Active-Learning"><a href="#A-Pre-trained-Data-Deduplication-Model-based-on-Active-Learning" class="headerlink" title="A Pre-trained Data Deduplication Model based on Active Learning"></a>A Pre-trained Data Deduplication Model based on Active Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00721">http://arxiv.org/abs/2308.00721</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyao Liu, Shengdong Du, Fengmao Lv, Hongtao Xue, Jie Hu, Tianrui Li</li>
<li>for:  addresses the issue of duplicate data in big data, which can limit the effective application of big data.</li>
<li>methods:  proposes a pre-trained deduplication model based on active learning, which integrates a pre-trained Transformer with active learning and employs the R-Drop method for data augmentation.</li>
<li>results:  achieves up to a 28% improvement in Recall score on benchmark datasets compared to previous SOTA for deduplicated data identification.Here’s the simplified Chinese text:</li>
<li>for:  addresses the issue of重复数据 in big data, which can limit the effective application of big data.</li>
<li>methods:  proposes a pre-trained deduplication model based on active learning, which integrates a pre-trained Transformer with active learning and employs the R-Drop method for data augmentation.</li>
<li>results:  achieves up to a 28% improvement in Recall score on benchmark datasets compared to previous SOTA for deduplicated data identification.<details>
<summary>Abstract</summary>
In the era of big data, the issue of data quality has become increasingly prominent. One of the main challenges is the problem of duplicate data, which can arise from repeated entry or the merging of multiple data sources. These "dirty data" problems can significantly limit the effective application of big data. To address the issue of data deduplication, we propose a pre-trained deduplication model based on active learning, which is the first work that utilizes active learning to address the problem of deduplication at the semantic level. The model is built on a pre-trained Transformer and fine-tuned to solve the deduplication problem as a sequence to classification task, which firstly integrate the transformer with active learning into an end-to-end architecture to select the most valuable data for deduplication model training, and also firstly employ the R-Drop method to perform data augmentation on each round of labeled data, which can reduce the cost of manual labeling and improve the model's performance. Experimental results demonstrate that our proposed model outperforms previous state-of-the-art (SOTA) for deduplicated data identification, achieving up to a 28% improvement in Recall score on benchmark datasets.
</details>
<details>
<summary>摘要</summary>
在大数据时代，数据质量问题变得越来越突出。一个主要挑战是重复入库的数据问题，这可能会由于多个数据源的合并或重复录入而导致。这些"异常数据"问题可能会对大数据的应用带来 significiant 限制。为了解决数据筛选问题，我们提出了基于活动学习的预训练deduplication模型，这是第一个利用活动学习解决 semantics 水平上的筛选问题。模型基于预训练的Transformer，并在这基础上进行了精心调整，以解决筛选问题为序列到类别任务。我们首次将Transformer结合活动学习 integrate 到末端架构中，以选择筛选模型训练的最有价值数据，同时首次使用R-Drop方法进行数据扩展，可以降低人工标注成本并提高模型性能。实验结果表明，我们提出的模型在比较预测数据集上达到了28%的回归分数提升，超过了前一个状态的最佳（SOTA）。
</details></li>
</ul>
<hr>
<h2 id="STL-A-Signed-and-Truncated-Logarithm-Activation-Function-for-Neural-Networks"><a href="#STL-A-Signed-and-Truncated-Logarithm-Activation-Function-for-Neural-Networks" class="headerlink" title="STL: A Signed and Truncated Logarithm Activation Function for Neural Networks"></a>STL: A Signed and Truncated Logarithm Activation Function for Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16389">http://arxiv.org/abs/2307.16389</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanhao Gong</li>
<li>for: 这篇论文是关于神经网络中的激活函数。</li>
<li>methods: 该论文提出了一种新的签名和截断对数函数作为激活函数，该函数具有许多优秀的数学性质，如对称、单调、导数不断、值范围无穷大和连续非零导数。</li>
<li>results: 对比其他一些常见的激活函数，该提出的激活函数表现出色，是当前领域的状态。该激活函数可以应用于各种神经网络中，where activation functions are necessary。<details>
<summary>Abstract</summary>
Activation functions play an essential role in neural networks. They provide the non-linearity for the networks. Therefore, their properties are important for neural networks' accuracy and running performance. In this paper, we present a novel signed and truncated logarithm function as activation function. The proposed activation function has significantly better mathematical properties, such as being odd function, monotone, differentiable, having unbounded value range, and a continuous nonzero gradient. These properties make it an excellent choice as an activation function. We compare it with other well-known activation functions in several well-known neural networks. The results confirm that it is the state-of-the-art. The suggested activation function can be applied in a large range of neural networks where activation functions are necessary.
</details>
<details>
<summary>摘要</summary>
aktivasi funksjoner spiller en sentral rolle i neurale nettverk. De gi non-lineære egenskaper til nettverkene. Dette gjør at egenskapene deres er viktig for nettverkeneksakkurat og løpske prestasjoner. I denne artikkelen presenteres en ny signert og avkortet logaritmfunksjon som aktivasjonsfunksjon. Den foreslåtte aktivasjonsfunksjonen har betydelig bedre matematiske egenskaper, som å være odd function, monoton, differensiabel, ha uavgrenset verdiområde og en kontinuerlig ukent zero gradient. Disse egenskapene gjør den til en utmerket valg som aktivasjonsfunksjon. Vi sammenligner den med andre velkjente aktivasjonsfunksjoner i flere velkjente neurale nettverk. Resultatene bekrefter at den er state-of-the-art. Foreslåtte aktivasjonsfunksjon kan bli brukt i et stort spekter av neurale nettverk der aktivasjonsfunksjoner er nødvendige.
</details></li>
</ul>
<hr>
<h2 id="Does-fine-tuning-GPT-3-with-the-OpenAI-API-leak-personally-identifiable-information"><a href="#Does-fine-tuning-GPT-3-with-the-OpenAI-API-leak-personally-identifiable-information" class="headerlink" title="Does fine-tuning GPT-3 with the OpenAI API leak personally-identifiable information?"></a>Does fine-tuning GPT-3 with the OpenAI API leak personally-identifiable information?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16382">http://arxiv.org/abs/2307.16382</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/albertsun1/gpt3-pii-attacks">https://github.com/albertsun1/gpt3-pii-attacks</a></li>
<li>paper_authors: Albert Yu Sun, Eliott Zemour, Arushi Saxena, Udith Vaidyanathan, Eric Lin, Christian Lau, Vaikkunth Mugunthan</li>
<li>for: 本研究旨在探讨 OpenAI 的 GPT-3 模型是否会在 fine-tuning 过程中记忆和泄露敏感信息。</li>
<li>methods: 我们使用了 Naive 提示方法和实用的 Autocomplete 任务来研究 GPT-3 在 fine-tuning 过程中是否会记忆和泄露 Personally Identifiable Information (PII)。</li>
<li>results: 我们发现，在 fine-tuning GPT-3 模型进行分类和 Autocomplete 任务时，模型会记忆和泄露来自原始 fine-tuning 数据集中的敏感信息。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Machine learning practitioners often fine-tune generative pre-trained models like GPT-3 to improve model performance at specific tasks. Previous works, however, suggest that fine-tuned machine learning models memorize and emit sensitive information from the original fine-tuning dataset. Companies such as OpenAI offer fine-tuning services for their models, but no prior work has conducted a memorization attack on any closed-source models. In this work, we simulate a privacy attack on GPT-3 using OpenAI's fine-tuning API. Our objective is to determine if personally identifiable information (PII) can be extracted from this model. We (1) explore the use of naive prompting methods on a GPT-3 fine-tuned classification model, and (2) we design a practical word generation task called Autocomplete to investigate the extent of PII memorization in fine-tuned GPT-3 within a real-world context. Our findings reveal that fine-tuning GPT3 for both tasks led to the model memorizing and disclosing critical personally identifiable information (PII) obtained from the underlying fine-tuning dataset. To encourage further research, we have made our codes and datasets publicly available on GitHub at: https://github.com/albertsun1/gpt3-pii-attacks
</details>
<details>
<summary>摘要</summary>
机器学习实践者经常微调生成预训练模型如GPT-3以提高模型在特定任务中的性能。先前的研究，然而，表明微调机器学习模型会将敏感信息从原始微调 dataset 内存储和频繁发送出来。如OpenAI提供的微调服务，但前一次的工作没有对任何关闭源代码模型进行 memorization 攻击。在这个工作中，我们模拟了一个隐私攻击，以查看GPT-3是否可以从这个模型中提取 personally identifiable information (PII)。我们（1）探索了使用简单提示方法在GPT-3微调分类模型上，并（2）我们设计了一个实用的字串生成任务，called Autocomplete，以调查微调GPT-3中PII的储存情况。我们的结果显示，对这两个任务进行微调GPT-3都会将模型将敏感信息从基础微调 dataset 内存储和频繁发送出来。为了鼓励更多的研究，我们在 GitHub 上公开了我们的代码和数据，请见：https://github.com/albertsun1/gpt3-pii-attacks。
</details></li>
</ul>
<hr>
<h2 id="UniAP-Unifying-Inter-and-Intra-Layer-Automatic-Parallelism-by-Mixed-Integer-Quadratic-Programming"><a href="#UniAP-Unifying-Inter-and-Intra-Layer-Automatic-Parallelism-by-Mixed-Integer-Quadratic-Programming" class="headerlink" title="UniAP: Unifying Inter- and Intra-Layer Automatic Parallelism by Mixed Integer Quadratic Programming"></a>UniAP: Unifying Inter- and Intra-Layer Automatic Parallelism by Mixed Integer Quadratic Programming</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16375">http://arxiv.org/abs/2307.16375</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Lin, Ke Wu, Jun Li, Wu-Jun Li</li>
<li>for: 提高深度学习模型的训练效率， automatize parallel strategy searching process.</li>
<li>methods: 使用杂度二次函数Programming（Mixed Integer Quadratic Programming，MIQP）统一inter-和intra-层自动并行，Search for globally optimal parallel strategy.</li>
<li>results: 与现有方法比较，UniAP在四种Transformer-like模型上提高了通过put和减少策略搜索时间，最高提高1.70倍和16倍。<details>
<summary>Abstract</summary>
Deep learning models have demonstrated impressive performance in various domains. However, the prolonged training time of these models remains a critical problem. Manually designed parallel training strategies could enhance efficiency but require considerable time and deliver little flexibility. Hence, automatic parallelism is proposed to automate the parallel strategy searching process. Even so, existing approaches suffer from sub-optimal strategy space because they treat automatic parallelism as two independent stages, namely inter- and intra-layer parallelism. To address this issue, we propose UniAP, which utilizes mixed integer quadratic programming to unify inter- and intra-layer automatic parallelism. To the best of our knowledge, UniAP is the first work to unify these two categories to search for a globally optimal strategy. The experimental results show that UniAP outperforms state-of-the-art methods by up to 1.70$\times$ in throughput and reduces strategy searching time by up to 16$\times$ across four Transformer-like models.
</details>
<details>
<summary>摘要</summary>
为解决这个问题，我们提出了UniAP，它使用混合整数二次函数编程来统一间层和内层自动并行。根据我们知道，UniAP是首次将这两个类别统一到搜索全球最佳策略。实验结果表明，UniAP在四种Transformer-like模型上比州先进方法提高了1.70倍的 Throughput，并将搜索策略时间减少了16倍。
</details></li>
</ul>
<hr>
<h2 id="BearingPGA-Net-A-Lightweight-and-Deployable-Bearing-Fault-Diagnosis-Network-via-Decoupled-Knowledge-Distillation-and-FPGA-Acceleration"><a href="#BearingPGA-Net-A-Lightweight-and-Deployable-Bearing-Fault-Diagnosis-Network-via-Decoupled-Knowledge-Distillation-and-FPGA-Acceleration" class="headerlink" title="BearingPGA-Net: A Lightweight and Deployable Bearing Fault Diagnosis Network via Decoupled Knowledge Distillation and FPGA Acceleration"></a>BearingPGA-Net: A Lightweight and Deployable Bearing Fault Diagnosis Network via Decoupled Knowledge Distillation and FPGA Acceleration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16363">http://arxiv.org/abs/2307.16363</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/asdvfghg/bearingpga-net">https://github.com/asdvfghg/bearingpga-net</a></li>
<li>paper_authors: Jing-Xiao Liao, Sheng-Lai Wei, Chen-Long Xie, Tieyong Zeng, Jinwei Sun, Shiping Zhang, Xiaoge Zhang, Feng-Lei Fan</li>
<li>for: 这个研究旨在提出一个轻量级且可部署的扩展网络模型，以应对滚统疲势诊断中的大型模型和复杂计算问题。</li>
<li>methods: 我们使用了一个已经受训的大型模型，透过分离知识传授来训练我们的 BearingPGA-Net 模型。我们还设计了一个 FPGA 加速方案，使用 Verilog 进行自适应量化和专案可程式逻辑门的设计，以提高计算速度。</li>
<li>results: 我们的部署方案在 CPU 上进行诊断 Speed 比较，得到了超过 200 倍的提升，而且在独立收集的滚统数据上保持了适用率下降在 0.4% 以下。我们的代码可以在 <a target="_blank" rel="noopener" href="https://github.com/asdvfghg/BearingPGA-Net">https://github.com/asdvfghg/BearingPGA-Net</a> 上获取。<details>
<summary>Abstract</summary>
Deep learning has achieved remarkable success in the field of bearing fault diagnosis. However, this success comes with larger models and more complex computations, which cannot be transferred into industrial fields requiring models to be of high speed, strong portability, and low power consumption. In this paper, we propose a lightweight and deployable model for bearing fault diagnosis, referred to as BearingPGA-Net, to address these challenges. Firstly, aided by a well-trained large model, we train BearingPGA-Net via decoupled knowledge distillation. Despite its small size, our model demonstrates excellent fault diagnosis performance compared to other lightweight state-of-the-art methods. Secondly, we design an FPGA acceleration scheme for BearingPGA-Net using Verilog. This scheme involves the customized quantization and designing programmable logic gates for each layer of BearingPGA-Net on the FPGA, with an emphasis on parallel computing and module reuse to enhance the computational speed. To the best of our knowledge, this is the first instance of deploying a CNN-based bearing fault diagnosis model on an FPGA. Experimental results reveal that our deployment scheme achieves over 200 times faster diagnosis speed compared to CPU, while achieving a lower-than-0.4\% performance drop in terms of F1, Recall, and Precision score on our independently-collected bearing dataset. Our code is available at \url{https://github.com/asdvfghg/BearingPGA-Net}.
</details>
<details>
<summary>摘要</summary>
深度学习在轮子短circuit故障诊断方面已经取得了非常出色的成绩。然而，这些成绩是由更大的模型和更复杂的计算所支持的，这些模型在工业环境中不可能实现高速、强可移植和低功耗的要求。在这篇论文中，我们提出了一种轻量级可部署的模型，称为BearingPGA-Net，以解决这些挑战。首先，我们通过使用已经训练过的大型模型，将BearingPGA-Net通过分离知识填充训练。尽管它的模型量小，但BearingPGA-Net仍然在其他轻量级现状的方法中表现出色，并且在我们独立收集的轮子数据集上达到了0.4%以下的F1、Recall和Precision分数。其次，我们为BearingPGA-Net设计了FPGA加速方案，使用Verilog编程。这种方案包括为每层BearingPGA-Net的FPGA自定义量化和设计可编程逻辑门。我们强调了并行计算和模块重用，以提高计算速度。到目前为止，这是第一次将CNN基于轮子短circuit故障诊断模型部署到FPGA上。实验结果显示，我们的部署方案可以在CPU上进行200倍以上的诊断速度提升，同时在F1、Recall和Precision分数上保持下降在0.4%以下。我们的代码可以在<https://github.com/asdvfghg/BearingPGA-Net>上获取。
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-and-Analyzing-Robust-Point-Cloud-Recognition-Bag-of-Tricks-for-Defending-Adversarial-Examples"><a href="#Benchmarking-and-Analyzing-Robust-Point-Cloud-Recognition-Bag-of-Tricks-for-Defending-Adversarial-Examples" class="headerlink" title="Benchmarking and Analyzing Robust Point Cloud Recognition: Bag of Tricks for Defending Adversarial Examples"></a>Benchmarking and Analyzing Robust Point Cloud Recognition: Bag of Tricks for Defending Adversarial Examples</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16361">http://arxiv.org/abs/2307.16361</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qiufan319/benchmark_pc_attack">https://github.com/qiufan319/benchmark_pc_attack</a></li>
<li>paper_authors: Qiufan Ji, Lin Wang, Cong Shi, Shengshan Hu, Yingying Chen, Lichao Sun</li>
<li>for: 本研究旨在提高深度神经网络（DNNs）在3D点云识别 task 中的鲁棒性，即使面临攻击者可能会通过添加、移动或删除点来生成攻击性的点云数据。</li>
<li>methods: 本研究首先建立了一个完整的、彻底的3D点云攻击 robustness 评估标准，以evaluate 攻击和防御策略的效果。然后，我们收集了现有的防御技巧，并进行了广泛和系统的实验，以确定一个有效的组合方法。最后，我们提出了一种hybrid training augmentation方法，考虑了多种类型的点云攻击例子，并将其与针对性训练相结合，以提高鲁棒性。</li>
<li>results: 通过 combining 多种防御技巧和 hybrid training augmentation方法，我们构建了一个更加鲁棒的防御框架，其中83.45%的批处率能够抵抗多种攻击。这表明我们的防御框架具有启发人的鲁棒性，可以帮助实现可靠的学习。我们的代码库在：\url{<a target="_blank" rel="noopener" href="https://github.com/qiufan319/benchmark_pc_attack.git%7D%E3%80%82">https://github.com/qiufan319/benchmark_pc_attack.git}。</a><details>
<summary>Abstract</summary>
Deep Neural Networks (DNNs) for 3D point cloud recognition are vulnerable to adversarial examples, threatening their practical deployment. Despite the many research endeavors have been made to tackle this issue in recent years, the diversity of adversarial examples on 3D point clouds makes them more challenging to defend against than those on 2D images. For examples, attackers can generate adversarial examples by adding, shifting, or removing points. Consequently, existing defense strategies are hard to counter unseen point cloud adversarial examples. In this paper, we first establish a comprehensive, and rigorous point cloud adversarial robustness benchmark to evaluate adversarial robustness, which can provide a detailed understanding of the effects of the defense and attack methods. We then collect existing defense tricks in point cloud adversarial defenses and then perform extensive and systematic experiments to identify an effective combination of these tricks. Furthermore, we propose a hybrid training augmentation methods that consider various types of point cloud adversarial examples to adversarial training, significantly improving the adversarial robustness. By combining these tricks, we construct a more robust defense framework achieving an average accuracy of 83.45\% against various attacks, demonstrating its capability to enabling robust learners. Our codebase are open-sourced on: \url{https://github.com/qiufan319/benchmark_pc_attack.git}.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNNs） для三维点云识别容易受到攻击，这在实际应用中受到威胁。尽管过去几年来有很多研究努力用于解决这一问题，但三维点云上的攻击者可以通过添加、移动或删除点来生成攻击性例子，使现有的防御策略很难对未看到的点云攻击性例子进行防御。在这篇论文中，我们首先建立了一个完整、严格的点云攻击Robustness指标，用于评估防御 robustness，可以为我们提供深入的理解防御和攻击方法的影响。然后，我们收集了现有的点云防御技巧，并进行了广泛和系统的实验，以确定有效的组合方法。此外，我们提议一种混合培育方法，考虑了多种点云攻击性例子，并将其与培育训练相结合，从而显著提高了对攻击的鲁棒性。通过组合这些技巧，我们构建了一个更加鲁棒的防御框架，其中83.45%的检测精度可以抵抗多种攻击，这表明了它的可行性。我们的代码库将在：\url{https://github.com/qiufan319/benchmark_pc_attack.git}上开源。
</details></li>
</ul>
<hr>
<h2 id="Probabilistically-robust-conformal-prediction"><a href="#Probabilistically-robust-conformal-prediction" class="headerlink" title="Probabilistically robust conformal prediction"></a>Probabilistically robust conformal prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16360">http://arxiv.org/abs/2307.16360</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/1995subhankar1995/PRCP">https://github.com/1995subhankar1995/PRCP</a></li>
<li>paper_authors: Subhankar Ghosh, Yuanjie Shi, Taha Belkhouja, Yan Yan, Jana Doppa, Brian Jones</li>
<li>for: 这篇论文目的是研究机器学习分类器中的不确定性评估，包括深度神经网络。</li>
<li>methods: 这篇论文使用了内在测试数据的概率性Robust Conformal Prediction（PRCP）框架，以提高机器学习分类器的不确定性评估。</li>
<li>results: 这篇论文的实验结果显示，使用了 novel adaptive PRCP（aPRCP）算法可以实现更好的不确定性评估，并且在CIFAR-10、CIFAR-100和ImageNet datasets上使用深度神经网络进行了试验。<details>
<summary>Abstract</summary>
Conformal prediction (CP) is a framework to quantify uncertainty of machine learning classifiers including deep neural networks. Given a testing example and a trained classifier, CP produces a prediction set of candidate labels with a user-specified coverage (i.e., true class label is contained with high probability). Almost all the existing work on CP assumes clean testing data and there is not much known about the robustness of CP algorithms w.r.t natural/adversarial perturbations to testing examples. This paper studies the problem of probabilistically robust conformal prediction (PRCP) which ensures robustness to most perturbations around clean input examples. PRCP generalizes the standard CP (cannot handle perturbations) and adversarially robust CP (ensures robustness w.r.t worst-case perturbations) to achieve better trade-offs between nominal performance and robustness. We propose a novel adaptive PRCP (aPRCP) algorithm to achieve probabilistically robust coverage. The key idea behind aPRCP is to determine two parallel thresholds, one for data samples and another one for the perturbations on data (aka "quantile-of-quantile" design). We provide theoretical analysis to show that aPRCP algorithm achieves robust coverage. Our experiments on CIFAR-10, CIFAR-100, and ImageNet datasets using deep neural networks demonstrate that aPRCP achieves better trade-offs than state-of-the-art CP and adversarially robust CP algorithms.
</details>
<details>
<summary>摘要</summary>
《匹配预测（CP）是一种框架，用于评估机器学习分类器，包括深度神经网络的不确定性。给定测试示例和已经训练过的分类器，CP 生成一个包含真正类别的可能性的预测集。现有大多数 CP 研究假设测试数据是干净的，并没有很多关于 CP 算法对自然/敌意扰动的Robustness的研究。这篇文章研究了 probablistically Robust Conformal Prediction（PRCP）问题，PRCP 可以 Ensure robustness to most perturbations around clean input examples。PRCP 扩展了标准 CP（不能处理扰动）和敌意Robust CP（对最差情况扰动 Ensure robustness），以 achieve better trade-offs between nominal performance and robustness。我们提出了一种 noval adaptive PRCP（aPRCP）算法，以实现 probabilistically robust coverage。aPRCP 的关键思想是在数据示例和数据扰动之间设置两个平行的阈值（即 "quantile-of-quantile" 设计）。我们提供了理论分析，证明 aPRCP 算法实现了 robust coverage。我们在 CIFAR-10、CIFAR-100 和 ImageNet  datasets上使用深度神经网络进行实验，demonstrate that aPRCP 实现了更好的 trade-offs than state-of-the-art CP 和 adversarially robust CP 算法。
</details></li>
</ul>
<hr>
<h2 id="Moreau-Yoshida-Variational-Transport-A-General-Framework-For-Solving-Regularized-Distributional-Optimization-Problems"><a href="#Moreau-Yoshida-Variational-Transport-A-General-Framework-For-Solving-Regularized-Distributional-Optimization-Problems" class="headerlink" title="Moreau-Yoshida Variational Transport: A General Framework For Solving Regularized Distributional Optimization Problems"></a>Moreau-Yoshida Variational Transport: A General Framework For Solving Regularized Distributional Optimization Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16358">http://arxiv.org/abs/2307.16358</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dai Hai Nguyen, Tetsuya Sakurai</li>
<li>for:  solves a regularized distributional optimization problem, which is widely used in machine learning and statistics.</li>
<li>methods:  proposes a novel method called Moreau-Yoshida Variational Transport (MYVT) to solve the regularized distributional optimization problem. The method uses the Moreau-Yoshida envelope for a smooth approximation of the nonsmooth function in the objective, and then reformulates the approximate problem as a concave-convex saddle point problem.</li>
<li>results:  provides theoretical analyses and experimental results to demonstrate the effectiveness of the proposed method.<details>
<summary>Abstract</summary>
We consider a general optimization problem of minimizing a composite objective functional defined over a class of probability distributions. The objective is composed of two functionals: one is assumed to possess the variational representation and the other is expressed in terms of the expectation operator of a possibly nonsmooth convex regularizer function. Such a regularized distributional optimization problem widely appears in machine learning and statistics, such as proximal Monte-Carlo sampling, Bayesian inference and generative modeling, for regularized estimation and generation.   We propose a novel method, dubbed as Moreau-Yoshida Variational Transport (MYVT), for solving the regularized distributional optimization problem. First, as the name suggests, our method employs the Moreau-Yoshida envelope for a smooth approximation of the nonsmooth function in the objective. Second, we reformulate the approximate problem as a concave-convex saddle point problem by leveraging the variational representation, and then develope an efficient primal-dual algorithm to approximate the saddle point. Furthermore, we provide theoretical analyses and report experimental results to demonstrate the effectiveness of the proposed method.
</details>
<details>
<summary>摘要</summary>
我们考虑一个总体优化问题，即将一个集合中的概率分布最小化一个复合目标函数。该目标函数由两个函数组成：一个假设具有变量表示，另一个是一个可能非光滑的凸违反函数。这种凸违反化优化问题广泛出现在机器学习和统计中，如距离 Monte Carlo 抽样、 bayesian 推理和生成模型，用于凸违反估计和生成。我们提出一种新方法，名为 Moreau-Yoshida 变量运输（MYVT），用于解决凸违反化优化问题。首先，我们使用 Moreau-Yoshida 缓和函数来简化目标函数中的非光滑函数。然后，我们将问题转换为一个凹凸融合点问题，并使用变量表示来减少问题的复杂性。最后，我们开发了一个高效的 primal-dual 算法来近似融合点。此外，我们还提供了理论分析和实验结果，以证明我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Hypertension-Detection-From-High-Dimensional-Representation-of-Photoplethysmogram-Signals"><a href="#Hypertension-Detection-From-High-Dimensional-Representation-of-Photoplethysmogram-Signals" class="headerlink" title="Hypertension Detection From High-Dimensional Representation of Photoplethysmogram Signals"></a>Hypertension Detection From High-Dimensional Representation of Photoplethysmogram Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02425">http://arxiv.org/abs/2308.02425</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/navidhasanzadeh/hypertension_ppg">https://github.com/navidhasanzadeh/hypertension_ppg</a></li>
<li>paper_authors: Navid Hasanzadeh, Shahrokh Valaee, Hojjat Salehinejad</li>
<li>For: 这个研究旨在探索用 Photoplethysmogram (PPG) 讯号来检测高血压 (hypertension) 的可能性。* Methods: 本研究使用高维度表示技术基于随机扩散 kernel，将 PPG 讯号转换为高维度特征表示。* Results: 研究结果显示，这种关系超出心率和血压，证明了高血压检测的可能性。此外，使用扩散 kernel 的变数对于时间序列特征提取器的表现亦较前一 studies 和现有的深度学习模型更好。<details>
<summary>Abstract</summary>
Hypertension is commonly referred to as the "silent killer", since it can lead to severe health complications without any visible symptoms. Early detection of hypertension is crucial in preventing significant health issues. Although some studies suggest a relationship between blood pressure and certain vital signals, such as Photoplethysmogram (PPG), reliable generalization of the proposed blood pressure estimation methods is not yet guaranteed. This lack of certainty has resulted in some studies doubting the existence of such relationships, or considering them weak and limited to heart rate and blood pressure. In this paper, a high-dimensional representation technique based on random convolution kernels is proposed for hypertension detection using PPG signals. The results show that this relationship extends beyond heart rate and blood pressure, demonstrating the feasibility of hypertension detection with generalization. Additionally, the utilized transform using convolution kernels, as an end-to-end time-series feature extractor, outperforms the methods proposed in the previous studies and state-of-the-art deep learning models.
</details>
<details>
<summary>摘要</summary>
高血压通常被称为"无音杀手"，因为它可以导致严重的健康问题而无需任何可见的 симптом。早期检测高血压非常重要，以预防严重的健康问题。虽然一些研究表明血压和某些生命函数之间存在关系，如脉搏图（PPG），但可靠地总结这些血压估计方法的可靠性并不是 garantizado。这种不确定性导致了一些研究质疑这些关系的存在，或者认为它们是弱的和限制于心率和血压。在这篇论文中，一种基于随机杂化核心的高维表示技术被提议用于高血压检测使用PPG信号。结果表明，这种关系超出了心率和血压，表明高血压检测的可能性。此外，使用杂化核心来实现端到端时间序列特征提取，比前一些研究和当前最佳深度学习模型更高效。
</details></li>
</ul>
<hr>
<h2 id="Rating-based-Reinforcement-Learning"><a href="#Rating-based-Reinforcement-Learning" class="headerlink" title="Rating-based Reinforcement Learning"></a>Rating-based Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16348">http://arxiv.org/abs/2307.16348</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Devin White, Mingkang Wu, Ellen Novoseller, Vernon Lawhern, Nick Waytowich, Yongcan Cao</li>
<li>for: 这个论文是为了开发一种基于评分的 reinforcement learning 方法，利用人类评分来获得人类指导。</li>
<li>methods: 该方法基于人类评分各个轨迹的评价，不需要对样本对比进行相对评价。它基于新的预测模型和多类损失函数。</li>
<li>results: 通过 synthetic 评分和实际人类评分进行多个实验研究，证明了该方法的效果和优势。<details>
<summary>Abstract</summary>
This paper develops a novel rating-based reinforcement learning approach that uses human ratings to obtain human guidance in reinforcement learning. Different from the existing preference-based and ranking-based reinforcement learning paradigms, based on human relative preferences over sample pairs, the proposed rating-based reinforcement learning approach is based on human evaluation of individual trajectories without relative comparisons between sample pairs. The rating-based reinforcement learning approach builds on a new prediction model for human ratings and a novel multi-class loss function. We conduct several experimental studies based on synthetic ratings and real human ratings to evaluate the effectiveness and benefits of the new rating-based reinforcement learning approach.
</details>
<details>
<summary>摘要</summary>
这个论文提出了一种新的评分基于束缚学习方法，该方法使用人类评分来获得人类指导在束缚学习中。与现有的偏好基于样本对比和排名基于样本对比不同，提出的评分基于束缚学习方法是基于人类评估个体轨迹而不需要对样本对比进行相对评价。该方法建立在新的人类评分预测模型和多类损失函数之上。我们通过对 sintetic 评分和实际人类评分进行多次实验来评估新评分基于束缚学习方法的效果和优势。
</details></li>
</ul>
<hr>
<h2 id="Proof-of-Federated-Learning-Subchain-Free-Partner-Selection-Subchain-Based-on-Federated-Learning"><a href="#Proof-of-Federated-Learning-Subchain-Free-Partner-Selection-Subchain-Based-on-Federated-Learning" class="headerlink" title="Proof-of-Federated-Learning-Subchain: Free Partner Selection Subchain Based on Federated Learning"></a>Proof-of-Federated-Learning-Subchain: Free Partner Selection Subchain Based on Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16342">http://arxiv.org/abs/2307.16342</a></li>
<li>repo_url: None</li>
<li>paper_authors: Boyang Li, Bingyu Shen, Qing Lu, Taeho Jung, Yiyu Shi</li>
<li>for: 这篇论文主要是为了提出一种新的分布式共识机制，以替代之前的Proof-of-Deep-Learning(PoDL)机制，以更有效地使用能源并维护区块链。</li>
<li>methods: 该论文提出了一种名为Proof-of-Federated-Learning-Subchain(PoFLSC)的新的共识机制，它使用了一个子链来记录训练、挑战和审核活动，并强调了合理的数据集的选择对伙伴的重要性。</li>
<li>results: 在 simulate 20 个矿工的情况下，该论文表明了 PoFLSC 机制的有效性，当减少了pool size时， Pool 中的矿工会根据其 Shapley Value (SV) 的高低来选择合适的矿工。在实验中，PoFLSC 机制帮助了子链管理员了解储值优先级和核心分区的参与者，以建立和维护竞争性的子链。<details>
<summary>Abstract</summary>
The continuous thriving of the Blockchain society motivates research in novel designs of schemes supporting cryptocurrencies. Previously multiple Proof-of-Deep-Learning(PoDL) consensuses have been proposed to replace hashing with useful work such as deep learning model training tasks. The energy will be more efficiently used while maintaining the ledger. However deep learning models are problem-specific and can be extremely complex. Current PoDL consensuses still require much work to realize in the real world. In this paper, we proposed a novel consensus named Proof-of-Federated-Learning-Subchain(PoFLSC) to fill the gap. We applied a subchain to record the training, challenging, and auditing activities and emphasized the importance of valuable datasets in partner selection. We simulated 20 miners in the subchain to demonstrate the effectiveness of PoFLSC. When we reduce the pool size concerning the reservation priority order, the drop rate difference in the performance in different scenarios further exhibits that the miner with a higher Shapley Value (SV) will gain a better opportunity to be selected when the size of the subchain pool is limited. In the conducted experiments, the PoFLSC consensus supported the subchain manager to be aware of reservation priority and the core partition of contributors to establish and maintain a competitive subchain.
</details>
<details>
<summary>摘要</summary>
continuous 社区的坚持发展激发了研究新的方案，以支持criptocurrency的 schemes。 previously， multiple  Proof-of-Deep-Learning（PoDL）consensuses have been proposed to replace hashing with useful work such as deep learning model training tasks. The energy will be more efficiently used while maintaining the ledger. However, deep learning models are problem-specific and can be extremely complex. Current PoDL consensuses still require much work to realize in the real world. In this paper, we proposed a novel consensus named Proof-of-Federated-Learning-Subchain（PoFLSC）to fill the gap. We applied a subchain to record the training, challenging, and auditing activities and emphasized the importance of valuable datasets in partner selection. We simulated 20 miners in the subchain to demonstrate the effectiveness of PoFLSC. When we reduce the pool size concerning the reservation priority order, the drop rate difference in the performance in different scenarios further exhibits that the miner with a higher Shapley Value (SV) will gain a better opportunity to be selected when the size of the subchain pool is limited. In the conducted experiments, the PoFLSC consensus supported the subchain manager to be aware of reservation priority and the core partition of contributors to establish and maintain a competitive subchain.Note: Please note that the translation is in Simplified Chinese, which is one of the two standard versions of Chinese used in mainland China and Singapore. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Theoretically-Principled-Trade-off-for-Stateful-Defenses-against-Query-Based-Black-Box-Attacks"><a href="#Theoretically-Principled-Trade-off-for-Stateful-Defenses-against-Query-Based-Black-Box-Attacks" class="headerlink" title="Theoretically Principled Trade-off for Stateful Defenses against Query-Based Black-Box Attacks"></a>Theoretically Principled Trade-off for Stateful Defenses against Query-Based Black-Box Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16331">http://arxiv.org/abs/2307.16331</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ashish Hooda, Neal Mangaokar, Ryan Feng, Kassem Fawaz, Somesh Jha, Atul Prakash</li>
<li>for: 本研究旨在提供对状态防御系统的 theoretically Characterization，以优化它们的抗击攻击性能。</li>
<li>methods: 本研究使用了一种普遍的 feature extractors 类型，并对其进行了系统的分析和优化。</li>
<li>results: 研究发现，状态防御系统的抗击攻击性能与 feature extractors 的选择和阈值设置有直接的关系。同时，对于某些特定的攻击方法，状态防御系统可以具有较高的抗击攻击性能。<details>
<summary>Abstract</summary>
Adversarial examples threaten the integrity of machine learning systems with alarming success rates even under constrained black-box conditions. Stateful defenses have emerged as an effective countermeasure, detecting potential attacks by maintaining a buffer of recent queries and detecting new queries that are too similar. However, these defenses fundamentally pose a trade-off between attack detection and false positive rates, and this trade-off is typically optimized by hand-picking feature extractors and similarity thresholds that empirically work well. There is little current understanding as to the formal limits of this trade-off and the exact properties of the feature extractors/underlying problem domain that influence it. This work aims to address this gap by offering a theoretical characterization of the trade-off between detection and false positive rates for stateful defenses. We provide upper bounds for detection rates of a general class of feature extractors and analyze the impact of this trade-off on the convergence of black-box attacks. We then support our theoretical findings with empirical evaluations across multiple datasets and stateful defenses.
</details>
<details>
<summary>摘要</summary>
针对机器学习系统的敌对示例具有较高的攻击成功率，即使在封装黑盒条件下。状态防御技术已经出现为有效的对抗手段，通过维护最近几个查询的缓存来检测潜在攻击，并且检测新的查询是否过于相似。然而，这些防御技术存在一定的负面影响和假阳性率的负担，这些负担通常通过手动选择特征提取器和相似性阈值来优化。现在，我们对这种负面影响和假阳性率的负担的正负限制尚未有很好的理解，以及特征提取器和下游问题领域的影响。这项工作的目标是提供状态防御技术的检测和假阳性率之间的正负限制的理论Characterization。我们提供一类通用特征提取器的检测率的Upper bound，并分析黑盒攻击的收敛行为受到这种负面影响的影响。最后，我们通过多个数据集和状态防御技术的实证验证我们的理论发现。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-ChatGPT-and-GPT-4-for-Visual-Programming"><a href="#Evaluating-ChatGPT-and-GPT-4-for-Visual-Programming" class="headerlink" title="Evaluating ChatGPT and GPT-4 for Visual Programming"></a>Evaluating ChatGPT and GPT-4 for Visual Programming</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02522">http://arxiv.org/abs/2308.02522</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adish Singla</li>
<li>for: 这种研究的目的是确定现代生成模型在视觉编程领域是否有高水平的表现，与文本编程领域的 Python 编程相比。</li>
<li>methods: 这种研究使用了两种模型：ChatGPT 和 GPT-4，并对它们在不同的场景下进行评估，以确定它们在视觉编程领域的表现。</li>
<li>results: 研究发现，这些模型在视觉编程领域表现不佳，尤其是在结合空间、逻辑和编程技能方面表现不佳，这些技能是视觉编程中非常重要的。这些结果还提供了未来发展生成模型在视觉编程领域的推动性的方向。<details>
<summary>Abstract</summary>
Generative AI and large language models have the potential to drastically improve the landscape of computing education by automatically generating personalized feedback and content. Recent works have studied the capabilities of these models for different programming education scenarios; however, these works considered only text-based programming, in particular, Python programming. Consequently, they leave open the question of how well these models would perform in visual programming domains popularly used for K-8 programming education. The main research question we study is: Do state-of-the-art generative models show advanced capabilities in visual programming on par with their capabilities in text-based Python programming? In our work, we evaluate two models, ChatGPT (based on GPT-3.5) and GPT-4, in visual programming domains for various scenarios and assess performance using expert-based annotations. In particular, we base our evaluation using reference tasks from the domains of Hour of Code: Maze Challenge by Code-dot-org and Karel. Our results show that these models perform poorly and struggle to combine spatial, logical, and programming skills crucial for visual programming. These results also provide exciting directions for future work on developing techniques to improve the performance of generative models in visual programming.
</details>
<details>
<summary>摘要</summary>
将文本翻译成简化中文。大数据生成AI和大语言模型有可能在计算教育领域带来巨大改变，通过自动生成个性化反馈和内容来提高学习效率。最近的研究已经研究了这些模型在不同的编程教育场景中的能力，但是这些研究仅考虑了文本编程，尤其是Python编程。因此，它们留下了如何在视觉编程领域中的表现的问题。我们的研究问题是：现代生成模型在视觉编程领域是否有 similarly advanced capabilities ？在我们的工作中，我们评估了两个模型：ChatGPT（基于 GPT-3.5）和 GPT-4，在不同的视觉编程场景中的表现。我们基于Reference Tasks from the domains of Hour of Code: Maze Challenge by Code-dot-org和Karel进行评估。我们的结果表明，这些模型在视觉编程中表现不佳，它们很难结合空间、逻辑和编程技能。这些结果还提供了未来开发生成模型在视觉编程中提高表现的激动人心的方向。
</details></li>
</ul>
<hr>
<h2 id="RoseNNa-A-performant-portable-library-for-neural-network-inference-with-application-to-computational-fluid-dynamics"><a href="#RoseNNa-A-performant-portable-library-for-neural-network-inference-with-application-to-computational-fluid-dynamics" class="headerlink" title="RoseNNa: A performant, portable library for neural network inference with application to computational fluid dynamics"></a>RoseNNa: A performant, portable library for neural network inference with application to computational fluid dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16322">http://arxiv.org/abs/2307.16322</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/comp-physics/roseNNa">https://github.com/comp-physics/roseNNa</a></li>
<li>paper_authors: Ajay Bati, Spencer H. Bryngelson</li>
<li>for: 这个论文主要用于探讨如何将神经网络学习推广到计算流体动力学（CFD）领域，以提高计算效率。</li>
<li>methods: 论文使用了多层感知器（MLP）和长短期记忆（LSTM）循环网络架构来表示下层物理效应，如湍流。</li>
<li>results: 实验结果显示，使用roseNNa库可以提高神经网络推广的效率，并且在使用PyTorch和libtorch时Remove overhead cost of API calls后，roseNNa仍然能够具有较高的速度。速度提高范围为约10倍到2倍。<details>
<summary>Abstract</summary>
The rise of neural network-based machine learning ushered in high-level libraries, including TensorFlow and PyTorch, to support their functionality. Computational fluid dynamics (CFD) researchers have benefited from this trend and produced powerful neural networks that promise shorter simulation times. For example, multilayer perceptrons (MLPs) and Long Short Term Memory (LSTM) recurrent-based (RNN) architectures can represent sub-grid physical effects, like turbulence. Implementing neural networks in CFD solvers is challenging because the programming languages used for machine learning and CFD are mostly non-overlapping, We present the roseNNa library, which bridges the gap between neural network inference and CFD. RoseNNa is a non-invasive, lightweight (1000 lines), and performant tool for neural network inference, with focus on the smaller networks used to augment PDE solvers, like those of CFD, which are typically written in C/C++ or Fortran. RoseNNa accomplishes this by automatically converting trained models from typical neural network training packages into a high-performance Fortran library with C and Fortran APIs. This reduces the effort needed to access trained neural networks and maintains performance in the PDE solvers that CFD researchers build and rely upon. Results show that RoseNNa reliably outperforms PyTorch (Python) and libtorch (C++) on MLPs and LSTM RNNs with less than 100 hidden layers and 100 neurons per layer, even after removing the overhead cost of API calls. Speedups range from a factor of about 10 and 2 faster than these established libraries for the smaller and larger ends of the neural network size ranges tested.
</details>
<details>
<summary>摘要</summary>
“从神经网络学习的出现开始，高水平的库，包括TensorFlow和PyTorch，支援其功能。 Computational fluid dynamics（CFD）研究人员受惠于这一趋势，生成了具有短化运算时间的强大神经网络。例如，多层感知器（MLP）和长期记忆运算（LSTM）类型的神经网络可以表现出裂隔物理效应，如湍流。将神经网络 integrate into CFD solvers 是一个挑战，因为机器学习和CFD的程式语言通常不 overlap。我们介绍了roseNNa库，它横跨神经网络评估和CFD的关系，并提供了一个非侵入性、轻量级（1000行）、高性能的工具。roseNNa 可以自动将训练好的模型从通常的神经网络训练套件转换为高性能的 Fortran 库，并提供了 C 和 Fortran API。这样可以缩短训练好的神经网络存取的努力，并维持 PDE 解析器中的性能。结果显示，roseNNa 可以高效地处理 MLP 和 LSTM RNN，即使将 API 调用的开关时间除掉。速度比较为约10倍和2倍于 Python 的 PyTorch 和 C++ 的 libtorch，对于较小的和较大的神经网络size range。”
</details></li>
</ul>
<hr>
<h2 id="Towards-Practical-Robustness-Auditing-for-Linear-Regression"><a href="#Towards-Practical-Robustness-Auditing-for-Linear-Regression" class="headerlink" title="Towards Practical Robustness Auditing for Linear Regression"></a>Towards Practical Robustness Auditing for Linear Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16315">http://arxiv.org/abs/2307.16315</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Freund, Samuel B. Hopkins</li>
<li>for: 这 paper 的目的是找到或证明一个 dataset 中小 subsets 的存在或不存在，这些 subsets 的 removing 可以使 ordinary least squares regression 的 coefficient 的符号变化。</li>
<li>methods: 这 paper 使用了一些已知的算法技术来实现这个目的，包括 mixed integer quadratically constrained optimization 和 exact greedy methods，这些方法在一些特定的情况下能够大幅提高性能。</li>
<li>results: 这 paper 的实验结果表明，这些方法在一些维度小的情况下能够提供有用的 robustness check，但是在维度大于 $3$ 的情况下，还存在 significiant computational bottlenecks，spectral algorithm 使用了来自最近的算法robust statistics 的想法，以解决这个挑战。<details>
<summary>Abstract</summary>
We investigate practical algorithms to find or disprove the existence of small subsets of a dataset which, when removed, reverse the sign of a coefficient in an ordinary least squares regression involving that dataset. We empirically study the performance of well-established algorithmic techniques for this task -- mixed integer quadratically constrained optimization for general linear regression problems and exact greedy methods for special cases. We show that these methods largely outperform the state of the art and provide a useful robustness check for regression problems in a few dimensions. However, significant computational bottlenecks remain, especially for the important task of disproving the existence of such small sets of influential samples for regression problems of dimension $3$ or greater. We make some headway on this challenge via a spectral algorithm using ideas drawn from recent innovations in algorithmic robust statistics. We summarize the limitations of known techniques in several challenge datasets to encourage further algorithmic innovation.
</details>
<details>
<summary>摘要</summary>
我团队 investigate 实用算法来找到或证明一个小集合的存在，当其从 dataset 中移除时，抵消了一个减法最小二乘问题中的 coefficient 的符号。我们对已有的算法技术进行实验研究——混合整数二次约束优化算法和特殊情况下的准确追加法。我们发现这些方法在大多数情况下表现出色，提供了有用的稳定性检查。但是，特别是在维度大于 3 的 regression 问题中，计算瓶颈仍然很大，特别是用于证明小集合的影响样本的存在。我们通过使用最近的算法robust统计技术中的spectral algorithm来做出一些进展。我们summarize 已知技术的局限性，以便鼓励更多的算法创新。
</details></li>
</ul>
<hr>
<h2 id="Mask-guided-Data-Augmentation-for-Multiparametric-MRI-Generation-with-a-Rare-Hepatocellular-Carcinoma"><a href="#Mask-guided-Data-Augmentation-for-Multiparametric-MRI-Generation-with-a-Rare-Hepatocellular-Carcinoma" class="headerlink" title="Mask-guided Data Augmentation for Multiparametric MRI Generation with a Rare Hepatocellular Carcinoma"></a>Mask-guided Data Augmentation for Multiparametric MRI Generation with a Rare Hepatocellular Carcinoma</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16314">http://arxiv.org/abs/2307.16314</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karen Sanchez, Carlos Hinojosa, Kevin Arias, Henry Arguello, Denis Kouame, Olivier Meyrignac, Adrian Basarab</li>
<li>for: 这个论文主要用于提高深度学习模型的总体性能，特别是在医疗应用中 multiparametric 数据集上。</li>
<li>methods: 该论文提出了一种新的数据扩展建议，通过一种生成深度学习方法生成多 paramagnetic（T1 血管、T1 门户和 T2）磁共振成像（MRI）图像，以模拟大规模某些肝癌细胞型。</li>
<li>results: 该论文通过对限制的 multiparametric MRI triplets 进行训练，生成了 1,000 个人工数据集和其对应的肝肿瘤面积。 Frechet Inception Distance 分数为 86.55。这种方法在 2021 年法国医学会数据扩展挑战中荣膺奖。<details>
<summary>Abstract</summary>
Data augmentation is classically used to improve the overall performance of deep learning models. It is, however, challenging in the case of medical applications, and in particular for multiparametric datasets. For example, traditional geometric transformations used in several applications to generate synthetic images can modify in a non-realistic manner the patients' anatomy. Therefore, dedicated image generation techniques are necessary in the medical field to, for example, mimic a given pathology realistically. This paper introduces a new data augmentation architecture that generates synthetic multiparametric (T1 arterial, T1 portal, and T2) magnetic resonance images (MRI) of massive macrotrabecular subtype hepatocellular carcinoma with their corresponding tumor masks through a generative deep learning approach. The proposed architecture creates liver tumor masks and abdominal edges used as input in a Pix2Pix network for synthetic data creation. The method's efficiency is demonstrated by training it on a limited multiparametric dataset of MRI triplets from $89$ patients with liver lesions to generate $1,000$ synthetic triplets and their corresponding liver tumor masks. The resulting Frechet Inception Distance score was $86.55$. The proposed approach was among the winners of the 2021 data augmentation challenge organized by the French Society of Radiology.
</details>
<details>
<summary>摘要</summary>
“数据扩充是通用深度学习模型的改进工具，但在医疗应用中具有挑战性，特别是在多 Parametric 数据集上。例如，传统的几何变换在许多应用中生成synthetic图像，可能会改变病人的解剖结构不实际。因此，医疗领域需要专门的图像生成技术，例如模拟给定疾病的真实方式。这篇文章介绍了一种新的数据扩充架构，通过生成多 Parametric（T1血管、T1门户和T2）核磁共振成像（MRI）的大规模肝癌细胞杂合型肝癌，并生成其相应的肿瘤面罩。提posed架构使用生成深度学习方法创建肝肿瘤面罩和腹部边缘，并将其作为Pix2Pix网络的输入进行 sintetic数据创建。方法的效率被证明通过对限制 multiparametric 数据集的 MRI  triplets（89名患者的肝脏病变）进行训练，生成 1,000 个 sintetic triplets和其相应的肝肿瘤面罩。得到的 Frechet Inception Distance 分数为 86.55。该方法在2021年的数据扩充挑战中获得了法国 radiology 学会的奖励。”
</details></li>
</ul>
<hr>
<h2 id="You-Shall-not-Pass-the-Zero-Gradient-Problem-in-Predict-and-Optimize-for-Convex-Optimization"><a href="#You-Shall-not-Pass-the-Zero-Gradient-Problem-in-Predict-and-Optimize-for-Convex-Optimization" class="headerlink" title="You Shall not Pass: the Zero-Gradient Problem in Predict and Optimize for Convex Optimization"></a>You Shall not Pass: the Zero-Gradient Problem in Predict and Optimize for Convex Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16304">http://arxiv.org/abs/2307.16304</a></li>
<li>repo_url: None</li>
<li>paper_authors: Grigorii Veviurko, Wendelin Böhmer, Mathijs de Weerdt</li>
<li>for: 预测和优化是一种越来越流行的决策方法，使用机器学习预测未知优化问题的参数。</li>
<li>methods: 这篇文章提出了一种新的方法，基于任务性能作为损失函数来训练预测模型。</li>
<li>results: 文章发现了一个未注意的缺陷（零梯度问题），并提出了一种解决方法，基于优化问题的数学性质。 two real-world benchmarks 被验证。<details>
<summary>Abstract</summary>
Predict and optimize is an increasingly popular decision-making paradigm that employs machine learning to predict unknown parameters of optimization problems. Instead of minimizing the prediction error of the parameters, it trains predictive models using task performance as a loss function. In the convex optimization domain, predict and optimize has seen significant progress due to recently developed methods for differentiating optimization problem solutions over the problem parameters. This paper identifies a yet unnoticed drawback of this approach -- the zero-gradient problem -- and introduces a method to solve it. The suggested method is based on the mathematical properties of differential optimization and is verified using two real-world benchmarks.
</details>
<details>
<summary>摘要</summary>
预测和优化是一种日益受欢迎的决策方法，它使用机器学习预测未知优化问题中的参数。而不是将预测参数的预测错误作为损失函数来训练预测模型，它使用任务性能作为损失函数来训练预测模型。在凸优化领域，预测和优化已经取得了显著进展，这主要归功于最近发展出的对优化问题解的方法。然而，这种方法还存在一个尚未受到注意的缺点——零梯度问题。本文描述了这个问题，并提出了一种基于数学优化的解决方法。该方法在两个实际 benchmark 中被验证。
</details></li>
</ul>
<hr>
<h2 id="Predicting-delays-in-Indian-lower-courts-using-AutoML-and-Decision-Forests"><a href="#Predicting-delays-in-Indian-lower-courts-using-AutoML-and-Decision-Forests" class="headerlink" title="Predicting delays in Indian lower courts using AutoML and Decision Forests"></a>Predicting delays in Indian lower courts using AutoML and Decision Forests</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16285">http://arxiv.org/abs/2307.16285</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mb7419/pendencyprediction">https://github.com/mb7419/pendencyprediction</a></li>
<li>paper_authors: Mohit Bhatnagar, Shivraj Huchhanavar</li>
<li>for: 这个论文旨在预测印度下级法院延迟的时间，基于提交案例信息。</li>
<li>methods: 作者使用AutoML开发了一个多类分类模型，使用10年的案例结果和420万个法律案件的数据集进行训练。</li>
<li>results: 最佳模型的准确率为81.4%，precision、recall和F1分别为0.81。研究证明了使用AI模型预测印度法院延迟的可能性，并提供了可用于进一步研究的数据集和Python代码文件。<details>
<summary>Abstract</summary>
This paper presents a classification model that predicts delays in Indian lower courts based on case information available at filing. The model is built on a dataset of 4.2 million court cases filed in 2010 and their outcomes over a 10-year period. The data set is drawn from 7000+ lower courts in India. The authors employed AutoML to develop a multi-class classification model over all periods of pendency and then used binary decision forest classifiers to improve predictive accuracy for the classification of delays. The best model achieved an accuracy of 81.4%, and the precision, recall, and F1 were found to be 0.81. The study demonstrates the feasibility of AI models for predicting delays in Indian courts, based on relevant data points such as jurisdiction, court, judge, subject, and the parties involved. The paper also discusses the results in light of relevant literature and suggests areas for improvement and future research. The authors have made the dataset and Python code files used for the analysis available for further research in the crucial and contemporary field of Indian judicial reform.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="zkDL-Efficient-Zero-Knowledge-Proofs-of-Deep-Learning-Training"><a href="#zkDL-Efficient-Zero-Knowledge-Proofs-of-Deep-Learning-Training" class="headerlink" title="zkDL: Efficient Zero-Knowledge Proofs of Deep Learning Training"></a>zkDL: Efficient Zero-Knowledge Proofs of Deep Learning Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16273">http://arxiv.org/abs/2307.16273</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haochen Sun, Hongyang Zhang</li>
<li>for: 防止不可信任的AI开发者侵犯知识产权，保护数据和模型参数的隐私。</li>
<li>methods: 使用特殊的零知识证明协议zkReLU，并将其集成到整个训练过程的证明系统中，以实现快速和有效的隐私保护。</li>
<li>results: 使用zkDL可以在一分钟内，对16层神经网络的200M参数进行完整和准确的证明，并且proof size仅20kB，比 tradicional方法减少了许多。<details>
<summary>Abstract</summary>
The recent advancements in deep learning have brought about significant changes in various aspects of people's lives. Meanwhile, these rapid developments have raised concerns about the legitimacy of the training process of deep networks. However, to protect the intellectual properties of untrusted AI developers, directly examining the training process by accessing the model parameters and training data by verifiers is often prohibited.   In response to this challenge, we present zkDL, an efficient zero-knowledge proof of deep learning training. At the core of zkDL is zkReLU, a specialized zero-knowledge proof protocol with optimized proving time and proof size for the ReLU activation function, a major obstacle in verifiable training due to its non-arithmetic nature. To integrate zkReLU into the proof system for the entire training process, we devise a novel construction of an arithmetic circuit from neural networks. By leveraging the abundant parallel computation resources, this construction reduces proving time and proof sizes by a factor of the network depth. As a result, zkDL enables the generation of complete and sound proofs, taking less than a minute with a size of less than 20 kB per training step, for a 16-layer neural network with 200M parameters, while ensuring the privacy of data and model parameters.
</details>
<details>
<summary>摘要</summary>
The core of zkDL is zkReLU, a specialized zero-knowledge proof protocol with optimized proving time and proof size for the ReLU activation function, which has been a major obstacle in verifiable training due to its non-arithmetic nature. To integrate zkReLU into the proof system for the entire training process, we have devised a novel construction of an arithmetic circuit from neural networks. By leveraging the abundant parallel computation resources, this construction reduces proving time and proof sizes by a factor of the network depth.With zkDL, we can generate complete and sound proofs in less than a minute, with a size of less than 20 kB per training step, for a 16-layer neural network with 200M parameters, while ensuring the privacy of data and model parameters. This efficient zero-knowledge proof system enables the protection of intellectual properties in deep learning, and paves the way for more transparent and trustworthy AI development.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/31/cs.LG_2023_07_31/" data-id="cllsiju1w002da388ediq2kd6" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_07_31" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/31/cs.SD_2023_07_31/" class="article-date">
  <time datetime="2023-07-30T16:00:00.000Z" itemprop="datePublished">2023-07-31</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/31/cs.SD_2023_07_31/">cs.SD - 2023-07-31 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Audio-Visual-Segmentation-by-Exploring-Cross-Modal-Mutual-Semantics"><a href="#Audio-Visual-Segmentation-by-Exploring-Cross-Modal-Mutual-Semantics" class="headerlink" title="Audio-Visual Segmentation by Exploring Cross-Modal Mutual Semantics"></a>Audio-Visual Segmentation by Exploring Cross-Modal Mutual Semantics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16620">http://arxiv.org/abs/2307.16620</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Liu, Peike Li, Xingqun Qi, Hu Zhang, Lincheng Li, Dadong Wang, Xin Yu</li>
<li>for: 本研究旨在解决现有Audio-Visual Segmentation（AVS）方法受 dataset bias 的问题，即尝试将各种视频中的各种对象都与音频信号相关联，即使这些对象并不是真正的听音对象。</li>
<li>methods: 我们提出了一种Audio-VisualInstance-Aware Segmentation方法，包括两个关键步骤：首先，使用对象分割网络对视频中的对象进行分割；其次，将音频信号与可能的听音对象相关联。为了解决对象分割网络的混淆，我们提出了一种静音对象检测目标函数。此外，我们还提出了一种基于音频视频语义相关性的音频分类目标函数，以便将音频信号与听音对象相关联。</li>
<li>results: 我们的方法在AVS数据集上进行实验，结果表明，我们的方法可以准确地分割听音对象，不受dataset bias的影响。<details>
<summary>Abstract</summary>
The audio-visual segmentation (AVS) task aims to segment sounding objects from a given video. Existing works mainly focus on fusing audio and visual features of a given video to achieve sounding object masks. However, we observed that prior arts are prone to segment a certain salient object in a video regardless of the audio information. This is because sounding objects are often the most salient ones in the AVS dataset. Thus, current AVS methods might fail to localize genuine sounding objects due to the dataset bias. In this work, we present an audio-visual instance-aware segmentation approach to overcome the dataset bias. In a nutshell, our method first localizes potential sounding objects in a video by an object segmentation network, and then associates the sounding object candidates with the given audio. We notice that an object could be a sounding object in one video but a silent one in another video. This would bring ambiguity in training our object segmentation network as only sounding objects have corresponding segmentation masks. We thus propose a silent object-aware segmentation objective to alleviate the ambiguity. Moreover, since the category information of audio is unknown, especially for multiple sounding sources, we propose to explore the audio-visual semantic correlation and then associate audio with potential objects. Specifically, we attend predicted audio category scores to potential instance masks and these scores will highlight corresponding sounding instances while suppressing inaudible ones. When we enforce the attended instance masks to resemble the ground-truth mask, we are able to establish audio-visual semantics correlation. Experimental results on the AVS benchmarks demonstrate that our method can effectively segment sounding objects without being biased to salient objects.
</details>
<details>
<summary>摘要</summary>
audio-visual分 segmentation（AVS）任务的目标是从视频中分割听起的对象。现有的方法主要是将视频和听起的特征进行融合，以实现听起的对象面积。然而，我们发现现有的方法容易因为数据偏见而分割某些突出的对象，这是因为听起的对象通常是视频中最突出的对象。因此，当前的AVS方法可能无法准确地 lokalisieren真正的听起的对象，这是因为数据偏见。在这个工作中，我们提出了一种audio-visual实例特征分割方法，以解决数据偏见问题。总的来说，我们的方法首先在视频中 lokalisieren潜在的听起对象，然后将这些对象与给定的听起相关联。我们注意到，一个对象可能在一个视频中是听起的对象，而在另一个视频中是无声的对象。这会导致我们的对象分割网络在训练时存在含糊不清的问题，因为只有听起的对象有相应的分割面积。我们因此提出了一种无声对象特征分割目标，以解决这个问题。此外，由于听起的音频类别信息未知，特别是多个听起源，我们提出了探索音频-视频semantic相关性，然后将音频与潜在对象相关联。具体来说，我们将预测的音频类别分数attend到 potential实例面积，这些分数会高亮对应的听起实例，而suppress不可见的实例。当我们强制enforce attend instance masks和ground truth mask相似，我们就能够建立音频-视频semantic相关性。实验结果表明，我们的方法可以效果地分割听起的对象，不受数据偏见的影响。
</details></li>
</ul>
<hr>
<h2 id="SAMbA-Speech-enhancement-with-Asynchronous-ad-hoc-Microphone-Arrays"><a href="#SAMbA-Speech-enhancement-with-Asynchronous-ad-hoc-Microphone-Arrays" class="headerlink" title="SAMbA: Speech enhancement with Asynchronous ad-hoc Microphone Arrays"></a>SAMbA: Speech enhancement with Asynchronous ad-hoc Microphone Arrays</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16582">http://arxiv.org/abs/2307.16582</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicolas Furnon, Romain Serizel, Slim Essid, Irina Illina</li>
<li>for: 提高随机麦克风数组中的语音增强，即使麦克风数组中的设备具有不同的采样时间和采样速率偏差。</li>
<li>methods: 使用深度神经网络（DNN）来实现语音增强，该方法适用于随机麦克风数组，并能够忍受不同设备的采样时间和采样速率偏差。我们使用注意力机制，使DNN和整个管道具有对偏差的抗性。</li>
<li>results: 我们表明，使用注意力机制可以使DNN和整个管道具有对偏差的抗性，并且不需要进行费时的数据同步。此外，我们还发现，注意力机制可以自动地学习出偏差参数，无需进行supervised学习。<details>
<summary>Abstract</summary>
Speech enhancement in ad-hoc microphone arrays is often hindered by the asynchronization of the devices composing the microphone array. Asynchronization comes from sampling time offset and sampling rate offset which inevitably occur when the microphones are embedded in different hardware components. In this paper, we propose a deep neural network (DNN)-based speech enhancement solution that is suited for applications in ad-hoc microphone arrays because it is distributed and copes with asynchronization. We show that asynchronization has a limited impact on the spatial filtering and mostly affects the performance of the DNNs. Instead of resynchronising the signals, which requires costly processing steps, we use an attention mechanism which makes the DNNs, thus our whole pipeline, robust to asynchronization. We also show that the attention mechanism leads to the asynchronization parameters in an unsupervised manner.
</details>
<details>
<summary>摘要</summary>
听音提升在协作式麦克麦得组中经常受到设备组成部件之间的不同步问题的干扰。不同步问题来自探测时间偏移和探测速率偏移，这些偏移是不可避免的，因为麦克麦得在不同的硬件组件中嵌入。在这篇论文中，我们提出了一种基于深度神经网络（DNN）的听音提升解决方案，适用于协作式麦克麦得组中的应用。我们表明，不同步问题对听音提升的空间滤波几乎没有影响，主要影响DNN的性能。而不如同步化信号，这需要成本高昂的处理步骤，我们使用了注意力机制，使DNNs、也就是我们整个管道，对不同步问题进行抗性。此外，我们还表明了注意力机制对不同步问题的无监督学习。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Conditional-Latent-Diffusion-for-Audio-visual-Segmentation"><a href="#Contrastive-Conditional-Latent-Diffusion-for-Audio-visual-Segmentation" class="headerlink" title="Contrastive Conditional Latent Diffusion for Audio-visual Segmentation"></a>Contrastive Conditional Latent Diffusion for Audio-visual Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16579">http://arxiv.org/abs/2307.16579</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxin Mao, Jing Zhang, Mochu Xiang, Yunqiu Lv, Yiran Zhong, Yuchao Dai<br>for:The paper is written for exploring the contribution of audio in audio-visual segmentation (AVS) tasks, and proposing a latent diffusion model with contrastive learning to explicitly maximize the contribution of audio.methods:The paper uses a latent diffusion model with contrastive learning to learn a semantic-correlated representation of audio and visual features, and to ensure that the conditional variable (audio) contributes to the final segmentation map.results:Experimental results on a benchmark dataset verify the effectiveness of the proposed solution, demonstrating improved performance in AVS tasks using the proposed method. The code and results are available online via the project page: <a target="_blank" rel="noopener" href="https://github.com/OpenNLPLab/DiffusionAVS">https://github.com/OpenNLPLab/DiffusionAVS</a>.Here’s the Chinese version:for:这篇论文是为了探讨音频在音视频分割（AVS）任务中的贡献，并提出一种含批学模型来显著地增强音频的贡献。methods:这篇论文使用含批学模型来学习音视频特征之间的含义相关性，并确保条件变量（音频）对最终分割图的贡献。results:对于一个 benchmark 数据集，实验结果表明提出的解决方案有效，在 AVS 任务中使用提posed方法可以提高性能。代码和结果可以在项目页面上获取：<a target="_blank" rel="noopener" href="https://github.com/OpenNLPLab/DiffusionAVS">https://github.com/OpenNLPLab/DiffusionAVS</a>.<details>
<summary>Abstract</summary>
We propose a latent diffusion model with contrastive learning for audio-visual segmentation (AVS) to extensively explore the contribution of audio. We interpret AVS as a conditional generation task, where audio is defined as the conditional variable for sound producer(s) segmentation. With our new interpretation, it is especially necessary to model the correlation between audio and the final segmentation map to ensure its contribution. We introduce a latent diffusion model to our framework to achieve semantic-correlated representation learning. Specifically, our diffusion model learns the conditional generation process of the ground-truth segmentation map, leading to ground-truth aware inference when we perform the denoising process at the test stage. As a conditional diffusion model, we argue it is essential to ensure that the conditional variable contributes to model output. We then introduce contrastive learning to our framework to learn audio-visual correspondence, which is proven consistent with maximizing the mutual information between model prediction and the audio data. In this way, our latent diffusion model via contrastive learning explicitly maximizes the contribution of audio for AVS. Experimental results on the benchmark dataset verify the effectiveness of our solution. Code and results are online via our project page: https://github.com/OpenNLPLab/DiffusionAVS.
</details>
<details>
<summary>摘要</summary>
我们提出了一种具有对比学习的潜在扩散模型，用于广泛探索音频的贡献。我们将音频视为条件变量，用于音频生产者 segmentation 的定义。我们新的解释需要模型音频和最终分割图的 correlation，以确保其贡献。我们引入了一种潜在扩散模型到我们的框架，以实现含义相关的表示学习。具体来说，我们的扩散模型学习了真实分割图的生成过程，使得在测试阶段进行锈除操作时，能够保证模型的输出具有真实分割图的意义。作为一个条件扩散模型，我们认为其必须确保条件变量对模型输出做出贡献。我们然后引入了对比学习，以学习音频和视频之间的对应关系，这已经被证明可以具有最大化模型预测和音频数据之间的共同信息。这样，我们的潜在扩散模型通过对比学习显著地提高了音频对 AVS 的贡献。实验结果表明我们的解决方案的效果。代码和结果可以在我们项目页面上找到：https://github.com/OpenNLPLab/DiffusionAVS。
</details></li>
</ul>
<hr>
<h2 id="DiffProsody-Diffusion-based-Latent-Prosody-Generation-for-Expressive-Speech-Synthesis-with-Prosody-Conditional-Adversarial-Training"><a href="#DiffProsody-Diffusion-based-Latent-Prosody-Generation-for-Expressive-Speech-Synthesis-with-Prosody-Conditional-Adversarial-Training" class="headerlink" title="DiffProsody: Diffusion-based Latent Prosody Generation for Expressive Speech Synthesis with Prosody Conditional Adversarial Training"></a>DiffProsody: Diffusion-based Latent Prosody Generation for Expressive Speech Synthesis with Prosody Conditional Adversarial Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16549">http://arxiv.org/abs/2307.16549</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hsoh0306/diffprosody">https://github.com/hsoh0306/diffprosody</a></li>
<li>paper_authors: Hyung-Seok Oh, Sang-Hoon Lee, Seong-Whan Lee</li>
<li>for: 这个研究旨在提高表达系统的表达性能，尤其是在考虑到谱系模型的问题。</li>
<li>methods: 该研究使用了一种名为DiffProsody的新方法，它使用了扩散基本的约束逻辑生成器和抽象训练来生成表达。</li>
<li>results: 实验结果表明，DiffProsody可以在16倍的速度上生成表达，并且表现出了更高的质量和精度。<details>
<summary>Abstract</summary>
Expressive text-to-speech systems have undergone significant advancements owing to prosody modeling, but conventional methods can still be improved. Traditional approaches have relied on the autoregressive method to predict the quantized prosody vector; however, it suffers from the issues of long-term dependency and slow inference. This study proposes a novel approach called DiffProsody in which expressive speech is synthesized using a diffusion-based latent prosody generator and prosody conditional adversarial training. Our findings confirm the effectiveness of our prosody generator in generating a prosody vector. Furthermore, our prosody conditional discriminator significantly improves the quality of the generated speech by accurately emulating prosody. We use denoising diffusion generative adversarial networks to improve the prosody generation speed. Consequently, DiffProsody is capable of generating prosody 16 times faster than the conventional diffusion model. The superior performance of our proposed method has been demonstrated via experiments.
</details>
<details>
<summary>摘要</summary>
<<SYS>>传统方法仅仅可以预测衡量vector，但是它们受到长期依赖和慢速预测的限制。这种研究提出了一种新的方法，即DiffProsody，它使用扩散基于秘密词法生成器和词法强制约束 adversarial training来生成表达主义的speech。我们的发现表明，DiffProsody可以快速生成高质量的speech，并且可以准确地模仿词法。我们使用了denoising扩散生成 adversarial networks来提高词法生成速度，因此DiffProsody可以在16倍之前生成词法。经过实验，我们证明了DiffProsody的优秀性。Note: Please note that the translation is in Simplified Chinese, which is the standard written form of Chinese used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that form instead.
</details></li>
</ul>
<hr>
<h2 id="SpatialNet-Extensively-Learning-Spatial-Information-for-Multichannel-Joint-Speech-Separation-Denoising-and-Dereverberation"><a href="#SpatialNet-Extensively-Learning-Spatial-Information-for-Multichannel-Joint-Speech-Separation-Denoising-and-Dereverberation" class="headerlink" title="SpatialNet: Extensively Learning Spatial Information for Multichannel Joint Speech Separation, Denoising and Dereverberation"></a>SpatialNet: Extensively Learning Spatial Information for Multichannel Joint Speech Separation, Denoising and Dereverberation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16516">http://arxiv.org/abs/2307.16516</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/audio-westlakeu/nbss">https://github.com/audio-westlakeu/nbss</a></li>
<li>paper_authors: Changsheng Quan, Xiaofei Li</li>
<li>for: 这个论文旨在提出一种基于神经网络的多通道联合音声分离、减阈和抑抑抗抑谔（joint speech separation, denoising, and dereverberation）方法，名为空间网络（SpatialNet）。</li>
<li>methods: 该网络在短时傅里叶变换（STFT）域内进行端到端音声提高。它主要由交叠的窄频块和跨频块组成，分别利用窄频和跨频的空间信息。窄频块独立处理频率，使用自我注意机制和时间卷积层进行空间特征基本的 speaker 集群和时间平滑&#x2F;过滤。跨频块独立处理帧，使用全频线性层和频谱卷积层来分别学习所有频率和邻近频率之间的相关性。</li>
<li>results: 在多个模拟和实际数据集上进行了实验，研究结果表明：1）提出的网络实现了大多数任务的状态之冠性表现；2）提出的网络受到频谱泛化问题的影响很小；3）提出的网络确实进行了 speaker 集群（通过注意图表示）。<details>
<summary>Abstract</summary>
This work proposes a neural network to extensively exploit spatial information for multichannel joint speech separation, denoising and dereverberation, named SpatialNet.In the short-time Fourier transform (STFT) domain, the proposed network performs end-to-end speech enhancement. It is mainly composed of interleaved narrow-band and cross-band blocks to respectively exploit narrow-band and cross-band spatial information. The narrow-band blocks process frequencies independently, and use self-attention mechanism and temporal convolutional layers to respectively perform spatial-feature-based speaker clustering and temporal smoothing/filtering. The cross-band blocks processes frames independently, and use full-band linear layer and frequency convolutional layers to respectively learn the correlation between all frequencies and adjacent frequencies. Experiments are conducted on various simulated and real datasets, and the results show that 1) the proposed network achieves the state-of-the-art performance on almost all tasks; 2) the proposed network suffers little from the spectral generalization problem; and 3) the proposed network is indeed performing speaker clustering (demonstrated by attention maps).
</details>
<details>
<summary>摘要</summary>
这个工作提出了一种神经网络，旨在广泛利用空间信息进行多通道共同声音分离、减震和抑噪，称为空间网络（SpatialNet）。在短时傅立干（STFT）频域内，该网络实现了端到端声音提升。它主要由交叠式窄频和跨频块组成，分别利用窄频和跨频空间信息。窄频块独立处理频率，使用自注意机制和时间径向层来分别进行空间特征基于的speaker分组和时间平滑/过滤。跨频块独立处理帧，使用全频线性层和频率径向层来分别学习所有频率和邻近频率之间的相关性。在各种 simulate 和实际数据集上进行了实验，结果显示：1）提出的网络在大多数任务上达到了状态之arte的性能; 2）提出的网络受到相对较少的spectral泛化问题; 3）提出的网络实际进行了speaker clustering（通过注意力地图来证明）。
</details></li>
</ul>
<hr>
<h2 id="LP-MusicCaps-LLM-Based-Pseudo-Music-Captioning"><a href="#LP-MusicCaps-LLM-Based-Pseudo-Music-Captioning" class="headerlink" title="LP-MusicCaps: LLM-Based Pseudo Music Captioning"></a>LP-MusicCaps: LLM-Based Pseudo Music Captioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16372">http://arxiv.org/abs/2307.16372</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/seungheondoh/lp-music-caps">https://github.com/seungheondoh/lp-music-caps</a></li>
<li>paper_authors: SeungHeon Doh, Keunwoo Choi, Jongpil Lee, Juhan Nam</li>
<li>for: 提高音乐数据的理解和管理</li>
<li>methods: 使用大型自然语言模型生成描述句</li>
<li>results: 比超级参考模型提高音乐描述模型的性能<details>
<summary>Abstract</summary>
Automatic music captioning, which generates natural language descriptions for given music tracks, holds significant potential for enhancing the understanding and organization of large volumes of musical data. Despite its importance, researchers face challenges due to the costly and time-consuming collection process of existing music-language datasets, which are limited in size. To address this data scarcity issue, we propose the use of large language models (LLMs) to artificially generate the description sentences from large-scale tag datasets. This results in approximately 2.2M captions paired with 0.5M audio clips. We term it Large Language Model based Pseudo music caption dataset, shortly, LP-MusicCaps. We conduct a systemic evaluation of the large-scale music captioning dataset with various quantitative evaluation metrics used in the field of natural language processing as well as human evaluation. In addition, we trained a transformer-based music captioning model with the dataset and evaluated it under zero-shot and transfer-learning settings. The results demonstrate that our proposed approach outperforms the supervised baseline model.
</details>
<details>
<summary>摘要</summary>
自动化音乐标题生成，可以生成音乐轨道上的自然语言描述，具有潜在的大量音乐数据理解和组织化的潜力。然而，研究人员面临数据缺乏问题，因为现有的音乐语言数据集收集成本费时和成本高。为解决这个数据缺乏问题，我们提议使用大型语言模型（LLM）人工生成描述句子，从大规模标签数据集中获得约220万个描述句子和50万个音频剪辑。我们称之为大语言模型基于假音乐描述数据集（LP-MusicCaps）。我们进行了音乐描述集的系统评估，使用了自然语言处理领域常用的评价指标，以及人工评估。此外，我们将 transformer 型音乐描述模型与数据集进行训练，并在零基础和转移学习设置下评估其性能。结果表明，我们的提议方法在超过基线模型的情况下表现出色。
</details></li>
</ul>
<hr>
<h2 id="Mispronunciation-detection-using-self-supervised-speech-representations"><a href="#Mispronunciation-detection-using-self-supervised-speech-representations" class="headerlink" title="Mispronunciation detection using self-supervised speech representations"></a>Mispronunciation detection using self-supervised speech representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16324">http://arxiv.org/abs/2307.16324</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/JazminVidal/ssl-mispron">https://github.com/JazminVidal/ssl-mispron</a></li>
<li>paper_authors: Jazmin Vidal, Pablo Riera, Luciana Ferrer</li>
<li>for: 这个论文是关于second language learners的发音识别 tasks的研究。</li>
<li>methods: 作者使用了自动学习（SSL）模型，并对其进行了多种表示方式的比较。</li>
<li>results: 研究发现，使用直接为目标任务训练的模型 perfoms最好，而大多数上游模型在该任务上的表现相似。I hope this helps! Let me know if you have any further questions.<details>
<summary>Abstract</summary>
In recent years, self-supervised learning (SSL) models have produced promising results in a variety of speech-processing tasks, especially in contexts of data scarcity. In this paper, we study the use of SSL models for the task of mispronunciation detection for second language learners. We compare two downstream approaches: 1) training the model for phone recognition (PR) using native English data, and 2) training a model directly for the target task using non-native English data. We compare the performance of these two approaches for various SSL representations as well as a representation extracted from a traditional DNN-based speech recognition model. We evaluate the models on L2Arctic and EpaDB, two datasets of non-native speech annotated with pronunciation labels at the phone level. Overall, we find that using a downstream model trained for the target task gives the best performance and that most upstream models perform similarly for the task.
</details>
<details>
<summary>摘要</summary>
Here's the Traditional Chinese translation:近年来，自主学习（SSL）模型在各种语音处理任务中获得了显著的成果，尤其在数据稀缺的情况下。在这篇文章中，我们研究了使用SSL模型来探索第二语言学习者的误发音探测。我们比较了两种下游方法：1）使用本地英文数据进行话音识别（PR）训练，和2）直接使用非本地英文数据进行目标任务的训练。我们对各种SSL表现进行比较，以及从传统的 Deep Neural Network（DNN）数据识别模型中提取的表现。我们将模型评估在L2Arctic和EpaDB两个非本地语音数据集上，这两个数据集各自包含了话音的标签。总的来说，我们发现使用直接进行目标任务的训练模型能够获得最好的性能，而大多数上游模型在这个任务上表现相似。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/31/cs.SD_2023_07_31/" data-id="cllsiju2l004la3881ddg0637" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_07_31" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/31/eess.AS_2023_07_31/" class="article-date">
  <time datetime="2023-07-30T16:00:00.000Z" itemprop="datePublished">2023-07-31</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/31/eess.AS_2023_07_31/">eess.AS - 2023-07-31 22:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Multilingual-context-based-pronunciation-learning-for-Text-to-Speech"><a href="#Multilingual-context-based-pronunciation-learning-for-Text-to-Speech" class="headerlink" title="Multilingual context-based pronunciation learning for Text-to-Speech"></a>Multilingual context-based pronunciation learning for Text-to-Speech</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16709">http://arxiv.org/abs/2307.16709</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giulia Comini, Manuel Sam Ribeiro, Fan Yang, Heereen Shim, Jaime Lorenzo-Trueba</li>
<li>for: 这个论文的目的是提出一个多语言统一的前端系统，用于解决语音识别相关的任务。</li>
<li>methods: 该论文使用的方法包括图书馆搜集和语音识别模型，以及一些语言特定的规则和约束。</li>
<li>results: 研究人员在这篇论文中提出了一个多语言统一的前端系统，并进行了对多种语言和任务的评估，发现该系统在不同语言和任务上具有竞争力。<details>
<summary>Abstract</summary>
Phonetic information and linguistic knowledge are an essential component of a Text-to-speech (TTS) front-end. Given a language, a lexicon can be collected offline and Grapheme-to-Phoneme (G2P) relationships are usually modeled in order to predict the pronunciation for out-of-vocabulary (OOV) words. Additionally, post-lexical phonology, often defined in the form of rule-based systems, is used to correct pronunciation within or between words. In this work we showcase a multilingual unified front-end system that addresses any pronunciation related task, typically handled by separate modules. We evaluate the proposed model on G2P conversion and other language-specific challenges, such as homograph and polyphones disambiguation, post-lexical rules and implicit diacritization. We find that the multilingual model is competitive across languages and tasks, however, some trade-offs exists when compared to equivalent monolingual solutions.
</details>
<details>
<summary>摘要</summary>
phonetic information和语言知识是文本读取（TTS）前端的重要组成部分。给定一种语言，一个词典可以在线收集，并且使用格拉姆-至-声音（G2P）关系预测未在词汇表中出现的词语的发音。此外，后 lexical phonology，通常通过规则系统表示，用于在或 между词语中 corrections 发音。在这种工作中，我们展示了一种多语言统一前端系统，可以处理任何发音相关的任务，通常由单独的模块处理。我们评估提议的模型在G2P转换和其他语言特有的挑战中，如同形词和多音字识别、后 lexical 规则和隐式 диакриت化。我们发现该多语言模型在语言和任务方面都具有竞争力，但有一些贸易offs existed 与等效的单语言解决方案相比。
</details></li>
</ul>
<hr>
<h2 id="Improving-grapheme-to-phoneme-conversion-by-learning-pronunciations-from-speech-recordings"><a href="#Improving-grapheme-to-phoneme-conversion-by-learning-pronunciations-from-speech-recordings" class="headerlink" title="Improving grapheme-to-phoneme conversion by learning pronunciations from speech recordings"></a>Improving grapheme-to-phoneme conversion by learning pronunciations from speech recordings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16643">http://arxiv.org/abs/2307.16643</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manuel Sam Ribeiro, Giulia Comini, Jaime Lorenzo-Trueba</li>
<li>for: 这 paper 的目的是提高 Grapheme-to-Phoneme (G2P) 任务的精度。</li>
<li>methods: 这 paper 使用了学习语音录制中的示例来改进 G2P 转换。</li>
<li>results: 实验结果表明，这种方法可以在不同语言和数据量不足的情况下 consistently 提高 G2P 系统的phone error rate。<details>
<summary>Abstract</summary>
The Grapheme-to-Phoneme (G2P) task aims to convert orthographic input into a discrete phonetic representation. G2P conversion is beneficial to various speech processing applications, such as text-to-speech and speech recognition. However, these tend to rely on manually-annotated pronunciation dictionaries, which are often time-consuming and costly to acquire. In this paper, we propose a method to improve the G2P conversion task by learning pronunciation examples from audio recordings. Our approach bootstraps a G2P with a small set of annotated examples. The G2P model is used to train a multilingual phone recognition system, which then decodes speech recordings with a phonetic representation. Given hypothesized phoneme labels, we learn pronunciation dictionaries for out-of-vocabulary words, and we use those to re-train the G2P system. Results indicate that our approach consistently improves the phone error rate of G2P systems across languages and amount of available data.
</details>
<details>
<summary>摘要</summary>
文本描述：grapheme-to-phoneme（G2P）任务的目标是将文字输入转换为精确的声音表示。G2P转换对于各种语音处理应用程序非常有利，如文本读取和语音识别。然而，这些应用程序通常需要手动编制的发音词典，这可能会耗费很多时间和成本。在这篇论文中，我们提出了一种方法，可以通过学习音频记录中的发音示例来改进G2P转换任务。我们的方法从一小 subsets of annotated examples开始，使用G2P模型来训练多语言的 телеphone recognition系统。给出的假设发音标签，我们可以学习出现在词汇表外的发音词典，并使用这些词典来重新训练G2P系统。结果表明，我们的方法可以在不同语言和数据量的情况下一直提高G2P系统的声音错误率。
</details></li>
</ul>
<hr>
<h2 id="All-In-One-Metrical-And-Functional-Structure-Analysis-With-Neighborhood-Attentions-on-Demixed-Audio"><a href="#All-In-One-Metrical-And-Functional-Structure-Analysis-With-Neighborhood-Attentions-on-Demixed-Audio" class="headerlink" title="All-In-One Metrical And Functional Structure Analysis With Neighborhood Attentions on Demixed Audio"></a>All-In-One Metrical And Functional Structure Analysis With Neighborhood Attentions on Demixed Audio</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16425">http://arxiv.org/abs/2307.16425</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mir-aidj/all-in-one">https://github.com/mir-aidj/all-in-one</a></li>
<li>paper_authors: Taejun Kim, Juhan Nam</li>
<li>for: 这篇论文的目的是提出一种可靠的音乐信息检索（MIR）模型，以捕捉音乐的复杂层次结构。</li>
<li>methods: 该模型使用分辨率增强的听spectrogram作为输入，并使用扩展 neighboorhood attention机制来捕捉音乐的长期时间关系，以及非扩展的attention机制来捕捉乐器的本地关系。</li>
<li>results: 根据 Harmonix Set 数据集的测试结果，该模型可以同时高效地完成 beat 跟踪、downbeat 跟踪、功能结构分割和标注等四个任务，而且相比最新的状态前景模型，该模型具有较低的参数数量。此外，我们的减少学习研究表明，同时学习 beat、downbeat 和段落可以导致提高表现，每个任务彼此互助。<details>
<summary>Abstract</summary>
Music is characterized by complex hierarchical structures. Developing a comprehensive model to capture these structures has been a significant challenge in the field of Music Information Retrieval (MIR). Prior research has mainly focused on addressing individual tasks for specific hierarchical levels, rather than providing a unified approach. In this paper, we introduce a versatile, all-in-one model that jointly performs beat and downbeat tracking as well as functional structure segmentation and labeling. The model leverages source-separated spectrograms as inputs and employs dilated neighborhood attentions to capture temporal long-term dependencies, along with non-dilated attentions for local instrumental dependencies. Consequently, the proposed model achieves state-of-the-art performance in all four tasks on the Harmonix Set while maintaining a relatively lower number of parameters compared to recent state-of-the-art models. Furthermore, our ablation study demonstrates that the concurrent learning of beats, downbeats, and segments can lead to enhanced performance, with each task mutually benefiting from the others.
</details>
<details>
<summary>摘要</summary>
音乐具有复杂的层次结构，开发一个完整的模型以捕捉这些结构是音乐信息检索（MIR）领域的主要挑战。先前的研究主要集中在解决特定层次级别的任务上，而不是提供一个统一的方法。在本文中，我们提出了一个通用的、一体化模型，同时进行了 Beat 和下 Beat 追踪、功能结构分 segmentation 和标注。该模型使用分离后的spectrogram作为输入，并使用扩展 neighborhood 的注意力来捕捉时间长期依赖关系，以及非扩展的注意力来捕捉本地乐器依赖关系。因此，我们提出的模型在 Harmonix 集上实现了状态太的表现在四个任务中，同时具有相对较低的参数数量 compared to 最新的状态太模型。此外，我们的ablation 研究表明，同时学习 Beat、下 Beat 和分 segmentation 可以导致提高表现，每个任务受到另外两个任务的帮助。
</details></li>
</ul>
<hr>
<h2 id="Robust-Self-Supervised-Speech-Embeddings-for-Child-Adult-Classification-in-Interactions-involving-Children-with-Autism"><a href="#Robust-Self-Supervised-Speech-Embeddings-for-Child-Adult-Classification-in-Interactions-involving-Children-with-Autism" class="headerlink" title="Robust Self Supervised Speech Embeddings for Child-Adult Classification in Interactions involving Children with Autism"></a>Robust Self Supervised Speech Embeddings for Child-Adult Classification in Interactions involving Children with Autism</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16398">http://arxiv.org/abs/2307.16398</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rimita Lahiri, Tiantian Feng, Rajat Hebbar, Catherine Lord, So Hyun Kim, Shrikanth Narayanan</li>
<li>for: 本研究旨在探讨听取儿童与成人之间的语音交流中的speaker classification问题，以便自动地确定听取的speaker是哪个人。</li>
<li>methods: 本研究使用了两种最新的自我超vision算法：Wav2vec 2.0和WavLM，并采用了对比性损失函数来预训练我们的模型。</li>
<li>results: 本研究在两个临床交流数据集中获得了9-13%的相对改进，与现有基eline相比，以达到更高的分类F1分数。此外，我们还分析了不同子群体的听取情况，并评估了不同的预训练条件下的模型性能。<details>
<summary>Abstract</summary>
We address the problem of detecting who spoke when in child-inclusive spoken interactions i.e., automatic child-adult speaker classification. Interactions involving children are richly heterogeneous due to developmental differences. The presence of neurodiversity e.g., due to Autism, contributes additional variability. We investigate the impact of additional pre-training with more unlabelled child speech on the child-adult classification performance. We pre-train our model with child-inclusive interactions, following two recent self-supervision algorithms, Wav2vec 2.0 and WavLM, with a contrastive loss objective. We report 9 - 13% relative improvement over the state-of-the-art baseline with regards to classification F1 scores on two clinical interaction datasets involving children with Autism. We also analyze the impact of pre-training under different conditions by evaluating our model on interactions involving different subgroups of children based on various demographic factors.
</details>
<details>
<summary>摘要</summary>
我们 Addresses 了儿童包含的语音交流自动听众识别问题（即儿童成人说话分类）。由于儿童的发展差异，这些交流具有丰富的多样性。另外，由于自闭症等神经多样性，会添加更多的不确定性。我们研究额外预训练使用更多的无标签儿童语音后，对儿童成人分类性能产生了9-13%的相对改进。我们使用了两种最新的自我超视觉算法（Wav2vec 2.0和WavLM），采用了对比损失目标函数进行预训练。我们在两个临床交流数据集中对儿童自闭症进行了分类，并分析了不同子群体儿童的影响。
</details></li>
</ul>
<hr>
<h2 id="Pre-training-End-to-end-ASR-Models-with-Augmented-Speech-Samples-Queried-by-Text"><a href="#Pre-training-End-to-end-ASR-Models-with-Augmented-Speech-Samples-Queried-by-Text" class="headerlink" title="Pre-training End-to-end ASR Models with Augmented Speech Samples Queried by Text"></a>Pre-training End-to-end ASR Models with Augmented Speech Samples Queried by Text</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16332">http://arxiv.org/abs/2307.16332</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eric Sun, Jinyu Li, Jian Xue, Yifan Gong</li>
<li>for: 提高语言扩展 END-to-END 自动语音识别系统的困难，增加语音数据的做准备。</li>
<li>methods: 提出一种新的方法，通过将无对应语音数据和文本数据混合生成增强样本，实现低成本的语音数据增强，不需要额外的语音数据。</li>
<li>results: 结果表明，将20,000小时增强样本与12,500小时原始译文数据混合，可以实现8.7%的字词错误率降低，与使用多语言原始75,000小时Raw speech数据预训练的模型相当。同时，将增强样本与多语言数据混合预训练新模型，可以实现12.2%的字词错误率降低，进一步证明了我们的方法的有效性。<details>
<summary>Abstract</summary>
In end-to-end automatic speech recognition system, one of the difficulties for language expansion is the limited paired speech and text training data. In this paper, we propose a novel method to generate augmented samples with unpaired speech feature segments and text data for model pre-training, which has the advantage of low cost without using additional speech data. When mixing 20,000 hours augmented speech data generated by our method with 12,500 hours original transcribed speech data for Italian Transformer transducer model pre-training, we achieve 8.7% relative word error rate reduction. The pre-trained model achieves similar performance as the model pre-trained with multilingual transcribed 75,000 hours raw speech data. When merging the augmented speech data with the multilingual data to pre-train a new model, we achieve even more relative word error rate reduction of 12.2% over the baseline, which further verifies the effectiveness of our method for speech data augmentation.
</details>
<details>
<summary>摘要</summary>
在全自动语音识别系统中，扩展语言的一个Difficulty是有限的对话语音和文本训练数据。在这篇论文中，我们提出了一种新的方法，用于生成增强样本，并使用无对应的语音特征段和文本数据进行模型预训练，这有优点，不需要额外的语音数据。当混合我们生成的20,000小时增强语音数据和12,500小时原始转录的语音数据进行意大 transformer 抽取器模型预训练，我们实现了8.7%的Relative Word Error Rate（RWER）减少。预训练模型与使用多语言 raw 语音数据75,000小时预训练的模型之间具有相似的性能。当把增强语音数据与多语言数据混合预训练一新模型时，我们实现了更高的RWER减少，达12.2%，这进一步证明了我们的方法对语音数据增强的有效性。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/31/eess.AS_2023_07_31/" data-id="cllsiju38006pa388cqca2x47" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_07_31" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/31/eess.IV_2023_07_31/" class="article-date">
  <time datetime="2023-07-30T16:00:00.000Z" itemprop="datePublished">2023-07-31</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/31/eess.IV_2023_07_31/">eess.IV - 2023-07-31 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Hybrid-quantum-transfer-learning-for-crack-image-classification-on-NISQ-hardware"><a href="#Hybrid-quantum-transfer-learning-for-crack-image-classification-on-NISQ-hardware" class="headerlink" title="Hybrid quantum transfer learning for crack image classification on NISQ hardware"></a>Hybrid quantum transfer learning for crack image classification on NISQ hardware</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16723">http://arxiv.org/abs/2307.16723</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander Geng, Ali Moghiseh, Claudia Redenbach, Katja Schladitz</li>
<li>for: 该论文旨在探讨量子计算机可以在减少量子比特数量的情况下处理数据的可能性，以及对现代量子机器学习算法的应用。</li>
<li>methods: 该论文使用了变量量子机器学习算法，并对量子传输学习进行了应用，用于检测灰度图像中的裂隙。</li>
<li>results: 研究发现，使用量子传输学习可以提高图像检测的性能和准确率，并且对于小型图像，量子传输学习的训练时间比标准量子机器学习算法更短。<details>
<summary>Abstract</summary>
Quantum computers possess the potential to process data using a remarkably reduced number of qubits compared to conventional bits, as per theoretical foundations. However, recent experiments have indicated that the practical feasibility of retrieving an image from its quantum encoded version is currently limited to very small image sizes. Despite this constraint, variational quantum machine learning algorithms can still be employed in the current noisy intermediate scale quantum (NISQ) era. An example is a hybrid quantum machine learning approach for edge detection. In our study, we present an application of quantum transfer learning for detecting cracks in gray value images. We compare the performance and training time of PennyLane's standard qubits with IBM's qasm\_simulator and real backends, offering insights into their execution efficiency.
</details>
<details>
<summary>摘要</summary>
量子计算机具有可能处理数据使用非常减少的量子比特数量，根据理论基础。然而，现实实验表明，现在可以从量子编码版本中恢复图像的实际可行性受限于非常小的图像大小。尽管存在这些限制，量子机器学习算法仍可以在当今的半古矿量化（NISQ）时代应用。我们的研究中描述了使用量子传输学习检测灰度图像中的裂隙。我们比较了彭尼lane的标准量子比特与IBM的qasm_simulator和真实后端的执行效率。这些比较提供了关于其执行效率的新的视角。
</details></li>
</ul>
<hr>
<h2 id="Conditioning-Generative-Latent-Optimization-to-solve-Imaging-Inverse-Problems"><a href="#Conditioning-Generative-Latent-Optimization-to-solve-Imaging-Inverse-Problems" class="headerlink" title="Conditioning Generative Latent Optimization to solve Imaging Inverse Problems"></a>Conditioning Generative Latent Optimization to solve Imaging Inverse Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16670">http://arxiv.org/abs/2307.16670</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas Braure, Kévin Ginsburger</li>
<li>for: 解决医学影像重构问题，提高影像重构质量</li>
<li>methods: 使用 score-based 生成模型，不需要大量的超参数和backward操作</li>
<li>results: 在缺省视图 CT 中，比起现有的得分基础方法，提高了重构质量，并且可以扩展到非线性 IIP 问题<details>
<summary>Abstract</summary>
Computed Tomography (CT) is a prominent example of Imaging Inverse Problem (IIP), highlighting the unrivalled performances of data-driven methods in degraded measurements setups like sparse X-ray projections. Although a significant proportion of deep learning approaches benefit from large supervised datasets to directly map experimental measurements to medical scans, they cannot generalize to unknown acquisition setups. In contrast, fully unsupervised techniques, most notably using score-based generative models, have recently demonstrated similar or better performances compared to supervised approaches to solve IIPs while being flexible at test time regarding the imaging setup. However, their use cases are limited by two factors: (a) they need considerable amounts of training data to have good generalization properties and (b) they require a backward operator, like Filtered-Back-Projection in the case of CT, to condition the learned prior distribution of medical scans to experimental measurements. To overcome these issues, we propose an unsupervised conditional approach to the Generative Latent Optimization framework (cGLO), in which the parameters of a decoder network are initialized on an unsupervised dataset. The decoder is then used for reconstruction purposes, by performing Generative Latent Optimization with a loss function directly comparing simulated measurements from proposed reconstructions to experimental measurements. The resulting approach, tested on sparse-view CT using multiple training dataset sizes, demonstrates better reconstruction quality compared to state-of-the-art score-based strategies in most data regimes and shows an increasing performance advantage for smaller training datasets and reduced projection angles. Furthermore, cGLO does not require any backward operator and could expand use cases even to non-linear IIPs.
</details>
<details>
<summary>摘要</summary>
计算Tomography（CT）是一个典型的成像反问题（IIP）示例， highlighting the unrivalled performances of data-driven methods in degraded measurements setups like sparse X-ray projections. Although a significant proportion of deep learning approaches benefit from large supervised datasets to directly map experimental measurements to medical scans, they cannot generalize to unknown acquisition setups. In contrast, fully unsupervised techniques, most notably using score-based generative models, have recently demonstrated similar or better performances compared to supervised approaches to solve IIPs while being flexible at test time regarding the imaging setup. However, their use cases are limited by two factors: (a) they need considerable amounts of training data to have good generalization properties and (b) they require a backward operator, like Filtered-Back-Projection in the case of CT, to condition the learned prior distribution of medical scans to experimental measurements. To overcome these issues, we propose an unsupervised conditional approach to the Generative Latent Optimization framework (cGLO), in which the parameters of a decoder network are initialized on an unsupervised dataset. The decoder is then used for reconstruction purposes, by performing Generative Latent Optimization with a loss function directly comparing simulated measurements from proposed reconstructions to experimental measurements. The resulting approach, tested on sparse-view CT using multiple training dataset sizes, demonstrates better reconstruction quality compared to state-of-the-art score-based strategies in most data regimes and shows an increasing performance advantage for smaller training datasets and reduced projection angles. Furthermore, cGLO does not require any backward operator and could expand use cases even to non-linear IIPs.
</details></li>
</ul>
<hr>
<h2 id="Towards-General-Low-Light-Raw-Noise-Synthesis-and-Modeling"><a href="#Towards-General-Low-Light-Raw-Noise-Synthesis-and-Modeling" class="headerlink" title="Towards General Low-Light Raw Noise Synthesis and Modeling"></a>Towards General Low-Light Raw Noise Synthesis and Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16508">http://arxiv.org/abs/2307.16508</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fengzhang427/LRD">https://github.com/fengzhang427/LRD</a></li>
<li>paper_authors: Feng Zhang, Bin Xu, Zhiqiang Li, Xinran Liu, Qingbo Lu, Changxin Gao, Nong Sang</li>
<li>for:  This paper aims to address the problem of modeling and synthesizing low-light raw noise in computational photography and image processing applications.</li>
<li>methods:  The proposed method uses a generative model to synthesize signal-independent noise, and a physics- and learning-based manner to synthesize signal-dependent and signal-independent noise. Additionally, an effective multi-scale discriminator called Fourier transformer discriminator (FTD) is used to distinguish the noise distribution accurately.</li>
<li>results:  The proposed method can generate highly similar noise distribution to real noise, and performs favorably against state-of-the-art methods on different sensors.Here is the summary in Simplified Chinese:</li>
<li>for: 这篇论文目的是解决计算 fotografraphy 和图像处理应用中的低光照 raw 噪声模型化和生成问题。</li>
<li>methods: 提议的方法使用生成模型来生成信号独立噪声，并在物理和学习基础上同时生成信号依赖和独立噪声。此外，使用高级多核 scale discriminator（FTD）来准确地 отличи出噪声分布。</li>
<li>results: 提议的方法可以生成高度类似于实际噪声的噪声分布，并在不同感知器上与现状方法进行比较。<details>
<summary>Abstract</summary>
Modeling and synthesizing low-light raw noise is a fundamental problem for computational photography and image processing applications. Although most recent works have adopted physics-based models to synthesize noise, the signal-independent noise in low-light conditions is far more complicated and varies dramatically across camera sensors, which is beyond the description of these models. To address this issue, we introduce a new perspective to synthesize the signal-independent noise by a generative model. Specifically, we synthesize the signal-dependent and signal-independent noise in a physics- and learning-based manner, respectively. In this way, our method can be considered as a general model, that is, it can simultaneously learn different noise characteristics for different ISO levels and generalize to various sensors. Subsequently, we present an effective multi-scale discriminator termed Fourier transformer discriminator (FTD) to distinguish the noise distribution accurately. Additionally, we collect a new low-light raw denoising (LRD) dataset for training and benchmarking. Qualitative validation shows that the noise generated by our proposed noise model can be highly similar to the real noise in terms of distribution. Furthermore, extensive denoising experiments demonstrate that our method performs favorably against state-of-the-art methods on different sensors.
</details>
<details>
<summary>摘要</summary>
模型和生成低光照Raw杂变是计算摄影和图像处理应用的基本问题。 Although most recent works have adopted physics-based models to synthesize noise, the signal-independent noise in low-light conditions is far more complicated and varies dramatically across camera sensors, which is beyond the description of these models. To address this issue, we introduce a new perspective to synthesize the signal-independent noise by a generative model. Specifically, we synthesize the signal-dependent and signal-independent noise in a physics- and learning-based manner, respectively. In this way, our method can be considered as a general model, that is, it can simultaneously learn different noise characteristics for different ISO levels and generalize to various sensors. Subsequently, we present an effective multi-scale discriminator termed Fourier transformer discriminator (FTD) to distinguish the noise distribution accurately. Additionally, we collect a new low-light raw denoising (LRD) dataset for training and benchmarking. Qualitative validation shows that the noise generated by our proposed noise model can be highly similar to the real noise in terms of distribution. Furthermore, extensive denoising experiments demonstrate that our method performs favorably against state-of-the-art methods on different sensors.Here's the translation in Traditional Chinese as well:模型和生成低光照Raw杂变是计算摄影和图像处理应用的基本问题。 Although most recent works have adopted physics-based models to synthesize noise, the signal-independent noise in low-light conditions is far more complicated and varies dramatically across camera sensors, which is beyond the description of these models. To address this issue, we introduce a new perspective to synthesize the signal-independent noise by a generative model. Specifically, we synthesize the signal-dependent and signal-independent noise in a physics- and learning-based manner, respectively. In this way, our method can be considered as a general model, that is, it can simultaneously learn different noise characteristics for different ISO levels and generalize to various sensors. Subsequently, we present an effective multi-scale discriminator termed Fourier transformer discriminator (FTD) to distinguish the noise distribution accurately. Additionally, we collect a new low-light raw denoising (LRD) dataset for training and benchmarking. Qualitative validation shows that the noise generated by our proposed noise model can be highly similar to the real noise in terms of distribution. Furthermore, extensive denoising experiments demonstrate that our method performs favorably against state-of-the-art methods on different sensors.
</details></li>
</ul>
<hr>
<h2 id="A-hybrid-approach-for-improving-U-Net-variants-in-medical-image-segmentation"><a href="#A-hybrid-approach-for-improving-U-Net-variants-in-medical-image-segmentation" class="headerlink" title="A hybrid approach for improving U-Net variants in medical image segmentation"></a>A hybrid approach for improving U-Net variants in medical image segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16462">http://arxiv.org/abs/2307.16462</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aitik Gupta, Dr. Joydip Dhar</li>
<li>for: 这个研究旨在降低预测网络参数需求，保持适用于一些医疗影像分类任务，如皮肤淋巴瘤分类，通过深度分解卷积和注意力系统。</li>
<li>methods: 这个研究主要专注于审查现有的深度学习方法，包括MultiResUNet、Attention U-Net 和其他变化。这些方法使用注意力特征向量或地图对重要信息加以重要性评估，以提高精度。但是，这些网络参数较多，导致过拟合和测试时间较长。</li>
<li>results: 这个研究希望通过使用深度分解卷积和注意力系统，实现降低网络参数需求，并维持适用于一些医疗影像分类任务的性能。<details>
<summary>Abstract</summary>
Medical image segmentation is vital to the area of medical imaging because it enables professionals to more accurately examine and understand the information offered by different imaging modalities. The technique of splitting a medical image into various segments or regions of interest is known as medical image segmentation. The segmented images that are produced can be used for many different things, including diagnosis, surgery planning, and therapy evaluation.   In initial phase of research, major focus has been given to review existing deep-learning approaches, including researches like MultiResUNet, Attention U-Net, classical U-Net, and other variants. The attention feature vectors or maps dynamically add important weights to critical information, and most of these variants use these to increase accuracy, but the network parameter requirements are somewhat more stringent. They face certain problems such as overfitting, as their number of trainable parameters is very high, and so is their inference time.   Therefore, the aim of this research is to reduce the network parameter requirements using depthwise separable convolutions, while maintaining performance over some medical image segmentation tasks such as skin lesion segmentation using attention system and residual connections.
</details>
<details>
<summary>摘要</summary>
医疗图像分割是医疗图像领域的关键技术，它使医生和科学家可以更加准确地检查和理解不同的医疗图像模式中提供的信息。医疗图像分割的过程包括将医疗图像分成多个区域或区域兴趣。生成的分割图像可以用于诊断、手术规划和治疗评估等多种应用。在初期研究阶段，主要关注了现有的深度学习方法，包括MultiResUNet、Attention U-Net、 классиical U-Net 和其他变体。这些变体使用注意力特征向量或地图来 dynamically添加重要的权重，以提高准确性。然而，这些变体的网络参数需求较高，即使在训练和执行时间方面也存在一定的问题，如过拟合和计算时间过长。因此，本研究的目标是通过depthwise separable convolutions来降低网络参数需求，保持一定的性能水平，而且在一些医疗图像分割任务中，如皮肤病变分割使用注意力系统和 residual connections。
</details></li>
</ul>
<hr>
<h2 id="High-Dynamic-Range-Image-Reconstruction-via-Deep-Explicit-Polynomial-Curve-Estimation"><a href="#High-Dynamic-Range-Image-Reconstruction-via-Deep-Explicit-Polynomial-Curve-Estimation" class="headerlink" title="High Dynamic Range Image Reconstruction via Deep Explicit Polynomial Curve Estimation"></a>High Dynamic Range Image Reconstruction via Deep Explicit Polynomial Curve Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16426">http://arxiv.org/abs/2307.16426</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jqtangust/epce-hdr">https://github.com/jqtangust/epce-hdr</a></li>
<li>paper_authors: Jiaqi Tang, Xiaogang Xu, Sixing Hu, Ying-Cong Chen<br>for:  This paper aims to address the challenge of High Dynamic Range (HDR) reconstruction from Low Dynamic Range (LDR) images, which is limited by the narrow dynamic range of digital images and the diverse tone-mapping functions used in different imaging systems.methods:  The proposed method uses a learnable network to estimate the tone mapping function and its corresponding HDR image in one network. The tone mapping function is described by a polynomial, which is automatically adjusted according to the tone space of the LDR image.results:  The proposed method achieves State-of-the-Art (SOTA) performance under different tone-mapping functions and generalizes well to new datasets, including both synthetic and real images.<details>
<summary>Abstract</summary>
Due to limited camera capacities, digital images usually have a narrower dynamic illumination range than real-world scene radiance. To resolve this problem, High Dynamic Range (HDR) reconstruction is proposed to recover the dynamic range to better represent real-world scenes. However, due to different physical imaging parameters, the tone-mapping functions between images and real radiance are highly diverse, which makes HDR reconstruction extremely challenging. Existing solutions can not explicitly clarify a corresponding relationship between the tone-mapping function and the generated HDR image, but this relationship is vital when guiding the reconstruction of HDR images. To address this problem, we propose a method to explicitly estimate the tone mapping function and its corresponding HDR image in one network. Firstly, based on the characteristics of the tone mapping function, we construct a model by a polynomial to describe the trend of the tone curve. To fit this curve, we use a learnable network to estimate the coefficients of the polynomial. This curve will be automatically adjusted according to the tone space of the Low Dynamic Range (LDR) image, and reconstruct the real HDR image. Besides, since all current datasets do not provide the corresponding relationship between the tone mapping function and the LDR image, we construct a new dataset with both synthetic and real images. Extensive experiments show that our method generalizes well under different tone-mapping functions and achieves SOTA performance.
</details>
<details>
<summary>摘要</summary>
To address this problem, we propose a method to explicitly estimate the tone mapping function and its corresponding HDR image in one network. Firstly, based on the characteristics of the tone mapping function, we construct a model using a polynomial to describe the trend of the tone curve. To fit this curve, we use a learnable network to estimate the coefficients of the polynomial. This curve will be automatically adjusted according to the tone space of the Low Dynamic Range (LDR) image, and reconstruct the real HDR image.Moreover, since all current datasets do not provide the corresponding relationship between the tone mapping function and the LDR image, we construct a new dataset with both synthetic and real images. Extensive experiments show that our method generalizes well under different tone-mapping functions and achieves state-of-the-art (SOTA) performance.
</details></li>
</ul>
<hr>
<h2 id="DRAW-Defending-Camera-shooted-RAW-against-Image-Manipulation"><a href="#DRAW-Defending-Camera-shooted-RAW-against-Image-Manipulation" class="headerlink" title="DRAW: Defending Camera-shooted RAW against Image Manipulation"></a>DRAW: Defending Camera-shooted RAW against Image Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16418">http://arxiv.org/abs/2307.16418</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoxiao Hu, Qichao Ying, Zhenxing Qian, Sheng Li, Xinpeng Zhang</li>
<li>for: 防止图像被黑客修改和篡改</li>
<li>methods: 使用 Multi-frequency Partial Fusion Network (MPF-Net) 和隐藏水印技术保护原始 RAW 数据</li>
<li>results: 在多个知名 RAW 数据集上进行了广泛的实验，证明方法的有效性<details>
<summary>Abstract</summary>
RAW files are the initial measurement of scene radiance widely used in most cameras, and the ubiquitously-used RGB images are converted from RAW data through Image Signal Processing (ISP) pipelines. Nowadays, digital images are risky of being nefariously manipulated. Inspired by the fact that innate immunity is the first line of body defense, we propose DRAW, a novel scheme of defending images against manipulation by protecting their sources, i.e., camera-shooted RAWs. Specifically, we design a lightweight Multi-frequency Partial Fusion Network (MPF-Net) friendly to devices with limited computing resources by frequency learning and partial feature fusion. It introduces invisible watermarks as protective signal into the RAW data. The protection capability can not only be transferred into the rendered RGB images regardless of the applied ISP pipeline, but also is resilient to post-processing operations such as blurring or compression. Once the image is manipulated, we can accurately identify the forged areas with a localization network. Extensive experiments on several famous RAW datasets, e.g., RAISE, FiveK and SIDD, indicate the effectiveness of our method. We hope that this technique can be used in future cameras as an option for image protection, which could effectively restrict image manipulation at the source.
</details>
<details>
<summary>摘要</summary>
Raw 文件是现场辐射广泛使用的初始测量数据，而通用的RGB图像则是通过图像信号处理（ISP）管道从 Raw 数据转换。在当今的数字图像中，图像被虚构性修改的风险日益增加。 Drawing 灵感于人体自然免疫系统的第一线防御，我们提出了一种新的图像防止修改方法，即保护图像的来源，即摄像机拍摄的 Raw。我们设计了一种轻量级多频分解网络（MPF-Net），可以在设备有限的计算资源下运行。这种网络通过频率学习和部分特征融合，将隐藏的水印插入 Raw 数据中。这种保护能力不仅可以在渲染后的 RGB 图像中传递，而且对压缩或滤清等后处理操作也具有抗性。如果图像被修改，我们可以使用本地化网络来准确地确定forge 区域。我们在许多知名 RAW 数据集（如 RAISE、FiveK 和 SIDD）进行了广泛的实验，并表明了我们的方法的有效性。我们希望这种技术可以在未来的相机中作为图像保护选项，以防止图像修改。
</details></li>
</ul>
<hr>
<h2 id="Multi-modal-Graph-Neural-Network-for-Early-Diagnosis-of-Alzheimer’s-Disease-from-sMRI-and-PET-Scans"><a href="#Multi-modal-Graph-Neural-Network-for-Early-Diagnosis-of-Alzheimer’s-Disease-from-sMRI-and-PET-Scans" class="headerlink" title="Multi-modal Graph Neural Network for Early Diagnosis of Alzheimer’s Disease from sMRI and PET Scans"></a>Multi-modal Graph Neural Network for Early Diagnosis of Alzheimer’s Disease from sMRI and PET Scans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16366">http://arxiv.org/abs/2307.16366</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanteng Zhanga, Xiaohai He, Yi Hao Chan, Qizhi Teng, Jagath C. Rajapakse</li>
<li>for: 这个论文是为了早期诊断阿尔兹海默病（AD）而写的。</li>
<li>methods: 这个论文使用了图神经网络（GNN）来处理非欧几何图像，并在人口图框架中结合了成像特征和临床信息。</li>
<li>results: 这个研究表明，结合多种数据类型的多模态方法可以提高AD诊断性能，并且提供了技术参考和支持多变量多模态诊断方法的需求。<details>
<summary>Abstract</summary>
In recent years, deep learning models have been applied to neuroimaging data for early diagnosis of Alzheimer's disease (AD). Structural magnetic resonance imaging (sMRI) and positron emission tomography (PET) images provide structural and functional information about the brain, respectively. Combining these features leads to improved performance than using a single modality alone in building predictive models for AD diagnosis. However, current multi-modal approaches in deep learning, based on sMRI and PET, are mostly limited to convolutional neural networks, which do not facilitate integration of both image and phenotypic information of subjects. We propose to use graph neural networks (GNN) that are designed to deal with problems in non-Euclidean domains. In this study, we demonstrate how brain networks can be created from sMRI or PET images and be used in a population graph framework that can combine phenotypic information with imaging features of these brain networks. Then, we present a multi-modal GNN framework where each modality has its own branch of GNN and a technique is proposed to combine the multi-modal data at both the level of node vectors and adjacency matrices. Finally, we perform late fusion to combine the preliminary decisions made in each branch and produce a final prediction. As multi-modality data becomes available, multi-source and multi-modal is the trend of AD diagnosis. We conducted explorative experiments based on multi-modal imaging data combined with non-imaging phenotypic information for AD diagnosis and analyzed the impact of phenotypic information on diagnostic performance. Results from experiments demonstrated that our proposed multi-modal approach improves performance for AD diagnosis, and this study also provides technical reference and support the need for multivariate multi-modal diagnosis methods.
</details>
<details>
<summary>摘要</summary>
在最近的几年中，深度学习模型已经应用于神经成像数据，以提前诊断阿尔ц海默病（AD）。Structural磁共振成像（sMRI）和 пози트рон发射Tomography（PET）图像提供了脑部结构和功能信息，分别。将这些特征相结合，可以提高建模AD诊断性能，比使用单一模式 alone。然而，目前的多Modal Approach在深度学习中，基于sMRI和PET，主要是基于卷积神经网络，这些网络不会涉及到图像和subject的特征信息的集成。我们提议使用图гра数据（GNN），这些GNN是非欧几何问题的专门设计。在这个研究中，我们示出了如何从sMRI或PET图像中创建脑网络，并在人群图ogram框架中结合图像特征和subject的特征信息。然后，我们提出了一种多Modal GNN框架，其中每个模式有自己的GNN分支，并提出了将多Modal数据在每个分支和邻接矩阵的水平进行组合。最后，我们进行了晚期融合，将每个分支的初步决策相互融合，并生成最终预测。随着多Modal数据变得更加可用，多源多Modal是AD诊断的趋势。我们基于多Modal成像数据和非成像特征信息进行了探索性实验，分析了影响诊断性能的非成像信息。实验结果表明，我们提出的多Modal方法可以提高AD诊断性能，这也提供了技术参考，支持多变量多Modal诊断方法的需求。
</details></li>
</ul>
<hr>
<h2 id="Cardiac-MRI-Orientation-Recognition-and-Standardization-using-Deep-Neural-Networks"><a href="#Cardiac-MRI-Orientation-Recognition-and-Standardization-using-Deep-Neural-Networks" class="headerlink" title="Cardiac MRI Orientation Recognition and Standardization using Deep Neural Networks"></a>Cardiac MRI Orientation Recognition and Standardization using Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00615">http://arxiv.org/abs/2308.00615</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rxzhen/mscmr-orient">https://github.com/rxzhen/mscmr-orient</a></li>
<li>paper_authors: Ruoxuan Zhen</li>
<li>for: 本研究旨在提高医疗影像处理任务的效果，通过深度学习方法对医疗影像的方向识别和预测进行改进。</li>
<li>methods: 本研究使用深度神经网络对医疗影像的方向进行分类和标准化，并提出了一种传输学习策略，以适应不同的模式和序列。</li>
<li>results: 在不同的CMR模式和序列上进行了广泛的实验，Validation accuracy分别为100.0%、100.0%和99.4%，证明了我们的模型的稳定性和效果。<details>
<summary>Abstract</summary>
Orientation recognition and standardization play a crucial role in the effectiveness of medical image processing tasks. Deep learning-based methods have proven highly advantageous in orientation recognition and prediction tasks. In this paper, we address the challenge of imaging orientation in cardiac MRI and present a method that employs deep neural networks to categorize and standardize the orientation. To cater to multiple sequences and modalities of MRI, we propose a transfer learning strategy, enabling adaptation of our model from a single modality to diverse modalities. We conducted comprehensive experiments on CMR images from various modalities, including bSSFP, T2, and LGE. The validation accuracies achieved were 100.0\%, 100.0\%, and 99.4\%, confirming the robustness and effectiveness of our model. Our source code and network models are available at https://github.com/rxzhen/MSCMR-orient
</details>
<details>
<summary>摘要</summary>
媒体方向识别和标准化在医学影像处理任务中发挥关键作用。深度学习基于方法在媒体方向识别和预测任务中表现出了高度的优势。本文探讨了卡达影像中的媒体方向识别挑战，并提出了使用深度神经网络来分类和标准化媒体方向。为了适应不同的序列和模式，我们提议了传输学习策略，以便我们的模型从单一模式适应到多种模式。我们在不同的CMR模式图像上进行了广泛的实验，包括bSSFP、T2和LGE。实验结果显示，我们的模型在这些模式上的验证精度分别为100.0%、100.0%和99.4%，这证明了我们的模型的稳定性和效果。我们的源代码和网络模型可以在<https://github.com/rxzhen/MSCMR-orient>获取。
</details></li>
</ul>
<hr>
<h2 id="An-objective-validation-of-polyp-and-instrument-segmentation-methods-in-colonoscopy-through-Medico-2020-polyp-segmentation-and-MedAI-2021-transparency-challenges"><a href="#An-objective-validation-of-polyp-and-instrument-segmentation-methods-in-colonoscopy-through-Medico-2020-polyp-segmentation-and-MedAI-2021-transparency-challenges" class="headerlink" title="An objective validation of polyp and instrument segmentation methods in colonoscopy through Medico 2020 polyp segmentation and MedAI 2021 transparency challenges"></a>An objective validation of polyp and instrument segmentation methods in colonoscopy through Medico 2020 polyp segmentation and MedAI 2021 transparency challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16262">http://arxiv.org/abs/2307.16262</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/georgebatch/kvasir-seg">https://github.com/georgebatch/kvasir-seg</a></li>
<li>paper_authors: Debesh Jha, Vanshali Sharma, Debapriya Banik, Debayan Bhattacharya, Kaushiki Roy, Steven A. Hicks, Nikhil Kumar Tomar, Vajira Thambawita, Adrian Krenzer, Ge-Peng Ji, Sahadev Poudel, George Batchkala, Saruar Alam, Awadelrahman M. A. Ahmed, Quoc-Huy Trinh, Zeshan Khan, Tien-Phat Nguyen, Shruti Shrestha, Sabari Nathan, Jeonghwan Gwak, Ritika K. Jha, Zheyuan Zhang, Alexander Schlaefer, Debotosh Bhattacharjee, M. K. Bhuyan, Pradip K. Das, Sravanthi Parsa, Sharib Ali, Michael A. Riegler, Pål Halvorsen, Ulas Bagci, Thomas De Lange</li>
<li>for: 这篇研究旨在探讨自动分析潜水镜影像的技术，以实时检测潜在癌症的变化。</li>
<li>methods: 这篇研究使用深度学习技术来协助镜头检查医生检测和分类潜在癌症和异常。</li>
<li>results: 这篇研究发现了许多高精度的方法，并且评估了这些方法的可读性和可重现性。<details>
<summary>Abstract</summary>
Automatic analysis of colonoscopy images has been an active field of research motivated by the importance of early detection of precancerous polyps. However, detecting polyps during the live examination can be challenging due to various factors such as variation of skills and experience among the endoscopists, lack of attentiveness, and fatigue leading to a high polyp miss-rate. Deep learning has emerged as a promising solution to this challenge as it can assist endoscopists in detecting and classifying overlooked polyps and abnormalities in real time. In addition to the algorithm's accuracy, transparency and interpretability are crucial to explaining the whys and hows of the algorithm's prediction. Further, most algorithms are developed in private data, closed source, or proprietary software, and methods lack reproducibility. Therefore, to promote the development of efficient and transparent methods, we have organized the "Medico automatic polyp segmentation (Medico 2020)" and "MedAI: Transparency in Medical Image Segmentation (MedAI 2021)" competitions. We present a comprehensive summary and analyze each contribution, highlight the strength of the best-performing methods, and discuss the possibility of clinical translations of such methods into the clinic. For the transparency task, a multi-disciplinary team, including expert gastroenterologists, accessed each submission and evaluated the team based on open-source practices, failure case analysis, ablation studies, usability and understandability of evaluations to gain a deeper understanding of the models' credibility for clinical deployment. Through the comprehensive analysis of the challenge, we not only highlight the advancements in polyp and surgical instrument segmentation but also encourage qualitative evaluation for building more transparent and understandable AI-based colonoscopy systems.
</details>
<details>
<summary>摘要</summary>
自动分析幽门影像是一个活跃的研究领域，受到早期发现前期癌肿的重要性启发。然而，在实时检查中检测肿块可以是困难的，因为医生的技能和经验水平各不相同，注意力不集中，疲劳导致肿块漏检率高。深度学习作为一种有前途的解决方案，可以帮助医生在实时检查中检测和分类掉肿和异常。除了算法的准确性外，透明性和可解释性是解释算法预测的关键因素。然而，大多数算法是在私有数据、关闭源代码或商业软件上开发的，lacks reproducibility。因此，为促进效率和透明的方法的发展，我们组织了“医疗自动肿块分割（Medico 2020）”和“MedAI：医疗图像分割透明（MedAI 2021）”比赛。我们对每一个提交进行了全面的总结和分析，抛光最佳方法的优势，并讨论了这些方法在临床应用中的可能性。在透明任务中，一个多 disciplinary 团队，包括专业的 Gastroenterologist，对每一个提交进行了评估，评估基于开源实践、失败案例分析、剪辑研究、可用性和理解度来深入了解模型的可靠性。通过全面分析挑战，我们不仅强调了肿块和手术工具分割的进步，还鼓励了更加TRANSPARENT和可理解的 AI 基于幽门检查系统的建立。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/31/eess.IV_2023_07_31/" data-id="cllsiju3x008na388eoem3bgo" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_07_30" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/30/cs.LG_2023_07_30/" class="article-date">
  <time datetime="2023-07-29T16:00:00.000Z" itemprop="datePublished">2023-07-30</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/30/cs.LG_2023_07_30/">cs.LG - 2023-07-30 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Efficient-Federated-Learning-via-Local-Adaptive-Amended-Optimizer-with-Linear-Speedup"><a href="#Efficient-Federated-Learning-via-Local-Adaptive-Amended-Optimizer-with-Linear-Speedup" class="headerlink" title="Efficient Federated Learning via Local Adaptive Amended Optimizer with Linear Speedup"></a>Efficient Federated Learning via Local Adaptive Amended Optimizer with Linear Speedup</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00522">http://arxiv.org/abs/2308.00522</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yan Sun, Li Shen, Hao Sun, Liang Ding, Dacheng Tao</li>
<li>for: 提高 federated learning（FL）中的 adaptive 优化效率，解决 rugged convergence 和 client drifts 问题。</li>
<li>methods: 提出了一种基于 momentum 的 globally adaptive 和 locally amended 优化算法， named Federated Local ADaptive Amended optimizer (\textit{FedLADA），通过 estimate global average offset 和 correct local offset 来提高 empirical training speed 和 mitigate heterogeneous over-fitting。</li>
<li>results: 在实际世界数据集上进行了广泛的实验，证明了我们提出的 \textit{FedLADA} 可以减少 communication rounds 并 достичь更高的准确率，比如基础方法更高。<details>
<summary>Abstract</summary>
Adaptive optimization has achieved notable success for distributed learning while extending adaptive optimizer to federated Learning (FL) suffers from severe inefficiency, including (i) rugged convergence due to inaccurate gradient estimation in global adaptive optimizer; (ii) client drifts exacerbated by local over-fitting with the local adaptive optimizer. In this work, we propose a novel momentum-based algorithm via utilizing the global gradient descent and locally adaptive amended optimizer to tackle these difficulties. Specifically, we incorporate a locally amended technique to the adaptive optimizer, named Federated Local ADaptive Amended optimizer (\textit{FedLADA}), which estimates the global average offset in the previous communication round and corrects the local offset through a momentum-like term to further improve the empirical training speed and mitigate the heterogeneous over-fitting. Theoretically, we establish the convergence rate of \textit{FedLADA} with a linear speedup property on the non-convex case under the partial participation settings. Moreover, we conduct extensive experiments on the real-world dataset to demonstrate the efficacy of our proposed \textit{FedLADA}, which could greatly reduce the communication rounds and achieves higher accuracy than several baselines.
</details>
<details>
<summary>摘要</summary>
适应优化在分布式学习中取得了显著的成功，但扩展适应优化到联邦学习（FL）中受到严重的不fficient，包括（i） rugged convergence due to inaccurate gradient estimation in global adaptive optimizer;（ii） client drifts exacerbated by local over-fitting with the local adaptive optimizer。在这种工作中，我们提议一种新的旋转矩阵-based algorithm，使用全球梯度下降和本地适应修改优化器来解决这些困难。Specifically, we incorporate a locally amended technique to the adaptive optimizer, named Federated Local ADaptive Amended optimizer (\textit{FedLADA}), which estimates the global average offset in the previous communication round and corrects the local offset through a momentum-like term to further improve the empirical training speed and mitigate the heterogeneous over-fitting。理论上，我们证明\textit{FedLADA}在非对称情况下具有线性快速性质。此外，我们对实际数据集进行了广泛的实验，以确认我们的提议的\textit{FedLADA}的有效性，它可以大幅减少通信runds和实现更高的准确率than several baselines。
</details></li>
</ul>
<hr>
<h2 id="DRL4Route-A-Deep-Reinforcement-Learning-Framework-for-Pick-up-and-Delivery-Route-Prediction"><a href="#DRL4Route-A-Deep-Reinforcement-Learning-Framework-for-Pick-up-and-Delivery-Route-Prediction" class="headerlink" title="DRL4Route: A Deep Reinforcement Learning Framework for Pick-up and Delivery Route Prediction"></a>DRL4Route: A Deep Reinforcement Learning Framework for Pick-up and Delivery Route Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16246">http://arxiv.org/abs/2307.16246</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/maoxiaowei97/drl4route">https://github.com/maoxiaowei97/drl4route</a></li>
<li>paper_authors: Xiaowei Mao, Haomin Wen, Hengrui Zhang, Huaiyu Wan, Lixia Wu, Jianbin Zheng, Haoyuan Hu, Youfang Lin</li>
<li>For: 预测工作者的服务路线（PDRP），即根据工作者当前任务池估算未来服务路线。* Methods: 基于强化学习的 Deep Reinforcement Learning（DRL）框架，combines 前一代深度学习模型的行为学习能力和强化学习的非 differentiable 目标优化能力。* Results: 对实际数据集进行了广泛的 Offline 实验和在线部署，并显示了对 Location Square Deviation（LSD）和 Accuracy@3（ACC@3）的改进，比 existed 方法提高 0.9%-2.7% 和 2.4%-3.2%。<details>
<summary>Abstract</summary>
Pick-up and Delivery Route Prediction (PDRP), which aims to estimate the future service route of a worker given his current task pool, has received rising attention in recent years. Deep neural networks based on supervised learning have emerged as the dominant model for the task because of their powerful ability to capture workers' behavior patterns from massive historical data. Though promising, they fail to introduce the non-differentiable test criteria into the training process, leading to a mismatch in training and test criteria. Which considerably trims down their performance when applied in practical systems. To tackle the above issue, we present the first attempt to generalize Reinforcement Learning (RL) to the route prediction task, leading to a novel RL-based framework called DRL4Route. It combines the behavior-learning abilities of previous deep learning models with the non-differentiable objective optimization ability of reinforcement learning. DRL4Route can serve as a plug-and-play component to boost the existing deep learning models. Based on the framework, we further implement a model named DRL4Route-GAE for PDRP in logistic service. It follows the actor-critic architecture which is equipped with a Generalized Advantage Estimator that can balance the bias and variance of the policy gradient estimates, thus achieving a more optimal policy. Extensive offline experiments and the online deployment show that DRL4Route-GAE improves Location Square Deviation (LSD) by 0.9%-2.7%, and Accuracy@3 (ACC@3) by 2.4%-3.2% over existing methods on the real-world dataset.
</details>
<details>
<summary>摘要</summary>
Pick-up and Delivery Route Prediction (PDRP) 已经在最近几年内收到了越来越多的关注，旨在预测工作者当前任务池中的未来服务路线。基于深度学习的神经网络在这个任务上已经成为主流模型，因为它们可以很好地捕捉工作者的行为模式从大量历史数据中。虽然有前途，但它们却无法将不可导的测试标准引入训练过程中，导致训练和测试标准之间的匹配性受到影响，从而限制它们在实际系统中的表现。为解决这个问题，我们提出了将强化学习（Reinforcement Learning，RL）应用于路径预测任务，并提出了一个基于RL的框架called DRL4Route。它将前面的深度学习模型的行为学习能力与RL的非导数优化能力结合起来，以提高路径预测的性能。DRL4Route可以作为现有深度学习模型的插件来使用。基于该框架，我们进一步实现了一个模型名为DRL4Route-GAE，用于PDRP在物流服务中。它采用actor-critic架构，并配备一个通用优化估计器，可以平衡政策评估器的偏移和方差，从而实现更优的政策。在大量的离线实验和在线部署中，DRL4Route-GAE在实际数据集上提高了Location Square Deviation（LSD）和Accuracy@3（ACC@3）的性能，分别提高了0.9%-2.7%和2.4%-3.2%。
</details></li>
</ul>
<hr>
<h2 id="Synaptic-Plasticity-Models-and-Bio-Inspired-Unsupervised-Deep-Learning-A-Survey"><a href="#Synaptic-Plasticity-Models-and-Bio-Inspired-Unsupervised-Deep-Learning-A-Survey" class="headerlink" title="Synaptic Plasticity Models and Bio-Inspired Unsupervised Deep Learning: A Survey"></a>Synaptic Plasticity Models and Bio-Inspired Unsupervised Deep Learning: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16236">http://arxiv.org/abs/2307.16236</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gabriele Lagani, Fabrizio Falchi, Claudio Gennaro, Giuseppe Amato</li>
<li>for: 探讨生物基于深度学习的研究方向，包括生物启发式 synaptic plasticity 模型的应用在深度学习场景下，以及与顺序神经网络（SNNs）的相似模型。</li>
<li>methods: 文章探讨了一系列生物启发式的深度学习模型，包括 synaptic plasticity 模型的应用在深度学习场景下，以及与 SNNs 的相似模型。</li>
<li>results: 文章表明，生物启发式的深度学习模型可以提高深度学习的稳定性和可靠性，并且可以启发新的深度学习算法和模型。<details>
<summary>Abstract</summary>
Recently emerged technologies based on Deep Learning (DL) achieved outstanding results on a variety of tasks in the field of Artificial Intelligence (AI). However, these encounter several challenges related to robustness to adversarial inputs, ecological impact, and the necessity of huge amounts of training data. In response, researchers are focusing more and more interest on biologically grounded mechanisms, which are appealing due to the impressive capabilities exhibited by biological brains. This survey explores a range of these biologically inspired models of synaptic plasticity, their application in DL scenarios, and the connections with models of plasticity in Spiking Neural Networks (SNNs). Overall, Bio-Inspired Deep Learning (BIDL) represents an exciting research direction, aiming at advancing not only our current technologies but also our understanding of intelligence.
</details>
<details>
<summary>摘要</summary>
现代深度学习（DL）技术在人工智能（AI）领域取得了杰出的成绩，但这些技术受到了对抗式输入、生态影响和对训练数据的需求等挑战。为了回应这些挑战，研究人员在注意力中心的生物灵活机制，这些机制具有生物大脑的优秀表现所吸引了研究人员的注意。本调查探讨了一些生物启发的 synaptic plasticity 模型，其应用于深度学习情况下，以及与对抗式神经网络（SNNs）的模型之间的连结。总之，生物启发深度学习（BIDL）是一个充满挑战和探索的研究方向，它不仅可以提高我们现有的技术，而且可以帮助我们更好地理解智慧的本质。
</details></li>
</ul>
<hr>
<h2 id="Spiking-Neural-Networks-and-Bio-Inspired-Supervised-Deep-Learning-A-Survey"><a href="#Spiking-Neural-Networks-and-Bio-Inspired-Supervised-Deep-Learning-A-Survey" class="headerlink" title="Spiking Neural Networks and Bio-Inspired Supervised Deep Learning: A Survey"></a>Spiking Neural Networks and Bio-Inspired Supervised Deep Learning: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16235">http://arxiv.org/abs/2307.16235</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gabriele Lagani, Fabrizio Falchi, Claudio Gennaro, Giuseppe Amato</li>
<li>for: 这个论文旨在为人工智能技术的发展提供生物学和神经科学领域的灵感，并进行了生物体系中计算和 synaptic plasticity 的介绍，以及现代神经网络模型的详细介绍。</li>
<li>methods: 论文使用了生物体系中计算的主要原则，以及现代神经网络模型的许多bio-inspired training方法，包括不同于backprop的优化方法，以提高现代神经网络模型的计算能力和生物可能性。</li>
<li>results: 论文提出了一种 Bio-Inspired Deep Learning (BIDL) 方法，可以提高现代神经网络模型的计算能力和生物可能性，并且可以作为传统的 backprop 优化方法的替代方案。<details>
<summary>Abstract</summary>
For a long time, biology and neuroscience fields have been a great source of inspiration for computer scientists, towards the development of Artificial Intelligence (AI) technologies. This survey aims at providing a comprehensive review of recent biologically-inspired approaches for AI. After introducing the main principles of computation and synaptic plasticity in biological neurons, we provide a thorough presentation of Spiking Neural Network (SNN) models, and we highlight the main challenges related to SNN training, where traditional backprop-based optimization is not directly applicable. Therefore, we discuss recent bio-inspired training methods, which pose themselves as alternatives to backprop, both for traditional and spiking networks. Bio-Inspired Deep Learning (BIDL) approaches towards advancing the computational capabilities and biological plausibility of current models.
</details>
<details>
<summary>摘要</summary>
for a long time, biology and neuroscience fields have been a great source of inspiration for computer scientists, towards the development of Artificial Intelligence (AI) technologies. This survey aims at providing a comprehensive review of recent biologically-inspired approaches for AI. After introducing the main principles of computation and synaptic plasticity in biological neurons, we provide a thorough presentation of Spiking Neural Network (SNN) models, and we highlight the main challenges related to SNN training, where traditional backprop-based optimization is not directly applicable. Therefore, we discuss recent bio-inspired training methods, which pose themselves as alternatives to backprop, both for traditional and spiking networks. Bio-Inspired Deep Learning (BIDL) approaches towards advancing the computational capabilities and biological plausibility of current models.Here's the text in Traditional Chinese:for a long time, biology and neuroscience fields have been a great source of inspiration for computer scientists, towards the development of Artificial Intelligence (AI) technologies. This survey aims at providing a comprehensive review of recent biologically-inspired approaches for AI. After introducing the main principles of computation and synaptic plasticity in biological neurons, we provide a thorough presentation of Spiking Neural Network (SNN) models, and we highlight the main challenges related to SNN training, where traditional backprop-based optimization is not directly applicable. Therefore, we discuss recent bio-inspired training methods, which pose themselves as alternatives to backprop, both for traditional and spiking networks. Bio-Inspired Deep Learning (BIDL) approaches towards advancing the computational capabilities and biological plausibility of current models.
</details></li>
</ul>
<hr>
<h2 id="Robust-Electric-Vehicle-Balancing-of-Autonomous-Mobility-On-Demand-System-A-Multi-Agent-Reinforcement-Learning-Approach"><a href="#Robust-Electric-Vehicle-Balancing-of-Autonomous-Mobility-On-Demand-System-A-Multi-Agent-Reinforcement-Learning-Approach" class="headerlink" title="Robust Electric Vehicle Balancing of Autonomous Mobility-On-Demand System: A Multi-Agent Reinforcement Learning Approach"></a>Robust Electric Vehicle Balancing of Autonomous Mobility-On-Demand System: A Multi-Agent Reinforcement Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16228">http://arxiv.org/abs/2307.16228</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sihong He, Shuo Han, Fei Miao<br>for:This paper focuses on designing a multi-agent reinforcement learning (MARL) framework for electric autonomous vehicles (EAVs) balancing in future autonomous mobility-on-demand (AMoD) systems, with the goal of improving the supply-demand ratio and charging utilization rate across the whole city.methods:The proposed MARL-based framework considers both the EAVs supply and mobility demand uncertainties, and uses adversarial agents to model these uncertainties. The proposed algorithm is called REBAMA (Robust E-AMoD Balancing MARL) and it trains a robust EAVs balancing policy to improve the reward, charging utilization fairness, and supply-demand fairness.results:Experiments show that the proposed REBAMA algorithm performs better compared with a non-robust MARL method and a robust optimization-based method, with improvements of 19.28%, 28.18%, and 3.97% in terms of reward, charging utilization fairness, and supply-demand fairness, respectively.<details>
<summary>Abstract</summary>
Electric autonomous vehicles (EAVs) are getting attention in future autonomous mobility-on-demand (AMoD) systems due to their economic and societal benefits. However, EAVs' unique charging patterns (long charging time, high charging frequency, unpredictable charging behaviors, etc.) make it challenging to accurately predict the EAVs supply in E-AMoD systems. Furthermore, the mobility demand's prediction uncertainty makes it an urgent and challenging task to design an integrated vehicle balancing solution under supply and demand uncertainties. Despite the success of reinforcement learning-based E-AMoD balancing algorithms, state uncertainties under the EV supply or mobility demand remain unexplored. In this work, we design a multi-agent reinforcement learning (MARL)-based framework for EAVs balancing in E-AMoD systems, with adversarial agents to model both the EAVs supply and mobility demand uncertainties that may undermine the vehicle balancing solutions. We then propose a robust E-AMoD Balancing MARL (REBAMA) algorithm to train a robust EAVs balancing policy to balance both the supply-demand ratio and charging utilization rate across the whole city. Experiments show that our proposed robust method performs better compared with a non-robust MARL method that does not consider state uncertainties; it improves the reward, charging utilization fairness, and supply-demand fairness by 19.28%, 28.18%, and 3.97%, respectively. Compared with a robust optimization-based method, the proposed MARL algorithm can improve the reward, charging utilization fairness, and supply-demand fairness by 8.21%, 8.29%, and 9.42%, respectively.
</details>
<details>
<summary>摘要</summary>
电动自驾车 (EAVs) 在未来的自动化移动服务系统 (AMoD) 中受到关注，因为它们具有经济和社会上的优势。然而，EAVs 的充电模式 (长时间充电、高频充电、不可预测的充电行为等) 使得预测 EAVs 供应很具有挑战性。此外，移动需求的预测不确定性使得设计一个集成的车辆均衡解决方案变得非常重要和挑战。虽然束规学学习基于 E-AMoD 的均衡算法得到了成功，但是状态不确定性下的 EV 供应或移动需求仍然未得到探讨。在这种情况下，我们设计了一个多代理束规学 (MARL) 基础框架，用于 EAVs 均衡在 E-AMoD 系统中。我们在这个框架中引入了对 EAVs 供应和移动需求不确定性的模型，使得我们可以训练一个robust的 EAVs 均衡策略，以保证供应和需求之间的均衡。实验表明，我们提出的robust方法在比较非robust MARL 方法时表现更好，提高了奖励、充电使用公平和供应需求公平的三个指标，分别提高了19.28%、28.18%和3.97%。相比于一种robust优化基于方法，我们的 MARL 算法可以提高这些指标的提升，分别为8.21%、8.29%和9.42%。
</details></li>
</ul>
<hr>
<h2 id="Optimizing-the-Neural-Network-Training-for-OCR-Error-Correction-of-Historical-Hebrew-Texts"><a href="#Optimizing-the-Neural-Network-Training-for-OCR-Error-Correction-of-Historical-Hebrew-Texts" class="headerlink" title="Optimizing the Neural Network Training for OCR Error Correction of Historical Hebrew Texts"></a>Optimizing the Neural Network Training for OCR Error Correction of Historical Hebrew Texts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16220">http://arxiv.org/abs/2307.16220</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/smartinternz02/SI-GuidedProject-2307-1622049182">https://github.com/smartinternz02/SI-GuidedProject-2307-1622049182</a></li>
<li>paper_authors: Omri Suissa, Avshalom Elmalech, Maayan Zhitomirsky-Geffet</li>
<li>for: 这篇论文的目的是提出一种可以在历史文献中进行Optical Character Recognition（OCR）后修正的轻量级神经网络训练方法，并 Investigate which type of dataset is the most effective for OCR post-correction of historical documents.</li>
<li>methods: 该论文使用了一种新的方法，即通过自动生成语言和任务特定的训练数据来提高神经网络的OCR后修正结果。</li>
<li>results: 研究发现，使用该方法训练神经网络可以更有效地 Correct OCR errors，并且其性能与其他现有的神经网络和复杂的检查器相比较高。<details>
<summary>Abstract</summary>
Over the past few decades, large archives of paper-based documents such as books and newspapers have been digitized using Optical Character Recognition. This technology is error-prone, especially for historical documents. To correct OCR errors, post-processing algorithms have been proposed based on natural language analysis and machine learning techniques such as neural networks. Neural network's disadvantage is the vast amount of manually labeled data required for training, which is often unavailable. This paper proposes an innovative method for training a light-weight neural network for Hebrew OCR post-correction using significantly less manually created data. The main research goal is to develop a method for automatically generating language and task-specific training data to improve the neural network results for OCR post-correction, and to investigate which type of dataset is the most effective for OCR post-correction of historical documents. To this end, a series of experiments using several datasets was conducted. The evaluation corpus was based on Hebrew newspapers from the JPress project. An analysis of historical OCRed newspapers was done to learn common language and corpus-specific OCR errors. We found that training the network using the proposed method is more effective than using randomly generated errors. The results also show that the performance of the neural network for OCR post-correction strongly depends on the genre and area of the training data. Moreover, neural networks that were trained with the proposed method outperform other state-of-the-art neural networks for OCR post-correction and complex spellcheckers. These results may have practical implications for many digital humanities projects.
</details>
<details>
<summary>摘要</summary>
在过去几十年，大量的纸质文档如书籍和报纸被使用光学字符识别技术进行数字化。这种技术存在误差，尤其是对历史文档。为了纠正OCR误差，基于自然语言分析和机器学习技术的后处理算法被提议。但是，这些算法需要大量的手动标注数据来训练，却经常不可得。这篇论文提出了一种创新的方法，用于使用较少的手动创建数据来训练轻量级神经网络进行希伯来文OCR后处理。研究的主要目标是开发一种自动生成语言和任务特定的训练数据，以改进神经网络的OCR后处理结果，并investigate历史文档OCR后处理中最有效的数据集。为此，我们进行了一系列实验，使用了多个数据集。评估集基于希伯来报纸JPress项目。我们分析了历史OCR后的报纸，以了解希伯来文OCR误差的常见语言和核心特征。我们发现，使用我们提议的方法训练神经网络比使用随机生成的误差更有效。结果还表明，神经网络在OCR后处理中的表现强烈取决于训练数据的类别和地区。此外，使用我们提议的方法训练的神经网络在OCR后处理和复杂的拼写检查器之间表现更好。这些结果可能对数字人文学科项目产生实际影响。
</details></li>
</ul>
<hr>
<h2 id="Improving-Probabilistic-Bisimulation-for-MDPs-Using-Machine-Learning"><a href="#Improving-Probabilistic-Bisimulation-for-MDPs-Using-Machine-Learning" class="headerlink" title="Improving Probabilistic Bisimulation for MDPs Using Machine Learning"></a>Improving Probabilistic Bisimulation for MDPs Using Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02519">http://arxiv.org/abs/2308.02519</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammadsadegh Mohaghegh, Khayyam Salehi</li>
<li>for:  This paper aims to address the state space explosion problem in applying model checking to complex systems, and to improve the efficiency of probabilistic bisimulation for stochastic systems with nondeterministic behaviors.</li>
<li>methods:  The paper proposes a new technique that uses machine learning classification techniques to approximate the partition of the state space of a given probabilistic model into its bisimulation classes. The technique is based on the PRISM program of the model and constructs small versions of the model to train a classifier.</li>
<li>results:  The experimental results show that the proposed approach can significantly decrease the running time compared to state-of-the-art tools.<details>
<summary>Abstract</summary>
The utilization of model checking has been suggested as a formal verification technique for analyzing critical systems. However, the primary challenge in applying to complex systems is state space explosion problem. To address this issue, bisimulation minimization has emerged as a prominent method for reducing the number of states in a labeled transition system, aiming to overcome the difficulties associated with the state space explosion problem. In the case of systems exhibiting stochastic behaviors, probabilistic bisimulation is employed to minimize a given model, obtaining its equivalent form with fewer states. Recently, various techniques have been introduced to decrease the time complexity of the iterative methods used to compute probabilistic bisimulation for stochastic systems that display nondeterministic behaviors. In this paper, we propose a new technique to partition the state space of a given probabilistic model to its bisimulation classes. This technique uses the PRISM program of a given model and constructs some small versions of the model to train a classifier. It then applies machine learning classification techniques to approximate the related partition. The resulting partition is used as an initial one for the standard bisimulation technique in order to reduce the running time of the method. The experimental results show that the approach can decrease significantly the running time compared to state-of-the-art tools.
</details>
<details>
<summary>摘要</summary>
utilization of model checking 被建议为形式验证技术，用于分析重要系统。然而，主要挑战在应用于复杂系统时是状态空间爆炸问题。为解决这个问题， bisimulation minimization  emerged as a prominent method for reducing the number of states in a labeled transition system, aiming to overcome the difficulties associated with the state space explosion problem. In the case of systems exhibiting stochastic behaviors, probabilistic bisimulation is employed to minimize a given model, obtaining its equivalent form with fewer states. Recently, various techniques have been introduced to decrease the time complexity of the iterative methods used to compute probabilistic bisimulation for stochastic systems that display nondeterministic behaviors. In this paper, we propose a new technique to partition the state space of a given probabilistic model to its bisimulation classes. This technique uses the PRISM program of a given model and constructs some small versions of the model to train a classifier. It then applies machine learning classification techniques to approximate the related partition. The resulting partition is used as an initial one for the standard bisimulation technique in order to reduce the running time of the method. The experimental results show that the approach can decrease significantly the running time compared to state-of-the-art tools.Note: Simplified Chinese is also known as "Mandarin" or "Standard Chinese". It is the official language of China and is widely spoken in many countries.
</details></li>
</ul>
<hr>
<h2 id="Text-Analysis-Using-Deep-Neural-Networks-in-Digital-Humanities-and-Information-Science"><a href="#Text-Analysis-Using-Deep-Neural-Networks-in-Digital-Humanities-and-Information-Science" class="headerlink" title="Text Analysis Using Deep Neural Networks in Digital Humanities and Information Science"></a>Text Analysis Using Deep Neural Networks in Digital Humanities and Information Science</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16217">http://arxiv.org/abs/2307.16217</a></li>
<li>repo_url: None</li>
<li>paper_authors: Omri Suissa, Avshalom Elmalech, Maayan Zhitomirsky-Geffet</li>
<li>for: This paper aims to explore the challenges of using deep neural networks (DNNs) for analyzing text resources in Digital Humanities (DH) research, and to provide a practical decision model for DH experts to choose the appropriate deep learning approaches for their research.</li>
<li>methods: The paper analyzes multiple use-cases of DH studies in recent literature and their possible solutions, and discusses the challenges of (un)availability of training data and a need for domain adaptation when using DNNs for NLP tasks in DH research.</li>
<li>results: The paper aims to raise awareness of the benefits of utilizing deep learning models in the DH community, and provides a practical decision model for DH experts to choose the appropriate deep learning approaches for their research.<details>
<summary>Abstract</summary>
Combining computational technologies and humanities is an ongoing effort aimed at making resources such as texts, images, audio, video, and other artifacts digitally available, searchable, and analyzable. In recent years, deep neural networks (DNN) dominate the field of automatic text analysis and natural language processing (NLP), in some cases presenting a super-human performance. DNNs are the state-of-the-art machine learning algorithms solving many NLP tasks that are relevant for Digital Humanities (DH) research, such as spell checking, language detection, entity extraction, author detection, question answering, and other tasks. These supervised algorithms learn patterns from a large number of "right" and "wrong" examples and apply them to new examples. However, using DNNs for analyzing the text resources in DH research presents two main challenges: (un)availability of training data and a need for domain adaptation. This paper explores these challenges by analyzing multiple use-cases of DH studies in recent literature and their possible solutions and lays out a practical decision model for DH experts for when and how to choose the appropriate deep learning approaches for their research. Moreover, in this paper, we aim to raise awareness of the benefits of utilizing deep learning models in the DH community.
</details>
<details>
<summary>摘要</summary>
使用计算机技术和人文学科结合，目前正在努力使文本、图像、音频、视频等资源成为数字化可用、搜索可能、分析可能的。在过去几年，深度神经网络（DNN）在自动文本分析和自然语言处理（NLP）领域占据了主导地位，在某些情况下表现出超人般的能力。DNN是目前最好的机器学习算法，用于解决数字人文学科（DH）研究中的许多NLP任务，如拼写检查、语言检测、实体提取、作者检测、问答等任务。这些有监督的算法从大量“正确”和“错误”的示例中学习出模式，并将其应用到新的示例上。然而，在DH研究中使用DNN分析文本资源存在两大挑战：数据库的可用性和领域适应。本文分析了一些DH研究中的多个用例，并探讨了其可能的解决方案，并提出了实用的决策模型，以帮助DH专家在选择适合的深度学习方法时作出决策。此外，本文的目的还是提高使用深度学习模型在DH社区的认识。
</details></li>
</ul>
<hr>
<h2 id="Question-Answering-with-Deep-Neural-Networks-for-Semi-Structured-Heterogeneous-Genealogical-Knowledge-Graphs"><a href="#Question-Answering-with-Deep-Neural-Networks-for-Semi-Structured-Heterogeneous-Genealogical-Knowledge-Graphs" class="headerlink" title="Question Answering with Deep Neural Networks for Semi-Structured Heterogeneous Genealogical Knowledge Graphs"></a>Question Answering with Deep Neural Networks for Semi-Structured Heterogeneous Genealogical Knowledge Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16214">http://arxiv.org/abs/2307.16214</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/omrivm/uncle-bert">https://github.com/omrivm/uncle-bert</a></li>
<li>paper_authors: Omri Suissa, Maayan Zhitomirsky-Geffet, Avshalom Elmalech</li>
<li>for: 这个研究旨在开发一种基于家谱树的问答系统，以便更好地回答家谱相关的问题。</li>
<li>methods: 该研究使用了转换器基于模型，将家谱数据转换成知识图，然后与不结构化文本结合，最后使用一个转换器基于模型进行训练。</li>
<li>results: 研究发现，相比于当前的状态艺术问答模型，专门为家谱问题开发的模型（叔叔BERT）在精度和复杂度之间取得了良好的平衡。此外，该方法可能有实际意义 для家谱研究和实际项目，使家谱数据更加可访问性。<details>
<summary>Abstract</summary>
With the rising popularity of user-generated genealogical family trees, new genealogical information systems have been developed. State-of-the-art natural question answering algorithms use deep neural network (DNN) architecture based on self-attention networks. However, some of these models use sequence-based inputs and are not suitable to work with graph-based structure, while graph-based DNN models rely on high levels of comprehensiveness of knowledge graphs that is nonexistent in the genealogical domain. Moreover, these supervised DNN models require training datasets that are absent in the genealogical domain. This study proposes an end-to-end approach for question answering using genealogical family trees by: 1) representing genealogical data as knowledge graphs, 2) converting them to texts, 3) combining them with unstructured texts, and 4) training a trans-former-based question answering model. To evaluate the need for a dedicated approach, a comparison between the fine-tuned model (Uncle-BERT) trained on the auto-generated genealogical dataset and state-of-the-art question-answering models was per-formed. The findings indicate that there are significant differences between answering genealogical questions and open-domain questions. Moreover, the proposed methodology reduces complexity while increasing accuracy and may have practical implications for genealogical research and real-world projects, making genealogical data accessible to experts as well as the general public.
</details>
<details>
<summary>摘要</summary>
随着用户生成的家谱树的流行，新的家谱信息系统已经被开发出来。现代自然问答算法使用深度神经网络（DNN）架构，基于自注意网络。然而，一些这些模型使用序列化输入，不适合处理图Structured data，而图Structured DNN模型需要高度完整的知识图的存在，在家谱领域是缺失的。此外，这些监督式DNN模型需要家谱领域缺失的训练数据。本研究提出了一种终端方法，通过以下步骤来回答问题：1）将家谱数据转换为知识图，2）将其转换为文本，3）将其与未结构化文本结合，4）使用 transformer 基于模型进行问答。为了评估需要专门的方法，对 fine-tuned 模型（Uncle-BERT）在自动生成的家谱数据上进行了训练，并与当前的问答模型进行了比较。研究发现，回答家谱问题和开放领域问题存在显著差异。此外，提出的方法可以减少复杂性，提高准确性，并有实际意义 для家谱研究和实际项目，让家谱数据对专家和普通公众都可访问。
</details></li>
</ul>
<hr>
<h2 id="Toward-a-Period-Specific-Optimized-Neural-Network-for-OCR-Error-Correction-of-Historical-Hebrew-Texts"><a href="#Toward-a-Period-Specific-Optimized-Neural-Network-for-OCR-Error-Correction-of-Historical-Hebrew-Texts" class="headerlink" title="Toward a Period-Specific Optimized Neural Network for OCR Error Correction of Historical Hebrew Texts"></a>Toward a Period-Specific Optimized Neural Network for OCR Error Correction of Historical Hebrew Texts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16213">http://arxiv.org/abs/2307.16213</a></li>
<li>repo_url: None</li>
<li>paper_authors: Omri Suissa, Maayan Zhitomirsky-Geffet, Avshalom Elmalech</li>
<li>For: The paper is written for the purpose of developing a new method for OCR post-correction in Hebrew using neural networks.* Methods: The paper uses a multi-phase method for generating artificial training datasets with OCR errors and optimizing hyperparameters for building an effective neural network for OCR post-correction in Hebrew.* Results: The paper aims to achieve high accuracy in OCR post-correction for Hebrew documents, despite the challenges posed by the language’s unique features and the lack of sufficient training data.Here is the information in Simplified Chinese text:</li>
<li>for: 本研究旨在开发一种基于神经网络的 Hebrew OCR 后修正方法。</li>
<li>methods: 本文使用多阶段方法生成人工训练集，以优化 OCR 错误和超参数，建立高效的 Hebrew OCR 后修正神经网络。</li>
<li>results: 本研究目标是实现高精度的 Hebrew OCR 后修正，突破语言特殊特征和训练数据不足的障碍。<details>
<summary>Abstract</summary>
Over the past few decades, large archives of paper-based historical documents, such as books and newspapers, have been digitized using the Optical Character Recognition (OCR) technology. Unfortunately, this broadly used technology is error-prone, especially when an OCRed document was written hundreds of years ago. Neural networks have shown great success in solving various text processing tasks, including OCR post-correction. The main disadvantage of using neural networks for historical corpora is the lack of sufficiently large training datasets they require to learn from, especially for morphologically-rich languages like Hebrew. Moreover, it is not clear what are the optimal structure and values of hyperparameters (predefined parameters) of neural networks for OCR error correction in Hebrew due to its unique features. Furthermore, languages change across genres and periods. These changes may affect the accuracy of OCR post-correction neural network models. To overcome these challenges, we developed a new multi-phase method for generating artificial training datasets with OCR errors and hyperparameters optimization for building an effective neural network for OCR post-correction in Hebrew.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "Optical Character Recognition" is translated as "文本识别" (wén tiān shí bǐ)* "Hebrew" is translated as "希伯来语" (xī bā lǐ yǔ)* "hyperparameters" is translated as "超参数" (chāo jiān xiàng)* "OCR post-correction" is translated as "OCR 后修正" (OCR hòu jiǔ zhèng)* "morphologically-rich languages" is translated as "富有形态语言" (fù yǒu xíng tài yǔ yán)* "genres and periods" is translated as "类型和时期" (lèi xìng yǔ shí qī)Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China.
</details></li>
</ul>
<hr>
<h2 id="Robust-Multi-Agent-Reinforcement-Learning-with-State-Uncertainty"><a href="#Robust-Multi-Agent-Reinforcement-Learning-with-State-Uncertainty" class="headerlink" title="Robust Multi-Agent Reinforcement Learning with State Uncertainty"></a>Robust Multi-Agent Reinforcement Learning with State Uncertainty</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16212">http://arxiv.org/abs/2307.16212</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sihongho/robust_marl_with_state_uncertainty">https://github.com/sihongho/robust_marl_with_state_uncertainty</a></li>
<li>paper_authors: Sihong He, Songyang Han, Sanbao Su, Shuo Han, Shaofeng Zou, Fei Miao</li>
<li>for: 研究多智能体强化学习（MARL）中存在不确定状态的问题，以提高其在实际应用中的稳定性。</li>
<li>methods: 提出了一种基于Markov游戏的状态干扰对手（MG-SPA）模型，并提出了一种robust平衡（RE）作为解题方法。采用了一种robust多智能体Q学习（RMAQ）算法和一种robust多智能体actor-critic（RMAAC）算法来实现RE。</li>
<li>results: 通过实验表明，RMAQ算法可以寻求optimal值函数，而RMAAC算法在多个多智能体环境中比较多个MARL和robust MARL方法表现更好，特别是在存在状态不确定性时。<details>
<summary>Abstract</summary>
In real-world multi-agent reinforcement learning (MARL) applications, agents may not have perfect state information (e.g., due to inaccurate measurement or malicious attacks), which challenges the robustness of agents' policies. Though robustness is getting important in MARL deployment, little prior work has studied state uncertainties in MARL, neither in problem formulation nor algorithm design. Motivated by this robustness issue and the lack of corresponding studies, we study the problem of MARL with state uncertainty in this work. We provide the first attempt to the theoretical and empirical analysis of this challenging problem. We first model the problem as a Markov Game with state perturbation adversaries (MG-SPA) by introducing a set of state perturbation adversaries into a Markov Game. We then introduce robust equilibrium (RE) as the solution concept of an MG-SPA. We conduct a fundamental analysis regarding MG-SPA such as giving conditions under which such a robust equilibrium exists. Then we propose a robust multi-agent Q-learning (RMAQ) algorithm to find such an equilibrium, with convergence guarantees. To handle high-dimensional state-action space, we design a robust multi-agent actor-critic (RMAAC) algorithm based on an analytical expression of the policy gradient derived in the paper. Our experiments show that the proposed RMAQ algorithm converges to the optimal value function; our RMAAC algorithm outperforms several MARL and robust MARL methods in multiple multi-agent environments when state uncertainty is present. The source code is public on \url{https://github.com/sihongho/robust_marl_with_state_uncertainty}.
</details>
<details>
<summary>摘要</summary>
在实际多智能体学习（MARL）应用中，智能体可能没有完美状态信息（例如由于不准确测量或恶意攻击），这会对智能体策略的稳定性造成挑战。虽然稳定性在MARL部署中变得越来越重要，但是前一个研究少了关于状态不确定性的问题， neither in problem formulation nor algorithm design. 为了解决这个稳定性问题和相关的研究不足，我们在这个工作中研究了MARL中的状态不确定性问题。我们首先将问题模型为一个Markov Game with state perturbation adversaries（MG-SPA），并将 robust equilibrium（RE）作为该问题的解题概念。我们进行了基本的分析，包括确定MG-SPA下的robust equilibrium是否存在的条件。然后，我们提出了一种robust multi-agent Q-learning（RMAQ）算法，用于找到该equilibrium，并提供了确定性保证。为了处理高维状态动作空间，我们设计了一种基于分析表达的多智能体actor-critic（RMAAC）算法。我们的实验表明，我们的RMAQ算法可以 converge to the optimal value function，而RMAAC算法在多个多智能体环境中出perform several MARL and robust MARL方法。代码可以在 \url{https://github.com/sihongho/robust_marl_with_state_uncertainty} 上找到。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Uncertainly-Missing-and-Ambiguous-Visual-Modality-in-Multi-Modal-Entity-Alignment"><a href="#Rethinking-Uncertainly-Missing-and-Ambiguous-Visual-Modality-in-Multi-Modal-Entity-Alignment" class="headerlink" title="Rethinking Uncertainly Missing and Ambiguous Visual Modality in Multi-Modal Entity Alignment"></a>Rethinking Uncertainly Missing and Ambiguous Visual Modality in Multi-Modal Entity Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16210">http://arxiv.org/abs/2307.16210</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zjukg/UMAEA">https://github.com/zjukg/UMAEA</a></li>
<li>paper_authors: Zhuo Chen, Lingbing Guo, Yin Fang, Yichi Zhang, Jiaoyan Chen, Jeff Z. Pan, Yangning Li, Huajun Chen, Wen Zhang</li>
<li>for: 本研究旨在解决多Modal Entity Alignment (MMEA)中存在的感知模式缺失和内在ambiguity问题，以提高MMEA的稳定性和可靠性。</li>
<li>methods: 本研究使用了最新的MMEA模型，并在我们提出的MMEA-UMVM数据集上进行了benchmarking，以评估模型的性能。我们还提出了一种新的Robust Multi-Modal Entity Alignment (UMAEA)方法，该方法能够有效地处理不确定的感知模式缺失和内在ambiguity问题。</li>
<li>results: 我们的研究表明，在面临感知模式缺失和内在ambiguity的情况下，现有的MMEA模型容易过拟合感知模式噪音，导致性能下降或很快衰退。而我们的UMAEA方法能够稳定地处理这些问题，在97个benchmark分区中都达到了最高水平，Significantly surpassing现有的基eline模型。<details>
<summary>Abstract</summary>
As a crucial extension of entity alignment (EA), multi-modal entity alignment (MMEA) aims to identify identical entities across disparate knowledge graphs (KGs) by exploiting associated visual information. However, existing MMEA approaches primarily concentrate on the fusion paradigm of multi-modal entity features, while neglecting the challenges presented by the pervasive phenomenon of missing and intrinsic ambiguity of visual images. In this paper, we present a further analysis of visual modality incompleteness, benchmarking latest MMEA models on our proposed dataset MMEA-UMVM, where the types of alignment KGs covering bilingual and monolingual, with standard (non-iterative) and iterative training paradigms to evaluate the model performance. Our research indicates that, in the face of modality incompleteness, models succumb to overfitting the modality noise, and exhibit performance oscillations or declines at high rates of missing modality. This proves that the inclusion of additional multi-modal data can sometimes adversely affect EA. To address these challenges, we introduce UMAEA , a robust multi-modal entity alignment approach designed to tackle uncertainly missing and ambiguous visual modalities. It consistently achieves SOTA performance across all 97 benchmark splits, significantly surpassing existing baselines with limited parameters and time consumption, while effectively alleviating the identified limitations of other models. Our code and benchmark data are available at https://github.com/zjukg/UMAEA.
</details>
<details>
<summary>摘要</summary>
如果你想要学习多modalentity alignment（MMEA），这篇文章可能会帮助你。MMEA是一种扩展entity alignment（EA）的技术，用于在不同知识图（KG）之间标识相同的实体，通过利用相关的视觉信息。然而，现有的MMEA方法主要集中在多modalentity特征的 fusions paradigma，而忽视了视觉图像中的普遍现象：缺失和内在的模糊性。在这篇文章中，我们进行了更进一步的视觉模态不完整性的分析，并使用我们提出的MMEA-UMVM数据集进行了最新的MMEA模型的benchmarking。我们的研究表明，在面临视觉模态不完整性时，模型容易过拟合modal noise，并且在高比例的缺失modal时会出现性能波动或下降。这表明，在 inclusion of additional multi-modal data可以有时对EA造成负面影响。为了解决这些挑战，我们提出了UMAEA，一种robust的多modalentity alignment方法，可以有效地处理不确定、缺失和模糊的视觉模态。它在所有97个benchmark split中均 achievement SOTA表现，明显超过了现有的基eline，同时具有限制parameters和时间消耗的优势。我们的代码和benchmark数据可以在https://github.com/zjukg/UMAEA中找到。
</details></li>
</ul>
<hr>
<h2 id="Around-the-GLOBE-Numerical-Aggregation-Question-Answering-on-Heterogeneous-Genealogical-Knowledge-Graphs-with-Deep-Neural-Networks"><a href="#Around-the-GLOBE-Numerical-Aggregation-Question-Answering-on-Heterogeneous-Genealogical-Knowledge-Graphs-with-Deep-Neural-Networks" class="headerlink" title="Around the GLOBE: Numerical Aggregation Question-Answering on Heterogeneous Genealogical Knowledge Graphs with Deep Neural Networks"></a>Around the GLOBE: Numerical Aggregation Question-Answering on Heterogeneous Genealogical Knowledge Graphs with Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16208">http://arxiv.org/abs/2307.16208</a></li>
<li>repo_url: None</li>
<li>paper_authors: Omri Suissa, Maayan Zhitomirsky-Geffet, Avshalom Elmalech</li>
<li>for: 这个研究旨在提供一种数字化文化遗产领域中的 numeral 聚合Question Answering（QA）方法，以便帮助研究者（以及一般公众）对大量文献进行 distant 读取和分析。</li>
<li>methods: 该研究使用了一种基于 transformer 的 end-to-end 方法，包括自动生成训练数据集、transformer 基于表格选择方法、优化的 transformer 基于 numeral 聚合 QA 模型。</li>
<li>results: 研究发现，提出的方法（GLOBE）在这个任务上的准确率为 87%，比现有的模型和管道高出 66%。这种方法可能有实际应用在 genealogical 信息中心和博物馆，使 genealogical 数据研究变得容易和可扩展。<details>
<summary>Abstract</summary>
One of the key AI tools for textual corpora exploration is natural language question-answering (QA). Unlike keyword-based search engines, QA algorithms receive and process natural language questions and produce precise answers to these questions, rather than long lists of documents that need to be manually scanned by the users. State-of-the-art QA algorithms based on DNNs were successfully employed in various domains. However, QA in the genealogical domain is still underexplored, while researchers in this field (and other fields in humanities and social sciences) can highly benefit from the ability to ask questions in natural language, receive concrete answers and gain insights hidden within large corpora. While some research has been recently conducted for factual QA in the genealogical domain, to the best of our knowledge, there is no previous research on the more challenging task of numerical aggregation QA (i.e., answering questions combining aggregation functions, e.g., count, average, max). Numerical aggregation QA is critical for distant reading and analysis for researchers (and the general public) interested in investigating cultural heritage domains. Therefore, in this study, we present a new end-to-end methodology for numerical aggregation QA for genealogical trees that includes: 1) an automatic method for training dataset generation; 2) a transformer-based table selection method, and 3) an optimized transformer-based numerical aggregation QA model. The findings indicate that the proposed architecture, GLOBE, outperforms the state-of-the-art models and pipelines by achieving 87% accuracy for this task compared to only 21% by current state-of-the-art models. This study may have practical implications for genealogical information centers and museums, making genealogical data research easy and scalable for experts as well as the general public.
</details>
<details>
<summary>摘要</summary>
一种关键的人工智能工具 для文本 corpus 探索是自然语言问答 (QA)。不同于关键词搜索引擎，QA 算法会根据自然语言问题提供和处理精确的答案，而不是需要用户手动扫描大量文档。现状的QA算法基于深度学习神经网络 (DNN) 在不同领域得到了成功应用。然而，在家谱领域，QA仍然受到了不 enough 的研究，而家谱领域的研究人员 (以及人文社科领域的其他研究人员) 可以很大程度上受益于可以使用自然语言问题提出问题，并获得文档中隐藏的智能。虽然有些最近的研究已经对家谱领域的事实 QA 进行了研究，但到目前为止，没有任何一项研究关于更加复杂的数字积算 QA (即回答组合函数，例如计数、平均值、最大值)。数字积算 QA 对于远程阅读和分析是非常重要的，因此在这种情况下，我们提出了一种新的综合方法。方法包括：1. 自动生成训练数据集方法2. 基于 transformer 的表格选择方法3. 优化 transformer 基于数字积算 QA 模型结果表明，我们提出的建筑 GLOBE 模型在这种任务上的准确率为 87%，比现状最佳模型和管道的准确率（21%）高出了大幅度。这种研究可能对家谱信息中心和博物馆产生实质性的实用效果，使家谱数据研究变得容易且可扩展。
</details></li>
</ul>
<hr>
<h2 id="Deep-Convolutional-Neural-Networks-with-Zero-Padding-Feature-Extraction-and-Learning"><a href="#Deep-Convolutional-Neural-Networks-with-Zero-Padding-Feature-Extraction-and-Learning" class="headerlink" title="Deep Convolutional Neural Networks with Zero-Padding: Feature Extraction and Learning"></a>Deep Convolutional Neural Networks with Zero-Padding: Feature Extraction and Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16203">http://arxiv.org/abs/2307.16203</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liubc17/eDCNN_zero_padding">https://github.com/liubc17/eDCNN_zero_padding</a></li>
<li>paper_authors: Zhi Han, Baichen Liu, Shao-Bo Lin, Ding-Xuan Zhou</li>
<li>for: 这 paper 研究了深度卷积神经网络 (DCNNs) 中 zero-padding 的表现。</li>
<li>methods: 这 paper 使用了 zero-padding 在特征提取和学习中，并验证了 zero-padding 的翻译相对性和抽象层 pooling 的翻译不变性。</li>
<li>results: 这 paper 显示了任何深度全连接神经网络 (DFCNs) 都可以通过 DCNNs  WITH zero-padding 来表示，这表明 DCNNs  WITH zero-padding 在特征提取方面比 DFCNs 更好。此外， paper 还证明了 DCNNs  WITH zero-padding 的通用一致性和学习过程中的翻译不变性。<details>
<summary>Abstract</summary>
This paper studies the performance of deep convolutional neural networks (DCNNs) with zero-padding in feature extraction and learning. After verifying the roles of zero-padding in enabling translation-equivalence, and pooling in its translation-invariance driven nature, we show that with similar number of free parameters, any deep fully connected networks (DFCNs) can be represented by DCNNs with zero-padding. This demonstrates that DCNNs with zero-padding is essentially better than DFCNs in feature extraction. Consequently, we derive universal consistency of DCNNs with zero-padding and show its translation-invariance in the learning process. All our theoretical results are verified by numerical experiments including both toy simulations and real-data running.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Shuffled-Differentially-Private-Federated-Learning-for-Time-Series-Data-Analytics"><a href="#Shuffled-Differentially-Private-Federated-Learning-for-Time-Series-Data-Analytics" class="headerlink" title="Shuffled Differentially Private Federated Learning for Time Series Data Analytics"></a>Shuffled Differentially Private Federated Learning for Time Series Data Analytics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16196">http://arxiv.org/abs/2307.16196</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenxi Huang, Chaoyang Jiang, Zhenghua Chen</li>
<li>for: 这个研究旨在提供一个隐私保证的联合学习算法，以便在时间序列数据上进行优化性的表现，同时保证客户端的隐私。</li>
<li>methods: 本研究使用了本地差异隐私和排序技术，以实现隐私增强和隐私扩大。具体来说，本研究使用了本地差异隐私来扩展隐私保证的信任范围到客户端，并使用排序技术来实现隐私增强。</li>
<li>results: 实验结果显示，在小客户和大客户情况下，本研究的方法仅对应不同程度的精度损失，而且在同等隐私保证水平下，本研究的方法比中央差异隐私联合学习方法表现更好。<details>
<summary>Abstract</summary>
Trustworthy federated learning aims to achieve optimal performance while ensuring clients' privacy. Existing privacy-preserving federated learning approaches are mostly tailored for image data, lacking applications for time series data, which have many important applications, like machine health monitoring, human activity recognition, etc. Furthermore, protective noising on a time series data analytics model can significantly interfere with temporal-dependent learning, leading to a greater decline in accuracy. To address these issues, we develop a privacy-preserving federated learning algorithm for time series data. Specifically, we employ local differential privacy to extend the privacy protection trust boundary to the clients. We also incorporate shuffle techniques to achieve a privacy amplification, mitigating the accuracy decline caused by leveraging local differential privacy. Extensive experiments were conducted on five time series datasets. The evaluation results reveal that our algorithm experienced minimal accuracy loss compared to non-private federated learning in both small and large client scenarios. Under the same level of privacy protection, our algorithm demonstrated improved accuracy compared to the centralized differentially private federated learning in both scenarios.
</details>
<details>
<summary>摘要</summary>
信任worthy的联合学习目标是实现最佳性能，同时保障客户端的隐私。现有的隐私保护联合学习方法大多是针对图像数据，缺乏应用于时间序列数据，这种数据在机器健康监测、人类活动识别等领域有重要应用。此外，在时间序列数据分析模型上加入保护噪声可能会对时间相互dependent的学习产生干扰，导致准确率下降。为解决这些问题，我们开发了一种针对时间序列数据的隐私保护联合学习算法。具体来说，我们使用本地差分隐私来扩展隐私保护信任圈到客户端。我们还 incorporate 混淆技术来实现隐私增强， Mitigating the accuracy decline caused by leveraging local differential privacy。我们对五个时间序列数据集进行了广泛的实验。结果表明，我们的算法在小客户和大客户场景中都体现出较少的准确率下降，与非隐私联合学习相比。在同一个隐私保护水平下，我们的算法在两个场景中表现出比中央差分隐私联合学习更好的准确率表现。
</details></li>
</ul>
<hr>
<h2 id="An-Efficient-Approach-to-Mitigate-Numerical-Instability-in-Backpropagation-for-16-bit-Neural-Network-Training"><a href="#An-Efficient-Approach-to-Mitigate-Numerical-Instability-in-Backpropagation-for-16-bit-Neural-Network-Training" class="headerlink" title="An Efficient Approach to Mitigate Numerical Instability in Backpropagation for 16-bit Neural Network Training"></a>An Efficient Approach to Mitigate Numerical Instability in Backpropagation for 16-bit Neural Network Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16189">http://arxiv.org/abs/2307.16189</a></li>
<li>repo_url: None</li>
<li>paper_authors: Juyoung Yun</li>
<li>for: 解决深度神经网络训练阶段的数值不稳定性问题，尤其是在使用流行的优化算法RMSProp和Adam时。</li>
<li>methods: 通过深入探究epsillon参数在这些优化算法中的作用，发现epsillon的值可以影响数值稳定性，并提出了一种新的约束方法来缓解这些问题。</li>
<li>results: 该方法可以有效约束数值不稳定性问题，使得16位计算中的深度神经网络训练得到改进，并开启了更有效和稳定的模型训练新途径。<details>
<summary>Abstract</summary>
In this research, we delve into the intricacies of the numerical instability observed in 16-bit computations of machine learning models, particularly when employing popular optimization algorithms such as RMSProp and Adam. This instability is commonly experienced during the training phase of deep neural networks, leading to disrupted learning processes and hindering the effective deployment of such models. We identify the single hyperparameter, epsilon, as the main culprit behind this numerical instability. An in-depth exploration of the role of epsilon in these optimizers within 16-bit computations reveals that a minor adjustment of its value can restore the functionality of RMSProp and Adam, consequently enabling the effective utilization of 16-bit neural networks. We propose a novel method to mitigate the identified numerical instability issues. This method capitalizes on the updates from the Adam optimizer and significantly improves the robustness of the learning process in 16-bit computations. This study contributes to better understanding of optimization in low-precision computations and provides an effective solution to a longstanding issue in training deep neural networks, opening new avenues for more efficient and stable model training.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="ESP-Exploiting-Symmetry-Prior-for-Multi-Agent-Reinforcement-Learning"><a href="#ESP-Exploiting-Symmetry-Prior-for-Multi-Agent-Reinforcement-Learning" class="headerlink" title="ESP: Exploiting Symmetry Prior for Multi-Agent Reinforcement Learning"></a>ESP: Exploiting Symmetry Prior for Multi-Agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16186">http://arxiv.org/abs/2307.16186</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xin Yu, Rongye Shi, Pu Feng, Yongkai Tian, Jie Luo, Wenjun Wu</li>
<li>for: 本文提出了一种基于多智能探索学习（MARL）的框架，用于利用先验知识来提高数据效率。</li>
<li>methods: 本文提出了一种将数据增强和一种有judicious设计的一致损失函数integrated into current MARL方法中。</li>
<li>results: 实验结果表明，提出的方法可以在多种复杂任务上提高数据效率，并且在物理多机器人测试环境中显示出优势。<details>
<summary>Abstract</summary>
Multi-agent reinforcement learning (MARL) has achieved promising results in recent years. However, most existing reinforcement learning methods require a large amount of data for model training. In addition, data-efficient reinforcement learning requires the construction of strong inductive biases, which are ignored in the current MARL approaches. Inspired by the symmetry phenomenon in multi-agent systems, this paper proposes a framework for exploiting prior knowledge by integrating data augmentation and a well-designed consistency loss into the existing MARL methods. In addition, the proposed framework is model-agnostic and can be applied to most of the current MARL algorithms. Experimental tests on multiple challenging tasks demonstrate the effectiveness of the proposed framework. Moreover, the proposed framework is applied to a physical multi-robot testbed to show its superiority.
</details>
<details>
<summary>摘要</summary>
多智能 reinforcement learning (MARL) 在过去几年取得了有前途的成果。然而，现有的 reinforcement learning 方法通常需要大量数据来训练模型。此外，数据效率 reinforcement learning 需要建立强大的推理假设，这些假设在当前的 MARL 方法中被忽略。 inspirited by the symmetry phenomenon in multi-agent systems, this paper proposes a framework for exploiting prior knowledge by integrating data augmentation and a well-designed consistency loss into the existing MARL methods. In addition, the proposed framework is model-agnostic and can be applied to most of the current MARL algorithms. Experimental tests on multiple challenging tasks demonstrate the effectiveness of the proposed framework. Moreover, the proposed framework is applied to a physical multi-robot testbed to show its superiority.Note: The translation is written in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Unified-Model-for-Image-Video-Audio-and-Language-Tasks"><a href="#Unified-Model-for-Image-Video-Audio-and-Language-Tasks" class="headerlink" title="Unified Model for Image, Video, Audio and Language Tasks"></a>Unified Model for Image, Video, Audio and Language Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16184">http://arxiv.org/abs/2307.16184</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mshukor/unival">https://github.com/mshukor/unival</a></li>
<li>paper_authors: Mustafa Shukor, Corentin Dancette, Alexandre Rame, Matthieu Cord</li>
<li>for: 这个论文的目标是建立一个可以支持多种模式和任务的大型语言模型（LLM），以实现通用Agent的梦想。</li>
<li>methods: 作者提出了一种基于任务均衡和多模式课程学习的方法，称为UnIVAL，可以有效地支持多种模式和任务。</li>
<li>results: 对于图像和视频文本任务，UnIVAL模型表现竞争力强，而且可以通过对audio文本任务进行训练而达到竞争力水平，即使没有直接使用audio模式进行训练。此外，作者还提出了一种多模式模型融合方法，通过将不同多模式任务的模型权重进行插值来实现。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have made the ambitious quest for generalist agents significantly far from being a fantasy. A key hurdle for building such general models is the diversity and heterogeneity of tasks and modalities. A promising solution is unification, allowing the support of a myriad of tasks and modalities within one unified framework. While few large models (e.g., Flamingo (Alayrac et al., 2022), trained on massive datasets, can support more than two modalities, current small to mid-scale unified models are still limited to 2 modalities, usually image-text or video-text. The question that we ask is: is it possible to build efficiently a unified model that can support all modalities? To answer this, we propose UnIVAL, a step further towards this ambitious goal. Without relying on fancy datasets sizes or models with billions of parameters, the ~ 0.25B parameter UnIVAL model goes beyond two modalities and unifies text, images, video, and audio into a single model. Our model is efficiently pretrained on many tasks, based on task balancing and multimodal curriculum learning. UnIVAL shows competitive performance to existing state-of-the-art approaches, across image and video-text tasks. The feature representations learned from image and video-text modalities, allows the model to achieve competitive performance when finetuned on audio-text tasks, despite not being pretrained on audio. Thanks to the unified model, we propose a novel study on multimodal model merging via weight interpolation of models trained on different multimodal tasks, showing their benefits in particular for out-of-distribution generalization. Finally, we motivate unification by showing the synergy between tasks. The model weights and code are released here: https://github.com/mshukor/UnIVAL.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLMs）已经让普通的通用代理人成为了现实。一个关键的障碍是任务和模式的多样性和不同性。一个有 Promise的解决方案是统一，允许支持多种任务和模式的内部框架。虽然有些大型模型（如Flamingo（Alayrac等，2022））可以支持更多 чем两种模式，当前小规模的统一模型仍然受限于2种模式，通常是图像文本或视频文本。我们提出的问题是：是否可以有效地建立一个统一模型，可以支持所有的模式？为了回答这个问题，我们提出了UnIVAL，是一个更进一步的目标。不需要庞大的数据集或多亿参数的模型，UnIVAL模型的约0.25B参数可以超越两种模式，并将文本、图像、视频和音频 integrate到一个模型中。我们的模型通过多任务的补做和多模式的学习环境来高效预训练。UnIVAL在图像和视频文本任务上显示出与现状最佳的性能，而且通过在音频文本任务上训练模型，即使模型没有直接预训练音频，也可以达到竞争性表现。此外，我们还提出了一种新的研究方法，即通过多模式模型的权重插值来合并不同的多模式任务，并证明其在特定的异常情况下的优势。最后，我们驱动了统一的理念，证明任务之间的共同性。模型参数和代码在GitHub上发布：https://github.com/mshukor/UnIVAL。
</details></li>
</ul>
<hr>
<h2 id="Redundancy-aware-unsupervised-rankings-for-collections-of-gene-sets"><a href="#Redundancy-aware-unsupervised-rankings-for-collections-of-gene-sets" class="headerlink" title="Redundancy-aware unsupervised rankings for collections of gene sets"></a>Redundancy-aware unsupervised rankings for collections of gene sets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16182">http://arxiv.org/abs/2307.16182</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chiara Balestra, Carlo Maj, Emmanuel Müller, Andreas Mayr</li>
<li>for: 本研究的目的是增加生物信息学中的集合（gene set）的可解释性和可读性，通过将重要性分数用于集合中的路径来排序和简化集合。</li>
<li>methods: 本研究使用了Shapley值来计算重要性分数，并提出了一种缓解常见的幂数复杂性问题的技巧。此外，本研究还包括了一种 redundancy 意识的排序方法，以便在获得的排序中包括重复的集合。</li>
<li>results: 本研究的结果表明，使用重要性分数可以减少集合中的维度，同时保持高度的覆盖率。此外，本研究还发现，通过包括重复的集合在内，可以提高Gene Sets Enrichment Analysis的有用性。<details>
<summary>Abstract</summary>
The biological roles of gene sets are used to group them into collections. These collections are often characterized by being high-dimensional, overlapping, and redundant families of sets, thus precluding a straightforward interpretation and study of their content. Bioinformatics looked for solutions to reduce their dimension or increase their intepretability. One possibility lies in aggregating overlapping gene sets to create larger pathways, but the modified biological pathways are hardly biologically justifiable. We propose to use importance scores to rank the pathways in the collections studying the context from a set covering perspective. The proposed Shapley values-based scores consider the distribution of the singletons and the size of the sets in the families; Furthermore, a trick allows us to circumvent the usual exponential complexity of Shapley values' computation. Finally, we address the challenge of including a redundancy awareness in the obtained rankings where, in our case, sets are redundant if they show prominent intersections.   The rankings can be used to reduce the dimension of collections of gene sets, such that they show lower redundancy and still a high coverage of the genes. We further investigate the impact of our selection on Gene Sets Enrichment Analysis. The proposed method shows a practical utility in bioinformatics to increase the interpretability of the collections of gene sets and a step forward to include redundancy into Shapley values computations.
</details>
<details>
<summary>摘要</summary>
生物学角色集合用于将集合分组。这些集合经常是高维ensional，重叠，并且异常的家族集合，从而阻碍直接解释和研究其内容。生物信息学寻找解决方案来减少其维度或增加其可读性。一种可能性在于将重叠的基因集合聚合成更大的路径，但修改的生物路径几乎不可能被生物学上正确地证明。我们提议使用重要性分数来排序路径集合，研究从集合覆盖角度来看。我们的提案基于值得推荐的值来计算分数，考虑集合内单个基因的分布和集合大小。此外，我们还使用一种技巧来绕过通常的指数复杂性计算值得推荐的值。最后，我们解决了包含重复性意识的获得的排名中的挑战，在我们的情况下，集合是重复的，如果它们显著交叉。这些排名可以用来减少集合基因集的维度，以便它们仍然具有较高的覆盖率。我们进一步调查了我们的选择对基因集sets强化分析的影响。我们的方法显示了生物信息学中实用的一步，以增加集合基因集的可读性，并且是包含重复性的Shapley值计算的一步进行。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-learning-of-density-ratios-in-RKHS"><a href="#Adaptive-learning-of-density-ratios-in-RKHS" class="headerlink" title="Adaptive learning of density ratios in RKHS"></a>Adaptive learning of density ratios in RKHS</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16164">http://arxiv.org/abs/2307.16164</a></li>
<li>repo_url: None</li>
<li>paper_authors: Werner Zellinger, Stefan Kindermann, Sergei V. Pereverzyev</li>
<li>for: 这篇论文关注于估计两个概率密度之间的比率，从数据分布中启发的机器学习和统计问题。</li>
<li>methods: 本文研究了一种使用正则化布雷格曼异常的拟合器希尔伯特空间（RKHS）中的概率密度比率估计方法，并提出了新的finite-sample error bounds。</li>
<li>results: 本文的方法可以在特定情况下实现最优的最小化error rate，并且可以自动选择最佳参数。<details>
<summary>Abstract</summary>
Estimating the ratio of two probability densities from finitely many observations of the densities is a central problem in machine learning and statistics with applications in two-sample testing, divergence estimation, generative modeling, covariate shift adaptation, conditional density estimation, and novelty detection. In this work, we analyze a large class of density ratio estimation methods that minimize a regularized Bregman divergence between the true density ratio and a model in a reproducing kernel Hilbert space (RKHS). We derive new finite-sample error bounds, and we propose a Lepskii type parameter choice principle that minimizes the bounds without knowledge of the regularity of the density ratio. In the special case of quadratic loss, our method adaptively achieves a minimax optimal error rate. A numerical illustration is provided.
</details>
<details>
<summary>摘要</summary>
计算两个概率密度的比率从有限多个观察值中是机器学习和统计中的中心问题，具有应用于两样本测试、差异估计、生成模型、covariate shift适应、Conditional density估计和新事物探测等领域。在这种工作中，我们分析了一大类density比率估计方法，这些方法在一个 reproduce kernel Hilbert space（RKHS）中减少了一个规范化Bregman divergence的值。我们提出了新的finite-sample error bounds，并提出了一种Lepskii类型参数选择原则，该原则可以在不知道概率密度比率的正规性情况下最小化 bounds。在特定的quadratic loss情况下，我们的方法可以自适应实现一个最优的error rate。一个数字图文示例。
</details></li>
</ul>
<hr>
<h2 id="Variance-Control-for-Distributional-Reinforcement-Learning"><a href="#Variance-Control-for-Distributional-Reinforcement-Learning" class="headerlink" title="Variance Control for Distributional Reinforcement Learning"></a>Variance Control for Distributional Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16152">http://arxiv.org/abs/2307.16152</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kuangqi927/qem">https://github.com/kuangqi927/qem</a></li>
<li>paper_authors: Qi Kuang, Zhoufan Zhu, Liwen Zhang, Fan Zhou</li>
<li>for: 这个论文主要研究了分布式强化学习（DRL）中获得的Q函数估计器的有效性。</li>
<li>methods: 作者通过分析错误的方法来了解Q函数估计器在分布式 Setting中的误差，并构建了一种新的估计器——量化扩展均值（QEM）。</li>
<li>results: 作者在多个Atari和Mujoco标准任务上广泛评估了他们的QEMRL算法，并证明了它在样本效率和吞吐量性方面达到了 significiant改进。<details>
<summary>Abstract</summary>
Although distributional reinforcement learning (DRL) has been widely examined in the past few years, very few studies investigate the validity of the obtained Q-function estimator in the distributional setting. To fully understand how the approximation errors of the Q-function affect the whole training process, we do some error analysis and theoretically show how to reduce both the bias and the variance of the error terms. With this new understanding, we construct a new estimator \emph{Quantiled Expansion Mean} (QEM) and introduce a new DRL algorithm (QEMRL) from the statistical perspective. We extensively evaluate our QEMRL algorithm on a variety of Atari and Mujoco benchmark tasks and demonstrate that QEMRL achieves significant improvement over baseline algorithms in terms of sample efficiency and convergence performance.
</details>
<details>
<summary>摘要</summary>
尽管分布式强化学习（DRL）在过去几年中得到了广泛的研究，但是非常少的研究检查了在分布上得到的Q函数估计器的有效性。为了全面理解Q函数估计器的抽象错误对整个训练过程的影响，我们进行了错误分析并从统计角度提出了一种新的估计器——量词扩展含义（QEM），以及一种基于统计学的新DRL算法（QEMRL）。我们对多种Atari和Mujoco benchmark任务进行了广泛的评估，并证明了QEMRL在样本效率和收敛性方面具有显著改善。
</details></li>
</ul>
<hr>
<h2 id="An-Effective-LSTM-DDPM-Scheme-for-Energy-Theft-Detection-and-Forecasting-in-Smart-Grid"><a href="#An-Effective-LSTM-DDPM-Scheme-for-Energy-Theft-Detection-and-Forecasting-in-Smart-Grid" class="headerlink" title="An Effective LSTM-DDPM Scheme for Energy Theft Detection and Forecasting in Smart Grid"></a>An Effective LSTM-DDPM Scheme for Energy Theft Detection and Forecasting in Smart Grid</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16149">http://arxiv.org/abs/2307.16149</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xun Yuan, Yang Yang, Arwa Alromih, Prosanta Gope, Biplab Sikdar</li>
<li>for: 本研究目的是解决智能电网系统中的能源偷窃检测和能源消耗预测两大挑战。</li>
<li>methods: 该研究提出的解决方案 combining long short-term memory（LSTM）和杂噪扩散概率模型（DDPM），通过生成输入重建和预测来实现输入重建和预测错误。</li>
<li>results: 经过广泛的实验，该研究的方案在智能电网系统中的能源偷窃检测和能源消耗预测问题中表现出色，与基eline方法相比，该方案可以更好地检测能源偷窃攻击。<details>
<summary>Abstract</summary>
Energy theft detection (ETD) and energy consumption forecasting (ECF) are two interconnected challenges in smart grid systems. Addressing these issues collectively is crucial for ensuring system security. This paper addresses the interconnected challenges of ETD and ECF in smart grid systems. The proposed solution combines long short-term memory (LSTM) and a denoising diffusion probabilistic model (DDPM) to generate input reconstruction and forecasting. By leveraging the reconstruction and forecasting errors, the system identifies instances of energy theft, with the methods based on reconstruction error and forecasting error complementing each other in detecting different types of attacks. Through extensive experiments on real-world and synthetic datasets, the proposed scheme outperforms baseline methods in ETD and ECF problems. The ensemble method significantly enhances ETD performance, accurately detecting energy theft attacks that baseline methods fail to detect. The research offers a comprehensive and effective solution for addressing ETD and ECF challenges, demonstrating promising results and improved security in smart grid systems.
</details>
<details>
<summary>摘要</summary>
智能电网系统中的能源盗取检测（ETD）和能源消耗预测（ECF）是两个相互连接的挑战。它们的解决需要共同进行，以确保系统安全。这篇论文解决了智能电网系统中的ETD和ECF两个挑战。提议的解决方案结合了长期短期记忆（LSTM）和杂散抽取概率模型（DDPM），生成输入重建和预测。通过利用重建和预测错误，系统可以识别能源盗取行为，基于重建错误和预测错误，不同类型的攻击可以得到优秀的识别。经过了实验，提议的方案在实际和synthetic数据集上出现了显著的改进，与基线方法相比，ETD和ECF问题中的性能显著提高。ensemble方法在ETD问题中显示出了杰出的表现，可以准确地检测基eline方法无法检测的能源盗取攻击。这项研究提供了智能电网系统中ETD和ECF问题的全面和有效的解决方案，实验结果表明，该方案在安全性方面具有明显的改进。
</details></li>
</ul>
<hr>
<h2 id="Pupil-Learning-Mechanism"><a href="#Pupil-Learning-Mechanism" class="headerlink" title="Pupil Learning Mechanism"></a>Pupil Learning Mechanism</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16141">http://arxiv.org/abs/2307.16141</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rua-Huan Tsaih, Yu-Hang Chien, Shih-Yi Chien</li>
<li>for: 本研究目的是解决人工神经网络中的消逝梯度和透过率问题。</li>
<li>methods: 本研究采用了诊眼学习过程，包括解释、选择、理解、填充和组织等步骤， derivation of the pupil learning mechanism (PLM) for modifying the network structure and weights of 2-layer neural networks (2LNNs).</li>
<li>results: 实验结果表明，PLM模块设计得当，并且提出的PLM模型在对比线性回归模型和传统的反propagation-based 2LNN模型的基础上显示出了超越性。<details>
<summary>Abstract</summary>
Studies on artificial neural networks rarely address both vanishing gradients and overfitting issues. In this study, we follow the pupil learning procedure, which has the features of interpreting, picking, understanding, cramming, and organizing, to derive the pupil learning mechanism (PLM) by which to modify the network structure and weights of 2-layer neural networks (2LNNs). The PLM consists of modules for sequential learning, adaptive learning, perfect learning, and less-overfitted learning. Based upon a copper price forecasting dataset, we conduct an experiment to validate the PLM module design modules, and an experiment to evaluate the performance of PLM. The empirical results indeed approve the PLM module design and show the superiority of the proposed PLM model over the linear regression model and the conventional backpropagation-based 2LNN model.
</details>
<details>
<summary>摘要</summary>
研究人工神经网络很少考虑过量衰减和过拟合问题。在这项研究中，我们采用了学生学习过程，具有解释、选择、理解、填充和组织等特点， derivation of pupil learning mechanism (PLM)，用于修改网络结构和权重。PLM包括顺序学习模块、适应学习模块、完美学习模块和较少过拟合学习模块。基于一个铜价预测数据集，我们进行了实验验证PLM模块设计和PLM模型的性能。实验结果确实证明了PLM模块的设计和提案的PLM模型的优越性，比Linear Regression模型和传统的反射层次神经网络模型更好。
</details></li>
</ul>
<hr>
<h2 id="User-Controlled-Knowledge-Fusion-in-Large-Language-Models-Balancing-Creativity-and-Hallucination"><a href="#User-Controlled-Knowledge-Fusion-in-Large-Language-Models-Balancing-Creativity-and-Hallucination" class="headerlink" title="User-Controlled Knowledge Fusion in Large Language Models: Balancing Creativity and Hallucination"></a>User-Controlled Knowledge Fusion in Large Language Models: Balancing Creativity and Hallucination</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16139">http://arxiv.org/abs/2307.16139</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Zhang</li>
<li>for:  This paper aims to improve the balance between the creativity and accuracy of large language models (LLMs) by introducing a user-controllable mechanism that modulates the degree of faithfulness to external knowledge.</li>
<li>methods:  The proposed approach uses a numerical tag during the fine-tuning phase of the LLM’s training to represent the degree of faithfulness to reference knowledge. The tag is computed through a combination of ROUGE scores, Sentence-BERT embeddings, and the LLM’s self-evaluation score. During model inference, users can manipulate the tag to control the degree of the LLM’s reliance on external knowledge.</li>
<li>results:  The paper presents extensive experiments across various scenarios, demonstrating the adaptability and efficacy of the proposed approach in ensuring the quality and accuracy of the LLM’s responses. The results show that the approach can enhance the versatility of LLMs while maintaining a balance between creativity and hallucination.<details>
<summary>Abstract</summary>
In modern dialogue systems, the use of Large Language Models (LLMs) has grown exponentially due to their capacity to generate diverse, relevant, and creative responses. Despite their strengths, striking a balance between the LLMs' creativity and their faithfulness to external knowledge remains a key challenge. This paper presents an innovative user-controllable mechanism that modulates the balance between an LLM's imaginative capabilities and its adherence to factual information. Our approach incorporates a numerical tag during the fine-tuning phase of the LLM's training, representing the degree of faithfulness to the reference knowledge in the generated responses. This degree is computed through an automated process that measures lexical overlap using ROUGE scores, semantic similarity using Sentence-BERT embeddings, and an LLM's self-evaluation score. During model inference, users can manipulate this numerical tag, thus controlling the degree of the LLM's reliance on external knowledge. We conduct extensive experiments across various scenarios, demonstrating the adaptability of our method and its efficacy in ensuring the quality and accuracy of the LLM's responses. The results highlight the potential of our approach to enhance the versatility of LLMs while maintaining a balance between creativity and hallucination.
</details>
<details>
<summary>摘要</summary>
现代对话系统中，大语言模型（LLM）的使用量已经呈指数增长趋势，这是因为它们可以生成多样、相关、创新的回答。然而，在这些模型的应用中，保持LLM的创造力和外部知识的准确性之间的平衡仍然是一个关键挑战。这篇论文提出了一种新的用户可控的机制，可以调整LLM的创造力和外部知识的准确性之间的平衡。我们的方法在训练LLM的细化阶段添加了一个数字标签，表示生成回答中对参考知识的忠诚度的程度。这个程度通过自动化的过程计算，使用ROUGE分数、句子BERT嵌入和LLM自我评价分数来测量lexical overlap和semantic similarity。在模型推理阶段，用户可以控制这个数字标签，以控制LLM对外部知识的依赖程度。我们进行了多种enario的广泛实验，证明了我们的方法的适应性和精度。结果显示了我们的方法可以提高LLM的多样性，同时保持创造力和幻想的平衡。
</details></li>
</ul>
<hr>
<h2 id="Deep-Unrolling-Networks-with-Recurrent-Momentum-Acceleration-for-Nonlinear-Inverse-Problems"><a href="#Deep-Unrolling-Networks-with-Recurrent-Momentum-Acceleration-for-Nonlinear-Inverse-Problems" class="headerlink" title="Deep Unrolling Networks with Recurrent Momentum Acceleration for Nonlinear Inverse Problems"></a>Deep Unrolling Networks with Recurrent Momentum Acceleration for Nonlinear Inverse Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16120">http://arxiv.org/abs/2307.16120</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhouqp631/dunets-rma">https://github.com/zhouqp631/dunets-rma</a></li>
<li>paper_authors: Qingping Zhou, Jiayu Qian, Junqi Tang, Jinglai Li</li>
<li>for: 解决非线性 inverse 图像问题</li>
<li>methods: 使用 momentum 加速 (RMA) 框架，包括 long short-term memory Recurrent Neural Network (LSTM-RNN) 模型，以便学习和保留前一个梯度的知识。</li>
<li>results: 在 two 个非线性 inverse 问题中，提供了实验结果，其中第一个问题中的改进效果随着问题的非线性度而增大，第二个问题中的结果进一步证明了 RMA 方案可以在强不稳定问题中明显提高 DuNets 的性能。<details>
<summary>Abstract</summary>
Combining the strengths of model-based iterative algorithms and data-driven deep learning solutions, deep unrolling networks (DuNets) have become a popular tool to solve inverse imaging problems. While DuNets have been successfully applied to many linear inverse problems, nonlinear problems tend to impair the performance of the method. Inspired by momentum acceleration techniques that are often used in optimization algorithms, we propose a recurrent momentum acceleration (RMA) framework that uses a long short-term memory recurrent neural network (LSTM-RNN) to simulate the momentum acceleration process. The RMA module leverages the ability of the LSTM-RNN to learn and retain knowledge from the previous gradients. We apply RMA to two popular DuNets -- the learned proximal gradient descent (LPGD) and the learned primal-dual (LPD) methods, resulting in LPGD-RMA and LPD-RMA respectively. We provide experimental results on two nonlinear inverse problems: a nonlinear deconvolution problem, and an electrical impedance tomography problem with limited boundary measurements. In the first experiment we have observed that the improvement due to RMA largely increases with respect to the nonlinearity of the problem. The results of the second example further demonstrate that the RMA schemes can significantly improve the performance of DuNets in strongly ill-posed problems.
</details>
<details>
<summary>摘要</summary>
通过结合模型基于迭代算法和数据驱动深度学习解决方案，深度拥叠网络（DuNets）已成为解析逆问题的流行工具。虽然DuNets已成功应用于许多线性逆问题，但非线性问题往往会伤害方法的性能。 draw inspiration from momentum acceleration techniques often used in optimization algorithms, we propose a recurrent momentum acceleration (RMA) framework that uses a long short-term memory recurrent neural network (LSTM-RNN) to simulate the momentum acceleration process. The RMA module leverages the ability of the LSTM-RNN to learn and retain knowledge from the previous gradients. We apply RMA to two popular DuNets -- the learned proximal gradient descent (LPGD) and the learned primal-dual (LPD) methods, resulting in LPGD-RMA and LPD-RMA respectively. We provide experimental results on two nonlinear inverse problems: a nonlinear deconvolution problem, and an electrical impedance tomography problem with limited boundary measurements. In the first experiment we have observed that the improvement due to RMA largely increases with respect to the nonlinearity of the problem. The results of the second example further demonstrate that the RMA schemes can significantly improve the performance of DuNets in strongly ill-posed problems.Here's the translation in Traditional Chinese:通过结合模型基于迭代算法和数据驱动深度学习解决方案，深度拥叠网络（DuNets）已成为解析逆问题的流行工具。处理非线性问题的性能往往会受到方法的影响。 draw inspiration from momentum acceleration techniques often used in optimization algorithms, we propose a recurrent momentum acceleration (RMA) framework that uses a long short-term memory recurrent neural network (LSTM-RNN) to simulate the momentum acceleration process. The RMA module leverages the ability of the LSTM-RNN to learn and retain knowledge from the previous gradients. We apply RMA to two popular DuNets -- the learned proximal gradient descent (LPGD) and the learned primal-dual (LPD) methods, resulting in LPGD-RMA and LPD-RMA respectively. We provide experimental results on two nonlinear inverse problems: a nonlinear deconvolution problem, and an electrical impedance tomography problem with limited boundary measurements. In the first experiment we have observed that the improvement due to RMA largely increases with respect to the nonlinearity of the problem. The results of the second example further demonstrate that the RMA schemes can significantly improve the performance of DuNets in strongly ill-posed problems.
</details></li>
</ul>
<hr>
<h2 id="TMPNN-High-Order-Polynomial-Regression-Based-on-Taylor-Map-Factorization"><a href="#TMPNN-High-Order-Polynomial-Regression-Based-on-Taylor-Map-Factorization" class="headerlink" title="TMPNN: High-Order Polynomial Regression Based on Taylor Map Factorization"></a>TMPNN: High-Order Polynomial Regression Based on Taylor Map Factorization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16105">http://arxiv.org/abs/2307.16105</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/andiva/tmpnn">https://github.com/andiva/tmpnn</a></li>
<li>paper_authors: Andrei Ivanov, Stefan Maria Ailuro</li>
<li>for: 这 paper 用于构建高阶多变量回归模型，以解决非线性模式的表达问题。</li>
<li>methods: 该方法基于泰勒地图分解，自然地实现多目标回归和内部关系的捕捉。</li>
<li>results: 通过对 UCI 开放访问数据集、Feynman 符号回归数据集和 Friedman-1 数据集进行比较，提出的方法与现状强度回归方法相当，并在特定任务上超越它们。<details>
<summary>Abstract</summary>
Polynomial regression is widely used and can help to express nonlinear patterns. However, considering very high polynomial orders may lead to overfitting and poor extrapolation ability for unseen data. The paper presents a method for constructing a high-order polynomial regression based on the Taylor map factorization. This method naturally implements multi-target regression and can capture internal relationships between targets. Additionally, we introduce an approach for model interpretation in the form of systems of differential equations. By benchmarking on UCI open access datasets, Feynman symbolic regression datasets, and Friedman-1 datasets, we demonstrate that the proposed method performs comparable to the state-of-the-art regression methods and outperforms them on specific tasks.
</details>
<details>
<summary>摘要</summary>
多项式回传函数广泛使用，可以表示非线性征式。然而，考虑非常高的多项式顺位可能会导致过拟合和未见数据的拟合能力不佳。本文提出一种基于泰勒对映缩减法的高顺位多项式回传函数建构方法。这种方法自然实现多目标回传和对目标之间的内部关系捕捉。此外，我们导入一种模型解释方法，即透过多元方程系统来实现。经过UCIC公开数据集、Feynman符号回传函数数据集和Friedman-1数据集的对比，我们示出了提案方法与现有回传方法相比，在特定任务上表现相似，甚至超过它们。
</details></li>
</ul>
<hr>
<h2 id="AI-Increases-Global-Access-to-Reliable-Flood-Forecasts"><a href="#AI-Increases-Global-Access-to-Reliable-Flood-Forecasts" class="headerlink" title="AI Increases Global Access to Reliable Flood Forecasts"></a>AI Increases Global Access to Reliable Flood Forecasts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16104">http://arxiv.org/abs/2307.16104</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/google-research-datasets/global_streamflow_model_paper">https://github.com/google-research-datasets/global_streamflow_model_paper</a></li>
<li>paper_authors: Grey Nearing, Deborah Cohen, Vusumuzi Dube, Martin Gauch, Oren Gilon, Shaun Harrigan, Avinatan Hassidim, Frederik Kratzert, Asher Metzger, Sella Nevo, Florian Pappenberger, Christel Prudhomme, Guy Shalev, Shlomo Shenzis, Tadele Tekalign, Dana Weitzner, Yoss Matias</li>
<li>for: 这个论文的目的是为了开发一种基于人工智能的洪水预测模型，以提供更加准确和有效的洪水预测。</li>
<li>methods: 这个论文使用了人工智能技术来预测洪水事件，并对比了这种方法与现有的全球ydrology模型。</li>
<li>results: 这个论文的结果表明，使用人工智能模型可以在不同的大陆和返回期下实现更高的准确率和更早的预测时间，特别是在无测流域中。<details>
<summary>Abstract</summary>
Floods are one of the most common and impactful natural disasters, with a disproportionate impact in developing countries that often lack dense streamflow monitoring networks. Accurate and timely warnings are critical for mitigating flood risks, but accurate hydrological simulation models typically must be calibrated to long data records in each watershed where they are applied. We developed an Artificial Intelligence (AI) model to predict extreme hydrological events at timescales up to 7 days in advance. This model significantly outperforms current state of the art global hydrology models (the Copernicus Emergency Management Service Global Flood Awareness System) across all continents, lead times, and return periods. AI is especially effective at forecasting in ungauged basins, which is important because only a few percent of the world's watersheds have stream gauges, with a disproportionate number of ungauged basins in developing countries that are especially vulnerable to the human impacts of flooding. We produce forecasts of extreme events in South America and Africa that achieve reliability approaching the current state of the art in Europe and North America, and we achieve reliability at between 4 and 6-day lead times that are similar to current state of the art nowcasts (0-day lead time). Additionally, we achieve accuracies over 10-year return period events that are similar to current accuracies over 2-year return period events, meaning that AI can provide warnings earlier and over larger and more impactful events. The model that we develop in this paper has been incorporated into an operational early warning system that produces publicly available (free and open) forecasts in real time in over 80 countries. This work using AI and open data highlights a need for increasing the availability of hydrological data to continue to improve global access to reliable flood warnings.
</details>
<details>
<summary>摘要</summary>
洪水是一种非常常见且影响深远的自然灾害，尤其在发展中国家，那里缺乏密集的流量监测网络。准确和及时的警告对洪水风险的减轻具有极大的重要性，但是需要准确的水文模型进行准确地预测。我们开发了一种人工智能（AI）模型，用于预测7天内的极端水文事件。这种模型在全球各大洲、不同的领head time和返回期下都有显著的优势，特别是在不具有流量监测的河流basin中。因为只有很少的世界河流basin拥有流量监测，而发展中国家占这些basin的大多数，因此AI的预测特别有用。我们在南美和非洲预测极端事件的可靠性与现有的全球水文模型（Copernicus Emergency Management Service Global Flood Awareness System）相似，并且在4-6天的领head time内达到了类似的可靠性。此外，我们在10年返回期内达到了类似的准确率，这意味着AI可以提供更早的警告和更大的影响。我们在这篇论文中开发的模型已经被integrated into an operational early warning system，该系统在实时生成公共可用（免费开放）的预测。这种使用AI和开放数据的工作 highlights the need for increasing the availability of hydrological data to continue to improve global access to reliable flood warnings.
</details></li>
</ul>
<hr>
<h2 id="On-Neural-Network-approximation-of-ideal-adversarial-attack-and-convergence-of-adversarial-training"><a href="#On-Neural-Network-approximation-of-ideal-adversarial-attack-and-convergence-of-adversarial-training" class="headerlink" title="On Neural Network approximation of ideal adversarial attack and convergence of adversarial training"></a>On Neural Network approximation of ideal adversarial attack and convergence of adversarial training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16099">http://arxiv.org/abs/2307.16099</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rajdeep Haldar, Qifan Song</li>
<li>for: 本文针对了适用于防御模型的敌对攻击。</li>
<li>methods: 本文使用了一个trainable函数来表示敌对攻击，不需要进一步的梯度计算。首先， authors 证明了理论上最佳的攻击可以表示为平滑的piece-wise函数（piece-wise Holder函数）。然后， authors 使用了一个神经网络来近似这些函数。最后， authors 将理想的攻击过程模拟为一个神经网络，并将防御训练降到了一个数学游戏中。</li>
<li>results: 本文获得了防御训练中敌对攻击损失的减少率，并且提出了这种减少率与训练数据量的关系。<details>
<summary>Abstract</summary>
Adversarial attacks are usually expressed in terms of a gradient-based operation on the input data and model, this results in heavy computations every time an attack is generated. In this work, we solidify the idea of representing adversarial attacks as a trainable function, without further gradient computation. We first motivate that the theoretical best attacks, under proper conditions, can be represented as smooth piece-wise functions (piece-wise H\"older functions). Then we obtain an approximation result of such functions by a neural network. Subsequently, we emulate the ideal attack process by a neural network and reduce the adversarial training to a mathematical game between an attack network and a training model (a defense network). We also obtain convergence rates of adversarial loss in terms of the sample size $n$ for adversarial training in such a setting.
</details>
<details>
<summary>摘要</summary>
“敌对攻击通常表示为输入数据和模型之间的梯度基础操作，这会导致每次产生攻击时需要严重的计算。在这个工作中，我们固化了代表敌对攻击为可训练函数的想法，不需要进一步的梯度计算。我们首先认为，在适当的情况下，理论上最佳的攻击可以表示为稳定的块状函数（块状Holder函数）。然后我们获得了这些函数的近似结果，使用神经网络。接着，我们模拟理想的攻击过程，使用神经网络，并将对抗训练降到了数学游戏中，即攻击网络和训练模型（防御网络）之间的游戏。我们还获得了对抗训练过程中攻击损失的数量随扩展大小 $n$ 的收敛速率。”
</details></li>
</ul>
<hr>
<h2 id="ADR-GNN-Advection-Diffusion-Reaction-Graph-Neural-Networks"><a href="#ADR-GNN-Advection-Diffusion-Reaction-Graph-Neural-Networks" class="headerlink" title="ADR-GNN: Advection-Diffusion-Reaction Graph Neural Networks"></a>ADR-GNN: Advection-Diffusion-Reaction Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16092">http://arxiv.org/abs/2307.16092</a></li>
<li>repo_url: None</li>
<li>paper_authors: Moshe Eliasof, Eldad Haber, Eran Treister</li>
<li>for: 本研究旨在提出一种基于扩散-吸引-反应系统的图 neural network 模型（ADR-GNN），用于解决图structured数据中复杂现象的学习表示。</li>
<li>methods: ADR-GNN 使用扩散、吸引和反应三种基本操作来模型信息的传递、散射和非线性变换。</li>
<li>results: 在实验中，ADR-GNN 在真实世界的节点分类和空间时间数据集上表现出优于或与状态顶尖网络相当的性能。<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) have shown remarkable success in learning representations for graph-structured data. However, GNNs still face challenges in modeling complex phenomena that involve advection. In this paper, we propose a novel GNN architecture based on Advection-Diffusion-Reaction systems, called ADR-GNN. Advection models the directed transportation of information, diffusion captures the local smoothing of information, and reaction represents the non-linear transformation of information in channels. We provide an analysis of the qualitative behavior of ADR-GNN, that shows the benefit of combining advection, diffusion, and reaction. To demonstrate its efficacy, we evaluate ADR-GNN on real-world node classification and spatio-temporal datasets, and show that it improves or offers competitive performance compared to state-of-the-art networks.
</details>
<details>
<summary>摘要</summary>
графические нейронные сети (GNNs) имеют огромный потенциал для обучения представлений для данных, имеющих графическую структуру. Однако, GNNs still face challenges in моделировании сложных явлений, которые involve advection. В этой статье, we propose a novel GNN architecture based on Advection-Diffusion-Reaction systems, called ADR-GNN. Advection models the directed transportation of information, diffusion captures the local smoothing of information, and reaction represents the non-linear transformation of information in channels. We provide an analysis of the qualitative behavior of ADR-GNN, that shows the benefit of combining advection, diffusion, and reaction. To demonstrate its efficacy, we evaluate ADR-GNN on real-world node classification and spatio-temporal datasets, and show that it improves or offers competitive performance compared to state-of-the-art networks.Here's the translation breakdown:* графические (graph-structured) -> 图Structured (图Structured)* GNNs -> GNNs (同义译法)* involve advection -> involve advection (同义译法)* Advection-Diffusion-Reaction systems -> Advection-Diffusion-Reaction systems (同义译法)* ADR-GNN -> ADR-GNN (同义译法)* Advection models the directed transportation of information -> Advection models the directed transportation of information (同义译法)* diffusion captures the local smoothing of information -> diffusion captures the local smoothing of information (同义译法)* reaction represents the non-linear transformation of information in channels -> reaction represents the non-linear transformation of information in channels (同义译法)* we provide an analysis of the qualitative behavior of ADR-GNN -> we provide an analysis of the qualitative behavior of ADR-GNN (同义译法)* demonstrate its efficacy -> demonstrate its efficacy (同义译法)* evaluate ADR-GNN on real-world node classification and spatio-temporal datasets -> evaluate ADR-GNN on real-world node classification and spatio-temporal datasets (同义译法)* improve or offer competitive performance -> improve or offer competitive performance (同义译法)Note that the translation is based on the simplified Chinese version of the text, and some of the vocabulary and grammar may be different from traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Rapid-Flood-Inundation-Forecast-Using-Fourier-Neural-Operator"><a href="#Rapid-Flood-Inundation-Forecast-Using-Fourier-Neural-Operator" class="headerlink" title="Rapid Flood Inundation Forecast Using Fourier Neural Operator"></a>Rapid Flood Inundation Forecast Using Fourier Neural Operator</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16090">http://arxiv.org/abs/2307.16090</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander Y. Sun, Zhi Li, Wonhyun Lee, Qixing Huang, Bridget R. Scanlon, Clint Dawson</li>
<li>for: 预测洪水覆盖范围和浸水深度</li>
<li>methods: 组合过程基于模型和数据驱动机器学习方法</li>
<li>results: FNO模型在预测洪水覆盖范围和浸水深度方面表现出色，对新地点应用也有良好的泛化能力。<details>
<summary>Abstract</summary>
Flood inundation forecast provides critical information for emergency planning before and during flood events. Real time flood inundation forecast tools are still lacking. High-resolution hydrodynamic modeling has become more accessible in recent years, however, predicting flood extents at the street and building levels in real-time is still computationally demanding. Here we present a hybrid process-based and data-driven machine learning (ML) approach for flood extent and inundation depth prediction. We used the Fourier neural operator (FNO), a highly efficient ML method, for surrogate modeling. The FNO model is demonstrated over an urban area in Houston (Texas, U.S.) by training using simulated water depths (in 15-min intervals) from six historical storm events and then tested over two holdout events. Results show FNO outperforms the baseline U-Net model. It maintains high predictability at all lead times tested (up to 3 hrs) and performs well when applying to new sites, suggesting strong generalization skill.
</details>
<details>
<summary>摘要</summary>
洪水涌入预测提供了紧急准备和洪水事件发生时的重要信息。现有的实时洪水涌入预测工具仍然缺乏。高分辨率 hidrodynamic 模型在过去几年内变得更加可 accessible，但是在实时预测洪水覆盖面和涌入深度方面仍然具有计算压力。我们采用了一种混合的进程基于和数据驱动的机器学习（ML）方法，用于预测洪水覆盖面和涌入深度。我们使用了Fourier neural operator（FNO）模型，这是一种非常高效的ML方法，用于模拟器。FNO 模型在德州休斯顿（Texas, U.S.）的城市区域上进行了训练，使用了六个历史洪水事件中的计算水深（每15分钟），然后在两个保留事件上进行测试。结果显示，FNO 模型在所有领先时间（最多3小时）中保持高预测性，并在应用于新的场景时表现良好，表明它具有强大的泛化能力。
</details></li>
</ul>
<hr>
<h2 id="Using-Implicit-Behavior-Cloning-and-Dynamic-Movement-Primitive-to-Facilitate-Reinforcement-Learning-for-Robot-Motion-Planning"><a href="#Using-Implicit-Behavior-Cloning-and-Dynamic-Movement-Primitive-to-Facilitate-Reinforcement-Learning-for-Robot-Motion-Planning" class="headerlink" title="Using Implicit Behavior Cloning and Dynamic Movement Primitive to Facilitate Reinforcement Learning for Robot Motion Planning"></a>Using Implicit Behavior Cloning and Dynamic Movement Primitive to Facilitate Reinforcement Learning for Robot Motion Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16062">http://arxiv.org/abs/2307.16062</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zengjie Zhang, Jayden Hong, Amir Soufi Enayati, Homayoun Najjaran</li>
<li>for: 提高多自由度机器人运动规划的效率，使其具有更快的训练速度和更好的通用性。</li>
<li>methods: 使用偏函数吸引（IBC）和动态运动基本模型（DMP）来提高非约束RL代理的训练速度和通用性。</li>
<li>results: 在模拟环境中比较研究表明，提案方法比普通RL代理更快地训练和更高的得分。在真实机器人实验中，对简单的组装任务进行了应用。<details>
<summary>Abstract</summary>
Reinforcement learning (RL) for motion planning of multi-degree-of-freedom robots still suffers from low efficiency in terms of slow training speed and poor generalizability. In this paper, we propose a novel RL-based robot motion planning framework that uses implicit behavior cloning (IBC) and dynamic movement primitive (DMP) to improve the training speed and generalizability of an off-policy RL agent. IBC utilizes human demonstration data to leverage the training speed of RL, and DMP serves as a heuristic model that transfers motion planning into a simpler planning space. To support this, we also create a human demonstration dataset using a pick-and-place experiment that can be used for similar studies. Comparison studies in simulation reveal the advantage of the proposed method over the conventional RL agents with faster training speed and higher scores. A real-robot experiment indicates the applicability of the proposed method to a simple assembly task. Our work provides a novel perspective on using motion primitives and human demonstration to leverage the performance of RL for robot applications.
</details>
<details>
<summary>摘要</summary>
现代人工智能技术中，虚拟套件学习（RL）在多度量自由机器人运动规划方面仍然受到低效率的困扰，即训练速度慢和泛化能力差。在这篇论文中，我们提出了一种基于RL的新型机器人运动规划框架，使用隐式行为假设（IBC）和动态运动原型（DMP）来提高RL Agent的训练速度和泛化能力。IBC利用人类示范数据来加速RL训练，而DMP作为一种简化计划空间的转移模型。为支持这一点，我们还创建了一个人类示范数据集，可以用于类似的研究。在模拟环境中的比较研究表明，我们的方法比普通RL Agent快速训练和高得分。一个真实机器人实验表明了我们的方法在简单的组装任务中的可行性。我们的工作为机器人应用中的RL技术提供了一个新的视角，并使用运动原型和人类示范来提高RL的表现。
</details></li>
</ul>
<hr>
<h2 id="Click-Conversion-Multi-Task-Model-with-Position-Bias-Mitigation-for-Sponsored-Search-in-eCommerce"><a href="#Click-Conversion-Multi-Task-Model-with-Position-Bias-Mitigation-for-Sponsored-Search-in-eCommerce" class="headerlink" title="Click-Conversion Multi-Task Model with Position Bias Mitigation for Sponsored Search in eCommerce"></a>Click-Conversion Multi-Task Model with Position Bias Mitigation for Sponsored Search in eCommerce</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16060">http://arxiv.org/abs/2307.16060</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yibo Wang, Yanbing Xue, Bo Liu, Musen Wen, Wenting Zhao, Stephen Guo, Philip S. Yu</li>
<li>for: 防止排名系统中的位置偏见，以提高搜索结果的公平性</li>
<li>methods: 提出了两种Position-Aware Click-Conversion (PACC)和PACC via Position Embedding (PACC-PE)模型，其中PACC通过概率 decomposing 模型 позиция信息，而PACC-PE通过神经网络模型产生产品特定的位置信息</li>
<li>results: 实验结果表明，提议的模型可以更好地预测Click-through rate (CTR)和Conversion rate (CVR)，同时可以减少位置偏见的影响，提高搜索结果的公平性。<details>
<summary>Abstract</summary>
Position bias, the phenomenon whereby users tend to focus on higher-ranked items of the search result list regardless of the actual relevance to queries, is prevailing in many ranking systems. Position bias in training data biases the ranking model, leading to increasingly unfair item rankings, click-through-rate (CTR), and conversion rate (CVR) predictions. To jointly mitigate position bias in both item CTR and CVR prediction, we propose two position-bias-free CTR and CVR prediction models: Position-Aware Click-Conversion (PACC) and PACC via Position Embedding (PACC-PE). PACC is built upon probability decomposition and models position information as a probability. PACC-PE utilizes neural networks to model product-specific position information as embedding. Experiments on the E-commerce sponsored product search dataset show that our proposed models have better ranking effectiveness and can greatly alleviate position bias in both CTR and CVR prediction.
</details>
<details>
<summary>摘要</summary>
“位置偏见”现象，用户倾向于强调搜索结果列表中的高排名项，不 regards 实际相关性，是许多排名系统中的 prevailing 现象。在训练数据中的位置偏见会偏移排名模型，导致搜索结果的不公正性，点击率（CTR）和转化率（CVR）预测也会增加不公正性。为了同时消除位置偏见在Item CTR和CVR预测中，我们提出了两种位置偏见自由的预测模型：Position-Aware Click-Conversion（PACC）和PACC via Position Embedding（PACC-PE）。PACC基于概率分解，将位置信息作为概率来model。PACC-PE通过神经网络来模型产品Specific的位置信息作为嵌入。在电商推荐搜索数据集上进行实验，我们的提出的模型得到了更好的排名效果，可以很好地消除位置偏见在CTR和CVR预测中。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-the-Robustness-of-Test-Selection-Methods-for-Deep-Neural-Networks"><a href="#Evaluating-the-Robustness-of-Test-Selection-Methods-for-Deep-Neural-Networks" class="headerlink" title="Evaluating the Robustness of Test Selection Methods for Deep Neural Networks"></a>Evaluating the Robustness of Test Selection Methods for Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01314">http://arxiv.org/abs/2308.01314</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiang Hu, Yuejun Guo, Xiaofei Xie, Maxime Cordy, Wei Ma, Mike Papadakis, Yves Le Traon</li>
<li>for: 这篇论文的目的是探讨深度学习系统测试中的挑战和解决方法。</li>
<li>methods: 该论文使用了多种测试选择方法，以降低标注采集的时间和劳动成本。</li>
<li>results: 研究发现，这些已经报告出色结果的测试选择方法在实际场景中可能并不是一定可靠，存在潜在的坑害。<details>
<summary>Abstract</summary>
Testing deep learning-based systems is crucial but challenging due to the required time and labor for labeling collected raw data. To alleviate the labeling effort, multiple test selection methods have been proposed where only a subset of test data needs to be labeled while satisfying testing requirements. However, we observe that such methods with reported promising results are only evaluated under simple scenarios, e.g., testing on original test data. This brings a question to us: are they always reliable? In this paper, we explore when and to what extent test selection methods fail for testing. Specifically, first, we identify potential pitfalls of 11 selection methods from top-tier venues based on their construction. Second, we conduct a study on five datasets with two model architectures per dataset to empirically confirm the existence of these pitfalls. Furthermore, we demonstrate how pitfalls can break the reliability of these methods. Concretely, methods for fault detection suffer from test data that are: 1) correctly classified but uncertain, or 2) misclassified but confident. Remarkably, the test relative coverage achieved by such methods drops by up to 86.85%. On the other hand, methods for performance estimation are sensitive to the choice of intermediate-layer output. The effectiveness of such methods can be even worse than random selection when using an inappropriate layer.
</details>
<details>
<summary>摘要</summary>
测试深度学习系统是必要但困难的，因为需要大量的时间和劳动来标注采集到的原始数据。为了减轻标注劳动，多种测试选择方法已经被提议，只需要标注一 subset 的测试数据，并且满足测试要求。然而，我们发现这些方法在简单的场景下（例如，测试原始测试数据）所报道的结果并不一定可靠。在这篇论文中，我们探索这些测试选择方法在测试时是否可靠，并特别是在哪些情况下会失败。首先，我们确定了11种选择方法的潜在坑害，根据它们的构造。然后，我们在五个数据集上进行了两种模型架构的实验，以确认这些坑害的存在。此外，我们还示出了这些坑害如何使测试选择方法失效。例如，用于缺陷检测的方法会面临以下两种情况：1）正确分类但不确定的测试数据，或2）错误分类但高自信的测试数据。这些方法的测试相对覆盖率下降了86.85%。另一方面，用于性能估计的方法对选择中间层输出的选择非常敏感。使用不合适的层时，这些方法的效果可以比Random Selection更差。
</details></li>
</ul>
<hr>
<h2 id="Unveiling-Exotic-Magnetic-Phases-in-Fibonacci-Quasicrystalline-Stacking-of-Ferromagnetic-Layers-through-Machine-Learning"><a href="#Unveiling-Exotic-Magnetic-Phases-in-Fibonacci-Quasicrystalline-Stacking-of-Ferromagnetic-Layers-through-Machine-Learning" class="headerlink" title="Unveiling Exotic Magnetic Phases in Fibonacci Quasicrystalline Stacking of Ferromagnetic Layers through Machine Learning"></a>Unveiling Exotic Magnetic Phases in Fibonacci Quasicrystalline Stacking of Ferromagnetic Layers through Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16052">http://arxiv.org/abs/2307.16052</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pablo S. Cornaglia, Matias Nuñez, D. J. Garcia</li>
<li>for: 这个研究是一个涵盖费波那契镁矿层堆栈的约束理论分析，可能通过磁性van der Waals材料实现。</li>
<li>methods: 我们构建了这个磁性异构体系的模型，包括第二邻居层磁相互作用，并使用机器学习方法来探索这个异构系统的磁性行为。</li>
<li>results: 我们发现了一个独特的梯形旋转螺旋相，在堆高的函数下，磁化程度下降对数减少。此外，我们还描述了这个磁性相位图，并发现了其他斜线和非斜线相。<details>
<summary>Abstract</summary>
In this study, we conduct a comprehensive theoretical analysis of a Fibonacci quasicrystalline stacking of ferromagnetic layers, potentially realizable using van der Waals magnetic materials. We construct a model of this magnetic heterostructure, which includes up to second neighbor interlayer magnetic interactions, that displays a complex relationship between geometric frustration and magnetic order in this quasicrystalline system. To navigate the parameter space and identify distinct magnetic phases, we employ a machine learning approach, which proves to be a powerful tool in revealing the complex magnetic behavior of this system. We offer a thorough description of the magnetic phase diagram as a function of the model parameters. Notably, we discover among other collinear and non-collinear phases, a unique ferromagnetic alternating helical phase. In this non-collinear quasiperiodic ferromagnetic configuration the magnetization decreases logarithmically with the stack height.
</details>
<details>
<summary>摘要</summary>
在这个研究中，我们进行了详细的理论分析，涉及到费波南尼克镧矿层排列，可能通过碳氢化合物磁 material实现。我们构建了这种磁化层结构的模型，包括最多第二邻居层磁相互作用，显示了这种各向异性束缚磁系统中的复杂关系。为了探索参数空间并确定不同磁相态，我们使用机器学习方法，证明是一种有力的工具，可以揭示这种磁系统的复杂磁性行为。我们提供了磁相态图，作为函数于模型参数的磁相态图。值得一提的是，我们发现了一种独特的排列式镧矿磁相态，在这种非排列征 periodic ferromagnetic配置中，磁化度随堆高度呈减少对数型变化。
</details></li>
</ul>
<hr>
<h2 id="Okapi-Instruction-tuned-Large-Language-Models-in-Multiple-Languages-with-Reinforcement-Learning-from-Human-Feedback"><a href="#Okapi-Instruction-tuned-Large-Language-Models-in-Multiple-Languages-with-Reinforcement-Learning-from-Human-Feedback" class="headerlink" title="Okapi: Instruction-tuned Large Language Models in Multiple Languages with Reinforcement Learning from Human Feedback"></a>Okapi: Instruction-tuned Large Language Models in Multiple Languages with Reinforcement Learning from Human Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16039">http://arxiv.org/abs/2307.16039</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nlp-uoregon/okapi">https://github.com/nlp-uoregon/okapi</a></li>
<li>paper_authors: Viet Dac Lai, Chien Van Nguyen, Nghia Trung Ngo, Thuat Nguyen, Franck Dernoncourt, Ryan A. Rossi, Thien Huu Nguyen</li>
<li>for: 这paper的目的是提高大型自然语言模型（LLM）的发展，特别是在多语言下进行 instrucion 调整，以实现模型的强大学习能力。</li>
<li>methods: 这paper使用了supervised fine-tuning（SFT）和人工反馈学习（RLHF）两种方法来进行 instrucion 调整，以便生成最佳的商业LLM（如ChatGPT）。</li>
<li>results: 这paper提出了Okapi，第一个基于RLHF的多语言 instrucion 调整系统，并提供了26种多样化语言的 instrucion 和回快数据，以便进行未来的多语言LLM研究。我们的实验表明，RLHF在多语言 instrucion 调整中比SFT更有优势。我们的框架和资源在<a target="_blank" rel="noopener" href="https://github.com/nlp-uoregon/Okapi%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/nlp-uoregon/Okapi上发布。</a><details>
<summary>Abstract</summary>
A key technology for the development of large language models (LLMs) involves instruction tuning that helps align the models' responses with human expectations to realize impressive learning abilities. Two major approaches for instruction tuning characterize supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF), which are currently applied to produce the best commercial LLMs (e.g., ChatGPT). To improve the accessibility of LLMs for research and development efforts, various instruction-tuned open-source LLMs have also been introduced recently, e.g., Alpaca, Vicuna, to name a few. However, existing open-source LLMs have only been instruction-tuned for English and a few popular languages, thus hindering their impacts and accessibility to many other languages in the world. Among a few very recent work to explore instruction tuning for LLMs in multiple languages, SFT has been used as the only approach to instruction-tune LLMs for multiple languages. This has left a significant gap for fine-tuned LLMs based on RLHF in diverse languages and raised important questions on how RLHF can boost the performance of multilingual instruction tuning. To overcome this issue, we present Okapi, the first system with instruction-tuned LLMs based on RLHF for multiple languages. Okapi introduces instruction and response-ranked data in 26 diverse languages to facilitate the experiments and development of future multilingual LLM research. We also present benchmark datasets to enable the evaluation of generative LLMs in multiple languages. Our experiments demonstrate the advantages of RLHF for multilingual instruction over SFT for different base models and datasets. Our framework and resources are released at https://github.com/nlp-uoregon/Okapi.
</details>
<details>
<summary>摘要</summary>
大频率模型的发展需要关键技术之一是指令调整，帮助模型的回答与人类期望保持一致，从而实现惊人的学习能力。目前最常用的两种方法是监督精度调整（SFT）和人类反馈学习（RLHF）。这两种方法目前已经用于生产最佳商业大型语言模型（如ChatGPT）。为了提高大型语言模型的研究和开发的可用性，各种指令调整的开源大型语言模型也在最近引入，例如Alpaca和Vicuna等。然而，现有的开源大型语言模型仅仅对英语和一些流行语言进行了指令调整，因此对全球其他语言的影响和可用性很差。在最近几个月内，有一些研究尝试了对大型语言模型进行多语言指令调整，但是这些研究仅仅使用了SFT方法。这种情况留下了一个大的 gap，即RLHF可以如何提高多语言指令调整的性能。为了解决这个问题，我们提出了Okapi，第一个基于RLHF的多语言指令调整系统。Okapi使用了26种多语言的指令和回答排名数据，以便实验和未来多语言大型语言模型的研究。我们还提供了多语言生成模型的评价数据集。我们的实验表明，RLHF在多语言指令调整中比SFT更有优势。我们的框架和资源在https://github.com/nlp-uoregon/Okapi上发布。
</details></li>
</ul>
<hr>
<h2 id="Developing-novel-ligands-with-enhanced-binding-affinity-for-the-sphingosine-1-phosphate-receptor-1-using-machine-learning"><a href="#Developing-novel-ligands-with-enhanced-binding-affinity-for-the-sphingosine-1-phosphate-receptor-1-using-machine-learning" class="headerlink" title="Developing novel ligands with enhanced binding affinity for the sphingosine 1-phosphate receptor 1 using machine learning"></a>Developing novel ligands with enhanced binding affinity for the sphingosine 1-phosphate receptor 1 using machine learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16037">http://arxiv.org/abs/2307.16037</a></li>
<li>repo_url: None</li>
<li>paper_authors: Colin Zhang, Yang Ha</li>
<li>for: 这项研究旨在用机器学习技术加速多发性脑膜炎（MS）的药物发现过程，并通过分析蛋白质-药物交互的化学性质，揭示新的药物发现方法和蛋白质-药物交互的机制。</li>
<li>methods: 该研究使用自适应神经网络模型，将化学式转化为数学向量，生成了超过500个分子变体基于斯普尼莫德（siponimod），并从中选择了25个蛋白质S1PR1的绑定亲和度最高的ligand。</li>
<li>results: 该研究发现了6种有药理性和易合成的蛋白质S1PR1拮抗剂，并通过分析这些拮抗剂的绑定交互，揭示了一些 contribuuting to high binding affinity to S1PR1的化学性质。<details>
<summary>Abstract</summary>
Multiple sclerosis (MS) is a debilitating neurological disease affecting nearly one million people in the United States. Sphingosine-1-phosphate receptor 1, or S1PR1, is a protein target for MS. Siponimod, a ligand of S1PR1, was approved by the FDA in 2019 for MS treatment, but there is a demonstrated need for better therapies. To this end, we finetuned an autoencoder machine learning model that converts chemical formulas into mathematical vectors and generated over 500 molecular variants based on siponimod, out of which 25 compounds had higher predicted binding affinity to S1PR1. The model was able to generate these ligands in just under one hour. Filtering these compounds led to the discovery of six promising candidates with good drug-like properties and ease of synthesis. Furthermore, by analyzing the binding interactions for these ligands, we uncovered several chemical properties that contribute to high binding affinity to S1PR1. This study demonstrates that machine learning can accelerate the drug discovery process and reveal new insights into protein-drug interactions.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="MUSE-Multi-View-Contrastive-Learning-for-Heterophilic-Graphs"><a href="#MUSE-Multi-View-Contrastive-Learning-for-Heterophilic-Graphs" class="headerlink" title="MUSE: Multi-View Contrastive Learning for Heterophilic Graphs"></a>MUSE: Multi-View Contrastive Learning for Heterophilic Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16026">http://arxiv.org/abs/2307.16026</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mengyi Yuan, Minjie Chen, Xiang Li</li>
<li>for: 本文提出了一种多视图对照学习模型（MUSE），用于解决传统Graph Neural Networks（GNN）中的标签依赖和泛化性问题。</li>
<li>methods: 本文使用了两种视图来捕捉 Egonode 和其邻居的信息，通过 GNNs 增强了对照学习，并将两种视图进行集成。在集成过程中，使用了对比学习来增强节点表示的效果。此外，本文还考虑了不同 Egonode 的邻居上下文相互作用的多样性，通过信息融合控制器来模型这种多样性。</li>
<li>results: 本文在 9 个 benchmark 数据集上进行了广泛的实验，结果显示 MUSE 在节点分类和划分任务上具有效果。<details>
<summary>Abstract</summary>
In recent years, self-supervised learning has emerged as a promising approach in addressing the issues of label dependency and poor generalization performance in traditional GNNs. However, existing self-supervised methods have limited effectiveness on heterophilic graphs, due to the homophily assumption that results in similar node representations for connected nodes. In this work, we propose a multi-view contrastive learning model for heterophilic graphs, namely, MUSE. Specifically, we construct two views to capture the information of the ego node and its neighborhood by GNNs enhanced with contrastive learning, respectively. Then we integrate the information from these two views to fuse the node representations. Fusion contrast is utilized to enhance the effectiveness of fused node representations. Further, considering that the influence of neighboring contextual information on information fusion may vary across different ego nodes, we employ an information fusion controller to model the diversity of node-neighborhood similarity at both the local and global levels. Finally, an alternating training scheme is adopted to ensure that unsupervised node representation learning and information fusion controller can mutually reinforce each other. We conduct extensive experiments to evaluate the performance of MUSE on 9 benchmark datasets. Our results show the effectiveness of MUSE on both node classification and clustering tasks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Discrete-neural-nets-and-polymorphic-learning"><a href="#Discrete-neural-nets-and-polymorphic-learning" class="headerlink" title="Discrete neural nets and polymorphic learning"></a>Discrete neural nets and polymorphic learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00677">http://arxiv.org/abs/2308.00677</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/caten2/tripods2021ua">https://github.com/caten2/tripods2021ua</a></li>
<li>paper_authors: Charlotte Aten</li>
<li>for: 这个论文主要是为了研究神经网络的 universal approximation  Results和 classical learning task 之间的关系。</li>
<li>methods: 作者使用了 polymorphisms of relational structures 来 introduce a learning algorithm，并用其进行 classical learning task 的解决。</li>
<li>results: 研究发现，这种 learning algorithm 可以以 polymorphisms of relational structures 来实现 classical learning task 的解决。<details>
<summary>Abstract</summary>
Theorems from universal algebra such as that of Murski\u{i} from the 1970s have a striking similarity to universal approximation results for neural nets along the lines of Cybenko's from the 1980s. We consider here a discrete analogue of the classical notion of a neural net which places these results in a unified setting. We introduce a learning algorithm based on polymorphisms of relational structures and show how to use it for a classical learning task.
</details>
<details>
<summary>摘要</summary>
theorem from universal algebra, such as Murski\u{i}'s from the 1970s, have a striking similarity to universal approximation results for neural networks along the lines of Cybenko's from the 1980s. We consider here a discrete analogue of the classical notion of a neural network, which places these results in a unified setting. We introduce a learning algorithm based on polymorphisms of relational structures and show how to use it for a classical learning task.Note:* "Murski\u{i}" should be translated as "穆尔斯基" (Mù'ěrskī)* "Cybenko" should be translated as "茨本科" (Cíbiānkē)* "polymorphisms" should be translated as "多态" (duōtāi)* "relational structures" should be translated as "关系结构" (guānxì jiégòu)
</details></li>
</ul>
<hr>
<h2 id="Fuzzy-Logic-Visual-Network-FLVN-A-neuro-symbolic-approach-for-visual-features-matching"><a href="#Fuzzy-Logic-Visual-Network-FLVN-A-neuro-symbolic-approach-for-visual-features-matching" class="headerlink" title="Fuzzy Logic Visual Network (FLVN): A neuro-symbolic approach for visual features matching"></a>Fuzzy Logic Visual Network (FLVN): A neuro-symbolic approach for visual features matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16019">http://arxiv.org/abs/2307.16019</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://gitlab.com/grains2/flvn">https://gitlab.com/grains2/flvn</a></li>
<li>paper_authors: Francesco Manigrasso, Lia Morra, Fabrizio Lamberti</li>
<li>for: 提高零shot学习（ZSL）分类的性能</li>
<li>methods: 使用逻辑tensor网络（LTN）把知识表示法与深度神经网络结合，并通过强大的高级假设来增强模型的一致性和稳定性</li>
<li>results: 在Generalized ZSL（GZSL）benchmark AWA2和CUB上达到了状态态的性能，与其他最新的ZSL方法相比，具有较少的计算开销<details>
<summary>Abstract</summary>
Neuro-symbolic integration aims at harnessing the power of symbolic knowledge representation combined with the learning capabilities of deep neural networks. In particular, Logic Tensor Networks (LTNs) allow to incorporate background knowledge in the form of logical axioms by grounding a first order logic language as differentiable operations between real tensors. Yet, few studies have investigated the potential benefits of this approach to improve zero-shot learning (ZSL) classification. In this study, we present the Fuzzy Logic Visual Network (FLVN) that formulates the task of learning a visual-semantic embedding space within a neuro-symbolic LTN framework. FLVN incorporates prior knowledge in the form of class hierarchies (classes and macro-classes) along with robust high-level inductive biases. The latter allow, for instance, to handle exceptions in class-level attributes, and to enforce similarity between images of the same class, preventing premature overfitting to seen classes and improving overall performance. FLVN reaches state of the art performance on the Generalized ZSL (GZSL) benchmarks AWA2 and CUB, improving by 1.3% and 3%, respectively. Overall, it achieves competitive performance to recent ZSL methods with less computational overhead. FLVN is available at https://gitlab.com/grains2/flvn.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/30/cs.LG_2023_07_30/" data-id="cllsiju1w002ba3882i0s0ns1" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_07_30" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/30/cs.SD_2023_07_30/" class="article-date">
  <time datetime="2023-07-29T16:00:00.000Z" itemprop="datePublished">2023-07-30</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/30/cs.SD_2023_07_30/">cs.SD - 2023-07-30 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="HierVST-Hierarchical-Adaptive-Zero-shot-Voice-Style-Transfer"><a href="#HierVST-Hierarchical-Adaptive-Zero-shot-Voice-Style-Transfer" class="headerlink" title="HierVST: Hierarchical Adaptive Zero-shot Voice Style Transfer"></a>HierVST: Hierarchical Adaptive Zero-shot Voice Style Transfer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16171">http://arxiv.org/abs/2307.16171</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sang-Hoon Lee, Ha-Yeong Choi, Hyung-Seok Oh, Seong-Whan Lee</li>
<li>for: 这篇论文目的是提出一个可以在零组件情况下进行声音风格转移的模型 HierVST。</li>
<li>methods: 这个模型使用了层次可变数据构造和自愿构造的方法，并且采用了类别分类和不条件生成技术。</li>
<li>results: 实验结果显示，这个方法在零组件情况下比其他声音风格转移模型表现更好。<details>
<summary>Abstract</summary>
Despite rapid progress in the voice style transfer (VST) field, recent zero-shot VST systems still lack the ability to transfer the voice style of a novel speaker. In this paper, we present HierVST, a hierarchical adaptive end-to-end zero-shot VST model. Without any text transcripts, we only use the speech dataset to train the model by utilizing hierarchical variational inference and self-supervised representation. In addition, we adopt a hierarchical adaptive generator that generates the pitch representation and waveform audio sequentially. Moreover, we utilize unconditional generation to improve the speaker-relative acoustic capacity in the acoustic representation. With a hierarchical adaptive structure, the model can adapt to a novel voice style and convert speech progressively. The experimental results demonstrate that our method outperforms other VST models in zero-shot VST scenarios. Audio samples are available at \url{https://hiervst.github.io/}.
</details>
<details>
<summary>摘要</summary>
尽管voice style transfer（VST）领域的进步较快，现有的零shot VST系统仍然缺乏将新speaker的voice style传递的能力。在这篇论文中，我们提出了层次适应式结束到终端的零shot VST模型，即HierVST。无需文本脚本，我们只使用了语音数据来训练模型，通过层次变量推断和无约束表示。此外，我们采用了层次适应生成器，生成抽象音频和波形声音 sequentially。此外，我们还利用了无条件生成来提高speaker-相对音频表示的能力。通过层次适应结构，模型可以逐渐适应新的voice style，并将语音进行转换。实验结果表明，我们的方法在零shot VST场景下超过了其他VST模型。听音样本可以在 \url{https://hiervst.github.io/} 上找到。
</details></li>
</ul>
<hr>
<h2 id="IroyinSpeech-A-multi-purpose-Yoruba-Speech-Corpus"><a href="#IroyinSpeech-A-multi-purpose-Yoruba-Speech-Corpus" class="headerlink" title="ÌròyìnSpeech: A multi-purpose Yorùbá Speech Corpus"></a>ÌròyìnSpeech: A multi-purpose Yorùbá Speech Corpus</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16071">http://arxiv.org/abs/2307.16071</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tolulope Ogunremi, Kola Tubosun, Anuoluwapo Aremu, Iroro Orife, David Ifeoluwa Adelani</li>
<li>for:  This paper is written for those who are interested in increasing the amount of high-quality, freely available, contemporary Yoruba speech.</li>
<li>methods: The paper uses a multi-purpose dataset that can be used for both TTS and ASR tasks. The dataset is curated from news and creative writing domains under an open license (CC-BY-4.0) and includes recordings from 80 volunteers.</li>
<li>results: The paper provides 5000 utterances to the Common Voice platform for crowdsource transcriptions online, and the dataset includes 38.5 hours of data in total.<details>
<summary>Abstract</summary>
We introduce the \`{I}r\`{o}y\`{i}nSpeech corpus -- a new dataset influenced by a desire to increase the amount of high quality, freely available, contemporary Yor\`{u}b\'{a} speech. We release a multi-purpose dataset that can be used for both TTS and ASR tasks. We curated text sentences from the news and creative writing domains under an open license i.e., CC-BY-4.0 and had multiple speakers record each sentence. We provide 5000 of our utterances to the Common Voice platform to crowdsource transcriptions online. The dataset has 38.5 hours of data in total, recorded by 80 volunteers.
</details>
<details>
<summary>摘要</summary>
我们介绍《IrōyinSpeech》集成 dataset -- 一个新的数据集，受到了提高高质量、开源、当代尤鲁巴语言讲话的愿望的影响。我们发布了多用途的数据集，可以用于 TTS 和 ASR 任务。我们从新闻和创作领域中挑选了 CC-BY-4.0 开源许可证下的文本句子，并请多位说话者录制每句句子。我们将 5000 句录音提供给 Common Voice 平台，以在线受欢迎投票转录。总共有 38.5 小时的数据，由 80 名志愿者录制。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/30/cs.SD_2023_07_30/" data-id="cllsiju2n004pa3889ohacnxf" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_07_30" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/30/eess.IV_2023_07_30/" class="article-date">
  <time datetime="2023-07-29T16:00:00.000Z" itemprop="datePublished">2023-07-30</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/30/eess.IV_2023_07_30/">eess.IV - 2023-07-30 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Unsupervised-Decomposition-Networks-for-Bias-Field-Correction-in-MR-Image"><a href="#Unsupervised-Decomposition-Networks-for-Bias-Field-Correction-in-MR-Image" class="headerlink" title="Unsupervised Decomposition Networks for Bias Field Correction in MR Image"></a>Unsupervised Decomposition Networks for Bias Field Correction in MR Image</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16219">http://arxiv.org/abs/2307.16219</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/leongdong/bias-decomposition-networks">https://github.com/leongdong/bias-decomposition-networks</a></li>
<li>paper_authors: Dong Liang, Xingyu Qiu, Kuanquan Wang, Gongning Luo, Wei Wang, Yashu Liu<br>for:这个论文主要是为了提出一种无监督的拟合网络方法，用于从偏倚图像中分离偏倚场和原始MR图像。methods:这个方法使用了一种新的卷积神经网络模型，其中包括分类部和估计部，两者在训练过程中相互优化。此外，基于多元偏倚场的损失函数也是提出的。results:实验结果表明，该方法可以准确地估计偏倚场并生成更好的偏倚 corrections。代码可以在以下链接上下载：<a target="_blank" rel="noopener" href="https://github.com/LeongDong/Bias-Decomposition-Networks%E3%80%82">https://github.com/LeongDong/Bias-Decomposition-Networks。</a><details>
<summary>Abstract</summary>
Bias field, which is caused by imperfect MR devices or imaged objects, introduces intensity inhomogeneity into MR images and degrades the performance of MR image analysis methods. Many retrospective algorithms were developed to facilitate the bias correction, to which the deep learning-based methods outperformed. However, in the training phase, the supervised deep learning-based methods heavily rely on the synthesized bias field. As the formation of the bias field is extremely complex, it is difficult to mimic the true physical property of MR images by synthesized data. While bias field correction and image segmentation are strongly related, the segmentation map is precisely obtained by decoupling the bias field from the original MR image, and the bias value is indicated by the segmentation map in reverse. Thus, we proposed novel unsupervised decomposition networks that are trained only with biased data to obtain the bias-free MR images. Networks are made up of: a segmentation part to predict the probability of every pixel belonging to each class, and an estimation part to calculate the bias field, which are optimized alternately. Furthermore, loss functions based on the combination of fuzzy clustering and the multiplicative bias field are also devised. The proposed loss functions introduce the smoothness of bias field and construct the soft relationships among different classes under intra-consistency constraints. Extensive experiments demonstrate that the proposed method can accurately estimate bias fields and produce better bias correction results. The code is available on the link: https://github.com/LeongDong/Bias-Decomposition-Networks.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translation::enable_literal扭曲场，由于偏斜MR设备或图像对象的不完美，会导致MR图像中的光度不均，从而降低MR图像分析方法的性能。许多Retrospective算法已经开发来抵消偏斜场，但在训练阶段，深度学习基于方法依赖于Synthesized偏斜场。由于偏斜场的形成非常复杂，难以通过Synthesized数据模拟真实的物理属性。而偏斜场 correction和图像 segmentation 是密切相关的，可以通过分离偏斜场和原始MR图像来获得精确的segmentation map，并且偏斜值可以通过segmentation map反向表示。因此，我们提出了一种新的无监督分解网络，用于从偏斜数据中获得偏斜场free MR图像。网络由以下两个部分组成：分类部分用于预测每个像素属于哪个类别的概率，以及估计部分用于计算偏斜场，这两个部分被 alternate 优化。此外，我们还提出了基于多项式偏斜场和杂化 clustering 的损失函数。这些损失函数引入了偏斜场的平滑性和不同类别之间的软连接，通过内部一致性约束。广泛的实验表明，提出的方法可以准确地估计偏斜场并生成更好的偏斜 correction 结果。代码可以在以下链接获取：https://github.com/LeongDong/Bias-Decomposition-Networks。
</details></li>
</ul>
<hr>
<h2 id="Gastrointestinal-Mucosal-Problems-Classification-with-Deep-Learning"><a href="#Gastrointestinal-Mucosal-Problems-Classification-with-Deep-Learning" class="headerlink" title="Gastrointestinal Mucosal Problems Classification with Deep Learning"></a>Gastrointestinal Mucosal Problems Classification with Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16198">http://arxiv.org/abs/2307.16198</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammadhasan Goharian, Vahid Goharian, Hamidreza Bolhasani</li>
<li>for: 这篇论文主要是研究用于识别肠胃膜变化，以便早期诊断和预防肠癌。</li>
<li>methods: 该论文使用了深度学习技术，特别是基于 transferred learning（TL）的卷积神经网络（CNNs），对肠胃膜变化进行预测。</li>
<li>results: 研究发现，使用 Transfer Learning 基于 CNNs 的模型可以达到93%的准确率，并在实际的Endoscopy和Colonoscopy影像中进行了应用和验证。<details>
<summary>Abstract</summary>
Gastrointestinal mucosal changes can cause cancers after some years and early diagnosing them can be very useful to prevent cancers and early treatment. In this article, 8 classes of mucosal changes and anatomical landmarks including Polyps, Ulcerative Colitis, Esophagitis, Normal Z-Line, Normal Pylorus, Normal Cecum, Dyed Lifted Polyps, and Dyed Lifted Margin were predicted by deep learning. We used neural networks in this article. It is a black box artificial intelligence algorithm that works like a human neural system. In this article, Transfer Learning (TL) based on the Convolutional Neural Networks (CNNs), which is one of the well-known types of neural networks in image processing is used. We compared some famous CNN architecture including VGG, Inception, Xception, and ResNet. Our best model got 93% accuracy in test images. At last, we used our model in some real endoscopy and colonoscopy movies to classify problems.
</details>
<details>
<summary>摘要</summary>
胃肠膜变化可能导致癌症，早期诊断可以有助于预防癌症和早期治疗。在这篇文章中，我们预测了8种胃肠膜变化和解剖特征，包括��ccolets、急性coliitis、食管炎、正常Z-线、正常pylorus、正常cecum、染料提取polyps和染料提取边缘。我们使用了神经网络来实现这一点。神经网络是一种人工智能算法，它工作如人脑神经系统一样。我们使用了传输学习（TL），基于Convolutional Neural Networks（CNNs），这是一种广泛应用于图像处理中的知名神经网络类型。我们比较了一些知名的CNN架构，包括VGG、Inception、Xception和ResNet。我们的最佳模型在测试图像中达到93%的准确率。最后，我们使用了我们的模型在实际末端摄影和colonoscopy视频中分类问题。
</details></li>
</ul>
<hr>
<h2 id="StarSRGAN-Improving-Real-World-Blind-Super-Resolution"><a href="#StarSRGAN-Improving-Real-World-Blind-Super-Resolution" class="headerlink" title="StarSRGAN: Improving Real-World Blind Super-Resolution"></a>StarSRGAN: Improving Real-World Blind Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16169">http://arxiv.org/abs/2307.16169</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kynthesis/StarSRGAN">https://github.com/kynthesis/StarSRGAN</a></li>
<li>paper_authors: Khoa D. Vo, Len T. Bui</li>
<li>for: 这个研究旨在提高电脑视觉中的照片分辨率，并且不需要知道变低分辨率的过程。</li>
<li>methods: 这个研究使用了5种不同的架构，包括Real-ESRGAN模型，以提高照片的分辨率。</li>
<li>results: 实验比较结果显示，StarSRGAN可以与Real-ESRGAN实现相似的分辨率，并且具有约10%更高的MANINA和AHIQ评分。此外，StarSRGAN Lite版本可以在实时上调高分辨率，并且可以保持约90%的图像质量。<details>
<summary>Abstract</summary>
The aim of blind super-resolution (SR) in computer vision is to improve the resolution of an image without prior knowledge of the degradation process that caused the image to be low-resolution. The State of the Art (SOTA) model Real-ESRGAN has advanced perceptual loss and produced visually compelling outcomes using more complex degradation models to simulate real-world degradations. However, there is still room to improve the super-resolved quality of Real-ESRGAN by implementing recent techniques. This research paper introduces StarSRGAN, a novel GAN model designed for blind super-resolution tasks that utilize 5 various architectures. Our model provides new SOTA performance with roughly 10% better on the MANIQA and AHIQ measures, as demonstrated by experimental comparisons with Real-ESRGAN. In addition, as a compact version, StarSRGAN Lite provides approximately 7.5 times faster reconstruction speed (real-time upsampling from 540p to 4K) but can still keep nearly 90% of image quality, thereby facilitating the development of a real-time SR experience for future research. Our codes are released at https://github.com/kynthesis/StarSRGAN.
</details>
<details>
<summary>摘要</summary>
文本：隐式超分辨率（SR）的计算机视觉目标是提高图像的分辨率，不具备图像压缩过程的先前知识。现有最佳实践（SOTA）模型Real-ESRGAN已经提出了更加复杂的压缩模型来模拟实际世界中的压缩。然而，仍然有很大的提高超分辨质量的空间。这篇研究论文介绍了StarSRGAN，一种新的GAN模型，用于隐式SR任务。我们的模型使用了5种不同的建筑，并提供了新的SOTA性能，在MANIQA和AHIQ测试中比Real-ESRGAN提高约10%。此外，StarSRGAN Lite版本可以在实时upsampling从540p到4K的过程中提高速度约7.5倍，仍然可以保持图像质量的90%，因此可以促进未来研究中的实时SR经验的发展。我们的代码在https://github.com/kynthesis/StarSRGAN中发布。翻译：目标：隐式超分辨率（SR）在计算机视觉中的目标是提高图像的分辨率，不具备图像压缩过程的先前知识。现有最佳实践（SOTA）模型Real-ESRGAN已经提出了更加复杂的压缩模型来模拟实际世界中的压缩。然而，仍然有很大的提高超分辨质量的空间。我们在这篇研究论文中介绍了StarSRGAN，一种新的GAN模型，用于隐式SR任务。我们的模型使用了5种不同的建筑，并提供了新的SOTA性能，在MANIQA和AHIQ测试中比Real-ESRGAN提高约10%。此外，StarSRGAN Lite版本可以在实时upsampling从540p到4K的过程中提高速度约7.5倍，仍然可以保持图像质量的90%，因此可以促进未来研究中的实时SR经验的发展。我们的代码在https://github.com/kynthesis/StarSRGAN中发布。
</details></li>
</ul>
<hr>
<h2 id="Structure-Preserving-Synthesis-MaskGAN-for-Unpaired-MR-CT-Translation"><a href="#Structure-Preserving-Synthesis-MaskGAN-for-Unpaired-MR-CT-Translation" class="headerlink" title="Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation"></a>Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16143">http://arxiv.org/abs/2307.16143</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/HieuPhan33/MaskGAN">https://github.com/HieuPhan33/MaskGAN</a></li>
<li>paper_authors: Minh Hieu Phan, Zhibin Liao, Johan W. Verjans, Minh-Son To</li>
<li>for: 这篇论文旨在解决医疗影像合成中的欠缺联系数据问题，使用CyclingGAN来利用无相关数据，但是这些方法通常会导致不准确的映射，导致影像的推导过程中的骨架结构被损坏。</li>
<li>methods: 这篇论文提出了一个名为MaskGAN的新的框架，它利用自动提取的粗糙Mask来确保结构一致性，并使用内容生成器来实现CT内容的生成。</li>
<li>results: 实验结果显示，MaskGAN在一个儿童 dataset 上表现出色，与现有的合成方法相比，MaskGAN 能够更好地保留骨架结构，并且不需要专家的标注。<details>
<summary>Abstract</summary>
Medical image synthesis is a challenging task due to the scarcity of paired data. Several methods have applied CycleGAN to leverage unpaired data, but they often generate inaccurate mappings that shift the anatomy. This problem is further exacerbated when the images from the source and target modalities are heavily misaligned. Recently, current methods have aimed to address this issue by incorporating a supplementary segmentation network. Unfortunately, this strategy requires costly and time-consuming pixel-level annotations. To overcome this problem, this paper proposes MaskGAN, a novel and cost-effective framework that enforces structural consistency by utilizing automatically extracted coarse masks. Our approach employs a mask generator to outline anatomical structures and a content generator to synthesize CT contents that align with these structures. Extensive experiments demonstrate that MaskGAN outperforms state-of-the-art synthesis methods on a challenging pediatric dataset, where MR and CT scans are heavily misaligned due to rapid growth in children. Specifically, MaskGAN excels in preserving anatomical structures without the need for expert annotations. The code for this paper can be found at https://github.com/HieuPhan33/MaskGAN.
</details>
<details>
<summary>摘要</summary>
医学图像生成是一项具有挑战性的任务，主要由于缺乏匹配数据。许多方法尝试使用CycleGAN将无匹配数据利用，但它们通常生成不准确的映射，导致身体结构的偏移。这个问题更加严重当图像来源和目标模式都是极度不一致的时候。现在，当前的方法尝试通过添加辅助分割网络来解决这个问题，但这种策略需要昂贵的和时间consuming的像素级注解。为了解决这个问题，本文提出了MaskGAN，一种新的和cost-effective的框架，它通过自动提取的大致mask来保持结构一致性。我们的方法使用mask生成器将解剖结构轮廓出来，并使用内容生成器将CT内容与这些结构相关。我们的实验表明，MaskGAN在一个具有挑战性的педиатric dataset上表现出色，其中MR和CT扫描是由儿童快速增长而导致的极度不一致。具体来说，MaskGAN能够保持解剖结构，不需要专家注解。相关代码可以在https://github.com/HieuPhan33/MaskGAN中找到。
</details></li>
</ul>
<hr>
<h2 id="Implicit-Neural-Representation-in-Medical-Imaging-A-Comparative-Survey"><a href="#Implicit-Neural-Representation-in-Medical-Imaging-A-Comparative-Survey" class="headerlink" title="Implicit Neural Representation in Medical Imaging: A Comparative Survey"></a>Implicit Neural Representation in Medical Imaging: A Comparative Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16142">http://arxiv.org/abs/2307.16142</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mindflow-institue/awesome-implicit-neural-representations-in-medical-imaging">https://github.com/mindflow-institue/awesome-implicit-neural-representations-in-medical-imaging</a></li>
<li>paper_authors: Amirali Molaei, Amirhossein Aminimehr, Armin Tavakoli, Amirhossein Kazerouni, Bobby Azad, Reza Azad, Dorit Merhof</li>
<li>for: 本文提供了一个全面的审视，探讨了在医疗图像分析领域中使用隐藏神经表示（INR）模型的可能性。</li>
<li>methods: 本文使用了INR模型来解决各种医疗图像分析任务，包括图像重建、分割、注册、新视角生成和压缩。</li>
<li>results: 本文总结了INR模型在医疗图像分析领域的优势和局限性，以及其在不同任务中的应用可能性。<details>
<summary>Abstract</summary>
Implicit neural representations (INRs) have gained prominence as a powerful paradigm in scene reconstruction and computer graphics, demonstrating remarkable results. By utilizing neural networks to parameterize data through implicit continuous functions, INRs offer several benefits. Recognizing the potential of INRs beyond these domains, this survey aims to provide a comprehensive overview of INR models in the field of medical imaging. In medical settings, numerous challenging and ill-posed problems exist, making INRs an attractive solution. The survey explores the application of INRs in various medical imaging tasks, such as image reconstruction, segmentation, registration, novel view synthesis, and compression. It discusses the advantages and limitations of INRs, highlighting their resolution-agnostic nature, memory efficiency, ability to avoid locality biases, and differentiability, enabling adaptation to different tasks. Furthermore, the survey addresses the challenges and considerations specific to medical imaging data, such as data availability, computational complexity, and dynamic clinical scene analysis. It also identifies future research directions and opportunities, including integration with multi-modal imaging, real-time and interactive systems, and domain adaptation for clinical decision support. To facilitate further exploration and implementation of INRs in medical image analysis, we have provided a compilation of cited studies along with their available open-source implementations on \href{https://github.com/mindflow-institue/Awesome-Implicit-Neural-Representations-in-Medical-imaging}. Finally, we aim to consistently incorporate the most recent and relevant papers regularly.
</details>
<details>
<summary>摘要</summary>
启发性神经表示（INR）在场景重建和计算机图形领域已经得到广泛应用，显示了惊人的成果。通过使用神经网络参数化数据 mediante implicit continuous functions，INRs 提供了多种优点。认识到 INRs 在医疗领域的潜力，这篇评论旨在为医疗影像分析领域中 INR 模型的全面概述。在医疗设置中，存在许多困难和不确定的问题，使得 INRs 成为一种有appeal的解决方案。评论 explore INRs 在各种医疗影像任务中的应用，如图像重建、分割、注册、新视图生成和压缩。它讨论了 INRs 的优点和局限性，包括其无关分辨率的 natura, 内存有效性、避免本地偏好和可导 differentiability，以及其适应不同任务的能力。此外，评论还讨论了医疗影像数据特有的挑战和考虑因素，如数据可用性、计算复杂度和动态临床场景分析。 finally, 评论 indentify 未来研究方向和机会，包括与多 modal 影像集成、实时交互系统和适应医疗决策支持。为便于进一步探索和应用 INRs 在医疗影像分析中，我们提供了一份 cited studies 和其可用的开源实现，可以在 \href{https://github.com/mindflow-institue/Awesome-Implicit-Neural-Representations-in-Medical-imaging} 中找到。最后，我们计划不断 incorporate 最新和最相关的论文，以便保持评论的有效性和相关性。
</details></li>
</ul>
<hr>
<h2 id="RIS-Enhanced-Semantic-Communications-Adaptive-to-User-Requirements"><a href="#RIS-Enhanced-Semantic-Communications-Adaptive-to-User-Requirements" class="headerlink" title="RIS-Enhanced Semantic Communications Adaptive to User Requirements"></a>RIS-Enhanced Semantic Communications Adaptive to User Requirements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16100">http://arxiv.org/abs/2307.16100</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peiwen Jiang, Chao-Kai Wen, Shi Jin, Geoffrey Ye Li<br>for:这个论文是为了提出一种基于智能表面的含义通信方法，以满足不同的用户需求和环境条件。methods:这个方法使用智能表面分配含义内容，并考虑用户运动和视线干扰，以保护重要的含义在困难的通信条件下。results:实验结果表明，该方法可以在多种通信条件下实现可靠的任务性能，但在严重的通信条件下，一些含义部分可能会被损坏。为此，提出了一种重建方法，以提高视觉接受度。此外，该方法还可以有效地调整智能表面资源，以实现多用户共享和有效的资源分配。<details>
<summary>Abstract</summary>
Semantic communication significantly reduces required bandwidth by understanding semantic meaning of the transmitted. However, current deep learning-based semantic communication methods rely on joint source-channel coding design and end-to-end training, which limits their adaptability to new physical channels and user requirements. Reconfigurable intelligent surfaces (RIS) offer a solution by customizing channels in different environments. In this study, we propose the RIS-SC framework, which allocates semantic contents with varying levels of RIS assistance to satisfy the changing user requirements. It takes into account user movement and line-of-sight obstructions, enabling the RIS resource to protect important semantics in challenging channel conditions. The simulation results indicate reasonable task performance, but some semantic parts that have no effect on task performances are abandoned under severe channel conditions. To address this issue, a reconstruction method is also introduced to improve visual acceptance by inferring those missing semantic parts. Furthermore, the framework can adjust RIS resources in friendly channel conditions to save and allocate them efficiently among multiple users. Simulation results demonstrate the adaptability and efficiency of the RIS-SC framework across diverse channel conditions and user requirements.
</details>
<details>
<summary>摘要</summary>
semantic communication 可以减少需要的带宽，因为它理解传输的 semantics 含义。但是，当前的深度学习基于 semantic communication 方法都是通过共同源-通道编码设计和端到端训练，这限制了它们在新的物理通道和用户需求中的适应性。智能表面重配置 (RIS) 提供了一个解决方案，通过自定义不同环境中的通道来满足用户的变化需求。本研究提出了 RIS-SC 框架，它根据用户的运动和视线干扰，对重要的 semantics 进行了保护，并允许 RIS 资源在具有挑战性通道条件下进行有效地分配。实验结果表明，任务性能是可理解的，但在严重的通道条件下，一些无关任务性能的 semantics 会被遗弃。为解决这个问题，我们还提出了一种重建方法，可以通过推理来恢复这些缺失的 semantics。此外，框架还可以在友好的通道条件下调整 RIS 资源，以有效地分配它们 Among multiple users。实验结果表明 RIS-SC 框架在多样化的通道条件和用户需求下具有适应性和效率。
</details></li>
</ul>
<hr>
<h2 id="A-New-Multi-Level-Hazy-Image-and-Video-Dataset-for-Benchmark-of-Dehazing-Methods"><a href="#A-New-Multi-Level-Hazy-Image-and-Video-Dataset-for-Benchmark-of-Dehazing-Methods" class="headerlink" title="A New Multi-Level Hazy Image and Video Dataset for Benchmark of Dehazing Methods"></a>A New Multi-Level Hazy Image and Video Dataset for Benchmark of Dehazing Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16050">http://arxiv.org/abs/2307.16050</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bedrettin Cetinkaya, Yucel Cimtay, Fatma Nazli Gunay, Gokce Nur Yilmaz</li>
<li>for: 本研究准备了一个新的多级雾化彩色图像数据集，以便比较不同的雾化方法和模型在这个数据集上的表现。</li>
<li>methods: 本研究使用了五种不同的雾化方法&#x2F;模型，包括传统方法和深度学习方法，并对其在数据集上的表现进行比较。</li>
<li>results: 研究结果显示，传统方法在不同场景下能够更好地普适化雾化问题，而深度学习方法在跨数据集雾化 Task 中表现较差。<details>
<summary>Abstract</summary>
The changing level of haze is one of the main factors which affects the success of the proposed dehazing methods. However, there is a lack of controlled multi-level hazy dataset in the literature. Therefore, in this study, a new multi-level hazy color image dataset is presented. Color video data is captured for two real scenes with a controlled level of haze. The distance of the scene objects from the camera, haze level, and ground truth (clear image) are available so that different dehazing methods and models can be benchmarked. In this study, the dehazing performance of five different dehazing methods/models is compared on the dataset based on SSIM, PSNR, VSI and DISTS image quality metrics. Results show that traditional methods can generalize the dehazing problem better than many deep learning based methods. The performance of deep models depends mostly on the scene and is generally poor on cross-dataset dehazing.
</details>
<details>
<summary>摘要</summary>
气凝度的变化是提议的抑霾方法的成功因素之一，但在文献中缺乏控制多级霾化数据集。因此，本研究提供了一个新的多级霾化彩色图像数据集。通过捕捉两个真实场景的颜色视频数据，实现控制霾度水平。场景对象与摄像头的距离、霾度水平和明确图像（清晰图像）的信息都可以用，以便不同的抑霾方法和模型进行比较。本研究根据SSIM、PSNR、VSI和DISTS图像质量指标对五种不同的抑霾方法/模型进行比较。结果显示，传统方法在不同场景下能更好地泛化抑霾问题，而深度学习基于方法在cross-dataset抑霾中表现较差。
</details></li>
</ul>
<hr>
<h2 id="CoVid-19-Detection-leveraging-Vision-Transformers-and-Explainable-AI"><a href="#CoVid-19-Detection-leveraging-Vision-Transformers-and-Explainable-AI" class="headerlink" title="CoVid-19 Detection leveraging Vision Transformers and Explainable AI"></a>CoVid-19 Detection leveraging Vision Transformers and Explainable AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16033">http://arxiv.org/abs/2307.16033</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pangoth Santhosh Kumar, Kundrapu Supriya, Mallikharjuna Rao K<br>for: The paper is written for the purpose of developing an end-to-end framework using vision transformers for the diagnosis of lung diseases, specifically pneumonia, Covid 19, and lung opacity.methods: The paper uses a specialized Compact Convolution Transformers (CCT) model, which is trained and evaluated on datasets such as the Covid 19 Radiography Database.results: The CCT model achieves better accuracy for both training and validation purposes on the Covid 19 Radiography Database, demonstrating its effectiveness in detecting lung diseases.Here’s the simplified Chinese text format you requested:for: 这篇论文的目的是为了开发一种基于视transformer的终端框架，用于诊断肺病，具体来说是识别 Covid 19、肺病和肺混浊等病种。methods: 这篇论文使用了一种特殊的 Compact Convolution Transformers (CCT) 模型，通过对 Covid 19 Radiography Database 等数据集进行训练和评估。results: CCT 模型在 Covid 19 Radiography Database 上的训练和验证目标上具有更高的准确率，表明其在诊断肺病方面的效果。<details>
<summary>Abstract</summary>
Lung disease is a common health problem in many parts of the world. It is a significant risk to people health and quality of life all across the globe since it is responsible for five of the top thirty leading causes of death. Among them are COVID 19, pneumonia, and tuberculosis, to name just a few. It is critical to diagnose lung diseases in their early stages. Several different models including machine learning and image processing have been developed for this purpose. The earlier a condition is diagnosed, the better the patient chances of making a full recovery and surviving into the long term. Thanks to deep learning algorithms, there is significant promise for the autonomous, rapid, and accurate identification of lung diseases based on medical imaging. Several different deep learning strategies, including convolutional neural networks (CNN), vanilla neural networks, visual geometry group based networks (VGG), and capsule networks , are used for the goal of making lung disease forecasts. The standard CNN has a poor performance when dealing with rotated, tilted, or other aberrant picture orientations. As a result of this, within the scope of this study, we have suggested a vision transformer based approach end to end framework for the diagnosis of lung disorders. In the architecture, data augmentation, training of the suggested models, and evaluation of the models are all included. For the purpose of detecting lung diseases such as pneumonia, Covid 19, lung opacity, and others, a specialised Compact Convolution Transformers (CCT) model have been tested and evaluated on datasets such as the Covid 19 Radiography Database. The model has achieved a better accuracy for both its training and validation purposes on the Covid 19 Radiography Database.
</details>
<details>
<summary>摘要</summary>
肺病是全球许多地区的常见健康问题。它对人类健康和生活质量产生了重大风险，因为它负责全球前三十名死亡原因之一的五种疾病。这些疾病包括 COVID-19、肺炎和 tuberkulose 等。EARLY DIAGNOSIS 是关键，因为根据医学影像识别肺病的早期可以提高病人恢复的可能性和长期生存的可能性。感谢深度学习算法，肺病的自动识别已经有了显著的前景。这些深度学习策略包括卷积神经网络（CNN）、普通神经网络、视 geometry group 基于网络（VGG）和卷积神经网络（Capsule Networks）。然而，标准的 CNN 在不正常的图像方向下表现不佳，因此在这种研究范围内，我们建议使用视transformer 基础的终端框架进行肺病诊断。在架构、数据增强、模型训练和模型评估方面都包括了具体的实现。为了检测肺病如肺炎、 COVID-19、肺透明度等，我们在 Covid 19 成像数据库上测试了一种专门的 Compact Convolution Transformers（CCT）模型。该模型在训练和验证目的上在 Covid 19 成像数据库上达到了更高的准确率。
</details></li>
</ul>
<hr>
<h2 id="LOTUS-Learning-to-Optimize-Task-based-US-representations"><a href="#LOTUS-Learning-to-Optimize-Task-based-US-representations" class="headerlink" title="LOTUS: Learning to Optimize Task-based US representations"></a>LOTUS: Learning to Optimize Task-based US representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16021">http://arxiv.org/abs/2307.16021</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yordanka Velikova, Mohammad Farid Azampour, Walter Simson, Vanessa Gonzalez Duque, Nassir Navab</li>
<li>for: 这个研究旨在提高ultrasound图像中的器官分类，以便应用于诊断和监控。</li>
<li>methods: 本研究使用了一种新的方法，即使用CT扫描来生成适当的ultrasound图像，并通过射线投影来模拟对质料的传播。</li>
<li>results: 研究获得了一些有 promise的量值结果，并且在调整过的图像上进行了质感评估。<details>
<summary>Abstract</summary>
Anatomical segmentation of organs in ultrasound images is essential to many clinical applications, particularly for diagnosis and monitoring. Existing deep neural networks require a large amount of labeled data for training in order to achieve clinically acceptable performance. Yet, in ultrasound, due to characteristic properties such as speckle and clutter, it is challenging to obtain accurate segmentation boundaries, and precise pixel-wise labeling of images is highly dependent on the expertise of physicians. In contrast, CT scans have higher resolution and improved contrast, easing organ identification. In this paper, we propose a novel approach for learning to optimize task-based ultra-sound image representations. Given annotated CT segmentation maps as a simulation medium, we model acoustic propagation through tissue via ray-casting to generate ultrasound training data. Our ultrasound simulator is fully differentiable and learns to optimize the parameters for generating physics-based ultrasound images guided by the downstream segmentation task. In addition, we train an image adaptation network between real and simulated images to achieve simultaneous image synthesis and automatic segmentation on US images in an end-to-end training setting. The proposed method is evaluated on aorta and vessel segmentation tasks and shows promising quantitative results. Furthermore, we also conduct qualitative results of optimized image representations on other organs.
</details>
<details>
<summary>摘要</summary>
医学应用中的组织部分分割是非常重要的，特别是诊断和监测。现有的深入神经网络需要大量标注数据来进行训练，以达到клиничеacceptable的性能。然而，在ultrasound中，由特征性质如斑点和噪声而导致的分割边界很难获取，并且专业医生们对图像进行精确的像素化标注受限。相比之下，CT扫描有更高的分辨率和更好的对比，使器官识别更容易。在这篇论文中，我们提出了一种新的方法，用于学习优化任务基于ultrasound图像表示。通过模拟射线折射来模拟声波的传播，我们生成了基于物理的ultrasound训练数据。我们的ultrasound simulator是完全可微分的，可以学习优化参数，以便通过下游分割任务来导引物理基于ultrasound图像的生成。此外，我们还训练了一种图像适应网络，以实现同时的图像合成和自动分割在US图像上。我们的提posed方法在AAA和血管分割任务中表现出了有力的量化结果。此外，我们还进行了其他器官的优化图像表示的质量分析。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/30/eess.IV_2023_07_30/" data-id="cllsiju3w008ja388cjwu9die" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/5/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><a class="page-number" href="/page/5/">5</a><span class="page-number current">6</span><a class="page-number" href="/page/7/">7</a><a class="page-number" href="/page/8/">8</a><span class="space">&hellip;</span><a class="page-number" href="/page/17/">17</a><a class="extend next" rel="next" href="/page/7/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CR/">cs.CR</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">43</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">42</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">44</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">53</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">114</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'', root:''}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
