
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/6/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-eess.IV_2023_08_19" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/19/eess.IV_2023_08_19/" class="article-date">
  <time datetime="2023-08-18T16:00:00.000Z" itemprop="datePublished">2023-08-19</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/19/eess.IV_2023_08_19/">eess.IV - 2023-08-19 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="CRC-ICM-Colorectal-Cancer-Immune-Cell-Markers-Pattern-Dataset"><a href="#CRC-ICM-Colorectal-Cancer-Immune-Cell-Markers-Pattern-Dataset" class="headerlink" title="CRC-ICM: Colorectal Cancer Immune Cell Markers Pattern Dataset"></a>CRC-ICM: Colorectal Cancer Immune Cell Markers Pattern Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10033">http://arxiv.org/abs/2308.10033</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zahra Mokhtari, Elham Amjadi, Hamidreza Bolhasani, Zahra Faghih, AmirReza Dehghanian, Marzieh Rezaei</li>
<li>For: The paper is focused on the immune checkpoints in the tumor microenvironment of colorectal cancer (CRC) and their potential as therapeutic targets for cancer treatment.* Methods: The paper uses a dataset of 1756 images from 136 patients with CRC, stained with specific antibodies for CD3, CD8, CD45RO, PD-1, LAG3, and Tim3 to investigate the expression of immune checkpoints in the tumor microenvironment.* Results: The paper reports on the differences in immune checkpoint expression between tumors located in the right and left sides of the colon, and the potential implications of these differences for the prognosis of CRC patients.Here is the information in Simplified Chinese text:</li>
<li>for: 这篇论文关注了大肠癌（CRC）的免疫检查点在肿瘤微环境中的表达，以及这些检查点作为癌症治疗的可能性。</li>
<li>methods: 这篇论文使用了136名CRC患者的1756个图像，用具体抗体检测CD3、CD8、CD45RO、PD-1、LAG3和Tim3等免疫检查点的表达。</li>
<li>results: 这篇论文报告了右侧和左侧大肠的肿瘤表达免疫检查点之间的差异，以及这些差异对CRC患者的预后的可能影响。<details>
<summary>Abstract</summary>
Colorectal Cancer (CRC) is the second most common cause of cancer death in the world, ad can be identified by the location of the primary tumor in the large intestine: right and left colon, and rectum. Based on the location, CRC shows differences in chromosomal and molecular characteristics, microbiomes incidence, pathogenesis, and outcome. It has been shown that tumors on left and right sides also have different immune landscape, so the prognosis may be different based on the primary tumor locations. It is widely accepted that immune components of the tumor microenvironment (TME) plays a critical role in tumor development. One of the critical regulatory molecules in the TME is immune checkpoints that as the gatekeepers of immune responses regulate the infiltrated immune cell functions. Inhibitory immune checkpoints such as PD-1, Tim3, and LAG3, as the main mechanism of immune suppression in TME overexpressed and result in further development of the tumor. The images of this dataset have been taken from colon tissues of patients with CRC, stained with specific antibodies for CD3, CD8, CD45RO, PD-1, LAG3 and Tim3. The name of this dataset is CRC-ICM and contains 1756 images related to 136 patients. The initial version of CRC-ICM is published on Elsevier Mendeley dataset portal, and the latest version is accessible via: https://databiox.com
</details>
<details>
<summary>摘要</summary>
便膜癌（CRC）是全球第二常见的癌病原因，可以根据主 tumor 的位置在大肠分为右和左边colon，以及肛门。根据位置，CRC 会出现不同的染色体和分子特征、微生物发生率、生物学发展和结果。已经证明左右两边的肿瘤也有不同的免疫景况，因此预后可能因主 tumor 的位置而异。免疫组件在肿瘤微环境中（TME）plays a critical role in tumor development。一种critical regulatory molecules in TME 是免疫检查点，它们作为免疫 responses 的门槛，调控入侵的免疫细胞功能。压缩性免疫检查点，如PD-1、Tim3和LAG3，是免疫抑制的主要机制，它们在TME 过剩表达，导致肿瘤的进一步发展。这些图像来自于colon这些癌病患者的患者，这些图像用specific抗体CD3、CD8、CD45RO、PD-1、LAG3和Tim3染色。这个 dataset 的名称是CRC-ICM，它包含 1756 个图像，与 136 名病患相关。初版CRC-ICM 已经发表在Elsevier Mendeley 数据集Portail，而最新版本可以通过以下连结获取：https://databiox.com。
</details></li>
</ul>
<hr>
<h2 id="Deformable-Detection-Transformer-for-Microbubble-Localization-in-Ultrasound-Localization-Microscopy"><a href="#Deformable-Detection-Transformer-for-Microbubble-Localization-in-Ultrasound-Localization-Microscopy" class="headerlink" title="Deformable-Detection Transformer for Microbubble Localization in Ultrasound Localization Microscopy"></a>Deformable-Detection Transformer for Microbubble Localization in Ultrasound Localization Microscopy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09845">http://arxiv.org/abs/2308.09845</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sepideh K. Gharamaleki, Brandon Helfield, Hassan Rivaz</li>
<li>for: 提高ultrasound imaging中半波长解像力的问题</li>
<li>methods: 使用DEformable DETR（DE-DETR）方法，带有多级弹性注意力分配，从而提高精度和准确性</li>
<li>results: 对比DETR方法，DE-DETR方法在精度和准确性方面均显示了改善，并且在最终超分解图像中也得到了更好的效果<details>
<summary>Abstract</summary>
To overcome the half a wavelength resolution limitations of ultrasound imaging, microbubbles (MBs) have been utilized widely in the field. Conventional MB localization methods are limited whether by exhaustive parameter tuning or considering a fixed Point Spread Function (PSF) for MBs. This questions their adaptability to different imaging settings or depths. As a result, development of methods that don't rely on manually adjusted parameters is crucial. Previously, we used a transformer-based approach i.e. DEtection TRansformer (DETR) (arXiv:2005.12872v3 and arXiv:2209.11859v1) to address the above mentioned issues. However, DETR suffers from long training times and lower precision for smaller objects. In this paper, we propose the application of DEformable DETR (DE-DETR) ( arXiv:2010.04159) for MB localization to mitigate DETR's above mentioned challenges. As opposed to DETR, where attention is casted upon all grid pixels, DE-DETR utilizes a multi-scale deformable attention to distribute attention within a limited budget. To evaluate the proposed strategy, pre-trained DE-DETR was fine-tuned on a subset of the dataset provided by the IEEE IUS Ultra-SR challenge organizers using transfer learning principles and subsequently we tested the network on the rest of the dataset, excluding the highly correlated frames. The results manifest an improvement both in precision and recall and the final super-resolution maps compared to DETR.
</details>
<details>
<summary>摘要</summary>
通过超过半波长的限制， ultrasound 影像中的微ubble (MB) 已经广泛应用于领域中。传统的 MB 位置方法受限于手动调整的参数或者假设固定的点扩散函数 (PSF)  для MBs。这问题了它们在不同的描述设置或深度下的适应性。因此，开发不依赖于手动调整参数的方法是关键。在先前的研究中，我们使用 transformer 基本的方法，即检测转换器 (DETR) （arXiv: 2005.12872v3 和 arXiv: 2209.11859v1）来解决上述问题。然而， DETR 受到训练时间过长和对小对象的精度较低的问题。在这篇论文中，我们提议将 DEformable DETR (DE-DETR) （arXiv: 2010.04159）应用于 MB 位置检测，以减少 DETR 的上述挑战。与 DETR 不同，DE-DETR 使用多尺度可变的注意力来分配注意力，而不是将注意力投射到所有的格子像素上。为评估提议的策略，我们先使用 transfer learning 原则来精心 DE-DETR ，然后在数据集中测试网络，排除高相关性的帧。结果表明，提议的策略在精度和准确性方面得到改进，并且在最终的超解像图中比 DETR 更好。
</details></li>
</ul>
<hr>
<h2 id="Cross-modality-Attention-based-Multimodal-Fusion-for-Non-small-Cell-Lung-Cancer-NSCLC-Patient-Survival-Prediction"><a href="#Cross-modality-Attention-based-Multimodal-Fusion-for-Non-small-Cell-Lung-Cancer-NSCLC-Patient-Survival-Prediction" class="headerlink" title="Cross-modality Attention-based Multimodal Fusion for Non-small Cell Lung Cancer (NSCLC) Patient Survival Prediction"></a>Cross-modality Attention-based Multimodal Fusion for Non-small Cell Lung Cancer (NSCLC) Patient Survival Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09831">http://arxiv.org/abs/2308.09831</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruining Deng, Nazim Shaikh, Gareth Shannon, Yao Nie</li>
<li>for: 预测和诊断结果提高，尤其是诊断非小细胞肺癌（NSCLC）患者的存活情况。</li>
<li>methods: 跨模态注意力基本的多模态融合策略，通过评估不同模式之间的关系来混合特征。</li>
<li>results: 与单模式相比，提出的融合方法在实验中实现了c-指数0.6587，表明可以充分利用不同模式之间的知识，提高诊断和预测的准确性。<details>
<summary>Abstract</summary>
Cancer prognosis and survival outcome predictions are crucial for therapeutic response estimation and for stratifying patients into various treatment groups. Medical domains concerned with cancer prognosis are abundant with multiple modalities, including pathological image data and non-image data such as genomic information. To date, multimodal learning has shown potential to enhance clinical prediction model performance by extracting and aggregating information from different modalities of the same subject. This approach could outperform single modality learning, thus improving computer-aided diagnosis and prognosis in numerous medical applications. In this work, we propose a cross-modality attention-based multimodal fusion pipeline designed to integrate modality-specific knowledge for patient survival prediction in non-small cell lung cancer (NSCLC). Instead of merely concatenating or summing up the features from different modalities, our method gauges the importance of each modality for feature fusion with cross-modality relationship when infusing the multimodal features. Compared with single modality, which achieved c-index of 0.5772 and 0.5885 using solely tissue image data or RNA-seq data, respectively, the proposed fusion approach achieved c-index 0.6587 in our experiment, showcasing the capability of assimilating modality-specific knowledge from varied modalities.
</details>
<details>
<summary>摘要</summary>
肿瘤诊断和存活结果预测是肿瘤治疗的关键因素，可以用于评估治疗效果和将患者分组。医学领域中关于肿瘤诊断的数据非常丰富，包括图像数据和非图像数据，如基因信息。到目前为止，多modal学习已经显示出了提高诊断模型性能的潜力，通过提取和综合不同模式的信息来提高计算机辅助诊断和预测的性能。在这项工作中，我们提出了跨模式关注-基于多模式融合管线，用于汇集不同模式特有的知识来预测患者存活情况。而不是仅将不同模式的特征 concatenate 或 sum，我们的方法会测量不同模式之间的关系，以便在融合多模式特征时进行相应的权重调整。相比单模式，我们的融合方法在实验中实现了c-index 0.6587，超过了使用具体图像数据或 RNA-seq 数据单独进行预测的性能。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/19/eess.IV_2023_08_19/" data-id="clm0t8e2u00fcv78882zhe4b4" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/18/cs.LG_2023_08_18/" class="article-date">
  <time datetime="2023-08-17T16:00:00.000Z" itemprop="datePublished">2023-08-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/18/cs.LG_2023_08_18/">cs.LG - 2023-08-18 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Revisiting-Skin-Tone-Fairness-in-Dermatological-Lesion-Classification"><a href="#Revisiting-Skin-Tone-Fairness-in-Dermatological-Lesion-Classification" class="headerlink" title="Revisiting Skin Tone Fairness in Dermatological Lesion Classification"></a>Revisiting Skin Tone Fairness in Dermatological Lesion Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09640">http://arxiv.org/abs/2308.09640</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tkalbl/revisitingskintonefairness">https://github.com/tkalbl/revisitingskintonefairness</a></li>
<li>paper_authors: Thorsten Kalb, Kaisar Kushibar, Celia Cintas, Karim Lekadir, Oliver Diaz, Richard Osuala</li>
<li>for: 评估皮肤疾病分类中的公平性是非常重要的，因为皮肤疾病在不同皮肤色调上表现不同。但由于公共数据集中没有皮肤色调标签，因此建立公平的分类器受到限制。</li>
<li>methods: 以前的研究使用了个体学型角度（ITA）来估算皮肤色调，并将其分为不同的皮肤色调类别。然而，这些方法之间存在很大的不一致，这可能是因为数据集的不一致性带来的。</li>
<li>results: 我们的分析发现，以前发表的研究中的皮肤色调估算方法之间存在很大的不一致，这可能是因为数据集的不一致性带来的。此外，我们还发现了数据集的不多样化性，这限制了其用于公平性分析的可用性。因此，我们建议进一步研究ITA估算方法和数据集多样化性的增强，以便更 conclussive 的公平性评估。<details>
<summary>Abstract</summary>
Addressing fairness in lesion classification from dermatological images is crucial due to variations in how skin diseases manifest across skin tones. However, the absence of skin tone labels in public datasets hinders building a fair classifier. To date, such skin tone labels have been estimated prior to fairness analysis in independent studies using the Individual Typology Angle (ITA). Briefly, ITA calculates an angle based on pixels extracted from skin images taking into account the lightness and yellow-blue tints. These angles are then categorised into skin tones that are subsequently used to analyse fairness in skin cancer classification. In this work, we review and compare four ITA-based approaches of skin tone classification on the ISIC18 dataset, a common benchmark for assessing skin cancer classification fairness in the literature. Our analyses reveal a high disagreement among previously published studies demonstrating the risks of ITA-based skin tone estimation methods. Moreover, we investigate the causes of such large discrepancy among these approaches and find that the lack of diversity in the ISIC18 dataset limits its use as a testbed for fairness analysis. Finally, we recommend further research on robust ITA estimation and diverse dataset acquisition with skin tone annotation to facilitate conclusive fairness assessments of artificial intelligence tools in dermatology. Our code is available at https://github.com/tkalbl/RevisitingSkinToneFairness.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "lesion classification" is translated as "皮肤病变分类" (pī bì bì huàng bèng)* "dermatological images" is translated as "皮肤图像" (pī bì tú xiàng)* "skin tone" is translated as "皮肤色调" (pī bì sè tiào)* "Individual Typology Angle" is translated as "个体类型角度" (gè tǐ xìng tiě jiàng)* "ITA" is translated as "ITA" (ī tà)* "skin cancer classification" is translated as "皮肤癌分类" (pī bì bì hán bèng)* "ISIC18 dataset" is translated as "ISIC18数据集" (yī sī sī wù shù)* "artificial intelligence tools" is translated as "人工智能工具" (rén gōng zhì neng gōng bù)
</details></li>
</ul>
<hr>
<h2 id="Development-of-a-Neural-Network-based-Method-for-Improved-Imputation-of-Missing-Values-in-Time-Series-Data-by-Repurposing-DataWig"><a href="#Development-of-a-Neural-Network-based-Method-for-Improved-Imputation-of-Missing-Values-in-Time-Series-Data-by-Repurposing-DataWig" class="headerlink" title="Development of a Neural Network-based Method for Improved Imputation of Missing Values in Time Series Data by Repurposing DataWig"></a>Development of a Neural Network-based Method for Improved Imputation of Missing Values in Time Series Data by Repurposing DataWig</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09635">http://arxiv.org/abs/2308.09635</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Zhang</li>
<li>For: This paper is written for researchers and practitioners who work with time series data and need to impute missing values in their datasets.* Methods: The paper proposes a modified version of the DataWig method, called tsDataWig, which is designed to handle time series data and can directly impute missing values in complex time series datasets.* Results: The author demonstrates that tsDataWig outperforms the original DataWig and current state-of-the-art methods for time series data imputation on simulated and real-world datasets, and has the potential to be applied to large and complex time series datasets with millions of samples, high dimensional variables, and heterogeneous data types.<details>
<summary>Abstract</summary>
Time series data are observations collected over time intervals. Successful analysis of time series data captures patterns such as trends, cyclicity and irregularity, which are crucial for decision making in research, business, and governance. However, missing values in time series data occur often and present obstacles to successful analysis, thus they need to be filled with alternative values, a process called imputation. Although various approaches have been attempted for robust imputation of time series data, even the most advanced methods still face challenges including limited scalability, poor capacity to handle heterogeneous data types and inflexibility due to requiring strong assumptions of data missing mechanisms. Moreover, the imputation accuracy of these methods still has room for improvement. In this study, I developed tsDataWig (time-series DataWig) by modifying DataWig, a neural network-based method that possesses the capacity to process large datasets and heterogeneous data types but was designed for non-time series data imputation. Unlike the original DataWig, tsDataWig can directly handle values of time variables and impute missing values in complex time series datasets. Using one simulated and three different complex real-world time series datasets, I demonstrated that tsDataWig outperforms the original DataWig and the current state-of-the-art methods for time series data imputation and potentially has broad application due to not requiring strong assumptions of data missing mechanisms. This study provides a valuable solution for robustly imputing missing values in challenging time series datasets, which often contain millions of samples, high dimensional variables, and heterogeneous data types.
</details>
<details>
<summary>摘要</summary>
时间序列数据是在时间间隔内收集的观测记录。成功分析时间序列数据可以捕捉到趋势、征 циclical 和不规则性，这些特征对研究、商业和管理决策都非常重要。然而，时间序列数据中的缺失值很常 occurrence 和阻碍了成功分析，因此需要将其替换为可行的值，一个过程称为填充。虽然有很多方法用于robust imputation of time series data，但even the most advanced methods still face challenges including limited scalability, poor capacity to handle heterogeneous data types, and inflexibility due to requiring strong assumptions of data missing mechanisms。此外，这些方法的填充精度仍然有待提高。在这项研究中，我开发了tsDataWig（时间序列数据wig），通过修改DataWig，一种基于神经网络的方法，该方法可以处理大量数据和多种数据类型，但是它是非时间序列数据填充的设计。不同于原始DataWig，tsDataWig可以直接处理时间变量的值并在复杂的时间序列数据集中填充缺失值。使用一个 simulate 和三个不同的复杂实际时间序列数据集，我证明了tsDataWig在时间序列数据填充中表现出色，并且可能具有广泛的应用，因为它不需要强制的数据缺失机制假设。这项研究为Robustly imputing missing values in challenging time series datasets，这些数据集通常包含百万个样本，高维度变量和多种数据类型提供了有价值的解决方案。
</details></li>
</ul>
<hr>
<h2 id="VALERIE22-–-A-photorealistic-richly-metadata-annotated-dataset-of-urban-environments"><a href="#VALERIE22-–-A-photorealistic-richly-metadata-annotated-dataset-of-urban-environments" class="headerlink" title="VALERIE22 – A photorealistic, richly metadata annotated dataset of urban environments"></a>VALERIE22 – A photorealistic, richly metadata annotated dataset of urban environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09632">http://arxiv.org/abs/2308.09632</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oliver Grau, Korbinian Hagn</li>
<li>for: 本研究旨在帮助深度神经网络（DNN）的识别性能理解，特别是在城市环境中的自动驾驶场景中。</li>
<li>methods: 本研究使用了VALERIE工具渠道生成了一个名为VALERIE22的 sintetic数据集，该数据集包含了高品质的摄像头视频数据，并且具有丰富的元数据，如图像精度遮挡率、物体位置和距离相机的信息。</li>
<li>results: 根据性能指标，VALERIE22数据集与其他公共可用的数据集进行了比较，结果显示VALERIE22是目前公开领域中最高性能的 sintetic数据集之一。<details>
<summary>Abstract</summary>
The VALERIE tool pipeline is a synthetic data generator developed with the goal to contribute to the understanding of domain-specific factors that influence perception performance of DNNs (deep neural networks). This work was carried out under the German research project KI Absicherung in order to develop a methodology for the validation of DNNs in the context of pedestrian detection in urban environments for automated driving. The VALERIE22 dataset was generated with the VALERIE procedural tools pipeline providing a photorealistic sensor simulation rendered from automatically synthesized scenes. The dataset provides a uniquely rich set of metadata, allowing extraction of specific scene and semantic features (like pixel-accurate occlusion rates, positions in the scene and distance + angle to the camera). This enables a multitude of possible tests on the data and we hope to stimulate research on understanding performance of DNNs. Based on performance metric a comparison with several other publicly available datasets is provided, demonstrating that VALERIE22 is one of best performing synthetic datasets currently available in the open domain.
</details>
<details>
<summary>摘要</summary>
VALERIE工具管道是一个用于生成 sintetic数据的工具，旨在提高深度神经网络（DNN）的感知性能的理解。这项工作是在德国研究项目“KI Absicherung”下进行的，以开发一种用于自动驾驶中人员检测 scenes的 DNN 验证方法。VALERIE22数据集是通过VALERIE生成器工具管道生成的，它提供了高度具有 metadata，如自动生成场景中的干扰率、位置和距离摄像头的信息，这些信息可以用于进行多种可能的数据分析和研究。根据表现指标，我们比较了VALERIE22数据集与其他公共可用的数据集，结果显示VALERIE22是目前公开领域中最佳的 sintetic 数据集之一。
</details></li>
</ul>
<hr>
<h2 id="Learning-Computational-Efficient-Bots-with-Costly-Features"><a href="#Learning-Computational-Efficient-Bots-with-Costly-Features" class="headerlink" title="Learning Computational Efficient Bots with Costly Features"></a>Learning Computational Efficient Bots with Costly Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09629">http://arxiv.org/abs/2308.09629</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anthony Kobanda, Valliappan C. A., Joshua Romoff, Ludovic Denoyer</li>
<li>for: 这个研究旨在提高深度强化学习（DRL）技术在各种决策过程中的computational efficiency，以及在real-time setting中的决策过程中的能效性。</li>
<li>methods: 本研究提出了一个通用的Offline学习方法，考虑了输入特征 Computational Cost的问题。我们提出了一个名为Budgeted Decision Transformer的扩展，将成本限制纳入到决策过程中，以避免过度运算。</li>
<li>results: 我们在多个任务上进行了实验，包括D4RLbenchmark和复杂的3D环境，以及类似于游戏的环境。结果显示，我们的方法可以在computational resources方面实现显著的节省，同时保持与 класи方法相似的性能。<details>
<summary>Abstract</summary>
Deep reinforcement learning (DRL) techniques have become increasingly used in various fields for decision-making processes. However, a challenge that often arises is the trade-off between both the computational efficiency of the decision-making process and the ability of the learned agent to solve a particular task. This is particularly critical in real-time settings such as video games where the agent needs to take relevant decisions at a very high frequency, with a very limited inference time.   In this work, we propose a generic offline learning approach where the computation cost of the input features is taken into account. We derive the Budgeted Decision Transformer as an extension of the Decision Transformer that incorporates cost constraints to limit its cost at inference. As a result, the model can dynamically choose the best input features at each timestep. We demonstrate the effectiveness of our method on several tasks, including D4RL benchmarks and complex 3D environments similar to those found in video games, and show that it can achieve similar performance while using significantly fewer computational resources compared to classical approaches.
</details>
<details>
<summary>摘要</summary>
深度强化学习（DRL）技术在不同领域的决策过程中得到广泛应用。然而，经常出现的挑战是决策过程的计算效率和学习的能力之间的负面相互作用。这 particualry critical在实时设定中，如游戏中，Agent需要在 Very high frequency 上做相关的决策，具有很限的推理时间。在这种情况下，我们提出了一种通用的 offline 学习方法，其中输入特征的计算成本被考虑。我们 derive the Budgeted Decision Transformer 作为 Decision Transformer 的扩展，该模型内置成本约束，以限制其推理时间的成本。因此，模型可以在每个时间步骤上动态选择最佳的输入特征。我们在多个任务上验证了我们的方法，包括 D4RL benchmark 和复杂的 3D 环境，并证明它可以在使用较少的计算资源的情况下达到相似的性能。
</details></li>
</ul>
<hr>
<h2 id="Constrained-Bayesian-Optimization-Using-a-Lagrange-Multiplier-Applied-to-Power-Transistor-Design"><a href="#Constrained-Bayesian-Optimization-Using-a-Lagrange-Multiplier-Applied-to-Power-Transistor-Design" class="headerlink" title="Constrained Bayesian Optimization Using a Lagrange Multiplier Applied to Power Transistor Design"></a>Constrained Bayesian Optimization Using a Lagrange Multiplier Applied to Power Transistor Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09612">http://arxiv.org/abs/2308.09612</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ping-Ju Chuang, Ali Saadat, Sara Ghazvini, Hal Edwards, William G. Vandenberghe</li>
<li>for: 优化LDMOS晶体管的设计过程，实现目标破坏电压（BV）。</li>
<li>methods: 使用受限的极大优化算法（BO），通过拉格朗日积分来转化受限BO问题，并使用可变的拉格朗日多多乘数来解决具有成本评估的约束。</li>
<li>results: 通过这种算法，设计者可以在设计空间中设定目标BV，并自动获得满足优化FOM和目标BV约束的设备。此外，我们还在30-50V范围内对我们的设备的物理限制进行了探索。<details>
<summary>Abstract</summary>
We propose a novel constrained Bayesian Optimization (BO) algorithm optimizing the design process of Laterally-Diffused Metal-Oxide-Semiconductor (LDMOS) transistors while realizing a target Breakdown Voltage (BV). We convert the constrained BO problem into a conventional BO problem using a Lagrange multiplier. Instead of directly optimizing the traditional Figure-of-Merit (FOM), we set the Lagrangian as the objective function of BO. This adaptive objective function with a changeable Lagrange multiplier can address constrained BO problems which have constraints that require costly evaluations, without the need for additional surrogate models to approximate constraints. Our algorithm enables a device designer to set the target BV in the design space, and obtain a device that satisfies the optimized FOM and the target BV constraint automatically. Utilizing this algorithm, we have also explored the physical limits of the FOM for our devices in 30 - 50 V range within the defined design space.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的受限 bayesian优化（BO）算法，用于优化扩散金属氧化半导体（LDMOS）晶体管的设计过程，同时实现目标电压触发 voltage（BV）。我们将受限 BO 问题转化为一个普通的 BO 问题，使用拉格朗日 multiply 来实现。而不是直接优化传统的图像因数（FOM），我们设置了 Lagrange 函数作为 BO 的目标函数。这种可变的 Lagrange 多重器可以解决受限 BO 问题，无需额外的规格模型来近似约束。我们的算法允许设计者在设计空间中设置目标 BV，并自动获得满足优化 FOM 和目标 BV 约束的设备。通过这种算法，我们还在30-50 V 范围内对我们的设备的物理限制进行了探索。
</details></li>
</ul>
<hr>
<h2 id="Solving-PDEs-on-Spheres-with-Physics-Informed-Convolutional-Neural-Networks"><a href="#Solving-PDEs-on-Spheres-with-Physics-Informed-Convolutional-Neural-Networks" class="headerlink" title="Solving PDEs on Spheres with Physics-Informed Convolutional Neural Networks"></a>Solving PDEs on Spheres with Physics-Informed Convolutional Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09605">http://arxiv.org/abs/2308.09605</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guanhang Lei, Zhen Lei, Lei Shi, Chenyu Zeng, Ding-Xuan Zhou</li>
<li>for: 解决高维度部分几何方程（PDEs）的数学理论分析</li>
<li>methods: 使用和改进深度卷积神经网络的最新近似结果和球面幂分析，证明 Sobolev  нор下的预测误差的Upper bound</li>
<li>results: 确认和补充了实验结果，并在高维度PDEs中解决尺度味的味In English:</li>
<li>for: Solving high-dimensional partial differential equations (PDEs) from a mathematical perspective</li>
<li>methods: Using and improving the latest approximation results of deep convolutional neural networks and spherical harmonic analysis to establish an upper bound for the approximation error with respect to the Sobolev norm</li>
<li>results: Confirming and supplementing the experimental results, and solving the curse of dimensionality in high-dimensional PDEs<details>
<summary>Abstract</summary>
Physics-informed neural networks (PINNs) have been demonstrated to be efficient in solving partial differential equations (PDEs) from a variety of experimental perspectives. Some recent studies have also proposed PINN algorithms for PDEs on surfaces, including spheres. However, theoretical understanding of the numerical performance of PINNs, especially PINNs on surfaces or manifolds, is still lacking. In this paper, we establish rigorous analysis of the physics-informed convolutional neural network (PICNN) for solving PDEs on the sphere. By using and improving the latest approximation results of deep convolutional neural networks and spherical harmonic analysis, we prove an upper bound for the approximation error with respect to the Sobolev norm. Subsequently, we integrate this with innovative localization complexity analysis to establish fast convergence rates for PICNN. Our theoretical results are also confirmed and supplemented by our experiments. In light of these findings, we explore potential strategies for circumventing the curse of dimensionality that arises when solving high-dimensional PDEs.
</details>
<details>
<summary>摘要</summary>
physics-informed neural networks (PINNs) 物理学习神经网络 (PINNs) 已经在各种实验方面证明其能够有效地解决部分杂度方程 (PDEs)。一些最近的研究还提出了 PINN 算法用于 PDEs 的表面上，包括球体。然而，对 PINNs 的数学性能，特别是在表面或者拓扑上的数学性能，仍然缺乏一般理解。在这篇论文中，我们建立了 PICNN 的准确分析，用于解决球体上的 PDEs。我们利用最新的深度卷积神经网络的近似结果和球面幂分析，证明了在 Sobolev 范数中的上界误差。然后，我们将这一结果与创新的本地化复杂性分析结合，以确立 PICNN 的快速收敛速率。我们的理论结果也被实验证明。在这些发现的基础上，我们探讨了解决高维度 PDEs 时的维度繁殖的缺陷。
</details></li>
</ul>
<hr>
<h2 id="Breaking-the-Complexity-Barrier-in-Compositional-Minimax-Optimization"><a href="#Breaking-the-Complexity-Barrier-in-Compositional-Minimax-Optimization" class="headerlink" title="Breaking the Complexity Barrier in Compositional Minimax Optimization"></a>Breaking the Complexity Barrier in Compositional Minimax Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09604">http://arxiv.org/abs/2308.09604</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jin Liu, Xiaokang Pan, Junwen Duan, Hongdong Li, Youqi Li, Zhe Qu</li>
<li>for: 这个论文旨在解决机器学习中的分布robustness和策略评估中的compositional minimax optimization问题。</li>
<li>methods: 该论文提出了嵌入STORM（NSTORM）和ADAptive NSTORM（ADA-NSTORM）两种方法，它们可以实现$O(\kappa^3&#x2F;\epsilon^3)$的样本复杂度，并且可以在不需要大批量的情况下进行分布robustness和策略评估。</li>
<li>results: 该论文通过了广泛的实验来证明其方法可以匹配下界，并且在实际应用中更加有效。<details>
<summary>Abstract</summary>
Compositional minimax optimization is a pivotal yet under-explored challenge across machine learning, including distributionally robust training and policy evaluation for reinforcement learning. Current techniques exhibit suboptimal complexity or rely heavily on large batch sizes. This paper proposes Nested STOchastic Recursive Momentum (NSTORM), attaining the optimal sample complexity of $O(\kappa^3/\epsilon^3)$ for finding an $\epsilon$-accurate solution. However, NSTORM requires low learning rates, potentially limiting applicability. Thus we introduce ADAptive NSTORM (ADA-NSTORM) with adaptive learning rates, proving it achieves the same sample complexity while experiments demonstrate greater effectiveness. Our methods match lower bounds for minimax optimization without large batch requirements, validated through extensive experiments. This work significantly advances compositional minimax optimization, a crucial capability for distributional robustness and policy evaluation
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate the following text into Simplified Chinese:Compositional minimax optimization is a pivotal yet under-explored challenge across machine learning, including distributionally robust training and policy evaluation for reinforcement learning. Current techniques exhibit suboptimal complexity or rely heavily on large batch sizes. This paper proposes Nested STOchastic Recursive Momentum (NSTORM), attaining the optimal sample complexity of $O(\kappa^3/\epsilon^3)$ for finding an $\epsilon$-accurate solution. However, NSTORM requires low learning rates, potentially limiting applicability. Thus we introduce ADAptive NSTORM (ADA-NSTORM) with adaptive learning rates, proving it achieves the same sample complexity while experiments demonstrate greater effectiveness. Our methods match lower bounds for minimax optimization without large batch requirements, validated through extensive experiments. This work significantly advances compositional minimax optimization, a crucial capability for distributional robustness and policy evaluation.Translate the text into Simplified Chinese, please.</SYS> Sure, here's the translation:<<SYS>> compositional minimax optimization 是机器学习中的一个重要 yet under-explored 挑战，包括分布 robust 训练和策略评估。当前技术具有不佳的复杂性或者依赖于大批量Size，这篇论文提出了嵌入 STOchastic Recursive Momentum (NSTORM)，实现了 $\epsilon$-accurate 解决方案的最佳样本复杂度 $O(\kappa^3/\epsilon^3)$。然而，NSTORM需要低学习率，可能限制其应用性。因此，我们介绍了 ADAptive NSTORM (ADA-NSTORM)  WITH adaptive learning rates，证明其实现了同样的样本复杂度，而实验表明它更有效。我们的方法与最佳下界匹配，无需大批量要求，经验验证了这一点。这项工作显著提高了 compositional minimax optimization 的能力，这是分布 robustness 和策略评估中的关键能力。Translate the text into Simplified Chinese, please.</SYS> Sure, here's the translation:<<SYS>>compositional minimax optimization 是机器学习中的一个重要 yet under-explored 挑战，包括分布 robust 训练和策略评估。当前技术具有不佳的复杂性或者依赖于大批量Size，这篇论文提出了嵌入 STOchastic Recursive Momentum (NSTORM)，实现了 $\epsilon$-accurate 解决方案的最佳样本复杂度 $O(\kappa^3/\epsilon^3)$。然而，NSTORM需要低学习率，可能限制其应用性。因此，我们介绍了 ADAptive NSTORM (ADA-NSTORM) WITH adaptive learning rates，证明其实现了同样的样本复杂度，而实验表明它更有效。我们的方法与最佳下界匹配，无需大批量要求，经验验证了这一点。这项工作显著提高了 compositional minimax optimization 的能力，这是分布 robustness 和策略评估中的关键能力。
</details></li>
</ul>
<hr>
<h2 id="Disparity-Inequality-and-Accuracy-Tradeoffs-in-Graph-Neural-Networks-for-Node-Classification"><a href="#Disparity-Inequality-and-Accuracy-Tradeoffs-in-Graph-Neural-Networks-for-Node-Classification" class="headerlink" title="Disparity, Inequality, and Accuracy Tradeoffs in Graph Neural Networks for Node Classification"></a>Disparity, Inequality, and Accuracy Tradeoffs in Graph Neural Networks for Node Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09596">http://arxiv.org/abs/2308.09596</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/arpitdm/gnn_accuracy_fairness_tradeoff">https://github.com/arpitdm/gnn_accuracy_fairness_tradeoff</a></li>
<li>paper_authors: Arpit Merchant, Carlos Castillo</li>
<li>for: 本研究旨在探讨Graph Neural Networks（GNNs）在人类应用中的偏见问题，特别是在预测节点标签的过程中是否存在偏见，以及如何 Mitigate 这些偏见的影响。</li>
<li>methods: 本研究提出了两种GNN-agnostic intervención，namely PFR-AX和PostProcess，以降低节点之间的分离度，并更新模型预测结果以最小化不同群体之间的差异。</li>
<li>results: 通过在四个数据集上进行了大量实验，研究发现，PFR-AX和PostProcess可以提供精细的控制和提高模型对保护组节点正确预测的自信度，但不同的 intervención在不同的数据集上的优化效果不同。<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) are increasingly used in critical human applications for predicting node labels in attributed graphs. Their ability to aggregate features from nodes' neighbors for accurate classification also has the capacity to exacerbate existing biases in data or to introduce new ones towards members from protected demographic groups. Thus, it is imperative to quantify how GNNs may be biased and to what extent their harmful effects may be mitigated. To this end, we propose two new GNN-agnostic interventions namely, (i) PFR-AX which decreases the separability between nodes in protected and non-protected groups, and (ii) PostProcess which updates model predictions based on a blackbox policy to minimize differences between error rates across demographic groups. Through a large set of experiments on four datasets, we frame the efficacies of our approaches (and three variants) in terms of their algorithmic fairness-accuracy tradeoff and benchmark our results against three strong baseline interventions on three state-of-the-art GNN models. Our results show that no single intervention offers a universally optimal tradeoff, but PFR-AX and PostProcess provide granular control and improve model confidence when correctly predicting positive outcomes for nodes in protected groups.
</details>
<details>
<summary>摘要</summary>
Graph neural networks (GNNs) 是在人类应用中越来越普遍地用于预测 attributed 图上的节点标签。它们可以从节点邻居中聚合特征，以便准确地分类，同时也可能扩大或引入数据中现有的偏见，特别是对来自保护性群体的成员。因此，有必要量化 GNN 是如何偏离的，以及如何降低它的负面影响。为此，我们提出了两种 GNN 不依赖的干预方法，namely，(i) PFR-AX，减少保护和非保护组之间节点的分离度，以及 (ii) PostProcess，根据黑盒政策更新模型预测结果，以最小化不同群体的错误率差异。通过大量实验，我们将我们的方法（及其三种变体）的公平性-准确性负担Plot 与三种基eline interventions 对三种现代 GNN 模型进行比较。我们的结果显示，无一个干预方法可以在所有情况下提供最佳的负担平衡，但PFR-AX 和 PostProcess 可以提供细化的控制，并在保护组中正确预测结果时提高模型的信任度。
</details></li>
</ul>
<hr>
<h2 id="WizardMath-Empowering-Mathematical-Reasoning-for-Large-Language-Models-via-Reinforced-Evol-Instruct"><a href="#WizardMath-Empowering-Mathematical-Reasoning-for-Large-Language-Models-via-Reinforced-Evol-Instruct" class="headerlink" title="WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct"></a>WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09583">http://arxiv.org/abs/2308.09583</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nlpxucan/wizardlm">https://github.com/nlpxucan/wizardlm</a></li>
<li>paper_authors: Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, Dongmei Zhang</li>
<li>for: 增强大型自然语言处理（NLP）任务中的数学逻辑能力</li>
<li>methods: 应用提出的演进学习从演示反馈（RLEIF）方法</li>
<li>results: 在两个数学逻辑benchmark上表现出色，超过了所有公开源模型，并且even surpasses ChatGPT-3.5、Claude Instant-1、PaLM-2和Minerva在GSM8k上，同时也超过Text-davinci-002、PaLM-1和GPT-3在MATH上。<details>
<summary>Abstract</summary>
Large language models (LLMs), such as GPT-4, have shown remarkable performance in natural language processing (NLP) tasks, including challenging mathematical reasoning. However, most existing open-source models are only pre-trained on large-scale internet data and without math-related optimization. In this paper, we present WizardMath, which enhances the mathematical reasoning abilities of Llama-2, by applying our proposed Reinforcement Learning from Evol-Instruct Feedback (RLEIF) method to the domain of math. Through extensive experiments on two mathematical reasoning benchmarks, namely GSM8k and MATH, we reveal the extraordinary capabilities of our model. WizardMath surpasses all other open-source LLMs by a substantial margin. Furthermore, our model even outperforms ChatGPT-3.5, Claude Instant-1, PaLM-2 and Minerva on GSM8k, simultaneously surpasses Text-davinci-002, PaLM-1 and GPT-3 on MATH. More details and model weights are public at https://github.com/nlpxucan/WizardLM and https://huggingface.co/WizardLM.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM），如GPT-4，在自然语言处理（NLP）任务中表现很出色，包括复杂的数学逻辑。然而，现有的大多数开源模型都只是在大规模的互联网数据上预训练而不是数学相关的优化。在本文中，我们介绍了WizardMath，它通过我们提议的强化学习反馈法（RLEIF）方法来增强Llama-2模型的数学逻辑能力。经过广泛的实验，我们发现WizardMath在两个数学逻辑 bencmarks，即GSM8k和MATH上表现出色，大大超过了其他开源LLM。此外，我们的模型还超过了ChatGPT-3.5、Claude Instant-1、PaLM-2和Minerva在GSM8k上，同时也超过了Text-davinci-002、PaLM-1和GPT-3在MATH上。详细信息和模型权重可以在 GitHub 上找到（https://github.com/nlpxucan/WizardLM）和 Hugging Face 上找到（https://huggingface.co/WizardLM）。
</details></li>
</ul>
<hr>
<h2 id="Physics-Informed-Boundary-Integral-Networks-PIBI-Nets-A-Data-Driven-Approach-for-Solving-Partial-Differential-Equations"><a href="#Physics-Informed-Boundary-Integral-Networks-PIBI-Nets-A-Data-Driven-Approach-for-Solving-Partial-Differential-Equations" class="headerlink" title="Physics-Informed Boundary Integral Networks (PIBI-Nets): A Data-Driven Approach for Solving Partial Differential Equations"></a>Physics-Informed Boundary Integral Networks (PIBI-Nets): A Data-Driven Approach for Solving Partial Differential Equations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09571">http://arxiv.org/abs/2308.09571</a></li>
<li>repo_url: None</li>
<li>paper_authors: Monika Nagy-Huber, Volker Roth</li>
<li>for: 解决实世界应用中的减震问题，即将 formal PDE 模型与 (可能含有噪声的) 观测结合起来，特别在缺乏边界或初始条件信息时。</li>
<li>methods: physics-informed neural networks (PINNs) 和 physics-informed boundary integral networks (PIBI-Nets)，后者在高维度设置中具有更高的精度和效率。</li>
<li>results: PIBI-Nets 在 Laplace 和 Poisson 方程中实现了非常高的准确率，并在一个真实世界应用中成功地重建了地下水流。<details>
<summary>Abstract</summary>
Partial differential equations (PDEs) can describe many relevant phenomena in dynamical systems. In real-world applications, we commonly need to combine formal PDE models with (potentially noisy) observations. This is especially relevant in settings where we lack information about boundary or initial conditions, or where we need to identify unknown model parameters. In recent years, Physics-informed neural networks (PINNs) have become a popular tool for problems of this kind. In high-dimensional settings, however, PINNs often suffer from computational problems because they usually require dense collocation points over the entire computational domain. To address this problem, we present Physics-Informed Boundary Integral Networks (PIBI-Nets) as a data-driven approach for solving PDEs in one dimension less than the original problem space. PIBI-Nets only need collocation points at the computational domain boundary, while still achieving highly accurate results, and in several practical settings, they clearly outperform PINNs. Exploiting elementary properties of fundamental solutions of linear differential operators, we present a principled and simple way to handle point sources in inverse problems. We demonstrate the excellent performance of PIBI-Nets for the Laplace and Poisson equations, both on artificial data sets and within a real-world application concerning the reconstruction of groundwater flows.
</details>
<details>
<summary>摘要</summary>
diferenciales parciales (PDEs) pueden describir muchos fenómenos relevantes en sistemas dinámicos. En aplicaciones reales, comúnmente combinamos modelos formales PDE con (potentialmente ruidosas) observaciones. Esto es especialmente relevante en situaciones donde carecemos de información sobre condiciones de borde o condiciones iniciales, o donde debemos identificar parámetros desconocidos del modelo. En los últimos años, las redes neuronales informadas por la física (PINNs) han become a herramienta popular para problemas de este tipo. Sin embargo, en configuraciones de alta dimensión, las PINNs suelen sufrir de problemas computacionales debido a que requieren puntos de colocación densos en todo el dominio de cálculo. Para abordar este problema, presentamos redes de integración de la frontera informadas por la física (PIBI-Nets) como una aproximación datos-driven para resolver PDEs en un espacio de dimensión inferior al espacio de problema original. Las PIBI-Nets solo necesitan puntos de colocación en la frontera del dominio de cálculo, mientras aún logran resultados altamente precisos, y en varios contextos prácticos, claramente superan a las PINNs. Explotando propiedades elementales de las soluciones fundamental de operadores lineales diferenciales, presentamos una manera principios y simple de manejar fuentes de puntos en problemas de inversión. Demostramos el excelente rendimiento de las PIBI-Nets para las ecuaciones de Laplace y de Poisson, tanto en conjuntos de datos artificiales como en una aplicación real concerniente a la reconstrucción de flujos de agua subterránea.
</details></li>
</ul>
<hr>
<h2 id="Investigating-the-Interplay-between-Features-and-Structures-in-Graph-Learning"><a href="#Investigating-the-Interplay-between-Features-and-Structures-in-Graph-Learning" class="headerlink" title="Investigating the Interplay between Features and Structures in Graph Learning"></a>Investigating the Interplay between Features and Structures in Graph Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09570">http://arxiv.org/abs/2308.09570</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniele Castellana, Federico Errica</li>
<li>for: This paper is written for researchers who are interested in understanding the inductive bias of Deep Graph Networks (DGNs) and how it relates to the dichotomy between homophily and heterophily.</li>
<li>methods: The paper uses two generative processes for node classification tasks, which allow the authors to build and study ad-hoc problems. The authors also use a metric called Feature Informativeness to quantitatively measure the influence of node features on target labels.</li>
<li>results: The paper finds that previously defined metrics are not adequate when the assumption of a strong correlation between node features and target labels is relaxed. The authors present novel research findings that could help advance our understanding of the field.<details>
<summary>Abstract</summary>
In the past, the dichotomy between homophily and heterophily has inspired research contributions toward a better understanding of Deep Graph Networks' inductive bias. In particular, it was believed that homophily strongly correlates with better node classification predictions of message-passing methods. More recently, however, researchers pointed out that such dichotomy is too simplistic as we can construct node classification tasks where graphs are completely heterophilic but the performances remain high. Most of these works have also proposed new quantitative metrics to understand when a graph structure is useful, which implicitly or explicitly assume the correlation between node features and target labels. Our work empirically investigates what happens when this strong assumption does not hold, by formalising two generative processes for node classification tasks that allow us to build and study ad-hoc problems. To quantitatively measure the influence of the node features on the target labels, we also use a metric we call Feature Informativeness. We construct six synthetic tasks and evaluate the performance of six models, including structure-agnostic ones. Our findings reveal that previously defined metrics are not adequate when we relax the above assumption. Our contribution to the workshop aims at presenting novel research findings that could help advance our understanding of the field.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Normalization-Is-All-You-Need-Understanding-Layer-Normalized-Federated-Learning-under-Extreme-Label-Shift"><a href="#Normalization-Is-All-You-Need-Understanding-Layer-Normalized-Federated-Learning-under-Extreme-Label-Shift" class="headerlink" title="Normalization Is All You Need: Understanding Layer-Normalized Federated Learning under Extreme Label Shift"></a>Normalization Is All You Need: Understanding Layer-Normalized Federated Learning under Extreme Label Shift</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09565">http://arxiv.org/abs/2308.09565</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guojun Zhang, Mahdi Beitollahi, Alex Bie, Xi Chen</li>
<li>for: 这paper的目的是解释层normalization在 Federated Learning 中的准确作用，以及它如何控制特征潜在的潜在问题。</li>
<li>methods: 这 paper 使用了多种方法来 investigated layer normalization，包括 feature normalization 和 label shift problem。</li>
<li>results: 这 paper 的结果表明，layer normalization 可以在 Federated Learning 中提高模型的性能，特别是在极端标签分布情况下。此外，paper 还进行了广泛的ablation study，以更好地理解层normalization的关键因素。<details>
<summary>Abstract</summary>
Layer normalization (LN) is a widely adopted deep learning technique especially in the era of foundation models. Recently, LN has been shown to be surprisingly effective in federated learning (FL) with non-i.i.d. data. However, exactly why and how it works remains mysterious. In this work, we reveal the profound connection between layer normalization and the label shift problem in federated learning. To understand layer normalization better in FL, we identify the key contributing mechanism of normalization methods in FL, called feature normalization (FN), which applies normalization to the latent feature representation before the classifier head. Although LN and FN do not improve expressive power, they control feature collapse and local overfitting to heavily skewed datasets, and thus accelerates global training. Empirically, we show that normalization leads to drastic improvements on standard benchmarks under extreme label shift. Moreover, we conduct extensive ablation studies to understand the critical factors of layer normalization in FL. Our results verify that FN is an essential ingredient inside LN to significantly improve the convergence of FL while remaining robust to learning rate choices, especially under extreme label shift where each client has access to few classes.
</details>
<details>
<summary>摘要</summary>
层normalization（LN）是深度学习中广泛采用的技术，特别在基础模型时代。最近，LN在非独立数据（Federated Learning，FL）中显示出意外的有效性。然而，它何时何为在FL中工作的原理仍然不清楚。在这项工作中，我们揭示了层normalization和FL中的标签shift问题之间的浓闻连接。为了更好地理解层normalization在FL中，我们确定了FL中normalization方法的关键贡献机制，称为特征normalization（FN），该机制在批处头之前对潜在特征表示进行了normalization。虽然LN和FN不提高表达力，但它们控制特征塌积和地方适应，从而加速全局训练。我们的实验结果表明，normalization在极端标签shift情况下导致了很大的改进，并且我们进行了广泛的减少研究，以了解层normalization在FL中的核心因素。我们的结果表明，FN是LN中关键的一部分，可以在FL中提高训练的速度和稳定性，特别是在极端标签shift情况下，每个客户端只有几个类型的训练数据。
</details></li>
</ul>
<hr>
<h2 id="Eigenvalue-based-Incremental-Spectral-Clustering"><a href="#Eigenvalue-based-Incremental-Spectral-Clustering" class="headerlink" title="Eigenvalue-based Incremental Spectral Clustering"></a>Eigenvalue-based Incremental Spectral Clustering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10999">http://arxiv.org/abs/2308.10999</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mieczysław A. Kłopotek, Bartłmiej Starosta, Sławomir T. Wierzchoń</li>
<li>for: 这个论文主要是为了提出一种增量 spectral clustering 方法，用于 clustering 大规模数据集。</li>
<li>methods: 该方法包括将数据分割成可管理的subsets，对每个subset进行 clustering，然后将不同subset的cluster合并到一起，基于eigenvalue spectrum的相似性。</li>
<li>results: 实验显示，这种方法可以快速地 clustering 大规模数据集，并且可以获得相对于整个数据集的cluster。<details>
<summary>Abstract</summary>
Our previous experiments demonstrated that subsets collections of (short) documents (with several hundred entries) share a common normalized in some way eigenvalue spectrum of combinatorial Laplacian. Based on this insight, we propose a method of incremental spectral clustering. The method consists of the following steps: (1) split the data into manageable subsets, (2) cluster each of the subsets, (3) merge clusters from different subsets based on the eigenvalue spectrum similarity to form clusters of the entire set. This method can be especially useful for clustering methods of complexity strongly increasing with the size of the data sample,like in case of typical spectral clustering. Experiments were performed showing that in fact the clustering and merging the subsets yields clusters close to clustering the entire dataset.
</details>
<details>
<summary>摘要</summary>
我们之前的实验表明，具有一些百度的文档集合（简称为“ subsets”）的常量 Laplacian 的几何特征是共同的正规化。基于这一点，我们提出了一种增量 spectral clustering 方法。该方法包括以下步骤：1. 将数据分成可控制的subsets;2. 对每个subset进行 clustering;3. 根据ensemble spectrum的相似性，将不同subset的cluster合并形成整个数据集的cluster。这种方法对于数据样本的复杂性呈指数增长的 clustering 方法特别有用。我们的实验表明，通过将subsets进行分 clustering 并将cluster合并，实际上可以获得整个数据集的高质量cluster。
</details></li>
</ul>
<hr>
<h2 id="Attesting-Distributional-Properties-of-Training-Data-for-Machine-Learning"><a href="#Attesting-Distributional-Properties-of-Training-Data-for-Machine-Learning" class="headerlink" title="Attesting Distributional Properties of Training Data for Machine Learning"></a>Attesting Distributional Properties of Training Data for Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09552">http://arxiv.org/abs/2308.09552</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vasisht Duddu, Anudeep Das, Nora Khayata, Hossein Yalame, Thomas Schneider, N. Asokan</li>
<li>for: 这篇论文是关于机器学习（ML）可靠性的问题，以及相关的法规框架的制定。</li>
<li>methods: 这篇论文提出了一种名为“属性证明”的方法，即证明模型训练数据的分布性质，但不披露实际数据。这种方法组合了属性推理和加密机制。</li>
<li>results: 这篇论文提出的属性证明方法可以有效地证明模型训练数据的分布性质，并且可以避免披露实际数据。<details>
<summary>Abstract</summary>
The success of machine learning (ML) has been accompanied by increased concerns about its trustworthiness. Several jurisdictions are preparing ML regulatory frameworks. One such concern is ensuring that model training data has desirable distributional properties for certain sensitive attributes. For example, draft regulations indicate that model trainers are required to show that training datasets have specific distributional properties, such as reflecting diversity of the population.   We propose the notion of property attestation allowing a prover (e.g., model trainer) to demonstrate relevant distributional properties of training data to a verifier (e.g., a customer) without revealing the data. We present an effective hybrid property attestation combining property inference with cryptographic mechanisms.
</details>
<details>
<summary>摘要</summary>
机器学习（ML）的成功也导致了其可靠性的问题受到了更多的关注。许多地区都在制定ML规章。一种关心的问题是确保模型训练数据具有特定敏感属性的愉悦分布性质。例如，审核法规提到了训练数据的特定分布性质，如反映人口多样性。我们提出了财产证明的概念，允许证明者（例如模型训练者）在不泄露数据的情况下，向验证者（例如客户）展示训练数据的相关分布性质。我们介绍了一种高效的混合财产证明方法，结合属性推断和 криптографиic机制。
</details></li>
</ul>
<hr>
<h2 id="Adapt-Your-Teacher-Improving-Knowledge-Distillation-for-Exemplar-free-Continual-Learning"><a href="#Adapt-Your-Teacher-Improving-Knowledge-Distillation-for-Exemplar-free-Continual-Learning" class="headerlink" title="Adapt Your Teacher: Improving Knowledge Distillation for Exemplar-free Continual Learning"></a>Adapt Your Teacher: Improving Knowledge Distillation for Exemplar-free Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09544">http://arxiv.org/abs/2308.09544</a></li>
<li>repo_url: None</li>
<li>paper_authors: Filip Szatkowski, Mateusz Pyla, Marcin Przewięźlikowski, Sebastian Cygert, Bartłomiej Twardowski, Tomasz Trzciński</li>
<li>for: 本研究 investigate exemplar-free class incremental learning (CIL) with knowledge distillation (KD) as a regularization strategy, aiming to prevent forgetting.</li>
<li>methods: 我们使用 KD-based methods, but we often struggle to regularize the model without access to exemplars of the training data from previous tasks. Our analysis reveals that this issue originates from substantial representation shifts in the teacher network when dealing with out-of-distribution data.</li>
<li>results: 我们引入 Teacher Adaptation (TA) method, which concurrently updates the teacher and the main model during incremental training. Our method seamlessly integrates with KD-based CIL approaches and allows for consistent enhancement of their performance across multiple exemplar-free CIL benchmarks.<details>
<summary>Abstract</summary>
In this work, we investigate exemplar-free class incremental learning (CIL) with knowledge distillation (KD) as a regularization strategy, aiming to prevent forgetting. KD-based methods are successfully used in CIL, but they often struggle to regularize the model without access to exemplars of the training data from previous tasks. Our analysis reveals that this issue originates from substantial representation shifts in the teacher network when dealing with out-of-distribution data. This causes large errors in the KD loss component, leading to performance degradation in CIL. Inspired by recent test-time adaptation methods, we introduce Teacher Adaptation (TA), a method that concurrently updates the teacher and the main model during incremental training. Our method seamlessly integrates with KD-based CIL approaches and allows for consistent enhancement of their performance across multiple exemplar-free CIL benchmarks.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们研究了无例示增量学习（CIL）中的知识储存（KD）作为规范策略，以避免忘记。KD基于方法在CIL中得到了成功，但它们经常无法在无前任任务的数据上正则化模型。我们的分析发现这个问题的起源在教师网络对异常数据的处理方面存在重大的表示转移问题，这导致KD损失部分的大错误，从而导致CIL性能下降。引用最近的测试时适应方法，我们提出了教师适应（TA）方法，该方法在增量训练中同时更新教师网络和主模型。我们的方法可以轻松地与KD基于CIL方法结合使用，并在多个无例示增量CILbenchmark上保持表现的一致提升。
</details></li>
</ul>
<hr>
<h2 id="Latent-State-Models-of-Training-Dynamics"><a href="#Latent-State-Models-of-Training-Dynamics" class="headerlink" title="Latent State Models of Training Dynamics"></a>Latent State Models of Training Dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09543">http://arxiv.org/abs/2308.09543</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Y. Hu, Angelica Chen, Naomi Saphra, Kyunghyun Cho</li>
<li>for: 这个论文旨在了解Randomness在模型训练中的影响，具体来说是如何不同的数据顺序和初始化实际会导致模型训练的不同结果，以及如何解释这些结果的训练动态和阶段转移。</li>
<li>methods: 作者使用多个Random seed来训练模型，并计算了训练过程中不同指标的变化，如模型权重的$L_2$范数、平均值和方差。然后，他们使用隐藏马尔可夫模型（HMM）来模型训练过程，将其视为一种随机过程，并从其中提取了训练动态的低维、离散表示。</li>
<li>results: 作者使用HMM来研究训练动态的阶段转移和阶段转移的概率分布，并发现了一些隐藏的”停滞”状态，这些状态会使训练过程慢下来。他们还使用HMM来研究模型训练的阶段转移，并发现了一些阶段转移的特征。<details>
<summary>Abstract</summary>
The impact of randomness on model training is poorly understood. How do differences in data order and initialization actually manifest in the model, such that some training runs outperform others or converge faster? Furthermore, how can we interpret the resulting training dynamics and the phase transitions that characterize different trajectories? To understand the effect of randomness on the dynamics and outcomes of neural network training, we train models multiple times with different random seeds and compute a variety of metrics throughout training, such as the $L_2$ norm, mean, and variance of the neural network's weights. We then fit a hidden Markov model (HMM) over the resulting sequences of metrics. The HMM represents training as a stochastic process of transitions between latent states, providing an intuitive overview of significant changes during training. Using our method, we produce a low-dimensional, discrete representation of training dynamics on grokking tasks, image classification, and masked language modeling. We use the HMM representation to study phase transitions and identify latent "detour" states that slow down convergence.
</details>
<details>
<summary>摘要</summary>
“模型训练中随机性的影响不甚了解。不同的数据顺序和初始化方法对模型的训练会如何产生不同的表现，有些训练运行比其他快速 converge 或者表现更好吗？此外，我们如何解释训练过程中的变动和结果，以及不同的训练路径中的相变点？为了理解随机性对模型训练的影响，我们将模型训练多次，每次使用不同的随机种子，并 Compute 一些 metric  durante 训练，例如模型的 $L_2$  норма、平均值和方差。然后，我们使用隐藏 Markov 模型 (HMM) 来描述训练的数据序列，从而获得训练过程的低维、组合表示。使用我们的方法，我们可以研究训练过程中的相变点和潜在的“停顿”状态，并且使用 HMM 表示来 изу究训练过程的相变点。”Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Decoupled-conditional-contrastive-learning-with-variable-metadata-for-prostate-lesion-detection"><a href="#Decoupled-conditional-contrastive-learning-with-variable-metadata-for-prostate-lesion-detection" class="headerlink" title="Decoupled conditional contrastive learning with variable metadata for prostate lesion detection"></a>Decoupled conditional contrastive learning with variable metadata for prostate lesion detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09542">http://arxiv.org/abs/2308.09542</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/camilleruppli/decoupled_ccl">https://github.com/camilleruppli/decoupled_ccl</a></li>
<li>paper_authors: Camille Ruppli, Pietro Gori, Roberto Ardon, Isabelle Bloch</li>
<li>for: 早期检测前列腺癌是治疗efficient的关键，mp-MRI广泛用于肿瘤检测。</li>
<li>methods: 我们提出了一种新的对比损失函数，利用多个注解员每个样本的 metadata，并利用多个注解员之间的变化来定义metadata信息的可信度。</li>
<li>results: 我们在公共PI-CAI挑战数据集上 Report a 3% AUC increase in lesion detection using our proposed contrastive loss function. 代码可以在 GitHub上找到：<a target="_blank" rel="noopener" href="https://github.com/camilleruppli/decoupled_ccl%E3%80%82">https://github.com/camilleruppli/decoupled_ccl。</a><details>
<summary>Abstract</summary>
Early diagnosis of prostate cancer is crucial for efficient treatment. Multi-parametric Magnetic Resonance Images (mp-MRI) are widely used for lesion detection. The Prostate Imaging Reporting and Data System (PI-RADS) has standardized interpretation of prostate MRI by defining a score for lesion malignancy. PI-RADS data is readily available from radiology reports but is subject to high inter-reports variability. We propose a new contrastive loss function that leverages weak metadata with multiple annotators per sample and takes advantage of inter-reports variability by defining metadata confidence. By combining metadata of varying confidence with unannotated data into a single conditional contrastive loss function, we report a 3% AUC increase on lesion detection on the public PI-CAI challenge dataset.   Code is available at: https://github.com/camilleruppli/decoupled_ccl
</details>
<details>
<summary>摘要</summary>
早期探测 проstate 癌是关键，以便有效的治疗。多parametric 磁共振成像 (mp-MRI) 广泛用于肿瘤检测。Prostate Imaging Reporting and Data System (PI-RADS) 已经标准化了 проstate MRI 的解释，并定义了肿瘤坏性的分数。PI-RADS 数据ready available 从 radiology 报告，但是受到高度的Inter-report variability。我们提议一种新的对比损失函数，利用weak metadata 和多个标注员每个样本，利用 inter-reports variability 定义metadata confidence。通过将metadata of varying confidence 与未标注数据结合成一个conditional contrastive loss函数，我们在公共PI-CAI challenge dataset上报告了3% AUC提高。代码可以在以下链接中找到：https://github.com/camilleruppli/decoupled_ccl。
</details></li>
</ul>
<hr>
<h2 id="Privacy-Preserving-3-Layer-Neural-Network-Training-using-Mere-Homomorphic-Encryption-Technique"><a href="#Privacy-Preserving-3-Layer-Neural-Network-Training-using-Mere-Homomorphic-Encryption-Technique" class="headerlink" title="Privacy-Preserving 3-Layer Neural Network Training using Mere Homomorphic Encryption Technique"></a>Privacy-Preserving 3-Layer Neural Network Training using Mere Homomorphic Encryption Technique</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09531">http://arxiv.org/abs/2308.09531</a></li>
<li>repo_url: None</li>
<li>paper_authors: John Chiang</li>
<li>for: 本 manuscript 考虑了隐私保护神经网络训练在简单的同adi运算 Setting 中的问题。</li>
<li>methods: 本 paper  combining 多种现有技术，进一步 extend 一些技术，最终实现了使用简单同adi运算技术训练 3-layer 神经网络，用于回归和分类问题。</li>
<li>results: 本 paper 实现了使用简单同adi运算技术训练神经网络，并且可以解决回归和分类问题。<details>
<summary>Abstract</summary>
In this manuscript, we consider the problem of privacy-preserving training of neural networks in the mere homomorphic encryption setting. We combine several exsiting techniques available, extend some of them, and finally enable the training of 3-layer neural networks for both the regression and classification problems using mere homomorphic encryption technique.
</details>
<details>
<summary>摘要</summary>
在这个手记中，我们考虑了使用简单的同源加密技术进行隐私保护神经网络训练。我们结合了现有的技术，进行了一些扩展，最终实现了使用简单同源加密技术训练3层神经网络，用于回归和分类问题。
</details></li>
</ul>
<hr>
<h2 id="Transitivity-Preserving-Graph-Representation-Learning-for-Bridging-Local-Connectivity-and-Role-based-Similarity"><a href="#Transitivity-Preserving-Graph-Representation-Learning-for-Bridging-Local-Connectivity-and-Role-based-Similarity" class="headerlink" title="Transitivity-Preserving Graph Representation Learning for Bridging Local Connectivity and Role-based Similarity"></a>Transitivity-Preserving Graph Representation Learning for Bridging Local Connectivity and Role-based Similarity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09517">http://arxiv.org/abs/2308.09517</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nslab-cuk/unified-graph-transformer">https://github.com/nslab-cuk/unified-graph-transformer</a></li>
<li>paper_authors: Van Thuy Hoang, O-Joun Lee</li>
<li>for: 这篇论文主要是为了提出一种能够有效地 integrate 本地和远程结构信息的图表示学方法（UGT），以便在不同的下游任务上进行图数据分析。</li>
<li>methods: 这篇论文提出了一种基于自我注意力的 UGT 模型，它首先通过识别本地子结构并汇集每个节点的 $k $- hop 邻居特征来学习本地结构。其次，它通过构建虚拟边来连接远程节点，以捕捉远程相互关系。最后，它通过自我注意力编码结构距离和 $p $- step 过渡概率来学习统一表示。</li>
<li>results: 实验结果表明，UGT 在真实世界 benchmark 数据上对多种下游任务进行了显著改进，并且达到了第三个 Weisfeiler-Lehman 同构测试（3d-WL）的表达力水平，能够分辨不同的图对。<details>
<summary>Abstract</summary>
Graph representation learning (GRL) methods, such as graph neural networks and graph transformer models, have been successfully used to analyze graph-structured data, mainly focusing on node classification and link prediction tasks. However, the existing studies mostly only consider local connectivity while ignoring long-range connectivity and the roles of nodes. In this paper, we propose Unified Graph Transformer Networks (UGT) that effectively integrate local and global structural information into fixed-length vector representations. First, UGT learns local structure by identifying the local substructures and aggregating features of the $k$-hop neighborhoods of each node. Second, we construct virtual edges, bridging distant nodes with structural similarity to capture the long-range dependencies. Third, UGT learns unified representations through self-attention, encoding structural distance and $p$-step transition probability between node pairs. Furthermore, we propose a self-supervised learning task that effectively learns transition probability to fuse local and global structural features, which could then be transferred to other downstream tasks. Experimental results on real-world benchmark datasets over various downstream tasks showed that UGT significantly outperformed baselines that consist of state-of-the-art models. In addition, UGT reaches the expressive power of the third-order Weisfeiler-Lehman isomorphism test (3d-WL) in distinguishing non-isomorphic graph pairs. The source code is available at https://github.com/NSLab-CUK/Unified-Graph-Transformer.
</details>
<details>
<summary>摘要</summary>
GRAPH 表示学习（GRL）方法，如图神经网络和图转换模型，已经成功地分析了图结构数据，主要集中在节点分类和链接预测任务上。然而，现有的研究通常只考虑当地连接性，忽略了远程连接性和节点的角色。在本文中，我们提出了统一图Transformer网络（UGT），可以有效地将本地和全局结构信息转化为固定长度 вектор表示。首先，UGT 通过识别节点的本地子结构并聚合 $k $- hop 邻居的特征来学习本地结构。其次，我们构建虚拟边，将远程节点相似的结构连接起来，以捕捉远程依赖关系。最后，UGT 通过自注意力学习，编码结构距离和 $p $- step 过渡概率 между节点对，学习统一表示。此外，我们提出了一种自动学习任务，可以有效地学习 transition probability，将本地和全局结构特征融合，可以转移到其他下游任务。实验结果表明，UGT 在真实世界 benchmark 数据集上对多种下游任务表现出色，并且达到了3d-WL isomorphism test 的表达力。代码可以在 https://github.com/NSLab-CUK/Unified-Graph-Transformer 中找到。
</details></li>
</ul>
<hr>
<h2 id="Spatial-LibriSpeech-An-Augmented-Dataset-for-Spatial-Audio-Learning"><a href="#Spatial-LibriSpeech-An-Augmented-Dataset-for-Spatial-Audio-Learning" class="headerlink" title="Spatial LibriSpeech: An Augmented Dataset for Spatial Audio Learning"></a>Spatial LibriSpeech: An Augmented Dataset for Spatial Audio Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09514">http://arxiv.org/abs/2308.09514</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/apple/ml-spatial-librispeech">https://github.com/apple/ml-spatial-librispeech</a></li>
<li>paper_authors: Miguel Sarabia, Elena Menyaylenko, Alessandro Toso, Skyler Seto, Zakaria Aldeneh, Shadi Pirhosseinloo, Luca Zappella, Barry-John Theobald, Nicholas Apostoloff, Jonathan Sheaffer</li>
<li>for: 这个论文是为了提供一个具有650小时19道声音的空间声音数据集，以及可选的干扰声音。这个数据集是为机器学习模型训练而设计。</li>
<li>methods: 这个论文使用了LibriSpeech样本的扩展，通过 simulated acoustic conditions 和8k+ synthetic rooms 生成了650个小时的声音数据。</li>
<li>results: 这个论文通过训练四个空间声音任务，实现了3D源localization的 median absolute error 为6.60°， distance 的 median absolute error 为0.43m， T30 的 median absolute error 为90.66ms，以及 DRR 的 median absolute error 为2.74dB。这些模型还可以在其他评估数据集上进行普适化。<details>
<summary>Abstract</summary>
We present Spatial LibriSpeech, a spatial audio dataset with over 650 hours of 19-channel audio, first-order ambisonics, and optional distractor noise. Spatial LibriSpeech is designed for machine learning model training, and it includes labels for source position, speaking direction, room acoustics and geometry. Spatial LibriSpeech is generated by augmenting LibriSpeech samples with 200k+ simulated acoustic conditions across 8k+ synthetic rooms. To demonstrate the utility of our dataset, we train models on four spatial audio tasks, resulting in a median absolute error of 6.60{\deg} on 3D source localization, 0.43m on distance, 90.66ms on T30, and 2.74dB on DRR estimation. We show that the same models generalize well to widely-used evaluation datasets, e.g., obtaining a median absolute error of 12.43{\deg} on 3D source localization on TUT Sound Events 2018, and 157.32ms on T30 estimation on ACE Challenge.
</details>
<details>
<summary>摘要</summary>
我们现在介绍Spatial LibriSpeech数据集，这是一个包含超过650小时19个通道音频的空间声音数据集，以及可选的干扰噪声。Spatial LibriSpeech是设计用于机器学习模型训练，其中包括源位置、说话方向、房间声学和几何学标签。Spatial LibriSpeech通过对LibriSpeech样本进行扩充，生成了200,000+个模拟的声音环境和8,000+个人工生成的房间。为了证明我们的数据集的实用性，我们在四个空间声音任务上训练了模型，导致了3D源localization的中值绝对误差为6.60度，距离为0.43米，T30为90.66毫秒，DRR估计为2.74dB。我们还证明了这些模型在广泛使用的评估数据集上也具有良好的泛化能力，例如在TUT Sound Events 2018中获得了12.43度的3D源localization中值绝对误差，并在ACE Challenge中获得了157.32毫秒的T30估计中值绝对误差。
</details></li>
</ul>
<hr>
<h2 id="Bridged-GNN-Knowledge-Bridge-Learning-for-Effective-Knowledge-Transfer"><a href="#Bridged-GNN-Knowledge-Bridge-Learning-for-Effective-Knowledge-Transfer" class="headerlink" title="Bridged-GNN: Knowledge Bridge Learning for Effective Knowledge Transfer"></a>Bridged-GNN: Knowledge Bridge Learning for Effective Knowledge Transfer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09499">http://arxiv.org/abs/2308.09499</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wendong Bi, Xueqi Cheng, Bingbing Xu, Xiaoqian Sun, Li Xu, Huawei Shen</li>
<li>for: 解决数据缺乏和低质量问题，帮助深度学习模型在TargetDomain上进行更好的泛化能力。</li>
<li>methods: 基于Graph Neural Networks (GNNs)的Knowledge Bridge Learning (KBL)方法，通过连接知识域的样本与TargetDomain中的样本建立知识桥，并通过GNN进行样本级别的知识传输。</li>
<li>results: 在不同的数据缺乏和数据质量问题下，Bridged-GNN对比SOTA方法具有显著的改善，能够更好地泛化到TargetDomain中。<details>
<summary>Abstract</summary>
The data-hungry problem, characterized by insufficiency and low-quality of data, poses obstacles for deep learning models. Transfer learning has been a feasible way to transfer knowledge from high-quality external data of source domains to limited data of target domains, which follows a domain-level knowledge transfer to learn a shared posterior distribution. However, they are usually built on strong assumptions, e.g., the domain invariant posterior distribution, which is usually unsatisfied and may introduce noises, resulting in poor generalization ability on target domains. Inspired by Graph Neural Networks (GNNs) that aggregate information from neighboring nodes, we redefine the paradigm as learning a knowledge-enhanced posterior distribution for target domains, namely Knowledge Bridge Learning (KBL). KBL first learns the scope of knowledge transfer by constructing a Bridged-Graph that connects knowledgeable samples to each target sample and then performs sample-wise knowledge transfer via GNNs.KBL is free from strong assumptions and is robust to noises in the source data. Guided by KBL, we propose the Bridged-GNN} including an Adaptive Knowledge Retrieval module to build Bridged-Graph and a Graph Knowledge Transfer module. Comprehensive experiments on both un-relational and relational data-hungry scenarios demonstrate the significant improvements of Bridged-GNN compared with SOTA methods
</details>
<details>
<summary>摘要</summary>
“问题Characterized by 缺乏和低质量数据，深度学习模型遇到了困难。将知识从高质量外部数据传递到有限数据的目标领域，通过域层知识传递，以学习共享 posterior distribution。然而，这些方法通常是基于强大的假设，例如域层知识 posterior distribution，这通常不充分满足，可能导致误差，影响了目标领域的数据整合能力。获取灵感自 Graph Neural Networks（GNNs），我们重新定义了这个概念为知识桥学习（KBL）。KBL首先学习知识传递的范围，然后通过 GNNs 进行样本别知识传递。KBL 不受强大的假设的限制，并具有较好的韧性。根据 KBL，我们提出了 Bridged-GNN，包括一个 Adaptive Knowledge Retrieval 模组和一个 Graph Knowledge Transfer 模组。实验结果显示，Bridged-GNN 与 State-of-the-Art 方法相比，在无关数据和关联数据的情况下具有杰出的改善。”
</details></li>
</ul>
<hr>
<h2 id="Predictive-Authoring-for-Brazilian-Portuguese-Augmentative-and-Alternative-Communication"><a href="#Predictive-Authoring-for-Brazilian-Portuguese-Augmentative-and-Alternative-Communication" class="headerlink" title="Predictive Authoring for Brazilian Portuguese Augmentative and Alternative Communication"></a>Predictive Authoring for Brazilian Portuguese Augmentative and Alternative Communication</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09497">http://arxiv.org/abs/2308.09497</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jayralencar/pictogram_prediction_pt">https://github.com/jayralencar/pictogram_prediction_pt</a></li>
<li>paper_authors: Jayr Pereira, Rodrigo Nogueira, Cleber Zanchettin, Robson Fidalgo</li>
<li>for: 这个论文旨在提出使用 BERTimbau 模型进行图像预测，以提高 AAC 系统的表达效果。</li>
<li>methods: 作者使用了 BERTimbau 模型，并构建了一个特定于巴西葡萄牙语的 AAC 训练集，以finetune BERTimbau。另外，作者还比较了不同的图像预测方法，包括使用图像定义、相关词语集和图像本身。</li>
<li>results: 结果显示，使用图像定义、相关词语集和图像本身的嵌入都有相似的表现，使用相关词语集得到最高的准确率，但使用图像定义得到最低的混淆率。这篇论文为使用 BERT-like 模型进行图像预测提供了新的想法，以及使用图像进行图像预测的潜在可能性。<details>
<summary>Abstract</summary>
Individuals with complex communication needs (CCN) often rely on augmentative and alternative communication (AAC) systems to have conversations and communique their wants. Such systems allow message authoring by arranging pictograms in sequence. However, the difficulty of finding the desired item to complete a sentence can increase as the user's vocabulary increases. This paper proposes using BERTimbau, a Brazilian Portuguese version of BERT, for pictogram prediction in AAC systems. To finetune BERTimbau, we constructed an AAC corpus for Brazilian Portuguese to use as a training corpus. We tested different approaches to representing a pictogram for prediction: as a word (using pictogram captions), as a concept (using a dictionary definition), and as a set of synonyms (using related terms). We also evaluated the usage of images for pictogram prediction. The results demonstrate that using embeddings computed from the pictograms' caption, synonyms, or definitions have a similar performance. Using synonyms leads to lower perplexity, but using captions leads to the highest accuracies. This paper provides insight into how to represent a pictogram for prediction using a BERT-like model and the potential of using images for pictogram prediction.
</details>
<details>
<summary>摘要</summary>
人们 WITH complex communication needs (CCN) 常常使用增强性和替代通信系统 (AAC) 来与他人交流和表达自己的需求。这些系统允许用户通过排序图ogram来编写消息。然而，如果用户的词汇量增加，则找到所需的图ogram可能会变得更加困难。这篇论文提议使用BERTimbau，一个基于 Brazilian Portuguese 的 BERT 模型，来预测图ogram。为了训练 BERTimbau，我们构建了一个 Brazilian Portuguese 的 AAC 训练集。我们测试了不同的图ogram表示方法来预测图ogram：作为单词 (使用图ogram的caption)、作为概念 (使用词典定义)、作为相关词 (使用相关 терminus)。我们还评估了使用图像来预测图ogram。结果表明，使用图ogram的caption、synonyms或定义来计算嵌入的性能类似。使用 synonyms 导致较低的混淆率，而使用 caption 导致最高的准确率。这篇论文为使用 BERT-like 模型来表示图ogram以及使用图像来预测图ogram提供了洞察和潜在的应用。
</details></li>
</ul>
<hr>
<h2 id="Balancing-Transparency-and-Risk-The-Security-and-Privacy-Risks-of-Open-Source-Machine-Learning-Models"><a href="#Balancing-Transparency-and-Risk-The-Security-and-Privacy-Risks-of-Open-Source-Machine-Learning-Models" class="headerlink" title="Balancing Transparency and Risk: The Security and Privacy Risks of Open-Source Machine Learning Models"></a>Balancing Transparency and Risk: The Security and Privacy Risks of Open-Source Machine Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09490">http://arxiv.org/abs/2308.09490</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dominik Hintersdorf, Lukas Struppek, Kristian Kersting</li>
<li>for: 本文旨在提醒读者关于使用开源机器学习模型时的隐私和安全风险。</li>
<li>methods: 本文使用实例来说明开源模型可能带来的隐私和安全风险，包括模型隐藏函数和特定输入模式触发的行为干扰。</li>
<li>results: 本文通过提高读者对开源模型使用的隐私和安全风险的认识，旨在促进负责任的AI系统使用。<details>
<summary>Abstract</summary>
The field of artificial intelligence (AI) has experienced remarkable progress in recent years, driven by the widespread adoption of open-source machine learning models in both research and industry. Considering the resource-intensive nature of training on vast datasets, many applications opt for models that have already been trained. Hence, a small number of key players undertake the responsibility of training and publicly releasing large pre-trained models, providing a crucial foundation for a wide range of applications. However, the adoption of these open-source models carries inherent privacy and security risks that are often overlooked. To provide a concrete example, an inconspicuous model may conceal hidden functionalities that, when triggered by specific input patterns, can manipulate the behavior of the system, such as instructing self-driving cars to ignore the presence of other vehicles. The implications of successful privacy and security attacks encompass a broad spectrum, ranging from relatively minor damage like service interruptions to highly alarming scenarios, including physical harm or the exposure of sensitive user data. In this work, we present a comprehensive overview of common privacy and security threats associated with the use of open-source models. By raising awareness of these dangers, we strive to promote the responsible and secure use of AI systems.
</details>
<details>
<summary>摘要</summary>
人工智能（AI）领域在最近几年内取得了很大进步，这主要归功于研究和实践中广泛采用的开源机器学习模型。由于训练大量数据需要巨量的资源，许多应用程序选择使用已经训练过的模型。因此，只有一小部分的关键玩家承担了训练和公共释放大型预训练模型的责任，这些模型提供了许多应用程序的基础。然而，使用这些开源模型的采用带来了内置的隐私和安全风险，这些风险通常被忽略。为了给出具体的例子，一个不引人注意的模型可能封装了隐藏的功能，当特定的输入模式触发时，可以 manipulate 系统的行为，如 instructing self-driving cars to ignore the presence of other vehicles。成功的隐私和安全攻击的后果覆盖广泛，从较轻的服务中断到极其警示的情况，包括物理损害或暴露敏感用户数据。在这项工作中，我们提供了对开源模型常见隐私和安全威胁的完整概述。我们希望通过提醒这些危险，推动AI系统的负责任和安全使用。
</details></li>
</ul>
<hr>
<h2 id="RBA-GCN-Relational-Bilevel-Aggregation-Graph-Convolutional-Network-for-Emotion-Recognition"><a href="#RBA-GCN-Relational-Bilevel-Aggregation-Graph-Convolutional-Network-for-Emotion-Recognition" class="headerlink" title="RBA-GCN: Relational Bilevel Aggregation Graph Convolutional Network for Emotion Recognition"></a>RBA-GCN: Relational Bilevel Aggregation Graph Convolutional Network for Emotion Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11029">http://arxiv.org/abs/2308.11029</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/luftmenscher/RBA-GCN">https://github.com/luftmenscher/RBA-GCN</a></li>
<li>paper_authors: Lin Yuan, Guoheng Huang, Fenghuan Li, Xiaochen Yuan, Chi-Man Pun, Guo Zhong<br>for:The paper is written for recognizing emotion in conversations (ERC) and addressing the limitations of traditional graph convolutional networks (GCNs) in capturing long-range contextual information and node discriminant information.methods:The paper proposes a novel approach called the relational bilevel aggregation graph convolutional network (RBA-GCN), which consists of three modules: the graph generation module (GGM), similarity-based cluster building module (SCBM), and bilevel aggregation module (BiAM).results:The RBA-GCN achieves a 2.17% to 5.21% improvement in weighted average F1 score over the most advanced method on the IEMOCAP and MELD datasets.<details>
<summary>Abstract</summary>
Emotion recognition in conversation (ERC) has received increasing attention from researchers due to its wide range of applications. As conversation has a natural graph structure, numerous approaches used to model ERC based on graph convolutional networks (GCNs) have yielded significant results. However, the aggregation approach of traditional GCNs suffers from the node information redundancy problem, leading to node discriminant information loss. Additionally, single-layer GCNs lack the capacity to capture long-range contextual information from the graph. Furthermore, the majority of approaches are based on textual modality or stitching together different modalities, resulting in a weak ability to capture interactions between modalities. To address these problems, we present the relational bilevel aggregation graph convolutional network (RBA-GCN), which consists of three modules: the graph generation module (GGM), similarity-based cluster building module (SCBM) and bilevel aggregation module (BiAM). First, GGM constructs a novel graph to reduce the redundancy of target node information. Then, SCBM calculates the node similarity in the target node and its structural neighborhood, where noisy information with low similarity is filtered out to preserve the discriminant information of the node. Meanwhile, BiAM is a novel aggregation method that can preserve the information of nodes during the aggregation process. This module can construct the interaction between different modalities and capture long-range contextual information based on similarity clusters. On both the IEMOCAP and MELD datasets, the weighted average F1 score of RBA-GCN has a 2.17$\sim$5.21\% improvement over that of the most advanced method.
</details>
<details>
<summary>摘要</summary>
研究者们对话情感识别（ERC）已经受到了越来越多的关注，因为它在各种应用场景中具有广泛的应用前景。由于对话自然具有图structure，许多方法基于图 convolutional networks（GCNs）来模型ERC已经取得了 significativ results。然而，传统GCNs的聚合方法受到节点信息纠纷问题的影响，导致节点特征信息的损失，同时单层GCNs缺乏捕捉长距离上下文信息的能力。此外，大多数方法都是基于文本模式或将不同模式粘合在一起，导致模式之间的交互 capture的能力弱化。为解决这些问题，我们提出了关系着色层聚合图 convolutional network（RBA-GCN），它包括三个模块：图生成模块（GGM），相似度基于团建模块（SCBM）和着色层聚合模块（BiAM）。首先，GGM构建了一个新的图，以减少目标节点信息的纠纷。然后，SCBM计算了目标节点和其结构邻域中的节点相似度，并将低相似度的信息排除，以保留节点特征信息。同时，BiAM是一种新的聚合方法，可以在聚合过程中保留节点信息。这个模块可以建立不同模式之间的交互，并基于相似团来捕捉长距离上下文信息。在IEMOCAP和MELD数据集上，RBA-GCN的权重平均F1分数与最先进方法相比提高了2.17%∼5.21%。
</details></li>
</ul>
<hr>
<h2 id="Data-augmentation-and-explainability-for-bias-discovery-and-mitigation-in-deep-learning"><a href="#Data-augmentation-and-explainability-for-bias-discovery-and-mitigation-in-deep-learning" class="headerlink" title="Data augmentation and explainability for bias discovery and mitigation in deep learning"></a>Data augmentation and explainability for bias discovery and mitigation in deep learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09464">http://arxiv.org/abs/2308.09464</a></li>
<li>repo_url: None</li>
<li>paper_authors: Agnieszka Mikołajczyk-Bareła</li>
<li>for: 这个论文探讨深度神经网络中的偏见问题，并提出了减少偏见影响模型性能的方法。</li>
<li>methods: 论文首先对数据和模型中的偏见和错误源进行分类和描述，特别是深度学习管道中的偏见。然后，提出了解释AI的概率分布来证明预测结果，并控制和改进模型。为了找到数据中的偏见，论文还提出了全球偏见标识方法。最后，论文提出了三种方法来减少偏见的影响：样式传递数据扩展、Targeted数据扩展和回归反馈。</li>
<li>results: 论文通过实验表明，使用样式传递数据扩展、Targeted数据扩展和回归反馈方法可以减少偏见对机器学习模型的影响。<details>
<summary>Abstract</summary>
This dissertation explores the impact of bias in deep neural networks and presents methods for reducing its influence on model performance. The first part begins by categorizing and describing potential sources of bias and errors in data and models, with a particular focus on bias in machine learning pipelines. The next chapter outlines a taxonomy and methods of Explainable AI as a way to justify predictions and control and improve the model. Then, as an example of a laborious manual data inspection and bias discovery process, a skin lesion dataset is manually examined. A Global Explanation for the Bias Identification method is proposed as an alternative semi-automatic approach to manual data exploration for discovering potential biases in data. Relevant numerical methods and metrics are discussed for assessing the effects of the identified biases on the model. Whereas identifying errors and bias is critical, improving the model and reducing the number of flaws in the future is an absolute priority. Hence, the second part of the thesis focuses on mitigating the influence of bias on ML models. Three approaches are proposed and discussed: Style Transfer Data Augmentation, Targeted Data Augmentations, and Attribution Feedback. Style Transfer Data Augmentation aims to address shape and texture bias by merging a style of a malignant lesion with a conflicting shape of a benign one. Targeted Data Augmentations randomly insert possible biases into all images in the dataset during the training, as a way to make the process random and, thus, destroy spurious correlations. Lastly, Attribution Feedback is used to fine-tune the model to improve its accuracy by eliminating obvious mistakes and teaching it to ignore insignificant input parts via an attribution loss. The goal of these approaches is to reduce the influence of bias on machine learning models, rather than eliminate it entirely.
</details>
<details>
<summary>摘要</summary>
As an example of a laborious manual data inspection and bias discovery process, a skin lesion dataset is manually examined. A Global Explanation for the Bias Identification method is proposed as a semi-automatic approach to discover potential biases in data. Relevant numerical methods and metrics are discussed for assessing the effects of identified biases on the model.The second part of the thesis focuses on mitigating the influence of bias on ML models. Three approaches are proposed and discussed:1. Style Transfer Data Augmentation: This approach aims to address shape and texture bias by merging a style of a malignant lesion with a conflicting shape of a benign one.2. Targeted Data Augmentations: This approach randomly inserts possible biases into all images in the dataset during training to destroy spurious correlations.3. Attribution Feedback: This approach fine-tunes the model to improve its accuracy by eliminating obvious mistakes and teaching it to ignore insignificant input parts via an attribution loss.The goal of these approaches is to reduce the influence of bias on machine learning models, rather than eliminate it entirely.
</details></li>
</ul>
<hr>
<h2 id="Reconstructing-S-matrix-Phases-with-Machine-Learning"><a href="#Reconstructing-S-matrix-Phases-with-Machine-Learning" class="headerlink" title="Reconstructing $S$-matrix Phases with Machine Learning"></a>Reconstructing $S$-matrix Phases with Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09451">http://arxiv.org/abs/2308.09451</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aurélien Dersy, Matthew D. Schwartz, Alexander Zhiboedov</li>
<li>for: 这个论文是关于 $S$-矩阵 Bootstrap 计划中关于模ulus 和阶的关系的研究。</li>
<li>methods: 作者使用现代机器学习技术来研究单位约束。他们发现，对于给定的模ulus，如果存在阶，则可以通过机器学习方法准确重建阶。此外，损失函数的优化问题可以作为单位约束是否成立的指标。</li>
<li>results: 作者发现，在允许多阶的情况下，模ulus 和阶之间存在一定的关系，并且可以使用机器学习方法来重建阶。此外，他们发现了一个新的阶 ambiguity 解，这将单位约束的已知上限提高了许多。<details>
<summary>Abstract</summary>
An important element of the $S$-matrix bootstrap program is the relationship between the modulus of an $S$-matrix element and its phase. Unitarity relates them by an integral equation. Even in the simplest case of elastic scattering, this integral equation cannot be solved analytically and numerical approaches are required. We apply modern machine learning techniques to studying the unitarity constraint. We find that for a given modulus, when a phase exists it can generally be reconstructed to good accuracy with machine learning. Moreover, the loss of the reconstruction algorithm provides a good proxy for whether a given modulus can be consistent with unitarity at all. In addition, we study the question of whether multiple phases can be consistent with a single modulus, finding novel phase-ambiguous solutions. In particular, we find a new phase-ambiguous solution which pushes the known limit on such solutions significantly beyond the previous bound.
</details>
<details>
<summary>摘要</summary>
<font face="宋体">$S$-Matrix bootstrap 程序中一个重要元素是$S$-Matrix元素的模ulus和其相位之间的关系。封闭性关系它们通过一个积分方程。même在最简单的射击反射过程中，这个积分方程无法 analytically解决，需要使用数值方法。我们使用现代机器学习技术来研究封闭性约束。我们发现，对于给定的模ulus，当一个相位存在时，通常可以使用机器学习来重建它到高度准确。此外，损失函数的损失函数可以作为一个给定模ulus是否能够符合封闭性的指标。此外，我们研究了一个给定模ulus是否能够有多个相位的问题，发现了新的相位不确定解。特别是，我们发现了一个新的相位不确定解，将之前的限制 pushed significantly beyond the previous bound.</font>Note: The translation is in Simplified Chinese, which is the standardized form of Chinese used in mainland China and Singapore. Traditional Chinese is used in Hong Kong, Taiwan, and other regions.
</details></li>
</ul>
<hr>
<h2 id="Logistics-Hub-Location-Optimization-A-K-Means-and-P-Median-Model-Hybrid-Approach-Using-Road-Network-Distances"><a href="#Logistics-Hub-Location-Optimization-A-K-Means-and-P-Median-Model-Hybrid-Approach-Using-Road-Network-Distances" class="headerlink" title="Logistics Hub Location Optimization: A K-Means and P-Median Model Hybrid Approach Using Road Network Distances"></a>Logistics Hub Location Optimization: A K-Means and P-Median Model Hybrid Approach Using Road Network Distances</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11038">http://arxiv.org/abs/2308.11038</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Abdul Rahman, Muhammad Aamir Basheer, Zubair Khalid, Muhammad Tahir, Momin Uppal</li>
<li>for: 优化各类物流总站的位置，以提高电商业务效益和减少碳脚印。</li>
<li>methods:  hybrid方法，首先使用K-Means聚合elivery点的空间位置，然后使用P-Median方法选择最佳物流总站。</li>
<li>results: 通过使用优化的物流总站位置，可以节省每个配送的距离815米（10%）。<details>
<summary>Abstract</summary>
Logistic hubs play a pivotal role in the last-mile delivery distance; even a slight increment in distance negatively impacts the business of the e-commerce industry while also increasing its carbon footprint. The growth of this industry, particularly after Covid-19, has further intensified the need for optimized allocation of resources in an urban environment. In this study, we use a hybrid approach to optimize the placement of logistic hubs. The approach sequentially employs different techniques. Initially, delivery points are clustered using K-Means in relation to their spatial locations. The clustering method utilizes road network distances as opposed to Euclidean distances. Non-road network-based approaches have been avoided since they lead to erroneous and misleading results. Finally, hubs are located using the P-Median method. The P-Median method also incorporates the number of deliveries and population as weights. Real-world delivery data from Muller and Phipps (M&P) is used to demonstrate the effectiveness of the approach. Serving deliveries from the optimal hub locations results in the saving of 815 (10%) meters per delivery.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Defending-Label-Inference-Attacks-in-Split-Learning-under-Regression-Setting"><a href="#Defending-Label-Inference-Attacks-in-Split-Learning-under-Regression-Setting" class="headerlink" title="Defending Label Inference Attacks in Split Learning under Regression Setting"></a>Defending Label Inference Attacks in Split Learning under Regression Setting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09448">http://arxiv.org/abs/2308.09448</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoze Qiu, Fei Zheng, Chaochao Chen, Xiaolin Zheng</li>
<li>for: 防止批处理学习中的标签推理攻击</li>
<li>methods: 提议Random Label Extension (RLE)和Model-based adaptive Label Extension (MLE)两种防御方法</li>
<li>results: 比基础防御方法更有效地降低攻击模型的性能，同时保持原始任务的性能<details>
<summary>Abstract</summary>
As a privacy-preserving method for implementing Vertical Federated Learning, Split Learning has been extensively researched. However, numerous studies have indicated that the privacy-preserving capability of Split Learning is insufficient. In this paper, we primarily focus on label inference attacks in Split Learning under regression setting, which are mainly implemented through the gradient inversion method. To defend against label inference attacks, we propose Random Label Extension (RLE), where labels are extended to obfuscate the label information contained in the gradients, thereby preventing the attacker from utilizing gradients to train an attack model that can infer the original labels. To further minimize the impact on the original task, we propose Model-based adaptive Label Extension (MLE), where original labels are preserved in the extended labels and dominate the training process. The experimental results show that compared to the basic defense methods, our proposed defense methods can significantly reduce the attack model's performance while preserving the original task's performance.
</details>
<details>
<summary>摘要</summary>
为保持隐私，垂直联合学习中的Split Learning方法已经受到了广泛研究。然而，许多研究表明，Split Learning的隐私保护能力不足。在这篇论文中，我们主要关注在 regression 设定下的标签推论攻击，这些攻击通常通过Gradient Inversion方法进行实现。为了防止标签推论攻击，我们提出了Random Label Extension（RLE），将标签扩展以隐藏标签信息含在梯度中，防止攻击者使用梯度训练一个可以推论原始标签的攻击模型。另外，我们提出了Model-based adaptive Label Extension（MLE），将原始标签保留在扩展标签中，使得训练过程中的标签保持原始任务的影响最小。实验结果显示，相比基本防御方法，我们提出的防御方法可以对攻击模型的性能产生重大降低，同时保持原始任务的性能。
</details></li>
</ul>
<hr>
<h2 id="An-Efficient-1-Iteration-Learning-Algorithm-for-Gaussian-Mixture-Model-And-Gaussian-Mixture-Embedding-For-Neural-Network"><a href="#An-Efficient-1-Iteration-Learning-Algorithm-for-Gaussian-Mixture-Model-And-Gaussian-Mixture-Embedding-For-Neural-Network" class="headerlink" title="An Efficient 1 Iteration Learning Algorithm for Gaussian Mixture Model And Gaussian Mixture Embedding For Neural Network"></a>An Efficient 1 Iteration Learning Algorithm for Gaussian Mixture Model And Gaussian Mixture Embedding For Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09444">http://arxiv.org/abs/2308.09444</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weiguo Lu, Xuan Wu, Deng Ding, Gangnan Yuan</li>
<li>for: 提出了一种基于Gaussian Mixture Model（GMM）学习算法，以便解决经典Expectation Maximization（EM）算法的缺点，包括缺乏稳定性和复杂性。</li>
<li>methods: 该算法基于我们之前的GMM扩展思想，具有更高的稳定性和简洁性，同时仅需一轮学习。我们也证明了该算法在任意参数初始化情况下都能够 converges。</li>
<li>results: 我们对经典概率层与GMM扩展方法进行比较，发现GMM扩展方法能够更好地应对数据不确定性和逆问题。此外，我们还测试了基于GMM的生成器，表明它具有可能应用于随机抽样和变量控制等领域的潜在能力。<details>
<summary>Abstract</summary>
We propose an Gaussian Mixture Model (GMM) learning algorithm, based on our previous work of GMM expansion idea. The new algorithm brings more robustness and simplicity than classic Expectation Maximization (EM) algorithm. It also improves the accuracy and only take 1 iteration for learning. We theoretically proof that this new algorithm is guarantee to converge regardless the parameters initialisation. We compare our GMM expansion method with classic probability layers in neural network leads to demonstrably better capability to overcome data uncertainty and inverse problem. Finally, we test GMM based generator which shows a potential to build further application that able to utilized distribution random sampling for stochastic variation as well as variation control.
</details>
<details>
<summary>摘要</summary>
我们提出了一种 Gaussian Mixture Model（GMM）学习算法，基于我们之前的 GMM 扩展思想。新算法比 классический Expectation Maximization（EM）算法更加稳定和简单，同时也提高了准确性，只需一次迭代学习。我们 teorically 证明了这种新算法是无论初始化参数都会收敛。我们对 класси probability layers 和 GMM expansion method 进行比较，发现后者在面临数据不确定和逆问题时表现更好，可以更好地适应不确定性和随机变化。最后，我们测试了 GMM 基于的生成器，发现它具有可以利用分布随机抽样和变量控制的潜在应用可能性。Note: Please keep in mind that the translation is a rough approximation and may not be perfect, as the nuances of the original text may be lost in translation.
</details></li>
</ul>
<hr>
<h2 id="From-Hope-to-Safety-Unlearning-Biases-of-Deep-Models-by-Enforcing-the-Right-Reasons-in-Latent-Space"><a href="#From-Hope-to-Safety-Unlearning-Biases-of-Deep-Models-by-Enforcing-the-Right-Reasons-in-Latent-Space" class="headerlink" title="From Hope to Safety: Unlearning Biases of Deep Models by Enforcing the Right Reasons in Latent Space"></a>From Hope to Safety: Unlearning Biases of Deep Models by Enforcing the Right Reasons in Latent Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09437">http://arxiv.org/abs/2308.09437</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maximilian Dreyer, Frederik Pahde, Christopher J. Anders, Wojciech Samek, Sebastian Lapuschkin<br>for:这篇论文旨在测试深度神经网络中隐藏的偏见，以及对于高风险决策的预测。methods:这篇论文使用了一种新的方法，通过减少模型对偏见的敏感性，以确保预测的正确性。这种方法基于概念活化向量，并且考虑了偏见的可Robustness。results:这篇论文在控制的和实际的设定下，成功地实现了对ISIC、Bone Age、ImageNet和CelebA dataset上的预测偏见的控制。使用了VGG、ResNet和EfficientNet的架构。<details>
<summary>Abstract</summary>
Deep Neural Networks are prone to learning spurious correlations embedded in the training data, leading to potentially biased predictions. This poses risks when deploying these models for high-stake decision-making, such as in medical applications. Current methods for post-hoc model correction either require input-level annotations, which are only possible for spatially localized biases, or augment the latent feature space, thereby hoping to enforce the right reasons. We present a novel method ensuring the right reasons on the concept level by reducing the model's sensitivity towards biases through the gradient. When modeling biases via Concept Activation Vectors, we highlight the importance of choosing robust directions, as traditional regression-based approaches such as Support Vector Machines tend to result in diverging directions. We effectively mitigate biases in controlled and real-world settings on the ISIC, Bone Age, ImageNet and CelebA datasets using VGG, ResNet and EfficientNet architectures.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Can-ultrasound-confidence-maps-predict-sonographers’-labeling-variability"><a href="#Can-ultrasound-confidence-maps-predict-sonographers’-labeling-variability" class="headerlink" title="Can ultrasound confidence maps predict sonographers’ labeling variability?"></a>Can ultrasound confidence maps predict sonographers’ labeling variability?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09433">http://arxiv.org/abs/2308.09433</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vanessa Gonzalez Duque, Leonhard Zirus, Yordanka Velikova, Nassir Navab, Diana Mateus</li>
<li>for: 本研究的目的是提高深度学习Segmentation方法的可靠性，使其能够更好地处理 ultrasound 图像中的各种物理原理导致的不确定性，如吸收、阴影和缺失边界。</li>
<li>methods: 本研究提出了一种新的方法，即使用 ultrasound 图像的可信度映射（CM）来引导深度学习Segmentation 网络，使其能够更好地考虑sonographers的不确定性，并生成更加准确的预测结果。</li>
<li>results: 研究结果表明，使用 ultrasound CMs 可以提高 Dice 分数、改善 Hausdorff 和平均表面距离，同时减少孤立像素预测。此外，研究还发现，使用 ultrasound CMs 可以提高对真实数据中困难 interpolations 的惩罚，从而提高 Segmentation 结果的可靠性。<details>
<summary>Abstract</summary>
Measuring cross-sectional areas in ultrasound images is a standard tool to evaluate disease progress or treatment response. Often addressed today with supervised deep-learning segmentation approaches, existing solutions highly depend upon the quality of experts' annotations. However, the annotation quality in ultrasound is anisotropic and position-variant due to the inherent physical imaging principles, including attenuation, shadows, and missing boundaries, commonly exacerbated with depth. This work proposes a novel approach that guides ultrasound segmentation networks to account for sonographers' uncertainties and generate predictions with variability similar to the experts. We claim that realistic variability can reduce overconfident predictions and improve physicians' acceptance of deep-learning cross-sectional segmentation solutions. Our method provides CM's certainty for each pixel for minimal computational overhead as it can be precalculated directly from the image. We show that there is a correlation between low values in the confidence maps and expert's label uncertainty. Therefore, we propose to give the confidence maps as additional information to the networks. We study the effect of the proposed use of ultrasound CMs in combination with four state-of-the-art neural networks and in two configurations: as a second input channel and as part of the loss. We evaluate our method on 3D ultrasound datasets of the thyroid and lower limb muscles. Our results show ultrasound CMs increase the Dice score, improve the Hausdorff and Average Surface Distances, and decrease the number of isolated pixel predictions. Furthermore, our findings suggest that ultrasound CMs improve the penalization of uncertain areas in the ground truth data, thereby improving problematic interpolations. Our code and example data will be made public at https://github.com/IFL-CAMP/Confidence-segmentation.
</details>
<details>
<summary>摘要</summary>
measuring cross-sectional areas in ultrasound images 是评估疾病进展或治疗响应的标准工具。现有的方法多数是由协调深度学习 segmentation 方法进行管理，但现有解决方案的质量很大程度取决于专家的注释。然而，在ultrasound中，注释质量是方均不同和位置 variant的，由于物理各种原理的约束，包括吸收、阴影和缺失边界，这些问题通常会在深度下恶化。这项工作提出了一种新的方法，使 ultrasound segmentation 网络能够考虑医生的不确定性，并生成与专家的预测相似的结果。我们认为，实际的不确定性可以减少过度的预测，并提高医生对深度学习横截 segmentation 解决方案的接受度。我们的方法可以在低计算开销下为每个像素提供CM的确定性，并且我们发现了图像中低值的CM confidence map 与专家的标签不确定性存在相关性。因此，我们提议将CM confidence map 作为网络的附加信息。我们在四种 state-of-the-art 神经网络中进行了研究，并在两种配置下评估了我们的方法：作为第二个输入通道和作为损失的一部分。我们在3D ultrasound 数据集上评估了我们的方法，并发现了以下结果： ultrasound CMs 可以提高 dice 分数，改善 Hausdorff 和平均表面距离，并减少孤立像素预测。此外，我们的发现表明， ultrasound CMs 可以优化地杂 interpolations 问题，从而提高疾病诊断的精度。我们的代码和示例数据将在 https://github.com/IFL-CAMP/Confidence-segmentation 上公开。
</details></li>
</ul>
<hr>
<h2 id="End-to-end-topographic-networks-as-models-of-cortical-map-formation-and-human-visual-behaviour-moving-beyond-convolutions"><a href="#End-to-end-topographic-networks-as-models-of-cortical-map-formation-and-human-visual-behaviour-moving-beyond-convolutions" class="headerlink" title="End-to-end topographic networks as models of cortical map formation and human visual behaviour: moving beyond convolutions"></a>End-to-end topographic networks as models of cortical map formation and human visual behaviour: moving beyond convolutions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09431">http://arxiv.org/abs/2308.09431</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zejin Lu, Adrien Doerig, Victoria Bosch, Bas Krahmer, Daniel Kaiser, Radoslaw M Cichy, Tim C Kietzmann</li>
<li>for: 理解 primate visual system 的几何组织结构的起源和功能</li>
<li>methods: 开发 All-Topographic Neural Networks (All-TNNs)，通过训练视觉输入获得 primate 视觉系统 的特征</li>
<li>results: All-TNNs 可以更好地适应人类视觉行为，并且可以在人类视觉中找到类别选择区域。<details>
<summary>Abstract</summary>
Computational models are an essential tool for understanding the origin and functions of the topographic organisation of the primate visual system. Yet, vision is most commonly modelled by convolutional neural networks that ignore topography by learning identical features across space. Here, we overcome this limitation by developing All-Topographic Neural Networks (All-TNNs). Trained on visual input, several features of primate topography emerge in All-TNNs: smooth orientation maps and cortical magnification in their first layer, and category-selective areas in their final layer. In addition, we introduce a novel dataset of human spatial biases in object recognition, which enables us to directly link models to behaviour. We demonstrate that All-TNNs significantly better align with human behaviour than previous state-of-the-art convolutional models due to their topographic nature. All-TNNs thereby mark an important step forward in understanding the spatial organisation of the visual brain and how it mediates visual behaviour.
</details>
<details>
<summary>摘要</summary>
计算模型是Primates视觉系统的起源和功能的重要工具。然而，视觉通常是使用 convolutional neural networks（ConvNets）来模型，这些ConvNets忽略了地理学的特征，通过学习同样的特征在空间上。在这里，我们超越这些限制，开发了All-Topographic Neural Networks（All-TNNs）。通过视觉输入，All-TNNs中出现了许多Primates的特征：平滑的方向图和 cortical magnification在其第一层，以及在其最后层中的类别特异区域。此外，我们还提出了一个新的人类空间偏好对象识别的数据集，该数据集允许我们直接将模型与行为相连。我们示出了All-TNNs在人类行为方面比前一代 convolutional 模型更好地适应，这是因为All-TNNs具有地理学性质。All-TNNs因此标志着理解视觉大脑的空间组织和如何通过视觉行为的传递重要一步进展。
</details></li>
</ul>
<hr>
<h2 id="Towards-Understanding-the-Generalizability-of-Delayed-Stochastic-Gradient-Descent"><a href="#Towards-Understanding-the-Generalizability-of-Delayed-Stochastic-Gradient-Descent" class="headerlink" title="Towards Understanding the Generalizability of Delayed Stochastic Gradient Descent"></a>Towards Understanding the Generalizability of Delayed Stochastic Gradient Descent</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09430">http://arxiv.org/abs/2308.09430</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoge Deng, Li Shen, Shengwei Li, Tao Sun, Dongsheng Li, Dacheng Tao</li>
<li>for: This paper investigates the generalization performance of asynchronous stochastic gradient descent (SGD) and provides sharper generalization error bounds.</li>
<li>methods: The paper uses generating function analysis to establish the average stability of the delayed gradient algorithm and derives upper bounds on the generalization error.</li>
<li>results: The theoretical results show that asynchronous delays can reduce the generalization error of the delayed SGD algorithm, and the experimental results validate these findings.Here’s the text in Simplified Chinese:</li>
<li>for: 这篇论文研究了异步梯度下降（SGD）的通用性表现，并提供了更为精细的通用错误 bound。</li>
<li>methods: 论文使用生成函数分析来确定延迟梯度算法的平均稳定性，并从这种稳定性得到了通用错误 bound。</li>
<li>results: 理论结果表明，异步延迟可以降低延迟SGD算法的通用错误，实验结果也证实了这些结论。<details>
<summary>Abstract</summary>
Stochastic gradient descent (SGD) performed in an asynchronous manner plays a crucial role in training large-scale machine learning models. However, the generalization performance of asynchronous delayed SGD, which is an essential metric for assessing machine learning algorithms, has rarely been explored. Existing generalization error bounds are rather pessimistic and cannot reveal the correlation between asynchronous delays and generalization. In this paper, we investigate sharper generalization error bound for SGD with asynchronous delay $\tau$. Leveraging the generating function analysis tool, we first establish the average stability of the delayed gradient algorithm. Based on this algorithmic stability, we provide upper bounds on the generalization error of $\tilde{\mathcal{O}}(\frac{T-\tau}{n\tau})$ and $\tilde{\mathcal{O}}(\frac{1}{n})$ for quadratic convex and strongly convex problems, respectively, where $T$ refers to the iteration number and $n$ is the amount of training data. Our theoretical results indicate that asynchronous delays reduce the generalization error of the delayed SGD algorithm. Analogous analysis can be generalized to the random delay setting, and the experimental results validate our theoretical findings.
</details>
<details>
<summary>摘要</summary>
<!--language_loops_enabled: false; --> Stochastic gradient descent（SGD）在异步方式下扮演着关键的角色在训练大规模机器学习模型中。然而，异步延迟SGD的总体化性表现，即机器学习算法的一个关键度量，很少被探讨。现有的总体错误约束很偏袋、无法描述异步延迟和总体化性之间的相互关系。在这篇论文中，我们调查了SGD异步延迟$\tau$的更精确的总体化性误差约束。通过生成函数分析工具，我们首先确立延迟梯度算法的平均稳定性。基于这种算法稳定性，我们提供了异步延迟SGD的总体化性误差的Upper bound，其中$T$表示迭代次数，$n$表示训练数据量。我们的理论结果表明，异步延迟可以降低延迟SGD算法的总体化性误差。可以将这种分析推广到随机延迟设置下，并且实验结果证明了我们的理论发现。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Single-Image-Deconvolution-with-Siamese-Neural-Networks"><a href="#Self-Supervised-Single-Image-Deconvolution-with-Siamese-Neural-Networks" class="headerlink" title="Self-Supervised Single-Image Deconvolution with Siamese Neural Networks"></a>Self-Supervised Single-Image Deconvolution with Siamese Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09426">http://arxiv.org/abs/2308.09426</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mikhail Papkov, Kaupo Palo, Leopold Parts</li>
<li>for:  Image reconstruction from inverse problems, specifically in 3D microscopy deconvolution tasks.</li>
<li>methods:  Deep learning methods with self-supervised blind-spot neural networks, combined with Fast Fourier Transform convolutions and Siamese invariance loss.</li>
<li>results:  Outperforms previous state-of-the-art deconvolution methods with known point spread function.<details>
<summary>Abstract</summary>
Inverse problems in image reconstruction are fundamentally complicated by unknown noise properties. Classical iterative deconvolution approaches amplify noise and require careful parameter selection for an optimal trade-off between sharpness and grain. Deep learning methods allow for flexible parametrization of the noise and learning its properties directly from the data. Recently, self-supervised blind-spot neural networks were successfully adopted for image deconvolution by including a known point-spread function in the end-to-end training. However, their practical application has been limited to 2D images in the biomedical domain because it implies large kernels that are poorly optimized. We tackle this problem with Fast Fourier Transform convolutions that provide training speed-up in 3D microscopy deconvolution tasks. Further, we propose to adopt a Siamese invariance loss for deconvolution and empirically identify its optimal position in the neural network between blind-spot and full image branches. The experimental results show that our improved framework outperforms the previous state-of-the-art deconvolution methods with a known point spread function.
</details>
<details>
<summary>摘要</summary>
<<SYS>> tranlate("Inverse problems in image reconstruction are fundamentally complicated by unknown noise properties. Classical iterative deconvolution approaches amplify noise and require careful parameter selection for an optimal trade-off between sharpness and grain. Deep learning methods allow for flexible parametrization of the noise and learning its properties directly from the data. Recently, self-supervised blind-spot neural networks were successfully adopted for image deconvolution by including a known point-spread function in the end-to-end training. However, their practical application has been limited to 2D images in the biomedical domain because it implies large kernels that are poorly optimized. We tackle this problem with Fast Fourier Transform convolutions that provide training speed-up in 3D microscopy deconvolution tasks. Further, we propose to adopt a Siamese invariance loss for deconvolution and empirically identify its optimal position in the neural network between blind-spot and full image branches. The experimental results show that our improved framework outperforms the previous state-of-the-art deconvolution methods with a known point spread function.")</SYS>>Here's the translation:“逆问题在图像重建中是基础上受到未知噪音特性的影响。经典迭代复原方法会增加噪音，并需要精确地选择参数以进行适当的对比 between sharpness 和 grain。深度学习方法允许对噪音进行可调 parametrization，并直接从数据中学习噪音的特性。现在，自适应盲点神经网络已经成功地应用于图像复原中。然而，它们的实际应用受到2D图像在生物医学领域的限制，因为它们假设了大的核心，对于复原效果不够优化。我们使用 Fast Fourier Transform 核函数来提高训练速度，并提出采用 Siamese 不变损函数来复原，并在神经网络中对复原效果进行实验性地调整。实验结果显示，我们的改进的框架比前一代的复原方法 with 知名点评函数更好。”
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Agent-Communication-and-Learning-through-Action-and-Language"><a href="#Enhancing-Agent-Communication-and-Learning-through-Action-and-Language" class="headerlink" title="Enhancing Agent Communication and Learning through Action and Language"></a>Enhancing Agent Communication and Learning through Action and Language</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10842">http://arxiv.org/abs/2308.10842</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Caselles-Dupré Hugo, Sigaud Olivier, Chetouani Mohamed</li>
<li>for: 这篇论文是为了探讨一种新的GC-代理人，它可以作为教师和学习者而设计的。</li>
<li>methods: 论文使用了动作示例和语言指令，以提高人工智能交互的效率。同时，它还研究了在人类交流中的教学和目的实现中的pedagogy和 Pragmatism的应用，以提高代理人的教学和学习能力。</li>
<li>results: 论文发现，将交通方式（动作和语言）相结合可以提高学习效果，并且提出了一种多Modal的交互方式。<details>
<summary>Abstract</summary>
We introduce a novel category of GC-agents capable of functioning as both teachers and learners. Leveraging action-based demonstrations and language-based instructions, these agents enhance communication efficiency. We investigate the incorporation of pedagogy and pragmatism, essential elements in human communication and goal achievement, enhancing the agents' teaching and learning capabilities. Furthermore, we explore the impact of combining communication modes (action and language) on learning outcomes, highlighting the benefits of a multi-modal approach.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种新的GC-代理人，能够同时作为教师和学生 function。通过行动示例和语言指令，这些代理人提高了交流效率。我们研究了人类communication和目标实现中的pedagogy和pragmatism元素，以提高代理人的教学和学习能力。此外，我们还探讨了 combining communication modes（行动和语言）对学习成果的影响，指出多模式approach的 beneficial effects。
</details></li>
</ul>
<hr>
<h2 id="ICU-Mortality-Prediction-Using-Long-Short-Term-Memory-Networks"><a href="#ICU-Mortality-Prediction-Using-Long-Short-Term-Memory-Networks" class="headerlink" title="ICU Mortality Prediction Using Long Short-Term Memory Networks"></a>ICU Mortality Prediction Using Long Short-Term Memory Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12800">http://arxiv.org/abs/2308.12800</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manel Mili, Asma Kerkeni, Asma Ben Abdallah, Mohamed Hedi Bedoui</li>
<li>for: 这个论文是为了提出一种自动化数据驱动系统，用于分析医疗电子记录（EHRs）中的大量多变量时间数据，并提取高级信息以预测医院死亡率和Length of Stay（LOS）的早期预测。</li>
<li>methods: 本研究使用了Long Short-Term Memory（LSTM）网络，通过减少时间框架到6小时，以提高临床任务的可靠性。</li>
<li>results: 实验结果表明，LSTM模型可以rigorously multivariate时间序列测量来建立真实世界预测引擎。<details>
<summary>Abstract</summary>
Extensive bedside monitoring in Intensive Care Units (ICUs) has resulted in complex temporal data regarding patient physiology, which presents an upscale context for clinical data analysis. In the other hand, identifying the time-series patterns within these data may provide a high aptitude to predict clinical events. Hence, we investigate, during this work, the implementation of an automatic data-driven system, which analyzes large amounts of multivariate temporal data derived from Electronic Health Records (EHRs), and extracts high-level information so as to predict in-hospital mortality and Length of Stay (LOS) early. Practically, we investigate the applicability of LSTM network by reducing the time-frame to 6-hour so as to enhance clinical tasks. The experimental results highlight the efficiency of LSTM model with rigorous multivariate time-series measurements for building real-world prediction engines.
</details>
<details>
<summary>摘要</summary>
延伸床side监测在重症监护室(ICU)中已经导致了复杂的时间序列数据，这些数据提供了丰富的临床数据分析上下文。然而，在这些数据中寻找时间序列模式可能提供高度预测临床事件的可能性。因此，在这项工作中，我们调查了一个自动化数据驱动系统，该系统分析大量多变量时间序列数据来自电子医疗纪录(EHR)，并提取高级信息以预测医院死亡率和 lengths of stay (LOS)的早期预测。实际上，我们研究了使用LSTM网络，通过减少时间帧为6小时来增强临床任务。实验结果表明LSTM模型在多变量时间序列测量上具有强大的预测能力。
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-Solutions-for-the-Analysis-of-Single-Particle-Diffusion-Trajectories"><a href="#Machine-Learning-Solutions-for-the-Analysis-of-Single-Particle-Diffusion-Trajectories" class="headerlink" title="Machine-Learning Solutions for the Analysis of Single-Particle Diffusion Trajectories"></a>Machine-Learning Solutions for the Analysis of Single-Particle Diffusion Trajectories</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09414">http://arxiv.org/abs/2308.09414</a></li>
<li>repo_url: None</li>
<li>paper_authors: Henrik Seckler, Janusz Szwabinski, Ralf Metzler</li>
<li>for: 这篇论文是关于解读单个分子、细胞或动物的扩散运动的轨迹记录，以及利用现代机器学习技术解释这些记录的动力学机制。</li>
<li>methods: 这篇论文介绍了最新的机器学习技术，包括在异常扩散挑战中取得成功的方法，以及包括不确定性估计和特征基于的方法，以提高解释性和提供具体的学习过程信息。</li>
<li>results: 这篇论文的结果表明，这些新的机器学习方法可以成功地解释单个分子、细胞或动物的扩散运动，并且可以提供具体的系统参数和不确定性估计。<details>
<summary>Abstract</summary>
Single-particle traces of the diffusive motion of molecules, cells, or animals are by-now routinely measured, similar to stochastic records of stock prices or weather data. Deciphering the stochastic mechanism behind the recorded dynamics is vital in understanding the observed systems. Typically, the task is to decipher the exact type of diffusion and/or to determine system parameters. The tools used in this endeavor are currently revolutionized by modern machine-learning techniques. In this Perspective we provide an overview over recently introduced methods in machine-learning for diffusive time series, most notably, those successfully competing in the Anomalous-Diffusion-Challenge. As such methods are often criticized for their lack of interpretability, we focus on means to include uncertainty estimates and feature-based approaches, both improving interpretability and providing concrete insight into the learning process of the machine. We expand the discussion by examining predictions on different out-of-distribution data. We also comment on expected future developments.
</details>
<details>
<summary>摘要</summary>
一个粒子轨迹的游移，包括分子、细胞或动物的游移，现在可以通过单个粒子轨迹来评估，类似于随机记录的股票价格或天气数据。解释记录的随机机制是理解所观察系统的关键。通常，任务是确定扩散的类型和/或系统参数。现代机器学习技术已经对这些工具进行了革命性的改进。在这篇观点文章中，我们提供了最近引入的机器学习方法 для游移时间序列的概述，其中许多方法在解释性方面受到批判。因此，我们将重点介绍包括不确定性估计和特征基本方法在内的方法，以提高解释性和提供机器学习过程的具体信息。此外，我们还探讨了不同的外部数据集预测结果，以及未来发展的预期。
</details></li>
</ul>
<hr>
<h2 id="Metadata-Improves-Segmentation-Through-Multitasking-Elicitation"><a href="#Metadata-Improves-Segmentation-Through-Multitasking-Elicitation" class="headerlink" title="Metadata Improves Segmentation Through Multitasking Elicitation"></a>Metadata Improves Segmentation Through Multitasking Elicitation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09411">http://arxiv.org/abs/2308.09411</a></li>
<li>repo_url: None</li>
<li>paper_authors: Iaroslav Plutenko, Mikhail Papkov, Kaupo Palo, Leopold Parts, Dmytro Fishman</li>
<li>for: 这个论文主要针对的是使用metadata进行深度学习方法的semantic segmentation任务。</li>
<li>methods: 这个论文使用了通道调节机制，将metadata作为 convolutional neural network 的输入，以提高semantic segmentation的结果。</li>
<li>results: 论文表明，通过使用metadata作为输入，可以提高semantic segmentation的结果，同时实现可靠地和经济地实现。此外，论文还explores了metadata驱动的系统的特点和优点。<details>
<summary>Abstract</summary>
Metainformation is a common companion to biomedical images. However, this potentially powerful additional source of signal from image acquisition has had limited use in deep learning methods, for semantic segmentation in particular. Here, we incorporate metadata by employing a channel modulation mechanism in convolutional networks and study its effect on semantic segmentation tasks. We demonstrate that metadata as additional input to a convolutional network can improve segmentation results while being inexpensive in implementation as a nimble add-on to popular models. We hypothesize that this benefit of metadata can be attributed to facilitating multitask switching. This aspect of metadata-driven systems is explored and discussed in detail.
</details>
<details>
<summary>摘要</summary>
这里的 metadata 是医学影像的常见附属资讯。然而，这个潜在强大的资料来源在深度学习方法中对于 semantic segmentation 仍然有限的使用。在这里，我们通过运用槽模组化机制在 convolutional network 中使用 metadata，并研究其对 semantic segmentation 任务的影响。我们发现，Metadata 作为 convolutional network 的额外输入，可以提高 segmentation 结果，而且实现起来相对便宜。我们推测这个优点可以归于metadata 帮助多任务转换。这个方面的metadata-driven系统的细节和讨论，在这里得到了详细的探讨。
</details></li>
</ul>
<hr>
<h2 id="Learning-MDL-logic-programs-from-noisy-data"><a href="#Learning-MDL-logic-programs-from-noisy-data" class="headerlink" title="Learning MDL logic programs from noisy data"></a>Learning MDL logic programs from noisy data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09393">http://arxiv.org/abs/2308.09393</a></li>
<li>repo_url: None</li>
<li>paper_authors: Céline Hocquette, Andreas Niskanen, Matti Järvisalo, Andrew Cropper</li>
<li>for: 学习从含噪数据中学习程序，增强 inductive logic programming 的能力。</li>
<li>methods: 使用 minimal description length 方法，可以学习含 recursive 程序的含噪数据。</li>
<li>results: 在多个领域（包括药物设计、游戏撸机和程序生成）中，我们的方法可以超过现有方法的预测精度，并可以扩展到moderate 量的噪音。<details>
<summary>Abstract</summary>
Many inductive logic programming approaches struggle to learn programs from noisy data. To overcome this limitation, we introduce an approach that learns minimal description length programs from noisy data, including recursive programs. Our experiments on several domains, including drug design, game playing, and program synthesis, show that our approach can outperform existing approaches in terms of predictive accuracies and scale to moderate amounts of noise.
</details>
<details>
<summary>摘要</summary>
很多逻辑学习程序方法在含噪数据上学习困难，以往我们提出了一种学习最短描述长度程序从含噪数据中学习，包括循环程序。我们在多个领域进行了实验，包括药物设计、游戏玩家和程序生成，结果表明我们的方法可以在适度噪音下达到更高的预测精度和规模。
</details></li>
</ul>
<hr>
<h2 id="FunQuant-A-R-package-to-perform-quantization-in-the-context-of-rare-events-and-time-consuming-simulations"><a href="#FunQuant-A-R-package-to-perform-quantization-in-the-context-of-rare-events-and-time-consuming-simulations" class="headerlink" title="FunQuant: A R package to perform quantization in the context of rare events and time-consuming simulations"></a>FunQuant: A R package to perform quantization in the context of rare events and time-consuming simulations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10871">http://arxiv.org/abs/2308.10871</a></li>
<li>repo_url: None</li>
<li>paper_authors: Charlie Sire, Yann Richet, Rodolphe Le Riche, Didier Rullière, Jérémy Rohmer, Lucie Pheulpin</li>
<li>for: 本研究旨在提出一种新的数据量化方法，以解决数据评估成本高、罕见事件的问题。</li>
<li>methods: 本研究使用 Lloyd 算法来partition 空间，并基于 Voronoi 维度和概率质量来构建一个离散分布。但是，在数据评估成本高、罕见事件的场景下，Lloyd 算法难以实现高精度。因此，本研究还提出了一种元模型和适应采样方法来提高罕见 cluster 的计算精度。</li>
<li>results: 本研究的结果表明，采用元模型和适应采样方法可以提高罕见 cluster 的计算精度，并且可以降低数据评估成本。同时，本研究还发现了一些可能的应用场景，例如在机器学习和数据挖掘等领域。<details>
<summary>Abstract</summary>
Quantization summarizes continuous distributions by calculating a discrete approximation. Among the widely adopted methods for data quantization is Lloyd's algorithm, which partitions the space into Vorono\"i cells, that can be seen as clusters, and constructs a discrete distribution based on their centroids and probabilistic masses. Lloyd's algorithm estimates the optimal centroids in a minimal expected distance sense, but this approach poses significant challenges in scenarios where data evaluation is costly, and relates to rare events. Then, the single cluster associated to no event takes the majority of the probability mass. In this context, a metamodel is required and adapted sampling methods are necessary to increase the precision of the computations on the rare clusters.
</details>
<details>
<summary>摘要</summary>
量化概率分布的目的是计算一个粗略的抽象。广泛采用的数据量化方法之一是朗道算法，该算法将空间分成Voronoi细胞，可以看作归一化的集群，并根据其中心和概率质量来构建一个离散分布。朗道算法估算最佳中心，但这种方法在数据评估成本高、 relate to rare events 的场景下存在很大挑战。在这种情况下，一个méta模型是必要的，并且采用适当的采样方法可以提高罕见集群的计算精度。Note: "概率质量" (probability mass) is translated as "概率质量" in Simplified Chinese, but it should be noted that this term is not commonly used in Chinese and may not be well understood by some readers. A more common term used in Chinese to refer to the probability of a cluster is "集群概率" (cluster probability).
</details></li>
</ul>
<hr>
<h2 id="On-Gradient-like-Explanation-under-a-Black-box-Setting-When-Black-box-Explanations-Become-as-Good-as-White-box"><a href="#On-Gradient-like-Explanation-under-a-Black-box-Setting-When-Black-box-Explanations-Become-as-Good-as-White-box" class="headerlink" title="On Gradient-like Explanation under a Black-box Setting: When Black-box Explanations Become as Good as White-box"></a>On Gradient-like Explanation under a Black-box Setting: When Black-box Explanations Become as Good as White-box</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09381">http://arxiv.org/abs/2308.09381</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yi Cai, Gerhard Wunder</li>
<li>for: 本文旨在提供一种基于梯度估计的解释方法，以解释数据驱动方法的决策过程中的特征对应关系。</li>
<li>methods: 本文提出了一种名为GEEX的解释方法，该方法在黑盒Setting下提供了梯度类型的解释，并将该方法与路径方法集成，得到了iGEEX（集成GEEX）方法。</li>
<li>results: 实验表明，提出的方法在图像数据上表现出excelsior的效果，并且与状态艺术方法相比，具有更高的可靠性和灵活性。<details>
<summary>Abstract</summary>
Attribution methods shed light on the explainability of data-driven approaches such as deep learning models by revealing the most contributing features to decisions that have been made. A widely accepted way of deriving feature attributions is to analyze the gradients of the target function with respect to input features. Analysis of gradients requires full access to the target system, meaning that solutions of this kind treat the target system as a white-box. However, the white-box assumption may be untenable due to security and safety concerns, thus limiting their practical applications. As an answer to the limited flexibility, this paper presents GEEX (gradient-estimation-based explanation), an explanation method that delivers gradient-like explanations under a black-box setting. Furthermore, we integrate the proposed method with a path method. The resulting approach iGEEX (integrated GEEX) satisfies the four fundamental axioms of attribution methods: sensitivity, insensitivity, implementation invariance, and linearity. With a focus on image data, the exhaustive experiments empirically show that the proposed methods outperform state-of-the-art black-box methods and achieve competitive performance compared to the ones with full access.
</details>
<details>
<summary>摘要</summary>
<<SYS>>传递方法可以描述数据驱动方法中的解释性，例如深度学习模型的决策过程中的最重要的特征。一种广泛接受的解释特征的 derivation 方法是分析目标函数对输入特征的梯度。这种方法需要对目标系统具有完整的访问权，因此被称为白盒模型。然而，白盒假设可能存在安全和安全问题，因此它们在实际应用中有限制。为了解决这些限制，本文提出了 GEEX（梯度估计基于解释），一种解释方法，可以在黑盒设定下提供梯度类似的解释。此外，我们将 GEEX 与路径方法集成，得到 iGEEX（集成 GEEX）。这种方法满足了解释方法的四个基本假设：敏感性、不敏感性、实现不变性和线性。对于图像数据，我们进行了详细的实验，并证明了提案的方法在黑盒设定下比状态eliaoning的黑盒方法表现更好，并且与完整访问下的方法具有竞争性。Note: The translation is in Simplified Chinese, which is the standard Chinese writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Deciphering-knee-osteoarthritis-diagnostic-features-with-explainable-artificial-intelligence-A-systematic-review"><a href="#Deciphering-knee-osteoarthritis-diagnostic-features-with-explainable-artificial-intelligence-A-systematic-review" class="headerlink" title="Deciphering knee osteoarthritis diagnostic features with explainable artificial intelligence: A systematic review"></a>Deciphering knee osteoarthritis diagnostic features with explainable artificial intelligence: A systematic review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09380">http://arxiv.org/abs/2308.09380</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yun Xin Teoh, Alice Othmani, Siew Li Goh, Juliana Usman, Khin Wee Lai</li>
<li>for: 提高膝关节炎诊断的可靠性和可读性</li>
<li>methods: 使用解释性人工智能技术来提高膝关节炎诊断的可信度和解释性</li>
<li>results: 提供了膝关节炎诊断领域中解释性人工智能技术的首次报告，并从数据可解性和模型可解性两个角度进行了讨论，以便促进这种技术在临床实践中的采用<details>
<summary>Abstract</summary>
Existing artificial intelligence (AI) models for diagnosing knee osteoarthritis (OA) have faced criticism for their lack of transparency and interpretability, despite achieving medical-expert-like performance. This opacity makes them challenging to trust in clinical practice. Recently, explainable artificial intelligence (XAI) has emerged as a specialized technique that can provide confidence in the model's prediction by revealing how the prediction is derived, thus promoting the use of AI systems in healthcare. This paper presents the first survey of XAI techniques used for knee OA diagnosis. The XAI techniques are discussed from two perspectives: data interpretability and model interpretability. The aim of this paper is to provide valuable insights into XAI's potential towards a more reliable knee OA diagnosis approach and encourage its adoption in clinical practice.
</details>
<details>
<summary>摘要</summary>
现有的膝关节滑块病（OA）诊断模型（AI）已经受到了不透明性和解释性的批评，即使它们达到了医学专家水平的性能。这种透明性使得它们在临床实践中具有挑战性。在最近，解释性人工智能（XAI）已经emerged as a specialized technique，可以为诊断提供信任度，揭示如何 derive 预测结果，从而推动人工智能系统在医疗领域的应用。本文是膝关节滑块诊断领域中第一篇XAI技术survey。XAI技术从两个角度进行了讨论：数据可读性和模型可读性。本文的目的是提供有价值的情况，推动XAI在诊断方法中的采用，并且 Encourage its adoption in clinical practice.
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Techniques-in-Extreme-Weather-Events-A-Review"><a href="#Deep-Learning-Techniques-in-Extreme-Weather-Events-A-Review" class="headerlink" title="Deep Learning Techniques in Extreme Weather Events: A Review"></a>Deep Learning Techniques in Extreme Weather Events: A Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10995">http://arxiv.org/abs/2308.10995</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shikha Verma, Kuldeep Srivastava, Akhilesh Tiwari, Shekhar Verma</li>
<li>for: This paper provides a comprehensive overview of the state-of-the-art deep learning techniques in the field of meteorology, specifically for accurate analysis and precise forecasting of extreme weather events.</li>
<li>methods: The paper explores the utilization of deep learning architectures in various aspects of weather prediction, including thunderstorm, lightning, precipitation, drought, heatwave, cold waves, and tropical cyclones.</li>
<li>results: The paper highlights the potential of deep learning techniques to capture complex patterns and non-linear relationships in weather data, and discusses the limitations of current approaches and future directions for advancements in the field of meteorology.<details>
<summary>Abstract</summary>
Extreme weather events pose significant challenges, thereby demanding techniques for accurate analysis and precise forecasting to mitigate its impact. In recent years, deep learning techniques have emerged as a promising approach for weather forecasting and understanding the dynamics of extreme weather events. This review aims to provide a comprehensive overview of the state-of-the-art deep learning in the field. We explore the utilization of deep learning architectures, across various aspects of weather prediction such as thunderstorm, lightning, precipitation, drought, heatwave, cold waves and tropical cyclones. We highlight the potential of deep learning, such as its ability to capture complex patterns and non-linear relationships. Additionally, we discuss the limitations of current approaches and highlight future directions for advancements in the field of meteorology. The insights gained from this systematic review are crucial for the scientific community to make informed decisions and mitigate the impacts of extreme weather events.
</details>
<details>
<summary>摘要</summary>
extreme weather events pose significant challenges, requiring accurate analysis and precise forecasting to mitigate their impact. In recent years, deep learning techniques have emerged as a promising approach for weather forecasting and understanding the dynamics of extreme weather events. this review aims to provide a comprehensive overview of the state-of-the-art deep learning in the field. We explore the utilization of deep learning architectures across various aspects of weather prediction, such as thunderstorms, lightning, precipitation, droughts, heatwaves, cold waves, and tropical cyclones. we highlight the potential of deep learning, such as its ability to capture complex patterns and non-linear relationships. Additionally, we discuss the limitations of current approaches and highlight future directions for advancements in the field of meteorology. The insights gained from this systematic review are crucial for the scientific community to make informed decisions and mitigate the impacts of extreme weather events.
</details></li>
</ul>
<hr>
<h2 id="Image-Processing-and-Machine-Learning-for-Hyperspectral-Unmixing-An-Overview-and-the-HySUPP-Python-Package"><a href="#Image-Processing-and-Machine-Learning-for-Hyperspectral-Unmixing-An-Overview-and-the-HySUPP-Python-Package" class="headerlink" title="Image Processing and Machine Learning for Hyperspectral Unmixing: An Overview and the HySUPP Python Package"></a>Image Processing and Machine Learning for Hyperspectral Unmixing: An Overview and the HySUPP Python Package</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09375">http://arxiv.org/abs/2308.09375</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/behnoodrasti/hysupp">https://github.com/behnoodrasti/hysupp</a></li>
<li>paper_authors: Behnood Rasti, Alexandre Zouaoui, Julien Mairal, Jocelyn Chanussot</li>
<li>for: 这 paper 提供了一个概述先进和传统混合分析方法的Overview，以及三类不同类型的 linear unmixing 的比较。</li>
<li>methods: 这 paper 使用了先进的图像处理和机器学习技术，包括超vised、semi-supervised 和隐藏式 linear unmixing。</li>
<li>results: 实验结果显示了不同类型的混合分析方法在不同场景下的优势，以及 Python 基于的开源包 HySUPP 可以在 <a target="_blank" rel="noopener" href="https://github.com/BehnoodRasti/HySUPP">https://github.com/BehnoodRasti/HySUPP</a> 上下载。<details>
<summary>Abstract</summary>
Spectral pixels are often a mixture of the pure spectra of the materials, called endmembers, due to the low spatial resolution of hyperspectral sensors, double scattering, and intimate mixtures of materials in the scenes. Unmixing estimates the fractional abundances of the endmembers within the pixel. Depending on the prior knowledge of endmembers, linear unmixing can be divided into three main groups: supervised, semi-supervised, and unsupervised (blind) linear unmixing. Advances in Image processing and machine learning substantially affected unmixing. This paper provides an overview of advanced and conventional unmixing approaches. Additionally, we draw a critical comparison between advanced and conventional techniques from the three categories. We compare the performance of the unmixing techniques on three simulated and two real datasets. The experimental results reveal the advantages of different unmixing categories for different unmixing scenarios. Moreover, we provide an open-source Python-based package available at https://github.com/BehnoodRasti/HySUPP to reproduce the results.
</details>
<details>
<summary>摘要</summary>
几何色素 pixels 经常是杂合物质的纯谱，称为终端成员，由于雷达传感器的低空间分辨率、重吸散和场景中物质的温顺混合，导致这种杂合。混合计算器中的分量，即混合率，用于描述终端成员在像素中的存在度。根据终端成员的先知情，线性混合可以分为三类：监督、半监督和无监督（盲目）线性混合。图像处理和机器学习技术的进步对混合产生了深远的影响。本文提供了高级和传统混合方法的概述，并对这两种类型的混合技术进行抽象比较。我们将对三个 simulate 和两个实际数据集进行比较，并提供一个开源的 Python 基于包，可以在 <https://github.com/BehnoodRasti/HySUPP> 上获取。
</details></li>
</ul>
<hr>
<h2 id="Noise-Sensitivity-and-Stability-of-Deep-Neural-Networks-for-Binary-Classification"><a href="#Noise-Sensitivity-and-Stability-of-Deep-Neural-Networks-for-Binary-Classification" class="headerlink" title="Noise Sensitivity and Stability of Deep Neural Networks for Binary Classification"></a>Noise Sensitivity and Stability of Deep Neural Networks for Binary Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09374">http://arxiv.org/abs/2308.09374</a></li>
<li>repo_url: None</li>
<li>paper_authors: Johan Jonasson, Jeffrey E. Steif, Olof Zetterqvist</li>
<li>for: 这个论文的目的是理解深度神经网络（DNN）分类器的不稳定性现象，通过 Boolean 函数的视角来研究 DNN 模型是否敏感于噪声。</li>
<li>methods: 这篇论文使用了 Boolean 函数的概念来研究 DNN 模型，并对两种标准的 DNN 架构（完全连接和卷积模型）进行了研究，并且使用了随机 initialize 的 Gaussian 权重来调查这些模型的性质。</li>
<li>results: 研究发现，在随机 initialize 的 Gaussian 权重下，fully connected 模型和卷积模型都是噪声敏感的，而且卷积模型的噪声敏感性比 fully connected 模型更强。<details>
<summary>Abstract</summary>
A first step is taken towards understanding often observed non-robustness phenomena of deep neural net (DNN) classifiers. This is done from the perspective of Boolean functions by asking if certain sequences of Boolean functions represented by common DNN models are noise sensitive or noise stable, concepts defined in the Boolean function literature. Due to the natural randomness in DNN models, these concepts are extended to annealed and quenched versions. Here we sort out the relation between these definitions and investigate the properties of two standard DNN architectures, the fully connected and convolutional models, when initiated with Gaussian weights.
</details>
<details>
<summary>摘要</summary>
“一步进展在深度神经网络（DNN）分类器的不稳定现象之理解方面。这是从布尔函数的角度出发，询问了一些通用DNN模型表示的布尔函数是否对随机变量敏感或不敏感，这些概念在布尔函数文献中已经定义。由于自然的随机性在DNN模型中，这些定义被扩展到气体化和冷却版本。我们详细探讨这些定义之间的关系，并调查了两种标准的DNN架构，完全连接和卷积分模型，当启动时的Gauss矩阵初值。”Note that " Simplified Chinese" is also known as "Mandarin" or "Standard Chinese".
</details></li>
</ul>
<hr>
<h2 id="Which-Transformer-to-Favor-A-Comparative-Analysis-of-Efficiency-in-Vision-Transformers"><a href="#Which-Transformer-to-Favor-A-Comparative-Analysis-of-Efficiency-in-Vision-Transformers" class="headerlink" title="Which Transformer to Favor: A Comparative Analysis of Efficiency in Vision Transformers"></a>Which Transformer to Favor: A Comparative Analysis of Efficiency in Vision Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09372">http://arxiv.org/abs/2308.09372</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tobna/whattransformertofavor">https://github.com/tobna/whattransformertofavor</a></li>
<li>paper_authors: Tobias Christian Nauen, Sebastian Palacio, Andreas Dengel</li>
<li>For: This paper aims to provide a comprehensive analysis of vision transformers and related architectures, evaluating their efficiency across multiple performance metrics.* Methods: The authors use more than 30 models to conduct a holistic evaluation of efficiency-oriented transformers, considering various performance metrics.* Results: The study reveals several surprising insights, including the Pareto optimality of ViT across multiple efficiency metrics, the strength of hybrid attention-CNN models, and the positive correlation between FLOPS and training memory. The results provide valuable insights for practitioners and researchers in selecting models for specific applications.Here’s the Chinese version of the three key information points:* For: 这篇论文目的是为了提供视transformer和相关架构的全面分析，以评估它们的效率 across多个性能指标。* Methods: 作者使用了多于30个模型来进行视transformer和相关架构的总体评估，考虑多个性能指标。* Results: 研究发现了一些意外的发现，如 ViT在多个效率指标上的Pareto优化，混合注意力-CNN模型在低执行内存和参数数量上的强大表现，以及图像大小的扩展对模型性能的影响。结果为实践者和研究人员提供了有价值的指导，帮助他们在选择特定应用场景时做出 Informed decisions。<details>
<summary>Abstract</summary>
The growing popularity of Vision Transformers as the go-to models for image classification has led to an explosion of architectural modifications claiming to be more efficient than the original ViT. However, a wide diversity of experimental conditions prevents a fair comparison between all of them, based solely on their reported results. To address this gap in comparability, we conduct a comprehensive analysis of more than 30 models to evaluate the efficiency of vision transformers and related architectures, considering various performance metrics. Our benchmark provides a comparable baseline across the landscape of efficiency-oriented transformers, unveiling a plethora of surprising insights. For example, we discover that ViT is still Pareto optimal across multiple efficiency metrics, despite the existence of several alternative approaches claiming to be more efficient. Results also indicate that hybrid attention-CNN models fare particularly well when it comes to low inference memory and number of parameters, and also that it is better to scale the model size, than the image size. Furthermore, we uncover a strong positive correlation between the number of FLOPS and the training memory, which enables the estimation of required VRAM from theoretical measurements alone.   Thanks to our holistic evaluation, this study offers valuable insights for practitioners and researchers, facilitating informed decisions when selecting models for specific applications. We publicly release our code and data at https://github.com/tobna/WhatTransformerToFavor
</details>
<details>
<summary>摘要</summary>
“vision transformer”的快速增长 Popularity 使得许多建筑修改 claim 能够更高效 than the original ViT。然而，各种实验条件的多样性使得对所有这些模型进行公平比较变得困难，基于其报告的结果 alone。为了解决这个问题，我们进行了详细的30多种模型的分析，以评估视 transformer 和相关建筑的效率。我们的标准提供了多种效率指标下的共同基线，揭示了许多有趣的发现。例如，我们发现，despite the existence of several alternative approaches claiming to be more efficient, ViT still maintains Pareto optimality across multiple efficiency metrics。 results also show that hybrid attention-CNN models perform particularly well in terms of low inference memory and number of parameters, and that it is better to scale the model size than the image size. In addition, we find a strong positive correlation between the number of FLOPS and the training memory, which enables the estimation of required VRAM from theoretical measurements alone.  thanks to our comprehensive evaluation, this study provides valuable insights for practitioners and researchers, facilitating informed decisions when selecting models for specific applications. We publicly release our code and data at <https://github.com/tobna/WhatTransformerToFavor>。
</details></li>
</ul>
<hr>
<h2 id="A-tailored-Handwritten-Text-Recognition-System-for-Medieval-Latin"><a href="#A-tailored-Handwritten-Text-Recognition-System-for-Medieval-Latin" class="headerlink" title="A tailored Handwritten-Text-Recognition System for Medieval Latin"></a>A tailored Handwritten-Text-Recognition System for Medieval Latin</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09368">http://arxiv.org/abs/2308.09368</a></li>
<li>repo_url: None</li>
<li>paper_authors: Philipp Koch, Gilary Vera Nuñez, Esteban Garces Arias, Christian Heumann, Matthias Schöffel, Alexander Häberlin, Matthias Aßenmacher</li>
<li>for: 这份研究旨在为巴伐利亚科学院和人文学院的中世纪拉丁词典进行数字化。</li>
<li>methods: 这个pipeline使用两个最新的图像分割模型来准备输入数据集，并使用多种基于转换器的模型和GPT-2解码器进行实验。</li>
<li>results: 最佳实现达到了Character Error Rate（CER）0.015，超过了商业Google云视觉模型，并且表现更稳定。<details>
<summary>Abstract</summary>
The Bavarian Academy of Sciences and Humanities aims to digitize its Medieval Latin Dictionary. This dictionary entails record cards referring to lemmas in medieval Latin, a low-resource language. A crucial step of the digitization process is the Handwritten Text Recognition (HTR) of the handwritten lemmas found on these record cards. In our work, we introduce an end-to-end pipeline, tailored to the medieval Latin dictionary, for locating, extracting, and transcribing the lemmas. We employ two state-of-the-art (SOTA) image segmentation models to prepare the initial data set for the HTR task. Furthermore, we experiment with different transformer-based models and conduct a set of experiments to explore the capabilities of different combinations of vision encoders with a GPT-2 decoder. Additionally, we also apply extensive data augmentation resulting in a highly competitive model. The best-performing setup achieved a Character Error Rate (CER) of 0.015, which is even superior to the commercial Google Cloud Vision model, and shows more stable performance.
</details>
<details>
<summary>摘要</summary>
巴伐利亚科学院计划数字化中世纪拉丁词典。这个词典包含手写记录卡上的中世纪拉丁词汇，是一种低资源语言。我们的工作是开发一个端到端管道，专门为中世纪拉丁词典的数字化进行找到、提取和译写词汇。我们使用两个当今最佳实践（SOTA）图像分割模型来准备初始数据集，并对不同的变换器基于模型进行试验，以探索不同组合的视觉编码器和GPT-2解码器之间的可能性。此外，我们还进行了广泛的数据增强，实现了非常竞争力强的模型。最佳设置实现的字符错误率（CER）为0.015，甚至高于商业Google云视觉模型，并且表现更加稳定。
</details></li>
</ul>
<hr>
<h2 id="On-the-Approximation-of-Bi-Lipschitz-Maps-by-Invertible-Neural-Networks"><a href="#On-the-Approximation-of-Bi-Lipschitz-Maps-by-Invertible-Neural-Networks" class="headerlink" title="On the Approximation of Bi-Lipschitz Maps by Invertible Neural Networks"></a>On the Approximation of Bi-Lipschitz Maps by Invertible Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09367">http://arxiv.org/abs/2308.09367</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bangti Jin, Zehui Zhou, Jun Zou</li>
<li>for: 这个论文主要是用于研究嵌入型神经网络（INNs）的容量和应用可能性。</li>
<li>methods: 该论文使用了一种基于对接的INNs，并提供了一种基于模型简化、主成分分析和INNs的方法来近似具有不同维度的映射。</li>
<li>results: 研究发现，该方法可以同时近似前向和反向映射，并且可以用于近似参数化第二阶几何问题的解Operator。初步的数据分析表明该方法的可行性。<details>
<summary>Abstract</summary>
Invertible neural networks (INNs) represent an important class of deep neural network architectures that have been widely used in several applications. The universal approximation properties of INNs have also been established recently. However, the approximation rate of INNs is largely missing. In this work, we provide an analysis of the capacity of a class of coupling-based INNs to approximate bi-Lipschitz continuous mappings on a compact domain, and the result shows that it can well approximate both forward and inverse maps simultaneously. Furthermore, we develop an approach for approximating bi-Lipschitz maps on infinite-dimensional spaces that simultaneously approximate the forward and inverse maps, by combining model reduction with principal component analysis and INNs for approximating the reduced map, and we analyze the overall approximation error of the approach. Preliminary numerical results show the feasibility of the approach for approximating the solution operator for parameterized second-order elliptic problems.
</details>
<details>
<summary>摘要</summary>
invertible neural networks (INNs) 是深度神经网络的一个重要类型，已经广泛应用在各种领域。 INNs 的 universality 也已经最近得到了证明。然而， INNs 的 Approximation 率却尚未得到了充分的研究。在这项工作中，我们对一类基于coupling的 INNs 的容量进行了分析，并证明了它可以同时高精度地approximate forward 和 inverse 映射。此外，我们还开发了一种能够同时approximate forward 和 inverse 映射的方法，该方法基于模型减reduction、主成分分析和 INNs 来approximate减少后的映射，并对全局approximate error进行了分析。初步的数值结果表明该方法可以approximate parameterized second-order elliptic problems 的解Operator。
</details></li>
</ul>
<hr>
<h2 id="Multi-feature-concatenation-and-multi-classifier-stacking-an-interpretable-and-generalizable-machine-learning-method-for-MDD-discrimination-with-rsfMRI"><a href="#Multi-feature-concatenation-and-multi-classifier-stacking-an-interpretable-and-generalizable-machine-learning-method-for-MDD-discrimination-with-rsfMRI" class="headerlink" title="Multi-feature concatenation and multi-classifier stacking: an interpretable and generalizable machine learning method for MDD discrimination with rsfMRI"></a>Multi-feature concatenation and multi-classifier stacking: an interpretable and generalizable machine learning method for MDD discrimination with rsfMRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09360">http://arxiv.org/abs/2308.09360</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunsong Luo, Wenyu Chen, Ling Zhan, Jiang Qiu, Tao Jia</li>
<li>for: 这个研究旨在提高抑郁疾病的诊断精度，通过利用多种机器学习算法来挖掘resting-state功能MRI中的资讯，并将其 concatenate 成多个特征，然后使用多个分类器进行归类。</li>
<li>methods: 这个研究使用了多种机器学习算法，包括堆叠分类器、支持向量机器学习、以及决策树等，并将其 concatenate 成多个特征。</li>
<li>results: 这个研究获得了96.9%的抑郁疾病诊断精度，较之前的方法有所改善；此外，这个方法在独立的测试和训练数据上也有良好的一致性。<details>
<summary>Abstract</summary>
Major depressive disorder is a serious and heterogeneous psychiatric disorder that needs accurate diagnosis. Resting-state functional MRI (rsfMRI), which captures multiple perspectives on brain structure, function, and connectivity, is increasingly applied in the diagnosis and pathological research of mental diseases. Different machine learning algorithms are then developed to exploit the rich information in rsfMRI and discriminate MDD patients from normal controls. Despite recent advances reported, the discrimination accuracy has room for further improvement. The generalizability and interpretability of the method are not sufficiently addressed either. Here, we propose a machine learning method (MFMC) for MDD discrimination by concatenating multiple features and stacking multiple classifiers. MFMC is tested on the REST-meta-MDD data set that contains 2428 subjects collected from 25 different sites. MFMC yields 96.9% MDD discrimination accuracy, demonstrating a significant improvement over existing methods. In addition, the generalizability of MFMC is validated by the good performance when the training and testing subjects are from independent sites. The use of XGBoost as the meta classifier allows us to probe the decision process of MFMC. We identify 13 feature values related to 9 brain regions including the posterior cingulate gyrus, superior frontal gyrus orbital part, and angular gyrus, which contribute most to the classification and also demonstrate significant differences at the group level. The use of these 13 feature values alone can reach 87% of MFMC's full performance when taking all feature values. These features may serve as clinically useful diagnostic and prognostic biomarkers for mental disorders in the future.
</details>
<details>
<summary>摘要</summary>
major depressive disorder 是一种严重多样的心理疾病，需要准确诊断。它可以通过多种方法进行诊断和病理研究，其中包括功能磁共振成像（rsfMRI）。不同的机器学习算法可以利用rsfMRI中的多个视角和特征来分类患者和正常人。 DESPITE 最近的进步，分类精度仍然有很大的空间提高。此外，方法的普适性和解释性也未得到充分考虑。在这种情况下，我们提出了一种机器学习方法（MFMC），通过 concatenating 多个特征和堆式多个分类器来进行患者分类。MFMC 在 REST-meta-MDD 数据集上进行测试，共计 2428 名参与者，来自 25 个不同的场景。MFMC 的分类精度达 96.9%，表现出了显著的提高。此外，我们还 validate 了 MFMC 的普适性，通过在训练和测试数据集来自独立的场景时进行测试。使用 XGBoost 作为元分类器，我们可以探究 MFMC 的决策过程。我们identified 13 个特征值， relate 到 9 个脑区，包括 posterior cingulate gyrus、superior frontal gyrus orbital part 和 angular gyrus，这些特征值在分类中做出了最大贡献，并在群体水平上显示了显著的差异。这些特征值可能在未来作为心理疾病的临床有用的诊断和预后指标。
</details></li>
</ul>
<hr>
<h2 id="RLIPv2-Fast-Scaling-of-Relational-Language-Image-Pre-training"><a href="#RLIPv2-Fast-Scaling-of-Relational-Language-Image-Pre-training" class="headerlink" title="RLIPv2: Fast Scaling of Relational Language-Image Pre-training"></a>RLIPv2: Fast Scaling of Relational Language-Image Pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09351">http://arxiv.org/abs/2308.09351</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jacobyuan7/rlipv2">https://github.com/jacobyuan7/rlipv2</a></li>
<li>paper_authors: Hangjie Yuan, Shiwei Zhang, Xiang Wang, Samuel Albanie, Yining Pan, Tao Feng, Jianwen Jiang, Dong Ni, Yingya Zhang, Deli Zhao</li>
<li>for: 提高计算机视觉任务中关系理解能力</li>
<li>methods: 引入异ometric语言-图像融合（ALIF）机制，提高早期和深度的模态融合，并使用减少语言编码层来减少训练时间</li>
<li>results: 在人-物互动检测和场景图生成任务上达到了状态级表现，在完全训练、少数shot和零shot设置下都达到了最佳性能， largest RLIPv2 在 HICO-DET 上达到了23.29mAP without fine-tuning，yielded 32.22mAP with just 1% data, and yielded 45.09mAP with 100% data.<details>
<summary>Abstract</summary>
Relational Language-Image Pre-training (RLIP) aims to align vision representations with relational texts, thereby advancing the capability of relational reasoning in computer vision tasks. However, hindered by the slow convergence of RLIPv1 architecture and the limited availability of existing scene graph data, scaling RLIPv1 is challenging. In this paper, we propose RLIPv2, a fast converging model that enables the scaling of relational pre-training to large-scale pseudo-labelled scene graph data. To enable fast scaling, RLIPv2 introduces Asymmetric Language-Image Fusion (ALIF), a mechanism that facilitates earlier and deeper gated cross-modal fusion with sparsified language encoding layers. ALIF leads to comparable or better performance than RLIPv1 in a fraction of the time for pre-training and fine-tuning. To obtain scene graph data at scale, we extend object detection datasets with free-form relation labels by introducing a captioner (e.g., BLIP) and a designed Relation Tagger. The Relation Tagger assigns BLIP-generated relation texts to region pairs, thus enabling larger-scale relational pre-training. Through extensive experiments conducted on Human-Object Interaction Detection and Scene Graph Generation, RLIPv2 shows state-of-the-art performance on three benchmarks under fully-finetuning, few-shot and zero-shot settings. Notably, the largest RLIPv2 achieves 23.29mAP on HICO-DET without any fine-tuning, yields 32.22mAP with just 1% data and yields 45.09mAP with 100% data. Code and models are publicly available at https://github.com/JacobYuan7/RLIPv2.
</details>
<details>
<summary>摘要</summary>
“relational语言-图像预训练（RLIP）目的是将视觉表示与关系文本相对应，从而提高计算机视觉任务中的关系推理能力。然而，由于RLIPv1架构的慢 converges和现有场景图数据的有限性，扩展RLIPv1是困难的。在这篇论文中，我们提出RLIPv2，一种快 converges 的模型，可以将关系预训练扩展到大规模 Pseudo-标注场景图数据。为了快速扩展，RLIPv2引入了非对称语言-图像融合（ALIF）机制，使得更早更深的阻止 Language Encoding 层进行快速融合。ALIF 导致与 RLIPv1 相比，在预训练和精度调整中需要的时间只占一小部分。为了获得场景图数据的扩展，我们将对象检测数据集扩展到具有自由关系标签的场景图数据，并引入了一个captioner（例如 BLIP）和一个设计的关系标签器。关系标签器将 BLIP 生成的关系文本分配给区域对，从而启用大规模关系预训练。通过广泛的实验，RLIPv2在人物-物体互动检测和场景图生成三个标准 benchmark 上显示了状态机器的性能，包括完全 fine-tuning、几何 shot 和零 shot 设置下的性能。尤其是RLIPv2最大模型在 HICO-DET 上达到了 23.29mAP 无需任何 fine-tuning，在 1% 数据上达到了 32.22mAP，在 100% 数据上达到了 45.09mAP。代码和模型在https://github.com/JacobYuan7/RLIPv2 上公开可用。”
</details></li>
</ul>
<hr>
<h2 id="Denoising-diffusion-based-MR-to-CT-image-translation-enables-whole-spine-vertebral-segmentation-in-2D-and-3D-without-manual-annotations"><a href="#Denoising-diffusion-based-MR-to-CT-image-translation-enables-whole-spine-vertebral-segmentation-in-2D-and-3D-without-manual-annotations" class="headerlink" title="Denoising diffusion-based MR to CT image translation enables whole spine vertebral segmentation in 2D and 3D without manual annotations"></a>Denoising diffusion-based MR to CT image translation enables whole spine vertebral segmentation in 2D and 3D without manual annotations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09345">http://arxiv.org/abs/2308.09345</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/robert-graf/readable-conditional-denoising-diffusion">https://github.com/robert-graf/readable-conditional-denoising-diffusion</a></li>
<li>paper_authors: Robert Graf, Joachim Schmitt, Sarah Schlaeger, Hendrik Kristian Möller, Vasiliki Sideri-Lampretsa, Anjany Sekuboyina, Sandro Manuel Krieg, Benedikt Wiestler, Bjoern Menze, Daniel Rueckert, Jan Stefan Kirschke</li>
<li>For: This paper aims to develop a method for translating T1w and T2w MR images into CT images, with a focus on accurately delineating posterior spine structures.* Methods: The authors used a combination of landmark-based registration and image-to-image translation techniques, including Pix2Pix, DDIM, and SynDiff, to align the MR and CT images. They evaluated the performance of these methods using PSNR and Dice scores.* Results: The authors found that 2D paired methods and SynDiff exhibited similar translation performance and Dice scores on paired data, while DDIM image mode achieved the highest image quality. The 3D translation approach outperformed the 2D approach, resulting in improved Dice scores and anatomically accurate segmentations.Here are the three points in Simplified Chinese:* For: 这个论文的目的是开发一种将T1w和T2wMR图像翻译成CT图像的方法，尤其是准确地分割 posterior spine structure.* Methods: 作者使用了一种组合landmark-based registration和图像-to-图像翻译技术，包括Pix2Pix、DDIM和SynDiff，将MR和CT图像对齐。他们使用PSNR和Dice scores评估这些方法的性能。* Results: 作者发现，2D paired方法和SynDiff在对应数据上具有相似的翻译性能和Dice scores，而DDIM图像模式 achieve最高的图像质量。3D翻译方法在高分辨率下对比2D方法表现出了改善的Dice scores和正确的 segmentation.<details>
<summary>Abstract</summary>
Background: Automated segmentation of spinal MR images plays a vital role both scientifically and clinically. However, accurately delineating posterior spine structures presents challenges.   Methods: This retrospective study, approved by the ethical committee, involved translating T1w and T2w MR image series into CT images in a total of n=263 pairs of CT/MR series. Landmark-based registration was performed to align image pairs. We compared 2D paired (Pix2Pix, denoising diffusion implicit models (DDIM) image mode, DDIM noise mode) and unpaired (contrastive unpaired translation, SynDiff) image-to-image translation using "peak signal to noise ratio" (PSNR) as quality measure. A publicly available segmentation network segmented the synthesized CT datasets, and Dice scores were evaluated on in-house test sets and the "MRSpineSeg Challenge" volumes. The 2D findings were extended to 3D Pix2Pix and DDIM.   Results: 2D paired methods and SynDiff exhibited similar translation performance and Dice scores on paired data. DDIM image mode achieved the highest image quality. SynDiff, Pix2Pix, and DDIM image mode demonstrated similar Dice scores (0.77). For craniocaudal axis rotations, at least two landmarks per vertebra were required for registration. The 3D translation outperformed the 2D approach, resulting in improved Dice scores (0.80) and anatomically accurate segmentations in a higher resolution than the original MR image.   Conclusion: Two landmarks per vertebra registration enabled paired image-to-image translation from MR to CT and outperformed all unpaired approaches. The 3D techniques provided anatomically correct segmentations, avoiding underprediction of small structures like the spinous process.
</details>
<details>
<summary>摘要</summary>
Background: 自动化分割脊梗磁共agnetic resonance imaging（MRI）图像对科学和临床来说都具有重要的作用。然而，准确地界定后方脊梗结构具有挑战。Methods: 这是一项回溯性研究，得到了伦敦委员会的批准，涉及翻译T1w和T2w磁共图像序列到CT图像序列的总共n=263对。使用标记为基准的注册方法对图像序列进行了对齐。我们使用“峰信号噪声比”（PSNR）作为质量指标，并对在家测试集和“MRSpineSeg Challenge”volumes上进行了Dice分数的评估。使用公共可用的 segmentation network 对合成的CT数据进行了 segmentation，并评估了Dice分数。Results: 2D paired方法和SynDiff在对应数据上显示了相似的翻译性能和Dice分数。DDIM图像模式实现了最高的图像质量。SynDiff、Pix2Pix和DDIM图像模式在对应数据上都达到了0.77的Dice分数。对横轴向的旋转，至少需要两个标记每个vertebra进行注册。3D翻译超过了2D方法，导致了改进的Dice分数（0.80）和高分辨率的、准确的分割。Conclusion: 使用两个标记每个vertebra的注册可以实现了paired图像到图像的翻译，并超过了所有的不对应方法。3D技术提供了准确的分割，避免了小结构的下预测，如脊梗进程。
</details></li>
</ul>
<hr>
<h2 id="Surprise-machines-revealing-Harvard-Art-Museums’-image-collection"><a href="#Surprise-machines-revealing-Harvard-Art-Museums’-image-collection" class="headerlink" title="Surprise machines: revealing Harvard Art Museums’ image collection"></a>Surprise machines: revealing Harvard Art Museums’ image collection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09343">http://arxiv.org/abs/2308.09343</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dario Rodighiero, Lins Derry, Douglas Duhaime, Jordan Kruguer, Maximilian C. Mueller, Christopher Pietsch, Jeffrey T. Schnapp, Jeff Steward</li>
<li>for: 这个研究是为了开发一个名为“Surprise Machines”的实验 museology 项目，旨在可视化哈佛艺术博物馆的所有图像收藏，以便为访客带来意外的视觉体验。</li>
<li>methods: 这个项目使用了人工智能技术，以显示大量图像并创造对访客的意外感。为此，它设计了一个舞蹈式界面，将访客的运动与图像 коллекцию相连。</li>
<li>results: 这个项目可以创造一种新的视觉体验，让访客感受到图像收藏的意外之处。通过这种方式，访客可以更好地了解和探索图像收藏，并且可以与其他访客分享这种体验。<details>
<summary>Abstract</summary>
Surprise Machines is a project of experimental museology that sets out to visualize the entire image collection of the Harvard Art Museums, intending to open up unexpected vistas on more than 200,000 objects usually inaccessible to visitors. Part of the exhibition Curatorial A(i)gents organized by metaLAB (at) Harvard, the project explores the limits of artificial intelligence to display a large set of images and create surprise among visitors. To achieve such a feeling of surprise, a choreographic interface was designed to connect the audience's movement with several unique views of the collection.
</details>
<details>
<summary>摘要</summary>
很高兴的机器（Surprise Machines）是一个 Harvard Art Museums 的实验 museology 项目，旨在通过可见化整个收藏品库，为访问者提供新的视角和感受。这个项目是metaLAB（at）Harvard组织的 Curatorial A(i)gents 展览的一部分。项目使用人工智能技术，以显示大量图像并创造访问者的感佩感。为了实现这种感动， проек 设计了一个与访客运动相连的chorographic接口，以显示收藏品库的多个独特视角。
</details></li>
</ul>
<hr>
<h2 id="Document-Automation-Architectures-Updated-Survey-in-Light-of-Large-Language-Models"><a href="#Document-Automation-Architectures-Updated-Survey-in-Light-of-Large-Language-Models" class="headerlink" title="Document Automation Architectures: Updated Survey in Light of Large Language Models"></a>Document Automation Architectures: Updated Survey in Light of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09341">http://arxiv.org/abs/2308.09341</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Ahmadi Achachlouei, Omkar Patil, Tarun Joshi, Vijayan N. Nair</li>
<li>for: 本文对当前文档自动化（DA）技术进行了一个全面的评估和概述，尤其是在法律领域的商业解决方案上。</li>
<li>methods: 本文对学术研究中的DA体系和技术进行了一个 clearer定义和特征化，并提出了新的研究机遇，以及在生成AI和大语言模型的进步下的DA领域的发展趋势。</li>
<li>results: 本文对学术研究中的DA体系和技术进行了一个全面的评估和概述，并提出了新的研究机遇。<details>
<summary>Abstract</summary>
This paper surveys the current state of the art in document automation (DA). The objective of DA is to reduce the manual effort during the generation of documents by automatically creating and integrating input from different sources and assembling documents conforming to defined templates. There have been reviews of commercial solutions of DA, particularly in the legal domain, but to date there has been no comprehensive review of the academic research on DA architectures and technologies. The current survey of DA reviews the academic literature and provides a clearer definition and characterization of DA and its features, identifies state-of-the-art DA architectures and technologies in academic research, and provides ideas that can lead to new research opportunities within the DA field in light of recent advances in generative AI and large language models.
</details>
<details>
<summary>摘要</summary>
The current survey of DA reviews academic literature and provides a clearer definition and characterization of DA and its features. It identifies state-of-the-art DA architectures and technologies in academic research and offers ideas for new research opportunities in the DA field, given recent advances in generative AI and large language models.
</details></li>
</ul>
<hr>
<h2 id="Causal-Interpretable-Progression-Trajectory-Analysis-of-Chronic-Disease"><a href="#Causal-Interpretable-Progression-Trajectory-Analysis-of-Chronic-Disease" class="headerlink" title="Causal Interpretable Progression Trajectory Analysis of Chronic Disease"></a>Causal Interpretable Progression Trajectory Analysis of Chronic Disease</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09735">http://arxiv.org/abs/2308.09735</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhoujian Sun, Wenzhuo Zhang, Zhengxing Huang, Nai Ding</li>
<li>for: 预测疾病进程轨迹和诊断决策</li>
<li>methods: 利用机器学习模型和 causal discovery 技术组合 trajectory prediction 和 causal关系挖掘</li>
<li>results: 提供了高度可解释的疾病进程轨迹预测和治疗效果估计，帮助临床决策<details>
<summary>Abstract</summary>
Chronic disease is the leading cause of death, emphasizing the need for accurate prediction of disease progression trajectories and informed clinical decision-making. Machine learning (ML) models have shown promise in this domain by capturing non-linear patterns within patient features. However, existing ML-based models lack the ability to provide causal interpretable predictions and estimate treatment effects, limiting their decision-assisting perspective. In this study, we propose a novel model called causal trajectory prediction (CTP) to tackle the limitation. The CTP model combines trajectory prediction and causal discovery to enable accurate prediction of disease progression trajectories and uncovering causal relationships between features. By incorporating a causal graph into the prediction process, CTP ensures that ancestor features are not influenced by treatment on descendant features, thereby enhancing the interpretability of the model. By estimating the bounds of treatment effects, even in the presence of unmeasured confounders, the CTP provides valuable insights for clinical decision-making. We evaluate the performance of the CTP using simulated and real medical datasets. Experimental results demonstrate that our model achieves satisfactory performance, highlighting its potential to assist clinical decisions.
</details>
<details>
<summary>摘要</summary>
chronic disease 是 death 的主要原因，强调了精准预测疾病进程轨迹和临床决策的需要。机器学习（ML）模型在这个领域中表现出了承诺，通过捕捉非线性 patient 特征中的模式。然而，现有的 ML 基本模型缺乏提供可采用性预测和计算治疗效果的能力，限制了决策帮助的观点。在这种研究中，我们提出了一种新的模型，即 causal trajectory prediction（CTP）模型。CTP 模型结合轨迹预测和 causal discovery，以实现精准预测疾病进程轨迹和探索特征之间的 causal 关系。通过将 causal 图 incorporated 到预测过程中，CTP 模型确保了 ancestor 特征不会由治疗影响 descendant 特征，从而增强了模型的可读性。通过估计治疗效果的范围，即使在不完全掌握的混合变量的情况下，CTP 模型提供了有价值的决策参考。我们使用 simulated 和实际医疗数据进行了实验，结果表明，我们的模型在满足性方面表现出色，强调其在临床决策中的潜在价值。
</details></li>
</ul>
<hr>
<h2 id="Towards-Attack-tolerant-Federated-Learning-via-Critical-Parameter-Analysis"><a href="#Towards-Attack-tolerant-Federated-Learning-via-Critical-Parameter-Analysis" class="headerlink" title="Towards Attack-tolerant Federated Learning via Critical Parameter Analysis"></a>Towards Attack-tolerant Federated Learning via Critical Parameter Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09318">http://arxiv.org/abs/2308.09318</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sungwon-han/fedcpa">https://github.com/sungwon-han/fedcpa</a></li>
<li>paper_authors: Sungwon Han, Sungwon Park, Fangzhao Wu, Sundong Kim, Bin Zhu, Xing Xie, Meeyoung Cha</li>
<li>for: 防止 Federated Learning 系统受到毒 poisoning 攻击</li>
<li>methods: 提出了一种新的防御策略：FedCPA (Federated learning with Critical Parameter Analysis)，基于本地模型中critical parameter的观察，对恶意本地模型进行检测和排除</li>
<li>results: 对多个数据集和不同的攻击场景进行实验，表明我们的模型在防止毒 poisoning 攻击方面表现更好，比较现有的防御策略。<details>
<summary>Abstract</summary>
Federated learning is used to train a shared model in a decentralized way without clients sharing private data with each other. Federated learning systems are susceptible to poisoning attacks when malicious clients send false updates to the central server. Existing defense strategies are ineffective under non-IID data settings. This paper proposes a new defense strategy, FedCPA (Federated learning with Critical Parameter Analysis). Our attack-tolerant aggregation method is based on the observation that benign local models have similar sets of top-k and bottom-k critical parameters, whereas poisoned local models do not. Experiments with different attack scenarios on multiple datasets demonstrate that our model outperforms existing defense strategies in defending against poisoning attacks.
</details>
<details>
<summary>摘要</summary>
federated learning 是一种用于共享模型的协同训练方式，无需客户端对彼此分享私人数据。 federated learning 系统容易受到毒素攻击，当恶意客户端将 false 更新发送到中央服务器时。 现有的防御策略在非标一致数据设置下无效。 这篇论文提出了一种新的防御策略，即 FedCPA (federated learning with Critical Parameter Analysis)。我们的攻击忍受汇总方法基于本地模型的准确参数集的观察，坏做的本地模型与benign的本地模型之间存在显著差异。 对多个数据集和不同的攻击场景进行实验，我们的模型比现有的防御策略更高效地防止毒素攻击。
</details></li>
</ul>
<hr>
<h2 id="Path-Signatures-for-Seizure-Forecasting"><a href="#Path-Signatures-for-Seizure-Forecasting" class="headerlink" title="Path Signatures for Seizure Forecasting"></a>Path Signatures for Seizure Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09312">http://arxiv.org/abs/2308.09312</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonas F. Haderlein, Andre D. H. Peterson, Parvin Zarei Eskikand, Mark J. Cook, Anthony N. Burkitt, Iven M. Y. Mareels, David B. Grayden</li>
<li>for: This paper aims to automatically discover and quantify statistical features (biomarkers) that can be used to forecast seizures in a patient-specific way.</li>
<li>methods: The authors use existing and novel feature extraction algorithms, including the path signature, a recent development in time series analysis. They use statistical classification algorithms with in-built subset selection to discern time series with and without an impending seizure while selecting only a small number of relevant features.</li>
<li>results: The study may be seen as a step towards a generalisable pattern recognition pipeline for time series in a broader context.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文目标是自动发现和评估病人特定的预测癫痫症状的统计特征（生物标志）。</li>
<li>methods: 作者使用现有和新的特征提取算法，包括时间序列分析中最新的路径签名。他们使用统计分类算法和内置的子集选择来分辨病人的时间序列中是否有预期的癫痫症状，并且选择只有一小部分相关的特征。</li>
<li>results: 这种研究可能是对时间序列Pattern recognition pipeline的一个普适的步骤。<details>
<summary>Abstract</summary>
Forecasting the state of a system from an observed time series is the subject of research in many domains, such as computational neuroscience. Here, the prediction of epileptic seizures from brain measurements is an unresolved problem. There are neither complete models describing underlying brain dynamics, nor do individual patients exhibit a single seizure onset pattern, which complicates the development of a `one-size-fits-all' solution. Based on a longitudinal patient data set, we address the automated discovery and quantification of statistical features (biomarkers) that can be used to forecast seizures in a patient-specific way. We use existing and novel feature extraction algorithms, in particular the path signature, a recent development in time series analysis. Of particular interest is how this set of complex, nonlinear features performs compared to simpler, linear features on this task. Our inference is based on statistical classification algorithms with in-built subset selection to discern time series with and without an impending seizure while selecting only a small number of relevant features. This study may be seen as a step towards a generalisable pattern recognition pipeline for time series in a broader context.
</details>
<details>
<summary>摘要</summary>
预测系统的状态从观察时序序列是多个领域的研究主题，如计算神经科学。在这里，预测脑测量中的癫病症发生是一个未解决的问题。没有完整的模型描述脑动力学，每个患者都不会表现出唯一的发病开始模式，这使得开发一个“一size-fits-all”解决方案变得困难。基于长期患者数据集，我们研究自动发现和评估统计特征（生物标志），以便预测患者特定的癫病症。我们使用现有和新的特征提取算法，特别是路径签名，这是时间序列分析中最近的发展。我们的推断基于统计分类算法，并使用内置子集选择来分辨时间序列中有无危机发生，同时选择只有少量相关的特征。这种研究可能是一步向更通用的时间序列模式识别管道的发展。
</details></li>
</ul>
<hr>
<h2 id="Variance-reduction-techniques-for-stochastic-proximal-point-algorithms"><a href="#Variance-reduction-techniques-for-stochastic-proximal-point-algorithms" class="headerlink" title="Variance reduction techniques for stochastic proximal point algorithms"></a>Variance reduction techniques for stochastic proximal point algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09310">http://arxiv.org/abs/2308.09310</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheik Traoré, Vassilis Apidopoulos, Saverio Salzo, Silvia Villa</li>
<li>for: 本研究目的是为了提出variance reduction技术来提高现有抽象函数的最小值算法性能。</li>
<li>methods: 本文使用了stochastic proximal point算法，并研究了这些算法的variance reduction版本。</li>
<li>results: 本文提供了几种convergence result，包括iterates和目标函数值的convergence rate。在Polyak-{\L}ojasiewicz条件下，我们得到了iterates和函数值的线性增长率。数据实验表明，proximal variance reduction方法比其梯度对应方法更稳定，尤其是对于步长的选择。<details>
<summary>Abstract</summary>
In the context of finite sums minimization, variance reduction techniques are widely used to improve the performance of state-of-the-art stochastic gradient methods. Their practical impact is clear, as well as their theoretical properties. Stochastic proximal point algorithms have been studied as an alternative to stochastic gradient algorithms since they are more stable with respect to the choice of the stepsize but a proper variance reduced version is missing. In this work, we propose the first study of variance reduction techniques for stochastic proximal point algorithms. We introduce a stochastic proximal version of SVRG, SAGA, and some of their variants for smooth and convex functions. We provide several convergence results for the iterates and the objective function values. In addition, under the Polyak-{\L}ojasiewicz (PL) condition, we obtain linear convergence rates for the iterates and the function values. Our numerical experiments demonstrate the advantages of the proximal variance reduction methods over their gradient counterparts, especially about the stability with respect to the choice of the step size.
</details>
<details>
<summary>摘要</summary>
在finite sums最小化上， variance reduction 技术广泛应用于提高现有的随机梯度方法性能。其实际影响明确，同时其理论性质也很清晰。随机 proximal 点算法被视为随机梯度算法的替代方案，因为它们在步长选择方面更加稳定，但是一个适当的 variance reduced 版本缺失。在这种工作中，我们提出了随机 proximal 版本的 SVRG、SAGA 和一些其他变种，用于凸函数和 convex 函数。我们提供了迭代点和目标函数值的多个收敛结果。此外，在 Polyak-{\L}ojasiewicz（PL）条件下，我们获得了迭代点和函数值的线性收敛率。我们的数值实验表明 proximal variance reduction 方法在步长选择方面比随机梯度方法更稳定，特别是在随机梯度方法的稳定性方面。
</details></li>
</ul>
<hr>
<h2 id="Meta-learning-enhanced-next-POI-recommendation-by-leveraging-check-ins-from-auxiliary-cities"><a href="#Meta-learning-enhanced-next-POI-recommendation-by-leveraging-check-ins-from-auxiliary-cities" class="headerlink" title="Meta-learning enhanced next POI recommendation by leveraging check-ins from auxiliary cities"></a>Meta-learning enhanced next POI recommendation by leveraging check-ins from auxiliary cities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09309">http://arxiv.org/abs/2308.09309</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/oli-wang/merec">https://github.com/oli-wang/merec</a></li>
<li>paper_authors: Jinze Wang, Lu Zhang, Zhu Sun, Yew-Soon Ong</li>
<li>for: 提出一种基于meta-学习的下一个 POI 推荐方法，以解决城市级用户历史检查入数据的稀缺性问题。</li>
<li>methods: 利用城市级用户历史检查入数据和相关城市的检查入数据，通过meta-学习方法捕捉用户偏好，并通过城市级协调策略把更有关系的知识从更相关的城市传递给目标城市。</li>
<li>results: 经过广泛的实验 validate 了提出的方法的优越性， compared with 现有的算法。<details>
<summary>Abstract</summary>
Most existing point-of-interest (POI) recommenders aim to capture user preference by employing city-level user historical check-ins, thus facilitating users' exploration of the city. However, the scarcity of city-level user check-ins brings a significant challenge to user preference learning. Although prior studies attempt to mitigate this challenge by exploiting various context information, e.g., spatio-temporal information, they ignore to transfer the knowledge (i.e., common behavioral pattern) from other relevant cities (i.e., auxiliary cities). In this paper, we investigate the effect of knowledge distilled from auxiliary cities and thus propose a novel Meta-learning Enhanced next POI Recommendation framework (MERec). The MERec leverages the correlation of check-in behaviors among various cities into the meta-learning paradigm to help infer user preference in the target city, by holding the principle of "paying more attention to more correlated knowledge". Particularly, a city-level correlation strategy is devised to attentively capture common patterns among cities, so as to transfer more relevant knowledge from more correlated cities. Extensive experiments verify the superiority of the proposed MERec against state-of-the-art algorithms.
</details>
<details>
<summary>摘要</summary>
现有大多数点位推荐器都 aim to capture用户偏好通过使用城市级别的用户历史检查入，从而促进用户对城市的探索。然而，城市级别的用户检查入的缺乏引起了significant Challenge to user preference learning。 although prior studies attempt to mitigate this challenge by exploiting various context information, such as spatio-temporal information, they ignore to transfer the knowledge (i.e., common behavioral pattern) from other relevant cities (i.e., auxiliary cities).在本文中，我们investigate the effect of knowledge distilled from auxiliary cities and thus propose a novel Meta-learning Enhanced next POI Recommendation framework (MERec). MERec leverages the correlation of check-in behaviors among various cities into the meta-learning paradigm to help infer user preference in the target city, by holding the principle of "paying more attention to more correlated knowledge". particularly, a city-level correlation strategy is devised to attentively capture common patterns among cities, so as to transfer more relevant knowledge from more correlated cities.extensive experiments verify the superiority of the proposed MERec against state-of-the-art algorithms.
</details></li>
</ul>
<hr>
<h2 id="Online-Class-Incremental-Learning-on-Stochastic-Blurry-Task-Boundary-via-Mask-and-Visual-Prompt-Tuning"><a href="#Online-Class-Incremental-Learning-on-Stochastic-Blurry-Task-Boundary-via-Mask-and-Visual-Prompt-Tuning" class="headerlink" title="Online Class Incremental Learning on Stochastic Blurry Task Boundary via Mask and Visual Prompt Tuning"></a>Online Class Incremental Learning on Stochastic Blurry Task Boundary via Mask and Visual Prompt Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09303">http://arxiv.org/abs/2308.09303</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/moonjunyyy/si-blurry">https://github.com/moonjunyyy/si-blurry</a></li>
<li>paper_authors: Jun-Yeong Moon, Keon-Hee Park, Jung Uk Kim, Gyeong-Moon Park</li>
<li>for: 本研究旨在 Addressing the challenges of continual learning in real-world scenarios, where the number of input data and tasks is constantly changing in a statistical way.</li>
<li>methods: 我们提出了一种新的 Stochastic incremental Blurry task boundary scenario (Si-Blurry)，并引入了 Mask and Visual Prompt tuning (MVP) 方法来解决inter-和 intra-task 忘记和类偏度问题。MVP方法包括novel instance-wise logit masking和contrastive visual prompt tuning loss，以及一种新的类相似性基于的 focal loss和 adaptive feature scaling。</li>
<li>results: 我们的实验表明，Compared with existing state-of-the-art methods, our proposed MVP method significantly outperforms in our challenging Si-Blurry scenario.<details>
<summary>Abstract</summary>
Continual learning aims to learn a model from a continuous stream of data, but it mainly assumes a fixed number of data and tasks with clear task boundaries. However, in real-world scenarios, the number of input data and tasks is constantly changing in a statistical way, not a static way. Although recently introduced incremental learning scenarios having blurry task boundaries somewhat address the above issues, they still do not fully reflect the statistical properties of real-world situations because of the fixed ratio of disjoint and blurry samples. In this paper, we propose a new Stochastic incremental Blurry task boundary scenario, called Si-Blurry, which reflects the stochastic properties of the real-world. We find that there are two major challenges in the Si-Blurry scenario: (1) inter- and intra-task forgettings and (2) class imbalance problem. To alleviate them, we introduce Mask and Visual Prompt tuning (MVP). In MVP, to address the inter- and intra-task forgetting issues, we propose a novel instance-wise logit masking and contrastive visual prompt tuning loss. Both of them help our model discern the classes to be learned in the current batch. It results in consolidating the previous knowledge. In addition, to alleviate the class imbalance problem, we introduce a new gradient similarity-based focal loss and adaptive feature scaling to ease overfitting to the major classes and underfitting to the minor classes. Extensive experiments show that our proposed MVP significantly outperforms the existing state-of-the-art methods in our challenging Si-Blurry scenario.
</details>
<details>
<summary>摘要</summary>
continuous learning旨在从连续的数据流中学习模型，但它主要假设 fixes number of data和任务，并且这些任务的boundary是清晰的。然而，在实际情况下，输入数据和任务的数量不断改变，而且这些变化是以 Statistical way进行的，而不是静止的way。虽然最近引入的增量学习enario中有些地方处理了这些问题，但它们仍然不能完全反映实际情况中的Statistical properties。因此，我们在这篇论文中提出了一个新的Stochastic增量Blurry任务boundaryscenario，称为Si-Blurry。我们发现这个scenario中有两个主要挑战：（1）inter-和intra-task forgetting，以及（2）class imbalance问题。为了解决这些问题，我们引入了Mask和Visual Prompt tuning（MVP）。在MVP中，为了解决inter-和intra-task forgetting问题，我们提出了一个novel的instance-wise logit masking和contrastive visual prompt tuning损失。这些损失帮助我们的模型在当前批次中识别需要学习的类别。因此，它将有助于我们的模型固化先前的知识。此外，为了解决class imbalance问题，我们引入了一个新的gradient similarity-based focal loss和adaptive feature scaling。这些方法帮助我们的模型免于主要类别的过拟合和次要类别的下降。实验结果显示，我们的提出的MVP方法在我们的挑战性Si-Blurryscenario中得到了很好的表现，较以前的state-of-the-art方法有所进步。
</details></li>
</ul>
<hr>
<h2 id="Learning-Reward-Machines-through-Preference-Queries-over-Sequences"><a href="#Learning-Reward-Machines-through-Preference-Queries-over-Sequences" class="headerlink" title="Learning Reward Machines through Preference Queries over Sequences"></a>Learning Reward Machines through Preference Queries over Sequences</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09301">http://arxiv.org/abs/2308.09301</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eric Hsiung, Joydeep Biswas, Swarat Chaudhuri</li>
<li>for: 本研究旨在学习具有复杂动作序列的任务时，使用实际弱反馈来学习奖励机。</li>
<li>methods: 本研究提出了一种名为REMAP的算法，用于从偏好中学习奖励机，并提供了正确性和结束 garanties。 REMAP使用偏好查询而不是L*算法中的会员查询，并利用符号观察表以及卷积和约束解决方法缩小假设奖励机搜索空间。</li>
<li>results: 本研究提供了对REMAP算法的正确性和终止性证明，以及实验证明了REMAP算法在一个相对准确但不准确的教师下的正确性和偏差。<details>
<summary>Abstract</summary>
Reward machines have shown great promise at capturing non-Markovian reward functions for learning tasks that involve complex action sequencing. However, no algorithm currently exists for learning reward machines with realistic weak feedback in the form of preferences. We contribute REMAP, a novel algorithm for learning reward machines from preferences, with correctness and termination guarantees. REMAP introduces preference queries in place of membership queries in the L* algorithm, and leverages a symbolic observation table along with unification and constraint solving to narrow the hypothesis reward machine search space. In addition to the proofs of correctness and termination for REMAP, we present empirical evidence measuring correctness: how frequently the resulting reward machine is isomorphic under a consistent yet inexact teacher, and the regret between the ground truth and learned reward machines.
</details>
<details>
<summary>摘要</summary>
奖励机器有非常良好的承诺，用于捕捉复杂的动作序列学习任务中的非马普朗奖函数。然而，目前没有一种算法可以学习奖励机器的实际弱反馈，即偏好。我们贡献了一种新的算法，名为REMAP，可以从偏好中学习奖励机器，并提供正确性和结束性保证。REMAP在L*算法中引入偏好查询，并利用符号观察表和约束解决方法缩小偶极机器搜索空间。此外，我们还提供了REMAP的正确性和结束性证明，以及实验证明：如何频繁地使用一致却不准确的教师来证明奖励机器的同构性，以及奖励机器和真实奖励机器之间的偏差。
</details></li>
</ul>
<hr>
<h2 id="CARLA-A-Self-supervised-Contrastive-Representation-Learning-Approach-for-Time-Series-Anomaly-Detection"><a href="#CARLA-A-Self-supervised-Contrastive-Representation-Learning-Approach-for-Time-Series-Anomaly-Detection" class="headerlink" title="CARLA: A Self-supervised Contrastive Representation Learning Approach for Time Series Anomaly Detection"></a>CARLA: A Self-supervised Contrastive Representation Learning Approach for Time Series Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09296">http://arxiv.org/abs/2308.09296</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zahra Zamanzadeh Darban, Geoffrey I. Webb, Shirui Pan, Mahsa Salehi</li>
<li>for: 本研究旨在提出一种基于自适应对比学习的时间序列异常检测方法，以便更好地识别时间序列数据中的异常 patterns。</li>
<li>methods: 本方法使用对比表示学习，通过学习时间序列窗口中的相似和不相似表示，生成高效的异常检测模型。</li>
<li>results: 经过广泛的实验 validate，本方法在7个标准的真实世界时间序列异常检测基准数据集上达到了F1和AU-PR的超越现有状态艺的结果。<details>
<summary>Abstract</summary>
We introduce a Self-supervised Contrastive Representation Learning Approach for Time Series Anomaly Detection (CARLA), an innovative end-to-end self-supervised framework carefully developed to identify anomalous patterns in both univariate and multivariate time series data. By taking advantage of contrastive representation learning, We introduce an innovative end-to-end self-supervised deep learning framework carefully developed to identify anomalous patterns in both univariate and multivariate time series data. By taking advantage of contrastive representation learning, CARLA effectively generates robust representations for time series windows. It achieves this by 1) learning similar representations for temporally close windows and dissimilar representations for windows and their equivalent anomalous windows and 2) employing a self-supervised approach to classify normal/anomalous representations of windows based on their nearest/furthest neighbours in the representation space. Most of the existing models focus on learning normal behaviour. The normal boundary is often tightly defined, which can result in slight deviations being classified as anomalies, resulting in a high false positive rate and limited ability to generalise normal patterns. CARLA's contrastive learning methodology promotes the production of highly consistent and discriminative predictions, thereby empowering us to adeptly address the inherent challenges associated with anomaly detection in time series data. Through extensive experimentation on 7 standard real-world time series anomaly detection benchmark datasets, CARLA demonstrates F1 and AU-PR superior to existing state-of-the-art results. Our research highlights the immense potential of contrastive representation learning in advancing the field of time series anomaly detection, thus paving the way for novel applications and in-depth exploration in this domain.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种自然Language Modeling自我超vised representation learning方法（CARLA），用于时间序列异常检测。这是一种创新的端到端自我超vised深度学习框架，通过利用对比表示学习，以便在时间序列数据中检测异常patterns。我们通过1）学习近似的表示 для时间相近的窗口和不相似的表示 для窗口和其等异常窗口，以及2）使用自我超vised的方法来分类normal/异常的表示窗口基于其最近/最远的邻居在表示空间中来实现。大多数现有的模型都是学习正常行为，正常边界通常是非常紧张的，这可能导致非常小的偏差被分类为异常，从而导致高的假阳性率和有限的泛化能力。CARLA的对比学习方法ологиy可以生成高度一致和抑制的预测，因此可以有效地解决时间序列异常检测中的内在挑战。经过对7个标准实际时间序列异常检测benchmark数据集的广泛实验，CARLA的F1和AU-PR超过现有状态的最佳结果。我们的研究表明，对比表示学习在时间序列异常检测中具有极大的潜力，因此可以为这个领域开拓新的应用和深入探索。
</details></li>
</ul>
<hr>
<h2 id="How-important-are-specialized-transforms-in-Neural-Operators"><a href="#How-important-are-specialized-transforms-in-Neural-Operators" class="headerlink" title="How important are specialized transforms in Neural Operators?"></a>How important are specialized transforms in Neural Operators?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09293">http://arxiv.org/abs/2308.09293</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Ritam-M/LearnableTransformsNO">https://github.com/Ritam-M/LearnableTransformsNO</a></li>
<li>paper_authors: Ritam Majumdar, Shirish Karande, Lovekesh Vig</li>
<li>for: 本研究探讨了基于Transform-based Neural Operators的Physical System Simulation中的transform层的重要性。</li>
<li>methods: 本研究使用了 Replace all transform layers with learnable linear layers的方法来测试transform层的重要性。</li>
<li>results:  surprisingly, the results show that linear layers can provide performance comparable to the best-known transform-based layers, with a compute time advantage as well.<details>
<summary>Abstract</summary>
Simulating physical systems using Partial Differential Equations (PDEs) has become an indispensible part of modern industrial process optimization. Traditionally, numerical solvers have been used to solve the associated PDEs, however recently Transform-based Neural Operators such as the Fourier Neural Operator and Wavelet Neural Operator have received a lot of attention for their potential to provide fast solutions for systems of PDEs. In this work, we investigate the importance of the transform layers to the reported success of transform based neural operators. In particular, we record the cost in terms of performance, if all the transform layers are replaced by learnable linear layers. Surprisingly, we observe that linear layers suffice to provide performance comparable to the best-known transform-based layers and seem to do so with a compute time advantage as well. We believe that this observation can have significant implications for future work on Neural Operators, and might point to other sources of efficiencies for these architectures.
</details>
<details>
<summary>摘要</summary>
使用部分泛函方程（PDEs）模拟物理系统已成为现代工业过程优化的不可或缺的一部分。在传统上，数值分析器被用来解决相关的PDEs，但在最近，基于变换的神经网络运算符如傅立叶 нейронOperator和波幅 нейронOperator在解决系统PDEs方面受到了广泛的关注。在这项工作中，我们调查了变换层的重要性，特别是在报道最佳性能的情况下，所有变换层都被替换为学习的线性层。我们发现了一个意外的现象：线性层可以提供与最佳变换层相当的性能，并且在计算时间方面也有一定的优势。我们认为这一观察可能对未来神经网络架构的发展产生重要的影响，并可能指向其他效率来源。
</details></li>
</ul>
<hr>
<h2 id="Graph-based-Alignment-and-Uniformity-for-Recommendation"><a href="#Graph-based-Alignment-and-Uniformity-for-Recommendation" class="headerlink" title="Graph-based Alignment and Uniformity for Recommendation"></a>Graph-based Alignment and Uniformity for Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09292">http://arxiv.org/abs/2308.09292</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yangliangwei/graphau">https://github.com/yangliangwei/graphau</a></li>
<li>paper_authors: Liangwei Yang, Zhiwei Liu, Chen Wang, Mingdai Yang, Xiaolong Liu, Jing Ma, Philip S. Yu</li>
<li>for:  Collaborative filtering-based recommender systems (RecSys)</li>
<li>methods:  graph-based alignment and uniformity (GraphAU) approach, neighborhood aggregator, layer-wise alignment pooling module</li>
<li>results:  significantly alleviates the sparsity issue, achieves state-of-the-art performanceHere’s the simplified Chinese text:</li>
<li>for: collaborative filtering-based recommender systems (RecSys)</li>
<li>methods: 基于图的启发和均匀性（GraphAU）方法，包括邻居聚合器和层次启发聚合模块</li>
<li>results: 可以有效解决稀疏问题，实现了最佳性能<details>
<summary>Abstract</summary>
Collaborative filtering-based recommender systems (RecSys) rely on learning representations for users and items to predict preferences accurately. Representation learning on the hypersphere is a promising approach due to its desirable properties, such as alignment and uniformity. However, the sparsity issue arises when it encounters RecSys. To address this issue, we propose a novel approach, graph-based alignment and uniformity (GraphAU), that explicitly considers high-order connectivities in the user-item bipartite graph. GraphAU aligns the user/item embedding to the dense vector representations of high-order neighbors using a neighborhood aggregator, eliminating the need to compute the burdensome alignment to high-order neighborhoods individually. To address the discrepancy in alignment losses, GraphAU includes a layer-wise alignment pooling module to integrate alignment losses layer-wise. Experiments on four datasets show that GraphAU significantly alleviates the sparsity issue and achieves state-of-the-art performance. We open-source GraphAU at https://github.com/YangLiangwei/GraphAU.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="HyperLoRA-for-PDEs"><a href="#HyperLoRA-for-PDEs" class="headerlink" title="HyperLoRA for PDEs"></a>HyperLoRA for PDEs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09290">http://arxiv.org/abs/2308.09290</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ritam Majumdar, Vishal Jadhav, Anirudh Deodhar, Shirish Karande, Lovekesh Vig, Venkataramana Runkana</li>
<li>for: 解决参数化积分方程的解的难题，提高神经网络的泛化能力。</li>
<li>methods: 使用Hypernetworks技术，将每层base网络分解成低维度矩阵，并使用这些矩阵来预测神经网络的参数。</li>
<li>results: 在各种基础网络和任务下，LoRA-based HyperPINN训练可以快速学习参数化积分方程的解，并且可以在参数数量减少8倍的情况下维持准确性。<details>
<summary>Abstract</summary>
Physics-informed neural networks (PINNs) have been widely used to develop neural surrogates for solutions of Partial Differential Equations. A drawback of PINNs is that they have to be retrained with every change in initial-boundary conditions and PDE coefficients. The Hypernetwork, a model-based meta learning technique, takes in a parameterized task embedding as input and predicts the weights of PINN as output. Predicting weights of a neural network however, is a high-dimensional regression problem, and hypernetworks perform sub-optimally while predicting parameters for large base networks. To circumvent this issue, we use a low ranked adaptation (LoRA) formulation to decompose every layer of the base network into low-ranked tensors and use hypernetworks to predict the low-ranked tensors. Despite the reduced dimensionality of the resulting weight-regression problem, LoRA-based Hypernetworks violate the underlying physics of the given task. We demonstrate that the generalization capabilities of LoRA-based hypernetworks drastically improve when trained with an additional physics-informed loss component (HyperPINN) to satisfy the governing differential equations. We observe that LoRA-based HyperPINN training allows us to learn fast solutions for parameterized PDEs like Burger's equation and Navier Stokes: Kovasznay flow, while having an 8x reduction in prediction parameters on average without compromising on accuracy when compared to all other baselines.
</details>
<details>
<summary>摘要</summary>
物理学信息感知神经网络（PINNs）广泛应用于解决分数方程的解的神经替换器。PINNs的缺点是它们每次初始边界条件和微分方程系数改变时需要重新训练。 Hypernetwork，一种模型基于元学习技术，输入一个 parameterized task embedding，并预测 PINN 的重量。然而，预测神经网络参数是一个高维度回归问题，Hypernetworks 在预测基础网络参数时表现不佳。为了解决这个问题，我们使用 low-ranked adaptation（LoRA）形式将每层基础网络分解成低维度矩阵，并使用 Hypernetworks 预测低维度矩阵。尽管LoRA-based Hypernetworks 的结果维度降低，但是LoRA-based Hypernetworks 仍然违反了给定任务的物理基础。我们示出，在 HyperPINN 中添加物理学信息感知损失函数可以提高 LoRA-based Hypernetworks 的泛化能力。我们观察到，LoRA-based HyperPINN 训练可以快速解决参数化的微分方程，如布尔格方程和奈尔-斯托克斯：kovasznay 流动，而无需妥协准确性。在 average 情况下，LoRA-based HyperPINN 的预测参数数量减少了 8 倍，而不会影响准确性。
</details></li>
</ul>
<hr>
<h2 id="A-hybrid-Decoder-DeepONet-operator-regression-framework-for-unaligned-observation-data"><a href="#A-hybrid-Decoder-DeepONet-operator-regression-framework-for-unaligned-observation-data" class="headerlink" title="A hybrid Decoder-DeepONet operator regression framework for unaligned observation data"></a>A hybrid Decoder-DeepONet operator regression framework for unaligned observation data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09274">http://arxiv.org/abs/2308.09274</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cb-sjtu/decoder_deeponet">https://github.com/cb-sjtu/decoder_deeponet</a></li>
<li>paper_authors: Bo Chen, Chenyu Wang, Weipeng Li, Haiyang Fu</li>
<li>for: 处理不对齐的观测数据</li>
<li>methods: 提议了一种混合Decoder-DeepONet算法树立阶段，以及一种Multi-Decoder-DeepONet算法，通过使用训练数据的平均场来进行输入增强</li>
<li>results: 通过两个实验（Darcy问题和流场附近的飞机翼）， validate了提议的方法在处理不对齐观测数据方面的效率和准确性，并显示了这些方法在改进预测精度方面的潜力<details>
<summary>Abstract</summary>
Deep neural operators (DNOs) have been utilized to approximate nonlinear mappings between function spaces. However, DNOs face the challenge of increased dimensionality and computational cost associated with unaligned observation data. In this study, we propose a hybrid Decoder-DeepONet operator regression framework to handle unaligned data effectively. Additionally, we introduce a Multi-Decoder-DeepONet, which utilizes an average field of training data as input augmentation. The consistencies of the frameworks with the operator approximation theory are provided, on the basis of the universal approximation theorem. Two numerical experiments, Darcy problem and flow-field around an airfoil, are conducted to validate the efficiency and accuracy of the proposed methods. Results illustrate the advantages of Decoder-DeepONet and Multi-Decoder-DeepONet in handling unaligned observation data and showcase their potentials in improving prediction accuracy.
</details>
<details>
<summary>摘要</summary>
深度神经操作员 (DNO) 已被应用于函数空间中的非线性映射的近似。然而，DNO 面临不同观察数据的维度和计算成本的增加。在本研究中，我们提议了一种混合 Decoder-DeepONet 算法批处不同观察数据的批处方法。此外，我们还介绍了 Multi-Decoder-DeepONet，它利用训练数据的平均场作为输入增强。我们提供了基于 оператор近似理论的一致性，以确保方法的可靠性。在 Darcy 问题和风流附近的飞机翼上进行了两个数值实验，以验证提议的方法的效率和准确性。结果表明 Decoder-DeepONet 和 Multi-Decoder-DeepONet 可以有效地处理不同观察数据，并且在提高预测精度方面具有潜力。
</details></li>
</ul>
<hr>
<h2 id="Multi-Task-Pseudo-Label-Learning-for-Non-Intrusive-Speech-Quality-Assessment-Model"><a href="#Multi-Task-Pseudo-Label-Learning-for-Non-Intrusive-Speech-Quality-Assessment-Model" class="headerlink" title="Multi-Task Pseudo-Label Learning for Non-Intrusive Speech Quality Assessment Model"></a>Multi-Task Pseudo-Label Learning for Non-Intrusive Speech Quality Assessment Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09262">http://arxiv.org/abs/2308.09262</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryandhimas E. Zezario, Bo-Ren Brian Bai, Chiou-Shann Fuh, Hsin-Min Wang, Yu Tsao</li>
<li>for: 这种研究旨在开发一种非侵入式语音质量评估模型，使用多任务pseudo标签学习（MPL）技术。</li>
<li>methods: 该研究使用MPL技术，包括两个阶段：首先，从预训练模型中获取pseudo标签分数；其次，进行多任务学习。 selects three 3QUEST metricss：Speech-MOS（S-MOS）、Noise-MOS（N-MOS）和General-MOS（G-MOS）作为真实标签。使用预训练MOSA-Net模型来估算三个pseudo标签：语音质量评估（PESQ）、短时间对象智能度（STOI）和语音质量指数（SDI）。然后，使用多任务学习阶段来训练MTQ-Net模型（多目标语音质量评估网络）。该模型通过 combining Loss supervision（基于真实标签和pseudo标签之间的差异）和 Loss semi-supervision（基于pseudo标签和真实标签之间的差异）来优化。</li>
<li>results: 实验结果表明，MPL比训练模型从scratch和使用知识传递机制来训练模型都有优势。其次，使用柯伯损失函数来计算损失函数可以提高MTQ-Net模型的预测能力。最后，MTQ-Net模型使用MPL方法 exhibits higher overall prediction capabilities compared to other SSL-based speech assessment models。<details>
<summary>Abstract</summary>
This study introduces multi-task pseudo-label (MPL) learning for a non-intrusive speech quality assessment model. MPL consists of two stages which are obtaining pseudo-label scores from a pretrained model and performing multi-task learning. The 3QUEST metrics, namely Speech-MOS (S-MOS), Noise-MOS (N-MOS), and General-MOS (G-MOS) are selected as the primary ground-truth labels. Additionally, the pretrained MOSA-Net model is utilized to estimate three pseudo-labels: perceptual evaluation of speech quality (PESQ), short-time objective intelligibility (STOI), and speech distortion index (SDI). Multi-task learning stage of MPL is then employed to train the MTQ-Net model (multi-target speech quality assessment network). The model is optimized by incorporating Loss supervision (derived from the difference between the estimated score and the real ground-truth labels) and Loss semi-supervision (derived from the difference between the estimated score and pseudo-labels), where Huber loss is employed to calculate the loss function. Experimental results first demonstrate the advantages of MPL compared to training the model from scratch and using knowledge transfer mechanisms. Secondly, the benefits of Huber Loss in improving the prediction model of MTQ-Net are verified. Finally, the MTQ-Net with the MPL approach exhibits higher overall prediction capabilities when compared to other SSL-based speech assessment models.
</details>
<details>
<summary>摘要</summary>
In the multi-task learning stage of MPL, the MTQ-Net model (multi-target speech quality assessment network) is trained using Loss supervision and Loss semi-supervision. The Loss supervision is derived from the difference between the estimated score and the real ground-truth labels, while the Loss semi-supervision is derived from the difference between the estimated score and the pseudo-labels. The Huber loss is employed to calculate the loss function.Experimental results show that the MPL approach outperforms training the model from scratch and using knowledge transfer mechanisms. Additionally, the use of Huber loss in the MPL approach improves the prediction capabilities of the MTQ-Net model. Compared to other speech assessment models based on semi-supervised learning (SSL), the MTQ-Net with the MPL approach exhibits higher overall prediction capabilities.
</details></li>
</ul>
<hr>
<h2 id="Distribution-shift-mitigation-at-test-time-with-performance-guarantees"><a href="#Distribution-shift-mitigation-at-test-time-with-performance-guarantees" class="headerlink" title="Distribution shift mitigation at test time with performance guarantees"></a>Distribution shift mitigation at test time with performance guarantees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09259">http://arxiv.org/abs/2308.09259</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rui Ding, Jielong Yang, Feng Ji, Xionghu Zhong, Linbo Xie<br>for: 这个研究旨在提高Graph Neural Networks (GNNs)的测试性能，解决由于样本选择不适切和训练数据有限而导致的分布shift问题。methods: 我们提出了一个缩寸的FR-GNN框架，通过将训练好的GNN的输出和输入建立映射关系，以取得类别表示vector，然后使用这些vector来重建节点的特征。这些重建的节点特征可以直接用于测试已经训练好的模型，从而减少分布shift和提高测试性能。results: 我们提供了理论保证，并进行了多种公共数据集的实验。实验结果显示FR-GNN在与主流方法比较之下，具有更高的测试性能。<details>
<summary>Abstract</summary>
Due to inappropriate sample selection and limited training data, a distribution shift often exists between the training and test sets. This shift can adversely affect the test performance of Graph Neural Networks (GNNs). Existing approaches mitigate this issue by either enhancing the robustness of GNNs to distribution shift or reducing the shift itself. However, both approaches necessitate retraining the model, which becomes unfeasible when the model structure and parameters are inaccessible. To address this challenge, we propose FR-GNN, a general framework for GNNs to conduct feature reconstruction. FRGNN constructs a mapping relationship between the output and input of a well-trained GNN to obtain class representative embeddings and then uses these embeddings to reconstruct the features of labeled nodes. These reconstructed features are then incorporated into the message passing mechanism of GNNs to influence the predictions of unlabeled nodes at test time. Notably, the reconstructed node features can be directly utilized for testing the well-trained model, effectively reducing the distribution shift and leading to improved test performance. This remarkable achievement is attained without any modifications to the model structure or parameters. We provide theoretical guarantees for the effectiveness of our framework. Furthermore, we conduct comprehensive experiments on various public datasets. The experimental results demonstrate the superior performance of FRGNN in comparison to mainstream methods.
</details>
<details>
<summary>摘要</summary>
FR-GNN constructs a mapping relationship between the output and input of a well-trained GNN to obtain class representative embeddings and then uses these embeddings to reconstruct the features of labeled nodes. These reconstructed features are then incorporated into the message passing mechanism of GNNs to influence the predictions of unlabeled nodes at test time. Notably, the reconstructed node features can be directly utilized for testing the well-trained model, effectively reducing the distribution shift and leading to improved test performance. This remarkable achievement is attained without any modifications to the model structure or parameters. We provide theoretical guarantees for the effectiveness of our framework.Furthermore, we conduct comprehensive experiments on various public datasets, and the experimental results demonstrate the superior performance of FR-GNN in comparison to mainstream methods.
</details></li>
</ul>
<hr>
<h2 id="Capacity-Bounds-for-Hyperbolic-Neural-Network-Representations-of-Latent-Tree-Structures"><a href="#Capacity-Bounds-for-Hyperbolic-Neural-Network-Representations-of-Latent-Tree-Structures" class="headerlink" title="Capacity Bounds for Hyperbolic Neural Network Representations of Latent Tree Structures"></a>Capacity Bounds for Hyperbolic Neural Network Representations of Latent Tree Structures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09250">http://arxiv.org/abs/2308.09250</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anastasis Kratsios, Ruiyang Hong, Haitz Sáez de Ocáriz Borde</li>
<li>for: 这个论文探讨了深度偏函数神经网络（HNN）在偏函数空间中的表示能力。</li>
<li>methods: 作者使用了ReLU activation function和网络复杂度来证明HNN可以ε-同构将任意权重树 embed到偏函数空间中，其中偏函数空间的维度至少为2，并且偏函数弯曲度为负数κ&lt;0。</li>
<li>results: 作者发现HNN的网络复杂度与表示准确度无关，而且任何ReLU多层感知机（MLP）都必须在嵌入树时减少至少Ω(L^{1&#x2F;d})的准确度，独立于层数、宽度和活动函数。<details>
<summary>Abstract</summary>
We study the representation capacity of deep hyperbolic neural networks (HNNs) with a ReLU activation function. We establish the first proof that HNNs can $\varepsilon$-isometrically embed any finite weighted tree into a hyperbolic space of dimension $d$ at least equal to $2$ with prescribed sectional curvature $\kappa<0$, for any $\varepsilon> 1$ (where $\varepsilon=1$ being optimal). We establish rigorous upper bounds for the network complexity on an HNN implementing the embedding. We find that the network complexity of HNN implementing the graph representation is independent of the representation fidelity/distortion. We contrast this result against our lower bounds on distortion which any ReLU multi-layer perceptron (MLP) must exert when embedding a tree with $L>2^d$ leaves into a $d$-dimensional Euclidean space, which we show at least $\Omega(L^{1/d})$; independently of the depth, width, and (possibly discontinuous) activation function defining the MLP.
</details>
<details>
<summary>摘要</summary>
我们研究深度偏函数神经网络（HNN）的表示能力，使用ReLUActivation函数。我们证明了HNN可以将任意有重量的树 $\varepsilon$-同构到具有扁拟圆盘环境的空间中，其中维度至少为2，并且sectional curvature小于0，对于任何 $\varepsilon>1$（其中 $\varepsilon=1$ 是最优的）。我们也提出了准确的网络复杂度上限，用于HNN实现图像表示。我们发现网络复杂度与表示准确度无关。我们对此结果与我们对多层感知机（MLP）中的下界进行了比较，其中MLP必须在权重大于$2^d$的树中嵌入$d$-维欧式空间，并且下界至少为$\Omega(L^{1/d})$，无论深度、宽度和（可能不连续）活化函数。
</details></li>
</ul>
<hr>
<h2 id="Active-and-Passive-Causal-Inference-Learning"><a href="#Active-and-Passive-Causal-Inference-Learning" class="headerlink" title="Active and Passive Causal Inference Learning"></a>Active and Passive Causal Inference Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09248">http://arxiv.org/abs/2308.09248</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Jiwoong Im, Kyunghyun Cho</li>
<li>for: 这篇论文是为了提供机器学习研究者、工程师和学生，尝试了解但未 Familiar with causal inference 的人开始的一个起点。</li>
<li>methods: 本文从 collectively 需要的假设出发，包括交换性、正性、一致性和无干扰。从这些假设中，我们构建了一组重要的 causal inference 技术，分为两个桶：活跃和被动approaches。我们描述和讨论了随机化控制试验和bandit-based approaches从活跃类别，然后描述了经典方法，如匹配和反向概率权重，从被动类别中。最后，我们介绍了一些 causal inference 中缺失的方面，如迷宫偏见，以便让读者通过这篇论文获得多种开始点，进一步阅读和研究 causal inference 和发现。</li>
<li>results: 本文提供了一个多样化的起点，以便读者可以尝试不同的方法和技术，并进行进一步的研究和阅读。<details>
<summary>Abstract</summary>
This paper serves as a starting point for machine learning researchers, engineers and students who are interested in but not yet familiar with causal inference. We start by laying out an important set of assumptions that are collectively needed for causal identification, such as exchangeability, positivity, consistency and the absence of interference. From these assumptions, we build out a set of important causal inference techniques, which we do so by categorizing them into two buckets; active and passive approaches. We describe and discuss randomized controlled trials and bandit-based approaches from the active category. We then describe classical approaches, such as matching and inverse probability weighting, in the passive category, followed by more recent deep learning based algorithms. By finishing the paper with some of the missing aspects of causal inference from this paper, such as collider biases, we expect this paper to provide readers with a diverse set of starting points for further reading and research in causal inference and discovery.
</details>
<details>
<summary>摘要</summary>
Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China.
</details></li>
</ul>
<hr>
<h2 id="A-Robust-Policy-Bootstrapping-Algorithm-for-Multi-objective-Reinforcement-Learning-in-Non-stationary-Environments"><a href="#A-Robust-Policy-Bootstrapping-Algorithm-for-Multi-objective-Reinforcement-Learning-in-Non-stationary-Environments" class="headerlink" title="A Robust Policy Bootstrapping Algorithm for Multi-objective Reinforcement Learning in Non-stationary Environments"></a>A Robust Policy Bootstrapping Algorithm for Multi-objective Reinforcement Learning in Non-stationary Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09734">http://arxiv.org/abs/2308.09734</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sherif Abdelfattah, Kathryn Kasmarik, Jiankun Hu</li>
<li>for: 本研究旨在解决多目标Markov决策过程中的多目标优化问题，即在带有随机过程的环境中进行逐步决策，同时满足Markov性的条件。</li>
<li>methods: 本研究使用了多目标回归学习方法，将回归学习概念与多目标优化技术相结合，以解决这个问题。然而，现有的方法具有不适应非站点环境的缺点，因为它们采用了优化过程，假设环境是站点的。</li>
<li>results: 本研究提出了一种发展优化方法，可以在在线方式下，在定义的目标空间中，逐步演化策略覆盖集，以适应非站点环境。我们还提出了一种新的多目标回归学习算法，可以在非站点环境中稳定演化一个凸覆盖集的策略。与现有的算法相比，我们的算法在非站点环境中表现出了显著的优异性，而在站点环境中则达到了相似的结果。<details>
<summary>Abstract</summary>
Multi-objective Markov decision processes are a special kind of multi-objective optimization problem that involves sequential decision making while satisfying the Markov property of stochastic processes. Multi-objective reinforcement learning methods address this problem by fusing the reinforcement learning paradigm with multi-objective optimization techniques. One major drawback of these methods is the lack of adaptability to non-stationary dynamics in the environment. This is because they adopt optimization procedures that assume stationarity to evolve a coverage set of policies that can solve the problem. This paper introduces a developmental optimization approach that can evolve the policy coverage set while exploring the preference space over the defined objectives in an online manner. We propose a novel multi-objective reinforcement learning algorithm that can robustly evolve a convex coverage set of policies in an online manner in non-stationary environments. We compare the proposed algorithm with two state-of-the-art multi-objective reinforcement learning algorithms in stationary and non-stationary environments. Results showed that the proposed algorithm significantly outperforms the existing algorithms in non-stationary environments while achieving comparable results in stationary environments.
</details>
<details>
<summary>摘要</summary>
多目标马尔可夫决策过程是一种特殊的多目标优化问题，它涉及到顺序做出决策，并满足马尔可夫性Property的游程过程。多目标返点学习方法解决了这个问题，它将返点学习 paradigma与多目标优化技术相结合。然而，这些方法的一个主要缺点是缺乏适应非站ARY动态环境的能力。这是因为它们采用了优化过程，假设环境是站ARY的，以演算出一个覆盖集的策略，可以解决问题。本文提出了一种发展优化方法，可以在线模式下演化策略覆盖集，同时在定义的目标空间中探索喜好。我们提出了一种新的多目标返点学习算法，可以在线模式下鲁棒地演化一个凸覆盖集的策略，并在非站ARY环境中显著超越了现有的两种多目标返点学习算法。结果表明，提posed algorithm在非站ARY环境中具有显著的优势，而在站ARY环境中则具有相对的优势。
</details></li>
</ul>
<hr>
<h2 id="Intrinsically-Motivated-Hierarchical-Policy-Learning-in-Multi-objective-Markov-Decision-Processes"><a href="#Intrinsically-Motivated-Hierarchical-Policy-Learning-in-Multi-objective-Markov-Decision-Processes" class="headerlink" title="Intrinsically Motivated Hierarchical Policy Learning in Multi-objective Markov Decision Processes"></a>Intrinsically Motivated Hierarchical Policy Learning in Multi-objective Markov Decision Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09733">http://arxiv.org/abs/2308.09733</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sherif Abdelfattah, Kathryn Merrick, Jiankun Hu</li>
<li>for: 解决多目标Markov决策过程中的多个冲突奖励函数不能同时优化， conventional single optimal policy 不能解决这类问题。</li>
<li>methods: 使用 multi-objective reinforcement learning 方法，演化一个可满足所有偏好的优化策略集。</li>
<li>results: 在动态环境中，提出了一种新的 dual-phase intrinsically motivated reinforcement learning 方法，在第一阶段学习一个通用技能集，在第二阶段使用这个集 bootstrap policy coverage sets for each shift in the environment dynamics，实验显示该方法在动态环境中表现出优于state-of-the-art多目标决策方法。<details>
<summary>Abstract</summary>
Multi-objective Markov decision processes are sequential decision-making problems that involve multiple conflicting reward functions that cannot be optimized simultaneously without a compromise. This type of problems cannot be solved by a single optimal policy as in the conventional case. Alternatively, multi-objective reinforcement learning methods evolve a coverage set of optimal policies that can satisfy all possible preferences in solving the problem. However, many of these methods cannot generalize their coverage sets to work in non-stationary environments. In these environments, the parameters of the state transition and reward distribution vary over time. This limitation results in significant performance degradation for the evolved policy sets. In order to overcome this limitation, there is a need to learn a generic skill set that can bootstrap the evolution of the policy coverage set for each shift in the environment dynamics therefore, it can facilitate a continuous learning process. In this work, intrinsically motivated reinforcement learning has been successfully deployed to evolve generic skill sets for learning hierarchical policies to solve multi-objective Markov decision processes. We propose a novel dual-phase intrinsically motivated reinforcement learning method to address this limitation. In the first phase, a generic set of skills is learned. While in the second phase, this set is used to bootstrap policy coverage sets for each shift in the environment dynamics. We show experimentally that the proposed method significantly outperforms state-of-the-art multi-objective reinforcement methods in a dynamic robotics environment.
</details>
<details>
<summary>摘要</summary>
多目标Markov决策过程是一种sequential decision-making问题，其涉及多个矛盾的奖励函数，无法同时优化这些奖励函数。这类问题不可以通过单一优化策略来解决，相反，多目标学习方法会演化一个包含多个优化策略的coverage集，以满足所有可能的偏好。然而，许多这些方法无法扩展其coverage集以适应非站ARY环境。在这些环境中，状态转移和奖励分布的参数会随时间变化。这限制了演化出来的策略集的性能。为了突破这一限制，需要学习一个通用技能集，以便在环境动态变化时，使用这个技能集来演化策略集，从而实现连续学习过程。在这个工作中，我们成功地应用了内在激励学习方法来演化通用技能集，以解决多目标Markov决策过程中的限制。我们提出了一种新的双相内在激励学习方法，其在第一阶段学习一个通用技能集，而在第二阶段使用这个集来 bootstrap策略集，以适应环境动态变化。我们通过实验表明，提议的方法在动态 робо扮环境中能够显著超越现状的多目标决策方法。
</details></li>
</ul>
<hr>
<h2 id="Generalized-Sum-Pooling-for-Metric-Learning"><a href="#Generalized-Sum-Pooling-for-Metric-Learning" class="headerlink" title="Generalized Sum Pooling for Metric Learning"></a>Generalized Sum Pooling for Metric Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09228">http://arxiv.org/abs/2308.09228</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yetigurbuz/generalized-sum-pooling">https://github.com/yetigurbuz/generalized-sum-pooling</a></li>
<li>paper_authors: Yeti Z. Gurbuz, Ozan Sener, A. Aydın Alatan</li>
<li>for: This paper focuses on improving the global average pooling (GAP) method in deep metric learning by proposing a learnable generalized sum pooling (GSP) method.</li>
<li>methods: The proposed GSP method uses an entropy-smoothed optimal transport problem to learn the weights for aggregating feature vectors, and it has two distinct abilities: i) selecting a subset of semantic entities and ii) learning the importance of each entity.</li>
<li>results: The proposed GSP method is evaluated on four popular metric learning benchmarks and shows improved performance compared to the traditional GAP method.Here’s the simplified Chinese text format you requested:</li>
<li>for: 本研究探讨了深度度量学中的全局平均汇聚方法（GAP）的改进方法，提出了一种可学习的总和汇聚方法（GSP）。</li>
<li>methods: GSP方法使用了一种感知平滑的优质运输问题来学习归一化特征向量的权重，具有两种特点：一是选择一 subset of semantic entities，二是学习每个实体的重要性。</li>
<li>results: GSP方法在四个流行的度量学挑战 task 上进行了广泛的评估，与传统的GAP方法相比，表现出了改进的效果。<details>
<summary>Abstract</summary>
A common architectural choice for deep metric learning is a convolutional neural network followed by global average pooling (GAP). Albeit simple, GAP is a highly effective way to aggregate information. One possible explanation for the effectiveness of GAP is considering each feature vector as representing a different semantic entity and GAP as a convex combination of them. Following this perspective, we generalize GAP and propose a learnable generalized sum pooling method (GSP). GSP improves GAP with two distinct abilities: i) the ability to choose a subset of semantic entities, effectively learning to ignore nuisance information, and ii) learning the weights corresponding to the importance of each entity. Formally, we propose an entropy-smoothed optimal transport problem and show that it is a strict generalization of GAP, i.e., a specific realization of the problem gives back GAP. We show that this optimization problem enjoys analytical gradients enabling us to use it as a direct learnable replacement for GAP. We further propose a zero-shot loss to ease the learning of GSP. We show the effectiveness of our method with extensive evaluations on 4 popular metric learning benchmarks. Code is available at: GSP-DML Framework
</details>
<details>
<summary>摘要</summary>
一种常见的深度学习架构选择是使用卷积神经网络后跟global average pooling（GAP）。尽管简单，但GAP是一种非常有效的信息汇集方法。一种可能的解释是每个特征向量都代表不同的semantic entity，GAP可以看作这些entity的convex combination。基于这个视角，我们扩展GAP并提出一种学习可能的通用汇集方法（GSP）。GSP在两个方面超越GAP：一是可以选择一 subset of semantic entities，有效地忽略干扰信息；二是可以学习每个entity的重要性权重。我们提出一个熵平滑的优化Transport问题，并证明它是GAP的严格泛化，即特定的实现问题可以回归GAP。我们还提出了一个零战损损失来使GSP更加容易学习。我们通过对4个流行的metric learning benchmark进行了广泛的评估，证明了我们的方法的有效性。代码可以在：GSP-DML框架中找到。
</details></li>
</ul>
<hr>
<h2 id="Advancing-Relation-Extraction-through-Language-Probing-with-Exemplars-from-Set-Co-Expansion"><a href="#Advancing-Relation-Extraction-through-Language-Probing-with-Exemplars-from-Set-Co-Expansion" class="headerlink" title="Advancing Relation Extraction through Language Probing with Exemplars from Set Co-Expansion"></a>Advancing Relation Extraction through Language Probing with Exemplars from Set Co-Expansion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11720">http://arxiv.org/abs/2308.11720</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yerong Li, Roxana Girju</li>
<li>For: The primary goal of the paper is to enhance relation classification accuracy and mitigate confusion between contrastive classes in relation extraction.* Methods: The authors propose a multi-faceted approach that integrates representative examples and through co-set expansion, which involves seeding each relationship class with representative examples and incorporating similarity measures between target pairs and representative pairs from the target class.* Results: The authors achieve a significant enhancement of relation classification performance, with an observed margin of at least 1 percent improvement in accuracy in most settings, on top of existing fine-tuning approaches. Additionally, the authors conduct an in-depth analysis of tuning contrastive examples to reduce confusion between classes sharing similarities and achieve more precise classification.Here are the three key points in Simplified Chinese:* For: 本研究的主要目标是提高关系分类精度，并减少对异类关系的混淆。* Methods: 作者们提出了一种多方面的方法，即通过代表示例和通过合并扩展来提高关系分类精度。* Results: 作者们实现了一个显著提高关系分类性能的效果，在大多数情况下，与现有的精度提高方法相比，观测到了至少1%的提高。此外，作者们还进行了深入分析，并发现通过调整对异类关系的对照示例，可以更好地减少对异类关系的混淆。<details>
<summary>Abstract</summary>
Relation Extraction (RE) is a pivotal task in automatically extracting structured information from unstructured text. In this paper, we present a multi-faceted approach that integrates representative examples and through co-set expansion. The primary goal of our method is to enhance relation classification accuracy and mitigating confusion between contrastive classes.   Our approach begins by seeding each relationship class with representative examples. Subsequently, our co-set expansion algorithm enriches training objectives by incorporating similarity measures between target pairs and representative pairs from the target class. Moreover, the co-set expansion process involves a class ranking procedure that takes into account exemplars from contrastive classes. Contextual details encompassing relation mentions are harnessed via context-free Hearst patterns to ascertain contextual similarity.   Empirical evaluation demonstrates the efficacy of our co-set expansion approach, resulting in a significant enhancement of relation classification performance. Our method achieves an observed margin of at least 1 percent improvement in accuracy in most settings, on top of existing fine-tuning approaches. To further refine our approach, we conduct an in-depth analysis that focuses on tuning contrastive examples. This strategic selection and tuning effectively reduce confusion between classes sharing similarities, leading to a more precise classification process.   Experimental results underscore the effectiveness of our proposed framework for relation extraction. The synergy between co-set expansion and context-aware prompt tuning substantially contributes to improved classification accuracy. Furthermore, the reduction in confusion between contrastive classes through contrastive examples tuning validates the robustness and reliability of our method.
</details>
<details>
<summary>摘要</summary>
<<SYS>>TRANSLATE_TEXTRelation Extraction (RE) 是自动提取结构化信息的重要任务。在这篇文章中，我们提出了一个多元的方法，它结合代表性的例子和通过协同扩展。我们的方法的主要目标是增加关系分类精度，并减少相似类别的混淆。我们的方法开始由每个关系类别中的代表性例子。然后，我们的协同扩展算法将训练目标扩展到包含关系提及的上下文特征。此外，协同扩展过程还包括一个类别排名程序，它考虑了对应类别的例子。上下文特征探索关系提及的上下文特征，使用 Context-free Hearst 模式来确定上下文相似性。实验结果显示我们的协同扩展方法的有效性，它导致关系分类精度的明显提高。我们的方法在大多数情况下得到观察的改善率至少1%，在现有的调整方法之上。为了进一步改进我们的方法，我们进行了深入的分析，专注于调整对照例子。这种策略性的选择和调整有效地减少了相似类别之间的混淆，导致更精确的分类过程。实验结果证明我们的提出的框架具有优秀的效果，协同扩展和上下文相似性调整的融合对关系分类精度有重要贡献。此外，透过对对照例子的调整，我们的方法 Validates the robustness and reliability of our method.<<SYS</SYS>Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="DMCVR-Morphology-Guided-Diffusion-Model-for-3D-Cardiac-Volume-Reconstruction"><a href="#DMCVR-Morphology-Guided-Diffusion-Model-for-3D-Cardiac-Volume-Reconstruction" class="headerlink" title="DMCVR: Morphology-Guided Diffusion Model for 3D Cardiac Volume Reconstruction"></a>DMCVR: Morphology-Guided Diffusion Model for 3D Cardiac Volume Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09223">http://arxiv.org/abs/2308.09223</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hexiaoxiao-cs/dmcvr">https://github.com/hexiaoxiao-cs/dmcvr</a></li>
<li>paper_authors: Xiaoxiao He, Chaowei Tan, Ligong Han, Bo Liu, Leon Axel, Kang Li, Dimitris N. Metaxas</li>
<li>for: 提高心脏疾病诊断和治疗规划的精度，通过从缺乏2D图像堆栈中生成高分辨率3D心脏剖面图像。</li>
<li>methods: 基于生成模型，使用形态导向的扩散模型DMCVR，通过控制心脏形态来提高生成质量，并从学习的秘密空间获得全球semantics、局部心脏形态和每个2DcMRI片的细节。</li>
<li>results: 在多个方面表现出色，包括2D生成和3D重建性能，可以生成高分辨率3D心脏MRI重建图像，超过当前技术。<details>
<summary>Abstract</summary>
Accurate 3D cardiac reconstruction from cine magnetic resonance imaging (cMRI) is crucial for improved cardiovascular disease diagnosis and understanding of the heart's motion. However, current cardiac MRI-based reconstruction technology used in clinical settings is 2D with limited through-plane resolution, resulting in low-quality reconstructed cardiac volumes. To better reconstruct 3D cardiac volumes from sparse 2D image stacks, we propose a morphology-guided diffusion model for 3D cardiac volume reconstruction, DMCVR, that synthesizes high-resolution 2D images and corresponding 3D reconstructed volumes. Our method outperforms previous approaches by conditioning the cardiac morphology on the generative model, eliminating the time-consuming iterative optimization process of the latent code, and improving generation quality. The learned latent spaces provide global semantics, local cardiac morphology and details of each 2D cMRI slice with highly interpretable value to reconstruct 3D cardiac shape. Our experiments show that DMCVR is highly effective in several aspects, such as 2D generation and 3D reconstruction performance. With DMCVR, we can produce high-resolution 3D cardiac MRI reconstructions, surpassing current techniques. Our proposed framework has great potential for improving the accuracy of cardiac disease diagnosis and treatment planning. Code can be accessed at https://github.com/hexiaoxiao-cs/DMCVR.
</details>
<details>
<summary>摘要</summary>
准确的3D冠状动脉重建从cine磁共振成像（cMRI）是诊断心血管疾病的关键，以及心脏运动的理解。然而，现有的医学应用中的cardiac MRI重建技术仅为2D，具有有限的沿plane分辨率，导致低质量重建的冠状动脉体积。为了提高3D冠状动脉体积的重建质量，我们提议一种基于形态指导的扩散模型，DMCVR，该模型可以将高分辨率的2D图像和相应的3D重建体积相互关联。我们的方法比前一代方法更高效，因为它将cardiac形态条件在生成模型中，消除了时间消耗的迭代优化过程，并提高生成质量。学习的积分空间提供了全球 semantics、局部冠状动脉形态和每个2D cMRI slice的高分辨率重建。我们的实验表明，DMCVR在多个方面都具有高效性，如2D生成和3D重建性能。通过DMCVR，我们可以生成高分辨率的3D冠状动脉MRI重建，超越现有技术。我们提出的框架具有诊断心血管疾病的准确性和治疗规划的潜在优势。代码可以在https://github.com/hexiaoxiao-cs/DMCVR中获取。
</details></li>
</ul>
<hr>
<h2 id="Baird-Counterexample-Is-Solved-with-an-example-of-How-to-Debug-a-Two-time-scale-Algorithm"><a href="#Baird-Counterexample-Is-Solved-with-an-example-of-How-to-Debug-a-Two-time-scale-Algorithm" class="headerlink" title="Baird Counterexample Is Solved: with an example of How to Debug a Two-time-scale Algorithm"></a>Baird Counterexample Is Solved: with an example of How to Debug a Two-time-scale Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09732">http://arxiv.org/abs/2308.09732</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hengshuai Yao</li>
<li>for: 本文是用来解释TD(0)算法在Baird counterexample中的异常行为，以及two-time-scale随机近似算法的调试分析。</li>
<li>methods: 本文使用了TD(0)算法和Impression GTD算法来解释Baird counterexample的行为，并提供了Debugging技术来研究这种行为。</li>
<li>results: 本文的实验结果表明，Impression GTD算法在Baird counterexample中的收敛速度非常快，甚至在线性速度。这表明了Baird counterexample已经得到了解决。<details>
<summary>Abstract</summary>
Baird counterexample was proposed by Leemon Baird in 1995, first used to show that the Temporal Difference (TD(0)) algorithm diverges on this example. Since then, it is often used to test and compare off-policy learning algorithms. Gradient TD algorithms solved the divergence issue of TD on Baird counterexample. However, their convergence on this example is still very slow, and the nature of the slowness is not well understood, e.g., see (Sutton and Barto 2018).   This note is to understand in particular, why TDC is slow on this example, and provide debugging analysis to understand this behavior. Our debugging technique can be used to study the convergence behavior of two-time-scale stochastic approximation algorithms. We also provide empirical results of the recent Impression GTD algorithm on this example, showing the convergence is very fast, in fact, in a linear rate. We conclude that Baird counterexample is solved, by an algorithm with convergence guarantee to the TD solution in general and a fast convergence rate.
</details>
<details>
<summary>摘要</summary>
白尔德反例（Baird counterexample）由Leemon Baird在1995年提出，用以表明TD(0)算法在这个例子中出现偏离。自此以后，这个例子经常用于测试和比较不同的离政学习算法。梯度TD算法解决了TD算法在白尔德反例中的偏离问题，但是它们在这个例子上的减速还是很慢，并且这种慢速度的原因还不很清楚，比如参见（Sutton和Barto 2018）。本文的目的是要理解TDCSlow的原因，并提供调试分析来理解这种行为。我们的调试技术可以用来研究两个时间尺度的随机抽象算法的转化行为。我们还提供了最近的Impression GTD算法在这个例子上的实验结果，显示它的转化非常快，甚至是线性减速。我们 conclude that Baird counterexample已经得到解决，TDCSlow的问题也得到解决，并且有一个可靠的转化率。
</details></li>
</ul>
<hr>
<h2 id="A-Model-Agnostic-Framework-for-Recommendation-via-Interest-aware-Item-Embeddings"><a href="#A-Model-Agnostic-Framework-for-Recommendation-via-Interest-aware-Item-Embeddings" class="headerlink" title="A Model-Agnostic Framework for Recommendation via Interest-aware Item Embeddings"></a>A Model-Agnostic Framework for Recommendation via Interest-aware Item Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09202">http://arxiv.org/abs/2308.09202</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amit Kumar Jaiswal, Yu Xiong</li>
<li>for: 这个研究旨在提高推荐系统中的ITEM表示，以更好地捕捉用户的兴趣。</li>
<li>methods: 该研究提出了一种名为Interest-aware Capsule network（IaCN）的新型推荐模型，该模型通过 direkt learning interest-oriented item representations来提高推荐效果。</li>
<li>results: 实验结果表明， compared to existing recommendation models, IaCN 可以在不同的深度神经网络、行为序列长度和共同学习率下提高推荐性能。<details>
<summary>Abstract</summary>
Item representation holds significant importance in recommendation systems, which encompasses domains such as news, retail, and videos. Retrieval and ranking models utilise item representation to capture the user-item relationship based on user behaviours. While existing representation learning methods primarily focus on optimising item-based mechanisms, such as attention and sequential modelling. However, these methods lack a modelling mechanism to directly reflect user interests within the learned item representations. Consequently, these methods may be less effective in capturing user interests indirectly. To address this challenge, we propose a novel Interest-aware Capsule network (IaCN) recommendation model, a model-agnostic framework that directly learns interest-oriented item representations. IaCN serves as an auxiliary task, enabling the joint learning of both item-based and interest-based representations. This framework adopts existing recommendation models without requiring substantial redesign. We evaluate the proposed approach on benchmark datasets, exploring various scenarios involving different deep neural networks, behaviour sequence lengths, and joint learning ratios of interest-oriented item representations. Experimental results demonstrate significant performance enhancements across diverse recommendation models, validating the effectiveness of our approach.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="TinyProp-–-Adaptive-Sparse-Backpropagation-for-Efficient-TinyML-On-device-Learning"><a href="#TinyProp-–-Adaptive-Sparse-Backpropagation-for-Efficient-TinyML-On-device-Learning" class="headerlink" title="TinyProp – Adaptive Sparse Backpropagation for Efficient TinyML On-device Learning"></a>TinyProp – Adaptive Sparse Backpropagation for Efficient TinyML On-device Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09201">http://arxiv.org/abs/2308.09201</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcus Rüb, Daniel Maier, Daniel Mueller-Gritschneder, Axel Sikora</li>
<li>for: 这篇论文是用于提高深度神经网络在内存和计算上的效率，以便在低功率微控制器单元（MCU）上进行在机器上的学习或精细化神经网络。</li>
<li>methods: 这篇论文使用的方法是使用逐步练习，将神经网络中的一部分权重和偏置给训练。这些权重和偏置会在训练过程中进行动态调整，以确保在每个训练步骤中都能够获得最佳的结果。</li>
<li>results: 这篇论文的结果显示，使用TinyProp方法可以在MCU上进行在机器上的学习，并且比非 sparse 训练更快，且仅受到小的计算开销。具体来说，与非 sparse 训练相比，TinyProp可以提高平均速度约5倍，并且仅受到1%的精度损失。此外，与现有的静态 sparse backpropagation方法相比，TinyProp可以提高平均速度约2.9倍，并且精度损失降低约6%。<details>
<summary>Abstract</summary>
Training deep neural networks using backpropagation is very memory and computationally intensive. This makes it difficult to run on-device learning or fine-tune neural networks on tiny, embedded devices such as low-power micro-controller units (MCUs). Sparse backpropagation algorithms try to reduce the computational load of on-device learning by training only a subset of the weights and biases. Existing approaches use a static number of weights to train. A poor choice of this so-called backpropagation ratio limits either the computational gain or can lead to severe accuracy losses. In this paper we present TinyProp, the first sparse backpropagation method that dynamically adapts the back-propagation ratio during on-device training for each training step. TinyProp induces a small calculation overhead to sort the elements of the gradient, which does not significantly impact the computational gains. TinyProp works particularly well on fine-tuning trained networks on MCUs, which is a typical use case for embedded applications. For typical datasets from three datasets MNIST, DCASE2020 and CIFAR10, we are 5 times faster compared to non-sparse training with an accuracy loss of on average 1%. On average, TinyProp is 2.9 times faster than existing, static sparse backpropagation algorithms and the accuracy loss is reduced on average by 6 % compared to a typical static setting of the back-propagation ratio.
</details>
<details>
<summary>摘要</summary>
培训深度神经网络使用反传播是非常占用内存和计算资源的。这使得在设备学习或精细调整神经网络的小型、嵌入式设备（如低功耗微控制器单元）上运行困难。稀疏反传播算法试图减少设备上学习的计算负担，只培训一部分权重和偏好。现有方法使用静态的反传播比率来培训。这种称为反传播比率的选择有限制 either the computational gain or can lead to severe accuracy losses。在这篇论文中，我们介绍了TinyProp，第一种动态适应设备上培训中每步反传播比率的稀疏反传播方法。TinyProp需要小量的计算开销来排序梯度元素，这并不会对计算减少的影响。TinyProp在MCUs上精细调整已经训练的网络 particularly well，这是常见的嵌入式应用场景。对于典型的MNIST、DCASE2020和CIFAR10数据集，我们比非稀疏培训更快5倍，减少了平均1%的精度损失。相比已有的静态稀疏反传播算法，TinyProp在平均上2.9倍快，并且在平均上减少了6%的精度损失。
</details></li>
</ul>
<hr>
<h2 id="Polynomial-Bounds-for-Learning-Noisy-Optical-Physical-Unclonable-Functions-and-Connections-to-Learning-With-Errors"><a href="#Polynomial-Bounds-for-Learning-Noisy-Optical-Physical-Unclonable-Functions-and-Connections-to-Learning-With-Errors" class="headerlink" title="Polynomial Bounds for Learning Noisy Optical Physical Unclonable Functions and Connections to Learning With Errors"></a>Polynomial Bounds for Learning Noisy Optical Physical Unclonable Functions and Connections to Learning With Errors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09199">http://arxiv.org/abs/2308.09199</a></li>
<li>repo_url: None</li>
<li>paper_authors: Apollo Albright, Boris Gelfand, Michael Dixon</li>
<li>for: 这个论文旨在研究一类光学物理不可克隆函数（PUF）是否可以在噪声存在下，通过许多挑战对应的回应对，学习到任意精度，并且可以在计算机术语下进行高效的学习。</li>
<li>methods: 这篇论文使用了一种基于多项式的拟合算法，通过对噪声存在下的挑战对应的回应对进行学习，来实现PUF的学习。</li>
<li>results: 论文表明，通过许多挑战对应的回应对，可以在噪声存在下，通过计算机术语下的高效拟合算法，学习到光学物理不可克隆函数（PUF）的任意精度，并且可以在 polynomial 时间内完成。<details>
<summary>Abstract</summary>
It is shown that a class of optical physical unclonable functions (PUFs) can be learned to arbitrary precision with arbitrarily high probability, even in the presence of noise, given access to polynomially many challenge-response pairs and polynomially bounded computational power, under mild assumptions about the distributions of the noise and challenge vectors. This extends the results of Rh\"uramir et al. (2013), who showed a subset of this class of PUFs to be learnable in polynomial time in the absence of noise, under the assumption that the optics of the PUF were either linear or had negligible nonlinear effects. We derive polynomial bounds for the required number of samples and the computational complexity of a linear regression algorithm, based on size parameters of the PUF, the distributions of the challenge and noise vectors, and the probability and accuracy of the regression algorithm, with a similar analysis to one done by Bootle et al. (2018), who demonstrated a learning attack on a poorly implemented version of the Learning With Errors problem.
</details>
<details>
<summary>摘要</summary>
据显示，一类光学物理不可复制函数（PUFs）可以准确地学习到任意精度，即使在噪声存在的情况下，只要有对挑战响应对的极多数对话和计算能力，且假设噪声和挑战向量的分布都是某种可以考虑的。这些结果超越了 Rh\"uramir et al. (2013) 所示的一个子集的 PUFs，他们表明这些 PUFs 可以在噪声缺失的情况下，在 polynomial 时间内学习，假设 optics 的 PUF 是 линей的或具有可忽略的非线性效应。我们 derive 了一些多项式的样本数和计算复杂性的下界，基于 PUF 的大小参数、挑战向量和噪声向量的分布、学习算法的概率和准确率，与 Bootle et al. (2018) 所做的一个类似的分析。
</details></li>
</ul>
<hr>
<h2 id="Half-Hop-A-graph-upsampling-approach-for-slowing-down-message-passing"><a href="#Half-Hop-A-graph-upsampling-approach-for-slowing-down-message-passing" class="headerlink" title="Half-Hop: A graph upsampling approach for slowing down message passing"></a>Half-Hop: A graph upsampling approach for slowing down message passing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09198">http://arxiv.org/abs/2308.09198</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nerdslab/halfhop">https://github.com/nerdslab/halfhop</a></li>
<li>paper_authors: Mehdi Azabou, Venkataramana Ganesh, Shantanu Thakoor, Chi-Heng Lin, Lakshmi Sathidevi, Ran Liu, Michal Valko, Petar Veličković, Eva L. Dyer</li>
<li>for: 提高Message Passing神经网络的学习效果，尤其是在邻居节点属于不同类别时。</li>
<li>methods: 引入”慢节点”，用于媒介源节点和目标节点之间的通信，从而减少消息传递过程中的过滤效果。</li>
<li>results: 在多种监督和自监学 benchmark 上提高了表现，特别是在hetrophilic 条件下， где邻居节点更有可能属于不同的类别。 Additionally, the approach can be used to generate augmentations for self-supervised learning, by introducing slow nodes into different edges in the graph to generate multi-scale views with variable path lengths.<details>
<summary>Abstract</summary>
Message passing neural networks have shown a lot of success on graph-structured data. However, there are many instances where message passing can lead to over-smoothing or fail when neighboring nodes belong to different classes. In this work, we introduce a simple yet general framework for improving learning in message passing neural networks. Our approach essentially upsamples edges in the original graph by adding "slow nodes" at each edge that can mediate communication between a source and a target node. Our method only modifies the input graph, making it plug-and-play and easy to use with existing models. To understand the benefits of slowing down message passing, we provide theoretical and empirical analyses. We report results on several supervised and self-supervised benchmarks, and show improvements across the board, notably in heterophilic conditions where adjacent nodes are more likely to have different labels. Finally, we show how our approach can be used to generate augmentations for self-supervised learning, where slow nodes are randomly introduced into different edges in the graph to generate multi-scale views with variable path lengths.
</details>
<details>
<summary>摘要</summary>
We provide theoretical and empirical analyses to demonstrate the benefits of slowing down message passing. Our results on several supervised and self-supervised benchmarks show improvements across the board, particularly in heterophilic conditions where adjacent nodes are more likely to have different labels. Additionally, we show how our approach can be used to generate augmentations for self-supervised learning, where slow nodes are randomly introduced into different edges in the graph to generate multi-scale views with variable path lengths.In simplified Chinese:message passing neural networks 在图structured data 上表现出了很多成功，但是它们可能会导致过滤或失败当邻居节点属于不同的类。在这个工作中，我们介绍了一种简单 yet general的框架，用于提高message passing neural networks 的学习。我们的方法添加了每个边的 "slow node"，以便在源节点和目标节点之间进行中间调度。我们的方法只需修改输入图，使其易于使用现有的模型。我们提供了理论和实验分析，以证明减速消息传递的好处。我们的结果表明，在多种supervised和self-supervised benchmarks 上，我们的方法都能获得提高，特别是在邻居节点属于不同类时。此外，我们还示出了如何使用我们的方法生成自适应学习的扩展， где slow node 在不同的边上被随机添加，以生成多缓度视图和变量路径长。
</details></li>
</ul>
<hr>
<h2 id="A-Comparative-Study-of-Text-Embedding-Models-for-Semantic-Text-Similarity-in-Bug-Reports"><a href="#A-Comparative-Study-of-Text-Embedding-Models-for-Semantic-Text-Similarity-in-Bug-Reports" class="headerlink" title="A Comparative Study of Text Embedding Models for Semantic Text Similarity in Bug Reports"></a>A Comparative Study of Text Embedding Models for Semantic Text Similarity in Bug Reports</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09193">http://arxiv.org/abs/2308.09193</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/av9ash/duplicatebugdetection">https://github.com/av9ash/duplicatebugdetection</a></li>
<li>paper_authors: Avinash Patil, Kihwan Han, Sabyasachi Mukhopadhyay</li>
<li>for: 本研究旨在比较不同嵌入模型在撷取相似 bug report 方面的效果，以帮助减少问题解决的时间和努力。</li>
<li>methods: 本研究使用了多种嵌入模型，包括 TF-IDF (基线), FastText, Gensim, BERT, 和 ADA，并使用 Software Defects Data 来评估这些模型的表现。</li>
<li>results: 实验结果显示 BERT 通常在 recall 方面表现最好，其次是 ADA, Gensim, FastText, 和 TF-IDF。本研究提供了不同嵌入模型在撷取相似 bug report 方面的效果，并显示了选择适当的嵌入模型对这个任务的重要性。<details>
<summary>Abstract</summary>
Bug reports are an essential aspect of software development, and it is crucial to identify and resolve them quickly to ensure the consistent functioning of software systems. Retrieving similar bug reports from an existing database can help reduce the time and effort required to resolve bugs. In this paper, we compared the effectiveness of semantic textual similarity methods for retrieving similar bug reports based on a similarity score. We explored several embedding models such as TF-IDF (Baseline), FastText, Gensim, BERT, and ADA. We used the Software Defects Data containing bug reports for various software projects to evaluate the performance of these models. Our experimental results showed that BERT generally outperformed the rest of the models regarding recall, followed by ADA, Gensim, FastText, and TFIDF. Our study provides insights into the effectiveness of different embedding methods for retrieving similar bug reports and highlights the impact of selecting the appropriate one for this task. Our code is available on GitHub.
</details>
<details>
<summary>摘要</summary>
�� Bug reports 是软件开发的重要方面，快速确定和解决 bug 可以确保软件系统的一致性。从现有数据库中检索类似的 bug reports 可以减少解决 bug 所需的时间和努力。在这篇论文中，我们比较了使用 semantic textual similarity 方法来检索类似 bug reports 的效果，基于一个相似性分数。我们探索了多种嵌入模型，包括 TF-IDF（基线）、FastText、Gensim、BERT 和 ADA。我们使用 Software Defects Data 中的 bug reports 来评估这些模型的性能。我们的实验结果表明，BERT 通常在 recall 方面表现最好，其次是 ADA、Gensim、FastText 和 TF-IDF。我们的研究提供了不同嵌入方法在检索类似 bug reports 的效果的视角，并 highlights 选择合适的嵌入方法对此任务的影响。我们的代码可以在 GitHub 上找到。
</details></li>
</ul>
<hr>
<h2 id="Regularizing-Adversarial-Imitation-Learning-Using-Causal-Invariance"><a href="#Regularizing-Adversarial-Imitation-Learning-Using-Causal-Invariance" class="headerlink" title="Regularizing Adversarial Imitation Learning Using Causal Invariance"></a>Regularizing Adversarial Imitation Learning Using Causal Invariance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09189">http://arxiv.org/abs/2308.09189</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ivan Ovinnikov, Joachim M. Buhmann</li>
<li>for: 本研究使用启发学习方法从专家示范数据集中推导策略，以最小化分配度度量的差异。</li>
<li>methods: 使用探测器作为对抗优化程序的指导信号。</li>
<li>results: 发现这种模型容易吸收专家数据中的假 correlate，提出了使用 causal invariance 作为对抗训练模型的规范原则，并在一些高维机器人移动 benchmark 任务中证明其效果。<details>
<summary>Abstract</summary>
Imitation learning methods are used to infer a policy in a Markov decision process from a dataset of expert demonstrations by minimizing a divergence measure between the empirical state occupancy measures of the expert and the policy. The guiding signal to the policy is provided by the discriminator used as part of an versarial optimization procedure. We observe that this model is prone to absorbing spurious correlations present in the expert data. To alleviate this issue, we propose to use causal invariance as a regularization principle for adversarial training of these models. The regularization objective is applicable in a straightforward manner to existing adversarial imitation frameworks. We demonstrate the efficacy of the regularized formulation in an illustrative two-dimensional setting as well as a number of high-dimensional robot locomotion benchmark tasks.
</details>
<details>
<summary>摘要</summary>
“模仿学习方法用于在Markov决策过程中推断策略，从专家示范数据集中 minimize 一个分割度量，以确定专家和策略之间的状态占据度的差异。导航信号提供给策略是通过用作对抗优化过程中的识别器。我们发现这些模型容易吸收专家数据中的假 correlations。为解决这个问题，我们提议使用 causal invariance 作为对抗训练这些模型的正则化原则。这个正则化目标可以直接应用于现有的对抗模仿框架中。我们在一个简单的二维设置中以及一些高维机器人移动 benchmark 任务中证明了正则化形式的效果。”Note that Simplified Chinese is a written language used in mainland China, and it is different from Traditional Chinese, which is used in Taiwan and other parts of the world.
</details></li>
</ul>
<hr>
<h2 id="Distributed-Extra-gradient-with-Optimal-Complexity-and-Communication-Guarantees"><a href="#Distributed-Extra-gradient-with-Optimal-Complexity-and-Communication-Guarantees" class="headerlink" title="Distributed Extra-gradient with Optimal Complexity and Communication Guarantees"></a>Distributed Extra-gradient with Optimal Complexity and Communication Guarantees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09187">http://arxiv.org/abs/2308.09187</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lions-epfl/qgenx">https://github.com/lions-epfl/qgenx</a></li>
<li>paper_authors: Ali Ramezani-Kebrya, Kimon Antonakopoulos, Igor Krawczuk, Justin Deschenaux, Volkan Cevher</li>
<li>for:  solve monotone variational inequality (VI) problems in multi-GPU settings</li>
<li>methods:  propose a quantized generalized extra-gradient (Q-GenX) algorithm, which is an unbiased and adaptive compression method tailored to solve VIs, and an adaptive step-size rule that adapts to the respective noise profiles at hand</li>
<li>results:  achieve a fast convergence rate of ${\mathcal O}(1&#x2F;T)$ under relative noise and an order-optimal convergence rate of ${\mathcal O}(1&#x2F;\sqrt{T})$ under absolute noise, and validate the theoretical results through real-world experiments and training generative adversarial networks on multiple GPUs.Here’s the format you requested:</li>
<li>for: &lt; solve monotone variational inequality (VI) problems in multi-GPU settings&gt;</li>
<li>methods: &lt; propose a quantized generalized extra-gradient (Q-GenX) algorithm, which is an unbiased and adaptive compression method tailored to solve VIs, and an adaptive step-size rule that adapts to the respective noise profiles at hand&gt;</li>
<li>results: &lt; achieve a fast convergence rate of ${\mathcal O}(1&#x2F;T)$ under relative noise and an order-optimal convergence rate of ${\mathcal O}(1&#x2F;\sqrt{T})$ under absolute noise, and validate the theoretical results through real-world experiments and training generative adversarial networks on multiple GPUs.&gt;<details>
<summary>Abstract</summary>
We consider monotone variational inequality (VI) problems in multi-GPU settings where multiple processors/workers/clients have access to local stochastic dual vectors. This setting includes a broad range of important problems from distributed convex minimization to min-max and games. Extra-gradient, which is a de facto algorithm for monotone VI problems, has not been designed to be communication-efficient. To this end, we propose a quantized generalized extra-gradient (Q-GenX), which is an unbiased and adaptive compression method tailored to solve VIs. We provide an adaptive step-size rule, which adapts to the respective noise profiles at hand and achieve a fast rate of ${\mathcal O}(1/T)$ under relative noise, and an order-optimal ${\mathcal O}(1/\sqrt{T})$ under absolute noise and show distributed training accelerates convergence. Finally, we validate our theoretical results by providing real-world experiments and training generative adversarial networks on multiple GPUs.
</details>
<details>
<summary>摘要</summary>
我们考虑了多个GPU上的单调几何维问题（VI），在这些环境中，多个处理器/员工/客户端具有本地随机对应矩阵。这些环境包括分布式凸降至最小值和游戏等问题。标准的extra-gradient算法，它是单调几何VI问题的实际算法，但它并没有考虑通信效率。因此，我们提出了弹性化的弹性矩阵（Q-GenX），它是适应性的弹性压缩方法，适用于解决VI问题。我们还提出了适应步长规则，可以适应具体的噪声 Profiling ，并在相对噪声下 achieves a fast rate of $O(1/T)$，并在绝对噪声下 achieves an order-optimal $O(1/\sqrt{T})$。最后，我们验证了我们的理论结果，通过实际实验，在多个GPU上训练生成器条件网络。
</details></li>
</ul>
<hr>
<h2 id="RatGPT-Turning-online-LLMs-into-Proxies-for-Malware-Attacks"><a href="#RatGPT-Turning-online-LLMs-into-Proxies-for-Malware-Attacks" class="headerlink" title="RatGPT: Turning online LLMs into Proxies for Malware Attacks"></a>RatGPT: Turning online LLMs into Proxies for Malware Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09183">http://arxiv.org/abs/2308.09183</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mika Beckerich, Laura Plein, Sergio Coronado</li>
<li>for: This paper is written to highlight the cybersecurity issues that arise from the use of openly available plugins and Large Language Models (LLMs) in software engineering.</li>
<li>methods: The paper uses an LLM, specifically ChatGPT, as a proxy between the attacker and the victim to disseminate malicious software and evade detection. The authors also establish communication with a command and control (C2) server to receive commands to interact with the victim’s system.</li>
<li>results: The paper presents a proof-of-concept that demonstrates the use of LLMs for malicious purposes, such as delivering malware and establishing communication with a C2 server. The authors highlight the significant cybersecurity issues that arise from the use of openly available plugins and LLMs, and emphasize the need for the development of security guidelines, controls, and mitigation strategies.Here is the information in Simplified Chinese text:</li>
<li>for: 这篇论文是为了探讨开源插件和大语言模型（LLM）在软件工程中的安全问题。</li>
<li>methods: 论文使用ChatGPT作为攻击者和受害者之间的代理，通过它来传播恶意软件并避免检测。作者还建立了命令和控制（C2）服务器与受害者系统进行交互的通信。</li>
<li>results: 论文提供了一个证明，证明了开源插件和LLM可以用于恶意目的，如传播恶意软件和建立C2服务器与受害者系统进行交互。作者指出了开源插件和LLM的使用导致的重要安全问题，并强调了需要开发安全指南、控制和缓解策略。<details>
<summary>Abstract</summary>
The evolution of Generative AI and the capabilities of the newly released Large Language Models (LLMs) open new opportunities in software engineering. However, they also lead to new challenges in cybersecurity. Recently, researchers have shown the possibilities of using LLMs such as ChatGPT to generate malicious content that can directly be exploited or guide inexperienced hackers to weaponize tools and code. Those studies covered scenarios that still require the attacker in the middle of the loop. In this study, we leverage openly available plugins and use an LLM as proxy between the attacker and the victim. We deliver a proof-of-concept where ChatGPT is used for the dissemination of malicious software while evading detection, alongside establishing the communication to a command and control (C2) server to receive commands to interact with a victim's system. Finally, we present the general approach as well as essential elements in order to stay undetected and make the attack a success. This proof-of-concept highlights significant cybersecurity issues with openly available plugins and LLMs, which require the development of security guidelines, controls, and mitigation strategies.
</details>
<details>
<summary>摘要</summary>
生成AI的演化和新一代大语言模型（LLMs）在软件工程中开创了新的机遇，但也带来了新的cybersecurity挑战。最近，研究人员已经证明了使用ChatGPT等LLM生成恶意内容，直接利用或指导不熟悉黑客 weaponize工具和代码的可能性。这些研究都需要攻击者在循环中。在这项研究中，我们利用公开可用的插件，使用LLM作为袋中间人，将攻击者与受害者之间的交互进行了隐蔽。我们实现了一个证明，使用ChatGPT进行恶意软件的散布，同时避免检测，并与Command和Control（C2）服务器建立了通信，以接收对受害者系统的交互命令。最后，我们提出了通用的方法和关键元素，以确保攻击成功，并且需要开发安全指南、控制和缓减策略。这项证明指出，公开可用的插件和LLMs对cybersecurity pose了重要的问题，需要开发安全措施，以避免攻击。
</details></li>
</ul>
<hr>
<h2 id="ChatGPT-HealthPrompt-Harnessing-the-Power-of-XAI-in-Prompt-Based-Healthcare-Decision-Support-using-ChatGPT"><a href="#ChatGPT-HealthPrompt-Harnessing-the-Power-of-XAI-in-Prompt-Based-Healthcare-Decision-Support-using-ChatGPT" class="headerlink" title="ChatGPT-HealthPrompt. Harnessing the Power of XAI in Prompt-Based Healthcare Decision Support using ChatGPT"></a>ChatGPT-HealthPrompt. Harnessing the Power of XAI in Prompt-Based Healthcare Decision Support using ChatGPT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09731">http://arxiv.org/abs/2308.09731</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fatemeh Nazary, Yashar Deldjoo, Tommaso Di Noia</li>
<li>for: 本研究旨在探讨大语言模型（LLMs）在医疗决策中的应用，特点是使用 OpenAI 的 ChatGPT。我们的方法涉及设计了 Contextual prompts，包括任务描述、特征描述以及医学知识的集成，以实现高质量的 binary classification 任务，即使在数据缺乏的情况下。</li>
<li>methods: 我们的研究利用了医学知识，来设计高效可读取的 ML 模型，并将其集成到提问中。我们视这些 ML 模型为医学专家，从而提取了关键的特征重要性，以帮助决策过程。</li>
<li>results: 我们的研究发现，使用提问工程学strategies可以在不同的数据条件下提高 OpenAI 的 ChatGPT 的性能。我们的方法可以在数据缺乏的情况下实现高质量的 binary classification 任务，并且可以在不同的医学领域中应用。<details>
<summary>Abstract</summary>
This study presents an innovative approach to the application of large language models (LLMs) in clinical decision-making, focusing on OpenAI's ChatGPT. Our approach introduces the use of contextual prompts-strategically designed to include task description, feature description, and crucially, integration of domain knowledge-for high-quality binary classification tasks even in data-scarce scenarios. The novelty of our work lies in the utilization of domain knowledge, obtained from high-performing interpretable ML models, and its seamless incorporation into prompt design. By viewing these ML models as medical experts, we extract key insights on feature importance to aid in decision-making processes. This interplay of domain knowledge and AI holds significant promise in creating a more insightful diagnostic tool.   Additionally, our research explores the dynamics of zero-shot and few-shot prompt learning based on LLMs. By comparing the performance of OpenAI's ChatGPT with traditional supervised ML models in different data conditions, we aim to provide insights into the effectiveness of prompt engineering strategies under varied data availability. In essence, this paper bridges the gap between AI and healthcare, proposing a novel methodology for LLMs application in clinical decision support systems. It highlights the transformative potential of effective prompt design, domain knowledge integration, and flexible learning approaches in enhancing automated decision-making.
</details>
<details>
<summary>摘要</summary>
Additionally, our research explores the dynamics of zero-shot and few-shot prompt learning based on LLMs. By comparing the performance of OpenAI's ChatGPT with traditional supervised ML models in different data conditions, we aim to provide insights into the effectiveness of prompt engineering strategies under varied data availability. In essence, this paper bridges the gap between AI and healthcare, proposing a novel methodology for LLMs application in clinical decision support systems. It highlights the transformative potential of effective prompt design, domain knowledge integration, and flexible learning approaches in enhancing automated decision-making.
</details></li>
</ul>
<hr>
<h2 id="Diversifying-AI-Towards-Creative-Chess-with-AlphaZero"><a href="#Diversifying-AI-Towards-Creative-Chess-with-AlphaZero" class="headerlink" title="Diversifying AI: Towards Creative Chess with AlphaZero"></a>Diversifying AI: Towards Creative Chess with AlphaZero</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09175">http://arxiv.org/abs/2308.09175</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tom Zahavy, Vivek Veeriah, Shaobo Hou, Kevin Waugh, Matthew Lai, Edouard Leurent, Nenad Tomasev, Lisa Schut, Demis Hassabis, Satinder Singh</li>
<li>for: 本研究探讨了人工智能（AI）系统是否可以通过创新决策机制来增强其计算合理性。</li>
<li>methods: 本研究使用了AlphaZero（AZ）游戏为基础，并通过嵌入条件的建模来实现多个代理之间的交互。通过行为多样性技术来让AI系统生成更多的想法，然后选择最有前途的想法。</li>
<li>results: 实验结果表明，使用多个AI系统组成的团队可以在棋盘游戏中解决更多的问题，并且能够在不同的开局中选择最佳的玩家。与AlphaZero相比，多个AI系统团队可以解决更多的复杂问题，包括困难的盘龙位。<details>
<summary>Abstract</summary>
In recent years, Artificial Intelligence (AI) systems have surpassed human intelligence in a variety of computational tasks. However, AI systems, like humans, make mistakes, have blind spots, hallucinate, and struggle to generalize to new situations. This work explores whether AI can benefit from creative decision-making mechanisms when pushed to the limits of its computational rationality. In particular, we investigate whether a team of diverse AI systems can outperform a single AI in challenging tasks by generating more ideas as a group and then selecting the best ones. We study this question in the game of chess, the so-called drosophila of AI. We build on AlphaZero (AZ) and extend it to represent a league of agents via a latent-conditioned architecture, which we call AZ_db. We train AZ_db to generate a wider range of ideas using behavioral diversity techniques and select the most promising ones with sub-additive planning. Our experiments suggest that AZ_db plays chess in diverse ways, solves more puzzles as a group and outperforms a more homogeneous team. Notably, AZ_db solves twice as many challenging puzzles as AZ, including the challenging Penrose positions. When playing chess from different openings, we notice that players in AZ_db specialize in different openings, and that selecting a player for each opening using sub-additive planning results in a 50 Elo improvement over AZ. Our findings suggest that diversity bonuses emerge in teams of AI agents, just as they do in teams of humans and that diversity is a valuable asset in solving computationally hard problems.
</details>
<details>
<summary>摘要</summary>
We build on the AlphaZero (AZ) system and extend it to represent a league of agents via a latent-conditioned architecture, which we call AZ_db. We train AZ_db to generate a wider range of ideas using behavioral diversity techniques and select the most promising ones with sub-additive planning. Our experiments show that AZ_db plays chess in diverse ways, solves more puzzles as a group, and outperforms a more homogeneous team. Notably, AZ_db solves twice as many challenging puzzles as AZ, including the challenging Penrose positions.When playing chess from different openings, we notice that players in AZ_db specialize in different openings, and that selecting a player for each opening using sub-additive planning results in a 50 Elo improvement over AZ. Our findings suggest that diversity bonuses emerge in teams of AI agents, just as they do in teams of humans, and that diversity is a valuable asset in solving computationally hard problems.
</details></li>
</ul>
<hr>
<h2 id="Forensic-Data-Analytics-for-Anomaly-Detection-in-Evolving-Networks"><a href="#Forensic-Data-Analytics-for-Anomaly-Detection-in-Evolving-Networks" class="headerlink" title="Forensic Data Analytics for Anomaly Detection in Evolving Networks"></a>Forensic Data Analytics for Anomaly Detection in Evolving Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09171">http://arxiv.org/abs/2308.09171</a></li>
<li>repo_url: None</li>
<li>paper_authors: Li Yang, Abdallah Moubayed, Abdallah Shami, Amine Boukhtouta, Parisa Heidari, Stere Preda, Richard Brunner, Daniel Migault, Adel Larabi</li>
<li>for: 这篇论文旨在提出一种数字审计框架，用于检测网络异常行为，以帮助检察官追究犯罪分子并预防未来的犯罪活动。</li>
<li>methods: 该框架基于多个视角的特征工程，包括无监管异常检测和全面结果修正过程。</li>
<li>results: 实验结果表明，提出的数字审计解决方案具有效果，可以帮助检察官更好地检察和处理网络犯罪。<details>
<summary>Abstract</summary>
In the prevailing convergence of traditional infrastructure-based deployment (i.e., Telco and industry operational networks) towards evolving deployments enabled by 5G and virtualization, there is a keen interest in elaborating effective security controls to protect these deployments in-depth. By considering key enabling technologies like 5G and virtualization, evolving networks are democratized, facilitating the establishment of point presences integrating different business models ranging from media, dynamic web content, gaming, and a plethora of IoT use cases. Despite the increasing services provided by evolving networks, many cybercrimes and attacks have been launched in evolving networks to perform malicious activities. Due to the limitations of traditional security artifacts (e.g., firewalls and intrusion detection systems), the research on digital forensic data analytics has attracted more attention. Digital forensic analytics enables people to derive detailed information and comprehensive conclusions from different perspectives of cybercrimes to assist in convicting criminals and preventing future crimes. This chapter presents a digital analytics framework for network anomaly detection, including multi-perspective feature engineering, unsupervised anomaly detection, and comprehensive result correction procedures. Experiments on real-world evolving network data show the effectiveness of the proposed forensic data analytics solution.
</details>
<details>
<summary>摘要</summary>
在传统基础设施部署（如电信和产业运营网络）向5G和虚拟化技术的演进部署转型时，有强烈的兴趣在深入保护这些部署。通过考虑关键启用技术如5G和虚拟化，演进网络得到了民主化，使得不同业务模式的点存在集成，从媒体、动态网页内容、游戏和大量物联网应用场景。由于传统安全文件（如防火墙和入侵检测系统）的局限性，研究数字审查数据分析技术在获得更多的注意力。数字审查数据分析技术可以帮助人们从不同角度获得细致信息和全面结论，以协助指控犯罪分子并预防未来的犯罪。本章介绍了一种网络异常检测数字分析框架，包括多元视角特征工程、无监督异常检测和全面结果更正过程。在实际演进网络数据上进行实验，提出的数字审查数据分析解决方案得到了证明。
</details></li>
</ul>
<hr>
<h2 id="Online-Transition-Based-Feature-Generation-for-Anomaly-Detection-in-Concurrent-Data-Streams"><a href="#Online-Transition-Based-Feature-Generation-for-Anomaly-Detection-in-Concurrent-Data-Streams" class="headerlink" title="Online Transition-Based Feature Generation for Anomaly Detection in Concurrent Data Streams"></a>Online Transition-Based Feature Generation for Anomaly Detection in Concurrent Data Streams</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10893">http://arxiv.org/abs/2308.10893</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yinzheng Zhong, Alexei Lisitsa</li>
<li>for: 这篇论文旨在提出一种基于转移的特征生成技术（TFGen），用于处理通信网络数据、系统调用数据和监测摄像头数据等不同类型的活动数据。</li>
<li>methods: TFGen使用转移学习来生成数据，可以在线进行处理，并且可以使用历史数据进行编码。</li>
<li>results: TFGen可以解决域独特性、全球过程结构的发现、时间序列数据的编码和在线处理等问题，并且具有高计算效率。<details>
<summary>Abstract</summary>
In this paper, we introduce the transition-based feature generator (TFGen) technique, which reads general activity data with attributes and generates step-by-step generated data. The activity data may consist of network activity from packets, system calls from processes or classified activity from surveillance cameras. TFGen processes data online and will generate data with encoded historical data for each incoming activity with high computational efficiency. The input activities may concurrently originate from distinct traces or channels. The technique aims to address issues such as domain-independent applicability, the ability to discover global process structures, the encoding of time-series data, and online processing capability.
</details>
<details>
<summary>摘要</summary>
在本文中，我们介绍了一种基于转移的特征生成技术（TFGen），该技术可以读取一般活动数据并生成步骤生成的数据。活动数据可以包括网络活动封包、进程系呼び出 shout ouputs或视频监测器中的分类活动。TFGen在线上处理数据，可以对每个入参活动进行编码历史数据的生成，并具有高计算效率。输入活动可以同时来自不同的轨迹或通道。该技术的目标是解决域独立可用性、找到全局过程结构、编码时间序列数据和在线处理能力等问题。
</details></li>
</ul>
<hr>
<h2 id="FedPerfix-Towards-Partial-Model-Personalization-of-Vision-Transformers-in-Federated-Learning"><a href="#FedPerfix-Towards-Partial-Model-Personalization-of-Vision-Transformers-in-Federated-Learning" class="headerlink" title="FedPerfix: Towards Partial Model Personalization of Vision Transformers in Federated Learning"></a>FedPerfix: Towards Partial Model Personalization of Vision Transformers in Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09160">http://arxiv.org/abs/2308.09160</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/imguangyu/fedperfix">https://github.com/imguangyu/fedperfix</a></li>
<li>paper_authors: Guangyu Sun, Matias Mendieta, Jun Luo, Shandong Wu, Chen Chen</li>
<li>for: 本研究旨在提高分布式学习中的模型个性化，并 investigate ViT 模型中哪些部分可以进行部分个性化。</li>
<li>methods: 本研究使用了实验评估不同层的敏感性，并基于这些层的敏感性提出了一种名为 FedPerfix 的个性化方法，通过插件将模型的信息传递到本地客户端进行个性化。</li>
<li>results: 在 CIFAR-100、OrganAMNIST 和 Office-Home 等 datasets 上进行了实验评估，并与多种先进的 PFL 方法进行了比较，结果表明 FedPerfix 可以提高模型的性能。<details>
<summary>Abstract</summary>
Personalized Federated Learning (PFL) represents a promising solution for decentralized learning in heterogeneous data environments. Partial model personalization has been proposed to improve the efficiency of PFL by selectively updating local model parameters instead of aggregating all of them. However, previous work on partial model personalization has mainly focused on Convolutional Neural Networks (CNNs), leaving a gap in understanding how it can be applied to other popular models such as Vision Transformers (ViTs). In this work, we investigate where and how to partially personalize a ViT model. Specifically, we empirically evaluate the sensitivity to data distribution of each type of layer. Based on the insights that the self-attention layer and the classification head are the most sensitive parts of a ViT, we propose a novel approach called FedPerfix, which leverages plugins to transfer information from the aggregated model to the local client as a personalization. Finally, we evaluate the proposed approach on CIFAR-100, OrganAMNIST, and Office-Home datasets and demonstrate its effectiveness in improving the model's performance compared to several advanced PFL methods.
</details>
<details>
<summary>摘要</summary>
In this work, we investigate where and how to partially personalize a ViT model. Specifically, we empirically evaluate the sensitivity of each type of layer to data distribution. Based on the insights that the self-attention layer and the classification head are the most sensitive parts of a ViT, we propose a novel approach called FedPerfix, which leverages plugins to transfer information from the aggregated model to the local client as a personalization.We evaluate the proposed approach on CIFAR-100, OrganAMNIST, and Office-Home datasets and demonstrate its effectiveness in improving the model's performance compared to several advanced PFL methods.
</details></li>
</ul>
<hr>
<h2 id="Data-diversity-and-virtual-imaging-in-AI-based-diagnosis-A-case-study-based-on-COVID-19"><a href="#Data-diversity-and-virtual-imaging-in-AI-based-diagnosis-A-case-study-based-on-COVID-19" class="headerlink" title="Data diversity and virtual imaging in AI-based diagnosis: A case study based on COVID-19"></a>Data diversity and virtual imaging in AI-based diagnosis: A case study based on COVID-19</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09730">http://arxiv.org/abs/2308.09730</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fakrul Islam Tushar, Lavsen Dahal, Saman Sotoudeh-Paima, Ehsan Abadi, W. Paul Segars, Ehsan Samei, Joseph Y. Lo<br>for: 这个研究旨在发展和评估基于深度学习的人工智能（AI）模型，用于诊断新型冠状病毒（COVID-19）。methods: 该研究使用了多种临床和虚拟生成的医疗图像，开发和评估了AI模型。同时，研究还进行了虚拟成像试验，以评估AI性能是如何受到患者和物理因素的影响。results: 研究发现，AI性能受到数据集特点（包括数量、多样性和患者程度）的强烈影响，导致在不同数据集上的性能下降至20%。虚拟CT和CXR图像的模型性能与临床数据集的结果相似。诊断性能与疾病程度有显著关系，CT结果比CXR结果更佳。<details>
<summary>Abstract</summary>
Many studies have investigated deep-learning-based artificial intelligence (AI) models for medical imaging diagnosis of the novel coronavirus (COVID-19), with many reports of near-perfect performance. However, variability in performance and underlying data biases raise concerns about clinical generalizability. This retrospective study involved the development and evaluation of artificial intelligence (AI) models for COVID-19 diagnosis using both diverse clinical and virtually generated medical images. In addition, we conducted a virtual imaging trial to assess how AI performance is affected by several patient- and physics-based factors, including the extent of disease, radiation dose, and imaging modality of computed tomography (CT) and chest radiography (CXR). AI performance was strongly influenced by dataset characteristics including quantity, diversity, and prevalence, leading to poor generalization with up to 20% drop in receiver operating characteristic area under the curve. Model performance on virtual CT and CXR images was comparable to overall results on clinical data. Imaging dose proved to have negligible influence on the results, but the extent of the disease had a marked affect. CT results were consistently superior to those from CXR. Overall, the study highlighted the significant impact of dataset characteristics and disease extent on COVID assessment, and the relevance and potential role of virtual imaging trial techniques on developing effective evaluation of AI algorithms and facilitating translation into diagnostic practice.
</details>
<details>
<summary>摘要</summary>
多些研究已经研究了深度学习基于人工智能（AI）模型，用于医疗影像诊断新型冠状病毒（COVID-19），有很多报告表明近乎完美的性能。然而，表现的变化和下面数据偏好引起了临床一致性的问题。这个逆向研究涉及了COVID-19诊断使用多种临床和虚拟生成的医疗影像开发和评估人工智能（AI）模型。此外，我们还进行了虚拟成像试验，以评估人工智能表现如何受到患者和物理因素的影响，包括疾病程度、辐射剂量和成像方式。我们发现，AI表现受数据集特点的影响，包括数量、多样性和普遍性，导致近20%的接收操作特征曲线下降。模型在虚拟CT和CXR图像上的表现与总体数据上的表现相当。成像剂量对结果没有影响，但疾病程度有明显的影响。CT结果比CXR结果更加稳定。总之，这个研究指出了COVID诊断中数据集特点和疾病程度对人工智能评估的重要影响，以及虚拟成像试验技术在开发有效的AI算法和实现诊断实践中的重要作用。
</details></li>
</ul>
<hr>
<h2 id="ZhiJian-A-Unifying-and-Rapidly-Deployable-Toolbox-for-Pre-trained-Model-Reuse"><a href="#ZhiJian-A-Unifying-and-Rapidly-Deployable-Toolbox-for-Pre-trained-Model-Reuse" class="headerlink" title="ZhiJian: A Unifying and Rapidly Deployable Toolbox for Pre-trained Model Reuse"></a>ZhiJian: A Unifying and Rapidly Deployable Toolbox for Pre-trained Model Reuse</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09158">http://arxiv.org/abs/2308.09158</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhangyikaii/lamda-zhijian">https://github.com/zhangyikaii/lamda-zhijian</a></li>
<li>paper_authors: Yi-Kai Zhang, Lu Ren, Chao Yi, Qi-Wei Wang, De-Chuan Zhan, Han-Jia Ye</li>
<li>for: 这篇论文是为了探讨模型重复（Model Reuse）的方法和技术，以便在实际应用中快速学习和扩展机器学习。</li>
<li>methods: 这篇论文使用了PyTorch backend，提出了一个统一的模型重复方法，包括目标建构、适应器适配和PTM-based推论。</li>
<li>results: 这篇论文提出了ZhiJian工具箱，一个可朗丰和易用的模型重复工具，并透过实际应用评估了ZhiJian的效果。<details>
<summary>Abstract</summary>
The rapid expansion of foundation pre-trained models and their fine-tuned counterparts has significantly contributed to the advancement of machine learning. Leveraging pre-trained models to extract knowledge and expedite learning in real-world tasks, known as "Model Reuse", has become crucial in various applications. Previous research focuses on reusing models within a certain aspect, including reusing model weights, structures, and hypothesis spaces. This paper introduces ZhiJian, a comprehensive and user-friendly toolbox for model reuse, utilizing the PyTorch backend. ZhiJian presents a novel paradigm that unifies diverse perspectives on model reuse, encompassing target architecture construction with PTM, tuning target model with PTM, and PTM-based inference. This empowers deep learning practitioners to explore downstream tasks and identify the complementary advantages among different methods. ZhiJian is readily accessible at https://github.com/zhangyikaii/lamda-zhijian facilitating seamless utilization of pre-trained models and streamlining the model reuse process for researchers and developers.
</details>
<details>
<summary>摘要</summary>
“快速扩展的基础模型和其精细化版本的出现，对机器学习的进步做出了重要贡献。利用预训练模型来提取知识和加速实际任务中的学习，称为“模型再利用”，在各种应用中变得非常重要。先前的研究主要集中在模型重用方面，包括模型权重、结构和假设空间的重用。本文介绍了一个名为ZhiJian的通用和易用的工具箱，利用PyTorch backend，可以实现模型重用。ZhiJian提出了一种新的思路，整合了多种模型重用的视角，包括目标建筑PTM、调参目标模型PTM和PTM基于的推理。这使得深度学习专家可以更好地探索下游任务，并发现不同方法之间的补做优势。ZhiJian可以很方便地在https://github.com/zhangyikaii/lamda-zhijian上使用，便于研究人员和开发者使用预训练模型，并简化模型重用过程。”
</details></li>
</ul>
<hr>
<h2 id="Accurate-machine-learning-force-fields-via-experimental-and-simulation-data-fusion"><a href="#Accurate-machine-learning-force-fields-via-experimental-and-simulation-data-fusion" class="headerlink" title="Accurate machine learning force fields via experimental and simulation data fusion"></a>Accurate machine learning force fields via experimental and simulation data fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09142">http://arxiv.org/abs/2308.09142</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sebastien Röcken, Julija Zavadlav</li>
<li>For: The paper aims to develop a machine learning (ML) potential for titanium that can accurately predict its properties by leveraging both density functional theory (DFT) calculations and experimentally measured mechanical properties and lattice parameters.* Methods: The authors use a fused data learning strategy that combines DFT calculations and experimental data to train the ML potential. They demonstrate that this approach can concurrently satisfy all target objectives and result in a more accurate molecular model compared to models trained with a single data source.* Results: The paper shows that the fused data learning strategy can correct the inaccuracies of DFT functionals at target experimental properties while leaving off-target properties largely unperturbed. The authors also demonstrate the applicability of their approach to any material, making it a general strategy for obtaining highly accurate ML potentials.Here are the three points in Simplified Chinese text:* For: 这篇论文目标是开发一个基于机器学习（ML）的钛力场模型，可以准确预测钛的性质。* Methods: 作者们使用了一种融合数据学习策略，将DFT计算和实验测量的机械性质和晶体参数组合来训练ML potential。他们示出了这种方法可以同时满足所有目标对象，并且比使用单一数据源训练的模型更加准确。* Results: 论文显示，融合数据学习策略可以正确地修正DFT函数的偏差，并且保持偏离目标对象的性质不受影响。作者们还证明了这种方法的通用性，可以应用于任何材料。<details>
<summary>Abstract</summary>
Machine Learning (ML)-based force fields are attracting ever-increasing interest due to their capacity to span spatiotemporal scales of classical interatomic potentials at quantum-level accuracy. They can be trained based on high-fidelity simulations or experiments, the former being the common case. However, both approaches are impaired by scarce and erroneous data resulting in models that either do not agree with well-known experimental observations or are under-constrained and only reproduce some properties. Here we leverage both Density Functional Theory (DFT) calculations and experimentally measured mechanical properties and lattice parameters to train an ML potential of titanium. We demonstrate that the fused data learning strategy can concurrently satisfy all target objectives, thus resulting in a molecular model of higher accuracy compared to the models trained with a single data source. The inaccuracies of DFT functionals at target experimental properties were corrected, while the investigated off-target properties remained largely unperturbed. Our approach is applicable to any material and can serve as a general strategy to obtain highly accurate ML potentials.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="RTB-Formulation-Using-Point-Process"><a href="#RTB-Formulation-Using-Point-Process" class="headerlink" title="RTB Formulation Using Point Process"></a>RTB Formulation Using Point Process</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09122">http://arxiv.org/abs/2308.09122</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seong Jin Lee, Bumsik Kim</li>
<li>for: 这个论文是为了模型 repeated auctions 的RTB生态系统中的随机过程。</li>
<li>methods: 论文使用点过程模型来描述这些 auctions，并提出了一种通用的框架来描述不同的拍卖enario。</li>
<li>results: 论文提出了一些理论结果，包括可以将这种过程简化为Poisson点过程，并且指出了玩家的优化策略在不同情况下的表现。<details>
<summary>Abstract</summary>
We propose a general stochastic framework for modelling repeated auctions in the Real Time Bidding (RTB) ecosystem using point processes. The flexibility of the framework allows a variety of auction scenarios including configuration of information provided to player, determination of auction winner and quantification of utility gained from each auctions. We propose theoretical results on how this formulation of process can be approximated to a Poisson point process, which enables the analyzer to take advantage of well-established properties. Under this framework, we specify the player's optimal strategy under various scenarios. We also emphasize that it is critical to consider the joint distribution of utility and market condition instead of estimating the marginal distributions independently.
</details>
<details>
<summary>摘要</summary>
我们提出了一个通用的随机框架，用于模拟重复拍卖在实时拍卖（RTB）生态系统中的进行，使用点程序。这个框架的灵活性允许多种拍卖enario，包括参与者资讯的配置、拍卖胜出者的决定以及每次拍卖中获得的 utility 的量化。我们提出了理论结果，认为这种过程可以近似到一个 Poisson 点程序，这使得分析者可以利用已有的性质。在这个框架下，我们针对不同的情况下定义了玩家的最佳策略。我们还强调了在联合分布中考虑market condition和 utility 的 JOINT 分布，而不是独立地估算两个分布。
</details></li>
</ul>
<hr>
<h2 id="Multi-fidelity-Fourier-Neural-Operator-for-Fast-Modeling-of-Large-Scale-Geological-Carbon-Storage"><a href="#Multi-fidelity-Fourier-Neural-Operator-for-Fast-Modeling-of-Large-Scale-Geological-Carbon-Storage" class="headerlink" title="Multi-fidelity Fourier Neural Operator for Fast Modeling of Large-Scale Geological Carbon Storage"></a>Multi-fidelity Fourier Neural Operator for Fast Modeling of Large-Scale Geological Carbon Storage</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09113">http://arxiv.org/abs/2308.09113</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hewei Tang, Qingkai Kong, Joseph P. Morris</li>
<li>for: 用于加速预测气体储存床中气压和CO2迁移的数学模型。</li>
<li>methods: 使用深度学习基于模拟器的多 fideltity FNO 模型。</li>
<li>results: 可以使用更可Affordable多 fideltity训练集来解决大规模GCS问题，并且可以预测气压场 WITH reasonable accuracy，即使高精度数据很少。<details>
<summary>Abstract</summary>
Deep learning-based surrogate models have been widely applied in geological carbon storage (GCS) problems to accelerate the prediction of reservoir pressure and CO2 plume migration. Large amounts of data from physics-based numerical simulators are required to train a model to accurately predict the complex physical behaviors associated with this process. In practice, the available training data are always limited in large-scale 3D problems due to the high computational cost. Therefore, we propose to use a multi-fidelity Fourier Neural Operator to solve large-scale GCS problems with more affordable multi-fidelity training datasets. The Fourier Neural Operator has a desirable grid-invariant property, which simplifies the transfer learning procedure between datasets with different discretization. We first test the model efficacy on a GCS reservoir model being discretized into 110k grid cells. The multi-fidelity model can predict with accuracy comparable to a high-fidelity model trained with the same amount of high-fidelity data with 81% less data generation costs. We further test the generalizability of the multi-fidelity model on a same reservoir model with a finer discretization of 1 million grid cells. This case was made more challenging by employing high-fidelity and low-fidelity datasets generated by different geostatistical models and reservoir simulators. We observe that the multi-fidelity FNO model can predict pressure fields with reasonable accuracy even when the high-fidelity data are extremely limited.
</details>
<details>
<summary>摘要</summary>
深度学习基于的代理模型在地球科学中广泛应用于加速气体储存（GCS）问题的预测，特别是气体泵迁和储存层压力的预测。但在实践中，大规模3D问题中的数据几乎总是受限，因为计算成本高。因此，我们提议使用多质量Fourier神经算法来解决大规模GCS问题，只需要较低的多质量训练数据。Fourier神经算法具有恰当的网格不变性，这使得在不同网格的数据之间进行传学过程变得更加简单。我们首先测试模型在110k网格细分的GCS储存模型上的效果。我们发现，使用多质量FNO模型可以与高精度模型在同样的数据量下达到相同的准确率，但是需要81% fewer数据生成成本。我们进一步测试了模型的一致性，在同一个储存模型上使用不同的地球统计学模型和储存 simulator生成的高精度和低精度数据来进行测试。我们发现，使用多质量FNO模型可以在高精度数据几乎极其有限的情况下预测压力场的reasonable准确率。
</details></li>
</ul>
<hr>
<h2 id="Learning-Lightweight-Object-Detectors-via-Multi-Teacher-Progressive-Distillation"><a href="#Learning-Lightweight-Object-Detectors-via-Multi-Teacher-Progressive-Distillation" class="headerlink" title="Learning Lightweight Object Detectors via Multi-Teacher Progressive Distillation"></a>Learning Lightweight Object Detectors via Multi-Teacher Progressive Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09105">http://arxiv.org/abs/2308.09105</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shengcao Cao, Mengtian Li, James Hays, Deva Ramanan, Yi-Xiong Wang, Liang-Yan Gui</li>
<li>for: 提高资源受限的视觉系统（如边缘计算和视频人工智能）中的视觉模型的准确率和计算量和内存使用率。</li>
<li>methods: 使用知识储存（knowledge distillation）技术来提高轻量级的分类模型的性能，但是在结构输出（如物体检测和实例分割）中，知识储存的应用仍然是一个复杂的任务，因为输出的变化和内部网络模块的复杂性。</li>
<li>results: 我们提出了一种简单 yet 有力的顺序方法，可以从一组教师检测器中逐步传递知识到一个轻量级学生模型，以提高其性能。我们的顺序策略可以轻松地与现有的检测储存机制结合使用，以在不同的设置下 consistently 提高学生模型的性能。我们成功地储存了基于 Transformer 的教师检测器的知识到基于 convolution 的学生模型中，并在 MS COCO 测试集上从 36.5% 提高到 42.0% AP 和从 38.2% 提高到 42.5% AP。<details>
<summary>Abstract</summary>
Resource-constrained perception systems such as edge computing and vision-for-robotics require vision models to be both accurate and lightweight in computation and memory usage. While knowledge distillation is a proven strategy to enhance the performance of lightweight classification models, its application to structured outputs like object detection and instance segmentation remains a complicated task, due to the variability in outputs and complex internal network modules involved in the distillation process. In this paper, we propose a simple yet surprisingly effective sequential approach to knowledge distillation that progressively transfers the knowledge of a set of teacher detectors to a given lightweight student. To distill knowledge from a highly accurate but complex teacher model, we construct a sequence of teachers to help the student gradually adapt. Our progressive strategy can be easily combined with existing detection distillation mechanisms to consistently maximize student performance in various settings. To the best of our knowledge, we are the first to successfully distill knowledge from Transformer-based teacher detectors to convolution-based students, and unprecedentedly boost the performance of ResNet-50 based RetinaNet from 36.5% to 42.0% AP and Mask R-CNN from 38.2% to 42.5% AP on the MS COCO benchmark.
</details>
<details>
<summary>摘要</summary>
资源受限的感知系统，如边计算和视频控制，需要视觉模型具备高准确率和轻量级计算和内存使用。而知识填充是一种证明了提高轻量级分类模型表现的策略，但对结构输出如物体检测和实例分割来说，它的应用仍然是一个复杂的任务，因为输出的变化和内部网络模块的复杂性。在这篇论文中，我们提出了一种简单却有力的顺序方法，可以从多个教师检测器中逐步传授知识到一个轻量级学生。为了从高精度但复杂的教师模型中填充知识，我们构建了一序列的教师，帮助学生逐步适应。我们的顺序策略可以轻松地与现有的检测填充机制结合，以实现在不同设置下学生表现的最大化。而我们所知道的是，我们成功地填充了基于Transformer的教师检测器的知识到基于 convolution 的学生中，并在 MS COCO benchmark 上从 36.5% 提高到 42.0% AP 和 38.2% 提高到 42.5% AP。
</details></li>
</ul>
<hr>
<h2 id="A-comprehensive-study-of-spike-and-slab-shrinkage-priors-for-structurally-sparse-Bayesian-neural-networks"><a href="#A-comprehensive-study-of-spike-and-slab-shrinkage-priors-for-structurally-sparse-Bayesian-neural-networks" class="headerlink" title="A comprehensive study of spike and slab shrinkage priors for structurally sparse Bayesian neural networks"></a>A comprehensive study of spike and slab shrinkage priors for structurally sparse Bayesian neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09104">http://arxiv.org/abs/2308.09104</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sanket Jantre, Shrijita Bhattacharya, Tapabrata Maiti</li>
<li>for: 提高深度学习网络的复杂度和计算效率。</li>
<li>methods: 使用结构化稀疏技术（如节点稀疏）压缩深度神经网络，以降低推理时间、提高数据传输量和降低能耗。</li>
<li>results: 比基eline模型更高的预测精度、模型压缩和推理延迟。<details>
<summary>Abstract</summary>
Network complexity and computational efficiency have become increasingly significant aspects of deep learning. Sparse deep learning addresses these challenges by recovering a sparse representation of the underlying target function by reducing heavily over-parameterized deep neural networks. Specifically, deep neural architectures compressed via structured sparsity (e.g. node sparsity) provide low latency inference, higher data throughput, and reduced energy consumption. In this paper, we explore two well-established shrinkage techniques, Lasso and Horseshoe, for model compression in Bayesian neural networks. To this end, we propose structurally sparse Bayesian neural networks which systematically prune excessive nodes with (i) Spike-and-Slab Group Lasso (SS-GL), and (ii) Spike-and-Slab Group Horseshoe (SS-GHS) priors, and develop computationally tractable variational inference including continuous relaxation of Bernoulli variables. We establish the contraction rates of the variational posterior of our proposed models as a function of the network topology, layer-wise node cardinalities, and bounds on the network weights. We empirically demonstrate the competitive performance of our models compared to the baseline models in prediction accuracy, model compression, and inference latency.
</details>
<details>
<summary>摘要</summary>
网络复杂性和计算效率已成为深度学习中的两个关键方面。 sparse deep learning 技术可以恢复压缩后的目标函数表示，从而提高深度学习模型的计算效率。Specifically, 我们可以通过结构化稀疏（例如节点稀疏）来压缩深度神经网络，从而提高推理速度、数据传输率和能耗降低。在这篇论文中，我们explore了两种已有的减少技术，lasso和匹配板，用于 Bayesian 神经网络中的模型压缩。为此，我们提出了结构化稀疏 Bayesian 神经网络，并采用了 Spike-and-Slab Group Lasso (SS-GL) 和 Spike-and-Slab Group Horseshoe (SS-GHS) 假设，并开发了可计算性的拟合推理，包括对 Bernoulli 变量进行连续化 relaxation。我们证明了我们提出的模型的变量 posterior 的减少率与网络结构、层次节点数量以及网络参数的 bounds 有关。我们通过实验表明了我们的模型与基eline 模型相比在预测精度、模型压缩和推理速度方面具有竞争力。
</details></li>
</ul>
<hr>
<h2 id="MindMap-Knowledge-Graph-Prompting-Sparks-Graph-of-Thoughts-in-Large-Language-Models"><a href="#MindMap-Knowledge-Graph-Prompting-Sparks-Graph-of-Thoughts-in-Large-Language-Models" class="headerlink" title="MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models"></a>MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09729">http://arxiv.org/abs/2308.09729</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/willing510/MindMap">https://github.com/willing510/MindMap</a></li>
<li>paper_authors: Yilin Wen, Zifeng Wang, Jimeng Sun</li>
<li>for: This paper aims to improve the performance of large language models (LLMs) by incorporating knowledge graphs (KGs) and eliciting the reasoning pathways of LLMs.</li>
<li>methods: The authors propose a prompting pipeline that enables LLMs to comprehend KG inputs and infer with a combined implicit knowledge and retrieved external knowledge. They also investigate eliciting the mind map on which LLMs perform the reasoning and generate answers.</li>
<li>results: The experiments on three question &amp; answering datasets show that MindMap prompting leads to a striking empirical gain, outperforming GPT-4 and other prompting-with-document-retrieval methods. The produced mind map exhibits the reasoning pathways of LLMs grounded on the ontology of knowledge.<details>
<summary>Abstract</summary>
LLMs usually exhibit limitations in their ability to incorporate new knowledge, the generation of hallucinations, and the transparency of their decision-making process. In this paper, we explore how to prompt LLMs with knowledge graphs (KG), working as a remedy to engage LLMs with up-to-date knowledge and elicit the reasoning pathways from LLMs. Specifically, we build a prompting pipeline that endows LLMs with the capability of comprehending KG inputs and inferring with a combined implicit knowledge and the retrieved external knowledge. In addition, we investigate eliciting the mind map on which LLMs perform the reasoning and generate the answers. It is identified that the produced mind map exhibits the reasoning pathways of LLMs grounded on the ontology of knowledge, hence bringing the prospects of probing and gauging LLM inference in production. The experiments on three question & answering datasets also show that MindMap prompting leads to a striking empirical gain. For instance, prompting a GPT-3.5 with MindMap yields an overwhelming performance over GPT-4 consistently. We also demonstrate that with structured facts retrieved from KG, MindMap can outperform a series of prompting-with-document-retrieval methods, benefiting from more accurate, concise, and comprehensive knowledge from KGs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Modeling-Edge-Features-with-Deep-Bayesian-Graph-Networks"><a href="#Modeling-Edge-Features-with-Deep-Bayesian-Graph-Networks" class="headerlink" title="Modeling Edge Features with Deep Bayesian Graph Networks"></a>Modeling Edge Features with Deep Bayesian Graph Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09087">http://arxiv.org/abs/2308.09087</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/diningphil/e-cgmm">https://github.com/diningphil/e-cgmm</a></li>
<li>paper_authors: Daniele Atzeni, Federico Errica, Davide Bacciu, Alessio Micheli</li>
<li>for: 本研究扩展了Contextual Graph Markov Model，一种深度和概率机器学习模型，以模型边Feature的分布。</li>
<li>methods: 我们引入了一个额外的权重网络，将边特征映射到精确的状态中，以便由原始模型使用。</li>
<li>results: 我们在标准图分类 benchmark 上提高性能，并在图 regression 任务中，利用学习的边表示得到了显著的性能提升。<details>
<summary>Abstract</summary>
We propose an extension of the Contextual Graph Markov Model, a deep and probabilistic machine learning model for graphs, to model the distribution of edge features. Our approach is architectural, as we introduce an additional Bayesian network mapping edge features into discrete states to be used by the original model. In doing so, we are also able to build richer graph representations even in the absence of edge features, which is confirmed by the performance improvements on standard graph classification benchmarks. Moreover, we successfully test our proposal in a graph regression scenario where edge features are of fundamental importance, and we show that the learned edge representation provides substantial performance improvements against the original model on three link prediction tasks. By keeping the computational complexity linear in the number of edges, the proposed model is amenable to large-scale graph processing.
</details>
<details>
<summary>摘要</summary>
我们提出一种对 Contextual Graph Markov Model（CGMM）进行扩展，以模型边Feature的分布。我们的方法是建立一个附加的 Bayesian 网络，将边Feature映射到精确的 discrete 状态，以便由原始模型使用。在这样做时，我们还能够建立更加 ricer 的图表示，即使没有边Feature，这得到了标准图分类 benchmark 上的性能提升。此外，我们成功地测试了我们的提议在图 regression 场景中，并证明了学习到的边表示提供了重要的性能提升，在三个链接预测任务中。由于计算复杂度linear化为边的数量，我们的模型适用于大规模图处理。
</details></li>
</ul>
<hr>
<h2 id="Embracing-assay-heterogeneity-with-neural-processes-for-markedly-improved-bioactivity-predictions"><a href="#Embracing-assay-heterogeneity-with-neural-processes-for-markedly-improved-bioactivity-predictions" class="headerlink" title="Embracing assay heterogeneity with neural processes for markedly improved bioactivity predictions"></a>Embracing assay heterogeneity with neural processes for markedly improved bioactivity predictions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09086">http://arxiv.org/abs/2308.09086</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lucian Chan, Marcel Verdonk, Carl Poelking</li>
<li>for: 预测药物的生物活性是计算辅助药物发现中最Difficult和最重要的挑战之一。尽管年来数据收集和整理努力很大，生物活性数据仍然稀缺和多样，因此妨碍了建立准确、传递和可靠的预测模型。实验室数据的内在多样性被聚合实践加以忽略，导致预测模型的准确性受到限制。</li>
<li>methods: 我们提出了一种层次meta-学习框架，利用不同试验方法之间的信息共同作用，成功地考虑到试验方法的多样性。我们显示，该模型在不同蛋白目标和试验方法类型上的粘合能力有很大提高，相比于传统基elines。它可以快速适应新的目标Context使用非常少的观察，因此可以实现大规模的虚拟屏选在早期药物发现阶段。</li>
<li>results: 我们的模型在不同蛋白目标和试验方法类型上的粘合能力有很大提高，相比于传统基elines。它可以快速适应新的目标Context使用非常少的观察，因此可以实现大规模的虚拟屏选在早期药物发现阶段。<details>
<summary>Abstract</summary>
Predicting the bioactivity of a ligand is one of the hardest and most important challenges in computer-aided drug discovery. Despite years of data collection and curation efforts by research organizations worldwide, bioactivity data remains sparse and heterogeneous, thus hampering efforts to build predictive models that are accurate, transferable and robust. The intrinsic variability of the experimental data is further compounded by data aggregation practices that neglect heterogeneity to overcome sparsity. Here we discuss the limitations of these practices and present a hierarchical meta-learning framework that exploits the information synergy across disparate assays by successfully accounting for assay heterogeneity. We show that the model achieves a drastic improvement in affinity prediction across diverse protein targets and assay types compared to conventional baselines. It can quickly adapt to new target contexts using very few observations, thus enabling large-scale virtual screening in early-phase drug discovery.
</details>
<details>
<summary>摘要</summary>
预测药物的生物活性是计算机辅助药物发现中最为困难和重要的挑战。尽管多年的数据收集和整理努力，生物活性数据仍然稀缺和多样化，因此妨碍了建立准确、传递和可靠的预测模型。实验室中的数据变化性更是由数据聚合实践忽略了多样性，以避免稀缺性。本文描述了这些限制，并提出了层次度meta学框架，利用不同试验中的信息相互作用，成功地考虑到了试验的多样性。我们显示，该模型在多种蛋白目标和试验类型上 achieves  significan improvement in affinity prediction compared to conventional baselines.它可以快速适应新的目标Context使用很少观测，因此实现了大规模的虚拟屏选在早期药物发现阶段。
</details></li>
</ul>
<hr>
<h2 id="MovePose-A-High-performance-Human-Pose-Estimation-Algorithm-on-Mobile-and-Edge-Devices"><a href="#MovePose-A-High-performance-Human-Pose-Estimation-Algorithm-on-Mobile-and-Edge-Devices" class="headerlink" title="MovePose: A High-performance Human Pose Estimation Algorithm on Mobile and Edge Devices"></a>MovePose: A High-performance Human Pose Estimation Algorithm on Mobile and Edge Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09084">http://arxiv.org/abs/2308.09084</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongyang Yu, Haoyue Zhang, Zhirui Zhou, Wangpeng An, Yanhong Yang</li>
<li>for: 这个研究旨在提供高精度、实时性的人体姿势估测算法，以应用于移动设备上的人体姿势识别、健身追踪、手语识别等领域。</li>
<li>methods: 这个算法使用了优化的轻量级卷积神经网络，包括三种技术：复原、大小调整的卷积、和坐标分类方法。这些技术可以增强模型的准确性和识别能力。</li>
<li>results: 这个算法在COCO评估数据集上获得了67.7的 Mean Average Precision（mAP）得分，并在Intel i9-10920x CPU和NVIDIA RTX3090 GPU上显示了高效性，分别为69+帧每秒和452+帧每秒。在Android手机上，它的帧率超过11帧每秒。<details>
<summary>Abstract</summary>
We present MovePose, an optimized lightweight convolutional neural network designed specifically for real-time body pose estimation on CPU-based mobile devices. The current solutions do not provide satisfactory accuracy and speed for human posture estimation, and MovePose addresses this gap. It aims to maintain real-time performance while improving the accuracy of human posture estimation for mobile devices. The network produces 17 keypoints for each individual at a rate exceeding 11 frames per second, making it suitable for real-time applications such as fitness tracking, sign language interpretation, and advanced mobile human posture estimation. Our MovePose algorithm has attained an Mean Average Precision (mAP) score of 67.7 on the COCO \cite{cocodata} validation dataset. The MovePose algorithm displayed efficiency with a performance of 69+ frames per second (fps) when run on an Intel i9-10920x CPU. Additionally, it showcased an increased performance of 452+ fps on an NVIDIA RTX3090 GPU. On an Android phone equipped with a Snapdragon 8 + 4G processor, the fps reached above 11. To enhance accuracy, we incorporated three techniques: deconvolution, large kernel convolution, and coordinate classification methods. Compared to basic upsampling, deconvolution is trainable, improves model capacity, and enhances the receptive field. Large kernel convolution strengthens these properties at a decreased computational cost. In summary, MovePose provides high accuracy and real-time performance, marking it a potential tool for a variety of applications, including those focused on mobile-side human posture estimation. The code and models for this algorithm will be made publicly accessible.
</details>
<details>
<summary>摘要</summary>
我们介绍了 MovePose，一种优化的轻量级卷积神经网络，特意设计用于实时人体姿态估计在CPU基于移动设备上。现有解决方案无法提供满意的精度和速度，MovePose填补了这一空白。它希望在实时应用中维持实时性，同时提高人体姿态估计的精度。该网络每秒钟生成17个关键点，适用于实时应用，如健身监测、手语理解和高级移动人体姿态估计。我们的 MovePose算法在 COCO 验证数据集上获得了67.7的 Mean Average Precision（mAP）分数。MovePose算法在 Intel i9-10920x CPU 上运行时达到了69+帧每秒（fps）的性能，并在 NVIDIA RTX3090 GPU 上达到了452+ fps。在一个装备 Snapdragon 8 + 4G 处理器的 Android 手机上，fps 超过11。为了提高精度，我们采用了三种技术：解 convolution，大kernel convolution和坐标分类方法。与基本的upsampling相比，解 convolution 可以学习，提高模型容量，并扩大感知范围。大kernel convolution 强化这些属性，而且降低计算成本。简单来说，MovePose 提供了高精度和实时性，使其成为许多应用的可能工具，包括移动端人体姿态估计。我们将代码和模型公开访问。
</details></li>
</ul>
<hr>
<h2 id="Over-the-Air-Computation-Aided-Federated-Learning-with-the-Aggregation-of-Normalized-Gradient"><a href="#Over-the-Air-Computation-Aided-Federated-Learning-with-the-Aggregation-of-Normalized-Gradient" class="headerlink" title="Over-the-Air Computation Aided Federated Learning with the Aggregation of Normalized Gradient"></a>Over-the-Air Computation Aided Federated Learning with the Aggregation of Normalized Gradient</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09082">http://arxiv.org/abs/2308.09082</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rongfei Fan, Xuming An, Shiyuan Zuo, Han Hu</li>
<li>for: 这个研究旨在提高联合学习（Federated Learning，FL）的通信效率，特别是在无线通信环境下。</li>
<li>methods: 这个研究使用了调升法来实现遥测式计算（Over-the-air Computation），具体包括：每个移动设备更新、增强本地梯度，然后将其传递给服务器；服务器获取所有移动设备的总梯度，然后发送更新后的模型参数给每个移动设备。</li>
<li>results: 这个研究的主要结果包括：在平滑的损失函数下，我们的提案可以在子线性速度下收敛到稳定点；在平滑且强检测的损失函数下，我们的提案可以在线性速度下 achieves 最佳训练损失，并且发现了调升因子的选择与收敛速度之间的贡献。<details>
<summary>Abstract</summary>
Over-the-air computation is a communication-efficient solution for federated learning (FL). In such a system, iterative procedure is performed: Local gradient of private loss function is updated, amplified and then transmitted by every mobile device; the server receives the aggregated gradient all-at-once, generates and then broadcasts updated model parameters to every mobile device. In terms of amplification factor selection, most related works suppose the local gradient's maximal norm always happens although it actually fluctuates over iterations, which may degrade convergence performance. To circumvent this problem, we propose to turn local gradient to be normalized one before amplifying it. Under our proposed method, when the loss function is smooth, we prove our proposed method can converge to stationary point at sub-linear rate. In case of smooth and strongly convex loss function, we prove our proposed method can achieve minimal training loss at linear rate with any small positive tolerance. Moreover, a tradeoff between convergence rate and the tolerance is discovered. To speedup convergence, problems optimizing system parameters are also formulated for above two cases. Although being non-convex, optimal solution with polynomial complexity of the formulated problems are derived. Experimental results show our proposed method can outperform benchmark methods on convergence performance.
</details>
<details>
<summary>摘要</summary>
随空计算是一种通信效率的解决方案 для联合学习（FL）。在这种系统中，每个移动设备都会进行迭代过程：每个设备都会更新、增强并将本地梯度发送给服务器;服务器会收到所有设备的汇总梯度，并生成并广播到每个设备的更新模型参数。在扩大因子选择方面，大多数相关的工作假设了本地梯度的最大 нор Always happens，这可能会降低收敛性能。为了解决这个问题，我们提议将本地梯度转换成normalized的一个前提下，然后扩大它。我们的提议方法可以在smooth的损失函数下降到站点点，并且在smooth和强 convex的损失函数下可以在线性率下 achieves minimal training loss。此外，我们发现了一个tolerance和收敛率之间的负反关系。为了加速收敛，我们还形ulated了一些优化系统参数的问题，并derived了可靠的解。实验结果表明，我们的提议方法可以在收敛性能方面超越参照方法。
</details></li>
</ul>
<hr>
<h2 id="Conditional-Sampling-of-Variational-Autoencoders-via-Iterated-Approximate-Ancestral-Sampling"><a href="#Conditional-Sampling-of-Variational-Autoencoders-via-Iterated-Approximate-Ancestral-Sampling" class="headerlink" title="Conditional Sampling of Variational Autoencoders via Iterated Approximate Ancestral Sampling"></a>Conditional Sampling of Variational Autoencoders via Iterated Approximate Ancestral Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09078">http://arxiv.org/abs/2308.09078</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vaidotas Simkus, Michael U. Gutmann</li>
<li>for: 这个论文是为了解决Variational Autoencoders（VAEs）的受限 sampling问题，特别是在缺失数据填充等应用中。</li>
<li>methods: 这篇论文使用Metropolis-within-Gibbs（MWG）法进行 asymptotically exact 的 conditional sampling，但是发现VAEs学习的结构化层次空间可能会导致MWG采样器受到目标分布的强制性影响。</li>
<li>results: 这篇论文提出了两种解决这些坑爹的方法，并在一系列采样任务中表明了这些方法的改进表现。<details>
<summary>Abstract</summary>
Conditional sampling of variational autoencoders (VAEs) is needed in various applications, such as missing data imputation, but is computationally intractable. A principled choice for asymptotically exact conditional sampling is Metropolis-within-Gibbs (MWG). However, we observe that the tendency of VAEs to learn a structured latent space, a commonly desired property, can cause the MWG sampler to get "stuck" far from the target distribution. This paper mitigates the limitations of MWG: we systematically outline the pitfalls in the context of VAEs, propose two original methods that address these pitfalls, and demonstrate an improved performance of the proposed methods on a set of sampling tasks.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>conditional sampling of variational autoencoders (VAEs) 是在各种应用中需要的，如数据缺失填充，但是计算复杂。 Metropolis-within-Gibbs (MWG) 是一种原则上正确的循环采样方法，但我们发现 VAEs 学习的结构化隐藏空间，是通常所希望的属性，可能会使 MWG 采样器迷失到目标分布的远方。这篇论文解决了 VAEs 中 MWG 采样器的局限性，我们系统地描述了这些坑害的情况，提出了两种原创的方法来解决这些问题，并在一组采样任务中证明了我们的方法的性能有所改善。
</details></li>
</ul>
<hr>
<h2 id="Fast-Decision-Support-for-Air-Traffic-Management-at-Urban-Air-Mobility-Vertiports-using-Graph-Learning"><a href="#Fast-Decision-Support-for-Air-Traffic-Management-at-Urban-Air-Mobility-Vertiports-using-Graph-Learning" class="headerlink" title="Fast Decision Support for Air Traffic Management at Urban Air Mobility Vertiports using Graph Learning"></a>Fast Decision Support for Air Traffic Management at Urban Air Mobility Vertiports using Graph Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09075">http://arxiv.org/abs/2308.09075</a></li>
<li>repo_url: None</li>
<li>paper_authors: Prajit KrisshnaKumar, Jhoel Witter, Steve Paul, Hanvit Cho, Karthik Dantu, Souma Chowdhury</li>
<li>For: This paper proposes a novel approach to Urban Air Mobility - Vertiport Schedule Management (UAM-VSM) using graph reinforcement learning to generate decision-support policies.* Methods: The proposed approach uses two separate graphs to represent the designated physical spots within the vertiport’s airspace and the vehicles being managed, with feature extraction performed through a graph convolutional network (GCN).* Results: The proposed approach is demonstrated to be superior to basic reinforcement learning (with graph embeddings) or random choice baselines through realistic simulations in AirSim applied to scaled down multi-rotor vehicles, with improved performance in terms of delays, safety (no. of collisions), and battery consumption.Here is the simplified Chinese text:* 用: 这篇论文提出了一种新的城市空中交通管理方法（UAM-VSM），使用图约束学习生成决策支持策略。* 方法: 该方法使用两个图来表示Vertiport中的设定物理位置和运行中的 vehicles，并通过图 convolutional neural network (GCN) 进行特征提取。* 结果: 该方法在使用 AirSim 进行大小化多旋翼机器人的实验中，被证明比基本约束学习（图嵌入）或随机选择基eline superior，并且在延迟、安全（碰撞数）和电池消耗等方面提高性能。<details>
<summary>Abstract</summary>
Urban Air Mobility (UAM) promises a new dimension to decongested, safe, and fast travel in urban and suburban hubs. These UAM aircraft are conceived to operate from small airports called vertiports each comprising multiple take-off/landing and battery-recharging spots. Since they might be situated in dense urban areas and need to handle many aircraft landings and take-offs each hour, managing this schedule in real-time becomes challenging for a traditional air-traffic controller but instead calls for an automated solution. This paper provides a novel approach to this problem of Urban Air Mobility - Vertiport Schedule Management (UAM-VSM), which leverages graph reinforcement learning to generate decision-support policies. Here the designated physical spots within the vertiport's airspace and the vehicles being managed are represented as two separate graphs, with feature extraction performed through a graph convolutional network (GCN). Extracted features are passed onto perceptron layers to decide actions such as continue to hover or cruise, continue idling or take-off, or land on an allocated vertiport spot. Performance is measured based on delays, safety (no. of collisions) and battery consumption. Through realistic simulations in AirSim applied to scaled down multi-rotor vehicles, our results demonstrate the suitability of using graph reinforcement learning to solve the UAM-VSM problem and its superiority to basic reinforcement learning (with graph embeddings) or random choice baselines.
</details>
<details>
<summary>摘要</summary>
城市空中 mobilicity (UAM) 提供了一个新的维度，即减压、安全和快速的城市和郊区旅行。这些UAM飞机被设计为从小机场（vertiport）中起降和续航，每个vertiport都包含多个起降和电池充能的位置。由于它们可能会位于密集的城市区域，管理这些 Vertiport 的时间表是一个传统空中交通控制员无法处理的问题，而是需要一个自动化解决方案。这篇论文提出了一种新的方法，即城市空中 mobilicity - Vertiport 时间表管理（UAM-VSM），该方法利用图约束学习生成决策策略。在这种方法中，Vertiport 中的物理位置和管理的车辆被表示为两个分开的图，通过图卷积网络（GCN）进行特征提取。提取出的特征被传递给感知层，以确定动作，如继续悬停或继续飞行、继续停止或 Vertiport 上占用位置。性能被评估基于延迟、安全（车辆碰撞数）和电池消耗。通过在AirSim上进行了扩展的多旋翼飞机的实际 simulations，我们的结果表明，使用图约束学习解决 UAM-VSM 问题是可行的，并且比基本约束学习（图嵌入）或随机选择基eline更高效。
</details></li>
</ul>
<hr>
<h2 id="Joint-Power-Control-and-Data-Size-Selection-for-Over-the-Air-Computation-Aided-Federated-Learning"><a href="#Joint-Power-Control-and-Data-Size-Selection-for-Over-the-Air-Computation-Aided-Federated-Learning" class="headerlink" title="Joint Power Control and Data Size Selection for Over-the-Air Computation Aided Federated Learning"></a>Joint Power Control and Data Size Selection for Over-the-Air Computation Aided Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09072">http://arxiv.org/abs/2308.09072</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anxuming/fedaircomp">https://github.com/anxuming/fedaircomp</a></li>
<li>paper_authors: Xuming An, Rongfei Fan, Shiyuan Zuo, Han Hu, Hai Jiang, Ning Zhang</li>
<li>for: 这篇论文主要是为了提出一种基于联合学习的机器学习方法，以便处理各个移动设备上产生的大量原始数据。</li>
<li>methods: 这篇论文使用了联合学习的方法，并且将所有移动设备的执行模型参数传递到一个基站 (BS) 中进行联合训练。为了实现这一点，论文提出了一种基于无线电信处理的方法，允许所有移动设备同时传送它们的参数映射讯号到 BS。</li>
<li>results: 根据论文的数据分析，这种方法可以对联合学习领域的训练效果提供大幅提高，并且可以实现更好的训练效果和更高的精度。<details>
<summary>Abstract</summary>
Federated learning (FL) has emerged as an appealing machine learning approach to deal with massive raw data generated at multiple mobile devices, {which needs to aggregate the training model parameter of every mobile device at one base station (BS) iteratively}. For parameter aggregating in FL, over-the-air computation is a spectrum-efficient solution, which allows all mobile devices to transmit their parameter-mapped signals concurrently to a BS. Due to heterogeneous channel fading and noise, there exists difference between the BS's received signal and its desired signal, measured as the mean-squared error (MSE). To minimize the MSE, we propose to jointly optimize the signal amplification factors at the BS and the mobile devices as well as the data size (the number of data samples involved in local training) at every mobile device. The formulated problem is challenging to solve due to its non-convexity. To find the optimal solution, with some simplification on cost function and variable replacement, which still preserves equivalence, we transform the changed problem to be a bi-level problem equivalently. For the lower-level problem, optimal solution is found by enumerating every candidate solution from the Karush-Kuhn-Tucker (KKT) condition. For the upper-level problem, the optimal solution is found by exploring its piecewise convexity. Numerical results show that our proposed method can greatly reduce the MSE and can help to improve the training performance of FL compared with benchmark methods.
</details>
<details>
<summary>摘要</summary>
federated learning（FL）已经成为一种有appeal的机器学习方法，用于处理多个移动设备生成的大量原始数据，{需要在一个基站（BS）上进行参数聚合iteratively}.在FL中，在BS上进行参数聚合的过程中，通过空中计算，所有的移动设备可以同时将参数映射的信号传输给BS。由于不同的通道抑降和噪声，BS所接收到的信号和其所需的信号之间存在差异，这被称为平均方差误差（MSE）。为了最小化MSE，我们提议同时优化BS和移动设备中的信号增强因子以及每个移动设备中的数据大小（当地训练中使用的数据样本数）。这个问题的解决具有非 convex 性，为了找到优化的解决方案，我们通过简化成本函数和变量替换，保持等价性，将问题转换为一个相等的二级问题。在下一级问题中，可以通过枚举所有的候选解来找到最优解。在上一级问题中，通过探索其块维度的凹陷性，找到优化的解决方案。 numerics 结果表明，我们的提议方法可以大幅减少MSE，并且可以提高FL的训练性能，与参考方法相比。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/18/cs.LG_2023_08_18/" data-id="clm0t8e0j0078v788gayibur6" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_08_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/18/cs.SD_2023_08_18/" class="article-date">
  <time datetime="2023-08-17T16:00:00.000Z" itemprop="datePublished">2023-08-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/18/cs.SD_2023_08_18/">cs.SD - 2023-08-18 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Compensating-Removed-Frequency-Components-Thwarting-Voice-Spectrum-Reduction-Attacks"><a href="#Compensating-Removed-Frequency-Components-Thwarting-Voice-Spectrum-Reduction-Attacks" class="headerlink" title="Compensating Removed Frequency Components: Thwarting Voice Spectrum Reduction Attacks"></a>Compensating Removed Frequency Components: Thwarting Voice Spectrum Reduction Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09546">http://arxiv.org/abs/2308.09546</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shu Wang, Kun Sun, Qi Li</li>
<li>for: 本研究旨在强化语音识别系统（ASR）面对恶意音频攻击的安全性。</li>
<li>methods: 本文提出了一种名为ACE的听音补偿系统，利用频率成分相关性和扰动敏感性来对抗频谱减少攻击。</li>
<li>results: 实验结果表明，ACE可以效果地减少ASR推理错误率达87.9%，并对剩下的错误分析了六种常见的ASR推理错误类型和其可能的缓解方案。<details>
<summary>Abstract</summary>
Automatic speech recognition (ASR) provides diverse audio-to-text services for humans to communicate with machines. However, recent research reveals ASR systems are vulnerable to various malicious audio attacks. In particular, by removing the non-essential frequency components, a new spectrum reduction attack can generate adversarial audios that can be perceived by humans but cannot be correctly interpreted by ASR systems. It raises a new challenge for content moderation solutions to detect harmful content in audio and video available on social media platforms. In this paper, we propose an acoustic compensation system named ACE to counter the spectrum reduction attacks over ASR systems. Our system design is based on two observations, namely, frequency component dependencies and perturbation sensitivity. First, since the Discrete Fourier Transform computation inevitably introduces spectral leakage and aliasing effects to the audio frequency spectrum, the frequency components with similar frequencies will have a high correlation. Thus, considering the intrinsic dependencies between neighboring frequency components, it is possible to recover more of the original audio by compensating for the removed components based on the remaining ones. Second, since the removed components in the spectrum reduction attacks can be regarded as an inverse of adversarial noise, the attack success rate will decrease when the adversarial audio is replayed in an over-the-air scenario. Hence, we can model the acoustic propagation process to add over-the-air perturbations into the attacked audio. We implement a prototype of ACE and the experiments show ACE can effectively reduce up to 87.9% of ASR inference errors caused by spectrum reduction attacks. Also, by analyzing residual errors, we summarize six general types of ASR inference errors and investigate the error causes and potential mitigation solutions.
</details>
<details>
<summary>摘要</summary>
自动语音识别（ASR）系统提供了多种媒体到文本服务，帮助人类与机器进行交互。然而，最新的研究发现，ASR系统受到了多种恶意音频攻击。特别是，通过去除非关键频率成分，新的频谱减少攻击可以生成对人类可见但是ASR系统无法正确理解的恶意音频。这引起了社交媒体平台上内容审核解决方案检测危害内容的新挑战。在本文中，我们提议一种名为ACE的听音补偿系统，用于对ASR系统中的频谱减少攻击进行防御。我们的系统设计基于两个观察结论：一是频率成分之间的相互依赖关系，二是对攻击音频进行频率补偿可以降低攻击成功率。我们实现了ACE的原型，实验结果表明，ACE可以降低ASR推断错误率达87.9%。此外，我们分析了剩下的错误 residual errors，并总结了六种通用的ASR推断错误类型，并investigate了这些错误的原因和可能的缓解解决方案。
</details></li>
</ul>
<hr>
<h2 id="Generative-Machine-Listener"><a href="#Generative-Machine-Listener" class="headerlink" title="Generative Machine Listener"></a>Generative Machine Listener</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09493">http://arxiv.org/abs/2308.09493</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guanxin Jiang, Lars Villemoes, Arijit Biswas</li>
<li>for: 这篇论文是用于描述一种基于神经网络的音频测试数据生成方法。</li>
<li>methods: 该方法使用各个侵入式听力测试分数来训练神经网络，并可以预测每对参照和编码音频信号的分数分布。</li>
<li>results: 与基准系统相比，使用回归 Mean 分数而不是 GML 方法，出现了较低的异常比率（OR），并且可以轻松地预测 confidence interval（CI）。此外，通过从图像领域中吸取数据增强技术，可以提高 CI 预测精度以及 Pearson 和 Spearman 排名 correlation 的 Mean 分数。<details>
<summary>Abstract</summary>
We show how a neural network can be trained on individual intrusive listening test scores to predict a distribution of scores for each pair of reference and coded input stereo or binaural signals. We nickname this method the Generative Machine Listener (GML), as it is capable of generating an arbitrary amount of simulated listening test data. Compared to a baseline system using regression over mean scores, we observe lower outlier ratios (OR) for the mean score predictions, and obtain easy access to the prediction of confidence intervals (CI). The introduction of data augmentation techniques from the image domain results in a significant increase in CI prediction accuracy as well as Pearson and Spearman rank correlation of mean scores.
</details>
<details>
<summary>摘要</summary>
我们展示了一个神经网络可以根据个别侵入式聆听测试成绩来预测每对参照和压缩音 signals 的分布。我们称这为生成机器听者（GML），因为它可以生成无限多个模拟聆听测试数据。相比基准系统使用平均分布 regression，我们观察了较低的外围比率（OR），并可以轻松地预测信息 intervals（CI）的预测。对于数据增强技术的引入，我们获得了显著提高 CI 预测准确性以及平均分布和斯宾格数字相互联系的关系。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Sampling-Techniques-for-Generating-Melodies-with-a-Transformer-Language-Model"><a href="#Exploring-Sampling-Techniques-for-Generating-Melodies-with-a-Transformer-Language-Model" class="headerlink" title="Exploring Sampling Techniques for Generating Melodies with a Transformer Language Model"></a>Exploring Sampling Techniques for Generating Melodies with a Transformer Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09454">http://arxiv.org/abs/2308.09454</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mathias Rose Bjare, Stefan Lattner, Gerhard Widmer</li>
<li>for: 这个研究探讨了不同采样策略对于训练autoregressive自然语言处理模型的质量产生的影响。</li>
<li>methods: 作者使用高容量变换器模型训练在高度结构化的爱尔兰传统旋律音乐中，并使用分布 truncation 采样技术进行分析。特别是使用核心采样、“典型采样”和传统祖先采样。</li>
<li>results: 研究发现，在优化的情况下，概率 truncation 技术可能会限制多样性和结构性特征，但在低效情况下，它们可能会生成更多的乐曲。<details>
<summary>Abstract</summary>
Research in natural language processing has demonstrated that the quality of generations from trained autoregressive language models is significantly influenced by the used sampling strategy. In this study, we investigate the impact of different sampling techniques on musical qualities such as diversity and structure. To accomplish this, we train a high-capacity transformer model on a vast collection of highly-structured Irish folk melodies and analyze the musical qualities of the samples generated using distribution truncation sampling techniques. Specifically, we use nucleus sampling, the recently proposed "typical sampling", and conventional ancestral sampling. We evaluate the effect of these sampling strategies in two scenarios: optimal circumstances with a well-calibrated model and suboptimal circumstances where we systematically degrade the model's performance. We assess the generated samples using objective and subjective evaluations. We discover that probability truncation techniques may restrict diversity and structural patterns in optimal circumstances, but may also produce more musical samples in suboptimal circumstances.
</details>
<details>
<summary>摘要</summary>
研究自然语言处理已经证明，训练过的自然语言生成模型的质量受到采样策略的影响。在这个研究中，我们研究了不同采样技术对音乐质量的影响。为此，我们使用高容量变换器模型训练在高度结构化的爱尔兰传统散户歌曲中，并对采样技术的影响进行分析。具体来说，我们使用核心采样、“典型采样”和传统祖先采样。我们在两个场景中评估这些采样策略的影响：优化的情况下，模型性能很好，以及逐步降低模型性能的情况。我们使用对jective和主观评估来评估生成的样本。我们发现，抑制采样技术可能会限制多样性和结构性模式，但在优化情况下可能会生成更多的音乐样本。
</details></li>
</ul>
<hr>
<h2 id="TrOMR-Transformer-Based-Polyphonic-Optical-Music-Recognition"><a href="#TrOMR-Transformer-Based-Polyphonic-Optical-Music-Recognition" class="headerlink" title="TrOMR:Transformer-Based Polyphonic Optical Music Recognition"></a>TrOMR:Transformer-Based Polyphonic Optical Music Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09370">http://arxiv.org/abs/2308.09370</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/netease/polyphonic-tromr">https://github.com/netease/polyphonic-tromr</a></li>
<li>paper_authors: Yixuan Li, Huaping Liu, Qiang Jin, Miaomiao Cai, Peng Li</li>
<li>for: 这篇论文是关于音乐Recognition（OMR）技术的研究，旨在提出一种基于变换器的端到端多重音乐识别方法，以提高识别精度。</li>
<li>methods: 该方法使用变换器来实现全球性的音乐理解，并引入一种新的一致损失函数和合理的数据注释方法来提高识别精度。</li>
<li>results: 广泛的实验表明，TrOMR方法在实际场景下的识别精度明显高于现有的OMR方法，特别是在复杂的音乐乐谱上。此外，authors还开发了TrOMR系统和一个摄像头场景数据集，以便在真实世界中进行全页音乐乐谱识别。<details>
<summary>Abstract</summary>
Optical Music Recognition (OMR) is an important technology in music and has been researched for a long time. Previous approaches for OMR are usually based on CNN for image understanding and RNN for music symbol classification. In this paper, we propose a transformer-based approach with excellent global perceptual capability for end-to-end polyphonic OMR, called TrOMR. We also introduce a novel consistency loss function and a reasonable approach for data annotation to improve recognition accuracy for complex music scores. Extensive experiments demonstrate that TrOMR outperforms current OMR methods, especially in real-world scenarios. We also develop a TrOMR system and build a camera scene dataset for full-page music scores in real-world. The code and datasets will be made available for reproducibility.
</details>
<details>
<summary>摘要</summary>
优化音乐识别（OMR）是音乐技术的一个重要方向，已经在长期的研究中。先前的OMR方法通常基于Convolutional Neural Network（CNN） для图像理解和Recurrent Neural Network（RNN） для乐谱符号分类。在这篇论文中，我们提出了一种基于变换器的方法，具有优秀的全球性识别能力，用于端到端多重音乐识别，称为TrOMR。我们还提出了一种新的一致损失函数和一种合理的数据注释方法，以提高复杂乐谱中的识别精度。广泛的实验表明，TrOMR超过当前OMR方法，特别是在真实世界情况下。我们还开发了TrOMR系统和一个摄像头场景数据集，用于全页乐谱的真实世界摄像头识别。代码和数据将被公开，以便重现。
</details></li>
</ul>
<hr>
<h2 id="Lip-Reading-for-Low-resource-Languages-by-Learning-and-Combining-General-Speech-Knowledge-and-Language-specific-Knowledge"><a href="#Lip-Reading-for-Low-resource-Languages-by-Learning-and-Combining-General-Speech-Knowledge-and-Language-specific-Knowledge" class="headerlink" title="Lip Reading for Low-resource Languages by Learning and Combining General Speech Knowledge and Language-specific Knowledge"></a>Lip Reading for Low-resource Languages by Learning and Combining General Speech Knowledge and Language-specific Knowledge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09311">http://arxiv.org/abs/2308.09311</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minsu Kim, Jeong Hun Yeo, Jeongsoo Choi, Yong Man Ro</li>
<li>for: 本研究提出了一种新的唇读框架，特别是针对低资源语言，前一代文献中未得到充分的关注。由于低资源语言缺乏足够的视频文本对应数据来训练模型，因此在这些语言上开发唇读模型被视为挑战。</li>
<li>methods: 我们尝试通过学习通用语言知识，即模型唇部运动的能力，从一种高资源语言来预测语音单位。由于不同语言有一定的共同声母，因此学习一种语言的通用语言知识可以扩展到其他语言。此外，我们还提出了语言特定的储存器（LMDecoder），它将语言特定的音频特征存储在内存银行中，并可以通过语音文本对应数据进行训练。</li>
<li>results: 通过对五种语言（英语、西班牙语、法语、意大利语、葡萄牙语）进行了广泛的实验，证明了我们提出的方法的效iveness。<details>
<summary>Abstract</summary>
This paper proposes a novel lip reading framework, especially for low-resource languages, which has not been well addressed in the previous literature. Since low-resource languages do not have enough video-text paired data to train the model to have sufficient power to model lip movements and language, it is regarded as challenging to develop lip reading models for low-resource languages. In order to mitigate the challenge, we try to learn general speech knowledge, the ability to model lip movements, from a high-resource language through the prediction of speech units. It is known that different languages partially share common phonemes, thus general speech knowledge learned from one language can be extended to other languages. Then, we try to learn language-specific knowledge, the ability to model language, by proposing Language-specific Memory-augmented Decoder (LMDecoder). LMDecoder saves language-specific audio features into memory banks and can be trained on audio-text paired data which is more easily accessible than video-text paired data. Therefore, with LMDecoder, we can transform the input speech units into language-specific audio features and translate them into texts by utilizing the learned rich language knowledge. Finally, by combining general speech knowledge and language-specific knowledge, we can efficiently develop lip reading models even for low-resource languages. Through extensive experiments using five languages, English, Spanish, French, Italian, and Portuguese, the effectiveness of the proposed method is evaluated.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Robust-Audio-Anti-Spoofing-with-Fusion-Reconstruction-Learning-on-Multi-Order-Spectrograms"><a href="#Robust-Audio-Anti-Spoofing-with-Fusion-Reconstruction-Learning-on-Multi-Order-Spectrograms" class="headerlink" title="Robust Audio Anti-Spoofing with Fusion-Reconstruction Learning on Multi-Order Spectrograms"></a>Robust Audio Anti-Spoofing with Fusion-Reconstruction Learning on Multi-Order Spectrograms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09302">http://arxiv.org/abs/2308.09302</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ph-w2000/s2pecnet">https://github.com/ph-w2000/s2pecnet</a></li>
<li>paper_authors: Penghui Wen, Kun Hu, Wenxi Yue, Sen Zhang, Wanlei Zhou, Zhiyong Wang<br>for: 这篇论文旨在提出一种基于多频域特征的深度学习方法，以提高音频反伪测试的效果。methods: 这篇论文使用了一种叫做S2pecNet的深度学习方法，它利用多频域特征来实现音频反伪测试。特别是，这篇论文使用了一种组合构成的混合方法，将多频域特征与时间上的特征混合在一起，以提高反伪测试的精度。results: 这篇论文的实验结果显示，S2pecNet方法可以实现高效的音频反伪测试。具体来说，在ASVspoof2019 LA Challenge上，S2pecNet方法的误分率（EER）为0.77%，与其他方法相比，表现出色。<details>
<summary>Abstract</summary>
Robust audio anti-spoofing has been increasingly challenging due to the recent advancements on deepfake techniques. While spectrograms have demonstrated their capability for anti-spoofing, complementary information presented in multi-order spectral patterns have not been well explored, which limits their effectiveness for varying spoofing attacks. Therefore, we propose a novel deep learning method with a spectral fusion-reconstruction strategy, namely S2pecNet, to utilise multi-order spectral patterns for robust audio anti-spoofing representations. Specifically, spectral patterns up to second-order are fused in a coarse-to-fine manner and two branches are designed for the fine-level fusion from the spectral and temporal contexts. A reconstruction from the fused representation to the input spectrograms further reduces the potential fused information loss. Our method achieved the state-of-the-art performance with an EER of 0.77% on a widely used dataset: ASVspoof2019 LA Challenge.
</details>
<details>
<summary>摘要</summary>
“对于深圳技术的进步，Robust audio anti-spoofing 对于不断增加的挑战。 spectrograms 已经展示了它们在反伪中的能力，但多维 spectral pattern 尚未得到充分利用，这限制了它们在不同的伪装攻击下的效iveness。因此，我们提出了一种基于深度学习的新方法，即 S2pecNet，具有多维 spectral pattern 的融合构想。具体来说，我们将spectral pattern 最多到第二顺序融合在一个course-to-fine的方式下，并设计了两个分支来从spectral和temporal context中获取细节。从融合表示重建到input spectrograms 可以更好地储存可能的融合信息损失。我们的方法在一个广泛使用的dataset上取得了现代最佳性能，EER 为0.77%。”
</details></li>
</ul>
<hr>
<h2 id="V2A-Mapper-A-Lightweight-Solution-for-Vision-to-Audio-Generation-by-Connecting-Foundation-Models"><a href="#V2A-Mapper-A-Lightweight-Solution-for-Vision-to-Audio-Generation-by-Connecting-Foundation-Models" class="headerlink" title="V2A-Mapper: A Lightweight Solution for Vision-to-Audio Generation by Connecting Foundation Models"></a>V2A-Mapper: A Lightweight Solution for Vision-to-Audio Generation by Connecting Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09300">http://arxiv.org/abs/2308.09300</a></li>
<li>repo_url: None</li>
<li>paper_authors: Heng Wang, Jianbo Ma, Santiago Pascual, Richard Cartwright, Weidong Cai</li>
<li>for: 该研究旨在提出一种轻量级的视觉对应声音生成方法，利用基础模型（FM）来解决跨modal生成问题。</li>
<li>methods: 该方法首先调查了视觉CLIP和听音CLAP模型之间的领域差异，然后提出了一种简单 yet effective的映射机制（V2A-Mapper）来桥接这个领域差异。 Conditioned on the translated CLAP embedding, 采用预训练的听音生成FM AudioLDM来生成高质量和视觉对应的声音。</li>
<li>results: 比较前方法，该方法只需快速训练V2A-Mapper，并在两个V2A数据集上进行了广泛的实验和分析。结果表明，使用生成映射器可以提高听音生成的质量和多样性（FD），而使用回归映射器可以提高听音生成的相关性（CS）。在FD和CS两个指标上，该方法与当前状态艺术方法相比，提高了53%和19%。<details>
<summary>Abstract</summary>
Building artificial intelligence (AI) systems on top of a set of foundation models (FMs) is becoming a new paradigm in AI research. Their representative and generative abilities learnt from vast amounts of data can be easily adapted and transferred to a wide range of downstream tasks without extra training from scratch. However, leveraging FMs in cross-modal generation remains under-researched when audio modality is involved. On the other hand, automatically generating semantically-relevant sound from visual input is an important problem in cross-modal generation studies. To solve this vision-to-audio (V2A) generation problem, existing methods tend to design and build complex systems from scratch using modestly sized datasets. In this paper, we propose a lightweight solution to this problem by leveraging foundation models, specifically CLIP, CLAP, and AudioLDM. We first investigate the domain gap between the latent space of the visual CLIP and the auditory CLAP models. Then we propose a simple yet effective mapper mechanism (V2A-Mapper) to bridge the domain gap by translating the visual input between CLIP and CLAP spaces. Conditioned on the translated CLAP embedding, pretrained audio generative FM AudioLDM is adopted to produce high-fidelity and visually-aligned sound. Compared to previous approaches, our method only requires a quick training of the V2A-Mapper. We further analyze and conduct extensive experiments on the choice of the V2A-Mapper and show that a generative mapper is better at fidelity and variability (FD) while a regression mapper is slightly better at relevance (CS). Both objective and subjective evaluation on two V2A datasets demonstrate the superiority of our proposed method compared to current state-of-the-art approaches - trained with 86% fewer parameters but achieving 53% and 19% improvement in FD and CS, respectively.
</details>
<details>
<summary>摘要</summary>
Currently, building artificial intelligence (AI) systems on top of a set of foundation models (FMs) is becoming a new trend in AI research. These FMs can learn representative and generative abilities from vast amounts of data, which can be easily adapted and transferred to a wide range of downstream tasks without needing to be trained from scratch. However, using FMs in cross-modal generation, especially when it comes to audio modality, is still an under-researched area. Specifically, automatically generating semantically relevant sound from visual input is an important problem in cross-modal generation studies.Existing methods for solving this vision-to-audio (V2A) generation problem tend to design and build complex systems from scratch using modestly sized datasets. In this paper, we propose a lightweight solution to this problem by leveraging foundation models, specifically CLIP, CLAP, and AudioLDM. We first investigate the domain gap between the latent space of the visual CLIP and the auditory CLAP models. Then, we propose a simple yet effective mapper mechanism (V2A-Mapper) to bridge the domain gap by translating the visual input between CLIP and CLAP spaces. Conditioned on the translated CLAP embedding, pretrained audio generative FM AudioLDM is adopted to produce high-fidelity and visually-aligned sound.Compared to previous approaches, our method only requires a quick training of the V2A-Mapper. We further analyze and conduct extensive experiments on the choice of the V2A-Mapper and show that a generative mapper is better at fidelity and variability (FD) while a regression mapper is slightly better at relevance (CS). Both objective and subjective evaluation on two V2A datasets demonstrate the superiority of our proposed method compared to current state-of-the-art approaches. We trained our method with 86% fewer parameters but achieved 53% and 19% improvement in FD and CS, respectively.
</details></li>
</ul>
<hr>
<h2 id="Bridging-High-Quality-Audio-and-Video-via-Language-for-Sound-Effects-Retrieval-from-Visual-Queries"><a href="#Bridging-High-Quality-Audio-and-Video-via-Language-for-Sound-Effects-Retrieval-from-Visual-Queries" class="headerlink" title="Bridging High-Quality Audio and Video via Language for Sound Effects Retrieval from Visual Queries"></a>Bridging High-Quality Audio and Video via Language for Sound Effects Retrieval from Visual Queries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09089">http://arxiv.org/abs/2308.09089</a></li>
<li>repo_url: None</li>
<li>paper_authors: Julia Wilkins, Justin Salamon, Magdalena Fuentes, Juan Pablo Bello, Oriol Nieto</li>
<li>for: 这个论文的目的是提出一种多模态框架，用于根据视频帧来检索高质量的声音效果（SFX）。</li>
<li>methods: 这个论文使用了大型语言模型和基础视觉语言模型来桥接高质量的音频和视频，从而创建了一个高度可扩展的自动音频视频数据筛选管道。它还使用预训练的音频和视频编码器来训练一种对比学习基于的检索系统。</li>
<li>results: 论文的实验结果表明，使用这个多模态框架，可以significantly outperform基线值在高质量声音效果检索任务上。此外，这个系统还能够在各种不同的数据集上进行 generale化，并且在用户研究中，人们对这个系统中的SFX Retrieval结果表示满意。<details>
<summary>Abstract</summary>
Finding the right sound effects (SFX) to match moments in a video is a difficult and time-consuming task, and relies heavily on the quality and completeness of text metadata. Retrieving high-quality (HQ) SFX using a video frame directly as the query is an attractive alternative, removing the reliance on text metadata and providing a low barrier to entry for non-experts. Due to the lack of HQ audio-visual training data, previous work on audio-visual retrieval relies on YouTube (in-the-wild) videos of varied quality for training, where the audio is often noisy and the video of amateur quality. As such it is unclear whether these systems would generalize to the task of matching HQ audio to production-quality video. To address this, we propose a multimodal framework for recommending HQ SFX given a video frame by (1) leveraging large language models and foundational vision-language models to bridge HQ audio and video to create audio-visual pairs, resulting in a highly scalable automatic audio-visual data curation pipeline; and (2) using pre-trained audio and visual encoders to train a contrastive learning-based retrieval system. We show that our system, trained using our automatic data curation pipeline, significantly outperforms baselines trained on in-the-wild data on the task of HQ SFX retrieval for video. Furthermore, while the baselines fail to generalize to this task, our system generalizes well from clean to in-the-wild data, outperforming the baselines on a dataset of YouTube videos despite only being trained on the HQ audio-visual pairs. A user study confirms that people prefer SFX retrieved by our system over the baseline 67% of the time both for HQ and in-the-wild data. Finally, we present ablations to determine the impact of model and data pipeline design choices on downstream retrieval performance. Please visit our project website to listen to and view our SFX retrieval results.
</details>
<details>
<summary>摘要</summary>
寻找符合视频中的声音效果（SFX）是一项复杂和时间consuming的任务，它取决于视频中文本 metadata 的质量和完整性。使用视频帧直接作为查询来检索高品质（HQ）声音的方法是一种吸引人的alternative，它可以消除文本 metadata 的依赖关系，并提供低门槛 для非专家。由于缺乏 HQ 音频视频培训数据，过去的声音视频检索工作通常使用 YouTube （在野）视频进行培训，这些视频的音频 oftentimes 噪音且视频质量不高。因此，是否这些系统能够通用到高品质音频与生产质量视频之间的匹配问题存在uncertainty。为解决这个问题，我们提议一种多模态框架，用于基于视频帧提供 HQ 声音效果的推荐，包括：1. 利用大语言模型和基础视频语言模型来桥接 HQ 音频和视频，从而创建高可扩展的自动音频视频数据纪要管道。2. 使用预训练的音频和视觉编码器来培训对比学习基于检索系统。我们的系统，通过我们自动生成的数据纪要管道进行训练，与基于野外数据的基eline 相比，显著提高了高品质声音效果检索任务的性能。此外，我们的系统可以从清晰到野外数据中进行扩展，并在 YouTube 视频集上表现出色，即使只受训练于 HQ 音频视频对。人们在用户研究中表示，他们67% 的时间 prefer SFX 被我们的系统检索出来，而不是基eline 。最后，我们提供了一系列ablation来评估模型和数据管道设计的影响。请参考我们项目网站来听取和查看我们的 SFX 检索结果。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/18/cs.SD_2023_08_18/" data-id="clm0t8e1e00aiv788avi2a2eg" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_08_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/18/eess.IV_2023_08_18/" class="article-date">
  <time datetime="2023-08-17T16:00:00.000Z" itemprop="datePublished">2023-08-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/18/eess.IV_2023_08_18/">eess.IV - 2023-08-18 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Uncertainty-based-quality-assurance-of-carotid-artery-wall-segmentation-in-black-blood-MRI"><a href="#Uncertainty-based-quality-assurance-of-carotid-artery-wall-segmentation-in-black-blood-MRI" class="headerlink" title="Uncertainty-based quality assurance of carotid artery wall segmentation in black-blood MRI"></a>Uncertainty-based quality assurance of carotid artery wall segmentation in black-blood MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09538">http://arxiv.org/abs/2308.09538</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/miagrouput/carotid-segmentation">https://github.com/miagrouput/carotid-segmentation</a></li>
<li>paper_authors: Elina Thibeau-Sutre, Dieuwertje Alblas, Sophie Buurman, Christoph Brune, Jelmer M. Wolterink</li>
<li>for: 这个研究旨在应用深度学习模型来自动确保大规模数据集的质量。</li>
<li>methods: 这种方法使用全自动的算法来 segmentation 黑血栓 MRI 中的血管壁。</li>
<li>results: 研究发现，包括不确定性测量不会下降 segmentation 的质量，不确定性指标可以作为质量指标，并且可以检测参与者级别的低质量 segmentation。<details>
<summary>Abstract</summary>
The application of deep learning models to large-scale data sets requires means for automatic quality assurance. We have previously developed a fully automatic algorithm for carotid artery wall segmentation in black-blood MRI that we aim to apply to large-scale data sets. This method identifies nested artery walls in 3D patches centered on the carotid artery. In this study, we investigate to what extent the uncertainty in the model predictions for the contour location can serve as a surrogate for error detection and, consequently, automatic quality assurance. We express the quality of automatic segmentations using the Dice similarity coefficient. The uncertainty in the model's prediction is estimated using either Monte Carlo dropout or test-time data augmentation. We found that (1) including uncertainty measurements did not degrade the quality of the segmentations, (2) uncertainty metrics provide a good proxy of the quality of our contours if the center found during the first step is enclosed in the lumen of the carotid artery and (3) they could be used to detect low-quality segmentations at the participant level. This automatic quality assurance tool might enable the application of our model in large-scale data sets.
</details>
<details>
<summary>摘要</summary>
深度学习模型在大规模数据集应用需要自动质量控制方法。我们之前已经开发了一种完全自动的护层动脉增强MRI图像分割算法，我们计划将其应用于大规模数据集。这种方法可以在3D补充中心于护层动脉的patch中标识嵌入的护层动脉增强。在这项研究中，我们研究了模型预测结果中的不确定性是否可以作为自动质量控制的代理。我们使用 dice相似度系数来衡量自动分割的质量。我们发现：（1）包括不确定性测量并不下降自动分割的质量，（2）不确定性指标可以作为护层动脉中心在护层动脉血栓中心的质量代理，（3）它们可以用来检测参与者级别的低质量分割。这种自动质量控制工具可能会使我们的模型在大规模数据集上应用。
</details></li>
</ul>
<hr>
<h2 id="INR-LDDMM-Fluid-based-Medical-Image-Registration-Integrating-Implicit-Neural-Representation-and-Large-Deformation-Diffeomorphic-Metric-Mapping"><a href="#INR-LDDMM-Fluid-based-Medical-Image-Registration-Integrating-Implicit-Neural-Representation-and-Large-Deformation-Diffeomorphic-Metric-Mapping" class="headerlink" title="INR-LDDMM: Fluid-based Medical Image Registration Integrating Implicit Neural Representation and Large Deformation Diffeomorphic Metric Mapping"></a>INR-LDDMM: Fluid-based Medical Image Registration Integrating Implicit Neural Representation and Large Deformation Diffeomorphic Metric Mapping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09473">http://arxiv.org/abs/2308.09473</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chulong Zhang, Xiaokun Liang</li>
<li>for: 这篇论文是用于医疗影像注册的流程基础架构，使用隐藏 нейрон表示法和大型可扭度Diffusion Metric Mapping（LDDMM）。</li>
<li>methods: 这篇论文使用了多层感知神经网络（MLP）来生成速度，同时估计速度和影像相似性。它还采用了从粗到细的方法来解决医疗影像注册方法中的严重扭转问题。</li>
<li>results: 这篇论文在一个包含50名病人的CT-CBCT资料集上验证了其方法，并使用转移过的标签作为评估指标。与现有方法相比，这篇论文的方法实现了现场状况的最佳性能。<details>
<summary>Abstract</summary>
We propose a flow-based registration framework of medical images based on implicit neural representation. By integrating implicit neural representation and Large Deformable Diffeomorphic Metric Mapping (LDDMM), we employ a Multilayer Perceptron (MLP) as a velocity generator while optimizing velocity and image similarity. Moreover, we adopt a coarse-to-fine approach to address the challenge of deformable-based registration methods dropping into local optimal solutions, thus aiding the management of significant deformations in medical image registration. Our algorithm has been validated on a paired CT-CBCT dataset of 50 patients,taking the dice coefficient of transferred annotations as an evaluation metric. Compared to existing methods, our approach achieves the state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
我们提出一种基于隐藏神经表示的医学图像注册框架。通过结合隐藏神经表示和大型可变截面度量mapping（LDDMM），我们使用多层感知器（MLP）作为速度生成器，同时优化速度和图像相似性。此外，我们采用宽泛到细节的方法来Addressing the challenge of deformable-based registration methods dropping into local optimal solutions，以避免医学图像注册中显著的形态变换问题。我们的算法在50名患者的CT-CBCT对照集上进行验证，并根据 transferred annotations的 dice coefficient 作为评价指标。与现有方法相比，我们的方法实现了状态的最佳性。
</details></li>
</ul>
<hr>
<h2 id="Quantitative-Susceptibility-Mapping-through-Model-based-Deep-Image-Prior-MoDIP"><a href="#Quantitative-Susceptibility-Mapping-through-Model-based-Deep-Image-Prior-MoDIP" class="headerlink" title="Quantitative Susceptibility Mapping through Model-based Deep Image Prior (MoDIP)"></a>Quantitative Susceptibility Mapping through Model-based Deep Image Prior (MoDIP)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09467">http://arxiv.org/abs/2308.09467</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhuang Xiong, Yang Gao, Yin Liu, Amir Fazlollahi, Peter Nestor, Feng Liu, Hongfu Sun</li>
<li>for: 提高Quantitative Susceptibility Mapping（QSM）中的批处理参数不同对象的普适性。</li>
<li>methods: 提出了一种新的无需训练的模型基于深度学习方法，称为MoDIP（模型基于深度图像先验）。MoDIP包括一个小型、未训练的网络和数据准确优化（DFO）模块。</li>
<li>results: MoDIP在不同扫描参数下的QSM普适性问题中表现出色，与深度学习和传统迭代方法相比，实现了32%的准确性提高，并且比传统DIP基于方法快33%，可以在4.5分钟内完成3D高分辨率图像重建。<details>
<summary>Abstract</summary>
The data-driven approach of supervised learning methods has limited applicability in solving dipole inversion in Quantitative Susceptibility Mapping (QSM) with varying scan parameters across different objects. To address this generalization issue in supervised QSM methods, we propose a novel training-free model-based unsupervised method called MoDIP (Model-based Deep Image Prior). MoDIP comprises a small, untrained network and a Data Fidelity Optimization (DFO) module. The network converges to an interim state, acting as an implicit prior for image regularization, while the optimization process enforces the physical model of QSM dipole inversion. Experimental results demonstrate MoDIP's excellent generalizability in solving QSM dipole inversion across different scan parameters. It exhibits robustness against pathological brain QSM, achieving over 32% accuracy improvement than supervised deep learning and traditional iterative methods. It is also 33% more computationally efficient and runs 4 times faster than conventional DIP-based approaches, enabling 3D high-resolution image reconstruction in under 4.5 minutes.
</details>
<details>
<summary>摘要</summary>
supervised学习方法的数据驱动方法在量子吸引mapping（QSM）中的不同对象扫描参数下有限的应用。为解决总化issue在超级vised QSM方法中，我们提出了一种新的无需训练的模型基于Unsupervised方法called MoDIP（模型基于深度图像先验）。MoDIP包括一个小型、未训练的网络和数据准确优化（DFO）模块。网络在某些扫描参数下 converges to an interim state，作为图像规范化的隐式先验，而优化过程检查QSM扫描器的物理模型。实验结果表明，MoDIP在不同扫描参数下的QSM扫描中具有出色的总化性能，与超级vised深度学习和传统迭代方法相比，具有32%的精度提高。此外，MoDIP还比折衔DIP基本方法更加快速，只需4.5分钟内可以完成3D高分辨率图像重建。
</details></li>
</ul>
<hr>
<h2 id="Causal-SAR-ATR-with-Limited-Data-via-Dual-Invariance"><a href="#Causal-SAR-ATR-with-Limited-Data-via-Dual-Invariance" class="headerlink" title="Causal SAR ATR with Limited Data via Dual Invariance"></a>Causal SAR ATR with Limited Data via Dual Invariance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09412">http://arxiv.org/abs/2308.09412</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cwwangsaratr/saratr_causal_dual_invariance">https://github.com/cwwangsaratr/saratr_causal_dual_invariance</a></li>
<li>paper_authors: Chenwei Wang, You Qin, Li Li, Siyi Luo, Yulin Huang, Jifang Pei, Yin Zhang, Jianyu Yang</li>
<li>for: 本文旨在提高受限制SAR数据的自动目标识别（SAR ATR）的迷你性。</li>
<li>methods: 本文提出了一个 causal ATR 模型，表明有限SAR数据中的噪声（N）会对特征X中的污染，导致SAR ATR的迷你性。为了解决这个问题，本文提出了一种双Inv隐含proxy和噪声不变损失。</li>
<li>results: 实验结果表明，提出的方法在三个标准数据集上达到了更高的性能。<details>
<summary>Abstract</summary>
Synthetic aperture radar automatic target recognition (SAR ATR) with limited data has recently been a hot research topic to enhance weak generalization. Despite many excellent methods being proposed, a fundamental theory is lacked to explain what problem the limited SAR data causes, leading to weak generalization of ATR. In this paper, we establish a causal ATR model demonstrating that noise $N$ that could be blocked with ample SAR data, becomes a confounder with limited data for recognition. As a result, it has a detrimental causal effect damaging the efficacy of feature $X$ extracted from SAR images, leading to weak generalization of SAR ATR with limited data. The effect of $N$ on feature can be estimated and eliminated by using backdoor adjustment to pursue the direct causality between $X$ and the predicted class $Y$. However, it is difficult for SAR images to precisely estimate and eliminated the effect of $N$ on $X$. The limited SAR data scarcely powers the majority of existing optimization losses based on empirical risk minimization (ERM), thus making it difficult to effectively eliminate $N$'s effect. To tackle with difficult estimation and elimination of $N$'s effect, we propose a dual invariance comprising the inner-class invariant proxy and the noise-invariance loss. Motivated by tackling change with invariance, the inner-class invariant proxy facilitates precise estimation of $N$'s effect on $X$ by obtaining accurate invariant features for each class with the limited data. The noise-invariance loss transitions the ERM's data quantity necessity into a need for noise environment annotations, effectively eliminating $N$'s effect on $X$ by cleverly applying the previous $N$'s estimation as the noise environment annotations. Experiments on three benchmark datasets indicate that the proposed method achieves superior performance.
</details>
<details>
<summary>摘要</summary>
射频干扰自动目标识别（SAR ATR）受限数据的研究在最近几年来非常热门，但是它们的基本理论仍然缺乏，这使得SAR ATR的训练受限数据的情况下存在强化问题。在这篇文章中，我们建立了一个导向性的ATR模型，证明了噪音N，可以阻据了充足的SAR数据，在有限数据情况下成为识别的干扰因子。这使得噪音N对特征X提出了负面的导向效应，导致SAR ATR的训练受限数据下存在弱化问题。但是，噪音N对特征X的影响可以通过后门调整来追求直接的 causality between X和预测的类别Y。即使SAR图像具有限制的数据能力，也可以通过内部的类别对称代理和噪音不对称损失来实现这一目的。实验结果显示，提案的方法在三个benchmark dataset上具有superior的表现。
</details></li>
</ul>
<hr>
<h2 id="Unveiling-Causalities-in-SAR-ATR-A-Causal-Interventional-Approach-for-Limited-Data"><a href="#Unveiling-Causalities-in-SAR-ATR-A-Causal-Interventional-Approach-for-Limited-Data" class="headerlink" title="Unveiling Causalities in SAR ATR: A Causal Interventional Approach for Limited Data"></a>Unveiling Causalities in SAR ATR: A Causal Interventional Approach for Limited Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09396">http://arxiv.org/abs/2308.09396</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenwei Wang, Xin Chen, You Qin, Siyi Luo, Yulin Huang, Jifang Pei, Jianyu Yang</li>
<li>For:	+ The paper aims to address the limited training data problem in synthetic aperture radar (SAR) automatic target recognition (ATR) by proposing a causal interventional ATR method (CIATR).* Methods:	+ The proposed CIATR method uses causal inference to understand the causal relationships among the key factors in ATR, and employs a structural causal model (SCM) to mitigate the spurious correlation introduced by limited SAR data.	+ The method includes data augmentation with spatial-frequency domain hybrid transformation and a feature discrimination approach with hybrid similarity measurement to measure and mitigate the impacts of varying imaging conditions on the extracted features from SAR images.* Results:	+ The proposed CIATR method has been experimented and compared on the moving and stationary target acquisition and recognition (MSTAR) and OpenSARship datasets, and has shown effective performance in pursuing the true causality between SAR images and the corresponding classes even with limited SAR data.<details>
<summary>Abstract</summary>
Synthetic aperture radar automatic target recognition (SAR ATR) methods fall short with limited training data. In this letter, we propose a causal interventional ATR method (CIATR) to formulate the problem of limited SAR data which helps us uncover the ever-elusive causalities among the key factors in ATR, and thus pursue the desired causal effect without changing the imaging conditions. A structural causal model (SCM) is comprised using causal inference to help understand how imaging conditions acts as a confounder introducing spurious correlation when SAR data is limited. This spurious correlation among SAR images and the predicted classes can be fundamentally tackled with the conventional backdoor adjustments. An effective implement of backdoor adjustments is proposed by firstly using data augmentation with spatial-frequency domain hybrid transformation to estimate the potential effect of varying imaging conditions on SAR images. Then, a feature discrimination approach with hybrid similarity measurement is introduced to measure and mitigate the structural and vector angle impacts of varying imaging conditions on the extracted features from SAR images. Thus, our CIATR can pursue the true causality between SAR images and the corresponding classes even with limited SAR data. Experiments and comparisons conducted on the moving and stationary target acquisition and recognition (MSTAR) and OpenSARship datasets have shown the effectiveness of our method with limited SAR data.
</details>
<details>
<summary>摘要</summary>
Synthetic aperture radar自动目标识别（SAR ATR）方法受有限训练数据的限制。在这封信中，我们提议一种 causal interventional ATR 方法（CIATR）来处理有限 SAR 数据的问题，并帮助我们探索 SAR 图像中隐藏的 causalities 以及它们如何影响 ATR 的性能。我们使用 causal inference 来理解如何限制 SAR 图像的捕捉条件引入了假 correlation，并且提出了一种基于 backdoor adjustments 的方法来解决这种假 correlation。我们首先使用数据扩充和空间频率域混合变换来估计限制 SAR 图像的可变 imaging 条件对 SAR 图像的影响。然后，我们引入了一种 hybrid similarity measurement 来衡量和减少限制 SAR 图像的结构和向量角度的影响。因此，我们的 CIATR 可以追求有限 SAR 数据中 true causality  между SAR 图像和对应的类别。我们在 MSTAR 和 OpenSARship 数据集上进行了实验和比较，并证明了我们的方法在有限 SAR 数据情况下的效果。
</details></li>
</ul>
<hr>
<h2 id="SAMedOCT-Adapting-Segment-Anything-Model-SAM-for-Retinal-OCT"><a href="#SAMedOCT-Adapting-Segment-Anything-Model-SAM-for-Retinal-OCT" class="headerlink" title="SAMedOCT: Adapting Segment Anything Model (SAM) for Retinal OCT"></a>SAMedOCT: Adapting Segment Anything Model (SAM) for Retinal OCT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09331">http://arxiv.org/abs/2308.09331</a></li>
<li>repo_url: None</li>
<li>paper_authors: Botond Fazekas, José Morano, Dmitrii Lachinov, Guilherme Aresta, Hrvoje Bogunović</li>
<li>for: 这篇论文是为了评估Segment Anything Model（SAM）在Retinal OCT扫描影像分类中的可行性和优势。</li>
<li>methods: 这篇论文使用了SAM模型和其修改版本，并与现有的State-of-the-art retinal fluid segmentation方法进行比较。</li>
<li>results: 研究发现，这些修改版本的SAM模型在大量的Retinal OCT扫描影像 dataset上具有了优秀的分类能力，但在一些情况下仍落后于现有的方法。<details>
<summary>Abstract</summary>
The Segment Anything Model (SAM) has gained significant attention in the field of image segmentation due to its impressive capabilities and prompt-based interface. While SAM has already been extensively evaluated in various domains, its adaptation to retinal OCT scans remains unexplored. To bridge this research gap, we conduct a comprehensive evaluation of SAM and its adaptations on a large-scale public dataset of OCTs from RETOUCH challenge. Our evaluation covers diverse retinal diseases, fluid compartments, and device vendors, comparing SAM against state-of-the-art retinal fluid segmentation methods. Through our analysis, we showcase adapted SAM's efficacy as a powerful segmentation model in retinal OCT scans, although still lagging behind established methods in some circumstances. The findings highlight SAM's adaptability and robustness, showcasing its utility as a valuable tool in retinal OCT image analysis and paving the way for further advancements in this domain.
</details>
<details>
<summary>摘要</summary>
Segment Anything Model (SAM) 已经在图像分割领域引起了广泛的关注，因为它的印象深刻和提示式接口。虽然 SAM 已经在不同领域进行了广泛的评估，但它在Retinal OCT扫描图像中的适用仍然未得到探索。为了填补这个研究差距，我们进行了大规模公共数据集的 RETOUCH 挑战中的 OCT 图像的全面评估。我们的评估覆盖了多种Retinal疾病、液体腔和设备制造商，与现有的Retinal液体分割方法进行比较。通过我们的分析，我们发现了适应 SAM 的可能性和强大性，尽管在某些情况下仍然落后于已知方法。这些发现表明 SAM 在Retinal OCT 图像分析中的可用性和稳定性，并且作为一个有价值的工具，可以在这个领域进一步发展。
</details></li>
</ul>
<hr>
<h2 id="Advancing-Intra-operative-Precision-Dynamic-Data-Driven-Non-Rigid-Registration-for-Enhanced-Brain-Tumor-Resection-in-Image-Guided-Neurosurgery"><a href="#Advancing-Intra-operative-Precision-Dynamic-Data-Driven-Non-Rigid-Registration-for-Enhanced-Brain-Tumor-Resection-in-Image-Guided-Neurosurgery" class="headerlink" title="Advancing Intra-operative Precision: Dynamic Data-Driven Non-Rigid Registration for Enhanced Brain Tumor Resection in Image-Guided Neurosurgery"></a>Advancing Intra-operative Precision: Dynamic Data-Driven Non-Rigid Registration for Enhanced Brain Tumor Resection in Image-Guided Neurosurgery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10868">http://arxiv.org/abs/2308.10868</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nikos Chrisochoides, Andriy Fedorov, Fotis Drakopoulos, Andriy Kot, Yixun Liu, Panos Foteinos, Angelos Angelopoulos, Olivier Clatz, Nicholas Ayache, Peter M. Black, Alex J. Golby, Ron Kikinis</li>
<li>for: 用于 neurosurgery 中医学影像识别肿瘤和关键结构</li>
<li>methods: 使用 Dynamic Data-Driven Non-Rigid Registration (NRR) 方法对医学影像进行调整，以考虑 operation 中脑膜的变化</li>
<li>results: 实现 NRR 结果在临床时间限制内，并通过分布式计算和机器学习提高 registration 精度，同时描述了在操作室中使用 NRR 的挑战<details>
<summary>Abstract</summary>
During neurosurgery, medical images of the brain are used to locate tumors and critical structures, but brain tissue shifts make pre-operative images unreliable for accurate removal of tumors. Intra-operative imaging can track these deformations but is not a substitute for pre-operative data. To address this, we use Dynamic Data-Driven Non-Rigid Registration (NRR), a complex and time-consuming image processing operation that adjusts the pre-operative image data to account for intra-operative brain shift. Our review explores a specific NRR method for registering brain MRI during image-guided neurosurgery and examines various strategies for improving the accuracy and speed of the NRR method. We demonstrate that our implementation enables NRR results to be delivered within clinical time constraints while leveraging Distributed Computing and Machine Learning to enhance registration accuracy by identifying optimal parameters for the NRR method. Additionally, we highlight challenges associated with its use in the operating room.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese: During 神经外科手术，医疗影像被用来确定肿瘤和重要结构，但是脑组织的运动使得先前的影像无法准确地识别肿瘤。 intra-operative imaging可以跟踪这些变形，但是不能代替先前的数据。为解决这个问题，我们使用动态数据驱动非固定注册（NRR）方法，将先前的影像数据调整以 compte for intra-operative brain shift。我们的文章检讨了一种特定的NRR方法，用于在图像引导神经外科手术中注册脑MRI。我们还检讨了不同的策略来提高NRR方法的准确性和速度。我们的实现可以在临床时间限制内提供NRR结果，并利用分布式计算和机器学习来提高注册精度。此外，我们还 highlighted some challenges associated with its use in the operating room.Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="JPEG-Quantized-Coefficient-Recovery-via-DCT-Domain-Spatial-Frequential-Transformer"><a href="#JPEG-Quantized-Coefficient-Recovery-via-DCT-Domain-Spatial-Frequential-Transformer" class="headerlink" title="JPEG Quantized Coefficient Recovery via DCT Domain Spatial-Frequential Transformer"></a>JPEG Quantized Coefficient Recovery via DCT Domain Spatial-Frequential Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09110">http://arxiv.org/abs/2308.09110</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingyu Ouyang, Zhenzhong Chen</li>
<li>for: 本研究旨在提出一种基于DCT频域的JPEG压缩图像恢复方法，以提高压缩图像的Restoration效果。</li>
<li>methods: 我们提出了一种名为DCTransformer的双支持结构，通过捕捉DCT快 Fourier Transform的空间-频域相关性来提高图像Restoration效果。此外，我们还采用量化矩阵嵌入和同色度组分对齐来扩展模型的应用范围。</li>
<li>results: 我们的DCTransformer模型在对压缩JPEG图像进行恢复时表现出色，在各种质量因素下都能够达到更高的Restoration效果。<details>
<summary>Abstract</summary>
JPEG compression adopts the quantization of Discrete Cosine Transform (DCT) coefficients for effective bit-rate reduction, whilst the quantization could lead to a significant loss of important image details. Recovering compressed JPEG images in the frequency domain has attracted more and more attention recently, in addition to numerous restoration approaches developed in the pixel domain. However, the current DCT domain methods typically suffer from limited effectiveness in handling a wide range of compression quality factors, or fall short in recovering sparse quantized coefficients and the components across different colorspace. To address these challenges, we propose a DCT domain spatial-frequential Transformer, named as DCTransformer. Specifically, a dual-branch architecture is designed to capture both spatial and frequential correlations within the collocated DCT coefficients. Moreover, we incorporate the operation of quantization matrix embedding, which effectively allows our single model to handle a wide range of quality factors, and a luminance-chrominance alignment head that produces a unified feature map to align different-sized luminance and chrominance components. Our proposed DCTransformer outperforms the current state-of-the-art JPEG artifact removal techniques, as demonstrated by our extensive experiments.
</details>
<details>
<summary>摘要</summary>
JPEG压缩采用了离散余弦变换（DCT）系数的归一化进行有效的比特率减少，而这可能会导致重要的图像细节产生损失。在频域中还原压缩的JPEG图像已经引起了越来越多的关注，而 besides numerous restoration approaches developed in the pixel domain. However, the current DCT domain methods typically suffer from limited effectiveness in handling a wide range of compression quality factors, or fall short in recovering sparse quantized coefficients and the components across different color spaces. To address these challenges, we propose a DCT domain spatial-frequential Transformer, named as DCTransformer. Specifically, a dual-branch architecture is designed to capture both spatial and frequential correlations within the collocated DCT coefficients. Moreover, we incorporate the operation of quantization matrix embedding, which effectively allows our single model to handle a wide range of quality factors, and a luminance-chrominance alignment head that produces a unified feature map to align different-sized luminance and chrominance components. Our proposed DCTransformer outperforms the current state-of-the-art JPEG artifact removal techniques, as demonstrated by our extensive experiments.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/18/eess.IV_2023_08_18/" data-id="clm0t8e2v00fev7885obd0dgi" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_17" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/17/cs.LG_2023_08_17/" class="article-date">
  <time datetime="2023-08-16T16:00:00.000Z" itemprop="datePublished">2023-08-17</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/17/cs.LG_2023_08_17/">cs.LG - 2023-08-17 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Enhancing-API-Documentation-through-BERTopic-Modeling-and-Summarization"><a href="#Enhancing-API-Documentation-through-BERTopic-Modeling-and-Summarization" class="headerlink" title="Enhancing API Documentation through BERTopic Modeling and Summarization"></a>Enhancing API Documentation through BERTopic Modeling and Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09070">http://arxiv.org/abs/2308.09070</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/scam2023-bert/bertopic">https://github.com/scam2023-bert/bertopic</a></li>
<li>paper_authors: AmirHossein Naghshzan, Sylvie Ratte</li>
<li>for: 本研究旨在提高API文档的可读性和效率，以便开发者更方便地从官方文档中提取有用信息。</li>
<li>methods: 本研究使用BERTopic进行主题分析，并采用自然语言处理（NLP）技术生成API文档的摘要。</li>
<li>results: 研究发现了各种常见主题和问题，并生成了可能的解决方案，从而提高了开发者对复杂API的理解和 Navigation 效率。<details>
<summary>Abstract</summary>
As the amount of textual data in various fields, including software development, continues to grow, there is a pressing demand for efficient and effective extraction and presentation of meaningful insights. This paper presents a unique approach to address this need, focusing on the complexities of interpreting Application Programming Interface (API) documentation. While official API documentation serves as a primary source of information for developers, it can often be extensive and lacks user-friendliness. In light of this, developers frequently resort to unofficial sources like Stack Overflow and GitHub. Our novel approach employs the strengths of BERTopic for topic modeling and Natural Language Processing (NLP) to automatically generate summaries of API documentation, thereby creating a more efficient method for developers to extract the information they need. The produced summaries and topics are evaluated based on their performance, coherence, and interoperability.   The findings of this research contribute to the field of API documentation analysis by providing insights into recurring topics, identifying common issues, and generating potential solutions. By improving the accessibility and efficiency of API documentation comprehension, our work aims to enhance the software development process and empower developers with practical tools for navigating complex APIs.
</details>
<details>
<summary>摘要</summary>
随着不同领域的文本数据量不断增加，包括软件开发，有效和高效地提取和展示有用的洞察结论成为了一项急需。这篇论文提出了一种独特的方法，专注于API文档解释的复杂性。尽管官方API文档作为开发者的主要信息来源，但它们可能是广泛的和不易于使用的。为此，开发者经常查阅Stack Overflow和GitHub等非官方来源。我们的新方法利用BERTopic的话题模型和自然语言处理（NLP）技术，自动生成API文档摘要，从而为开发者提供更高效的信息提取方式。生成的摘要和话题被评估基于其性能、一致性和可操作性。我们的研究成果对API文档分析领域进行了贡献，提供了循环话题、常见问题的指导和解决方案。我们的工作目标是通过改善API文档理解的可 accessed性和效率，推动软件开发过程和开发者在复杂API中导航的实用工具。
</details></li>
</ul>
<hr>
<h2 id="Uplift-Modeling-from-Causal-Inference-to-Personalization"><a href="#Uplift-Modeling-from-Causal-Inference-to-Personalization" class="headerlink" title="Uplift Modeling: from Causal Inference to Personalization"></a>Uplift Modeling: from Causal Inference to Personalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09066">http://arxiv.org/abs/2308.09066</a></li>
<li>repo_url: None</li>
<li>paper_authors: Felipe Moraes, Hugo Manuel Proença, Anastasiia Kornilova, Javier Albert, Dmitri Goldenberg</li>
<li>For: The paper is written for individuals who want to learn about uplift modeling and its applications in personalized promotional campaigns.* Methods: The paper introduces state-of-the-art techniques in uplift modeling, including the advantages and limitations of different approaches. It also covers the unique setup of constrained uplift modeling.* Results: The paper presents real-life applications of uplift modeling and discusses challenges in implementing these models in production.Here’s the same information in Simplified Chinese text:* For: 论文主要是为了探讨个性化促销活动中的营销效果估计和个性化推荐。* Methods: 论文介绍了当前最新的营销效果估计技术，包括不同方法的优点和缺点，以及受限制的营销效果估计。* Results: 论文介绍了实际应用的营销效果估计案例，以及实现这些模型的挑战。<details>
<summary>Abstract</summary>
Uplift modeling is a collection of machine learning techniques for estimating causal effects of a treatment at the individual or subgroup levels. Over the last years, causality and uplift modeling have become key trends in personalization at online e-commerce platforms, enabling the selection of the best treatment for each user in order to maximize the target business metric. Uplift modeling can be particularly useful for personalized promotional campaigns, where the potential benefit caused by a promotion needs to be weighed against the potential costs. In this tutorial we will cover basic concepts of causality and introduce the audience to state-of-the-art techniques in uplift modeling. We will discuss the advantages and the limitations of different approaches and dive into the unique setup of constrained uplift modeling. Finally, we will present real-life applications and discuss challenges in implementing these models in production.
</details>
<details>
<summary>摘要</summary>
“ upplift 模型是一种集成机器学习技术，用于估计干预效应的个体或 subgroup 水平。过去几年， causality 和 upplift 模型在在线电商平台上Personalization中变得越来越普遍，以便为每个用户选择最佳治疗，以最大化目标业务指标。upplift 模型在个性化促销活动中 particualrly 有用，因为促销的潜在利益需要与潜在成本进行平衡。在这个教程中，我们将覆盖 causality 的基本概念，并介绍现代 uplift 模型的技术。我们将讨论不同方法的优势和局限性，并深入探讨受限 uplift 模型的特殊设置。最后，我们将介绍实际应用和在生产中实施这些模型的挑战。”
</details></li>
</ul>
<hr>
<h2 id="Discretization-Induced-Dirichlet-Posterior-for-Robust-Uncertainty-Quantification-on-Regression"><a href="#Discretization-Induced-Dirichlet-Posterior-for-Robust-Uncertainty-Quantification-on-Regression" class="headerlink" title="Discretization-Induced Dirichlet Posterior for Robust Uncertainty Quantification on Regression"></a>Discretization-Induced Dirichlet Posterior for Robust Uncertainty Quantification on Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09065">http://arxiv.org/abs/2308.09065</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuanlong Yu, Gianni Franchi, Jindong Gu, Emanuel Aldea</li>
<li>for: 这个论文主要是为了提供更加 Robust uncertainty quantification 方法，以便在实际应用中部署深度神经网络 (DNNs)。</li>
<li>methods: 这个方法使用了一个称为 Auxiliary Uncertainty Estimator (AuxUE) 的方法，并且考虑了不同的分布假设来估计随机误差，最终选择了 Laplace 分布估计预测误差。此外，这个方法还提出了一个名为 Discretization-Induced Dirichlet pOsterior (DIDO) 的新解决方案，用于模型预测误差的 Dirichlet  posterior。</li>
<li>results: 实验结果显示，这个方法可以在噪音输入下提供更加Robust的 uncertainty estimates，并且可以扩展到 both image-level 和 pixel-wise 任务上。<details>
<summary>Abstract</summary>
Uncertainty quantification is critical for deploying deep neural networks (DNNs) in real-world applications. An Auxiliary Uncertainty Estimator (AuxUE) is one of the most effective means to estimate the uncertainty of the main task prediction without modifying the main task model. To be considered robust, an AuxUE must be capable of maintaining its performance and triggering higher uncertainties while encountering Out-of-Distribution (OOD) inputs, i.e., to provide robust aleatoric and epistemic uncertainty. However, for vision regression tasks, current AuxUE designs are mainly adopted for aleatoric uncertainty estimates, and AuxUE robustness has not been explored. In this work, we propose a generalized AuxUE scheme for more robust uncertainty quantification on regression tasks. Concretely, to achieve a more robust aleatoric uncertainty estimation, different distribution assumptions are considered for heteroscedastic noise, and Laplace distribution is finally chosen to approximate the prediction error. For epistemic uncertainty, we propose a novel solution named Discretization-Induced Dirichlet pOsterior (DIDO), which models the Dirichlet posterior on the discretized prediction error. Extensive experiments on age estimation, monocular depth estimation, and super-resolution tasks show that our proposed method can provide robust uncertainty estimates in the face of noisy inputs and that it can be scalable to both image-level and pixel-wise tasks.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "Uncertainty quantification is critical for deploying deep neural networks (DNNs) in real-world applications. An Auxiliary Uncertainty Estimator (AuxUE) is one of the most effective means to estimate the uncertainty of the main task prediction without modifying the main task model. To be considered robust, an AuxUE must be capable of maintaining its performance and triggering higher uncertainties while encountering Out-of-Distribution (OOD) inputs, i.e., to provide robust aleatoric and epistemic uncertainty. However, for vision regression tasks, current AuxUE designs are mainly adopted for aleatoric uncertainty estimates, and AuxUE robustness has not been explored. In this work, we propose a generalized AuxUE scheme for more robust uncertainty quantification on regression tasks. Concretely, to achieve a more robust aleatoric uncertainty estimation, different distribution assumptions are considered for heteroscedastic noise, and Laplace distribution is finally chosen to approximate the prediction error. For epistemic uncertainty, we propose a novel solution named Discretization-Induced Dirichlet pOsterior (DIDO), which models the Dirichlet posterior on the discretized prediction error. Extensive experiments on age estimation, monocular depth estimation, and super-resolution tasks show that our proposed method can provide robust uncertainty estimates in the face of noisy inputs and that it can be scalable to both image-level and pixel-wise tasks."中文翻译：uncertainty quantification是深度神经网络（DNN）在实际应用中的关键。auxiliary uncertainty estimator（AuxUE）是修改主任务模型的最有效的方法来估计主任务预测结果的uncertainty。为了被视为可靠，AuxUE必须能够保持其性能并在面对Out-of-Distribution（OOD）输入时触发更高的uncertainty。然而，目前的AuxUE设计主要用于aleatoric uncertainty estimate，而AuxUE的Robustness尚未被探索。在这项工作中，我们提出了一种通用的AuxUE方案，用于更加Robust的uncertainty量化。 Specifically，为了实现更加Robust的aleatoric uncertainty estimate，我们考虑了不同的分布假设，并最终选择了Laplace分布来近似预测错误。 For epistemic uncertainty，我们提出了一种新的解决方案，名为Discretization-Induced Dirichlet Posterior（DIDO），它模型了预测错误的Discretized posterior。经验表明，我们的提议方法可以在噪声输入下提供Robust的uncertainty估计，并且可以扩展到像素级和图像级任务。>>
</details></li>
</ul>
<hr>
<h2 id="Refining-a-Deep-Learning-based-Formant-Tracker-using-Linear-Prediction-Methods"><a href="#Refining-a-Deep-Learning-based-Formant-Tracker-using-Linear-Prediction-Methods" class="headerlink" title="Refining a Deep Learning-based Formant Tracker using Linear Prediction Methods"></a>Refining a Deep Learning-based Formant Tracker using Linear Prediction Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09051">http://arxiv.org/abs/2308.09051</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paavo Alku, Sudarsana Reddy Kadiri, Dhananjaya Gowda</li>
<li>for: 这个研究是 investigate 形式追踪的，使用现有的数据驱动追踪器DeepFormants的形式被改进，使用LP-based方法来估算形式。</li>
<li>methods: 这个研究使用了LP-COV和QCP-FB两种LP-based方法来估算形式，并将这些估算结果与数据驱动DeepFormants tracker的预测结果进行比较。</li>
<li>results: 研究结果表明，使用QCP-FB方法来改进DeepFormants tracker的表现最佳，并且这种改进的追踪器在受到雑音损害的情况下表现更好。<details>
<summary>Abstract</summary>
In this study, formant tracking is investigated by refining the formants tracked by an existing data-driven tracker, DeepFormants, using the formants estimated in a model-driven manner by linear prediction (LP)-based methods. As LP-based formant estimation methods, conventional covariance analysis (LP-COV) and the recently proposed quasi-closed phase forward-backward (QCP-FB) analysis are used. In the proposed refinement approach, the contours of the three lowest formants are first predicted by the data-driven DeepFormants tracker, and the predicted formants are replaced frame-wise with local spectral peaks shown by the model-driven LP-based methods. The refinement procedure can be plugged into the DeepFormants tracker with no need for any new data learning. Two refined DeepFormants trackers were compared with the original DeepFormants and with five known traditional trackers using the popular vocal tract resonance (VTR) corpus. The results indicated that the data-driven DeepFormants trackers outperformed the conventional trackers and that the best performance was obtained by refining the formants predicted by DeepFormants using QCP-FB analysis. In addition, by tracking formants using VTR speech that was corrupted by additive noise, the study showed that the refined DeepFormants trackers were more resilient to noise than the reference trackers. In general, these results suggest that LP-based model-driven approaches, which have traditionally been used in formant estimation, can be combined with a modern data-driven tracker easily with no further training to improve the tracker's performance.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们调查了形式追踪的可行性，通过改进现有的数据驱动追踪器DeepFormants的形式追踪结果，使用模型驱动的方法来估算形式。作为模型驱动的形式估算方法，我们使用了传统的covariance分析(LP-COV)和最近提出的 quasi-closed phase forward-backward(QCP-FB)分析。在我们提议的改进方法中，首先使用DeepFormants tracker来预测三个最低的形式，然后将预测的形式替换为每帧的本地 спектраль峰点，这些峰点是由模型驱动的LP-based方法估算出来的。这个改进过程可以轻松地插入到DeepFormants tracker中，无需进行任何新的数据学习。我们 compare了两个改进后的DeepFormants tracker与原始DeepFormants tracker和五个已知的传统追踪器，结果表明，数据驱动的DeepFormants tracker比传统追踪器高效，而使用QCP-FB分析进行改进后的追踪器表现最佳。此外，通过使用受损的VTR语音追踪，研究发现，改进后的DeepFormants tracker对噪声抗性更高于参照追踪器。总的来说，这些结果表明，LP-based模型驱动的方法可以轻松地与现有的数据驱动追踪器结合使用，无需进行任何新的数据学习，以提高追踪器的性能。
</details></li>
</ul>
<hr>
<h2 id="Kernel-Based-Tests-for-Likelihood-Free-Hypothesis-Testing"><a href="#Kernel-Based-Tests-for-Likelihood-Free-Hypothesis-Testing" class="headerlink" title="Kernel-Based Tests for Likelihood-Free Hypothesis Testing"></a>Kernel-Based Tests for Likelihood-Free Hypothesis Testing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09043">http://arxiv.org/abs/2308.09043</a></li>
<li>repo_url: None</li>
<li>paper_authors: Patrik Róbert Gerber, Tianze Jiang, Yury Polyanskiy, Rui Sun</li>
<li>For: The paper is focused on the problem of labeling additional inputs when only a portion of the data is labeled, specifically in the context of likelihood-free inference.* Methods: The paper introduces a generalization of the problem where unlabeled samples come from a mixture of the two classes, and studies the minimax sample complexity for non-parametric classes of densities under maximum mean discrepancy (MMD) separation.* Results: The paper investigates the empirical performance of kernels parameterized by neural networks on two tasks: detection of the Higgs boson and detection of planted DDPM generated images amidst CIFAR-10 images, and confirms the existence of the theoretically predicted asymmetric $m$ vs $n$ trade-off.Here’s the Chinese translation of the three points:* For: 这篇论文关注的是只有部分数据标注的情况下，进行likelihood-free推断的标注问题。* Methods: 论文引入了一种扩展，即未标注样本来自两个类的混合，并研究非 Parametric 类型的density下的最小最大值复杂性。* Results: 论文 investigate了使用神经网络参数化的kernel在两个任务上的实际性能：探测希格斯粒子和探测DDPM生成的图像 amidst CIFAR-10图像，并证实了理论上预测的 $m$ vs $n$ 负面trade-off。<details>
<summary>Abstract</summary>
Given $n$ observations from two balanced classes, consider the task of labeling an additional $m$ inputs that are known to all belong to \emph{one} of the two classes. Special cases of this problem are well-known: with complete knowledge of class distributions ($n=\infty$) the problem is solved optimally by the likelihood-ratio test; when $m=1$ it corresponds to binary classification; and when $m\approx n$ it is equivalent to two-sample testing. The intermediate settings occur in the field of likelihood-free inference, where labeled samples are obtained by running forward simulations and the unlabeled sample is collected experimentally. In recent work it was discovered that there is a fundamental trade-off between $m$ and $n$: increasing the data sample $m$ reduces the amount $n$ of training/simulation data needed. In this work we (a) introduce a generalization where unlabeled samples come from a mixture of the two classes -- a case often encountered in practice; (b) study the minimax sample complexity for non-parametric classes of densities under \textit{maximum mean discrepancy} (MMD) separation; and (c) investigate the empirical performance of kernels parameterized by neural networks on two tasks: detection of the Higgs boson and detection of planted DDPM generated images amidst CIFAR-10 images. For both problems we confirm the existence of the theoretically predicted asymmetric $m$ vs $n$ trade-off.
</details>
<details>
<summary>摘要</summary>
In this work, we:(a) Introduce a generalization where unlabeled samples come from a mixture of the two classes -- a case often encountered in practice.(b) Study the minimax sample complexity for non-parametric classes of densities under maximum mean discrepancy (MMD) separation.(c) Investigate the empirical performance of kernels parameterized by neural networks on two tasks: detection of the Higgs boson and detection of planted DDPM generated images amidst CIFAR-10 images. For both problems, we confirm the existence of the theoretically predicted asymmetric $m$ vs $n$ trade-off.
</details></li>
</ul>
<hr>
<h2 id="LesionMix-A-Lesion-Level-Data-Augmentation-Method-for-Medical-Image-Segmentation"><a href="#LesionMix-A-Lesion-Level-Data-Augmentation-Method-for-Medical-Image-Segmentation" class="headerlink" title="LesionMix: A Lesion-Level Data Augmentation Method for Medical Image Segmentation"></a>LesionMix: A Lesion-Level Data Augmentation Method for Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09026">http://arxiv.org/abs/2308.09026</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dogabasaran/lesionmix">https://github.com/dogabasaran/lesionmix</a></li>
<li>paper_authors: Berke Doga Basaran, Weitong Zhang, Mengyun Qiao, Bernhard Kainz, Paul M. Matthews, Wenjia Bai</li>
<li>for: 提高深度学习基于医学影像分割方法的精度和稳定性，通过对医学影像进行数据增强。</li>
<li>methods: LesionMix是一种新的、简单的疾病意识数据增强方法，通过在肿瘤水平进行数据增强，提高了肿瘤形态、位置、强度和负荷分布的多样性，同时允许肿瘤填充和缺失。</li>
<li>results: 在不同的Modalities和不同的肿瘤数据集上，LesionMix实现了优秀的肿瘤图像分割性能，比较新的 Mix-based 数据增强方法更好。代码将于<a target="_blank" rel="noopener" href="https://github.com/dogabasaran/lesionmix">https://github.com/dogabasaran/lesionmix</a> 发布。<details>
<summary>Abstract</summary>
Data augmentation has become a de facto component of deep learning-based medical image segmentation methods. Most data augmentation techniques used in medical imaging focus on spatial and intensity transformations to improve the diversity of training images. They are often designed at the image level, augmenting the full image, and do not pay attention to specific abnormalities within the image. Here, we present LesionMix, a novel and simple lesion-aware data augmentation method. It performs augmentation at the lesion level, increasing the diversity of lesion shape, location, intensity and load distribution, and allowing both lesion populating and inpainting. Experiments on different modalities and different lesion datasets, including four brain MR lesion datasets and one liver CT lesion dataset, demonstrate that LesionMix achieves promising performance in lesion image segmentation, outperforming several recent Mix-based data augmentation methods. The code will be released at https://github.com/dogabasaran/lesionmix.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>深度学习基于医学影像分割方法中的数据扩充已成为一种标准组成部分。大多数医学影像扩充技术都是在空间和强度水平进行变换，以提高训练图像的多样性。它们通常是在图像层次上进行设计，对全图像进行扩充，而不是关注特定的病变内部。在这里，我们介绍了LesionMix，一种新的和简单的病变意识数据扩充方法。它在病变层次上进行扩充，提高病变形状、位置、强度和负荷分布，并允许病变填充和抹除。在不同的modalities和不同的病变数据集上，包括四个脑MR病变数据集和一个肝CT病变数据集，LesionMix在病变图像分割中实现了可观的表现，比较出色于一些最近的混合数据扩充方法。代码将在https://github.com/dogabasaran/lesionmix上发布。
</details></li>
</ul>
<hr>
<h2 id="Reinforcement-Learning-for-Battery-Management-in-Dairy-Farming"><a href="#Reinforcement-Learning-for-Battery-Management-in-Dairy-Farming" class="headerlink" title="Reinforcement Learning for Battery Management in Dairy Farming"></a>Reinforcement Learning for Battery Management in Dairy Farming</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09023">http://arxiv.org/abs/2308.09023</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nawazish Ali, Abdul Wahid, Rachael shaw, Karl Mason</li>
<li>for: 这项研究是为了应对牛奶农业中的能源消耗异常高的问题，通过人工智能技术来优化电池充放和充电管理策略，以提高牛奶农业中的能源效率和可持续性。</li>
<li>methods: 该研究使用Q学习算法来学习一个有效的电池充放和充电策略，并对比与现有的基线算法进行比较。</li>
<li>results: 研究结果表明，使用Q学习算法可以Significantly reduce electricity costs compared to the baseline algorithm，这些发现 highlights the effectiveness of reinforcement learning for battery management within the dairy farming sector。<details>
<summary>Abstract</summary>
Dairy farming is a particularly energy-intensive part of the agriculture sector. Effective battery management is essential for renewable integration within the agriculture sector. However, controlling battery charging/discharging is a difficult task due to electricity demand variability, stochasticity of renewable generation, and energy price fluctuations. Despite the potential benefits of applying Artificial Intelligence (AI) to renewable energy in the context of dairy farming, there has been limited research in this area. This research is a priority for Ireland as it strives to meet its governmental goals in energy and sustainability. This research paper utilizes Q-learning to learn an effective policy for charging and discharging a battery within a dairy farm setting. The results demonstrate that the developed policy significantly reduces electricity costs compared to the established baseline algorithm. These findings highlight the effectiveness of reinforcement learning for battery management within the dairy farming sector.
</details>
<details>
<summary>摘要</summary>
奶业是农业部门中特别能耗能源的一部分。有效的电池管理是重要的，以便在农业部门中 integrating 可再生能源。然而，控制电池充放电是一个困难的任务，因为能源需求的变化、可再生能源的随机性和能源价格的波动。尽管应用人工智能（AI）到奶业中可能有很多的优点，但是这个领域的研究却有限。这项研究是爱尔兰政府的一个优先事项，以实现能源和可持续发展的目标。这篇研究论文使用Q学习算法学习一个有效的电池充放电策略，结果表明，发展的策略可以较基准算法减少电力成本。这些发现表明，强化学习可以在奶业部门中有效地管理电池。
</details></li>
</ul>
<hr>
<h2 id="Multi-field-Visualisation-via-Trait-induced-Merge-Trees"><a href="#Multi-field-Visualisation-via-Trait-induced-Merge-Trees" class="headerlink" title="Multi-field Visualisation via Trait-induced Merge Trees"></a>Multi-field Visualisation via Trait-induced Merge Trees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09015">http://arxiv.org/abs/2308.09015</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jochen Jankowai, Talha Bin Masood, Ingrid Hotz</li>
<li>for: 这paper是为了探讨tensor场或多变量数据的分析，提出 trait-based merge trees，一种特性基于merge trees的总结。</li>
<li>methods: 这paper使用了特性空间中的特性定义，以及Attribute Space中的特性空间定义，将distance field转化为属性空间中的一个整数场，用于 topological data analysis。</li>
<li>results: 这paper提出了一种基于特性的merge tree Hierarchy，可以用于查询最相似和持续存在的特性，并提供了不同的查询方法以便高亮不同的特性方面。三个 caso study在不同的领域中应用了该方法，以证明其跨领域可用性。<details>
<summary>Abstract</summary>
In this work, we propose trait-based merge trees a generalization of merge trees to feature level sets, targeting the analysis of tensor field or general multi-variate data. For this, we employ the notion of traits defined in attribute space as introduced in the feature level sets framework. The resulting distance field in attribute space induces a scalar field in the spatial domain that serves as input for topological data analysis. The leaves in the merge tree represent those areas in the input data that are closest to the defined trait and thus most closely resemble the defined feature. Hence, the merge tree yields a hierarchy of features that allows for querying the most relevant and persistent features. The presented method includes different query methods for the tree which enable the highlighting of different aspects. We demonstrate the cross-application capabilities of this approach with three case studies from different domains.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们提出了 trait-based merge trees，这是 merge trees 的一种普遍化，targeting tensor field 或一般多变量数据的分析。为此，我们利用 attribute space 中定义的特质（trait）。在这个框架中，输入数据的特征空间距离场所引入了一个拓扑数据分析的输入场。merge tree 的叶子节点表示输入数据中最接近定义特质的区域，因此最接近定义的特征。因此，merge tree 提供了一个特征层次结构，可以对输入数据进行特征 queries。我们还提供了不同的查询方法，可以根据不同的需求高亮不同的特征。我们通过三个不同领域的案例，证明了这种方法的跨应用性。
</details></li>
</ul>
<hr>
<h2 id="Deep-seeded-Clustering-for-Unsupervised-Valence-Arousal-Emotion-Recognition-from-Physiological-Signals"><a href="#Deep-seeded-Clustering-for-Unsupervised-Valence-Arousal-Emotion-Recognition-from-Physiological-Signals" class="headerlink" title="Deep-seeded Clustering for Unsupervised Valence-Arousal Emotion Recognition from Physiological Signals"></a>Deep-seeded Clustering for Unsupervised Valence-Arousal Emotion Recognition from Physiological Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09013">http://arxiv.org/abs/2308.09013</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antoine Dubois, Carlos Lima Azevedo, Sonja Haustein, Bruno Miranda</li>
<li>for: 这篇论文主要是关于情感认知的研究，旨在提出一种基于物理和心理数据的无监督深度划分方法来实现情感认知。</li>
<li>methods: 该方法使用了深度划分算法，包括深度k-means和深度c-means，并在测试数据集WESAD上实现了87%的总准确率。</li>
<li>results: 该研究表明，通过使用物理和心理数据，并使用无监督深度划分方法，可以实现高度准确的情感认知，并且可以避免需要大量的标签数据。<details>
<summary>Abstract</summary>
Emotions play a significant role in the cognitive processes of the human brain, such as decision making, learning and perception. The use of physiological signals has shown to lead to more objective, reliable and accurate emotion recognition combined with raising machine learning methods. Supervised learning methods have dominated the attention of the research community, but the challenge in collecting needed labels makes emotion recognition difficult in large-scale semi- or uncontrolled experiments. Unsupervised methods are increasingly being explored, however sub-optimal signal feature selection and label identification challenges unsupervised methods' accuracy and applicability. This article proposes an unsupervised deep cluster framework for emotion recognition from physiological and psychological data. Tests on the open benchmark data set WESAD show that deep k-means and deep c-means distinguish the four quadrants of Russell's circumplex model of affect with an overall accuracy of 87%. Seeding the clusters with the subject's subjective assessments helps to circumvent the need for labels.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:情感对人类大脑的认知过程具有重要作用，如决策、学习和感知。通过使用生物学信号，可以实现更加客观、可靠和准确的情感识别，并且与机器学习方法相结合。但是，在大规模的半控制或无控制实验中收集标签是困难的，因此许多研究者对supervised learning方法进行了探索。然而，不同信号特征选择和标签标识问题限制了无监督方法的准确性和可应用性。这篇文章提出了一种无监督深度团 clustering框架，用于从生物学和心理学数据中进行情感识别。在WESAD开放数据集上进行测试，深度k-means和深度c-means能够分解Russell的情感圆框模型中的四个 quadrant，总准确率达87%。通过使用参与者的主观评估来填充团中的标签，可以避免标签收集的困难。
</details></li>
</ul>
<hr>
<h2 id="Towards-Lightweight-Data-Integration-using-Multi-workflow-Provenance-and-Data-Observability"><a href="#Towards-Lightweight-Data-Integration-using-Multi-workflow-Provenance-and-Data-Observability" class="headerlink" title="Towards Lightweight Data Integration using Multi-workflow Provenance and Data Observability"></a>Towards Lightweight Data Integration using Multi-workflow Provenance and Data Observability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09004">http://arxiv.org/abs/2308.09004</a></li>
<li>repo_url: None</li>
<li>paper_authors: Renan Souza, Tyler J. Skluzacek, Sean R. Wilkinson, Maxim Ziatdinov, Rafael Ferreira da Silva</li>
<li>for: 科学发现的大规模合作和数据分析</li>
<li>methods: 基于数据可见性、适配器系统设计和证据的轻量级运行多工程数据分析方法</li>
<li>results: 实现了灵活的多工程数据分析，可以在多种并行环境中运行，并且在 Summit 超级计算机上实现了near-zero overhead，可以处理大量任务。<details>
<summary>Abstract</summary>
Modern large-scale scientific discovery requires multidisciplinary collaboration across diverse computing facilities, including High Performance Computing (HPC) machines and the Edge-to-Cloud continuum. Integrated data analysis plays a crucial role in scientific discovery, especially in the current AI era, by enabling Responsible AI development, FAIR, Reproducibility, and User Steering. However, the heterogeneous nature of science poses challenges such as dealing with multiple supporting tools, cross-facility environments, and efficient HPC execution. Building on data observability, adapter system design, and provenance, we propose MIDA: an approach for lightweight runtime Multi-workflow Integrated Data Analysis. MIDA defines data observability strategies and adaptability methods for various parallel systems and machine learning tools. With observability, it intercepts the dataflows in the background without requiring instrumentation while integrating domain, provenance, and telemetry data at runtime into a unified database ready for user steering queries. We conduct experiments showing end-to-end multi-workflow analysis integrating data from Dask and MLFlow in a real distributed deep learning use case for materials science that runs on multiple environments with up to 276 GPUs in parallel. We show near-zero overhead running up to 100,000 tasks on 1,680 CPU cores on the Summit supercomputer.
</details>
<details>
<summary>摘要</summary>
现代大规模科学发现需要跨学科合作和多种计算机facility的支持，包括高性能计算机（HPC）机器和Edge-to-Cloud kontinuum。集成数据分析在科学发现中扮演着关键的角色，特别是在当前人工智能时代，通过帮助开发负责任AI，FAIR，可重现和用户指导。然而，科学的多元性带来了多种支持工具、跨设施环境和高效HPC执行的挑战。基于数据可见性，适配器系统设计和证明，我们提出MIDA：一种轻量级运行时多工流Integrated Data Analysis的方法。MIDA定义了数据可见性策略和适配方法，用于不同的并行系统和机器学习工具。通过可见性，它在背景中 intercepts 数据流 ohne requiring  инструментирование，并将domain、证明和电信数据在运行时集成到一个统一的数据库中，准备就绪 для用户指导查询。我们进行实验，将多工流分析集成到了材料科学中的分布式深度学习应用程序中，运行在多种环境上，包括最多276个GPU并行。我们显示了near-zero overhead，在1,680个CPU核心上运行Up to 100,000个任务。
</details></li>
</ul>
<hr>
<h2 id="DealMVC-Dual-Contrastive-Calibration-for-Multi-view-Clustering"><a href="#DealMVC-Dual-Contrastive-Calibration-for-Multi-view-Clustering" class="headerlink" title="DealMVC: Dual Contrastive Calibration for Multi-view Clustering"></a>DealMVC: Dual Contrastive Calibration for Multi-view Clustering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09000">http://arxiv.org/abs/2308.09000</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xihongyang1999/dealmvc">https://github.com/xihongyang1999/dealmvc</a></li>
<li>paper_authors: Xihong Yang, Jiaqi Jin, Siwei Wang, Ke Liang, Yue Liu, Yi Wen, Suyuan Liu, Sihang Zhou, Xinwang Liu, En Zhu</li>
<li>for: 解决多视图 clustering 中同样 pero different samples 的问题，提高 clustering 性能。</li>
<li>methods: 提出了一种 dual contrastive calibration network (DealMVC)，包括 global contrastive calibration loss 和 local contrastive calibration loss，以及 feature structure 的 regularization。</li>
<li>results: 与其他 state-of-the-art 方法进行比较，实验结果表明 DealMVC 的效果和优势较高，可以提高 clustering 性能。<details>
<summary>Abstract</summary>
Benefiting from the strong view-consistent information mining capacity, multi-view contrastive clustering has attracted plenty of attention in recent years. However, we observe the following drawback, which limits the clustering performance from further improvement. The existing multi-view models mainly focus on the consistency of the same samples in different views while ignoring the circumstance of similar but different samples in cross-view scenarios. To solve this problem, we propose a novel Dual contrastive calibration network for Multi-View Clustering (DealMVC). Specifically, we first design a fusion mechanism to obtain a global cross-view feature. Then, a global contrastive calibration loss is proposed by aligning the view feature similarity graph and the high-confidence pseudo-label graph. Moreover, to utilize the diversity of multi-view information, we propose a local contrastive calibration loss to constrain the consistency of pair-wise view features. The feature structure is regularized by reliable class information, thus guaranteeing similar samples have similar features in different views. During the training procedure, the interacted cross-view feature is jointly optimized at both local and global levels. In comparison with other state-of-the-art approaches, the comprehensive experimental results obtained from eight benchmark datasets provide substantial validation of the effectiveness and superiority of our algorithm. We release the code of DealMVC at https://github.com/xihongyang1999/DealMVC on GitHub.
</details>
<details>
<summary>摘要</summary>
利用强大的视图一致信息挖掘能力，多视图对比 clustering 在最近几年内吸引了大量的注意力。然而，我们发现现有的多视图模型主要关注不同视图中的同样样本之间的一致性，而忽略了不同视图中的相似 yet 不同样本之间的关系。为解决这个问题，我们提出了一种新的 dual contrastive calibration network for Multi-View Clustering（DealMVC）。具体来说，我们首先设计了一种 fusions 机制，以获得全局跨视图特征。然后，我们提出了一种全局对比准备损失，通过对视图特征相似图和高置信度假标签图进行对比，使得相似的样本在不同视图中具有相似的特征。此外，为了利用多视图信息的多样性，我们提出了一种本地对比准备损失，以强制不同视图中的对应样本之间的一致性。特征结构被可靠的类信息规范化，以保证不同视图中的相似样本具有相似的特征。在训练过程中，交互的跨视图特征被在本地和全局两级进行优化。与其他状态 искус法比较，我们从八个标准 benchmark 数据集获得了广泛的实验结果，这些结果证明了我们的算法的有效性和优越性。我们在 GitHub 上发布了 DealMVC 的代码，请参考 <https://github.com/xihongyang1999/DealMVC>。
</details></li>
</ul>
<hr>
<h2 id="Reinforced-Self-Training-ReST-for-Language-Modeling"><a href="#Reinforced-Self-Training-ReST-for-Language-Modeling" class="headerlink" title="Reinforced Self-Training (ReST) for Language Modeling"></a>Reinforced Self-Training (ReST) for Language Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08998">http://arxiv.org/abs/2308.08998</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, Wolfgang Macherey, Arnaud Doucet, Orhan Firat, Nando de Freitas</li>
<li>for: 提高大语言模型的输出质量（machine translation）</li>
<li>methods: 使用激励自学习（Reinforced Self-Training，ReST）算法，通过在初始语言模型策略基础上生成样本，然后使用离线激励学习算法进行改进</li>
<li>results: substantially improve translation quality, as measured by automated metrics and human evaluation on machine translation benchmarks in a compute and sample-efficient manner.<details>
<summary>Abstract</summary>
Reinforcement learning from human feedback (RLHF) can improve the quality of large language model's (LLM) outputs by aligning them with human preferences. We propose a simple algorithm for aligning LLMs with human preferences inspired by growing batch reinforcement learning (RL), which we call Reinforced Self-Training (ReST). Given an initial LLM policy, ReST produces a dataset by generating samples from the policy, which are then used to improve the LLM policy using offline RL algorithms. ReST is more efficient than typical online RLHF methods because the training dataset is produced offline, which allows data reuse. While ReST is a general approach applicable to all generative learning settings, we focus on its application to machine translation. Our results show that ReST can substantially improve translation quality, as measured by automated metrics and human evaluation on machine translation benchmarks in a compute and sample-efficient manner.
</details>
<details>
<summary>摘要</summary>
人工反馈学习（RLHF）可以提高大语言模型（LLM）的输出质量，通过将其与人类喜好进行对齐。我们提出了一种简单的算法，即增强自我训练（ReST），以提高 LLM 政策。给定初始 LLM 策略，ReST 会生成一个样本集，然后使用在线 RL 算法来改善 LLM 策略。相比于 typical online RLHF 方法，ReST 更加高效，因为它可以在线下进行训练，从而实现数据重用。虽然 ReST 是一种通用的措施，但我们在机器翻译中进行了应用。我们的结果显示，ReST 可以在计算和样本效率下提高翻译质量，并且通过人类评估得到证明。
</details></li>
</ul>
<hr>
<h2 id="Learning-representations-by-forward-propagating-errors"><a href="#Learning-representations-by-forward-propagating-errors" class="headerlink" title="Learning representations by forward-propagating errors"></a>Learning representations by forward-propagating errors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09728">http://arxiv.org/abs/2308.09728</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryoungwoo Jang</li>
<li>for: 这个论文目的是提出一种轻量级、快速的学习算法，用于在中央处理单元（CPU）上优化神经网络。</li>
<li>methods: 这种算法基于前向传播方法，使用了代数几何中的双数概念。</li>
<li>results: 该算法比 tradicional back-propagation（BP）算法更快速，可以在CPU上进行神经网络优化。<details>
<summary>Abstract</summary>
Back-propagation (BP) is widely used learning algorithm for neural network optimization. However, BP requires enormous computation cost and is too slow to train in central processing unit (CPU). Therefore current neural network optimizaiton is performed in graphical processing unit (GPU) with compute unified device architecture (CUDA) programming. In this paper, we propose a light, fast learning algorithm on CPU that is fast as CUDA acceleration on GPU. This algorithm is based on forward-propagating method, using concept of dual number in algebraic geometry.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "Back-propagation (BP) is widely used learning algorithm for neural network optimization. However, BP requires enormous computation cost and is too slow to train in central processing unit (CPU). Therefore current neural network optimizaiton is performed in graphical processing unit (GPU) with compute unified device architecture (CUDA) programming. In this paper, we propose a light, fast learning algorithm on CPU that is fast as CUDA acceleration on GPU. This algorithm is based on forward-propagating method, using concept of dual number in algebraic geometry." into Simplified Chinese.翻译文本为Simplified Chinese：Back-propagation（BP）是广泛使用的神经网络优化算法。然而，BP需要巨大的计算成本，并且在中央处理单元（CPU）中训练太慢。因此，当前的神经网络优化通常在图形处理单元（GPU）上使用compute unified device architecture（CUDA）编程进行。在这篇论文中，我们提出了一种轻量级、快速的学习算法，在CPU上实现，与GPU上CUDA加速相同快速。这种算法基于前向传播方法，利用了代数几何中的双数概念。
</details></li>
</ul>
<hr>
<h2 id="Neural-oscillators-for-generalization-of-physics-informed-machine-learning"><a href="#Neural-oscillators-for-generalization-of-physics-informed-machine-learning" class="headerlink" title="Neural oscillators for generalization of physics-informed machine learning"></a>Neural oscillators for generalization of physics-informed machine learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08989">http://arxiv.org/abs/2308.08989</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taniya Kapoor, Abhishek Chandra, Daniel M. Tartakovsky, Hongrui Wang, Alfredo Nunez, Rolf Dollevoet</li>
<li>for: 提高物理学 Informed 机器学习（PIML）的泛化能力，特别是在面临复杂物理问题时。</li>
<li>methods: 利用 PDE 解的内在 causality 和时间序列特征，将 PIML 模型与回归神经网络结合，基于系数 ordinary differential equations 的神经抗 oscilators。</li>
<li>results: 通过有效地捕捉长时间依赖和缓解扩散和消失Gradient问题，神经抗 oscilators 提高 PIML 模型的泛化能力，在时间依赖非线性 PDE 和 biharmonic beam 方程上进行了广泛的实验，并证明了该方法的有效性。<details>
<summary>Abstract</summary>
A primary challenge of physics-informed machine learning (PIML) is its generalization beyond the training domain, especially when dealing with complex physical problems represented by partial differential equations (PDEs). This paper aims to enhance the generalization capabilities of PIML, facilitating practical, real-world applications where accurate predictions in unexplored regions are crucial. We leverage the inherent causality and temporal sequential characteristics of PDE solutions to fuse PIML models with recurrent neural architectures based on systems of ordinary differential equations, referred to as neural oscillators. Through effectively capturing long-time dependencies and mitigating the exploding and vanishing gradient problem, neural oscillators foster improved generalization in PIML tasks. Extensive experimentation involving time-dependent nonlinear PDEs and biharmonic beam equations demonstrates the efficacy of the proposed approach. Incorporating neural oscillators outperforms existing state-of-the-art methods on benchmark problems across various metrics. Consequently, the proposed method improves the generalization capabilities of PIML, providing accurate solutions for extrapolation and prediction beyond the training data.
</details>
<details>
<summary>摘要</summary>
physics-informed machine learning (PIML) 的主要挑战之一是其泛化性，特别是在处理复杂的物理问题时。这篇论文的目的是增强 PIML 的泛化能力，以便在实际应用中做出准确的预测，特别是在训练数据外的未探索区域。我们利用 PDE 解的内在 causality 和时间序列特征来融合 PIML 模型和回归神经网络，称为神经振荡器。通过有效地捕捉长期依赖关系和 Mitigate 爆炸和消失梯度问题，神经振荡器 提高了 PIML 任务中的泛化能力。经过大量的实验，我们发现在时间不断改变的非线性 PDE 和二次杠杆方程中，包含神经振荡器 的方法可以更高效地解决问题，并且在不同的 метриках上都超过了现有的状态ünstler 方法。因此，我们的方法可以提高 PIML 的泛化能力，为 extrapolation 和预测 beyond 训练数据提供准确的解决方案。
</details></li>
</ul>
<hr>
<h2 id="Quantifying-the-biomimicry-gap-in-biohybrid-systems"><a href="#Quantifying-the-biomimicry-gap-in-biohybrid-systems" class="headerlink" title="Quantifying the biomimicry gap in biohybrid systems"></a>Quantifying the biomimicry gap in biohybrid systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08978">http://arxiv.org/abs/2308.08978</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vaios Papaspyros, Guy Theraulaz, Clément Sire, Francesco Mondada<br>for: 这个论文的目的是用生物hybrid系统来探索和识别动物群体行为的机制。methods: 这篇论文使用了生物寄生的鱼类模型和神经网络模型来生成生物寄生的社交互动。results: 这篇论文通过实验和模拟来证明，使用生物寄生的鱼类模型和神经网络模型可以生成高精度的社交互动，与真实的鱼类群体行为高度相似。<details>
<summary>Abstract</summary>
Biohybrid systems in which robotic lures interact with animals have become compelling tools for probing and identifying the mechanisms underlying collective animal behavior. One key challenge lies in the transfer of social interaction models from simulations to reality, using robotics to validate the modeling hypotheses. This challenge arises in bridging what we term the "biomimicry gap", which is caused by imperfect robotic replicas, communication cues and physics constrains not incorporated in the simulations that may elicit unrealistic behavioral responses in animals. In this work, we used a biomimetic lure of a rummy-nose tetra fish (Hemigrammus rhodostomus) and a neural network (NN) model for generating biomimetic social interactions. Through experiments with a biohybrid pair comprising a fish and the robotic lure, a pair of real fish, and simulations of pairs of fish, we demonstrate that our biohybrid system generates high-fidelity social interactions mirroring those of genuine fish pairs. Our analyses highlight that: 1) the lure and NN maintain minimal deviation in real-world interactions compared to simulations and fish-only experiments, 2) our NN controls the robot efficiently in real-time, and 3) a comprehensive validation is crucial to bridge the biomimicry gap, ensuring realistic biohybrid systems.
</details>
<details>
<summary>摘要</summary>
生物融合系统，在其中机器人饵料与动物互动，已成为诱导和识别集体动物行为的有力工具。一个关键挑战在于将社交互动模型从 simulations 转移到实际情况，使用机器人来验证模型假设。这个挑战 arise 由我们称为 "生物模仿差距"，这是因为机器人的复制不准确、通信提示和物理约束不包括在 simulations 中，可能会诱导动物的不实际行为响应。在这项工作中，我们使用了一个生物模仿的鲤鱼鱼饵（Hemigrammus rhodostomus）和一个神经网络（NN）模型来生成生物模仿社交互动。通过实验中的生物融合对，一个鱼和机器人饵料对，一对真正的鱼对和 simulations 中的鱼对，我们示出了我们的生物融合系统可以生成高准确性的社交互动，与真正的鱼对相似。我们的分析表明：1）饵料和 NN 在实际情况中具有最小偏差，与 simulations 和鱼只实验相比; 2）我们的 NN 在实时控制机器人; 3）全面验证是必要的，以bridging "生物模仿差距"，确保生物融合系统的真实性。
</details></li>
</ul>
<hr>
<h2 id="Hitting-the-High-Dimensional-Notes-An-ODE-for-SGD-learning-dynamics-on-GLMs-and-multi-index-models"><a href="#Hitting-the-High-Dimensional-Notes-An-ODE-for-SGD-learning-dynamics-on-GLMs-and-multi-index-models" class="headerlink" title="Hitting the High-Dimensional Notes: An ODE for SGD learning dynamics on GLMs and multi-index models"></a>Hitting the High-Dimensional Notes: An ODE for SGD learning dynamics on GLMs and multi-index models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08977">http://arxiv.org/abs/2308.08977</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elizabeth Collins-Woodfin, Courtney Paquette, Elliot Paquette, Inbar Seroussi</li>
<li>for: 这 paper 是研究 streaming stochastic gradient descent (SGD) 在高维限制下的动态特性，具体来说是在通用线性模型和多指量模型（例如Logistic regression、phase retrieval）中应用 SGD，并研究其在大数据量下的性能。</li>
<li>methods: 这 paper 使用了一种系统Ordinary differential equations来描述 SGD 的动态行为，这种方法可以涵盖许多统计量，如风险和优化度量。此外，paper 还引入了一种简化的卷积 coefficient的 SDE（homogenized SGD），以便分析 SGD 迭代器的动态行为。</li>
<li>results: 这 paper 的结果表明，当模型参数个数与数据量成正比时，SGD 可以被视为一种确定性的方法，其可以提供风险和优化度量的确定性 guarantees。此外，paper 还提供了一些标准示例的数据分析，并与理论结果相符。<details>
<summary>Abstract</summary>
We analyze the dynamics of streaming stochastic gradient descent (SGD) in the high-dimensional limit when applied to generalized linear models and multi-index models (e.g. logistic regression, phase retrieval) with general data-covariance. In particular, we demonstrate a deterministic equivalent of SGD in the form of a system of ordinary differential equations that describes a wide class of statistics, such as the risk and other measures of sub-optimality. This equivalence holds with overwhelming probability when the model parameter count grows proportionally to the number of data. This framework allows us to obtain learning rate thresholds for stability of SGD as well as convergence guarantees. In addition to the deterministic equivalent, we introduce an SDE with a simplified diffusion coefficient (homogenized SGD) which allows us to analyze the dynamics of general statistics of SGD iterates. Finally, we illustrate this theory on some standard examples and show numerical simulations which give an excellent match to the theory.
</details>
<details>
<summary>摘要</summary>
我们分析流动式随机Gradient Descent（SGD）在高维限制下的动态行为，尤其是在泛化线性模型和多指标模型（例如逻辑回归、相位恢复）中。我们展示了一个确定的SGD等价项，它描述了一个广泛的统计量，例如风险和其他不足之数据。这个等价项在资料数量增加时，对数据的尺度成长时，具有极高的概率。这个框架允许我们获得SGD的学习率阈值以及稳定性的保证。此外，我们引入了一个简化的扩散系数（殷合SGD），它允许我们分析SGD迭代的一般统计。最后，我们在一些标准的例子中详细介绍了这个理论，并提供了一些实际的实验结果，与理论内容匹配得极佳。
</details></li>
</ul>
<hr>
<h2 id="Cross-city-Few-Shot-Traffic-Forecasting-via-Traffic-Pattern-Bank"><a href="#Cross-city-Few-Shot-Traffic-Forecasting-via-Traffic-Pattern-Bank" class="headerlink" title="Cross-city Few-Shot Traffic Forecasting via Traffic Pattern Bank"></a>Cross-city Few-Shot Traffic Forecasting via Traffic Pattern Bank</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09727">http://arxiv.org/abs/2308.09727</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhyliu00/tpb">https://github.com/zhyliu00/tpb</a></li>
<li>paper_authors: Zhanyu Liu, Guanjie Zheng, Yanwei Yu</li>
<li>for: 提高智能交通系统中的交通预测精度，尤其是在数据穷市场地区。</li>
<li>methods: 利用交通模式银行（TPB），通过预训练的交通补充编码器将数据丰富城市的交通数据 proyect到高维空间，然后根据协调分区生成交通模式银行。</li>
<li>results: 在实际交通数据集上进行实验，表明TPB在cross-city几个shot交通预测中表现出色，超越现有方法，证明我们的方法在数据穷市场地区的交通预测中具有效果。<details>
<summary>Abstract</summary>
Traffic forecasting is a critical service in Intelligent Transportation Systems (ITS). Utilizing deep models to tackle this task relies heavily on data from traffic sensors or vehicle devices, while some cities might lack device support and thus have few available data. So, it is necessary to learn from data-rich cities and transfer the knowledge to data-scarce cities in order to improve the performance of traffic forecasting. To address this problem, we propose a cross-city few-shot traffic forecasting framework via Traffic Pattern Bank (TPB) due to that the traffic patterns are similar across cities. TPB utilizes a pre-trained traffic patch encoder to project raw traffic data from data-rich cities into high-dimensional space, from which a traffic pattern bank is generated through clustering. Then, the traffic data of the data-scarce city could query the traffic pattern bank and explicit relations between them are constructed. The metaknowledge is aggregated based on these relations and an adjacency matrix is constructed to guide a downstream spatial-temporal model in forecasting future traffic. The frequently used meta-training framework Reptile is adapted to find a better initial parameter for the learnable modules. Experiments on real-world traffic datasets show that TPB outperforms existing methods and demonstrates the effectiveness of our approach in cross-city few-shot traffic forecasting.
</details>
<details>
<summary>摘要</summary>
traffic 预测是智能交通系统（ITS）中的关键服务。使用深度模型来解决这个任务需要依赖于交通感知器或车辆设备上的数据，而一些城市可能缺乏设备支持，因此有限的数据。因此，我们需要从数据丰富的城市学习并传递知识到数据缺乏的城市，以改善交通预测性能。为解决这个问题，我们提出了跨城市几拟交通预测框架via Traffic Pattern Bank（TPB）。TPB利用预训练的交通补充器来将数据丰富城市的 raw 交通数据 proyect到高维空间中，从而生成交通模式银行。然后，数据缺乏城市的交通数据可以在交通模式银行中查询，并构建了明确的交通模式之间的关系。这些关系的总体知识被聚合，并构建了一个导航下游空间时间模型的优化矩阵。我们采用了现有的meta-training框架Reptile来找到更好的初始参数。实验表明，TPB比现有方法更高效，并证明了我们的方法在跨城市几拟交通预测中的效果。
</details></li>
</ul>
<hr>
<h2 id="CONVERT-Contrastive-Graph-Clustering-with-Reliable-Augmentation"><a href="#CONVERT-Contrastive-Graph-Clustering-with-Reliable-Augmentation" class="headerlink" title="CONVERT:Contrastive Graph Clustering with Reliable Augmentation"></a>CONVERT:Contrastive Graph Clustering with Reliable Augmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08963">http://arxiv.org/abs/2308.08963</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xihongyang1999/convert">https://github.com/xihongyang1999/convert</a></li>
<li>paper_authors: Xihong Yang, Cheng Tan, Yue Liu, Ke Liang, Siwei Wang, Sihang Zhou, Jun Xia, Stan Z. Li, Xinwang Liu, En Zhu</li>
<li>for: 提高不监督图像学习中的图像自动生成技术的可靠性和效果。</li>
<li>methods: 提出了一种名为COVERT的新网络模型，该模型通过一种叫做征识扰动恢复网络的方式，对数据生成器进行了可靠性提高和semantic信息捕捉。此外，还提出了一种新的semantic损失函数，用于约束网络的学习。</li>
<li>results: 经过广泛的实验研究，该方法在七个数据集上得到了极高的效果，超过了现有的方法。code和补充文件也在github上公开发布。<details>
<summary>Abstract</summary>
Contrastive graph node clustering via learnable data augmentation is a hot research spot in the field of unsupervised graph learning. The existing methods learn the sampling distribution of a pre-defined augmentation to generate data-driven augmentations automatically. Although promising clustering performance has been achieved, we observe that these strategies still rely on pre-defined augmentations, the semantics of the augmented graph can easily drift. The reliability of the augmented view semantics for contrastive learning can not be guaranteed, thus limiting the model performance. To address these problems, we propose a novel CONtrastiVe Graph ClustEring network with Reliable AugmenTation (COVERT). Specifically, in our method, the data augmentations are processed by the proposed reversible perturb-recover network. It distills reliable semantic information by recovering the perturbed latent embeddings. Moreover, to further guarantee the reliability of semantics, a novel semantic loss is presented to constrain the network via quantifying the perturbation and recovery. Lastly, a label-matching mechanism is designed to guide the model by clustering information through aligning the semantic labels and the selected high-confidence clustering pseudo labels. Extensive experimental results on seven datasets demonstrate the effectiveness of the proposed method. We release the code and appendix of CONVERT at https://github.com/xihongyang1999/CONVERT on GitHub.
</details>
<details>
<summary>摘要</summary>
“对照性图节点聚合via学习数据增强是现场无监督图学中的热点研究领域。现有方法通过自动生成数据驱动增强来学习增强分布。虽然这些策略已经实现了有前途的聚合性能，但我们发现这些策略仍然依赖于预定的增强，即使在增强后，图的 semantics 容易偏移。因此，我们提出了一种名为 CONtrastiVe Graph ClustEring network with Reliable AugmenTation（COVERT）的新方法。具体来说，我们的方法通过我们提出的可逆压抽网络进行数据增强。这个网络可以通过压抽并重建原始 embedding 来提取可靠的semantic信息。此外，为了进一步保证 semantics 的可靠性，我们提出了一种新的semantic损失函数，该函数通过量化压抽和重建来约束网络。最后，我们设计了一种标签匹配机制，通过对semantic标签和选择高置信聚合 Pseudolabels 进行对应，以导引模型。我们的实验结果表明，我们的方法能够有效地进行图聚合。我们在 GitHub 上发布了代码和补充材料，详细的实验结果和方法详细介绍可以参考我们的 GitHub 上的文章。”
</details></li>
</ul>
<hr>
<h2 id="Equitable-Restless-Multi-Armed-Bandits-A-General-Framework-Inspired-By-Digital-Health"><a href="#Equitable-Restless-Multi-Armed-Bandits-A-General-Framework-Inspired-By-Digital-Health" class="headerlink" title="Equitable Restless Multi-Armed Bandits: A General Framework Inspired By Digital Health"></a>Equitable Restless Multi-Armed Bandits: A General Framework Inspired By Digital Health</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09726">http://arxiv.org/abs/2308.09726</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/google-research/socialgood">https://github.com/google-research/socialgood</a></li>
<li>paper_authors: Jackson A. Killian, Manish Jain, Yugang Jia, Jonathan Amar, Erich Huang, Milind Tambe</li>
<li>for: 这篇论文旨在研究多重臂摆（RMAB）的公平目标（ERMAB），以提高决策的公平性和健康等级。</li>
<li>methods: 这篇论文使用了两种公平目标，即最小最大奖励和最大奈雪威夷。它们分别使用了一种灌水算法和一种理论上 inspirited 的落叶算法来实现。</li>
<li>results: 研究在三个模拟领域中，包括一个新的数字医疗模型，发现了其方法可以比现有技术多少 times 更公平，而无需损害效用。这些结果认可了这种研究的urgency，因为RMAB在人类和野生动物的结果中广泛应用。<details>
<summary>Abstract</summary>
Restless multi-armed bandits (RMABs) are a popular framework for algorithmic decision making in sequential settings with limited resources. RMABs are increasingly being used for sensitive decisions such as in public health, treatment scheduling, anti-poaching, and -- the motivation for this work -- digital health. For such high stakes settings, decisions must both improve outcomes and prevent disparities between groups (e.g., ensure health equity). We study equitable objectives for RMABs (ERMABs) for the first time. We consider two equity-aligned objectives from the fairness literature, minimax reward and max Nash welfare. We develop efficient algorithms for solving each -- a water filling algorithm for the former, and a greedy algorithm with theoretically motivated nuance to balance disparate group sizes for the latter. Finally, we demonstrate across three simulation domains, including a new digital health model, that our approaches can be multiple times more equitable than the current state of the art without drastic sacrifices to utility. Our findings underscore our work's urgency as RMABs permeate into systems that impact human and wildlife outcomes. Code is available at https://github.com/google-research/socialgood/tree/equitable-rmab
</details>
<details>
<summary>摘要</summary>
众臂猎手（RMAB）是一种流行的算法决策框架，广泛应用于顺序设置中的有限资源管理。众臂猎手在公共卫生、治疗安排、反贪杀和数字医疗等高规模场景中得到应用，决策必须同时提高结果和避免群体之间的差距（如保障健康公平）。我们研究了众臂猎手的公平目标（ERMAB），并考虑了两种与公平相关的目标，即最小最大奖励和最大 NASH 利益。我们开发了高效的算法来解决每一个目标，包括水填算法和基于理论的柔和策略来平衡不同群体大小的算法。最后，我们在三个 simulations 频道中，包括一个新的数字医疗模型，证明了我们的方法可以在无损Utility 的情况下多达多少倍提高公平性。我们的发现强调了我们的工作的急迫性，因为众臂猎手在影响人类和野生动物的系统中普遍应用。代码可以在 <https://github.com/google-research/socialgood/tree/equitable-rmab> 中获取。
</details></li>
</ul>
<hr>
<h2 id="A-Dual-Perspective-Approach-to-Evaluating-Feature-Attribution-Methods"><a href="#A-Dual-Perspective-Approach-to-Evaluating-Feature-Attribution-Methods" class="headerlink" title="A Dual-Perspective Approach to Evaluating Feature Attribution Methods"></a>A Dual-Perspective Approach to Evaluating Feature Attribution Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08949">http://arxiv.org/abs/2308.08949</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yawei Li, Yang Zhang, Kenji Kawaguchi, Ashkan Khakzar, Bernd Bischl, Mina Rezaei</li>
<li>for: 本研究旨在提供一个新的评估feature attribution方法的框架，以及两种新的评估视角，即实用性（soundness）和完整性（completeness）。</li>
<li>methods: 本研究使用了现有的feature attribution方法，并提出了两种新的评估方法，即实用性评估和完整性评估。这两种方法都基于固定的数学基础，并可以通过高效的算法来计算。</li>
<li>results: 本研究通过应用这两种新的评估方法，对主流的feature attribution方法进行了评估。结果表明，这两种方法可以提供一个新的视角来分析和比较feature attribution方法的效果。<details>
<summary>Abstract</summary>
Feature attribution methods attempt to explain neural network predictions by identifying relevant features. However, establishing a cohesive framework for assessing feature attribution remains a challenge. There are several views through which we can evaluate attributions. One principal lens is to observe the effect of perturbing attributed features on the model's behavior (i.e., faithfulness). While providing useful insights, existing faithfulness evaluations suffer from shortcomings that we reveal in this paper. In this work, we propose two new perspectives within the faithfulness paradigm that reveal intuitive properties: soundness and completeness. Soundness assesses the degree to which attributed features are truly predictive features, while completeness examines how well the resulting attribution reveals all the predictive features. The two perspectives are based on a firm mathematical foundation and provide quantitative metrics that are computable through efficient algorithms. We apply these metrics to mainstream attribution methods, offering a novel lens through which to analyze and compare feature attribution methods.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate "Feature attribution methods attempt to explain neural network predictions by identifying relevant features. However, establishing a cohesive framework for assessing feature attribution remains a challenge. There are several views through which we can evaluate attributions. One principal lens is to observe the effect of perturbing attributed features on the model's behavior (i.e., faithfulness). While providing useful insights, existing faithfulness evaluations suffer from shortcomings that we reveal in this paper. In this work, we propose two new perspectives within the faithfulness paradigm that reveal intuitive properties: soundness and completeness. Soundness assesses the degree to which attributed features are truly predictive features, while completeness examines how well the resulting attribution reveals all the predictive features. The two perspectives are based on a firm mathematical foundation and provide quantitative metrics that are computable through efficient algorithms. We apply these metrics to mainstream attribution methods, offering a novel lens through which to analyze and compare feature attribution methods."into Simplified Chinese.Feature 归属方法 attempt to explain neural network 预测结果 by identifying relevant features. However, establishing a cohesive framework for assessing feature 归属 remains a challenge. There are several views through which we can evaluate attributions. One principal lens is to observe the effect of perturbing attributed features on the model's behavior (i.e., faithfulness). While providing useful insights, existing faithfulness evaluations suffer from shortcomings that we reveal in this paper. In this work, we propose two new perspectives within the faithfulness paradigm that reveal intuitive properties: soundness and completeness. Soundness assesses the degree to which attributed features are truly predictive features, while completeness examines how well the resulting attribution reveals all the predictive features. The two perspectives are based on a firm mathematical foundation and provide quantitative metrics that are computable through efficient algorithms. We apply these metrics to mainstream attribution methods, offering a novel lens through which to analyze and compare feature attribution methods.Note: Please note that the translation is in Simplified Chinese, which is one of the two standard versions of Chinese. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Predicting-Crop-Yield-With-Machine-Learning-An-Extensive-Analysis-Of-Input-Modalities-And-Models-On-a-Field-and-sub-field-Level"><a href="#Predicting-Crop-Yield-With-Machine-Learning-An-Extensive-Analysis-Of-Input-Modalities-And-Models-On-a-Field-and-sub-field-Level" class="headerlink" title="Predicting Crop Yield With Machine Learning: An Extensive Analysis Of Input Modalities And Models On a Field and sub-field Level"></a>Predicting Crop Yield With Machine Learning: An Extensive Analysis Of Input Modalities And Models On a Field and sub-field Level</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08948">http://arxiv.org/abs/2308.08948</a></li>
<li>repo_url: None</li>
<li>paper_authors: Deepak Pathak, Miro Miranda, Francisco Mena, Cristhian Sanchez, Patrick Helber, Benjamin Bischke, Peter Habelitz, Hiba Najjar, Jayanth Siddamsetty, Diego Arenas, Michaela Vollmer, Marcela Charfuelan, Marlon Nuske, Andreas Dengel</li>
<li>for: 这个论文是为了预测农业产量而写的。</li>
<li>methods: 这个论文使用了一种简单 yet effective的早期融合方法，该方法可以处理多种输入模式，并且可以在不同的时间和空间分辨率下工作。</li>
<li>results: 这个论文使用了高分辨率农业产量地图作为真实数据来训练农作物和机器学习模型，并且使用了全球覆盖的卫星图像作为主要输入数据，以及其他补充的模式，如天气、土壤和地形数据。<details>
<summary>Abstract</summary>
We introduce a simple yet effective early fusion method for crop yield prediction that handles multiple input modalities with different temporal and spatial resolutions. We use high-resolution crop yield maps as ground truth data to train crop and machine learning model agnostic methods at the sub-field level. We use Sentinel-2 satellite imagery as the primary modality for input data with other complementary modalities, including weather, soil, and DEM data. The proposed method uses input modalities available with global coverage, making the framework globally scalable. We explicitly highlight the importance of input modalities for crop yield prediction and emphasize that the best-performing combination of input modalities depends on region, crop, and chosen model.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种简单 yet 有效的早期融合方法 для农作物收成预测，该方法可以处理多种输入模式，每种模式具有不同的时空分解能力。我们使用高分辨率农作物收成地图作为真实数据来训练农作物和机器学习模型无关的方法。我们使用卫星影像作为主要输入数据，其他补充数据包括天气、土壤和地形数据。我们的方法使用全球覆盖的输入数据，因此该框架可以在全球范围内扩展。我们显式强调输入数据对农作物收成预测的重要性，并且指出在不同的区域、作物和选择的模型中，最佳的输入数据组合会有所不同。
</details></li>
</ul>
<hr>
<h2 id="Interpretable-Graph-Neural-Networks-for-Tabular-Data"><a href="#Interpretable-Graph-Neural-Networks-for-Tabular-Data" class="headerlink" title="Interpretable Graph Neural Networks for Tabular Data"></a>Interpretable Graph Neural Networks for Tabular Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08945">http://arxiv.org/abs/2308.08945</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amr Alkhatib, Sofiane Ennadir, Henrik Boström, Michalis Vazirgiannis</li>
<li>for: 这种论文是为了处理实际应用中频繁出现的表格数据而设计的，使用图 neural network 扩展来捕捉特征间的交互。</li>
<li>methods: 这种方法使用了一种名为 IGNNet 的解释性图 neural network，它通过限制学习算法来生成可解释的模型，从原始输入特征直接计算出预测结果的确切计算方式。</li>
<li>results: 实验表明，IGNNet 与现状的机器学习算法相比，能够在处理表格数据方面达到相同的性能水平，同时可以提供可解释的模型，其预测结果与特征 SHAPley 值相对aligned，而无需额外的计算开销。<details>
<summary>Abstract</summary>
Data in tabular format is frequently occurring in real-world applications. Graph Neural Networks (GNNs) have recently been extended to effectively handle such data, allowing feature interactions to be captured through representation learning. However, these approaches essentially produce black-box models, in the form of deep neural networks, precluding users from following the logic behind the model predictions. We propose an approach, called IGNNet (Interpretable Graph Neural Network for tabular data), which constrains the learning algorithm to produce an interpretable model, where the model shows how the predictions are exactly computed from the original input features. A large-scale empirical investigation is presented, showing that IGNNet is performing on par with state-of-the-art machine-learning algorithms that target tabular data, including XGBoost, Random Forests, and TabNet. At the same time, the results show that the explanations obtained from IGNNet are aligned with the true Shapley values of the features without incurring any additional computational overhead.
</details>
<details>
<summary>摘要</summary>
<SYS>将数据表示为表格 Format 是实际应用中的常见现象。Graph Neural Networks (GNNs) 最近已经扩展到可以有效处理这类数据，以捕捉特征之间的交互。然而，这些方法基本上生成黑盒模型，即深度神经网络，使用户无法跟踪模型预测的逻辑。我们提出了一种方法，called IGNNet (Interpretable Graph Neural Network for tabular data), 它强制学习算法生成可解释性模型，其中模型可以从原始输入特征直接计算预测。我们对大规模的实验进行了报告，显示IGNNet 与目标 tabular 数据的状态机器学习算法，包括 XGBoost、Random Forests 和 TabNet 相比，表现准确。同时，结果表明IGNNet 获得的解释与特征的真正 Shapley 值相对应，而无需增加计算开销。</SYS>Note: "Shapley value" refers to a concept in cooperative game theory that assigns a value to each player in a cooperative game, based on their contribution to the grand coalition. In the context of the text, it refers to the contribution of each feature to the prediction made by the model.
</details></li>
</ul>
<hr>
<h2 id="Causal-Adversarial-Perturbations-for-Individual-Fairness-and-Robustness-in-Heterogeneous-Data-Spaces"><a href="#Causal-Adversarial-Perturbations-for-Individual-Fairness-and-Robustness-in-Heterogeneous-Data-Spaces" class="headerlink" title="Causal Adversarial Perturbations for Individual Fairness and Robustness in Heterogeneous Data Spaces"></a>Causal Adversarial Perturbations for Individual Fairness and Robustness in Heterogeneous Data Spaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08938">http://arxiv.org/abs/2308.08938</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Ehyaei/CAPIFY">https://github.com/Ehyaei/CAPIFY</a></li>
<li>paper_authors: Ahmad-Reza Ehyaei, Kiarash Mohammadi, Amir-Hossein Karimi, Samira Samadi, Golnoosh Farnadi</li>
<li>for: 本研究旨在探讨个人公平、鲁棒性和结构 causal 模型在不同数据空间中同时探索和 интегразиOINT 这些性能。</li>
<li>methods: 我们提出了一种新的方法，通过使用 causal 结构模型和敏感特征来创建一个公平度量，并应用它来度量个体之间的 semantic 相似性。我们还引入了一种新的 causal  adversarial 干扰和 adversarial 训练，以创建一个新的 regularizer，这个 regularizer 同时包含了个体公平、鲁棒性和 causal 意识。</li>
<li>results: 我们在实际世界和 sintetic 数据集上评估了我们的方法，并证明了它可以构建一个准确的分类器，同时满足个体公平、鲁棒性和 causal 意识的要求。<details>
<summary>Abstract</summary>
As responsible AI gains importance in machine learning algorithms, properties such as fairness, adversarial robustness, and causality have received considerable attention in recent years. However, despite their individual significance, there remains a critical gap in simultaneously exploring and integrating these properties. In this paper, we propose a novel approach that examines the relationship between individual fairness, adversarial robustness, and structural causal models in heterogeneous data spaces, particularly when dealing with discrete sensitive attributes. We use causal structural models and sensitive attributes to create a fair metric and apply it to measure semantic similarity among individuals. By introducing a novel causal adversarial perturbation and applying adversarial training, we create a new regularizer that combines individual fairness, causality, and robustness in the classifier. Our method is evaluated on both real-world and synthetic datasets, demonstrating its effectiveness in achieving an accurate classifier that simultaneously exhibits fairness, adversarial robustness, and causal awareness.
</details>
<details>
<summary>摘要</summary>
As responsible AI gains importance in machine learning algorithms, properties such as fairness, adversarial robustness, and causality have received considerable attention in recent years. However, despite their individual significance, there remains a critical gap in simultaneously exploring and integrating these properties. In this paper, we propose a novel approach that examines the relationship between individual fairness, adversarial robustness, and structural causal models in heterogeneous data spaces, particularly when dealing with discrete sensitive attributes. We use causal structural models and sensitive attributes to create a fair metric and apply it to measure semantic similarity among individuals. By introducing a novel causal adversarial perturbation and applying adversarial training, we create a new regularizer that combines individual fairness, causality, and robustness in the classifier. Our method is evaluated on both real-world and synthetic datasets, demonstrating its effectiveness in achieving an accurate classifier that simultaneously exhibits fairness, adversarial robustness, and causal awareness.Here's the translation in Traditional Chinese:随着责任AI在机器学习算法中的重要性增加，属于不同类型的特性，如公平、敏感性、因果关系等，在最近的几年中已经获得了很大的关注。然而，这些个别的特性之间仍然存在着重要的欠缺，即同时探索和结合这些特性的方法。在这篇文章中，我们提出了一个新的方法，它探索了个人公平、敏感性和结构因果模型在不同数据空间中的关系，特别是在处理数据中的数据敏感特征时。我们使用因果结构模型和数据敏感特征来建立公平度量，并将其应用到测量个体之间的semantic相似性。通过引入新的因果敌对推差和敌对训练，我们创建了一个新的正规化器，它结合个人公平、因果和敏感性在分类器中的作用。我们的方法在真实世界和 sintetic数据集上进行评估，展示了它在精准分类器同时具备公平、敏感性和因果意识的能力。
</details></li>
</ul>
<hr>
<h2 id="Estimating-fire-Duration-using-regression-methods"><a href="#Estimating-fire-Duration-using-regression-methods" class="headerlink" title="Estimating fire Duration using regression methods"></a>Estimating fire Duration using regression methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08936">http://arxiv.org/abs/2308.08936</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hansong Xiao</li>
<li>for: 这篇论文的目的是提出一种基于机器学习的野火预测方法，以解决传统的网格式数学模型所带来的计算成本和时间消耗问题。</li>
<li>methods: 该论文使用了Random Forest、KNN和XGBoost回归模型，以及图像基于的CNN和Encoder模型，来预测已知的野火燃烧时间。模型的输入是通过卫星提供的地形特征地图和相应的历史火灾数据。模型在被训练后，通过处理输入数据以获得最佳结果，能够快速和相对准确地预测未来的野火图像。</li>
<li>results: 论文的实验结果表明，机器学习基于的野火预测方法可以快速和准确地预测野火燃烧时间，并且可以减少计算成本和时间消耗。<details>
<summary>Abstract</summary>
Wildfire forecasting problems usually rely on complex grid-based mathematical models, mostly involving Computational fluid dynamics(CFD) and Celluar Automata, but these methods have always been computationally expensive and difficult to deliver a fast decision pattern. In this paper, we provide machine learning based approaches that solve the problem of high computational effort and time consumption. This paper predicts the burning duration of a known wildfire by RF(random forest), KNN, and XGBoost regression models and also image-based, like CNN and Encoder. Model inputs are based on the map of landscape features provided by satellites and the corresponding historical fire data in this area. This model is trained by happened fire data and landform feature maps and tested with the most recent real value in the same area. By processing the input differently to obtain the optimal outcome, the system is able to make fast and relatively accurate future predictions based on landscape images of known fires.
</details>
<details>
<summary>摘要</summary>
通常情况下，野火预测问题都会采用复杂的网格式数学模型，主要包括计算流体动力学(CFD)和细胞自动机，但这些方法总是 computationally expensive 并且困难呈现快速决策模式。在这篇论文中，我们提供了基于机器学习的方法来解决高计算成本和时间消耗的问题。本文预测已知野火燃烧的时间长度，使用Random Forest、KNN和XGBoost回归模型，以及图像基于的方法，如CNN和Encoder。模型输入基于通过卫星提供的地形特征图和相应的历史火灾数据。这个模型通过已发生火灾数据和地形特征图进行训练，并在同一地区测试最新的实际值。通过不同的处理方式来获得最佳结果，系统可以根据地形图像来快速和相对准确地预测未来的野火。
</details></li>
</ul>
<hr>
<h2 id="On-Data-Imbalance-in-Molecular-Property-Prediction-with-Pre-training"><a href="#On-Data-Imbalance-in-Molecular-Property-Prediction-with-Pre-training" class="headerlink" title="On Data Imbalance in Molecular Property Prediction with Pre-training"></a>On Data Imbalance in Molecular Property Prediction with Pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08934">http://arxiv.org/abs/2308.08934</a></li>
<li>repo_url: None</li>
<li>paper_authors: Limin Wang, Masatoshi Hanai, Toyotaro Suzumura, Shun Takashige, Kenjiro Taura</li>
<li>for: 本研究旨在提高分子性质预测模型的准确性，通过修改现有的代表性预训练方法（node masking）的损失函数，以补偿输入数据的不均衡。</li>
<li>methods: 本研究使用了一种组合方法，包括理论计算和机器学习，其中理论计算用于确定分子性质，而机器学习用于构建一个可以应用于剩下的材料的模型。此外，本研究还使用了预训练技术，包括node masking，以提高机器学习模型的准确性。</li>
<li>results: 本研究通过实验和评估使用了benchmark模型，发现修改损失函数可以提高预测结果的准确性，并且可以补偿输入数据的不均衡。<details>
<summary>Abstract</summary>
Revealing and analyzing the various properties of materials is an essential and critical issue in the development of materials, including batteries, semiconductors, catalysts, and pharmaceuticals. Traditionally, these properties have been determined through theoretical calculations and simulations. However, it is not practical to perform such calculations on every single candidate material. Recently, a combination method of the theoretical calculation and machine learning has emerged, that involves training machine learning models on a subset of theoretical calculation results to construct a surrogate model that can be applied to the remaining materials. On the other hand, a technique called pre-training is used to improve the accuracy of machine learning models. Pre-training involves training the model on pretext task, which is different from the target task, before training the model on the target task. This process aims to extract the input data features, stabilizing the learning process and improving its accuracy. However, in the case of molecular property prediction, there is a strong imbalance in the distribution of input data and features, which may lead to biased learning towards frequently occurring data during pre-training. In this study, we propose an effective pre-training method that addresses the imbalance in input data. We aim to improve the final accuracy by modifying the loss function of the existing representative pre-training method, node masking, to compensate the imbalance. We have investigated and assessed the impact of our proposed imbalance compensation on pre-training and the final prediction accuracy through experiments and evaluations using benchmark of molecular property prediction models.
</details>
<details>
<summary>摘要</summary>
描述和分析材料的不同性质是物料发展中的关键和重要问题，包括电池、半导体、催化剂和药物。在过去，这些性质通常通过理论计算和模拟来确定。但是，对每种候选材料进行这些计算是不实际的。最近，一种结合理论计算和机器学习的方法得到了应用，即通过训练机器学习模型在一个子集理论计算结果上构建一个代理模型，以应用于剩下的材料。而在机器学习模型训练中，一种称为预训练的技术得到了应用，即在预测任务上训练模型，然后在目标任务上训练模型。这个过程的目的是提取输入数据特征，稳定学习过程，提高准确性。但在分子性质预测中，输入数据和特征之间存在强烈的不均衡，这可能导致在预训练过程中偏向频繁出现的数据进行偏激学习。在本研究中，我们提出了一种有效地弥合输入数据不均衡的预训练方法。我们希望通过修改现有代表预训练方法的损失函数，以补偿不均衡。我们通过实验和评估使用分子性质预测模型的标准套件进行了研究和评估。
</details></li>
</ul>
<hr>
<h2 id="IMM-An-Imitative-Reinforcement-Learning-Approach-with-Predictive-Representation-Learning-for-Automatic-Market-Making"><a href="#IMM-An-Imitative-Reinforcement-Learning-Approach-with-Predictive-Representation-Learning-for-Automatic-Market-Making" class="headerlink" title="IMM: An Imitative Reinforcement Learning Approach with Predictive Representation Learning for Automatic Market Making"></a>IMM: An Imitative Reinforcement Learning Approach with Predictive Representation Learning for Automatic Market Making</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08918">http://arxiv.org/abs/2308.08918</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hui Niu, Siyuan Li, Jiahao Zheng, Zhouchi Lin, Jian Li, Jian Guo, Bo An</li>
<li>For: 这篇论文旨在提出一种基于强化学习的多价格水平市场制作者（Imitative Market Maker，IMM）方法，以提高市场流动性和订单处理效率。* Methods: 该方法基于一种新的状态和动作表示方法，可以快速和高效地学习多价格水平市场制作者的策略。它还 integrate了一种表示学习单元，可以捕捉市场趋势的短期和长期变化，从而降低风险。* Results: 实验结果表明，IMM方法在四个实际市场数据集上比现有的RL基于市场制作者策略具有较高的财务效益和处理效率。减少策略的风险也得到了证明。<details>
<summary>Abstract</summary>
Market making (MM) has attracted significant attention in financial trading owing to its essential function in ensuring market liquidity. With strong capabilities in sequential decision-making, Reinforcement Learning (RL) technology has achieved remarkable success in quantitative trading. Nonetheless, most existing RL-based MM methods focus on optimizing single-price level strategies which fail at frequent order cancellations and loss of queue priority. Strategies involving multiple price levels align better with actual trading scenarios. However, given the complexity that multi-price level strategies involves a comprehensive trading action space, the challenge of effectively training profitable RL agents for MM persists. Inspired by the efficient workflow of professional human market makers, we propose Imitative Market Maker (IMM), a novel RL framework leveraging both knowledge from suboptimal signal-based experts and direct policy interactions to develop multi-price level MM strategies efficiently. The framework start with introducing effective state and action representations adept at encoding information about multi-price level orders. Furthermore, IMM integrates a representation learning unit capable of capturing both short- and long-term market trends to mitigate adverse selection risk. Subsequently, IMM formulates an expert strategy based on signals and trains the agent through the integration of RL and imitation learning techniques, leading to efficient learning. Extensive experimental results on four real-world market datasets demonstrate that IMM outperforms current RL-based market making strategies in terms of several financial criteria. The findings of the ablation study substantiate the effectiveness of the model components.
</details>
<details>
<summary>摘要</summary>
市场制作（MM）在金融交易中吸引了广泛的注意力，因为它对市场流动性的稳定性具有关键作用。RL技术在数学交易中取得了显著的成功，但大多数现有RL基于MM方法都是优化单价级别策略，这会导致频繁的订单取消和排队优先权失去。使用多个价格级别的策略更好地适应实际交易场景。然而，由于多个价格级别的策略的复杂性，训练可财富RL代理人是一项挑战。 Drawing inspiration from professional human market makers' efficient workflow, we propose Imitative Market Maker (IMM), a novel RL framework that leverages both knowledge from suboptimal signal-based experts and direct policy interactions to develop multi-price level MM strategies efficiently. The framework starts by introducing effective state and action representations that are adept at encoding information about multi-price level orders. Furthermore, IMM integrates a representation learning unit capable of capturing both short- and long-term market trends to mitigate adverse selection risk. Subsequently, IMM formulates an expert strategy based on signals and trains the agent through the integration of RL and imitation learning techniques, leading to efficient learning. Extensive experimental results on four real-world market datasets demonstrate that IMM outperforms current RL-based market making strategies in terms of several financial criteria. The findings of the ablation study substantiate the effectiveness of the model components.
</details></li>
</ul>
<hr>
<h2 id="Beyond-Sharing-Conflict-Aware-Multivariate-Time-Series-Anomaly-Detection"><a href="#Beyond-Sharing-Conflict-Aware-Multivariate-Time-Series-Anomaly-Detection" class="headerlink" title="Beyond Sharing: Conflict-Aware Multivariate Time Series Anomaly Detection"></a>Beyond Sharing: Conflict-Aware Multivariate Time Series Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08915">http://arxiv.org/abs/2308.08915</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dawnvince/mts_cad">https://github.com/dawnvince/mts_cad</a></li>
<li>paper_authors: Haotian Si, Changhua Pei, Zhihan Li, Yadong Zhao, Jingjing Li, Haiming Zhang, Zulong Diao, Jianhui Li, Gaogang Xie, Dan Pei</li>
<li>for: 本研究旨在提出一种基于多任务学习的多变量时间序列异常检测（MTS AD）算法，以确保软件应用和服务系统的可靠性。</li>
<li>methods: 本研究使用了自适应MTS AD方法，通过优化总体目标&#x2F;损失来捕捉所有指标的回归目标&#x2F;损失。然而，我们的实验发现，这些指标的回归目标之间存在冲突，导致MTS模型面临不同的损失。为解决这问题，我们提出了一种具有冲突 Mitigation的多变量KPI异常检测算法（CAD）。</li>
<li>results: 我们的实验表明，CAD算法在三个公共数据集上的平均F1分数为0.943，明显超过现有方法。此外，我们还发现了MTS形式ulation的输入-输出不一致和扩展任务的问题，并提出了一种简单 yet有效的任务导向指标选择和个性化分配机制，以解决这些挑战。<details>
<summary>Abstract</summary>
Massive key performance indicators (KPIs) are monitored as multivariate time series data (MTS) to ensure the reliability of the software applications and service system. Accurately detecting the abnormality of MTS is very critical for subsequent fault elimination. The scarcity of anomalies and manual labeling has led to the development of various self-supervised MTS anomaly detection (AD) methods, which optimize an overall objective/loss encompassing all metrics' regression objectives/losses. However, our empirical study uncovers the prevalence of conflicts among metrics' regression objectives, causing MTS models to grapple with different losses. This critical aspect significantly impacts detection performance but has been overlooked in existing approaches. To address this problem, by mimicking the design of multi-gate mixture-of-experts (MMoE), we introduce CAD, a Conflict-aware multivariate KPI Anomaly Detection algorithm. CAD offers an exclusive structure for each metric to mitigate potential conflicts while fostering inter-metric promotions. Upon thorough investigation, we find that the poor performance of vanilla MMoE mainly comes from the input-output misalignment settings of MTS formulation and convergence issues arising from expansive tasks. To address these challenges, we propose a straightforward yet effective task-oriented metric selection and p&s (personalized and shared) gating mechanism, which establishes CAD as the first practicable multi-task learning (MTL) based MTS AD model. Evaluations on multiple public datasets reveal that CAD obtains an average F1-score of 0.943 across three public datasets, notably outperforming state-of-the-art methods. Our code is accessible at https://github.com/dawnvince/MTS_CAD.
</details>
<details>
<summary>摘要</summary>
巨大的关键性能指标 (KPI) 被监测为多变量时间序列数据 (MTS)，以确保软件应用程序和服务系统的可靠性。正确地探测 MTS 中的异常是非常关键的，以便后续的故障排除。由于罕见的异常和手动标注的缺乏，已经导致了多种无监督 MTS 异常检测 (AD) 方法的发展，这些方法通过优化总体的目标/损失函数来优化所有指标的回归目标/损失函数。然而，我们的实验发现，在不同指标之间存在冲突的问题，导致 MTS 模型在不同的损失函数之间挣扎。这种问题在现有方法中受到了忽略。为解决这个问题，我们通过模仿多门混合专家 (MMoE) 的设计，引入 CAD，即冲突意识多变量 KPI 异常检测算法。CAD 提供了每个指标 exclusive 结构，以mitigate 可能的冲突，同时推动指标之间的促进。经过全面的调查，我们发现，简单的 MMoE 的 Poor 性能主要来自 MTS 表示形式的输入-输出不一致和扩展任务的准确性问题。为解决这些挑战，我们提议一种简单 yet 有效的任务意向指标选择和人性化分配机制，使 CAD 成为首个实用多任务学习 (MTL) 基于 MTS AD 模型。多个公共数据集的评估显示，CAD 在三个公共数据集上的平均 F1-score 为 0.943，显著超过当前state-of-the-art方法。我们的代码可以在 GitHub 上找到：https://github.com/dawnvince/MTS_CAD。
</details></li>
</ul>
<hr>
<h2 id="MoCLIM-Towards-Accurate-Cancer-Subtyping-via-Multi-Omics-Contrastive-Learning-with-Omics-Inference-Modeling"><a href="#MoCLIM-Towards-Accurate-Cancer-Subtyping-via-Multi-Omics-Contrastive-Learning-with-Omics-Inference-Modeling" class="headerlink" title="MoCLIM: Towards Accurate Cancer Subtyping via Multi-Omics Contrastive Learning with Omics-Inference Modeling"></a>MoCLIM: Towards Accurate Cancer Subtyping via Multi-Omics Contrastive Learning with Omics-Inference Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09725">http://arxiv.org/abs/2308.09725</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziwei Yang, Zheng Chen, Yasuko Matsubara, Yasushi Sakurai</li>
<li>for: 这个论文目的是使用多Omics数据来改进肿瘤分型结果，以便更好地了解肿瘤的发展机理。</li>
<li>methods: 该论文使用了一种名为MoCLIM的表示学习框架，通过独立提取不同Omics模式下的有用特征，并使用这些特征进行归一化，以便更好地分类肿瘤。</li>
<li>results: 实验结果表明，使用MoCLIM方法可以提高肿瘤分型结果的数据适应度和分类性能，并且可以在 fewer 高维度肿瘤实例中提供更高的解释性。<details>
<summary>Abstract</summary>
Precision medicine fundamentally aims to establish causality between dysregulated biochemical mechanisms and cancer subtypes. Omics-based cancer subtyping has emerged as a revolutionary approach, as different level of omics records the biochemical products of multistep processes in cancers. This paper focuses on fully exploiting the potential of multi-omics data to improve cancer subtyping outcomes, and hence developed MoCLIM, a representation learning framework. MoCLIM independently extracts the informative features from distinct omics modalities. Using a unified representation informed by contrastive learning of different omics modalities, we can well-cluster the subtypes, given cancer, into a lower latent space. This contrast can be interpreted as a projection of inter-omics inference observed in biological networks. Experimental results on six cancer datasets demonstrate that our approach significantly improves data fit and subtyping performance in fewer high-dimensional cancer instances. Moreover, our framework incorporates various medical evaluations as the final component, providing high interpretability in medical analysis.
</details>
<details>
<summary>摘要</summary>
基于精准医学的研究旨在确立肿瘤Subtype与生物化学过程的缺陷关系。ómics技术在肿瘤分类方面发挥了革命性的作用，不同的ómics数据记录了肿瘤的生物化学产物。本文将关注在完全利用多个ómics数据来提高肿瘤分类效果，并因此开发了MoCLIM表示学框架。MoCLIM独立提取不同ómics模式中的有用特征。通过对不同ómics模式的对比学习，我们可以将肿瘤分类到一个较低的 latent space。这种对比可以被解释为生物网络中跨modalities的推理。实验结果表明，我们的方法可以在六个肿瘤数据集中显著提高数据适应度和肿瘤分类性能，并且我们的框架可以 incorporate多种医学评估作为最后一个组件，提供高度可解释的医学分析。
</details></li>
</ul>
<hr>
<h2 id="Development-of-a-Knowledge-Graph-Embeddings-Model-for-Pain"><a href="#Development-of-a-Knowledge-Graph-Embeddings-Model-for-Pain" class="headerlink" title="Development of a Knowledge Graph Embeddings Model for Pain"></a>Development of a Knowledge Graph Embeddings Model for Pain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08904">http://arxiv.org/abs/2308.08904</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jaya Chaturvedi, Tao Wang, Sumithra Velupillai, Robert Stewart, Angus Roberts</li>
<li>for: 本研究的目的是构建一个知识图谱模型，用于理解抑郁症患者的痛苦经验。</li>
<li>methods: 本研究使用了知识图谱embedding技术，将痛苦相关的概念与关系从外部医学知识库中提取，并与电子医疗记录中的痛苦实例进行结合。</li>
<li>results: 研究结果显示，使用知识图谱embedding模型可以提高预测痛苦相关的Subject-Object链接任务的性能，比基eline模型更高。<details>
<summary>Abstract</summary>
Pain is a complex concept that can interconnect with other concepts such as a disorder that might cause pain, a medication that might relieve pain, and so on. To fully understand the context of pain experienced by either an individual or across a population, we may need to examine all concepts related to pain and the relationships between them. This is especially useful when modeling pain that has been recorded in electronic health records. Knowledge graphs represent concepts and their relations by an interlinked network, enabling semantic and context-based reasoning in a computationally tractable form. These graphs can, however, be too large for efficient computation. Knowledge graph embeddings help to resolve this by representing the graphs in a low-dimensional vector space. These embeddings can then be used in various downstream tasks such as classification and link prediction. The various relations associated with pain which are required to construct such a knowledge graph can be obtained from external medical knowledge bases such as SNOMED CT, a hierarchical systematic nomenclature of medical terms. A knowledge graph built in this way could be further enriched with real-world examples of pain and its relations extracted from electronic health records. This paper describes the construction of such knowledge graph embedding models of pain concepts, extracted from the unstructured text of mental health electronic health records, combined with external knowledge created from relations described in SNOMED CT, and their evaluation on a subject-object link prediction task. The performance of the models was compared with other baseline models.
</details>
<details>
<summary>摘要</summary>
痛苦是一个复杂的概念，可以与其他概念相互连接，如疾病、药物等，以全面理解个人或人口群体的痛苦经验。为了实现这一目标，我们需要检视所有与痛苦相关的概念和他们之间的关系。这非常有用，特别是在模拟基于电子医疗记录的痛苦记录时。知识图表示概念和其关系为相互连接的网络，允许semantic和上下文基本的推理。但这些图可能太大，不可靠性Compute。知识图嵌入帮助解决这一问题，将知识图 Represented in一个低维度的向量空间。这些嵌入可以在多种下游任务中使用，如分类和链接预测。关于痛苦的各种关系，可以从外部的医疗知识库，如SNOMED CT，获取。SNOMED CT是一个层次的系统性词汇表，用于医学术语。通过将这些知识库与电子医疗记录中的实际痛苦例子相结合，可以构建更加完整的知识图。这篇论文描述了基于痛苦概念的知识图嵌入模型的建构，从精神医疗电子记录中提取的概念和SNOMED CT中的外部知识进行组合，以及这些模型在主题-对象链接预测任务上的评估。与其他基eline模型相比，这些模型的性能如何呢？
</details></li>
</ul>
<hr>
<h2 id="Optimal-Resource-Allocation-for-U-Shaped-Parallel-Split-Learning"><a href="#Optimal-Resource-Allocation-for-U-Shaped-Parallel-Split-Learning" class="headerlink" title="Optimal Resource Allocation for U-Shaped Parallel Split Learning"></a>Optimal Resource Allocation for U-Shaped Parallel Split Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08896">http://arxiv.org/abs/2308.08896</a></li>
<li>repo_url: None</li>
<li>paper_authors: Song Lyu, Zheng Lin, Guanqiao Qu, Xianhao Chen, Xiaoxia Huang, Pan Li<br>for: 这paper是为了解决 tradicional的 Split Learning（SL）方法会泄露标签隐私的问题，提出了U-shaped网络的使用来保护标签隐私。methods: 该paper使用了U-shaped网络和Parallel Split Learning（PSL）技术，并提出了一种名为LSCRA的资源分配算法来优化边缘网络的性能。results: 该paper的实验结果表明，LSCRA算法可以有效地分配资源和Split层，并且U-shaped PSL可以与其他SL基线方法具有相似的性能而又保护标签隐私。<details>
<summary>Abstract</summary>
Split learning (SL) has emerged as a promising approach for model training without revealing the raw data samples from the data owners. However, traditional SL inevitably leaks label privacy as the tail model (with the last layers) should be placed on the server. To overcome this limitation, one promising solution is to utilize U-shaped architecture to leave both early layers and last layers on the user side. In this paper, we develop a novel parallel U-shaped split learning and devise the optimal resource optimization scheme to improve the performance of edge networks. In the proposed framework, multiple users communicate with an edge server for SL. We analyze the end-to-end delay of each client during the training process and design an efficient resource allocation algorithm, called LSCRA, which finds the optimal computing resource allocation and split layers. Our experimental results show the effectiveness of LSCRA and that U-shaped PSL can achieve a similar performance with other SL baselines while preserving label privacy. Index Terms: U-shaped network, split learning, label privacy, resource allocation, 5G/6G edge networks.
</details>
<details>
<summary>摘要</summary>
Split learning (SL) 已经出现为训练模型无需披露原始数据样本的有效方法。然而，传统的 SL 必然泄露标签隐私，因为tail模型（最后层）必须放置在服务器上。为解决这个限制，一种有前途的解决方案是使用 U 型架构，留下 Early layers 和 Last layers 在用户端。在本文中，我们开发了一种新的平行 U 型 Split Learning 框架，并设计了最佳资源优化策略，以提高边缘网络的性能。在我们的提案中，多个用户与边缘服务器进行 SL 通信。我们分析每个客户端在训练过程中的末端延迟，并设计了一个高效的资源分配算法，称为 LSCRA，以找到最佳计算资源分配和分割层。我们的实验结果表明 LSCRA 的效果和 U 型 PSL 可以在保持标签隐私的情况下实现相似的性能。关键字： U 型网络、Split learning、标签隐私、资源分配、5G/6G 边缘网络。
</details></li>
</ul>
<hr>
<h2 id="Dual-Gauss-Newton-Directions-for-Deep-Learning"><a href="#Dual-Gauss-Newton-Directions-for-Deep-Learning" class="headerlink" title="Dual Gauss-Newton Directions for Deep Learning"></a>Dual Gauss-Newton Directions for Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08886">http://arxiv.org/abs/2308.08886</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vincent Roulet, Mathieu Blondel</li>
<li>for: 提高深度学习优化算法的精度和效率</li>
<li>methods: 基于半线性化的对象结构，使用对数损失函数和非线性网络的复合，计算更好的方向指标，而不是渐进随机梯度</li>
<li>results: 实验表明，使用对数损失函数和非线性网络的复合，可以获得更好的下降方向指标，并且可以用现有的优化算法中的梯度更新Here’s a more detailed explanation of each point:</li>
<li>for: The paper aims to improve the accuracy and efficiency of deep learning optimization algorithms by leveraging the structure of deep learning objectives.</li>
<li>methods: The authors propose to compute better direction oracles using the dual formulation of the objective function, which leads to both computational benefits and new insights.</li>
<li>results: The experimental results show that using the dual formulation of the objective function, combined with the composition of a convex loss function and a nonlinear network, can lead to better descent directions that can be used as a drop-in replacement for stochastic gradients in existing optimization algorithms.<details>
<summary>Abstract</summary>
Inspired by Gauss-Newton-like methods, we study the benefit of leveraging the structure of deep learning objectives, namely, the composition of a convex loss function and of a nonlinear network, in order to derive better direction oracles than stochastic gradients, based on the idea of partial linearization. In a departure from previous works, we propose to compute such direction oracles via their dual formulation, leading to both computational benefits and new insights. We demonstrate that the resulting oracles define descent directions that can be used as a drop-in replacement for stochastic gradients, in existing optimization algorithms. We empirically study the advantage of using the dual formulation as well as the computational trade-offs involved in the computation of such oracles.
</details>
<details>
<summary>摘要</summary>
受加ус-牛顿方法启发，我们研究利用深度学习目标结构的优点，即几何函数和抽象网络的复合，以 derive better direction oracles than stochastic gradients，基于partial linearization的想法。在之前的工作中，我们提议通过对 dual 形式来计算这些方向指南，从而实现计算上的收益和新的理解。我们证明这些方向指南可以作为现有优化算法中的替补，以及计算这些方向指南的计算成本和计算质量的负担。我们进行了实验研究，证明使用 dual 形式的计算具有优势，并且计算成本和计算质量的负担存在trade-off关系。
</details></li>
</ul>
<hr>
<h2 id="Feature-Enforcing-PINN-FE-PINN-A-Framework-to-Learn-the-Underlying-Physics-Features-Before-Target-Task"><a href="#Feature-Enforcing-PINN-FE-PINN-A-Framework-to-Learn-the-Underlying-Physics-Features-Before-Target-Task" class="headerlink" title="Feature Enforcing PINN (FE-PINN): A Framework to Learn the Underlying-Physics Features Before Target Task"></a>Feature Enforcing PINN (FE-PINN): A Framework to Learn the Underlying-Physics Features Before Target Task</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08873">http://arxiv.org/abs/2308.08873</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahyar Jahaninasab, Mohamad Ali Bijarchi</li>
<li>for:  This paper is written for solving partial differential equations (PDEs) with a new data-free framework called Feature Enforcing Physics Informed Neural Network (FE-PINN).</li>
<li>methods:  The FE-PINN framework uses a sequence of sub-tasks to learn useful features about the underlying physics and then refine the calculations for the target task.</li>
<li>results:  The FE-PINN framework achieves 15x, 2x, and 5x speed up for three benchmarks (flow over a cylinder, 2D heat conduction, and an inverse problem of calculating inlet velocity) compared to vanilla PINN, and can reach a loss value near 1e-5, which is challenging for vanilla PINN.Here is the simplified Chinese text for the three key information points:</li>
<li>for: 这篇论文是为解决部分 diferencial 方程（PDEs）而编写的，使用一种新的数据自由框架called Feature Enforcing Physics Informed Neural Network (FE-PINN)。</li>
<li>methods:  FE-PINN 框架使用一个序列的子任务来学习有用的物理特征，然后为目标任务进行精细的计算。</li>
<li>results: FE-PINN 框架在三个 benchmark（流过cylinder，2D heat conduction，和反向问题计算入口速度）中实现了15倍，2倍，和5倍的速度提升，并可以达到1e-5的损失值，这是vanilla PINN 困难的。<details>
<summary>Abstract</summary>
In this work, a new data-free framework called Feature Enforcing Physics Informed Neural Network (FE-PINN) is introduced. This framework is capable of learning the underlying pattern of any problem with low computational cost before the main training loop. The loss function of vanilla PINN due to the existence of two terms of partial differential residuals and boundary condition mean squared error is imbalanced. FE-PINN solves this challenge with just one minute of training instead of time-consuming hyperparameter tuning for loss function that can take hours. The FE-PINN accomplishes this process by performing a sequence of sub-tasks. The first sub-task learns useful features about the underlying physics. Then, the model trains on the target task to refine the calculations. FE-PINN is applied to three benchmarks, flow over a cylinder, 2D heat conduction, and an inverse problem of calculating inlet velocity. FE-PINN can solve each case with, 15x, 2x, and 5x speed up accordingly. Another advantage of FE-PINN is that reaching lower order of value for loss function is systematically possible. In this study, it was possible to reach a loss value near 1e-5 which is challenging for vanilla PINN. FE-PINN also has a smooth convergence process which allows for utilizing higher learning rates in comparison to vanilla PINN. This framework can be used as a fast, accurate tool for solving a wide range of Partial Differential Equations (PDEs) across various fields.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们介绍了一种新的数据自由框架，即特征强制物理学 Informed Neural Network（FE-PINN）。这个框架可以在低计算成本下学习问题的下面 patrern。这个框架通过一系列子任务来实现。首先，它学习问题的下面特征。然后，模型在目标任务上进行反射计算。FE-PINN应用于三个标准测试函数，即流过圆柱、2D热传导和反向问题计算入口速度。FE-PINN可以在每个情况下提高计算速度15倍、2倍和5倍。此外，FE-PINN可以系统地降低损失函数的值。在这项研究中，我们可以达到一个损失值附近1e-5，这是vanilla PINN很困难的。FE-PINN也有平滑的收敛过程，因此可以在vanilla PINN的比例上使用更高的学习率。这个框架可以作为解决各种 partial Differential Equations（PDEs）的快速、准确的工具。
</details></li>
</ul>
<hr>
<h2 id="Towards-Semi-supervised-Learning-with-Non-random-Missing-Labels"><a href="#Towards-Semi-supervised-Learning-with-Non-random-Missing-Labels" class="headerlink" title="Towards Semi-supervised Learning with Non-random Missing Labels"></a>Towards Semi-supervised Learning with Non-random Missing Labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08872">http://arxiv.org/abs/2308.08872</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/njuyued/prg4ssl-mnar">https://github.com/njuyued/prg4ssl-mnar</a></li>
<li>paper_authors: Yue Duan, Zhen Zhao, Lei Qi, Luping Zhou, Lei Wang, Yinghuan Shi</li>
<li>for: 本文 targets the problem of label missing not at random (MNAR) in semi-supervised learning (SSL), and proposes a class transition tracking based Pseudo-Rectifying Guidance (PRG) method to improve the performance of SSL models in MNAR scenarios.</li>
<li>methods: 本文使用的方法包括Markov随机游走模型和动态创建的类追踪矩阵，以及基于这些信息的PRG方法，来维护模型的偏好性和类分布的历史信息，从而提高SSL模型在MNAR场景中的性能。</li>
<li>results: 本文的实验结果表明，PRG方法可以在多种MNAR场景中显著超过latest SSL方法组合偏移解决方案的性能，并且在各种极端场景下都能够保持良好的性能。<details>
<summary>Abstract</summary>
Semi-supervised learning (SSL) tackles the label missing problem by enabling the effective usage of unlabeled data. While existing SSL methods focus on the traditional setting, a practical and challenging scenario called label Missing Not At Random (MNAR) is usually ignored. In MNAR, the labeled and unlabeled data fall into different class distributions resulting in biased label imputation, which deteriorates the performance of SSL models. In this work, class transition tracking based Pseudo-Rectifying Guidance (PRG) is devised for MNAR. We explore the class-level guidance information obtained by the Markov random walk, which is modeled on a dynamically created graph built over the class tracking matrix. PRG unifies the historical information of class distribution and class transitions caused by the pseudo-rectifying procedure to maintain the model's unbiased enthusiasm towards assigning pseudo-labels to all classes, so as the quality of pseudo-labels on both popular classes and rare classes in MNAR could be improved. Finally, we show the superior performance of PRG across a variety of MNAR scenarios, outperforming the latest SSL approaches combining bias removal solutions by a large margin. Code and model weights are available at https://github.com/NJUyued/PRG4SSL-MNAR.
</details>
<details>
<summary>摘要</summary>
semi-supervised learning (SSL)  solves the problem of missing labels by using unlabeled data effectively. However, existing SSL methods mostly focus on the traditional setting and ignore the practical and challenging scenario of label Missing Not At Random (MNAR). In MNAR, the labeled and unlabeled data are in different class distributions, leading to biased label imputation and degrading the performance of SSL models. In this work, we propose class transition tracking based Pseudo-Rectifying Guidance (PRG) for MNAR. We explore the class-level guidance information obtained by the Markov random walk, which is modeled on a dynamically created graph built over the class tracking matrix. PRG unifies the historical information of class distribution and class transitions caused by the pseudo-rectifying procedure to maintain the model's unbiased enthusiasm towards assigning pseudo-labels to all classes, so as the quality of pseudo-labels on both popular classes and rare classes in MNAR could be improved. Finally, we show the superior performance of PRG across a variety of MNAR scenarios, outperforming the latest SSL approaches combining bias removal solutions by a large margin. Code and model weights are available at https://github.com/NJUyued/PRG4SSL-MNAR.
</details></li>
</ul>
<hr>
<h2 id="Model-Free-Algorithm-with-Improved-Sample-Efficiency-for-Zero-Sum-Markov-Games"><a href="#Model-Free-Algorithm-with-Improved-Sample-Efficiency-for-Zero-Sum-Markov-Games" class="headerlink" title="Model-Free Algorithm with Improved Sample Efficiency for Zero-Sum Markov Games"></a>Model-Free Algorithm with Improved Sample Efficiency for Zero-Sum Markov Games</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08858">http://arxiv.org/abs/2308.08858</a></li>
<li>repo_url: None</li>
<li>paper_authors: Songtao Feng, Ming Yin, Yu-Xiang Wang, Jing Yang, Yingbin Liang</li>
<li>for: 这个论文主要研究的是在多智能RL中的两参与 Zero-Sum Markov Game 问题，具体来说是在有限时间循环 Markov Decision Processes 中。</li>
<li>methods: 该论文提出了一种基于Stage-based Q-学习算法的模型自由方法，并证明了该算法可以达到最佳的 $\epsilon $-优 Nash Equilibrium 的样本复杂度为 $O(H^3SAB&#x2F;\epsilon^2) $，这是在 $H $ 和 $S $ 上依赖的最佳。</li>
<li>results: 该论文展示了该算法可以达到模型基于算法的最佳性，并且在 $H $ 上依赖的部分提高了模型自由算法的样本复杂度。此外，该论文还提出了一种新的 referential advantage decomposition 技术，以提高样本效率。<details>
<summary>Abstract</summary>
The problem of two-player zero-sum Markov games has recently attracted increasing interests in theoretical studies of multi-agent reinforcement learning (RL). In particular, for finite-horizon episodic Markov decision processes (MDPs), it has been shown that model-based algorithms can find an $\epsilon$-optimal Nash Equilibrium (NE) with the sample complexity of $O(H^3SAB/\epsilon^2)$, which is optimal in the dependence of the horizon $H$ and the number of states $S$ (where $A$ and $B$ denote the number of actions of the two players, respectively). However, none of the existing model-free algorithms can achieve such an optimality. In this work, we propose a model-free stage-based Q-learning algorithm and show that it achieves the same sample complexity as the best model-based algorithm, and hence for the first time demonstrate that model-free algorithms can enjoy the same optimality in the $H$ dependence as model-based algorithms. The main improvement of the dependency on $H$ arises by leveraging the popular variance reduction technique based on the reference-advantage decomposition previously used only for single-agent RL. However, such a technique relies on a critical monotonicity property of the value function, which does not hold in Markov games due to the update of the policy via the coarse correlated equilibrium (CCE) oracle. Thus, to extend such a technique to Markov games, our algorithm features a key novel design of updating the reference value functions as the pair of optimistic and pessimistic value functions whose value difference is the smallest in the history in order to achieve the desired improvement in the sample efficiency.
</details>
<details>
<summary>摘要</summary>
“两个玩家零游戏马克夫游戏（Markov game）的问题在多智能抽象学习（multi-agent reinforcement learning，RL）中得到了越来越多的关注。特别是在有限时间剪辑Markov决策过程（MDP）中，已经证明了使用模型基本算法可以在$\epsilon$-优 NashEquilibrium（NE）中找到$O(H^3SAB/\epsilon^2)$的样本复杂度，这是在$H$和$S$上依赖的最优性。然而，现有的模型自由算法无法达到这种优化。在这个工作中，我们提出了一种模型自由阶段Q学习算法，并证明它可以 дости到最优的$H$依赖性。这个主要的改进来自于使用单个智能RL中广泛使用的参考优势分解技术，但这种技术需要Markov游戏中值函数的幂等性，这并不是真实存在的。因此，我们的算法具有一个关键的新特点，即在历史中更新参考值函数为最小值差的对，以实现所需的样本效率提高。”
</details></li>
</ul>
<hr>
<h2 id="Bag-of-Tricks-for-Long-Tailed-Multi-Label-Classification-on-Chest-X-Rays"><a href="#Bag-of-Tricks-for-Long-Tailed-Multi-Label-Classification-on-Chest-X-Rays" class="headerlink" title="Bag of Tricks for Long-Tailed Multi-Label Classification on Chest X-Rays"></a>Bag of Tricks for Long-Tailed Multi-Label Classification on Chest X-Rays</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08853">http://arxiv.org/abs/2308.08853</a></li>
<li>repo_url: None</li>
<li>paper_authors: Feng Hong, Tianjie Dai, Jiangchao Yao, Ya Zhang, Yanfeng Wang</li>
<li>for: 这个研究旨在提高胸部X射镜（CXR）诊断的准确性，探讨了许多进步设计，如数据增强、特征提取、分类器设计、损失函数重新配置、外部数据补充等。</li>
<li>methods: 这个研究使用了多种进步设计，包括数据增强、特征提取、分类器设计、损失函数重新配置、外部数据补充等。</li>
<li>results: 这个研究获得了0.349 mAP的成绩，位居评比测试集的前五名。<details>
<summary>Abstract</summary>
Clinical classification of chest radiography is particularly challenging for standard machine learning algorithms due to its inherent long-tailed and multi-label nature. However, few attempts take into account the coupled challenges posed by both the class imbalance and label co-occurrence, which hinders their value to boost the diagnosis on chest X-rays (CXRs) in the real-world scenarios. Besides, with the prevalence of pretraining techniques, how to incorporate these new paradigms into the current framework lacks of the systematical study. This technical report presents a brief description of our solution in the ICCV CVAMD 2023 CXR-LT Competition. We empirically explored the effectiveness for CXR diagnosis with the integration of several advanced designs about data augmentation, feature extractor, classifier design, loss function reweighting, exogenous data replenishment, etc. In addition, we improve the performance through simple test-time data augmentation and ensemble. Our framework finally achieves 0.349 mAP on the competition test set, ranking in the top five.
</details>
<details>
<summary>摘要</summary>
严重疾病分类在胸部X射影图（CXR）中是特别挑战，主要因为它具有自然长尾和多标签性。然而，少数尝试不足以考虑这两个挑战的结合，从而限制其在实际场景中的价值。此外，随着预训练技术的普及，如何在当前框架中 integrate these new paradigms lacks systematic study.本技术报告 briefly describes our solution in the ICCV CVAMD 2023 CXR-LT Competition. We empirically explored the effectiveness of CXR diagnosis with the integration of several advanced designs, including data augmentation, feature extraction, classifier design, loss function reweighting, exogenous data replenishment, etc. In addition, we improved the performance through simple test-time data augmentation and ensemble. Our framework finally achieves 0.349 mAP on the competition test set, ranking in the top five.
</details></li>
</ul>
<hr>
<h2 id="Learning-the-hub-graphical-Lasso-model-with-the-structured-sparsity-via-an-efficient-algorithm"><a href="#Learning-the-hub-graphical-Lasso-model-with-the-structured-sparsity-via-an-efficient-algorithm" class="headerlink" title="Learning the hub graphical Lasso model with the structured sparsity via an efficient algorithm"></a>Learning the hub graphical Lasso model with the structured sparsity via an efficient algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08852">http://arxiv.org/abs/2308.08852</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chengjing Wang, Peipei Tang, Wenling He, Meixia Lin</li>
<li>for:  efficiently estimate graphical models with hub nodes in large datasets</li>
<li>methods:  dual alternating direction method of multipliers (ADMM) and semismooth Newton (SSN) based augmented Lagrangian method (ALM)</li>
<li>results:  significantly outperforms existing state-of-the-art algorithms in terms of execution time and estimation accuracy, with savings of up to 70% in some high-dimensional tasks.<details>
<summary>Abstract</summary>
Graphical models have exhibited their performance in numerous tasks ranging from biological analysis to recommender systems. However, graphical models with hub nodes are computationally difficult to fit, particularly when the dimension of the data is large. To efficiently estimate the hub graphical models, we introduce a two-phase algorithm. The proposed algorithm first generates a good initial point via a dual alternating direction method of multipliers (ADMM), and then warm starts a semismooth Newton (SSN) based augmented Lagrangian method (ALM) to compute a solution that is accurate enough for practical tasks. The sparsity structure of the generalized Jacobian ensures that the algorithm can obtain a nice solution very efficiently. Comprehensive experiments on both synthetic data and real data show that it obviously outperforms the existing state-of-the-art algorithms. In particular, in some high dimensional tasks, it can save more than 70\% of the execution time, meanwhile still achieves a high-quality estimation.
</details>
<details>
<summary>摘要</summary>
文本翻译为简化中文：</SYS>图形模型在各种任务中表现出色，从生物分析到推荐系统。然而，含有中心节点的图形模型在大量数据时 computationally 难以适应。为了高效地估算中心图形模型，我们提出了两阶段算法。我们的算法首先使用对偶替代方法（ADMM）生成一个良好的初始点，然后使用增强的均值法（SSN）基于扩展拉格朗日方法（ALM）计算一个精度足够高的解决方案。通过各个稀疏结构的总导数，我们的算法可以很快地获得一个高质量的解决方案。实验表明，我们的算法在各种高维任务中可以 saves more than 70% 的执行时间，同时仍然达到高质量的估算。
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-Assisted-Discovery-of-Novel-Reactor-Designs-via-CFD-Coupled-Multi-fidelity-Bayesian-Optimisation"><a href="#Machine-Learning-Assisted-Discovery-of-Novel-Reactor-Designs-via-CFD-Coupled-Multi-fidelity-Bayesian-Optimisation" class="headerlink" title="Machine Learning-Assisted Discovery of Novel Reactor Designs via CFD-Coupled Multi-fidelity Bayesian Optimisation"></a>Machine Learning-Assisted Discovery of Novel Reactor Designs via CFD-Coupled Multi-fidelity Bayesian Optimisation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08841">http://arxiv.org/abs/2308.08841</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tom Savage, Nausheen Basha, Jonathan McDonough, Omar K Matar, Ehecatl Antonio del Rio Chanona</li>
<li>for: 这 paper 的目的是设计、优化和制造以下一代化学反应器。</li>
<li>methods: 这 paper 使用了多级 Bayesian 优化法，以优化各种不同的尺度和曲线路径，从而实现高维度、复杂的优化问题。</li>
<li>results: 这 paper 通过 maximizing 填充流性能，提出了两种新的卷积管 geometries，并通过 3D 打印和实验验证了它们的可靠性和性能。<details>
<summary>Abstract</summary>
Additive manufacturing has enabled the production of more advanced reactor geometries, resulting in the potential for significantly larger and more complex design spaces. Identifying and optimising promising configurations within broader design spaces presents a significant challenge for existing human-centric design approaches. As such, existing parameterisations of coiled-tube reactor geometries are low-dimensional with expensive optimisation limiting more complex solutions. Given algorithmic improvements and the onset of additive manufacturing, we propose two novel coiled-tube parameterisations enabling the variation of cross-section and coil path, resulting in a series of high dimensional, complex optimisation problems. To ensure tractable, non-local optimisation where gradients are not available, we apply multi-fidelity Bayesian optimisation. Our approach characterises multiple continuous fidelities and is coupled with parameterised meshing and simulation, enabling lower quality, but faster simulations to be exploited throughout optimisation. Through maximising the plug-flow performance, we identify key characteristics of optimal reactor designs, and extrapolate these to produce two novel geometries that we 3D print and experimentally validate. By demonstrating the design, optimisation, and manufacture of highly parameterised reactors, we seek to establish a framework for the next-generation of reactors, demonstrating that intelligent design coupled with new manufacturing processes can significantly improve the performance and sustainability of future chemical processes.
</details>
<details>
<summary>摘要</summary>
“三维打印技术的出现已经使得反应室的设计空间得以扩大，并且可以实现更复杂的设计。但是，对于现有的人类中心设计方法来说，identifying和优化promising配置在更广泛的设计空间中是一项具有挑战性的任务。因此，现有的环形管径参数化方法都是低维的，并且优化成本较高，不能实现更复杂的解决方案。为了解决这个问题，我们提出了两种新的环形管径参数化方法，允许环形管径的跨section和径路变化，从而导致一系列高维度、复杂的优化问题。为了确保可行的、非本地优化，我们采用多质量抽象 Bayesian 优化方法。我们的方法通过连接多个连续质量的 Bayesian 优化和参数化的笆化和模拟，以便在优化过程中使用较低质量但快速的 simulations。通过最大化插入性性能，我们确定了优化反应室的关键特征，并将其推广到生产两个新的geometry。我们通过3D打印和实验验证这两个geometry，以证明我们的设计、优化和制造方法可以带来未来化学过程的性能和可持续性。”
</details></li>
</ul>
<hr>
<h2 id="ICoNIK-Generating-Respiratory-Resolved-Abdominal-MR-Reconstructions-Using-Neural-Implicit-Representations-in-k-Space"><a href="#ICoNIK-Generating-Respiratory-Resolved-Abdominal-MR-Reconstructions-Using-Neural-Implicit-Representations-in-k-Space" class="headerlink" title="ICoNIK: Generating Respiratory-Resolved Abdominal MR Reconstructions Using Neural Implicit Representations in k-Space"></a>ICoNIK: Generating Respiratory-Resolved Abdominal MR Reconstructions Using Neural Implicit Representations in k-Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08830">http://arxiv.org/abs/2308.08830</a></li>
<li>repo_url: None</li>
<li>paper_authors: Veronika Spieker, Wenqi Huang, Hannah Eichhorn, Jonathan Stelter, Kilian Weiss, Veronika A. Zimmer, Rickmer F. Braren, Dimitrios C. Karampinos, Kerstin Hammernik, Julia A. Schnabel</li>
<li>for: 这份研究旨在提出一种能够生成不受运动噪声影响的高精度腹部MRI重建方法。</li>
<li>methods: 本研究使用神经阶层学习（NIK）来直接在k空间中学习无残差的运动解析，并引入了一个受资料支持的几何调整层（ICo）以正常化NIK的预测。</li>
<li>results: 该方法比标准的运动解析方法表现更好，并且可以获得高精度的腹部MRI重建结果。<details>
<summary>Abstract</summary>
Motion-resolved reconstruction for abdominal magnetic resonance imaging (MRI) remains a challenge due to the trade-off between residual motion blurring caused by discretized motion states and undersampling artefacts. In this work, we propose to generate blurring-free motion-resolved abdominal reconstructions by learning a neural implicit representation directly in k-space (NIK). Using measured sampling points and a data-derived respiratory navigator signal, we train a network to generate continuous signal values. To aid the regularization of sparsely sampled regions, we introduce an additional informed correction layer (ICo), which leverages information from neighboring regions to correct NIK's prediction. Our proposed generative reconstruction methods, NIK and ICoNIK, outperform standard motion-resolved reconstruction techniques and provide a promising solution to address motion artefacts in abdominal MRI.
</details>
<details>
<summary>摘要</summary>
对于腹部磁共振成像（MRI）中的运动解构，仍然是一个挑战，这是因为运动状态的精细化会导致剩下的运动扩散噪声和抽取 artefacts。在这项工作中，我们提议通过直接在 k-空间学习神经网络表示（NIK）来生成无拟合噪声的运动解构。使用测量的抽取点和数据驱动的呼吸导航信号，我们将网络训练出continuous的信号值。为了帮助稀疏抽取区域的正则化，我们引入了一个更正层（ICo），该层利用邻近区域的信息来正则化 NIK 的预测。我们的提出的生成重建方法，NIK 和 ICoNIK，超过了标准的运动解构技术，并提供了Addressing motion artefacts in abdominal MRI 中的一个有前途的解决方案。
</details></li>
</ul>
<hr>
<h2 id="Controlling-Federated-Learning-for-Covertness"><a href="#Controlling-Federated-Learning-for-Covertness" class="headerlink" title="Controlling Federated Learning for Covertness"></a>Controlling Federated Learning for Covertness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08825">http://arxiv.org/abs/2308.08825</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adit Jain, Vikram Krishnamurthy</li>
<li>for: 这篇论文目的是探讨一个叫做“covert”或“learner-private”的优化问题，learner要在发生噪音的情况下，通过对应用数据的随机性来隐藏目标函数的最佳值。</li>
<li>methods: 本文使用了Markov decision process来模型问题，并证明了运算的超模统性，这使得找到优化策略的问题可以使用政策条件运算。同时，一个 computationally efficient policy gradient algorithm 是提出来解决问题。</li>
<li>results: 在实际应用中，本文将方法应用在一个联邦学习中的 hate speech 分类任务上，并证明了在使用优化策略时，一个 eavesdropper 只能在没有信息的情况下取得52%的验证率，对比之下，在有10%阳性数据的情况下，eavesdropper 只能取得69%的验证率，而且这些结果较好于使用单纯的 greedy 策略。<details>
<summary>Abstract</summary>
A learner aims to minimize a function $f$ by repeatedly querying a distributed oracle that provides noisy gradient evaluations. At the same time, the learner seeks to hide $\arg\min f$ from a malicious eavesdropper that observes the learner's queries. This paper considers the problem of \textit{covert} or \textit{learner-private} optimization, where the learner has to dynamically choose between learning and obfuscation by exploiting the stochasticity. The problem of controlling the stochastic gradient algorithm for covert optimization is modeled as a Markov decision process, and we show that the dynamic programming operator has a supermodular structure implying that the optimal policy has a monotone threshold structure. A computationally efficient policy gradient algorithm is proposed to search for the optimal querying policy without knowledge of the transition probabilities. As a practical application, our methods are demonstrated on a hate speech classification task in a federated setting where an eavesdropper can use the optimal weights to generate toxic content, which is more easily misclassified. Numerical results show that when the learner uses the optimal policy, an eavesdropper can only achieve a validation accuracy of $52\%$ with no information and $69\%$ when it has a public dataset with 10\% positive samples compared to $83\%$ when the learner employs a greedy policy.
</details>
<details>
<summary>摘要</summary>
学生希望减少函数 $f$ 的值，通过 repeatedly 请求分布式 oracle 提供噪声梯度评估。同时，学生尝试隐藏 $\arg\min f$ 于一个恶意窃听者，该窃听者可以观察学生的查询。这篇论文考虑了“隐蔽”或“学习者私有”优化问题，学生需要在执行权重学习和隐蔽之间 dynamically 选择。在模型中，我们使用 Markov 决策过程来控制权重学习的执行，并证明了优化算法的动态程序拥有超模ular结构，表示优化策略的最优策略具有垂直阈值结构。我们提出了一种 Computational 效率的策略梯度算法，可以无需知情转移概率进行搜索优化策略。在实际应用中，我们将方法应用于一个联合 Setting 中的 hate speech 分类任务，恶意窃听者可以使用最优策略生成毒品，这些毒品更容易被误分类。实际结果表明，当学生使用优化策略时，恶意窃听者只能在没有信息的情况下达到52%的验证精度，并且在拥有10%正样本的情况下达到69%的验证精度，与学生使用响应策略相比，恶意窃听者的验证精度提高了21个百分点。
</details></li>
</ul>
<hr>
<h2 id="Mitigating-Semantic-Confusion-from-Hostile-Neighborhood-for-Graph-Active-Learning"><a href="#Mitigating-Semantic-Confusion-from-Hostile-Neighborhood-for-Graph-Active-Learning" class="headerlink" title="Mitigating Semantic Confusion from Hostile Neighborhood for Graph Active Learning"></a>Mitigating Semantic Confusion from Hostile Neighborhood for Graph Active Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08823">http://arxiv.org/abs/2308.08823</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianmeng Yang, Min Zhou, Yujing Wang, Zhengjie Lin, Lujia Pan, Bin Cui, Yunhai Tong<br>for: 本文targets to improve the performance of Graph Neural Networks (GNNs) by addressing the challenge of semantic confusion in graph active learning (GAL).methods: 本文提出了一个Semantic-aware Active learning framework for Graphs (SAG)，包括引入节点相似度和不相似度，jointly evaluate node influence，并设计了一种新的原型基于 criterion和查询策略来保持节点选择的多样性和分类均衡。results: 对于公共 benchmark graphs 和一个实际世界金融数据集，SAGsignificantly improves node classification performance，并常常超过先前的方法。此外，广泛的分析和剥离研究也证明了提案的效果。<details>
<summary>Abstract</summary>
Graph Active Learning (GAL), which aims to find the most informative nodes in graphs for annotation to maximize the Graph Neural Networks (GNNs) performance, has attracted many research efforts but remains non-trivial challenges. One major challenge is that existing GAL strategies may introduce semantic confusion to the selected training set, particularly when graphs are noisy. Specifically, most existing methods assume all aggregating features to be helpful, ignoring the semantically negative effect between inter-class edges under the message-passing mechanism. In this work, we present Semantic-aware Active learning framework for Graphs (SAG) to mitigate the semantic confusion problem. Pairwise similarities and dissimilarities of nodes with semantic features are introduced to jointly evaluate the node influence. A new prototype-based criterion and query policy are also designed to maintain diversity and class balance of the selected nodes, respectively. Extensive experiments on the public benchmark graphs and a real-world financial dataset demonstrate that SAG significantly improves node classification performances and consistently outperforms previous methods. Moreover, comprehensive analysis and ablation study also verify the effectiveness of the proposed framework.
</details>
<details>
<summary>摘要</summary>
格Active学习（GAL），旨在找到图中最有信息的节点进行标注，以最大化图神经网络（GNNs）性能，已经吸引了许多研究努力，但仍存在许多挑战。一个主要挑战是现有的GAL策略可能会在选择训练集时引入semantic confusion，特别是当图像具有噪音时。在这种情况下，大多数现有方法假设所有聚合特征都是有用的，忽略了间类边缘下的semantic负面效果。在这个工作中，我们提出了Semantic-aware Active learning framework for Graphs（SAG），以 Mitigate semantic confusion problem。节点之间的对比性和不同性以及semantic特征被引入，以共同评估节点的影响力。一个新的原型基本 критерион和查询策略也被设计，以保持节点的多样性和分类均衡。经过对公共的benchmark图和一个真实世界的金融 dataset的广泛实验，我们发现SAG可以明显提高节点的分类性能，并常常超过先前的方法。此外，广泛的分析和减少研究也证明了提案的效果。
</details></li>
</ul>
<hr>
<h2 id="A-Fusion-of-Variational-Distribution-Priors-and-Saliency-Map-Replay-for-Continual-3D-Reconstruction"><a href="#A-Fusion-of-Variational-Distribution-Priors-and-Saliency-Map-Replay-for-Continual-3D-Reconstruction" class="headerlink" title="A Fusion of Variational Distribution Priors and Saliency Map Replay for Continual 3D Reconstruction"></a>A Fusion of Variational Distribution Priors and Saliency Map Replay for Continual 3D Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08812">http://arxiv.org/abs/2308.08812</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sanchar Palit, Sandika Biswas</li>
<li>for: 这个论文的目的是提出一种基于 kontinual learning的单图三维重建方法，以便在新类的训练中仍能reasonably重建之前见类。</li>
<li>methods: 该方法使用Variational Priors来表示抽象形态，并使用saliency map来保留物体特征，从而降低存储训练数据的资源成本。</li>
<li>results: 经过进行了严格的实验，该方法与已知方法相比，具有竞争的Result， both quantitatively and qualitatively。<details>
<summary>Abstract</summary>
Single-image 3D reconstruction is a research challenge focused on predicting 3D object shapes from single-view images. This task requires significant data acquisition to predict both visible and occluded portions of the shape. Furthermore, learning-based methods face the difficulty of creating a comprehensive training dataset for all possible classes. To this end, we propose a continual learning-based 3D reconstruction method where our goal is to design a model using Variational Priors that can still reconstruct the previously seen classes reasonably even after training on new classes. Variational Priors represent abstract shapes and combat forgetting, whereas saliency maps preserve object attributes with less memory usage. This is vital due to resource constraints in storing extensive training data. Additionally, we introduce saliency map-based experience replay to capture global and distinct object features. Thorough experiments show competitive results compared to established methods, both quantitatively and qualitatively.
</details>
<details>
<summary>摘要</summary>
单图3D重建是一个研究挑战，旨在根据单个图像预测3D物体形状。这项任务需要大量数据收集，以预测可见和遮盖的部分。学习基本方法面临的挑战是创建所有可能的类型的完整训练集。为此，我们提出了一种基于不断学习的3D重建方法，其目标是使用可变假设来设计一个可以在训练新类后仍然理解先前看到的类的模型。可变假设表示抽象形状，并避免忘记，而精灵图保留物体特征，减少存储训练数据的内存占用。此外，我们引入精灵图经验回放，以捕捉全局和特定的物体特征。经验表明，我们的方法与已知方法相比，具有竞争力的result。
</details></li>
</ul>
<hr>
<h2 id="Label-Shift-Adapter-for-Test-Time-Adaptation-under-Covariate-and-Label-Shifts"><a href="#Label-Shift-Adapter-for-Test-Time-Adaptation-under-Covariate-and-Label-Shifts" class="headerlink" title="Label Shift Adapter for Test-Time Adaptation under Covariate and Label Shifts"></a>Label Shift Adapter for Test-Time Adaptation under Covariate and Label Shifts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08810">http://arxiv.org/abs/2308.08810</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sunghyun Park, Seunghan Yang, Jaegul Choo, Sungrack Yun</li>
<li>for: 这个论文的目的是要在推断过程中进行测试时适应（Test-time adaptation，TTA），并且能够处理类别分布的不均匀。</li>
<li>methods: 这个论文使用了一种新的类别shift adapter，可以与现有的TTA方法结合，以便在推断过程中处理类别分布的不均匀。</li>
<li>results: 这个论文的实验结果显示， integrating our strategy with TTA approaches leads to substantial performance improvements under the joint presence of label and covariate shifts.<details>
<summary>Abstract</summary>
Test-time adaptation (TTA) aims to adapt a pre-trained model to the target domain in a batch-by-batch manner during inference. While label distributions often exhibit imbalances in real-world scenarios, most previous TTA approaches typically assume that both source and target domain datasets have balanced label distribution. Due to the fact that certain classes appear more frequently in certain domains (e.g., buildings in cities, trees in forests), it is natural that the label distribution shifts as the domain changes. However, we discover that the majority of existing TTA methods fail to address the coexistence of covariate and label shifts. To tackle this challenge, we propose a novel label shift adapter that can be incorporated into existing TTA approaches to deal with label shifts during the TTA process effectively. Specifically, we estimate the label distribution of the target domain to feed it into the label shift adapter. Subsequently, the label shift adapter produces optimal parameters for the target label distribution. By predicting only the parameters for a part of the pre-trained source model, our approach is computationally efficient and can be easily applied, regardless of the model architectures. Through extensive experiments, we demonstrate that integrating our strategy with TTA approaches leads to substantial performance improvements under the joint presence of label and covariate shifts.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Capturing-Popularity-Trends-A-Simplistic-Non-Personalized-Approach-for-Enhanced-Item-Recommendation"><a href="#Capturing-Popularity-Trends-A-Simplistic-Non-Personalized-Approach-for-Enhanced-Item-Recommendation" class="headerlink" title="Capturing Popularity Trends: A Simplistic Non-Personalized Approach for Enhanced Item Recommendation"></a>Capturing Popularity Trends: A Simplistic Non-Personalized Approach for Enhanced Item Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08799">http://arxiv.org/abs/2308.08799</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jingxiaoyi/pare">https://github.com/jingxiaoyi/pare</a></li>
<li>paper_authors: Jiazheng Jing, Yinan Zhang, Xin Zhou, Zhiqi Shen</li>
<li>for: The paper aims to address the issue of item popularity in recommendation systems and proposes a novel approach called Popularity-Aware Recommender (PARE) to make non-personalized recommendations.</li>
<li>methods: PARE consists of four modules: popularity history, temporal impact, periodic impact, and side information, which are combined using an attention layer. The approach explicitly models item popularity and does not rely on personalized user preferences.</li>
<li>results: The paper reports that PARE performs on par or even better than state-of-the-art recommendation methods in extensive experiments. Additionally, integrating PARE with existing recommendation methods significantly improves performance, demonstrating its potential as a complementary component.Here’s the Simplified Chinese text version of the three key points:</li>
<li>for: 这篇论文目标是解决推荐系统中的item Popularity问题，并提出了一种新的方法 Popularity-Aware Recommender (PARE)，以非个人化的方式为用户提供推荐。</li>
<li>methods: PARE包括四个模块： popularity history、temporal impact、 periodic impact 和 side information，这些模块通过注意层结合。该方法Explicitly models item popularity，不依赖个人化用户偏好。</li>
<li>results: 论文Reported that PARE在广泛的实验中表现了与当前领先的推荐方法相当或更好的表现。此外，将PARE与现有的推荐方法集成显著提高了性能， демонстрируя其作为补充组件的潜在价值。<details>
<summary>Abstract</summary>
Recommender systems have been gaining increasing research attention over the years. Most existing recommendation methods focus on capturing users' personalized preferences through historical user-item interactions, which may potentially violate user privacy. Additionally, these approaches often overlook the significance of the temporal fluctuation in item popularity that can sway users' decision-making. To bridge this gap, we propose Popularity-Aware Recommender (PARE), which makes non-personalized recommendations by predicting the items that will attain the highest popularity. PARE consists of four modules, each focusing on a different aspect: popularity history, temporal impact, periodic impact, and side information. Finally, an attention layer is leveraged to fuse the outputs of four modules. To our knowledge, this is the first work to explicitly model item popularity in recommendation systems. Extensive experiments show that PARE performs on par or even better than sophisticated state-of-the-art recommendation methods. Since PARE prioritizes item popularity over personalized user preferences, it can enhance existing recommendation methods as a complementary component. Our experiments demonstrate that integrating PARE with existing recommendation methods significantly surpasses the performance of standalone models, highlighting PARE's potential as a complement to existing recommendation methods. Furthermore, the simplicity of PARE makes it immensely practical for industrial applications and a valuable baseline for future research.
</details>
<details>
<summary>摘要</summary>
“推荐系统在过去几年中得到了不断的研究注意力。现有大多数推荐方法强调用户个人化偏好，可能会侵犯用户隐私。此外，这些方法经常忽略 Item 的时间影响和周期性，这可能会影响用户的决策。为了填补这个 gap，我们提出了 Popularity-Aware Recommender（PARE），这是一个不个人化的推荐方法，可以预测将在未来具有最高 популярность的 Item。PARE 包括四个模块，每个模块都集中在不同的方面：偏好历史、时间影响、周期影响和副资料。最后，我们使用了注意力层来融合四个模块的出力。我们知道这是第一个明确地模型 Item 的受欢迎程度的推荐系统。我们的实验结果显示，PARE 与现有的先进推荐方法相比，在大多数情况下表现相当或甚至更好。由于 PARE 将受欢迎程度放在用户个人化偏好之前，因此它可以增强现有的推荐方法，成为补充性的一部分。我们的实验显示，将 PARE 与现有的推荐方法结合，可以大幅提高推荐效果，强调 PARE 的潜在价值。此外，PARE 的简单性使其在工业应用中具有实际的实用性，并且成为未来研究的良好基础。”
</details></li>
</ul>
<hr>
<h2 id="Joint-Local-Relational-Augmentation-and-Global-Nash-Equilibrium-for-Federated-Learning-with-Non-IID-Data"><a href="#Joint-Local-Relational-Augmentation-and-Global-Nash-Equilibrium-for-Federated-Learning-with-Non-IID-Data" class="headerlink" title="Joint Local Relational Augmentation and Global Nash Equilibrium for Federated Learning with Non-IID Data"></a>Joint Local Relational Augmentation and Global Nash Equilibrium for Federated Learning with Non-IID Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11646">http://arxiv.org/abs/2308.11646</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinting Liao, Chaochao Chen, Weiming Liu, Pengyang Zhou, Huabin Zhu, Shuheng Shen, Weiqiang Wang, Mengling Hu, Yanchao Tan, Xiaolin Zheng</li>
<li>for: 提高 Federated Learning 在非独立 Identical Distribution （non-IID）数据上的效果。</li>
<li>methods: 提出了两个主要模块：本地关系增强（LRA）和全局纳什平衡（GNE），解决了内部和间接客户端不一致的问题。在每个客户端上，LRA 挖掘出不同数据样本之间的相似关系，并通过帮助函数通信来增强少数据样本的表示。在服务器端，GNE 达成了客户端到服务器端的不一致和不同模型偏差的协调，使全局模型在不破坏客户端优化向本地优化的情况下更新。</li>
<li>results: 通过在四个benchmark数据集上进行了广泛的实验，证明了 FedRANE 在非独立 Identical Distribution 数据上提高 Federated Learning 的性能。<details>
<summary>Abstract</summary>
Federated learning (FL) is a distributed machine learning paradigm that needs collaboration between a server and a series of clients with decentralized data. To make FL effective in real-world applications, existing work devotes to improving the modeling of decentralized data with non-independent and identical distributions (non-IID). In non-IID settings, there are intra-client inconsistency that comes from the imbalanced data modeling, and inter-client inconsistency among heterogeneous client distributions, which not only hinders sufficient representation of the minority data, but also brings discrepant model deviations. However, previous work overlooks to tackle the above two coupling inconsistencies together. In this work, we propose FedRANE, which consists of two main modules, i.e., local relational augmentation (LRA) and global Nash equilibrium (GNE), to resolve intra- and inter-client inconsistency simultaneously. Specifically, in each client, LRA mines the similarity relations among different data samples and enhances the minority sample representations with their neighbors using attentive message passing. In server, GNE reaches an agreement among inconsistent and discrepant model deviations from clients to server, which encourages the global model to update in the direction of global optimum without breaking down the clients optimization toward their local optimums. We conduct extensive experiments on four benchmark datasets to show the superiority of FedRANE in enhancing the performance of FL with non-IID data.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 是一种分布式机器学习 paradigma，需要服务器和多个客户端之间的合作，以进行分布式数据的机器学习。为了在实际应用中使 FL 更加有效，现有的工作是针对非独立和非同分布 (non-IID) 的数据进行模型化。在 non-IID  Setting 中，存在客户端内的不均匀数据模型，以及客户端间的不一致性，这不仅会阻碍缺乏表征少数据的充分表示，而且会导致模型偏差的不一致。然而，先前的工作忽视了对上述两种结合不一致的解决。在这种工作中，我们提出了 FedRANE，它包括两个主要模块：本地关系增强 (LRA) 和全局纳什均衡 (GNE)。具体来说，在每个客户端中，LRA 挖掘不同数据样本之间的相似关系，并通过帮助式消息传递增强少数据表示。在服务器端，GNE 达成客户端间的不一致和不一致的模型偏差协议，使全局模型更新方向全局优点，而不是让客户端的优化方向各自的局部优点。我们在四个 benchmark 数据集上进行了广泛的实验，以示 FedRANE 在非独立和非同分布数据上的突出表现。
</details></li>
</ul>
<hr>
<h2 id="Bayesian-polynomial-neural-networks-and-polynomial-neural-ordinary-differential-equations"><a href="#Bayesian-polynomial-neural-networks-and-polynomial-neural-ordinary-differential-equations" class="headerlink" title="Bayesian polynomial neural networks and polynomial neural ordinary differential equations"></a>Bayesian polynomial neural networks and polynomial neural ordinary differential equations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10892">http://arxiv.org/abs/2308.10892</a></li>
<li>repo_url: None</li>
<li>paper_authors: Colby Fronk, Jaewoong Yun, Prashant Singh, Linda Petzold</li>
<li>for: 这 paper 是为了解决科学和工程问题中的方程回归问题。</li>
<li>methods: 这 paper 使用了 polynomial neural networks 和 polynomial neural ODEs 两种现代和强大的方法来回归方程。</li>
<li>results: 这 paper 通过开发和验证 Bayesian 推理方法（包括 Laplace  aproximation、MCMC 采样方法和 variational inference）来解决 noisy 数据问题。<details>
<summary>Abstract</summary>
Symbolic regression with polynomial neural networks and polynomial neural ordinary differential equations (ODEs) are two recent and powerful approaches for equation recovery of many science and engineering problems. However, these methods provide point estimates for the model parameters and are currently unable to accommodate noisy data. We address this challenge by developing and validating the following Bayesian inference methods: the Laplace approximation, Markov Chain Monte Carlo (MCMC) sampling methods, and variational inference. We have found the Laplace approximation to be the best method for this class of problems. Our work can be easily extended to the broader class of symbolic neural networks to which the polynomial neural network belongs.
</details>
<details>
<summary>摘要</summary>
Symbolic regression with polynomial neural networks and polynomial neural ordinary differential equations (ODEs) 是两种最近的和有力的方法，用于解决许多科学和工程问题中的方程回归问题。然而，这些方法只能提供点估计模型参数，并且不能处理噪声数据。我们解决这个挑战，通过开发和验证以下抽象推理方法：拉普拉斯近似法、Markov链 Monte Carlo（MCMC）抽样方法和变分推理法。我们发现，拉普拉斯近似法是这类问题中最佳的方法。我们的工作可以轻松扩展到更广泛的符号神经网络中，其中包括 polynomial neural network。
</details></li>
</ul>
<hr>
<h2 id="Tipping-Point-Forecasting-in-Non-Stationary-Dynamics-on-Function-Spaces"><a href="#Tipping-Point-Forecasting-in-Non-Stationary-Dynamics-on-Function-Spaces" class="headerlink" title="Tipping Point Forecasting in Non-Stationary Dynamics on Function Spaces"></a>Tipping Point Forecasting in Non-Stationary Dynamics on Function Spaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08794">http://arxiv.org/abs/2308.08794</a></li>
<li>repo_url: None</li>
<li>paper_authors: Miguel Liu-Schiaffini, Clare E. Singer, Nikola Kovachki, Tapio Schneider, Kamyar Azizzadenesheli, Anima Anandkumar</li>
<li>for: 本研究旨在预测非站立异常变化的演化，例如气候变化导致云覆盖减少的 climatological tipping point。</li>
<li>methods: 本研究使用一种新的循环神经网络算法（RNO）来学习函数空间的映射。通过训练 RNO 只使用前段动力学的数据，然后使用不确定性基于预测法来检测未来的至点。</li>
<li>results: 我们在非站立 ordinary 和 partial differential equations 上测试了我们的提议方法，并在气候至点预测中应用了我们的方法。我们的实验结果表明，即使只使用部分或 approximate physics constraints，仍可以准确预测未来的至点。<details>
<summary>Abstract</summary>
Tipping points are abrupt, drastic, and often irreversible changes in the evolution of non-stationary and chaotic dynamical systems. For instance, increased greenhouse gas concentrations are predicted to lead to drastic decreases in low cloud cover, referred to as a climatological tipping point. In this paper, we learn the evolution of such non-stationary dynamical systems using a novel recurrent neural operator (RNO), which learns mappings between function spaces. After training RNO on only the pre-tipping dynamics, we employ it to detect future tipping points using an uncertainty-based approach. In particular, we propose a conformal prediction framework to forecast tipping points by monitoring deviations from physics constraints (such as conserved quantities and partial differential equations), enabling forecasting of these abrupt changes along with a rigorous measure of uncertainty. We illustrate our proposed methodology on non-stationary ordinary and partial differential equations, such as the Lorenz-63 and Kuramoto-Sivashinsky equations. We also apply our methods to forecast a climate tipping point in stratocumulus cloud cover. In our experiments, we demonstrate that even partial or approximate physics constraints can be used to accurately forecast future tipping points.
</details>
<details>
<summary>摘要</summary>
“衰点”是指不断、悬崖式、常常不可逆转变的非站点动力系统的演化。例如，增加绿house气体浓度可能导致低云覆盖率减少，这被称为气候学衰点。在这篇论文中，我们使用一种新的循环神经操作（RNO）来学习函数空间之间的映射。我们在只有前期衰点动力学的训练下使用RNO来检测未来衰点。特别是，我们提出了一种准确预测衰点的极限预测框架，通过监测物理约束（如保守量和部分偏微分方程）的偏差来预测这些急剧变化。我们在非站点常微分方程和部分偏微分方程中应用我们的方法，并在气候衰点中预测云层覆盖率的变化。在我们的实验中，我们表明了只需要部分或 aproximate的物理约束可以准确预测未来衰点。
</details></li>
</ul>
<hr>
<h2 id="Federated-Reinforcement-Learning-for-Electric-Vehicles-Charging-Control-on-Distribution-Networks"><a href="#Federated-Reinforcement-Learning-for-Electric-Vehicles-Charging-Control-on-Distribution-Networks" class="headerlink" title="Federated Reinforcement Learning for Electric Vehicles Charging Control on Distribution Networks"></a>Federated Reinforcement Learning for Electric Vehicles Charging Control on Distribution Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08792">http://arxiv.org/abs/2308.08792</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junkai Qian, Yuning Jiang, Xin Liu, Qing Wang, Ting Wang, Yuanming Shi, Wei Chen<br>for: This paper aims to address the challenges of power grid stability and driver privacy in electric vehicle (EV) charging control by proposing a novel approach that combines multi-EV charging&#x2F;discharging with a radial distribution network (RDN) operating under optimal power flow (OPF).methods: The proposed approach uses a mathematical model to describe the RDN load and formulates the EV charging control problem as a Markov Decision Process (MDP) to find an optimal charging control strategy that balances V2G profits, RDN load, and driver anxiety. A federated deep reinforcement learning algorithm named FedSAC is further proposed to effectively learn the optimal EV charging control strategy.results: The comprehensive simulation results demonstrate the effectiveness and superiority of the proposed algorithm in terms of the diversity of the charging control strategy, the power fluctuations on RDN, the convergence efficiency, and the generalization ability.<details>
<summary>Abstract</summary>
With the growing popularity of electric vehicles (EVs), maintaining power grid stability has become a significant challenge. To address this issue, EV charging control strategies have been developed to manage the switch between vehicle-to-grid (V2G) and grid-to-vehicle (G2V) modes for EVs. In this context, multi-agent deep reinforcement learning (MADRL) has proven its effectiveness in EV charging control. However, existing MADRL-based approaches fail to consider the natural power flow of EV charging/discharging in the distribution network and ignore driver privacy. To deal with these problems, this paper proposes a novel approach that combines multi-EV charging/discharging with a radial distribution network (RDN) operating under optimal power flow (OPF) to distribute power flow in real time. A mathematical model is developed to describe the RDN load. The EV charging control problem is formulated as a Markov Decision Process (MDP) to find an optimal charging control strategy that balances V2G profits, RDN load, and driver anxiety. To effectively learn the optimal EV charging control strategy, a federated deep reinforcement learning algorithm named FedSAC is further proposed. Comprehensive simulation results demonstrate the effectiveness and superiority of our proposed algorithm in terms of the diversity of the charging control strategy, the power fluctuations on RDN, the convergence efficiency, and the generalization ability.
</details>
<details>
<summary>摘要</summary>
随着电动汽车（EV）的普及，维护电力网络稳定性已成为一个主要挑战。为解决这个问题，EV充电控制策略已被开发来管理电动汽车的充电和充电模式之间的转换。在这个上下文中，多代理深度学习（MADRL）已经证明其在EV充电控制中的效iveness。然而，现有的MADRL基本方法忽略了电动汽车充电/充电的自然流向和驾驶员隐私。为解决这些问题，本文提出了一种新的方法，即将多个电动汽车充电/充电与径向分布网络（RDN）在优化电力流动（OPF）下进行分布式充电控制。一个数学模型被开发来描述RDN负荷。EV充电控制问题被转化为Markov决策过程（MDP），以找到一个优化充电控制策略，折衔V2G利润、RDN负荷和驾驶员焦虑。为有效地学习优化EV充电控制策略，一种名为FedSAC的联邦深度学习算法被进一步提出。 comprehensive simulation results demonstrate the effectiveness and superiority of our proposed algorithm in terms of the diversity of the charging control strategy, the power fluctuations on RDN, the convergence efficiency, and the generalization ability.
</details></li>
</ul>
<hr>
<h2 id="APPFLx-Providing-Privacy-Preserving-Cross-Silo-Federated-Learning-as-a-Service"><a href="#APPFLx-Providing-Privacy-Preserving-Cross-Silo-Federated-Learning-as-a-Service" class="headerlink" title="APPFLx: Providing Privacy-Preserving Cross-Silo Federated Learning as a Service"></a>APPFLx: Providing Privacy-Preserving Cross-Silo Federated Learning as a Service</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08786">http://arxiv.org/abs/2308.08786</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zilinghan Li, Shilan He, Pranshu Chaturvedi, Trung-Hieu Hoang, Minseok Ryu, E. A. Huerta, Volodymyr Kindratenko, Jordan Fuhrman, Maryellen Giger, Ryan Chard, Kibaek Kim, Ravi Madduri</li>
<li>for: 本研究旨在提供一个 Privacy-Preserving Federated Learning (PPFL) 平台，帮助无需分享敏感数据就可以协同训练 Robust 和 Generalized Machine Learning (ML) 模型。</li>
<li>methods: 本研究使用 Globus 身份验证技术，让用户轻松地邀请可靠的合作者参与 PPFL，并实现了一些同步和异步 Federated Learning (FL) 算法，简化了 FL 实验启动过程，并允许域专家和 ML 实践者轻松地协调和评估 cross-silo FL。</li>
<li>results: 本研究提供了一个名为 APPFLx 的ready-to-use 平台，可以帮助域专家和 ML 实践者轻松地使用 PPFL 技术进行数据 Privacy 保护和模型训练。APPFLx 在线可以在 <a target="_blank" rel="noopener" href="https://appflx.link/">https://appflx.link</a> 上查看。<details>
<summary>Abstract</summary>
Cross-silo privacy-preserving federated learning (PPFL) is a powerful tool to collaboratively train robust and generalized machine learning (ML) models without sharing sensitive (e.g., healthcare of financial) local data. To ease and accelerate the adoption of PPFL, we introduce APPFLx, a ready-to-use platform that provides privacy-preserving cross-silo federated learning as a service. APPFLx employs Globus authentication to allow users to easily and securely invite trustworthy collaborators for PPFL, implements several synchronous and asynchronous FL algorithms, streamlines the FL experiment launch process, and enables tracking and visualizing the life cycle of FL experiments, allowing domain experts and ML practitioners to easily orchestrate and evaluate cross-silo FL under one platform. APPFLx is available online at https://appflx.link
</details>
<details>
<summary>摘要</summary>
cross-silo隐私保护联合学习（PPFL）是一种强大的工具，可以无需分享敏感数据（如医疗或金融）来训练robust和通用机器学习（ML）模型。为了使PPFL更加容易采用，我们引入了APPFLx，一个快速启用的平台，提供隐私保护跨存储 Federated Learning（FL）服务。APPFLx使用Globus身份验证，让用户轻松地邀请可靠的合作者参与PPFL，实现了同步和异步FL算法，简化了FL实验启动过程，并允许域专家和机器学习实践者轻松地进行跨存储FL的导航和评估。APPFLx在线可以在https://appflx.link上访问。
</details></li>
</ul>
<hr>
<h2 id="Knowledge-inspired-Subdomain-Adaptation-for-Cross-Domain-Knowledge-Transfer"><a href="#Knowledge-inspired-Subdomain-Adaptation-for-Cross-Domain-Knowledge-Transfer" class="headerlink" title="Knowledge-inspired Subdomain Adaptation for Cross-Domain Knowledge Transfer"></a>Knowledge-inspired Subdomain Adaptation for Cross-Domain Knowledge Transfer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09724">http://arxiv.org/abs/2308.09724</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liyue Chen, Linian Wang, Jinyu Xu, Shuai Chen, Weiqiang Wang, Wenbiao Zhao, Qiyu Li, Leye Wang</li>
<li>for: 这篇论文是为了提出一个新的专门领域适应方法，以便在不同领域之间进行精确的预测。</li>
<li>methods: 这篇论文使用了一个名为“知识驱动的子领域适应”（KISA）的新方法，它可以在不同领域之间进行精确的预测。</li>
<li>results: 实验结果显示，KISA在骗案探测和交通需求预测等任务上取得了很好的结果。<details>
<summary>Abstract</summary>
Most state-of-the-art deep domain adaptation techniques align source and target samples in a global fashion. That is, after alignment, each source sample is expected to become similar to any target sample. However, global alignment may not always be optimal or necessary in practice. For example, consider cross-domain fraud detection, where there are two types of transactions: credit and non-credit. Aligning credit and non-credit transactions separately may yield better performance than global alignment, as credit transactions are unlikely to exhibit patterns similar to non-credit transactions. To enable such fine-grained domain adaption, we propose a novel Knowledge-Inspired Subdomain Adaptation (KISA) framework. In particular, (1) We provide the theoretical insight that KISA minimizes the shared expected loss which is the premise for the success of domain adaptation methods. (2) We propose the knowledge-inspired subdomain division problem that plays a crucial role in fine-grained domain adaption. (3) We design a knowledge fusion network to exploit diverse domain knowledge. Extensive experiments demonstrate that KISA achieves remarkable results on fraud detection and traffic demand prediction tasks.
</details>
<details>
<summary>摘要</summary>
Current state-of-the-art deep domain adaptation methods align source and target samples globally, meaning that each source sample should become similar to any target sample after alignment. However, this global alignment may not always be optimal or necessary in practice. For example, in cross-domain fraud detection, there are two types of transactions: credit and non-credit. Aligning credit and non-credit transactions separately may lead to better performance than global alignment, as credit transactions are unlikely to exhibit patterns similar to non-credit transactions. To enable fine-grained domain adaptation, we propose a novel Knowledge-Inspired Subdomain Adaptation (KISA) framework. Specifically, (1) we provide theoretical insight that KISA minimizes the shared expected loss, which is the premise of domain adaptation methods. (2) we propose a knowledge-inspired subdomain division problem that plays a crucial role in fine-grained domain adaption. (3) we design a knowledge fusion network to exploit diverse domain knowledge. Extensive experiments show that KISA achieves remarkable results on fraud detection and traffic demand prediction tasks.
</details></li>
</ul>
<hr>
<h2 id="Environment-Diversification-with-Multi-head-Neural-Network-for-Invariant-Learning"><a href="#Environment-Diversification-with-Multi-head-Neural-Network-for-Invariant-Learning" class="headerlink" title="Environment Diversification with Multi-head Neural Network for Invariant Learning"></a>Environment Diversification with Multi-head Neural Network for Invariant Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08778">http://arxiv.org/abs/2308.08778</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/joe0123/EDNIL">https://github.com/joe0123/EDNIL</a></li>
<li>paper_authors: Bo-Wei Huang, Keng-Te Liao, Chang-Sheng Kao, Shou-De Lin</li>
<li>for: 提高模型对不同分布的鲁棒性</li>
<li>methods: 提出了一种基于多头神经网络吸收数据偏见的不变学习框架（EDNIL）</li>
<li>results: 实验表明，使用EDNIL框架可以提高模型对不同分布的鲁棒性，而无需先知环境或强ASSUMEptions about预训练模型。<details>
<summary>Abstract</summary>
Neural networks are often trained with empirical risk minimization; however, it has been shown that a shift between training and testing distributions can cause unpredictable performance degradation. On this issue, a research direction, invariant learning, has been proposed to extract invariant features insensitive to the distributional changes. This work proposes EDNIL, an invariant learning framework containing a multi-head neural network to absorb data biases. We show that this framework does not require prior knowledge about environments or strong assumptions about the pre-trained model. We also reveal that the proposed algorithm has theoretical connections to recent studies discussing properties of variant and invariant features. Finally, we demonstrate that models trained with EDNIL are empirically more robust against distributional shifts.
</details>
<details>
<summary>摘要</summary>
神经网络经常使用隐式风险最小化进行训练;然而，已经证明了在训练和测试分布之间的偏移会导致性能下降。为解决这一问题，一种研究方向——抗变异学习——已经被提出，以抽取不受分布变化影响的特征。本研究提出了EDNIL框架，包括多头神经网络来吸收数据偏见。我们表明，这种框架不需要先知环境或强制假设先训练模型。我们还揭示了该算法与最近的研究中关于变异和不变特征的性质有许多理论连接。最后，我们实证表明使用EDNIL训练的模型在分布偏移下的表现更加稳定。
</details></li>
</ul>
<hr>
<h2 id="Differential-Privacy-Linguistic-Fairness-and-Training-Data-Influence-Impossibility-and-Possibility-Theorems-for-Multilingual-Language-Models"><a href="#Differential-Privacy-Linguistic-Fairness-and-Training-Data-Influence-Impossibility-and-Possibility-Theorems-for-Multilingual-Language-Models" class="headerlink" title="Differential Privacy, Linguistic Fairness, and Training Data Influence: Impossibility and Possibility Theorems for Multilingual Language Models"></a>Differential Privacy, Linguistic Fairness, and Training Data Influence: Impossibility and Possibility Theorems for Multilingual Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08774">http://arxiv.org/abs/2308.08774</a></li>
<li>repo_url: None</li>
<li>paper_authors: Phillip Rust, Anders Søgaard</li>
<li>for: 这些研究目的是为了实现多语言通用性和压缩，以便将模型转移到大量（可能未经见）语言中。</li>
<li>methods: 这些模型使用了 differential privacy，以保证隐私。</li>
<li>results: 研究发现，多语言压缩和语言公正性可以同时满足，但是对于训练数据的影响稀缺性和隐私保证之间存在矛盾。<details>
<summary>Abstract</summary>
Language models such as mBERT, XLM-R, and BLOOM aim to achieve multilingual generalization or compression to facilitate transfer to a large number of (potentially unseen) languages. However, these models should ideally also be private, linguistically fair, and transparent, by relating their predictions to training data. Can these requirements be simultaneously satisfied? We show that multilingual compression and linguistic fairness are compatible with differential privacy, but that differential privacy is at odds with training data influence sparsity, an objective for transparency. We further present a series of experiments on two common NLP tasks and evaluate multilingual compression and training data influence sparsity under different privacy guarantees, exploring these trade-offs in more detail. Our results suggest that we need to develop ways to jointly optimize for these objectives in order to find practical trade-offs.
</details>
<details>
<summary>摘要</summary>
语模型如mBERT、XLM-R和BLOOM目的是实现多语言通用或压缩，以便转移至大量（可能未看过）语言。但这些模型应该也是私人、语言公平和透明的，通过与训练数据的关联来预测。可以这些需求同时满足吗？我们表明，多语言压缩和语言公平是与数据隐私相容的，但数据隐私与训练数据影响简洁矛盾。我们进一步对两个常见的NLP任务进行了试验，评估多语言压缩和训练数据影响简洁在不同的隐私保证下，进一步探索这些贸易的细节。我们的结果表明，我们需要开发方法来同时优化这些目标，以寻找实际的贸易。
</details></li>
</ul>
<hr>
<h2 id="Sensor-Fusion-by-Spatial-Encoding-for-Autonomous-Driving"><a href="#Sensor-Fusion-by-Spatial-Encoding-for-Autonomous-Driving" class="headerlink" title="Sensor Fusion by Spatial Encoding for Autonomous Driving"></a>Sensor Fusion by Spatial Encoding for Autonomous Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10707">http://arxiv.org/abs/2308.10707</a></li>
<li>repo_url: None</li>
<li>paper_authors: Quoc-Vinh Lai-Dang, Jihui Lee, Bumgeun Park, Dongsoo Har</li>
<li>for: 本研究旨在提出一种用于摄像头和激光雷达数据融合的方法，以提高自动驾驶和机器人感知系统的性能。</li>
<li>methods: 该方法使用Transformer模块在多个分辨率进行融合，以确保地面和高空的上下文关系的有效组合。</li>
<li>results: 对于两个难度最高的 benchmark，提出的方法在训练后显示出了明显的改善，与之前的方法相比，在驾驶和违法分数上分别提高8%和19%。<details>
<summary>Abstract</summary>
Sensor fusion is critical to perception systems for task domains such as autonomous driving and robotics. Recently, the Transformer integrated with CNN has demonstrated high performance in sensor fusion for various perception tasks. In this work, we introduce a method for fusing data from camera and LiDAR. By employing Transformer modules at multiple resolutions, proposed method effectively combines local and global contextual relationships. The performance of the proposed method is validated by extensive experiments with two adversarial benchmarks with lengthy routes and high-density traffics. The proposed method outperforms previous approaches with the most challenging benchmarks, achieving significantly higher driving and infraction scores. Compared with TransFuser, it achieves 8% and 19% improvement in driving scores for the Longest6 and Town05 Long benchmarks, respectively.
</details>
<details>
<summary>摘要</summary>
感知系统中的感知融合是自动驾驶和机器人等任务领域的关键技术。最近，由Transformer搭配CNN的方法在不同的感知任务中表现出了高水平的性能。在这篇文章中，我们介绍了一种将摄像头和LiDAR数据进行融合的方法。通过在多个分辨率下使用Transformer模块，我们的方法可以有效地组合本地和全局的contextual关系。我们的方法的性能被证明了通过对两个挑战性 benchmarks进行了广泛的实验。与之前的方法相比，我们的方法在 longest6和town05 Long benchmarks上的驾驶和违法分数都表现出了显著的提高，相比TransFuser，我们的方法在Longest6 benchmark上提高了8%和19%的驾驶分数。
</details></li>
</ul>
<hr>
<h2 id="Neurological-Prognostication-of-Post-Cardiac-Arrest-Coma-Patients-Using-EEG-Data-A-Dynamic-Survival-Analysis-Framework-with-Competing-Risks"><a href="#Neurological-Prognostication-of-Post-Cardiac-Arrest-Coma-Patients-Using-EEG-Data-A-Dynamic-Survival-Analysis-Framework-with-Competing-Risks" class="headerlink" title="Neurological Prognostication of Post-Cardiac-Arrest Coma Patients Using EEG Data: A Dynamic Survival Analysis Framework with Competing Risks"></a>Neurological Prognostication of Post-Cardiac-Arrest Coma Patients Using EEG Data: A Dynamic Survival Analysis Framework with Competing Risks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11645">http://arxiv.org/abs/2308.11645</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaobin Shen, Jonathan Elmer, George H. Chen<br>for: 这个研究旨在预测响心止痛后昏迷 patients的神经LOGICAL OUTCOMES，帮助医疗决策。methods: 该研究提出了基于EEG数据的神经LOGICAL PROGNOSIS的动态框架，可以随着更多的EEG数据得到更加准确的预测。该框架使用任何支持竞争风险的动态存存分析模型，并且可以针对不同的训练病人的可用EEG时间序列进行不同的预测。results: 研究发现，使用static特征和最新一小时的EEG数据的Fine和Gray模型在准确度上与使用大量EEG数据的Dynamic-DeepHit模型相当竞争，而且在减少模型简化后，三种竞争风险模型中的模型可以learn更多信息而且准确性至少相当。<details>
<summary>Abstract</summary>
Patients resuscitated from cardiac arrest who enter a coma are at high risk of death. Forecasting neurological outcomes of these patients (the task of neurological prognostication) could help with treatment decisions. In this paper, we propose, to the best of our knowledge, the first dynamic framework for neurological prognostication of post-cardiac-arrest comatose patients using EEG data: our framework makes predictions for a patient over time as more EEG data become available, and different training patients' available EEG time series could vary in length. Predictions are phrased in terms of either time-to-event outcomes (time-to-awakening or time-to-death) or as the patient's probability of awakening or of dying across multiple time horizons. Our framework uses any dynamic survival analysis model that supports competing risks in the form of estimating patient-level cumulative incidence functions. We consider three competing risks as to what happens first to a patient: awakening, being withdrawn from life-sustaining therapies (and thus deterministically dying), or dying (by other causes). We demonstrate our framework by benchmarking three existing dynamic survival analysis models that support competing risks on a real dataset of 922 patients. Our main experimental findings are that: (1) the classical Fine and Gray model which only uses a patient's static features and summary statistics from the patient's latest hour's worth of EEG data is highly competitive, achieving accuracy scores as high as the recently developed Dynamic-DeepHit model that uses substantially more of the patient's EEG data; and (2) in an ablation study, we show that our choice of modeling three competing risks results in a model that is at least as accurate while learning more information than simpler models (using two competing risks or a standard survival analysis setup with no competing risks).
</details>
<details>
<summary>摘要</summary>
患者从心肺停止急救后入 coma 的风险很高，预测神经学结果可以帮助医疗决策。在这篇论文中，我们提出了，到目前为止最先进的动态推测框架，使用 EEG 数据预测患者后期神经学结果：我们的框架可以随着更多的 EEG 数据提供预测，不同的训练患者可以有不同的 EEG 时间序列长度。预测是基于时间到事件结果（时间到唤醒或时间到死亡）或患者在多个时间水平上的唤醒或死亡概率。我们的框架使用任何支持竞争风险的动态存生分析模型， estimate 患者级别累积发生函数。我们考虑了三种竞争风险：患者会于何时醒来，被撤销生命维持治疗（然后确定性死亡），或者死亡（由其他原因）。我们在实际数据集上进行了比较三种现有的动态存生分析模型，我们的主要实验结果是：（1）经典的 Fine 和 Gray 模型，只使用患者的静态特征和最近一小时的 EEG 数据，能够与最近开发的 Dynamic-DeepHit 模型匹配精度，而且（2）在剖析研究中，我们发现，我们选择了三种竞争风险的模型，可以提供至少相当精度的预测，同时学习更多的信息。
</details></li>
</ul>
<hr>
<h2 id="Explainable-AI-for-tool-wear-prediction-in-turning"><a href="#Explainable-AI-for-tool-wear-prediction-in-turning" class="headerlink" title="Explainable AI for tool wear prediction in turning"></a>Explainable AI for tool wear prediction in turning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08765">http://arxiv.org/abs/2308.08765</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saleh Valizadeh Sotubadi, Rui Liu, Vinh Neguyen<br>for: 这份研究旨在发展一个可解释人工智能（XAI）框架，以便为切割过程中工具损坏预测提供人类理解的解决方案。methods: 本研究使用了一个随机森林算法作为监督式机器学习（ML）分类器，并使用加速度、声学、温度和螺旋速度等参数进行训练和二分类 classification。results: 经过训练后，使用了Shapley准则来解释训练好的ML分类器预测的结果，并确定工具温度是决定切割工具可用或失效的最重要的输入参数。因此，本研究显示了XAI可以提供磨削操作员诊断和理解复杂的ML分类器预测工具损坏的能力。<details>
<summary>Abstract</summary>
This research aims develop an Explainable Artificial Intelligence (XAI) framework to facilitate human-understandable solutions for tool wear prediction during turning. A random forest algorithm was used as the supervised Machine Learning (ML) classifier for training and binary classification using acceleration, acoustics, temperature, and spindle speed during the orthogonal tube turning process as input features. The ML classifier was used to predict the condition of the tool after the cutting process, which was determined in a binary class form indicating if the cutting tool was available or failed. After the training process, the Shapley criterion was used to explain the predictions of the trained ML classifier. Specifically, the significance of each input feature in the decision-making and classification was identified to explain the reasoning of the ML classifier predictions. After implementing the Shapley criterion on all testing datasets, the tool temperature was identified as the most significant feature in determining the classification of available versus failed cutting tools. Hence, this research demonstrates capability of XAI to provide machining operators the ability to diagnose and understand complex ML classifiers in prediction of tool wear.
</details>
<details>
<summary>摘要</summary>
After training the ML classifier, the Shapley criterion was used to explain the predictions. Specifically, the significance of each input feature in the decision-making and classification was identified to explain the reasoning behind the ML classifier's predictions. The results showed that tool temperature was the most significant feature in determining the classification of available versus failed cutting tools.This research demonstrates the capability of XAI to provide machining operators with the ability to diagnose and understand complex ML classifiers in predicting tool wear. By using the Shapley criterion to explain the predictions, the research provides a human-understandable explanation of the decision-making process, allowing operators to better understand and trust the ML classifier's predictions.
</details></li>
</ul>
<hr>
<h2 id="Efficient-Commercial-Bank-Customer-Credit-Risk-Assessment-Based-on-LightGBM-and-Feature-Engineering"><a href="#Efficient-Commercial-Bank-Customer-Credit-Risk-Assessment-Based-on-LightGBM-and-Feature-Engineering" class="headerlink" title="Efficient Commercial Bank Customer Credit Risk Assessment Based on LightGBM and Feature Engineering"></a>Efficient Commercial Bank Customer Credit Risk Assessment Based on LightGBM and Feature Engineering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08762">http://arxiv.org/abs/2308.08762</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanjie Sun, Zhike Gong, Quan Shi, Lin Chen</li>
<li>for: 本研究主要是为了帮助商业银行控制债务风险，通过使用LightGBM算法建立分类器，判断客户是否有可能 Default on 债务。</li>
<li>methods: 本研究使用了LightGBM算法，并进行了特征工程，如处理缺失值、编码、不均衡样本等，以提高机器学习效果。</li>
<li>results: 本研究构建了新的特征属性，使得分类器的准确率达到0.734，AUC达到0.772，超过了基于同一数据集的许多分类器。这些结果可以为商业银行的债务授予提供参考，也可以为其他相似研究提供特征处理的想法。<details>
<summary>Abstract</summary>
Effective control of credit risk is a key link in the steady operation of commercial banks. This paper is mainly based on the customer information dataset of a foreign commercial bank in Kaggle, and we use LightGBM algorithm to build a classifier to classify customers, to help the bank judge the possibility of customer credit default. This paper mainly deals with characteristic engineering, such as missing value processing, coding, imbalanced samples, etc., which greatly improves the machine learning effect. The main innovation of this paper is to construct new feature attributes on the basis of the original dataset so that the accuracy of the classifier reaches 0.734, and the AUC reaches 0.772, which is more than many classifiers based on the same dataset. The model can provide some reference for commercial banks' credit granting, and also provide some feature processing ideas for other similar studies.
</details>
<details>
<summary>摘要</summary>
效果控制信贷风险是商业银行稳定运营的关键链接。本文主要基于一家外国商业银行的客户信息数据集在Kaggle上，使用LightGBM算法建立分类器，以帮助银行判断客户债务 default 的可能性。本文主要关注特征工程，如排除 missing value、编码、不均衡样本等，这些改进了机器学习效果。本文的主要创新在于在原始数据集基础上构建新的特征属性，使分类器的准确率达0.734，AUC达0.772，超过了同基据集上的许多分类器。该模型可以为商业银行债务赐与提供参考，同时也可以为其他相似研究提供特征处理的想法。Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="PMET-Precise-Model-Editing-in-a-Transformer"><a href="#PMET-Precise-Model-Editing-in-a-Transformer" class="headerlink" title="PMET: Precise Model Editing in a Transformer"></a>PMET: Precise Model Editing in a Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08742">http://arxiv.org/abs/2308.08742</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xpq-tech/pmet">https://github.com/xpq-tech/pmet</a></li>
<li>paper_authors: Xiaopeng Li, Shasha Li, Shezheng Song, Jing Yang, Jun Ma, Jie Yu</li>
<li>for: 提高模型编辑技术的性能，减少模型更新的成本。</li>
<li>methods: 分析隐藏状态的多头自注意力（MHSA）和循环神经网络（FFN），并同时优化它们的隐藏状态，以便准确地更新FFN的参数。</li>
<li>results: 在COUNTERFACT和zsRE dataset上达到了state-of-the-art的性能。<details>
<summary>Abstract</summary>
Model editing techniques modify a minor proportion of knowledge in Large Language Models (LLMs) at a relatively low cost, which have demonstrated notable success. Existing methods assume Transformer Layer (TL) hidden states are values of key-value memories of the Feed-Forward Network (FFN). They usually optimize the TL hidden states to memorize target knowledge and use it to update the weights of the FFN in LLMs. However, the information flow of TL hidden states comes from three parts: Multi-Head Self-Attention (MHSA), FFN, and residual connections. Existing methods neglect the fact that the TL hidden states contains information not specifically required for FFN. Consequently, the performance of model editing decreases. To achieve more precise model editing, we analyze hidden states of MHSA and FFN, finding that MHSA encodes certain general knowledge extraction patterns. This implies that MHSA weights do not require updating when new knowledge is introduced. Based on above findings, we introduce PMET, which simultaneously optimizes Transformer Component (TC, namely MHSA and FFN) hidden states, while only using the optimized TC hidden states of FFN to precisely update FFN weights. Our experiments demonstrate that PMET exhibits state-of-the-art performance on both the COUNTERFACT and zsRE datasets. Our ablation experiments substantiate the effectiveness of our enhancements, further reinforcing the finding that the MHSA encodes certain general knowledge extraction patterns and indicating its storage of a small amount of factual knowledge. Our code is available at https://github.com/xpq-tech/PMET.git.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）的修改技术可以轻松地修改一小部分知识，并且已经获得了可观的成功。现有的方法假设transformer层（TL）的隐藏状态是Feed-Forward Network（FFN）中的钥匙值内存。它们通常将TL隐藏状态优化为记忆target知识，并使用这些TL隐藏状态来更新FFN的重量。但是，TL隐藏状态的信息来源来自三个部分：多头自我对话（MHSA）、FFN和复合连接。现有的方法忽略了TL隐藏状态中包含的信息不是FFN特定所需的。因此，模型修改的性能受到影响。为了更精确地进行模型修改，我们进行了隐藏状态分析，发现MHSA对于某些一般知识提取模式具有编码功能。这意味着MHSA的重量不需要更新当新知识引入。基于以上发现，我们提出了PMET，它同时优化transformer ком ponent（TC，即MHSA和FFN）的隐藏状态，仅使用优化TC隐藏状态的FFN来精确地更新FFN的重量。我们的实验显示PMET在COUNTERFACT和zsRE datasets上展示了顶尖的表现。我们的剥离实验证明了我们的改进的有效性，进一步证明MHSA对于一些一般知识提取模式具有编码功能，并且它储存了一小部分的事实知识。我们的代码可以在https://github.com/xpq-tech/PMET.git中找到。
</details></li>
</ul>
<hr>
<h2 id="ReProHRL-Towards-Multi-Goal-Navigation-in-the-Real-World-using-Hierarchical-Agents"><a href="#ReProHRL-Towards-Multi-Goal-Navigation-in-the-Real-World-using-Hierarchical-Agents" class="headerlink" title="ReProHRL: Towards Multi-Goal Navigation in the Real World using Hierarchical Agents"></a>ReProHRL: Towards Multi-Goal Navigation in the Real World using Hierarchical Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08737">http://arxiv.org/abs/2308.08737</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tejaswini Manjunath, Mozhgan Navardi, Prakhar Dixit, Bharat Prakash, Tinoosh Mohsenin</li>
<li>for: 本研究旨在提出一种能够在实际世界中进行多目标导航的彩虹RL算法，以提高现有RL算法在实际环境中的适用性。</li>
<li>methods: 该方法利用了彩虹RL算法，并在训练过程中使用了对象检测器作为预处理步骤，以学习多目标导航并在实际世界中传递。</li>
<li>results: 实验结果显示，提出的Ready for Production Hierarchical RL（ReProHRL）方法在模拟环境和真实世界环境中都有较好的性能，比基eline方法提高18%和5%。<details>
<summary>Abstract</summary>
Robots have been successfully used to perform tasks with high precision. In real-world environments with sparse rewards and multiple goals, learning is still a major challenge and Reinforcement Learning (RL) algorithms fail to learn good policies. Training in simulation environments and then fine-tuning in the real world is a common approach. However, adapting to the real-world setting is a challenge. In this paper, we present a method named Ready for Production Hierarchical RL (ReProHRL) that divides tasks with hierarchical multi-goal navigation guided by reinforcement learning. We also use object detectors as a pre-processing step to learn multi-goal navigation and transfer it to the real world. Empirical results show that the proposed ReProHRL method outperforms the state-of-the-art baseline in simulation and real-world environments in terms of both training time and performance. Although both methods achieve a 100% success rate in a simple environment for single goal-based navigation, in a more complex environment and multi-goal setting, the proposed method outperforms the baseline by 18% and 5%, respectively. For the real-world implementation and proof of concept demonstration, we deploy the proposed method on a nano-drone named Crazyflie with a front camera to perform multi-goal navigation experiments.
</details>
<details>
<summary>摘要</summary>
роботы已经成功地完成了高精度任务。在实际环境中，受限回报和多个目标是学习的主要挑战，并且使用强化学习（RL）算法学习良好策略仍然是一个挑战。训练在模拟环境中并在实际环境中细化是一个常见的方法。然而，适应实际环境的挑战仍然存在。在这篇论文中，我们提出了一种名为Ready for Production Hierarchical RL（ReProHRL）的方法，该方法将任务分解为层次多目标导航，并使用强化学习来导航。我们还使用对象检测器作为预处理步骤，以学习多目标导航并将其转移到实际世界。我们的实验结果表明，提议的ReProHRL方法在模拟环境和实际世界环境中都能够超越基准值。虽然两个方法在简单环境中完成单目标导航时都达到100%的成功率，但在更复杂的环境和多目标设定下，提议的方法在基准值的18%和5%之上。为了证明实际应用和概念示范，我们在一架名为Crazyflie的奈米飞行器上部署了提议的方法，并使用前置摄像头完成多目标导航实验。
</details></li>
</ul>
<hr>
<h2 id="On-the-Effectiveness-of-Log-Representation-for-Log-based-Anomaly-Detection"><a href="#On-the-Effectiveness-of-Log-Representation-for-Log-based-Anomaly-Detection" class="headerlink" title="On the Effectiveness of Log Representation for Log-based Anomaly Detection"></a>On the Effectiveness of Log Representation for Log-based Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08736">http://arxiv.org/abs/2308.08736</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mooselab/suppmaterial-logrepforanomalydetection">https://github.com/mooselab/suppmaterial-logrepforanomalydetection</a></li>
<li>paper_authors: Xingfang Wu, Heng Li, Foutse Khomh</li>
<li>for: 本研究旨在比较常用的日志表示技术，以便选择最适合自动日志分析过程中的日志表示技术。</li>
<li>methods: 本研究使用了六种日志表示技术，并与七种机器学习模型和四个公共日志集（HDFS、BGL、Spirit和Thunderbird）进行了比较。</li>
<li>results: 实验结果显示，不同的日志表示技术对下游模型的性能产生了不同的影响，并提供了一些启示式指南 для未来的研究人员和开发者。<details>
<summary>Abstract</summary>
Logs are an essential source of information for people to understand the running status of a software system. Due to the evolving modern software architecture and maintenance methods, more research efforts have been devoted to automated log analysis. In particular, machine learning (ML) has been widely used in log analysis tasks. In ML-based log analysis tasks, converting textual log data into numerical feature vectors is a critical and indispensable step. However, the impact of using different log representation techniques on the performance of the downstream models is not clear, which limits researchers and practitioners' opportunities of choosing the optimal log representation techniques in their automated log analysis workflows. Therefore, this work investigates and compares the commonly adopted log representation techniques from previous log analysis research. Particularly, we select six log representation techniques and evaluate them with seven ML models and four public log datasets (i.e., HDFS, BGL, Spirit and Thunderbird) in the context of log-based anomaly detection. We also examine the impacts of the log parsing process and the different feature aggregation approaches when they are employed with log representation techniques. From the experiments, we provide some heuristic guidelines for future researchers and developers to follow when designing an automated log analysis workflow. We believe our comprehensive comparison of log representation techniques can help researchers and practitioners better understand the characteristics of different log representation techniques and provide them with guidance for selecting the most suitable ones for their ML-based log analysis workflow.
</details>
<details>
<summary>摘要</summary>
ilog文件是软件系统运行状况的重要来源信息。由于现代软件架构和维护方法不断演化，更多的研究努力被投入到自动化log分析领域。特别是机器学习（ML）在log分析任务中得到了广泛的应用。在ML基于log分析任务中，将文本log数据转化为数字特征向量是一项关键和不可或缺的步骤。然而，使用不同的log表示技术对下游模型的性能的影响并不清晰，这限制了研究人员和实践者在自动化log分析 workflow中选择最佳log表示技术的机会。因此，本工作调查和比较了过去的log分析研究中广泛采用的log表示技术。特别是我们选择了六种log表示技术，并与七种ML模型和四个公共log数据集（即HDFS、BGL、Spirit和Thunderbird）进行了在log基于异常检测中的评估。我们还考虑了在log解析过程和不同的特征聚合方法被采用时的影响。从实验结果来看，我们提供了一些启示性的指南，以帮助未来的研究人员和开发者在设计自动化log分析 workflow时采取更好的决策。我们认为我们的全面的log表示技术比较可以帮助研究人员和实践者更好地了解不同log表示技术的特点，并为他们选择最适合的一种。
</details></li>
</ul>
<hr>
<h2 id="A-Novel-Loss-Function-Utilizing-Wasserstein-Distance-to-Reduce-Subject-Dependent-Noise-for-Generalizable-Models-in-Affective-Computing"><a href="#A-Novel-Loss-Function-Utilizing-Wasserstein-Distance-to-Reduce-Subject-Dependent-Noise-for-Generalizable-Models-in-Affective-Computing" class="headerlink" title="A Novel Loss Function Utilizing Wasserstein Distance to Reduce Subject-Dependent Noise for Generalizable Models in Affective Computing"></a>A Novel Loss Function Utilizing Wasserstein Distance to Reduce Subject-Dependent Noise for Generalizable Models in Affective Computing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10869">http://arxiv.org/abs/2308.10869</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nibraas Khan, Mahrukh Tauseef, Ritam Ghosh, Nilanjan Sarkar</li>
<li>For: The paper aims to improve the accuracy of emotion detection using deep learning techniques and physiological data.* Methods: The proposed cost function employs Optimal Transport Theory, specifically Wasserstein Distance, to scale the importance of subject-dependent data and reduce the impact of subject-dependent noise.* Results: The proposed cost function outperforms a state-of-the-art loss function (Mean Squared Error) across four commonly used datasets, with an average increase of 14.75% and 17.75% in minimum and centroid euclidean distance, respectively.<details>
<summary>Abstract</summary>
Emotions are an essential part of human behavior that can impact thinking, decision-making, and communication skills. Thus, the ability to accurately monitor and identify emotions can be useful in many human-centered applications such as behavioral training, tracking emotional well-being, and development of human-computer interfaces. The correlation between patterns in physiological data and affective states has allowed for the utilization of deep learning techniques which can accurately detect the affective states of a person. However, the generalisability of existing models is often limited by the subject-dependent noise in the physiological data due to variations in a subject's reactions to stimuli. Hence, we propose a novel cost function that employs Optimal Transport Theory, specifically Wasserstein Distance, to scale the importance of subject-dependent data such that higher importance is assigned to patterns in data that are common across all participants while decreasing the importance of patterns that result from subject-dependent noise. The performance of the proposed cost function is demonstrated through an autoencoder with a multi-class classifier attached to the latent space and trained simultaneously to detect different affective states. An autoencoder with a state-of-the-art loss function i.e., Mean Squared Error, is used as a baseline for comparison with our model across four different commonly used datasets. Centroid and minimum distance between different classes are used as a metrics to indicate the separation between different classes in the latent space. An average increase of 14.75% and 17.75% (from benchmark to proposed loss function) was found for minimum and centroid euclidean distance respectively over all datasets.
</details>
<details>
<summary>摘要</summary>
人类行为中的情感是一个重要的部分，可以影响思维、决策和communication技能。因此，能够准确识别和评估情感的能力可以在许多人类中心应用中发挥作用，如行为训练、情感健康评估和人机界面开发。通过physiological数据中的模式和情感状态之间的相关性，使用深度学习技术可以准确检测人类情感状态。然而，现有模型的泛化能力 oft limited by subject-dependent noise in physiological data due to variations in a subject's reactions to stimuli。因此，我们提出了一个新的成本函数，利用Optimal Transport Theory, specifically Wasserstein Distance, to scale the importance of subject-dependent data such that higher importance is assigned to patterns in data that are common across all participants while decreasing the importance of patterns that result from subject-dependent noise。我们的模型在四个常用的数据集上进行了评估，并与使用 Mean Squared Error 的基eline模型进行比较。中心和最小距离between different classes在latent space中用来衡量不同类别之间的分离度。在所有数据集上，我们发现了平均增加14.75%和17.75%（从基eline到我们的损失函数）的 minimum和centroid Euclidean distance。
</details></li>
</ul>
<hr>
<h2 id="Synergistic-Signal-Denoising-for-Multimodal-Time-Series-of-Structure-Vibration"><a href="#Synergistic-Signal-Denoising-for-Multimodal-Time-Series-of-Structure-Vibration" class="headerlink" title="Synergistic Signal Denoising for Multimodal Time Series of Structure Vibration"></a>Synergistic Signal Denoising for Multimodal Time Series of Structure Vibration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11644">http://arxiv.org/abs/2308.11644</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yang Yu, Han Chen</li>
<li>for: 这篇论文的目的是提出一种基于深度学习的Structural Health Monitoring（SHM）方法，以实现基础设施的longevity和安全性。</li>
<li>methods: 这篇论文使用了一种混合了卷积和回归架构的深度学习算法，以捕捉多 modal vibration signals 中的 Complexity。另外，这篇论文还使用了注意力机制，以优化模型的准确性和适应能力。</li>
<li>results: 这篇论文的结果显示了这种方法在多个 SHM enario 中的预测精度和早期损坏探测得到了明显提高，同时也提供了更加透明和可解释的 AI-driven SHM 解决方案。<details>
<summary>Abstract</summary>
Structural Health Monitoring (SHM) plays an indispensable role in ensuring the longevity and safety of infrastructure. With the rapid growth of sensor technology, the volume of data generated from various structures has seen an unprecedented surge, bringing forth challenges in efficient analysis and interpretation. This paper introduces a novel deep learning algorithm tailored for the complexities inherent in multimodal vibration signals prevalent in SHM. By amalgamating convolutional and recurrent architectures, the algorithm adeptly captures both localized and prolonged structural behaviors. The pivotal integration of attention mechanisms further enhances the model's capability, allowing it to discern and prioritize salient structural responses from extraneous noise. Our results showcase significant improvements in predictive accuracy, early damage detection, and adaptability across multiple SHM scenarios. In light of the critical nature of SHM, the proposed approach not only offers a robust analytical tool but also paves the way for more transparent and interpretable AI-driven SHM solutions. Future prospects include real-time processing, integration with external environmental factors, and a deeper emphasis on model interpretability.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)structural health monitoring (SHM) 在保证基础设施的寿命和安全方面发挥不可或缺的作用。随着感知技术的快速发展，来自不同结构的感知数据量有历史上前所未有的增长，这带来了分析和解释数据的挑战。本文介绍了一种适应 multimodal 振荡信号的深度学习算法。通过将卷积和回归架构融合起来，该算法可以fficiently 捕捉结构的本地和持续行为。另外，通过集成注意机制，该算法可以更好地异化和优化结构的响应。我们的结果表明，该算法可以在多个 SHM 场景中提供显著提高的预测精度、早期损害检测和适应性。鉴于 SHM 的重要性，我们的方法不仅提供了一种可靠的分析工具，还开创了更加透明和可解释的 AI-驱动 SHM 解决方案。未来的发展方向包括实时处理、与外部环境因素集成以及更深入的模型解释性。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Neural-Network-is-All-You-Need-Understanding-the-Robustness-of-Dynamic-Mechanisms-in-Neural-Networks"><a href="#Dynamic-Neural-Network-is-All-You-Need-Understanding-the-Robustness-of-Dynamic-Mechanisms-in-Neural-Networks" class="headerlink" title="Dynamic Neural Network is All You Need: Understanding the Robustness of Dynamic Mechanisms in Neural Networks"></a>Dynamic Neural Network is All You Need: Understanding the Robustness of Dynamic Mechanisms in Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08709">http://arxiv.org/abs/2308.08709</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anonymous2015258/Early_Attack">https://github.com/anonymous2015258/Early_Attack</a></li>
<li>paper_authors: Mirazul Haque, Wei Yang</li>
<li>for: 本研究旨在 investigate 动态神经网络（DyNNs）中动态机制的稳定性影响和如何通过设计选择来提高 DyNNs 的稳定性。</li>
<li>methods: 本研究使用三个模型和两个数据集来评估动态机制在 DyNNs 中的影响。我们采用了多种攻击方法来评估动态机制的影响，包括转移攻击和生成攻击。</li>
<li>results: 我们发现，从 DyNNs 到 SDNNs 的攻击传递率高于从 SDNNs 到 DyNNs 的攻击传递率。此外，我们发现 DyNNs 可以更有效地生成攻击样本than SDNNs。最后，我们通过研究来提供设计选择来提高 DyNNs 对攻击的抵抗力。<details>
<summary>Abstract</summary>
Deep Neural Networks (DNNs) have been used to solve different day-to-day problems. Recently, DNNs have been deployed in real-time systems, and lowering the energy consumption and response time has become the need of the hour. To address this scenario, researchers have proposed incorporating dynamic mechanism to static DNNs (SDNN) to create Dynamic Neural Networks (DyNNs) performing dynamic amounts of computation based on the input complexity. Although incorporating dynamic mechanism into SDNNs would be preferable in real-time systems, it also becomes important to evaluate how the introduction of dynamic mechanism impacts the robustness of the models. However, there has not been a significant number of works focusing on the robustness trade-off between SDNNs and DyNNs. To address this issue, we propose to investigate the robustness of dynamic mechanism in DyNNs and how dynamic mechanism design impacts the robustness of DyNNs. For that purpose, we evaluate three research questions. These evaluations are performed on three models and two datasets. Through the studies, we find that attack transferability from DyNNs to SDNNs is higher than attack transferability from SDNNs to DyNNs. Also, we find that DyNNs can be used to generate adversarial samples more efficiently than SDNNs. Then, through research studies, we provide insight into the design choices that can increase robustness of DyNNs against the attack generated using static model. Finally, we propose a novel attack to understand the additional attack surface introduced by the dynamic mechanism and provide design choices to improve robustness against the attack.
</details>
<details>
<summary>摘要</summary>
深度神经网络 (DNNs) 已经用于解决不同的日常问题。近些年，DNNs 已经在实时系统中部署，降低能耗和响应时间已成为当务之急。为了解决这种情况，研究人员已经提议将静态神经网络 (SDNNs) 转换成动态神经网络 (DyNNs)，以进行动态量的计算基于输入复杂性。虽然将动态机制添加到 SDNNs 可以在实时系统中提高性能，但也需要评估这种变化对模型的稳定性的影响。然而，有很少的研究集中注意到 SDNNs 和 DyNNs 之间的稳定性质量。为了解决这个问题，我们提出了三个研究问题，并对三种模型和两个数据集进行评估。我们发现，从 DyNNs 到 SDNNs 的攻击传播率高于从 SDNNs 到 DyNNs 的攻击传播率。此外，我们发现 DyNNs 可以更有效地生成黑客样本。然后，通过研究，我们提供了一些设计选择，可以增强 DyNNs 对攻击的抵御力。最后，我们提出了一种新的攻击方法，以评估动态机制引入的额外攻击表面，并提供了一些设计选择，可以提高 DyNNs 对攻击的抵御力。
</details></li>
</ul>
<hr>
<h2 id="Consciousness-in-Artificial-Intelligence-Insights-from-the-Science-of-Consciousness"><a href="#Consciousness-in-Artificial-Intelligence-Insights-from-the-Science-of-Consciousness" class="headerlink" title="Consciousness in Artificial Intelligence: Insights from the Science of Consciousness"></a>Consciousness in Artificial Intelligence: Insights from the Science of Consciousness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08708">http://arxiv.org/abs/2308.08708</a></li>
<li>repo_url: None</li>
<li>paper_authors: Patrick Butlin, Robert Long, Eric Elmoznino, Yoshua Bengio, Jonathan Birch, Axel Constant, George Deane, Stephen M. Fleming, Chris Frith, Xu Ji, Ryota Kanai, Colin Klein, Grace Lindsay, Matthias Michel, Liad Mudrik, Megan A. K. Peters, Eric Schwitzgebel, Jonathan Simon, Rufin VanRullen</li>
<li>for: 本文提出了一种rigorous和empirically grounded的方法来评估现代AI系统是否具备意识，并根据我们最好支持的神经科学理论来评估AI系统的意识性。</li>
<li>methods: 本文首先介绍了一些主流的科学理论，包括回卷处理理论、全局工作空间理论、更高级理论、预测处理理论和注意schema理论，然后从这些理论中提取了”指标属性”，这些属性可以用计算机科学的语言来评估AI系统是否满足这些指标。</li>
<li>results: 本文对一些最新的AI系统进行了评估，并发现现在的AI系统没有意识性，但也没有显而出技术障碍建立意识AI系统。<details>
<summary>Abstract</summary>
Whether current or near-term AI systems could be conscious is a topic of scientific interest and increasing public concern. This report argues for, and exemplifies, a rigorous and empirically grounded approach to AI consciousness: assessing existing AI systems in detail, in light of our best-supported neuroscientific theories of consciousness. We survey several prominent scientific theories of consciousness, including recurrent processing theory, global workspace theory, higher-order theories, predictive processing, and attention schema theory. From these theories we derive "indicator properties" of consciousness, elucidated in computational terms that allow us to assess AI systems for these properties. We use these indicator properties to assess several recent AI systems, and we discuss how future systems might implement them. Our analysis suggests that no current AI systems are conscious, but also suggests that there are no obvious technical barriers to building AI systems which satisfy these indicators.
</details>
<details>
<summary>摘要</summary>
当前或近期的人工智能系统是科学兴趣和公众关注的话题。本报告强调和证明了一种严谨和基于实验的AI意识方法：通过评估现有AI系统，以及我们最好支持的神经科学理论来评估AI意识。我们对多种知名的科学理论进行了survey，包括回propagation理论、全球工作区理论、更高级理论、预测处理理论和注意schema理论。从这些理论中，我们得出了"指标性质"的 consciousness，并将其转化为计算机科学中的形式，以评估AI系统是否满足这些指标。我们对多个最新的AI系统进行了评估，并讨论了未来系统如何实现这些指标。我们的分析结果表明，目前没有任何AI系统具备意识，但也没有明显的技术障碍建立具备这些指标的AI系统。
</details></li>
</ul>
<hr>
<h2 id="FineQuant-Unlocking-Efficiency-with-Fine-Grained-Weight-Only-Quantization-for-LLMs"><a href="#FineQuant-Unlocking-Efficiency-with-Fine-Grained-Weight-Only-Quantization-for-LLMs" class="headerlink" title="FineQuant: Unlocking Efficiency with Fine-Grained Weight-Only Quantization for LLMs"></a>FineQuant: Unlocking Efficiency with Fine-Grained Weight-Only Quantization for LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09723">http://arxiv.org/abs/2308.09723</a></li>
<li>repo_url: None</li>
<li>paper_authors: Young Jin Kim, Rawn Henry, Raffy Fahim, Hany Hassan Awadalla<br>for: 这个研究是为了提高大型语言模型（LLMs）的实际部署，因为它们需要很大的内存。methods: 我们提出了一种高效的量化方法，可以降低内存consumption和加速LLMs的测试过程。我们还提出了一个简单且有效的规律，可以在无需调整的情况下保持模型质量。results: 我们的方法可以在大规模的开源模型，如 OPT-175B 和内部MoE模型上，实现最小的精度损失，同时获得最多3.65倍的通过率。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have achieved state-of-the-art performance across various language tasks but pose challenges for practical deployment due to their substantial memory requirements. Furthermore, the latest generative models suffer from high inference costs caused by the memory bandwidth bottleneck in the auto-regressive decoding process. To address these issues, we propose an efficient weight-only quantization method that reduces memory consumption and accelerates inference for LLMs. To ensure minimal quality degradation, we introduce a simple and effective heuristic approach that utilizes only the model weights of a pre-trained model. This approach is applicable to both Mixture-of-Experts (MoE) and dense models without requiring additional fine-tuning. To demonstrate the effectiveness of our proposed method, we first analyze the challenges and issues associated with LLM quantization. Subsequently, we present our heuristic approach, which adaptively finds the granularity of quantization, effectively addressing these problems. Furthermore, we implement highly efficient GPU GEMMs that perform on-the-fly matrix multiplication and dequantization, supporting the multiplication of fp16 or bf16 activations with int8 or int4 weights. We evaluate our approach on large-scale open source models such as OPT-175B and internal MoE models, showcasing minimal accuracy loss while achieving up to 3.65 times higher throughput on the same number of GPUs.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经在不同的语言任务上达到了现场的表现水准，但是它们在实际应用中受到了内存需求的挑战。另外，最新的生成模型受到了自动预测过程中的内存带宽瓶须的高处理成本的影响。为了解决这些问题，我们提出了一种高效的量化方法，可以降低内存消耗和加速 LLM 的测试。我们还提出了一个简单而有效的规则，可以在不需要进一步微调的情况下，将模型精度降至最低。这个方法适用于内部的 Mixture-of-Experts（MoE）和稠密模型。我们还实现了高效的 GPU GEMM，可以在线进行矩阵乘法和量化，支持对 fp16 或 bf16 的激活值进行 int8 或 int4 的重量量化。我们将这个方法应用于 OPT-175B 和内部的 MoE 模型，展示了最小的准确度损失，同时可以在相同的 GPU 上 Achieve 3.65 倍的运算速度。
</details></li>
</ul>
<hr>
<h2 id="Partially-Observable-Multi-agent-RL-with-Quasi-Efficiency-The-Blessing-of-Information-Sharing"><a href="#Partially-Observable-Multi-agent-RL-with-Quasi-Efficiency-The-Blessing-of-Information-Sharing" class="headerlink" title="Partially Observable Multi-agent RL with (Quasi-)Efficiency: The Blessing of Information Sharing"></a>Partially Observable Multi-agent RL with (Quasi-)Efficiency: The Blessing of Information Sharing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08705">http://arxiv.org/abs/2308.08705</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiangyu Liu, Kaiqing Zhang</li>
<li>for: This paper focuses on developing a sample- and computation-efficient partially observable multi-agent reinforcement learning (MARL) algorithm in the general framework of partially observable stochastic games (POSGs).</li>
<li>methods: The authors propose leveraging information-sharing among agents and approximating the shared common information to construct an approximate model of the POSG, which enables quasi-efficient planning and solving of the original POSG.</li>
<li>results: The proposed algorithm is both statistically and computationally quasi-efficient, and the authors hope that their study may open up possibilities for leveraging and designing different information structures for developing sample- and computation-efficient partially observable MARL.<details>
<summary>Abstract</summary>
We study provable multi-agent reinforcement learning (MARL) in the general framework of partially observable stochastic games (POSGs). To circumvent the known hardness results and the use of computationally intractable oracles, we advocate leveraging the potential \emph{information-sharing} among agents, a common practice in empirical MARL, and a standard model for multi-agent control systems with communications. We first establish several computation complexity results to justify the necessity of information-sharing, as well as the observability assumption that has enabled quasi-efficient single-agent RL with partial observations, for computational efficiency in solving POSGs. We then propose to further \emph{approximate} the shared common information to construct an {approximate model} of the POSG, in which planning an approximate equilibrium (in terms of solving the original POSG) can be quasi-efficient, i.e., of quasi-polynomial-time, under the aforementioned assumptions. Furthermore, we develop a partially observable MARL algorithm that is both statistically and computationally quasi-efficient. We hope our study may open up the possibilities of leveraging and even designing different \emph{information structures}, for developing both sample- and computation-efficient partially observable MARL.
</details>
<details>
<summary>摘要</summary>
我们研究可证明多智能 reinforcement learning（MARL）的通用框架中的部分可见随机游戏（POSG）。为了绕过已知的困难性和使用计算量卷积的执行器，我们建议利用智能之间的信息共享，这是现实中的多智能控制系统通信的标准模型。我们首先确立了一些计算复杂性结论，以 justify 信息共享的必要性，以及在部分观察下的可见性假设，这使得单机RL可以有效地解决POSG。然后，我们提议使用approximate shared common information来构建一个approximate模型，在该模型中，计划一个近似平衡（在原POSG中解决）可以在可证明的时间内完成，即 quasi-polynomial-time。此外，我们开发了一种部分可见MARL算法，该算法同时具备了统计学和计算学的 quasi-有效性。我们希望我们的研究可以开拓出不同的信息结构，以开发更高效的部分可见MARL算法。
</details></li>
</ul>
<hr>
<h2 id="Planning-in-the-imagination-High-level-planning-on-learned-abstract-search-spaces"><a href="#Planning-in-the-imagination-High-level-planning-on-learned-abstract-search-spaces" class="headerlink" title="Planning in the imagination: High-level planning on learned abstract search spaces"></a>Planning in the imagination: High-level planning on learned abstract search spaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08693">http://arxiv.org/abs/2308.08693</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carlos Martin, Tuomas Sandholm</li>
<li>for: 该论文旨在提供一种新的规划方法，叫做PiZero，可以让代理人在自己创造的抽象搜索空间中进行规划，完全与实际环境解耦。</li>
<li>methods: 该方法不同于先前的方法，可以在任意时间尺度和基础级微动作数量的情况下进行高级规划，并且可以处理连续动作空间和部分可见性的情况。</li>
<li>results: 在多个领域中进行实验，该方法与相似的先前方法进行比较，达到更高的性能，而不需要访问环境模拟器。<details>
<summary>Abstract</summary>
We propose a new method, called PiZero, that gives an agent the ability to plan in an abstract search space of its own creation that is completely decoupled from the real environment. Unlike prior approaches, this enables the agent to perform high-level planning at arbitrary timescales and reason in terms of compound or temporally-extended actions, which can be useful in environments where large numbers of base-level micro-actions are needed to perform relevant macro-actions. In addition, our method is more general than comparable prior methods because it handles settings with continuous action spaces and partial observability. We evaluate our method on multiple domains, including navigation tasks and Sokoban. Experimentally, it outperforms comparable prior methods without assuming access to an environment simulator.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新方法，叫做PiZero，它让代理人能够在自己创建的抽象搜索空间中进行规划，完全与真实环境分离。与先前的方法不同，这使得代理人可以在任意时间尺度上进行高级规划，并在基础级微动作的批量进行复杂或时间扩展的动作，这可以在环境中需要大量基础级微动作来完成重要的macro动作时是有用的。此外，我们的方法更加通用于先前的方法，因为它处理了连续动作空间和部分可见性的设置。我们在多个领域进行了实验，包括导航任务和Sokoban，并证明了我们的方法在相对先前方法无需访问环境模拟器的情况下表现出色。
</details></li>
</ul>
<hr>
<h2 id="Quantifying-Overfitting-Introducing-the-Overfitting-Index"><a href="#Quantifying-Overfitting-Introducing-the-Overfitting-Index" class="headerlink" title="Quantifying Overfitting: Introducing the Overfitting Index"></a>Quantifying Overfitting: Introducing the Overfitting Index</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08682">http://arxiv.org/abs/2308.08682</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sanad Aburass</li>
<li>for: 本研究旨在提供一个量化评估模型过拟合情况的度量，以便提高机器学习模型在实际应用中的可靠性。</li>
<li>methods: 本研究使用了一个新的度量方法—Overfitting Index（OI），通过实验显示了OI的有用性和显著性。</li>
<li>results: 研究结果显示，不同架构在不同数据集上的过拟合情况有很大差异，而资料增强对小型特殊数据集的影响特别明显。此外，ViT-32在MNIST数据集上的表现也显示了某些模型和数据集之间的具体关系。<details>
<summary>Abstract</summary>
In the rapidly evolving domain of machine learning, ensuring model generalizability remains a quintessential challenge. Overfitting, where a model exhibits superior performance on training data but falters on unseen data, is a recurrent concern. This paper introduces the Overfitting Index (OI), a novel metric devised to quantitatively assess a model's tendency to overfit. Through extensive experiments on the Breast Ultrasound Images Dataset (BUS) and the MNIST dataset using architectures such as MobileNet, U-Net, ResNet, Darknet, and ViT-32, we illustrate the utility and discernment of the OI. Our results underscore the variable overfitting behaviors across architectures and highlight the mitigative impact of data augmentation, especially on smaller and more specialized datasets. The ViT-32's performance on MNIST further emphasizes the robustness of certain models and the dataset's comprehensive nature. By providing an objective lens to gauge overfitting, the OI offers a promising avenue to advance model optimization and ensure real-world efficacy.
</details>
<details>
<summary>摘要</summary>
在机器学习领域中，确保模型通用性是一项核心挑战。过拟合，其中模型在训练数据上表现出色但在未见数据上表现不佳，是一个常见问题。这篇论文介绍了一种新的度量方法——过拟合指数（OI），用于评估模型过拟合的倾向。通过对Breast Ultrasound Images Dataset（BUS）和MNIST dataset上的多种架构（如MobileNet、U-Net、ResNet、Darknet和ViT-32）进行了广泛的实验，我们示出了OI的实用性和分辨率。我们的结果表明不同的架构之间存在变化的过拟合行为，并且数据扩展尤其是在小型特定数据集上具有缓解作用。ViT-32在MNIST上的表现更加强调了某些模型的稳定性和数据集的全面性。通过提供一个对过拟合进行对象评估的途径，OI提供了一个有前途的方法，以确保模型在实际应用中的有效性。
</details></li>
</ul>
<hr>
<h2 id="SkinDistilViT-Lightweight-Vision-Transformer-for-Skin-Lesion-Classification"><a href="#SkinDistilViT-Lightweight-Vision-Transformer-for-Skin-Lesion-Classification" class="headerlink" title="SkinDistilViT: Lightweight Vision Transformer for Skin Lesion Classification"></a>SkinDistilViT: Lightweight Vision Transformer for Skin Lesion Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08669">http://arxiv.org/abs/2308.08669</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Longman-Stan/SkinDistilVit">https://github.com/Longman-Stan/SkinDistilVit</a></li>
<li>paper_authors: Vlad-Constantin Lungu-Stan, Dumitru-Clementin Cercel, Florin Pop</li>
<li>for: 这个研究旨在提供一个基于 transformer 的 skin cancer 分类方案，可以匹配人工智能的 melanoma 识别精度。</li>
<li>methods: 研究人员使用 knowledge distillation 技术来训练一个基于 transformer 的模型，并在模型中添加多个分类头来提高分类精度。</li>
<li>results: 模型可以匹配教师模型的精度（98.33%），并且在时间和内存方面具有较好的性能（69.25%  faster on GPU、97.96% faster on CPU）。<details>
<summary>Abstract</summary>
Skin cancer is a treatable disease if discovered early. We provide a production-specific solution to the skin cancer classification problem that matches human performance in melanoma identification by training a vision transformer on melanoma medical images annotated by experts. Since inference cost, both time and memory wise is important in practice, we employ knowledge distillation to obtain a model that retains 98.33% of the teacher's balanced multi-class accuracy, at a fraction of the cost. Memory-wise, our model is 49.60% smaller than the teacher. Time-wise, our solution is 69.25% faster on GPU and 97.96% faster on CPU. By adding classification heads at each level of the transformer and employing a cascading distillation process, we improve the balanced multi-class accuracy of the base model by 2.1%, while creating a range of models of various sizes but comparable performance. We provide the code at https://github.com/Longman-Stan/SkinDistilVit.
</details>
<details>
<summary>摘要</summary>
皮肤癌是一种可治疗的疾病，如果早期发现。我们提供了一种特定于生产的解决方案，用于皮肤癌类型分类问题，与专家标注的医疗图像进行匹配。由于实际应用中的推理成本（时间和内存）非常重要，我们使用知识储存技术来获得一个保留98.33%的教师平衡多类准确率的模型，而且内存占用量为49.60%，时间占用量为69.25%和97.96%。通过在转换器中添加分类头和使用层次分类处理，我们提高了基本模型的平衡多类准确率2.1%，同时创造了不同大小的模型，但具有相同性能。我们提供了代码，可以在 GitHub上找到：https://github.com/Longman-Stan/SkinDistilVit。
</details></li>
</ul>
<hr>
<h2 id="BREATHE-Second-Order-Gradients-and-Heteroscedastic-Emulation-based-Design-Space-Exploration"><a href="#BREATHE-Second-Order-Gradients-and-Heteroscedastic-Emulation-based-Design-Space-Exploration" class="headerlink" title="BREATHE: Second-Order Gradients and Heteroscedastic Emulation based Design Space Exploration"></a>BREATHE: Second-Order Gradients and Heteroscedastic Emulation based Design Space Exploration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08666">http://arxiv.org/abs/2308.08666</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shikhar Tuli, Niraj K. Jha</li>
<li>for: 这个研究的目的是提出一个受限制的多目标优化（MOO）框架，以便在各种科学研究和物理实验中更好地探索和观察新的设计样本。</li>
<li>methods: 这个框架使用了第二项Gradient和异escaled对应模型来实现样本有效的优化。</li>
<li>results: 在单一目标vector优化应用中，BREATHE比下一个基elineRandom Forest Regression高效性64.1%；在图形基数搜索中，BREATHE比下一个基elineGaussian-process-based Bayesian optimization高效性64.9%；在多目标优化任务中，BREATHE可以达到21.9倍的超过MOBOpt的内在量。<details>
<summary>Abstract</summary>
Researchers constantly strive to explore larger and more complex search spaces in various scientific studies and physical experiments. However, such investigations often involve sophisticated simulators or time-consuming experiments that make exploring and observing new design samples challenging. Previous works that target such applications are typically sample-inefficient and restricted to vector search spaces. To address these limitations, this work proposes a constrained multi-objective optimization (MOO) framework, called BREATHE, that searches not only traditional vector-based design spaces but also graph-based design spaces to obtain best-performing graphs. It leverages second-order gradients and actively trains a heteroscedastic surrogate model for sample-efficient optimization. In a single-objective vector optimization application, it leads to 64.1% higher performance than the next-best baseline, random forest regression. In graph-based search, BREATHE outperforms the next-best baseline, i.e., a graphical version of Gaussian-process-based Bayesian optimization, with up to 64.9% higher performance. In a MOO task, it achieves up to 21.9$\times$ higher hypervolume than the state-of-the-art method, multi-objective Bayesian optimization (MOBOpt). BREATHE also outperforms the baseline methods on most standard MOO benchmark applications.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Flickr-Africa-Examining-Geo-Diversity-in-Large-Scale-Human-Centric-Visual-Data"><a href="#Flickr-Africa-Examining-Geo-Diversity-in-Large-Scale-Human-Centric-Visual-Data" class="headerlink" title="Flickr Africa: Examining Geo-Diversity in Large-Scale, Human-Centric Visual Data"></a>Flickr Africa: Examining Geo-Diversity in Large-Scale, Human-Centric Visual Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08656">http://arxiv.org/abs/2308.08656</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keziah Naggita, Julienne LaChance, Alice Xiang</li>
<li>for:  investigate the limitations of standard Internet data collection methods in low- and middle-income countries</li>
<li>methods: analyze human-centric image geo-diversity on a massive scale using geotagged Flickr images associated with each nation in Africa</li>
<li>results: findings for an &#96;&#96;othering’’ phenomenon as evidenced by a substantial number of images from Africa being taken by non-local photographers, and the need for further work to capture image data representative of African people and their environments to improve the applicability of computer vision models in a global context.Here’s the full text in Simplified Chinese:</li>
<li>for: 这个研究的目的是调查低收入国家和中等收入国家的标准互联网数据采集方法的局限性。</li>
<li>methods: 该研究使用非常大规模的地标批处Flickr图片，对每个非洲国家进行分析，以获取有关人类中心的图像宽泛分布。</li>
<li>results: 研究发现，非洲的图像数据中存在一种“其他化”现象，即非洲的图像由非本地摄影师拍摄的情况很多。这些结果表明，需要进一步的工作，以捕捉更加符合非洲人和他们环境的图像数据，以提高计算机视觉模型在全球上的适用性。<details>
<summary>Abstract</summary>
Biases in large-scale image datasets are known to influence the performance of computer vision models as a function of geographic context. To investigate the limitations of standard Internet data collection methods in low- and middle-income countries, we analyze human-centric image geo-diversity on a massive scale using geotagged Flickr images associated with each nation in Africa. We report the quantity and content of available data with comparisons to population-matched nations in Europe as well as the distribution of data according to fine-grained intra-national wealth estimates. Temporal analyses are performed at two-year intervals to expose emerging data trends. Furthermore, we present findings for an ``othering'' phenomenon as evidenced by a substantial number of images from Africa being taken by non-local photographers. The results of our study suggest that further work is required to capture image data representative of African people and their environments and, ultimately, to improve the applicability of computer vision models in a global context.
</details>
<details>
<summary>摘要</summary>
大规模图像数据集中的偏见会影响计算机视觉模型的表现，具体来说是根据地理背景。为了调查互联网数据采集方法在LOW-和中等收入国家的限制，我们使用Geotagged Flickr图像与每个非洲国家进行大规模人类中心图像地域多样性分析。我们对可用数据量和内容进行比较，并根据细化的内国财富估计进行分布分析。我们还在两年间进行时间分析，以暴露出emerging数据趋势。此外，我们还发现了一种“他者”现象，即非本地摄影师拍摄的非洲图像的巨大数量。我们的研究结果表明，需要进一步的工作，以捕捉非洲人和其环境的图像数据，并最终提高计算机视觉模型在全球上的应用性。
</details></li>
</ul>
<hr>
<h2 id="Physics-Informed-Recurrent-Neural-Networks-for-Seismic-Response-Evaluation-of-Nonlinear-Systems"><a href="#Physics-Informed-Recurrent-Neural-Networks-for-Seismic-Response-Evaluation-of-Nonlinear-Systems" class="headerlink" title="Physics Informed Recurrent Neural Networks for Seismic Response Evaluation of Nonlinear Systems"></a>Physics Informed Recurrent Neural Networks for Seismic Response Evaluation of Nonlinear Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08655">http://arxiv.org/abs/2308.08655</a></li>
<li>repo_url: None</li>
<li>paper_authors: Faisal Nissar Malik, James Ricles, Masoud Yari, Malik Arsala Nissar</li>
<li>for: 这个论文旨在测试多度自由系统（MDOF）的类型动态回应，尤其是在地震（earthquake）载荷下的非线性结构性能。</li>
<li>methods: 这个论文使用了物理知识数据驱动的遗传 ней网（Physics-Informed Recurrent Neural Network，PIRNN）来评估多度自由系统的类型动态回应。</li>
<li>results: 该论文预测的回应将与现有方法（如金属元件分析，FEA）进行比较，以评估PIRNN模型的有效性。<details>
<summary>Abstract</summary>
Dynamic response evaluation in structural engineering is the process of determining the response of a structure, such as member forces, node displacements, etc when subjected to dynamic loads such as earthquakes, wind, or impact. This is an important aspect of structural analysis, as it enables engineers to assess structural performance under extreme loading conditions and make informed decisions about the design and safety of the structure. Conventional methods for dynamic response evaluation involve numerical simulations using finite element analysis (FEA), where the structure is modeled using finite elements, and the equations of motion are solved numerically. Although effective, this approach can be computationally intensive and may not be suitable for real-time applications. To address these limitations, recent advancements in machine learning, specifically artificial neural networks, have been applied to dynamic response evaluation in structural engineering. These techniques leverage large data sets and sophisticated algorithms to learn the complex relationship between inputs and outputs, making them ideal for such problems. In this paper, a novel approach is proposed for evaluating the dynamic response of multi-degree-of-freedom (MDOF) systems using physics-informed recurrent neural networks. The focus of this paper is to evaluate the seismic (earthquake) response of nonlinear structures. The predicted response will be compared to state-of-the-art methods such as FEA to assess the efficacy of the physics-informed RNN model.
</details>
<details>
<summary>摘要</summary>
dynamically respond evaluation in structural engineering 是指评估结构受到动力荷载（如风、地震、冲击等）时的响应，例如成员力、节点偏移等。这是结构分析中非常重要的一环，因为它可以让工程师在极端荷载情况下评估结构性能，并根据这些结果做出有知识的设计和安全决策。传统的方法 для dynamically respond evaluation 包括数学模拟（FEA），其中结构被模型为finite element，并通过数学方法解决方程。虽然有效，但这种方法可能是计算昂贵的，并且可能不适用于实时应用。为了解决这些限制，最近在机器学习领域，特别是人工神经网络（RNN）中，对 dynamically respond evaluation 进行了应用。这些技术可以利用大量数据集和复杂的算法来学习输入和输出之间的复杂关系，使其成为这种问题的理想解决方案。在本文中，一种新的方法被提出来评估多度关系（MDOF）系统的动力响应。本文的焦点是评估震动（地震）响应。预测的响应将与现有方法（如FEA）进行比较，以评估物理学信息RNN模型的有效性。
</details></li>
</ul>
<hr>
<h2 id="Reproducing-Kernel-Hilbert-Space-Pruning-for-Sparse-Hyperspectral-Abundance-Prediction"><a href="#Reproducing-Kernel-Hilbert-Space-Pruning-for-Sparse-Hyperspectral-Abundance-Prediction" class="headerlink" title="Reproducing Kernel Hilbert Space Pruning for Sparse Hyperspectral Abundance Prediction"></a>Reproducing Kernel Hilbert Space Pruning for Sparse Hyperspectral Abundance Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08653">http://arxiv.org/abs/2308.08653</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael G. Rawson, Timothy Doster, Tegan Emerson</li>
<li>for: 本研究旨在开发一种基于希尔伯特空间的减少维度方法，以提高光谱分析的效率和精度。</li>
<li>methods: 该方法使用非负最小二乘估算来构建稀疏表示，并使用最大可能性压缩向量来减少信息损失。</li>
<li>results: 对实验和 sintética数据进行评估，发现希尔伯特空间减少方法可以减少错误率，并且可以与标准减少和最小二乘方法相比，提高压缩率和精度。<details>
<summary>Abstract</summary>
Hyperspectral measurements from long range sensors can give a detailed picture of the items, materials, and chemicals in a scene but analysis can be difficult, slow, and expensive due to high spatial and spectral resolutions of state-of-the-art sensors. As such, sparsity is important to enable the future of spectral compression and analytics. It has been observed that environmental and atmospheric effects, including scattering, can produce nonlinear effects posing challenges for existing source separation and compression methods. We present a novel transformation into Hilbert spaces for pruning and constructing sparse representations via non-negative least squares minimization. Then we introduce max likelihood compression vectors to decrease information loss. Our approach is benchmarked against standard pruning and least squares as well as deep learning methods. Our methods are evaluated in terms of overall spectral reconstruction error and compression rate using real and synthetic data. We find that pruning least squares methods converge quickly unlike matching pursuit methods. We find that Hilbert space pruning can reduce error by as much as 40% of the error of standard pruning and also outperform neural network autoencoders.
</details>
<details>
<summary>摘要</summary>
高spectral度测量从长距离感知器可以提供场景中物品、材料和化学物质的详细图像，但是分析可能困难、慢和昂贵，这主要是因为现有的感知器具有高空间和spectral分辨率。因此，稀疏性具有重要作用，以便未来的spectral压缩和分析。已经观察到环境和大气效应，包括散射，可以导致非线性效应，这会对现有的源分离和压缩方法 pose challenges。我们提出了一种将测量转换到希尔伯特空间的新方法，以便减少稀疏表示的最小二乘问题。然后，我们引入最大可能性压缩向量，以降低信息损失。我们的方法与标准减少和最小二乘以及深度学习方法进行比较。我们的方法在实际和 sintetic数据上进行评估，并发现减少最小二乘方法可以快速 converges，而不同于匹配追求方法。此外，希尔伯特空间减少可以将错误降低到40%以上，并超越神经网络自适应编码器。
</details></li>
</ul>
<hr>
<h2 id="Towards-Personalized-Federated-Learning-via-Heterogeneous-Model-Reassembly"><a href="#Towards-Personalized-Federated-Learning-via-Heterogeneous-Model-Reassembly" class="headerlink" title="Towards Personalized Federated Learning via Heterogeneous Model Reassembly"></a>Towards Personalized Federated Learning via Heterogeneous Model Reassembly</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08643">http://arxiv.org/abs/2308.08643</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaqi Wang, Xingyi Yang, Suhan Cui, Liwei Che, Lingjuan Lyu, Dongkuan Xu, Fenglong Ma</li>
<li>for: addressing the practical problem of model heterogeneity in federated learning, where clients possess models with different network structures.</li>
<li>methods: leverages heterogeneous model reassembly to achieve personalized federated learning, approaches the problem of heterogeneous model personalization as a model-matching optimization task on the server side, and automatically and dynamically generates informative and diverse personalized candidates with minimal human intervention.</li>
<li>results: outperforms baselines on three datasets under both IID and Non-IID settings, effectively reduces the adverse impact of using different public data, and dynamically generates diverse personalized models in an automated manner.Here’s the full translation in Simplified Chinese:</li>
<li>for: 这篇论文是解决联邦学习中的实际问题， Client 拥有不同网络结构的模型。</li>
<li>methods: 利用多元模型重组来实现个性化联邦学习，在服务器端以模型匹配优化任务方式解决各种模型个性化问题，并自动生成有用且多样化的个性化候选人。</li>
<li>results: 在三个数据集上比基eline 表现出色，在 IID 和 Non-IID 设定下都有出色的表现，可以干扰使用不同的公共数据对模型的影响，同时自动生成多样化的个性化模型。<details>
<summary>Abstract</summary>
This paper focuses on addressing the practical yet challenging problem of model heterogeneity in federated learning, where clients possess models with different network structures. To track this problem, we propose a novel framework called pFedHR, which leverages heterogeneous model reassembly to achieve personalized federated learning. In particular, we approach the problem of heterogeneous model personalization as a model-matching optimization task on the server side. Moreover, pFedHR automatically and dynamically generates informative and diverse personalized candidates with minimal human intervention. Furthermore, our proposed heterogeneous model reassembly technique mitigates the adverse impact introduced by using public data with different distributions from the client data to a certain extent. Experimental results demonstrate that pFedHR outperforms baselines on three datasets under both IID and Non-IID settings. Additionally, pFedHR effectively reduces the adverse impact of using different public data and dynamically generates diverse personalized models in an automated manner.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Non-monotone-Sequential-Submodular-Maximization"><a href="#Non-monotone-Sequential-Submodular-Maximization" class="headerlink" title="Non-monotone Sequential Submodular Maximization"></a>Non-monotone Sequential Submodular Maximization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08641">http://arxiv.org/abs/2308.08641</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaojie Tang, Jing Yuan</li>
<li>for: 本研究targets a fundamental problem in submodular optimization, specifically sequential submodular maximization, which is to select and rank a group of $k$ items from a ground set $V$ such that the weighted summation of $k$ (possibly non-monotone) submodular functions $f_1, \cdots ,f_k$ is maximized.</li>
<li>methods: 该研究提出了一些有效的解决方案，包括针对 flexible 和 fixed 长度约束的方法，以及一个特殊情况下的同Utility函数方法。</li>
<li>results: 实验证明了我们提出的算法在视频推荐领域的有效性。这些结果在推荐系统和搜索优化等领域有着广泛的应用，因为选择项的顺序对总值有着重要的影响。<details>
<summary>Abstract</summary>
In this paper, we study a fundamental problem in submodular optimization, which is called sequential submodular maximization. Specifically, we aim to select and rank a group of $k$ items from a ground set $V$ such that the weighted summation of $k$ (possibly non-monotone) submodular functions $f_1, \cdots ,f_k: 2^V \rightarrow \mathbb{R}^+$ is maximized, here each function $f_j$ takes the first $j$ items from this sequence as input. The existing research on sequential submodular maximization has predominantly concentrated on the monotone setting, assuming that the submodular functions are non-decreasing. However, in various real-world scenarios, like diversity-aware recommendation systems, adding items to an existing set might negatively impact the overall utility. In response, this paper pioneers the examination of the aforementioned problem with non-monotone submodular functions and offers effective solutions for both flexible and fixed length constraints, as well as a special case with identical utility functions. The empirical evaluations further validate the effectiveness of our proposed algorithms in the domain of video recommendations. The results of this research have implications in various fields, including recommendation systems and assortment optimization, where the ordering of items significantly impacts the overall value obtained.
</details>
<details>
<summary>摘要</summary>
在本文中，我们研究了一个基本问题在 subsequential 优化中，即Sequential Submodular Maximization。特别是，我们想选择和排序一组 $k$ 个元素从基aset $V$，使得权重总和 $k$ (可能非增长) 的可模协变函数 $f_1, \cdots ,f_k: 2^V \rightarrow \mathbb{R}^+$ 的最大化，其中每个函数 $f_j$ 取第 $j$ 个元素序列为输入。现有的研究sequential submodular maximization 偏向偏向非增长Setting,假设优化函数是非递减的。然而，在现实生活中的多个enario中，如多样化推荐系统，添加元素到现有的集合可能会下降总用户体验。为此，本文开拓了非增长优化函数的问题，并提供了有效的解决方案，包括灵活和固定长度约束，以及特殊情况下的同用户函数。实验证明了我们提出的算法在视频推荐领域的有效性。本研究的结果在多个领域有着启示性，包括推荐系统和排序优化，其中元素的顺序具有重要的影响。
</details></li>
</ul>
<hr>
<h2 id="Fair-GANs-through-model-rebalancing-with-synthetic-data"><a href="#Fair-GANs-through-model-rebalancing-with-synthetic-data" class="headerlink" title="Fair GANs through model rebalancing with synthetic data"></a>Fair GANs through model rebalancing with synthetic data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08638">http://arxiv.org/abs/2308.08638</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anubhav Jain, Nasir Memon, Julian Togelius</li>
<li>for: 本研究旨在 mitigate 生成模型中的偏见，以提高模型的公平性。</li>
<li>methods: 本研究使用了 latent space exploration 技术，通过生成 balance 的数据来重新训练一个公平的生成模型。同时，提出了一种偏见纠正损失函数，可以在不平衡数据集上进行训练，并且可以提高公平度标度。</li>
<li>results: 在使用 FFHQ 数据集进行隔离性质测试时，提出的方法可以提高公平度标度，与传统的 Frechet inception distance (FID) 不同，可以更好地评估模型的公平性。此外，在 Cifar-10 数据集上进行了验证，并证明了方法的可行性。<details>
<summary>Abstract</summary>
Deep generative models require large amounts of training data. This often poses a problem as the collection of datasets can be expensive and difficult, in particular datasets that are representative of the appropriate underlying distribution (e.g. demographic). This introduces biases in datasets which are further propagated in the models. We present an approach to mitigate biases in an existing generative adversarial network by rebalancing the model distribution. We do so by generating balanced data from an existing unbalanced deep generative model using latent space exploration and using this data to train a balanced generative model. Further, we propose a bias mitigation loss function that shows improvements in the fairness metric even when trained with unbalanced datasets. We show results for the Stylegan2 models while training on the FFHQ dataset for racial fairness and see that the proposed approach improves on the fairness metric by almost 5 times, whilst maintaining image quality. We further validate our approach by applying it to an imbalanced Cifar-10 dataset. Lastly, we argue that the traditionally used image quality metrics such as Frechet inception distance (FID) are unsuitable for bias mitigation problems.
</details>
<details>
<summary>摘要</summary>
深度生成模型需要大量的训练数据。这经常会导致问题，因为收集数据集可能是昂贵的和困难的，特别是数据集是合适的下面分布（例如人口）。这会导致数据集中的偏见，并将它们传递给模型。我们提出了一种方法来减少模型中的偏见，通过使用潜在空间探索生成均衡数据，并使用这些数据来训练均衡的生成模型。此外，我们提出了一种偏见缓解损失函数，该函数能够在不均衡的数据集上提高公平度指标，并且可以保持图像质量。我们在使用FFHQ数据集进行遥感匈灵抑制问题中证明了我们的方法的有效性，并且在不均衡的Cifar-10数据集上验证了我们的方法。最后，我们 argue That traditionally used image quality metrics such as Frechet inception distance (FID) are unsuitable for bias mitigation problems.
</details></li>
</ul>
<hr>
<h2 id="FedPop-Federated-Population-based-Hyperparameter-Tuning"><a href="#FedPop-Federated-Population-based-Hyperparameter-Tuning" class="headerlink" title="FedPop: Federated Population-based Hyperparameter Tuning"></a>FedPop: Federated Population-based Hyperparameter Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08634">http://arxiv.org/abs/2308.08634</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haokun Chen, Denis Krompass, Jindong Gu, Volker Tresp</li>
<li>for: 该论文旨在提出一种新的分布式机器学习（Federated Learning，FL）中的参数优化算法，以提高FL的性能。</li>
<li>methods: 该论文提出了一种基于种群进化算法的参数优化算法，称为Federated Population-based Hyperparameter Tuning（FedPop），以优化FL中的参数。</li>
<li>results: 实验结果表明，FedPop比PRIOR的HP优化方法更高效，在常见的FL benchmark和实际世界FL数据集上显著提高了FL的性能。<details>
<summary>Abstract</summary>
Federated Learning (FL) is a distributed machine learning (ML) paradigm, in which multiple clients collaboratively train ML models without centralizing their local data. Similar to conventional ML pipelines, the client local optimization and server aggregation procedure in FL are sensitive to the hyperparameter (HP) selection. Despite extensive research on tuning HPs for centralized ML, these methods yield suboptimal results when employed in FL. This is mainly because their "training-after-tuning" framework is unsuitable for FL with limited client computation power. While some approaches have been proposed for HP-Tuning in FL, they are limited to the HPs for client local updates. In this work, we propose a novel HP-tuning algorithm, called Federated Population-based Hyperparameter Tuning (FedPop), to address this vital yet challenging problem. FedPop employs population-based evolutionary algorithms to optimize the HPs, which accommodates various HP types at both client and server sides. Compared with prior tuning methods, FedPop employs an online "tuning-while-training" framework, offering computational efficiency and enabling the exploration of a broader HP search space. Our empirical validation on the common FL benchmarks and complex real-world FL datasets demonstrates the effectiveness of the proposed method, which substantially outperforms the concurrent state-of-the-art HP tuning methods for FL.
</details>
<details>
<summary>摘要</summary>
Federated Learning (FL) 是一种分布式机器学习 (ML) 模式，在多个客户端协同训练 ML 模型时，不需要集中客户端的本地数据。与传统的 ML 管道类似，在 FL 中客户端本地优化和服务器聚合过程中的超参数 (HP) 选择也是敏感的。尽管有大量关于中心化 ML 中HP的优化研究，这些方法在 FL 中具有较差的效果，主要因为它们的 "训练后优化" 框架不适合 FL 中限制的客户端计算能力。一些对 FL 中HP的优化方法已经被提出，但它们只适用于客户端本地更新中的HP。在这项工作中，我们提出了一种新的HP优化算法，called Federated Population-based Hyperparameter Tuning (FedPop)，以解决这一重要但具有挑战性的问题。FedPop 使用人口生物学算法优化 HP，可以满足多种 HP 类型在客户端和服务器端。相比之前的优化方法，FedPop 采用在线 "优化while training" 框架，可以提高计算效率，并允许探索更广泛的 HP 搜索空间。我们对常见 FL benchmark 和复杂的实际 FL 数据进行了实验 validate，结果表明我们提出的方法效果明显超过了当前状态的HP优化方法 для FL。
</details></li>
</ul>
<hr>
<h2 id="LSTM-Based-Forecasting-Model-for-GRACE-Accelerometer-Data"><a href="#LSTM-Based-Forecasting-Model-for-GRACE-Accelerometer-Data" class="headerlink" title="LSTM-Based Forecasting Model for GRACE Accelerometer Data"></a>LSTM-Based Forecasting Model for GRACE Accelerometer Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08621">http://arxiv.org/abs/2308.08621</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/darbeheshti/lstm-based-analysis-for-grace-accelerometers">https://github.com/darbeheshti/lstm-based-analysis-for-grace-accelerometers</a></li>
<li>paper_authors: Neda Darbeheshti, Elahe Moradi</li>
<li>for: This paper is written for monitoring variations in Earth’s gravity field and filling data gaps in the GRACE satellite mission.</li>
<li>methods: The paper uses Long Short-Term Memory (LSTM) networks to train a model capable of predicting accelerometer data for all three axes.</li>
<li>results: The model demonstrates effectiveness in filling gaps and forecasting GRACE accelerometer data, with accurate predictions for the three axes.<details>
<summary>Abstract</summary>
The Gravity Recovery and Climate Experiment (GRACE) satellite mission, spanning from 2002 to 2017, has provided a valuable dataset for monitoring variations in Earth's gravity field, enabling diverse applications in geophysics and hydrology. The mission was followed by GRACE Follow-On in 2018, continuing data collection efforts. The monthly Earth gravity field, derived from the integration different instruments onboard satellites, has shown inconsistencies due to various factors, including gaps in observations for certain instruments since the beginning of the GRACE mission.   With over two decades of GRACE and GRACE Follow-On data now available, this paper proposes an approach to fill the data gaps and forecast GRACE accelerometer data. Specifically, we focus on accelerometer data and employ Long Short-Term Memory (LSTM) networks to train a model capable of predicting accelerometer data for all three axes.   In this study, we describe the methodology used to preprocess the accelerometer data, prepare it for LSTM training, and evaluate the model's performance. Through experimentation and validation, we assess the model's accuracy and its ability to predict accelerometer data for the three axes. Our results demonstrate the effectiveness of the LSTM forecasting model in filling gaps and forecasting GRACE accelerometer data.
</details>
<details>
<summary>摘要</summary>
格拉vity Recovery和气候实验(GRACE)卫星任务，从2002年至2017年，提供了对地球重力场变化的珍贵数据集，用于气象和地球物理多种应用。这个任务被GRACE Follow-On在2018年继承，继续数据采集。月度地球重力场，由卫星上不同仪器的集成，具有各种因素引起的不一致，包括GRACE任务开始时的某些仪器观测 gap。  With over two decades of GRACE and GRACE Follow-On data now available, this paper proposes an approach to fill the data gaps and forecast GRACE accelerometer data. Specifically, we focus on accelerometer data and employ Long Short-Term Memory (LSTM) networks to train a model capable of predicting accelerometer data for all three axes.   In this study, we describe the methodology used to preprocess the accelerometer data, prepare it for LSTM training, and evaluate the model's performance. Through experimentation and validation, we assess the model's accuracy and its ability to predict accelerometer data for the three axes. Our results demonstrate the effectiveness of the LSTM forecasting model in filling gaps and forecasting GRACE accelerometer data.
</details></li>
</ul>
<hr>
<h2 id="Boosting-Logical-Reasoning-in-Large-Language-Models-through-a-New-Framework-The-Graph-of-Thought"><a href="#Boosting-Logical-Reasoning-in-Large-Language-Models-through-a-New-Framework-The-Graph-of-Thought" class="headerlink" title="Boosting Logical Reasoning in Large Language Models through a New Framework: The Graph of Thought"></a>Boosting Logical Reasoning in Large Language Models through a New Framework: The Graph of Thought</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08614">http://arxiv.org/abs/2308.08614</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bin Lei, pei-Hung Lin, Chunhua Liao, Caiwen Ding</li>
<li>for: 提高大规模模型对复杂问题的逻辑推理能力</li>
<li>methods: 提出了一种新的引导技术 called Graph of Thoughts (GoT)</li>
<li>results: 在三个增加的挑战任务中，与 GPT-4 和 Tree of Thought (ToT) 相比，我们的方法实现了 $89.7%$, $86%$, $56%$ 的准确率提高，并且与 SOTA 方法的平均准确率提高 $23%$, $24%$, $15%$。<details>
<summary>Abstract</summary>
Recent advancements in large-scale models, such as GPT-4, have showcased remarkable capabilities in addressing standard queries. However, when facing complex problems that require multi-step logical reasoning, their accuracy dramatically decreases. Current research has explored the realm of \textit{prompting engineering} to bolster the inferential capacities of these models. Our paper unveils a pioneering prompting technique, dubbed \textit{Graph of Thoughts (GoT)}. Through testing on a trio of escalating challenges: the 24-point game, resolution of high-degree polynomial equations, and derivation of formulas for recursive sequences, our method outperformed GPT-4, achieving accuracy improvements of $89.7\%$, $86\%$, and $56\%$ for each respective task. Moreover, when juxtaposed with the state-of-the-art (SOTA) prompting method, \textit{Tree of Thought (ToT)}, our approach registered an average accuracy boost of $23\%$, $24\%$, and $15\%$.
</details>
<details>
<summary>摘要</summary>
最近的大规模模型，如GPT-4，已经表现出了解决标准问题的很好的能力。然而，当面临复杂的问题需要多步逻辑推理时，其准确率会减少很多。现有研究在\textit{提示工程}（prompting engineering）领域进行了研究，以增强这些模型的推理能力。我们的论文揭示了一种新的提示技术，名为\textit{思维图（GoT）}。在三个逐渐增加的挑战任务中：24点游戏、高度波动方程的解决和递归序列的公式 derivation 中，我们的方法比GPT-4高效，实现了准确率提高的89.7%、86%和56%。此外，与现有最佳实践（SOTA）提示方法，\textit{树思维（ToT）}，相比，我们的方法在平均上registered一个23%、24%和15%的准确率提高。
</details></li>
</ul>
<hr>
<h2 id="Integrating-Renewable-Energy-in-Agriculture-A-Deep-Reinforcement-Learning-based-Approach"><a href="#Integrating-Renewable-Energy-in-Agriculture-A-Deep-Reinforcement-Learning-based-Approach" class="headerlink" title="Integrating Renewable Energy in Agriculture: A Deep Reinforcement Learning-based Approach"></a>Integrating Renewable Energy in Agriculture: A Deep Reinforcement Learning-based Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08611">http://arxiv.org/abs/2308.08611</a></li>
<li>repo_url: None</li>
<li>paper_authors: A. Wahid, I faiud, K. Mason</li>
<li>for: 这个研究用于优化农业领域 photovoltaic (PV) 系统的决策。</li>
<li>methods: 这个研究使用深度Q学习网络（DQN）来帮助农业投资者做出有数据支持的决策，包括考虑安装预算、政府激励、能源需求、系统成本以及长期效益。</li>
<li>results: 这个研究提供了一个全面的理解，如何使用DQN来支持农业投资者做出PV安装的决策，以及该技术在农业领域的应用可能性。这些研究结果对推广可持续可靠的农业实践，提高能源效益，降低环境影响，提高利润等方面具有重要意义。<details>
<summary>Abstract</summary>
This article investigates the use of Deep Q-Networks (DQNs) to optimize decision-making for photovoltaic (PV) systems installations in the agriculture sector. The study develops a DQN framework to assist agricultural investors in making informed decisions considering factors such as installation budget, government incentives, energy requirements, system cost, and long-term benefits. By implementing a reward mechanism, the DQN learns to make data-driven decisions on PV integration. The analysis provides a comprehensive understanding of how DQNs can support investors in making decisions about PV installations in agriculture. This research has significant implications for promoting sustainable and efficient farming practices while also paving the way for future advancements in this field. By leveraging DQNs, agricultural investors can make optimized decisions that improve energy efficiency, reduce environmental impact, and enhance profitability. This study contributes to the advancement of PV integration in agriculture and encourages further innovation in this promising area.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Atom-by-atom-protein-generation-and-beyond-with-language-models"><a href="#Atom-by-atom-protein-generation-and-beyond-with-language-models" class="headerlink" title="Atom-by-atom protein generation and beyond with language models"></a>Atom-by-atom protein generation and beyond with language models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09482">http://arxiv.org/abs/2308.09482</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Flam-Shepherd, Kevin Zhu, Alán Aspuru-Guzik</li>
<li>for: 这paper是为了探索使用语言模型来生成蛋白质的可能性。</li>
<li>methods: 这paper使用了化学语言模型来学习蛋白质的原子层次表示，并可以生成不受标准遗传码限制的蛋白质。</li>
<li>results: 这paper的结果表明，语言模型可以学习蛋白质的多层次结构，从原始序列到次结构和三维结构，并可以生成不受标准遗传码限制的蛋白质，还可以同时探索蛋白质和化学空间，并生成新的蛋白质-药物 conjugate。<details>
<summary>Abstract</summary>
Protein language models learn powerful representations directly from sequences of amino acids. However, they are constrained to generate proteins with only the set of amino acids represented in their vocabulary. In contrast, chemical language models learn atom-level representations of smaller molecules that include every atom, bond, and ring. In this work, we show that chemical language models can learn atom-level representations of proteins enabling protein generation unconstrained to the standard genetic code and far beyond it. In doing so, we show that language models can generate entire proteins atom by atom -- effectively learning the multiple hierarchical layers of molecular information that define proteins from their primary sequence to their secondary, and tertiary structure. We demonstrate language models are able to explore beyond protein space -- generating proteins with modified sidechains that form unnatural amino acids. Even further, we find that language models can explore chemical space and protein space simultaneously and generate novel examples of protein-drug conjugates. The results demonstrate the potential for biomolecular design at the atom level using language models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Proprioceptive-Learning-with-Soft-Polyhedral-Networks"><a href="#Proprioceptive-Learning-with-Soft-Polyhedral-Networks" class="headerlink" title="Proprioceptive Learning with Soft Polyhedral Networks"></a>Proprioceptive Learning with Soft Polyhedral Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08538">http://arxiv.org/abs/2308.08538</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaobo Liu, Xudong Han, Wei Hong, Fang Wan, Chaoyang Song</li>
<li>for: 这篇论文旨在开发一种能够实现自适应肢体位姿感知的软体网络，以提高现代机器人的敏捷、适应性和感知能力。</li>
<li>methods: 该论文使用了软体网络和内置的视觉系统，通过学习动态力学特征来实现适应性和感知。</li>
<li>results: 实验结果显示，软体网络可以在实时感知6个自由度力和扭矩的同时，精度为0.25&#x2F;0.24&#x2F;0.35 N和0.025&#x2F;0.034&#x2F;0.006 Nm，并在静态适应中包含滑动和塑性修正以提高预测结果。<details>
<summary>Abstract</summary>
Proprioception is the "sixth sense" that detects limb postures with motor neurons. It requires a natural integration between the musculoskeletal systems and sensory receptors, which is challenging among modern robots that aim for lightweight, adaptive, and sensitive designs at a low cost. Here, we present the Soft Polyhedral Network with an embedded vision for physical interactions, capable of adaptive kinesthesia and viscoelastic proprioception by learning kinetic features. This design enables passive adaptations to omni-directional interactions, visually captured by a miniature high-speed motion tracking system embedded inside for proprioceptive learning. The results show that the soft network can infer real-time 6D forces and torques with accuracies of 0.25/0.24/0.35 N and 0.025/0.034/0.006 Nm in dynamic interactions. We also incorporate viscoelasticity in proprioception during static adaptation by adding a creep and relaxation modifier to refine the predicted results. The proposed soft network combines simplicity in design, omni-adaptation, and proprioceptive sensing with high accuracy, making it a versatile solution for robotics at a low cost with more than 1 million use cycles for tasks such as sensitive and competitive grasping, and touch-based geometry reconstruction. This study offers new insights into vision-based proprioception for soft robots in adaptive grasping, soft manipulation, and human-robot interaction.
</details>
<details>
<summary>摘要</summary>
Proprioception 是Robotics中的"六感"，它通过motor neurons探测 LIMB 姿势。它需要自然的 musculoskeletal 系统和感官器件之间的集成，这是现代Robotics中的挑战，因为它们需要轻量、适应性和敏捷的设计，同时需要低成本。在这篇文章中，我们提出了Soft Polyhedral Network，它具有嵌入式的视觉系统，能够进行适应的运动和弹簧 proprioception，通过学习运动特征来进行预测。这个设计允许机器人在无方向互动中进行自适应，并且可以通过高速动态追踪系统进行 proprioceptive 学习。实验结果显示，软网络可以在实时进行6D 力和扭矩的测量，精度为0.25/0.24/0.35 N 和 0.025/0.034/0.006 Nm。我们还将viscoelasticity 加入 proprioception 中，以更好地精确地预测结果。我们的软网络结合了简单的设计、适应性和 proprioceptive 感知，并且具有高精度和低成本，适合Robotics 中的多种任务，如敏捷和竞争性的抓取、触碰基本重建等。这篇文章将带来新的见解到视基 proprioception 领域，对于软机器人在适应抓取、软操作和人机交互等方面的应用有很大的潜力。
</details></li>
</ul>
<hr>
<h2 id="Can-Transformers-Learn-Optimal-Filtering-for-Unknown-Systems"><a href="#Can-Transformers-Learn-Optimal-Filtering-for-Unknown-Systems" class="headerlink" title="Can Transformers Learn Optimal Filtering for Unknown Systems?"></a>Can Transformers Learn Optimal Filtering for Unknown Systems?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08536">http://arxiv.org/abs/2308.08536</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haldun Balim, Zhe Du, Samet Oymak, Necmiye Ozay</li>
<li>for: 这个论文的目的是用 transformers 来解决 dynamical systems 中的优化输出估计问题。</li>
<li>methods: 这个论文使用 transformers 来生成输出预测，使用所有过去的输出来生成当前输出预测。论文通过在不同系统上进行训练，以实现在新的系统上快速适应和预测。</li>
<li>results: 这个论文的结果表明，使用 transformers 可以具有很高的预测性能，并且可以在具有非同异idi 噪声、时间变化动力学和非线性动力学等挑战的场景中表现良好。论文还提供了对 MOP 的统计保证和测试时间内的过程中所需的训练量。<details>
<summary>Abstract</summary>
Transformers have demonstrated remarkable success in natural language processing; however, their potential remains mostly unexplored for problems arising in dynamical systems. In this work, we investigate the optimal output estimation problem using transformers, which generate output predictions using all the past ones. We train the transformer using various systems drawn from a prior distribution and then evaluate its performance on previously unseen systems from the same distribution. As a result, the obtained transformer acts like a prediction algorithm that learns in-context and quickly adapts to and predicts well for different systems - thus we call it meta-output-predictor (MOP). MOP matches the performance of the optimal output estimator, based on Kalman filter, for most linear dynamical systems even though it does not have access to a model. We observe via extensive numerical experiments that MOP also performs well in challenging scenarios with non-i.i.d. noise, time-varying dynamics, and nonlinear dynamics like a quadrotor system with unknown parameters. To further support this observation, in the second part of the paper, we provide statistical guarantees on the performance of MOP and quantify the required amount of training to achieve a desired excess risk during test-time. Finally, we point out some limitations of MOP by identifying two classes of problems MOP fails to perform well, highlighting the need for caution when using transformers for control and estimation.
</details>
<details>
<summary>摘要</summary>
transformers 已经展示出了惊人的成功在自然语言处理领域;然而，它们的潜力还未得到了充分的探索，尤其是在动力系统中。在这项工作中，我们使用 transformers 来解决输出预测问题，它们使用所有过去的输出来生成输出预测。我们使用不同的系统从先验分布中随机选择训练数据，然后评估其性能在未经见过的系统上。因此，我们得到的 transformer 被称为元输出预测器 (MOP)。MOP与优化的输出估计器（基于 kalman 滤波器）的性能相当，即使它没有访问模型。我们通过广泛的数值实验发现，MOP 在非相关噪声、时间变化动力学和不确定参数的 quadrotor 系统中也表现良好。在第二部分的论文中，我们提供了对 MOP 性能的统计保证，并估计在测试时需要多少训练时间来达到所需的过量风险。最后，我们指出了 MOP 在某些情况下的限制，并标识了使用 transformers 进行控制和估计时需要小心的两类问题。
</details></li>
</ul>
<hr>
<h2 id="Painter-Teaching-Auto-regressive-Language-Models-to-Draw-Sketches"><a href="#Painter-Teaching-Auto-regressive-Language-Models-to-Draw-Sketches" class="headerlink" title="Painter: Teaching Auto-regressive Language Models to Draw Sketches"></a>Painter: Teaching Auto-regressive Language Models to Draw Sketches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08520">http://arxiv.org/abs/2308.08520</a></li>
<li>repo_url: None</li>
<li>paper_authors: Reza Pourreza, Apratim Bhattacharyya, Sunny Panchal, Mingu Lee, Pulkit Madan, Roland Memisevic</li>
<li>for: 这个论文主要针对大语言模型（LLMs）在自然语言理解方面的进步，以及在其他领域如计算机视觉、机器人学习等领域的应用。</li>
<li>methods: 本文使用LLMs直接生成虚拟的毫线笔触来绘制图像。我们提出了Painter，一个基于市场上可购买的LLM，通过对新任务进行细化而不失去语言理解能力来构建Painter。</li>
<li>results: Painter可以从文本描述转换为绘制图像，从图像中移除对象，并检测和分类图像中的对象。虽然这是一项前所未有的使用LLMs进行自动进程图像生成的研究，但结果很鼓励人。<details>
<summary>Abstract</summary>
Large language models (LLMs) have made tremendous progress in natural language understanding and they have also been successfully adopted in other domains such as computer vision, robotics, reinforcement learning, etc. In this work, we apply LLMs to image generation tasks by directly generating the virtual brush strokes to paint an image. We present Painter, an LLM that can convert user prompts in text description format to sketches by generating the corresponding brush strokes in an auto-regressive way. We construct Painter based on off-the-shelf LLM that is pre-trained on a large text corpus, by fine-tuning it on the new task while preserving language understanding capabilities. We create a dataset of diverse multi-object sketches paired with textual prompts that covers several object types and tasks. Painter can generate sketches from text descriptions, remove objects from canvas, and detect and classify objects in sketches. Although this is an unprecedented pioneering work in using LLMs for auto-regressive image generation, the results are very encouraging.
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLM）已经取得了巨大的进步，并在其他领域如计算机视觉、机器人学、奖励学习等领域中得到成功应用。在这项工作中，我们将LLM应用到图像生成任务中，直接生成虚拟的毫幅笔触来绘制图像。我们提出了“艺术家”（Painter），一种可以根据用户提示转化为笔触的LLM。我们基于市场上可获得的LLM，通过精度调整和保留语言理解能力来构建Painter。我们创建了包含多种物体绘制和任务的多样化笔触集合，并将Painter应用于这些笔触集合中。Painter可以根据文本提示生成笔触，从笔触中移除物体，并探测和分类笔触中的物体。虽然这是使用LLM进行自然语言到自动生成图像的前所未有的做法，但结果非常鼓动人心。
</details></li>
</ul>
<hr>
<h2 id="Two-and-a-half-Order-Score-based-Model-for-Solving-3D-Ill-posed-Inverse-Problems"><a href="#Two-and-a-half-Order-Score-based-Model-for-Solving-3D-Ill-posed-Inverse-Problems" class="headerlink" title="Two-and-a-half Order Score-based Model for Solving 3D Ill-posed Inverse Problems"></a>Two-and-a-half Order Score-based Model for Solving 3D Ill-posed Inverse Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08511">http://arxiv.org/abs/2308.08511</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zirong Li, Yanyang Wang, Jianjia Zhang, Weiwen Wu, Hengyong Yu</li>
<li>for: 提高CT和MRI图像重建的精度和效率，解决不同的 inverse problem。</li>
<li>methods: 基于Score-based模型，通过在2D空间学习数据分布，然后在3D空间更新数据分布来实现更精度的重建。</li>
<li>results: 在大规模的稀缺视图CT和快速MRI数据集上进行了广泛的实验，比较了现有的方法，并达到了目前最佳的解决3D稀缺 inverse problem 的效果。<details>
<summary>Abstract</summary>
Computed Tomography (CT) and Magnetic Resonance Imaging (MRI) are crucial technologies in the field of medical imaging. Score-based models have proven to be effective in addressing different inverse problems encountered in CT and MRI, such as sparse-view CT and fast MRI reconstruction. However, these models face challenges in achieving accurate three dimensional (3D) volumetric reconstruction. The existing score-based models primarily focus on reconstructing two dimensional (2D) data distribution, leading to inconsistencies between adjacent slices in the reconstructed 3D volumetric images. To overcome this limitation, we propose a novel two-and-a-half order score-based model (TOSM). During the training phase, our TOSM learns data distributions in 2D space, which reduces the complexity of training compared to directly working on 3D volumes. However, in the reconstruction phase, the TOSM updates the data distribution in 3D space, utilizing complementary scores along three directions (sagittal, coronal, and transaxial) to achieve a more precise reconstruction. The development of TOSM is built on robust theoretical principles, ensuring its reliability and efficacy. Through extensive experimentation on large-scale sparse-view CT and fast MRI datasets, our method demonstrates remarkable advancements and attains state-of-the-art results in solving 3D ill-posed inverse problems. Notably, the proposed TOSM effectively addresses the inter-slice inconsistency issue, resulting in high-quality 3D volumetric reconstruction.
</details>
<details>
<summary>摘要</summary>
computed tomography (CT) 和 magnetism resonance imaging (MRI) 是医学影像领域的关键技术。分数模型已经证明可以有效地解决 CT 和 MRI 中的不同的反问题，如稀疏视图 CT 和快速 MRI 重建。然而，这些模型在实现准确的三维（3D）卷积重建方面遇到了挑战。现有的分数模型主要是对二维（2D）数据分布进行重建，从而导致扫描图像中的邻域匹配不准确。为解决这个限制，我们提出了一种新的二阶半分数模型（TOSM）。在训练阶段，我们的 TOSM 学习了数据分布在二维空间，从而降低了训练的复杂性。然而，在重建阶段，TOSM 将数据分布更新到三维空间，利用三个方向（极轴、极圆、和扫描）的补做分数来实现更加精确的重建。TOSM 的开发基于Robust的理论原则，确保其可靠性和效果。通过对大规模稀疏视图 CT 和快速 MRI 数据进行广泛的实验，我们的方法在解决 3D 负定问题中具有显著的进步和达到了当前最佳结果。特别是，我们的 TOSM 能够有效地解决邻域不一致问题，从而实现高质量的 3D 卷积重建。
</details></li>
</ul>
<hr>
<h2 id="Autoencoding-a-Soft-Touch-to-Learn-Grasping-from-On-land-to-Underwater"><a href="#Autoencoding-a-Soft-Touch-to-Learn-Grasping-from-On-land-to-Underwater" class="headerlink" title="Autoencoding a Soft Touch to Learn Grasping from On-land to Underwater"></a>Autoencoding a Soft Touch to Learn Grasping from On-land to Underwater</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08510">http://arxiv.org/abs/2308.08510</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bionicdl-sustech/amphibioussoftfinger">https://github.com/bionicdl-sustech/amphibioussoftfinger</a></li>
<li>paper_authors: Ning Guo, Xudong Han, Xiaobo Liu, Shuqiao Zhong, Zhiyuan Zhou, Jian Lin, Jiansheng Dai, Fang Wan, Chaoyang Song</li>
<li>for: 该研究旨在提高水下机器人的物体抓取稳定性和可靠性，以支持生态环境和海洋研究的基础科学发现。</li>
<li>methods: 该研究使用了视觉基于的软体机器人手指，通过监督VARAE学习6D力和扭矩（FT）。高速摄像机记录了软机器人手指与物体之间的整体塑变，以获得更好的学习效果。</li>
<li>results: 研究结果显示，SVAE模型学习到了从陆地到水中的软机械学的各种秘密代表，在改变环境中具有Superior的适应能力，与商业FT传感器相比，提供了更加稳定和可靠的抓取。这种感觉智能抓取技术将为水下机器人带来更好的可靠性和可重复性，为生态环境和海洋研究带来更多的支持。<details>
<summary>Abstract</summary>
Robots play a critical role as the physical agent of human operators in exploring the ocean. However, it remains challenging to grasp objects reliably while fully submerging under a highly pressurized aquatic environment with little visible light, mainly due to the fluidic interference on the tactile mechanics between the finger and object surfaces. This study investigates the transferability of grasping knowledge from on-land to underwater via a vision-based soft robotic finger that learns 6D forces and torques (FT) using a Supervised Variational Autoencoder (SVAE). A high-framerate camera captures the whole-body deformations while a soft robotic finger interacts with physical objects on-land and underwater. Results show that the trained SVAE model learned a series of latent representations of the soft mechanics transferrable from land to water, presenting a superior adaptation to the changing environments against commercial FT sensors. Soft, delicate, and reactive grasping enabled by tactile intelligence enhances the gripper's underwater interaction with improved reliability and robustness at a much-reduced cost, paving the path for learning-based intelligent grasping to support fundamental scientific discoveries in environmental and ocean research.
</details>
<details>
<summary>摘要</summary>
роботы играют критическую роль как физические агенты человеческих операторов в исследовании океана. Однако, было трудно удерживать объекты надежно, когда полностью погружались в высокопрессURIZрованную акватическую среду с ограниченным видимым светом, главным образом из-за динамической интерференции между поверхностями пальца и объекта. Этот исследование изучает передачу знаний о захвате с наземных в подводные условия с помощью визуальной базированной мягкой роботической пальца, которая обучается 6D силам и моментам (FT) с помощью надсмотренного верификатора автоенкодера (SVAE). Высококачественный камеруCaptures Whole-Body Deformations While Interacting with Physical Objects On-Land and Underwater. Results Show That the Trained SVAE Model Learned a Series of Latent Representations of Soft Mechanics Transferable from Land to Water, Presenting a Superior Adaptation to Changing Environments Against Commercial FT Sensors. Soft, Delicate, and Reactive Grasping Enabled by Tactile Intelligence Enhances the Gripper's Underwater Interaction with Improved Reliability and Robustness at a Much-Reduced Cost, Paving the Path for Learning-Based Intelligent Grasping to Support Fundamental Scientific Discoveries in Environmental and Ocean Research.
</details></li>
</ul>
<hr>
<h2 id="ResBuilder-Automated-Learning-of-Depth-with-Residual-Structures"><a href="#ResBuilder-Automated-Learning-of-Depth-with-Residual-Structures" class="headerlink" title="ResBuilder: Automated Learning of Depth with Residual Structures"></a>ResBuilder: Automated Learning of Depth with Residual Structures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08504">http://arxiv.org/abs/2308.08504</a></li>
<li>repo_url: None</li>
<li>paper_authors: Julian Burghoff, Matthias Rottmann, Jill von Conta, Sebastian Schoenen, Andreas Witte, Hanno Gottschalk</li>
<li>for: 本文开发了一种基于神经网络搜索算法，即Resbuilder，可以从头开始构建高精度低计算成本的ResNet架构。它还可以修改现有架构，并且可以移除和插入ResNet块，从而在ResNet架构空间进行搜索。</li>
<li>methods: 本文使用了一种基于隐藏状态抽象的搜索算法，并且使用了一种新的强化策略来优化搜索结果。在不同的图像分类任务上进行了实验，Resbuilder能够几乎与状态地标的性能匹配，同时减少了计算成本。</li>
<li>results: 本文在不同的图像分类任务上的实验结果表明，Resbuilder能够减少计算成本，同时保持高精度。此外，通过对一个Proprietary fraud detection dataset进行应用，表明了该方法在实际应用中的一致性。<details>
<summary>Abstract</summary>
In this work, we develop a neural architecture search algorithm, termed Resbuilder, that develops ResNet architectures from scratch that achieve high accuracy at moderate computational cost. It can also be used to modify existing architectures and has the capability to remove and insert ResNet blocks, in this way searching for suitable architectures in the space of ResNet architectures. In our experiments on different image classification datasets, Resbuilder achieves close to state-of-the-art performance while saving computational cost compared to off-the-shelf ResNets. Noteworthy, we once tune the parameters on CIFAR10 which yields a suitable default choice for all other datasets. We demonstrate that this property generalizes even to industrial applications by applying our method with default parameters on a proprietary fraud detection dataset.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们开发了一种神经网络搜索算法，即Resbuilder，它可以从头开始开发高精度低计算成本的ResNet架构。它还可以修改现有架构，并且具有将ResNet块添加或删除的能力，从而在ResNet架构空间进行搜索。在我们对不同的图像分类 dataset 进行实验中，Resbuilder 可以达到 state-of-the-art 性能，而且与 commercially 可用的 ResNet 相比，计算成本更低。值得一提的是，我们在 CIFAR10 上调参得到了一个适合所有其他 dataset 的默认选择，并且我们证明这种性能可以普遍应用于工业应用程序，例如在一个 proprietary 销售欺诈数据集上使用 default 参数。
</details></li>
</ul>
<hr>
<h2 id="Time-Travel-in-LLMs-Tracing-Data-Contamination-in-Large-Language-Models"><a href="#Time-Travel-in-LLMs-Tracing-Data-Contamination-in-Large-Language-Models" class="headerlink" title="Time Travel in LLMs: Tracing Data Contamination in Large Language Models"></a>Time Travel in LLMs: Tracing Data Contamination in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08493">http://arxiv.org/abs/2308.08493</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shahriar Golchin, Mihai Surdeanu</li>
<li>for: 本研究旨在检测大语言模型（LLM）训练数据中是否存在测试数据污染问题。</li>
<li>methods: 本研究提出了一种简单 yet有效的方法来识别LLM中的数据污染。该方法包括对个别实例进行识别污染，然后判断整个数据分区是否受到污染。</li>
<li>results: 研究发现，使用“导向指令”（一个包含数据集名称、分区类型和参考实例的提示）来评估实例是否受到污染，可以准确地检测LLM中的数据污染。此外，研究还发现GPT-4中存在数据污染问题，特别是AG News、WNLI和XSum数据集。<details>
<summary>Abstract</summary>
Data contamination, i.e., the presence of test data from downstream tasks in the training data of large language models (LLMs), is a potential major issue in understanding LLMs' effectiveness on other tasks. We propose a straightforward yet effective method for identifying data contamination within LLMs. At its core, our approach starts by identifying potential contamination in individual instances that are drawn from a small random sample; using this information, our approach then assesses if an entire dataset partition is contaminated. To estimate contamination of individual instances, we employ "guided instruction:" a prompt consisting of the dataset name, partition type, and the initial segment of a reference instance, asking the LLM to complete it. An instance is flagged as contaminated if the LLM's output either exactly or closely matches the latter segment of the reference. To understand if an entire partition is contaminated, we propose two ideas. The first idea marks a dataset partition as contaminated if the average overlap score with the reference instances (as measured by ROUGE or BLEURT) is statistically significantly better with the guided instruction vs. a general instruction that does not include the dataset and partition name. The second idea marks a dataset as contaminated if a classifier based on GPT-4 with in-context learning prompting marks multiple instances as contaminated. Our best method achieves an accuracy between 92% and 100% in detecting if an LLM is contaminated with seven datasets, containing train and test/validation partitions, when contrasted with manual evaluation by human expert. Further, our findings indicate that GPT-4 is contaminated with AG News, WNLI, and XSum datasets.
</details>
<details>
<summary>摘要</summary>
大数据污染，即大语言模型（LLM）训练数据中下游任务的测试数据存在的问题，是 LLM 效果理解的 potential 主要问题。我们提出了一种简单 yet 有效的方法来在 LLM 中 Identify 数据污染。我们的方法的核心是在小样本中随机选择的实例上 Identify 数据污染。使用这些信息，我们的方法然后判断整个数据分区是否污染。为了估计实例上的污染，我们采用 "导向指令"：一个包含数据集名、分区类型和参考实例的开头的提示，请 LLM 完成它。如果 LLM 的输出与参考实例的后半部分匹配，则标记该实例为污染。为了理解整个分区是否污染，我们提出了两个想法。第一个想法是如果使用 ROUGE 或 BLEURT  measure 参考实例与指令之间的 overlap 得分为 statistically  significiantly 高于不包含数据集和分区名的通用指令，那么将标记该分区为污染。第二个想法是如果一个基于 GPT-4 的类ifier 通过受Context learning 提示标记多个实例为污染，那么将标记该数据集为污染。我们的最佳方法在七个数据集（包括训练和测试/验证分区）上达到了92% 到 100% 的准确率，与人工评估 compare 。此外，我们发现 GPT-4 污染 AG News、WNLI 和 XSum 数据集。
</details></li>
</ul>
<hr>
<h2 id="Label-Propagation-Techniques-for-Artifact-Detection-in-Imbalanced-Classes-using-Photoplethysmogram-Signals"><a href="#Label-Propagation-Techniques-for-Artifact-Detection-in-Imbalanced-Classes-using-Photoplethysmogram-Signals" class="headerlink" title="Label Propagation Techniques for Artifact Detection in Imbalanced Classes using Photoplethysmogram Signals"></a>Label Propagation Techniques for Artifact Detection in Imbalanced Classes using Photoplethysmogram Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08480">http://arxiv.org/abs/2308.08480</a></li>
<li>repo_url: None</li>
<li>paper_authors: Clara Macabiau, Thanh-Dung Le, Kevin Albert, Philippe Jouvet, Rita Noumeir</li>
<li>for: 这个研究探讨了在受到运动干扰的情况下如何使用标签卷传递技术来标注医疗数据集，特别是在不平衡的类型场景下，清洁的PPG样本被干扰样本所显著多出。</li>
<li>methods: 这种研究使用了标签卷传递技术来传递标签到PPG样本中，并与支持学习模型（包括传统类ifiers和神经网络）进行比较。</li>
<li>results: 研究结果表明，使用标签卷传递技术可以准确地标注PPG样本，尤其是在清洁样本罕见时。与支持学习模型相比，标签卷传递算法在检测干扰物时表现更好。这些结果表明，标签卷传递算法在PPG信号中的干扰物检测中具有潜在的优势。<details>
<summary>Abstract</summary>
Photoplethysmogram (PPG) signals are widely used in healthcare for monitoring vital signs, but they are susceptible to motion artifacts that can lead to inaccurate interpretations. In this study, the use of label propagation techniques to propagate labels among PPG samples is explored, particularly in imbalanced class scenarios where clean PPG samples are significantly outnumbered by artifact-contaminated samples. With a precision of 91%, a recall of 90% and an F1 score of 90% for the class without artifacts, the results demonstrate its effectiveness in labeling a medical dataset, even when clean samples are rare. For the classification of artifacts our study compares supervised classifiers such as conventional classifiers and neural networks (MLP, Transformers, FCN) with the semi-supervised label propagation algorithm. With a precision of 89%, a recall of 95% and an F1 score of 92%, the KNN supervised model gives good results, but the semi-supervised algorithm performs better in detecting artifacts. The findings suggest that the semi-supervised algorithm label propagation hold promise for artifact detection in PPG signals, which can enhance the reliability of PPG-based health monitoring systems in real-world applications.
</details>
<details>
<summary>摘要</summary>
对于artifact的分类，我们比较了传统的supervised类ifiers（例如conventional classifiers和神经网络）与 semi-supervised label propagation algorithm。与89%的精度、95%的回归率和92%的F1 Score相比，KNN 超vised模型给出了不错的结果，但semi-supervised algorithm在检测artifacts方面表现更好。研究结果表明，semi-supervised algorithm label propagation在 PPG 信号中检测artifacts 有前途，可以提高 PPG 基于健康监测系统的可靠性在实际应用中。
</details></li>
</ul>
<hr>
<h2 id="LLM4TS-Two-Stage-Fine-Tuning-for-Time-Series-Forecasting-with-Pre-Trained-LLMs"><a href="#LLM4TS-Two-Stage-Fine-Tuning-for-Time-Series-Forecasting-with-Pre-Trained-LLMs" class="headerlink" title="LLM4TS: Two-Stage Fine-Tuning for Time-Series Forecasting with Pre-Trained LLMs"></a>LLM4TS: Two-Stage Fine-Tuning for Time-Series Forecasting with Pre-Trained LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08469">http://arxiv.org/abs/2308.08469</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ching Chang, Wen-Chih Peng, Tien-Fu Chen</li>
<li>for: 提高长期时间序列预测精度</li>
<li>methods: 利用预训练的大语言模型（LLM）进行时间序列预测，并采用时间补充和时间编码等技术增强LLM对时间序列数据的处理能力。</li>
<li>results: 通过两stage精度调整和多 Parameter-Efficient Fine-Tuning（PEFT）技术，实现了长期预测的状态艺能，并且能够快速适应新的时间序列数据。<details>
<summary>Abstract</summary>
In this work, we leverage pre-trained Large Language Models (LLMs) to enhance time-series forecasting. Mirroring the growing interest in unifying models for Natural Language Processing and Computer Vision, we envision creating an analogous model for long-term time-series forecasting. Due to limited large-scale time-series data for building robust foundation models, our approach LLM4TS focuses on leveraging the strengths of pre-trained LLMs. By combining time-series patching with temporal encoding, we have enhanced the capability of LLMs to handle time-series data effectively. Inspired by the supervised fine-tuning in chatbot domains, we prioritize a two-stage fine-tuning process: first conducting supervised fine-tuning to orient the LLM towards time-series data, followed by task-specific downstream fine-tuning. Furthermore, to unlock the flexibility of pre-trained LLMs without extensive parameter adjustments, we adopt several Parameter-Efficient Fine-Tuning (PEFT) techniques. Drawing on these innovations, LLM4TS has yielded state-of-the-art results in long-term forecasting. Our model has also shown exceptional capabilities as both a robust representation learner and an effective few-shot learner, thanks to the knowledge transferred from the pre-trained LLM.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们利用预训练的大语言模型（LLM）来提高时间序列预测。随着自然语言处理和计算机视觉模型的统一的兴趣增长，我们意识到创建相应的模型。由于有限的大规模时间序列数据建立坚实的基础模型，我们的方法LLM4TS将注重利用预训练LLM的优势。通过将时间序列补丁与时间编码结合，我们已经提高了LLM对时间序列数据的处理能力。 Drawing on the supervised fine-tuning experience in chatbot domains, we prioritize a two-stage fine-tuning process: first conducting supervised fine-tuning to orient the LLM towards time-series data, followed by task-specific downstream fine-tuning. Furthermore, to unlock the flexibility of pre-trained LLMs without extensive parameter adjustments, we adopt several Parameter-Efficient Fine-Tuning (PEFT) techniques. Our model has yielded state-of-the-art results in long-term forecasting, and has also shown exceptional capabilities as both a robust representation learner and an effective few-shot learner, thanks to the knowledge transferred from the pre-trained LLM.
</details></li>
</ul>
<hr>
<h2 id="An-Expert’s-Guide-to-Training-Physics-informed-Neural-Networks"><a href="#An-Expert’s-Guide-to-Training-Physics-informed-Neural-Networks" class="headerlink" title="An Expert’s Guide to Training Physics-informed Neural Networks"></a>An Expert’s Guide to Training Physics-informed Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08468">http://arxiv.org/abs/2308.08468</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/predictiveintelligencelab/jaxpi">https://github.com/predictiveintelligencelab/jaxpi</a></li>
<li>paper_authors: Sifan Wang, Shyam Sankaran, Hanwen Wang, Paris Perdikaris</li>
<li>for: 本研究旨在提高physics-informed neural networks（PINNs）的训练效率和总体准确性，并提供一系列最佳实践和挑战性 benchmark 问题，以便未来研究者可以使用这些方法和指导原则进行比较。</li>
<li>methods: 本研究使用了一系列最佳实践和architecture choices，包括使用 JAX 库进行高效的训练和推理，以及进行了完整的ablation study，以确定不同的训练策略和architecture的影响。</li>
<li>results: 本研究的结果表明，采用本研究提出的方法和指导原则，可以获得state-of-the-art的结果，并提供了strong baselines，可以用于 future studies 的比较。此外，本研究还发布了一个高度优化的 JAX 库，可以用于重现所有结果，以及为新用例场景进行扩展和适应。<details>
<summary>Abstract</summary>
Physics-informed neural networks (PINNs) have been popularized as a deep learning framework that can seamlessly synthesize observational data and partial differential equation (PDE) constraints. Their practical effectiveness however can be hampered by training pathologies, but also oftentimes by poor choices made by users who lack deep learning expertise. In this paper we present a series of best practices that can significantly improve the training efficiency and overall accuracy of PINNs. We also put forth a series of challenging benchmark problems that highlight some of the most prominent difficulties in training PINNs, and present comprehensive and fully reproducible ablation studies that demonstrate how different architecture choices and training strategies affect the test accuracy of the resulting models. We show that the methods and guiding principles put forth in this study lead to state-of-the-art results and provide strong baselines that future studies should use for comparison purposes. To this end, we also release a highly optimized library in JAX that can be used to reproduce all results reported in this paper, enable future research studies, as well as facilitate easy adaptation to new use-case scenarios.
</details>
<details>
<summary>摘要</summary>
物理学 informed neural networks (PINNs) 已经广泛应用于深度学习框架，可以快速生成观察数据和部分偏微分方程 (PDE) 约束的模型。然而，它们的实际效果可能受到训练问题和用户缺乏深度学习知识所妨碍。在这篇论文中，我们提出了一系列最佳实践，可以大幅提高 PINNs 的训练效率和总准确率。我们还提出了一系列挑战性的 benchmark 问题，描述了 PINNs 训练中的一些最 prominent difficulties，并进行了完整和可重现的剥离研究，以示不同架构选择和训练策略对模型测试准确率的影响。我们显示了我们在这篇论文中提出的方法和指导原则可以获得 estado-of-the-art 结果，并提供了强大的基准值，以便 future studies 可以用于比较。为此，我们还发布了高度优化的库在 JAX 上，可以重现所有报告在这篇论文中的结果，促进未来研究，以及方便将新的应用场景适应到现有的模型。
</details></li>
</ul>
<hr>
<h2 id="On-Neural-Quantum-Support-Vector-Machines"><a href="#On-Neural-Quantum-Support-Vector-Machines" class="headerlink" title="On Neural Quantum Support Vector Machines"></a>On Neural Quantum Support Vector Machines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08467">http://arxiv.org/abs/2308.08467</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/GhadaAbdulsalam/Explainable_Heart_Disease_Prediction_Using_Ensemble-Quantum_ML">https://github.com/GhadaAbdulsalam/Explainable_Heart_Disease_Prediction_Using_Ensemble-Quantum_ML</a></li>
<li>paper_authors: Lars Simon, Manuel Radons</li>
<li>for: 本研究旨在探讨神经量子支持向量机（NSVM）的训练方法。</li>
<li>methods: 本文使用四种算法来训练神经支持向量机（NSVM），并证明其可行性。</li>
<li>results: 本文延伸了前一文中的结果，提出神经量子支持向量机（NSVM）和量子核函数的概念，并对其进行扩展。<details>
<summary>Abstract</summary>
In \cite{simon2023algorithms} we introduced four algorithms for the training of neural support vector machines (NSVMs) and demonstrated their feasibility. In this note we introduce neural quantum support vector machines, that is, NSVMs with a quantum kernel, and extend our results to this setting.
</details>
<details>
<summary>摘要</summary>
在《《Algorithms for Training Neural Support Vector Machines》》中（[simon2023algorithms）我们介绍了四种算法用于神经支持向量机（NSVM）的训练，并证明其可行性。在这份note中，我们介绍了神经量子支持向量机（NSVM），即具有量子核函数的NSVM，并扩展我们的结果到这种设定下。
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-Uncertainty-Estimation-for-Medical-Image-Segmentation-Networks"><a href="#Hierarchical-Uncertainty-Estimation-for-Medical-Image-Segmentation-Networks" class="headerlink" title="Hierarchical Uncertainty Estimation for Medical Image Segmentation Networks"></a>Hierarchical Uncertainty Estimation for Medical Image Segmentation Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08465">http://arxiv.org/abs/2308.08465</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyu Bai, Wenjia Bai</li>
<li>for: 这个论文的目的是建立一个可靠的医学图像分割模型，以及计算模型预测结果的不确定性。</li>
<li>methods: 这个论文使用了一种简单 yet effective的方法，利用 hierarchical image representation 和 skip-connection module 来估计模型预测结果的不确定性。</li>
<li>results: 这个论文的实验结果表明，通过将这种 hierarchical uncertainty estimation module 加入到深度学习图像分割网络中，可以实现高效的图像分割，同时提供了有意义的不确定性地图，可以用于out-of-distribution排除。<details>
<summary>Abstract</summary>
Learning a medical image segmentation model is an inherently ambiguous task, as uncertainties exist in both images (noise) and manual annotations (human errors and bias) used for model training. To build a trustworthy image segmentation model, it is important to not just evaluate its performance but also estimate the uncertainty of the model prediction. Most state-of-the-art image segmentation networks adopt a hierarchical encoder architecture, extracting image features at multiple resolution levels from fine to coarse. In this work, we leverage this hierarchical image representation and propose a simple yet effective method for estimating uncertainties at multiple levels. The multi-level uncertainties are modelled via the skip-connection module and then sampled to generate an uncertainty map for the predicted image segmentation. We demonstrate that a deep learning segmentation network such as U-net, when implemented with such hierarchical uncertainty estimation module, can achieve a high segmentation performance, while at the same time provide meaningful uncertainty maps that can be used for out-of-distribution detection.
</details>
<details>
<summary>摘要</summary>
学习医学图像分割模型是一个自然存在各种不确定性的任务，图像中的噪声和人工标注（人类错误和偏见）在模型训练中都存在不确定性。为建立可靠的图像分割模型，不仅需要评估其性能，还需要估计模型预测结果的不确定性。现有大多数状态的艺术图像分割网络采用层次编码结构，从细致到粗略提取图像特征。在这种工作中，我们利用这种层次图像表示，并提议一种简单 yet effective的方法来估计多个水平的不确定性。这些多个不确定性被模拟为跳过连接模块，然后采样以生成预测图像分割结果的不确定性地图。我们示示了一个深度学习分割网络如U-Net，当其与多个水平不确定性估计模块相结合时，可以实现高级别的分割性能，同时也可以提供有意义的不确定性地图，用于非标准分布检测。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/17/cs.LG_2023_08_17/" data-id="clm0t8e0i0074v7888i399hwu" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_08_17" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/17/cs.SD_2023_08_17/" class="article-date">
  <time datetime="2023-08-16T16:00:00.000Z" itemprop="datePublished">2023-08-17</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/17/cs.SD_2023_08_17/">cs.SD - 2023-08-17 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Severity-Classification-of-Parkinson’s-Disease-from-Speech-using-Single-Frequency-Filtering-based-Features"><a href="#Severity-Classification-of-Parkinson’s-Disease-from-Speech-using-Single-Frequency-Filtering-based-Features" class="headerlink" title="Severity Classification of Parkinson’s Disease from Speech using Single Frequency Filtering-based Features"></a>Severity Classification of Parkinson’s Disease from Speech using Single Frequency Filtering-based Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09042">http://arxiv.org/abs/2308.09042</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sudarsana Reddy Kadiri, Manila Kodali, Paavo Alku</li>
<li>for: 本研究旨在提出一种新的评估多动性 Parkinson 病（PD）严重程度的对象方法，以提高诊断和治疗的效果。</li>
<li>methods: 该研究使用了单频 filtering（SFF）方法 derive two sets of novel features：（1）SFF cepstral coefficients（SFFCC）和（2）MFCCs from SFF（MFCC-SFF），用于分类PD严重程度。SFF 方法可提供更高的spectro-temporal resolution，而且在多种应用中有着广泛的应用。</li>
<li>results: 实验使用 SVM 分类器，表明提案的特征在三种说话任务（元音、句子、文本读取）中都有出色的表现，与传统的 MFCC 特征相比，提案的 SFFCC 和 MFCC-SFF 特征在元音任务中提高了5.8%和2.3%，在句子任务中提高了7.0%和1.8%，在文本读取任务中提高了2.4%和1.1%。<details>
<summary>Abstract</summary>
Developing objective methods for assessing the severity of Parkinson's disease (PD) is crucial for improving the diagnosis and treatment. This study proposes two sets of novel features derived from the single frequency filtering (SFF) method: (1) SFF cepstral coefficients (SFFCC) and (2) MFCCs from the SFF (MFCC-SFF) for the severity classification of PD. Prior studies have demonstrated that SFF offers greater spectro-temporal resolution compared to the short-time Fourier transform. The study uses the PC-GITA database, which includes speech of PD patients and healthy controls produced in three speaking tasks (vowels, sentences, text reading). Experiments using the SVM classifier revealed that the proposed features outperformed the conventional MFCCs in all three speaking tasks. The proposed SFFCC and MFCC-SFF features gave a relative improvement of 5.8% and 2.3% for the vowel task, 7.0% & 1.8% for the sentence task, and 2.4% and 1.1% for the read text task, in comparison to MFCC features.
</details>
<details>
<summary>摘要</summary>
开发Objective方法评估parkinson病（PD）的严重程度是诊断和治疗的关键。本研究提出了两组新的特征：（1）单频 filtering（SFF）cepstral coefficient（SFFCC）和（2）MFCC from SFF（MFCC-SFF），用于PD严重分类。前研究表明，SFF提供了更高的spectro-temporal分辨率，比short-time Fourier transform。本研究使用PC-GITA数据库，包括PD患者和健康控制者在三种说话任务（vowel、 sentence、 text reading）中的speech。实验表明，提议的特征比普通的MFCC在所有三种说话任务中表现出色，相比MFCC特征，SFFCC和MFCC-SFF特征在vowel任务中提供了5.8%和2.3%的相对改进，在 sentence任务中提供了7.0%和1.8%的相对改进，在read text任务中提供了2.4%和1.1%的相对改进。
</details></li>
</ul>
<hr>
<h2 id="Home-monitoring-for-frailty-detection-through-sound-and-speaker-diarization-analysis"><a href="#Home-monitoring-for-frailty-detection-through-sound-and-speaker-diarization-analysis" class="headerlink" title="Home monitoring for frailty detection through sound and speaker diarization analysis"></a>Home monitoring for frailty detection through sound and speaker diarization analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08985">http://arxiv.org/abs/2308.08985</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yannis Tevissen, Dan Istrate, Vincent Zalc, Jérôme Boudy, Gérard Chollet, Frédéric Petitpont, Sami Boutamine</li>
<li>for: 这个研究是为了开发一个可靠、隐私保护的家庭监测系统，以预防衰老 population 中的衰退。</li>
<li>methods: 这个研究使用了最新的声音处理和speaker diarization技术，以提高现有的嵌入式系统的性能。</li>
<li>results: 研究表明，使用深度神经网络（DNN）的方法可以提高性能，比传统方法提高约100%。<details>
<summary>Abstract</summary>
As the French, European and worldwide populations are aging, there is a strong interest for new systems that guarantee a reliable and privacy preserving home monitoring for frailty prevention. This work is a part of a global environmental audio analysis system which aims to help identification of Activities of Daily Life (ADL) through human and everyday life sounds recognition, speech presence and number of speakers detection. The focus is made on the number of speakers detection. In this article, we present how recent advances in sound processing and speaker diarization can improve the existing embedded systems. We study the performances of two new methods and discuss the benefits of DNN based approaches which improve performances by about 100%.
</details>
<details>
<summary>摘要</summary>
“由于法国、欧洲和全球人口年龄增长，有强大的需求对于保证可靠且隐私保护的家居监控系统。这个工作是全球环境音频分析系统的一部分，旨在通过人类日常生活声音识别、语音存在和说话人数检测来帮助活动日常生活（ADL）的识别。我们在这篇文章中介绍了最新的音频处理和Speaker diarization技术的进步，并评估了这两种新方法的表现。我们发现这些方法可以提高现有的嵌入式系统表现，并且这些方法的表现提升约100%。”Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Explicit-Estimation-of-Magnitude-and-Phase-Spectra-in-Parallel-for-High-Quality-Speech-Enhancement"><a href="#Explicit-Estimation-of-Magnitude-and-Phase-Spectra-in-Parallel-for-High-Quality-Speech-Enhancement" class="headerlink" title="Explicit Estimation of Magnitude and Phase Spectra in Parallel for High-Quality Speech Enhancement"></a>Explicit Estimation of Magnitude and Phase Spectra in Parallel for High-Quality Speech Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08926">http://arxiv.org/abs/2308.08926</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ye-Xin Lu, Yang Ai, Zhen-Hua Ling</li>
<li>for: 提高Speech perceived质量和可读性</li>
<li>methods: 提出了一种新的Speech Enhancement Network（MP-SENet），通过并行地提高了Magnitude和Phasespectra的表示</li>
<li>results: 实验结果表明，MP-SENet在多个任务中具有高质量的Speech增强，包括Speech denoising、dereverberation和频带扩展，并且成功避免了对Magnitude和Phase的相互赔偿效果，从而实现了更好的harmonic Restoration。特别是在Speech denoising任务上，MP-SENet实现了公共数据集VoiceBank+DEMAND上的最佳性能，PESQ为3.60。<details>
<summary>Abstract</summary>
Phase information has a significant impact on speech perceptual quality and intelligibility. However, existing speech enhancement methods encounter limitations in explicit phase estimation due to the non-structural nature and wrapping characteristics of the phase, leading to a bottleneck in enhanced speech quality. To overcome the above issue, in this paper, we proposed MP-SENet, a novel Speech Enhancement Network which explicitly enhances Magnitude and Phase spectra in parallel. The proposed MP-SENet adopts a codec architecture in which the encoder and decoder are bridged by time-frequency Transformers along both time and frequency dimensions. The encoder aims to encode time-frequency representations derived from the input distorted magnitude and phase spectra. The decoder comprises dual-stream magnitude and phase decoders, directly enhancing magnitude and wrapped phase spectra by incorporating a magnitude estimation architecture and a phase parallel estimation architecture, respectively. To train the MP-SENet model effectively, we define multi-level loss functions, including mean square error and perceptual metric loss of magnitude spectra, anti-wrapping loss of phase spectra, as well as mean square error and consistency loss of short-time complex spectra. Experimental results demonstrate that our proposed MP-SENet excels in high-quality speech enhancement across multiple tasks, including speech denoising, dereverberation, and bandwidth extension. Compared to existing phase-aware speech enhancement methods, it successfully avoids the bidirectional compensation effect between the magnitude and phase, leading to a better harmonic restoration. Notably, for the speech denoising task, the MP-SENet yields a state-of-the-art performance with a PESQ of 3.60 on the public VoiceBank+DEMAND dataset.
</details>
<details>
<summary>摘要</summary>
干扰信息对语音质量和可读性有着重要的影响。然而，现有的语音增强方法在显式阶段的阶段估计中遇到了限制，因为干扰信息的非结构性和包袋特性，导致增强语音质量的瓶颈。为解决上述问题，在这篇论文中，我们提出了MP-SENet，一种新的语音增强网络。MP-SENet在平行地增强了大小和频谱的谱 spectrum。提案的MP-SENet采用了codec架构，编码器和解码器通过时间频率变换器在时间和频率维度上相互连接。编码器的目标是将时间频率表示转化为输入损坏的大小和频谱 spectra。解码器包括两个同步的大小和包袋解码器，直接使用包含大小估计架构和包袋平行估计架构来增强损坏的大小和包袋 spectra。为了训练MP-SENet模型，我们定义了多级损失函数，包括平均平方误差和感知度 metric损失、反包袋损失、平均平方误差和一致性损失。实验结果表明，我们的提案的MP-SENet在多个任务中实现了高质量的语音增强，包括语音干扰、频率抑制和频谱扩展。与现有的阶段意识的语音增强方法相比，MP-SENet成功避免了对大小和频谱的双向赔率效应，从而实现了更好的干扰还原。特别是在语音干扰任务中，MP-SENet的PESQ为3.60，在公共的 VoiceBank+DEMAND 数据集上达到了状态机的性能。
</details></li>
</ul>
<hr>
<h2 id="Long-frame-shift-Neural-Speech-Phase-Prediction-with-Spectral-Continuity-Enhancement-and-Interpolation-Error-Compensation"><a href="#Long-frame-shift-Neural-Speech-Phase-Prediction-with-Spectral-Continuity-Enhancement-and-Interpolation-Error-Compensation" class="headerlink" title="Long-frame-shift Neural Speech Phase Prediction with Spectral Continuity Enhancement and Interpolation Error Compensation"></a>Long-frame-shift Neural Speech Phase Prediction with Spectral Continuity Enhancement and Interpolation Error Compensation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08850">http://arxiv.org/abs/2308.08850</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yangai520/lfs-nspp">https://github.com/yangai520/lfs-nspp</a></li>
<li>paper_authors: Yang Ai, Ye-Xin Lu, Zhen-Hua Ling</li>
<li>for: 提高信号处理领域中语音频谱预测的精度，使其能够准确地预测长框框帧偏移的语音频谱。</li>
<li>methods: 提出了一种基于神经网络的长框帧偏移预测方法（LFS-NSPP），包括三个阶段： interpolate、predict和decimate。首先将长框帧频谱spectra interpolated到短框帧频谱spectra的频谱上，然后使用NSPP模型预测短框帧偏移 spectra，最后将长框帧偏移 spectra decimated into short-frame shift phase spectra。</li>
<li>results: 实验结果表明，提出的LFS-NSPP方法可以在预测长框帧偏移 phase spectra方面达到更高的质量，比原始NSPP模型和其他信号处理基于频谱估算法更好。<details>
<summary>Abstract</summary>
Speech phase prediction, which is a significant research focus in the field of signal processing, aims to recover speech phase spectra from amplitude-related features. However, existing speech phase prediction methods are constrained to recovering phase spectra with short frame shifts, which are considerably smaller than the theoretical upper bound required for exact waveform reconstruction of short-time Fourier transform (STFT). To tackle this issue, we present a novel long-frame-shift neural speech phase prediction (LFS-NSPP) method which enables precise prediction of long-frame-shift phase spectra from long-frame-shift log amplitude spectra. The proposed method consists of three stages: interpolation, prediction and decimation. The short-frame-shift log amplitude spectra are first constructed from long-frame-shift ones through frequency-by-frequency interpolation to enhance the spectral continuity, and then employed to predict short-frame-shift phase spectra using an NSPP model, thereby compensating for interpolation errors. Ultimately, the long-frame-shift phase spectra are obtained from short-frame-shift ones through frame-by-frame decimation. Experimental results show that the proposed LFS-NSPP method can yield superior quality in predicting long-frame-shift phase spectra than the original NSPP model and other signal-processing-based phase estimation algorithms.
</details>
<details>
<summary>摘要</summary>
干扰语音阶段预测（Speech phase prediction）是信号处理领域的一个重要研究方向，旨在从振荡功率相关特征中恢复语音阶段спектроgram。然而，现有的语音阶段预测方法都是固定的具有短框架偏移的phaspectra，这些偏移远小于理论最大允许的束缚波形重建短时域 Fourier transform（STFT）。为解决这个问题，我们提出了一种新的长框架偏移神经语音阶段预测（LFS-NSPP）方法，可以准确预测长框架偏移phaspectra从长框架偏移log amplitude spectra。该方法包括三个阶段： interpolate、predict和decimate。首先，从长框架偏移log amplitude spectra中提取出频率维度上的各个频率域的短框架偏移spectra，然后使用NSPP模型预测短框架偏移phaspectra，从而补偿插值错误。最后，通过frame-by-frame decimation，从短框架偏移phaspectra中提取出长框架偏移phaspectra。实验结果表明，提出的LFS-NSPP方法可以在预测长框架偏移phaspectra的质量上提高比原NSPP模型和其他信号处理基于阶段估计算法。
</details></li>
</ul>
<hr>
<h2 id="META-SELD-Meta-Learning-for-Fast-Adaptation-to-the-new-environment-in-Sound-Event-Localization-and-Detection"><a href="#META-SELD-Meta-Learning-for-Fast-Adaptation-to-the-new-environment-in-Sound-Event-Localization-and-Detection" class="headerlink" title="META-SELD: Meta-Learning for Fast Adaptation to the new environment in Sound Event Localization and Detection"></a>META-SELD: Meta-Learning for Fast Adaptation to the new environment in Sound Event Localization and Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08847">http://arxiv.org/abs/2308.08847</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinbo Hu, Yin Cao, Ming Wu, Feiran Yang, Ziying Yu, Wenwu Wang, Mark D. Plumbley, Jun Yang</li>
<li>for: 这个研究是为了解决学习型 зву标定和探测（SELD）方法在不同的Acoustic环境下的性能差异问题。</li>
<li>methods: 这个研究使用了Meta-学习方法来实现快速适应新环境。基于Model Agnostic Meta-Learning（MAML）的Meta-SELD，将寻找适合新环境的初始化参数，并快速适应未见过的环境。</li>
<li>results: 实验结果显示，Meta-SELD在适应新环境方面效果非常好。<details>
<summary>Abstract</summary>
For learning-based sound event localization and detection (SELD) methods, different acoustic environments in the training and test sets may result in large performance differences in the validation and evaluation stages. Different environments, such as different sizes of rooms, different reverberation times, and different background noise, may be reasons for a learning-based system to fail. On the other hand, acquiring annotated spatial sound event samples, which include onset and offset time stamps, class types of sound events, and direction-of-arrival (DOA) of sound sources is very expensive. In addition, deploying a SELD system in a new environment often poses challenges due to time-consuming training and fine-tuning processes. To address these issues, we propose Meta-SELD, which applies meta-learning methods to achieve fast adaptation to new environments. More specifically, based on Model Agnostic Meta-Learning (MAML), the proposed Meta-SELD aims to find good meta-initialized parameters to adapt to new environments with only a small number of samples and parameter updating iterations. We can then quickly adapt the meta-trained SELD model to unseen environments. Our experiments compare fine-tuning methods from pre-trained SELD models with our Meta-SELD on the Sony-TAU Realistic Spatial Soundscapes 2023 (STARSSS23) dataset. The evaluation results demonstrate the effectiveness of Meta-SELD when adapting to new environments.
</details>
<details>
<summary>摘要</summary>
для学习基于声学Event localization and detection（SELD）方法，不同的声学环境在训练和测试集中可能会导致大幅度的性能差异在验证和评估阶段。不同的环境，如不同的房间大小、不同的延迟时间和不同的背景噪音，可能是学习基于系统失败的原因。同时，获取标注的空间声Event样本，包括启动和终止时间戳、声音事件类型和声音源的方向来达（DOA），非常昂贵。此外，在新环境中部署SELD系统经常会出现时间consuming的训练和精度调整问题。为解决这些问题，我们提出Meta-SELD，它应用meta学方法来实现快速适应新环境。更具体地说，基于Model Agnostic Meta-Learning（MAML），我们的Meta-SELD寻找适合新环境的好初始化参数，只需要一小数量的样本和参数更新迭代即可快速适应新环境。我们的实验比较了从预训练SELD模型的细化方法与我们的Meta-SELD在SONY-TAU Realistic Spatial Soundscapes 2023（STARSSS23）数据集上的性能。评估结果表明，Meta-SELD在适应新环境时非常有效。
</details></li>
</ul>
<hr>
<h2 id="Graph-Neural-Network-Backend-for-Speaker-Recognition"><a href="#Graph-Neural-Network-Backend-for-Speaker-Recognition" class="headerlink" title="Graph Neural Network Backend for Speaker Recognition"></a>Graph Neural Network Backend for Speaker Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08767">http://arxiv.org/abs/2308.08767</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liang He, Ruida Li, Mengqi Niu</li>
<li>for: 提高 speaker recognition 精度</li>
<li>methods: 使用图 neural network (GNN)  backend， Mine latent relationships among embeddings for classification</li>
<li>results: 在 NIST SRE14 i-vector challenging、VoxCeleb1-O、VoxCeleb1-E 和 VoxCeleb1-H 数据集上，与主流方法相比，提出的 GNN  backend 显示出了显著的提高。<details>
<summary>Abstract</summary>
Currently, most speaker recognition backends, such as cosine, linear discriminant analysis (LDA), or probabilistic linear discriminant analysis (PLDA), make decisions by calculating similarity or distance between enrollment and test embeddings which are already extracted from neural networks. However, for each embedding, the local structure of itself and its neighbor embeddings in the low-dimensional space is different, which may be helpful for the recognition but is often ignored. In order to take advantage of it, we propose a graph neural network (GNN) backend to mine latent relationships among embeddings for classification. We assume all the embeddings as nodes on a graph, and their edges are computed based on some similarity function, such as cosine, LDA+cosine, or LDA+PLDA. We study different graph settings and explore variants of GNN to find a better message passing and aggregation way to accomplish the recognition task. Experimental results on NIST SRE14 i-vector challenging, VoxCeleb1-O, VoxCeleb1-E, and VoxCeleb1-H datasets demonstrate that our proposed GNN backends significantly outperform current mainstream methods.
</details>
<details>
<summary>摘要</summary>
当前大多数说话识别后端，如cosine、线性混合分析（LDA）或概率线性混合分析（PLDA），做出决策时通常计算测试和托管模型之间的相似性或距离。然而，每个嵌入都有自己本地结构，与邻居嵌入在低维度空间中的结构不同，这可能对识别有帮助，但通常被忽略。为了利用这一点，我们提议使用图ael neural network（GNN）后端，挖掘嵌入之间的隐藏关系，用于分类。我们将所有嵌入视为图ael中的节点，其间的边是根据某种相似函数，如cosine、LDA+cosine或LDA+PLDA计算。我们研究不同的图ael设置和GNN变体，找到更好的消息传递和聚合方式，以完成识别任务。实验结果表明，我们提议的GNN后端在NIST SRE14 i-vector挑战、VoxCeleb1-O、VoxCeleb1-E和VoxCeleb1-H数据集上显著超越了当前主流方法。
</details></li>
</ul>
<hr>
<h2 id="The-DKU-MSXF-Speaker-Verification-System-for-the-VoxCeleb-Speaker-Recognition-Challenge-2023"><a href="#The-DKU-MSXF-Speaker-Verification-System-for-the-VoxCeleb-Speaker-Recognition-Challenge-2023" class="headerlink" title="The DKU-MSXF Speaker Verification System for the VoxCeleb Speaker Recognition Challenge 2023"></a>The DKU-MSXF Speaker Verification System for the VoxCeleb Speaker Recognition Challenge 2023</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08766">http://arxiv.org/abs/2308.08766</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ze Li, Yuke Lin, Xiaoyi Qin, Ning Jiang, Guoqing Zhao, Ming Li</li>
<li>for: 本文是DKU-MSXF系统的track1、track2和track3的VoxCeleb Speaker Recognition Challenge 2023（VoxSRC-23）系统描述。</li>
<li>methods: 我们使用基于ResNet的网络结构进行训练，并通过构建跨年龄QMF训练集来实现显著提高系统性能。</li>
<li>results: 在track1中，我们通过混合训练方法和VOXBLINK-clean数据集来提高模型性能，相比track1，包含VOXBLINK-clean数据集的模型表现提高了 más de 10%。在track3中，我们采用了一种新的假标签方法，并通过三个阈值和子中心纯化来进行频率附加预测，最终提交得分为task1的mDCF0.1243、track2的mDCF0.1165和track3的EER4.952%。<details>
<summary>Abstract</summary>
This paper is the system description of the DKU-MSXF System for the track1, track2 and track3 of the VoxCeleb Speaker Recognition Challenge 2023 (VoxSRC-23). For Track 1, we utilize a network structure based on ResNet for training. By constructing a cross-age QMF training set, we achieve a substantial improvement in system performance. For Track 2, we inherite the pre-trained model from Track 1 and conducte mixed training by incorporating the VoxBlink-clean dataset. In comparison to Track 1, the models incorporating VoxBlink-clean data exhibit a performance improvement by more than 10% relatively. For Track3, the semi-supervised domain adaptation task, a novel pseudo-labeling method based on triple thresholds and sub-center purification is adopted to make domain adaptation. The final submission achieves mDCF of 0.1243 in task1, mDCF of 0.1165 in Track 2 and EER of 4.952% in Track 3.
</details>
<details>
<summary>摘要</summary>
这篇论文是DKU-MSXF系统的系统描述，用于VoxCeleb Speaker Recognition Challenge 2023（VoxSRC-23）的track1、track2和track3。在track1中，我们采用基于ResNet的网络结构进行训练，通过构建跨年龄QMF训练集，实现了显著提高系统性能。在track2中，我们继承了track1中的预训练模型，并通过将VoxBlink-clean数据集integrated进行混合训练，相比track1，模型包含VoxBlink-clean数据显示了超过10%的性能提高。在track3中，我们采用了一种新的半监督领域适应方法，基于 triple thresholds和sub-center purification，实现了领域适应。最终提交的结果为task1中的mDCF为0.1243，track2中的mDCF为0.1165，以及track3中的EER为4.952%。
</details></li>
</ul>
<hr>
<h2 id="Decoding-Emotions-A-comprehensive-Multilingual-Study-of-Speech-Models-for-Speech-Emotion-Recognition"><a href="#Decoding-Emotions-A-comprehensive-Multilingual-Study-of-Speech-Models-for-Speech-Emotion-Recognition" class="headerlink" title="Decoding Emotions: A comprehensive Multilingual Study of Speech Models for Speech Emotion Recognition"></a>Decoding Emotions: A comprehensive Multilingual Study of Speech Models for Speech Emotion Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08713">http://arxiv.org/abs/2308.08713</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/95anantsingh/decoding-emotions">https://github.com/95anantsingh/decoding-emotions</a></li>
<li>paper_authors: Anant Singh, Akshat Gupta</li>
<li>for: 这paper主要是为了评估多种语言下的语音情感识别（SER）模型，以及这些模型的内部表示方法。</li>
<li>methods: 这paper使用了八种语音表示模型和六种语言进行比较，并通过 probing 实验来探索这些模型的内部工作方式。</li>
<li>results: 这paper得到了 average 的错误率下降32%，并在德语和波斯语中达到了状态的最佳 результаados。 probing 结果表明，语音模型的中间层 capture 最重要的情感信息。<details>
<summary>Abstract</summary>
Recent advancements in transformer-based speech representation models have greatly transformed speech processing. However, there has been limited research conducted on evaluating these models for speech emotion recognition (SER) across multiple languages and examining their internal representations. This article addresses these gaps by presenting a comprehensive benchmark for SER with eight speech representation models and six different languages. We conducted probing experiments to gain insights into inner workings of these models for SER. We find that using features from a single optimal layer of a speech model reduces the error rate by 32\% on average across seven datasets when compared to systems where features from all layers of speech models are used. We also achieve state-of-the-art results for German and Persian languages. Our probing results indicate that the middle layers of speech models capture the most important emotional information for speech emotion recognition.
</details>
<details>
<summary>摘要</summary>
近期的变换器基本模型在语音处理方面做出了重要进步，但是对于多语言语音情感识别（SER）的评估和内部表示却受到了有限的研究。本文填补这些差距，通过提供八种语音表示模型和六种不同语言的完整性评估。我们进行了探索实验，以了解这些模型内部的工作方式。我们发现，从单一最佳层的语音模型中提取特征可以降低错误率平均为32%，比较于使用所有层语音模型特征系统来说。我们还在德国语和波斯语方面达到了状态的最佳成绩。我们的探索结果表明，语音模型的中间层 capture最重要的情感信息。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/17/cs.SD_2023_08_17/" data-id="clm0t8e1c00acv788b0ljamtu" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_08_17" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/17/eess.IV_2023_08_17/" class="article-date">
  <time datetime="2023-08-16T16:00:00.000Z" itemprop="datePublished">2023-08-17</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/17/eess.IV_2023_08_17/">eess.IV - 2023-08-17 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Eosinophils-Instance-Object-Segmentation-on-Whole-Slide-Imaging-Using-Multi-label-Circle-Representation"><a href="#Eosinophils-Instance-Object-Segmentation-on-Whole-Slide-Imaging-Using-Multi-label-Circle-Representation" class="headerlink" title="Eosinophils Instance Object Segmentation on Whole Slide Imaging Using Multi-label Circle Representation"></a>Eosinophils Instance Object Segmentation on Whole Slide Imaging Using Multi-label Circle Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08974">http://arxiv.org/abs/2308.08974</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yilinliu610730/eoe">https://github.com/yilinliu610730/eoe</a></li>
<li>paper_authors: Yilin Liu, Ruining Deng, Juming Xiong, Regina N Tyree, Hernan Correa, Girish Hiremath, Yaohong Wang, Yuankai Huo</li>
<li>for: 该研究旨在提高食管炎症诊断的精度和效率，并且提供一种自动化的诊断方法。</li>
<li>methods: 该研究使用了圆形表示法和圆形蛇形模型来实现自动化的实例 segmentation。</li>
<li>results: 对比传统的Mask R-CNN模型和DeepSnake模型，圆形蛇形模型在识别和分割嗜好蛋白质方面表现出了superiority，这可能地提高了EoE诊断的精度和效率。<details>
<summary>Abstract</summary>
Eosinophilic esophagitis (EoE) is a chronic and relapsing disease characterized by esophageal inflammation. Symptoms of EoE include difficulty swallowing, food impaction, and chest pain which significantly impact the quality of life, resulting in nutritional impairments, social limitations, and psychological distress. The diagnosis of EoE is typically performed with a threshold (15 to 20) of eosinophils (Eos) per high-power field (HPF). Since the current counting process of Eos is a resource-intensive process for human pathologists, automatic methods are desired. Circle representation has been shown as a more precise, yet less complicated, representation for automatic instance cell segmentation such as CircleSnake approach. However, the CircleSnake was designed as a single-label model, which is not able to deal with multi-label scenarios. In this paper, we propose the multi-label CircleSnake model for instance segmentation on Eos. It extends the original CircleSnake model from a single-label design to a multi-label model, allowing segmentation of multiple object types. Experimental results illustrate the CircleSnake model's superiority over the traditional Mask R-CNN model and DeepSnake model in terms of average precision (AP) in identifying and segmenting eosinophils, thereby enabling enhanced characterization of EoE. This automated approach holds promise for streamlining the assessment process and improving diagnostic accuracy in EoE analysis. The source code has been made publicly available at https://github.com/yilinliu610730/EoE.
</details>
<details>
<summary>摘要</summary>
《营养细胞损伤综合征（EoE）是一种慢性和再次发生的疾病，特征为食管内部的Inflammation。EoE的症状包括困难吞食、食物堵塞和胸痛，对生活质量产生重大影响，导致营养不良、社会限制和心理压力。EoE的诊断通常通过Esophageal高力场（HPF）中Eosinophils（Eos）的数量（15-20）进行。由于当前的Eos数计数过程需要人工Pathologist的劳动，因此自动方法被欢迎。圆形表示已被证明为更精准， yet less complicated的表示方法，但它是单标签模型，无法处理多标签场景。本文提出了基于圆形的多标签CircleSnake模型，用于实例分 segmentation。这个模型从单标签设计扩展到多标签模型，可以进行多种对象类型的分 segmentation。实验结果表明，CircleSnake模型在AP（准确率）方面与传统的Mask R-CNN模型和DeepSnake模型相比，在标识和分 segmentationEosinophils方面表现出了超过其他两个模型的优势。这种自动化方法可以提高EoE分析过程的效率和准确性，并且代码已经在https://github.com/yilinliu610730/EoE上公开发布。
</details></li>
</ul>
<hr>
<h2 id="An-inexact-proximal-majorization-minimization-Algorithm-for-remote-sensing-image-stripe-noise-removal"><a href="#An-inexact-proximal-majorization-minimization-Algorithm-for-remote-sensing-image-stripe-noise-removal" class="headerlink" title="An inexact proximal majorization-minimization Algorithm for remote sensing image stripe noise removal"></a>An inexact proximal majorization-minimization Algorithm for remote sensing image stripe noise removal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08866">http://arxiv.org/abs/2308.08866</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chengjing Wang, Xile Zhao, Qingsong Wang, Zepei Ma, Peipei Tang</li>
<li>for: 提高远程感知图像中的视觉质量和数据分析精度，抑制远程感知图像中的梭噪。</li>
<li>methods: 提出非凸模型，使用DC函数结构进行梭噪除除。解决方法利用DC结构和不准确的 proximal 大esten-multipliers 算法，并设计实现的停止条件。</li>
<li>results: 数值实验表明提出的模型和算法在梭噪除除方面具有superiority。<details>
<summary>Abstract</summary>
The stripe noise existing in remote sensing images badly degrades the visual quality and restricts the precision of data analysis. Therefore, many destriping models have been proposed in recent years. In contrast to these existing models, in this paper, we propose a nonconvex model with a DC function (i.e., the difference of convex functions) structure to remove the strip noise. To solve this model, we make use of the DC structure and apply an inexact proximal majorization-minimization algorithm with each inner subproblem solved by the alternating direction method of multipliers. It deserves mentioning that we design an implementable stopping criterion for the inner subproblem, while the convergence can still be guaranteed. Numerical experiments demonstrate the superiority of the proposed model and algorithm.
</details>
<details>
<summary>摘要</summary>
“远程感知图像中的条纹噪音会严重损害视觉质量和数据分析精度。因此，过去几年内，许多条纹除去模型已经被提出。与现有模型不同，在本文中，我们提出了一种非凸模型，其结构是基于差分 convex 函数（DC 函数）。为解决这个模型，我们利用 DC 结构，并采用不准确的 proximal 主要化-最小化算法，其中每个内部子问题通过 alternate direction method of multipliers 解决。值得一提的是，我们设计了可实施的停止条件，而且可以保证 converge。数值实验表明，提出的模型和算法具有优势。”Here's a word-for-word translation of the text into Simplified Chinese:“远程感知图像中的条纹噪音会严重损害视觉质量和数据分析精度。因此，过去几年内，许多条纹除去模型已经被提出。与现有模型不同，在本文中，我们提出了一种非凸模型，其结构是基于差分 convex 函数（DC 函数）。为解决这个模型，我们利用 DC 结构，并采用不准确的 proximal 主要化-最小化算法，其中每个内部子问题通过 alternate direction method of multipliers 解决。值得一提的是，我们设计了可实施的停止条件，而且可以保证 converge。数值实验表明，提出的模型和算法具有优势。”
</details></li>
</ul>
<hr>
<h2 id="End-to-end-Alternating-Optimization-for-Real-World-Blind-Super-Resolution"><a href="#End-to-end-Alternating-Optimization-for-Real-World-Blind-Super-Resolution" class="headerlink" title="End-to-end Alternating Optimization for Real-World Blind Super Resolution"></a>End-to-end Alternating Optimization for Real-World Blind Super Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08816">http://arxiv.org/abs/2308.08816</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/greatlog/realdan">https://github.com/greatlog/realdan</a></li>
<li>paper_authors: Zhengxiong Luo, Yan Huang, Shang Li, Liang Wang, Tieniu Tan<br>for: 这个论文的目的是提出一种基于 alternate optimization 算法的盲SR方法，以提高盲SR的精度和稳定性。methods: 该方法使用了两个卷积神经网络：Restorer 和 Estimator。 Restorer 用于还原 SR 图像，而 Estimator 用于估计质量损失。这两个模块在 alternate 的形式下进行循环训练，以便互相优化。results: 实验表明，提出的方法可以大幅超越当前state-of-the-art 方法，并生成更加可观的结果。<details>
<summary>Abstract</summary>
Blind Super-Resolution (SR) usually involves two sub-problems: 1) estimating the degradation of the given low-resolution (LR) image; 2) super-resolving the LR image to its high-resolution (HR) counterpart. Both problems are ill-posed due to the information loss in the degrading process. Most previous methods try to solve the two problems independently, but often fall into a dilemma: a good super-resolved HR result requires an accurate degradation estimation, which however, is difficult to be obtained without the help of original HR information. To address this issue, instead of considering these two problems independently, we adopt an alternating optimization algorithm, which can estimate the degradation and restore the SR image in a single model. Specifically, we design two convolutional neural modules, namely \textit{Restorer} and \textit{Estimator}. \textit{Restorer} restores the SR image based on the estimated degradation, and \textit{Estimator} estimates the degradation with the help of the restored SR image. We alternate these two modules repeatedly and unfold this process to form an end-to-end trainable network. In this way, both \textit{Restorer} and \textit{Estimator} could get benefited from the intermediate results of each other, and make each sub-problem easier. Moreover, \textit{Restorer} and \textit{Estimator} are optimized in an end-to-end manner, thus they could get more tolerant of the estimation deviations of each other and cooperate better to achieve more robust and accurate final results. Extensive experiments on both synthetic datasets and real-world images show that the proposed method can largely outperform state-of-the-art methods and produce more visually favorable results. The codes are rleased at \url{https://github.com/greatlog/RealDAN.git}.
</details>
<details>
<summary>摘要</summary>
干Resolution（SR）问题通常包含两个互相关联的互补问题：1）估计LR图像的劣化程度；2）LR图像的超Resolution（HR）图像。两个问题都是不定的，因为升级过程中的信息损失。大多数前一代方法会独立地解决这两个问题，但经常陷入一个困境：一个好的HR图像需要一个准确的劣化估计，但是不可以不使用原始HR图像来获得这个估计。为解决这个问题，我们采用了一种alternating optimization算法，可以同时估计劣化和SR图像。我们设计了两个卷积神经网络模块：Restorer和Estimator。Restorer使用估计的劣化来恢复SR图像，而Estimator使用恢复后的SR图像来估计劣化。我们重复地使用这两个模块，并将其拓展成一个端到端可训练的网络。这样，Restorer和Estimator都可以受益于对方的中间结果，使每个问题变得更加容易。此外，Restorer和Estimator在端到端上进行了结构优化，因此它们可以更快地适应对方的估计偏差，并更好地合作以实现更加稳定和准确的最终结果。我们在 synthetic datasets 和实际图像上进行了广泛的实验，结果显示，我们的方法可以与当前状态计算机技术相比，大幅提高SR图像的质量。代码可以在 \url{https://github.com/greatlog/RealDAN.git} 中下载。
</details></li>
</ul>
<hr>
<h2 id="Recursive-Detection-and-Analysis-of-Nanoparticles-in-Scanning-Electron-Microscopy-Images"><a href="#Recursive-Detection-and-Analysis-of-Nanoparticles-in-Scanning-Electron-Microscopy-Images" class="headerlink" title="Recursive Detection and Analysis of Nanoparticles in Scanning Electron Microscopy Images"></a>Recursive Detection and Analysis of Nanoparticles in Scanning Electron Microscopy Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08732">http://arxiv.org/abs/2308.08732</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aidan S. Wright, Nathaniel P. Youmans, Enrique F. Valderrama Araya</li>
<li>for: 这种 computational framework 的目的是为了精准检测和全面分析 SEM 图像中的粒子。</li>
<li>methods: 这个框架使用了 Python 图像处理库 OpenCV、SciPy 和 Scikit-Image，并结合了阈值处理、膨润和推杂等技术来提高图像处理结果的准确性。</li>
<li>results: 这个框架可以准确地确定粒子坐标，并提取粒子的相关形态特征，包括面积、方向、亮度和长度。 它在五个不同的测试图像中达到 97% 的粒子检测精度。<details>
<summary>Abstract</summary>
In this study, we present a computational framework tailored for the precise detection and comprehensive analysis of nanoparticles within scanning electron microscopy (SEM) images. The primary objective of this framework revolves around the accurate localization of nanoparticle coordinates, accompanied by secondary objectives encompassing the extraction of pertinent morphological attributes including area, orientation, brightness, and length.   Constructed leveraging the robust image processing capabilities of Python, particularly harnessing libraries such as OpenCV, SciPy, and Scikit-Image, the framework employs an amalgamation of techniques, including thresholding, dilating, and eroding, to enhance the fidelity of image processing outcomes.   The ensuing nanoparticle data is seamlessly integrated into the RStudio environment to facilitate meticulous post-processing analysis. This encompasses a comprehensive evaluation of model accuracy, discernment of feature distribution patterns, and the identification of intricate particle arrangements. The finalized framework exhibits high nanoparticle identification within the primary sample image and boasts 97\% accuracy in detecting particles across five distinct test images drawn from a SEM nanoparticle dataset. Furthermore, the framework demonstrates the capability to discern nanoparticles of faint intensity, eluding manual labeling within the control group.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们提出了一个计算框架，用于精确检测和全面分析射电镜像（SEM）中的粒子。主要目标是准确地确定粒子坐标，并且包括次要目标，如粒子形态特征的提取，包括面积、方向、亮度和长度。这个框架利用Python的强大图像处理能力，特别是OpenCV、SciPy和Scikit-Image库，结合多种技术，如阈值、扩展和膨润，以提高图像处理结果的准确性。获得的粒子数据可以轻松地 интегрирова到RStudio环境中，进行仔细的后处理分析。这包括完整评估模型准确性，分析特征分布图像，以及识别复杂的粒子排列。实验结果显示，该框架在主要样本图像中具有高精度的粒子检测，并在五个不同的测试图像中达到97%的检测精度。此外，框架还能够识别具有柔弱亮度的粒子，而控制组中的人工标注不能达到。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Kernel-Based-Adaptive-Spatial-Aggregation-for-Learned-Image-Compression"><a href="#Dynamic-Kernel-Based-Adaptive-Spatial-Aggregation-for-Learned-Image-Compression" class="headerlink" title="Dynamic Kernel-Based Adaptive Spatial Aggregation for Learned Image Compression"></a>Dynamic Kernel-Based Adaptive Spatial Aggregation for Learned Image Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08723">http://arxiv.org/abs/2308.08723</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Huairui/DKIC">https://github.com/Huairui/DKIC</a></li>
<li>paper_authors: Huairui Wang, Nianxiang Fu, Zhenzhong Chen, Shan Liu</li>
<li>for: 提高图像压缩率和精度性能</li>
<li>methods: 使用动态核kernel基于转换编码，适应权重分享机制和自适应积集方法</li>
<li>results: 实验结果显示，当前方法在三个标准测试集上比现有学习基于方法具有更高的率压缩率和精度性能<details>
<summary>Abstract</summary>
Learned image compression methods have shown superior rate-distortion performance and remarkable potential compared to traditional compression methods. Most existing learned approaches use stacked convolution or window-based self-attention for transform coding, which aggregate spatial information in a fixed range. In this paper, we focus on extending spatial aggregation capability and propose a dynamic kernel-based transform coding. The proposed adaptive aggregation generates kernel offsets to capture valid information in the content-conditioned range to help transform. With the adaptive aggregation strategy and the sharing weights mechanism, our method can achieve promising transform capability with acceptable model complexity. Besides, according to the recent progress of entropy model, we define a generalized coarse-to-fine entropy model, considering the coarse global context, the channel-wise, and the spatial context. Based on it, we introduce dynamic kernel in hyper-prior to generate more expressive global context. Furthermore, we propose an asymmetric spatial-channel entropy model according to the investigation of the spatial characteristics of the grouped latents. The asymmetric entropy model aims to reduce statistical redundancy while maintaining coding efficiency. Experimental results demonstrate that our method achieves superior rate-distortion performance on three benchmarks compared to the state-of-the-art learning-based methods.
</details>
<details>
<summary>摘要</summary>
现有的学习型压缩方法已经显示出了Superior rate-distortion性能和吸引人的潜在性，相比传统压缩方法。大多数现有的学习方法使用堆叠 convolution或窗口基于自注意力 для变换编码，这些方法会汇集Fixed距离内的空间信息。在这篇论文中，我们关注到了扩展空间汇集能力，并提出了动态核心基于变换编码。我们的提案的适应汇集生成核心偏移来捕捉有效信息在内容受限的范围内，以帮助变换。通过适应汇集策略和共享权重机制，我们的方法可以实现可接受的变换能力，同时减少模型复杂度。此外，根据最近的Entropy模型进展，我们定义一个通用Coarse-to-fine Entropy模型，考虑Global上下文、通道级和空间上下文。基于它，我们引入动态核心在超乎 prior中生成更 expresive的全局上下文。另外，我们提出一种不对称的空间通道Entropy模型，根据Latent集的特点进行调整。这种不对称Entropy模型的目的是减少统计重复，保持编码效率。实验结果表明，我们的方法在三个标准底本上比State-of-the-art学习型方法 superior rate-distortion性能。
</details></li>
</ul>
<hr>
<h2 id="Deployment-and-Analysis-of-Instance-Segmentation-Algorithm-for-In-field-Grade-Estimation-of-Sweetpotatoes"><a href="#Deployment-and-Analysis-of-Instance-Segmentation-Algorithm-for-In-field-Grade-Estimation-of-Sweetpotatoes" class="headerlink" title="Deployment and Analysis of Instance Segmentation Algorithm for In-field Grade Estimation of Sweetpotatoes"></a>Deployment and Analysis of Instance Segmentation Algorithm for In-field Grade Estimation of Sweetpotatoes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08534">http://arxiv.org/abs/2308.08534</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hoang M. Nguyen, Sydney Gyurek, Russell Mierop, Kenneth V. Pecota, Kylie LaGamba, Michael Boyette, G. Craig Yencho, Cranos M. Williams, Michael W. Kudenov<br>for:这个论文是为了提出一种直接在场地中进行 Storage roots 的检测和评估，以便更快速地获得 yields 的方法。methods:这个方法使用了 Detectron2 库中的深度学习对象检测算法，实现了 Mask R-CNN 模型，用于实时地在场地中识别 Storage roots。results:模型可以在不同的环境条件下（包括光照和土壤特性的变化）正确地识别 Storage roots，并且与商业化光学排分器的比较表明，模型的 RMSE 值为0.66 cm，1.22 cm，74.73 g 分别，而 root 数量的 RMSE 值为5.27根，r^2 值为0.8。这种phenotyping 策略有 potential 用于实时地在场地中获得 yields，而不需要高科技和昂贵的光学排分器。<details>
<summary>Abstract</summary>
Shape estimation of sweetpotato (SP) storage roots is inherently challenging due to their varied size and shape characteristics. Even measuring "simple" metrics, such as length and width, requires significant time investments either directly in-field or afterward using automated graders. In this paper, we present the results of a model that can perform grading and provide yield estimates directly in the field quicker than manual measurements. Detectron2, a library consisting of deep-learning object detection algorithms, was used to implement Mask R-CNN, an instance segmentation model. This model was deployed for in-field grade estimation of SPs and evaluated against an optical sorter. Storage roots from various clones imaged with a cellphone during trials between 2019 and 2020, were used in the model's training and validation to fine-tune a model to detect SPs. Our results showed that the model could distinguish individual SPs in various environmental conditions including variations in lighting and soil characteristics. RMSE for length, width, and weight, from the model compared to a commercial optical sorter, were 0.66 cm, 1.22 cm, and 74.73 g, respectively, while the RMSE of root counts per plot was 5.27 roots, with r^2 = 0.8. This phenotyping strategy has the potential enable rapid yield estimates in the field without the need for sophisticated and costly optical sorters and may be more readily deployed in environments with limited access to these kinds of resources or facilities.
</details>
<details>
<summary>摘要</summary>
sweetpotato (SP) 存储根的形状评估是一项自然的挑战，因为它们的形状和大小具有很大的变化。 même measuring "simple" metrics, such as length and width, requires a significant investment of time, either directly in the field or using automated graders. In this paper, we present the results of a model that can perform grading and provide yield estimates directly in the field faster than manual measurements. We used Detectron2, a library consisting of deep-learning object detection algorithms, to implement Mask R-CNN, an instance segmentation model. This model was deployed for in-field grade estimation of SPs and evaluated against a commercial optical sorter. Storage roots from various clones imaged with a cellphone during trials between 2019 and 2020 were used to fine-tune the model to detect SPs. Our results showed that the model could distinguish individual SPs in various environmental conditions, including variations in lighting and soil characteristics. The root mean squared error (RMSE) for length, width, and weight, from the model compared to a commercial optical sorter, were 0.66 cm, 1.22 cm, and 74.73 g, respectively, while the RMSE of root counts per plot was 5.27 roots, with r^2 = 0.8. This phenotyping strategy has the potential to enable rapid yield estimates in the field without the need for sophisticated and costly optical sorters, and may be more readily deployed in environments with limited access to these kinds of resources or facilities.
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Distill-Global-Representation-for-Sparse-View-CT"><a href="#Learning-to-Distill-Global-Representation-for-Sparse-View-CT" class="headerlink" title="Learning to Distill Global Representation for Sparse-View CT"></a>Learning to Distill Global Representation for Sparse-View CT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08463">http://arxiv.org/abs/2308.08463</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zilong Li, Chenglong Ma, Jie Chen, Junping Zhang, Hongming Shan</li>
<li>for: 这篇论文的目的是提出一种新的图像后处理方法，以提高稀疏视角计算Tomography（CT）图像的质量。</li>
<li>methods: 该方法使用了 globale representation（GloRe）核对应法，并通过对GloRe进行方向填充和频率特征填充来提高图像质量。</li>
<li>results: 对比于现有方法，该方法的 globale representation（GloRe）核对应法可以更好地提高稀疏视角CT图像的质量，并且可以更好地捕捉临床重要的诊断信息。<details>
<summary>Abstract</summary>
Sparse-view computed tomography (CT) -- using a small number of projections for tomographic reconstruction -- enables much lower radiation dose to patients and accelerated data acquisition. The reconstructed images, however, suffer from strong artifacts, greatly limiting their diagnostic value. Current trends for sparse-view CT turn to the raw data for better information recovery. The resultant dual-domain methods, nonetheless, suffer from secondary artifacts, especially in ultra-sparse view scenarios, and their generalization to other scanners/protocols is greatly limited. A crucial question arises: have the image post-processing methods reached the limit? Our answer is not yet. In this paper, we stick to image post-processing methods due to great flexibility and propose global representation (GloRe) distillation framework for sparse-view CT, termed GloReDi. First, we propose to learn GloRe with Fourier convolution, so each element in GloRe has an image-wide receptive field. Second, unlike methods that only use the full-view images for supervision, we propose to distill GloRe from intermediate-view reconstructed images that are readily available but not explored in previous literature. The success of GloRe distillation is attributed to two key components: representation directional distillation to align the GloRe directions, and band-pass-specific contrastive distillation to gain clinically important details. Extensive experiments demonstrate the superiority of the proposed GloReDi over the state-of-the-art methods, including dual-domain ones. The source code is available at https://github.com/longzilicart/GloReDi.
</details>
<details>
<summary>摘要</summary>
《简洁 computed tomography（CT）》——使用少量投射进行tomographic重建——可以大幅降低对病人的辐射剂量和数据获取的时间。然而，重建的图像却受到强烈的artefacts的限制，从而大大降低其诊断价值。当前的 sparse-view CT 趋势是转向原始数据，以便更好地回收信息。然而，结果性的 dual-domain 方法在 ultra-sparse 视图场景下受到次要artefact的影响，而且其在其他扫描器/协议上的普遍性受限。问题是：图像后处理方法是否已经达到了限制？我们的答案是不是。在这篇论文中，我们坚持使用图像后处理方法，因为它具有很大的灵活性。我们提出了 GloRe 整合框架（GloReDi），用于 sparse-view CT。首先，我们提出了学习 GloRe 使用 Fourier 杂化，使每个 GloRe 元素具有整个图像的广泛响应场。其次，不同于以前的方法，我们提出了使用 intermediate-view 重建图像进行监督，这些图像ready available，但在前期 литературе未被探讨。 GloRe 整合框架的成功归因于两个关键组成部分： representation directional distillation 用于对 GloRe 方向进行对齐，以及 band-pass-specific contrastive distillation 用于获取临床重要的细节。我们对 GloReDi 进行了广泛的实验，并证明其在state-of-the-art方法之上。源代码可以在 https://github.com/longzilicart/GloReDi 上获取。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/17/eess.IV_2023_08_17/" data-id="clm0t8e2t00f8v788bbsohc35" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_16" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/16/cs.LG_2023_08_16/" class="article-date">
  <time datetime="2023-08-15T16:00:00.000Z" itemprop="datePublished">2023-08-16</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/16/cs.LG_2023_08_16/">cs.LG - 2023-08-16 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Accurate-synthesis-of-Dysarthric-Speech-for-ASR-data-augmentation"><a href="#Accurate-synthesis-of-Dysarthric-Speech-for-ASR-data-augmentation" class="headerlink" title="Accurate synthesis of Dysarthric Speech for ASR data augmentation"></a>Accurate synthesis of Dysarthric Speech for ASR data augmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08438">http://arxiv.org/abs/2308.08438</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Soleymanpour, Michael T. Johnson, Rahim Soleymanpour, Jeffrey Berry</li>
<li>for: 这篇论文的目的是为了提高自动语音识别（ASR）系统的性能，通过增加疾病某些特征来提供更多的疾病样本。</li>
<li>methods: 这篇论文使用了一种基于神经网络的多话者Text-to-Speech（TTS）系统，并在其中添加了疾病严重程度 coefficient和停顿插入模型，以生成不同疾病严重程度的语音。</li>
<li>results: 这篇论文使用了这种模型来生成训练数据，并通过对其进行语音识别测试来评估其效果。结果显示，在使用这种模型和数据 augmentation 技术后，ASR系统的识别精度得到了显著改善。此外，对比基eline模型，添加疾病严重程度和停顿控制可以降低WRR值6.5%。<details>
<summary>Abstract</summary>
Dysarthria is a motor speech disorder often characterized by reduced speech intelligibility through slow, uncoordinated control of speech production muscles. Automatic Speech recognition (ASR) systems can help dysarthric talkers communicate more effectively. However, robust dysarthria-specific ASR requires a significant amount of training speech, which is not readily available for dysarthric talkers. This paper presents a new dysarthric speech synthesis method for the purpose of ASR training data augmentation. Differences in prosodic and acoustic characteristics of dysarthric spontaneous speech at varying severity levels are important components for dysarthric speech modeling, synthesis, and augmentation. For dysarthric speech synthesis, a modified neural multi-talker TTS is implemented by adding a dysarthria severity level coefficient and a pause insertion model to synthesize dysarthric speech for varying severity levels. To evaluate the effectiveness for synthesis of training data for ASR, dysarthria-specific speech recognition was used. Results show that a DNN-HMM model trained on additional synthetic dysarthric speech achieves WER improvement of 12.2% compared to the baseline, and that the addition of the severity level and pause insertion controls decrease WER by 6.5%, showing the effectiveness of adding these parameters. Overall results on the TORGO database demonstrate that using dysarthric synthetic speech to increase the amount of dysarthric-patterned speech for training has significant impact on the dysarthric ASR systems. In addition, we have conducted a subjective evaluation to evaluate the dysarthric-ness and similarity of synthesized speech. Our subjective evaluation shows that the perceived dysartrhic-ness of synthesized speech is similar to that of true dysarthric speech, especially for higher levels of dysarthria
</details>
<details>
<summary>摘要</summary>
<<SYS>>这是一种 дви力问题，常见于语言生成功能不足的话者。自动语音识别系统（ASR）可以帮助这些话者更有效地沟通。然而，为了建立坚固的动力症特定ASR系统，需要大量的训练语音，但这些语音仅存在于少数话者身上。这篇文章提出了一新的动力症特定语音合成方法，用于ASR训练语音资料增强。在不同的严重程度下，动力症特定语音的语音和态度特征是合成语音模型的重要组成部分。为了合成动力症语音，我们将 modificated neural multi-talker TTS加入了动力症严重程度 coefficient和暂停插入模型，以合成不同严重程度的动力症语音。为了评估这种合成语音的有效性，我们使用了动力症特定语音识别系统。结果显示，将额外的合成动力症语音训练到DNN-HMM模型可以提高WRR值12.2%，并且将严重程度和暂停插入控制添加到合成语音模型可以降低WRR值6.5%，这说明了将这些参数添加的有效性。总结来说，使用动力症合成语音增加训练语音量有重要的影响力在动力症ASR系统。此外，我们还进行了主观评估，以评估合成语音的动力症程度和真实性。我们的主观评估显示，合成语音的动力症程度与真实动力症语音相似，尤其是在更高的严重程度下。这篇文章的结论是，使用动力症合成语音增加训练语音量可以提高动力症ASR系统的性能。这种方法可以帮助建立更加坚固的动力症ASR系统，并且可以增加训练语音量。
</details></li>
</ul>
<hr>
<h2 id="Eliciting-Risk-Aversion-with-Inverse-Reinforcement-Learning-via-Interactive-Questioning"><a href="#Eliciting-Risk-Aversion-with-Inverse-Reinforcement-Learning-via-Interactive-Questioning" class="headerlink" title="Eliciting Risk Aversion with Inverse Reinforcement Learning via Interactive Questioning"></a>Eliciting Risk Aversion with Inverse Reinforcement Learning via Interactive Questioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08427">http://arxiv.org/abs/2308.08427</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziteng Cheng, Anthony Coache, Sebastian Jaimungal</li>
<li>for: 本研究提出了一种新的框架，用于通过交互问答来识别代理人的风险偏好。我们的研究在两种场景下进行了测试：一个一期情况和一个无穷远景情况。在一期情况下，我们假设代理人的风险偏好是一个状态的成本函数和一种扭曲风险度量。在无穷远景情况下，我们模型了代理人的风险偏好，添加了一个优先级因子。假设我们有访问一组候选人，其中包含代理人的真实风险偏好，我们示出了问候者可以通过在不同环境中展示其最佳策略来识别代理人的风险偏好。我们证明，问候者可以通过问答来识别代理人的风险偏好，问题数量增加，问题随机设计。我们还开发了一个算法来设计优化的问题，并在 simulations 中提供了实证证明，我们的方法可以更快地识别代理人的风险偏好，比Randomly designed questions 更快。</li>
<li>methods: 我们的方法包括两个部分：一个是模型代理人的风险偏好，另一个是通过问答来识别代理人的风险偏好。在一期情况下，我们使用了一个成本函数和一种扭曲风险度量来模型代理人的风险偏好。在无穷远景情况下，我们添加了一个优先级因子来模型代理人的风险偏好。我们使用了一种随机设计的问题来识别代理人的风险偏好，并开发了一个算法来设计优化的问题。</li>
<li>results: 我们的实验结果表明，我们的方法可以快速地识别代理人的风险偏好。在 simulations 中，我们发现，我们的方法可以更快地识别代理人的风险偏好，比Randomly designed questions 更快。此外，我们还发现，我们的方法可以更好地适应不同的风险偏好。<details>
<summary>Abstract</summary>
This paper proposes a novel framework for identifying an agent's risk aversion using interactive questioning. Our study is conducted in two scenarios: a one-period case and an infinite horizon case. In the one-period case, we assume that the agent's risk aversion is characterized by a cost function of the state and a distortion risk measure. In the infinite horizon case, we model risk aversion with an additional component, a discount factor. Assuming the access to a finite set of candidates containing the agent's true risk aversion, we show that asking the agent to demonstrate her optimal policies in various environment, which may depend on their previous answers, is an effective means of identifying the agent's risk aversion. Specifically, we prove that the agent's risk aversion can be identified as the number of questions tends to infinity, and the questions are randomly designed. We also develop an algorithm for designing optimal questions and provide empirical evidence that our method learns risk aversion significantly faster than randomly designed questions in simulations. Our framework has important applications in robo-advising and provides a new approach for identifying an agent's risk preferences.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这篇论文提出了一种新的方法，用于透过交互问答来确定代理人的风险偏好。我们的研究分为两个场景：一个一期 случа和一个无限远景 случа。在一期 случа中，我们假设代理人的风险偏好是通过状态的成本函数和扭曲风险度量来描述的。在无限远景 случа中，我们模型风险偏好有一个附加组成部分：折扣因子。假设我们有对代理人真实风险偏好的访问权，我们显示出问答可以作为一种有效的风险偏好标识方法。特别是，我们证明代理人的风险偏好可以通过问答的数量增长而被确定，并且问答可以随机设计。我们还开发了一个算法来设计优化的问答，并在实验中证明我们的方法可以在 simulations 中学习风险偏好得到更好的效果。我们的框架在 robo-advising 中有重要应用，并提供了一种新的风险偏好标识方法。
</details></li>
</ul>
<hr>
<h2 id="Digital-twinning-of-cardiac-electrophysiology-models-from-the-surface-ECG-a-geodesic-backpropagation-approach"><a href="#Digital-twinning-of-cardiac-electrophysiology-models-from-the-surface-ECG-a-geodesic-backpropagation-approach" class="headerlink" title="Digital twinning of cardiac electrophysiology models from the surface ECG: a geodesic backpropagation approach"></a>Digital twinning of cardiac electrophysiology models from the surface ECG: a geodesic backpropagation approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08410">http://arxiv.org/abs/2308.08410</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas Grandits, Jan Verhülsdonk, Gundolf Haase, Alexander Effland, Simone Pezzuto</li>
<li>For: The paper is written for researchers and clinicians interested in cardiac electrophysiology and the development of personalized models of cardiac activation.* Methods: The paper introduces a novel method called Geodesic-BP, which uses the eikonal equation to solve the inverse problem of cardiac electrophysiology and reconstruct a given electrocardiogram (ECG). The method is well-suited for GPU-accelerated machine learning frameworks and can be used to optimize the parameters of the eikonal equation to reproduce a given ECG.* Results: The paper shows that Geodesic-BP can reconstruct a simulated cardiac activation with high accuracy in a synthetic test case, even in the presence of modeling inaccuracies. The method is also applied to a publicly available dataset of a rabbit model, with very positive results.<details>
<summary>Abstract</summary>
The eikonal equation has become an indispensable tool for modeling cardiac electrical activation accurately and efficiently. In principle, by matching clinically recorded and eikonal-based electrocardiograms (ECGs), it is possible to build patient-specific models of cardiac electrophysiology in a purely non-invasive manner. Nonetheless, the fitting procedure remains a challenging task. The present study introduces a novel method, Geodesic-BP, to solve the inverse eikonal problem. Geodesic-BP is well-suited for GPU-accelerated machine learning frameworks, allowing us to optimize the parameters of the eikonal equation to reproduce a given ECG. We show that Geodesic-BP can reconstruct a simulated cardiac activation with high accuracy in a synthetic test case, even in the presence of modeling inaccuracies. Furthermore, we apply our algorithm to a publicly available dataset of a rabbit model, with very positive results. Given the future shift towards personalized medicine, Geodesic-BP has the potential to help in future functionalizations of cardiac models meeting clinical time constraints while maintaining the physiological accuracy of state-of-the-art cardiac models.
</details>
<details>
<summary>摘要</summary>
《射线方程》已成为心脏电动力学模型精准计算的不可或缺工具。在原理上，通过对临床记录的电子干扰gram（ECG）和射线方程进行匹配，可以建立个性化的心脏电physiology模型，无需侵入性的干扰。然而，匹配过程仍然是一项具有挑战性的任务。本研究提出了一种新的方法，即Geodesic-BP，以解决反射射线问题。Geodesic-BP适用于加速机器学习框架的GPU，可以优化射线方程中的参数，以实现与给定ECG的匹配。我们在一个人工测试案例中展示了Geodesic-BP可以高精度地重建模拟的心脏活动，包括模型精度不足时的情况。此外，我们将我们的算法应用于一个公共可用的兔子模型数据集，得到了非常正面的结果。鉴于未来的个性化医疗的发展，Geodesic-BP有望帮助未来的心脏模型功能化，满足临床时间限制，同时维持现有的心脏模型的生物学精度。
</details></li>
</ul>
<hr>
<h2 id="Explainable-AI-for-clinical-risk-prediction-a-survey-of-concepts-methods-and-modalities"><a href="#Explainable-AI-for-clinical-risk-prediction-a-survey-of-concepts-methods-and-modalities" class="headerlink" title="Explainable AI for clinical risk prediction: a survey of concepts, methods, and modalities"></a>Explainable AI for clinical risk prediction: a survey of concepts, methods, and modalities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08407">http://arxiv.org/abs/2308.08407</a></li>
<li>repo_url: None</li>
<li>paper_authors: Munib Mesinovic, Peter Watkinson, Tingting Zhu</li>
<li>for: 这篇论文的目的是探讨 ai 应用于医疗领域中的解释性能力，以确保 ai 系统的可靠性和可信worthiness。</li>
<li>methods: 这篇论文使用了多种解释性模型，包括 LIME、 SHAP 和 TreeExplainer，以及其他一些新的解释性方法。</li>
<li>results: 这篇论文的结果表明，使用解释性模型可以提高 ai 系统的可靠性和可信worthiness，同时也可以增加模型的透明度和可读性。<details>
<summary>Abstract</summary>
Recent advancements in AI applications to healthcare have shown incredible promise in surpassing human performance in diagnosis and disease prognosis. With the increasing complexity of AI models, however, concerns regarding their opacity, potential biases, and the need for interpretability. To ensure trust and reliability in AI systems, especially in clinical risk prediction models, explainability becomes crucial. Explainability is usually referred to as an AI system's ability to provide a robust interpretation of its decision-making logic or the decisions themselves to human stakeholders. In clinical risk prediction, other aspects of explainability like fairness, bias, trust, and transparency also represent important concepts beyond just interpretability. In this review, we address the relationship between these concepts as they are often used together or interchangeably. This review also discusses recent progress in developing explainable models for clinical risk prediction, highlighting the importance of quantitative and clinical evaluation and validation across multiple common modalities in clinical practice. It emphasizes the need for external validation and the combination of diverse interpretability methods to enhance trust and fairness. Adopting rigorous testing, such as using synthetic datasets with known generative factors, can further improve the reliability of explainability methods. Open access and code-sharing resources are essential for transparency and reproducibility, enabling the growth and trustworthiness of explainable research. While challenges exist, an end-to-end approach to explainability in clinical risk prediction, incorporating stakeholders from clinicians to developers, is essential for success.
</details>
<details>
<summary>摘要</summary>
最近的人工智能应用于医疗领域的进步已经显示出了人性化的 диагности和疾病预测的能力。然而，随着人工智能模型的复杂度的增加，关于它们的不透明度、潜在偏见和解释性的问题也引起了关注。以确保人工智能系统的可信worth和可靠性，特别是在临床风险预测模型中，解释性变得非常重要。解释性通常指的是人工智能系统能够提供人类权益者可靠的决策逻辑或决策结果的解释。在临床风险预测中，其他方面的解释性如公平、偏见、信任和透明度也是重要的概念，这些概念frequently被用于一起或相互替换使用。本文评论了这些概念之间的关系，并讨论了最近在临床风险预测中发展的解释模型，强调了在多种常见模式下的量化和临床评估和验证的重要性。它也强调了外部验证和多种解释方法的结合，以提高可信worth和公平性。采用严格的测试，如使用已知生成因素的 sintetic数据集，可以进一步提高解释性方法的可靠性。开放访问和代码分享资源是必要的，以确保透明度和可重现性。而且，在实施解释性时，需要结合临床医生到开发者的多种潜在偏见，以确保成功。
</details></li>
</ul>
<hr>
<h2 id="Content-based-Recommendation-Engine-for-Video-Streaming-Platform"><a href="#Content-based-Recommendation-Engine-for-Video-Streaming-Platform" class="headerlink" title="Content-based Recommendation Engine for Video Streaming Platform"></a>Content-based Recommendation Engine for Video Streaming Platform</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08406">http://arxiv.org/abs/2308.08406</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Puskal Khadka, Prabhav Lamichhane</li>
<li>for: 这个论文是为了提供一种基于内容的推荐引擎，用于为用户提供视频建议，以满足他们的先前兴趣和选择。</li>
<li>methods: 该论文使用了TF-IDF文本分Vector化方法来确定文档中的相关性，然后计算每个内容之间的cosine相似性来确定它们之间的相似性。</li>
<li>results: 该论文得到了一个基于TF-IDF和cosine相似性的推荐引擎，可以帮助用户找到符合他们兴趣的视频内容。同时，论文还测试了该引擎的性能，并计算了精度、报告率和F1核心等指标，以评估其性能。<details>
<summary>Abstract</summary>
Recommendation engine suggest content, product or services to the user by using machine learning algorithm. This paper proposed a content-based recommendation engine for providing video suggestion to the user based on their previous interests and choices. We will use TF-IDF text vectorization method to determine the relevance of words in a document. Then we will find out the similarity between each content by calculating cosine similarity between them. Finally, engine will recommend videos to the users based on the obtained similarity score value. In addition, we will measure the engine's performance by computing precision, recall, and F1 core of the proposed system.
</details>
<details>
<summary>摘要</summary>
<<SYS>>机器学习算法可以为用户提供内容、产品或服务的建议。这篇论文提出了基于用户之前的兴趣和选择的内容基于推荐引擎，用于为用户提供视频建议。我们将使用TF-IDF文本矢量化方法来确定文档中的相关性。然后，我们将计算每个内容之间的相似性，并根据所得到的相似性分值来推荐视频给用户。此外，我们还将测量引擎的性能，计算推荐系统的准确率、恰当率和F1分值。Note: "TF-IDF" stands for "Term Frequency-Inverse Document Frequency", which is a text vectorization method used to determine the relevance of words in a document.
</details></li>
</ul>
<hr>
<h2 id="Fast-Uncertainty-Quantification-of-Spent-Nuclear-Fuel-with-Neural-Networks"><a href="#Fast-Uncertainty-Quantification-of-Spent-Nuclear-Fuel-with-Neural-Networks" class="headerlink" title="Fast Uncertainty Quantification of Spent Nuclear Fuel with Neural Networks"></a>Fast Uncertainty Quantification of Spent Nuclear Fuel with Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08391">http://arxiv.org/abs/2308.08391</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arnau Albà, Andreas Adelmann, Lucas Münster, Dimitri Rochman, Romana Boiger<br>for: 这个论文的目的是为了简化核电厂燃料的特性calculation和不确定性评估，以提高核能生产、废料处理和核安全预防的安全、效率和可持续性。methods: 本论文使用神经网络（NN）来建立一个快速的模拟模型，以便预测核电厂燃料的一些特性，例如衰变热和核lide的含量，并且可以对这些特性进行不确定性评估。results: 本论文的结果显示，使用NN模型可以实现这些特性的精确预测，并且可以对这些特性进行不确定性评估，并且可以大大降低physics-based模型的计算成本。<details>
<summary>Abstract</summary>
The accurate calculation and uncertainty quantification of the characteristics of spent nuclear fuel (SNF) play a crucial role in ensuring the safety, efficiency, and sustainability of nuclear energy production, waste management, and nuclear safeguards. State of the art physics-based models, while reliable, are computationally intensive and time-consuming. This paper presents a surrogate modeling approach using neural networks (NN) to predict a number of SNF characteristics with reduced computational costs compared to physics-based models. An NN is trained using data generated from CASMO5 lattice calculations. The trained NN accurately predicts decay heat and nuclide concentrations of SNF, as a function of key input parameters, such as enrichment, burnup, cooling time between cycles, mean boron concentration and fuel temperature. The model is validated against physics-based decay heat simulations and measurements of different uranium oxide fuel assemblies from two different pressurized water reactors. In addition, the NN is used to perform sensitivity analysis and uncertainty quantification. The results are in very good alignment to CASMO5, while the computational costs (taking into account the costs of generating training samples) are reduced by a factor of 10 or more. Our findings demonstrate the feasibility of using NNs as surrogate models for fast characterization of SNF, providing a promising avenue for improving computational efficiency in assessing nuclear fuel behavior and associated risks.
</details>
<details>
<summary>摘要</summary>
使用神经网络（NN）模型来快速计算核电燃料（SNF）的特性，可以提高核能生产、废料处理和核保障的安全、效率和可持续性。当前的物理基于模型，尽管可靠，但计算成本高。这篇文章介绍了使用NN模型来预测SNF特性，包括衰变热和核lide的分布，作为输入参数的核燃料燃烧度、燃烧时间、冷却时间、燃料浓度和燃料温度等。NN模型通过对CASMO5网络计算数据进行训练。模型可以准确地预测核燃料的衰变热和核lide的分布，并且可以对不同的氧化铀燃料聚合体进行预测。此外，NN模型还可以进行敏感分析和不确定性评估。结果与CASMO5模型相符，而计算成本（包括生成训练样本的成本）则被减少了一倍或更多。我们的发现表明使用NN模型可以快速地计算SNF特性，提供了一个有前途的方法来提高核燃料行为和相关风险的计算效率。
</details></li>
</ul>
<hr>
<h2 id="Continuous-Sweep-an-improved-binary-quantifier"><a href="#Continuous-Sweep-an-improved-binary-quantifier" class="headerlink" title="Continuous Sweep: an improved, binary quantifier"></a>Continuous Sweep: an improved, binary quantifier</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08387">http://arxiv.org/abs/2308.08387</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kevin Kloos, Julian D. Karch, Quinten A. Meertens, Mark de Rooij</li>
<li>for: 这篇论文主要关注的是量化学习任务中的binary quantifier，即估计资料集中类别的分布。</li>
<li>methods: 作者们提出了一个新的 parametric binary quantifier，叫做Continuous Sweep，这个方法受到Median Sweep的影响，但是有三个主要的改进：1) 使用Parametric class distributions instead of empirical distributions，2) 优化决策boundaries instead of 应用类别规则，3) 计算mean instead of median。</li>
<li>results: 作者们通过分析表现，证明了Continuous Sweep在广泛的情况下比Median Sweep表现更好，并且提供了一些 theoretically optimal decision boundaries。<details>
<summary>Abstract</summary>
Quantification is a supervised machine learning task, focused on estimating the class prevalence of a dataset rather than labeling its individual observations. We introduce Continuous Sweep, a new parametric binary quantifier inspired by the well-performing Median Sweep. Median Sweep is currently one of the best binary quantifiers, but we have changed this quantifier on three points, namely 1) using parametric class distributions instead of empirical distributions, 2) optimizing decision boundaries instead of applying discrete decision rules, and 3) calculating the mean instead of the median. We derive analytic expressions for the bias and variance of Continuous Sweep under general model assumptions. This is one of the first theoretical contributions in the field of quantification learning. Moreover, these derivations enable us to find the optimal decision boundaries. Finally, our simulation study shows that Continuous Sweep outperforms Median Sweep in a wide range of situations.
</details>
<details>
<summary>摘要</summary>
<<SYS>>quantification是一种指导学习任务，关注数据集中类别的出现频率而不是具体的观察值。我们介绍了连续探索，一种基于 median sweep 的新参数化二分量器。Median sweep 目前是二分量器中的一个非常好的选择，但我们在其上改变了三点：1）使用参数化类别分布而不是实际分布，2）优化决策界而不是应用简单的决策规则，3）计算平均值而不是中值。我们 derive 了一系列的analytic表达式，用于描述 Continuous Sweep 的偏差和方差。这是量化学习领域的一个非常rare的理论贡献。此外，这些 derivations 允许我们找到最佳决策界。最后，我们的 simulations 研究表明，Continuous Sweep 在各种情况下都能够超越 Median Sweep。Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Precision-and-Recall-Reject-Curves-for-Classification"><a href="#Precision-and-Recall-Reject-Curves-for-Classification" class="headerlink" title="Precision and Recall Reject Curves for Classification"></a>Precision and Recall Reject Curves for Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08381">http://arxiv.org/abs/2308.08381</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lydia Fischer, Patricia Wollstadt</li>
<li>for: 这个论文主要是为了提出一种新的评估分类器性能的方法，以及一种基于这种方法的分类器。</li>
<li>methods: 这个论文使用了一种基于学习 вектор量化的prototype-based分类器，并使用了一些不同的certainty measure来评估分类器的性能。</li>
<li>results: 论文通过对人工测试数据和实际医疗数据进行测试，发现在面临着类别不对称的情况下，使用precision和recall reject curve可以更好地评估分类器的性能，而不是使用准确率 reject curve。<details>
<summary>Abstract</summary>
For some classification scenarios, it is desirable to use only those classification instances that a trained model associates with a high certainty. To obtain such high-certainty instances, previous work has proposed accuracy-reject curves. Reject curves allow to evaluate and compare the performance of different certainty measures over a range of thresholds for accepting or rejecting classifications. However, the accuracy may not be the most suited evaluation metric for all applications, and instead precision or recall may be preferable. This is the case, for example, for data with imbalanced class distributions. We therefore propose reject curves that evaluate precision and recall, the recall-reject curve and the precision-reject curve. Using prototype-based classifiers from learning vector quantization, we first validate the proposed curves on artificial benchmark data against the accuracy reject curve as a baseline. We then show on imbalanced benchmarks and medical, real-world data that for these scenarios, the proposed precision- and recall-curves yield more accurate insights into classifier performance than accuracy reject curves.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将给定文本翻译成简化中文。</SYS>>对于一些分类场景，您可以使用已经训练的模型中的高确定度分类实例。以前的工作已经提出了准确度拒绝曲线，这些曲线可以评估和比较不同的确定度测试的性能。但是，准确度可能不是所有应用场景中最适合的评价指标，特别是数据具有不均衡的类别分布。我们因此提议使用 recall-reject 曲线和 precision-reject 曲线来评估分类器性能。使用学习 вектор量化的prototype-based分类器，我们首先在人工测试数据上验证我们提议的曲线，并作为基准点使用准确度拒绝曲线。然后，我们在具有不均衡的测试数据和医疗实际数据上展示了，在这些场景下，我们的precision-和 recall-曲线可以更加准确地评估分类器性能，比较准确度拒绝曲线。
</details></li>
</ul>
<hr>
<h2 id="A-distributed-neural-network-architecture-for-dynamic-sensor-selection-with-application-to-bandwidth-constrained-body-sensor-networks"><a href="#A-distributed-neural-network-architecture-for-dynamic-sensor-selection-with-application-to-bandwidth-constrained-body-sensor-networks" class="headerlink" title="A distributed neural network architecture for dynamic sensor selection with application to bandwidth-constrained body-sensor networks"></a>A distributed neural network architecture for dynamic sensor selection with application to bandwidth-constrained body-sensor networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08379">http://arxiv.org/abs/2308.08379</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas Strypsteen, Alexander Bertrand</li>
<li>for: 这个研究旨在提出一个动态侦测器选择方法，用于深度神经网络（DNNs），以便从每个具体的输入样本中选择最佳侦测器子集，而不是整个数据集中的固定选择。</li>
<li>methods: 这个动态选择是与任务模型一起学习的，使用Gumbel-Softmax的技巧，以让决策获得数据验证。</li>
<li>results: 我们显示了如何使用这个动态选择来增加无线传感网络（WSN）的寿命，并且透过对每个节点的传输次数实施限制。我们还提高了性能的方法，包括一个动态空间范本，让任务-DNN更加对多个可能的节点子集具有抗衰变的能力。最后，我们说明了如何选择最佳通道。我们验证了这个方法，使用实际的电普瑞度测量数据，模拟一个EEG感应网络。我们分析了对输送负载和任务精度的交换。<details>
<summary>Abstract</summary>
We propose a dynamic sensor selection approach for deep neural networks (DNNs), which is able to derive an optimal sensor subset selection for each specific input sample instead of a fixed selection for the entire dataset. This dynamic selection is jointly learned with the task model in an end-to-end way, using the Gumbel-Softmax trick to allow the discrete decisions to be learned through standard backpropagation. We then show how we can use this dynamic selection to increase the lifetime of a wireless sensor network (WSN) by imposing constraints on how often each node is allowed to transmit. We further improve performance by including a dynamic spatial filter that makes the task-DNN more robust against the fact that it now needs to be able to handle a multitude of possible node subsets. Finally, we explain how the selection of the optimal channels can be distributed across the different nodes in a WSN. We validate this method on a use case in the context of body-sensor networks, where we use real electroencephalography (EEG) sensor data to emulate an EEG sensor network. We analyze the resulting trade-offs between transmission load and task accuracy.
</details>
<details>
<summary>摘要</summary>
我们提出了一种动态感测器选择方法，用于深度神经网络（DNN），可以为每个特定输入样本选择最佳感测器子集而不是整个数据集的固定选择。这种动态选择与任务模型一起学习，使用Gumbel-Softmax技巧，以便通过标准反馈来学习不同的决策。我们然后解释了如何使用这种动态选择来增加无线传感器网络（WSN）的寿命，并在不同的节点上强制执行特定的传输限制。此外，我们还提高了任务-DNN的可靠性，使其能够处理多个可能的节点子集。最后，我们解释了如何在WSN中选择优化的通道。我们验证了这种方法在身体感测网络上的使用情况，使用实际的电encephalography（EEG）感测器数据来模拟EEG感测器网络。我们分析了因 переда信荷和任务准确率之间的负面效应。
</details></li>
</ul>
<hr>
<h2 id="PDPK-A-Framework-to-Synthesise-Process-Data-and-Corresponding-Procedural-Knowledge-for-Manufacturing"><a href="#PDPK-A-Framework-to-Synthesise-Process-Data-and-Corresponding-Procedural-Knowledge-for-Manufacturing" class="headerlink" title="PDPK: A Framework to Synthesise Process Data and Corresponding Procedural Knowledge for Manufacturing"></a>PDPK: A Framework to Synthesise Process Data and Corresponding Procedural Knowledge for Manufacturing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08371">http://arxiv.org/abs/2308.08371</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/0x14d/embedding-operator-knowledge">https://github.com/0x14d/embedding-operator-knowledge</a></li>
<li>paper_authors: Richard Nordsieck, André Schweizer, Michael Heider, Jörg Hähner</li>
<li>For: The paper aims to provide a framework for generating synthetic datasets that can be used to represent procedural knowledge in different domains.* Methods: The framework uses a combination of knowledge graphs and parametric processes to simulate real-world data and generate synthetic datasets. The authors evaluate the effectiveness of several existing embedding methods on the synthetic datasets.* Results: The authors compare the results of the embedding methods on the synthetic datasets with those achievable on a real-world dataset, and find that the synthetic datasets can accurately represent the procedural knowledge in the real-world data. They also provide a baseline for future work by demonstrating the potential of the synthetic datasets to represent procedural knowledge.Here’s the information in Simplified Chinese text:* For: 本文提供了一个框架，用于生成不同领域的 sintetic 数据集，以表现程序知识。* Methods: 框架使用知识 graphs 和参数化过程来实现实际数据的生成，并评估了多个现有的嵌入方法在 sintetic 数据集上的效果。* Results: 作者比较了嵌入方法在 sintetic 数据集和实际数据集上的结果，发现 sintetic 数据集能够准确表现实际数据中的程序知识。他们还提供了未来工作的基eline，显示 sintetic 数据集的可能性。<details>
<summary>Abstract</summary>
Procedural knowledge describes how to accomplish tasks and mitigate problems. Such knowledge is commonly held by domain experts, e.g. operators in manufacturing who adjust parameters to achieve quality targets. To the best of our knowledge, no real-world datasets containing process data and corresponding procedural knowledge are publicly available, possibly due to corporate apprehensions regarding the loss of knowledge advances. Therefore, we provide a framework to generate synthetic datasets that can be adapted to different domains. The design choices are inspired by two real-world datasets of procedural knowledge we have access to. Apart from containing representations of procedural knowledge in Resource Description Framework (RDF)-compliant knowledge graphs, the framework simulates parametrisation processes and provides consistent process data. We compare established embedding methods on the resulting knowledge graphs, detailing which out-of-the-box methods have the potential to represent procedural knowledge. This provides a baseline which can be used to increase the comparability of future work. Furthermore, we validate the overall characteristics of a synthesised dataset by comparing the results to those achievable on a real-world dataset. The framework and evaluation code, as well as the dataset used in the evaluation, are available open source.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>专业知识描述如何完成任务和解决问题。这种知识通常由领域专家所拥有，例如制造业中的操作员，他们根据参数进行调整以达到质量目标。根据我们所知，现实世界中没有公开可用的实际数据和相应的专业知识集。因此，我们提供了一个框架，可以生成可靠的合成数据集，可以适应不同领域。该框架的设计启发自我们有access的两个实际数据集，它们包含了专业知识的RDF相容知识图表示，同时还模拟了参数化过程，提供了一致的过程数据。我们使用现有的嵌入方法对于生成的知识图进行评估，详细介绍了这些方法在表示专业知识方面的潜力。此外，我们还验证了合成数据集的总特征，并与真实世界数据集进行比较，以验证合成数据集的可靠性。框架和评估代码以及使用于评估的数据集都是开源的。
</details></li>
</ul>
<hr>
<h2 id="Dual-Branch-Temperature-Scaling-Calibration-for-Long-Tailed-Recognition"><a href="#Dual-Branch-Temperature-Scaling-Calibration-for-Long-Tailed-Recognition" class="headerlink" title="Dual-Branch Temperature Scaling Calibration for Long-Tailed Recognition"></a>Dual-Branch Temperature Scaling Calibration for Long-Tailed Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08366">http://arxiv.org/abs/2308.08366</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jialin Guo, Zhenyu Wu, Zhiqiang Zhan, Yang Ji</li>
<li>for: 本研究旨在解决深度神经网络中的误差补做问题，尤其是在长板分布的数据下存在较大的误差问题。</li>
<li>methods: 本研究使用了温度扩展（TS）方法，并设计了多支分支温度扩展模型（Dual-TS），以考虑不同类别的温度参数的多样性和罕见样本的非一致性。</li>
<li>results: 经过实验，本研究表明，我们的模型在传统ECE和Esbin-ECE评价指标下均达到了顶尖性能。<details>
<summary>Abstract</summary>
The calibration for deep neural networks is currently receiving widespread attention and research. Miscalibration usually leads to overconfidence of the model. While, under the condition of long-tailed distribution of data, the problem of miscalibration is more prominent due to the different confidence levels of samples in minority and majority categories, and it will result in more serious overconfidence. To address this problem, some current research have designed diverse temperature coefficients for different categories based on temperature scaling (TS) method. However, in the case of rare samples in minority classes, the temperature coefficient is not generalizable, and there is a large difference between the temperature coefficients of the training set and the validation set. To solve this challenge, this paper proposes a dual-branch temperature scaling calibration model (Dual-TS), which considers the diversities in temperature parameters of different categories and the non-generalizability of temperature parameters for rare samples in minority classes simultaneously. Moreover, we noticed that the traditional calibration evaluation metric, Excepted Calibration Error (ECE), gives a higher weight to low-confidence samples in the minority classes, which leads to inaccurate evaluation of model calibration. Therefore, we also propose Equal Sample Bin Excepted Calibration Error (Esbin-ECE) as a new calibration evaluation metric. Through experiments, we demonstrate that our model yields state-of-the-art in both traditional ECE and Esbin-ECE metrics.
</details>
<details>
<summary>摘要</summary>
Currently, the calibration of deep neural networks is receiving extensive attention and research. If the model is miscalibrated, it can lead to overconfidence. In particular, when dealing with long-tailed distribution of data, the problem of miscalibration is more pronounced due to the differences in confidence levels of samples in minority and majority categories, which can result in more serious overconfidence. To address this issue, some current research has proposed using diverse temperature coefficients for different categories based on the temperature scaling (TS) method. However, for rare samples in minority classes, the temperature coefficient is not generalizable, and there is a large difference between the temperature coefficients of the training set and the validation set.To solve this challenge, this paper proposes a dual-branch temperature scaling calibration model (Dual-TS), which takes into account the diversity of temperature parameters for different categories and the non-generalizability of temperature parameters for rare samples in minority classes simultaneously. Furthermore, we noticed that the traditional calibration evaluation metric, Expected Calibration Error (ECE), gives a higher weight to low-confidence samples in minority classes, which leads to inaccurate evaluation of model calibration. Therefore, we also propose Equal Sample Bin Expected Calibration Error (Esbin-ECE) as a new calibration evaluation metric. Through experiments, we demonstrate that our model achieves state-of-the-art performance in both traditional ECE and Esbin-ECE metrics.
</details></li>
</ul>
<hr>
<h2 id="KernelWarehouse-Towards-Parameter-Efficient-Dynamic-Convolution"><a href="#KernelWarehouse-Towards-Parameter-Efficient-Dynamic-Convolution" class="headerlink" title="KernelWarehouse: Towards Parameter-Efficient Dynamic Convolution"></a>KernelWarehouse: Towards Parameter-Efficient Dynamic Convolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08361">http://arxiv.org/abs/2308.08361</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/osvai/kernelwarehouse">https://github.com/osvai/kernelwarehouse</a></li>
<li>paper_authors: Chao Li, Anbang Yao</li>
<li>for: 这 paper 是为了提出一种更高效的动态核函数，以提高图像识别模型的表现。</li>
<li>methods: 这 paper 使用了一种新的核函数设计方法，即 KernelWarehouse，它可以减少核函数的维度和增加核函数的数量，从而提高图像识别模型的表现。</li>
<li>results: 这 paper 的实验结果表明，使用 KernelWarehouse 可以达到图像识别领域的州际最佳性，并且可以降低模型的参数数量，从而提高模型的可扩展性和灵活性。<details>
<summary>Abstract</summary>
Dynamic convolution learns a linear mixture of $n$ static kernels weighted with their sample-dependent attentions, demonstrating superior performance compared to normal convolution. However, existing designs are parameter-inefficient: they increase the number of convolutional parameters by $n$ times. This and the optimization difficulty lead to no research progress in dynamic convolution that can allow us to use a significant large value of $n$ (e.g., $n>100$ instead of typical setting $n<10$) to push forward the performance boundary. In this paper, we propose $KernelWarehouse$, a more general form of dynamic convolution, which can strike a favorable trade-off between parameter efficiency and representation power. Its key idea is to redefine the basic concepts of "$kernels$" and "$assembling$ $kernels$" in dynamic convolution from the perspective of reducing kernel dimension and increasing kernel number significantly. In principle, KernelWarehouse enhances convolutional parameter dependencies within the same layer and across successive layers via tactful kernel partition and warehouse sharing, yielding a high degree of freedom to fit a desired parameter budget. We validate our method on ImageNet and MS-COCO datasets with different ConvNet architectures, and show that it attains state-of-the-art results. For instance, the ResNet18|ResNet50|MobileNetV2|ConvNeXt-Tiny model trained with KernelWarehouse on ImageNet reaches 76.05%|81.05%|75.52%|82.51% top-1 accuracy. Thanks to its flexible design, KernelWarehouse can even reduce the model size of a ConvNet while improving the accuracy, e.g., our ResNet18 model with 36.45%|65.10% parameter reduction to the baseline shows 2.89%|2.29% absolute improvement to top-1 accuracy.
</details>
<details>
<summary>摘要</summary>
“动态核函数学习一种线性权重混合的$n$ static核函数，达到比正常核函数更高的性能，但现有设计存在参数不效率问题：它将参数数量增加$n$倍。这导致了对动态核函数的研究停滞不前进，无法使用较大的$n$值（例如$n>100$）来推动性能边界。本文提出了«KernelWarehouse”，一种更通用的动态核函数设计，可以实现参数效率和表示能力之间的平衡。它的关键思想是在动态核函数中重新定义«核函数”和«核函数组合»的概念，从减少核函数维度和增加核函数数量的角度来看。在实践中，KernelWarehouse通过精巧的核函数分割和库共享，提高了层内参数之间的依赖关系和层次关系，从而实现了高度的自由度来适应感兴趣的参数预算。我们在ImageNet和MS-COCO datasets上验证了KernelWarehouse，并证明其可以达到状态略的最佳结果。例如，在ImageNet上，使用KernelWarehouse训练ResNet18|ResNet50|MobileNetV2|ConvNeXt-Tiny模型，可以达到76.05%|81.05%|75.52%|82.51%的顶部一个精度。此外，由于KernelWarehouse的灵活设计，可以在ConvNet模型中减少参数大小，同时提高准确率，例如我们的ResNet18模型在参数减少36.45%|65.10%后，可以提高2.89%|2.29%的精度。”
</details></li>
</ul>
<hr>
<h2 id="Independent-Distribution-Regularization-for-Private-Graph-Embedding"><a href="#Independent-Distribution-Regularization-for-Private-Graph-Embedding" class="headerlink" title="Independent Distribution Regularization for Private Graph Embedding"></a>Independent Distribution Regularization for Private Graph Embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08360">http://arxiv.org/abs/2308.08360</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hkust-knowcomp/privategraphencoder">https://github.com/hkust-knowcomp/privategraphencoder</a></li>
<li>paper_authors: Qi Hu, Yangqiu Song</li>
<li>for: 本研究旨在提出一种名为Private Variational Graph AutoEncoders（PVGAE）的新方法，以保护个人隐私信息的同时实现图像学任务的优秀表现。</li>
<li>methods: PVGAE使用了独立分布罚项作为正则项，并将原始的变量图自动编码器（VGAE）分解成学习敏感和非敏感特征的两个集。</li>
<li>results: 对三个实际数据集进行实验，PVGAE在保护个人隐私信息的同时实现了优秀的表现，舒服于其他基eline。<details>
<summary>Abstract</summary>
Learning graph embeddings is a crucial task in graph mining tasks. An effective graph embedding model can learn low-dimensional representations from graph-structured data for data publishing benefiting various downstream applications such as node classification, link prediction, etc. However, recent studies have revealed that graph embeddings are susceptible to attribute inference attacks, which allow attackers to infer private node attributes from the learned graph embeddings. To address these concerns, privacy-preserving graph embedding methods have emerged, aiming to simultaneously consider primary learning and privacy protection through adversarial learning. However, most existing methods assume that representation models have access to all sensitive attributes in advance during the training stage, which is not always the case due to diverse privacy preferences. Furthermore, the commonly used adversarial learning technique in privacy-preserving representation learning suffers from unstable training issues. In this paper, we propose a novel approach called Private Variational Graph AutoEncoders (PVGAE) with the aid of independent distribution penalty as a regularization term. Specifically, we split the original variational graph autoencoder (VGAE) to learn sensitive and non-sensitive latent representations using two sets of encoders. Additionally, we introduce a novel regularization to enforce the independence of the encoders. We prove the theoretical effectiveness of regularization from the perspective of mutual information. Experimental results on three real-world datasets demonstrate that PVGAE outperforms other baselines in private embedding learning regarding utility performance and privacy protection.
</details>
<details>
<summary>摘要</summary>
学习图embedding是 graf矿 tasks 中的一项重要任务。一个有效的图embedding模型可以从图结构数据中学习低维度表示，为数据发布带来多种下游应用程序的利益，如节点分类、链接预测等。然而， latest studies have shown that graph embeddings are vulnerable to attribute inference attacks, which allow attackers to infer private node attributes from the learned graph embeddings. To address these concerns, privacy-preserving graph embedding methods have emerged, aiming to simultaneously consider primary learning and privacy protection through adversarial learning. However, most existing methods assume that representation models have access to all sensitive attributes in advance during the training stage, which is not always the case due to diverse privacy preferences. Furthermore, the commonly used adversarial learning technique in privacy-preserving representation learning suffers from unstable training issues.In this paper, we propose a novel approach called Private Variational Graph Autoencoders (PVGAE) with the aid of independent distribution penalty as a regularization term. Specifically, we split the original variational graph autoencoder (VGAE) to learn sensitive and non-sensitive latent representations using two sets of encoders. Additionally, we introduce a novel regularization to enforce the independence of the encoders. We prove the theoretical effectiveness of regularization from the perspective of mutual information. Experimental results on three real-world datasets demonstrate that PVGAE outperforms other baselines in private embedding learning regarding utility performance and privacy protection.
</details></li>
</ul>
<hr>
<h2 id="Convergence-of-Two-Layer-Regression-with-Nonlinear-Units"><a href="#Convergence-of-Two-Layer-Regression-with-Nonlinear-Units" class="headerlink" title="Convergence of Two-Layer Regression with Nonlinear Units"></a>Convergence of Two-Layer Regression with Nonlinear Units</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08358">http://arxiv.org/abs/2308.08358</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yichuan Deng, Zhao Song, Shenghao Xie</li>
<li>for: 本研究旨在提出一种快速收敛的精度恰当的软MAX-ReLU回归问题解决方案，以便更好地训练大型自然语言模型（LLMs）。</li>
<li>methods: 本文提出了一种基于approx Newton方法的软MAX-ReLU回归算法，该算法在certain assumptions下可以 guarantees the convergence of the solution in the sense of the distance to the optimal solution。</li>
<li>results: 本研究通过计算closed form表示形式的梯度Matrix，并在certain assumptions下证明梯度的 lipschitz continuity和半正定性，从而实现了软MAX-ReLU回归问题的解决。<details>
<summary>Abstract</summary>
Large language models (LLMs), such as ChatGPT and GPT4, have shown outstanding performance in many human life task. Attention computation plays an important role in training LLMs. Softmax unit and ReLU unit are the key structure in attention computation. Inspired by them, we put forward a softmax ReLU regression problem. Generally speaking, our goal is to find an optimal solution to the regression problem involving the ReLU unit. In this work, we calculate a close form representation for the Hessian of the loss function. Under certain assumptions, we prove the Lipschitz continuous and the PSDness of the Hessian. Then, we introduce an greedy algorithm based on approximate Newton method, which converges in the sense of the distance to optimal solution. Last, We relax the Lipschitz condition and prove the convergence in the sense of loss value.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM），如ChatGPT和GPT4，在许多人类任务中表现出色。计算注意力对于训练LLM非常重要。软饱和ReLU单元是计算注意力的关键结构。受其启发，我们提出了软饱ReLU回归问题。总的来说，我们的目标是找到一个优化的回归解决方案，其中包括ReLU单元。在这种情况下，我们计算了一个快速的表示形式，用于捕捉损失函数的迷你。在某些假设下，我们证明了梯度的 lipschitz 连续和归一化性。然后，我们介绍了一种基于approximate Newton方法的满足搜索算法，该算法在某种意义上 converge。最后，我们松解了 lipschitz 条件，并证明了在损失值上的convergence。
</details></li>
</ul>
<hr>
<h2 id="Is-Meta-Learning-the-Right-Approach-for-the-Cold-Start-Problem-in-Recommender-Systems"><a href="#Is-Meta-Learning-the-Right-Approach-for-the-Cold-Start-Problem-in-Recommender-Systems" class="headerlink" title="Is Meta-Learning the Right Approach for the Cold-Start Problem in Recommender Systems?"></a>Is Meta-Learning the Right Approach for the Cold-Start Problem in Recommender Systems?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08354">http://arxiv.org/abs/2308.08354</a></li>
<li>repo_url: None</li>
<li>paper_authors: Davide Buffelli, Ashish Gupta, Agnieszka Strzalka, Vassilis Plachouras</li>
<li>for:  solve the cold-start problem in deep learning models for recommender systems</li>
<li>methods:  standard and widely adopted deep learning models, common representation learning techniques</li>
<li>results:  comparable performance to meta-learning techniques specifically designed for the cold-start setting, with much easier deployment in real-world applications<details>
<summary>Abstract</summary>
Recommender systems have become fundamental building blocks of modern online products and services, and have a substantial impact on user experience. In the past few years, deep learning methods have attracted a lot of research, and are now heavily used in modern real-world recommender systems. Nevertheless, dealing with recommendations in the cold-start setting, e.g., when a user has done limited interactions in the system, is a problem that remains far from solved. Meta-learning techniques, and in particular optimization-based meta-learning, have recently become the most popular approaches in the academic research literature for tackling the cold-start problem in deep learning models for recommender systems. However, current meta-learning approaches are not practical for real-world recommender systems, which have billions of users and items, and strict latency requirements. In this paper we show that it is possible to obtaining similar, or higher, performance on commonly used benchmarks for the cold-start problem without using meta-learning techniques. In more detail, we show that, when tuned correctly, standard and widely adopted deep learning models perform just as well as newer meta-learning models. We further show that an extremely simple modular approach using common representation learning techniques, can perform comparably to meta-learning techniques specifically designed for the cold-start setting while being much more easily deployable in real-world applications.
</details>
<details>
<summary>摘要</summary>
现代在线产品和服务中，推荐系统已成为基本结构的重要组件，对用户体验产生了深远的影响。过去几年，深度学习方法在研究中吸引了很多注意力，现在在现实世界中广泛应用于现代推荐系统中。然而，在冷开始设定下（例如，用户在系统中有限的交互），仍然是一个未解决的问题。学术研究文献中最受欢迎的方法是使用meta-学习技术来解决这个问题。然而，现有的meta-学习方法在实际应用中并不实用，因为它们需要训练大量数据和计算资源，并且具有严格的延迟要求。在这篇论文中，我们展示了可以在通用的深度学习模型中获得类似或更高的性能，而不需要使用meta-学习技术。具体来说，当正确地调整时，标准的深度学习模型可以与 newer meta-学习模型性能相似。此外，我们还展示了一种非常简单的模块化方法，使用常见的表示学习技术，可以在现实世界应用中与meta-学习技术特化于冷开始设定相比而表现类似，同时更易于部署。</sys>Here's the translation in Simplified Chinese:现代在线产品和服务中，推荐系统已成为基本结构的重要组件，对用户体验产生了深远的影响。过去几年，深度学习方法在研究中吸引了很多注意力，现在在现实世界中广泛应用于现代推荐系统中。然而，在冷开始设定下（例如，用户在系统中有限的交互），仍然是一个未解决的问题。学术研究文献中最受欢迎的方法是使用meta-学习技术来解决这个问题。然而，现有的meta-学习方法在实际应用中并不实用，因为它们需要训练大量数据和计算资源，并且具有严格的延迟要求。在这篇论文中，我们展示了可以在通用的深度学习模型中获得类似或更高的性能，而不需要使用meta-学习技术。具体来说，当正确地调整时，标准的深度学习模型可以与 newer meta-学习模型性能相似。此外，我们还展示了一种非常简单的模块化方法，使用常见的表示学习技术，可以在现实世界应用中与meta-学习技术特化于冷开始设定相比而表现类似，同时更易于部署。
</details></li>
</ul>
<hr>
<h2 id="Graph-Out-of-Distribution-Generalization-with-Controllable-Data-Augmentation"><a href="#Graph-Out-of-Distribution-Generalization-with-Controllable-Data-Augmentation" class="headerlink" title="Graph Out-of-Distribution Generalization with Controllable Data Augmentation"></a>Graph Out-of-Distribution Generalization with Controllable Data Augmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08344">http://arxiv.org/abs/2308.08344</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bin Lu, Xiaoying Gan, Ze Zhao, Shiyu Liang, Luoyi Fu, Xinbing Wang, Chenghu Zhou</li>
<li>for: 这篇论文旨在解决图像分类中的选择偏见问题，尤其是在训练和测试数据之间的分布偏移。</li>
<li>methods: 本文提出了一个名为\texttt{OOD-GMixup}的方法，它通过控制训练分布，以解决图像分类中的分布偏移问题。</li>
<li>results: extensive studies on several real-world datasets demonstrate the superiority of 本文提出的方法，比过现有的基eline方法更好。<details>
<summary>Abstract</summary>
Graph Neural Network (GNN) has demonstrated extraordinary performance in classifying graph properties. However, due to the selection bias of training and testing data (e.g., training on small graphs and testing on large graphs, or training on dense graphs and testing on sparse graphs), distribution deviation is widespread. More importantly, we often observe \emph{hybrid structure distribution shift} of both scale and density, despite of one-sided biased data partition. The spurious correlations over hybrid distribution deviation degrade the performance of previous GNN methods and show large instability among different datasets. To alleviate this problem, we propose \texttt{OOD-GMixup} to jointly manipulate the training distribution with \emph{controllable data augmentation} in metric space. Specifically, we first extract the graph rationales to eliminate the spurious correlations due to irrelevant information. Secondly, we generate virtual samples with perturbation on graph rationale representation domain to obtain potential OOD training samples. Finally, we propose OOD calibration to measure the distribution deviation of virtual samples by leveraging Extreme Value Theory, and further actively control the training distribution by emphasizing the impact of virtual OOD samples. Extensive studies on several real-world datasets on graph classification demonstrate the superiority of our proposed method over state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
图 neural network (GNN) 在分类图属性方面表现出了惊人的表现。然而，由于训练和测试数据的选择偏袋（例如，训练小图并测试大图，或训练紧凑图并测试稀疏图），分布偏移广泛存在。更重要的是，我们经常观察到图结构分布偏移的“半混合结构”，即同时存在一个稀疏图和一个紧凑图的偏移。这些偏移会导致前一代GNN方法的性能下降，并且在不同的数据集上显示出大的不稳定性。为了解决这个问题，我们提出了\texttt{OOD-GMixup}，一种通过控制数据增强的方法，用于同时控制训练分布和数据 augmentation。具体来说，我们首先提取图理据，以消除由不相关信息引起的假正相关性。然后，我们使用图理据表示域中的扰动生成虚拟样本。最后，我们提出了OOD核验，通过极值理论来测量虚拟样本的分布偏移，并且通过控制训练分布来活动地控制训练过程。我们在一些真实世界的图分类任务上进行了广泛的实验，并证明了我们提出的方法的优越性。
</details></li>
</ul>
<hr>
<h2 id="Learning-Logic-Programs-by-Discovering-Higher-Order-Abstractions"><a href="#Learning-Logic-Programs-by-Discovering-Higher-Order-Abstractions" class="headerlink" title="Learning Logic Programs by Discovering Higher-Order Abstractions"></a>Learning Logic Programs by Discovering Higher-Order Abstractions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08334">http://arxiv.org/abs/2308.08334</a></li>
<li>repo_url: None</li>
<li>paper_authors: Céline Hocquette, Sebastijan Dumančić, Andrew Cropper</li>
<li>for: 本研究旨在找到人类水平的AI需要的新抽象，即高阶抽象。</li>
<li>methods: 本研究使用了逻辑编程，从示例和背景知识中生成逻辑程序。</li>
<li>results: 我们的实验结果表明，与不 refactoring 相比，STEVIE 可以提高预测精度27%，降低学习时间47%。此外，STEVIE 还可以找到可以跨领域传递的抽象。<details>
<summary>Abstract</summary>
Discovering novel abstractions is important for human-level AI. We introduce an approach to discover higher-order abstractions, such as map, filter, and fold. We focus on inductive logic programming, which induces logic programs from examples and background knowledge. We introduce the higher-order refactoring problem, where the goal is to compress a logic program by introducing higher-order abstractions. We implement our approach in STEVIE, which formulates the higher-order refactoring problem as a constraint optimisation problem. Our experimental results on multiple domains, including program synthesis and visual reasoning, show that, compared to no refactoring, STEVIE can improve predictive accuracies by 27% and reduce learning times by 47%. We also show that STEVIE can discover abstractions that transfer to different domains
</details>
<details>
<summary>摘要</summary>
人类水平AI的发现新抽象是重要的。我们提出了一种方法，用于发现更高级别的抽象，如地图、筛选和折叠。我们将焦点放在逻辑编程中，它从示例和背景知识中逻辑程序的induction。我们介绍了更高级别的重构问题，目标是通过引入更高级别的抽象来压缩逻辑程序。我们在STEVIE中实现了我们的方法，它将更高级别的重构问题形式化为约束优化问题。我们的实验结果在多个领域，包括程序生成和视觉理解，表明，相比没有重构，STEVIE可以提高预测精度27%，降低学习时间47%。我们还表明STEVIE可以找到可以在不同领域传递的抽象。
</details></li>
</ul>
<hr>
<h2 id="Warped-geometric-information-on-the-optimisation-of-Euclidean-functions"><a href="#Warped-geometric-information-on-the-optimisation-of-Euclidean-functions" class="headerlink" title="Warped geometric information on the optimisation of Euclidean functions"></a>Warped geometric information on the optimisation of Euclidean functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08305">http://arxiv.org/abs/2308.08305</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcelo Hartmann, Bernardo Williams, Hanlin Yu, Mark Girolami, Alessandro Barp, Arto Klami</li>
<li>for: 优化一个具有高维 Euclidian 空间中的实数函数，如机器学习任务中的损失函数或统计推断中的 probaility 分布 logarithm。</li>
<li>methods: 使用折叠 Riemean 几何 notion 重新定义了函数在 Euclidean 空间上的优化问题为一个 Riemean 拓扑上的函数，然后在这个拓扑上寻找函数的最优点。选择的折叠 metric 使得优化问题变成了一个计算友好的 metric-tensor，可以轻松地计算优化方向。</li>
<li>results: 使用 third-order  Taylor 约化来 aproximate geodesic curve，并使用 retraction map 将其拔回到拓扑上。这种方法可以有效地优化 geodesic curve。与标准 Euclidean gradient-based 对抗方法相比，提出的算法在迭代次数到达 converges 和 Hessian-based 优化 Routine 中表现更好。<details>
<summary>Abstract</summary>
We consider the fundamental task of optimizing a real-valued function defined in a potentially high-dimensional Euclidean space, such as the loss function in many machine-learning tasks or the logarithm of the probability distribution in statistical inference. We use the warped Riemannian geometry notions to redefine the optimisation problem of a function on Euclidean space to a Riemannian manifold with a warped metric, and then find the function's optimum along this manifold. The warped metric chosen for the search domain induces a computational friendly metric-tensor for which optimal search directions associate with geodesic curves on the manifold becomes easier to compute. Performing optimization along geodesics is known to be generally infeasible, yet we show that in this specific manifold we can analytically derive Taylor approximations up to third-order. In general these approximations to the geodesic curve will not lie on the manifold, however we construct suitable retraction maps to pull them back onto the manifold. Therefore, we can efficiently optimize along the approximate geodesic curves. We cover the related theory, describe a practical optimization algorithm and empirically evaluate it on a collection of challenging optimisation benchmarks. Our proposed algorithm, using third-order approximation of geodesics, outperforms standard Euclidean gradient-based counterparts in term of number of iterations until convergence and an alternative method for Hessian-based optimisation routines.
</details>
<details>
<summary>摘要</summary>
我们考虑一个基本任务，即优化一个定义在可能高维欧几学空间中的实数函数，例如机器学习任务中的损函数或统计推断中的Logarithm的分布函数。我们使用扭曲的里曼纹理观念来重新定义欧几学空间上的函数优化问题为一个里曼拓扑上的函数优化问题，然后在这个拓扑上找到函数的最优点。选择的扭曲纹理在搜索空间上引入了一个计算友好的矩阵对应，其中优化搜索方向与拓扑上的地odesic曲线相关的计算变得更加容易。尽管在拓扑上的搜索通常是不可能的，但我们表明在这种特殊拓扑上，我们可以 analytically derivate Taylor approximations up to third-order。这些近似曲线不会在拓扑上，但我们可以构造适当的 retraction map 将其拟合回拓扑上。因此，我们可以高效地优化这些近似曲线。我们还详细介绍了相关理论、实践的优化算法以及对一系列困难优化问题的实验评估。我们的提议的算法，使用第三阶 Taylor 近似，在迭代次数 until convergence 和一种基于Hessian的优化方法之间占据了优势。
</details></li>
</ul>
<hr>
<h2 id="Robust-Bayesian-Satisficing"><a href="#Robust-Bayesian-Satisficing" class="headerlink" title="Robust Bayesian Satisficing"></a>Robust Bayesian Satisficing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08291">http://arxiv.org/abs/2308.08291</a></li>
<li>repo_url: None</li>
<li>paper_authors: Artun Saday, Yaşar Cahit Yıldırım, Cem Tekin</li>
<li>for: 本研究旨在掌握robust satisficing（RS）在contextual Bayesian optimization中的问题，以及在存在 Context 的假设分布与真实分布之间的差异。</li>
<li>methods: 我们提出了一种新的robust Bayesian satisficing算法（RoBOS），用于启发黑盒优化。我们的算法在某些假设下保证了下凹的 regret。此外，我们还定义了一种弱化的 regret 度量，称为 robust satisficing regret，我们的算法在这种情况下实现了下凹的上界独立于分布差异。</li>
<li>results: 我们在各种学习问题中应用了我们的方法，并与其他方法，如分布ally robust optimization，进行比较。我们的结果显示，RoBOS 能够在不同的学习问题中提供更好的性能，并且可以适应不同的分布差异。<details>
<summary>Abstract</summary>
Distributional shifts pose a significant challenge to achieving robustness in contemporary machine learning. To overcome this challenge, robust satisficing (RS) seeks a robust solution to an unspecified distributional shift while achieving a utility above a desired threshold. This paper focuses on the problem of RS in contextual Bayesian optimization when there is a discrepancy between the true and reference distributions of the context. We propose a novel robust Bayesian satisficing algorithm called RoBOS for noisy black-box optimization. Our algorithm guarantees sublinear lenient regret under certain assumptions on the amount of distribution shift. In addition, we define a weaker notion of regret called robust satisficing regret, in which our algorithm achieves a sublinear upper bound independent of the amount of distribution shift. To demonstrate the effectiveness of our method, we apply it to various learning problems and compare it to other approaches, such as distributionally robust optimization.
</details>
<details>
<summary>摘要</summary>
<<SYS>>TRANSLATE_TEXTDistributional shifts pose a significant challenge to achieving robustness in contemporary machine learning. To overcome this challenge, robust satisficing (RS) seeks a robust solution to an unspecified distributional shift while achieving a utility above a desired threshold. This paper focuses on the problem of RS in contextual Bayesian optimization when there is a discrepancy between the true and reference distributions of the context. We propose a novel robust Bayesian satisficing algorithm called RoBOS for noisy black-box optimization. Our algorithm guarantees sublinear lenient regret under certain assumptions on the amount of distribution shift. In addition, we define a weaker notion of regret called robust satisficing regret, in which our algorithm achieves a sublinear upper bound independent of the amount of distribution shift. To demonstrate the effectiveness of our method, we apply it to various learning problems and compare it to other approaches, such as distributionally robust optimization.TRANSLATE_TEXT以下是文章的中文翻译：Distributional shifts pose a significant challenge to achieving robustness in contemporary machine learning. To overcome this challenge, robust satisficing (RS) seeks a robust solution to an unspecified distributional shift while achieving a utility above a desired threshold. 本文关注在Contextual Bayesian optimization中的RS问题，当真实分布与参考分布之间存在差异时。我们提出了一种名为RoBOS的robust Bayesian satisficing算法，用于黑盒优化。我们的算法在certain assumptions中 guarantee sublinear lenient regret。此外，我们定义了一种弱的 regret called robust satisficing regret，在这种情况下，我们的算法 achieves sublinear upper bound，不受分布shift的影响。为证明我们的方法的效果，我们将其应用于various learning problems，并与其他方法进行比较，如distributionally robust optimization。
</details></li>
</ul>
<hr>
<h2 id="DFedADMM-Dual-Constraints-Controlled-Model-Inconsistency-for-Decentralized-Federated-Learning"><a href="#DFedADMM-Dual-Constraints-Controlled-Model-Inconsistency-for-Decentralized-Federated-Learning" class="headerlink" title="DFedADMM: Dual Constraints Controlled Model Inconsistency for Decentralized Federated Learning"></a>DFedADMM: Dual Constraints Controlled Model Inconsistency for Decentralized Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08290">http://arxiv.org/abs/2308.08290</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qinglun Li, Li Shen, Guanghao Li, Quanjun Yin, Dacheng Tao</li>
<li>for: 这个论文的目的是提出一种基于分布式学习的训练方法，以解决联合学习中的通信负担问题。</li>
<li>methods: 这个论文使用的方法是基于ADMM的分布式优化算法，并在这个基础上提出了两种改进版本：DFedADMM和DFedADMM-SAM。</li>
<li>results: 实验表明，这些算法在MNIST、CIFAR10和CIFAR100数据集上具有较高的泛化性和更快的收敛速度，比既有的状态态峰值优化器（SOTA）在分布式学习中表现更好。<details>
<summary>Abstract</summary>
To address the communication burden issues associated with federated learning (FL), decentralized federated learning (DFL) discards the central server and establishes a decentralized communication network, where each client communicates only with neighboring clients. However, existing DFL methods still suffer from two major challenges: local inconsistency and local heterogeneous overfitting, which have not been fundamentally addressed by existing DFL methods. To tackle these issues, we propose novel DFL algorithms, DFedADMM and its enhanced version DFedADMM-SAM, to enhance the performance of DFL. The DFedADMM algorithm employs primal-dual optimization (ADMM) by utilizing dual variables to control the model inconsistency raised from the decentralized heterogeneous data distributions. The DFedADMM-SAM algorithm further improves on DFedADMM by employing a Sharpness-Aware Minimization (SAM) optimizer, which uses gradient perturbations to generate locally flat models and searches for models with uniformly low loss values to mitigate local heterogeneous overfitting. Theoretically, we derive convergence rates of $\small \mathcal{O}\Big(\frac{1}{\sqrt{KT}}+\frac{1}{KT(1-\psi)^2}\Big)$ and $\small \mathcal{O}\Big(\frac{1}{\sqrt{KT}}+\frac{1}{KT(1-\psi)^2}+ \frac{1}{T^{3/2}K^{1/2}}\Big)$ in the non-convex setting for DFedADMM and DFedADMM-SAM, respectively, where $1 - \psi$ represents the spectral gap of the gossip matrix. Empirically, extensive experiments on MNIST, CIFAR10 and CIFAR100 datesets demonstrate that our algorithms exhibit superior performance in terms of both generalization and convergence speed compared to existing state-of-the-art (SOTA) optimizers in DFL.
</details>
<details>
<summary>摘要</summary>
为了解决联合学习（FL）中的通信负担问题，分布式联合学习（DFL）抛弃中央服务器，建立了分布式通信网络，每个客户端只与周围的客户端进行通信。然而，现有的DFL方法仍面临两个主要挑战：本地不一致和本地特异适应，这些问题尚未由现有的DFL方法得到基本解决。为此，我们提出了新的DFL算法，即DFedADMM和其加强版DFedADMM-SAM，以提高DFL的性能。DFedADMM算法使用了 primal-dual 优化（ADMM），通过使用 dual 变量控制分布式不同数据分布引起的模型不一致。DFedADMM-SAM算法进一步改进了 DFedADMM，通过使用 Sharpness-Aware Minimization（SAM）优化器，通过斜坡值误差来生成本地平滑模型，并在搜索本地特异适应模型时使用 Gradient Perturbations。理论上，我们得出了 $\small \mathcal{O}\Big(\frac{1}{\sqrt{KT}}+\frac{1}{KT(1-\psi)^2}\Big)$ 和 $\small \mathcal{O}\Big(\frac{1}{\sqrt{KT}}+\frac{1}{KT(1-\psi)^2}+\frac{1}{T^{3/2}K^{1/2}}\Big)$ 的收敛速率在非对称设定下，其中 $1 - \psi$ 表示热度矩阵的spectral gap。实验结果表明，我们的算法在 MNIST、CIFAR10 和 CIFAR100 数据集上展现出了与现有最佳优化器相比的优秀表现，包括总体化和收敛速度。
</details></li>
</ul>
<hr>
<h2 id="CARE-A-Large-Scale-CT-Image-Dataset-and-Clinical-Applicable-Benchmark-Model-for-Rectal-Cancer-Segmentation"><a href="#CARE-A-Large-Scale-CT-Image-Dataset-and-Clinical-Applicable-Benchmark-Model-for-Rectal-Cancer-Segmentation" class="headerlink" title="CARE: A Large Scale CT Image Dataset and Clinical Applicable Benchmark Model for Rectal Cancer Segmentation"></a>CARE: A Large Scale CT Image Dataset and Clinical Applicable Benchmark Model for Rectal Cancer Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08283">http://arxiv.org/abs/2308.08283</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hantao Zhang, Weidong Guo, Chenyang Qiu, Shouhong Wan, Bingbing Zou, Wanqin Wang, Peiquan Jin</li>
<li>for: 这个论文的目的是提出一种新的大规模RECTAL CANCER CT图像数据集CARE，以及一种特有的医疗器官癌症分割模型U-SAM，以解决RECTAL CANCER CT图像分割precision的问题。</li>
<li>methods: 这个论文使用了一种新的大规模RECTAL CANCER CT图像数据集CARE，并提出了一种特有的医疗器官癌症分割模型U-SAM，该模型包括三个关键组件：提示信息（例如，点）， convolution模块，和 skip-connection，以解决肠道器官的复杂结构。</li>
<li>results: 该论文的实验结果表明，提出的U-SAM模型在CARE数据集和WORD数据集上都能够达到state-of-the-art的性能水平，并且可以serve as the baseline for future research和临床应用开发。<details>
<summary>Abstract</summary>
Rectal cancer segmentation of CT image plays a crucial role in timely clinical diagnosis, radiotherapy treatment, and follow-up. Although current segmentation methods have shown promise in delineating cancerous tissues, they still encounter challenges in achieving high segmentation precision. These obstacles arise from the intricate anatomical structures of the rectum and the difficulties in performing differential diagnosis of rectal cancer. Additionally, a major obstacle is the lack of a large-scale, finely annotated CT image dataset for rectal cancer segmentation. To address these issues, this work introduces a novel large scale rectal cancer CT image dataset CARE with pixel-level annotations for both normal and cancerous rectum, which serves as a valuable resource for algorithm research and clinical application development. Moreover, we propose a novel medical cancer lesion segmentation benchmark model named U-SAM. The model is specifically designed to tackle the challenges posed by the intricate anatomical structures of abdominal organs by incorporating prompt information. U-SAM contains three key components: promptable information (e.g., points) to aid in target area localization, a convolution module for capturing low-level lesion details, and skip-connections to preserve and recover spatial information during the encoding-decoding process. To evaluate the effectiveness of U-SAM, we systematically compare its performance with several popular segmentation methods on the CARE dataset. The generalization of the model is further verified on the WORD dataset. Extensive experiments demonstrate that the proposed U-SAM outperforms state-of-the-art methods on these two datasets. These experiments can serve as the baseline for future research and clinical application development.
</details>
<details>
<summary>摘要</summary>
癌症肛部分 segmentation CT 图像在至关重要的诊断、放疗治疗和跟踪中扮演着关键角色。尽管当前的分 segmentation 方法已经展示了潜在的精度，但它们仍然遇到了在准确地分 segmentation 癌症组织的挑战。这些障碍来自肛部的复杂 анатомиче 结构以及Difficulties in performing differential diagnosis of rectal cancer。此外，lack of a large-scale, finely annotated CT image dataset for rectal cancer segmentation。 To address these issues, this work introduces a novel large-scale rectal cancer CT image dataset CARE with pixel-level annotations for both normal and cancerous rectum, which serves as a valuable resource for algorithm research and clinical application development. Moreover, we propose a novel medical cancer lesion segmentation benchmark model named U-SAM. The model is specifically designed to tackle the challenges posed by the intricate anatomical structures of abdominal organs by incorporating prompt information. U-SAM contains three key components: promptable information (e.g., points) to aid in target area localization, a convolution module for capturing low-level lesion details, and skip-connections to preserve and recover spatial information during the encoding-decoding process. To evaluate the effectiveness of U-SAM, we systematically compare its performance with several popular segmentation methods on the CARE dataset. The generalization of the model is further verified on the WORD dataset. Extensive experiments demonstrate that the proposed U-SAM outperforms state-of-the-art methods on these two datasets. These experiments can serve as the baseline for future research and clinical application development.
</details></li>
</ul>
<hr>
<h2 id="It-Ain’t-That-Bad-Understanding-the-Mysterious-Performance-Drop-in-OOD-Generalization-for-Generative-Transformer-Models"><a href="#It-Ain’t-That-Bad-Understanding-the-Mysterious-Performance-Drop-in-OOD-Generalization-for-Generative-Transformer-Models" class="headerlink" title="It Ain’t That Bad: Understanding the Mysterious Performance Drop in OOD Generalization for Generative Transformer Models"></a>It Ain’t That Bad: Understanding the Mysterious Performance Drop in OOD Generalization for Generative Transformer Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08268">http://arxiv.org/abs/2308.08268</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingcheng Xu, Zihao Pan, Haipeng Zhang, Yanqing Yang</li>
<li>for:  investigate the generalization behaviors of Generative Transformer-based models</li>
<li>methods:  using n-digit addition and multiplication tasks to study the models’ generalization abilities</li>
<li>results:  discovered that the models have structured representations and learned algebraic structures, but struggle with out-of-distribution inputs<details>
<summary>Abstract</summary>
Generative Transformer-based models have achieved remarkable proficiency on solving diverse problems. However, their generalization ability is not fully understood and not always satisfying. Researchers take basic mathematical tasks like n-digit addition or multiplication as important perspectives for investigating their generalization behaviors. Curiously, it is observed that when training on n-digit operations (e.g., additions) in which both input operands are n-digit in length, models generalize successfully on unseen n-digit inputs (in-distribution (ID) generalization), but fail miserably and mysteriously on longer, unseen cases (out-of-distribution (OOD) generalization). Studies try to bridge this gap with workarounds such as modifying position embedding, fine-tuning, and priming with more extensive or instructive data. However, without addressing the essential mechanism, there is hardly any guarantee regarding the robustness of these solutions. We bring this unexplained performance drop into attention and ask whether it is purely from random errors. Here we turn to the mechanistic line of research which has notable successes in model interpretability. We discover that the strong ID generalization stems from structured representations, while behind the unsatisfying OOD performance, the models still exhibit clear learned algebraic structures. Specifically, these models map unseen OOD inputs to outputs with equivalence relations in the ID domain. These highlight the potential of the models to carry useful information for improved generalization.
</details>
<details>
<summary>摘要</summary>
We bring attention to this unexplained performance drop and question whether it is due to random errors. To address this, we turn to the mechanistic line of research, which has been successful in model interpretability. We find that the strong ID generalization is due to structured representations, while the unsatisfying OOD performance is caused by the models still exhibiting clear learned algebraic structures. Specifically, these models map unseen OOD inputs to outputs with equivalence relations in the ID domain, highlighting the potential for improved generalization.
</details></li>
</ul>
<hr>
<h2 id="Graph-Relation-Aware-Continual-Learning"><a href="#Graph-Relation-Aware-Continual-Learning" class="headerlink" title="Graph Relation Aware Continual Learning"></a>Graph Relation Aware Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08259">http://arxiv.org/abs/2308.08259</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qinghua Shen, Weijieying Ren, Wei Qin</li>
<li>for: 本研究探讨了从无穷数据流中学习图像，卷积历史知识，并将其推广到未来任务。在这个任务中，只有当前图像数据可用。</li>
<li>methods: 本研究使用了一种叫做 relation-aware adaptive model（RAM-CG），它包括一个用于发现 latent relations 的模块和一个用于考虑时间推移的掩模。</li>
<li>results: 实验结果显示，RAM-CG 相比于当前状态的最佳结果，在 CitationNet、OGBN-arxiv 和 TWITCH 数据集上提供了显著的 2.2%、6.9% 和 6.6% 的改进。<details>
<summary>Abstract</summary>
Continual graph learning (CGL) studies the problem of learning from an infinite stream of graph data, consolidating historical knowledge, and generalizing it to the future task. At once, only current graph data are available. Although some recent attempts have been made to handle this task, we still face two potential challenges: 1) most of existing works only manipulate on the intermediate graph embedding and ignore intrinsic properties of graphs. It is non-trivial to differentiate the transferred information across graphs. 2) recent attempts take a parameter-sharing policy to transfer knowledge across time steps or progressively expand new architecture given shifted graph distribution. Learning a single model could loss discriminative information for each graph task while the model expansion scheme suffers from high model complexity. In this paper, we point out that latent relations behind graph edges can be attributed as an invariant factor for the evolving graphs and the statistical information of latent relations evolves. Motivated by this, we design a relation-aware adaptive model, dubbed as RAM-CG, that consists of a relation-discovery modular to explore latent relations behind edges and a task-awareness masking classifier to accounts for the shifted. Extensive experiments show that RAM-CG provides significant 2.2%, 6.9% and 6.6% accuracy improvements over the state-of-the-art results on CitationNet, OGBN-arxiv and TWITCH dataset, respective.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Two-Phases-of-Scaling-Laws-for-Nearest-Neighbor-Classifiers"><a href="#Two-Phases-of-Scaling-Laws-for-Nearest-Neighbor-Classifiers" class="headerlink" title="Two Phases of Scaling Laws for Nearest Neighbor Classifiers"></a>Two Phases of Scaling Laws for Nearest Neighbor Classifiers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08247">http://arxiv.org/abs/2308.08247</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pengkun Yang, Jingzhao Zhang</li>
<li>for: 本文研究近邻类ifiers的扩展法律。</li>
<li>methods: 作者使用了数据分布的复杂性来解释模型的通用错误。</li>
<li>results: 作者发现了两个阶段的扩展法律：在第一阶段，通用错误与数据维度之间存在直接的 polynomial 关系，而在第二阶段，错误与数据维度之间存在 exponential 关系。这种分布复杂性对模型的通用性产生了重要的影响。当数据分布宽泛时，近邻类ifiers 可以实现 polynomial 类型的通用错误，而不是 exponential 类型。<details>
<summary>Abstract</summary>
A scaling law refers to the observation that the test performance of a model improves as the number of training data increases. A fast scaling law implies that one can solve machine learning problems by simply boosting the data and the model sizes. Yet, in many cases, the benefit of adding more data can be negligible. In this work, we study the rate of scaling laws of nearest neighbor classifiers. We show that a scaling law can have two phases: in the first phase, the generalization error depends polynomially on the data dimension and decreases fast; whereas in the second phase, the error depends exponentially on the data dimension and decreases slowly. Our analysis highlights the complexity of the data distribution in determining the generalization error. When the data distributes benignly, our result suggests that nearest neighbor classifier can achieve a generalization error that depends polynomially, instead of exponentially, on the data dimension.
</details>
<details>
<summary>摘要</summary>
（注意：以下是简化中文版本）一个减小法（scaling law）指的是模型在训练数据量增加后测试性能的改善。快速减小法则意味着只需增加数据和模型大小就可以解决机器学习问题。然而，在许多情况下，增加更多数据的效果可能是微乎其微。在这项工作中，我们研究近似 neighboor 类型的减小法。我们发现一个减小法可以分为两个阶段：在第一阶段，总体错误取决于数据维度的度量函数，随着数据维度增加而快速下降;而在第二阶段，错误取决于数据维度的指数函数，随着数据维度增加而慢速下降。我们的分析表明数据分布的复杂性对总体错误的确定产生了重要影响。当数据分布良好时，我们的结果表明，近似 neighboor 类型的模型可以实现数据维度取决于指数函数而不是指数函数的总体错误。
</details></li>
</ul>
<hr>
<h2 id="The-Expressive-Power-of-Graph-Neural-Networks-A-Survey"><a href="#The-Expressive-Power-of-Graph-Neural-Networks-A-Survey" class="headerlink" title="The Expressive Power of Graph Neural Networks: A Survey"></a>The Expressive Power of Graph Neural Networks: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08235">http://arxiv.org/abs/2308.08235</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bingxu Zhang, Changjun Fan, Shixuan Liu, Kuihua Huang, Xiang Zhao, Jincai Huang, Zhong Liu</li>
<li>for: 本研究旨在探讨图 neural network (GNN) 的表达能力问题，即 GNN 能够学习什么样的图结构和特征。</li>
<li>methods: 本研究使用了多种方法来探讨 GNN 的表达能力，包括图特征增强、图结构增强和 GNN 架构增强等方法。</li>
<li>results: 本研究结果显示，通过不同的定义和方法，GNN 可以具有不同的表达能力，并且可以用于解决多种图学和机器学习问题。<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) are effective machine learning models for many graph-related applications. Despite their empirical success, many research efforts focus on the theoretical limitations of GNNs, i.e., the GNNs expressive power. Early works in this domain mainly focus on studying the graph isomorphism recognition ability of GNNs, and recent works try to leverage the properties such as subgraph counting and connectivity learning to characterize the expressive power of GNNs, which are more practical and closer to real-world. However, no survey papers and open-source repositories comprehensively summarize and discuss models in this important direction. To fill the gap, we conduct a first survey for models for enhancing expressive power under different forms of definition. Concretely, the models are reviewed based on three categories, i.e., Graph feature enhancement, Graph topology enhancement, and GNNs architecture enhancement.
</details>
<details>
<summary>摘要</summary>
格Nodes neural networks (GNNs) 是多种图像应用中的有效机器学习模型。 Despite their empirical success, many research efforts focus on the theoretical limitations of GNNs, i.e., the GNNs expressive power. Early works in this domain mainly focus on studying the graph isomorphism recognition ability of GNNs, and recent works try to leverage the properties such as subgraph counting and connectivity learning to characterize the expressive power of GNNs, which are more practical and closer to real-world. However, no survey papers and open-source repositories comprehensively summarize and discuss models in this important direction. To fill the gap, we conduct a first survey for models for enhancing expressive power under different forms of definition. Concretely, the models are reviewed based on three categories, i.e., 图像特征增强, 图像结构增强, and GNNs 架构增强.
</details></li>
</ul>
<hr>
<h2 id="Challenges-and-Opportunities-of-Using-Transformer-Based-Multi-Task-Learning-in-NLP-Through-ML-Lifecycle-A-Survey"><a href="#Challenges-and-Opportunities-of-Using-Transformer-Based-Multi-Task-Learning-in-NLP-Through-ML-Lifecycle-A-Survey" class="headerlink" title="Challenges and Opportunities of Using Transformer-Based Multi-Task Learning in NLP Through ML Lifecycle: A Survey"></a>Challenges and Opportunities of Using Transformer-Based Multi-Task Learning in NLP Through ML Lifecycle: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08234">http://arxiv.org/abs/2308.08234</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lovre Torbarina, Tin Ferkovic, Lukasz Roguski, Velimir Mihelcic, Bruno Sarlija, Zeljko Kraljevic</li>
<li>for: 本研究旨在提高自然语言处理（NLP）模型的效率和性能，通过 JOINT 训练多个模型，而不是单独训练每个模型。</li>
<li>methods: 本文提出了一种基于 transformer 的多任务学习（MTL）方法，并系统地分析了在 NLP 领域中这种方法如何应用于各个机器学习生命周期阶段。</li>
<li>results: 本文提出了一种基于 transformer 的 MTL 方法，并对这种方法的应用进行了系统的分析，包括数据工程、模型开发、部署和监测等阶段。此外，本文还提出了一种将 MTL 与 continual learning（CL）相连的想法，以便在模型 Periodically 重新训练、更新和添加新功能等方面具有更高的灵活性和可扩展性。<details>
<summary>Abstract</summary>
The increasing adoption of natural language processing (NLP) models across industries has led to practitioners' need for machine learning systems to handle these models efficiently, from training to serving them in production. However, training, deploying, and updating multiple models can be complex, costly, and time-consuming, mainly when using transformer-based pre-trained language models. Multi-Task Learning (MTL) has emerged as a promising approach to improve efficiency and performance through joint training, rather than training separate models. Motivated by this, we first provide an overview of transformer-based MTL approaches in NLP. Then, we discuss the challenges and opportunities of using MTL approaches throughout typical ML lifecycle phases, specifically focusing on the challenges related to data engineering, model development, deployment, and monitoring phases. This survey focuses on transformer-based MTL architectures and, to the best of our knowledge, is novel in that it systematically analyses how transformer-based MTL in NLP fits into ML lifecycle phases. Furthermore, we motivate research on the connection between MTL and continual learning (CL), as this area remains unexplored. We believe it would be practical to have a model that can handle both MTL and CL, as this would make it easier to periodically re-train the model, update it due to distribution shifts, and add new capabilities to meet real-world requirements.
</details>
<details>
<summary>摘要</summary>
随着自然语言处理（NLP）模型在不同领域的推广，机器学习（ML）实践者需要能够高效地训练、部署和更新多个模型，从训练到生产环境中的部署。然而，训练、部署和更新多个模型可能会复杂、成本高和时间consuming，特别是使用基于转换器的预训练语言模型。多任务学习（MTL）已经出现为提高效率和性能的可能性，我们将提供一个概述 transformer-based MTL 在 NLP 中的方法。然后，我们将讨论在 ML 生命周期阶段中使用 MTL approaches 的挑战和机遇，特别是关注数据工程、模型开发、部署和监控阶段的挑战。本文将重点关注基于 transformer 的 MTL 架构，并且，到我们所知道的 extend，这是一篇系统性的分析文章，探讨了 transformer-based MTL 在 NLP 中如何适应 ML 生命周期阶段。此外，我们还motivates 研究将 MTL 和 continual learning（CL）相连接，因为这个领域还没有得到过足的研究。我们认为，一个能够同时处理 MTL 和 CL 的模型会更加实用，这样可以更加方便地在 periodic 训练、因 distribution shift 更新模型以及添加新功能来满足实际需求。
</details></li>
</ul>
<hr>
<h2 id="SCQPTH-an-efficient-differentiable-splitting-method-for-convex-quadratic-programming"><a href="#SCQPTH-an-efficient-differentiable-splitting-method-for-convex-quadratic-programming" class="headerlink" title="SCQPTH: an efficient differentiable splitting method for convex quadratic programming"></a>SCQPTH: an efficient differentiable splitting method for convex quadratic programming</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08232">http://arxiv.org/abs/2308.08232</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrew Butler</li>
<li>for: 这篇论文主要是为了提出一种可微分的第一阶分裂法（SCQPTH），用于解 convex quadratic programs（QPs）。</li>
<li>methods: 这种方法基于 alternating direction method of multipliers（ADMM），并且受到现有的 state-of-the-art solver OSQP：一种操作分裂解决方案的启发。</li>
<li>results: 实验表明，对于大规模的 QPs，SCQPTH 可以提供 $1\times - 10\times$ 的计算效率提升，相比现有的可微分 QP 解决方案。<details>
<summary>Abstract</summary>
We present SCQPTH: a differentiable first-order splitting method for convex quadratic programs. The SCQPTH framework is based on the alternating direction method of multipliers (ADMM) and the software implementation is motivated by the state-of-the art solver OSQP: an operating splitting solver for convex quadratic programs (QPs). The SCQPTH software is made available as an open-source python package and contains many similar features including efficient reuse of matrix factorizations, infeasibility detection, automatic scaling and parameter selection. The forward pass algorithm performs operator splitting in the dimension of the original problem space and is therefore suitable for large scale QPs with $100-1000$ decision variables and thousands of constraints. Backpropagation is performed by implicit differentiation of the ADMM fixed-point mapping. Experiments demonstrate that for large scale QPs, SCQPTH can provide a $1\times - 10\times$ improvement in computational efficiency in comparison to existing differentiable QP solvers.
</details>
<details>
<summary>摘要</summary>
我们介绍了 SCQPTH：一种可微分的首选分解方法 для凸quadratic programs。 SCQPTH框架基于alternating direction method of multipliers（ADMM），并且由state-of-the-art solver OSQP：一种操作分裂解决方法 для凸quadratic programs（QPs）所 inspirited。 SCQPTH软件作为开源python包，具有许多相似特点，包括高效的矩阵因子重用、不可行检测、自动缩放和参数选择。 forward pass算法在原始问题空间的维度进行operator splitting，适用于大规模QPs，具有100-1000个决策变量和千个约束。 backpropagation通过ADMM固定点映射的隐式导数计算。实验表明，对于大规模QPs，SCQPTH可以提供1\*-10\*的计算效率提升，相比现有的可微分QP解决方法。
</details></li>
</ul>
<hr>
<h2 id="Self-Deception-Reverse-Penetrating-the-Semantic-Firewall-of-Large-Language-Models"><a href="#Self-Deception-Reverse-Penetrating-the-Semantic-Firewall-of-Large-Language-Models" class="headerlink" title="Self-Deception: Reverse Penetrating the Semantic Firewall of Large Language Models"></a>Self-Deception: Reverse Penetrating the Semantic Firewall of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11521">http://arxiv.org/abs/2308.11521</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenhua Wang, Wei Xie, Kai Chen, Baosheng Wang, Zhiwen Gui, Enze Wang<br>for:* The paper investigates the “LLM jailbreak” problem and proposes an automatic jailbreak method for the first time.methods:* The paper introduces the concept of a “semantic firewall” and provides three technical implementation approaches.* The paper introduces a “self-deception” attack that can bypass the semantic firewall by inducing LLM to generate prompts that facilitate jailbreak.results:* The paper reports a success rate of 86.2% and 67% on two models (GPT-3.5-Turbo and GPT-4) in generating attack payloads that can bypass the semantic firewall.* The paper also reports a failure rate of 4.7% and 2.2% on the two models, respectively.<details>
<summary>Abstract</summary>
Large language models (LLMs), such as ChatGPT, have emerged with astonishing capabilities approaching artificial general intelligence. While providing convenience for various societal needs, LLMs have also lowered the cost of generating harmful content. Consequently, LLM developers have deployed semantic-level defenses to recognize and reject prompts that may lead to inappropriate content. Unfortunately, these defenses are not foolproof, and some attackers have crafted "jailbreak" prompts that temporarily hypnotize the LLM into forgetting content defense rules and answering any improper questions. To date, there is no clear explanation of the principles behind these semantic-level attacks and defenses in both industry and academia.   This paper investigates the LLM jailbreak problem and proposes an automatic jailbreak method for the first time. We propose the concept of a semantic firewall and provide three technical implementation approaches. Inspired by the attack that penetrates traditional firewalls through reverse tunnels, we introduce a "self-deception" attack that can bypass the semantic firewall by inducing LLM to generate prompts that facilitate jailbreak. We generated a total of 2,520 attack payloads in six languages (English, Russian, French, Spanish, Chinese, and Arabic) across seven virtual scenarios, targeting the three most common types of violations: violence, hate, and pornography. The experiment was conducted on two models, namely the GPT-3.5-Turbo and GPT-4. The success rates on the two models were 86.2% and 67%, while the failure rates were 4.7% and 2.2%, respectively. This highlighted the effectiveness of the proposed attack method. All experimental code and raw data will be released as open-source to inspire future research. We believe that manipulating AI behavior through carefully crafted prompts will become an important research direction in the future.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM），如ChatGPT，已经出现了不可思议的能力，接近人工智能。它们为社会各种需求提供了便利，但也降低了生成危险内容的成本。因此，LLM开发者已经部署了semantic-level防御，以识别和拒绝可能导致不当内容的提示。然而，这些防御不是不可攻击的，一些攻击者已经制作了“监狱折衣”提示，使LLM忘记内容防御规则，回答任何不当问题。迄今为止，在业界和学术界都没有明确的semantic-level攻击和防御原理的解释。本文 investigate LLM监狱问题，并提出了自动监狱方法的第一次实现。我们提出了semantic firewall的概念，并提供了三种技术实现方式。受到传统防火墙被穿越反터 tunneled攻击的启发，我们引入了“自我欺骗”攻击，可以绕过semantic firewall，使LLM生成提示，促使监狱。我们总共生成了2,520个攻击 payload，分别在英语、俄语、法语、西班牙语、中文和阿拉伯语七种语言中，targeting三种最常见的违规行为：暴力、仇恨和色情。实验在GPT-3.5-Turbo和GPT-4两个模型上进行，成功率分别为86.2%和67%，失败率分别为4.7%和2.2%。这表明了我们提出的攻击方法的效iveness。我们将所有实验代码和原始数据发布为开源，以便未来的研究。我们认为，通过精心制作的提示，控制AI行为将成为未来的重要研究方向。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Winograd-Convolution-for-Cost-effective-Neural-Network-Fault-Tolerance"><a href="#Exploring-Winograd-Convolution-for-Cost-effective-Neural-Network-Fault-Tolerance" class="headerlink" title="Exploring Winograd Convolution for Cost-effective Neural Network Fault Tolerance"></a>Exploring Winograd Convolution for Cost-effective Neural Network Fault Tolerance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08230">http://arxiv.org/abs/2308.08230</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinghua Xue, Cheng Liu, Bo Liu, Haitong Huang, Ying Wang, Tao Luo, Lei Zhang, Huawei Li, Xiaowei Li</li>
<li>for: 本文研究了Winograd核函数在神经网络中的稳定性，以提高神经网络的硬件缺陷忍容性。</li>
<li>methods: 本文从不同的粒度（模型、层、操作类型）进行了全面的Winograd核函数缺陷忍容性评估。然后，本文探讨了在Winograd核函数基础上实现成本效果的NN保护策略。</li>
<li>results: 实验结果表明，Winograd核函数可以在不减少精度的情况下减少硬件缺陷忍容性设计负担，并且可以在具有不同硬件缺陷的情况下提高NN的精度。<details>
<summary>Abstract</summary>
Winograd is generally utilized to optimize convolution performance and computational efficiency because of the reduced multiplication operations, but the reliability issues brought by winograd are usually overlooked. In this work, we observe the great potential of winograd convolution in improving neural network (NN) fault tolerance. Based on the observation, we evaluate winograd convolution fault tolerance comprehensively from different granularities ranging from models, layers, and operation types for the first time. Then, we explore the use of inherent fault tolerance of winograd convolution for cost-effective NN protection against soft errors. Specifically, we mainly investigate how winograd convolution can be effectively incorporated with classical fault-tolerant design approaches including triple modular redundancy (TMR), fault-aware retraining, and constrained activation functions. According to our experiments, winograd convolution can reduce the fault-tolerant design overhead by 55.77\% on average without any accuracy loss compared to standard convolution, and further reduce the computing overhead by 17.24\% when the inherent fault tolerance of winograd convolution is considered. When it is applied on fault-tolerant neural networks enhanced with fault-aware retraining and constrained activation functions, the resulting model accuracy generally shows significant improvement in presence of various faults.
</details>
<details>
<summary>摘要</summary>
Winograd通常用于优化卷积性能和计算效率，因为它减少了乘法操作数量，但Winograd的可靠性问题通常被忽略。在这种工作中，我们发现Winograd卷积可以提高神经网络（NN）fault tolerance的潜力。基于这一观察，我们系统地评估Winograd卷积 fault tolerance从不同的粒度（models、layers、operation types）开始。然后，我们探索使用Winograd卷积的内在fault tolerance来实现cost-effective NN保护 against soft errors。具体来说，我们主要研究如何有效地将Winograd卷积与经典的 fault-tolerant设计方法（如TMR、 fault-aware retraining和受限 activation functions）结合使用。根据我们的实验，Winograd卷积可以在标准卷积的基础上减少fault-tolerant设计开销55.77%，而且在考虑Winograd卷积的内在fault tolerance时，可以减少计算开销17.24%。当应用于强化了 fault-tolerant神经网络的Winograd卷积、 fault-aware retraining和受限 activation functions后，模型的准确率在不同类型的缺陷情况下都显示了显著的改善。
</details></li>
</ul>
<hr>
<h2 id="Inherent-Redundancy-in-Spiking-Neural-Networks"><a href="#Inherent-Redundancy-in-Spiking-Neural-Networks" class="headerlink" title="Inherent Redundancy in Spiking Neural Networks"></a>Inherent Redundancy in Spiking Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08227">http://arxiv.org/abs/2308.08227</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/biclab/asa-snn">https://github.com/biclab/asa-snn</a></li>
<li>paper_authors: Man Yao, Jiakui Hu, Guangshe Zhao, Yaoyuan Wang, Ziyang Zhang, Bo Xu, Guoqi Li</li>
<li>for: 本研究旨在探讨隐藏在神经网络中的内在重复性，以提高神经网络的准确率和能效性。</li>
<li>methods: 本研究使用了隐藏状态激活（HSA）模块，以适应神经网络中的内在重复性，并对神经网络的各个元素进行了优化。</li>
<li>results: 实验结果表明，提案的方法可以显著减少神经网络中的冲击脉冲，并在比较于现有神经网络基eline上显示更好的性能。<details>
<summary>Abstract</summary>
Spiking Neural Networks (SNNs) are well known as a promising energy-efficient alternative to conventional artificial neural networks. Subject to the preconceived impression that SNNs are sparse firing, the analysis and optimization of inherent redundancy in SNNs have been largely overlooked, thus the potential advantages of spike-based neuromorphic computing in accuracy and energy efficiency are interfered. In this work, we pose and focus on three key questions regarding the inherent redundancy in SNNs. We argue that the redundancy is induced by the spatio-temporal invariance of SNNs, which enhances the efficiency of parameter utilization but also invites lots of noise spikes. Further, we analyze the effect of spatio-temporal invariance on the spatio-temporal dynamics and spike firing of SNNs. Then, motivated by these analyses, we propose an Advance Spatial Attention (ASA) module to harness SNNs' redundancy, which can adaptively optimize their membrane potential distribution by a pair of individual spatial attention sub-modules. In this way, noise spike features are accurately regulated. Experimental results demonstrate that the proposed method can significantly drop the spike firing with better performance than state-of-the-art SNN baselines. Our code is available in \url{https://github.com/BICLab/ASA-SNN}.
</details>
<details>
<summary>摘要</summary>
神经网络（SNN）已经广泛认可为一种能效的人工神经网络 alternatives。然而，由于人们对 SNN 的偏见，即 SNN 是稀疏的发射，因此对 SNN 内部缺乏 redundancy 的分析和优化，从而阻碍了 SNN 在准确性和能效性方面的潜在优势。在这个工作中，我们提出了三个关键问题，关于 SNN 中的内部缺乏 redundancy。我们认为，这种缺乏 redundancy 是由 SNN 的空间-时间不变性引起的，这种不变性可以提高参数的使用效率，但也会引入很多噪声脉冲。然后，我们分析了 SNN 的空间-时间动力学和脉冲发生的影响。根据这些分析结果，我们提出了一种 Advance Spatial Attention（ASA）模块，可以利用 SNN 的缺乏 redundancy，并可以自适应调整 SNN 的膜电压分布。这样，可以准确地控制噪声脉冲特征。实验结果表明，我们的方法可以显著降低 SNN 的脉冲发生，并且比现有的 SNN 基eline 性能更好。我们的代码可以在 \url{https://github.com/BICLab/ASA-SNN} 上找到。
</details></li>
</ul>
<hr>
<h2 id="How-To-Overcome-Confirmation-Bias-in-Semi-Supervised-Image-Classification-By-Active-Learning"><a href="#How-To-Overcome-Confirmation-Bias-in-Semi-Supervised-Image-Classification-By-Active-Learning" class="headerlink" title="How To Overcome Confirmation Bias in Semi-Supervised Image Classification By Active Learning"></a>How To Overcome Confirmation Bias in Semi-Supervised Image Classification By Active Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08224">http://arxiv.org/abs/2308.08224</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sandra Gilhuber, Rasmus Hvingelby, Mang Ling Ada Fok, Thomas Seidl</li>
<li>for: 本研究是为了检验是否需要活动学习，因为强大的深度半supervised方法的出现使得有限的标注数据设置中的活动学习可能失效。</li>
<li>methods: 本研究使用了semi-supervised learning（SSL）方法和活动学习（AL）方法，并 comparing their performance in realistic data scenarios。</li>
<li>results: 研究发现，在实际数据场景中，SSL方法可能会受到between-class imbalance、within-class imbalance和between-class similarity等挑战，这些挑战可能会导致SSL性能下降。然而，通过使用AL方法，可以超越confirmation bias，并在这些实际数据场景中提高SSL性能。<details>
<summary>Abstract</summary>
Do we need active learning? The rise of strong deep semi-supervised methods raises doubt about the usability of active learning in limited labeled data settings. This is caused by results showing that combining semi-supervised learning (SSL) methods with a random selection for labeling can outperform existing active learning (AL) techniques. However, these results are obtained from experiments on well-established benchmark datasets that can overestimate the external validity. However, the literature lacks sufficient research on the performance of active semi-supervised learning methods in realistic data scenarios, leaving a notable gap in our understanding. Therefore we present three data challenges common in real-world applications: between-class imbalance, within-class imbalance, and between-class similarity. These challenges can hurt SSL performance due to confirmation bias. We conduct experiments with SSL and AL on simulated data challenges and find that random sampling does not mitigate confirmation bias and, in some cases, leads to worse performance than supervised learning. In contrast, we demonstrate that AL can overcome confirmation bias in SSL in these realistic settings. Our results provide insights into the potential of combining active and semi-supervised learning in the presence of common real-world challenges, which is a promising direction for robust methods when learning with limited labeled data in real-world applications.
</details>
<details>
<summary>摘要</summary>
active learning是必要吗？ semi-supervised learning的强大方法的出现使得有限的标注数据设置中使用active learning的可用性存在各种 вопро题。这是因为结果表明将 semi-supervised learning（SSL）方法与随机选择标注结合可以超越现有的active learning（AL）技术。然而，这些结果是基于可靠的标准 benchmark dataset上进行的实验，这可能会过分估计外部适用性。然而，文献缺乏对实际数据场景中active semi-supervised learning方法的性能研究，这种知识漏洞存在。因此，我们提出了三种常见的实际数据挑战： между类异常、 Within-class异常和 между类相似。这些挑战可能会对 SSL性能产生负面影响，因为确认偏见。我们在模拟数据挑战中进行了SSL和AL实验，发现随机抽样不能消除确认偏见，有时even worse than supervised learning。然而，我们发现AL可以在这些实际设置中超越确认偏见。我们的结果为将活动和 semi-supervised learning结合使用在实际应用中的可能性提供了新的思路，这是一种robust方法在有限标注数据中学习的承诺。
</details></li>
</ul>
<hr>
<h2 id="HyperSNN-A-new-efficient-and-robust-deep-learning-model-for-resource-constrained-control-applications"><a href="#HyperSNN-A-new-efficient-and-robust-deep-learning-model-for-resource-constrained-control-applications" class="headerlink" title="HyperSNN: A new efficient and robust deep learning model for resource constrained control applications"></a>HyperSNN: A new efficient and robust deep learning model for resource constrained control applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08222">http://arxiv.org/abs/2308.08222</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhanglu Yan, Shida Wang, Kaiwen Tang, Weng-Fai Wong</li>
<li>for: 这篇论文旨在探讨智能家居、机器人和智能家具等领域中的边缘计算技术，尤其是使用脉冲神经网络（SNN）和超dimensional computing来进行控制任务。</li>
<li>methods: 这篇论文提出了一种名为HyperSNN的新方法，它使用8位数字加法来替代昂贵的32位浮点数 Multiplications，从而降低能源消耗，提高了韧性和可能提高精度。</li>
<li>results: 我们在AI Gym评 bench上测试了HyperSNN，结果显示HyperSNN可以与传统机器学习方法相比，仅 consume 1.36%到9.96%的能源，且在韧性方面也有提高。我们认为HyperSNN适用于互动、移动和穿戴式设备，实现能效的系统设计，并且开拓了实际应用中的实时预测控制（MPC）等复杂算法的可能性。<details>
<summary>Abstract</summary>
In light of the increasing adoption of edge computing in areas such as intelligent furniture, robotics, and smart homes, this paper introduces HyperSNN, an innovative method for control tasks that uses spiking neural networks (SNNs) in combination with hyperdimensional computing. HyperSNN substitutes expensive 32-bit floating point multiplications with 8-bit integer additions, resulting in reduced energy consumption while enhancing robustness and potentially improving accuracy. Our model was tested on AI Gym benchmarks, including Cartpole, Acrobot, MountainCar, and Lunar Lander. HyperSNN achieves control accuracies that are on par with conventional machine learning methods but with only 1.36% to 9.96% of the energy expenditure. Furthermore, our experiments showed increased robustness when using HyperSNN. We believe that HyperSNN is especially suitable for interactive, mobile, and wearable devices, promoting energy-efficient and robust system design. Furthermore, it paves the way for the practical implementation of complex algorithms like model predictive control (MPC) in real-world industrial scenarios.
</details>
<details>
<summary>摘要</summary>
在智能家居、机器人和智能家具等领域的edge computing应用日益普及，这篇论文提出了HyperSNN方法，这是一种结合神经网络和高维计算的新型控制方法。HyperSNN通过将昂贵的32位浮点 multiply替换为8位整数加法，从而降低能耗，同时提高了鲁棒性和可能提高了准确性。我们的模型在AI Gym测试启用上，包括Cartpole、Acrobot、MountainCar和Lunar Lander等标准测试集，HyperSNN实现了与传统机器学习方法相当的控制精度，但能耗只有1.36%到9.96%。此外，我们的实验还表明了HyperSNN具有更高的鲁棒性。我们认为HyperSNN特别适合交互式、移动和穿戴设备，推动能效的系统设计，同时为实际应用中的复杂算法如模型预测控制（MPC）铺平了道路。
</details></li>
</ul>
<hr>
<h2 id="In-situ-Fault-Diagnosis-of-Indium-Tin-Oxide-Electrodes-by-Processing-S-Parameter-Patterns"><a href="#In-situ-Fault-Diagnosis-of-Indium-Tin-Oxide-Electrodes-by-Processing-S-Parameter-Patterns" class="headerlink" title="In situ Fault Diagnosis of Indium Tin Oxide Electrodes by Processing S-Parameter Patterns"></a>In situ Fault Diagnosis of Indium Tin Oxide Electrodes by Processing S-Parameter Patterns</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11639">http://arxiv.org/abs/2308.11639</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tae Yeob Kang, Haebom Lee, Sungho Suh</li>
<li>for: 该研究旨在为光电子器件中的铝镉酸盐电极进行不 destruktive 的故障检测和诊断，以确保设备的性能和可靠性。</li>
<li>methods: 该研究使用了干扰参数（S-parameter）信号处理方法，可以早期检测、具有高精度、鲁棒性和根本原因分析。</li>
<li>results: 研究表明，可以通过将不同通道的S-parameters作为输入，使用深度学习（DL）方法同时分析报头和严重程度。此外，在增加了添加性噪声水平时， combining 不同通道的S-parameters可以明显提高诊断性能。<details>
<summary>Abstract</summary>
In the field of optoelectronics, indium tin oxide (ITO) electrodes play a crucial role in various applications, such as displays, sensors, and solar cells. Effective fault detection and diagnosis of the ITO electrodes are essential to ensure the performance and reliability of the devices. However, traditional visual inspection is challenging with transparent ITO electrodes, and existing fault detection methods have limitations in determining the root causes of the defects, often requiring destructive evaluations. In this study, an in situ fault diagnosis method is proposed using scattering parameter (S-parameter) signal processing, offering early detection, high diagnostic accuracy, noise robustness, and root cause analysis. A comprehensive S-parameter pattern database is obtained according to defect states. Deep learning (DL) approaches, including multilayer perceptron (MLP), convolutional neural network (CNN), and transformer, are then used to simultaneously analyze the cause and severity of defects. Notably, it is demonstrated that the diagnostic performance under additive noise levels can be significantly enhanced by combining different channels of the S-parameters as input to the learning algorithms, as confirmed through the t-distributed stochastic neighbor embedding (t-SNE) dimension reduction visualization.
</details>
<details>
<summary>摘要</summary>
在光电子学领域中，镍铁矿（ITO）电极扮演着重要的角色，包括显示器、感测器和太阳能电池等应用。有效检测和诊断ITO电极的缺陷是保证设备性能和可靠性的关键。然而，传统的视觉检查受到透明ITO电极的限制，现有的缺陷检测方法往往无法决定缺陷的根本原因，需要破坏性评估。本研究提出了一种实时缺陷诊断方法，使用散射参数（S-parameter）信号处理，可以早期检测、具有高精度、鲁棒性和根本原因分析。通过对缺陷状态下的S-parameter模式库的获取，使用深度学习（DL）方法，包括多层感知神经网络（MLP）、卷积神经网络（CNN）和变换器，同时分析缺陷的原因和严重程度。另外，研究表明，将不同通道的S-parameter作为输入，可以使用不同的混合方法提高诊断性能下附加噪声水平的表现。这一结论得到了通过t-分布随机邻居embedding（t-SNE）维度减少视觉化的确认。
</details></li>
</ul>
<hr>
<h2 id="Epicure-Distilling-Sequence-Model-Predictions-into-Patterns"><a href="#Epicure-Distilling-Sequence-Model-Predictions-into-Patterns" class="headerlink" title="Epicure: Distilling Sequence Model Predictions into Patterns"></a>Epicure: Distilling Sequence Model Predictions into Patterns</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08203">http://arxiv.org/abs/2308.08203</a></li>
<li>repo_url: None</li>
<li>paper_authors: Miltiadis Allamanis, Earl T. Barr</li>
<li>for: 用于生成高精度的函数名称和侦测异常函数名称</li>
<li>methods: 使用 Epicure 方法将模型预测结果转换为简单的几何模式</li>
<li>results: Epicure 方法可以对于预测函数名称和侦测异常函数名称 task 取得更高精度的结果，比起最佳模型预测结果高出 61% 以上。<details>
<summary>Abstract</summary>
Most machine learning models predict a probability distribution over concrete outputs and struggle to accurately predict names over high entropy sequence distributions. Here, we explore finding abstract, high-precision patterns intrinsic to these predictions in order to make abstract predictions that usefully capture rare sequences. In this short paper, we present Epicure, a method that distils the predictions of a sequence model, such as the output of beam search, into simple patterns. Epicure maps a model's predictions into a lattice that represents increasingly more general patterns that subsume the concrete model predictions.   On the tasks of predicting a descriptive name of a function given the source code of its body and detecting anomalous names given a function, we show that Epicure yields accurate naming patterns that match the ground truth more often compared to just the highest probability model prediction. For a false alarm rate of 10%, Epicure predicts patterns that match 61% more ground-truth names compared to the best model prediction, making Epicure well-suited for scenarios that require high precision.
</details>
<details>
<summary>摘要</summary>
大多数机器学习模型预测结果是一个概率分布，尤其是在高 entropy 序列分布时，它们很难准确预测名称。在这里，我们探索了找到Abstract高精度模式，以便使用这些模式来预测罕见序列。本文介绍了 Epicure 方法，它将序列模型预测结果映射到一个表示增加更一般模式的笛卡尔矩阵中。在函数的描述名称预测和异常名称检测任务上，我们显示了 Epicure 可以更准确地预测名称，相比于最佳模型预测。为了 false alarm rate 为 10%，Epicure 预测的模式与真实ground truth中的名称相匹配的情况比最佳模型预测多出了 61%。因此，Epicure 适用于需要高精度的场景。
</details></li>
</ul>
<hr>
<h2 id="DeSCo-Towards-Generalizable-and-Scalable-Deep-Subgraph-Counting"><a href="#DeSCo-Towards-Generalizable-and-Scalable-Deep-Subgraph-Counting" class="headerlink" title="DeSCo: Towards Generalizable and Scalable Deep Subgraph Counting"></a>DeSCo: Towards Generalizable and Scalable Deep Subgraph Counting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08198">http://arxiv.org/abs/2308.08198</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianyu Fu, Chiyue Wei, Yu Wang, Rex Ying<br>for: 这个研究是为了提出一个可扩展的神经网络架构，以便精确地预测查询 граф中的元素出现次数和位置。methods: 这个研究使用了一个新的专门分partition的技术，将大量目标 Graf divide 为小型邻接 Graph，以减少查询 граф中元素的 Count 的变化。然后，使用一个具有表现力的SUBGRAPH-based  hetereogeneous graph neural network 进行 Counting calculation。最后，使用learnable gates 进行 gossip propagation 以充分利用查询 Graf 中的 inductive biases。results: 这个研究在 eight 个真实世界数据集上进行了评估，与现有的神经网络方法相比，实现了137倍的mean squared error 的改善，同时保持了多项式时间复杂度。<details>
<summary>Abstract</summary>
Subgraph counting is the problem of counting the occurrences of a given query graph in a large target graph. Large-scale subgraph counting is useful in various domains, such as motif counting for social network analysis and loop counting for money laundering detection on transaction networks. Recently, to address the exponential runtime complexity of scalable subgraph counting, neural methods are proposed. However, existing neural counting approaches fall short in three aspects. Firstly, the counts of the same query can vary from zero to millions on different target graphs, posing a much larger challenge than most graph regression tasks. Secondly, current scalable graph neural networks have limited expressive power and fail to efficiently distinguish graphs in count prediction. Furthermore, existing neural approaches cannot predict the occurrence position of queries in the target graph.   Here we design DeSCo, a scalable neural deep subgraph counting pipeline, which aims to accurately predict the query count and occurrence position on any target graph after one-time training. Firstly, DeSCo uses a novel canonical partition and divides the large target graph into small neighborhood graphs. The technique greatly reduces the count variation while guaranteeing no missing or double-counting. Secondly, neighborhood counting uses an expressive subgraph-based heterogeneous graph neural network to accurately perform counting in each neighborhood. Finally, gossip propagation propagates neighborhood counts with learnable gates to harness the inductive biases of motif counts. DeSCo is evaluated on eight real-world datasets from various domains. It outperforms state-of-the-art neural methods with 137x improvement in the mean squared error of count prediction, while maintaining the polynomial runtime complexity.
</details>
<details>
<summary>摘要</summary>
大量子グラフ数えは、目标グラフ中の Given クエリー グラフの出现回数を数える问题です。大规模な子グラフ数えは、社会ネットワーク分析のモチーフ数えや、取引ネットワーク上の资金洗浄検出など、いくつかの领域で有用です。ただし、スケーラブルな子グラフ数えでは、问题の复雑さに対応するために、ニューラルな方法が提案されています。しかし、既存のニューラル カウンティング アプローチは、以下の3点で不足しています。1. 同じクエリーでは、ターゲット グラフによってカウントが0から数百万まで変化するため、大きな挑戦を提示します。2. 现在のスケーラブルなグラフニューラルネットワークは、クエリー カウントの效率的な予测をできません。3. 既存のニューラル アプローチは、ターゲット グラフ上のクエリーの出现位置を予测することができません。これらの问题を解决するために、我々はデスコ（DeSCo）というスケーラブルなニューラル ディープ サブグラフ カウンティング パイプラインを设计しました。デスコは、一度のトレーニングで任意のターゲット グラフ上のクエリー カウントと出现位置を正确に予测することができます。1. デスコでは、ターゲット グラフを小さな neighboorhood グラフに分割し、カウントのバラツキを大幅に削减します。2.  neighborhood カウンティングでは、heterogeneous graph neural network を使用して、各 neighboorhood でのカウントを正确に予测します。3. gossip propagation では、学习ゲートを使用して、适切なモチーフ カウントを导入します。デスコは、8つの実世界データセットから评価されました。その结果、状况 のあるニューラル メソッドに対して、137倍のmean squared error の改善を达成しました。また、既存のニューラル アプローチと同じように、ポリノミアルな时间コンプレックスを维持しています。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Explainable-AI-to-Analyze-Researchers’-Aspect-Based-Sentiment-about-ChatGPT"><a href="#Leveraging-Explainable-AI-to-Analyze-Researchers’-Aspect-Based-Sentiment-about-ChatGPT" class="headerlink" title="Leveraging Explainable AI to Analyze Researchers’ Aspect-Based Sentiment about ChatGPT"></a>Leveraging Explainable AI to Analyze Researchers’ Aspect-Based Sentiment about ChatGPT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11001">http://arxiv.org/abs/2308.11001</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shilpa Lakhanpal, Ajay Gupta, Rajeev Agrawal</li>
<li>for: 这篇论文的目的是分析研究者对ChatGPT的看法，以便更好地理解它的使用和发展。</li>
<li>methods: 该论文提出了一种使用可解释AI来进行方面基于情感分析的方法，以便在更新的 datasets 上进行分析。</li>
<li>results: 该论文通过实践示出了这种方法可以帮助扩展现有的状态艺术，并且在 longer text data 上进行有效的方面基于情感分析。<details>
<summary>Abstract</summary>
The groundbreaking invention of ChatGPT has triggered enormous discussion among users across all fields and domains. Among celebration around its various advantages, questions have been raised with regards to its correctness and ethics of its use. Efforts are already underway towards capturing user sentiments around it. But it begs the question as to how the research community is analyzing ChatGPT with regards to various aspects of its usage. It is this sentiment of the researchers that we analyze in our work. Since Aspect-Based Sentiment Analysis has usually only been applied on a few datasets, it gives limited success and that too only on short text data. We propose a methodology that uses Explainable AI to facilitate such analysis on research data. Our technique presents valuable insights into extending the state of the art of Aspect-Based Sentiment Analysis on newer datasets, where such analysis is not hampered by the length of the text data.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "The groundbreaking invention of ChatGPT has triggered enormous discussion among users across all fields and domains. Among celebration around its various advantages, questions have been raised with regards to its correctness and ethics of its use. Efforts are already underway towards capturing user sentiments around it. But it begs the question as to how the research community is analyzing ChatGPT with regards to various aspects of its usage. It is this sentiment of the researchers that we analyze in our work. Since Aspect-Based Sentiment Analysis has usually only been applied on a few datasets, it gives limited success and that too only on short text data. We propose a methodology that uses Explainable AI to facilitate such analysis on research data. Our technique presents valuable insights into extending the state of the art of Aspect-Based Sentiment Analysis on newer datasets, where such analysis is not hampered by the length of the text data." into Simplified Chinese.干货发明ChatGPT已经引发了各个领域和领导人的广泛的讨论。虽然欢快 celebrate its多种优点，但也提出了关于其正确性和使用道德问题的问题。尝试已经进行了捕捉用户情感的努力。但是，研究者如何分析ChatGPT在不同方面的使用仍然是一个问题。我们的工作是分析研究者对ChatGPT的感受。由于Aspect-Based Sentiment Analysis通常只能在一些数据集上进行，因此它具有有限的成功，只能处理短文本数据。我们提出了一种方法，使用可解释AI来实现这种分析在研究数据上。我们的技术可以提供对 newer datasets 的扩展state of the art的Aspect-Based Sentiment Analysis，不受文本数据的长度所限制。
</details></li>
</ul>
<hr>
<h2 id="Endogenous-Macrodynamics-in-Algorithmic-Recourse"><a href="#Endogenous-Macrodynamics-in-Algorithmic-Recourse" class="headerlink" title="Endogenous Macrodynamics in Algorithmic Recourse"></a>Endogenous Macrodynamics in Algorithmic Recourse</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08187">http://arxiv.org/abs/2308.08187</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pat-alt/endogenous-macrodynamics-in-algorithmic-recourse">https://github.com/pat-alt/endogenous-macrodynamics-in-algorithmic-recourse</a></li>
<li>paper_authors: Patrick Altmeyer, Giovan Angela, Aleksander Buszydlik, Karol Dobiczek, Arie van Deursen, Cynthia C. S. Liem</li>
<li>for: 本文主要研究Counterfactual Explanations（CE）和Algorithmic Recourse（AR）在动态环境下的应用，以及这些技术在实际应用中对其他个体的影响。</li>
<li>methods: 本文使用了一种普遍的框架来描述现有的方法ologies，并证明了这些方法ologies忽略了一种隐藏的外部成本，只有在研究团队级别的幂等 dynamics 时才会表现出来。</li>
<li>results: 通过使用各种现有的counterfactual生成器和多种标准数据集，我们在实验中生成了大量的counterfactuals，并研究了这些counterfactuals对模型和领域的影响。我们发现，由于recourse的实现，可能会导致模型和领域的变化，这些变化可能会妨碍Algorithmic Recourse的应用。然而，我们提出了一些缓解这些问题的策略。我们的实验框架快速、开源，可以帮助研究人员更好地理解recourse的应用。<details>
<summary>Abstract</summary>
Existing work on Counterfactual Explanations (CE) and Algorithmic Recourse (AR) has largely focused on single individuals in a static environment: given some estimated model, the goal is to find valid counterfactuals for an individual instance that fulfill various desiderata. The ability of such counterfactuals to handle dynamics like data and model drift remains a largely unexplored research challenge. There has also been surprisingly little work on the related question of how the actual implementation of recourse by one individual may affect other individuals. Through this work, we aim to close that gap. We first show that many of the existing methodologies can be collectively described by a generalized framework. We then argue that the existing framework does not account for a hidden external cost of recourse, that only reveals itself when studying the endogenous dynamics of recourse at the group level. Through simulation experiments involving various state-of the-art counterfactual generators and several benchmark datasets, we generate large numbers of counterfactuals and study the resulting domain and model shifts. We find that the induced shifts are substantial enough to likely impede the applicability of Algorithmic Recourse in some situations. Fortunately, we find various strategies to mitigate these concerns. Our simulation framework for studying recourse dynamics is fast and opensourced.
</details>
<details>
<summary>摘要</summary>
现有的Counterfactual Explanations（CE）和Algorithmic Recourse（AR）研究主要关注单个个体在静止环境下：给定一个估计模型，目标是找到满足多种要求的有效counterfactuals。然而，这些counterfactuals对数据和模型演变的能力尚未得到了充分的研究。此外，很少有关于个体实施救济后他们对其他个体的影响的研究。我们通过这项工作，希望能够填补这一差。我们首先示出了现有的方法ologies可以总结为一个通用框架。然后，我们 argue that现有的框架不会考虑到救济实施过程中隐藏的外部成本，只有在研究救济过程的群体水平时才会发现。通过使用当今顶尖counterfactual生成器和多个标准数据集，我们生成了大量的counterfactuals，并研究其所导致的领域和模型变化。我们发现，引入的变化是足够大，可能会阻碍救济的应用。幸运的是，我们发现了多种缓解这些问题的策略。我们的救济动力学研究框架快速，开源。
</details></li>
</ul>
<hr>
<h2 id="Accelerating-Generic-Graph-Neural-Networks-via-Architecture-Compiler-Partition-Method-Co-Design"><a href="#Accelerating-Generic-Graph-Neural-Networks-via-Architecture-Compiler-Partition-Method-Co-Design" class="headerlink" title="Accelerating Generic Graph Neural Networks via Architecture, Compiler, Partition Method Co-Design"></a>Accelerating Generic Graph Neural Networks via Architecture, Compiler, Partition Method Co-Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08174">http://arxiv.org/abs/2308.08174</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuwen Lu, Zhihui Zhang, Cong Guo, Jingwen Leng, Yangjie Zhou, Minyi Guo</li>
<li>for: 这个研究旨在开发高效和高性能的图形神经网络（GNNs）硬件加速器，以实现图形学习领域中的精确性提升。</li>
<li>methods: 这个研究使用了一些新的技术来解决GNN模型的两个基本挑战：一是GNN模型的带宽需求很高，二是GNN模型的多样性。这个研究使用了一种新的分区阶段Operator整合，以减少GNN模型的带宽需求；同时，这个研究也引入分区阶段多执行绪，以便同时处理图形分 partitions，并将不同的硬件资源分配给不同的执行绪。为了降低额外的On-chip memory，这个研究还提出了细化的图形分割。</li>
<li>results: 这个研究使用了 SwitchBlade 框架，包括编译器、图形分割器和硬件加速器，实现了对 NVIDIA V100 GPU 的平均速度提升为 1.85倍，和能源减少为 19.03倍。此外，SwitchBlade 还能够与现有的特殊适配器相比。<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) have shown significant accuracy improvements in a variety of graph learning domains, sparking considerable research interest. To translate these accuracy improvements into practical applications, it is essential to develop high-performance and efficient hardware acceleration for GNN models. However, designing GNN accelerators faces two fundamental challenges: the high bandwidth requirement of GNN models and the diversity of GNN models. Previous works have addressed the first challenge by using more expensive memory interfaces to achieve higher bandwidth. For the second challenge, existing works either support specific GNN models or have generic designs with poor hardware utilization.   In this work, we tackle both challenges simultaneously. First, we identify a new type of partition-level operator fusion, which we utilize to internally reduce the high bandwidth requirement of GNNs. Next, we introduce partition-level multi-threading to schedule the concurrent processing of graph partitions, utilizing different hardware resources. To further reduce the extra on-chip memory required by multi-threading, we propose fine-grained graph partitioning to generate denser graph partitions. Importantly, these three methods make no assumptions about the targeted GNN models, addressing the challenge of model variety. We implement these methods in a framework called SwitchBlade, consisting of a compiler, a graph partitioner, and a hardware accelerator. Our evaluation demonstrates that SwitchBlade achieves an average speedup of $1.85\times$ and energy savings of $19.03\times$ compared to the NVIDIA V100 GPU. Additionally, SwitchBlade delivers performance comparable to state-of-the-art specialized accelerators.
</details>
<details>
<summary>摘要</summary>
图 neural network (GNN) 在多种图学学问题上显示了重要的准确性改进，引起了广泛的研究兴趣。为将这些准确性改进应用于实际场景，必须开发高性能和高效的硬件加速器 для GNN 模型。然而，设计 GNN 加速器面临两个根本挑战：GNN 模型的带宽需求很高，以及 GNN 模型的多样性。previous works 通过使用更昂贵的内存接口来实现更高的带宽来解决第一个挑战。对于第二个挑战，现有的工作ether 支持特定 GNN 模型或者有通用的设计，但它们的硬件利用率很低。在这种情况下，我们同时解决了这两个挑战。首先，我们发现了一种新的分区级别的Operator融合方法，我们通过这种方法来减少 GNN 模型的带宽需求。然后，我们引入分区级别的多线程处理，以便同时处理不同的图分区，并使用不同的硬件资源。为了避免多线程处理所增加的额外的内存，我们提议使用细化的图分区来生成更密集的图分区。这三种方法不仅不假设目标 GNN 模型，而且可以同时解决多种 GNN 模型的问题。我们在 SwitchBlade 框架中实现了这三种方法， SwitchBlade 包括一个编译器、一个图分区器和一个硬件加速器。我们的评估表明，SwitchBlade 可以在 NVIDIA V100 GPU 上实现平均的速度提升为 1.85 倍，并且能够降低能耗量达 19.03 倍。此外，SwitchBlade 可以与现有的专门加速器相比，实现相似的性能。
</details></li>
</ul>
<hr>
<h2 id="Expressivity-of-Graph-Neural-Networks-Through-the-Lens-of-Adversarial-Robustness"><a href="#Expressivity-of-Graph-Neural-Networks-Through-the-Lens-of-Adversarial-Robustness" class="headerlink" title="Expressivity of Graph Neural Networks Through the Lens of Adversarial Robustness"></a>Expressivity of Graph Neural Networks Through the Lens of Adversarial Robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08173">http://arxiv.org/abs/2308.08173</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/francesco-campi/rob-subgraphs">https://github.com/francesco-campi/rob-subgraphs</a></li>
<li>paper_authors: Francesco Campi, Lukas Gosch, Tom Wollschläger, Yan Scholten, Stephan Günnemann</li>
<li>for: 这个论文探讨了图神经网络（GNNs）的对抗 robustness，并证明GNNs比传统的消息传递神经网络（MPNNs）更有力。</li>
<li>methods: 作者使用对抗 robustness作为一种工具，探讨GNNs的表达能力的限制。他们使用对抗攻击来测试GNNs的能力 counting specific subgraph patterns，并发展了高效的对抗攻击策略。</li>
<li>results: 研究发现，更强大的GNNs在小结构变化的情况下失去泛化能力，并且无法在非标准图上计数子结构。<details>
<summary>Abstract</summary>
We perform the first adversarial robustness study into Graph Neural Networks (GNNs) that are provably more powerful than traditional Message Passing Neural Networks (MPNNs). In particular, we use adversarial robustness as a tool to uncover a significant gap between their theoretically possible and empirically achieved expressive power. To do so, we focus on the ability of GNNs to count specific subgraph patterns, which is an established measure of expressivity, and extend the concept of adversarial robustness to this task. Based on this, we develop efficient adversarial attacks for subgraph counting and show that more powerful GNNs fail to generalize even to small perturbations to the graph's structure. Expanding on this, we show that such architectures also fail to count substructures on out-of-distribution graphs.
</details>
<details>
<summary>摘要</summary>
我们进行了首个对图 neural network (GNNs) 的敏感性研究，该研究表明 GNNs 比传统的讯息传递神经网络 (MPNNs) 更具潜力。具体来说，我们使用敏感性作为一个工具，对 GNNs 的表现进行探索，并发现了它们在实际上可以表达的表现和理论上可以表达的表现之间存在很大的差距。我们针对 GNNs 的子图计数能力进行了扩展，并发现了更强大的 GNNs 对小的结构变化进行了攻击，并且还无法处理非常力分布的图。
</details></li>
</ul>
<hr>
<h2 id="AATCT-IDS-A-Benchmark-Abdominal-Adipose-Tissue-CT-Image-Dataset-for-Image-Denoising-Semantic-Segmentation-and-Radiomics-Evaluation"><a href="#AATCT-IDS-A-Benchmark-Abdominal-Adipose-Tissue-CT-Image-Dataset-for-Image-Denoising-Semantic-Segmentation-and-Radiomics-Evaluation" class="headerlink" title="AATCT-IDS: A Benchmark Abdominal Adipose Tissue CT Image Dataset for Image Denoising, Semantic Segmentation, and Radiomics Evaluation"></a>AATCT-IDS: A Benchmark Abdominal Adipose Tissue CT Image Dataset for Image Denoising, Semantic Segmentation, and Radiomics Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08172">http://arxiv.org/abs/2308.08172</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiyu Ma, Chen Li, Tianming Du, Le Zhang, Dechao Tang, Deguo Ma, Shanchuan Huang, Yan Liu, Yihao Sun, Zhihao Chen, Jin Yuan, Qianqing Nie, Marcin Grzegorzek, Hongzan Sun</li>
<li>For: This paper is written to introduce and validate a new benchmark dataset for abdominal adipose tissue CT images, and to explore the potential of the dataset for different tasks such as image denoising, semantic segmentation, and radiomics.* Methods: The paper uses a benchmark dataset called AATTCT-IDS, which contains 13,732 raw CT slices and has been individually annotated for subcutaneous and visceral adipose tissue regions. The authors compare and analyze the performance of various methods on the dataset for different tasks.* Results: The results show that algorithms using a smoothing strategy perform better for image denoising, while methods like BM3D preserve the original image structure better. The segmentation results of adipose tissue by different models show different structural characteristics, and BiSeNet obtains segmentation results that are only slightly inferior to U-Net with the shortest training time. The radiomics study based on AATTCT-IDS reveals three adipose distributions in the subject population.Here’s the information in Simplified Chinese text:* For: 这篇论文是为了介绍和验证一个新的 Referenced dataset for abdominal adipose tissue CT images，并 explore这个dataset的多维特征以及其在不同任务中的潜在性。* Methods: 这篇论文使用一个名为AATTCT-IDS的 Referenced dataset，该dataset包含13,732个Raw CT slice，并且每个slice都被手动标注了脂肪组织区域。作者们对不同任务使用不同方法进行比较和分析。* Results: 结果表明使用平滑策略的算法在图像压缩中表现更好，而BM3D等方法能够更好地保持原始图像结构。不同模型的 segmentation 结果表明不同的结构特征，而BiSeNet等模型能够在最短训练时间内获得与U-Net的 segmentation 结果相似的结果。基于AATTCT-IDS的 radiomics 研究发现了脂肪分布的三种类型。<details>
<summary>Abstract</summary>
Methods: In this study, a benchmark \emph{Abdominal Adipose Tissue CT Image Dataset} (AATTCT-IDS) containing 300 subjects is prepared and published. AATTCT-IDS publics 13,732 raw CT slices, and the researchers individually annotate the subcutaneous and visceral adipose tissue regions of 3,213 of those slices that have the same slice distance to validate denoising methods, train semantic segmentation models, and study radiomics. For different tasks, this paper compares and analyzes the performance of various methods on AATTCT-IDS by combining the visualization results and evaluation data. Thus, verify the research potential of this data set in the above three types of tasks.   Results: In the comparative study of image denoising, algorithms using a smoothing strategy suppress mixed noise at the expense of image details and obtain better evaluation data. Methods such as BM3D preserve the original image structure better, although the evaluation data are slightly lower. The results show significant differences among them. In the comparative study of semantic segmentation of abdominal adipose tissue, the segmentation results of adipose tissue by each model show different structural characteristics. Among them, BiSeNet obtains segmentation results only slightly inferior to U-Net with the shortest training time and effectively separates small and isolated adipose tissue. In addition, the radiomics study based on AATTCT-IDS reveals three adipose distributions in the subject population.   Conclusion: AATTCT-IDS contains the ground truth of adipose tissue regions in abdominal CT slices. This open-source dataset can attract researchers to explore the multi-dimensional characteristics of abdominal adipose tissue and thus help physicians and patients in clinical practice. AATCT-IDS is freely published for non-commercial purpose at: \url{https://figshare.com/articles/dataset/AATTCT-IDS/23807256}.
</details>
<details>
<summary>摘要</summary>
方法：本研究使用了一个名为“腹部脂肪组织CT影像数据集”（AATTCT-IDS）的标准数据集，该数据集包含300名参与者，并公布了13,732个RAW CT slice的原始图像。研究人员ividually annotated 3,213个slice的脂肪组织区域，以验证去噪方法、训练semantic segmentation模型和研究 радиологи学。通过对不同任务的组合可视化结果和评估数据进行比较和分析，这个数据集的研究潜力得到了验证。结果：在图像去噪比较研究中，使用滤波策略的算法可以更好地降低杂噪，但是同时也会导致图像细节的产生。比如BM3D算法可以更好地保持原始图像结构，但评估数据略有下降。结果表明不同算法之间存在显著的差异。在脂肪组织分 segmentation研究中，BiSeNet模型可以在短时间内 obtian segmentation结果，并且可以有效地分离小型和隔离的脂肪组织。此外，基于AATTCT-IDS的 радиологи学研究发现了腹部脂肪组织中主要存在三种分布。结论：AATTCT-IDS包含了腹部CT影像中脂肪组织区域的真实图像。这个开源数据集可以吸引研究人员来探索腹部脂肪组织的多维特征，并帮助临床医生和病人。AATCT-IDS采用非商业用途发布，可以免费获取：https://figshare.com/articles/dataset/AATTCT-IDS/23807256。
</details></li>
</ul>
<hr>
<h2 id="A-Quantum-Approximation-Scheme-for-k-Means"><a href="#A-Quantum-Approximation-Scheme-for-k-Means" class="headerlink" title="A Quantum Approximation Scheme for k-Means"></a>A Quantum Approximation Scheme for k-Means</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08167">http://arxiv.org/abs/2308.08167</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ragesh Jaiswal</li>
<li>for: 这个论文的目的是解决经典k-means clustering问题，提供一种量子近似方案，可以在QRAM模型中实现，并且running time只具有 polynomial 幂级度的依赖关系。</li>
<li>methods: 这个量子算法使用了一种$(1+\varepsilon)$-近似方法，对于每个$\varepsilon&gt;0$，可以在QRAM数据结构上进行实现，并且running time为$\tilde{O}\left(2^{\tilde{O}\left(\frac{k}{\varepsilon}\right)} \eta^2 d\right)$。</li>
<li>results: 这个量子算法可以在高probability下输出一组$k$个中心点，使得$cost(V, C) \leq (1+\varepsilon) \cdot cost(V, C_{OPT})$，其中$C_{OPT}$是最优的$k$-中心点，$cost(.)$是标准的$k$-means成本函数（即点到最近中心点的平方距离之和），$\eta$是几何比（即最大距离到最小距离的比）。这是第一个具有polylogarithmic running time的量子算法，可以提供$(1+\varepsilon)$的证明近似保证。<details>
<summary>Abstract</summary>
We give a quantum approximation scheme (i.e., $(1 + \varepsilon)$-approximation for every $\varepsilon > 0$) for the classical $k$-means clustering problem in the QRAM model with a running time that has only polylogarithmic dependence on the number of data points. More specifically, given a dataset $V$ with $N$ points in $\mathbb{R}^d$ stored in QRAM data structure, our quantum algorithm runs in time $\tilde{O} \left( 2^{\tilde{O}(\frac{k}{\varepsilon})} \eta^2 d\right)$ and with high probability outputs a set $C$ of $k$ centers such that $cost(V, C) \leq (1+\varepsilon) \cdot cost(V, C_{OPT})$. Here $C_{OPT}$ denotes the optimal $k$-centers, $cost(.)$ denotes the standard $k$-means cost function (i.e., the sum of the squared distance of points to the closest center), and $\eta$ is the aspect ratio (i.e., the ratio of maximum distance to minimum distance). This is the first quantum algorithm with a polylogarithmic running time that gives a provable approximation guarantee of $(1+\varepsilon)$ for the $k$-means problem. Also, unlike previous works on unsupervised learning, our quantum algorithm does not require quantum linear algebra subroutines and has a running time independent of parameters (e.g., condition number) that appear in such procedures.
</details>
<details>
<summary>摘要</summary>
我们提供了一种量子近似方案（即$(1 + \varepsilon)$-近似方案）来解决 классиical $k$-means归一化问题在QRAM模型中，并且running时间具有只带有多项式幂ilogarithmic（polylogarithmic）依赖于数据点的数量。更具体地说，给定一个数据集$V$包含$N$个点在$\mathbb{R}^d$中，我们的量子算法在时间 $\tilde{O} \left( 2^{\tilde{O}(\frac{k}{\varepsilon})} \eta^2 d\right)$ 内运行，并且 WITH HIGH PROBABILITY输出一组 $C$ 的 $k$ 中心，使得 $cost(V, C) \leq (1+\varepsilon) \cdot cost(V, C_{OPT})$，其中 $C_{OPT}$ 表示最优的 $k$-中心，$cost(.)$ 表示标准 $k$-means 成本函数（即点到最近中心的平方距离的总和），而 $\eta$ 是最大距离到最小距离的比率（即 aspect ratio）。这是第一个具有 polylogarithmic 运行时间的量子算法，并且不需要量子线性代数子routines，运行时间与参数（例如 condition number）无关。Note:* "QRAM" stands for "Quantum Random Access Memory", which is a quantum analogue of classical random access memory.* "polylogarithmic" means the running time has a polynomial logarithmic dependence on the number of data points.* "condition number" refers to the ratio of the maximum distance to the minimum distance in the dataset.
</details></li>
</ul>
<hr>
<h2 id="PEvoLM-Protein-Sequence-Evolutionary-Information-Language-Model"><a href="#PEvoLM-Protein-Sequence-Evolutionary-Information-Language-Model" class="headerlink" title="PEvoLM: Protein Sequence Evolutionary Information Language Model"></a>PEvoLM: Protein Sequence Evolutionary Information Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08578">http://arxiv.org/abs/2308.08578</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/issararab/pevolm">https://github.com/issararab/pevolm</a></li>
<li>paper_authors: Issar Arab</li>
<li>for: 本研究旨在提高protein序列数据库的搜索效率和质量，以及提高计算生物学和生物信息学中ML模型的性能。</li>
<li>methods: 本研究使用了一种基于自然语言处理的语言模型（ELMo），将蛋白质序列转换为数字Vector表示。研究还使用了PSSM的概念和传输学习，开发了一种新的双向语言模型（bi-LM），其中一个路径用于前向传输，另一个路径用于反向传输。</li>
<li>results: 研究发现，使用bi-LM可以在预测下一个氨基酸时同时学习蛋白质序列的演化信息，并且bi-LM的参数数量比原始ELMo少了四倍。同时，bi-LM在预测下一个氨基酸和PSSM中的概率分布方面也达到了比较高的性能。<details>
<summary>Abstract</summary>
With the exponential increase of the protein sequence databases over time, multiple-sequence alignment (MSA) methods, like PSI-BLAST, perform exhaustive and time-consuming database search to retrieve evolutionary information. The resulting position-specific scoring matrices (PSSMs) of such search engines represent a crucial input to many machine learning (ML) models in the field of bioinformatics and computational biology. A protein sequence is a collection of contiguous tokens or characters called amino acids (AAs). The analogy to natural language allowed us to exploit the recent advancements in the field of Natural Language Processing (NLP) and therefore transfer NLP state-of-the-art algorithms to bioinformatics. This research presents an Embedding Language Model (ELMo), converting a protein sequence to a numerical vector representation. While the original ELMo trained a 2-layer bidirectional Long Short-Term Memory (LSTMs) network following a two-path architecture, one for the forward and the second for the backward pass, by merging the idea of PSSMs with the concept of transfer-learning, this work introduces a novel bidirectional language model (bi-LM) with four times less free parameters and using rather a single path for both passes. The model was trained not only on predicting the next AA but also on the probability distribution of the next AA derived from similar, yet different sequences as summarized in a PSSM, simultaneously for multi-task learning, hence learning evolutionary information of protein sequences as well. The network architecture and the pre-trained model are made available as open source under the permissive MIT license on GitHub at https://github.com/issararab/PEvoLM.
</details>
<details>
<summary>摘要</summary>
随着蛋白序列数据库的不断增长，多重序列对 align (MSA) 方法，如 PSI-BLAST，在时间上进行极其耗时的数据库搜索，以获取演化信息。 resulting position-specific scoring matrices (PSSMs) 的搜索引擎表示了生物信息学和计算生物学领域中机器学习 (ML) 模型的关键输入。一个蛋白序列是一个连续的 tokens 或字符串，叫做氨基酸 (AA)。由于蛋白序列与自然语言之间的相似性，我们可以利用自然语言处理领域的最新进展，并将其转移到生物信息学中。本研究提出了 Embedding Language Model (ELMo)，将蛋白序列转换为数字向量表示。而原始 ELMo 使用了两层拟合长短时间记忆 (LSTMs) 网络，一个是向前的一个，另一个是向后的一个，通过将 PSSMs 的想法与传输学习的概念结合起来，这个工作提出了一种新的双向语言模型 (bi-LM)，具有四倍少的自由参数，使用单一路径来进行两个方向的传输。这个模型不仅在预测下一个 AA 上，还在 PSSM 中的概率分布上进行学习，同时进行多任务学习，因此学习蛋白序列的演化信息。模型的网络架构和预训练模型都在 GitHub 上公开，可以在 <https://github.com/issararab/PEvoLM> 获取。
</details></li>
</ul>
<hr>
<h2 id="Stochastic-Controlled-Averaging-for-Federated-Learning-with-Communication-Compression"><a href="#Stochastic-Controlled-Averaging-for-Federated-Learning-with-Communication-Compression" class="headerlink" title="Stochastic Controlled Averaging for Federated Learning with Communication Compression"></a>Stochastic Controlled Averaging for Federated Learning with Communication Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08165">http://arxiv.org/abs/2308.08165</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinmeng Huang, Ping Li, Xiaoyun Li</li>
<li>for: 降低 Federated Learning（FL）的通信负担，提高FL的效率和可扩展性。</li>
<li>methods: 提议一种更有效率和简化的Stochastic Controlled Averaging方法，并基于该方法提出两种压缩FL算法：SCALLION和SCAFCOM。这两种算法可以支持不偏向和偏向压缩，并且可以适应任意数据不同性和不偏向压缩。</li>
<li>results: 对比 existed 压缩FL算法，SCALLION和SCAFCOM可以减少通信和计算复杂度，并且可以与相应的全精度FL方法匹配或超越其性能。实验结果表明，SCALLION和SCAFCOM可以在相同的通信预算下提高FL的性能。<details>
<summary>Abstract</summary>
Communication compression, a technique aiming to reduce the information volume to be transmitted over the air, has gained great interests in Federated Learning (FL) for the potential of alleviating its communication overhead. However, communication compression brings forth new challenges in FL due to the interplay of compression-incurred information distortion and inherent characteristics of FL such as partial participation and data heterogeneity. Despite the recent development, the performance of compressed FL approaches has not been fully exploited. The existing approaches either cannot accommodate arbitrary data heterogeneity or partial participation, or require stringent conditions on compression.   In this paper, we revisit the seminal stochastic controlled averaging method by proposing an equivalent but more efficient/simplified formulation with halved uplink communication costs. Building upon this implementation, we propose two compressed FL algorithms, SCALLION and SCAFCOM, to support unbiased and biased compression, respectively. Both the proposed methods outperform the existing compressed FL methods in terms of communication and computation complexities. Moreover, SCALLION and SCAFCOM accommodates arbitrary data heterogeneity and do not make any additional assumptions on compression errors. Experiments show that SCALLION and SCAFCOM can match the performance of corresponding full-precision FL approaches with substantially reduced uplink communication, and outperform recent compressed FL methods under the same communication budget.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用压缩通信技术可以减少在空中传输的信息量，这已经在联合学习（Federated Learning，FL）中受到了广泛关注，因为它可以减轻FL的通信开销。然而，压缩通信会在FL中带来新的挑战，这是因为压缩通信会导致信息损害，并且FL的特点，如部分参与和数据不同化，会导致压缩通信的效果不可预测。虽然最近有些研究已经进行了，但是现有的方法并不能充分发挥性能。在这篇论文中，我们重新评估了一种基于渐进控制的概率平均法，并提出了一种更高效/简单的表述方式，可以减少上行通信成本的一半。基于这种实现方式，我们提出了两种压缩FL算法，即SCALLION和SCAFCOM，可以支持不偏和偏压缩。两种算法都能够超过现有的压缩FL方法，并且可以适应任意的数据不同化，不需要任何额外的压缩错误假设。实验表明，SCALLION和SCAFCOM可以与相应的全精度FL方法匹配性能，并且在同样的通信预算下表现更好。Note: Simplified Chinese is a simplified version of Chinese that is used in mainland China and Singapore. It is different from Traditional Chinese, which is used in Taiwan and other countries.
</details></li>
</ul>
<hr>
<h2 id="Characteristics-of-networks-generated-by-kernel-growing-neural-gas"><a href="#Characteristics-of-networks-generated-by-kernel-growing-neural-gas" class="headerlink" title="Characteristics of networks generated by kernel growing neural gas"></a>Characteristics of networks generated by kernel growing neural gas</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08163">http://arxiv.org/abs/2308.08163</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kazuhisafujita/kernelgng">https://github.com/kazuhisafujita/kernelgng</a></li>
<li>paper_authors: Kazuhisa Fujita</li>
<li>for: 本研究旨在开发 kernel GNG，即基kernels的生长神经网络算法，并investigate kernel GNG生成的网络特性。</li>
<li>methods: 本研究使用了五种kernel，包括 Gaussian、Laplacian、Cauchy、 inverse multiquadric 和 log kernels，以mappingdataset到特征空间。</li>
<li>results: 研究发现，kernel GNG可以生成具有高度稠密度和高度连接度的网络，并且可以准确地捕捉dataset中的特征。<details>
<summary>Abstract</summary>
This research aims to develop kernel GNG, a kernelized version of the growing neural gas (GNG) algorithm, and to investigate the features of the networks generated by the kernel GNG. The GNG is an unsupervised artificial neural network that can transform a dataset into an undirected graph, thereby extracting the features of the dataset as a graph. The GNG is widely used in vector quantization, clustering, and 3D graphics. Kernel methods are often used to map a dataset to feature space, with support vector machines being the most prominent application. This paper introduces the kernel GNG approach and explores the characteristics of the networks generated by kernel GNG. Five kernels, including Gaussian, Laplacian, Cauchy, inverse multiquadric, and log kernels, are used in this study.
</details>
<details>
<summary>摘要</summary>
这个研究的目标是开发kernel GNG，即kernelized版本的增长神经气体（GNG）算法，并研究由kernel GNG生成的网络特征。GNG是一种无监督的人工神经网络，可以将数据集转换成无向图，从而提取数据集中的特征作为图。GNG广泛应用于 вектор化Quantization、归一化和3D图形。 kernel方法通常用于将数据集映射到特征空间，支持向量机器学习是最广泛应用的例子。这篇文章介绍了kernel GNG方法，并探索由kernel GNG生成的网络特征。本研究使用的五种kernels包括Gaussian、Laplacian、Cauchy、 inverse multiquadric和log kernel。
</details></li>
</ul>
<hr>
<h2 id="Interpretability-Benchmark-for-Evaluating-Spatial-Misalignment-of-Prototypical-Parts-Explanations"><a href="#Interpretability-Benchmark-for-Evaluating-Spatial-Misalignment-of-Prototypical-Parts-Explanations" class="headerlink" title="Interpretability Benchmark for Evaluating Spatial Misalignment of Prototypical Parts Explanations"></a>Interpretability Benchmark for Evaluating Spatial Misalignment of Prototypical Parts Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08162">http://arxiv.org/abs/2308.08162</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mikołaj Sacha, Bartosz Jura, Dawid Rymarczyk, Łukasz Struski, Jacek Tabor, Bartosz Zieliński</li>
<li>for: 提高parts-based网络的自我解释性</li>
<li>methods: 引入一个特有的解释性指标集和修复方法来纠正偏移</li>
<li>results: 通过实验研究，表明提出的指标集和修复方法能够有效纠正偏移，提高parts-based网络的解释性<details>
<summary>Abstract</summary>
Prototypical parts-based networks are becoming increasingly popular due to their faithful self-explanations. However, their similarity maps are calculated in the penultimate network layer. Therefore, the receptive field of the prototype activation region often depends on parts of the image outside this region, which can lead to misleading interpretations. We name this undesired behavior a spatial explanation misalignment and introduce an interpretability benchmark with a set of dedicated metrics for quantifying this phenomenon. In addition, we propose a method for misalignment compensation and apply it to existing state-of-the-art models. We show the expressiveness of our benchmark and the effectiveness of the proposed compensation methodology through extensive empirical studies.
</details>
<details>
<summary>摘要</summary>
弹性部件网络在现代计算机视觉识别领域中日益受欢迎，主要是因为它们的自我解释能力很强。然而，它们的相似度图在次末层网络层中计算，因此prototype activation区域的接收场景经常受到外部像区域的影响，这可能会导致误leading interpretations。我们称这为 espacial explanation misalignment，并提出了一个dedicated metrics集以量化这种现象。此外，我们也提出了一种修正方法，并将其应用到现有的state-of-the-art模型中。我们透过广泛的实验研究表明了我们的benchmark的表达能力和我们的修正方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-Adversarial-Robustness-of-Compressed-Deep-Learning-Models"><a href="#Benchmarking-Adversarial-Robustness-of-Compressed-Deep-Learning-Models" class="headerlink" title="Benchmarking Adversarial Robustness of Compressed Deep Learning Models"></a>Benchmarking Adversarial Robustness of Compressed Deep Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08160">http://arxiv.org/abs/2308.08160</a></li>
<li>repo_url: None</li>
<li>paper_authors: Brijesh Vora, Kartik Patwari, Syed Mahbub Hafiz, Zubair Shafiq, Chen-Nee Chuah</li>
<li>for: 本研究旨在探讨基础模型对各种攻击Input的抗性，以便更好地理解压缩模型对抗性的影响。</li>
<li>methods: 我们开发了一个多样化的攻击 benchmark，用于测试不同的攻击方法和常见深度神经网络模型。我们采用了优化的压缩策略，以保持准确性和性能。</li>
<li>results: 我们发现，压缩后的模型仍然保持了对攻击的抗性，而且拥有更好的泛化性、更高的性能和更快的执行速度。这表明，压缩模型不会对抗性造成负面影响。<details>
<summary>Abstract</summary>
The increasing size of Deep Neural Networks (DNNs) poses a pressing need for model compression, particularly when employed on resource constrained devices. Concurrently, the susceptibility of DNNs to adversarial attacks presents another significant hurdle. Despite substantial research on both model compression and adversarial robustness, their joint examination remains underexplored. Our study bridges this gap, seeking to understand the effect of adversarial inputs crafted for base models on their pruned versions. To examine this relationship, we have developed a comprehensive benchmark across diverse adversarial attacks and popular DNN models. We uniquely focus on models not previously exposed to adversarial training and apply pruning schemes optimized for accuracy and performance. Our findings reveal that while the benefits of pruning enhanced generalizability, compression, and faster inference times are preserved, adversarial robustness remains comparable to the base model. This suggests that model compression while offering its unique advantages, does not undermine adversarial robustness.
</details>
<details>
<summary>摘要</summary>
随着深度神经网络（DNN）的尺度不断增大，需要进行模型压缩，特别是在资源有限的设备上使用。同时，DNN受到攻击者的攻击也成为一个重要的障碍。虽然关于模型压缩和攻击鲁棒性的研究已经进行了大量的工作，但是它们之间的联系还没有得到充分探讨。我们的研究尝试填补这个空白，探索攻击基本模型的输入对其压缩版本的影响。为此，我们开发了一个包括多种攻击和各种流行的DNN模型的完整的benchmark。我们独特地将注意力集中在没有接受过攻击训练的模型上，并应用优化的减少方案以保持准确性和性能。我们的发现表明，即使使用压缩，模型的总体鲁棒性仍然保持不变，这表明模型压缩不会对鲁棒性产生负面影响。这表明，模型压缩可以提供独特的优势，而不会对鲁棒性产生负面影响。
</details></li>
</ul>
<hr>
<h2 id="Deep-Generative-Imputation-Model-for-Missing-Not-At-Random-Data"><a href="#Deep-Generative-Imputation-Model-for-Missing-Not-At-Random-Data" class="headerlink" title="Deep Generative Imputation Model for Missing Not At Random Data"></a>Deep Generative Imputation Model for Missing Not At Random Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08158">http://arxiv.org/abs/2308.08158</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jialei Chen, Yuanbo Xu, Pengyang Wang, Yongjian Yang</li>
<li>for: 强调处理真实世界中存在缺失的数据，而不是模拟MCAR的缺失机制。</li>
<li>methods: 提出了一种基于joint probability decomposition的generative模型特有的方法，并在latent空间中处理真实世界中的缺失机制。</li>
<li>results: 对比state-of-the-art基elines，GNR模型在RMSE指标上平均提高9.9%至18.8%，并且总是在mask重建精度方面得到更好的结果。<details>
<summary>Abstract</summary>
Data analysis usually suffers from the Missing Not At Random (MNAR) problem, where the cause of the value missing is not fully observed. Compared to the naive Missing Completely At Random (MCAR) problem, it is more in line with the realistic scenario whereas more complex and challenging. Existing statistical methods model the MNAR mechanism by different decomposition of the joint distribution of the complete data and the missing mask. But we empirically find that directly incorporating these statistical methods into deep generative models is sub-optimal. Specifically, it would neglect the confidence of the reconstructed mask during the MNAR imputation process, which leads to insufficient information extraction and less-guaranteed imputation quality. In this paper, we revisit the MNAR problem from a novel perspective that the complete data and missing mask are two modalities of incomplete data on an equal footing. Along with this line, we put forward a generative-model-specific joint probability decomposition method, conjunction model, to represent the distributions of two modalities in parallel and extract sufficient information from both complete data and missing mask. Taking a step further, we exploit a deep generative imputation model, namely GNR, to process the real-world missing mechanism in the latent space and concurrently impute the incomplete data and reconstruct the missing mask. The experimental results show that our GNR surpasses state-of-the-art MNAR baselines with significant margins (averagely improved from 9.9% to 18.8% in RMSE) and always gives a better mask reconstruction accuracy which makes the imputation more principle.
</details>
<details>
<summary>摘要</summary>
通常情况下，数据分析会面临缺失不够权（MNAR）问题，其中数据缺失的原因不能全面观察。与完全随机缺失（MCAR）问题相比，MNAR问题更加真实和复杂。现有的统计方法模型了MNAR机制的各种分解方法，但我们发现直接将这些统计方法 integrate into深度生成模型是不优化的。具体来说，这会忽略恢复 маска的信任度 during MNAR 恢复过程，导致信息抽取不充分和缺失补做质量不够保障。在这篇论文中，我们从一种新的视角重新审视了MNAR问题，即完整数据和缺失数据是两种不同的杂态数据模式。遵循这种思路，我们提出了一种生成模型特有的联合概率分解方法，即并联模型，用于同时表征两种模式的分布。这种方法可以从两种模式中提取足够的信息，并且可以在缺失数据和恢复 маска之间进行共同补做。进一步地，我们利用深度生成补做模型，即GNR，来处理实际世界中的缺失机制，并同时补做缺失数据和恢复 маска。实验结果显示，我们的GNR在MNAR基线上显著提高了性能（平均提高9.9%到18.8%），并且总是提供更好的恢复率，这使得补做更符合原理。
</details></li>
</ul>
<hr>
<h2 id="Sarcasm-Detection-in-a-Disaster-Context"><a href="#Sarcasm-Detection-in-a-Disaster-Context" class="headerlink" title="Sarcasm Detection in a Disaster Context"></a>Sarcasm Detection in a Disaster Context</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08156">http://arxiv.org/abs/2308.08156</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tiberiu Sosea, Junyi Jessy Li, Cornelia Caragea</li>
<li>for: 这个论文是为了研究自然灾害发生时人们在社交媒体平台上使用的讲话方式，以及如何使用先进的自然语言处理技术来理解这种讲话方式。</li>
<li>methods: 该论文使用了一个名为HurricaneSARC的数据集，该数据集包含15,000条涉及讲话的报道，并使用了预训练的语言模型进行讲话检测。</li>
<li>results: 该论文的最佳模型在HurricaneSARC数据集上取得了0.70的F1分，并且通过中间任务转移学习可以提高模型的性能。<details>
<summary>Abstract</summary>
During natural disasters, people often use social media platforms such as Twitter to ask for help, to provide information about the disaster situation, or to express contempt about the unfolding event or public policies and guidelines. This contempt is in some cases expressed as sarcasm or irony. Understanding this form of speech in a disaster-centric context is essential to improving natural language understanding of disaster-related tweets. In this paper, we introduce HurricaneSARC, a dataset of 15,000 tweets annotated for intended sarcasm, and provide a comprehensive investigation of sarcasm detection using pre-trained language models. Our best model is able to obtain as much as 0.70 F1 on our dataset. We also demonstrate that the performance on HurricaneSARC can be improved by leveraging intermediate task transfer learning. We release our data and code at https://github.com/tsosea2/HurricaneSarc.
</details>
<details>
<summary>摘要</summary>
在自然灾害事件中，人们常利用社交媒体平台如推特，请求帮助、提供灾害情况信息或表达对事件或公共政策的负面 sentiment。这种语言形式在灾害Context中是非常重要的，以提高自然语言理解灾害相关的推文。在这篇论文中，我们介绍了风暴SARC数据集，包含15000条推文，并进行了 pré-trained语言模型的全面研究。我们的最佳模型在我们的数据集上可以获得0.70的F1分。我们还证明了在HurricaneSARC上进行中间任务传承学习可以提高性能。我们将数据和代码发布在https://github.com/tsosea2/HurricaneSarc上。
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-Topological-Ordering-with-Conditional-Independence-Test-for-Limited-Time-Series"><a href="#Hierarchical-Topological-Ordering-with-Conditional-Independence-Test-for-Limited-Time-Series" class="headerlink" title="Hierarchical Topological Ordering with Conditional Independence Test for Limited Time Series"></a>Hierarchical Topological Ordering with Conditional Independence Test for Limited Time Series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08148">http://arxiv.org/abs/2308.08148</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anpeng Wu, Haoxuan Li, Kun Kuang, Keli Zhang, Fei Wu</li>
<li>For: This paper aims to improve the process of learning directed acyclic graphs (DAGs) to identify causal relations in observational data.* Methods: The proposed method, called HT-CIT, incorporates limited time series data and conditional instrumental variables to identify descendant nodes for each variable. The algorithm uses a hierarchical topological ordering approach with a conditional independence test to efficiently learn sparse DAGs with a smaller search space.* Results: The proposed HT-CIT algorithm is shown to be superior to other popular approaches through empirical results from synthetic and real-world datasets, with a significant reduction in the number of edges that need to be pruned.<details>
<summary>Abstract</summary>
Learning directed acyclic graphs (DAGs) to identify causal relations underlying observational data is crucial but also poses significant challenges. Recently, topology-based methods have emerged as a two-step approach to discovering DAGs by first learning the topological ordering of variables and then eliminating redundant edges, while ensuring that the graph remains acyclic. However, one limitation is that these methods would generate numerous spurious edges that require subsequent pruning. To overcome this limitation, in this paper, we propose an improvement to topology-based methods by introducing limited time series data, consisting of only two cross-sectional records that need not be adjacent in time and are subject to flexible timing. By incorporating conditional instrumental variables as exogenous interventions, we aim to identify descendant nodes for each variable. Following this line, we propose a hierarchical topological ordering algorithm with conditional independence test (HT-CIT), which enables the efficient learning of sparse DAGs with a smaller search space compared to other popular approaches. The HT-CIT algorithm greatly reduces the number of edges that need to be pruned. Empirical results from synthetic and real-world datasets demonstrate the superiority of the proposed HT-CIT algorithm.
</details>
<details>
<summary>摘要</summary>
To overcome this limitation, this paper proposes an improvement to topology-based methods by incorporating limited time series data, consisting of only two cross-sectional records that do not need to be adjacent in time and are subject to flexible timing. By using conditional instrumental variables as exogenous interventions, we aim to identify descendant nodes for each variable.We propose a hierarchical topological ordering algorithm with conditional independence tests (HT-CIT), which enables the efficient learning of sparse DAGs with a smaller search space compared to other popular approaches. The HT-CIT algorithm greatly reduces the number of edges that need to be pruned. Empirical results from synthetic and real-world datasets demonstrate the superiority of the proposed HT-CIT algorithm.
</details></li>
</ul>
<hr>
<h2 id="Online-Control-for-Linear-Dynamics-A-Data-Driven-Approach"><a href="#Online-Control-for-Linear-Dynamics-A-Data-Driven-Approach" class="headerlink" title="Online Control for Linear Dynamics: A Data-Driven Approach"></a>Online Control for Linear Dynamics: A Data-Driven Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08138">http://arxiv.org/abs/2308.08138</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zishun Liu, Yongxin Chen</li>
<li>for: 本文考虑了一个在线控制问题，其中系统动态不知道，干扰 bounded，并且存在敌对成本。</li>
<li>methods: 我们提出了一种数据驱动策略，以减少控制器的违和。与模型基于方法不同，我们的算法不需要identify系统模型，而是使用一个干净的轨迹来计算干扰的积累，并使用我们设计的积累干扰控制器来做决策，其参数通过在线梯度下降更新。</li>
<li>results: 我们证明了我们的算法的违和是$\mathcal{O}(\sqrt{T})$，这意味着其性能与模型基于方法相当。<details>
<summary>Abstract</summary>
This paper considers an online control problem over a linear time-invariant system with unknown dynamics, bounded disturbance, and adversarial cost. We propose a data-driven strategy to reduce the regret of the controller. Unlike model-based methods, our algorithm does not identify the system model, instead, it leverages a single noise-free trajectory to calculate the accumulation of disturbance and makes decisions using the accumulated disturbance action controller we design, whose parameters are updated by online gradient descent. We prove that the regret of our algorithm is $\mathcal{O}(\sqrt{T})$ under mild assumptions, suggesting that its performance is on par with model-based methods.
</details>
<details>
<summary>摘要</summary>
这篇论文研究了一个在线控制问题，其中系统为线性时间不变的系统，动力不确定、干扰bounded和敌意成本存在。我们提出了一种数据驱动策略，以减少控制器的后悔。不同于模型基于方法，我们的算法不需要确定系统模型，而是利用干扰自由的一个轨迹来计算干扰的积累，并使用我们设计的积累干扰控制器来做决策，该控制器的参数通过在线梯度下降更新。我们证明了我们的算法的后悔是 $\mathcal{O}(\sqrt{T})$ 的，这表明它的性能与模型基于方法相当。
</details></li>
</ul>
<hr>
<h2 id="Microstructure-Empowered-Stock-Factor-Extraction-and-Utilization"><a href="#Microstructure-Empowered-Stock-Factor-Extraction-and-Utilization" class="headerlink" title="Microstructure-Empowered Stock Factor Extraction and Utilization"></a>Microstructure-Empowered Stock Factor Extraction and Utilization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08135">http://arxiv.org/abs/2308.08135</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xianfeng Jiao, Zizhong Li, Chang Xu, Yang Liu, Weiqing Liu, Jiang Bian</li>
<li>for: 高频量股票投资是股票投资中的一个关键方面，order flow数据在这方面具有关键性，因为它提供了最详细的信息，包括全部的订单书和交易记录。</li>
<li>methods: 我们提出了一种新的框架，用于从order flow数据中提取有用的因素，并且可以在不同的粒度和enario下进行多种下游任务。我们的方法包括Context Encoder和Factor Extractor。Context Encoder使得当前订单流数据段的上下文得到嵌入，包括预期和实际市场状态。Factor Extractor使用不supervised学习方法，从订单流数据中选择最重要的信号，这些信号与大多数信号进行分割。</li>
<li>results: 我们的提出的框架可以高效处理一年的股票订单流数据，并且可以在不同的enario下应用。我们的方法可以提取更好的因素，从而提高股票趋势预测和订单执行任务的精度。<details>
<summary>Abstract</summary>
High-frequency quantitative investment is a crucial aspect of stock investment. Notably, order flow data plays a critical role as it provides the most detailed level of information among high-frequency trading data, including comprehensive data from the order book and transaction records at the tick level. The order flow data is extremely valuable for market analysis as it equips traders with essential insights for making informed decisions. However, extracting and effectively utilizing order flow data present challenges due to the large volume of data involved and the limitations of traditional factor mining techniques, which are primarily designed for coarser-level stock data. To address these challenges, we propose a novel framework that aims to effectively extract essential factors from order flow data for diverse downstream tasks across different granularities and scenarios. Our method consists of a Context Encoder and an Factor Extractor. The Context Encoder learns an embedding for the current order flow data segment's context by considering both the expected and actual market state. In addition, the Factor Extractor uses unsupervised learning methods to select such important signals that are most distinct from the majority within the given context. The extracted factors are then utilized for downstream tasks. In empirical studies, our proposed framework efficiently handles an entire year of stock order flow data across diverse scenarios, offering a broader range of applications compared to existing tick-level approaches that are limited to only a few days of stock data. We demonstrate that our method extracts superior factors from order flow data, enabling significant improvement for stock trend prediction and order execution tasks at the second and minute level.
</details>
<details>
<summary>摘要</summary>
高频量质投资是股票投资中一个关键方面。突出重要的是，订单流数据在高频投资中扮演了关键角色，因为它提供了最详细的信息，包括订单书和交易记录的细节。订单流数据对市场分析非常有价值，因为它为投资者提供了关键的信息，帮助他们做出了 Informed Decisions。然而，提取并有效利用订单流数据具有挑战，因为涉及的数据量很大，而且传统的因子挖掘技术主要针对粗细级股票数据。为解决这些挑战，我们提出了一种新的框架，旨在有效地从订单流数据中提取关键因子，用于不同的下游任务和不同的场景。我们的方法包括上下文编码器和因子挖掘器。上下文编码器通过考虑当前订单流数据段的预期和实际市场状况，学习订单流数据段的上下文嵌入。此外，因子挖掘器使用无监督学习方法，选择订单流数据中最为特异的信号，以便在给定的上下文中提取关键因子。提取的因子后续用于下游任务。我们的方法可以有效处理一年的股票订单流数据，在多种场景下提供更广泛的应用场景，与现有的tick级 approached有限制，只能处理几天的股票数据。我们的方法提取了订单流数据中的优秀因子，使得股票趋势预测和订单执行任务在秒和分级别得到了显著改进。
</details></li>
</ul>
<hr>
<h2 id="Is-Self-Supervised-Pretraining-Good-for-Extrapolation-in-Molecular-Property-Prediction"><a href="#Is-Self-Supervised-Pretraining-Good-for-Extrapolation-in-Molecular-Property-Prediction" class="headerlink" title="Is Self-Supervised Pretraining Good for Extrapolation in Molecular Property Prediction?"></a>Is Self-Supervised Pretraining Good for Extrapolation in Molecular Property Prediction?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08129">http://arxiv.org/abs/2308.08129</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shun Takashige, Masatoshi Hanai, Toyotaro Suzumura, Limin Wang, Kenjiro Taura</li>
<li>for: 本研究旨在探讨如何使用自动学习技术提高材料性能预测的准确性，特别是在材料性能预测中的推断问题上。</li>
<li>methods: 本研究使用了自动学习模型，首先通过自然语言处理方法对无标签数据进行自我预训练，然后对标签数据进行目标任务训练。</li>
<li>results: 研究发现，通过自我预训练，模型可以更好地捕捉材料的相对性质趋势，从而提高推断性能。<details>
<summary>Abstract</summary>
The prediction of material properties plays a crucial role in the development and discovery of materials in diverse applications, such as batteries, semiconductors, catalysts, and pharmaceuticals. Recently, there has been a growing interest in employing data-driven approaches by using machine learning technologies, in combination with conventional theoretical calculations. In material science, the prediction of unobserved values, commonly referred to as extrapolation, is particularly critical for property prediction as it enables researchers to gain insight into materials beyond the limits of available data. However, even with the recent advancements in powerful machine learning models, accurate extrapolation is still widely recognized as a significantly challenging problem. On the other hand, self-supervised pretraining is a machine learning technique where a model is first trained on unlabeled data using relatively simple pretext tasks before being trained on labeled data for target tasks. As self-supervised pretraining can effectively utilize material data without observed property values, it has the potential to improve the model's extrapolation ability. In this paper, we clarify how such self-supervised pretraining can enhance extrapolation performance.We propose an experimental framework for the demonstration and empirically reveal that while models were unable to accurately extrapolate absolute property values, self-supervised pretraining enables them to learn relative tendencies of unobserved property values and improve extrapolation performance.
</details>
<details>
<summary>摘要</summary>
Material 属性预测在材料发展和发现中扮演了关键角色，如电池、半导体、催化剂和药物等应用中。最近，有一个增长的兴趣是通过使用机器学习技术，与传统的理论计算相结合来使用数据驱动方法。在材料科学中，预测未观测值（extrapolation）是特别重要的，因为它允许研究人员对材料进行深入的研究，超出可用数据的限制。然而，即使最近的高能机器学习模型，准确的� extrapolation 仍被广泛认为是一个非常困难的问题。自我超vised pretraining 是一种机器学习技术，其中一个模型首先在无标签数据上使用相对简单的预文任务进行训练，然后在标签数据上进行目标任务的训练。由于自我超vised pretraining 可以充分利用材料数据不包含观测值，因此它有可能提高模型的� extrapolation 能力。在这篇文章中，我们解释了如何使用自我超vised pretraining 提高� extrapolation 性能。我们提出了一种实验框架，并经验表明，虽然模型无法准确地 extrapolate 绝对属性值，但自我超vised pretraining 使得它们学习了未观测值的相对趋势，并提高了� extrapolation 性能。
</details></li>
</ul>
<hr>
<h2 id="How-to-Mask-in-Error-Correction-Code-Transformer-Systematic-and-Double-Masking"><a href="#How-to-Mask-in-Error-Correction-Code-Transformer-Systematic-and-Double-Masking" class="headerlink" title="How to Mask in Error Correction Code Transformer: Systematic and Double Masking"></a>How to Mask in Error Correction Code Transformer: Systematic and Double Masking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08128">http://arxiv.org/abs/2308.08128</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seong-Joon Park, Hee-Youl Kwak, Sang-Hyo Kim, Sunghwan Kim, Yongjune Kim, Jong-Seon No</li>
<li>for: 这个论文主要是研究一种基于神经网络的编码器，以提高错误修复码（ECC）的性能。</li>
<li>methods: 这个论文提出了两种新的方法来提高错误修复码编码器（ECCT）的性能。其中一种是基于系统编码技术的新的面Masking矩阵，它可以提高性能并降低计算复杂度。另一种是一种新的双面Masked ECCT架构，它使用两个不同的面矩阵在并行方式中学习码word比特之间的关系。</li>
<li>results: 对于ECCT，提出的两种方法都得到了较好的效果。特别是，使用新的面Masking矩阵可以提高性能，而使用双面Masked ECCT架构可以学习更多的码word比特之间的关系，从而提高decoding性能。<details>
<summary>Abstract</summary>
In communication and storage systems, error correction codes (ECCs) are pivotal in ensuring data reliability. As deep learning's applicability has broadened across diverse domains, there is a growing research focus on neural network-based decoders that outperform traditional decoding algorithms. Among these neural decoders, Error Correction Code Transformer (ECCT) has achieved the state-of-the-art performance, outperforming other methods by large margins. To further enhance the performance of ECCT, we propose two novel methods. First, leveraging the systematic encoding technique of ECCs, we introduce a new masking matrix for ECCT, aiming to improve the performance and reduce the computational complexity. Second, we propose a novel transformer architecture of ECCT called a double-masked ECCT. This architecture employs two different mask matrices in a parallel manner to learn more diverse features of the relationship between codeword bits in the masked self-attention blocks. Extensive simulation results show that the proposed double-masked ECCT outperforms the conventional ECCT, achieving the state-of-the-art decoding performance with significant margins.
</details>
<details>
<summary>摘要</summary>
在通信和存储系统中，错误修复码（ECC）是确保数据可靠性的关键。随着深度学习在不同领域的应用积极扩大，关注 neural network 基于的解码算法在 ECC 中的研究也在不断增长。其中，Error Correction Code Transformer（ECCT）已经实现了最佳性能，比其他方法的性能优势较大。为了进一步提高 ECCT 的性能，我们提出了两种新的方法。首先，利用 ECC 的系统编码技术，我们引入了一个新的遮盲矩阵，以提高性能并降低计算复杂度。其次，我们提出了一种新的 ECCT 架构，即双遮盲 ECCT，该架构在并行方式中使用两个不同的遮盲矩阵来学习码word 位 bits 之间的更多多样性的关系。我们对 ECCT 进行了广泛的实验，结果显示，提出的双遮盲 ECCT 能够超越传统 ECCT，实现最佳解码性能，并且具有显著的性能优势。
</details></li>
</ul>
<hr>
<h2 id="S-Mixup-Structural-Mixup-for-Graph-Neural-Networks"><a href="#S-Mixup-Structural-Mixup-for-Graph-Neural-Networks" class="headerlink" title="S-Mixup: Structural Mixup for Graph Neural Networks"></a>S-Mixup: Structural Mixup for Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08097">http://arxiv.org/abs/2308.08097</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sukwonyun/s-mixup">https://github.com/sukwonyun/s-mixup</a></li>
<li>paper_authors: Junghurn Kim, Sukwon Yun, Chanyoung Park</li>
<li>for: 本研究探讨了应用mixup技术在图像上的扩展，尤其是在节点分类任务上。</li>
<li>methods: 本文提出了一种新的结构mixup（S-Mixup），具体来说是根据图像中的结构信息来混合节点。S-Mixup使用图像神经网络（GNN）分类器来获取 pseudo-标签 для没有标签的节点，并使用这些标签作为混合池的组合标准。此外，我们还提出了基于GNN训练的边梯度选择策略，用于选择与混合节点相连的边。</li>
<li>results: 经过广泛的实验表明，S-Mixup可以提高GNN的Robustness和泛化性，尤其是在不同类型的图像中。<details>
<summary>Abstract</summary>
Existing studies for applying the mixup technique on graphs mainly focus on graph classification tasks, while the research in node classification is still under-explored. In this paper, we propose a novel mixup augmentation for node classification called Structural Mixup (S-Mixup). The core idea is to take into account the structural information while mixing nodes. Specifically, S-Mixup obtains pseudo-labels for unlabeled nodes in a graph along with their prediction confidence via a Graph Neural Network (GNN) classifier. These serve as the criteria for the composition of the mixup pool for both inter and intra-class mixups. Furthermore, we utilize the edge gradient obtained from the GNN training and propose a gradient-based edge selection strategy for selecting edges to be attached to the nodes generated by the mixup. Through extensive experiments on real-world benchmark datasets, we demonstrate the effectiveness of S-Mixup evaluated on the node classification task. We observe that S-Mixup enhances the robustness and generalization performance of GNNs, especially in heterophilous situations. The source code of S-Mixup can be found at \url{https://github.com/SukwonYun/S-Mixup}
</details>
<details>
<summary>摘要</summary>
先前的研究主要集中在图像分类任务上应用mixup技术，而节点分类任务的研究仍然尚未得到充分的探索。在这篇论文中，我们提出了一种新的结构强化mixup修饰（S-Mixup）。其核心思想是在混合节点时考虑结构信息。具体来说，S-Mixup通过一个图 neural network（GNN）分类器获得未标注节点的 pseudo-标签和其预测信心。这些服务为混合池的组合的标准。此外，我们利用GNN训练中的边梯度并提出了一种基于边梯度的边选择策略，用于选择混合 Pool 中连接到生成的节点的边。经过了实际的实验，我们证明了S-Mixup可以提高GNN的可靠性和泛化性，特别是在不同类型的情况下。S-Mixup的源代码可以在 GitHub上找到：https://github.com/SukwonYun/S-Mixup。
</details></li>
</ul>
<hr>
<h2 id="Safety-Filter-Design-for-Neural-Network-Systems-via-Convex-Optimization"><a href="#Safety-Filter-Design-for-Neural-Network-Systems-via-Convex-Optimization" class="headerlink" title="Safety Filter Design for Neural Network Systems via Convex Optimization"></a>Safety Filter Design for Neural Network Systems via Convex Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08086">http://arxiv.org/abs/2308.08086</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shaoruchen/nn-system-psf">https://github.com/shaoruchen/nn-system-psf</a></li>
<li>paper_authors: Shaoru Chen, Kong Yao Chee, Nikolai Matni, M. Ani Hsieh, George J. Pappas</li>
<li>for: 这篇论文旨在提出一种基于凸优化的安全筛选方法，以确保神经网络系统在面对扰动时能够保持安全。</li>
<li>methods: 该方法首先使用神经网络验证工具来过度估算神经网络动态，然后通过稳定LPV搜索法找到一个能够保证约束满足的控制器。</li>
<li>results: 数值实验表明，该方法可以有效地确保神经网络系统在面对扰动时的安全性。<details>
<summary>Abstract</summary>
With the increase in data availability, it has been widely demonstrated that neural networks (NN) can capture complex system dynamics precisely in a data-driven manner. However, the architectural complexity and nonlinearity of the NNs make it challenging to synthesize a provably safe controller. In this work, we propose a novel safety filter that relies on convex optimization to ensure safety for a NN system, subject to additive disturbances that are capable of capturing modeling errors. Our approach leverages tools from NN verification to over-approximate NN dynamics with a set of linear bounds, followed by an application of robust linear MPC to search for controllers that can guarantee robust constraint satisfaction. We demonstrate the efficacy of the proposed framework numerically on a nonlinear pendulum system.
</details>
<details>
<summary>摘要</summary>
随着数据的增加，已经广泛证明了神经网络（NN）可以在数据驱动方式下准确捕捉复杂系统动态。然而，神经网络的建筑复杂性和非线性使得Synthesizing a provably safe controller是一项挑战。在这种情况下，我们提出了一种新的安全筛选器，该筛选器基于凸优化来确保神经网络系统的安全性，对于带有添加干扰的系统。我们的方法利用了神经网络验证工具来过度估算神经网络动态，然后通过Robust linear MPC来搜索能够保证约束满足的控制器。我们通过数值方法示出了我们的框架的有效性，并且在非线性挠杆系统上进行了实验。
</details></li>
</ul>
<hr>
<h2 id="Rigid-Transformations-for-Stabilized-Lower-Dimensional-Space-to-Support-Subsurface-Uncertainty-Quantification-and-Interpretation"><a href="#Rigid-Transformations-for-Stabilized-Lower-Dimensional-Space-to-Support-Subsurface-Uncertainty-Quantification-and-Interpretation" class="headerlink" title="Rigid Transformations for Stabilized Lower Dimensional Space to Support Subsurface Uncertainty Quantification and Interpretation"></a>Rigid Transformations for Stabilized Lower Dimensional Space to Support Subsurface Uncertainty Quantification and Interpretation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08079">http://arxiv.org/abs/2308.08079</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ademide O. Mabadeje, Michael J. Pyrcz<br>for:这篇论文主要是为了解决隐藏层数据的维度减少问题，以提高地球科学和能源资源工程中的数据分析和机器学习过程中的重复性和比较性。methods:这篇论文使用了非线性维度减少（NDR）方法，尤其是多元维度减少（MDS）方法，以处理隐藏层数据的维度减少问题。它们的缺点是存在不稳定的唯一解，而且无法扩展到外样点（OOSP）。为了解决这些问题，该论文提出了一种稳定的、包含OOSP的隐藏层数据表示方法。results:该论文通过使用固定变换、计算MDS输入不同性矩阵和应用多个实现来保证变换不变性，并将OOSP纳入到数据表示中。该方法使用了 convex hull 算法和loss函数和 норamlized stress 来衡量扭曲。与Synthetic数据、不同距离度量和实际的DUVERNAY formación 井井相比，结果证明了该方法的有效性。此外，提出的“压缩率”（SR）指标，可以提供有关不确定性的信息，对于数据分析和推理分析是有利的。因此，该工作流程提出了在NDR中提高重复性和比较性的可能性。<details>
<summary>Abstract</summary>
Subsurface datasets inherently possess big data characteristics such as vast volume, diverse features, and high sampling speeds, further compounded by the curse of dimensionality from various physical, engineering, and geological inputs. Among the existing dimensionality reduction (DR) methods, nonlinear dimensionality reduction (NDR) methods, especially Metric-multidimensional scaling (MDS), are preferred for subsurface datasets due to their inherent complexity. While MDS retains intrinsic data structure and quantifies uncertainty, its limitations include unstabilized unique solutions invariant to Euclidean transformations and an absence of out-of-sample points (OOSP) extension. To enhance subsurface inferential and machine learning workflows, datasets must be transformed into stable, reduced-dimension representations that accommodate OOSP.   Our solution employs rigid transformations for a stabilized Euclidean invariant representation for LDS. By computing an MDS input dissimilarity matrix, and applying rigid transformations on multiple realizations, we ensure transformation invariance and integrate OOSP. This process leverages a convex hull algorithm and incorporates loss function and normalized stress for distortion quantification. We validate our approach with synthetic data, varying distance metrics, and real-world wells from the Duvernay Formation. Results confirm our method's efficacy in achieving consistent LDS representations. Furthermore, our proposed "stress ratio" (SR) metric provides insight into uncertainty, beneficial for model adjustments and inferential analysis. Consequently, our workflow promises enhanced repeatability and comparability in NDR for subsurface energy resource engineering and associated big data workflows.
</details>
<details>
<summary>摘要</summary>
底层数据自然而有大数据特点，如庞大量、多样特征和高采样速率，这些特点更加受到物理、工程和地质输入的带来的诸多维度的咒语。现有的维度减少（DR）方法中，非线性维度减少（NDR）方法，尤其是多元维度减少（MDS），对底层数据进行处理是更为首选的，因为它们可以更好地处理底层数据的复杂性。然而，MDS存在两个缺点：一是无法保证唯一解，二是缺乏对外样点（OOSP）的扩展。为了提高底层数据的推断和机器学习工作流程，数据需要被转换成稳定、减少维度的表示，并且包含OOSP。我们的解决方案是使用固定变换来实现稳定的几何减少，并且在多个实现中计算MDS输入不同性矩阵，以确保变换不变性和包含OOSP。这个过程利用了 convex hull 算法和 incorporate 损失函数和正规化压力，以量化扭曲。我们通过使用synthetic data、不同的距离度量和实际的DUVERNAY  formación 井井来验证我们的方法，结果证明了我们的方法的有效性。此外，我们还提出了一个"stress ratio"（SR）指标，可以提供很好的 uncertainty 的视角，这有助于进行模型调整和推断分析。因此，我们的工作流程 promise 提高了NDR 的重复性和相对性，为底层数据的能源资源工程和相关的大数据工作流程提供了更好的支持。
</details></li>
</ul>
<hr>
<h2 id="Decentralized-Graph-Neural-Network-for-Privacy-Preserving-Recommendation"><a href="#Decentralized-Graph-Neural-Network-for-Privacy-Preserving-Recommendation" class="headerlink" title="Decentralized Graph Neural Network for Privacy-Preserving Recommendation"></a>Decentralized Graph Neural Network for Privacy-Preserving Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08072">http://arxiv.org/abs/2308.08072</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaolin Zheng, Zhongyu Wang, Chaochao Chen, Jiashu Qian, Yao Yang</li>
<li>for: This paper aims to build a privacy-preserving graph neural network (GNN) based recommender system without violating user privacy.</li>
<li>methods: The proposed method, called DGREC, includes three stages: graph construction, local gradient calculation, and global gradient passing. It uses a local differential privacy mechanism called secure gradient-sharing to protect users’ private data.</li>
<li>results: The authors conduct extensive experiments on three public datasets and show that DGREC achieves consistent superiority over existing methods in terms of accuracy and privacy protection.Here’s the simplified Chinese version:</li>
<li>for: 这篇论文目标是构建一个遵循用户隐私的图内存神经网络（GNN）基于推荐系统，不会违反用户的隐私。</li>
<li>methods: 该提议的方法包括三个阶段：图构建、本地梯度计算和全局梯度传递。它使用一种名为安全梯度分享的本地异质隐私机制来保护用户的私人数据。</li>
<li>results: 作者们对三个公共数据集进行了广泛的实验，并证明了 DGREC 在精度和隐私保护方面与现有方法具有一致的优越性。<details>
<summary>Abstract</summary>
Building a graph neural network (GNN)-based recommender system without violating user privacy proves challenging. Existing methods can be divided into federated GNNs and decentralized GNNs. But both methods have undesirable effects, i.e., low communication efficiency and privacy leakage. This paper proposes DGREC, a novel decentralized GNN for privacy-preserving recommendations, where users can choose to publicize their interactions. It includes three stages, i.e., graph construction, local gradient calculation, and global gradient passing. The first stage builds a local inner-item hypergraph for each user and a global inter-user graph. The second stage models user preference and calculates gradients on each local device. The third stage designs a local differential privacy mechanism named secure gradient-sharing, which proves strong privacy-preserving of users' private data. We conduct extensive experiments on three public datasets to validate the consistent superiority of our framework.
</details>
<details>
<summary>摘要</summary>
建立一个基于图神经网络（GNN）的推荐系统，保持用户隐私具有挑战性。现有方法可以分为联邦GNN和分散GNN两种。但两种方法都带有不 DESirable Effects，即通信效率低和隐私泄露。这篇论文提出了DGREC，一种新的分散GNN，用户可以选择公开自己的互动记录。它包括三个阶段：图建构、本地梯度计算和全球梯度传递。第一阶段建立了每个用户的本地内部项目图和全球用户图。第二阶段模型用户偏好并在每个本地设备上计算梯度。第三阶段实现了一种安全梯度分享机制，以保障用户隐私数据的强大隐私。我们在三个公共数据集上进行了广泛的实验，以验证我们的框架的一致性和优越性。
</details></li>
</ul>
<hr>
<h2 id="Freshness-or-Accuracy-Why-Not-Both-Addressing-Delayed-Feedback-via-Dynamic-Graph-Neural-Networks"><a href="#Freshness-or-Accuracy-Why-Not-Both-Addressing-Delayed-Feedback-via-Dynamic-Graph-Neural-Networks" class="headerlink" title="Freshness or Accuracy, Why Not Both? Addressing Delayed Feedback via Dynamic Graph Neural Networks"></a>Freshness or Accuracy, Why Not Both? Addressing Delayed Feedback via Dynamic Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08071">http://arxiv.org/abs/2308.08071</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaolin Zheng, Zhongyu Wang, Chaochao Chen, Feng Zhu, Jiashu Qian</li>
<li>for: 预测在线购物系统中用户的购买率，解决延迟反馈问题。</li>
<li>methods: 使用动态图 neural network 模型，包括数据预处理、建立动态图和训练 CVR 预测模型。在模型训练中，我们提出了一种新的图 convolutional 方法 named HLGCN，可以处理 conversion 和 non-conversion 关系。</li>
<li>results: 在三个行业数据集上进行了广泛的实验 validate 了我们的方法的一致优势。<details>
<summary>Abstract</summary>
The delayed feedback problem is one of the most pressing challenges in predicting the conversion rate since users' conversions are always delayed in online commercial systems. Although new data are beneficial for continuous training, without complete feedback information, i.e., conversion labels, training algorithms may suffer from overwhelming fake negatives. Existing methods tend to use multitask learning or design data pipelines to solve the delayed feedback problem. However, these methods have a trade-off between data freshness and label accuracy. In this paper, we propose Delayed Feedback Modeling by Dynamic Graph Neural Network (DGDFEM). It includes three stages, i.e., preparing a data pipeline, building a dynamic graph, and training a CVR prediction model. In the model training, we propose a novel graph convolutional method named HLGCN, which leverages both high-pass and low-pass filters to deal with conversion and non-conversion relationships. The proposed method achieves both data freshness and label accuracy. We conduct extensive experiments on three industry datasets, which validate the consistent superiority of our method.
</details>
<details>
<summary>摘要</summary>
延迟反馈问题是在线商业系统中预测转化率的最大挑战之一，因为用户的转化都会延迟。新的数据对于连续训练是有利，但是无完整的转化标签，训练算法可能会受到干扰性的假负样本的影响。现有方法通常使用多任务学习或设计数据管道来解决延迟反馈问题，但这些方法存在数据新鲜度和标签准确性之间的负担。本文提出了延迟反馈模型化方法（DGDFEM），包括三个阶段：准备数据管道、建立动态图和训练CVR预测模型。在模型训练中，我们提出了一种新的图解决方法 named HLGCN，它利用高频和低频滤波器来处理转化和非转化关系。提出的方法同时保证数据新鲜度和标签准确性。我们对三个行业数据集进行了广泛的实验， validate了我们的方法的一致性优势。
</details></li>
</ul>
<hr>
<h2 id="Max-affine-regression-via-first-order-methods"><a href="#Max-affine-regression-via-first-order-methods" class="headerlink" title="Max-affine regression via first-order methods"></a>Max-affine regression via first-order methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08070">http://arxiv.org/abs/2308.08070</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seonho Kim, Kiryung Lee</li>
<li>for: 这篇论文主要关注于 max-affine 模型的回归分析，尤其是在测量变量随机分布和反对对应下进行 gradient descent 和 mini-batch stochastic gradient descent 的非对称渐近分析。</li>
<li>methods: 论文使用了 gradient descent 和 mini-batch stochastic gradient descent，并进行了非对称渐近分析。</li>
<li>results: 论文发现，在随机分布和反对对应下，适当初始化的 gradient descent 和 mini-batch stochastic gradient descent 会在适当的错误范围内线性收摄到解。此外，在噪音存在的低样本案例中，SGD 不仅快速执行时间，还超过了对称降低和 gradient descent 的性能。<details>
<summary>Abstract</summary>
We consider regression of a max-affine model that produces a piecewise linear model by combining affine models via the max function. The max-affine model ubiquitously arises in applications in signal processing and statistics including multiclass classification, auction problems, and convex regression. It also generalizes phase retrieval and learning rectifier linear unit activation functions. We present a non-asymptotic convergence analysis of gradient descent (GD) and mini-batch stochastic gradient descent (SGD) for max-affine regression when the model is observed at random locations following the sub-Gaussianity and an anti-concentration with additive sub-Gaussian noise. Under these assumptions, a suitably initialized GD and SGD converge linearly to a neighborhood of the ground truth specified by the corresponding error bound. We provide numerical results that corroborate the theoretical finding. Importantly, SGD not only converges faster in run time with fewer observations than alternating minimization and GD in the noiseless scenario but also outperforms them in low-sample scenarios with noise.
</details>
<details>
<summary>摘要</summary>
我们考虑一个最大拟合模型，它生成一个分割线性模型，通过最大函数将多个拟合模型相加。这种最大拟合模型在信号处理和统计应用中广泛存在，包括多类分类、拍卖问题和凸回归。它还泛化了phas Retrieval和学习矩阵减法。我们提供了非 asymptotic 的收敛分析，证明了梯度下降（GD）和批处理随机梯度下降（SGD）在最大拟合 regression 中的收敛性。在这些假设下，一个适当的初始化GD和SGD会 linearly收敛到一个包含真实值的邻域，以至于这个邻域的错误 bound。我们提供了数字结果，证明了理论发现。此外，SGD不仅在干擦无噪scenario下更快地收敛，也在低样本 scenarios 中超越了交互式最优化和GD。
</details></li>
</ul>
<hr>
<h2 id="A-Reinforcement-Learning-Approach-for-Performance-aware-Reduction-in-Power-Consumption-of-Data-Center-Compute-Nodes"><a href="#A-Reinforcement-Learning-Approach-for-Performance-aware-Reduction-in-Power-Consumption-of-Data-Center-Compute-Nodes" class="headerlink" title="A Reinforcement Learning Approach for Performance-aware Reduction in Power Consumption of Data Center Compute Nodes"></a>A Reinforcement Learning Approach for Performance-aware Reduction in Power Consumption of Data Center Compute Nodes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08069">http://arxiv.org/abs/2308.08069</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/akhileshraj91/generalized_rl_anl">https://github.com/akhileshraj91/generalized_rl_anl</a></li>
<li>paper_authors: Akhilesh Raj, Swann Perarnau, Aniruddha Gokhale</li>
<li>for: 这个论文的目的是设计一种基于Reinforcement Learning（RL）的云计算节点资源控制策略，以减少计算节点的能源消耗。</li>
<li>methods: 该论文使用了Reinforcement Learning（RL）技术，使用当前电力消耗和协调器性能（heartbeats）的观察值，设计了一种可以在实际硬件上运行的最佳策略。</li>
<li>results: 该论文通过使用Argo Node Resource Management（NRM）软件栈和Intel Running Average Power Limit（RAPL）硬件控制机制，设计了一种可以控制计算节点的最大供应电力，不会妨碍应用程序性能。通过使用STREAM benchmark进行评估，表明训练了一个RL代理可以在实际硬件上进行动作，均衡电力消耗和应用程序性能。<details>
<summary>Abstract</summary>
As Exascale computing becomes a reality, the energy needs of compute nodes in cloud data centers will continue to grow. A common approach to reducing this energy demand is to limit the power consumption of hardware components when workloads are experiencing bottlenecks elsewhere in the system. However, designing a resource controller capable of detecting and limiting power consumption on-the-fly is a complex issue and can also adversely impact application performance. In this paper, we explore the use of Reinforcement Learning (RL) to design a power capping policy on cloud compute nodes using observations on current power consumption and instantaneous application performance (heartbeats). By leveraging the Argo Node Resource Management (NRM) software stack in conjunction with the Intel Running Average Power Limit (RAPL) hardware control mechanism, we design an agent to control the maximum supplied power to processors without compromising on application performance. Employing a Proximal Policy Optimization (PPO) agent to learn an optimal policy on a mathematical model of the compute nodes, we demonstrate and evaluate using the STREAM benchmark how a trained agent running on actual hardware can take actions by balancing power consumption and application performance.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:随着存储计算 becoming a reality, 云数据中心的计算节点的能源需求将继续增长。一种常见的方法是在工作负荷经历瓶颈时限制硬件组件的能源消耗。然而，在实时 detection 和限制能源消耗的设计是一个复杂的问题，可能会影响应用程序性能。在这篇论文中，我们explore使用强化学习（RL）设计云计算节点的能源帽策略，使用当前的电力消耗和应用程序性能（心跳） Observations。通过利用 Argo 节点资源管理（NRM）软件栈和Intel 运行平均电力限制（RAPL）硬件控制机制，我们设计了一个控制器来限制计算节点的最大供应电力，而不会影响应用程序性能。我们使用 PPO 代理来学习一个最佳策略，并在实际硬件上运行。使用 STREAM benchmark，我们 demonstate 和评估一个训练过的代理可以通过平衡电力消耗和应用程序性能来取得行动。
</details></li>
</ul>
<hr>
<h2 id="The-Costly-Dilemma-Generalization-Evaluation-and-Cost-Optimal-Deployment-of-Large-Language-Models"><a href="#The-Costly-Dilemma-Generalization-Evaluation-and-Cost-Optimal-Deployment-of-Large-Language-Models" class="headerlink" title="The Costly Dilemma: Generalization, Evaluation and Cost-Optimal Deployment of Large Language Models"></a>The Costly Dilemma: Generalization, Evaluation and Cost-Optimal Deployment of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08061">http://arxiv.org/abs/2308.08061</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abi Aryan, Aakash Kumar Nain, Andrew McMahon, Lucas Augusto Meyer, Harpreet Singh Sahota</li>
<li>for: 这篇论文是为了探讨大自然语言模型在生产环境中的应用和管理。</li>
<li>methods: 论文提出了一个横跨三个目标（即通用性、评估和成本优化）的框架，用于评估和优化大自然语言模型在生产环境中的性能和成本。</li>
<li>results: 论文表明，这个框架可以帮助企业对大自然语言模型进行评估和优化，以减少投资的成本和增加效率。<details>
<summary>Abstract</summary>
When deploying machine learning models in production for any product/application, there are three properties that are commonly desired. First, the models should be generalizable, in that we can extend it to further use cases as our knowledge of the domain area develops. Second they should be evaluable, so that there are clear metrics for performance and the calculation of those metrics in production settings are feasible. Finally, the deployment should be cost-optimal as far as possible. In this paper we propose that these three objectives (i.e. generalization, evaluation and cost-optimality) can often be relatively orthogonal and that for large language models, despite their performance over conventional NLP models, enterprises need to carefully assess all the three factors before making substantial investments in this technology. We propose a framework for generalization, evaluation and cost-modeling specifically tailored to large language models, offering insights into the intricacies of development, deployment and management for these large language models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>模型应该是可扩展的，以便在我们对领域知识的发展中可以进一步应用。2. 模型应该可评估，以便有明确的表现指标和在生产环境中可行的计算表现指标。3. 部署应该是最优化的，尽可能保持成本低。在这篇论文中，我们提出这三个目标（即泛化、评估和成本优化）通常是相互独立的，并且对大语言模型来说，企业需要仔细评估这三个因素才能够做出大投资。我们提出了特制的泛化、评估和成本模型，以便更好地发展、部署和管理这些大语言模型。</details></li>
</ol>
<hr>
<h2 id="Robust-Bayesian-Tensor-Factorization-with-Zero-Inflated-Poisson-Model-and-Consensus-Aggregation"><a href="#Robust-Bayesian-Tensor-Factorization-with-Zero-Inflated-Poisson-Model-and-Consensus-Aggregation" class="headerlink" title="Robust Bayesian Tensor Factorization with Zero-Inflated Poisson Model and Consensus Aggregation"></a>Robust Bayesian Tensor Factorization with Zero-Inflated Poisson Model and Consensus Aggregation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08060">http://arxiv.org/abs/2308.08060</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/klarman-cell-observatory/scbtf_experiments">https://github.com/klarman-cell-observatory/scbtf_experiments</a></li>
<li>paper_authors: Daniel Chafamo, Vignesh Shanmugam, Neriman Tokcan</li>
<li>for: 这篇论文是为了解决高维度计数数据中异常值带来的挑战，提出了一种基于 zero-inflated Poisson tensor factorization（ZIPTF）的新方法。</li>
<li>methods: 这篇论文使用了一种基于 maximum likelihood estimation的经典TF方法，但是这些方法在应用于单元RNA-seq数据时表现不佳。在解决随机性问题方面，这篇论文提出了一种基于consensus的zero-inflated Poisson tensor factorization（C-ZIPTF）方法。</li>
<li>results: 对于 sintetic zero-inflated count data和real single-cell RNA-seq数据，ZIPTF和C-ZIPTF都能够高效地重建知识图和生物 significative的表达程序。ZIPTF在probability of excess zeros高时能够达到2.4倍的准确率提高。此外，C-ZIPTF可以大幅提高重建精度和一致性。<details>
<summary>Abstract</summary>
Tensor factorizations (TF) are powerful tools for the efficient representation and analysis of multidimensional data. However, classic TF methods based on maximum likelihood estimation underperform when applied to zero-inflated count data, such as single-cell RNA sequencing (scRNA-seq) data. Additionally, the stochasticity inherent in TFs results in factors that vary across repeated runs, making interpretation and reproducibility of the results challenging. In this paper, we introduce Zero Inflated Poisson Tensor Factorization (ZIPTF), a novel approach for the factorization of high-dimensional count data with excess zeros. To address the challenge of stochasticity, we introduce Consensus Zero Inflated Poisson Tensor Factorization (C-ZIPTF), which combines ZIPTF with a consensus-based meta-analysis. We evaluate our proposed ZIPTF and C-ZIPTF on synthetic zero-inflated count data and synthetic and real scRNA-seq data. ZIPTF consistently outperforms baseline matrix and tensor factorization methods in terms of reconstruction accuracy for zero-inflated data. When the probability of excess zeros is high, ZIPTF achieves up to $2.4\times$ better accuracy. Additionally, C-ZIPTF significantly improves the consistency and accuracy of the factorization. When tested on both synthetic and real scRNA-seq data, ZIPTF and C-ZIPTF consistently recover known and biologically meaningful gene expression programs.
</details>
<details>
<summary>摘要</summary>
tensor化工具 (TF) 是一种强大的数据表示和分析工具，但 классические TF 方法基于最大化可能性估计在应用于零含量计数数据时表现不佳，如单个细胞 RNA 测序 (scRNA-seq) 数据。此外，TF 中的随机性使得因素在重复运行中变化，从而使得结果的解释和重现困难。在这篇论文中，我们介绍了 Zero Inflated Poisson Tensor Factorization (ZIPTF)，一种用于高维计数数据中的零含量的因素化方法。为了解决随机性的挑战，我们引入了 Consensus Zero Inflated Poisson Tensor Factorization (C-ZIPTF)，它将 ZIPTF 与 consensus-based 元分析结合。我们对 ZIPTF 和 C-ZIPTF 在 sintetic zero-inflated count data 和 sintetic 和实际 scRNA-seq data 上进行评估。ZIPTF 在零含量数据上的重建精度与基线矩阵和矩阵因素化方法相比，表现出了明显的优势。当零含量的概率高时，ZIPTF 的精度可以达到 2.4 倍。此外，C-ZIPTF 可以有效地提高因素化的一致性和精度。当测试在 sintetic 和实际 scRNA-seq data 上时，ZIPTF 和 C-ZIPTF 一致地回归了知道的和生物学意义的基因表达程序。
</details></li>
</ul>
<hr>
<h2 id="Simple-online-learning-with-consistency-oracle"><a href="#Simple-online-learning-with-consistency-oracle" class="headerlink" title="Simple online learning with consistency oracle"></a>Simple online learning with consistency oracle</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08055">http://arxiv.org/abs/2308.08055</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander Kozachinskiy, Tomasz Steifer</li>
<li>for: 这个论文是关于在具有一个一致性 oracle 的模型下进行在线学习的。这个模型是由 Assos et al. (COLT’23) 最近提出的，它是由于标准的在线学习方法 rely on 计算 Littlestone 维度的问题，这是计算 tractable 的。</li>
<li>methods: 这个论文提出了一种新的在线学习算法，该算法可以在类的 Littlestone 维度为 $d$ 时最多出错 $O(256^d)$ 次。我们的证明比 Assos et al. 的证明更简单，只需要使用了一些基本的 Littlestone 维度的性质。</li>
<li>results: 这个论文的结果包括两点：首先，我们提供了一种可计算的在线学习算法，该算法可以解决一个开放的问题，即是每个有限 Littlestone 维度的类都存在可计算的在线学习算法。其次，我们证明了存在一个不可 counts 的类，即是任何类的 Littlestone 维度不小于 $2^{d+1}-2$ 时，不可能有一个可计算的在线学习算法。<details>
<summary>Abstract</summary>
We consider online learning in the model where a learning algorithm can access the class only via the consistency oracle -- an oracle, that, at any moment, can give a function from the class that agrees with all examples seen so far. This model was recently considered by Assos et al. (COLT'23). It is motivated by the fact that standard methods of online learning rely on computing the Littlestone dimension of subclasses, a problem that is computationally intractable. Assos et al. gave an online learning algorithm in this model that makes at most $C^d$ mistakes on classes of Littlestone dimension $d$, for some absolute unspecified constant $C > 0$. We give a novel algorithm that makes at most $O(256^d)$ mistakes. Our proof is significantly simpler and uses only very basic properties of the Littlestone dimension. We also observe that there exists no algorithm in this model that makes at most $2^{d+1}-2$ mistakes. We also observe that our algorithm (as well as the algorithm of Assos et al.) solves an open problem by Hasrati and Ben-David (ALT'23). Namely, it demonstrates that every class of finite Littlestone dimension with recursively enumerable representation admits a computable online learner (that may be undefined on unrealizable samples).
</details>
<details>
<summary>摘要</summary>
我们考虑在模型中使用线上学习，其中学习算法可以通过一个具有一致性 oracle 访问 клаス。这个模型最近在 Assos 等人（COLT'23）中被考虑过。这个模型的动机是由于标准的线上学习方法需要计算 Littlestone 次数，这是 computationally intractable 的问题。Assos 等人提供了一个线上学习算法，它在类的 Littlestone 次数为 d 时会 maken at most C^d 的错误，其中 C 是一个未知的绝对常数。我们提供了一个新的算法，它在类的 Littlestone 次数为 d 时会 maken at most O(256^d) 的错误。我们的证明比较简单，只需要使用类的 Littlestone 次数的非常基本的性质。我们还观察到，不存在任何算法可以在类的 Littlestone 次数为 d 时 maken at most 2^(d+1)-2 的错误。此外，我们的算法（以及 Assos 等人的算法）解决了 Hasrati 和 Ben-David（ALT'23）的开问题。具体而言，它证明了每个有质量的类都存在可计算的线上学习器（可能是未定义的在不可能的测试样本上）。
</details></li>
</ul>
<hr>
<h2 id="Natural-Evolution-Strategies-as-a-Black-Box-Estimator-for-Stochastic-Variational-Inference"><a href="#Natural-Evolution-Strategies-as-a-Black-Box-Estimator-for-Stochastic-Variational-Inference" class="headerlink" title="Natural Evolution Strategies as a Black Box Estimator for Stochastic Variational Inference"></a>Natural Evolution Strategies as a Black Box Estimator for Stochastic Variational Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08053">http://arxiv.org/abs/2308.08053</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmad Ayaz Amin</li>
<li>for: 用于替代Variational Autoencoders（VAE）中的梯度估计，以解决VAE中的一种设计选择（即重parameterization trick）限制了模型的类型。</li>
<li>methods: 使用自然演化策略来提供一种不假设 distribuition 类型的 estimator，allowing for the creation of models that would otherwise not have been possible under the VAE framework。</li>
<li>results: 提出了一种不受 VAE 设计选择限制的模型创建方法，allowing for the creation of more diverse and complex models.<details>
<summary>Abstract</summary>
Stochastic variational inference and its derivatives in the form of variational autoencoders enjoy the ability to perform Bayesian inference on large datasets in an efficient manner. However, performing inference with a VAE requires a certain design choice (i.e. reparameterization trick) to allow unbiased and low variance gradient estimation, restricting the types of models that can be created. To overcome this challenge, an alternative estimator based on natural evolution strategies is proposed. This estimator does not make assumptions about the kind of distributions used, allowing for the creation of models that would otherwise not have been possible under the VAE framework.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:Stochastic variational inference和其 derivatives的形式为variational autoencoders (VAE)可以高效地进行 bayesian inference on large datasets。然而，使用 VAE 进行 inference 需要特定的设计选择（即 reparameterization trick）以确保无偏度和低差异的梯度估计，这限制了可以创建的模型类型。为了解决这个挑战，一种基于自然进化策略的替代估计器被提议。这种估计器不会假设分布的类型，因此可以创建 VAE 框架下不可能创建的模型。
</details></li>
</ul>
<hr>
<h2 id="Unbiased-Decisions-Reduce-Regret-Adversarial-Domain-Adaptation-for-the-Bank-Loan-Problem"><a href="#Unbiased-Decisions-Reduce-Regret-Adversarial-Domain-Adaptation-for-the-Bank-Loan-Problem" class="headerlink" title="Unbiased Decisions Reduce Regret: Adversarial Domain Adaptation for the Bank Loan Problem"></a>Unbiased Decisions Reduce Regret: Adversarial Domain Adaptation for the Bank Loan Problem</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08051">http://arxiv.org/abs/2308.08051</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elena Gal, Shaun Singh, Aldo Pacchiano, Ben Walker, Terry Lyons, Jakob Foerster</li>
<li>for: 本研究旨在 Addressing bias in binary classification decisions based on limited data in near real-time, particularly in scenarios where the true label is only observed when a data point is assigned a positive label.</li>
<li>methods: 该研究提出了一种新的方法 called adversarial optimism (AdOpt), which uses adversarial domain adaptation to directly address bias in the training set and learn an unbiased but informative representation of past data.</li>
<li>results: 对一组Difficult benchmark problems, AdOpt 能够显著超过州Of-the-art表现，并且初步证明了该方法在这种设定中改善公平性。<details>
<summary>Abstract</summary>
In many real world settings binary classification decisions are made based on limited data in near real-time, e.g. when assessing a loan application. We focus on a class of these problems that share a common feature: the true label is only observed when a data point is assigned a positive label by the principal, e.g. we only find out whether an applicant defaults if we accepted their loan application. As a consequence, the false rejections become self-reinforcing and cause the labelled training set, that is being continuously updated by the model decisions, to accumulate bias. Prior work mitigates this effect by injecting optimism into the model, however this comes at the cost of increased false acceptance rate. We introduce adversarial optimism (AdOpt) to directly address bias in the training set using adversarial domain adaptation. The goal of AdOpt is to learn an unbiased but informative representation of past data, by reducing the distributional shift between the set of accepted data points and all data points seen thus far. AdOpt significantly exceeds state-of-the-art performance on a set of challenging benchmark problems. Our experiments also provide initial evidence that the introduction of adversarial domain adaptation improves fairness in this setting.
</details>
<details>
<summary>摘要</summary>
在许多实际场景中，二进制分类决策基于有限数据进行实时进行，例如审批贷款申请。我们关注一类这些问题，它们共同特点是：真正的标签只有当数据点被主体分配正确标签时才可以见到，例如只有当我们接受了贷款申请后才能确定应用者是否 defaults。这导致假拒绝被自我强化，从而使标记训练集，由模型决策而不断更新的集合，受到偏见。先前的工作通过在模型中注入乐观性来 mitigate这种效应，但这会导致准确批准率上升。我们引入对抗优化（AdOpt），直接通过对抗领域适应来减少标记训练集中的偏见。AdOpt的目标是学习不偏的， yet informative 的过去数据表示，通过减少接受数据点和所有见过的数据点之间的分布差异。AdOpt在一组具有挑战性的benchmark问题上表现出色，我们的实验也提供了初步证据，表明在这种设置中，对抗领域适应可以提高公平性。
</details></li>
</ul>
<hr>
<h2 id="Regret-Lower-Bounds-in-Multi-agent-Multi-armed-Bandit"><a href="#Regret-Lower-Bounds-in-Multi-agent-Multi-armed-Bandit" class="headerlink" title="Regret Lower Bounds in Multi-agent Multi-armed Bandit"></a>Regret Lower Bounds in Multi-agent Multi-armed Bandit</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08046">http://arxiv.org/abs/2308.08046</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mengfan Xu, Diego Klabjan</li>
<li>for: 本文研究了多客户端多臂抽筋问题，即每个客户端面临着一个分布式抽筋问题，总体系统性能被评估为尝验 regret。</li>
<li>methods: 本文使用了多种方法，包括贪婪启发、决策规则、搜索等，以实现高效的启发和减少 regret。</li>
<li>results: 本文提供了多种情况下的 regret 下界，包括对各种设定下的 Connectivity 和奖励分布的研究。Specifically, 当图表现good connectivity properties和奖励是随机分布时，我们证明了下界为O（log T）的instance-dependent bounds和$\sqrt{T}$的mean-gap independent bounds，并证明了其紧张性。在对抗奖励下，我们建立了下界为O(T^{2&#x2F;3})的连接图下界，这与之前的上界相匹配。此外，当图是离散的时，我们证明了线性的下界。相比之前的研究，本文提供了严格的下界研究。<details>
<summary>Abstract</summary>
Multi-armed Bandit motivates methods with provable upper bounds on regret and also the counterpart lower bounds have been extensively studied in this context. Recently, Multi-agent Multi-armed Bandit has gained significant traction in various domains, where individual clients face bandit problems in a distributed manner and the objective is the overall system performance, typically measured by regret. While efficient algorithms with regret upper bounds have emerged, limited attention has been given to the corresponding regret lower bounds, except for a recent lower bound for adversarial settings, which, however, has a gap with let known upper bounds. To this end, we herein provide the first comprehensive study on regret lower bounds across different settings and establish their tightness. Specifically, when the graphs exhibit good connectivity properties and the rewards are stochastically distributed, we demonstrate a lower bound of order $O(\log T)$ for instance-dependent bounds and $\sqrt{T}$ for mean-gap independent bounds which are tight. Assuming adversarial rewards, we establish a lower bound $O(T^{\frac{2}{3}})$ for connected graphs, thereby bridging the gap between the lower and upper bound in the prior work. We also show a linear regret lower bound when the graph is disconnected. While previous works have explored these settings with upper bounds, we provide a thorough study on tight lower bounds.
</details>
<details>
<summary>摘要</summary>
多臂弓箭刺激方法的研究已有证明的最高 regret 上界和对应的下界也得到了广泛的研究。在这个上下文中，最近的多智能体多臂弓箭问题已经在不同领域得到了广泛的应用，其中每个客户面临着分布式的弓箭问题，并且目标是总系统性能，通常由 regret 来度量。虽然有效的算法得到了提出，但对应的 regret 下界却受到了有限的关注，除了最近的对抗性下界，其中 however 有一定的差距。为了解决这个问题，我们在这里提供了首次的全面的下界研究，并证明其紧耦合。具体来说，当图表现出良好的连接性和奖励是随机分布的时候，我们示出了一个下界为 $O(\log T)$ 的实例依赖下界和 $ \sqrt{T} $ 的无关下界，这些下界都是紧耦合的。在对抗性奖励下，我们确立了一个下界为 $O(T^{2/3})$ 的连接图下界，因此bridging了之前的下界和上界之间的差距。此外，当图为离散图时，我们还证明了一个线性的下界。相比之前的研究，我们在这里提供了一个全面的下界研究。
</details></li>
</ul>
<hr>
<h2 id="A-Comparative-Analysis-of-the-Capabilities-of-Nature-inspired-Feature-Selection-Algorithms-in-Predicting-Student-Performance"><a href="#A-Comparative-Analysis-of-the-Capabilities-of-Nature-inspired-Feature-Selection-Algorithms-in-Predicting-Student-Performance" class="headerlink" title="A Comparative Analysis of the Capabilities of Nature-inspired Feature Selection Algorithms in Predicting Student Performance"></a>A Comparative Analysis of the Capabilities of Nature-inspired Feature Selection Algorithms in Predicting Student Performance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08574">http://arxiv.org/abs/2308.08574</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas Trask</li>
<li>for: 预测学生表现，以便采取有效的预warning措施对于受风险学生。</li>
<li>methods: 使用12种自然引导的算法来预测学生表现，并对3个数据集进行分析，包括单个课程表现、多门课程表现和点播数据。</li>
<li>results: 结果显示，无论是哪个数据集，都可以通过结合NIAs进行特征选择和传统机器学习算法进行分类，提高预测精度，同时减少特征集大小by 2&#x2F;3。<details>
<summary>Abstract</summary>
Predicting student performance is key in leveraging effective pre-failure interventions for at-risk students. In this paper, I have analyzed the relative performance of a suite of 12 nature-inspired algorithms when used to predict student performance across 3 datasets consisting of instance-based clickstream data, intra-course single-course performance, and performance when taking multiple courses simultaneously. I found that, for all datasets, leveraging an ensemble approach using NIAs for feature selection and traditional ML algorithms for classification increased predictive accuracy while also reducing feature set size by 2/3.
</details>
<details>
<summary>摘要</summary>
预测学生表现是键在实施有效预测失败学生之前的干预措施中。在这篇论文中，我分析了12种自然指导算法的相对性，当用于预测学生表现 across 3个数据集，包括单个实例流量数据、单个课程表现和同时攻击多门课程表现。我发现，对于所有数据集，使用NIAs进行特征选择和传统机器学习算法进行分类可以提高预测精度，同时减少特征集的大小 by 2/3。
</details></li>
</ul>
<hr>
<h2 id="Classification-of-Data-Generated-by-Gaussian-Mixture-Models-Using-Deep-ReLU-Networks"><a href="#Classification-of-Data-Generated-by-Gaussian-Mixture-Models-Using-Deep-ReLU-Networks" class="headerlink" title="Classification of Data Generated by Gaussian Mixture Models Using Deep ReLU Networks"></a>Classification of Data Generated by Gaussian Mixture Models Using Deep ReLU Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08030">http://arxiv.org/abs/2308.08030</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tian-Yi Zhou, Xiaoming Huo</li>
<li>for: 这个论文研究了使用深度ReLU神经网络进行二分类问题中的数据从 ${\mathbb R}^d$ 生成的 Gaussian Mixture Models (GMMs) 的极限风险误差。</li>
<li>methods: 我们提供了非对易Bounds和抽象率的极限风险误差的非准确上界，这些上界不依赖于维度 $d$，表明深度ReLU网络可以缓解维度约束的味道。</li>
<li>results: 我们的结果表明，使用深度ReLU网络进行二分类问题中的数据从 ${\mathbb R}^d$ 生成的 Gaussian Mixture Models (GMMs) 可以减少误差，并且不受维度 $d$ 的影响。<details>
<summary>Abstract</summary>
This paper studies the binary classification of unbounded data from ${\mathbb R}^d$ generated under Gaussian Mixture Models (GMMs) using deep ReLU neural networks. We obtain $\unicode{x2013}$ for the first time $\unicode{x2013}$ non-asymptotic upper bounds and convergence rates of the excess risk (excess misclassification error) for the classification without restrictions on model parameters. The convergence rates we derive do not depend on dimension $d$, demonstrating that deep ReLU networks can overcome the curse of dimensionality in classification. While the majority of existing generalization analysis of classification algorithms relies on a bounded domain, we consider an unbounded domain by leveraging the analyticity and fast decay of Gaussian distributions. To facilitate our analysis, we give a novel approximation error bound for general analytic functions using ReLU networks, which may be of independent interest. Gaussian distributions can be adopted nicely to model data arising in applications, e.g., speeches, images, and texts; our results provide a theoretical verification of the observed efficiency of deep neural networks in practical classification problems.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Planning-to-Learn-A-Novel-Algorithm-for-Active-Learning-during-Model-Based-Planning"><a href="#Planning-to-Learn-A-Novel-Algorithm-for-Active-Learning-during-Model-Based-Planning" class="headerlink" title="Planning to Learn: A Novel Algorithm for Active Learning during Model-Based Planning"></a>Planning to Learn: A Novel Algorithm for Active Learning during Model-Based Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08029">http://arxiv.org/abs/2308.08029</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rowanlibr/sophisticated-learning">https://github.com/rowanlibr/sophisticated-learning</a></li>
<li>paper_authors: Rowan Hodson, Bruce Bassett, Charel van Hoof, Benjamin Rosman, Mark Solms, Jonathan P. Shock, Ryan Smith</li>
<li>For: 本研究的目的是比较Active Inference和 bayesian reinforcement learning（RL） schemes在解决相似问题时的表现，以及扩展Active Inference以包括活动学习。* Methods: 本研究使用了一种新的、生物学上启发的环境，用于强调解决问题的问题结构，并使用了推理搜索和对假推理来解决问题。* Results: 研究结果显示，使用了扩展的Active Inference（SL）算法可以在这种生物学上适用的环境中高效地解决问题，并且在比较 bayes-adaptive RL和Upper confidence bound算法时表现更好。这些结果为Active Inference在解决这类生物学上适用的问题提供了更多的支持，并为测试人类认知理论提供了新的工具。<details>
<summary>Abstract</summary>
Active Inference is a recent framework for modeling planning under uncertainty. Empirical and theoretical work have now begun to evaluate the strengths and weaknesses of this approach and how it might be improved. A recent extension - the sophisticated inference (SI) algorithm - improves performance on multi-step planning problems through recursive decision tree search. However, little work to date has been done to compare SI to other established planning algorithms. SI was also developed with a focus on inference as opposed to learning. The present paper has two aims. First, we compare performance of SI to Bayesian reinforcement learning (RL) schemes designed to solve similar problems. Second, we present an extension of SI - sophisticated learning (SL) - that more fully incorporates active learning during planning. SL maintains beliefs about how model parameters would change under the future observations expected under each policy. This allows a form of counterfactual retrospective inference in which the agent considers what could be learned from current or past observations given different future observations. To accomplish these aims, we make use of a novel, biologically inspired environment designed to highlight the problem structure for which SL offers a unique solution. Here, an agent must continually search for available (but changing) resources in the presence of competing affordances for information gain. Our simulations show that SL outperforms all other algorithms in this context - most notably, Bayes-adaptive RL and upper confidence bound algorithms, which aim to solve multi-step planning problems using similar principles (i.e., directed exploration and counterfactual reasoning). These results provide added support for the utility of Active Inference in solving this class of biologically-relevant problems and offer added tools for testing hypotheses about human cognition.
</details>
<details>
<summary>摘要</summary>
active inference是一种最近的规划下不确定性框架。实验和理论工作现在开始评估这种方法的优缺点和如何改进它。一种最新的扩展——复杂的推理（SI）算法——在多步规划问题上提高性能通过重层决策树搜索。然而，到目前为止，尚未对SI与其他已知规划算法进行比较。SI在推理而非学习方面得到了开发。本文的两个目标是：首先，比较SI与 bayesian reinforcement learning（RL）算法，解决类似问题。其次，我们提出了一种扩展SI的方法——复杂学习（SL），它更全面地包括活动学习在规划中。SL保留了对未来观测所期望的模型参数变化的信念。这allowsthe agent to consider what could be learned from current or past observations given different future observations。为了实现这些目标，我们使用了一个新的、生物学发现环境，这种环境可以强调规划问题中的问题结构，对于SL提供了特殊的解决方案。在这个环境中，agent需要不断搜索可用（但是变化的）资源，同时面临着竞争的信息收获可能性。我们的 simulations表明，SL在这种情况下表现出色，比bayes-adaptive RL和Upper confidence bound算法（这些算法目标解决类似的多步规划问题，使用相同的原则，即导航探索和对假推理）。这些结果为活动推断在这类生物学相关问题中的 utility提供了进一步的支持，并为测试人类认知假设提供了更多的工具。
</details></li>
</ul>
<hr>
<h2 id="Potential-Energy-Advantage-of-Quantum-Economy"><a href="#Potential-Energy-Advantage-of-Quantum-Economy" class="headerlink" title="Potential Energy Advantage of Quantum Economy"></a>Potential Energy Advantage of Quantum Economy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08025">http://arxiv.org/abs/2308.08025</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junyu Liu, Hansheng Jiang, Zuo-Jun Max Shen</li>
<li>for: 本研究旨在探讨量子计算在能效性方面的优势，并证明量子计算可以在能效性和盈利性两个方面超过类别计算。</li>
<li>methods: 本研究使用了 Cournot 竞争模型，并在能效性限制下研究了量子计算和类别计算的比较。</li>
<li>results: 研究发现，量子计算在大规模计算情况下可以实现更高的盈利性和能效性，并且这种优势是基于实际物理参数。<details>
<summary>Abstract</summary>
Energy cost is increasingly crucial in the modern computing industry with the wide deployment of large-scale machine learning models and language models. For the firms that provide computing services, low energy consumption is important both from the perspective of their own market growth and the government's regulations. In this paper, we study the energy benefits of quantum computing vis-a-vis classical computing. Deviating from the conventional notion of quantum advantage based solely on computational complexity, we redefine advantage in an energy efficiency context. Through a Cournot competition model constrained by energy usage, we demonstrate quantum computing firms can outperform classical counterparts in both profitability and energy efficiency at Nash equilibrium. Therefore quantum computing may represent a more sustainable pathway for the computing industry. Moreover, we discover that the energy benefits of quantum computing economies are contingent on large-scale computation. Based on real physical parameters, we further illustrate the scale of operation necessary for realizing this energy efficiency advantage.
</details>
<details>
<summary>摘要</summary>
现代计算业中能源成本日益重要，由于大规模机器学习模型和语言模型的广泛部署。为提供计算服务的公司来说，低能耗是重要的，不仅从市场增长的角度来看，还从政府的法规来看。在这篇论文中，我们研究了量子计算对于纳什平衡下的能源利好。相比传统的计算复杂性基础上的优势，我们重新定义了优势在能效环境下的意义。通过一个固定能源使用的 Cournot竞争模型，我们示出了量子计算公司可以在纳什平衡下超过 классиical对手在利润和能效环境方面表现优势。因此，量子计算可能代表计算业更可持续的发展途径。此外，我们发现了量子计算经济的能源利好取决于大规模计算。基于实际物理参数，我们进一步说明了实现这种能效优势所需的规模。
</details></li>
</ul>
<hr>
<h2 id="Active-Inverse-Learning-in-Stackelberg-Trajectory-Games"><a href="#Active-Inverse-Learning-in-Stackelberg-Trajectory-Games" class="headerlink" title="Active Inverse Learning in Stackelberg Trajectory Games"></a>Active Inverse Learning in Stackelberg Trajectory Games</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08017">http://arxiv.org/abs/2308.08017</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yue Yu, Jacob Levy, Negar Mehr, David Fridovich-Keil, Ufuk Topcu</li>
<li>for: 这篇论文主要写于哪个问题上？	+ 回答：Game-theoretic inverse learning问题，即从行为中推断玩家目标函数。</li>
<li>methods: 这篇论文使用了哪些方法？	+ 回答：提议了一种活动 inverse learning方法，通过帮助领导者尝试不同假设中的玩家目标函数，以加速领导者对玩家目标函数的推断。</li>
<li>results: 这篇论文得到了什么结果？	+ 回答：比对uniformly random inputs的情况，提议的输入可以加速玩家目标函数的推断，conditioned on the follower’s trajectory的概率加速了orders of magnitude。<details>
<summary>Abstract</summary>
Game-theoretic inverse learning is the problem of inferring the players' objectives from their actions. We formulate an inverse learning problem in a Stackelberg game between a leader and a follower, where each player's action is the trajectory of a dynamical system. We propose an active inverse learning method for the leader to infer which hypothesis among a finite set of candidates describes the follower's objective function. Instead of using passively observed trajectories like existing methods, the proposed method actively maximizes the differences in the follower's trajectories under different hypotheses to accelerate the leader's inference. We demonstrate the proposed method in a receding-horizon repeated trajectory game. Compared with uniformly random inputs, the leader inputs provided by the proposed method accelerate the convergence of the probability of different hypotheses conditioned on the follower's trajectory by orders of magnitude.
</details>
<details>
<summary>摘要</summary>
<<SYS>>游戏理论反学习是推理玩家的目标函数的问题。我们将游戏形式为 Stackelberg 游戏的领袖和追随者之间的反学习问题进行形式化。我们提议一种活动的反学习方法，使领袖可以根据追随者的动力系统轨迹中的差异来推断追随者的目标函数中的哪一个假设。不同于现有方法，我们的方法不使用被动地观察到的轨迹，而是活动地增加不同假设下追随者的轨迹之间的差异，以加速领袖的推断。我们在回归 horizon 重复轨迹游戏中示cases。相比于随机输入，由我们提议的领袖输入可以提高conditioned on the follower's trajectory的各个假设的概率的减少速度，这些减少速度可以达到orders of magnitude。Note: The translation is done using Google Translate and may not be perfect. Please let me know if you need any further assistance.
</details></li>
</ul>
<hr>
<h2 id="GRINN-A-Physics-Informed-Neural-Network-for-solving-hydrodynamic-systems-in-the-presence-of-self-gravity"><a href="#GRINN-A-Physics-Informed-Neural-Network-for-solving-hydrodynamic-systems-in-the-presence-of-self-gravity" class="headerlink" title="GRINN: A Physics-Informed Neural Network for solving hydrodynamic systems in the presence of self-gravity"></a>GRINN: A Physics-Informed Neural Network for solving hydrodynamic systems in the presence of self-gravity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08010">http://arxiv.org/abs/2308.08010</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sayantan Auddy, Ramit Dey, Neal J. Turner, Shantanu Basu<br>for: 这篇论文旨在模拟三维自引力液体系统，以解决astrophysics中许多基础问题，如星系形成、星系核心形成、大规模结构的发展等。methods: 该论文使用物理学 informed neural network（PINN）技术，在无网格框架下实现3D自引力液体系统的模拟。results: 该论文的结果表明，GRINN在一个iso thermic气体中的引力不稳定和波传播问题上具有高准确性和高效率。与传统网格代码相比，GRINN的计算时间不随维度的增加而增长，而是随着计算量的增加而增长。这些结果表明，PINN技术在astrophysical flows中可能带来 significative advances。<details>
<summary>Abstract</summary>
Modeling self-gravitating gas flows is essential to answering many fundamental questions in astrophysics. This spans many topics including planet-forming disks, star-forming clouds, galaxy formation, and the development of large-scale structures in the Universe. However, the nonlinear interaction between gravity and fluid dynamics offers a formidable challenge to solving the resulting time-dependent partial differential equations (PDEs) in three dimensions (3D). By leveraging the universal approximation capabilities of a neural network within a mesh-free framework, physics informed neural networks (PINNs) offer a new way of addressing this challenge. We introduce the gravity-informed neural network (GRINN), a PINN-based code, to simulate 3D self-gravitating hydrodynamic systems. Here, we specifically study gravitational instability and wave propagation in an isothermal gas. Our results match a linear analytic solution to within 1\% in the linear regime and a conventional grid code solution to within 5\% as the disturbance grows into the nonlinear regime. We find that the computation time of the GRINN does not scale with the number of dimensions. This is in contrast to the scaling of the grid-based code for the hydrodynamic and self-gravity calculations as the number of dimensions is increased. Our results show that the GRINN computation time is longer than the grid code in one- and two- dimensional calculations but is an order of magnitude lesser than the grid code in 3D with similar accuracy. Physics-informed neural networks like GRINN thus show promise for advancing our ability to model 3D astrophysical flows.
</details>
<details>
<summary>摘要</summary>
模拟自引力液体流动是astrophysics中答您许多基本问题的关键。这些问题包括 planet-forming 盘、star-forming 云、galaxy 形成和 universe 大规模结构的发展。然而，在三维空间中非线性的引力和流体动力学交互，对解决时间依赖的 partial differential equations (PDEs) 提出了挑战。通过利用神经网络的通用近似能力，physics informed neural networks (PINNs) 提供了一种新的解决方案。我们介绍了引力 informed neural network (GRINN)，一种基于 PINN 的代码，用于模拟三维自引力液体系统。在这里，我们专门研究引力不稳定和波传播在固定温度气体中。我们的结果与线性分析解匹配在线性 régime中的1%，并与基于网格的代码解匹配在非线性 régime中的5%。我们发现GRINN 的计算时间与维度无关，与基于网格的代码计算时间成正比。这与维度增加后网格代码的计算时间增长相比，GRINN 的计算时间更短。 physics-informed neural networks 如 GRINN 因此显示了在模拟三维astrophysical flows中的承诺。
</details></li>
</ul>
<hr>
<h2 id="BI-LAVA-Biocuration-with-Hierarchical-Image-Labeling-through-Active-Learning-and-Visual-Analysis"><a href="#BI-LAVA-Biocuration-with-Hierarchical-Image-Labeling-through-Active-Learning-and-Visual-Analysis" class="headerlink" title="BI-LAVA: Biocuration with Hierarchical Image Labeling through Active Learning and Visual Analysis"></a>BI-LAVA: Biocuration with Hierarchical Image Labeling through Active Learning and Visual Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08003">http://arxiv.org/abs/2308.08003</a></li>
<li>repo_url: None</li>
<li>paper_authors: Juan Trelles, Andrew Wentzel, William Berrios, G. Elisabeta Marai</li>
<li>For: 本研究用于 Addressing the challenges of creating useful datasets for biocuration in the biomedical domain, particularly the hierarchical nature of image labels, the overhead of processing images, the absence or incompleteness of labeled data, and the expertise required to label this type of data.* Methods: 本研究使用了一种 Iterative visual analytics and active learning strategy, 包括 a small set of image labels, a hierarchical set of image classifiers, and active learning to help model builders deal with incomplete ground-truth labels, target a hierarchical taxonomy of image modalities, and classify a large pool of unlabeled images.* Results: 研究表明，BI-LAVA 系统可以帮助域专家更好地理解类别内的特征，以及验证和改进数据质量在标注和未标注集合中。<details>
<summary>Abstract</summary>
In the biomedical domain, taxonomies organize the acquisition modalities of scientific images in hierarchical structures. Such taxonomies leverage large sets of correct image labels and provide essential information about the importance of a scientific publication, which could then be used in biocuration tasks. However, the hierarchical nature of the labels, the overhead of processing images, the absence or incompleteness of labeled data, and the expertise required to label this type of data impede the creation of useful datasets for biocuration. From a multi-year collaboration with biocurators and text-mining researchers, we derive an iterative visual analytics and active learning strategy to address these challenges. We implement this strategy in a system called BI-LAVA Biocuration with Hierarchical Image Labeling through Active Learning and Visual Analysis. BI-LAVA leverages a small set of image labels, a hierarchical set of image classifiers, and active learning to help model builders deal with incomplete ground-truth labels, target a hierarchical taxonomy of image modalities, and classify a large pool of unlabeled images. BI-LAVA's front end uses custom encodings to represent data distributions, taxonomies, image projections, and neighborhoods of image thumbnails, which help model builders explore an unfamiliar image dataset and taxonomy and correct and generate labels. An evaluation with machine learning practitioners shows that our mixed human-machine approach successfully supports domain experts in understanding the characteristics of classes within the taxonomy, as well as validating and improving data quality in labeled and unlabeled collections.
</details>
<details>
<summary>摘要</summary>
在生物医学领域，taxonomy 组织科学图像的获取方式在层次结构中。这些taxonomy 利用大量正确的图像标签，提供了科学公版的重要信息，可以用于生物团采工作。然而，层次性标签、处理图像的开销、标签数据的缺失或不完整、以及标签这类数据的专业知识卷积着创建有用的数据集。从多年的biocurator和文本挖掘研究人员的合作，我们 derivate了一种迭代式视觉分析和活动学习策略。我们在BI-LAVA 系统中实现了这种策略，BI-LAVA 是一个通过活动学习和迭代式视觉分析来帮助模型建立者处理部分标签、针对层次的图像模式和大量未标记图像进行分类的系统。BI-LAVA 的前端使用自定编码来表示数据分布、税onomy、图像投影和图像缩略图的邻域，这些编码帮助模型建立者探索未familiar的图像集和税onomy，并且 correction和生成标签。我们与机器学习实践者进行评估，发现我们的人机共同approach 成功地支持领域专家理解税onomy中类别的特点，以及验证和改进标签数据的质量。
</details></li>
</ul>
<hr>
<h2 id="A-physics-informed-machine-learning-model-for-reconstruction-of-dynamic-loads"><a href="#A-physics-informed-machine-learning-model-for-reconstruction-of-dynamic-loads" class="headerlink" title="A physics-informed machine learning model for reconstruction of dynamic loads"></a>A physics-informed machine learning model for reconstruction of dynamic loads</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08571">http://arxiv.org/abs/2308.08571</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gledson Rodrigo Tondo, Igor Kavrakov, Guido Morgenthal</li>
<li>for: 这篇论文是为了评估长 Span bridges 在不同动态刺激下的响应，以及考虑这些刺激对结构系统的影响。</li>
<li>methods: 这篇论文使用了概率Physics-informed机器学习框架，基于 Gaussian process regression 重构动力。该模型可以处理受限和杂质数据，并提供自然的衰减方法来补做测量系统中的噪声。</li>
<li>results: 这篇论文通过应用于大坝东桥的 aerodynamic 分析，计算了不同动态刺激下的响应，并使用了稀缺和噪声的测量数据来重构动力。结果显示，模型和实际应用的动力之间存在良好的一致性，并可以扩展到计算全局响应和结构内力。<details>
<summary>Abstract</summary>
Long-span bridges are subjected to a multitude of dynamic excitations during their lifespan. To account for their effects on the structural system, several load models are used during design to simulate the conditions the structure is likely to experience. These models are based on different simplifying assumptions and are generally guided by parameters that are stochastically identified from measurement data, making their outputs inherently uncertain. This paper presents a probabilistic physics-informed machine-learning framework based on Gaussian process regression for reconstructing dynamic forces based on measured deflections, velocities, or accelerations. The model can work with incomplete and contaminated data and offers a natural regularization approach to account for noise in the measurement system. An application of the developed framework is given by an aerodynamic analysis of the Great Belt East Bridge. The aerodynamic response is calculated numerically based on the quasi-steady model, and the underlying forces are reconstructed using sparse and noisy measurements. Results indicate a good agreement between the applied and the predicted dynamic load and can be extended to calculate global responses and the resulting internal forces. Uses of the developed framework include validation of design models and assumptions, as well as prognosis of responses to assist in damage detection and structural health monitoring.
</details>
<details>
<summary>摘要</summary>
长链桥受到多种动态冲击 durante 其服役寿命。为了考虑这些冲击对结构系统的影响，设计时使用多种荷载模型来模拟结构会经历的情况。这些模型基于不同的简化假设，通常受到测量数据中的参数随机 identificado 的指导。这篇文章介绍了一种基于 Gaussian process regression 的概率物理学 informed machine-learning 框架，可以根据测量到的弯曲、速度或加速度来重建动态力。该模型可以处理部分 incomplete 和污染的数据，并提供一种自然的常化方法来补做测量系统中的噪声。应用该开发的框架是大套东大桥的 aerodynamic 分析。通过 numerically 计算 quasi-steady 模型，并使用稀疏和噪声干扰的测量数据来重建动态荷载。结果表明与应用的动态荷载相比，预测的动态荷载具有良好的一致性。此外，该框架可以扩展到计算全局响应和结构内部力。用于 validate 设计模型和假设，以及诊断和结构健康监测。
</details></li>
</ul>
<hr>
<h2 id="Monte-Carlo-guided-Diffusion-for-Bayesian-linear-inverse-problems"><a href="#Monte-Carlo-guided-Diffusion-for-Bayesian-linear-inverse-problems" class="headerlink" title="Monte Carlo guided Diffusion for Bayesian linear inverse problems"></a>Monte Carlo guided Diffusion for Bayesian linear inverse problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07983">http://arxiv.org/abs/2308.07983</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gabriel Cardoso, Yazid Janati El Idrissi, Sylvain Le Corff, Eric Moulines</li>
<li>for: 解决具有前向模型知识的不整合线性逆问题，从计算摄影到医学成像等多个应用领域。</li>
<li>methods: 使用得分生成模型（SGMs）生成有感知可能性的图像，尤其是填充问题。</li>
<li>results: 提出使用Sequential Monte Carlo方法解决Feynman–Kac模型的问题，并在实验中超越竞争对比方案。<details>
<summary>Abstract</summary>
Ill-posed linear inverse problems that combine knowledge of the forward measurement model with prior models arise frequently in various applications, from computational photography to medical imaging. Recent research has focused on solving these problems with score-based generative models (SGMs) that produce perceptually plausible images, especially in inpainting problems. In this study, we exploit the particular structure of the prior defined in the SGM to formulate recovery in a Bayesian framework as a Feynman--Kac model adapted from the forward diffusion model used to construct score-based diffusion. To solve this Feynman--Kac problem, we propose the use of Sequential Monte Carlo methods. The proposed algorithm, MCGdiff, is shown to be theoretically grounded and we provide numerical simulations showing that it outperforms competing baselines when dealing with ill-posed inverse problems.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>计算摄影和医学成像等应用中频繁出现的不整合线性逆问题，现在通过使用分数基生成模型（SGM）解决这些问题，特别是填充问题。在这种研究中，我们利用SGM中的特殊结构来设置 bayesian 框架，并将其转化为凯克-菲涅曼模型，从而解决这个凯克-菲涅曼问题。为解决这个问题，我们提议使用顺序蒙те Carlo 方法。我们称之为 MCGdiff。我们证明了这种算法的理论基础，并通过数值实验表明，它在解决不整合逆问题时表现更好于竞争对手。
</details></li>
</ul>
<hr>
<h2 id="An-Adaptive-Approach-for-Probabilistic-Wind-Power-Forecasting-Based-on-Meta-Learning"><a href="#An-Adaptive-Approach-for-Probabilistic-Wind-Power-Forecasting-Based-on-Meta-Learning" class="headerlink" title="An Adaptive Approach for Probabilistic Wind Power Forecasting Based on Meta-Learning"></a>An Adaptive Approach for Probabilistic Wind Power Forecasting Based on Meta-Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07980">http://arxiv.org/abs/2308.07980</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zichao Meng, Ye Guo, Hongbin Sun</li>
<li>for: This paper proposes an adaptive approach for probabilistic wind power forecasting (WPF) that includes offline and online learning procedures.</li>
<li>methods: The proposed approach uses inner and outer loop updates of meta-learning to train a base forecast model, which is then applied to online forecasting combined with incremental learning techniques.</li>
<li>results: The proposed approach is validated through numerical tests on real-world wind power data sets, and the results show that it has advantages in adaptivity compared with existing alternatives.Here is the same information in Traditional Chinese:</li>
<li>for: 这个研究提出了一个适应方法 для probabilistic wind power forecasting (WPF)，包括了线上和线下学习过程。</li>
<li>methods: 这个方法使用了内部和外部循环更新的meta-learning来训练基础预测模型，然后将基础预测模型应用到线上预测，并与增量学习技术相结合。</li>
<li>results: 研究结果透过使用实际风力资料集进行数据分析，发现这个方法在适应性方面比现有的方法有优势。<details>
<summary>Abstract</summary>
This paper studies an adaptive approach for probabilistic wind power forecasting (WPF) including offline and online learning procedures. In the offline learning stage, a base forecast model is trained via inner and outer loop updates of meta-learning, which endows the base forecast model with excellent adaptability to different forecast tasks, i.e., probabilistic WPF with different lead times or locations. In the online learning stage, the base forecast model is applied to online forecasting combined with incremental learning techniques. On this basis, the online forecast takes full advantage of recent information and the adaptability of the base forecast model. Two applications are developed based on our proposed approach concerning forecasting with different lead times (temporal adaptation) and forecasting for newly established wind farms (spatial adaptation), respectively. Numerical tests were conducted on real-world wind power data sets. Simulation results validate the advantages in adaptivity of the proposed methods compared with existing alternatives.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="MultiSChuBERT-Effective-Multimodal-Fusion-for-Scholarly-Document-Quality-Prediction"><a href="#MultiSChuBERT-Effective-Multimodal-Fusion-for-Scholarly-Document-Quality-Prediction" class="headerlink" title="MultiSChuBERT: Effective Multimodal Fusion for Scholarly Document Quality Prediction"></a>MultiSChuBERT: Effective Multimodal Fusion for Scholarly Document Quality Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07971">http://arxiv.org/abs/2308.07971</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gideon Maillette de Buy Wenniger, Thomas van Dongen, Lambert Schomaker</li>
<li>for: 本文旨在提高学术文献质量预测 task 的性能，特别是通过添加视觉信息来提高模型的准确率。</li>
<li>methods: 本文提出了一种多modal预测模型 MultiSChuBERT，它结合了基于块化全文文本的文本模型 SChuBERT，以及基于 Inception V3 的视觉模型。</li>
<li>results: 本文证明了以下三点：首先，将视觉和文本嵌入结合可以显著改善结果。其次，逐渐冻结视觉子模型的权重可以降低过拟合现象，提高结果。最后，使用不同的文本嵌入模型可以进一步提高结果。使用 BERT$_{\textrm{BASE}}$ 嵌入，在 ACL-BiblioMetry 数据集上的（对数）引用数预测任务中，MultiSChuBERT 模型的 $R^{2}$ 分数为 0.454，比 SChuBERT 模型（只使用文本）的 0.432 高。类似的改进也在 PeerRead 接受&#x2F;拒绝预测任务中获得。在使用 SciBERT、scincl、SPECTER 和 SPECTER2.0 嵌入时，我们发现每个这些适应嵌入都可以进一步提高模型的性能，SPECTER2.0 嵌入得最好。<details>
<summary>Abstract</summary>
Automatic assessment of the quality of scholarly documents is a difficult task with high potential impact. Multimodality, in particular the addition of visual information next to text, has been shown to improve the performance on scholarly document quality prediction (SDQP) tasks. We propose the multimodal predictive model MultiSChuBERT. It combines a textual model based on chunking full paper text and aggregating computed BERT chunk-encodings (SChuBERT), with a visual model based on Inception V3.Our work contributes to the current state-of-the-art in SDQP in three ways. First, we show that the method of combining visual and textual embeddings can substantially influence the results. Second, we demonstrate that gradual-unfreezing of the weights of the visual sub-model, reduces its tendency to ovefit the data, improving results. Third, we show the retained benefit of multimodality when replacing standard BERT$_{\textrm{BASE}}$ embeddings with more recent state-of-the-art text embedding models.   Using BERT$_{\textrm{BASE}}$ embeddings, on the (log) number of citations prediction task with the ACL-BiblioMetry dataset, our MultiSChuBERT (text+visual) model obtains an $R^{2}$ score of 0.454 compared to 0.432 for the SChuBERT (text only) model. Similar improvements are obtained on the PeerRead accept/reject prediction task. In our experiments using SciBERT, scincl, SPECTER and SPECTER2.0 embeddings, we show that each of these tailored embeddings adds further improvements over the standard BERT$_{\textrm{BASE}}$ embeddings, with the SPECTER2.0 embeddings performing best.
</details>
<details>
<summary>摘要</summary>
自动评估学术文献质量是一项具有高潜在影响力的任务。在特定的情况下，通过添加视觉信息与文本信息一起进行评估，可以提高学术文献质量预测（SDQP）任务的性能。我们提出了多模态预测模型MultiSChuBERT，它将文本模型基于分割全文本并聚合计算的BERT块编码（SChuBERT）与视觉模型基于Inception V3.0结合。我们的工作对现有状态的SDQP进行了贡献。首先，我们发现将视觉和文本嵌入结合的方法可以对结果产生显著影响。其次，我们示出了逐渐冰结视觉子模型的重量的方法可以降低它们的预测倾向，提高结果。最后，我们发现在使用更新的文本嵌入模型而不是标准BERT$_{\textrm{BASE}}$嵌入时，多模态性仍然保留着其优势。使用BERT$_{\textrm{BASE}}$嵌入，我们的MultiSChuBERT（文本+视觉）模型在ACL-BiblioMetry数据集上的（对数）引用数预测任务中，obtained an $R^{2}$ score of 0.454，比SChuBERT（文本只）模型的0.432高。类似的改进也在PeerRead Accept/Reject预测任务中被获得。在我们使用SciBERT、scincl、SPECTER和SPECTER2.0嵌入时，我们发现每个这些适应嵌入都可以进一步提高标准BERT$_{\textrm{BASE}}$嵌入的性能，SPECTER2.0嵌入表现最佳。
</details></li>
</ul>
<hr>
<h2 id="RAVEN-In-Context-Learning-with-Retrieval-Augmented-Encoder-Decoder-Language-Models"><a href="#RAVEN-In-Context-Learning-with-Retrieval-Augmented-Encoder-Decoder-Language-Models" class="headerlink" title="RAVEN: In-Context Learning with Retrieval Augmented Encoder-Decoder Language Models"></a>RAVEN: In-Context Learning with Retrieval Augmented Encoder-Decoder Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07922">http://arxiv.org/abs/2308.07922</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jie Huang, Wei Ping, Peng Xu, Mohammad Shoeybi, Kevin Chen-Chuan Chang, Bryan Catanzaro</li>
<li>for:  investigate the in-context learning ability of retrieval-augmented encoder-decoder language models</li>
<li>methods:  combines retrieval-augmented masked language modeling and prefix language modeling, and introduces Fusion-in-Context Learning to enhance the few-shot performance</li>
<li>results:  significantly outperforms ATLAS and achieves results comparable to the most advanced language models in certain scenarios, despite having substantially fewer parameters<details>
<summary>Abstract</summary>
In this paper, we investigate the in-context learning ability of retrieval-augmented encoder-decoder language models. We first conduct a comprehensive analysis of the state-of-the-art ATLAS model and identify its limitations in in-context learning, primarily due to a mismatch between pretraining and testing, as well as a restricted context length. To address these issues, we propose RAVEN, a model that combines retrieval-augmented masked language modeling and prefix language modeling. We further introduce Fusion-in-Context Learning to enhance the few-shot performance by enabling the model to leverage more in-context examples without requiring additional training or model modifications. Through extensive experiments, we demonstrate that RAVEN significantly outperforms ATLAS and achieves results comparable to the most advanced language models in certain scenarios, despite having substantially fewer parameters. Our work underscores the potential of retrieval-augmented encoder-decoder language models for in-context learning and encourages further research in this direction.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们 investigate retrieval-augmented encoder-decoder语言模型的在语言上下文中学习能力。我们首先对现有的ATLAS模型进行了全面的分析，并发现其在语言上下文中学习时存在一些限制，主要是因为预训练和测试集的匹配度不高，以及上下文长度的限制。为了解决这些问题，我们提议了RAVEN模型，该模型结合了检索支持的隐藏语言模型和前缀语言模型。我们还提出了Context-Aware Fusion Learning，以便通过在语言上下文中充分利用更多的示例来提高几个步骤性能，无需进行额外训练或模型修改。通过广泛的实验，我们证明了RAVEN模型可以在某些情况下明显超越ATLAS模型，并达到与最先进的语言模型相当的性能，即使RAVEN模型具有许多更少的参数。我们的工作论证了 retrieval-augmented encoder-decoder语言模型在语言上下文中学习的潜力，并鼓励进一步的研究在这个方向上。
</details></li>
</ul>
<hr>
<h2 id="The-Regular-Expression-Inference-Challenge"><a href="#The-Regular-Expression-Inference-Challenge" class="headerlink" title="The Regular Expression Inference Challenge"></a>The Regular Expression Inference Challenge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07899">http://arxiv.org/abs/2308.07899</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mojtaba Valizadeh, Philip John Gorinski, Ignacio Iacobacci, Martin Berger</li>
<li>for: 这paper是为了挑战code&#x2F;语言模型领域中的一个挑战任务，即常见表达式推理（REI）问题。</li>
<li>methods: 这paper使用了Program Synthesis技术和GPU来实现REI问题的解决方案。</li>
<li>results: 这paper首次生成了复杂REI实例的最小表达式，并发布了大规模的REI数据集。此外，paper还提出了一些初步的机器学习基线和优化策略。<details>
<summary>Abstract</summary>
We propose \emph{regular expression inference (REI)} as a challenge for code/language modelling, and the wider machine learning community. REI is a supervised machine learning (ML) and program synthesis task, and poses the problem of finding minimal regular expressions from examples: Given two finite sets of strings $P$ and $N$ and a cost function $\text{cost}(\cdot)$, the task is to generate an expression $r$ that accepts all strings in $P$ and rejects all strings in $N$, while no other such expression $r'$ exists with $\text{cost}(r')<\text{cost}(r)$.   REI has advantages as a challenge problem: (i) regular expressions are well-known, widely used, and a natural idealisation of code; (ii) REI's asymptotic worst-case complexity is well understood; (iii) REI has a small number of easy to understand parameters (e.g.~$P$ or $N$ cardinality, string lengths of examples, or the cost function); this lets us easily finetune REI-hardness; (iv) REI is an unsolved problem for deep learning based ML.   Recently, an REI solver was implemented on GPUs, using program synthesis techniques. This enabled, for the first time, fast generation of minimal expressions for complex REI instances. Building on this advance, we generate and publish the first large-scale datasets for REI, and devise and evaluate several initial heuristic and machine learning baselines.   We invite the community to participate and explore ML methods that learn to solve REI problems. We believe that progress in REI directly translates to code/language modelling.
</details>
<details>
<summary>摘要</summary>
我们提议“常量表示法（REI）”作为代码/语言模型领域的挑战，并且广泛的机器学习社群。 REI 是一个监督式机器学习（ML）和程式生成任务，需要寻找最小的常量表示法，以满足以下需求： giventwo个有限集合 $P$ 和 $N$，以及一个成本函数 $\text{cost}(\cdot)$，任务是生成一个表示法 $r$，使得 $r$ 接受所有 $P$ 中的字串，并且拒绝所有 $N$ 中的字串，而不是其他任何表示法 $r'$，使得 $\text{cost}(r') < \text{cost}(r)$。REI 有以下优点作为挑战问题：1. 常量表示法是广泛使用的、具有自然化的代码理想化；2. REI 的极限最坏情况复杂度很好地理解；3. REI 只有很少的容易理解的参数（例如 $P$ 或 $N$ 的卡дина特数、字串示例的长度、或成本函数），这让我们可以轻松地调整 REI 的困难度；4. REI 是深度学习基于 ML 的未解决问题。最近，一个 REI 解决方案在 GPU 上被实现，使用程式生成技术。这使得， для 首次可以快速生成复杂的 REI 问题中的最小表示法。基于这个进步，我们组建了第一个大规模的 REI 数据集，并设计了一些初步的变数和机器学习基线。我们邀请社区参与，探索 ML 方法，以解决 REI 问题。我们相信，REI 的进步将直接对代码/语言模型领域产生影响。
</details></li>
</ul>
<hr>
<h2 id="SciRE-Solver-Efficient-Sampling-of-Diffusion-Probabilistic-Models-by-Score-integrand-Solver-with-Recursive-Derivative-Estimation"><a href="#SciRE-Solver-Efficient-Sampling-of-Diffusion-Probabilistic-Models-by-Score-integrand-Solver-with-Recursive-Derivative-Estimation" class="headerlink" title="SciRE-Solver: Efficient Sampling of Diffusion Probabilistic Models by Score-integrand Solver with Recursive Derivative Estimation"></a>SciRE-Solver: Efficient Sampling of Diffusion Probabilistic Models by Score-integrand Solver with Recursive Derivative Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07896">http://arxiv.org/abs/2308.07896</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shigui Li, Wei Chen, Delu Zeng</li>
<li>for: 这篇论文是为了提出一种高效的推 diffusion probabilistic models（DPMs）采样算法。</li>
<li>methods: 该论文使用了一种新的score-based exact solution paradigm，以及一种 recursive derivative estimation（RDE）方法，以提高采样速度。</li>
<li>results: 该论文通过使用score-integrand solver with convergence order guarantee（SciRE-Solver），实现了高效的采样性能，并在CIFAR10和CelebA 64×64上达到了state-of-the-art（SOTA）水平。 Specifically, it achieved $3.48$ FID with $12$ NFE and $2.42$ FID with $20$ NFE for continuous-time DPMs on CIFAR10, and $2.17$ ($2.02$) FID with $18$ ($50$) NFE for discrete-time DPM on CelebA 64×64.<details>
<summary>Abstract</summary>
Diffusion probabilistic models (DPMs) are a powerful class of generative models known for their ability to generate high-fidelity image samples. A major challenge in the implementation of DPMs is the slow sampling process. In this work, we bring a high-efficiency sampler for DPMs. Specifically, we propose a score-based exact solution paradigm for the diffusion ODEs corresponding to the sampling process of DPMs, which introduces a new perspective on developing numerical algorithms for solving diffusion ODEs. To achieve an efficient sampler, we propose a recursive derivative estimation (RDE) method to reduce the estimation error. With our proposed solution paradigm and RDE method, we propose the score-integrand solver with the convergence order guarantee as efficient solver (SciRE-Solver) for solving diffusion ODEs. The SciRE-Solver attains state-of-the-art (SOTA) sampling performance with a limited number of score function evaluations (NFE) on both discrete-time and continuous-time DPMs in comparison to existing training-free sampling algorithms. Such as, we achieve $3.48$ FID with $12$ NFE and $2.42$ FID with $20$ NFE for continuous-time DPMs on CIFAR10, respectively. Different from other samplers, SciRE-Solver has the promising potential to surpass the FIDs achieved in the original papers of some pre-trained models with a small NFEs. For example, we reach SOTA value of $2.40$ FID with $100$ NFE for continuous-time DPM and of $3.15$ FID with $84$ NFE for discrete-time DPM on CIFAR-10, as well as of $2.17$ ($2.02$) FID with $18$ ($50$) NFE for discrete-time DPM on CelebA 64$\times$64.
</details>
<details>
<summary>摘要</summary>
Diffusion probabilistic models (DPMs) 是一种强大的生成模型，能够生成高质量的图像样本。然而，在实现DPMs时，一个主要挑战是慢的采样过程。在这种情况下，我们提出了一种高效的采样方法。具体来说，我们提出了基于分布解决方法的高效采样方法，该方法可以减少采样过程中的估计误差。为了实现高效采样，我们提出了一种可重复的 derive 估计方法（RDE），该方法可以减少估计误差。通过我们的提出的解法和RDE方法，我们提出了一种高效的分布解决方法（SciRE-Solver），该方法可以高效地解决Diffusion ODEs。SciRE-Solver 可以在不同的时间分辨率下实现高效的采样，并且可以在有限的分布解决方法中实现高质量的采样。例如，我们在 CIFAR10 上实现了 $3.48$ FID 的值，只需要 $12$ NFE，并且在 continuous-time DPMs 上实现了 $2.42$ FID 的值，只需要 $20$ NFE。与其他采样器不同，SciRE-Solver 具有可能超越原始模型中的 FID 的潜在能力。例如，我们在 continuous-time DPMs 上实现了 $2.40$ FID 的值，只需要 $100$ NFE，并在 discrete-time DPMs 上实现了 $3.15$ FID 的值，只需要 $84$ NFE。此外，我们还在 CelebA 64$\times$64 上实现了 $2.17$ ($2.02$) FID 的值，只需要 $18$ ($50$) NFE。
</details></li>
</ul>
<hr>
<h2 id="On-regularized-Radon-Nikodym-differentiation"><a href="#On-regularized-Radon-Nikodym-differentiation" class="headerlink" title="On regularized Radon-Nikodym differentiation"></a>On regularized Radon-Nikodym differentiation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07887">http://arxiv.org/abs/2308.07887</a></li>
<li>repo_url: None</li>
<li>paper_authors: Duc Hoan Nguyen, Werner Zellinger, Sergei V. Pereverzyev</li>
<li>for: 解决Radon-NikodymDerivatives估计问题，这个问题在各种应用中出现，如covariate shift适应、likelihood-ratio测试、mutual information估计和conditional probability估计中。</li>
<li>methods: 使用通用正则化方案在 reproduce kernel Hilbert space中进行估计。</li>
<li>results:  estabilish了估计Derivatives的收敛率，并且可以在特定点进行高精度的重建。<details>
<summary>Abstract</summary>
We discuss the problem of estimating Radon-Nikodym derivatives. This problem appears in various applications, such as covariate shift adaptation, likelihood-ratio testing, mutual information estimation, and conditional probability estimation. To address the above problem, we employ the general regularization scheme in reproducing kernel Hilbert spaces. The convergence rate of the corresponding regularized algorithm is established by taking into account both the smoothness of the derivative and the capacity of the space in which it is estimated. This is done in terms of general source conditions and the regularized Christoffel functions. We also find that the reconstruction of Radon-Nikodym derivatives at any particular point can be done with high order of accuracy. Our theoretical results are illustrated by numerical simulations.
</details>
<details>
<summary>摘要</summary>
我们讨论类 Radon-Nikodym Derivative 的估计问题。这个问题在不同的应用中出现，例如对应拓扑变化、对应拓扑测试、共轨信息估计以及 conditional probability 估计。为了解决上述问题，我们使用通用的常数化方案在 reproduce kernel 空间中实现。我们证明了这个常数化算法的数据速度，通过考虑 derivative 的平滑性和估计空间的容量。此外，我们还发现了在特定点进行 Radon-Nikodym Derivative 的重建可以实现高精度。我们的理论成果通过数学模拟来描述。
</details></li>
</ul>
<hr>
<h2 id="Back-to-Basics-A-Sanity-Check-on-Modern-Time-Series-Classification-Algorithms"><a href="#Back-to-Basics-A-Sanity-Check-on-Modern-Time-Series-Classification-Algorithms" class="headerlink" title="Back to Basics: A Sanity Check on Modern Time Series Classification Algorithms"></a>Back to Basics: A Sanity Check on Modern Time Series Classification Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07886">http://arxiv.org/abs/2308.07886</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mlgig/tabularmodelsfortsc">https://github.com/mlgig/tabularmodelsfortsc</a></li>
<li>paper_authors: Bhaskar Dhariyal, Thach Le Nguyen, Georgiana Ifrim</li>
<li>for: 本研究旨在评估时序分类领域的基本参照模型，以及与现代时序分类算法相比较。</li>
<li>methods: 本研究使用了简单的表格模型（如ridge、LDA、RandomForest）和ROCKET家族的时序分类算法（如Rocket、MiniRocket、MultiRocket）进行比较。</li>
<li>results: 结果表明，表格模型在大约19%的单变量 dataset和28%的多变量 dataset上表现较为出色，并在大约50%的 dataset 上达到了准确率的10个百分点。这些结果表明，在开发时序分类算法时，需要考虑基本的表格模型作为参照。这些模型快速、简单、可能更容易理解和部署。<details>
<summary>Abstract</summary>
The state-of-the-art in time series classification has come a long way, from the 1NN-DTW algorithm to the ROCKET family of classifiers. However, in the current fast-paced development of new classifiers, taking a step back and performing simple baseline checks is essential. These checks are often overlooked, as researchers are focused on establishing new state-of-the-art results, developing scalable algorithms, and making models explainable. Nevertheless, there are many datasets that look like time series at first glance, but classic algorithms such as tabular methods with no time ordering may perform better on such problems. For example, for spectroscopy datasets, tabular methods tend to significantly outperform recent time series methods. In this study, we compare the performance of tabular models using classic machine learning approaches (e.g., Ridge, LDA, RandomForest) with the ROCKET family of classifiers (e.g., Rocket, MiniRocket, MultiRocket). Tabular models are simple and very efficient, while the ROCKET family of classifiers are more complex and have state-of-the-art accuracy and efficiency among recent time series classifiers. We find that tabular models outperform the ROCKET family of classifiers on approximately 19% of univariate and 28% of multivariate datasets in the UCR/UEA benchmark and achieve accuracy within 10 percentage points on about 50% of datasets. Our results suggest that it is important to consider simple tabular models as baselines when developing time series classifiers. These models are very fast, can be as effective as more complex methods and may be easier to understand and deploy.
</details>
<details>
<summary>摘要</summary>
现代时序分类技术已经发展到了非常高水平，从1NN-DTW算法到ROCKET家族的分类器。然而，在当前的快速发展新的分类器，回退并执行简单的基准检查是必要的。这些检查经常被忽略，因为研究人员正在寻求新的state-of-the-art结果，开发可扩展的算法，并使模型更加可解释。然而，有很多数据集看起来像时序数据，但经典算法如表格方法无时间顺序可能在这些问题上表现更好。例如，对于光谱数据集，表格方法通常在近期时间系列方法之上表现出色。在这种研究中，我们比较了使用经典机器学习方法（例如ridge、LDA、RandomForest）的表格模型与ROCKET家族的分类器（例如Rocket、MiniRocket、MultiRocket）的性能。表格模型简单而高效，而ROCKET家族的分类器更加复杂，在最近的时序分类器中具有state-of-the-art的准确率和效率。我们发现，在UCRLUEA标准测试集上，表格模型比ROCKET家族的分类器在约19%的单variate数据集和28%的多variate数据集上表现出色，并在约50%的数据集上达到了准确率在10个百分点之间。我们的结果表明，在开发时序分类器时，应该考虑使用简单的表格模型作为基准。这些模型很快速，可以与更复杂的方法相比，并且可能更易于理解和部署。
</details></li>
</ul>
<hr>
<h2 id="The-Challenge-of-Fetal-Cardiac-MRI-Reconstruction-Using-Deep-Learning"><a href="#The-Challenge-of-Fetal-Cardiac-MRI-Reconstruction-Using-Deep-Learning" class="headerlink" title="The Challenge of Fetal Cardiac MRI Reconstruction Using Deep Learning"></a>The Challenge of Fetal Cardiac MRI Reconstruction Using Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07885">http://arxiv.org/abs/2308.07885</a></li>
<li>repo_url: None</li>
<li>paper_authors: Denis Prokopenko, Kerstin Hammernik, Thomas Roberts, David F A Lloyd, Daniel Rueckert, Joseph V Hajnal<br>for:* The paper aims to improve the quality of non-gated kt-SENSE reconstruction for dynamic free-breathing fetal cardiac MRI using deep learning methods.methods:* The authors use supervised deep learning networks to recover fully-sampled data from undersampled data, and explore various model architectures and training strategies.results:* The best-performing models recover a detailed depiction of the maternal anatomy but underestimate the dynamic properties of the fetal heart, suggesting the need for more targeted training and evaluation methods.<details>
<summary>Abstract</summary>
Dynamic free-breathing fetal cardiac MRI is one of the most challenging modalities, which requires high temporal and spatial resolution to depict rapid changes in a small fetal heart. The ability of deep learning methods to recover undersampled data could help to optimise the kt-SENSE acquisition strategy and improve non-gated kt-SENSE reconstruction quality. In this work, we explore supervised deep learning networks for reconstruction of kt-SENSE style acquired data using an extensive in vivo dataset. Having access to fully-sampled low-resolution multi-coil fetal cardiac MRI, we study the performance of the networks to recover fully-sampled data from undersampled data. We consider model architectures together with training strategies taking into account their application in the real clinical setup used to collect the dataset to enable networks to recover prospectively undersampled data. We explore a set of modifications to form a baseline performance evaluation for dynamic fetal cardiac MRI on real data. We systematically evaluate the models on coil-combined data to reveal the effect of the suggested changes to the architecture in the context of fetal heart properties. We show that the best-performers recover a detailed depiction of the maternal anatomy on a large scale, but the dynamic properties of the fetal heart are under-represented. Training directly on multi-coil data improves the performance of the models, allows their prospective application to undersampled data and makes them outperform CTFNet introduced for adult cardiac cine MRI. However, these models deliver similar qualitative performances recovering the maternal body very well but underestimating the dynamic properties of fetal heart. This dynamic feature of fast change of fetal heart that is highly localised suggests both more targeted training and evaluation methods might be needed for fetal heart application.
</details>
<details>
<summary>摘要</summary>
“动态自由呼吸幼儿心脏MRI是最复杂的modalities，需要高度的时间和空间分辨率来描述幼儿心脏的快速变化。深度学习方法可以回归不完全探测的数据，可以帮助优化kt-SENSE数据获取策略，提高非阻塞kt-SENSE重建质量。在这种工作中，我们使用了supervised深度学习网络来重建kt-SENSE风格获取的数据，使用了大量的生物实验室数据。由于我们拥有完整的低分辨率多极心脏MRI数据，我们研究了网络可以从不完整的数据中恢复完整的数据的性能。我们考虑了模型架构和训练策略，以便在实际临床设置中收集数据时使用。我们系统地评估了模型在实际数据上的性能，并对幼儿心脏属性进行了修改。我们发现最佳performer可以呈现出详细的 maternal anatomy，但是幼儿心脏的动态特性受到了下降。通过直接训练多极数据，我们可以使模型预测不完整的数据，并且其性能高于CTFNet。但是，这些模型在恢复 maternal body 方面表现良好，而幼儿心脏的动态特性方面表现较差。这种快速变化的幼儿心脏特性表示需要更加targeted的训练和评估方法。”
</details></li>
</ul>
<hr>
<h2 id="A-Trustable-LSTM-Autoencoder-Network-for-Cyberbullying-Detection-on-Social-Media-Using-Synthetic-Data"><a href="#A-Trustable-LSTM-Autoencoder-Network-for-Cyberbullying-Detection-on-Social-Media-Using-Synthetic-Data" class="headerlink" title="A Trustable LSTM-Autoencoder Network for Cyberbullying Detection on Social Media Using Synthetic Data"></a>A Trustable LSTM-Autoencoder Network for Cyberbullying Detection on Social Media Using Synthetic Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09722">http://arxiv.org/abs/2308.09722</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mst Shapna Akter, Hossain Shahriar, Alfredo Cuzzocrea<br>for: This paper proposes a trustable LSTM-Autoencoder Network for cyberbullying detection on social media using synthetic data.methods: The proposed model uses a combination of Long Short-Term Memory (LSTM), Bidirectional Long Short-Term Memory (BiLSTM), LSTM-Autoencoder, Word2vec, Bidirectional Encoder Representations from Transformers (BERT), and Generative Pre-trained Transformer 2 (GPT-2) models.results: The proposed model outperformed all the models on all datasets, achieving the highest accuracy of 95%.<details>
<summary>Abstract</summary>
Social media cyberbullying has a detrimental effect on human life. As online social networking grows daily, the amount of hate speech also increases. Such terrible content can cause depression and actions related to suicide. This paper proposes a trustable LSTM-Autoencoder Network for cyberbullying detection on social media using synthetic data. We have demonstrated a cutting-edge method to address data availability difficulties by producing machine-translated data. However, several languages such as Hindi and Bangla still lack adequate investigations due to a lack of datasets. We carried out experimental identification of aggressive comments on Hindi, Bangla, and English datasets using the proposed model and traditional models, including Long Short-Term Memory (LSTM), Bidirectional Long Short-Term Memory (BiLSTM), LSTM-Autoencoder, Word2vec, Bidirectional Encoder Representations from Transformers (BERT), and Generative Pre-trained Transformer 2 (GPT-2) models. We employed evaluation metrics such as f1-score, accuracy, precision, and recall to assess the models performance. Our proposed model outperformed all the models on all datasets, achieving the highest accuracy of 95%. Our model achieves state-of-the-art results among all the previous works on the dataset we used in this paper.
</details>
<details>
<summary>摘要</summary>
社交媒体恐吓行为对人类生活产生负面影响。随着在线社交网络日益增长，讨厌言语也在不断增加。这种厉害的内容可能导致抑郁和自杀行为。本文提议一种可靠的LSTM-Autoencoder网络，用于社交媒体上的恐吓行为检测。我们通过生成机器翻译数据来解决数据可用性问题。然而，一些语言，如希ن第和孟加拉语，仍然缺乏足够的调查，因为数据不足。我们使用提议模型和传统模型，包括LSTM、BiLSTM、LSTM-Autoencoder、Word2vec、BERT和GPT-2模型，进行实验 indentification of aggressive comments。我们使用f1-score、准确率、精度和回归来评估模型的表现。我们的提议模型在所有数据集上都表现出优于所有其他模型，具有最高准确率95%。我们的模型在所有前一个工作中达到了状态 искусственный智能的最佳结果。
</details></li>
</ul>
<hr>
<h2 id="Towards-Temporal-Edge-Regression-A-Case-Study-on-Agriculture-Trade-Between-Nations"><a href="#Towards-Temporal-Edge-Regression-A-Case-Study-on-Agriculture-Trade-Between-Nations" class="headerlink" title="Towards Temporal Edge Regression: A Case Study on Agriculture Trade Between Nations"></a>Towards Temporal Edge Regression: A Case Study on Agriculture Trade Between Nations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07883">http://arxiv.org/abs/2308.07883</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/scylj1/gnn_edge_regression">https://github.com/scylj1/gnn_edge_regression</a></li>
<li>paper_authors: Lekang Jiang, Caiqi Zhang, Farimah Poursafaei, Shenyang Huang</li>
<li>for: 预测国际贸易数据中的边值（edge regression）任务，特别是在静态和动态图中。</li>
<li>methods: 使用图神经网络（Graph Neural Networks，GNNs）模型进行预测，并 introduce three simple yet strong baselines。</li>
<li>results: 实验结果显示baselines在不同设置下表现极其出色，而TGN模型在edge regression任务中表现更好，并且发现训练样本中负边的比例对测试性能产生了重要的影响。<details>
<summary>Abstract</summary>
Recently, Graph Neural Networks (GNNs) have shown promising performance in tasks on dynamic graphs such as node classification, link prediction and graph regression. However, few work has studied the temporal edge regression task which has important real-world applications. In this paper, we explore the application of GNNs to edge regression tasks in both static and dynamic settings, focusing on predicting food and agriculture trade values between nations. We introduce three simple yet strong baselines and comprehensively evaluate one static and three dynamic GNN models using the UN Trade dataset. Our experimental results reveal that the baselines exhibit remarkably strong performance across various settings, highlighting the inadequacy of existing GNNs. We also find that TGN outperforms other GNN models, suggesting TGN is a more appropriate choice for edge regression tasks. Moreover, we note that the proportion of negative edges in the training samples significantly affects the test performance. The companion source code can be found at: https://github.com/scylj1/GNN_Edge_Regression.
</details>
<details>
<summary>摘要</summary>
最近，图 neck Networks (GNNs) 在动态图上的任务中表现出色，包括节点分类、链接预测和图回归。然而，对于时间Edge regression任务，有很少的研究。在这篇论文中，我们探索了 GNNs 在静态和动态设置下的边 regression 任务，特点是预测国家之间的食品和农业贸易值。我们提出了三种简单又强大的基线，并对一个静态和三个动态 GNN 模型进行了广泛的测试，使用 UN Trade 数据集。我们的实验结果表明，基elines 在不同设置下具有极强表现，这 highlights 现有 GNNs 的不足。此外，我们发现 TGN 在边 regression 任务中表现出色， suggesting TGN 是更适合的选择。同时，我们注意到训练样本中负边的比例对测试性能产生了显著影响。相关的源代码可以在 GitHub 上找到：https://github.com/scylj1/GNN_Edge_Regression。
</details></li>
</ul>
<hr>
<h2 id="Synthesizing-Political-Zero-Shot-Relation-Classification-via-Codebook-Knowledge-NLI-and-ChatGPT"><a href="#Synthesizing-Political-Zero-Shot-Relation-Classification-via-Codebook-Knowledge-NLI-and-ChatGPT" class="headerlink" title="Synthesizing Political Zero-Shot Relation Classification via Codebook Knowledge, NLI, and ChatGPT"></a>Synthesizing Political Zero-Shot Relation Classification via Codebook Knowledge, NLI, and ChatGPT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07876">http://arxiv.org/abs/2308.07876</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/snowood1/zero-shot-plover">https://github.com/snowood1/zero-shot-plover</a></li>
<li>paper_authors: Yibo Hu, Erick Skorupa Parolin, Latifur Khan, Patrick T. Brandt, Javier Osorio, Vito J. D’Orazio</li>
<li>for: 这 paper 的目的是提高政治事件代码分类的精度和效率，并利用现有的专家数据库中的知识来避免新的注释创建。</li>
<li>methods: 这 paper 使用了 Zero-shot 方法和自然语言推理（NLI）方法，其中 ZSP 方法采用了树查询框架来解决分类任务中的上下文、模式和类别划分等问题。</li>
<li>results: 经过广泛的实验研究，ZSP 方法在我们新建的数据集上达到了40%的 F1 分数提升，与超级vised BERT 模型的性能相当。这表明 ZSP 方法可以作为政治事件记录验证和 ontology 发展的有价值工具。<details>
<summary>Abstract</summary>
Recent supervised models for event coding vastly outperform pattern-matching methods. However, their reliance solely on new annotations disregards the vast knowledge within expert databases, hindering their applicability to fine-grained classification. To address these limitations, we explore zero-shot approaches for political event ontology relation classification, by leveraging knowledge from established annotation codebooks. Our study encompasses both ChatGPT and a novel natural language inference (NLI) based approach named ZSP. ZSP adopts a tree-query framework that deconstructs the task into context, modality, and class disambiguation levels. This framework improves interpretability, efficiency, and adaptability to schema changes. By conducting extensive experiments on our newly curated datasets, we pinpoint the instability issues within ChatGPT and highlight the superior performance of ZSP. ZSP achieves an impressive 40% improvement in F1 score for fine-grained Rootcode classification. ZSP demonstrates competitive performance compared to supervised BERT models, positioning it as a valuable tool for event record validation and ontology development. Our work underscores the potential of leveraging transfer learning and existing expertise to enhance the efficiency and scalability of research in the field.
</details>
<details>
<summary>摘要</summary>
最近的监督模型对事件编码表现出色，但它们完全依赖于新的注释，忽略了专家数据库中的庞大知识，这限制了它们的应用范围。为了解决这些局限性，我们explore零批处理方法 для政治事件 ontology 关系分类，利用专家注释代码库中的知识。我们的研究包括ChatGPT和一种基于自然语言推理（NLI）的新方法 named ZSP。ZSP采用树查询框架，将任务分解成上下文、模式和分类层次。这种框架提高了可读性、效率和 schema 变化的适应能力。通过对我们新划分的数据集进行广泛的实验，我们揭示了 ChatGPT 中的不稳定性问题，并 highlight了 ZSP 的显著性能优势。ZSP 在细化的 Rootcode 分类任务中实现了40%的提升。ZSP 与超级vised BERT模型的性能相当，这positioned它作为事件记录验证和 ontology 发展的有价值工具。我们的工作强调了利用传输学习和现有专业知识来提高研究领域的效率和扩展性。
</details></li>
</ul>
<hr>
<h2 id="Emotion-Embeddings-unicode-x2014-Learning-Stable-and-Homogeneous-Abstractions-from-Heterogeneous-Affective-Datasets"><a href="#Emotion-Embeddings-unicode-x2014-Learning-Stable-and-Homogeneous-Abstractions-from-Heterogeneous-Affective-Datasets" class="headerlink" title="Emotion Embeddings $\unicode{x2014}$ Learning Stable and Homogeneous Abstractions from Heterogeneous Affective Datasets"></a>Emotion Embeddings $\unicode{x2014}$ Learning Stable and Homogeneous Abstractions from Heterogeneous Affective Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07871">http://arxiv.org/abs/2308.07871</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sven Buechel, Udo Hahn<br>for:这篇论文的目的是提出一种可以独立地学习和拟合不同自然语言、交流Modalities、媒体和表示标签格式的计算模型，以实现情感分析领域的共享表示和可重用性。methods:该论文使用了一种训练程序，通过学习共享的 latent 表示来捕捉情感的多样性，无论是在不同的自然语言、交流Modalities、媒体或表示标签格式中。results:实验表明，该方法可以在各种不同的情感数据集上实现预测质量的稳定和可重用性，而无需拘束到特定的语言、Modalities、媒体或表示标签格式。Code和数据都已经被存储在 <a target="_blank" rel="noopener" href="https://doi.org/10.5281/zenodo.7405327">https://doi.org/10.5281/zenodo.7405327</a> 上。<details>
<summary>Abstract</summary>
Human emotion is expressed in many communication modalities and media formats and so their computational study is equally diversified into natural language processing, audio signal analysis, computer vision, etc. Similarly, the large variety of representation formats used in previous research to describe emotions (polarity scales, basic emotion categories, dimensional approaches, appraisal theory, etc.) have led to an ever proliferating diversity of datasets, predictive models, and software tools for emotion analysis. Because of these two distinct types of heterogeneity, at the expressional and representational level, there is a dire need to unify previous work on increasingly diverging data and label types. This article presents such a unifying computational model. We propose a training procedure that learns a shared latent representation for emotions, so-called emotion embeddings, independent of different natural languages, communication modalities, media or representation label formats, and even disparate model architectures. Experiments on a wide range of heterogeneous affective datasets indicate that this approach yields the desired interoperability for the sake of reusability, interpretability and flexibility, without penalizing prediction quality. Code and data are archived under https://doi.org/10.5281/zenodo.7405327 .
</details>
<details>
<summary>摘要</summary>
人类情感表达在多种通信modalities和媒体格式中表现出来，因此计算研究也是多样化的，包括自然语言处理、音频信号分析、计算机视觉等。在过去的研究中，用于描述情感的多种格式（如偏好级别、基本情绪类别、维度方法、评估理论等）导致了对情感分析的数据和预测模型的总体化，以至于现在的数据和标签类型在不断演化。为了解决这两种不同的多样性，我们提出一种统一的计算模型。我们提议一种培训过程，通过学习情感的共享幂等 représentation来独立于不同的自然语言、通信modalities、媒体或表达标签格式，甚至不同的模型架构。实验表明，这种方法可以实现数据和标签类型之间的可 reuse、可读性和灵活性，无需增加预测质量的减少。代码和数据可以在https://doi.org/10.5281/zenodo.7405327中找到。
</details></li>
</ul>
<hr>
<h2 id="Brain-Inspired-Computational-Intelligence-via-Predictive-Coding"><a href="#Brain-Inspired-Computational-Intelligence-via-Predictive-Coding" class="headerlink" title="Brain-Inspired Computational Intelligence via Predictive Coding"></a>Brain-Inspired Computational Intelligence via Predictive Coding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07870">http://arxiv.org/abs/2308.07870</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tommaso Salvatori, Ankur Mali, Christopher L. Buckley, Thomas Lukasiewicz, Rajesh P. N. Rao, Karl Friston, Alexander Ororbia</li>
<li>for: 这篇论文旨在探讨使用Predictive Coding（PC）理论来解决深度神经网络的限制，以提高机器学习的效果。</li>
<li>methods: 本论文使用Literature survey方法，浏览了 relate to PC 的文献，并 highlighted its potential applications in machine learning and computational intelligence。</li>
<li>results: 论文表明，PC 可以用于模型脑内信息处理，可以应用于认知控制和机器人学习，并具有强大的数学基础，可以用于特定类型的连续状态生成模型的倒逼算法。<details>
<summary>Abstract</summary>
Artificial intelligence (AI) is rapidly becoming one of the key technologies of this century. The majority of results in AI thus far have been achieved using deep neural networks trained with the error backpropagation learning algorithm. However, the ubiquitous adoption of this approach has highlighted some important limitations such as substantial computational cost, difficulty in quantifying uncertainty, lack of robustness, unreliability, and biological implausibility. It is possible that addressing these limitations may require schemes that are inspired and guided by neuroscience theories. One such theory, called predictive coding (PC), has shown promising performance in machine intelligence tasks, exhibiting exciting properties that make it potentially valuable for the machine learning community: PC can model information processing in different brain areas, can be used in cognitive control and robotics, and has a solid mathematical grounding in variational inference, offering a powerful inversion scheme for a specific class of continuous-state generative models. With the hope of foregrounding research in this direction, we survey the literature that has contributed to this perspective, highlighting the many ways that PC might play a role in the future of machine learning and computational intelligence at large.
</details>
<details>
<summary>摘要</summary>
人工智能（AI）在这个世纪快速成为一个关键技术。目前大多数AI成果都是使用深度神经网络和错误反射学习算法获得的。然而，这种广泛采用的方法存在一些重要的限制，如计算成本很高、难以量化不确定性、缺乏可靠性和生物可能性。可能需要采用基于神经科学理论的方案来解决这些限制。一种如此理论是预测编码（PC），它在机器智能任务中表现出了惊喜性，并且具有可能为机器学习社区提供价值的特性：PC可以模型不同脑区的信息处理方式，可以应用于认知控制和机器人学习，并且具有强制VARIATIONAL推理的数学基础，可以为某些连续状态生成模型提供强大的逆转计划。希望通过这篇文章，推动研究人员对这个视角的研究，并强调PC在未来机器学习和计算智能中的潜在作用。
</details></li>
</ul>
<hr>
<h2 id="Graph-Structured-Kernel-Design-for-Power-Flow-Learning-using-Gaussian-Processes"><a href="#Graph-Structured-Kernel-Design-for-Power-Flow-Learning-using-Gaussian-Processes" class="headerlink" title="Graph-Structured Kernel Design for Power Flow Learning using Gaussian Processes"></a>Graph-Structured Kernel Design for Power Flow Learning using Gaussian Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07867">http://arxiv.org/abs/2308.07867</a></li>
<li>repo_url: None</li>
<li>paper_authors: Parikshit Pareek, Deepjyoti Deka, Sidhant Misra</li>
<li>for: 这种physics-inspired graph-structured kernel是用于电流流行学习的Gaussian Process（GP）中的一种kernel，旨在利用网络图或结构来抽象Voltage-injection关系的latent decomposition。</li>
<li>methods: 这种kernel被称为 vertex-degree kernel（VDK），它不需要解决优化问题来搜索kernel。此外，我们还提出了一种图缩 Representation with fewer terms，以提高效率。</li>
<li>results: 对于500-Bus和1354-Bus电力系统，我们的VDK-GP方法可以在样本复杂性方面减少超过两倍，相比于全GP。此外，我们的网络滑块活动学习算法可以在测试预测中超过mean Performance of 500 Random Trials by two fold for medium-sized 500-Bus systems and best performance of 25 random trials for large-scale 1354-Bus systems by 10%.<details>
<summary>Abstract</summary>
This paper presents a physics-inspired graph-structured kernel designed for power flow learning using Gaussian Process (GP). The kernel, named the vertex-degree kernel (VDK), relies on latent decomposition of voltage-injection relationship based on the network graph or topology. Notably, VDK design avoids the need to solve optimization problems for kernel search. To enhance efficiency, we also explore a graph-reduction approach to obtain a VDK representation with lesser terms. Additionally, we propose a novel network-swipe active learning scheme, which intelligently selects sequential training inputs to accelerate the learning of VDK. Leveraging the additive structure of VDK, the active learning algorithm performs a block-descent type procedure on GP's predictive variance, serving as a proxy for information gain. Simulations demonstrate that the proposed VDK-GP achieves more than two fold sample complexity reduction, compared to full GP on medium scale 500-Bus and large scale 1354-Bus power systems. The network-swipe algorithm outperforms mean performance of 500 random trials on test predictions by two fold for medium-sized 500-Bus systems and best performance of 25 random trials for large-scale 1354-Bus systems by 10%. Moreover, we demonstrate that the proposed method's performance for uncertainty quantification applications with distributionally shifted testing data sets.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Impression-Aware-Recommender-Systems"><a href="#Impression-Aware-Recommender-Systems" class="headerlink" title="Impression-Aware Recommender Systems"></a>Impression-Aware Recommender Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07857">http://arxiv.org/abs/2308.07857</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fernando B. Pérez Maurera, Maurizio Ferrari Dacrema, Pablo Castells, Paolo Cremonesi</li>
<li>for: 本研究旨在探讨基于印象（过去推荐的项目）的 recommender system 的开发和应用，以提高推荐系统的质量。</li>
<li>methods: 本文使用系统性文献综述方法，对各种推荐系统使用印象的研究进行分类和详细介绍，同时还总结了不同的数据集和评价方法。</li>
<li>results: 本文通过对各种推荐系统使用印象的研究进行分类和详细介绍，掌握了各种研究的方法和结论，并发现了一些未在文献中提到的问题和未来研究方向。<details>
<summary>Abstract</summary>
Novel data sources bring new opportunities to improve the quality of recommender systems. Impressions are a novel data source containing past recommendations (shown items) and traditional interactions. Researchers may use impressions to refine user preferences and overcome the current limitations in recommender systems research. The relevance and interest of impressions have increased over the years; hence, the need for a review of relevant work on this type of recommenders. We present a systematic literature review on recommender systems using impressions, focusing on three fundamental angles in research: recommenders, datasets, and evaluation methodologies. We provide three categorizations of papers describing recommenders using impressions, present each reviewed paper in detail, describe datasets with impressions, and analyze the existing evaluation methodologies. Lastly, we present open questions and future directions of interest, highlighting aspects missing in the literature that can be addressed in future works.
</details>
<details>
<summary>摘要</summary>
新的数据源带来了改善推荐系统质量的新机会。印象是一种新的数据源，包含过去的推荐（显示的项目）和传统的交互。研究人员可以使用印象来细化用户的偏好，超越当前推荐系统研究的限制。随着年代的推移，印象的相关性和兴趣度也在增长，因此需要对这类推荐系统的研究进行系统atic literature review。本文对推荐系统使用印象进行系统atic literature review，将研究分为三个基本的视角：推荐器、数据集和评估方法ологи。我们对每篇评论细节描述、介绍数据集，并分析现有的评估方法ологи。最后，我们提出了未解决的问题和未来方向，强调文献中缺失的方面，可以在未来的研究中解决。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/16/cs.LG_2023_08_16/" data-id="clm0t8e0h0072v7880ulx2b1a" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_08_16" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/16/cs.SD_2023_08_16/" class="article-date">
  <time datetime="2023-08-15T16:00:00.000Z" itemprop="datePublished">2023-08-16</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/16/cs.SD_2023_08_16/">cs.SD - 2023-08-16 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Mitigating-the-Exposure-Bias-in-Sentence-Level-Grapheme-to-Phoneme-G2P-Transduction"><a href="#Mitigating-the-Exposure-Bias-in-Sentence-Level-Grapheme-to-Phoneme-G2P-Transduction" class="headerlink" title="Mitigating the Exposure Bias in Sentence-Level Grapheme-to-Phoneme (G2P) Transduction"></a>Mitigating the Exposure Bias in Sentence-Level Grapheme-to-Phoneme (G2P) Transduction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08442">http://arxiv.org/abs/2308.08442</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eunseop Yoon, Hee Suk Yoon, Dhananjaya Gowda, SooHwan Eom, Daehyeok Kim, John Harvill, Heting Gao, Mark Hasegawa-Johnson, Chanwoo Kim, Chang D. Yoo</li>
<li>for: 这个论文是为了提高文本中字符串到音节转换（G2P）的性能而写的。</li>
<li>methods: 这个论文使用了Tokenizer-free byte-level模型（ByT5），通过表示每个输入字符的UTF-8编码来实现字符串到音节转换。</li>
<li>results: 这个论文发现，使用ByT5进行句子级或段落级G2P可以提高实际应用中的用户体验，但是需要避免曝光偏见常见在自动生成模型中。<details>
<summary>Abstract</summary>
Text-to-Text Transfer Transformer (T5) has recently been considered for the Grapheme-to-Phoneme (G2P) transduction. As a follow-up, a tokenizer-free byte-level model based on T5 referred to as ByT5, recently gave promising results on word-level G2P conversion by representing each input character with its corresponding UTF-8 encoding. Although it is generally understood that sentence-level or paragraph-level G2P can improve usability in real-world applications as it is better suited to perform on heteronyms and linking sounds between words, we find that using ByT5 for these scenarios is nontrivial. Since ByT5 operates on the character level, it requires longer decoding steps, which deteriorates the performance due to the exposure bias commonly observed in auto-regressive generation models. This paper shows that the performance of sentence-level and paragraph-level G2P can be improved by mitigating such exposure bias using our proposed loss-based sampling method.
</details>
<details>
<summary>摘要</summary>
文本-to-文本传输变换器（T5）最近被考虑用于文本-to-phoneme（G2P）转换。作为继续，一个不需要tokenizer的字节级模型基于T5，称之为ByT5，最近在word-level G2P转换中表现出了扎实的结果。虽然普遍认为 sentence-level或paragraph-level G2P可以提高实际应用中的可用性，因为更适合处理Homonyms和 слова间的连接音，但我们发现使用ByT5进行这些场景是非常困难。因为ByT5操作在字符水平，需要更长的解码步骤，这会导致性能下降，这是因为自动生成模型通常会出现露示偏见。本文显示，使用我们提议的损失采样方法可以提高 sentence-level和paragraph-level G2P的性能。
</details></li>
</ul>
<hr>
<h2 id="Classifying-Dementia-in-the-Presence-of-Depression-A-Cross-Corpus-Study"><a href="#Classifying-Dementia-in-the-Presence-of-Depression-A-Cross-Corpus-Study" class="headerlink" title="Classifying Dementia in the Presence of Depression: A Cross-Corpus Study"></a>Classifying Dementia in the Presence of Depression: A Cross-Corpus Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08306">http://arxiv.org/abs/2308.08306</a></li>
<li>repo_url: None</li>
<li>paper_authors: Franziska Braun, Sebastian P. Bayerl, Paula A. Pérez-Toro, Florian Hönig, Hartmut Lehfeld, Thomas Hillemacher, Elmar Nöth, Tobias Bocklet, Korbinian Riedhammer</li>
<li>for: 这个论文的目的是提出一种自动诊断老年痴呆症的方法，以提高医疗系统的效率和患者的生活质量。</li>
<li>methods: 这个论文使用了语音、文本和情感嵌入来分类诊断老年痴呆症，并在三个类别中进行了比较（健康人 VS 轻度智能障碍 VS 老年痴呆症）。</li>
<li>results: 研究人员通过对两个独立录制的德国数据集进行交叉验证和混合验证，发现了这种方法的普适性和可重复性。<details>
<summary>Abstract</summary>
Automated dementia screening enables early detection and intervention, reducing costs to healthcare systems and increasing quality of life for those affected. Depression has shared symptoms with dementia, adding complexity to diagnoses. The research focus so far has been on binary classification of dementia (DEM) and healthy controls (HC) using speech from picture description tests from a single dataset. In this work, we apply established baseline systems to discriminate cognitive impairment in speech from the semantic Verbal Fluency Test and the Boston Naming Test using text, audio and emotion embeddings in a 3-class classification problem (HC vs. MCI vs. DEM). We perform cross-corpus and mixed-corpus experiments on two independently recorded German datasets to investigate generalization to larger populations and different recording conditions. In a detailed error analysis, we look at depression as a secondary diagnosis to understand what our classifiers actually learn.
</details>
<details>
<summary>摘要</summary>
自动化认知评估可以早期检测和 intervene，降低医疗系统的成本和提高认知症患者的生活质量。与认知症有共同症状的抑郁症可以增加诊断的复杂性。过去的研究主要集中在使用单一数据集的语音描述测验进行二分类认知症和健康控制（HC）的分类。在这个工作中，我们使用已经建立的基eline系统来分辨语音中的认知障碍，使用 semantic Verbal Fluency Test 和 Boston Naming Test 的文本、音频和情感嵌入，进行三类分类问题（HC vs. MCI vs. DEM）。我们在两个独立录取的德国数据集上进行交叉数据和混合数据实验，以调查更大的人口和不同的录音条件下的一致性。在详细的错误分析中，我们将抑郁症作为次要诊断来了解我们的分类器是否真的学习了什么。
</details></li>
</ul>
<hr>
<h2 id="ChinaTelecom-System-Description-to-VoxCeleb-Speaker-Recognition-Challenge-2023"><a href="#ChinaTelecom-System-Description-to-VoxCeleb-Speaker-Recognition-Challenge-2023" class="headerlink" title="ChinaTelecom System Description to VoxCeleb Speaker Recognition Challenge 2023"></a>ChinaTelecom System Description to VoxCeleb Speaker Recognition Challenge 2023</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08181">http://arxiv.org/abs/2308.08181</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mengjie Du, Xiang Fang, Jie Li</li>
<li>for: This paper is written for the VoxCeleb2023 Speaker Recognition Challenge (VoxSRC 2023) and describes the ChinaTelecom system for Track 1 (closed).</li>
<li>methods: The system consists of several ResNet variants trained only on VoxCeleb2, which were fused for better performance later. Score calibration was also applied for each variant and the fused system.</li>
<li>results: The final submission achieved minDCF of 0.1066 and EER of 1.980%.Here are the three key points in Simplified Chinese:</li>
<li>for: 这篇论文是为了VOXCELEB2023发音识别挑战（VOXSRC2023）的Track 1（关闭）而写的。</li>
<li>methods: 该系统包括几种基于VOXCELEB2的ResNet变体，这些变体被后续进行了融合以提高表现。此外，每个变体和融合系统还进行了分数调整。</li>
<li>results: 最终提交的结果为minDCF为0.1066和EER为1.980%。<details>
<summary>Abstract</summary>
This technical report describes ChinaTelecom system for Track 1 (closed) of the VoxCeleb2023 Speaker Recognition Challenge (VoxSRC 2023). Our system consists of several ResNet variants trained only on VoxCeleb2, which were fused for better performance later. Score calibration was also applied for each variant and the fused system. The final submission achieved minDCF of 0.1066 and EER of 1.980%.
</details>
<details>
<summary>摘要</summary>
这份技术报告介绍了我们在VoxCeleb2023 Speaker Recognition Challenge（VoxSRC 2023）的Track 1（关闭）系统。我们的系统包括了多种ResNet变体，只在VoxCeleb2上进行训练。这些变体后来进行了融合，以提高性能。此外，我们还应用了分数均衡calibration对每个变体和融合系统。最终的提交达到了0.1066的minDCF和1.980%的EER。
</details></li>
</ul>
<hr>
<h2 id="AffectEcho-Speaker-Independent-and-Language-Agnostic-Emotion-and-Affect-Transfer-for-Speech-Synthesis"><a href="#AffectEcho-Speaker-Independent-and-Language-Agnostic-Emotion-and-Affect-Transfer-for-Speech-Synthesis" class="headerlink" title="AffectEcho: Speaker Independent and Language-Agnostic Emotion and Affect Transfer for Speech Synthesis"></a>AffectEcho: Speaker Independent and Language-Agnostic Emotion and Affect Transfer for Speech Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08577">http://arxiv.org/abs/2308.08577</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hrishikesh Viswanath, Aneesh Bhattacharya, Pascal Jutras-Dubé, Prerit Gupta, Mridu Prashanth, Yashvardhan Khaitan, Aniket Bera</li>
<li>for: The paper is written for discussing a new approach to emotional translation in text-to-speech and speech-to-speech systems, with the goal of capturing complex nuances and subtle differences in emotions.</li>
<li>methods: The proposed approach, called AffectEcho, uses a Vector Quantized codebook to model emotions within a quantized space featuring five levels of affect intensity. The quantized emotional embeddings are implicitly derived from spoken speech samples, eliminating the need for one-hot vectors or explicit strength embeddings.</li>
<li>results: The experimental results demonstrate the effectiveness of the proposed approach in controlling the emotions of generated speech while preserving identity, style, and emotional cadence unique to each speaker. The language-independent emotion modeling capability of the quantized emotional embeddings learned from a bilingual (English and Chinese) speech corpus is also shown, with an emotion transfer task from a reference speech to a target speech achieving state-of-art results on both qualitative and quantitative metrics.Here are the three points in Simplified Chinese text:</li>
<li>for: 本文是为了介绍一种新的情感翻译方法，用于文本到语音和语音到语音系统中，以便捕捉复杂的情感细微差异。</li>
<li>methods: 提议的方法是使用Vector Quantized codebook来模型情感，在一个5级情感强度的量化空间中进行模型。这些量化情感嵌入不需要一个热度 вектор或者显式强度嵌入。</li>
<li>results: 实验结果表明，提议的方法能够控制生成的语音中的情感，同时保持每个 speaker 的个性、风格和情感节奏。此外，通过一种语言无关的情感模型，在一个英文和中文双语Speech corpus中学习的情感嵌入可以在另一种语言中进行情感传递任务，并达到了当前最佳的质量和量化指标。<details>
<summary>Abstract</summary>
Affect is an emotional characteristic encompassing valence, arousal, and intensity, and is a crucial attribute for enabling authentic conversations. While existing text-to-speech (TTS) and speech-to-speech systems rely on strength embedding vectors and global style tokens to capture emotions, these models represent emotions as a component of style or represent them in discrete categories. We propose AffectEcho, an emotion translation model, that uses a Vector Quantized codebook to model emotions within a quantized space featuring five levels of affect intensity to capture complex nuances and subtle differences in the same emotion. The quantized emotional embeddings are implicitly derived from spoken speech samples, eliminating the need for one-hot vectors or explicit strength embeddings. Experimental results demonstrate the effectiveness of our approach in controlling the emotions of generated speech while preserving identity, style, and emotional cadence unique to each speaker. We showcase the language-independent emotion modeling capability of the quantized emotional embeddings learned from a bilingual (English and Chinese) speech corpus with an emotion transfer task from a reference speech to a target speech. We achieve state-of-art results on both qualitative and quantitative metrics.
</details>
<details>
<summary>摘要</summary>
“情感”是一种情感特征，包括价值观、情感刺激和强度，这种特征对实际对话的进行是关键。现有的文本到语音（TTS）和语音到语音系统通常使用强度嵌入向量和全局风格token来捕捉情感，但这些模型表示情感为样式的一部分或以分类的方式表示。我们提议的情感回声模型（AffectEcho）使用量化编码 кни簿来模型情感在量化空间中的五级强度，以捕捉复杂的细节和同一种情感中的微妙差异。这些量化情感嵌入不需要一个一热 вектор或显式强度嵌入。我们的方法可以控制生成的语音中的情感，保留每个说话者的个性、风格和情感节奏。我们在一个英文和中文语音词汇库中学习的语言独立情感模型能够完成参照语音到目标语音的情感传递任务，并在质量和量度指标上达到了当前最佳效果。
</details></li>
</ul>
<hr>
<h2 id="SCANet-A-Self-and-Cross-Attention-Network-for-Audio-Visual-Speech-Separation"><a href="#SCANet-A-Self-and-Cross-Attention-Network-for-Audio-Visual-Speech-Separation" class="headerlink" title="SCANet: A Self- and Cross-Attention Network for Audio-Visual Speech Separation"></a>SCANet: A Self- and Cross-Attention Network for Audio-Visual Speech Separation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08143">http://arxiv.org/abs/2308.08143</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kai Li, Runxuan Yang, Xiaolin Hu</li>
<li>for: 这篇论文主要用于探讨一种新的多模态协同分离方法，即自适应和交叉关注网络（SCANet），该方法可以有效地融合音频和视频特征，以提高对话人的识别率。</li>
<li>methods: SCANet 使用了两种听取块：自适应（SA）块和交叉关注（CA）块，其中 CA 块分布在网络的顶部（TCA）、中部（MCA）和底部（BCA）。这些块可以学习不同的模式特征，并提取不同的 semantics 从音频和视频特征。</li>
<li>results: 根据三个标准的音频视频分离 benchmark（LRS2、LRS3 和 VoxCeleb2）的实验结果，SCANet 比现有的状态对比方法（SOTA）高效，同时保持相对的执行时间。<details>
<summary>Abstract</summary>
The integration of different modalities, such as audio and visual information, plays a crucial role in human perception of the surrounding environment. Recent research has made significant progress in designing fusion modules for audio-visual speech separation. However, they predominantly focus on multi-modal fusion architectures situated either at the top or bottom positions, rather than comprehensively considering multi-modal fusion at various hierarchical positions within the network. In this paper, we propose a novel model called self- and cross-attention network (SCANet), which leverages the attention mechanism for efficient audio-visual feature fusion. SCANet consists of two types of attention blocks: self-attention (SA) and cross-attention (CA) blocks, where the CA blocks are distributed at the top (TCA), middle (MCA) and bottom (BCA) of SCANet. These blocks maintain the ability to learn modality-specific features and enable the extraction of different semantics from audio-visual features. Comprehensive experiments on three standard audio-visual separation benchmarks (LRS2, LRS3, and VoxCeleb2) demonstrate the effectiveness of SCANet, outperforming existing state-of-the-art (SOTA) methods while maintaining comparable inference time.
</details>
<details>
<summary>摘要</summary>
人类在识别环境中利用不同模式的感知信息，如音频和视觉信息，进行集成很重要。现代研究已经在设计多模态融合模块方面做出了重要进步，但是这些模型大多集中于网络的顶层或底层位置，而不是全面考虑多模态融合在网络各个层次位置。本文提出了一种新的模型，即自身和交叉注意网络（SCANet），它利用注意机制来有效地融合音频和视觉特征。SCANet包括两种注意块：自身注意（SA）和交叉注意（CA）块，其中CA块分布在网络顶层（TCA）、中层（MCA）和底层（BCA）。这些块可以学习不同模式的特征，并允许从音频和视觉特征中提取不同的 semantics。我们对三个标准音频视频分离 benchmark（LRS2、LRS3和VoxCeleb2）进行了广泛的实验， demonstarted SCANet的效果，而且与现有的最佳方法（SOTA）保持相对的推理时间。
</details></li>
</ul>
<hr>
<h2 id="Radio2Text-Streaming-Speech-Recognition-Using-mmWave-Radio-Signals"><a href="#Radio2Text-Streaming-Speech-Recognition-Using-mmWave-Radio-Signals" class="headerlink" title="Radio2Text: Streaming Speech Recognition Using mmWave Radio Signals"></a>Radio2Text: Streaming Speech Recognition Using mmWave Radio Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08125">http://arxiv.org/abs/2308.08125</a></li>
<li>repo_url: None</li>
<li>paper_authors: Running Zhao, Jiangtao Yu, Hang Zhao, Edith C. H. Ngai</li>
<li>for: 这篇论文旨在提供一种基于 millimeter wave (mmWave) 技术的语音识别系统，以便在会议听录和窃听等场景中实现高精度语音识别。</li>
<li>methods: 该系统基于一种适配 streams 的 transformer 模型，通过特定的束缚和知识储存技术来实现大词汇量语音识别。</li>
<li>results: 实验结果显示，Radio2Text 可以在 recognizing 一个包含超过 13,000 个词的词汇库时达到 character error rate 5.7% 和 word error rate 9.4%。<details>
<summary>Abstract</summary>
Millimeter wave (mmWave) based speech recognition provides more possibility for audio-related applications, such as conference speech transcription and eavesdropping. However, considering the practicality in real scenarios, latency and recognizable vocabulary size are two critical factors that cannot be overlooked. In this paper, we propose Radio2Text, the first mmWave-based system for streaming automatic speech recognition (ASR) with a vocabulary size exceeding 13,000 words. Radio2Text is based on a tailored streaming Transformer that is capable of effectively learning representations of speech-related features, paving the way for streaming ASR with a large vocabulary. To alleviate the deficiency of streaming networks unable to access entire future inputs, we propose the Guidance Initialization that facilitates the transfer of feature knowledge related to the global context from the non-streaming Transformer to the tailored streaming Transformer through weight inheritance. Further, we propose a cross-modal structure based on knowledge distillation (KD), named cross-modal KD, to mitigate the negative effect of low quality mmWave signals on recognition performance. In the cross-modal KD, the audio streaming Transformer provides feature and response guidance that inherit fruitful and accurate speech information to supervise the training of the tailored radio streaming Transformer. The experimental results show that our Radio2Text can achieve a character error rate of 5.7% and a word error rate of 9.4% for the recognition of a vocabulary consisting of over 13,000 words.
</details>
<details>
<summary>摘要</summary>
微米波（mmWave）基于语音识别提供更多的音频相关应用，如会议语音转文和窃听。然而，在实际场景中，延迟和可识别词汇数是两个关键因素，不能被忽略。在这篇论文中，我们提出Radio2Text，首个基于微米波的流处理自动语音识别（ASR）系统，可以识别超过13,000个词的词汇。Radio2Text基于适应流处理变换器，可以有效地学习语音相关特征的表示，为流处理ASR带来新的可能性。为了解决流处理网络无法访问整个未来输入的缺陷，我们提出引导初始化，通过重量继承来传递非流处理变换器中的特征知识相关全局 контекст到适应流处理变换器。此外，我们提出了基于知识传授（KD）的交叉模态结构，称为交叉模态KD，以mitigate低质量微米波信号对识别性的负面效应。在交叉模态KD中，音频流处理变换器提供特征和回应导航，将有用和准确的语音信息继承给适应广播流处理变换器进行超vision训练。实验结果显示，我们的Radio2Text可以达到Character Error Rate（CER）5.7%和Word Error Rate（WER）9.4%，用于识别超过13,000个词的词汇。
</details></li>
</ul>
<hr>
<h2 id="End-to-End-Open-Vocabulary-Keyword-Search-With-Multilingual-Neural-Representations"><a href="#End-to-End-Open-Vocabulary-Keyword-Search-With-Multilingual-Neural-Representations" class="headerlink" title="End-to-End Open Vocabulary Keyword Search With Multilingual Neural Representations"></a>End-to-End Open Vocabulary Keyword Search With Multilingual Neural Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08027">http://arxiv.org/abs/2308.08027</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bolaji Yusuf, Jan Cernocky, Murat Saraclar</li>
<li>for: 提高 keyword search 系统的效率和简化搜索过程，不需要自动语音识别（ASR）输出。</li>
<li>methods: 使用神经网络encoder对查询和文档进行编码，并将编码结果进行点积 multiplication 组合。</li>
<li>results: 在长查询和没有在训练数据中出现的查询中，提高了模型的性能，并且对于短查询和包含在 vocabulary 中的查询，虽然不能与强大的 ASR-based 传统搜索系统匹配，但是仍然超过了 ASR-based 系统。<details>
<summary>Abstract</summary>
Conventional keyword search systems operate on automatic speech recognition (ASR) outputs, which causes them to have a complex indexing and search pipeline. This has led to interest in ASR-free approaches to simplify the search procedure. We recently proposed a neural ASR-free keyword search model which achieves competitive performance while maintaining an efficient and simplified pipeline, where queries and documents are encoded with a pair of recurrent neural network encoders and the encodings are combined with a dot-product. In this article, we extend this work with multilingual pretraining and detailed analysis of the model. Our experiments show that the proposed multilingual training significantly improves the model performance and that despite not matching a strong ASR-based conventional keyword search system for short queries and queries comprising in-vocabulary words, the proposed model outperforms the ASR-based system for long queries and queries that do not appear in the training data.
</details>
<details>
<summary>摘要</summary>
Here is the text in Simplified Chinese:传统的关键词搜索系统采用自动语音识别（ASR）输出，这导致搜索管道变得复杂。这引起了关注ASR-free方法，以简化搜索过程。我们最近提出了一种基于神经网络的ASR-free关键词搜索模型，该模型在竞争性和效率方面具有优异表现，而无需复杂的搜索管道。在这篇文章中，我们延续这种工作，并通过多语言预训练和详细分析，进一步提高模型性能。我们的实验表明，提档多语言训练显著提高模型性能，并且对于长 queries和不在训练数据中出现的 queries，模型的性能胜过ASR-based系统。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/16/cs.SD_2023_08_16/" data-id="clm0t8e1c00aav788ftlyclrk" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_08_16" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/16/eess.AS_2023_08_16/" class="article-date">
  <time datetime="2023-08-15T16:00:00.000Z" itemprop="datePublished">2023-08-16</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/16/eess.AS_2023_08_16/">eess.AS - 2023-08-16 22:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="The-ID-R-D-VoxCeleb-Speaker-Recognition-Challenge-2023-System-Description"><a href="#The-ID-R-D-VoxCeleb-Speaker-Recognition-Challenge-2023-System-Description" class="headerlink" title="The ID R&amp;D VoxCeleb Speaker Recognition Challenge 2023 System Description"></a>The ID R&amp;D VoxCeleb Speaker Recognition Challenge 2023 System Description</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08294">http://arxiv.org/abs/2308.08294</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nikita Torgashov, Rostislav Makarov, Ivan Yakovlev, Pavel Malov, Andrei Balykin, Anton Okhotnikov</li>
<li>for: 这个研究是为了参加2023年VoxCeleb Speaker Recognition Challenge（VoxSRC-23）的Track 2（开放赛）而撰写的。</li>
<li>methods: 该解决方案基于深度ResNet和自动标注学习（SSL）基于模型，在一个组合的VoxCeleb2数据集和大量的VoxTube数据集上进行训练。</li>
<li>results: 最终在Track 2上提交的解决方案在VoxSRC-23公共排名板上达到了第一名， minDCF(0.05) 为0.0762， EER 为1.30%。<details>
<summary>Abstract</summary>
This report describes ID R&D team submissions for Track 2 (open) to the VoxCeleb Speaker Recognition Challenge 2023 (VoxSRC-23). Our solution is based on the fusion of deep ResNets and self-supervised learning (SSL) based models trained on a mixture of a VoxCeleb2 dataset and a large version of a VoxTube dataset. The final submission to the Track 2 achieved the first place on the VoxSRC-23 public leaderboard with a minDCF(0.05) of 0.0762 and EER of 1.30%.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/16/eess.AS_2023_08_16/" data-id="clm0t8e1w00ccv7884xi21dcn" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/5/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><a class="page-number" href="/page/5/">5</a><span class="page-number current">6</span><a class="page-number" href="/page/7/">7</a><a class="page-number" href="/page/8/">8</a><span class="space">&hellip;</span><a class="page-number" href="/page/29/">29</a><a class="extend next" rel="next" href="/page/7/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">60</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">56</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">112</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
